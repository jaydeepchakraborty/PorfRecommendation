Complex engineered systems arise throughout computing, communications, and networking. Many factors, each having a finite number of levels, impact the behaviour of the system either singly or in interaction with one another. Testing or evaluating such a system involves formulating a set of tests, when executed, responses or outcomes from the tests are analyzed. A single round of testing is conducted. To witness the effect of an interaction, some test must cover it, this does not suffice in general to locate the interaction or to measure its effect. When there are few factors or many tests, experimental designs can measure (and hence locate) the interactions. When there are many factors and few tests, can we locate the interaction(s)? Can we efficiently detect them?Combinatorial arrays, locating and detecting arrays, are introduced to address such location and detection in the context of combinatorial testing. Locating and detecting arrays are contrasted with covering arrays and with experimental designs. An application to a 75 factor protocol stack for file transfer is given to demonstrate their practical use. Finally, their place in the literature of combinatorial testing is discussed and some directions are outlined.IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

1

A Combinatorial Approach to X-Tolerant Compaction Circuits
Yuichiro Fujiwara and Charles J. Colbourn

arXiv:1508.00481v1 [cs.IT] 3 Aug 2015

Abstract--Test response compaction for integrated circuits (ICs) with scan-based design-for-testability (DFT) support in the presence of unknown logic values (Xs) is investigated from a combinatorial viewpoint. The theoretical foundations of Xcodes, employed in an X-tolerant compaction technique called X-compact, are examined. Through the formulation of a combinatorial model of X-compact, novel design techniques are developed for X-codes to detect a specified maximum number of errors in the presence of a specified maximum number of unknown logic values, while requiring only small fan-out. The special class of X-codes that results leads to an avoidance problem for configurations in combinatorial designs. General design methods and nonconstructive existence theorems to estimate the compaction ratio of an optimal X-compactor are also derived. Index Terms--Circuit testing, built-in self-test (BIST), compaction, X-compact, test compression, X-code, superimposed code, Steiner system, configuration.

I. I NTRODUCTION HIS work discusses a class of codes that arise in data volume compaction of responses from integrated circuits (ICs) under scan-based test. We first recall briefly the background of the X-tolerant compaction technique in digital circuit testing. Digital circuit testing applies test patterns to a circuit under test and monitors the circuit's responses to the applied patterns. A tester compares the observed response to a test pattern to the expected response and, if there is a mismatch, declares the circuit chip defective. Usually the expected responses are obtained through fault-free simulation of the chip. Test cost for traditional scan-based testing is dominated by test data volume and test time [1]. Therefore various test compression techniques have been developed to reduce test cost. One way to achieve this is to reduce test application time and the number of test patterns by employing automatic test pattern generation (ATPG) (see [2]­[5] and references therein). We are interested in the other kind of technique, using methods to hash responses while maintaining test quality. Signature analyzers (e.g., [6]­[10]) are vulnerable to error masking caused by unknown logic values (Xs) [11]. X-compact has been proposed in order to conduct reliable testing in the presence of Xs [12]. A response compaction circuit based
This work was supported in part by JSPS Research Fellowships for Young Scientists (YF) and by DOD grants N00014-08-1-1069 and N00014-08-11070 (CJC). Y. Fujiwara is with the Department of Mathematical Sciences, Michigan Technological University, Houghton, MI 49931 USA. yfujiwar@mtu.edu. C. J. Colbourn is with the School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287-8809 USA. charles.colbourn@asu.edu

T

on X-compact is an X-compactor. X-compactors have proved their high error detection ability in actual systems [11], [13]. An X-compactor can be written in matrix form as an X-code [14]. Basic properties of X-codes have been studied [14], [15]. Graph theoretic techniques have been employed to minimize fan-out of inputs [16]; in general an X-compactor tolerates the presence of Xs in exchange for large fan-out. These studies focus on particular classes of X-codes rather than the general coding theoretic aspects. The purpose of the present paper is to investigate theoretical foundations of X-codes and to provide general construction techniques. In Section II we outline the combinatorial requirements for the X-compact technique and present an equivalent definition of X-codes in order to investigate X-compactors as codes and combinatorial designs. Some basic properties of X-codes are also presented. In Section III we investigate X-codes that require only small fan-out and have good error detectability and X-tolerance. We prove the equivalence between a class of Steiner t-designs and particular X-codes having the maximum number of codewords and the minimum fan-out. This allows us to give constructions and to show existence of such X-codes. Section IV deals with existence of X-codes in the more general situation. Both constructive and nonconstructive theorems are provided. Finally we conclude in Section V. II. C OMBINATORIAL R EQUIREMENTS
AND

X-C ODES

We do not describe scan-based testing and response compaction in detail here, instead referring the reader to [11], [12]. Scan-based testing repeatedly applies vectors of test inputs to the circuit, and for each test captures a vector from {0, 1}n as the test output. Naturally it is important that the test output be the correct one. To determine this, the function of the circuit is simulated (in a fault-free manner) to produce a reference output. When the test and reference outputs agree, no fault has been detected. The first major obstacle is that fault-free simulation may be unable to determine whether a specific output is 0 or 1, and hence it is an unknown logic value X. The second is that if each output requires a separate pin on the chip, the number of tests that can be accommodated is quite restricted. We deal with these two problems in turn. We define an algebraic system to describe the behavior of Xs. The X-algebra X2 = ({0, 1, X}, +, ·) over the field F2 is the set {0, 1} of elements of F2 and a third element X, equipped with two binary operations "+" (addition) and "·" (multiplication) satisfying: 1) a + b and a · b are performed in F2 for a, b  F2 ;

2

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

2) a + X = X + a = X for a  F2 ; 3) 0 · X = X · 0 = 0 for the additive identity 0; 4) 1 · X = X · 1 = X. The element X is termed an unknown logic value. Now consider a test output b = (b1 , . . . , bn )  {0, 1}n and a reference output c = (c1 , . . . , cn )  {0, 1, X}n . When ci  {0, 1}, the test and reference outputs agree on the ith bit when bi = ci ; otherwise the ith bit is an error bit. When ci = X, whatever the value of bi , no error is detected. Thus the ith bit is (known to be) in error if and only if bi + ci = 1, using addition in X2 . Turning to the second problem, an X-compact matrix is an n × m matrix H with elements from {0, 1}. The compaction ratio of H is n/m. The number of 1s in the ith row is the weight, or fan-out, of row i. Output (or response) compaction is performed by computing the vector d = (d1 , . . . , dm ) = bH for output (arithmetic is in X2 ). In the same way, the reference output can be compacted using the same matrix to form r = (r1 , . . . , rm ) = cH . As before, if di = ri and ri = X (that is, if di + ri = 1), an error is detected. To be of practical value, an X-compact matrix H should detect the presence of error bits in b with respect to c given the compacted vectors d and r under `reasonable' restrictions on the number of errors and number of unknown logic values. Suppose that b + c = 1 (so that there is a fault to be detected). In principle, whenever hj = 1, the fault could be observed on output j . Let L = {j : hj = 1}. Suppose then n that j  L. If it happens that i=1 ci hij = X, the error at position  is masked for output j (that is, dj + rj = X, and no error is observed). On the other hand, if
n n n

of X-codes. In order to investigate X-codes from coding and design theoretic views, we introduce an equivalent definition. Consider two m-dimensional vectors s1 = (1) (1) (1) (2) (2) (2) (s1 , s2 , . . . , sm ) and s2 = (s1 , s2 , . . . , sm ), where (j ) si  F2 . The addition of s1 and s2 is bit-by-bit addition, denoted by s1  s2 ; that is,
(2) s1  s2 = (s1 + s1 , s2 + s2 , . . . , s(1) m + sm ). (1) (2) (1) (2)

The superimposed sum of s1 and s2 , denoted s1  s2 , is
(2) s1  s2 = (s1  s1 , s2  s2 , . . . , s(1) m  sm ), (1) (2) (1) (2) (j ) ( l) (j ) ( l)

where si  sk = 0 if si = sk = 0, otherwise 1. An m-dimensional vector s1 covers an m-dimensional vector s2 if s1  s2 = s1 . For a finite set S = {s1 , . . . , ss } of m-dimensional vectors, define S = s1  · · ·  ss and S = s1  · · ·  ss .

When S = {s1 } is a singleton, S = S = s1 . For S =  we define S = S = 0, the zero vector. Let d be a positive integer and x a nonnegative integer. An (m, n, d, x) X-code X = {s1 , s2 , . . . , sn } is a set of mdimensional vectors over F2 such that |X | = n and ( S1 )  ( S2 ) = S1 .

dj + rj =
i=1

bi hij +
i=1

ci hij =
i=1

(bi + ci )hij = 0

then no error is observed. This occurs when there are an even number of values of i for which hij = 1 and bi + ci = 1; because this holds when i =  by hypothesis, the error at position  is canceled for output j when the number of such errors is even. When an error is masked or canceled for every output j  L, it is not detected. Otherwise, it is detected by an output that is neither masked nor canceled. Treating X's as erasures and using traditional codes can increase the error detectability of an X-compactor [17]. Unfortunately, this involves postprocessing test responses and cannot be easily implemented [12]. Therefore, we focus on X-compaction in which an error is only detected by the simple comparison described here. There are numerous criteria in defining a "good" X-compact matrix. It should have a high compaction ratio and be able to detect any faulty circuit behavior anticipated in actual testing. Power requirements, compactor delay, and wireability dictate that the weight of each row in a matrix be small to meet practical limitations on fan-in and fan-out [11], [16]. The fundamental problem in X-tolerant response compaction is to design an X-compact matrix with large compaction ratio that detects faulty circuit behavior. To achieve this, X-codes (which represent X-compact matrices) were introduced [14]. In this section, we discuss basic properties

for any pair of mutually disjoint subsets S1 and S2 of X with |S1 | = x and 1  |S2 |  d. A vector si  X is a codeword. (i) (i) The weight of a codeword si is |{sj = 0 : sj  si }|. The ratio n/m is the compaction ratio of X . An X-code forming an orthonormal basis of the m-dimensional linear space over F2 is trivial. Roughly speaking, an X-code is a set of codewords such that for every positive integer d  d no superimposed sum of any x codewords covers the vector obtained by adding up any d codewords chosen from the rest of the n - x codewords. Now we present a method of designing an X-compact matrix from an X-code. Proposition 1: There exists an (m, n, d, x) X-code X if and only if there exists an n × m X-compact matrix H which detects any combination of d faults (1  d  d) in the presence of at most x unknown logic values. Proof: First we prove necessity. Assume that X is an (m, n, d, x) X-code. Write X = {s1 , s2 , . . . , sn }, where si = (i) (i) (i) (s1 , s2 , . . . , sm ) for 1  i  n. Define an n × m matrix (i) H = (hi,j ) as hi,j = sj . We show that H forms an Xcompact matrix that detects a fault if the test output b contains d error bits, 1  d  d, and up to x Xs. Let E = {k : bk + ck = 1}, the set of indices of error bits, have cardinality d . Let X = {k : ck = X}, the set of indices of unknown logic values, have cardinality x. Now comparing d and r , d + r =
k

bk · hk, +
k

ck · hk,

=
kE,X

(bk + ck ) · hk, 1 · hk, +
k E k X

=

X · hk, ,

(1)

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

3

with operations performed in X2 . Because the set of rows of H forms the set of codewords of X , no superimposed sum of x rows covers the vector obtained by an addition of any d rows. Hence there exists a column c such that 1 · hk,c = 1 and
k E k X

X · hk,c = 0.

(2)

Then (1) and (2) imply dc + rc = 1, that is, H detects a fault. Because (2) holds if and only if the right hand side of (1) equals one for l = c, sufficiency is straightforward. By virtue of this equivalence, we can employ various known results and techniques in coding theory to design an X-compactor with good error detection ability, X-tolerance, and a high compaction ratio. For the case when x = 0, an (m, n, d, 0) X-code forms an n × m X-compact matrix which is a parity-check matrix of a binary linear code of length n and minimum distance d. In fact, since the condition that x = 0 implies the absence of Xs, this special case is reduced to traditional space compaction. Because our focus is compaction in the presence of unknown logic values, we assume that x  1 henceforth unless otherwise stated. In the absence of Xs, see [18], [19]. By definition, an (m, n, d, x) X-code, d  2, is also an (m, n, d - 1, x) X-code. Also an (m, n, d, x) X-code forms an (m, n, d, x - 1) X-code. Moreover, an (m, n, d, x) X-code is an (m, n, d + 1, x - 1) X-code [14]. It can be difficult to design an X-compactor having both the necessary error detectability and the exact number of inputs needed. One trivial solution is to discard codewords from a larger X-code with sufficient error detection ability and Xtolerance. The following is another simple way to adjust the number of inputs. Proposition 2: If an (m, n, d, x) X-code and an (m , n , d , x ) X-code exist, there exists an (m + m , n + n , min{d, d }, min{x, x }) X-code. Proof: Let X = {s1 , . . . , sn } be an (m, n, d, x) Xcode and Y = {t1 , . . . , tn } an (m , n , d , x ) X-code. (i) (i) Extend each codeword si = (s1 , . . . , sm ) of X by appending m 0's so that extended vectors have the form (i) (i) s i = (s1 , . . . , sm , 0, . . . , 0). Similarly extend each codeword (j ) (j ) tj = (t1 , . . . , tm ) of Y by appending m 0s so that extended (j ) (j ) vectors have the form t j = (0, . . . , 0, t1 , . . . , tm ). The extended (m + m )-dimensional vectors form an (m + m , n + n , min{d, d }, min{x, x }) X-code. Proposition 2 says that given an (m, n, d, x) X-code, a codeword of weight less than or equal to x does not essentially contribute to the compaction ratio (see also [14]). In fact, (i) (i) if X contains such a codeword si = (s1 , . . . , sm ), there (i) exists at least one coordinate m such that sm = 1 and (j ) sm = 0 for any other codeword sj  X . Hence we can delete si and coordinate m from X while keeping d and x. By applying Proposition 2 and combining a trivial X-code and another X-code, we can obtain an X-code having the same number of codewords with compaction ratio no smaller. For this reason, when constructing an (m, n, d, x) X-code explicitly, we assume that every codeword has weight greater than x.

Let M (m, d, x) be the maximum number n of codewords for which there exists an (m, n, d, x) X-code. More codewords means a higher compaction ratio. Hence an (m, n, d, x) Xcode satisfying n = M (m, d, x) is optimal. Determining the exact value of M (m, d, x) seems difficult except for M (m, 1, 1). As pointed out in [14], a special case of M (m, d, x) has been extensively studied in the context of superimposed codes [20]. An (1, x)-superimposed code of size m × n is an m × n matrix S with entries in F2 such that no superimposed sum of any x columns of S covers any other column of S . Superimposed codes are also called cover-free families and disjunct matrices. By definition, a (1, x)-superimposed code of size m × n is equivalent to the transpose of an X-compact matrix obtained from an (m, n, 1, x) X-code. Hence known results on the maximum ratio n/m for superimposed codes immediately give information about M (m, 1, x). For completeness, we list useful results on M (m, 1, x). By Sperner's theorem, Theorem 2.1: (see [21], [22]) For m  2 an integer, M (m, 1, 1)  m . m/2

Indeed by taking all the m-dimensional vectors of weight m/2 as codewords, we attain the bound. The same argument is also found in [14]. The following is a simple upper bound on M (m, 1, x): Theorem 2.2: [22] For any x  2, log2 M (m, 1, x)  cm log2 x x2

for some constant c. Several different proofs of Theorem 2.2 are known. Bounds on the constant c are approximately two in [23], approximately four in [24], and approximately eight in [25]. The asymptotic behavior of the maximum possible number of codewords has been also investigated for superimposed codes. Define the ratio R(x) as log2 M (m, 1, x) . m m The best lower bound R(x)  R(x) can be found in [26] and the best upper bound R(x)  R(x) in [23]. The descriptive asymptotic form of the best bounds as x   is R(x) = lim R(x)  1 2 log2 x , and R(x)  x2 log2 e x2

where e is Napier's constant. For a detailed summary of the known lower and upper bounds, see [27]. Constructions with many codewords have been studied in [28], [29]. See also [30]­[33] and references therein. III. X-C OMPACTORS
WITH

S MALL FAN -O UT

In this section we consider an X-compactor having sufficient tolerance for errors and Xs, a high compaction ratio, and small fan-out. This section is divided into four parts. Subsection III-A deals with background and known results of the fanout problem in X-compactors. Then in Subsection III-B we investigate X-codes that tolerate up to two X's and have

4

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

the minimum fan-out. X-Codes with further error detection ability and X-tolerance are investigated in Subsection III-C. In Subsection III-D we give a brief overview of the performance of our X-codes given in this section and compare them with other codes. A. Background: Fan-Out in X-Codes X-compact reduces the number of bits in the compacted output while keeping error detection ability by propagating each single bit to many signal lines. In fact, each output of the X-compactor in [12] connects to about half of all inputs. However, larger fan-in increases power requirements, area, and delay [16]. When these disadvantages are concerns, fan-out of inputs of a compactor should be small to reduce fan-in values. In terms of X-codes, the required fan-out of input i in an X-compactor is the weight of codeword si of the X-code. Hence, in order to address the fan-out problem, it is desirable for a codeword to have small weight. However, as mentioned in Section I, an (m, n, d, x) X-code containing a codeword with weight at most x is not essential in the sense of the compaction ratio. Hence, throughout this section, we restrict ourselves to (m, n, d, x) X-codes in which every codeword has weight precisely x + 1, namely constant weight codes. When a compactor is required to tolerate only a single unknown logic value, fan-out is minimized when every codeword of an X-code has constant weight two. This extreme case was addressed in [16] by considering a simple graph. We briefly restate their theorems in terms of X-codes. A graph G is a pair (V, E ) such that V is a finite set and E is a set of pairs of distinct elements of V . An element of V is called a vertex, and an element of E is called an edge. The girth g of G is the minimal size |C | of a subset C  E such that each vertex appearing in C is contained in exactly two edges. The edge-vertex incidence matrix H of a graph G = (V, E ) is a |E| × |V | binary matrix H = (hi,j ) such that rows and columns are indexed by edges and vertices respectively and hi,j = 1 if the ith edge contains the j th vertex, otherwise 0. By considering the edge-vertex incidence matrix of a graph and Proposition 1, we obtain: Theorem 3.1: [16] There exists a graph G = (V, E ) of girth g if and only if there exists a (|V |, |E|, g - 2, 1) X-code of constant weight two. Theorem 3.2: [16] A set X of m-dimensional vectors is an (m, n, d - 1, 1) X-code of constant weight two if and only if it is an (m, n, d, 0) X-code of weight two. These two theorems say that in order to design an Xcompactor with high error detection ability, we only need to find a graph with large girth. The same argument is also found in [14]. For existence of such graphs and more details on Xcodes of constant weight two, see [16] and references therein. B. Two X's and Fan-Out Three Multiple X's can occur; here we present X-codes that are tolerant to two X's and have the maximum compaction ratio. To accept up to two unknown logic values, we need an X-code

of constant weight three. We employ a well-known class of combinatorial designs. A set system is an ordered pair (V, B ) such that V is a finite set of points, and B is a family of subsets (blocks) of V . A Steiner t-design S (t, k, v ) is a set system (V, B ), where V is a finite set of cardinality v and B is a family of k -subsets of V such that each t-subset of V is contained in exactly one block. Parameters v and k are the order and block size of a Steiner t-design. When t = 2 and k = 3, an S (2, 3, v ) is a Steiner triple system of order v , STS(v ). An STS(v ) exists if and only if v  1, 3 (mod 6) [34]. A triple packing of order v is a set system (V, B ) such that B is a family of triples of a finite set V of cardinality v and any pair of elements of V appear in B at most once. An STS(v ) is a triple packing of order v  1, 3 (mod 6) containing the maximum number of triples. The point-block incidence matrix of a set system (V, B ) is the binary |V | × |B| matrix H = (hi,j ) such that rows are indexed by points, columns are indexed by blocks, and hi,j = 1 if the ith point is contained in the j th block, otherwise 0. The block-point incidence matrix is its transpose. When d = 1, an (m, n, 1, 2) X-code of constant weight three is equivalent to a (1, 2)-superimposed code of size m × n of constant column weight three. It is well known that the pointblock incidence matrix of an S (t, k, v ) forms an (1, k/(t - k 1) - 1)-superimposed code of size v × v t / t . Hence, by using an STS(v ), we obtain for every v  1, 3 (mod 6) a (v, v (v - 1)/6, 1, 2) X-code. An upper bound on the number of codewords of (1, 2)-superimposed codes of constant weight k is available: Theorem 3.3: [35] Let nk (m) denote the maximum number of columns of a (1, 2)-superimposed code such that and every column is of length m and has constant weight k . Then, n2t-1 (m)  n2t (m + 1) 
m t 2t-1 t

with equality if and only if there exists a Steiner t-design S (t, 2t - 1, m). The following is an immediate consequence: Theorem 3.4: For any (m, n, 1, 2) X-code of constant -1) with equality if and only if there weight three, n  m(m 6 exists an STS(m). Hence for d = 1, x = 2, and fan-out three, an X-code from any STS(v ) has the maximum compaction ratio (v - 1)/6. One may ask for larger error detectability of an (m, n, 1, 2) X-code when one (or zero) unknown logic value is assumed. An (m, n, d, x) X-code is also an (m, n, d + 1, x - 1) X-code, and hence any (m, n, 1, 2) X-code from an STS(m) is also an (m, n, 2, 1) X-code. However, a careful choice of Steiner triple systems gives higher error detectability while maintaining the compaction ratio. A configuration C in a triple packing, (V, B ), is a subset C  B . The set of points appearing in at least one block of a configuration C is denoted by V (C ). Two configurations C and C  are isomorphic, denoted C  = C  , if there exists a bijection   : V (C )  V (C ) such that for each block B  C , the image (B ) is a block in C  . When |C| = i, a configuration C is an i-configuration. A configuration C is even if for every point

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

5

a appearing in C the number |{B : a  B  C}| of blocks containing a is even. Because every block in a triple packing has three points, no i-configuration for i odd is even. A triple packing is r-even-free if for every integer i satisfying 1  i  r it contains no even i-configurations. By definition every r-even-free triple packing, r  2, is also (r - 1)-even-free. For an even integer r, an r-even-free triple packing is also (r + 1)-even-free. Every triple packing is trivially 3-even-free. For v > 3 an STS(v ) may or may not be 4-even-free. Up to isomorphism, the only even 4-configuration is the Pasch configuration. It can be written on six points and four blocks: {{a, b, c}, {a, d, e}, {f, b, d}, {f, c, e}}. For the list of all the small configurations in a triple packing and more complete treatments, we refer the reader to [34] and [36]. Because a 4-even-free STS is 5-even-free, an STS is 5-evenfree if and only if it contains no Pasch configuration. Lemma 3.5: If there exists a 5-even-free STS(v ), there exists a (v, v (v - 1)/6, 3, 1) X-code of constant weight three. The code is a (v, v (v - 1)/6, 5, 0) X-code of constant weight three. Proof: Let (V, B ) be a 5-even-free STS(v ). For every Bi  B define a v -dimensional vector si such that each (i) coordinate sj  si is indexed by a distinct point j  V (i) and sj = 1 if j  Bi , otherwise 0. Then we obtain a (v, v (v - 1)/6, 1, 2) X-code S = {si : Bi  B} of constant weight three. We prove that S is a (v, v (v - 1)/6, 3, 1) X-code that is also a (v, v (v - 1)/6, 5, 0) X-code. By definition, for 1  i  5 no i-configuration C  B is even. Hence {si : Bi  C} = 0. This implies that S is a (v, v (v - 1)/6, 5, 0) X-code. On the other hand, since no pair of points appears twice, for any mutually distinct blocks Bi , Bj , Bk  B , si = sj and si  (sj  sk ) = si . It remains to show that no codeword in S covers addition of three others. Suppose to the contrary that there exist four distinct codewords si , sj , sk , and sl such that si  (sj  sk  sl ) = si . Because no pair of points appears twice and every block has exactly three points, the only possible case is that the 4configuration {Bi , Bj , Bk , Bl } forms a Pasch configuration, and hence it is even, a contradiction. Steiner triple systems avoiding Pasch configurations have been long studied as anti-Pasch STSs [34]. Theorem 3.6: [37] There exists a 5-even-free STS(v ) if and only if v  1, 3 (mod 6) and v  {7, 13}. By combining Theorem 3.6 and Lemma 3.5, we obtain: Theorem 3.7: For every v  1, 3 (mod 6) and v  {7, 13}, there exists a (v, v (v - 1)/6, 1, 2) X-code of constant weight three that is a (v, v (v - 1)/6, 3, 1) X-code and a (v, v (v - 1)/6, 5, 0) X-code. An X-compactor designed from these can detect any odd number of errors unless there is an unknown logic value. One may want to take advantage of the high compaction ratio of the optimal (m, n, 1, 2) X-codes arising from 4-even-free STSs

when there is only a small possibility that more than two Xs occur or multiple errors happen with multiple Xs. Our X-codes from 4-even-free STSs also have high performance in such situations: Theorem 3.8: The probability that a (v, v (v - 1)/6, 1, 2) Xcode from a 4-even-free STS(v ) fails to detect a single error 162(v -3)2 when there are exactly three Xs is (v+2)(v+3)( v -4)(v 2 -v -18) . Proof: Because there is only one error, an X-code fails to detect this error when all three points in the block that corresponds to the error are contained in at least one block corresponding to an X. The number of occurrences of each 4configuration in an STS(v ) is determined by v and the number of Pasch configurations (see [34], for example). A simple calculation proves the assertion. Theorem 3.9: The probability that a (v, v (v - 1)/6, 1, 2) X-code from a 4-even-free STS(v ) fails to detect errors when there are exactly two Xs and exactly two errors is 1296 (v +2)(v +3)(v -4)(v 2 -v -18) . Proof: A (v, v (v - 1)/6, 1, 2) X-code from a 4-evenfree STS(v ) fails to detect errors when there are exactly two Xs and exactly two errors only when corresponding four blocks form a 4-configuration isomorphic to {{a, b, c}, {d, e, f }, {a, e, g }, {c, f, g }} where the first two blocks represent Xs and the other two blocks correspond to errors. The number of occurrences of the 4-configuration in v -3) a 4-even-free STS(v ) is v(v-1)( , and the total number of 4 occurrences of all 4-configurations is
v (v -1)(v -3) 4 4 2
v(v-1) 6 v(v-1) 6

4

[34]. Divide

to obtain the probability that the by 4 X-code fails to detect the two errors. Hence when a 4-even-free STS of sufficiently large order is used, the probability that the corresponding X-code fails to detect errors when the sum of the numbers of errors and Xs is at most four is close to zero. A more complicated counting argument is necessary to calculate the performance of X-codes from STSs when the sum of the numbers of errors and Xs is greater than four. For more complete treatments and current research results on counting configurations in Steiner triple systems, we refer the reader to [36] and references therein. Useful explicit constructions for 5-even-free STS(v ) can be found in [34], [37]­[41]. The cyclic 5-sparse Steiner triple systems in [42] provide examples of 5-even-free STS(v ) for v  97, because cyclic 5-sparse systems are all anti-Pasch. Further r-even-freeness improves the error detectability of the resulting X-code: Theorem 3.10: For r  4, if there exists an r-even-free triple packing (V, B ), there exists a (|V |, |B|, 1, 2) X-code of constant weight three that is also a (|V |, |B|, 3, 1) X-code and a (|V |, |B|, r, 0) X-code. Proof: Let (V, B ) be an r-even-free triple packing of order v . For every Bi  B define a v -dimensional vector si such (i) that each coordinate sj  si is indexed by a distinct point (i) j  V and sj = 1 if j  Bi , otherwise 0. Then we obtain a (|V |, |B|, 1, 2) X-code S = {si : Bi  B} of constant weight three. It suffices to prove that S forms a (|V |, |B|, r, 0) X-code. Suppose to the contrary that S is not a (|V |, |B|, r, 0) X-code. Then for some r  r there exists a set of r codewords si ,

6

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

sj . . . , sk such that si  sj · · ·  sk = 0. However, the set of the corresponding blocks Bi , Bj ,. . . ,Bk forms an even r -configuration, a contradiction. One may want an r-even-free STS with large r to obtain higher error detection ability while keeping the maximum compaction ratio. Although it is known that every Steiner triple system has a configuration with seven or fewer blocks so that every element of the configuration belongs to at least two [36], it may happen that none of these are even. Nevertheless, the following gives an upper bound of even-freeness of Steiner triple systems. Theorem 3.11: For v > 3 there exists no 8-even-free STS(v ). Proof: Suppose to the contrary that there exists an STS(v ), S , that is 8-even free. Consider a 4-configuration C isomorphic to {{a, b, e}, {c, d, e}, {a, c, f }, {b, d, g}}; the points f and g are each contained in exactly one block. For any anti-Pasch STS(v ) the number of occurrences of configurations isomorphic to C is v (v - 1)(v - 3)/4 [43] (see also [44]). Because v  7, we have v (v - 1)(v - 3)/4 > v 2 . Hence there is a pair of configurations A and B such that A  = C and they share the two points contained in = B  exactly one block. In other words, there exists a pair A and B having the form {{a, b, e}, {c, d, e}, {a, c, f }, {b, d, g}} and {{a , b , e }, {c , d , e }, {a , c , f }, {b , d , g }} respectively. If there is no common block between A and B , then the merged configuration A  B forms an even configuration consisting of eight blocks, a contradiction. Otherwise, there is at least one block contained in both A and B . Removing blocks shared between A and B from their union, we obtain an even configuration on four or six blocks, a contradiction. By combining Theorems 3.4, 3.10, and 3.11, we have: Theorem 3.12: There exists no (m, n, 1, 2) X-code that achieves the maximum compaction ratio (m - 1)/6 and is also an (m, n, 3, 1) X-code and an (m, n, 8, 0) X-code. An STS is 7-even-free if and only if it is 6-evenfree. Up to isomorphism, there are two kinds of even 6configurations which may appear in an STS. One is called the grid and the other is the double triangle. Both 6configurations are described by nine points and six blocks: {{a, b, c}, {d, e, f }, {g, h, i}, {a, d, g }, {b, e, h}, {c, f, i}} and {{a, b, c}, {a, d, e}, {c, f, e}, {b, g, h}, {d, h, i}, {f, g, i}} respectively. By definition, an STS is 6-even-free if it simultaneously avoids Pasches, grids, and double triangles. We do not know whether there exists a 6-even-free STS(v ) for any v > 3. However, a moderately large number of triples can be included while keeping 6-even-freeness: Theorem 3.13: There exists a constant c > 0 such that for sufficiently large v there exists a 6-even-free triple packing of order v with cv 1.8 triples. Proof: Let C  be a set of representatives of all of the nonisomorphic even configurations on six or fewer triples and let C  be a configuration consisting of pair of distinct triples sharing a pair of elements. Let C = C   C  . Pick uniformly 6 at random triples from V with probability p = c v - 5 inde1 10   pendently, where c satisfies 0 < c < ( 41·79·83 ) 5 . Let bC be

a random variable counting the configurations isomorphic to a member of C in the resulting set of triples. Define E (bC ) as its expected value. Then v 4
9 3 4 3

E (bC )  =

6

2 4 c6 v 1.8 + f (v ), 9!

p2 +

v 6

6 3

p4 +

v 9

9 3

6

p6

where f (v ) = O(v 1.6 ). By Markov's Inequality, P (bC  2E (bC ))  Hence, P bC  2
9 3

1 . 2

6

c6 v 1.8 + 2f (v ) 9!



1 . 2

Let t be a random variable counting the triples and E (t) its expected value. Then E (t) = p v 3 = c 1.8 v - g (v ), 6

where g (v ) = O(v 0.8 ). Because t is a binomial random variable, by Chernoff's inequality, for sufficiently large v P t< E (t) 2 < e-
E (t) 8

<

1 . 2

Hence, if v is sufficiently large, then with positive probabil(t) ity we have a set B of triples with the property that |B| > E2 and the number of configurations in B isomorphic to a member of C is at most 2
9 3

6

c6 v 1.8 + 2f (v ). 9!

Let ex(v, r) be the maximum cardinality |B| such that there exists an r-even-free triple packing. By deleting a triple from each configuration isomorphic to a member of C , we obtain ex(v, 6)  E (t) -2 2
9 3

6

c6 v 1.8 + h(v ), 9!

where h(v ) = O(v 1.6 ). Then for some positive constant c and sufficiently large v , it holds that ex(v, 6)  cv 1.8 . Hence we have: Theorem 3.14: There exists a constant c > 0 such that for sufficiently large v there exists a (v, cv 1.8 , 1, 2) X-code that is also a (v, cv 1.8 , 3, 1) X-code and a (v, cv 1.8 , 6, 0) X-code. An STS(v ) has approximately v 2 /6 triples. The same technique can be used to obtain a lower bound on ex(v, r) for 12 r  8. In fact, ex(v, 8) is at least O(v 7 ), and hence for sufficiently large v there exists a constant c > 0 such that 12 12 there exists a (v, cv 7 , 1, 2) X-code that is also a (v, cv 7 , 3, 1) 12 X-code and a (v, cv 7 , 8, 0) X-code.

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

7

C. Higher X-Tolerance with the Minimum Fan-Out In general, the probability that a defective digital circuit produces an error at a specific signal output line is quite small. In fact, several errors are unlikely happen simultaneously [11], [45]. Also, multiple Xs with errors are rare [12]. Therefore, Xcodes given in Theorems 3.7 and 3.13 are particularly useful for relatively simple scan-based testing such as built-in selftest (BIST) where the tester is only required to detect defective chips. Nonetheless, more sophisticated X-codes are also useful to improve test quality and/or to identify or narrow down the error sources by taking advantage of more detailed information about when incorrect responses are produced [12]. Hence, for use in higher quality testing and error diagnosis support, it is of theoretical and practical interest to consider (m, n, d, x) X-codes of constant weight x + 1, where x  3 or d  6. For (m, n, 1, 2) X-codes of constant weight three, we employed Theorem 3.3 to obtain an upper bound on the number of codewords. The following theorem gives a generalized upper bound: Theorem 3.15: [46] Let n(x, m, k ) denote the maximum number of columns of a (1, x)-superimposed code such that every column is of length m and has constant weight k . Then, for every x, t and i = 0, 1 or i  x/2t2 , n(x, m, x(t - 1) + 1 + i)  m-i k-i / t t

for all sufficiently large m, with equality if and only if there exists a Steiner t-design S (t, x(t - 1) + 1, m - i). By putting t = 2 and i = 0, we obtain: Corollary 3.16: For an (m, n, 1, x) X-code of constant weight x + 1, m x+1 n / 2 2 for all sufficiently large m, with equality if and only if there is an S (2, x + 1, m). Because the set of columns of the block-point incidence matrix of any S (2, x + 1, m) forms an (m, m(m - 1)/x(x + 1), 1, x) X-code of constant weight x + 1, the existence of Steiner 2-designs is our next interest. For k  {4, 5}, necessary and sufficient conditions for existence of an S (2, k, v ) are known: Theorem 3.17: [47] There exists an S (2, 4, v ) if and only if v  1, 4 (mod 12). Theorem 3.18: [48] There exists an S (2, 5, v ) if and only if v  1, 5 (mod 20). For k  6, the necessary and sufficient conditions on v for existence of an S (2, k, v ) are not known in general; the existence of a Steiner 2-design is solved only in an asymptotic sense [49], although for `small' values of k substantial results are known. For a comprehensive table of known Steiner 2designs, see [50]. As with X-codes from Steiner triple systems, the error detectability can be improved by considering avoidance of even configurations. An S (2, k, v ), (V, B ), is r-even-free if for 1  i  r it contains no subset C  B such that |C| = i and each point appearing in C is contained in exactly an even number

of blocks in C . A generalized Pasch configuration in an S (2, k, v ), (V, B ), is a subset C  B such that |C| = k + 1 and each point appearing in C is contained exactly two blocks of C . As with triple systems, an S (2, k, v ) is (k + 1)-even-free if and only if it contains no generalized Pasch configurations. Theorem 3.19: If an r-even-free S (2, k, v ) for r  k + 1 exists, there exists a (v, v (v - 1)/k (k - 1), 1, k - 1) X-code of constant weight k that is also a (v, v (v - 1)/k (k - 1), k, 1) X-code and a (v, v (v - 1)/k (k - 1), r, 0) X-code. Proof: Let (V, B ) be an r-even-free S (2, k, v ). For every Bi  B define a v -dimensional vector si such that each (i) coordinate sj  si is indexed by a distinct point j  V (i) and sj = 1 if j  Bi , otherwise 0. Then we obtain a (v, v (v - 1)/k (k - 1), 1, k - 1) X-code S = {si : Bi  B} of constant weight k . By definition of an r-even-free S (2, k, v ), it is straightforward to see that S is also a (v, v (v - 1)/k (k - 1), r, 0) X-code. It suffices to prove that S can also be used as a (v, v (v - 1)/k (k - 1), k, 1) X-code. Assume that this is not the case. Then, by following the argument in the proof of Lemma 3.5, B contains a generalized Pasch configuration, a contradiction. Existence of an r-even-free design has been investigated in the study of erasure-resilient codes for redundant array of independent disks (RAID) [51]. In fact, infinitely many r-evenfree S (2, k, v )s can be obtained from affine spaces over Fq [52]. Theorem 3.20: [52] For any odd prime power q and positive integer n  2 the points and lines of AG(n, q ) form a (2q - 1)even-free S (2, q, q n ). By combining Theorems 3.19 and 3.20, we obtain: Theorem 3.21: For any odd prime power q and positive integer n  2, there exists a (q n , q n-1 (q n - 1)/(q - 1), 1, q - 1) X-code of constant weight q that is also an (q n , q n-1 (q n - 1)/(q - 1), q, 1) X-code and a (q n , q n-1 (q n - 1)/(q - 1), 2q - 1, 0) X-code. D. Characteristics of X-Codes from Combinatorial Designs We have given tight upper bounds of compaction ratio for (m, n, 1, x) X-codes with the minimum fan-out and presented explicit construction methods for X-codes that attain the bounds. As far as the authors are aware, these are the first mathematical bounds and construction techniques for this type of optimal X-code with constant weight greater than two. Optimal X-codes given in Theorems 3.7 and 3.21 in particular have higher error detection ability when the number of Xs is smaller than x. The known construction technique using hypergraphs, briefly mentioned in [16], can not guarantee the same error detection ability. To illustrate the usefulness of our X-codes, here we compare the error detection ability of an example X-code that can be generated using Theorem 3.7 with characteristics of Xcodes proposed in [11]. The probability that the example (50, 500, 1, 1) X-code in Table 5 in [11] fails to detect a single error when there are exactly two Xs is around 4.2 × 10-6 . The fan-out of this code is 11. Our X-code from Theorem 3.7, which has the same compaction ratio, has parameters (61, 610, 1, 2). The probability that this X-code fails to detect

8

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

a single error in the same situation is exactly 0. Its fan-out is 3, which is significantly smaller. While the multiple error detection ability of the (50, 500, 1, 1) X-code is not specified in [11], our code can always detect up to three errors when there is only one X, and up to five errors when there is no X. By Theorem 3.9 the probability that our (61, 610, 1, 2) X-code fails to detect errors when there are exactly two Xs and two errors is 1.5 × 10-6 . Therefore, our X-code is ideal when the fan-out problem is critical and/or fault-free simulation rarely produces three or more Xs in an expected response. Very large optimal X-codes with very high error detecting ability and compaction ratio can be easily constructed by the same method. For example, Theorem 3.7 and known results on anti-Pasch STSs immediately give a (601, 60100, 1, 2) Xcode with fan-out 3 and compaction ratio 100. This code is also a (601, 60100, 3, 1) X-code and a (601, 60100, 5, 0) Xcode. Moreover, the probability that it fails to detect errors when there are exactly two Xs and two errors (or exactly three Xs and a single error) is around 1.6 × 10-11 (or 7.3-7 respectively). As far as the authors know, there have been no X-codes available that guarantee as high error detection ability and have very small fan-out. As Theorems 3.7, 3.8, and 3.9 indicate, larger X-codes designed with this method have an even higher compaction ratio and better error detection rate. Because discarding codewords does not affect error detection ability, one may use part of a large X-code to achieve very high test quality when compaction ratio can be compromised to an extent. IV. X-C ODES
OF

binary matrix H  . Taking each row of H  as a codeword, we obtain a set X of mN -dimensional vectors. It suffices to show that for any two arbitrary subsets D, X  X satisfying |D| = d  d, |X | = x  x, and D  X = , it holds that ( X)  ( D) = X. (3)

A RBITRARY W EIGHT

The restriction to low-weight codewords severely limits the compaction ratio of an X-code. Hence, when fan-in and fanout are not of concern, it is desirable to use X-codes with arbitrary weight. In this section we study the compaction ratio and construction methods of such general X-codes. For d = x = 2, a (log2 n(log2 n + 1), n, 2, 2) X-code was constructed for any integer n  2 [14]. Theorem 4.1: [14] For any optimal (m, n, 2, 2) X-code, m  log2 n(log2 n + 1). They also gave an explicit construction method of a (3log3 n, n, 1, 3) X-code. In order to give a more general construction, we employ design theoretic techniques for arrays. Let n  w  2. A perfect hash family, PHF(N ; u, n, w), is a set F of N functions f : Y  X where |Y | = u and |X | = n, such that, for any C  Y with |C | = w, there exists at least one function f  F such that f |C is one-to-one. A PHF(N ; u, n, w) can be described by a u × N matrix with entries from a set of n symbols such that for any w rows there exists at least one column in which each element is distinct. Theorem 4.2: If an (m, n, d, x) X-code and a PHF(N ; u, n, max{d, x} + 1) exist, there exists an (mN, u, d, x) X-code. Proof: Let H be a u × N n-ary matrix representing a PHF(N ; u, n, max{d, x} + 1). Assign each codeword of an (m, n, d, x) X-code to a distinct symbol of the PHF and replace each entry of H by the m-dimensional row vector representing the assigned codeword. Then we obtain a u × mN

By considering a one-to-one function in the PHF, for any max{d, x} + 1 codewords of X at least one set of m coordinates forms max{d, x} + 1 distinct codewords of the original (m, n, d, x) X-code. Hence, for any choice of D and X there exists a subset Y  X of cardinality |Y | = max{0, d + x - (max{d, x} + 1)} such that at least one set of m coordinates in D  (X \ Y ) forms distinct codewords of the original (m, n, d, x) X-code. Because |Y |  d - 1 < |D|, (3) holds for any D and X . Hence, the resulting set X forms an (mN, u, d, x) X-code. Since their introduction in [53], much progress has been made on existence and construction techniques for perfect hash families (see [54]­[58] for recent results). A concise list of known results on perfect hash families is available in [50]. We can use perfect hash families from algebraic curves over finite fields: Theorem 4.3: [59] For positive integers n  w, there exists an explicit construction for an infinite family of PHF(N ; u, n, w) such that N is O(log u). Indeed when n is fixed, a perfect hash family with O(log u) rows can be determined in polynomial time by a greedy method [60]. By combining Theorems 4.2 and 4.3, we can construct infinitely many (m, n, d, x) X-codes where m is O(log n). Theorem 4.4: For any positive integer d and nonnegative integer x, there exists an explicit construction for an infinite family of (m, n, d, x) X-codes, where m is O(log n). The following is a combinatorial recursion for X-codes. Theorem 4.5: If an (m, n, d, x) X-code and an (, n, d 2 , x) X-code exist, there exists an ( + m, 2n, d, x) X-code. Proof: Let X = {s1 , . . . , sn } be an (m, n, d, x) X-code and Y = {t1 , . . . , tn } an (, n, d 2 , x) X-code. Extend each (i) (i) codeword si = (s1 , . . . , sm ) of X by appending  0's so that (i) (i) extended vectors have the form s i = (s1 , . . . , sm , 0, . . . , 0). (i) (i) Extend each codeword ti = (t1 , . . . , tl ) of Y by combining si so that extended vectors have the form t i = (i) (i) (i) (i)   (s1 , . . . , sm , t1 , . . . , tl ). Define A = {s1 , . . . , sn }, B =  {t 1 , . . . , tn }, and C = A  B . We prove that C is an ( + m, 2n, d, x) X-code. Take two subsets D, X  C satisfying |D| = d  d, |X | = x  x, and D X = . As in the proof of Theorem 4.2, it suffices to show that for any choice of D and X the vector obtained by adding all the codewords in D is not covered by the superimposed sum of X , that is, (3) holds. Define a sur(i) (i) (i) (i) jection f of C to X as f : (c1 , . . . , c+m )  (c1 , . . . , cm ). Mapping all codewords of C under f generates two copies of X ; one is from A and the other is from B . Define a (i) (i) surjection g of C to Y  {0} as g : (c1 , . . . , c+m )  (i) (cm + 1(i) , . . . , c+m ). By definition, {g (c) : c  B } = Y and for any c  A the image g (c) is an -dimensional zero

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

9

vector. Let a = |D  A| and b = |D  B |. Because Y is an d (, n, d 2 , x) X-code, if b  2 , g( X )  g( D) = g ( X ). (4)
d 2

If E (AX ) < 1, there exists an (m, n, d, x) X-code. Taking logarithms, m> Hence, if m  2x+1 (d + x) log n > -(d + x) log n , log (1 - 2-x-1 ) -(d + x) log n . log (1 - 2-x-1 )

Hence, we only need to consider the case when b > Suppose to the contrary that (3) does not hold. Then, f( Let a = |{c  X : f (c) = f (d), d  D  B }| and a = |{c  D  A : f (c) = f (d), d  D  B }|. X)  f( D) = f ( X ).

.

(5)

Because {f (c) : c  A} = {f (c) : c  B} = X and (5) holds, b = a + a . As a + b = d and b > d 2 , b  a + a d  + a . 2 (6)

there exists an (m, n, d, x) X-code. Hence, for any optimal (m, n, d, x) X-code with n  max{2d, d + x}, m is at most O(log n). For example, by putting d = x = 2 we know that there exists an (m, n, d, x) X-code if m  32 log n. This significantly improves the upper bound in Theorem 4.1 proved in [14]. V. C ONCLUSIONS By formulating X-tolerant space compaction of test responses combinatorially, an equivalent, alternative definition of X-codes has been introduced. This combinatorial approach gives general design methods for X-codes and bounds on the compaction ratio. Using this model with restricted fanout leads to well-studied objects, the Steiner 2-designs. These provide constructions for X-codes having sufficient error detectability, X-tolerance, maximum compaction ratio, and minimum fan-out. Constant weight X-codes with high error detectability profit from a deep connection with configurations, particularly the Pasch configuration. The combinatorial formulation of X-tolerant compaction can also be applied in conjunction with another compaction technique (such as time compaction). If a tester wants an X-compactor with additional properties, the necessary structure of the compactor may be expressed in design theoretic terms. Our formulation can also be useful for the study of higher error detectability and error diagnosis support employing the appropriate assistance from an Automatic Test Equipment (ATE) [12]. For example, the compaction technique called iCompact can be understood in terms of the model in Section II [17]. The essential idea underlying Theorem 4.6 is the stochastic coding technique for X-tolerant signature analysis [61]. We used a naive value 1/2 as the probability p in the proof of Theorem 4.6. To obtain a better constant coefficient, p should be chosen so that it minimizes the expected value E (AX ), that is, it should minimize m 
d i=1

On the other hand, |X  B|  x - a . Because Y is also an   (, n, d 2 + a , x - a ) X-code, (4) holds, a contradiction. Next, we present a simple nonconstructive existence result for (m, n, d, x) X-codes. Theorem 4.6: Let d, x be a positive integers. For n  max{2d, d + x}, if m  2x+1 (d + x) log n, there exists an (m, n, d, x) X-code. Proof: Let X = {s1 , s2 , . . . , sn } be a set of n m(i) (i) (i) dimensional vectors si = (s1 , s2 , . . . , sm , ) in which each (i) entry sj is defined to be 1 with probability p = 1/2. Let X be a set of x vectors of X and Di a set of i vectors in X \ X . Define A(Di , X ) = 0 if ( X )  ( 1 otherwise, Di ) = X,

and let E (A(Di , X )) be its expected value. Then  i -i  2  j m

E (A(Di , X )) = = Let AX =
X X |X |=x

 -x 1 - 2
d

1j i j

odd

(1 - 2-x-1 )m .

A(Di , X )
i=1
Di Di X =

and E (AX ) its expected value. Then
d

E (AX )

=
X X |X |=x

E (A(Di , X ))
i=1
Di Di X =

d

=
i=1

n x

n-x (1 - 2-x-1 )m i

< nd+x (1 - 2-x-1 )m .

While this optimization does not affect the logarithmic order in Theorem 4.6, it may help a tester determine the target compaction ratio and estimate the error cancellation and masking rate of an X-tolerant Multiple Input Signature Register (XMISR) based on stochastic coding [61]. In this paper we focused on space compaction. Nevertheless, time compaction is of great importance as well. We expect the combinatorial formulation developed here to provide a useful framework for exploring time compaction as well.

n-x  1 - i

1j i j

odd

i j  p (1 - p)i-j +x  . j

10

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

ACKNOWLEDGMENT A substantial part of the research was done while the first author was visiting the Department of Computer Science and Engineering of Arizona State University. He thanks the department for its hospitality. The authors thank an anonymous referee and the editor for helpful comments and valuable suggestions. R EFERENCES
[1] E. J. McCluskey, D. Burek, B. Koenemann, S. Mitra, J. H. Patel, J. Rajski, and J. A. Waicukauski, "Test compression roundtable," IEEE Des. Test. Comput., vol. 20, pp. 76­87, Mar./Apr. 2003. [2] A. Lempel and M. Cohn, "Design of universal test sequences for VLSI," IEEE Trans. Inf. Theory, vol. 31, pp. 10­17, Jan. 1985. [3] G. Seroussi and N. H. Bshouty, "Vector sets for exhaustive testing of logic circuits," IEEE Trans. Inf. Theory, vol. 34, pp. 513­522, May 1988. [4] H. Hollmann, "Design of test sequences for VLSI self-testing using LFSR," IEEE Trans. Inf. Theory, vol. 36, pp. 386­392, Mar. 1990. [5] G. D. Cohen and G. Zemor, "Intersecting codes and independent families," IEEE Trans. Inf. Theory, vol. 40, pp. 1872­1881, Nov. 1994. [6] N. Benowitz, D. F. Calhoun, G. E. Alderson, J. E. Bauer, and C. T. Joeckel, "An advanced fault isolation system for digital logic," IEEE Trans. Comput., vol. C-24, pp. 489­497, May 1975. [7] E. J. McCluskey, Logic Design Principles with Emphasis on Testable Semi-Custom Circuits. Englewood Cliffs, NJ: Prentice-Hall, 1986. [8] N. R. Saxena and E. J. McCluskey, "Parallel signature analysis design with bounds on aliasing," IEEE Trans. Comput., vol. 46, pp. 425­438, Apr. 1997. [9] C. Barnhart, V. Brunkhorst, F. Distler, O. Farnsworth, B. Keller, and B. Koenemann, "OPMISR: The foundation for compressed ATPG vectors," in Proc. Int. Test Conf., 2001, pp. 748­757. [10] C. Barnhart, V. Brunkhorst, F. Distler, O. Farnsworth, A. Ferko, B. Keller, D. Scott, B. Koenemann, and T. Onodera, "Extending OPMISR beyond 10x scan test efficiency," IEEE Design Test Comput., vol. 19, pp. 65­73, Sep. 2002. [11] S. Mitra, S. S. Lumetta, M. Mitzenmacher, and N. Patil, "X-tolerant test response compaction," IEEE Des. Test. Comput., vol. 22, pp. 566­574, Nov. 2005. [12] S. Mitra and K. S. Kim, "X-compact: An efficient response compaction technique," IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., vol. 23, pp. 421­432, Mar. 2004. [13] S. Mitra, S. Kallepalli, and K. S. Kim, "Analysis of X-compact for industrial designs," Intel Corp., 2003. [14] S. S. Lumetta and S. Mitra, "X-codes: Theory and applications of unknowable inputs," Center for Reliable and High-Performance Computing, Univ. of Illinois at Urbana Champaign, Tech. Rep. CRHC-03-08 (also UILU-ENG-03-2217), Aug. 2003. [15] ----, "X-codes: Error control with unknowable inputs," in Proc. IEEE Intl. Symp. Information Theory, Yokohama, Japan, June 2003, p. 102. [16] P. Wohl and L. Huisman, "Analysis and design of optimal combinational compactors," in Proc. 21st IEEE VLSI Test Symp., April/May 2003, pp. 101­106. [17] J. H. Patel, S. S. Lumetta, and S. M. Reddy, "Application of SalujaKarpovsky compactors to test responses with many unknowns," in Proc. 21st IEEE VLSI Test Symp., 2003, pp. 107­112. [18] T. R. N. Rao and E. Fujiwara, Error-Control Coding for Computer Systems. Englewood Cliffs, NJ: Prentice-Hall, 1989. [19] K. K. Saluja and M. Karpovsky, "Testing computer hardware through data compression in space and time," in Proc. Int. Test Conf., 1983, pp. 83­93. [20] W. H. Kautz and R. R. Singleton, "Nonrandom binary superimposed codes," IEEE Trans. Inf. Theory, vol. 10, pp. 363­377, Jul. 1964. [21] E. Sperner, "Ein satz u ¨ ber Untermengen einer endlichen Menge," Math. Z., vol. 27, pp. 544­548, 1928. [22] D. R. Stinson and R. Wei, "Some new upper bounds for cover-free families," J. Combin. Theory Ser. A, vol. 90, pp. 224­234, 2000. [23] A. G. D'yachkov and V. V. Rykov, "Bounds on the length of disjunctive codes," Probl. Contr. Inform. Theory, vol. 11, pp. 7­33, 1982, in Russian. [24] Z. F¨ uredi, "On r -cover-free families," J. Combin. Theory, Ser. A, vol. 73, pp. 172­173, 1996. [25] M. Ruszink´ o, "On the upper bound of the size of the r -cover-free families," J. Combin. Theory, Ser. A, vol. 66, pp. 302­310, 1994.

[26] A. G. D'yachkov, V. V. Rykov, and A. M. Rashad, "Superimposed distance codes," Probl. Contr. Inform. Theory, vol. 18, pp. 237­250, 1989. [27] D. Z. Du and F. K. Hwang, Combinatorial Group Testing and Its Applications, 2nd ed. Singapore: World Scientific, 2000. [28] H. L. Fu and F. K. Hwang, "A novel use of t-packings to construct d-disjunct matrices," Discrete Appl. Math., vol. 154, pp. 1759­1762, 2006. [29] A. G. D'yachkov, A. J. Macula, and V. V. Rykov, "New constructions of superimposed codes," IEEE Trans. Inf. Theory, vol. 46, pp. 284­290, Jan. 2000. [30] A. J. Macula, "A simple construction of d-disjunct matrices with certain constant weights," Discrete Math., vol. 162, pp. 311­312, 1996. [31] ----, "Error-correcting nonadaptive group testing with de -disjunct matrices," Discrete Appl. Math., vol. 80, pp. 217­222, 1997. [32] H. G. Yeh, "d-Disjunct matrices: bounds and Lov´ asz Local Lemma," Discrete Math., vol. 253, pp. 97­107, 2002. [33] A. De Bonis and U. Vaccaro, "Constructions of generalized superimposed codes with applications to group testing and conflict resolution in multiple access channels," Theor. Comput. Sci., vol. 306, pp. 223­243, 2003. [34] C. J. Colbourn and A. Rosa, Triple Systems. Oxford: Oxford Univ. Press, 1999. [35] P. Erd os, P. Frankl, and Z. F¨ uredi, "Families of finite sets in which no set is covered by the union of two others," J. Combin. Theory, Ser. A, vol. 33, pp. 158­166, 1982. [36] C. J. Colbourn and Y. Fujiwara, "Small stopping sets in Steiner triple systems," Cryptography and Communications, vol. 1, no. 1, pp. 31­46, 2009. [37] M. J. Grannell, T. S. Griggs, and C. A. Whitehead, "The resolution of the anti-Pasch conjecture," J. Combin. Des., vol. 8, pp. 300­309, 2000. [38] A. C. H. Ling, C. J. Colbourn, M. J. Grannell, and T. S. Griggs, "Construction techniques for anti-Pasch Steiner triple systems," J. Lond. Math. Soc. (2), vol. 61, pp. 641­657, 2000. [39] D. R. Stinson and Y. J. Wei, "Some results on quadrilaterals in Steiner triple systems," Discrete Math., vol. 105, pp. 207­219, 1992. [40] M. J. Grannell, T. S. Griggs, and J. S. Phelan, "A new look at an old construction for Steiner triple systems," Ars Combinat., vol. 25A, pp. 55­60, 1988. [41] A. E. Brouwer, "Steiner triple systems without forbidden subconfigurations," Mathematisch Centrum Amsterdam, ZW 104/77, 1977.  an [42] C. J. Colbourn, E. Mendelsohn, A. Rosa, and J. Sir´  , "Anti-Mitre Steiner triple systems," Graphs Combin., vol. 10, pp. 215­224, 1994. [43] M. J. Grannell, T. S. Griggs, and E. Mendelsohn, "A small basis for fourline configurations in Steiner triple systems," J. Combin. Des., vol. 3, pp. 51­59, 1995. [44] C. J. Colbourn, "The configuration polytope of -line configurations in Steiner triple systems," Mathematica Slovaca, vol. 59, no. 1, pp. 77­108, 2009. [45] P. Wohl, J. A. Waicukauski, and T. W. Williams, "Design of compactors for signature-analyzers in built-in-self-test," in Proc. Int. Test Conf., 2001, pp. 54­63. [46] P. Erd os, P. Frankl, and Z. F¨ uredi, "Families of finite sets in which no set is covered by the union of r others," Israel J. Math., vol. 51, pp. 75­89, 1985. [47] H. Hanani, "The existence and construction of balanced imcomplete block designs," Ann. Math. Statist., vol. 32, pp. 361­386, 1961. [48] ----, "On balanced incomplete block designs with blocks having five elements," J. Combin. Theory Ser. A, vol. 12, pp. 184­201, 1972. [49] R. M. Wilson, "An existence theory for pairwise balanced designs. III. Proof of the existence conjectures," J. Combin. Theory Ser. A, vol. 18, pp. 71­79, 1975. [50] C. J. Colbourn and J. H. Dinitz, Eds., Handbook of Combinatorial Designs. Boca Raton, FL: Chapman & Hall/CRC, 2007. [51] Y. M. Chee, C. J. Colbourn, and A. C. H. Ling, "Asymptotically optimal erasure-resilient codes for large disk arrays," Discrete Appl. Math., vol. 102, pp. 3­36, 2000. [52] M. M¨ uller and M. Jimbo, "Erasure-resilient codes from affine spaces," Discrete Appl. Math., vol. 143, pp. 292­297, 2004. [53] K. Mehlhorn, Data Structures and Algorithms 1. Berlin, Germany: Springer, 1984. [54] D. Tonien and R. Safavi-Naini, "Recursive constructions of secure codes and hash families using difference function families," J. Combin. Theory Ser. A, vol. 113, pp. 664­674, 2006. [55] Tran van Trung and S. S. Martirosyan, "New constructions for IPP codes," Des. Codes Cryptgr., vol. 32, pp. 227­239, 2005.

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

11

[56] D. Deng, D. R. Stinson, and R. Wei, "The Lov´ asz local lemma and its applications to some combinatorial arrays," Des. Codes Cryptgr., vol. 32, pp. 121­134, 2004. [57] R. A. Walker II and C. J. Colbourn, "Perfect hash families: Construction and existence," Journal of Mathematical Cryptology, vol. 1, pp. 125­ 150, 2007. [58] S. S. Martirosyan and Tran van Trung, "Explicit constructions for perfect hash families," Des. Codes Cryptogr., vol. 46, no. 1, pp. 97­112, 2008. [59] H. Wang and C. Xing, "Explicit constructions of perfect hash families from algebraic curves over finite fields," J. Combin. Theory Ser. A, vol. 93, pp. 112­124, 2001. [60] C. J. Colbourn, "Constructing perfect hash families using a greedy algorithm," in Coding and Cryptology, Y. Li, S. Zhang, S. Ling, H. Wang, C. Xing, and H. Niederreiter, Eds. Singapore: World Scientific, 2008. [61] S. Mitra, S. S. Lumetta, and M. Mitzenmacher, "X-tolerant signature analysis," in Proc. Int. Test Conf., 2004, pp. 432­441.

Necessary and sufficient conditions are established for an integer vector to be the f-vector of some pure simplicial complex of rank three, and also for an integer vector to be thef-vector of some pure simplicial multicomplex of rank three. For specified numbers of sets of cardinality one and cardinality two, an upper bound on the number of sets of cardinality three is established using shifting arguments. Then techniques from combinatorial design theory are used to establish a lower bound. Then it is shown that every number of sets of cardinality three between the lower and the upper bound can be realized. This characterization is restated to determine the precise spectrum of possible numbers of sets of cardinality two for specified numbers of sets of cardinality one and three. For simplicial complexes, these spectra are not always intervals, and the gaps are determined precisely. For simplicial multicomplexes, an alternative proof is given that these spectra are always intervals.IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

575

A Discrete Particle Swarm Optimization for Covering Array Generation
Huayao Wu, Changhai Nie, Member, IEEE, Fei-Ching Kuo, Member, IEEE, Hareton Leung, Member, IEEE, and Charles J. Colbourn

Abstract--Software behavior depends on many factors. Combinatorial testing (CT) aims to generate small sets of test cases to uncover defects caused by those factors and their interactions. Covering array generation, a discrete optimization problem, is the most popular research area in the field of CT. Particle swarm optimization (PSO), an evolutionary search-based heuristic technique, has succeeded in generating covering arrays that are competitive in size. However, current PSO methods for covering array generation simply round the particle's position to an integer to handle the discrete search space. Moreover, no guidelines are available to effectively set PSOs parameters for this problem. In this paper, we extend the set-based PSO, an existing discrete PSO (DPSO) method, to covering array generation. Two auxiliary strategies (particle reinitialization and additional evaluation of gbest) are proposed to improve performance, and thus a novel DPSO for covering array generation is developed. Guidelines for parameter settings both for conventional PSO (CPSO) and for DPSO are developed systematically here. Discrete extensions of four existing PSO variants are developed, in order to further investigate the effectiveness of DPSO for covering array generation. Experiments show that CPSO can produce better results using the guidelines for parameter settings, and that DPSO can generate smaller covering arrays than CPSO and other existing evolutionary algorithms. DPSO is a promising improvement on PSO for covering array generation. Index Terms--Combinatorial testing (CT), covering array generation, particle swarm optimization (PSO).

I. I NTRODUCTION S SOFTWARE functions and run-time environments become more complex, testing of modern software systems is becoming more expensive. Effective detection of

A

Manuscript received May 12, 2013; revised December 29, 2013 and May 18, 2014; accepted September 28, 2014. Date of publication October 9, 2014; date of current version July 28, 2015. This work was supported in part by the National Natural Science Foundation of China under Grant 61272079, in part by the Research Fund for the Doctoral Program of Higher Education of China under Grant 20130091110032, in part by the Science Fund for Creative Research Groups of the National Natural Science Foundation of China under Grant 61321491, in part by the Major Program of National Natural Science Foundation of China under Grant 91318301, and in part by the Australian Research Council Linkage under Grant LP100200208. (Corresponding author: Changhai Nie.) H. Wu and C. Nie are with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China (e-mail: hywu@outlook.com; changhainie@nju.edu.cn). F.-C. Kuo is with the Faculty of Information and Communication Technologies, Swinburne University of Technology, Hawthorn, VIC 3122, Australia (e-mail: dkuo@swin.edu.au). H. Leung is with the Department of Computing, Hong Kong Polytechnic University, Hong Kong (e-mail: hareton.leung@polyu.edu.hk). C. J. Colbourn is with Arizona State University, Tempe, AZ 85287-8809, USA (e-mail: colbourn@asu.edu). Digital Object Identifier 10.1109/TEVC.2014.2362532

failures at a low cost is a key issue for test case generation. Combinatorial testing (CT) is a popular testing method to detect failures triggered by various factors and their interactions [1]. By employing covering arrays as test suites, the CT method aims to sample the large combination space with few test cases to cover different interactions among a fixed number of factors. Kuhn and Reilly [2] shows that more than 70% of the failures in certain software were caused by the interactions of one or two factors, and almost all the failures could be detected by checking the interactions among six factors. Therefore, CT can be an effective method in practice. Generating a covering array with fewest tests (minimum size) is a major challenge in CT. In general, the minimum size of a covering array is unknown; hence, methods have focused on finding covering arrays that have as few tests as possible at reasonable search cost. The many methods that have been proposed can be classified into two main groups: 1) mathematical methods and 2) computational methods [1]. Mathematical (algebraic or combinatorial) methods typically exploit some known combinatorial structure. Computational methods primarily use greedy strategies or heuristic-search techniques to generate covering arrays, due to the size of the search space. Mathematical methods yield the best possible covering arrays in certain cases. For example, orthogonal arrays used in the design of experiments provide covering arrays with a number of tests that is provably minimum. However, all known mathematical methods can be applied only for restrictive sets of factors. This limitation has led to an emphasis on computational methods. Greedy algorithms have been quite effective in generating covering arrays, but their accuracy suffers from becoming trapped in local optima. In recent years, search-based software engineering (SBSE) has focused on using search-based optimization algorithms to find high-quality solutions for software engineering problems. Inspired by SBSE, many artificial intelligence-based heuristic-search techniques have been applied to software testing. For example, simulated annealing (SA) [3]­[7], genetic algorithm (GA) [8]­[10], and ant colony optimization (ACO) [9], [11], [12] have all been applied to covering array generation. These techniques can generate any types of covering arrays, and the constraint solving and prioritization techniques can be easily integrated. Their applications have been shown to be effective, producing relatively small covering arrays in many cases. Particle swarm optimization (PSO), a relatively new evolutionary algorithm,

1089-778X c 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

576

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

has also been used in this area [13]­[16]. It is easy to implement and has fast initial progress. The conventional PSO (CPSO) algorithm was originally designed to find optimal or near optimal solutions in a continuous space. Nevertheless, many discrete PSO (DPSO) algorithms and frameworks have been developed to solve discrete problems [17]­[22]. For covering array generation, current discrete methods [13]­[16] simply round the particle's position to an integer while keeping the velocity as a real number. They suffer from two main shortcomings. First, the performance of PSO is significantly impacted by its parameter settings. In [23], effects of the general parameter selection and initial population of PSO have been analyzed, but no guidelines on parameter settings have been reported for covering array generation. Hence, a clear understanding of how to set these execution parameters is needed. Second, simple rounding fractional positions to integers introduces a substantial source of errors in the search. Instead, a specialized DPSO version is needed. Because current PSO methods show promise for generating covering arrays, these two main shortcomings should be addressed. In this paper, we adapt set-based PSO (S-PSO) [18] to generate covering arrays. S-PSO utilizes set and probability theories to develop a new representation scheme for combinatorial optimization problems as well as refining the related evolution operators. In our adaptation of S-PSO, two auxiliary strategies are proposed to enhance performance, and a novel DPSO algorithm is thus proposed. DPSO has the same conceptual basis and exhibits similar search behavior to CPSO, with parameters playing similar roles. Then, we explore the optimal parameter settings for both CPSO and DPSO to improve their performance, and identify recommended settings. Furthermore, because many CPSO variants [24]­[27] can be easily extended to discrete versions based on our DPSO, the performance of these discrete versions is also compared with their original ones. Finally, we compare CPSO and DPSO with existing GA and ACO [9], [11] algorithms to generate covering arrays. The main contributions of this paper are as follows. 1) Based on the set-based representation, we design a version of S-PSO [18] for covering array generation. 2) We propose two auxiliary strategies, particle reinitialization, and additional evaluation of gbest, to enhance the performance of PSO. A novel DPSO for covering array generation is proposed. 3) We design experiments to explore the optimal parameter settings for CPSO and DPSO for covering array generation. 4) We implement original and discrete versions of four representative PSO variants (TVAC [24], CLPSO [25], APSO [26], and DMS-PSO [27]) to compare their efficacy for covering array generation. The rest of this paper is organized as follows. Section II gives background on CT, covering array generation, and the CPSO algorithm. Section III summarizes related work. Section IV presents our DPSO algorithm, including the representation scheme, related operators, and two auxiliary strategies. Section V evaluates the performance of CPSO and

TABLE I E-C OMMERCE S OFTWARE S YSTEM

DPSO, and explores optimal parameter settings. Section VI gives a comparison among CPSO, DPSO, and original and discrete variants. Section VII compares CPSO and DPSO with GA and ACO. Section VIII concludes this paper and outlines future work. II. BACKGROUND A. CT Suppose that the behavior of the software under test (SUT) is controlled by n independent factors, which may represent configuration parameters, internal or external events, user inputs, and the like. The ith factor has i discrete levels, chosen from the finite set Vi of size i . An n-tuple (x1 , x2 , . . . , xn ) forms a test case, where xi  Vi for 1  i  n. Consider a simple e-commerce software system [15]. This system consists of five different components. Each of these five components can be regarded as a factor, and its configurations can be regarded as different levels. Table I shows these five factors and their corresponding levels. In this example, n = 5, 1 = 2 = 3 = 2, 4 = 5 = 3. System failures are often triggered by interactions among some factors, which can be represented by the combinations of factor levels. In order to detect these failures, their combinations should be covered at least once by the test suite. A t-way schema can be used to represent them. Definition 1 (t-way schema): The n-tuple (-, y1 , . . . , yt , . . . ) is a t-way schema (t > 0) when some t factors have fixed levels and the others can take any valid levels, represented as "-." For example, suppose that when factor Payment Server takes the level Master and factor Web Server takes the level Apache, a system failure occurs. To detect this failure, the 2-way schema (Master, ­, Apache, ­, ­) must be covered at least once by the test suite. To simplify later discussion, we use the index in the level set of each factor to present a schema. For example, (0, ­, 1, ­, ­) is used to represent (Master, ­, Apache, ­, ­). Exhaustive testing covers all n-way schemas of the system; in our example, it uses 2 × 2 × 2 × 3 × 3 = 72 test cases. Testing cost becomes prohibitive as the numbers of factors and levels increase. Moreover, because only interactions among few factors are likely to trigger failures [2], testing high-way schemas can lead to many uninformative test cases. At the other extreme, if we only guarantee to cover each 1-way schema once, only three test cases are needed (a single test case can cover five 1-way schemas at most). But we may fail to detect some interaction triggered failures involving two factors.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

577

TABLE II C OVERING A RRAY CA(9; 2, 23 32 )

TABLE III A DDING T HREE T EST C ASES TO C ONSTRUCT VCA(12; 2, 23 32 , CA(3, 22 31 ))

Instead, CT covers all t-way schemas. Such a test suite is a t-way covering array, with t being the covering strength. The value of t determines the depth of coverage. It is a key setting of CT, and should be decided by the testers. We give a precise definition. Definition 2 (Covering Array): If an N ×n array, where N is the number of test cases, has the following properties: 1) each column i (1  i  n) contains only elements from the set Vi with i = |Vi | and 2) the rows of each N × t sub array cover all |Vk1 | × |Vk2 | × . . . × |Vkt | combinations of the t columns at least once, where t  n and 1  k1 < . . . < kt  n, then it is a t-way covering array, denoted by CA(N ; t, n, ( 1 , 2 , . . . , n )). When 1 = 2 = . . . = n = , it is denoted by CA(N ; t, n, ). Reference [2] demonstrated that more than 70% failures can be detected by a 2-way covering array, and almost all failures can be detected by a 6-way covering array. Hence, using CT, we can detect many failures of the system by applying relatively low strength covering arrays. In other words, CT can greatly reduce the size of test suite while maintaining high fault detection ability. In the example of Table I, if we only consider the interactions between any two factors, only nine test cases are required to construct a 2-way covering array instead of 72 for exhaustive testing. Table II shows a covering array, where each row represents one test case. This table covers all possible 2-way schemas for any two factors corresponding to the columns. For example, consider factor Payment Server and User Browser, all 2 × 3 = 6 schemas, (Master, ­, ­, Firefox, ­) (Master, ­, ­, Explorer, ­), (Master, ­, ­, Chrome, ­), (VISA, ­, ­, Firefox, ­), (VISA, ­, ­, Explorer, ­), (VISA, ­, ­, Chrome, ­), can be found in the table. For convenience, if several groups of gi factors (gi < n) have g the same number of levels ak , ak i can be used to represent these factors and their levels. Thus, the covering array can g g g gi = n, be denoted by CA(N ; t, a11 , a22 , . . . , ak k ) where n or CA(N ; t, a ) when g1 = n and a1 = a. For example, the covering array in Table II is a CA(9; 2, 23 32 ). In many software systems, the impacts of the interactions among factors are not uniform. Some interactions may be more prone to trigger system failures, while other may have little or no impact on the system. To effectively detect these different interactions, variable strength (VS) covering arrays can be applied. This can offer different covering strengths

to different groups of factors, and can therefore provide a practical approach to test real applications. Definition 3 (VS Covering Array): A VS covering array, m 1 denoted by VCA(N ; t, a11 . . . akk , CA1 (t1 , bm . . . bp p ), . . . , 1 nq 1 CAj (tj , cn 1 . . . cq )), is an N × n covering array of covering strength t containing one or more sub covering arrays, namely CA1 , . . . , CAj , each of which has covering strength t1 , . . . , tj all larger than t. Consider the e-commerce system shown in Table I. If the interactions of three factors, Payment Server, Web Server, and Business Database, have a higher probability to trigger system failures, then a VS covering array can be constructed. As in Table II, only three more test cases (Table III) are needed to construct the VCA(12; 2, 23 32 , CA(3, 22 31 )). With these 12 test cases, not only are all 2-way schemas of all five factors covered, but also all 3-way schemas of these three factors (Payment Server, Web Server, and Business Database) are covered. B. Covering Array Generation Covering arrays are used as test suites in CT. Covering array generation is the process of test suite construction. It is the most active area in CT with more than 50% of research papers focusing on this field [1]. Due to limited testing resources, all aim to construct a minimal covering array while still maintaining full coverage of combinations. Computational methods have been used widely for covering array generation because they can be applied to any systems. In general, these methods generate all possible combinations first. Then they generate test cases to cover these combinations one-by-one. One-testat-a-time is the most widely used strategy among evolutionary algorithms to generate a covering array. The one-test-at-a-time strategy was popularized by AETG [28] and was further used by Bryce and Colbourn [29]. This strategy takes the model of SUT(n, 1 , . . . , n ) where n is the number of factors and i is the number of valid levels of factor i, and the covering strength t as input. At first, an empty test suite TS and a set S of t-way schemas to be covered are initialized. In each iteration, a test case is generated with the highest fitness value according to some heuristic techniques. Then it is added to TS and the t-way schemas covered by it are removed. When all the t-way schemas have been covered, the final test suite TS is returned. This process is shown in Algorithm 1. In this strategy, a fitness function must be used to evaluate the quality of a candidate test case (line 6 in Algorithm 1). It is an important part of all heuristic techniques. In covering array generation, the fitness function takes the test case as the input and then outputs a fitness value representing its "goodness." It is defined as follows.

578

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

Algorithm 1 One-Test-at-a-Time Strategy 1: Input: SUT(n, 1 , . . . , n ) and covering strength t 2: Output: covering array TS 3: TS =  4: Construct S (all the t-way schemas to be covered) based on n, 1 , . . . , n and t 5: while S =  do 6: Generate a test case p with the highest fitness value according to some heuristics 7: Add p to the test suite TS 8: Remove the t-way schemas covered by p from S 9: end while 10: return TS

its corresponding position: vi,j (k + 1) =  × vi,j (k) + c1 × r1,j × (pbesti,j - xi,j (k)) (2) + c2 × r2,j × (gbestj - xi,j (k)) xi,j (k + 1) = xi,j (k) + vi,j (k + 1). (3) The best position of particle i in its history is pbesti , and gbest is the best position among all particles. The velocityupdate equation (2) captures the three basic concepts of PSO. The first is inertia, the tendency of the particle to continue in the direction it has been moving. The second is memory of the best position ever found by itself. The third is cooperation using the best position found by other particles. The parameter  is inertia weight. It controls the balance between exploration (global search state) and exploitation (local search state). Two positive real numbers c1 and c2 are acceleration constants that control the movement tendency toward the individual and global best position. Most studies set  = 0.9, and c1 = c2 = 2 to get the best balance [17], [31], [32]. In addition, r1,j and r2,j are two uniformly distributed random numbers in the range of [0.0, 1.0], used to ensure the diversity of the population. If the problem domain (the search space of particles) has bounds, a bound handling strategy is adopted to keep the particles inside the search space. Many different strategies have been proposed [33]. In the reflecting strategy, when a particle exceeds the bound of the search space in any dimension, the particle direction is reversed in this dimension to get back to the search space. For example, in case of a dimension with a range of values from 0 to 2, if a particle moves to 3, its position is reversed to 1. In addition, as the velocity can increase over time, a limit is set on velocity to prevent an infinite velocity or invalid position for the particle. Setting a maximum velocity, which determines the distance of movement from the current position to the possible target position, can reduce the likelihood of explosion of the swarm traveling distance [31]. Generally, the value of the maximum velocity is selected as i /2, where i is the range of dimension i. The pseudocode in Algorithm 2 presents the process of generating a test case by PSO. This algorithm can be invoked by line 6 of Algorithm 1 to generate a test case for t-way schemas. The n factors of the test case can be treated as an n-dimensional hyperspace. A particle pi = (x1 , x2 , . . . , xn ) can be regarded as a candidate test case. The fitness function is that of Definition 4, the number of uncovered t-way schemas in the generated test suite that are covered by particle pi . PSO employs real numbers but the valid values are integers for covering array generation, so each dimension of particle's position can be rounded to an integer while maintaining the velocity as a real number. This method is used in all prior research applying PSO to covering array generation [13]­[16]. III. R ELATED W ORK In this section, three different but related aspects are discussed. We first summarize search-based CT, especially the current applications of PSO for covering array generation. Then, we introduce prior research on discrete versions of PSO,

Definition 4 (Fitness Function): Let TS be the generated test set, and p be a test case. Then fitness(p) is the number of uncovered t-way schemas in TS that are covered by p. The fitness function can be formulated as fitness(p) = |schemat ({p}) - schemat (TS)| (1)

where schemat (TS) represents the set of all t-way schemas covered by test set TS, and | · | stands for cardinality. When t t-way schemas covered by p are not covered by TS, all Cn the fitness function reaches the maximum value fitness(p) = t. |schemat ({p})| = Cn For example, consider the 2-way covering array generation of the e-commerce system shown in Table II. Suppose that TS consists of test cases (0, 0, 0, 0, 0) and (0, 0, 1, 1, 1). The fitness function computes the fitness value for a candidate test case p = (1, 0, 1, 2, 2) as follows: because p covers ten 2-way schemas, namely schema2 ({p}) = {(1, 0, ­, ­, ­), (1, ­, 1, ­, ­,), (1, ­, ­, 2, ­), (1, ­, ­, ­, 2), (­, 0, 1, ­, ­), (­, 0, ­, 2, ­), (­, 0, ­, ­, 2), (­, ­, 1, 2, ­), (­, ­, 1, ­, 2), (­, ­, ­, 2, 2)} and TS only covers (­, 0, 1, ­, ­) in p, the function returns fitness(p) = 9. C. PSO PSO is a swarm-based evolutionary computation technique. It was developed by Kennedy et al. [30], inspired by the social behavior of bird flocking and fish schooling. PSO utilizes a population of particles as a set of candidate solutions. Each of the particles represents a certain position in the problem hyperspace with a given velocity. A fitness function is used to evaluate the quality of each particle. Initially, particles are distributed in the hyperspace uniformly. Then each particle repeatedly updates its state according to the individual best position in its history (pbest) and the current global best position (gbest). Eventually, each particle possibly moves toward the direction of the individual optimum and global optimum, and finds an optimal or near optimal solution. Suppose that the problem domain is a D-dimensional hyperspace. Then the position and velocity of particle i can be represented by xi  RD and vi  RD respectively. CPSO uses the following equations to update a particle's velocity and position, where vi,j (k) represents the jth component of the velocity of particle i at the kth iteration, and xi,j (k) represents

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

579

Algorithm 2 Generate Test Case by PSO 1: Input: SUT(n, 1 , . . . , n ), covering strength t and the related parameters of PSO 2: Output: the best test case gbest 3: it = 0, gbest = NULL 4: for each particle pi do 5: Initialize the position xi and velocity vi randomly 6: end for 7: while it < maximum iteration do 8: for each particle pi do 9: Compute the fitness value fitness(pi ) 10: if fitness(pi ) > fitness(pbesti ) then 11: pbesti = pi 12: end if 13: if fitness(pi ) > fitness(gbest) then 14: gbest = pi 15: end if 16: end for 17: for each particle pi do 18: Update the velocity and position according to Equations 2 and 3 19: Apply maximum velocity limitation and bound handling strategy 20: end for 21: it = it + 1 22: end while 23: return gbest

applied PSO to 2-way covering array generation. They further used a test suite minimization algorithm to reduce the size of the generated covering array. Current applications of PSO for covering array generation can yield smaller covering arrays than most greedy algorithms, but they all apply the same rounding operator to the particle's position, and they lack guidelines on the parameter settings. B. Discrete Versions of PSO PSO was initially developed to solve problems in continuous space, but PSO can also be applied to some discrete optimization problems including binary or general integer variables. Many discrete versions of PSO have been proposed [17]­[22]. Chen et al. [18] classify existing algorithms into four types. 1) Swap operator-based PSO [19] uses a permutation of numbers as position and a set of swaps as velocity. 2) Space transformation-based PSO [20] uses various techniques to transform the position, defined as a vector of real numbers, to the corresponding solution in discrete space. 3) Fuzzy matrix-based PSO [21] defines the position and velocity as a fuzzy matrix, and decode it to a feasible solution. 4) Incorporating-based PSO [22] consists of hybrid methods incorporating some heuristic or problem dependent techniques. Chen et al. [18] also propose a S-PSO method based on sets with probabilities, which we later adapt to represent a particle's velocity. S-PSO uses a set-based scheme to represent the discrete search space. The velocity is defined as a set with probabilities, and the operators are all replaced by procedures defined on the set. They extend some PSO variants to discrete versions and test them on the traveling salesman problem and the multidimensional knapsack problem. They show that the discrete version of CLPSO [25] can perform better than other variants. S-PSO was found to characterize the combinatorial optimization problems very well. Gong et al. [36] then employed such a set-based representation scheme for solving vehicle routing problems to obtain a new method, S-PSO-VRPTW. C. PSO Variants The original PSO may become trapped in a local optimum. In order to improve the performance, many variants have been proposed [17], [24]­[27]. Chen et al. [37] classify these variants into four different groups. The first group of variants aims to adjust the control parameters during the evolution, for example by decreasing inertia weight  linearly from 0.9 to 0.4 over the search process. The second group employs different neighborhood topologies. The basic local version of PSO makes the particle learn from the local best position lbesti found by particle i's neighborhood instead of the global best position gbest. RPSO and VPSO are two common versions which use a ring topology and a Von Neumann topology, respectively. The third group uses hybrid strategies with other search techniques. Many types of operators, such as genetic operators

to improve CPSO for discrete problems. Finally, some PSO variants are introduced. These try to enhance PSO for conventional continuous problems by adopting strategies that can further improve covering array generation. A. PSO in Search-Based CT SBSE has grown quickly in recent years. Many problems in software engineering have been formulated into searchbased optimization problems, and heuristic techniques have been used to find solutions. Software testing is a major topic in software engineering. Many heuristic techniques have also been applied to this field, including functional testing, mutation testing, stress testing, temporal testing, CT, and regression testing. Currently, many classic heuristic techniques, such as SA [3]­[7], GA [8]­[10], and ACO [9], [11], [12] have been applied to generate uniform and VS covering arrays successfully. PSO has also been applied to software testing. Windisch et al. [34] applied PSO to structural testing, and compared its performance with GA. They showed that PSO outperformed GA for most cases in terms of effectiveness and efficiency. Ganjali [35] proposed a framework for automatic test partitioning based on PSO, observing that PSO performed better than other existing heuristic techniques. PSO has been applied to covering array generation. Ahmed and Zamli [15] proposed frameworks of PSO to generate different kinds of covering arrays, including t-way (PSTG [14]) and VS-PSTG. Then they extended PSTG to support VS covering array generation [16]. Chen et al. [18] also

580

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE IV D IFFERENT G ROUPS OF DPSO AND PSO VARIANTS

(selection, crossover, and mutation), evolutionary computation paradigms and biology inspired operators have been used. The fourth uses multiswarm techniques. Several sets of swarms optimize different components of the solution concurrently or cooperatively. In our experiments, four representative PSO variants are included, as follows. 1) Ratnaweera et al. [24] proposed a typical variant of the first group, TVAC, which uses a time varying inertia weight and acceleration constant to adjust the parameters. 2) Liang et al. [25] proposed the CLPSO method, a wellknown variant of the second group, which allows the particle to learn from other particles' individual best positions in different dimensions. 3) Zhan et al. [26] proposed an adaptive PSO (APSO) that can be seen as a variant of the third group. They developed an evolutionary state estimation to adaptively control the parameter and used an elitist learning strategy. 4) DMS-PSO [27], a variant of the fourth group, was proposed by Liang and Suganthan [27]. It is characterized by a set of swarms with small sizes and these swarms are frequently regrouped. In summary, Table IV lists the different groups of discrete versions of PSO and PSO variants. IV. DPSO In this section, a new DPSO for covering array generation is presented. We firstly illustrate the weakness of CPSO with a simple example. Then the representation scheme of a particle's velocity and the corresponding redefined operators are introduced. Finally, we give two auxiliary strategies to enhance the performance of DPSO. A. Example In CPSO, a particle's position represents a candidate test case; its velocity, a real vector, represents the movement tendency of this particle in each dimension. This scheme is meaningful in a continuous optimization problem, because an optimal solution may exist near the current best particle's position. So it is desirable to move the particle to this area for further search. This may not hold for covering array generation.

Here, we use CA(N ; 2, 34 ) as an example. Suppose that three test cases (0, 1, 0, 1), (2, 1, 1, 2), and (1, 2, 0, 0) have been generated and added to TS in Algorithm 1, and the fourth one is to be generated according to CPSO. A possible candidate particle pi may have the position (0, 0, 0, 0) and velocity (0.5, 0.6, -0.4, 0.2) with fitness value 4, and its individual best position pbesti may be (0, 0, 1, 1) with fitness value 5. The global best position gbest may be (0, 2, 2, 2) with fitness value 6. According to the update (2), if we take  = 0.9 and c1 × r1 = c2 × r2  0.65, in the next iteration, pi 's velocity may be transformed to (0.45, 1.84, 1.59, 2.13). Thus, pi moves to the new position (0, 1, 1, 1) with a limitation on the velocity (the velocity is kept in [- i /2, i /2]), or (0, 1, 1, 2) with no limitation. In both cases, pi only has fitness value 2 after updating. When this occurs, pi evolves to a worse situation, although its position is closer to the pbesti and gbest than its original one. Analyzing the fitness measurement, the main contribution to the fitness value is the combinations that the test case can cover, not the concrete "position" at which it is located. For example, test case (2, 1, 1, 2) has a larger fitness value than (0, 0, 0, 0) because it covers six new schemas [(2, 1, ­, ­), (2, ­, 1, ­), (2, ­, ­, 2) etc.], not because of its relative distance to other particles. B. DPSO To overcome this weakness, the movement of particles should be modified. Inspired by the set-based representation scheme of velocity, DPSO is designed as a version of S-PSO [18] to make the particle learn from the individual and global best more effectively when generating covering arrays. Unlike S-PSO, the element of the velocity set in DPSO is designed for covering array generation, and DPSO does not classify the velocity set into different dimensions to avoid the inconsistency of different dimensions when updating velocities in S-PSO. In DPSO, a particle's position represents a candidate test case, while its velocity is changed to a set of t-way schemas with probabilities. Other than the velocity's representation and the newly defined operators, the evolution procedure of DPSO is the same as CPSO (Algorithm 2). Definition 5 (Velocity): The velocity is a set of pairs (s, p(s)), where s is a possible t-way schema of the covering array and p(s) is a real number between 0 and 1 representing the probability of the selection of schema s to update the current position. In the initialization of the swarm, the particle's position is t possible different schemas are randomly assigned. Then Cn selected randomly and each of them is assigned a random t pairs form the initial velocprobability p(s)  (0, 1). These Cn ity set of this particle; the size of this set changes dynamically during the evaluation. We consider the same example CA(N ; 2, 34 ). In DPSO, when particle pi is initialized, its position xi (k) may be (0, 0, 0, 0) representing a candidate test case as before, and its velocity vi (k) may be such a set {((1, 1, ­, ­), 0.7), ((0, ­, 0, ­), 0.3), ((­, 0, ­, 1), 0.8), ((0, ­, ­, 2), 0.9),

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

581

(a)

(b)

(c)

(d)

(e)

Fig. 1. Example of pi 's velocity updating. (a) 0.9 × vi (k). (b) 2 × r1 × (pbesti - xi (k)). (c) 2 × r2 × (gbest - xi (k)). (d) vi (k + 1). (e) Final vi (k + 1) where pro1 = 0.5.
2 = 6 ((­, 2, 1, ­), 0.5), ((­, ­, 0, 1), 0.2)} which contains C4 pairs. DPSO follows the conventional evolution procedure (Algorithm 2) that uses (2) and (3) to update the velocity and position of a particle. To adapt the new scheme for velocity in Definition 5, the related operators in these equations must be redefined. 1) Coefficient × Velocity: The coefficient is a real number which may be a parameter or a random number. It modifies all the probabilities in the velocity. Definition 6 (Coefficient × Velocity): Let a be a nonnegative real number and v be a velocity, a × v = {(s, p(s) × a)|(s, p(s))  v}. (If p(s) × a > 1, p(s) × a = 1.) For example, Fig. 1(a) shows the result for  × vi (k) where  = 0.9. 2) Position­Position: The difference of two positions gives the direction on which a particle moves. The results of the minus operator is a set of (s, p(s)) pairs, as velocity. Definition 7 (Position­Position): Let x1 and x2 be two positions. Then x1 - x2 = {(s, 0.5)|s is a schema that exists in x1 but not in x2 }. In the newly generated schema, probability p(s) for s is set to 0.5 so that the acceleration constants take similar values in both CPSO and DPSO. As in (2), the result of position­position is multiplied by ci × ri . In CPSO, ci is often set to 2 and ri is a random number between 0 and 1 (recall Section II-C). In DPSO, we want the value of final probability to have a range between 0 and 1 after multiplying by ci . Setting p(s) to 0.5 puts the result in [0, 1] when ci = 2. For example, suppose that xi (k) = (0, 0, 0, 0), pbesti = (0, 0, 1, 1), and gbest = (0, 2, 2, 2) as before. We can get pbesti - xi (k) = {((0, ­, 1, ­, ­), 0.5), ((0, ­, ­, 1), 0.5), ((­, 0, 1, ­), 0.5), ((­, 0, ­, 1), 0.5), ((­, ­, 1, 1), 0.5)}. Fig. 1(b) and (c) shows the results for 2 × r1 × (pbesti - xi (k)) and 2 × r2 × (gbest - xi (k)) respectively. 3) Velocity + Velocity: The addition of velocities gives a particle's movement path. The plus operator results in the union of two velocities. Definition 8 (Velocity + Velocity): Let v1 and v2 be two velocities. Then v1 + v2 = {(s, p(s))| if (s, p1 (s))  v1 and (s, p2 (s))  v2 , p(s) = p1 (s) + p2 (s); if (s, pi (s))  v1 and (s, pi (s))  / v2 or (s, pi (s))  v2 and (s, pi (s))  / v1 , p(s) = pi (s)}. (If p1 (s) + p2 (s) > 1, p1 (s) + p2 (s) = 1.) For example, Fig. 1(d) shows the results for pi 's new velocity vi (k + 1) after updating velocity.

Algorithm 3 Position Updating 1: Input: position xi (k), velocity vi (k + 1), pro2 and pro3 2: Output: new position xi (k + 1) 3: xi (k + 1) = (­, ­, . . . , ­) 4: Sort vi (k + 1) in descending order of p(s) 5: for each pair (si ,p(si )) in vi (k + 1) do 6: Generate a random number   [0, 1] 7: if  < p(si ) then 8: for each fixed level in si do 9: Generate a random number   [0, 1] 10: if  < pro2 and the corresponding factor of has not been fixed in xi (k + 1) then 11: Update xi (k + 1) with 12: end if 13: end for 14: end if 15: end for 16: if xi (k + 1) has unfixed factors then 17: Fill these factors by the same levels of previous position xi (k) 18: end if 19: Generate a random number   [0, 1] 20: if  < pro3 then 21: randomly change the level of one factor of xi (k + 1) 22: end if 23: return xi (k + 1)

In the velocity-updating phase of DPSO (2), we introduce a new parameter pro1 to control the size of the final velocity set. If p(si ) < pro1 , the pair (si , p(si )) is removed from the final velocity. For example, if we set pro1 = 0.5, vi (k + 1) is removed as shown in Fig. 1(e). Here, the velocity has been sorted in descending order of p(s), where if several p(s) have the same value, they are in an arbitrary order. If vi (k + 1) becomes empty, it stays empty until new pairs are added to it. As long as the velocity is empty, the particle's position is not updated and no better solutions can be found from this particle. In Section IV-C1, we discuss how to reinitialize this particle. 4) Position + Velocity: Position plus velocity is the position updating phase. Algorithm 3 gives the pseudo code of this procedure. Here, two new parameters, pro2 and pro3 , numbers in the range [0, 1], are introduced. pro2 is used to determine

582

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

the probability of selecting each fixed level in schema s and pro3 is a mutation probability to make the particle mutate randomly. An example helps to describe this procedure. We already have pi 's current position xi (k) = (0, 0, 0, 0), and its updated velocity vi (k + 1) in descending order of p(s) as shown in Fig. 1(e). We also assume that pro2 and pro3 are both set to 0.5. Each schema si here is selected to update the position with probability p(si ). For the first pair ((0, ­, ­, 2), 1.0), suppose that the random number  satisfies  < 1.0. Then, for each fixed level of this pair, namely level 0 of the first factor and level 2 of the fourth factor, its corresponding factor has not been fixed in xi (k + 1). Suppose that we have the first  < 0.5 but the second  > 0.5, the first factor will be selected to update the position and the second factor will not. So the new position becomes (0, ­, ­, ­). For the second pair, we regenerate the random number  , and compare it with the probability 0.9. If  < 0.9, the second pair is selected. If we generate  < 0.5 in two rounds, the new position becomes (0, 2, 2, ­). Accordingly, if the third pair is selected, and its second factor's level 0 is chosen to update, it does not change position because this factor has been set to a fixed level 2. This procedure is repeated until all factors in the new position xi (k + 1) are set to fixed levels. If all pairs in velocity have been considered, unfixed factors of xi (k + 1) are filled by the same levels of previous position xi (k). For example, after finishing the For loop in line 15, if the fourth factor of xi (k + 1) has not been given any level, the fourth factor of xi (k) is used to update it. Then xi (k + 1) becomes (0, 2, 2, 0). C. Auxiliary Strategies Auxiliary strategies can be introduced to improve the performance of PSO. Two of them are added to DPSO: 1) particle reinitialization to make the search more effective and 2) additional evaluation of gbest to improve the selection of the global best test case. 1) Particle Reinitialization: The PSO algorithm starts with a random distribution of particles, which finally converge. Then the best position that has been found is returned. The swarm may jump out of a local optimum, but this can not be guaranteed because CPSO lacks specific strategies for this. When applying PSO for covering array generation, increasing the number of iterations does not improve the ability to escape a local optimum. Hence, particle reinitialization, a widely used method, is employed to help DPSO to jump out of the local optimum. The main issue with particle reinitialization is when to reinitialize the particle. We may select a threshold, so that the reinitialization is done when the number of iterations exceeds the threshold. In DPSO, a better method can be applied. Using the new representation for velocity, when the current particle pi 's position equals its individual best position pbesti and global best position gbest, the size (norm) of pi 's velocity reduces gradually, because no pairs are generated from (pbesti - xi ) and (gbest - xi ), and the original pairs in velocity are removed gradually under the influence of  × v (reduce the p(s) of original pairs) and parameter pro1 . After a few fluctuations around gbest, the particle may stay at gbest, and

TABLE V T WO D IFFERENT C ONSTRUCTIONS OF CA(N ; 2, 34 )

its size (norm) of velocity is zero. We can use this scenario to trigger reinitialization of the particle. When the reinitialization is done, each dimension of a particle's position is randomly assigned a valid value, and its velocity is regenerated as in the initialization of the swarm. 2) Additional Evaluation of gbest: Current PSO methods to generate covering arrays generally employ the fitness function in Definition 4. This fitness function only focuses on the current candidate, and does not consider the partial test suite TS. Consider the generation of CA(N ; 2, 34 ). Table V shows two different construction processes. Both constructions generate (0, 0, 0, 0) as the first test case. Then they choose different test cases, but each of the first three reaches the largest number of 2 = 6. The difference between newly covered combinations, C4 these two constructions emerges when generating the fourth test case. In Construction 1, because the combinations with the same level between any two factors have all been covered, we cannot find a new test case that can still cover six combinations. However, in Construction 2, such a new test case can be found, (1, 0, 1, 2). Because the minimum size of CA(N ; 2, 34 ) is 9, each test case is required to cover six new combinations. Thus, Construction 1 cannot generate the minimum test suite, but Construction 2 can. In general, there may exist multiple test cases with the same highest fitness value, which make them equally qualified to be gbest in Algorithm 2. Instead of arbitrarily selecting one as gbest, it is better to apply additional distance metric to select one among them. As shown in Table V, if the new test case is similar to the existing tests [as (0, 1, 1, 1) is closer to (0, 0, 0, 0) than (1, 1, 1, 1)], there may be a better chance to find test cases with larger fitness subsequently. In order to measure the "similarity" between a test case t and an existing test suite TS, we use the average Hamming distance. The Hamming distance d12 indicates the number of factors that have different levels between two test cases t1 and t2 . Hence, the similarity between t and TS can be defined by the average Hamming distance H (t, TS) = 1 |TS| dtk .
kTS

(4)

The new gbest selection strategy selects the best candidate gbest based on two metrics. A test case that has the minimum average Hamming distance (i.e., the maximum similarity) from the set of test cases that all have the same highest fitness value is selected as gbest in Algorithm 2. For example, when we generate the second test case for CA(N ; 2, 34 ) in Table V, although (1, 1, 1, 1) and (0, 1, 1, 1) both have the highest fitness value 6, (0, 1, 1, 1) is selected as gbest

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

583

TABLE VI C OMPARING P ERFORMANCE B ETWEEN CPSO AND DPSO

because its average Hamming distance, 3, is smaller than that of (1, 1, 1, 1), 4. We expect that this additional evaluation can enhance the probability of generating a smaller test suite. In addition, because we still want to make the particle follow the conventional search behavior on the individual best direction, this additional distance metric will not be used in updating the pbest of DPSO. V. E VALUATION AND PARAMETER T UNING In this section, we first evaluate the effectiveness of DPSO in some representative cases, and compare the results against CPSO. Then the optimal parameter settings for both CPSO and DPSO are explored. The goal of evaluation and parameter tuning is to make the size of generated covering array as small as possible. Five representative cases of covering arrays, listed below, are selected for our experiments CA1 (N ; 2, 610 ) CA2 (N ; 3, 57 ) CA3 (N ; 4, 39 ) CA4 (N ; 2, 43 53 62 ) VCA(N ; 2, 315 , CA(3, 34 )). We consider four independent parameters, iteration number (iter), population size (size), inertia weight (), and acceleration constant (c), which play similar roles in both CPSO and DPSO, and three new parameters for DPSO, pro1 , pro2 , and pro3 . We carry out a base choice experiment to study the impact of various values of these parameters on CPSO and DPSO's performance and find the recommended settings for them. First, a base setting is chosen as the basic configuration. Then the value of each parameter is changed to create a series of configurations, while leaving the other parameters unchanged. Initially, we set iter = 50, size = 20,  = 0.9, c = 1.3, and pro1 = pro2 = pro3 = 0.5 as a basic configuration based on the previous studies [13], [16] and our empirical experience. To obtain statistically significant results, the generation of each case of covering array is executed 30 times. A. Evaluation of DPSO We compare the performance between CPSO and DPSO with the basic configuration. Five classes of covering arrays are generated by these two algorithms. The sizes obtained and average execution time per test case are shown as CPSO1 and DPSO in Table VI. The best and mean array sizes of DPSO are all better than those of CPSO1 . The new discrete representation scheme and auxiliary strategies improve PSO for covering array generation. DPSO can produce smaller covering arrays than CPSO with the basic configuration. However, DPSO spends more time

during the evolution, because its new operators for updating are more intricate than the conventional ones. DPSO needs to deal with many elements of the velocity set, whereas the conventional scheme only needs simple arithmetic operations. To compare the performance between CPSO and DPSO given the same execution time, for each case we let the execution time per test case for CPSO equal to that for DPSO, so that CPSO can spend more time in searching. We refer to this version of CPSO as CPSO2 . In addition, a t-test between CPSO2 and DPSO is conducted and the corresponding p-value is presented. A p-value smaller than 0.05 indicates that the performance of these two algorithms is statistically different with 95% confidence. From CPSO1 , CPSO2 , and DPSO in Table VI, the performance of CPSO is improved with more search time. DPSO, which must use fewer iterations, still works better than CPSO. The effectiveness of DPSO comes from the essential improvement of its new representation scheme for velocity and the auxiliary strategies. The results of the t-test demonstrate the significance of these differences. Therefore, we can conclude that DPSO performs better than CPSO with fewer iterations for covering array generation. B. Parameter Tuning In the base choice experiment, because the sizes of the various covering arrays differ, the mean size of each case (si ) obtained is normalized using si = si - smin . smax - smin

This normalization enables the graphical representation of different cases on a common scale. Because some parameters may not significantly impact the performance, we use ANOVA (significance level = 0.05) to test whether there exist significant differences among the mean results obtained by different parameter settings. When changing the parameter settings have no significant impact on the generation results, these results will be presented as dotted lines in the corresponding figures. For example, CA4 is presented as a dotted line in Fig. 2(b). It means that the iteration number does not significantly impact the generation of this case of covering array, and so this case will not be further considered when identifying the optimal settings. 1) Iteration Number (iter): Iteration number determines the number of updates. PSO typically requires thousands of iterations to evolve. Nevertheless, only a few iterations are required to generate covering array according to [14]­[16]. So we

584

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

(a)
Fig. 2. Comparing array sizes under different iteration numbers. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 3. Comparing array sizes under different population sizes. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 4.

(b)

Comparing array sizes under different settings of population size and iteration number. (a) CPSO. (b) DPSO.

change its value from 50, the base setting, to 1950 incrementally. Much larger values are not used because the execution time may increase markedly without a commensurate increase in the quality of the results. Fig. 2 shows the results for different choices of iteration numbers, where each line represents a covering array. Performance is improved with increasing iterations for both CPSO and DPSO. A small number of iterations may not be appropriate due to insufficient searching. Because the optimal settings are different among different cases, and several settings can be chosen to generate the minimum covering arrays, it is open to debate which setting is the "best" one. Given time constraints, for a population size of 20, a good setting of iteration number could be approximately 1000 for both CPSO and DPSO. 2) Population Size (Size): Population size determines the initial search space. Generally a small value, e.g., 20, is satisfactory for most cases. A large population size may bring a higher diversity and find a better solution, but it also increases the evolving time. For the same reason as before, we change the value of population size from 10 to 100.

Fig. 3 shows the results for different choices of population size. There is no doubt that the mean array size obtained decreases as population size increases. A large population size can have more chances to generate smaller covering arrays, but its execution time can become prohibitive. In addition, because iteration number and population size together determine the search effort of PSO, we explore the combinations between these two parameters. We let iter × size be a constant 20 000, and generate each case under different settings of these two parameters. Fig. 4 shows the results, where PSO prefers a relatively larger population size. In CPSO, ten particles with 2000 iterations is the worst setting, and most cases produce good results with 60 or 80 particles. In DPSO, the best choice of population size is still 80. In both CPSO and DPSO, the largest population size 100 cannot produce as good results as that of 80 due to having fewer iterations. So achieving smallest covering arrays requires a good balance between these two parameters, and a moderately large population size is necessary for both CPSO and DPSO. Thus, we can set iteration number to 250, and population size to 80, as the recommended settings for both CPSO and DPSO.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

585

(a)
Fig. 5. Comparing array sizes under different inertia weights. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 6. Comparing array sizes under different acceleration constants. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 7.

(b)

(c)

Comparing array sizes under different settings of (a) pro1 , (b) pro2 , and (c) pro3 . TABLE VII R ECOMMENDED PARAMETER S ETTINGS

3) Inertia Weight (): Inertia weight determines the tendency of the particle to continue in the same direction. A small value help the particle move primarily toward the best position, while a large one is helpful to continue its previous movement. A linearly decreasing value is also used as it can make the swarm gradually narrow the search space. Here, we investigate both fixed values and a linearly decreasing value from 0.9 to 0.4 over the whole evolution (presented as "dec"). Fig. 5 shows the results for different choices of inertia weight. In CPSO, most of the smallest covering arrays are generated by large fixed inertia weights. The decreasing value does not perform as well as the large values, such as 0.9 for CA1 , CA2 , and CA4 . So 0.9 can be the recommended choice for CPSO. In DPSO, 1.1 is the worst choice because the particle may update its position based on its own velocity and thus fail to learn from individual and global best positions. The decreasing value can perform reasonably well, but a fixed value 0.5 may be a better choice in that it keeps the effort of global search moderate. Thus, we can recommend the fixed inertia weight of 0.9 for CPSO, and 0.5 for DPSO. 4) Acceleration Constant (c): Acceleration Constants c1 and c2 control the learning from pbest and gbest, respectively. Generally, they are set to the same value to balance

the influence of these two positions. Setting c1 and c2 to a large value may make the particle more likely attracted to the best position ever found, while a small one may make the search far from the current optimal region. In this paper, we set c1 = c2 = c, and vary c from 0.1 to 2.9. Fig. 6 shows the results for different choices of acceleration constant. Unlike other parameters, there is a consistent trend in all five cases. In CPSO, the values larger than 0.5 all produce good results. In DPSO, 1.3 can definitely be regarded as the optimal value. Thus, we set 1.3 as the recommended value of acceleration constant for both CPSO and DPSO. 5) pro1 , pro2 , and pro3 : These three parameters are new to DPSO. Parameter pro1 determines the size of the final velocity. As those pairs whose probabilities are smaller than pro1 are removed from the final velocity set, a small value may keep more pairs in the velocity for the next evolution, but it

586

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE VIII S IZES (N ) OF C OVERING A RRAYS FOR n FACTORS , E ACH H AVING T HREE L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, n, 3)

also requires more computation time. Parameter pro2 determines the probability of selecting each fixed level from each schema when updating positions. A larger value may lead to a quick construction of new position, but it may also lead to fast convergence toward a local optimum. Parameter pro3 determines the mutation probability when updating positions. A larger value may enhance the randomness, but it also lowers the convergence speed. In this paper, values from 0.1 to 1.0 for these three parameters are investigated. Fig. 7 shows the results. For pro1 , a large value is not appropriate because it removes nearly all pairs from the final velocity set. A medium value 0.5, which appears to lead to the best result, may be the best choice. For pro2 , the smallest value 0.1 yields the best results. Slower construction of the new position may slow the convergence toward a local optimum, but it also takes longer. So, in order to balance performance and execution time, we take 0.3 as the recommended value for pro2 . For pro3 , a larger value may be a good choice. The frequent mutation of new position may bring better results, but also takes longer to converge. So, we take 0.7 as the recommended value for pro3 . In summary, the recommended parameter settings for PSO for covering array generation are different from previously suggested ones [13], [16]. Some parameters may significantly impact the performance in some cases, and parameter tuning is necessary to enhance heuristic techniques for particular applications. Naturally, the optimal settings vary for different cases of covering arrays. There may not exist a common setting that can always lead to the best results. Because it is

impractical to tune parameters for each case in real life, we suggest two general settings for CPSO and DPSO, as shown in Table VII, which can typically lead to better performance within reasonable execution time. VI. C OMPARING A MONG PSO S In this section, we compare the best reported array sizes generated by PSO in [16] with our findings for CPSO, DPSO, and four representative variants. Because the research in [16] demonstrated that their generation results typically outperform greedy algorithms, in this paper, we do not compare CPSO and DPSO with greedy algorithms. We implement both the original and discrete versions of four variants (TVAC [24], CLPSO [25], APSO [26], and DMS-PSO [27]) to generate covering arrays. Their discrete versions are extended based on new representation scheme of velocity and auxiliary strategies. We name them D-TVAC, DCLPSO, D-APSO, and D-DMS-PSO, respectively. In CLPSO, a particle can learn from different particles' pbest in each dimension whereas we do not distinguish dimensions strictly in DPSO. So in D-CLPSO, a particle can fully learn from different particles' pbest in all dimensions. That may weaken the search ability of CLPSO. For the other three variants, they can be directly extended based on DPSO. All algorithms are compared using the same number of fitness function evaluations. CPSO and DPSO use the recommended settings shown in Table VII. For the variants, iter, size and pro1 , pro2 and pro3 for discrete versions are set to

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

587

TABLE IX S IZES (N ) OF C OVERING A RRAYS FOR S EVEN FACTORS , E ACH H AVING

L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, 7, )

their recommended values.  and c are also set to recommended values unless they are adaptively adjusted during the evolution, in which case their range is set to [0.4, 0.9] and [0.8, 1.8], respectively. The new control parameters for these variants follow their suggested settings. Tables VIII­XIII give the results. Because of the execution time, we only consider covering strengths from 2 to 4, and the generation of each covering array is repeated 30 times. A t-test (significance level = 0.05) is also conducted to test whether there exists a significant difference between the mean sizes produced by the two algorithms. In the first three columns, we report the best and mean array sizes obtained from previous results, CPSO and DPSO, where boldface numbers indicate that the difference between CPSO and DPSO is significant based on the t-test. In the last four columns, we report the mean array sizes from the original and discrete versions of each PSO variant (presented as meanc and meand respectively), where boldface numbers indicate that the difference between meanc and meand of each variant is significant. A. Uniform Covering Arrays Tables VIII­X present the results for uniform covering arrays. We extend the cases considered in [16], where "­" indicates the not available cases. In Tables VIII, we report array sizes for n factors, each having three levels. In Tables IX and X, we report array sizes for 7 and 10 factors, each having levels. Their covering strengths all range from 2 to 4. Typically, CPSO can produce smaller sizes than those reported in [16], demonstrating the effectiveness of parameter tuning. Furthermore, DPSO can produce the smallest best and mean sizes in almost all cases, and usually the performance of DPSO is significantly better than CPSO. When the covering strength t = 2 is adopted (Table VIII), DPSO does

not generally outperform CPSO. But when covering strength increases, DPSO performs better. Sometimes the mean sizes for DPSO are smaller than the best sizes for CPSO. Because generating a covering array with higher covering strength is more complicated, DPSO may be more appropriate for generating such covering arrays. In Tables IX and X, we also find that DPSO is superior for covering arrays with high covering strength. Overall, these results show the effectiveness of DPSO for uniform covering array generation. Surprisingly, DPSO does not beat previous results for CPSO when covering arrays have two levels for each factor (Table IX). This appears to be a weakness of DPSO. Although DPSO generates smaller covering arrays than previous results and CPSO, other techniques may still yield better results than DPSO (some best known sizes can be found in [38]). B. VS Covering Array Tables XI­XIII give the results for VS covering arrays. Based on CA(N ; 2, 315 ), CA(N ; 3, 315 ), and CA(N ; 2, 42 52 63 ), some different cases of sub covering arrays conducted in [16] are examined. Their covering strengths are at most 4. Generally, we can draw similar conclusions as for uniform covering arrays. CPSO with the suggested parameter setting can produce better results than reported sizes in some cases. DPSO also usually beats them on the best and mean sizes. In Table XI, often the difference between CPSO and DPSO is not significant. In part this is because for the CA(3, 33 ), CA(3, 33 )2 , CA(3, 33 )3 , and CA(4, 34 ), the same best results are provably the minimum (e.g., the minimum size of the CA(3, 33 ) is 3 × 3 × 3 = 27). For the other cases, although sometimes the difference is not significant, DPSO can still generate smaller covering arrays. DPSO remains a good choice for generating VS covering arrays. In Tables XII and XIII, similar results can be found.

588

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE X S IZES (N ) OF C OVERING A RRAYS FOR T EN FACTORS , E ACH H AVING

L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, 10, )

TABLE XI S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 2, 315 , CA)

TABLE XII S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 3, 315 , CA)

For both uniform and variable cases of covering arrays, parameter tuning can enhance CPSO to generate smaller covering arrays than previous work. DPSO can produce the smallest covering arrays in nearly all cases, and in general the difference of performance between CPSO and DPSO is significant. DPSO is an effective discrete version of PSO for covering array generation. C. PSO Variants In order to further investigate the effectiveness of DPSO for covering array generation, we implement the original versions of four representative variants of PSO and extend them to their discrete versions based on DPSO. We apply them to generate

the same cases for covering arrays. The mean sizes obtained are shown in the last four columns in Tables VIII­XIII. We first compare the mean sizes of original PSO variants with those of CPSO and DPSO. In our experiments, TVAC's mean sizes are always larger than CPSO's. The linear adjustment of inertia weight and acceleration constant is not helpful for CPSO for covering array generation. Typically, APSO's mean sizes are also larger than CPSO's. Because APSO uses a fuzzy system to classify different evolutionary states, its ineffectiveness may result from inappropriate parameter settings. CLPSO and DMS-PSO can outperform CPSO typically. Although on occasion they achieve comparable performance with DPSO, they cannot perform as well as DPSO in most

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

589

TABLE XIII S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 2, 43 53 62 , CA)

TABLE XIV C OMPARING CPSO AND DPSO W ITH GA AND ACO

cases. That further demonstrates the effectiveness of DPSO for covering array generation. Because these four algorithms are representative PSO variants (as shown in Table IV), designing different neighborhood topologies and using multiswarm techniques may have potential to improve CPSO for covering array generation. We next compare the original and discrete versions of each variant. Except for CLPSO, the other three variants can be improved using their discrete versions, and the improvement is also significant. TVAC cannot outperform CPSO, but it is enhanced by DPSO so that D-TVAC can produce smaller mean sizes than CPSO in most cases. The linear adjustment is helpful for the discrete version. For CLPSO, only in a few cases is it improved by DPSO. Sometimes D-CLPSO even leads to worse results (see Table X), due primarily to the weakened search ability of its discrete version as explained in Section VI. For APSO, DPSO can enhance its original version, but D-APSO is still worse than DPSO. That may result from inappropriate settings as explained before. For DMS-PSO, sometimes DPSO does not enhance it (see Table XI). However, in most cases D-DMS-PSO can outperform DMS-PSO and has comparable performance with D-TVAC. The multiswarm strategy is also helpful for the discrete version.

In summary, the comparison study reveals that our suggested parameter settings are more suitable for covering array generation. DPSO is an effective discrete version of PSO. It can significantly outperform previous results and CPSO in nearly all cases for uniform and VS covering arrays. Furthermore, DPSO's representation scheme of a particle's velocity and auxiliary strategies not only enhance CPSO, but also typically enhance PSO variants. DPSO is a promising improvement on PSO for covering array generation. VII. C OMPARING DPSO W ITH GA AND ACO Because GAs and ACO [8]­[12] have also been successfully used for covering array generation, we compare CPSO and DPSO with the reported array sizes in [9] and [11]. There are no widely accepted benchmarks for the comparison of searchbased covering array generation, so we only consider these two representative and competitive works. Shiba et al. [9] applied both GA and ACO to generate uniform covering arrays (CA1 to CA8 in Table XIV), and Chen et al. [11] applied ACO to generate VS covering arrays (VCA9 to VCA12 in Table XIV). They both set algorithm parameters according to recommendations in related research fields without parameter tuning, while our CPSO and DPSO

590

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

use the recommended settings shown in Table VII. To compare these algorithms under the same number of fitness evaluations, for each case of covering array, the population size and the number of iterations of CPSO and DPSO are modified accordingly to satisfy the settings in [9] and [11]. Moreover, Shiba et al. [9] and Chen et al. [11] both applied test minimization algorithms to further reduce the size of generated covering arrays, while our CPSO and DPSO do not apply any minimization algorithms. Table XIV shows the comparison results, where boldface numbers indicate the best array sizes obtained, and "­" represents that the corresponding data is not available. The generation of each case of covering array of CPSO and DPSO is executed 30 times and the best and average results are presented. Because we do not implement GA and ACO for covering array generation, no statistical tests can be conducted here. In addition, because the platforms used for collecting the results differ, the comparison of computational time would not be informative. We nevertheless present the execution times of our CPSO and DPSO, which can serve as references for practitioners. From Table XIV, DPSO can outperform existing GA and ACO for covering array generation, despite the latter two applying test minimization algorithms. Because our DPSO is a version of PSO designed and tuned for covering array generation, it suggests that tuned versions of GAs and ACOs may be a promising area for further study. In addition, CPSO performs worse than GA and ACO in seven of 12 cases. That may result from the improvement by minimization algorithms in [9] and [11]. But for CA2 , CA3 , and CA6 , CPSO can still achieve smaller covering arrays. In summary, the results further demonstrate that DPSO is an effective discrete version of PSO for covering array generation. Further investigations of tuned versions of GA and ACO should be considered. VIII. C ONCLUSION Covering array generation is a key issue in CT. We developed a new DPSO for covering array generation, by adapting S-PSO to generate covering arrays and incorporating two auxiliary strategies to improve the performance. Parameter tuning was applied to both CPSO and DPSO to identify their best parameter settings. The original and discrete versions of four representative PSO variants were implemented and their efficacy for covering array generation was compared. The performance of DPSO was also compared with other existing evolutionary algorithms, GA and ACO. DPSO can perform better than CPSO with fewer iterations, and the performance of CPSO and DPSO is significantly impacted by their parameter settings. Different cases require different parameter settings; there may not exist a single choice that leads to the best results. After parameter tuning, we identified two recommended settings, which lead to relatively good performance for CPSO and DPSO. Indeed, CPSO with our recommended parameter settings improves on previously reported results, and our DPSO with recommended parameter settings usually beats CPSO. Typically, DPSO also significantly

enhances the performance of PSO variants. In addition, DPSO often outperforms GA and ACO to generate covering arrays. Consequently, DPSO is a promising improvement of PSO for covering array generation. Improvements on the methods here may be possible in a number of ways. One would be to investigate further evolution procedures and strategies proposed in PSO, such as hybridizing with penalty approaches to handle discrete unknowns [39], and compare the results with some exact schemes like branch and bound method. A second would be to examine one-column-at-a-time approaches or methods that construct the entire array, rather than the one-row-at-a-time approach adopted here. A third would be to incorporate DPSO with other methods, in particular with test minimization methods. R EFERENCES
[1] C. Nie and H. Leung, "A survey of combinatorial testing," ACM Comput. Surv., vol. 43, no. 2, pp. 11.1­11.29, 2011. [2] D. Kuhn and M. Reilly, "An investigation of the applicability of design of experiments to software testing," in Proc. 27th Annu. NASA Goddard/IEEE Softw. Eng. Workshop, Greenbelt, MD, USA, 2002, pp. 91­95. [3] M. Cohen, P. Gibbons, W. Mugridge, and C. Colbourn, "Constructing test suites for interaction testing," in Proc. 25th Int. Conf. Softw. Eng., Portland, OR, USA, 2003, pp. 38­48. [4] M. B. Cohen, C. J. Colbourn, and A. C. Ling, "Constructing strength three covering arrays with augmented annealing," Discrete Math., vol. 308, no. 13, pp. 2709­2722, 2008. [5] J. Torres-Jimenez and E. Rodriguez-Tello, "Simulated annealing for constructing binary covering arrays of variable strength," in Proc. Congr. Evol. Comput., Barcelona, Spain, Jul. 2010, pp. 1­8. [6] B. Garvin, M. Cohen, and M. Dwyer, "Evaluating improvements to a meta-heuristic search for constrained interaction testing," Empir. Softw. Eng., vol. 16, no. 1, pp. 61­102, 2011. [7] J. Torres-Jimenez and E. Rodriguez-Tello, "New bounds for binary covering arrays using simulated annealing," Inf. Sci., vol. 185, no. 1, pp. 137­152, 2012. [8] S. Ghazi and M. Ahmed, "Pair-wise test coverage using genetic algorithms," in Proc. Congr. Evol. Comput., vol. 2. Canberra, ACT, Australia, 2003, pp. 1420­1424. [9] T. Shiba, T. Tsuchiya, and T. Kikuno, "Using artificial life techniques to generate test cases for combinatorial testing," in Proc. 28th Annu. Int. Comput. Softw. Appl. Conf., vol. 1. Hong Kong, 2004, pp. 72­77. [10] J. McCaffrey, "An empirical study of pairwise test set generation using a genetic algorithm," in Proc. 7th Int. Conf. Inf. Technol. New Gener., Las Vegas, NV, USA, 2010, pp. 992­997. [11] X. Chen, Q. Gu, A. Li, and D. Chen, "Variable strength interaction testing with an ant colony system approach," in Proc. Asia-Pacific Softw. Eng. Conf., Penang, Malaysia, 2009, pp. 160­167. [12] X. Chen, Q. Gu, X. Zhang, and D. Chen, "Building prioritized pairwise interaction test suites with ant colony optimization," in Proc. 9th Int. Conf. Qual. Softw., Jeju-do, Korea, 2009, pp. 347­352. [13] X. Chen, Q. Gu, J. Qi, and D. Chen, "Applying particle swarm optimization to pairwise testing," in Proc. 34th Annu. Comput. Softw. Appl. Conf., Seoul, Korea, 2010, pp. 107­116. [14] B. S. Ahmed and K. Z. Zamli, "PSTG: A T-way strategy adopting particle swarm optimization," in Proc. 4th Asia Int. Conf. Math. Anal. Model. Comput. Simulat., Kota Kinabalu, Malaysia, 2010, pp. 1­5. [15] B. S. Ahmed and K. Z. Zamli, "A variable strength interaction test suites generation strategy using particle swarm optimization," J. Syst. Softw., vol. 84, no. 12, pp. 2171­2185, 2011. [16] B. S. Ahmed, K. Z. Zamli, and C. P. Lim, "Application of particle swarm optimization to uniform and variable strength covering array construction," Appl. Soft Comput., vol. 12, no. 4, pp. 1330­1347, 2012. [17] Y. del Valle, G. Venayagamoorthy, S. Mohagheghi, J.-C. Hernandez, and R. Harley, "Particle swarm optimization: Basic concepts, variants and applications in power systems," IEEE Trans. Evol. Comput., vol. 12, no. 2, pp. 171­195, Apr. 2008.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

591

[18] W.-N. Chen et al., "A novel set-based particle swarm optimization method for discrete optimization problems," IEEE Trans. Evol. Comput., vol. 14, no. 2, pp. 278­300, Apr. 2010. [19] M. Clerc, Discrete Particle Swarm Optimization (New Optimization Techniques in Engineering). New York, NY, USA: Springer, 2004. [20] W. Pang et al., "Modified particle swarm optimization based on space transformation for solving traveling salesman problem," in Proc. Int. Conf. Mach. Learn. Cybern., vol. 4. Shanghai, China, Aug. 2004, pp. 2342­2346. [21] W. Pang, K.-P. Wang, C.-G. Zhou, and L.-J. Dong, "Fuzzy discrete particle swarm optimization for solving traveling salesman problem," in Proc. 4th Int. Conf. Comput. Inf. Technol., Wuhan, China, 2004, pp. 796­800. [22] Y. Wang et al., "A novel quantum swarm evolutionary algorithm and its applications," Neurocomputing, vol. 70, nos. 4­6, pp. 633­640, 2007. [23] E. Campana, G. Fasano, and A. Pinto, "Dynamic analysis for the selection of parameters and initial population, in particle swarm optimization," J. Global Optim., vol. 48, no. 3, pp. 347­397, 2010. [24] A. Ratnaweera, S. Halgamuge, and H. Watson, "Self-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients," IEEE Trans. Evol. Comput., vol. 8, no. 3, pp. 240­255, Jun. 2004. [25] J. Liang, A. Qin, P. Suganthan, and S. Baskar, "Comprehensive learning particle swarm optimizer for global optimization of multimodal functions," IEEE Trans. Evol. Comput., vol. 10, no. 3, pp. 281­295, Jun. 2006. [26] Z.-H. Zhan, J. Zhang, Y. Li, and H.-H. Chung, "Adaptive particle swarm optimization," IEEE Trans. Syst., Man, Cybern., B, Cybern., vol. 39, no. 6, pp. 1362­1381, Dec. 2009. [27] J. Liang and P. Suganthan, "Dynamic multi-swarm particle swarm optimizer," in Proc. IEEE Swarm Intell. Symp., Pasadena, CA, USA, 2005, pp. 124­129. [28] D. Cohen, S. Dalal, M. Fredman, and G. Patton, "The AETG system: An approach to testing based on combinatorial design," IEEE Trans. Softw. Eng., vol. 23, no. 7, pp. 437­444, Jul. 1997. [29] R. C. Bryce and C. J. Colbourn, "One-test-at-a-time heuristic search for interaction test suites," in Proc. 9th Annu. Conf. Genet. Evol. Comput., London, U.K., 2007, pp. 1082­1089. [30] J. Kennedy and R. Eberhart, "Particle swarm optimization," in Proc. Int. Conf. Neural Netw., vol. 4. Perth, WA, Australia, 1995, pp. 1942­1948. [31] R. Eberhart and Y. Shi, "Particle swarm optimization: Developments, applications and resources," in Proc. Congr. Evol. Comput., vol. 1. Seoul, Korea, 2001, pp. 81­86. [32] R. Poli, J. Kennedy, and T. Blackwell, "Particle swarm optimization," Swarm Intell., vol. 1, no. 1, pp. 33­57, 2007. [33] S. Helwig, J. Branke, and S. Mostaghim, "Experimental analysis of bound handling techniques in particle swarm optimization," IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 259­271, Apr. 2013. [34] A. Windisch, S. Wappler, and J. Wegener, "Applying particle swarm optimization to software testing," in Proc. 9th Annu. Conf. Genet. Evol. Comput., London, U.K., 2007, pp. 1121­1128. [35] A. Ganjali, "A requirements-based partition testing framework using particle swarm optimization technique," M.S. thesis, Dept. Electr. Comput. Eng., Univ. Waterloo, Waterloo, ON, Canada, 2008. [36] Y.-J. Gong et al., "Optimizing the vehicle routing problem with time windows: A discrete particle swarm optimization approach," IEEE Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 42, no. 2, pp. 254­267, Mar. 2012. [37] W.-N. Chen et al., "Particle swarm optimization with an aging leader and challengers," IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 241­258, Apr. 2013. [38] C. J. Colbourn. (Apr. 2013). Covering Array Tables for t=2, 3, 4, 5, 6. [Online]. Available: http://www.public.asu.edu/ ccolbou/src/ tabby/catable.html [39] M. Corazza, G. Fasano, and R. Gusso, "Particle swarm optimization with non-smooth penalty reformulation, for a complex portfolio selection problem," Appl. Math. Comput., vol. 224, pp. 611­624, Nov. 2013.

Huayao Wu received the B.S degree from Southeast University, Nanjing, China and the M.S degree from Nanjing University, Nanjing, China, where he is currently working toward the Ph.D. degree from Nanjing University, Nanjing. His research interests include software testing, especially on combinatorial testing and search-based software testing.

Changhai Nie (M'12) received the B.S. and M.S. degrees in mathematics from Harbin Institute of Technology, Harbin, China, and the Ph.D. degree in computer science from Southeast University, Nanjing, China. He is a Professor of Software Engineering with State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing. His research interests include software analysis, testing and debugging.

Fei-Ching Kuo (M'06) received the B.Sc. (Hons.) degree in computer science and the Ph.D. degree in software engineering from Swinburne University of Technology, Hawthorn, VIC, Australia. She was a Lecturer with University of Wollongong, Wollongong, NSW, Australia. She is currently a Senior Lecturer with the Swinburne University of Technology. Her research interests include software analysis, testing, and debugging.

Hareton Leung (M'90) received the Ph.D. degree in computer science from University of Alberta, Edmonton, AB, Canada. He is an Associate Professor and a Director of the Laboratory for Software Development and Management, Department of Computing, Hong Kong Polytechnic University, Hong Kong. His research interests include software testing, project management, risk management, quality and process improvement, and software metrics.

Charles J. Colbourn received the Ph.D. degree from University of Toronto, Toronto, ON, Canada, in 1980. He is a Professor of Computer Science and Engineering with Arizona State University, Tempe, AZ, USA. He has authored the books The Combinatorics of Network Reliability (Oxford) and Triple Systems (Oxford), and also 320 refereed journal papers focussing on combinatorial designs and graphs with applications in networking, computing, and communications. Prof. Colbourn received the Euler Medal for Lifetime Research Achievement by the Institute for Combinatorics and its Applications in 2004.

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

575

A Discrete Particle Swarm Optimization for Covering Array Generation
Huayao Wu, Changhai Nie, Member, IEEE, Fei-Ching Kuo, Member, IEEE, Hareton Leung, Member, IEEE, and Charles J. Colbourn

Abstract--Software behavior depends on many factors. Combinatorial testing (CT) aims to generate small sets of test cases to uncover defects caused by those factors and their interactions. Covering array generation, a discrete optimization problem, is the most popular research area in the field of CT. Particle swarm optimization (PSO), an evolutionary search-based heuristic technique, has succeeded in generating covering arrays that are competitive in size. However, current PSO methods for covering array generation simply round the particle's position to an integer to handle the discrete search space. Moreover, no guidelines are available to effectively set PSOs parameters for this problem. In this paper, we extend the set-based PSO, an existing discrete PSO (DPSO) method, to covering array generation. Two auxiliary strategies (particle reinitialization and additional evaluation of gbest) are proposed to improve performance, and thus a novel DPSO for covering array generation is developed. Guidelines for parameter settings both for conventional PSO (CPSO) and for DPSO are developed systematically here. Discrete extensions of four existing PSO variants are developed, in order to further investigate the effectiveness of DPSO for covering array generation. Experiments show that CPSO can produce better results using the guidelines for parameter settings, and that DPSO can generate smaller covering arrays than CPSO and other existing evolutionary algorithms. DPSO is a promising improvement on PSO for covering array generation. Index Terms--Combinatorial testing (CT), covering array generation, particle swarm optimization (PSO).

I. I NTRODUCTION S SOFTWARE functions and run-time environments become more complex, testing of modern software systems is becoming more expensive. Effective detection of

A

Manuscript received May 12, 2013; revised December 29, 2013 and May 18, 2014; accepted September 28, 2014. Date of publication October 9, 2014; date of current version July 28, 2015. This work was supported in part by the National Natural Science Foundation of China under Grant 61272079, in part by the Research Fund for the Doctoral Program of Higher Education of China under Grant 20130091110032, in part by the Science Fund for Creative Research Groups of the National Natural Science Foundation of China under Grant 61321491, in part by the Major Program of National Natural Science Foundation of China under Grant 91318301, and in part by the Australian Research Council Linkage under Grant LP100200208. (Corresponding author: Changhai Nie.) H. Wu and C. Nie are with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China (e-mail: hywu@outlook.com; changhainie@nju.edu.cn). F.-C. Kuo is with the Faculty of Information and Communication Technologies, Swinburne University of Technology, Hawthorn, VIC 3122, Australia (e-mail: dkuo@swin.edu.au). H. Leung is with the Department of Computing, Hong Kong Polytechnic University, Hong Kong (e-mail: hareton.leung@polyu.edu.hk). C. J. Colbourn is with Arizona State University, Tempe, AZ 85287-8809, USA (e-mail: colbourn@asu.edu). Digital Object Identifier 10.1109/TEVC.2014.2362532

failures at a low cost is a key issue for test case generation. Combinatorial testing (CT) is a popular testing method to detect failures triggered by various factors and their interactions [1]. By employing covering arrays as test suites, the CT method aims to sample the large combination space with few test cases to cover different interactions among a fixed number of factors. Kuhn and Reilly [2] shows that more than 70% of the failures in certain software were caused by the interactions of one or two factors, and almost all the failures could be detected by checking the interactions among six factors. Therefore, CT can be an effective method in practice. Generating a covering array with fewest tests (minimum size) is a major challenge in CT. In general, the minimum size of a covering array is unknown; hence, methods have focused on finding covering arrays that have as few tests as possible at reasonable search cost. The many methods that have been proposed can be classified into two main groups: 1) mathematical methods and 2) computational methods [1]. Mathematical (algebraic or combinatorial) methods typically exploit some known combinatorial structure. Computational methods primarily use greedy strategies or heuristic-search techniques to generate covering arrays, due to the size of the search space. Mathematical methods yield the best possible covering arrays in certain cases. For example, orthogonal arrays used in the design of experiments provide covering arrays with a number of tests that is provably minimum. However, all known mathematical methods can be applied only for restrictive sets of factors. This limitation has led to an emphasis on computational methods. Greedy algorithms have been quite effective in generating covering arrays, but their accuracy suffers from becoming trapped in local optima. In recent years, search-based software engineering (SBSE) has focused on using search-based optimization algorithms to find high-quality solutions for software engineering problems. Inspired by SBSE, many artificial intelligence-based heuristic-search techniques have been applied to software testing. For example, simulated annealing (SA) [3]­[7], genetic algorithm (GA) [8]­[10], and ant colony optimization (ACO) [9], [11], [12] have all been applied to covering array generation. These techniques can generate any types of covering arrays, and the constraint solving and prioritization techniques can be easily integrated. Their applications have been shown to be effective, producing relatively small covering arrays in many cases. Particle swarm optimization (PSO), a relatively new evolutionary algorithm,

1089-778X c 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

576

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

has also been used in this area [13]­[16]. It is easy to implement and has fast initial progress. The conventional PSO (CPSO) algorithm was originally designed to find optimal or near optimal solutions in a continuous space. Nevertheless, many discrete PSO (DPSO) algorithms and frameworks have been developed to solve discrete problems [17]­[22]. For covering array generation, current discrete methods [13]­[16] simply round the particle's position to an integer while keeping the velocity as a real number. They suffer from two main shortcomings. First, the performance of PSO is significantly impacted by its parameter settings. In [23], effects of the general parameter selection and initial population of PSO have been analyzed, but no guidelines on parameter settings have been reported for covering array generation. Hence, a clear understanding of how to set these execution parameters is needed. Second, simple rounding fractional positions to integers introduces a substantial source of errors in the search. Instead, a specialized DPSO version is needed. Because current PSO methods show promise for generating covering arrays, these two main shortcomings should be addressed. In this paper, we adapt set-based PSO (S-PSO) [18] to generate covering arrays. S-PSO utilizes set and probability theories to develop a new representation scheme for combinatorial optimization problems as well as refining the related evolution operators. In our adaptation of S-PSO, two auxiliary strategies are proposed to enhance performance, and a novel DPSO algorithm is thus proposed. DPSO has the same conceptual basis and exhibits similar search behavior to CPSO, with parameters playing similar roles. Then, we explore the optimal parameter settings for both CPSO and DPSO to improve their performance, and identify recommended settings. Furthermore, because many CPSO variants [24]­[27] can be easily extended to discrete versions based on our DPSO, the performance of these discrete versions is also compared with their original ones. Finally, we compare CPSO and DPSO with existing GA and ACO [9], [11] algorithms to generate covering arrays. The main contributions of this paper are as follows. 1) Based on the set-based representation, we design a version of S-PSO [18] for covering array generation. 2) We propose two auxiliary strategies, particle reinitialization, and additional evaluation of gbest, to enhance the performance of PSO. A novel DPSO for covering array generation is proposed. 3) We design experiments to explore the optimal parameter settings for CPSO and DPSO for covering array generation. 4) We implement original and discrete versions of four representative PSO variants (TVAC [24], CLPSO [25], APSO [26], and DMS-PSO [27]) to compare their efficacy for covering array generation. The rest of this paper is organized as follows. Section II gives background on CT, covering array generation, and the CPSO algorithm. Section III summarizes related work. Section IV presents our DPSO algorithm, including the representation scheme, related operators, and two auxiliary strategies. Section V evaluates the performance of CPSO and

TABLE I E-C OMMERCE S OFTWARE S YSTEM

DPSO, and explores optimal parameter settings. Section VI gives a comparison among CPSO, DPSO, and original and discrete variants. Section VII compares CPSO and DPSO with GA and ACO. Section VIII concludes this paper and outlines future work. II. BACKGROUND A. CT Suppose that the behavior of the software under test (SUT) is controlled by n independent factors, which may represent configuration parameters, internal or external events, user inputs, and the like. The ith factor has i discrete levels, chosen from the finite set Vi of size i . An n-tuple (x1 , x2 , . . . , xn ) forms a test case, where xi  Vi for 1  i  n. Consider a simple e-commerce software system [15]. This system consists of five different components. Each of these five components can be regarded as a factor, and its configurations can be regarded as different levels. Table I shows these five factors and their corresponding levels. In this example, n = 5, 1 = 2 = 3 = 2, 4 = 5 = 3. System failures are often triggered by interactions among some factors, which can be represented by the combinations of factor levels. In order to detect these failures, their combinations should be covered at least once by the test suite. A t-way schema can be used to represent them. Definition 1 (t-way schema): The n-tuple (-, y1 , . . . , yt , . . . ) is a t-way schema (t > 0) when some t factors have fixed levels and the others can take any valid levels, represented as "-." For example, suppose that when factor Payment Server takes the level Master and factor Web Server takes the level Apache, a system failure occurs. To detect this failure, the 2-way schema (Master, ­, Apache, ­, ­) must be covered at least once by the test suite. To simplify later discussion, we use the index in the level set of each factor to present a schema. For example, (0, ­, 1, ­, ­) is used to represent (Master, ­, Apache, ­, ­). Exhaustive testing covers all n-way schemas of the system; in our example, it uses 2 × 2 × 2 × 3 × 3 = 72 test cases. Testing cost becomes prohibitive as the numbers of factors and levels increase. Moreover, because only interactions among few factors are likely to trigger failures [2], testing high-way schemas can lead to many uninformative test cases. At the other extreme, if we only guarantee to cover each 1-way schema once, only three test cases are needed (a single test case can cover five 1-way schemas at most). But we may fail to detect some interaction triggered failures involving two factors.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

577

TABLE II C OVERING A RRAY CA(9; 2, 23 32 )

TABLE III A DDING T HREE T EST C ASES TO C ONSTRUCT VCA(12; 2, 23 32 , CA(3, 22 31 ))

Instead, CT covers all t-way schemas. Such a test suite is a t-way covering array, with t being the covering strength. The value of t determines the depth of coverage. It is a key setting of CT, and should be decided by the testers. We give a precise definition. Definition 2 (Covering Array): If an N ×n array, where N is the number of test cases, has the following properties: 1) each column i (1  i  n) contains only elements from the set Vi with i = |Vi | and 2) the rows of each N × t sub array cover all |Vk1 | × |Vk2 | × . . . × |Vkt | combinations of the t columns at least once, where t  n and 1  k1 < . . . < kt  n, then it is a t-way covering array, denoted by CA(N ; t, n, ( 1 , 2 , . . . , n )). When 1 = 2 = . . . = n = , it is denoted by CA(N ; t, n, ). Reference [2] demonstrated that more than 70% failures can be detected by a 2-way covering array, and almost all failures can be detected by a 6-way covering array. Hence, using CT, we can detect many failures of the system by applying relatively low strength covering arrays. In other words, CT can greatly reduce the size of test suite while maintaining high fault detection ability. In the example of Table I, if we only consider the interactions between any two factors, only nine test cases are required to construct a 2-way covering array instead of 72 for exhaustive testing. Table II shows a covering array, where each row represents one test case. This table covers all possible 2-way schemas for any two factors corresponding to the columns. For example, consider factor Payment Server and User Browser, all 2 × 3 = 6 schemas, (Master, ­, ­, Firefox, ­) (Master, ­, ­, Explorer, ­), (Master, ­, ­, Chrome, ­), (VISA, ­, ­, Firefox, ­), (VISA, ­, ­, Explorer, ­), (VISA, ­, ­, Chrome, ­), can be found in the table. For convenience, if several groups of gi factors (gi < n) have g the same number of levels ak , ak i can be used to represent these factors and their levels. Thus, the covering array can g g g gi = n, be denoted by CA(N ; t, a11 , a22 , . . . , ak k ) where n or CA(N ; t, a ) when g1 = n and a1 = a. For example, the covering array in Table II is a CA(9; 2, 23 32 ). In many software systems, the impacts of the interactions among factors are not uniform. Some interactions may be more prone to trigger system failures, while other may have little or no impact on the system. To effectively detect these different interactions, variable strength (VS) covering arrays can be applied. This can offer different covering strengths

to different groups of factors, and can therefore provide a practical approach to test real applications. Definition 3 (VS Covering Array): A VS covering array, m 1 denoted by VCA(N ; t, a11 . . . akk , CA1 (t1 , bm . . . bp p ), . . . , 1 nq 1 CAj (tj , cn 1 . . . cq )), is an N × n covering array of covering strength t containing one or more sub covering arrays, namely CA1 , . . . , CAj , each of which has covering strength t1 , . . . , tj all larger than t. Consider the e-commerce system shown in Table I. If the interactions of three factors, Payment Server, Web Server, and Business Database, have a higher probability to trigger system failures, then a VS covering array can be constructed. As in Table II, only three more test cases (Table III) are needed to construct the VCA(12; 2, 23 32 , CA(3, 22 31 )). With these 12 test cases, not only are all 2-way schemas of all five factors covered, but also all 3-way schemas of these three factors (Payment Server, Web Server, and Business Database) are covered. B. Covering Array Generation Covering arrays are used as test suites in CT. Covering array generation is the process of test suite construction. It is the most active area in CT with more than 50% of research papers focusing on this field [1]. Due to limited testing resources, all aim to construct a minimal covering array while still maintaining full coverage of combinations. Computational methods have been used widely for covering array generation because they can be applied to any systems. In general, these methods generate all possible combinations first. Then they generate test cases to cover these combinations one-by-one. One-testat-a-time is the most widely used strategy among evolutionary algorithms to generate a covering array. The one-test-at-a-time strategy was popularized by AETG [28] and was further used by Bryce and Colbourn [29]. This strategy takes the model of SUT(n, 1 , . . . , n ) where n is the number of factors and i is the number of valid levels of factor i, and the covering strength t as input. At first, an empty test suite TS and a set S of t-way schemas to be covered are initialized. In each iteration, a test case is generated with the highest fitness value according to some heuristic techniques. Then it is added to TS and the t-way schemas covered by it are removed. When all the t-way schemas have been covered, the final test suite TS is returned. This process is shown in Algorithm 1. In this strategy, a fitness function must be used to evaluate the quality of a candidate test case (line 6 in Algorithm 1). It is an important part of all heuristic techniques. In covering array generation, the fitness function takes the test case as the input and then outputs a fitness value representing its "goodness." It is defined as follows.

578

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

Algorithm 1 One-Test-at-a-Time Strategy 1: Input: SUT(n, 1 , . . . , n ) and covering strength t 2: Output: covering array TS 3: TS =  4: Construct S (all the t-way schemas to be covered) based on n, 1 , . . . , n and t 5: while S =  do 6: Generate a test case p with the highest fitness value according to some heuristics 7: Add p to the test suite TS 8: Remove the t-way schemas covered by p from S 9: end while 10: return TS

its corresponding position: vi,j (k + 1) =  × vi,j (k) + c1 × r1,j × (pbesti,j - xi,j (k)) (2) + c2 × r2,j × (gbestj - xi,j (k)) xi,j (k + 1) = xi,j (k) + vi,j (k + 1). (3) The best position of particle i in its history is pbesti , and gbest is the best position among all particles. The velocityupdate equation (2) captures the three basic concepts of PSO. The first is inertia, the tendency of the particle to continue in the direction it has been moving. The second is memory of the best position ever found by itself. The third is cooperation using the best position found by other particles. The parameter  is inertia weight. It controls the balance between exploration (global search state) and exploitation (local search state). Two positive real numbers c1 and c2 are acceleration constants that control the movement tendency toward the individual and global best position. Most studies set  = 0.9, and c1 = c2 = 2 to get the best balance [17], [31], [32]. In addition, r1,j and r2,j are two uniformly distributed random numbers in the range of [0.0, 1.0], used to ensure the diversity of the population. If the problem domain (the search space of particles) has bounds, a bound handling strategy is adopted to keep the particles inside the search space. Many different strategies have been proposed [33]. In the reflecting strategy, when a particle exceeds the bound of the search space in any dimension, the particle direction is reversed in this dimension to get back to the search space. For example, in case of a dimension with a range of values from 0 to 2, if a particle moves to 3, its position is reversed to 1. In addition, as the velocity can increase over time, a limit is set on velocity to prevent an infinite velocity or invalid position for the particle. Setting a maximum velocity, which determines the distance of movement from the current position to the possible target position, can reduce the likelihood of explosion of the swarm traveling distance [31]. Generally, the value of the maximum velocity is selected as i /2, where i is the range of dimension i. The pseudocode in Algorithm 2 presents the process of generating a test case by PSO. This algorithm can be invoked by line 6 of Algorithm 1 to generate a test case for t-way schemas. The n factors of the test case can be treated as an n-dimensional hyperspace. A particle pi = (x1 , x2 , . . . , xn ) can be regarded as a candidate test case. The fitness function is that of Definition 4, the number of uncovered t-way schemas in the generated test suite that are covered by particle pi . PSO employs real numbers but the valid values are integers for covering array generation, so each dimension of particle's position can be rounded to an integer while maintaining the velocity as a real number. This method is used in all prior research applying PSO to covering array generation [13]­[16]. III. R ELATED W ORK In this section, three different but related aspects are discussed. We first summarize search-based CT, especially the current applications of PSO for covering array generation. Then, we introduce prior research on discrete versions of PSO,

Definition 4 (Fitness Function): Let TS be the generated test set, and p be a test case. Then fitness(p) is the number of uncovered t-way schemas in TS that are covered by p. The fitness function can be formulated as fitness(p) = |schemat ({p}) - schemat (TS)| (1)

where schemat (TS) represents the set of all t-way schemas covered by test set TS, and | · | stands for cardinality. When t t-way schemas covered by p are not covered by TS, all Cn the fitness function reaches the maximum value fitness(p) = t. |schemat ({p})| = Cn For example, consider the 2-way covering array generation of the e-commerce system shown in Table II. Suppose that TS consists of test cases (0, 0, 0, 0, 0) and (0, 0, 1, 1, 1). The fitness function computes the fitness value for a candidate test case p = (1, 0, 1, 2, 2) as follows: because p covers ten 2-way schemas, namely schema2 ({p}) = {(1, 0, ­, ­, ­), (1, ­, 1, ­, ­,), (1, ­, ­, 2, ­), (1, ­, ­, ­, 2), (­, 0, 1, ­, ­), (­, 0, ­, 2, ­), (­, 0, ­, ­, 2), (­, ­, 1, 2, ­), (­, ­, 1, ­, 2), (­, ­, ­, 2, 2)} and TS only covers (­, 0, 1, ­, ­) in p, the function returns fitness(p) = 9. C. PSO PSO is a swarm-based evolutionary computation technique. It was developed by Kennedy et al. [30], inspired by the social behavior of bird flocking and fish schooling. PSO utilizes a population of particles as a set of candidate solutions. Each of the particles represents a certain position in the problem hyperspace with a given velocity. A fitness function is used to evaluate the quality of each particle. Initially, particles are distributed in the hyperspace uniformly. Then each particle repeatedly updates its state according to the individual best position in its history (pbest) and the current global best position (gbest). Eventually, each particle possibly moves toward the direction of the individual optimum and global optimum, and finds an optimal or near optimal solution. Suppose that the problem domain is a D-dimensional hyperspace. Then the position and velocity of particle i can be represented by xi  RD and vi  RD respectively. CPSO uses the following equations to update a particle's velocity and position, where vi,j (k) represents the jth component of the velocity of particle i at the kth iteration, and xi,j (k) represents

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

579

Algorithm 2 Generate Test Case by PSO 1: Input: SUT(n, 1 , . . . , n ), covering strength t and the related parameters of PSO 2: Output: the best test case gbest 3: it = 0, gbest = NULL 4: for each particle pi do 5: Initialize the position xi and velocity vi randomly 6: end for 7: while it < maximum iteration do 8: for each particle pi do 9: Compute the fitness value fitness(pi ) 10: if fitness(pi ) > fitness(pbesti ) then 11: pbesti = pi 12: end if 13: if fitness(pi ) > fitness(gbest) then 14: gbest = pi 15: end if 16: end for 17: for each particle pi do 18: Update the velocity and position according to Equations 2 and 3 19: Apply maximum velocity limitation and bound handling strategy 20: end for 21: it = it + 1 22: end while 23: return gbest

applied PSO to 2-way covering array generation. They further used a test suite minimization algorithm to reduce the size of the generated covering array. Current applications of PSO for covering array generation can yield smaller covering arrays than most greedy algorithms, but they all apply the same rounding operator to the particle's position, and they lack guidelines on the parameter settings. B. Discrete Versions of PSO PSO was initially developed to solve problems in continuous space, but PSO can also be applied to some discrete optimization problems including binary or general integer variables. Many discrete versions of PSO have been proposed [17]­[22]. Chen et al. [18] classify existing algorithms into four types. 1) Swap operator-based PSO [19] uses a permutation of numbers as position and a set of swaps as velocity. 2) Space transformation-based PSO [20] uses various techniques to transform the position, defined as a vector of real numbers, to the corresponding solution in discrete space. 3) Fuzzy matrix-based PSO [21] defines the position and velocity as a fuzzy matrix, and decode it to a feasible solution. 4) Incorporating-based PSO [22] consists of hybrid methods incorporating some heuristic or problem dependent techniques. Chen et al. [18] also propose a S-PSO method based on sets with probabilities, which we later adapt to represent a particle's velocity. S-PSO uses a set-based scheme to represent the discrete search space. The velocity is defined as a set with probabilities, and the operators are all replaced by procedures defined on the set. They extend some PSO variants to discrete versions and test them on the traveling salesman problem and the multidimensional knapsack problem. They show that the discrete version of CLPSO [25] can perform better than other variants. S-PSO was found to characterize the combinatorial optimization problems very well. Gong et al. [36] then employed such a set-based representation scheme for solving vehicle routing problems to obtain a new method, S-PSO-VRPTW. C. PSO Variants The original PSO may become trapped in a local optimum. In order to improve the performance, many variants have been proposed [17], [24]­[27]. Chen et al. [37] classify these variants into four different groups. The first group of variants aims to adjust the control parameters during the evolution, for example by decreasing inertia weight  linearly from 0.9 to 0.4 over the search process. The second group employs different neighborhood topologies. The basic local version of PSO makes the particle learn from the local best position lbesti found by particle i's neighborhood instead of the global best position gbest. RPSO and VPSO are two common versions which use a ring topology and a Von Neumann topology, respectively. The third group uses hybrid strategies with other search techniques. Many types of operators, such as genetic operators

to improve CPSO for discrete problems. Finally, some PSO variants are introduced. These try to enhance PSO for conventional continuous problems by adopting strategies that can further improve covering array generation. A. PSO in Search-Based CT SBSE has grown quickly in recent years. Many problems in software engineering have been formulated into searchbased optimization problems, and heuristic techniques have been used to find solutions. Software testing is a major topic in software engineering. Many heuristic techniques have also been applied to this field, including functional testing, mutation testing, stress testing, temporal testing, CT, and regression testing. Currently, many classic heuristic techniques, such as SA [3]­[7], GA [8]­[10], and ACO [9], [11], [12] have been applied to generate uniform and VS covering arrays successfully. PSO has also been applied to software testing. Windisch et al. [34] applied PSO to structural testing, and compared its performance with GA. They showed that PSO outperformed GA for most cases in terms of effectiveness and efficiency. Ganjali [35] proposed a framework for automatic test partitioning based on PSO, observing that PSO performed better than other existing heuristic techniques. PSO has been applied to covering array generation. Ahmed and Zamli [15] proposed frameworks of PSO to generate different kinds of covering arrays, including t-way (PSTG [14]) and VS-PSTG. Then they extended PSTG to support VS covering array generation [16]. Chen et al. [18] also

580

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE IV D IFFERENT G ROUPS OF DPSO AND PSO VARIANTS

(selection, crossover, and mutation), evolutionary computation paradigms and biology inspired operators have been used. The fourth uses multiswarm techniques. Several sets of swarms optimize different components of the solution concurrently or cooperatively. In our experiments, four representative PSO variants are included, as follows. 1) Ratnaweera et al. [24] proposed a typical variant of the first group, TVAC, which uses a time varying inertia weight and acceleration constant to adjust the parameters. 2) Liang et al. [25] proposed the CLPSO method, a wellknown variant of the second group, which allows the particle to learn from other particles' individual best positions in different dimensions. 3) Zhan et al. [26] proposed an adaptive PSO (APSO) that can be seen as a variant of the third group. They developed an evolutionary state estimation to adaptively control the parameter and used an elitist learning strategy. 4) DMS-PSO [27], a variant of the fourth group, was proposed by Liang and Suganthan [27]. It is characterized by a set of swarms with small sizes and these swarms are frequently regrouped. In summary, Table IV lists the different groups of discrete versions of PSO and PSO variants. IV. DPSO In this section, a new DPSO for covering array generation is presented. We firstly illustrate the weakness of CPSO with a simple example. Then the representation scheme of a particle's velocity and the corresponding redefined operators are introduced. Finally, we give two auxiliary strategies to enhance the performance of DPSO. A. Example In CPSO, a particle's position represents a candidate test case; its velocity, a real vector, represents the movement tendency of this particle in each dimension. This scheme is meaningful in a continuous optimization problem, because an optimal solution may exist near the current best particle's position. So it is desirable to move the particle to this area for further search. This may not hold for covering array generation.

Here, we use CA(N ; 2, 34 ) as an example. Suppose that three test cases (0, 1, 0, 1), (2, 1, 1, 2), and (1, 2, 0, 0) have been generated and added to TS in Algorithm 1, and the fourth one is to be generated according to CPSO. A possible candidate particle pi may have the position (0, 0, 0, 0) and velocity (0.5, 0.6, -0.4, 0.2) with fitness value 4, and its individual best position pbesti may be (0, 0, 1, 1) with fitness value 5. The global best position gbest may be (0, 2, 2, 2) with fitness value 6. According to the update (2), if we take  = 0.9 and c1 × r1 = c2 × r2  0.65, in the next iteration, pi 's velocity may be transformed to (0.45, 1.84, 1.59, 2.13). Thus, pi moves to the new position (0, 1, 1, 1) with a limitation on the velocity (the velocity is kept in [- i /2, i /2]), or (0, 1, 1, 2) with no limitation. In both cases, pi only has fitness value 2 after updating. When this occurs, pi evolves to a worse situation, although its position is closer to the pbesti and gbest than its original one. Analyzing the fitness measurement, the main contribution to the fitness value is the combinations that the test case can cover, not the concrete "position" at which it is located. For example, test case (2, 1, 1, 2) has a larger fitness value than (0, 0, 0, 0) because it covers six new schemas [(2, 1, ­, ­), (2, ­, 1, ­), (2, ­, ­, 2) etc.], not because of its relative distance to other particles. B. DPSO To overcome this weakness, the movement of particles should be modified. Inspired by the set-based representation scheme of velocity, DPSO is designed as a version of S-PSO [18] to make the particle learn from the individual and global best more effectively when generating covering arrays. Unlike S-PSO, the element of the velocity set in DPSO is designed for covering array generation, and DPSO does not classify the velocity set into different dimensions to avoid the inconsistency of different dimensions when updating velocities in S-PSO. In DPSO, a particle's position represents a candidate test case, while its velocity is changed to a set of t-way schemas with probabilities. Other than the velocity's representation and the newly defined operators, the evolution procedure of DPSO is the same as CPSO (Algorithm 2). Definition 5 (Velocity): The velocity is a set of pairs (s, p(s)), where s is a possible t-way schema of the covering array and p(s) is a real number between 0 and 1 representing the probability of the selection of schema s to update the current position. In the initialization of the swarm, the particle's position is t possible different schemas are randomly assigned. Then Cn selected randomly and each of them is assigned a random t pairs form the initial velocprobability p(s)  (0, 1). These Cn ity set of this particle; the size of this set changes dynamically during the evaluation. We consider the same example CA(N ; 2, 34 ). In DPSO, when particle pi is initialized, its position xi (k) may be (0, 0, 0, 0) representing a candidate test case as before, and its velocity vi (k) may be such a set {((1, 1, ­, ­), 0.7), ((0, ­, 0, ­), 0.3), ((­, 0, ­, 1), 0.8), ((0, ­, ­, 2), 0.9),

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

581

(a)

(b)

(c)

(d)

(e)

Fig. 1. Example of pi 's velocity updating. (a) 0.9 × vi (k). (b) 2 × r1 × (pbesti - xi (k)). (c) 2 × r2 × (gbest - xi (k)). (d) vi (k + 1). (e) Final vi (k + 1) where pro1 = 0.5.
2 = 6 ((­, 2, 1, ­), 0.5), ((­, ­, 0, 1), 0.2)} which contains C4 pairs. DPSO follows the conventional evolution procedure (Algorithm 2) that uses (2) and (3) to update the velocity and position of a particle. To adapt the new scheme for velocity in Definition 5, the related operators in these equations must be redefined. 1) Coefficient × Velocity: The coefficient is a real number which may be a parameter or a random number. It modifies all the probabilities in the velocity. Definition 6 (Coefficient × Velocity): Let a be a nonnegative real number and v be a velocity, a × v = {(s, p(s) × a)|(s, p(s))  v}. (If p(s) × a > 1, p(s) × a = 1.) For example, Fig. 1(a) shows the result for  × vi (k) where  = 0.9. 2) Position­Position: The difference of two positions gives the direction on which a particle moves. The results of the minus operator is a set of (s, p(s)) pairs, as velocity. Definition 7 (Position­Position): Let x1 and x2 be two positions. Then x1 - x2 = {(s, 0.5)|s is a schema that exists in x1 but not in x2 }. In the newly generated schema, probability p(s) for s is set to 0.5 so that the acceleration constants take similar values in both CPSO and DPSO. As in (2), the result of position­position is multiplied by ci × ri . In CPSO, ci is often set to 2 and ri is a random number between 0 and 1 (recall Section II-C). In DPSO, we want the value of final probability to have a range between 0 and 1 after multiplying by ci . Setting p(s) to 0.5 puts the result in [0, 1] when ci = 2. For example, suppose that xi (k) = (0, 0, 0, 0), pbesti = (0, 0, 1, 1), and gbest = (0, 2, 2, 2) as before. We can get pbesti - xi (k) = {((0, ­, 1, ­, ­), 0.5), ((0, ­, ­, 1), 0.5), ((­, 0, 1, ­), 0.5), ((­, 0, ­, 1), 0.5), ((­, ­, 1, 1), 0.5)}. Fig. 1(b) and (c) shows the results for 2 × r1 × (pbesti - xi (k)) and 2 × r2 × (gbest - xi (k)) respectively. 3) Velocity + Velocity: The addition of velocities gives a particle's movement path. The plus operator results in the union of two velocities. Definition 8 (Velocity + Velocity): Let v1 and v2 be two velocities. Then v1 + v2 = {(s, p(s))| if (s, p1 (s))  v1 and (s, p2 (s))  v2 , p(s) = p1 (s) + p2 (s); if (s, pi (s))  v1 and (s, pi (s))  / v2 or (s, pi (s))  v2 and (s, pi (s))  / v1 , p(s) = pi (s)}. (If p1 (s) + p2 (s) > 1, p1 (s) + p2 (s) = 1.) For example, Fig. 1(d) shows the results for pi 's new velocity vi (k + 1) after updating velocity.

Algorithm 3 Position Updating 1: Input: position xi (k), velocity vi (k + 1), pro2 and pro3 2: Output: new position xi (k + 1) 3: xi (k + 1) = (­, ­, . . . , ­) 4: Sort vi (k + 1) in descending order of p(s) 5: for each pair (si ,p(si )) in vi (k + 1) do 6: Generate a random number   [0, 1] 7: if  < p(si ) then 8: for each fixed level in si do 9: Generate a random number   [0, 1] 10: if  < pro2 and the corresponding factor of has not been fixed in xi (k + 1) then 11: Update xi (k + 1) with 12: end if 13: end for 14: end if 15: end for 16: if xi (k + 1) has unfixed factors then 17: Fill these factors by the same levels of previous position xi (k) 18: end if 19: Generate a random number   [0, 1] 20: if  < pro3 then 21: randomly change the level of one factor of xi (k + 1) 22: end if 23: return xi (k + 1)

In the velocity-updating phase of DPSO (2), we introduce a new parameter pro1 to control the size of the final velocity set. If p(si ) < pro1 , the pair (si , p(si )) is removed from the final velocity. For example, if we set pro1 = 0.5, vi (k + 1) is removed as shown in Fig. 1(e). Here, the velocity has been sorted in descending order of p(s), where if several p(s) have the same value, they are in an arbitrary order. If vi (k + 1) becomes empty, it stays empty until new pairs are added to it. As long as the velocity is empty, the particle's position is not updated and no better solutions can be found from this particle. In Section IV-C1, we discuss how to reinitialize this particle. 4) Position + Velocity: Position plus velocity is the position updating phase. Algorithm 3 gives the pseudo code of this procedure. Here, two new parameters, pro2 and pro3 , numbers in the range [0, 1], are introduced. pro2 is used to determine

582

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

the probability of selecting each fixed level in schema s and pro3 is a mutation probability to make the particle mutate randomly. An example helps to describe this procedure. We already have pi 's current position xi (k) = (0, 0, 0, 0), and its updated velocity vi (k + 1) in descending order of p(s) as shown in Fig. 1(e). We also assume that pro2 and pro3 are both set to 0.5. Each schema si here is selected to update the position with probability p(si ). For the first pair ((0, ­, ­, 2), 1.0), suppose that the random number  satisfies  < 1.0. Then, for each fixed level of this pair, namely level 0 of the first factor and level 2 of the fourth factor, its corresponding factor has not been fixed in xi (k + 1). Suppose that we have the first  < 0.5 but the second  > 0.5, the first factor will be selected to update the position and the second factor will not. So the new position becomes (0, ­, ­, ­). For the second pair, we regenerate the random number  , and compare it with the probability 0.9. If  < 0.9, the second pair is selected. If we generate  < 0.5 in two rounds, the new position becomes (0, 2, 2, ­). Accordingly, if the third pair is selected, and its second factor's level 0 is chosen to update, it does not change position because this factor has been set to a fixed level 2. This procedure is repeated until all factors in the new position xi (k + 1) are set to fixed levels. If all pairs in velocity have been considered, unfixed factors of xi (k + 1) are filled by the same levels of previous position xi (k). For example, after finishing the For loop in line 15, if the fourth factor of xi (k + 1) has not been given any level, the fourth factor of xi (k) is used to update it. Then xi (k + 1) becomes (0, 2, 2, 0). C. Auxiliary Strategies Auxiliary strategies can be introduced to improve the performance of PSO. Two of them are added to DPSO: 1) particle reinitialization to make the search more effective and 2) additional evaluation of gbest to improve the selection of the global best test case. 1) Particle Reinitialization: The PSO algorithm starts with a random distribution of particles, which finally converge. Then the best position that has been found is returned. The swarm may jump out of a local optimum, but this can not be guaranteed because CPSO lacks specific strategies for this. When applying PSO for covering array generation, increasing the number of iterations does not improve the ability to escape a local optimum. Hence, particle reinitialization, a widely used method, is employed to help DPSO to jump out of the local optimum. The main issue with particle reinitialization is when to reinitialize the particle. We may select a threshold, so that the reinitialization is done when the number of iterations exceeds the threshold. In DPSO, a better method can be applied. Using the new representation for velocity, when the current particle pi 's position equals its individual best position pbesti and global best position gbest, the size (norm) of pi 's velocity reduces gradually, because no pairs are generated from (pbesti - xi ) and (gbest - xi ), and the original pairs in velocity are removed gradually under the influence of  × v (reduce the p(s) of original pairs) and parameter pro1 . After a few fluctuations around gbest, the particle may stay at gbest, and

TABLE V T WO D IFFERENT C ONSTRUCTIONS OF CA(N ; 2, 34 )

its size (norm) of velocity is zero. We can use this scenario to trigger reinitialization of the particle. When the reinitialization is done, each dimension of a particle's position is randomly assigned a valid value, and its velocity is regenerated as in the initialization of the swarm. 2) Additional Evaluation of gbest: Current PSO methods to generate covering arrays generally employ the fitness function in Definition 4. This fitness function only focuses on the current candidate, and does not consider the partial test suite TS. Consider the generation of CA(N ; 2, 34 ). Table V shows two different construction processes. Both constructions generate (0, 0, 0, 0) as the first test case. Then they choose different test cases, but each of the first three reaches the largest number of 2 = 6. The difference between newly covered combinations, C4 these two constructions emerges when generating the fourth test case. In Construction 1, because the combinations with the same level between any two factors have all been covered, we cannot find a new test case that can still cover six combinations. However, in Construction 2, such a new test case can be found, (1, 0, 1, 2). Because the minimum size of CA(N ; 2, 34 ) is 9, each test case is required to cover six new combinations. Thus, Construction 1 cannot generate the minimum test suite, but Construction 2 can. In general, there may exist multiple test cases with the same highest fitness value, which make them equally qualified to be gbest in Algorithm 2. Instead of arbitrarily selecting one as gbest, it is better to apply additional distance metric to select one among them. As shown in Table V, if the new test case is similar to the existing tests [as (0, 1, 1, 1) is closer to (0, 0, 0, 0) than (1, 1, 1, 1)], there may be a better chance to find test cases with larger fitness subsequently. In order to measure the "similarity" between a test case t and an existing test suite TS, we use the average Hamming distance. The Hamming distance d12 indicates the number of factors that have different levels between two test cases t1 and t2 . Hence, the similarity between t and TS can be defined by the average Hamming distance H (t, TS) = 1 |TS| dtk .
kTS

(4)

The new gbest selection strategy selects the best candidate gbest based on two metrics. A test case that has the minimum average Hamming distance (i.e., the maximum similarity) from the set of test cases that all have the same highest fitness value is selected as gbest in Algorithm 2. For example, when we generate the second test case for CA(N ; 2, 34 ) in Table V, although (1, 1, 1, 1) and (0, 1, 1, 1) both have the highest fitness value 6, (0, 1, 1, 1) is selected as gbest

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

583

TABLE VI C OMPARING P ERFORMANCE B ETWEEN CPSO AND DPSO

because its average Hamming distance, 3, is smaller than that of (1, 1, 1, 1), 4. We expect that this additional evaluation can enhance the probability of generating a smaller test suite. In addition, because we still want to make the particle follow the conventional search behavior on the individual best direction, this additional distance metric will not be used in updating the pbest of DPSO. V. E VALUATION AND PARAMETER T UNING In this section, we first evaluate the effectiveness of DPSO in some representative cases, and compare the results against CPSO. Then the optimal parameter settings for both CPSO and DPSO are explored. The goal of evaluation and parameter tuning is to make the size of generated covering array as small as possible. Five representative cases of covering arrays, listed below, are selected for our experiments CA1 (N ; 2, 610 ) CA2 (N ; 3, 57 ) CA3 (N ; 4, 39 ) CA4 (N ; 2, 43 53 62 ) VCA(N ; 2, 315 , CA(3, 34 )). We consider four independent parameters, iteration number (iter), population size (size), inertia weight (), and acceleration constant (c), which play similar roles in both CPSO and DPSO, and three new parameters for DPSO, pro1 , pro2 , and pro3 . We carry out a base choice experiment to study the impact of various values of these parameters on CPSO and DPSO's performance and find the recommended settings for them. First, a base setting is chosen as the basic configuration. Then the value of each parameter is changed to create a series of configurations, while leaving the other parameters unchanged. Initially, we set iter = 50, size = 20,  = 0.9, c = 1.3, and pro1 = pro2 = pro3 = 0.5 as a basic configuration based on the previous studies [13], [16] and our empirical experience. To obtain statistically significant results, the generation of each case of covering array is executed 30 times. A. Evaluation of DPSO We compare the performance between CPSO and DPSO with the basic configuration. Five classes of covering arrays are generated by these two algorithms. The sizes obtained and average execution time per test case are shown as CPSO1 and DPSO in Table VI. The best and mean array sizes of DPSO are all better than those of CPSO1 . The new discrete representation scheme and auxiliary strategies improve PSO for covering array generation. DPSO can produce smaller covering arrays than CPSO with the basic configuration. However, DPSO spends more time

during the evolution, because its new operators for updating are more intricate than the conventional ones. DPSO needs to deal with many elements of the velocity set, whereas the conventional scheme only needs simple arithmetic operations. To compare the performance between CPSO and DPSO given the same execution time, for each case we let the execution time per test case for CPSO equal to that for DPSO, so that CPSO can spend more time in searching. We refer to this version of CPSO as CPSO2 . In addition, a t-test between CPSO2 and DPSO is conducted and the corresponding p-value is presented. A p-value smaller than 0.05 indicates that the performance of these two algorithms is statistically different with 95% confidence. From CPSO1 , CPSO2 , and DPSO in Table VI, the performance of CPSO is improved with more search time. DPSO, which must use fewer iterations, still works better than CPSO. The effectiveness of DPSO comes from the essential improvement of its new representation scheme for velocity and the auxiliary strategies. The results of the t-test demonstrate the significance of these differences. Therefore, we can conclude that DPSO performs better than CPSO with fewer iterations for covering array generation. B. Parameter Tuning In the base choice experiment, because the sizes of the various covering arrays differ, the mean size of each case (si ) obtained is normalized using si = si - smin . smax - smin

This normalization enables the graphical representation of different cases on a common scale. Because some parameters may not significantly impact the performance, we use ANOVA (significance level = 0.05) to test whether there exist significant differences among the mean results obtained by different parameter settings. When changing the parameter settings have no significant impact on the generation results, these results will be presented as dotted lines in the corresponding figures. For example, CA4 is presented as a dotted line in Fig. 2(b). It means that the iteration number does not significantly impact the generation of this case of covering array, and so this case will not be further considered when identifying the optimal settings. 1) Iteration Number (iter): Iteration number determines the number of updates. PSO typically requires thousands of iterations to evolve. Nevertheless, only a few iterations are required to generate covering array according to [14]­[16]. So we

584

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

(a)
Fig. 2. Comparing array sizes under different iteration numbers. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 3. Comparing array sizes under different population sizes. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 4.

(b)

Comparing array sizes under different settings of population size and iteration number. (a) CPSO. (b) DPSO.

change its value from 50, the base setting, to 1950 incrementally. Much larger values are not used because the execution time may increase markedly without a commensurate increase in the quality of the results. Fig. 2 shows the results for different choices of iteration numbers, where each line represents a covering array. Performance is improved with increasing iterations for both CPSO and DPSO. A small number of iterations may not be appropriate due to insufficient searching. Because the optimal settings are different among different cases, and several settings can be chosen to generate the minimum covering arrays, it is open to debate which setting is the "best" one. Given time constraints, for a population size of 20, a good setting of iteration number could be approximately 1000 for both CPSO and DPSO. 2) Population Size (Size): Population size determines the initial search space. Generally a small value, e.g., 20, is satisfactory for most cases. A large population size may bring a higher diversity and find a better solution, but it also increases the evolving time. For the same reason as before, we change the value of population size from 10 to 100.

Fig. 3 shows the results for different choices of population size. There is no doubt that the mean array size obtained decreases as population size increases. A large population size can have more chances to generate smaller covering arrays, but its execution time can become prohibitive. In addition, because iteration number and population size together determine the search effort of PSO, we explore the combinations between these two parameters. We let iter × size be a constant 20 000, and generate each case under different settings of these two parameters. Fig. 4 shows the results, where PSO prefers a relatively larger population size. In CPSO, ten particles with 2000 iterations is the worst setting, and most cases produce good results with 60 or 80 particles. In DPSO, the best choice of population size is still 80. In both CPSO and DPSO, the largest population size 100 cannot produce as good results as that of 80 due to having fewer iterations. So achieving smallest covering arrays requires a good balance between these two parameters, and a moderately large population size is necessary for both CPSO and DPSO. Thus, we can set iteration number to 250, and population size to 80, as the recommended settings for both CPSO and DPSO.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

585

(a)
Fig. 5. Comparing array sizes under different inertia weights. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 6. Comparing array sizes under different acceleration constants. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 7.

(b)

(c)

Comparing array sizes under different settings of (a) pro1 , (b) pro2 , and (c) pro3 . TABLE VII R ECOMMENDED PARAMETER S ETTINGS

3) Inertia Weight (): Inertia weight determines the tendency of the particle to continue in the same direction. A small value help the particle move primarily toward the best position, while a large one is helpful to continue its previous movement. A linearly decreasing value is also used as it can make the swarm gradually narrow the search space. Here, we investigate both fixed values and a linearly decreasing value from 0.9 to 0.4 over the whole evolution (presented as "dec"). Fig. 5 shows the results for different choices of inertia weight. In CPSO, most of the smallest covering arrays are generated by large fixed inertia weights. The decreasing value does not perform as well as the large values, such as 0.9 for CA1 , CA2 , and CA4 . So 0.9 can be the recommended choice for CPSO. In DPSO, 1.1 is the worst choice because the particle may update its position based on its own velocity and thus fail to learn from individual and global best positions. The decreasing value can perform reasonably well, but a fixed value 0.5 may be a better choice in that it keeps the effort of global search moderate. Thus, we can recommend the fixed inertia weight of 0.9 for CPSO, and 0.5 for DPSO. 4) Acceleration Constant (c): Acceleration Constants c1 and c2 control the learning from pbest and gbest, respectively. Generally, they are set to the same value to balance

the influence of these two positions. Setting c1 and c2 to a large value may make the particle more likely attracted to the best position ever found, while a small one may make the search far from the current optimal region. In this paper, we set c1 = c2 = c, and vary c from 0.1 to 2.9. Fig. 6 shows the results for different choices of acceleration constant. Unlike other parameters, there is a consistent trend in all five cases. In CPSO, the values larger than 0.5 all produce good results. In DPSO, 1.3 can definitely be regarded as the optimal value. Thus, we set 1.3 as the recommended value of acceleration constant for both CPSO and DPSO. 5) pro1 , pro2 , and pro3 : These three parameters are new to DPSO. Parameter pro1 determines the size of the final velocity. As those pairs whose probabilities are smaller than pro1 are removed from the final velocity set, a small value may keep more pairs in the velocity for the next evolution, but it

586

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE VIII S IZES (N ) OF C OVERING A RRAYS FOR n FACTORS , E ACH H AVING T HREE L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, n, 3)

also requires more computation time. Parameter pro2 determines the probability of selecting each fixed level from each schema when updating positions. A larger value may lead to a quick construction of new position, but it may also lead to fast convergence toward a local optimum. Parameter pro3 determines the mutation probability when updating positions. A larger value may enhance the randomness, but it also lowers the convergence speed. In this paper, values from 0.1 to 1.0 for these three parameters are investigated. Fig. 7 shows the results. For pro1 , a large value is not appropriate because it removes nearly all pairs from the final velocity set. A medium value 0.5, which appears to lead to the best result, may be the best choice. For pro2 , the smallest value 0.1 yields the best results. Slower construction of the new position may slow the convergence toward a local optimum, but it also takes longer. So, in order to balance performance and execution time, we take 0.3 as the recommended value for pro2 . For pro3 , a larger value may be a good choice. The frequent mutation of new position may bring better results, but also takes longer to converge. So, we take 0.7 as the recommended value for pro3 . In summary, the recommended parameter settings for PSO for covering array generation are different from previously suggested ones [13], [16]. Some parameters may significantly impact the performance in some cases, and parameter tuning is necessary to enhance heuristic techniques for particular applications. Naturally, the optimal settings vary for different cases of covering arrays. There may not exist a common setting that can always lead to the best results. Because it is

impractical to tune parameters for each case in real life, we suggest two general settings for CPSO and DPSO, as shown in Table VII, which can typically lead to better performance within reasonable execution time. VI. C OMPARING A MONG PSO S In this section, we compare the best reported array sizes generated by PSO in [16] with our findings for CPSO, DPSO, and four representative variants. Because the research in [16] demonstrated that their generation results typically outperform greedy algorithms, in this paper, we do not compare CPSO and DPSO with greedy algorithms. We implement both the original and discrete versions of four variants (TVAC [24], CLPSO [25], APSO [26], and DMS-PSO [27]) to generate covering arrays. Their discrete versions are extended based on new representation scheme of velocity and auxiliary strategies. We name them D-TVAC, DCLPSO, D-APSO, and D-DMS-PSO, respectively. In CLPSO, a particle can learn from different particles' pbest in each dimension whereas we do not distinguish dimensions strictly in DPSO. So in D-CLPSO, a particle can fully learn from different particles' pbest in all dimensions. That may weaken the search ability of CLPSO. For the other three variants, they can be directly extended based on DPSO. All algorithms are compared using the same number of fitness function evaluations. CPSO and DPSO use the recommended settings shown in Table VII. For the variants, iter, size and pro1 , pro2 and pro3 for discrete versions are set to

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

587

TABLE IX S IZES (N ) OF C OVERING A RRAYS FOR S EVEN FACTORS , E ACH H AVING

L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, 7, )

their recommended values.  and c are also set to recommended values unless they are adaptively adjusted during the evolution, in which case their range is set to [0.4, 0.9] and [0.8, 1.8], respectively. The new control parameters for these variants follow their suggested settings. Tables VIII­XIII give the results. Because of the execution time, we only consider covering strengths from 2 to 4, and the generation of each covering array is repeated 30 times. A t-test (significance level = 0.05) is also conducted to test whether there exists a significant difference between the mean sizes produced by the two algorithms. In the first three columns, we report the best and mean array sizes obtained from previous results, CPSO and DPSO, where boldface numbers indicate that the difference between CPSO and DPSO is significant based on the t-test. In the last four columns, we report the mean array sizes from the original and discrete versions of each PSO variant (presented as meanc and meand respectively), where boldface numbers indicate that the difference between meanc and meand of each variant is significant. A. Uniform Covering Arrays Tables VIII­X present the results for uniform covering arrays. We extend the cases considered in [16], where "­" indicates the not available cases. In Tables VIII, we report array sizes for n factors, each having three levels. In Tables IX and X, we report array sizes for 7 and 10 factors, each having levels. Their covering strengths all range from 2 to 4. Typically, CPSO can produce smaller sizes than those reported in [16], demonstrating the effectiveness of parameter tuning. Furthermore, DPSO can produce the smallest best and mean sizes in almost all cases, and usually the performance of DPSO is significantly better than CPSO. When the covering strength t = 2 is adopted (Table VIII), DPSO does

not generally outperform CPSO. But when covering strength increases, DPSO performs better. Sometimes the mean sizes for DPSO are smaller than the best sizes for CPSO. Because generating a covering array with higher covering strength is more complicated, DPSO may be more appropriate for generating such covering arrays. In Tables IX and X, we also find that DPSO is superior for covering arrays with high covering strength. Overall, these results show the effectiveness of DPSO for uniform covering array generation. Surprisingly, DPSO does not beat previous results for CPSO when covering arrays have two levels for each factor (Table IX). This appears to be a weakness of DPSO. Although DPSO generates smaller covering arrays than previous results and CPSO, other techniques may still yield better results than DPSO (some best known sizes can be found in [38]). B. VS Covering Array Tables XI­XIII give the results for VS covering arrays. Based on CA(N ; 2, 315 ), CA(N ; 3, 315 ), and CA(N ; 2, 42 52 63 ), some different cases of sub covering arrays conducted in [16] are examined. Their covering strengths are at most 4. Generally, we can draw similar conclusions as for uniform covering arrays. CPSO with the suggested parameter setting can produce better results than reported sizes in some cases. DPSO also usually beats them on the best and mean sizes. In Table XI, often the difference between CPSO and DPSO is not significant. In part this is because for the CA(3, 33 ), CA(3, 33 )2 , CA(3, 33 )3 , and CA(4, 34 ), the same best results are provably the minimum (e.g., the minimum size of the CA(3, 33 ) is 3 × 3 × 3 = 27). For the other cases, although sometimes the difference is not significant, DPSO can still generate smaller covering arrays. DPSO remains a good choice for generating VS covering arrays. In Tables XII and XIII, similar results can be found.

588

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE X S IZES (N ) OF C OVERING A RRAYS FOR T EN FACTORS , E ACH H AVING

L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, 10, )

TABLE XI S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 2, 315 , CA)

TABLE XII S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 3, 315 , CA)

For both uniform and variable cases of covering arrays, parameter tuning can enhance CPSO to generate smaller covering arrays than previous work. DPSO can produce the smallest covering arrays in nearly all cases, and in general the difference of performance between CPSO and DPSO is significant. DPSO is an effective discrete version of PSO for covering array generation. C. PSO Variants In order to further investigate the effectiveness of DPSO for covering array generation, we implement the original versions of four representative variants of PSO and extend them to their discrete versions based on DPSO. We apply them to generate

the same cases for covering arrays. The mean sizes obtained are shown in the last four columns in Tables VIII­XIII. We first compare the mean sizes of original PSO variants with those of CPSO and DPSO. In our experiments, TVAC's mean sizes are always larger than CPSO's. The linear adjustment of inertia weight and acceleration constant is not helpful for CPSO for covering array generation. Typically, APSO's mean sizes are also larger than CPSO's. Because APSO uses a fuzzy system to classify different evolutionary states, its ineffectiveness may result from inappropriate parameter settings. CLPSO and DMS-PSO can outperform CPSO typically. Although on occasion they achieve comparable performance with DPSO, they cannot perform as well as DPSO in most

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

589

TABLE XIII S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 2, 43 53 62 , CA)

TABLE XIV C OMPARING CPSO AND DPSO W ITH GA AND ACO

cases. That further demonstrates the effectiveness of DPSO for covering array generation. Because these four algorithms are representative PSO variants (as shown in Table IV), designing different neighborhood topologies and using multiswarm techniques may have potential to improve CPSO for covering array generation. We next compare the original and discrete versions of each variant. Except for CLPSO, the other three variants can be improved using their discrete versions, and the improvement is also significant. TVAC cannot outperform CPSO, but it is enhanced by DPSO so that D-TVAC can produce smaller mean sizes than CPSO in most cases. The linear adjustment is helpful for the discrete version. For CLPSO, only in a few cases is it improved by DPSO. Sometimes D-CLPSO even leads to worse results (see Table X), due primarily to the weakened search ability of its discrete version as explained in Section VI. For APSO, DPSO can enhance its original version, but D-APSO is still worse than DPSO. That may result from inappropriate settings as explained before. For DMS-PSO, sometimes DPSO does not enhance it (see Table XI). However, in most cases D-DMS-PSO can outperform DMS-PSO and has comparable performance with D-TVAC. The multiswarm strategy is also helpful for the discrete version.

In summary, the comparison study reveals that our suggested parameter settings are more suitable for covering array generation. DPSO is an effective discrete version of PSO. It can significantly outperform previous results and CPSO in nearly all cases for uniform and VS covering arrays. Furthermore, DPSO's representation scheme of a particle's velocity and auxiliary strategies not only enhance CPSO, but also typically enhance PSO variants. DPSO is a promising improvement on PSO for covering array generation. VII. C OMPARING DPSO W ITH GA AND ACO Because GAs and ACO [8]­[12] have also been successfully used for covering array generation, we compare CPSO and DPSO with the reported array sizes in [9] and [11]. There are no widely accepted benchmarks for the comparison of searchbased covering array generation, so we only consider these two representative and competitive works. Shiba et al. [9] applied both GA and ACO to generate uniform covering arrays (CA1 to CA8 in Table XIV), and Chen et al. [11] applied ACO to generate VS covering arrays (VCA9 to VCA12 in Table XIV). They both set algorithm parameters according to recommendations in related research fields without parameter tuning, while our CPSO and DPSO

590

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

use the recommended settings shown in Table VII. To compare these algorithms under the same number of fitness evaluations, for each case of covering array, the population size and the number of iterations of CPSO and DPSO are modified accordingly to satisfy the settings in [9] and [11]. Moreover, Shiba et al. [9] and Chen et al. [11] both applied test minimization algorithms to further reduce the size of generated covering arrays, while our CPSO and DPSO do not apply any minimization algorithms. Table XIV shows the comparison results, where boldface numbers indicate the best array sizes obtained, and "­" represents that the corresponding data is not available. The generation of each case of covering array of CPSO and DPSO is executed 30 times and the best and average results are presented. Because we do not implement GA and ACO for covering array generation, no statistical tests can be conducted here. In addition, because the platforms used for collecting the results differ, the comparison of computational time would not be informative. We nevertheless present the execution times of our CPSO and DPSO, which can serve as references for practitioners. From Table XIV, DPSO can outperform existing GA and ACO for covering array generation, despite the latter two applying test minimization algorithms. Because our DPSO is a version of PSO designed and tuned for covering array generation, it suggests that tuned versions of GAs and ACOs may be a promising area for further study. In addition, CPSO performs worse than GA and ACO in seven of 12 cases. That may result from the improvement by minimization algorithms in [9] and [11]. But for CA2 , CA3 , and CA6 , CPSO can still achieve smaller covering arrays. In summary, the results further demonstrate that DPSO is an effective discrete version of PSO for covering array generation. Further investigations of tuned versions of GA and ACO should be considered. VIII. C ONCLUSION Covering array generation is a key issue in CT. We developed a new DPSO for covering array generation, by adapting S-PSO to generate covering arrays and incorporating two auxiliary strategies to improve the performance. Parameter tuning was applied to both CPSO and DPSO to identify their best parameter settings. The original and discrete versions of four representative PSO variants were implemented and their efficacy for covering array generation was compared. The performance of DPSO was also compared with other existing evolutionary algorithms, GA and ACO. DPSO can perform better than CPSO with fewer iterations, and the performance of CPSO and DPSO is significantly impacted by their parameter settings. Different cases require different parameter settings; there may not exist a single choice that leads to the best results. After parameter tuning, we identified two recommended settings, which lead to relatively good performance for CPSO and DPSO. Indeed, CPSO with our recommended parameter settings improves on previously reported results, and our DPSO with recommended parameter settings usually beats CPSO. Typically, DPSO also significantly

enhances the performance of PSO variants. In addition, DPSO often outperforms GA and ACO to generate covering arrays. Consequently, DPSO is a promising improvement of PSO for covering array generation. Improvements on the methods here may be possible in a number of ways. One would be to investigate further evolution procedures and strategies proposed in PSO, such as hybridizing with penalty approaches to handle discrete unknowns [39], and compare the results with some exact schemes like branch and bound method. A second would be to examine one-column-at-a-time approaches or methods that construct the entire array, rather than the one-row-at-a-time approach adopted here. A third would be to incorporate DPSO with other methods, in particular with test minimization methods. R EFERENCES
[1] C. Nie and H. Leung, "A survey of combinatorial testing," ACM Comput. Surv., vol. 43, no. 2, pp. 11.1­11.29, 2011. [2] D. Kuhn and M. Reilly, "An investigation of the applicability of design of experiments to software testing," in Proc. 27th Annu. NASA Goddard/IEEE Softw. Eng. Workshop, Greenbelt, MD, USA, 2002, pp. 91­95. [3] M. Cohen, P. Gibbons, W. Mugridge, and C. Colbourn, "Constructing test suites for interaction testing," in Proc. 25th Int. Conf. Softw. Eng., Portland, OR, USA, 2003, pp. 38­48. [4] M. B. Cohen, C. J. Colbourn, and A. C. Ling, "Constructing strength three covering arrays with augmented annealing," Discrete Math., vol. 308, no. 13, pp. 2709­2722, 2008. [5] J. Torres-Jimenez and E. Rodriguez-Tello, "Simulated annealing for constructing binary covering arrays of variable strength," in Proc. Congr. Evol. Comput., Barcelona, Spain, Jul. 2010, pp. 1­8. [6] B. Garvin, M. Cohen, and M. Dwyer, "Evaluating improvements to a meta-heuristic search for constrained interaction testing," Empir. Softw. Eng., vol. 16, no. 1, pp. 61­102, 2011. [7] J. Torres-Jimenez and E. Rodriguez-Tello, "New bounds for binary covering arrays using simulated annealing," Inf. Sci., vol. 185, no. 1, pp. 137­152, 2012. [8] S. Ghazi and M. Ahmed, "Pair-wise test coverage using genetic algorithms," in Proc. Congr. Evol. Comput., vol. 2. Canberra, ACT, Australia, 2003, pp. 1420­1424. [9] T. Shiba, T. Tsuchiya, and T. Kikuno, "Using artificial life techniques to generate test cases for combinatorial testing," in Proc. 28th Annu. Int. Comput. Softw. Appl. Conf., vol. 1. Hong Kong, 2004, pp. 72­77. [10] J. McCaffrey, "An empirical study of pairwise test set generation using a genetic algorithm," in Proc. 7th Int. Conf. Inf. Technol. New Gener., Las Vegas, NV, USA, 2010, pp. 992­997. [11] X. Chen, Q. Gu, A. Li, and D. Chen, "Variable strength interaction testing with an ant colony system approach," in Proc. Asia-Pacific Softw. Eng. Conf., Penang, Malaysia, 2009, pp. 160­167. [12] X. Chen, Q. Gu, X. Zhang, and D. Chen, "Building prioritized pairwise interaction test suites with ant colony optimization," in Proc. 9th Int. Conf. Qual. Softw., Jeju-do, Korea, 2009, pp. 347­352. [13] X. Chen, Q. Gu, J. Qi, and D. Chen, "Applying particle swarm optimization to pairwise testing," in Proc. 34th Annu. Comput. Softw. Appl. Conf., Seoul, Korea, 2010, pp. 107­116. [14] B. S. Ahmed and K. Z. Zamli, "PSTG: A T-way strategy adopting particle swarm optimization," in Proc. 4th Asia Int. Conf. Math. Anal. Model. Comput. Simulat., Kota Kinabalu, Malaysia, 2010, pp. 1­5. [15] B. S. Ahmed and K. Z. Zamli, "A variable strength interaction test suites generation strategy using particle swarm optimization," J. Syst. Softw., vol. 84, no. 12, pp. 2171­2185, 2011. [16] B. S. Ahmed, K. Z. Zamli, and C. P. Lim, "Application of particle swarm optimization to uniform and variable strength covering array construction," Appl. Soft Comput., vol. 12, no. 4, pp. 1330­1347, 2012. [17] Y. del Valle, G. Venayagamoorthy, S. Mohagheghi, J.-C. Hernandez, and R. Harley, "Particle swarm optimization: Basic concepts, variants and applications in power systems," IEEE Trans. Evol. Comput., vol. 12, no. 2, pp. 171­195, Apr. 2008.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

591

[18] W.-N. Chen et al., "A novel set-based particle swarm optimization method for discrete optimization problems," IEEE Trans. Evol. Comput., vol. 14, no. 2, pp. 278­300, Apr. 2010. [19] M. Clerc, Discrete Particle Swarm Optimization (New Optimization Techniques in Engineering). New York, NY, USA: Springer, 2004. [20] W. Pang et al., "Modified particle swarm optimization based on space transformation for solving traveling salesman problem," in Proc. Int. Conf. Mach. Learn. Cybern., vol. 4. Shanghai, China, Aug. 2004, pp. 2342­2346. [21] W. Pang, K.-P. Wang, C.-G. Zhou, and L.-J. Dong, "Fuzzy discrete particle swarm optimization for solving traveling salesman problem," in Proc. 4th Int. Conf. Comput. Inf. Technol., Wuhan, China, 2004, pp. 796­800. [22] Y. Wang et al., "A novel quantum swarm evolutionary algorithm and its applications," Neurocomputing, vol. 70, nos. 4­6, pp. 633­640, 2007. [23] E. Campana, G. Fasano, and A. Pinto, "Dynamic analysis for the selection of parameters and initial population, in particle swarm optimization," J. Global Optim., vol. 48, no. 3, pp. 347­397, 2010. [24] A. Ratnaweera, S. Halgamuge, and H. Watson, "Self-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients," IEEE Trans. Evol. Comput., vol. 8, no. 3, pp. 240­255, Jun. 2004. [25] J. Liang, A. Qin, P. Suganthan, and S. Baskar, "Comprehensive learning particle swarm optimizer for global optimization of multimodal functions," IEEE Trans. Evol. Comput., vol. 10, no. 3, pp. 281­295, Jun. 2006. [26] Z.-H. Zhan, J. Zhang, Y. Li, and H.-H. Chung, "Adaptive particle swarm optimization," IEEE Trans. Syst., Man, Cybern., B, Cybern., vol. 39, no. 6, pp. 1362­1381, Dec. 2009. [27] J. Liang and P. Suganthan, "Dynamic multi-swarm particle swarm optimizer," in Proc. IEEE Swarm Intell. Symp., Pasadena, CA, USA, 2005, pp. 124­129. [28] D. Cohen, S. Dalal, M. Fredman, and G. Patton, "The AETG system: An approach to testing based on combinatorial design," IEEE Trans. Softw. Eng., vol. 23, no. 7, pp. 437­444, Jul. 1997. [29] R. C. Bryce and C. J. Colbourn, "One-test-at-a-time heuristic search for interaction test suites," in Proc. 9th Annu. Conf. Genet. Evol. Comput., London, U.K., 2007, pp. 1082­1089. [30] J. Kennedy and R. Eberhart, "Particle swarm optimization," in Proc. Int. Conf. Neural Netw., vol. 4. Perth, WA, Australia, 1995, pp. 1942­1948. [31] R. Eberhart and Y. Shi, "Particle swarm optimization: Developments, applications and resources," in Proc. Congr. Evol. Comput., vol. 1. Seoul, Korea, 2001, pp. 81­86. [32] R. Poli, J. Kennedy, and T. Blackwell, "Particle swarm optimization," Swarm Intell., vol. 1, no. 1, pp. 33­57, 2007. [33] S. Helwig, J. Branke, and S. Mostaghim, "Experimental analysis of bound handling techniques in particle swarm optimization," IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 259­271, Apr. 2013. [34] A. Windisch, S. Wappler, and J. Wegener, "Applying particle swarm optimization to software testing," in Proc. 9th Annu. Conf. Genet. Evol. Comput., London, U.K., 2007, pp. 1121­1128. [35] A. Ganjali, "A requirements-based partition testing framework using particle swarm optimization technique," M.S. thesis, Dept. Electr. Comput. Eng., Univ. Waterloo, Waterloo, ON, Canada, 2008. [36] Y.-J. Gong et al., "Optimizing the vehicle routing problem with time windows: A discrete particle swarm optimization approach," IEEE Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 42, no. 2, pp. 254­267, Mar. 2012. [37] W.-N. Chen et al., "Particle swarm optimization with an aging leader and challengers," IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 241­258, Apr. 2013. [38] C. J. Colbourn. (Apr. 2013). Covering Array Tables for t=2, 3, 4, 5, 6. [Online]. Available: http://www.public.asu.edu/ ccolbou/src/ tabby/catable.html [39] M. Corazza, G. Fasano, and R. Gusso, "Particle swarm optimization with non-smooth penalty reformulation, for a complex portfolio selection problem," Appl. Math. Comput., vol. 224, pp. 611­624, Nov. 2013.

Huayao Wu received the B.S degree from Southeast University, Nanjing, China and the M.S degree from Nanjing University, Nanjing, China, where he is currently working toward the Ph.D. degree from Nanjing University, Nanjing. His research interests include software testing, especially on combinatorial testing and search-based software testing.

Changhai Nie (M'12) received the B.S. and M.S. degrees in mathematics from Harbin Institute of Technology, Harbin, China, and the Ph.D. degree in computer science from Southeast University, Nanjing, China. He is a Professor of Software Engineering with State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing. His research interests include software analysis, testing and debugging.

Fei-Ching Kuo (M'06) received the B.Sc. (Hons.) degree in computer science and the Ph.D. degree in software engineering from Swinburne University of Technology, Hawthorn, VIC, Australia. She was a Lecturer with University of Wollongong, Wollongong, NSW, Australia. She is currently a Senior Lecturer with the Swinburne University of Technology. Her research interests include software analysis, testing, and debugging.

Hareton Leung (M'90) received the Ph.D. degree in computer science from University of Alberta, Edmonton, AB, Canada. He is an Associate Professor and a Director of the Laboratory for Software Development and Management, Department of Computing, Hong Kong Polytechnic University, Hong Kong. His research interests include software testing, project management, risk management, quality and process improvement, and software metrics.

Charles J. Colbourn received the Ph.D. degree from University of Toronto, Toronto, ON, Canada, in 1980. He is a Professor of Computer Science and Engineering with Arizona State University, Tempe, AZ, USA. He has authored the books The Combinatorics of Network Reliability (Oxford) and Triple Systems (Oxford), and also 320 refereed journal papers focussing on combinatorial designs and graphs with applications in networking, computing, and communications. Prof. Colbourn received the Euler Medal for Lifetime Research Achievement by the Institute for Combinatorics and its Applications in 2004.

ATLAS: Adaptive Topology- and Load-Aware Scheduling
Jonathan Lutz, Charles J. Colbourn, and Violet R. Syrotiuk CIDSE, Arizona State University, Tempe, AZ 85287-8809 Email: {jlutz, colbourn, syrotiuk}@asu.edu

arXiv:1305.4897v2 [cs.NI] 4 Nov 2013

Abstract--The largest strength of contention-based MAC protocols is simultaneously the largest weakness of their scheduled counterparts: the ability to adapt to changes in network conditions. For scheduling to be competitive in mobile wireless networks, continuous adaptation must be addressed. We propose ATLAS, an Adaptive Topology- and Load-Aware Scheduling protocol to address this problem. In ATLAS, each node employs a random schedule achieving its persistence, the fraction of time a node is permitted to transmit, that is computed in a topology and load dependent manner. A distributed auction (REACT) piggybacks offers and claims onto existing network traffic to compute a lexicographic max-min channel allocation. A node's persistence p is related to its allocation. Its schedule achieving p is updated where and when needed, without waiting for a frame boundary. We study how ATLAS adapts to controlled changes in topology and load. Our results show that ATLAS adapts to most network changes in less than 0.1s, with about 20% relative error, scaling with network size. We further study ATLAS in more dynamic networks showing that it keeps up with changes in topology and load sufficient for TCP to sustain multi-hop flows, a struggle in IEEE 802.11 networks. The stable performance of ATLAS supports the design of higher-layer services that inform, and are informed by, the underlying communication network. Index Terms--Wireless networks, medium access control, adaptation.

I. I NTRODUCTION Despite the well known shortcomings of IEEE 802.11 and other contention-based MAC protocols for mobile wireless networks--such as probabilistic delay guarantees, severe short-term unfairness, and poor performance at high load-- they remain the access method of choice. The primary reason is their ease in adapting to changes in network conditions, specifically to changes in topology and in load. The lack of timely adaptation is the most serious limitation facing scheduled MAC protocols. For scheduling to be competitive, continuous adaptation is required. Topology-dependent approaches to adaptation in scheduling alternate a contention phase with a scheduled phase. In the contention phase, nodes exchange topology information used to compute a conflict-free schedule that is followed in the subsequent scheduled phase (see, as examples, [5], [30]). However, changes in topology and load do not always align with the phases of the algorithm resulting in a schedule that often lags behind the network state. In contrast, the idea behind topology-transparent scheduling is to design schedules independent of the detailed network topology [3], [15]. Specifically, the schedules do not depend on the identity of a node's neighbours, but rather on how many

of them are transmitting. Even if a node's neighbours change, its schedule does not; if the number of neighbours does not exceed the designed bound then the schedule guarantees success. Though such schedules are robust to network conditions that deviate from the design parameters [27], because the schedules do not adapt, the technique remains a theoretical curiosity. In contention-based schemes, such as IEEE 802.11, a node computes implicitly when to access the channel, basing its decisions on perceived channel contention. We instead compute a node's persistence--the fraction of time it is permitted to transmit--explicitly in a way that tracks the current topology and load. To achieve this, we propose ATLAS, an Adaptive Topology- and Load-Aware Scheduling protocol. Channel allocation is a resource allocation problem where the demands correspond to transmitters, and the resources to receivers. ATLAS implements the REsource AlloCaTion computed by REACT, a distributed auction that runs continuously. REACT piggybacks offers and claims onto existing network traffic to compute the lexicographic max-min allocation to transmitters which we call the TLA allocation, emphasizing that it is both topology- and load-aware. Each node's random schedule, achieving a persistence informed by its allocation, is updated whenever a change in topology or load results in a change in allocation. While the slots of the schedule are grouped into frames, this is done only to reduce the variance in delay [6]; there is no need to wait for a frame boundary to update the schedule. Even though the random schedules may not be conflict-free, ATLAS is not contention-based; it does not select persistences or make scheduling decisions based on perceived channel contention--its decisions are based solely on topology and load. We study how ATLAS adapts to controlled changes in topology and load, measuring convergence time, relative error, and scalability. We also assess the ability of ATLAS to adapt in more dynamic network conditions. To the best of our knowledge, ATLAS is the first scheduled MAC protocol able to adapt to changes in topology and load that is competitive with contention-based protocols in throughput and delay while realizing superior delay variance. It achieves this through the continuous computation of the TLA allocation, and updating the schedule on-the-fly. These updates occur only where and when needed. By not requiring phases of execution and by computing persistences rather than conflict-free schedules, ATLAS eliminates the complexity of, and lag inherent in, topology-dependent approaches. By not being dependent on the identity of neighbours, ATLAS shares the best of topology-transparent schemes (and also their

Submitted to IEEE Transactions on Mobile Computing ­ c 2013 IEEE

2

potential for collisions) yet overcomes its weakness by being adaptive. By not forcing updates to be frame synchronized, ATLAS shares the critical features of continuous adaptation with contention-based protocols. As a result, ATLAS achieves predictable throughput and delay characteristics. Such characteristics and information about localized capacity at the MAC layer may be used to inform higher layers, while end-toend characteristics at higher layers may be used to inform ATLAS. This may support the development of an agile, higher performing protocol stack. The primary contributions of this paper are twofold: (1) The REACT algorithm, an asynchronous, adaptive, and distributed auction that solves a general resource allocation problem to produce the TLA allocation. (2) ATLAS, a MAC protocol that uses REACT to solve the specific problem of channel allocation in a wireless network where each node produces a random schedule with the number of transmission slots determined by its allocation. The sequel is organized as follows: Section II defines a general resource allocation problem and presents the REACT algorithm, proving its correctness. Section III expresses channel allocation as a resource allocation problem and defines ATLAS. Related work is described in Section IV. After describing the simulation set-up in Section V, Section VI studies how ATLAS adapts to controlled changes in topology and load, and to dynamic network conditions. In Section VII, we discuss open issues and potential applications of REACT, including the design of higher-layer services that inform, and are informed by, the underlying communication channel. II. D ISTRIBUTED R ESOURCE A LLOCATION -- REACT We consider a general resource allocation problem. Let R be a set of N resources with capacity c = (c1 , . . . , cN ). Let D be a set of M demands with magnitudes w = (w1 , . . . , wM ). Resource j  R is required by demands Dj  D. Demand i  D consumes capacity at all resources in Ri  R simultaneously. The resource allocation s = (s1 , . . . , sM ), si  0 defines the capacity reserved for the demands. Resource allocation s is feasible if iDj si  cj for all j  R and si  wi for all i  D. Demand i is satisfied if si  wi . Resource j is saturated if iDj si  cj . Throughout, capacity refers to the magnitude of a resource. Definition 1: [22] A feasible allocation s is lexicographically max-min if, for every demand i  D, either i is satisfied, or there exists a saturated resource j with i  Dj where si = max(sk : k  Dj ). We now describe REACT, a distributed auction that computes the lexicographic max-min allocation. In it, resources are represented by auctioneers and demands by bidders. Each auctioneer maintains an offer--the maximum capacity consumed by any adjacent bidder--and each bidder maintains a claim--the capacity the bidder intends to consume at adjacent auctions. The final claim of bidder i defines allocation si . Auctioneer j satisfies Def. 1 locally by increasing its offer in an attempt to become saturated while maintaining a feasible allocation. Bidder i satisfies Def. 1 locally for demand i by increasing its claim until it is satisfied or has a maximal claim

Algorithm 1 REACT Bidder for Demand i.
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: upon initialization Ri   wi  0 U PDATE C LAIM () end upon upon receiving a new demand magnitude wi U PDATE C LAIM () end upon upon receiving offer from auctioneer j offers[j ]  offer // Remember the offer of auctioneer j . U PDATE C LAIM () end upon upon bidder i joining auction j Ri  Ri  j // Resource j is now required by demand i. U PDATE C LAIM () end upon upon bidder i leaving auction j Ri  Ri \ j // Resource j is no longer required by demand i. U PDATE C LAIM () end upon procedure U PDATE C LAIM () // Select the claim to be no larger than the smallest offer or wi . claim  min ({offers[j ] : j  Ri }, wi ) send claim to all auctions in Ri end procedure

at an adjacent auction. Through continuous updates of offers and claims, the auctioneers and bidders eventually converge on the lexicographic max-min allocation. We give precise definitions of auction and bidder behaviour next. Bidder i knows wi and maintains set Ri . Offers are stored in offers[]; offers[j ] holds the offer last received from auctioneer j . Bidder i constrains its claim to be no larger than wi or the smallest offer from auctioneers in Ri , claim = min ({offers[j ] : j  Ri }, wi ) . (1)

Auctioneer j knows cj and maintains set Dj . Bidder claims are stored in claims[]; claims[i] holds the claim last received  from bidder i. Auctioneer j identifies set Dj  Dj containing bidders with claims strictly smaller than its offer,
 Dj = {b : b  Dj , claims[b] < offer}.

(2)

 Bidders in Dj are either satisfied or are constrained by another auction and cannot increase their claims in response to a larger  offer from auctioneer j . Bidders in Dj \ Dj are constrained by auction j . They may increase their claims in response to a  larger offer. Resources left unclaimed by bidders in Dj ,

Aj = cj -

 iDj

claims[i] ,

(3)

remain available to be offered in equal portions to bidders in  Dj \Dj . If claims of all bidders in Dj are smaller than the offer  (i.e., Dj = Dj ), there are no bidders to share the available resources in Aj . The auctioneer sets its offer to Aj plus the largest claim, ensuring that any bidder in Dj can increase its claim to consume resources in Aj : offer =
  Aj /|Dj \ Dj |, if Dj = Dj , (4) Aj + max (claims[i] : i  Dj ) , otherwise.

Alg. 1 and Alg. 2 describe actions taken by the bidders and auctioneers of REACT in response to externally triggered

3

Algorithm 2 REACT Auctioneer for Resource j .
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: upon initialization Dj   cj  0 U PDATE O FFER () end upon upon receiving a new capacity of cj U PDATE O FFER () end upon upon receiving claim from bidder i claims[i]  claim // Remember the claim of bidder i. U PDATE O FFER () end upon upon bidder i joining auction j Dj  Dj  i // Demand i now requires resource j . U PDATE O FFER () end upon upon bidder i leaving auction j Dj  Dj \ i // Demand i no longer requires resource j . U PDATE O FFER () end upon procedure U PDATE O FFER ()   Dj Aj  cj done  False while ( done = False ) do  contains all bidders in D , then auction j does not // If Dj j // constrain any of the bidders in Dj .  = D ) then if ( Dj j done  True offer  Aj + max ({claims[i] : i  Dj }) // Otherwise, auction j constrains at least one bidder in Dj . else done  True // What remains available is offered in equal portions to the // bidders constrained by auction j . | offer  Aj /|Dj \ Dj  and compute A for the new offer. // Construct Dj j  } do for all b  {Dj \ Dj if ( claims[b] < offer ) then   D  b Dj j Aj  Aj - claims[b] done  False send offer to all bidders in Dj end procedure

events. Collectively, auctioneers and bidders know the inputs to the allocation problem and bidder claims converge on the lexicographic max-min allocation; the claim of bidder i converges on si . The correctness of Alg. 1 and Alg. 2 is established in two steps: Lemma 1 establishes forward progress on the number of auctioneers to have converged on their final offer. Theorem 1 employs Lemma 1 to show eventual convergence to the lexicographic max-min allocation. Let claimi denote the claim of bidder i and offerj the offer of auctioneer j . Assume that the resource allocation remains constant for the period of analysis, that bidder i knows Ri and wi , and that auctioneer j knows Dj and cj . Further assume communication between adjacent auctioneers and bidders is not delayed indefinitely. A claim or offer is stable if it has converged on its final value. Denote by Astable the set of auctioneers whose offers are stable and remain the smallest among all offers. Lemma 1: Suppose Astable contains k auctioneers, 0  k < N . Then, within finite time, at least one auctioneer converges

on the next smallest offer omin . Offers equal to omin are stable and remain smaller than all other offers not in Astable . Proof: Wait sufficient time for every bidder i to send a new claim to auctioneers in Ri and for every auctioneer j to send a new offer to bidders in Dj . Let omin be the smallest offer of an auctioneer not in Astable . Assume to the contrary that offerx for some x  / Astable is the first to become smaller than omin . By Eq. 2 and 4, a decrease to offerx can only occur after a bidder y at auction x with claimy < offerx increases its claim. By Eq. 1, claimy can increase only after its limiting constraint starts out smaller than offerx and increases. Constraints in the system smaller than offerx are maximum claims, offers from Astable , and offers equal to omin . Maximum claims and offers from Astable do not change, leaving some x with offerx = omin as the only potential limiting constraint for claimy . By Eq. 2 and 4, offerx can increase only after one of its bidders y reduces its claim to be smaller than offerx . By Eq. 1, claimy can get smaller only after one of its auctioneers, say x , reduces its offer to be offerx < omin = offerx contradicting the assumption that offerx is the first to become smaller than omin . Therefore, offers equal to omin remain smaller than offers not from Astable . By Eq. 2 and 4, any j offering omin can change only after a bidder i at auction j with claimi  omin changes. By Eq. 1, claimi only changes if its limiting constraint changes. Potential limiting constraints include wi , offers from Astable , and offers equal to omin . These constraints are stable; therefore, offers equal to omin are stable. Theorem 1: Bidders and auctioneers of Alg. 1 and Alg. 2 compute the lexicographic max-min allocation. Proof: We apply Lemma 1 to show by induction that every auctioneer eventually computes a stable offer. Base Case: Consider an allocation problem with arbitrary wi , cj , Ri , and Dj for 1  i  M , 1  j  N . Let |Astable | = 0. By Lemma 1, at least one auctioneer eventually converges on a smallest offer omin . Offers equal to omin are stable and remain smallest among all offers. Add auctioneers offering omin to Astable ; |Astable |  1. Inductive Step: Let |Astable | = k , 1  k < N . Then, by Lemma 1 a non-empty set of auctioneers A+ with A+  Astable =  eventually converge on the next smallest offer. Offers from A+ remain smaller than offers not from A+ or Astable and are stable. Add A+ to Astable ; |Astable |  k + 1. By induction, all auctioneers are eventually added to Astable . Wait for auctioneers to send their offers to adjacent bidders. Bidder claims are now stable. By Eq. 1, bidder i is either satisfied with its claim (claimi = wi ) or its claim is maximal at an auction in Ri . By Definition 1, the claims are lexicographic max-min. III. T HE ATLAS MAC P ROTOCOL Channel allocation in wireless networks can be expressed as a resource allocation problem. In this context, transmitters correspond to the demands in D and receivers to the resources in R. Label transmitters {1, . . . , M } and receivers {1, . . . , N }. A transmitter with a non-zero demand magnitude is active. Receiver j is in Ri if it is within transmission range of

4

Slot x Node A Node B Node C
Header Fields pkt #1 collision pkt #2 claim offer

Slot x + 1

Slot x + 2
pkt #1 pkt #1 pkt #1 ack #1 ack #1 ack #1

Slot x + 3
pkt #2 pkt #2 pkt #2 ack #2 ack #2 ack #2
Node # 1 w1 = 0.45 s1 = 0.25 s 1 = 0.20

Node # 7 w7 = 0.30 s7 = 0.30 s 7 = 0.20

MAC Payload

Header Fields

ACK Fields

MAC Header

MAC Header

Node # 3 w3 = 0.50 s3 = 0.25 s 3 = 0.20 Node # 4 w4 = 0.40 s4 = 0.25 s 4 = 0.20 Bidirectional Link New Bidirectional Link

claim offer

Node # 5 w5 = 0.75 s5 = 0.45 s 5 = 0.55

Fig. 1. Example transmissions in ATLAS of two packets in a network of three fully connected nodes. The first packet is sent from node A to node C . The second packet is sent from node C to node B . Transmissions are coloured white and receptions are shaded grey. The frame structure is shown for a data packet and an acknowledgement.

Node # 2 w2 = 0.55 s2 = 0.25 s 2 = 0.20

Node # 6 w6 = 0.05 s6 = 0.05 s 6 = 0.05

transmitter i and transmitter i is active. Dj contains the active transmitters for which receiver j is within transmission range. Receiver j is adjacent to transmitter i if j  Ri and i  Dj . The sets Dj and Ri capture the network topology for active transmitters. For load, wi is set to the percentage of slots required to support the demand at transmitter i. Transmitters with no demand (i.e., wi = 0) receive an allocation of zero slots: they are not active. Receiver capacities are set to one, targeting 100% channel allocation. The lexicographic max-min solution s = (s1 , . . . , sM ) for a given topology and traffic load is the TLA allocation. To apply REACT to channel allocation, we integrate it into ATLAS, a simple random scheduled MAC protocol. Although REACT could instead augment contention-based schemes, we choose to work within a scheduled environment, a traditionally difficult setting for adaptation. In ATLAS, each node runs a REACT bidder (Alg. 1) and a REACT auctioneer (Alg. 2) continuously. Auctioneers and bidders discover each other as they hear from one another and rely on the host node to detect lost adjacencies. The network topology is implicit in the sets Ri and Dj . Each node updates its bidder's demand magnitude to accurately reflect its traffic load. Offers and claims are encoded using eight bits each and are embedded within the MAC header of all transmissions to be piggybacked on existing network traffic. The encoding supports a total of 256 values for offers, claims, and persistences uniformly distributed between 0 and 1; the error in the representation does not exceed 0.004. Adding fields for an offer and claim to data packets and acknowledgements results in a communication overhead of four bytes per packet. For the slot size and data rate simulated in Section VI, the overhead is 0.36%. A node's offer and claim are eventually received by all single-hop neighbours reaching the bidders and auctioneers that need to know the offer and claim. In time, the bidder claims in REACT converge on the TLA allocation s. Packets are acknowledged within the slot they are transmitted and slots are sized accordingly. Unacknowledged MAC packets are retransmitted up to ten times before they are dropped by the sender. Fig. 1 shows that collisions are possible in ATLAS, and that successful transmissions are acknowledged in the same slot. The transmissions collide in slot x; they are repeated (successfully) in slots x + 2 and x + 3. Fig. 1 also shows the frame structure. The TLA allocation can be interpreted directly as a set of

Fig. 2. Example network showing the TLA allocation computed by REACT before and after an added link in the topology. wi identifies a node's demand, si its initial TLA allocation, and s i its TLA allocation after the added link. Resource capacities are set to one. Double-lined circles identify nodes with saturated resources.

persistences in a p-persistent MAC [28]. However, we achieve lower variation in delay by introducing the notion of a frame [6]. Specifically, ATLAS divides time into slots which are organized into frames of v slots. Node i operates at persistence pi = si . At the start of every frame and upon any change to pi , node i computes ki = pi v + 1 with probability i and ki = pi v with probability 1 - i where i = pi v - pi v . Node i constructs a transmission schedule of ki slots selected uniformly at random. Over many frames, E [ki ]/v equals pi where E [ki ] is the expectation for ki . Fig. 2 shows the TLA allocation in a small example network before and after a change in topology. Node 7 starts out disconnected from the other nodes and moves within range of node 3. In REACT, node 3 starts out offering 0.25 which is claimed by the bidders of nodes 1, 2, 3, and 4. With the claims of node 3 and 4 limited by the offer of node 3 and the claim of node 6 limited by its demand, the auctioneer at node 4 is free to offer 0.45, which is claimed by node 5. Upon detecting node 7 as a neighbour, the auctioneer at node 3 decreases its offer to 0.20. The bidders at nodes 1, 2, 3, 4, and 7 respond by reducing their claims accordingly. The smaller claims of the bidders at nodes 3 and 4 allow the auctioneer at node 4 to increase its offer to 0.55. The bidder at node 5 responds by increasing its claim to 0.55. It can be verified that, before and after the topology change, the claims of the bidders (i.e., the values of si and s i ) are lexicographically max-min; that is, every claim is satisfied or is maximal at an adjacent auction. Consider the topology with node 3 and node 7 connected. The bidder at node 6 is satisfied. The bidders at nodes 1, 2, 3, 4, and 7 are maximal at the auction of node 3. The bidder at node 5 is maximal at the auction of node 4. There are many implementation choices to be made in applying REACT to channel allocation. We identify three binary choices--lazy or eager persistences, physical layer or MAC layer receivers, and weighted or non-weighted bidders--and three configurable parameters--pmin , pdefault , and tlostNbr . The choices are described here; they are evaluated in Section VI.

5

A. Lazy or Eager Persistences A lazy approach sets persistence pi equal to the claim of bidder i. Once converged, pi matches the TLA allocation interpreted as a persistence. There is a potential disadvantage with being lazy. For many applications, nodes cannot predict future demand for the channel; they can only estimate demand based on past events, i.e., packet arrival rate or queue depth. As a consequence, wi lags the true magnitude of the demand at node i. If wi is the limiting constraint for the claim of bidder i, pi can be sluggish in response to increases in demand. Alternatively, an eager approach sets persistence pi = min (offers[j ] : j  Ri ), breaking the direct dependence on wi . Under stable conditions, a node's channel occupancy, the fraction of time it spends transmitting, matches its TLA allocation; its occupancy is limited by the availability of packets to transmit which is no larger than wi , even when pi > wi . By allowing pi > wi , the persistence is made more responsive to sudden increases in demand. B. Physical Layer or MAC Layer Receivers A central objective of the TLA allocation is to ensure that no receiver is overrun. In a wireless network, receivers can be defined in terms of physical layer or MAC layer communication. At the physical layer, every node is a receiver. At the MAC layer, packets are filtered by destination address; a node is only a receiver if one of its neighbours has MAC packets destined to it. MAC layer receivers can increase channel allocation by over-allocating at non-receiving nodes. However, the overallocation can slow detection of new receivers. Physical receivers prevent overallocation at any receiver, making the allocation more responsive to changes in traffic where nodes become receivers. C. Weighted or Non-Weighted Bidders We have described a MAC protocol where transmitters are represented by equally weighted bidders. For applications requiring multiple demands per transmitter, i.e., nodes servicing more than one traffic flow, we propose the weighted TLA allocation. The demands of weighted bidders are comprised of one or more demand fragments; the number of fragments accumulated into a demand is the demand's weight. Let i be the weight for demand i. Demand fragments in demand i have magnitude wi /i . The weighted TLA allocation defines the lexicographically max-min vector u = (u1 , . . . , uN ) where ui is the allocation to each demand fragment in demand i for a total allocation of ui i to demand i. REACT can be extended to compute the weighted TLA allocation. To do this, each bidder must inform adjacent auctions of its weight. Sixteen unique weights (with a four-bit representation) may be sufficient for many applications. D. Minimum Persistence pmin A node can maintain a persistence of zero without impacting the communication requirements of its bidder. For auctioneers, a persistence of zero is problematic. If a receiver becomes

overwhelmed by neighbouring transmitters, a non-zero persistence is needed to quiet the neighbours. To accomplish this, the node enforces a minimum persistence pmin , creating dummy packets if necessary, whenever the sum of claims from adjacent bidders exceeds the auction capacity. E. Overriding the TLA Allocation with pdefault There are two conditions where a node constrains its persistence to be no larger than pdefault . The first is when it has no neighbours. While the TLA allocation permits an isolated node to consume 100% of the channel, it cannot discover new neighbours if it does so. The second time a node employs pdefault is for a short period after the discovery of a new neighbour. It is possible for several nodes operating with large persistences to join a neighbourhood at about the same time. If the persistences are large enough, neighbour discovery can be hindered. For both scenarios, limiting the persistence to pdefault facilitates efficient neighbour discovery. F. Adaptation to Topology Changes and tlostNbr Changes in network topology are detected externally to REACT. In ATLAS, neighbour discovery is performed independently by each node. If a node hears from a new neighbour, then the node notifies its bidder of the new auction and its auctioneer of the new bidder. Conversely, if a node has not heard from a neighbour in more than tlostNbr seconds, it presumes the node is no longer a neighbour and informs its auctioneer and bidder accordingly. IV. R ELATED W ORK This paper focuses on the TLA allocation, its continuous distributed computation, and its application to setting transmitter persistences. In this section, we review a representative set of scheduled MAC protocols, observing how each selects a node's persistence and adapts to topology and load. Any finite schedule used in a cyclically repeated way can be generalized as a (k, v )-schedule with k transmission slots per frame of v slots, producing an effective persistence of p = k/v . Examples include the random schedules of [6], [18] where each node selects its k transmission slots randomly from the set of v slots in the frame. Topology transparent schemes [3], [15], [27] also implement (k, v )-schedules. These schedules rely on only two design parameters: N , the number of nodes in the network, and Dmax , the maximum supported neighbourhood size. These schedules guarantee each node a collision-free transmission opportunity from each of its neighbours at least once per frame, provided the node's neighbourhood size does not exceed Dmax . (k, v )-schedules do not adapt to variations in neighbourhood size or traffic load. The combinatorial requirements for variable-weight topology transparent schedules (variable k ) are explored in [19], but no construction nor protocol using them is given. A class of topology-dependent scheduled protocols compute distance-2 vertex colourings of the network graph to achieve TDMA schedules with spatial reuse. The colourings assign one transmission slot to each node and do not adapt to

6

TABLE I ATLAS CONFIGURATIONS SELECTED FOR SIMULATION . Configuration Name Nominal Lazy Persistences Physical Receivers Weighted Bidders Eager (0) or Lazy (1) 0 1 0 0 MAC (0) or Physical (1) 0 0 1 0 Unweighted (0) or Weighted (1) 0 0 0 1

A. Scenario Details Unless otherwise noted, all four configurations run with pdefault = 0.05, tlostNbr = 0.5s, and pmin = 0.01. The selection of pdefault and tlostNbr are justified by results in Figs. 5, 11a, and 11b. The selection of pmin is based on [20]. Frames contain v = 100 slots of length 800µs (1100 bytes per slot). Simulations are run using the ns-2 simulator [21]. Each wireless node is equipped with a single half-duplex transceiver and omni-directional antenna whose physical properties match those of the 914 MHz Lucent WaveLAN DSS radio. The data rate for all simulations is 11 Mbps. The transmission and carrier sense ranges are 250m. Each simulation runs a network scenario composed of a randomly generated topology and a randomly generated traffic load. Unless specified otherwise, topologies contain 50 randomly placed nodes constrained to a 300 × 1500m2 area. With the exception of the multi-hop TCP flows in Section VI-F, each traffic load consists of single-hop constant rate traffic. Four traffic loads are simulated: 20% and 80% of nodes loaded with small demands (75 ± 50 pkts/s), 20% and 80% of nodes loaded with large demands (500 ± 50 pkts/s). Nodes loaded with traffic are selected at random and the demand magnitudes are selected uniformly at random from the specified range. The packet destination is selected dynamically from the set of neighbouring nodes as the packet is passed down to the MAC layer. For the Weighted Bidders configuration, each demand is assigned a random integer weight between one and five. Traffic is generated by constant bit rate generators and transported over UDP; packets are 900 bytes in length, leaving room in each slot for header bytes and a MAC layer acknowledgement. Combined with the random placement of nodes and the addition of mobility, these four traffic loads enable simulation of a wide variety of network conditions. B. Relative Error A metric of interest is the average relative error for a node's persistence with respect to the TLA allocation. Error is reported in two parts: relative excess and deficit persistence error. Errors are measured per node over 80ms consecutive intervals in time (equal to the length of one MAC frame). We compute the average relative excess error and average relative deficit error for a given sample set of persistence measurements. The relative errors are ratios, requiring use of the geometric rather than arithmetic mean. But, the errors are often zero, preventing direct use of their mean. Instead, we convert errors into accuracies eliminating zeros from the data set for a more meaningful geometric average. The average relative accuracies are converted back to relative errors. VI. E VALUATION OF ATLAS Results from [20] show the TLA allocation applied in a static network to maintain expected delay and throughput compared to IEEE 802.11, while reducing the variance for both metrics. The TLA allocation nearly eliminates packets dropped by the MAC layer. In this section, we build on these results, focusing on the efficient distributed computation of the

traffic load. One of the first distributed protocols to bound the number of colours is proposed in [5]. Distributed-RAND (DRAND) [25] is a distributed implementation of RAND (a centralized algorithm for distance-2 colouring [23]). DRAND runs a series of loosely synchronized rounds. A colour is assigned in each round to one or more nodes in different two-hop neighbourhoods. DRAND is employed by ZebraMAC (Z-MAC) [24] to compute schedules over which to run CSMA/CA. Nodes are given priority access to their own slot, but also allowed to contend for access in other unused slots, as is done in [4]. Due to the complexity of DRAND, schedules are only computed once during network initialization. Other topology-dependent schemes support variable persistences. The periodic slot chains proposed in [14] are not limited to the structure of a fixed length frame and can support variable and arbitrarily precise persistences. A slot chain is defined by its starting transmission slot and period between its consecutive transmission slots. By combining multiple slot chains with different periods, schedules are constructed targeting any rational persistence in the range [0, 1]. The computation of slot chains provided in [14] is centralized; a distributed mechanism to adaptively compute the slot chains remains an open problem. In [30], a five phase reservation protocol (FPRP) computes conflict-free schedules where a node can reserve one or more transmission slots in the frame to achieve variable persistences. Reservation frames are run periodically rather than on a demand basis and, therefore, may not accommodate the current topology and traffic load. In SEEDEX [26], nodes do not attempt to derive conflictfree schedules. They learn the identities of their two-hop neighbours and adjust transmission probabilities (i.e., persistences) to improve the likelihood of collision-free transmissions. The transmission probabilities accommodate the number and identity of neighbours, but not traffic load. In our earlier work [20], a distributed algorithm for computing the TLA allocation is provided; however, the algorithm assumes a fixed topology and does not adapt to changes in the network. REACT solves these limitations by asynchronously adapting to changes in both topology and traffic demand. V. S IMULATION S ET- UP We now describe the simulations used to produce the experimental results presented in Section VI. Table I lists the four ATLAS configurations simulated. The Nominal configuration employs eager persistences, defines receivers in terms of MAC layer communication, and operates with unweighted bidders. The other three configurations differ from the Nominal case by a single choice and are named accordingly.

7

Fig. 3.

Convergence time following network initialization.

simulations of 1000 network scenarios, 250 of each traffic load. The scenarios are simulated eight times each, once per default persistence: 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, and 0.4. Small default persistences (pdefault  0.01) limit a node's ability to communicate during neighbour discovery, slowing convergence. Large default persistences (pdefault  0.3) permit nodes to transmit with large persistences before they discover their neighbours. In networks with 40 large demands, the large persistences can overwhelm the channel preventing neighbour discovery and delaying convergence. ATLAS is robust to the selection of pdefault with a suitable range of [0.05­0.2]. For the remaining simulations, pdefault is 0.05. B. Convergence after a Change in Demand

TLA allocation in the face of changes in topology and load. The results presented here work to answer four questions: 1) Can ATLAS converge quickly on the TLA allocation? 2) Can ATLAS scale to larger networks? 3) Can ATLAS keep up with changes in a mobile network? 4) Can ATLAS adapt to multi-hop traffic flows? The first question is addressed Sections VI-A, VI-B, and VI-C. The second is addressed in Section VI-D. The third and fourth questions are addressed in Sections VI-E and VI-F, respectively. Continuing the focus on adaptation, Section VI-G provides comparisons with several scheduled protocols. A. Convergence after Network Initialization Fig. 3 reports average convergence times for all four ATLAS configurations. Error bars denote the arithmetic standard deviation from the mean for each sample set. Convergence is measured from network initialization (time = 0) to the time ATLAS converges on the TLA allocation. Times are collected from simulations of 1000 network scenarios simulated four times each, once per configuration. There are 250 scenarios for each traffic load. The Physical Receivers configuration converges fastest in less than 0.4s on average for networks with 40 large demands and faster for other traffic loads. The extra step of detecting MAC receivers slows convergence. The Lazy Persistences configuration is the slowest with an average convergence time of 0.67s for networks with 40 large demands. The strict limit on persistences enforced by this configuration slows convergence compared to the others. Fig. 4a shows average excess and deficit relative persistence errors for all four configurations. The averages are computed for nodes with a non-zero TLA allocation and only during convergence. Nodes are observed to operate within approximately 20% of their TLA allocation regardless of configuration. Deficit errors are larger than excess errors reflecting a tendency to converge from below, rather than above, the TLA allocation. In Fig. 4b, each data point reflects the convergence time (xcoordinate) and total relative persistence error (y -coordinate) for one simulation of the nominal configuration. The data shows relative persistence error to be fairly consistent from network to network with a maximum observed error of 27%. Fig. 5 reports convergence time for the Nominal configuration while varying pdefault . Convergence is measured for

Fig. 6 reports convergence times and relative persistence errors for the Nominal configuration following a change to a single demand magnitude. Four types of demand are simulated: a new small demand, a new large demand, a removed small demand, and a removed large demand. New demands start with magnitude zero and change to 75 ± 50 pkts/s for small demands and to 500 ± 50 pkts/s for large demands. Removed small demands and removed large demands start at 75 ± 50 pkts/s and at 500 ± 50 pkts/s, respectively; both change to zero. The four demand change types are simulated under the four traffic loads. REACT is allowed to converge on the initial TLA allocation prior to the demand change. Convergence times and error measurements are taken from simulations of 4000 network scenarios, 250 for each of the 16 demand change and traffic load combinations. Fig. 6a reports convergence times measured from the time of the change to the time of convergence on the new TLA allocation. The largest convergence times of approximately 0.175s are found in networks loaded with 40 demands. The average convergence time for the other scenarios is 0.125s or smaller. Fig. 6b shows relative persistence errors measured during convergence at nodes whose TLA allocation are affected by the demand change. Persistences are observed to be within 10% of the TLA allocation. C. Convergence after a Change in Topology Fig. 7 reports convergence time and relative persistence error following two types of topology change: the creation of a link and the removal of a link between a pair of nodes. Simulations are run on 2000 network scenarios, 250 for each topology change type and traffic load combination. Networks that lose a link are simulated once per neighbour timeout tlostNbr of 0.5s, 2.0s and 5.0s. Network topologies are generated as follows. A first node is placed at a random location in the simulation area. For topologies gaining a link, a second node is placed just outside the transmission range of the first node with a trajectory toward the first node. For topologies losing a link, the second node is placed just inside the transmission range of the first node with a trajectory away from the first node. The remaining 48 nodes are placed at random locations in the simulation area. The distance travelled by the second node is constrained to avoid unintentional topology changes.

8

(a) Relative excess and deficit persistence errors. Fig. 4. Relative persistence error for ATLAS.

(b) Convergence time vs. error for the Nominal configuration.

Fig. 5.

Convergence times when run with varying default persistences.

convergence is reached in an average of 0.89s, a mere 40% increase compared to networks spanning 4.8 hops. The impressive convergence times, particularly those of networks spanning 12 or more hops, suggest that convergence happens locally, allowing distant neighbourhoods to converge in parallel. This local behaviour is captured in Fig. 9 which reports the average distance between a network change and a node whose bidder changes its claim in response. Distances are reported in hops. A node that changes its demand or gains/loses a neighbour has distance zero. Neighbours of this node have distance one, and so on. Range of impact is reported for the six types of change evaluated in Sections VI-B and VI-C. Each type of change is simulated in 1000 network scenarios, 250 of each traffic load. The range of impact is less than 1.75 hops on average. E. Performance with Node Mobility Section VI-C addresses the robustness of ATLAS to single topology changes. We now evaluate its performance in networks with continuous mobility which may not have the opportunity to converge on the TLA allocation. Fig. 10a reports persistence error for node speeds ranging from 0 m/s to 120 m/s with 200 scenarios simulated for each node speed, 50 of each traffic load. Node movements are generated using the steady-state mobility model generator of [13] with a pause time of zero. Simulations are run for 20s. As node speeds increase, so do deficit persistence errors. The larger deficit errors are an artifact of lost neighbour detection which is delayed by tlostNbr = 0.5s. As a result, nodes tend to think their neighbourhoods are more crowded than they are, a tendency that gets worse as node speeds increase. In terms of REACT, auctioneers and bidders unnecessarily constrain their offers and claims to accommodate lost neighbours. The deficit persistences translate to degraded throughput. Fig. 10b reports MAC throughput for the simulations of Fig. 10a. Even with node speeds of 120 m/s where a node travels its transmission range in 2.1s, throughput degrades modestly, decreasing by less than 20% compared to static networks. Fig. 11 shows that a large tlostNbr exacerbates deficit persistence error and further degrades throughput. Data is collected from 200 scenarios, 50 of each traffic load. Each scenario is simulated five times with neighbour timeouts ranging from

The expected convergence time following the addition of a new link is 0.025s. For tlostNbr =0.5s, convergence is reached in less than 0.13s on average. For tlostNbr =2.0s and tlostNbr =5.0s, the large convergence times are dominated by tlostNbr . Except for the simulations of Fig. 11, all others configure ATLAS with tlostNbr =0.5s. During convergence, nodes affected by the topology change are observed to operate within 4% of their TLA allocation on average. These numbers are striking. The small convergence times stem from a counterintuitive feature of the TLA allocation: the majority of topology changes do not affect the TLA allocation. A new link only has an effect if the link connects a bidder with an auction that lacks the capacity to support the bidder's claim. Even in heavily loaded networks, many auctions have spare capacity to support a new bidder. For these scenarios, convergence is instantaneous. D. Scalability to Large Networks We now turn to results demonstrating ATLAS's scalability. We simulate 10 network sizes with the x-dimension ranging from 600m (2.4 hops) to 6000m (24 hops) in 600m increments; the y -dimension is held constant at 300m. The number of nodes is selected to keep the average neighbourhood density constant across all network sizes. Fig. 8 reports convergence times for 4000 network scenarios, 100 of each traffic load and network size combination. The convergence of ATLAS in large networks is striking. In networks spanning 24 hops,

9

(a) Convergence time after a demand change. Fig. 6.

(b) Relative persistence error after a demand change.

Convergence time and relative persistence error during convergence following a single demand change.

(a) Convergence time after a topology change. Fig. 7.

(b) Relative persistence error after a topology change.

Convergence time and relative persistence error following a single topology change.

Fig. 8.

Convergence times as the width of the network grows.

0.1s to 15.0s. Node speeds are fixed at 30 m/s. Degraded performance is observed for large timeouts, tlostNbr  0.5s, but also for small timeouts, tlostNbr = 0.1s. In networks loaded with 10 large demands, tlostNbr = 0.1s causes nodes to falsely identify lost neighbours that must be rediscovered. The remaining simulations are run with tlostNbr = 0.5s. Fig. 12 reports packet delay for ATLAS and IEEE 802.11 for the 200 network scenarios of Fig. 10 with node speeds equal to 30 m/s. IEEE 802.11 is configured with a maximum packet retry count of seven for RTS, CTS, and ACKs and four for data packets [11], a mini-slot length of 20µs, and minimum and maximum contention window sizes of 32 and 1024 slots, respectively. Each point in the scatter plot reports the average packet delay (x-coordinate) and variation in packet delay (y coordinate) for a single node. The largest reported average delay is 0.047s for ATLAS and 0.058s for IEEE 802.11. The largest reported variation in delay for ATLAS is 0.0016s2 , just 3.6% of the 0.0444s2 reported for IEEE 802.11. This impressive reduction in delay variance is crucial to the support of TCP, which we evaluate next. F. Multi-hop TCP Flows To this point, we have used MAC layer traffic to simulate a diverse set of network scenarios. We now evaluate the performance of ATLAS using multi-hop TCP flows. To accommodate the dynamic nature of these flows, each node estimates its own demand by monitoring queue behaviour. Demand is estimated as the sum of two parts: wenqueue and wlevel . wenqueue

Fig. 9.

Average range of impact (in hops) for a demand or topology change.

10

(a) Relative persistence error. Fig. 10. Relative persistence error and total MAC throughput for varying levels of node mobility.

(b) Total MAC throughput.

(a) Relative persistence error. Fig. 11. Relative persistence error and total MAC throughput for varying neighbour timeouts.

(b) Total MAC throughput.

Fig. 12.

Delays for ATLAS and IEEE 802.11 with node speeds of 30 m/s.

is the percentage of channel required to keep up with the current enqueue packet rate, wenqueue = (packet enqueue rate) × (slot length). wlevel is the percentage of channel required to transmit all packets in the queue within 0.2s (i.e., 25 slots), wlevel = [(# packets in queue)/0.02s] × (slot length). To avoid cross-layer interactions between the MAC and routing protocols, Dijkstra's shortest path algorithm [28] using accurate knowledge of the global topology computes the next hop address for all packet transmissions. FTP agents emulate transfer of infinite size files to create flows with throughput limited only by the performance of the network. Transfers start at time zero and run for 20s. Nodes are statically placed at random locations in a 300 × 1500m2 simulation area. The source and destination nodes for each file transfer are selected

at random. Each FTP transfer is transported over TCP Reno configured for selective acknowledgements, the extensions of RFC 1323 [1], and 900 byte TCP segments. The return ACKs are not combined with each other or with other data packets. Consequently, the transmission of a single 40-byte TCP ACK consumes an entire transmission slot in ATLAS. The maximum congestion window size is 32 packets. Network scenarios are simulated for three traffic loads: networks with 2, 8, and 25 TCP flows. The number of replicates per traffic load are chosen so that 3000 TCP flows are simulated for each. Fifteen hundred scenarios are simulated with two TCP flows, 375 with eight TCP flows, and 120 with 25 TCP flows. We simulate TCP traffic on five MAC protocols: the four configurations of ATLAS and IEEE 802.11. The configurations of ATLAS use pdefault = 0.05, tlostNbr = 0.5s, and pmin = 0.01. IEEE 802.11 parameters match those described in Section VI-E. Each node dynamically sets its bidder weight to one or the number of outgoing TCP flows it services, whichever is larger. The 15 sub-plots in Fig. 13 show the percentage of flows (y -axis) achieving a minimum throughput (x-axis). The distinguishing characteristics of the three unweighted ATLAS configurations are seen in the throughput curves for networks with two flows. These networks are loaded lightly enough for the auctions at non-receiver nodes to make a difference in the allocation, improving throughput for 2- and 4-hop flows. These networks also demonstrate how the longer initial packet delays of the Lazy Persistences configuration increase round trip time

11

Networks with 2 Flows

Networks with 8 Flows

Networks with 25 Flows

All flows

y -axis shows % of TCP flows achieving minimum required throughput.

1-hop flows 2-hop flows 3-hop flows 4- and 5-hop flows

x-axis shows minimum required throughput for TCP flows in packets/second.
Fig. 13. Percent of TCP flows (y -axis) achieving a minimum throughput (x-axis). Plots in the left, center, and right columns report on flows from simulations of 2, 8, and 25 flows, respectively. The plots in the top row report on all flows, regardless of hop count. Plots in the second, third, and fourth rows report on 1-hop, 2-hop, and 3-hop flows, respectively. Plots in the fifth row report on 4- and 5-hop flows.

for 4- and 5-hop flows, preventing TCP from achieving its best throughput. The Weighted Bidders configuration performs well for multi-hop flows in networks with eight and 25 flows by allocating more to multi-hop flows at the expense of singlehop flows. Because one-hop flows tend to achieve higher throughput, the configuration maintains a tighter variation in flow throughputs as indicated by the steeper slope of the Weighted Bidders curve in the top right plot of Fig. 13. Regardless of configuration, ATLAS surpasses IEEE 802.11

in support of concurrent multi-hop flows. The interaction between the IEEE 802.11 back-off algorithm and TCP's congestion control is well known [9]. In testbed experiments, a single TCP flow with no competition has difficulty reaching a destination four hops away [16]. Our simulations corroborate these findings, as approximately 50% of the 4- and 5-hop flows report a throughput of zero. For networks with 25 demands, nearly 75% of 2-hop flows are non-functional; 3-, 4-, and 5hop flows are almost completely shut out. The throughput of

12

ATLAS is achieved in spite of channel wasted transmitting 40 byte TCP ACKs in their own slots.

VII. D ISCUSSION In this section we discuss open issues and suggest potential applications for REACT and ATLAS. A. Improved Reliable Transport TCP's congestion control algorithm is known to suffer cross-layer interactions with binary exponential back-off (BEB) employed by IEEE 802.11 [9]. BEB is short term unfair, allowing a single node to capture the channel at the expense of its neighbours [2], [10] causing high variation in packet delay and making it difficult for TCP to estimate roundtrip delay. Many modifications have been proposed to improve TCP performance over wireless networks [17]; common approaches are detection of packet loss (differentiating it from congestion) and improved estimation of round trip time. An alternative is to minimize packet loss and control variation in packet delay at the MAC layer. ATLAS demonstrates a remarkable control of variation in delay (Fig. 12) enabling TCP to reliably support 3-, 4-, and 5-hop flows over heavily loaded networks (Fig. 13). However, TCP throughput still degrades considerably as the number of hops grows. Potential areas for future work include the integration of ATLAS into a cross-layer solution for reliable transport over wireless networks and the use of REACT to inform TCP's congestion window size. B. Selection of Configurable Parameters ATLAS has three configurable parameters: pdefault , tlostNbr , and pmin . Based on our simulations, [0.01­0.2] is an acceptable range for pdefault (Fig. 5) and [0.1s­2s] is an acceptable range for tlostNbr (Figs. 11a, 11b). In [20], pmin =0.1s is found to be acceptable for a protocol that enforces pmin at all nodes and at all times. Because ATLAS employs pmin temporarily, and only when needed, it is less sensitive to the selection of pmin . Although results show ATLAS to be robust to parameter selection, tuning may be required in other scenarios or in a hardware implementation. C. Dynamic Selection of Auction Capacity ATLAS targets 100% channel allocation by setting auction capacities in REACT to one. Although simulation results show this to be an adequate choice, it is not clear whether performance can be improved by under- or over-allocating the channel. Indeed, optimal auction capacities (however optimal is defined) are dependent on network topology and quality of the communication channel. We leave a thorough analysis of auction capacity selection to future work, pointing out here that REACT adapts continuously, allowing auction capacities to be adjusted dynamically, if necessary. D. Potential Applications for REACT The weighted TLA allocation opens doors for several potential uses. In the simulations of Section VI-F, a bidder's weight is set according to the number of flows it services. It may be desirable to set weights according to queue levels,

G. Comparison with other Scheduled MAC Protocols Here, we compare the adaptation of ATLAS with several other scheduled protocols including DRAND, Z-MAC, FPRP, and SEEDEX. Although the first three compute conflict-free schedules, an NP-hard problem [7], a comparison highlights the agility of ATLAS. 1) Adaptation to Topology Changes: For the simulations of Section VI-E, the number of neighbour changes (i.e., gained or lost neighbours) per second experienced by a node is correlated to the node speed. When the nodes move at 30 m/s, each node is expected to gain, or lose, a neighbour 2.21 times per second; within 6.3s, the number of neighbour changes is expected to exceed the neighbourhood size. Based on the run times reported in [25, Fig. 10], we estimate DRAND to compute schedules for the networks in Section VI-E in approximately 4.9s (adjusting for data rate and a two-hop neighbourhood size of 27). In this time, the topology changes caused by nodes moving at 30 m/s are expected to invalidate the computed schedule. Z-MAC has the same limitation and, although it compensates by running CSMA/CA to resolve collisions, it does not benefit from its TDMA schedule when nodes are mobile. In [25], the run times reported for FPRP schedule generation are comparable to DRAND. For SEEDEX, nodes discover their two-hop neighbours using a fan-in/fan-out procedure described in [26]. However, a practical integration of the procedure into the MAC protocol is not described or evaluated, preventing a comparison of its agility with other MAC protocols. In contrast to the slow schedule computation times of DRAND, Z-MAC, and FPRP, ATLAS is shown to handle node speeds of up to 120 m/s with only moderate degradation to MAC throughput. 2) Adaptation to Changes in Traffic Load: The persistences achieved by DRAND and SEEDEX are dependent on topology alone; neither adapts to traffic load. Although Z-MAC adapts to load, it does so by deviating from its underlying schedule, which does not adapt. FPRP can adapt to load by scheduling a variable number of slots per node; this is done at the expense of both longer frame lengths and longer run times for schedule computation. In contrast, ATLAS adapts to traffic load, responding quickly enough to establish and maintain multi-hop TCP flows. 3) Continuous Adaptation: Common to the scheduled schemes mentioned here is the use of a distinct phase for schedule computation (or neighbour discovery for SEEDEX). The schedules must be updated in order for the MAC to adapt. Any fixed period between schedule updates must be selected a priori; it cannot be adjusted for variations in network mobility. If schedules are to be updated when needed, a mechanism is required to trigger the schedule update. This coordination, by itself, is a challenge in an ad hoc network. In contrast, ATLAS does not employ a schedule computation (or a neighbour discovery) phase and adapts continuously to changes in both topology and traffic load.

13

demand magnitudes, neighbourhood sizes, node betweenness [8], distance from a point of interest (i.e., an access point or a common sink), position in a multicast/broadcast tree, or path hop count. The key observation is that ATLAS maintains flexibility by allowing nodes to define bidder weights arbitrarily to suit the needs of the network. While computation of persistences is the primary motivation for this work, REACT is not limited to this purpose. Consider the Physical Receivers configuration with node demands set to one. The resulting allocation is independent of actions taken by the upper network layers and, therefore, can inform decisions made by those layers. It can serve as a measure of potential network congestion--small allocations are assigned in dense neighbourhoods containing many potentially active neighbours. The routing protocol can use the allocation to discover alternate routes around congestion. An intriguing application is the implementation of differentiated service at the MAC layer. IEEE 802.11e [12] enhances the distributed coordination function by implementing four access categories; an instance of the back-off algorithm is run per access category, each with its own queue. The probability of transmission of each access category is manipulated independently through selection of contention window size and inter-frame space. This permits higher priority traffic to capture the channel from lower priority traffic. Similar results can be achieved by four instances of REACT, each computing the allocation for a single access category. Prioritization is achieved through dynamic coordination of the four auction capacities at each node. A potential strategy sets the capacity for each access category equal to one minus the allocation to higher priority access categories. As a result, higher priority auctions are permitted to starve lower priority auctions of capacity, effectively distributing channel access to high priority traffic. Alternatively, auction capacities can be selected to ensure a minimum or maximum percentage of the channel is offered to an access category. A network can run multiple instances of REACT. For example, an instance of the Physical Receivers configuration with all demands set to one can be run concurrently with four instances configured to support differentiated service. Alternatively, multiple instances of REACT can be used to allocate more than one set of resources concurrently. E. Assumptions Made by ATLAS Two key assumptions are made by ATLAS in its computation of the TLA allocation using REACT: (1) The offers and claims received by a node are accurate. (2) The offers and claims of a node are eventually received by all neighbouring nodes. The first assumption is reasonable, provided received packets are checked for errors by the link layer. The second assumption is almost certainly invalid; asymmetric communication, interference beyond the range of transmission, and signal fading are common in wireless communication and can prevent the delivery of offers and claims. Under realistic conditions, REACT may not converge on the TLA allocation, risking overallocation of the channel. In practice, auctions can adjust their capacities to mitigate the over-allocation. Every node knows

the persistences of its neighbours (from bidder claims) and can compute the expectation for collisions on the channel. Significant deviations above this expectation can trigger the auction to lower its capacity. An evaluation in a testbed of real radios is necessary to understand the sensitivity to anomalies on the wireless channel and the effectiveness of adjusting auction capacities to accommodate channel conditions. The evaluation of ATLAS in Section VI assumes both slot and frame synchronization; ATLAS does not require either. The computation of the TLA allocation by REACT does not rely on a frame structure and the expected performance of the random schedules is not affected by loss of frame synchronization. Even without slot synchronization, REACT can compute the TLA allocation; however, loss of slot synchronization may reduce channel capacity by 50% (see Aloha vs. slotted Aloha in [28]). ATLAS can accommodate the lower channel capacity by reducing auction capacity. This technique may allow ATLAS to be run on commodity IEEE 802.11 hardware [29] that lacks native support for slot synchronization. This is a subject of our current research. F. Enhancing Existing MAC Protocols We have used REACT to compute persistences to be employed within ATLAS, a slotted MAC protocol. Alternatively, REACT can be run on top of the IEEE 802.11 MAC by embedding claims and offers in the headers of existing control and data messages. The TLA allocation can be used to inform the selection of contention window sizes, eliminating the need for (and negative side effects of) binary exponential back off. We are currently working to integrate REACT into IEEE 802.11. Another alternative (and more ambitious) approach is to implement TLA persistences in a topology-dependent MAC that computes conflict-free schedules. Only a few topologydependent schemes allow a node to reserve more than one slot in a frame (i.e., [14], [30]), and those do not define how many slots a node should reserve. The TLA allocation can establish a permissible number of slots to be reserved by each node, given the current topology and traffic load. VIII. C ONCLUSION We have proposed REACT, a distributed auction that converges continuously on the TLA allocation, adapting to changes in both topology and traffic load. The utility of REACT is demonstrated through integration into ATLAS which we simulate under a wide variety of network scenarios. The results presented suggest that REACT can effectively inform the selection of transmitter persistences, and that ATLAS can provide robust, reliable, and scalable services. The application of REACT is not restricted to the computation of transmitter persistences. It has the potential to inform routing and admission control decisions, to enable differentiation of service at the MAC layer, and even to allocate other node resources. In this context, the REACT algorithm provides a potential solution to the immediate challenge of medium access control, but also shows promise as a tool for use in network protocol design in general.

14

ACKNOWLEDGEMENT The authors appreciate the useful comments provided by the anonymous reviewers. R EFERENCES
[1] RFC 1323: TCP Extentions for High Performance, 1992. [2] V. Bharghavan, A. Demers, S. Shenker, and L. Zhang. MACAW: A medium access protocol for wireless LANs. In Proceedings of the ACM Conference on Communications Architectures, Protocols and Applications (SIGCOMM'94), pages 212­225, 1994. [3] I. Chlamtac and A. Farag´ o. Making transmission schedules immune to topology changes in multi-hop packet radio networks. IEEE/ACM Transactions on Networking, 2(1):23­29, 1994. [4] I. Chlamtac, A. Farag´ o, A. D. Myers, V. R. Syrotiuk, and G. Z´ aruba. ADAPT: A dynamically self-adjusting media access control protocol for ad hoc networks. In Proceedings of the IEEE Global Telecommunications Conference (GLOBECOM'99), pages 11­15, 1999. [5] I. Chlamtac and S. S. Pinter. Distributed nodes organization algorithm for channel access in a multihop dynamic radio network. IEEE Transactions on Computers, C-36(6):728­737, June 1987. [6] C. J. Colbourn and V. R. Syrotiuk. Scheduled persistence for medium access control in sensor networks. In Proceedings from the First IEEE International Conference on Mobile Ad hoc and Sensor Systems (MASS'04), pages 264­273, 2004. [7] S. Even, O. Goldreich, S. Moran, and P. Tong. On the NP-completeness of certain network testing problems. Networks, 14(1):1­24, 1984. [8] L. C. Freeman. A set of measures of centrality based on betweenness. Sociometry, 40(1):35­41, 1977. [9] M. Gerla, R. Bagrodia, L. Zhang, K. Tang, and L. Wang. TCP over wireless multi-hop protocols: Simulation and experiments. In Proceedings of the 1999 IEEE International Conference on Communication (ICC'99), pages 1089­1094, 1999. [10] J. Hastad, T. Leighton, and B. Rogoff. Analysis of backoff protocols for multiple access channels. In Proceedings of the 19th annual ACM Symposium on Theory of Computing (STOC'87), pages 740­744, 1987. [11] IEEE. IEEE 802.11, Wireless LAN medium access control (MAC) and physical layer (PHY) specifications, 1997. [12] IEEE. IEEE 802.11e, Enhancements: QoS, including packet bursting, 2007. [13] J. Boleng, N. Bauer, T. Camp, and W. Navidi. Random Waypoint Steady State Mobility Generator (mobgen-ss). http://toilers.mines.edu/. [14] G. Jakllari, M. Neufeld, and R. Ramanathan. A framework for frameless TDMA using slot chains. In Proceedings of the 9th IEEE International Conference on Mobile Ad hoc and Sensor Systems (MASS'12), 2012. [15] J. Ju and V. O. K. Li. An optimal topology-transparent scheduling method in multihop packet radio networks. IEEE/ACM Transactions on Networking, 6(3):298­305, 1998. [16] D. Koutsonikolas, J. Dyaberi, P. Garimella, S. Fahmy, and Y. C. Hu. On TCP throughput and window size in a multihop wireless network testbed. In Proceedings of the 2nd ACM International Workshop on Wireless network testbeds, experimental evaluation and characterization (WiNTECH'07), 2007. [17] K. Leung and V. O. K. Li. Transmission control protocol (TCP) in wireless networks: Issues, approaches, and challenges. IEEE Communications Surveys & Tutorials, 8:64­79, 2006. [18] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Apples and oranges: Comparing schedule- and contention-based medium access control. In Proceedings of the 13th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems (MSWiM'10), pages 319­326, 2010. [19] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Variable weight sequences for adaptive scheduled access in MANETs. In Proceedings of Sequences and their Applications (SETA'12), pages 53­64, 2012. [20] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Topological persistence for medium access control. IEEE Transactions on Mobile Computing, 12(8):1598­1612, 2013. [21] The Network Simulator ns-2. http://www.isi.edu/nsnam/ns/. [22] M. Pi´ oro and D. Medhi. Routing, Flow, and Capacity Design in Communication and Computer Networks. Elsevier Inc., 2004. [23] R. Ramanathan. A unified framework and algorithm for (T/F/C)DMA channel assignment in wireless networks. In Proceedings of the 16th Annual Joint Conference of the IEEE Computer and Communications Societies (INFOCOM'97), pages 900­907, 1997.

[24] I. Rhee, A. Warrier, M. Aia, J. Min, and M. L. Sichitiu. Z-MAC: A hybrid MAC for wireless sensor networks. IEEE Transactions on Networking, 16(3):511­524, 2008. [25] I. Rhee, A. Warrier, J. Min, and L. Xu. DRAND: Distributed randomized TDMA scheduling for wireless ad-hoc networks. IEEE Transactions on Mobile Computing, 8(10):1384­1396, 2009. [26] R. Rozovsky and P. R. Kumar. SEEDEX: A MAC protocol for ad hoc networks. In Proceedings of the 2nd ACM International Symposium on Mobile Ad Hoc Networking and Computing (MOBIHOC'01), pages 67­75, 2001. [27] V. R. Syrotiuk, C. J. Colbourn, and S. Yellamraju. Rateless forward error correction for topology-transparent scheduling. IEEE/ACM Transactions on Networking, 16(2):464­472, 2008. [28] A. S. Tanenbaum. Computer Networks. McGraw Hill, fourth edition, 2003. [29] I. Tinnirello, G. Bianchi, P. Gallo, D. Garlisi, F. Giuliano, and F. Gringoli. Wireless MAC processors: Programming MAC protocols on commodity hardware. In Proceedings of the 31st Annual Joint Conference of the IEEE Computer and Communications Societies (INFOCOM'12), pages 1269­1277, 2012. [30] C. Zhu and M. S. Corson. A five-phase reservation protocol (FPRP) for mobile ad hoc networks. Wireless Networks, 7(4):371­384, 2001.

Jonathan Lutz earned his B.S. in Electrical Engineering from Arizona State University, Tempe, Arizona, in 2000 and his M.S. in Computer Engineering from the University of Waterloo, Waterloo, Canada, in 2003. He is currently working on his Ph.D. in Computer Science at Arizona State University. His research interests include medium access control in mobile ad hoc networks.

Charles J. Colbourn earned his Ph.D. in 1980 from the University of Toronto, and is a Professor of Computer Science and Engineering at Arizona State University. He is the author of The Combinatorics of Network Reliability (Oxford), Triple Systems (Oxford), and 320 refereed journal papers focussing on combinatorial designs and graphs with applications in networking, computing, and communications. In 2004, he was awarded the Euler Medal for Lifetime Research Achievement by the Institute for Combinatorics and its Applications.

Violet R. Syrotiuk earned her Ph.D. in Computer Science from the University of Waterloo (Canada). She is an Associate Professor of Computer Science and Engineering at Arizona State University. Her research has been supported by grants from NSF, ONR, and DSTO, and contracts with LANL, Raytheon, General Dynamics, and ATC. She serves on the editorial boards of Computer Networks and Computer Communications, as well as on the technical program and organizing committees of several major conferences sponsored by ACM and IEEE.

1

Hierarchical Recovery in Compressive Sensing
Charles J. Colbourn, Daniel Horsley, and Violet R. Syrotiuk, Senior Member, IEEE
Abstract A combinatorial approach to compressive sensing based on a deterministic column replacement technique is proposed. Informally, it takes as input a pattern matrix and ingredient measurement matrices, and results in a larger measurement matrix by replacing elements of the pattern matrix with columns from the ingredient matrices. This hierarchical technique yields great flexibility in sparse signal recovery. Specifically, recovery for the resulting measurement matrix does not depend on any fixed algorithm but rather on the recovery scheme of each ingredient matrix. In this paper, we investigate certain trade-offs for signal recovery, considering the computational investment required. Coping with noise in signal recovery requires additional conditions, both on the pattern matrix and on the ingredient measurement matrices. Index Terms compressive sensing, hierarchical signal recovery, deterministic column replacement, hash families

arXiv:1403.1835v1 [cs.IT] 4 Mar 2014

I. I NTRODUCTION Nyquist's sampling theorem provides a sufficient condition for full recovery of a band-limited signal: sample the signal at a rate that is twice the band-limit. However, there are cases when full recovery may be achieved with a sub-Nyquist sampling rate. This occurs with signals that are sparse (or compressible) in some domain, such as those that arise in applications in sensing, imaging, and communications, and has given rise to the field of compressive sensing [2], [6] (also called compressive sampling). Consider the following framework for compressive sensing. An admissible signal of dimension n is a vector in Rn that is known a priori to be taken from a given set   Rn . A measurement matrix A is a matrix from Rm×n . Sampling a signal x  Rn corresponds to computing the product Ax = b. Once sampled, recovery involves determining the unique signal x   that satisfies Ax = b using only A and b. If  = Rn , recovery can be accomplished only if A has rank n, and hence m  n. However for more restrictive admissible sets , recovery may be accomplished when m < n. Given a measurement matrix A, an equivalence relation A is defined so that for signals x, y  Rn , we have x A y if and only if Ax = Ay. If for every equivalence class P under A , the set P   contains at most one signal then in principle recovery is possible. Because Ax = Ay ensures that A(x - y) = 0, this can be stated more simply: An equivalence class P of A can be represented as {x + y : y  N (A)} for any x  P , where N (A) is the null space of A, i.e., the set {x  Rn : Ax = 0}. Recoverability is therefore equivalent to requiring that, for every signal x  , there is no y  N (A) \{0} with x + y  . In order to make use of these observations, a reasonable a priori restriction on the signals to be sampled is identified, suitable measurement matrices with m  n are formed, and a reasonably efficient computational strategy for recovering the signal is provided. A signal is t-sparse if at most t of its n coordinates are nonzero. The recovery of t-sparse signals is the domain of compressive sensing. An admissible set of signals  has sparsity t when every signal in  is t-sparse. An admissible set of signals  is t-sparsifiable if there is a full rank matrix B  Rn×n for which {B x : x  } has sparsity t. We assume throughout that when the signals are sparsifiable, a change of basis B is applied so that the admissible signals have sparsity t. A measurement matrix has (0 , t)-recoverability when it permits exact recovery of all t-sparse signals. A basic problem is to design measurement matrices with (0 , t)-recoverability where m  n such that recovery can be accomplished efficiently. Suppose that measurement matrix A has (0 , t)-recoverability. Then in principle, given A and b, recovery of the signal x can be accomplished by solving the 0 -minimization problem min{||x||0 : Ax = b}. To do so the possible supports of signals from fewest nonzero entries to most are first listed. For each, reduce A to A and x to x by eliminating coordinates in the signal assumed to be zero. Examine the now overdetermined system A x = b. When equality holds, a solution is found; we are guaranteed to find one by considering all possible supports with at most t nonzero entries. Such an enumerative strategy is prohibitively time-consuming, examining as many as n t linear systems when the signal has sparsity t. Natarajan [27] showed that we cannot expect to find a substantially more efficient solution, because the problem is NP-hard. Instead of the 0 -minimization problem, Chen, Donoho, Huo, and Saunders [11], [18] suggest considering the 1 -minimization problem min{||x||1 : Ax = b}. While this can be solved using standard linear programming techniques, to be effective it is necessary that for each t-sparse signal x, the unique solution to min{||z||1 : Az = Ax} is x. This property is (1 , t)recoverability. A necessary and sufficient condition for (1 , t)-recoverability has been explored, beginning with Donoho and Huo [18] and subsequently in [19]­[21], [24], [30], [31], [33].
C. J. Colbourn and V. R. Syrotiuk are with the School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, U.S.A., 85287-8809, {colbourn,syrotiuk}@asu.edu D. Horsley is with the School of Mathematical Sciences, Monash University, Vic 3800, Australia, daniel.horsley@monash.edu

2

A measurement matrix A meets the (0 , t)-null space condition if and only if N (A) \ {0} contains no (2t)-sparse vector. For y  Rn and C  {1, . . . , n}, define y|C  Rn to be the vector such that (y|C ) = y if   C and (y|C ) = 0 otherwise. A measurement matrix A meets the (1 , t)-null space condition if and only if for every y  N (A) \ {0} and every C  {1, . . . , n} with |C | = t, ||y|C ||1 < 1 2 ||y||1 . Lemma 1: ([13], for example) Measurement matrix A  Rm×n has (0 , t)-recoverability if and only if A meets the (0 , t)null space condition. Lemma 2: ([33], for example) Measurement matrix A  Rm×n has (1 , t)-recoverability if and only if A meets the (1 , t)null space condition.

To establish (1 , t)-recoverability, and hence also (0 , t)-recoverability, Cand` es and Tao [7], [9] introduced the Restricted Isometry Property (RIP). For A  Rm×n , the dth RIP parameter of A, d (A), is the smallest  so that, for some constant R > 0, (1 -  )R(||x||2 )2  (||Ax||2 )2  (1 +  )R(||x||2 )2 , for all x with ||x||0  d. The dth RIP parameter is better when d (A) is smaller as the bounds are tighter. The RIP parameters have been employed extensively to establish (1 , t)-recoverability, particularly for randomly generated measurement matrices [8]­[10], but also for those generated using deterministic con structions [12], [17]. Commonly, 2t < 2 - 1 is required for (1 , t)-recoverability; see [7] for example. The property of (1 , t)-recoverability in the presence of noise has also been considered. Conditions on the RIP parameters are sufficient but in general not necessary for recoverability. Combinatorial approaches to compressive sensing are detailed in [3], [16], [22], [23], [25], [26], [32]. We pursue a different combinatorial approach here, using a deterministic column replacement technique based on hash families. The use of an heterogeneous hash family provides an explicit hierarchical construction of a large measurement matrix from a library of small ingredient matrices. Strengthening hash families provide a means to increase the level of sparsity supporte, allowing the ingredient matrices to be designed for lower sparsity than the larger measurement matrix produced. In this paper we show that the heterogeneity extends to signal recovery: it is interesting that the ingredient measurement matrices need not all employ the same recovery algorithm. This enables hierarchical recovery for the large measurement matrix; however, this can be computationally prohibitive. By restricting the hash family to be linear, recovery for the large measurement matrix can be achieved in sublinear time even when computationally intensive methods are used for each ingredient matrix. To be practical, recovery methods based on hash families must deal with noise in the signal effectively. Suitable restrictions on the hash family and on each ingredient matrix used in the hierarchical method are shown to be sufficient to permit recovery in the presence of noise. The rest of this paper is organized as follows. The results on homogeneous hash families in Section II demonstrate that a recovery scheme based on (0 , t)- or (1 , t)-recoverability can be `lifted' from the ingredient measurement matrices to the matrix resulting from column replacement. Section III considers a generalization of hash families to allow for ingredient matrices with other recovery algorithms, and the computational investment to recover the signal. Signal recovery without noise is considered first, and the conditions for a sublinear time recovery algorithm described. Section IV considers the recovery of almost-sparse signals to deal with noise in the signal. Finally, Section V draws relevant conclusions. II. H ASH FAMILIES
AND

C OMPRESSIVE S ENSING

A. Column Replacement and Hash Families for Compressive Sensing Let A  Rr×k , A = (aij ), be an ingredient matrix. Let P  {1, . . . , k }m×n , P = (pij ), be a pattern matrix. The columns of A are indexed by elements of P . For each row i of P , replace element pij with a copy of column pij of A. The result is an rm × n matrix B , the column replacement of A into P . Fig. 1 gives an example of column replacement.   a11 a12 a13 a11  a21 a22 a23 a21  1231 a11 a12 a13  B= P = A=  a13 a11 a12 a11  a21 a22 a23 3121 a23 a21 a22 a21
B is the column replacement of A into P .

Fig. 1.

When the ingredient matrix A is a measurement matrix that meets one of the null space conditions for a given sparsity, our interest is to ensure that the sparsity supported by B is at least that of A. Not every pattern matrix P suffices for this purpose. Therefore, we examine the requirements on P . Let m, n, and k be positive integers. An hash family HF(m; n, k ), P = (pij ), is an m × n array, in which each cell contains one symbol from a set of k symbols. An hash family is perfect of strength t, denoted PHF(m; n, k, t), if in every m × t

3

subarray of P at least one row consists of distinct symbols; see [1], [28]. Fig. 2 gives an example of a perfect hash family PHF(6; 12, 3, 3). For example, for the 6 × 3 subarray involving columns 4, 5, and 6, only the fourth row consists of distinct symbols.  2 0 2 1 1 2  1 2 2 2 2 1  2 2 2 0 1 1


Fig. 2. A perfect hash family PHF(6; 12, 3, 3).

0 0 1 2 2 2

1 2 0 0 0 0

2 1 0 1 2 1

2 2 1 2 0 2

0 1 1 0 2 2

1 0 2 1 2 0

1 1 1 1 1 1

0 2 0 2 1 2

0 1 2 1 0 1

A perfect hash family has at least one row that separates the t columns into t parts in every m × t subarray. A weaker condition separates the t columns into classes. A {w1 , . . . , ws }-separating hash family, denoted SHF(m; n, k, {w1 , . . . , ws }), s with t = i=1 wi , is an m × n array on k symbols in which for every m × t subarray, and every way to partition the t columns into classes of sizes w1 , . . . , ws , there is at least one row in which no two classes contain the same symbol; see [4], [29]. A W -separating hash family, denoted SHF(m; n, k, W ), is a {w1 , . . . , ws }-separating hash family for each {w1 , . . . , ws }  W . Fig. 3 gives an example of a {1, 2}-separating hash family SHF(3; 16, 4, {1, 2}). For the 3 × 3 subarray consisting of columns 11, 15, and 16, for example, the last row separates columns {11, 16} from column {15}. 1 1 1 1 2 2 1 3 3 1 4 4 2 1 2 2 2 1 2 3 4 2 4 3 3 1 3 3 2 4  3 3 1 3 4 2 4 1 4 4 2 3  4 3 2  4 4 1


Fig. 3.

A {1, 2}-separating hash family SHF(3; 16, 4, {1, 2}).
s

A distributing hash family DHF(m; n, k, t, s) is an SHF(m; n, k, W ) with W = {{w1 , . . . , ws } : t = i=1 wi }. Fig. 4 gives an example of a DHF(10; 13, 9, 5, 2). For the 10 × 5 subarray consisting of columns 8 through 12, row 4 separates columns {8, 9, 10, 11} from column {12} (a {1, 4}-separation), and row 5 separates columns {8, 9, 12} from columns {10, 11} (a {2, 3}-separation). 6 3 8 0 0 1 1 1 0 0 7 1 5 2 0 1 0 1 0 0 8 1 1 0 2 2 1 0 3 0 3 7 4 2 1 2 2 1 0 0 4 2 2 2 1 2 0 0 1 0 0 6 3 0 1 0 0 4 0 1 2 8 2 0 2 1 2 2 0 0  2 4 6 1 0 0 0 0 2 0  3 3 7 1 0 0 0 2 4 1  0 0 0 1 2 2 1 0 0 0  5 2 1 1 2 1 2 1 0 0  1 0 3 2 0 0 2 0 1 0 1 5 0 0 1 0 1 2 0 1

a {1, 4}-separation  a {2, 3}-separation 

Fig. 4.

A distributing hash family DHF(10; 13, 9, 5, 2).

Now, we are in a position to state the requirements on a pattern matrix P that ensure that the sparsity supported by the matrix B resulting from column replacement is at least that of A. Theorem 1: [13] Suppose that A is an r × k measurement matrix that meets the (0 , t)-null space condition, that P is an SHF(m; n, k, {1, t}), and that B is the column replacement of A into P . Then B is an rm × n measurement matrix that meets the (0 , t)-null space condition.

Theorem 2: [13] Suppose that A is an r × k measurement matrix that meets the (1 , t)-null space condition, that P is a DHF(m; n, k, t + 1, 2), and that B is the column replacement of A into P . Then B is an rm × n measurement matrix that meets the (1 , t)-null space condition.

4

B. Exploiting Heterogeneity in Column Replacement All the standard definitions of hash families may be generalized by replacing k by k = (k1 , . . . , km ), a tuple of positive integers. Now, an heterogeneous hash family HF(m; n, k), P = (pij ), is an m × n array in which each cell from row i contains one symbol from a set of ki symbols, 1  i  m. Column replacement may be extended to exploit heterogeneity in an hash family. Let P = (pij ) be an HF(m; n, k) and, for 1  i  m, let Ai be an ri × ki ingredient matrix whose columns are indexed by the ki elements in row i of P . For each row i of P , replace the element pij with a copy of column pij of Ai , 1  j  n. The result is a ( m i=1 ri ) × n matrix B , the column replacement of A1 , . . . , Am into P . Fig. 5 gives an example of column replacement using an heterogeneous hash family.  1 1 1 1 1 a1 11 a13 a12 a11 a12 a13 1 1 1 1 1   a1 21 a23 a22 a21 a22 a23  B= 2 2 2 2 2   a2 11 a11 a11 a12 a12 a12 2 2 2 2 2 a2 21 a21 a21 a22 a22 a22 

P =

132123 111222

A1 =

1 1 a1 11 a12 a13 1 1 1 a21 a22 a23

A2 =

2 a2 11 a12 2 2 a21 a22

Fig. 5.

B is the column replacement of A1 , A2 into P .

An hierarchical method for compressive sensing is obtained using column replacement in an heterogeneous hash family. Suppose that Ai is a measurement matrix for a signal of dimension ki supporting the recovery of sparsity qi , for 1  i  m. We now describe the properties the pattern matrix needs to satisfy to support recovery of signals of dimension n and sparsity t. In Section II-A, we saw that a perfect hash family separates t columns into t parts, and that a separating hash family separates t columns into classes. We now define a particular type of separating hash family in which the number of symbols used to accomplish the separations is restricted. Let d = (d1 , . . . , dm ) be a tuple of positive integers, and let  be a positive integer. Let W = {W1 , . . . , Wr }, where for si 1  i  r, Wi = {wi1 , . . . , wisi } is a multiset of nonnegative integers, and i = j =1 wij . An SHF(m; n, k, W ), P = (pij ), is (d,  )-strengthening if whenever 1  i  r, · C is a set of i columns, · C1 , . . . , Csi is a partition of C with |Cj | = wij for 1  j  si , and · T is a set of  columns with |C  T | = min(i ,  ), there exists a row  for which px = py whenever x  Ce , y  Cf and e = f and the multiset {px : x  T } contains no more than d different symbols. When  = max{i : 1  i  r}, we omit  and write d-strengthening. Because rows of P can be arbitrarily permuted (while permuting the ingredient matrices in the same manner), the order of elements in k and d is us 1 inconsequential. Hence we often use exponential notation, writing xu 1 · · · xs , with ui a non-negative integer for 1  i  s,  -1  s s for a vector (y1 , . . . , y j=1 uj ) in which y = xj for j =1 uj <   j =1 uj for 1    j =1 uj . Fig. 6 gives a heterogeneous d-strengthening DHF(19; 13, k, 5, 2) with k = (56 41 312 ) and d = (46 313 ). This is equivalent to a d-strengthening SHF(19; 13, k, {{1, 4}, {2, 3}}). Consider the separation of columns {1, 7} from columns {2, 6, 11}. Row 8 accomplishes the required separation because it uses no more than d8 = 3 symbols. Consider instead columns {1, . . . , 5}. While the first row separates {1, 2, 3} from {4, 5}, it uses 5 symbols instead of d1 = 4 and so does not accomplish the required separation; this separation is accomplished in row 3. Next the properties are determined for an heterogeneous hash family to support recovery of signals of dimension n and sparsity t using a column replacement technique. Theorem 3: [14] Let k = (k1 , . . . , km ) and q = (q1 , . . . , qm ) be tuples of positive integers. Let d = (2q1 , . . . , 2qm ). For 1  i  m, let Ai  Rri ×ki be a measurement matrix that meets the (0 , qi )-null space condition. Let P be a (d, 2t)strengthening SHF(m; n, k, {1, t}), and let B be the column replacement of A1 , . . . , Am into P . Then B meets the (0 , t)-null space condition. Theorem 4: [14] Let k = (k1 , . . . , km ) and q = (q1 , . . . , qm ) be tuples of positive integers. For 1  i  m, let Ai  Rri ×ki be a measurement matrix that meets the (1 , qi )-null space condition. Let P be a (q, t)-strengthening DHF(m; n, k, t + 1, 2), and let B be the column replacement of A1 , . . . , Am into P . Then B meets the (1 , t)-null space condition.

Revisiting the d-strengthening DHF(19; 13, k, 5, 2) pattern matrix in Fig. 6, the results of Theorems 3 and 4 indicate that the number of symbols in each row need not be the same. In general, there may be as many ingredient matrices Ai as there are rows of the pattern matrix P . Moreover, the strength of each ingredient matrix Ai may be different! In this example, the

5

k1 = . . . = k6 = 5 symbols; d1 = . . . = d6 = 4 used to separate

k2 = 4 symbols; d2 = 3 used to separate 

k8 = . . . = k19 = 3 symbols; d8 = . . . = d19 = 3 used to separate

 4 0 0 2 2 3 0 0 1 0 0 0 2 2 0 1 1 2 0

 0 0 2 4 1 4 0 1 0 1 2 1 1 1 0 2 0 2 0

2 1 4 1 2 0 1 0 2 2 2 1 0 2 0 0 2 0 2

1 1 1 0 2 1 0 1 0 2 2 0 1 0 1 1 1 0 1

3 2 1 3 4 0 0 1 0 1 1 1 2 2 0 1 1 1 1

 3 3 2 0 0 3 2 2 2 2 2 2 0 2 1 1 0 2 0

 0 1 0 3 0 2 2 0 1 1 0 1 1 0 1 2 0 1 1

0 3 1 1 4 4 0 2 1 0 0 0 2 0 2 2 2 0 2

1 2 2 1 0 2 0 0 0 0 1 0 0 1 0 0 0 0 0

4 4 3 4 1 1 1 0 2 0 1 2 2 0 1 0 0 1 2

 2 2 0 2 1 1 3 2 0 1 0 0 0 0 2 0 2 2 2

2 0 3 0 3 2 0 1 2 0 0 2 0 1 2 2 1 1 1

1 4 4 2 3 0 0 2 1 2 1 2 1 1 2 0 0 2 1

Fig. 6.

A heterogeneous d-strengthening DHF(19; 13, k, 5, 2) with k = (56 41 312 ) and d = (46 313 ).

first 6 rows use 4 symbols to separate, so the corresponding ingredient matrices must have strength at least 4. The remaining rows use 3 symbols to separate, so the corresponding ingredient matrices must have strength at least 3. In [14], we showed that heterogeneity gives great flexibility in construction of measurement matrices using column replacement. The hierarchical structure of the measurement matrices produced by column replacement can also aid in recovery, and be used to support hybrid recovery schemes. We examine this problem next, considering a generalization of hash families that removes the restriction to those strategies based only on (0 , t)- or (1 , t)-recoverability. We also consider the computational investment required to recover the signal. III. H ASH FAMILIES FOR R ECOVERY In order to tackle signal recovery, we require another generalization of hash families. As before, let k = (k1 , . . . , km ) be a tuple of positive integers. An HF (m; n, k) is an m × n array, P = (pij ), in which each cell contains one symbol, and for each row 1  i  m, {pij : 1  j  n}  {, 1, . . . , ki }. The symbol , when present, is interpreted as representing a `missing' entry. When the pattern matrix P = (pij ) is an HF (m; n, k), and for 1  i  m the ingredient matrix Ai is ri × ki with columns indexed by the ki symbols in row i of P other than , the column replacement of A1 , . . . , Am into P is as before, except that when pij = , it is replaced with an all zero column vector of length ri . As we will see, the separating properties of the hash families we use allow us to locate the nonzero coordinates of the signal and hence perform the recovery. The definition of a W -separating hash family encompasses perfect, {w1 , . . . , ws }-separating, and distributing hash families. Therefore, we need only extend the definition of W -separating hash families to include the  symbol. To do so, we allow some of the elements of the multisets in W to be marked with a  superscript to form a set of marked multisets W  ; the multisets in W  are indexed. Then an HF (m; n, k) is W  -separating if, for each {w1 , . . . , ws }  W (with some elements possibly marked), s · whenever C is a set of i=1 wi columns, and · C1 , . . . , Cs is an (indexed) partition of C with |Ci | = wi for 1  i  s then there exists a row that separates C1 , . . . , Cs in which, for 1  j  s, if  appears in a column in Cj then wj is marked. As we will see, to recover the signal, the idea is to effect a separation where a significant coordinate of the signal is present in one class such that any other class does not prevent its recovery. A. Signal Recovery without Noise Theorems 3 and 4 suggest that a recovery scheme based on (0 , t)- or (1 , t)-recoverability can be `lifted' from the ingredient measurement matrices A1 , . . . , Am to the larger measurement matrix B obtained from column replacement. However, such a method appears to have two main drawbacks. First, it is restricted to recovery strategies based on (0 , t)- or (1 , t)-recoverability. Secondly, and perhaps more importantly, it appears to necessitate a large computational investment to recover the signal, given B.

6

In order to overcome these problems, we consider two cases. The positive case arises when the signal is known a priori to be in Rn 0 . The general case arises when the signal can be positive, negative, or zero. In each case we develop a recovery scheme for the matrix B resulting from column replacement that does not depend on any fixed algorithm, but rather on the recovery schemes for the ingredient matrices A1 , . . . , Am . We suppose that P = (pij ) is an HF (m; n, k). For each 1  i  m, we suppose that Ai is an ri × ki measurement matrix that has (0 , t)-recoverability, equipped with a recovery algorithm Ri that determines the unique t-sparse vector zi that solves Ai zi = yi . We further suppose that B is the column replacement of A1 , . . . , Am into P , and that y is the result of sampling an (unknown) t-sparse vector x = (x1 , . . . , xn ) using B . For 1  i  m, the ith row of P induces a partition {Si , Si1 , . . . , Siki } of the column indices {1, . . . , n}, where Si = {j : pij = , 1  j  n} for   {, 1, . . . , ki }. Assume that we have employed the recovery algorithms Ri to find solutions zi . For 1  i  m and   {, 1, . . . , ki }, the partition class Si is discarded if  = , insignificant if  =  and zi = 0, significant positive if zi > 0, and significant negative if zi < 0. For 1  i  m, let wi = (wi1 , . . . , wiki ) where wi = j Si xj . The vector wi can be considered as a projection of x induced by the symbol pattern in row i of P . These facts follow: i · For 1  i  m, by the definition of B and because Bi x = yi , zi = wi is a solution to A zi = yi . i · For 1  i  m, because A has (0 , t)-recoverability and wi is t-sparse (because x is t-sparse), zi = wi is the unique solution to Ai zi = yi , and so Ri returns wi . We now consider the positive case and the general case for recovery in succession. B. Signal Recovery: The Positive Case We establish that in the positive case with t-sparse signals, it suffices to use a separating hash family of suitable strength, along with suitable ingredient matrices. An SHF (m; n, k, {1, t }) separates t + 1 columns into two parts, one part of size one that cannot include the symbol , and the other of size t that may include . Theorem 5: Suppose that P is an SHF (m; n, k, {1, t }). For 1  i  m, let Ai  Rri ×ki be a measurement matrix that has (0 , t)-recoverability equipped with a recovery algorithm Ri that determines the unique t-sparse vector zi that solves Ai zi = yi . Further suppose that B is the column replacement of measurement matrices A1 , . . . , Am into P and that y is the result of sampling an (unknown) t-sparse vector x = (x1 , . . . , xn )  Rn 0 using B . Then the t-sparse solution x to B x = y can be recovered.

Proof: It suffices to determine whether xi is positive or zero for each 1  i  n, because once this is accomplished we can find the values of the positive xi by solving the overdetermined system that remains. For 1  i  m, apply recovery algorithm Ri to find the unique t-sparse vector zi such that Ai zi = yi . We claim that, for 1    n, x is positive if and only if for each i  {1, . . . , m} the partition class that contains  is either significant positive or discarded. Suppose first that x is positive. If Si is a partition class that contains , then either  =  and Si is insignificant or  = , zi = j Si xj  x > 0, and Si is significant positive. Now suppose that x = 0. Let C = {j : xj > 0, 1  j  n}; |C |  t. There must be a row  of P that separates C from {} such that p = . Let  = p . Then   S and S  C = , so S is insignificant. One useful application of Theorem 5 takes the pattern matrix P to be an SHF (m; n, 1, {1, t }), and each Ai to be a 1 × 1 matrix whose only element is 1; in this case, column replacement yields a matrix B isomorphic to P . In P for every column  and every set C of t columns with   C , there is a row in which all columns of C contain , while column  contains 1. Then the measurement matrices Ai have (0 , t)-recoverability and the recovery algorithms Ri are trivial. Hence in these cases, a matrix isomorphic to P itself supports recovery. Theorem 5 leads to a straightforward recovery algorithm. First, Ri is used to solve Ai zi = yi for 1  i  m. Then the classes Sij are classified as positive when zij > 0, discarded when j = , and insignificant when j =  and zij = 0; this can m be done in O( i=1 ki ) time. We need only compute, for each row, the complement of the union of the insignificant classes, and then compute the intersection over all rows of these complements. However, without additional structure this appears to require the examination of each coordinate; hence, this gives an (n) lower bound. It is not difficult, nevertheless, to obtain sublinear recovery times by restricting the hash family; we return to this problem in Section III-D. C. Signal Recovery: The General Case When the signal takes on both positive and negative values, cancellation of positive and negative contributions can yield a zero measurement despite the presence of a signal. Nevertheless, an additional requirement on the structure of the hash family

7

suffices to address this problem, as we show next. Theorem 6: Suppose that P is an SHF (m; n, k, {{, (t + 1 -  ) } : 1    t}). For 1  i  m, let Ai  Rri ×ki be a measurement matrix that has (0 , t)-recoverability equipped with a recovery algorithm Ri that determines the unique t-sparse vector zi that solves Ai zi = yi . Further suppose that B is the column replacement of measurement matrices A1 , . . . , Am into P and that y is the result of sampling an (unknown) t-sparse vector x = (x1 , . . . , xn ) using B . Then the t-sparse solution x to B x = y can be recovered.

Proof: As in the proof of Theorem 5, it suffices to determine whether xi is nonzero or zero for each 1  i  n, because once this is accomplished we can find the values of the nonzero xi by solving the overdetermined system that remains. For 1  i  m, apply recovery algorithm Ri to find the unique t-sparse vector zi such that Ai zi = yi . Let - z+ i = (max(0, zij ) : 1  j  ki ) and zi = (min(0, zij ) : 1  j  ki ). +  A row i of P is maximum positive if ||z+ i ||1  ||zi ||1 for 1  i  m. Let M  {1, . . . , m} index the maximum positive rows. We claim that a coordinate x is positive if and only if, for every   M ,  is in a significant positive class of the partition induced by row . Suppose first that x is positive and let   M . Because  indexes a maximum positive row, the partition class induced by row  that contains  is not discarded and does not contain the index of any negative variable. Thus it is in a significant positive partition class. Now suppose that x  0. Because P is an SHF (m; n, k, {{, (t + 1 -  ) } : 1    t}) and x is t-sparse, there is a row  of P that separates {j : xj > 0, 1  j  n} from {j : xj < 0, 1  j  n}  {} in which the symbol  only appears in a subset of the columns indexed by {j : xj < 0, 1  j  n}  {}. It follows that  is a maximum positive row of P and that the partition class induced by  containing  does not contain the index of any positive coordinate. So   M , but  is not in a significant positive class of the partition induced by row . In the same manner, all negative coordinates can be identified using maximum negative rows.

Again a straightforward recovery algorithm is given by Theorem 6 but, as in the positive case, it naively involves examining each of the n coordinates. D. Sublinear Time Signal Recovery Recovery can be accomplished in time that is sublinear in k when the hash family has suitable structure; we develop a general approach, and one example, here. In each case, for some subset M of the rows of P , sets are identified that must contain the indices of all positive coordinates (the indices of the negative coordinates, if they exist, can be located similarly). Recall from Section III-A, that the positive case arises when the signal is known a priori to be in Rn 0 and the general case arises when the signal can be positive, negative, or zero. In the positive case, M contains all rows and for   M , the candidate indices are V+ = { : p = , or x  Sj and zj > 0}. In the general case, M contains all rows that index maximum positive rows, and for   M , the candidate indices are V+ = { : x  Sj and zj > 0}. In both cases, we are to determine + + M V . In order to avoid the examination of each coordinate, we do not list the members of V explicitly, but rather use + an implicit representation to list the members of M V . First we give an implicit representation of an hash family HF(q + 1; q  , q ), P , where q is a prime power and 2    q . Let {0 , . . . , q-1 } be the elements of the finite field of order q , Fq . Index the rows of P by {}  {0 , . . . , q-1 }. Index the columns of P by the q  polynomials of degree less than  in indeterminate x, with coefficients in Fq . Now the entry of P with row index  and column indexed by polynomial f (x) is determined as f ( ) when   {0 , . . . , q-1 }, and as the coefficient of x-1 in f (x) when  = . By deleting rows, we form an HF(m; q  , q ) for some 1  m  q + 1. An hash family is linear if it is obtained in this way. The separation properties of such an hash family are crucial [1], [5]. For our purposes, the observation of interest is from [15]: if m  ( - 1)w1 w2 + 1, then a linear HF(m; n, q ) is {w1 , w2 }-separating. (This can be established by a simple argument: When two polynomials of degree less than  evaluate to the same value at  different points, they are the same polynomial.) In some cases, fewer rows suffice to ensure separation. In particular, Blackburn and Wild [5] establish that when q is sufficiently large, one needs at most (w1 + w2 - 1) rows; and in [15] specific small separations are examined to determine the set of prime powers for which various numbers of rows less than ( - 1)w1 w2 + 1 suffice. We proceed with the general statement so as not to impose additional conditions. When m  ( - 1)t + 1, P is {1, t}-separating; in addition, every {1, t - 1}-separation is accomplished in at least  rows. t+1 When m  ( - 1) t+1 2  2  + 1, P is {w, t + 1 - w}-separating for each 1  w  t; in addition, every {w, t - w}-separation t+1 t t t+1 is accomplished in at least  rows, because  t+1 2  2  =  2  2  +  2 . Thus in either case, M contains at least  rows of P .

8

+ + | vectors V + = Choose any  rows U = {1 , . . .  }  M . Now consider the sets {V :   U }. Define U |V + + {(g1 , . . . , g ) : gi  {pi  :   Vi } for 1  i  }. Each (g1 , . . . , g )  V defines a unique column of the hash family, corresponding to the unique polynomial L of degree at most  - 1 satisfying L(i ) = gi for 1  i  . Any column that does not arise in this way from a member of V + cannot be the column for a positive coordinate, because in the partition induced by one of the selected maximum rows it is not in a significant positive class. However, columns arising from vectors in V + need not arise from positive coordinates, because we may not have examined all of the rows of M . Nevertheless, we can now generate each of the columns arising from vectors in V + , and check for each whether it occurs in positive classes for all rows of M , not just the  selected. Now |V + | is O(t ), so when t is o(q ), the size of V + is o(n) (because n = q  ). For concreteness, taking q = t for t a prime power, we can permit  to be as large as t -2 . (For the positive case, we can permit  to be as large as t -1 . ) Hence, by restricting the hash family to one that is linear, it is possible to obtain recovery of the signal in sublinear time. In general, a hash family together with its ingredient matrices can be represented more concisely compared to a random measurement matrix for signal recovery. Furthermore, the hash family is an integer matrix, not a matrix of real numbers, and may therefore be easier to encode. When the hash family is linear an implicit representation of it may be used, further compacting its representation. The results of this section provide some evidence that column replacement enables recoverability conditions to be met. In Section IV, we show that it also preserves the basic machinery to deal with noise in the signal.

E. Adding Strengthening As the signal length increases, it is natural to support high sparsity. Yet the techniques developed until this point only preserve sparsity. Strengthening hash families provide a means to increase the level of sparsity supported. Theorem 7: Suppose that P is a d-strengthening SHF(m; n, k, {{, (t + 1 -  )} : 1    t}). For each 1  i  m, we suppose that Ai is an ri × ki measurement matrix that has (1 , di )-recoverability, equipped with a recovery algorithm Ri , that either determines the unique di -sparse vector zi that solves Ai zi = yi or indicates that no such vector exists. Further suppose that B is the column replacement of A1 , . . . , Am into P , and that y is the result of sampling an (unknown) t-sparse vector x = (x1 , . . . , xn ) using B . Then the t-sparse solution x to B x = y can be recovered.

Proof: Again it suffices to locate the nonzero coordinates of x. For 1  i  m, if recovery algorithm Ri returns a solution zi such that ||zi ||1  ||z||1 for any solution z returned by an oracle Rj , then zi is a maximum solution, and row i of P is a maximum row. Because P is a d-strengthening SHF(m; n, k, {{, (t + 1 -  )} : 1    t}), and x is t-sparse, there is a row  of P that separates {j : xj > 0, 1  j  n} from {j : xj < 0, 1  j  n} with the property that at most d symbols appear in the columns indexed by {j : xj = 0, 1  j  n}. So the projected vector w is d -sparse and it is the solution returned by R . By the definition of , ||w ||1 = ||x||1 . It follows that the 1 -norm of any maximum solution is at least ||x||1 . We claim that if Ri returns a maximum solution zi , then zi = wi . Suppose otherwise. Then, because zi is a maximum solution, we have ||zi ||1  ||x||1 . Further, it is clear from the definition of ||wi || that ||wi ||1  ||x||1 . Thus Ai zi = Ai wi , zi is di -sparse, and ||zi ||1  ||wi ||1 , which is a contradiction to the fact that Ai has (1 , di )-recoverability. Having established our claim, we can now use arguments similar to those used in the proof of Theorem 6 to show that a coordinate x is positive (negative) if and only if, for every maximum row in P ,  is in a significant positive (significant negative) class of the partition induced by that row. IV. R ECOVERY
WITH

N OISE

We now treat the recovery of signals with noise. A signal (x1 , . . . , xn ) is (s, t)-almost sparse if there is a set T of at most t coordinate indices such that i{1...,n}\T |xi | < s. Theorem 8: Suppose that P is an SHF(m; n, k, {{, (t + 1 -  )} : 1    t}). For each 1  i  m, we suppose that Ai is an ri × ki measurement matrix, equipped with recovery algorithm Ri , which, when applied to the sample obtained from an (s, t)-almost sparse signal xi , returns a vector zi such that ||zi - xi ||1 < . Further suppose that B is the column replacement of A1 , . . . , Am into P , and that y is the result of sampling an (unknown) (s, t)-almost sparse vector x = (x1 , . . . , xn ) using B .    Then, a (perfectly) t-sparse vector x = (x 1 , . . . , xn ) such that for 1  i  n, |xi | < 2(s + ) if xi = 0, and |xi - xi | < s +    if xi > 0, and such that B x = y, can be recovered. Proof: We provide a sketch first, and then the details. The idea is to write each coordinate of z as a sum of the signal coordinates in T that contribute to it, and of a noise term e that includes both the small contributions from coordinates outside T and the error less than  from the recovery algorithm. For each row  of P , we then split this sum into two parts: one part

9

  containing terms with the same sign as the z coordinate to which they contribute (indexed by sets T and E ), and another part   containing terms with the opposite sign to the z coordinate to which they contribute (indexed by sets T and E ). The key 1 1  observation is that the sum of the terms with indices in T can be approximated by 2 (||x|| - ||z ||) and hence by 2 (q - ||z ||)   because if T is empty then z has norm close to ||x||, and every term with index in T reduces ||z ||. + Let T be a set of at most t coordinate indices such that i{1...,n}\T |xi | < s. Let T = {i  T : xi  0}, T - = {i  T : xi < 0} and q  = iT |xi |. For 1  i  m, apply Ri to yi to find a vector zi such that ||zi - wi ||1 < . For i  {1, . . . , m}, call ||zi ||1 the signature of row i of P and let q be the maximum signature of any row of P . For 1  i  n, we calculate upper and lower estimates u(i) and (i) for xi . For each row index   {1, . . . , m} and each symbol   {1, . . . , k } we define u and  as follows. 1 1 · If z  0, then u = |z | + 2 (q - ||z ||1 ) and  = - 2 (q - ||z ||1 ). 1 1 · If z < 0, then u = 2 (q - ||z ||1 ) and  = -|z | - 2 (q - ||z ||1 ). For each i  {1, . . . , n} define u (i) = u and  (i) =  , where  is the symbol in row  of P such that i  S , and define u(i) = min{u (i) : 1    m} and (i) = max{ (i) : 1    m}. By first examining a row of maximum signature,  we can immediately conclude for each i  {1, . . . , n} either that u(i) = 0 or that (i) = 0. Define a vector x = (x 1 , . . . , xn )   by setting xi = 0 if |ui |, |i |  s + , and otherwise setting xi equal to whichever of u(i) or (i) has the greater absolute value. We claim that x satisfies the required conditions. To establish this claim we prove that, for 1  j  n, (i) for each   {1, . . . , m},  (j ) - (s + ) < xj < u (j ) + (s + ); (ii) there is some   {1, . . . , m} such that  (j ) > -(s + ) if xj  0 and u (j ) < s +  if xj < 0; and (iii) there is some   {1, . . . , m} such that u (j ) - (s + ) < xj if xj  0 and xj <  (j ) + (s + ) if xj < 0. We begin with some observations used throughout the proof. Let  be a row of P . For 1    k , we have z = k  + : zpi  0}  {i  T - : zpi < 0} ( iT S |xi |) + e for some e . Note that  =1 |e |  s + . Let T = {i  T      and let T = T \ T . Further, let E = {  {1, . . . , k } : e , z  0 or e , z < 0} and let E = {1, . . . , k } \ E . For 1    k , we have that       where  = 1 if   E and  = -1 if   E . Summing over the symbols in row  of P , we see      

|z | = 

 S iT 

|xi | - 

 S iT 

|xi | +  |e |

(1)

||z ||1 = q  - 2  - ||z ||1 ) =  

 iT

|xi | +  

  E

|e | -  

  E

|e |

(2)

and it follows that
1  2 (q

iT

 1 |xi | -  2  
 \S iT 

  E

 1 |e | +  2 1 2 
  E

  E

|e | . 1 2 
  E



(3)

Adding (1) to (3), we obtain
  |z | + 1 2 (q - ||z ||1 ) =


 S iT 

|xi | + 



|xi | -



|e | +



|e | +  |e |.



(4)

It follows from (2) that each row of P has signature less than q  + (s + ) and that any row of P that separates T + from T - has signature greater than q  - (s + ). Thus, q  - (s + ) < q < q  + (s + ) and hence
1 2 (q 1  1 1 - ||z ||1 ) - 1 2 (s + ) < 2 (q - ||z ||1 ) < 2 (q - ||z ||1 ) + 2 (s + ).

(5)

Let j  {1, . . . , n}. We next show that (i), (ii) and (iii) hold in the case where xj  0. The proof in the case where xj < 0 is similar. Proof of (i). Let  index any row of P and let S be the partition class induced by row  of P that contains j . Now  (j ) - (s + ) < xj because  (j )  0. If j  / T , then xj < s and xj < u (j ) + (s + ) because u (j )  0. If j  T and 1 z < 0, then xj  iT  |xi | and we see from (3) and (5) that xj < 2 (q - ||z ||1 ) + (s + ). If j  T and z  0, then  1 |xi | and we see from (4) and (5) that xj < |z | + 2 (q - ||z ||1 ) + (s + ). xj  iT  S  Proof of (ii). Let  index a row of P that separates T +  {j } from T - and let S be the partition class induced by row   or T  S = .  of P that contains j . If z  0, then, for 1    k , either iT  S |xi |  |e | and   E  1   . Using this, it follows from (3) and (5) that  (j ) = - 2 (q - ||z ||1 ) > -(s + ). If z < 0, then T  S =  and   E   Furthermore, for   {1, . . . , k } \ { }, either iT  S |xi | < |e | and   E or T  S = . Using these facts, it  follows from (4) and (5) that  (j ) = -|z | - 1 2 (q - ||z ||1 ) > -(s + ).

10

Proof of (iii). Let  index a row of P that separates T + \ {j } from T -  {j } and let S be the partition class induced by row  of P that contains j . If z < 0, then iT  S |xi |  xj . Furthermore, for each symbol   {1, . . . , k } \ { }, either  1   |xi |  |e | and   E or T S = . Then, it follows from (3) and (5) that u (j ) = 2 (q -||z ||1 )-(s+) < xj .  S iT  If z  0, then iT  S |xi |  xj . Furthermore, for each symbol   {1, . . . , k } \ { }, either iT |xi |  |e |  S    and   E or T  S = . Then, it follows from (4) and (5) that u (j ) = |z | + 1 ( q - || z || ) - ( s +  ) < x  1 j. 2 V. C ONCLUSION Hierarchical construction of measurement matrices by column replacement permits the explicit construction of large measurement matrices from small ones. The use of heterogeneous hash families supports the use of a library of smaller ingredient matrices, while the use of strengthening hash families allows the ingredient matrices to be designed for lower sparsity than the larger measurement matrix produced. Perhaps surprisingly, the ingredient measurement matrices need not all employ the same recovery algorithm; rather recovery for the large measurement matrix can use arbitrary routines for recovery that are provided with the ingredient matrices. In this way, computationally intensive recovery methods can be used for the ingredient matrices, which permits the selection of smaller matrices in general, while still enabling recovery for the large measurement matrix. Nevertheless, recovery using the large measurement matrix can be computationally prohibitive without further restrictions. Therefore it is shown that using a standard construction of linear hash families over the finite field, recovery for the large measurement matrix can be effected in sublinear time. Indeed sublinear recovery time can be obtained even when computationally intensive methods are used for each ingredient matrix. A practical implementation of these recovery methods requires that the methods deal effectively with noise in the signal. Suitable restrictions on the hash family and on each ingredient matrix used in column replacement are shown to be sufficient to permit recovery even in the presence of such noise. Measurement matrices that result from one column replacement have been studied here. Because recovery does not depend on the method by which recovery is done for the ingredient matrices, it is possible that the ingredient matrices themselves are constructed by column replacement from even smaller ingredient matrices. The merits and demerits of repeated column replacement deserve further study. ACKNOWLEDGEMENTS The work of D. Horsley and C. J. Colbourn is supported in part by the Australian Research Council through grant DP120103067. R EFERENCES
[1] N. Alon. Explicit construction of exponential sized families of k -independent sets. Discrete Mathematics, 58:191­193, 1986. [2] R. Baraniuk. Compressive sensing. IEEE Signal Processing Magazine, 24:227­234, 2007. [3] R. Berinde, A. C. Gilbert, P. Indyk, H. Karloff, and M. J. Strauss. Combining geometry and combinatorics: A unified approach to sparse signal recovery. In Proceedings of the 46th Annual Allerton Conference on Communication, Control, and Computing, pages 798­805, 2008. [4] S. R. Blackburn, T. Etzion, D. R. Stinson, and G. M. Zaverucha. A bound on the size of separating hash families. Journal of Combinatorial Theory, Series A, 115((7):1246­1256, 2008. [5] S. R. Blackburn and P. R. Wild. Optimal linear perfect hash families. Journal of Combinatorial Theory, Series A, 83:233­250, 1998. [6] E. J. Cand` es. Compressive sampling. In International Congress of Mathematicians, volume 3, pages 1433­1452, 2006. [7] E. J. Cand` es. The restricted isometry property and its implications for compressed sensing. Compte Rendus de l'Academie des Sciences, Series I, 346:589­592, 2008. [8] E. J. Cand` es, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 52:489­509, 2006. [9] E. J. Cand` es and T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory, 51:4203­4215, 2005. [10] E. J. Cand` es and T. Tao. Near optimal signal recovery from random projections: Universal encoding strategies. IEEE Transactions on Information Theory, 52:5406­5425, 2006. [11] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientific Computing, 20(1):33­61, 1998. [12] A. Cohen, W. Dahmen, and R. A. DeVore. Compressed sensing and best k -term approximation. Journal of the American Mathematical Society, 22:211­231, 2009. [13] C. J. Colbourn, D. Horsley, and C. McLean. Compressive sensing matrices and hash families. IEEE Transactions on Communications, 59(7):1840­1845, 2011. [14] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk. Strengthening hash families and compressive sensing. Journal of Discrete Algorithms, 16:170­186, 2012. [15] C. J. Colbourn and A. C. H. Ling. Linear hash families and forbidden configurations. Designs, Codes and Cryptography, 59:25­55, 2009. [16] G. Cormode and S. Muthukrishnan. Combinatorial algorithms for compressed sensing. In Lecture Notes in Computer Science, volume 4056, pages 280­294, 2006. [17] R. A. DeVore. Deterministic constructions of compressed sensing matrices. Journal of Complexity, 23:918­925, 2007. [18] D. L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE Transactions on Information Theory, 47:2845­2862, 2001. [19] M. Elad and A. M. Bruckstein. A generalized uncertainty principle and sparse representation in pairs of bases. IEEE Transactions on Information Theory, 48:2558­2567, 2002. [20] J. J. Fuchs. On sparse representations in arbitrary redundant bases. IEEE Transactions on Information Theory, 50:1341­1344, 2004. [21] J. J. Fuchs. Recovery of exact sparse representations in the presence of bounded noise. IEEE Transactions on Information Theory, 51:3601­3608, 2005. [22] A. C. Gilbert, M. A. Iwen, and M. J. Strauss. Group testing and sparse signal recovery. In Proceedings of the 42nd Asilomar Conference on Signals, Systems, pages 1059­1063, 2008. [23] A. C. Gilbert, M. J. Strauss, J. Tropp, and R. Vershynin. One sketch for all: Fast algorithms for compressed sensing. In Proceedings of the ACM Symposium on Theory of Computing, pages 237­246, 2007.

11

[24] R. Gribonval and M. Nielsen. Sparse representations in unions of bases. IEEE Transactions on Information Theory, 49:3320­3325, 2003. [25] M. A. Iwen. Combinatorial sublinear-time Fourier algorithms. Foundations of Computational Mathematics, 10:303­338, 2010. [26] S. Jafarpour, W. Xu, B. Hassibi, and R. Calderbank. Efficient and robust compressed sensing using optimized expander graphs. IEEE Transactions on Information Theory, 55:4299­4308, 2009. [27] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24:227­234, 1995. [28] D. R. Stinson, Tran Van Trung, and R. Wei. Secure frameproof codes, key distribution patterns, group testing algorithms and related structures. Journal of Statistical Planning and Inference, 86:595­617, 2000. [29] D. R. Stinson, R. Wei, and K. Chen. On generalized separating hash families. Journal of Combinatorial Theory, Series A, 115:105­120, 2008. [30] M. Stojnic, W. Xu, and B. Hassibi. Compressed sensing-probabilistic analysis of a null-space characterization. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, pages 3377­3380, 2008. [31] J. A. Tropp. Recovery of short, complex linear combinations via l1 minimization. IEEE Transactions on Information Theory, 51:1568­1570, 2005. [32] W. Xu and B. Hassibi. Efficient compressive sensing with deterministic guarantees using expander graphs. In Proceedings of IEEE Information Theory Workshop, 2007. [33] Y. Zhang. On theory of compressive sensing via 1 -minimization: Simple derivations and extensions. Technical Report Technical Report CAAM TR08-11, Rice University, 2008.

Covering subsequences by sets of permutations arises in numerous applications. Given a set of permutations that cover a specific set of subsequences, it is of interest not just to know how few permutations can be used, but also to find a set of size equal to or close to the minimum. These permutation construction problems have proved to be computationally challenging; few explicit constructions have been found for small sets of permutations of intermediate length, mostly arising from greedy algorithms. A different strategy is developed here. Starting with a set that covers the specific subsequences required, we determine local changes that can be made in the permutations without losing the required coverage. By selecting these local changes (using linear extensions) so as to make one or more permutations less âimportantâ for coverage, the method attempts to make a permutation redundant so that it can be removed and the set size reduced. A post-optimization method to do this is developed, and preliminary results on sequence covering arrays show that it is surprisingly effective.The construction of covering arrays with the fewest rows remains a challenging problem. Most computational and recursive constructions result in extensive repetition of coverage. While some is necessary, some is not. By reducing the repeated coverage, metaheuristic search techniques typically outperform simpler computational methods, but they have been applied in a limited set of cases. Time constraints often prevent them from finding an array of competitive size. We examine a different approach. Having used a simple computation or construction to find a covering array, we employ a post-optimization technique that repeatedly adjusts the array in an attempt to reduce its number of rows. At every stage the array retains full coverage. We demonstrate its value on a collection of previously best known arrays by eliminating, in some cases, 10% of their rows. In the well-studied case of strength two with twenty factors having ten values each, post-optimization produces a covering array with only 162 rows, improving on a wide variety of computational and combinatorial methods. We identify certain important features of covering arrays for which post-optimization is successful.Search, test, and measurement problems in sparse domains often require the construction of arrays in which every t or fewer columns satisfy a simply stated combinatorial condition. Such t-restriction problems often ask for the construction of an array satisfying the t-restriction while having as few rows as possible. Combinatorial, algebraic, and probabilistic methods have been brought to bear for specific t-restriction problems; yet in most cases they do not succeed in constructing arrays with a number of rows near the minimum, at least when the number of columns is small. To address this, an algorithmic method is proposed that, given an array satisfying a t-restriction, attempts to improve the array by removing rows. The key idea is to determine the necessity of the entry in each cell of the array in meeting the t-restriction, and repeatedly replacing unnecessary entries, with the goal of producing an entire row of unnecessary entries. Such a row can then be deleted, improving the array, and the process can be iterated. For certain t-restrictions, it is shown that by determining conflict graphs, entries that are necessary can nonetheless be changed without violating the t-restriction. This permits a richer set of ways to improve the arrays. The efficacy of these methods is demonstrated via computational results.SaaS (Software-as-a-Service) often uses multi-tenancy architecture (MTA) where tenant developers compose their applications online using the components stored in the SaaS database. Tenant applications need to be tested, and combinatorial testing can be used. While numerous combinatorial testing techniques are available, most of them produce static sequences of test configurations and their goal is often to provide sufficient coverage such as 2-way interaction coverage. But the goal of SaaS testing is to identify those compositions that are faulty for tenant applications. This paper proposes an adaptive test configuration generation algorithm AR (Adaptive Reasoning) that can rapidly identify those faulty combinations so that those faulty combinations cannot be selected by tenant developers for composition. The AR algorithm has been evaluated by both simulation and real experimentation using a MTA SaaS sample running on GAE (Google App Engine). Both the simulation and experiment showed show that the AR algorithm can identify those faulty combinations rapidly. Whenever a new component is submitted to the SaaS database, the AR algorithm can be applied so that any faulty interactions with new components can be identified to continue to support future tenant applications.Partial Covering Arrays: Algorithms and Asymptotics
Kaushik Sarkar1 , Charles J. Colbourn1 , Annalisa De Bonis2 , and Ugo Vaccaro2

arXiv:1605.02131v1 [math.CO] 7 May 2016

2

CIDSE, Arizona State University, U.S.A. Dipartimento di Informatica, University of Salerno, Italy

1

Abstract. A covering array CA(N ; t, k, v ) is an N × k array with entries in {1, 2, . . . , v }, for which every N × t subarray contains each ttuple of {1, 2, . . . , v }t among its rows. Covering arrays find application in interaction testing, including software and hardware testing, advanced materials development, and biological systems. A central question is to determine or bound CAN(t, k, v ), the minimum number N of rows of a CA(N ; t, k, v ). The well known bound CAN(t, k, v ) = O((t - 1)v t log k) is not too far from being asymptotically optimal. Sensible relaxations of the covering requirement arise when (1) the set {1, 2, . . . , v }t need only be contained among the rows of at least (1 - ) k of the N × t subarrays t and (2) the rows of every N × t subarray need only contain a (large) subset of {1, 2, . . . , v }t . In this paper, using probabilistic methods, significant improvements on the covering array upper bound are established for both relaxations, and for the conjunction of the two. In each case, a randomized algorithm constructs such arrays in expected polynomial time.

1

Introduction

Let [n] denote the set {1, 2, . . . , n}. Let N, t, k, and v be integers such that k  t  2 and v  2. Let A be an N × k array where each entry is from the set [v ]. For I = {j1 , . . . , j }  [k ] where j1 < . . . < j , let AI denote the N ×  array in which AI (i, ) = A(i, j ) for 1  i  N and 1   ; AI is the projection of A onto the columns in I . A covering array CA(N ; t, k, v ) is an N × k array A with each entry from ] t [v ] so that for each t-set of columns C  [k t , each t-tuple x  [v ] appears as a row in AC . The smallest N for which a CA(N ; t, k, v ) exists is denoted by CAN(t, k, v ). Covering arrays find important application in software and hardware testing (see [22] and references therein). Applications of covering arrays also arise in experimental testing for advanced materials [4], inference of interactions that regulate gene expression [29], fault-tolerance of parallel architectures [15], synchronization of robot behavior [17], drug screening [30], and learning of boolean functions [11]. Covering arrays have been studied using different nomenclature, as qualitatively independent partitions [13], t-surjective arrays [5], and (k, t)universal sets [19], among others. Covering arrays are closely related to hash families [10] and orthogonal arrays [8].

2

Background and Motivation

The exact or approximate determination of CAN(t, k, v ) is central in applications of covering arrays, but remains an open problem. For fixed t and v , only when t = v = 2 is CAN(t, k, v ) known precisely for infinitely many values of k . Kleitman and Spencer [21] and Katona [20] independently proved that the largest k for -1 which a CA(N ; 2, k, 2) exists satisfies k = N orner, N/2 . When t = 2, Gargano, K and Vaccaro [13] establish that CAN(2, k, v ) = v log k (1 + o(1)). 2 (1)

(We write log for logarithms base 2, and ln for natural logarithms.) Several researchers [2,5,14,16] establish a general asymptotic upper bound on CAN(t, k, v ): CAN(t, k, v )  t-1 log k (1 + o(1)). t log vtv-1 (2)

A slight improvement on (2) has recently been proved [12,28]. An (essentially) equivalent but more convenient form of (2) is: CAN(t, k, v )  (t - 1)v t log k (1 + o(1)). (3)

A lower bound on CAN(t, k, v ) results from the inequality CAN(t, k, v )  v · CAN(t - 1, k - 1, v ) obtained by derivation, together with (1), to establish that CAN(t, k, v )  v t-2 · CAN(2, k - t + 2, v ) = v t-2 · v 2 log(k - t + 2)(1 + o(1)). When t < 1, we obtain: k CAN(t, k, v ) =  (v t-1 log k ). (4) Because (4) ensures that the number of rows in covering arrays can be considerable, researchers have suggested the need for relaxations in which not all interactions must be covered [7,18,23,24] in order to reduce the number of rows. The practical relevance is that each row corresponds to a test to be performed, adding to the cost of testing. For example, an array covers a t-set of columns when it covers each of the v t interactions on this t-set. Hartman and Raskin [18] consider arrays with a fixed number of rows that cover the maximum number of t-sets of columns. A similar question was also considered in [24]. In [23,24] a more refined measure of the (partial) coverage of an N × k array A is introduced. For a given q  [0, 1], let (A, q ) be the number of N × t submatrices of A with the property that at least qv t elements of [v ]t appear in their set of rows; the (q, t)-completeness of A is (A, q )/ k t . Then for practical purposes one wants "high" (q, t)-completeness with few rows. In these works, no theoretical results on partial coverage appear to have been stated; earlier contributions focus on experimental investigations of heuristic construction methods. Our purpose is to initiate a mathematical investigation of arrays offering "partial" coverage. More precisely, we address:

­ Can one obtain a significant improvement on the upper bound (3) if the set [v ]t is only required to be contained among the rows of at least (1 - ) k t subarrays of A of dimension N × t? ­ Can one obtain a significant improvement if, among the rows of every N × t subarray of A, only a (large) subset of [v ]t is required to be contained? ­ Can one obtain a significant improvement if the set [v ]t is only required to be contained among the rows of at least (1 - ) k t subarrays of A of dimension N × t, and among the rows of each of the k t subarrays that remain, a (large) subset of [v ]t is required to be contained? We answer these questions both theoretically and algorithmically in the following sections.

3

Partial Covering Arrays

When 1  m  v t , a partial m-covering array, PCA(N ; t, k, v, m), is an N × k ] array A with each entry from [v ] so that for each t-set of columns C  [k t , at t least m distinct tuples x  [v ] appear as rows in AC . Hence a covering array CA(N ; t, k, v ) is precisely a partial v t -covering array PCA(N ; t, k, v, v t ). Theorem 1. For integers t, k, v , and m where k  t  2, v  2 and 1  m  v t there exists a PCA(N ; t, k, v, m) with ln N ln . Proof. Let r = v t -m+1, and A be a random N ×k array where each entry is cho] sen independently from [v ] with uniform probability. For C  [k t , let BC denote t the event that at least r tuples from [v ] are missing in AC . The probability that r N a particular r-set of tuples from [v ]t is missing in AC is 1 - v . Applying the t
r . union bound to all r-sets of tuples from [v ]t , we obtain Pr[BC ]  v 1- v t r By linearity of expectation, the expected number of t-sets C for which AC misses vt r N at least r tuples from [v ]t is at most k 1- v . When A has at least t t r vt ln (k )( ) t m-1 rows this expected number is less than 1. Therefore, an array A vt ln( m -1 ) ] exists with the required number of rows such that for all C  [k t , AC misses t at most r - 1 tuples from [v ] , i.e. AC covers at least m tuples from [v ]t .
t

k t

vt m-1 vt m-1

.

(5)

N

Theorem 1 can be improved upon using the Lov´ asz local lemma. Lemma 1. (Lov´ asz local lemma; symmetric case) (see [1]) Let A1 , A2 , . . . , An events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at most d, and that Pr[Ai ]  ¯ p for all 1  i  n. If ep(d + 1)  1, then Pr[n i=1 Ai ] > 0.

Lemma 1 provides an upper bound on the probability of a "bad" event in terms of the dependence structure among such bad events, so that there is a guaranteed outcome in which all "bad" events are avoided. This lemma is most useful when there is limited dependence among the "bad" events, as in the following: Theorem 2. For integers t, k, v and m where v, t  2, k  2t and 1  m  v t there exists a PCA(N ; t, k, v, m) with 1 + ln t N ln
k t-1 vt m-1 vt m-1

.

(6)

] Proof. When k  2t, each event BC with C  [k (that is, at least v t - m + 1 t t k-1 k tuples are missing in AC ) is independent of all but at most 1 t-1 < t t-1 [k ] events in {BC : C  t \ {C }}. Applying Lemma 1, Pr[C ([k]) BC ] > 0 when
t

e

vt r

1-

r vt

N

t

k t-1

 1.

(7)

Solve (7) to obtain the required upper bound on N . When m = v t , apply the Taylor series expansion to obtain ln and thereby recover the upper bound (3). Theorem 2 implies: Corollary 1. Given q  [0, 1] and integers 2  t  k , v  2, there exists an N × k array on [v ] with (q, t)-completeness equal to 1 (i.e., maximal), whose number N of rows satisfies 1 + ln t N ln
k t-1 vt qv t -1 vt m-1



1 vt ,

vt qv t -1

.

Rewriting (6), setting r = v t - m + 1, and using the Taylor series expansion r of ln 1 - v t , we get 1 + ln t N ln
k t-1 vt v t -r vt r



v t (t - 1) ln k r

1-

ln r + o(1) . ln k

(8)

Hence when r = v (t - 1) (or equivalently, m = v t - v (t - 1) + 1), there is a partial m-covering array with (v t-1 ln k ) rows. This matches the lower bound (4) asymptotically for covering arrays by missing, in each t-set of columns, no more than v (t - 1) - 1 of the v t possible rows. The dependence of the bound (6) on the number of v -ary t-vectors that must appear in the t-tuples of columns is particularly of interest when test suites are run sequentially until a fault is revealed, as in [3]. Indeed the arguments here may have useful consequences for the rate of fault detection.

Algorithm 1: Moser-Tardos type algorithm for partial m-covering arrays.
Input: Integers N, t, k, v and m where v, t  2, k  2t and 1  m  v t Output: A : a PCA(N ; t, k, v, m) k vt 1+ln t(t- 1)(m-1) Let N := ; t v
ln m-1

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

Construct an N × k array A where each entry is chosen independently and uniformly at random from [v ]; repeat Set covered := true; ] for each column t-set C  [k do t if AC does not cover at least m distinct t-tuples x  [v ]t then Set covered := false; Set missing-column-set := C ; break; end end if covered = false then Choose all the entries in the t columns of missing-column-set independently and uniformly at random from [v ]; end until covered = true ; Output A;

Lemma 1 and hence Theorem 2 have proofs that are non-constructive in nature. Nevertheless, Moser and Tardos [26] provide a randomized algorithm with the same guarantee. Patterned on their method, Algorithm 1 constructs a partial m-covering array with exactly the same number of rows as (6) in expected polynomial time. Indeed, for fixed t, the expected number of times the resampling step (line 13) is repeated is linear in k (see [26] for more details).

4

Almost Partial Covering Arrays

For 0 < < 1, an -almost partial m-covering array, APCA(N ; t, k, v, m, ), is an N × k array A with each entry from [v ] so that for at least (1 - ) k t column [k] t t-sets C  t , AC covers at least m distinct tuples x  [v ] . Again, a covering array CA(N ; t, k, v ) is precisely an APCA(N ; t, k, v, v t , ) when < 1/ k t . Our first result on -almost partial m-covering arrays is the following. Theorem 3. For integers t, k, v, m and real where k  t  2, v  2, 1  m  v t and 0   1, there exists an APCA(N ; t, k, v, m, ) with ln N ln
vt m-1 vt m-1

/ . (9)

Proof. Parallelling the proof of Theorem 1 we compute an upper bound on the ] t expected number of t-sets C  [k t for which AC misses at least r tuples x  [v ] . k When this expected number is at most t , an array A is guaranteed to exist [k] with at least (1 - ) k such that AC misses at most t t-sets of columns C  t t r - 1 distinct tuples x  [v ] . Thus A is an APCA(N ; t, k, v, m, ). To establish the theorem, solve the following for N : k t vt r 1- r vt
N



k . t

When < 1/ k t we recover the bound from Theorem 1 for partial m-covering arrays. In terms of (q, t)-completeness, Theorem 3 yields the following. Corollary 2. For q  [0, 1] and integers 2  t  k , v  2, there exists an N × k array on [v ] with (q, t)-completeness equal to 1 - , with ln N ln
vt m-1 vt m-1

/ .

When m = v t , an -almost covering array exists with N  v t ln v rows. Improvements result by focussing on covering arrays in which the symbols are acted on by a finite group. In this setting, one chooses orbit representatives of rows that collectively cover orbit representatives of t-way interactions under the group action; see [9], for example. Such group actions have been used in direct and computational methods for covering arrays [6,25], and in randomized and derandomized methods [9,27,28]. We employ the sharply transitive action of the cyclic group of order v , adapting the earlier arguments using methods from [28]: Theorem 4. For integers t, k, v and real where k  t  2, v  2 and 0   1 there exists an APCA(N ; t, k, v, v t , ) with N  v t ln v t-1 . (10)

t

Proof. The action of the cyclic group of order v partitions [v ]t into v t-1 orbits, each of length v . Let n = N and let A be an n × k random array v where each entry is chosen independently from the set [v ] with uniform prob] ability. For C  [k t , AC covers the orbit X if at least one tuple x  X is present in AC . The probability that the orbit X is not covered in A is n v n 1- v = 1 - vt1 . Let DC denote the event that AC does not cover t -1 n at least one orbit. Applying the union bound, Pr[DC ]  v t-1 1 - vt1 . By -1 linearity of expectation, the expected number of column t-sets C for which DC n t-1 occurs is at most k 1 - vt1 . As earlier, set this expected value to be -1 t v

at most k t and solve for n. An array exists that covers all orbits in at least (1 - ) k column t-sets. Develop this array over the cyclic group to obtain the t desired array. As in [28], further improvements result by considering a group, like the Frobenius group, that acts sharply 2-transitively on [v ]. When v is a prime power, the Frobenius group is the group of permutations of Fv of the form {x  ax + b : a, b  Fv , a = 0}. Theorem 5. For integers t, k, v and real where k  t  2, v  2, v is a prime power and 0   1 there exists an APCA(N ; t, k, v, v t , ) with N  v t ln 2v t-2 + v.
t- 1

(11)

1 Proof. The action of the Frobenius group partitions [v ]t into v v-- orbits of 1 length v (v - 1) (full orbits) each and 1 orbit of length v (a short orbit). The short orbit consists of tuples of the form (x1 , . . . , xt )  [v ]t where x1 = . . . = xt . -v Let n = vN (v -1) and let A be an n × k random array where each entry is chosen independently from the set [v ] with uniform probability. Our strategy is to construct A so that it covers all full orbits for the required number of arrays ] {AC : C  [k t }. Develop A over the Frobenius group and add v rows of the form (x1 , . . . , xk )  [v ]t with x1 = . . . = xk to obtain an APCA(N ; t, k, v, v t , ) with the desired value of N . Following the lines of the proof of Theorem 4, A covers all full orbits in at least (1 - ) k t column t-sets C when

k v t-1 - 1 t v-1 Because
v t-1 -1 v -1

1-

v-1 v t-1

n



k . t

 2v t-2 for v  2, we obtain the desired bound.

Using group action when m = v t affords useful improvements. Does this improvement extend to cases when m < v t ? Unfortunately, the answer appears to be no. Consider the case for PCA(N ; t, k, v, m) when m  v t using the action of the cyclic group of order v on [v ]t . Let A be a random n × k array over [v ]. When v t - vs + 1  m  v t - v (s - 1) for 1  s  v t-1 , this implies that ] t for all C  [k t , AC misses at most s - 1 orbits of [v ] . Then we obtain that n  1 + ln t
k t-1 v t-1 s

/ ln

v t-1 v t-1 -s

. Developing A over the cyclic group

we obtain a PCA(N ; t, k, v, m) with 1 + ln N v ln
k t-1 v t-1 s

v t-1 v t-1 -s

(12)

Figure 1 compares (12) and (6). In Figure 1a we plot the size of the partial m-covering array as obtained by (12) and (6) for v t - 6v + 1  m  v t and

9

x 10

4

10 Eq. (12) Eq. (6)

6

8

Eq. (12) Eq. (6)

7 N - number of rows N - number of rows 4075 4080 4085 m 4090 4095 4100

6

10

5

5

4

3
4

2 4070

10 1 10

10

2

10 k

3

10

4

(a) t = 6, k = 20, v = 4

(b) t = 6, v = 4, m = v t - v

Fig. 1: Comparison of (12) and (6). Figure (a) compares the sizes of the partial m-covering arrays when v t - 6v + 1  m  v t . Except for m = v t = 4096 the bound from (6) outperforms the bound obtained by assuming group action. Figure (b) shows that for m = v t - v = 4092, (6) outperforms (12) for all values of k . t = 6, k = 20, v = 4. Except when m = v t = 4096, the covering array case, (6) outperforms (12). Similarly, Figure 1b shows that for m = v t - v = 4092, (6) consistently outperforms (12) for all values of k when t = 6, v = 4. We observe similar behavior for different values of t and v . Next we consider even stricter coverage restrictions, combining Theorems 2 and 4. Theorem 6. For integers t, k, v, m and real where k  t  2, v  2, 0   1 k and m  v t + 1 - ln(v/ln 1/(t-1) ) there exists an N × k array A with entries from [v ] such that
] t 1. for each C  [k t , AC covers at least m tuples x  [v ] , t 2. for at least (1 - ) k t column t-sets C , AC covers all tuples x  [v ] ,

3. N = O(v t ln

v t-1

).

Proof. We vertically juxtapose a partial m-covering array and an -almost v t k t covering array. For r = ln(v/ln 1/(t-1) ) and m = v - r + 1, (8) guarantees the existence of a partial m-covering array with v t ln
t v t-1

{1 + o(1)} rows. The-

orem 4 guarantees the existence of an -almost v -covering array with at most t-1 v t ln v rows. Corollary 3. There exists an N × k array A such that: 1. for any t-set of columns C  distinct t-tuples x  [v ]t ,
[k] t

, AC covers at least m  v t + 1 - v (t - 1)

2. for at least 1 -
t

v t- 1 k1/v

k t

column t-sets C , AC covers all the distinct t-tuples

x  [v ] . 3. N = O(v t-1 ln k ). Proof. Apply Theorem 6 with m = v t + 1 -
ln k t ln(v/
1/(t-1) )

ln k ln(v/
1/(t-1) )

. There are at most

- 1 missing t-tuples x  [v ] in the AC for each of the at most k t column t-sets C that do not satisfy the second condition of Theorem 6. To bound from above the number of missing tuples to a certain small function f (t) of t, it
1 f (t)+1 is sufficient that  v t-1 k . Then the number of missing t-tuples x  [v ]t in AC is bounded from above by f (t) whenever is not larger than
t- 1

v

t-1

1 k

t- 1 f (t)+1

(13)
v t-1

On the other hand, in order for the number N = O v t-1 ln

of rows is not (14)

of A to be asymptotically equal to the lower bound (4), it suffices that smaller than v t-1 1 . kv

When f (t) = v (t - 1) - 1, (13) and (14) agree asymptotically, completing the proof. Once again we obtain a size that is O(v t-1 log k ), a goal that has not been reached for covering arrays. This is evidence that even a small relaxation of covering arrays provides arrays of the best sizes one can hope for. Next we consider the efficient construction of the arrays whose existence is ensured by Theorem 6. Algorithm 2 is a randomized method to construct an APCA(N ; t, k, v, m, ) of a size N that is very close to the bound of Theorem 3. By Markov's inequality the condition in line 9 of Algorithm 2 is met with probability at most 1/2. Therefore, the expected number of times the loop in line 2 repeats is at most 2. To prove Theorem 3, t-wise independence among the variables is sufficient. Hence, Algorithm 2 can be derandomized using t-wise independent random variables. We can also derandomize the algorithm using the method of conditional expectation. In this method we construct A by considering the k columns one by one and fixing all N entries of a column. Given a set of already fixed columns, to fix the entries of the next column we consider all possible v N choices, and choose one that provides the maximum conditional expectation of the number of ] column t-sets C  [k such that AC covers at least m tuples x  [v ]t . Because t N v = O(poly(1/ )), this derandomized algorithm constructs the desired array in polynomial time. Similar randomized and derandomized strategies can be applied to construct the array guaranteed by Theorem 4. Together with Algorithm 1 this implies that the array in Theorem 6 is also efficiently constructible.

Algorithm 2: Randomized algorithm for -almost partial m-covering arrays.
Input: Integers N, t, k, v and m where v, t  2, k  2t and 1  m  v t , and real 0< <1 Output: A : an APCA(N ; t, k, v, m, ) vt ln 2(m / -1) Let N := ; vt
ln m-1

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

repeat Construct an N × k array A where each entry is chosen independently and uniformly at random from [v ]; Set isAPCA:= true; Set defectiveCount := 0; ] for each column t-set C  [k do t if AC does not cover at least m distinct t-tuples x  [v ]t then Set defectiveCount := defectiveCount + 1; k if defectiveCount > then t Set isAPCA:= false; break; end end end until isAPCA = true ; Output A;

5

Final Remarks

We have shown that by relaxing the coverage requirement of a covering array somewhat, powerful upper bounds on the sizes of the arrays can be established. Indeed the upper bounds are substantially smaller than the best known bounds for a covering array; they are of the same order as the lower bound for CAN(t, k, v ). As importantly, the techniques not only provide asymptotic bounds but also randomized polynomial time construction algorithms for such arrays. Our approach seems flexible enough to handle variations of these problems. For instance, some applications require arrays that satisfy, for different subsets of columns, different coverage or separation requirements [8]. In [16] several interesting examples of combinatorial problems are presented that can be unified and expressed in the framework of S -constrained matrices. Given a set of vectors ] S each of length t, an N × k matrix M is S -constrained if for every t-set C  [k t , MC contains as a row each of the vectors in S . The parameter to optimize is, as usual, the number of rows of M . One potential direction is to ask for arrays that, in every t-tuple of columns, cover at least m of the vectors in S , or that all vectors in S are covered by all but a small number of t-tuples of columns. Exploiting the structure of the members of S appears to require an extension of the results developed here.

Acknowledgements
Research of KS and CJC was supported in part by the National Science Foundation under Grant No. 1421058.

References
1. Noga Alon and Joel H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics and Optimization. John Wiley & Sons, Inc., Hoboken, NJ, third edition, 2008. 2. B. Becker and H.-U. Simon. How robust is the n-cube? Inform. and Comput., 77:162­178, 1988. 3. Ren´ ee C. Bryce, Yinong Chen, and Charles J. Colbourn. Biased covering arrays for progressive ranking and composition of web services. Int. J. Simulation Process Modelling, 3(1/2):80­87, 2007. 4. J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE Global Research Technical Report, 29:769­781, 2002. 5. Ashok K. Chandra, Lawrence T. Kou, George Markowsky, and Shmuel Zaks. On sets of boolean n-vectors with all k-projections surjective. Acta Informatica, 20(1):103­111, 1983. 6. M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength 3. Des. Codes Crypt., 16:235­242, 1999. 7. Baiqiang Chen and Jian Zhang. Tuple density: a new metric for combinatorial test suites. In Proceedings of the 33rd International Conference on Software Engineering, ICSE 2011, Waikiki, Honolulu , HI, USA, May 21-28, 2011, pages 876­879, 2011. 8. C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121­167, 2004. 9. C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal of Combinatorial Mathematics and Combinatorial Computing, 90:97­115, 2014. 10. Charles J. Colbourn. Covering arrays and hash families. In D. Crnkovi c and V. Tonchev, editors, Information Security, Coding Theory, and Related Combinatorics, NATO Science for Peace and Security Series, pages 99­135. IOS Press, 2011. 11. Peter Damaschke. Adaptive versus nonadaptive attribute-efficient learning. Machine Learning, 41(2):197­215, 2000. 12. N. Franceti´ c and B. Stevens. Asymptotic size of covering arrays: an application of entropy compression. ArXiv e-prints, March 2015. 13. L. Gargano, J. K¨ orner, and U. Vaccaro. Sperner capacities. Graphs and Combinatorics, 9:31­46, 1993. 14. A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds and Poisson approximations. Combinatorics, Probability and Computing, 5:105­ 118, 1996. 15. N. Graham, F. Harary, M. Livingston, and Q.F. Stout. Subcube fault-tolerance in hypercubes. Information and Computation, 102(2):280 ­ 314, 1993. 16. Sylvain Gravier and Bernard Ycart. S-constrained random matrices. DMTCS Proceedings, 0(1), 2006.

17. A. Hartman. Software and hardware testing using combinatorial covering suites. In M. C. Golumbic and I. B.-A. Hartman, editors, Interdisciplinary Applications of Graph Theory, Combinatorics, and Algorithms, pages 237­266. Springer, Norwell, MA, 2005. 18. Alan Hartman and Leonid Raskin. Problems and algorithms for covering arrays. Discrete Mathematics, 284(13):149 ­ 156, 2004. 19. Stasys Jukna. Extremal Combinatorics: With Applications in Computer Science. Springer Publishing Company, Incorporated, 1st edition, 2010. 20. G. O. H. Katona. Two applications (for search theory and truth functions) of Sperner type theorems. Periodica Math., 3:19­26, 1973. 21. D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math., 6:255­262, 1973. 22. D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC Press, 2013. 23. D. R. Kuhn, I. D. Mendoza, R. N. Kacker, and Y. Lei. Combinatorial coverage measurement concepts and applications. In Software Testing, Verification and Validation Workshops (ICSTW), 2013 IEEE Sixth International Conference on, pages 352­361, March 2013. 24. J. R. Maximoff, M. D. Trela, D. R. Kuhn, and R. Kacker. A method for analyzing system state-space coverage within a t-wise testing framework. In 4th Annual IEEE Systems Conference, pages 598­603, 2010. 25. K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin. Des., 13:70­77, 2005. 26. Robin A. Moser and G´ abor Tardos. A constructive proof of the general Lov´ asz local lemma. J. ACM, 57(2):Art. 11, 15, 2010. 27. K. Sarkar and C. J. Colbourn. Two-stage algorithms for covering array construction. submitted for publication. 28. K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv e-prints, March 2016. 29. D. E. Shasha, A. Y. Kouranov, L. V. Lejay, M. F. Chou, and G. M. Coruzzi. Using combinatorial design to study regulation by multiple input signals: A tool for parsimony in the post-genomics era. Plant Physiol., 127:1590­2594, 2001. 30. A. J. Tong, Y. G. Wu, and L. D. Li. Room-temperature phosphorimetry studies of some addictive drugs following dansyl chloride labelling. Talanta, 43(9):14291436, September 1996.

Test Algebra for Combinatorial Testing
Wei-Tek Tsai , Charles J. Colbourn , Jie Luo , Guanqiu Qi , Qingyang Li , Xiaoying Bai
 School

of Computing, Informatics, and Decision Systems Engineering Arizona State University, Tempe, AZ, USA  State Key Laboratory of Software Development Environment School of Computer Science and Engineering, Beihang University, Beijing, China  Department of Computer Science and Technology, INLIST Tsinghua University, Beijing, China {wtsai, colbourn}@asu.edu, luojie@nlsde.buaa.edu.cn {guanqiuq, qingyan2b}@asu.edu, baixy@tsinghua.edu.cn

Abstract--This paper proposes a new algebraic system, Test Algebra (T A), for identifying faults in combinatorial testing for SaaS (Software-as-a-Service) applications. SaaS as a part of cloud computing is a new software delivery model, and mission-critical applications are composed, deployed, and executed in cloud platforms. Testing SaaS applications is a challenging task because new applications need to be tested when they are composed before they can be deployed for execution. Combinatorial testing algorithms can be used to identify faulty configurations and interactions from 2-way all the way to k-way where k is the number of components in the application. The T A defines rules to identify faulty configurations and interactions. Using the rules defined in the T A, a collection of configurations can be tested concurrently in different servers and in any order and the results obtained will be still same due to the algebraic constraints. Index Terms--Combinatorial testing, algebra, SaaS

I. I NTRODUCTION Software-as-a-Service (SaaS) is a new software delivery model. SaaS often supports three features: customization, multi-tenancy architecture (MTA), and scalability. MTA means using one code base to develop multiple tenant applications, and each tenant application essentially is a customization of the base code [12]. A SaaS system often also supports scalability as it can supply additional computing resources when the workload is heavy. Tenants' applications are often customized by using components stored in the SaaS database [14], [1], [11] including GUI, workflow, service, and data components. Once tenant applications are composed, they need to be tested, but a SaaS system can have millions of components, and hundreds of thousands of tenant applications. Testing tenant applications becomes a challenge as new tenant applications and components are added into the SaaS system continuously. New tenant applications are added on a daily basis while other tenant applications are running on the SaaS platform. As new tenant applications are composed, new components are added into the SaaS system. Each tenant application represents a customer for the SaaS system, and thus it needs to be tested. Combinatorial testing is a popular testing technique to test an application with different configurations. It often assumes that each component within a configuration has been tested

already, but interactions among components in the configuration may cause failures. Traditional combinatorial testing techniques often focus on test case generation to detect the presence of faults, but fault location is an active research area. Each configuration needs to be tested, as each configuration represents a tenant application. Traditional combinatorial testing such as AETG [2] can reveal the existence of faults by using few test cases to support t-way coverage for t  2. But knowing the existence of a fault does not indicate which tway interactions are faulty. When the problem size is small, an engineer can identify faults. However, when the problem is large, it can be a challenge to identify faults. As the IT industries moves to Big Data and cloud computing where hundreds of thousand processors are available, potentially, a large number of processors with distributed databases can be used to perform large-scale combinatorial testing. Furthermore, they also provide concurrent and asynchronous computing mechanisms such as MapReduce, automated redundancy and recovery management, automated resource provisioning, and automated migration for scalability. These capabilities provide significant computing power that was not available before. One simple way of performing combinatorial testing in a cloud environment is: 1) Partition the testing tasks; 2) Allocate these testing tasks to different processors in the cloud platform for test execution; 3) Collect results done by these processors. However, this is not efficient as while the number of computing and storage resources have increased significantly, the number of combinations to be considered is still too high. For example, a large SaaS system may have millions of components, and testing all of these combinations can still consume all the resources in a cloud platform. Two ways to improve this approach and both are based on learning from the previous test results:
·

·

Devise a mechanism to merge test results from different processors so that testing results can be merged quickly, and detect any inconsistency in testing; Based on the existing testing results, eliminate any con-

figurations or interactions from future testing. Due to the asynchronous and autonomous nature of cloud computing, test results may arrive asynchronously and autonomously, and this necessitates a new testing framework. This paper proposes a new algebraic system, Test Algebra (TA), to facilitate concurrent combinatorial testing. The key feature of TA is that the algebraic rules follow the combinatorial structure, and thus can track the test results obtained. The TA can then be used to determine whether a tenant application is faulty, and which interactions need to be tested. The TA is an algebraic system in which elements and operations are formally defined. Each element represents a unique component in the SaaS system, and a set of components represents a tenant application. Assuming each component has been tested by developers, testing a tenant application is equivalent to ensuring that there is no t-way interaction faults for t  2 among the elements in a set. The TA uses the principle that if a t-way interaction is faulty, every (t + 1)-way interaction that contains the t-way interaction as a subset is necessarily faulty. The TA provides guidance for the testing process based on test results so far. Each new test result may indicate if additional tests are needed to test a specific configuration. The TA is an algebraic system, primarily intended to track the test results without knowing how these results were obtained. Specifically, it does not record the execution sequence of previously executed test cases. Because of this, it is possible to allocate different configurations to different processors for execution in parallel or in any order, and the test results are merged following the TA rules. The execution order and the merge order do not affect the merged results if the merging follows the TA operation rules. This paper is structured as follows: Section II discusses the related work; Section III proposes TA and shows its details; and Section IV concludes this paper. Appendix provides proofs of TA associativity properties. II. R ELATED W ORK SaaS testing is a new research topic [14], [4], [10]. Using policies and metadata, test cases can be generated to test SaaS applications. Testing can be embedded in the cloud platform where tenant applications are run [14]. Gao proposed a framework for testing cloud applications [4], and proposed a scalability measure for testing cloud application scalability. Another scalability measure was proposed by [10]. Testing all combinations of inputs and preconditions is not feasible, even with a simple product [6], [8]. The number of defects in a software product can be large, and defects occurring infrequently are difficult to find [15]. Combinatorial test design is used to identify a small number of tests needed to get the coverage of important combinations. Combinatorial test design methods enable one to build structure variation into test cases for having greater test coverage with fewer test cases. Determining the presence of faults caused by a small number of interacting elements has been extensively studied

in component-based software testing. When interactions are to be examined, testing involves a combination-based strategy [5]. When every interaction among t or fewer elements is to be tested, methods have been developed that provide pairwise or t-way coverage. Among the early methods, AET G [2] popularized greedy one-test-at-a-time methods for constructing test suites. In the literature, the test suite is usually called a covering array, defined as follows. Suppose that there are k configurable elements, numbered from 1 to k . Suppose that for element c, there are vc valid options. A t-way interaction is a selection of t of the k configurable elements, and a valid option for each. A test selects a valid option for every element, and it covers a t-way interaction if, when one restricts the attention to the t selected elements, each has the same option in the interaction as it does in the test. A covering array of strength t is a collection of tests so that every t-way interaction is covered by at least one of the tests. Covering arrays reveal faults that arise from improper interaction of t or fewer elements [9]. There are numerous computational and mathematical approaches for construction of covering arrays with a number of tests as small as possible [3], [7]. If a t-way interaction causes a fault, then executing all tests of a covering array will reveal the presence of at least one faulty interaction. SaaS testing is interested in identifying those interactions that are faulty including their numbers and locations, as faulty configurations cannot be used in tenant applications. Furthermore, the number and location of faults keep on changing as new components can be added into the SaaS database continuously. By then executing each test, certain interactions are known not to be faulty, while others appear only in tests that reveal faults, and hence may be faulty. At this point, a classification tree analysis builds decision trees for characterizing possible sets of faults. This classification analysis is then used either to permit a system developer to focus on a small collection of possible faults, or to design additional tests to further restrict the set of possible faults. In [16], empirical results demonstrate the effectiveness of this strategy at limiting the possible faulty interactions to a manageable number. Assuming that interactions of more than t elements do not produce faults, a covering array can use few tests to certify that no fault arises from a t-way interaction. The Adaptive Reasoning algorithm (AR) is a strategy to detect faults in SaaS [13]. The algorithm uses earlier test results to generate new test cases to detect faults in tenant applications. It uses three principles:
·

·

·

Principle 1: When a tenant application (or configuration) fails the testing, there is at least one fault (but there may be more) in the tenant configuration. Principle 2: When a tenant application passes the testing, there is no fault in the tenant configuration resulting from a t-way interactions among components in the configuration. Principle 3: Whenever a configuration contains one or more faulty interactions, it is faulty.

III. T EST A LGEBRA Let C be a finite set of components. A configuration is a subset T  C . One is concerned with determining the operational status of configurations. To do this, one can execute certain tests; every test is a configuration, but there may be restrictions on which configurations can be used as tests. If a certain test can be executed, its execution results in an outcome of passed (operational) or failed (faulty). When a test execution yields result, all configurations that are subsets of the test are operational. However, when a test execution yields a faulty result, one only knows that at least one subset causes the fault, but it is unclear which of these subsets caused the failure. Among a set of configurations that may be responsible for faults, the objective is to determine, which cause faults and which do not. To do this, one must identify the set of candidates to be faulty. Because faults are expected to arise from an interaction among relatively few components, one considers t-way interactions. The t-way interactions are It = {U  C : |U | = t}. Hence the goal is to select tests, so that from the execution results of these tests, one can ascertain the status of all t-way interactions for some fixed small value of t. Because interactions and configurations are represented as subsets, one can use set-theoretic operations such as union, and their associated algebraic properties such as commutativity, associativity, and self-absorption. The structure of subsets and supersets also plays a key role. To permit this classification, one can use a valuation function V , so that for every subset S of components, V (S ) indicates the current knowledge about the operational status consistent with the components in S . The focus is on determining V (S ) whenever S is an interaction in I1  · · ·  It . These interactions can have one of five states. · Infeasible (X): For certain interactions, it may happen that no feasible test is permitted to contain this interaction. For example, it may be infeasible to select two GUI components in one configuration such that one says the wall is GREEN but the other says RED. · Faulty (F): If the interaction has been found to be faulty. · Operational (P): Among the rest, if an interaction has appeared in a test whose execution gave an operational result, the interaction cannot be faulty. · Irrelevant (N): For some feasible interactions, it may be the case that certain interactions are not expected to arise, so while it is possible to run a test containing the interaction, there is no requirement to do so. · Unknown (U): If neither of these occurs then the status of the interaction is required but not currently known. Any given stage of testing, an interaction has one of five possible status indicators. These five status indicators are ordered by X F P N U under a relation , and it has a natural interpretation to be explained in a moment. A. Learning from Previous Test Results The motivation for developing an algebra is to automate the deduction of the status of an interaction from the status

of tests and other interactions, particularly in combining the status of two interactions. Specifically, one is often interested in determining V (T1  T2 ) from V (T1 ) and V (T2 ). To do this, a binary operation  on {X, F, P, N, U} can be defined, with operation table as follows:  X F P N U X X X X X X F X F F F F P X F U N U N X F N N N U X F U N U

Using this definition, one can verify that the binary operation  has the following properties of commutativity and associativity. V (T1 )  V (T2 ) = V (T2 )  V (T1 ), V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). Using this operation, one observes that V (T1  T2 ) V (T1 )  V (T2 ). It follows that 1) Every superset of an infeasible interaction is infeasible. 2) Every superset of a failed interaction is failed or infeasible. 3) Every superset of an irrelevant interaction is irrelevant, failed, passed, or infeasible. A set S is an X-implicant if V (S ) = X but whenever S  S , V (S )  X. The X-implicants provide a compact representation for all interactions that are infeasible. Indeed for any interaction T that contains an X-implicant, V (T ) = X. Furthermore, a set S is an F-implicant if V (S ) = F but whenever S  S , V (S )  F. For any interaction T that contains an F-implicant, V (T ) F. In the same way, a set S is an N-implicant if V (S ) = N but whenever S  S , V (S ) = U. For any interaction T that contains an N-implicant, V (T ) N. An analogous statement holds for passed interactions, but here the implication is for subsets. A set S is a P-implicant if V (S ) = P but whenever S  S , V (S ) F. For any interaction T that is contained in a P-implicant, V (T ) = P. Implicants are defined with respect to the current knowledge about the status of interactions. When a t-way interaction is known to be infeasible, failed, or irrelevant, it must contain an X-, F-, or N-implicant. By repeatedly proceeding from tway to (t + 1)-way interactions, then, one avoids the need for any tests for (t + 1)-way interactions that contain any infeasible, failed, or irrelevant t-way interaction. Hence testing typically proceeds by determining the status of the 1-way interactions, then proceeding to 2-way, 3-way, and so on. The operation  is useful in determining the implied status of (t + 1)-way interactions from the computed results for t-way interactions, by examining unions of the t-way and smaller interactions and determining implications of the rule that V (T1  T2 ) V (T1 )  V (T2 ). Moreover, when adding further interactions to consider, all interactions previously tested that passed are contained in a P-implicant, and every (t + 1) interaction contained in one of these interactions can be assigned status P.

For example, V (a, b) = P, V (a, e) = X, and other 2way interactions exist as atomic interaction in TA. Based on the defined  operation, values of t-way interactions can be deduced from the atomic interactions and their contained interactions, such as V (a, b, e) V (a, b)  V (a, e) = X, i.e. V (a, b, e) = X. The 3-way interaction (a, b, c) can have inferred results from 2-way interactions (a, b), (a, c), (b, c). If any contained 2-way interaction has value F, the determining value of 3-way is F, without further testing needed. But if all values of contained 2-way interactions are P, (a, b, c) the interaction needs to be tested. In this case, U needs to be changed to non-U such as F or P, assuming the 3-way is not X or N. B. Changing Test Result Status When testing a configuration with n components, one should test individual components, 2-way interactions, 3-way interactions, all the way to n-way interactions. Since any combination of interactions is relevant in this case, the status of any interaction can be either X, F, P, or U. The status of a configuration is determined by the status of all interactions. 1) If an interaction has status X (F), the configuration has status X (F). 2) If all interactions have status P, the configuration has status P. 3) If some interactions still have status U, further tests are needed. It is important to determine when an interaction with status U can be deduced to have status F or P instead. It can never obtain status X or N once having had status U. To change U to P: An interaction is assigned status P if and only if it is a subset of a test that leads to proper operation. To change U to F: Consider the candidate T , one can conclude that V (T ) = F if there is a test containing T that yields a failed result, but for every other candidate interaction T that appears in this test, V (T ) = P. In other words, the only possible explanation for the failure is the failure of T . C. Matrix Representation Suppose that each individual component passed the testing. Then the operation table starts from 2-way interactions, then enlarges to t-way interactions step by step. During the procedure, many test results can be deduced from the existing results following TA rules. For example, all possible configurations of (a, b, c, d, e, f ) can be expressed in the form of matrix, or operation table. First, we show the operation table for 2-way interactions. The entries in the operation table are symmetric and those on the main diagonal are not necessary. So only half of the entries are shown. As shown in Figure 1, 3-way interactions can be composed by using 2-way interactions and components. Thus, following the TA implication rules, the 3-way interactions operation table is composed based on the results of 2-way combinations. Here, (a, b, c, d, e, f ) has more 3-way interactions than 2-way interactions. As seen in Figure 1, a 3-way interaction can be obtained through different combinations of 2-way interactions

and components. For example, {a, b, c} = {a}  {b, c} = {b}{a, c} = {c}{a, b} = {a, b}{a, c} = {a, b}{b, c} = {a, c}  {b, c}. V (a)  V (b, c) = V (c)  V (a, b) = V (a, b)  V (b, c) = P  P = U. But V (b)  V (a, c) = V (a, b)  V (a, c) = V (b, c)  V (a, c) = P  F = F. As TA defines the order of the five status indicators, the result should be the value with highest order. So V (a, b, c) = F.  a a b c d e f b P c d e F N X P X N F P F f U F P X U

D. Merging Concurrent Testing Results One way to achieve efficient testing is to allocate (overlapping or non-overlapping) tenant applications into different clusters, and each cluster is sent to a different set of servers for execution. Once each cluster completes its execution, the test results can be merged. The testing results of a specific interaction T in different servers should satisfy the following constraints. · If V (T ) = U in one cluster, then in other clusters, the same V (T ) can be either F, P, N, or U. · If V (T ) = N in one cluster, then in other clusters, the same V (T ) can be either F, P, N, or U. · If V (T ) = P in one cluster, then the same V (T ) can be either P, N, or U in all clusters; · If V (T ) = F in one cluster, then in other clusters, the same V (T ) can be F, N, or U. · If V (T ) = X in one cluster, then in other clusters, the same V (T ) can be X only. If these constraints are satisfied, then the testing results can be merged. Otherwise, there must be an error in the testing results. To represent this situation, a new status indicator, error (E), is introduced and E X. We define a binary operation  on {E, X, F, P, N, U}, with operation table as follows:  E X F P N U E E E E E E E X E X E E E E F E E F E F F P E E E P P P N E E F P N U U E E F P U U

 also has the properties of commutativity and associativity. See Appendix for proof of associativity. Using this operation, merging two testing results from two different servers can be defined as Vmerged (T ) = Vcluster1 (T )  Vcluster2 (T ). The merge can be performed in any order due to the commutativity and associativity of , and if the constraints of merge are satisfied and V (T ) = X, F, or P, the results cannot be changed by any further testing or merging of test results unless there are some errors in testing. If V (T ) = E, the testing

 a b c . . . f (a, b) (a, c) . . . (b, c) . . . (e, f )

a (a)

b (a, b) (b)

c (a, c) (b, c) (c)

··· ··· ··· ··· .. .

f (a, f ) (b, f ) (c, f ) . . . (f )

(a, b) (a, b) (a, b) (a, b, c) . . . (a, b, f ) (a, b)

(a, c) (a, c) (a, b, c) (a, c) . . .

··· ··· ··· ··· . . .

(b, c) (a, b, c) (b, c) (b, c) . . .

··· ··· ··· ··· . . .

(e, f ) (a, e, f ) (b, e, f ) (c, e, f ) . . . (e, f ) (a, b, e, f ) (a, c, e, f ) . . . (b, c, e, f ) . . . (e, f )

(a, c, f ) · · · (a, b, c) · · · (a, c) ··· .. .

(b, c, f ) · · · (a, b, c) · · · (a, b, c) · · · . . . . . . (b, c) ··· .. . ··· ··· ··· ··· . . . ··· ··· ··· . . . ··· .. . (e, f ) U U U . . . U U F . . . U . . .

 a b c . . . f (a, b) (a, c) . . . (b, c) . . . (e, f )

a

b P

c F P

··· ··· ··· ··· .. .

f U F P . . .

(a, b) U U U . . . U

(a, c) F F F . . . F F

··· ··· ··· ··· . . . ··· ··· ··· .. .

(b, c) U U U . . . U U F . . .

Fig. 1. 3-way interaction operation table

environment must be corrected and tests executed again after fixing the error(s) in testing. For example, Vcluster1 (a, c, e) = X and Vcluster2 (a, c, e) = F, so Vmerged (a, c, e) = X  F = E. It means that there is something wrong with the tests of interaction (a, c, e), and the problem must be fixed before doing further testing. Following the  associative rule, one can derive the following. V1 (T )  V2 (T )  V3 (T ) = (V1 (T )  V2 (T ))  V3 (T ) = V1 (T )  (V2 (T )  V3 (T )) = V1 (T )  V2 (T )  V3 (T )  V3 (T ) = (V1 (T )  V2 (T ))  (V2 (T )  V3 (T )) = ((V1 (T )  V2 (T ))  V2 (T ))  V3 (T ) = (V3 (T )  V2 (T ))  (V3 (T )  V1 (T )) Thus the  rule allows one to partition the configurations into different sets for different servers to run testing, and these sets do not need to be non-overlapping. In conventional cloud computing operations such as MapReduce, data should not overlap, otherwise incorrect data may be produced. For example, counting the items in a set can be performed by MapReduce, but data allocated to different servers cannot overlap, otherwise items may be counted more than once. In TA, this is not a concern due to the nature of the TA operations

. Once the results are available from each server, the testing results can be merged either incrementally, in parallel, or in any order. Furthermore, test results can be merged repeatedly without changing the final results. Using this approach, one can test say 10 batches of 100 tenant applications each, for a total of 1, 000 tenant applications. Once this is done, another batch of 1, 000 tenant applications can be tested with each 100 tenant application allocated to a server for execution. In this way, after running 100 batches, 100, 000 tenant applications can be evaluated completely. The following example illustrates the testing process of fifteen configurations. According to the definition of configuration testing, all feasible interactions should be tested. For simplicity, assume that only interaction (c, d, f ) is faulty, and only interaction (c, d, e) is infeasible, and all other interactions pass the testing. If one assigns configurations 1, 3, 5, 7, 9, 11, 13, 15 into Server1 , configurations 2, 4, 6, 8, 10, 12, 14 into Server2 , and 4-11 configurations into Server3 . If Server1 and Server3 do their own testing first, Server2 can reuse test results of interactions from them to eliminate interactions that need to be tested. For example, when testing 2-way interactions of configuration (b, c, d, f ) in Server2 , it can reuse the test results of (b, c), (b, d) of configuration (b, c, d, e) from Server3 , (b, f ) of configuration (a, b, c, f )

from Server1 . They are all passed, and it can reuse the test results of (b, c, d) of configuration (a, b, c, d) from Server1 , (b, c, f ) of configuration (a, b, c, f ) from Server1 , (b, d, f ) of configuration (a, b, d, f ) from Server1 , and (c, d, f ) of configuration (a, c, d, f ) from Server3 . Because (c, d, f ) is faulty, it can deduce that 4-way interaction (b, c, d, f ) is also faulty. For the sets of configuration that are overlapping, their returned test results from different servers are the same. The merged results of these results also stay the same. Not only interactions, sets of configurations, CS1 , CS2 , . . . , CSK can be allocated to different processors (or clusters) for testing, and the test results can then be merged. The sets can be non-overlapping or overlapping, and the merge process can be arbitrary. For example, say the result of CSi is RCSi , the merge process can be (· · · ((((RCS1 + RCS2 ) + RCS3 ) + RCS4 ) + · · · + RCSK ), or (· · · ((((RCSK + RCSk-1 ) + RCSk-2 ) + · · · + RCS1 ), or any other sequence that includes all RCSi , for i = 1 to K . This is true because RCS is simply a set of V (Tj ) for any intercation Tj in the configuration CSi . (a,b,c,d) (a,b,c,e) (a,b,c,f) (a,b,d,e) (a,b,d,f) (a,b,e,f) (a,c,d,e) (a,c,d,f) (a,c,e,f) (a,d,e,f) (b,c,d,e) (b,c,d,f) (b,c,e,f) (b,d,e,f) (c,d,e,f) Server1 P P P P P X F P P X F P P X P P P X F P P X Server2 P Server3

IV. C ONCLUSION This paper proposes TA to address SaaS combinatorial testing. The TA provides a foundation for concurrent combinatorial testing. The TA has two operations and test results can have five states with a priority. By using the TA operations, many combinatorial tests can be eliminated as the TA identifies those interactions that need not be tested. Also the TA defines operation rules to merge test results done by different processors, so that combinatorial tests can be done in a concurrent manner. The TA rules ensure that either merged results are consistent or a testing error has been detected so that retest is needed. In this way, large-scale combinatorial testing can be carried out in a cloud platform with a large number of processors to perform test execution in parallel to identify faulty interactions. ACKNOWLEDGMENT

This project is sponsored by U.S. National Science Foundation project DUE 0942453 and National Science Foundation M erged Results China (No.61073003), National Basic Research Program of China (No.2011CB302505), and the State Key Laboratory of P Software Development Environment (No. SKLSDE-2012ZXP 18), and Fujitsu Laboratory. P P R EFERENCES P [1] X. Bai, M. Li, B. Chen, W.-T. Tsai, and J. Gao. Cloud Testing Tools. In P Proceedings of IEEE 6th International Symposium on Service Oriented X System Engineering (SOSE), pages 1­12, Irvine, CA, USA, 2011. F [2] D. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG System: An Approach to Testing Based on Combinatorial Design. P Journal of IEEE Transactions on Software Engineering, 23:437­444, P 1997. X [3] C. J. Colbourn. Covering arrays and hash families. In D. Crnkovic and V. D. Tonchev, editors, Information Security, Coding Theory and Related F Combinatorics, volume 29 of NATO Science for Peace and Security P Series - D: Information and Communication Security. IOS Press, 2011. P [4] J. Gao, X. Bai, and W.-T. Tsai. SaaS Performance and Scalability Evaluation in Cloud. In Proceedings of The 6th IEEE International X
Symposium on Service Oriented System Engineering, SOSE '11, 2011. [5] M. Grindal, J. Offutt, and S. F. Andler. Combination Testing Strategies: A Survey. Software Testing, Verification, and Reliability, 15:167­199, 2005. [6] C. Kaner, J. Falk, and H. Q. Nguyen. Testing Computer Software, 2nd Edition. Wiley, New York, NY, USA, 1999. [7] V. V. Kuliamin and A. A. Petukhov. A Survey of Methods for Constructing Covering Arrays. Journal of Program Computer Software, 37(3):121­146, may 2011. [8] T. Muller and D. Friedenberg. Certified Tester Foundation Level Syllabus. Journal of International Software Testing Qualifications Board. [9] A. A. Porter, C. Yilmaz, A. M. Memon, D. C. Schmidt, and B. Natarajan. Skoll: A Process and Infrastructure for Distributed Continuous Quality Assurance. IEEE Transactions on Software Engineering, 33(8):510­525, 2007. [10] W.-T. Tsai, Y. Huang, X. Bai, and J. Gao. Scalable Architecture for SaaS. In Proceedings of 15th IEEE International Symposium on Object Component Service-oriented Real-time Distributed Computing, ISORC '12, Apr. 2012. [11] W.-T. Tsai, Y. Huang, and Q. Shao. Testing the Scalability of SaaS Applications. In Proceedings of IEEE International Conference on ServiceOriented Computing and Applications (SOCA), pages 1­4, Irvine, CA, USA, 2011. [12] W.-T. Tsai, Y. Huang, Q. Shao, and X. Bai. Data Partitioning and Redundancy Management for Robust Multi-Tenancy SaaS. International Journal of Software and Informatics (IJSI), 4(3):437­471, 2010.

E. Modified Testing Process Perform 2-way interaction testing first. Before going on to 3way interaction testing, use the results of 2-way testing to eliminate cases. The testing process stops when finishing testing all t-way interactions. The analysis of t-way interactions is based on the P T Rs of all (t - i)-way interactions for 1  i < t. The superset of infeasible, irrelevant, and faulty test cases do not need to be tested. The test results of the superset can be obtained by TA operations and must be infeasible, irrelevant, or faulty. But the superset of test cases with unknown indicator must be tested. In this way, a large repeating testing workload can be reduced. For n components, all t-way interactions for t  2 are composed by 2-way, 3-way, ..., t-way interactions. In n components combinatorial testing, the number of 2-way interactions is equal to n 2 . In general, the number of t-way interactions is equal to n t . More interactions are treated when n n > , which happens when t  n 2 . The total number t t-1 t of interactions examined is i=2 n . i

[13] W.-T. Tsai, Q. Li, C. J. Colbourn, and X. Bai. Adaptive Fault Detection for Testing Tenant Applications in Multi-Tenancy SaaS Systems. In Proceedings of IEEE International Conference on Cloud Engineering (IC2E), March 2013. [14] W.-T. Tsai, Q. Shao, and W. Li. OIC: Ontology-based Intelligent Customization Framework for SaaS. In Proceedings of International Conference on Service Oriented Computing and Applications(SOCA'10), Perth, Australia, Dec. 2010. [15] Wikipedia. Software Testing, 2013. [16] C. Yilmaz, M. B. Cohen, and A. Porter. Covering Arrays for Efficient Fault Characterization in Complex Configuration Spaces. In Proceedings of the 2004 ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA '04, pages 45­54, New York, NY, USA, 2004. ACM.

A PPENDIX The associativity of binary operation . V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). Proof: We will prove this property in the following cases. (1) At least one of V (T1 ), V (T2 ), and V (T3 ) is X. Without loss of generality, suppose that V (T1 ) = X, then according to the operation table of , V (T1 )  (V (T2 )  V (T3 )) = X  (V (T2 )  V (T3 )) = X, (V (T1 )  V (T2 ))  V (T3 ) = (X  V (T2 ))  V (T3 ) = X  V (T3 ) = X. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (2) V (T1 ), V (T2 ), and V (T3 ) are not X and at least one of V (T1 ), V (T2 ), and V (T3 ) is F. Without loss of generality, suppose that V (T1 ) = F, then according to the operation table of , the value of V (T2 )  V (T3 ) can only be F, N or U. So V (T1 )  (V (T2 )  V (T3 )) = F  (V (T2 )  V (T3 )) = F, (V (T1 )  V (T2 ))  V (T3 ) = (F  V (T2 ))  V (T3 ) = F  V (T3 ) = F. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (3) V (T1 ), V (T2 ), and V (T3 ) are not X or F and at least one of V (T1 ), V (T2 ), and V (T3 ) is N. Without loss of generality, suppose that V (T1 ) = N, then according to the operation table of , the value of V (T2 )  V (T3 ) can only be N or U. So V (T1 )  (V (T2 )  V (T3 )) = N  (V (T2 )  V (T3 )) = N, (V (T1 )  V (T2 ))  V (T3 ) = (N  V (T2 ))  V (T3 ) = N  V (T3 ) = N. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (4) V (T1 ), V (T2 ), and V (T3 ) are not X, F or N. In this case, V (T1 ), V (T2 ), and V (T3 ) can only be P or U. According to the operation table of , the value of V (T1 )  V (T2 ) and V (T2 )  V (T3 ) are U. So V (T1 )  (V (T2 )  V (T3 )) = V (T1 )  U = U, (V (T1 )  V (T2 ))  V (T3 ) = U  V (T3 ) = U. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). The associativity of binary operation . V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ). Proof: We will prove this property in the following cases. (1) One of V1 (T ), V2 (T ), and V3 (T ) is E. Without loss of generality, suppose that V1 (T ) = E, then according to the operation table of , V1 (T )(V2 (T )V3 (T )) = E(V2 (T ) V3 (T )) = E, (V1 (T )  V2 (T ))  V3 (T ) = (E  V2 (T ))  V3 (T ) = E  V3 (T ) = E. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ).

(2) V1 (T ), V2 (T ), and V3 (T ) are not E, and there is a pair of V1 (T ), V2 (T ), and V3 (T ) does not satisfy the constrains. Without loss of generality, suppose that V1 (T ) and V2 (T ) does not satisfy the constrains, then according to the operation table of , V1 (T )  V2 (T ) = E. So (V1 (T )  V2 (T ))  V3 (T ) = E  V3 (T ) = E. Since V1 (T ) and V2 (T ) does not satisfy the constrains, there can be two cases: (a) one of them is X and the other is not, or (b) one of them is P and the other is F. (a) If V1 (T ) = X, then V2 (T )  V3 (T ) cannot be X because V2 (T ) cannot be X. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. If V2 (T ) = X, then V2 (T )  V3 (T ) = X can only be E or X. Since V1 (T ) cannot be X, V1 (T )  (V2 (T )  V3 (T )) = E. (b) If V1 (T ) = P and V2 (T ) = F, then V2 (T )  V3 (T ) can only be E or F. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. If V1 (T ) = F and V2 (T ) = P, then V2 (T )  V3 (T ) can only be E or P. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ). (3) V1 (T ), V2 (T ), and V3 (T ) are not E, and V1 (T ), V2 (T ), and V3 (T ) satisfy the constrains. (a) One of V1 (T ), V2 (T ), and V3 (T ) is X. Without loss of generality, suppose that V1 (T ) = X, then V2 (T ) = V3 (T ) = X. So V1 (T )  (V2 (T )  V3 (T )) = X  (X  X) = X  X = X and (V1 (T )  V2 (T ))  V3 (T ) = (X  X)  X = X  X = X. (b) V1 (T ), V2 (T ), and V3 (T ) are not X, and one of V1 (T ), V2 (T ), and V3 (T ) is F. Without loss of generality, suppose that V1 (T ) = F, then V2 (T ) and V3 (T ) can only be F, N, or U. According to operation table of , V2 (T )  V3 (T ) can only be F, N, or U, and V1 (T )  V2 (T ) can only be F. So V1 (T )  (V2 (T )  V3 (T )) = F  (V2 (T )  V3 (T )) = F and (V1 (T )  V2 (T ))  V3 (T ) = F  V3 (T ) = F. (c) V1 (T ), V2 (T ), and V3 (T ) are not X or F, and one of V1 (T ), V2 (T ), and V3 (T ) is P. Without loss of generality, suppose that V1 (T ) = P, then V2 (T ) and V3 (T ) can only be P, N, or U. According to operation table of , V2 (T )  V3 (T ) can only be P, N, or U, and V1 (T )  V2 (T ) can only be F. So V1 (T )  (V2 (T )  V3 (T )) = P  (V2 (T )  V3 (T )) = P and (V1 (T )  V2 (T ))  V3 (T ) = P  V3 (T ) = P. (d) V1 (T ), V2 (T ), and V3 (T ) are not X, F or P, and one of V1 (T ), V2 (T ), and V3 (T ) is U. Without loss of generality, suppose that V1 (T ) = U, then V2 (T ) and V3 (T ) can only be N, or U. According to operation table of , V2 (T )  V3 (T ) can only be N, or U, and V1 (T )  V2 (T ) can only be U. So V1 (T )  (V2 (T )  V3 (T )) = U  (V2 (T )  V3 (T )) = U and (V1 (T )  V2 (T ))  V3 (T ) = U  V3 (T ) = U. (e) V1 (T ), V2 (T ), and V3 (T ) are N. V1 (T )  (V2 (T )  V3 (T )) = N  (N  N) = N  N = N and (V1 (T )  V2 (T ))  V3 (T ) = (N  N)  N = N  N = N. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ).

Test Algebra for Combinatorial Testing
Wei-Tek Tsai , Charles J. Colbourn , Jie Luo , Guanqiu Qi , Qingyang Li , Xiaoying Bai
 School

of Computing, Informatics, and Decision Systems Engineering Arizona State University, Tempe, AZ, USA  State Key Laboratory of Software Development Environment School of Computer Science and Engineering, Beihang University, Beijing, China  Department of Computer Science and Technology, INLIST Tsinghua University, Beijing, China {wtsai, colbourn}@asu.edu, luojie@nlsde.buaa.edu.cn {guanqiuq, qingyan2b}@asu.edu, baixy@tsinghua.edu.cn

Abstract--This paper proposes a new algebraic system, Test Algebra (T A), for identifying faults in combinatorial testing for SaaS (Software-as-a-Service) applications. SaaS as a part of cloud computing is a new software delivery model, and mission-critical applications are composed, deployed, and executed in cloud platforms. Testing SaaS applications is a challenging task because new applications need to be tested when they are composed before they can be deployed for execution. Combinatorial testing algorithms can be used to identify faulty configurations and interactions from 2-way all the way to k-way where k is the number of components in the application. The T A defines rules to identify faulty configurations and interactions. Using the rules defined in the T A, a collection of configurations can be tested concurrently in different servers and in any order and the results obtained will be still same due to the algebraic constraints. Index Terms--Combinatorial testing, algebra, SaaS

I. I NTRODUCTION Software-as-a-Service (SaaS) is a new software delivery model. SaaS often supports three features: customization, multi-tenancy architecture (MTA), and scalability. MTA means using one code base to develop multiple tenant applications, and each tenant application essentially is a customization of the base code [12]. A SaaS system often also supports scalability as it can supply additional computing resources when the workload is heavy. Tenants' applications are often customized by using components stored in the SaaS database [14], [1], [11] including GUI, workflow, service, and data components. Once tenant applications are composed, they need to be tested, but a SaaS system can have millions of components, and hundreds of thousands of tenant applications. Testing tenant applications becomes a challenge as new tenant applications and components are added into the SaaS system continuously. New tenant applications are added on a daily basis while other tenant applications are running on the SaaS platform. As new tenant applications are composed, new components are added into the SaaS system. Each tenant application represents a customer for the SaaS system, and thus it needs to be tested. Combinatorial testing is a popular testing technique to test an application with different configurations. It often assumes that each component within a configuration has been tested

already, but interactions among components in the configuration may cause failures. Traditional combinatorial testing techniques often focus on test case generation to detect the presence of faults, but fault location is an active research area. Each configuration needs to be tested, as each configuration represents a tenant application. Traditional combinatorial testing such as AETG [2] can reveal the existence of faults by using few test cases to support t-way coverage for t  2. But knowing the existence of a fault does not indicate which tway interactions are faulty. When the problem size is small, an engineer can identify faults. However, when the problem is large, it can be a challenge to identify faults. As the IT industries moves to Big Data and cloud computing where hundreds of thousand processors are available, potentially, a large number of processors with distributed databases can be used to perform large-scale combinatorial testing. Furthermore, they also provide concurrent and asynchronous computing mechanisms such as MapReduce, automated redundancy and recovery management, automated resource provisioning, and automated migration for scalability. These capabilities provide significant computing power that was not available before. One simple way of performing combinatorial testing in a cloud environment is: 1) Partition the testing tasks; 2) Allocate these testing tasks to different processors in the cloud platform for test execution; 3) Collect results done by these processors. However, this is not efficient as while the number of computing and storage resources have increased significantly, the number of combinations to be considered is still too high. For example, a large SaaS system may have millions of components, and testing all of these combinations can still consume all the resources in a cloud platform. Two ways to improve this approach and both are based on learning from the previous test results:
·

·

Devise a mechanism to merge test results from different processors so that testing results can be merged quickly, and detect any inconsistency in testing; Based on the existing testing results, eliminate any con-

figurations or interactions from future testing. Due to the asynchronous and autonomous nature of cloud computing, test results may arrive asynchronously and autonomously, and this necessitates a new testing framework. This paper proposes a new algebraic system, Test Algebra (TA), to facilitate concurrent combinatorial testing. The key feature of TA is that the algebraic rules follow the combinatorial structure, and thus can track the test results obtained. The TA can then be used to determine whether a tenant application is faulty, and which interactions need to be tested. The TA is an algebraic system in which elements and operations are formally defined. Each element represents a unique component in the SaaS system, and a set of components represents a tenant application. Assuming each component has been tested by developers, testing a tenant application is equivalent to ensuring that there is no t-way interaction faults for t  2 among the elements in a set. The TA uses the principle that if a t-way interaction is faulty, every (t + 1)-way interaction that contains the t-way interaction as a subset is necessarily faulty. The TA provides guidance for the testing process based on test results so far. Each new test result may indicate if additional tests are needed to test a specific configuration. The TA is an algebraic system, primarily intended to track the test results without knowing how these results were obtained. Specifically, it does not record the execution sequence of previously executed test cases. Because of this, it is possible to allocate different configurations to different processors for execution in parallel or in any order, and the test results are merged following the TA rules. The execution order and the merge order do not affect the merged results if the merging follows the TA operation rules. This paper is structured as follows: Section II discusses the related work; Section III proposes TA and shows its details; and Section IV concludes this paper. Appendix provides proofs of TA associativity properties. II. R ELATED W ORK SaaS testing is a new research topic [14], [4], [10]. Using policies and metadata, test cases can be generated to test SaaS applications. Testing can be embedded in the cloud platform where tenant applications are run [14]. Gao proposed a framework for testing cloud applications [4], and proposed a scalability measure for testing cloud application scalability. Another scalability measure was proposed by [10]. Testing all combinations of inputs and preconditions is not feasible, even with a simple product [6], [8]. The number of defects in a software product can be large, and defects occurring infrequently are difficult to find [15]. Combinatorial test design is used to identify a small number of tests needed to get the coverage of important combinations. Combinatorial test design methods enable one to build structure variation into test cases for having greater test coverage with fewer test cases. Determining the presence of faults caused by a small number of interacting elements has been extensively studied

in component-based software testing. When interactions are to be examined, testing involves a combination-based strategy [5]. When every interaction among t or fewer elements is to be tested, methods have been developed that provide pairwise or t-way coverage. Among the early methods, AET G [2] popularized greedy one-test-at-a-time methods for constructing test suites. In the literature, the test suite is usually called a covering array, defined as follows. Suppose that there are k configurable elements, numbered from 1 to k . Suppose that for element c, there are vc valid options. A t-way interaction is a selection of t of the k configurable elements, and a valid option for each. A test selects a valid option for every element, and it covers a t-way interaction if, when one restricts the attention to the t selected elements, each has the same option in the interaction as it does in the test. A covering array of strength t is a collection of tests so that every t-way interaction is covered by at least one of the tests. Covering arrays reveal faults that arise from improper interaction of t or fewer elements [9]. There are numerous computational and mathematical approaches for construction of covering arrays with a number of tests as small as possible [3], [7]. If a t-way interaction causes a fault, then executing all tests of a covering array will reveal the presence of at least one faulty interaction. SaaS testing is interested in identifying those interactions that are faulty including their numbers and locations, as faulty configurations cannot be used in tenant applications. Furthermore, the number and location of faults keep on changing as new components can be added into the SaaS database continuously. By then executing each test, certain interactions are known not to be faulty, while others appear only in tests that reveal faults, and hence may be faulty. At this point, a classification tree analysis builds decision trees for characterizing possible sets of faults. This classification analysis is then used either to permit a system developer to focus on a small collection of possible faults, or to design additional tests to further restrict the set of possible faults. In [16], empirical results demonstrate the effectiveness of this strategy at limiting the possible faulty interactions to a manageable number. Assuming that interactions of more than t elements do not produce faults, a covering array can use few tests to certify that no fault arises from a t-way interaction. The Adaptive Reasoning algorithm (AR) is a strategy to detect faults in SaaS [13]. The algorithm uses earlier test results to generate new test cases to detect faults in tenant applications. It uses three principles:
·

·

·

Principle 1: When a tenant application (or configuration) fails the testing, there is at least one fault (but there may be more) in the tenant configuration. Principle 2: When a tenant application passes the testing, there is no fault in the tenant configuration resulting from a t-way interactions among components in the configuration. Principle 3: Whenever a configuration contains one or more faulty interactions, it is faulty.

III. T EST A LGEBRA Let C be a finite set of components. A configuration is a subset T  C . One is concerned with determining the operational status of configurations. To do this, one can execute certain tests; every test is a configuration, but there may be restrictions on which configurations can be used as tests. If a certain test can be executed, its execution results in an outcome of passed (operational) or failed (faulty). When a test execution yields result, all configurations that are subsets of the test are operational. However, when a test execution yields a faulty result, one only knows that at least one subset causes the fault, but it is unclear which of these subsets caused the failure. Among a set of configurations that may be responsible for faults, the objective is to determine, which cause faults and which do not. To do this, one must identify the set of candidates to be faulty. Because faults are expected to arise from an interaction among relatively few components, one considers t-way interactions. The t-way interactions are It = {U  C : |U | = t}. Hence the goal is to select tests, so that from the execution results of these tests, one can ascertain the status of all t-way interactions for some fixed small value of t. Because interactions and configurations are represented as subsets, one can use set-theoretic operations such as union, and their associated algebraic properties such as commutativity, associativity, and self-absorption. The structure of subsets and supersets also plays a key role. To permit this classification, one can use a valuation function V , so that for every subset S of components, V (S ) indicates the current knowledge about the operational status consistent with the components in S . The focus is on determining V (S ) whenever S is an interaction in I1  · · ·  It . These interactions can have one of five states. · Infeasible (X): For certain interactions, it may happen that no feasible test is permitted to contain this interaction. For example, it may be infeasible to select two GUI components in one configuration such that one says the wall is GREEN but the other says RED. · Faulty (F): If the interaction has been found to be faulty. · Operational (P): Among the rest, if an interaction has appeared in a test whose execution gave an operational result, the interaction cannot be faulty. · Irrelevant (N): For some feasible interactions, it may be the case that certain interactions are not expected to arise, so while it is possible to run a test containing the interaction, there is no requirement to do so. · Unknown (U): If neither of these occurs then the status of the interaction is required but not currently known. Any given stage of testing, an interaction has one of five possible status indicators. These five status indicators are ordered by X F P N U under a relation , and it has a natural interpretation to be explained in a moment. A. Learning from Previous Test Results The motivation for developing an algebra is to automate the deduction of the status of an interaction from the status

of tests and other interactions, particularly in combining the status of two interactions. Specifically, one is often interested in determining V (T1  T2 ) from V (T1 ) and V (T2 ). To do this, a binary operation  on {X, F, P, N, U} can be defined, with operation table as follows:  X F P N U X X X X X X F X F F F F P X F U N U N X F N N N U X F U N U

Using this definition, one can verify that the binary operation  has the following properties of commutativity and associativity. V (T1 )  V (T2 ) = V (T2 )  V (T1 ), V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). Using this operation, one observes that V (T1  T2 ) V (T1 )  V (T2 ). It follows that 1) Every superset of an infeasible interaction is infeasible. 2) Every superset of a failed interaction is failed or infeasible. 3) Every superset of an irrelevant interaction is irrelevant, failed, passed, or infeasible. A set S is an X-implicant if V (S ) = X but whenever S  S , V (S )  X. The X-implicants provide a compact representation for all interactions that are infeasible. Indeed for any interaction T that contains an X-implicant, V (T ) = X. Furthermore, a set S is an F-implicant if V (S ) = F but whenever S  S , V (S )  F. For any interaction T that contains an F-implicant, V (T ) F. In the same way, a set S is an N-implicant if V (S ) = N but whenever S  S , V (S ) = U. For any interaction T that contains an N-implicant, V (T ) N. An analogous statement holds for passed interactions, but here the implication is for subsets. A set S is a P-implicant if V (S ) = P but whenever S  S , V (S ) F. For any interaction T that is contained in a P-implicant, V (T ) = P. Implicants are defined with respect to the current knowledge about the status of interactions. When a t-way interaction is known to be infeasible, failed, or irrelevant, it must contain an X-, F-, or N-implicant. By repeatedly proceeding from tway to (t + 1)-way interactions, then, one avoids the need for any tests for (t + 1)-way interactions that contain any infeasible, failed, or irrelevant t-way interaction. Hence testing typically proceeds by determining the status of the 1-way interactions, then proceeding to 2-way, 3-way, and so on. The operation  is useful in determining the implied status of (t + 1)-way interactions from the computed results for t-way interactions, by examining unions of the t-way and smaller interactions and determining implications of the rule that V (T1  T2 ) V (T1 )  V (T2 ). Moreover, when adding further interactions to consider, all interactions previously tested that passed are contained in a P-implicant, and every (t + 1) interaction contained in one of these interactions can be assigned status P.

For example, V (a, b) = P, V (a, e) = X, and other 2way interactions exist as atomic interaction in TA. Based on the defined  operation, values of t-way interactions can be deduced from the atomic interactions and their contained interactions, such as V (a, b, e) V (a, b)  V (a, e) = X, i.e. V (a, b, e) = X. The 3-way interaction (a, b, c) can have inferred results from 2-way interactions (a, b), (a, c), (b, c). If any contained 2-way interaction has value F, the determining value of 3-way is F, without further testing needed. But if all values of contained 2-way interactions are P, (a, b, c) the interaction needs to be tested. In this case, U needs to be changed to non-U such as F or P, assuming the 3-way is not X or N. B. Changing Test Result Status When testing a configuration with n components, one should test individual components, 2-way interactions, 3-way interactions, all the way to n-way interactions. Since any combination of interactions is relevant in this case, the status of any interaction can be either X, F, P, or U. The status of a configuration is determined by the status of all interactions. 1) If an interaction has status X (F), the configuration has status X (F). 2) If all interactions have status P, the configuration has status P. 3) If some interactions still have status U, further tests are needed. It is important to determine when an interaction with status U can be deduced to have status F or P instead. It can never obtain status X or N once having had status U. To change U to P: An interaction is assigned status P if and only if it is a subset of a test that leads to proper operation. To change U to F: Consider the candidate T , one can conclude that V (T ) = F if there is a test containing T that yields a failed result, but for every other candidate interaction T that appears in this test, V (T ) = P. In other words, the only possible explanation for the failure is the failure of T . C. Matrix Representation Suppose that each individual component passed the testing. Then the operation table starts from 2-way interactions, then enlarges to t-way interactions step by step. During the procedure, many test results can be deduced from the existing results following TA rules. For example, all possible configurations of (a, b, c, d, e, f ) can be expressed in the form of matrix, or operation table. First, we show the operation table for 2-way interactions. The entries in the operation table are symmetric and those on the main diagonal are not necessary. So only half of the entries are shown. As shown in Figure 1, 3-way interactions can be composed by using 2-way interactions and components. Thus, following the TA implication rules, the 3-way interactions operation table is composed based on the results of 2-way combinations. Here, (a, b, c, d, e, f ) has more 3-way interactions than 2-way interactions. As seen in Figure 1, a 3-way interaction can be obtained through different combinations of 2-way interactions

and components. For example, {a, b, c} = {a}  {b, c} = {b}{a, c} = {c}{a, b} = {a, b}{a, c} = {a, b}{b, c} = {a, c}  {b, c}. V (a)  V (b, c) = V (c)  V (a, b) = V (a, b)  V (b, c) = P  P = U. But V (b)  V (a, c) = V (a, b)  V (a, c) = V (b, c)  V (a, c) = P  F = F. As TA defines the order of the five status indicators, the result should be the value with highest order. So V (a, b, c) = F.  a a b c d e f b P c d e F N X P X N F P F f U F P X U

D. Merging Concurrent Testing Results One way to achieve efficient testing is to allocate (overlapping or non-overlapping) tenant applications into different clusters, and each cluster is sent to a different set of servers for execution. Once each cluster completes its execution, the test results can be merged. The testing results of a specific interaction T in different servers should satisfy the following constraints. · If V (T ) = U in one cluster, then in other clusters, the same V (T ) can be either F, P, N, or U. · If V (T ) = N in one cluster, then in other clusters, the same V (T ) can be either F, P, N, or U. · If V (T ) = P in one cluster, then the same V (T ) can be either P, N, or U in all clusters; · If V (T ) = F in one cluster, then in other clusters, the same V (T ) can be F, N, or U. · If V (T ) = X in one cluster, then in other clusters, the same V (T ) can be X only. If these constraints are satisfied, then the testing results can be merged. Otherwise, there must be an error in the testing results. To represent this situation, a new status indicator, error (E), is introduced and E X. We define a binary operation  on {E, X, F, P, N, U}, with operation table as follows:  E X F P N U E E E E E E E X E X E E E E F E E F E F F P E E E P P P N E E F P N U U E E F P U U

 also has the properties of commutativity and associativity. See Appendix for proof of associativity. Using this operation, merging two testing results from two different servers can be defined as Vmerged (T ) = Vcluster1 (T )  Vcluster2 (T ). The merge can be performed in any order due to the commutativity and associativity of , and if the constraints of merge are satisfied and V (T ) = X, F, or P, the results cannot be changed by any further testing or merging of test results unless there are some errors in testing. If V (T ) = E, the testing

 a b c . . . f (a, b) (a, c) . . . (b, c) . . . (e, f )

a (a)

b (a, b) (b)

c (a, c) (b, c) (c)

··· ··· ··· ··· .. .

f (a, f ) (b, f ) (c, f ) . . . (f )

(a, b) (a, b) (a, b) (a, b, c) . . . (a, b, f ) (a, b)

(a, c) (a, c) (a, b, c) (a, c) . . .

··· ··· ··· ··· . . .

(b, c) (a, b, c) (b, c) (b, c) . . .

··· ··· ··· ··· . . .

(e, f ) (a, e, f ) (b, e, f ) (c, e, f ) . . . (e, f ) (a, b, e, f ) (a, c, e, f ) . . . (b, c, e, f ) . . . (e, f )

(a, c, f ) · · · (a, b, c) · · · (a, c) ··· .. .

(b, c, f ) · · · (a, b, c) · · · (a, b, c) · · · . . . . . . (b, c) ··· .. . ··· ··· ··· ··· . . . ··· ··· ··· . . . ··· .. . (e, f ) U U U . . . U U F . . . U . . .

 a b c . . . f (a, b) (a, c) . . . (b, c) . . . (e, f )

a

b P

c F P

··· ··· ··· ··· .. .

f U F P . . .

(a, b) U U U . . . U

(a, c) F F F . . . F F

··· ··· ··· ··· . . . ··· ··· ··· .. .

(b, c) U U U . . . U U F . . .

Fig. 1. 3-way interaction operation table

environment must be corrected and tests executed again after fixing the error(s) in testing. For example, Vcluster1 (a, c, e) = X and Vcluster2 (a, c, e) = F, so Vmerged (a, c, e) = X  F = E. It means that there is something wrong with the tests of interaction (a, c, e), and the problem must be fixed before doing further testing. Following the  associative rule, one can derive the following. V1 (T )  V2 (T )  V3 (T ) = (V1 (T )  V2 (T ))  V3 (T ) = V1 (T )  (V2 (T )  V3 (T )) = V1 (T )  V2 (T )  V3 (T )  V3 (T ) = (V1 (T )  V2 (T ))  (V2 (T )  V3 (T )) = ((V1 (T )  V2 (T ))  V2 (T ))  V3 (T ) = (V3 (T )  V2 (T ))  (V3 (T )  V1 (T )) Thus the  rule allows one to partition the configurations into different sets for different servers to run testing, and these sets do not need to be non-overlapping. In conventional cloud computing operations such as MapReduce, data should not overlap, otherwise incorrect data may be produced. For example, counting the items in a set can be performed by MapReduce, but data allocated to different servers cannot overlap, otherwise items may be counted more than once. In TA, this is not a concern due to the nature of the TA operations

. Once the results are available from each server, the testing results can be merged either incrementally, in parallel, or in any order. Furthermore, test results can be merged repeatedly without changing the final results. Using this approach, one can test say 10 batches of 100 tenant applications each, for a total of 1, 000 tenant applications. Once this is done, another batch of 1, 000 tenant applications can be tested with each 100 tenant application allocated to a server for execution. In this way, after running 100 batches, 100, 000 tenant applications can be evaluated completely. The following example illustrates the testing process of fifteen configurations. According to the definition of configuration testing, all feasible interactions should be tested. For simplicity, assume that only interaction (c, d, f ) is faulty, and only interaction (c, d, e) is infeasible, and all other interactions pass the testing. If one assigns configurations 1, 3, 5, 7, 9, 11, 13, 15 into Server1 , configurations 2, 4, 6, 8, 10, 12, 14 into Server2 , and 4-11 configurations into Server3 . If Server1 and Server3 do their own testing first, Server2 can reuse test results of interactions from them to eliminate interactions that need to be tested. For example, when testing 2-way interactions of configuration (b, c, d, f ) in Server2 , it can reuse the test results of (b, c), (b, d) of configuration (b, c, d, e) from Server3 , (b, f ) of configuration (a, b, c, f )

from Server1 . They are all passed, and it can reuse the test results of (b, c, d) of configuration (a, b, c, d) from Server1 , (b, c, f ) of configuration (a, b, c, f ) from Server1 , (b, d, f ) of configuration (a, b, d, f ) from Server1 , and (c, d, f ) of configuration (a, c, d, f ) from Server3 . Because (c, d, f ) is faulty, it can deduce that 4-way interaction (b, c, d, f ) is also faulty. For the sets of configuration that are overlapping, their returned test results from different servers are the same. The merged results of these results also stay the same. Not only interactions, sets of configurations, CS1 , CS2 , . . . , CSK can be allocated to different processors (or clusters) for testing, and the test results can then be merged. The sets can be non-overlapping or overlapping, and the merge process can be arbitrary. For example, say the result of CSi is RCSi , the merge process can be (· · · ((((RCS1 + RCS2 ) + RCS3 ) + RCS4 ) + · · · + RCSK ), or (· · · ((((RCSK + RCSk-1 ) + RCSk-2 ) + · · · + RCS1 ), or any other sequence that includes all RCSi , for i = 1 to K . This is true because RCS is simply a set of V (Tj ) for any intercation Tj in the configuration CSi . (a,b,c,d) (a,b,c,e) (a,b,c,f) (a,b,d,e) (a,b,d,f) (a,b,e,f) (a,c,d,e) (a,c,d,f) (a,c,e,f) (a,d,e,f) (b,c,d,e) (b,c,d,f) (b,c,e,f) (b,d,e,f) (c,d,e,f) Server1 P P P P P X F P P X F P P X P P P X F P P X Server2 P Server3

IV. C ONCLUSION This paper proposes TA to address SaaS combinatorial testing. The TA provides a foundation for concurrent combinatorial testing. The TA has two operations and test results can have five states with a priority. By using the TA operations, many combinatorial tests can be eliminated as the TA identifies those interactions that need not be tested. Also the TA defines operation rules to merge test results done by different processors, so that combinatorial tests can be done in a concurrent manner. The TA rules ensure that either merged results are consistent or a testing error has been detected so that retest is needed. In this way, large-scale combinatorial testing can be carried out in a cloud platform with a large number of processors to perform test execution in parallel to identify faulty interactions. ACKNOWLEDGMENT

This project is sponsored by U.S. National Science Foundation project DUE 0942453 and National Science Foundation M erged Results China (No.61073003), National Basic Research Program of China (No.2011CB302505), and the State Key Laboratory of P Software Development Environment (No. SKLSDE-2012ZXP 18), and Fujitsu Laboratory. P P R EFERENCES P [1] X. Bai, M. Li, B. Chen, W.-T. Tsai, and J. Gao. Cloud Testing Tools. In P Proceedings of IEEE 6th International Symposium on Service Oriented X System Engineering (SOSE), pages 1­12, Irvine, CA, USA, 2011. F [2] D. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG System: An Approach to Testing Based on Combinatorial Design. P Journal of IEEE Transactions on Software Engineering, 23:437­444, P 1997. X [3] C. J. Colbourn. Covering arrays and hash families. In D. Crnkovic and V. D. Tonchev, editors, Information Security, Coding Theory and Related F Combinatorics, volume 29 of NATO Science for Peace and Security P Series - D: Information and Communication Security. IOS Press, 2011. P [4] J. Gao, X. Bai, and W.-T. Tsai. SaaS Performance and Scalability Evaluation in Cloud. In Proceedings of The 6th IEEE International X
Symposium on Service Oriented System Engineering, SOSE '11, 2011. [5] M. Grindal, J. Offutt, and S. F. Andler. Combination Testing Strategies: A Survey. Software Testing, Verification, and Reliability, 15:167­199, 2005. [6] C. Kaner, J. Falk, and H. Q. Nguyen. Testing Computer Software, 2nd Edition. Wiley, New York, NY, USA, 1999. [7] V. V. Kuliamin and A. A. Petukhov. A Survey of Methods for Constructing Covering Arrays. Journal of Program Computer Software, 37(3):121­146, may 2011. [8] T. Muller and D. Friedenberg. Certified Tester Foundation Level Syllabus. Journal of International Software Testing Qualifications Board. [9] A. A. Porter, C. Yilmaz, A. M. Memon, D. C. Schmidt, and B. Natarajan. Skoll: A Process and Infrastructure for Distributed Continuous Quality Assurance. IEEE Transactions on Software Engineering, 33(8):510­525, 2007. [10] W.-T. Tsai, Y. Huang, X. Bai, and J. Gao. Scalable Architecture for SaaS. In Proceedings of 15th IEEE International Symposium on Object Component Service-oriented Real-time Distributed Computing, ISORC '12, Apr. 2012. [11] W.-T. Tsai, Y. Huang, and Q. Shao. Testing the Scalability of SaaS Applications. In Proceedings of IEEE International Conference on ServiceOriented Computing and Applications (SOCA), pages 1­4, Irvine, CA, USA, 2011. [12] W.-T. Tsai, Y. Huang, Q. Shao, and X. Bai. Data Partitioning and Redundancy Management for Robust Multi-Tenancy SaaS. International Journal of Software and Informatics (IJSI), 4(3):437­471, 2010.

E. Modified Testing Process Perform 2-way interaction testing first. Before going on to 3way interaction testing, use the results of 2-way testing to eliminate cases. The testing process stops when finishing testing all t-way interactions. The analysis of t-way interactions is based on the P T Rs of all (t - i)-way interactions for 1  i < t. The superset of infeasible, irrelevant, and faulty test cases do not need to be tested. The test results of the superset can be obtained by TA operations and must be infeasible, irrelevant, or faulty. But the superset of test cases with unknown indicator must be tested. In this way, a large repeating testing workload can be reduced. For n components, all t-way interactions for t  2 are composed by 2-way, 3-way, ..., t-way interactions. In n components combinatorial testing, the number of 2-way interactions is equal to n 2 . In general, the number of t-way interactions is equal to n t . More interactions are treated when n n > , which happens when t  n 2 . The total number t t-1 t of interactions examined is i=2 n . i

[13] W.-T. Tsai, Q. Li, C. J. Colbourn, and X. Bai. Adaptive Fault Detection for Testing Tenant Applications in Multi-Tenancy SaaS Systems. In Proceedings of IEEE International Conference on Cloud Engineering (IC2E), March 2013. [14] W.-T. Tsai, Q. Shao, and W. Li. OIC: Ontology-based Intelligent Customization Framework for SaaS. In Proceedings of International Conference on Service Oriented Computing and Applications(SOCA'10), Perth, Australia, Dec. 2010. [15] Wikipedia. Software Testing, 2013. [16] C. Yilmaz, M. B. Cohen, and A. Porter. Covering Arrays for Efficient Fault Characterization in Complex Configuration Spaces. In Proceedings of the 2004 ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA '04, pages 45­54, New York, NY, USA, 2004. ACM.

A PPENDIX The associativity of binary operation . V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). Proof: We will prove this property in the following cases. (1) At least one of V (T1 ), V (T2 ), and V (T3 ) is X. Without loss of generality, suppose that V (T1 ) = X, then according to the operation table of , V (T1 )  (V (T2 )  V (T3 )) = X  (V (T2 )  V (T3 )) = X, (V (T1 )  V (T2 ))  V (T3 ) = (X  V (T2 ))  V (T3 ) = X  V (T3 ) = X. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (2) V (T1 ), V (T2 ), and V (T3 ) are not X and at least one of V (T1 ), V (T2 ), and V (T3 ) is F. Without loss of generality, suppose that V (T1 ) = F, then according to the operation table of , the value of V (T2 )  V (T3 ) can only be F, N or U. So V (T1 )  (V (T2 )  V (T3 )) = F  (V (T2 )  V (T3 )) = F, (V (T1 )  V (T2 ))  V (T3 ) = (F  V (T2 ))  V (T3 ) = F  V (T3 ) = F. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (3) V (T1 ), V (T2 ), and V (T3 ) are not X or F and at least one of V (T1 ), V (T2 ), and V (T3 ) is N. Without loss of generality, suppose that V (T1 ) = N, then according to the operation table of , the value of V (T2 )  V (T3 ) can only be N or U. So V (T1 )  (V (T2 )  V (T3 )) = N  (V (T2 )  V (T3 )) = N, (V (T1 )  V (T2 ))  V (T3 ) = (N  V (T2 ))  V (T3 ) = N  V (T3 ) = N. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (4) V (T1 ), V (T2 ), and V (T3 ) are not X, F or N. In this case, V (T1 ), V (T2 ), and V (T3 ) can only be P or U. According to the operation table of , the value of V (T1 )  V (T2 ) and V (T2 )  V (T3 ) are U. So V (T1 )  (V (T2 )  V (T3 )) = V (T1 )  U = U, (V (T1 )  V (T2 ))  V (T3 ) = U  V (T3 ) = U. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). The associativity of binary operation . V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ). Proof: We will prove this property in the following cases. (1) One of V1 (T ), V2 (T ), and V3 (T ) is E. Without loss of generality, suppose that V1 (T ) = E, then according to the operation table of , V1 (T )(V2 (T )V3 (T )) = E(V2 (T ) V3 (T )) = E, (V1 (T )  V2 (T ))  V3 (T ) = (E  V2 (T ))  V3 (T ) = E  V3 (T ) = E. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ).

(2) V1 (T ), V2 (T ), and V3 (T ) are not E, and there is a pair of V1 (T ), V2 (T ), and V3 (T ) does not satisfy the constrains. Without loss of generality, suppose that V1 (T ) and V2 (T ) does not satisfy the constrains, then according to the operation table of , V1 (T )  V2 (T ) = E. So (V1 (T )  V2 (T ))  V3 (T ) = E  V3 (T ) = E. Since V1 (T ) and V2 (T ) does not satisfy the constrains, there can be two cases: (a) one of them is X and the other is not, or (b) one of them is P and the other is F. (a) If V1 (T ) = X, then V2 (T )  V3 (T ) cannot be X because V2 (T ) cannot be X. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. If V2 (T ) = X, then V2 (T )  V3 (T ) = X can only be E or X. Since V1 (T ) cannot be X, V1 (T )  (V2 (T )  V3 (T )) = E. (b) If V1 (T ) = P and V2 (T ) = F, then V2 (T )  V3 (T ) can only be E or F. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. If V1 (T ) = F and V2 (T ) = P, then V2 (T )  V3 (T ) can only be E or P. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ). (3) V1 (T ), V2 (T ), and V3 (T ) are not E, and V1 (T ), V2 (T ), and V3 (T ) satisfy the constrains. (a) One of V1 (T ), V2 (T ), and V3 (T ) is X. Without loss of generality, suppose that V1 (T ) = X, then V2 (T ) = V3 (T ) = X. So V1 (T )  (V2 (T )  V3 (T )) = X  (X  X) = X  X = X and (V1 (T )  V2 (T ))  V3 (T ) = (X  X)  X = X  X = X. (b) V1 (T ), V2 (T ), and V3 (T ) are not X, and one of V1 (T ), V2 (T ), and V3 (T ) is F. Without loss of generality, suppose that V1 (T ) = F, then V2 (T ) and V3 (T ) can only be F, N, or U. According to operation table of , V2 (T )  V3 (T ) can only be F, N, or U, and V1 (T )  V2 (T ) can only be F. So V1 (T )  (V2 (T )  V3 (T )) = F  (V2 (T )  V3 (T )) = F and (V1 (T )  V2 (T ))  V3 (T ) = F  V3 (T ) = F. (c) V1 (T ), V2 (T ), and V3 (T ) are not X or F, and one of V1 (T ), V2 (T ), and V3 (T ) is P. Without loss of generality, suppose that V1 (T ) = P, then V2 (T ) and V3 (T ) can only be P, N, or U. According to operation table of , V2 (T )  V3 (T ) can only be P, N, or U, and V1 (T )  V2 (T ) can only be F. So V1 (T )  (V2 (T )  V3 (T )) = P  (V2 (T )  V3 (T )) = P and (V1 (T )  V2 (T ))  V3 (T ) = P  V3 (T ) = P. (d) V1 (T ), V2 (T ), and V3 (T ) are not X, F or P, and one of V1 (T ), V2 (T ), and V3 (T ) is U. Without loss of generality, suppose that V1 (T ) = U, then V2 (T ) and V3 (T ) can only be N, or U. According to operation table of , V2 (T )  V3 (T ) can only be N, or U, and V1 (T )  V2 (T ) can only be U. So V1 (T )  (V2 (T )  V3 (T )) = U  (V2 (T )  V3 (T )) = U and (V1 (T )  V2 (T ))  V3 (T ) = U  V3 (T ) = U. (e) V1 (T ), V2 (T ), and V3 (T ) are N. V1 (T )  (V2 (T )  V3 (T )) = N  (N  N) = N  N = N and (V1 (T )  V2 (T ))  V3 (T ) = (N  N)  N = N  N = N. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ).

Abstract:
Covering array generation is a key issue in combinatorial testing. A number of researchers have been applying greedy algorithms for covering array construction. A greedy framework has been built to integrate most greedy algorithms and evaluate new approaches derived from this framework. However, this framework is affected by multiple factors, which makes its deployment and optimization very challenging. In order to identify the best configuration, we propose a search method that combines pairwise coverage with either base choice or hill climbing techniques. We conduct three different groups of experiments based on six decisions of the greedy framework. The influence of these decisions and their interactions are studied systematically, and the selected greedy algorithm for covering array generation is shown to be better than the existing greedy algorithms.Given v, t, and m, does there exist a partial Steiner triple system of order v with t triples whose triples can be ordered so that any m consecutive triples are pairwise disjoint? Given v, t, and m1, m2, . . . , ms with  t=âsi=1mit=âi=1smi  , does there exist a partial Steiner triple system with t triples whose triples can be partitioned into partial parallel classes of sizes m1, . . . , ms? An affirmative answer to the first question gives an affirmative answer to the second when mi â¤ m for each  iâ{1,2,â¦,s}iâ{1,2,â¦,s}  . These questions arise in the analysis of erasure codes for disk arrays and that of codes for unipolar communication, respectively. A complete solution for the first problem is given when m is at most  13(vâ(9v)2/3)+O(v1/3)13(vâ(9v)2/3)+O(v1/3)  .Electronic Journal of Differential Equations, Vol. 2012 (2012), No. 96, pp. 1­11. ISSN: 1072-6691. URL: http://ejde.math.txstate.edu or http://ejde.math.unt.edu ftp ejde.math.txstate.edu

EXISTENCE AND ASYMPTOTIC BEHAVIOR OF SOLUTIONS TO THE GENERALIZED DAMPED BOUSSINESQ EQUATION
YINXIA WANG

Abstract. We consider the Cauchy problem for the n-dimensional generalized damped Boussinesq equation. Based on decay estimates of solutions to the corresponding linear equation, we define a solution space with time weighted norms. Under small condition on the initial value, the existence and asymptotic behavior of global solutions in the corresponding Sobolev spaces are established by the contraction mapping principle.

1. Introduction We study the Cauchy problem of the generalized damped Boussinesq equation in n space dimensions utt - autt - 2but - 3 u +  2 u - u = f (u) with the initial value t=0: u = u0 (x), ut = u1 (x). (1.2) (1.1)

Here u = u(x, t) is the unknown function of x = (x1 , · · · , xn )  Rn and t > 0, a, b, ,  are positive constants. The nonlinear term f (u) = O(u1+ ) and  is a positive integer. The first initial boundary value problem for utt - autt - 2but - 3 u +  2 u - u =  (u2 ) (1.3)

in a unit circle was investigated in [16], where a, b, ,  are positive constants and  is a constant. The existence and the uniqueness of strong solution was established and the solutions were constructed in the form of series in the small parameter present in the initial conditions. The long-time asymptotics was also obtained in the explicit form. In [1], the authors considered the initial-boundary value problem for (1.3) in the unit ball B  R3 , similar results were established. It is well-known that the equation (1.3) is closely contacted with many wave equations. For example, the equation (which we call the Bq equation) utt - uxx + uxxxx = (u2 )xx ,
2000 Mathematics Subject Classification. 35L30, 35L75. Key words and phrases. Generalized damped equation; global solution; asymptotic behavior. c 2012 Texas State University - San Marcos. Submitted May 31, 2012. Published June 10, 2012.
1

2

Y. WANG

EJDE-2012/96

which was derived by Boussinesq in 1872 to describe shallow water waves. The improved Bq equation(which we call IBq equation) is utt - uxx - uxxtt = (u2 )xx . A modification of the IBq equation analogous of the MKdV equation yields utt - uxx - uxxtt = (u3 )xx , which we call the IMBq equation (see [5]). (1.1) is a higher order wave equation. In [8], we considered the Cauchy problem for the Cahn-Hilliard equation with inertial term. Combining high frequency, low frequency technique and energy methods, we obtained global existence and asymptotic behavior of solutions. Wang, Liu and Zhang [13] investigated a fourth wave equation that is of the regularity-loss type. Based on the decay property of the solution operators, global existence and asymptotic behavior of solutions are obtained. For global existence and asymptotic behavior of solutions to higher order wave equations, we refer to [2]-[3] and [6]-[15] and references therein. The main purpose of this paper is to establish global existence and asymptotic behavior of solutions to (1.1), (1.2) by using the contraction mapping principle. Firstly, we consider the decay property of the following linear equation utt - autt - 2but - 3 u +  2 u - u = 0. (1.4)

We obtain the following decay estimate of solutions to (1.4) associated with initial condition (1.2),
k x u(t) L2

 C (1 + t)- 4 - 2 - 2 ( u0

n

k

1

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(1.5)

(k  s + 2),
h x ut (t) L2

 C (1 + t)- 4 - 2 -1 ( u0

n

h

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(1.6)

(h  s) Based on the estimates (1.5) and (1.6), we define a solution space with time weighted norms. Then global existence and asymptotic behavior of classical solutions to (1.1), (1.2) are obtained by using the contraction mapping principle. We give notation which is used in this paper. Let F [u] denote the Fourier transform of u defined by u ^( ) = F [u] =
Rn

e-i·x u(x)dx,

and we denote its inverse transform by F -1 . For 1  p  , Lp = Lp (Rn ) denotes the usual Lebesgue space with the s norm · Lp . The usual Sobolev space of s is defined by Hp = (I - )-s/2 Lp s = with the norm f Hp (I - )s/2 f Lp ; the homogeneous Sobolev space of s is s - s/  p = (-) 2 Lp with the norm f H s = (-)s/2 f Lp ; especially defined by H p s s s  s . Moreover, we know that H s = Lp  H p H s = H2 ,H = H for s  0. p 2 Finally, in this paper, we denote every positive constant by the same symbol C or c without confusion. [·] is the Gauss symbol. The article is organized as follows. In Section 2 we derive the solution formula of our semi-linear problem. We study the decay property of the solution operators appearing in the solution formula in section 3. Then, in Section 4, we discuss the linear problem and show the decay estimates. Finally, we prove global existence

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

3

and asymptotic behavior of solutions for the Cauchy problem (1.1), (1.2) in Section 5. 2. Solution formula The aim of this section is to derive the solution formula for problem (1.1), (1.2). We first investigate the equation (1.4). Taking the Fourier transform, we have (1 + a| |2 )^ utt + 2b| |2 u ^t + (| |6 +  | |4 + | |2 )^ u = 0. The corresponding initial value are t=0: u ^=u ^0 ( ), u ^t = u ^1 ( ). (2.2) (2.1)

The characteristic equation of (2.1) is (1 + a| |2 )2 + 2b| |2  + | |6 +  | |4 + | |2 = 0. Let  = ± ( ) be the corresponding eigenvalues of (2.3), we obtain ± ( ) = -b| |2 ± | | -1 - (a +  - b2 )| |2 - ( + a )| |4 - a| |6 . 1 + a| |2 ^ (, t)^ ^ (, t)^ u1 ( ) + H u0 ( ), u ^(, t) = G where ^ (, t) = G and 1 (+ ( )e- ()t - - ( )e+ ()t ). + ( ) - - ( ) We define G(x, t) and H (x, t) by ^ (, t) = H ^ (, t)](x), G(x, t) = F -1 [G ^ (, t)](x), H (x, t) = F -1 [H (2.7) 1 (e+ ()t - e- ()t ) + ( ) - - ( ) (2.6) (2.4) (2.3)

The solution to the problem (2.1)-(2.2) is given in the form (2.5)

respectively, where F -1 denotes the inverse Fourier transform. Then, applying F -1 to (2.5), we obtain u(t) = G(t)  u1 + H (t)  u0 . By the Duhamel principle, we obtain the solution formula to (1.1), (1.2),
t

(2.8)

u(t) = G(t)  u1 + H (t)  u0 +
0

G(t -  )  (I - a)-1 f (u)( )d.

(2.9)

3. Decay Property The aim of this section is to establish decay estimates of the solution operators G(t) and H (t) appearing in the solution formula (2.8). Lemma 3.1. The solution of problem (2.1), (2.2) satisfies | |2 (1+ | |2 )|u ^(, t)|2 + |u ^t (, t)|2  Ce-c()t (| |2 (1+ | |2 )|u ^0 ( )|2 + |u ^1 ( )|2 ), (3.1) for   Rn and t  0, where  ( ) =
| |2 1+| |2 .

4

Y. WANG

EJDE-2012/96

¯ Proof. Multiplying (2.1) by u ^t and taking the real part yields 1 d {(1 + a| |2 )|u ^t |2 + (| |6 +  | |4 + | |2 )|u ^|2 } + 2b| |2 |u ^t |2 = 0. 2 dt ¯ Multiplying (2.1) by u ^ and taking the real part, we obtain (3.2)

1 d ¯ {b| |2 |u ^|2 + 2(1 + a| |2 )Re(^ ut u ^)} + (| |6 +  | |4 + | |2 )|u ^|2 - (1 + a| |2 )|u ^t |2 = 0. 2 dt (3.3) Multiplying both sides of (3.2) and (3.3) by (1 + a| |2 ) and b| |2 respectively, summing up the products yields d E + F = 0, (3.4) dt where 1 E = (1 + a| |2 )2 |u ^t |2 + (1 + a| |2 )(| |6 +  | |4 + | |2 )|u ^|2 + b2 | |4 |u ^|2 2 ¯ + b| |2 (1 + a| |2 ) Re(^ ut u ^) and F = b| |2 (| |6 +  | |4 + | |2 )|u ^|2 + b| |2 (1 + a| |2 )|u ^t |2 . A simple computation implies that C (1 + | |2 )2 E0  E  C (1 + | |2 )2 E0 , where E0 = | |2 (1 + | |2 )|u ^|2 + |u ^t |2 . Note that F  c| |2 E0 . It follows from (3.5) that F  c ( )E, where  ( ) = Using (3.4) and (3.6), we obtain d E + c ( )E  0. dt Thus E (, t)  e-cw()t E (, 0), which together with (3.5) proves the desired estimates (3.1). Then the proof is complete. ^ (, t) and H ^ (, t) be the fundamental solution of (1.4) in the Lemma 3.2. Let G Fourier space, which are given in (2.6) and (2.7), respectively. Then we have the estimates ^ (, t)|2 + |G ^ t (, t)|2  Ce-c()t | |2 (1 + | |2 )|G (3.7) and ^ (, t)|2 + |H ^ t (, t)|2  C | |2 (1 + | |2 )e-c()t | |2 (1 + | |2 )|H for   Rn and t  0, where  ( ) =
| |2 1+| |2 .

(3.5)

(3.6)

| |2 . 1 + | |2

(3.8)

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

5

Proof. If u ^0 ( ) = 0, from (2.5), we obtain ^ (, t)^ u ^(, t) = G u1 ( ), ^ t (, t)^ u ^t (, t) = G u1 ( ).

Substituting the equalities into (3.1) with u ^0 ( ) = 0, we obtain (3.7). In what follows, we consider u ^1 ( ) = 0, it follows from (2.5) that ^ (, t)^ ^ t (, t)^ u ^(, t) = H u0 ( ), u ^t (, t) = H u0 ( ). Substituting the equalities into (3.1) with u ^1 ( ) = 0, we obtain the desired estimate (3.8). The Lemma is proved. Lemma 3.3. Let k  0 and 1  p  2. Then we have
k x G(t)   k H (t) x k x Gt (t) L2

 C (1 + t)-( 2 ( p - 2 )+ 2 + 2 - 2 ) 
L2 L2

n

1

1

k

l

1

-l p H

(k-2)+ + Ce-ct x 

L2 ,

(3.9) (3.10) (3.11) (3.12) (3.13) (3.14)

 
L2

 C (1 + t)  C (1 + t)

1 1 k l -( n 2 ( p - 2 )+ 2 + 2 )

 

-l p H -l p H -l p H

+ Ce + Ce

-ct

k x  L2 k x  L2 , L2 L2 , L2 ,

1 1 k l -( n 2 ( p - 2 )+ 2 + 2 ) n 1 1 k l 1

-ct

k x Ht (t)  

 C (1 + t)-( 2 ( p - 2 )+ 2 + 2 + 2 ) 
n k 1

k+2 + Ce-ct x  L1 L1 k + Ce-ct x g k + Ce-ct x g

k x G(t)  (I - a)-1 g k x Gt (t)  (I - a)-1 g

L2 L2

 C (1 + t)-( 4 + 2 + 2 ) g  C (1 + t)
k -( n 4 + 2 +1)

g

where (k - 2)+ = max{0, k - 2}. Proof. Firstly, we prove (3.9). By the Plancherel theorem and (3.7), we obtain
k x G(t)   2 L2

=
| |R0

^( )|2 d + ^ (, t)|2 | | |2k |G
| |R0

^( )|2 d ^ (, t)|2 | | |2k |G

C
| |R0

| |
-ct

2k-2 -c| |2 t

e

^( )| d |
2

(3.15) | | (| | (1 + | | ))
| |R0 2 Lp 2k 2 2 -1

+ Ce

^( )|2 d |
2

^( )  C | |-l  + Ce
-ct

| |(2k-2+2l)q e-cq|| t d
| |R0

1/q

(k-2)+ x  2 L2 , 1 p

where R0 is a small positive constant and Hausdorff-Young inequality that ^( ) | |-l 
Lp

+

1 p

= 1,
l

2 p

+

1 q

= 1. It follows from (3.16)

 C (-)- 2 

Lp .

By a straight computation, we obtain | |(2k-2+2l)q e-cq|| t d
| |R0
2

1/q

 C (1 + t)-( 2q +k-1+l) (3.17)  C (1 + t)-(n( p - 2 )+k-1+l) .
1 1

n

Combining (3.15), (3.16) and (3.17) yields (3.9). Similarly, using (3.7) and (3.8), respectively, we can prove (3.10)-(3.12).

6

Y. WANG

EJDE-2012/96

In what follows, we prove (3.13). By the Plancherel theorem, (3.7), and Hausdorff-Young inequality, we have
k x G(t)  (I - a)-1 g 2 L2

=
| |R0

^ (, t)|2 | |4 (1 + a| |2 )-2 |g | |2k |G ^( )|2 d ^ (, t)|2 | |4 (1 + | |2 )-2 |g | |2k |G ^( )|2 d
| |R0

+ C

| |2k+2 e-c|| t |g ^( )|2 d + Ce-ct
| |R0 | |R0 k | |2k+2 e-c|| t d + Ce-ct x g | |R0 2 L1 k + Ce-ct x g 2 L2 .
2

2

| |2k |g ^( )|2 d
2 L2

C g ^( )

2 L
n

 C (1 + t)-( 2 +k+1) g

where R0 is a small positive constant. Thus (3.13) follows. Similarly, we can prove (3.14). Thus we have completed the proof of lemma. 4. Decay estimate for solutions to the linear equation  -1 (Rn ), u1  H s (Rn )  H  -2 (Rn ) Theorem 4.1. Assume that u0  H s+2 (Rn )  H 1 1 n (s  [ 2 ] + 5). Then the classical solution u(x, t) to (1.4) associated with initial condition (1.2), which is given by the formula (2.8), satisfies the decay estimates
k x u(t) L2

 C (1 + t)- 4 - 2 - 2 ( u0
n h

n

k

1

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(4.1)

for k  s + 2,
h x ut (t) L2

 C (1 + t)- 4 - 2 -1 ( u0
n m 1

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(4.2)

for h  s,
m x u(t) L

 C (1 + t)- 2 - 2 - 2 ( u0

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(4.3)

for m  s + 1 - [ n 2 ]. Proof. Firstly, we prove (4.1). Using (3.9) and (3.10), we obtain
k x u(t) L2 k x G(t) h + C x H (t)  u0



 u1

L2

L2  -2 ) H 1  -2 H 1

 C (1 + t)  C (1 + t)

k 1 -n 4 -2-2

( u0 ( u0

 -1 H 1  -1 H 1

+ u1 + u1

+ Ce-ct ( u0
H s+2

H s+2

+ u1

Hs )

k 1 -n 4 -2-2

+ u0

+ u1

H s ).

Similar to the proof of (4.1), using (3.11) and (3.12), we can prove (4.2). In what follows, we prove (4.3). Using (4.1) and Gagliardo-Nirenberg inequality, it is not difficult to get (4.3). The Lemma is proved. 5. Existence of global solution and asymptotic behavior The purpose of this section is to prove the existence and asymptotic behavior of global solutions to the Cauchy problem (1.1), (1.2). We need the following Lemma, which come from [4] (see also [17]).

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

7

Lemma 5.1. Let s and  be positive integers,  > 0, p, q, r  [1, ] satisfy 1 r = 1 1 s + , and let k  { 0 , 1 , 2 , · · · , s } . Assume that F ( v ) is a class of C and satisfies p r
l |v F (v )|  Cl, |v |+1-l ,

|v |  ,

0  l  s, l <  + 1

and
l |v F (v )|  Cl, , |v |  , l  s,  + 1  l.

If v  Lp  W k,q  L and v F (v )
 x F (v ) Lr

L

  , then
W k,q

W k,r

 Ck, v
Lq

v v

Lp

v

 -1 L ,

  Ck, x v

v

Lp

 -1 L ,

||  k.

1 Lemma 5.2. Let s and  be positive integers,  > 0, p, q, r  [1, ] satisfy 1 r = p+ 1 r , and let k  {0, 1, 2, · · · , s}. Let F (v ) be a function that satisfies the assumptions of Lemma 5.1. Moreover, assume that s s |v F (v1 ) - v F (v2 )|  C (|v1 | + |v2 |)max{-s,} |v1 - v2 |,

|v1 |  ,

|v2 |  .

If v1 , v2  L  W

p

k,q

L



and v1
Lr

L

 , v2

L

  , then for ||  k , we have

 x (F (v1 ) - F (v2 ))

 Ck, {( + ( v1

 x v1 Lq Lp

 + x v2 Lp )

Lq )

v1 - v2

Lp

+ v2

 x (v1

- v2 )

Lq }(

v1

L

+ v2

 -1 . L )

Based on the estimates (4.1)-(4.3) of solutions to (1.4) associated with initial condition (1.2), we define the following solution space X = {u  C ([0, ); H s+2 (Rn ))  C 1 ([0, ); H s (Rn )) : u where u
X k u(t) (1 + t) 4 + 2 + 2 x ks+2 X
n k 1 n h

X

< },

= sup
t0

L2

+
hs

h ut (t) (1 + t) 4 + 2 +1 x

L2 },

For R > 0, we define XR = {u  X : u Gagliardo-Nirenberg inequality, we obtain
m x u(t) L

 R}. For m  s + 1 - [ n 2 ], using
n m 1

 C (1 + t)-( 2 + 2 + 2 ) u

X.

(5.1)

 -1 (Rn ), u1  H s (Rn )  H  -2 (Rn ) Theorem 5.3. Assume that u0  H s+2 (Rn )  H 1 1 n s+2 (s  [ 2 ] + 5) and integer   2. Let f (u) be a function of class C and satisfy Lemmas 5.1 and 5.2. Put E0 = u 0
 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs .

If E0 is suitably small, the Cauchy problem (1.1)-(1.2) has a unique global classical solution u(x, t) satisfying u  C ([0, ); H s+2 (Rn )), ut  C ([0, ); H s (Rn )), utt  L ([0, ); H s-2 (Rn )). Moreover, the solution satisfies the decay estimate
k x u(t) h x ut (t) L2

 CE0 (1 + t)- 4 - 2 - 2 ,  CE0 (1 + t)
h -n 4 - 2 -1

n

k

1

(5.2) (5.3)

L2

for k  s + 2 and h  s.

8

Y. WANG

EJDE-2012/96

Proof. Define the mapping
t

(u) = G(t)  u1 + H (t)  u0 +
0

G(t -  )  (I - a)-1 f (u( ))d.

(5.4)

Using (3.9)-(3.10), (3.13), Lemma 5.1 and (5.1), for k  s + 2 we obtain
k x (u) L2 L2 k + C x H (t)  u0 L2 L2 d H s+2 + u1
Hs )

k  C x G(t)  u1 t

+C
0

k x G(t -  )  (I - a)-1 f (u( ))
n k 1

 C (1 + t)- 4 - 2 - 2 ( u0
t/2

 -1 H 1
k

+ u1
1

 -2 ) H 1 L1 d

+ Ce-ct ( u0

+C
0 t

(1 + t -  )- 4 - 2 - 2 f (u)
k (1 + t -  )- 4 - 2 x f (u) t/2 t k e-c(t- ) x f (u) 0
n k 1 n 1

n

+C +C

L1 d

L2 d

 C (1 + t)- 4 - 2 - 2 ( u0
t/2

 -1 H 1
k

+ u1
1

 -2 ) H 1

+ Ce-ct ( u0

H s+2 + u1

Hs )

+C
0 t

(1 + t -  )- 4 - 2 - 2 u
k (1 + t -  )- 4 - 2 x u t/2
n k 1 n 1

n

2 L2

u

 -1 L d t k e-c(t- ) x u 0 H s+2 + u1
n 1 Hs )

+C

2 L2

u

 -1 L d +C

L2

u

 L d

 C (1 + t)- 4 - 2 - 2 ( u0
t/2

 -1 H 1
n

+ u1
k 1

 -2 ) H 1

+ Ce-ct ( u0
n

+CR+1
0 t

(1 + t -  )- 4 - 2 - 2 (1 +  )-( 2 +1) (1 +  )-( 2 + 2 )(-1) d (1 + t -  )- 4 - 2 (1 +  )- 2 -k-1 (1 +  )-( 2 + 2 )(-1) d
t/2 t
n 1 n n 1

+CR+1 +CR+1  C (1 + t) Thus

e-c(t- ) (1 +  )- 4 - 2 - 2 (1 +  )-( 2 + 2 ) d
0 k 1 -n 4 -2-2

n

k

1

n

1

{( u 0
n

 -1 H 1
k 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

+ R+1 }. (5.5)

k (1 + t) 4 + 2 + 2 x (u) It follows from (5.4) that t

L2

 CE0 + CR+1 .

(u)t = Gt (t)  u1 + Ht (t)  u0 +
0

Gt (t -  )  (I - a)-1 f (u( ))d.

(5.6)

Using (3.11)-(3.12), (3.14) Lemma 5.1 and (5.1), for h  s we have
h x (u)t L2 h x Ht (t)  u0 L2 L2 d h  C x Gt (t)  u1 L2 + C t h +C x Gt (t -  )  (I 0

- a)-1 f (u( ))

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR
n h

9 Hs )

 C (1 + t)- 4 - 2 -1 ( u0
t/2

 -1 H 1
h

+ u1

 -2 ) H 1 L1 d

+ Ce-ct ( u0

H s+2

+ u1

+C
0 t

(1 + t -  )- 4 - 2 -1 f (u)
h f (u) (1 + t -  )- 4 -1 x t/2
n h n

n

t L1 d

+C

+C
0

h e-c(t- ) x f (u)

L2 d Hs )

 C (1 + t)- 4 - 2 -1 ( u0
t/2

 -1 H 1
h

+ u1
2 L2

 -2 ) H 1

+ Ce-ct ( u0

H s+2

+ u1

+C
0 t

(1 + t -  )- 4 - 2 -1 u
h u (1 + t -  )- 4 -1 x t/2
h -n 4 - 2 -1 n

n

u

 -1 L d t

+C

2 L2

u

 -1 L d

+C
0 -ct

h e-c(t- ) x u

L2

u

 L d

 C (1 + t) +CR+1

( u0

 -1 H 1
n

+ u1
h

 -2 ) H 1

+ Ce
n

( u0

H s+2
n

+ u1
1

Hs )

t/2 0 t

(1 + t -  )- 4 - 2 -1 (1 +  )-( 2 +1) (1 +  )-( 2 + 2 )(-1) d (1 + t -  )- 4 -1 (1 +  )- 2 -h-1 (1 +  )-( 2 + 2 )(-1) d
t/2 t
n n n 1

+CR+1 +CR+1  C (1 + t) Thus

e-c(t- ) (1 +  )- 4 - 2 -1 (1 +  )-( 2 + 2 ) d
0 h -n 4 - 2 -1

n

h

n

1

{( u 0
n

 -1 H 1
h

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

+ R+1 }. (5.7) (5.8)

h (1 + t) 4 + 2 +1 x (u)t L2  CE0 + CR+1 . Combining (5.5), (5.7) and taking E0 and R suitably small yields

(u) For u ~, u ¯  XR , by using (5.4), we have
t

X

 R.

(~ u) - (¯ u) =
0

G(t -  )  (I - a)-1 [f (~ u) - f (¯ u)]d.

(5.9)

Using (5.9), (3.13) and Lemma 5.2, (5.1), for k  s + 2 we obtain
k x (~ u) - (¯ u)) t L2 L2 d


0

k x G(t -  )  (I - a)-1 [f (~ u) - f (¯ u)] t/2

C
0 t

(1 + t -  )- 4 - 2 - 2 (f (~ u) - f (¯ u))
k (1 + t -  )- 4 - 2 x (f (~ u) - f (¯ u)) t/2 t k e-c(t- ) x (f (~ u) - f (¯ u)) 0 t/2
n 1

n

k

1

L1 d

+C +C C
0

L1 d

L2 d

(1 + t -  )- 4 - 2 - 2 ( u ~
L

n

k

1

L2

+ u ¯

L2 )

u ~-u ¯

L2

×( u ~

+ u ¯

 -1 d L )

10 t

Y. WANG

EJDE-2012/96

+C
t/2

k u ~ (1 + t -  )- 4 - 2 {( x L2 t

n

1

L2

k + x u ~

L2 )

u ~-u ¯

L2

+( u ~ +C
0

+ u ¯

L2 )

k x (~ u-u ¯) L2

L2 }(

u ~
L2 )

L

+ u ¯

 -1 d L )

k e-c(t- ) {( x u ~ L

k + x u ~

u ~-u ¯
L

L  -1 d L )
n 1

+( u ~

+ u ¯
X

L ) t/2

k x (~ u-u ¯)

L2 }(
n k

u ~
1

+ u ¯

 CR u ~-u ¯ +CR u ~-u ¯

(1 + t -  )- 4 - 2 - 2 (1 +  )-( 2 + 2 ) d
0 t

X t/2

(1 + t -  )- 4 - 2 (1 +  )-( 2 (n+1)+
t

n

1



k+1 2 )

d

~-u ¯ +CCR u
n

X 0
k 1

e-c(t- ) (1 +  )-( 4 + 2 + 2 + 2 ) d
X,

n

n

k

1

 CR (1 + t)- 4 - 2 - 2 u ~-u ¯ which implies

k (1 + t) 4 + 2 + 2 x ((~ u) - (¯ u))

n

k

1

L2

 CR u ~-u ¯

X.

(5.10)

Similarly for h  s, from (5.6), (3.14) and (5.1), we have
t h x ((~ u) - (¯ u))t L2


0

h x Gt (t -  )  (I - a)-1 [f (~ u) - f (¯ u)] t/2

L2 d

C
0 t

u) - f (¯ u)) (1 + t -  )- 4 - 2 -1 (f (~
h (1 + t -  )- 4 -1 x (f (~ u) - f (¯ u)) t/2 t h e-c(t- ) x (f (~ u) - f (¯ u)) 0
n h n

n

h

L1 d

+C +C

L1 d

L2 d

 CR (1 + t)- 4 - 2 -1 u ~-u ¯ which implies
h (1 + t) 4 + 2 +1 x ((~ u) - (¯ u))t
n h

X,

L2

 CR u ~-u ¯

X.

(5.11)

Using (5.10), (5.11) and taking R suitably small yields (~ u) - (¯ u)
X



1 u ~-u ¯ 2

X.

(5.12)

From (5.8) and (5.12), we know that  is strictly contracting mapping. Consequently, we conclude that there exists a fixed point u  XR of the mapping , which is a classical solution to (1.1), (1.2). This completes the proof. Acknowledgements. The author would like to thank the anonymous referee for his/her comments and suggestions. This work was supported in part by grants 11101144 from the NNSF of China, and 201031 from the Research Initiation Project for High-level Talents of North China University of Water Resources and Electric Power.

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

11

References
[1] S. Lai, Y. Wang, Y. Wu and Q. Lin; An initial-boundary value problem for a generalized Boussinesq water system in a ball, Int. J. Appl. Math. Sci. 3 (2006), 117-133. [2] Y. Liu and S. Kawashima; Global existence and asymptotic behavior of solutions for quasilinear dissipative plate equation, Discrete Contin. Dyn. Syst. 29 (2011) 1113-1139. [3] Y. Liu and S. Kawashima; Global existence and decay of solutions for a quasi-linear dissipative plate equation, J. Hyperbolic Differential Equations 8 (2011) 591-614. [4] T. T. Li and Y. M. Chen; Nonlinear Evolution Equations, Scientific Press, 1989, (in Chinese). [5] V. G. Makhankov; Dynamics of classical solitons(in nonintegrable systems), Physics Reports, A review section of Phys. Lett.(Section C) 35(1) (1978), 1-128. [6] S. Wang and H. Xu; On the asymptotic behavior of solution for the generalized IBq equation with hydrodynamical damped term, J. Differ. Equ.252 (2012) 4243-4258. [7] S. Wang and F. Da; On the asymptotic behavior of solution for the generalized double dipersion equation, Appl. Anal. in press. [8] Y. Wang and Z. Wei; Global existence and asymptotic behavior of solutions to Cahn-Hilliard equation with inertial term, Internationl Journal of mathematics, accepted. [9] Y. Wang; Global existence and asymptotic behaviour of solutions for the generalized Boussinesq equation, Nonlinear Anal. 70 (2009) 465-482. [10] Y. Wang; Global existence of classical solutions to the minimal surface equation in two space dimensions with slow decay initial value, J. Math. Phys. 50 (2009) 103506-01-103506-01-14. [11] Y. Wang; Existence and nonexistence of global solutions for a class of nonlinear wave equations of higher order, Nonlinear Anal. 72 (2010) 4500-4507. [12] Y. Wang and Y. Wang; Global existence of classical solutions to the minimal surface equation with slow decay initial value, Appl. Math. Comput. 216 (2010) 576-583. [13] Y. Wang, F. Liu and Y. Zhang; Global existence and asymptotic of solutions for a semi-linear wave equation, J. Math. Anal. Appl. 385 (2012) 836-853. [14] Y. Wang and Y. Wang; Global existence and asymptotic behavior of solutions to a nonlinear wave equation of fourth-order, J. Math. Phys. 53 (2012) 013512-01-013512-13. [15] Z. Yang; Longtime behavior of the Kirchhoff type equation with strong damping on Rn , J. Differ. Equ. 242 (2007) 269-286. [16] Y. Zhang, Q. Lin and S. Lai; Long time asymptotic for the damped Boussinesq equation in a circle, J. Partial Dif. Eqs. 18(2005), 97-113. [17] S. M. Zheng; Nonlinear Evolution Equations, Monographs and Surveys in Pure and Applied Mathematics, 133, Chapan Hall/CRC, 2004. Yinxia Wang School of Mathematics and Information Sciences, North China University of Water Resources and Electric Power, Zhengzhou 450011, China E-mail address : yinxia117@126.com

Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

Contents lists available at SciVerse ScienceDirect

Journal of Combinatorial Theory, Series A
www.elsevier.com/locate/jcta

Covering and packing for pairs
Yeow Meng Chee a , Charles J. Colbourn b , Alan C.H. Ling c , Richard M. Wilson d
a

Division of Mathematical Sciences, School of Physical and Mathematical Sciences, Nanyang Technological University, 21 Nanyang Link, Singapore 637371, Singapore b Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287-8809, USA c Department of Computer Science, University of Vermont, Burlington, VT 05405, USA d Department of Mathematics, 253-37, California Institute of Technology, Pasadena, CA 91125, USA

a r t i c l e

i n f o

a b s t r a c t
When a v -set can be equipped with a set of k-subsets so that every 2-subset of the v -set appears in exactly (or at most, or at least) one of the chosen k-subsets, the result is a balanced incomplete block design (or packing, or covering, respectively). For each k, balanced incomplete block designs are known to exist for all sufficiently large values of v that meet certain divisibility conditions. When these conditions are not met, one can ask for the packing with the most blocks and/or the covering with the fewest blocks. Elementary necessary conditions furnish an upper bound on the number of blocks in a packing and a lower bound on the number of blocks in a covering. In this paper it is shown that for all sufficiently large values of v , a packing and a covering on v elements exist whose numbers of blocks differ from the basic bounds by no more than an additive constant depending only on k. © 2013 Elsevier Inc. All rights reserved.

Article history: Received 24 September 2011 Available online xxxx Keywords: Balanced incomplete block design Pair packing Pair covering Group divisible design Pairwise balanced design

1. Introduction Let v , k , and t be integers with v > k > t 2. Let  be a positive integer. A (t , )-packing of order v and blocksize k is a set V of v elements, and a collection B of k -element subsets (blocks) of V , so that every t -subset of V appears in at most  blocks. A (t , )-covering of order v and blocksize k is a set V of v elements, and a collection B of k -element subsets (blocks) of V , so that every t -subset

E-mail addresses: ymchee@ntu.edu.sg (Y.M. Chee), charles.colbourn@asu.edu (C.J. Colbourn), aling@cems.uvm.edu (A.C.H. Ling), rmw@caltech.edu (R.M. Wilson). 0097-3165/$ ­ see front matter © 2013 Elsevier Inc. All rights reserved. http://dx.doi.org/10.1016/j.jcta.2013.04.005

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1441

of V appears in at least  blocks. When  = 1, the simpler notation of t -packing or t -covering is used. When ( V , B ) is both a (t , )-packing and a (t , )-covering with blocksize k , it is a t -( v , k, ) design. v k A t -( v , k, ) design, if one exists, has  t / t blocks. When the required number of blocks is not integral, no such design can exist. Selecting all blocks containing a particular element x  V and deleting x from each forms the derived (t - 1)-( v - 1, k - 1, ) design (with respect to x). For a design v -i k -i to exist, evidently the derived design must exist; hence for a t -( v , k, ) design to exist,  t -i / t -i must be integral for every 0 i t . When these conditions are not all met, one can ask instead for the largest (t , )-packing, or for the smallest (t , )-covering, of order v and blocksize k . The Johnson bound [13] states that such a packing can have no more than

n k

···

n - t + 2 (n - t + 1) k-t +2 k-t +1

···

blocks, while the Schönheim bound [21] states that such a covering can have no fewer than

n k

···

n - t + 2 (n - t + 1) k-t +2 k-t +1

···

blocks. Our main result is that when t = 2 and  = 1, there exist packings and coverings whose sizes are within a constant of these bounds. Determining when these bounds are met exactly is a challenging question.  and Hanani [9] conjectured that, for fixed k and t , with all blocks of size k , In 1963, Erdos n a t -packing on n elements with /k (1 - o(1)) blocks and a t -covering on n elements with t t

/ t (1 + o(1)) blocks both exist. This was proved by Rödl [20], and has spawned a large literature t (for example, [10,11,14,15,23]). However, even when t = 2, all of these general constructions deviate from the Johnson and Schönheim bounds by an amount that grows as a function of the number of elements. Wilson [25] established that the necessary divisibility conditions for a 2-( v , k, ) design to exist are asymptotically sufficient (i.e., for fixed k and , and sufficiently large v ). This provides a  different means to establish the Erdos­Hanani conjecture for t = 2, but also does not immediately imply that one can find packings or coverings whose sizes are within a constant of the optimal sizes. Wilson [24] earlier considered this more challenging problem for packings, but the solution for the analogous problem for coverings has remained elusive. We focus on the case when t = 2 and  = 1 here. Caro and Yuster state stronger results for covering [3] and packing [2] than we prove here. Their approach relies in an essential manner on a strong statement by Gustavsson [12]:
Proposition 1.1. Let H be a graph with  vertices and h edges, having degree sequence (d1 , . . . , d ). Then there exist a constant N H and a constant H > 0, both depending only on H , such that for all n > N H , if G is a graph on n vertices, m edges, and degree sequence (1 , . . . , n ) so that min(1 , . . . , n ) n(1 - H ), gcd(d1 , . . . , d ) | gcd(1 , . . . , n ), and h | m, then G has an edge partition (decomposition) into graphs isomorphic to H . We have not been able to verify the proof of Proposition 1.1. Indeed, while the result has been used a number of times in the literature, no satisfactory proof of it appears there. While we expect that the statement is true, we do not think that the proof in [12] is sufficient at this time to employ the statement as a foundation for further results. Therefore we adopt a strategy that is completely independent of Proposition 1.1, and independent of the results built on it. In the remainder of the paper, we first recall relevant known results. Then in Section 3, we determine the possible structure of optimal packings and coverings, in order to determine what can remain uncovered in a packing, and what must be covered more than once in a covering. This is done in general for packings and coverings with a single hole, in order to limit any deviation from the desired bound to the manner in which a (fixed size) hole is filled. In Section 4, the most important part of the proof is established, namely that in each congruence class, one finite example can be produced. Finally in Section 5, these single examples are shown to form the required ingredients to establish asymptotic existence.

n

k

1442

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

2. Background To proceed more formally, we require a number of definitions and preliminary results from combinatorial design theory; related background material can be found in [1,22]. A balanced incomplete block design (BIBD) is a 2-( v , k, ) design. Balanced incomplete block designs have been extensively studied because of their central role in numerous applications in experimental design, coding and information theory, communications, and connections with fundamental topics in algebra, finite geometry, number theory, and combinatorics (see [5,7] for examples). The general divisibility conditions (stated for v k general t earlier) require that  2  0 mod 2 and ( v - 1)  0 (mod k - 1). A group divisible design ( V , G , B ) is a finite set V of elements or points; a partition G = {G 1 , . . . , G s } of V (groups); and a set B of subsets of V (blocks), with the property that every 2-subset of V lying within a group appears in no block, while every 2-subset of V with elements from different groups appears in exactly  blocks. When K is a set of positive integers for which | B |  K whenever B  B , the design is a ( K , )-GDD. When  = 1, we write simply K -GDD. Its order is | V |, its index is , and u u when the multiset of group sizes {|G i |: 1 i s} is the same as the multiset its type is 1 1 · · ·  j . We write (k, )-GDD (or k -GDD formed by including u j copies of  j when  j = 0, for all 1 when  = 1) when K = {k}. A transversal design TD (k, n) is a (k, )-GDD of type nk . We write TD(k, n) when  = 1. A transversal design is idempotent if its element set is {1, . . . , k} × {1, . . . , n}, and its block set contains {{(i , j ): 1 i k}: 1 j n}. A pairwise balanced design with blocksizes K and order v (( K , )-PBD of order v ) is a ( K , )-GDD of type 1 v ; we write K -PBD when  = 1. Then a balanced incomplete block design ((k, )-BIBD) is a (k, )-PBD; we write k -BIBD when  = 1. An incomplete pairwise balanced design of order v with holesize h , blocksizes K , and index  is a triple ( V , H , B ) for which | V | = v , | H | = h , H  V , B contains a set of subsets of V for which | B |  K whenever B  B , and for every pair of distinct elements x, y  V , the number of blocks in {{x, y }  B  B } is 0 if {x, y }  H and  otherwise. The notation ( K , )-IPBD( v , h) is used; we may omit  when it is 1, and write k instead of K when K = {k}. Let K be a set of positive integers, each at least 2. Then define  ( K ) = gcd{k - 1: k  K } and k ( K ) = gcd 2 : kK . Wilson establishes a crucial asymptotic existence result: Theorem 2.1. (See [25].) Let K be a set of integers, each at least 2. Let  be a positive integer. For all sufficiently n large n satisfying (n - 1)  0 (mod  ( K )) and  2  0 (mod ( K )), there exists a ( K , )-PBD of order n. In particular for K = {k}, when (n - 1)  0 (mod k - 1),  exists a (k, )-BIBD of order n. Colbourn and Rödl prove a variant that we use: Theorem 2.2. (See [6].) Let  > 0. Let K = {k1 , . . . , km } be a set of block sizes. Let { p 1 , . . . , pm } be nonm negative numbers with i =1 p i = 1. For all sufficiently large v satisfying v - 1  0 (mod  ( K )) and v  0 (mod ( K )), there is a K -PBD of order v in which, for each 1 i m, the fraction of pairs appearing 2 in blocks having size ki is in the range [ p i -  , p i +  ]. A stronger version of Theorem 2.2 is given in [26], and a variant for resolvable designs appears in [8]. Perhaps the most powerful generalization of Theorem 2.1 is due to Lamken and Wilson [16]. We in(r ,) be a complete digraph on n vertices with exactly  edges of color i joining troduce this next. Let K n (r ,) is any vertex x to any vertex y for every color i in a set of r colors. A family F of subgraphs of K n (r ,) (r ,) if every edge e  E ( K n ) belongs to exactly one member in F . Given a fama decomposition of K n (r ,) is a decomposition F such that every ily  of edge-r -colored digraphs, a  -decomposition of K n graph F  F is isomorphic to some graph G   . For a vertex x of an edge-r -colored digraph G , the degree-vector of x is the 2r -vector d(x) = (in1 (x), out1 (x), in2 (x), out2 (x), . . . , inr (x), outr (x)), where in j (x) and out j (x) denote the indegree and outdegree of vertex x in the spanning subgraph of G by
n 2

 0 mod

k 2

, and n is sufficiently large, there

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1443

edges of color j , respectively, for 1 j r . We denote by  (G ) the greatest common divisor of the integers t such that the 2r -vector (t , t , . . . , t ) is an integral linear combination of the vectors d(x) as x ranges over the vertex set V (G ) of G . Equivalently,  (G ) is the smallest positive integer t 0 such that (t 0 , t 0 , . . . , t 0 ) is an integral linear combination of the vectors {d(x)}. Let  be a family of simple edge-r -colored digraphs and let  () denote the greatest common divisor of the integers t such that the 2r -vector (t , t , . . . , t ) is an integral linear combination of the vectors {d(x)} as x ranges over all vertices of all graphs in  . For each graph G   , let (G ) = (m1 , m2 , . . . , mr ), where mi is the number of edges of color i in G . We denote by () the greatest common divisor of the integers m such that (m, m, . . . , m) is an integral linear combination of the vectors {(G ): G   }. Equivalently, () is the smallest positive integer m0 such that (m0 , m0 , . . . , m0 ) is an integral linear combination of the (r ,) vectors {(G )}. A graph G 0   is useless when it cannot occur in any  -decomposition of K n .  is admissible when no member of  is useless. Theorem 2.3. (See [16].) Let  be an admissible family of simple edge-r-colored digraphs. For all sufficiently (r ,) large n satisfying (n - 1)  0 (mod  ()) and n(n - 1)  0 (mod ()), a  -decomposition of K n exists. Theorem 2.3 has numerous consequences for the existence of various classes of combinatorial designs. Building on Theorem 2.3, Liu establishes the following: Theorem 2.4. (See [17].) Let K be a set of integers, each at least 2. Let m and  be positive integers. For all n sufficiently large n satisfying m(n - 1)  0 (mod  ( K )) and m2 2  0 (mod ( K )), there exists a ( K , )n GDD of order m . Mohácsy and Ray-Chaudhuri prove a result for a fixed number of groups when the index is 1. Theorem 2.5. (See [18,19].) Let k and u be integers with u k 2. For all sufficiently large m satisfying m(u - 1)  0 (mod k - 1) and m2 u (u - 1)  0 (mod k(k - 1)), there exists a k-GDD of type mu .  and Straus: This subsumes a classical result of Chowla, Erdos, Theorem 2.6. (See [4].) Let k 2 be an integer. For all sufficiently large m, there exists a TD(k, m).

3. Packings, coverings, and the optima We use known asymptotic existence results to treat asymptotic existence of packings and coverings in the cases that a k -BIBD does not exist. We require further definitions, to extend packings and coverings to have a `hole'. A packing with blocksize k with a hole ( V , H , B ) is a set V of elements, a subset (hole) H  V , and a set B of k -subsets of V , so that for every {x, y }  V , {x, y }  H , there is at most one B  B with {x, y }  B ; when {x, y }  H , there is no block B  B with {x, y }  B . The leave  of ( V , B ) is a graph with vertex set V ; pair {x, y } appears as an edge if and only if {x, y } H and is not a subset of any block of B . A covering with blocksize k with a hole ( V , H , B ) is a set V of elements, a subset H  V , and a set B of k -subsets of V , so that for every {x, y }  V , {x, y }  H , there is at least one B  B with {x, y }  B . The excess  of ( V , B ) is a multigraph with vertex set V ; the number of times pair {x, y } appears as an edge is exactly xy when {x, y }  H , and xy - 1 otherwise, where xy is the number of blocks of B that contain {x, y }. A packing with blocksize k ( V , B ) is a packing with blocksize k with a hole ( V , , B ), and a covering with blocksize k ( V , B ) is a covering with blocksize k with a hole ( V , , B ). A maximum packing with blocksize k is a packing with blocksize k ( V , B ) with the most blocks among all packings with blocksize k on | V | elements; equivalently, its leave has the fewest edges. A minimum covering with blocksize

1444

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

k is a covering with blocksize k ( V , B ) with fewest blocks among all coverings with blocksize k on | V | elements; equivalently, its excess has the fewest edges. Suppose that ( V , H , B ) is a packing with blocksize k with a hole, with v = | V |, h = | H |, and n = | V \ H |. Let x be a vertex in V \ H . The number of pairs on V that contain x is congruent to v - 1 modulo k - 1. The number containing x that appear in blocks of B is congruent to 0 modulo k - 1. Hence x has degree congruent to v - 1 modulo k - 1 in the leave. When the hole is nonempty, elements in the hole have degrees congruent to n modulo k - 1 in the leave. By the same token, in the excess of a covering with blocksize k with a hole, x has degree congruent to -( v - 1) modulo k - 1; elements in the hole have degrees congruent to -n modulo k - 1. We employ specific types of packings and coverings with holes in which the leave or excess has all vertices in the hole of degree 0. For an integer n  0 (mod k(k - 1)) and an integer h 1, let   h - 1 (mod k - 1) and  -(h - 1) (mod k - 1) with 0 , < k - 1. Then an optimum packing with blocksize k with a hole, k -OP(n + h, h), is a packing with blocksize k on n + h elements whose leave has degree  on each vertex not in the hole, and 0 on each vertex in the hole; and an optimum covering with blocksize k with a hole, k -OC(n + h, h), is a covering with blocksize k on n + h elements whose excess has degree on each vertex not in the hole, and 0 on each vertex in the hole. When h  1 (mod k - 1),  = = 0. In this case, a k-OP( v , h) and a k-OC( v , h) are the same, and are equivalent to a k -IPBD( v , h). In any packing with blocksize k on v = n + h elements with n  0 (mod k(k - 1)), no vertex can have degree smaller than  in the leave; and in any covering with blocksize k on v = n + h in the excess. Indeed, elements with n  0 (mod k(k - 1)), no vertex can have degree smaller than choosing and  so that v - 1 ; ,   v - 1 (mod k - 1);  - < k - 1; and =  when v v  1 (mod k - 1), every packing with blocksize k on v elements contains at most v ,k = k(k- 1)
v blocks, while every covering with blocksize k on v elements contains at least L v ,k = k( blocks. k-1) Then v ,k is at least the Johnson bound, and  v ,k is at most the Schönheim bound. The purpose of this paper is to prove the following two results.

Theorem 3.1. There is a constant pk such that for all v k, the number of blocks in a maximum packing with blocksize k on v elements is at least v ,k - pk and at most v ,k . Theorem 3.2. There is a constant ak such that for all v k, the number of blocks in a minimum covering with blocksize k on v elements is at least L v ,k and at most L v ,k + ak . We establish these results in a number of steps. Treating an arbitrary but fixed value of k , in Section 4, we show that for every c satisfying 0 c < k(k - 1), there exist positive integers nc  0 (mod k(k - 1)) and hc  c (mod k(k - 1)) so that a k -OP(nc + hc , hc ) exists; we also show that for every c satisfying 0 c < k(k - 1), there exist positive integers mc  0 (mod k(k - 1)) and c  c (mod k(k - 1)) so that a k -OC(mc + c , c ) exists. This provides a single example for optimal packings and coverings with a hole in every congruence class modulo k(k - 1). In Section 5, we use these results to establish that there exist integers k and uk , depending only on k , so that whenever v k , uk for which a k -OP( v , h) exists, and there also exists an uk for which a there exists an h k -OC( v , ) exists. From this, because uk is fixed and independent of v , we establish Theorems 3.1 and 3.2 by filling the holes. The crucial step, particularly for coverings, is producing one example in each congruence class. We treat this next. 4. One example in each congruence class In the case when h  1 (mod k - 1), a k -OP( v , h) and a k -OC( v , h) coincide with a k -IPBD( v , h), so we treat this situation first; subsequently the packing and covering cases differ. 4.1. Packing and covering: v  1 (mod k - 1) An incomplete transversal design ITD(k, n +  ; ) is a set V of k(n + ) elements, of which k form a hole H . The elements are partitioned into k groups G 1 , . . . , G k so that |G i  H | =  for 1 i k .

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1445

This set is equipped with a set of k -subsets (blocks) with the property that every pair of elements that appears in a group or appears in the hole H appears in no block, and every other pair appears in exactly one block. Lemma 4.1. Let k 2 be an integer. Let 0



k. For all sufficiently large n, an ITD(k, n +  ; ) exists.

Proof. Using Theorem 2.6, choose  so that a TD(k + 1, ), a TD(k + 1,  + 1), a TD(k + 1,  + 2), and a TD(k + 1,  + 3) all exist. Delete one group in each to form an idempotent TD(k, v ) for each v  {,  + 1,  + 2,  + 3}. For n sufficiently large, there is an { + 1,  + 2,  + 3}-PBD of order n +  + 1 containing a block of size  + 1 by Theorem 2.2. (Because  ({ + 1,  + 2,  + 3}) = 1 and ({ + 1,  + 2,  + 3}) = 1, this follows by choosing 0 <  < 1 and choosing the fraction of pairs 4 in blocks of size  + 1 to be 2 .) Delete all but  elements from a block of size  + 1, and remove the block of size  making a hole, to form an {,  + 1,  + 2,  + 3}-IPBD(n + , ). Give every element weight k , and use the idempotent TDs to inflate all blocks. The k elements arising from the  elements of hole in the IPBD form the hole of the ITD. 2 Lemma 4.2. Let h be an integer for which h  1 (mod k - 1) and k infinitely many integers  for which a k-IPBD( k(k - 1) + h, h) exists. h k(k - 1) + 1. Then there exist

h-k Proof. Let  = k -1 . Choose  so that a k -BIBD of order  (k - 1) + k and an ITD(k,  (k - 1) +  ; ) both exist. (Use Lemma 4.1 for the existence of the ITD.) Start with the ITD on the elements of V having a hole on the elements in H  V . Add k -  new elements N  . For 1 i k , let N i consist of the  elements in the i th group of the ITD that appear in H . Place on the elements of the i th group, together with N  , the blocks of a copy of the k -BIBD, omitting a block on the elements of N i  N  . On the  k(k - 1) + (k - 1) + k =  k(k - 1) + h elements of V  N  , all pairs are covered except k those within the hole on elements N   i =1 N i of size h = (k - 1) + k . 2

Corollary 4.3. Whenever c  1 (mod k - 1) and 0 c < k(k - 1), there are infinitely many integers nc and hc with nc  0 (mod k(k - 1)) and hc  c (mod k(k - 1)) so that a k-OP(nc + hc , hc ) exists. Proof. Set hc = c if c nc =  k(k - 1). 2 k , and hc = k(k - 1) + 1 if c = 1. Apply Lemma 4.2 with h = hc , and set

The same argument establishes: Corollary 4.4. Whenever c  1 (mod k - 1) and 0 c < k(k - 1), there are infinitely many integers mc and with mc  0 (mod k(k - 1)) and c  c (mod k(k - 1)) so that a k-OC(mc + c , c ) exists. 4.2. Packing: v  1 (mod k - 1) Lemma 4.5. For every integer c satisfying 0 c < k(k - 1), there exist an nc  0 (mod k(k - 1)) and an hc  c (mod k(k - 1)) for which a k-OP(nc + hc , hc ) exists. Proof. When c > 0, write c = s(k - 1) + d with 1 d < k . When c = 0, set s = d = k - 1. If d = 1, apply Lemma 4.3. Otherwise choose   1 (mod k(k - 1)) and N >  so that N   (mod k - 1); a k -GDD of type d exists (Theorem 2.4); an  -BIBD of order N exists (Theorem 2.1); and an ITD(k, d( N -  ) + s, s) exists (Lemma 4.1). Treat the  -BIBD as an  -GDD of type 1 N -  1 by removing a block, and inflate using the k GDD of type d to form a k -GDD of type d N - (d )1 . Adjoin d - s infinite elements to the
c

1446

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

ITD(k, d( N -  ) + s, s). On each group together with the infinite elements, place a copy of the k GDD of type d N - (d )1 , aligning the group of size d on the s elements in the intersection of the group and the hole of the ITD, together with the d - s infinite elements. The result is a k GDD of type dk( N - ) (d + s(k - 1))1 . Treat this as a packing. On the dk( N -  ) points not in the large hole, the leave has degree d - 1, so the result is a k -OP(nc + hc , hc ) with nc = dk( N -  ) and hc = d + s(k - 1). Because dk  0 (mod k) and N -   0 (mod k - 1), nc  0 (mod k(k - 1)). Because d  d (mod k(k - 1)), hc  c (mod k(k - 1)). 2 4.3. Covering: v  1 (mod k - 1) We employ some further, more specialized, combinatorial objects to treat coverings for the remaining congruence classes. Let V be a set of elements; B be a set of k -subsets of V ; G = {G 1 , . . . , G r } be a partition of V , j t. and H = { H 1 , . . . , H t } be a partition of V . Suppose that |G i  H j | =  for all 1 i r , 1
j i Further suppose that for every 2-subset {x, y }  V , either {x, y }  , or there  i =1 2 j =1 2 is exactly one B  B with {x, y }  B , but not both. Then ( V , G , H, B ) is a double group divisible design with blocksize k (k -DGDD) of type (r )t . A holey transversal design with blocksize k (k -HTD) of type r is a k -DGDD of type (r )k .

r

G

t

H

Theorem 4.6. Let k

2 be an integer. For all sufficiently large r, there exists a k-HTD of type 2r .

Proof. Choose K = {x1 , . . . , xs } so that  ( K ) = ( K ) = 1, and so that for each 1 i s , xi is large enough to ensure that Theorem 2.6 yields a TD(k + 1, xi ). Remove one group (and rename elements as needed) to form an idempotent TD(k, xi ). When r is large enough, Theorem 2.4 yields a K -GDD ( V , G , B ) of type 2r with groups G = {G 1 , . . . , G r }. The elements of the k-HTD to be formed are V × {0, . . . , k - 1}. For each block B  B , on the elements B × {0, . . . , k - 1}, align the k groups on { B × {i }: 0 i < k} to place the blocks of an idempotent TD(k, | B |). In the resulting design, one set of j r. 2 groups is formed by V × {i } for 0 i < k , the other by G j × {0, . . . , k - 1} for 1 Theorem 4.7. Let k 2 be an integer. For all sufficiently large integers r and t satisfying t - 1  0 (mod k - 1) t k and 2  0 mod 2 , there exists a k-DGDD of type (2r )t . Proof. Apply Theorem 2.1 to form a k -BIBD ( V , B ) with t elements. Apply Theorem 4.6 to form a k -HTD of type 2r . To form the k -DGDD, use elements V × {a, b} × {1, . . . , r }. For every B  B , place a copy of the HTD on B × {a, b} × {1, . . . , r }, aligning groups of size 2k on B × {a, b} × {i } for 1 i r , and groups of size 2r on {x} × {a, b} × {1, . . . , r } for x  B . 2 The key construction follows: Theorem 4.8. Let t , r , y be positive integers so that r  0 (mod k(k - 1)), t  1 (mod k(k - 1)), and y  2 (mod k - 1). Suppose that there exist (1) a k-DGDD of type (2r )t ; (2) a k-BIBD on 2t + k - 2 elements; (3) a k-OC(2r + y , y ). Then there is a k-OC(2rt + k - 2 + y , 2r + y + k - 2). Proof. Let V = {ai , j , b i , j : 1 i r , 1 j t } be the elements of the k -DGDD, with groups aligned j t } and H j = {ai , j , b i , j : 1 i r }. Let B be its set of blocks. Adjoin a so that G i = {ai , j , b i , j : 1 set C of k - 2 new elements. For 1 i r , on C  G i , form a k -BIBD on 2t + k - 2 elements, aligning a block on C  {ait , b it }; then delete that block, and call the resulting set of blocks Di . Adjoin a set R

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1447

with y further new elements. For 1 j < t , on R  H j place a k -OC(2r + y , y ) with the hole aligned on R , whose block set is E j . r We consider the design on the 2rt + k - 2 + y elements V  R  C with block set B  i =1 Di  All blocks have size k because each ingredient contains only blocks of size k . First we show that the design is a covering with a hole on R  C  H t . Two elements in the hole do not appear together in a block. An element from G i  H j with j < t appears in a block with each element of C  (G i  H t ) in Di ; it appears in a block with each element of R in E j ; and it appears with each element of H t \ G i in a block of B . Consider two distinct elements x  G i  H j and y  G m  H n with j , n < t . If i = m and j = n, then {x, y } = {ai , j , b i , j } appears in a block of Di (and also in at least one block of E j ). If i = m and j = n, then {x, y } appears in at least one block of E j . If i = m and j = n, then {x, y } appears in one block of Di . If i = m and j = n, then {x, y } appears in one block of B . Hence the design is a covering with a hole on R  C  H t . Secondly, we establish that it has the correct excess degrees to be an optimal covering with a hole, a k -OC. The design has 2rt + k - 2 + y elements. Because r  0 (mod k - 1), the number of elements satisfies 2rt + k - 2 + y  y - 1 (mod k - 1). The hole has 2r + k - 2 + y elements. Because r  0 (mod k - 1), the number of elements in the hole satisfies 2r + k - 2 + y  y - 1 (mod k - 1). y < k - 1. We must show that every element not in the Let y  -( y - 1) (mod k - 1) with 0 hole has degree y + 1 in the excess, and every element in the hole has degree 0 in the excess. r }. It We treat elements in the hole first. Each element of C appears only in blocks {Di : 1 i appears in r (t - 1) pairs to be covered, and appears in r (t - 1)/(k - 1) blocks, with (t - 1)/(k - 1) blocks arising in each of {Di : 1 i r } because this was constructed from a BIBD. Each element j < t }. Because elements of R have excess degree 0 in the k of R appears only in blocks {E j : 1 OC(2r + y , y ) forming E j , they have excess degree 0 in the union. Each element of H t appears only in blocks of B , and has excess degree 0. Now consider an element x  G i  H j , with j = t so that x is not in the hole. Then x appears in elements of B , Di and E j . It appears in 2(r - 1)(t - 1)/(k - 1) blocks of B , because it arises from the DGDD. It appears in (2t + k - 3)/(k - 1) blocks of Di , because it arises from a BIBD. Now in E j , x is not in the hole of the k -OC(2r + y , y ), and hence it arises 1 (2rt + k - 2 + y + y ) blocks, and in (2r - 1 + y + y )/(k - 1) blocks. So in total x appears in k- 1 because it appears in (2rt + k - 2 + y ) - 1 pairs, its excess degree is y + 1. Because 2r + y + k - 2  y - 1 (mod k - 1) and y  2 (mod k - 1), the result is the k -OC(2rt + k - 2 + y , 2r + y + k - 2). 2 Corollary 4.9. For each 0 c < k(k - 1), there exist integers mc and c  c (mod k(k - 1)) for which a k-OC(mc + c , c ) exists.
c t -1 j =1 E j .

with mc  0 (mod k(k - 1)) and

Proof. Let t 0 and r0 be integers with t 0  1 (mod k(k - 1)) and t 0 > 1 so that whenever r and t  1 (mod k(k - 1)), (1) there is a k -DGDD of type (2r )t (apply Theorem 4.7), and (2) there is a k -BIBD on 2t + k - 2 ( k (mod k(k - 1))) elements (apply Theorem 2.1).

r0 , t

t0 ,

When c  1 (mod k - 1), apply Corollary 4.4 to choose one k -OC(mc + c , c ) with mc  0 (mod k(k - 1)) and mc r0 . In general, when a k -OC(mc + c , c ) with mc  0 (mod k(k - 1)) and mc r0 exists, Theorem 4.8 produces a k -OC(mc t 0 + k - 2 + c , mc + c + k - 2). Set mc +k-2 mod k(k-1) = mc (t 0 - 1), which exceeds r0 and is a multiple of k(k - 1). Set c +k-2 mod k(k-1) = mc + c + k - 2  c + k - 2 (mod k(k - 1)). Then k - 2 applications of Theorem 4.8 handle all congruence classes. 2 5. Asymptotic existence Our next task is to handle not just one example for hole size in each congruence class modulo k(k - 1), but to extend to all sufficiently large orders. Theorem 5.1. Let k 2 be an integer. Then there are constants k and uk so that whenever v k-OP( v , h) and a k-OC( v , h) with h uk .

k , there is a

1448

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

Proof. By Corollary 4.9, for 0 c < k(k - 1) there are integers mc  0 (mod k(k - 1)) and c  c (mod k(k - 1)) for which a k -OC(mc + c , c ) exists. By Lemma 4.5, for 0 c < k(k - 1) there are integers nc  0 (mod k(k - 1)) and hc  c (mod k(k - 1)) for which a k -OP(nc + hc , hc ) exists. Set uk = max{nc + hc , mc + c : 0 c < k(k - 1)}. Using Theorem 2.4, choose an integer x for which k -GDDs of type (k(k - 1))x exist for all x  {x, x + 1, x + 2, x + 3}. Again using Theorem 2.4, choose an integer r for which an {x + 1, x + 2, x + 3}p uk and all g r . Then set k = rk(k - 1) + uk , a constant GDD of type p g exists for all 1 depending only on k . We develop the remainder of the proof for packings; that for coverings parallels it very closely. Let v k be an integer, and write v =  k(k - 1) + c with 0 c < k(k - 1). Write v - hc = gnc + d so that d  0 (mod k(k - 1)) and d < nc . Let n = nc /(k(k - 1)) and d = d/(k(k - 1)). g Construct a k -GDD of type nc d1 as follows. Form an {x + 1, x + 2, x + 3}-GDD of type (n ) g +1 . Delete all but d elements in one group to form an {x, x + 1, x + 2, x + 3}-GDD of type (n ) g (d )1 . Inflate using weight k(k - 1), employing k -GDDs of type (k(k - 1))x for x  {x, x + 1, x + 2, x + 3}, to g form a k -GDD of type nc d1 . Then add hc new elements, and place a k -OP(nc + hc , hc ) on each group of size nc together with the hc new elements, aligning the hole on these hc elements. The result is a k -OP( v , hc + d), and hc + d uk as required. 2 Proof of Theorem 3.1. When v < k , a maximum packing with blocksize k contains at least v ,k - 2k  blocks, and 2k is a constant. When v k , form a k -OP( v , h) with h uk , which has at least v ,k - uk uk blocks and is a constant. 2 2 2 Proof of Theorem 3.2. When v < k , a minimum covering with blocksize k requires at most 2k blocks, which is a constant. When v k , form a k-OC( v , h) with h uk , which has at most L v ,k blocks. A covering on h points in which every block contains some pair that is covered only once has u at most 2k blocks, which is a constant independent of v . Use this to fill the hole. 2 6. Conclusion For t = 2, our results establish that the elementary Johnson and Schönheim bounds are essentially the correct ones, in that the respective optima cannot differ from them by more than an additive constant. Unless this constant can be shown to be quite small, the specific value obtained for the constant is not of particular interest. Without recourse to Proposition 1.1 or a similar statement, we see no way at present to obtain differences from the bounds that are bounded by a quantity as small as (say) k in general, although it is plausible that such bounds hold. Acknowledgment We thank an anonymous referee for helpful comments on the presentation. References
[1] T. Beth, D. Jungnickel, H. Lenz, Design Theory, vol. I, second edition, Encyclopedia Math. Appl., vol. 69, Cambridge University Press, Cambridge, 1999. [2] Y. Caro, R. Yuster, Packing graphs: the packing problem solved, Electron. J. Combin. 4 (1) (1997), Research Paper 1, approx. 7 pp. (electronic). [3] Y. Caro, R. Yuster, Covering graphs: the covering problem solved, J. Combin. Theory Ser. A 83 (2) (1998) 273­282.  E.G. Straus, On the maximal number of pairwise orthogonal Latin squares of a given order, Canad. J. [4] S. Chowla, P. Erdos, Math. 12 (1960) 204­208. [5] C.J. Colbourn, J.H. Dinitz, D.R. Stinson, Applications of combinatorial designs to communications, cryptography, and networking, in: Surveys in Combinatorics, Canterbury, 1999, in: London Math. Soc. Lecture Note Ser., vol. 267, Cambridge Univ. Press, Cambridge, 1999, pp. 37­100. [6] C.J. Colbourn, V. Rödl, Percentages in pairwise balanced designs, Discrete Math. 77 (1­3) (1989) 57­63. [7] C.J. Colbourn, P.C. van Oorschot, Applications of combinatorial designs in computer science, ACM Comput. Surv. 21 (2) (1989) 223­250. [8] P. Dukes, A.C.H. Ling, Asymptotic existence of resolvable graph designs, Canad. Math. Bull. 50 (4) (2007) 504­518.





Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1449

[9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24]

[25] [26]

 H. Hanani, On a limit theorem in combinatorial analysis, Publ. Math. Debrecen 10 (1963) 10­13. P. Erdos, P. Frankl, V. Rödl, Near perfect coverings in graphs and hypergraphs, European J. Combin. 6 (4) (1985) 317­326. D.A. Grable, More-than-nearly-perfect packings and partial designs, Combinatorica 19 (2) (1999) 221­239. T. Gustavsson, Decompositions of large graphs and digraphs with high minimum degree, PhD thesis, Dept. of Mathematics, Univ. of Stockholm, 1991. S.M. Johnson, A new upper bound for error-correcting codes, IRE Trans. IT-8 (1962) 203­207. A.V. Kostochka, V. Rödl, Partial Steiner systems and matchings in hypergraphs, Random Structures Algorithms 13 (3­4) (1998) 335­347. N.N. Kuzjurin, On the difference between asymptotically good packings and coverings, European J. Combin. 16 (1) (1995) 35­40. E.R. Lamken, R.M. Wilson, Decompositions of edge-colored complete graphs, J. Combin. Theory Ser. A 89 (2) (2000) 149­ 200. J. Liu, Asymptotic existence theorems for frames and group divisible designs, J. Combin. Theory Ser. A 114 (3) (2007) 410­420. H. Mohácsy, The asymptotic existence of group divisible designs of large order with index one, J. Combin. Theory Ser. A 118 (7) (2011) 1915­1924. H. Mohácsy, D.K. Ray-Chaudhuri, An existence theorem for group divisible designs of large order, J. Combin. Theory Ser. A 98 (1) (2002) 163­174. V. Rödl, On a packing and covering problem, European J. Combin. 6 (1) (1985) 69­78. J. Schönheim, On coverings, Pacific J. Math. 14 (1964) 1405­1411. D.R. Stinson, Combinatorial Designs, Springer-Verlag, New York, 2004. V.H. Vu, New bounds on nearly perfect matchings in hypergraphs: higher codegrees do help, Random Structures Algorithms 17 (1) (2000) 29­63. R.M. Wilson, The construction of group divisible designs and partial planes having the maximum number of lines of a given size, in: Proc. Second Chapel Hill Conf. on Combinatorial Mathematics and its Applications, Univ. North Carolina, Chapel Hill, NC, 1970, Univ. North Carolina, Chapel Hill, NC, 1970, pp. 488­497. R.M. Wilson, An existence theory for pairwise balanced designs. III. Proof of the existence conjectures, J. Combin. Theory Ser. A 18 (1975) 71­79. R.M. Wilson, The proportion of various graphs in graph designs, in: R.A. Brualdi, S. Hedayat, H. Kharaghani, G. Khosrovshahi, S. Shahriari (Eds.), Combinatorics and Graphs: The Twentieth Anniversary Conference of IPM Combinatorics, American Mathematical Society, Providence, RI, 2010, pp. 251­255.

Perspectives

The BioIntelligence Framework: a new computational platform for biomedical knowledge computing
Toni Farley,1 Jeff Kiefer,1 Preston Lee,1 Daniel Von Hoff,1 Jeffrey M Trent,1 Charles Colbourn,2 Spyro Mousses1
< Additional material are

published online only. To view these files please visit the journal online (http://dx.doi.org/ 10.1136/amiajnl-2011-000646).
1

The Translational Genomics Research Institute (TGen), Center for BioIntelligence, Phoenix, Arizona, USA 2 School of Computing, Informatics, Decision Systems Engineering, Arizona State University, Tempe, Arizona, USA Correspondence to Dr Spyro Mousses, The Translational Genomics Research Institute (TGen), Center for BioIntelligence, 445 N. Fifth Street, Phoenix, AZ 85004, USA; smousses@tgen.org Received 19 October 2011 Accepted 7 July 2012 Published Online First 2 August 2012

ABSTRACT Breakthroughs in molecular profiling technologies are enabling a new data-intensive approach to biomedical research, with the potential to revolutionize how we study, manage, and treat complex diseases. The next great challenge for clinical applications of these innovations will be to create scalable computational solutions for intelligently linking complex biomedical patient data to clinically actionable knowledge. Traditional database management systems (DBMS) are not well suited to representing complex syntactic and semantic relationships in unstructured biomedical information, introducing barriers to realizing such solutions. We propose a scalable computational framework for addressing this need, which leverages a hypergraph-based data model and query language that may be better suited for representing complex multilateral, multi-scalar, and multi-dimensional relationships. We also discuss how this framework can be used to create rapid learning knowledge base systems to intelligently capture and relate complex patient data to biomedical knowledge in order to automate the recovery of clinically actionable information.

genomic interpretation require: (a) a fundamentally different computational framework for storing and representing disparate data types with complex relationships, and (b) advanced software applications that leverage this framework to structure the representation of prior knowledge so that it can be intelligently linked to patient data. We propose a framework conceptually based on requirements and cognitive strategies for knowledge computing, previously introduced as the BioIntelligence Framework.2 Our framework is compatible with future directions toward computational intelligence. Since it can support the capturing and querying of multilateral and multi-scalar relations among genomes, phenotype, environment, lifestyle, medical history, and clinical outcome data, our platform can support systems with higher order functions such as inference and learning. This will ultimately allow genomic data to be intelligently repurposed beyond personalized medicine to support more sophisticated translational research and highly iterative knowledge discovery.

BIOINTELLIGENCE FRAMEWORK INTRODUCTION
Next generation genomic profiling technologies are generating deep and detailed characterizations of patients and disease states. This data-intensive approach is providing unprecedented insights that can be used to resolve mechanistic complexity and clinical heterogeneity, thereby revolutionizing how we study, manage, and treat complex diseases. To support this revolution, bioinformatics tools are rapidly emerging to process and analyze large-scale complex molecular data sets for discovery research applications. Unfortunately, when it comes to clinical (n¼1) applications of genomics, the data deluge is rapidly outpacing our capacity to interpret rich data sets to extract medically useful and meaningful knowledge. The next great challenge will be to address the manual interpretation bottleneck through the development of computational solutions for intelligently linking complex patient data to actionable biomedical knowledge. This illuminates a need to represent and query large-scale complex relationships distributed across disparate types of biomedical knowledge. A recent report states a goal for the community is to transition from traditional database management to managing potentially unstructured data across many repositories.1 We propose the key challenges for intelligently linking prior knowledge to partially automate
128

Systems biology is concerned with emergent properties in complex interactions of systems of systems involving disparate data elements. Extracting useful information requires syntactic and semantic linking of data within and across large data sets. Systems modeled as networks based on binary graphs (where edges connect node pairs) are suited to capturing bilateral relationships and interactions. To represent multilateral relationships requires a fundamental change in how we model systems. We generalize the binary graph model to a hypergraph model, an approach which has been previously suggested,3 and introduce a hypergraphbased solution for representing multilateral relations and multi-scalar networks. Biological systems may benefit from a flexible data model that supports nesting of data elements and concept abstraction in a more natural manner than functionally equivalent relational counterparts, and the ability to readily query across multiple systems and abstraction layers representing complex relationships, leading to systems compatible with learning, reasoning, and inferencing. Following a model for human intelligence, information lives in different levels of the neocortex: from highly variable data inputs, to patterns, to patterns of patterns, to invariant concepts.4 Inspired by this model of intelligence, we extend the notion of a hypergraph to allow

J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

Perspectives
links among edges to capture relationships that cross bounds of scale and dimension, and develop a novel generic framework for capturing information that can benefit systems biology and other areas. We desire a solution that is flexible to include various types of data from disparate sources, extensible to scale to massive stores of information, and accessible to permit the efficient extraction of patient-centric knowledge. Figure 1 outlines the architecture of our BioIntelligence Framework, the components of which are: 1. A public hypergraph-based network for representing knowledge, including a. A scalable hypergraph-like model for representing a knowledge network b. Processes to automate populating and updating the network with public domain knowledge from multiple sources c. An efficient database solution for storing the network platform 2. A Patient Data Locker application built on top of the knowledge network, including: d. An accessible web-based solution for storing patient-centric knowledge e. Processes for structuring and formatting patient genomic and health data, inducing patient-centric subgraphs on the public hypergraph, and stratifying patients based on information in their lockers 3. A process for structuring and formatting analyst interpretation to facilitate feedback and rapid automated learning in the system.

Public hypergraph
A graph is defined G(V,E) where V is a set of vertices (nodes) and E is a set of edges (links) between two vertices. A hypergraph is a generalization of a graph in which an edge can connect any number of vertices. Biological networks have traditionally been modeled as graphs/networks. These graph models capture bilateral relationships among node pairs. Using hypergraphs as a modeling paradigm supports the characterization of multilateral relationships and processes.3 For example, in a general graph,

Figure 1 A BioIntelligence Framework for creating a hypergraph-like store of public knowledge and using this, along with an individual's genomic and other patient information, to derive a personalized genome-based knowledge store for clinical translation and discovery research.
J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646 129

Perspectives
an edge might represent a relationship between a gene and disease state. That relationship may change in the context of a drug, and a hypergraph can represent this contextual knowledge with an edge containing all three elements. Hypergraphs are proving useful for capturing semantic and biomedical information in semantic web technologies for biological knowledge management and semantic knowledge networks.5 6 Approaches to extracting knowledge from biological research literature to store in a hypergraph have been proposed,7 8 with similar techniques used for population stratification.9 To support a data intelligent system, we wish for information to be stored not only explicitly in the data itself, but implicitly in how the data are linked, and capture this in our model. Abstraction is permitted by allowing hyperedges to contain other edges, forming a nested graph structure. A machine learning model called hierarchical temporal memory (HTM) mimics the human neocortex.4 10 Inspired by this design, we store knowledge at different levels of granularity, from single points of data, to collections of points, abstracting out to collections of collections. Thus, we perceive edges in lower levels of abstraction as nodes in higher levels, thereby permitting the network to be viewed and operated on within and across different scales of abstraction. Information is stored on the nodes and edges of the network and accessed via processes. Populating the network platform requires processes to pull different types of information from multiple sources, such as the Cancer Biomedical Informatics Grid (caBIG) and BioWarehouse,11 12 and other tools and techniques.13e15 The processes are represented as S0. Sm in figure 1, and take unstructured, public domain knowledge as input and structures it as input to the public network. The most common database management system (DBMS) is based on the relational data model, which is best suited to capturing data structured in a predefined schema, and presents limitations in handling complexity and scalability.16 Our solution handles unstructured and semi-structured data, and is in the scope of non-relational (NoSQL) databases, which do not require pre-defined schemas.16 17 Such databases include those based on graph models and triplestores (eg, RDF), and are best suited to capturing binary relationships between two elements (a triplestore is even more restrictive as it is essentially a `directed' binary graph). To effectively represent multilateral relationships and interactions present in biomedical data, hypergraph-based approaches have been suggested.3 18e21 The flexibility granted by allowing elements to contain elements allows processing knowledge at different levels of abstraction, in a less restrictive way than hierarchical models that restrict the network's topology. HyperGraphDB resembles our model, but uses a directed hypergraph and is built on a traditional database.22 Our model is more general, based on less restrictive undirected edges, and intended to be implemented natively, although a DBMS using our model may use other DB solutions as a persistent data store. In fact, other solutions can be built on our model as it is a generalization of other models. found in the online supplementary material, and demonstrates the added effort involved in capturing these complex relationships in a SQL database. The entities in figure 2 are represented as elements in our database. An attribute is a key/value pair, and a list of attributes (Attribute Set) is stored with each element. In this way, we can arbitrarily add any type of attribute to an element without changing the database structure. In the relational model, each entity type requires its own table, with pre-defined fields for attributes, and adding an attribute requires adding a field to the table, and migrating the database. A characteristic of the relational model is that all entities of the same type are stored in the same table. Our model is flexible in that it does not require prestructuring data, but we can certainly mimic this behavior if desired by enforcing a rule that requires all elements to have an attribute with key¼`type.' Using our model, this decision is left to the database designer, and not enforced by the model itself. An element in our model can contain an arbitrary number of other elements (allowing `has-a' and `has-many ' relationships). These elements are referenced in the `Internal Element Set' of the element. For example, in figure 2, element g is a gene and contains three gene variant elements (v1, v2, v3) in its internal element set. This is an example of a multi-lateral relationship as g can be viewed as a `hyperedge' in a hypergraph, connecting three `nodes.' While this behavior is easy to model in a relational database, using a one-to-many relation, it becomes more complex when an element contains an arbitrary number of arbitrary types of elements. For example, the element R2 represents a molecular state, which in this case is a protein p associated with a protein state ps1. In a relational database, we can capture these relationships in a `molecular state' table with foreign key fields pointing to a `protein' table and a `protein state' table. Now consider a molecular state that exists in the context of a modifier drug (ie, the state of the protein is perturbed by a modifier drug in a laboratory experiment). To capture this in our model, we can simply create a new element with three internal elements: p, ps1 and the element representing the modifier drug. To capture this in the relational database, we would need to either add a field to the `molecular state' table, which may be blank for many records, or create a new table with the three related fields. Both of the later options require a change to the underlying data structure, and migrating the database. The elements shown in figure 2C represent higher-level concepts and recursive nesting of elements at different levels, illustrating the ability to flexibly and efficiently capture multiscalar relationships among elements, the motivation behind our data model. For example, note that R1 captures the relationship that gene g codes for protein p. C1 captures the genetic event concept, where gene variant v2 changes the state of the protein (the molecular state represented by R3). C2 is a drug response concept representing the higher-level concept that C1, in the context of the pharmaceutical ph, leads to a changed molecular state, R2. These combined biological and pharmacologic effects lead to a change in disease state to ds2, captured by the clinical response concept C3. The internal (nested) element sets contain the topology of the network, that is, they define how entities are related, and capture meaning in those relationships. Further details to describe the nature of the relationships can always be stored in attributes of the elements. The `External Element Set' of an element is the inverse of internal element relations. For example, v1, v2, and v3 all have g in their external element set. While it is not necessary to store this information (the network of relationships among elements
J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

Data model
An example use of our model is shown in figure 2. This example captures multilateral, multiscalar, and multidimensional relationships using a general model (A), and viewing the network at different levels of abstraction (B and C). The elements shown in this solution are described in the legend of the image, and based on a real-world problem we are exploring. A possible schema model for a relational database capturing these same data can be
130

Perspectives
Figure 2 An illustrative example of storing biomedical information in our proposed knowledge base: a component of the BioIntelligence Framework. A shows our data model, and describes its components. B and C show the elements described in the legend (at the bottom of the figure) at two different levels of abstraction.

can be constructed via the internal element sets alone), it does aid in querying the database. We have defined three new types of queries associated with our model: recover, context, and expand. The expand and context queries retrieve all internal and external elements of an element, respectively, optionally limited by modifiers presented with the query. Recover is a combination of these, and returns both internal and external elements. All three query actions have an optional level constraint, defining how deep to traverse the graph when retrieving related elements. For instance, expand n will retrieve all internal elements, and their internal elements, recursively up to n times. For instance, we can view C3 as an abstraction; a clinical effect that we can relate to patients and other concepts. Expanding C3 by one level shows us that it represents a disease state ds2, triggered by a pharmacologic effect C2. Expanding C3 by two levels shows us the
J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

details of the pharmacologic effect, and so on. In this way, we can choose which level of abstraction we wish to view and compute over, and we can create new concepts that cross these layers of abstraction (multi-scalar relationships). By the definition of the internal and external element sets, it follows that our model naturally handles many-to-many relationships as well. In summary, we do not argue that a relational database is incapable of capturing the types of relationships we discuss here, rather that it requires more work, and added layers of complexity to the underlying structure of the database, which makes capturing and querying complex biomedical relationships more difficult. Our model is an abstraction of other models, including relational, graph, hierarchical, and object-oriented, and can therefore be used to model data represented using any and all of these models at once. The potential benefit of our model is
131

Perspectives
Figure 3 The network on the left is an example knowledge network platform. The darkened nodes represent gene variants present in an individual genome. The network on the top right is a genome-induced subgraph of the network. The network on the bottom right is a genome-induced subgraph, expanded out to include additional knowledge stored on edges in the connected-component each data element is contained in.

that it provides levels of scalability and flexibility that are difficult to achieve with existing models. We are currently developing a solution based on this model and will present additional details of the model and related query language in future publications.

Patient data locker
Given a patient's data (genomic, health, etc), we wish to recover related knowledge from our network using BioIntelligence Tools (BIT). The first step (BIT1 in figure 1) is a process to structure data as input to a process for inducing patient-relevant subgraphs of the knowledge network (BIT2 in figure 1). BIT1 integrates many types of data sets across multiple databases to support electronic medical and health records (EMR/EHRs), and is designed as a modular based system to provide metadata and indexing for queries. The next step (BIT2 in figure 1) is a process to extract relevant knowledge from the network based on individual patient information. An induced subgraph H(S,T) of network G(V,E) has the properties S3V, and for every vertex set Si of H, the set Si is an edge of H if and only if it is an edge in G. That is, H has the same edges that appear in G over the same set of nodes. We say that H is an induced subgraph of G, and H is induced by S. In our system, V is the set of nodes in the platform network G, and S is the set of nodes that map to an individual's genomic and health information. Thus, a subgraph is induced by an individual genome. The architecture in figure 1 shows an example public hypergraph, and private subgraphs stored in a data locker. These networks are detailed in figure 3, where the network on the left is a public knowledge network, and the darkened nodes are elements relevant to an individual's information (based on input patient data). The network on the top right is induced by this information, and contains all of the darkened nodes, and edges incident to them. Alternately, we can expand the information retrieved to include all connected components of the induced subgraph. An example of a connected-component induced subgraph is shown on the bottom right of figure 3. The most important characteristic of the data locker is that it contains all relevant knowledge to facilitate clinical translation. Induced subgraphs can be used to transform a large set of patient-relevant data to smaller, task-tailored formats void of extraneous detail. The patient data locker is linked to the public
132

knowledge store, and automatically updated to contain only a subset of information related to the patient. Thus, an expert need not develop their own intricate search queries and perform the tedious task of progressively reducing the amount of irrelevant data returned by the query. Any query that can be run on the entire knowledge network, can be run on the subgraph in a patient's locker, leading to a more precise subset of knowledge returned, and potentially faster querying speeds as the search space is reduced. The expert analyst is provided with knowledge tailored to a particular patient, partially automating the interpretation process, and a process (BIT3 in figure 1) allows the analyst to input new interpretation knowledge into the public network. We envision this type of feedback mechanism will support the inclusion of a learning model for our system, and allow the community to contribute to its growth. The system is diverse, providing framework and template libraries, allowing users to integrate their own tools for analysis, data collection, and beyond.

CONCLUSION
A deluge of biomedical data generated from next-generation sequencing (NGS) and clinical applications is overwhelming our ability to efficiently extract value from it. Existing bioinformatics tools were not developed to support clinical translation for an individual patient, causing an n¼1 translation bottleneck. A new architecture for managing biomedical data is desired, and we present the BioIntelligence Framework as a genomecompatible biomedical knowledge representation platform. Our future efforts to achieve the goals outlined in this paper include ensuring that we develop algorithms on this framework that minimally meet the performance expectations of existing solutions in practice.
Contributors All authors contributed to the ideas behind the framework. TF, CC, and PL contributed computer science expertise. JK, DVH, JMT, and SM contributed expertise in clinical genomics and translational research. Competing interests None. Provenance and peer review Commissioned; externally peer reviewed.

Open Access This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 3.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/3.0/ J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

Perspectives
REFERENCES
1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. Agrawal R, Ailamaki A, Bernstein PA, et al. The Claremont report on database research. ACM SIGMOD Record 2008;37:9e19. Mousses S, Kiefer J, Von Hoff D, et al. Using biointelligence to search the cancer genome: an epistemological perspective on knowledge recovery strategies to enable precision medical genomics. Oncogene 2008;27:S58e66. Klamt S, Haus U, Theis F. Hypergraphs and cellular networks. PLoS Comput Biol 2009;5:e1000385. Hawkins J, Blakeslee S. On Intelligence. New York: Times Books, 2004. Antezana E, Kuiper M, Mironov V. Biological knowledge management: the emerging role of the semantic web technologies. Brief Bioinform 2009;10:392e407. Zhen L, Jiang Z. Hy-SN: hyper-graph based semantic network. Knowledge-Based Systems 2010;23:809e16. Vailaya A, Bluvas P, Kincaid R, et al. An architecture for biological information extraction and representation. Bioinformatics 2005;21:430e8. Mukhopadhyay S, Palakal M, Maddu K. Multi-way association extraction and visualization from biological text documents using hyper-graphs: applications to genetic association studies for diseases. Artif Intell Med 2010;49:145e54. Vazquez A. Population stratification using a statistical model on hypergraphs. Phys Rev E Stat Nonlin Soft Matter Phys 2008;77:1e7. George D. How the Brain Might Work: a Hierarchical and Temporal Model for Learning and Recognition [dissertation]. Palo Alto, California: Stanford University, 2008. NCI. Cancer Biomedical Informatics Grid (caBIG). https://cabig.nci.nih.gov/ (accessed 16 Sep 2011). 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. Karp P. Biowarehouse database integration for bioinformatics. http://biowarehouse ai.sri.com/ (accessed 16 Sept 2011). Chen H, Ding L, Wu Z, et al. Semantic web for integrated network analysis in biomedicine. Components 2009;10:177e92. Tudor CO, Schmidt CJ, Vijay-Shanker K. eGIFT: mining gene information from the literature. BMC Bioinformatics 2010;11:418. Valentin F, Squizzato S, Goujon M, et al. Fast and efficient searching of biological data resourceseusing EB-eye. Brief Bioinform 2010;11:375e84. Leavitt N. Will NoSQL databases live up to their promise? Computer 2010;43:12e14. Angles R, Gutierrez C. Survey of graph database models. ACM Computing Surveys 2008;40:1e39. Olken F. Graph data management for molecular biology. OMICS 2003;7:75e8. Hu Z, Mellor J, Wu J, et al. Towards zoomable multidimensional maps of the cell. Nat Biotechnol 2007;25:547e55. Spreckelsen C, Spitzer K. Formalising and acquiring model-based hypertext in medicine: an integrative approach. Methods Inform Med 1998;37:239e46. Wu G, Li J, Hu J, et al. System: a native RDF repository based on the hypergraph representation for RDF data model. J Comput Sci Technol 2009;24:652e64. Iordanov B. HyperGraphDB: a generalized graph database. Proceedings of the 2010 International Conference on Web-age Information Management 2010:25e36.

PAGE fraction trail=5.5

J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

133

SIAM J. DISCRETE MATH. Vol. 27, No. 4, pp. 1844­1861

c 2013 Society for Industrial and Applied Mathematics 

SEQUENCE COVERING ARRAYS

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

YEOW MENG CHEE , CHARLES J. COLBOURN , DANIEL HORSLEY§ , AND JUNLING ZHOU¶ Abstract. Sequential processes can encounter faults as a result of improper ordering of subsets of the events. In order to reveal faults caused by the relative ordering of t or fewer of v events, for some fixed t, a test suite must provide tests so that every ordering of every set of t or fewer events is exercised. Such a test suite is equivalent to a sequence covering array, a set of permutations on v events for which every subsequence of t or fewer events arises in at least one of the permutations. Equivalently it is a (different) set of permutations, a completely t-scrambling set of permutations, in which the images of every set of t chosen events include each of the t! possible "patterns." In event sequence testing, minimizing the number of permutations used is the principal objective. By developing a connection with covering arrays, lower bounds on this minimum in terms of the minimum number of rows in covering arrays are obtained. An existing bound on the largest v for which the minimum can equal t! is improved. A conditional expectation algorithm is developed to generate sequence covering arrays whose number of permutations never exceeds a specified logarithmic function of v when t is fixed, and this method is shown to operate in polynomial time. A recursive product construction is established when t = 3 to construct sequence covering arrays on vw events from ones on v and w events. Finally computational results are given for t  {3, 4, 5} to demonstrate the utility of the conditional expectation algorithm and the product construction. Key words. sequence covering array, completely scrambling set of permutations, covering array, directed t-design AMS subject classifications. 05B40, 05B15, 05B30, 05A05 DOI. 10.1137/120894099

1. Introduction. A set of permutations {1 , . . . , N } of a v -element set X is completely t-scrambling if for every ordered t-set (x1 , . . . , xt ) with xi  X for 1  i  t, there is some  (1    N ) for which  (xi ) <  (xj ) if and only if i < j . Spencer [32] first explored the existence of completely t-scrambling sets of permutations in generalizing a question of Dushnik [15] on linear extensions. Recently Kuhn et al. [23, 24] examined an equivalent combinatorial object, the sequence covering array. For parameters N , t, and v , such an array is a set of n permutations of v letters so that every permutation of every t of the v letters appears--in the specified order--in at least one of the n permutations. The motivation for finding sequence covering arrays with small values of n arises in event sequence testing. Suppose that a process involves a sequence of v tasks or events. The operator may, unfortunately, fail to do the tasks in the correct sequence. When this happens, errors may occur. But we anticipate that errors can be attributed to the (improper) ordering of a small
 Received by the editors October 8, 2012; accepted for publication (in revised form) September 4, 2013; published electronically October 28, 2013. http://www.siam.org/journals/sidma/27-4/89409.html  School of Physical and Mathematical Sciences, Nanyang Technological University 637371, Singapore (YMChee@ntu.edu.sg).  School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85257, and State Key Laboratory of Software Development Environment, Beihang University, Beijing 100191, China (colbourn@asu.edu). This author was supported by Australian Research Council grant DP120103067. § School of Mathematical Sciences, Monash University, Melbourne, Australia (danhorsley@ gmail.com). This author was supported by Australian Research Council grants DP120103067 and DE120100040. ¶ Department of Mathematics, Beijing Jiaotong University, Beijing, China (jlzhou@bjtu.edu.cn).

1844

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1845

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

subset of tasks. When each permutation of a sequence covering array is used in turn to specify a task order, every potential ordering of t or fewer tasks will be tried and hence all errors found that result solely from the improper ordering of t or fewer tasks. Applications are discussed further in [19, 23, 39, 40]; related event sequence testing problems in which tasks can be repeated are discussed in [42, 43, 44]. While the application of these combinatorial structures is of much practical concern, our interest is in bounds on the size of sequence covering arrays and their explicit construction. We first state the problem formally. Let  = {0, . . . , v - 1} be symbols that represent the v tasks or events. A t-subsequence of  is a t-tuple (x1 , . . . , xt ) with xi   for 1  i  t, and xi = xj when i = j . A permutation  of  covers the t-subsequence (x1 , . . . , xt ) if  -1 (xi ) <  -1 (xj ) whenever i < j . For example, with v = 5 and t = 3, (4, 0, 3) is a 3-subsequence that is covered by the permutation 4 2 0 3 1. A sequence covering array of order v and strength t, or SeqCA(N ; t, v ), is a set  = {1 , . . . , N }, where i is a permutation of , and every t-subsequence of  is covered by at least one of the permutations {1 , . . . , N }. Often the permutations are written as an N × v array. We use an array representation for completely t-scrambling sets of permutations as well. An N × v array is a completely t-scrambling set of permutations of strength t on v symbols, or CSSP(N ; t, v ), when the columns are indexed by  and the symbols by , and for every way c1 , . . . , ct to choose t distinct columns and every permutation  of {1, . . . , t}, there is a row  for which, for every 1  a < b  t, the entry in cell (, c(a) ) is less than the entry in cell (, c(b) ). Lemma 1.1. A CSSP(N ; t, v ) is equivalent to a SeqCA(N ; t, v ). Proof. If 1 , . . . , N are the N permutations of a SeqCA(N ; t, v ), form an N × v -1 array A in which cell (i, j ) contains i (j ). Then A is a CSSP(N ; t, v ). In the opposite direction, if A is a CSSP(N ; t, v ), define permutation i by setting i (aij ) = j for 1  i  N and 0  j < v . Then 1 , . . . , N form the N permutations of a SeqCA(N ; t, v ). In the CSSP(8;3,5) of Table 1.1, the symbols {1, 2, 3} appear as 123 once, 132 once, 213 once, 231 zero times, 312 four times, and 321 once. Hence the rows of a completely t-scrambling set of permutations do not necessarily produce a sequence covering array; nevertheless they are conjugates, obtained by interchanging the roles of columns and symbols. Permutation problems concerning the avoidance of specified patterns of subsequences have been extensively studied in algebraic and probabilistic combinatorics; see [33] for an excellent survey. (Here two subsequences (x1 , . . . , xt ) and (y1 , . . . , yt ) have the same pattern when, for 1  i < t, xi < xi+1 if and only if yi < yi+1 .)
Table 1.1 Example: SeqCA(8;3,5) ­ t = 3, v = 5, N = 8. SeqCA 4203 1430 3120 0241 2134 0341 3021 4120 CSSP 2413 3042 3120 0314 4102 0341 1320 3124

1 2 4 3 0 2 4 3

0 1 4 2 3 2 4 0

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1846

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

While cosmetically similar to pattern avoidance problems, the existence problem for sequence covering arrays requires coverage rather than avoidance and requires that all subsequences be covered and not simply every pattern. The question of principal concern in this paper is as follows: Given t and v , what is the smallest N for which a CSSP(N ; t, v ) (equivalently, a SeqCA(N ; t, v )) exists? Call this number SeqCAN(t, v ). In the vernacular of completely t-scrambling sets of permutations, Spencer [32] did the foundational work, and F¨ uredi [17], Ishigami [20, 21], Radhakrishnan [31], and Tarui [36] made improvements. In a sequence covering array, every t symbols must appear in each of the t! possible orderings, and there are v t t! t-subsequences in total, so t!  SeqCAN(t, v )  v t! t

Both bounds are trivial, but the lower bound is the correct one when t = 2. Lemma 1.2. SeqCAN(2, v ) = 2 for all v  2. Proof. Any permutation on v symbols and its reversal form a SeqCA(2; 2, v ). When t  3, neither bound is correct as v increases. Indeed the growth as a function of v for fixed t is logarithmic. Theorem 1.3 (see [31, 32]). For t  3, 1+ 2 log2 (e) (t - 1)! v 2v - t + 1 log2 (v - t + 2)  SeqCAN(t, v )  t log(v ) log
t! t!-1

.

The question of when SeqCAN(t, v ) = t! is of independent interest in yet another setting. Let V be a finite set; an element of V is a vertex. A transitive tournament on V is a directed graph in which (1) for all x  V , (x, x) is not an arc; (2) for distinct x, y  V , (x, y ) is an arc if and only if (y, x) is not an arc; and (3) whenever (x, y ) and (y, z ) are arcs, so is (x, z ). A transitive tournament T = (V, A) has transitive tournament T  = (W, B ) as a subdigraph, denoted T   T , whenever W  V and B  A. Let (V, T ) be a finite set V of cardinality v and a collection T with every T  T being a transitive tournament on k of the vertices in V ; members of T are blocks. Then (V, T ) is a (t, )-directed packing of blocksize k and order v , or DP (t, k, v ), if for every X  V with |X | = t and every transitive tournament T  on vertex set X , |{T  T : T   T }|  . On the other hand, (V, T ) is a (t, )-directed covering of blocksize k and order v , or DC (t, k, v )), if for every X  V with |X | = t and every transitive tournament T  on vertex set X , |{T  T : T   T }|  . When (V, T ) is a DP (t, k, v ) and also a DC (t, k, v ), it is a (t, )-directed design of blocksize k and order v , or DD (t, k, v ). In this notation, the subscript is often omitted when  = 1. Directed designs with t = 2 have been extensively studied as generalizations of balanced incomplete block designs. The study of (t, 1)-directed packings has also been extensive as a result of their equivalence to "deletion-correcting codes" (see Levenshtein [25]). The connection with our investigation follows.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1847

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Lemma 1.4. A CSSP(N ; t, v ) is equivalent to a DC(t, v, v ) with N blocks. Moreover, a DD(t, v, v ) exists if and only if SeqCAN(t, v ) = t!. Proof. For each row (a0 , . . . , av-1 ) of the CSSP, form a transitive tournament on vertex set {0, . . . , v - 1} by including, for 0  i < j < v , arc (i, j ) if ai < aj and arc (j, i) otherwise. Each transitive tournament on t of these vertices is a subdigraph of at least one of these N tournaments. The other direction is similar. When N = t!, every t-subsequence is covered exactly once, and every transitive tournament of order t arises as a subdigraph exactly once. Theorem 1.5. Sufficient conditions for a DD(t, v, v ) to exist include 1. t  3 and t  v  t + 1 [25] and 2. t = 4 and v = 6 [28]. Necessary conditions for a DD(t, v, v ) to exist include 1. v  t + 1 for t  {3, 5, 6} [28], 2. v  t + 2 for t = 4 [28], and - 1 for t  7 [28]. 3. v  t+1 2 Levenshtein [25] had conjectured that v  t + 1 whenever a DD(t, v, v ) exists for t  3. As stated in Theorem 1.5, this does not hold for t = 4, but this is the only known exception to Levenshtein's conjecture. In the next section we significantly reduce the upper bound on the largest v for which SeqCAN(t, v ) can equal t!. 2. Lower bounds. Here we extend a technique used in [17, Theorem 5.1], improving on a method of Ishigami [21]. We require a number of previous results on covering arrays, introduced next. See [8] for a more thorough introduction to them. Let N , k , t, and v be positive integers. Let C be an N × k array with entries from an alphabet  of size v ; we typically take  = {0, . . . , v - 1}. When (1 , . . . , t ) is a t-tuple with i   for 1  i  t, (c1 , . . . , ct ) is a tuple of t column indices (ci  {1, . . . , k }), and ci = cj whenever i = j , the t-tuple {(ci , i ) : 1  i  t} is a t-way interaction. The array covers the t-way interaction {(ci , i ) : 1  i  t} if, in at least one row  of C, the entry in row  and column ci is i for 1  i  t. Array C is a covering array CA(N ; t, k, v ) of strength t if it covers every t-way interaction. CAN(t, k, v ) is the minimum N for which a CA(N ; t, k, v ) exists. The basic goal is to minimize the number of rows (tests) required and hence to determine CAN(t, k, v ). When t  2 and v  2 are both fixed, CAN(t, k, v ) is (log k ) (see, for example, [8]). We strengthen this standard definition somewhat. For a t-way interaction T = {(ci , i ) : 1  i  t} with symbols chosen from  = {0, . . . , v - 1}, let  (T ) = -1 |{i : i =  }|. Then define µ(T ) = v =0  (T )!. The natural interpretation is that µ(T ) is the number of ways to permute the columns (c1 , . . . , ct ) so that the symbols appear in the same order. A covering array provides excess coverage when every tway interaction T is covered by at least µ(T ) rows; such a covering array is denoted by CAX (N ; t, k, v ). More generally, the subscript X is used to extend notation from covering arrays to those having excess coverage. Theorem 2.1. Let v , t, and a be integers satisfying v  t  3 and t  a  0. Then SeqCAN(t, v )  a!CANX (t - a, v - a, a + 1). Proof. Let S be a CSSP(N ; t, v ). Choose any a columns of S, {e1 , . . . , ea }. For each ordering  of these columns, form a matrix C that contains all rows of S in which the entry in column  (ei ) is less than that in column  (ei+1 ) for 1  i < a. Because there are a! orderings and every row of S appears in exactly one of the {C }, it suffices to show that for every choice of  the number n of rows in C is at least CANX (t - a, v - a, a + 1). To do this, form an n × (v - a) array A whose columns are the columns of C that are not among the a selected. To determine the content of

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1848

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

cell (r, c) of A , examine the symbol  in column c and row r of C . If  is less than the symbol in row r and column  (e1 ), the entry is set to 0. If  is greater than the symbol in row r and column  (ea ), the entry is set to a. Otherwise find the unique j for which  is greater than the symbol in row r and column  (ej ) but less than the symbol in row r and column  (ej +1 ), and set the entry to j . Now we claim that A is a CAX (n; t - a, v - a, a + 1). The verification requires demonstrating that every (t - a)-way interaction T is covered at least µ(T ) times. So let T = {(fi , i ) : 1  i  t - a}, noting that i  {0, . . . , a} and fi indexes a column of A. We form permutations of {e1 , . . . , ea }  {f1 , . . . , ft-a } that are consistent with  on {e1 , . . . , ea } in that these columns appear in the order prescribed by  . To do this, there are 0 (T ) columns with entry 0; place one of the 0 (T )! orderings of these columns so that all appear before  (e1 ). There are a (T ) columns with entry a; place one of the a (T )! orderings of these columns so that all appear after  (ea ). For 1  j < a, there are j (T ) columns with entry j ; place one of the j (T )! orderings of these columns so that all appear before  (ej ) and after  (ej +1 ). In this way we can form µ(T ) permutations of {e1 , . . . , ea }  {f1 , . . . , ft-a }, each consistent with  . Because each is consistent with  , it appears in C . But each such appearance in C results in a different row of A that covers T , and hence T is indeed covered at least µ(T ) times. The easiest applications of Theorem 2.1 result from using CAN(t, k, v ) as a lower bound for CANX (t, k, v ). Apply it with a = t - 1, noting that CAN(1, k, t) = t for all k  1, to recover the trivial lower bound that SeqCAN(t, v )  t!. Apply it with a = t - 2 to establish that SeqCAN(t, v )  (t - 2)!CAN(2, v - t + 2, t - 1). Now we return to the question of when SeqCAN(t, v ) can equal t!, or equivalently when a DD(t, v, v ) can exist. Theorem 2.1 ensures that SeqCAN(t, v )  (t - 2)!CANX (2, v - t + 2, t - 1), so SeqCAN(t, v ) = t! can hold only when CANX (2, v - t + 2, t - 1)  t(t - 1). The 2-way interaction T = {(c1 , 1 ), (c2 , 2 )} has µ(T ) = 2 exactly when 1 = 2 (called a constant pair) and has µ(T ) = 1 otherwise (a nonconstant pair). Because, for each pair of columns, t - 1 constant pairs must be covered twice each, and (t - 1)(t - 2) nonconstant pairs must be covered at least once each, CANX (2, v - t + 2, t - 1)  t(t - 1). So we are concerned with when equality can hold. Lemma 2.2. For v  4, CANX (2, k, v ) = v (v + 1) only if k  v + 2. Proof. Suppose that a CAX (v (v + 1); 2, k, v ) exists with columns indexed by {1, . . . , k } and symbols by {0, . . . , v - 1}. We form sets on symbols V = ({1, . . . , k } × {0, . . . , v - 1})  {}. The system of sets (blocks) B is formed as follows. For every row (x1 , . . . , xk ) of the covering array, a set {(i, xi ) : 1  i  k } is placed in B . Then for every 1  i  k , a set {(i, j ) : 0  j < v }  {} is placed in B . The set system (V, B ) has kv + 1 symbols and v (v + 1) + k blocks. By construction, every two different symbols appear together in exactly one block, unless the pair is of the form {(i, j ), (i , j )} corresponding to a constant pair and therefore occurring in exactly two blocks. Now form a (kv + 1) × (v (v + 1) + k ) matrix A, which is the symbol-block incidence matrix, as follows. Rows are indexed by symbols, columns by blocks. The matrix contains the entry 1 in row r and column c when symbol r appears in block c and 0 otherwise. Now examine B = AAT , which has rows and columns indexed by V . Its diagonal entries are k in entry (, ) and v + 2 elsewhere. Its off-diagonal entries are 2 in cells indexed by ((i, j ), (i , j )) with i = i and 1 otherwise. The rank of B cannot exceed the number of columns in A, namely, v (v + 1) + k . So in order to bound k , we bound the rank of B.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1849

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Write D = B - J. The rows indexed by V \ {} can be partitioned into v parts; a part is formed by including all rows and columns with indices {(i, j ) : 1  i  k } for some j with 0  j < v . Then D can be written as a block diagonal matrix with v k × k block matrices each equal to X = v I + J and a single 1 × 1 block matrix with entry k - 1. Now det(D) = (k - 1)(det(X))v , and det(X) = v k (1 + k v ) by the matrix determinant lemma. Hence rank(D) = kv + 1. Because B is obtained from D by a rank one update, rank(B)  kv . v +1) Consequently, kv  v (v + 1) + k , or k  v( v -1 . Thus k  v + 2 when v  4, because k is an integer. This enables us to establish a substantial improvement on the bound of Mathon and Tran Van Trung [28] stated in Theorem 1.5. Theorem 2.3. If t  3 and SeqCAN(t, v ) = t! (or equivalently, a DD(t, v, v ) exists), then v  2t - 1. Proof. If 3  t  6, this follows from Theorem 1.5. By Theorem 2.1, t! = SeqCAN(t, v )  (t - 2)!CANX (2, v - t + 2, t - 1) and thus CANX (2, v - t + 2, t - 1) = t(t - 1). By Lemma 2.2, v - t - 2  t - 1 + 2 and hence v  2t - 1 as required. It appears plausible that the bound should be t +2 rather than 2t - 1; nevertheless, the method here gives the first bound that is linear in t. 3. Upper bounds from probabilistic methods. Spencer [32] analyzed a method that selects a set of N permutations on v symbols uniformly at random; we explore this first. v! t! Lemma 3.1. For fixed t  3, SeqCAN(t, v )  1 + (log( (v- t)! ))/(log( t!-1 )). Proof. A permutation of {0, . . . , v - 1} chosen uniformly at random covers any 1 specific t-subsequence with probability t ! and so fails to cover it with probability t!-1 . Then N permutations of { 0 , . . . , v - 1} chosen uniformly at random and int!
-1 dependently together fail to cover a specific t-subsequence with probability t!t . ! v! There are (v-t)! t-subsequences. When N permutations are chosen, each subsequence N t!-1 N . t!

is not covered with probability
v! (v -t)!

Thus the expected number of uncovered 1, a SeqCA(N ; t, v ) must

t-subsequences is

t!-1 N . t!

exist. This holds whenever N >

t!-1 N < t! v! t! (log( (v-t)! ))/(log( t!-1 )).

When

v! (v -t)!

3.1. One permutation at a time. Lemma 3.1 provides a useful upper bound on the size of completely t-scrambling sets of permutations but does not provide an effective method to find such arrays. Stein [34], Lov´ asz [26], and Johnson [22] develop a general strategy for finding solutions to covering problems; this algorithm has been shown to lead to polynomial time methods in many combinatorial covering problems [3, 4, 7, 9, 10]. We extend that strategy here to treat sequence covering arrays. The basic approach is greedy. Repeatedly select one permutation to add that covers a large number of as-yet-uncovered t-subsequences, until all are covered. Stein [34], Lov´ asz [26], and Johnson [22] each suggest selecting to maximize the number of newly covered elements, but their analyses require only that the next selection cover at least the average. If after i permutations are selected there remain Ui uncovered t-subsequences, then a permutation selected uniformly at random is expected to cover 1 Ui t ! t-subsequences for the first time. Provided that we select the (i +1)st permutation 1 t!-1 to cover at least Ui t ! t-subsequences for the first time, we have that Ui+1  Ui t! . v! v! t!-1 i Because U0 = (v-t)! , we have that Ui  (v-t)! ( t! ) . Choose N to be the smallest

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1850

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

value for which UN < 1; then there must be a sequence covering array with N permutations. This simply restates the argument of Lemma 3.1, but with two important improvements. It derandomizes the method by ensuring that appropriate selection of each permutation guarantees that the bound is met, rather than asserting the existence of a set of permutations that meets it. More importantly, the time to construct the sequence covering array is polynomial in the number of permutations and the time to select a permutation that covers at least the average. For fixed t the number of permutations is logarithmic in v , and so an algorithm with running time polynomial in v will result if we can select the next permutation in time polynomial in v . Because the details are quite similar to earlier approaches, we merely outline how this can be done. Suppose that U consists of the as-yet-uncovered t-subsequences. For U  U , let cov( , U ) = 1 when  covers U , and cov( , U ) = 0 otherwise. Let R be an rsubsequence; the r symbols in R are fixed, and the remaining v - r are free. There ! is a set PR of v r ! permutations that cover R. We focus on the expected number of members of U that are covered by a member of PR chosen uniformly at random; this is ec(R) = r! v! cov( , U ).
 PR U U

The strategy is to find a sequence of subsequences P0 , . . . , Pv , so that Pi is an i-subsequence, Pi+1 covers Pi for 0  i < v , symbol i is free in Pi but fixed in Pi+1 , 1 and ec(Pi+1 )  ec(Pi ) for 0  i < v . Because ec(P0 ) = t ! |U|, it follows that Pv is a 1 permutation that covers at least t! |U| of the as-yet-uncovered t-subsequences. Given a selection of Pi , there are precisely i + 1 candidates {C1 , . . . , Ci+1 } for Pi+1 obtained by placing symbol i in one of the i + 1 positions of Pi . Our task is to choose one for which ec(Cj )  ec(Pi ), in order to set Pi+1 = Cj . A naive computation of ec(Cj ) would enumerate members of U and of PCj , but the latter may have size exponential in v . Instead, for U  U let ec(U, R) = r!  PR cov( , U ), and observe that v! ec(R) =
U U

ec(U, R).

When t is fixed, U contains fewer than v t subsequences, which is polynomial in v . Therefore it suffices to compute ec(U, R) efficiently, given a t-subsequence U and an r-subsequence R. Let  be the number of symbols appearing in both U and R. When the  symbols in common do not appear in the same order in U and R, ec(U, R) = 0. Otherwise let T be the  -subsequence that they have in common. Then ! ec(U, R) = ec(U, T ) =  t! . i+1 1 The key observation in selecting Pi+1 is that ec(Pi ) = i+1 j =1 ec(Cj ). Computing ec(Cj ) for 1  j  i + 1, and selecting Pi+1 to be the one that maximizes ec(Cj ), we are then sure that ec(Pi+1 )  ec(Pi ). Combining all of these arguments, we have established the next theorem. Theorem 3.2. For fixed t and input v , there is an algorithm to construct a v! t! SeqCA(N ; t, v ) having N  1 + (log( (v- t)! ))/(log( t!-1 )) permutations in time that is polynomial in v . This algorithm can be easily implemented, and we report results from it in section 5. One immediate improvement results from observing that the counts of

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1851

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

as-yet-uncovered t-subsequences (R0 , R1 , . . . , RN ) must be integers. Hence we have -1 that Ri+1  Ri t!t ! . In specific cases this improves on the bound, without the need to construct the sequence covering array. If, however, the sequence covering array is explicitly constructed, at each selection of Pi+1 from Pi , we can choose Pi+1 to maximize ec() among the i + 1 candidates. 3.2. Greedy methods with reversals. The methods developed are greedy in that they attempt to cover the largest number of as-yet-uncovered t-subsequences. The very first permutation chosen is arbitrary; all are equally effective at coverage. Once one is selected, however, there is a genuine choice for the second. Greedy selection indicates that we should choose one that covers no t-subsequence already covered by the first. Indeed when t = 2, choosing the reversal of this first covers all remaining t-subsequences. For t  3, suppose that we have chosen 2s permutations 1 , . . . , 2s , and suppose further that 2i is the reverse of 2i-1 for 1  i  s. It follows that the number of as-yet-uncovered t-subsequences covered by a permutation  is precisely the same as the number covered by the reverse of  . Yet  and its reverse never cover the same t-subsequence. Hence if the algorithm were to select  next, the reverse of  remains an equally beneficial choice immediately thereafter. Therefore a useful variant of the algorithm developed, after adding a permutation  to the array, always adds the reverse of  as well. Pursuing this, we obtain the following. Theorem 3.3. For fixed t and input v , there is an algorithm to construct a v! t! SeqCA(N ; t, v ) having N  2(log( (v- t)! ))/(log( t!-2 )) permutations in time that is polynomial in v . Naturally, we could again obtain small improvements in practice because every count of as-yet-uncovered t-subsequences is an integer. In principle, always including reversals improves slightly on the bound (that is, Theorem 3.3 improves on Theorem 3.2). Whether this is a practical improvement remains to be seen; we return to this point. 4. Product constructions. Product (or "cut-and-paste" or "Roux-type") constructions are well studied for covering arrays; see, for example, [11, 6]. We develop a product construction for completely 3-scrambling sets of permutations. To do this, we first introduce an auxiliary property. A signing of a CSSP(N ; t, v ) A = (aij ) is an N × v matrix S = (sij ) with entries {, }. A CSSP(N ; t, v ) A is properly signed by an N × v matrix S with entries {, }. When for every set of t - 1 distinct columns c1 , . . . , ct-1 , each sign s  {, }, and every permutation  of 1, . . . , t - 1, there exists a row  of A for which, for every 1  a < b < t, the entry in cell (, c(a) ) is less than the entry in row (, c(b) ), and the sign s,c1 = s. A properly signed CSSP(7;3,5) is shown in Table 4.1. We defer for the moment the question of how to sign a completely t-scrambling set of permutations. 4.1. Products for strength three. Theorem 4.1. If a properly signed CSSP(N ; 3, v ) and a properly signed CSSP(M ; 3, w) both exist, so does a properly signed CSSP(N + M ; 3, vw). Proof. Let A = (aij ) be a CSSP(N ; 3, v ) having sign matrix S = (sij ) with columns indexed by {0, . . . , v - 1}. Let B = (bij ) be a CSSP(M ; 3, w) having sign matrix T = (tij ) with columns indexed by {0, . . . , w - 1}. Form an array C = (c,(i,j ) ) on N + M rows and vw columns with columns indexed by {0, . . . , v - 1} × {0, . . . , w - 1}. In row  for 1    N , in column (i, j ), place the entry ai w + j if si =, ai w + (w - 1 - j )

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1852

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Table 4.1 Properly signed CSSP(7; 3, 5) ­ t = 3, v = 5, N = 7. All signs not specified can be selected arbitrarily.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

0 1 2 2 2 4 4

4 2 0 1 3 1 3

2 3 4 0 0 2 2 (7;3,5)

3 0 3 3 4 3 0

1 4 1 4 1 0 1

if si =. In row N +  for 1    M , in column (i, j ), place the entry bj v + i if tj =, bj v + (v - 1 - i) if tj =. To show that C is a CSSP(N + M ; 3, vw), we must establish that every 3subsequence is covered. Consider three columns (i1 , j1 ), (i2 , j2 ), (i3 , j3 ), in this order. If i1 , i2 , and i3 are all distinct, there is a row  of A in which ai1 < ai2 < ai3 . Then in C row  has the three specified columns in the chosen order. By the same token, if j1 , j2 , and j3 are all distinct, there is a row  of B in which bj1 < bj2 < bj3 . Then in C row  + N has the three specified columns in the chosen order. If {i1 , i2 , i3 } contains only one element, {j1 , j2 , j3 } contains three distinct elements; symmetrically, if {j1 , j2 , j3 } contains only one element, {i1 , i2 , i3 } contains three distinct elements. So it remains only to treat cases in which both {i1 , i2 , i3 } and {j1 , j2 , j3 } contain two distinct elements. Now suppose that the three columns are {(i1 , j1 ), (i1 , j2 ), (i2 , j1 )}; we are concerned with the six orderings of the elements {c(i1 ,j1 ) , c(i1 ,j2 ) , c(i2 ,j1 ) }, which we represent by giving the indices in the sorted order for {c(i1 ,j1 ) , c(i1 ,j2 ) , c(i2 ,j1 ) }, as shown next. j2 j1 i1 c(i1 ,j1 ) c(i1 ,j2 ) 1 2 1 3 2 1 2 3 3 1 3 2 i2 c(i2 ,j1 ) 3 2 3 1 2 1 Table 4.2 gives the rows in which each of the six orderings is covered; we use sgn(x - y ) to be  if x < y ,  if x > y . Two of the orderings are covered at least twice. To sign C properly, assign  to each entry in each of the first N rows and  to each entry in each of the last M rows. A small example, combining two CSSP(6;3,4)s to form a CSSP(12;3,16), is shown in Table 4.3. Use the strategy in the proof of Theorem 4.1, taking B and T from   0 1  0 1     1  0  , 1 0

to establish the next theorem. Theorem 4.2. If a properly signed CSSP(N ; 3, v ) exists, so does a properly signed CSSP(N + 4; 3, 2v ).

4.2. Signing a completely t -scrambling set of permutations. We first give one technique for signing that applies for all strengths.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS Table 4.2 Verification for six orderings.

1853

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Row in C     +N +N +N +N

Condition on  ai1 < ai2 ai1 < ai2 ai1 > ai2 ai1 > ai2 bj1 < bj2 bj1 < bj2 bj1 > bj2 bj1 > bj2

Sign condition si1 = sgn(j2 - j1 ) si1 = sgn(j1 - j2 ) si1 = sgn(j2 - j1 ) si1 = sgn(j1 - j2 ) tj1 = sgn(i2 - i1 ) tj1 = sgn(i1 - i2 ) tj1 = sgn(i2 - i1 ) tj1 = sgn(i1 - i2 )

Ordering 1 3 2 3 2 1 3 1 1 2 2 1 2 3 3 2 2 1 3 2 3 3 1 1

Table 4.3 Example: Properly signed CSSP(6; 3, 4) and its product with itself, a CSSP(12; 3, 16). 3 4 4 12 4 15 3 4 4 12 4 15 2 5 5 13 5 14 4 3 15 4 12 4 1 6 6 14 6 13 8 15 3 0 8 8 0 7 7 15 7 12 15 8 8 8 0 3 4 3 15 4 12 4 2 5 5 13 5 14 5 2 14 5 13 5 5 2 14 5 13 5 6 1 13 6 14 6 9 14 2 1 9 9 7 0 12 7 15 7 14 9 9 9 1 2 8 15 3 0 8 8 1 6 6 14 6 13 9 14 2 1 9 9 6 1 13 6 14 6 10 13 1 2 10 10 10 13 1 2 10 10 11 12 0 3 11 11 13 10 10 10 2 1 15 8 8 8 0 3 0 7 7 15 7 12 14 9 9 9 1 2 7 0 12 7 15 7 13 10 10 10 2 1 11 12 0 3 11 11 12 11 11 11 3 0 12 11 11 11 3 0

0 1 1 3 1 3

1 0 3 1 3 1

2 3 0 0 2 2

3 2 2 2 0 0

Lemma 4.3. Whenever a CSSP(N ; t, v ) exists, a properly signed CSSP(N ; t, v - 1) exists. Proof. Let A = (aij ) be a CSSP(N ; t, v ). Form an N × (v - 1) array S = (sij ) with sij = sgn(aij - ai,v-1 ). Form an N × (v - 1) array B = (bij ) with bij = aij if aij < ai,v-1 and bij = aij - 1 otherwise. Then B is a CSSP(N ; t, v - 1) that is properly signed by S. Let A be a CSSP(N ; t, v ) and A1 , A2 be arrays that partition the rows of A. When, for i = 1, 2, Ai is a CSSP(Ni ; t - 1, v ), A is a partitionable CSSP. Lemma 4.4. Whenever a partitionable CSSP(N ; t, v ) exists, a properly signed CSSP(N ; t, v ) exists. Proof. Let A be a CSSP(N ; t, v ) with partition A1 , A2 . Assign sign  to every entry of A1 and  to every entry of A2 . Corollary 4.5. Whenever a CSSP(N ; 3, v ) contains a row and its reverse, it is partitionable and hence can be properly signed. Proof. Place the row and its reverse in A1 and all other rows in A2 . Then A1 is a CSSP(N ; 2, v ). Moreover, A2 is a CSSP(N ; 2, v ) because for every i, j  {0, . . . , v - 1} with i = j , in A there are at least three rows in which the entry in column i is less than that in column j . Then A is partitionable, so apply Lemma 4.4.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1854

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Lemma 4.4 provides a sufficient condition for a CSSP(N ; 3, v ) to have a proper signing but considers only signings in which all entries in each row receive the same sign. Column c is properly signed when, for every set of t - 1 distinct columns c1 , c2 , . . . , ct-1 with c1 = c, each sign s  {+, -}, and every permutation  of 1, . . . , t - 1, there exists a row  of A for which, for every 1  a < b < t, the entry in cell (, c(a) ) is less than the entry in row (, c(b) ), and the sign s,c = s. Properly signing the N × v array A is equivalent to properly signing each column of A; the important fact is that signs assigned in one column are unrelated to signs in any other, and so one can (hope to) proceed by signing each column separately. Consider the case of strength t = 3. What does it mean to properly sign a specific column c? For every column i other than c we form two sets: Ai contains the row indices in which the entry in column i is larger than that in column c, and Bi (= {1, . . . , N }\ Ai ) contains the row indices in which the entry in column i is smaller than that in column c. We can consider these sets as the edges of a hypergraph H on vertex set {1, . . . , N }. Then H has 2v - 2 edges each containing at least three vertices and a proper 2-coloring of H corresponds to a proper signing of c. Lemma 4.4 and Corollary 4.5 give proper 2-colorings. In all examples that we have examined, each column can be properly signed by finding a suitable 2-coloring. Hence it is plausible that every CSSP(N ; 3, v ) can be properly signed, but if this is true the proof is elusive at the moment. 5. Computational results. In [23], a simple greedy method is used to compute upper bounds on SeqCAN(t, v ) for t  {3, 4} and small values of v . These are reported in column K in Tables 5.1 and 5.2. Results from a more sophisticated greedy method by Erdem et al. [16] are reported in column ER. Using techniques from constraint satisfaction, in particular answer set programming, much more sophisticated search methods have been applied to strengths three and four [1, 2, 16]. Banbara, Tamura, and Inoue [1] implement an answer set programming method and show bounds for SeqCAN(3, v ) for v  80 and for SeqCAN(4, v ) for v  23. These bounds appear in column BTI in Tables 5.1 and 5.2. Results by Brain et al. [2] are reported in column BR in Tables 5.1 and 5.2. In [18], bounds for t  {3, 4, 5} and v  10 are reported from a method called the "bees algorithm"; these offer modest improvements on the greedy method in [23]. We do not report them for t  {3, 4} because they are not competitive with the results in [1]; we do report them for t = 5 in column BA, because they are the only published computational results. When t = 3, Tarui [36] establishes q/2 )  q for all q  4; these are reported in by direct construction that SeqCAN(3,  q/4 column TA. The bound U is obtained by computing the number Ui of as-yet-uncovered t-1 subsequences using Ui+1 =  t!t ! Ui  and terminating with the first value N for which UN = 0. (This does not explicitly construct the array but rather yields a number of rows for which it can surely be produced.) In the same manner, bound UR is obtained 1 by including reversals, so that Ui+2 = Ui - 2 t ! Ui  when i is even. The bound D is obtained by applying the algorithm that establishes Theorem 3.2 and that of DR by applying the algorithm that establishes Theorem 3.3. Table 5.1 reports results for t = 3. The theoretical results indicate that including reversals accelerates coverage, and so the bound UR improves on the bound U. Neither is competitive with greedy bounds from [23], given by K. In turn these are improved upon by the greedy method from [16], given by ER. Implementing our greedy approach nevertheless results in useful improvement to the two earlier greedy bounds, whether reversals are included or not. It comes as no surprise that the answer set programming

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS Table 5.1 Upper bounds on SeqCAN(3, v).

1855

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Events v 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 40 50 60 70 80 90

TA 8 8 8 10 10 10 10 12 12 12 12 12 12 12 12 12 12 14 14 14 14 14 14 14 14 14 14 16 16 16 16 18 18

U 12 17 20 23 26 28 30 32 33 35 36 37 39 40 41 42 42 43 44 45 46 46 47 48 48 49 49 54 58 61 64 66 68

UR 12 16 18 22 24 26 28 30 30 32 34 34 36 36 38 38 38 40 40 40 42 42 42 44 44 44 46 50 52 56 58 60 62

t=3 K ER 8 10 12 12 14 14 11 14 16 16 16 18 18 20 20 22 22 19 22 22 24 24 24 24 26 26 26 26 23 32 27 34 31 38 34 40 36 42 38 44

DR 8 10 10 12 12 12 14 14 14 16 16 16 16 18 18 18 18 18 20 20 20 20 20 20 20 22 22 24 26 26 28 30 30

D 6 8 8 9 10 11 12 12 13 13 14 14 15 15 16 16 16 17 17 17 17 18 18 18 18 19 19 21 23 24 25 26 27

BTI 7 8 8 8 9 9 10 10 10 10 10 11 11 12 12 12 12 13 14 14 14 14 14 14 15 15 17 19 21 22 24

BR 7 8 8 8 9 9 10 10 10 10 10 10 11 12 12 12 12 12 13 13 14 14 14 14 14 15 17 18 20 22 23

methods from [1, 2] (BTI, BR) yield consistent improvements on all the greedy methods for strength three. However, the direct construction of Tarui [36] provides better results at this time whenever v  30. Perhaps the most perplexing pattern is the regularity with which D yields a better bound than does DR . Remarkably, we consistently produce smaller sequence covering arrays when we do not automatically include reversals! The reasons for this are quite unclear at the present time. Table 5.2 gives results for strengths four and five. For strength four, our improvements on the method from [23] are more dramatic than for strength three. Surprisingly, the answer set programming technique from [1] obtains a better result than our greedy methods only when v  8. For 9  v  23, our greedy method yields much smaller arrays. (For v = 23, we employ 98 permutations as opposed to the 112 in BTI.) A similar comparison applies with the results from [2, 16] reported in column BR. Of course, we expect that given enough time, the answer set programming techniques would improve upon our greedy bounds. However, our methods require polynomial time in theory and are effective in practice for larger problems than those considered in [2, 1]; despite these "limitations," our methods appear to yield better results within the time available.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1856

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU Table 5.2 Upper bounds on SeqCAN(t, v) for t  {4, 5}.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Events v 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 40 50 60 70 80 90

U 24 54 79 98 114 128 140 151 160 169 177 184 191 197 203 209 214 219 224 228 232 236 240 244 248 251 255 283 305 322 337 350 361

UR 24 54 78 96 112 126 138 148 158 166 174 180 188 194 200 204 210 214 220 224 228 232 236 240 242 246 250 278 298 316 330 342 354

K 24 29 38 50 56 68 72 78 86 92 100 108 112 118 122 128 134 134 140 146 146 152 158 160 162 166 166 198 214 238 250 264 -

t=4 DR 24 24 32 40 44 50 56 60 64 70 74 78 80 84 86 90 92 96 98 98 102 104 106 108 110 112 114 132 146 154 166 174 180

D 24 26 34 41 47 52 57 61 66 71 73 78 81 84 86 91 92 95 97 99 101 104 105 107 110 111 113 128 141 151 160 168 176

BTI 24 24 38 44 52 58 65 69 77 81 84 89 91 97 100 105 104 111 112

BR

U 120 294 437 552 648 731 803 868 926 978 1027 1072 1113 1152 1189 1223 1256 1286 1316 1344 1370 1396 1420 1444 1466 1488 1671 1811 1924 2019 2101 2173

UR 120 294 436 550 646 728 800 864 922 976 1024 1068 1110 1148 1184 1218 1252 1282 1310 1338 1366 1390 1416 1438 1460 1482 1664 1804 1916 2012 2092 2164

t=5 DR 120 148 198 242 282 318 354 384 416 446 470 496 518 540 560 582 600 622 636 654 674 688 706 718 734 748

D 120 149 200 243 284 322 356 386 419 448 475 501 521 547 570 590 610 629 646 665 682 698 715 732 746 760

BA

55

159 212 271 329 383

104

149 181

A somewhat different pattern with respect to reversals is evident for strength four: The theoretical bound profits by including reversals throughout, but the implemented construction method appears first to benefit from reversals (for v  20) but later no longer benefit (for 40  v  90). Again the reasons for this are unclear. When v = 90, our methods track the coverage of 61,324,560 4-subsequences; thus, while the methods scale polynomially with v , the computations are nonetheless quite extensive. There is a CSSP(24;4,6) [28], but our methods do not yield fewer than 32 permutations. For strength five, none of the published methods in [1, 16, 23] report computational results, so it is difficult to make any comments about relative accuracy. However, the answer set programming methods do appear to require substantially more storage, which limits to a degree their effective range. To apply our methods would require tracking the coverage of 78,960,960 5-subsequences for v = 40; despite the efficiency of our methods, a straightforward implementation encounters both storage and time limitations. The method BA [18] is not competitive with our greedy methods. Within the range computed, including reversals improves our results. The pattern thereafter is unknown. Again, there is a CSSP(120;5,6) [25], but our methods do not yield fewer than 148 permutations.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1857

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

6. Using the product construction. For strength three, Theorem 4.1 provides substantial improvements on the computational results from the greedy methods. We properly signed a CSSP(6;3,4) in Table 4.3 and a CSSP(7;3,5) in Table 4.1. Table 6.1 shows proper signings for further arrays from [1]. We obtain CSSP(N ; 3, v ) for (v, N )  {(40, 15), (80, 17), (128, 18), (160, 19), (256, 20), (288, 21)} by using these in Theorem 4.1. These improve upon all the computational results! For example, while in [1] it is shown that SeqCAN(3, 80)  24 and in [2] that SeqCAN(3, 80)  23, here it is shown that SeqCAN(3, 80)  17. The examples given also provide better bounds than those of Tarui [36], but Theorem 4.1 does not outperform the direct construction asymptotically. 7. Constraints. In the testing application, it may happen that not every permutation of the events can in fact be executed; see [2, 23, 24]. It is therefore reasonable to ask how constraints on the execution order affect the number of permutations needed and how they affect the difficulty of finding a sequence covering array. We briefly consider the latter, in order to examine connections with further problems. Let  = {0, . . . , v - 1}. Let C be a set of subpermutations of , called constraints. A constrained sequence covering array SeqCA(N ; t, v, C ) is a set  = {1 , . . . , N } where i is a permutation of  that does not cover any subpermutation in C , and every t-subsequence of  that does not cover any subpermutation in C is covered by at least one of the permutations {1 , . . . , N }. Even in the easiest case, when t = 2 and all constraints are 2-subpermutations, the nature of the problem changes dramatically. Imposing the subpermutation constraint that b cannot precede a is the same as enforcing the precedence constraint that a precede b. When the precedence constraints contain a cycle, it is impossible to meet all constraints. This can be easily checked. When the constraints are acyclic, there is a permutation that covers no constraint. However, covering all 2-subpermutations not in C requires more. Let C r be the set of 2-subpermutations obtained by reversing each 2-subpermutation in C . Suppose that a SeqCA(N ; 2, v, C ) exists. Every permutation in the sequence covering array covers every 2-subpermutation in C r . Equivalently, treating C r as a partial order, every permutation gives a linear extension of the partial order. When (a, b)  C , (b, a) must be covered by every permutation in the sequence covering array. When {(a, b), (b, a)}  C = , some but not all permutations in the sequence covering array cover (a, b)--and the rest cover (b, a). Hence the set of 2-subpermutations covered by every permutation in the sequence covering array is exactly C r . This establishes a connection with the theory of partial orders. The dimension of a partial order is the smallest number of linear extensions whose intersection is the partial order [37, 38]. Our discussion establishes that a SeqCA(N ; 2, v, C ) exists if and only if the dimension of the partial order induced by C r is at most N . Hence we have the next lemma. Lemma 7.1. Deciding whether a SeqCA(N ; 2, v, C ) exists is NP-complete, even when C is an acyclic set of 2-subpermutations. Proof. Yannakakis [41] shows that determining whether a partial order has dimension at most 3 is NP-complete. Brain et al. [2] establish the NP-completeness of a related problem in which the subsequences to be covered, the constraints, and the permutations allowed are all specified. Lemma 7.1 is in stark contrast with the existence of sequence covering arrays of strength two without constraints. Nevertheless, the complexity arises in determining whether a small sequence covering array exists in these cases, not in

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1858

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU Table 6.1 Small properly signed CSSP(N ; 3, v)s. Signs not shown can be chosen arbitrarily.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

8 5 6 2 3 7 1 4 1 2 8 4 5 7 6 3 6 5 7 8 3 2 4 1 5 7 4 3 8 1 6 2 3 7 4 2 1 5 8 6 6 2 1 3 8 7 4 5 2 7 3 6 4 5 1 8 5 1 3 8 4 2 6 7 (8;3,8)

10 4 5 6 8 1 3 2 9 7 2 10 6 5 4 8 7 1 9 3 8 6 1 5 10 3 9 7 4 2 5 3 9 10 1 8 4 6 7 2 2 5 7 4 8 9 3 10 1 6 6 8 1 10 7 5 2 4 3 9 9 10 8 3 5 2 4 7 1 6 5 1 7 3 2 4 9 8 6 10 3 5 1 2 9 7 10 4 6 8 (9;3,10)

2 14 15 5 8 3 1 11 9 7 13 12 16 6 4 10 14 8 3 6 7 11 1 16 12 13 2 10 15 5 9 4 12 16 4 6 3 5 7 2 1 9 15 11 8 14 10 13 12 2 13 4 3 14 9 6 11 1 5 15 10 7 16 8 16 4 10 5 12 9 11 14 1 8 13 2 7 15 6 3 6 4 12 15 3 1 11 13 8 14 10 5 2 9 16 7 2 15 12 11 6 8 13 5 14 7 3 4 9 16 10 1 5 9 7 14 16 13 6 10 12 1 11 2 8 3 4 15 5 6 1 15 12 16 10 2 3 13 4 11 9 8 7 14 11 7 8 4 15 5 16 6 14 12 9 13 1 2 3 10 (10;3,16) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 4 14 11 10 6 9 13 7 17 1 18 3 2 12 16 5 8 15 4 17 3 2 18 16 14 7 9 13 11 12 10 8 1 6 15 5 6 13 7 8 12 10 14 4 16 1 2 18 17 15 9 11 3 5 7 18 6 16 4 5 1 17 13 11 10 12 14 15 8 2 9 3 8 3 15 4 9 17 2 16 11 12 14 13 10 7 6 1 5 18 8 11 3 14 6 15 13 5 2 16 17 1 18 7 10 9 4 12 10 7 16 4 5 9 13 15 1 18 3 17 2 12 14 8 6 11 13 9 16 14 15 11 7 12 5 4 2 1 3 10 6 18 17 8 18 4 15 17 16 2 8 1 11 13 9 12 10 3 6 14 7 5 18 7 2 5 4 3 17 10 15 11 14 12 13 1 16 9 8 6 (11;3,18) 19 3 5 2 6 18 9 10 14 21 20 1 15 4 12 22 16 7 13 8 11 23 17 2 22 13 21 7 4 1 6 18 23 15 12 8 10 19 16 17 11 20 3 5 14 9 1 21 6 7 4 20 16 18 10 23 3 13 17 9 11 14 2 19 8 22 12 15 5 10 12 5 15 8 13 23 17 22 11 18 14 9 3 4 1 16 2 20 21 19 6 7 3 17 22 4 14 2 7 13 1 10 9 16 11 12 23 8 20 19 18 15 21 5 6 10 1 19 15 3 14 23 4 12 11 8 18 2 21 16 6 17 20 7 5 9 13 22 20 13 3 17 18 7 14 10 22 9 1 16 11 21 15 8 4 6 2 5 23 19 12 8 18 12 3 22 11 14 1 15 6 21 5 10 19 9 13 4 2 20 17 16 7 23 21 2 12 15 22 23 3 19 8 5 16 17 1 20 6 18 13 7 9 11 14 4 10 18 10 9 14 7 17 8 6 5 15 16 13 23 11 4 12 20 21 1 22 3 2 19 16 10 14 9 21 12 7 23 13 2 4 19 20 1 22 3 8 17 18 6 5 15 11 15 16 23 21 12 3 19 17 4 9 13 1 18 14 5 22 7 11 10 8 6 20 2 (12;3,23)

determining whether a sequence covering array exists. The situation is worse when constraints have strength three. Consider a collection T of ordered triples of distinct elements of , and associate with (a, b, c) the constraints {(b, a, c), (b, c, a), (a, c, b), (c, a, b)}. Meeting these constraints requires that b lie between a and c, and a collection of constraints of this type forms an instance of the betweenness problem [5] in which one is required to

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1859

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

order all items so that for every triple (a, b, c)  T , b lies between a and c. Forming C = {(b, a, c), (b, c, a), (a, c, b), (c, a, b) : (a, b, c)  T }, even finding a single permutation that covers no 3-subpermutation in C appears hard: Lemma 7.2 (see [30]). Determining whether an instance of the betweenness problem has a solution is NP-complete. These complexity results suggest that constraints pose severe additional challenges in the construction of sequence covering arrays. Checking feasibility can become difficult; even when feasibility is easily checked, the minimization problem is substantially more complicated. 8. Conclusions. The close connection between sequence covering arrays and covering arrays has proved useful in establishing bounds on the sizes of sequence covering arrays. The efficient conditional expectation algorithm for generating sequence covering arrays and the product construction for strength three parallel analogous results for covering arrays. Unfortunately, while sequence covering arrays lead to covering arrays with excess coverage, additional conditions on such a covering array would be required in order to recover a sequence covering array. Hence the parallels between the extensive literature on covering arrays and the existence problem for sequence covering arrays are primarily by analogy. We have examined numerous formulations for sequence covering arrays. In closing, we indicate one more (see also [27]). A perfect hash family PHF(N ; k, w, t) is an N × k array on w symbols in which in every N × t subarray, at least one row consists of distinct symbols. Mehlhorn [29] introduced perfect hash families as an efficient tool for compact storage and fast retrieval of frequently used information; see also [14]. Stinson et al. [35] establish that perfect hash families can be used to construct separating systems, key distribution patterns, group testing algorithms, cover-free families, and secure frameproof codes. They are also used extensively in product constructions for covering arrays [8, 12, 13]. Completely t-scrambling sets of permutations can be viewed as an ordered analogue of perfect hash families in which k = w, no element appears twice in a row, and for every way to select t distinct columns in order there is a row in which the elements in these columns are in increasing order. In particular, a completely t-scrambling set of permutations provides a perfect hash family in which, for every set of t columns, there are at least t! rows containing distinct symbols in the chosen columns, and at least one for each of the t! symbol orderings. For this reason, it appears reasonable to expect that constructions for perfect hash families may also prove to be useful for sequence covering arrays. Acknowledgments. Thanks to two anonymous referees for pointing out relevant references. Special thanks to Mutsunori Banbara and Johannes Oetsch for providing explicit solutions for small sequence covering arrays with t = 3.
REFERENCES [1] M. Banbara, N. Tamura, and K. Inoue, Generating event-sequence test cases by answer set programming with the incidence matrix, in Technical Communications of the 28th International Conference on Logic Programming (ICLP12), 2012, pp. 86­97. ¨ hrer, H. Tompits, and C. Yilmaz, Event[2] M. Brain, E. Erdem, K. Inoue, J. Oetsch, J. Pu sequence testing using answer-set programming, Internat. J. Advances Software, 5 (2012), pp. 237­251. [3] R. C. Bryce and C. J. Colbourn, The density algorithm for pairwise interaction testing, Software Testing, Verification, and Reliability, 17 (2007), pp. 159­182.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1860

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

[4] R. C. Bryce and C. J. Colbourn, A density-based greedy algorithm for higher strength covering arrays, Software Testing Verification Reliability, 19 (2009), pp. 37­53. [5] B. Chor and M. Sudan, A geometric approach to betweenness, SIAM J. Discrete Math., 11 (1998), pp. 511­523. [6] M. B. Cohen, C. J. Colbourn, and A. C. H. Ling, Constructing strength three covering arrays with augmented annealing, Discrete Math., 308 (2008), pp. 2709­2722. [7] C. J. Colbourn, Constructing perfect hash families using a greedy algorithm, in Coding and Cryptology, Y. Li, S. Zhang, S. Ling, H. Wang, C. Xing, and H. Niederreiter, eds., World Scientific, Singapore, 2008, pp. 109­118. [8] C. J. Colbourn, Covering arrays and hash families, in Information Security and Related Combinatorics, NATO Peace and Information Security, IOS Press, Amsterdam, 2011, pp. 99­136. [9] C. J. Colbourn, Efficient conditional expectation algorithms for constructing hash families, in Combinatorial Algorithms, Lecture Notes in Comput. Sci., 7056, Springer-Verlag, Berlin, 2011, pp. 144­155. [10] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk, Strengthening hash families and compressive sensing, J. Discrete Algorithms, 16 (2012), pp. 170­186. [11] C. J. Colbourn, S. S. Martirosyan, Tran Van Trung, and R. A. Walker II, Roux-type constructions for covering arrays of strengths three and four, Des. Codes Cryptogr., 41 (2006), pp. 33­57. [12] C. J. Colbourn and J. Torres-Jim´ enez, Heterogeneous hash families and covering arrays, Contemp. Math., 523 (2010), pp. 3­15. [13] C. J. Colbourn and J. Zhou, Improving two recursive constructions for covering arrays, J. Statist. Theory Practice, 6 (2012), pp. 30­47. [14] Z. J. Czech, G. Havas, and B. S. Majewski, Perfect hashing, Theoret. Comput. Sci., 182 (1997), pp. 1­143. [15] B. Dushnik, Concerning a certain set of arrangements, Proc. Amer. Math. Soc., 1 (1950), pp. 788­796. ¨ hrer, H. Tompits, and C. Yilmaz, Answer-set pro[16] E. Erdem, K. Inoue, J. Oetsch, J. Pu gramming as a new approach to event-sequence testing, in Proceedings of the 2nd International Conference on Advances in System Testing and Validation Lifecycle, Xpert Publishing Services, 2011, pp. 25­34. ¨ redi, Scrambling permutations and entropy of hypergraphs, Random Structures [17] Z. Fu Algorithms, 8 (1996), pp. 97­104. [18] M. M. Z. Hazli, K. Z. Zamli, and R. R. Othman, Sequence-based interaction testing implementation using bees algorithm, in Proceedings of the IEEE Symposium on Computers and Informatics, 2012, pp. 81­85. [19] S. Huang, M. B. Cohen, and A. M. Memon, Repairing GUI test suites using a genetic algorithm, in Proceedings of the 3rd International Conference on Software Testing, Verification and Validation (ICST), 2010, pp. 245­254. [20] Y. Ishigami, Containment problems in high-dimensional spaces, Graphs Combin., 11 (1995), pp. 327­335. [21] Y. Ishigami, An extremal problem of d permutations containing every permutation of every t elements, Discrete Math., 159 (1996), pp. 279­283. [22] D. S. Johnson, Approximation algorithms for combinatorial problems, J. Comput. System Sci., 9 (1974), pp. 256­278. [23] D. R. Kuhn, J. M. Higdon, J. F. Lawrence, R. N. Kacker, and Y. Lei, Combinatorial methods for event sequence testing, in Proceedings of the IEEE 5th International Conference on Software Testing, Verification and Validation (ICST), 2012, pp. 601­609. [24] D. R. Kuhn, J. M. Higdon, J. F. Lawrence, R. N. Kacker, and Y. Lei, Combinatorial methods for event sequence testing, CrossTalk J. Defense Software Engineering, 25 (2012), pp. 15­18. [25] V. I. Levenshtein, Perfect codes in the metric of deletions and insertions, Diskret. Mat., 3 (1991), pp. 3­20. ´ sz, On the ratio of optimal integral and fractional covers, Discrete Math., 13 (1975), [26] L. Lova pp. 383­390. [27] O. Margalit, Better bounds for event sequence testing, in Proceedings of the 2nd International Workshop on Combinatorial Testing, 2013. [28] R. Mathon and Tran Van Trung, Directed t-packings and directed t-Steiner systems, Des. Codes Cryptogr., 18 (1999), pp. 187­198. [29] K. Mehlhorn, Data Structures and Algorithms 1: Sorting and Searching, Springer-Verlag, Berlin, 1984.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1861

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

´ , Total ordering problem, SIAM J. Comput., 8 (1979), pp. 111­114. [30] J. Opatrny [31] J. Radhakrishnan, A note on scrambling permutations, Random Structures Algorithms, 22 (2003), pp. 435­439. [32] J. Spencer, Minimal scrambling sets of simple orders, Acta Math. Acad. Sci. Hungar., 22 (1971/72), pp. 349­353. [33] R. P. Stanley, Increasing and decreasing subsequences and their variants, in Proceedings of the International Congress of Mathematicians, vol. I, Madrid, 2007, pp. 545­579. [34] S. K. Stein, Two combinatorial covering theorems, J. Combin. Theory Ser. A, 16 (1974), pp. 391­397. [35] D. R. Stinson, Tran Van Trung, and R. Wei, Secure frameproof codes, key distribution patterns, group testing algorithms and related structures, J. Statist. Plann. Inference, 86 (2000), pp. 595­617. [36] J. Tarui, On the minimum number of completely 3-scrambling permutations, Discrete Math., 308 (2008), pp. 1350­1354. [37] W. T. Trotter, Jr., Some combinatorial problems for permutations, in Proceedings of the 8th Southeastern Conference on Combinatorics, Graph Theory and Computing, Baton Rouge, La., 1977, Utilitas Mathematica, Winnipeg, pp. 619­632. [38] W. T. Trotter, Jr., Combinatorics and partially ordered sets, in Dimension Theory, Johns Hopkins Ser. Math. Sci., Johns Hopkins University Press, Baltimore, MD, 1992. [39] W. Wang, Y. Lei, S. Sampath, R. Kacker, D. Kuhn, and J. Lawrence, A combinatorial approach to building navigation graphs for dynamic web applications, in Proceedings of the 25th International Conference on Software Maintenance, 2009, pp. 211­220. [40] W. Wang, S. Sampath, Y. Lei, and R. Kacker, An interaction-based test sequence generation approach for testing web applications, in 11th IEEE High Assurance Systems Engineering Symposium, 2008, pp. 209­218. [41] M. Yannakakis, The complexity of the partial order dimension problem, SIAM J. Algebraic Discrete Methods, 3 (1982), pp. 351­358. [42] X. Yuan, M. B. Cohen, and A. M. Memon, Towards dynamic adaptive automated test generation for graphical user interfaces, in International Conference on Software Testing, Verification and Validation Workshops, 2009, pp. 263­266. [43] X. Yuan, M. B. Cohen, and A. M. Memon, GUI interaction testing: Incorporating event context, IEEE Trans. Software Engrg., 37 (2011), pp. 559­574. [44] X. Yuan and A. M. Memon, Generating event sequence-based test cases using GUI runtime state feedback, IEEE Trans. Software Engrg., 36 (2010), pp. 81­95.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Two-stage algorithms for covering array construction
Kaushik Sarkar and Charles J. Colbourn School of Computing, Informatics, and Decision Systems Engineering Arizona State University, PO Box 878809 Tempe, Arizona, 85287-8809, U.S.A.

arXiv:1606.06730v1 [cs.DS] 21 Jun 2016

June 23, 2016
Abstract Modern software systems often consist of many different components, each with a number of options. Although unit tests may reveal faulty options for individual components, functionally correct components may interact in unforeseen ways to cause a fault. Covering arrays are used to test for interactions among components systematically. A two-stage framework, providing a number of concrete algorithms, is developed for the efficient construction of covering arrays. In the first stage, a time and memory efficient randomized algorithm covers most of the interactions. In the second stage, a more sophisticated search covers the remainder in relatively few tests. In this way, the storage limitations of the sophisticated search algorithms are avoided; hence the range of the number of components for which the algorithm can be applied is extended, without increasing the number of tests. Many of the framework instantiations can be tuned to optimize a memory-quality trade-off, so that fewer tests can be achieved using more memory. The algorithms developed outperform the currently best known methods when the number of components ranges from 20 to 60, the number of options for each ranges from 3 to 6, and t-way interactions are covered for t  {5, 6}. In some cases a reduction in the number of tests by more than 50% is achieved.

Keywords: Covering array, Software interaction testing, Combinatorial construction algorithm

1

Introduction

Real world software and engineered systems are composed of many different components, each with a number of options, that are required to work together in a variety of circumstances. Components are factors, and options for a component form the levels of its factor. Although each level for an individual factor can be tested in isolation, faults in deployed software can arise from interactions among levels of different factors. When an interaction involves levels of t different factors, it is a t-way interaction. Testing for faults caused by t-way interactions for every t is generally infeasible, as a result of a combinatorial explosion. However, empirical research on real world software systems indicates that testing all possible 2-way (or 3-way) interactions would detect 70% (or 90%) of all faults [25]. Moreover, testing all possible 6-way interactions is sufficient for detection of 100% of all faults in the systems examined in [25]. Testing all possible t-way interactions for some 2  t  6 is pseudo-exhaustive testing [24], and is accomplished with a combinatorial array known as a covering array. Formally, let N, t, k, and v be integers with k  t  2 and v  2. A covering array CA(N ; t, k, v ) is an N × k array A in which each entry is from a v -ary alphabet , and for every N × t sub-array B of A and every x  t , there is a row of B that equals x. Then t is the strength of the covering array, k is the number of factors, and v is the number of levels. When k is a positive integer, [k ] denotes the set {1, . . . , k }. A t-way interaction is {(ci , ai ) : 1  i  t, ci  [k ], ci = cj for i = j, and ai  }. So an interaction is an assignment of levels from  to t of the k t factors. It,k,v denotes the set of all k t v interactions for given t, k and v . An N × k array A covers the 1

interaction  = {(ci , ai ) : 1  i  t, ci  [k ], ci = cj for i = j, and ai  } if there is a row r in A such that A(r, ci ) = ai for 1  i  t. When there is no such row in A,  is not covered in A. Hence a CA(N ; t, k, v ) covers all interactions in It,k,v . Covering arrays are used extensively for interaction testing in complex engineered systems. To ensure that all possible combinations of options of t components function together correctly, one needs examine all possible t-way interactions. When the number of components is k , and the number of different options available for each component is v , each row of CA(N ; t, k, v ) represents a test case. The N test cases collectively test all t-way interactions. For this reason, covering arrays have been used in combinatorial interaction testing in varied fields like software and hardware engineering, design of composite materials, and biological networks [8, 24, 26, 32, 34]. The cost of testing is directly related to the number of test cases. Therefore, one is interested in covering arrays with the fewest rows. The smallest value of N for which CA(N ; t, k, v ) exists is denoted by CAN(t, k, v ). Efforts to determine or bound CAN(t, k, v ) have been extensive; see [12, 14, 24, 31] for example. Naturally one would prefer to determine CAN(t, k, v ) exactly. Katona [22] and Kleitman and Spencer [23] independently showed that for t = v = 2, the minimum number of rows N in a CA(N ; 2, k, 2) is the smallest N for which -1 . Exact determination of CAN(t, k, v ) for other values of t and v has remained open. However, k  NN 2 some progress has been made in determining upper bounds for CAN(t, k, v ) in the general case; for recent results, see [33]. For practical applications such bounds are often unhelpful, because one needs explicit covering arrays to use as test suites. Explicit constructions can be recursive, producing larger covering arrays using smaller ones as ingredients (see [14] for a survey), or direct. Direct methods for some specific cases arise from algebraic, geometric, or number-theoretic techniques; general direct methods are computational in nature. Indeed when k is relatively small, the best known results arise from computational techniques [13], and these are in turn essential for the successes of recursive methods. Unfortunately, the existing computational methods encounter difficulties as k increases, but is still within the range needed for practical applications. Typically such difficulties arise either as a result of storage or time limitations or by producing covering arrays that are too big to compete with those arising from simpler recursive methods. Cohen [11] discusses commercial software where the number of factors often exceeds 50. Aldaco et al. [1] analyze a complex engineered system having 75 factors, using a variant of covering arrays. Android [3] uses a Configuration class to describe the device configuration; there are 17 different configuration parameters with 3 - 20 different levels. In each of these cases, while existing techniques are effective when the strength is small, these moderately large values of k pose concerns for larger strengths. In this paper, we focus on situations in which every factor has the same number of levels. These cases have been most extensively studied, and hence provide a basis for making comparisons. In practice, however, often different components have different number of levels, which is captured by extending the notion of a covering array. A mixed covering array MCA(N ; t, k, (v1 , v2 , . . . , vk )) is an N × k array in which the ith column contains vi symbols for 1  i  k . When {i1 , . . . , it }  {1, . . . , k } is a set of t columns, in the N × t t subarray obtained by selecting columns i1 , . . . , it of the MCA, each of the j =1 vij distinct t-tuples appears as a row at least once. Although we examine the uniform case in which v1 = · · · = vk , the methods developed here can all be directly applied to mixed covering arrays as well. Inevitably, when k > max(t + 1, v + 2), a covering array must cover some interactions more than once, for if not they are orthogonal arrays [20]. Treating the rows of a covering array in a fixed order, each row covers some number of interactions not covered by any earlier row. For a variety of known constructions, the initial rows cover many new interactions, while the later ones cover very few [7]. Comparing this rate of coverage for a purely random method and for one of the sophisticated search techniques, one finds little difference in the initial rows, but very substantial differences in the final ones. This suggests strategies to build the covering array in stages, investing more effort as the number of remaining uncovered interactions declines. In this paper we propose a new algorithmic framework for covering array construction, the two-stage framework. In the first stage, a randomized row construction method builds a specified number of rows to cover all but at most a specified, small number of interactions. As we see later, by dint of being randomized

2

this uses very little memory. The second stage is a more sophisticated search that adds few rows to cover the remaining uncovered interactions. We choose search algorithms whose requirements depend on the number of interactions to be covered, to profit from the fact that few interactions remain. By mixing randomized and deterministic methods, we hope to retain the fast execution and small storage of the randomized methods, along with the accuracy of the deterministic search techniques. We introduce a number of algorithms within the two-stage framework. Some improve upon best known bounds on CAN(t, k, v ) (see [33]) in principle. But our focus is on the practical consequences: The twostage algorithms are indeed quite efficient for higher strength (t  {5, 6}) and moderate number of levels (v  {3, 4, 5, 6}), when the number of factors k is moderately high (approximately in the range of 20 - 80 depending on value of t and v ). In fact, for many combination of t, k and v values the two-stage algorithms beat the previously best known bounds. Torres-Jimenez et al. [36] explore a related two-stage strategy. In their first stage, an in-parameter-order greedy strategy (as used in ACTS [24]) adds a column to an existing array; in their second stage, simulated annealing is applied to cover the remaining interactions. They apply their methods when t = v = 3, when the storage and time requirements for both stages remain acceptable. In addition to the issues in handling larger strengths, their methods provide no a priori bound on the size of the resulting array. In contrast with their methods, ours provide a guarantee prior to execution with much more modest storage and time. The rest of the paper is organized as follows. Section 2 reviews algorithmic methods of covering array construction, specifically the randomized algorithm and the density algorithm. This section contrasts these two methods and points out their limitations. Then it gives an intuitive answer to the question of why a two stage based strategy might work and introduces the general two-stage framework. Section 3 introduces some specific two-stage algorithms. Section 3.1 analyzes and evaluates the na¨ ive strategy. Section 3.2 describes a two-stage algorithm that combines the randomized and the density algorithm. Section 3.3 introduces graph coloring based techniques in the second stage. Section 3.4 examines the effect of group action on the size of the constructed covering arrays. Section 4 compares the results of various two-stage algorithms with the presently best known sizes. In Section 5 we discuss the Lov´ asz local lemma (LLL) bounds on CAN(t, k, v ) and provide a Moser-Tardos type randomized algorithm for covering array construction that matches the bound. Although the bound was known [18], the proof was non-constructive, and a constructive algorithm to match this bound seems to be absent in the literature. We explore potentially better randomized algorithms for the first stage using LLL based techniques, We also obtain a two-stage bound that improves the LLL bound for CAN(t, k, v ). We conclude the paper in Section 6.

2

Algorithmic construction of covering arrays

Available algorithms for the construction of covering arrays are primarily heuristic in nature; indeed exact algorithms have succeeded for very few cases. Computationally intensive metaheuristic search methods such as simulated annealing, tabu search, constraint programming, and genetic algorithms have been employed when the strength is relatively small or the number of factors and levels is small. These methods have established many of the best known bounds on sizes of covering arrays [13], but for many problems of practical size their time and storage requirements are prohibitive. For larger problems, the best available methods are greedy. The IPO family of algorithms [24] repeatedly adds one column at a time, and then adds -1 new rows to ensure complete coverage. In this way, at any point in time, the status of v t k t-1 interactions may be stored. AETG [10] pioneered a different method, which greedily selects one row at a time to cover a large number of as-yet-uncovered interactions. They establish that if a row can be chosen that covers the maximum number, a good a priori bound on the size of the covering array can be computed. Unfortunately selecting the maximum is NP-hard, and even if one selects the maximum there is no guarantee that the covering array is the smallest possible [7], so AETG resorts to a good heuristic selection of the next row by examining the stored status of v t k t interactions. None of the methods so far mentioned therefore guarantee to reach an a priori bound. An extension of the AETG strategy, the density algorithm [5, 6, 15], stores additional statistics for all v t k t interactions in order to ensure the selection of a good next row, and hence guarantees to produce an array with at most the precomputed number of rows. Variants of the density 3

Algorithm 1: A randomized algorithm for covering array construction. Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) log (k t )+t log v 1 Set N := ; vt
log
v t -1

2 3

4 5 6 7 8 9 10 11 12

repeat Construct an N × k array A where each entry is chosen independently and uniformly at random from a v -ary alphabet; Set covered := true; for each interaction   It,k,v do if  is not covered in A then Set covered := false; break; end end until covered = true ; Output A;

algorithm have proved to be most effective for problems of moderately large size. For even larger problems, pure random approaches have been applied. To produce methods that provide a guarantee on size, it is natural to focus on the density algorithm in order to understand its strengths and weaknesses. To do this, we contrast it with a basic randomized algorithm. Algorithm 1 shows a simple randomized algorithm for covering array construction. The algorithm constructs an array of a particular size randomly and checks whether all the interactions are covered. It repeats until it finds an array that covers all the interactions. log (k t )+t log v A CA(N ; t, k, v ) with N = is guaranteed to exist: vt
log
v t -1

Theorem 1. [21, 27, 35] (Stein-Lov´ asz-Johnson (SLJ) bound): Let t, k, v be integers with k  t  2, and v  2. Then as k  , CAN(t, k, v )  log
k t

+ t log v
vt v t -1

log

In fact, the probability that the N × k array constructed in line 3 of Algorithm 1 is a valid covering array is high enough that the expected number of times the loop in line 2 is repeated is a small constant. An alternative strategy is to add rows one by one instead of constructing the full array at the outset. We start with an empty array, and whenever we add a new row we ensure that it covers at least the expected number of previously uncovered interactions for a randomly chosen row. The probability that an uncovered interaction is covered by a random row is 1/v t . If the number of uncovered interactions is u, then by linearity of expectation, the expected number of newly covered interactions in a randomly chosen row is uv -t . If each row added covers exactly this expected number, we obtain the same number of rows as the SLJ bound, realized in Algorithm 1. But because the actual number of newly covered interactions is always an integer, each added row covers at least uv -t interactions. This is especially helpful towards the end when the expected number is a small fraction. Algorithm 2 follows this strategy. Again the probability that a randomly chosen row covers at least the expected number of previously uncovered interactions is high enough that the expected number of times the row selection loop in line 6 of Algorithm 2 is repeated is bounded by a small constant. We can obtain an upper bound on the size produced by Algorithm 2 by assuming that each new row added covers exactly uv -t previously uncovered interactions. This bound is the discrete Stein-Lov´ asz4

Algorithm 2: A randomized algorithm for covering array construction using the discrete SLJ strategy. Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) 1 Let A be an empty array; k t 2 Initialize a table T indexed by all t v interactions, marking every interaction "uncovered" ; 3 while there is an interaction marked "uncovered" in T do 4 Let u be the number of interactions marked "uncovered" in T ; u 5 Set expectedCoverage := v t ; 6 repeat 7 Let r be a row of length k where each entry is chosen independently and uniformly at random from a v -ary alphabet; 8 Let coverage be the number of "uncovered" interactions in T that are covered in row r; 9 until coverage > expectedCoverage ; 10 Add r to A; 11 Mark all interactions covered by r as "covered" in T ; 12 end 13 Output A;

Johnson (discrete SLJ) bound. Figure 1 compares the sizes of covering arrays obtained from the SLJ and the discrete SLJ bounds for different values of k when t = 6 and v = 3. Consider a concrete example, when t = 5, k = 20, and v = 3. The SLJ bound guarantees the existence of a covering array with 12499 rows, whereas the discrete SLJ bound guarantees the existence of a covering array with only 8117 rows. The density algorithm replaces the loop at line 6 of Algorithm 2 by a conditional expectation derandomized method. For fixed v and t the density algorithm selects a row efficiently (time polynomial in k ) and deterministically that is guaranteed to cover at least uv -t previously uncovered interactions. In practice, for small values of k the density algorithm works quite well, often covering many more interactions than the minimum. Many of the currently best known CAN(t, k, v ) upper bounds are obtained by the density algorithm in combination with various post-optimization techniques [13]. However, the practical applicability of Algorithm 2 and the density algorithm is limited by the storage t of the table T , representing each of the k t v interactions. Even when t = 6, v = 3, and k = 54, there are 18,828,003,285 6-way interactions. This huge memory requirement renders the density algorithm impractical for rather small values of k when t  {5, 6} and v  3. We present an idea to circumvent this large requirement for memory, and develop it in full in Section 3.

2.1

Why does a two stage based strategy make sense?

Compare the two extremes, the density algorithm and Algorithm 1. On one hand, Algorithm 1 does not suffer from any substantial storage restriction, but appears to generate many more rows than the density algorithm. On the other hand, the density algorithm constructs fewer rows for small values of k , but becomes impractical when k is moderately large. One wants algorithms that behave like Algorithm 1 in terms of memory, but yield a number of rows competitive with the density algorithm. For t = 6, k = 16, and v = 3, Figure 2 compares the coverage profile for the density algorithm and Algorithm 1. We plot the number of newly covered interactions for each row in the density algorithm, and the expected number of newly covered interactions for each row for Algorithm 1. The qualitative features exhibited by this plot are representative of the rates of coverage for other parameters. Two key observations are suggested by Figure 2. First, the expected coverage in the initial random rows is similar to the rows chosen by the density algorithm. In this example, the partial arrays consisting of the first 1000 rows exhibit similar coverage, yet the randomized algorithm needed no extensive bookkeeping. Secondly, as later rows are added, the judicious selections of the density algorithm produce much larger coverage per row than Algorithm 1. Consequently it appears sensible to invest few computational resources 5

3.5

x 10

4

SLJ bound Discrete SLJ bound 3

N - number of rows

2.5

2

1.5

1

0.5 0

100

200

300

400

500 k

600

700

800

900

1000

Figure 1: Comparison of covering array sizes obtained from SLJ bound and discrete SLJ bound for different values of k , when t = 6 and v = 3.

9000 8000 Number of newly covered interactions 7000 6000 5000 4000 3000 2000 1000 0 Density Basic Random

0

2000

4000

6000 Row number

8000

10000

12000

Figure 2: For t = 6, k = 16 and v = 3, the actual number of newly covered interactions of the density algorithm and the expected number of newly covered interactions in a random array.

6

on the initial rows, while making more careful selections in the later ones. This forms the blueprint of our general two-stage algorithmic framework shown in Algorithm 3. Algorithm 3: The general two-stage framework for covering array construction. Input: t : strength of the required covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) 1 Choose a number n of rows and a number  of interactions; // First Stage 2 Use a randomized algorithm to construct an n × k array A ; 3 Ensure that A covers all but at most  interactions; 4 Make a list L of interactions that are not covered in A (L contains at most  interactions); // Second Stage 5 Use a deterministic procedure to add N - n rows to A to cover all the interactions in L; 6 Output A; A specific covering array construction algorithm results by specifying the randomized method in the first stage, the deterministic method in the second stage, and the computation of n and . Any such algorithm produces a covering array, but we wish to make selections so that the resulting algorithms are practical while still providing a guarantee on the size of the array. In Section 3 we describe different algorithms from the two-stage family, determine the size of the partial array to be constructed in the first stage, and establish upper bound guarantees. In Section 4 we explore how good the algorithms are in practice.

3

Two-stage framework
Rand MT the basic randomized algorithm the Moser-Tardos type algorithm

For the first stage we consider two methods:

We defer the development of method MT until Section 5. Method Rand uses a simple variant of Algorithm 1, choosing a random n × k array. For the second stage we consider four methods: Naive Greedy Den Col the the the the na¨ ive strategy, one row per uncovered interaction online greedy coloring strategy density algorithm graph coloring algorithm

Using these abbreviations, we adopt a uniform naming convention for the algorithms: TS A, B is the algorithm in which A is used in the first stage, and B is used in the second stage. For example, TS MT, Greedy denotes a two-stage algorithm where the first stage is a Moser-Tardos type randomized algorithm and the second stage is a greedy coloring algorithm. Later when the need arises we refine these algorithm names.

3.1

One row per uncovered interaction in the second stage (TS Rand, Naive )

In the second stage each of the uncovered interactions after the first stage is covered using a new row. Algorithm 4 describes the method in more detail. This simple strategy improves on the basic randomized strategy when n is chosen judiciously. For example, when t = 6, k = 54 and v = 3, Algorithm 1 constructs a covering array with 17, 236 rows. Figure 3 plots an upper bound on the size of the covering array against the number n of rows in the partial array.

7

Algorithm 4: Na¨ ive two-stage algorithm (TS Rand, Naive ). Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) vt log (k t )+t log v +log log v t -1 ; 1 Let n := t v
log
v t -1

2 3 4

Let  =

1 log
vt v t -1

;

5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

repeat Let A be an n × k array where each entry is chosen independently and uniformly at random from a v -ary alphabet; Let uncovNum := 0 and unCovList be an empty list of interactions; Set covered := true; for each interaction   It,k,v do if  is not covered in A then Set uncovNum :=uncovNum +1; Add  to unCovList ; if uncovNum >  then Set covered := false; break; end end end until covered= true ; for each interaction  uncovList do Add a row to A that covers ; end Output A;

8

1.75 1.7 1.65 1.6 1.55 1.5 1.45 1.4 1.35 1.3

x 10

4

Total number of rows in the Covering array

1

1.1 1.2 1.3 1.4 1.5 1.6 1.7 n -- number of rows in the partial array of the first stage

1.8 x 10
4

Figure 3: An upper bound on the size of the covering array against n, the size of the partial array constructed in the first stage when t = 6, k = 54, and v = 3, with one new row added per uncovered interaction in the second stage. The minimum size of 13, 162 is obtained when n = 12, 402. Algorithm 1 requires 17, 236 rows, and the currently best known covering array has 17, 197 rows.

9

The smallest covering array is obtained when n = 12, 402 which, when completed, yields a covering array with at most 13, 162 rows--a big improvement over Algorithm 1. A theorem from [33] tells us the optimal value of n in general: Theorem 2. [33] Let t, k, v be integers with k  t  2, and v  2. Then log CAN(t, k, v ) 
k t

+ t log v + log log log
vt v t -1
vt v t -1

vt v t -1

+1 .

The bound is obtained by setting n =
t

log (k t )+t log v +log log log
vt v t -1

. The expected number of uncovered

interactions is exactly  = 1/ log vtv-1 . Figure 4 compares SLJ, discrete SLJ and two-stage bounds for k  100, when t = 6 and v = 3. The two-stage bound does not deteriorate in comparison to discrete SLJ bound as k increases; it consistently takes only 307-309 more rows. Thus when k = 12 the two-stage bound requires only 6% more rows and when k = 100 only 2% more rows than the discrete SLJ bound.
x 10
4

2.2 2 1.8 N - number of rows 1.6 1.4 1.2 1 0.8 0.6

SLJ bound Discrete SLJ bound Two-stage bound

0.4 10

20

30

40

50 k

60

70

80

90

100

Figure 4: Comparison of covering array sizes obtained from SLJ bound, discrete SLJ bound and two-stage bound for k  100, when t = 6 and v = 3. In this range of k values the two-stage bound requires 307-309 more rows than the discrete SLJ bound, that is, 2-6% more rows. To ensure that the loop in line 7 of Algorithm 4 does not repeat too many times we need to know the probability with which a random n × k array leaves at most  interactions uncovered. Using Chebyshev's inequality and the second moment method developed in [2, Chapter 4], we next show that in a random n × k 1 n t array the number of uncovered interactions is almost always close to its expectation, i.e. k . t v 1 - vt Substituting the value of n from line 1, this expected value is equal to µ, as in line 2. Therefore, the probability that a random n × k array covers the desired number of interactions is constant, and the expected number of times the loop in line 7 is repeated is also a constant (around 2 in practice).

10

Because the theory of the second moment method is developed in considerable detail in [2], here we briefly m mention the relevant concepts and results. Suppose that X = i=1 Xi , where Xi is the indicator random variable for event Ai for 1  i  m. For indices i, j , we write i  j if i = j and the events Ai , Aj are not independent. Also suppose that X1 , . . . , Xm are symmetric, i.e. for every i = j there is a measure preserving mapping of the underlying probability space that sends event Ai to event Aj . Define  = j i Pr [Aj |Ai ]. Then by [2, Corollary 4.3.4]: Lemma 3. [2] If E[X ]   and  = o(E[X ]) then X  E[X ] almost always. In our case, Ai denotes the event that the ith interaction is not covered in a n × k array where each entry 1 n . Because is chosen independently and uniformly at random from a v -ary alphabet. Then Pr[Xi ] = 1 - v t k t 1 n t , and E [ X ]   as there are k v interactions in total, by linearity of expectation, E [ X ] = v 1 - t t vt k  . Distinct events Ai and Aj are independent if the ith and j th interactions share no column. Therefore, k  the event Ai is not independent of at most t t- j i Pr [Aj |Ai ]  j i 1  1 other events Aj . So  = k t t-1 = o(E[X ]) when v and t are constants. By Lemma 3, the number of uncovered interactions in a random n × k array is close to the expected number of uncovered interactions. This guarantees that Algorithm 4 is an efficient randomized algorithm for constructing covering arrays with a number of rows upper bounded by Theorem 2. In keeping with the general two-stage framework, Algorithm 4 does not store the coverage status of each interaction. We only need store the interactions that are uncovered in A, of which there are at most 1  =  v t . This quantity depends only on v and t and is independent of k , so is effectively a vt
log
v t -1

t constant that is much smaller than k t v , the storage requirement for the density algorithm. Hence the algorithm can be applied to a higher range of k values. Although Theorem 5 provides asymptotically tighter bounds than Theorem 2, in a range of k values that are relevant for practical application, Theorem 2 provides better results. Figure 5 compares the bounds on CAN(t, k, v ) with the currently best known results.
4 6

3

x 10

2.5 Best known Two-stage (simple) GSS bound 2

x 10

2.5

Best known Two-stage (simple) GSS bound

N - number of rows

N - number of rows 10 20 30 40 k 50 60 70 80 90

2

1.5

1.5

1

1

0.5

0.5

0 0

0

5

10

15

20

25 k

30

35

40

45

50

(a) t = 6, v = 3

(b) t = 6, v = 6

Figure 5: Comparison of GSS bound and two-stage bound with the currently best known results

3.2

The density algorithm in the second stage (TS Rand, Den )

Next we apply the density algorithm in the second stage. Figure 6 plots an upper bound on the size of the covering array against the size of the partial array constructed in the first stage when the density algorithm is used in the second stage, and compares it with TS Rand, Naive . The size of the covering array decreases 11

as n decreases. This is expected because with smaller partial arrays, more interactions remain for the second stage to be covered by the density algorithm. In fact if we cover all the interactions using the density algorithm (as when n = 0) we would get an even smaller covering array. However, our motivation was precisely to avoid doing that. Therefore, we need a "cut-off" for the first stage.
x 10
4

1.9

Total number of rows in the Covering array

1.8

Basic Two-stage Two-stage with density in second stage

1.7

1.6

1.5

1.4

1.3

1.2

1

1.1 1.2 1.3 1.4 1.5 1.6 1.7 n -- number of rows in the partial array of the first stage

1.8 x 10
4

Figure 6: Comparison of covering array sizes from two-stage algorithms with Den and Naive in the second stage. With Den there is no minimum point in the curve; the size of the covering array keeps decreasing as we leave more uncovered interactions for the second stage. We are presented with a trade-off. If we construct a smaller partial array in the first stage, we obtain a smaller covering array overall. But we then pay for more storage and computation time for the second stage. To appreciate the nature of this trade-off, look at Figure 7, which plots an upper bound on the covering array size and the number of uncovered interactions in the first stage against n. The improvement in the covering array size plateaus after a certain point. The three horizontal lines indicate  ( v t ), 2 and 3 uncovered interactions in the first stage. (In the na¨ ive method of Section 3.1, the partial array after the first stage leaves at most  uncovered interactions.) In Figure 7 the final covering array size appears to plateau when the number of uncovered interactions left by the first stage is around 2. After that we see diminishing returns -- the density algorithm needs to cover more interactions in return for a smaller improvement in the covering array size. Let r be the maximum number of interactions allowed to remain uncovered after the first stage. Then r can be specified in the two-stage algorithm. To accommodate this, we denote by TS A, B ; r the two-stage algorithm where A is the first stage strategy, B is the second stage strategy, and r is the maximum number of uncovered interactions after the first stage. For example, TS Rand, Den; 2 applies the basic randomized algorithm in the first stage to cover all but at most 2 interactions, and the density algorithm to cover the remaining interactions in the second stage.

3.3

Coloring in the second stage (TS Rand, Col and TS Rand, Greedy )

Now we describe strategies using graph coloring in the second stage. Construct a graph G = (V, E ), the incompatibility graph, in which V is the set of uncovered interactions, and there is an edge between two 12

18000 Number of rows / Number of uncovered interactions 16000 14000 12000 10000 8000 6000 4000 2000 0 0.8 Num. of rows in the completed CA Num. of uncovered interaction in first stage

0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 4 n -- number of rows in the partial array of the first stage x 10

Figure 7: Final covering array size against the number of uncovered interactions after the first stage. As the size n of the partial array decreases, the number of uncovered interactions in the first stage increases. Den is used in the second stage. From bottom to top, the green lines denote , 2, and 3 uncovered interactions. interactions exactly when they share a column in which they have different symbols. A single row can cover a set of interactions if and only if it forms an independent set in G. Hence the minimum number of rows required to cover all interactions of G is exactly its chromatic number (G), the minimum number of colors in a proper coloring of G. Graph coloring is an NP-hard problem, so we employ heuristics to bound the chromatic number. Moreover, G only has vertices for the uncovered interactions after the first stage, so is size is small relative to the total number of interactions. The expected number of edges in the incompatibility graph after choosing n rows uniformly at random n t k t t k -t 1 1 n t t-i is  = 2 ) 1- v . Using the elementary upper bound on 1 - (vt -1 t i=1 i t-i (v - v t v v t-i ) the chromatic number  
1 2

+

2m + 1 4 , where m is the number of edges [16, Chapter 5.2], we can surely

cover the remaining interactions with at most 1 2m + 1 2 + 4 rows. The actual number of edges m that remain after the first stage is a random variable with mean  . In principle, the first stage could be repeatedly applied until m   , so we call m =  the optimistic estimate. To ensure that the first stage is expected to be run a small constant number of times, we increase the estimate. With probability more than 1/2 the incompatibility graph has m  2 edges, so m = 2 is the conservative estimate. For t = 6, k = 56, and v = 3, Figure 8 shows the effect on the minimum number of rows when the bound on the chromatic number in the second stage is used, for the conservative or optimistic estimates. The Na¨ ive method is plotted for comparison. Better coloring bounds shift the minima leftward, reducing the number of rows produced in both stages. Thus far we have considered bounds on the chromatic number. Better estimation of (G) is complicated by the fact that we do not have much information about the structure of G until the first stage is run. In practice, however, G is known after the first stage and hence an algorithmic method to bound its chromatic number can be applied. Because the number of vertices in G equals the number of uncovered interactions

13

2.2

x 10

4

2

Conservative estimate Optimistic estimate Simple

Number of rows required

1.8

1.6

1.4

1.2

1

1

1.1

1.2

1.3

1.4 N

1.5

1.6

1.7

1.8 x 10
4

Figure 8: Size of the partial array vs. size of the complete CA. t = 6, k = 56, v = 3. Stein-Lov´ asz-Johnson bound requires 17, 403 rows, discrete Stein-Lov´ asz-Johnson bound requires 13, 021 rows. Simple estimate for the two stage algorithm is 13, 328 rows, conservative estimate assuming m = 2 is 12, 159 rows, and optimistic estimate assuming m =  is 11, 919 rows. Even the conservative estimate beats the discrete Stein-Lov´ asz-Johnson bound.

14

after the first stage, we encounter the same trade-off between time and storage, and final array size, as seen earlier for density. Hence we again parametrize by the expected number of uncovered interactions in the first stage. We employ two different greedy algorithms to color the incompatibility graph. In method Col we first construct the incompatibility graph G after the first stage. Then we apply the commonly used smallest last order heuristic to order the vertices for greedy coloring: At each stage, find a vertex vi of minimum degree in Gi , order the vertices of Gi - vi , and then place vi at the end. More precisely, we order the vertices of G as v1 , v2 , . . . , vn , such that vi is a vertex of minimum degree in Gi , where Gi = G - {vi+1 , . . . , vn }. A graph is d-degenerate if all of its subgraphs have a vertex with degree at most d. When G is d-degenerate but not (d - 1)-degenerate, the Coloring number col(G) is d + 1. If we then greedily color the vertices with the first available color, at most col(G) colors are used. In method Greedy we employ an on-line, greedy approach that colors the interactions as they are discovered in the first stage. In this way, the incompatibility graph is never constructed. We instead maintain a set of rows. Some entries in rows are fixed to a specific value; others are flexible to take on any value. Whenever a new interaction is found to be uncovered in the first stage, we check if any of the rows is compatible with this interaction. If such a row is found then entries in the row are fixed so that the row now covers the interaction. If no such row exists, a new row with exactly t fixed entries corresponding to the interaction is added to the set of rows. This method is much faster than method Col in practice.

3.4

Using group action

Covering arrays that are invariant under the action of a permutation group on their symbols can be easier to construct and are often smaller [15]. Direct and computational constructions using group actions are explored in [9, 28]. Sarkar et al. [33] establish the asymptotically tightest known bounds on CAN(t, k, v ) using group actions. In this section we explore the implications of group actions on two-stage algorithms. Let  be a permutation group on the set of symbols. The action of this group partitions the set of t-way interactions into orbits. We construct an array A such that for every orbit, at least one row covers an interaction from that orbit. Then we develop the rows of A over  to obtain a covering array that is invariant under the action of . Effort then focuses on covering all the orbits of t-way interactions, instead of the individual interactions. If  acts sharply transitively on the set of symbols (for example, if  is a cyclic group of order v ) then k t-1 t the action of  partitions k orbits of length v each. Following the lines of the t v interactions into t v v t-1 log (k t )+(t-1) log v +log log v t-1 -1 +1 that covers at proof of Theorem 2, there exists an n × k array with n = v t-1
log
v t-1 -1

least one interaction from each orbit. Therefore, log CAN(t, k, v )  v
k t

+ (t - 1) log v + log log log
v t- 1 v t- 1 - 1

v t-1 v t-1 -1

+1 . (1)

Similarly, we can employ a Frobenius group. When v is a prime power, the Frobenius group is the group of permutations of Fv of the form {x  ax + b : a, b  Fv , a = 0}. The action of the Frobenius group t-1 1 partitions the set of t-tuples on v symbols into v v-- 1 orbits of length v (v - 1) (full orbits) each and 1 orbit of length v (a short orbit). The short orbit consists of tuples of the form (x1 , . . . , xt ) where x1 = . . . = xt . Therefore, we can obtain a covering array by first constructing an array that covers all the full orbits, and then developing all the rows over the Frobenius group and adding v constant rows. Using the two stage strategy in conjunction with the Frobenius group action we obtain: log CAN(t, k, v )  v (v - 1)
k t

+ log

v t-1 -1 v -1

+ log log
v t- 1 v t-1 -v +1

v t-1 v t-1 -v +1

+1 + v. (2)

log

15

1.5

x 10

4

1.45

Two-stage (simple) Two-stage (cyclic group action) Two-stage (Frobenius group action)

N - number of rows

1.4

1.35

1.3

1.25 50

55

60 k

65

70

75

Figure 9: Comparison of the simple two-stage bound with the cyclic and the Frobenius two-stage bounds. t = 6, v = 3 and 50  k  75. Group action reduces the required number of rows slightly. Figure 9 compares the simple two-stage bound with the cyclic and Frobenius two-stage bounds. For t = 6, v = 3 and 12  k  100, the cyclic bound requires 7-21 (on average 16) fewer rows than the simple bound. In the same range the Frobenius bound requires 17 - 51 (on average 40) fewer rows. Group action can be applied in other methods for the second stage as well. Colbourn [15] incorporates group action into the density algorithm, allowing us to apply method Den in the second stage. Greedy extends easily to use group action, as we do not construct an explicit incompatibility graph. Whenever we fix entries in a row to cover an uncovered orbit, we commit to a specific orbit representative. However, applying group action to the incompatibility graph coloring for Col is more complicated. We need to modify the definition of the incompatibility graph for two reasons. First the vertices no longer represent uncovered interactions, but rather uncovered orbits of interaction. Secondly, and perhaps more importantly, pairwise compatibility between every two orbits in a set no longer implies mutual compatibility among all orbits in the set. One approach is to form a vertex for each uncovered orbit, placing an edge between two when they share a column. Rather than the usual coloring, however, one asks for a partition of the vertex set into classes so that every class induces an acyclic subgraph. Problems of this type are generalized graph coloring problems [4]. Within each class of such a vertex partition, consistent representatives of each orbit can be selected to form a row; when a cycle is present, this may not be possible. Unfortunately, heuristics for solving these types of problems appear to be weak, so we adopt a second approach. As we build the incompatibility graph, we commit to specific orbit representatives. When a vertex for an uncovered orbit is added, we check its compatibility with the orbit representatives chosen for the orbits already handled with which it shares columns; we commit to an orbit representative and add edges to those with which it is now incompatible. Once completed, we have a (standard) coloring problem for the resulting graph. Because group action can be applied using each of the methods for the two stages, we extend our naming to TS A, B ; r,  , where  can be Trivial (i.e. no group action), Cyclic, or Frobenius.

16

4

Computational results

Figure 5 indicates that even a simple two-stage bound can improve on best known covering array numbers. Therefore we investigate the actual performance of our two-stage algorithms for covering arrays of strength 5 and 6. First we present results for t = 6, when v  {3, 4, 5, 6} and no group action is assumed. Table 1 shows the results for different v values. In each case we select the range of k values where the two-stage bound predicts smaller covering arrays than the previously known best ones, setting the maximum number of uncovered t interactions as  = 1/ log vtv-1  v t . For each value of k we construct a single partial array and then run the different second stage algorithms on it consecutively. In this way all the second stage algorithms cover the same set of uncovered interactions. The column tab lists the best known CAN(t, k, v ) upper bounds from [13]. The column bound shows the upper bounds obtained from the two-stage bound (2). The columns na¨ ive, greedy, col and den show results obtained from running the TS Rand, Naive; , Trivial , TS Rand, Greedy; , Trivial , TS Rand, Col; , Trivial and TS Rand, Den; , Trivial algorithms, respectively. The na¨ ive method always finds a covering array that is smaller than the two-stage bound. This happens because we repeat the first stage of Algorithm 4 until the array has fewer than v t uncovered interactions. (If the first stage were not repeated, the algorithm still produce covering arrays that are not too far from the bound.) For v = 3 Greedy and Den have comparable performance. Method Col produces covering arrays that are smaller. However, for v  {4, 5, 6} Den and Col are competitive. Table 2 shows the results obtained by the different second stage algorithms when the maximum number of uncovered interactions in the first stage is set to 2 and 3 respectively. When more interactions are covered in the second stage, we obtain smaller arrays as expected. However, the improvement in size does not approach 50%. There is no clear winner. Next we investigate the covering arrays that are invariant under the action of a cyclic group. In Table 3 the column bound shows the upper bounds from Equation (1). The columns na¨ ive, greedy, col and den show results obtained from running TS Rand, Naive; , Cyclic , TS Rand, Greedy; , Cyclic , TS Rand, Col; , Cyclic and TS Rand, Den; , Cyclic , respectively. Table 4 presents results for cyclic group action based algorithms when the number of maximum uncovered interactions in the first stage is set to 2 and 3 respectively. For the Frobenius group action, we show results only for v  {3, 5} in Table 5. The column bound shows the upper bounds obtained from Equation (2). Table 6 presents results for Frobenius group action algorithms when the number of maximum uncovered interactions in the first stage is 2 or 3. Next we present a handful of results when t = 5. In the cases examined, using the trivial group action is too time consuming to be practical. However, the cyclic or Frobenius cases are feasible. Tables 7 and 8 compare two stage algorithms when the number of uncovered interactions in the first stage is at most 2. In almost all cases there is no clear winner among the three second stage methods. Methods Den and Greedy are, however, substantially faster and use less memory than method Col; for practical purposes they would be preferred. All code used in this experimentation is available from the github repository https://github.com/ksarkar/CoveringArray under an open source GPLv3 license.

5

Limited dependence and the Moser-Tardos algorithm

Here we explore a different randomized algorithm that produces smaller covering arrays than Algorithm 1. When k > 2t, there are interactions that share no column. The events of coverage of such interactions are independent. Moser et al. [29, 30] provide an efficient randomized construction method that exploits this

17

k 53 54 55 56 57 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27

tab 13021 14155 17161 19033 20185 68314 71386 86554 94042 99994 104794 233945 258845 281345 293845 306345 356045 506713 583823 653756 694048 783784 844834 985702 1035310 1112436 1146173 1184697

bound 13076 13162 13246 13329 13410 65520 66186 66834 67465 68081 68681 226700 229950 233080 236120 239050 241900 486310 505230 522940 539580 555280 570130 584240 597660 610460 622700 634430

na¨ ive greedy t = 6, v = 3 13056 12421 13160 12510 13192 12590 13304 12671 13395 12752 t = 6, v = 4 65452 61913 66125 62573 66740 63209 67408 63819 68064 64438 68556 65021 t = 6, v = 5 226503 213244 229829 216444 232929 219514 235933 222516 238981 225410 241831 228205 t = 6, v = 6 486302 449950 505197 468449 522596 485694 539532 502023 555254 517346 569934 531910 584194 545763 597152 558898 610389 571389 622589 583473 634139 594933

col 12415 12503 12581 12665 12748 61862 62826 63160 64077 64935 65739 212942 217479 219215 222242 226379 230202 448922 467206 484434 500788 516083 530728 544547 557917 570316 582333 593857

den 12423 12512 12591 12674 12757 61886 62835 63186 64082 64907 65703 212940 217326 219241 222244 226270 229942 447864 466438 483820 500194 515584 530242 548307 557316 569911 582028 593546

Table 1: Comparison of different TS Rand, -; , Trivial algorithms.

18

k greedy 53 54 55 56 57 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27 11968 12135 12286 12429 12562 59433 60090 60715 61330 61936 62530 204105 207243 210308 213267 216082 218884 425053 443236 460315 476456 491570 505966 519611 532612 544967 556821 568135

2 col 11958 12126 12129 12204 12290 59323 60479 61527 62488 61839 62899 203500 206659 209716 212675 215521 218314 -

den t = 6, v 11968 12050 12131 12218 12296 t = 6, v 59326 59976 60615 61242 61836 62428 t = 6, v 203302 206440 209554 212508 215389 218172 t = 6, v 420333 438754 455941 472198 487501 502009 515774 528868 541353 553377 564827

greedy =3 11716 11804 11877 11961 12044 =4 58095 58742 59369 59974 60575 61158 =5 199230 202342 205386 208285 211118 213872 =6 412275 430402 447198 463071 478269 492425 505980 518746 531042 542788 554052

3 col 11705 11787 11875 12055 12211 57951 58583 59867 61000 60407 61004 198361 201490 204548 -

den 11708 11790 11872 11950 12034 57888 58544 59187 59796 60393 60978 197889 201068 204107 207060 209936 212707 405093 423493 440532 456725 471946 486306 500038 513047 525536 537418 548781

Table 2: Comparison of TS Rand, -; 2, Trivial and TS Rand, -; 3, Trivial algorithms.

19

k 53 54 55 56 57 k 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27

tab 13021 14155 17161 19033 20185 tab 68314 71386 86554 94042 99994 104794 226000 244715 263145 235835 238705 256935 506713 583823 653756 694048 783784 844834 985702 1035310 1112436 1146173 1184697

bound 13059 13145 13229 13312 13393 bound 65498 66163 66811 67442 68057 68658 226680 229920 233050 236090 239020 241870 486290 505210 522910 539550 555250 570110 584210 597630 610430 622670 624400

na¨ ive greedy t = 6, v = 3 13053 12405 13119 12489 13209 12573 13284 12660 13368 12744 t = 6, v = 4 na¨ ive greedy 65452 61896 66080 62516 66740 63184 67408 63800 68032 64408 68556 64988 t = 6, v = 5 226000 213165 229695 216440 233015 219450 235835 222450 238705 225330 241470 228140 t = 6, v = 6 485616 449778 504546 468156 522258 485586 539280 501972 554082 517236 569706 531852 583716 545562 597378 558888 610026 571380 622290 583320 633294 594786

col 12405 12543 12663 12651 12744 col 61860 62820 63144 63780 64692 64964 212945 217585 221770 222300 225130 229235 448530 467232 490488 500880 521730 530832 549660 557790 575010 582546 598620

den 12411 12546 12663 12663 12750 den 61864 62784 63152 63784 64680 64976 212890 217270 221290 222210 225120 229020 447732 466326 488454 500172 519966 530178 548196 557280 573882 582030 597246

Table 3: Comparison of TS Rand, -; , Cyclic algorithms.

20

k greedy 53 54 55 56 57 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27 11958 12039 12120 12204 12276 59412 60040 60700 61320 61908 62512 204060 207165 207165 213225 216050 218835 424842 443118 460014 476328 491514 505884 519498 532368 544842 543684 568050

2 col 11955 12027 12183 12342 12474 59336 59996 61156 62196 63192 64096 203650 209110 209865 212830 217795 218480 422736 440922 457944 474252 489270 503580 517458 530340 542688 543684 566244

den t = 6, v 11958 12036 12195 12324 12450 t = 6, v 59304 59964 61032 61976 62852 63672 t = 6, v 203265 208225 209540 212510 217070 218155 t = 6, v 420252 438762 455994 472158 487500 501852 515718 528828 541332 543684 564756

greedy =3 11700 11790 11862 11949 12027 =4 58076 58716 59356 59932 60568 61152 =5 199180 202255 205380 208225 211080 213770 =6 411954 430506 447186 463062 478038 492372 505824 518700 530754 542664 553704

3 col 11691 11874 12057 11937 12021 57976 58616 59252 59840 61124 61048 198455 204495 204720 207790 213425 213185 409158 427638 456468 460164 486180 489336 502806 515754 538056 539922 560820

den 11694 11868 12027 11943 12024 57864 58520 59160 59760 60904 60988 197870 203250 204080 207025 212040 212695 405018 423468 449148 456630 479970 486264 500040 512940 532662 537396 555756

Table 4: Comparison of TS Rand, -; 2, Cyclic and TS Rand, -; 3, Cyclic algorithms.

21

k 53 54 55 56 57 31 32 33 34 35 36

tab 13021 14155 17161 19033 20185 233945 258845 281345 293845 306345 356045

bound 13034 13120 13203 13286 13366 226570 229820 232950 235980 238920 241760

na¨ ive greedy t = 6, v = 3 13029 12393 13071 12465 13179 12561 13245 12633 13365 12723 t = 6, v = 5 226425 213025 229585 216225 232725 219285 234905 222265 238185 225205 241525 227925

col 12387 12513 12549 12627 12717 212865 216085 219205 223445 227445 231145

den 12393 12531 12567 12639 12735 212865 216065 219145 223265 227065 230645

Table 5: Comparison of TS Rand, -; , Frobenius algorithms.

k greedy 53 54 55 56 57 70 75 80 85 90 31 32 33 34 35 36 50 55 60 65 11931 12021 12105 12171 12255 13167 13473 13773 14031 14289 203785 206965 209985 213005 215765 218605 250625 259785 268185 275785

2 col 11919 12087 12237 12171 12249 13155 13473 13767 14025 14283 203485 208965 209645 214825 215545 218285 250365 259625 268025 275665

den greedy t = 6, v = 3 11931 11700 12087 11790 12231 11862 12183 11949 12255 12027 13179 13479 13779 14037 14301 t = 6, v = 5 203225 198945 208065 201845 209405 205045 214145 208065 215265 210705 218025 213525 250325 259565 267945 275665 -

3 col 11691 11874 12057 11937 12021 198445 204505 209845 207545 210365 213105 -

den 11694 11868 12027 11943 12024 197825 203105 207865 206985 209885 212645 -

Table 6: Comparison of TS Rand, -; 2, Frobenius and TS Rand, -; 3, Frobenius algorithms.

22

k 67 68 69 70 71

tab 59110 60991 60991 60991 60991

greedy 48325 48565 48765 49005 49245

col 48285 48565 49005 48985 49205

den 48305 48585 48985 49025 49245

Table 7: Comparison of TS Rand, -; 2, Frobenius algorithms. t = 5, v = 5 k 49 50 51 52 53 tab 122718 125520 128637 135745 137713 greedy 108210 109014 109734 110556 111306 col 108072 108894 110394 110436 111180 den 107988 108822 110166 110364 111120

Table 8: Comparison of TS Rand, -; 2, Cyclic algorithms. t = 5, v = 6 limited dependence. Specializing their method to covering arrays, we obtain Algorithm 5. For the specified value of N in the algorithm it is guaranteed that the expected number of times the loop in line 3 of Algorithm 5 is repeated is linearly bounded in k (See Theorem 1.2 of [30]). The upper bound on CAN(t, k, v ) guaranteed by Algorithm 5 is obtained by applying the Lov´ asz local lemma (LLL). Lemma 4. (Lov´ asz local lemma; symmetric case) (see [2]) Let A1 , A2 , . . . , An events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at ¯ most d, and that Pr[Ai ]  p for all 1  i  n. If ep(d + 1)  1, then Pr[n i=1 Ai ] > 0. The symmetric version of Lov´ asz local lemma provides an upper bound on the probability of a "bad" event in terms of the maximum degree of a bad event in a dependence graph, so that the probability that all the bad events are avoided is non zero. Godbole, Skipper, and Sunley [18] apply Lemma 4 essentially to obtain the bound on CAN(t, k, v ) in line 1 of Algorithm 5. Theorem 5. [18] Let t, v and k  2t be integers with t, v  2. Then log CAN (t, k, v ) 
k t

-

k-t t

+ t log v + 1

log

vt v t -1

The bound on the size of covering arrays obtained from Theorem 5 is asymptotically tighter than the one obtained from Theorem 1. Figure 10 compares the bounds for t = 6 and v = 3. The original proof of LLL is essentially non-constructive and does not immediately lead to a polynomial time construction algorithm for covering arrays satisfying the bound of Theorem 5. Indeed no previous construction algorithms appear to be based on it. However the Moser-Tardos method of Algorithm 5 does provide a construction algorithm running in expected polynomial time. For sufficiently large values of k Algorithm 5 produces smaller covering arrays than the Algorithm 1. But the question remains: Does Algorithm 5 produce smaller covering arrays than the currently best known results within the range that it can be effectively computed? Perhaps surprisingly, we show that the answer is affirmative. In Algorithm 5 we do not need to store the coverage information of individual interactions in memory because each time an uncovered interaction is encountered we re-sample the columns involved in that interaction and start the check afresh (checking the coverage in interactions in the same order each time). Consequently, Algorithm 5 can be applied for larger values of k than the density algorithm. 23

Algorithm 5: Moser-Tardos type algorithm for covering array construction. Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) k -t log{(k t )-( t )}+t. log v +1 ; 1 Let N := vt
log
v t -1

2

3 4 5 6 7 8 9 10 11 12 13

14 15 16

Construct an N × k array A where each entry is chosen independently and uniformly at random from a v -ary alphabet; repeat Set covered := true; for each interaction   It,k,v do if  is not covered in A then Set covered := false; Set missing-interaction := ; break; end end if covered = false then Choose all the entries in the t columns involved in missing-interaction independently and uniformly at random from the v -ary alphabet; end until covered = true ; Output A;

Smaller covering arrays can be obtained by exploiting a group action using LLL, as shown in [33]. Table 9 shows the sizes of the covering arrays constructed by a variant of Algorithm 5 that employs cyclic and Frobenius group actions. While this single stage algorithm produces smaller arrays than the currently best known [13], these are already superseded by the two-stage based algorithms. k 56 57 58 59 60 tab 19033 20185 23299 23563 23563 MT 16281 16353 16425 16491 16557 k 44 45 46 47 48 tab 411373 417581 417581 423523 423523 MT 358125 360125 362065 363965 365805 k 25 26 27 28 29 tab 1006326 1040063 1082766 1105985 1149037 MT 1020630 1032030 1042902 1053306 1063272

(a) Frobenius. t = 6, v = 3

(b) Frobenius. t = 6, v = 5

(c) Cyclic. t = 6, v = 6

Table 9: Comparison of covering array size from Algorithm 5 (MT) with the best known results [13] (tab).

5.1

Moser-Tardos type algorithm for the first stage

The linearity of expectation arguments used in the SLJ bounds permit one to consider situations in which a few of the "bad" events are allowed to occur, a fact that we exploited in the first stage of the algorithms thus far. However, the Lov´ asz local lemma does not address this situation directly. The conditional Lov´ asz local lemma (LLL) distribution, introduced in [19], is a very useful tool. Lemma 6. (Conditional LLL distribution; symmetric case) (see [2, 33]) Let A = {A1 , A2 , . . . , Al } be a set of l events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at most d, and that Pr[Ai ]  p for all 1  i  l. Also suppose that ep(d +1)  1 ¯ (Therefore, by LLL (Lemma 4) Pr[l / A be another event in the same probability space i=1 Ai ] > 0). Let B  24

10

5

SLJ bound GSS bound

N - number of rows

10

4

10 1 10

3

10

2

10 k

3

10

4

10

5

Figure 10: Comparison of SLJ (Theorem 1) and GSS (Theorem 5) bounds for t = 6 and v = 3. The graph is plotted in log-log scale to highlight the asymptotic difference between the two bounds. with Pr[B ]  q , such that B is also mutually independent of a set of all other events Aj  A except for at ¯ most d. Then Pr[B | l i=1 Ai ]  eq . We apply the conditional LLL distribution to obtain an upper bound on the size of partial array that t leaves at most log vtv-1  v t interactions uncovered. For a positive integer k , let I = {j1 , . . . , j }  [k ] where j1 < . . . < j . Let A be an n × k array where each entry is from the set [v ]. Let AI denote the n ×  array in which AI (i, ) = A(i, j ) for 1  i  N and 1   ; AI is the projection of A onto the columns in I . ] Let M  [v ]t be a set of m t-tuples of symbols, and C  [k be a set of t columns. Suppose the t entries in the array A are chosen independently from [v ] with uniform probability. Let BC denote the event that at least one of the tuples in M is not covered in AC . There are  = k t such events, and for all of 1 n them Pr[BC ]  m 1 - v . Moreover, when k  2t, each of the events is mutually independent of all t k k -t other events except for at most  = k - 1 < t t- asz local lemma, when t - t 1 . Therefore, by the Lov´ 1 n em 1 - vt  1, none of the events BC occur. Solving for n, when n log(em) log
vt v t -1

(3)

] there exists an n × k array A over [v ] such that for all C  [k t , AC covers all the m tuples in M . In fact we can use a Moser-Tardos type algorithm to construct such an array. Let  be an interaction whose t-tuple of symbols is not in M . Then the probability that  is not covered 1 n in an n × k array is at most 1 - v when each entry of the array is chosen independently from [v ] with t uniform probability. Therefore, by the conditional LLL distribution the probability that  is not covered ] 1 n in the array A where for all C  [k . Moreover, t , AC covers all the m tuples in M is at most e 1 - v t there are  (v t - m) such interactions . By the linearity of expectation, the expected number of uncovered

25

interactions in A is less than v t when  (v t - m)e 1 - n

1 n vt

 v t . Solving for n, we obtain
m vt

log e 1 - log
vt v t -1

.

(4)

Therefore, there exists an n × k array with n = max

log(em) log
vt v t -1

,

m log{e(1- v t )}

log

vt v t -1

that has at most v t

uncovered interactions. To compute n explicitly, we must choose m. We can select a value of m to minimize n graphically for given values of t, k and v . For example, Figure 11 plots Equations 3 and 4 against m for t = 3, k = 350, v = 3, and finds the minimum value of n.
460 Equation (3) Equation (4) n - number of rows in the partial array n - number of rows in the partial array 440 440 445 max(Equation (3), Equation (4))

420

435

400

430

380

360

425

340 0

5

10

15 m

20

25

30

420 0

5

10

15 m

20

25

30

(a) Equations 3 and 4 against m.

(b) Maximum of the two sizes against m.

Figure 11: The minimum is at n = 422, when m = 16. t = 3, k = 350, v = 3 We compare the size of the partial array from the na¨ ive two-stage method (Algorithm 4) with the size obtained by the graphical methods in Figure 12. The Lov´ asz local lemma based method is asymptotically better than the simple randomized method. However, except for the small values of t and v , in the range of k values relevant for practical applications the simple randomized algorithm requires fewer rows than the Lov´ asz local lemma based method.

5.2

Lov´ asz local lemma based two-stage bound

We can apply the techniques from Section 5.1 to obtain a two-stage bound similar to Theorem 2 using the Lov´ asz local lemma and conditional LLL distribution. First we extend a result from [33]. Theorem 7. Let t, k, v be integers with k  t  2, v  2 and let  =
v log 
t vt v t -1

k t

, and  =

k t

-

k -t t

. If

 v t Then log CAN(t, k, v ) 
k t

+ t log v + log log log
vt v t -1

vt v t -1

+2

 - . 

Proof. Let M  [v ]t be a set of m t-tuples of symbols. Following the arguments of Section 5.1, when ) ] n  log(em there exists an n × k array A over [v ] such that for all C  [k t , AC covers all m tuples in M . vt
log
v t -1

At most  (v t - m) interactions are uncovered in such an array. Using the conditional LLL distribution, 1 n the probability that one such interaction is not covered in A is at most e 1 - v . Therefore, by the t 26

n - number of rows in partial array with vt missing interactions

550 500 450 400 350 300 250 200 150 100 50 Randomized (Algorithm 4) LLL based 200 400 600 k 800 1000 1200

n - number of rows in partial array with vt missing interactions

2500

2000

1500

1000

500

Randomized (Algorithm 4) LLL based

0

0

0

500

1000

1500

2000 k

2500

3000

3500

4000

4500

(a) t = 3, v = 3.

(b) t = 4, v = 3.

Figure 12: Comparison of the size of the partial array constructed in the first stage. Compares the size of the partial array specified in Algorithm 4 in Section 3.1, and the size derived in Section 5.1.
v 1 = linearity of expectation, we can find one such array A that leaves at most e (v t - m) 1 - v t  m -1 interactions uncovered. Adding one row per uncovered interactions to A, we obtain a covering array with at most N rows, where log(em)  vt N= + -1 t  m log tv v -1 v t log 
vt v t -1

n

t

The value of N is minimized when m =

. Because m  v t , we obtain the desired bound.

When m = v t this recaptures the bound of Theorem 5. Figure 13 compares the LLL based two-stage bound from Theorem 7 to the standard two-stage bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. Although the LLL based two-stage bound is tighter than the LLL based Godbole et al. bound, even for quite large values of k the standard two-stage bound is tighter than the LLL based two-stage bound. In practical terms, this specific LLL based two-stage method does not look very promising, unless the parameters are quite large.

6

Conclusion and open problems

Many concrete algorithms within a two-stage framework for covering array construction have been introduced and evaluated. The two-stage approach extends the range of parameters for which competitive covering arrays can be constructed, each meeting an a priori guarantee on its size. Indeed as a consequence a number of best known covering arrays have been improved upon. Although each of the methods proposed has useful features, our experimental evaluation suggests that TS Rand, Greedy; 2,  and TS Rand, Den; 2. with   {Cyclic, Frobenius} realize a good trade-off between running time and size of the constructed covering array. Improvements in the bounds, or in the algorithms that realize them, are certainly of interest. We mention some specific directions. Establishing tighter bounds on the coloring based methods of Section 3.3 is a challenging problem. Either better estimates of the chromatic number of the incompatibility graph after a random first stage, or a first stage designed to limit the chromatic number, could lead to improvements in the bounds. In Section 5.1 and 5.2 we explored using a Moser-Tardos type algorithm for the first stage. Although this is useful for asymptotic bounds, practical improvements appear to be limited. Perhaps a different approach of 27

550 500 450 N - number of rows N - number of rows SLJ bound Godbole LLL-2-stage 2-stage 400 350 300 250 200 150 100

10000 9000 8000 7000 6000 5000 4000 3000 2000 1000 0

SLJ bound Godbole LLL-2-stage 2-stage

0

50

100

150

200 k

250

300

350

400

450

1000

2000

3000

4000 k

5000

6000

7000

8000

(a) t = 3, v = 3.

(b) t = 4, v = 4.

Figure 13: Comparison among the LLL based two-stage bound from Theorem 7, the standard two-stage bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. reducing the number of bad events to be avoided explicitly by the algorithm may lead to a better algorithm. A potential approach may look like following: "Bad" events would denote non-coverage of an interaction over a t-set of columns. We would select a set of column t-sets such that the dependency graph of the corresponding bad events have a bounded maximum degree (less than the original dependency graph). We would devise a Moser-Tardos type algorithm for covering all the interactions on our chosen column t-sets, and then apply the conditional LLL distribution to obtain an upper bound on the number of uncovered interactions. However, the difficulty lies in the fact that "all vertices have degree  " is a non-trivial, "hereditary" property for induced subgraphs, and for such properties finding a maximum induced subgraph with the property is an NP-hard optimization problem [17]. There is still hope for a randomized or "nibble" like strategy that may find a reasonably good induced subgraph with a bounded maximum degree. Further exploration of this idea seems to be a promising research avenue. In general, one could consider more than two stages. Establishing the benefit (or not) of having more than two stages is also an interesting open problem. Finally, the application of the methods developed to mixed covering arrays appears to provide useful techniques for higher strengths; this merits further study as well.

Acknowledgments
The research was supported in part by the National Science Foundation under Grant No. 1421058.

References
[1] A. N. Aldaco, C. J. Colbourn, and V. R. Syrotiuk. Locating arrays: A new experimental design for screening complex engineered systems. SIGOPS Oper. Syst. Rev., 49(1):31­40, Jan. 2015. [2] N. Alon and J. H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics and Optimization. John Wiley & Sons, Inc., Hoboken, NJ, third edition, 2008. With an appendix on the life and work of Paul Erd os. [3] Android. Android configuration class, 2016. http://developer.android.com/reference/android/content/ res/Configuration.html. [4] J. I. Brown. The complexity of generalized graph colorings. Discrete Appl. Math., 69(3):257­270, 1996. 28

[5] R. C. Bryce and C. J. Colbourn. The density algorithm for pairwise interaction testing. Software Testing, Verification, and Reliability, 17:159­182, 2007. [6] R. C. Bryce and C. J. Colbourn. A density-based greedy algorithm for higher strength covering arrays. Software Testing, Verification, and Reliability, 19:37­53, 2009. [7] R. C. Bryce and C. J. Colbourn. Expected time to detection of interaction faults. Journal of Combinatorial Mathematics and Combinatorial Computing, 86:87­110, 2013. [8] J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE Global Research Technical Report, 29:769­781, 2002. [9] M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength 3. Des. Codes Crypt., 16:235­242, 1999. [10] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG system: An approach to testing based on combinatorial design. IEEE Transactions on Software Engineering, 23:437­44, 1997. [11] M. B. Cohen. Designing test suites for software interaction testing. PhD thesis, The University of Auckland, Department of Computer Science, 2004. [12] C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121­167, 2004. [13] C. J. Colbourn. Covering array tables, 2005-2015. http://www.public.asu.edu/ccolbou/src/tabby. [14] C. J. Colbourn. Covering arrays and hash families. In Information Security and Related Combinatorics, NATO Peace and Information Security, pages 99­136. IOS Press, 2011. [15] C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal of Combinatorial Mathematics and Combinatorial Computing, 90:97­115, 2014. [16] R. Diestel. Graph Theory. Graduate Texts in Mathematics. Springer, fourth edition, 2010. [17] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman & Co., New York, NY, USA, 1979. [18] A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds and Poisson approximations. Combinatorics, Probability and Computing, 5:105­118, 1996. [19] B. Haeupler, B. Saha, and A. Srinivasan. New constructive aspects of the Lov´ asz local lemma. J. ACM, 58(6):Art. 28, 28, 2011. [20] A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal Arrays. Springer-Verlag, New York, 1999. [21] D. S. Johnson. Approximation algorithms for combinatorial problems. J. Comput. System Sci., 9:256­ 278, 1974. [22] G. O. H. Katona. Two applications (for search theory and truth functions) of Sperner type theorems. Periodica Math., 3:19­26, 1973. [23] D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math., 6:255­262, 1973. [24] D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC Press, 2013. [25] D. R. Kuhn and M. Reilly. An investigation of the applicability of design of experiments to software testing. In Proc. 27th Annual NASA Goddard/IEEE Software Engineering Workshop, pages 91­95, Los Alamitos, CA, 2002. IEEE.

29

[26] D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault interactions and implications for software testing. IEEE Trans. Software Engineering, 30:418­421, 2004. [27] L. Lov´ asz. On the ratio of optimal integral and fractional covers. Discrete Math., 13(4):383­390, 1975. [28] K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin. Des., 13:70­77, 2005. [29] R. A. Moser. A constructive proof of the Lov´ asz local lemma. In STOC'09--Proceedings of the 2009 ACM International Symposium on Theory of Computing, pages 343­350. ACM, New York, 2009. [30] R. A. Moser and G. Tardos. A constructive proof of the general Lov´ asz local lemma. J. ACM, 57(2):Art. 11, 15, 2010. [31] C. Nie and H. Leung. A survey of combinatorial testing. ACM Computing Surveys, 43(2):#11, 2011. [32] A. H. Ronneseth and C. J. Colbourn. Merging covering arrays and compressing multiple sequence alignments. Discrete Appl. Math., 157:2177­2190, 2009. [33] K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv e-prints. [34] G. Seroussi and N. H. Bshouty. Vector sets for exhaustive testing of logic circuits. IEEE Trans. Inform. Theory, 34:513­522, 1988. [35] S. K. Stein. Two combinatorial covering theorems. J. Combinatorial Theory Ser. A, 16:391­397, 1974. [36] J. Torres-Jimenez, H. Avila-George, and I. Izquierdo-Marquez. A two-stage algorithm for combinatorial testing. Optimization Letters, pages 1­13, 2016.

30

Two-stage algorithms for covering array construction
Kaushik Sarkar and Charles J. Colbourn School of Computing, Informatics, and Decision Systems Engineering Arizona State University, PO Box 878809 Tempe, Arizona, 85287-8809, U.S.A.

arXiv:1606.06730v1 [cs.DS] 21 Jun 2016

June 23, 2016
Abstract Modern software systems often consist of many different components, each with a number of options. Although unit tests may reveal faulty options for individual components, functionally correct components may interact in unforeseen ways to cause a fault. Covering arrays are used to test for interactions among components systematically. A two-stage framework, providing a number of concrete algorithms, is developed for the efficient construction of covering arrays. In the first stage, a time and memory efficient randomized algorithm covers most of the interactions. In the second stage, a more sophisticated search covers the remainder in relatively few tests. In this way, the storage limitations of the sophisticated search algorithms are avoided; hence the range of the number of components for which the algorithm can be applied is extended, without increasing the number of tests. Many of the framework instantiations can be tuned to optimize a memory-quality trade-off, so that fewer tests can be achieved using more memory. The algorithms developed outperform the currently best known methods when the number of components ranges from 20 to 60, the number of options for each ranges from 3 to 6, and t-way interactions are covered for t  {5, 6}. In some cases a reduction in the number of tests by more than 50% is achieved.

Keywords: Covering array, Software interaction testing, Combinatorial construction algorithm

1

Introduction

Real world software and engineered systems are composed of many different components, each with a number of options, that are required to work together in a variety of circumstances. Components are factors, and options for a component form the levels of its factor. Although each level for an individual factor can be tested in isolation, faults in deployed software can arise from interactions among levels of different factors. When an interaction involves levels of t different factors, it is a t-way interaction. Testing for faults caused by t-way interactions for every t is generally infeasible, as a result of a combinatorial explosion. However, empirical research on real world software systems indicates that testing all possible 2-way (or 3-way) interactions would detect 70% (or 90%) of all faults [25]. Moreover, testing all possible 6-way interactions is sufficient for detection of 100% of all faults in the systems examined in [25]. Testing all possible t-way interactions for some 2  t  6 is pseudo-exhaustive testing [24], and is accomplished with a combinatorial array known as a covering array. Formally, let N, t, k, and v be integers with k  t  2 and v  2. A covering array CA(N ; t, k, v ) is an N × k array A in which each entry is from a v -ary alphabet , and for every N × t sub-array B of A and every x  t , there is a row of B that equals x. Then t is the strength of the covering array, k is the number of factors, and v is the number of levels. When k is a positive integer, [k ] denotes the set {1, . . . , k }. A t-way interaction is {(ci , ai ) : 1  i  t, ci  [k ], ci = cj for i = j, and ai  }. So an interaction is an assignment of levels from  to t of the k t factors. It,k,v denotes the set of all k t v interactions for given t, k and v . An N × k array A covers the 1

interaction  = {(ci , ai ) : 1  i  t, ci  [k ], ci = cj for i = j, and ai  } if there is a row r in A such that A(r, ci ) = ai for 1  i  t. When there is no such row in A,  is not covered in A. Hence a CA(N ; t, k, v ) covers all interactions in It,k,v . Covering arrays are used extensively for interaction testing in complex engineered systems. To ensure that all possible combinations of options of t components function together correctly, one needs examine all possible t-way interactions. When the number of components is k , and the number of different options available for each component is v , each row of CA(N ; t, k, v ) represents a test case. The N test cases collectively test all t-way interactions. For this reason, covering arrays have been used in combinatorial interaction testing in varied fields like software and hardware engineering, design of composite materials, and biological networks [8, 24, 26, 32, 34]. The cost of testing is directly related to the number of test cases. Therefore, one is interested in covering arrays with the fewest rows. The smallest value of N for which CA(N ; t, k, v ) exists is denoted by CAN(t, k, v ). Efforts to determine or bound CAN(t, k, v ) have been extensive; see [12, 14, 24, 31] for example. Naturally one would prefer to determine CAN(t, k, v ) exactly. Katona [22] and Kleitman and Spencer [23] independently showed that for t = v = 2, the minimum number of rows N in a CA(N ; 2, k, 2) is the smallest N for which -1 . Exact determination of CAN(t, k, v ) for other values of t and v has remained open. However, k  NN 2 some progress has been made in determining upper bounds for CAN(t, k, v ) in the general case; for recent results, see [33]. For practical applications such bounds are often unhelpful, because one needs explicit covering arrays to use as test suites. Explicit constructions can be recursive, producing larger covering arrays using smaller ones as ingredients (see [14] for a survey), or direct. Direct methods for some specific cases arise from algebraic, geometric, or number-theoretic techniques; general direct methods are computational in nature. Indeed when k is relatively small, the best known results arise from computational techniques [13], and these are in turn essential for the successes of recursive methods. Unfortunately, the existing computational methods encounter difficulties as k increases, but is still within the range needed for practical applications. Typically such difficulties arise either as a result of storage or time limitations or by producing covering arrays that are too big to compete with those arising from simpler recursive methods. Cohen [11] discusses commercial software where the number of factors often exceeds 50. Aldaco et al. [1] analyze a complex engineered system having 75 factors, using a variant of covering arrays. Android [3] uses a Configuration class to describe the device configuration; there are 17 different configuration parameters with 3 - 20 different levels. In each of these cases, while existing techniques are effective when the strength is small, these moderately large values of k pose concerns for larger strengths. In this paper, we focus on situations in which every factor has the same number of levels. These cases have been most extensively studied, and hence provide a basis for making comparisons. In practice, however, often different components have different number of levels, which is captured by extending the notion of a covering array. A mixed covering array MCA(N ; t, k, (v1 , v2 , . . . , vk )) is an N × k array in which the ith column contains vi symbols for 1  i  k . When {i1 , . . . , it }  {1, . . . , k } is a set of t columns, in the N × t t subarray obtained by selecting columns i1 , . . . , it of the MCA, each of the j =1 vij distinct t-tuples appears as a row at least once. Although we examine the uniform case in which v1 = · · · = vk , the methods developed here can all be directly applied to mixed covering arrays as well. Inevitably, when k > max(t + 1, v + 2), a covering array must cover some interactions more than once, for if not they are orthogonal arrays [20]. Treating the rows of a covering array in a fixed order, each row covers some number of interactions not covered by any earlier row. For a variety of known constructions, the initial rows cover many new interactions, while the later ones cover very few [7]. Comparing this rate of coverage for a purely random method and for one of the sophisticated search techniques, one finds little difference in the initial rows, but very substantial differences in the final ones. This suggests strategies to build the covering array in stages, investing more effort as the number of remaining uncovered interactions declines. In this paper we propose a new algorithmic framework for covering array construction, the two-stage framework. In the first stage, a randomized row construction method builds a specified number of rows to cover all but at most a specified, small number of interactions. As we see later, by dint of being randomized

2

this uses very little memory. The second stage is a more sophisticated search that adds few rows to cover the remaining uncovered interactions. We choose search algorithms whose requirements depend on the number of interactions to be covered, to profit from the fact that few interactions remain. By mixing randomized and deterministic methods, we hope to retain the fast execution and small storage of the randomized methods, along with the accuracy of the deterministic search techniques. We introduce a number of algorithms within the two-stage framework. Some improve upon best known bounds on CAN(t, k, v ) (see [33]) in principle. But our focus is on the practical consequences: The twostage algorithms are indeed quite efficient for higher strength (t  {5, 6}) and moderate number of levels (v  {3, 4, 5, 6}), when the number of factors k is moderately high (approximately in the range of 20 - 80 depending on value of t and v ). In fact, for many combination of t, k and v values the two-stage algorithms beat the previously best known bounds. Torres-Jimenez et al. [36] explore a related two-stage strategy. In their first stage, an in-parameter-order greedy strategy (as used in ACTS [24]) adds a column to an existing array; in their second stage, simulated annealing is applied to cover the remaining interactions. They apply their methods when t = v = 3, when the storage and time requirements for both stages remain acceptable. In addition to the issues in handling larger strengths, their methods provide no a priori bound on the size of the resulting array. In contrast with their methods, ours provide a guarantee prior to execution with much more modest storage and time. The rest of the paper is organized as follows. Section 2 reviews algorithmic methods of covering array construction, specifically the randomized algorithm and the density algorithm. This section contrasts these two methods and points out their limitations. Then it gives an intuitive answer to the question of why a two stage based strategy might work and introduces the general two-stage framework. Section 3 introduces some specific two-stage algorithms. Section 3.1 analyzes and evaluates the na¨ ive strategy. Section 3.2 describes a two-stage algorithm that combines the randomized and the density algorithm. Section 3.3 introduces graph coloring based techniques in the second stage. Section 3.4 examines the effect of group action on the size of the constructed covering arrays. Section 4 compares the results of various two-stage algorithms with the presently best known sizes. In Section 5 we discuss the Lov´ asz local lemma (LLL) bounds on CAN(t, k, v ) and provide a Moser-Tardos type randomized algorithm for covering array construction that matches the bound. Although the bound was known [18], the proof was non-constructive, and a constructive algorithm to match this bound seems to be absent in the literature. We explore potentially better randomized algorithms for the first stage using LLL based techniques, We also obtain a two-stage bound that improves the LLL bound for CAN(t, k, v ). We conclude the paper in Section 6.

2

Algorithmic construction of covering arrays

Available algorithms for the construction of covering arrays are primarily heuristic in nature; indeed exact algorithms have succeeded for very few cases. Computationally intensive metaheuristic search methods such as simulated annealing, tabu search, constraint programming, and genetic algorithms have been employed when the strength is relatively small or the number of factors and levels is small. These methods have established many of the best known bounds on sizes of covering arrays [13], but for many problems of practical size their time and storage requirements are prohibitive. For larger problems, the best available methods are greedy. The IPO family of algorithms [24] repeatedly adds one column at a time, and then adds -1 new rows to ensure complete coverage. In this way, at any point in time, the status of v t k t-1 interactions may be stored. AETG [10] pioneered a different method, which greedily selects one row at a time to cover a large number of as-yet-uncovered interactions. They establish that if a row can be chosen that covers the maximum number, a good a priori bound on the size of the covering array can be computed. Unfortunately selecting the maximum is NP-hard, and even if one selects the maximum there is no guarantee that the covering array is the smallest possible [7], so AETG resorts to a good heuristic selection of the next row by examining the stored status of v t k t interactions. None of the methods so far mentioned therefore guarantee to reach an a priori bound. An extension of the AETG strategy, the density algorithm [5, 6, 15], stores additional statistics for all v t k t interactions in order to ensure the selection of a good next row, and hence guarantees to produce an array with at most the precomputed number of rows. Variants of the density 3

Algorithm 1: A randomized algorithm for covering array construction. Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) log (k t )+t log v 1 Set N := ; vt
log
v t -1

2 3

4 5 6 7 8 9 10 11 12

repeat Construct an N × k array A where each entry is chosen independently and uniformly at random from a v -ary alphabet; Set covered := true; for each interaction   It,k,v do if  is not covered in A then Set covered := false; break; end end until covered = true ; Output A;

algorithm have proved to be most effective for problems of moderately large size. For even larger problems, pure random approaches have been applied. To produce methods that provide a guarantee on size, it is natural to focus on the density algorithm in order to understand its strengths and weaknesses. To do this, we contrast it with a basic randomized algorithm. Algorithm 1 shows a simple randomized algorithm for covering array construction. The algorithm constructs an array of a particular size randomly and checks whether all the interactions are covered. It repeats until it finds an array that covers all the interactions. log (k t )+t log v A CA(N ; t, k, v ) with N = is guaranteed to exist: vt
log
v t -1

Theorem 1. [21, 27, 35] (Stein-Lov´ asz-Johnson (SLJ) bound): Let t, k, v be integers with k  t  2, and v  2. Then as k  , CAN(t, k, v )  log
k t

+ t log v
vt v t -1

log

In fact, the probability that the N × k array constructed in line 3 of Algorithm 1 is a valid covering array is high enough that the expected number of times the loop in line 2 is repeated is a small constant. An alternative strategy is to add rows one by one instead of constructing the full array at the outset. We start with an empty array, and whenever we add a new row we ensure that it covers at least the expected number of previously uncovered interactions for a randomly chosen row. The probability that an uncovered interaction is covered by a random row is 1/v t . If the number of uncovered interactions is u, then by linearity of expectation, the expected number of newly covered interactions in a randomly chosen row is uv -t . If each row added covers exactly this expected number, we obtain the same number of rows as the SLJ bound, realized in Algorithm 1. But because the actual number of newly covered interactions is always an integer, each added row covers at least uv -t interactions. This is especially helpful towards the end when the expected number is a small fraction. Algorithm 2 follows this strategy. Again the probability that a randomly chosen row covers at least the expected number of previously uncovered interactions is high enough that the expected number of times the row selection loop in line 6 of Algorithm 2 is repeated is bounded by a small constant. We can obtain an upper bound on the size produced by Algorithm 2 by assuming that each new row added covers exactly uv -t previously uncovered interactions. This bound is the discrete Stein-Lov´ asz4

Algorithm 2: A randomized algorithm for covering array construction using the discrete SLJ strategy. Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) 1 Let A be an empty array; k t 2 Initialize a table T indexed by all t v interactions, marking every interaction "uncovered" ; 3 while there is an interaction marked "uncovered" in T do 4 Let u be the number of interactions marked "uncovered" in T ; u 5 Set expectedCoverage := v t ; 6 repeat 7 Let r be a row of length k where each entry is chosen independently and uniformly at random from a v -ary alphabet; 8 Let coverage be the number of "uncovered" interactions in T that are covered in row r; 9 until coverage > expectedCoverage ; 10 Add r to A; 11 Mark all interactions covered by r as "covered" in T ; 12 end 13 Output A;

Johnson (discrete SLJ) bound. Figure 1 compares the sizes of covering arrays obtained from the SLJ and the discrete SLJ bounds for different values of k when t = 6 and v = 3. Consider a concrete example, when t = 5, k = 20, and v = 3. The SLJ bound guarantees the existence of a covering array with 12499 rows, whereas the discrete SLJ bound guarantees the existence of a covering array with only 8117 rows. The density algorithm replaces the loop at line 6 of Algorithm 2 by a conditional expectation derandomized method. For fixed v and t the density algorithm selects a row efficiently (time polynomial in k ) and deterministically that is guaranteed to cover at least uv -t previously uncovered interactions. In practice, for small values of k the density algorithm works quite well, often covering many more interactions than the minimum. Many of the currently best known CAN(t, k, v ) upper bounds are obtained by the density algorithm in combination with various post-optimization techniques [13]. However, the practical applicability of Algorithm 2 and the density algorithm is limited by the storage t of the table T , representing each of the k t v interactions. Even when t = 6, v = 3, and k = 54, there are 18,828,003,285 6-way interactions. This huge memory requirement renders the density algorithm impractical for rather small values of k when t  {5, 6} and v  3. We present an idea to circumvent this large requirement for memory, and develop it in full in Section 3.

2.1

Why does a two stage based strategy make sense?

Compare the two extremes, the density algorithm and Algorithm 1. On one hand, Algorithm 1 does not suffer from any substantial storage restriction, but appears to generate many more rows than the density algorithm. On the other hand, the density algorithm constructs fewer rows for small values of k , but becomes impractical when k is moderately large. One wants algorithms that behave like Algorithm 1 in terms of memory, but yield a number of rows competitive with the density algorithm. For t = 6, k = 16, and v = 3, Figure 2 compares the coverage profile for the density algorithm and Algorithm 1. We plot the number of newly covered interactions for each row in the density algorithm, and the expected number of newly covered interactions for each row for Algorithm 1. The qualitative features exhibited by this plot are representative of the rates of coverage for other parameters. Two key observations are suggested by Figure 2. First, the expected coverage in the initial random rows is similar to the rows chosen by the density algorithm. In this example, the partial arrays consisting of the first 1000 rows exhibit similar coverage, yet the randomized algorithm needed no extensive bookkeeping. Secondly, as later rows are added, the judicious selections of the density algorithm produce much larger coverage per row than Algorithm 1. Consequently it appears sensible to invest few computational resources 5

3.5

x 10

4

SLJ bound Discrete SLJ bound 3

N - number of rows

2.5

2

1.5

1

0.5 0

100

200

300

400

500 k

600

700

800

900

1000

Figure 1: Comparison of covering array sizes obtained from SLJ bound and discrete SLJ bound for different values of k , when t = 6 and v = 3.

9000 8000 Number of newly covered interactions 7000 6000 5000 4000 3000 2000 1000 0 Density Basic Random

0

2000

4000

6000 Row number

8000

10000

12000

Figure 2: For t = 6, k = 16 and v = 3, the actual number of newly covered interactions of the density algorithm and the expected number of newly covered interactions in a random array.

6

on the initial rows, while making more careful selections in the later ones. This forms the blueprint of our general two-stage algorithmic framework shown in Algorithm 3. Algorithm 3: The general two-stage framework for covering array construction. Input: t : strength of the required covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) 1 Choose a number n of rows and a number  of interactions; // First Stage 2 Use a randomized algorithm to construct an n × k array A ; 3 Ensure that A covers all but at most  interactions; 4 Make a list L of interactions that are not covered in A (L contains at most  interactions); // Second Stage 5 Use a deterministic procedure to add N - n rows to A to cover all the interactions in L; 6 Output A; A specific covering array construction algorithm results by specifying the randomized method in the first stage, the deterministic method in the second stage, and the computation of n and . Any such algorithm produces a covering array, but we wish to make selections so that the resulting algorithms are practical while still providing a guarantee on the size of the array. In Section 3 we describe different algorithms from the two-stage family, determine the size of the partial array to be constructed in the first stage, and establish upper bound guarantees. In Section 4 we explore how good the algorithms are in practice.

3

Two-stage framework
Rand MT the basic randomized algorithm the Moser-Tardos type algorithm

For the first stage we consider two methods:

We defer the development of method MT until Section 5. Method Rand uses a simple variant of Algorithm 1, choosing a random n × k array. For the second stage we consider four methods: Naive Greedy Den Col the the the the na¨ ive strategy, one row per uncovered interaction online greedy coloring strategy density algorithm graph coloring algorithm

Using these abbreviations, we adopt a uniform naming convention for the algorithms: TS A, B is the algorithm in which A is used in the first stage, and B is used in the second stage. For example, TS MT, Greedy denotes a two-stage algorithm where the first stage is a Moser-Tardos type randomized algorithm and the second stage is a greedy coloring algorithm. Later when the need arises we refine these algorithm names.

3.1

One row per uncovered interaction in the second stage (TS Rand, Naive )

In the second stage each of the uncovered interactions after the first stage is covered using a new row. Algorithm 4 describes the method in more detail. This simple strategy improves on the basic randomized strategy when n is chosen judiciously. For example, when t = 6, k = 54 and v = 3, Algorithm 1 constructs a covering array with 17, 236 rows. Figure 3 plots an upper bound on the size of the covering array against the number n of rows in the partial array.

7

Algorithm 4: Na¨ ive two-stage algorithm (TS Rand, Naive ). Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) vt log (k t )+t log v +log log v t -1 ; 1 Let n := t v
log
v t -1

2 3 4

Let  =

1 log
vt v t -1

;

5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

repeat Let A be an n × k array where each entry is chosen independently and uniformly at random from a v -ary alphabet; Let uncovNum := 0 and unCovList be an empty list of interactions; Set covered := true; for each interaction   It,k,v do if  is not covered in A then Set uncovNum :=uncovNum +1; Add  to unCovList ; if uncovNum >  then Set covered := false; break; end end end until covered= true ; for each interaction  uncovList do Add a row to A that covers ; end Output A;

8

1.75 1.7 1.65 1.6 1.55 1.5 1.45 1.4 1.35 1.3

x 10

4

Total number of rows in the Covering array

1

1.1 1.2 1.3 1.4 1.5 1.6 1.7 n -- number of rows in the partial array of the first stage

1.8 x 10
4

Figure 3: An upper bound on the size of the covering array against n, the size of the partial array constructed in the first stage when t = 6, k = 54, and v = 3, with one new row added per uncovered interaction in the second stage. The minimum size of 13, 162 is obtained when n = 12, 402. Algorithm 1 requires 17, 236 rows, and the currently best known covering array has 17, 197 rows.

9

The smallest covering array is obtained when n = 12, 402 which, when completed, yields a covering array with at most 13, 162 rows--a big improvement over Algorithm 1. A theorem from [33] tells us the optimal value of n in general: Theorem 2. [33] Let t, k, v be integers with k  t  2, and v  2. Then log CAN(t, k, v ) 
k t

+ t log v + log log log
vt v t -1
vt v t -1

vt v t -1

+1 .

The bound is obtained by setting n =
t

log (k t )+t log v +log log log
vt v t -1

. The expected number of uncovered

interactions is exactly  = 1/ log vtv-1 . Figure 4 compares SLJ, discrete SLJ and two-stage bounds for k  100, when t = 6 and v = 3. The two-stage bound does not deteriorate in comparison to discrete SLJ bound as k increases; it consistently takes only 307-309 more rows. Thus when k = 12 the two-stage bound requires only 6% more rows and when k = 100 only 2% more rows than the discrete SLJ bound.
x 10
4

2.2 2 1.8 N - number of rows 1.6 1.4 1.2 1 0.8 0.6

SLJ bound Discrete SLJ bound Two-stage bound

0.4 10

20

30

40

50 k

60

70

80

90

100

Figure 4: Comparison of covering array sizes obtained from SLJ bound, discrete SLJ bound and two-stage bound for k  100, when t = 6 and v = 3. In this range of k values the two-stage bound requires 307-309 more rows than the discrete SLJ bound, that is, 2-6% more rows. To ensure that the loop in line 7 of Algorithm 4 does not repeat too many times we need to know the probability with which a random n × k array leaves at most  interactions uncovered. Using Chebyshev's inequality and the second moment method developed in [2, Chapter 4], we next show that in a random n × k 1 n t array the number of uncovered interactions is almost always close to its expectation, i.e. k . t v 1 - vt Substituting the value of n from line 1, this expected value is equal to µ, as in line 2. Therefore, the probability that a random n × k array covers the desired number of interactions is constant, and the expected number of times the loop in line 7 is repeated is also a constant (around 2 in practice).

10

Because the theory of the second moment method is developed in considerable detail in [2], here we briefly m mention the relevant concepts and results. Suppose that X = i=1 Xi , where Xi is the indicator random variable for event Ai for 1  i  m. For indices i, j , we write i  j if i = j and the events Ai , Aj are not independent. Also suppose that X1 , . . . , Xm are symmetric, i.e. for every i = j there is a measure preserving mapping of the underlying probability space that sends event Ai to event Aj . Define  = j i Pr [Aj |Ai ]. Then by [2, Corollary 4.3.4]: Lemma 3. [2] If E[X ]   and  = o(E[X ]) then X  E[X ] almost always. In our case, Ai denotes the event that the ith interaction is not covered in a n × k array where each entry 1 n . Because is chosen independently and uniformly at random from a v -ary alphabet. Then Pr[Xi ] = 1 - v t k t 1 n t , and E [ X ]   as there are k v interactions in total, by linearity of expectation, E [ X ] = v 1 - t t vt k  . Distinct events Ai and Aj are independent if the ith and j th interactions share no column. Therefore, k  the event Ai is not independent of at most t t- j i Pr [Aj |Ai ]  j i 1  1 other events Aj . So  = k t t-1 = o(E[X ]) when v and t are constants. By Lemma 3, the number of uncovered interactions in a random n × k array is close to the expected number of uncovered interactions. This guarantees that Algorithm 4 is an efficient randomized algorithm for constructing covering arrays with a number of rows upper bounded by Theorem 2. In keeping with the general two-stage framework, Algorithm 4 does not store the coverage status of each interaction. We only need store the interactions that are uncovered in A, of which there are at most 1  =  v t . This quantity depends only on v and t and is independent of k , so is effectively a vt
log
v t -1

t constant that is much smaller than k t v , the storage requirement for the density algorithm. Hence the algorithm can be applied to a higher range of k values. Although Theorem 5 provides asymptotically tighter bounds than Theorem 2, in a range of k values that are relevant for practical application, Theorem 2 provides better results. Figure 5 compares the bounds on CAN(t, k, v ) with the currently best known results.
4 6

3

x 10

2.5 Best known Two-stage (simple) GSS bound 2

x 10

2.5

Best known Two-stage (simple) GSS bound

N - number of rows

N - number of rows 10 20 30 40 k 50 60 70 80 90

2

1.5

1.5

1

1

0.5

0.5

0 0

0

5

10

15

20

25 k

30

35

40

45

50

(a) t = 6, v = 3

(b) t = 6, v = 6

Figure 5: Comparison of GSS bound and two-stage bound with the currently best known results

3.2

The density algorithm in the second stage (TS Rand, Den )

Next we apply the density algorithm in the second stage. Figure 6 plots an upper bound on the size of the covering array against the size of the partial array constructed in the first stage when the density algorithm is used in the second stage, and compares it with TS Rand, Naive . The size of the covering array decreases 11

as n decreases. This is expected because with smaller partial arrays, more interactions remain for the second stage to be covered by the density algorithm. In fact if we cover all the interactions using the density algorithm (as when n = 0) we would get an even smaller covering array. However, our motivation was precisely to avoid doing that. Therefore, we need a "cut-off" for the first stage.
x 10
4

1.9

Total number of rows in the Covering array

1.8

Basic Two-stage Two-stage with density in second stage

1.7

1.6

1.5

1.4

1.3

1.2

1

1.1 1.2 1.3 1.4 1.5 1.6 1.7 n -- number of rows in the partial array of the first stage

1.8 x 10
4

Figure 6: Comparison of covering array sizes from two-stage algorithms with Den and Naive in the second stage. With Den there is no minimum point in the curve; the size of the covering array keeps decreasing as we leave more uncovered interactions for the second stage. We are presented with a trade-off. If we construct a smaller partial array in the first stage, we obtain a smaller covering array overall. But we then pay for more storage and computation time for the second stage. To appreciate the nature of this trade-off, look at Figure 7, which plots an upper bound on the covering array size and the number of uncovered interactions in the first stage against n. The improvement in the covering array size plateaus after a certain point. The three horizontal lines indicate  ( v t ), 2 and 3 uncovered interactions in the first stage. (In the na¨ ive method of Section 3.1, the partial array after the first stage leaves at most  uncovered interactions.) In Figure 7 the final covering array size appears to plateau when the number of uncovered interactions left by the first stage is around 2. After that we see diminishing returns -- the density algorithm needs to cover more interactions in return for a smaller improvement in the covering array size. Let r be the maximum number of interactions allowed to remain uncovered after the first stage. Then r can be specified in the two-stage algorithm. To accommodate this, we denote by TS A, B ; r the two-stage algorithm where A is the first stage strategy, B is the second stage strategy, and r is the maximum number of uncovered interactions after the first stage. For example, TS Rand, Den; 2 applies the basic randomized algorithm in the first stage to cover all but at most 2 interactions, and the density algorithm to cover the remaining interactions in the second stage.

3.3

Coloring in the second stage (TS Rand, Col and TS Rand, Greedy )

Now we describe strategies using graph coloring in the second stage. Construct a graph G = (V, E ), the incompatibility graph, in which V is the set of uncovered interactions, and there is an edge between two 12

18000 Number of rows / Number of uncovered interactions 16000 14000 12000 10000 8000 6000 4000 2000 0 0.8 Num. of rows in the completed CA Num. of uncovered interaction in first stage

0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 4 n -- number of rows in the partial array of the first stage x 10

Figure 7: Final covering array size against the number of uncovered interactions after the first stage. As the size n of the partial array decreases, the number of uncovered interactions in the first stage increases. Den is used in the second stage. From bottom to top, the green lines denote , 2, and 3 uncovered interactions. interactions exactly when they share a column in which they have different symbols. A single row can cover a set of interactions if and only if it forms an independent set in G. Hence the minimum number of rows required to cover all interactions of G is exactly its chromatic number (G), the minimum number of colors in a proper coloring of G. Graph coloring is an NP-hard problem, so we employ heuristics to bound the chromatic number. Moreover, G only has vertices for the uncovered interactions after the first stage, so is size is small relative to the total number of interactions. The expected number of edges in the incompatibility graph after choosing n rows uniformly at random n t k t t k -t 1 1 n t t-i is  = 2 ) 1- v . Using the elementary upper bound on 1 - (vt -1 t i=1 i t-i (v - v t v v t-i ) the chromatic number  
1 2

+

2m + 1 4 , where m is the number of edges [16, Chapter 5.2], we can surely

cover the remaining interactions with at most 1 2m + 1 2 + 4 rows. The actual number of edges m that remain after the first stage is a random variable with mean  . In principle, the first stage could be repeatedly applied until m   , so we call m =  the optimistic estimate. To ensure that the first stage is expected to be run a small constant number of times, we increase the estimate. With probability more than 1/2 the incompatibility graph has m  2 edges, so m = 2 is the conservative estimate. For t = 6, k = 56, and v = 3, Figure 8 shows the effect on the minimum number of rows when the bound on the chromatic number in the second stage is used, for the conservative or optimistic estimates. The Na¨ ive method is plotted for comparison. Better coloring bounds shift the minima leftward, reducing the number of rows produced in both stages. Thus far we have considered bounds on the chromatic number. Better estimation of (G) is complicated by the fact that we do not have much information about the structure of G until the first stage is run. In practice, however, G is known after the first stage and hence an algorithmic method to bound its chromatic number can be applied. Because the number of vertices in G equals the number of uncovered interactions

13

2.2

x 10

4

2

Conservative estimate Optimistic estimate Simple

Number of rows required

1.8

1.6

1.4

1.2

1

1

1.1

1.2

1.3

1.4 N

1.5

1.6

1.7

1.8 x 10
4

Figure 8: Size of the partial array vs. size of the complete CA. t = 6, k = 56, v = 3. Stein-Lov´ asz-Johnson bound requires 17, 403 rows, discrete Stein-Lov´ asz-Johnson bound requires 13, 021 rows. Simple estimate for the two stage algorithm is 13, 328 rows, conservative estimate assuming m = 2 is 12, 159 rows, and optimistic estimate assuming m =  is 11, 919 rows. Even the conservative estimate beats the discrete Stein-Lov´ asz-Johnson bound.

14

after the first stage, we encounter the same trade-off between time and storage, and final array size, as seen earlier for density. Hence we again parametrize by the expected number of uncovered interactions in the first stage. We employ two different greedy algorithms to color the incompatibility graph. In method Col we first construct the incompatibility graph G after the first stage. Then we apply the commonly used smallest last order heuristic to order the vertices for greedy coloring: At each stage, find a vertex vi of minimum degree in Gi , order the vertices of Gi - vi , and then place vi at the end. More precisely, we order the vertices of G as v1 , v2 , . . . , vn , such that vi is a vertex of minimum degree in Gi , where Gi = G - {vi+1 , . . . , vn }. A graph is d-degenerate if all of its subgraphs have a vertex with degree at most d. When G is d-degenerate but not (d - 1)-degenerate, the Coloring number col(G) is d + 1. If we then greedily color the vertices with the first available color, at most col(G) colors are used. In method Greedy we employ an on-line, greedy approach that colors the interactions as they are discovered in the first stage. In this way, the incompatibility graph is never constructed. We instead maintain a set of rows. Some entries in rows are fixed to a specific value; others are flexible to take on any value. Whenever a new interaction is found to be uncovered in the first stage, we check if any of the rows is compatible with this interaction. If such a row is found then entries in the row are fixed so that the row now covers the interaction. If no such row exists, a new row with exactly t fixed entries corresponding to the interaction is added to the set of rows. This method is much faster than method Col in practice.

3.4

Using group action

Covering arrays that are invariant under the action of a permutation group on their symbols can be easier to construct and are often smaller [15]. Direct and computational constructions using group actions are explored in [9, 28]. Sarkar et al. [33] establish the asymptotically tightest known bounds on CAN(t, k, v ) using group actions. In this section we explore the implications of group actions on two-stage algorithms. Let  be a permutation group on the set of symbols. The action of this group partitions the set of t-way interactions into orbits. We construct an array A such that for every orbit, at least one row covers an interaction from that orbit. Then we develop the rows of A over  to obtain a covering array that is invariant under the action of . Effort then focuses on covering all the orbits of t-way interactions, instead of the individual interactions. If  acts sharply transitively on the set of symbols (for example, if  is a cyclic group of order v ) then k t-1 t the action of  partitions k orbits of length v each. Following the lines of the t v interactions into t v v t-1 log (k t )+(t-1) log v +log log v t-1 -1 +1 that covers at proof of Theorem 2, there exists an n × k array with n = v t-1
log
v t-1 -1

least one interaction from each orbit. Therefore, log CAN(t, k, v )  v
k t

+ (t - 1) log v + log log log
v t- 1 v t- 1 - 1

v t-1 v t-1 -1

+1 . (1)

Similarly, we can employ a Frobenius group. When v is a prime power, the Frobenius group is the group of permutations of Fv of the form {x  ax + b : a, b  Fv , a = 0}. The action of the Frobenius group t-1 1 partitions the set of t-tuples on v symbols into v v-- 1 orbits of length v (v - 1) (full orbits) each and 1 orbit of length v (a short orbit). The short orbit consists of tuples of the form (x1 , . . . , xt ) where x1 = . . . = xt . Therefore, we can obtain a covering array by first constructing an array that covers all the full orbits, and then developing all the rows over the Frobenius group and adding v constant rows. Using the two stage strategy in conjunction with the Frobenius group action we obtain: log CAN(t, k, v )  v (v - 1)
k t

+ log

v t-1 -1 v -1

+ log log
v t- 1 v t-1 -v +1

v t-1 v t-1 -v +1

+1 + v. (2)

log

15

1.5

x 10

4

1.45

Two-stage (simple) Two-stage (cyclic group action) Two-stage (Frobenius group action)

N - number of rows

1.4

1.35

1.3

1.25 50

55

60 k

65

70

75

Figure 9: Comparison of the simple two-stage bound with the cyclic and the Frobenius two-stage bounds. t = 6, v = 3 and 50  k  75. Group action reduces the required number of rows slightly. Figure 9 compares the simple two-stage bound with the cyclic and Frobenius two-stage bounds. For t = 6, v = 3 and 12  k  100, the cyclic bound requires 7-21 (on average 16) fewer rows than the simple bound. In the same range the Frobenius bound requires 17 - 51 (on average 40) fewer rows. Group action can be applied in other methods for the second stage as well. Colbourn [15] incorporates group action into the density algorithm, allowing us to apply method Den in the second stage. Greedy extends easily to use group action, as we do not construct an explicit incompatibility graph. Whenever we fix entries in a row to cover an uncovered orbit, we commit to a specific orbit representative. However, applying group action to the incompatibility graph coloring for Col is more complicated. We need to modify the definition of the incompatibility graph for two reasons. First the vertices no longer represent uncovered interactions, but rather uncovered orbits of interaction. Secondly, and perhaps more importantly, pairwise compatibility between every two orbits in a set no longer implies mutual compatibility among all orbits in the set. One approach is to form a vertex for each uncovered orbit, placing an edge between two when they share a column. Rather than the usual coloring, however, one asks for a partition of the vertex set into classes so that every class induces an acyclic subgraph. Problems of this type are generalized graph coloring problems [4]. Within each class of such a vertex partition, consistent representatives of each orbit can be selected to form a row; when a cycle is present, this may not be possible. Unfortunately, heuristics for solving these types of problems appear to be weak, so we adopt a second approach. As we build the incompatibility graph, we commit to specific orbit representatives. When a vertex for an uncovered orbit is added, we check its compatibility with the orbit representatives chosen for the orbits already handled with which it shares columns; we commit to an orbit representative and add edges to those with which it is now incompatible. Once completed, we have a (standard) coloring problem for the resulting graph. Because group action can be applied using each of the methods for the two stages, we extend our naming to TS A, B ; r,  , where  can be Trivial (i.e. no group action), Cyclic, or Frobenius.

16

4

Computational results

Figure 5 indicates that even a simple two-stage bound can improve on best known covering array numbers. Therefore we investigate the actual performance of our two-stage algorithms for covering arrays of strength 5 and 6. First we present results for t = 6, when v  {3, 4, 5, 6} and no group action is assumed. Table 1 shows the results for different v values. In each case we select the range of k values where the two-stage bound predicts smaller covering arrays than the previously known best ones, setting the maximum number of uncovered t interactions as  = 1/ log vtv-1  v t . For each value of k we construct a single partial array and then run the different second stage algorithms on it consecutively. In this way all the second stage algorithms cover the same set of uncovered interactions. The column tab lists the best known CAN(t, k, v ) upper bounds from [13]. The column bound shows the upper bounds obtained from the two-stage bound (2). The columns na¨ ive, greedy, col and den show results obtained from running the TS Rand, Naive; , Trivial , TS Rand, Greedy; , Trivial , TS Rand, Col; , Trivial and TS Rand, Den; , Trivial algorithms, respectively. The na¨ ive method always finds a covering array that is smaller than the two-stage bound. This happens because we repeat the first stage of Algorithm 4 until the array has fewer than v t uncovered interactions. (If the first stage were not repeated, the algorithm still produce covering arrays that are not too far from the bound.) For v = 3 Greedy and Den have comparable performance. Method Col produces covering arrays that are smaller. However, for v  {4, 5, 6} Den and Col are competitive. Table 2 shows the results obtained by the different second stage algorithms when the maximum number of uncovered interactions in the first stage is set to 2 and 3 respectively. When more interactions are covered in the second stage, we obtain smaller arrays as expected. However, the improvement in size does not approach 50%. There is no clear winner. Next we investigate the covering arrays that are invariant under the action of a cyclic group. In Table 3 the column bound shows the upper bounds from Equation (1). The columns na¨ ive, greedy, col and den show results obtained from running TS Rand, Naive; , Cyclic , TS Rand, Greedy; , Cyclic , TS Rand, Col; , Cyclic and TS Rand, Den; , Cyclic , respectively. Table 4 presents results for cyclic group action based algorithms when the number of maximum uncovered interactions in the first stage is set to 2 and 3 respectively. For the Frobenius group action, we show results only for v  {3, 5} in Table 5. The column bound shows the upper bounds obtained from Equation (2). Table 6 presents results for Frobenius group action algorithms when the number of maximum uncovered interactions in the first stage is 2 or 3. Next we present a handful of results when t = 5. In the cases examined, using the trivial group action is too time consuming to be practical. However, the cyclic or Frobenius cases are feasible. Tables 7 and 8 compare two stage algorithms when the number of uncovered interactions in the first stage is at most 2. In almost all cases there is no clear winner among the three second stage methods. Methods Den and Greedy are, however, substantially faster and use less memory than method Col; for practical purposes they would be preferred. All code used in this experimentation is available from the github repository https://github.com/ksarkar/CoveringArray under an open source GPLv3 license.

5

Limited dependence and the Moser-Tardos algorithm

Here we explore a different randomized algorithm that produces smaller covering arrays than Algorithm 1. When k > 2t, there are interactions that share no column. The events of coverage of such interactions are independent. Moser et al. [29, 30] provide an efficient randomized construction method that exploits this

17

k 53 54 55 56 57 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27

tab 13021 14155 17161 19033 20185 68314 71386 86554 94042 99994 104794 233945 258845 281345 293845 306345 356045 506713 583823 653756 694048 783784 844834 985702 1035310 1112436 1146173 1184697

bound 13076 13162 13246 13329 13410 65520 66186 66834 67465 68081 68681 226700 229950 233080 236120 239050 241900 486310 505230 522940 539580 555280 570130 584240 597660 610460 622700 634430

na¨ ive greedy t = 6, v = 3 13056 12421 13160 12510 13192 12590 13304 12671 13395 12752 t = 6, v = 4 65452 61913 66125 62573 66740 63209 67408 63819 68064 64438 68556 65021 t = 6, v = 5 226503 213244 229829 216444 232929 219514 235933 222516 238981 225410 241831 228205 t = 6, v = 6 486302 449950 505197 468449 522596 485694 539532 502023 555254 517346 569934 531910 584194 545763 597152 558898 610389 571389 622589 583473 634139 594933

col 12415 12503 12581 12665 12748 61862 62826 63160 64077 64935 65739 212942 217479 219215 222242 226379 230202 448922 467206 484434 500788 516083 530728 544547 557917 570316 582333 593857

den 12423 12512 12591 12674 12757 61886 62835 63186 64082 64907 65703 212940 217326 219241 222244 226270 229942 447864 466438 483820 500194 515584 530242 548307 557316 569911 582028 593546

Table 1: Comparison of different TS Rand, -; , Trivial algorithms.

18

k greedy 53 54 55 56 57 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27 11968 12135 12286 12429 12562 59433 60090 60715 61330 61936 62530 204105 207243 210308 213267 216082 218884 425053 443236 460315 476456 491570 505966 519611 532612 544967 556821 568135

2 col 11958 12126 12129 12204 12290 59323 60479 61527 62488 61839 62899 203500 206659 209716 212675 215521 218314 -

den t = 6, v 11968 12050 12131 12218 12296 t = 6, v 59326 59976 60615 61242 61836 62428 t = 6, v 203302 206440 209554 212508 215389 218172 t = 6, v 420333 438754 455941 472198 487501 502009 515774 528868 541353 553377 564827

greedy =3 11716 11804 11877 11961 12044 =4 58095 58742 59369 59974 60575 61158 =5 199230 202342 205386 208285 211118 213872 =6 412275 430402 447198 463071 478269 492425 505980 518746 531042 542788 554052

3 col 11705 11787 11875 12055 12211 57951 58583 59867 61000 60407 61004 198361 201490 204548 -

den 11708 11790 11872 11950 12034 57888 58544 59187 59796 60393 60978 197889 201068 204107 207060 209936 212707 405093 423493 440532 456725 471946 486306 500038 513047 525536 537418 548781

Table 2: Comparison of TS Rand, -; 2, Trivial and TS Rand, -; 3, Trivial algorithms.

19

k 53 54 55 56 57 k 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27

tab 13021 14155 17161 19033 20185 tab 68314 71386 86554 94042 99994 104794 226000 244715 263145 235835 238705 256935 506713 583823 653756 694048 783784 844834 985702 1035310 1112436 1146173 1184697

bound 13059 13145 13229 13312 13393 bound 65498 66163 66811 67442 68057 68658 226680 229920 233050 236090 239020 241870 486290 505210 522910 539550 555250 570110 584210 597630 610430 622670 624400

na¨ ive greedy t = 6, v = 3 13053 12405 13119 12489 13209 12573 13284 12660 13368 12744 t = 6, v = 4 na¨ ive greedy 65452 61896 66080 62516 66740 63184 67408 63800 68032 64408 68556 64988 t = 6, v = 5 226000 213165 229695 216440 233015 219450 235835 222450 238705 225330 241470 228140 t = 6, v = 6 485616 449778 504546 468156 522258 485586 539280 501972 554082 517236 569706 531852 583716 545562 597378 558888 610026 571380 622290 583320 633294 594786

col 12405 12543 12663 12651 12744 col 61860 62820 63144 63780 64692 64964 212945 217585 221770 222300 225130 229235 448530 467232 490488 500880 521730 530832 549660 557790 575010 582546 598620

den 12411 12546 12663 12663 12750 den 61864 62784 63152 63784 64680 64976 212890 217270 221290 222210 225120 229020 447732 466326 488454 500172 519966 530178 548196 557280 573882 582030 597246

Table 3: Comparison of TS Rand, -; , Cyclic algorithms.

20

k greedy 53 54 55 56 57 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27 11958 12039 12120 12204 12276 59412 60040 60700 61320 61908 62512 204060 207165 207165 213225 216050 218835 424842 443118 460014 476328 491514 505884 519498 532368 544842 543684 568050

2 col 11955 12027 12183 12342 12474 59336 59996 61156 62196 63192 64096 203650 209110 209865 212830 217795 218480 422736 440922 457944 474252 489270 503580 517458 530340 542688 543684 566244

den t = 6, v 11958 12036 12195 12324 12450 t = 6, v 59304 59964 61032 61976 62852 63672 t = 6, v 203265 208225 209540 212510 217070 218155 t = 6, v 420252 438762 455994 472158 487500 501852 515718 528828 541332 543684 564756

greedy =3 11700 11790 11862 11949 12027 =4 58076 58716 59356 59932 60568 61152 =5 199180 202255 205380 208225 211080 213770 =6 411954 430506 447186 463062 478038 492372 505824 518700 530754 542664 553704

3 col 11691 11874 12057 11937 12021 57976 58616 59252 59840 61124 61048 198455 204495 204720 207790 213425 213185 409158 427638 456468 460164 486180 489336 502806 515754 538056 539922 560820

den 11694 11868 12027 11943 12024 57864 58520 59160 59760 60904 60988 197870 203250 204080 207025 212040 212695 405018 423468 449148 456630 479970 486264 500040 512940 532662 537396 555756

Table 4: Comparison of TS Rand, -; 2, Cyclic and TS Rand, -; 3, Cyclic algorithms.

21

k 53 54 55 56 57 31 32 33 34 35 36

tab 13021 14155 17161 19033 20185 233945 258845 281345 293845 306345 356045

bound 13034 13120 13203 13286 13366 226570 229820 232950 235980 238920 241760

na¨ ive greedy t = 6, v = 3 13029 12393 13071 12465 13179 12561 13245 12633 13365 12723 t = 6, v = 5 226425 213025 229585 216225 232725 219285 234905 222265 238185 225205 241525 227925

col 12387 12513 12549 12627 12717 212865 216085 219205 223445 227445 231145

den 12393 12531 12567 12639 12735 212865 216065 219145 223265 227065 230645

Table 5: Comparison of TS Rand, -; , Frobenius algorithms.

k greedy 53 54 55 56 57 70 75 80 85 90 31 32 33 34 35 36 50 55 60 65 11931 12021 12105 12171 12255 13167 13473 13773 14031 14289 203785 206965 209985 213005 215765 218605 250625 259785 268185 275785

2 col 11919 12087 12237 12171 12249 13155 13473 13767 14025 14283 203485 208965 209645 214825 215545 218285 250365 259625 268025 275665

den greedy t = 6, v = 3 11931 11700 12087 11790 12231 11862 12183 11949 12255 12027 13179 13479 13779 14037 14301 t = 6, v = 5 203225 198945 208065 201845 209405 205045 214145 208065 215265 210705 218025 213525 250325 259565 267945 275665 -

3 col 11691 11874 12057 11937 12021 198445 204505 209845 207545 210365 213105 -

den 11694 11868 12027 11943 12024 197825 203105 207865 206985 209885 212645 -

Table 6: Comparison of TS Rand, -; 2, Frobenius and TS Rand, -; 3, Frobenius algorithms.

22

k 67 68 69 70 71

tab 59110 60991 60991 60991 60991

greedy 48325 48565 48765 49005 49245

col 48285 48565 49005 48985 49205

den 48305 48585 48985 49025 49245

Table 7: Comparison of TS Rand, -; 2, Frobenius algorithms. t = 5, v = 5 k 49 50 51 52 53 tab 122718 125520 128637 135745 137713 greedy 108210 109014 109734 110556 111306 col 108072 108894 110394 110436 111180 den 107988 108822 110166 110364 111120

Table 8: Comparison of TS Rand, -; 2, Cyclic algorithms. t = 5, v = 6 limited dependence. Specializing their method to covering arrays, we obtain Algorithm 5. For the specified value of N in the algorithm it is guaranteed that the expected number of times the loop in line 3 of Algorithm 5 is repeated is linearly bounded in k (See Theorem 1.2 of [30]). The upper bound on CAN(t, k, v ) guaranteed by Algorithm 5 is obtained by applying the Lov´ asz local lemma (LLL). Lemma 4. (Lov´ asz local lemma; symmetric case) (see [2]) Let A1 , A2 , . . . , An events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at ¯ most d, and that Pr[Ai ]  p for all 1  i  n. If ep(d + 1)  1, then Pr[n i=1 Ai ] > 0. The symmetric version of Lov´ asz local lemma provides an upper bound on the probability of a "bad" event in terms of the maximum degree of a bad event in a dependence graph, so that the probability that all the bad events are avoided is non zero. Godbole, Skipper, and Sunley [18] apply Lemma 4 essentially to obtain the bound on CAN(t, k, v ) in line 1 of Algorithm 5. Theorem 5. [18] Let t, v and k  2t be integers with t, v  2. Then log CAN (t, k, v ) 
k t

-

k-t t

+ t log v + 1

log

vt v t -1

The bound on the size of covering arrays obtained from Theorem 5 is asymptotically tighter than the one obtained from Theorem 1. Figure 10 compares the bounds for t = 6 and v = 3. The original proof of LLL is essentially non-constructive and does not immediately lead to a polynomial time construction algorithm for covering arrays satisfying the bound of Theorem 5. Indeed no previous construction algorithms appear to be based on it. However the Moser-Tardos method of Algorithm 5 does provide a construction algorithm running in expected polynomial time. For sufficiently large values of k Algorithm 5 produces smaller covering arrays than the Algorithm 1. But the question remains: Does Algorithm 5 produce smaller covering arrays than the currently best known results within the range that it can be effectively computed? Perhaps surprisingly, we show that the answer is affirmative. In Algorithm 5 we do not need to store the coverage information of individual interactions in memory because each time an uncovered interaction is encountered we re-sample the columns involved in that interaction and start the check afresh (checking the coverage in interactions in the same order each time). Consequently, Algorithm 5 can be applied for larger values of k than the density algorithm. 23

Algorithm 5: Moser-Tardos type algorithm for covering array construction. Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) k -t log{(k t )-( t )}+t. log v +1 ; 1 Let N := vt
log
v t -1

2

3 4 5 6 7 8 9 10 11 12 13

14 15 16

Construct an N × k array A where each entry is chosen independently and uniformly at random from a v -ary alphabet; repeat Set covered := true; for each interaction   It,k,v do if  is not covered in A then Set covered := false; Set missing-interaction := ; break; end end if covered = false then Choose all the entries in the t columns involved in missing-interaction independently and uniformly at random from the v -ary alphabet; end until covered = true ; Output A;

Smaller covering arrays can be obtained by exploiting a group action using LLL, as shown in [33]. Table 9 shows the sizes of the covering arrays constructed by a variant of Algorithm 5 that employs cyclic and Frobenius group actions. While this single stage algorithm produces smaller arrays than the currently best known [13], these are already superseded by the two-stage based algorithms. k 56 57 58 59 60 tab 19033 20185 23299 23563 23563 MT 16281 16353 16425 16491 16557 k 44 45 46 47 48 tab 411373 417581 417581 423523 423523 MT 358125 360125 362065 363965 365805 k 25 26 27 28 29 tab 1006326 1040063 1082766 1105985 1149037 MT 1020630 1032030 1042902 1053306 1063272

(a) Frobenius. t = 6, v = 3

(b) Frobenius. t = 6, v = 5

(c) Cyclic. t = 6, v = 6

Table 9: Comparison of covering array size from Algorithm 5 (MT) with the best known results [13] (tab).

5.1

Moser-Tardos type algorithm for the first stage

The linearity of expectation arguments used in the SLJ bounds permit one to consider situations in which a few of the "bad" events are allowed to occur, a fact that we exploited in the first stage of the algorithms thus far. However, the Lov´ asz local lemma does not address this situation directly. The conditional Lov´ asz local lemma (LLL) distribution, introduced in [19], is a very useful tool. Lemma 6. (Conditional LLL distribution; symmetric case) (see [2, 33]) Let A = {A1 , A2 , . . . , Al } be a set of l events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at most d, and that Pr[Ai ]  p for all 1  i  l. Also suppose that ep(d +1)  1 ¯ (Therefore, by LLL (Lemma 4) Pr[l / A be another event in the same probability space i=1 Ai ] > 0). Let B  24

10

5

SLJ bound GSS bound

N - number of rows

10

4

10 1 10

3

10

2

10 k

3

10

4

10

5

Figure 10: Comparison of SLJ (Theorem 1) and GSS (Theorem 5) bounds for t = 6 and v = 3. The graph is plotted in log-log scale to highlight the asymptotic difference between the two bounds. with Pr[B ]  q , such that B is also mutually independent of a set of all other events Aj  A except for at ¯ most d. Then Pr[B | l i=1 Ai ]  eq . We apply the conditional LLL distribution to obtain an upper bound on the size of partial array that t leaves at most log vtv-1  v t interactions uncovered. For a positive integer k , let I = {j1 , . . . , j }  [k ] where j1 < . . . < j . Let A be an n × k array where each entry is from the set [v ]. Let AI denote the n ×  array in which AI (i, ) = A(i, j ) for 1  i  N and 1   ; AI is the projection of A onto the columns in I . ] Let M  [v ]t be a set of m t-tuples of symbols, and C  [k be a set of t columns. Suppose the t entries in the array A are chosen independently from [v ] with uniform probability. Let BC denote the event that at least one of the tuples in M is not covered in AC . There are  = k t such events, and for all of 1 n them Pr[BC ]  m 1 - v . Moreover, when k  2t, each of the events is mutually independent of all t k k -t other events except for at most  = k - 1 < t t- asz local lemma, when t - t 1 . Therefore, by the Lov´ 1 n em 1 - vt  1, none of the events BC occur. Solving for n, when n log(em) log
vt v t -1

(3)

] there exists an n × k array A over [v ] such that for all C  [k t , AC covers all the m tuples in M . In fact we can use a Moser-Tardos type algorithm to construct such an array. Let  be an interaction whose t-tuple of symbols is not in M . Then the probability that  is not covered 1 n in an n × k array is at most 1 - v when each entry of the array is chosen independently from [v ] with t uniform probability. Therefore, by the conditional LLL distribution the probability that  is not covered ] 1 n in the array A where for all C  [k . Moreover, t , AC covers all the m tuples in M is at most e 1 - v t there are  (v t - m) such interactions . By the linearity of expectation, the expected number of uncovered

25

interactions in A is less than v t when  (v t - m)e 1 - n

1 n vt

 v t . Solving for n, we obtain
m vt

log e 1 - log
vt v t -1

.

(4)

Therefore, there exists an n × k array with n = max

log(em) log
vt v t -1

,

m log{e(1- v t )}

log

vt v t -1

that has at most v t

uncovered interactions. To compute n explicitly, we must choose m. We can select a value of m to minimize n graphically for given values of t, k and v . For example, Figure 11 plots Equations 3 and 4 against m for t = 3, k = 350, v = 3, and finds the minimum value of n.
460 Equation (3) Equation (4) n - number of rows in the partial array n - number of rows in the partial array 440 440 445 max(Equation (3), Equation (4))

420

435

400

430

380

360

425

340 0

5

10

15 m

20

25

30

420 0

5

10

15 m

20

25

30

(a) Equations 3 and 4 against m.

(b) Maximum of the two sizes against m.

Figure 11: The minimum is at n = 422, when m = 16. t = 3, k = 350, v = 3 We compare the size of the partial array from the na¨ ive two-stage method (Algorithm 4) with the size obtained by the graphical methods in Figure 12. The Lov´ asz local lemma based method is asymptotically better than the simple randomized method. However, except for the small values of t and v , in the range of k values relevant for practical applications the simple randomized algorithm requires fewer rows than the Lov´ asz local lemma based method.

5.2

Lov´ asz local lemma based two-stage bound

We can apply the techniques from Section 5.1 to obtain a two-stage bound similar to Theorem 2 using the Lov´ asz local lemma and conditional LLL distribution. First we extend a result from [33]. Theorem 7. Let t, k, v be integers with k  t  2, v  2 and let  =
v log 
t vt v t -1

k t

, and  =

k t

-

k -t t

. If

 v t Then log CAN(t, k, v ) 
k t

+ t log v + log log log
vt v t -1

vt v t -1

+2

 - . 

Proof. Let M  [v ]t be a set of m t-tuples of symbols. Following the arguments of Section 5.1, when ) ] n  log(em there exists an n × k array A over [v ] such that for all C  [k t , AC covers all m tuples in M . vt
log
v t -1

At most  (v t - m) interactions are uncovered in such an array. Using the conditional LLL distribution, 1 n the probability that one such interaction is not covered in A is at most e 1 - v . Therefore, by the t 26

n - number of rows in partial array with vt missing interactions

550 500 450 400 350 300 250 200 150 100 50 Randomized (Algorithm 4) LLL based 200 400 600 k 800 1000 1200

n - number of rows in partial array with vt missing interactions

2500

2000

1500

1000

500

Randomized (Algorithm 4) LLL based

0

0

0

500

1000

1500

2000 k

2500

3000

3500

4000

4500

(a) t = 3, v = 3.

(b) t = 4, v = 3.

Figure 12: Comparison of the size of the partial array constructed in the first stage. Compares the size of the partial array specified in Algorithm 4 in Section 3.1, and the size derived in Section 5.1.
v 1 = linearity of expectation, we can find one such array A that leaves at most e (v t - m) 1 - v t  m -1 interactions uncovered. Adding one row per uncovered interactions to A, we obtain a covering array with at most N rows, where log(em)  vt N= + -1 t  m log tv v -1 v t log 
vt v t -1

n

t

The value of N is minimized when m =

. Because m  v t , we obtain the desired bound.

When m = v t this recaptures the bound of Theorem 5. Figure 13 compares the LLL based two-stage bound from Theorem 7 to the standard two-stage bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. Although the LLL based two-stage bound is tighter than the LLL based Godbole et al. bound, even for quite large values of k the standard two-stage bound is tighter than the LLL based two-stage bound. In practical terms, this specific LLL based two-stage method does not look very promising, unless the parameters are quite large.

6

Conclusion and open problems

Many concrete algorithms within a two-stage framework for covering array construction have been introduced and evaluated. The two-stage approach extends the range of parameters for which competitive covering arrays can be constructed, each meeting an a priori guarantee on its size. Indeed as a consequence a number of best known covering arrays have been improved upon. Although each of the methods proposed has useful features, our experimental evaluation suggests that TS Rand, Greedy; 2,  and TS Rand, Den; 2. with   {Cyclic, Frobenius} realize a good trade-off between running time and size of the constructed covering array. Improvements in the bounds, or in the algorithms that realize them, are certainly of interest. We mention some specific directions. Establishing tighter bounds on the coloring based methods of Section 3.3 is a challenging problem. Either better estimates of the chromatic number of the incompatibility graph after a random first stage, or a first stage designed to limit the chromatic number, could lead to improvements in the bounds. In Section 5.1 and 5.2 we explored using a Moser-Tardos type algorithm for the first stage. Although this is useful for asymptotic bounds, practical improvements appear to be limited. Perhaps a different approach of 27

550 500 450 N - number of rows N - number of rows SLJ bound Godbole LLL-2-stage 2-stage 400 350 300 250 200 150 100

10000 9000 8000 7000 6000 5000 4000 3000 2000 1000 0

SLJ bound Godbole LLL-2-stage 2-stage

0

50

100

150

200 k

250

300

350

400

450

1000

2000

3000

4000 k

5000

6000

7000

8000

(a) t = 3, v = 3.

(b) t = 4, v = 4.

Figure 13: Comparison among the LLL based two-stage bound from Theorem 7, the standard two-stage bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. reducing the number of bad events to be avoided explicitly by the algorithm may lead to a better algorithm. A potential approach may look like following: "Bad" events would denote non-coverage of an interaction over a t-set of columns. We would select a set of column t-sets such that the dependency graph of the corresponding bad events have a bounded maximum degree (less than the original dependency graph). We would devise a Moser-Tardos type algorithm for covering all the interactions on our chosen column t-sets, and then apply the conditional LLL distribution to obtain an upper bound on the number of uncovered interactions. However, the difficulty lies in the fact that "all vertices have degree  " is a non-trivial, "hereditary" property for induced subgraphs, and for such properties finding a maximum induced subgraph with the property is an NP-hard optimization problem [17]. There is still hope for a randomized or "nibble" like strategy that may find a reasonably good induced subgraph with a bounded maximum degree. Further exploration of this idea seems to be a promising research avenue. In general, one could consider more than two stages. Establishing the benefit (or not) of having more than two stages is also an interesting open problem. Finally, the application of the methods developed to mixed covering arrays appears to provide useful techniques for higher strengths; this merits further study as well.

Acknowledgments
The research was supported in part by the National Science Foundation under Grant No. 1421058.

References
[1] A. N. Aldaco, C. J. Colbourn, and V. R. Syrotiuk. Locating arrays: A new experimental design for screening complex engineered systems. SIGOPS Oper. Syst. Rev., 49(1):31­40, Jan. 2015. [2] N. Alon and J. H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics and Optimization. John Wiley & Sons, Inc., Hoboken, NJ, third edition, 2008. With an appendix on the life and work of Paul Erd os. [3] Android. Android configuration class, 2016. http://developer.android.com/reference/android/content/ res/Configuration.html. [4] J. I. Brown. The complexity of generalized graph colorings. Discrete Appl. Math., 69(3):257­270, 1996. 28

[5] R. C. Bryce and C. J. Colbourn. The density algorithm for pairwise interaction testing. Software Testing, Verification, and Reliability, 17:159­182, 2007. [6] R. C. Bryce and C. J. Colbourn. A density-based greedy algorithm for higher strength covering arrays. Software Testing, Verification, and Reliability, 19:37­53, 2009. [7] R. C. Bryce and C. J. Colbourn. Expected time to detection of interaction faults. Journal of Combinatorial Mathematics and Combinatorial Computing, 86:87­110, 2013. [8] J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE Global Research Technical Report, 29:769­781, 2002. [9] M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength 3. Des. Codes Crypt., 16:235­242, 1999. [10] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG system: An approach to testing based on combinatorial design. IEEE Transactions on Software Engineering, 23:437­44, 1997. [11] M. B. Cohen. Designing test suites for software interaction testing. PhD thesis, The University of Auckland, Department of Computer Science, 2004. [12] C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121­167, 2004. [13] C. J. Colbourn. Covering array tables, 2005-2015. http://www.public.asu.edu/ccolbou/src/tabby. [14] C. J. Colbourn. Covering arrays and hash families. In Information Security and Related Combinatorics, NATO Peace and Information Security, pages 99­136. IOS Press, 2011. [15] C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal of Combinatorial Mathematics and Combinatorial Computing, 90:97­115, 2014. [16] R. Diestel. Graph Theory. Graduate Texts in Mathematics. Springer, fourth edition, 2010. [17] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman & Co., New York, NY, USA, 1979. [18] A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds and Poisson approximations. Combinatorics, Probability and Computing, 5:105­118, 1996. [19] B. Haeupler, B. Saha, and A. Srinivasan. New constructive aspects of the Lov´ asz local lemma. J. ACM, 58(6):Art. 28, 28, 2011. [20] A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal Arrays. Springer-Verlag, New York, 1999. [21] D. S. Johnson. Approximation algorithms for combinatorial problems. J. Comput. System Sci., 9:256­ 278, 1974. [22] G. O. H. Katona. Two applications (for search theory and truth functions) of Sperner type theorems. Periodica Math., 3:19­26, 1973. [23] D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math., 6:255­262, 1973. [24] D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC Press, 2013. [25] D. R. Kuhn and M. Reilly. An investigation of the applicability of design of experiments to software testing. In Proc. 27th Annual NASA Goddard/IEEE Software Engineering Workshop, pages 91­95, Los Alamitos, CA, 2002. IEEE.

29

[26] D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault interactions and implications for software testing. IEEE Trans. Software Engineering, 30:418­421, 2004. [27] L. Lov´ asz. On the ratio of optimal integral and fractional covers. Discrete Math., 13(4):383­390, 1975. [28] K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin. Des., 13:70­77, 2005. [29] R. A. Moser. A constructive proof of the Lov´ asz local lemma. In STOC'09--Proceedings of the 2009 ACM International Symposium on Theory of Computing, pages 343­350. ACM, New York, 2009. [30] R. A. Moser and G. Tardos. A constructive proof of the general Lov´ asz local lemma. J. ACM, 57(2):Art. 11, 15, 2010. [31] C. Nie and H. Leung. A survey of combinatorial testing. ACM Computing Surveys, 43(2):#11, 2011. [32] A. H. Ronneseth and C. J. Colbourn. Merging covering arrays and compressing multiple sequence alignments. Discrete Appl. Math., 157:2177­2190, 2009. [33] K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv e-prints. [34] G. Seroussi and N. H. Bshouty. Vector sets for exhaustive testing of logic circuits. IEEE Trans. Inform. Theory, 34:513­522, 1988. [35] S. K. Stein. Two combinatorial covering theorems. J. Combinatorial Theory Ser. A, 16:391­397, 1974. [36] J. Torres-Jimenez, H. Avila-George, and I. Izquierdo-Marquez. A two-stage algorithm for combinatorial testing. Optimization Letters, pages 1­13, 2016.

30

Des. Codes Cryptogr. (2015) 77:479­491 DOI 10.1007/s10623-015-0084-4

Optimal low-power coding for error correction and crosstalk avoidance in on-chip data buses
Yeow Meng Chee1 · Charles J. Colbourn2 · Alan Chi Hung Ling3 · Hui Zhang1 · Xiande Zhang1

Received: 30 October 2014 / Revised: 12 April 2015 / Accepted: 16 April 2015 / Published online: 8 May 2015 © Springer Science+Business Media New York 2015

Abstract Coupled switched capacitance causes crosstalk in ultra deep submicron/nanometer VLSI fabrication, which leads to power dissipation, delay faults, and logical malfunctions. We present the first memoryless transition bus-encoding technique for power minimization, errorcorrection, and elimination of crosstalk simultaneously. To accomplish this, we generalize balanced sampling plans avoiding adjacent units, which are widely used in the statistical design of experiments. Optimal or asymptotically optimal constant weight codes eliminating each kind of crosstalk are constructed. Keywords Constant weight codes · Packing sampling plan avoiding adjacent units · Crosstalk avoidance · Low power code · Packing by triples · Balanced sampling plan Mathematics Subject Classification 94B25 · 05B40 · 05B07 · 62K10

This is one of several papers published in Designs, Codes and Cryptography comprising the "Special Issue on Cryptography, Codes, Designs and Finite Fields: In Memory of Scott A. Vanstone".

B

Charles J. Colbourn charles.colbourn@asu.edu Yeow Meng Chee ymchee@ntu.edu.sg Alan Chi Hung Ling aling@emba.uvm.edu Hui Zhang huizhang@ntu.edu.sg Xiande Zhang xiandezhang@ntu.edu.sg

1

Division of Mathematical Sciences, School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore 637371, Singapore School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287, USA Department of Computer Science, University of Vermont, Burlington, VT 05405, USA

2

3

123

480

Y. M. Chee et al.

1 Introduction
The ever-decreasing feature size of VLSI fabrication process has led to many challenges in VLSI circuit design. One of the most important issues concerns the characteristics of onchip wires [11]. The wires' cross-sectional areas and spacings have fallen dramatically with the move into the ultra deep submicron/nanometer (UDSM) regime. This has increased the resistance and capacitance of wires. To help reduce resistance, wires today are taller than they are wide, and they are poised to grow even taller as technology continues to scale. The resulting growth of side-to-side capacitance between long parallel wires causes coupled switch capacitance to dominate the wire-to-substrate capacitance in UDSM circuits by several orders of magnitude [21]. Coupled switched capacitance in turn leads to crosstalks, which result in power dissipation, delay faults, and logical malfunctions. The problem of eliminating or minimising crosstalks is considered the biggest signal integrity challenge for long on-chip buses implemented in UDSM CMOS technology [12]. The worst crosstalk couplings have been classified into four types [6,12], as described in Table 1. The coupled switched capacitance resulting from type-1, -2, -3, and -4 crosstalks is in the ratio of 1:2:3:4. Hence, it is particularly important to avoid crosstalks of higher types. Type-1 crosstalks cannot be avoided in any useful communication channel. However, type-1 crosstalks give rise to power dissipation and must be limited, because low power is a critical design objective in recent years. Another factor that has emerged as a new challenge for VLSI circuit designers is UDSM noise, caused by high-leakage transistors, power-grid fluctuations, ground bounce, IR drops, clock jitter, and electromagnetic radiation. The effects of such noise are difficult to predict or prevent. For example, noise in radiation-hardened circuits for satellite communication systems is random and does not correlate with particular switching patterns on the buses. A further source of faults is manufacturing defects. In nanotechnology, circuits are manufactured with a significant proportion of faults, and occasional errors may be unavoidable. Hence, preventive techniques are insufficient, and active error correction is required. Various researchers have proposed coding techniques to encode data on a bus for crosstalk avoidance [6,17,28], for low power dissipation [3,15,19,22,26], and for error correction [1,8]. Coding schemes that simultaneously satisfy two of these three criteria have also been investigated: · crosstalk avoidance and low power dissipation [12,27]; · crosstalk avoidance and error correction [14]; and · low power dissipation and error correction [2,16,18].

Table 1 Types of worst crosstalk couplings Type-1 Type-2 001  110 011  100 Center wire in opposite transition to an adjacent wire. The other wire in same transition as center wire Type-3 001  010 010  100 011  101 101  110 Center wire in opposite transition to an adjacent wire. The other wire maintains previous state Type-4

0  1 Single wire undergoes transition. Adjacent wires maintain previous states

010  101 All three adjacent wires undergo opposite transitions

123

Optimal low-power coding for crosstalk avoidance

481

Despite many efforts, the only families of optimal codes known are those for low power dissipation [3]. Many of the results on the comparative performance of existing codes are based on simulations rather than rigorous mathematical analysis. In this paper, we begin the study of codes for UDSM buses that simultaneously provide for low power dissipation, crosstalk avoidance, and error correction. In particular, we exhibit the first infinite families of such codes that are provably optimal. The paper is organized as follows. Section 2 establishes necessary terminology and gives a mathematical formulation of the problem of designing low-power codes that avoid crosstalks and correct errors. In Sect. 3, we present the relation of codes of each type with packing sampling plans avoiding adjacent units. In Sect. 4, we focus on optimal solutions for k = 3 for all positive integer n . In Sect. 5, the sizes of optimal codes of all types with small lengths are determined by computer search, and brief conclusion is given.

2 Background
2.1 Coding framework
A coding framework for data buses was introduced by Ramprasad et al. [15]. A bus interconnecting two embedded systems on a systems-on-chip (SoC) platform can be modelled generically as in Fig. 1. The source encoder (decoder) compresses (decompresses) the input data so that the number of bits required in the representation of the source is minimised. While the source encoder removes redundancy, the channel encoder adds redundancy to combat errors that may arise due to noise in the bus. Ramprasad et al. [15] considered various combinations of source-channel encoder-decoder pairs and presented simulation results for their power dissipation. Their approach is what is known as joint source-channel coding in the information theory literature. Shannon's information separation theorem [20] states that reliable transmission can be accomplished by separate source and channel coding, where the source encoder and decoder need not take into account the channel statistics and the channel encoder and decoder need not take into account the source statistics. This applies, however, only for point-to-point transmissions and for infinite sequence length. The first condition (point-to-point transmission) holds for a UDSM bus but the second requirement for infinite sequence length is clearly undesirable for bus coding, because it could give rise to circuits of unbounded delay. Moreover, joint source-channel coding is useful only when we know the statistics of the source and channel. In the absence of such statistics, one can only fall back on optimising the source and channel separately. Indeed, Ramprasad et al. [15] considered coding schemes and simulations on certain source data with better understood statistics (for example, pop music, classical music, video, and speech).

noisy channel
source encoder channel encoder channel decoder source decoder

transmitter
Fig. 1 Framework for systems-on-chip

receiver

123

482

Y. M. Chee et al.

In many systems, the behaviour of source data is hard to predict and so the joint sourcechannel coding approach loses its power. Many researchers have therefore fallen back on addressing the source coding and channel coding problems separately. This is also the approach taken in this paper. We focus on designing optimal channel coding schemes for the scenario where the source statistics are unknown.

2.2 Codes
The Hamming n -space is the set H(n ) = {0, 1}n , endowed with the (Hamming) distance dH (·, ·) defined as follows: for u, v  H(n ), dH (u, v) is the number of positions where u and v differ. The (Hamming) weight of a vector u  H(n ) is the number of positions in u with nonzero value, and is denoted wH (u). The i th component of u is denoted ui . The support of a vector u  H(n ), denoted supp(u), is the set {i : ui = 1}. A (binary) code of length n is a subset C  H(n ). C is said to be of constant weight w if wH (u) = w for all u  C . The elements of a code are called codewords and the size of a code is the number of codewords it contains. The support of C is supp(C ) = {supp(u) : u  C }. The minimum distance of C is dmin (C ) = min{dH (u, v) : u, v  C and u = v}. A constant-weight code of length n , minimum distance d , and weight w is denoted as an (n , d , w) code. A code that is capable of correcting any occurrence of e or fewer symbol errors is said to be e-error-correcting. A code C is e-error-correcting if and only if dmin (C )  2e + 1 [9].

2.3 Set systems and graphs
For integers i < j , the set {i , i + 1, . . . , j } is abbreviated as [i , j ]. We further abbreviate [1, j ] to [ j ]. For a finite set X and k  | X |, we define 2 X = { B : B  X }, and X k = { B  X : | B | = k }.

A set system is a pair S = ( X , B), where X is a finite set of points and B  2 X . The elements of B are called blocks. The order of S is the number of points, | X |, and the size of S is the number of blocks, |B|. A set system ( X , B) is said to be k -uniform if B  X k .A graph is a 2-uniform set system and it is common to refer to the points and blocks of a graph as vertices and edges, respectively. A path of length n is an alternating sequence of vertices and edges W = v0 , e1 , v1 , e2 , . . . , en , vn , such that all the vertices vi , i  [0, n ] and edges ei , i  [n ] are all distinct from one another, except possibly the first and last vertices. A cycle is a path in which the first and last vertices are the same. Let ( X , B) be a set system of order n . The incidence vector of a block B  B is the vector ( B )  H(n ) such that 1, if i  B ( B )i = 0, otherwise. There is a natural correspondence between the Hamming n -space and the complete set system ( X , 2 X ): the positions of vectors in H(n ) correspond to points in X , a vector u  H(n ) corresponds to the block supp(u), and dH (u, v) = |(supp(u) \ supp(v))  (supp(v) \ supp(u))|. From this, it follows that there is a bijection between the set of all codes of length n and the set of all set systems of order n . An (n , k , )-packing is a k -uniform set system ( X , B) with | X | = n such that every element of X 2 is contained in at most  blocks of B . Let D (n , k , ) denote the largest size among all (n , k , )-packings. The leave graph of ( X , B) is the multigraph ( X , E ), where E

123

Optimal low-power coding for crosstalk avoidance

483

contains each e  X 2 exactly  - d (e) times, where d (e) is the number of blocks containing e. When  = 1, we omit  in the notation; in this case, the leave is a simple graph. When the leave contains no edges, the packing is a balanced incomplete block design. The balanced sampling plan avoiding adjacent units (BSA) was introduced to design sampling plans that exclude contiguous units in statistical experiments [10,25]; for more recent work, see [7,29]. In statistical applications, in a circular or linear order of the elements, elements that are "close" do not appear together, while those more distant all appear the same number of times together. A (circular) BSA (n , k ; ) is an (n , k , )-packing ( X , B) with X = Zn whose leave graph consists of all the edges {i , j } with i - j  ±1, . . . , ± (mod n ), and every other pair appears in  blocks. A (linear) LBSA (n , k ; ) is an (n , k , )-packing ( X , B) with X = [0, n - 1] whose leave graph consists of all the edges {i , j } with 0  i < j < n for which j - i   , and every other pair appears in  blocks. We employ these only when  = 1, and so omit  in the notation. We generalize circular and linear BSAs (with  = 1) to a packing sampling plan avoiding adjacent units (PSA). A (circular) CPSA(n , k ; ) is an (n , k )-packing ( X , B) with X = Zn whose leave graph contains all the edges {i , j } with i - j  ±1, . . . , ± (mod n ), and every other pair appears in at most one block. A (linear) LPSA(n , k ; ) is an (n , k )-packing ( X , B) with X = [0, n - 1] whose leave graph contains all the edges {i , j } with 0  i < j < n for which j - i   , and every other pair appears in at most one block. (In this case, every CPSA(n , k ; ) is an LPSA(n , k ; ) but the converse need not hold.) Let B (n , k ; ) denote the largest size of any LPSA(n , k ; ); the LPSA is optimal if its size is B (n , k ; ). Similarly, let B  (n , k ; ) denote the largest size of any CPSA(n , k ; ); the CPSA is optimal if its size is B  (n , k ; ). Let U (n , k ; ) =
2
 -1 i =0 n - -i -1 k -1

+(n -2) k

n -2 -1 k -1

.

Lemma 2.1 B (n , k ; )  U (n , k ; ). Proof For an LPSA(n , k ; ) constructed on [0, n - 1], for each i  [0,  - 1], the points i  -i -1 and n - 1 - i appear in at most n -k blocks, and all the other points appear in at most -1
n -2 -1 k -1

blocks. Then k B (n , k ; )  2

 -1 i =0

n - -i -1 k -1

+ (n - 2)

n -2 -1 k -1

.

When  = 1, we omit it in the notation. If there is an (n , k )-packing with leave graph containing a path of length n - 1, we can always relabel the points to get an LPSA(n , k ). Corollary 2.2 B (n , k ) 
2
n -2 k -1

+(n -2) k

n -3 k -1

.

Theorem 4.1 shows that when k = 3, this inequality is tight.

2.4 Problem formulation
Limited weight codes have been widely exploited for the case of on-chip communication to achieve crosstalk coupling elimination and energy efficiency [12,23]. We consider an n bit parallel bus in a single metal layer, for which we want memoryless codes to weaken crosstalk, reduce power consumption, and correct errors. We use constant weight codes with small weight to achieve low power similarly by reducing the node switching activity, that is, reducing the total number of transitions occurring between the newly arrived data and the present data on the bus.

123

484

Y. M. Chee et al.

Assume an n -bit bus, consisting of signals b0 , b1 , b2 , . . . , bn -1 . Consider a group of three wires in an on-chip bus, which are driven by signals bi -1 , bi and bi +1 . The delay and energy consumption are primarily affected by transition patterns based on the bus signals bi -1 , bi and bi +1 as the crosstalk patterns in Table 1. The selection of codeword does not depend on previous history, so the environment is memoryless. Consequently coding must address the possibility that any two codewords can appear one after the other. Therefore to avoid crosstalk and correct errors, we are interested in constant weight codes of length n , weight w and minimum distance d  3 satisfying the condition that there do not exist three consecutive coordinates i - 1, i , i + 1 such that the crosstalk couplings of type-2 (or -3, -4) occur in any two different codewords. We denote such a code avoiding crosstalk of each type as an (n , d , w)-II (or -III, -IV) code. The maximum size of these codes are denoted as A I I (n , d , w) (or A I I I (n , d , w), A I V (n , d , w)), and any code achieving this size is optimal. When S  { I I , I I I , I V }, the maximum size of a code that is simultaneously an (n , d , w)- S code for each S  S is denoted by AS (n , d , w). When d = 2w , the following results are straightforward. Lemma 2.3 For all positive integers n and w ,
n ; (i) A I I (n , 2w, w) = A I V (n , 2w, w) = w n I I I (ii) A (n , 2w, w) = w when w = 1; A I I I (n , 2, 1) = n +1 2

.

n is an upper bound on the size of the desired code in each case. Proof The quantity s = w We construct codes of size s as follows. The code with support

{{i , s + i , 2s + i , . . . , (w - 1)s + i } : i  [0, s - 1]} is an optimal (n , 2w, w)-II code. The code with support {{wi , 1 + wi , . . . , (w - 1) + wi } : i  [0, s - 1]} is an optimal (n , 2w, w)-IV code, and an optimal (n , 2w, w)-III code when w = 1. When 1 w = 1, the code with support {{2i } : i  [0, n - 2 ]} is an optimal (n , 2, 1)-III code. Next we show there is close connection between (n , 2k - 2, k ) codes of each type and optimal LPSA(n , k )s. Hence, optimal codes are constructed based on the construction of optimal LPSA(n , k )s.

3 Codes and LPSA(n, k; )s
In this section, we establish connections between optimal LPSA(n , k ; )s and the codes of each type. We begin with optimal (n , 2k - 2, k )-II codes for sufficiently large n . Theorem 3.1 Let k  3. Then A I I (n , 2k - 2, k )  B (n , k ). Further, if B (n , k ) = U (n , k ) and n  3k 2 + 2k - 3, then A I I (n , 2k - 2, k ) = B (n , k ). Proof Whenever ( X , B) is an LPSA(n , k ), the code with support B is an (n , 2k - 2, k )-II code. Now suppose that ( X , B) is an optimal LPSA(n , k ) of size U (n , k ). We prove that U (n , k ) is the largest possible size of an (n , 2k - 2, k )-II code. Assume that D is an (n , 2k - 2, k )-II code of size M . Partition the code into three parts as follows. The first part A contains all codewords with at least one segment "11". Because n > k , for each codeword in A, there always exist three adjacent coordinates such that "110" or "011"

123

Optimal low-power coding for crosstalk avoidance

485

appears in these coordinates. Let S = {i : u  A, s.t.,u has "110 in coordinates i - 2, i - 1, i , or "011" in coordinates i , i + 1, i + 2}, and let s = | S |. For each i  S , there exist at most two codewords in A that have "110" in i - 2, i - 1, i or "011" in i , i + 1, i + 2. Hence | A|  2 s . The second part T  D \ A contains all codewords with "1" in at least one coordinate in S . Without loss of generality, if there exists a codeword in A with "110" in the coordinates i - 2, i - 1, i for some i , then the codewords in T with "1" in i must have segment "101" in these coordinates to avoid type-2 crosstalk. Because dmin (D) = 2k - 2, there is only one such codeword. So for each i  S , there is at most one codeword in T with "1" in i . Hence |T |  s . Finally, let C = D \ (A  T ). Then M = |A| + |T | + |C |. Because each codeword in C has "0" in all coordinates in S , we can shorten C to a code C by deleting all coordinates in S . Then C is an (n - s , 2k - 2, k ) code, and supp(C ) is an (n - s , k )-packing. The shortening process partitions the coordinates of C into at most s + 1 classes, separated in C by the coordinates deleted to form C . No codeword of C has "11" in consecutive coordinates of any single class. Let x be the number of isolated coordinates in this partition, and m be the number of classes with at least two coordinates; then x + m  s + 1. We now estimate the size of C using the packing. s -1 s -2 s -3 Let a0 = n - , a1 = n - , a2 = n - . Then we have: k -1 k -1 k -1 | C | = |C |  x · a0 + 2m · a1 + (n - s - 2m - x ) · a2 . k    

Because x - y - 1  x - y  x - y , we have:   x n -s -1 - n -s -3 + 2m n -s -2 - n -s -3 + (n - s ) n -s -3  k -1 k -1 k -1 k -1 k -1 M  3s +  k   2 1 n -s -3  x   k -1 + 1 + 2m k -1 + 1 + (n - s ) k -1   3s +  k      2(s + 1) + (n - s ) n -s -3   2 x + 2m + (n - s ) n -s -3      k -1 k -1   3s +  .  3s +  k k Let F (s ) = 3s +
2(s +1)+(n -s ) k
n -s -3 k -1

. We claim that because n  3k 2 + 2k - 3,
n -s -3 k -1

U (n , k )  maxs [1,n ] F (s ). Because F (s ) = 3s +
2(s +2)+(n -s -1) k
n -s -4 k -1

2(s +1)+(n -s ) k
n -s -4 k -1

and F (s + 1) = 3(s + 1) +
n -s -4 k -1

, we have

-2

k

-3  F (s )- F (s +1)  n - 2k - s . k-1

+n -s -2 k

-2. Further, we have: n - 3k 2 - 1 - s k (k - 1)  F (s ) - F (s + 1) 

So when s  n - 3k 2 - 1, F (s ) - F (s + 1)  0, i.e., F (s ) is decreasing; and when s  n - 2k , F (s ) - F (s + 1)  0, i.e., F (s ) is increasing. When s  [n - 3k 2 , n - 2k - 1],

123

486

Y. M. Chee et al.

F (s )  F (1); because the verification is tedious, we omit it here. We therefore only need to compare F (1) and F (n ) to find the maximum value of F (s ).    4 + (n - 1) n -4  2  k -1   - 3n - 2(n + 1)  (n - 1)(n - 3k - 1) . F (1) - F (n ) = 3 +  k k k (k - 1)

Because n  3k 2 + 2k - 3  3k 2 + 1, F (1)  F (n ) and maxs [1,n ] F (s ) = F (1).    2 n -2 + (n - 2) n -3 - 4 - (n - 1) n -4   k -1 k -1 k -1  -3 U (n , k ) - F (1)   k    n -2 + (n - 1) n -3 - 4 - (n - 1) n -4   k -1 k -1 k -1  -3  k    n -2 - 4  2  k -1   - 3  n - 3k - 2 k + 3  0 .  k k (k - 1)

Hence U (n , k )  maxs [1,n ] F (s ). For (n , 2k - 2, k )-III codes and (n , 2k - 2, k )-IV codes, we establish lower bounds. Lemma 3.2 1. A I I I (n , 2k - 2, k )  A I I , I I I , I V (n , 2k - 2, k )  D ( n -1 2. A I I I (n , 4, 3)  A I I , I I I , I V (n , 4, 3)  B ( n 2 , 3) + 2 . n I I I I I , I I I , I V  3. A (n , 4, 3)  A (n , 4, 3)  B ( 2 , 3) + n 2 .
n 2

, k ).

Proof For the first inequality, take an ( n 2 , k )-packing ( X , B ), and construct a code C of n length n 2 by taking supp(C ) = B . View C as an |B | × 2 array. When n  1 (mod 2), we add one column of all zeroes between every two consecutive columns of C , and when n  0 (mod 2) we add one further column of all zeroes after C to get an (n , 2k - 2, k )-III code. The verification is straightforward, because every second column is all zeroes. The construction for the second is similar. Apply the same inflation to an LPSA n 2 ,3 of size B n 2 , 3 to obtain a code C1 . In every codeword of C1 , two 1s are separated by three (or more) coordinates, and different codewords cannot have 1s in adjacent coordinates. Now 1 form code C2 , consisting of all codewords with support {2i , 2i + 1, 2i + 2} for 0  i < n - 2 . No prohibited situation arises from 000 or 111 in three consecutive coordinates of a codeword. In consecutive coordinates in which two codewords of C2 are neither 000 nor 111, the two codewords contain 011 and 110, which is permitted. So we consider one codeword from 1 C1 and one from C2 . The coordinates with indices in {2i + 1 : 0  i < n - 2 } appear in only one codeword, which is {2i , 2i + 1, 2i + 2}. So in the consecutive coordinates in which two such codewords are neither 000 nor 111, and are not equal, the two codewords contain {001, 100}, {010, 011}, or {010, 110}. All are permitted. The bound in the third case is equal to that in the second unless n is even and B  n 2 ,3 = B n C and C as in the second case; one further , 3 . When both occur, use a CPSA to form 1 2 2 codeword can be added with support {0, n - 2, n - 1}.

123

Optimal low-power coding for crosstalk avoidance

487

Lemma 3.3 A I V (n , 2k - 2, k )  B (n , k ). Proof Take an LPSA(n , k ) ( X , B) of size B (n , k ). Apply to the points in [0, n - 1] the permutation i  2i , if i < n /2 , and i  2i - 2 n /2 + 1, if i  n /2 , to get ( X , B ). The code C with supp(C ) = B is an (n , 2k - 2, k )-IV code. We give another construction for an (n , 2k - 2, k )-IV code from an optimal LPSA(n , k ; k - 1). When k = 3, this construction gives a better lower bound than Lemma 3.3. Lemma 3.4 Let k  3. 1. A I I , I V (n , 2k - 2, k )  B (n , k ; k - 1), 2. A I V (n , 2k - 2, k )  B (n , k ; k - 1) + 3. A I V (n , 2k - 2, k )  B  (n , k ; k - 1) +
n -1 k -1 n k -1

, and .

-1 Proof Let s = n k -1 , and ( X , B ) be an LPSA(n , k ; k - 1) of size B (n , k ; k - 1). Then the code C with supp(C ) = { B : B  B} is an (n , 2k - 2, k )-II code and an (n , 2k - 2, k )-IV code. Further, the code C with supp(C ) = { B : B  B}{{(k -1)i , (k -1)i +1, . . . , (k -1)i +k -1} : i  [0, s - 1]} is an (n , 2k - 2, k )-IV code. When n  0 (mod k - 1), statement (3) is implied by statement (2). So suppose that n  0 (mod k - 1). Using instead a CPSA(n , k ; k - 1) of size B  (n , k ; k - 1), adjoin the block {(k - 1)s , (k - 1)s + 1, . . . , (k - 1)s + k - 2, 0}.

Lemma 3.5 A I I , I V (n , 4, 3)  U (n , 3; 2) when n  13. Proof Computational results reported in Table 2 show that A I I , I V (13, 4, 3) = U (13, 3; 2) = 16, A I I , I V (14, 4, 3) = U (14, 3; 2) = 20, A I I , I V (15, 4, 3) = U (15, 3; 2) = 25, and
Table 2 Sizes of optimal codes for n  20 n D (n , 3) B (n , 3) B  (n , 3) B (n , 3; 2) B  (n , 3; 2) A I I .(n , 4, 3) A I I I (n , 4, 3) A I V (n , 4, 3) A I I , I I I (n , 4, 3) A I I , I V (n , 4, 3) A I I I , I V (n , 4, 3) A I I , I I I , I V (n , 4, 3) 3 1 0 0 0 0 1 1 1 1 1 1 1 4 1 0 0 0 0 1 1 1 1 1 1 1 5 2 1 0 0 0 2 2 2 2 2 2 2 6 4 2 2 0 0 4 3 4 3 4 3 3 7 7 4 3 1 0 5 4 6 3 4 4 3 8 8 6 5 2 0 6 5 7 4 4 5 4 9 12 9 9 4 3 9 6 10 5 7 6 5 10 13 10 10 6 5 10 7 12 6 8 7 6 11 17 14 13 9 8 14 8 15 7 12 8 7 12 20 16 16 12 12 16 9 19 8 13 9 8 13 26 21 20 16 15 21 10 23 10 16 10 10 14 28 24 23 20 18 15 35 30 30 25 25 16 37 32 32 28 26 17 44 39 38 34 34 18 48 42 42 37 36 19 57 50 49 45 43 20 60 54 53 48 46

24
11

30
13

32
14

39
17

42
18

50
19

54
21

26
11 20 11 11

32
13 25 13 13

35
13 28 14 13

42
17 34 17 17

45
18 37 18 18

54
19 45 19 19

57
20 48 21 20

Lower bounds and exact values

123

488

Y. M. Chee et al.

A I I , I V (16, 4, 3) = U (16, 3; 2) = 32. Suppose to the contrary that A I I , I V (n , 4, 3) > U (n , 3; 2) for some n  17, and let n be the smallest such value. When n  17 we have U (n , 3; 2)  U (n - 1, 3; 2) + 3 and U (n , 3; 2)  U (n - 3, 3; 2) + 4. (See Table 2 for small values.) Let ( X , B) be the support of an (n , 4, 3)-{II,IV} code of size A I I , I V (n , 4, 3). Some triple of B covers a pair of the form {a , b}  {{i , i + 1}, {i , i + 2}} because it is not an LPSA(n , 3; 2). Case 1 Some element appears in at most one triple. Suppose that element i appears in no triple. Shorten the code by deleting coordinate i and delete the triples (if any) containing pairs {i - 1, i + 1}, {i - 2, i + 1}, and {i - 1, i + 2}. The result is a type II and IV code, so the given code has at most A I I , I V (n - 1, 4, 3) + 3 triples, a contradiction. Suppose now that element i appears in exactly one triple T . Then if i  {0, n - 1}, delete coordinate i and triple T to get a contradiction. If i  {1, n - 2}, delete coordinate i and delete triple T , along with triples containing {0, 2} and {0, 3} when i = 1 or {n - 4, n - 1} and {n - 3, n - 1}, to get a contradiction. So 2  i  n -3. If T contains neither i -1 nor i +1, then no triple contains both i - 1 and i + 1, because the code is type IV. Shorten by deleting coordinate i and delete triple T and the triples (if any) containing pairs {i - 2, i + 1} and {i - 1, i + 2}, yielding a contradiction. Otherwise, without loss of generality T also contains i - 1 but does not contain i + 1. But then if some triple T contains i - 2 and i + 1, it cannot contain i . If T does not also contain i - 1, then we have T  {i - 1, i , i + 1} = {i - 1, i } and T  {i - 1, i , i + 1} = {i + 1}, which cannot happen in a type II code. So T = {i - 2, i , i + 1}. Hence there are at most two triples among those containing pairs {i - 1, i + 1}, {i - 2, i + 1}, and {i - 1, i + 2}, so shorten as before. Case 2 Some triple T satisfies |T  {i , i + 1, i + 2}| = 2 for some 0  i  n - 3. Suppose that {a , b} = T  {i , i + 1, i + 2} and let {c} = {i , i + 1, i + 2} \ {a , b}. There can be no triple containing c but neither a nor b, because the code is type II and type IV. So c is in exactly two triples, T that contains a and T that contains b; only T contains both a and b. Applying the same argument to T and T , a and b each appear in exactly two triples. So there are only three triples (T , T , and T ) that contain a , b, or c. Shorten by deleting coordinate i + 1 and the triples T , T , and T to obtain a contradiction. Case 3 No triple T satisfies |T  {i , i + 1, i + 2}| = 2 for any 0  i  n - 3. If a triple T satisfies |T  {i , i + 1, i + 2}| = 3 for some 0  i  n - 3, equivalently it satisfies |T  {i + 1, i + 2, i + 3}| = 2 for some 0  i  n - 4 or |T  {i - 1, i , i + 1}| = 2 for some 1  i  n - 3. Apply Case 2. Otherwise every triple T satisfies |T  {i , i + 1, i + 2}|  1 for 0  i  n - 3. But then ( X , B) is an LPSA(n , 3; 2) and hence we have at most B (n , 3; 2)  U (n , 3; 2) triples, the final contradiction.

4 Optimal packing sampling plans
By Corollary 2.2, we have the upper bound:
n -2 2

U (n , 3) =

2

+ (n - 2) 3

n -3 2

 2 n -4n +4  ,  6    n 2 -3n , 6 = n2 - 4n   6 ,   2  n -3n -4 ,
6

if n  2 (mod 6), if n  3 (mod 6), if n  0, 4 (mod 6), if n  1, 5 (mod 6).

Theorem 4.1 B (n , 3) = U (n , 3) for all n  0.

123

Optimal low-power coding for crosstalk avoidance

489

(n -1) -4(n -1)+4 3n 3 blocks, we get an LPSA(n - 1, 3) of size n - - n- by removing 6 2 = 6 the point n - 1 and all blocks containing it, which is optimal. When n  1, 5 (mod 6), Colbourn and Rosa [4] showed there exists an (n , 3)-packing of 2 n +2 size n -3 , whose leave graph consists of a cycle of length n - 1 and one isolated point. 6 Assume n - 1 is the isolated point. Remove the block {x , n - 2, n - 1} for some x  [0, n - 3]; 3 the result is an optimal LPSA(n , 3). Now, n - 1 appears in n - 2 blocks. Removing n - 1 and all blocks containing it from the optimal LPSA(n , 3) constructed above, we obtain an 2 (n -1)2 -4(n -1) n -4 3 LPSA(n - 1, 3) of size n -3 - n- , which is optimal. 6 2 = 6 n -3 2
2 2

Proof When n  3 (mod 6), Colbourn and Rosa [4] (and Colbourn and Ling [5]) construct 2 3n a BSA(n , 3) of size n - 6 , which is an optimal LPSA(n , 3). Because each point appears in

Theorem 4.2 1. B  (n , 3) = U (n , 3) when n  0, 3, 4 (mod 6). 2. B  (n , 3) = U (n , 3) - 1 when n  1, 2, 5 (mod 6).
-3) blocks when n  Proof The constructions in Theorem 4.1 yield a CPSA(n , 3) with n (n6 n (n -4) 3 (mod 6) and with 6 blocks when n  0, 4 (mod 6). A CPSA(n , 3) can have at most n n -3 blocks, which equals U (n , 3) when n  0, 3, 4 (mod 6), so these are optimal. 3 2 n -3 When n  2 (mod 6), n = U (n , 3) - 1 so B  (n , 3)  U (n , 3) - 1. When 3 2

n  1, 5 (mod 6), if there were a CPSA(n , 3) with U (n , 3) =
n (n -1) 2 3(n 2 -3n -4)

n 2 -3n -4 6

codewords, then the

- = n + 2. The leave must be an number of edges in the leave graph is 6 n -cycle with two additional edges, but every vertex in the leave must have even degree, which cannot occur. So B  (n , 3)  U (n , 3) - 1. To establish equality when n  1, 2, 5 (mod 6), remove the block {0, n - 1, x } from an LPSA(n , 3) from Theorem 4.1. Lemma 4.3 B  (n , 3; 2) = B (n , 3; 2) = U (n , 3; 2) whenever n  3, 5 (mod 6) and n  15. B  (n , 3; 2) + 2 = B (n , 3; 2) = U (n , 3; 2) whenever n  2, 4 (mod 6) and n  14. Proof Zhang and Chang [30] establish that whenever n  15 and n  3, 5 (mod 6), there is a -5) BSA(n , 3; 2) having n (n6 blocks; this is also an optimal CPSA(n , 3; 2) and LPSA(n , 3; 2). Now suppose that n  14 and n  2, 4 (mod 6). When n  2 (mod 6), writing n = 6t + 2, U (6t + 2, 3; 2) = (2t )(3t - 1). Delete element 6t + 2 from a BSA(6t + 3, 3; 2) with (2t + 1)(3t - 1) blocks, removing 3t - 1 blocks to obtain an LPSA(6t + 2, 3; 2), which is therefore optimal. When n  4 (mod 6), writing n = 6t + 4, U (6t + 4, 3; 2) = t (6t + 2). Delete element 6t + 4 from a BSA(6t + 5, 3; 2) with t (6t + 5) blocks, removing 3t blocks to obtain an LPSA(6t + 4, 3; 2), which is therefore optimal. Remove the blocks {0, n - 2, x }, {1, n - 1, y } for some x and y from the optimal LPSA(n , 3; 2) constructed above to obtain an optimal CPSA(n , 3; 2) when n  2, 4 (mod 6). For n = 6t , U (6t , 3) = 6t (t - 1) + 1, and
6t +1 3 6t -4 2 6t 3 6t -5 2

= 6t (t - 1). For n = 6t + 1,

U (6t +1, 3) = t (6t -3), and = t (6t -3)-1. However, if a CPSA(6t +1, 3; 2) were to have t (6t - 3) - 1 blocks, its leave must have 2(6t + 1) + 1 edges and every such graph with minimum degree 4 has two vertices of degree 5. Because all vertices in the leave must have even degree, no CPSA(6t + 1, 3; 2) can exist with more than t (6t - 3) - 2 blocks. We provide bounds to apply when n  0, 1 (mod 6). Lemma 4.4 B (2n , 3; 2)  4 B (n , 3), and B (2n + 1, 3; 2)  4 B (n , 3) + n - 2. In addition, B  (2n , 3; 2)  4 B  (n , 3), and B  (2n + 1, 3; 2)  4 B  (n , 3) + n - 3.

123

490

Y. M. Chee et al.

Proof Start with an LPSA(n , 3) on [0, n - 1]. We form an LPSA(2n , 3; 2) on [0, 2n - 1]. For each block {a , b, c} in the LPSA, form four blocks {{2a + , 2b + , 2c +  } : , ,   {0, 1},  +  +   0 (mod 2)}. The verification is straightforward. To form an LPSA(2n + 1, 3) on [0, 2n ], adjoin {{2i , 2i + 3, 2n } : 0  i  n - 3}. The construction for CPSAs is the same, except that one does not adjoin {0, 3, 2n }.

5 Conclusion
Applying Theorem 3.1 with the results in Theorem 4.1, we have optimal (n , 4, 3)-II codes for all n  30. By computer search (using cliquer [13] and hill-climbing (a variant of [24])), we determined the sizes of optimal LPSA(n , 3; )s, CPSA(n , 3; )s, and (n , 4, 3) codes of lengths n  20. The sizes are listed in Table 2 and corresponding optimal codes are available from the authors; those in slanted font are lower bounds from Theorem 3.1 and Lemma 3.4. In this paper, we present the first memoryless transition bus-encoding technique for power minimization, error-correcting and elimination of crosstalk simultaneously. We establish the connection between codes avoiding crosstalk of each type with packing sampling plans avoiding adjacent units. Optimal codes of each type are constructed.

References
1. Bertozzi D., Benini L., de Micheli G.: Low power error resilient encoding for on-chip data buses. In: DATE'02: Proceedings of the Conference on Design. Automation and Test in Europe, pp. 102­109. IEEE Computer Society, Washington, DC (2002). 2. Bertozzi D. Benini L., Ricco B.: Energy-efficient and reliable low-swing signaling for on-chip buses based on redundant coding. In: ISCAS'02: Proceedings of the IEEE International Symposium on Circuits and Systems, vol. 1, pp. 93­96. IEEE Press, Piscataway, NJ (2002). 3. Chee Y.M., Colbourn C.J., Ling A.C.H.: Optimal memoryless encoding for low power off-chip data buses. In: ICCAD'06: Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided Design, pp. 369­374. ACM Press, New York (2006). 4. Colbourn C.J., Rosa A.: Quadratic leaves of maximal partial triple systems. Graphs Comb. 2(4), 317­337 (1986). 5. Colbourn C.J., Ling A.C.H.: A class of partial triple systems with applications in survey sampling. Commun. Stat. Theory Methods 27(4), 1009­1018 (1998). 6. Duan C., Tirumala A., Khatri S.P.: Analysis and avoidance of crosstalk in on-chip buses. In: Hot Interconnects'01: Proceedings of the 9th Annual Symposium on High-Performance Interconnects, pp. 133­138. IEEE, Piscataway (2001). 7. Dukes P.J., Ling A.C.H.: Existence of balanced sampling plans avoiding cyclic distances. Metrika 70, 131­140 (2009). 8. Favalli M., Metra C.: Bus crosstalk fault-detection capabilities of error-detecting codes for on-line testing. IEEE Trans. Very Large Scale Integr. Syst. 7, 392­396 (1999). 9. Hamming R.W.: Error detecting and error correcting codes. Bell Syst. Tech. J. 29(2), 147­160 (1950). 10. Hedayat A.S., Rao C.R., Stufken J.: Sampling plans excluding contiguous units. J. Stat. Plan. Inference 19(2), 159­170 (1988). 11. Ho R.: On-chip wires: Scaling and efficiency, Ph.D. dissertation, Department of Electrical Engineering, Stanford University, Palo Alto, CA (2003). 12. Khan Z., Arslan T., Erdogan A.T.: A dual low-power and crosstalk immune encoding scheme for systemon-chip buses. In: PATMOS'04: Proceedings of the 14th International Workshop on Power and Timing Modeling, Optimization and Simulation. Lecture Notes in Computer Science, vol. 3254, pp. 585­592. Springer, Berlin (2004). 13. Niskanen S., Östergård P.R.J.: Cliquer User's Guide, Version 1.0, Communications Laboratory, Helsinki University of Technology, Espoo. Tech. Rep. T48, (2003). 14. Patel K.N., Markov I.L.: Error-correction and crosstalk avoidance in DSM busses. IEEE Trans. Very Large Scale Integr. Syst. 12(10), 1076­1080 (2004).

123

Optimal low-power coding for crosstalk avoidance

491

15. Ramprasad S., Shanbhag N.R., Hajj I.N.: A coding framework for low-power address and data busses. IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 7, 212­221 (1999). 16. Rossi D., van Dijk E.S., Kleihorst R.P., Nieuwland A.K., Metra C.: Coding scheme for low energy consumption fault-tolerant bus. In: IOLTW'02: Proceedings of the Eighth IEEE International On-Line Testing Workshop, pp. 8­12. IEEE Computer Society, Washington, DC (2002). 17. Rossi D., Cavallotti S., Metra C.: Error correcting codes for crosstalk effect minimization. In: DFT'03: Proceedings of the 18th IEEE International Symposium on Defect and Fault-Tolerance in VLSI Systems, pp. 257­266. IEEE Computer Society, Washington, DC (2003). 18. Rossi D., van Dijk E.S., Kleihorst R.P., Nieuwland, A.K., Metra, C.: Power consumption of fault tolerant codes: the active elements. In IOLTW'03: Proceedings of the Ninth IEEE International On-line Testing Workshop, pp. 61­67. IEEE Computer Society, Washington, DC (2003). 19. Samala N.K., Radhakrishnan D., Izadi B.: A novel deep sub-micron bus coding for low energy. In: ESA'04: Proceedings of the International Conference on Embedded Systems and Applications, pp. 25­30. CSREA Press, Leuven (2004). 20. Shannon C.E.: A mathematical theory of communications, Bell Syst. Tech. J. 27(3), 379­423, 623­656 (1948). 21. Sotiriadis P.P., Chandrakasan A.: Bus energy minimization by transition pattern coding (TPC) in deep sub-micron technologies. In: ICCAD'00--Proceedings of the 2000 IEEE/ACM International Conference on Computer-Aided Design, pp. 322­327. IEEE, Piscataway, NJ (2000). 22. Stan M.R., Burleson W.P.: Bus-invert coding for low-power I/O. IEEE Trans. Very Large Scale Integr. Syst. 3(1), 49­58 (1995). 23. Stan M.R., Burleson W.P.: Coding a terminated bus for low power. In: Great Lakes Symposium VLSI, pp. 70­73, Buffalo, NY (1995). 24. Stinson D.R.: Hill-climbing algorithms for the construction of combinatorial designs. In: Algorithms in Combinatorial Design Theory. Annals of Discrete Mathematics, vol. 26, pp. 321­334. Elsevier, NorthHolland (1985). 25. Stufken J.: Combinatorial and statistical aspects of sampling plans to avoid the selection of adjacent units. J. Comb. Inf. Syst. Sci. 18(1­2), 149­160 (1993). 26. Su C.L., Tsui C.Y., Despain A.M.: Saving power in the control path of embedded processors. IEEE Des. Comput. 11, 24­30 (1994). 27. Subrahmanya P., Manimegalai R., Kamakoti V., Mutyam M.: A bus encoding technique for power and cross-talk minimization. In: VLSI Design 2004: 17th International Conference on VLSI Design, pp. 443­448. IEEE Computer Society, Xi'an (2004). 28. Victor B., Keutzer K.: Bus encoding to prevent crosstalk delay. In: ICCAD'01--Proceedings of the 2001 IEEE/ACM International Conference on Computer-Aided Design, pp. 57­63. IEEE, Piscataway, NJ (2001). 29. Wright J.H., Stufken J.: New balanced sampling plans excluding adjacent units. J. Stat. Plan. Inference 138, 3326­3335 (2008). 30. Zhang J., Chang Y.X.: The spectrum of BSA(v, 3, ; ) with  = 2, 3. J. Comb. Des. 15, 61­76 (2007).

123

Augmentation is an operation to increase the number of symbols in a covering array, without unnecessarily increasing the number of rows. For covering arrays of strength two, one type of augmentation forms a covering array on  vv  symbols from one on  vâ1vâ1  symbols together with  vâ1vâ1  covering arrays each on two symbols. A careful analysis of the structure of the optimal binary covering arrays underlies an augmentation operation that reduces the number of rows required. Consequently a number of covering array numbers are improved.Software behavior depends on many factors, and some failures occur only when certain factors interact. This is known as an interaction triggered failure, and the corresponding selection of factor values can be modeled as a Minimal Failure-causing Schema (MFS). (An MFS involving m factors is an m-MFS.) Combinatorial Testing (CT) has been developed to exercise (âhitâ) all MFS with few tests. Adaptive Random Resting (ART) endeavors to make tests as different as possible, ensuring that testing of MFS is not unnecessarily repeated. Random Testing (RT) chooses tests at random without regard to the MFS already treated. CT might be expected to improve on RT for finding interaction triggered faults, and yet some studies report no significant difference. CT can also be expected to be better than ART, and yet other studies report that ART can be much better than RT. In light of these, the relative merits of CT, ART, and RT for finding interaction triggered faults are unclear.Gao SW, Lv JH, Du BL et al. Balancing frequencies and fault detection in the in-parameter-order algorithm. JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY 30(5): 957­968 Sept. 2015. DOI 10.1007/s11390-015-1574-6

Balancing Frequencies and Fault Detection in the In-Parameter-Order Algorithm
Shi-Wei Gao 1 ( ), Jiang-Hua Lv 1, ( 1 ) and Shi-Long Ma (
1 2

Ô å

ù×), Bing-Lei Du

1

(

), Charles J. Colbourn 2

State Key Laboratory of Software Development Environment, Beihang University, Beijing 100191, China School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe AZ 85287-8809, U.S.A.

E-mail: ge89@163.com; jhlv@nlsde.buaa.edu.cn; binglei.du@gmail.com; Charles.Colbourn@asu.edu E-mail: slma@nlsde.buaa.edu.cn Received March 16, 2015; revised June 29, 2015.
Abstract The In-Parameter-Order (IPO) algorithm is a widely used strategy for the construction of software test suites for combinatorial testing (CT) whose goal is to reveal faults triggered by interactions among parameters. Variants of IPO have been shown to produce test suites within reasonable amounts of time that are often not much larger than the smallest test suites known. When an entire test suite is executed, all faults that arise from t-way interactions for some fixed t are surely found. However, when tests are executed one at a time, it is desirable to detect a fault as early as possible so that it can be repaired. The basic IPO strategies of horizontal and vertical growth address test suite size, but not the early detection of faults. In this paper, the growth strategies in IPO are modified to attempt to evenly distribute the values of each parameter across the tests. Together with a reordering strategy that we add, this modification to IPO improves the rate of fault detection dramatically (improved by 31% on average). Moreover, our modifications always reduce generation time (2 times faster on average) and in some cases also reduce test suite size. Keywords combinatorial testing, IPO, test suite generation, expected time to fault detection, software under test

1

Introduction

Modern software systems are highly configurable. Their behavior is controlled by many parameters. Interactions among these parameters may cause severe failures, resulting in poor reliability. Therefore, software testing and reliability assessment are crucial in the design of effective software, as discussed in [1-3] for reliability and in [4-15] for software testing. Software testing serves two main purposes: 1) to ensure that software has as few errors as possible prior to release, and 2) to detect and isolate faults in the software. A generic model of such a software system identifies a finite set of parameters, and a finite set of possible values

for each parameter. Faults may arise due to a choice of a value for a single parameter, interactions among the values of a subset of the parameters, or a result of environmental conditions not included in the software model. We focus on the faults that arise from the parameters identified and the interactions among them. It is nearly always impractical to exhaustively test all combinations of parameter values because of resource constraints. Fortunately, this is not necessary in general: in some real software systems, more than 70 percent of faults are caused by interactions between two parameters[16], and all known faults are caused by interactions among six or fewer parameters[17-18] .

Regular Paper Special Section on Software Systems This work was supported by the National Natural Science Foundation of China under Grant Nos. 61300007 and 61305054, the Fundamental Research Funds for the Central Universities of China under Grant Nos. YWF-15-GJSYS-106 and YWF-14-JSJXY-007, and the Project of the State Key Laboratory of Software Development Environment of China under Grant Nos. SKLSDE-2015ZX-09 and SKLSDE-2014ZX-06.  Corresponding Author ©2015 Springer Science + Business Media, LLC & Science Press, China

958

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

For these reasons, combinatorial testing (CT) or tway testing chooses a strength t (the largest number of parameters interacting to cause a fault), and forms a software interaction test suite as follows. Every row of the test suite is a test or a test case. For each parameter in the system, each test specifies an admissible value for the parameter. The defining property is that, no matter how one chooses t parameters and an admissible value for each (a t-way interaction), at least one test has the specified parameters set to the indicated values. This coverage property ensures that every possible interaction among t or fewer parameter values must arise in at least one of the test cases. CT has proved to be an efficient testing technique for software[6,9,19]. Indeed, empirical studies have shown that t-way testing can effectively detect faults in various applications[17-18,20-22] . A primary objective in producing a test suite is to minimize the cost of executing the tests; hence minimizing the number of tests is desired. At the same time, however, the time to produce the test suite is also crucial. Hence the most effort has been invested in finding a variety of test suite generation algorithms. Some invest additional computational resources in minimizing the size of the test suite, while others focus on fast generation methods for test suites of acceptable but not minimum size. General methods providing fast generation have primarily involved greedy algorithms[9]. One-test-at-a-time methods start with an empty test suite, and keep track of the as-yetuncovered t-way interactions. Then repeatedly a test is selected, which attempts to maximize the number of such interactions that are covered by the test, until all interactions are covered. This strategy was pioneered in AETG[23] , and later proved to be within a constant factor of the optimal size[24-25] . In practice, maintaining a list of all t-way interactions can be prohibitive when the number of parameters is large. One-parameter-ata-time methods instead construct a test suite for t of the parameters (this contains all of the possible tests). Then it repeatedly adds a new parameter, and chooses a value for this parameter in each of the existing tests (horizontal growth). Because it is possible that some t-way interactions involving the new parameter have not been covered yet, further tests are selected to cover all such interactions (vertical growth). This requires maintaining a list of (t - 1)-way interactions, and hence can involve less bookkeeping. The pioneering example here is IPO[26] and its extensions, IPOG[27] , and IPOG-F and IPOG-F2[28], which will be discussed in more detail in Section 2. Both strategies typically pro-

duce test suites of acceptable size[26,29] . It has been observed that one-test-at-a-time methods produce slightly smaller test suites in general, while one-parameter-ata-time methods are somewhat faster at generation[26]. As mentioned earlier, software interaction test suites serve as two complementary roles[30]: to verify that no t-way interaction of SUT (software under test) causes a fault, or to locate such a fault. These two roles are different: certifying absence of a fault requires running the whole test suite, while locating a fault may not. Indeed in [30], it is shown that minimum test suite size is not the correct objective for fault location; the structure of the test suite can be more important than its size alone. An improved rate of fault detection can provide faster feedback to testers[31] . Recent studies have shown that CT is an effective fault detection technique and that early fault detection can be improved by reordering the generated test suites using interaction-based prioritization approaches[32-34] . Many strategies have been proposed to guide prioritization using evaluation measures such as interaction coverage based prioritization[30,35-39] and incremental interaction coverage based prioritization[40-41] . In [30], an evaluation measure of the expected time to fault detection is given. Test case prioritization techniques have been explored for the one-test-at-a-time methods, but little is known for the one-parameter-at-a-time methods. Bryce et al.[35-36,42] presented techniques that combine generation and prioritization. Pure prioritization[32-34,39] instead reorders an existing interaction test suite, using the metric of normalized average percentage of faults detected (NAPFD). However, existing pure prioritization techniques use explicit fault measurements of real systems, and hence are not directly suitable for the IPO algorithm. The main contributions of our work are: 1) We modify the IPO algorithm in order to accelerate the method and make it effective for fault detection. Our modifications attempt to make the values of each parameter more evenly distributed during generation. We focus on choosing values for the extension to an additional parameter during the horizontal growth of the algorithm and filling values for don't care positions. (See Section 3.) 2) We develop a pure prioritization technique (a reordering strategy) for the IPO algorithm based on the evaluation measure presented in [30]. Our method can reduce the expected time to fault detection effectively. (See Section 4.)

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO

959

3) We conduct experiments to demonstrate the effectiveness of the modifications (see Section 5). We conclude that the modifications to the IPO strategy result in faster generation (2 times faster on average according to the experimental results in Subsection 5.1), sometimes in smaller test suites, and together with the pure prioritization, in less time to detect the first fault (improved by 31% on average according to the experimental results in Subsection 5.2). 2 Framework of the IPO Algorithm

IPO comprises a family of methods of the oneparameter-at-a-time type. We focus on IPOG as a representative implementation. The basic operation is to add a new parameter to an existing interaction test suite of strength t. To initialize the method, whenever the number of parameters is at most t, all possible rows are included, which is necessary and sufficient to obtain a test suite. Thereafter, to introduce a new parameter, the set  of all t-way interactions involving the new parameter is computed. Horizontal growth adds a value of the new parameter to each existing row so that this extended row covers the most interactions in  ; the interactions covered are removed from  . Then if  still contains uncovered interactions, vertical growth adds new rows to cover them. This process is outlined in the flowchart in Fig.1. Existing variants of the IPO strategy alter the selection of values for the new parameter during horizontal growth and the selection of additional rows during

vertical growth. During both horizontal and vertical growth, it frequently happens that the value for one or more parameters in a row can be chosen arbitrarily without affecting the coverage of the row. Such entries are don't care positions[26] in the test suite. The IPO methods exploit the fact that selecting values for don't care positions can be deferred; then they can be filled during horizontal growth when the next parameter is introduced. Every variant of IPO must therefore deal with two basic problems: · choose values for the new parameter to maximize the number of uncovered interactions covered during horizontal growth; · assign values for don't care positions that arise. In the next section, we explore an implementation of this IPO framework in which the objective is not just to ensure coverage, but also to attempt to make each value appear as equally often as possible for each parameter. The latter is a balance condition. 3 Balance in the IPO Algorithm

A test suite must cover all t-way interactions. Consider a specific parameter and the t-way interactions that contain it. For each value of the parameter, the numbers of these t-way interactions with each different value of the parameter are the same. Now consider the frequencies of values of the parameter within the tests of a test suite. Because each value must provide the coverage of the same number of interactions, it appears to be reasonable to attempt to make the frequencies

Create Set  of Uncovered t-Way Combinations of Values Involving the Next Parameter

START

Horizontal Growth (Remove the Covered Combinatons from )

Build a t-Way Test Set for the First t Parameters

Yes  Is Empty?

All the Parameters Are Included in the Set?

No

No Vertical Growth (Remove the Covered Combinations from )

Yes END

Fig.1. Flowchart of IPOG algorithm.

960

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

close to equal. The same argument applies to fault detection. Two issues arise. First, current IPO algorithms do not make any explicit effort to balance the frequencies of values. Second, it is not at all clear how such an objective might affect the sizes of test suites produced, or the time to generate them, or their rates of fault detection. In this section, we develop modifications of IPO to address frequencies of values. Subsequent sections treat their impacts. 3.1 Choosing a New Parameter's Values

While shown in Algorithm 1 for IPOG, this simple strategy can also be used in IPOG-F and IPOG-F2. We show the modification for IPOG-F. The IPOG-F algorithm greedily selects over both the row and the value with which the covering array is extended, and the extended row/value pair (i; a) is greedily selected by the following formula[28] : tn = n-1 - T c [i ; a ], t-1

During horizontal growth, the IPOG algorithm chooses to add a value of the new parameter to cover the greatest number of interactions in  . In many situations, more than one value achieves this goal, and we must choose one. A naive strategy treats the values as ordered, and selects the smallest value that covers the most interactions in  . This introduces a bias towards the smaller values of each parameter, sometimes resulting in smaller values appearing much more frequently than larger ones. Here a different strategy, shown in Algorithm 1, is proposed. The essential change is to treat the values as being cyclically ordered, recording the value selected for the previous row. Then possible values for this row are considered by starting from the value following the previous one selected. For this modification, vertical growth remains unchanged.
Algorithm 1. Modified Horizontal Growth 1. Cov[r ; v] is the number of interactions that the extended row (r ; v) covers 2. q  |P | 3. prev  q 4. for each row r in the covering array ca do 5. max  (prev + 1) mod q 6. j  (max + 1) mod q 7. while j = ((prev + 1) mod q ) do 8. if Cov[r, vj ] > Cov[r, vmax ] then 9. max  j 10. end if 11. j  (j + 1) mod q 12. end while 13. r  (r, vmax ) 14. prev  max 15. end for

where n is the number of parameters, Tc [i; a] denotes the t-tuples that have previously been covered by already extended rows, and tn denotes the number of new t-tuples the row/value pair would cover if we extend row i with value a. The metric of optimal selection for the extended row (i; a) is that the extended row (i; a) would maximize tn . The original pseudo-code for horizontal growth in IPOG-F is shown in Algorithm 2. The modification replaces line 6 to line 10 of Algorithm 2 as shown in Algorithm 3. Similar modifications can be applied to IPOG-F2.
Algorithm 2. Horizontal Growth of IPOG-F 1. Tc [r ; a] is the number of t-tuples covered by (r ; a) 2. Cov[, v] is true if the interaction with column tuple  and value tuple v is covered false otherwise 3. Tc [i; a]  0, i, a 4. Cov[, v]  false, , a 5. while some row is non-extended do 6. Find non-extended row i and value a -1 7. so that tn = k - Tc [i; a] is maximum t -1 8. if tn = 0 then 9. Stop horizontal growth 10. end if 11. Extend row i with value a 12. for all non-extended row j do 13. S  set of columns where rows i and j have identical entries 14. for all column tuples   S do 15. v  the value tuple in row i and column tuple  16. if Cov[, v] = false then 17. Tc [j ; a]  Tc [j ; a] + 1 18. end if 19. end for 20. end for 21. for all column tuples  do 22. v  the value tuple in row r and column tuple  23. if Cov[, v] = false then 24. Cov[, v]  true 25. end if 26. end for 27. end while

Algorithm 1 incurs additional time to track the previous value selected, but this small addition is dominated by the computation of coverage, and hence makes no change in the complexity of the method.

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO
Algorithm 3. Modification (Lines 610) 1. max  (prev + 1) mod q 2. j  (max + 1) mod q 3. while j = ((prev + 1) mod q ) do 4. if Tc [i; vj ] < Tc [i, vmax ] then 5. max  j 6. j  (j + 1) mod q 7. end if 8. end while 9. a  vmax -1 10. tn  k - Tc [, a] t -1 11. if tn = 0 then 12. Stop horizontal growth 13. end if 14. Extend row i with value a 15. prev  max Algorithm 4. Addressing don't care Positions 1. Number the values of Pi as v1 , v2 , . . . , v|Pi | 2. f req [Pi , j ] is the frequency of value vj of Pi appears in the existing test set 3. e is an entry in column i 4. if e is a don't care position then 5. Find min that f req [Pi , min] is minimum in f req [Pi , 1], . . . , f req [Pi , |Pi |] 6. Assign e with vmin 7. end if

961

3.2

Addressing don't care Positions

our balance strategy only examines frequencies. Savings are only incurred with the balance strategy when don't care positions arise during vertical growth. In both cases, the worst-case complexity is dominated by the cost of horizontal growth, so in principle the two methods have the same asymptotic complexity. However, in practice, every don't care position results in a saving in computation time for the balance strategy. 4 Reducing the Expected Time to Fault Detection

In horizontal growth, when the maximum number of interactions that the extended row (r; v ) can cover is 0, the value at this position is a don't care. The don't care positions can be addressed using the method of Subsection 3.1. In vertical growth, new rows that are created to cover the t-way combinations in  not covered by horizontal growth can leave positions not needed to cover interactions in  as don't care. The selection of these values can influence the extension for the remaining parameters. To exploit these don't care positions, one strategy focuses on coverage, and the other on balance. The balance strategy attempts to make values of all parameters distributed evenly: as each don't care arises, it is filled with a value for this parameter that currently appears the least often; ties are handled by taking the next in the cyclic order of values after the previous selection. The coverage strategy is greedy. Don't care positions produced in vertical growth are left unassigned until the next horizontal growth. Then a value is chosen so that the row covers the most uncovered interactions, using the method described in Subsection 3.1. Focusing on coverage is generally slightly superior in reducing the size of test suites. However, the balance strategy reduces the time to generate the test suite. Because of our interest in fault detection, and the fact that existing IPO variants use a coverage strategy, we adopt the balance strategy here. The pseudo-code for the balance strategy is shown in Algorithm 4. Vertical growth treating don't care positions using a coverage strategy examines all t-way interactions, while

In [30], a measurement of the goodness of a test suite at detecting a fault is defined. Suppose that every test takes the same time to run. Further suppose that faults are randomly distributed among the t-way interactions, and that there is no a priori information about their location. For a system with s faults, the expected time to fault detection is determined by the expected number of tests to detect the presence of a fault. s denotes the expected number of tests to detect the first fault in a system with s faults. s =
N ui i=1 s  s

.

Here ui is the number of uncovered interactions before executing the i-th row, N is the number of rows of the test suite, and  is the total number of t-way interactions. This measure applies to any test suite when faults arise randomly, and is not intended to examine particular patterns of faults in specific systems. As such, it can serve as a means to evaluate test suites for use in an as-yet-unknown application. Minimizing the expected time to fault detection means constructing a test suite to minimize s given s. Rather than constructing a test suite to minimize s directly, we can reorder the rows of a test suite to reduce s . Because all faults of interest are caused by parameter interactions, the more uncovered interactions con-

962

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

tained in the test, the more likely a fault is to be revealed. Hence placing the tests that cover the greatest number of the uncovered interactions early can increase the probability of detecting a fault. To see this, we rewrite the formula as follows: s =
N ui i=1 s  s N

=
i=1

ui s  s

.

Then the problem becomes minimizing the average i (u s ) value of  , the likelihood that all faults remain unde(s) tected after running i tests. The method for reordering the test suite is Algorithm 5. There may be a tie for row rj where Tc [rj ] is the largest -- if there is, the tie would be broken randomly.
Algorithm 5. Reordering Test Suites 1. n  N 2. for j from 1 to n do 3. 4. for each row r1 , . . . , rn Determine the number Tc [ri ] of t-way interactions covered in ri but not covered in r1 , . . . , ri-1 5. 6. 7. 8. 9. 10. 11. 12. 13. end for end for Choose a row rj from ri , . . . , rn for which Tc [rj ] is the largest if Tc [rj ] = 0 then Remove all rows ri , . . . , rn from the suite n i-1 else Swap ri and rj in the suite end if

5

Experiments

We employ the tool ACTS-2.8 (Advanced Combinatorial Testing System)[43] , including implementations of IPOG, IPOG-F and IPOG-F2, etc. We compare the tool ACTS-2.8 with our variants of IPOG, IPOG-F and IPOG-F2 in which the handling of don't care positions attempts to balance frequencies of values; our versions are coded in C++. All of the experimental results reported here are performed on a laptop with CoreTM 2 Duo Intel processor clocked at 2.60 GHz and 4 GB memory. 5.1 Test Suite Size and Execution Time

First we examine the relative performance for different numbers of values for the parameters. The notation dt indicates that there are t parameters, each with

d values. To start, we vary the number of values. Table 1 shows execution time and test suite sizes when the strength is 4, and there are five parameters whose number of values is 5, 10, 15, or 20. As expected, the execution time for our methods is substantially smaller (see Fig.2). What is more surprising is that our methods consistently produce test suites no larger than the original methods, and sometimes produce much smaller ones. Now we vary the number of parameters. Table 2 shows results when the strength is 4, the number of parameters is 10, 15, 20, or 25, and the number of values is 5. Again the execution time for our methods shows improvements (see Fig.3). However, as the number of parameters increases, the deferral in filling don't care positions by the original methods generally produces smaller test suite sizes. Now we vary the strength. Table 3 presents results for 106 when the strength is 2, 3, 4, or 5. Once again, the execution time for our methods is substantially lower (see Fig.4). Our methods do not fare as well with respect to test suite size, but appear to be very effective when the strength is larger. Our methods appear to improve execution time consistently as expected. Nevertheless, they also improve on test suite sizes in some cases, especially when the strength is large or the number of values is large. Real systems rarely have the same number of values for each parameter, so we also consider situations in which different parameters can have different numbers of values. Table 4 presents results with strength 4 for five different sets of numbers of values for 10 parameters. Execution time improvements again arise for our algorithms. Moreover, a pattern for test suite sizes is clear: our methods improve when there is more variation in numbers of values. Next we examine the relative performance using the Traffic Collision Avoidance System (TCAS), which has been utilized in several other studies of software testing[27,44-46] . TCAS has 12 parameters: seven parameters have two values, two parameters have three values, one parameter has four values, and two parameters have 10 values. Table 5 gives the results. (In [46], similar results for the original IPOG versions are given for the TCAS system.) While our improvements in execution time are evident, no obvious pattern indicates which method produces the smallest test suite. Our methods have simplified the manner in which don't care positions are treated in order to balance the frequencies of values. Our experimental results all con-

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO
Table 1. Results for Five Parameters with 5 to 20 Values for 4-Way Testing Parameter Config. 55 105 155 205 Our IPOG Size 000 745 011 990 058 410 184 680 Time (s) 0.001 0.078 1.101 9.666 IPOG(ACTS) Size Time (s) 000 790 000.015 012 298 000.827 061 945 016.329 191 652 120.220 Our IPOG-F Size Time (s) 000 625 000.000 010 000 000.673 050 625 018.469 160 000 200.020 IPOG-F(ACTS) Size Time (s) 000625 1 000.047 010 000 1 006.109 050 625 1 146.730 160 000 1 376.000 Our IPOG-F2 Size Time (s) 000 625 000.000 010 000 000.500 050 625 012.782 160 000 209.290

963

IPOG-F2(ACTS) Size Time (s) 100 788 1 000.031 112 394 1 004.859 161 615 1 184.450 192 082 1 966.200

140 120 100 Time (s) Time (s) 80 60 40 20 0 5 10 15 Domain Size (a) 20
Our IPOG IPOG(ACTS)

1400 1200 1000 800 600 400 200 0 5 10 15 Domain Size (b) 20 Time (s)
Our IPOG-F IPOG-F(ACTS)

2000
Our IPOG-F2 IPOG-F2(ACTS)

1500

1000

500

0

5

10 15 Domain Size (c)

20

Fig.2. Execution time, varying the number of values (4-way). (a) IPOG. (b) IPOG-F. (c) IPOG-F2. Table 2. Results for 10 to 25 5-Value Parameters for 4-Way Testing Parameter Config. 510 515 520 525 Our IPOG Size 1 890 2 584 3 114 3 540 Time (s) 0.056 0.517 2.140 7.012 IPOG(ACTS) Size 1 859 2 534 3 032 3 434 Time (s) 00.188 00.954 04.094 16.049 Our IPOG-F Size 1 833 2 461 2 951 3 338 Time (s) 000.625 007.109 034.361 111.150 IPOG-F(ACTS) Size 1 882 2 454 2 898 3 279 Time (s) 001.750 014.579 060.987 176.340 Our IPOG-F2 Size 1 965 2 736 3 308 3 763 Time (s) 0.187 1.282 4.329 8.752 IPOG-F2(ACTS) Size 1 905 2 644 3 180 3 589 Time (s) 0.297 1.421 4.344 9.188

10 15 Time (s) Our IPOG IPOG(ACTS) Time (s) 150 Our IPOG-F IPOG-F(ACTS) Time (s) 8 6 4 2 0 10 0 0 Our IPOG-F2 IPOG-F2(ACTS)

10

100

5

50

15 20 25 Number of Parameters (a)

10

15 20 25 Number of Parameters (b)

10

15 20 25 Number of Parameters (c)

Fig.3. Execution time, increasing the number of parameters (4-way). (a) IPOG. (b) IPOG-F. (c) IPOG-F2. Table 3. Results for Six 10-Value Parameters for 25-Way Testing t Our IPOG Size 2 000 149 3 001 633 4 016 293 5 123 060 Time (s) 0.000 0.010 0.195 5.139 IPOG(ACTS) Size 000 130 001 633 016 496 130 728 Time (s) 000.005 000.059 004.276 116.470 Our IPOG-F Size 000 133 001 577 015 594 100 000 Time (s) 00.000 00.047 02.704 88.692 IPOG-F(ACTS) Size 000 134 001 553 015 467 100 000 Time (s) 000.031 000.266 018.126 575.150 Our IPOG-F2 Size 000 135 001 629 015 631 100 000 Time (s) 00.000 00.032 01.594 54.971 IPOG-F2(ACTS) Size 000 134 001 625 016 347 132 428 Time (s) 000.016 000.140 009.297 449.330

964

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5
Table 4. Results for Five Systems with Different Numbers of Values in 4-Way Testing

Parameter Config. 1010 105 95 153 104 53 161 152 104 52 41

Our IPOG Size 29 915 23 878 41 128 42 913 Time (s) 1.942 1.295 1.734 1.750 1.844

IPOG(ACTS) Size 29 466 23 961 45 128 47 591 52 991 Time (s) 28.040 14.583 13.689 14.532 14.860
600

Our IPOG-F Size 28 437 22 521 41 505 43 774 48 847 Time (s) 129.57 094.27 236.87 249.37 235.95

IPOG-F(ACTS) Size 28 079 22 726 43 306 45 693 50 287 Time (s) 359.17 248.04 757.68 289.72 333.89

Our IPOG-F2 Size Time (s) 31 744 045.237 25 222 031.611 46 509 162.850 48 660 148.510 54 099 189.290

IPOG-F2(ACTS) Size 30 986 24 741 48 295 51 147 57 634 Time (s) 053.440 039.736 262.170 149.510 199.810

171 161 151 104 51 42 47 248
120 100 Time (s) 80 60 40 20 0 2

Our IPOG IPOG(ACTS)

500 Time (s) 400 300 200 100 0

Our IPOG-F IPOG-F(ACTS)

400 300 200 100 0

Our IPOG-F2 IPOG-F2(ACTS)

3 4 Strength of Coverage (a)

5

2

3 4 Strength of Coverage (b)

5

Time (s)

2

3 4 Strength of Coverage (c)

5

Fig.4. Execution time, increasing the test strength. (a) IPOG. (b) IPOG-F. (c) IPOG-F2. Table 5. Results for TCAS t 2 3 4 5 6 Our IPOG Size 00 100 00 404 01 306 04 464 11 774 Time (s) 0.001 0.009 0.065 0.411 1.463 IPOG(ACTS) Size 00 100 00 400 01 359 04 233 11 021 Time (s) 0.002 0.007 0.031 0.219 3.233 Our IPOG-F Size 00 100 00 400 01 269 04 068 11 381 Time (s) 00.002 00.025 00.323 04.104 32.870 IPOG-F(ACTS) Size 00 100 00 402 01 349 04 245 11 257 Time (s) 000.015 000.087 001.117 013.405 101.330 Our IPOG-F2 Size 00 100 00 431 01 639 05 129 13 323 Time (s) 00.004 00.044 00.489 04.133 18.030 IPOG-F2(ACTS) Size 00 100 00 438 01 653 05 034 13 379 Time (s) 00.017 00.061 00.572 04.379 20.959

firm that this can dramatically reduce the execution time. One might have expected a substantial degradation in the test suite sizes produced. However, our results indicate not only that the balancing strategy is competitive, but also that it can improve test suite sizes. Fast methods such as IPO do not generally produce the smallest test suites possible. To illustrate this, we apply a post-optimization method from [4748] to some of the TCAS results. For strength 4, we treat the solutions for IPOG-F2; within 10 minutes of computation, post-optimization reduces the solution by our method from 1 639 to 1 201 rows, and the solution by the original method from 1 653 to 1 205 rows. For strength 5, we treat the solutions for IPOG-F; within one hour of computation, post-optimization reduces the solution by our method from 4 068 to 3 600 rows, and the solution by the original method from 4 245 also to 3 600 rows. For strength 6, we treat the solutions

for IPOG; within 10 hours, post-optimization reduces the solution by our method from 11 774 to 9 794 rows, and the solution by the original method from 11 021 to 9 798 rows. By contrast, in a comparison of six different one-parameter-at-a-time methods[46] , the best result has 10 851 rows. While the test suites from oneparameter-at-a-time methods are therefore definitely not the smallest, post-optimization is much more timeconsuming and it requires a test suite as input. As the number of parameters increases, the speed with which an initial test suite can be constructed is crucial. 5.2 Expected Time to Fault Detection

Accelerating the IPO methods, even with a possible loss of accuracy in test suite size, can be worthwhile. However, a second concern is with potential performance in revealing faults. We examine the TCAS system, using our and the original versions of the three IPO variants. We examine the time to find the first

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO

965

fault when 1, 2, or 3 faults are present and when the strength is between 2 and 6. In our model, the time to execute each test is the same, so the expected time is directly proportional to the expected number of tests or rows needed. We consider test suites before and after our reordering. Table 6 gives the results. To assess the efficacy of our modifications, we report two lines for each method and each strength; the first reports results for our methods, and the second for the original methods. 1 , 2 and 3 denote the expected number of tests to detect the first fault when there are one, two or three faults that are randomly chosen. These results indicate that reordering is effective in reducing the time to fault detection, both for our met-

hods and for the original ones. Fig.5 shows 2 for each strength before and after the reordering for our methods, showing a substantial reduction from reordering. Fig.6 instead shows the expected number of tests when zero, one, two, or three faults are present. It appears that the reordering method is the most effective when the number of faults is small. This should be expected, because the presence of many faults ensures that one will be found early no matter what ordering is used. Our methods, despite often producing larger test suites, fare well with respect to expected time to fault detection. Comparing the performance of ours and the original IPOG when t = 6, for example, although our test suite is larger, it would yield smaller expected time to detect faults once reordered. Evidently the size of the

Table 6. Expected Time to Fault Detection for TCAS Before and After Reordering Algorithm t 1 Before IPOG 2 3 4 5 6 IPOG-F 2 3 4 5 6 IPOG-F2 2 3 4 5 6 00 24.80 00 24.81 0 117.90 0 117.68 0 407.20 0 408.42 1 348.26 1 348.14 3 015.32 3 007.69 00 28.36 00 27.19 0 120.96 0 120.94 0 411.37 0 411.97 1 353.42 1 354.28 3 076.17 3 017.29 00 26.44 00 26.27 0 120.61 0 121.04 0 419.07 0 421.75 1 378.15 1 377.84 3 129.21 3 138.02 After 00 19.65 00 19.65 00 82.15 00 82.12 0 275.38 0 276.34 0 850.74 0 848.20 2 127.94 2 140.04 00 20.43 00 20.67 00 81.47 00 81.34 0 272.86 0 269.36 0 828.83 0 822.73 2 090.57 2 059.33 00 20.52 00 20.19 00 82.75 00 81.63 0 275.05 0 278.10 0 844.54 0 838.77 2 127.62 2 121.97 Before 00 10.72 00 10.73 00 53.30 00 53.18 0 200.45 0 201.60 0 707.82 0 708.24 1 682.31 1 680.95 00 12.73 00 12.18 00 55.81 00 55.99 0 204.59 0 204.73 0 716.08 0 715.52 1 722.82 1 693.94 00 11.90 00 11.67 00 55.26 00 55.61 0 207.11 0 208.20 0 724.94 0 725.02 1 732.44 1 736.29 Number of Faults 2 After 000 9.26 000 9.26 00 38.57 00 38.47 0 131.93 0 132.73 0 421.49 0 421.30 1 095.54 1 106.03 000 9.58 000 9.67 00 38.33 00 38.18 0 132.18 0 129.68 0 411.92 0 410.22 1 065.49 1 063.99 000 9.75 000 9.56 00 38.36 00 38.15 0 130.39 0 131.82 0 412.79 0 409.71 1 068.73 1 062.64 Before 000 6.27 000 6.27 00 30.08 00 30.03 0 118.33 0 119.08 0 436.03 0 436.40 1 097.27 1 096.56 000 7.29 000 7.08 00 31.84 00 32.13 0 121.66 0 121.75 0 444.08 0 443.26 1 129.80 1 109.05 000 6.98 000 6.80 00 31.49 00 31.76 0 123.20 0 123.79 0 449.34 0 449.50 1 133.78 1 136.22 3 After 005.83 005.85 023.69 023.56 082.38 082.77 268.03 268.37 719.43 725.45 006.01 006.02 023.71 023.72 082.83 081.77 263.23 261.36 698.08 700.68 006.13 006.00 023.84 023.55 081.95 082.43 263.65 261.35 699.08 695.18

966
2000 Expected Number of Tests Expected Number of Tests 1500 1000 500 0 Before Reorder After Reorder 2000

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5
2000 Expected Number of Tests Before Reorder After Reorder 1500 1000 500 0 Before Reorder After Reorder

1500 1000 500 0

2

3 4 5 Test Strength (a)

6

2

3 4 5 Test Strength (b)

6

2

3 4 5 Test Strength (c)

6

Fig.5. Expected number of tests for 2 . (a) IPOG. (b) IPOG-F. (c) IPOG-F2.

Expected Number of Tests

Expected Number of Tests

Before Reorder After Reorder 1000

Before Reorder After Reorder 1000

Expected Number of Tests

1500

1500

2000 Before Reorder After Reorder

1500

1000

500

500

500

0

0

1 2 3 Number of Faults (a)

4

0

0

1 2 3 Number of Faults (b)

4

0

0

1 2 3 Number of Faults (c)

4

Fig.6. Expected number of tests, increasing the number of faults in TCAS (4-way). (a) IPOG. (b) IPOG-F. (c) IPOG-F2.

test suite, while relevant, is not the only factor affecting the expected time. Our results suggest that faster IPO implementations remain competitive, and hence that the objective of balancing frequencies of values is a reasonable one to pursue. 6 Conclusions

We identified three main goals in generating a test suite: time to generate the test suite, time to execute the test suite (test suite size), and the rate of fault detection. Our methods focus on reducing the time for generation, without severe negative impact on test suite size and fault detection. We accelerated variants of the IPO method by simplifying the manner in which don't care positions are filled. This results in a consistent improvement in the execution time to construct a test suite, but sacrifices to some extent the algorithm's ability to exploit such positions in repeated horizontal growth phases. This is reflected in our experimental results. While in numerous cases, our modifications find smaller test suites, in the others they do not. This occurs particularly when the number of parameters is large.

Any method to fill don't care positions immediately would be expected to accelerate the methods; however we devised a simple method that strives to balance the frequency of values for each parameter. We argued that such an objective can result in more effective horizontal growth, and that it can permit us to retain effective rates of fault detection. Both of these motivations are borne out by the experimental data. One-test-at-a-time generation methods explicitly aim for good rates of fault detection by covering interactions early in the test suite, while one-parameterat-a-time methods like IPO do not. Nevertheless, we showed that a reordering strategy can be applied to make dramatic improvement on the rate of fault detection. If test suite size is a primary objective, using our methods together with randomized postoptimization[47-48] appears to be worthwhile. If expected time to fault detection is paramount, extending reordering to discover and replace don't care positions appears to be viable. Both merit further study. We suggest that both can benefit from balancing frequencies of values, a fast and simple way to generate useful test suites.

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO

967

References
[1] Birnbaum Z W. On the importance of different components in a multicomponent system. In Multivariate Analysis, Krishnaiah P R (ed.), New York: Academic Press, 1969, pp.591-592. [2] Kuo W, Zhu X. Relations and generalizations of importance measures in reliability. IEEE Trans. Rel., 2012, 61(3): 659674. [3] Kuo W, Zhu X. Some recent advances on importance measures in reliability. IEEE Trans. Rel., 2012, 61(2): 344-360. [4] Anand S, Burke E K, Chen T Y et al. An orchestrated survey of methodologies for automated software test case generation. J. Sys. Software, 2013, 86(8): 1978-2001. [5] Chen T Y, Kuo F C, Liu H et al. Code coverage of adaptive random testing. IEEE Trans. Rel., 2013, 62(1): 226-237. [6] Grindal M, Offutt J, Andler S F. Combination testing strategies: A survey. Softw. Test. Verif. Rel., 2005, 15(3): 167-199. [7] Hao D, Zhang L M, Zhang L et al. A unified test-case prioritization approach. ACM Trans. Soft. Eng. Method, 2014, 24(2): 10:1-10:31. [8] Harman M, McMinn P. A theoretical and empirical study of search-based testing: Local, global, and hybrid search. IEEE Trans. Software Eng., 2010, 36(2): 226-247. [9] Nie C H, Leung H. A survey of combinatorial testing. ACM Comput. Surv., 2011, 43(2): 11:1-11:29. [10] Nebut C, Fleurey F, Le Traon Y et al. Automatic test generation: A use case driven approach. IEEE. Trans. Software Eng., 2006, 32(3): 140-155. [11] Perrouin G, Oster S, Sen S et al. Pairwise testing for software product lines: Comparison of two approaches. Software. Qual. J., 2012, 20(3/4): 605-643. [12] Xie T, Zhang L, Xiao X et al. Cooperative software testing and analysis: Advances and challenges. Journal of Computer Science and Technology, 2014, 29(4): 713-723. [13] Yoo S, Harman M. Regression testing minimization, selection and prioritization: A survey. Softw. Test. Verif. Rel., 2012, 22(2): 67-120. [14] Zhang D M, Xie T. Software analytics: Achievements and challenges. In Proc. the 35th Int. Conf. Software Eng., May 2013, p.1487. [15] Yu K, Lin M, Chen J et al. Towards automated debugging in software evolution: Evaluating delta debugging on real regression bugs from the developers' perspectives. J. Sys. Software, 2012, 85(10): 2305-2317. [16] Bryce R C, Colbourn C J. One-test-at-a-time heuristic search for interaction test suites. In Proc. the 9th Annu. Conf. Genetic and Evolutionary Computation, Jul. 2007, pp.1082-1089. [17] Kuhn D R, Reilly M J. An investigation of the applicability of design of experiments to software testing. In Proc. the 27th Annu. NASA Goddard Workshop on Software Eng., Dec. 2002, pp.91-95. [18] Kuhn D R, Wallace D R, Gallo Jr J A. Software fault interactions and implications for software testing. IEEE Trans. Software Eng., 2004, 30(6): 418-421. [19] Cohen M B, Dwyer M B, Shi J. Constructing interaction test suites for highly-configurable systems in the presence of constraints: A greedy approach. IEEE Trans. Software Eng., 2008, 34(5): 633-650.

[20] Lei Y, Kacker R, Kuhn D R et al. IPOG/IPOG-D: Efficient test generation for multi-way combinatorial testing. Softw. Test. Verif. Rel., 2008, 18(3): 125-148. [21] Tung Y W, Aldiwan W S. Automating test case generation for the new generation mission software system. In Proc. IEEE Aerospace Con., March 2000, pp.431-437. [22] Wallace D R, Kuhn D R. Failure modes in medical device software: An analysis of 15 years of recall data. Int. J. Rel., Quality and Safety Eng., 2001, 8(4): 351-371. [23] Cohen D M, Dalal S R, Kajla A et al. The automatic efficient tests generator (AETG) system. In Proc. the 5th Int. Sympo. Software Rel. Eng., Nov. 1994, pp.303-309. [24] Bryce R C, Colbourn C J. The density algorithm for pairwise interaction testing. Softw. Test. Verif. Rel., 2007, 17(3): 159-182. [25] Bryce R C, Colbourn C J. A density-based greedy algorithm for higher strength covering arrays. Softw. Test. Verif. Rel., 2009, 19(1): 37-53. [26] Lei Y, Tai K C. In-parameter-order: A test generation strategy for pairwise testing. In Proc. the 3rd Int. Symp. HighAssurance Sys. Eng., Nov. 1998, pp.254-261. [27] Lei Y, Kacker R, Kuhn D R et al. IPOG: A general strategy for t-way software testing. In Proc. the 14th Annu. Int. Conf. Worshop. Eng. Computer-Based Sys., March 2007, pp.549-556. [28] Forbes M, Lawrence J, Lei Y, Kacker R N, Kuhn D R. Refining the in-parameter-order strategy for constructing covering arrays. Journal of Research of the National Institute of Standards and Technology, 2008, 113(5): 287-297. [29] Cohen M B, Gibbons P B, Mugridge W B et al. Constructing test cases for interaction testing. In Proc. the 25th Int. Conf. Software Eng., May 2003, pp.38-48. [30] Bryce R C, Colbourn C J. Expected time to detection of interaction faults. J. Combin. Mathematics and Combin. Comput., 2013, 86: 87-110. [31] Rothermel G, Untch R H, Chu C et al. Prioritizing test cases for regression testing. IEEE Trans. Software Eng., 2001, 27(10): 929-948. [32] Qu X, Cohen M B. A study in prioritization for higher strength combinatorial testing. In Proc. the 6th Int. Con. Software Testing, Verification and Validation, the 2nd Int. Workshops on Combinatorial Testing, March 2013, pp.285294. [33] Qu X. Configuration aware prioritization techniques in regression testing. In Proc. the 31st Int. Conf. Software Engineering, Companion Volume, May 2009, pp.375-378. [34] Qu X, Cohen M B, Rothermel G. Configuration-aware regression testing: An empirical study of sampling and prioritization. In Proc. Int. Symp. Software Tesing and Analysis, July 2008, pp.75-86. [35] Bryce R C, Colbourn C J. Test prioritization for pairwise interaction coverage. In Proc. the 1st Int. Workshop on Advances in Model-Based Testing, May 2005. [36] Bryce R C, Colbourn C J. Prioritized interaction testing for pair-wise coverage with seeding and constraints. Inform. Software Tech., 2006, 48(10): 960-970. [37] Huang R, Chen J, Li Z, Wang R, Lu Y. Adaptive random prioritization for interaction test suites. In Proc. the 29th Symp. Appl. Comput., March 2014, pp.1058-1063.

968
[38] Petke J, Yoo S, Cohen M B, Harman M. Efficiency and early fault detection with lower and higher strength combinatorial interaction testing. In Proc. the 12th Joint Meeting on European Software Engineering Conf. and the ACM SIGSOFT Symp. the Foundations of Software Eng. (ESEC/FSE 2013), August 2013, pp.26-36. [39] Qu X, Cohen M B, Woolf K M. Combinatorial interaction regression testing: A study of test case generation and prioritization. In Proc. the 23rd Int. Conf. Software Maintenance, Oct. 2007, pp.255-264. [40] Huang R, Chen J, Zhang T, Wang R, Lu Y. Prioritizing variable-strength covering array. In Proc. the 37th IEEE Annu. Computer Software and Applications Conf., July 2013, pp.502-511. [41] Huang R, Xie X, Towey D, Chen T Y, Lu Y, Chen J. Prioritization of combinatorial test cases by incremental interaction coverage. Int. J. Softw. Eng. Know., 2014, 23(10): 1427-1457. [42] Bryce R C, Memon A M. Test suite prioritization by interaction coverage. In Proc. Workshop on Domain Specific Approaches to Software Test Automation, September 2007, pp.1-7. [43] Lei Y, Kuhn D R. Advanced combinatorial testing suite (ACTS). http://csrc.nist.gov/groups/SNS/acts/index.html, Aug. 2015. [44] Hutchins M, Foster H, Goradia T et al. Experiments of the effectiveness of dataflow and control-flow-based test adequacy criteria. In Proc. the 16th Int. Conf. Software Eng., May 1994, pp.191-200. [45] Kuhn D R, Okun V. Pseudo-exhaustive testing for software. In Proc. the 30th Annu. IEEE/NASA Software Engineering Workshop, April 2006, pp.153-158. [46] Soh Z H C, Abdullah S A C, Zamil K Z. A distributed tway test suite generation using "One-Parameter-at-a-Time" approach. Int. J. Advance Soft Compu. Appl., 2013, 5(3): 91-103. [47] Li X, Dong Z, Wu H et al. Refining a randomized postoptimization method for covering arrays. In Proc. the 7th IEEE Int. Conf. Software Testing, Verification and Validation Workshops (ICSTW), March 31-April 4, 2014, pp.143152. [48] Nayeri P, Colbourn C J, Konjevod G. Randomized postoptimization of covering arrays. Eur. J. Combin., 2013, 34(1): 91-103.

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

Jiang-Hua Lv received her B.S. and Ph.D. degrees in computer science from Jilin University, Changchun, in 1998 and 2003, respectively. Currently she is an assistant professor in the School of Computer Science and Engineering of Beihang University, Beijing. She is a member of the State Key Laboratory of Software Development Environment of Beihang University. Her research focuses on formal theory and technology of software, theory and technology of testing, automatic testing of safety critical systems, and device collaboration. Bing-Lei Du is currently an undergraduate in the School of Computer Science and Engineering of Beihang University, Beijing, and has been an intern in the State Key Laboratory of Software Development Environment of Beihang University since 2013. His research interest is software testing. Charles J. Colbourn earned his Ph.D. degree in computer science from the University of Toronto in 1980, and is a professor of computer science and engineering at Arizona State University. He is the author of The Combinatorics of Network Reliability (Oxford), Triple Systems (Oxford), and 320 refereed journal papers focusing on combinatorial designs and graphs with applications in networking, computing, and communications. In 2004, he was awarded the Euler Medal for Lifetime Research Achievement by the Institute for Combinatorics and its Applications. Shi-Long Ma is currently a professor and doctor tutor of the School of Computer Science and Engineering of Beihang University, Beijing. He is a member of the State Key Laboratory of Software Development Environment of Beihang University. His main research focus is on computation models in networks, logic reasoning and behaviors in network computing, and the theory of automatic testing.

Shi-Wei Gao received his B.S. degree in computer science and technology from Dezhou University, Dezhou, in 2007, and M.S. degree in information science and engineering from Yanshan University, Qinhuangdao, in 2010. He is currently a Ph.D. candidate in the School of Computer Science and Engineering of Beihang University, Beijing. He is a member of the State Key Laboratory of Software Development Environment of Beihang University. His research interests include software testing, software reliability theory, and formal methods.

Locating Arrays: A New Experimental Design for Screening Complex Engineered Systems
Abraham N. Aldaco, Charles J. Colbourn, and Violet R. Syrotiuk
School of Computing, Informatics, and Decision Systems Engineering Arizona State University, Tempe, AZ, U.S.A. 85287-8809

{aaldacog, colbourn, syrotiuk}@asu.edu ABSTRACT
The purpose of a screening experiment is to identify significant factors and interactions on a response for a system. Engineered systems are complex in part due to their size. To apply traditional experimental designs for screening in complex engineered systems requires either restricting the factors considered, which automatically restricts the interactions to those in the set, or restricting interest to main effects, which fails to consider any possible interactions. To address this problem we propose a locating array (LA) as a screening design. Locating arrays exhibit logarithmic growth in the number of factors because their focus is on identification rather than on measurement. This makes practical the consideration of an order of magnitude more factors in experimentation than traditional screening designs. We present preliminary results applying an LA for screening the response of TCP throughput in a simulation model of a mobile wireless network. The full-factorial design for this system is infeasible (over 1043 design points!) yet an LA has only 421 design points. We validate the significance of the identified factors and interactions independently using the statistical software JMP. Screening using locating arrays is viable and yields useful models. Screening: Which factors and interactions are most influential on a response? Confirmation: Is the system currently performing in the same way as it did in the past? Discovery: What happens when new operating conditions, materials, factors, etc., are explored? Robustness: Under what conditions does a response degrade? Stability: How can variability in a response be reduced? Our focus is on screening using techniques from statistical design of experiments (DoE). DoE refers to the process of planning an experiment so that appropriate data are collected and analyzed by statistical methods, in order to result in valid and objective conclusions. Hence any experimental problem includes both the design of the experiment and the statistical analysis of the data. Suppose that there are k factors, F1 , . . . , Fk , and that each factor Fj has a set Lj = {vj,1 , . . . , vj, j }, of j possible levels (or values). A design point is an assignment of a level from Lj to Fj , for each factor j = 1, . . . , k. An experimental design is a collection of design points. When a design has N design points, it can be represented by an N × k array A = (ai,j ) in which each row i corresponds to a design point and each column j to a factor; the entry ai,j gives the level assigned to factor j in the ith design point. When run, a design point results in one or more observable responses. A t-way interaction (or interaction of strength t) in A is a choice of t columns i1 , . . . , it , and the selection of a level ij  Lij for 1  j  t, represented as T = {(ij , ij ) : 1  j  t}. Every interactions of strength t. design point in A covers k t When the objective of experimentation is screening, it is often recommended to keep the number of factors low. It has been considered impractical to experiment with "many" factors; about ten factors is a suggested maximum [23, 31]. Generally, two levels for each factor is considered to work well in screening experiments. Methods for screening seek to reduce the number of design points required because the exhaustive full-factorial design [9, 31] is too large. For k factors each with two levels it has 2k design points. An analysis of variance (ANOVA) allows the significant factors and interactions on the response to be identified.
-p A fractional factorial design 2k is a 21 p fraction of a full factoR rial design with k two-level factors. The design is described by p

Categories and Subject Descriptors
General and reference [Cross-computing tools and techniques]: Experimentation; Mathematics of computing [Discrete mathematics]: Combinatorics

General Terms
Experimentation

Keywords
Screening experiments, Locating arrays

1.

INTRODUCTION

Computer and networked systems are examples of complex engineered systems (CESs). The complexity of an engineered system is not just due to its size, but also arises from its structure, operation (including control and management), evolution over time, and that people are involved in its design and operation [35]. Experimentation is often used to study the performance of CESs. At its most basic, a system may be viewed as transforming some input variables, or factors, into one or more observable output variables, or responses. Some factors of a system are controllable, whereas others are not. Objectives of experimentation include: Copyright is held by the authors.

31

generators, expressions of factors that are confounded; the generators determine the alias structure. A design is of resolution R if no m-factor effect is aliased with another effect containing fewer than R - m factors. A D-optimal design is a popular experimental design among those using optimality criteria. A model to fit, and a bound N on the number of design points, must be specified a priori; this restricts the factors to be analyzed to those in the model. The size of a Doptimal design is bounded by the size of a full-factorial design. Some designs aggregate the factors into groups, e.g., sequential bifurcation [24], to improve design efficiency. Grouping requires care to ensure that factor effects do not cancel. This presents a "chicken and egg" problem: we need to know how to group in order to group. Often, a domain expert is expected to make such grouping decisions. While such experts may have considerable knowledge, it is doubtful whether an expert knows the importance of a specific factor or interaction in a CES. An interaction graph depicts how a change in the level of one factor affects the other factor with respect to a response. Figure 1 shows an interaction graph for the factors of routing and medium access control (MAC) protocol on average delay in a network. The choice of MAC protocol (EDCF or IEEE 802.11) has little impact on the average delay in the AODV routing protocol, while for the DSR routing protocol the impact is very large; see [53]. If MAC protocols were aggregated, this significant interaction would be lost.
-0.2 Log10(Average delay) -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1 AODV DSR Routing protocol EDCF IEEE 802.11

tential to transform experimentation in huge factor spaces such as those found in CESs. The rest of this paper is organized as follows. §2 defines a locating array, and gives an example of how a design is used for location. §3 presents preliminary results applying an LA for screening the response of TCP throughput in a simulation model of a mobile wireless network. The full-factorial design for this system is infeasible -- it has over 1043 design points! Yet there is an LA with only 421 design points. We develop an algorithm using the LA to identify the significant factors and interactions from the data collected, providing a small example. In §4 we validate the significance of the identified factors and interactions independently using the statistical software JMP. Finally, in §5 we summarize, discuss potential threats to our approach, directions for this research, and conclude.

2.

LOCATING ARRAYS

Reducing the number of design points required relies on a sparsity of effects assumption, that interactions of interest involve at most a small, known number t of interacting factors. As one means of reduction, we define locating arrays (LAs) [8]. For a set of factors each taking on a number of levels, an LA permits the identification of a small number of significant interactions among small sets of (factor, level) combinations. LAs differ from standard designed experiments, which are used to measure interactions and to develop a model for the response as a function of these [31]. "Search designs" [17, 48, 49] also attempt to locate interactions of higher strength, but their focus remains on measurement and hence on balanced designs. Rao [20] shows that the number of design points in a balanced design must be at least as large as the number of interactions considered. Thus if t-way interactions among k factors each having v levels are to be examined, balanced designs only reduce the v k exhaustive design points to O(kt ). The selection of few factors from hundreds of candidates by this reduction is not viable. By lessening the requirement from measurement to identification, LAs are not subject to the Rao bound. Fortunately LAs behave more like covering arrays, experimental designs in which every t-way interaction among factors appears in at least one design point. Unlike designed experiments, the number of design points in a covering array for k factors grows as a logarithmic function of k (see [43], for example). In [8], a construction of LAs using covering arrays of higher strength is given, and hence LAs also exhibit this logarithmic growth, making them asymptotically much more efficient than balanced designs. This motivates the consideration of covering arrays, which have been the subject of extensive study [4, 5, 19, 36]. They are used in testing software [10,13,25,26], hardware [46,50], composite materials [3], biological networks [44, 47], and others. Their use to facilitate location of interactions is examined in [29, 56], and measurement in [21, 22]. Covering arrays form the basis for combinatorial methods to learn an unknown classification function using few evaluations -- these arise in computational learning and classification, and hinge on locating the relevant attributes (factors) [11]. Algorithms for generating covering arrays range from greedy (e.g., [2, 16]) through heuristic search (e.g., [38, 52]). However, combinatorial constructions (see [5]) provide the only available deterministic means of producing covering arrays with more than a few hundred factors. A design point, when run, yields one or more responses. For ease of exposition, we classify the responses in two groups, those that

Figure 1: Interaction of routing and MAC protocols on delay [53]. A fractional factorial design is saturated when it investigates k = N - 1 factors in N design points [31]. In a supersaturated design, the number of factors k > N - 1; such designs contain more factors than design points. These designs are only able to estimate a main effects model [27, 31]. Thus they cannot consider possible interactions at all. Even with substantial and detailed domain knowledge, it is imperative not to eliminate or aggregate factors a priori. Our goal, therefore, is an automatic and objective approach to screening. To address this problem we have formulated the definition of a locating array (LA) [8]. Locating arrays exhibit logarithmic growth in the number of factors because their focus is on identification rather than on measurement. This makes practical the consideration of an order of magnitude more factors in experimentation, removing the need for the elimination of factors. As a result, LAs have the po-

32

exceed a specified threshold and those that do not. So we suppose that the outcome of a run of a design point is a single binary response ("pass" or "fail"). A fault is caused by one or more t-way interactions, and is evidenced by a run failing. Given an experimental design and the set of interactions that cause faults, the outcomes can be easily calculated: A run fails exactly when it contains one or more of the faulty interactions, and does not fail otherwise. In order to observe a fault, the interaction must be covered by at least one design point. With no restriction on the interactions that can cause faults, every interaction must be covered. Then the best one can do is to form all k j =1 j possible design points, the exhaustive design. Using sparsity of effects, an upper bound t is placed on the strength of interactions that may be faulty. Then we require that every t-way interaction be covered; in other words, the design is a covering array of strength t. Let A = (ai,j ) be an experimental design, an N × k array where in each row i, levels in the j th column are chosen from a set Lj of size j . For array A and t-way interaction T = {(ij , ij ) : 1  j  t}, define (A, T ) = {r : ar,ij = ij , 1  j  t} as the set of rows of A in which T is covered. For a set T of interactions, (A, T ) = T T (A, T ). Locating faults requires that T be recovered from (A, T ), whenever T is a possible set of faults. Let It be the set of all t-way interactions for an array, and let It be the set of all interactions of strength at most t. Consider an interaction T  It of strength less than t. Any interaction T of strength t that contains T necessarily has (A, T )  (A, T ). In this case, when T is faulty we are unable to determine whether or not T is also faulty. Call a subset T of interactions in It independent if there do not exist T, T  T with T  T . In general, some interactions in It (or perhaps It ) are believed to be faulty, but their number and identity are unknown. The faulty interactions cannot be identified precisely from the outcomes, even if the full factorial design is employed, without some restriction on their number. (Consider the situation in which every design point run fails.) We therefore suppose that a maximum number d of faulty interactions is specified. D EFINITION 2.1 ( [8]). An array A is (d, t)-locating if whenever T1 , T2  It and T1  T2 is independent, |T1 |  d, and |T2 |  d, it holds that (A, T1 ) = (A, T2 )  T1 = T2 . If there is any set of d interactions of strength t that produce exactly the outcomes obtained when using a (d, t)-locating array A to conduct experiments, then there is exactly one such set of interactions. To avoid enumeration of all sets of d interactions of strength t, one can employ a stronger condition that for every interaction T of strength at most T and every set T1  It that does not contain T and for which T1  {T } is independent, it holds that (A, T ) = (A, T1 )  T  T1 . A locating array meeting this stronger condition is termed a detecting array in [8]. When using a detecting array, if there are at most d independent faulty interactions each of strength at most t, they are characterized precisely as the interactions that appear in no run that passes. We typically employ the term locating array to refer to both, but for reasons of computational efficiency the locating arrays that we use are, in fact, detecting arrays. In practice, one does not know a priori how many interactions are faulty, or their strengths. Nevertheless, when responses are contin-

uous, we can select a threshold on the responses so as to limit the number of design points yielding a "fail" outcome to locate those that make the most substantial contribution to the response. We exploit this fact later in §3.2.

2.1

A Small Example

An example is provided to demonstrate fault location, and show the limitations of covering arrays for this purpose. Suppose that we use the experimental design for five binary factors in Table 1. It is a covering array in which each of the 22 5 = 40 two-way 2 interactions is covered. A response for each design point run is listed in the adjacent column. Table 1: Experimental design and response for each run. Factors 3 4 1 1 1 0 0 0 0 1 0 0 0 1

Design Points

1 2 3 4 5 6

1 0 1 0 1 0 1

2 1 0 1 0 0 1

5 1 0 0 1 1 0

Response Fail Pass Fail Pass Pass Pass

First, let us locate faults due to main effects (i.e., the individual factors or one-way interactions). The second design point run passes, so all (factor, level) pairs in it are known not to be faulty. Therefore in Table 2(a), that considers only the second design point, when factor 1 is set to one, the run is not faulty. Similarly, for factors 2, 3, 4, and 5 set to zero, one, zero, and zero, respectively. This is indicated by a check-mark ( ) in the table. Repeating to check coverage of each one-way interaction for each successful run, no single (factor, level) error accounts for the faults; see Table 2(b). Table 2: Locating faults due to main effects. (a) Run 2 Factors 0 1 1 2 3 4 5 (b) All Runs Factors 0 1 1 2 3 4 5

Computing (T ) for every one-way interaction, we obtain the sets in Table 3. Because no two sets are equal, the array is (1, 1)locating and when there is a single faulty one-way interaction it can be located. However, because {1, 3, 5}  {2, 3, 5} = {1, 3, 5}  {1, 2}, when rows 1, 3, and 5 fail and 2, 4, and 6 pass, we cannot determine the two faulty interactions -- the array is not (2, 1)locating. Table 3: (T ) for one-way interactions T = {(c,  )}. c 0 1 1 {1,3,5} {2,4,6} 2 {2,4,5} {1,3,6} 3 {3,4,5,6} {1,2} 4 {2,3,5} {1,4,6} 5 {2,3,6} {1,4,6}

Now, let us try to locate faults due to two-way interactions. Because the second design point run passes, all two-way interactions in it are known not to be faulty; Table 4(a) records the results. Repeating to check for coverage of each two-way interaction for each successful run, those interactions not found to pass in this way in

33

Table 4: Locating faults due to two-way interactions. (a) Run 2 00 01 10 (b) All Runs 00 01 10

Factors 1, 2 1, 3 1, 4 1, 5 2, 3 2, 4 2, 5 3, 4 3, 5 4, 5

11

Factors 1, 2 1, 3 1, 4 1, 5 2, 3 2, 4 2, 5 3, 4 3, 5 4, 5

11

Table 4(b) form a set of candidate faults. In this example, there are nine interactions in the set of candidate faults. Now for the twoway interaction {(1, 0), (2, 1)}, ({(1, 0), (2, 1)}) = {1, 3}, and it is the only two-way interaction for which this holds; and, no oneway interaction T has (T ) = {1, 3}. Hence if there is a single fault, it must be {(1, 0), (2, 1)}, and we have located the fault. Our success for one response is not sufficient, however. Because ({(1, 0), (2, 1)}) = {1} = ({(2, 1), (3, 1)}), if only run 1 fails, there are at least two equally plausible explanations using only a single two-way interaction. Indeed A is not (1, 2)-locating. Thus the ability to locate is more than simply coverage!

of levels for each factor to the desired number, eliminating rows in the process and forming an array C with 143 design points. The resulting array provides coverage of two-way interactions but does not support location. When T and T are interactions, to distinguish them we require that (T ) = (T ), but we ask for more, namely that |(T ) \ (T )|  2 and |(T ) \ (T )|  2; this ensures that for every two interactions of interest, there are at least two design points containing one but not the other. To accomplish this, we formed three copies of C , randomly permuted their symbols within each column, and formed their union (so that every two-way interaction is covered at least three times). The resulting array B with 429 rows turned out to be (1, 2)-detecting. Three rows were selected by a greedy method to ensure the stronger condition that |(T ) \ (T )|  2 for every pair T, T of interactions; then eleven rows were deleted by a greedy algorithm to remove redundant rows, ultimately producing a design with 421 rows. Appendix A gives a pointer to the locating array used as the experimental design. Our objective was not to find the smallest possible array, because a fair evaluation of the efficacy of locating arrays should not rely on substantial additional structure being present. Ten replicates of each design point in the LA are run in ns-2; for each a response of TCP throughput is measured. These are averaged for each design point resulting in a vector with 421 entries of observed average TCP throughput obsT h.

3.2

Screening Algorithm

3.

SCREENING AN ENGINEERED SYSTEM

We now apply locating arrays for screening in a complex engineered system. One example of a CES for which it has been particularly difficult to develop models is a mobile ad hoc network (MANET). A MANET is a collection of mobile wireless nodes that self-organize without the use of any fixed infrastructure or centralized control. We seek to use a locating array to screen for the influential factors and interactions on average transport control protocol (TCP) throughput in a simulation model of a MANET.

We describe an algorithm for screening at a high level to facilitate understanding. In each iteration of the algorithm the most significant main effect or two-way interaction is identified. These terms are accumulated in a screening model of average TCP throughput. However, this screening model is not intended as a predictive model; the quality of its current estimate allows the algorithm to select the next most significant term. The screening model is used only to identify influential main effects and two-way interactions. With its output, a predictive model can be built; see §4. Initially, the screening model has no terms. With no other information, it should estimate the average TCP throughput to be the average of the vector of observed average throughput. This is unlikely to be a very good estimation! Our strategy to identify the most significant factor or interaction as the term to add to the screening model is as follows. Suppose that factor Fj , 1  k  75, has j levels Lj = {vj,1 , . . . vj, j }. For each level , 1   j , of factor Fj iterate through each of the 421 design points of the locating array A. For each design point i, 1  i  421, partition the contribution of the (factor Fj , level vj, ) combination into one of two sets: S or S . If the design point has the factor Fj set to level , i.e., ai,j = vj, , then add the throughput measured for design point i, obsT h[i], to S ; otherwise add obsT h[i] to S . Then, compute the (absolute) difference of the average of sets S and S . (Of course, metrics other than the difference of averages could be used.) Either the difference is zero (i.e., the average TCP throughput collected in the sets S and S is the same), or it is non-zero. If the difference is non-zero, then one possible explanation is that the (factor Fj , level vj, ) combination is responsible for the difference. Our hypothesis is that the (factor Fj , level vj, ) combination over all combinations for which the difference between the sets is the greatest is the most significant one. If this is correct, then a term of the form c · (Fj , vj, ) is added to the screening model. The

3.1

Designing the Experiment

We use the ns-2 simulator [37], version 2.34, for our experimentation. Since our response of interest is average TCP throughput, we select the file transfer protocol (FTP) as our application because it uses TCP for reliability. We select the internet protocol (IP), the Ad hoc On-demand Distance Vector routing protocol (AODV) [42], and IEEE 802.11b direct sequence spread spectrum (DSSS) as protocols at the network, data link, and physical layers of the protocol stack. We also use the mobility, energy, error, and propagation models in ns-2. From these protocols and models we identify 75 controllable factors. The region of interest for each factor, i.e., the range over which the factor is varied, ranges from two to ten levels, with some set according to recommendations in [33]. See Appendix A for a pointer to details of the factors and their levels. The full-factorial design for this factor space is infeasible; it has over 1043 design points! In contrast, the locating array constructed and checked manually has only 421 design points. Except for small locating arrays [51], no general construction methods have been published. We adopted a heuristic approach to construct the LA. Initially we selected a covering array with 75 factors and 10 levels per factor, constructed using a standard product construction [7]. We applied a post-optimization method [34] to reduce the number

34

coefficient c is equal to the difference in average TCP throughput of each set. When this term is added to the screening model, it makes the same estimation for average TCP throughput for sets S and S . In the first iteration of this algorithm, the estimate (i.e., the average of the vector of observed average TCP throughput) is used to determine deviations from each entry in the vector obsT h. We now have a screening model that apparently includes the most significant factor. It is now used to produce a new estimate of average TCP throughput and update the vector of residual throughput. The algorithm can be applied repeatedly to the residuals to identify the next most important factor or interaction. While this algorithm is described for (factor, level) combinations, we actually iterate over all one-way (i.e., all (factor, level) combinations) and all two-way interactions (i.e., all pairs of (factor, level) combinations) to identify the main effect or two-way interaction of highest significance. Any number of stopping conditions may be used to decide when to terminate the model development. We use the R2 , the coefficient of determination, indicating how well data fits a line or curve; when it shows marginal improvement, we stop. The locating array constructed for our CES is a (d = 1, t = 2)locating array, meaning it only guarantees to be able to locate (identify) at most one (d = 1) main effect or two-way (i.e., up to t = 2way) interaction. It is interesting that the LA may be used iteratively to identify subsequent significant main effects or interactions. In this sense, the algorithm uses a "heavy-hitters" approach as in compressive sensing [6].

design point with this initial fitted value. Now, we iterate over each (factor,level) combination. Factor 1 is set 4 to its low level in design points 1­4. Therefore S = 1 1 resT h[i] = 4 8 -134347 1 134344 = - 33586 and S = resT h [ i ] = = 33586. 5 4 4 4 The absolute difference, |S - S | = |-33586 - 33586| = 67172. Repeating for each (factor, level) combination, as well as all twoway interactions, we find that it is a main effect that has highest absolute difference with a value of 131255. It occurs when factor 3 is set to its lowest level, namely when the number of flows at the application layer is only one. Hence we attribute this as the explanation for the largest difference and add the term c · (F3 , v3,0 ) to the model. The method of ordinary least squares (OLS) is used to fit the intercept and coefficient c of the new term. This results in an updated model of T = 12410 + 131255 · (F3 , v3,0 ). Its coefficient of determination is R2 = 0.33. Using this updated model, the residuals can be recomputed as input to the next iteration of the algorithm. Next, we describe some of the obstacles arising in the practical application of the screening algorithm.

3.4

Applying the Screening Algorithm

3.3

Example of the Screening Algorithm

A small example is provided to step through one iteration of the screening algorithm. Suppose that we use the experimental design for four binary factors in Table 5. It is a covering array of strength three and therefore also a (2, 1)-detecting array. Factor 1 corresponds to the distribution function used for introducing errors (uniformly or exponentially distributed), factor 2 to the error rate (10-7 or 10-5 ), factor 3 to the number of flows at the application layer (1 or 18), and factor 4 to the TCP packet size (64 or 2048); the levels are taken as "binary" for this example. All remaining factors are set to their default levels for experimentation. A response of observed TCP throughput for each design point, averaged over ten replicates, is listed in the column obsT h. (All measures are truncated to integers for simplicity.) Table 5: Experimental design and average TCP throughput. Factors 2 3 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1

In applying the screening algorithm to our CES, several obstacles arose. The first is that the measured average TCP throughput is not normally distributed, as Figure 2 shows; this is not uncommon in systems experimentation [12]. The best transformation of the data is a natural logarithm (Figure 3a). From the normal probability plot (Figure 3b), we find that the transformed data are still not normally distributed; nevertheless, we work with this transformation of the data.
72%

300

0.985

0.94
250

0.88 Normal Probability
18% 5% 2% 1% 1% 0% 0% 0%

0.75

200 Frequency

0.5

150

0.25 0.12

100

0.06

50

0.015 0.003
0%

0

50,000

100,000

150,000

200,000

250,000

0

50,000

100,000 obsTH1

150,000

200,000

250,00

obsTH1

(a) Throughput distribution.

(b) Normal probability plot.

Figure 2: Distribution of the original observed average throughput, and corresponding normal probability plot. 4 0 1 1 0 1 0 0 1 obsT h 63339 29860 80801 3804 373866 3879 56656 12095 resT h -14699 -48178 2764 -74234 295828 -74159 -21382 -65943 A much larger problem arises from the fact that the LA does not cover each main effect and two-way interaction the same number of times. Indeed, binary factors are covered much more frequently (some as many as two hundred times in the 421 row LA) compared to two-way interactions of factors with ten levels (only a handful of times). This is unavoidable when one-way and two-way interactions are compared, and when factors have a different numbers of levels. Consider the behaviour of the screening algorithm. For a binary factor the sets S and S have the same or nearly the same size and, as a result, the average of each set has small variance. In the example in §3.3, each (factor, level) combination is covered four times

1 2 3 4 5 6 7 8

1 0 0 0 0 1 1 1 1

The overall mean of the obsT h is 78038. Therefore, the screening model initially estimates this value for average TCP throughput, i.e., T = 78038. The residuals (resT h) are computed in Table 5 by taking the difference of the observed average throughput for each

Design Points

35

26%

0.985

24%

100

0.985

100

0.94
21%

0.94 0.88
80
18%

0.88
Normal Probability
9% 6% 5% 1%

Normal Probability

80

0.75
Frequency

0.75

Frequency

0.5

60
13% 12%

0.5

60

14%

0.25
40

0.25 0.12 0.06

40
8% 7% 6% 5% 5% 4%

0.12 0.06
20

8%

20

0.015 0.003 2 4 6 ln(obsTH1 8 10 12

0.015 0.003 -6
-6 -4 -2 0 2 Residuals of TCP throughput after iteration 1 4

1%

1% 1%

1% 0%

1%

-4

-2

0

2

4

2

4

6 8 ln(obsTH1)

10

12

Residuals of TCP throughput after iteration 1

(a) ln transformation.

(b) Normal probability plot.

(a) Distribution of first residual.

(b) Normal probability plot.

Figure 3: Natural logarithm transformation of the original observed throughput, and corresponding normal probability plot.

Figure 4: Distribution of residuals after the first iteration of the screening algorithm, and corresponding normal probability plot. intercept and i is the coefficient of term i, 1  i  12. Table 6: Screening model with twelve terms. t-Test 52.6 34.5 32.8 -29.1 -11.8 -12.1 -9.3 6.5 6.6 8.4 6.3 5.5 5.2 i 5.6 4.4 4.0 -4.7 -1.6 -1.5 -1.2 0.9 0.7 1.1 1.1 0.7 0.5 Factor or interaction, and level(s) ErrorModel_ranvar_ U nif orm ErrorModel_unit_ pkt) (ErrorModel_ranvar_ U nif orm) * (ErrorModel_unit_ pkt) TCP_packetSize_ 64 MAC_RTSThreshold_ 0 TCP_packetSize_ 128 (TCP_RTTvar_exp_ 2) * (TCP_min_max_RTO_ 0.1) TCP_min_max_RTO_ 0.2 (ErrorModel_unit_ pkt ) * (ErrorModel_rate_ 1.0E -07) (ErrorModel_ranvar_ U nif orm) * (MAC_RTSThreshold_ 0) APP_flows_ 1 RWP_Area_ 8

(each column of the array has four zeros and four ones). However in general, as the number of levels for a factor increases, the size of the sets S and S may become markedly different, and the variance of the average of each set may increase greatly. Returning to the example in §3.3, the two-way interactions are not covered equally. Consider the two-way interaction {(1, 0), (2, 0)}. It is covered in only two rows of the array, namely |({(1, 0), (2, 0)})| = |{1, 2}| = 2 (this is true for all two-way interactions in this example). Even in this small array, the coverage of two-way interactions is unbalanced resulting in S accumulating two values and S accumulating six values. This makes any direct comparison among (factor, level) combinations and/or two-way interactions impossible. To address this problem, factors are grouped according to the number of times each level is covered in the LA; see Appendix A for a pointer to the details on how groups are formed. Now, in each iteration of the screening algorithm, the first step is to select the most significant factor or interaction from each group. Then from these candidates, the most significant factor or interaction overall is selected. The Figure 4 shows the graphical tests for normality of the residuals after the first iteration of the screening algorithm. (Similar behaviour of the residuals is observed after each iteration.) While the figures indicate that the residuals are close to normally distributed, we check using the non-parametric Shapiro-Wilk test. This test indicates that the residuals are still not normally distributed. Hence, we use the Wilcoxon rank sum test and the Mann-Whitney U test [14, 28, 54] to select the most significant factor or two-way interaction within each group. Then, to select the most significant factor or interaction over all groups, the Akaike information criterion (AICC ) [1] is used. We still need to fit the intercept and the coefficients of the terms. For a linear model with the assumptions of expected error of zero and expected variance in the error to be equal, the method of ordinary least squares (OLS) is used. However, if the expected variance in the error is unequal, OLS is no longer appropriate [32]. In this case, the method of weighted least squares (WLS) is used to fit the intercept and coefficients of the terms in the screening model.

The first notable observation about this screening model is that it contains both main effects and two-way interactions. Moreover, it contains factors from across the layers of the protocol stack (application, transport, and MAC) and not just the transport layer; in addition, it includes factors from the error model and the mobility model. Aside from these differences with other models of TCP throughput (such as [15, 18, 30, 39­41, 55, 57, 58]), the screening model includes not just which factors or two-way interactions are significant, but the level at which each is significant. From the statistical point of view, Table 7 shows a strong correlation among the regressors and the response of average TCP throughput. The F statistic indicates that the model is significant to the response. Table 7: Summary statistics of the screening model in Table 6. R2 and Adjusted R2 : 0.84 Standard deviation: 0.92 F statistic: 180.6 on 12 and 408 df, p-value < 7.89e-155 We are encouraged by the factors and interactions identified. This includes how and into what unit errors are introduced (using a uni-

3.4.1

The Resulting Screening Model

Table 6 gives the screening model for average TCP throughput developed in twelve iterations of the screening algorithm; Table 8 lists its unique factors. A Student's t-test was run on each term in the screening model and each was found to be significant; 0 is the

36

form distribution into packets rather than bit errors), and their interaction. Smaller sized packets (64 and 128 bytes) tend to reduce throughput. When RTS/CTS is always on (i.e., the threshold is zero bytes), there is a negative impact on throughput compared to when it is configured to 1500 or 3000 bytes (always off). The retransmission timeout (RTO) and round trip time (RTT) are part of TCP's congestion control mechanism; the RTO infers packet loss by observing duplicate acknowledgements and the RTT is related to the propagation delay. The RTO is significant by itself, and in its interaction with the RTT as they work to correct and prevent network congestion. The synthetic error model of the simulator drops packets comparing them with data from an uniform distribution at a steady-state loss event rate of 1.0E -07; this is the lowest error rate used and naturally it corresponds with higher throughput. Smaller simulation areas also result in higher throughput; a larger area has longer average shortest-hop path lengths and average higher network partition rates both of which negatively affect throughput. The throughput response is higher with fewer flows because increasing the number of flows not only may overload the network but more flows are more challenging to route in a MANET.

Table 9: Partial results of a 29 full-factorial screening experiment using JMP 11.0 on the nine factors in Table 8. Term ErrorModel_ranvar_*ErrorModel_unit_ ErrorModel_ranvar_ ErrorModel_unit_ TCP_packetSize_ APP_flows_ TCP_min_max_RTO_ RWP_Area_ MAC_RTSThreshold_ ErrorModel_unit_*TCP_packetSize_ ErrorModel_rate_ ErrorModel_ranvar_*MAC_RTSThreshold_ APP_flows_*RWP_Area_ ErrorModel_unit_*ErrorModel_rate_ TCP_packetSize_*ErrorModel_rate_ ErrorModel_unit_*MAC_RTSThreshold_ ErrorModel_ranvar_*APP_flows_ APP_flows_*TCP_min_max_RTO_ ErrorModel_unit_*APP_flows_ ErrorModel_ranvar_*TCP_min_max_RTO_ ErrorModel_ranvar_*TCP_packetSize_ TCP_packetSize_*APP_flows_ TCP_min_max_RTO_*RWP_Area_ ErrorModel_ranvar_*RWP_Area_ MAC_RTSThreshold_*ErrorModel_rate_ TCP_min_max_RTO_*ErrorModel_rate_ TCP_min_max_RTO_*TCP_rttvar_exp_ ErrorModel_unit_*TCP_min_max_RTO_ APP_flows_*ErrorModel_rate_ RWP_Area_*MAC_RTSThreshold_ ErrorModel_unit_*RWP_Area_ TCP_rttvar_exp_ TCP_packetSize_*RWP_Area_ APP_flows_*MAC_RTSThreshold_ RWP_Area_*ErrorModel_rate_ ErrorModel_ranvar_*TCP_rttvar_exp_ p-Value <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* 0.0001 0.0001 0.0003 0.0006 0.001 0.0012 0.002 0.0116 0.0444 0.0515

4.

VALIDATION AND VERIFICATION

From the 75 controllable factors used in experimentation, nine unique factors are present in the twelve terms in the screening model in Table 6; these are listed in Table 8. Table 8: Unique factors in the screening model in Table 6. Level Factor TCP_RTTvar_exp_ ErrorModel_ranvar_ ErrorModel_unit_ MAC_RTSThreshold_ ErrorModel_rate_ RWP_Area_ TCP_min_max_RTO_ APP_flows_ TCP_packetSize_ Minimum 2 U nif orm pkt 0 1.0E -07 8 0 .1 1 64 Maximum 4 Exponential bit 3000 1.0E -05 40 40 18 2048

In order to validate the factors and interactions identified, we first conduct a full-factorial experiment for these nine factors using the extremes of their region of interest, using the statistical software JMP to analyze the results. From this, we produce a predictive model of average TCP throughput. We then examine the quality of this predictive model by comparing how it performs on random design points (i.e., a design point in which the level of each factor is selected at random). We present our validation results next.

in Table 6. Indeed, both models have the same four most significant terms (though in a different order), and all factors and interactions in Table 6 are a subset of the terms in Table 9. Appendix A gives a pointer to the details of the predictive model for average TCP throughput that was fit using a subset of the significant terms in Table 9. Figure 5 shows the results of evaluating the JMP predictive model as a function of the TCP packet size, for the three levels of error rate. As in the experimentation, all remaining factors are fixed at their default levels. As expected, the results show that the highest TCP throughput is achieved when the error rate is at the lowest level (1.0E -07). For a given error rate the TCP throughput increases as a function of packet size, after which it decreases. An exception is for packet size 1024. Aside from this exception, these results also confirm our intuition of TCP throughput behaviour. The reason for this exception deserves further study but may be related to the default settings used for the other 66 factors not varied in this screening experiment. We now examine the predictive accuracy of the JMP model for random design points.

We conduct an independent 29 full-factorial experiment on the nine factors in Table 8. All remaining 75 - 9 = 66 factors are fixed to their default levels. Ten replicates of each of the 29 design points is run, and TCP throughput measured. The results of the experimentation are input to the JMP statistical software, version 11.0 [45]. The results from the full-factorial screening experiment are given in Table 9. It includes only the main effects and two-way interactions sorted in increasing order by the p-value. The results indicate high commonality with the main effects and two-factor interactions selected by the screening algorithm that formed the screening model

4.1

Full-Factorial Screening in JMP

37

120000 JMP 1.0e-7 JMP 1.0e-6 JMP 1.0e-5

100000 TCP throughput (bps)

factors and two-way interactions, the screening model developed also reflects the actual behaviour well. Despite this, the method aims only to deal with many factors and their interactions to identify the significant ones. We advocate that further experimentation is necessary after the screening is completed, both to confirm the screening results and to build a predictive model. One must be cautious not to over-fit the experimental results and claim unwarranted confidence; confirmation is needed. This is particularly a concern if the stopping criterion chosen locates too many or too few significant interactions; while our choice of R2 appears to have worked well, future effort should address the impact of different stopping criteria. A second concern is the selection criterion for the next factor or interaction to include. Subsequent selections depend upon selections already made, so our method could in principle be misdirected by a bad selection. Our criterion of using the differences between responses for S and those for S has also worked well, but we cannot be certain that such a simple selection suffices in general. Finally, we have employed only a few locating arrays; while they have worked well in our analyses, constructing a suitable locating array remains a challenging problem that merits further research. Certainly further experimentation is needed to assess the merit of screening using LAs, in particular on physical not just simulated complex engineered systems, and draw firm conclusions. What we can conclude is that in a challenging CES arising from a MANET, screening using locating arrays is viable and yields useful models.

80000

60000

40000

20000

0 64 128 256 512 768 1024 Packet size (bytes) 1280 1536 1792 2048

Figure 5: TCP throughput as a function of packet size as predicted the by JMP model; all other factors are at their default levels.

4.2

Predictive Accuracy of JMP Model

In order to test the predictive accuracy of the JMP model, a new experimental design of one hundred random design points is constructed. In constructing each design point, for each of factor Fj , 1  j  75, a random level from Lj is selected. New mobility scenarios are also generated. Ten replicates of each of the random design points are run in the ns-2 simulator, and the TCP throughput measured. In addition, for each experiment in the design, the JMP model is evaluated generating a new data set of fitted TCP throughput. Figure 6 shows the average TCP throughput from simulation, and the fitted throughput from the JMP model corresponding to this random design. The mean TCP throughput from the simulations is 20,892 bps whereas the mean from the JMP model is lower, only 13,946 bps. However, the standard deviation of the results from the JMP model is smaller than the standard deviation from the simulations. Both models exhibit a few outliers. Approximately 94% of the results predicted for TCP throughput from the JMP model are in one standard deviation of the simulation results. Considering the size of the factor space, we conclude that the predicted average TCP throughput of the JMP model is similar to the average TCP throughput measured in simulation.

Acknowledgment
Thanks to Doug Montgomery for his advice on all things statistical. This material is based in part upon work supported by the National Science Foundation under Grant No. 1421058.

6.

REFERENCES

4.3

Predictive Accuracy of Screening Model

While the model developed in applying the screening algorithm based on the LA (Table 6) is not intended to be used as a predictive model, we were curious about its predictive accuracy. Appendix A gives a pointer to a summary of results similar to those in this section for the screening model. To our surprise, the predictive accuracy of the screening model is reasonably good. The screening model does appear to have more variability than the model developed in JMP.

5.

CONCLUSIONS

Locating arrays capture the intuition that in order to see the effect of a main effect or interaction, some design point must cover it; and in order to distinguish it, the responses for the set of design points that cover it must not be equally explained by another small set of main effects or interactions. In a complex engineered system, many main effects and interactions may be significant, but our method identifies them one at a time, iteratively improving a screening model. In this way, an experimental design must be able to repeatedly locate a single "most significant" main effect or interaction. Our results show that using locating arrays for screening appears promising. Indeed while the screening targeted the identification of significant

[1] H. Akaike. A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19(6):716­723, 1974. [2] R. C. Bryce and C. J. Colbourn. A density-based greedy algorithm for higher strength covering arrays. Software Testing, Verification, and Reliability, 19:37­53, 2009. [3] J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE Global Research Technical Report, 29(9):769­781, 2002. [4] C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121­167, 2004. [5] C. J. Colbourn. Covering arrays and hash families. In Information Security and Related Combinatorics, NATO Peace and Information Security, pages 99­136. IOS Press, 2011. [6] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk. Frameproof codes and compressive sensing. In Proc. 48th Annual Allerton Conference on Communication, Control, and Computing, 2010. [7] C. J. Colbourn, S. S. Martirosyan, G. L. Mullen, D. E. Shasha, G. B. Sherwood, and J. L. Yucas. Products of mixed covering arrays of strength two. Journal of Combinatorial Designs, 14(2):124­138, 2006. [8] C. J. Colbourn and D. W. McClary. Locating and detecting arrays for interaction faults. Journal of Combinatorial Optimization, 15:17­48, 2008. [9] C. Croarkin, P. Tobias, J. J. Filliben, B. Hembree,

38

160000 140000 120000 TCP throughput (bps) 100000 80000 60000 40000
+StDev Fitted JMP model

Fitted JMP model Mean fitted JMP model

20000 0 0 10 20
-StDev Fitted JMP model

30

40

50 Random tests

60

70

80

90

100

(a) Predictions by JMP.
160000 140000 120000 TCP throughput (bps) 100000 80000 60000
+StDev Simulated

Simulated Mean Simulated

40000 20000 0
-StDev Simulated

0

10

20

30

40

50 Random tests

60

70

80

90

100

(b) Simulation results.

Figure 6: Predictions by the JMP model and simulation results for random design points.

[10]

[11]

[12]

[13]

[14]

[15]

[16]

W. Guthrie, L. Trutna, and J. Prins, editors. NIST/SEMATECH e-Handbook of Statistical Methods. NIST/SEMATECH, 2012. S. R. Dalal, A. J. N. Karunanithi, J. M. L. Leaton, G. C. P. Patton, and B. M. Horowitz. Model-based testing in practice. In Proc. Intl. Conf. on Software Engineering (ICSE '99), pages 285­294, 1999. P. Damaschke. Adaptive versus nonadaptive attribute-efficient learning. Machine Learning, 41:197­215, 2000. A. B. de Oliveira, S. Fischmeister, A. Diwan, M. Hauswirth, and P. F. Sweeney. Why you should care about quantile regression. In Proc. of the ACM Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS), March 2013. S. Dunietz, W. K. Ehrlich, B. D. Szablak, C. L. Mallows, and A. Iannino. Applying design of experiments to software testing. In Proc. Intl. Conf. on Software Engineering (ICSE '97), pages 205­215, Los Alamitos, CA, 1997. IEEE. M. Fay and M. Proschan. Wilcoxon-Mann-Whitney or t-test? On assumptions for hypothesis tests and multiple interpretations of decision rules. Statistics Surveys, 4:1­39, 2010. S. Floyd, M. Handley, J. Padhye, and J. Widmer. Equation-based congestion control for unicast applications: The extended version. SIGCOMM Computing Communications Review, 30:43­56, 2000. M. Forbes, J. Lawrence, Y. Lei, R. N. Kacker, and D. R.

[17]

[18]

[19]

[20] [21]

[22]

[23]

[24]

Kuhn. Refining the in-parameter-order strategy for constructing covering arrays. J. Res. Nat. Inst. Stand. Tech., 113:287­297, 2008. S. Ghosh and C. Burns. Comparison of four new general classes of search designs. Austral. New Zealand J. Stat., 44:357­366, 2002. K.-J. Grinnemo and A. Brunstrom. A simulation based performance analysis of a TCP extension for best-effort multimedia applications. In Proceedings of the 35th Annual Simulation Symposium, 2002. A. Hartman. Software and hardware testing using combinatorial covering suites. In M. C. Golumbic and I. B.-A. Hartman, editors, Interdisciplinary Applications of Graph Theory, Combinatorics, and Algorithms, pages 237­266. Springer, Norwell, MA, 2005. A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal Arrays. Springer-Verlag, New York, 1999. D. S. Hoskins, C. J. Colbourn, and M. Kulahci. Truncated D-optimal designs for screening experiments. American Journal of Mathematical and Management Sciences, 28:359­383, 2008. D. S. Hoskins, C. J. Colbourn, and D. C. Montgomery. D-optimal designs with interaction coverage. Journal of Statistical Theory and Practice, 3:817­830, 2009. J. P. C. Kleijnen. An overview of the design and analysis of simulation experiments for sensitivity analysis. European Journal of Operational Research, 164:287­300, 2005. J. P. C. Kleijnen, B. Bettonvil, and F. Persson. Screening for

39

[25]

[26]

[27]

[28]

[29]

[30]

[31] [32]

[33]

[34]

[35]

[36] [37] [38]

[39]

[40]

[41]

[42]

[43]

the important factors in large discrete-even simulation models: Sequential bifurcation and its applications. In A. M. Dean and S. M. Lewis, editors, Screening: Methods for Experimentation in Industry, Drug Discovery and Genetics, chapter 13, pages 287­307. Springer-Verlag, 2006. D. Kuhn and M. Reilly. An investigation of the applicability of design of experiments to software testing. In Proc. 27th Annual NASA Goddard/IEEE Software Engineering Workshop, pages 91­95, Los Alamitos, CA, 2002. IEEE. D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault interactions and implications for software testing. IEEE Trans. Software Engineering, 30(6):418­421, 2004. R. Li and D. K. J. Lin. Analysis methods for supersaturated designs: Some comparisons. Journal of Data Science, pages 249­260, 2003. H. B. Mann and D. R. Whitney. On a test of whether one of two random variables is stochastically larger than the other. Annals of Mathematical Statistics, 18:50­60, 1947. C. Martínez, L. Moura, D. Panario, and B. Stevens. Locating errors using ELAs, covering arrays, and adaptive testing algorithms. SIAM J. Discrete Math., 23:1776­1799, 2009/10. M. Mathis, J. Semke, J. Mahdavi, and T. Ott. The macroscopic behavior of the TCP congestion avoidance algorithm. SIGCOMM Comput. Commun. Rev., 27:67­82, 1997. D. C. Montgomery. Design and Analysis of Experiments. John Wiley & Sons, Inc., 8 edition, 2012. D. C. Montgomery, E. A. Peck, and C. G. Vining. Introduction to Linear Regression Analysis. John Wiley & Sons, Inc., 4th edition, 2006. A. Munjal, T. Camp, and W. Navidi. Constructing rigorous MANET simulation scenarios with realistic mobility. In European Wireless Conference (EW), pages 817­824, 2010. P. Nayeri, C. J. Colbourn, and G. Konjevod. Randomized postoptimization of covering arrays. European Journal of Combinatorics, 34:91­103, 2013. Networking and information technology research and development (NITRD) large scale networking (LSN) workshop report on complex engineered networks, 2012. C. Nie and H. Leung. A survey of combinatorial testing. ACM Computing Surveys, 43, 2011. The Network Simulator - ns-2. http://www.isi.edu/nsnam/ns. K. Nurmela. Upper bounds for covering arrays by tabu search. Discrete Applied Mathematics, 138(9):143­152, 2004. J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. Modeling TCP throughput: a simple model and its empirical validation. SIGCOMM Computing Communications Review, 28:303­314, 1998. J. Padhye, V. Firoiu, D. F. Towsley, and J. F. Kurose. Modeling TCP Reno performance: A simple model and its empirical validation. IEEE/ACM Transactions on Networking, 8:133­145, 2000. N. Parvez, A. Mahanti, and C. Williamson. An analytic throughput model for TCP NewReno. IEEE/ACM Transactions on Networking, 18:448­461, 2010. C. E. Perkins and E. M. Royer. Ad hoc on-demand distance vector routing. In Proc. Second IEEE Workshop on Mobile Computing Systems and Applications, pages 90­100, 1999. S. Poljak, A. Pultr, and V. Rödl. On qualitatively independent

[44]

[45] [46]

[47]

[48]

[49]

[50]

[51]

[52]

[53]

[54] [55]

[56]

[57]

[58]

partitions and related problems. Discrete Applied Math., 6:193­205, 1983. A. H. Ronneseth and C. J. Colbourn. Merging covering arrays and compressing multiple sequence alignments. Discrete Applied Mathematics, 157:2177­2190, 2009. JMP statistical software from SAS. http://www.jmp.com. G. Seroussi and N. H. Bshouty. Vector sets for exhaustive testing of logic circuits. IEEE Transactions on Information Theory, 34:513­522, 1988. D. E. Shasha, A. Y. Kouranov, L. V. Lejay, M. F. Chou, and G. M. Coruzzi. Using combinatorial design to study regulation by multiple input signals: A tool for parsimony in the post-genomics era. Plant Physiology, 127:1590­1594, 2001. T. Shirakura, T. Takahashi, and J. N. Srivastava. Searching probabilities for nonzero effects in search designs for the noisy case. Ann. Statist., 24:2560­2568, 1996. J. N. Srivastava. Designs for searching non-negligible effects. In J. N. Srivastava, editor, A Survey of Statistical Design and Linear Models, pages 507­519. North­Holland, 1975. D. T. Tang and C. L. Chen. Iterative exhaustive pattern generation for logic testing. IBM Journal Research and Development, 28:212­219, 1984. Y. Tang, C. J. Colbourn, and J. Yin. Optimality and constructions of locating arrays. J. Stat. Theory Pract., 6(1):20­29, 2012. J. Torres-Jimenez and E. Rodriguez-Tello. New upper bounds for binary covering arrays using simulated annealing. Information Sciences, 185:137­152, 2012. K. K. Vadde and V. R. Syrotiuk. Factor interaction on service delivery in mobile ad hoc networks. IEEE Journal on Selected Areas in Communications, 22:1335­1346, 2004. F. Wilcoxon. Individual comparisons by ranking methods. Biometrics Bulletin, 1:80­83, 1945. I. Yeom and A. L. N. Reddy. Modeling TCP behavior in a differentiated services network. IEEE/ACM Transactions on Networking, 9:31­46, 1999. C. Yilmaz, M. B. Cohen, and A. Porter. Covering arrays for efficient fault characterization in complex configuration spaces. IEEE Transactions on Software Engineering, 31:20­34, 2006. B. Zhou, C. P. Fu, D.-M. Chiu, C. T. Lau, and L. H. Ngoh. A simple throughput model for TCP Reno. In Proceedings of the IEEE International Communications Conference (ICC'06), 2006. M. Zorzi, A. Chockalingam, and R. R. Rao. Throughput analysis of TCP on channels with memory. IEEE Journal on Selected Areas in Communications, 18:1289­1300, 2000.

APPENDIX A. GITHUB REPOSITORY
A GitHub repository provides supplementary material at: https://github.com/locatingarray/screening.git Specifically, it includes the 75 controllable factors in the ns-2 simulator used in experimentation and their levels (§3.1), the 421 × 75 LA used as the experimental design (§3.1), a description of how factors are grouped (§3.4), the JMP model for TCP throughput, along with some statistical analysis (§4.1), and some analysis of the predictive capability of the screening model (§4.3).

40

Complex engineered systems arise throughout computing, communications, and networking. Many factors, each having a finite number of levels, impact the behaviour of the system either singly or in interaction with one another. Testing or evaluating such a system involves formulating a set of tests, when executed, responses or outcomes from the tests are analyzed. A single round of testing is conducted. To witness the effect of an interaction, some test must cover it, this does not suffice in general to locate the interaction or to measure its effect. When there are few factors or many tests, experimental designs can measure (and hence locate) the interactions. When there are many factors and few tests, can we locate the interaction(s)? Can we efficiently detect them?Combinatorial arrays, locating and detecting arrays, are introduced to address such location and detection in the context of combinatorial testing. Locating and detecting arrays are contrasted with covering arrays and with experimental designs. An application to a 75 factor protocol stack for file transfer is given to demonstrate their practical use. Finally, their place in the literature of combinatorial testing is discussed and some directions are outlined.IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

1

A Combinatorial Approach to X-Tolerant Compaction Circuits
Yuichiro Fujiwara and Charles J. Colbourn

arXiv:1508.00481v1 [cs.IT] 3 Aug 2015

Abstract--Test response compaction for integrated circuits (ICs) with scan-based design-for-testability (DFT) support in the presence of unknown logic values (Xs) is investigated from a combinatorial viewpoint. The theoretical foundations of Xcodes, employed in an X-tolerant compaction technique called X-compact, are examined. Through the formulation of a combinatorial model of X-compact, novel design techniques are developed for X-codes to detect a specified maximum number of errors in the presence of a specified maximum number of unknown logic values, while requiring only small fan-out. The special class of X-codes that results leads to an avoidance problem for configurations in combinatorial designs. General design methods and nonconstructive existence theorems to estimate the compaction ratio of an optimal X-compactor are also derived. Index Terms--Circuit testing, built-in self-test (BIST), compaction, X-compact, test compression, X-code, superimposed code, Steiner system, configuration.

I. I NTRODUCTION HIS work discusses a class of codes that arise in data volume compaction of responses from integrated circuits (ICs) under scan-based test. We first recall briefly the background of the X-tolerant compaction technique in digital circuit testing. Digital circuit testing applies test patterns to a circuit under test and monitors the circuit's responses to the applied patterns. A tester compares the observed response to a test pattern to the expected response and, if there is a mismatch, declares the circuit chip defective. Usually the expected responses are obtained through fault-free simulation of the chip. Test cost for traditional scan-based testing is dominated by test data volume and test time [1]. Therefore various test compression techniques have been developed to reduce test cost. One way to achieve this is to reduce test application time and the number of test patterns by employing automatic test pattern generation (ATPG) (see [2]­[5] and references therein). We are interested in the other kind of technique, using methods to hash responses while maintaining test quality. Signature analyzers (e.g., [6]­[10]) are vulnerable to error masking caused by unknown logic values (Xs) [11]. X-compact has been proposed in order to conduct reliable testing in the presence of Xs [12]. A response compaction circuit based
This work was supported in part by JSPS Research Fellowships for Young Scientists (YF) and by DOD grants N00014-08-1-1069 and N00014-08-11070 (CJC). Y. Fujiwara is with the Department of Mathematical Sciences, Michigan Technological University, Houghton, MI 49931 USA. yfujiwar@mtu.edu. C. J. Colbourn is with the School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287-8809 USA. charles.colbourn@asu.edu

T

on X-compact is an X-compactor. X-compactors have proved their high error detection ability in actual systems [11], [13]. An X-compactor can be written in matrix form as an X-code [14]. Basic properties of X-codes have been studied [14], [15]. Graph theoretic techniques have been employed to minimize fan-out of inputs [16]; in general an X-compactor tolerates the presence of Xs in exchange for large fan-out. These studies focus on particular classes of X-codes rather than the general coding theoretic aspects. The purpose of the present paper is to investigate theoretical foundations of X-codes and to provide general construction techniques. In Section II we outline the combinatorial requirements for the X-compact technique and present an equivalent definition of X-codes in order to investigate X-compactors as codes and combinatorial designs. Some basic properties of X-codes are also presented. In Section III we investigate X-codes that require only small fan-out and have good error detectability and X-tolerance. We prove the equivalence between a class of Steiner t-designs and particular X-codes having the maximum number of codewords and the minimum fan-out. This allows us to give constructions and to show existence of such X-codes. Section IV deals with existence of X-codes in the more general situation. Both constructive and nonconstructive theorems are provided. Finally we conclude in Section V. II. C OMBINATORIAL R EQUIREMENTS
AND

X-C ODES

We do not describe scan-based testing and response compaction in detail here, instead referring the reader to [11], [12]. Scan-based testing repeatedly applies vectors of test inputs to the circuit, and for each test captures a vector from {0, 1}n as the test output. Naturally it is important that the test output be the correct one. To determine this, the function of the circuit is simulated (in a fault-free manner) to produce a reference output. When the test and reference outputs agree, no fault has been detected. The first major obstacle is that fault-free simulation may be unable to determine whether a specific output is 0 or 1, and hence it is an unknown logic value X. The second is that if each output requires a separate pin on the chip, the number of tests that can be accommodated is quite restricted. We deal with these two problems in turn. We define an algebraic system to describe the behavior of Xs. The X-algebra X2 = ({0, 1, X}, +, ·) over the field F2 is the set {0, 1} of elements of F2 and a third element X, equipped with two binary operations "+" (addition) and "·" (multiplication) satisfying: 1) a + b and a · b are performed in F2 for a, b  F2 ;

2

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

2) a + X = X + a = X for a  F2 ; 3) 0 · X = X · 0 = 0 for the additive identity 0; 4) 1 · X = X · 1 = X. The element X is termed an unknown logic value. Now consider a test output b = (b1 , . . . , bn )  {0, 1}n and a reference output c = (c1 , . . . , cn )  {0, 1, X}n . When ci  {0, 1}, the test and reference outputs agree on the ith bit when bi = ci ; otherwise the ith bit is an error bit. When ci = X, whatever the value of bi , no error is detected. Thus the ith bit is (known to be) in error if and only if bi + ci = 1, using addition in X2 . Turning to the second problem, an X-compact matrix is an n × m matrix H with elements from {0, 1}. The compaction ratio of H is n/m. The number of 1s in the ith row is the weight, or fan-out, of row i. Output (or response) compaction is performed by computing the vector d = (d1 , . . . , dm ) = bH for output (arithmetic is in X2 ). In the same way, the reference output can be compacted using the same matrix to form r = (r1 , . . . , rm ) = cH . As before, if di = ri and ri = X (that is, if di + ri = 1), an error is detected. To be of practical value, an X-compact matrix H should detect the presence of error bits in b with respect to c given the compacted vectors d and r under `reasonable' restrictions on the number of errors and number of unknown logic values. Suppose that b + c = 1 (so that there is a fault to be detected). In principle, whenever hj = 1, the fault could be observed on output j . Let L = {j : hj = 1}. Suppose then n that j  L. If it happens that i=1 ci hij = X, the error at position  is masked for output j (that is, dj + rj = X, and no error is observed). On the other hand, if
n n n

of X-codes. In order to investigate X-codes from coding and design theoretic views, we introduce an equivalent definition. Consider two m-dimensional vectors s1 = (1) (1) (1) (2) (2) (2) (s1 , s2 , . . . , sm ) and s2 = (s1 , s2 , . . . , sm ), where (j ) si  F2 . The addition of s1 and s2 is bit-by-bit addition, denoted by s1  s2 ; that is,
(2) s1  s2 = (s1 + s1 , s2 + s2 , . . . , s(1) m + sm ). (1) (2) (1) (2)

The superimposed sum of s1 and s2 , denoted s1  s2 , is
(2) s1  s2 = (s1  s1 , s2  s2 , . . . , s(1) m  sm ), (1) (2) (1) (2) (j ) ( l) (j ) ( l)

where si  sk = 0 if si = sk = 0, otherwise 1. An m-dimensional vector s1 covers an m-dimensional vector s2 if s1  s2 = s1 . For a finite set S = {s1 , . . . , ss } of m-dimensional vectors, define S = s1  · · ·  ss and S = s1  · · ·  ss .

When S = {s1 } is a singleton, S = S = s1 . For S =  we define S = S = 0, the zero vector. Let d be a positive integer and x a nonnegative integer. An (m, n, d, x) X-code X = {s1 , s2 , . . . , sn } is a set of mdimensional vectors over F2 such that |X | = n and ( S1 )  ( S2 ) = S1 .

dj + rj =
i=1

bi hij +
i=1

ci hij =
i=1

(bi + ci )hij = 0

then no error is observed. This occurs when there are an even number of values of i for which hij = 1 and bi + ci = 1; because this holds when i =  by hypothesis, the error at position  is canceled for output j when the number of such errors is even. When an error is masked or canceled for every output j  L, it is not detected. Otherwise, it is detected by an output that is neither masked nor canceled. Treating X's as erasures and using traditional codes can increase the error detectability of an X-compactor [17]. Unfortunately, this involves postprocessing test responses and cannot be easily implemented [12]. Therefore, we focus on X-compaction in which an error is only detected by the simple comparison described here. There are numerous criteria in defining a "good" X-compact matrix. It should have a high compaction ratio and be able to detect any faulty circuit behavior anticipated in actual testing. Power requirements, compactor delay, and wireability dictate that the weight of each row in a matrix be small to meet practical limitations on fan-in and fan-out [11], [16]. The fundamental problem in X-tolerant response compaction is to design an X-compact matrix with large compaction ratio that detects faulty circuit behavior. To achieve this, X-codes (which represent X-compact matrices) were introduced [14]. In this section, we discuss basic properties

for any pair of mutually disjoint subsets S1 and S2 of X with |S1 | = x and 1  |S2 |  d. A vector si  X is a codeword. (i) (i) The weight of a codeword si is |{sj = 0 : sj  si }|. The ratio n/m is the compaction ratio of X . An X-code forming an orthonormal basis of the m-dimensional linear space over F2 is trivial. Roughly speaking, an X-code is a set of codewords such that for every positive integer d  d no superimposed sum of any x codewords covers the vector obtained by adding up any d codewords chosen from the rest of the n - x codewords. Now we present a method of designing an X-compact matrix from an X-code. Proposition 1: There exists an (m, n, d, x) X-code X if and only if there exists an n × m X-compact matrix H which detects any combination of d faults (1  d  d) in the presence of at most x unknown logic values. Proof: First we prove necessity. Assume that X is an (m, n, d, x) X-code. Write X = {s1 , s2 , . . . , sn }, where si = (i) (i) (i) (s1 , s2 , . . . , sm ) for 1  i  n. Define an n × m matrix (i) H = (hi,j ) as hi,j = sj . We show that H forms an Xcompact matrix that detects a fault if the test output b contains d error bits, 1  d  d, and up to x Xs. Let E = {k : bk + ck = 1}, the set of indices of error bits, have cardinality d . Let X = {k : ck = X}, the set of indices of unknown logic values, have cardinality x. Now comparing d and r , d + r =
k

bk · hk, +
k

ck · hk,

=
kE,X

(bk + ck ) · hk, 1 · hk, +
k E k X

=

X · hk, ,

(1)

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

3

with operations performed in X2 . Because the set of rows of H forms the set of codewords of X , no superimposed sum of x rows covers the vector obtained by an addition of any d rows. Hence there exists a column c such that 1 · hk,c = 1 and
k E k X

X · hk,c = 0.

(2)

Then (1) and (2) imply dc + rc = 1, that is, H detects a fault. Because (2) holds if and only if the right hand side of (1) equals one for l = c, sufficiency is straightforward. By virtue of this equivalence, we can employ various known results and techniques in coding theory to design an X-compactor with good error detection ability, X-tolerance, and a high compaction ratio. For the case when x = 0, an (m, n, d, 0) X-code forms an n × m X-compact matrix which is a parity-check matrix of a binary linear code of length n and minimum distance d. In fact, since the condition that x = 0 implies the absence of Xs, this special case is reduced to traditional space compaction. Because our focus is compaction in the presence of unknown logic values, we assume that x  1 henceforth unless otherwise stated. In the absence of Xs, see [18], [19]. By definition, an (m, n, d, x) X-code, d  2, is also an (m, n, d - 1, x) X-code. Also an (m, n, d, x) X-code forms an (m, n, d, x - 1) X-code. Moreover, an (m, n, d, x) X-code is an (m, n, d + 1, x - 1) X-code [14]. It can be difficult to design an X-compactor having both the necessary error detectability and the exact number of inputs needed. One trivial solution is to discard codewords from a larger X-code with sufficient error detection ability and Xtolerance. The following is another simple way to adjust the number of inputs. Proposition 2: If an (m, n, d, x) X-code and an (m , n , d , x ) X-code exist, there exists an (m + m , n + n , min{d, d }, min{x, x }) X-code. Proof: Let X = {s1 , . . . , sn } be an (m, n, d, x) Xcode and Y = {t1 , . . . , tn } an (m , n , d , x ) X-code. (i) (i) Extend each codeword si = (s1 , . . . , sm ) of X by appending m 0's so that extended vectors have the form (i) (i) s i = (s1 , . . . , sm , 0, . . . , 0). Similarly extend each codeword (j ) (j ) tj = (t1 , . . . , tm ) of Y by appending m 0s so that extended (j ) (j ) vectors have the form t j = (0, . . . , 0, t1 , . . . , tm ). The extended (m + m )-dimensional vectors form an (m + m , n + n , min{d, d }, min{x, x }) X-code. Proposition 2 says that given an (m, n, d, x) X-code, a codeword of weight less than or equal to x does not essentially contribute to the compaction ratio (see also [14]). In fact, (i) (i) if X contains such a codeword si = (s1 , . . . , sm ), there (i) exists at least one coordinate m such that sm = 1 and (j ) sm = 0 for any other codeword sj  X . Hence we can delete si and coordinate m from X while keeping d and x. By applying Proposition 2 and combining a trivial X-code and another X-code, we can obtain an X-code having the same number of codewords with compaction ratio no smaller. For this reason, when constructing an (m, n, d, x) X-code explicitly, we assume that every codeword has weight greater than x.

Let M (m, d, x) be the maximum number n of codewords for which there exists an (m, n, d, x) X-code. More codewords means a higher compaction ratio. Hence an (m, n, d, x) Xcode satisfying n = M (m, d, x) is optimal. Determining the exact value of M (m, d, x) seems difficult except for M (m, 1, 1). As pointed out in [14], a special case of M (m, d, x) has been extensively studied in the context of superimposed codes [20]. An (1, x)-superimposed code of size m × n is an m × n matrix S with entries in F2 such that no superimposed sum of any x columns of S covers any other column of S . Superimposed codes are also called cover-free families and disjunct matrices. By definition, a (1, x)-superimposed code of size m × n is equivalent to the transpose of an X-compact matrix obtained from an (m, n, 1, x) X-code. Hence known results on the maximum ratio n/m for superimposed codes immediately give information about M (m, 1, x). For completeness, we list useful results on M (m, 1, x). By Sperner's theorem, Theorem 2.1: (see [21], [22]) For m  2 an integer, M (m, 1, 1)  m . m/2

Indeed by taking all the m-dimensional vectors of weight m/2 as codewords, we attain the bound. The same argument is also found in [14]. The following is a simple upper bound on M (m, 1, x): Theorem 2.2: [22] For any x  2, log2 M (m, 1, x)  cm log2 x x2

for some constant c. Several different proofs of Theorem 2.2 are known. Bounds on the constant c are approximately two in [23], approximately four in [24], and approximately eight in [25]. The asymptotic behavior of the maximum possible number of codewords has been also investigated for superimposed codes. Define the ratio R(x) as log2 M (m, 1, x) . m m The best lower bound R(x)  R(x) can be found in [26] and the best upper bound R(x)  R(x) in [23]. The descriptive asymptotic form of the best bounds as x   is R(x) = lim R(x)  1 2 log2 x , and R(x)  x2 log2 e x2

where e is Napier's constant. For a detailed summary of the known lower and upper bounds, see [27]. Constructions with many codewords have been studied in [28], [29]. See also [30]­[33] and references therein. III. X-C OMPACTORS
WITH

S MALL FAN -O UT

In this section we consider an X-compactor having sufficient tolerance for errors and Xs, a high compaction ratio, and small fan-out. This section is divided into four parts. Subsection III-A deals with background and known results of the fanout problem in X-compactors. Then in Subsection III-B we investigate X-codes that tolerate up to two X's and have

4

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

the minimum fan-out. X-Codes with further error detection ability and X-tolerance are investigated in Subsection III-C. In Subsection III-D we give a brief overview of the performance of our X-codes given in this section and compare them with other codes. A. Background: Fan-Out in X-Codes X-compact reduces the number of bits in the compacted output while keeping error detection ability by propagating each single bit to many signal lines. In fact, each output of the X-compactor in [12] connects to about half of all inputs. However, larger fan-in increases power requirements, area, and delay [16]. When these disadvantages are concerns, fan-out of inputs of a compactor should be small to reduce fan-in values. In terms of X-codes, the required fan-out of input i in an X-compactor is the weight of codeword si of the X-code. Hence, in order to address the fan-out problem, it is desirable for a codeword to have small weight. However, as mentioned in Section I, an (m, n, d, x) X-code containing a codeword with weight at most x is not essential in the sense of the compaction ratio. Hence, throughout this section, we restrict ourselves to (m, n, d, x) X-codes in which every codeword has weight precisely x + 1, namely constant weight codes. When a compactor is required to tolerate only a single unknown logic value, fan-out is minimized when every codeword of an X-code has constant weight two. This extreme case was addressed in [16] by considering a simple graph. We briefly restate their theorems in terms of X-codes. A graph G is a pair (V, E ) such that V is a finite set and E is a set of pairs of distinct elements of V . An element of V is called a vertex, and an element of E is called an edge. The girth g of G is the minimal size |C | of a subset C  E such that each vertex appearing in C is contained in exactly two edges. The edge-vertex incidence matrix H of a graph G = (V, E ) is a |E| × |V | binary matrix H = (hi,j ) such that rows and columns are indexed by edges and vertices respectively and hi,j = 1 if the ith edge contains the j th vertex, otherwise 0. By considering the edge-vertex incidence matrix of a graph and Proposition 1, we obtain: Theorem 3.1: [16] There exists a graph G = (V, E ) of girth g if and only if there exists a (|V |, |E|, g - 2, 1) X-code of constant weight two. Theorem 3.2: [16] A set X of m-dimensional vectors is an (m, n, d - 1, 1) X-code of constant weight two if and only if it is an (m, n, d, 0) X-code of weight two. These two theorems say that in order to design an Xcompactor with high error detection ability, we only need to find a graph with large girth. The same argument is also found in [14]. For existence of such graphs and more details on Xcodes of constant weight two, see [16] and references therein. B. Two X's and Fan-Out Three Multiple X's can occur; here we present X-codes that are tolerant to two X's and have the maximum compaction ratio. To accept up to two unknown logic values, we need an X-code

of constant weight three. We employ a well-known class of combinatorial designs. A set system is an ordered pair (V, B ) such that V is a finite set of points, and B is a family of subsets (blocks) of V . A Steiner t-design S (t, k, v ) is a set system (V, B ), where V is a finite set of cardinality v and B is a family of k -subsets of V such that each t-subset of V is contained in exactly one block. Parameters v and k are the order and block size of a Steiner t-design. When t = 2 and k = 3, an S (2, 3, v ) is a Steiner triple system of order v , STS(v ). An STS(v ) exists if and only if v  1, 3 (mod 6) [34]. A triple packing of order v is a set system (V, B ) such that B is a family of triples of a finite set V of cardinality v and any pair of elements of V appear in B at most once. An STS(v ) is a triple packing of order v  1, 3 (mod 6) containing the maximum number of triples. The point-block incidence matrix of a set system (V, B ) is the binary |V | × |B| matrix H = (hi,j ) such that rows are indexed by points, columns are indexed by blocks, and hi,j = 1 if the ith point is contained in the j th block, otherwise 0. The block-point incidence matrix is its transpose. When d = 1, an (m, n, 1, 2) X-code of constant weight three is equivalent to a (1, 2)-superimposed code of size m × n of constant column weight three. It is well known that the pointblock incidence matrix of an S (t, k, v ) forms an (1, k/(t - k 1) - 1)-superimposed code of size v × v t / t . Hence, by using an STS(v ), we obtain for every v  1, 3 (mod 6) a (v, v (v - 1)/6, 1, 2) X-code. An upper bound on the number of codewords of (1, 2)-superimposed codes of constant weight k is available: Theorem 3.3: [35] Let nk (m) denote the maximum number of columns of a (1, 2)-superimposed code such that and every column is of length m and has constant weight k . Then, n2t-1 (m)  n2t (m + 1) 
m t 2t-1 t

with equality if and only if there exists a Steiner t-design S (t, 2t - 1, m). The following is an immediate consequence: Theorem 3.4: For any (m, n, 1, 2) X-code of constant -1) with equality if and only if there weight three, n  m(m 6 exists an STS(m). Hence for d = 1, x = 2, and fan-out three, an X-code from any STS(v ) has the maximum compaction ratio (v - 1)/6. One may ask for larger error detectability of an (m, n, 1, 2) X-code when one (or zero) unknown logic value is assumed. An (m, n, d, x) X-code is also an (m, n, d + 1, x - 1) X-code, and hence any (m, n, 1, 2) X-code from an STS(m) is also an (m, n, 2, 1) X-code. However, a careful choice of Steiner triple systems gives higher error detectability while maintaining the compaction ratio. A configuration C in a triple packing, (V, B ), is a subset C  B . The set of points appearing in at least one block of a configuration C is denoted by V (C ). Two configurations C and C  are isomorphic, denoted C  = C  , if there exists a bijection   : V (C )  V (C ) such that for each block B  C , the image (B ) is a block in C  . When |C| = i, a configuration C is an i-configuration. A configuration C is even if for every point

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

5

a appearing in C the number |{B : a  B  C}| of blocks containing a is even. Because every block in a triple packing has three points, no i-configuration for i odd is even. A triple packing is r-even-free if for every integer i satisfying 1  i  r it contains no even i-configurations. By definition every r-even-free triple packing, r  2, is also (r - 1)-even-free. For an even integer r, an r-even-free triple packing is also (r + 1)-even-free. Every triple packing is trivially 3-even-free. For v > 3 an STS(v ) may or may not be 4-even-free. Up to isomorphism, the only even 4-configuration is the Pasch configuration. It can be written on six points and four blocks: {{a, b, c}, {a, d, e}, {f, b, d}, {f, c, e}}. For the list of all the small configurations in a triple packing and more complete treatments, we refer the reader to [34] and [36]. Because a 4-even-free STS is 5-even-free, an STS is 5-evenfree if and only if it contains no Pasch configuration. Lemma 3.5: If there exists a 5-even-free STS(v ), there exists a (v, v (v - 1)/6, 3, 1) X-code of constant weight three. The code is a (v, v (v - 1)/6, 5, 0) X-code of constant weight three. Proof: Let (V, B ) be a 5-even-free STS(v ). For every Bi  B define a v -dimensional vector si such that each (i) coordinate sj  si is indexed by a distinct point j  V (i) and sj = 1 if j  Bi , otherwise 0. Then we obtain a (v, v (v - 1)/6, 1, 2) X-code S = {si : Bi  B} of constant weight three. We prove that S is a (v, v (v - 1)/6, 3, 1) X-code that is also a (v, v (v - 1)/6, 5, 0) X-code. By definition, for 1  i  5 no i-configuration C  B is even. Hence {si : Bi  C} = 0. This implies that S is a (v, v (v - 1)/6, 5, 0) X-code. On the other hand, since no pair of points appears twice, for any mutually distinct blocks Bi , Bj , Bk  B , si = sj and si  (sj  sk ) = si . It remains to show that no codeword in S covers addition of three others. Suppose to the contrary that there exist four distinct codewords si , sj , sk , and sl such that si  (sj  sk  sl ) = si . Because no pair of points appears twice and every block has exactly three points, the only possible case is that the 4configuration {Bi , Bj , Bk , Bl } forms a Pasch configuration, and hence it is even, a contradiction. Steiner triple systems avoiding Pasch configurations have been long studied as anti-Pasch STSs [34]. Theorem 3.6: [37] There exists a 5-even-free STS(v ) if and only if v  1, 3 (mod 6) and v  {7, 13}. By combining Theorem 3.6 and Lemma 3.5, we obtain: Theorem 3.7: For every v  1, 3 (mod 6) and v  {7, 13}, there exists a (v, v (v - 1)/6, 1, 2) X-code of constant weight three that is a (v, v (v - 1)/6, 3, 1) X-code and a (v, v (v - 1)/6, 5, 0) X-code. An X-compactor designed from these can detect any odd number of errors unless there is an unknown logic value. One may want to take advantage of the high compaction ratio of the optimal (m, n, 1, 2) X-codes arising from 4-even-free STSs

when there is only a small possibility that more than two Xs occur or multiple errors happen with multiple Xs. Our X-codes from 4-even-free STSs also have high performance in such situations: Theorem 3.8: The probability that a (v, v (v - 1)/6, 1, 2) Xcode from a 4-even-free STS(v ) fails to detect a single error 162(v -3)2 when there are exactly three Xs is (v+2)(v+3)( v -4)(v 2 -v -18) . Proof: Because there is only one error, an X-code fails to detect this error when all three points in the block that corresponds to the error are contained in at least one block corresponding to an X. The number of occurrences of each 4configuration in an STS(v ) is determined by v and the number of Pasch configurations (see [34], for example). A simple calculation proves the assertion. Theorem 3.9: The probability that a (v, v (v - 1)/6, 1, 2) X-code from a 4-even-free STS(v ) fails to detect errors when there are exactly two Xs and exactly two errors is 1296 (v +2)(v +3)(v -4)(v 2 -v -18) . Proof: A (v, v (v - 1)/6, 1, 2) X-code from a 4-evenfree STS(v ) fails to detect errors when there are exactly two Xs and exactly two errors only when corresponding four blocks form a 4-configuration isomorphic to {{a, b, c}, {d, e, f }, {a, e, g }, {c, f, g }} where the first two blocks represent Xs and the other two blocks correspond to errors. The number of occurrences of the 4-configuration in v -3) a 4-even-free STS(v ) is v(v-1)( , and the total number of 4 occurrences of all 4-configurations is
v (v -1)(v -3) 4 4 2
v(v-1) 6 v(v-1) 6

4

[34]. Divide

to obtain the probability that the by 4 X-code fails to detect the two errors. Hence when a 4-even-free STS of sufficiently large order is used, the probability that the corresponding X-code fails to detect errors when the sum of the numbers of errors and Xs is at most four is close to zero. A more complicated counting argument is necessary to calculate the performance of X-codes from STSs when the sum of the numbers of errors and Xs is greater than four. For more complete treatments and current research results on counting configurations in Steiner triple systems, we refer the reader to [36] and references therein. Useful explicit constructions for 5-even-free STS(v ) can be found in [34], [37]­[41]. The cyclic 5-sparse Steiner triple systems in [42] provide examples of 5-even-free STS(v ) for v  97, because cyclic 5-sparse systems are all anti-Pasch. Further r-even-freeness improves the error detectability of the resulting X-code: Theorem 3.10: For r  4, if there exists an r-even-free triple packing (V, B ), there exists a (|V |, |B|, 1, 2) X-code of constant weight three that is also a (|V |, |B|, 3, 1) X-code and a (|V |, |B|, r, 0) X-code. Proof: Let (V, B ) be an r-even-free triple packing of order v . For every Bi  B define a v -dimensional vector si such (i) that each coordinate sj  si is indexed by a distinct point (i) j  V and sj = 1 if j  Bi , otherwise 0. Then we obtain a (|V |, |B|, 1, 2) X-code S = {si : Bi  B} of constant weight three. It suffices to prove that S forms a (|V |, |B|, r, 0) X-code. Suppose to the contrary that S is not a (|V |, |B|, r, 0) X-code. Then for some r  r there exists a set of r codewords si ,

6

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

sj . . . , sk such that si  sj · · ·  sk = 0. However, the set of the corresponding blocks Bi , Bj ,. . . ,Bk forms an even r -configuration, a contradiction. One may want an r-even-free STS with large r to obtain higher error detection ability while keeping the maximum compaction ratio. Although it is known that every Steiner triple system has a configuration with seven or fewer blocks so that every element of the configuration belongs to at least two [36], it may happen that none of these are even. Nevertheless, the following gives an upper bound of even-freeness of Steiner triple systems. Theorem 3.11: For v > 3 there exists no 8-even-free STS(v ). Proof: Suppose to the contrary that there exists an STS(v ), S , that is 8-even free. Consider a 4-configuration C isomorphic to {{a, b, e}, {c, d, e}, {a, c, f }, {b, d, g}}; the points f and g are each contained in exactly one block. For any anti-Pasch STS(v ) the number of occurrences of configurations isomorphic to C is v (v - 1)(v - 3)/4 [43] (see also [44]). Because v  7, we have v (v - 1)(v - 3)/4 > v 2 . Hence there is a pair of configurations A and B such that A  = C and they share the two points contained in = B  exactly one block. In other words, there exists a pair A and B having the form {{a, b, e}, {c, d, e}, {a, c, f }, {b, d, g}} and {{a , b , e }, {c , d , e }, {a , c , f }, {b , d , g }} respectively. If there is no common block between A and B , then the merged configuration A  B forms an even configuration consisting of eight blocks, a contradiction. Otherwise, there is at least one block contained in both A and B . Removing blocks shared between A and B from their union, we obtain an even configuration on four or six blocks, a contradiction. By combining Theorems 3.4, 3.10, and 3.11, we have: Theorem 3.12: There exists no (m, n, 1, 2) X-code that achieves the maximum compaction ratio (m - 1)/6 and is also an (m, n, 3, 1) X-code and an (m, n, 8, 0) X-code. An STS is 7-even-free if and only if it is 6-evenfree. Up to isomorphism, there are two kinds of even 6configurations which may appear in an STS. One is called the grid and the other is the double triangle. Both 6configurations are described by nine points and six blocks: {{a, b, c}, {d, e, f }, {g, h, i}, {a, d, g }, {b, e, h}, {c, f, i}} and {{a, b, c}, {a, d, e}, {c, f, e}, {b, g, h}, {d, h, i}, {f, g, i}} respectively. By definition, an STS is 6-even-free if it simultaneously avoids Pasches, grids, and double triangles. We do not know whether there exists a 6-even-free STS(v ) for any v > 3. However, a moderately large number of triples can be included while keeping 6-even-freeness: Theorem 3.13: There exists a constant c > 0 such that for sufficiently large v there exists a 6-even-free triple packing of order v with cv 1.8 triples. Proof: Let C  be a set of representatives of all of the nonisomorphic even configurations on six or fewer triples and let C  be a configuration consisting of pair of distinct triples sharing a pair of elements. Let C = C   C  . Pick uniformly 6 at random triples from V with probability p = c v - 5 inde1 10   pendently, where c satisfies 0 < c < ( 41·79·83 ) 5 . Let bC be

a random variable counting the configurations isomorphic to a member of C in the resulting set of triples. Define E (bC ) as its expected value. Then v 4
9 3 4 3

E (bC )  =

6

2 4 c6 v 1.8 + f (v ), 9!

p2 +

v 6

6 3

p4 +

v 9

9 3

6

p6

where f (v ) = O(v 1.6 ). By Markov's Inequality, P (bC  2E (bC ))  Hence, P bC  2
9 3

1 . 2

6

c6 v 1.8 + 2f (v ) 9!



1 . 2

Let t be a random variable counting the triples and E (t) its expected value. Then E (t) = p v 3 = c 1.8 v - g (v ), 6

where g (v ) = O(v 0.8 ). Because t is a binomial random variable, by Chernoff's inequality, for sufficiently large v P t< E (t) 2 < e-
E (t) 8

<

1 . 2

Hence, if v is sufficiently large, then with positive probabil(t) ity we have a set B of triples with the property that |B| > E2 and the number of configurations in B isomorphic to a member of C is at most 2
9 3

6

c6 v 1.8 + 2f (v ). 9!

Let ex(v, r) be the maximum cardinality |B| such that there exists an r-even-free triple packing. By deleting a triple from each configuration isomorphic to a member of C , we obtain ex(v, 6)  E (t) -2 2
9 3

6

c6 v 1.8 + h(v ), 9!

where h(v ) = O(v 1.6 ). Then for some positive constant c and sufficiently large v , it holds that ex(v, 6)  cv 1.8 . Hence we have: Theorem 3.14: There exists a constant c > 0 such that for sufficiently large v there exists a (v, cv 1.8 , 1, 2) X-code that is also a (v, cv 1.8 , 3, 1) X-code and a (v, cv 1.8 , 6, 0) X-code. An STS(v ) has approximately v 2 /6 triples. The same technique can be used to obtain a lower bound on ex(v, r) for 12 r  8. In fact, ex(v, 8) is at least O(v 7 ), and hence for sufficiently large v there exists a constant c > 0 such that 12 12 there exists a (v, cv 7 , 1, 2) X-code that is also a (v, cv 7 , 3, 1) 12 X-code and a (v, cv 7 , 8, 0) X-code.

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

7

C. Higher X-Tolerance with the Minimum Fan-Out In general, the probability that a defective digital circuit produces an error at a specific signal output line is quite small. In fact, several errors are unlikely happen simultaneously [11], [45]. Also, multiple Xs with errors are rare [12]. Therefore, Xcodes given in Theorems 3.7 and 3.13 are particularly useful for relatively simple scan-based testing such as built-in selftest (BIST) where the tester is only required to detect defective chips. Nonetheless, more sophisticated X-codes are also useful to improve test quality and/or to identify or narrow down the error sources by taking advantage of more detailed information about when incorrect responses are produced [12]. Hence, for use in higher quality testing and error diagnosis support, it is of theoretical and practical interest to consider (m, n, d, x) X-codes of constant weight x + 1, where x  3 or d  6. For (m, n, 1, 2) X-codes of constant weight three, we employed Theorem 3.3 to obtain an upper bound on the number of codewords. The following theorem gives a generalized upper bound: Theorem 3.15: [46] Let n(x, m, k ) denote the maximum number of columns of a (1, x)-superimposed code such that every column is of length m and has constant weight k . Then, for every x, t and i = 0, 1 or i  x/2t2 , n(x, m, x(t - 1) + 1 + i)  m-i k-i / t t

for all sufficiently large m, with equality if and only if there exists a Steiner t-design S (t, x(t - 1) + 1, m - i). By putting t = 2 and i = 0, we obtain: Corollary 3.16: For an (m, n, 1, x) X-code of constant weight x + 1, m x+1 n / 2 2 for all sufficiently large m, with equality if and only if there is an S (2, x + 1, m). Because the set of columns of the block-point incidence matrix of any S (2, x + 1, m) forms an (m, m(m - 1)/x(x + 1), 1, x) X-code of constant weight x + 1, the existence of Steiner 2-designs is our next interest. For k  {4, 5}, necessary and sufficient conditions for existence of an S (2, k, v ) are known: Theorem 3.17: [47] There exists an S (2, 4, v ) if and only if v  1, 4 (mod 12). Theorem 3.18: [48] There exists an S (2, 5, v ) if and only if v  1, 5 (mod 20). For k  6, the necessary and sufficient conditions on v for existence of an S (2, k, v ) are not known in general; the existence of a Steiner 2-design is solved only in an asymptotic sense [49], although for `small' values of k substantial results are known. For a comprehensive table of known Steiner 2designs, see [50]. As with X-codes from Steiner triple systems, the error detectability can be improved by considering avoidance of even configurations. An S (2, k, v ), (V, B ), is r-even-free if for 1  i  r it contains no subset C  B such that |C| = i and each point appearing in C is contained in exactly an even number

of blocks in C . A generalized Pasch configuration in an S (2, k, v ), (V, B ), is a subset C  B such that |C| = k + 1 and each point appearing in C is contained exactly two blocks of C . As with triple systems, an S (2, k, v ) is (k + 1)-even-free if and only if it contains no generalized Pasch configurations. Theorem 3.19: If an r-even-free S (2, k, v ) for r  k + 1 exists, there exists a (v, v (v - 1)/k (k - 1), 1, k - 1) X-code of constant weight k that is also a (v, v (v - 1)/k (k - 1), k, 1) X-code and a (v, v (v - 1)/k (k - 1), r, 0) X-code. Proof: Let (V, B ) be an r-even-free S (2, k, v ). For every Bi  B define a v -dimensional vector si such that each (i) coordinate sj  si is indexed by a distinct point j  V (i) and sj = 1 if j  Bi , otherwise 0. Then we obtain a (v, v (v - 1)/k (k - 1), 1, k - 1) X-code S = {si : Bi  B} of constant weight k . By definition of an r-even-free S (2, k, v ), it is straightforward to see that S is also a (v, v (v - 1)/k (k - 1), r, 0) X-code. It suffices to prove that S can also be used as a (v, v (v - 1)/k (k - 1), k, 1) X-code. Assume that this is not the case. Then, by following the argument in the proof of Lemma 3.5, B contains a generalized Pasch configuration, a contradiction. Existence of an r-even-free design has been investigated in the study of erasure-resilient codes for redundant array of independent disks (RAID) [51]. In fact, infinitely many r-evenfree S (2, k, v )s can be obtained from affine spaces over Fq [52]. Theorem 3.20: [52] For any odd prime power q and positive integer n  2 the points and lines of AG(n, q ) form a (2q - 1)even-free S (2, q, q n ). By combining Theorems 3.19 and 3.20, we obtain: Theorem 3.21: For any odd prime power q and positive integer n  2, there exists a (q n , q n-1 (q n - 1)/(q - 1), 1, q - 1) X-code of constant weight q that is also an (q n , q n-1 (q n - 1)/(q - 1), q, 1) X-code and a (q n , q n-1 (q n - 1)/(q - 1), 2q - 1, 0) X-code. D. Characteristics of X-Codes from Combinatorial Designs We have given tight upper bounds of compaction ratio for (m, n, 1, x) X-codes with the minimum fan-out and presented explicit construction methods for X-codes that attain the bounds. As far as the authors are aware, these are the first mathematical bounds and construction techniques for this type of optimal X-code with constant weight greater than two. Optimal X-codes given in Theorems 3.7 and 3.21 in particular have higher error detection ability when the number of Xs is smaller than x. The known construction technique using hypergraphs, briefly mentioned in [16], can not guarantee the same error detection ability. To illustrate the usefulness of our X-codes, here we compare the error detection ability of an example X-code that can be generated using Theorem 3.7 with characteristics of Xcodes proposed in [11]. The probability that the example (50, 500, 1, 1) X-code in Table 5 in [11] fails to detect a single error when there are exactly two Xs is around 4.2 × 10-6 . The fan-out of this code is 11. Our X-code from Theorem 3.7, which has the same compaction ratio, has parameters (61, 610, 1, 2). The probability that this X-code fails to detect

8

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

a single error in the same situation is exactly 0. Its fan-out is 3, which is significantly smaller. While the multiple error detection ability of the (50, 500, 1, 1) X-code is not specified in [11], our code can always detect up to three errors when there is only one X, and up to five errors when there is no X. By Theorem 3.9 the probability that our (61, 610, 1, 2) X-code fails to detect errors when there are exactly two Xs and two errors is 1.5 × 10-6 . Therefore, our X-code is ideal when the fan-out problem is critical and/or fault-free simulation rarely produces three or more Xs in an expected response. Very large optimal X-codes with very high error detecting ability and compaction ratio can be easily constructed by the same method. For example, Theorem 3.7 and known results on anti-Pasch STSs immediately give a (601, 60100, 1, 2) Xcode with fan-out 3 and compaction ratio 100. This code is also a (601, 60100, 3, 1) X-code and a (601, 60100, 5, 0) Xcode. Moreover, the probability that it fails to detect errors when there are exactly two Xs and two errors (or exactly three Xs and a single error) is around 1.6 × 10-11 (or 7.3-7 respectively). As far as the authors know, there have been no X-codes available that guarantee as high error detection ability and have very small fan-out. As Theorems 3.7, 3.8, and 3.9 indicate, larger X-codes designed with this method have an even higher compaction ratio and better error detection rate. Because discarding codewords does not affect error detection ability, one may use part of a large X-code to achieve very high test quality when compaction ratio can be compromised to an extent. IV. X-C ODES
OF

binary matrix H  . Taking each row of H  as a codeword, we obtain a set X of mN -dimensional vectors. It suffices to show that for any two arbitrary subsets D, X  X satisfying |D| = d  d, |X | = x  x, and D  X = , it holds that ( X)  ( D) = X. (3)

A RBITRARY W EIGHT

The restriction to low-weight codewords severely limits the compaction ratio of an X-code. Hence, when fan-in and fanout are not of concern, it is desirable to use X-codes with arbitrary weight. In this section we study the compaction ratio and construction methods of such general X-codes. For d = x = 2, a (log2 n(log2 n + 1), n, 2, 2) X-code was constructed for any integer n  2 [14]. Theorem 4.1: [14] For any optimal (m, n, 2, 2) X-code, m  log2 n(log2 n + 1). They also gave an explicit construction method of a (3log3 n, n, 1, 3) X-code. In order to give a more general construction, we employ design theoretic techniques for arrays. Let n  w  2. A perfect hash family, PHF(N ; u, n, w), is a set F of N functions f : Y  X where |Y | = u and |X | = n, such that, for any C  Y with |C | = w, there exists at least one function f  F such that f |C is one-to-one. A PHF(N ; u, n, w) can be described by a u × N matrix with entries from a set of n symbols such that for any w rows there exists at least one column in which each element is distinct. Theorem 4.2: If an (m, n, d, x) X-code and a PHF(N ; u, n, max{d, x} + 1) exist, there exists an (mN, u, d, x) X-code. Proof: Let H be a u × N n-ary matrix representing a PHF(N ; u, n, max{d, x} + 1). Assign each codeword of an (m, n, d, x) X-code to a distinct symbol of the PHF and replace each entry of H by the m-dimensional row vector representing the assigned codeword. Then we obtain a u × mN

By considering a one-to-one function in the PHF, for any max{d, x} + 1 codewords of X at least one set of m coordinates forms max{d, x} + 1 distinct codewords of the original (m, n, d, x) X-code. Hence, for any choice of D and X there exists a subset Y  X of cardinality |Y | = max{0, d + x - (max{d, x} + 1)} such that at least one set of m coordinates in D  (X \ Y ) forms distinct codewords of the original (m, n, d, x) X-code. Because |Y |  d - 1 < |D|, (3) holds for any D and X . Hence, the resulting set X forms an (mN, u, d, x) X-code. Since their introduction in [53], much progress has been made on existence and construction techniques for perfect hash families (see [54]­[58] for recent results). A concise list of known results on perfect hash families is available in [50]. We can use perfect hash families from algebraic curves over finite fields: Theorem 4.3: [59] For positive integers n  w, there exists an explicit construction for an infinite family of PHF(N ; u, n, w) such that N is O(log u). Indeed when n is fixed, a perfect hash family with O(log u) rows can be determined in polynomial time by a greedy method [60]. By combining Theorems 4.2 and 4.3, we can construct infinitely many (m, n, d, x) X-codes where m is O(log n). Theorem 4.4: For any positive integer d and nonnegative integer x, there exists an explicit construction for an infinite family of (m, n, d, x) X-codes, where m is O(log n). The following is a combinatorial recursion for X-codes. Theorem 4.5: If an (m, n, d, x) X-code and an (, n, d 2 , x) X-code exist, there exists an ( + m, 2n, d, x) X-code. Proof: Let X = {s1 , . . . , sn } be an (m, n, d, x) X-code and Y = {t1 , . . . , tn } an (, n, d 2 , x) X-code. Extend each (i) (i) codeword si = (s1 , . . . , sm ) of X by appending  0's so that (i) (i) extended vectors have the form s i = (s1 , . . . , sm , 0, . . . , 0). (i) (i) Extend each codeword ti = (t1 , . . . , tl ) of Y by combining si so that extended vectors have the form t i = (i) (i) (i) (i)   (s1 , . . . , sm , t1 , . . . , tl ). Define A = {s1 , . . . , sn }, B =  {t 1 , . . . , tn }, and C = A  B . We prove that C is an ( + m, 2n, d, x) X-code. Take two subsets D, X  C satisfying |D| = d  d, |X | = x  x, and D X = . As in the proof of Theorem 4.2, it suffices to show that for any choice of D and X the vector obtained by adding all the codewords in D is not covered by the superimposed sum of X , that is, (3) holds. Define a sur(i) (i) (i) (i) jection f of C to X as f : (c1 , . . . , c+m )  (c1 , . . . , cm ). Mapping all codewords of C under f generates two copies of X ; one is from A and the other is from B . Define a (i) (i) surjection g of C to Y  {0} as g : (c1 , . . . , c+m )  (i) (cm + 1(i) , . . . , c+m ). By definition, {g (c) : c  B } = Y and for any c  A the image g (c) is an -dimensional zero

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

9

vector. Let a = |D  A| and b = |D  B |. Because Y is an d (, n, d 2 , x) X-code, if b  2 , g( X )  g( D) = g ( X ). (4)
d 2

If E (AX ) < 1, there exists an (m, n, d, x) X-code. Taking logarithms, m> Hence, if m  2x+1 (d + x) log n > -(d + x) log n , log (1 - 2-x-1 ) -(d + x) log n . log (1 - 2-x-1 )

Hence, we only need to consider the case when b > Suppose to the contrary that (3) does not hold. Then, f( Let a = |{c  X : f (c) = f (d), d  D  B }| and a = |{c  D  A : f (c) = f (d), d  D  B }|. X)  f( D) = f ( X ).

.

(5)

Because {f (c) : c  A} = {f (c) : c  B} = X and (5) holds, b = a + a . As a + b = d and b > d 2 , b  a + a d  + a . 2 (6)

there exists an (m, n, d, x) X-code. Hence, for any optimal (m, n, d, x) X-code with n  max{2d, d + x}, m is at most O(log n). For example, by putting d = x = 2 we know that there exists an (m, n, d, x) X-code if m  32 log n. This significantly improves the upper bound in Theorem 4.1 proved in [14]. V. C ONCLUSIONS By formulating X-tolerant space compaction of test responses combinatorially, an equivalent, alternative definition of X-codes has been introduced. This combinatorial approach gives general design methods for X-codes and bounds on the compaction ratio. Using this model with restricted fanout leads to well-studied objects, the Steiner 2-designs. These provide constructions for X-codes having sufficient error detectability, X-tolerance, maximum compaction ratio, and minimum fan-out. Constant weight X-codes with high error detectability profit from a deep connection with configurations, particularly the Pasch configuration. The combinatorial formulation of X-tolerant compaction can also be applied in conjunction with another compaction technique (such as time compaction). If a tester wants an X-compactor with additional properties, the necessary structure of the compactor may be expressed in design theoretic terms. Our formulation can also be useful for the study of higher error detectability and error diagnosis support employing the appropriate assistance from an Automatic Test Equipment (ATE) [12]. For example, the compaction technique called iCompact can be understood in terms of the model in Section II [17]. The essential idea underlying Theorem 4.6 is the stochastic coding technique for X-tolerant signature analysis [61]. We used a naive value 1/2 as the probability p in the proof of Theorem 4.6. To obtain a better constant coefficient, p should be chosen so that it minimizes the expected value E (AX ), that is, it should minimize m 
d i=1

On the other hand, |X  B|  x - a . Because Y is also an   (, n, d 2 + a , x - a ) X-code, (4) holds, a contradiction. Next, we present a simple nonconstructive existence result for (m, n, d, x) X-codes. Theorem 4.6: Let d, x be a positive integers. For n  max{2d, d + x}, if m  2x+1 (d + x) log n, there exists an (m, n, d, x) X-code. Proof: Let X = {s1 , s2 , . . . , sn } be a set of n m(i) (i) (i) dimensional vectors si = (s1 , s2 , . . . , sm , ) in which each (i) entry sj is defined to be 1 with probability p = 1/2. Let X be a set of x vectors of X and Di a set of i vectors in X \ X . Define A(Di , X ) = 0 if ( X )  ( 1 otherwise, Di ) = X,

and let E (A(Di , X )) be its expected value. Then  i -i  2  j m

E (A(Di , X )) = = Let AX =
X X |X |=x

 -x 1 - 2
d

1j i j

odd

(1 - 2-x-1 )m .

A(Di , X )
i=1
Di Di X =

and E (AX ) its expected value. Then
d

E (AX )

=
X X |X |=x

E (A(Di , X ))
i=1
Di Di X =

d

=
i=1

n x

n-x (1 - 2-x-1 )m i

< nd+x (1 - 2-x-1 )m .

While this optimization does not affect the logarithmic order in Theorem 4.6, it may help a tester determine the target compaction ratio and estimate the error cancellation and masking rate of an X-tolerant Multiple Input Signature Register (XMISR) based on stochastic coding [61]. In this paper we focused on space compaction. Nevertheless, time compaction is of great importance as well. We expect the combinatorial formulation developed here to provide a useful framework for exploring time compaction as well.

n-x  1 - i

1j i j

odd

i j  p (1 - p)i-j +x  . j

10

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

ACKNOWLEDGMENT A substantial part of the research was done while the first author was visiting the Department of Computer Science and Engineering of Arizona State University. He thanks the department for its hospitality. The authors thank an anonymous referee and the editor for helpful comments and valuable suggestions. R EFERENCES
[1] E. J. McCluskey, D. Burek, B. Koenemann, S. Mitra, J. H. Patel, J. Rajski, and J. A. Waicukauski, "Test compression roundtable," IEEE Des. Test. Comput., vol. 20, pp. 76­87, Mar./Apr. 2003. [2] A. Lempel and M. Cohn, "Design of universal test sequences for VLSI," IEEE Trans. Inf. Theory, vol. 31, pp. 10­17, Jan. 1985. [3] G. Seroussi and N. H. Bshouty, "Vector sets for exhaustive testing of logic circuits," IEEE Trans. Inf. Theory, vol. 34, pp. 513­522, May 1988. [4] H. Hollmann, "Design of test sequences for VLSI self-testing using LFSR," IEEE Trans. Inf. Theory, vol. 36, pp. 386­392, Mar. 1990. [5] G. D. Cohen and G. Zemor, "Intersecting codes and independent families," IEEE Trans. Inf. Theory, vol. 40, pp. 1872­1881, Nov. 1994. [6] N. Benowitz, D. F. Calhoun, G. E. Alderson, J. E. Bauer, and C. T. Joeckel, "An advanced fault isolation system for digital logic," IEEE Trans. Comput., vol. C-24, pp. 489­497, May 1975. [7] E. J. McCluskey, Logic Design Principles with Emphasis on Testable Semi-Custom Circuits. Englewood Cliffs, NJ: Prentice-Hall, 1986. [8] N. R. Saxena and E. J. McCluskey, "Parallel signature analysis design with bounds on aliasing," IEEE Trans. Comput., vol. 46, pp. 425­438, Apr. 1997. [9] C. Barnhart, V. Brunkhorst, F. Distler, O. Farnsworth, B. Keller, and B. Koenemann, "OPMISR: The foundation for compressed ATPG vectors," in Proc. Int. Test Conf., 2001, pp. 748­757. [10] C. Barnhart, V. Brunkhorst, F. Distler, O. Farnsworth, A. Ferko, B. Keller, D. Scott, B. Koenemann, and T. Onodera, "Extending OPMISR beyond 10x scan test efficiency," IEEE Design Test Comput., vol. 19, pp. 65­73, Sep. 2002. [11] S. Mitra, S. S. Lumetta, M. Mitzenmacher, and N. Patil, "X-tolerant test response compaction," IEEE Des. Test. Comput., vol. 22, pp. 566­574, Nov. 2005. [12] S. Mitra and K. S. Kim, "X-compact: An efficient response compaction technique," IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., vol. 23, pp. 421­432, Mar. 2004. [13] S. Mitra, S. Kallepalli, and K. S. Kim, "Analysis of X-compact for industrial designs," Intel Corp., 2003. [14] S. S. Lumetta and S. Mitra, "X-codes: Theory and applications of unknowable inputs," Center for Reliable and High-Performance Computing, Univ. of Illinois at Urbana Champaign, Tech. Rep. CRHC-03-08 (also UILU-ENG-03-2217), Aug. 2003. [15] ----, "X-codes: Error control with unknowable inputs," in Proc. IEEE Intl. Symp. Information Theory, Yokohama, Japan, June 2003, p. 102. [16] P. Wohl and L. Huisman, "Analysis and design of optimal combinational compactors," in Proc. 21st IEEE VLSI Test Symp., April/May 2003, pp. 101­106. [17] J. H. Patel, S. S. Lumetta, and S. M. Reddy, "Application of SalujaKarpovsky compactors to test responses with many unknowns," in Proc. 21st IEEE VLSI Test Symp., 2003, pp. 107­112. [18] T. R. N. Rao and E. Fujiwara, Error-Control Coding for Computer Systems. Englewood Cliffs, NJ: Prentice-Hall, 1989. [19] K. K. Saluja and M. Karpovsky, "Testing computer hardware through data compression in space and time," in Proc. Int. Test Conf., 1983, pp. 83­93. [20] W. H. Kautz and R. R. Singleton, "Nonrandom binary superimposed codes," IEEE Trans. Inf. Theory, vol. 10, pp. 363­377, Jul. 1964. [21] E. Sperner, "Ein satz u ¨ ber Untermengen einer endlichen Menge," Math. Z., vol. 27, pp. 544­548, 1928. [22] D. R. Stinson and R. Wei, "Some new upper bounds for cover-free families," J. Combin. Theory Ser. A, vol. 90, pp. 224­234, 2000. [23] A. G. D'yachkov and V. V. Rykov, "Bounds on the length of disjunctive codes," Probl. Contr. Inform. Theory, vol. 11, pp. 7­33, 1982, in Russian. [24] Z. F¨ uredi, "On r -cover-free families," J. Combin. Theory, Ser. A, vol. 73, pp. 172­173, 1996. [25] M. Ruszink´ o, "On the upper bound of the size of the r -cover-free families," J. Combin. Theory, Ser. A, vol. 66, pp. 302­310, 1994.

[26] A. G. D'yachkov, V. V. Rykov, and A. M. Rashad, "Superimposed distance codes," Probl. Contr. Inform. Theory, vol. 18, pp. 237­250, 1989. [27] D. Z. Du and F. K. Hwang, Combinatorial Group Testing and Its Applications, 2nd ed. Singapore: World Scientific, 2000. [28] H. L. Fu and F. K. Hwang, "A novel use of t-packings to construct d-disjunct matrices," Discrete Appl. Math., vol. 154, pp. 1759­1762, 2006. [29] A. G. D'yachkov, A. J. Macula, and V. V. Rykov, "New constructions of superimposed codes," IEEE Trans. Inf. Theory, vol. 46, pp. 284­290, Jan. 2000. [30] A. J. Macula, "A simple construction of d-disjunct matrices with certain constant weights," Discrete Math., vol. 162, pp. 311­312, 1996. [31] ----, "Error-correcting nonadaptive group testing with de -disjunct matrices," Discrete Appl. Math., vol. 80, pp. 217­222, 1997. [32] H. G. Yeh, "d-Disjunct matrices: bounds and Lov´ asz Local Lemma," Discrete Math., vol. 253, pp. 97­107, 2002. [33] A. De Bonis and U. Vaccaro, "Constructions of generalized superimposed codes with applications to group testing and conflict resolution in multiple access channels," Theor. Comput. Sci., vol. 306, pp. 223­243, 2003. [34] C. J. Colbourn and A. Rosa, Triple Systems. Oxford: Oxford Univ. Press, 1999. [35] P. Erd os, P. Frankl, and Z. F¨ uredi, "Families of finite sets in which no set is covered by the union of two others," J. Combin. Theory, Ser. A, vol. 33, pp. 158­166, 1982. [36] C. J. Colbourn and Y. Fujiwara, "Small stopping sets in Steiner triple systems," Cryptography and Communications, vol. 1, no. 1, pp. 31­46, 2009. [37] M. J. Grannell, T. S. Griggs, and C. A. Whitehead, "The resolution of the anti-Pasch conjecture," J. Combin. Des., vol. 8, pp. 300­309, 2000. [38] A. C. H. Ling, C. J. Colbourn, M. J. Grannell, and T. S. Griggs, "Construction techniques for anti-Pasch Steiner triple systems," J. Lond. Math. Soc. (2), vol. 61, pp. 641­657, 2000. [39] D. R. Stinson and Y. J. Wei, "Some results on quadrilaterals in Steiner triple systems," Discrete Math., vol. 105, pp. 207­219, 1992. [40] M. J. Grannell, T. S. Griggs, and J. S. Phelan, "A new look at an old construction for Steiner triple systems," Ars Combinat., vol. 25A, pp. 55­60, 1988. [41] A. E. Brouwer, "Steiner triple systems without forbidden subconfigurations," Mathematisch Centrum Amsterdam, ZW 104/77, 1977.  an [42] C. J. Colbourn, E. Mendelsohn, A. Rosa, and J. Sir´  , "Anti-Mitre Steiner triple systems," Graphs Combin., vol. 10, pp. 215­224, 1994. [43] M. J. Grannell, T. S. Griggs, and E. Mendelsohn, "A small basis for fourline configurations in Steiner triple systems," J. Combin. Des., vol. 3, pp. 51­59, 1995. [44] C. J. Colbourn, "The configuration polytope of -line configurations in Steiner triple systems," Mathematica Slovaca, vol. 59, no. 1, pp. 77­108, 2009. [45] P. Wohl, J. A. Waicukauski, and T. W. Williams, "Design of compactors for signature-analyzers in built-in-self-test," in Proc. Int. Test Conf., 2001, pp. 54­63. [46] P. Erd os, P. Frankl, and Z. F¨ uredi, "Families of finite sets in which no set is covered by the union of r others," Israel J. Math., vol. 51, pp. 75­89, 1985. [47] H. Hanani, "The existence and construction of balanced imcomplete block designs," Ann. Math. Statist., vol. 32, pp. 361­386, 1961. [48] ----, "On balanced incomplete block designs with blocks having five elements," J. Combin. Theory Ser. A, vol. 12, pp. 184­201, 1972. [49] R. M. Wilson, "An existence theory for pairwise balanced designs. III. Proof of the existence conjectures," J. Combin. Theory Ser. A, vol. 18, pp. 71­79, 1975. [50] C. J. Colbourn and J. H. Dinitz, Eds., Handbook of Combinatorial Designs. Boca Raton, FL: Chapman & Hall/CRC, 2007. [51] Y. M. Chee, C. J. Colbourn, and A. C. H. Ling, "Asymptotically optimal erasure-resilient codes for large disk arrays," Discrete Appl. Math., vol. 102, pp. 3­36, 2000. [52] M. M¨ uller and M. Jimbo, "Erasure-resilient codes from affine spaces," Discrete Appl. Math., vol. 143, pp. 292­297, 2004. [53] K. Mehlhorn, Data Structures and Algorithms 1. Berlin, Germany: Springer, 1984. [54] D. Tonien and R. Safavi-Naini, "Recursive constructions of secure codes and hash families using difference function families," J. Combin. Theory Ser. A, vol. 113, pp. 664­674, 2006. [55] Tran van Trung and S. S. Martirosyan, "New constructions for IPP codes," Des. Codes Cryptgr., vol. 32, pp. 227­239, 2005.

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

11

[56] D. Deng, D. R. Stinson, and R. Wei, "The Lov´ asz local lemma and its applications to some combinatorial arrays," Des. Codes Cryptgr., vol. 32, pp. 121­134, 2004. [57] R. A. Walker II and C. J. Colbourn, "Perfect hash families: Construction and existence," Journal of Mathematical Cryptology, vol. 1, pp. 125­ 150, 2007. [58] S. S. Martirosyan and Tran van Trung, "Explicit constructions for perfect hash families," Des. Codes Cryptogr., vol. 46, no. 1, pp. 97­112, 2008. [59] H. Wang and C. Xing, "Explicit constructions of perfect hash families from algebraic curves over finite fields," J. Combin. Theory Ser. A, vol. 93, pp. 112­124, 2001. [60] C. J. Colbourn, "Constructing perfect hash families using a greedy algorithm," in Coding and Cryptology, Y. Li, S. Zhang, S. Ling, H. Wang, C. Xing, and H. Niederreiter, Eds. Singapore: World Scientific, 2008. [61] S. Mitra, S. S. Lumetta, and M. Mitzenmacher, "X-tolerant signature analysis," in Proc. Int. Test Conf., 2004, pp. 432­441.

Necessary and sufficient conditions are established for an integer vector to be the f-vector of some pure simplicial complex of rank three, and also for an integer vector to be thef-vector of some pure simplicial multicomplex of rank three. For specified numbers of sets of cardinality one and cardinality two, an upper bound on the number of sets of cardinality three is established using shifting arguments. Then techniques from combinatorial design theory are used to establish a lower bound. Then it is shown that every number of sets of cardinality three between the lower and the upper bound can be realized. This characterization is restated to determine the precise spectrum of possible numbers of sets of cardinality two for specified numbers of sets of cardinality one and three. For simplicial complexes, these spectra are not always intervals, and the gaps are determined precisely. For simplicial multicomplexes, an alternative proof is given that these spectra are always intervals.IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

575

A Discrete Particle Swarm Optimization for Covering Array Generation
Huayao Wu, Changhai Nie, Member, IEEE, Fei-Ching Kuo, Member, IEEE, Hareton Leung, Member, IEEE, and Charles J. Colbourn

Abstract--Software behavior depends on many factors. Combinatorial testing (CT) aims to generate small sets of test cases to uncover defects caused by those factors and their interactions. Covering array generation, a discrete optimization problem, is the most popular research area in the field of CT. Particle swarm optimization (PSO), an evolutionary search-based heuristic technique, has succeeded in generating covering arrays that are competitive in size. However, current PSO methods for covering array generation simply round the particle's position to an integer to handle the discrete search space. Moreover, no guidelines are available to effectively set PSOs parameters for this problem. In this paper, we extend the set-based PSO, an existing discrete PSO (DPSO) method, to covering array generation. Two auxiliary strategies (particle reinitialization and additional evaluation of gbest) are proposed to improve performance, and thus a novel DPSO for covering array generation is developed. Guidelines for parameter settings both for conventional PSO (CPSO) and for DPSO are developed systematically here. Discrete extensions of four existing PSO variants are developed, in order to further investigate the effectiveness of DPSO for covering array generation. Experiments show that CPSO can produce better results using the guidelines for parameter settings, and that DPSO can generate smaller covering arrays than CPSO and other existing evolutionary algorithms. DPSO is a promising improvement on PSO for covering array generation. Index Terms--Combinatorial testing (CT), covering array generation, particle swarm optimization (PSO).

I. I NTRODUCTION S SOFTWARE functions and run-time environments become more complex, testing of modern software systems is becoming more expensive. Effective detection of

A

Manuscript received May 12, 2013; revised December 29, 2013 and May 18, 2014; accepted September 28, 2014. Date of publication October 9, 2014; date of current version July 28, 2015. This work was supported in part by the National Natural Science Foundation of China under Grant 61272079, in part by the Research Fund for the Doctoral Program of Higher Education of China under Grant 20130091110032, in part by the Science Fund for Creative Research Groups of the National Natural Science Foundation of China under Grant 61321491, in part by the Major Program of National Natural Science Foundation of China under Grant 91318301, and in part by the Australian Research Council Linkage under Grant LP100200208. (Corresponding author: Changhai Nie.) H. Wu and C. Nie are with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China (e-mail: hywu@outlook.com; changhainie@nju.edu.cn). F.-C. Kuo is with the Faculty of Information and Communication Technologies, Swinburne University of Technology, Hawthorn, VIC 3122, Australia (e-mail: dkuo@swin.edu.au). H. Leung is with the Department of Computing, Hong Kong Polytechnic University, Hong Kong (e-mail: hareton.leung@polyu.edu.hk). C. J. Colbourn is with Arizona State University, Tempe, AZ 85287-8809, USA (e-mail: colbourn@asu.edu). Digital Object Identifier 10.1109/TEVC.2014.2362532

failures at a low cost is a key issue for test case generation. Combinatorial testing (CT) is a popular testing method to detect failures triggered by various factors and their interactions [1]. By employing covering arrays as test suites, the CT method aims to sample the large combination space with few test cases to cover different interactions among a fixed number of factors. Kuhn and Reilly [2] shows that more than 70% of the failures in certain software were caused by the interactions of one or two factors, and almost all the failures could be detected by checking the interactions among six factors. Therefore, CT can be an effective method in practice. Generating a covering array with fewest tests (minimum size) is a major challenge in CT. In general, the minimum size of a covering array is unknown; hence, methods have focused on finding covering arrays that have as few tests as possible at reasonable search cost. The many methods that have been proposed can be classified into two main groups: 1) mathematical methods and 2) computational methods [1]. Mathematical (algebraic or combinatorial) methods typically exploit some known combinatorial structure. Computational methods primarily use greedy strategies or heuristic-search techniques to generate covering arrays, due to the size of the search space. Mathematical methods yield the best possible covering arrays in certain cases. For example, orthogonal arrays used in the design of experiments provide covering arrays with a number of tests that is provably minimum. However, all known mathematical methods can be applied only for restrictive sets of factors. This limitation has led to an emphasis on computational methods. Greedy algorithms have been quite effective in generating covering arrays, but their accuracy suffers from becoming trapped in local optima. In recent years, search-based software engineering (SBSE) has focused on using search-based optimization algorithms to find high-quality solutions for software engineering problems. Inspired by SBSE, many artificial intelligence-based heuristic-search techniques have been applied to software testing. For example, simulated annealing (SA) [3]­[7], genetic algorithm (GA) [8]­[10], and ant colony optimization (ACO) [9], [11], [12] have all been applied to covering array generation. These techniques can generate any types of covering arrays, and the constraint solving and prioritization techniques can be easily integrated. Their applications have been shown to be effective, producing relatively small covering arrays in many cases. Particle swarm optimization (PSO), a relatively new evolutionary algorithm,

1089-778X c 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

576

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

has also been used in this area [13]­[16]. It is easy to implement and has fast initial progress. The conventional PSO (CPSO) algorithm was originally designed to find optimal or near optimal solutions in a continuous space. Nevertheless, many discrete PSO (DPSO) algorithms and frameworks have been developed to solve discrete problems [17]­[22]. For covering array generation, current discrete methods [13]­[16] simply round the particle's position to an integer while keeping the velocity as a real number. They suffer from two main shortcomings. First, the performance of PSO is significantly impacted by its parameter settings. In [23], effects of the general parameter selection and initial population of PSO have been analyzed, but no guidelines on parameter settings have been reported for covering array generation. Hence, a clear understanding of how to set these execution parameters is needed. Second, simple rounding fractional positions to integers introduces a substantial source of errors in the search. Instead, a specialized DPSO version is needed. Because current PSO methods show promise for generating covering arrays, these two main shortcomings should be addressed. In this paper, we adapt set-based PSO (S-PSO) [18] to generate covering arrays. S-PSO utilizes set and probability theories to develop a new representation scheme for combinatorial optimization problems as well as refining the related evolution operators. In our adaptation of S-PSO, two auxiliary strategies are proposed to enhance performance, and a novel DPSO algorithm is thus proposed. DPSO has the same conceptual basis and exhibits similar search behavior to CPSO, with parameters playing similar roles. Then, we explore the optimal parameter settings for both CPSO and DPSO to improve their performance, and identify recommended settings. Furthermore, because many CPSO variants [24]­[27] can be easily extended to discrete versions based on our DPSO, the performance of these discrete versions is also compared with their original ones. Finally, we compare CPSO and DPSO with existing GA and ACO [9], [11] algorithms to generate covering arrays. The main contributions of this paper are as follows. 1) Based on the set-based representation, we design a version of S-PSO [18] for covering array generation. 2) We propose two auxiliary strategies, particle reinitialization, and additional evaluation of gbest, to enhance the performance of PSO. A novel DPSO for covering array generation is proposed. 3) We design experiments to explore the optimal parameter settings for CPSO and DPSO for covering array generation. 4) We implement original and discrete versions of four representative PSO variants (TVAC [24], CLPSO [25], APSO [26], and DMS-PSO [27]) to compare their efficacy for covering array generation. The rest of this paper is organized as follows. Section II gives background on CT, covering array generation, and the CPSO algorithm. Section III summarizes related work. Section IV presents our DPSO algorithm, including the representation scheme, related operators, and two auxiliary strategies. Section V evaluates the performance of CPSO and

TABLE I E-C OMMERCE S OFTWARE S YSTEM

DPSO, and explores optimal parameter settings. Section VI gives a comparison among CPSO, DPSO, and original and discrete variants. Section VII compares CPSO and DPSO with GA and ACO. Section VIII concludes this paper and outlines future work. II. BACKGROUND A. CT Suppose that the behavior of the software under test (SUT) is controlled by n independent factors, which may represent configuration parameters, internal or external events, user inputs, and the like. The ith factor has i discrete levels, chosen from the finite set Vi of size i . An n-tuple (x1 , x2 , . . . , xn ) forms a test case, where xi  Vi for 1  i  n. Consider a simple e-commerce software system [15]. This system consists of five different components. Each of these five components can be regarded as a factor, and its configurations can be regarded as different levels. Table I shows these five factors and their corresponding levels. In this example, n = 5, 1 = 2 = 3 = 2, 4 = 5 = 3. System failures are often triggered by interactions among some factors, which can be represented by the combinations of factor levels. In order to detect these failures, their combinations should be covered at least once by the test suite. A t-way schema can be used to represent them. Definition 1 (t-way schema): The n-tuple (-, y1 , . . . , yt , . . . ) is a t-way schema (t > 0) when some t factors have fixed levels and the others can take any valid levels, represented as "-." For example, suppose that when factor Payment Server takes the level Master and factor Web Server takes the level Apache, a system failure occurs. To detect this failure, the 2-way schema (Master, ­, Apache, ­, ­) must be covered at least once by the test suite. To simplify later discussion, we use the index in the level set of each factor to present a schema. For example, (0, ­, 1, ­, ­) is used to represent (Master, ­, Apache, ­, ­). Exhaustive testing covers all n-way schemas of the system; in our example, it uses 2 × 2 × 2 × 3 × 3 = 72 test cases. Testing cost becomes prohibitive as the numbers of factors and levels increase. Moreover, because only interactions among few factors are likely to trigger failures [2], testing high-way schemas can lead to many uninformative test cases. At the other extreme, if we only guarantee to cover each 1-way schema once, only three test cases are needed (a single test case can cover five 1-way schemas at most). But we may fail to detect some interaction triggered failures involving two factors.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

577

TABLE II C OVERING A RRAY CA(9; 2, 23 32 )

TABLE III A DDING T HREE T EST C ASES TO C ONSTRUCT VCA(12; 2, 23 32 , CA(3, 22 31 ))

Instead, CT covers all t-way schemas. Such a test suite is a t-way covering array, with t being the covering strength. The value of t determines the depth of coverage. It is a key setting of CT, and should be decided by the testers. We give a precise definition. Definition 2 (Covering Array): If an N ×n array, where N is the number of test cases, has the following properties: 1) each column i (1  i  n) contains only elements from the set Vi with i = |Vi | and 2) the rows of each N × t sub array cover all |Vk1 | × |Vk2 | × . . . × |Vkt | combinations of the t columns at least once, where t  n and 1  k1 < . . . < kt  n, then it is a t-way covering array, denoted by CA(N ; t, n, ( 1 , 2 , . . . , n )). When 1 = 2 = . . . = n = , it is denoted by CA(N ; t, n, ). Reference [2] demonstrated that more than 70% failures can be detected by a 2-way covering array, and almost all failures can be detected by a 6-way covering array. Hence, using CT, we can detect many failures of the system by applying relatively low strength covering arrays. In other words, CT can greatly reduce the size of test suite while maintaining high fault detection ability. In the example of Table I, if we only consider the interactions between any two factors, only nine test cases are required to construct a 2-way covering array instead of 72 for exhaustive testing. Table II shows a covering array, where each row represents one test case. This table covers all possible 2-way schemas for any two factors corresponding to the columns. For example, consider factor Payment Server and User Browser, all 2 × 3 = 6 schemas, (Master, ­, ­, Firefox, ­) (Master, ­, ­, Explorer, ­), (Master, ­, ­, Chrome, ­), (VISA, ­, ­, Firefox, ­), (VISA, ­, ­, Explorer, ­), (VISA, ­, ­, Chrome, ­), can be found in the table. For convenience, if several groups of gi factors (gi < n) have g the same number of levels ak , ak i can be used to represent these factors and their levels. Thus, the covering array can g g g gi = n, be denoted by CA(N ; t, a11 , a22 , . . . , ak k ) where n or CA(N ; t, a ) when g1 = n and a1 = a. For example, the covering array in Table II is a CA(9; 2, 23 32 ). In many software systems, the impacts of the interactions among factors are not uniform. Some interactions may be more prone to trigger system failures, while other may have little or no impact on the system. To effectively detect these different interactions, variable strength (VS) covering arrays can be applied. This can offer different covering strengths

to different groups of factors, and can therefore provide a practical approach to test real applications. Definition 3 (VS Covering Array): A VS covering array, m 1 denoted by VCA(N ; t, a11 . . . akk , CA1 (t1 , bm . . . bp p ), . . . , 1 nq 1 CAj (tj , cn 1 . . . cq )), is an N × n covering array of covering strength t containing one or more sub covering arrays, namely CA1 , . . . , CAj , each of which has covering strength t1 , . . . , tj all larger than t. Consider the e-commerce system shown in Table I. If the interactions of three factors, Payment Server, Web Server, and Business Database, have a higher probability to trigger system failures, then a VS covering array can be constructed. As in Table II, only three more test cases (Table III) are needed to construct the VCA(12; 2, 23 32 , CA(3, 22 31 )). With these 12 test cases, not only are all 2-way schemas of all five factors covered, but also all 3-way schemas of these three factors (Payment Server, Web Server, and Business Database) are covered. B. Covering Array Generation Covering arrays are used as test suites in CT. Covering array generation is the process of test suite construction. It is the most active area in CT with more than 50% of research papers focusing on this field [1]. Due to limited testing resources, all aim to construct a minimal covering array while still maintaining full coverage of combinations. Computational methods have been used widely for covering array generation because they can be applied to any systems. In general, these methods generate all possible combinations first. Then they generate test cases to cover these combinations one-by-one. One-testat-a-time is the most widely used strategy among evolutionary algorithms to generate a covering array. The one-test-at-a-time strategy was popularized by AETG [28] and was further used by Bryce and Colbourn [29]. This strategy takes the model of SUT(n, 1 , . . . , n ) where n is the number of factors and i is the number of valid levels of factor i, and the covering strength t as input. At first, an empty test suite TS and a set S of t-way schemas to be covered are initialized. In each iteration, a test case is generated with the highest fitness value according to some heuristic techniques. Then it is added to TS and the t-way schemas covered by it are removed. When all the t-way schemas have been covered, the final test suite TS is returned. This process is shown in Algorithm 1. In this strategy, a fitness function must be used to evaluate the quality of a candidate test case (line 6 in Algorithm 1). It is an important part of all heuristic techniques. In covering array generation, the fitness function takes the test case as the input and then outputs a fitness value representing its "goodness." It is defined as follows.

578

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

Algorithm 1 One-Test-at-a-Time Strategy 1: Input: SUT(n, 1 , . . . , n ) and covering strength t 2: Output: covering array TS 3: TS =  4: Construct S (all the t-way schemas to be covered) based on n, 1 , . . . , n and t 5: while S =  do 6: Generate a test case p with the highest fitness value according to some heuristics 7: Add p to the test suite TS 8: Remove the t-way schemas covered by p from S 9: end while 10: return TS

its corresponding position: vi,j (k + 1) =  × vi,j (k) + c1 × r1,j × (pbesti,j - xi,j (k)) (2) + c2 × r2,j × (gbestj - xi,j (k)) xi,j (k + 1) = xi,j (k) + vi,j (k + 1). (3) The best position of particle i in its history is pbesti , and gbest is the best position among all particles. The velocityupdate equation (2) captures the three basic concepts of PSO. The first is inertia, the tendency of the particle to continue in the direction it has been moving. The second is memory of the best position ever found by itself. The third is cooperation using the best position found by other particles. The parameter  is inertia weight. It controls the balance between exploration (global search state) and exploitation (local search state). Two positive real numbers c1 and c2 are acceleration constants that control the movement tendency toward the individual and global best position. Most studies set  = 0.9, and c1 = c2 = 2 to get the best balance [17], [31], [32]. In addition, r1,j and r2,j are two uniformly distributed random numbers in the range of [0.0, 1.0], used to ensure the diversity of the population. If the problem domain (the search space of particles) has bounds, a bound handling strategy is adopted to keep the particles inside the search space. Many different strategies have been proposed [33]. In the reflecting strategy, when a particle exceeds the bound of the search space in any dimension, the particle direction is reversed in this dimension to get back to the search space. For example, in case of a dimension with a range of values from 0 to 2, if a particle moves to 3, its position is reversed to 1. In addition, as the velocity can increase over time, a limit is set on velocity to prevent an infinite velocity or invalid position for the particle. Setting a maximum velocity, which determines the distance of movement from the current position to the possible target position, can reduce the likelihood of explosion of the swarm traveling distance [31]. Generally, the value of the maximum velocity is selected as i /2, where i is the range of dimension i. The pseudocode in Algorithm 2 presents the process of generating a test case by PSO. This algorithm can be invoked by line 6 of Algorithm 1 to generate a test case for t-way schemas. The n factors of the test case can be treated as an n-dimensional hyperspace. A particle pi = (x1 , x2 , . . . , xn ) can be regarded as a candidate test case. The fitness function is that of Definition 4, the number of uncovered t-way schemas in the generated test suite that are covered by particle pi . PSO employs real numbers but the valid values are integers for covering array generation, so each dimension of particle's position can be rounded to an integer while maintaining the velocity as a real number. This method is used in all prior research applying PSO to covering array generation [13]­[16]. III. R ELATED W ORK In this section, three different but related aspects are discussed. We first summarize search-based CT, especially the current applications of PSO for covering array generation. Then, we introduce prior research on discrete versions of PSO,

Definition 4 (Fitness Function): Let TS be the generated test set, and p be a test case. Then fitness(p) is the number of uncovered t-way schemas in TS that are covered by p. The fitness function can be formulated as fitness(p) = |schemat ({p}) - schemat (TS)| (1)

where schemat (TS) represents the set of all t-way schemas covered by test set TS, and | · | stands for cardinality. When t t-way schemas covered by p are not covered by TS, all Cn the fitness function reaches the maximum value fitness(p) = t. |schemat ({p})| = Cn For example, consider the 2-way covering array generation of the e-commerce system shown in Table II. Suppose that TS consists of test cases (0, 0, 0, 0, 0) and (0, 0, 1, 1, 1). The fitness function computes the fitness value for a candidate test case p = (1, 0, 1, 2, 2) as follows: because p covers ten 2-way schemas, namely schema2 ({p}) = {(1, 0, ­, ­, ­), (1, ­, 1, ­, ­,), (1, ­, ­, 2, ­), (1, ­, ­, ­, 2), (­, 0, 1, ­, ­), (­, 0, ­, 2, ­), (­, 0, ­, ­, 2), (­, ­, 1, 2, ­), (­, ­, 1, ­, 2), (­, ­, ­, 2, 2)} and TS only covers (­, 0, 1, ­, ­) in p, the function returns fitness(p) = 9. C. PSO PSO is a swarm-based evolutionary computation technique. It was developed by Kennedy et al. [30], inspired by the social behavior of bird flocking and fish schooling. PSO utilizes a population of particles as a set of candidate solutions. Each of the particles represents a certain position in the problem hyperspace with a given velocity. A fitness function is used to evaluate the quality of each particle. Initially, particles are distributed in the hyperspace uniformly. Then each particle repeatedly updates its state according to the individual best position in its history (pbest) and the current global best position (gbest). Eventually, each particle possibly moves toward the direction of the individual optimum and global optimum, and finds an optimal or near optimal solution. Suppose that the problem domain is a D-dimensional hyperspace. Then the position and velocity of particle i can be represented by xi  RD and vi  RD respectively. CPSO uses the following equations to update a particle's velocity and position, where vi,j (k) represents the jth component of the velocity of particle i at the kth iteration, and xi,j (k) represents

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

579

Algorithm 2 Generate Test Case by PSO 1: Input: SUT(n, 1 , . . . , n ), covering strength t and the related parameters of PSO 2: Output: the best test case gbest 3: it = 0, gbest = NULL 4: for each particle pi do 5: Initialize the position xi and velocity vi randomly 6: end for 7: while it < maximum iteration do 8: for each particle pi do 9: Compute the fitness value fitness(pi ) 10: if fitness(pi ) > fitness(pbesti ) then 11: pbesti = pi 12: end if 13: if fitness(pi ) > fitness(gbest) then 14: gbest = pi 15: end if 16: end for 17: for each particle pi do 18: Update the velocity and position according to Equations 2 and 3 19: Apply maximum velocity limitation and bound handling strategy 20: end for 21: it = it + 1 22: end while 23: return gbest

applied PSO to 2-way covering array generation. They further used a test suite minimization algorithm to reduce the size of the generated covering array. Current applications of PSO for covering array generation can yield smaller covering arrays than most greedy algorithms, but they all apply the same rounding operator to the particle's position, and they lack guidelines on the parameter settings. B. Discrete Versions of PSO PSO was initially developed to solve problems in continuous space, but PSO can also be applied to some discrete optimization problems including binary or general integer variables. Many discrete versions of PSO have been proposed [17]­[22]. Chen et al. [18] classify existing algorithms into four types. 1) Swap operator-based PSO [19] uses a permutation of numbers as position and a set of swaps as velocity. 2) Space transformation-based PSO [20] uses various techniques to transform the position, defined as a vector of real numbers, to the corresponding solution in discrete space. 3) Fuzzy matrix-based PSO [21] defines the position and velocity as a fuzzy matrix, and decode it to a feasible solution. 4) Incorporating-based PSO [22] consists of hybrid methods incorporating some heuristic or problem dependent techniques. Chen et al. [18] also propose a S-PSO method based on sets with probabilities, which we later adapt to represent a particle's velocity. S-PSO uses a set-based scheme to represent the discrete search space. The velocity is defined as a set with probabilities, and the operators are all replaced by procedures defined on the set. They extend some PSO variants to discrete versions and test them on the traveling salesman problem and the multidimensional knapsack problem. They show that the discrete version of CLPSO [25] can perform better than other variants. S-PSO was found to characterize the combinatorial optimization problems very well. Gong et al. [36] then employed such a set-based representation scheme for solving vehicle routing problems to obtain a new method, S-PSO-VRPTW. C. PSO Variants The original PSO may become trapped in a local optimum. In order to improve the performance, many variants have been proposed [17], [24]­[27]. Chen et al. [37] classify these variants into four different groups. The first group of variants aims to adjust the control parameters during the evolution, for example by decreasing inertia weight  linearly from 0.9 to 0.4 over the search process. The second group employs different neighborhood topologies. The basic local version of PSO makes the particle learn from the local best position lbesti found by particle i's neighborhood instead of the global best position gbest. RPSO and VPSO are two common versions which use a ring topology and a Von Neumann topology, respectively. The third group uses hybrid strategies with other search techniques. Many types of operators, such as genetic operators

to improve CPSO for discrete problems. Finally, some PSO variants are introduced. These try to enhance PSO for conventional continuous problems by adopting strategies that can further improve covering array generation. A. PSO in Search-Based CT SBSE has grown quickly in recent years. Many problems in software engineering have been formulated into searchbased optimization problems, and heuristic techniques have been used to find solutions. Software testing is a major topic in software engineering. Many heuristic techniques have also been applied to this field, including functional testing, mutation testing, stress testing, temporal testing, CT, and regression testing. Currently, many classic heuristic techniques, such as SA [3]­[7], GA [8]­[10], and ACO [9], [11], [12] have been applied to generate uniform and VS covering arrays successfully. PSO has also been applied to software testing. Windisch et al. [34] applied PSO to structural testing, and compared its performance with GA. They showed that PSO outperformed GA for most cases in terms of effectiveness and efficiency. Ganjali [35] proposed a framework for automatic test partitioning based on PSO, observing that PSO performed better than other existing heuristic techniques. PSO has been applied to covering array generation. Ahmed and Zamli [15] proposed frameworks of PSO to generate different kinds of covering arrays, including t-way (PSTG [14]) and VS-PSTG. Then they extended PSTG to support VS covering array generation [16]. Chen et al. [18] also

580

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE IV D IFFERENT G ROUPS OF DPSO AND PSO VARIANTS

(selection, crossover, and mutation), evolutionary computation paradigms and biology inspired operators have been used. The fourth uses multiswarm techniques. Several sets of swarms optimize different components of the solution concurrently or cooperatively. In our experiments, four representative PSO variants are included, as follows. 1) Ratnaweera et al. [24] proposed a typical variant of the first group, TVAC, which uses a time varying inertia weight and acceleration constant to adjust the parameters. 2) Liang et al. [25] proposed the CLPSO method, a wellknown variant of the second group, which allows the particle to learn from other particles' individual best positions in different dimensions. 3) Zhan et al. [26] proposed an adaptive PSO (APSO) that can be seen as a variant of the third group. They developed an evolutionary state estimation to adaptively control the parameter and used an elitist learning strategy. 4) DMS-PSO [27], a variant of the fourth group, was proposed by Liang and Suganthan [27]. It is characterized by a set of swarms with small sizes and these swarms are frequently regrouped. In summary, Table IV lists the different groups of discrete versions of PSO and PSO variants. IV. DPSO In this section, a new DPSO for covering array generation is presented. We firstly illustrate the weakness of CPSO with a simple example. Then the representation scheme of a particle's velocity and the corresponding redefined operators are introduced. Finally, we give two auxiliary strategies to enhance the performance of DPSO. A. Example In CPSO, a particle's position represents a candidate test case; its velocity, a real vector, represents the movement tendency of this particle in each dimension. This scheme is meaningful in a continuous optimization problem, because an optimal solution may exist near the current best particle's position. So it is desirable to move the particle to this area for further search. This may not hold for covering array generation.

Here, we use CA(N ; 2, 34 ) as an example. Suppose that three test cases (0, 1, 0, 1), (2, 1, 1, 2), and (1, 2, 0, 0) have been generated and added to TS in Algorithm 1, and the fourth one is to be generated according to CPSO. A possible candidate particle pi may have the position (0, 0, 0, 0) and velocity (0.5, 0.6, -0.4, 0.2) with fitness value 4, and its individual best position pbesti may be (0, 0, 1, 1) with fitness value 5. The global best position gbest may be (0, 2, 2, 2) with fitness value 6. According to the update (2), if we take  = 0.9 and c1 × r1 = c2 × r2  0.65, in the next iteration, pi 's velocity may be transformed to (0.45, 1.84, 1.59, 2.13). Thus, pi moves to the new position (0, 1, 1, 1) with a limitation on the velocity (the velocity is kept in [- i /2, i /2]), or (0, 1, 1, 2) with no limitation. In both cases, pi only has fitness value 2 after updating. When this occurs, pi evolves to a worse situation, although its position is closer to the pbesti and gbest than its original one. Analyzing the fitness measurement, the main contribution to the fitness value is the combinations that the test case can cover, not the concrete "position" at which it is located. For example, test case (2, 1, 1, 2) has a larger fitness value than (0, 0, 0, 0) because it covers six new schemas [(2, 1, ­, ­), (2, ­, 1, ­), (2, ­, ­, 2) etc.], not because of its relative distance to other particles. B. DPSO To overcome this weakness, the movement of particles should be modified. Inspired by the set-based representation scheme of velocity, DPSO is designed as a version of S-PSO [18] to make the particle learn from the individual and global best more effectively when generating covering arrays. Unlike S-PSO, the element of the velocity set in DPSO is designed for covering array generation, and DPSO does not classify the velocity set into different dimensions to avoid the inconsistency of different dimensions when updating velocities in S-PSO. In DPSO, a particle's position represents a candidate test case, while its velocity is changed to a set of t-way schemas with probabilities. Other than the velocity's representation and the newly defined operators, the evolution procedure of DPSO is the same as CPSO (Algorithm 2). Definition 5 (Velocity): The velocity is a set of pairs (s, p(s)), where s is a possible t-way schema of the covering array and p(s) is a real number between 0 and 1 representing the probability of the selection of schema s to update the current position. In the initialization of the swarm, the particle's position is t possible different schemas are randomly assigned. Then Cn selected randomly and each of them is assigned a random t pairs form the initial velocprobability p(s)  (0, 1). These Cn ity set of this particle; the size of this set changes dynamically during the evaluation. We consider the same example CA(N ; 2, 34 ). In DPSO, when particle pi is initialized, its position xi (k) may be (0, 0, 0, 0) representing a candidate test case as before, and its velocity vi (k) may be such a set {((1, 1, ­, ­), 0.7), ((0, ­, 0, ­), 0.3), ((­, 0, ­, 1), 0.8), ((0, ­, ­, 2), 0.9),

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

581

(a)

(b)

(c)

(d)

(e)

Fig. 1. Example of pi 's velocity updating. (a) 0.9 × vi (k). (b) 2 × r1 × (pbesti - xi (k)). (c) 2 × r2 × (gbest - xi (k)). (d) vi (k + 1). (e) Final vi (k + 1) where pro1 = 0.5.
2 = 6 ((­, 2, 1, ­), 0.5), ((­, ­, 0, 1), 0.2)} which contains C4 pairs. DPSO follows the conventional evolution procedure (Algorithm 2) that uses (2) and (3) to update the velocity and position of a particle. To adapt the new scheme for velocity in Definition 5, the related operators in these equations must be redefined. 1) Coefficient × Velocity: The coefficient is a real number which may be a parameter or a random number. It modifies all the probabilities in the velocity. Definition 6 (Coefficient × Velocity): Let a be a nonnegative real number and v be a velocity, a × v = {(s, p(s) × a)|(s, p(s))  v}. (If p(s) × a > 1, p(s) × a = 1.) For example, Fig. 1(a) shows the result for  × vi (k) where  = 0.9. 2) Position­Position: The difference of two positions gives the direction on which a particle moves. The results of the minus operator is a set of (s, p(s)) pairs, as velocity. Definition 7 (Position­Position): Let x1 and x2 be two positions. Then x1 - x2 = {(s, 0.5)|s is a schema that exists in x1 but not in x2 }. In the newly generated schema, probability p(s) for s is set to 0.5 so that the acceleration constants take similar values in both CPSO and DPSO. As in (2), the result of position­position is multiplied by ci × ri . In CPSO, ci is often set to 2 and ri is a random number between 0 and 1 (recall Section II-C). In DPSO, we want the value of final probability to have a range between 0 and 1 after multiplying by ci . Setting p(s) to 0.5 puts the result in [0, 1] when ci = 2. For example, suppose that xi (k) = (0, 0, 0, 0), pbesti = (0, 0, 1, 1), and gbest = (0, 2, 2, 2) as before. We can get pbesti - xi (k) = {((0, ­, 1, ­, ­), 0.5), ((0, ­, ­, 1), 0.5), ((­, 0, 1, ­), 0.5), ((­, 0, ­, 1), 0.5), ((­, ­, 1, 1), 0.5)}. Fig. 1(b) and (c) shows the results for 2 × r1 × (pbesti - xi (k)) and 2 × r2 × (gbest - xi (k)) respectively. 3) Velocity + Velocity: The addition of velocities gives a particle's movement path. The plus operator results in the union of two velocities. Definition 8 (Velocity + Velocity): Let v1 and v2 be two velocities. Then v1 + v2 = {(s, p(s))| if (s, p1 (s))  v1 and (s, p2 (s))  v2 , p(s) = p1 (s) + p2 (s); if (s, pi (s))  v1 and (s, pi (s))  / v2 or (s, pi (s))  v2 and (s, pi (s))  / v1 , p(s) = pi (s)}. (If p1 (s) + p2 (s) > 1, p1 (s) + p2 (s) = 1.) For example, Fig. 1(d) shows the results for pi 's new velocity vi (k + 1) after updating velocity.

Algorithm 3 Position Updating 1: Input: position xi (k), velocity vi (k + 1), pro2 and pro3 2: Output: new position xi (k + 1) 3: xi (k + 1) = (­, ­, . . . , ­) 4: Sort vi (k + 1) in descending order of p(s) 5: for each pair (si ,p(si )) in vi (k + 1) do 6: Generate a random number   [0, 1] 7: if  < p(si ) then 8: for each fixed level in si do 9: Generate a random number   [0, 1] 10: if  < pro2 and the corresponding factor of has not been fixed in xi (k + 1) then 11: Update xi (k + 1) with 12: end if 13: end for 14: end if 15: end for 16: if xi (k + 1) has unfixed factors then 17: Fill these factors by the same levels of previous position xi (k) 18: end if 19: Generate a random number   [0, 1] 20: if  < pro3 then 21: randomly change the level of one factor of xi (k + 1) 22: end if 23: return xi (k + 1)

In the velocity-updating phase of DPSO (2), we introduce a new parameter pro1 to control the size of the final velocity set. If p(si ) < pro1 , the pair (si , p(si )) is removed from the final velocity. For example, if we set pro1 = 0.5, vi (k + 1) is removed as shown in Fig. 1(e). Here, the velocity has been sorted in descending order of p(s), where if several p(s) have the same value, they are in an arbitrary order. If vi (k + 1) becomes empty, it stays empty until new pairs are added to it. As long as the velocity is empty, the particle's position is not updated and no better solutions can be found from this particle. In Section IV-C1, we discuss how to reinitialize this particle. 4) Position + Velocity: Position plus velocity is the position updating phase. Algorithm 3 gives the pseudo code of this procedure. Here, two new parameters, pro2 and pro3 , numbers in the range [0, 1], are introduced. pro2 is used to determine

582

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

the probability of selecting each fixed level in schema s and pro3 is a mutation probability to make the particle mutate randomly. An example helps to describe this procedure. We already have pi 's current position xi (k) = (0, 0, 0, 0), and its updated velocity vi (k + 1) in descending order of p(s) as shown in Fig. 1(e). We also assume that pro2 and pro3 are both set to 0.5. Each schema si here is selected to update the position with probability p(si ). For the first pair ((0, ­, ­, 2), 1.0), suppose that the random number  satisfies  < 1.0. Then, for each fixed level of this pair, namely level 0 of the first factor and level 2 of the fourth factor, its corresponding factor has not been fixed in xi (k + 1). Suppose that we have the first  < 0.5 but the second  > 0.5, the first factor will be selected to update the position and the second factor will not. So the new position becomes (0, ­, ­, ­). For the second pair, we regenerate the random number  , and compare it with the probability 0.9. If  < 0.9, the second pair is selected. If we generate  < 0.5 in two rounds, the new position becomes (0, 2, 2, ­). Accordingly, if the third pair is selected, and its second factor's level 0 is chosen to update, it does not change position because this factor has been set to a fixed level 2. This procedure is repeated until all factors in the new position xi (k + 1) are set to fixed levels. If all pairs in velocity have been considered, unfixed factors of xi (k + 1) are filled by the same levels of previous position xi (k). For example, after finishing the For loop in line 15, if the fourth factor of xi (k + 1) has not been given any level, the fourth factor of xi (k) is used to update it. Then xi (k + 1) becomes (0, 2, 2, 0). C. Auxiliary Strategies Auxiliary strategies can be introduced to improve the performance of PSO. Two of them are added to DPSO: 1) particle reinitialization to make the search more effective and 2) additional evaluation of gbest to improve the selection of the global best test case. 1) Particle Reinitialization: The PSO algorithm starts with a random distribution of particles, which finally converge. Then the best position that has been found is returned. The swarm may jump out of a local optimum, but this can not be guaranteed because CPSO lacks specific strategies for this. When applying PSO for covering array generation, increasing the number of iterations does not improve the ability to escape a local optimum. Hence, particle reinitialization, a widely used method, is employed to help DPSO to jump out of the local optimum. The main issue with particle reinitialization is when to reinitialize the particle. We may select a threshold, so that the reinitialization is done when the number of iterations exceeds the threshold. In DPSO, a better method can be applied. Using the new representation for velocity, when the current particle pi 's position equals its individual best position pbesti and global best position gbest, the size (norm) of pi 's velocity reduces gradually, because no pairs are generated from (pbesti - xi ) and (gbest - xi ), and the original pairs in velocity are removed gradually under the influence of  × v (reduce the p(s) of original pairs) and parameter pro1 . After a few fluctuations around gbest, the particle may stay at gbest, and

TABLE V T WO D IFFERENT C ONSTRUCTIONS OF CA(N ; 2, 34 )

its size (norm) of velocity is zero. We can use this scenario to trigger reinitialization of the particle. When the reinitialization is done, each dimension of a particle's position is randomly assigned a valid value, and its velocity is regenerated as in the initialization of the swarm. 2) Additional Evaluation of gbest: Current PSO methods to generate covering arrays generally employ the fitness function in Definition 4. This fitness function only focuses on the current candidate, and does not consider the partial test suite TS. Consider the generation of CA(N ; 2, 34 ). Table V shows two different construction processes. Both constructions generate (0, 0, 0, 0) as the first test case. Then they choose different test cases, but each of the first three reaches the largest number of 2 = 6. The difference between newly covered combinations, C4 these two constructions emerges when generating the fourth test case. In Construction 1, because the combinations with the same level between any two factors have all been covered, we cannot find a new test case that can still cover six combinations. However, in Construction 2, such a new test case can be found, (1, 0, 1, 2). Because the minimum size of CA(N ; 2, 34 ) is 9, each test case is required to cover six new combinations. Thus, Construction 1 cannot generate the minimum test suite, but Construction 2 can. In general, there may exist multiple test cases with the same highest fitness value, which make them equally qualified to be gbest in Algorithm 2. Instead of arbitrarily selecting one as gbest, it is better to apply additional distance metric to select one among them. As shown in Table V, if the new test case is similar to the existing tests [as (0, 1, 1, 1) is closer to (0, 0, 0, 0) than (1, 1, 1, 1)], there may be a better chance to find test cases with larger fitness subsequently. In order to measure the "similarity" between a test case t and an existing test suite TS, we use the average Hamming distance. The Hamming distance d12 indicates the number of factors that have different levels between two test cases t1 and t2 . Hence, the similarity between t and TS can be defined by the average Hamming distance H (t, TS) = 1 |TS| dtk .
kTS

(4)

The new gbest selection strategy selects the best candidate gbest based on two metrics. A test case that has the minimum average Hamming distance (i.e., the maximum similarity) from the set of test cases that all have the same highest fitness value is selected as gbest in Algorithm 2. For example, when we generate the second test case for CA(N ; 2, 34 ) in Table V, although (1, 1, 1, 1) and (0, 1, 1, 1) both have the highest fitness value 6, (0, 1, 1, 1) is selected as gbest

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

583

TABLE VI C OMPARING P ERFORMANCE B ETWEEN CPSO AND DPSO

because its average Hamming distance, 3, is smaller than that of (1, 1, 1, 1), 4. We expect that this additional evaluation can enhance the probability of generating a smaller test suite. In addition, because we still want to make the particle follow the conventional search behavior on the individual best direction, this additional distance metric will not be used in updating the pbest of DPSO. V. E VALUATION AND PARAMETER T UNING In this section, we first evaluate the effectiveness of DPSO in some representative cases, and compare the results against CPSO. Then the optimal parameter settings for both CPSO and DPSO are explored. The goal of evaluation and parameter tuning is to make the size of generated covering array as small as possible. Five representative cases of covering arrays, listed below, are selected for our experiments CA1 (N ; 2, 610 ) CA2 (N ; 3, 57 ) CA3 (N ; 4, 39 ) CA4 (N ; 2, 43 53 62 ) VCA(N ; 2, 315 , CA(3, 34 )). We consider four independent parameters, iteration number (iter), population size (size), inertia weight (), and acceleration constant (c), which play similar roles in both CPSO and DPSO, and three new parameters for DPSO, pro1 , pro2 , and pro3 . We carry out a base choice experiment to study the impact of various values of these parameters on CPSO and DPSO's performance and find the recommended settings for them. First, a base setting is chosen as the basic configuration. Then the value of each parameter is changed to create a series of configurations, while leaving the other parameters unchanged. Initially, we set iter = 50, size = 20,  = 0.9, c = 1.3, and pro1 = pro2 = pro3 = 0.5 as a basic configuration based on the previous studies [13], [16] and our empirical experience. To obtain statistically significant results, the generation of each case of covering array is executed 30 times. A. Evaluation of DPSO We compare the performance between CPSO and DPSO with the basic configuration. Five classes of covering arrays are generated by these two algorithms. The sizes obtained and average execution time per test case are shown as CPSO1 and DPSO in Table VI. The best and mean array sizes of DPSO are all better than those of CPSO1 . The new discrete representation scheme and auxiliary strategies improve PSO for covering array generation. DPSO can produce smaller covering arrays than CPSO with the basic configuration. However, DPSO spends more time

during the evolution, because its new operators for updating are more intricate than the conventional ones. DPSO needs to deal with many elements of the velocity set, whereas the conventional scheme only needs simple arithmetic operations. To compare the performance between CPSO and DPSO given the same execution time, for each case we let the execution time per test case for CPSO equal to that for DPSO, so that CPSO can spend more time in searching. We refer to this version of CPSO as CPSO2 . In addition, a t-test between CPSO2 and DPSO is conducted and the corresponding p-value is presented. A p-value smaller than 0.05 indicates that the performance of these two algorithms is statistically different with 95% confidence. From CPSO1 , CPSO2 , and DPSO in Table VI, the performance of CPSO is improved with more search time. DPSO, which must use fewer iterations, still works better than CPSO. The effectiveness of DPSO comes from the essential improvement of its new representation scheme for velocity and the auxiliary strategies. The results of the t-test demonstrate the significance of these differences. Therefore, we can conclude that DPSO performs better than CPSO with fewer iterations for covering array generation. B. Parameter Tuning In the base choice experiment, because the sizes of the various covering arrays differ, the mean size of each case (si ) obtained is normalized using si = si - smin . smax - smin

This normalization enables the graphical representation of different cases on a common scale. Because some parameters may not significantly impact the performance, we use ANOVA (significance level = 0.05) to test whether there exist significant differences among the mean results obtained by different parameter settings. When changing the parameter settings have no significant impact on the generation results, these results will be presented as dotted lines in the corresponding figures. For example, CA4 is presented as a dotted line in Fig. 2(b). It means that the iteration number does not significantly impact the generation of this case of covering array, and so this case will not be further considered when identifying the optimal settings. 1) Iteration Number (iter): Iteration number determines the number of updates. PSO typically requires thousands of iterations to evolve. Nevertheless, only a few iterations are required to generate covering array according to [14]­[16]. So we

584

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

(a)
Fig. 2. Comparing array sizes under different iteration numbers. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 3. Comparing array sizes under different population sizes. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 4.

(b)

Comparing array sizes under different settings of population size and iteration number. (a) CPSO. (b) DPSO.

change its value from 50, the base setting, to 1950 incrementally. Much larger values are not used because the execution time may increase markedly without a commensurate increase in the quality of the results. Fig. 2 shows the results for different choices of iteration numbers, where each line represents a covering array. Performance is improved with increasing iterations for both CPSO and DPSO. A small number of iterations may not be appropriate due to insufficient searching. Because the optimal settings are different among different cases, and several settings can be chosen to generate the minimum covering arrays, it is open to debate which setting is the "best" one. Given time constraints, for a population size of 20, a good setting of iteration number could be approximately 1000 for both CPSO and DPSO. 2) Population Size (Size): Population size determines the initial search space. Generally a small value, e.g., 20, is satisfactory for most cases. A large population size may bring a higher diversity and find a better solution, but it also increases the evolving time. For the same reason as before, we change the value of population size from 10 to 100.

Fig. 3 shows the results for different choices of population size. There is no doubt that the mean array size obtained decreases as population size increases. A large population size can have more chances to generate smaller covering arrays, but its execution time can become prohibitive. In addition, because iteration number and population size together determine the search effort of PSO, we explore the combinations between these two parameters. We let iter × size be a constant 20 000, and generate each case under different settings of these two parameters. Fig. 4 shows the results, where PSO prefers a relatively larger population size. In CPSO, ten particles with 2000 iterations is the worst setting, and most cases produce good results with 60 or 80 particles. In DPSO, the best choice of population size is still 80. In both CPSO and DPSO, the largest population size 100 cannot produce as good results as that of 80 due to having fewer iterations. So achieving smallest covering arrays requires a good balance between these two parameters, and a moderately large population size is necessary for both CPSO and DPSO. Thus, we can set iteration number to 250, and population size to 80, as the recommended settings for both CPSO and DPSO.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

585

(a)
Fig. 5. Comparing array sizes under different inertia weights. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 6. Comparing array sizes under different acceleration constants. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 7.

(b)

(c)

Comparing array sizes under different settings of (a) pro1 , (b) pro2 , and (c) pro3 . TABLE VII R ECOMMENDED PARAMETER S ETTINGS

3) Inertia Weight (): Inertia weight determines the tendency of the particle to continue in the same direction. A small value help the particle move primarily toward the best position, while a large one is helpful to continue its previous movement. A linearly decreasing value is also used as it can make the swarm gradually narrow the search space. Here, we investigate both fixed values and a linearly decreasing value from 0.9 to 0.4 over the whole evolution (presented as "dec"). Fig. 5 shows the results for different choices of inertia weight. In CPSO, most of the smallest covering arrays are generated by large fixed inertia weights. The decreasing value does not perform as well as the large values, such as 0.9 for CA1 , CA2 , and CA4 . So 0.9 can be the recommended choice for CPSO. In DPSO, 1.1 is the worst choice because the particle may update its position based on its own velocity and thus fail to learn from individual and global best positions. The decreasing value can perform reasonably well, but a fixed value 0.5 may be a better choice in that it keeps the effort of global search moderate. Thus, we can recommend the fixed inertia weight of 0.9 for CPSO, and 0.5 for DPSO. 4) Acceleration Constant (c): Acceleration Constants c1 and c2 control the learning from pbest and gbest, respectively. Generally, they are set to the same value to balance

the influence of these two positions. Setting c1 and c2 to a large value may make the particle more likely attracted to the best position ever found, while a small one may make the search far from the current optimal region. In this paper, we set c1 = c2 = c, and vary c from 0.1 to 2.9. Fig. 6 shows the results for different choices of acceleration constant. Unlike other parameters, there is a consistent trend in all five cases. In CPSO, the values larger than 0.5 all produce good results. In DPSO, 1.3 can definitely be regarded as the optimal value. Thus, we set 1.3 as the recommended value of acceleration constant for both CPSO and DPSO. 5) pro1 , pro2 , and pro3 : These three parameters are new to DPSO. Parameter pro1 determines the size of the final velocity. As those pairs whose probabilities are smaller than pro1 are removed from the final velocity set, a small value may keep more pairs in the velocity for the next evolution, but it

586

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE VIII S IZES (N ) OF C OVERING A RRAYS FOR n FACTORS , E ACH H AVING T HREE L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, n, 3)

also requires more computation time. Parameter pro2 determines the probability of selecting each fixed level from each schema when updating positions. A larger value may lead to a quick construction of new position, but it may also lead to fast convergence toward a local optimum. Parameter pro3 determines the mutation probability when updating positions. A larger value may enhance the randomness, but it also lowers the convergence speed. In this paper, values from 0.1 to 1.0 for these three parameters are investigated. Fig. 7 shows the results. For pro1 , a large value is not appropriate because it removes nearly all pairs from the final velocity set. A medium value 0.5, which appears to lead to the best result, may be the best choice. For pro2 , the smallest value 0.1 yields the best results. Slower construction of the new position may slow the convergence toward a local optimum, but it also takes longer. So, in order to balance performance and execution time, we take 0.3 as the recommended value for pro2 . For pro3 , a larger value may be a good choice. The frequent mutation of new position may bring better results, but also takes longer to converge. So, we take 0.7 as the recommended value for pro3 . In summary, the recommended parameter settings for PSO for covering array generation are different from previously suggested ones [13], [16]. Some parameters may significantly impact the performance in some cases, and parameter tuning is necessary to enhance heuristic techniques for particular applications. Naturally, the optimal settings vary for different cases of covering arrays. There may not exist a common setting that can always lead to the best results. Because it is

impractical to tune parameters for each case in real life, we suggest two general settings for CPSO and DPSO, as shown in Table VII, which can typically lead to better performance within reasonable execution time. VI. C OMPARING A MONG PSO S In this section, we compare the best reported array sizes generated by PSO in [16] with our findings for CPSO, DPSO, and four representative variants. Because the research in [16] demonstrated that their generation results typically outperform greedy algorithms, in this paper, we do not compare CPSO and DPSO with greedy algorithms. We implement both the original and discrete versions of four variants (TVAC [24], CLPSO [25], APSO [26], and DMS-PSO [27]) to generate covering arrays. Their discrete versions are extended based on new representation scheme of velocity and auxiliary strategies. We name them D-TVAC, DCLPSO, D-APSO, and D-DMS-PSO, respectively. In CLPSO, a particle can learn from different particles' pbest in each dimension whereas we do not distinguish dimensions strictly in DPSO. So in D-CLPSO, a particle can fully learn from different particles' pbest in all dimensions. That may weaken the search ability of CLPSO. For the other three variants, they can be directly extended based on DPSO. All algorithms are compared using the same number of fitness function evaluations. CPSO and DPSO use the recommended settings shown in Table VII. For the variants, iter, size and pro1 , pro2 and pro3 for discrete versions are set to

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

587

TABLE IX S IZES (N ) OF C OVERING A RRAYS FOR S EVEN FACTORS , E ACH H AVING

L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, 7, )

their recommended values.  and c are also set to recommended values unless they are adaptively adjusted during the evolution, in which case their range is set to [0.4, 0.9] and [0.8, 1.8], respectively. The new control parameters for these variants follow their suggested settings. Tables VIII­XIII give the results. Because of the execution time, we only consider covering strengths from 2 to 4, and the generation of each covering array is repeated 30 times. A t-test (significance level = 0.05) is also conducted to test whether there exists a significant difference between the mean sizes produced by the two algorithms. In the first three columns, we report the best and mean array sizes obtained from previous results, CPSO and DPSO, where boldface numbers indicate that the difference between CPSO and DPSO is significant based on the t-test. In the last four columns, we report the mean array sizes from the original and discrete versions of each PSO variant (presented as meanc and meand respectively), where boldface numbers indicate that the difference between meanc and meand of each variant is significant. A. Uniform Covering Arrays Tables VIII­X present the results for uniform covering arrays. We extend the cases considered in [16], where "­" indicates the not available cases. In Tables VIII, we report array sizes for n factors, each having three levels. In Tables IX and X, we report array sizes for 7 and 10 factors, each having levels. Their covering strengths all range from 2 to 4. Typically, CPSO can produce smaller sizes than those reported in [16], demonstrating the effectiveness of parameter tuning. Furthermore, DPSO can produce the smallest best and mean sizes in almost all cases, and usually the performance of DPSO is significantly better than CPSO. When the covering strength t = 2 is adopted (Table VIII), DPSO does

not generally outperform CPSO. But when covering strength increases, DPSO performs better. Sometimes the mean sizes for DPSO are smaller than the best sizes for CPSO. Because generating a covering array with higher covering strength is more complicated, DPSO may be more appropriate for generating such covering arrays. In Tables IX and X, we also find that DPSO is superior for covering arrays with high covering strength. Overall, these results show the effectiveness of DPSO for uniform covering array generation. Surprisingly, DPSO does not beat previous results for CPSO when covering arrays have two levels for each factor (Table IX). This appears to be a weakness of DPSO. Although DPSO generates smaller covering arrays than previous results and CPSO, other techniques may still yield better results than DPSO (some best known sizes can be found in [38]). B. VS Covering Array Tables XI­XIII give the results for VS covering arrays. Based on CA(N ; 2, 315 ), CA(N ; 3, 315 ), and CA(N ; 2, 42 52 63 ), some different cases of sub covering arrays conducted in [16] are examined. Their covering strengths are at most 4. Generally, we can draw similar conclusions as for uniform covering arrays. CPSO with the suggested parameter setting can produce better results than reported sizes in some cases. DPSO also usually beats them on the best and mean sizes. In Table XI, often the difference between CPSO and DPSO is not significant. In part this is because for the CA(3, 33 ), CA(3, 33 )2 , CA(3, 33 )3 , and CA(4, 34 ), the same best results are provably the minimum (e.g., the minimum size of the CA(3, 33 ) is 3 × 3 × 3 = 27). For the other cases, although sometimes the difference is not significant, DPSO can still generate smaller covering arrays. DPSO remains a good choice for generating VS covering arrays. In Tables XII and XIII, similar results can be found.

588

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE X S IZES (N ) OF C OVERING A RRAYS FOR T EN FACTORS , E ACH H AVING

L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, 10, )

TABLE XI S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 2, 315 , CA)

TABLE XII S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 3, 315 , CA)

For both uniform and variable cases of covering arrays, parameter tuning can enhance CPSO to generate smaller covering arrays than previous work. DPSO can produce the smallest covering arrays in nearly all cases, and in general the difference of performance between CPSO and DPSO is significant. DPSO is an effective discrete version of PSO for covering array generation. C. PSO Variants In order to further investigate the effectiveness of DPSO for covering array generation, we implement the original versions of four representative variants of PSO and extend them to their discrete versions based on DPSO. We apply them to generate

the same cases for covering arrays. The mean sizes obtained are shown in the last four columns in Tables VIII­XIII. We first compare the mean sizes of original PSO variants with those of CPSO and DPSO. In our experiments, TVAC's mean sizes are always larger than CPSO's. The linear adjustment of inertia weight and acceleration constant is not helpful for CPSO for covering array generation. Typically, APSO's mean sizes are also larger than CPSO's. Because APSO uses a fuzzy system to classify different evolutionary states, its ineffectiveness may result from inappropriate parameter settings. CLPSO and DMS-PSO can outperform CPSO typically. Although on occasion they achieve comparable performance with DPSO, they cannot perform as well as DPSO in most

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

589

TABLE XIII S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 2, 43 53 62 , CA)

TABLE XIV C OMPARING CPSO AND DPSO W ITH GA AND ACO

cases. That further demonstrates the effectiveness of DPSO for covering array generation. Because these four algorithms are representative PSO variants (as shown in Table IV), designing different neighborhood topologies and using multiswarm techniques may have potential to improve CPSO for covering array generation. We next compare the original and discrete versions of each variant. Except for CLPSO, the other three variants can be improved using their discrete versions, and the improvement is also significant. TVAC cannot outperform CPSO, but it is enhanced by DPSO so that D-TVAC can produce smaller mean sizes than CPSO in most cases. The linear adjustment is helpful for the discrete version. For CLPSO, only in a few cases is it improved by DPSO. Sometimes D-CLPSO even leads to worse results (see Table X), due primarily to the weakened search ability of its discrete version as explained in Section VI. For APSO, DPSO can enhance its original version, but D-APSO is still worse than DPSO. That may result from inappropriate settings as explained before. For DMS-PSO, sometimes DPSO does not enhance it (see Table XI). However, in most cases D-DMS-PSO can outperform DMS-PSO and has comparable performance with D-TVAC. The multiswarm strategy is also helpful for the discrete version.

In summary, the comparison study reveals that our suggested parameter settings are more suitable for covering array generation. DPSO is an effective discrete version of PSO. It can significantly outperform previous results and CPSO in nearly all cases for uniform and VS covering arrays. Furthermore, DPSO's representation scheme of a particle's velocity and auxiliary strategies not only enhance CPSO, but also typically enhance PSO variants. DPSO is a promising improvement on PSO for covering array generation. VII. C OMPARING DPSO W ITH GA AND ACO Because GAs and ACO [8]­[12] have also been successfully used for covering array generation, we compare CPSO and DPSO with the reported array sizes in [9] and [11]. There are no widely accepted benchmarks for the comparison of searchbased covering array generation, so we only consider these two representative and competitive works. Shiba et al. [9] applied both GA and ACO to generate uniform covering arrays (CA1 to CA8 in Table XIV), and Chen et al. [11] applied ACO to generate VS covering arrays (VCA9 to VCA12 in Table XIV). They both set algorithm parameters according to recommendations in related research fields without parameter tuning, while our CPSO and DPSO

590

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

use the recommended settings shown in Table VII. To compare these algorithms under the same number of fitness evaluations, for each case of covering array, the population size and the number of iterations of CPSO and DPSO are modified accordingly to satisfy the settings in [9] and [11]. Moreover, Shiba et al. [9] and Chen et al. [11] both applied test minimization algorithms to further reduce the size of generated covering arrays, while our CPSO and DPSO do not apply any minimization algorithms. Table XIV shows the comparison results, where boldface numbers indicate the best array sizes obtained, and "­" represents that the corresponding data is not available. The generation of each case of covering array of CPSO and DPSO is executed 30 times and the best and average results are presented. Because we do not implement GA and ACO for covering array generation, no statistical tests can be conducted here. In addition, because the platforms used for collecting the results differ, the comparison of computational time would not be informative. We nevertheless present the execution times of our CPSO and DPSO, which can serve as references for practitioners. From Table XIV, DPSO can outperform existing GA and ACO for covering array generation, despite the latter two applying test minimization algorithms. Because our DPSO is a version of PSO designed and tuned for covering array generation, it suggests that tuned versions of GAs and ACOs may be a promising area for further study. In addition, CPSO performs worse than GA and ACO in seven of 12 cases. That may result from the improvement by minimization algorithms in [9] and [11]. But for CA2 , CA3 , and CA6 , CPSO can still achieve smaller covering arrays. In summary, the results further demonstrate that DPSO is an effective discrete version of PSO for covering array generation. Further investigations of tuned versions of GA and ACO should be considered. VIII. C ONCLUSION Covering array generation is a key issue in CT. We developed a new DPSO for covering array generation, by adapting S-PSO to generate covering arrays and incorporating two auxiliary strategies to improve the performance. Parameter tuning was applied to both CPSO and DPSO to identify their best parameter settings. The original and discrete versions of four representative PSO variants were implemented and their efficacy for covering array generation was compared. The performance of DPSO was also compared with other existing evolutionary algorithms, GA and ACO. DPSO can perform better than CPSO with fewer iterations, and the performance of CPSO and DPSO is significantly impacted by their parameter settings. Different cases require different parameter settings; there may not exist a single choice that leads to the best results. After parameter tuning, we identified two recommended settings, which lead to relatively good performance for CPSO and DPSO. Indeed, CPSO with our recommended parameter settings improves on previously reported results, and our DPSO with recommended parameter settings usually beats CPSO. Typically, DPSO also significantly

enhances the performance of PSO variants. In addition, DPSO often outperforms GA and ACO to generate covering arrays. Consequently, DPSO is a promising improvement of PSO for covering array generation. Improvements on the methods here may be possible in a number of ways. One would be to investigate further evolution procedures and strategies proposed in PSO, such as hybridizing with penalty approaches to handle discrete unknowns [39], and compare the results with some exact schemes like branch and bound method. A second would be to examine one-column-at-a-time approaches or methods that construct the entire array, rather than the one-row-at-a-time approach adopted here. A third would be to incorporate DPSO with other methods, in particular with test minimization methods. R EFERENCES
[1] C. Nie and H. Leung, "A survey of combinatorial testing," ACM Comput. Surv., vol. 43, no. 2, pp. 11.1­11.29, 2011. [2] D. Kuhn and M. Reilly, "An investigation of the applicability of design of experiments to software testing," in Proc. 27th Annu. NASA Goddard/IEEE Softw. Eng. Workshop, Greenbelt, MD, USA, 2002, pp. 91­95. [3] M. Cohen, P. Gibbons, W. Mugridge, and C. Colbourn, "Constructing test suites for interaction testing," in Proc. 25th Int. Conf. Softw. Eng., Portland, OR, USA, 2003, pp. 38­48. [4] M. B. Cohen, C. J. Colbourn, and A. C. Ling, "Constructing strength three covering arrays with augmented annealing," Discrete Math., vol. 308, no. 13, pp. 2709­2722, 2008. [5] J. Torres-Jimenez and E. Rodriguez-Tello, "Simulated annealing for constructing binary covering arrays of variable strength," in Proc. Congr. Evol. Comput., Barcelona, Spain, Jul. 2010, pp. 1­8. [6] B. Garvin, M. Cohen, and M. Dwyer, "Evaluating improvements to a meta-heuristic search for constrained interaction testing," Empir. Softw. Eng., vol. 16, no. 1, pp. 61­102, 2011. [7] J. Torres-Jimenez and E. Rodriguez-Tello, "New bounds for binary covering arrays using simulated annealing," Inf. Sci., vol. 185, no. 1, pp. 137­152, 2012. [8] S. Ghazi and M. Ahmed, "Pair-wise test coverage using genetic algorithms," in Proc. Congr. Evol. Comput., vol. 2. Canberra, ACT, Australia, 2003, pp. 1420­1424. [9] T. Shiba, T. Tsuchiya, and T. Kikuno, "Using artificial life techniques to generate test cases for combinatorial testing," in Proc. 28th Annu. Int. Comput. Softw. Appl. Conf., vol. 1. Hong Kong, 2004, pp. 72­77. [10] J. McCaffrey, "An empirical study of pairwise test set generation using a genetic algorithm," in Proc. 7th Int. Conf. Inf. Technol. New Gener., Las Vegas, NV, USA, 2010, pp. 992­997. [11] X. Chen, Q. Gu, A. Li, and D. Chen, "Variable strength interaction testing with an ant colony system approach," in Proc. Asia-Pacific Softw. Eng. Conf., Penang, Malaysia, 2009, pp. 160­167. [12] X. Chen, Q. Gu, X. Zhang, and D. Chen, "Building prioritized pairwise interaction test suites with ant colony optimization," in Proc. 9th Int. Conf. Qual. Softw., Jeju-do, Korea, 2009, pp. 347­352. [13] X. Chen, Q. Gu, J. Qi, and D. Chen, "Applying particle swarm optimization to pairwise testing," in Proc. 34th Annu. Comput. Softw. Appl. Conf., Seoul, Korea, 2010, pp. 107­116. [14] B. S. Ahmed and K. Z. Zamli, "PSTG: A T-way strategy adopting particle swarm optimization," in Proc. 4th Asia Int. Conf. Math. Anal. Model. Comput. Simulat., Kota Kinabalu, Malaysia, 2010, pp. 1­5. [15] B. S. Ahmed and K. Z. Zamli, "A variable strength interaction test suites generation strategy using particle swarm optimization," J. Syst. Softw., vol. 84, no. 12, pp. 2171­2185, 2011. [16] B. S. Ahmed, K. Z. Zamli, and C. P. Lim, "Application of particle swarm optimization to uniform and variable strength covering array construction," Appl. Soft Comput., vol. 12, no. 4, pp. 1330­1347, 2012. [17] Y. del Valle, G. Venayagamoorthy, S. Mohagheghi, J.-C. Hernandez, and R. Harley, "Particle swarm optimization: Basic concepts, variants and applications in power systems," IEEE Trans. Evol. Comput., vol. 12, no. 2, pp. 171­195, Apr. 2008.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

591

[18] W.-N. Chen et al., "A novel set-based particle swarm optimization method for discrete optimization problems," IEEE Trans. Evol. Comput., vol. 14, no. 2, pp. 278­300, Apr. 2010. [19] M. Clerc, Discrete Particle Swarm Optimization (New Optimization Techniques in Engineering). New York, NY, USA: Springer, 2004. [20] W. Pang et al., "Modified particle swarm optimization based on space transformation for solving traveling salesman problem," in Proc. Int. Conf. Mach. Learn. Cybern., vol. 4. Shanghai, China, Aug. 2004, pp. 2342­2346. [21] W. Pang, K.-P. Wang, C.-G. Zhou, and L.-J. Dong, "Fuzzy discrete particle swarm optimization for solving traveling salesman problem," in Proc. 4th Int. Conf. Comput. Inf. Technol., Wuhan, China, 2004, pp. 796­800. [22] Y. Wang et al., "A novel quantum swarm evolutionary algorithm and its applications," Neurocomputing, vol. 70, nos. 4­6, pp. 633­640, 2007. [23] E. Campana, G. Fasano, and A. Pinto, "Dynamic analysis for the selection of parameters and initial population, in particle swarm optimization," J. Global Optim., vol. 48, no. 3, pp. 347­397, 2010. [24] A. Ratnaweera, S. Halgamuge, and H. Watson, "Self-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients," IEEE Trans. Evol. Comput., vol. 8, no. 3, pp. 240­255, Jun. 2004. [25] J. Liang, A. Qin, P. Suganthan, and S. Baskar, "Comprehensive learning particle swarm optimizer for global optimization of multimodal functions," IEEE Trans. Evol. Comput., vol. 10, no. 3, pp. 281­295, Jun. 2006. [26] Z.-H. Zhan, J. Zhang, Y. Li, and H.-H. Chung, "Adaptive particle swarm optimization," IEEE Trans. Syst., Man, Cybern., B, Cybern., vol. 39, no. 6, pp. 1362­1381, Dec. 2009. [27] J. Liang and P. Suganthan, "Dynamic multi-swarm particle swarm optimizer," in Proc. IEEE Swarm Intell. Symp., Pasadena, CA, USA, 2005, pp. 124­129. [28] D. Cohen, S. Dalal, M. Fredman, and G. Patton, "The AETG system: An approach to testing based on combinatorial design," IEEE Trans. Softw. Eng., vol. 23, no. 7, pp. 437­444, Jul. 1997. [29] R. C. Bryce and C. J. Colbourn, "One-test-at-a-time heuristic search for interaction test suites," in Proc. 9th Annu. Conf. Genet. Evol. Comput., London, U.K., 2007, pp. 1082­1089. [30] J. Kennedy and R. Eberhart, "Particle swarm optimization," in Proc. Int. Conf. Neural Netw., vol. 4. Perth, WA, Australia, 1995, pp. 1942­1948. [31] R. Eberhart and Y. Shi, "Particle swarm optimization: Developments, applications and resources," in Proc. Congr. Evol. Comput., vol. 1. Seoul, Korea, 2001, pp. 81­86. [32] R. Poli, J. Kennedy, and T. Blackwell, "Particle swarm optimization," Swarm Intell., vol. 1, no. 1, pp. 33­57, 2007. [33] S. Helwig, J. Branke, and S. Mostaghim, "Experimental analysis of bound handling techniques in particle swarm optimization," IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 259­271, Apr. 2013. [34] A. Windisch, S. Wappler, and J. Wegener, "Applying particle swarm optimization to software testing," in Proc. 9th Annu. Conf. Genet. Evol. Comput., London, U.K., 2007, pp. 1121­1128. [35] A. Ganjali, "A requirements-based partition testing framework using particle swarm optimization technique," M.S. thesis, Dept. Electr. Comput. Eng., Univ. Waterloo, Waterloo, ON, Canada, 2008. [36] Y.-J. Gong et al., "Optimizing the vehicle routing problem with time windows: A discrete particle swarm optimization approach," IEEE Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 42, no. 2, pp. 254­267, Mar. 2012. [37] W.-N. Chen et al., "Particle swarm optimization with an aging leader and challengers," IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 241­258, Apr. 2013. [38] C. J. Colbourn. (Apr. 2013). Covering Array Tables for t=2, 3, 4, 5, 6. [Online]. Available: http://www.public.asu.edu/ ccolbou/src/ tabby/catable.html [39] M. Corazza, G. Fasano, and R. Gusso, "Particle swarm optimization with non-smooth penalty reformulation, for a complex portfolio selection problem," Appl. Math. Comput., vol. 224, pp. 611­624, Nov. 2013.

Huayao Wu received the B.S degree from Southeast University, Nanjing, China and the M.S degree from Nanjing University, Nanjing, China, where he is currently working toward the Ph.D. degree from Nanjing University, Nanjing. His research interests include software testing, especially on combinatorial testing and search-based software testing.

Changhai Nie (M'12) received the B.S. and M.S. degrees in mathematics from Harbin Institute of Technology, Harbin, China, and the Ph.D. degree in computer science from Southeast University, Nanjing, China. He is a Professor of Software Engineering with State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing. His research interests include software analysis, testing and debugging.

Fei-Ching Kuo (M'06) received the B.Sc. (Hons.) degree in computer science and the Ph.D. degree in software engineering from Swinburne University of Technology, Hawthorn, VIC, Australia. She was a Lecturer with University of Wollongong, Wollongong, NSW, Australia. She is currently a Senior Lecturer with the Swinburne University of Technology. Her research interests include software analysis, testing, and debugging.

Hareton Leung (M'90) received the Ph.D. degree in computer science from University of Alberta, Edmonton, AB, Canada. He is an Associate Professor and a Director of the Laboratory for Software Development and Management, Department of Computing, Hong Kong Polytechnic University, Hong Kong. His research interests include software testing, project management, risk management, quality and process improvement, and software metrics.

Charles J. Colbourn received the Ph.D. degree from University of Toronto, Toronto, ON, Canada, in 1980. He is a Professor of Computer Science and Engineering with Arizona State University, Tempe, AZ, USA. He has authored the books The Combinatorics of Network Reliability (Oxford) and Triple Systems (Oxford), and also 320 refereed journal papers focussing on combinatorial designs and graphs with applications in networking, computing, and communications. Prof. Colbourn received the Euler Medal for Lifetime Research Achievement by the Institute for Combinatorics and its Applications in 2004.

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

575

A Discrete Particle Swarm Optimization for Covering Array Generation
Huayao Wu, Changhai Nie, Member, IEEE, Fei-Ching Kuo, Member, IEEE, Hareton Leung, Member, IEEE, and Charles J. Colbourn

Abstract--Software behavior depends on many factors. Combinatorial testing (CT) aims to generate small sets of test cases to uncover defects caused by those factors and their interactions. Covering array generation, a discrete optimization problem, is the most popular research area in the field of CT. Particle swarm optimization (PSO), an evolutionary search-based heuristic technique, has succeeded in generating covering arrays that are competitive in size. However, current PSO methods for covering array generation simply round the particle's position to an integer to handle the discrete search space. Moreover, no guidelines are available to effectively set PSOs parameters for this problem. In this paper, we extend the set-based PSO, an existing discrete PSO (DPSO) method, to covering array generation. Two auxiliary strategies (particle reinitialization and additional evaluation of gbest) are proposed to improve performance, and thus a novel DPSO for covering array generation is developed. Guidelines for parameter settings both for conventional PSO (CPSO) and for DPSO are developed systematically here. Discrete extensions of four existing PSO variants are developed, in order to further investigate the effectiveness of DPSO for covering array generation. Experiments show that CPSO can produce better results using the guidelines for parameter settings, and that DPSO can generate smaller covering arrays than CPSO and other existing evolutionary algorithms. DPSO is a promising improvement on PSO for covering array generation. Index Terms--Combinatorial testing (CT), covering array generation, particle swarm optimization (PSO).

I. I NTRODUCTION S SOFTWARE functions and run-time environments become more complex, testing of modern software systems is becoming more expensive. Effective detection of

A

Manuscript received May 12, 2013; revised December 29, 2013 and May 18, 2014; accepted September 28, 2014. Date of publication October 9, 2014; date of current version July 28, 2015. This work was supported in part by the National Natural Science Foundation of China under Grant 61272079, in part by the Research Fund for the Doctoral Program of Higher Education of China under Grant 20130091110032, in part by the Science Fund for Creative Research Groups of the National Natural Science Foundation of China under Grant 61321491, in part by the Major Program of National Natural Science Foundation of China under Grant 91318301, and in part by the Australian Research Council Linkage under Grant LP100200208. (Corresponding author: Changhai Nie.) H. Wu and C. Nie are with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China (e-mail: hywu@outlook.com; changhainie@nju.edu.cn). F.-C. Kuo is with the Faculty of Information and Communication Technologies, Swinburne University of Technology, Hawthorn, VIC 3122, Australia (e-mail: dkuo@swin.edu.au). H. Leung is with the Department of Computing, Hong Kong Polytechnic University, Hong Kong (e-mail: hareton.leung@polyu.edu.hk). C. J. Colbourn is with Arizona State University, Tempe, AZ 85287-8809, USA (e-mail: colbourn@asu.edu). Digital Object Identifier 10.1109/TEVC.2014.2362532

failures at a low cost is a key issue for test case generation. Combinatorial testing (CT) is a popular testing method to detect failures triggered by various factors and their interactions [1]. By employing covering arrays as test suites, the CT method aims to sample the large combination space with few test cases to cover different interactions among a fixed number of factors. Kuhn and Reilly [2] shows that more than 70% of the failures in certain software were caused by the interactions of one or two factors, and almost all the failures could be detected by checking the interactions among six factors. Therefore, CT can be an effective method in practice. Generating a covering array with fewest tests (minimum size) is a major challenge in CT. In general, the minimum size of a covering array is unknown; hence, methods have focused on finding covering arrays that have as few tests as possible at reasonable search cost. The many methods that have been proposed can be classified into two main groups: 1) mathematical methods and 2) computational methods [1]. Mathematical (algebraic or combinatorial) methods typically exploit some known combinatorial structure. Computational methods primarily use greedy strategies or heuristic-search techniques to generate covering arrays, due to the size of the search space. Mathematical methods yield the best possible covering arrays in certain cases. For example, orthogonal arrays used in the design of experiments provide covering arrays with a number of tests that is provably minimum. However, all known mathematical methods can be applied only for restrictive sets of factors. This limitation has led to an emphasis on computational methods. Greedy algorithms have been quite effective in generating covering arrays, but their accuracy suffers from becoming trapped in local optima. In recent years, search-based software engineering (SBSE) has focused on using search-based optimization algorithms to find high-quality solutions for software engineering problems. Inspired by SBSE, many artificial intelligence-based heuristic-search techniques have been applied to software testing. For example, simulated annealing (SA) [3]­[7], genetic algorithm (GA) [8]­[10], and ant colony optimization (ACO) [9], [11], [12] have all been applied to covering array generation. These techniques can generate any types of covering arrays, and the constraint solving and prioritization techniques can be easily integrated. Their applications have been shown to be effective, producing relatively small covering arrays in many cases. Particle swarm optimization (PSO), a relatively new evolutionary algorithm,

1089-778X c 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

576

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

has also been used in this area [13]­[16]. It is easy to implement and has fast initial progress. The conventional PSO (CPSO) algorithm was originally designed to find optimal or near optimal solutions in a continuous space. Nevertheless, many discrete PSO (DPSO) algorithms and frameworks have been developed to solve discrete problems [17]­[22]. For covering array generation, current discrete methods [13]­[16] simply round the particle's position to an integer while keeping the velocity as a real number. They suffer from two main shortcomings. First, the performance of PSO is significantly impacted by its parameter settings. In [23], effects of the general parameter selection and initial population of PSO have been analyzed, but no guidelines on parameter settings have been reported for covering array generation. Hence, a clear understanding of how to set these execution parameters is needed. Second, simple rounding fractional positions to integers introduces a substantial source of errors in the search. Instead, a specialized DPSO version is needed. Because current PSO methods show promise for generating covering arrays, these two main shortcomings should be addressed. In this paper, we adapt set-based PSO (S-PSO) [18] to generate covering arrays. S-PSO utilizes set and probability theories to develop a new representation scheme for combinatorial optimization problems as well as refining the related evolution operators. In our adaptation of S-PSO, two auxiliary strategies are proposed to enhance performance, and a novel DPSO algorithm is thus proposed. DPSO has the same conceptual basis and exhibits similar search behavior to CPSO, with parameters playing similar roles. Then, we explore the optimal parameter settings for both CPSO and DPSO to improve their performance, and identify recommended settings. Furthermore, because many CPSO variants [24]­[27] can be easily extended to discrete versions based on our DPSO, the performance of these discrete versions is also compared with their original ones. Finally, we compare CPSO and DPSO with existing GA and ACO [9], [11] algorithms to generate covering arrays. The main contributions of this paper are as follows. 1) Based on the set-based representation, we design a version of S-PSO [18] for covering array generation. 2) We propose two auxiliary strategies, particle reinitialization, and additional evaluation of gbest, to enhance the performance of PSO. A novel DPSO for covering array generation is proposed. 3) We design experiments to explore the optimal parameter settings for CPSO and DPSO for covering array generation. 4) We implement original and discrete versions of four representative PSO variants (TVAC [24], CLPSO [25], APSO [26], and DMS-PSO [27]) to compare their efficacy for covering array generation. The rest of this paper is organized as follows. Section II gives background on CT, covering array generation, and the CPSO algorithm. Section III summarizes related work. Section IV presents our DPSO algorithm, including the representation scheme, related operators, and two auxiliary strategies. Section V evaluates the performance of CPSO and

TABLE I E-C OMMERCE S OFTWARE S YSTEM

DPSO, and explores optimal parameter settings. Section VI gives a comparison among CPSO, DPSO, and original and discrete variants. Section VII compares CPSO and DPSO with GA and ACO. Section VIII concludes this paper and outlines future work. II. BACKGROUND A. CT Suppose that the behavior of the software under test (SUT) is controlled by n independent factors, which may represent configuration parameters, internal or external events, user inputs, and the like. The ith factor has i discrete levels, chosen from the finite set Vi of size i . An n-tuple (x1 , x2 , . . . , xn ) forms a test case, where xi  Vi for 1  i  n. Consider a simple e-commerce software system [15]. This system consists of five different components. Each of these five components can be regarded as a factor, and its configurations can be regarded as different levels. Table I shows these five factors and their corresponding levels. In this example, n = 5, 1 = 2 = 3 = 2, 4 = 5 = 3. System failures are often triggered by interactions among some factors, which can be represented by the combinations of factor levels. In order to detect these failures, their combinations should be covered at least once by the test suite. A t-way schema can be used to represent them. Definition 1 (t-way schema): The n-tuple (-, y1 , . . . , yt , . . . ) is a t-way schema (t > 0) when some t factors have fixed levels and the others can take any valid levels, represented as "-." For example, suppose that when factor Payment Server takes the level Master and factor Web Server takes the level Apache, a system failure occurs. To detect this failure, the 2-way schema (Master, ­, Apache, ­, ­) must be covered at least once by the test suite. To simplify later discussion, we use the index in the level set of each factor to present a schema. For example, (0, ­, 1, ­, ­) is used to represent (Master, ­, Apache, ­, ­). Exhaustive testing covers all n-way schemas of the system; in our example, it uses 2 × 2 × 2 × 3 × 3 = 72 test cases. Testing cost becomes prohibitive as the numbers of factors and levels increase. Moreover, because only interactions among few factors are likely to trigger failures [2], testing high-way schemas can lead to many uninformative test cases. At the other extreme, if we only guarantee to cover each 1-way schema once, only three test cases are needed (a single test case can cover five 1-way schemas at most). But we may fail to detect some interaction triggered failures involving two factors.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

577

TABLE II C OVERING A RRAY CA(9; 2, 23 32 )

TABLE III A DDING T HREE T EST C ASES TO C ONSTRUCT VCA(12; 2, 23 32 , CA(3, 22 31 ))

Instead, CT covers all t-way schemas. Such a test suite is a t-way covering array, with t being the covering strength. The value of t determines the depth of coverage. It is a key setting of CT, and should be decided by the testers. We give a precise definition. Definition 2 (Covering Array): If an N ×n array, where N is the number of test cases, has the following properties: 1) each column i (1  i  n) contains only elements from the set Vi with i = |Vi | and 2) the rows of each N × t sub array cover all |Vk1 | × |Vk2 | × . . . × |Vkt | combinations of the t columns at least once, where t  n and 1  k1 < . . . < kt  n, then it is a t-way covering array, denoted by CA(N ; t, n, ( 1 , 2 , . . . , n )). When 1 = 2 = . . . = n = , it is denoted by CA(N ; t, n, ). Reference [2] demonstrated that more than 70% failures can be detected by a 2-way covering array, and almost all failures can be detected by a 6-way covering array. Hence, using CT, we can detect many failures of the system by applying relatively low strength covering arrays. In other words, CT can greatly reduce the size of test suite while maintaining high fault detection ability. In the example of Table I, if we only consider the interactions between any two factors, only nine test cases are required to construct a 2-way covering array instead of 72 for exhaustive testing. Table II shows a covering array, where each row represents one test case. This table covers all possible 2-way schemas for any two factors corresponding to the columns. For example, consider factor Payment Server and User Browser, all 2 × 3 = 6 schemas, (Master, ­, ­, Firefox, ­) (Master, ­, ­, Explorer, ­), (Master, ­, ­, Chrome, ­), (VISA, ­, ­, Firefox, ­), (VISA, ­, ­, Explorer, ­), (VISA, ­, ­, Chrome, ­), can be found in the table. For convenience, if several groups of gi factors (gi < n) have g the same number of levels ak , ak i can be used to represent these factors and their levels. Thus, the covering array can g g g gi = n, be denoted by CA(N ; t, a11 , a22 , . . . , ak k ) where n or CA(N ; t, a ) when g1 = n and a1 = a. For example, the covering array in Table II is a CA(9; 2, 23 32 ). In many software systems, the impacts of the interactions among factors are not uniform. Some interactions may be more prone to trigger system failures, while other may have little or no impact on the system. To effectively detect these different interactions, variable strength (VS) covering arrays can be applied. This can offer different covering strengths

to different groups of factors, and can therefore provide a practical approach to test real applications. Definition 3 (VS Covering Array): A VS covering array, m 1 denoted by VCA(N ; t, a11 . . . akk , CA1 (t1 , bm . . . bp p ), . . . , 1 nq 1 CAj (tj , cn 1 . . . cq )), is an N × n covering array of covering strength t containing one or more sub covering arrays, namely CA1 , . . . , CAj , each of which has covering strength t1 , . . . , tj all larger than t. Consider the e-commerce system shown in Table I. If the interactions of three factors, Payment Server, Web Server, and Business Database, have a higher probability to trigger system failures, then a VS covering array can be constructed. As in Table II, only three more test cases (Table III) are needed to construct the VCA(12; 2, 23 32 , CA(3, 22 31 )). With these 12 test cases, not only are all 2-way schemas of all five factors covered, but also all 3-way schemas of these three factors (Payment Server, Web Server, and Business Database) are covered. B. Covering Array Generation Covering arrays are used as test suites in CT. Covering array generation is the process of test suite construction. It is the most active area in CT with more than 50% of research papers focusing on this field [1]. Due to limited testing resources, all aim to construct a minimal covering array while still maintaining full coverage of combinations. Computational methods have been used widely for covering array generation because they can be applied to any systems. In general, these methods generate all possible combinations first. Then they generate test cases to cover these combinations one-by-one. One-testat-a-time is the most widely used strategy among evolutionary algorithms to generate a covering array. The one-test-at-a-time strategy was popularized by AETG [28] and was further used by Bryce and Colbourn [29]. This strategy takes the model of SUT(n, 1 , . . . , n ) where n is the number of factors and i is the number of valid levels of factor i, and the covering strength t as input. At first, an empty test suite TS and a set S of t-way schemas to be covered are initialized. In each iteration, a test case is generated with the highest fitness value according to some heuristic techniques. Then it is added to TS and the t-way schemas covered by it are removed. When all the t-way schemas have been covered, the final test suite TS is returned. This process is shown in Algorithm 1. In this strategy, a fitness function must be used to evaluate the quality of a candidate test case (line 6 in Algorithm 1). It is an important part of all heuristic techniques. In covering array generation, the fitness function takes the test case as the input and then outputs a fitness value representing its "goodness." It is defined as follows.

578

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

Algorithm 1 One-Test-at-a-Time Strategy 1: Input: SUT(n, 1 , . . . , n ) and covering strength t 2: Output: covering array TS 3: TS =  4: Construct S (all the t-way schemas to be covered) based on n, 1 , . . . , n and t 5: while S =  do 6: Generate a test case p with the highest fitness value according to some heuristics 7: Add p to the test suite TS 8: Remove the t-way schemas covered by p from S 9: end while 10: return TS

its corresponding position: vi,j (k + 1) =  × vi,j (k) + c1 × r1,j × (pbesti,j - xi,j (k)) (2) + c2 × r2,j × (gbestj - xi,j (k)) xi,j (k + 1) = xi,j (k) + vi,j (k + 1). (3) The best position of particle i in its history is pbesti , and gbest is the best position among all particles. The velocityupdate equation (2) captures the three basic concepts of PSO. The first is inertia, the tendency of the particle to continue in the direction it has been moving. The second is memory of the best position ever found by itself. The third is cooperation using the best position found by other particles. The parameter  is inertia weight. It controls the balance between exploration (global search state) and exploitation (local search state). Two positive real numbers c1 and c2 are acceleration constants that control the movement tendency toward the individual and global best position. Most studies set  = 0.9, and c1 = c2 = 2 to get the best balance [17], [31], [32]. In addition, r1,j and r2,j are two uniformly distributed random numbers in the range of [0.0, 1.0], used to ensure the diversity of the population. If the problem domain (the search space of particles) has bounds, a bound handling strategy is adopted to keep the particles inside the search space. Many different strategies have been proposed [33]. In the reflecting strategy, when a particle exceeds the bound of the search space in any dimension, the particle direction is reversed in this dimension to get back to the search space. For example, in case of a dimension with a range of values from 0 to 2, if a particle moves to 3, its position is reversed to 1. In addition, as the velocity can increase over time, a limit is set on velocity to prevent an infinite velocity or invalid position for the particle. Setting a maximum velocity, which determines the distance of movement from the current position to the possible target position, can reduce the likelihood of explosion of the swarm traveling distance [31]. Generally, the value of the maximum velocity is selected as i /2, where i is the range of dimension i. The pseudocode in Algorithm 2 presents the process of generating a test case by PSO. This algorithm can be invoked by line 6 of Algorithm 1 to generate a test case for t-way schemas. The n factors of the test case can be treated as an n-dimensional hyperspace. A particle pi = (x1 , x2 , . . . , xn ) can be regarded as a candidate test case. The fitness function is that of Definition 4, the number of uncovered t-way schemas in the generated test suite that are covered by particle pi . PSO employs real numbers but the valid values are integers for covering array generation, so each dimension of particle's position can be rounded to an integer while maintaining the velocity as a real number. This method is used in all prior research applying PSO to covering array generation [13]­[16]. III. R ELATED W ORK In this section, three different but related aspects are discussed. We first summarize search-based CT, especially the current applications of PSO for covering array generation. Then, we introduce prior research on discrete versions of PSO,

Definition 4 (Fitness Function): Let TS be the generated test set, and p be a test case. Then fitness(p) is the number of uncovered t-way schemas in TS that are covered by p. The fitness function can be formulated as fitness(p) = |schemat ({p}) - schemat (TS)| (1)

where schemat (TS) represents the set of all t-way schemas covered by test set TS, and | · | stands for cardinality. When t t-way schemas covered by p are not covered by TS, all Cn the fitness function reaches the maximum value fitness(p) = t. |schemat ({p})| = Cn For example, consider the 2-way covering array generation of the e-commerce system shown in Table II. Suppose that TS consists of test cases (0, 0, 0, 0, 0) and (0, 0, 1, 1, 1). The fitness function computes the fitness value for a candidate test case p = (1, 0, 1, 2, 2) as follows: because p covers ten 2-way schemas, namely schema2 ({p}) = {(1, 0, ­, ­, ­), (1, ­, 1, ­, ­,), (1, ­, ­, 2, ­), (1, ­, ­, ­, 2), (­, 0, 1, ­, ­), (­, 0, ­, 2, ­), (­, 0, ­, ­, 2), (­, ­, 1, 2, ­), (­, ­, 1, ­, 2), (­, ­, ­, 2, 2)} and TS only covers (­, 0, 1, ­, ­) in p, the function returns fitness(p) = 9. C. PSO PSO is a swarm-based evolutionary computation technique. It was developed by Kennedy et al. [30], inspired by the social behavior of bird flocking and fish schooling. PSO utilizes a population of particles as a set of candidate solutions. Each of the particles represents a certain position in the problem hyperspace with a given velocity. A fitness function is used to evaluate the quality of each particle. Initially, particles are distributed in the hyperspace uniformly. Then each particle repeatedly updates its state according to the individual best position in its history (pbest) and the current global best position (gbest). Eventually, each particle possibly moves toward the direction of the individual optimum and global optimum, and finds an optimal or near optimal solution. Suppose that the problem domain is a D-dimensional hyperspace. Then the position and velocity of particle i can be represented by xi  RD and vi  RD respectively. CPSO uses the following equations to update a particle's velocity and position, where vi,j (k) represents the jth component of the velocity of particle i at the kth iteration, and xi,j (k) represents

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

579

Algorithm 2 Generate Test Case by PSO 1: Input: SUT(n, 1 , . . . , n ), covering strength t and the related parameters of PSO 2: Output: the best test case gbest 3: it = 0, gbest = NULL 4: for each particle pi do 5: Initialize the position xi and velocity vi randomly 6: end for 7: while it < maximum iteration do 8: for each particle pi do 9: Compute the fitness value fitness(pi ) 10: if fitness(pi ) > fitness(pbesti ) then 11: pbesti = pi 12: end if 13: if fitness(pi ) > fitness(gbest) then 14: gbest = pi 15: end if 16: end for 17: for each particle pi do 18: Update the velocity and position according to Equations 2 and 3 19: Apply maximum velocity limitation and bound handling strategy 20: end for 21: it = it + 1 22: end while 23: return gbest

applied PSO to 2-way covering array generation. They further used a test suite minimization algorithm to reduce the size of the generated covering array. Current applications of PSO for covering array generation can yield smaller covering arrays than most greedy algorithms, but they all apply the same rounding operator to the particle's position, and they lack guidelines on the parameter settings. B. Discrete Versions of PSO PSO was initially developed to solve problems in continuous space, but PSO can also be applied to some discrete optimization problems including binary or general integer variables. Many discrete versions of PSO have been proposed [17]­[22]. Chen et al. [18] classify existing algorithms into four types. 1) Swap operator-based PSO [19] uses a permutation of numbers as position and a set of swaps as velocity. 2) Space transformation-based PSO [20] uses various techniques to transform the position, defined as a vector of real numbers, to the corresponding solution in discrete space. 3) Fuzzy matrix-based PSO [21] defines the position and velocity as a fuzzy matrix, and decode it to a feasible solution. 4) Incorporating-based PSO [22] consists of hybrid methods incorporating some heuristic or problem dependent techniques. Chen et al. [18] also propose a S-PSO method based on sets with probabilities, which we later adapt to represent a particle's velocity. S-PSO uses a set-based scheme to represent the discrete search space. The velocity is defined as a set with probabilities, and the operators are all replaced by procedures defined on the set. They extend some PSO variants to discrete versions and test them on the traveling salesman problem and the multidimensional knapsack problem. They show that the discrete version of CLPSO [25] can perform better than other variants. S-PSO was found to characterize the combinatorial optimization problems very well. Gong et al. [36] then employed such a set-based representation scheme for solving vehicle routing problems to obtain a new method, S-PSO-VRPTW. C. PSO Variants The original PSO may become trapped in a local optimum. In order to improve the performance, many variants have been proposed [17], [24]­[27]. Chen et al. [37] classify these variants into four different groups. The first group of variants aims to adjust the control parameters during the evolution, for example by decreasing inertia weight  linearly from 0.9 to 0.4 over the search process. The second group employs different neighborhood topologies. The basic local version of PSO makes the particle learn from the local best position lbesti found by particle i's neighborhood instead of the global best position gbest. RPSO and VPSO are two common versions which use a ring topology and a Von Neumann topology, respectively. The third group uses hybrid strategies with other search techniques. Many types of operators, such as genetic operators

to improve CPSO for discrete problems. Finally, some PSO variants are introduced. These try to enhance PSO for conventional continuous problems by adopting strategies that can further improve covering array generation. A. PSO in Search-Based CT SBSE has grown quickly in recent years. Many problems in software engineering have been formulated into searchbased optimization problems, and heuristic techniques have been used to find solutions. Software testing is a major topic in software engineering. Many heuristic techniques have also been applied to this field, including functional testing, mutation testing, stress testing, temporal testing, CT, and regression testing. Currently, many classic heuristic techniques, such as SA [3]­[7], GA [8]­[10], and ACO [9], [11], [12] have been applied to generate uniform and VS covering arrays successfully. PSO has also been applied to software testing. Windisch et al. [34] applied PSO to structural testing, and compared its performance with GA. They showed that PSO outperformed GA for most cases in terms of effectiveness and efficiency. Ganjali [35] proposed a framework for automatic test partitioning based on PSO, observing that PSO performed better than other existing heuristic techniques. PSO has been applied to covering array generation. Ahmed and Zamli [15] proposed frameworks of PSO to generate different kinds of covering arrays, including t-way (PSTG [14]) and VS-PSTG. Then they extended PSTG to support VS covering array generation [16]. Chen et al. [18] also

580

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE IV D IFFERENT G ROUPS OF DPSO AND PSO VARIANTS

(selection, crossover, and mutation), evolutionary computation paradigms and biology inspired operators have been used. The fourth uses multiswarm techniques. Several sets of swarms optimize different components of the solution concurrently or cooperatively. In our experiments, four representative PSO variants are included, as follows. 1) Ratnaweera et al. [24] proposed a typical variant of the first group, TVAC, which uses a time varying inertia weight and acceleration constant to adjust the parameters. 2) Liang et al. [25] proposed the CLPSO method, a wellknown variant of the second group, which allows the particle to learn from other particles' individual best positions in different dimensions. 3) Zhan et al. [26] proposed an adaptive PSO (APSO) that can be seen as a variant of the third group. They developed an evolutionary state estimation to adaptively control the parameter and used an elitist learning strategy. 4) DMS-PSO [27], a variant of the fourth group, was proposed by Liang and Suganthan [27]. It is characterized by a set of swarms with small sizes and these swarms are frequently regrouped. In summary, Table IV lists the different groups of discrete versions of PSO and PSO variants. IV. DPSO In this section, a new DPSO for covering array generation is presented. We firstly illustrate the weakness of CPSO with a simple example. Then the representation scheme of a particle's velocity and the corresponding redefined operators are introduced. Finally, we give two auxiliary strategies to enhance the performance of DPSO. A. Example In CPSO, a particle's position represents a candidate test case; its velocity, a real vector, represents the movement tendency of this particle in each dimension. This scheme is meaningful in a continuous optimization problem, because an optimal solution may exist near the current best particle's position. So it is desirable to move the particle to this area for further search. This may not hold for covering array generation.

Here, we use CA(N ; 2, 34 ) as an example. Suppose that three test cases (0, 1, 0, 1), (2, 1, 1, 2), and (1, 2, 0, 0) have been generated and added to TS in Algorithm 1, and the fourth one is to be generated according to CPSO. A possible candidate particle pi may have the position (0, 0, 0, 0) and velocity (0.5, 0.6, -0.4, 0.2) with fitness value 4, and its individual best position pbesti may be (0, 0, 1, 1) with fitness value 5. The global best position gbest may be (0, 2, 2, 2) with fitness value 6. According to the update (2), if we take  = 0.9 and c1 × r1 = c2 × r2  0.65, in the next iteration, pi 's velocity may be transformed to (0.45, 1.84, 1.59, 2.13). Thus, pi moves to the new position (0, 1, 1, 1) with a limitation on the velocity (the velocity is kept in [- i /2, i /2]), or (0, 1, 1, 2) with no limitation. In both cases, pi only has fitness value 2 after updating. When this occurs, pi evolves to a worse situation, although its position is closer to the pbesti and gbest than its original one. Analyzing the fitness measurement, the main contribution to the fitness value is the combinations that the test case can cover, not the concrete "position" at which it is located. For example, test case (2, 1, 1, 2) has a larger fitness value than (0, 0, 0, 0) because it covers six new schemas [(2, 1, ­, ­), (2, ­, 1, ­), (2, ­, ­, 2) etc.], not because of its relative distance to other particles. B. DPSO To overcome this weakness, the movement of particles should be modified. Inspired by the set-based representation scheme of velocity, DPSO is designed as a version of S-PSO [18] to make the particle learn from the individual and global best more effectively when generating covering arrays. Unlike S-PSO, the element of the velocity set in DPSO is designed for covering array generation, and DPSO does not classify the velocity set into different dimensions to avoid the inconsistency of different dimensions when updating velocities in S-PSO. In DPSO, a particle's position represents a candidate test case, while its velocity is changed to a set of t-way schemas with probabilities. Other than the velocity's representation and the newly defined operators, the evolution procedure of DPSO is the same as CPSO (Algorithm 2). Definition 5 (Velocity): The velocity is a set of pairs (s, p(s)), where s is a possible t-way schema of the covering array and p(s) is a real number between 0 and 1 representing the probability of the selection of schema s to update the current position. In the initialization of the swarm, the particle's position is t possible different schemas are randomly assigned. Then Cn selected randomly and each of them is assigned a random t pairs form the initial velocprobability p(s)  (0, 1). These Cn ity set of this particle; the size of this set changes dynamically during the evaluation. We consider the same example CA(N ; 2, 34 ). In DPSO, when particle pi is initialized, its position xi (k) may be (0, 0, 0, 0) representing a candidate test case as before, and its velocity vi (k) may be such a set {((1, 1, ­, ­), 0.7), ((0, ­, 0, ­), 0.3), ((­, 0, ­, 1), 0.8), ((0, ­, ­, 2), 0.9),

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

581

(a)

(b)

(c)

(d)

(e)

Fig. 1. Example of pi 's velocity updating. (a) 0.9 × vi (k). (b) 2 × r1 × (pbesti - xi (k)). (c) 2 × r2 × (gbest - xi (k)). (d) vi (k + 1). (e) Final vi (k + 1) where pro1 = 0.5.
2 = 6 ((­, 2, 1, ­), 0.5), ((­, ­, 0, 1), 0.2)} which contains C4 pairs. DPSO follows the conventional evolution procedure (Algorithm 2) that uses (2) and (3) to update the velocity and position of a particle. To adapt the new scheme for velocity in Definition 5, the related operators in these equations must be redefined. 1) Coefficient × Velocity: The coefficient is a real number which may be a parameter or a random number. It modifies all the probabilities in the velocity. Definition 6 (Coefficient × Velocity): Let a be a nonnegative real number and v be a velocity, a × v = {(s, p(s) × a)|(s, p(s))  v}. (If p(s) × a > 1, p(s) × a = 1.) For example, Fig. 1(a) shows the result for  × vi (k) where  = 0.9. 2) Position­Position: The difference of two positions gives the direction on which a particle moves. The results of the minus operator is a set of (s, p(s)) pairs, as velocity. Definition 7 (Position­Position): Let x1 and x2 be two positions. Then x1 - x2 = {(s, 0.5)|s is a schema that exists in x1 but not in x2 }. In the newly generated schema, probability p(s) for s is set to 0.5 so that the acceleration constants take similar values in both CPSO and DPSO. As in (2), the result of position­position is multiplied by ci × ri . In CPSO, ci is often set to 2 and ri is a random number between 0 and 1 (recall Section II-C). In DPSO, we want the value of final probability to have a range between 0 and 1 after multiplying by ci . Setting p(s) to 0.5 puts the result in [0, 1] when ci = 2. For example, suppose that xi (k) = (0, 0, 0, 0), pbesti = (0, 0, 1, 1), and gbest = (0, 2, 2, 2) as before. We can get pbesti - xi (k) = {((0, ­, 1, ­, ­), 0.5), ((0, ­, ­, 1), 0.5), ((­, 0, 1, ­), 0.5), ((­, 0, ­, 1), 0.5), ((­, ­, 1, 1), 0.5)}. Fig. 1(b) and (c) shows the results for 2 × r1 × (pbesti - xi (k)) and 2 × r2 × (gbest - xi (k)) respectively. 3) Velocity + Velocity: The addition of velocities gives a particle's movement path. The plus operator results in the union of two velocities. Definition 8 (Velocity + Velocity): Let v1 and v2 be two velocities. Then v1 + v2 = {(s, p(s))| if (s, p1 (s))  v1 and (s, p2 (s))  v2 , p(s) = p1 (s) + p2 (s); if (s, pi (s))  v1 and (s, pi (s))  / v2 or (s, pi (s))  v2 and (s, pi (s))  / v1 , p(s) = pi (s)}. (If p1 (s) + p2 (s) > 1, p1 (s) + p2 (s) = 1.) For example, Fig. 1(d) shows the results for pi 's new velocity vi (k + 1) after updating velocity.

Algorithm 3 Position Updating 1: Input: position xi (k), velocity vi (k + 1), pro2 and pro3 2: Output: new position xi (k + 1) 3: xi (k + 1) = (­, ­, . . . , ­) 4: Sort vi (k + 1) in descending order of p(s) 5: for each pair (si ,p(si )) in vi (k + 1) do 6: Generate a random number   [0, 1] 7: if  < p(si ) then 8: for each fixed level in si do 9: Generate a random number   [0, 1] 10: if  < pro2 and the corresponding factor of has not been fixed in xi (k + 1) then 11: Update xi (k + 1) with 12: end if 13: end for 14: end if 15: end for 16: if xi (k + 1) has unfixed factors then 17: Fill these factors by the same levels of previous position xi (k) 18: end if 19: Generate a random number   [0, 1] 20: if  < pro3 then 21: randomly change the level of one factor of xi (k + 1) 22: end if 23: return xi (k + 1)

In the velocity-updating phase of DPSO (2), we introduce a new parameter pro1 to control the size of the final velocity set. If p(si ) < pro1 , the pair (si , p(si )) is removed from the final velocity. For example, if we set pro1 = 0.5, vi (k + 1) is removed as shown in Fig. 1(e). Here, the velocity has been sorted in descending order of p(s), where if several p(s) have the same value, they are in an arbitrary order. If vi (k + 1) becomes empty, it stays empty until new pairs are added to it. As long as the velocity is empty, the particle's position is not updated and no better solutions can be found from this particle. In Section IV-C1, we discuss how to reinitialize this particle. 4) Position + Velocity: Position plus velocity is the position updating phase. Algorithm 3 gives the pseudo code of this procedure. Here, two new parameters, pro2 and pro3 , numbers in the range [0, 1], are introduced. pro2 is used to determine

582

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

the probability of selecting each fixed level in schema s and pro3 is a mutation probability to make the particle mutate randomly. An example helps to describe this procedure. We already have pi 's current position xi (k) = (0, 0, 0, 0), and its updated velocity vi (k + 1) in descending order of p(s) as shown in Fig. 1(e). We also assume that pro2 and pro3 are both set to 0.5. Each schema si here is selected to update the position with probability p(si ). For the first pair ((0, ­, ­, 2), 1.0), suppose that the random number  satisfies  < 1.0. Then, for each fixed level of this pair, namely level 0 of the first factor and level 2 of the fourth factor, its corresponding factor has not been fixed in xi (k + 1). Suppose that we have the first  < 0.5 but the second  > 0.5, the first factor will be selected to update the position and the second factor will not. So the new position becomes (0, ­, ­, ­). For the second pair, we regenerate the random number  , and compare it with the probability 0.9. If  < 0.9, the second pair is selected. If we generate  < 0.5 in two rounds, the new position becomes (0, 2, 2, ­). Accordingly, if the third pair is selected, and its second factor's level 0 is chosen to update, it does not change position because this factor has been set to a fixed level 2. This procedure is repeated until all factors in the new position xi (k + 1) are set to fixed levels. If all pairs in velocity have been considered, unfixed factors of xi (k + 1) are filled by the same levels of previous position xi (k). For example, after finishing the For loop in line 15, if the fourth factor of xi (k + 1) has not been given any level, the fourth factor of xi (k) is used to update it. Then xi (k + 1) becomes (0, 2, 2, 0). C. Auxiliary Strategies Auxiliary strategies can be introduced to improve the performance of PSO. Two of them are added to DPSO: 1) particle reinitialization to make the search more effective and 2) additional evaluation of gbest to improve the selection of the global best test case. 1) Particle Reinitialization: The PSO algorithm starts with a random distribution of particles, which finally converge. Then the best position that has been found is returned. The swarm may jump out of a local optimum, but this can not be guaranteed because CPSO lacks specific strategies for this. When applying PSO for covering array generation, increasing the number of iterations does not improve the ability to escape a local optimum. Hence, particle reinitialization, a widely used method, is employed to help DPSO to jump out of the local optimum. The main issue with particle reinitialization is when to reinitialize the particle. We may select a threshold, so that the reinitialization is done when the number of iterations exceeds the threshold. In DPSO, a better method can be applied. Using the new representation for velocity, when the current particle pi 's position equals its individual best position pbesti and global best position gbest, the size (norm) of pi 's velocity reduces gradually, because no pairs are generated from (pbesti - xi ) and (gbest - xi ), and the original pairs in velocity are removed gradually under the influence of  × v (reduce the p(s) of original pairs) and parameter pro1 . After a few fluctuations around gbest, the particle may stay at gbest, and

TABLE V T WO D IFFERENT C ONSTRUCTIONS OF CA(N ; 2, 34 )

its size (norm) of velocity is zero. We can use this scenario to trigger reinitialization of the particle. When the reinitialization is done, each dimension of a particle's position is randomly assigned a valid value, and its velocity is regenerated as in the initialization of the swarm. 2) Additional Evaluation of gbest: Current PSO methods to generate covering arrays generally employ the fitness function in Definition 4. This fitness function only focuses on the current candidate, and does not consider the partial test suite TS. Consider the generation of CA(N ; 2, 34 ). Table V shows two different construction processes. Both constructions generate (0, 0, 0, 0) as the first test case. Then they choose different test cases, but each of the first three reaches the largest number of 2 = 6. The difference between newly covered combinations, C4 these two constructions emerges when generating the fourth test case. In Construction 1, because the combinations with the same level between any two factors have all been covered, we cannot find a new test case that can still cover six combinations. However, in Construction 2, such a new test case can be found, (1, 0, 1, 2). Because the minimum size of CA(N ; 2, 34 ) is 9, each test case is required to cover six new combinations. Thus, Construction 1 cannot generate the minimum test suite, but Construction 2 can. In general, there may exist multiple test cases with the same highest fitness value, which make them equally qualified to be gbest in Algorithm 2. Instead of arbitrarily selecting one as gbest, it is better to apply additional distance metric to select one among them. As shown in Table V, if the new test case is similar to the existing tests [as (0, 1, 1, 1) is closer to (0, 0, 0, 0) than (1, 1, 1, 1)], there may be a better chance to find test cases with larger fitness subsequently. In order to measure the "similarity" between a test case t and an existing test suite TS, we use the average Hamming distance. The Hamming distance d12 indicates the number of factors that have different levels between two test cases t1 and t2 . Hence, the similarity between t and TS can be defined by the average Hamming distance H (t, TS) = 1 |TS| dtk .
kTS

(4)

The new gbest selection strategy selects the best candidate gbest based on two metrics. A test case that has the minimum average Hamming distance (i.e., the maximum similarity) from the set of test cases that all have the same highest fitness value is selected as gbest in Algorithm 2. For example, when we generate the second test case for CA(N ; 2, 34 ) in Table V, although (1, 1, 1, 1) and (0, 1, 1, 1) both have the highest fitness value 6, (0, 1, 1, 1) is selected as gbest

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

583

TABLE VI C OMPARING P ERFORMANCE B ETWEEN CPSO AND DPSO

because its average Hamming distance, 3, is smaller than that of (1, 1, 1, 1), 4. We expect that this additional evaluation can enhance the probability of generating a smaller test suite. In addition, because we still want to make the particle follow the conventional search behavior on the individual best direction, this additional distance metric will not be used in updating the pbest of DPSO. V. E VALUATION AND PARAMETER T UNING In this section, we first evaluate the effectiveness of DPSO in some representative cases, and compare the results against CPSO. Then the optimal parameter settings for both CPSO and DPSO are explored. The goal of evaluation and parameter tuning is to make the size of generated covering array as small as possible. Five representative cases of covering arrays, listed below, are selected for our experiments CA1 (N ; 2, 610 ) CA2 (N ; 3, 57 ) CA3 (N ; 4, 39 ) CA4 (N ; 2, 43 53 62 ) VCA(N ; 2, 315 , CA(3, 34 )). We consider four independent parameters, iteration number (iter), population size (size), inertia weight (), and acceleration constant (c), which play similar roles in both CPSO and DPSO, and three new parameters for DPSO, pro1 , pro2 , and pro3 . We carry out a base choice experiment to study the impact of various values of these parameters on CPSO and DPSO's performance and find the recommended settings for them. First, a base setting is chosen as the basic configuration. Then the value of each parameter is changed to create a series of configurations, while leaving the other parameters unchanged. Initially, we set iter = 50, size = 20,  = 0.9, c = 1.3, and pro1 = pro2 = pro3 = 0.5 as a basic configuration based on the previous studies [13], [16] and our empirical experience. To obtain statistically significant results, the generation of each case of covering array is executed 30 times. A. Evaluation of DPSO We compare the performance between CPSO and DPSO with the basic configuration. Five classes of covering arrays are generated by these two algorithms. The sizes obtained and average execution time per test case are shown as CPSO1 and DPSO in Table VI. The best and mean array sizes of DPSO are all better than those of CPSO1 . The new discrete representation scheme and auxiliary strategies improve PSO for covering array generation. DPSO can produce smaller covering arrays than CPSO with the basic configuration. However, DPSO spends more time

during the evolution, because its new operators for updating are more intricate than the conventional ones. DPSO needs to deal with many elements of the velocity set, whereas the conventional scheme only needs simple arithmetic operations. To compare the performance between CPSO and DPSO given the same execution time, for each case we let the execution time per test case for CPSO equal to that for DPSO, so that CPSO can spend more time in searching. We refer to this version of CPSO as CPSO2 . In addition, a t-test between CPSO2 and DPSO is conducted and the corresponding p-value is presented. A p-value smaller than 0.05 indicates that the performance of these two algorithms is statistically different with 95% confidence. From CPSO1 , CPSO2 , and DPSO in Table VI, the performance of CPSO is improved with more search time. DPSO, which must use fewer iterations, still works better than CPSO. The effectiveness of DPSO comes from the essential improvement of its new representation scheme for velocity and the auxiliary strategies. The results of the t-test demonstrate the significance of these differences. Therefore, we can conclude that DPSO performs better than CPSO with fewer iterations for covering array generation. B. Parameter Tuning In the base choice experiment, because the sizes of the various covering arrays differ, the mean size of each case (si ) obtained is normalized using si = si - smin . smax - smin

This normalization enables the graphical representation of different cases on a common scale. Because some parameters may not significantly impact the performance, we use ANOVA (significance level = 0.05) to test whether there exist significant differences among the mean results obtained by different parameter settings. When changing the parameter settings have no significant impact on the generation results, these results will be presented as dotted lines in the corresponding figures. For example, CA4 is presented as a dotted line in Fig. 2(b). It means that the iteration number does not significantly impact the generation of this case of covering array, and so this case will not be further considered when identifying the optimal settings. 1) Iteration Number (iter): Iteration number determines the number of updates. PSO typically requires thousands of iterations to evolve. Nevertheless, only a few iterations are required to generate covering array according to [14]­[16]. So we

584

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

(a)
Fig. 2. Comparing array sizes under different iteration numbers. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 3. Comparing array sizes under different population sizes. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 4.

(b)

Comparing array sizes under different settings of population size and iteration number. (a) CPSO. (b) DPSO.

change its value from 50, the base setting, to 1950 incrementally. Much larger values are not used because the execution time may increase markedly without a commensurate increase in the quality of the results. Fig. 2 shows the results for different choices of iteration numbers, where each line represents a covering array. Performance is improved with increasing iterations for both CPSO and DPSO. A small number of iterations may not be appropriate due to insufficient searching. Because the optimal settings are different among different cases, and several settings can be chosen to generate the minimum covering arrays, it is open to debate which setting is the "best" one. Given time constraints, for a population size of 20, a good setting of iteration number could be approximately 1000 for both CPSO and DPSO. 2) Population Size (Size): Population size determines the initial search space. Generally a small value, e.g., 20, is satisfactory for most cases. A large population size may bring a higher diversity and find a better solution, but it also increases the evolving time. For the same reason as before, we change the value of population size from 10 to 100.

Fig. 3 shows the results for different choices of population size. There is no doubt that the mean array size obtained decreases as population size increases. A large population size can have more chances to generate smaller covering arrays, but its execution time can become prohibitive. In addition, because iteration number and population size together determine the search effort of PSO, we explore the combinations between these two parameters. We let iter × size be a constant 20 000, and generate each case under different settings of these two parameters. Fig. 4 shows the results, where PSO prefers a relatively larger population size. In CPSO, ten particles with 2000 iterations is the worst setting, and most cases produce good results with 60 or 80 particles. In DPSO, the best choice of population size is still 80. In both CPSO and DPSO, the largest population size 100 cannot produce as good results as that of 80 due to having fewer iterations. So achieving smallest covering arrays requires a good balance between these two parameters, and a moderately large population size is necessary for both CPSO and DPSO. Thus, we can set iteration number to 250, and population size to 80, as the recommended settings for both CPSO and DPSO.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

585

(a)
Fig. 5. Comparing array sizes under different inertia weights. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 6. Comparing array sizes under different acceleration constants. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 7.

(b)

(c)

Comparing array sizes under different settings of (a) pro1 , (b) pro2 , and (c) pro3 . TABLE VII R ECOMMENDED PARAMETER S ETTINGS

3) Inertia Weight (): Inertia weight determines the tendency of the particle to continue in the same direction. A small value help the particle move primarily toward the best position, while a large one is helpful to continue its previous movement. A linearly decreasing value is also used as it can make the swarm gradually narrow the search space. Here, we investigate both fixed values and a linearly decreasing value from 0.9 to 0.4 over the whole evolution (presented as "dec"). Fig. 5 shows the results for different choices of inertia weight. In CPSO, most of the smallest covering arrays are generated by large fixed inertia weights. The decreasing value does not perform as well as the large values, such as 0.9 for CA1 , CA2 , and CA4 . So 0.9 can be the recommended choice for CPSO. In DPSO, 1.1 is the worst choice because the particle may update its position based on its own velocity and thus fail to learn from individual and global best positions. The decreasing value can perform reasonably well, but a fixed value 0.5 may be a better choice in that it keeps the effort of global search moderate. Thus, we can recommend the fixed inertia weight of 0.9 for CPSO, and 0.5 for DPSO. 4) Acceleration Constant (c): Acceleration Constants c1 and c2 control the learning from pbest and gbest, respectively. Generally, they are set to the same value to balance

the influence of these two positions. Setting c1 and c2 to a large value may make the particle more likely attracted to the best position ever found, while a small one may make the search far from the current optimal region. In this paper, we set c1 = c2 = c, and vary c from 0.1 to 2.9. Fig. 6 shows the results for different choices of acceleration constant. Unlike other parameters, there is a consistent trend in all five cases. In CPSO, the values larger than 0.5 all produce good results. In DPSO, 1.3 can definitely be regarded as the optimal value. Thus, we set 1.3 as the recommended value of acceleration constant for both CPSO and DPSO. 5) pro1 , pro2 , and pro3 : These three parameters are new to DPSO. Parameter pro1 determines the size of the final velocity. As those pairs whose probabilities are smaller than pro1 are removed from the final velocity set, a small value may keep more pairs in the velocity for the next evolution, but it

586

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE VIII S IZES (N ) OF C OVERING A RRAYS FOR n FACTORS , E ACH H AVING T HREE L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, n, 3)

also requires more computation time. Parameter pro2 determines the probability of selecting each fixed level from each schema when updating positions. A larger value may lead to a quick construction of new position, but it may also lead to fast convergence toward a local optimum. Parameter pro3 determines the mutation probability when updating positions. A larger value may enhance the randomness, but it also lowers the convergence speed. In this paper, values from 0.1 to 1.0 for these three parameters are investigated. Fig. 7 shows the results. For pro1 , a large value is not appropriate because it removes nearly all pairs from the final velocity set. A medium value 0.5, which appears to lead to the best result, may be the best choice. For pro2 , the smallest value 0.1 yields the best results. Slower construction of the new position may slow the convergence toward a local optimum, but it also takes longer. So, in order to balance performance and execution time, we take 0.3 as the recommended value for pro2 . For pro3 , a larger value may be a good choice. The frequent mutation of new position may bring better results, but also takes longer to converge. So, we take 0.7 as the recommended value for pro3 . In summary, the recommended parameter settings for PSO for covering array generation are different from previously suggested ones [13], [16]. Some parameters may significantly impact the performance in some cases, and parameter tuning is necessary to enhance heuristic techniques for particular applications. Naturally, the optimal settings vary for different cases of covering arrays. There may not exist a common setting that can always lead to the best results. Because it is

impractical to tune parameters for each case in real life, we suggest two general settings for CPSO and DPSO, as shown in Table VII, which can typically lead to better performance within reasonable execution time. VI. C OMPARING A MONG PSO S In this section, we compare the best reported array sizes generated by PSO in [16] with our findings for CPSO, DPSO, and four representative variants. Because the research in [16] demonstrated that their generation results typically outperform greedy algorithms, in this paper, we do not compare CPSO and DPSO with greedy algorithms. We implement both the original and discrete versions of four variants (TVAC [24], CLPSO [25], APSO [26], and DMS-PSO [27]) to generate covering arrays. Their discrete versions are extended based on new representation scheme of velocity and auxiliary strategies. We name them D-TVAC, DCLPSO, D-APSO, and D-DMS-PSO, respectively. In CLPSO, a particle can learn from different particles' pbest in each dimension whereas we do not distinguish dimensions strictly in DPSO. So in D-CLPSO, a particle can fully learn from different particles' pbest in all dimensions. That may weaken the search ability of CLPSO. For the other three variants, they can be directly extended based on DPSO. All algorithms are compared using the same number of fitness function evaluations. CPSO and DPSO use the recommended settings shown in Table VII. For the variants, iter, size and pro1 , pro2 and pro3 for discrete versions are set to

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

587

TABLE IX S IZES (N ) OF C OVERING A RRAYS FOR S EVEN FACTORS , E ACH H AVING

L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, 7, )

their recommended values.  and c are also set to recommended values unless they are adaptively adjusted during the evolution, in which case their range is set to [0.4, 0.9] and [0.8, 1.8], respectively. The new control parameters for these variants follow their suggested settings. Tables VIII­XIII give the results. Because of the execution time, we only consider covering strengths from 2 to 4, and the generation of each covering array is repeated 30 times. A t-test (significance level = 0.05) is also conducted to test whether there exists a significant difference between the mean sizes produced by the two algorithms. In the first three columns, we report the best and mean array sizes obtained from previous results, CPSO and DPSO, where boldface numbers indicate that the difference between CPSO and DPSO is significant based on the t-test. In the last four columns, we report the mean array sizes from the original and discrete versions of each PSO variant (presented as meanc and meand respectively), where boldface numbers indicate that the difference between meanc and meand of each variant is significant. A. Uniform Covering Arrays Tables VIII­X present the results for uniform covering arrays. We extend the cases considered in [16], where "­" indicates the not available cases. In Tables VIII, we report array sizes for n factors, each having three levels. In Tables IX and X, we report array sizes for 7 and 10 factors, each having levels. Their covering strengths all range from 2 to 4. Typically, CPSO can produce smaller sizes than those reported in [16], demonstrating the effectiveness of parameter tuning. Furthermore, DPSO can produce the smallest best and mean sizes in almost all cases, and usually the performance of DPSO is significantly better than CPSO. When the covering strength t = 2 is adopted (Table VIII), DPSO does

not generally outperform CPSO. But when covering strength increases, DPSO performs better. Sometimes the mean sizes for DPSO are smaller than the best sizes for CPSO. Because generating a covering array with higher covering strength is more complicated, DPSO may be more appropriate for generating such covering arrays. In Tables IX and X, we also find that DPSO is superior for covering arrays with high covering strength. Overall, these results show the effectiveness of DPSO for uniform covering array generation. Surprisingly, DPSO does not beat previous results for CPSO when covering arrays have two levels for each factor (Table IX). This appears to be a weakness of DPSO. Although DPSO generates smaller covering arrays than previous results and CPSO, other techniques may still yield better results than DPSO (some best known sizes can be found in [38]). B. VS Covering Array Tables XI­XIII give the results for VS covering arrays. Based on CA(N ; 2, 315 ), CA(N ; 3, 315 ), and CA(N ; 2, 42 52 63 ), some different cases of sub covering arrays conducted in [16] are examined. Their covering strengths are at most 4. Generally, we can draw similar conclusions as for uniform covering arrays. CPSO with the suggested parameter setting can produce better results than reported sizes in some cases. DPSO also usually beats them on the best and mean sizes. In Table XI, often the difference between CPSO and DPSO is not significant. In part this is because for the CA(3, 33 ), CA(3, 33 )2 , CA(3, 33 )3 , and CA(4, 34 ), the same best results are provably the minimum (e.g., the minimum size of the CA(3, 33 ) is 3 × 3 × 3 = 27). For the other cases, although sometimes the difference is not significant, DPSO can still generate smaller covering arrays. DPSO remains a good choice for generating VS covering arrays. In Tables XII and XIII, similar results can be found.

588

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE X S IZES (N ) OF C OVERING A RRAYS FOR T EN FACTORS , E ACH H AVING

L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, 10, )

TABLE XI S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 2, 315 , CA)

TABLE XII S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 3, 315 , CA)

For both uniform and variable cases of covering arrays, parameter tuning can enhance CPSO to generate smaller covering arrays than previous work. DPSO can produce the smallest covering arrays in nearly all cases, and in general the difference of performance between CPSO and DPSO is significant. DPSO is an effective discrete version of PSO for covering array generation. C. PSO Variants In order to further investigate the effectiveness of DPSO for covering array generation, we implement the original versions of four representative variants of PSO and extend them to their discrete versions based on DPSO. We apply them to generate

the same cases for covering arrays. The mean sizes obtained are shown in the last four columns in Tables VIII­XIII. We first compare the mean sizes of original PSO variants with those of CPSO and DPSO. In our experiments, TVAC's mean sizes are always larger than CPSO's. The linear adjustment of inertia weight and acceleration constant is not helpful for CPSO for covering array generation. Typically, APSO's mean sizes are also larger than CPSO's. Because APSO uses a fuzzy system to classify different evolutionary states, its ineffectiveness may result from inappropriate parameter settings. CLPSO and DMS-PSO can outperform CPSO typically. Although on occasion they achieve comparable performance with DPSO, they cannot perform as well as DPSO in most

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

589

TABLE XIII S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 2, 43 53 62 , CA)

TABLE XIV C OMPARING CPSO AND DPSO W ITH GA AND ACO

cases. That further demonstrates the effectiveness of DPSO for covering array generation. Because these four algorithms are representative PSO variants (as shown in Table IV), designing different neighborhood topologies and using multiswarm techniques may have potential to improve CPSO for covering array generation. We next compare the original and discrete versions of each variant. Except for CLPSO, the other three variants can be improved using their discrete versions, and the improvement is also significant. TVAC cannot outperform CPSO, but it is enhanced by DPSO so that D-TVAC can produce smaller mean sizes than CPSO in most cases. The linear adjustment is helpful for the discrete version. For CLPSO, only in a few cases is it improved by DPSO. Sometimes D-CLPSO even leads to worse results (see Table X), due primarily to the weakened search ability of its discrete version as explained in Section VI. For APSO, DPSO can enhance its original version, but D-APSO is still worse than DPSO. That may result from inappropriate settings as explained before. For DMS-PSO, sometimes DPSO does not enhance it (see Table XI). However, in most cases D-DMS-PSO can outperform DMS-PSO and has comparable performance with D-TVAC. The multiswarm strategy is also helpful for the discrete version.

In summary, the comparison study reveals that our suggested parameter settings are more suitable for covering array generation. DPSO is an effective discrete version of PSO. It can significantly outperform previous results and CPSO in nearly all cases for uniform and VS covering arrays. Furthermore, DPSO's representation scheme of a particle's velocity and auxiliary strategies not only enhance CPSO, but also typically enhance PSO variants. DPSO is a promising improvement on PSO for covering array generation. VII. C OMPARING DPSO W ITH GA AND ACO Because GAs and ACO [8]­[12] have also been successfully used for covering array generation, we compare CPSO and DPSO with the reported array sizes in [9] and [11]. There are no widely accepted benchmarks for the comparison of searchbased covering array generation, so we only consider these two representative and competitive works. Shiba et al. [9] applied both GA and ACO to generate uniform covering arrays (CA1 to CA8 in Table XIV), and Chen et al. [11] applied ACO to generate VS covering arrays (VCA9 to VCA12 in Table XIV). They both set algorithm parameters according to recommendations in related research fields without parameter tuning, while our CPSO and DPSO

590

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

use the recommended settings shown in Table VII. To compare these algorithms under the same number of fitness evaluations, for each case of covering array, the population size and the number of iterations of CPSO and DPSO are modified accordingly to satisfy the settings in [9] and [11]. Moreover, Shiba et al. [9] and Chen et al. [11] both applied test minimization algorithms to further reduce the size of generated covering arrays, while our CPSO and DPSO do not apply any minimization algorithms. Table XIV shows the comparison results, where boldface numbers indicate the best array sizes obtained, and "­" represents that the corresponding data is not available. The generation of each case of covering array of CPSO and DPSO is executed 30 times and the best and average results are presented. Because we do not implement GA and ACO for covering array generation, no statistical tests can be conducted here. In addition, because the platforms used for collecting the results differ, the comparison of computational time would not be informative. We nevertheless present the execution times of our CPSO and DPSO, which can serve as references for practitioners. From Table XIV, DPSO can outperform existing GA and ACO for covering array generation, despite the latter two applying test minimization algorithms. Because our DPSO is a version of PSO designed and tuned for covering array generation, it suggests that tuned versions of GAs and ACOs may be a promising area for further study. In addition, CPSO performs worse than GA and ACO in seven of 12 cases. That may result from the improvement by minimization algorithms in [9] and [11]. But for CA2 , CA3 , and CA6 , CPSO can still achieve smaller covering arrays. In summary, the results further demonstrate that DPSO is an effective discrete version of PSO for covering array generation. Further investigations of tuned versions of GA and ACO should be considered. VIII. C ONCLUSION Covering array generation is a key issue in CT. We developed a new DPSO for covering array generation, by adapting S-PSO to generate covering arrays and incorporating two auxiliary strategies to improve the performance. Parameter tuning was applied to both CPSO and DPSO to identify their best parameter settings. The original and discrete versions of four representative PSO variants were implemented and their efficacy for covering array generation was compared. The performance of DPSO was also compared with other existing evolutionary algorithms, GA and ACO. DPSO can perform better than CPSO with fewer iterations, and the performance of CPSO and DPSO is significantly impacted by their parameter settings. Different cases require different parameter settings; there may not exist a single choice that leads to the best results. After parameter tuning, we identified two recommended settings, which lead to relatively good performance for CPSO and DPSO. Indeed, CPSO with our recommended parameter settings improves on previously reported results, and our DPSO with recommended parameter settings usually beats CPSO. Typically, DPSO also significantly

enhances the performance of PSO variants. In addition, DPSO often outperforms GA and ACO to generate covering arrays. Consequently, DPSO is a promising improvement of PSO for covering array generation. Improvements on the methods here may be possible in a number of ways. One would be to investigate further evolution procedures and strategies proposed in PSO, such as hybridizing with penalty approaches to handle discrete unknowns [39], and compare the results with some exact schemes like branch and bound method. A second would be to examine one-column-at-a-time approaches or methods that construct the entire array, rather than the one-row-at-a-time approach adopted here. A third would be to incorporate DPSO with other methods, in particular with test minimization methods. R EFERENCES
[1] C. Nie and H. Leung, "A survey of combinatorial testing," ACM Comput. Surv., vol. 43, no. 2, pp. 11.1­11.29, 2011. [2] D. Kuhn and M. Reilly, "An investigation of the applicability of design of experiments to software testing," in Proc. 27th Annu. NASA Goddard/IEEE Softw. Eng. Workshop, Greenbelt, MD, USA, 2002, pp. 91­95. [3] M. Cohen, P. Gibbons, W. Mugridge, and C. Colbourn, "Constructing test suites for interaction testing," in Proc. 25th Int. Conf. Softw. Eng., Portland, OR, USA, 2003, pp. 38­48. [4] M. B. Cohen, C. J. Colbourn, and A. C. Ling, "Constructing strength three covering arrays with augmented annealing," Discrete Math., vol. 308, no. 13, pp. 2709­2722, 2008. [5] J. Torres-Jimenez and E. Rodriguez-Tello, "Simulated annealing for constructing binary covering arrays of variable strength," in Proc. Congr. Evol. Comput., Barcelona, Spain, Jul. 2010, pp. 1­8. [6] B. Garvin, M. Cohen, and M. Dwyer, "Evaluating improvements to a meta-heuristic search for constrained interaction testing," Empir. Softw. Eng., vol. 16, no. 1, pp. 61­102, 2011. [7] J. Torres-Jimenez and E. Rodriguez-Tello, "New bounds for binary covering arrays using simulated annealing," Inf. Sci., vol. 185, no. 1, pp. 137­152, 2012. [8] S. Ghazi and M. Ahmed, "Pair-wise test coverage using genetic algorithms," in Proc. Congr. Evol. Comput., vol. 2. Canberra, ACT, Australia, 2003, pp. 1420­1424. [9] T. Shiba, T. Tsuchiya, and T. Kikuno, "Using artificial life techniques to generate test cases for combinatorial testing," in Proc. 28th Annu. Int. Comput. Softw. Appl. Conf., vol. 1. Hong Kong, 2004, pp. 72­77. [10] J. McCaffrey, "An empirical study of pairwise test set generation using a genetic algorithm," in Proc. 7th Int. Conf. Inf. Technol. New Gener., Las Vegas, NV, USA, 2010, pp. 992­997. [11] X. Chen, Q. Gu, A. Li, and D. Chen, "Variable strength interaction testing with an ant colony system approach," in Proc. Asia-Pacific Softw. Eng. Conf., Penang, Malaysia, 2009, pp. 160­167. [12] X. Chen, Q. Gu, X. Zhang, and D. Chen, "Building prioritized pairwise interaction test suites with ant colony optimization," in Proc. 9th Int. Conf. Qual. Softw., Jeju-do, Korea, 2009, pp. 347­352. [13] X. Chen, Q. Gu, J. Qi, and D. Chen, "Applying particle swarm optimization to pairwise testing," in Proc. 34th Annu. Comput. Softw. Appl. Conf., Seoul, Korea, 2010, pp. 107­116. [14] B. S. Ahmed and K. Z. Zamli, "PSTG: A T-way strategy adopting particle swarm optimization," in Proc. 4th Asia Int. Conf. Math. Anal. Model. Comput. Simulat., Kota Kinabalu, Malaysia, 2010, pp. 1­5. [15] B. S. Ahmed and K. Z. Zamli, "A variable strength interaction test suites generation strategy using particle swarm optimization," J. Syst. Softw., vol. 84, no. 12, pp. 2171­2185, 2011. [16] B. S. Ahmed, K. Z. Zamli, and C. P. Lim, "Application of particle swarm optimization to uniform and variable strength covering array construction," Appl. Soft Comput., vol. 12, no. 4, pp. 1330­1347, 2012. [17] Y. del Valle, G. Venayagamoorthy, S. Mohagheghi, J.-C. Hernandez, and R. Harley, "Particle swarm optimization: Basic concepts, variants and applications in power systems," IEEE Trans. Evol. Comput., vol. 12, no. 2, pp. 171­195, Apr. 2008.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

591

[18] W.-N. Chen et al., "A novel set-based particle swarm optimization method for discrete optimization problems," IEEE Trans. Evol. Comput., vol. 14, no. 2, pp. 278­300, Apr. 2010. [19] M. Clerc, Discrete Particle Swarm Optimization (New Optimization Techniques in Engineering). New York, NY, USA: Springer, 2004. [20] W. Pang et al., "Modified particle swarm optimization based on space transformation for solving traveling salesman problem," in Proc. Int. Conf. Mach. Learn. Cybern., vol. 4. Shanghai, China, Aug. 2004, pp. 2342­2346. [21] W. Pang, K.-P. Wang, C.-G. Zhou, and L.-J. Dong, "Fuzzy discrete particle swarm optimization for solving traveling salesman problem," in Proc. 4th Int. Conf. Comput. Inf. Technol., Wuhan, China, 2004, pp. 796­800. [22] Y. Wang et al., "A novel quantum swarm evolutionary algorithm and its applications," Neurocomputing, vol. 70, nos. 4­6, pp. 633­640, 2007. [23] E. Campana, G. Fasano, and A. Pinto, "Dynamic analysis for the selection of parameters and initial population, in particle swarm optimization," J. Global Optim., vol. 48, no. 3, pp. 347­397, 2010. [24] A. Ratnaweera, S. Halgamuge, and H. Watson, "Self-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients," IEEE Trans. Evol. Comput., vol. 8, no. 3, pp. 240­255, Jun. 2004. [25] J. Liang, A. Qin, P. Suganthan, and S. Baskar, "Comprehensive learning particle swarm optimizer for global optimization of multimodal functions," IEEE Trans. Evol. Comput., vol. 10, no. 3, pp. 281­295, Jun. 2006. [26] Z.-H. Zhan, J. Zhang, Y. Li, and H.-H. Chung, "Adaptive particle swarm optimization," IEEE Trans. Syst., Man, Cybern., B, Cybern., vol. 39, no. 6, pp. 1362­1381, Dec. 2009. [27] J. Liang and P. Suganthan, "Dynamic multi-swarm particle swarm optimizer," in Proc. IEEE Swarm Intell. Symp., Pasadena, CA, USA, 2005, pp. 124­129. [28] D. Cohen, S. Dalal, M. Fredman, and G. Patton, "The AETG system: An approach to testing based on combinatorial design," IEEE Trans. Softw. Eng., vol. 23, no. 7, pp. 437­444, Jul. 1997. [29] R. C. Bryce and C. J. Colbourn, "One-test-at-a-time heuristic search for interaction test suites," in Proc. 9th Annu. Conf. Genet. Evol. Comput., London, U.K., 2007, pp. 1082­1089. [30] J. Kennedy and R. Eberhart, "Particle swarm optimization," in Proc. Int. Conf. Neural Netw., vol. 4. Perth, WA, Australia, 1995, pp. 1942­1948. [31] R. Eberhart and Y. Shi, "Particle swarm optimization: Developments, applications and resources," in Proc. Congr. Evol. Comput., vol. 1. Seoul, Korea, 2001, pp. 81­86. [32] R. Poli, J. Kennedy, and T. Blackwell, "Particle swarm optimization," Swarm Intell., vol. 1, no. 1, pp. 33­57, 2007. [33] S. Helwig, J. Branke, and S. Mostaghim, "Experimental analysis of bound handling techniques in particle swarm optimization," IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 259­271, Apr. 2013. [34] A. Windisch, S. Wappler, and J. Wegener, "Applying particle swarm optimization to software testing," in Proc. 9th Annu. Conf. Genet. Evol. Comput., London, U.K., 2007, pp. 1121­1128. [35] A. Ganjali, "A requirements-based partition testing framework using particle swarm optimization technique," M.S. thesis, Dept. Electr. Comput. Eng., Univ. Waterloo, Waterloo, ON, Canada, 2008. [36] Y.-J. Gong et al., "Optimizing the vehicle routing problem with time windows: A discrete particle swarm optimization approach," IEEE Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 42, no. 2, pp. 254­267, Mar. 2012. [37] W.-N. Chen et al., "Particle swarm optimization with an aging leader and challengers," IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 241­258, Apr. 2013. [38] C. J. Colbourn. (Apr. 2013). Covering Array Tables for t=2, 3, 4, 5, 6. [Online]. Available: http://www.public.asu.edu/ ccolbou/src/ tabby/catable.html [39] M. Corazza, G. Fasano, and R. Gusso, "Particle swarm optimization with non-smooth penalty reformulation, for a complex portfolio selection problem," Appl. Math. Comput., vol. 224, pp. 611­624, Nov. 2013.

Huayao Wu received the B.S degree from Southeast University, Nanjing, China and the M.S degree from Nanjing University, Nanjing, China, where he is currently working toward the Ph.D. degree from Nanjing University, Nanjing. His research interests include software testing, especially on combinatorial testing and search-based software testing.

Changhai Nie (M'12) received the B.S. and M.S. degrees in mathematics from Harbin Institute of Technology, Harbin, China, and the Ph.D. degree in computer science from Southeast University, Nanjing, China. He is a Professor of Software Engineering with State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing. His research interests include software analysis, testing and debugging.

Fei-Ching Kuo (M'06) received the B.Sc. (Hons.) degree in computer science and the Ph.D. degree in software engineering from Swinburne University of Technology, Hawthorn, VIC, Australia. She was a Lecturer with University of Wollongong, Wollongong, NSW, Australia. She is currently a Senior Lecturer with the Swinburne University of Technology. Her research interests include software analysis, testing, and debugging.

Hareton Leung (M'90) received the Ph.D. degree in computer science from University of Alberta, Edmonton, AB, Canada. He is an Associate Professor and a Director of the Laboratory for Software Development and Management, Department of Computing, Hong Kong Polytechnic University, Hong Kong. His research interests include software testing, project management, risk management, quality and process improvement, and software metrics.

Charles J. Colbourn received the Ph.D. degree from University of Toronto, Toronto, ON, Canada, in 1980. He is a Professor of Computer Science and Engineering with Arizona State University, Tempe, AZ, USA. He has authored the books The Combinatorics of Network Reliability (Oxford) and Triple Systems (Oxford), and also 320 refereed journal papers focussing on combinatorial designs and graphs with applications in networking, computing, and communications. Prof. Colbourn received the Euler Medal for Lifetime Research Achievement by the Institute for Combinatorics and its Applications in 2004.

ATLAS: Adaptive Topology- and Load-Aware Scheduling
Jonathan Lutz, Charles J. Colbourn, and Violet R. Syrotiuk CIDSE, Arizona State University, Tempe, AZ 85287-8809 Email: {jlutz, colbourn, syrotiuk}@asu.edu

arXiv:1305.4897v2 [cs.NI] 4 Nov 2013

Abstract--The largest strength of contention-based MAC protocols is simultaneously the largest weakness of their scheduled counterparts: the ability to adapt to changes in network conditions. For scheduling to be competitive in mobile wireless networks, continuous adaptation must be addressed. We propose ATLAS, an Adaptive Topology- and Load-Aware Scheduling protocol to address this problem. In ATLAS, each node employs a random schedule achieving its persistence, the fraction of time a node is permitted to transmit, that is computed in a topology and load dependent manner. A distributed auction (REACT) piggybacks offers and claims onto existing network traffic to compute a lexicographic max-min channel allocation. A node's persistence p is related to its allocation. Its schedule achieving p is updated where and when needed, without waiting for a frame boundary. We study how ATLAS adapts to controlled changes in topology and load. Our results show that ATLAS adapts to most network changes in less than 0.1s, with about 20% relative error, scaling with network size. We further study ATLAS in more dynamic networks showing that it keeps up with changes in topology and load sufficient for TCP to sustain multi-hop flows, a struggle in IEEE 802.11 networks. The stable performance of ATLAS supports the design of higher-layer services that inform, and are informed by, the underlying communication network. Index Terms--Wireless networks, medium access control, adaptation.

I. I NTRODUCTION Despite the well known shortcomings of IEEE 802.11 and other contention-based MAC protocols for mobile wireless networks--such as probabilistic delay guarantees, severe short-term unfairness, and poor performance at high load-- they remain the access method of choice. The primary reason is their ease in adapting to changes in network conditions, specifically to changes in topology and in load. The lack of timely adaptation is the most serious limitation facing scheduled MAC protocols. For scheduling to be competitive, continuous adaptation is required. Topology-dependent approaches to adaptation in scheduling alternate a contention phase with a scheduled phase. In the contention phase, nodes exchange topology information used to compute a conflict-free schedule that is followed in the subsequent scheduled phase (see, as examples, [5], [30]). However, changes in topology and load do not always align with the phases of the algorithm resulting in a schedule that often lags behind the network state. In contrast, the idea behind topology-transparent scheduling is to design schedules independent of the detailed network topology [3], [15]. Specifically, the schedules do not depend on the identity of a node's neighbours, but rather on how many

of them are transmitting. Even if a node's neighbours change, its schedule does not; if the number of neighbours does not exceed the designed bound then the schedule guarantees success. Though such schedules are robust to network conditions that deviate from the design parameters [27], because the schedules do not adapt, the technique remains a theoretical curiosity. In contention-based schemes, such as IEEE 802.11, a node computes implicitly when to access the channel, basing its decisions on perceived channel contention. We instead compute a node's persistence--the fraction of time it is permitted to transmit--explicitly in a way that tracks the current topology and load. To achieve this, we propose ATLAS, an Adaptive Topology- and Load-Aware Scheduling protocol. Channel allocation is a resource allocation problem where the demands correspond to transmitters, and the resources to receivers. ATLAS implements the REsource AlloCaTion computed by REACT, a distributed auction that runs continuously. REACT piggybacks offers and claims onto existing network traffic to compute the lexicographic max-min allocation to transmitters which we call the TLA allocation, emphasizing that it is both topology- and load-aware. Each node's random schedule, achieving a persistence informed by its allocation, is updated whenever a change in topology or load results in a change in allocation. While the slots of the schedule are grouped into frames, this is done only to reduce the variance in delay [6]; there is no need to wait for a frame boundary to update the schedule. Even though the random schedules may not be conflict-free, ATLAS is not contention-based; it does not select persistences or make scheduling decisions based on perceived channel contention--its decisions are based solely on topology and load. We study how ATLAS adapts to controlled changes in topology and load, measuring convergence time, relative error, and scalability. We also assess the ability of ATLAS to adapt in more dynamic network conditions. To the best of our knowledge, ATLAS is the first scheduled MAC protocol able to adapt to changes in topology and load that is competitive with contention-based protocols in throughput and delay while realizing superior delay variance. It achieves this through the continuous computation of the TLA allocation, and updating the schedule on-the-fly. These updates occur only where and when needed. By not requiring phases of execution and by computing persistences rather than conflict-free schedules, ATLAS eliminates the complexity of, and lag inherent in, topology-dependent approaches. By not being dependent on the identity of neighbours, ATLAS shares the best of topology-transparent schemes (and also their

Submitted to IEEE Transactions on Mobile Computing ­ c 2013 IEEE

2

potential for collisions) yet overcomes its weakness by being adaptive. By not forcing updates to be frame synchronized, ATLAS shares the critical features of continuous adaptation with contention-based protocols. As a result, ATLAS achieves predictable throughput and delay characteristics. Such characteristics and information about localized capacity at the MAC layer may be used to inform higher layers, while end-toend characteristics at higher layers may be used to inform ATLAS. This may support the development of an agile, higher performing protocol stack. The primary contributions of this paper are twofold: (1) The REACT algorithm, an asynchronous, adaptive, and distributed auction that solves a general resource allocation problem to produce the TLA allocation. (2) ATLAS, a MAC protocol that uses REACT to solve the specific problem of channel allocation in a wireless network where each node produces a random schedule with the number of transmission slots determined by its allocation. The sequel is organized as follows: Section II defines a general resource allocation problem and presents the REACT algorithm, proving its correctness. Section III expresses channel allocation as a resource allocation problem and defines ATLAS. Related work is described in Section IV. After describing the simulation set-up in Section V, Section VI studies how ATLAS adapts to controlled changes in topology and load, and to dynamic network conditions. In Section VII, we discuss open issues and potential applications of REACT, including the design of higher-layer services that inform, and are informed by, the underlying communication channel. II. D ISTRIBUTED R ESOURCE A LLOCATION -- REACT We consider a general resource allocation problem. Let R be a set of N resources with capacity c = (c1 , . . . , cN ). Let D be a set of M demands with magnitudes w = (w1 , . . . , wM ). Resource j  R is required by demands Dj  D. Demand i  D consumes capacity at all resources in Ri  R simultaneously. The resource allocation s = (s1 , . . . , sM ), si  0 defines the capacity reserved for the demands. Resource allocation s is feasible if iDj si  cj for all j  R and si  wi for all i  D. Demand i is satisfied if si  wi . Resource j is saturated if iDj si  cj . Throughout, capacity refers to the magnitude of a resource. Definition 1: [22] A feasible allocation s is lexicographically max-min if, for every demand i  D, either i is satisfied, or there exists a saturated resource j with i  Dj where si = max(sk : k  Dj ). We now describe REACT, a distributed auction that computes the lexicographic max-min allocation. In it, resources are represented by auctioneers and demands by bidders. Each auctioneer maintains an offer--the maximum capacity consumed by any adjacent bidder--and each bidder maintains a claim--the capacity the bidder intends to consume at adjacent auctions. The final claim of bidder i defines allocation si . Auctioneer j satisfies Def. 1 locally by increasing its offer in an attempt to become saturated while maintaining a feasible allocation. Bidder i satisfies Def. 1 locally for demand i by increasing its claim until it is satisfied or has a maximal claim

Algorithm 1 REACT Bidder for Demand i.
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: upon initialization Ri   wi  0 U PDATE C LAIM () end upon upon receiving a new demand magnitude wi U PDATE C LAIM () end upon upon receiving offer from auctioneer j offers[j ]  offer // Remember the offer of auctioneer j . U PDATE C LAIM () end upon upon bidder i joining auction j Ri  Ri  j // Resource j is now required by demand i. U PDATE C LAIM () end upon upon bidder i leaving auction j Ri  Ri \ j // Resource j is no longer required by demand i. U PDATE C LAIM () end upon procedure U PDATE C LAIM () // Select the claim to be no larger than the smallest offer or wi . claim  min ({offers[j ] : j  Ri }, wi ) send claim to all auctions in Ri end procedure

at an adjacent auction. Through continuous updates of offers and claims, the auctioneers and bidders eventually converge on the lexicographic max-min allocation. We give precise definitions of auction and bidder behaviour next. Bidder i knows wi and maintains set Ri . Offers are stored in offers[]; offers[j ] holds the offer last received from auctioneer j . Bidder i constrains its claim to be no larger than wi or the smallest offer from auctioneers in Ri , claim = min ({offers[j ] : j  Ri }, wi ) . (1)

Auctioneer j knows cj and maintains set Dj . Bidder claims are stored in claims[]; claims[i] holds the claim last received  from bidder i. Auctioneer j identifies set Dj  Dj containing bidders with claims strictly smaller than its offer,
 Dj = {b : b  Dj , claims[b] < offer}.

(2)

 Bidders in Dj are either satisfied or are constrained by another auction and cannot increase their claims in response to a larger  offer from auctioneer j . Bidders in Dj \ Dj are constrained by auction j . They may increase their claims in response to a  larger offer. Resources left unclaimed by bidders in Dj ,

Aj = cj -

 iDj

claims[i] ,

(3)

remain available to be offered in equal portions to bidders in  Dj \Dj . If claims of all bidders in Dj are smaller than the offer  (i.e., Dj = Dj ), there are no bidders to share the available resources in Aj . The auctioneer sets its offer to Aj plus the largest claim, ensuring that any bidder in Dj can increase its claim to consume resources in Aj : offer =
  Aj /|Dj \ Dj |, if Dj = Dj , (4) Aj + max (claims[i] : i  Dj ) , otherwise.

Alg. 1 and Alg. 2 describe actions taken by the bidders and auctioneers of REACT in response to externally triggered

3

Algorithm 2 REACT Auctioneer for Resource j .
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: upon initialization Dj   cj  0 U PDATE O FFER () end upon upon receiving a new capacity of cj U PDATE O FFER () end upon upon receiving claim from bidder i claims[i]  claim // Remember the claim of bidder i. U PDATE O FFER () end upon upon bidder i joining auction j Dj  Dj  i // Demand i now requires resource j . U PDATE O FFER () end upon upon bidder i leaving auction j Dj  Dj \ i // Demand i no longer requires resource j . U PDATE O FFER () end upon procedure U PDATE O FFER ()   Dj Aj  cj done  False while ( done = False ) do  contains all bidders in D , then auction j does not // If Dj j // constrain any of the bidders in Dj .  = D ) then if ( Dj j done  True offer  Aj + max ({claims[i] : i  Dj }) // Otherwise, auction j constrains at least one bidder in Dj . else done  True // What remains available is offered in equal portions to the // bidders constrained by auction j . | offer  Aj /|Dj \ Dj  and compute A for the new offer. // Construct Dj j  } do for all b  {Dj \ Dj if ( claims[b] < offer ) then   D  b Dj j Aj  Aj - claims[b] done  False send offer to all bidders in Dj end procedure

events. Collectively, auctioneers and bidders know the inputs to the allocation problem and bidder claims converge on the lexicographic max-min allocation; the claim of bidder i converges on si . The correctness of Alg. 1 and Alg. 2 is established in two steps: Lemma 1 establishes forward progress on the number of auctioneers to have converged on their final offer. Theorem 1 employs Lemma 1 to show eventual convergence to the lexicographic max-min allocation. Let claimi denote the claim of bidder i and offerj the offer of auctioneer j . Assume that the resource allocation remains constant for the period of analysis, that bidder i knows Ri and wi , and that auctioneer j knows Dj and cj . Further assume communication between adjacent auctioneers and bidders is not delayed indefinitely. A claim or offer is stable if it has converged on its final value. Denote by Astable the set of auctioneers whose offers are stable and remain the smallest among all offers. Lemma 1: Suppose Astable contains k auctioneers, 0  k < N . Then, within finite time, at least one auctioneer converges

on the next smallest offer omin . Offers equal to omin are stable and remain smaller than all other offers not in Astable . Proof: Wait sufficient time for every bidder i to send a new claim to auctioneers in Ri and for every auctioneer j to send a new offer to bidders in Dj . Let omin be the smallest offer of an auctioneer not in Astable . Assume to the contrary that offerx for some x  / Astable is the first to become smaller than omin . By Eq. 2 and 4, a decrease to offerx can only occur after a bidder y at auction x with claimy < offerx increases its claim. By Eq. 1, claimy can increase only after its limiting constraint starts out smaller than offerx and increases. Constraints in the system smaller than offerx are maximum claims, offers from Astable , and offers equal to omin . Maximum claims and offers from Astable do not change, leaving some x with offerx = omin as the only potential limiting constraint for claimy . By Eq. 2 and 4, offerx can increase only after one of its bidders y reduces its claim to be smaller than offerx . By Eq. 1, claimy can get smaller only after one of its auctioneers, say x , reduces its offer to be offerx < omin = offerx contradicting the assumption that offerx is the first to become smaller than omin . Therefore, offers equal to omin remain smaller than offers not from Astable . By Eq. 2 and 4, any j offering omin can change only after a bidder i at auction j with claimi  omin changes. By Eq. 1, claimi only changes if its limiting constraint changes. Potential limiting constraints include wi , offers from Astable , and offers equal to omin . These constraints are stable; therefore, offers equal to omin are stable. Theorem 1: Bidders and auctioneers of Alg. 1 and Alg. 2 compute the lexicographic max-min allocation. Proof: We apply Lemma 1 to show by induction that every auctioneer eventually computes a stable offer. Base Case: Consider an allocation problem with arbitrary wi , cj , Ri , and Dj for 1  i  M , 1  j  N . Let |Astable | = 0. By Lemma 1, at least one auctioneer eventually converges on a smallest offer omin . Offers equal to omin are stable and remain smallest among all offers. Add auctioneers offering omin to Astable ; |Astable |  1. Inductive Step: Let |Astable | = k , 1  k < N . Then, by Lemma 1 a non-empty set of auctioneers A+ with A+  Astable =  eventually converge on the next smallest offer. Offers from A+ remain smaller than offers not from A+ or Astable and are stable. Add A+ to Astable ; |Astable |  k + 1. By induction, all auctioneers are eventually added to Astable . Wait for auctioneers to send their offers to adjacent bidders. Bidder claims are now stable. By Eq. 1, bidder i is either satisfied with its claim (claimi = wi ) or its claim is maximal at an auction in Ri . By Definition 1, the claims are lexicographic max-min. III. T HE ATLAS MAC P ROTOCOL Channel allocation in wireless networks can be expressed as a resource allocation problem. In this context, transmitters correspond to the demands in D and receivers to the resources in R. Label transmitters {1, . . . , M } and receivers {1, . . . , N }. A transmitter with a non-zero demand magnitude is active. Receiver j is in Ri if it is within transmission range of

4

Slot x Node A Node B Node C
Header Fields pkt #1 collision pkt #2 claim offer

Slot x + 1

Slot x + 2
pkt #1 pkt #1 pkt #1 ack #1 ack #1 ack #1

Slot x + 3
pkt #2 pkt #2 pkt #2 ack #2 ack #2 ack #2
Node # 1 w1 = 0.45 s1 = 0.25 s 1 = 0.20

Node # 7 w7 = 0.30 s7 = 0.30 s 7 = 0.20

MAC Payload

Header Fields

ACK Fields

MAC Header

MAC Header

Node # 3 w3 = 0.50 s3 = 0.25 s 3 = 0.20 Node # 4 w4 = 0.40 s4 = 0.25 s 4 = 0.20 Bidirectional Link New Bidirectional Link

claim offer

Node # 5 w5 = 0.75 s5 = 0.45 s 5 = 0.55

Fig. 1. Example transmissions in ATLAS of two packets in a network of three fully connected nodes. The first packet is sent from node A to node C . The second packet is sent from node C to node B . Transmissions are coloured white and receptions are shaded grey. The frame structure is shown for a data packet and an acknowledgement.

Node # 2 w2 = 0.55 s2 = 0.25 s 2 = 0.20

Node # 6 w6 = 0.05 s6 = 0.05 s 6 = 0.05

transmitter i and transmitter i is active. Dj contains the active transmitters for which receiver j is within transmission range. Receiver j is adjacent to transmitter i if j  Ri and i  Dj . The sets Dj and Ri capture the network topology for active transmitters. For load, wi is set to the percentage of slots required to support the demand at transmitter i. Transmitters with no demand (i.e., wi = 0) receive an allocation of zero slots: they are not active. Receiver capacities are set to one, targeting 100% channel allocation. The lexicographic max-min solution s = (s1 , . . . , sM ) for a given topology and traffic load is the TLA allocation. To apply REACT to channel allocation, we integrate it into ATLAS, a simple random scheduled MAC protocol. Although REACT could instead augment contention-based schemes, we choose to work within a scheduled environment, a traditionally difficult setting for adaptation. In ATLAS, each node runs a REACT bidder (Alg. 1) and a REACT auctioneer (Alg. 2) continuously. Auctioneers and bidders discover each other as they hear from one another and rely on the host node to detect lost adjacencies. The network topology is implicit in the sets Ri and Dj . Each node updates its bidder's demand magnitude to accurately reflect its traffic load. Offers and claims are encoded using eight bits each and are embedded within the MAC header of all transmissions to be piggybacked on existing network traffic. The encoding supports a total of 256 values for offers, claims, and persistences uniformly distributed between 0 and 1; the error in the representation does not exceed 0.004. Adding fields for an offer and claim to data packets and acknowledgements results in a communication overhead of four bytes per packet. For the slot size and data rate simulated in Section VI, the overhead is 0.36%. A node's offer and claim are eventually received by all single-hop neighbours reaching the bidders and auctioneers that need to know the offer and claim. In time, the bidder claims in REACT converge on the TLA allocation s. Packets are acknowledged within the slot they are transmitted and slots are sized accordingly. Unacknowledged MAC packets are retransmitted up to ten times before they are dropped by the sender. Fig. 1 shows that collisions are possible in ATLAS, and that successful transmissions are acknowledged in the same slot. The transmissions collide in slot x; they are repeated (successfully) in slots x + 2 and x + 3. Fig. 1 also shows the frame structure. The TLA allocation can be interpreted directly as a set of

Fig. 2. Example network showing the TLA allocation computed by REACT before and after an added link in the topology. wi identifies a node's demand, si its initial TLA allocation, and s i its TLA allocation after the added link. Resource capacities are set to one. Double-lined circles identify nodes with saturated resources.

persistences in a p-persistent MAC [28]. However, we achieve lower variation in delay by introducing the notion of a frame [6]. Specifically, ATLAS divides time into slots which are organized into frames of v slots. Node i operates at persistence pi = si . At the start of every frame and upon any change to pi , node i computes ki = pi v + 1 with probability i and ki = pi v with probability 1 - i where i = pi v - pi v . Node i constructs a transmission schedule of ki slots selected uniformly at random. Over many frames, E [ki ]/v equals pi where E [ki ] is the expectation for ki . Fig. 2 shows the TLA allocation in a small example network before and after a change in topology. Node 7 starts out disconnected from the other nodes and moves within range of node 3. In REACT, node 3 starts out offering 0.25 which is claimed by the bidders of nodes 1, 2, 3, and 4. With the claims of node 3 and 4 limited by the offer of node 3 and the claim of node 6 limited by its demand, the auctioneer at node 4 is free to offer 0.45, which is claimed by node 5. Upon detecting node 7 as a neighbour, the auctioneer at node 3 decreases its offer to 0.20. The bidders at nodes 1, 2, 3, 4, and 7 respond by reducing their claims accordingly. The smaller claims of the bidders at nodes 3 and 4 allow the auctioneer at node 4 to increase its offer to 0.55. The bidder at node 5 responds by increasing its claim to 0.55. It can be verified that, before and after the topology change, the claims of the bidders (i.e., the values of si and s i ) are lexicographically max-min; that is, every claim is satisfied or is maximal at an adjacent auction. Consider the topology with node 3 and node 7 connected. The bidder at node 6 is satisfied. The bidders at nodes 1, 2, 3, 4, and 7 are maximal at the auction of node 3. The bidder at node 5 is maximal at the auction of node 4. There are many implementation choices to be made in applying REACT to channel allocation. We identify three binary choices--lazy or eager persistences, physical layer or MAC layer receivers, and weighted or non-weighted bidders--and three configurable parameters--pmin , pdefault , and tlostNbr . The choices are described here; they are evaluated in Section VI.

5

A. Lazy or Eager Persistences A lazy approach sets persistence pi equal to the claim of bidder i. Once converged, pi matches the TLA allocation interpreted as a persistence. There is a potential disadvantage with being lazy. For many applications, nodes cannot predict future demand for the channel; they can only estimate demand based on past events, i.e., packet arrival rate or queue depth. As a consequence, wi lags the true magnitude of the demand at node i. If wi is the limiting constraint for the claim of bidder i, pi can be sluggish in response to increases in demand. Alternatively, an eager approach sets persistence pi = min (offers[j ] : j  Ri ), breaking the direct dependence on wi . Under stable conditions, a node's channel occupancy, the fraction of time it spends transmitting, matches its TLA allocation; its occupancy is limited by the availability of packets to transmit which is no larger than wi , even when pi > wi . By allowing pi > wi , the persistence is made more responsive to sudden increases in demand. B. Physical Layer or MAC Layer Receivers A central objective of the TLA allocation is to ensure that no receiver is overrun. In a wireless network, receivers can be defined in terms of physical layer or MAC layer communication. At the physical layer, every node is a receiver. At the MAC layer, packets are filtered by destination address; a node is only a receiver if one of its neighbours has MAC packets destined to it. MAC layer receivers can increase channel allocation by over-allocating at non-receiving nodes. However, the overallocation can slow detection of new receivers. Physical receivers prevent overallocation at any receiver, making the allocation more responsive to changes in traffic where nodes become receivers. C. Weighted or Non-Weighted Bidders We have described a MAC protocol where transmitters are represented by equally weighted bidders. For applications requiring multiple demands per transmitter, i.e., nodes servicing more than one traffic flow, we propose the weighted TLA allocation. The demands of weighted bidders are comprised of one or more demand fragments; the number of fragments accumulated into a demand is the demand's weight. Let i be the weight for demand i. Demand fragments in demand i have magnitude wi /i . The weighted TLA allocation defines the lexicographically max-min vector u = (u1 , . . . , uN ) where ui is the allocation to each demand fragment in demand i for a total allocation of ui i to demand i. REACT can be extended to compute the weighted TLA allocation. To do this, each bidder must inform adjacent auctions of its weight. Sixteen unique weights (with a four-bit representation) may be sufficient for many applications. D. Minimum Persistence pmin A node can maintain a persistence of zero without impacting the communication requirements of its bidder. For auctioneers, a persistence of zero is problematic. If a receiver becomes

overwhelmed by neighbouring transmitters, a non-zero persistence is needed to quiet the neighbours. To accomplish this, the node enforces a minimum persistence pmin , creating dummy packets if necessary, whenever the sum of claims from adjacent bidders exceeds the auction capacity. E. Overriding the TLA Allocation with pdefault There are two conditions where a node constrains its persistence to be no larger than pdefault . The first is when it has no neighbours. While the TLA allocation permits an isolated node to consume 100% of the channel, it cannot discover new neighbours if it does so. The second time a node employs pdefault is for a short period after the discovery of a new neighbour. It is possible for several nodes operating with large persistences to join a neighbourhood at about the same time. If the persistences are large enough, neighbour discovery can be hindered. For both scenarios, limiting the persistence to pdefault facilitates efficient neighbour discovery. F. Adaptation to Topology Changes and tlostNbr Changes in network topology are detected externally to REACT. In ATLAS, neighbour discovery is performed independently by each node. If a node hears from a new neighbour, then the node notifies its bidder of the new auction and its auctioneer of the new bidder. Conversely, if a node has not heard from a neighbour in more than tlostNbr seconds, it presumes the node is no longer a neighbour and informs its auctioneer and bidder accordingly. IV. R ELATED W ORK This paper focuses on the TLA allocation, its continuous distributed computation, and its application to setting transmitter persistences. In this section, we review a representative set of scheduled MAC protocols, observing how each selects a node's persistence and adapts to topology and load. Any finite schedule used in a cyclically repeated way can be generalized as a (k, v )-schedule with k transmission slots per frame of v slots, producing an effective persistence of p = k/v . Examples include the random schedules of [6], [18] where each node selects its k transmission slots randomly from the set of v slots in the frame. Topology transparent schemes [3], [15], [27] also implement (k, v )-schedules. These schedules rely on only two design parameters: N , the number of nodes in the network, and Dmax , the maximum supported neighbourhood size. These schedules guarantee each node a collision-free transmission opportunity from each of its neighbours at least once per frame, provided the node's neighbourhood size does not exceed Dmax . (k, v )-schedules do not adapt to variations in neighbourhood size or traffic load. The combinatorial requirements for variable-weight topology transparent schedules (variable k ) are explored in [19], but no construction nor protocol using them is given. A class of topology-dependent scheduled protocols compute distance-2 vertex colourings of the network graph to achieve TDMA schedules with spatial reuse. The colourings assign one transmission slot to each node and do not adapt to

6

TABLE I ATLAS CONFIGURATIONS SELECTED FOR SIMULATION . Configuration Name Nominal Lazy Persistences Physical Receivers Weighted Bidders Eager (0) or Lazy (1) 0 1 0 0 MAC (0) or Physical (1) 0 0 1 0 Unweighted (0) or Weighted (1) 0 0 0 1

A. Scenario Details Unless otherwise noted, all four configurations run with pdefault = 0.05, tlostNbr = 0.5s, and pmin = 0.01. The selection of pdefault and tlostNbr are justified by results in Figs. 5, 11a, and 11b. The selection of pmin is based on [20]. Frames contain v = 100 slots of length 800µs (1100 bytes per slot). Simulations are run using the ns-2 simulator [21]. Each wireless node is equipped with a single half-duplex transceiver and omni-directional antenna whose physical properties match those of the 914 MHz Lucent WaveLAN DSS radio. The data rate for all simulations is 11 Mbps. The transmission and carrier sense ranges are 250m. Each simulation runs a network scenario composed of a randomly generated topology and a randomly generated traffic load. Unless specified otherwise, topologies contain 50 randomly placed nodes constrained to a 300 × 1500m2 area. With the exception of the multi-hop TCP flows in Section VI-F, each traffic load consists of single-hop constant rate traffic. Four traffic loads are simulated: 20% and 80% of nodes loaded with small demands (75 ± 50 pkts/s), 20% and 80% of nodes loaded with large demands (500 ± 50 pkts/s). Nodes loaded with traffic are selected at random and the demand magnitudes are selected uniformly at random from the specified range. The packet destination is selected dynamically from the set of neighbouring nodes as the packet is passed down to the MAC layer. For the Weighted Bidders configuration, each demand is assigned a random integer weight between one and five. Traffic is generated by constant bit rate generators and transported over UDP; packets are 900 bytes in length, leaving room in each slot for header bytes and a MAC layer acknowledgement. Combined with the random placement of nodes and the addition of mobility, these four traffic loads enable simulation of a wide variety of network conditions. B. Relative Error A metric of interest is the average relative error for a node's persistence with respect to the TLA allocation. Error is reported in two parts: relative excess and deficit persistence error. Errors are measured per node over 80ms consecutive intervals in time (equal to the length of one MAC frame). We compute the average relative excess error and average relative deficit error for a given sample set of persistence measurements. The relative errors are ratios, requiring use of the geometric rather than arithmetic mean. But, the errors are often zero, preventing direct use of their mean. Instead, we convert errors into accuracies eliminating zeros from the data set for a more meaningful geometric average. The average relative accuracies are converted back to relative errors. VI. E VALUATION OF ATLAS Results from [20] show the TLA allocation applied in a static network to maintain expected delay and throughput compared to IEEE 802.11, while reducing the variance for both metrics. The TLA allocation nearly eliminates packets dropped by the MAC layer. In this section, we build on these results, focusing on the efficient distributed computation of the

traffic load. One of the first distributed protocols to bound the number of colours is proposed in [5]. Distributed-RAND (DRAND) [25] is a distributed implementation of RAND (a centralized algorithm for distance-2 colouring [23]). DRAND runs a series of loosely synchronized rounds. A colour is assigned in each round to one or more nodes in different two-hop neighbourhoods. DRAND is employed by ZebraMAC (Z-MAC) [24] to compute schedules over which to run CSMA/CA. Nodes are given priority access to their own slot, but also allowed to contend for access in other unused slots, as is done in [4]. Due to the complexity of DRAND, schedules are only computed once during network initialization. Other topology-dependent schemes support variable persistences. The periodic slot chains proposed in [14] are not limited to the structure of a fixed length frame and can support variable and arbitrarily precise persistences. A slot chain is defined by its starting transmission slot and period between its consecutive transmission slots. By combining multiple slot chains with different periods, schedules are constructed targeting any rational persistence in the range [0, 1]. The computation of slot chains provided in [14] is centralized; a distributed mechanism to adaptively compute the slot chains remains an open problem. In [30], a five phase reservation protocol (FPRP) computes conflict-free schedules where a node can reserve one or more transmission slots in the frame to achieve variable persistences. Reservation frames are run periodically rather than on a demand basis and, therefore, may not accommodate the current topology and traffic load. In SEEDEX [26], nodes do not attempt to derive conflictfree schedules. They learn the identities of their two-hop neighbours and adjust transmission probabilities (i.e., persistences) to improve the likelihood of collision-free transmissions. The transmission probabilities accommodate the number and identity of neighbours, but not traffic load. In our earlier work [20], a distributed algorithm for computing the TLA allocation is provided; however, the algorithm assumes a fixed topology and does not adapt to changes in the network. REACT solves these limitations by asynchronously adapting to changes in both topology and traffic demand. V. S IMULATION S ET- UP We now describe the simulations used to produce the experimental results presented in Section VI. Table I lists the four ATLAS configurations simulated. The Nominal configuration employs eager persistences, defines receivers in terms of MAC layer communication, and operates with unweighted bidders. The other three configurations differ from the Nominal case by a single choice and are named accordingly.

7

Fig. 3.

Convergence time following network initialization.

simulations of 1000 network scenarios, 250 of each traffic load. The scenarios are simulated eight times each, once per default persistence: 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, and 0.4. Small default persistences (pdefault  0.01) limit a node's ability to communicate during neighbour discovery, slowing convergence. Large default persistences (pdefault  0.3) permit nodes to transmit with large persistences before they discover their neighbours. In networks with 40 large demands, the large persistences can overwhelm the channel preventing neighbour discovery and delaying convergence. ATLAS is robust to the selection of pdefault with a suitable range of [0.05­0.2]. For the remaining simulations, pdefault is 0.05. B. Convergence after a Change in Demand

TLA allocation in the face of changes in topology and load. The results presented here work to answer four questions: 1) Can ATLAS converge quickly on the TLA allocation? 2) Can ATLAS scale to larger networks? 3) Can ATLAS keep up with changes in a mobile network? 4) Can ATLAS adapt to multi-hop traffic flows? The first question is addressed Sections VI-A, VI-B, and VI-C. The second is addressed in Section VI-D. The third and fourth questions are addressed in Sections VI-E and VI-F, respectively. Continuing the focus on adaptation, Section VI-G provides comparisons with several scheduled protocols. A. Convergence after Network Initialization Fig. 3 reports average convergence times for all four ATLAS configurations. Error bars denote the arithmetic standard deviation from the mean for each sample set. Convergence is measured from network initialization (time = 0) to the time ATLAS converges on the TLA allocation. Times are collected from simulations of 1000 network scenarios simulated four times each, once per configuration. There are 250 scenarios for each traffic load. The Physical Receivers configuration converges fastest in less than 0.4s on average for networks with 40 large demands and faster for other traffic loads. The extra step of detecting MAC receivers slows convergence. The Lazy Persistences configuration is the slowest with an average convergence time of 0.67s for networks with 40 large demands. The strict limit on persistences enforced by this configuration slows convergence compared to the others. Fig. 4a shows average excess and deficit relative persistence errors for all four configurations. The averages are computed for nodes with a non-zero TLA allocation and only during convergence. Nodes are observed to operate within approximately 20% of their TLA allocation regardless of configuration. Deficit errors are larger than excess errors reflecting a tendency to converge from below, rather than above, the TLA allocation. In Fig. 4b, each data point reflects the convergence time (xcoordinate) and total relative persistence error (y -coordinate) for one simulation of the nominal configuration. The data shows relative persistence error to be fairly consistent from network to network with a maximum observed error of 27%. Fig. 5 reports convergence time for the Nominal configuration while varying pdefault . Convergence is measured for

Fig. 6 reports convergence times and relative persistence errors for the Nominal configuration following a change to a single demand magnitude. Four types of demand are simulated: a new small demand, a new large demand, a removed small demand, and a removed large demand. New demands start with magnitude zero and change to 75 ± 50 pkts/s for small demands and to 500 ± 50 pkts/s for large demands. Removed small demands and removed large demands start at 75 ± 50 pkts/s and at 500 ± 50 pkts/s, respectively; both change to zero. The four demand change types are simulated under the four traffic loads. REACT is allowed to converge on the initial TLA allocation prior to the demand change. Convergence times and error measurements are taken from simulations of 4000 network scenarios, 250 for each of the 16 demand change and traffic load combinations. Fig. 6a reports convergence times measured from the time of the change to the time of convergence on the new TLA allocation. The largest convergence times of approximately 0.175s are found in networks loaded with 40 demands. The average convergence time for the other scenarios is 0.125s or smaller. Fig. 6b shows relative persistence errors measured during convergence at nodes whose TLA allocation are affected by the demand change. Persistences are observed to be within 10% of the TLA allocation. C. Convergence after a Change in Topology Fig. 7 reports convergence time and relative persistence error following two types of topology change: the creation of a link and the removal of a link between a pair of nodes. Simulations are run on 2000 network scenarios, 250 for each topology change type and traffic load combination. Networks that lose a link are simulated once per neighbour timeout tlostNbr of 0.5s, 2.0s and 5.0s. Network topologies are generated as follows. A first node is placed at a random location in the simulation area. For topologies gaining a link, a second node is placed just outside the transmission range of the first node with a trajectory toward the first node. For topologies losing a link, the second node is placed just inside the transmission range of the first node with a trajectory away from the first node. The remaining 48 nodes are placed at random locations in the simulation area. The distance travelled by the second node is constrained to avoid unintentional topology changes.

8

(a) Relative excess and deficit persistence errors. Fig. 4. Relative persistence error for ATLAS.

(b) Convergence time vs. error for the Nominal configuration.

Fig. 5.

Convergence times when run with varying default persistences.

convergence is reached in an average of 0.89s, a mere 40% increase compared to networks spanning 4.8 hops. The impressive convergence times, particularly those of networks spanning 12 or more hops, suggest that convergence happens locally, allowing distant neighbourhoods to converge in parallel. This local behaviour is captured in Fig. 9 which reports the average distance between a network change and a node whose bidder changes its claim in response. Distances are reported in hops. A node that changes its demand or gains/loses a neighbour has distance zero. Neighbours of this node have distance one, and so on. Range of impact is reported for the six types of change evaluated in Sections VI-B and VI-C. Each type of change is simulated in 1000 network scenarios, 250 of each traffic load. The range of impact is less than 1.75 hops on average. E. Performance with Node Mobility Section VI-C addresses the robustness of ATLAS to single topology changes. We now evaluate its performance in networks with continuous mobility which may not have the opportunity to converge on the TLA allocation. Fig. 10a reports persistence error for node speeds ranging from 0 m/s to 120 m/s with 200 scenarios simulated for each node speed, 50 of each traffic load. Node movements are generated using the steady-state mobility model generator of [13] with a pause time of zero. Simulations are run for 20s. As node speeds increase, so do deficit persistence errors. The larger deficit errors are an artifact of lost neighbour detection which is delayed by tlostNbr = 0.5s. As a result, nodes tend to think their neighbourhoods are more crowded than they are, a tendency that gets worse as node speeds increase. In terms of REACT, auctioneers and bidders unnecessarily constrain their offers and claims to accommodate lost neighbours. The deficit persistences translate to degraded throughput. Fig. 10b reports MAC throughput for the simulations of Fig. 10a. Even with node speeds of 120 m/s where a node travels its transmission range in 2.1s, throughput degrades modestly, decreasing by less than 20% compared to static networks. Fig. 11 shows that a large tlostNbr exacerbates deficit persistence error and further degrades throughput. Data is collected from 200 scenarios, 50 of each traffic load. Each scenario is simulated five times with neighbour timeouts ranging from

The expected convergence time following the addition of a new link is 0.025s. For tlostNbr =0.5s, convergence is reached in less than 0.13s on average. For tlostNbr =2.0s and tlostNbr =5.0s, the large convergence times are dominated by tlostNbr . Except for the simulations of Fig. 11, all others configure ATLAS with tlostNbr =0.5s. During convergence, nodes affected by the topology change are observed to operate within 4% of their TLA allocation on average. These numbers are striking. The small convergence times stem from a counterintuitive feature of the TLA allocation: the majority of topology changes do not affect the TLA allocation. A new link only has an effect if the link connects a bidder with an auction that lacks the capacity to support the bidder's claim. Even in heavily loaded networks, many auctions have spare capacity to support a new bidder. For these scenarios, convergence is instantaneous. D. Scalability to Large Networks We now turn to results demonstrating ATLAS's scalability. We simulate 10 network sizes with the x-dimension ranging from 600m (2.4 hops) to 6000m (24 hops) in 600m increments; the y -dimension is held constant at 300m. The number of nodes is selected to keep the average neighbourhood density constant across all network sizes. Fig. 8 reports convergence times for 4000 network scenarios, 100 of each traffic load and network size combination. The convergence of ATLAS in large networks is striking. In networks spanning 24 hops,

9

(a) Convergence time after a demand change. Fig. 6.

(b) Relative persistence error after a demand change.

Convergence time and relative persistence error during convergence following a single demand change.

(a) Convergence time after a topology change. Fig. 7.

(b) Relative persistence error after a topology change.

Convergence time and relative persistence error following a single topology change.

Fig. 8.

Convergence times as the width of the network grows.

0.1s to 15.0s. Node speeds are fixed at 30 m/s. Degraded performance is observed for large timeouts, tlostNbr  0.5s, but also for small timeouts, tlostNbr = 0.1s. In networks loaded with 10 large demands, tlostNbr = 0.1s causes nodes to falsely identify lost neighbours that must be rediscovered. The remaining simulations are run with tlostNbr = 0.5s. Fig. 12 reports packet delay for ATLAS and IEEE 802.11 for the 200 network scenarios of Fig. 10 with node speeds equal to 30 m/s. IEEE 802.11 is configured with a maximum packet retry count of seven for RTS, CTS, and ACKs and four for data packets [11], a mini-slot length of 20µs, and minimum and maximum contention window sizes of 32 and 1024 slots, respectively. Each point in the scatter plot reports the average packet delay (x-coordinate) and variation in packet delay (y coordinate) for a single node. The largest reported average delay is 0.047s for ATLAS and 0.058s for IEEE 802.11. The largest reported variation in delay for ATLAS is 0.0016s2 , just 3.6% of the 0.0444s2 reported for IEEE 802.11. This impressive reduction in delay variance is crucial to the support of TCP, which we evaluate next. F. Multi-hop TCP Flows To this point, we have used MAC layer traffic to simulate a diverse set of network scenarios. We now evaluate the performance of ATLAS using multi-hop TCP flows. To accommodate the dynamic nature of these flows, each node estimates its own demand by monitoring queue behaviour. Demand is estimated as the sum of two parts: wenqueue and wlevel . wenqueue

Fig. 9.

Average range of impact (in hops) for a demand or topology change.

10

(a) Relative persistence error. Fig. 10. Relative persistence error and total MAC throughput for varying levels of node mobility.

(b) Total MAC throughput.

(a) Relative persistence error. Fig. 11. Relative persistence error and total MAC throughput for varying neighbour timeouts.

(b) Total MAC throughput.

Fig. 12.

Delays for ATLAS and IEEE 802.11 with node speeds of 30 m/s.

is the percentage of channel required to keep up with the current enqueue packet rate, wenqueue = (packet enqueue rate) × (slot length). wlevel is the percentage of channel required to transmit all packets in the queue within 0.2s (i.e., 25 slots), wlevel = [(# packets in queue)/0.02s] × (slot length). To avoid cross-layer interactions between the MAC and routing protocols, Dijkstra's shortest path algorithm [28] using accurate knowledge of the global topology computes the next hop address for all packet transmissions. FTP agents emulate transfer of infinite size files to create flows with throughput limited only by the performance of the network. Transfers start at time zero and run for 20s. Nodes are statically placed at random locations in a 300 × 1500m2 simulation area. The source and destination nodes for each file transfer are selected

at random. Each FTP transfer is transported over TCP Reno configured for selective acknowledgements, the extensions of RFC 1323 [1], and 900 byte TCP segments. The return ACKs are not combined with each other or with other data packets. Consequently, the transmission of a single 40-byte TCP ACK consumes an entire transmission slot in ATLAS. The maximum congestion window size is 32 packets. Network scenarios are simulated for three traffic loads: networks with 2, 8, and 25 TCP flows. The number of replicates per traffic load are chosen so that 3000 TCP flows are simulated for each. Fifteen hundred scenarios are simulated with two TCP flows, 375 with eight TCP flows, and 120 with 25 TCP flows. We simulate TCP traffic on five MAC protocols: the four configurations of ATLAS and IEEE 802.11. The configurations of ATLAS use pdefault = 0.05, tlostNbr = 0.5s, and pmin = 0.01. IEEE 802.11 parameters match those described in Section VI-E. Each node dynamically sets its bidder weight to one or the number of outgoing TCP flows it services, whichever is larger. The 15 sub-plots in Fig. 13 show the percentage of flows (y -axis) achieving a minimum throughput (x-axis). The distinguishing characteristics of the three unweighted ATLAS configurations are seen in the throughput curves for networks with two flows. These networks are loaded lightly enough for the auctions at non-receiver nodes to make a difference in the allocation, improving throughput for 2- and 4-hop flows. These networks also demonstrate how the longer initial packet delays of the Lazy Persistences configuration increase round trip time

11

Networks with 2 Flows

Networks with 8 Flows

Networks with 25 Flows

All flows

y -axis shows % of TCP flows achieving minimum required throughput.

1-hop flows 2-hop flows 3-hop flows 4- and 5-hop flows

x-axis shows minimum required throughput for TCP flows in packets/second.
Fig. 13. Percent of TCP flows (y -axis) achieving a minimum throughput (x-axis). Plots in the left, center, and right columns report on flows from simulations of 2, 8, and 25 flows, respectively. The plots in the top row report on all flows, regardless of hop count. Plots in the second, third, and fourth rows report on 1-hop, 2-hop, and 3-hop flows, respectively. Plots in the fifth row report on 4- and 5-hop flows.

for 4- and 5-hop flows, preventing TCP from achieving its best throughput. The Weighted Bidders configuration performs well for multi-hop flows in networks with eight and 25 flows by allocating more to multi-hop flows at the expense of singlehop flows. Because one-hop flows tend to achieve higher throughput, the configuration maintains a tighter variation in flow throughputs as indicated by the steeper slope of the Weighted Bidders curve in the top right plot of Fig. 13. Regardless of configuration, ATLAS surpasses IEEE 802.11

in support of concurrent multi-hop flows. The interaction between the IEEE 802.11 back-off algorithm and TCP's congestion control is well known [9]. In testbed experiments, a single TCP flow with no competition has difficulty reaching a destination four hops away [16]. Our simulations corroborate these findings, as approximately 50% of the 4- and 5-hop flows report a throughput of zero. For networks with 25 demands, nearly 75% of 2-hop flows are non-functional; 3-, 4-, and 5hop flows are almost completely shut out. The throughput of

12

ATLAS is achieved in spite of channel wasted transmitting 40 byte TCP ACKs in their own slots.

VII. D ISCUSSION In this section we discuss open issues and suggest potential applications for REACT and ATLAS. A. Improved Reliable Transport TCP's congestion control algorithm is known to suffer cross-layer interactions with binary exponential back-off (BEB) employed by IEEE 802.11 [9]. BEB is short term unfair, allowing a single node to capture the channel at the expense of its neighbours [2], [10] causing high variation in packet delay and making it difficult for TCP to estimate roundtrip delay. Many modifications have been proposed to improve TCP performance over wireless networks [17]; common approaches are detection of packet loss (differentiating it from congestion) and improved estimation of round trip time. An alternative is to minimize packet loss and control variation in packet delay at the MAC layer. ATLAS demonstrates a remarkable control of variation in delay (Fig. 12) enabling TCP to reliably support 3-, 4-, and 5-hop flows over heavily loaded networks (Fig. 13). However, TCP throughput still degrades considerably as the number of hops grows. Potential areas for future work include the integration of ATLAS into a cross-layer solution for reliable transport over wireless networks and the use of REACT to inform TCP's congestion window size. B. Selection of Configurable Parameters ATLAS has three configurable parameters: pdefault , tlostNbr , and pmin . Based on our simulations, [0.01­0.2] is an acceptable range for pdefault (Fig. 5) and [0.1s­2s] is an acceptable range for tlostNbr (Figs. 11a, 11b). In [20], pmin =0.1s is found to be acceptable for a protocol that enforces pmin at all nodes and at all times. Because ATLAS employs pmin temporarily, and only when needed, it is less sensitive to the selection of pmin . Although results show ATLAS to be robust to parameter selection, tuning may be required in other scenarios or in a hardware implementation. C. Dynamic Selection of Auction Capacity ATLAS targets 100% channel allocation by setting auction capacities in REACT to one. Although simulation results show this to be an adequate choice, it is not clear whether performance can be improved by under- or over-allocating the channel. Indeed, optimal auction capacities (however optimal is defined) are dependent on network topology and quality of the communication channel. We leave a thorough analysis of auction capacity selection to future work, pointing out here that REACT adapts continuously, allowing auction capacities to be adjusted dynamically, if necessary. D. Potential Applications for REACT The weighted TLA allocation opens doors for several potential uses. In the simulations of Section VI-F, a bidder's weight is set according to the number of flows it services. It may be desirable to set weights according to queue levels,

G. Comparison with other Scheduled MAC Protocols Here, we compare the adaptation of ATLAS with several other scheduled protocols including DRAND, Z-MAC, FPRP, and SEEDEX. Although the first three compute conflict-free schedules, an NP-hard problem [7], a comparison highlights the agility of ATLAS. 1) Adaptation to Topology Changes: For the simulations of Section VI-E, the number of neighbour changes (i.e., gained or lost neighbours) per second experienced by a node is correlated to the node speed. When the nodes move at 30 m/s, each node is expected to gain, or lose, a neighbour 2.21 times per second; within 6.3s, the number of neighbour changes is expected to exceed the neighbourhood size. Based on the run times reported in [25, Fig. 10], we estimate DRAND to compute schedules for the networks in Section VI-E in approximately 4.9s (adjusting for data rate and a two-hop neighbourhood size of 27). In this time, the topology changes caused by nodes moving at 30 m/s are expected to invalidate the computed schedule. Z-MAC has the same limitation and, although it compensates by running CSMA/CA to resolve collisions, it does not benefit from its TDMA schedule when nodes are mobile. In [25], the run times reported for FPRP schedule generation are comparable to DRAND. For SEEDEX, nodes discover their two-hop neighbours using a fan-in/fan-out procedure described in [26]. However, a practical integration of the procedure into the MAC protocol is not described or evaluated, preventing a comparison of its agility with other MAC protocols. In contrast to the slow schedule computation times of DRAND, Z-MAC, and FPRP, ATLAS is shown to handle node speeds of up to 120 m/s with only moderate degradation to MAC throughput. 2) Adaptation to Changes in Traffic Load: The persistences achieved by DRAND and SEEDEX are dependent on topology alone; neither adapts to traffic load. Although Z-MAC adapts to load, it does so by deviating from its underlying schedule, which does not adapt. FPRP can adapt to load by scheduling a variable number of slots per node; this is done at the expense of both longer frame lengths and longer run times for schedule computation. In contrast, ATLAS adapts to traffic load, responding quickly enough to establish and maintain multi-hop TCP flows. 3) Continuous Adaptation: Common to the scheduled schemes mentioned here is the use of a distinct phase for schedule computation (or neighbour discovery for SEEDEX). The schedules must be updated in order for the MAC to adapt. Any fixed period between schedule updates must be selected a priori; it cannot be adjusted for variations in network mobility. If schedules are to be updated when needed, a mechanism is required to trigger the schedule update. This coordination, by itself, is a challenge in an ad hoc network. In contrast, ATLAS does not employ a schedule computation (or a neighbour discovery) phase and adapts continuously to changes in both topology and traffic load.

13

demand magnitudes, neighbourhood sizes, node betweenness [8], distance from a point of interest (i.e., an access point or a common sink), position in a multicast/broadcast tree, or path hop count. The key observation is that ATLAS maintains flexibility by allowing nodes to define bidder weights arbitrarily to suit the needs of the network. While computation of persistences is the primary motivation for this work, REACT is not limited to this purpose. Consider the Physical Receivers configuration with node demands set to one. The resulting allocation is independent of actions taken by the upper network layers and, therefore, can inform decisions made by those layers. It can serve as a measure of potential network congestion--small allocations are assigned in dense neighbourhoods containing many potentially active neighbours. The routing protocol can use the allocation to discover alternate routes around congestion. An intriguing application is the implementation of differentiated service at the MAC layer. IEEE 802.11e [12] enhances the distributed coordination function by implementing four access categories; an instance of the back-off algorithm is run per access category, each with its own queue. The probability of transmission of each access category is manipulated independently through selection of contention window size and inter-frame space. This permits higher priority traffic to capture the channel from lower priority traffic. Similar results can be achieved by four instances of REACT, each computing the allocation for a single access category. Prioritization is achieved through dynamic coordination of the four auction capacities at each node. A potential strategy sets the capacity for each access category equal to one minus the allocation to higher priority access categories. As a result, higher priority auctions are permitted to starve lower priority auctions of capacity, effectively distributing channel access to high priority traffic. Alternatively, auction capacities can be selected to ensure a minimum or maximum percentage of the channel is offered to an access category. A network can run multiple instances of REACT. For example, an instance of the Physical Receivers configuration with all demands set to one can be run concurrently with four instances configured to support differentiated service. Alternatively, multiple instances of REACT can be used to allocate more than one set of resources concurrently. E. Assumptions Made by ATLAS Two key assumptions are made by ATLAS in its computation of the TLA allocation using REACT: (1) The offers and claims received by a node are accurate. (2) The offers and claims of a node are eventually received by all neighbouring nodes. The first assumption is reasonable, provided received packets are checked for errors by the link layer. The second assumption is almost certainly invalid; asymmetric communication, interference beyond the range of transmission, and signal fading are common in wireless communication and can prevent the delivery of offers and claims. Under realistic conditions, REACT may not converge on the TLA allocation, risking overallocation of the channel. In practice, auctions can adjust their capacities to mitigate the over-allocation. Every node knows

the persistences of its neighbours (from bidder claims) and can compute the expectation for collisions on the channel. Significant deviations above this expectation can trigger the auction to lower its capacity. An evaluation in a testbed of real radios is necessary to understand the sensitivity to anomalies on the wireless channel and the effectiveness of adjusting auction capacities to accommodate channel conditions. The evaluation of ATLAS in Section VI assumes both slot and frame synchronization; ATLAS does not require either. The computation of the TLA allocation by REACT does not rely on a frame structure and the expected performance of the random schedules is not affected by loss of frame synchronization. Even without slot synchronization, REACT can compute the TLA allocation; however, loss of slot synchronization may reduce channel capacity by 50% (see Aloha vs. slotted Aloha in [28]). ATLAS can accommodate the lower channel capacity by reducing auction capacity. This technique may allow ATLAS to be run on commodity IEEE 802.11 hardware [29] that lacks native support for slot synchronization. This is a subject of our current research. F. Enhancing Existing MAC Protocols We have used REACT to compute persistences to be employed within ATLAS, a slotted MAC protocol. Alternatively, REACT can be run on top of the IEEE 802.11 MAC by embedding claims and offers in the headers of existing control and data messages. The TLA allocation can be used to inform the selection of contention window sizes, eliminating the need for (and negative side effects of) binary exponential back off. We are currently working to integrate REACT into IEEE 802.11. Another alternative (and more ambitious) approach is to implement TLA persistences in a topology-dependent MAC that computes conflict-free schedules. Only a few topologydependent schemes allow a node to reserve more than one slot in a frame (i.e., [14], [30]), and those do not define how many slots a node should reserve. The TLA allocation can establish a permissible number of slots to be reserved by each node, given the current topology and traffic load. VIII. C ONCLUSION We have proposed REACT, a distributed auction that converges continuously on the TLA allocation, adapting to changes in both topology and traffic load. The utility of REACT is demonstrated through integration into ATLAS which we simulate under a wide variety of network scenarios. The results presented suggest that REACT can effectively inform the selection of transmitter persistences, and that ATLAS can provide robust, reliable, and scalable services. The application of REACT is not restricted to the computation of transmitter persistences. It has the potential to inform routing and admission control decisions, to enable differentiation of service at the MAC layer, and even to allocate other node resources. In this context, the REACT algorithm provides a potential solution to the immediate challenge of medium access control, but also shows promise as a tool for use in network protocol design in general.

14

ACKNOWLEDGEMENT The authors appreciate the useful comments provided by the anonymous reviewers. R EFERENCES
[1] RFC 1323: TCP Extentions for High Performance, 1992. [2] V. Bharghavan, A. Demers, S. Shenker, and L. Zhang. MACAW: A medium access protocol for wireless LANs. In Proceedings of the ACM Conference on Communications Architectures, Protocols and Applications (SIGCOMM'94), pages 212­225, 1994. [3] I. Chlamtac and A. Farag´ o. Making transmission schedules immune to topology changes in multi-hop packet radio networks. IEEE/ACM Transactions on Networking, 2(1):23­29, 1994. [4] I. Chlamtac, A. Farag´ o, A. D. Myers, V. R. Syrotiuk, and G. Z´ aruba. ADAPT: A dynamically self-adjusting media access control protocol for ad hoc networks. In Proceedings of the IEEE Global Telecommunications Conference (GLOBECOM'99), pages 11­15, 1999. [5] I. Chlamtac and S. S. Pinter. Distributed nodes organization algorithm for channel access in a multihop dynamic radio network. IEEE Transactions on Computers, C-36(6):728­737, June 1987. [6] C. J. Colbourn and V. R. Syrotiuk. Scheduled persistence for medium access control in sensor networks. In Proceedings from the First IEEE International Conference on Mobile Ad hoc and Sensor Systems (MASS'04), pages 264­273, 2004. [7] S. Even, O. Goldreich, S. Moran, and P. Tong. On the NP-completeness of certain network testing problems. Networks, 14(1):1­24, 1984. [8] L. C. Freeman. A set of measures of centrality based on betweenness. Sociometry, 40(1):35­41, 1977. [9] M. Gerla, R. Bagrodia, L. Zhang, K. Tang, and L. Wang. TCP over wireless multi-hop protocols: Simulation and experiments. In Proceedings of the 1999 IEEE International Conference on Communication (ICC'99), pages 1089­1094, 1999. [10] J. Hastad, T. Leighton, and B. Rogoff. Analysis of backoff protocols for multiple access channels. In Proceedings of the 19th annual ACM Symposium on Theory of Computing (STOC'87), pages 740­744, 1987. [11] IEEE. IEEE 802.11, Wireless LAN medium access control (MAC) and physical layer (PHY) specifications, 1997. [12] IEEE. IEEE 802.11e, Enhancements: QoS, including packet bursting, 2007. [13] J. Boleng, N. Bauer, T. Camp, and W. Navidi. Random Waypoint Steady State Mobility Generator (mobgen-ss). http://toilers.mines.edu/. [14] G. Jakllari, M. Neufeld, and R. Ramanathan. A framework for frameless TDMA using slot chains. In Proceedings of the 9th IEEE International Conference on Mobile Ad hoc and Sensor Systems (MASS'12), 2012. [15] J. Ju and V. O. K. Li. An optimal topology-transparent scheduling method in multihop packet radio networks. IEEE/ACM Transactions on Networking, 6(3):298­305, 1998. [16] D. Koutsonikolas, J. Dyaberi, P. Garimella, S. Fahmy, and Y. C. Hu. On TCP throughput and window size in a multihop wireless network testbed. In Proceedings of the 2nd ACM International Workshop on Wireless network testbeds, experimental evaluation and characterization (WiNTECH'07), 2007. [17] K. Leung and V. O. K. Li. Transmission control protocol (TCP) in wireless networks: Issues, approaches, and challenges. IEEE Communications Surveys & Tutorials, 8:64­79, 2006. [18] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Apples and oranges: Comparing schedule- and contention-based medium access control. In Proceedings of the 13th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems (MSWiM'10), pages 319­326, 2010. [19] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Variable weight sequences for adaptive scheduled access in MANETs. In Proceedings of Sequences and their Applications (SETA'12), pages 53­64, 2012. [20] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Topological persistence for medium access control. IEEE Transactions on Mobile Computing, 12(8):1598­1612, 2013. [21] The Network Simulator ns-2. http://www.isi.edu/nsnam/ns/. [22] M. Pi´ oro and D. Medhi. Routing, Flow, and Capacity Design in Communication and Computer Networks. Elsevier Inc., 2004. [23] R. Ramanathan. A unified framework and algorithm for (T/F/C)DMA channel assignment in wireless networks. In Proceedings of the 16th Annual Joint Conference of the IEEE Computer and Communications Societies (INFOCOM'97), pages 900­907, 1997.

[24] I. Rhee, A. Warrier, M. Aia, J. Min, and M. L. Sichitiu. Z-MAC: A hybrid MAC for wireless sensor networks. IEEE Transactions on Networking, 16(3):511­524, 2008. [25] I. Rhee, A. Warrier, J. Min, and L. Xu. DRAND: Distributed randomized TDMA scheduling for wireless ad-hoc networks. IEEE Transactions on Mobile Computing, 8(10):1384­1396, 2009. [26] R. Rozovsky and P. R. Kumar. SEEDEX: A MAC protocol for ad hoc networks. In Proceedings of the 2nd ACM International Symposium on Mobile Ad Hoc Networking and Computing (MOBIHOC'01), pages 67­75, 2001. [27] V. R. Syrotiuk, C. J. Colbourn, and S. Yellamraju. Rateless forward error correction for topology-transparent scheduling. IEEE/ACM Transactions on Networking, 16(2):464­472, 2008. [28] A. S. Tanenbaum. Computer Networks. McGraw Hill, fourth edition, 2003. [29] I. Tinnirello, G. Bianchi, P. Gallo, D. Garlisi, F. Giuliano, and F. Gringoli. Wireless MAC processors: Programming MAC protocols on commodity hardware. In Proceedings of the 31st Annual Joint Conference of the IEEE Computer and Communications Societies (INFOCOM'12), pages 1269­1277, 2012. [30] C. Zhu and M. S. Corson. A five-phase reservation protocol (FPRP) for mobile ad hoc networks. Wireless Networks, 7(4):371­384, 2001.

Jonathan Lutz earned his B.S. in Electrical Engineering from Arizona State University, Tempe, Arizona, in 2000 and his M.S. in Computer Engineering from the University of Waterloo, Waterloo, Canada, in 2003. He is currently working on his Ph.D. in Computer Science at Arizona State University. His research interests include medium access control in mobile ad hoc networks.

Charles J. Colbourn earned his Ph.D. in 1980 from the University of Toronto, and is a Professor of Computer Science and Engineering at Arizona State University. He is the author of The Combinatorics of Network Reliability (Oxford), Triple Systems (Oxford), and 320 refereed journal papers focussing on combinatorial designs and graphs with applications in networking, computing, and communications. In 2004, he was awarded the Euler Medal for Lifetime Research Achievement by the Institute for Combinatorics and its Applications.

Violet R. Syrotiuk earned her Ph.D. in Computer Science from the University of Waterloo (Canada). She is an Associate Professor of Computer Science and Engineering at Arizona State University. Her research has been supported by grants from NSF, ONR, and DSTO, and contracts with LANL, Raytheon, General Dynamics, and ATC. She serves on the editorial boards of Computer Networks and Computer Communications, as well as on the technical program and organizing committees of several major conferences sponsored by ACM and IEEE.

1

Hierarchical Recovery in Compressive Sensing
Charles J. Colbourn, Daniel Horsley, and Violet R. Syrotiuk, Senior Member, IEEE
Abstract A combinatorial approach to compressive sensing based on a deterministic column replacement technique is proposed. Informally, it takes as input a pattern matrix and ingredient measurement matrices, and results in a larger measurement matrix by replacing elements of the pattern matrix with columns from the ingredient matrices. This hierarchical technique yields great flexibility in sparse signal recovery. Specifically, recovery for the resulting measurement matrix does not depend on any fixed algorithm but rather on the recovery scheme of each ingredient matrix. In this paper, we investigate certain trade-offs for signal recovery, considering the computational investment required. Coping with noise in signal recovery requires additional conditions, both on the pattern matrix and on the ingredient measurement matrices. Index Terms compressive sensing, hierarchical signal recovery, deterministic column replacement, hash families

arXiv:1403.1835v1 [cs.IT] 4 Mar 2014

I. I NTRODUCTION Nyquist's sampling theorem provides a sufficient condition for full recovery of a band-limited signal: sample the signal at a rate that is twice the band-limit. However, there are cases when full recovery may be achieved with a sub-Nyquist sampling rate. This occurs with signals that are sparse (or compressible) in some domain, such as those that arise in applications in sensing, imaging, and communications, and has given rise to the field of compressive sensing [2], [6] (also called compressive sampling). Consider the following framework for compressive sensing. An admissible signal of dimension n is a vector in Rn that is known a priori to be taken from a given set   Rn . A measurement matrix A is a matrix from Rm×n . Sampling a signal x  Rn corresponds to computing the product Ax = b. Once sampled, recovery involves determining the unique signal x   that satisfies Ax = b using only A and b. If  = Rn , recovery can be accomplished only if A has rank n, and hence m  n. However for more restrictive admissible sets , recovery may be accomplished when m < n. Given a measurement matrix A, an equivalence relation A is defined so that for signals x, y  Rn , we have x A y if and only if Ax = Ay. If for every equivalence class P under A , the set P   contains at most one signal then in principle recovery is possible. Because Ax = Ay ensures that A(x - y) = 0, this can be stated more simply: An equivalence class P of A can be represented as {x + y : y  N (A)} for any x  P , where N (A) is the null space of A, i.e., the set {x  Rn : Ax = 0}. Recoverability is therefore equivalent to requiring that, for every signal x  , there is no y  N (A) \{0} with x + y  . In order to make use of these observations, a reasonable a priori restriction on the signals to be sampled is identified, suitable measurement matrices with m  n are formed, and a reasonably efficient computational strategy for recovering the signal is provided. A signal is t-sparse if at most t of its n coordinates are nonzero. The recovery of t-sparse signals is the domain of compressive sensing. An admissible set of signals  has sparsity t when every signal in  is t-sparse. An admissible set of signals  is t-sparsifiable if there is a full rank matrix B  Rn×n for which {B x : x  } has sparsity t. We assume throughout that when the signals are sparsifiable, a change of basis B is applied so that the admissible signals have sparsity t. A measurement matrix has (0 , t)-recoverability when it permits exact recovery of all t-sparse signals. A basic problem is to design measurement matrices with (0 , t)-recoverability where m  n such that recovery can be accomplished efficiently. Suppose that measurement matrix A has (0 , t)-recoverability. Then in principle, given A and b, recovery of the signal x can be accomplished by solving the 0 -minimization problem min{||x||0 : Ax = b}. To do so the possible supports of signals from fewest nonzero entries to most are first listed. For each, reduce A to A and x to x by eliminating coordinates in the signal assumed to be zero. Examine the now overdetermined system A x = b. When equality holds, a solution is found; we are guaranteed to find one by considering all possible supports with at most t nonzero entries. Such an enumerative strategy is prohibitively time-consuming, examining as many as n t linear systems when the signal has sparsity t. Natarajan [27] showed that we cannot expect to find a substantially more efficient solution, because the problem is NP-hard. Instead of the 0 -minimization problem, Chen, Donoho, Huo, and Saunders [11], [18] suggest considering the 1 -minimization problem min{||x||1 : Ax = b}. While this can be solved using standard linear programming techniques, to be effective it is necessary that for each t-sparse signal x, the unique solution to min{||z||1 : Az = Ax} is x. This property is (1 , t)recoverability. A necessary and sufficient condition for (1 , t)-recoverability has been explored, beginning with Donoho and Huo [18] and subsequently in [19]­[21], [24], [30], [31], [33].
C. J. Colbourn and V. R. Syrotiuk are with the School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, U.S.A., 85287-8809, {colbourn,syrotiuk}@asu.edu D. Horsley is with the School of Mathematical Sciences, Monash University, Vic 3800, Australia, daniel.horsley@monash.edu

2

A measurement matrix A meets the (0 , t)-null space condition if and only if N (A) \ {0} contains no (2t)-sparse vector. For y  Rn and C  {1, . . . , n}, define y|C  Rn to be the vector such that (y|C ) = y if   C and (y|C ) = 0 otherwise. A measurement matrix A meets the (1 , t)-null space condition if and only if for every y  N (A) \ {0} and every C  {1, . . . , n} with |C | = t, ||y|C ||1 < 1 2 ||y||1 . Lemma 1: ([13], for example) Measurement matrix A  Rm×n has (0 , t)-recoverability if and only if A meets the (0 , t)null space condition. Lemma 2: ([33], for example) Measurement matrix A  Rm×n has (1 , t)-recoverability if and only if A meets the (1 , t)null space condition.

To establish (1 , t)-recoverability, and hence also (0 , t)-recoverability, Cand` es and Tao [7], [9] introduced the Restricted Isometry Property (RIP). For A  Rm×n , the dth RIP parameter of A, d (A), is the smallest  so that, for some constant R > 0, (1 -  )R(||x||2 )2  (||Ax||2 )2  (1 +  )R(||x||2 )2 , for all x with ||x||0  d. The dth RIP parameter is better when d (A) is smaller as the bounds are tighter. The RIP parameters have been employed extensively to establish (1 , t)-recoverability, particularly for randomly generated measurement matrices [8]­[10], but also for those generated using deterministic con structions [12], [17]. Commonly, 2t < 2 - 1 is required for (1 , t)-recoverability; see [7] for example. The property of (1 , t)-recoverability in the presence of noise has also been considered. Conditions on the RIP parameters are sufficient but in general not necessary for recoverability. Combinatorial approaches to compressive sensing are detailed in [3], [16], [22], [23], [25], [26], [32]. We pursue a different combinatorial approach here, using a deterministic column replacement technique based on hash families. The use of an heterogeneous hash family provides an explicit hierarchical construction of a large measurement matrix from a library of small ingredient matrices. Strengthening hash families provide a means to increase the level of sparsity supporte, allowing the ingredient matrices to be designed for lower sparsity than the larger measurement matrix produced. In this paper we show that the heterogeneity extends to signal recovery: it is interesting that the ingredient measurement matrices need not all employ the same recovery algorithm. This enables hierarchical recovery for the large measurement matrix; however, this can be computationally prohibitive. By restricting the hash family to be linear, recovery for the large measurement matrix can be achieved in sublinear time even when computationally intensive methods are used for each ingredient matrix. To be practical, recovery methods based on hash families must deal with noise in the signal effectively. Suitable restrictions on the hash family and on each ingredient matrix used in the hierarchical method are shown to be sufficient to permit recovery in the presence of noise. The rest of this paper is organized as follows. The results on homogeneous hash families in Section II demonstrate that a recovery scheme based on (0 , t)- or (1 , t)-recoverability can be `lifted' from the ingredient measurement matrices to the matrix resulting from column replacement. Section III considers a generalization of hash families to allow for ingredient matrices with other recovery algorithms, and the computational investment to recover the signal. Signal recovery without noise is considered first, and the conditions for a sublinear time recovery algorithm described. Section IV considers the recovery of almost-sparse signals to deal with noise in the signal. Finally, Section V draws relevant conclusions. II. H ASH FAMILIES
AND

C OMPRESSIVE S ENSING

A. Column Replacement and Hash Families for Compressive Sensing Let A  Rr×k , A = (aij ), be an ingredient matrix. Let P  {1, . . . , k }m×n , P = (pij ), be a pattern matrix. The columns of A are indexed by elements of P . For each row i of P , replace element pij with a copy of column pij of A. The result is an rm × n matrix B , the column replacement of A into P . Fig. 1 gives an example of column replacement.   a11 a12 a13 a11  a21 a22 a23 a21  1231 a11 a12 a13  B= P = A=  a13 a11 a12 a11  a21 a22 a23 3121 a23 a21 a22 a21
B is the column replacement of A into P .

Fig. 1.

When the ingredient matrix A is a measurement matrix that meets one of the null space conditions for a given sparsity, our interest is to ensure that the sparsity supported by B is at least that of A. Not every pattern matrix P suffices for this purpose. Therefore, we examine the requirements on P . Let m, n, and k be positive integers. An hash family HF(m; n, k ), P = (pij ), is an m × n array, in which each cell contains one symbol from a set of k symbols. An hash family is perfect of strength t, denoted PHF(m; n, k, t), if in every m × t

3

subarray of P at least one row consists of distinct symbols; see [1], [28]. Fig. 2 gives an example of a perfect hash family PHF(6; 12, 3, 3). For example, for the 6 × 3 subarray involving columns 4, 5, and 6, only the fourth row consists of distinct symbols.  2 0 2 1 1 2  1 2 2 2 2 1  2 2 2 0 1 1


Fig. 2. A perfect hash family PHF(6; 12, 3, 3).

0 0 1 2 2 2

1 2 0 0 0 0

2 1 0 1 2 1

2 2 1 2 0 2

0 1 1 0 2 2

1 0 2 1 2 0

1 1 1 1 1 1

0 2 0 2 1 2

0 1 2 1 0 1

A perfect hash family has at least one row that separates the t columns into t parts in every m × t subarray. A weaker condition separates the t columns into classes. A {w1 , . . . , ws }-separating hash family, denoted SHF(m; n, k, {w1 , . . . , ws }), s with t = i=1 wi , is an m × n array on k symbols in which for every m × t subarray, and every way to partition the t columns into classes of sizes w1 , . . . , ws , there is at least one row in which no two classes contain the same symbol; see [4], [29]. A W -separating hash family, denoted SHF(m; n, k, W ), is a {w1 , . . . , ws }-separating hash family for each {w1 , . . . , ws }  W . Fig. 3 gives an example of a {1, 2}-separating hash family SHF(3; 16, 4, {1, 2}). For the 3 × 3 subarray consisting of columns 11, 15, and 16, for example, the last row separates columns {11, 16} from column {15}. 1 1 1 1 2 2 1 3 3 1 4 4 2 1 2 2 2 1 2 3 4 2 4 3 3 1 3 3 2 4  3 3 1 3 4 2 4 1 4 4 2 3  4 3 2  4 4 1


Fig. 3.

A {1, 2}-separating hash family SHF(3; 16, 4, {1, 2}).
s

A distributing hash family DHF(m; n, k, t, s) is an SHF(m; n, k, W ) with W = {{w1 , . . . , ws } : t = i=1 wi }. Fig. 4 gives an example of a DHF(10; 13, 9, 5, 2). For the 10 × 5 subarray consisting of columns 8 through 12, row 4 separates columns {8, 9, 10, 11} from column {12} (a {1, 4}-separation), and row 5 separates columns {8, 9, 12} from columns {10, 11} (a {2, 3}-separation). 6 3 8 0 0 1 1 1 0 0 7 1 5 2 0 1 0 1 0 0 8 1 1 0 2 2 1 0 3 0 3 7 4 2 1 2 2 1 0 0 4 2 2 2 1 2 0 0 1 0 0 6 3 0 1 0 0 4 0 1 2 8 2 0 2 1 2 2 0 0  2 4 6 1 0 0 0 0 2 0  3 3 7 1 0 0 0 2 4 1  0 0 0 1 2 2 1 0 0 0  5 2 1 1 2 1 2 1 0 0  1 0 3 2 0 0 2 0 1 0 1 5 0 0 1 0 1 2 0 1

a {1, 4}-separation  a {2, 3}-separation 

Fig. 4.

A distributing hash family DHF(10; 13, 9, 5, 2).

Now, we are in a position to state the requirements on a pattern matrix P that ensure that the sparsity supported by the matrix B resulting from column replacement is at least that of A. Theorem 1: [13] Suppose that A is an r × k measurement matrix that meets the (0 , t)-null space condition, that P is an SHF(m; n, k, {1, t}), and that B is the column replacement of A into P . Then B is an rm × n measurement matrix that meets the (0 , t)-null space condition.

Theorem 2: [13] Suppose that A is an r × k measurement matrix that meets the (1 , t)-null space condition, that P is a DHF(m; n, k, t + 1, 2), and that B is the column replacement of A into P . Then B is an rm × n measurement matrix that meets the (1 , t)-null space condition.

4

B. Exploiting Heterogeneity in Column Replacement All the standard definitions of hash families may be generalized by replacing k by k = (k1 , . . . , km ), a tuple of positive integers. Now, an heterogeneous hash family HF(m; n, k), P = (pij ), is an m × n array in which each cell from row i contains one symbol from a set of ki symbols, 1  i  m. Column replacement may be extended to exploit heterogeneity in an hash family. Let P = (pij ) be an HF(m; n, k) and, for 1  i  m, let Ai be an ri × ki ingredient matrix whose columns are indexed by the ki elements in row i of P . For each row i of P , replace the element pij with a copy of column pij of Ai , 1  j  n. The result is a ( m i=1 ri ) × n matrix B , the column replacement of A1 , . . . , Am into P . Fig. 5 gives an example of column replacement using an heterogeneous hash family.  1 1 1 1 1 a1 11 a13 a12 a11 a12 a13 1 1 1 1 1   a1 21 a23 a22 a21 a22 a23  B= 2 2 2 2 2   a2 11 a11 a11 a12 a12 a12 2 2 2 2 2 a2 21 a21 a21 a22 a22 a22 

P =

132123 111222

A1 =

1 1 a1 11 a12 a13 1 1 1 a21 a22 a23

A2 =

2 a2 11 a12 2 2 a21 a22

Fig. 5.

B is the column replacement of A1 , A2 into P .

An hierarchical method for compressive sensing is obtained using column replacement in an heterogeneous hash family. Suppose that Ai is a measurement matrix for a signal of dimension ki supporting the recovery of sparsity qi , for 1  i  m. We now describe the properties the pattern matrix needs to satisfy to support recovery of signals of dimension n and sparsity t. In Section II-A, we saw that a perfect hash family separates t columns into t parts, and that a separating hash family separates t columns into classes. We now define a particular type of separating hash family in which the number of symbols used to accomplish the separations is restricted. Let d = (d1 , . . . , dm ) be a tuple of positive integers, and let  be a positive integer. Let W = {W1 , . . . , Wr }, where for si 1  i  r, Wi = {wi1 , . . . , wisi } is a multiset of nonnegative integers, and i = j =1 wij . An SHF(m; n, k, W ), P = (pij ), is (d,  )-strengthening if whenever 1  i  r, · C is a set of i columns, · C1 , . . . , Csi is a partition of C with |Cj | = wij for 1  j  si , and · T is a set of  columns with |C  T | = min(i ,  ), there exists a row  for which px = py whenever x  Ce , y  Cf and e = f and the multiset {px : x  T } contains no more than d different symbols. When  = max{i : 1  i  r}, we omit  and write d-strengthening. Because rows of P can be arbitrarily permuted (while permuting the ingredient matrices in the same manner), the order of elements in k and d is us 1 inconsequential. Hence we often use exponential notation, writing xu 1 · · · xs , with ui a non-negative integer for 1  i  s,  -1  s s for a vector (y1 , . . . , y j=1 uj ) in which y = xj for j =1 uj <   j =1 uj for 1    j =1 uj . Fig. 6 gives a heterogeneous d-strengthening DHF(19; 13, k, 5, 2) with k = (56 41 312 ) and d = (46 313 ). This is equivalent to a d-strengthening SHF(19; 13, k, {{1, 4}, {2, 3}}). Consider the separation of columns {1, 7} from columns {2, 6, 11}. Row 8 accomplishes the required separation because it uses no more than d8 = 3 symbols. Consider instead columns {1, . . . , 5}. While the first row separates {1, 2, 3} from {4, 5}, it uses 5 symbols instead of d1 = 4 and so does not accomplish the required separation; this separation is accomplished in row 3. Next the properties are determined for an heterogeneous hash family to support recovery of signals of dimension n and sparsity t using a column replacement technique. Theorem 3: [14] Let k = (k1 , . . . , km ) and q = (q1 , . . . , qm ) be tuples of positive integers. Let d = (2q1 , . . . , 2qm ). For 1  i  m, let Ai  Rri ×ki be a measurement matrix that meets the (0 , qi )-null space condition. Let P be a (d, 2t)strengthening SHF(m; n, k, {1, t}), and let B be the column replacement of A1 , . . . , Am into P . Then B meets the (0 , t)-null space condition. Theorem 4: [14] Let k = (k1 , . . . , km ) and q = (q1 , . . . , qm ) be tuples of positive integers. For 1  i  m, let Ai  Rri ×ki be a measurement matrix that meets the (1 , qi )-null space condition. Let P be a (q, t)-strengthening DHF(m; n, k, t + 1, 2), and let B be the column replacement of A1 , . . . , Am into P . Then B meets the (1 , t)-null space condition.

Revisiting the d-strengthening DHF(19; 13, k, 5, 2) pattern matrix in Fig. 6, the results of Theorems 3 and 4 indicate that the number of symbols in each row need not be the same. In general, there may be as many ingredient matrices Ai as there are rows of the pattern matrix P . Moreover, the strength of each ingredient matrix Ai may be different! In this example, the

5

k1 = . . . = k6 = 5 symbols; d1 = . . . = d6 = 4 used to separate

k2 = 4 symbols; d2 = 3 used to separate 

k8 = . . . = k19 = 3 symbols; d8 = . . . = d19 = 3 used to separate

 4 0 0 2 2 3 0 0 1 0 0 0 2 2 0 1 1 2 0

 0 0 2 4 1 4 0 1 0 1 2 1 1 1 0 2 0 2 0

2 1 4 1 2 0 1 0 2 2 2 1 0 2 0 0 2 0 2

1 1 1 0 2 1 0 1 0 2 2 0 1 0 1 1 1 0 1

3 2 1 3 4 0 0 1 0 1 1 1 2 2 0 1 1 1 1

 3 3 2 0 0 3 2 2 2 2 2 2 0 2 1 1 0 2 0

 0 1 0 3 0 2 2 0 1 1 0 1 1 0 1 2 0 1 1

0 3 1 1 4 4 0 2 1 0 0 0 2 0 2 2 2 0 2

1 2 2 1 0 2 0 0 0 0 1 0 0 1 0 0 0 0 0

4 4 3 4 1 1 1 0 2 0 1 2 2 0 1 0 0 1 2

 2 2 0 2 1 1 3 2 0 1 0 0 0 0 2 0 2 2 2

2 0 3 0 3 2 0 1 2 0 0 2 0 1 2 2 1 1 1

1 4 4 2 3 0 0 2 1 2 1 2 1 1 2 0 0 2 1

Fig. 6.

A heterogeneous d-strengthening DHF(19; 13, k, 5, 2) with k = (56 41 312 ) and d = (46 313 ).

first 6 rows use 4 symbols to separate, so the corresponding ingredient matrices must have strength at least 4. The remaining rows use 3 symbols to separate, so the corresponding ingredient matrices must have strength at least 3. In [14], we showed that heterogeneity gives great flexibility in construction of measurement matrices using column replacement. The hierarchical structure of the measurement matrices produced by column replacement can also aid in recovery, and be used to support hybrid recovery schemes. We examine this problem next, considering a generalization of hash families that removes the restriction to those strategies based only on (0 , t)- or (1 , t)-recoverability. We also consider the computational investment required to recover the signal. III. H ASH FAMILIES FOR R ECOVERY In order to tackle signal recovery, we require another generalization of hash families. As before, let k = (k1 , . . . , km ) be a tuple of positive integers. An HF (m; n, k) is an m × n array, P = (pij ), in which each cell contains one symbol, and for each row 1  i  m, {pij : 1  j  n}  {, 1, . . . , ki }. The symbol , when present, is interpreted as representing a `missing' entry. When the pattern matrix P = (pij ) is an HF (m; n, k), and for 1  i  m the ingredient matrix Ai is ri × ki with columns indexed by the ki symbols in row i of P other than , the column replacement of A1 , . . . , Am into P is as before, except that when pij = , it is replaced with an all zero column vector of length ri . As we will see, the separating properties of the hash families we use allow us to locate the nonzero coordinates of the signal and hence perform the recovery. The definition of a W -separating hash family encompasses perfect, {w1 , . . . , ws }-separating, and distributing hash families. Therefore, we need only extend the definition of W -separating hash families to include the  symbol. To do so, we allow some of the elements of the multisets in W to be marked with a  superscript to form a set of marked multisets W  ; the multisets in W  are indexed. Then an HF (m; n, k) is W  -separating if, for each {w1 , . . . , ws }  W (with some elements possibly marked), s · whenever C is a set of i=1 wi columns, and · C1 , . . . , Cs is an (indexed) partition of C with |Ci | = wi for 1  i  s then there exists a row that separates C1 , . . . , Cs in which, for 1  j  s, if  appears in a column in Cj then wj is marked. As we will see, to recover the signal, the idea is to effect a separation where a significant coordinate of the signal is present in one class such that any other class does not prevent its recovery. A. Signal Recovery without Noise Theorems 3 and 4 suggest that a recovery scheme based on (0 , t)- or (1 , t)-recoverability can be `lifted' from the ingredient measurement matrices A1 , . . . , Am to the larger measurement matrix B obtained from column replacement. However, such a method appears to have two main drawbacks. First, it is restricted to recovery strategies based on (0 , t)- or (1 , t)-recoverability. Secondly, and perhaps more importantly, it appears to necessitate a large computational investment to recover the signal, given B.

6

In order to overcome these problems, we consider two cases. The positive case arises when the signal is known a priori to be in Rn 0 . The general case arises when the signal can be positive, negative, or zero. In each case we develop a recovery scheme for the matrix B resulting from column replacement that does not depend on any fixed algorithm, but rather on the recovery schemes for the ingredient matrices A1 , . . . , Am . We suppose that P = (pij ) is an HF (m; n, k). For each 1  i  m, we suppose that Ai is an ri × ki measurement matrix that has (0 , t)-recoverability, equipped with a recovery algorithm Ri that determines the unique t-sparse vector zi that solves Ai zi = yi . We further suppose that B is the column replacement of A1 , . . . , Am into P , and that y is the result of sampling an (unknown) t-sparse vector x = (x1 , . . . , xn ) using B . For 1  i  m, the ith row of P induces a partition {Si , Si1 , . . . , Siki } of the column indices {1, . . . , n}, where Si = {j : pij = , 1  j  n} for   {, 1, . . . , ki }. Assume that we have employed the recovery algorithms Ri to find solutions zi . For 1  i  m and   {, 1, . . . , ki }, the partition class Si is discarded if  = , insignificant if  =  and zi = 0, significant positive if zi > 0, and significant negative if zi < 0. For 1  i  m, let wi = (wi1 , . . . , wiki ) where wi = j Si xj . The vector wi can be considered as a projection of x induced by the symbol pattern in row i of P . These facts follow: i · For 1  i  m, by the definition of B and because Bi x = yi , zi = wi is a solution to A zi = yi . i · For 1  i  m, because A has (0 , t)-recoverability and wi is t-sparse (because x is t-sparse), zi = wi is the unique solution to Ai zi = yi , and so Ri returns wi . We now consider the positive case and the general case for recovery in succession. B. Signal Recovery: The Positive Case We establish that in the positive case with t-sparse signals, it suffices to use a separating hash family of suitable strength, along with suitable ingredient matrices. An SHF (m; n, k, {1, t }) separates t + 1 columns into two parts, one part of size one that cannot include the symbol , and the other of size t that may include . Theorem 5: Suppose that P is an SHF (m; n, k, {1, t }). For 1  i  m, let Ai  Rri ×ki be a measurement matrix that has (0 , t)-recoverability equipped with a recovery algorithm Ri that determines the unique t-sparse vector zi that solves Ai zi = yi . Further suppose that B is the column replacement of measurement matrices A1 , . . . , Am into P and that y is the result of sampling an (unknown) t-sparse vector x = (x1 , . . . , xn )  Rn 0 using B . Then the t-sparse solution x to B x = y can be recovered.

Proof: It suffices to determine whether xi is positive or zero for each 1  i  n, because once this is accomplished we can find the values of the positive xi by solving the overdetermined system that remains. For 1  i  m, apply recovery algorithm Ri to find the unique t-sparse vector zi such that Ai zi = yi . We claim that, for 1    n, x is positive if and only if for each i  {1, . . . , m} the partition class that contains  is either significant positive or discarded. Suppose first that x is positive. If Si is a partition class that contains , then either  =  and Si is insignificant or  = , zi = j Si xj  x > 0, and Si is significant positive. Now suppose that x = 0. Let C = {j : xj > 0, 1  j  n}; |C |  t. There must be a row  of P that separates C from {} such that p = . Let  = p . Then   S and S  C = , so S is insignificant. One useful application of Theorem 5 takes the pattern matrix P to be an SHF (m; n, 1, {1, t }), and each Ai to be a 1 × 1 matrix whose only element is 1; in this case, column replacement yields a matrix B isomorphic to P . In P for every column  and every set C of t columns with   C , there is a row in which all columns of C contain , while column  contains 1. Then the measurement matrices Ai have (0 , t)-recoverability and the recovery algorithms Ri are trivial. Hence in these cases, a matrix isomorphic to P itself supports recovery. Theorem 5 leads to a straightforward recovery algorithm. First, Ri is used to solve Ai zi = yi for 1  i  m. Then the classes Sij are classified as positive when zij > 0, discarded when j = , and insignificant when j =  and zij = 0; this can m be done in O( i=1 ki ) time. We need only compute, for each row, the complement of the union of the insignificant classes, and then compute the intersection over all rows of these complements. However, without additional structure this appears to require the examination of each coordinate; hence, this gives an (n) lower bound. It is not difficult, nevertheless, to obtain sublinear recovery times by restricting the hash family; we return to this problem in Section III-D. C. Signal Recovery: The General Case When the signal takes on both positive and negative values, cancellation of positive and negative contributions can yield a zero measurement despite the presence of a signal. Nevertheless, an additional requirement on the structure of the hash family

7

suffices to address this problem, as we show next. Theorem 6: Suppose that P is an SHF (m; n, k, {{, (t + 1 -  ) } : 1    t}). For 1  i  m, let Ai  Rri ×ki be a measurement matrix that has (0 , t)-recoverability equipped with a recovery algorithm Ri that determines the unique t-sparse vector zi that solves Ai zi = yi . Further suppose that B is the column replacement of measurement matrices A1 , . . . , Am into P and that y is the result of sampling an (unknown) t-sparse vector x = (x1 , . . . , xn ) using B . Then the t-sparse solution x to B x = y can be recovered.

Proof: As in the proof of Theorem 5, it suffices to determine whether xi is nonzero or zero for each 1  i  n, because once this is accomplished we can find the values of the nonzero xi by solving the overdetermined system that remains. For 1  i  m, apply recovery algorithm Ri to find the unique t-sparse vector zi such that Ai zi = yi . Let - z+ i = (max(0, zij ) : 1  j  ki ) and zi = (min(0, zij ) : 1  j  ki ). +  A row i of P is maximum positive if ||z+ i ||1  ||zi ||1 for 1  i  m. Let M  {1, . . . , m} index the maximum positive rows. We claim that a coordinate x is positive if and only if, for every   M ,  is in a significant positive class of the partition induced by row . Suppose first that x is positive and let   M . Because  indexes a maximum positive row, the partition class induced by row  that contains  is not discarded and does not contain the index of any negative variable. Thus it is in a significant positive partition class. Now suppose that x  0. Because P is an SHF (m; n, k, {{, (t + 1 -  ) } : 1    t}) and x is t-sparse, there is a row  of P that separates {j : xj > 0, 1  j  n} from {j : xj < 0, 1  j  n}  {} in which the symbol  only appears in a subset of the columns indexed by {j : xj < 0, 1  j  n}  {}. It follows that  is a maximum positive row of P and that the partition class induced by  containing  does not contain the index of any positive coordinate. So   M , but  is not in a significant positive class of the partition induced by row . In the same manner, all negative coordinates can be identified using maximum negative rows.

Again a straightforward recovery algorithm is given by Theorem 6 but, as in the positive case, it naively involves examining each of the n coordinates. D. Sublinear Time Signal Recovery Recovery can be accomplished in time that is sublinear in k when the hash family has suitable structure; we develop a general approach, and one example, here. In each case, for some subset M of the rows of P , sets are identified that must contain the indices of all positive coordinates (the indices of the negative coordinates, if they exist, can be located similarly). Recall from Section III-A, that the positive case arises when the signal is known a priori to be in Rn 0 and the general case arises when the signal can be positive, negative, or zero. In the positive case, M contains all rows and for   M , the candidate indices are V+ = { : p = , or x  Sj and zj > 0}. In the general case, M contains all rows that index maximum positive rows, and for   M , the candidate indices are V+ = { : x  Sj and zj > 0}. In both cases, we are to determine + + M V . In order to avoid the examination of each coordinate, we do not list the members of V explicitly, but rather use + an implicit representation to list the members of M V . First we give an implicit representation of an hash family HF(q + 1; q  , q ), P , where q is a prime power and 2    q . Let {0 , . . . , q-1 } be the elements of the finite field of order q , Fq . Index the rows of P by {}  {0 , . . . , q-1 }. Index the columns of P by the q  polynomials of degree less than  in indeterminate x, with coefficients in Fq . Now the entry of P with row index  and column indexed by polynomial f (x) is determined as f ( ) when   {0 , . . . , q-1 }, and as the coefficient of x-1 in f (x) when  = . By deleting rows, we form an HF(m; q  , q ) for some 1  m  q + 1. An hash family is linear if it is obtained in this way. The separation properties of such an hash family are crucial [1], [5]. For our purposes, the observation of interest is from [15]: if m  ( - 1)w1 w2 + 1, then a linear HF(m; n, q ) is {w1 , w2 }-separating. (This can be established by a simple argument: When two polynomials of degree less than  evaluate to the same value at  different points, they are the same polynomial.) In some cases, fewer rows suffice to ensure separation. In particular, Blackburn and Wild [5] establish that when q is sufficiently large, one needs at most (w1 + w2 - 1) rows; and in [15] specific small separations are examined to determine the set of prime powers for which various numbers of rows less than ( - 1)w1 w2 + 1 suffice. We proceed with the general statement so as not to impose additional conditions. When m  ( - 1)t + 1, P is {1, t}-separating; in addition, every {1, t - 1}-separation is accomplished in at least  rows. t+1 When m  ( - 1) t+1 2  2  + 1, P is {w, t + 1 - w}-separating for each 1  w  t; in addition, every {w, t - w}-separation t+1 t t t+1 is accomplished in at least  rows, because  t+1 2  2  =  2  2  +  2 . Thus in either case, M contains at least  rows of P .

8

+ + | vectors V + = Choose any  rows U = {1 , . . .  }  M . Now consider the sets {V :   U }. Define U |V + + {(g1 , . . . , g ) : gi  {pi  :   Vi } for 1  i  }. Each (g1 , . . . , g )  V defines a unique column of the hash family, corresponding to the unique polynomial L of degree at most  - 1 satisfying L(i ) = gi for 1  i  . Any column that does not arise in this way from a member of V + cannot be the column for a positive coordinate, because in the partition induced by one of the selected maximum rows it is not in a significant positive class. However, columns arising from vectors in V + need not arise from positive coordinates, because we may not have examined all of the rows of M . Nevertheless, we can now generate each of the columns arising from vectors in V + , and check for each whether it occurs in positive classes for all rows of M , not just the  selected. Now |V + | is O(t ), so when t is o(q ), the size of V + is o(n) (because n = q  ). For concreteness, taking q = t for t a prime power, we can permit  to be as large as t -2 . (For the positive case, we can permit  to be as large as t -1 . ) Hence, by restricting the hash family to one that is linear, it is possible to obtain recovery of the signal in sublinear time. In general, a hash family together with its ingredient matrices can be represented more concisely compared to a random measurement matrix for signal recovery. Furthermore, the hash family is an integer matrix, not a matrix of real numbers, and may therefore be easier to encode. When the hash family is linear an implicit representation of it may be used, further compacting its representation. The results of this section provide some evidence that column replacement enables recoverability conditions to be met. In Section IV, we show that it also preserves the basic machinery to deal with noise in the signal.

E. Adding Strengthening As the signal length increases, it is natural to support high sparsity. Yet the techniques developed until this point only preserve sparsity. Strengthening hash families provide a means to increase the level of sparsity supported. Theorem 7: Suppose that P is a d-strengthening SHF(m; n, k, {{, (t + 1 -  )} : 1    t}). For each 1  i  m, we suppose that Ai is an ri × ki measurement matrix that has (1 , di )-recoverability, equipped with a recovery algorithm Ri , that either determines the unique di -sparse vector zi that solves Ai zi = yi or indicates that no such vector exists. Further suppose that B is the column replacement of A1 , . . . , Am into P , and that y is the result of sampling an (unknown) t-sparse vector x = (x1 , . . . , xn ) using B . Then the t-sparse solution x to B x = y can be recovered.

Proof: Again it suffices to locate the nonzero coordinates of x. For 1  i  m, if recovery algorithm Ri returns a solution zi such that ||zi ||1  ||z||1 for any solution z returned by an oracle Rj , then zi is a maximum solution, and row i of P is a maximum row. Because P is a d-strengthening SHF(m; n, k, {{, (t + 1 -  )} : 1    t}), and x is t-sparse, there is a row  of P that separates {j : xj > 0, 1  j  n} from {j : xj < 0, 1  j  n} with the property that at most d symbols appear in the columns indexed by {j : xj = 0, 1  j  n}. So the projected vector w is d -sparse and it is the solution returned by R . By the definition of , ||w ||1 = ||x||1 . It follows that the 1 -norm of any maximum solution is at least ||x||1 . We claim that if Ri returns a maximum solution zi , then zi = wi . Suppose otherwise. Then, because zi is a maximum solution, we have ||zi ||1  ||x||1 . Further, it is clear from the definition of ||wi || that ||wi ||1  ||x||1 . Thus Ai zi = Ai wi , zi is di -sparse, and ||zi ||1  ||wi ||1 , which is a contradiction to the fact that Ai has (1 , di )-recoverability. Having established our claim, we can now use arguments similar to those used in the proof of Theorem 6 to show that a coordinate x is positive (negative) if and only if, for every maximum row in P ,  is in a significant positive (significant negative) class of the partition induced by that row. IV. R ECOVERY
WITH

N OISE

We now treat the recovery of signals with noise. A signal (x1 , . . . , xn ) is (s, t)-almost sparse if there is a set T of at most t coordinate indices such that i{1...,n}\T |xi | < s. Theorem 8: Suppose that P is an SHF(m; n, k, {{, (t + 1 -  )} : 1    t}). For each 1  i  m, we suppose that Ai is an ri × ki measurement matrix, equipped with recovery algorithm Ri , which, when applied to the sample obtained from an (s, t)-almost sparse signal xi , returns a vector zi such that ||zi - xi ||1 < . Further suppose that B is the column replacement of A1 , . . . , Am into P , and that y is the result of sampling an (unknown) (s, t)-almost sparse vector x = (x1 , . . . , xn ) using B .    Then, a (perfectly) t-sparse vector x = (x 1 , . . . , xn ) such that for 1  i  n, |xi | < 2(s + ) if xi = 0, and |xi - xi | < s +    if xi > 0, and such that B x = y, can be recovered. Proof: We provide a sketch first, and then the details. The idea is to write each coordinate of z as a sum of the signal coordinates in T that contribute to it, and of a noise term e that includes both the small contributions from coordinates outside T and the error less than  from the recovery algorithm. For each row  of P , we then split this sum into two parts: one part

9

  containing terms with the same sign as the z coordinate to which they contribute (indexed by sets T and E ), and another part   containing terms with the opposite sign to the z coordinate to which they contribute (indexed by sets T and E ). The key 1 1  observation is that the sum of the terms with indices in T can be approximated by 2 (||x|| - ||z ||) and hence by 2 (q - ||z ||)   because if T is empty then z has norm close to ||x||, and every term with index in T reduces ||z ||. + Let T be a set of at most t coordinate indices such that i{1...,n}\T |xi | < s. Let T = {i  T : xi  0}, T - = {i  T : xi < 0} and q  = iT |xi |. For 1  i  m, apply Ri to yi to find a vector zi such that ||zi - wi ||1 < . For i  {1, . . . , m}, call ||zi ||1 the signature of row i of P and let q be the maximum signature of any row of P . For 1  i  n, we calculate upper and lower estimates u(i) and (i) for xi . For each row index   {1, . . . , m} and each symbol   {1, . . . , k } we define u and  as follows. 1 1 · If z  0, then u = |z | + 2 (q - ||z ||1 ) and  = - 2 (q - ||z ||1 ). 1 1 · If z < 0, then u = 2 (q - ||z ||1 ) and  = -|z | - 2 (q - ||z ||1 ). For each i  {1, . . . , n} define u (i) = u and  (i) =  , where  is the symbol in row  of P such that i  S , and define u(i) = min{u (i) : 1    m} and (i) = max{ (i) : 1    m}. By first examining a row of maximum signature,  we can immediately conclude for each i  {1, . . . , n} either that u(i) = 0 or that (i) = 0. Define a vector x = (x 1 , . . . , xn )   by setting xi = 0 if |ui |, |i |  s + , and otherwise setting xi equal to whichever of u(i) or (i) has the greater absolute value. We claim that x satisfies the required conditions. To establish this claim we prove that, for 1  j  n, (i) for each   {1, . . . , m},  (j ) - (s + ) < xj < u (j ) + (s + ); (ii) there is some   {1, . . . , m} such that  (j ) > -(s + ) if xj  0 and u (j ) < s +  if xj < 0; and (iii) there is some   {1, . . . , m} such that u (j ) - (s + ) < xj if xj  0 and xj <  (j ) + (s + ) if xj < 0. We begin with some observations used throughout the proof. Let  be a row of P . For 1    k , we have z = k  + : zpi  0}  {i  T - : zpi < 0} ( iT S |xi |) + e for some e . Note that  =1 |e |  s + . Let T = {i  T      and let T = T \ T . Further, let E = {  {1, . . . , k } : e , z  0 or e , z < 0} and let E = {1, . . . , k } \ E . For 1    k , we have that       where  = 1 if   E and  = -1 if   E . Summing over the symbols in row  of P , we see      

|z | = 

 S iT 

|xi | - 

 S iT 

|xi | +  |e |

(1)

||z ||1 = q  - 2  - ||z ||1 ) =  

 iT

|xi | +  

  E

|e | -  

  E

|e |

(2)

and it follows that
1  2 (q

iT

 1 |xi | -  2  
 \S iT 

  E

 1 |e | +  2 1 2 
  E

  E

|e | . 1 2 
  E



(3)

Adding (1) to (3), we obtain
  |z | + 1 2 (q - ||z ||1 ) =


 S iT 

|xi | + 



|xi | -



|e | +



|e | +  |e |.



(4)

It follows from (2) that each row of P has signature less than q  + (s + ) and that any row of P that separates T + from T - has signature greater than q  - (s + ). Thus, q  - (s + ) < q < q  + (s + ) and hence
1 2 (q 1  1 1 - ||z ||1 ) - 1 2 (s + ) < 2 (q - ||z ||1 ) < 2 (q - ||z ||1 ) + 2 (s + ).

(5)

Let j  {1, . . . , n}. We next show that (i), (ii) and (iii) hold in the case where xj  0. The proof in the case where xj < 0 is similar. Proof of (i). Let  index any row of P and let S be the partition class induced by row  of P that contains j . Now  (j ) - (s + ) < xj because  (j )  0. If j  / T , then xj < s and xj < u (j ) + (s + ) because u (j )  0. If j  T and 1 z < 0, then xj  iT  |xi | and we see from (3) and (5) that xj < 2 (q - ||z ||1 ) + (s + ). If j  T and z  0, then  1 |xi | and we see from (4) and (5) that xj < |z | + 2 (q - ||z ||1 ) + (s + ). xj  iT  S  Proof of (ii). Let  index a row of P that separates T +  {j } from T - and let S be the partition class induced by row   or T  S = .  of P that contains j . If z  0, then, for 1    k , either iT  S |xi |  |e | and   E  1   . Using this, it follows from (3) and (5) that  (j ) = - 2 (q - ||z ||1 ) > -(s + ). If z < 0, then T  S =  and   E   Furthermore, for   {1, . . . , k } \ { }, either iT  S |xi | < |e | and   E or T  S = . Using these facts, it  follows from (4) and (5) that  (j ) = -|z | - 1 2 (q - ||z ||1 ) > -(s + ).

10

Proof of (iii). Let  index a row of P that separates T + \ {j } from T -  {j } and let S be the partition class induced by row  of P that contains j . If z < 0, then iT  S |xi |  xj . Furthermore, for each symbol   {1, . . . , k } \ { }, either  1   |xi |  |e | and   E or T S = . Then, it follows from (3) and (5) that u (j ) = 2 (q -||z ||1 )-(s+) < xj .  S iT  If z  0, then iT  S |xi |  xj . Furthermore, for each symbol   {1, . . . , k } \ { }, either iT |xi |  |e |  S    and   E or T  S = . Then, it follows from (4) and (5) that u (j ) = |z | + 1 ( q - || z || ) - ( s +  ) < x  1 j. 2 V. C ONCLUSION Hierarchical construction of measurement matrices by column replacement permits the explicit construction of large measurement matrices from small ones. The use of heterogeneous hash families supports the use of a library of smaller ingredient matrices, while the use of strengthening hash families allows the ingredient matrices to be designed for lower sparsity than the larger measurement matrix produced. Perhaps surprisingly, the ingredient measurement matrices need not all employ the same recovery algorithm; rather recovery for the large measurement matrix can use arbitrary routines for recovery that are provided with the ingredient matrices. In this way, computationally intensive recovery methods can be used for the ingredient matrices, which permits the selection of smaller matrices in general, while still enabling recovery for the large measurement matrix. Nevertheless, recovery using the large measurement matrix can be computationally prohibitive without further restrictions. Therefore it is shown that using a standard construction of linear hash families over the finite field, recovery for the large measurement matrix can be effected in sublinear time. Indeed sublinear recovery time can be obtained even when computationally intensive methods are used for each ingredient matrix. A practical implementation of these recovery methods requires that the methods deal effectively with noise in the signal. Suitable restrictions on the hash family and on each ingredient matrix used in column replacement are shown to be sufficient to permit recovery even in the presence of such noise. Measurement matrices that result from one column replacement have been studied here. Because recovery does not depend on the method by which recovery is done for the ingredient matrices, it is possible that the ingredient matrices themselves are constructed by column replacement from even smaller ingredient matrices. The merits and demerits of repeated column replacement deserve further study. ACKNOWLEDGEMENTS The work of D. Horsley and C. J. Colbourn is supported in part by the Australian Research Council through grant DP120103067. R EFERENCES
[1] N. Alon. Explicit construction of exponential sized families of k -independent sets. Discrete Mathematics, 58:191­193, 1986. [2] R. Baraniuk. Compressive sensing. IEEE Signal Processing Magazine, 24:227­234, 2007. [3] R. Berinde, A. C. Gilbert, P. Indyk, H. Karloff, and M. J. Strauss. Combining geometry and combinatorics: A unified approach to sparse signal recovery. In Proceedings of the 46th Annual Allerton Conference on Communication, Control, and Computing, pages 798­805, 2008. [4] S. R. Blackburn, T. Etzion, D. R. Stinson, and G. M. Zaverucha. A bound on the size of separating hash families. Journal of Combinatorial Theory, Series A, 115((7):1246­1256, 2008. [5] S. R. Blackburn and P. R. Wild. Optimal linear perfect hash families. Journal of Combinatorial Theory, Series A, 83:233­250, 1998. [6] E. J. Cand` es. Compressive sampling. In International Congress of Mathematicians, volume 3, pages 1433­1452, 2006. [7] E. J. Cand` es. The restricted isometry property and its implications for compressed sensing. Compte Rendus de l'Academie des Sciences, Series I, 346:589­592, 2008. [8] E. J. Cand` es, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 52:489­509, 2006. [9] E. J. Cand` es and T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory, 51:4203­4215, 2005. [10] E. J. Cand` es and T. Tao. Near optimal signal recovery from random projections: Universal encoding strategies. IEEE Transactions on Information Theory, 52:5406­5425, 2006. [11] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientific Computing, 20(1):33­61, 1998. [12] A. Cohen, W. Dahmen, and R. A. DeVore. Compressed sensing and best k -term approximation. Journal of the American Mathematical Society, 22:211­231, 2009. [13] C. J. Colbourn, D. Horsley, and C. McLean. Compressive sensing matrices and hash families. IEEE Transactions on Communications, 59(7):1840­1845, 2011. [14] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk. Strengthening hash families and compressive sensing. Journal of Discrete Algorithms, 16:170­186, 2012. [15] C. J. Colbourn and A. C. H. Ling. Linear hash families and forbidden configurations. Designs, Codes and Cryptography, 59:25­55, 2009. [16] G. Cormode and S. Muthukrishnan. Combinatorial algorithms for compressed sensing. In Lecture Notes in Computer Science, volume 4056, pages 280­294, 2006. [17] R. A. DeVore. Deterministic constructions of compressed sensing matrices. Journal of Complexity, 23:918­925, 2007. [18] D. L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE Transactions on Information Theory, 47:2845­2862, 2001. [19] M. Elad and A. M. Bruckstein. A generalized uncertainty principle and sparse representation in pairs of bases. IEEE Transactions on Information Theory, 48:2558­2567, 2002. [20] J. J. Fuchs. On sparse representations in arbitrary redundant bases. IEEE Transactions on Information Theory, 50:1341­1344, 2004. [21] J. J. Fuchs. Recovery of exact sparse representations in the presence of bounded noise. IEEE Transactions on Information Theory, 51:3601­3608, 2005. [22] A. C. Gilbert, M. A. Iwen, and M. J. Strauss. Group testing and sparse signal recovery. In Proceedings of the 42nd Asilomar Conference on Signals, Systems, pages 1059­1063, 2008. [23] A. C. Gilbert, M. J. Strauss, J. Tropp, and R. Vershynin. One sketch for all: Fast algorithms for compressed sensing. In Proceedings of the ACM Symposium on Theory of Computing, pages 237­246, 2007.

11

[24] R. Gribonval and M. Nielsen. Sparse representations in unions of bases. IEEE Transactions on Information Theory, 49:3320­3325, 2003. [25] M. A. Iwen. Combinatorial sublinear-time Fourier algorithms. Foundations of Computational Mathematics, 10:303­338, 2010. [26] S. Jafarpour, W. Xu, B. Hassibi, and R. Calderbank. Efficient and robust compressed sensing using optimized expander graphs. IEEE Transactions on Information Theory, 55:4299­4308, 2009. [27] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24:227­234, 1995. [28] D. R. Stinson, Tran Van Trung, and R. Wei. Secure frameproof codes, key distribution patterns, group testing algorithms and related structures. Journal of Statistical Planning and Inference, 86:595­617, 2000. [29] D. R. Stinson, R. Wei, and K. Chen. On generalized separating hash families. Journal of Combinatorial Theory, Series A, 115:105­120, 2008. [30] M. Stojnic, W. Xu, and B. Hassibi. Compressed sensing-probabilistic analysis of a null-space characterization. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, pages 3377­3380, 2008. [31] J. A. Tropp. Recovery of short, complex linear combinations via l1 minimization. IEEE Transactions on Information Theory, 51:1568­1570, 2005. [32] W. Xu and B. Hassibi. Efficient compressive sensing with deterministic guarantees using expander graphs. In Proceedings of IEEE Information Theory Workshop, 2007. [33] Y. Zhang. On theory of compressive sensing via 1 -minimization: Simple derivations and extensions. Technical Report Technical Report CAAM TR08-11, Rice University, 2008.

Covering subsequences by sets of permutations arises in numerous applications. Given a set of permutations that cover a specific set of subsequences, it is of interest not just to know how few permutations can be used, but also to find a set of size equal to or close to the minimum. These permutation construction problems have proved to be computationally challenging; few explicit constructions have been found for small sets of permutations of intermediate length, mostly arising from greedy algorithms. A different strategy is developed here. Starting with a set that covers the specific subsequences required, we determine local changes that can be made in the permutations without losing the required coverage. By selecting these local changes (using linear extensions) so as to make one or more permutations less âimportantâ for coverage, the method attempts to make a permutation redundant so that it can be removed and the set size reduced. A post-optimization method to do this is developed, and preliminary results on sequence covering arrays show that it is surprisingly effective.The construction of covering arrays with the fewest rows remains a challenging problem. Most computational and recursive constructions result in extensive repetition of coverage. While some is necessary, some is not. By reducing the repeated coverage, metaheuristic search techniques typically outperform simpler computational methods, but they have been applied in a limited set of cases. Time constraints often prevent them from finding an array of competitive size. We examine a different approach. Having used a simple computation or construction to find a covering array, we employ a post-optimization technique that repeatedly adjusts the array in an attempt to reduce its number of rows. At every stage the array retains full coverage. We demonstrate its value on a collection of previously best known arrays by eliminating, in some cases, 10% of their rows. In the well-studied case of strength two with twenty factors having ten values each, post-optimization produces a covering array with only 162 rows, improving on a wide variety of computational and combinatorial methods. We identify certain important features of covering arrays for which post-optimization is successful.Search, test, and measurement problems in sparse domains often require the construction of arrays in which every t or fewer columns satisfy a simply stated combinatorial condition. Such t-restriction problems often ask for the construction of an array satisfying the t-restriction while having as few rows as possible. Combinatorial, algebraic, and probabilistic methods have been brought to bear for specific t-restriction problems; yet in most cases they do not succeed in constructing arrays with a number of rows near the minimum, at least when the number of columns is small. To address this, an algorithmic method is proposed that, given an array satisfying a t-restriction, attempts to improve the array by removing rows. The key idea is to determine the necessity of the entry in each cell of the array in meeting the t-restriction, and repeatedly replacing unnecessary entries, with the goal of producing an entire row of unnecessary entries. Such a row can then be deleted, improving the array, and the process can be iterated. For certain t-restrictions, it is shown that by determining conflict graphs, entries that are necessary can nonetheless be changed without violating the t-restriction. This permits a richer set of ways to improve the arrays. The efficacy of these methods is demonstrated via computational results.SaaS (Software-as-a-Service) often uses multi-tenancy architecture (MTA) where tenant developers compose their applications online using the components stored in the SaaS database. Tenant applications need to be tested, and combinatorial testing can be used. While numerous combinatorial testing techniques are available, most of them produce static sequences of test configurations and their goal is often to provide sufficient coverage such as 2-way interaction coverage. But the goal of SaaS testing is to identify those compositions that are faulty for tenant applications. This paper proposes an adaptive test configuration generation algorithm AR (Adaptive Reasoning) that can rapidly identify those faulty combinations so that those faulty combinations cannot be selected by tenant developers for composition. The AR algorithm has been evaluated by both simulation and real experimentation using a MTA SaaS sample running on GAE (Google App Engine). Both the simulation and experiment showed show that the AR algorithm can identify those faulty combinations rapidly. Whenever a new component is submitted to the SaaS database, the AR algorithm can be applied so that any faulty interactions with new components can be identified to continue to support future tenant applications.Partial Covering Arrays: Algorithms and Asymptotics
Kaushik Sarkar1 , Charles J. Colbourn1 , Annalisa De Bonis2 , and Ugo Vaccaro2

arXiv:1605.02131v1 [math.CO] 7 May 2016

2

CIDSE, Arizona State University, U.S.A. Dipartimento di Informatica, University of Salerno, Italy

1

Abstract. A covering array CA(N ; t, k, v ) is an N × k array with entries in {1, 2, . . . , v }, for which every N × t subarray contains each ttuple of {1, 2, . . . , v }t among its rows. Covering arrays find application in interaction testing, including software and hardware testing, advanced materials development, and biological systems. A central question is to determine or bound CAN(t, k, v ), the minimum number N of rows of a CA(N ; t, k, v ). The well known bound CAN(t, k, v ) = O((t - 1)v t log k) is not too far from being asymptotically optimal. Sensible relaxations of the covering requirement arise when (1) the set {1, 2, . . . , v }t need only be contained among the rows of at least (1 - ) k of the N × t subarrays t and (2) the rows of every N × t subarray need only contain a (large) subset of {1, 2, . . . , v }t . In this paper, using probabilistic methods, significant improvements on the covering array upper bound are established for both relaxations, and for the conjunction of the two. In each case, a randomized algorithm constructs such arrays in expected polynomial time.

1

Introduction

Let [n] denote the set {1, 2, . . . , n}. Let N, t, k, and v be integers such that k  t  2 and v  2. Let A be an N × k array where each entry is from the set [v ]. For I = {j1 , . . . , j }  [k ] where j1 < . . . < j , let AI denote the N ×  array in which AI (i, ) = A(i, j ) for 1  i  N and 1   ; AI is the projection of A onto the columns in I . A covering array CA(N ; t, k, v ) is an N × k array A with each entry from ] t [v ] so that for each t-set of columns C  [k t , each t-tuple x  [v ] appears as a row in AC . The smallest N for which a CA(N ; t, k, v ) exists is denoted by CAN(t, k, v ). Covering arrays find important application in software and hardware testing (see [22] and references therein). Applications of covering arrays also arise in experimental testing for advanced materials [4], inference of interactions that regulate gene expression [29], fault-tolerance of parallel architectures [15], synchronization of robot behavior [17], drug screening [30], and learning of boolean functions [11]. Covering arrays have been studied using different nomenclature, as qualitatively independent partitions [13], t-surjective arrays [5], and (k, t)universal sets [19], among others. Covering arrays are closely related to hash families [10] and orthogonal arrays [8].

2

Background and Motivation

The exact or approximate determination of CAN(t, k, v ) is central in applications of covering arrays, but remains an open problem. For fixed t and v , only when t = v = 2 is CAN(t, k, v ) known precisely for infinitely many values of k . Kleitman and Spencer [21] and Katona [20] independently proved that the largest k for -1 which a CA(N ; 2, k, 2) exists satisfies k = N orner, N/2 . When t = 2, Gargano, K and Vaccaro [13] establish that CAN(2, k, v ) = v log k (1 + o(1)). 2 (1)

(We write log for logarithms base 2, and ln for natural logarithms.) Several researchers [2,5,14,16] establish a general asymptotic upper bound on CAN(t, k, v ): CAN(t, k, v )  t-1 log k (1 + o(1)). t log vtv-1 (2)

A slight improvement on (2) has recently been proved [12,28]. An (essentially) equivalent but more convenient form of (2) is: CAN(t, k, v )  (t - 1)v t log k (1 + o(1)). (3)

A lower bound on CAN(t, k, v ) results from the inequality CAN(t, k, v )  v · CAN(t - 1, k - 1, v ) obtained by derivation, together with (1), to establish that CAN(t, k, v )  v t-2 · CAN(2, k - t + 2, v ) = v t-2 · v 2 log(k - t + 2)(1 + o(1)). When t < 1, we obtain: k CAN(t, k, v ) =  (v t-1 log k ). (4) Because (4) ensures that the number of rows in covering arrays can be considerable, researchers have suggested the need for relaxations in which not all interactions must be covered [7,18,23,24] in order to reduce the number of rows. The practical relevance is that each row corresponds to a test to be performed, adding to the cost of testing. For example, an array covers a t-set of columns when it covers each of the v t interactions on this t-set. Hartman and Raskin [18] consider arrays with a fixed number of rows that cover the maximum number of t-sets of columns. A similar question was also considered in [24]. In [23,24] a more refined measure of the (partial) coverage of an N × k array A is introduced. For a given q  [0, 1], let (A, q ) be the number of N × t submatrices of A with the property that at least qv t elements of [v ]t appear in their set of rows; the (q, t)-completeness of A is (A, q )/ k t . Then for practical purposes one wants "high" (q, t)-completeness with few rows. In these works, no theoretical results on partial coverage appear to have been stated; earlier contributions focus on experimental investigations of heuristic construction methods. Our purpose is to initiate a mathematical investigation of arrays offering "partial" coverage. More precisely, we address:

­ Can one obtain a significant improvement on the upper bound (3) if the set [v ]t is only required to be contained among the rows of at least (1 - ) k t subarrays of A of dimension N × t? ­ Can one obtain a significant improvement if, among the rows of every N × t subarray of A, only a (large) subset of [v ]t is required to be contained? ­ Can one obtain a significant improvement if the set [v ]t is only required to be contained among the rows of at least (1 - ) k t subarrays of A of dimension N × t, and among the rows of each of the k t subarrays that remain, a (large) subset of [v ]t is required to be contained? We answer these questions both theoretically and algorithmically in the following sections.

3

Partial Covering Arrays

When 1  m  v t , a partial m-covering array, PCA(N ; t, k, v, m), is an N × k ] array A with each entry from [v ] so that for each t-set of columns C  [k t , at t least m distinct tuples x  [v ] appear as rows in AC . Hence a covering array CA(N ; t, k, v ) is precisely a partial v t -covering array PCA(N ; t, k, v, v t ). Theorem 1. For integers t, k, v , and m where k  t  2, v  2 and 1  m  v t there exists a PCA(N ; t, k, v, m) with ln N ln . Proof. Let r = v t -m+1, and A be a random N ×k array where each entry is cho] sen independently from [v ] with uniform probability. For C  [k t , let BC denote t the event that at least r tuples from [v ] are missing in AC . The probability that r N a particular r-set of tuples from [v ]t is missing in AC is 1 - v . Applying the t
r . union bound to all r-sets of tuples from [v ]t , we obtain Pr[BC ]  v 1- v t r By linearity of expectation, the expected number of t-sets C for which AC misses vt r N at least r tuples from [v ]t is at most k 1- v . When A has at least t t r vt ln (k )( ) t m-1 rows this expected number is less than 1. Therefore, an array A vt ln( m -1 ) ] exists with the required number of rows such that for all C  [k t , AC misses t at most r - 1 tuples from [v ] , i.e. AC covers at least m tuples from [v ]t .
t

k t

vt m-1 vt m-1

.

(5)

N

Theorem 1 can be improved upon using the Lov´ asz local lemma. Lemma 1. (Lov´ asz local lemma; symmetric case) (see [1]) Let A1 , A2 , . . . , An events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at most d, and that Pr[Ai ]  ¯ p for all 1  i  n. If ep(d + 1)  1, then Pr[n i=1 Ai ] > 0.

Lemma 1 provides an upper bound on the probability of a "bad" event in terms of the dependence structure among such bad events, so that there is a guaranteed outcome in which all "bad" events are avoided. This lemma is most useful when there is limited dependence among the "bad" events, as in the following: Theorem 2. For integers t, k, v and m where v, t  2, k  2t and 1  m  v t there exists a PCA(N ; t, k, v, m) with 1 + ln t N ln
k t-1 vt m-1 vt m-1

.

(6)

] Proof. When k  2t, each event BC with C  [k (that is, at least v t - m + 1 t t k-1 k tuples are missing in AC ) is independent of all but at most 1 t-1 < t t-1 [k ] events in {BC : C  t \ {C }}. Applying Lemma 1, Pr[C ([k]) BC ] > 0 when
t

e

vt r

1-

r vt

N

t

k t-1

 1.

(7)

Solve (7) to obtain the required upper bound on N . When m = v t , apply the Taylor series expansion to obtain ln and thereby recover the upper bound (3). Theorem 2 implies: Corollary 1. Given q  [0, 1] and integers 2  t  k , v  2, there exists an N × k array on [v ] with (q, t)-completeness equal to 1 (i.e., maximal), whose number N of rows satisfies 1 + ln t N ln
k t-1 vt qv t -1 vt m-1



1 vt ,

vt qv t -1

.

Rewriting (6), setting r = v t - m + 1, and using the Taylor series expansion r of ln 1 - v t , we get 1 + ln t N ln
k t-1 vt v t -r vt r



v t (t - 1) ln k r

1-

ln r + o(1) . ln k

(8)

Hence when r = v (t - 1) (or equivalently, m = v t - v (t - 1) + 1), there is a partial m-covering array with (v t-1 ln k ) rows. This matches the lower bound (4) asymptotically for covering arrays by missing, in each t-set of columns, no more than v (t - 1) - 1 of the v t possible rows. The dependence of the bound (6) on the number of v -ary t-vectors that must appear in the t-tuples of columns is particularly of interest when test suites are run sequentially until a fault is revealed, as in [3]. Indeed the arguments here may have useful consequences for the rate of fault detection.

Algorithm 1: Moser-Tardos type algorithm for partial m-covering arrays.
Input: Integers N, t, k, v and m where v, t  2, k  2t and 1  m  v t Output: A : a PCA(N ; t, k, v, m) k vt 1+ln t(t- 1)(m-1) Let N := ; t v
ln m-1

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

Construct an N × k array A where each entry is chosen independently and uniformly at random from [v ]; repeat Set covered := true; ] for each column t-set C  [k do t if AC does not cover at least m distinct t-tuples x  [v ]t then Set covered := false; Set missing-column-set := C ; break; end end if covered = false then Choose all the entries in the t columns of missing-column-set independently and uniformly at random from [v ]; end until covered = true ; Output A;

Lemma 1 and hence Theorem 2 have proofs that are non-constructive in nature. Nevertheless, Moser and Tardos [26] provide a randomized algorithm with the same guarantee. Patterned on their method, Algorithm 1 constructs a partial m-covering array with exactly the same number of rows as (6) in expected polynomial time. Indeed, for fixed t, the expected number of times the resampling step (line 13) is repeated is linear in k (see [26] for more details).

4

Almost Partial Covering Arrays

For 0 < < 1, an -almost partial m-covering array, APCA(N ; t, k, v, m, ), is an N × k array A with each entry from [v ] so that for at least (1 - ) k t column [k] t t-sets C  t , AC covers at least m distinct tuples x  [v ] . Again, a covering array CA(N ; t, k, v ) is precisely an APCA(N ; t, k, v, v t , ) when < 1/ k t . Our first result on -almost partial m-covering arrays is the following. Theorem 3. For integers t, k, v, m and real where k  t  2, v  2, 1  m  v t and 0   1, there exists an APCA(N ; t, k, v, m, ) with ln N ln
vt m-1 vt m-1

/ . (9)

Proof. Parallelling the proof of Theorem 1 we compute an upper bound on the ] t expected number of t-sets C  [k t for which AC misses at least r tuples x  [v ] . k When this expected number is at most t , an array A is guaranteed to exist [k] with at least (1 - ) k such that AC misses at most t t-sets of columns C  t t r - 1 distinct tuples x  [v ] . Thus A is an APCA(N ; t, k, v, m, ). To establish the theorem, solve the following for N : k t vt r 1- r vt
N



k . t

When < 1/ k t we recover the bound from Theorem 1 for partial m-covering arrays. In terms of (q, t)-completeness, Theorem 3 yields the following. Corollary 2. For q  [0, 1] and integers 2  t  k , v  2, there exists an N × k array on [v ] with (q, t)-completeness equal to 1 - , with ln N ln
vt m-1 vt m-1

/ .

When m = v t , an -almost covering array exists with N  v t ln v rows. Improvements result by focussing on covering arrays in which the symbols are acted on by a finite group. In this setting, one chooses orbit representatives of rows that collectively cover orbit representatives of t-way interactions under the group action; see [9], for example. Such group actions have been used in direct and computational methods for covering arrays [6,25], and in randomized and derandomized methods [9,27,28]. We employ the sharply transitive action of the cyclic group of order v , adapting the earlier arguments using methods from [28]: Theorem 4. For integers t, k, v and real where k  t  2, v  2 and 0   1 there exists an APCA(N ; t, k, v, v t , ) with N  v t ln v t-1 . (10)

t

Proof. The action of the cyclic group of order v partitions [v ]t into v t-1 orbits, each of length v . Let n = N and let A be an n × k random array v where each entry is chosen independently from the set [v ] with uniform prob] ability. For C  [k t , AC covers the orbit X if at least one tuple x  X is present in AC . The probability that the orbit X is not covered in A is n v n 1- v = 1 - vt1 . Let DC denote the event that AC does not cover t -1 n at least one orbit. Applying the union bound, Pr[DC ]  v t-1 1 - vt1 . By -1 linearity of expectation, the expected number of column t-sets C for which DC n t-1 occurs is at most k 1 - vt1 . As earlier, set this expected value to be -1 t v

at most k t and solve for n. An array exists that covers all orbits in at least (1 - ) k column t-sets. Develop this array over the cyclic group to obtain the t desired array. As in [28], further improvements result by considering a group, like the Frobenius group, that acts sharply 2-transitively on [v ]. When v is a prime power, the Frobenius group is the group of permutations of Fv of the form {x  ax + b : a, b  Fv , a = 0}. Theorem 5. For integers t, k, v and real where k  t  2, v  2, v is a prime power and 0   1 there exists an APCA(N ; t, k, v, v t , ) with N  v t ln 2v t-2 + v.
t- 1

(11)

1 Proof. The action of the Frobenius group partitions [v ]t into v v-- orbits of 1 length v (v - 1) (full orbits) each and 1 orbit of length v (a short orbit). The short orbit consists of tuples of the form (x1 , . . . , xt )  [v ]t where x1 = . . . = xt . -v Let n = vN (v -1) and let A be an n × k random array where each entry is chosen independently from the set [v ] with uniform probability. Our strategy is to construct A so that it covers all full orbits for the required number of arrays ] {AC : C  [k t }. Develop A over the Frobenius group and add v rows of the form (x1 , . . . , xk )  [v ]t with x1 = . . . = xk to obtain an APCA(N ; t, k, v, v t , ) with the desired value of N . Following the lines of the proof of Theorem 4, A covers all full orbits in at least (1 - ) k t column t-sets C when

k v t-1 - 1 t v-1 Because
v t-1 -1 v -1

1-

v-1 v t-1

n



k . t

 2v t-2 for v  2, we obtain the desired bound.

Using group action when m = v t affords useful improvements. Does this improvement extend to cases when m < v t ? Unfortunately, the answer appears to be no. Consider the case for PCA(N ; t, k, v, m) when m  v t using the action of the cyclic group of order v on [v ]t . Let A be a random n × k array over [v ]. When v t - vs + 1  m  v t - v (s - 1) for 1  s  v t-1 , this implies that ] t for all C  [k t , AC misses at most s - 1 orbits of [v ] . Then we obtain that n  1 + ln t
k t-1 v t-1 s

/ ln

v t-1 v t-1 -s

. Developing A over the cyclic group

we obtain a PCA(N ; t, k, v, m) with 1 + ln N v ln
k t-1 v t-1 s

v t-1 v t-1 -s

(12)

Figure 1 compares (12) and (6). In Figure 1a we plot the size of the partial m-covering array as obtained by (12) and (6) for v t - 6v + 1  m  v t and

9

x 10

4

10 Eq. (12) Eq. (6)

6

8

Eq. (12) Eq. (6)

7 N - number of rows N - number of rows 4075 4080 4085 m 4090 4095 4100

6

10

5

5

4

3
4

2 4070

10 1 10

10

2

10 k

3

10

4

(a) t = 6, k = 20, v = 4

(b) t = 6, v = 4, m = v t - v

Fig. 1: Comparison of (12) and (6). Figure (a) compares the sizes of the partial m-covering arrays when v t - 6v + 1  m  v t . Except for m = v t = 4096 the bound from (6) outperforms the bound obtained by assuming group action. Figure (b) shows that for m = v t - v = 4092, (6) outperforms (12) for all values of k . t = 6, k = 20, v = 4. Except when m = v t = 4096, the covering array case, (6) outperforms (12). Similarly, Figure 1b shows that for m = v t - v = 4092, (6) consistently outperforms (12) for all values of k when t = 6, v = 4. We observe similar behavior for different values of t and v . Next we consider even stricter coverage restrictions, combining Theorems 2 and 4. Theorem 6. For integers t, k, v, m and real where k  t  2, v  2, 0   1 k and m  v t + 1 - ln(v/ln 1/(t-1) ) there exists an N × k array A with entries from [v ] such that
] t 1. for each C  [k t , AC covers at least m tuples x  [v ] , t 2. for at least (1 - ) k t column t-sets C , AC covers all tuples x  [v ] ,

3. N = O(v t ln

v t-1

).

Proof. We vertically juxtapose a partial m-covering array and an -almost v t k t covering array. For r = ln(v/ln 1/(t-1) ) and m = v - r + 1, (8) guarantees the existence of a partial m-covering array with v t ln
t v t-1

{1 + o(1)} rows. The-

orem 4 guarantees the existence of an -almost v -covering array with at most t-1 v t ln v rows. Corollary 3. There exists an N × k array A such that: 1. for any t-set of columns C  distinct t-tuples x  [v ]t ,
[k] t

, AC covers at least m  v t + 1 - v (t - 1)

2. for at least 1 -
t

v t- 1 k1/v

k t

column t-sets C , AC covers all the distinct t-tuples

x  [v ] . 3. N = O(v t-1 ln k ). Proof. Apply Theorem 6 with m = v t + 1 -
ln k t ln(v/
1/(t-1) )

ln k ln(v/
1/(t-1) )

. There are at most

- 1 missing t-tuples x  [v ] in the AC for each of the at most k t column t-sets C that do not satisfy the second condition of Theorem 6. To bound from above the number of missing tuples to a certain small function f (t) of t, it
1 f (t)+1 is sufficient that  v t-1 k . Then the number of missing t-tuples x  [v ]t in AC is bounded from above by f (t) whenever is not larger than
t- 1

v

t-1

1 k

t- 1 f (t)+1

(13)
v t-1

On the other hand, in order for the number N = O v t-1 ln

of rows is not (14)

of A to be asymptotically equal to the lower bound (4), it suffices that smaller than v t-1 1 . kv

When f (t) = v (t - 1) - 1, (13) and (14) agree asymptotically, completing the proof. Once again we obtain a size that is O(v t-1 log k ), a goal that has not been reached for covering arrays. This is evidence that even a small relaxation of covering arrays provides arrays of the best sizes one can hope for. Next we consider the efficient construction of the arrays whose existence is ensured by Theorem 6. Algorithm 2 is a randomized method to construct an APCA(N ; t, k, v, m, ) of a size N that is very close to the bound of Theorem 3. By Markov's inequality the condition in line 9 of Algorithm 2 is met with probability at most 1/2. Therefore, the expected number of times the loop in line 2 repeats is at most 2. To prove Theorem 3, t-wise independence among the variables is sufficient. Hence, Algorithm 2 can be derandomized using t-wise independent random variables. We can also derandomize the algorithm using the method of conditional expectation. In this method we construct A by considering the k columns one by one and fixing all N entries of a column. Given a set of already fixed columns, to fix the entries of the next column we consider all possible v N choices, and choose one that provides the maximum conditional expectation of the number of ] column t-sets C  [k such that AC covers at least m tuples x  [v ]t . Because t N v = O(poly(1/ )), this derandomized algorithm constructs the desired array in polynomial time. Similar randomized and derandomized strategies can be applied to construct the array guaranteed by Theorem 4. Together with Algorithm 1 this implies that the array in Theorem 6 is also efficiently constructible.

Algorithm 2: Randomized algorithm for -almost partial m-covering arrays.
Input: Integers N, t, k, v and m where v, t  2, k  2t and 1  m  v t , and real 0< <1 Output: A : an APCA(N ; t, k, v, m, ) vt ln 2(m / -1) Let N := ; vt
ln m-1

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

repeat Construct an N × k array A where each entry is chosen independently and uniformly at random from [v ]; Set isAPCA:= true; Set defectiveCount := 0; ] for each column t-set C  [k do t if AC does not cover at least m distinct t-tuples x  [v ]t then Set defectiveCount := defectiveCount + 1; k if defectiveCount > then t Set isAPCA:= false; break; end end end until isAPCA = true ; Output A;

5

Final Remarks

We have shown that by relaxing the coverage requirement of a covering array somewhat, powerful upper bounds on the sizes of the arrays can be established. Indeed the upper bounds are substantially smaller than the best known bounds for a covering array; they are of the same order as the lower bound for CAN(t, k, v ). As importantly, the techniques not only provide asymptotic bounds but also randomized polynomial time construction algorithms for such arrays. Our approach seems flexible enough to handle variations of these problems. For instance, some applications require arrays that satisfy, for different subsets of columns, different coverage or separation requirements [8]. In [16] several interesting examples of combinatorial problems are presented that can be unified and expressed in the framework of S -constrained matrices. Given a set of vectors ] S each of length t, an N × k matrix M is S -constrained if for every t-set C  [k t , MC contains as a row each of the vectors in S . The parameter to optimize is, as usual, the number of rows of M . One potential direction is to ask for arrays that, in every t-tuple of columns, cover at least m of the vectors in S , or that all vectors in S are covered by all but a small number of t-tuples of columns. Exploiting the structure of the members of S appears to require an extension of the results developed here.

Acknowledgements
Research of KS and CJC was supported in part by the National Science Foundation under Grant No. 1421058.

References
1. Noga Alon and Joel H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics and Optimization. John Wiley & Sons, Inc., Hoboken, NJ, third edition, 2008. 2. B. Becker and H.-U. Simon. How robust is the n-cube? Inform. and Comput., 77:162­178, 1988. 3. Ren´ ee C. Bryce, Yinong Chen, and Charles J. Colbourn. Biased covering arrays for progressive ranking and composition of web services. Int. J. Simulation Process Modelling, 3(1/2):80­87, 2007. 4. J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE Global Research Technical Report, 29:769­781, 2002. 5. Ashok K. Chandra, Lawrence T. Kou, George Markowsky, and Shmuel Zaks. On sets of boolean n-vectors with all k-projections surjective. Acta Informatica, 20(1):103­111, 1983. 6. M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength 3. Des. Codes Crypt., 16:235­242, 1999. 7. Baiqiang Chen and Jian Zhang. Tuple density: a new metric for combinatorial test suites. In Proceedings of the 33rd International Conference on Software Engineering, ICSE 2011, Waikiki, Honolulu , HI, USA, May 21-28, 2011, pages 876­879, 2011. 8. C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121­167, 2004. 9. C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal of Combinatorial Mathematics and Combinatorial Computing, 90:97­115, 2014. 10. Charles J. Colbourn. Covering arrays and hash families. In D. Crnkovi c and V. Tonchev, editors, Information Security, Coding Theory, and Related Combinatorics, NATO Science for Peace and Security Series, pages 99­135. IOS Press, 2011. 11. Peter Damaschke. Adaptive versus nonadaptive attribute-efficient learning. Machine Learning, 41(2):197­215, 2000. 12. N. Franceti´ c and B. Stevens. Asymptotic size of covering arrays: an application of entropy compression. ArXiv e-prints, March 2015. 13. L. Gargano, J. K¨ orner, and U. Vaccaro. Sperner capacities. Graphs and Combinatorics, 9:31­46, 1993. 14. A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds and Poisson approximations. Combinatorics, Probability and Computing, 5:105­ 118, 1996. 15. N. Graham, F. Harary, M. Livingston, and Q.F. Stout. Subcube fault-tolerance in hypercubes. Information and Computation, 102(2):280 ­ 314, 1993. 16. Sylvain Gravier and Bernard Ycart. S-constrained random matrices. DMTCS Proceedings, 0(1), 2006.

17. A. Hartman. Software and hardware testing using combinatorial covering suites. In M. C. Golumbic and I. B.-A. Hartman, editors, Interdisciplinary Applications of Graph Theory, Combinatorics, and Algorithms, pages 237­266. Springer, Norwell, MA, 2005. 18. Alan Hartman and Leonid Raskin. Problems and algorithms for covering arrays. Discrete Mathematics, 284(13):149 ­ 156, 2004. 19. Stasys Jukna. Extremal Combinatorics: With Applications in Computer Science. Springer Publishing Company, Incorporated, 1st edition, 2010. 20. G. O. H. Katona. Two applications (for search theory and truth functions) of Sperner type theorems. Periodica Math., 3:19­26, 1973. 21. D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math., 6:255­262, 1973. 22. D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC Press, 2013. 23. D. R. Kuhn, I. D. Mendoza, R. N. Kacker, and Y. Lei. Combinatorial coverage measurement concepts and applications. In Software Testing, Verification and Validation Workshops (ICSTW), 2013 IEEE Sixth International Conference on, pages 352­361, March 2013. 24. J. R. Maximoff, M. D. Trela, D. R. Kuhn, and R. Kacker. A method for analyzing system state-space coverage within a t-wise testing framework. In 4th Annual IEEE Systems Conference, pages 598­603, 2010. 25. K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin. Des., 13:70­77, 2005. 26. Robin A. Moser and G´ abor Tardos. A constructive proof of the general Lov´ asz local lemma. J. ACM, 57(2):Art. 11, 15, 2010. 27. K. Sarkar and C. J. Colbourn. Two-stage algorithms for covering array construction. submitted for publication. 28. K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv e-prints, March 2016. 29. D. E. Shasha, A. Y. Kouranov, L. V. Lejay, M. F. Chou, and G. M. Coruzzi. Using combinatorial design to study regulation by multiple input signals: A tool for parsimony in the post-genomics era. Plant Physiol., 127:1590­2594, 2001. 30. A. J. Tong, Y. G. Wu, and L. D. Li. Room-temperature phosphorimetry studies of some addictive drugs following dansyl chloride labelling. Talanta, 43(9):14291436, September 1996.

Test Algebra for Combinatorial Testing
Wei-Tek Tsai , Charles J. Colbourn , Jie Luo , Guanqiu Qi , Qingyang Li , Xiaoying Bai
 School

of Computing, Informatics, and Decision Systems Engineering Arizona State University, Tempe, AZ, USA  State Key Laboratory of Software Development Environment School of Computer Science and Engineering, Beihang University, Beijing, China  Department of Computer Science and Technology, INLIST Tsinghua University, Beijing, China {wtsai, colbourn}@asu.edu, luojie@nlsde.buaa.edu.cn {guanqiuq, qingyan2b}@asu.edu, baixy@tsinghua.edu.cn

Abstract--This paper proposes a new algebraic system, Test Algebra (T A), for identifying faults in combinatorial testing for SaaS (Software-as-a-Service) applications. SaaS as a part of cloud computing is a new software delivery model, and mission-critical applications are composed, deployed, and executed in cloud platforms. Testing SaaS applications is a challenging task because new applications need to be tested when they are composed before they can be deployed for execution. Combinatorial testing algorithms can be used to identify faulty configurations and interactions from 2-way all the way to k-way where k is the number of components in the application. The T A defines rules to identify faulty configurations and interactions. Using the rules defined in the T A, a collection of configurations can be tested concurrently in different servers and in any order and the results obtained will be still same due to the algebraic constraints. Index Terms--Combinatorial testing, algebra, SaaS

I. I NTRODUCTION Software-as-a-Service (SaaS) is a new software delivery model. SaaS often supports three features: customization, multi-tenancy architecture (MTA), and scalability. MTA means using one code base to develop multiple tenant applications, and each tenant application essentially is a customization of the base code [12]. A SaaS system often also supports scalability as it can supply additional computing resources when the workload is heavy. Tenants' applications are often customized by using components stored in the SaaS database [14], [1], [11] including GUI, workflow, service, and data components. Once tenant applications are composed, they need to be tested, but a SaaS system can have millions of components, and hundreds of thousands of tenant applications. Testing tenant applications becomes a challenge as new tenant applications and components are added into the SaaS system continuously. New tenant applications are added on a daily basis while other tenant applications are running on the SaaS platform. As new tenant applications are composed, new components are added into the SaaS system. Each tenant application represents a customer for the SaaS system, and thus it needs to be tested. Combinatorial testing is a popular testing technique to test an application with different configurations. It often assumes that each component within a configuration has been tested

already, but interactions among components in the configuration may cause failures. Traditional combinatorial testing techniques often focus on test case generation to detect the presence of faults, but fault location is an active research area. Each configuration needs to be tested, as each configuration represents a tenant application. Traditional combinatorial testing such as AETG [2] can reveal the existence of faults by using few test cases to support t-way coverage for t  2. But knowing the existence of a fault does not indicate which tway interactions are faulty. When the problem size is small, an engineer can identify faults. However, when the problem is large, it can be a challenge to identify faults. As the IT industries moves to Big Data and cloud computing where hundreds of thousand processors are available, potentially, a large number of processors with distributed databases can be used to perform large-scale combinatorial testing. Furthermore, they also provide concurrent and asynchronous computing mechanisms such as MapReduce, automated redundancy and recovery management, automated resource provisioning, and automated migration for scalability. These capabilities provide significant computing power that was not available before. One simple way of performing combinatorial testing in a cloud environment is: 1) Partition the testing tasks; 2) Allocate these testing tasks to different processors in the cloud platform for test execution; 3) Collect results done by these processors. However, this is not efficient as while the number of computing and storage resources have increased significantly, the number of combinations to be considered is still too high. For example, a large SaaS system may have millions of components, and testing all of these combinations can still consume all the resources in a cloud platform. Two ways to improve this approach and both are based on learning from the previous test results:
·

·

Devise a mechanism to merge test results from different processors so that testing results can be merged quickly, and detect any inconsistency in testing; Based on the existing testing results, eliminate any con-

figurations or interactions from future testing. Due to the asynchronous and autonomous nature of cloud computing, test results may arrive asynchronously and autonomously, and this necessitates a new testing framework. This paper proposes a new algebraic system, Test Algebra (TA), to facilitate concurrent combinatorial testing. The key feature of TA is that the algebraic rules follow the combinatorial structure, and thus can track the test results obtained. The TA can then be used to determine whether a tenant application is faulty, and which interactions need to be tested. The TA is an algebraic system in which elements and operations are formally defined. Each element represents a unique component in the SaaS system, and a set of components represents a tenant application. Assuming each component has been tested by developers, testing a tenant application is equivalent to ensuring that there is no t-way interaction faults for t  2 among the elements in a set. The TA uses the principle that if a t-way interaction is faulty, every (t + 1)-way interaction that contains the t-way interaction as a subset is necessarily faulty. The TA provides guidance for the testing process based on test results so far. Each new test result may indicate if additional tests are needed to test a specific configuration. The TA is an algebraic system, primarily intended to track the test results without knowing how these results were obtained. Specifically, it does not record the execution sequence of previously executed test cases. Because of this, it is possible to allocate different configurations to different processors for execution in parallel or in any order, and the test results are merged following the TA rules. The execution order and the merge order do not affect the merged results if the merging follows the TA operation rules. This paper is structured as follows: Section II discusses the related work; Section III proposes TA and shows its details; and Section IV concludes this paper. Appendix provides proofs of TA associativity properties. II. R ELATED W ORK SaaS testing is a new research topic [14], [4], [10]. Using policies and metadata, test cases can be generated to test SaaS applications. Testing can be embedded in the cloud platform where tenant applications are run [14]. Gao proposed a framework for testing cloud applications [4], and proposed a scalability measure for testing cloud application scalability. Another scalability measure was proposed by [10]. Testing all combinations of inputs and preconditions is not feasible, even with a simple product [6], [8]. The number of defects in a software product can be large, and defects occurring infrequently are difficult to find [15]. Combinatorial test design is used to identify a small number of tests needed to get the coverage of important combinations. Combinatorial test design methods enable one to build structure variation into test cases for having greater test coverage with fewer test cases. Determining the presence of faults caused by a small number of interacting elements has been extensively studied

in component-based software testing. When interactions are to be examined, testing involves a combination-based strategy [5]. When every interaction among t or fewer elements is to be tested, methods have been developed that provide pairwise or t-way coverage. Among the early methods, AET G [2] popularized greedy one-test-at-a-time methods for constructing test suites. In the literature, the test suite is usually called a covering array, defined as follows. Suppose that there are k configurable elements, numbered from 1 to k . Suppose that for element c, there are vc valid options. A t-way interaction is a selection of t of the k configurable elements, and a valid option for each. A test selects a valid option for every element, and it covers a t-way interaction if, when one restricts the attention to the t selected elements, each has the same option in the interaction as it does in the test. A covering array of strength t is a collection of tests so that every t-way interaction is covered by at least one of the tests. Covering arrays reveal faults that arise from improper interaction of t or fewer elements [9]. There are numerous computational and mathematical approaches for construction of covering arrays with a number of tests as small as possible [3], [7]. If a t-way interaction causes a fault, then executing all tests of a covering array will reveal the presence of at least one faulty interaction. SaaS testing is interested in identifying those interactions that are faulty including their numbers and locations, as faulty configurations cannot be used in tenant applications. Furthermore, the number and location of faults keep on changing as new components can be added into the SaaS database continuously. By then executing each test, certain interactions are known not to be faulty, while others appear only in tests that reveal faults, and hence may be faulty. At this point, a classification tree analysis builds decision trees for characterizing possible sets of faults. This classification analysis is then used either to permit a system developer to focus on a small collection of possible faults, or to design additional tests to further restrict the set of possible faults. In [16], empirical results demonstrate the effectiveness of this strategy at limiting the possible faulty interactions to a manageable number. Assuming that interactions of more than t elements do not produce faults, a covering array can use few tests to certify that no fault arises from a t-way interaction. The Adaptive Reasoning algorithm (AR) is a strategy to detect faults in SaaS [13]. The algorithm uses earlier test results to generate new test cases to detect faults in tenant applications. It uses three principles:
·

·

·

Principle 1: When a tenant application (or configuration) fails the testing, there is at least one fault (but there may be more) in the tenant configuration. Principle 2: When a tenant application passes the testing, there is no fault in the tenant configuration resulting from a t-way interactions among components in the configuration. Principle 3: Whenever a configuration contains one or more faulty interactions, it is faulty.

III. T EST A LGEBRA Let C be a finite set of components. A configuration is a subset T  C . One is concerned with determining the operational status of configurations. To do this, one can execute certain tests; every test is a configuration, but there may be restrictions on which configurations can be used as tests. If a certain test can be executed, its execution results in an outcome of passed (operational) or failed (faulty). When a test execution yields result, all configurations that are subsets of the test are operational. However, when a test execution yields a faulty result, one only knows that at least one subset causes the fault, but it is unclear which of these subsets caused the failure. Among a set of configurations that may be responsible for faults, the objective is to determine, which cause faults and which do not. To do this, one must identify the set of candidates to be faulty. Because faults are expected to arise from an interaction among relatively few components, one considers t-way interactions. The t-way interactions are It = {U  C : |U | = t}. Hence the goal is to select tests, so that from the execution results of these tests, one can ascertain the status of all t-way interactions for some fixed small value of t. Because interactions and configurations are represented as subsets, one can use set-theoretic operations such as union, and their associated algebraic properties such as commutativity, associativity, and self-absorption. The structure of subsets and supersets also plays a key role. To permit this classification, one can use a valuation function V , so that for every subset S of components, V (S ) indicates the current knowledge about the operational status consistent with the components in S . The focus is on determining V (S ) whenever S is an interaction in I1  · · ·  It . These interactions can have one of five states. · Infeasible (X): For certain interactions, it may happen that no feasible test is permitted to contain this interaction. For example, it may be infeasible to select two GUI components in one configuration such that one says the wall is GREEN but the other says RED. · Faulty (F): If the interaction has been found to be faulty. · Operational (P): Among the rest, if an interaction has appeared in a test whose execution gave an operational result, the interaction cannot be faulty. · Irrelevant (N): For some feasible interactions, it may be the case that certain interactions are not expected to arise, so while it is possible to run a test containing the interaction, there is no requirement to do so. · Unknown (U): If neither of these occurs then the status of the interaction is required but not currently known. Any given stage of testing, an interaction has one of five possible status indicators. These five status indicators are ordered by X F P N U under a relation , and it has a natural interpretation to be explained in a moment. A. Learning from Previous Test Results The motivation for developing an algebra is to automate the deduction of the status of an interaction from the status

of tests and other interactions, particularly in combining the status of two interactions. Specifically, one is often interested in determining V (T1  T2 ) from V (T1 ) and V (T2 ). To do this, a binary operation  on {X, F, P, N, U} can be defined, with operation table as follows:  X F P N U X X X X X X F X F F F F P X F U N U N X F N N N U X F U N U

Using this definition, one can verify that the binary operation  has the following properties of commutativity and associativity. V (T1 )  V (T2 ) = V (T2 )  V (T1 ), V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). Using this operation, one observes that V (T1  T2 ) V (T1 )  V (T2 ). It follows that 1) Every superset of an infeasible interaction is infeasible. 2) Every superset of a failed interaction is failed or infeasible. 3) Every superset of an irrelevant interaction is irrelevant, failed, passed, or infeasible. A set S is an X-implicant if V (S ) = X but whenever S  S , V (S )  X. The X-implicants provide a compact representation for all interactions that are infeasible. Indeed for any interaction T that contains an X-implicant, V (T ) = X. Furthermore, a set S is an F-implicant if V (S ) = F but whenever S  S , V (S )  F. For any interaction T that contains an F-implicant, V (T ) F. In the same way, a set S is an N-implicant if V (S ) = N but whenever S  S , V (S ) = U. For any interaction T that contains an N-implicant, V (T ) N. An analogous statement holds for passed interactions, but here the implication is for subsets. A set S is a P-implicant if V (S ) = P but whenever S  S , V (S ) F. For any interaction T that is contained in a P-implicant, V (T ) = P. Implicants are defined with respect to the current knowledge about the status of interactions. When a t-way interaction is known to be infeasible, failed, or irrelevant, it must contain an X-, F-, or N-implicant. By repeatedly proceeding from tway to (t + 1)-way interactions, then, one avoids the need for any tests for (t + 1)-way interactions that contain any infeasible, failed, or irrelevant t-way interaction. Hence testing typically proceeds by determining the status of the 1-way interactions, then proceeding to 2-way, 3-way, and so on. The operation  is useful in determining the implied status of (t + 1)-way interactions from the computed results for t-way interactions, by examining unions of the t-way and smaller interactions and determining implications of the rule that V (T1  T2 ) V (T1 )  V (T2 ). Moreover, when adding further interactions to consider, all interactions previously tested that passed are contained in a P-implicant, and every (t + 1) interaction contained in one of these interactions can be assigned status P.

For example, V (a, b) = P, V (a, e) = X, and other 2way interactions exist as atomic interaction in TA. Based on the defined  operation, values of t-way interactions can be deduced from the atomic interactions and their contained interactions, such as V (a, b, e) V (a, b)  V (a, e) = X, i.e. V (a, b, e) = X. The 3-way interaction (a, b, c) can have inferred results from 2-way interactions (a, b), (a, c), (b, c). If any contained 2-way interaction has value F, the determining value of 3-way is F, without further testing needed. But if all values of contained 2-way interactions are P, (a, b, c) the interaction needs to be tested. In this case, U needs to be changed to non-U such as F or P, assuming the 3-way is not X or N. B. Changing Test Result Status When testing a configuration with n components, one should test individual components, 2-way interactions, 3-way interactions, all the way to n-way interactions. Since any combination of interactions is relevant in this case, the status of any interaction can be either X, F, P, or U. The status of a configuration is determined by the status of all interactions. 1) If an interaction has status X (F), the configuration has status X (F). 2) If all interactions have status P, the configuration has status P. 3) If some interactions still have status U, further tests are needed. It is important to determine when an interaction with status U can be deduced to have status F or P instead. It can never obtain status X or N once having had status U. To change U to P: An interaction is assigned status P if and only if it is a subset of a test that leads to proper operation. To change U to F: Consider the candidate T , one can conclude that V (T ) = F if there is a test containing T that yields a failed result, but for every other candidate interaction T that appears in this test, V (T ) = P. In other words, the only possible explanation for the failure is the failure of T . C. Matrix Representation Suppose that each individual component passed the testing. Then the operation table starts from 2-way interactions, then enlarges to t-way interactions step by step. During the procedure, many test results can be deduced from the existing results following TA rules. For example, all possible configurations of (a, b, c, d, e, f ) can be expressed in the form of matrix, or operation table. First, we show the operation table for 2-way interactions. The entries in the operation table are symmetric and those on the main diagonal are not necessary. So only half of the entries are shown. As shown in Figure 1, 3-way interactions can be composed by using 2-way interactions and components. Thus, following the TA implication rules, the 3-way interactions operation table is composed based on the results of 2-way combinations. Here, (a, b, c, d, e, f ) has more 3-way interactions than 2-way interactions. As seen in Figure 1, a 3-way interaction can be obtained through different combinations of 2-way interactions

and components. For example, {a, b, c} = {a}  {b, c} = {b}{a, c} = {c}{a, b} = {a, b}{a, c} = {a, b}{b, c} = {a, c}  {b, c}. V (a)  V (b, c) = V (c)  V (a, b) = V (a, b)  V (b, c) = P  P = U. But V (b)  V (a, c) = V (a, b)  V (a, c) = V (b, c)  V (a, c) = P  F = F. As TA defines the order of the five status indicators, the result should be the value with highest order. So V (a, b, c) = F.  a a b c d e f b P c d e F N X P X N F P F f U F P X U

D. Merging Concurrent Testing Results One way to achieve efficient testing is to allocate (overlapping or non-overlapping) tenant applications into different clusters, and each cluster is sent to a different set of servers for execution. Once each cluster completes its execution, the test results can be merged. The testing results of a specific interaction T in different servers should satisfy the following constraints. · If V (T ) = U in one cluster, then in other clusters, the same V (T ) can be either F, P, N, or U. · If V (T ) = N in one cluster, then in other clusters, the same V (T ) can be either F, P, N, or U. · If V (T ) = P in one cluster, then the same V (T ) can be either P, N, or U in all clusters; · If V (T ) = F in one cluster, then in other clusters, the same V (T ) can be F, N, or U. · If V (T ) = X in one cluster, then in other clusters, the same V (T ) can be X only. If these constraints are satisfied, then the testing results can be merged. Otherwise, there must be an error in the testing results. To represent this situation, a new status indicator, error (E), is introduced and E X. We define a binary operation  on {E, X, F, P, N, U}, with operation table as follows:  E X F P N U E E E E E E E X E X E E E E F E E F E F F P E E E P P P N E E F P N U U E E F P U U

 also has the properties of commutativity and associativity. See Appendix for proof of associativity. Using this operation, merging two testing results from two different servers can be defined as Vmerged (T ) = Vcluster1 (T )  Vcluster2 (T ). The merge can be performed in any order due to the commutativity and associativity of , and if the constraints of merge are satisfied and V (T ) = X, F, or P, the results cannot be changed by any further testing or merging of test results unless there are some errors in testing. If V (T ) = E, the testing

 a b c . . . f (a, b) (a, c) . . . (b, c) . . . (e, f )

a (a)

b (a, b) (b)

c (a, c) (b, c) (c)

··· ··· ··· ··· .. .

f (a, f ) (b, f ) (c, f ) . . . (f )

(a, b) (a, b) (a, b) (a, b, c) . . . (a, b, f ) (a, b)

(a, c) (a, c) (a, b, c) (a, c) . . .

··· ··· ··· ··· . . .

(b, c) (a, b, c) (b, c) (b, c) . . .

··· ··· ··· ··· . . .

(e, f ) (a, e, f ) (b, e, f ) (c, e, f ) . . . (e, f ) (a, b, e, f ) (a, c, e, f ) . . . (b, c, e, f ) . . . (e, f )

(a, c, f ) · · · (a, b, c) · · · (a, c) ··· .. .

(b, c, f ) · · · (a, b, c) · · · (a, b, c) · · · . . . . . . (b, c) ··· .. . ··· ··· ··· ··· . . . ··· ··· ··· . . . ··· .. . (e, f ) U U U . . . U U F . . . U . . .

 a b c . . . f (a, b) (a, c) . . . (b, c) . . . (e, f )

a

b P

c F P

··· ··· ··· ··· .. .

f U F P . . .

(a, b) U U U . . . U

(a, c) F F F . . . F F

··· ··· ··· ··· . . . ··· ··· ··· .. .

(b, c) U U U . . . U U F . . .

Fig. 1. 3-way interaction operation table

environment must be corrected and tests executed again after fixing the error(s) in testing. For example, Vcluster1 (a, c, e) = X and Vcluster2 (a, c, e) = F, so Vmerged (a, c, e) = X  F = E. It means that there is something wrong with the tests of interaction (a, c, e), and the problem must be fixed before doing further testing. Following the  associative rule, one can derive the following. V1 (T )  V2 (T )  V3 (T ) = (V1 (T )  V2 (T ))  V3 (T ) = V1 (T )  (V2 (T )  V3 (T )) = V1 (T )  V2 (T )  V3 (T )  V3 (T ) = (V1 (T )  V2 (T ))  (V2 (T )  V3 (T )) = ((V1 (T )  V2 (T ))  V2 (T ))  V3 (T ) = (V3 (T )  V2 (T ))  (V3 (T )  V1 (T )) Thus the  rule allows one to partition the configurations into different sets for different servers to run testing, and these sets do not need to be non-overlapping. In conventional cloud computing operations such as MapReduce, data should not overlap, otherwise incorrect data may be produced. For example, counting the items in a set can be performed by MapReduce, but data allocated to different servers cannot overlap, otherwise items may be counted more than once. In TA, this is not a concern due to the nature of the TA operations

. Once the results are available from each server, the testing results can be merged either incrementally, in parallel, or in any order. Furthermore, test results can be merged repeatedly without changing the final results. Using this approach, one can test say 10 batches of 100 tenant applications each, for a total of 1, 000 tenant applications. Once this is done, another batch of 1, 000 tenant applications can be tested with each 100 tenant application allocated to a server for execution. In this way, after running 100 batches, 100, 000 tenant applications can be evaluated completely. The following example illustrates the testing process of fifteen configurations. According to the definition of configuration testing, all feasible interactions should be tested. For simplicity, assume that only interaction (c, d, f ) is faulty, and only interaction (c, d, e) is infeasible, and all other interactions pass the testing. If one assigns configurations 1, 3, 5, 7, 9, 11, 13, 15 into Server1 , configurations 2, 4, 6, 8, 10, 12, 14 into Server2 , and 4-11 configurations into Server3 . If Server1 and Server3 do their own testing first, Server2 can reuse test results of interactions from them to eliminate interactions that need to be tested. For example, when testing 2-way interactions of configuration (b, c, d, f ) in Server2 , it can reuse the test results of (b, c), (b, d) of configuration (b, c, d, e) from Server3 , (b, f ) of configuration (a, b, c, f )

from Server1 . They are all passed, and it can reuse the test results of (b, c, d) of configuration (a, b, c, d) from Server1 , (b, c, f ) of configuration (a, b, c, f ) from Server1 , (b, d, f ) of configuration (a, b, d, f ) from Server1 , and (c, d, f ) of configuration (a, c, d, f ) from Server3 . Because (c, d, f ) is faulty, it can deduce that 4-way interaction (b, c, d, f ) is also faulty. For the sets of configuration that are overlapping, their returned test results from different servers are the same. The merged results of these results also stay the same. Not only interactions, sets of configurations, CS1 , CS2 , . . . , CSK can be allocated to different processors (or clusters) for testing, and the test results can then be merged. The sets can be non-overlapping or overlapping, and the merge process can be arbitrary. For example, say the result of CSi is RCSi , the merge process can be (· · · ((((RCS1 + RCS2 ) + RCS3 ) + RCS4 ) + · · · + RCSK ), or (· · · ((((RCSK + RCSk-1 ) + RCSk-2 ) + · · · + RCS1 ), or any other sequence that includes all RCSi , for i = 1 to K . This is true because RCS is simply a set of V (Tj ) for any intercation Tj in the configuration CSi . (a,b,c,d) (a,b,c,e) (a,b,c,f) (a,b,d,e) (a,b,d,f) (a,b,e,f) (a,c,d,e) (a,c,d,f) (a,c,e,f) (a,d,e,f) (b,c,d,e) (b,c,d,f) (b,c,e,f) (b,d,e,f) (c,d,e,f) Server1 P P P P P X F P P X F P P X P P P X F P P X Server2 P Server3

IV. C ONCLUSION This paper proposes TA to address SaaS combinatorial testing. The TA provides a foundation for concurrent combinatorial testing. The TA has two operations and test results can have five states with a priority. By using the TA operations, many combinatorial tests can be eliminated as the TA identifies those interactions that need not be tested. Also the TA defines operation rules to merge test results done by different processors, so that combinatorial tests can be done in a concurrent manner. The TA rules ensure that either merged results are consistent or a testing error has been detected so that retest is needed. In this way, large-scale combinatorial testing can be carried out in a cloud platform with a large number of processors to perform test execution in parallel to identify faulty interactions. ACKNOWLEDGMENT

This project is sponsored by U.S. National Science Foundation project DUE 0942453 and National Science Foundation M erged Results China (No.61073003), National Basic Research Program of China (No.2011CB302505), and the State Key Laboratory of P Software Development Environment (No. SKLSDE-2012ZXP 18), and Fujitsu Laboratory. P P R EFERENCES P [1] X. Bai, M. Li, B. Chen, W.-T. Tsai, and J. Gao. Cloud Testing Tools. In P Proceedings of IEEE 6th International Symposium on Service Oriented X System Engineering (SOSE), pages 1­12, Irvine, CA, USA, 2011. F [2] D. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG System: An Approach to Testing Based on Combinatorial Design. P Journal of IEEE Transactions on Software Engineering, 23:437­444, P 1997. X [3] C. J. Colbourn. Covering arrays and hash families. In D. Crnkovic and V. D. Tonchev, editors, Information Security, Coding Theory and Related F Combinatorics, volume 29 of NATO Science for Peace and Security P Series - D: Information and Communication Security. IOS Press, 2011. P [4] J. Gao, X. Bai, and W.-T. Tsai. SaaS Performance and Scalability Evaluation in Cloud. In Proceedings of The 6th IEEE International X
Symposium on Service Oriented System Engineering, SOSE '11, 2011. [5] M. Grindal, J. Offutt, and S. F. Andler. Combination Testing Strategies: A Survey. Software Testing, Verification, and Reliability, 15:167­199, 2005. [6] C. Kaner, J. Falk, and H. Q. Nguyen. Testing Computer Software, 2nd Edition. Wiley, New York, NY, USA, 1999. [7] V. V. Kuliamin and A. A. Petukhov. A Survey of Methods for Constructing Covering Arrays. Journal of Program Computer Software, 37(3):121­146, may 2011. [8] T. Muller and D. Friedenberg. Certified Tester Foundation Level Syllabus. Journal of International Software Testing Qualifications Board. [9] A. A. Porter, C. Yilmaz, A. M. Memon, D. C. Schmidt, and B. Natarajan. Skoll: A Process and Infrastructure for Distributed Continuous Quality Assurance. IEEE Transactions on Software Engineering, 33(8):510­525, 2007. [10] W.-T. Tsai, Y. Huang, X. Bai, and J. Gao. Scalable Architecture for SaaS. In Proceedings of 15th IEEE International Symposium on Object Component Service-oriented Real-time Distributed Computing, ISORC '12, Apr. 2012. [11] W.-T. Tsai, Y. Huang, and Q. Shao. Testing the Scalability of SaaS Applications. In Proceedings of IEEE International Conference on ServiceOriented Computing and Applications (SOCA), pages 1­4, Irvine, CA, USA, 2011. [12] W.-T. Tsai, Y. Huang, Q. Shao, and X. Bai. Data Partitioning and Redundancy Management for Robust Multi-Tenancy SaaS. International Journal of Software and Informatics (IJSI), 4(3):437­471, 2010.

E. Modified Testing Process Perform 2-way interaction testing first. Before going on to 3way interaction testing, use the results of 2-way testing to eliminate cases. The testing process stops when finishing testing all t-way interactions. The analysis of t-way interactions is based on the P T Rs of all (t - i)-way interactions for 1  i < t. The superset of infeasible, irrelevant, and faulty test cases do not need to be tested. The test results of the superset can be obtained by TA operations and must be infeasible, irrelevant, or faulty. But the superset of test cases with unknown indicator must be tested. In this way, a large repeating testing workload can be reduced. For n components, all t-way interactions for t  2 are composed by 2-way, 3-way, ..., t-way interactions. In n components combinatorial testing, the number of 2-way interactions is equal to n 2 . In general, the number of t-way interactions is equal to n t . More interactions are treated when n n > , which happens when t  n 2 . The total number t t-1 t of interactions examined is i=2 n . i

[13] W.-T. Tsai, Q. Li, C. J. Colbourn, and X. Bai. Adaptive Fault Detection for Testing Tenant Applications in Multi-Tenancy SaaS Systems. In Proceedings of IEEE International Conference on Cloud Engineering (IC2E), March 2013. [14] W.-T. Tsai, Q. Shao, and W. Li. OIC: Ontology-based Intelligent Customization Framework for SaaS. In Proceedings of International Conference on Service Oriented Computing and Applications(SOCA'10), Perth, Australia, Dec. 2010. [15] Wikipedia. Software Testing, 2013. [16] C. Yilmaz, M. B. Cohen, and A. Porter. Covering Arrays for Efficient Fault Characterization in Complex Configuration Spaces. In Proceedings of the 2004 ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA '04, pages 45­54, New York, NY, USA, 2004. ACM.

A PPENDIX The associativity of binary operation . V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). Proof: We will prove this property in the following cases. (1) At least one of V (T1 ), V (T2 ), and V (T3 ) is X. Without loss of generality, suppose that V (T1 ) = X, then according to the operation table of , V (T1 )  (V (T2 )  V (T3 )) = X  (V (T2 )  V (T3 )) = X, (V (T1 )  V (T2 ))  V (T3 ) = (X  V (T2 ))  V (T3 ) = X  V (T3 ) = X. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (2) V (T1 ), V (T2 ), and V (T3 ) are not X and at least one of V (T1 ), V (T2 ), and V (T3 ) is F. Without loss of generality, suppose that V (T1 ) = F, then according to the operation table of , the value of V (T2 )  V (T3 ) can only be F, N or U. So V (T1 )  (V (T2 )  V (T3 )) = F  (V (T2 )  V (T3 )) = F, (V (T1 )  V (T2 ))  V (T3 ) = (F  V (T2 ))  V (T3 ) = F  V (T3 ) = F. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (3) V (T1 ), V (T2 ), and V (T3 ) are not X or F and at least one of V (T1 ), V (T2 ), and V (T3 ) is N. Without loss of generality, suppose that V (T1 ) = N, then according to the operation table of , the value of V (T2 )  V (T3 ) can only be N or U. So V (T1 )  (V (T2 )  V (T3 )) = N  (V (T2 )  V (T3 )) = N, (V (T1 )  V (T2 ))  V (T3 ) = (N  V (T2 ))  V (T3 ) = N  V (T3 ) = N. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (4) V (T1 ), V (T2 ), and V (T3 ) are not X, F or N. In this case, V (T1 ), V (T2 ), and V (T3 ) can only be P or U. According to the operation table of , the value of V (T1 )  V (T2 ) and V (T2 )  V (T3 ) are U. So V (T1 )  (V (T2 )  V (T3 )) = V (T1 )  U = U, (V (T1 )  V (T2 ))  V (T3 ) = U  V (T3 ) = U. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). The associativity of binary operation . V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ). Proof: We will prove this property in the following cases. (1) One of V1 (T ), V2 (T ), and V3 (T ) is E. Without loss of generality, suppose that V1 (T ) = E, then according to the operation table of , V1 (T )(V2 (T )V3 (T )) = E(V2 (T ) V3 (T )) = E, (V1 (T )  V2 (T ))  V3 (T ) = (E  V2 (T ))  V3 (T ) = E  V3 (T ) = E. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ).

(2) V1 (T ), V2 (T ), and V3 (T ) are not E, and there is a pair of V1 (T ), V2 (T ), and V3 (T ) does not satisfy the constrains. Without loss of generality, suppose that V1 (T ) and V2 (T ) does not satisfy the constrains, then according to the operation table of , V1 (T )  V2 (T ) = E. So (V1 (T )  V2 (T ))  V3 (T ) = E  V3 (T ) = E. Since V1 (T ) and V2 (T ) does not satisfy the constrains, there can be two cases: (a) one of them is X and the other is not, or (b) one of them is P and the other is F. (a) If V1 (T ) = X, then V2 (T )  V3 (T ) cannot be X because V2 (T ) cannot be X. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. If V2 (T ) = X, then V2 (T )  V3 (T ) = X can only be E or X. Since V1 (T ) cannot be X, V1 (T )  (V2 (T )  V3 (T )) = E. (b) If V1 (T ) = P and V2 (T ) = F, then V2 (T )  V3 (T ) can only be E or F. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. If V1 (T ) = F and V2 (T ) = P, then V2 (T )  V3 (T ) can only be E or P. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ). (3) V1 (T ), V2 (T ), and V3 (T ) are not E, and V1 (T ), V2 (T ), and V3 (T ) satisfy the constrains. (a) One of V1 (T ), V2 (T ), and V3 (T ) is X. Without loss of generality, suppose that V1 (T ) = X, then V2 (T ) = V3 (T ) = X. So V1 (T )  (V2 (T )  V3 (T )) = X  (X  X) = X  X = X and (V1 (T )  V2 (T ))  V3 (T ) = (X  X)  X = X  X = X. (b) V1 (T ), V2 (T ), and V3 (T ) are not X, and one of V1 (T ), V2 (T ), and V3 (T ) is F. Without loss of generality, suppose that V1 (T ) = F, then V2 (T ) and V3 (T ) can only be F, N, or U. According to operation table of , V2 (T )  V3 (T ) can only be F, N, or U, and V1 (T )  V2 (T ) can only be F. So V1 (T )  (V2 (T )  V3 (T )) = F  (V2 (T )  V3 (T )) = F and (V1 (T )  V2 (T ))  V3 (T ) = F  V3 (T ) = F. (c) V1 (T ), V2 (T ), and V3 (T ) are not X or F, and one of V1 (T ), V2 (T ), and V3 (T ) is P. Without loss of generality, suppose that V1 (T ) = P, then V2 (T ) and V3 (T ) can only be P, N, or U. According to operation table of , V2 (T )  V3 (T ) can only be P, N, or U, and V1 (T )  V2 (T ) can only be F. So V1 (T )  (V2 (T )  V3 (T )) = P  (V2 (T )  V3 (T )) = P and (V1 (T )  V2 (T ))  V3 (T ) = P  V3 (T ) = P. (d) V1 (T ), V2 (T ), and V3 (T ) are not X, F or P, and one of V1 (T ), V2 (T ), and V3 (T ) is U. Without loss of generality, suppose that V1 (T ) = U, then V2 (T ) and V3 (T ) can only be N, or U. According to operation table of , V2 (T )  V3 (T ) can only be N, or U, and V1 (T )  V2 (T ) can only be U. So V1 (T )  (V2 (T )  V3 (T )) = U  (V2 (T )  V3 (T )) = U and (V1 (T )  V2 (T ))  V3 (T ) = U  V3 (T ) = U. (e) V1 (T ), V2 (T ), and V3 (T ) are N. V1 (T )  (V2 (T )  V3 (T )) = N  (N  N) = N  N = N and (V1 (T )  V2 (T ))  V3 (T ) = (N  N)  N = N  N = N. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ).

Test Algebra for Combinatorial Testing
Wei-Tek Tsai , Charles J. Colbourn , Jie Luo , Guanqiu Qi , Qingyang Li , Xiaoying Bai
 School

of Computing, Informatics, and Decision Systems Engineering Arizona State University, Tempe, AZ, USA  State Key Laboratory of Software Development Environment School of Computer Science and Engineering, Beihang University, Beijing, China  Department of Computer Science and Technology, INLIST Tsinghua University, Beijing, China {wtsai, colbourn}@asu.edu, luojie@nlsde.buaa.edu.cn {guanqiuq, qingyan2b}@asu.edu, baixy@tsinghua.edu.cn

Abstract--This paper proposes a new algebraic system, Test Algebra (T A), for identifying faults in combinatorial testing for SaaS (Software-as-a-Service) applications. SaaS as a part of cloud computing is a new software delivery model, and mission-critical applications are composed, deployed, and executed in cloud platforms. Testing SaaS applications is a challenging task because new applications need to be tested when they are composed before they can be deployed for execution. Combinatorial testing algorithms can be used to identify faulty configurations and interactions from 2-way all the way to k-way where k is the number of components in the application. The T A defines rules to identify faulty configurations and interactions. Using the rules defined in the T A, a collection of configurations can be tested concurrently in different servers and in any order and the results obtained will be still same due to the algebraic constraints. Index Terms--Combinatorial testing, algebra, SaaS

I. I NTRODUCTION Software-as-a-Service (SaaS) is a new software delivery model. SaaS often supports three features: customization, multi-tenancy architecture (MTA), and scalability. MTA means using one code base to develop multiple tenant applications, and each tenant application essentially is a customization of the base code [12]. A SaaS system often also supports scalability as it can supply additional computing resources when the workload is heavy. Tenants' applications are often customized by using components stored in the SaaS database [14], [1], [11] including GUI, workflow, service, and data components. Once tenant applications are composed, they need to be tested, but a SaaS system can have millions of components, and hundreds of thousands of tenant applications. Testing tenant applications becomes a challenge as new tenant applications and components are added into the SaaS system continuously. New tenant applications are added on a daily basis while other tenant applications are running on the SaaS platform. As new tenant applications are composed, new components are added into the SaaS system. Each tenant application represents a customer for the SaaS system, and thus it needs to be tested. Combinatorial testing is a popular testing technique to test an application with different configurations. It often assumes that each component within a configuration has been tested

already, but interactions among components in the configuration may cause failures. Traditional combinatorial testing techniques often focus on test case generation to detect the presence of faults, but fault location is an active research area. Each configuration needs to be tested, as each configuration represents a tenant application. Traditional combinatorial testing such as AETG [2] can reveal the existence of faults by using few test cases to support t-way coverage for t  2. But knowing the existence of a fault does not indicate which tway interactions are faulty. When the problem size is small, an engineer can identify faults. However, when the problem is large, it can be a challenge to identify faults. As the IT industries moves to Big Data and cloud computing where hundreds of thousand processors are available, potentially, a large number of processors with distributed databases can be used to perform large-scale combinatorial testing. Furthermore, they also provide concurrent and asynchronous computing mechanisms such as MapReduce, automated redundancy and recovery management, automated resource provisioning, and automated migration for scalability. These capabilities provide significant computing power that was not available before. One simple way of performing combinatorial testing in a cloud environment is: 1) Partition the testing tasks; 2) Allocate these testing tasks to different processors in the cloud platform for test execution; 3) Collect results done by these processors. However, this is not efficient as while the number of computing and storage resources have increased significantly, the number of combinations to be considered is still too high. For example, a large SaaS system may have millions of components, and testing all of these combinations can still consume all the resources in a cloud platform. Two ways to improve this approach and both are based on learning from the previous test results:
·

·

Devise a mechanism to merge test results from different processors so that testing results can be merged quickly, and detect any inconsistency in testing; Based on the existing testing results, eliminate any con-

figurations or interactions from future testing. Due to the asynchronous and autonomous nature of cloud computing, test results may arrive asynchronously and autonomously, and this necessitates a new testing framework. This paper proposes a new algebraic system, Test Algebra (TA), to facilitate concurrent combinatorial testing. The key feature of TA is that the algebraic rules follow the combinatorial structure, and thus can track the test results obtained. The TA can then be used to determine whether a tenant application is faulty, and which interactions need to be tested. The TA is an algebraic system in which elements and operations are formally defined. Each element represents a unique component in the SaaS system, and a set of components represents a tenant application. Assuming each component has been tested by developers, testing a tenant application is equivalent to ensuring that there is no t-way interaction faults for t  2 among the elements in a set. The TA uses the principle that if a t-way interaction is faulty, every (t + 1)-way interaction that contains the t-way interaction as a subset is necessarily faulty. The TA provides guidance for the testing process based on test results so far. Each new test result may indicate if additional tests are needed to test a specific configuration. The TA is an algebraic system, primarily intended to track the test results without knowing how these results were obtained. Specifically, it does not record the execution sequence of previously executed test cases. Because of this, it is possible to allocate different configurations to different processors for execution in parallel or in any order, and the test results are merged following the TA rules. The execution order and the merge order do not affect the merged results if the merging follows the TA operation rules. This paper is structured as follows: Section II discusses the related work; Section III proposes TA and shows its details; and Section IV concludes this paper. Appendix provides proofs of TA associativity properties. II. R ELATED W ORK SaaS testing is a new research topic [14], [4], [10]. Using policies and metadata, test cases can be generated to test SaaS applications. Testing can be embedded in the cloud platform where tenant applications are run [14]. Gao proposed a framework for testing cloud applications [4], and proposed a scalability measure for testing cloud application scalability. Another scalability measure was proposed by [10]. Testing all combinations of inputs and preconditions is not feasible, even with a simple product [6], [8]. The number of defects in a software product can be large, and defects occurring infrequently are difficult to find [15]. Combinatorial test design is used to identify a small number of tests needed to get the coverage of important combinations. Combinatorial test design methods enable one to build structure variation into test cases for having greater test coverage with fewer test cases. Determining the presence of faults caused by a small number of interacting elements has been extensively studied

in component-based software testing. When interactions are to be examined, testing involves a combination-based strategy [5]. When every interaction among t or fewer elements is to be tested, methods have been developed that provide pairwise or t-way coverage. Among the early methods, AET G [2] popularized greedy one-test-at-a-time methods for constructing test suites. In the literature, the test suite is usually called a covering array, defined as follows. Suppose that there are k configurable elements, numbered from 1 to k . Suppose that for element c, there are vc valid options. A t-way interaction is a selection of t of the k configurable elements, and a valid option for each. A test selects a valid option for every element, and it covers a t-way interaction if, when one restricts the attention to the t selected elements, each has the same option in the interaction as it does in the test. A covering array of strength t is a collection of tests so that every t-way interaction is covered by at least one of the tests. Covering arrays reveal faults that arise from improper interaction of t or fewer elements [9]. There are numerous computational and mathematical approaches for construction of covering arrays with a number of tests as small as possible [3], [7]. If a t-way interaction causes a fault, then executing all tests of a covering array will reveal the presence of at least one faulty interaction. SaaS testing is interested in identifying those interactions that are faulty including their numbers and locations, as faulty configurations cannot be used in tenant applications. Furthermore, the number and location of faults keep on changing as new components can be added into the SaaS database continuously. By then executing each test, certain interactions are known not to be faulty, while others appear only in tests that reveal faults, and hence may be faulty. At this point, a classification tree analysis builds decision trees for characterizing possible sets of faults. This classification analysis is then used either to permit a system developer to focus on a small collection of possible faults, or to design additional tests to further restrict the set of possible faults. In [16], empirical results demonstrate the effectiveness of this strategy at limiting the possible faulty interactions to a manageable number. Assuming that interactions of more than t elements do not produce faults, a covering array can use few tests to certify that no fault arises from a t-way interaction. The Adaptive Reasoning algorithm (AR) is a strategy to detect faults in SaaS [13]. The algorithm uses earlier test results to generate new test cases to detect faults in tenant applications. It uses three principles:
·

·

·

Principle 1: When a tenant application (or configuration) fails the testing, there is at least one fault (but there may be more) in the tenant configuration. Principle 2: When a tenant application passes the testing, there is no fault in the tenant configuration resulting from a t-way interactions among components in the configuration. Principle 3: Whenever a configuration contains one or more faulty interactions, it is faulty.

III. T EST A LGEBRA Let C be a finite set of components. A configuration is a subset T  C . One is concerned with determining the operational status of configurations. To do this, one can execute certain tests; every test is a configuration, but there may be restrictions on which configurations can be used as tests. If a certain test can be executed, its execution results in an outcome of passed (operational) or failed (faulty). When a test execution yields result, all configurations that are subsets of the test are operational. However, when a test execution yields a faulty result, one only knows that at least one subset causes the fault, but it is unclear which of these subsets caused the failure. Among a set of configurations that may be responsible for faults, the objective is to determine, which cause faults and which do not. To do this, one must identify the set of candidates to be faulty. Because faults are expected to arise from an interaction among relatively few components, one considers t-way interactions. The t-way interactions are It = {U  C : |U | = t}. Hence the goal is to select tests, so that from the execution results of these tests, one can ascertain the status of all t-way interactions for some fixed small value of t. Because interactions and configurations are represented as subsets, one can use set-theoretic operations such as union, and their associated algebraic properties such as commutativity, associativity, and self-absorption. The structure of subsets and supersets also plays a key role. To permit this classification, one can use a valuation function V , so that for every subset S of components, V (S ) indicates the current knowledge about the operational status consistent with the components in S . The focus is on determining V (S ) whenever S is an interaction in I1  · · ·  It . These interactions can have one of five states. · Infeasible (X): For certain interactions, it may happen that no feasible test is permitted to contain this interaction. For example, it may be infeasible to select two GUI components in one configuration such that one says the wall is GREEN but the other says RED. · Faulty (F): If the interaction has been found to be faulty. · Operational (P): Among the rest, if an interaction has appeared in a test whose execution gave an operational result, the interaction cannot be faulty. · Irrelevant (N): For some feasible interactions, it may be the case that certain interactions are not expected to arise, so while it is possible to run a test containing the interaction, there is no requirement to do so. · Unknown (U): If neither of these occurs then the status of the interaction is required but not currently known. Any given stage of testing, an interaction has one of five possible status indicators. These five status indicators are ordered by X F P N U under a relation , and it has a natural interpretation to be explained in a moment. A. Learning from Previous Test Results The motivation for developing an algebra is to automate the deduction of the status of an interaction from the status

of tests and other interactions, particularly in combining the status of two interactions. Specifically, one is often interested in determining V (T1  T2 ) from V (T1 ) and V (T2 ). To do this, a binary operation  on {X, F, P, N, U} can be defined, with operation table as follows:  X F P N U X X X X X X F X F F F F P X F U N U N X F N N N U X F U N U

Using this definition, one can verify that the binary operation  has the following properties of commutativity and associativity. V (T1 )  V (T2 ) = V (T2 )  V (T1 ), V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). Using this operation, one observes that V (T1  T2 ) V (T1 )  V (T2 ). It follows that 1) Every superset of an infeasible interaction is infeasible. 2) Every superset of a failed interaction is failed or infeasible. 3) Every superset of an irrelevant interaction is irrelevant, failed, passed, or infeasible. A set S is an X-implicant if V (S ) = X but whenever S  S , V (S )  X. The X-implicants provide a compact representation for all interactions that are infeasible. Indeed for any interaction T that contains an X-implicant, V (T ) = X. Furthermore, a set S is an F-implicant if V (S ) = F but whenever S  S , V (S )  F. For any interaction T that contains an F-implicant, V (T ) F. In the same way, a set S is an N-implicant if V (S ) = N but whenever S  S , V (S ) = U. For any interaction T that contains an N-implicant, V (T ) N. An analogous statement holds for passed interactions, but here the implication is for subsets. A set S is a P-implicant if V (S ) = P but whenever S  S , V (S ) F. For any interaction T that is contained in a P-implicant, V (T ) = P. Implicants are defined with respect to the current knowledge about the status of interactions. When a t-way interaction is known to be infeasible, failed, or irrelevant, it must contain an X-, F-, or N-implicant. By repeatedly proceeding from tway to (t + 1)-way interactions, then, one avoids the need for any tests for (t + 1)-way interactions that contain any infeasible, failed, or irrelevant t-way interaction. Hence testing typically proceeds by determining the status of the 1-way interactions, then proceeding to 2-way, 3-way, and so on. The operation  is useful in determining the implied status of (t + 1)-way interactions from the computed results for t-way interactions, by examining unions of the t-way and smaller interactions and determining implications of the rule that V (T1  T2 ) V (T1 )  V (T2 ). Moreover, when adding further interactions to consider, all interactions previously tested that passed are contained in a P-implicant, and every (t + 1) interaction contained in one of these interactions can be assigned status P.

For example, V (a, b) = P, V (a, e) = X, and other 2way interactions exist as atomic interaction in TA. Based on the defined  operation, values of t-way interactions can be deduced from the atomic interactions and their contained interactions, such as V (a, b, e) V (a, b)  V (a, e) = X, i.e. V (a, b, e) = X. The 3-way interaction (a, b, c) can have inferred results from 2-way interactions (a, b), (a, c), (b, c). If any contained 2-way interaction has value F, the determining value of 3-way is F, without further testing needed. But if all values of contained 2-way interactions are P, (a, b, c) the interaction needs to be tested. In this case, U needs to be changed to non-U such as F or P, assuming the 3-way is not X or N. B. Changing Test Result Status When testing a configuration with n components, one should test individual components, 2-way interactions, 3-way interactions, all the way to n-way interactions. Since any combination of interactions is relevant in this case, the status of any interaction can be either X, F, P, or U. The status of a configuration is determined by the status of all interactions. 1) If an interaction has status X (F), the configuration has status X (F). 2) If all interactions have status P, the configuration has status P. 3) If some interactions still have status U, further tests are needed. It is important to determine when an interaction with status U can be deduced to have status F or P instead. It can never obtain status X or N once having had status U. To change U to P: An interaction is assigned status P if and only if it is a subset of a test that leads to proper operation. To change U to F: Consider the candidate T , one can conclude that V (T ) = F if there is a test containing T that yields a failed result, but for every other candidate interaction T that appears in this test, V (T ) = P. In other words, the only possible explanation for the failure is the failure of T . C. Matrix Representation Suppose that each individual component passed the testing. Then the operation table starts from 2-way interactions, then enlarges to t-way interactions step by step. During the procedure, many test results can be deduced from the existing results following TA rules. For example, all possible configurations of (a, b, c, d, e, f ) can be expressed in the form of matrix, or operation table. First, we show the operation table for 2-way interactions. The entries in the operation table are symmetric and those on the main diagonal are not necessary. So only half of the entries are shown. As shown in Figure 1, 3-way interactions can be composed by using 2-way interactions and components. Thus, following the TA implication rules, the 3-way interactions operation table is composed based on the results of 2-way combinations. Here, (a, b, c, d, e, f ) has more 3-way interactions than 2-way interactions. As seen in Figure 1, a 3-way interaction can be obtained through different combinations of 2-way interactions

and components. For example, {a, b, c} = {a}  {b, c} = {b}{a, c} = {c}{a, b} = {a, b}{a, c} = {a, b}{b, c} = {a, c}  {b, c}. V (a)  V (b, c) = V (c)  V (a, b) = V (a, b)  V (b, c) = P  P = U. But V (b)  V (a, c) = V (a, b)  V (a, c) = V (b, c)  V (a, c) = P  F = F. As TA defines the order of the five status indicators, the result should be the value with highest order. So V (a, b, c) = F.  a a b c d e f b P c d e F N X P X N F P F f U F P X U

D. Merging Concurrent Testing Results One way to achieve efficient testing is to allocate (overlapping or non-overlapping) tenant applications into different clusters, and each cluster is sent to a different set of servers for execution. Once each cluster completes its execution, the test results can be merged. The testing results of a specific interaction T in different servers should satisfy the following constraints. · If V (T ) = U in one cluster, then in other clusters, the same V (T ) can be either F, P, N, or U. · If V (T ) = N in one cluster, then in other clusters, the same V (T ) can be either F, P, N, or U. · If V (T ) = P in one cluster, then the same V (T ) can be either P, N, or U in all clusters; · If V (T ) = F in one cluster, then in other clusters, the same V (T ) can be F, N, or U. · If V (T ) = X in one cluster, then in other clusters, the same V (T ) can be X only. If these constraints are satisfied, then the testing results can be merged. Otherwise, there must be an error in the testing results. To represent this situation, a new status indicator, error (E), is introduced and E X. We define a binary operation  on {E, X, F, P, N, U}, with operation table as follows:  E X F P N U E E E E E E E X E X E E E E F E E F E F F P E E E P P P N E E F P N U U E E F P U U

 also has the properties of commutativity and associativity. See Appendix for proof of associativity. Using this operation, merging two testing results from two different servers can be defined as Vmerged (T ) = Vcluster1 (T )  Vcluster2 (T ). The merge can be performed in any order due to the commutativity and associativity of , and if the constraints of merge are satisfied and V (T ) = X, F, or P, the results cannot be changed by any further testing or merging of test results unless there are some errors in testing. If V (T ) = E, the testing

 a b c . . . f (a, b) (a, c) . . . (b, c) . . . (e, f )

a (a)

b (a, b) (b)

c (a, c) (b, c) (c)

··· ··· ··· ··· .. .

f (a, f ) (b, f ) (c, f ) . . . (f )

(a, b) (a, b) (a, b) (a, b, c) . . . (a, b, f ) (a, b)

(a, c) (a, c) (a, b, c) (a, c) . . .

··· ··· ··· ··· . . .

(b, c) (a, b, c) (b, c) (b, c) . . .

··· ··· ··· ··· . . .

(e, f ) (a, e, f ) (b, e, f ) (c, e, f ) . . . (e, f ) (a, b, e, f ) (a, c, e, f ) . . . (b, c, e, f ) . . . (e, f )

(a, c, f ) · · · (a, b, c) · · · (a, c) ··· .. .

(b, c, f ) · · · (a, b, c) · · · (a, b, c) · · · . . . . . . (b, c) ··· .. . ··· ··· ··· ··· . . . ··· ··· ··· . . . ··· .. . (e, f ) U U U . . . U U F . . . U . . .

 a b c . . . f (a, b) (a, c) . . . (b, c) . . . (e, f )

a

b P

c F P

··· ··· ··· ··· .. .

f U F P . . .

(a, b) U U U . . . U

(a, c) F F F . . . F F

··· ··· ··· ··· . . . ··· ··· ··· .. .

(b, c) U U U . . . U U F . . .

Fig. 1. 3-way interaction operation table

environment must be corrected and tests executed again after fixing the error(s) in testing. For example, Vcluster1 (a, c, e) = X and Vcluster2 (a, c, e) = F, so Vmerged (a, c, e) = X  F = E. It means that there is something wrong with the tests of interaction (a, c, e), and the problem must be fixed before doing further testing. Following the  associative rule, one can derive the following. V1 (T )  V2 (T )  V3 (T ) = (V1 (T )  V2 (T ))  V3 (T ) = V1 (T )  (V2 (T )  V3 (T )) = V1 (T )  V2 (T )  V3 (T )  V3 (T ) = (V1 (T )  V2 (T ))  (V2 (T )  V3 (T )) = ((V1 (T )  V2 (T ))  V2 (T ))  V3 (T ) = (V3 (T )  V2 (T ))  (V3 (T )  V1 (T )) Thus the  rule allows one to partition the configurations into different sets for different servers to run testing, and these sets do not need to be non-overlapping. In conventional cloud computing operations such as MapReduce, data should not overlap, otherwise incorrect data may be produced. For example, counting the items in a set can be performed by MapReduce, but data allocated to different servers cannot overlap, otherwise items may be counted more than once. In TA, this is not a concern due to the nature of the TA operations

. Once the results are available from each server, the testing results can be merged either incrementally, in parallel, or in any order. Furthermore, test results can be merged repeatedly without changing the final results. Using this approach, one can test say 10 batches of 100 tenant applications each, for a total of 1, 000 tenant applications. Once this is done, another batch of 1, 000 tenant applications can be tested with each 100 tenant application allocated to a server for execution. In this way, after running 100 batches, 100, 000 tenant applications can be evaluated completely. The following example illustrates the testing process of fifteen configurations. According to the definition of configuration testing, all feasible interactions should be tested. For simplicity, assume that only interaction (c, d, f ) is faulty, and only interaction (c, d, e) is infeasible, and all other interactions pass the testing. If one assigns configurations 1, 3, 5, 7, 9, 11, 13, 15 into Server1 , configurations 2, 4, 6, 8, 10, 12, 14 into Server2 , and 4-11 configurations into Server3 . If Server1 and Server3 do their own testing first, Server2 can reuse test results of interactions from them to eliminate interactions that need to be tested. For example, when testing 2-way interactions of configuration (b, c, d, f ) in Server2 , it can reuse the test results of (b, c), (b, d) of configuration (b, c, d, e) from Server3 , (b, f ) of configuration (a, b, c, f )

from Server1 . They are all passed, and it can reuse the test results of (b, c, d) of configuration (a, b, c, d) from Server1 , (b, c, f ) of configuration (a, b, c, f ) from Server1 , (b, d, f ) of configuration (a, b, d, f ) from Server1 , and (c, d, f ) of configuration (a, c, d, f ) from Server3 . Because (c, d, f ) is faulty, it can deduce that 4-way interaction (b, c, d, f ) is also faulty. For the sets of configuration that are overlapping, their returned test results from different servers are the same. The merged results of these results also stay the same. Not only interactions, sets of configurations, CS1 , CS2 , . . . , CSK can be allocated to different processors (or clusters) for testing, and the test results can then be merged. The sets can be non-overlapping or overlapping, and the merge process can be arbitrary. For example, say the result of CSi is RCSi , the merge process can be (· · · ((((RCS1 + RCS2 ) + RCS3 ) + RCS4 ) + · · · + RCSK ), or (· · · ((((RCSK + RCSk-1 ) + RCSk-2 ) + · · · + RCS1 ), or any other sequence that includes all RCSi , for i = 1 to K . This is true because RCS is simply a set of V (Tj ) for any intercation Tj in the configuration CSi . (a,b,c,d) (a,b,c,e) (a,b,c,f) (a,b,d,e) (a,b,d,f) (a,b,e,f) (a,c,d,e) (a,c,d,f) (a,c,e,f) (a,d,e,f) (b,c,d,e) (b,c,d,f) (b,c,e,f) (b,d,e,f) (c,d,e,f) Server1 P P P P P X F P P X F P P X P P P X F P P X Server2 P Server3

IV. C ONCLUSION This paper proposes TA to address SaaS combinatorial testing. The TA provides a foundation for concurrent combinatorial testing. The TA has two operations and test results can have five states with a priority. By using the TA operations, many combinatorial tests can be eliminated as the TA identifies those interactions that need not be tested. Also the TA defines operation rules to merge test results done by different processors, so that combinatorial tests can be done in a concurrent manner. The TA rules ensure that either merged results are consistent or a testing error has been detected so that retest is needed. In this way, large-scale combinatorial testing can be carried out in a cloud platform with a large number of processors to perform test execution in parallel to identify faulty interactions. ACKNOWLEDGMENT

This project is sponsored by U.S. National Science Foundation project DUE 0942453 and National Science Foundation M erged Results China (No.61073003), National Basic Research Program of China (No.2011CB302505), and the State Key Laboratory of P Software Development Environment (No. SKLSDE-2012ZXP 18), and Fujitsu Laboratory. P P R EFERENCES P [1] X. Bai, M. Li, B. Chen, W.-T. Tsai, and J. Gao. Cloud Testing Tools. In P Proceedings of IEEE 6th International Symposium on Service Oriented X System Engineering (SOSE), pages 1­12, Irvine, CA, USA, 2011. F [2] D. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG System: An Approach to Testing Based on Combinatorial Design. P Journal of IEEE Transactions on Software Engineering, 23:437­444, P 1997. X [3] C. J. Colbourn. Covering arrays and hash families. In D. Crnkovic and V. D. Tonchev, editors, Information Security, Coding Theory and Related F Combinatorics, volume 29 of NATO Science for Peace and Security P Series - D: Information and Communication Security. IOS Press, 2011. P [4] J. Gao, X. Bai, and W.-T. Tsai. SaaS Performance and Scalability Evaluation in Cloud. In Proceedings of The 6th IEEE International X
Symposium on Service Oriented System Engineering, SOSE '11, 2011. [5] M. Grindal, J. Offutt, and S. F. Andler. Combination Testing Strategies: A Survey. Software Testing, Verification, and Reliability, 15:167­199, 2005. [6] C. Kaner, J. Falk, and H. Q. Nguyen. Testing Computer Software, 2nd Edition. Wiley, New York, NY, USA, 1999. [7] V. V. Kuliamin and A. A. Petukhov. A Survey of Methods for Constructing Covering Arrays. Journal of Program Computer Software, 37(3):121­146, may 2011. [8] T. Muller and D. Friedenberg. Certified Tester Foundation Level Syllabus. Journal of International Software Testing Qualifications Board. [9] A. A. Porter, C. Yilmaz, A. M. Memon, D. C. Schmidt, and B. Natarajan. Skoll: A Process and Infrastructure for Distributed Continuous Quality Assurance. IEEE Transactions on Software Engineering, 33(8):510­525, 2007. [10] W.-T. Tsai, Y. Huang, X. Bai, and J. Gao. Scalable Architecture for SaaS. In Proceedings of 15th IEEE International Symposium on Object Component Service-oriented Real-time Distributed Computing, ISORC '12, Apr. 2012. [11] W.-T. Tsai, Y. Huang, and Q. Shao. Testing the Scalability of SaaS Applications. In Proceedings of IEEE International Conference on ServiceOriented Computing and Applications (SOCA), pages 1­4, Irvine, CA, USA, 2011. [12] W.-T. Tsai, Y. Huang, Q. Shao, and X. Bai. Data Partitioning and Redundancy Management for Robust Multi-Tenancy SaaS. International Journal of Software and Informatics (IJSI), 4(3):437­471, 2010.

E. Modified Testing Process Perform 2-way interaction testing first. Before going on to 3way interaction testing, use the results of 2-way testing to eliminate cases. The testing process stops when finishing testing all t-way interactions. The analysis of t-way interactions is based on the P T Rs of all (t - i)-way interactions for 1  i < t. The superset of infeasible, irrelevant, and faulty test cases do not need to be tested. The test results of the superset can be obtained by TA operations and must be infeasible, irrelevant, or faulty. But the superset of test cases with unknown indicator must be tested. In this way, a large repeating testing workload can be reduced. For n components, all t-way interactions for t  2 are composed by 2-way, 3-way, ..., t-way interactions. In n components combinatorial testing, the number of 2-way interactions is equal to n 2 . In general, the number of t-way interactions is equal to n t . More interactions are treated when n n > , which happens when t  n 2 . The total number t t-1 t of interactions examined is i=2 n . i

[13] W.-T. Tsai, Q. Li, C. J. Colbourn, and X. Bai. Adaptive Fault Detection for Testing Tenant Applications in Multi-Tenancy SaaS Systems. In Proceedings of IEEE International Conference on Cloud Engineering (IC2E), March 2013. [14] W.-T. Tsai, Q. Shao, and W. Li. OIC: Ontology-based Intelligent Customization Framework for SaaS. In Proceedings of International Conference on Service Oriented Computing and Applications(SOCA'10), Perth, Australia, Dec. 2010. [15] Wikipedia. Software Testing, 2013. [16] C. Yilmaz, M. B. Cohen, and A. Porter. Covering Arrays for Efficient Fault Characterization in Complex Configuration Spaces. In Proceedings of the 2004 ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA '04, pages 45­54, New York, NY, USA, 2004. ACM.

A PPENDIX The associativity of binary operation . V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). Proof: We will prove this property in the following cases. (1) At least one of V (T1 ), V (T2 ), and V (T3 ) is X. Without loss of generality, suppose that V (T1 ) = X, then according to the operation table of , V (T1 )  (V (T2 )  V (T3 )) = X  (V (T2 )  V (T3 )) = X, (V (T1 )  V (T2 ))  V (T3 ) = (X  V (T2 ))  V (T3 ) = X  V (T3 ) = X. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (2) V (T1 ), V (T2 ), and V (T3 ) are not X and at least one of V (T1 ), V (T2 ), and V (T3 ) is F. Without loss of generality, suppose that V (T1 ) = F, then according to the operation table of , the value of V (T2 )  V (T3 ) can only be F, N or U. So V (T1 )  (V (T2 )  V (T3 )) = F  (V (T2 )  V (T3 )) = F, (V (T1 )  V (T2 ))  V (T3 ) = (F  V (T2 ))  V (T3 ) = F  V (T3 ) = F. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (3) V (T1 ), V (T2 ), and V (T3 ) are not X or F and at least one of V (T1 ), V (T2 ), and V (T3 ) is N. Without loss of generality, suppose that V (T1 ) = N, then according to the operation table of , the value of V (T2 )  V (T3 ) can only be N or U. So V (T1 )  (V (T2 )  V (T3 )) = N  (V (T2 )  V (T3 )) = N, (V (T1 )  V (T2 ))  V (T3 ) = (N  V (T2 ))  V (T3 ) = N  V (T3 ) = N. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (4) V (T1 ), V (T2 ), and V (T3 ) are not X, F or N. In this case, V (T1 ), V (T2 ), and V (T3 ) can only be P or U. According to the operation table of , the value of V (T1 )  V (T2 ) and V (T2 )  V (T3 ) are U. So V (T1 )  (V (T2 )  V (T3 )) = V (T1 )  U = U, (V (T1 )  V (T2 ))  V (T3 ) = U  V (T3 ) = U. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). The associativity of binary operation . V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ). Proof: We will prove this property in the following cases. (1) One of V1 (T ), V2 (T ), and V3 (T ) is E. Without loss of generality, suppose that V1 (T ) = E, then according to the operation table of , V1 (T )(V2 (T )V3 (T )) = E(V2 (T ) V3 (T )) = E, (V1 (T )  V2 (T ))  V3 (T ) = (E  V2 (T ))  V3 (T ) = E  V3 (T ) = E. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ).

(2) V1 (T ), V2 (T ), and V3 (T ) are not E, and there is a pair of V1 (T ), V2 (T ), and V3 (T ) does not satisfy the constrains. Without loss of generality, suppose that V1 (T ) and V2 (T ) does not satisfy the constrains, then according to the operation table of , V1 (T )  V2 (T ) = E. So (V1 (T )  V2 (T ))  V3 (T ) = E  V3 (T ) = E. Since V1 (T ) and V2 (T ) does not satisfy the constrains, there can be two cases: (a) one of them is X and the other is not, or (b) one of them is P and the other is F. (a) If V1 (T ) = X, then V2 (T )  V3 (T ) cannot be X because V2 (T ) cannot be X. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. If V2 (T ) = X, then V2 (T )  V3 (T ) = X can only be E or X. Since V1 (T ) cannot be X, V1 (T )  (V2 (T )  V3 (T )) = E. (b) If V1 (T ) = P and V2 (T ) = F, then V2 (T )  V3 (T ) can only be E or F. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. If V1 (T ) = F and V2 (T ) = P, then V2 (T )  V3 (T ) can only be E or P. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ). (3) V1 (T ), V2 (T ), and V3 (T ) are not E, and V1 (T ), V2 (T ), and V3 (T ) satisfy the constrains. (a) One of V1 (T ), V2 (T ), and V3 (T ) is X. Without loss of generality, suppose that V1 (T ) = X, then V2 (T ) = V3 (T ) = X. So V1 (T )  (V2 (T )  V3 (T )) = X  (X  X) = X  X = X and (V1 (T )  V2 (T ))  V3 (T ) = (X  X)  X = X  X = X. (b) V1 (T ), V2 (T ), and V3 (T ) are not X, and one of V1 (T ), V2 (T ), and V3 (T ) is F. Without loss of generality, suppose that V1 (T ) = F, then V2 (T ) and V3 (T ) can only be F, N, or U. According to operation table of , V2 (T )  V3 (T ) can only be F, N, or U, and V1 (T )  V2 (T ) can only be F. So V1 (T )  (V2 (T )  V3 (T )) = F  (V2 (T )  V3 (T )) = F and (V1 (T )  V2 (T ))  V3 (T ) = F  V3 (T ) = F. (c) V1 (T ), V2 (T ), and V3 (T ) are not X or F, and one of V1 (T ), V2 (T ), and V3 (T ) is P. Without loss of generality, suppose that V1 (T ) = P, then V2 (T ) and V3 (T ) can only be P, N, or U. According to operation table of , V2 (T )  V3 (T ) can only be P, N, or U, and V1 (T )  V2 (T ) can only be F. So V1 (T )  (V2 (T )  V3 (T )) = P  (V2 (T )  V3 (T )) = P and (V1 (T )  V2 (T ))  V3 (T ) = P  V3 (T ) = P. (d) V1 (T ), V2 (T ), and V3 (T ) are not X, F or P, and one of V1 (T ), V2 (T ), and V3 (T ) is U. Without loss of generality, suppose that V1 (T ) = U, then V2 (T ) and V3 (T ) can only be N, or U. According to operation table of , V2 (T )  V3 (T ) can only be N, or U, and V1 (T )  V2 (T ) can only be U. So V1 (T )  (V2 (T )  V3 (T )) = U  (V2 (T )  V3 (T )) = U and (V1 (T )  V2 (T ))  V3 (T ) = U  V3 (T ) = U. (e) V1 (T ), V2 (T ), and V3 (T ) are N. V1 (T )  (V2 (T )  V3 (T )) = N  (N  N) = N  N = N and (V1 (T )  V2 (T ))  V3 (T ) = (N  N)  N = N  N = N. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ).

Abstract:
Covering array generation is a key issue in combinatorial testing. A number of researchers have been applying greedy algorithms for covering array construction. A greedy framework has been built to integrate most greedy algorithms and evaluate new approaches derived from this framework. However, this framework is affected by multiple factors, which makes its deployment and optimization very challenging. In order to identify the best configuration, we propose a search method that combines pairwise coverage with either base choice or hill climbing techniques. We conduct three different groups of experiments based on six decisions of the greedy framework. The influence of these decisions and their interactions are studied systematically, and the selected greedy algorithm for covering array generation is shown to be better than the existing greedy algorithms.Given v, t, and m, does there exist a partial Steiner triple system of order v with t triples whose triples can be ordered so that any m consecutive triples are pairwise disjoint? Given v, t, and m1, m2, . . . , ms with  t=âsi=1mit=âi=1smi  , does there exist a partial Steiner triple system with t triples whose triples can be partitioned into partial parallel classes of sizes m1, . . . , ms? An affirmative answer to the first question gives an affirmative answer to the second when mi â¤ m for each  iâ{1,2,â¦,s}iâ{1,2,â¦,s}  . These questions arise in the analysis of erasure codes for disk arrays and that of codes for unipolar communication, respectively. A complete solution for the first problem is given when m is at most  13(vâ(9v)2/3)+O(v1/3)13(vâ(9v)2/3)+O(v1/3)  .Electronic Journal of Differential Equations, Vol. 2012 (2012), No. 96, pp. 1­11. ISSN: 1072-6691. URL: http://ejde.math.txstate.edu or http://ejde.math.unt.edu ftp ejde.math.txstate.edu

EXISTENCE AND ASYMPTOTIC BEHAVIOR OF SOLUTIONS TO THE GENERALIZED DAMPED BOUSSINESQ EQUATION
YINXIA WANG

Abstract. We consider the Cauchy problem for the n-dimensional generalized damped Boussinesq equation. Based on decay estimates of solutions to the corresponding linear equation, we define a solution space with time weighted norms. Under small condition on the initial value, the existence and asymptotic behavior of global solutions in the corresponding Sobolev spaces are established by the contraction mapping principle.

1. Introduction We study the Cauchy problem of the generalized damped Boussinesq equation in n space dimensions utt - autt - 2but - 3 u +  2 u - u = f (u) with the initial value t=0: u = u0 (x), ut = u1 (x). (1.2) (1.1)

Here u = u(x, t) is the unknown function of x = (x1 , · · · , xn )  Rn and t > 0, a, b, ,  are positive constants. The nonlinear term f (u) = O(u1+ ) and  is a positive integer. The first initial boundary value problem for utt - autt - 2but - 3 u +  2 u - u =  (u2 ) (1.3)

in a unit circle was investigated in [16], where a, b, ,  are positive constants and  is a constant. The existence and the uniqueness of strong solution was established and the solutions were constructed in the form of series in the small parameter present in the initial conditions. The long-time asymptotics was also obtained in the explicit form. In [1], the authors considered the initial-boundary value problem for (1.3) in the unit ball B  R3 , similar results were established. It is well-known that the equation (1.3) is closely contacted with many wave equations. For example, the equation (which we call the Bq equation) utt - uxx + uxxxx = (u2 )xx ,
2000 Mathematics Subject Classification. 35L30, 35L75. Key words and phrases. Generalized damped equation; global solution; asymptotic behavior. c 2012 Texas State University - San Marcos. Submitted May 31, 2012. Published June 10, 2012.
1

2

Y. WANG

EJDE-2012/96

which was derived by Boussinesq in 1872 to describe shallow water waves. The improved Bq equation(which we call IBq equation) is utt - uxx - uxxtt = (u2 )xx . A modification of the IBq equation analogous of the MKdV equation yields utt - uxx - uxxtt = (u3 )xx , which we call the IMBq equation (see [5]). (1.1) is a higher order wave equation. In [8], we considered the Cauchy problem for the Cahn-Hilliard equation with inertial term. Combining high frequency, low frequency technique and energy methods, we obtained global existence and asymptotic behavior of solutions. Wang, Liu and Zhang [13] investigated a fourth wave equation that is of the regularity-loss type. Based on the decay property of the solution operators, global existence and asymptotic behavior of solutions are obtained. For global existence and asymptotic behavior of solutions to higher order wave equations, we refer to [2]-[3] and [6]-[15] and references therein. The main purpose of this paper is to establish global existence and asymptotic behavior of solutions to (1.1), (1.2) by using the contraction mapping principle. Firstly, we consider the decay property of the following linear equation utt - autt - 2but - 3 u +  2 u - u = 0. (1.4)

We obtain the following decay estimate of solutions to (1.4) associated with initial condition (1.2),
k x u(t) L2

 C (1 + t)- 4 - 2 - 2 ( u0

n

k

1

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(1.5)

(k  s + 2),
h x ut (t) L2

 C (1 + t)- 4 - 2 -1 ( u0

n

h

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(1.6)

(h  s) Based on the estimates (1.5) and (1.6), we define a solution space with time weighted norms. Then global existence and asymptotic behavior of classical solutions to (1.1), (1.2) are obtained by using the contraction mapping principle. We give notation which is used in this paper. Let F [u] denote the Fourier transform of u defined by u ^( ) = F [u] =
Rn

e-i·x u(x)dx,

and we denote its inverse transform by F -1 . For 1  p  , Lp = Lp (Rn ) denotes the usual Lebesgue space with the s norm · Lp . The usual Sobolev space of s is defined by Hp = (I - )-s/2 Lp s = with the norm f Hp (I - )s/2 f Lp ; the homogeneous Sobolev space of s is s - s/  p = (-) 2 Lp with the norm f H s = (-)s/2 f Lp ; especially defined by H p s s s  s . Moreover, we know that H s = Lp  H p H s = H2 ,H = H for s  0. p 2 Finally, in this paper, we denote every positive constant by the same symbol C or c without confusion. [·] is the Gauss symbol. The article is organized as follows. In Section 2 we derive the solution formula of our semi-linear problem. We study the decay property of the solution operators appearing in the solution formula in section 3. Then, in Section 4, we discuss the linear problem and show the decay estimates. Finally, we prove global existence

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

3

and asymptotic behavior of solutions for the Cauchy problem (1.1), (1.2) in Section 5. 2. Solution formula The aim of this section is to derive the solution formula for problem (1.1), (1.2). We first investigate the equation (1.4). Taking the Fourier transform, we have (1 + a| |2 )^ utt + 2b| |2 u ^t + (| |6 +  | |4 + | |2 )^ u = 0. The corresponding initial value are t=0: u ^=u ^0 ( ), u ^t = u ^1 ( ). (2.2) (2.1)

The characteristic equation of (2.1) is (1 + a| |2 )2 + 2b| |2  + | |6 +  | |4 + | |2 = 0. Let  = ± ( ) be the corresponding eigenvalues of (2.3), we obtain ± ( ) = -b| |2 ± | | -1 - (a +  - b2 )| |2 - ( + a )| |4 - a| |6 . 1 + a| |2 ^ (, t)^ ^ (, t)^ u1 ( ) + H u0 ( ), u ^(, t) = G where ^ (, t) = G and 1 (+ ( )e- ()t - - ( )e+ ()t ). + ( ) - - ( ) We define G(x, t) and H (x, t) by ^ (, t) = H ^ (, t)](x), G(x, t) = F -1 [G ^ (, t)](x), H (x, t) = F -1 [H (2.7) 1 (e+ ()t - e- ()t ) + ( ) - - ( ) (2.6) (2.4) (2.3)

The solution to the problem (2.1)-(2.2) is given in the form (2.5)

respectively, where F -1 denotes the inverse Fourier transform. Then, applying F -1 to (2.5), we obtain u(t) = G(t)  u1 + H (t)  u0 . By the Duhamel principle, we obtain the solution formula to (1.1), (1.2),
t

(2.8)

u(t) = G(t)  u1 + H (t)  u0 +
0

G(t -  )  (I - a)-1 f (u)( )d.

(2.9)

3. Decay Property The aim of this section is to establish decay estimates of the solution operators G(t) and H (t) appearing in the solution formula (2.8). Lemma 3.1. The solution of problem (2.1), (2.2) satisfies | |2 (1+ | |2 )|u ^(, t)|2 + |u ^t (, t)|2  Ce-c()t (| |2 (1+ | |2 )|u ^0 ( )|2 + |u ^1 ( )|2 ), (3.1) for   Rn and t  0, where  ( ) =
| |2 1+| |2 .

4

Y. WANG

EJDE-2012/96

¯ Proof. Multiplying (2.1) by u ^t and taking the real part yields 1 d {(1 + a| |2 )|u ^t |2 + (| |6 +  | |4 + | |2 )|u ^|2 } + 2b| |2 |u ^t |2 = 0. 2 dt ¯ Multiplying (2.1) by u ^ and taking the real part, we obtain (3.2)

1 d ¯ {b| |2 |u ^|2 + 2(1 + a| |2 )Re(^ ut u ^)} + (| |6 +  | |4 + | |2 )|u ^|2 - (1 + a| |2 )|u ^t |2 = 0. 2 dt (3.3) Multiplying both sides of (3.2) and (3.3) by (1 + a| |2 ) and b| |2 respectively, summing up the products yields d E + F = 0, (3.4) dt where 1 E = (1 + a| |2 )2 |u ^t |2 + (1 + a| |2 )(| |6 +  | |4 + | |2 )|u ^|2 + b2 | |4 |u ^|2 2 ¯ + b| |2 (1 + a| |2 ) Re(^ ut u ^) and F = b| |2 (| |6 +  | |4 + | |2 )|u ^|2 + b| |2 (1 + a| |2 )|u ^t |2 . A simple computation implies that C (1 + | |2 )2 E0  E  C (1 + | |2 )2 E0 , where E0 = | |2 (1 + | |2 )|u ^|2 + |u ^t |2 . Note that F  c| |2 E0 . It follows from (3.5) that F  c ( )E, where  ( ) = Using (3.4) and (3.6), we obtain d E + c ( )E  0. dt Thus E (, t)  e-cw()t E (, 0), which together with (3.5) proves the desired estimates (3.1). Then the proof is complete. ^ (, t) and H ^ (, t) be the fundamental solution of (1.4) in the Lemma 3.2. Let G Fourier space, which are given in (2.6) and (2.7), respectively. Then we have the estimates ^ (, t)|2 + |G ^ t (, t)|2  Ce-c()t | |2 (1 + | |2 )|G (3.7) and ^ (, t)|2 + |H ^ t (, t)|2  C | |2 (1 + | |2 )e-c()t | |2 (1 + | |2 )|H for   Rn and t  0, where  ( ) =
| |2 1+| |2 .

(3.5)

(3.6)

| |2 . 1 + | |2

(3.8)

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

5

Proof. If u ^0 ( ) = 0, from (2.5), we obtain ^ (, t)^ u ^(, t) = G u1 ( ), ^ t (, t)^ u ^t (, t) = G u1 ( ).

Substituting the equalities into (3.1) with u ^0 ( ) = 0, we obtain (3.7). In what follows, we consider u ^1 ( ) = 0, it follows from (2.5) that ^ (, t)^ ^ t (, t)^ u ^(, t) = H u0 ( ), u ^t (, t) = H u0 ( ). Substituting the equalities into (3.1) with u ^1 ( ) = 0, we obtain the desired estimate (3.8). The Lemma is proved. Lemma 3.3. Let k  0 and 1  p  2. Then we have
k x G(t)   k H (t) x k x Gt (t) L2

 C (1 + t)-( 2 ( p - 2 )+ 2 + 2 - 2 ) 
L2 L2

n

1

1

k

l

1

-l p H

(k-2)+ + Ce-ct x 

L2 ,

(3.9) (3.10) (3.11) (3.12) (3.13) (3.14)

 
L2

 C (1 + t)  C (1 + t)

1 1 k l -( n 2 ( p - 2 )+ 2 + 2 )

 

-l p H -l p H -l p H

+ Ce + Ce

-ct

k x  L2 k x  L2 , L2 L2 , L2 ,

1 1 k l -( n 2 ( p - 2 )+ 2 + 2 ) n 1 1 k l 1

-ct

k x Ht (t)  

 C (1 + t)-( 2 ( p - 2 )+ 2 + 2 + 2 ) 
n k 1

k+2 + Ce-ct x  L1 L1 k + Ce-ct x g k + Ce-ct x g

k x G(t)  (I - a)-1 g k x Gt (t)  (I - a)-1 g

L2 L2

 C (1 + t)-( 4 + 2 + 2 ) g  C (1 + t)
k -( n 4 + 2 +1)

g

where (k - 2)+ = max{0, k - 2}. Proof. Firstly, we prove (3.9). By the Plancherel theorem and (3.7), we obtain
k x G(t)   2 L2

=
| |R0

^( )|2 d + ^ (, t)|2 | | |2k |G
| |R0

^( )|2 d ^ (, t)|2 | | |2k |G

C
| |R0

| |
-ct

2k-2 -c| |2 t

e

^( )| d |
2

(3.15) | | (| | (1 + | | ))
| |R0 2 Lp 2k 2 2 -1

+ Ce

^( )|2 d |
2

^( )  C | |-l  + Ce
-ct

| |(2k-2+2l)q e-cq|| t d
| |R0

1/q

(k-2)+ x  2 L2 , 1 p

where R0 is a small positive constant and Hausdorff-Young inequality that ^( ) | |-l 
Lp

+

1 p

= 1,
l

2 p

+

1 q

= 1. It follows from (3.16)

 C (-)- 2 

Lp .

By a straight computation, we obtain | |(2k-2+2l)q e-cq|| t d
| |R0
2

1/q

 C (1 + t)-( 2q +k-1+l) (3.17)  C (1 + t)-(n( p - 2 )+k-1+l) .
1 1

n

Combining (3.15), (3.16) and (3.17) yields (3.9). Similarly, using (3.7) and (3.8), respectively, we can prove (3.10)-(3.12).

6

Y. WANG

EJDE-2012/96

In what follows, we prove (3.13). By the Plancherel theorem, (3.7), and Hausdorff-Young inequality, we have
k x G(t)  (I - a)-1 g 2 L2

=
| |R0

^ (, t)|2 | |4 (1 + a| |2 )-2 |g | |2k |G ^( )|2 d ^ (, t)|2 | |4 (1 + | |2 )-2 |g | |2k |G ^( )|2 d
| |R0

+ C

| |2k+2 e-c|| t |g ^( )|2 d + Ce-ct
| |R0 | |R0 k | |2k+2 e-c|| t d + Ce-ct x g | |R0 2 L1 k + Ce-ct x g 2 L2 .
2

2

| |2k |g ^( )|2 d
2 L2

C g ^( )

2 L
n

 C (1 + t)-( 2 +k+1) g

where R0 is a small positive constant. Thus (3.13) follows. Similarly, we can prove (3.14). Thus we have completed the proof of lemma. 4. Decay estimate for solutions to the linear equation  -1 (Rn ), u1  H s (Rn )  H  -2 (Rn ) Theorem 4.1. Assume that u0  H s+2 (Rn )  H 1 1 n (s  [ 2 ] + 5). Then the classical solution u(x, t) to (1.4) associated with initial condition (1.2), which is given by the formula (2.8), satisfies the decay estimates
k x u(t) L2

 C (1 + t)- 4 - 2 - 2 ( u0
n h

n

k

1

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(4.1)

for k  s + 2,
h x ut (t) L2

 C (1 + t)- 4 - 2 -1 ( u0
n m 1

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(4.2)

for h  s,
m x u(t) L

 C (1 + t)- 2 - 2 - 2 ( u0

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(4.3)

for m  s + 1 - [ n 2 ]. Proof. Firstly, we prove (4.1). Using (3.9) and (3.10), we obtain
k x u(t) L2 k x G(t) h + C x H (t)  u0



 u1

L2

L2  -2 ) H 1  -2 H 1

 C (1 + t)  C (1 + t)

k 1 -n 4 -2-2

( u0 ( u0

 -1 H 1  -1 H 1

+ u1 + u1

+ Ce-ct ( u0
H s+2

H s+2

+ u1

Hs )

k 1 -n 4 -2-2

+ u0

+ u1

H s ).

Similar to the proof of (4.1), using (3.11) and (3.12), we can prove (4.2). In what follows, we prove (4.3). Using (4.1) and Gagliardo-Nirenberg inequality, it is not difficult to get (4.3). The Lemma is proved. 5. Existence of global solution and asymptotic behavior The purpose of this section is to prove the existence and asymptotic behavior of global solutions to the Cauchy problem (1.1), (1.2). We need the following Lemma, which come from [4] (see also [17]).

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

7

Lemma 5.1. Let s and  be positive integers,  > 0, p, q, r  [1, ] satisfy 1 r = 1 1 s + , and let k  { 0 , 1 , 2 , · · · , s } . Assume that F ( v ) is a class of C and satisfies p r
l |v F (v )|  Cl, |v |+1-l ,

|v |  ,

0  l  s, l <  + 1

and
l |v F (v )|  Cl, , |v |  , l  s,  + 1  l.

If v  Lp  W k,q  L and v F (v )
 x F (v ) Lr

L

  , then
W k,q

W k,r

 Ck, v
Lq

v v

Lp

v

 -1 L ,

  Ck, x v

v

Lp

 -1 L ,

||  k.

1 Lemma 5.2. Let s and  be positive integers,  > 0, p, q, r  [1, ] satisfy 1 r = p+ 1 r , and let k  {0, 1, 2, · · · , s}. Let F (v ) be a function that satisfies the assumptions of Lemma 5.1. Moreover, assume that s s |v F (v1 ) - v F (v2 )|  C (|v1 | + |v2 |)max{-s,} |v1 - v2 |,

|v1 |  ,

|v2 |  .

If v1 , v2  L  W

p

k,q

L



and v1
Lr

L

 , v2

L

  , then for ||  k , we have

 x (F (v1 ) - F (v2 ))

 Ck, {( + ( v1

 x v1 Lq Lp

 + x v2 Lp )

Lq )

v1 - v2

Lp

+ v2

 x (v1

- v2 )

Lq }(

v1

L

+ v2

 -1 . L )

Based on the estimates (4.1)-(4.3) of solutions to (1.4) associated with initial condition (1.2), we define the following solution space X = {u  C ([0, ); H s+2 (Rn ))  C 1 ([0, ); H s (Rn )) : u where u
X k u(t) (1 + t) 4 + 2 + 2 x ks+2 X
n k 1 n h

X

< },

= sup
t0

L2

+
hs

h ut (t) (1 + t) 4 + 2 +1 x

L2 },

For R > 0, we define XR = {u  X : u Gagliardo-Nirenberg inequality, we obtain
m x u(t) L

 R}. For m  s + 1 - [ n 2 ], using
n m 1

 C (1 + t)-( 2 + 2 + 2 ) u

X.

(5.1)

 -1 (Rn ), u1  H s (Rn )  H  -2 (Rn ) Theorem 5.3. Assume that u0  H s+2 (Rn )  H 1 1 n s+2 (s  [ 2 ] + 5) and integer   2. Let f (u) be a function of class C and satisfy Lemmas 5.1 and 5.2. Put E0 = u 0
 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs .

If E0 is suitably small, the Cauchy problem (1.1)-(1.2) has a unique global classical solution u(x, t) satisfying u  C ([0, ); H s+2 (Rn )), ut  C ([0, ); H s (Rn )), utt  L ([0, ); H s-2 (Rn )). Moreover, the solution satisfies the decay estimate
k x u(t) h x ut (t) L2

 CE0 (1 + t)- 4 - 2 - 2 ,  CE0 (1 + t)
h -n 4 - 2 -1

n

k

1

(5.2) (5.3)

L2

for k  s + 2 and h  s.

8

Y. WANG

EJDE-2012/96

Proof. Define the mapping
t

(u) = G(t)  u1 + H (t)  u0 +
0

G(t -  )  (I - a)-1 f (u( ))d.

(5.4)

Using (3.9)-(3.10), (3.13), Lemma 5.1 and (5.1), for k  s + 2 we obtain
k x (u) L2 L2 k + C x H (t)  u0 L2 L2 d H s+2 + u1
Hs )

k  C x G(t)  u1 t

+C
0

k x G(t -  )  (I - a)-1 f (u( ))
n k 1

 C (1 + t)- 4 - 2 - 2 ( u0
t/2

 -1 H 1
k

+ u1
1

 -2 ) H 1 L1 d

+ Ce-ct ( u0

+C
0 t

(1 + t -  )- 4 - 2 - 2 f (u)
k (1 + t -  )- 4 - 2 x f (u) t/2 t k e-c(t- ) x f (u) 0
n k 1 n 1

n

+C +C

L1 d

L2 d

 C (1 + t)- 4 - 2 - 2 ( u0
t/2

 -1 H 1
k

+ u1
1

 -2 ) H 1

+ Ce-ct ( u0

H s+2 + u1

Hs )

+C
0 t

(1 + t -  )- 4 - 2 - 2 u
k (1 + t -  )- 4 - 2 x u t/2
n k 1 n 1

n

2 L2

u

 -1 L d t k e-c(t- ) x u 0 H s+2 + u1
n 1 Hs )

+C

2 L2

u

 -1 L d +C

L2

u

 L d

 C (1 + t)- 4 - 2 - 2 ( u0
t/2

 -1 H 1
n

+ u1
k 1

 -2 ) H 1

+ Ce-ct ( u0
n

+CR+1
0 t

(1 + t -  )- 4 - 2 - 2 (1 +  )-( 2 +1) (1 +  )-( 2 + 2 )(-1) d (1 + t -  )- 4 - 2 (1 +  )- 2 -k-1 (1 +  )-( 2 + 2 )(-1) d
t/2 t
n 1 n n 1

+CR+1 +CR+1  C (1 + t) Thus

e-c(t- ) (1 +  )- 4 - 2 - 2 (1 +  )-( 2 + 2 ) d
0 k 1 -n 4 -2-2

n

k

1

n

1

{( u 0
n

 -1 H 1
k 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

+ R+1 }. (5.5)

k (1 + t) 4 + 2 + 2 x (u) It follows from (5.4) that t

L2

 CE0 + CR+1 .

(u)t = Gt (t)  u1 + Ht (t)  u0 +
0

Gt (t -  )  (I - a)-1 f (u( ))d.

(5.6)

Using (3.11)-(3.12), (3.14) Lemma 5.1 and (5.1), for h  s we have
h x (u)t L2 h x Ht (t)  u0 L2 L2 d h  C x Gt (t)  u1 L2 + C t h +C x Gt (t -  )  (I 0

- a)-1 f (u( ))

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR
n h

9 Hs )

 C (1 + t)- 4 - 2 -1 ( u0
t/2

 -1 H 1
h

+ u1

 -2 ) H 1 L1 d

+ Ce-ct ( u0

H s+2

+ u1

+C
0 t

(1 + t -  )- 4 - 2 -1 f (u)
h f (u) (1 + t -  )- 4 -1 x t/2
n h n

n

t L1 d

+C

+C
0

h e-c(t- ) x f (u)

L2 d Hs )

 C (1 + t)- 4 - 2 -1 ( u0
t/2

 -1 H 1
h

+ u1
2 L2

 -2 ) H 1

+ Ce-ct ( u0

H s+2

+ u1

+C
0 t

(1 + t -  )- 4 - 2 -1 u
h u (1 + t -  )- 4 -1 x t/2
h -n 4 - 2 -1 n

n

u

 -1 L d t

+C

2 L2

u

 -1 L d

+C
0 -ct

h e-c(t- ) x u

L2

u

 L d

 C (1 + t) +CR+1

( u0

 -1 H 1
n

+ u1
h

 -2 ) H 1

+ Ce
n

( u0

H s+2
n

+ u1
1

Hs )

t/2 0 t

(1 + t -  )- 4 - 2 -1 (1 +  )-( 2 +1) (1 +  )-( 2 + 2 )(-1) d (1 + t -  )- 4 -1 (1 +  )- 2 -h-1 (1 +  )-( 2 + 2 )(-1) d
t/2 t
n n n 1

+CR+1 +CR+1  C (1 + t) Thus

e-c(t- ) (1 +  )- 4 - 2 -1 (1 +  )-( 2 + 2 ) d
0 h -n 4 - 2 -1

n

h

n

1

{( u 0
n

 -1 H 1
h

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

+ R+1 }. (5.7) (5.8)

h (1 + t) 4 + 2 +1 x (u)t L2  CE0 + CR+1 . Combining (5.5), (5.7) and taking E0 and R suitably small yields

(u) For u ~, u ¯  XR , by using (5.4), we have
t

X

 R.

(~ u) - (¯ u) =
0

G(t -  )  (I - a)-1 [f (~ u) - f (¯ u)]d.

(5.9)

Using (5.9), (3.13) and Lemma 5.2, (5.1), for k  s + 2 we obtain
k x (~ u) - (¯ u)) t L2 L2 d


0

k x G(t -  )  (I - a)-1 [f (~ u) - f (¯ u)] t/2

C
0 t

(1 + t -  )- 4 - 2 - 2 (f (~ u) - f (¯ u))
k (1 + t -  )- 4 - 2 x (f (~ u) - f (¯ u)) t/2 t k e-c(t- ) x (f (~ u) - f (¯ u)) 0 t/2
n 1

n

k

1

L1 d

+C +C C
0

L1 d

L2 d

(1 + t -  )- 4 - 2 - 2 ( u ~
L

n

k

1

L2

+ u ¯

L2 )

u ~-u ¯

L2

×( u ~

+ u ¯

 -1 d L )

10 t

Y. WANG

EJDE-2012/96

+C
t/2

k u ~ (1 + t -  )- 4 - 2 {( x L2 t

n

1

L2

k + x u ~

L2 )

u ~-u ¯

L2

+( u ~ +C
0

+ u ¯

L2 )

k x (~ u-u ¯) L2

L2 }(

u ~
L2 )

L

+ u ¯

 -1 d L )

k e-c(t- ) {( x u ~ L

k + x u ~

u ~-u ¯
L

L  -1 d L )
n 1

+( u ~

+ u ¯
X

L ) t/2

k x (~ u-u ¯)

L2 }(
n k

u ~
1

+ u ¯

 CR u ~-u ¯ +CR u ~-u ¯

(1 + t -  )- 4 - 2 - 2 (1 +  )-( 2 + 2 ) d
0 t

X t/2

(1 + t -  )- 4 - 2 (1 +  )-( 2 (n+1)+
t

n

1



k+1 2 )

d

~-u ¯ +CCR u
n

X 0
k 1

e-c(t- ) (1 +  )-( 4 + 2 + 2 + 2 ) d
X,

n

n

k

1

 CR (1 + t)- 4 - 2 - 2 u ~-u ¯ which implies

k (1 + t) 4 + 2 + 2 x ((~ u) - (¯ u))

n

k

1

L2

 CR u ~-u ¯

X.

(5.10)

Similarly for h  s, from (5.6), (3.14) and (5.1), we have
t h x ((~ u) - (¯ u))t L2


0

h x Gt (t -  )  (I - a)-1 [f (~ u) - f (¯ u)] t/2

L2 d

C
0 t

u) - f (¯ u)) (1 + t -  )- 4 - 2 -1 (f (~
h (1 + t -  )- 4 -1 x (f (~ u) - f (¯ u)) t/2 t h e-c(t- ) x (f (~ u) - f (¯ u)) 0
n h n

n

h

L1 d

+C +C

L1 d

L2 d

 CR (1 + t)- 4 - 2 -1 u ~-u ¯ which implies
h (1 + t) 4 + 2 +1 x ((~ u) - (¯ u))t
n h

X,

L2

 CR u ~-u ¯

X.

(5.11)

Using (5.10), (5.11) and taking R suitably small yields (~ u) - (¯ u)
X



1 u ~-u ¯ 2

X.

(5.12)

From (5.8) and (5.12), we know that  is strictly contracting mapping. Consequently, we conclude that there exists a fixed point u  XR of the mapping , which is a classical solution to (1.1), (1.2). This completes the proof. Acknowledgements. The author would like to thank the anonymous referee for his/her comments and suggestions. This work was supported in part by grants 11101144 from the NNSF of China, and 201031 from the Research Initiation Project for High-level Talents of North China University of Water Resources and Electric Power.

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

11

References
[1] S. Lai, Y. Wang, Y. Wu and Q. Lin; An initial-boundary value problem for a generalized Boussinesq water system in a ball, Int. J. Appl. Math. Sci. 3 (2006), 117-133. [2] Y. Liu and S. Kawashima; Global existence and asymptotic behavior of solutions for quasilinear dissipative plate equation, Discrete Contin. Dyn. Syst. 29 (2011) 1113-1139. [3] Y. Liu and S. Kawashima; Global existence and decay of solutions for a quasi-linear dissipative plate equation, J. Hyperbolic Differential Equations 8 (2011) 591-614. [4] T. T. Li and Y. M. Chen; Nonlinear Evolution Equations, Scientific Press, 1989, (in Chinese). [5] V. G. Makhankov; Dynamics of classical solitons(in nonintegrable systems), Physics Reports, A review section of Phys. Lett.(Section C) 35(1) (1978), 1-128. [6] S. Wang and H. Xu; On the asymptotic behavior of solution for the generalized IBq equation with hydrodynamical damped term, J. Differ. Equ.252 (2012) 4243-4258. [7] S. Wang and F. Da; On the asymptotic behavior of solution for the generalized double dipersion equation, Appl. Anal. in press. [8] Y. Wang and Z. Wei; Global existence and asymptotic behavior of solutions to Cahn-Hilliard equation with inertial term, Internationl Journal of mathematics, accepted. [9] Y. Wang; Global existence and asymptotic behaviour of solutions for the generalized Boussinesq equation, Nonlinear Anal. 70 (2009) 465-482. [10] Y. Wang; Global existence of classical solutions to the minimal surface equation in two space dimensions with slow decay initial value, J. Math. Phys. 50 (2009) 103506-01-103506-01-14. [11] Y. Wang; Existence and nonexistence of global solutions for a class of nonlinear wave equations of higher order, Nonlinear Anal. 72 (2010) 4500-4507. [12] Y. Wang and Y. Wang; Global existence of classical solutions to the minimal surface equation with slow decay initial value, Appl. Math. Comput. 216 (2010) 576-583. [13] Y. Wang, F. Liu and Y. Zhang; Global existence and asymptotic of solutions for a semi-linear wave equation, J. Math. Anal. Appl. 385 (2012) 836-853. [14] Y. Wang and Y. Wang; Global existence and asymptotic behavior of solutions to a nonlinear wave equation of fourth-order, J. Math. Phys. 53 (2012) 013512-01-013512-13. [15] Z. Yang; Longtime behavior of the Kirchhoff type equation with strong damping on Rn , J. Differ. Equ. 242 (2007) 269-286. [16] Y. Zhang, Q. Lin and S. Lai; Long time asymptotic for the damped Boussinesq equation in a circle, J. Partial Dif. Eqs. 18(2005), 97-113. [17] S. M. Zheng; Nonlinear Evolution Equations, Monographs and Surveys in Pure and Applied Mathematics, 133, Chapan Hall/CRC, 2004. Yinxia Wang School of Mathematics and Information Sciences, North China University of Water Resources and Electric Power, Zhengzhou 450011, China E-mail address : yinxia117@126.com

Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

Contents lists available at SciVerse ScienceDirect

Journal of Combinatorial Theory, Series A
www.elsevier.com/locate/jcta

Covering and packing for pairs
Yeow Meng Chee a , Charles J. Colbourn b , Alan C.H. Ling c , Richard M. Wilson d
a

Division of Mathematical Sciences, School of Physical and Mathematical Sciences, Nanyang Technological University, 21 Nanyang Link, Singapore 637371, Singapore b Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287-8809, USA c Department of Computer Science, University of Vermont, Burlington, VT 05405, USA d Department of Mathematics, 253-37, California Institute of Technology, Pasadena, CA 91125, USA

a r t i c l e

i n f o

a b s t r a c t
When a v -set can be equipped with a set of k-subsets so that every 2-subset of the v -set appears in exactly (or at most, or at least) one of the chosen k-subsets, the result is a balanced incomplete block design (or packing, or covering, respectively). For each k, balanced incomplete block designs are known to exist for all sufficiently large values of v that meet certain divisibility conditions. When these conditions are not met, one can ask for the packing with the most blocks and/or the covering with the fewest blocks. Elementary necessary conditions furnish an upper bound on the number of blocks in a packing and a lower bound on the number of blocks in a covering. In this paper it is shown that for all sufficiently large values of v , a packing and a covering on v elements exist whose numbers of blocks differ from the basic bounds by no more than an additive constant depending only on k. © 2013 Elsevier Inc. All rights reserved.

Article history: Received 24 September 2011 Available online xxxx Keywords: Balanced incomplete block design Pair packing Pair covering Group divisible design Pairwise balanced design

1. Introduction Let v , k , and t be integers with v > k > t 2. Let  be a positive integer. A (t , )-packing of order v and blocksize k is a set V of v elements, and a collection B of k -element subsets (blocks) of V , so that every t -subset of V appears in at most  blocks. A (t , )-covering of order v and blocksize k is a set V of v elements, and a collection B of k -element subsets (blocks) of V , so that every t -subset

E-mail addresses: ymchee@ntu.edu.sg (Y.M. Chee), charles.colbourn@asu.edu (C.J. Colbourn), aling@cems.uvm.edu (A.C.H. Ling), rmw@caltech.edu (R.M. Wilson). 0097-3165/$ ­ see front matter © 2013 Elsevier Inc. All rights reserved. http://dx.doi.org/10.1016/j.jcta.2013.04.005

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1441

of V appears in at least  blocks. When  = 1, the simpler notation of t -packing or t -covering is used. When ( V , B ) is both a (t , )-packing and a (t , )-covering with blocksize k , it is a t -( v , k, ) design. v k A t -( v , k, ) design, if one exists, has  t / t blocks. When the required number of blocks is not integral, no such design can exist. Selecting all blocks containing a particular element x  V and deleting x from each forms the derived (t - 1)-( v - 1, k - 1, ) design (with respect to x). For a design v -i k -i to exist, evidently the derived design must exist; hence for a t -( v , k, ) design to exist,  t -i / t -i must be integral for every 0 i t . When these conditions are not all met, one can ask instead for the largest (t , )-packing, or for the smallest (t , )-covering, of order v and blocksize k . The Johnson bound [13] states that such a packing can have no more than

n k

···

n - t + 2 (n - t + 1) k-t +2 k-t +1

···

blocks, while the Schönheim bound [21] states that such a covering can have no fewer than

n k

···

n - t + 2 (n - t + 1) k-t +2 k-t +1

···

blocks. Our main result is that when t = 2 and  = 1, there exist packings and coverings whose sizes are within a constant of these bounds. Determining when these bounds are met exactly is a challenging question.  and Hanani [9] conjectured that, for fixed k and t , with all blocks of size k , In 1963, Erdos n a t -packing on n elements with /k (1 - o(1)) blocks and a t -covering on n elements with t t

/ t (1 + o(1)) blocks both exist. This was proved by Rödl [20], and has spawned a large literature t (for example, [10,11,14,15,23]). However, even when t = 2, all of these general constructions deviate from the Johnson and Schönheim bounds by an amount that grows as a function of the number of elements. Wilson [25] established that the necessary divisibility conditions for a 2-( v , k, ) design to exist are asymptotically sufficient (i.e., for fixed k and , and sufficiently large v ). This provides a  different means to establish the Erdos­Hanani conjecture for t = 2, but also does not immediately imply that one can find packings or coverings whose sizes are within a constant of the optimal sizes. Wilson [24] earlier considered this more challenging problem for packings, but the solution for the analogous problem for coverings has remained elusive. We focus on the case when t = 2 and  = 1 here. Caro and Yuster state stronger results for covering [3] and packing [2] than we prove here. Their approach relies in an essential manner on a strong statement by Gustavsson [12]:
Proposition 1.1. Let H be a graph with  vertices and h edges, having degree sequence (d1 , . . . , d ). Then there exist a constant N H and a constant H > 0, both depending only on H , such that for all n > N H , if G is a graph on n vertices, m edges, and degree sequence (1 , . . . , n ) so that min(1 , . . . , n ) n(1 - H ), gcd(d1 , . . . , d ) | gcd(1 , . . . , n ), and h | m, then G has an edge partition (decomposition) into graphs isomorphic to H . We have not been able to verify the proof of Proposition 1.1. Indeed, while the result has been used a number of times in the literature, no satisfactory proof of it appears there. While we expect that the statement is true, we do not think that the proof in [12] is sufficient at this time to employ the statement as a foundation for further results. Therefore we adopt a strategy that is completely independent of Proposition 1.1, and independent of the results built on it. In the remainder of the paper, we first recall relevant known results. Then in Section 3, we determine the possible structure of optimal packings and coverings, in order to determine what can remain uncovered in a packing, and what must be covered more than once in a covering. This is done in general for packings and coverings with a single hole, in order to limit any deviation from the desired bound to the manner in which a (fixed size) hole is filled. In Section 4, the most important part of the proof is established, namely that in each congruence class, one finite example can be produced. Finally in Section 5, these single examples are shown to form the required ingredients to establish asymptotic existence.

n

k

1442

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

2. Background To proceed more formally, we require a number of definitions and preliminary results from combinatorial design theory; related background material can be found in [1,22]. A balanced incomplete block design (BIBD) is a 2-( v , k, ) design. Balanced incomplete block designs have been extensively studied because of their central role in numerous applications in experimental design, coding and information theory, communications, and connections with fundamental topics in algebra, finite geometry, number theory, and combinatorics (see [5,7] for examples). The general divisibility conditions (stated for v k general t earlier) require that  2  0 mod 2 and ( v - 1)  0 (mod k - 1). A group divisible design ( V , G , B ) is a finite set V of elements or points; a partition G = {G 1 , . . . , G s } of V (groups); and a set B of subsets of V (blocks), with the property that every 2-subset of V lying within a group appears in no block, while every 2-subset of V with elements from different groups appears in exactly  blocks. When K is a set of positive integers for which | B |  K whenever B  B , the design is a ( K , )-GDD. When  = 1, we write simply K -GDD. Its order is | V |, its index is , and u u when the multiset of group sizes {|G i |: 1 i s} is the same as the multiset its type is 1 1 · · ·  j . We write (k, )-GDD (or k -GDD formed by including u j copies of  j when  j = 0, for all 1 when  = 1) when K = {k}. A transversal design TD (k, n) is a (k, )-GDD of type nk . We write TD(k, n) when  = 1. A transversal design is idempotent if its element set is {1, . . . , k} × {1, . . . , n}, and its block set contains {{(i , j ): 1 i k}: 1 j n}. A pairwise balanced design with blocksizes K and order v (( K , )-PBD of order v ) is a ( K , )-GDD of type 1 v ; we write K -PBD when  = 1. Then a balanced incomplete block design ((k, )-BIBD) is a (k, )-PBD; we write k -BIBD when  = 1. An incomplete pairwise balanced design of order v with holesize h , blocksizes K , and index  is a triple ( V , H , B ) for which | V | = v , | H | = h , H  V , B contains a set of subsets of V for which | B |  K whenever B  B , and for every pair of distinct elements x, y  V , the number of blocks in {{x, y }  B  B } is 0 if {x, y }  H and  otherwise. The notation ( K , )-IPBD( v , h) is used; we may omit  when it is 1, and write k instead of K when K = {k}. Let K be a set of positive integers, each at least 2. Then define  ( K ) = gcd{k - 1: k  K } and k ( K ) = gcd 2 : kK . Wilson establishes a crucial asymptotic existence result: Theorem 2.1. (See [25].) Let K be a set of integers, each at least 2. Let  be a positive integer. For all sufficiently n large n satisfying (n - 1)  0 (mod  ( K )) and  2  0 (mod ( K )), there exists a ( K , )-PBD of order n. In particular for K = {k}, when (n - 1)  0 (mod k - 1),  exists a (k, )-BIBD of order n. Colbourn and Rödl prove a variant that we use: Theorem 2.2. (See [6].) Let  > 0. Let K = {k1 , . . . , km } be a set of block sizes. Let { p 1 , . . . , pm } be nonm negative numbers with i =1 p i = 1. For all sufficiently large v satisfying v - 1  0 (mod  ( K )) and v  0 (mod ( K )), there is a K -PBD of order v in which, for each 1 i m, the fraction of pairs appearing 2 in blocks having size ki is in the range [ p i -  , p i +  ]. A stronger version of Theorem 2.2 is given in [26], and a variant for resolvable designs appears in [8]. Perhaps the most powerful generalization of Theorem 2.1 is due to Lamken and Wilson [16]. We in(r ,) be a complete digraph on n vertices with exactly  edges of color i joining troduce this next. Let K n (r ,) is any vertex x to any vertex y for every color i in a set of r colors. A family F of subgraphs of K n (r ,) (r ,) if every edge e  E ( K n ) belongs to exactly one member in F . Given a fama decomposition of K n (r ,) is a decomposition F such that every ily  of edge-r -colored digraphs, a  -decomposition of K n graph F  F is isomorphic to some graph G   . For a vertex x of an edge-r -colored digraph G , the degree-vector of x is the 2r -vector d(x) = (in1 (x), out1 (x), in2 (x), out2 (x), . . . , inr (x), outr (x)), where in j (x) and out j (x) denote the indegree and outdegree of vertex x in the spanning subgraph of G by
n 2

 0 mod

k 2

, and n is sufficiently large, there

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1443

edges of color j , respectively, for 1 j r . We denote by  (G ) the greatest common divisor of the integers t such that the 2r -vector (t , t , . . . , t ) is an integral linear combination of the vectors d(x) as x ranges over the vertex set V (G ) of G . Equivalently,  (G ) is the smallest positive integer t 0 such that (t 0 , t 0 , . . . , t 0 ) is an integral linear combination of the vectors {d(x)}. Let  be a family of simple edge-r -colored digraphs and let  () denote the greatest common divisor of the integers t such that the 2r -vector (t , t , . . . , t ) is an integral linear combination of the vectors {d(x)} as x ranges over all vertices of all graphs in  . For each graph G   , let (G ) = (m1 , m2 , . . . , mr ), where mi is the number of edges of color i in G . We denote by () the greatest common divisor of the integers m such that (m, m, . . . , m) is an integral linear combination of the vectors {(G ): G   }. Equivalently, () is the smallest positive integer m0 such that (m0 , m0 , . . . , m0 ) is an integral linear combination of the (r ,) vectors {(G )}. A graph G 0   is useless when it cannot occur in any  -decomposition of K n .  is admissible when no member of  is useless. Theorem 2.3. (See [16].) Let  be an admissible family of simple edge-r-colored digraphs. For all sufficiently (r ,) large n satisfying (n - 1)  0 (mod  ()) and n(n - 1)  0 (mod ()), a  -decomposition of K n exists. Theorem 2.3 has numerous consequences for the existence of various classes of combinatorial designs. Building on Theorem 2.3, Liu establishes the following: Theorem 2.4. (See [17].) Let K be a set of integers, each at least 2. Let m and  be positive integers. For all n sufficiently large n satisfying m(n - 1)  0 (mod  ( K )) and m2 2  0 (mod ( K )), there exists a ( K , )n GDD of order m . Mohácsy and Ray-Chaudhuri prove a result for a fixed number of groups when the index is 1. Theorem 2.5. (See [18,19].) Let k and u be integers with u k 2. For all sufficiently large m satisfying m(u - 1)  0 (mod k - 1) and m2 u (u - 1)  0 (mod k(k - 1)), there exists a k-GDD of type mu .  and Straus: This subsumes a classical result of Chowla, Erdos, Theorem 2.6. (See [4].) Let k 2 be an integer. For all sufficiently large m, there exists a TD(k, m).

3. Packings, coverings, and the optima We use known asymptotic existence results to treat asymptotic existence of packings and coverings in the cases that a k -BIBD does not exist. We require further definitions, to extend packings and coverings to have a `hole'. A packing with blocksize k with a hole ( V , H , B ) is a set V of elements, a subset (hole) H  V , and a set B of k -subsets of V , so that for every {x, y }  V , {x, y }  H , there is at most one B  B with {x, y }  B ; when {x, y }  H , there is no block B  B with {x, y }  B . The leave  of ( V , B ) is a graph with vertex set V ; pair {x, y } appears as an edge if and only if {x, y } H and is not a subset of any block of B . A covering with blocksize k with a hole ( V , H , B ) is a set V of elements, a subset H  V , and a set B of k -subsets of V , so that for every {x, y }  V , {x, y }  H , there is at least one B  B with {x, y }  B . The excess  of ( V , B ) is a multigraph with vertex set V ; the number of times pair {x, y } appears as an edge is exactly xy when {x, y }  H , and xy - 1 otherwise, where xy is the number of blocks of B that contain {x, y }. A packing with blocksize k ( V , B ) is a packing with blocksize k with a hole ( V , , B ), and a covering with blocksize k ( V , B ) is a covering with blocksize k with a hole ( V , , B ). A maximum packing with blocksize k is a packing with blocksize k ( V , B ) with the most blocks among all packings with blocksize k on | V | elements; equivalently, its leave has the fewest edges. A minimum covering with blocksize

1444

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

k is a covering with blocksize k ( V , B ) with fewest blocks among all coverings with blocksize k on | V | elements; equivalently, its excess has the fewest edges. Suppose that ( V , H , B ) is a packing with blocksize k with a hole, with v = | V |, h = | H |, and n = | V \ H |. Let x be a vertex in V \ H . The number of pairs on V that contain x is congruent to v - 1 modulo k - 1. The number containing x that appear in blocks of B is congruent to 0 modulo k - 1. Hence x has degree congruent to v - 1 modulo k - 1 in the leave. When the hole is nonempty, elements in the hole have degrees congruent to n modulo k - 1 in the leave. By the same token, in the excess of a covering with blocksize k with a hole, x has degree congruent to -( v - 1) modulo k - 1; elements in the hole have degrees congruent to -n modulo k - 1. We employ specific types of packings and coverings with holes in which the leave or excess has all vertices in the hole of degree 0. For an integer n  0 (mod k(k - 1)) and an integer h 1, let   h - 1 (mod k - 1) and  -(h - 1) (mod k - 1) with 0 , < k - 1. Then an optimum packing with blocksize k with a hole, k -OP(n + h, h), is a packing with blocksize k on n + h elements whose leave has degree  on each vertex not in the hole, and 0 on each vertex in the hole; and an optimum covering with blocksize k with a hole, k -OC(n + h, h), is a covering with blocksize k on n + h elements whose excess has degree on each vertex not in the hole, and 0 on each vertex in the hole. When h  1 (mod k - 1),  = = 0. In this case, a k-OP( v , h) and a k-OC( v , h) are the same, and are equivalent to a k -IPBD( v , h). In any packing with blocksize k on v = n + h elements with n  0 (mod k(k - 1)), no vertex can have degree smaller than  in the leave; and in any covering with blocksize k on v = n + h in the excess. Indeed, elements with n  0 (mod k(k - 1)), no vertex can have degree smaller than choosing and  so that v - 1 ; ,   v - 1 (mod k - 1);  - < k - 1; and =  when v v  1 (mod k - 1), every packing with blocksize k on v elements contains at most v ,k = k(k- 1)
v blocks, while every covering with blocksize k on v elements contains at least L v ,k = k( blocks. k-1) Then v ,k is at least the Johnson bound, and  v ,k is at most the Schönheim bound. The purpose of this paper is to prove the following two results.

Theorem 3.1. There is a constant pk such that for all v k, the number of blocks in a maximum packing with blocksize k on v elements is at least v ,k - pk and at most v ,k . Theorem 3.2. There is a constant ak such that for all v k, the number of blocks in a minimum covering with blocksize k on v elements is at least L v ,k and at most L v ,k + ak . We establish these results in a number of steps. Treating an arbitrary but fixed value of k , in Section 4, we show that for every c satisfying 0 c < k(k - 1), there exist positive integers nc  0 (mod k(k - 1)) and hc  c (mod k(k - 1)) so that a k -OP(nc + hc , hc ) exists; we also show that for every c satisfying 0 c < k(k - 1), there exist positive integers mc  0 (mod k(k - 1)) and c  c (mod k(k - 1)) so that a k -OC(mc + c , c ) exists. This provides a single example for optimal packings and coverings with a hole in every congruence class modulo k(k - 1). In Section 5, we use these results to establish that there exist integers k and uk , depending only on k , so that whenever v k , uk for which a k -OP( v , h) exists, and there also exists an uk for which a there exists an h k -OC( v , ) exists. From this, because uk is fixed and independent of v , we establish Theorems 3.1 and 3.2 by filling the holes. The crucial step, particularly for coverings, is producing one example in each congruence class. We treat this next. 4. One example in each congruence class In the case when h  1 (mod k - 1), a k -OP( v , h) and a k -OC( v , h) coincide with a k -IPBD( v , h), so we treat this situation first; subsequently the packing and covering cases differ. 4.1. Packing and covering: v  1 (mod k - 1) An incomplete transversal design ITD(k, n +  ; ) is a set V of k(n + ) elements, of which k form a hole H . The elements are partitioned into k groups G 1 , . . . , G k so that |G i  H | =  for 1 i k .

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1445

This set is equipped with a set of k -subsets (blocks) with the property that every pair of elements that appears in a group or appears in the hole H appears in no block, and every other pair appears in exactly one block. Lemma 4.1. Let k 2 be an integer. Let 0



k. For all sufficiently large n, an ITD(k, n +  ; ) exists.

Proof. Using Theorem 2.6, choose  so that a TD(k + 1, ), a TD(k + 1,  + 1), a TD(k + 1,  + 2), and a TD(k + 1,  + 3) all exist. Delete one group in each to form an idempotent TD(k, v ) for each v  {,  + 1,  + 2,  + 3}. For n sufficiently large, there is an { + 1,  + 2,  + 3}-PBD of order n +  + 1 containing a block of size  + 1 by Theorem 2.2. (Because  ({ + 1,  + 2,  + 3}) = 1 and ({ + 1,  + 2,  + 3}) = 1, this follows by choosing 0 <  < 1 and choosing the fraction of pairs 4 in blocks of size  + 1 to be 2 .) Delete all but  elements from a block of size  + 1, and remove the block of size  making a hole, to form an {,  + 1,  + 2,  + 3}-IPBD(n + , ). Give every element weight k , and use the idempotent TDs to inflate all blocks. The k elements arising from the  elements of hole in the IPBD form the hole of the ITD. 2 Lemma 4.2. Let h be an integer for which h  1 (mod k - 1) and k infinitely many integers  for which a k-IPBD( k(k - 1) + h, h) exists. h k(k - 1) + 1. Then there exist

h-k Proof. Let  = k -1 . Choose  so that a k -BIBD of order  (k - 1) + k and an ITD(k,  (k - 1) +  ; ) both exist. (Use Lemma 4.1 for the existence of the ITD.) Start with the ITD on the elements of V having a hole on the elements in H  V . Add k -  new elements N  . For 1 i k , let N i consist of the  elements in the i th group of the ITD that appear in H . Place on the elements of the i th group, together with N  , the blocks of a copy of the k -BIBD, omitting a block on the elements of N i  N  . On the  k(k - 1) + (k - 1) + k =  k(k - 1) + h elements of V  N  , all pairs are covered except k those within the hole on elements N   i =1 N i of size h = (k - 1) + k . 2

Corollary 4.3. Whenever c  1 (mod k - 1) and 0 c < k(k - 1), there are infinitely many integers nc and hc with nc  0 (mod k(k - 1)) and hc  c (mod k(k - 1)) so that a k-OP(nc + hc , hc ) exists. Proof. Set hc = c if c nc =  k(k - 1). 2 k , and hc = k(k - 1) + 1 if c = 1. Apply Lemma 4.2 with h = hc , and set

The same argument establishes: Corollary 4.4. Whenever c  1 (mod k - 1) and 0 c < k(k - 1), there are infinitely many integers mc and with mc  0 (mod k(k - 1)) and c  c (mod k(k - 1)) so that a k-OC(mc + c , c ) exists. 4.2. Packing: v  1 (mod k - 1) Lemma 4.5. For every integer c satisfying 0 c < k(k - 1), there exist an nc  0 (mod k(k - 1)) and an hc  c (mod k(k - 1)) for which a k-OP(nc + hc , hc ) exists. Proof. When c > 0, write c = s(k - 1) + d with 1 d < k . When c = 0, set s = d = k - 1. If d = 1, apply Lemma 4.3. Otherwise choose   1 (mod k(k - 1)) and N >  so that N   (mod k - 1); a k -GDD of type d exists (Theorem 2.4); an  -BIBD of order N exists (Theorem 2.1); and an ITD(k, d( N -  ) + s, s) exists (Lemma 4.1). Treat the  -BIBD as an  -GDD of type 1 N -  1 by removing a block, and inflate using the k GDD of type d to form a k -GDD of type d N - (d )1 . Adjoin d - s infinite elements to the
c

1446

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

ITD(k, d( N -  ) + s, s). On each group together with the infinite elements, place a copy of the k GDD of type d N - (d )1 , aligning the group of size d on the s elements in the intersection of the group and the hole of the ITD, together with the d - s infinite elements. The result is a k GDD of type dk( N - ) (d + s(k - 1))1 . Treat this as a packing. On the dk( N -  ) points not in the large hole, the leave has degree d - 1, so the result is a k -OP(nc + hc , hc ) with nc = dk( N -  ) and hc = d + s(k - 1). Because dk  0 (mod k) and N -   0 (mod k - 1), nc  0 (mod k(k - 1)). Because d  d (mod k(k - 1)), hc  c (mod k(k - 1)). 2 4.3. Covering: v  1 (mod k - 1) We employ some further, more specialized, combinatorial objects to treat coverings for the remaining congruence classes. Let V be a set of elements; B be a set of k -subsets of V ; G = {G 1 , . . . , G r } be a partition of V , j t. and H = { H 1 , . . . , H t } be a partition of V . Suppose that |G i  H j | =  for all 1 i r , 1
j i Further suppose that for every 2-subset {x, y }  V , either {x, y }  , or there  i =1 2 j =1 2 is exactly one B  B with {x, y }  B , but not both. Then ( V , G , H, B ) is a double group divisible design with blocksize k (k -DGDD) of type (r )t . A holey transversal design with blocksize k (k -HTD) of type r is a k -DGDD of type (r )k .

r

G

t

H

Theorem 4.6. Let k

2 be an integer. For all sufficiently large r, there exists a k-HTD of type 2r .

Proof. Choose K = {x1 , . . . , xs } so that  ( K ) = ( K ) = 1, and so that for each 1 i s , xi is large enough to ensure that Theorem 2.6 yields a TD(k + 1, xi ). Remove one group (and rename elements as needed) to form an idempotent TD(k, xi ). When r is large enough, Theorem 2.4 yields a K -GDD ( V , G , B ) of type 2r with groups G = {G 1 , . . . , G r }. The elements of the k-HTD to be formed are V × {0, . . . , k - 1}. For each block B  B , on the elements B × {0, . . . , k - 1}, align the k groups on { B × {i }: 0 i < k} to place the blocks of an idempotent TD(k, | B |). In the resulting design, one set of j r. 2 groups is formed by V × {i } for 0 i < k , the other by G j × {0, . . . , k - 1} for 1 Theorem 4.7. Let k 2 be an integer. For all sufficiently large integers r and t satisfying t - 1  0 (mod k - 1) t k and 2  0 mod 2 , there exists a k-DGDD of type (2r )t . Proof. Apply Theorem 2.1 to form a k -BIBD ( V , B ) with t elements. Apply Theorem 4.6 to form a k -HTD of type 2r . To form the k -DGDD, use elements V × {a, b} × {1, . . . , r }. For every B  B , place a copy of the HTD on B × {a, b} × {1, . . . , r }, aligning groups of size 2k on B × {a, b} × {i } for 1 i r , and groups of size 2r on {x} × {a, b} × {1, . . . , r } for x  B . 2 The key construction follows: Theorem 4.8. Let t , r , y be positive integers so that r  0 (mod k(k - 1)), t  1 (mod k(k - 1)), and y  2 (mod k - 1). Suppose that there exist (1) a k-DGDD of type (2r )t ; (2) a k-BIBD on 2t + k - 2 elements; (3) a k-OC(2r + y , y ). Then there is a k-OC(2rt + k - 2 + y , 2r + y + k - 2). Proof. Let V = {ai , j , b i , j : 1 i r , 1 j t } be the elements of the k -DGDD, with groups aligned j t } and H j = {ai , j , b i , j : 1 i r }. Let B be its set of blocks. Adjoin a so that G i = {ai , j , b i , j : 1 set C of k - 2 new elements. For 1 i r , on C  G i , form a k -BIBD on 2t + k - 2 elements, aligning a block on C  {ait , b it }; then delete that block, and call the resulting set of blocks Di . Adjoin a set R

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1447

with y further new elements. For 1 j < t , on R  H j place a k -OC(2r + y , y ) with the hole aligned on R , whose block set is E j . r We consider the design on the 2rt + k - 2 + y elements V  R  C with block set B  i =1 Di  All blocks have size k because each ingredient contains only blocks of size k . First we show that the design is a covering with a hole on R  C  H t . Two elements in the hole do not appear together in a block. An element from G i  H j with j < t appears in a block with each element of C  (G i  H t ) in Di ; it appears in a block with each element of R in E j ; and it appears with each element of H t \ G i in a block of B . Consider two distinct elements x  G i  H j and y  G m  H n with j , n < t . If i = m and j = n, then {x, y } = {ai , j , b i , j } appears in a block of Di (and also in at least one block of E j ). If i = m and j = n, then {x, y } appears in at least one block of E j . If i = m and j = n, then {x, y } appears in one block of Di . If i = m and j = n, then {x, y } appears in one block of B . Hence the design is a covering with a hole on R  C  H t . Secondly, we establish that it has the correct excess degrees to be an optimal covering with a hole, a k -OC. The design has 2rt + k - 2 + y elements. Because r  0 (mod k - 1), the number of elements satisfies 2rt + k - 2 + y  y - 1 (mod k - 1). The hole has 2r + k - 2 + y elements. Because r  0 (mod k - 1), the number of elements in the hole satisfies 2r + k - 2 + y  y - 1 (mod k - 1). y < k - 1. We must show that every element not in the Let y  -( y - 1) (mod k - 1) with 0 hole has degree y + 1 in the excess, and every element in the hole has degree 0 in the excess. r }. It We treat elements in the hole first. Each element of C appears only in blocks {Di : 1 i appears in r (t - 1) pairs to be covered, and appears in r (t - 1)/(k - 1) blocks, with (t - 1)/(k - 1) blocks arising in each of {Di : 1 i r } because this was constructed from a BIBD. Each element j < t }. Because elements of R have excess degree 0 in the k of R appears only in blocks {E j : 1 OC(2r + y , y ) forming E j , they have excess degree 0 in the union. Each element of H t appears only in blocks of B , and has excess degree 0. Now consider an element x  G i  H j , with j = t so that x is not in the hole. Then x appears in elements of B , Di and E j . It appears in 2(r - 1)(t - 1)/(k - 1) blocks of B , because it arises from the DGDD. It appears in (2t + k - 3)/(k - 1) blocks of Di , because it arises from a BIBD. Now in E j , x is not in the hole of the k -OC(2r + y , y ), and hence it arises 1 (2rt + k - 2 + y + y ) blocks, and in (2r - 1 + y + y )/(k - 1) blocks. So in total x appears in k- 1 because it appears in (2rt + k - 2 + y ) - 1 pairs, its excess degree is y + 1. Because 2r + y + k - 2  y - 1 (mod k - 1) and y  2 (mod k - 1), the result is the k -OC(2rt + k - 2 + y , 2r + y + k - 2). 2 Corollary 4.9. For each 0 c < k(k - 1), there exist integers mc and c  c (mod k(k - 1)) for which a k-OC(mc + c , c ) exists.
c t -1 j =1 E j .

with mc  0 (mod k(k - 1)) and

Proof. Let t 0 and r0 be integers with t 0  1 (mod k(k - 1)) and t 0 > 1 so that whenever r and t  1 (mod k(k - 1)), (1) there is a k -DGDD of type (2r )t (apply Theorem 4.7), and (2) there is a k -BIBD on 2t + k - 2 ( k (mod k(k - 1))) elements (apply Theorem 2.1).

r0 , t

t0 ,

When c  1 (mod k - 1), apply Corollary 4.4 to choose one k -OC(mc + c , c ) with mc  0 (mod k(k - 1)) and mc r0 . In general, when a k -OC(mc + c , c ) with mc  0 (mod k(k - 1)) and mc r0 exists, Theorem 4.8 produces a k -OC(mc t 0 + k - 2 + c , mc + c + k - 2). Set mc +k-2 mod k(k-1) = mc (t 0 - 1), which exceeds r0 and is a multiple of k(k - 1). Set c +k-2 mod k(k-1) = mc + c + k - 2  c + k - 2 (mod k(k - 1)). Then k - 2 applications of Theorem 4.8 handle all congruence classes. 2 5. Asymptotic existence Our next task is to handle not just one example for hole size in each congruence class modulo k(k - 1), but to extend to all sufficiently large orders. Theorem 5.1. Let k 2 be an integer. Then there are constants k and uk so that whenever v k-OP( v , h) and a k-OC( v , h) with h uk .

k , there is a

1448

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

Proof. By Corollary 4.9, for 0 c < k(k - 1) there are integers mc  0 (mod k(k - 1)) and c  c (mod k(k - 1)) for which a k -OC(mc + c , c ) exists. By Lemma 4.5, for 0 c < k(k - 1) there are integers nc  0 (mod k(k - 1)) and hc  c (mod k(k - 1)) for which a k -OP(nc + hc , hc ) exists. Set uk = max{nc + hc , mc + c : 0 c < k(k - 1)}. Using Theorem 2.4, choose an integer x for which k -GDDs of type (k(k - 1))x exist for all x  {x, x + 1, x + 2, x + 3}. Again using Theorem 2.4, choose an integer r for which an {x + 1, x + 2, x + 3}p uk and all g r . Then set k = rk(k - 1) + uk , a constant GDD of type p g exists for all 1 depending only on k . We develop the remainder of the proof for packings; that for coverings parallels it very closely. Let v k be an integer, and write v =  k(k - 1) + c with 0 c < k(k - 1). Write v - hc = gnc + d so that d  0 (mod k(k - 1)) and d < nc . Let n = nc /(k(k - 1)) and d = d/(k(k - 1)). g Construct a k -GDD of type nc d1 as follows. Form an {x + 1, x + 2, x + 3}-GDD of type (n ) g +1 . Delete all but d elements in one group to form an {x, x + 1, x + 2, x + 3}-GDD of type (n ) g (d )1 . Inflate using weight k(k - 1), employing k -GDDs of type (k(k - 1))x for x  {x, x + 1, x + 2, x + 3}, to g form a k -GDD of type nc d1 . Then add hc new elements, and place a k -OP(nc + hc , hc ) on each group of size nc together with the hc new elements, aligning the hole on these hc elements. The result is a k -OP( v , hc + d), and hc + d uk as required. 2 Proof of Theorem 3.1. When v < k , a maximum packing with blocksize k contains at least v ,k - 2k  blocks, and 2k is a constant. When v k , form a k -OP( v , h) with h uk , which has at least v ,k - uk uk blocks and is a constant. 2 2 2 Proof of Theorem 3.2. When v < k , a minimum covering with blocksize k requires at most 2k blocks, which is a constant. When v k , form a k-OC( v , h) with h uk , which has at most L v ,k blocks. A covering on h points in which every block contains some pair that is covered only once has u at most 2k blocks, which is a constant independent of v . Use this to fill the hole. 2 6. Conclusion For t = 2, our results establish that the elementary Johnson and Schönheim bounds are essentially the correct ones, in that the respective optima cannot differ from them by more than an additive constant. Unless this constant can be shown to be quite small, the specific value obtained for the constant is not of particular interest. Without recourse to Proposition 1.1 or a similar statement, we see no way at present to obtain differences from the bounds that are bounded by a quantity as small as (say) k in general, although it is plausible that such bounds hold. Acknowledgment We thank an anonymous referee for helpful comments on the presentation. References
[1] T. Beth, D. Jungnickel, H. Lenz, Design Theory, vol. I, second edition, Encyclopedia Math. Appl., vol. 69, Cambridge University Press, Cambridge, 1999. [2] Y. Caro, R. Yuster, Packing graphs: the packing problem solved, Electron. J. Combin. 4 (1) (1997), Research Paper 1, approx. 7 pp. (electronic). [3] Y. Caro, R. Yuster, Covering graphs: the covering problem solved, J. Combin. Theory Ser. A 83 (2) (1998) 273­282.  E.G. Straus, On the maximal number of pairwise orthogonal Latin squares of a given order, Canad. J. [4] S. Chowla, P. Erdos, Math. 12 (1960) 204­208. [5] C.J. Colbourn, J.H. Dinitz, D.R. Stinson, Applications of combinatorial designs to communications, cryptography, and networking, in: Surveys in Combinatorics, Canterbury, 1999, in: London Math. Soc. Lecture Note Ser., vol. 267, Cambridge Univ. Press, Cambridge, 1999, pp. 37­100. [6] C.J. Colbourn, V. Rödl, Percentages in pairwise balanced designs, Discrete Math. 77 (1­3) (1989) 57­63. [7] C.J. Colbourn, P.C. van Oorschot, Applications of combinatorial designs in computer science, ACM Comput. Surv. 21 (2) (1989) 223­250. [8] P. Dukes, A.C.H. Ling, Asymptotic existence of resolvable graph designs, Canad. Math. Bull. 50 (4) (2007) 504­518.





Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1449

[9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24]

[25] [26]

 H. Hanani, On a limit theorem in combinatorial analysis, Publ. Math. Debrecen 10 (1963) 10­13. P. Erdos, P. Frankl, V. Rödl, Near perfect coverings in graphs and hypergraphs, European J. Combin. 6 (4) (1985) 317­326. D.A. Grable, More-than-nearly-perfect packings and partial designs, Combinatorica 19 (2) (1999) 221­239. T. Gustavsson, Decompositions of large graphs and digraphs with high minimum degree, PhD thesis, Dept. of Mathematics, Univ. of Stockholm, 1991. S.M. Johnson, A new upper bound for error-correcting codes, IRE Trans. IT-8 (1962) 203­207. A.V. Kostochka, V. Rödl, Partial Steiner systems and matchings in hypergraphs, Random Structures Algorithms 13 (3­4) (1998) 335­347. N.N. Kuzjurin, On the difference between asymptotically good packings and coverings, European J. Combin. 16 (1) (1995) 35­40. E.R. Lamken, R.M. Wilson, Decompositions of edge-colored complete graphs, J. Combin. Theory Ser. A 89 (2) (2000) 149­ 200. J. Liu, Asymptotic existence theorems for frames and group divisible designs, J. Combin. Theory Ser. A 114 (3) (2007) 410­420. H. Mohácsy, The asymptotic existence of group divisible designs of large order with index one, J. Combin. Theory Ser. A 118 (7) (2011) 1915­1924. H. Mohácsy, D.K. Ray-Chaudhuri, An existence theorem for group divisible designs of large order, J. Combin. Theory Ser. A 98 (1) (2002) 163­174. V. Rödl, On a packing and covering problem, European J. Combin. 6 (1) (1985) 69­78. J. Schönheim, On coverings, Pacific J. Math. 14 (1964) 1405­1411. D.R. Stinson, Combinatorial Designs, Springer-Verlag, New York, 2004. V.H. Vu, New bounds on nearly perfect matchings in hypergraphs: higher codegrees do help, Random Structures Algorithms 17 (1) (2000) 29­63. R.M. Wilson, The construction of group divisible designs and partial planes having the maximum number of lines of a given size, in: Proc. Second Chapel Hill Conf. on Combinatorial Mathematics and its Applications, Univ. North Carolina, Chapel Hill, NC, 1970, Univ. North Carolina, Chapel Hill, NC, 1970, pp. 488­497. R.M. Wilson, An existence theory for pairwise balanced designs. III. Proof of the existence conjectures, J. Combin. Theory Ser. A 18 (1975) 71­79. R.M. Wilson, The proportion of various graphs in graph designs, in: R.A. Brualdi, S. Hedayat, H. Kharaghani, G. Khosrovshahi, S. Shahriari (Eds.), Combinatorics and Graphs: The Twentieth Anniversary Conference of IPM Combinatorics, American Mathematical Society, Providence, RI, 2010, pp. 251­255.

Perspectives

The BioIntelligence Framework: a new computational platform for biomedical knowledge computing
Toni Farley,1 Jeff Kiefer,1 Preston Lee,1 Daniel Von Hoff,1 Jeffrey M Trent,1 Charles Colbourn,2 Spyro Mousses1
< Additional material are

published online only. To view these files please visit the journal online (http://dx.doi.org/ 10.1136/amiajnl-2011-000646).
1

The Translational Genomics Research Institute (TGen), Center for BioIntelligence, Phoenix, Arizona, USA 2 School of Computing, Informatics, Decision Systems Engineering, Arizona State University, Tempe, Arizona, USA Correspondence to Dr Spyro Mousses, The Translational Genomics Research Institute (TGen), Center for BioIntelligence, 445 N. Fifth Street, Phoenix, AZ 85004, USA; smousses@tgen.org Received 19 October 2011 Accepted 7 July 2012 Published Online First 2 August 2012

ABSTRACT Breakthroughs in molecular profiling technologies are enabling a new data-intensive approach to biomedical research, with the potential to revolutionize how we study, manage, and treat complex diseases. The next great challenge for clinical applications of these innovations will be to create scalable computational solutions for intelligently linking complex biomedical patient data to clinically actionable knowledge. Traditional database management systems (DBMS) are not well suited to representing complex syntactic and semantic relationships in unstructured biomedical information, introducing barriers to realizing such solutions. We propose a scalable computational framework for addressing this need, which leverages a hypergraph-based data model and query language that may be better suited for representing complex multilateral, multi-scalar, and multi-dimensional relationships. We also discuss how this framework can be used to create rapid learning knowledge base systems to intelligently capture and relate complex patient data to biomedical knowledge in order to automate the recovery of clinically actionable information.

genomic interpretation require: (a) a fundamentally different computational framework for storing and representing disparate data types with complex relationships, and (b) advanced software applications that leverage this framework to structure the representation of prior knowledge so that it can be intelligently linked to patient data. We propose a framework conceptually based on requirements and cognitive strategies for knowledge computing, previously introduced as the BioIntelligence Framework.2 Our framework is compatible with future directions toward computational intelligence. Since it can support the capturing and querying of multilateral and multi-scalar relations among genomes, phenotype, environment, lifestyle, medical history, and clinical outcome data, our platform can support systems with higher order functions such as inference and learning. This will ultimately allow genomic data to be intelligently repurposed beyond personalized medicine to support more sophisticated translational research and highly iterative knowledge discovery.

BIOINTELLIGENCE FRAMEWORK INTRODUCTION
Next generation genomic profiling technologies are generating deep and detailed characterizations of patients and disease states. This data-intensive approach is providing unprecedented insights that can be used to resolve mechanistic complexity and clinical heterogeneity, thereby revolutionizing how we study, manage, and treat complex diseases. To support this revolution, bioinformatics tools are rapidly emerging to process and analyze large-scale complex molecular data sets for discovery research applications. Unfortunately, when it comes to clinical (n¼1) applications of genomics, the data deluge is rapidly outpacing our capacity to interpret rich data sets to extract medically useful and meaningful knowledge. The next great challenge will be to address the manual interpretation bottleneck through the development of computational solutions for intelligently linking complex patient data to actionable biomedical knowledge. This illuminates a need to represent and query large-scale complex relationships distributed across disparate types of biomedical knowledge. A recent report states a goal for the community is to transition from traditional database management to managing potentially unstructured data across many repositories.1 We propose the key challenges for intelligently linking prior knowledge to partially automate
128

Systems biology is concerned with emergent properties in complex interactions of systems of systems involving disparate data elements. Extracting useful information requires syntactic and semantic linking of data within and across large data sets. Systems modeled as networks based on binary graphs (where edges connect node pairs) are suited to capturing bilateral relationships and interactions. To represent multilateral relationships requires a fundamental change in how we model systems. We generalize the binary graph model to a hypergraph model, an approach which has been previously suggested,3 and introduce a hypergraphbased solution for representing multilateral relations and multi-scalar networks. Biological systems may benefit from a flexible data model that supports nesting of data elements and concept abstraction in a more natural manner than functionally equivalent relational counterparts, and the ability to readily query across multiple systems and abstraction layers representing complex relationships, leading to systems compatible with learning, reasoning, and inferencing. Following a model for human intelligence, information lives in different levels of the neocortex: from highly variable data inputs, to patterns, to patterns of patterns, to invariant concepts.4 Inspired by this model of intelligence, we extend the notion of a hypergraph to allow

J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

Perspectives
links among edges to capture relationships that cross bounds of scale and dimension, and develop a novel generic framework for capturing information that can benefit systems biology and other areas. We desire a solution that is flexible to include various types of data from disparate sources, extensible to scale to massive stores of information, and accessible to permit the efficient extraction of patient-centric knowledge. Figure 1 outlines the architecture of our BioIntelligence Framework, the components of which are: 1. A public hypergraph-based network for representing knowledge, including a. A scalable hypergraph-like model for representing a knowledge network b. Processes to automate populating and updating the network with public domain knowledge from multiple sources c. An efficient database solution for storing the network platform 2. A Patient Data Locker application built on top of the knowledge network, including: d. An accessible web-based solution for storing patient-centric knowledge e. Processes for structuring and formatting patient genomic and health data, inducing patient-centric subgraphs on the public hypergraph, and stratifying patients based on information in their lockers 3. A process for structuring and formatting analyst interpretation to facilitate feedback and rapid automated learning in the system.

Public hypergraph
A graph is defined G(V,E) where V is a set of vertices (nodes) and E is a set of edges (links) between two vertices. A hypergraph is a generalization of a graph in which an edge can connect any number of vertices. Biological networks have traditionally been modeled as graphs/networks. These graph models capture bilateral relationships among node pairs. Using hypergraphs as a modeling paradigm supports the characterization of multilateral relationships and processes.3 For example, in a general graph,

Figure 1 A BioIntelligence Framework for creating a hypergraph-like store of public knowledge and using this, along with an individual's genomic and other patient information, to derive a personalized genome-based knowledge store for clinical translation and discovery research.
J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646 129

Perspectives
an edge might represent a relationship between a gene and disease state. That relationship may change in the context of a drug, and a hypergraph can represent this contextual knowledge with an edge containing all three elements. Hypergraphs are proving useful for capturing semantic and biomedical information in semantic web technologies for biological knowledge management and semantic knowledge networks.5 6 Approaches to extracting knowledge from biological research literature to store in a hypergraph have been proposed,7 8 with similar techniques used for population stratification.9 To support a data intelligent system, we wish for information to be stored not only explicitly in the data itself, but implicitly in how the data are linked, and capture this in our model. Abstraction is permitted by allowing hyperedges to contain other edges, forming a nested graph structure. A machine learning model called hierarchical temporal memory (HTM) mimics the human neocortex.4 10 Inspired by this design, we store knowledge at different levels of granularity, from single points of data, to collections of points, abstracting out to collections of collections. Thus, we perceive edges in lower levels of abstraction as nodes in higher levels, thereby permitting the network to be viewed and operated on within and across different scales of abstraction. Information is stored on the nodes and edges of the network and accessed via processes. Populating the network platform requires processes to pull different types of information from multiple sources, such as the Cancer Biomedical Informatics Grid (caBIG) and BioWarehouse,11 12 and other tools and techniques.13e15 The processes are represented as S0. Sm in figure 1, and take unstructured, public domain knowledge as input and structures it as input to the public network. The most common database management system (DBMS) is based on the relational data model, which is best suited to capturing data structured in a predefined schema, and presents limitations in handling complexity and scalability.16 Our solution handles unstructured and semi-structured data, and is in the scope of non-relational (NoSQL) databases, which do not require pre-defined schemas.16 17 Such databases include those based on graph models and triplestores (eg, RDF), and are best suited to capturing binary relationships between two elements (a triplestore is even more restrictive as it is essentially a `directed' binary graph). To effectively represent multilateral relationships and interactions present in biomedical data, hypergraph-based approaches have been suggested.3 18e21 The flexibility granted by allowing elements to contain elements allows processing knowledge at different levels of abstraction, in a less restrictive way than hierarchical models that restrict the network's topology. HyperGraphDB resembles our model, but uses a directed hypergraph and is built on a traditional database.22 Our model is more general, based on less restrictive undirected edges, and intended to be implemented natively, although a DBMS using our model may use other DB solutions as a persistent data store. In fact, other solutions can be built on our model as it is a generalization of other models. found in the online supplementary material, and demonstrates the added effort involved in capturing these complex relationships in a SQL database. The entities in figure 2 are represented as elements in our database. An attribute is a key/value pair, and a list of attributes (Attribute Set) is stored with each element. In this way, we can arbitrarily add any type of attribute to an element without changing the database structure. In the relational model, each entity type requires its own table, with pre-defined fields for attributes, and adding an attribute requires adding a field to the table, and migrating the database. A characteristic of the relational model is that all entities of the same type are stored in the same table. Our model is flexible in that it does not require prestructuring data, but we can certainly mimic this behavior if desired by enforcing a rule that requires all elements to have an attribute with key¼`type.' Using our model, this decision is left to the database designer, and not enforced by the model itself. An element in our model can contain an arbitrary number of other elements (allowing `has-a' and `has-many ' relationships). These elements are referenced in the `Internal Element Set' of the element. For example, in figure 2, element g is a gene and contains three gene variant elements (v1, v2, v3) in its internal element set. This is an example of a multi-lateral relationship as g can be viewed as a `hyperedge' in a hypergraph, connecting three `nodes.' While this behavior is easy to model in a relational database, using a one-to-many relation, it becomes more complex when an element contains an arbitrary number of arbitrary types of elements. For example, the element R2 represents a molecular state, which in this case is a protein p associated with a protein state ps1. In a relational database, we can capture these relationships in a `molecular state' table with foreign key fields pointing to a `protein' table and a `protein state' table. Now consider a molecular state that exists in the context of a modifier drug (ie, the state of the protein is perturbed by a modifier drug in a laboratory experiment). To capture this in our model, we can simply create a new element with three internal elements: p, ps1 and the element representing the modifier drug. To capture this in the relational database, we would need to either add a field to the `molecular state' table, which may be blank for many records, or create a new table with the three related fields. Both of the later options require a change to the underlying data structure, and migrating the database. The elements shown in figure 2C represent higher-level concepts and recursive nesting of elements at different levels, illustrating the ability to flexibly and efficiently capture multiscalar relationships among elements, the motivation behind our data model. For example, note that R1 captures the relationship that gene g codes for protein p. C1 captures the genetic event concept, where gene variant v2 changes the state of the protein (the molecular state represented by R3). C2 is a drug response concept representing the higher-level concept that C1, in the context of the pharmaceutical ph, leads to a changed molecular state, R2. These combined biological and pharmacologic effects lead to a change in disease state to ds2, captured by the clinical response concept C3. The internal (nested) element sets contain the topology of the network, that is, they define how entities are related, and capture meaning in those relationships. Further details to describe the nature of the relationships can always be stored in attributes of the elements. The `External Element Set' of an element is the inverse of internal element relations. For example, v1, v2, and v3 all have g in their external element set. While it is not necessary to store this information (the network of relationships among elements
J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

Data model
An example use of our model is shown in figure 2. This example captures multilateral, multiscalar, and multidimensional relationships using a general model (A), and viewing the network at different levels of abstraction (B and C). The elements shown in this solution are described in the legend of the image, and based on a real-world problem we are exploring. A possible schema model for a relational database capturing these same data can be
130

Perspectives
Figure 2 An illustrative example of storing biomedical information in our proposed knowledge base: a component of the BioIntelligence Framework. A shows our data model, and describes its components. B and C show the elements described in the legend (at the bottom of the figure) at two different levels of abstraction.

can be constructed via the internal element sets alone), it does aid in querying the database. We have defined three new types of queries associated with our model: recover, context, and expand. The expand and context queries retrieve all internal and external elements of an element, respectively, optionally limited by modifiers presented with the query. Recover is a combination of these, and returns both internal and external elements. All three query actions have an optional level constraint, defining how deep to traverse the graph when retrieving related elements. For instance, expand n will retrieve all internal elements, and their internal elements, recursively up to n times. For instance, we can view C3 as an abstraction; a clinical effect that we can relate to patients and other concepts. Expanding C3 by one level shows us that it represents a disease state ds2, triggered by a pharmacologic effect C2. Expanding C3 by two levels shows us the
J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

details of the pharmacologic effect, and so on. In this way, we can choose which level of abstraction we wish to view and compute over, and we can create new concepts that cross these layers of abstraction (multi-scalar relationships). By the definition of the internal and external element sets, it follows that our model naturally handles many-to-many relationships as well. In summary, we do not argue that a relational database is incapable of capturing the types of relationships we discuss here, rather that it requires more work, and added layers of complexity to the underlying structure of the database, which makes capturing and querying complex biomedical relationships more difficult. Our model is an abstraction of other models, including relational, graph, hierarchical, and object-oriented, and can therefore be used to model data represented using any and all of these models at once. The potential benefit of our model is
131

Perspectives
Figure 3 The network on the left is an example knowledge network platform. The darkened nodes represent gene variants present in an individual genome. The network on the top right is a genome-induced subgraph of the network. The network on the bottom right is a genome-induced subgraph, expanded out to include additional knowledge stored on edges in the connected-component each data element is contained in.

that it provides levels of scalability and flexibility that are difficult to achieve with existing models. We are currently developing a solution based on this model and will present additional details of the model and related query language in future publications.

Patient data locker
Given a patient's data (genomic, health, etc), we wish to recover related knowledge from our network using BioIntelligence Tools (BIT). The first step (BIT1 in figure 1) is a process to structure data as input to a process for inducing patient-relevant subgraphs of the knowledge network (BIT2 in figure 1). BIT1 integrates many types of data sets across multiple databases to support electronic medical and health records (EMR/EHRs), and is designed as a modular based system to provide metadata and indexing for queries. The next step (BIT2 in figure 1) is a process to extract relevant knowledge from the network based on individual patient information. An induced subgraph H(S,T) of network G(V,E) has the properties S3V, and for every vertex set Si of H, the set Si is an edge of H if and only if it is an edge in G. That is, H has the same edges that appear in G over the same set of nodes. We say that H is an induced subgraph of G, and H is induced by S. In our system, V is the set of nodes in the platform network G, and S is the set of nodes that map to an individual's genomic and health information. Thus, a subgraph is induced by an individual genome. The architecture in figure 1 shows an example public hypergraph, and private subgraphs stored in a data locker. These networks are detailed in figure 3, where the network on the left is a public knowledge network, and the darkened nodes are elements relevant to an individual's information (based on input patient data). The network on the top right is induced by this information, and contains all of the darkened nodes, and edges incident to them. Alternately, we can expand the information retrieved to include all connected components of the induced subgraph. An example of a connected-component induced subgraph is shown on the bottom right of figure 3. The most important characteristic of the data locker is that it contains all relevant knowledge to facilitate clinical translation. Induced subgraphs can be used to transform a large set of patient-relevant data to smaller, task-tailored formats void of extraneous detail. The patient data locker is linked to the public
132

knowledge store, and automatically updated to contain only a subset of information related to the patient. Thus, an expert need not develop their own intricate search queries and perform the tedious task of progressively reducing the amount of irrelevant data returned by the query. Any query that can be run on the entire knowledge network, can be run on the subgraph in a patient's locker, leading to a more precise subset of knowledge returned, and potentially faster querying speeds as the search space is reduced. The expert analyst is provided with knowledge tailored to a particular patient, partially automating the interpretation process, and a process (BIT3 in figure 1) allows the analyst to input new interpretation knowledge into the public network. We envision this type of feedback mechanism will support the inclusion of a learning model for our system, and allow the community to contribute to its growth. The system is diverse, providing framework and template libraries, allowing users to integrate their own tools for analysis, data collection, and beyond.

CONCLUSION
A deluge of biomedical data generated from next-generation sequencing (NGS) and clinical applications is overwhelming our ability to efficiently extract value from it. Existing bioinformatics tools were not developed to support clinical translation for an individual patient, causing an n¼1 translation bottleneck. A new architecture for managing biomedical data is desired, and we present the BioIntelligence Framework as a genomecompatible biomedical knowledge representation platform. Our future efforts to achieve the goals outlined in this paper include ensuring that we develop algorithms on this framework that minimally meet the performance expectations of existing solutions in practice.
Contributors All authors contributed to the ideas behind the framework. TF, CC, and PL contributed computer science expertise. JK, DVH, JMT, and SM contributed expertise in clinical genomics and translational research. Competing interests None. Provenance and peer review Commissioned; externally peer reviewed.

Open Access This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 3.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/3.0/ J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

Perspectives
REFERENCES
1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. Agrawal R, Ailamaki A, Bernstein PA, et al. The Claremont report on database research. ACM SIGMOD Record 2008;37:9e19. Mousses S, Kiefer J, Von Hoff D, et al. Using biointelligence to search the cancer genome: an epistemological perspective on knowledge recovery strategies to enable precision medical genomics. Oncogene 2008;27:S58e66. Klamt S, Haus U, Theis F. Hypergraphs and cellular networks. PLoS Comput Biol 2009;5:e1000385. Hawkins J, Blakeslee S. On Intelligence. New York: Times Books, 2004. Antezana E, Kuiper M, Mironov V. Biological knowledge management: the emerging role of the semantic web technologies. Brief Bioinform 2009;10:392e407. Zhen L, Jiang Z. Hy-SN: hyper-graph based semantic network. Knowledge-Based Systems 2010;23:809e16. Vailaya A, Bluvas P, Kincaid R, et al. An architecture for biological information extraction and representation. Bioinformatics 2005;21:430e8. Mukhopadhyay S, Palakal M, Maddu K. Multi-way association extraction and visualization from biological text documents using hyper-graphs: applications to genetic association studies for diseases. Artif Intell Med 2010;49:145e54. Vazquez A. Population stratification using a statistical model on hypergraphs. Phys Rev E Stat Nonlin Soft Matter Phys 2008;77:1e7. George D. How the Brain Might Work: a Hierarchical and Temporal Model for Learning and Recognition [dissertation]. Palo Alto, California: Stanford University, 2008. NCI. Cancer Biomedical Informatics Grid (caBIG). https://cabig.nci.nih.gov/ (accessed 16 Sep 2011). 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. Karp P. Biowarehouse database integration for bioinformatics. http://biowarehouse ai.sri.com/ (accessed 16 Sept 2011). Chen H, Ding L, Wu Z, et al. Semantic web for integrated network analysis in biomedicine. Components 2009;10:177e92. Tudor CO, Schmidt CJ, Vijay-Shanker K. eGIFT: mining gene information from the literature. BMC Bioinformatics 2010;11:418. Valentin F, Squizzato S, Goujon M, et al. Fast and efficient searching of biological data resourceseusing EB-eye. Brief Bioinform 2010;11:375e84. Leavitt N. Will NoSQL databases live up to their promise? Computer 2010;43:12e14. Angles R, Gutierrez C. Survey of graph database models. ACM Computing Surveys 2008;40:1e39. Olken F. Graph data management for molecular biology. OMICS 2003;7:75e8. Hu Z, Mellor J, Wu J, et al. Towards zoomable multidimensional maps of the cell. Nat Biotechnol 2007;25:547e55. Spreckelsen C, Spitzer K. Formalising and acquiring model-based hypertext in medicine: an integrative approach. Methods Inform Med 1998;37:239e46. Wu G, Li J, Hu J, et al. System: a native RDF repository based on the hypergraph representation for RDF data model. J Comput Sci Technol 2009;24:652e64. Iordanov B. HyperGraphDB: a generalized graph database. Proceedings of the 2010 International Conference on Web-age Information Management 2010:25e36.

PAGE fraction trail=5.5

J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

133

SIAM J. DISCRETE MATH. Vol. 27, No. 4, pp. 1844­1861

c 2013 Society for Industrial and Applied Mathematics 

SEQUENCE COVERING ARRAYS

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

YEOW MENG CHEE , CHARLES J. COLBOURN , DANIEL HORSLEY§ , AND JUNLING ZHOU¶ Abstract. Sequential processes can encounter faults as a result of improper ordering of subsets of the events. In order to reveal faults caused by the relative ordering of t or fewer of v events, for some fixed t, a test suite must provide tests so that every ordering of every set of t or fewer events is exercised. Such a test suite is equivalent to a sequence covering array, a set of permutations on v events for which every subsequence of t or fewer events arises in at least one of the permutations. Equivalently it is a (different) set of permutations, a completely t-scrambling set of permutations, in which the images of every set of t chosen events include each of the t! possible "patterns." In event sequence testing, minimizing the number of permutations used is the principal objective. By developing a connection with covering arrays, lower bounds on this minimum in terms of the minimum number of rows in covering arrays are obtained. An existing bound on the largest v for which the minimum can equal t! is improved. A conditional expectation algorithm is developed to generate sequence covering arrays whose number of permutations never exceeds a specified logarithmic function of v when t is fixed, and this method is shown to operate in polynomial time. A recursive product construction is established when t = 3 to construct sequence covering arrays on vw events from ones on v and w events. Finally computational results are given for t  {3, 4, 5} to demonstrate the utility of the conditional expectation algorithm and the product construction. Key words. sequence covering array, completely scrambling set of permutations, covering array, directed t-design AMS subject classifications. 05B40, 05B15, 05B30, 05A05 DOI. 10.1137/120894099

1. Introduction. A set of permutations {1 , . . . , N } of a v -element set X is completely t-scrambling if for every ordered t-set (x1 , . . . , xt ) with xi  X for 1  i  t, there is some  (1    N ) for which  (xi ) <  (xj ) if and only if i < j . Spencer [32] first explored the existence of completely t-scrambling sets of permutations in generalizing a question of Dushnik [15] on linear extensions. Recently Kuhn et al. [23, 24] examined an equivalent combinatorial object, the sequence covering array. For parameters N , t, and v , such an array is a set of n permutations of v letters so that every permutation of every t of the v letters appears--in the specified order--in at least one of the n permutations. The motivation for finding sequence covering arrays with small values of n arises in event sequence testing. Suppose that a process involves a sequence of v tasks or events. The operator may, unfortunately, fail to do the tasks in the correct sequence. When this happens, errors may occur. But we anticipate that errors can be attributed to the (improper) ordering of a small
 Received by the editors October 8, 2012; accepted for publication (in revised form) September 4, 2013; published electronically October 28, 2013. http://www.siam.org/journals/sidma/27-4/89409.html  School of Physical and Mathematical Sciences, Nanyang Technological University 637371, Singapore (YMChee@ntu.edu.sg).  School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85257, and State Key Laboratory of Software Development Environment, Beihang University, Beijing 100191, China (colbourn@asu.edu). This author was supported by Australian Research Council grant DP120103067. § School of Mathematical Sciences, Monash University, Melbourne, Australia (danhorsley@ gmail.com). This author was supported by Australian Research Council grants DP120103067 and DE120100040. ¶ Department of Mathematics, Beijing Jiaotong University, Beijing, China (jlzhou@bjtu.edu.cn).

1844

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1845

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

subset of tasks. When each permutation of a sequence covering array is used in turn to specify a task order, every potential ordering of t or fewer tasks will be tried and hence all errors found that result solely from the improper ordering of t or fewer tasks. Applications are discussed further in [19, 23, 39, 40]; related event sequence testing problems in which tasks can be repeated are discussed in [42, 43, 44]. While the application of these combinatorial structures is of much practical concern, our interest is in bounds on the size of sequence covering arrays and their explicit construction. We first state the problem formally. Let  = {0, . . . , v - 1} be symbols that represent the v tasks or events. A t-subsequence of  is a t-tuple (x1 , . . . , xt ) with xi   for 1  i  t, and xi = xj when i = j . A permutation  of  covers the t-subsequence (x1 , . . . , xt ) if  -1 (xi ) <  -1 (xj ) whenever i < j . For example, with v = 5 and t = 3, (4, 0, 3) is a 3-subsequence that is covered by the permutation 4 2 0 3 1. A sequence covering array of order v and strength t, or SeqCA(N ; t, v ), is a set  = {1 , . . . , N }, where i is a permutation of , and every t-subsequence of  is covered by at least one of the permutations {1 , . . . , N }. Often the permutations are written as an N × v array. We use an array representation for completely t-scrambling sets of permutations as well. An N × v array is a completely t-scrambling set of permutations of strength t on v symbols, or CSSP(N ; t, v ), when the columns are indexed by  and the symbols by , and for every way c1 , . . . , ct to choose t distinct columns and every permutation  of {1, . . . , t}, there is a row  for which, for every 1  a < b  t, the entry in cell (, c(a) ) is less than the entry in cell (, c(b) ). Lemma 1.1. A CSSP(N ; t, v ) is equivalent to a SeqCA(N ; t, v ). Proof. If 1 , . . . , N are the N permutations of a SeqCA(N ; t, v ), form an N × v -1 array A in which cell (i, j ) contains i (j ). Then A is a CSSP(N ; t, v ). In the opposite direction, if A is a CSSP(N ; t, v ), define permutation i by setting i (aij ) = j for 1  i  N and 0  j < v . Then 1 , . . . , N form the N permutations of a SeqCA(N ; t, v ). In the CSSP(8;3,5) of Table 1.1, the symbols {1, 2, 3} appear as 123 once, 132 once, 213 once, 231 zero times, 312 four times, and 321 once. Hence the rows of a completely t-scrambling set of permutations do not necessarily produce a sequence covering array; nevertheless they are conjugates, obtained by interchanging the roles of columns and symbols. Permutation problems concerning the avoidance of specified patterns of subsequences have been extensively studied in algebraic and probabilistic combinatorics; see [33] for an excellent survey. (Here two subsequences (x1 , . . . , xt ) and (y1 , . . . , yt ) have the same pattern when, for 1  i < t, xi < xi+1 if and only if yi < yi+1 .)
Table 1.1 Example: SeqCA(8;3,5) ­ t = 3, v = 5, N = 8. SeqCA 4203 1430 3120 0241 2134 0341 3021 4120 CSSP 2413 3042 3120 0314 4102 0341 1320 3124

1 2 4 3 0 2 4 3

0 1 4 2 3 2 4 0

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1846

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

While cosmetically similar to pattern avoidance problems, the existence problem for sequence covering arrays requires coverage rather than avoidance and requires that all subsequences be covered and not simply every pattern. The question of principal concern in this paper is as follows: Given t and v , what is the smallest N for which a CSSP(N ; t, v ) (equivalently, a SeqCA(N ; t, v )) exists? Call this number SeqCAN(t, v ). In the vernacular of completely t-scrambling sets of permutations, Spencer [32] did the foundational work, and F¨ uredi [17], Ishigami [20, 21], Radhakrishnan [31], and Tarui [36] made improvements. In a sequence covering array, every t symbols must appear in each of the t! possible orderings, and there are v t t! t-subsequences in total, so t!  SeqCAN(t, v )  v t! t

Both bounds are trivial, but the lower bound is the correct one when t = 2. Lemma 1.2. SeqCAN(2, v ) = 2 for all v  2. Proof. Any permutation on v symbols and its reversal form a SeqCA(2; 2, v ). When t  3, neither bound is correct as v increases. Indeed the growth as a function of v for fixed t is logarithmic. Theorem 1.3 (see [31, 32]). For t  3, 1+ 2 log2 (e) (t - 1)! v 2v - t + 1 log2 (v - t + 2)  SeqCAN(t, v )  t log(v ) log
t! t!-1

.

The question of when SeqCAN(t, v ) = t! is of independent interest in yet another setting. Let V be a finite set; an element of V is a vertex. A transitive tournament on V is a directed graph in which (1) for all x  V , (x, x) is not an arc; (2) for distinct x, y  V , (x, y ) is an arc if and only if (y, x) is not an arc; and (3) whenever (x, y ) and (y, z ) are arcs, so is (x, z ). A transitive tournament T = (V, A) has transitive tournament T  = (W, B ) as a subdigraph, denoted T   T , whenever W  V and B  A. Let (V, T ) be a finite set V of cardinality v and a collection T with every T  T being a transitive tournament on k of the vertices in V ; members of T are blocks. Then (V, T ) is a (t, )-directed packing of blocksize k and order v , or DP (t, k, v ), if for every X  V with |X | = t and every transitive tournament T  on vertex set X , |{T  T : T   T }|  . On the other hand, (V, T ) is a (t, )-directed covering of blocksize k and order v , or DC (t, k, v )), if for every X  V with |X | = t and every transitive tournament T  on vertex set X , |{T  T : T   T }|  . When (V, T ) is a DP (t, k, v ) and also a DC (t, k, v ), it is a (t, )-directed design of blocksize k and order v , or DD (t, k, v ). In this notation, the subscript is often omitted when  = 1. Directed designs with t = 2 have been extensively studied as generalizations of balanced incomplete block designs. The study of (t, 1)-directed packings has also been extensive as a result of their equivalence to "deletion-correcting codes" (see Levenshtein [25]). The connection with our investigation follows.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1847

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Lemma 1.4. A CSSP(N ; t, v ) is equivalent to a DC(t, v, v ) with N blocks. Moreover, a DD(t, v, v ) exists if and only if SeqCAN(t, v ) = t!. Proof. For each row (a0 , . . . , av-1 ) of the CSSP, form a transitive tournament on vertex set {0, . . . , v - 1} by including, for 0  i < j < v , arc (i, j ) if ai < aj and arc (j, i) otherwise. Each transitive tournament on t of these vertices is a subdigraph of at least one of these N tournaments. The other direction is similar. When N = t!, every t-subsequence is covered exactly once, and every transitive tournament of order t arises as a subdigraph exactly once. Theorem 1.5. Sufficient conditions for a DD(t, v, v ) to exist include 1. t  3 and t  v  t + 1 [25] and 2. t = 4 and v = 6 [28]. Necessary conditions for a DD(t, v, v ) to exist include 1. v  t + 1 for t  {3, 5, 6} [28], 2. v  t + 2 for t = 4 [28], and - 1 for t  7 [28]. 3. v  t+1 2 Levenshtein [25] had conjectured that v  t + 1 whenever a DD(t, v, v ) exists for t  3. As stated in Theorem 1.5, this does not hold for t = 4, but this is the only known exception to Levenshtein's conjecture. In the next section we significantly reduce the upper bound on the largest v for which SeqCAN(t, v ) can equal t!. 2. Lower bounds. Here we extend a technique used in [17, Theorem 5.1], improving on a method of Ishigami [21]. We require a number of previous results on covering arrays, introduced next. See [8] for a more thorough introduction to them. Let N , k , t, and v be positive integers. Let C be an N × k array with entries from an alphabet  of size v ; we typically take  = {0, . . . , v - 1}. When (1 , . . . , t ) is a t-tuple with i   for 1  i  t, (c1 , . . . , ct ) is a tuple of t column indices (ci  {1, . . . , k }), and ci = cj whenever i = j , the t-tuple {(ci , i ) : 1  i  t} is a t-way interaction. The array covers the t-way interaction {(ci , i ) : 1  i  t} if, in at least one row  of C, the entry in row  and column ci is i for 1  i  t. Array C is a covering array CA(N ; t, k, v ) of strength t if it covers every t-way interaction. CAN(t, k, v ) is the minimum N for which a CA(N ; t, k, v ) exists. The basic goal is to minimize the number of rows (tests) required and hence to determine CAN(t, k, v ). When t  2 and v  2 are both fixed, CAN(t, k, v ) is (log k ) (see, for example, [8]). We strengthen this standard definition somewhat. For a t-way interaction T = {(ci , i ) : 1  i  t} with symbols chosen from  = {0, . . . , v - 1}, let  (T ) = -1 |{i : i =  }|. Then define µ(T ) = v =0  (T )!. The natural interpretation is that µ(T ) is the number of ways to permute the columns (c1 , . . . , ct ) so that the symbols appear in the same order. A covering array provides excess coverage when every tway interaction T is covered by at least µ(T ) rows; such a covering array is denoted by CAX (N ; t, k, v ). More generally, the subscript X is used to extend notation from covering arrays to those having excess coverage. Theorem 2.1. Let v , t, and a be integers satisfying v  t  3 and t  a  0. Then SeqCAN(t, v )  a!CANX (t - a, v - a, a + 1). Proof. Let S be a CSSP(N ; t, v ). Choose any a columns of S, {e1 , . . . , ea }. For each ordering  of these columns, form a matrix C that contains all rows of S in which the entry in column  (ei ) is less than that in column  (ei+1 ) for 1  i < a. Because there are a! orderings and every row of S appears in exactly one of the {C }, it suffices to show that for every choice of  the number n of rows in C is at least CANX (t - a, v - a, a + 1). To do this, form an n × (v - a) array A whose columns are the columns of C that are not among the a selected. To determine the content of

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1848

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

cell (r, c) of A , examine the symbol  in column c and row r of C . If  is less than the symbol in row r and column  (e1 ), the entry is set to 0. If  is greater than the symbol in row r and column  (ea ), the entry is set to a. Otherwise find the unique j for which  is greater than the symbol in row r and column  (ej ) but less than the symbol in row r and column  (ej +1 ), and set the entry to j . Now we claim that A is a CAX (n; t - a, v - a, a + 1). The verification requires demonstrating that every (t - a)-way interaction T is covered at least µ(T ) times. So let T = {(fi , i ) : 1  i  t - a}, noting that i  {0, . . . , a} and fi indexes a column of A. We form permutations of {e1 , . . . , ea }  {f1 , . . . , ft-a } that are consistent with  on {e1 , . . . , ea } in that these columns appear in the order prescribed by  . To do this, there are 0 (T ) columns with entry 0; place one of the 0 (T )! orderings of these columns so that all appear before  (e1 ). There are a (T ) columns with entry a; place one of the a (T )! orderings of these columns so that all appear after  (ea ). For 1  j < a, there are j (T ) columns with entry j ; place one of the j (T )! orderings of these columns so that all appear before  (ej ) and after  (ej +1 ). In this way we can form µ(T ) permutations of {e1 , . . . , ea }  {f1 , . . . , ft-a }, each consistent with  . Because each is consistent with  , it appears in C . But each such appearance in C results in a different row of A that covers T , and hence T is indeed covered at least µ(T ) times. The easiest applications of Theorem 2.1 result from using CAN(t, k, v ) as a lower bound for CANX (t, k, v ). Apply it with a = t - 1, noting that CAN(1, k, t) = t for all k  1, to recover the trivial lower bound that SeqCAN(t, v )  t!. Apply it with a = t - 2 to establish that SeqCAN(t, v )  (t - 2)!CAN(2, v - t + 2, t - 1). Now we return to the question of when SeqCAN(t, v ) can equal t!, or equivalently when a DD(t, v, v ) can exist. Theorem 2.1 ensures that SeqCAN(t, v )  (t - 2)!CANX (2, v - t + 2, t - 1), so SeqCAN(t, v ) = t! can hold only when CANX (2, v - t + 2, t - 1)  t(t - 1). The 2-way interaction T = {(c1 , 1 ), (c2 , 2 )} has µ(T ) = 2 exactly when 1 = 2 (called a constant pair) and has µ(T ) = 1 otherwise (a nonconstant pair). Because, for each pair of columns, t - 1 constant pairs must be covered twice each, and (t - 1)(t - 2) nonconstant pairs must be covered at least once each, CANX (2, v - t + 2, t - 1)  t(t - 1). So we are concerned with when equality can hold. Lemma 2.2. For v  4, CANX (2, k, v ) = v (v + 1) only if k  v + 2. Proof. Suppose that a CAX (v (v + 1); 2, k, v ) exists with columns indexed by {1, . . . , k } and symbols by {0, . . . , v - 1}. We form sets on symbols V = ({1, . . . , k } × {0, . . . , v - 1})  {}. The system of sets (blocks) B is formed as follows. For every row (x1 , . . . , xk ) of the covering array, a set {(i, xi ) : 1  i  k } is placed in B . Then for every 1  i  k , a set {(i, j ) : 0  j < v }  {} is placed in B . The set system (V, B ) has kv + 1 symbols and v (v + 1) + k blocks. By construction, every two different symbols appear together in exactly one block, unless the pair is of the form {(i, j ), (i , j )} corresponding to a constant pair and therefore occurring in exactly two blocks. Now form a (kv + 1) × (v (v + 1) + k ) matrix A, which is the symbol-block incidence matrix, as follows. Rows are indexed by symbols, columns by blocks. The matrix contains the entry 1 in row r and column c when symbol r appears in block c and 0 otherwise. Now examine B = AAT , which has rows and columns indexed by V . Its diagonal entries are k in entry (, ) and v + 2 elsewhere. Its off-diagonal entries are 2 in cells indexed by ((i, j ), (i , j )) with i = i and 1 otherwise. The rank of B cannot exceed the number of columns in A, namely, v (v + 1) + k . So in order to bound k , we bound the rank of B.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1849

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Write D = B - J. The rows indexed by V \ {} can be partitioned into v parts; a part is formed by including all rows and columns with indices {(i, j ) : 1  i  k } for some j with 0  j < v . Then D can be written as a block diagonal matrix with v k × k block matrices each equal to X = v I + J and a single 1 × 1 block matrix with entry k - 1. Now det(D) = (k - 1)(det(X))v , and det(X) = v k (1 + k v ) by the matrix determinant lemma. Hence rank(D) = kv + 1. Because B is obtained from D by a rank one update, rank(B)  kv . v +1) Consequently, kv  v (v + 1) + k , or k  v( v -1 . Thus k  v + 2 when v  4, because k is an integer. This enables us to establish a substantial improvement on the bound of Mathon and Tran Van Trung [28] stated in Theorem 1.5. Theorem 2.3. If t  3 and SeqCAN(t, v ) = t! (or equivalently, a DD(t, v, v ) exists), then v  2t - 1. Proof. If 3  t  6, this follows from Theorem 1.5. By Theorem 2.1, t! = SeqCAN(t, v )  (t - 2)!CANX (2, v - t + 2, t - 1) and thus CANX (2, v - t + 2, t - 1) = t(t - 1). By Lemma 2.2, v - t - 2  t - 1 + 2 and hence v  2t - 1 as required. It appears plausible that the bound should be t +2 rather than 2t - 1; nevertheless, the method here gives the first bound that is linear in t. 3. Upper bounds from probabilistic methods. Spencer [32] analyzed a method that selects a set of N permutations on v symbols uniformly at random; we explore this first. v! t! Lemma 3.1. For fixed t  3, SeqCAN(t, v )  1 + (log( (v- t)! ))/(log( t!-1 )). Proof. A permutation of {0, . . . , v - 1} chosen uniformly at random covers any 1 specific t-subsequence with probability t ! and so fails to cover it with probability t!-1 . Then N permutations of { 0 , . . . , v - 1} chosen uniformly at random and int!
-1 dependently together fail to cover a specific t-subsequence with probability t!t . ! v! There are (v-t)! t-subsequences. When N permutations are chosen, each subsequence N t!-1 N . t!

is not covered with probability
v! (v -t)!

Thus the expected number of uncovered 1, a SeqCA(N ; t, v ) must

t-subsequences is

t!-1 N . t!

exist. This holds whenever N >

t!-1 N < t! v! t! (log( (v-t)! ))/(log( t!-1 )).

When

v! (v -t)!

3.1. One permutation at a time. Lemma 3.1 provides a useful upper bound on the size of completely t-scrambling sets of permutations but does not provide an effective method to find such arrays. Stein [34], Lov´ asz [26], and Johnson [22] develop a general strategy for finding solutions to covering problems; this algorithm has been shown to lead to polynomial time methods in many combinatorial covering problems [3, 4, 7, 9, 10]. We extend that strategy here to treat sequence covering arrays. The basic approach is greedy. Repeatedly select one permutation to add that covers a large number of as-yet-uncovered t-subsequences, until all are covered. Stein [34], Lov´ asz [26], and Johnson [22] each suggest selecting to maximize the number of newly covered elements, but their analyses require only that the next selection cover at least the average. If after i permutations are selected there remain Ui uncovered t-subsequences, then a permutation selected uniformly at random is expected to cover 1 Ui t ! t-subsequences for the first time. Provided that we select the (i +1)st permutation 1 t!-1 to cover at least Ui t ! t-subsequences for the first time, we have that Ui+1  Ui t! . v! v! t!-1 i Because U0 = (v-t)! , we have that Ui  (v-t)! ( t! ) . Choose N to be the smallest

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1850

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

value for which UN < 1; then there must be a sequence covering array with N permutations. This simply restates the argument of Lemma 3.1, but with two important improvements. It derandomizes the method by ensuring that appropriate selection of each permutation guarantees that the bound is met, rather than asserting the existence of a set of permutations that meets it. More importantly, the time to construct the sequence covering array is polynomial in the number of permutations and the time to select a permutation that covers at least the average. For fixed t the number of permutations is logarithmic in v , and so an algorithm with running time polynomial in v will result if we can select the next permutation in time polynomial in v . Because the details are quite similar to earlier approaches, we merely outline how this can be done. Suppose that U consists of the as-yet-uncovered t-subsequences. For U  U , let cov( , U ) = 1 when  covers U , and cov( , U ) = 0 otherwise. Let R be an rsubsequence; the r symbols in R are fixed, and the remaining v - r are free. There ! is a set PR of v r ! permutations that cover R. We focus on the expected number of members of U that are covered by a member of PR chosen uniformly at random; this is ec(R) = r! v! cov( , U ).
 PR U U

The strategy is to find a sequence of subsequences P0 , . . . , Pv , so that Pi is an i-subsequence, Pi+1 covers Pi for 0  i < v , symbol i is free in Pi but fixed in Pi+1 , 1 and ec(Pi+1 )  ec(Pi ) for 0  i < v . Because ec(P0 ) = t ! |U|, it follows that Pv is a 1 permutation that covers at least t! |U| of the as-yet-uncovered t-subsequences. Given a selection of Pi , there are precisely i + 1 candidates {C1 , . . . , Ci+1 } for Pi+1 obtained by placing symbol i in one of the i + 1 positions of Pi . Our task is to choose one for which ec(Cj )  ec(Pi ), in order to set Pi+1 = Cj . A naive computation of ec(Cj ) would enumerate members of U and of PCj , but the latter may have size exponential in v . Instead, for U  U let ec(U, R) = r!  PR cov( , U ), and observe that v! ec(R) =
U U

ec(U, R).

When t is fixed, U contains fewer than v t subsequences, which is polynomial in v . Therefore it suffices to compute ec(U, R) efficiently, given a t-subsequence U and an r-subsequence R. Let  be the number of symbols appearing in both U and R. When the  symbols in common do not appear in the same order in U and R, ec(U, R) = 0. Otherwise let T be the  -subsequence that they have in common. Then ! ec(U, R) = ec(U, T ) =  t! . i+1 1 The key observation in selecting Pi+1 is that ec(Pi ) = i+1 j =1 ec(Cj ). Computing ec(Cj ) for 1  j  i + 1, and selecting Pi+1 to be the one that maximizes ec(Cj ), we are then sure that ec(Pi+1 )  ec(Pi ). Combining all of these arguments, we have established the next theorem. Theorem 3.2. For fixed t and input v , there is an algorithm to construct a v! t! SeqCA(N ; t, v ) having N  1 + (log( (v- t)! ))/(log( t!-1 )) permutations in time that is polynomial in v . This algorithm can be easily implemented, and we report results from it in section 5. One immediate improvement results from observing that the counts of

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1851

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

as-yet-uncovered t-subsequences (R0 , R1 , . . . , RN ) must be integers. Hence we have -1 that Ri+1  Ri t!t ! . In specific cases this improves on the bound, without the need to construct the sequence covering array. If, however, the sequence covering array is explicitly constructed, at each selection of Pi+1 from Pi , we can choose Pi+1 to maximize ec() among the i + 1 candidates. 3.2. Greedy methods with reversals. The methods developed are greedy in that they attempt to cover the largest number of as-yet-uncovered t-subsequences. The very first permutation chosen is arbitrary; all are equally effective at coverage. Once one is selected, however, there is a genuine choice for the second. Greedy selection indicates that we should choose one that covers no t-subsequence already covered by the first. Indeed when t = 2, choosing the reversal of this first covers all remaining t-subsequences. For t  3, suppose that we have chosen 2s permutations 1 , . . . , 2s , and suppose further that 2i is the reverse of 2i-1 for 1  i  s. It follows that the number of as-yet-uncovered t-subsequences covered by a permutation  is precisely the same as the number covered by the reverse of  . Yet  and its reverse never cover the same t-subsequence. Hence if the algorithm were to select  next, the reverse of  remains an equally beneficial choice immediately thereafter. Therefore a useful variant of the algorithm developed, after adding a permutation  to the array, always adds the reverse of  as well. Pursuing this, we obtain the following. Theorem 3.3. For fixed t and input v , there is an algorithm to construct a v! t! SeqCA(N ; t, v ) having N  2(log( (v- t)! ))/(log( t!-2 )) permutations in time that is polynomial in v . Naturally, we could again obtain small improvements in practice because every count of as-yet-uncovered t-subsequences is an integer. In principle, always including reversals improves slightly on the bound (that is, Theorem 3.3 improves on Theorem 3.2). Whether this is a practical improvement remains to be seen; we return to this point. 4. Product constructions. Product (or "cut-and-paste" or "Roux-type") constructions are well studied for covering arrays; see, for example, [11, 6]. We develop a product construction for completely 3-scrambling sets of permutations. To do this, we first introduce an auxiliary property. A signing of a CSSP(N ; t, v ) A = (aij ) is an N × v matrix S = (sij ) with entries {, }. A CSSP(N ; t, v ) A is properly signed by an N × v matrix S with entries {, }. When for every set of t - 1 distinct columns c1 , . . . , ct-1 , each sign s  {, }, and every permutation  of 1, . . . , t - 1, there exists a row  of A for which, for every 1  a < b < t, the entry in cell (, c(a) ) is less than the entry in row (, c(b) ), and the sign s,c1 = s. A properly signed CSSP(7;3,5) is shown in Table 4.1. We defer for the moment the question of how to sign a completely t-scrambling set of permutations. 4.1. Products for strength three. Theorem 4.1. If a properly signed CSSP(N ; 3, v ) and a properly signed CSSP(M ; 3, w) both exist, so does a properly signed CSSP(N + M ; 3, vw). Proof. Let A = (aij ) be a CSSP(N ; 3, v ) having sign matrix S = (sij ) with columns indexed by {0, . . . , v - 1}. Let B = (bij ) be a CSSP(M ; 3, w) having sign matrix T = (tij ) with columns indexed by {0, . . . , w - 1}. Form an array C = (c,(i,j ) ) on N + M rows and vw columns with columns indexed by {0, . . . , v - 1} × {0, . . . , w - 1}. In row  for 1    N , in column (i, j ), place the entry ai w + j if si =, ai w + (w - 1 - j )

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1852

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Table 4.1 Properly signed CSSP(7; 3, 5) ­ t = 3, v = 5, N = 7. All signs not specified can be selected arbitrarily.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

0 1 2 2 2 4 4

4 2 0 1 3 1 3

2 3 4 0 0 2 2 (7;3,5)

3 0 3 3 4 3 0

1 4 1 4 1 0 1

if si =. In row N +  for 1    M , in column (i, j ), place the entry bj v + i if tj =, bj v + (v - 1 - i) if tj =. To show that C is a CSSP(N + M ; 3, vw), we must establish that every 3subsequence is covered. Consider three columns (i1 , j1 ), (i2 , j2 ), (i3 , j3 ), in this order. If i1 , i2 , and i3 are all distinct, there is a row  of A in which ai1 < ai2 < ai3 . Then in C row  has the three specified columns in the chosen order. By the same token, if j1 , j2 , and j3 are all distinct, there is a row  of B in which bj1 < bj2 < bj3 . Then in C row  + N has the three specified columns in the chosen order. If {i1 , i2 , i3 } contains only one element, {j1 , j2 , j3 } contains three distinct elements; symmetrically, if {j1 , j2 , j3 } contains only one element, {i1 , i2 , i3 } contains three distinct elements. So it remains only to treat cases in which both {i1 , i2 , i3 } and {j1 , j2 , j3 } contain two distinct elements. Now suppose that the three columns are {(i1 , j1 ), (i1 , j2 ), (i2 , j1 )}; we are concerned with the six orderings of the elements {c(i1 ,j1 ) , c(i1 ,j2 ) , c(i2 ,j1 ) }, which we represent by giving the indices in the sorted order for {c(i1 ,j1 ) , c(i1 ,j2 ) , c(i2 ,j1 ) }, as shown next. j2 j1 i1 c(i1 ,j1 ) c(i1 ,j2 ) 1 2 1 3 2 1 2 3 3 1 3 2 i2 c(i2 ,j1 ) 3 2 3 1 2 1 Table 4.2 gives the rows in which each of the six orderings is covered; we use sgn(x - y ) to be  if x < y ,  if x > y . Two of the orderings are covered at least twice. To sign C properly, assign  to each entry in each of the first N rows and  to each entry in each of the last M rows. A small example, combining two CSSP(6;3,4)s to form a CSSP(12;3,16), is shown in Table 4.3. Use the strategy in the proof of Theorem 4.1, taking B and T from   0 1  0 1     1  0  , 1 0

to establish the next theorem. Theorem 4.2. If a properly signed CSSP(N ; 3, v ) exists, so does a properly signed CSSP(N + 4; 3, 2v ).

4.2. Signing a completely t -scrambling set of permutations. We first give one technique for signing that applies for all strengths.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS Table 4.2 Verification for six orderings.

1853

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Row in C     +N +N +N +N

Condition on  ai1 < ai2 ai1 < ai2 ai1 > ai2 ai1 > ai2 bj1 < bj2 bj1 < bj2 bj1 > bj2 bj1 > bj2

Sign condition si1 = sgn(j2 - j1 ) si1 = sgn(j1 - j2 ) si1 = sgn(j2 - j1 ) si1 = sgn(j1 - j2 ) tj1 = sgn(i2 - i1 ) tj1 = sgn(i1 - i2 ) tj1 = sgn(i2 - i1 ) tj1 = sgn(i1 - i2 )

Ordering 1 3 2 3 2 1 3 1 1 2 2 1 2 3 3 2 2 1 3 2 3 3 1 1

Table 4.3 Example: Properly signed CSSP(6; 3, 4) and its product with itself, a CSSP(12; 3, 16). 3 4 4 12 4 15 3 4 4 12 4 15 2 5 5 13 5 14 4 3 15 4 12 4 1 6 6 14 6 13 8 15 3 0 8 8 0 7 7 15 7 12 15 8 8 8 0 3 4 3 15 4 12 4 2 5 5 13 5 14 5 2 14 5 13 5 5 2 14 5 13 5 6 1 13 6 14 6 9 14 2 1 9 9 7 0 12 7 15 7 14 9 9 9 1 2 8 15 3 0 8 8 1 6 6 14 6 13 9 14 2 1 9 9 6 1 13 6 14 6 10 13 1 2 10 10 10 13 1 2 10 10 11 12 0 3 11 11 13 10 10 10 2 1 15 8 8 8 0 3 0 7 7 15 7 12 14 9 9 9 1 2 7 0 12 7 15 7 13 10 10 10 2 1 11 12 0 3 11 11 12 11 11 11 3 0 12 11 11 11 3 0

0 1 1 3 1 3

1 0 3 1 3 1

2 3 0 0 2 2

3 2 2 2 0 0

Lemma 4.3. Whenever a CSSP(N ; t, v ) exists, a properly signed CSSP(N ; t, v - 1) exists. Proof. Let A = (aij ) be a CSSP(N ; t, v ). Form an N × (v - 1) array S = (sij ) with sij = sgn(aij - ai,v-1 ). Form an N × (v - 1) array B = (bij ) with bij = aij if aij < ai,v-1 and bij = aij - 1 otherwise. Then B is a CSSP(N ; t, v - 1) that is properly signed by S. Let A be a CSSP(N ; t, v ) and A1 , A2 be arrays that partition the rows of A. When, for i = 1, 2, Ai is a CSSP(Ni ; t - 1, v ), A is a partitionable CSSP. Lemma 4.4. Whenever a partitionable CSSP(N ; t, v ) exists, a properly signed CSSP(N ; t, v ) exists. Proof. Let A be a CSSP(N ; t, v ) with partition A1 , A2 . Assign sign  to every entry of A1 and  to every entry of A2 . Corollary 4.5. Whenever a CSSP(N ; 3, v ) contains a row and its reverse, it is partitionable and hence can be properly signed. Proof. Place the row and its reverse in A1 and all other rows in A2 . Then A1 is a CSSP(N ; 2, v ). Moreover, A2 is a CSSP(N ; 2, v ) because for every i, j  {0, . . . , v - 1} with i = j , in A there are at least three rows in which the entry in column i is less than that in column j . Then A is partitionable, so apply Lemma 4.4.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1854

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Lemma 4.4 provides a sufficient condition for a CSSP(N ; 3, v ) to have a proper signing but considers only signings in which all entries in each row receive the same sign. Column c is properly signed when, for every set of t - 1 distinct columns c1 , c2 , . . . , ct-1 with c1 = c, each sign s  {+, -}, and every permutation  of 1, . . . , t - 1, there exists a row  of A for which, for every 1  a < b < t, the entry in cell (, c(a) ) is less than the entry in row (, c(b) ), and the sign s,c = s. Properly signing the N × v array A is equivalent to properly signing each column of A; the important fact is that signs assigned in one column are unrelated to signs in any other, and so one can (hope to) proceed by signing each column separately. Consider the case of strength t = 3. What does it mean to properly sign a specific column c? For every column i other than c we form two sets: Ai contains the row indices in which the entry in column i is larger than that in column c, and Bi (= {1, . . . , N }\ Ai ) contains the row indices in which the entry in column i is smaller than that in column c. We can consider these sets as the edges of a hypergraph H on vertex set {1, . . . , N }. Then H has 2v - 2 edges each containing at least three vertices and a proper 2-coloring of H corresponds to a proper signing of c. Lemma 4.4 and Corollary 4.5 give proper 2-colorings. In all examples that we have examined, each column can be properly signed by finding a suitable 2-coloring. Hence it is plausible that every CSSP(N ; 3, v ) can be properly signed, but if this is true the proof is elusive at the moment. 5. Computational results. In [23], a simple greedy method is used to compute upper bounds on SeqCAN(t, v ) for t  {3, 4} and small values of v . These are reported in column K in Tables 5.1 and 5.2. Results from a more sophisticated greedy method by Erdem et al. [16] are reported in column ER. Using techniques from constraint satisfaction, in particular answer set programming, much more sophisticated search methods have been applied to strengths three and four [1, 2, 16]. Banbara, Tamura, and Inoue [1] implement an answer set programming method and show bounds for SeqCAN(3, v ) for v  80 and for SeqCAN(4, v ) for v  23. These bounds appear in column BTI in Tables 5.1 and 5.2. Results by Brain et al. [2] are reported in column BR in Tables 5.1 and 5.2. In [18], bounds for t  {3, 4, 5} and v  10 are reported from a method called the "bees algorithm"; these offer modest improvements on the greedy method in [23]. We do not report them for t  {3, 4} because they are not competitive with the results in [1]; we do report them for t = 5 in column BA, because they are the only published computational results. When t = 3, Tarui [36] establishes q/2 )  q for all q  4; these are reported in by direct construction that SeqCAN(3,  q/4 column TA. The bound U is obtained by computing the number Ui of as-yet-uncovered t-1 subsequences using Ui+1 =  t!t ! Ui  and terminating with the first value N for which UN = 0. (This does not explicitly construct the array but rather yields a number of rows for which it can surely be produced.) In the same manner, bound UR is obtained 1 by including reversals, so that Ui+2 = Ui - 2 t ! Ui  when i is even. The bound D is obtained by applying the algorithm that establishes Theorem 3.2 and that of DR by applying the algorithm that establishes Theorem 3.3. Table 5.1 reports results for t = 3. The theoretical results indicate that including reversals accelerates coverage, and so the bound UR improves on the bound U. Neither is competitive with greedy bounds from [23], given by K. In turn these are improved upon by the greedy method from [16], given by ER. Implementing our greedy approach nevertheless results in useful improvement to the two earlier greedy bounds, whether reversals are included or not. It comes as no surprise that the answer set programming

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS Table 5.1 Upper bounds on SeqCAN(3, v).

1855

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Events v 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 40 50 60 70 80 90

TA 8 8 8 10 10 10 10 12 12 12 12 12 12 12 12 12 12 14 14 14 14 14 14 14 14 14 14 16 16 16 16 18 18

U 12 17 20 23 26 28 30 32 33 35 36 37 39 40 41 42 42 43 44 45 46 46 47 48 48 49 49 54 58 61 64 66 68

UR 12 16 18 22 24 26 28 30 30 32 34 34 36 36 38 38 38 40 40 40 42 42 42 44 44 44 46 50 52 56 58 60 62

t=3 K ER 8 10 12 12 14 14 11 14 16 16 16 18 18 20 20 22 22 19 22 22 24 24 24 24 26 26 26 26 23 32 27 34 31 38 34 40 36 42 38 44

DR 8 10 10 12 12 12 14 14 14 16 16 16 16 18 18 18 18 18 20 20 20 20 20 20 20 22 22 24 26 26 28 30 30

D 6 8 8 9 10 11 12 12 13 13 14 14 15 15 16 16 16 17 17 17 17 18 18 18 18 19 19 21 23 24 25 26 27

BTI 7 8 8 8 9 9 10 10 10 10 10 11 11 12 12 12 12 13 14 14 14 14 14 14 15 15 17 19 21 22 24

BR 7 8 8 8 9 9 10 10 10 10 10 10 11 12 12 12 12 12 13 13 14 14 14 14 14 15 17 18 20 22 23

methods from [1, 2] (BTI, BR) yield consistent improvements on all the greedy methods for strength three. However, the direct construction of Tarui [36] provides better results at this time whenever v  30. Perhaps the most perplexing pattern is the regularity with which D yields a better bound than does DR . Remarkably, we consistently produce smaller sequence covering arrays when we do not automatically include reversals! The reasons for this are quite unclear at the present time. Table 5.2 gives results for strengths four and five. For strength four, our improvements on the method from [23] are more dramatic than for strength three. Surprisingly, the answer set programming technique from [1] obtains a better result than our greedy methods only when v  8. For 9  v  23, our greedy method yields much smaller arrays. (For v = 23, we employ 98 permutations as opposed to the 112 in BTI.) A similar comparison applies with the results from [2, 16] reported in column BR. Of course, we expect that given enough time, the answer set programming techniques would improve upon our greedy bounds. However, our methods require polynomial time in theory and are effective in practice for larger problems than those considered in [2, 1]; despite these "limitations," our methods appear to yield better results within the time available.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1856

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU Table 5.2 Upper bounds on SeqCAN(t, v) for t  {4, 5}.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Events v 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 40 50 60 70 80 90

U 24 54 79 98 114 128 140 151 160 169 177 184 191 197 203 209 214 219 224 228 232 236 240 244 248 251 255 283 305 322 337 350 361

UR 24 54 78 96 112 126 138 148 158 166 174 180 188 194 200 204 210 214 220 224 228 232 236 240 242 246 250 278 298 316 330 342 354

K 24 29 38 50 56 68 72 78 86 92 100 108 112 118 122 128 134 134 140 146 146 152 158 160 162 166 166 198 214 238 250 264 -

t=4 DR 24 24 32 40 44 50 56 60 64 70 74 78 80 84 86 90 92 96 98 98 102 104 106 108 110 112 114 132 146 154 166 174 180

D 24 26 34 41 47 52 57 61 66 71 73 78 81 84 86 91 92 95 97 99 101 104 105 107 110 111 113 128 141 151 160 168 176

BTI 24 24 38 44 52 58 65 69 77 81 84 89 91 97 100 105 104 111 112

BR

U 120 294 437 552 648 731 803 868 926 978 1027 1072 1113 1152 1189 1223 1256 1286 1316 1344 1370 1396 1420 1444 1466 1488 1671 1811 1924 2019 2101 2173

UR 120 294 436 550 646 728 800 864 922 976 1024 1068 1110 1148 1184 1218 1252 1282 1310 1338 1366 1390 1416 1438 1460 1482 1664 1804 1916 2012 2092 2164

t=5 DR 120 148 198 242 282 318 354 384 416 446 470 496 518 540 560 582 600 622 636 654 674 688 706 718 734 748

D 120 149 200 243 284 322 356 386 419 448 475 501 521 547 570 590 610 629 646 665 682 698 715 732 746 760

BA

55

159 212 271 329 383

104

149 181

A somewhat different pattern with respect to reversals is evident for strength four: The theoretical bound profits by including reversals throughout, but the implemented construction method appears first to benefit from reversals (for v  20) but later no longer benefit (for 40  v  90). Again the reasons for this are unclear. When v = 90, our methods track the coverage of 61,324,560 4-subsequences; thus, while the methods scale polynomially with v , the computations are nonetheless quite extensive. There is a CSSP(24;4,6) [28], but our methods do not yield fewer than 32 permutations. For strength five, none of the published methods in [1, 16, 23] report computational results, so it is difficult to make any comments about relative accuracy. However, the answer set programming methods do appear to require substantially more storage, which limits to a degree their effective range. To apply our methods would require tracking the coverage of 78,960,960 5-subsequences for v = 40; despite the efficiency of our methods, a straightforward implementation encounters both storage and time limitations. The method BA [18] is not competitive with our greedy methods. Within the range computed, including reversals improves our results. The pattern thereafter is unknown. Again, there is a CSSP(120;5,6) [25], but our methods do not yield fewer than 148 permutations.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1857

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

6. Using the product construction. For strength three, Theorem 4.1 provides substantial improvements on the computational results from the greedy methods. We properly signed a CSSP(6;3,4) in Table 4.3 and a CSSP(7;3,5) in Table 4.1. Table 6.1 shows proper signings for further arrays from [1]. We obtain CSSP(N ; 3, v ) for (v, N )  {(40, 15), (80, 17), (128, 18), (160, 19), (256, 20), (288, 21)} by using these in Theorem 4.1. These improve upon all the computational results! For example, while in [1] it is shown that SeqCAN(3, 80)  24 and in [2] that SeqCAN(3, 80)  23, here it is shown that SeqCAN(3, 80)  17. The examples given also provide better bounds than those of Tarui [36], but Theorem 4.1 does not outperform the direct construction asymptotically. 7. Constraints. In the testing application, it may happen that not every permutation of the events can in fact be executed; see [2, 23, 24]. It is therefore reasonable to ask how constraints on the execution order affect the number of permutations needed and how they affect the difficulty of finding a sequence covering array. We briefly consider the latter, in order to examine connections with further problems. Let  = {0, . . . , v - 1}. Let C be a set of subpermutations of , called constraints. A constrained sequence covering array SeqCA(N ; t, v, C ) is a set  = {1 , . . . , N } where i is a permutation of  that does not cover any subpermutation in C , and every t-subsequence of  that does not cover any subpermutation in C is covered by at least one of the permutations {1 , . . . , N }. Even in the easiest case, when t = 2 and all constraints are 2-subpermutations, the nature of the problem changes dramatically. Imposing the subpermutation constraint that b cannot precede a is the same as enforcing the precedence constraint that a precede b. When the precedence constraints contain a cycle, it is impossible to meet all constraints. This can be easily checked. When the constraints are acyclic, there is a permutation that covers no constraint. However, covering all 2-subpermutations not in C requires more. Let C r be the set of 2-subpermutations obtained by reversing each 2-subpermutation in C . Suppose that a SeqCA(N ; 2, v, C ) exists. Every permutation in the sequence covering array covers every 2-subpermutation in C r . Equivalently, treating C r as a partial order, every permutation gives a linear extension of the partial order. When (a, b)  C , (b, a) must be covered by every permutation in the sequence covering array. When {(a, b), (b, a)}  C = , some but not all permutations in the sequence covering array cover (a, b)--and the rest cover (b, a). Hence the set of 2-subpermutations covered by every permutation in the sequence covering array is exactly C r . This establishes a connection with the theory of partial orders. The dimension of a partial order is the smallest number of linear extensions whose intersection is the partial order [37, 38]. Our discussion establishes that a SeqCA(N ; 2, v, C ) exists if and only if the dimension of the partial order induced by C r is at most N . Hence we have the next lemma. Lemma 7.1. Deciding whether a SeqCA(N ; 2, v, C ) exists is NP-complete, even when C is an acyclic set of 2-subpermutations. Proof. Yannakakis [41] shows that determining whether a partial order has dimension at most 3 is NP-complete. Brain et al. [2] establish the NP-completeness of a related problem in which the subsequences to be covered, the constraints, and the permutations allowed are all specified. Lemma 7.1 is in stark contrast with the existence of sequence covering arrays of strength two without constraints. Nevertheless, the complexity arises in determining whether a small sequence covering array exists in these cases, not in

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1858

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU Table 6.1 Small properly signed CSSP(N ; 3, v)s. Signs not shown can be chosen arbitrarily.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

8 5 6 2 3 7 1 4 1 2 8 4 5 7 6 3 6 5 7 8 3 2 4 1 5 7 4 3 8 1 6 2 3 7 4 2 1 5 8 6 6 2 1 3 8 7 4 5 2 7 3 6 4 5 1 8 5 1 3 8 4 2 6 7 (8;3,8)

10 4 5 6 8 1 3 2 9 7 2 10 6 5 4 8 7 1 9 3 8 6 1 5 10 3 9 7 4 2 5 3 9 10 1 8 4 6 7 2 2 5 7 4 8 9 3 10 1 6 6 8 1 10 7 5 2 4 3 9 9 10 8 3 5 2 4 7 1 6 5 1 7 3 2 4 9 8 6 10 3 5 1 2 9 7 10 4 6 8 (9;3,10)

2 14 15 5 8 3 1 11 9 7 13 12 16 6 4 10 14 8 3 6 7 11 1 16 12 13 2 10 15 5 9 4 12 16 4 6 3 5 7 2 1 9 15 11 8 14 10 13 12 2 13 4 3 14 9 6 11 1 5 15 10 7 16 8 16 4 10 5 12 9 11 14 1 8 13 2 7 15 6 3 6 4 12 15 3 1 11 13 8 14 10 5 2 9 16 7 2 15 12 11 6 8 13 5 14 7 3 4 9 16 10 1 5 9 7 14 16 13 6 10 12 1 11 2 8 3 4 15 5 6 1 15 12 16 10 2 3 13 4 11 9 8 7 14 11 7 8 4 15 5 16 6 14 12 9 13 1 2 3 10 (10;3,16) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 4 14 11 10 6 9 13 7 17 1 18 3 2 12 16 5 8 15 4 17 3 2 18 16 14 7 9 13 11 12 10 8 1 6 15 5 6 13 7 8 12 10 14 4 16 1 2 18 17 15 9 11 3 5 7 18 6 16 4 5 1 17 13 11 10 12 14 15 8 2 9 3 8 3 15 4 9 17 2 16 11 12 14 13 10 7 6 1 5 18 8 11 3 14 6 15 13 5 2 16 17 1 18 7 10 9 4 12 10 7 16 4 5 9 13 15 1 18 3 17 2 12 14 8 6 11 13 9 16 14 15 11 7 12 5 4 2 1 3 10 6 18 17 8 18 4 15 17 16 2 8 1 11 13 9 12 10 3 6 14 7 5 18 7 2 5 4 3 17 10 15 11 14 12 13 1 16 9 8 6 (11;3,18) 19 3 5 2 6 18 9 10 14 21 20 1 15 4 12 22 16 7 13 8 11 23 17 2 22 13 21 7 4 1 6 18 23 15 12 8 10 19 16 17 11 20 3 5 14 9 1 21 6 7 4 20 16 18 10 23 3 13 17 9 11 14 2 19 8 22 12 15 5 10 12 5 15 8 13 23 17 22 11 18 14 9 3 4 1 16 2 20 21 19 6 7 3 17 22 4 14 2 7 13 1 10 9 16 11 12 23 8 20 19 18 15 21 5 6 10 1 19 15 3 14 23 4 12 11 8 18 2 21 16 6 17 20 7 5 9 13 22 20 13 3 17 18 7 14 10 22 9 1 16 11 21 15 8 4 6 2 5 23 19 12 8 18 12 3 22 11 14 1 15 6 21 5 10 19 9 13 4 2 20 17 16 7 23 21 2 12 15 22 23 3 19 8 5 16 17 1 20 6 18 13 7 9 11 14 4 10 18 10 9 14 7 17 8 6 5 15 16 13 23 11 4 12 20 21 1 22 3 2 19 16 10 14 9 21 12 7 23 13 2 4 19 20 1 22 3 8 17 18 6 5 15 11 15 16 23 21 12 3 19 17 4 9 13 1 18 14 5 22 7 11 10 8 6 20 2 (12;3,23)

determining whether a sequence covering array exists. The situation is worse when constraints have strength three. Consider a collection T of ordered triples of distinct elements of , and associate with (a, b, c) the constraints {(b, a, c), (b, c, a), (a, c, b), (c, a, b)}. Meeting these constraints requires that b lie between a and c, and a collection of constraints of this type forms an instance of the betweenness problem [5] in which one is required to

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1859

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

order all items so that for every triple (a, b, c)  T , b lies between a and c. Forming C = {(b, a, c), (b, c, a), (a, c, b), (c, a, b) : (a, b, c)  T }, even finding a single permutation that covers no 3-subpermutation in C appears hard: Lemma 7.2 (see [30]). Determining whether an instance of the betweenness problem has a solution is NP-complete. These complexity results suggest that constraints pose severe additional challenges in the construction of sequence covering arrays. Checking feasibility can become difficult; even when feasibility is easily checked, the minimization problem is substantially more complicated. 8. Conclusions. The close connection between sequence covering arrays and covering arrays has proved useful in establishing bounds on the sizes of sequence covering arrays. The efficient conditional expectation algorithm for generating sequence covering arrays and the product construction for strength three parallel analogous results for covering arrays. Unfortunately, while sequence covering arrays lead to covering arrays with excess coverage, additional conditions on such a covering array would be required in order to recover a sequence covering array. Hence the parallels between the extensive literature on covering arrays and the existence problem for sequence covering arrays are primarily by analogy. We have examined numerous formulations for sequence covering arrays. In closing, we indicate one more (see also [27]). A perfect hash family PHF(N ; k, w, t) is an N × k array on w symbols in which in every N × t subarray, at least one row consists of distinct symbols. Mehlhorn [29] introduced perfect hash families as an efficient tool for compact storage and fast retrieval of frequently used information; see also [14]. Stinson et al. [35] establish that perfect hash families can be used to construct separating systems, key distribution patterns, group testing algorithms, cover-free families, and secure frameproof codes. They are also used extensively in product constructions for covering arrays [8, 12, 13]. Completely t-scrambling sets of permutations can be viewed as an ordered analogue of perfect hash families in which k = w, no element appears twice in a row, and for every way to select t distinct columns in order there is a row in which the elements in these columns are in increasing order. In particular, a completely t-scrambling set of permutations provides a perfect hash family in which, for every set of t columns, there are at least t! rows containing distinct symbols in the chosen columns, and at least one for each of the t! symbol orderings. For this reason, it appears reasonable to expect that constructions for perfect hash families may also prove to be useful for sequence covering arrays. Acknowledgments. Thanks to two anonymous referees for pointing out relevant references. Special thanks to Mutsunori Banbara and Johannes Oetsch for providing explicit solutions for small sequence covering arrays with t = 3.
REFERENCES [1] M. Banbara, N. Tamura, and K. Inoue, Generating event-sequence test cases by answer set programming with the incidence matrix, in Technical Communications of the 28th International Conference on Logic Programming (ICLP12), 2012, pp. 86­97. ¨ hrer, H. Tompits, and C. Yilmaz, Event[2] M. Brain, E. Erdem, K. Inoue, J. Oetsch, J. Pu sequence testing using answer-set programming, Internat. J. Advances Software, 5 (2012), pp. 237­251. [3] R. C. Bryce and C. J. Colbourn, The density algorithm for pairwise interaction testing, Software Testing, Verification, and Reliability, 17 (2007), pp. 159­182.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1860

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

[4] R. C. Bryce and C. J. Colbourn, A density-based greedy algorithm for higher strength covering arrays, Software Testing Verification Reliability, 19 (2009), pp. 37­53. [5] B. Chor and M. Sudan, A geometric approach to betweenness, SIAM J. Discrete Math., 11 (1998), pp. 511­523. [6] M. B. Cohen, C. J. Colbourn, and A. C. H. Ling, Constructing strength three covering arrays with augmented annealing, Discrete Math., 308 (2008), pp. 2709­2722. [7] C. J. Colbourn, Constructing perfect hash families using a greedy algorithm, in Coding and Cryptology, Y. Li, S. Zhang, S. Ling, H. Wang, C. Xing, and H. Niederreiter, eds., World Scientific, Singapore, 2008, pp. 109­118. [8] C. J. Colbourn, Covering arrays and hash families, in Information Security and Related Combinatorics, NATO Peace and Information Security, IOS Press, Amsterdam, 2011, pp. 99­136. [9] C. J. Colbourn, Efficient conditional expectation algorithms for constructing hash families, in Combinatorial Algorithms, Lecture Notes in Comput. Sci., 7056, Springer-Verlag, Berlin, 2011, pp. 144­155. [10] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk, Strengthening hash families and compressive sensing, J. Discrete Algorithms, 16 (2012), pp. 170­186. [11] C. J. Colbourn, S. S. Martirosyan, Tran Van Trung, and R. A. Walker II, Roux-type constructions for covering arrays of strengths three and four, Des. Codes Cryptogr., 41 (2006), pp. 33­57. [12] C. J. Colbourn and J. Torres-Jim´ enez, Heterogeneous hash families and covering arrays, Contemp. Math., 523 (2010), pp. 3­15. [13] C. J. Colbourn and J. Zhou, Improving two recursive constructions for covering arrays, J. Statist. Theory Practice, 6 (2012), pp. 30­47. [14] Z. J. Czech, G. Havas, and B. S. Majewski, Perfect hashing, Theoret. Comput. Sci., 182 (1997), pp. 1­143. [15] B. Dushnik, Concerning a certain set of arrangements, Proc. Amer. Math. Soc., 1 (1950), pp. 788­796. ¨ hrer, H. Tompits, and C. Yilmaz, Answer-set pro[16] E. Erdem, K. Inoue, J. Oetsch, J. Pu gramming as a new approach to event-sequence testing, in Proceedings of the 2nd International Conference on Advances in System Testing and Validation Lifecycle, Xpert Publishing Services, 2011, pp. 25­34. ¨ redi, Scrambling permutations and entropy of hypergraphs, Random Structures [17] Z. Fu Algorithms, 8 (1996), pp. 97­104. [18] M. M. Z. Hazli, K. Z. Zamli, and R. R. Othman, Sequence-based interaction testing implementation using bees algorithm, in Proceedings of the IEEE Symposium on Computers and Informatics, 2012, pp. 81­85. [19] S. Huang, M. B. Cohen, and A. M. Memon, Repairing GUI test suites using a genetic algorithm, in Proceedings of the 3rd International Conference on Software Testing, Verification and Validation (ICST), 2010, pp. 245­254. [20] Y. Ishigami, Containment problems in high-dimensional spaces, Graphs Combin., 11 (1995), pp. 327­335. [21] Y. Ishigami, An extremal problem of d permutations containing every permutation of every t elements, Discrete Math., 159 (1996), pp. 279­283. [22] D. S. Johnson, Approximation algorithms for combinatorial problems, J. Comput. System Sci., 9 (1974), pp. 256­278. [23] D. R. Kuhn, J. M. Higdon, J. F. Lawrence, R. N. Kacker, and Y. Lei, Combinatorial methods for event sequence testing, in Proceedings of the IEEE 5th International Conference on Software Testing, Verification and Validation (ICST), 2012, pp. 601­609. [24] D. R. Kuhn, J. M. Higdon, J. F. Lawrence, R. N. Kacker, and Y. Lei, Combinatorial methods for event sequence testing, CrossTalk J. Defense Software Engineering, 25 (2012), pp. 15­18. [25] V. I. Levenshtein, Perfect codes in the metric of deletions and insertions, Diskret. Mat., 3 (1991), pp. 3­20. ´ sz, On the ratio of optimal integral and fractional covers, Discrete Math., 13 (1975), [26] L. Lova pp. 383­390. [27] O. Margalit, Better bounds for event sequence testing, in Proceedings of the 2nd International Workshop on Combinatorial Testing, 2013. [28] R. Mathon and Tran Van Trung, Directed t-packings and directed t-Steiner systems, Des. Codes Cryptogr., 18 (1999), pp. 187­198. [29] K. Mehlhorn, Data Structures and Algorithms 1: Sorting and Searching, Springer-Verlag, Berlin, 1984.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1861

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

´ , Total ordering problem, SIAM J. Comput., 8 (1979), pp. 111­114. [30] J. Opatrny [31] J. Radhakrishnan, A note on scrambling permutations, Random Structures Algorithms, 22 (2003), pp. 435­439. [32] J. Spencer, Minimal scrambling sets of simple orders, Acta Math. Acad. Sci. Hungar., 22 (1971/72), pp. 349­353. [33] R. P. Stanley, Increasing and decreasing subsequences and their variants, in Proceedings of the International Congress of Mathematicians, vol. I, Madrid, 2007, pp. 545­579. [34] S. K. Stein, Two combinatorial covering theorems, J. Combin. Theory Ser. A, 16 (1974), pp. 391­397. [35] D. R. Stinson, Tran Van Trung, and R. Wei, Secure frameproof codes, key distribution patterns, group testing algorithms and related structures, J. Statist. Plann. Inference, 86 (2000), pp. 595­617. [36] J. Tarui, On the minimum number of completely 3-scrambling permutations, Discrete Math., 308 (2008), pp. 1350­1354. [37] W. T. Trotter, Jr., Some combinatorial problems for permutations, in Proceedings of the 8th Southeastern Conference on Combinatorics, Graph Theory and Computing, Baton Rouge, La., 1977, Utilitas Mathematica, Winnipeg, pp. 619­632. [38] W. T. Trotter, Jr., Combinatorics and partially ordered sets, in Dimension Theory, Johns Hopkins Ser. Math. Sci., Johns Hopkins University Press, Baltimore, MD, 1992. [39] W. Wang, Y. Lei, S. Sampath, R. Kacker, D. Kuhn, and J. Lawrence, A combinatorial approach to building navigation graphs for dynamic web applications, in Proceedings of the 25th International Conference on Software Maintenance, 2009, pp. 211­220. [40] W. Wang, S. Sampath, Y. Lei, and R. Kacker, An interaction-based test sequence generation approach for testing web applications, in 11th IEEE High Assurance Systems Engineering Symposium, 2008, pp. 209­218. [41] M. Yannakakis, The complexity of the partial order dimension problem, SIAM J. Algebraic Discrete Methods, 3 (1982), pp. 351­358. [42] X. Yuan, M. B. Cohen, and A. M. Memon, Towards dynamic adaptive automated test generation for graphical user interfaces, in International Conference on Software Testing, Verification and Validation Workshops, 2009, pp. 263­266. [43] X. Yuan, M. B. Cohen, and A. M. Memon, GUI interaction testing: Incorporating event context, IEEE Trans. Software Engrg., 37 (2011), pp. 559­574. [44] X. Yuan and A. M. Memon, Generating event sequence-based test cases using GUI runtime state feedback, IEEE Trans. Software Engrg., 36 (2010), pp. 81­95.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

Two-stage algorithms for covering array construction
Kaushik Sarkar and Charles J. Colbourn School of Computing, Informatics, and Decision Systems Engineering Arizona State University, PO Box 878809 Tempe, Arizona, 85287-8809, U.S.A.

arXiv:1606.06730v1 [cs.DS] 21 Jun 2016

June 23, 2016
Abstract Modern software systems often consist of many different components, each with a number of options. Although unit tests may reveal faulty options for individual components, functionally correct components may interact in unforeseen ways to cause a fault. Covering arrays are used to test for interactions among components systematically. A two-stage framework, providing a number of concrete algorithms, is developed for the efficient construction of covering arrays. In the first stage, a time and memory efficient randomized algorithm covers most of the interactions. In the second stage, a more sophisticated search covers the remainder in relatively few tests. In this way, the storage limitations of the sophisticated search algorithms are avoided; hence the range of the number of components for which the algorithm can be applied is extended, without increasing the number of tests. Many of the framework instantiations can be tuned to optimize a memory-quality trade-off, so that fewer tests can be achieved using more memory. The algorithms developed outperform the currently best known methods when the number of components ranges from 20 to 60, the number of options for each ranges from 3 to 6, and t-way interactions are covered for t  {5, 6}. In some cases a reduction in the number of tests by more than 50% is achieved.

Keywords: Covering array, Software interaction testing, Combinatorial construction algorithm

1

Introduction

Real world software and engineered systems are composed of many different components, each with a number of options, that are required to work together in a variety of circumstances. Components are factors, and options for a component form the levels of its factor. Although each level for an individual factor can be tested in isolation, faults in deployed software can arise from interactions among levels of different factors. When an interaction involves levels of t different factors, it is a t-way interaction. Testing for faults caused by t-way interactions for every t is generally infeasible, as a result of a combinatorial explosion. However, empirical research on real world software systems indicates that testing all possible 2-way (or 3-way) interactions would detect 70% (or 90%) of all faults [25]. Moreover, testing all possible 6-way interactions is sufficient for detection of 100% of all faults in the systems examined in [25]. Testing all possible t-way interactions for some 2  t  6 is pseudo-exhaustive testing [24], and is accomplished with a combinatorial array known as a covering array. Formally, let N, t, k, and v be integers with k  t  2 and v  2. A covering array CA(N ; t, k, v ) is an N × k array A in which each entry is from a v -ary alphabet , and for every N × t sub-array B of A and every x  t , there is a row of B that equals x. Then t is the strength of the covering array, k is the number of factors, and v is the number of levels. When k is a positive integer, [k ] denotes the set {1, . . . , k }. A t-way interaction is {(ci , ai ) : 1  i  t, ci  [k ], ci = cj for i = j, and ai  }. So an interaction is an assignment of levels from  to t of the k t factors. It,k,v denotes the set of all k t v interactions for given t, k and v . An N × k array A covers the 1

interaction  = {(ci , ai ) : 1  i  t, ci  [k ], ci = cj for i = j, and ai  } if there is a row r in A such that A(r, ci ) = ai for 1  i  t. When there is no such row in A,  is not covered in A. Hence a CA(N ; t, k, v ) covers all interactions in It,k,v . Covering arrays are used extensively for interaction testing in complex engineered systems. To ensure that all possible combinations of options of t components function together correctly, one needs examine all possible t-way interactions. When the number of components is k , and the number of different options available for each component is v , each row of CA(N ; t, k, v ) represents a test case. The N test cases collectively test all t-way interactions. For this reason, covering arrays have been used in combinatorial interaction testing in varied fields like software and hardware engineering, design of composite materials, and biological networks [8, 24, 26, 32, 34]. The cost of testing is directly related to the number of test cases. Therefore, one is interested in covering arrays with the fewest rows. The smallest value of N for which CA(N ; t, k, v ) exists is denoted by CAN(t, k, v ). Efforts to determine or bound CAN(t, k, v ) have been extensive; see [12, 14, 24, 31] for example. Naturally one would prefer to determine CAN(t, k, v ) exactly. Katona [22] and Kleitman and Spencer [23] independently showed that for t = v = 2, the minimum number of rows N in a CA(N ; 2, k, 2) is the smallest N for which -1 . Exact determination of CAN(t, k, v ) for other values of t and v has remained open. However, k  NN 2 some progress has been made in determining upper bounds for CAN(t, k, v ) in the general case; for recent results, see [33]. For practical applications such bounds are often unhelpful, because one needs explicit covering arrays to use as test suites. Explicit constructions can be recursive, producing larger covering arrays using smaller ones as ingredients (see [14] for a survey), or direct. Direct methods for some specific cases arise from algebraic, geometric, or number-theoretic techniques; general direct methods are computational in nature. Indeed when k is relatively small, the best known results arise from computational techniques [13], and these are in turn essential for the successes of recursive methods. Unfortunately, the existing computational methods encounter difficulties as k increases, but is still within the range needed for practical applications. Typically such difficulties arise either as a result of storage or time limitations or by producing covering arrays that are too big to compete with those arising from simpler recursive methods. Cohen [11] discusses commercial software where the number of factors often exceeds 50. Aldaco et al. [1] analyze a complex engineered system having 75 factors, using a variant of covering arrays. Android [3] uses a Configuration class to describe the device configuration; there are 17 different configuration parameters with 3 - 20 different levels. In each of these cases, while existing techniques are effective when the strength is small, these moderately large values of k pose concerns for larger strengths. In this paper, we focus on situations in which every factor has the same number of levels. These cases have been most extensively studied, and hence provide a basis for making comparisons. In practice, however, often different components have different number of levels, which is captured by extending the notion of a covering array. A mixed covering array MCA(N ; t, k, (v1 , v2 , . . . , vk )) is an N × k array in which the ith column contains vi symbols for 1  i  k . When {i1 , . . . , it }  {1, . . . , k } is a set of t columns, in the N × t t subarray obtained by selecting columns i1 , . . . , it of the MCA, each of the j =1 vij distinct t-tuples appears as a row at least once. Although we examine the uniform case in which v1 = · · · = vk , the methods developed here can all be directly applied to mixed covering arrays as well. Inevitably, when k > max(t + 1, v + 2), a covering array must cover some interactions more than once, for if not they are orthogonal arrays [20]. Treating the rows of a covering array in a fixed order, each row covers some number of interactions not covered by any earlier row. For a variety of known constructions, the initial rows cover many new interactions, while the later ones cover very few [7]. Comparing this rate of coverage for a purely random method and for one of the sophisticated search techniques, one finds little difference in the initial rows, but very substantial differences in the final ones. This suggests strategies to build the covering array in stages, investing more effort as the number of remaining uncovered interactions declines. In this paper we propose a new algorithmic framework for covering array construction, the two-stage framework. In the first stage, a randomized row construction method builds a specified number of rows to cover all but at most a specified, small number of interactions. As we see later, by dint of being randomized

2

this uses very little memory. The second stage is a more sophisticated search that adds few rows to cover the remaining uncovered interactions. We choose search algorithms whose requirements depend on the number of interactions to be covered, to profit from the fact that few interactions remain. By mixing randomized and deterministic methods, we hope to retain the fast execution and small storage of the randomized methods, along with the accuracy of the deterministic search techniques. We introduce a number of algorithms within the two-stage framework. Some improve upon best known bounds on CAN(t, k, v ) (see [33]) in principle. But our focus is on the practical consequences: The twostage algorithms are indeed quite efficient for higher strength (t  {5, 6}) and moderate number of levels (v  {3, 4, 5, 6}), when the number of factors k is moderately high (approximately in the range of 20 - 80 depending on value of t and v ). In fact, for many combination of t, k and v values the two-stage algorithms beat the previously best known bounds. Torres-Jimenez et al. [36] explore a related two-stage strategy. In their first stage, an in-parameter-order greedy strategy (as used in ACTS [24]) adds a column to an existing array; in their second stage, simulated annealing is applied to cover the remaining interactions. They apply their methods when t = v = 3, when the storage and time requirements for both stages remain acceptable. In addition to the issues in handling larger strengths, their methods provide no a priori bound on the size of the resulting array. In contrast with their methods, ours provide a guarantee prior to execution with much more modest storage and time. The rest of the paper is organized as follows. Section 2 reviews algorithmic methods of covering array construction, specifically the randomized algorithm and the density algorithm. This section contrasts these two methods and points out their limitations. Then it gives an intuitive answer to the question of why a two stage based strategy might work and introduces the general two-stage framework. Section 3 introduces some specific two-stage algorithms. Section 3.1 analyzes and evaluates the na¨ ive strategy. Section 3.2 describes a two-stage algorithm that combines the randomized and the density algorithm. Section 3.3 introduces graph coloring based techniques in the second stage. Section 3.4 examines the effect of group action on the size of the constructed covering arrays. Section 4 compares the results of various two-stage algorithms with the presently best known sizes. In Section 5 we discuss the Lov´ asz local lemma (LLL) bounds on CAN(t, k, v ) and provide a Moser-Tardos type randomized algorithm for covering array construction that matches the bound. Although the bound was known [18], the proof was non-constructive, and a constructive algorithm to match this bound seems to be absent in the literature. We explore potentially better randomized algorithms for the first stage using LLL based techniques, We also obtain a two-stage bound that improves the LLL bound for CAN(t, k, v ). We conclude the paper in Section 6.

2

Algorithmic construction of covering arrays

Available algorithms for the construction of covering arrays are primarily heuristic in nature; indeed exact algorithms have succeeded for very few cases. Computationally intensive metaheuristic search methods such as simulated annealing, tabu search, constraint programming, and genetic algorithms have been employed when the strength is relatively small or the number of factors and levels is small. These methods have established many of the best known bounds on sizes of covering arrays [13], but for many problems of practical size their time and storage requirements are prohibitive. For larger problems, the best available methods are greedy. The IPO family of algorithms [24] repeatedly adds one column at a time, and then adds -1 new rows to ensure complete coverage. In this way, at any point in time, the status of v t k t-1 interactions may be stored. AETG [10] pioneered a different method, which greedily selects one row at a time to cover a large number of as-yet-uncovered interactions. They establish that if a row can be chosen that covers the maximum number, a good a priori bound on the size of the covering array can be computed. Unfortunately selecting the maximum is NP-hard, and even if one selects the maximum there is no guarantee that the covering array is the smallest possible [7], so AETG resorts to a good heuristic selection of the next row by examining the stored status of v t k t interactions. None of the methods so far mentioned therefore guarantee to reach an a priori bound. An extension of the AETG strategy, the density algorithm [5, 6, 15], stores additional statistics for all v t k t interactions in order to ensure the selection of a good next row, and hence guarantees to produce an array with at most the precomputed number of rows. Variants of the density 3

Algorithm 1: A randomized algorithm for covering array construction. Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) log (k t )+t log v 1 Set N := ; vt
log
v t -1

2 3

4 5 6 7 8 9 10 11 12

repeat Construct an N × k array A where each entry is chosen independently and uniformly at random from a v -ary alphabet; Set covered := true; for each interaction   It,k,v do if  is not covered in A then Set covered := false; break; end end until covered = true ; Output A;

algorithm have proved to be most effective for problems of moderately large size. For even larger problems, pure random approaches have been applied. To produce methods that provide a guarantee on size, it is natural to focus on the density algorithm in order to understand its strengths and weaknesses. To do this, we contrast it with a basic randomized algorithm. Algorithm 1 shows a simple randomized algorithm for covering array construction. The algorithm constructs an array of a particular size randomly and checks whether all the interactions are covered. It repeats until it finds an array that covers all the interactions. log (k t )+t log v A CA(N ; t, k, v ) with N = is guaranteed to exist: vt
log
v t -1

Theorem 1. [21, 27, 35] (Stein-Lov´ asz-Johnson (SLJ) bound): Let t, k, v be integers with k  t  2, and v  2. Then as k  , CAN(t, k, v )  log
k t

+ t log v
vt v t -1

log

In fact, the probability that the N × k array constructed in line 3 of Algorithm 1 is a valid covering array is high enough that the expected number of times the loop in line 2 is repeated is a small constant. An alternative strategy is to add rows one by one instead of constructing the full array at the outset. We start with an empty array, and whenever we add a new row we ensure that it covers at least the expected number of previously uncovered interactions for a randomly chosen row. The probability that an uncovered interaction is covered by a random row is 1/v t . If the number of uncovered interactions is u, then by linearity of expectation, the expected number of newly covered interactions in a randomly chosen row is uv -t . If each row added covers exactly this expected number, we obtain the same number of rows as the SLJ bound, realized in Algorithm 1. But because the actual number of newly covered interactions is always an integer, each added row covers at least uv -t interactions. This is especially helpful towards the end when the expected number is a small fraction. Algorithm 2 follows this strategy. Again the probability that a randomly chosen row covers at least the expected number of previously uncovered interactions is high enough that the expected number of times the row selection loop in line 6 of Algorithm 2 is repeated is bounded by a small constant. We can obtain an upper bound on the size produced by Algorithm 2 by assuming that each new row added covers exactly uv -t previously uncovered interactions. This bound is the discrete Stein-Lov´ asz4

Algorithm 2: A randomized algorithm for covering array construction using the discrete SLJ strategy. Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) 1 Let A be an empty array; k t 2 Initialize a table T indexed by all t v interactions, marking every interaction "uncovered" ; 3 while there is an interaction marked "uncovered" in T do 4 Let u be the number of interactions marked "uncovered" in T ; u 5 Set expectedCoverage := v t ; 6 repeat 7 Let r be a row of length k where each entry is chosen independently and uniformly at random from a v -ary alphabet; 8 Let coverage be the number of "uncovered" interactions in T that are covered in row r; 9 until coverage > expectedCoverage ; 10 Add r to A; 11 Mark all interactions covered by r as "covered" in T ; 12 end 13 Output A;

Johnson (discrete SLJ) bound. Figure 1 compares the sizes of covering arrays obtained from the SLJ and the discrete SLJ bounds for different values of k when t = 6 and v = 3. Consider a concrete example, when t = 5, k = 20, and v = 3. The SLJ bound guarantees the existence of a covering array with 12499 rows, whereas the discrete SLJ bound guarantees the existence of a covering array with only 8117 rows. The density algorithm replaces the loop at line 6 of Algorithm 2 by a conditional expectation derandomized method. For fixed v and t the density algorithm selects a row efficiently (time polynomial in k ) and deterministically that is guaranteed to cover at least uv -t previously uncovered interactions. In practice, for small values of k the density algorithm works quite well, often covering many more interactions than the minimum. Many of the currently best known CAN(t, k, v ) upper bounds are obtained by the density algorithm in combination with various post-optimization techniques [13]. However, the practical applicability of Algorithm 2 and the density algorithm is limited by the storage t of the table T , representing each of the k t v interactions. Even when t = 6, v = 3, and k = 54, there are 18,828,003,285 6-way interactions. This huge memory requirement renders the density algorithm impractical for rather small values of k when t  {5, 6} and v  3. We present an idea to circumvent this large requirement for memory, and develop it in full in Section 3.

2.1

Why does a two stage based strategy make sense?

Compare the two extremes, the density algorithm and Algorithm 1. On one hand, Algorithm 1 does not suffer from any substantial storage restriction, but appears to generate many more rows than the density algorithm. On the other hand, the density algorithm constructs fewer rows for small values of k , but becomes impractical when k is moderately large. One wants algorithms that behave like Algorithm 1 in terms of memory, but yield a number of rows competitive with the density algorithm. For t = 6, k = 16, and v = 3, Figure 2 compares the coverage profile for the density algorithm and Algorithm 1. We plot the number of newly covered interactions for each row in the density algorithm, and the expected number of newly covered interactions for each row for Algorithm 1. The qualitative features exhibited by this plot are representative of the rates of coverage for other parameters. Two key observations are suggested by Figure 2. First, the expected coverage in the initial random rows is similar to the rows chosen by the density algorithm. In this example, the partial arrays consisting of the first 1000 rows exhibit similar coverage, yet the randomized algorithm needed no extensive bookkeeping. Secondly, as later rows are added, the judicious selections of the density algorithm produce much larger coverage per row than Algorithm 1. Consequently it appears sensible to invest few computational resources 5

3.5

x 10

4

SLJ bound Discrete SLJ bound 3

N - number of rows

2.5

2

1.5

1

0.5 0

100

200

300

400

500 k

600

700

800

900

1000

Figure 1: Comparison of covering array sizes obtained from SLJ bound and discrete SLJ bound for different values of k , when t = 6 and v = 3.

9000 8000 Number of newly covered interactions 7000 6000 5000 4000 3000 2000 1000 0 Density Basic Random

0

2000

4000

6000 Row number

8000

10000

12000

Figure 2: For t = 6, k = 16 and v = 3, the actual number of newly covered interactions of the density algorithm and the expected number of newly covered interactions in a random array.

6

on the initial rows, while making more careful selections in the later ones. This forms the blueprint of our general two-stage algorithmic framework shown in Algorithm 3. Algorithm 3: The general two-stage framework for covering array construction. Input: t : strength of the required covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) 1 Choose a number n of rows and a number  of interactions; // First Stage 2 Use a randomized algorithm to construct an n × k array A ; 3 Ensure that A covers all but at most  interactions; 4 Make a list L of interactions that are not covered in A (L contains at most  interactions); // Second Stage 5 Use a deterministic procedure to add N - n rows to A to cover all the interactions in L; 6 Output A; A specific covering array construction algorithm results by specifying the randomized method in the first stage, the deterministic method in the second stage, and the computation of n and . Any such algorithm produces a covering array, but we wish to make selections so that the resulting algorithms are practical while still providing a guarantee on the size of the array. In Section 3 we describe different algorithms from the two-stage family, determine the size of the partial array to be constructed in the first stage, and establish upper bound guarantees. In Section 4 we explore how good the algorithms are in practice.

3

Two-stage framework
Rand MT the basic randomized algorithm the Moser-Tardos type algorithm

For the first stage we consider two methods:

We defer the development of method MT until Section 5. Method Rand uses a simple variant of Algorithm 1, choosing a random n × k array. For the second stage we consider four methods: Naive Greedy Den Col the the the the na¨ ive strategy, one row per uncovered interaction online greedy coloring strategy density algorithm graph coloring algorithm

Using these abbreviations, we adopt a uniform naming convention for the algorithms: TS A, B is the algorithm in which A is used in the first stage, and B is used in the second stage. For example, TS MT, Greedy denotes a two-stage algorithm where the first stage is a Moser-Tardos type randomized algorithm and the second stage is a greedy coloring algorithm. Later when the need arises we refine these algorithm names.

3.1

One row per uncovered interaction in the second stage (TS Rand, Naive )

In the second stage each of the uncovered interactions after the first stage is covered using a new row. Algorithm 4 describes the method in more detail. This simple strategy improves on the basic randomized strategy when n is chosen judiciously. For example, when t = 6, k = 54 and v = 3, Algorithm 1 constructs a covering array with 17, 236 rows. Figure 3 plots an upper bound on the size of the covering array against the number n of rows in the partial array.

7

Algorithm 4: Na¨ ive two-stage algorithm (TS Rand, Naive ). Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) vt log (k t )+t log v +log log v t -1 ; 1 Let n := t v
log
v t -1

2 3 4

Let  =

1 log
vt v t -1

;

5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

repeat Let A be an n × k array where each entry is chosen independently and uniformly at random from a v -ary alphabet; Let uncovNum := 0 and unCovList be an empty list of interactions; Set covered := true; for each interaction   It,k,v do if  is not covered in A then Set uncovNum :=uncovNum +1; Add  to unCovList ; if uncovNum >  then Set covered := false; break; end end end until covered= true ; for each interaction  uncovList do Add a row to A that covers ; end Output A;

8

1.75 1.7 1.65 1.6 1.55 1.5 1.45 1.4 1.35 1.3

x 10

4

Total number of rows in the Covering array

1

1.1 1.2 1.3 1.4 1.5 1.6 1.7 n -- number of rows in the partial array of the first stage

1.8 x 10
4

Figure 3: An upper bound on the size of the covering array against n, the size of the partial array constructed in the first stage when t = 6, k = 54, and v = 3, with one new row added per uncovered interaction in the second stage. The minimum size of 13, 162 is obtained when n = 12, 402. Algorithm 1 requires 17, 236 rows, and the currently best known covering array has 17, 197 rows.

9

The smallest covering array is obtained when n = 12, 402 which, when completed, yields a covering array with at most 13, 162 rows--a big improvement over Algorithm 1. A theorem from [33] tells us the optimal value of n in general: Theorem 2. [33] Let t, k, v be integers with k  t  2, and v  2. Then log CAN(t, k, v ) 
k t

+ t log v + log log log
vt v t -1
vt v t -1

vt v t -1

+1 .

The bound is obtained by setting n =
t

log (k t )+t log v +log log log
vt v t -1

. The expected number of uncovered

interactions is exactly  = 1/ log vtv-1 . Figure 4 compares SLJ, discrete SLJ and two-stage bounds for k  100, when t = 6 and v = 3. The two-stage bound does not deteriorate in comparison to discrete SLJ bound as k increases; it consistently takes only 307-309 more rows. Thus when k = 12 the two-stage bound requires only 6% more rows and when k = 100 only 2% more rows than the discrete SLJ bound.
x 10
4

2.2 2 1.8 N - number of rows 1.6 1.4 1.2 1 0.8 0.6

SLJ bound Discrete SLJ bound Two-stage bound

0.4 10

20

30

40

50 k

60

70

80

90

100

Figure 4: Comparison of covering array sizes obtained from SLJ bound, discrete SLJ bound and two-stage bound for k  100, when t = 6 and v = 3. In this range of k values the two-stage bound requires 307-309 more rows than the discrete SLJ bound, that is, 2-6% more rows. To ensure that the loop in line 7 of Algorithm 4 does not repeat too many times we need to know the probability with which a random n × k array leaves at most  interactions uncovered. Using Chebyshev's inequality and the second moment method developed in [2, Chapter 4], we next show that in a random n × k 1 n t array the number of uncovered interactions is almost always close to its expectation, i.e. k . t v 1 - vt Substituting the value of n from line 1, this expected value is equal to µ, as in line 2. Therefore, the probability that a random n × k array covers the desired number of interactions is constant, and the expected number of times the loop in line 7 is repeated is also a constant (around 2 in practice).

10

Because the theory of the second moment method is developed in considerable detail in [2], here we briefly m mention the relevant concepts and results. Suppose that X = i=1 Xi , where Xi is the indicator random variable for event Ai for 1  i  m. For indices i, j , we write i  j if i = j and the events Ai , Aj are not independent. Also suppose that X1 , . . . , Xm are symmetric, i.e. for every i = j there is a measure preserving mapping of the underlying probability space that sends event Ai to event Aj . Define  = j i Pr [Aj |Ai ]. Then by [2, Corollary 4.3.4]: Lemma 3. [2] If E[X ]   and  = o(E[X ]) then X  E[X ] almost always. In our case, Ai denotes the event that the ith interaction is not covered in a n × k array where each entry 1 n . Because is chosen independently and uniformly at random from a v -ary alphabet. Then Pr[Xi ] = 1 - v t k t 1 n t , and E [ X ]   as there are k v interactions in total, by linearity of expectation, E [ X ] = v 1 - t t vt k  . Distinct events Ai and Aj are independent if the ith and j th interactions share no column. Therefore, k  the event Ai is not independent of at most t t- j i Pr [Aj |Ai ]  j i 1  1 other events Aj . So  = k t t-1 = o(E[X ]) when v and t are constants. By Lemma 3, the number of uncovered interactions in a random n × k array is close to the expected number of uncovered interactions. This guarantees that Algorithm 4 is an efficient randomized algorithm for constructing covering arrays with a number of rows upper bounded by Theorem 2. In keeping with the general two-stage framework, Algorithm 4 does not store the coverage status of each interaction. We only need store the interactions that are uncovered in A, of which there are at most 1  =  v t . This quantity depends only on v and t and is independent of k , so is effectively a vt
log
v t -1

t constant that is much smaller than k t v , the storage requirement for the density algorithm. Hence the algorithm can be applied to a higher range of k values. Although Theorem 5 provides asymptotically tighter bounds than Theorem 2, in a range of k values that are relevant for practical application, Theorem 2 provides better results. Figure 5 compares the bounds on CAN(t, k, v ) with the currently best known results.
4 6

3

x 10

2.5 Best known Two-stage (simple) GSS bound 2

x 10

2.5

Best known Two-stage (simple) GSS bound

N - number of rows

N - number of rows 10 20 30 40 k 50 60 70 80 90

2

1.5

1.5

1

1

0.5

0.5

0 0

0

5

10

15

20

25 k

30

35

40

45

50

(a) t = 6, v = 3

(b) t = 6, v = 6

Figure 5: Comparison of GSS bound and two-stage bound with the currently best known results

3.2

The density algorithm in the second stage (TS Rand, Den )

Next we apply the density algorithm in the second stage. Figure 6 plots an upper bound on the size of the covering array against the size of the partial array constructed in the first stage when the density algorithm is used in the second stage, and compares it with TS Rand, Naive . The size of the covering array decreases 11

as n decreases. This is expected because with smaller partial arrays, more interactions remain for the second stage to be covered by the density algorithm. In fact if we cover all the interactions using the density algorithm (as when n = 0) we would get an even smaller covering array. However, our motivation was precisely to avoid doing that. Therefore, we need a "cut-off" for the first stage.
x 10
4

1.9

Total number of rows in the Covering array

1.8

Basic Two-stage Two-stage with density in second stage

1.7

1.6

1.5

1.4

1.3

1.2

1

1.1 1.2 1.3 1.4 1.5 1.6 1.7 n -- number of rows in the partial array of the first stage

1.8 x 10
4

Figure 6: Comparison of covering array sizes from two-stage algorithms with Den and Naive in the second stage. With Den there is no minimum point in the curve; the size of the covering array keeps decreasing as we leave more uncovered interactions for the second stage. We are presented with a trade-off. If we construct a smaller partial array in the first stage, we obtain a smaller covering array overall. But we then pay for more storage and computation time for the second stage. To appreciate the nature of this trade-off, look at Figure 7, which plots an upper bound on the covering array size and the number of uncovered interactions in the first stage against n. The improvement in the covering array size plateaus after a certain point. The three horizontal lines indicate  ( v t ), 2 and 3 uncovered interactions in the first stage. (In the na¨ ive method of Section 3.1, the partial array after the first stage leaves at most  uncovered interactions.) In Figure 7 the final covering array size appears to plateau when the number of uncovered interactions left by the first stage is around 2. After that we see diminishing returns -- the density algorithm needs to cover more interactions in return for a smaller improvement in the covering array size. Let r be the maximum number of interactions allowed to remain uncovered after the first stage. Then r can be specified in the two-stage algorithm. To accommodate this, we denote by TS A, B ; r the two-stage algorithm where A is the first stage strategy, B is the second stage strategy, and r is the maximum number of uncovered interactions after the first stage. For example, TS Rand, Den; 2 applies the basic randomized algorithm in the first stage to cover all but at most 2 interactions, and the density algorithm to cover the remaining interactions in the second stage.

3.3

Coloring in the second stage (TS Rand, Col and TS Rand, Greedy )

Now we describe strategies using graph coloring in the second stage. Construct a graph G = (V, E ), the incompatibility graph, in which V is the set of uncovered interactions, and there is an edge between two 12

18000 Number of rows / Number of uncovered interactions 16000 14000 12000 10000 8000 6000 4000 2000 0 0.8 Num. of rows in the completed CA Num. of uncovered interaction in first stage

0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 4 n -- number of rows in the partial array of the first stage x 10

Figure 7: Final covering array size against the number of uncovered interactions after the first stage. As the size n of the partial array decreases, the number of uncovered interactions in the first stage increases. Den is used in the second stage. From bottom to top, the green lines denote , 2, and 3 uncovered interactions. interactions exactly when they share a column in which they have different symbols. A single row can cover a set of interactions if and only if it forms an independent set in G. Hence the minimum number of rows required to cover all interactions of G is exactly its chromatic number (G), the minimum number of colors in a proper coloring of G. Graph coloring is an NP-hard problem, so we employ heuristics to bound the chromatic number. Moreover, G only has vertices for the uncovered interactions after the first stage, so is size is small relative to the total number of interactions. The expected number of edges in the incompatibility graph after choosing n rows uniformly at random n t k t t k -t 1 1 n t t-i is  = 2 ) 1- v . Using the elementary upper bound on 1 - (vt -1 t i=1 i t-i (v - v t v v t-i ) the chromatic number  
1 2

+

2m + 1 4 , where m is the number of edges [16, Chapter 5.2], we can surely

cover the remaining interactions with at most 1 2m + 1 2 + 4 rows. The actual number of edges m that remain after the first stage is a random variable with mean  . In principle, the first stage could be repeatedly applied until m   , so we call m =  the optimistic estimate. To ensure that the first stage is expected to be run a small constant number of times, we increase the estimate. With probability more than 1/2 the incompatibility graph has m  2 edges, so m = 2 is the conservative estimate. For t = 6, k = 56, and v = 3, Figure 8 shows the effect on the minimum number of rows when the bound on the chromatic number in the second stage is used, for the conservative or optimistic estimates. The Na¨ ive method is plotted for comparison. Better coloring bounds shift the minima leftward, reducing the number of rows produced in both stages. Thus far we have considered bounds on the chromatic number. Better estimation of (G) is complicated by the fact that we do not have much information about the structure of G until the first stage is run. In practice, however, G is known after the first stage and hence an algorithmic method to bound its chromatic number can be applied. Because the number of vertices in G equals the number of uncovered interactions

13

2.2

x 10

4

2

Conservative estimate Optimistic estimate Simple

Number of rows required

1.8

1.6

1.4

1.2

1

1

1.1

1.2

1.3

1.4 N

1.5

1.6

1.7

1.8 x 10
4

Figure 8: Size of the partial array vs. size of the complete CA. t = 6, k = 56, v = 3. Stein-Lov´ asz-Johnson bound requires 17, 403 rows, discrete Stein-Lov´ asz-Johnson bound requires 13, 021 rows. Simple estimate for the two stage algorithm is 13, 328 rows, conservative estimate assuming m = 2 is 12, 159 rows, and optimistic estimate assuming m =  is 11, 919 rows. Even the conservative estimate beats the discrete Stein-Lov´ asz-Johnson bound.

14

after the first stage, we encounter the same trade-off between time and storage, and final array size, as seen earlier for density. Hence we again parametrize by the expected number of uncovered interactions in the first stage. We employ two different greedy algorithms to color the incompatibility graph. In method Col we first construct the incompatibility graph G after the first stage. Then we apply the commonly used smallest last order heuristic to order the vertices for greedy coloring: At each stage, find a vertex vi of minimum degree in Gi , order the vertices of Gi - vi , and then place vi at the end. More precisely, we order the vertices of G as v1 , v2 , . . . , vn , such that vi is a vertex of minimum degree in Gi , where Gi = G - {vi+1 , . . . , vn }. A graph is d-degenerate if all of its subgraphs have a vertex with degree at most d. When G is d-degenerate but not (d - 1)-degenerate, the Coloring number col(G) is d + 1. If we then greedily color the vertices with the first available color, at most col(G) colors are used. In method Greedy we employ an on-line, greedy approach that colors the interactions as they are discovered in the first stage. In this way, the incompatibility graph is never constructed. We instead maintain a set of rows. Some entries in rows are fixed to a specific value; others are flexible to take on any value. Whenever a new interaction is found to be uncovered in the first stage, we check if any of the rows is compatible with this interaction. If such a row is found then entries in the row are fixed so that the row now covers the interaction. If no such row exists, a new row with exactly t fixed entries corresponding to the interaction is added to the set of rows. This method is much faster than method Col in practice.

3.4

Using group action

Covering arrays that are invariant under the action of a permutation group on their symbols can be easier to construct and are often smaller [15]. Direct and computational constructions using group actions are explored in [9, 28]. Sarkar et al. [33] establish the asymptotically tightest known bounds on CAN(t, k, v ) using group actions. In this section we explore the implications of group actions on two-stage algorithms. Let  be a permutation group on the set of symbols. The action of this group partitions the set of t-way interactions into orbits. We construct an array A such that for every orbit, at least one row covers an interaction from that orbit. Then we develop the rows of A over  to obtain a covering array that is invariant under the action of . Effort then focuses on covering all the orbits of t-way interactions, instead of the individual interactions. If  acts sharply transitively on the set of symbols (for example, if  is a cyclic group of order v ) then k t-1 t the action of  partitions k orbits of length v each. Following the lines of the t v interactions into t v v t-1 log (k t )+(t-1) log v +log log v t-1 -1 +1 that covers at proof of Theorem 2, there exists an n × k array with n = v t-1
log
v t-1 -1

least one interaction from each orbit. Therefore, log CAN(t, k, v )  v
k t

+ (t - 1) log v + log log log
v t- 1 v t- 1 - 1

v t-1 v t-1 -1

+1 . (1)

Similarly, we can employ a Frobenius group. When v is a prime power, the Frobenius group is the group of permutations of Fv of the form {x  ax + b : a, b  Fv , a = 0}. The action of the Frobenius group t-1 1 partitions the set of t-tuples on v symbols into v v-- 1 orbits of length v (v - 1) (full orbits) each and 1 orbit of length v (a short orbit). The short orbit consists of tuples of the form (x1 , . . . , xt ) where x1 = . . . = xt . Therefore, we can obtain a covering array by first constructing an array that covers all the full orbits, and then developing all the rows over the Frobenius group and adding v constant rows. Using the two stage strategy in conjunction with the Frobenius group action we obtain: log CAN(t, k, v )  v (v - 1)
k t

+ log

v t-1 -1 v -1

+ log log
v t- 1 v t-1 -v +1

v t-1 v t-1 -v +1

+1 + v. (2)

log

15

1.5

x 10

4

1.45

Two-stage (simple) Two-stage (cyclic group action) Two-stage (Frobenius group action)

N - number of rows

1.4

1.35

1.3

1.25 50

55

60 k

65

70

75

Figure 9: Comparison of the simple two-stage bound with the cyclic and the Frobenius two-stage bounds. t = 6, v = 3 and 50  k  75. Group action reduces the required number of rows slightly. Figure 9 compares the simple two-stage bound with the cyclic and Frobenius two-stage bounds. For t = 6, v = 3 and 12  k  100, the cyclic bound requires 7-21 (on average 16) fewer rows than the simple bound. In the same range the Frobenius bound requires 17 - 51 (on average 40) fewer rows. Group action can be applied in other methods for the second stage as well. Colbourn [15] incorporates group action into the density algorithm, allowing us to apply method Den in the second stage. Greedy extends easily to use group action, as we do not construct an explicit incompatibility graph. Whenever we fix entries in a row to cover an uncovered orbit, we commit to a specific orbit representative. However, applying group action to the incompatibility graph coloring for Col is more complicated. We need to modify the definition of the incompatibility graph for two reasons. First the vertices no longer represent uncovered interactions, but rather uncovered orbits of interaction. Secondly, and perhaps more importantly, pairwise compatibility between every two orbits in a set no longer implies mutual compatibility among all orbits in the set. One approach is to form a vertex for each uncovered orbit, placing an edge between two when they share a column. Rather than the usual coloring, however, one asks for a partition of the vertex set into classes so that every class induces an acyclic subgraph. Problems of this type are generalized graph coloring problems [4]. Within each class of such a vertex partition, consistent representatives of each orbit can be selected to form a row; when a cycle is present, this may not be possible. Unfortunately, heuristics for solving these types of problems appear to be weak, so we adopt a second approach. As we build the incompatibility graph, we commit to specific orbit representatives. When a vertex for an uncovered orbit is added, we check its compatibility with the orbit representatives chosen for the orbits already handled with which it shares columns; we commit to an orbit representative and add edges to those with which it is now incompatible. Once completed, we have a (standard) coloring problem for the resulting graph. Because group action can be applied using each of the methods for the two stages, we extend our naming to TS A, B ; r,  , where  can be Trivial (i.e. no group action), Cyclic, or Frobenius.

16

4

Computational results

Figure 5 indicates that even a simple two-stage bound can improve on best known covering array numbers. Therefore we investigate the actual performance of our two-stage algorithms for covering arrays of strength 5 and 6. First we present results for t = 6, when v  {3, 4, 5, 6} and no group action is assumed. Table 1 shows the results for different v values. In each case we select the range of k values where the two-stage bound predicts smaller covering arrays than the previously known best ones, setting the maximum number of uncovered t interactions as  = 1/ log vtv-1  v t . For each value of k we construct a single partial array and then run the different second stage algorithms on it consecutively. In this way all the second stage algorithms cover the same set of uncovered interactions. The column tab lists the best known CAN(t, k, v ) upper bounds from [13]. The column bound shows the upper bounds obtained from the two-stage bound (2). The columns na¨ ive, greedy, col and den show results obtained from running the TS Rand, Naive; , Trivial , TS Rand, Greedy; , Trivial , TS Rand, Col; , Trivial and TS Rand, Den; , Trivial algorithms, respectively. The na¨ ive method always finds a covering array that is smaller than the two-stage bound. This happens because we repeat the first stage of Algorithm 4 until the array has fewer than v t uncovered interactions. (If the first stage were not repeated, the algorithm still produce covering arrays that are not too far from the bound.) For v = 3 Greedy and Den have comparable performance. Method Col produces covering arrays that are smaller. However, for v  {4, 5, 6} Den and Col are competitive. Table 2 shows the results obtained by the different second stage algorithms when the maximum number of uncovered interactions in the first stage is set to 2 and 3 respectively. When more interactions are covered in the second stage, we obtain smaller arrays as expected. However, the improvement in size does not approach 50%. There is no clear winner. Next we investigate the covering arrays that are invariant under the action of a cyclic group. In Table 3 the column bound shows the upper bounds from Equation (1). The columns na¨ ive, greedy, col and den show results obtained from running TS Rand, Naive; , Cyclic , TS Rand, Greedy; , Cyclic , TS Rand, Col; , Cyclic and TS Rand, Den; , Cyclic , respectively. Table 4 presents results for cyclic group action based algorithms when the number of maximum uncovered interactions in the first stage is set to 2 and 3 respectively. For the Frobenius group action, we show results only for v  {3, 5} in Table 5. The column bound shows the upper bounds obtained from Equation (2). Table 6 presents results for Frobenius group action algorithms when the number of maximum uncovered interactions in the first stage is 2 or 3. Next we present a handful of results when t = 5. In the cases examined, using the trivial group action is too time consuming to be practical. However, the cyclic or Frobenius cases are feasible. Tables 7 and 8 compare two stage algorithms when the number of uncovered interactions in the first stage is at most 2. In almost all cases there is no clear winner among the three second stage methods. Methods Den and Greedy are, however, substantially faster and use less memory than method Col; for practical purposes they would be preferred. All code used in this experimentation is available from the github repository https://github.com/ksarkar/CoveringArray under an open source GPLv3 license.

5

Limited dependence and the Moser-Tardos algorithm

Here we explore a different randomized algorithm that produces smaller covering arrays than Algorithm 1. When k > 2t, there are interactions that share no column. The events of coverage of such interactions are independent. Moser et al. [29, 30] provide an efficient randomized construction method that exploits this

17

k 53 54 55 56 57 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27

tab 13021 14155 17161 19033 20185 68314 71386 86554 94042 99994 104794 233945 258845 281345 293845 306345 356045 506713 583823 653756 694048 783784 844834 985702 1035310 1112436 1146173 1184697

bound 13076 13162 13246 13329 13410 65520 66186 66834 67465 68081 68681 226700 229950 233080 236120 239050 241900 486310 505230 522940 539580 555280 570130 584240 597660 610460 622700 634430

na¨ ive greedy t = 6, v = 3 13056 12421 13160 12510 13192 12590 13304 12671 13395 12752 t = 6, v = 4 65452 61913 66125 62573 66740 63209 67408 63819 68064 64438 68556 65021 t = 6, v = 5 226503 213244 229829 216444 232929 219514 235933 222516 238981 225410 241831 228205 t = 6, v = 6 486302 449950 505197 468449 522596 485694 539532 502023 555254 517346 569934 531910 584194 545763 597152 558898 610389 571389 622589 583473 634139 594933

col 12415 12503 12581 12665 12748 61862 62826 63160 64077 64935 65739 212942 217479 219215 222242 226379 230202 448922 467206 484434 500788 516083 530728 544547 557917 570316 582333 593857

den 12423 12512 12591 12674 12757 61886 62835 63186 64082 64907 65703 212940 217326 219241 222244 226270 229942 447864 466438 483820 500194 515584 530242 548307 557316 569911 582028 593546

Table 1: Comparison of different TS Rand, -; , Trivial algorithms.

18

k greedy 53 54 55 56 57 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27 11968 12135 12286 12429 12562 59433 60090 60715 61330 61936 62530 204105 207243 210308 213267 216082 218884 425053 443236 460315 476456 491570 505966 519611 532612 544967 556821 568135

2 col 11958 12126 12129 12204 12290 59323 60479 61527 62488 61839 62899 203500 206659 209716 212675 215521 218314 -

den t = 6, v 11968 12050 12131 12218 12296 t = 6, v 59326 59976 60615 61242 61836 62428 t = 6, v 203302 206440 209554 212508 215389 218172 t = 6, v 420333 438754 455941 472198 487501 502009 515774 528868 541353 553377 564827

greedy =3 11716 11804 11877 11961 12044 =4 58095 58742 59369 59974 60575 61158 =5 199230 202342 205386 208285 211118 213872 =6 412275 430402 447198 463071 478269 492425 505980 518746 531042 542788 554052

3 col 11705 11787 11875 12055 12211 57951 58583 59867 61000 60407 61004 198361 201490 204548 -

den 11708 11790 11872 11950 12034 57888 58544 59187 59796 60393 60978 197889 201068 204107 207060 209936 212707 405093 423493 440532 456725 471946 486306 500038 513047 525536 537418 548781

Table 2: Comparison of TS Rand, -; 2, Trivial and TS Rand, -; 3, Trivial algorithms.

19

k 53 54 55 56 57 k 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27

tab 13021 14155 17161 19033 20185 tab 68314 71386 86554 94042 99994 104794 226000 244715 263145 235835 238705 256935 506713 583823 653756 694048 783784 844834 985702 1035310 1112436 1146173 1184697

bound 13059 13145 13229 13312 13393 bound 65498 66163 66811 67442 68057 68658 226680 229920 233050 236090 239020 241870 486290 505210 522910 539550 555250 570110 584210 597630 610430 622670 624400

na¨ ive greedy t = 6, v = 3 13053 12405 13119 12489 13209 12573 13284 12660 13368 12744 t = 6, v = 4 na¨ ive greedy 65452 61896 66080 62516 66740 63184 67408 63800 68032 64408 68556 64988 t = 6, v = 5 226000 213165 229695 216440 233015 219450 235835 222450 238705 225330 241470 228140 t = 6, v = 6 485616 449778 504546 468156 522258 485586 539280 501972 554082 517236 569706 531852 583716 545562 597378 558888 610026 571380 622290 583320 633294 594786

col 12405 12543 12663 12651 12744 col 61860 62820 63144 63780 64692 64964 212945 217585 221770 222300 225130 229235 448530 467232 490488 500880 521730 530832 549660 557790 575010 582546 598620

den 12411 12546 12663 12663 12750 den 61864 62784 63152 63784 64680 64976 212890 217270 221290 222210 225120 229020 447732 466326 488454 500172 519966 530178 548196 557280 573882 582030 597246

Table 3: Comparison of TS Rand, -; , Cyclic algorithms.

20

k greedy 53 54 55 56 57 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27 11958 12039 12120 12204 12276 59412 60040 60700 61320 61908 62512 204060 207165 207165 213225 216050 218835 424842 443118 460014 476328 491514 505884 519498 532368 544842 543684 568050

2 col 11955 12027 12183 12342 12474 59336 59996 61156 62196 63192 64096 203650 209110 209865 212830 217795 218480 422736 440922 457944 474252 489270 503580 517458 530340 542688 543684 566244

den t = 6, v 11958 12036 12195 12324 12450 t = 6, v 59304 59964 61032 61976 62852 63672 t = 6, v 203265 208225 209540 212510 217070 218155 t = 6, v 420252 438762 455994 472158 487500 501852 515718 528828 541332 543684 564756

greedy =3 11700 11790 11862 11949 12027 =4 58076 58716 59356 59932 60568 61152 =5 199180 202255 205380 208225 211080 213770 =6 411954 430506 447186 463062 478038 492372 505824 518700 530754 542664 553704

3 col 11691 11874 12057 11937 12021 57976 58616 59252 59840 61124 61048 198455 204495 204720 207790 213425 213185 409158 427638 456468 460164 486180 489336 502806 515754 538056 539922 560820

den 11694 11868 12027 11943 12024 57864 58520 59160 59760 60904 60988 197870 203250 204080 207025 212040 212695 405018 423468 449148 456630 479970 486264 500040 512940 532662 537396 555756

Table 4: Comparison of TS Rand, -; 2, Cyclic and TS Rand, -; 3, Cyclic algorithms.

21

k 53 54 55 56 57 31 32 33 34 35 36

tab 13021 14155 17161 19033 20185 233945 258845 281345 293845 306345 356045

bound 13034 13120 13203 13286 13366 226570 229820 232950 235980 238920 241760

na¨ ive greedy t = 6, v = 3 13029 12393 13071 12465 13179 12561 13245 12633 13365 12723 t = 6, v = 5 226425 213025 229585 216225 232725 219285 234905 222265 238185 225205 241525 227925

col 12387 12513 12549 12627 12717 212865 216085 219205 223445 227445 231145

den 12393 12531 12567 12639 12735 212865 216065 219145 223265 227065 230645

Table 5: Comparison of TS Rand, -; , Frobenius algorithms.

k greedy 53 54 55 56 57 70 75 80 85 90 31 32 33 34 35 36 50 55 60 65 11931 12021 12105 12171 12255 13167 13473 13773 14031 14289 203785 206965 209985 213005 215765 218605 250625 259785 268185 275785

2 col 11919 12087 12237 12171 12249 13155 13473 13767 14025 14283 203485 208965 209645 214825 215545 218285 250365 259625 268025 275665

den greedy t = 6, v = 3 11931 11700 12087 11790 12231 11862 12183 11949 12255 12027 13179 13479 13779 14037 14301 t = 6, v = 5 203225 198945 208065 201845 209405 205045 214145 208065 215265 210705 218025 213525 250325 259565 267945 275665 -

3 col 11691 11874 12057 11937 12021 198445 204505 209845 207545 210365 213105 -

den 11694 11868 12027 11943 12024 197825 203105 207865 206985 209885 212645 -

Table 6: Comparison of TS Rand, -; 2, Frobenius and TS Rand, -; 3, Frobenius algorithms.

22

k 67 68 69 70 71

tab 59110 60991 60991 60991 60991

greedy 48325 48565 48765 49005 49245

col 48285 48565 49005 48985 49205

den 48305 48585 48985 49025 49245

Table 7: Comparison of TS Rand, -; 2, Frobenius algorithms. t = 5, v = 5 k 49 50 51 52 53 tab 122718 125520 128637 135745 137713 greedy 108210 109014 109734 110556 111306 col 108072 108894 110394 110436 111180 den 107988 108822 110166 110364 111120

Table 8: Comparison of TS Rand, -; 2, Cyclic algorithms. t = 5, v = 6 limited dependence. Specializing their method to covering arrays, we obtain Algorithm 5. For the specified value of N in the algorithm it is guaranteed that the expected number of times the loop in line 3 of Algorithm 5 is repeated is linearly bounded in k (See Theorem 1.2 of [30]). The upper bound on CAN(t, k, v ) guaranteed by Algorithm 5 is obtained by applying the Lov´ asz local lemma (LLL). Lemma 4. (Lov´ asz local lemma; symmetric case) (see [2]) Let A1 , A2 , . . . , An events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at ¯ most d, and that Pr[Ai ]  p for all 1  i  n. If ep(d + 1)  1, then Pr[n i=1 Ai ] > 0. The symmetric version of Lov´ asz local lemma provides an upper bound on the probability of a "bad" event in terms of the maximum degree of a bad event in a dependence graph, so that the probability that all the bad events are avoided is non zero. Godbole, Skipper, and Sunley [18] apply Lemma 4 essentially to obtain the bound on CAN(t, k, v ) in line 1 of Algorithm 5. Theorem 5. [18] Let t, v and k  2t be integers with t, v  2. Then log CAN (t, k, v ) 
k t

-

k-t t

+ t log v + 1

log

vt v t -1

The bound on the size of covering arrays obtained from Theorem 5 is asymptotically tighter than the one obtained from Theorem 1. Figure 10 compares the bounds for t = 6 and v = 3. The original proof of LLL is essentially non-constructive and does not immediately lead to a polynomial time construction algorithm for covering arrays satisfying the bound of Theorem 5. Indeed no previous construction algorithms appear to be based on it. However the Moser-Tardos method of Algorithm 5 does provide a construction algorithm running in expected polynomial time. For sufficiently large values of k Algorithm 5 produces smaller covering arrays than the Algorithm 1. But the question remains: Does Algorithm 5 produce smaller covering arrays than the currently best known results within the range that it can be effectively computed? Perhaps surprisingly, we show that the answer is affirmative. In Algorithm 5 we do not need to store the coverage information of individual interactions in memory because each time an uncovered interaction is encountered we re-sample the columns involved in that interaction and start the check afresh (checking the coverage in interactions in the same order each time). Consequently, Algorithm 5 can be applied for larger values of k than the density algorithm. 23

Algorithm 5: Moser-Tardos type algorithm for covering array construction. Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) k -t log{(k t )-( t )}+t. log v +1 ; 1 Let N := vt
log
v t -1

2

3 4 5 6 7 8 9 10 11 12 13

14 15 16

Construct an N × k array A where each entry is chosen independently and uniformly at random from a v -ary alphabet; repeat Set covered := true; for each interaction   It,k,v do if  is not covered in A then Set covered := false; Set missing-interaction := ; break; end end if covered = false then Choose all the entries in the t columns involved in missing-interaction independently and uniformly at random from the v -ary alphabet; end until covered = true ; Output A;

Smaller covering arrays can be obtained by exploiting a group action using LLL, as shown in [33]. Table 9 shows the sizes of the covering arrays constructed by a variant of Algorithm 5 that employs cyclic and Frobenius group actions. While this single stage algorithm produces smaller arrays than the currently best known [13], these are already superseded by the two-stage based algorithms. k 56 57 58 59 60 tab 19033 20185 23299 23563 23563 MT 16281 16353 16425 16491 16557 k 44 45 46 47 48 tab 411373 417581 417581 423523 423523 MT 358125 360125 362065 363965 365805 k 25 26 27 28 29 tab 1006326 1040063 1082766 1105985 1149037 MT 1020630 1032030 1042902 1053306 1063272

(a) Frobenius. t = 6, v = 3

(b) Frobenius. t = 6, v = 5

(c) Cyclic. t = 6, v = 6

Table 9: Comparison of covering array size from Algorithm 5 (MT) with the best known results [13] (tab).

5.1

Moser-Tardos type algorithm for the first stage

The linearity of expectation arguments used in the SLJ bounds permit one to consider situations in which a few of the "bad" events are allowed to occur, a fact that we exploited in the first stage of the algorithms thus far. However, the Lov´ asz local lemma does not address this situation directly. The conditional Lov´ asz local lemma (LLL) distribution, introduced in [19], is a very useful tool. Lemma 6. (Conditional LLL distribution; symmetric case) (see [2, 33]) Let A = {A1 , A2 , . . . , Al } be a set of l events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at most d, and that Pr[Ai ]  p for all 1  i  l. Also suppose that ep(d +1)  1 ¯ (Therefore, by LLL (Lemma 4) Pr[l / A be another event in the same probability space i=1 Ai ] > 0). Let B  24

10

5

SLJ bound GSS bound

N - number of rows

10

4

10 1 10

3

10

2

10 k

3

10

4

10

5

Figure 10: Comparison of SLJ (Theorem 1) and GSS (Theorem 5) bounds for t = 6 and v = 3. The graph is plotted in log-log scale to highlight the asymptotic difference between the two bounds. with Pr[B ]  q , such that B is also mutually independent of a set of all other events Aj  A except for at ¯ most d. Then Pr[B | l i=1 Ai ]  eq . We apply the conditional LLL distribution to obtain an upper bound on the size of partial array that t leaves at most log vtv-1  v t interactions uncovered. For a positive integer k , let I = {j1 , . . . , j }  [k ] where j1 < . . . < j . Let A be an n × k array where each entry is from the set [v ]. Let AI denote the n ×  array in which AI (i, ) = A(i, j ) for 1  i  N and 1   ; AI is the projection of A onto the columns in I . ] Let M  [v ]t be a set of m t-tuples of symbols, and C  [k be a set of t columns. Suppose the t entries in the array A are chosen independently from [v ] with uniform probability. Let BC denote the event that at least one of the tuples in M is not covered in AC . There are  = k t such events, and for all of 1 n them Pr[BC ]  m 1 - v . Moreover, when k  2t, each of the events is mutually independent of all t k k -t other events except for at most  = k - 1 < t t- asz local lemma, when t - t 1 . Therefore, by the Lov´ 1 n em 1 - vt  1, none of the events BC occur. Solving for n, when n log(em) log
vt v t -1

(3)

] there exists an n × k array A over [v ] such that for all C  [k t , AC covers all the m tuples in M . In fact we can use a Moser-Tardos type algorithm to construct such an array. Let  be an interaction whose t-tuple of symbols is not in M . Then the probability that  is not covered 1 n in an n × k array is at most 1 - v when each entry of the array is chosen independently from [v ] with t uniform probability. Therefore, by the conditional LLL distribution the probability that  is not covered ] 1 n in the array A where for all C  [k . Moreover, t , AC covers all the m tuples in M is at most e 1 - v t there are  (v t - m) such interactions . By the linearity of expectation, the expected number of uncovered

25

interactions in A is less than v t when  (v t - m)e 1 - n

1 n vt

 v t . Solving for n, we obtain
m vt

log e 1 - log
vt v t -1

.

(4)

Therefore, there exists an n × k array with n = max

log(em) log
vt v t -1

,

m log{e(1- v t )}

log

vt v t -1

that has at most v t

uncovered interactions. To compute n explicitly, we must choose m. We can select a value of m to minimize n graphically for given values of t, k and v . For example, Figure 11 plots Equations 3 and 4 against m for t = 3, k = 350, v = 3, and finds the minimum value of n.
460 Equation (3) Equation (4) n - number of rows in the partial array n - number of rows in the partial array 440 440 445 max(Equation (3), Equation (4))

420

435

400

430

380

360

425

340 0

5

10

15 m

20

25

30

420 0

5

10

15 m

20

25

30

(a) Equations 3 and 4 against m.

(b) Maximum of the two sizes against m.

Figure 11: The minimum is at n = 422, when m = 16. t = 3, k = 350, v = 3 We compare the size of the partial array from the na¨ ive two-stage method (Algorithm 4) with the size obtained by the graphical methods in Figure 12. The Lov´ asz local lemma based method is asymptotically better than the simple randomized method. However, except for the small values of t and v , in the range of k values relevant for practical applications the simple randomized algorithm requires fewer rows than the Lov´ asz local lemma based method.

5.2

Lov´ asz local lemma based two-stage bound

We can apply the techniques from Section 5.1 to obtain a two-stage bound similar to Theorem 2 using the Lov´ asz local lemma and conditional LLL distribution. First we extend a result from [33]. Theorem 7. Let t, k, v be integers with k  t  2, v  2 and let  =
v log 
t vt v t -1

k t

, and  =

k t

-

k -t t

. If

 v t Then log CAN(t, k, v ) 
k t

+ t log v + log log log
vt v t -1

vt v t -1

+2

 - . 

Proof. Let M  [v ]t be a set of m t-tuples of symbols. Following the arguments of Section 5.1, when ) ] n  log(em there exists an n × k array A over [v ] such that for all C  [k t , AC covers all m tuples in M . vt
log
v t -1

At most  (v t - m) interactions are uncovered in such an array. Using the conditional LLL distribution, 1 n the probability that one such interaction is not covered in A is at most e 1 - v . Therefore, by the t 26

n - number of rows in partial array with vt missing interactions

550 500 450 400 350 300 250 200 150 100 50 Randomized (Algorithm 4) LLL based 200 400 600 k 800 1000 1200

n - number of rows in partial array with vt missing interactions

2500

2000

1500

1000

500

Randomized (Algorithm 4) LLL based

0

0

0

500

1000

1500

2000 k

2500

3000

3500

4000

4500

(a) t = 3, v = 3.

(b) t = 4, v = 3.

Figure 12: Comparison of the size of the partial array constructed in the first stage. Compares the size of the partial array specified in Algorithm 4 in Section 3.1, and the size derived in Section 5.1.
v 1 = linearity of expectation, we can find one such array A that leaves at most e (v t - m) 1 - v t  m -1 interactions uncovered. Adding one row per uncovered interactions to A, we obtain a covering array with at most N rows, where log(em)  vt N= + -1 t  m log tv v -1 v t log 
vt v t -1

n

t

The value of N is minimized when m =

. Because m  v t , we obtain the desired bound.

When m = v t this recaptures the bound of Theorem 5. Figure 13 compares the LLL based two-stage bound from Theorem 7 to the standard two-stage bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. Although the LLL based two-stage bound is tighter than the LLL based Godbole et al. bound, even for quite large values of k the standard two-stage bound is tighter than the LLL based two-stage bound. In practical terms, this specific LLL based two-stage method does not look very promising, unless the parameters are quite large.

6

Conclusion and open problems

Many concrete algorithms within a two-stage framework for covering array construction have been introduced and evaluated. The two-stage approach extends the range of parameters for which competitive covering arrays can be constructed, each meeting an a priori guarantee on its size. Indeed as a consequence a number of best known covering arrays have been improved upon. Although each of the methods proposed has useful features, our experimental evaluation suggests that TS Rand, Greedy; 2,  and TS Rand, Den; 2. with   {Cyclic, Frobenius} realize a good trade-off between running time and size of the constructed covering array. Improvements in the bounds, or in the algorithms that realize them, are certainly of interest. We mention some specific directions. Establishing tighter bounds on the coloring based methods of Section 3.3 is a challenging problem. Either better estimates of the chromatic number of the incompatibility graph after a random first stage, or a first stage designed to limit the chromatic number, could lead to improvements in the bounds. In Section 5.1 and 5.2 we explored using a Moser-Tardos type algorithm for the first stage. Although this is useful for asymptotic bounds, practical improvements appear to be limited. Perhaps a different approach of 27

550 500 450 N - number of rows N - number of rows SLJ bound Godbole LLL-2-stage 2-stage 400 350 300 250 200 150 100

10000 9000 8000 7000 6000 5000 4000 3000 2000 1000 0

SLJ bound Godbole LLL-2-stage 2-stage

0

50

100

150

200 k

250

300

350

400

450

1000

2000

3000

4000 k

5000

6000

7000

8000

(a) t = 3, v = 3.

(b) t = 4, v = 4.

Figure 13: Comparison among the LLL based two-stage bound from Theorem 7, the standard two-stage bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. reducing the number of bad events to be avoided explicitly by the algorithm may lead to a better algorithm. A potential approach may look like following: "Bad" events would denote non-coverage of an interaction over a t-set of columns. We would select a set of column t-sets such that the dependency graph of the corresponding bad events have a bounded maximum degree (less than the original dependency graph). We would devise a Moser-Tardos type algorithm for covering all the interactions on our chosen column t-sets, and then apply the conditional LLL distribution to obtain an upper bound on the number of uncovered interactions. However, the difficulty lies in the fact that "all vertices have degree  " is a non-trivial, "hereditary" property for induced subgraphs, and for such properties finding a maximum induced subgraph with the property is an NP-hard optimization problem [17]. There is still hope for a randomized or "nibble" like strategy that may find a reasonably good induced subgraph with a bounded maximum degree. Further exploration of this idea seems to be a promising research avenue. In general, one could consider more than two stages. Establishing the benefit (or not) of having more than two stages is also an interesting open problem. Finally, the application of the methods developed to mixed covering arrays appears to provide useful techniques for higher strengths; this merits further study as well.

Acknowledgments
The research was supported in part by the National Science Foundation under Grant No. 1421058.

References
[1] A. N. Aldaco, C. J. Colbourn, and V. R. Syrotiuk. Locating arrays: A new experimental design for screening complex engineered systems. SIGOPS Oper. Syst. Rev., 49(1):31­40, Jan. 2015. [2] N. Alon and J. H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics and Optimization. John Wiley & Sons, Inc., Hoboken, NJ, third edition, 2008. With an appendix on the life and work of Paul Erd os. [3] Android. Android configuration class, 2016. http://developer.android.com/reference/android/content/ res/Configuration.html. [4] J. I. Brown. The complexity of generalized graph colorings. Discrete Appl. Math., 69(3):257­270, 1996. 28

[5] R. C. Bryce and C. J. Colbourn. The density algorithm for pairwise interaction testing. Software Testing, Verification, and Reliability, 17:159­182, 2007. [6] R. C. Bryce and C. J. Colbourn. A density-based greedy algorithm for higher strength covering arrays. Software Testing, Verification, and Reliability, 19:37­53, 2009. [7] R. C. Bryce and C. J. Colbourn. Expected time to detection of interaction faults. Journal of Combinatorial Mathematics and Combinatorial Computing, 86:87­110, 2013. [8] J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE Global Research Technical Report, 29:769­781, 2002. [9] M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength 3. Des. Codes Crypt., 16:235­242, 1999. [10] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG system: An approach to testing based on combinatorial design. IEEE Transactions on Software Engineering, 23:437­44, 1997. [11] M. B. Cohen. Designing test suites for software interaction testing. PhD thesis, The University of Auckland, Department of Computer Science, 2004. [12] C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121­167, 2004. [13] C. J. Colbourn. Covering array tables, 2005-2015. http://www.public.asu.edu/ccolbou/src/tabby. [14] C. J. Colbourn. Covering arrays and hash families. In Information Security and Related Combinatorics, NATO Peace and Information Security, pages 99­136. IOS Press, 2011. [15] C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal of Combinatorial Mathematics and Combinatorial Computing, 90:97­115, 2014. [16] R. Diestel. Graph Theory. Graduate Texts in Mathematics. Springer, fourth edition, 2010. [17] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman & Co., New York, NY, USA, 1979. [18] A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds and Poisson approximations. Combinatorics, Probability and Computing, 5:105­118, 1996. [19] B. Haeupler, B. Saha, and A. Srinivasan. New constructive aspects of the Lov´ asz local lemma. J. ACM, 58(6):Art. 28, 28, 2011. [20] A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal Arrays. Springer-Verlag, New York, 1999. [21] D. S. Johnson. Approximation algorithms for combinatorial problems. J. Comput. System Sci., 9:256­ 278, 1974. [22] G. O. H. Katona. Two applications (for search theory and truth functions) of Sperner type theorems. Periodica Math., 3:19­26, 1973. [23] D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math., 6:255­262, 1973. [24] D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC Press, 2013. [25] D. R. Kuhn and M. Reilly. An investigation of the applicability of design of experiments to software testing. In Proc. 27th Annual NASA Goddard/IEEE Software Engineering Workshop, pages 91­95, Los Alamitos, CA, 2002. IEEE.

29

[26] D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault interactions and implications for software testing. IEEE Trans. Software Engineering, 30:418­421, 2004. [27] L. Lov´ asz. On the ratio of optimal integral and fractional covers. Discrete Math., 13(4):383­390, 1975. [28] K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin. Des., 13:70­77, 2005. [29] R. A. Moser. A constructive proof of the Lov´ asz local lemma. In STOC'09--Proceedings of the 2009 ACM International Symposium on Theory of Computing, pages 343­350. ACM, New York, 2009. [30] R. A. Moser and G. Tardos. A constructive proof of the general Lov´ asz local lemma. J. ACM, 57(2):Art. 11, 15, 2010. [31] C. Nie and H. Leung. A survey of combinatorial testing. ACM Computing Surveys, 43(2):#11, 2011. [32] A. H. Ronneseth and C. J. Colbourn. Merging covering arrays and compressing multiple sequence alignments. Discrete Appl. Math., 157:2177­2190, 2009. [33] K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv e-prints. [34] G. Seroussi and N. H. Bshouty. Vector sets for exhaustive testing of logic circuits. IEEE Trans. Inform. Theory, 34:513­522, 1988. [35] S. K. Stein. Two combinatorial covering theorems. J. Combinatorial Theory Ser. A, 16:391­397, 1974. [36] J. Torres-Jimenez, H. Avila-George, and I. Izquierdo-Marquez. A two-stage algorithm for combinatorial testing. Optimization Letters, pages 1­13, 2016.

30

Two-stage algorithms for covering array construction
Kaushik Sarkar and Charles J. Colbourn School of Computing, Informatics, and Decision Systems Engineering Arizona State University, PO Box 878809 Tempe, Arizona, 85287-8809, U.S.A.

arXiv:1606.06730v1 [cs.DS] 21 Jun 2016

June 23, 2016
Abstract Modern software systems often consist of many different components, each with a number of options. Although unit tests may reveal faulty options for individual components, functionally correct components may interact in unforeseen ways to cause a fault. Covering arrays are used to test for interactions among components systematically. A two-stage framework, providing a number of concrete algorithms, is developed for the efficient construction of covering arrays. In the first stage, a time and memory efficient randomized algorithm covers most of the interactions. In the second stage, a more sophisticated search covers the remainder in relatively few tests. In this way, the storage limitations of the sophisticated search algorithms are avoided; hence the range of the number of components for which the algorithm can be applied is extended, without increasing the number of tests. Many of the framework instantiations can be tuned to optimize a memory-quality trade-off, so that fewer tests can be achieved using more memory. The algorithms developed outperform the currently best known methods when the number of components ranges from 20 to 60, the number of options for each ranges from 3 to 6, and t-way interactions are covered for t  {5, 6}. In some cases a reduction in the number of tests by more than 50% is achieved.

Keywords: Covering array, Software interaction testing, Combinatorial construction algorithm

1

Introduction

Real world software and engineered systems are composed of many different components, each with a number of options, that are required to work together in a variety of circumstances. Components are factors, and options for a component form the levels of its factor. Although each level for an individual factor can be tested in isolation, faults in deployed software can arise from interactions among levels of different factors. When an interaction involves levels of t different factors, it is a t-way interaction. Testing for faults caused by t-way interactions for every t is generally infeasible, as a result of a combinatorial explosion. However, empirical research on real world software systems indicates that testing all possible 2-way (or 3-way) interactions would detect 70% (or 90%) of all faults [25]. Moreover, testing all possible 6-way interactions is sufficient for detection of 100% of all faults in the systems examined in [25]. Testing all possible t-way interactions for some 2  t  6 is pseudo-exhaustive testing [24], and is accomplished with a combinatorial array known as a covering array. Formally, let N, t, k, and v be integers with k  t  2 and v  2. A covering array CA(N ; t, k, v ) is an N × k array A in which each entry is from a v -ary alphabet , and for every N × t sub-array B of A and every x  t , there is a row of B that equals x. Then t is the strength of the covering array, k is the number of factors, and v is the number of levels. When k is a positive integer, [k ] denotes the set {1, . . . , k }. A t-way interaction is {(ci , ai ) : 1  i  t, ci  [k ], ci = cj for i = j, and ai  }. So an interaction is an assignment of levels from  to t of the k t factors. It,k,v denotes the set of all k t v interactions for given t, k and v . An N × k array A covers the 1

interaction  = {(ci , ai ) : 1  i  t, ci  [k ], ci = cj for i = j, and ai  } if there is a row r in A such that A(r, ci ) = ai for 1  i  t. When there is no such row in A,  is not covered in A. Hence a CA(N ; t, k, v ) covers all interactions in It,k,v . Covering arrays are used extensively for interaction testing in complex engineered systems. To ensure that all possible combinations of options of t components function together correctly, one needs examine all possible t-way interactions. When the number of components is k , and the number of different options available for each component is v , each row of CA(N ; t, k, v ) represents a test case. The N test cases collectively test all t-way interactions. For this reason, covering arrays have been used in combinatorial interaction testing in varied fields like software and hardware engineering, design of composite materials, and biological networks [8, 24, 26, 32, 34]. The cost of testing is directly related to the number of test cases. Therefore, one is interested in covering arrays with the fewest rows. The smallest value of N for which CA(N ; t, k, v ) exists is denoted by CAN(t, k, v ). Efforts to determine or bound CAN(t, k, v ) have been extensive; see [12, 14, 24, 31] for example. Naturally one would prefer to determine CAN(t, k, v ) exactly. Katona [22] and Kleitman and Spencer [23] independently showed that for t = v = 2, the minimum number of rows N in a CA(N ; 2, k, 2) is the smallest N for which -1 . Exact determination of CAN(t, k, v ) for other values of t and v has remained open. However, k  NN 2 some progress has been made in determining upper bounds for CAN(t, k, v ) in the general case; for recent results, see [33]. For practical applications such bounds are often unhelpful, because one needs explicit covering arrays to use as test suites. Explicit constructions can be recursive, producing larger covering arrays using smaller ones as ingredients (see [14] for a survey), or direct. Direct methods for some specific cases arise from algebraic, geometric, or number-theoretic techniques; general direct methods are computational in nature. Indeed when k is relatively small, the best known results arise from computational techniques [13], and these are in turn essential for the successes of recursive methods. Unfortunately, the existing computational methods encounter difficulties as k increases, but is still within the range needed for practical applications. Typically such difficulties arise either as a result of storage or time limitations or by producing covering arrays that are too big to compete with those arising from simpler recursive methods. Cohen [11] discusses commercial software where the number of factors often exceeds 50. Aldaco et al. [1] analyze a complex engineered system having 75 factors, using a variant of covering arrays. Android [3] uses a Configuration class to describe the device configuration; there are 17 different configuration parameters with 3 - 20 different levels. In each of these cases, while existing techniques are effective when the strength is small, these moderately large values of k pose concerns for larger strengths. In this paper, we focus on situations in which every factor has the same number of levels. These cases have been most extensively studied, and hence provide a basis for making comparisons. In practice, however, often different components have different number of levels, which is captured by extending the notion of a covering array. A mixed covering array MCA(N ; t, k, (v1 , v2 , . . . , vk )) is an N × k array in which the ith column contains vi symbols for 1  i  k . When {i1 , . . . , it }  {1, . . . , k } is a set of t columns, in the N × t t subarray obtained by selecting columns i1 , . . . , it of the MCA, each of the j =1 vij distinct t-tuples appears as a row at least once. Although we examine the uniform case in which v1 = · · · = vk , the methods developed here can all be directly applied to mixed covering arrays as well. Inevitably, when k > max(t + 1, v + 2), a covering array must cover some interactions more than once, for if not they are orthogonal arrays [20]. Treating the rows of a covering array in a fixed order, each row covers some number of interactions not covered by any earlier row. For a variety of known constructions, the initial rows cover many new interactions, while the later ones cover very few [7]. Comparing this rate of coverage for a purely random method and for one of the sophisticated search techniques, one finds little difference in the initial rows, but very substantial differences in the final ones. This suggests strategies to build the covering array in stages, investing more effort as the number of remaining uncovered interactions declines. In this paper we propose a new algorithmic framework for covering array construction, the two-stage framework. In the first stage, a randomized row construction method builds a specified number of rows to cover all but at most a specified, small number of interactions. As we see later, by dint of being randomized

2

this uses very little memory. The second stage is a more sophisticated search that adds few rows to cover the remaining uncovered interactions. We choose search algorithms whose requirements depend on the number of interactions to be covered, to profit from the fact that few interactions remain. By mixing randomized and deterministic methods, we hope to retain the fast execution and small storage of the randomized methods, along with the accuracy of the deterministic search techniques. We introduce a number of algorithms within the two-stage framework. Some improve upon best known bounds on CAN(t, k, v ) (see [33]) in principle. But our focus is on the practical consequences: The twostage algorithms are indeed quite efficient for higher strength (t  {5, 6}) and moderate number of levels (v  {3, 4, 5, 6}), when the number of factors k is moderately high (approximately in the range of 20 - 80 depending on value of t and v ). In fact, for many combination of t, k and v values the two-stage algorithms beat the previously best known bounds. Torres-Jimenez et al. [36] explore a related two-stage strategy. In their first stage, an in-parameter-order greedy strategy (as used in ACTS [24]) adds a column to an existing array; in their second stage, simulated annealing is applied to cover the remaining interactions. They apply their methods when t = v = 3, when the storage and time requirements for both stages remain acceptable. In addition to the issues in handling larger strengths, their methods provide no a priori bound on the size of the resulting array. In contrast with their methods, ours provide a guarantee prior to execution with much more modest storage and time. The rest of the paper is organized as follows. Section 2 reviews algorithmic methods of covering array construction, specifically the randomized algorithm and the density algorithm. This section contrasts these two methods and points out their limitations. Then it gives an intuitive answer to the question of why a two stage based strategy might work and introduces the general two-stage framework. Section 3 introduces some specific two-stage algorithms. Section 3.1 analyzes and evaluates the na¨ ive strategy. Section 3.2 describes a two-stage algorithm that combines the randomized and the density algorithm. Section 3.3 introduces graph coloring based techniques in the second stage. Section 3.4 examines the effect of group action on the size of the constructed covering arrays. Section 4 compares the results of various two-stage algorithms with the presently best known sizes. In Section 5 we discuss the Lov´ asz local lemma (LLL) bounds on CAN(t, k, v ) and provide a Moser-Tardos type randomized algorithm for covering array construction that matches the bound. Although the bound was known [18], the proof was non-constructive, and a constructive algorithm to match this bound seems to be absent in the literature. We explore potentially better randomized algorithms for the first stage using LLL based techniques, We also obtain a two-stage bound that improves the LLL bound for CAN(t, k, v ). We conclude the paper in Section 6.

2

Algorithmic construction of covering arrays

Available algorithms for the construction of covering arrays are primarily heuristic in nature; indeed exact algorithms have succeeded for very few cases. Computationally intensive metaheuristic search methods such as simulated annealing, tabu search, constraint programming, and genetic algorithms have been employed when the strength is relatively small or the number of factors and levels is small. These methods have established many of the best known bounds on sizes of covering arrays [13], but for many problems of practical size their time and storage requirements are prohibitive. For larger problems, the best available methods are greedy. The IPO family of algorithms [24] repeatedly adds one column at a time, and then adds -1 new rows to ensure complete coverage. In this way, at any point in time, the status of v t k t-1 interactions may be stored. AETG [10] pioneered a different method, which greedily selects one row at a time to cover a large number of as-yet-uncovered interactions. They establish that if a row can be chosen that covers the maximum number, a good a priori bound on the size of the covering array can be computed. Unfortunately selecting the maximum is NP-hard, and even if one selects the maximum there is no guarantee that the covering array is the smallest possible [7], so AETG resorts to a good heuristic selection of the next row by examining the stored status of v t k t interactions. None of the methods so far mentioned therefore guarantee to reach an a priori bound. An extension of the AETG strategy, the density algorithm [5, 6, 15], stores additional statistics for all v t k t interactions in order to ensure the selection of a good next row, and hence guarantees to produce an array with at most the precomputed number of rows. Variants of the density 3

Algorithm 1: A randomized algorithm for covering array construction. Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) log (k t )+t log v 1 Set N := ; vt
log
v t -1

2 3

4 5 6 7 8 9 10 11 12

repeat Construct an N × k array A where each entry is chosen independently and uniformly at random from a v -ary alphabet; Set covered := true; for each interaction   It,k,v do if  is not covered in A then Set covered := false; break; end end until covered = true ; Output A;

algorithm have proved to be most effective for problems of moderately large size. For even larger problems, pure random approaches have been applied. To produce methods that provide a guarantee on size, it is natural to focus on the density algorithm in order to understand its strengths and weaknesses. To do this, we contrast it with a basic randomized algorithm. Algorithm 1 shows a simple randomized algorithm for covering array construction. The algorithm constructs an array of a particular size randomly and checks whether all the interactions are covered. It repeats until it finds an array that covers all the interactions. log (k t )+t log v A CA(N ; t, k, v ) with N = is guaranteed to exist: vt
log
v t -1

Theorem 1. [21, 27, 35] (Stein-Lov´ asz-Johnson (SLJ) bound): Let t, k, v be integers with k  t  2, and v  2. Then as k  , CAN(t, k, v )  log
k t

+ t log v
vt v t -1

log

In fact, the probability that the N × k array constructed in line 3 of Algorithm 1 is a valid covering array is high enough that the expected number of times the loop in line 2 is repeated is a small constant. An alternative strategy is to add rows one by one instead of constructing the full array at the outset. We start with an empty array, and whenever we add a new row we ensure that it covers at least the expected number of previously uncovered interactions for a randomly chosen row. The probability that an uncovered interaction is covered by a random row is 1/v t . If the number of uncovered interactions is u, then by linearity of expectation, the expected number of newly covered interactions in a randomly chosen row is uv -t . If each row added covers exactly this expected number, we obtain the same number of rows as the SLJ bound, realized in Algorithm 1. But because the actual number of newly covered interactions is always an integer, each added row covers at least uv -t interactions. This is especially helpful towards the end when the expected number is a small fraction. Algorithm 2 follows this strategy. Again the probability that a randomly chosen row covers at least the expected number of previously uncovered interactions is high enough that the expected number of times the row selection loop in line 6 of Algorithm 2 is repeated is bounded by a small constant. We can obtain an upper bound on the size produced by Algorithm 2 by assuming that each new row added covers exactly uv -t previously uncovered interactions. This bound is the discrete Stein-Lov´ asz4

Algorithm 2: A randomized algorithm for covering array construction using the discrete SLJ strategy. Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) 1 Let A be an empty array; k t 2 Initialize a table T indexed by all t v interactions, marking every interaction "uncovered" ; 3 while there is an interaction marked "uncovered" in T do 4 Let u be the number of interactions marked "uncovered" in T ; u 5 Set expectedCoverage := v t ; 6 repeat 7 Let r be a row of length k where each entry is chosen independently and uniformly at random from a v -ary alphabet; 8 Let coverage be the number of "uncovered" interactions in T that are covered in row r; 9 until coverage > expectedCoverage ; 10 Add r to A; 11 Mark all interactions covered by r as "covered" in T ; 12 end 13 Output A;

Johnson (discrete SLJ) bound. Figure 1 compares the sizes of covering arrays obtained from the SLJ and the discrete SLJ bounds for different values of k when t = 6 and v = 3. Consider a concrete example, when t = 5, k = 20, and v = 3. The SLJ bound guarantees the existence of a covering array with 12499 rows, whereas the discrete SLJ bound guarantees the existence of a covering array with only 8117 rows. The density algorithm replaces the loop at line 6 of Algorithm 2 by a conditional expectation derandomized method. For fixed v and t the density algorithm selects a row efficiently (time polynomial in k ) and deterministically that is guaranteed to cover at least uv -t previously uncovered interactions. In practice, for small values of k the density algorithm works quite well, often covering many more interactions than the minimum. Many of the currently best known CAN(t, k, v ) upper bounds are obtained by the density algorithm in combination with various post-optimization techniques [13]. However, the practical applicability of Algorithm 2 and the density algorithm is limited by the storage t of the table T , representing each of the k t v interactions. Even when t = 6, v = 3, and k = 54, there are 18,828,003,285 6-way interactions. This huge memory requirement renders the density algorithm impractical for rather small values of k when t  {5, 6} and v  3. We present an idea to circumvent this large requirement for memory, and develop it in full in Section 3.

2.1

Why does a two stage based strategy make sense?

Compare the two extremes, the density algorithm and Algorithm 1. On one hand, Algorithm 1 does not suffer from any substantial storage restriction, but appears to generate many more rows than the density algorithm. On the other hand, the density algorithm constructs fewer rows for small values of k , but becomes impractical when k is moderately large. One wants algorithms that behave like Algorithm 1 in terms of memory, but yield a number of rows competitive with the density algorithm. For t = 6, k = 16, and v = 3, Figure 2 compares the coverage profile for the density algorithm and Algorithm 1. We plot the number of newly covered interactions for each row in the density algorithm, and the expected number of newly covered interactions for each row for Algorithm 1. The qualitative features exhibited by this plot are representative of the rates of coverage for other parameters. Two key observations are suggested by Figure 2. First, the expected coverage in the initial random rows is similar to the rows chosen by the density algorithm. In this example, the partial arrays consisting of the first 1000 rows exhibit similar coverage, yet the randomized algorithm needed no extensive bookkeeping. Secondly, as later rows are added, the judicious selections of the density algorithm produce much larger coverage per row than Algorithm 1. Consequently it appears sensible to invest few computational resources 5

3.5

x 10

4

SLJ bound Discrete SLJ bound 3

N - number of rows

2.5

2

1.5

1

0.5 0

100

200

300

400

500 k

600

700

800

900

1000

Figure 1: Comparison of covering array sizes obtained from SLJ bound and discrete SLJ bound for different values of k , when t = 6 and v = 3.

9000 8000 Number of newly covered interactions 7000 6000 5000 4000 3000 2000 1000 0 Density Basic Random

0

2000

4000

6000 Row number

8000

10000

12000

Figure 2: For t = 6, k = 16 and v = 3, the actual number of newly covered interactions of the density algorithm and the expected number of newly covered interactions in a random array.

6

on the initial rows, while making more careful selections in the later ones. This forms the blueprint of our general two-stage algorithmic framework shown in Algorithm 3. Algorithm 3: The general two-stage framework for covering array construction. Input: t : strength of the required covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) 1 Choose a number n of rows and a number  of interactions; // First Stage 2 Use a randomized algorithm to construct an n × k array A ; 3 Ensure that A covers all but at most  interactions; 4 Make a list L of interactions that are not covered in A (L contains at most  interactions); // Second Stage 5 Use a deterministic procedure to add N - n rows to A to cover all the interactions in L; 6 Output A; A specific covering array construction algorithm results by specifying the randomized method in the first stage, the deterministic method in the second stage, and the computation of n and . Any such algorithm produces a covering array, but we wish to make selections so that the resulting algorithms are practical while still providing a guarantee on the size of the array. In Section 3 we describe different algorithms from the two-stage family, determine the size of the partial array to be constructed in the first stage, and establish upper bound guarantees. In Section 4 we explore how good the algorithms are in practice.

3

Two-stage framework
Rand MT the basic randomized algorithm the Moser-Tardos type algorithm

For the first stage we consider two methods:

We defer the development of method MT until Section 5. Method Rand uses a simple variant of Algorithm 1, choosing a random n × k array. For the second stage we consider four methods: Naive Greedy Den Col the the the the na¨ ive strategy, one row per uncovered interaction online greedy coloring strategy density algorithm graph coloring algorithm

Using these abbreviations, we adopt a uniform naming convention for the algorithms: TS A, B is the algorithm in which A is used in the first stage, and B is used in the second stage. For example, TS MT, Greedy denotes a two-stage algorithm where the first stage is a Moser-Tardos type randomized algorithm and the second stage is a greedy coloring algorithm. Later when the need arises we refine these algorithm names.

3.1

One row per uncovered interaction in the second stage (TS Rand, Naive )

In the second stage each of the uncovered interactions after the first stage is covered using a new row. Algorithm 4 describes the method in more detail. This simple strategy improves on the basic randomized strategy when n is chosen judiciously. For example, when t = 6, k = 54 and v = 3, Algorithm 1 constructs a covering array with 17, 236 rows. Figure 3 plots an upper bound on the size of the covering array against the number n of rows in the partial array.

7

Algorithm 4: Na¨ ive two-stage algorithm (TS Rand, Naive ). Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) vt log (k t )+t log v +log log v t -1 ; 1 Let n := t v
log
v t -1

2 3 4

Let  =

1 log
vt v t -1

;

5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

repeat Let A be an n × k array where each entry is chosen independently and uniformly at random from a v -ary alphabet; Let uncovNum := 0 and unCovList be an empty list of interactions; Set covered := true; for each interaction   It,k,v do if  is not covered in A then Set uncovNum :=uncovNum +1; Add  to unCovList ; if uncovNum >  then Set covered := false; break; end end end until covered= true ; for each interaction  uncovList do Add a row to A that covers ; end Output A;

8

1.75 1.7 1.65 1.6 1.55 1.5 1.45 1.4 1.35 1.3

x 10

4

Total number of rows in the Covering array

1

1.1 1.2 1.3 1.4 1.5 1.6 1.7 n -- number of rows in the partial array of the first stage

1.8 x 10
4

Figure 3: An upper bound on the size of the covering array against n, the size of the partial array constructed in the first stage when t = 6, k = 54, and v = 3, with one new row added per uncovered interaction in the second stage. The minimum size of 13, 162 is obtained when n = 12, 402. Algorithm 1 requires 17, 236 rows, and the currently best known covering array has 17, 197 rows.

9

The smallest covering array is obtained when n = 12, 402 which, when completed, yields a covering array with at most 13, 162 rows--a big improvement over Algorithm 1. A theorem from [33] tells us the optimal value of n in general: Theorem 2. [33] Let t, k, v be integers with k  t  2, and v  2. Then log CAN(t, k, v ) 
k t

+ t log v + log log log
vt v t -1
vt v t -1

vt v t -1

+1 .

The bound is obtained by setting n =
t

log (k t )+t log v +log log log
vt v t -1

. The expected number of uncovered

interactions is exactly  = 1/ log vtv-1 . Figure 4 compares SLJ, discrete SLJ and two-stage bounds for k  100, when t = 6 and v = 3. The two-stage bound does not deteriorate in comparison to discrete SLJ bound as k increases; it consistently takes only 307-309 more rows. Thus when k = 12 the two-stage bound requires only 6% more rows and when k = 100 only 2% more rows than the discrete SLJ bound.
x 10
4

2.2 2 1.8 N - number of rows 1.6 1.4 1.2 1 0.8 0.6

SLJ bound Discrete SLJ bound Two-stage bound

0.4 10

20

30

40

50 k

60

70

80

90

100

Figure 4: Comparison of covering array sizes obtained from SLJ bound, discrete SLJ bound and two-stage bound for k  100, when t = 6 and v = 3. In this range of k values the two-stage bound requires 307-309 more rows than the discrete SLJ bound, that is, 2-6% more rows. To ensure that the loop in line 7 of Algorithm 4 does not repeat too many times we need to know the probability with which a random n × k array leaves at most  interactions uncovered. Using Chebyshev's inequality and the second moment method developed in [2, Chapter 4], we next show that in a random n × k 1 n t array the number of uncovered interactions is almost always close to its expectation, i.e. k . t v 1 - vt Substituting the value of n from line 1, this expected value is equal to µ, as in line 2. Therefore, the probability that a random n × k array covers the desired number of interactions is constant, and the expected number of times the loop in line 7 is repeated is also a constant (around 2 in practice).

10

Because the theory of the second moment method is developed in considerable detail in [2], here we briefly m mention the relevant concepts and results. Suppose that X = i=1 Xi , where Xi is the indicator random variable for event Ai for 1  i  m. For indices i, j , we write i  j if i = j and the events Ai , Aj are not independent. Also suppose that X1 , . . . , Xm are symmetric, i.e. for every i = j there is a measure preserving mapping of the underlying probability space that sends event Ai to event Aj . Define  = j i Pr [Aj |Ai ]. Then by [2, Corollary 4.3.4]: Lemma 3. [2] If E[X ]   and  = o(E[X ]) then X  E[X ] almost always. In our case, Ai denotes the event that the ith interaction is not covered in a n × k array where each entry 1 n . Because is chosen independently and uniformly at random from a v -ary alphabet. Then Pr[Xi ] = 1 - v t k t 1 n t , and E [ X ]   as there are k v interactions in total, by linearity of expectation, E [ X ] = v 1 - t t vt k  . Distinct events Ai and Aj are independent if the ith and j th interactions share no column. Therefore, k  the event Ai is not independent of at most t t- j i Pr [Aj |Ai ]  j i 1  1 other events Aj . So  = k t t-1 = o(E[X ]) when v and t are constants. By Lemma 3, the number of uncovered interactions in a random n × k array is close to the expected number of uncovered interactions. This guarantees that Algorithm 4 is an efficient randomized algorithm for constructing covering arrays with a number of rows upper bounded by Theorem 2. In keeping with the general two-stage framework, Algorithm 4 does not store the coverage status of each interaction. We only need store the interactions that are uncovered in A, of which there are at most 1  =  v t . This quantity depends only on v and t and is independent of k , so is effectively a vt
log
v t -1

t constant that is much smaller than k t v , the storage requirement for the density algorithm. Hence the algorithm can be applied to a higher range of k values. Although Theorem 5 provides asymptotically tighter bounds than Theorem 2, in a range of k values that are relevant for practical application, Theorem 2 provides better results. Figure 5 compares the bounds on CAN(t, k, v ) with the currently best known results.
4 6

3

x 10

2.5 Best known Two-stage (simple) GSS bound 2

x 10

2.5

Best known Two-stage (simple) GSS bound

N - number of rows

N - number of rows 10 20 30 40 k 50 60 70 80 90

2

1.5

1.5

1

1

0.5

0.5

0 0

0

5

10

15

20

25 k

30

35

40

45

50

(a) t = 6, v = 3

(b) t = 6, v = 6

Figure 5: Comparison of GSS bound and two-stage bound with the currently best known results

3.2

The density algorithm in the second stage (TS Rand, Den )

Next we apply the density algorithm in the second stage. Figure 6 plots an upper bound on the size of the covering array against the size of the partial array constructed in the first stage when the density algorithm is used in the second stage, and compares it with TS Rand, Naive . The size of the covering array decreases 11

as n decreases. This is expected because with smaller partial arrays, more interactions remain for the second stage to be covered by the density algorithm. In fact if we cover all the interactions using the density algorithm (as when n = 0) we would get an even smaller covering array. However, our motivation was precisely to avoid doing that. Therefore, we need a "cut-off" for the first stage.
x 10
4

1.9

Total number of rows in the Covering array

1.8

Basic Two-stage Two-stage with density in second stage

1.7

1.6

1.5

1.4

1.3

1.2

1

1.1 1.2 1.3 1.4 1.5 1.6 1.7 n -- number of rows in the partial array of the first stage

1.8 x 10
4

Figure 6: Comparison of covering array sizes from two-stage algorithms with Den and Naive in the second stage. With Den there is no minimum point in the curve; the size of the covering array keeps decreasing as we leave more uncovered interactions for the second stage. We are presented with a trade-off. If we construct a smaller partial array in the first stage, we obtain a smaller covering array overall. But we then pay for more storage and computation time for the second stage. To appreciate the nature of this trade-off, look at Figure 7, which plots an upper bound on the covering array size and the number of uncovered interactions in the first stage against n. The improvement in the covering array size plateaus after a certain point. The three horizontal lines indicate  ( v t ), 2 and 3 uncovered interactions in the first stage. (In the na¨ ive method of Section 3.1, the partial array after the first stage leaves at most  uncovered interactions.) In Figure 7 the final covering array size appears to plateau when the number of uncovered interactions left by the first stage is around 2. After that we see diminishing returns -- the density algorithm needs to cover more interactions in return for a smaller improvement in the covering array size. Let r be the maximum number of interactions allowed to remain uncovered after the first stage. Then r can be specified in the two-stage algorithm. To accommodate this, we denote by TS A, B ; r the two-stage algorithm where A is the first stage strategy, B is the second stage strategy, and r is the maximum number of uncovered interactions after the first stage. For example, TS Rand, Den; 2 applies the basic randomized algorithm in the first stage to cover all but at most 2 interactions, and the density algorithm to cover the remaining interactions in the second stage.

3.3

Coloring in the second stage (TS Rand, Col and TS Rand, Greedy )

Now we describe strategies using graph coloring in the second stage. Construct a graph G = (V, E ), the incompatibility graph, in which V is the set of uncovered interactions, and there is an edge between two 12

18000 Number of rows / Number of uncovered interactions 16000 14000 12000 10000 8000 6000 4000 2000 0 0.8 Num. of rows in the completed CA Num. of uncovered interaction in first stage

0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 4 n -- number of rows in the partial array of the first stage x 10

Figure 7: Final covering array size against the number of uncovered interactions after the first stage. As the size n of the partial array decreases, the number of uncovered interactions in the first stage increases. Den is used in the second stage. From bottom to top, the green lines denote , 2, and 3 uncovered interactions. interactions exactly when they share a column in which they have different symbols. A single row can cover a set of interactions if and only if it forms an independent set in G. Hence the minimum number of rows required to cover all interactions of G is exactly its chromatic number (G), the minimum number of colors in a proper coloring of G. Graph coloring is an NP-hard problem, so we employ heuristics to bound the chromatic number. Moreover, G only has vertices for the uncovered interactions after the first stage, so is size is small relative to the total number of interactions. The expected number of edges in the incompatibility graph after choosing n rows uniformly at random n t k t t k -t 1 1 n t t-i is  = 2 ) 1- v . Using the elementary upper bound on 1 - (vt -1 t i=1 i t-i (v - v t v v t-i ) the chromatic number  
1 2

+

2m + 1 4 , where m is the number of edges [16, Chapter 5.2], we can surely

cover the remaining interactions with at most 1 2m + 1 2 + 4 rows. The actual number of edges m that remain after the first stage is a random variable with mean  . In principle, the first stage could be repeatedly applied until m   , so we call m =  the optimistic estimate. To ensure that the first stage is expected to be run a small constant number of times, we increase the estimate. With probability more than 1/2 the incompatibility graph has m  2 edges, so m = 2 is the conservative estimate. For t = 6, k = 56, and v = 3, Figure 8 shows the effect on the minimum number of rows when the bound on the chromatic number in the second stage is used, for the conservative or optimistic estimates. The Na¨ ive method is plotted for comparison. Better coloring bounds shift the minima leftward, reducing the number of rows produced in both stages. Thus far we have considered bounds on the chromatic number. Better estimation of (G) is complicated by the fact that we do not have much information about the structure of G until the first stage is run. In practice, however, G is known after the first stage and hence an algorithmic method to bound its chromatic number can be applied. Because the number of vertices in G equals the number of uncovered interactions

13

2.2

x 10

4

2

Conservative estimate Optimistic estimate Simple

Number of rows required

1.8

1.6

1.4

1.2

1

1

1.1

1.2

1.3

1.4 N

1.5

1.6

1.7

1.8 x 10
4

Figure 8: Size of the partial array vs. size of the complete CA. t = 6, k = 56, v = 3. Stein-Lov´ asz-Johnson bound requires 17, 403 rows, discrete Stein-Lov´ asz-Johnson bound requires 13, 021 rows. Simple estimate for the two stage algorithm is 13, 328 rows, conservative estimate assuming m = 2 is 12, 159 rows, and optimistic estimate assuming m =  is 11, 919 rows. Even the conservative estimate beats the discrete Stein-Lov´ asz-Johnson bound.

14

after the first stage, we encounter the same trade-off between time and storage, and final array size, as seen earlier for density. Hence we again parametrize by the expected number of uncovered interactions in the first stage. We employ two different greedy algorithms to color the incompatibility graph. In method Col we first construct the incompatibility graph G after the first stage. Then we apply the commonly used smallest last order heuristic to order the vertices for greedy coloring: At each stage, find a vertex vi of minimum degree in Gi , order the vertices of Gi - vi , and then place vi at the end. More precisely, we order the vertices of G as v1 , v2 , . . . , vn , such that vi is a vertex of minimum degree in Gi , where Gi = G - {vi+1 , . . . , vn }. A graph is d-degenerate if all of its subgraphs have a vertex with degree at most d. When G is d-degenerate but not (d - 1)-degenerate, the Coloring number col(G) is d + 1. If we then greedily color the vertices with the first available color, at most col(G) colors are used. In method Greedy we employ an on-line, greedy approach that colors the interactions as they are discovered in the first stage. In this way, the incompatibility graph is never constructed. We instead maintain a set of rows. Some entries in rows are fixed to a specific value; others are flexible to take on any value. Whenever a new interaction is found to be uncovered in the first stage, we check if any of the rows is compatible with this interaction. If such a row is found then entries in the row are fixed so that the row now covers the interaction. If no such row exists, a new row with exactly t fixed entries corresponding to the interaction is added to the set of rows. This method is much faster than method Col in practice.

3.4

Using group action

Covering arrays that are invariant under the action of a permutation group on their symbols can be easier to construct and are often smaller [15]. Direct and computational constructions using group actions are explored in [9, 28]. Sarkar et al. [33] establish the asymptotically tightest known bounds on CAN(t, k, v ) using group actions. In this section we explore the implications of group actions on two-stage algorithms. Let  be a permutation group on the set of symbols. The action of this group partitions the set of t-way interactions into orbits. We construct an array A such that for every orbit, at least one row covers an interaction from that orbit. Then we develop the rows of A over  to obtain a covering array that is invariant under the action of . Effort then focuses on covering all the orbits of t-way interactions, instead of the individual interactions. If  acts sharply transitively on the set of symbols (for example, if  is a cyclic group of order v ) then k t-1 t the action of  partitions k orbits of length v each. Following the lines of the t v interactions into t v v t-1 log (k t )+(t-1) log v +log log v t-1 -1 +1 that covers at proof of Theorem 2, there exists an n × k array with n = v t-1
log
v t-1 -1

least one interaction from each orbit. Therefore, log CAN(t, k, v )  v
k t

+ (t - 1) log v + log log log
v t- 1 v t- 1 - 1

v t-1 v t-1 -1

+1 . (1)

Similarly, we can employ a Frobenius group. When v is a prime power, the Frobenius group is the group of permutations of Fv of the form {x  ax + b : a, b  Fv , a = 0}. The action of the Frobenius group t-1 1 partitions the set of t-tuples on v symbols into v v-- 1 orbits of length v (v - 1) (full orbits) each and 1 orbit of length v (a short orbit). The short orbit consists of tuples of the form (x1 , . . . , xt ) where x1 = . . . = xt . Therefore, we can obtain a covering array by first constructing an array that covers all the full orbits, and then developing all the rows over the Frobenius group and adding v constant rows. Using the two stage strategy in conjunction with the Frobenius group action we obtain: log CAN(t, k, v )  v (v - 1)
k t

+ log

v t-1 -1 v -1

+ log log
v t- 1 v t-1 -v +1

v t-1 v t-1 -v +1

+1 + v. (2)

log

15

1.5

x 10

4

1.45

Two-stage (simple) Two-stage (cyclic group action) Two-stage (Frobenius group action)

N - number of rows

1.4

1.35

1.3

1.25 50

55

60 k

65

70

75

Figure 9: Comparison of the simple two-stage bound with the cyclic and the Frobenius two-stage bounds. t = 6, v = 3 and 50  k  75. Group action reduces the required number of rows slightly. Figure 9 compares the simple two-stage bound with the cyclic and Frobenius two-stage bounds. For t = 6, v = 3 and 12  k  100, the cyclic bound requires 7-21 (on average 16) fewer rows than the simple bound. In the same range the Frobenius bound requires 17 - 51 (on average 40) fewer rows. Group action can be applied in other methods for the second stage as well. Colbourn [15] incorporates group action into the density algorithm, allowing us to apply method Den in the second stage. Greedy extends easily to use group action, as we do not construct an explicit incompatibility graph. Whenever we fix entries in a row to cover an uncovered orbit, we commit to a specific orbit representative. However, applying group action to the incompatibility graph coloring for Col is more complicated. We need to modify the definition of the incompatibility graph for two reasons. First the vertices no longer represent uncovered interactions, but rather uncovered orbits of interaction. Secondly, and perhaps more importantly, pairwise compatibility between every two orbits in a set no longer implies mutual compatibility among all orbits in the set. One approach is to form a vertex for each uncovered orbit, placing an edge between two when they share a column. Rather than the usual coloring, however, one asks for a partition of the vertex set into classes so that every class induces an acyclic subgraph. Problems of this type are generalized graph coloring problems [4]. Within each class of such a vertex partition, consistent representatives of each orbit can be selected to form a row; when a cycle is present, this may not be possible. Unfortunately, heuristics for solving these types of problems appear to be weak, so we adopt a second approach. As we build the incompatibility graph, we commit to specific orbit representatives. When a vertex for an uncovered orbit is added, we check its compatibility with the orbit representatives chosen for the orbits already handled with which it shares columns; we commit to an orbit representative and add edges to those with which it is now incompatible. Once completed, we have a (standard) coloring problem for the resulting graph. Because group action can be applied using each of the methods for the two stages, we extend our naming to TS A, B ; r,  , where  can be Trivial (i.e. no group action), Cyclic, or Frobenius.

16

4

Computational results

Figure 5 indicates that even a simple two-stage bound can improve on best known covering array numbers. Therefore we investigate the actual performance of our two-stage algorithms for covering arrays of strength 5 and 6. First we present results for t = 6, when v  {3, 4, 5, 6} and no group action is assumed. Table 1 shows the results for different v values. In each case we select the range of k values where the two-stage bound predicts smaller covering arrays than the previously known best ones, setting the maximum number of uncovered t interactions as  = 1/ log vtv-1  v t . For each value of k we construct a single partial array and then run the different second stage algorithms on it consecutively. In this way all the second stage algorithms cover the same set of uncovered interactions. The column tab lists the best known CAN(t, k, v ) upper bounds from [13]. The column bound shows the upper bounds obtained from the two-stage bound (2). The columns na¨ ive, greedy, col and den show results obtained from running the TS Rand, Naive; , Trivial , TS Rand, Greedy; , Trivial , TS Rand, Col; , Trivial and TS Rand, Den; , Trivial algorithms, respectively. The na¨ ive method always finds a covering array that is smaller than the two-stage bound. This happens because we repeat the first stage of Algorithm 4 until the array has fewer than v t uncovered interactions. (If the first stage were not repeated, the algorithm still produce covering arrays that are not too far from the bound.) For v = 3 Greedy and Den have comparable performance. Method Col produces covering arrays that are smaller. However, for v  {4, 5, 6} Den and Col are competitive. Table 2 shows the results obtained by the different second stage algorithms when the maximum number of uncovered interactions in the first stage is set to 2 and 3 respectively. When more interactions are covered in the second stage, we obtain smaller arrays as expected. However, the improvement in size does not approach 50%. There is no clear winner. Next we investigate the covering arrays that are invariant under the action of a cyclic group. In Table 3 the column bound shows the upper bounds from Equation (1). The columns na¨ ive, greedy, col and den show results obtained from running TS Rand, Naive; , Cyclic , TS Rand, Greedy; , Cyclic , TS Rand, Col; , Cyclic and TS Rand, Den; , Cyclic , respectively. Table 4 presents results for cyclic group action based algorithms when the number of maximum uncovered interactions in the first stage is set to 2 and 3 respectively. For the Frobenius group action, we show results only for v  {3, 5} in Table 5. The column bound shows the upper bounds obtained from Equation (2). Table 6 presents results for Frobenius group action algorithms when the number of maximum uncovered interactions in the first stage is 2 or 3. Next we present a handful of results when t = 5. In the cases examined, using the trivial group action is too time consuming to be practical. However, the cyclic or Frobenius cases are feasible. Tables 7 and 8 compare two stage algorithms when the number of uncovered interactions in the first stage is at most 2. In almost all cases there is no clear winner among the three second stage methods. Methods Den and Greedy are, however, substantially faster and use less memory than method Col; for practical purposes they would be preferred. All code used in this experimentation is available from the github repository https://github.com/ksarkar/CoveringArray under an open source GPLv3 license.

5

Limited dependence and the Moser-Tardos algorithm

Here we explore a different randomized algorithm that produces smaller covering arrays than Algorithm 1. When k > 2t, there are interactions that share no column. The events of coverage of such interactions are independent. Moser et al. [29, 30] provide an efficient randomized construction method that exploits this

17

k 53 54 55 56 57 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27

tab 13021 14155 17161 19033 20185 68314 71386 86554 94042 99994 104794 233945 258845 281345 293845 306345 356045 506713 583823 653756 694048 783784 844834 985702 1035310 1112436 1146173 1184697

bound 13076 13162 13246 13329 13410 65520 66186 66834 67465 68081 68681 226700 229950 233080 236120 239050 241900 486310 505230 522940 539580 555280 570130 584240 597660 610460 622700 634430

na¨ ive greedy t = 6, v = 3 13056 12421 13160 12510 13192 12590 13304 12671 13395 12752 t = 6, v = 4 65452 61913 66125 62573 66740 63209 67408 63819 68064 64438 68556 65021 t = 6, v = 5 226503 213244 229829 216444 232929 219514 235933 222516 238981 225410 241831 228205 t = 6, v = 6 486302 449950 505197 468449 522596 485694 539532 502023 555254 517346 569934 531910 584194 545763 597152 558898 610389 571389 622589 583473 634139 594933

col 12415 12503 12581 12665 12748 61862 62826 63160 64077 64935 65739 212942 217479 219215 222242 226379 230202 448922 467206 484434 500788 516083 530728 544547 557917 570316 582333 593857

den 12423 12512 12591 12674 12757 61886 62835 63186 64082 64907 65703 212940 217326 219241 222244 226270 229942 447864 466438 483820 500194 515584 530242 548307 557316 569911 582028 593546

Table 1: Comparison of different TS Rand, -; , Trivial algorithms.

18

k greedy 53 54 55 56 57 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27 11968 12135 12286 12429 12562 59433 60090 60715 61330 61936 62530 204105 207243 210308 213267 216082 218884 425053 443236 460315 476456 491570 505966 519611 532612 544967 556821 568135

2 col 11958 12126 12129 12204 12290 59323 60479 61527 62488 61839 62899 203500 206659 209716 212675 215521 218314 -

den t = 6, v 11968 12050 12131 12218 12296 t = 6, v 59326 59976 60615 61242 61836 62428 t = 6, v 203302 206440 209554 212508 215389 218172 t = 6, v 420333 438754 455941 472198 487501 502009 515774 528868 541353 553377 564827

greedy =3 11716 11804 11877 11961 12044 =4 58095 58742 59369 59974 60575 61158 =5 199230 202342 205386 208285 211118 213872 =6 412275 430402 447198 463071 478269 492425 505980 518746 531042 542788 554052

3 col 11705 11787 11875 12055 12211 57951 58583 59867 61000 60407 61004 198361 201490 204548 -

den 11708 11790 11872 11950 12034 57888 58544 59187 59796 60393 60978 197889 201068 204107 207060 209936 212707 405093 423493 440532 456725 471946 486306 500038 513047 525536 537418 548781

Table 2: Comparison of TS Rand, -; 2, Trivial and TS Rand, -; 3, Trivial algorithms.

19

k 53 54 55 56 57 k 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27

tab 13021 14155 17161 19033 20185 tab 68314 71386 86554 94042 99994 104794 226000 244715 263145 235835 238705 256935 506713 583823 653756 694048 783784 844834 985702 1035310 1112436 1146173 1184697

bound 13059 13145 13229 13312 13393 bound 65498 66163 66811 67442 68057 68658 226680 229920 233050 236090 239020 241870 486290 505210 522910 539550 555250 570110 584210 597630 610430 622670 624400

na¨ ive greedy t = 6, v = 3 13053 12405 13119 12489 13209 12573 13284 12660 13368 12744 t = 6, v = 4 na¨ ive greedy 65452 61896 66080 62516 66740 63184 67408 63800 68032 64408 68556 64988 t = 6, v = 5 226000 213165 229695 216440 233015 219450 235835 222450 238705 225330 241470 228140 t = 6, v = 6 485616 449778 504546 468156 522258 485586 539280 501972 554082 517236 569706 531852 583716 545562 597378 558888 610026 571380 622290 583320 633294 594786

col 12405 12543 12663 12651 12744 col 61860 62820 63144 63780 64692 64964 212945 217585 221770 222300 225130 229235 448530 467232 490488 500880 521730 530832 549660 557790 575010 582546 598620

den 12411 12546 12663 12663 12750 den 61864 62784 63152 63784 64680 64976 212890 217270 221290 222210 225120 229020 447732 466326 488454 500172 519966 530178 548196 557280 573882 582030 597246

Table 3: Comparison of TS Rand, -; , Cyclic algorithms.

20

k greedy 53 54 55 56 57 39 40 41 42 43 44 31 32 33 34 35 36 17 18 19 20 21 22 23 24 25 26 27 11958 12039 12120 12204 12276 59412 60040 60700 61320 61908 62512 204060 207165 207165 213225 216050 218835 424842 443118 460014 476328 491514 505884 519498 532368 544842 543684 568050

2 col 11955 12027 12183 12342 12474 59336 59996 61156 62196 63192 64096 203650 209110 209865 212830 217795 218480 422736 440922 457944 474252 489270 503580 517458 530340 542688 543684 566244

den t = 6, v 11958 12036 12195 12324 12450 t = 6, v 59304 59964 61032 61976 62852 63672 t = 6, v 203265 208225 209540 212510 217070 218155 t = 6, v 420252 438762 455994 472158 487500 501852 515718 528828 541332 543684 564756

greedy =3 11700 11790 11862 11949 12027 =4 58076 58716 59356 59932 60568 61152 =5 199180 202255 205380 208225 211080 213770 =6 411954 430506 447186 463062 478038 492372 505824 518700 530754 542664 553704

3 col 11691 11874 12057 11937 12021 57976 58616 59252 59840 61124 61048 198455 204495 204720 207790 213425 213185 409158 427638 456468 460164 486180 489336 502806 515754 538056 539922 560820

den 11694 11868 12027 11943 12024 57864 58520 59160 59760 60904 60988 197870 203250 204080 207025 212040 212695 405018 423468 449148 456630 479970 486264 500040 512940 532662 537396 555756

Table 4: Comparison of TS Rand, -; 2, Cyclic and TS Rand, -; 3, Cyclic algorithms.

21

k 53 54 55 56 57 31 32 33 34 35 36

tab 13021 14155 17161 19033 20185 233945 258845 281345 293845 306345 356045

bound 13034 13120 13203 13286 13366 226570 229820 232950 235980 238920 241760

na¨ ive greedy t = 6, v = 3 13029 12393 13071 12465 13179 12561 13245 12633 13365 12723 t = 6, v = 5 226425 213025 229585 216225 232725 219285 234905 222265 238185 225205 241525 227925

col 12387 12513 12549 12627 12717 212865 216085 219205 223445 227445 231145

den 12393 12531 12567 12639 12735 212865 216065 219145 223265 227065 230645

Table 5: Comparison of TS Rand, -; , Frobenius algorithms.

k greedy 53 54 55 56 57 70 75 80 85 90 31 32 33 34 35 36 50 55 60 65 11931 12021 12105 12171 12255 13167 13473 13773 14031 14289 203785 206965 209985 213005 215765 218605 250625 259785 268185 275785

2 col 11919 12087 12237 12171 12249 13155 13473 13767 14025 14283 203485 208965 209645 214825 215545 218285 250365 259625 268025 275665

den greedy t = 6, v = 3 11931 11700 12087 11790 12231 11862 12183 11949 12255 12027 13179 13479 13779 14037 14301 t = 6, v = 5 203225 198945 208065 201845 209405 205045 214145 208065 215265 210705 218025 213525 250325 259565 267945 275665 -

3 col 11691 11874 12057 11937 12021 198445 204505 209845 207545 210365 213105 -

den 11694 11868 12027 11943 12024 197825 203105 207865 206985 209885 212645 -

Table 6: Comparison of TS Rand, -; 2, Frobenius and TS Rand, -; 3, Frobenius algorithms.

22

k 67 68 69 70 71

tab 59110 60991 60991 60991 60991

greedy 48325 48565 48765 49005 49245

col 48285 48565 49005 48985 49205

den 48305 48585 48985 49025 49245

Table 7: Comparison of TS Rand, -; 2, Frobenius algorithms. t = 5, v = 5 k 49 50 51 52 53 tab 122718 125520 128637 135745 137713 greedy 108210 109014 109734 110556 111306 col 108072 108894 110394 110436 111180 den 107988 108822 110166 110364 111120

Table 8: Comparison of TS Rand, -; 2, Cyclic algorithms. t = 5, v = 6 limited dependence. Specializing their method to covering arrays, we obtain Algorithm 5. For the specified value of N in the algorithm it is guaranteed that the expected number of times the loop in line 3 of Algorithm 5 is repeated is linearly bounded in k (See Theorem 1.2 of [30]). The upper bound on CAN(t, k, v ) guaranteed by Algorithm 5 is obtained by applying the Lov´ asz local lemma (LLL). Lemma 4. (Lov´ asz local lemma; symmetric case) (see [2]) Let A1 , A2 , . . . , An events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at ¯ most d, and that Pr[Ai ]  p for all 1  i  n. If ep(d + 1)  1, then Pr[n i=1 Ai ] > 0. The symmetric version of Lov´ asz local lemma provides an upper bound on the probability of a "bad" event in terms of the maximum degree of a bad event in a dependence graph, so that the probability that all the bad events are avoided is non zero. Godbole, Skipper, and Sunley [18] apply Lemma 4 essentially to obtain the bound on CAN(t, k, v ) in line 1 of Algorithm 5. Theorem 5. [18] Let t, v and k  2t be integers with t, v  2. Then log CAN (t, k, v ) 
k t

-

k-t t

+ t log v + 1

log

vt v t -1

The bound on the size of covering arrays obtained from Theorem 5 is asymptotically tighter than the one obtained from Theorem 1. Figure 10 compares the bounds for t = 6 and v = 3. The original proof of LLL is essentially non-constructive and does not immediately lead to a polynomial time construction algorithm for covering arrays satisfying the bound of Theorem 5. Indeed no previous construction algorithms appear to be based on it. However the Moser-Tardos method of Algorithm 5 does provide a construction algorithm running in expected polynomial time. For sufficiently large values of k Algorithm 5 produces smaller covering arrays than the Algorithm 1. But the question remains: Does Algorithm 5 produce smaller covering arrays than the currently best known results within the range that it can be effectively computed? Perhaps surprisingly, we show that the answer is affirmative. In Algorithm 5 we do not need to store the coverage information of individual interactions in memory because each time an uncovered interaction is encountered we re-sample the columns involved in that interaction and start the check afresh (checking the coverage in interactions in the same order each time). Consequently, Algorithm 5 can be applied for larger values of k than the density algorithm. 23

Algorithm 5: Moser-Tardos type algorithm for covering array construction. Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor Output: A : a CA(N ; t, k, v ) k -t log{(k t )-( t )}+t. log v +1 ; 1 Let N := vt
log
v t -1

2

3 4 5 6 7 8 9 10 11 12 13

14 15 16

Construct an N × k array A where each entry is chosen independently and uniformly at random from a v -ary alphabet; repeat Set covered := true; for each interaction   It,k,v do if  is not covered in A then Set covered := false; Set missing-interaction := ; break; end end if covered = false then Choose all the entries in the t columns involved in missing-interaction independently and uniformly at random from the v -ary alphabet; end until covered = true ; Output A;

Smaller covering arrays can be obtained by exploiting a group action using LLL, as shown in [33]. Table 9 shows the sizes of the covering arrays constructed by a variant of Algorithm 5 that employs cyclic and Frobenius group actions. While this single stage algorithm produces smaller arrays than the currently best known [13], these are already superseded by the two-stage based algorithms. k 56 57 58 59 60 tab 19033 20185 23299 23563 23563 MT 16281 16353 16425 16491 16557 k 44 45 46 47 48 tab 411373 417581 417581 423523 423523 MT 358125 360125 362065 363965 365805 k 25 26 27 28 29 tab 1006326 1040063 1082766 1105985 1149037 MT 1020630 1032030 1042902 1053306 1063272

(a) Frobenius. t = 6, v = 3

(b) Frobenius. t = 6, v = 5

(c) Cyclic. t = 6, v = 6

Table 9: Comparison of covering array size from Algorithm 5 (MT) with the best known results [13] (tab).

5.1

Moser-Tardos type algorithm for the first stage

The linearity of expectation arguments used in the SLJ bounds permit one to consider situations in which a few of the "bad" events are allowed to occur, a fact that we exploited in the first stage of the algorithms thus far. However, the Lov´ asz local lemma does not address this situation directly. The conditional Lov´ asz local lemma (LLL) distribution, introduced in [19], is a very useful tool. Lemma 6. (Conditional LLL distribution; symmetric case) (see [2, 33]) Let A = {A1 , A2 , . . . , Al } be a set of l events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at most d, and that Pr[Ai ]  p for all 1  i  l. Also suppose that ep(d +1)  1 ¯ (Therefore, by LLL (Lemma 4) Pr[l / A be another event in the same probability space i=1 Ai ] > 0). Let B  24

10

5

SLJ bound GSS bound

N - number of rows

10

4

10 1 10

3

10

2

10 k

3

10

4

10

5

Figure 10: Comparison of SLJ (Theorem 1) and GSS (Theorem 5) bounds for t = 6 and v = 3. The graph is plotted in log-log scale to highlight the asymptotic difference between the two bounds. with Pr[B ]  q , such that B is also mutually independent of a set of all other events Aj  A except for at ¯ most d. Then Pr[B | l i=1 Ai ]  eq . We apply the conditional LLL distribution to obtain an upper bound on the size of partial array that t leaves at most log vtv-1  v t interactions uncovered. For a positive integer k , let I = {j1 , . . . , j }  [k ] where j1 < . . . < j . Let A be an n × k array where each entry is from the set [v ]. Let AI denote the n ×  array in which AI (i, ) = A(i, j ) for 1  i  N and 1   ; AI is the projection of A onto the columns in I . ] Let M  [v ]t be a set of m t-tuples of symbols, and C  [k be a set of t columns. Suppose the t entries in the array A are chosen independently from [v ] with uniform probability. Let BC denote the event that at least one of the tuples in M is not covered in AC . There are  = k t such events, and for all of 1 n them Pr[BC ]  m 1 - v . Moreover, when k  2t, each of the events is mutually independent of all t k k -t other events except for at most  = k - 1 < t t- asz local lemma, when t - t 1 . Therefore, by the Lov´ 1 n em 1 - vt  1, none of the events BC occur. Solving for n, when n log(em) log
vt v t -1

(3)

] there exists an n × k array A over [v ] such that for all C  [k t , AC covers all the m tuples in M . In fact we can use a Moser-Tardos type algorithm to construct such an array. Let  be an interaction whose t-tuple of symbols is not in M . Then the probability that  is not covered 1 n in an n × k array is at most 1 - v when each entry of the array is chosen independently from [v ] with t uniform probability. Therefore, by the conditional LLL distribution the probability that  is not covered ] 1 n in the array A where for all C  [k . Moreover, t , AC covers all the m tuples in M is at most e 1 - v t there are  (v t - m) such interactions . By the linearity of expectation, the expected number of uncovered

25

interactions in A is less than v t when  (v t - m)e 1 - n

1 n vt

 v t . Solving for n, we obtain
m vt

log e 1 - log
vt v t -1

.

(4)

Therefore, there exists an n × k array with n = max

log(em) log
vt v t -1

,

m log{e(1- v t )}

log

vt v t -1

that has at most v t

uncovered interactions. To compute n explicitly, we must choose m. We can select a value of m to minimize n graphically for given values of t, k and v . For example, Figure 11 plots Equations 3 and 4 against m for t = 3, k = 350, v = 3, and finds the minimum value of n.
460 Equation (3) Equation (4) n - number of rows in the partial array n - number of rows in the partial array 440 440 445 max(Equation (3), Equation (4))

420

435

400

430

380

360

425

340 0

5

10

15 m

20

25

30

420 0

5

10

15 m

20

25

30

(a) Equations 3 and 4 against m.

(b) Maximum of the two sizes against m.

Figure 11: The minimum is at n = 422, when m = 16. t = 3, k = 350, v = 3 We compare the size of the partial array from the na¨ ive two-stage method (Algorithm 4) with the size obtained by the graphical methods in Figure 12. The Lov´ asz local lemma based method is asymptotically better than the simple randomized method. However, except for the small values of t and v , in the range of k values relevant for practical applications the simple randomized algorithm requires fewer rows than the Lov´ asz local lemma based method.

5.2

Lov´ asz local lemma based two-stage bound

We can apply the techniques from Section 5.1 to obtain a two-stage bound similar to Theorem 2 using the Lov´ asz local lemma and conditional LLL distribution. First we extend a result from [33]. Theorem 7. Let t, k, v be integers with k  t  2, v  2 and let  =
v log 
t vt v t -1

k t

, and  =

k t

-

k -t t

. If

 v t Then log CAN(t, k, v ) 
k t

+ t log v + log log log
vt v t -1

vt v t -1

+2

 - . 

Proof. Let M  [v ]t be a set of m t-tuples of symbols. Following the arguments of Section 5.1, when ) ] n  log(em there exists an n × k array A over [v ] such that for all C  [k t , AC covers all m tuples in M . vt
log
v t -1

At most  (v t - m) interactions are uncovered in such an array. Using the conditional LLL distribution, 1 n the probability that one such interaction is not covered in A is at most e 1 - v . Therefore, by the t 26

n - number of rows in partial array with vt missing interactions

550 500 450 400 350 300 250 200 150 100 50 Randomized (Algorithm 4) LLL based 200 400 600 k 800 1000 1200

n - number of rows in partial array with vt missing interactions

2500

2000

1500

1000

500

Randomized (Algorithm 4) LLL based

0

0

0

500

1000

1500

2000 k

2500

3000

3500

4000

4500

(a) t = 3, v = 3.

(b) t = 4, v = 3.

Figure 12: Comparison of the size of the partial array constructed in the first stage. Compares the size of the partial array specified in Algorithm 4 in Section 3.1, and the size derived in Section 5.1.
v 1 = linearity of expectation, we can find one such array A that leaves at most e (v t - m) 1 - v t  m -1 interactions uncovered. Adding one row per uncovered interactions to A, we obtain a covering array with at most N rows, where log(em)  vt N= + -1 t  m log tv v -1 v t log 
vt v t -1

n

t

The value of N is minimized when m =

. Because m  v t , we obtain the desired bound.

When m = v t this recaptures the bound of Theorem 5. Figure 13 compares the LLL based two-stage bound from Theorem 7 to the standard two-stage bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. Although the LLL based two-stage bound is tighter than the LLL based Godbole et al. bound, even for quite large values of k the standard two-stage bound is tighter than the LLL based two-stage bound. In practical terms, this specific LLL based two-stage method does not look very promising, unless the parameters are quite large.

6

Conclusion and open problems

Many concrete algorithms within a two-stage framework for covering array construction have been introduced and evaluated. The two-stage approach extends the range of parameters for which competitive covering arrays can be constructed, each meeting an a priori guarantee on its size. Indeed as a consequence a number of best known covering arrays have been improved upon. Although each of the methods proposed has useful features, our experimental evaluation suggests that TS Rand, Greedy; 2,  and TS Rand, Den; 2. with   {Cyclic, Frobenius} realize a good trade-off between running time and size of the constructed covering array. Improvements in the bounds, or in the algorithms that realize them, are certainly of interest. We mention some specific directions. Establishing tighter bounds on the coloring based methods of Section 3.3 is a challenging problem. Either better estimates of the chromatic number of the incompatibility graph after a random first stage, or a first stage designed to limit the chromatic number, could lead to improvements in the bounds. In Section 5.1 and 5.2 we explored using a Moser-Tardos type algorithm for the first stage. Although this is useful for asymptotic bounds, practical improvements appear to be limited. Perhaps a different approach of 27

550 500 450 N - number of rows N - number of rows SLJ bound Godbole LLL-2-stage 2-stage 400 350 300 250 200 150 100

10000 9000 8000 7000 6000 5000 4000 3000 2000 1000 0

SLJ bound Godbole LLL-2-stage 2-stage

0

50

100

150

200 k

250

300

350

400

450

1000

2000

3000

4000 k

5000

6000

7000

8000

(a) t = 3, v = 3.

(b) t = 4, v = 4.

Figure 13: Comparison among the LLL based two-stage bound from Theorem 7, the standard two-stage bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. reducing the number of bad events to be avoided explicitly by the algorithm may lead to a better algorithm. A potential approach may look like following: "Bad" events would denote non-coverage of an interaction over a t-set of columns. We would select a set of column t-sets such that the dependency graph of the corresponding bad events have a bounded maximum degree (less than the original dependency graph). We would devise a Moser-Tardos type algorithm for covering all the interactions on our chosen column t-sets, and then apply the conditional LLL distribution to obtain an upper bound on the number of uncovered interactions. However, the difficulty lies in the fact that "all vertices have degree  " is a non-trivial, "hereditary" property for induced subgraphs, and for such properties finding a maximum induced subgraph with the property is an NP-hard optimization problem [17]. There is still hope for a randomized or "nibble" like strategy that may find a reasonably good induced subgraph with a bounded maximum degree. Further exploration of this idea seems to be a promising research avenue. In general, one could consider more than two stages. Establishing the benefit (or not) of having more than two stages is also an interesting open problem. Finally, the application of the methods developed to mixed covering arrays appears to provide useful techniques for higher strengths; this merits further study as well.

Acknowledgments
The research was supported in part by the National Science Foundation under Grant No. 1421058.

References
[1] A. N. Aldaco, C. J. Colbourn, and V. R. Syrotiuk. Locating arrays: A new experimental design for screening complex engineered systems. SIGOPS Oper. Syst. Rev., 49(1):31­40, Jan. 2015. [2] N. Alon and J. H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics and Optimization. John Wiley & Sons, Inc., Hoboken, NJ, third edition, 2008. With an appendix on the life and work of Paul Erd os. [3] Android. Android configuration class, 2016. http://developer.android.com/reference/android/content/ res/Configuration.html. [4] J. I. Brown. The complexity of generalized graph colorings. Discrete Appl. Math., 69(3):257­270, 1996. 28

[5] R. C. Bryce and C. J. Colbourn. The density algorithm for pairwise interaction testing. Software Testing, Verification, and Reliability, 17:159­182, 2007. [6] R. C. Bryce and C. J. Colbourn. A density-based greedy algorithm for higher strength covering arrays. Software Testing, Verification, and Reliability, 19:37­53, 2009. [7] R. C. Bryce and C. J. Colbourn. Expected time to detection of interaction faults. Journal of Combinatorial Mathematics and Combinatorial Computing, 86:87­110, 2013. [8] J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE Global Research Technical Report, 29:769­781, 2002. [9] M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength 3. Des. Codes Crypt., 16:235­242, 1999. [10] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG system: An approach to testing based on combinatorial design. IEEE Transactions on Software Engineering, 23:437­44, 1997. [11] M. B. Cohen. Designing test suites for software interaction testing. PhD thesis, The University of Auckland, Department of Computer Science, 2004. [12] C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121­167, 2004. [13] C. J. Colbourn. Covering array tables, 2005-2015. http://www.public.asu.edu/ccolbou/src/tabby. [14] C. J. Colbourn. Covering arrays and hash families. In Information Security and Related Combinatorics, NATO Peace and Information Security, pages 99­136. IOS Press, 2011. [15] C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal of Combinatorial Mathematics and Combinatorial Computing, 90:97­115, 2014. [16] R. Diestel. Graph Theory. Graduate Texts in Mathematics. Springer, fourth edition, 2010. [17] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman & Co., New York, NY, USA, 1979. [18] A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds and Poisson approximations. Combinatorics, Probability and Computing, 5:105­118, 1996. [19] B. Haeupler, B. Saha, and A. Srinivasan. New constructive aspects of the Lov´ asz local lemma. J. ACM, 58(6):Art. 28, 28, 2011. [20] A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal Arrays. Springer-Verlag, New York, 1999. [21] D. S. Johnson. Approximation algorithms for combinatorial problems. J. Comput. System Sci., 9:256­ 278, 1974. [22] G. O. H. Katona. Two applications (for search theory and truth functions) of Sperner type theorems. Periodica Math., 3:19­26, 1973. [23] D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math., 6:255­262, 1973. [24] D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC Press, 2013. [25] D. R. Kuhn and M. Reilly. An investigation of the applicability of design of experiments to software testing. In Proc. 27th Annual NASA Goddard/IEEE Software Engineering Workshop, pages 91­95, Los Alamitos, CA, 2002. IEEE.

29

[26] D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault interactions and implications for software testing. IEEE Trans. Software Engineering, 30:418­421, 2004. [27] L. Lov´ asz. On the ratio of optimal integral and fractional covers. Discrete Math., 13(4):383­390, 1975. [28] K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin. Des., 13:70­77, 2005. [29] R. A. Moser. A constructive proof of the Lov´ asz local lemma. In STOC'09--Proceedings of the 2009 ACM International Symposium on Theory of Computing, pages 343­350. ACM, New York, 2009. [30] R. A. Moser and G. Tardos. A constructive proof of the general Lov´ asz local lemma. J. ACM, 57(2):Art. 11, 15, 2010. [31] C. Nie and H. Leung. A survey of combinatorial testing. ACM Computing Surveys, 43(2):#11, 2011. [32] A. H. Ronneseth and C. J. Colbourn. Merging covering arrays and compressing multiple sequence alignments. Discrete Appl. Math., 157:2177­2190, 2009. [33] K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv e-prints. [34] G. Seroussi and N. H. Bshouty. Vector sets for exhaustive testing of logic circuits. IEEE Trans. Inform. Theory, 34:513­522, 1988. [35] S. K. Stein. Two combinatorial covering theorems. J. Combinatorial Theory Ser. A, 16:391­397, 1974. [36] J. Torres-Jimenez, H. Avila-George, and I. Izquierdo-Marquez. A two-stage algorithm for combinatorial testing. Optimization Letters, pages 1­13, 2016.

30

Des. Codes Cryptogr. (2015) 77:479­491 DOI 10.1007/s10623-015-0084-4

Optimal low-power coding for error correction and crosstalk avoidance in on-chip data buses
Yeow Meng Chee1 · Charles J. Colbourn2 · Alan Chi Hung Ling3 · Hui Zhang1 · Xiande Zhang1

Received: 30 October 2014 / Revised: 12 April 2015 / Accepted: 16 April 2015 / Published online: 8 May 2015 © Springer Science+Business Media New York 2015

Abstract Coupled switched capacitance causes crosstalk in ultra deep submicron/nanometer VLSI fabrication, which leads to power dissipation, delay faults, and logical malfunctions. We present the first memoryless transition bus-encoding technique for power minimization, errorcorrection, and elimination of crosstalk simultaneously. To accomplish this, we generalize balanced sampling plans avoiding adjacent units, which are widely used in the statistical design of experiments. Optimal or asymptotically optimal constant weight codes eliminating each kind of crosstalk are constructed. Keywords Constant weight codes · Packing sampling plan avoiding adjacent units · Crosstalk avoidance · Low power code · Packing by triples · Balanced sampling plan Mathematics Subject Classification 94B25 · 05B40 · 05B07 · 62K10

This is one of several papers published in Designs, Codes and Cryptography comprising the "Special Issue on Cryptography, Codes, Designs and Finite Fields: In Memory of Scott A. Vanstone".

B

Charles J. Colbourn charles.colbourn@asu.edu Yeow Meng Chee ymchee@ntu.edu.sg Alan Chi Hung Ling aling@emba.uvm.edu Hui Zhang huizhang@ntu.edu.sg Xiande Zhang xiandezhang@ntu.edu.sg

1

Division of Mathematical Sciences, School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore 637371, Singapore School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287, USA Department of Computer Science, University of Vermont, Burlington, VT 05405, USA

2

3

123

480

Y. M. Chee et al.

1 Introduction
The ever-decreasing feature size of VLSI fabrication process has led to many challenges in VLSI circuit design. One of the most important issues concerns the characteristics of onchip wires [11]. The wires' cross-sectional areas and spacings have fallen dramatically with the move into the ultra deep submicron/nanometer (UDSM) regime. This has increased the resistance and capacitance of wires. To help reduce resistance, wires today are taller than they are wide, and they are poised to grow even taller as technology continues to scale. The resulting growth of side-to-side capacitance between long parallel wires causes coupled switch capacitance to dominate the wire-to-substrate capacitance in UDSM circuits by several orders of magnitude [21]. Coupled switched capacitance in turn leads to crosstalks, which result in power dissipation, delay faults, and logical malfunctions. The problem of eliminating or minimising crosstalks is considered the biggest signal integrity challenge for long on-chip buses implemented in UDSM CMOS technology [12]. The worst crosstalk couplings have been classified into four types [6,12], as described in Table 1. The coupled switched capacitance resulting from type-1, -2, -3, and -4 crosstalks is in the ratio of 1:2:3:4. Hence, it is particularly important to avoid crosstalks of higher types. Type-1 crosstalks cannot be avoided in any useful communication channel. However, type-1 crosstalks give rise to power dissipation and must be limited, because low power is a critical design objective in recent years. Another factor that has emerged as a new challenge for VLSI circuit designers is UDSM noise, caused by high-leakage transistors, power-grid fluctuations, ground bounce, IR drops, clock jitter, and electromagnetic radiation. The effects of such noise are difficult to predict or prevent. For example, noise in radiation-hardened circuits for satellite communication systems is random and does not correlate with particular switching patterns on the buses. A further source of faults is manufacturing defects. In nanotechnology, circuits are manufactured with a significant proportion of faults, and occasional errors may be unavoidable. Hence, preventive techniques are insufficient, and active error correction is required. Various researchers have proposed coding techniques to encode data on a bus for crosstalk avoidance [6,17,28], for low power dissipation [3,15,19,22,26], and for error correction [1,8]. Coding schemes that simultaneously satisfy two of these three criteria have also been investigated: · crosstalk avoidance and low power dissipation [12,27]; · crosstalk avoidance and error correction [14]; and · low power dissipation and error correction [2,16,18].

Table 1 Types of worst crosstalk couplings Type-1 Type-2 001  110 011  100 Center wire in opposite transition to an adjacent wire. The other wire in same transition as center wire Type-3 001  010 010  100 011  101 101  110 Center wire in opposite transition to an adjacent wire. The other wire maintains previous state Type-4

0  1 Single wire undergoes transition. Adjacent wires maintain previous states

010  101 All three adjacent wires undergo opposite transitions

123

Optimal low-power coding for crosstalk avoidance

481

Despite many efforts, the only families of optimal codes known are those for low power dissipation [3]. Many of the results on the comparative performance of existing codes are based on simulations rather than rigorous mathematical analysis. In this paper, we begin the study of codes for UDSM buses that simultaneously provide for low power dissipation, crosstalk avoidance, and error correction. In particular, we exhibit the first infinite families of such codes that are provably optimal. The paper is organized as follows. Section 2 establishes necessary terminology and gives a mathematical formulation of the problem of designing low-power codes that avoid crosstalks and correct errors. In Sect. 3, we present the relation of codes of each type with packing sampling plans avoiding adjacent units. In Sect. 4, we focus on optimal solutions for k = 3 for all positive integer n . In Sect. 5, the sizes of optimal codes of all types with small lengths are determined by computer search, and brief conclusion is given.

2 Background
2.1 Coding framework
A coding framework for data buses was introduced by Ramprasad et al. [15]. A bus interconnecting two embedded systems on a systems-on-chip (SoC) platform can be modelled generically as in Fig. 1. The source encoder (decoder) compresses (decompresses) the input data so that the number of bits required in the representation of the source is minimised. While the source encoder removes redundancy, the channel encoder adds redundancy to combat errors that may arise due to noise in the bus. Ramprasad et al. [15] considered various combinations of source-channel encoder-decoder pairs and presented simulation results for their power dissipation. Their approach is what is known as joint source-channel coding in the information theory literature. Shannon's information separation theorem [20] states that reliable transmission can be accomplished by separate source and channel coding, where the source encoder and decoder need not take into account the channel statistics and the channel encoder and decoder need not take into account the source statistics. This applies, however, only for point-to-point transmissions and for infinite sequence length. The first condition (point-to-point transmission) holds for a UDSM bus but the second requirement for infinite sequence length is clearly undesirable for bus coding, because it could give rise to circuits of unbounded delay. Moreover, joint source-channel coding is useful only when we know the statistics of the source and channel. In the absence of such statistics, one can only fall back on optimising the source and channel separately. Indeed, Ramprasad et al. [15] considered coding schemes and simulations on certain source data with better understood statistics (for example, pop music, classical music, video, and speech).

noisy channel
source encoder channel encoder channel decoder source decoder

transmitter
Fig. 1 Framework for systems-on-chip

receiver

123

482

Y. M. Chee et al.

In many systems, the behaviour of source data is hard to predict and so the joint sourcechannel coding approach loses its power. Many researchers have therefore fallen back on addressing the source coding and channel coding problems separately. This is also the approach taken in this paper. We focus on designing optimal channel coding schemes for the scenario where the source statistics are unknown.

2.2 Codes
The Hamming n -space is the set H(n ) = {0, 1}n , endowed with the (Hamming) distance dH (·, ·) defined as follows: for u, v  H(n ), dH (u, v) is the number of positions where u and v differ. The (Hamming) weight of a vector u  H(n ) is the number of positions in u with nonzero value, and is denoted wH (u). The i th component of u is denoted ui . The support of a vector u  H(n ), denoted supp(u), is the set {i : ui = 1}. A (binary) code of length n is a subset C  H(n ). C is said to be of constant weight w if wH (u) = w for all u  C . The elements of a code are called codewords and the size of a code is the number of codewords it contains. The support of C is supp(C ) = {supp(u) : u  C }. The minimum distance of C is dmin (C ) = min{dH (u, v) : u, v  C and u = v}. A constant-weight code of length n , minimum distance d , and weight w is denoted as an (n , d , w) code. A code that is capable of correcting any occurrence of e or fewer symbol errors is said to be e-error-correcting. A code C is e-error-correcting if and only if dmin (C )  2e + 1 [9].

2.3 Set systems and graphs
For integers i < j , the set {i , i + 1, . . . , j } is abbreviated as [i , j ]. We further abbreviate [1, j ] to [ j ]. For a finite set X and k  | X |, we define 2 X = { B : B  X }, and X k = { B  X : | B | = k }.

A set system is a pair S = ( X , B), where X is a finite set of points and B  2 X . The elements of B are called blocks. The order of S is the number of points, | X |, and the size of S is the number of blocks, |B|. A set system ( X , B) is said to be k -uniform if B  X k .A graph is a 2-uniform set system and it is common to refer to the points and blocks of a graph as vertices and edges, respectively. A path of length n is an alternating sequence of vertices and edges W = v0 , e1 , v1 , e2 , . . . , en , vn , such that all the vertices vi , i  [0, n ] and edges ei , i  [n ] are all distinct from one another, except possibly the first and last vertices. A cycle is a path in which the first and last vertices are the same. Let ( X , B) be a set system of order n . The incidence vector of a block B  B is the vector ( B )  H(n ) such that 1, if i  B ( B )i = 0, otherwise. There is a natural correspondence between the Hamming n -space and the complete set system ( X , 2 X ): the positions of vectors in H(n ) correspond to points in X , a vector u  H(n ) corresponds to the block supp(u), and dH (u, v) = |(supp(u) \ supp(v))  (supp(v) \ supp(u))|. From this, it follows that there is a bijection between the set of all codes of length n and the set of all set systems of order n . An (n , k , )-packing is a k -uniform set system ( X , B) with | X | = n such that every element of X 2 is contained in at most  blocks of B . Let D (n , k , ) denote the largest size among all (n , k , )-packings. The leave graph of ( X , B) is the multigraph ( X , E ), where E

123

Optimal low-power coding for crosstalk avoidance

483

contains each e  X 2 exactly  - d (e) times, where d (e) is the number of blocks containing e. When  = 1, we omit  in the notation; in this case, the leave is a simple graph. When the leave contains no edges, the packing is a balanced incomplete block design. The balanced sampling plan avoiding adjacent units (BSA) was introduced to design sampling plans that exclude contiguous units in statistical experiments [10,25]; for more recent work, see [7,29]. In statistical applications, in a circular or linear order of the elements, elements that are "close" do not appear together, while those more distant all appear the same number of times together. A (circular) BSA (n , k ; ) is an (n , k , )-packing ( X , B) with X = Zn whose leave graph consists of all the edges {i , j } with i - j  ±1, . . . , ± (mod n ), and every other pair appears in  blocks. A (linear) LBSA (n , k ; ) is an (n , k , )-packing ( X , B) with X = [0, n - 1] whose leave graph consists of all the edges {i , j } with 0  i < j < n for which j - i   , and every other pair appears in  blocks. We employ these only when  = 1, and so omit  in the notation. We generalize circular and linear BSAs (with  = 1) to a packing sampling plan avoiding adjacent units (PSA). A (circular) CPSA(n , k ; ) is an (n , k )-packing ( X , B) with X = Zn whose leave graph contains all the edges {i , j } with i - j  ±1, . . . , ± (mod n ), and every other pair appears in at most one block. A (linear) LPSA(n , k ; ) is an (n , k )-packing ( X , B) with X = [0, n - 1] whose leave graph contains all the edges {i , j } with 0  i < j < n for which j - i   , and every other pair appears in at most one block. (In this case, every CPSA(n , k ; ) is an LPSA(n , k ; ) but the converse need not hold.) Let B (n , k ; ) denote the largest size of any LPSA(n , k ; ); the LPSA is optimal if its size is B (n , k ; ). Similarly, let B  (n , k ; ) denote the largest size of any CPSA(n , k ; ); the CPSA is optimal if its size is B  (n , k ; ). Let U (n , k ; ) =
2
 -1 i =0 n - -i -1 k -1

+(n -2) k

n -2 -1 k -1

.

Lemma 2.1 B (n , k ; )  U (n , k ; ). Proof For an LPSA(n , k ; ) constructed on [0, n - 1], for each i  [0,  - 1], the points i  -i -1 and n - 1 - i appear in at most n -k blocks, and all the other points appear in at most -1
n -2 -1 k -1

blocks. Then k B (n , k ; )  2

 -1 i =0

n - -i -1 k -1

+ (n - 2)

n -2 -1 k -1

.

When  = 1, we omit it in the notation. If there is an (n , k )-packing with leave graph containing a path of length n - 1, we can always relabel the points to get an LPSA(n , k ). Corollary 2.2 B (n , k ) 
2
n -2 k -1

+(n -2) k

n -3 k -1

.

Theorem 4.1 shows that when k = 3, this inequality is tight.

2.4 Problem formulation
Limited weight codes have been widely exploited for the case of on-chip communication to achieve crosstalk coupling elimination and energy efficiency [12,23]. We consider an n bit parallel bus in a single metal layer, for which we want memoryless codes to weaken crosstalk, reduce power consumption, and correct errors. We use constant weight codes with small weight to achieve low power similarly by reducing the node switching activity, that is, reducing the total number of transitions occurring between the newly arrived data and the present data on the bus.

123

484

Y. M. Chee et al.

Assume an n -bit bus, consisting of signals b0 , b1 , b2 , . . . , bn -1 . Consider a group of three wires in an on-chip bus, which are driven by signals bi -1 , bi and bi +1 . The delay and energy consumption are primarily affected by transition patterns based on the bus signals bi -1 , bi and bi +1 as the crosstalk patterns in Table 1. The selection of codeword does not depend on previous history, so the environment is memoryless. Consequently coding must address the possibility that any two codewords can appear one after the other. Therefore to avoid crosstalk and correct errors, we are interested in constant weight codes of length n , weight w and minimum distance d  3 satisfying the condition that there do not exist three consecutive coordinates i - 1, i , i + 1 such that the crosstalk couplings of type-2 (or -3, -4) occur in any two different codewords. We denote such a code avoiding crosstalk of each type as an (n , d , w)-II (or -III, -IV) code. The maximum size of these codes are denoted as A I I (n , d , w) (or A I I I (n , d , w), A I V (n , d , w)), and any code achieving this size is optimal. When S  { I I , I I I , I V }, the maximum size of a code that is simultaneously an (n , d , w)- S code for each S  S is denoted by AS (n , d , w). When d = 2w , the following results are straightforward. Lemma 2.3 For all positive integers n and w ,
n ; (i) A I I (n , 2w, w) = A I V (n , 2w, w) = w n I I I (ii) A (n , 2w, w) = w when w = 1; A I I I (n , 2, 1) = n +1 2

.

n is an upper bound on the size of the desired code in each case. Proof The quantity s = w We construct codes of size s as follows. The code with support

{{i , s + i , 2s + i , . . . , (w - 1)s + i } : i  [0, s - 1]} is an optimal (n , 2w, w)-II code. The code with support {{wi , 1 + wi , . . . , (w - 1) + wi } : i  [0, s - 1]} is an optimal (n , 2w, w)-IV code, and an optimal (n , 2w, w)-III code when w = 1. When 1 w = 1, the code with support {{2i } : i  [0, n - 2 ]} is an optimal (n , 2, 1)-III code. Next we show there is close connection between (n , 2k - 2, k ) codes of each type and optimal LPSA(n , k )s. Hence, optimal codes are constructed based on the construction of optimal LPSA(n , k )s.

3 Codes and LPSA(n, k; )s
In this section, we establish connections between optimal LPSA(n , k ; )s and the codes of each type. We begin with optimal (n , 2k - 2, k )-II codes for sufficiently large n . Theorem 3.1 Let k  3. Then A I I (n , 2k - 2, k )  B (n , k ). Further, if B (n , k ) = U (n , k ) and n  3k 2 + 2k - 3, then A I I (n , 2k - 2, k ) = B (n , k ). Proof Whenever ( X , B) is an LPSA(n , k ), the code with support B is an (n , 2k - 2, k )-II code. Now suppose that ( X , B) is an optimal LPSA(n , k ) of size U (n , k ). We prove that U (n , k ) is the largest possible size of an (n , 2k - 2, k )-II code. Assume that D is an (n , 2k - 2, k )-II code of size M . Partition the code into three parts as follows. The first part A contains all codewords with at least one segment "11". Because n > k , for each codeword in A, there always exist three adjacent coordinates such that "110" or "011"

123

Optimal low-power coding for crosstalk avoidance

485

appears in these coordinates. Let S = {i : u  A, s.t.,u has "110 in coordinates i - 2, i - 1, i , or "011" in coordinates i , i + 1, i + 2}, and let s = | S |. For each i  S , there exist at most two codewords in A that have "110" in i - 2, i - 1, i or "011" in i , i + 1, i + 2. Hence | A|  2 s . The second part T  D \ A contains all codewords with "1" in at least one coordinate in S . Without loss of generality, if there exists a codeword in A with "110" in the coordinates i - 2, i - 1, i for some i , then the codewords in T with "1" in i must have segment "101" in these coordinates to avoid type-2 crosstalk. Because dmin (D) = 2k - 2, there is only one such codeword. So for each i  S , there is at most one codeword in T with "1" in i . Hence |T |  s . Finally, let C = D \ (A  T ). Then M = |A| + |T | + |C |. Because each codeword in C has "0" in all coordinates in S , we can shorten C to a code C by deleting all coordinates in S . Then C is an (n - s , 2k - 2, k ) code, and supp(C ) is an (n - s , k )-packing. The shortening process partitions the coordinates of C into at most s + 1 classes, separated in C by the coordinates deleted to form C . No codeword of C has "11" in consecutive coordinates of any single class. Let x be the number of isolated coordinates in this partition, and m be the number of classes with at least two coordinates; then x + m  s + 1. We now estimate the size of C using the packing. s -1 s -2 s -3 Let a0 = n - , a1 = n - , a2 = n - . Then we have: k -1 k -1 k -1 | C | = |C |  x · a0 + 2m · a1 + (n - s - 2m - x ) · a2 . k    

Because x - y - 1  x - y  x - y , we have:   x n -s -1 - n -s -3 + 2m n -s -2 - n -s -3 + (n - s ) n -s -3  k -1 k -1 k -1 k -1 k -1 M  3s +  k   2 1 n -s -3  x   k -1 + 1 + 2m k -1 + 1 + (n - s ) k -1   3s +  k      2(s + 1) + (n - s ) n -s -3   2 x + 2m + (n - s ) n -s -3      k -1 k -1   3s +  .  3s +  k k Let F (s ) = 3s +
2(s +1)+(n -s ) k
n -s -3 k -1

. We claim that because n  3k 2 + 2k - 3,
n -s -3 k -1

U (n , k )  maxs [1,n ] F (s ). Because F (s ) = 3s +
2(s +2)+(n -s -1) k
n -s -4 k -1

2(s +1)+(n -s ) k
n -s -4 k -1

and F (s + 1) = 3(s + 1) +
n -s -4 k -1

, we have

-2

k

-3  F (s )- F (s +1)  n - 2k - s . k-1

+n -s -2 k

-2. Further, we have: n - 3k 2 - 1 - s k (k - 1)  F (s ) - F (s + 1) 

So when s  n - 3k 2 - 1, F (s ) - F (s + 1)  0, i.e., F (s ) is decreasing; and when s  n - 2k , F (s ) - F (s + 1)  0, i.e., F (s ) is increasing. When s  [n - 3k 2 , n - 2k - 1],

123

486

Y. M. Chee et al.

F (s )  F (1); because the verification is tedious, we omit it here. We therefore only need to compare F (1) and F (n ) to find the maximum value of F (s ).    4 + (n - 1) n -4  2  k -1   - 3n - 2(n + 1)  (n - 1)(n - 3k - 1) . F (1) - F (n ) = 3 +  k k k (k - 1)

Because n  3k 2 + 2k - 3  3k 2 + 1, F (1)  F (n ) and maxs [1,n ] F (s ) = F (1).    2 n -2 + (n - 2) n -3 - 4 - (n - 1) n -4   k -1 k -1 k -1  -3 U (n , k ) - F (1)   k    n -2 + (n - 1) n -3 - 4 - (n - 1) n -4   k -1 k -1 k -1  -3  k    n -2 - 4  2  k -1   - 3  n - 3k - 2 k + 3  0 .  k k (k - 1)

Hence U (n , k )  maxs [1,n ] F (s ). For (n , 2k - 2, k )-III codes and (n , 2k - 2, k )-IV codes, we establish lower bounds. Lemma 3.2 1. A I I I (n , 2k - 2, k )  A I I , I I I , I V (n , 2k - 2, k )  D ( n -1 2. A I I I (n , 4, 3)  A I I , I I I , I V (n , 4, 3)  B ( n 2 , 3) + 2 . n I I I I I , I I I , I V  3. A (n , 4, 3)  A (n , 4, 3)  B ( 2 , 3) + n 2 .
n 2

, k ).

Proof For the first inequality, take an ( n 2 , k )-packing ( X , B ), and construct a code C of n length n 2 by taking supp(C ) = B . View C as an |B | × 2 array. When n  1 (mod 2), we add one column of all zeroes between every two consecutive columns of C , and when n  0 (mod 2) we add one further column of all zeroes after C to get an (n , 2k - 2, k )-III code. The verification is straightforward, because every second column is all zeroes. The construction for the second is similar. Apply the same inflation to an LPSA n 2 ,3 of size B n 2 , 3 to obtain a code C1 . In every codeword of C1 , two 1s are separated by three (or more) coordinates, and different codewords cannot have 1s in adjacent coordinates. Now 1 form code C2 , consisting of all codewords with support {2i , 2i + 1, 2i + 2} for 0  i < n - 2 . No prohibited situation arises from 000 or 111 in three consecutive coordinates of a codeword. In consecutive coordinates in which two codewords of C2 are neither 000 nor 111, the two codewords contain 011 and 110, which is permitted. So we consider one codeword from 1 C1 and one from C2 . The coordinates with indices in {2i + 1 : 0  i < n - 2 } appear in only one codeword, which is {2i , 2i + 1, 2i + 2}. So in the consecutive coordinates in which two such codewords are neither 000 nor 111, and are not equal, the two codewords contain {001, 100}, {010, 011}, or {010, 110}. All are permitted. The bound in the third case is equal to that in the second unless n is even and B  n 2 ,3 = B n C and C as in the second case; one further , 3 . When both occur, use a CPSA to form 1 2 2 codeword can be added with support {0, n - 2, n - 1}.

123

Optimal low-power coding for crosstalk avoidance

487

Lemma 3.3 A I V (n , 2k - 2, k )  B (n , k ). Proof Take an LPSA(n , k ) ( X , B) of size B (n , k ). Apply to the points in [0, n - 1] the permutation i  2i , if i < n /2 , and i  2i - 2 n /2 + 1, if i  n /2 , to get ( X , B ). The code C with supp(C ) = B is an (n , 2k - 2, k )-IV code. We give another construction for an (n , 2k - 2, k )-IV code from an optimal LPSA(n , k ; k - 1). When k = 3, this construction gives a better lower bound than Lemma 3.3. Lemma 3.4 Let k  3. 1. A I I , I V (n , 2k - 2, k )  B (n , k ; k - 1), 2. A I V (n , 2k - 2, k )  B (n , k ; k - 1) + 3. A I V (n , 2k - 2, k )  B  (n , k ; k - 1) +
n -1 k -1 n k -1

, and .

-1 Proof Let s = n k -1 , and ( X , B ) be an LPSA(n , k ; k - 1) of size B (n , k ; k - 1). Then the code C with supp(C ) = { B : B  B} is an (n , 2k - 2, k )-II code and an (n , 2k - 2, k )-IV code. Further, the code C with supp(C ) = { B : B  B}{{(k -1)i , (k -1)i +1, . . . , (k -1)i +k -1} : i  [0, s - 1]} is an (n , 2k - 2, k )-IV code. When n  0 (mod k - 1), statement (3) is implied by statement (2). So suppose that n  0 (mod k - 1). Using instead a CPSA(n , k ; k - 1) of size B  (n , k ; k - 1), adjoin the block {(k - 1)s , (k - 1)s + 1, . . . , (k - 1)s + k - 2, 0}.

Lemma 3.5 A I I , I V (n , 4, 3)  U (n , 3; 2) when n  13. Proof Computational results reported in Table 2 show that A I I , I V (13, 4, 3) = U (13, 3; 2) = 16, A I I , I V (14, 4, 3) = U (14, 3; 2) = 20, A I I , I V (15, 4, 3) = U (15, 3; 2) = 25, and
Table 2 Sizes of optimal codes for n  20 n D (n , 3) B (n , 3) B  (n , 3) B (n , 3; 2) B  (n , 3; 2) A I I .(n , 4, 3) A I I I (n , 4, 3) A I V (n , 4, 3) A I I , I I I (n , 4, 3) A I I , I V (n , 4, 3) A I I I , I V (n , 4, 3) A I I , I I I , I V (n , 4, 3) 3 1 0 0 0 0 1 1 1 1 1 1 1 4 1 0 0 0 0 1 1 1 1 1 1 1 5 2 1 0 0 0 2 2 2 2 2 2 2 6 4 2 2 0 0 4 3 4 3 4 3 3 7 7 4 3 1 0 5 4 6 3 4 4 3 8 8 6 5 2 0 6 5 7 4 4 5 4 9 12 9 9 4 3 9 6 10 5 7 6 5 10 13 10 10 6 5 10 7 12 6 8 7 6 11 17 14 13 9 8 14 8 15 7 12 8 7 12 20 16 16 12 12 16 9 19 8 13 9 8 13 26 21 20 16 15 21 10 23 10 16 10 10 14 28 24 23 20 18 15 35 30 30 25 25 16 37 32 32 28 26 17 44 39 38 34 34 18 48 42 42 37 36 19 57 50 49 45 43 20 60 54 53 48 46

24
11

30
13

32
14

39
17

42
18

50
19

54
21

26
11 20 11 11

32
13 25 13 13

35
13 28 14 13

42
17 34 17 17

45
18 37 18 18

54
19 45 19 19

57
20 48 21 20

Lower bounds and exact values

123

488

Y. M. Chee et al.

A I I , I V (16, 4, 3) = U (16, 3; 2) = 32. Suppose to the contrary that A I I , I V (n , 4, 3) > U (n , 3; 2) for some n  17, and let n be the smallest such value. When n  17 we have U (n , 3; 2)  U (n - 1, 3; 2) + 3 and U (n , 3; 2)  U (n - 3, 3; 2) + 4. (See Table 2 for small values.) Let ( X , B) be the support of an (n , 4, 3)-{II,IV} code of size A I I , I V (n , 4, 3). Some triple of B covers a pair of the form {a , b}  {{i , i + 1}, {i , i + 2}} because it is not an LPSA(n , 3; 2). Case 1 Some element appears in at most one triple. Suppose that element i appears in no triple. Shorten the code by deleting coordinate i and delete the triples (if any) containing pairs {i - 1, i + 1}, {i - 2, i + 1}, and {i - 1, i + 2}. The result is a type II and IV code, so the given code has at most A I I , I V (n - 1, 4, 3) + 3 triples, a contradiction. Suppose now that element i appears in exactly one triple T . Then if i  {0, n - 1}, delete coordinate i and triple T to get a contradiction. If i  {1, n - 2}, delete coordinate i and delete triple T , along with triples containing {0, 2} and {0, 3} when i = 1 or {n - 4, n - 1} and {n - 3, n - 1}, to get a contradiction. So 2  i  n -3. If T contains neither i -1 nor i +1, then no triple contains both i - 1 and i + 1, because the code is type IV. Shorten by deleting coordinate i and delete triple T and the triples (if any) containing pairs {i - 2, i + 1} and {i - 1, i + 2}, yielding a contradiction. Otherwise, without loss of generality T also contains i - 1 but does not contain i + 1. But then if some triple T contains i - 2 and i + 1, it cannot contain i . If T does not also contain i - 1, then we have T  {i - 1, i , i + 1} = {i - 1, i } and T  {i - 1, i , i + 1} = {i + 1}, which cannot happen in a type II code. So T = {i - 2, i , i + 1}. Hence there are at most two triples among those containing pairs {i - 1, i + 1}, {i - 2, i + 1}, and {i - 1, i + 2}, so shorten as before. Case 2 Some triple T satisfies |T  {i , i + 1, i + 2}| = 2 for some 0  i  n - 3. Suppose that {a , b} = T  {i , i + 1, i + 2} and let {c} = {i , i + 1, i + 2} \ {a , b}. There can be no triple containing c but neither a nor b, because the code is type II and type IV. So c is in exactly two triples, T that contains a and T that contains b; only T contains both a and b. Applying the same argument to T and T , a and b each appear in exactly two triples. So there are only three triples (T , T , and T ) that contain a , b, or c. Shorten by deleting coordinate i + 1 and the triples T , T , and T to obtain a contradiction. Case 3 No triple T satisfies |T  {i , i + 1, i + 2}| = 2 for any 0  i  n - 3. If a triple T satisfies |T  {i , i + 1, i + 2}| = 3 for some 0  i  n - 3, equivalently it satisfies |T  {i + 1, i + 2, i + 3}| = 2 for some 0  i  n - 4 or |T  {i - 1, i , i + 1}| = 2 for some 1  i  n - 3. Apply Case 2. Otherwise every triple T satisfies |T  {i , i + 1, i + 2}|  1 for 0  i  n - 3. But then ( X , B) is an LPSA(n , 3; 2) and hence we have at most B (n , 3; 2)  U (n , 3; 2) triples, the final contradiction.

4 Optimal packing sampling plans
By Corollary 2.2, we have the upper bound:
n -2 2

U (n , 3) =

2

+ (n - 2) 3

n -3 2

 2 n -4n +4  ,  6    n 2 -3n , 6 = n2 - 4n   6 ,   2  n -3n -4 ,
6

if n  2 (mod 6), if n  3 (mod 6), if n  0, 4 (mod 6), if n  1, 5 (mod 6).

Theorem 4.1 B (n , 3) = U (n , 3) for all n  0.

123

Optimal low-power coding for crosstalk avoidance

489

(n -1) -4(n -1)+4 3n 3 blocks, we get an LPSA(n - 1, 3) of size n - - n- by removing 6 2 = 6 the point n - 1 and all blocks containing it, which is optimal. When n  1, 5 (mod 6), Colbourn and Rosa [4] showed there exists an (n , 3)-packing of 2 n +2 size n -3 , whose leave graph consists of a cycle of length n - 1 and one isolated point. 6 Assume n - 1 is the isolated point. Remove the block {x , n - 2, n - 1} for some x  [0, n - 3]; 3 the result is an optimal LPSA(n , 3). Now, n - 1 appears in n - 2 blocks. Removing n - 1 and all blocks containing it from the optimal LPSA(n , 3) constructed above, we obtain an 2 (n -1)2 -4(n -1) n -4 3 LPSA(n - 1, 3) of size n -3 - n- , which is optimal. 6 2 = 6 n -3 2
2 2

Proof When n  3 (mod 6), Colbourn and Rosa [4] (and Colbourn and Ling [5]) construct 2 3n a BSA(n , 3) of size n - 6 , which is an optimal LPSA(n , 3). Because each point appears in

Theorem 4.2 1. B  (n , 3) = U (n , 3) when n  0, 3, 4 (mod 6). 2. B  (n , 3) = U (n , 3) - 1 when n  1, 2, 5 (mod 6).
-3) blocks when n  Proof The constructions in Theorem 4.1 yield a CPSA(n , 3) with n (n6 n (n -4) 3 (mod 6) and with 6 blocks when n  0, 4 (mod 6). A CPSA(n , 3) can have at most n n -3 blocks, which equals U (n , 3) when n  0, 3, 4 (mod 6), so these are optimal. 3 2 n -3 When n  2 (mod 6), n = U (n , 3) - 1 so B  (n , 3)  U (n , 3) - 1. When 3 2

n  1, 5 (mod 6), if there were a CPSA(n , 3) with U (n , 3) =
n (n -1) 2 3(n 2 -3n -4)

n 2 -3n -4 6

codewords, then the

- = n + 2. The leave must be an number of edges in the leave graph is 6 n -cycle with two additional edges, but every vertex in the leave must have even degree, which cannot occur. So B  (n , 3)  U (n , 3) - 1. To establish equality when n  1, 2, 5 (mod 6), remove the block {0, n - 1, x } from an LPSA(n , 3) from Theorem 4.1. Lemma 4.3 B  (n , 3; 2) = B (n , 3; 2) = U (n , 3; 2) whenever n  3, 5 (mod 6) and n  15. B  (n , 3; 2) + 2 = B (n , 3; 2) = U (n , 3; 2) whenever n  2, 4 (mod 6) and n  14. Proof Zhang and Chang [30] establish that whenever n  15 and n  3, 5 (mod 6), there is a -5) BSA(n , 3; 2) having n (n6 blocks; this is also an optimal CPSA(n , 3; 2) and LPSA(n , 3; 2). Now suppose that n  14 and n  2, 4 (mod 6). When n  2 (mod 6), writing n = 6t + 2, U (6t + 2, 3; 2) = (2t )(3t - 1). Delete element 6t + 2 from a BSA(6t + 3, 3; 2) with (2t + 1)(3t - 1) blocks, removing 3t - 1 blocks to obtain an LPSA(6t + 2, 3; 2), which is therefore optimal. When n  4 (mod 6), writing n = 6t + 4, U (6t + 4, 3; 2) = t (6t + 2). Delete element 6t + 4 from a BSA(6t + 5, 3; 2) with t (6t + 5) blocks, removing 3t blocks to obtain an LPSA(6t + 4, 3; 2), which is therefore optimal. Remove the blocks {0, n - 2, x }, {1, n - 1, y } for some x and y from the optimal LPSA(n , 3; 2) constructed above to obtain an optimal CPSA(n , 3; 2) when n  2, 4 (mod 6). For n = 6t , U (6t , 3) = 6t (t - 1) + 1, and
6t +1 3 6t -4 2 6t 3 6t -5 2

= 6t (t - 1). For n = 6t + 1,

U (6t +1, 3) = t (6t -3), and = t (6t -3)-1. However, if a CPSA(6t +1, 3; 2) were to have t (6t - 3) - 1 blocks, its leave must have 2(6t + 1) + 1 edges and every such graph with minimum degree 4 has two vertices of degree 5. Because all vertices in the leave must have even degree, no CPSA(6t + 1, 3; 2) can exist with more than t (6t - 3) - 2 blocks. We provide bounds to apply when n  0, 1 (mod 6). Lemma 4.4 B (2n , 3; 2)  4 B (n , 3), and B (2n + 1, 3; 2)  4 B (n , 3) + n - 2. In addition, B  (2n , 3; 2)  4 B  (n , 3), and B  (2n + 1, 3; 2)  4 B  (n , 3) + n - 3.

123

490

Y. M. Chee et al.

Proof Start with an LPSA(n , 3) on [0, n - 1]. We form an LPSA(2n , 3; 2) on [0, 2n - 1]. For each block {a , b, c} in the LPSA, form four blocks {{2a + , 2b + , 2c +  } : , ,   {0, 1},  +  +   0 (mod 2)}. The verification is straightforward. To form an LPSA(2n + 1, 3) on [0, 2n ], adjoin {{2i , 2i + 3, 2n } : 0  i  n - 3}. The construction for CPSAs is the same, except that one does not adjoin {0, 3, 2n }.

5 Conclusion
Applying Theorem 3.1 with the results in Theorem 4.1, we have optimal (n , 4, 3)-II codes for all n  30. By computer search (using cliquer [13] and hill-climbing (a variant of [24])), we determined the sizes of optimal LPSA(n , 3; )s, CPSA(n , 3; )s, and (n , 4, 3) codes of lengths n  20. The sizes are listed in Table 2 and corresponding optimal codes are available from the authors; those in slanted font are lower bounds from Theorem 3.1 and Lemma 3.4. In this paper, we present the first memoryless transition bus-encoding technique for power minimization, error-correcting and elimination of crosstalk simultaneously. We establish the connection between codes avoiding crosstalk of each type with packing sampling plans avoiding adjacent units. Optimal codes of each type are constructed.

References
1. Bertozzi D., Benini L., de Micheli G.: Low power error resilient encoding for on-chip data buses. In: DATE'02: Proceedings of the Conference on Design. Automation and Test in Europe, pp. 102­109. IEEE Computer Society, Washington, DC (2002). 2. Bertozzi D. Benini L., Ricco B.: Energy-efficient and reliable low-swing signaling for on-chip buses based on redundant coding. In: ISCAS'02: Proceedings of the IEEE International Symposium on Circuits and Systems, vol. 1, pp. 93­96. IEEE Press, Piscataway, NJ (2002). 3. Chee Y.M., Colbourn C.J., Ling A.C.H.: Optimal memoryless encoding for low power off-chip data buses. In: ICCAD'06: Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided Design, pp. 369­374. ACM Press, New York (2006). 4. Colbourn C.J., Rosa A.: Quadratic leaves of maximal partial triple systems. Graphs Comb. 2(4), 317­337 (1986). 5. Colbourn C.J., Ling A.C.H.: A class of partial triple systems with applications in survey sampling. Commun. Stat. Theory Methods 27(4), 1009­1018 (1998). 6. Duan C., Tirumala A., Khatri S.P.: Analysis and avoidance of crosstalk in on-chip buses. In: Hot Interconnects'01: Proceedings of the 9th Annual Symposium on High-Performance Interconnects, pp. 133­138. IEEE, Piscataway (2001). 7. Dukes P.J., Ling A.C.H.: Existence of balanced sampling plans avoiding cyclic distances. Metrika 70, 131­140 (2009). 8. Favalli M., Metra C.: Bus crosstalk fault-detection capabilities of error-detecting codes for on-line testing. IEEE Trans. Very Large Scale Integr. Syst. 7, 392­396 (1999). 9. Hamming R.W.: Error detecting and error correcting codes. Bell Syst. Tech. J. 29(2), 147­160 (1950). 10. Hedayat A.S., Rao C.R., Stufken J.: Sampling plans excluding contiguous units. J. Stat. Plan. Inference 19(2), 159­170 (1988). 11. Ho R.: On-chip wires: Scaling and efficiency, Ph.D. dissertation, Department of Electrical Engineering, Stanford University, Palo Alto, CA (2003). 12. Khan Z., Arslan T., Erdogan A.T.: A dual low-power and crosstalk immune encoding scheme for systemon-chip buses. In: PATMOS'04: Proceedings of the 14th International Workshop on Power and Timing Modeling, Optimization and Simulation. Lecture Notes in Computer Science, vol. 3254, pp. 585­592. Springer, Berlin (2004). 13. Niskanen S., Östergård P.R.J.: Cliquer User's Guide, Version 1.0, Communications Laboratory, Helsinki University of Technology, Espoo. Tech. Rep. T48, (2003). 14. Patel K.N., Markov I.L.: Error-correction and crosstalk avoidance in DSM busses. IEEE Trans. Very Large Scale Integr. Syst. 12(10), 1076­1080 (2004).

123

Optimal low-power coding for crosstalk avoidance

491

15. Ramprasad S., Shanbhag N.R., Hajj I.N.: A coding framework for low-power address and data busses. IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 7, 212­221 (1999). 16. Rossi D., van Dijk E.S., Kleihorst R.P., Nieuwland A.K., Metra C.: Coding scheme for low energy consumption fault-tolerant bus. In: IOLTW'02: Proceedings of the Eighth IEEE International On-Line Testing Workshop, pp. 8­12. IEEE Computer Society, Washington, DC (2002). 17. Rossi D., Cavallotti S., Metra C.: Error correcting codes for crosstalk effect minimization. In: DFT'03: Proceedings of the 18th IEEE International Symposium on Defect and Fault-Tolerance in VLSI Systems, pp. 257­266. IEEE Computer Society, Washington, DC (2003). 18. Rossi D., van Dijk E.S., Kleihorst R.P., Nieuwland, A.K., Metra, C.: Power consumption of fault tolerant codes: the active elements. In IOLTW'03: Proceedings of the Ninth IEEE International On-line Testing Workshop, pp. 61­67. IEEE Computer Society, Washington, DC (2003). 19. Samala N.K., Radhakrishnan D., Izadi B.: A novel deep sub-micron bus coding for low energy. In: ESA'04: Proceedings of the International Conference on Embedded Systems and Applications, pp. 25­30. CSREA Press, Leuven (2004). 20. Shannon C.E.: A mathematical theory of communications, Bell Syst. Tech. J. 27(3), 379­423, 623­656 (1948). 21. Sotiriadis P.P., Chandrakasan A.: Bus energy minimization by transition pattern coding (TPC) in deep sub-micron technologies. In: ICCAD'00--Proceedings of the 2000 IEEE/ACM International Conference on Computer-Aided Design, pp. 322­327. IEEE, Piscataway, NJ (2000). 22. Stan M.R., Burleson W.P.: Bus-invert coding for low-power I/O. IEEE Trans. Very Large Scale Integr. Syst. 3(1), 49­58 (1995). 23. Stan M.R., Burleson W.P.: Coding a terminated bus for low power. In: Great Lakes Symposium VLSI, pp. 70­73, Buffalo, NY (1995). 24. Stinson D.R.: Hill-climbing algorithms for the construction of combinatorial designs. In: Algorithms in Combinatorial Design Theory. Annals of Discrete Mathematics, vol. 26, pp. 321­334. Elsevier, NorthHolland (1985). 25. Stufken J.: Combinatorial and statistical aspects of sampling plans to avoid the selection of adjacent units. J. Comb. Inf. Syst. Sci. 18(1­2), 149­160 (1993). 26. Su C.L., Tsui C.Y., Despain A.M.: Saving power in the control path of embedded processors. IEEE Des. Comput. 11, 24­30 (1994). 27. Subrahmanya P., Manimegalai R., Kamakoti V., Mutyam M.: A bus encoding technique for power and cross-talk minimization. In: VLSI Design 2004: 17th International Conference on VLSI Design, pp. 443­448. IEEE Computer Society, Xi'an (2004). 28. Victor B., Keutzer K.: Bus encoding to prevent crosstalk delay. In: ICCAD'01--Proceedings of the 2001 IEEE/ACM International Conference on Computer-Aided Design, pp. 57­63. IEEE, Piscataway, NJ (2001). 29. Wright J.H., Stufken J.: New balanced sampling plans excluding adjacent units. J. Stat. Plan. Inference 138, 3326­3335 (2008). 30. Zhang J., Chang Y.X.: The spectrum of BSA(v, 3, ; ) with  = 2, 3. J. Comb. Des. 15, 61­76 (2007).

123

Augmentation is an operation to increase the number of symbols in a covering array, without unnecessarily increasing the number of rows. For covering arrays of strength two, one type of augmentation forms a covering array on  vv  symbols from one on  vâ1vâ1  symbols together with  vâ1vâ1  covering arrays each on two symbols. A careful analysis of the structure of the optimal binary covering arrays underlies an augmentation operation that reduces the number of rows required. Consequently a number of covering array numbers are improved.Software behavior depends on many factors, and some failures occur only when certain factors interact. This is known as an interaction triggered failure, and the corresponding selection of factor values can be modeled as a Minimal Failure-causing Schema (MFS). (An MFS involving m factors is an m-MFS.) Combinatorial Testing (CT) has been developed to exercise (âhitâ) all MFS with few tests. Adaptive Random Resting (ART) endeavors to make tests as different as possible, ensuring that testing of MFS is not unnecessarily repeated. Random Testing (RT) chooses tests at random without regard to the MFS already treated. CT might be expected to improve on RT for finding interaction triggered faults, and yet some studies report no significant difference. CT can also be expected to be better than ART, and yet other studies report that ART can be much better than RT. In light of these, the relative merits of CT, ART, and RT for finding interaction triggered faults are unclear.Gao SW, Lv JH, Du BL et al. Balancing frequencies and fault detection in the in-parameter-order algorithm. JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY 30(5): 957­968 Sept. 2015. DOI 10.1007/s11390-015-1574-6

Balancing Frequencies and Fault Detection in the In-Parameter-Order Algorithm
Shi-Wei Gao 1 ( ), Jiang-Hua Lv 1, ( 1 ) and Shi-Long Ma (
1 2

Ô å

ù×), Bing-Lei Du

1

(

), Charles J. Colbourn 2

State Key Laboratory of Software Development Environment, Beihang University, Beijing 100191, China School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe AZ 85287-8809, U.S.A.

E-mail: ge89@163.com; jhlv@nlsde.buaa.edu.cn; binglei.du@gmail.com; Charles.Colbourn@asu.edu E-mail: slma@nlsde.buaa.edu.cn Received March 16, 2015; revised June 29, 2015.
Abstract The In-Parameter-Order (IPO) algorithm is a widely used strategy for the construction of software test suites for combinatorial testing (CT) whose goal is to reveal faults triggered by interactions among parameters. Variants of IPO have been shown to produce test suites within reasonable amounts of time that are often not much larger than the smallest test suites known. When an entire test suite is executed, all faults that arise from t-way interactions for some fixed t are surely found. However, when tests are executed one at a time, it is desirable to detect a fault as early as possible so that it can be repaired. The basic IPO strategies of horizontal and vertical growth address test suite size, but not the early detection of faults. In this paper, the growth strategies in IPO are modified to attempt to evenly distribute the values of each parameter across the tests. Together with a reordering strategy that we add, this modification to IPO improves the rate of fault detection dramatically (improved by 31% on average). Moreover, our modifications always reduce generation time (2 times faster on average) and in some cases also reduce test suite size. Keywords combinatorial testing, IPO, test suite generation, expected time to fault detection, software under test

1

Introduction

Modern software systems are highly configurable. Their behavior is controlled by many parameters. Interactions among these parameters may cause severe failures, resulting in poor reliability. Therefore, software testing and reliability assessment are crucial in the design of effective software, as discussed in [1-3] for reliability and in [4-15] for software testing. Software testing serves two main purposes: 1) to ensure that software has as few errors as possible prior to release, and 2) to detect and isolate faults in the software. A generic model of such a software system identifies a finite set of parameters, and a finite set of possible values

for each parameter. Faults may arise due to a choice of a value for a single parameter, interactions among the values of a subset of the parameters, or a result of environmental conditions not included in the software model. We focus on the faults that arise from the parameters identified and the interactions among them. It is nearly always impractical to exhaustively test all combinations of parameter values because of resource constraints. Fortunately, this is not necessary in general: in some real software systems, more than 70 percent of faults are caused by interactions between two parameters[16], and all known faults are caused by interactions among six or fewer parameters[17-18] .

Regular Paper Special Section on Software Systems This work was supported by the National Natural Science Foundation of China under Grant Nos. 61300007 and 61305054, the Fundamental Research Funds for the Central Universities of China under Grant Nos. YWF-15-GJSYS-106 and YWF-14-JSJXY-007, and the Project of the State Key Laboratory of Software Development Environment of China under Grant Nos. SKLSDE-2015ZX-09 and SKLSDE-2014ZX-06.  Corresponding Author ©2015 Springer Science + Business Media, LLC & Science Press, China

958

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

For these reasons, combinatorial testing (CT) or tway testing chooses a strength t (the largest number of parameters interacting to cause a fault), and forms a software interaction test suite as follows. Every row of the test suite is a test or a test case. For each parameter in the system, each test specifies an admissible value for the parameter. The defining property is that, no matter how one chooses t parameters and an admissible value for each (a t-way interaction), at least one test has the specified parameters set to the indicated values. This coverage property ensures that every possible interaction among t or fewer parameter values must arise in at least one of the test cases. CT has proved to be an efficient testing technique for software[6,9,19]. Indeed, empirical studies have shown that t-way testing can effectively detect faults in various applications[17-18,20-22] . A primary objective in producing a test suite is to minimize the cost of executing the tests; hence minimizing the number of tests is desired. At the same time, however, the time to produce the test suite is also crucial. Hence the most effort has been invested in finding a variety of test suite generation algorithms. Some invest additional computational resources in minimizing the size of the test suite, while others focus on fast generation methods for test suites of acceptable but not minimum size. General methods providing fast generation have primarily involved greedy algorithms[9]. One-test-at-a-time methods start with an empty test suite, and keep track of the as-yetuncovered t-way interactions. Then repeatedly a test is selected, which attempts to maximize the number of such interactions that are covered by the test, until all interactions are covered. This strategy was pioneered in AETG[23] , and later proved to be within a constant factor of the optimal size[24-25] . In practice, maintaining a list of all t-way interactions can be prohibitive when the number of parameters is large. One-parameter-ata-time methods instead construct a test suite for t of the parameters (this contains all of the possible tests). Then it repeatedly adds a new parameter, and chooses a value for this parameter in each of the existing tests (horizontal growth). Because it is possible that some t-way interactions involving the new parameter have not been covered yet, further tests are selected to cover all such interactions (vertical growth). This requires maintaining a list of (t - 1)-way interactions, and hence can involve less bookkeeping. The pioneering example here is IPO[26] and its extensions, IPOG[27] , and IPOG-F and IPOG-F2[28], which will be discussed in more detail in Section 2. Both strategies typically pro-

duce test suites of acceptable size[26,29] . It has been observed that one-test-at-a-time methods produce slightly smaller test suites in general, while one-parameter-ata-time methods are somewhat faster at generation[26]. As mentioned earlier, software interaction test suites serve as two complementary roles[30]: to verify that no t-way interaction of SUT (software under test) causes a fault, or to locate such a fault. These two roles are different: certifying absence of a fault requires running the whole test suite, while locating a fault may not. Indeed in [30], it is shown that minimum test suite size is not the correct objective for fault location; the structure of the test suite can be more important than its size alone. An improved rate of fault detection can provide faster feedback to testers[31] . Recent studies have shown that CT is an effective fault detection technique and that early fault detection can be improved by reordering the generated test suites using interaction-based prioritization approaches[32-34] . Many strategies have been proposed to guide prioritization using evaluation measures such as interaction coverage based prioritization[30,35-39] and incremental interaction coverage based prioritization[40-41] . In [30], an evaluation measure of the expected time to fault detection is given. Test case prioritization techniques have been explored for the one-test-at-a-time methods, but little is known for the one-parameter-at-a-time methods. Bryce et al.[35-36,42] presented techniques that combine generation and prioritization. Pure prioritization[32-34,39] instead reorders an existing interaction test suite, using the metric of normalized average percentage of faults detected (NAPFD). However, existing pure prioritization techniques use explicit fault measurements of real systems, and hence are not directly suitable for the IPO algorithm. The main contributions of our work are: 1) We modify the IPO algorithm in order to accelerate the method and make it effective for fault detection. Our modifications attempt to make the values of each parameter more evenly distributed during generation. We focus on choosing values for the extension to an additional parameter during the horizontal growth of the algorithm and filling values for don't care positions. (See Section 3.) 2) We develop a pure prioritization technique (a reordering strategy) for the IPO algorithm based on the evaluation measure presented in [30]. Our method can reduce the expected time to fault detection effectively. (See Section 4.)

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO

959

3) We conduct experiments to demonstrate the effectiveness of the modifications (see Section 5). We conclude that the modifications to the IPO strategy result in faster generation (2 times faster on average according to the experimental results in Subsection 5.1), sometimes in smaller test suites, and together with the pure prioritization, in less time to detect the first fault (improved by 31% on average according to the experimental results in Subsection 5.2). 2 Framework of the IPO Algorithm

IPO comprises a family of methods of the oneparameter-at-a-time type. We focus on IPOG as a representative implementation. The basic operation is to add a new parameter to an existing interaction test suite of strength t. To initialize the method, whenever the number of parameters is at most t, all possible rows are included, which is necessary and sufficient to obtain a test suite. Thereafter, to introduce a new parameter, the set  of all t-way interactions involving the new parameter is computed. Horizontal growth adds a value of the new parameter to each existing row so that this extended row covers the most interactions in  ; the interactions covered are removed from  . Then if  still contains uncovered interactions, vertical growth adds new rows to cover them. This process is outlined in the flowchart in Fig.1. Existing variants of the IPO strategy alter the selection of values for the new parameter during horizontal growth and the selection of additional rows during

vertical growth. During both horizontal and vertical growth, it frequently happens that the value for one or more parameters in a row can be chosen arbitrarily without affecting the coverage of the row. Such entries are don't care positions[26] in the test suite. The IPO methods exploit the fact that selecting values for don't care positions can be deferred; then they can be filled during horizontal growth when the next parameter is introduced. Every variant of IPO must therefore deal with two basic problems: · choose values for the new parameter to maximize the number of uncovered interactions covered during horizontal growth; · assign values for don't care positions that arise. In the next section, we explore an implementation of this IPO framework in which the objective is not just to ensure coverage, but also to attempt to make each value appear as equally often as possible for each parameter. The latter is a balance condition. 3 Balance in the IPO Algorithm

A test suite must cover all t-way interactions. Consider a specific parameter and the t-way interactions that contain it. For each value of the parameter, the numbers of these t-way interactions with each different value of the parameter are the same. Now consider the frequencies of values of the parameter within the tests of a test suite. Because each value must provide the coverage of the same number of interactions, it appears to be reasonable to attempt to make the frequencies

Create Set  of Uncovered t-Way Combinations of Values Involving the Next Parameter

START

Horizontal Growth (Remove the Covered Combinatons from )

Build a t-Way Test Set for the First t Parameters

Yes  Is Empty?

All the Parameters Are Included in the Set?

No

No Vertical Growth (Remove the Covered Combinations from )

Yes END

Fig.1. Flowchart of IPOG algorithm.

960

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

close to equal. The same argument applies to fault detection. Two issues arise. First, current IPO algorithms do not make any explicit effort to balance the frequencies of values. Second, it is not at all clear how such an objective might affect the sizes of test suites produced, or the time to generate them, or their rates of fault detection. In this section, we develop modifications of IPO to address frequencies of values. Subsequent sections treat their impacts. 3.1 Choosing a New Parameter's Values

While shown in Algorithm 1 for IPOG, this simple strategy can also be used in IPOG-F and IPOG-F2. We show the modification for IPOG-F. The IPOG-F algorithm greedily selects over both the row and the value with which the covering array is extended, and the extended row/value pair (i; a) is greedily selected by the following formula[28] : tn = n-1 - T c [i ; a ], t-1

During horizontal growth, the IPOG algorithm chooses to add a value of the new parameter to cover the greatest number of interactions in  . In many situations, more than one value achieves this goal, and we must choose one. A naive strategy treats the values as ordered, and selects the smallest value that covers the most interactions in  . This introduces a bias towards the smaller values of each parameter, sometimes resulting in smaller values appearing much more frequently than larger ones. Here a different strategy, shown in Algorithm 1, is proposed. The essential change is to treat the values as being cyclically ordered, recording the value selected for the previous row. Then possible values for this row are considered by starting from the value following the previous one selected. For this modification, vertical growth remains unchanged.
Algorithm 1. Modified Horizontal Growth 1. Cov[r ; v] is the number of interactions that the extended row (r ; v) covers 2. q  |P | 3. prev  q 4. for each row r in the covering array ca do 5. max  (prev + 1) mod q 6. j  (max + 1) mod q 7. while j = ((prev + 1) mod q ) do 8. if Cov[r, vj ] > Cov[r, vmax ] then 9. max  j 10. end if 11. j  (j + 1) mod q 12. end while 13. r  (r, vmax ) 14. prev  max 15. end for

where n is the number of parameters, Tc [i; a] denotes the t-tuples that have previously been covered by already extended rows, and tn denotes the number of new t-tuples the row/value pair would cover if we extend row i with value a. The metric of optimal selection for the extended row (i; a) is that the extended row (i; a) would maximize tn . The original pseudo-code for horizontal growth in IPOG-F is shown in Algorithm 2. The modification replaces line 6 to line 10 of Algorithm 2 as shown in Algorithm 3. Similar modifications can be applied to IPOG-F2.
Algorithm 2. Horizontal Growth of IPOG-F 1. Tc [r ; a] is the number of t-tuples covered by (r ; a) 2. Cov[, v] is true if the interaction with column tuple  and value tuple v is covered false otherwise 3. Tc [i; a]  0, i, a 4. Cov[, v]  false, , a 5. while some row is non-extended do 6. Find non-extended row i and value a -1 7. so that tn = k - Tc [i; a] is maximum t -1 8. if tn = 0 then 9. Stop horizontal growth 10. end if 11. Extend row i with value a 12. for all non-extended row j do 13. S  set of columns where rows i and j have identical entries 14. for all column tuples   S do 15. v  the value tuple in row i and column tuple  16. if Cov[, v] = false then 17. Tc [j ; a]  Tc [j ; a] + 1 18. end if 19. end for 20. end for 21. for all column tuples  do 22. v  the value tuple in row r and column tuple  23. if Cov[, v] = false then 24. Cov[, v]  true 25. end if 26. end for 27. end while

Algorithm 1 incurs additional time to track the previous value selected, but this small addition is dominated by the computation of coverage, and hence makes no change in the complexity of the method.

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO
Algorithm 3. Modification (Lines 610) 1. max  (prev + 1) mod q 2. j  (max + 1) mod q 3. while j = ((prev + 1) mod q ) do 4. if Tc [i; vj ] < Tc [i, vmax ] then 5. max  j 6. j  (j + 1) mod q 7. end if 8. end while 9. a  vmax -1 10. tn  k - Tc [, a] t -1 11. if tn = 0 then 12. Stop horizontal growth 13. end if 14. Extend row i with value a 15. prev  max Algorithm 4. Addressing don't care Positions 1. Number the values of Pi as v1 , v2 , . . . , v|Pi | 2. f req [Pi , j ] is the frequency of value vj of Pi appears in the existing test set 3. e is an entry in column i 4. if e is a don't care position then 5. Find min that f req [Pi , min] is minimum in f req [Pi , 1], . . . , f req [Pi , |Pi |] 6. Assign e with vmin 7. end if

961

3.2

Addressing don't care Positions

our balance strategy only examines frequencies. Savings are only incurred with the balance strategy when don't care positions arise during vertical growth. In both cases, the worst-case complexity is dominated by the cost of horizontal growth, so in principle the two methods have the same asymptotic complexity. However, in practice, every don't care position results in a saving in computation time for the balance strategy. 4 Reducing the Expected Time to Fault Detection

In horizontal growth, when the maximum number of interactions that the extended row (r; v ) can cover is 0, the value at this position is a don't care. The don't care positions can be addressed using the method of Subsection 3.1. In vertical growth, new rows that are created to cover the t-way combinations in  not covered by horizontal growth can leave positions not needed to cover interactions in  as don't care. The selection of these values can influence the extension for the remaining parameters. To exploit these don't care positions, one strategy focuses on coverage, and the other on balance. The balance strategy attempts to make values of all parameters distributed evenly: as each don't care arises, it is filled with a value for this parameter that currently appears the least often; ties are handled by taking the next in the cyclic order of values after the previous selection. The coverage strategy is greedy. Don't care positions produced in vertical growth are left unassigned until the next horizontal growth. Then a value is chosen so that the row covers the most uncovered interactions, using the method described in Subsection 3.1. Focusing on coverage is generally slightly superior in reducing the size of test suites. However, the balance strategy reduces the time to generate the test suite. Because of our interest in fault detection, and the fact that existing IPO variants use a coverage strategy, we adopt the balance strategy here. The pseudo-code for the balance strategy is shown in Algorithm 4. Vertical growth treating don't care positions using a coverage strategy examines all t-way interactions, while

In [30], a measurement of the goodness of a test suite at detecting a fault is defined. Suppose that every test takes the same time to run. Further suppose that faults are randomly distributed among the t-way interactions, and that there is no a priori information about their location. For a system with s faults, the expected time to fault detection is determined by the expected number of tests to detect the presence of a fault. s denotes the expected number of tests to detect the first fault in a system with s faults. s =
N ui i=1 s  s

.

Here ui is the number of uncovered interactions before executing the i-th row, N is the number of rows of the test suite, and  is the total number of t-way interactions. This measure applies to any test suite when faults arise randomly, and is not intended to examine particular patterns of faults in specific systems. As such, it can serve as a means to evaluate test suites for use in an as-yet-unknown application. Minimizing the expected time to fault detection means constructing a test suite to minimize s given s. Rather than constructing a test suite to minimize s directly, we can reorder the rows of a test suite to reduce s . Because all faults of interest are caused by parameter interactions, the more uncovered interactions con-

962

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

tained in the test, the more likely a fault is to be revealed. Hence placing the tests that cover the greatest number of the uncovered interactions early can increase the probability of detecting a fault. To see this, we rewrite the formula as follows: s =
N ui i=1 s  s N

=
i=1

ui s  s

.

Then the problem becomes minimizing the average i (u s ) value of  , the likelihood that all faults remain unde(s) tected after running i tests. The method for reordering the test suite is Algorithm 5. There may be a tie for row rj where Tc [rj ] is the largest -- if there is, the tie would be broken randomly.
Algorithm 5. Reordering Test Suites 1. n  N 2. for j from 1 to n do 3. 4. for each row r1 , . . . , rn Determine the number Tc [ri ] of t-way interactions covered in ri but not covered in r1 , . . . , ri-1 5. 6. 7. 8. 9. 10. 11. 12. 13. end for end for Choose a row rj from ri , . . . , rn for which Tc [rj ] is the largest if Tc [rj ] = 0 then Remove all rows ri , . . . , rn from the suite n i-1 else Swap ri and rj in the suite end if

5

Experiments

We employ the tool ACTS-2.8 (Advanced Combinatorial Testing System)[43] , including implementations of IPOG, IPOG-F and IPOG-F2, etc. We compare the tool ACTS-2.8 with our variants of IPOG, IPOG-F and IPOG-F2 in which the handling of don't care positions attempts to balance frequencies of values; our versions are coded in C++. All of the experimental results reported here are performed on a laptop with CoreTM 2 Duo Intel processor clocked at 2.60 GHz and 4 GB memory. 5.1 Test Suite Size and Execution Time

First we examine the relative performance for different numbers of values for the parameters. The notation dt indicates that there are t parameters, each with

d values. To start, we vary the number of values. Table 1 shows execution time and test suite sizes when the strength is 4, and there are five parameters whose number of values is 5, 10, 15, or 20. As expected, the execution time for our methods is substantially smaller (see Fig.2). What is more surprising is that our methods consistently produce test suites no larger than the original methods, and sometimes produce much smaller ones. Now we vary the number of parameters. Table 2 shows results when the strength is 4, the number of parameters is 10, 15, 20, or 25, and the number of values is 5. Again the execution time for our methods shows improvements (see Fig.3). However, as the number of parameters increases, the deferral in filling don't care positions by the original methods generally produces smaller test suite sizes. Now we vary the strength. Table 3 presents results for 106 when the strength is 2, 3, 4, or 5. Once again, the execution time for our methods is substantially lower (see Fig.4). Our methods do not fare as well with respect to test suite size, but appear to be very effective when the strength is larger. Our methods appear to improve execution time consistently as expected. Nevertheless, they also improve on test suite sizes in some cases, especially when the strength is large or the number of values is large. Real systems rarely have the same number of values for each parameter, so we also consider situations in which different parameters can have different numbers of values. Table 4 presents results with strength 4 for five different sets of numbers of values for 10 parameters. Execution time improvements again arise for our algorithms. Moreover, a pattern for test suite sizes is clear: our methods improve when there is more variation in numbers of values. Next we examine the relative performance using the Traffic Collision Avoidance System (TCAS), which has been utilized in several other studies of software testing[27,44-46] . TCAS has 12 parameters: seven parameters have two values, two parameters have three values, one parameter has four values, and two parameters have 10 values. Table 5 gives the results. (In [46], similar results for the original IPOG versions are given for the TCAS system.) While our improvements in execution time are evident, no obvious pattern indicates which method produces the smallest test suite. Our methods have simplified the manner in which don't care positions are treated in order to balance the frequencies of values. Our experimental results all con-

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO
Table 1. Results for Five Parameters with 5 to 20 Values for 4-Way Testing Parameter Config. 55 105 155 205 Our IPOG Size 000 745 011 990 058 410 184 680 Time (s) 0.001 0.078 1.101 9.666 IPOG(ACTS) Size Time (s) 000 790 000.015 012 298 000.827 061 945 016.329 191 652 120.220 Our IPOG-F Size Time (s) 000 625 000.000 010 000 000.673 050 625 018.469 160 000 200.020 IPOG-F(ACTS) Size Time (s) 000625 1 000.047 010 000 1 006.109 050 625 1 146.730 160 000 1 376.000 Our IPOG-F2 Size Time (s) 000 625 000.000 010 000 000.500 050 625 012.782 160 000 209.290

963

IPOG-F2(ACTS) Size Time (s) 100 788 1 000.031 112 394 1 004.859 161 615 1 184.450 192 082 1 966.200

140 120 100 Time (s) Time (s) 80 60 40 20 0 5 10 15 Domain Size (a) 20
Our IPOG IPOG(ACTS)

1400 1200 1000 800 600 400 200 0 5 10 15 Domain Size (b) 20 Time (s)
Our IPOG-F IPOG-F(ACTS)

2000
Our IPOG-F2 IPOG-F2(ACTS)

1500

1000

500

0

5

10 15 Domain Size (c)

20

Fig.2. Execution time, varying the number of values (4-way). (a) IPOG. (b) IPOG-F. (c) IPOG-F2. Table 2. Results for 10 to 25 5-Value Parameters for 4-Way Testing Parameter Config. 510 515 520 525 Our IPOG Size 1 890 2 584 3 114 3 540 Time (s) 0.056 0.517 2.140 7.012 IPOG(ACTS) Size 1 859 2 534 3 032 3 434 Time (s) 00.188 00.954 04.094 16.049 Our IPOG-F Size 1 833 2 461 2 951 3 338 Time (s) 000.625 007.109 034.361 111.150 IPOG-F(ACTS) Size 1 882 2 454 2 898 3 279 Time (s) 001.750 014.579 060.987 176.340 Our IPOG-F2 Size 1 965 2 736 3 308 3 763 Time (s) 0.187 1.282 4.329 8.752 IPOG-F2(ACTS) Size 1 905 2 644 3 180 3 589 Time (s) 0.297 1.421 4.344 9.188

10 15 Time (s) Our IPOG IPOG(ACTS) Time (s) 150 Our IPOG-F IPOG-F(ACTS) Time (s) 8 6 4 2 0 10 0 0 Our IPOG-F2 IPOG-F2(ACTS)

10

100

5

50

15 20 25 Number of Parameters (a)

10

15 20 25 Number of Parameters (b)

10

15 20 25 Number of Parameters (c)

Fig.3. Execution time, increasing the number of parameters (4-way). (a) IPOG. (b) IPOG-F. (c) IPOG-F2. Table 3. Results for Six 10-Value Parameters for 25-Way Testing t Our IPOG Size 2 000 149 3 001 633 4 016 293 5 123 060 Time (s) 0.000 0.010 0.195 5.139 IPOG(ACTS) Size 000 130 001 633 016 496 130 728 Time (s) 000.005 000.059 004.276 116.470 Our IPOG-F Size 000 133 001 577 015 594 100 000 Time (s) 00.000 00.047 02.704 88.692 IPOG-F(ACTS) Size 000 134 001 553 015 467 100 000 Time (s) 000.031 000.266 018.126 575.150 Our IPOG-F2 Size 000 135 001 629 015 631 100 000 Time (s) 00.000 00.032 01.594 54.971 IPOG-F2(ACTS) Size 000 134 001 625 016 347 132 428 Time (s) 000.016 000.140 009.297 449.330

964

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5
Table 4. Results for Five Systems with Different Numbers of Values in 4-Way Testing

Parameter Config. 1010 105 95 153 104 53 161 152 104 52 41

Our IPOG Size 29 915 23 878 41 128 42 913 Time (s) 1.942 1.295 1.734 1.750 1.844

IPOG(ACTS) Size 29 466 23 961 45 128 47 591 52 991 Time (s) 28.040 14.583 13.689 14.532 14.860
600

Our IPOG-F Size 28 437 22 521 41 505 43 774 48 847 Time (s) 129.57 094.27 236.87 249.37 235.95

IPOG-F(ACTS) Size 28 079 22 726 43 306 45 693 50 287 Time (s) 359.17 248.04 757.68 289.72 333.89

Our IPOG-F2 Size Time (s) 31 744 045.237 25 222 031.611 46 509 162.850 48 660 148.510 54 099 189.290

IPOG-F2(ACTS) Size 30 986 24 741 48 295 51 147 57 634 Time (s) 053.440 039.736 262.170 149.510 199.810

171 161 151 104 51 42 47 248
120 100 Time (s) 80 60 40 20 0 2

Our IPOG IPOG(ACTS)

500 Time (s) 400 300 200 100 0

Our IPOG-F IPOG-F(ACTS)

400 300 200 100 0

Our IPOG-F2 IPOG-F2(ACTS)

3 4 Strength of Coverage (a)

5

2

3 4 Strength of Coverage (b)

5

Time (s)

2

3 4 Strength of Coverage (c)

5

Fig.4. Execution time, increasing the test strength. (a) IPOG. (b) IPOG-F. (c) IPOG-F2. Table 5. Results for TCAS t 2 3 4 5 6 Our IPOG Size 00 100 00 404 01 306 04 464 11 774 Time (s) 0.001 0.009 0.065 0.411 1.463 IPOG(ACTS) Size 00 100 00 400 01 359 04 233 11 021 Time (s) 0.002 0.007 0.031 0.219 3.233 Our IPOG-F Size 00 100 00 400 01 269 04 068 11 381 Time (s) 00.002 00.025 00.323 04.104 32.870 IPOG-F(ACTS) Size 00 100 00 402 01 349 04 245 11 257 Time (s) 000.015 000.087 001.117 013.405 101.330 Our IPOG-F2 Size 00 100 00 431 01 639 05 129 13 323 Time (s) 00.004 00.044 00.489 04.133 18.030 IPOG-F2(ACTS) Size 00 100 00 438 01 653 05 034 13 379 Time (s) 00.017 00.061 00.572 04.379 20.959

firm that this can dramatically reduce the execution time. One might have expected a substantial degradation in the test suite sizes produced. However, our results indicate not only that the balancing strategy is competitive, but also that it can improve test suite sizes. Fast methods such as IPO do not generally produce the smallest test suites possible. To illustrate this, we apply a post-optimization method from [4748] to some of the TCAS results. For strength 4, we treat the solutions for IPOG-F2; within 10 minutes of computation, post-optimization reduces the solution by our method from 1 639 to 1 201 rows, and the solution by the original method from 1 653 to 1 205 rows. For strength 5, we treat the solutions for IPOG-F; within one hour of computation, post-optimization reduces the solution by our method from 4 068 to 3 600 rows, and the solution by the original method from 4 245 also to 3 600 rows. For strength 6, we treat the solutions

for IPOG; within 10 hours, post-optimization reduces the solution by our method from 11 774 to 9 794 rows, and the solution by the original method from 11 021 to 9 798 rows. By contrast, in a comparison of six different one-parameter-at-a-time methods[46] , the best result has 10 851 rows. While the test suites from oneparameter-at-a-time methods are therefore definitely not the smallest, post-optimization is much more timeconsuming and it requires a test suite as input. As the number of parameters increases, the speed with which an initial test suite can be constructed is crucial. 5.2 Expected Time to Fault Detection

Accelerating the IPO methods, even with a possible loss of accuracy in test suite size, can be worthwhile. However, a second concern is with potential performance in revealing faults. We examine the TCAS system, using our and the original versions of the three IPO variants. We examine the time to find the first

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO

965

fault when 1, 2, or 3 faults are present and when the strength is between 2 and 6. In our model, the time to execute each test is the same, so the expected time is directly proportional to the expected number of tests or rows needed. We consider test suites before and after our reordering. Table 6 gives the results. To assess the efficacy of our modifications, we report two lines for each method and each strength; the first reports results for our methods, and the second for the original methods. 1 , 2 and 3 denote the expected number of tests to detect the first fault when there are one, two or three faults that are randomly chosen. These results indicate that reordering is effective in reducing the time to fault detection, both for our met-

hods and for the original ones. Fig.5 shows 2 for each strength before and after the reordering for our methods, showing a substantial reduction from reordering. Fig.6 instead shows the expected number of tests when zero, one, two, or three faults are present. It appears that the reordering method is the most effective when the number of faults is small. This should be expected, because the presence of many faults ensures that one will be found early no matter what ordering is used. Our methods, despite often producing larger test suites, fare well with respect to expected time to fault detection. Comparing the performance of ours and the original IPOG when t = 6, for example, although our test suite is larger, it would yield smaller expected time to detect faults once reordered. Evidently the size of the

Table 6. Expected Time to Fault Detection for TCAS Before and After Reordering Algorithm t 1 Before IPOG 2 3 4 5 6 IPOG-F 2 3 4 5 6 IPOG-F2 2 3 4 5 6 00 24.80 00 24.81 0 117.90 0 117.68 0 407.20 0 408.42 1 348.26 1 348.14 3 015.32 3 007.69 00 28.36 00 27.19 0 120.96 0 120.94 0 411.37 0 411.97 1 353.42 1 354.28 3 076.17 3 017.29 00 26.44 00 26.27 0 120.61 0 121.04 0 419.07 0 421.75 1 378.15 1 377.84 3 129.21 3 138.02 After 00 19.65 00 19.65 00 82.15 00 82.12 0 275.38 0 276.34 0 850.74 0 848.20 2 127.94 2 140.04 00 20.43 00 20.67 00 81.47 00 81.34 0 272.86 0 269.36 0 828.83 0 822.73 2 090.57 2 059.33 00 20.52 00 20.19 00 82.75 00 81.63 0 275.05 0 278.10 0 844.54 0 838.77 2 127.62 2 121.97 Before 00 10.72 00 10.73 00 53.30 00 53.18 0 200.45 0 201.60 0 707.82 0 708.24 1 682.31 1 680.95 00 12.73 00 12.18 00 55.81 00 55.99 0 204.59 0 204.73 0 716.08 0 715.52 1 722.82 1 693.94 00 11.90 00 11.67 00 55.26 00 55.61 0 207.11 0 208.20 0 724.94 0 725.02 1 732.44 1 736.29 Number of Faults 2 After 000 9.26 000 9.26 00 38.57 00 38.47 0 131.93 0 132.73 0 421.49 0 421.30 1 095.54 1 106.03 000 9.58 000 9.67 00 38.33 00 38.18 0 132.18 0 129.68 0 411.92 0 410.22 1 065.49 1 063.99 000 9.75 000 9.56 00 38.36 00 38.15 0 130.39 0 131.82 0 412.79 0 409.71 1 068.73 1 062.64 Before 000 6.27 000 6.27 00 30.08 00 30.03 0 118.33 0 119.08 0 436.03 0 436.40 1 097.27 1 096.56 000 7.29 000 7.08 00 31.84 00 32.13 0 121.66 0 121.75 0 444.08 0 443.26 1 129.80 1 109.05 000 6.98 000 6.80 00 31.49 00 31.76 0 123.20 0 123.79 0 449.34 0 449.50 1 133.78 1 136.22 3 After 005.83 005.85 023.69 023.56 082.38 082.77 268.03 268.37 719.43 725.45 006.01 006.02 023.71 023.72 082.83 081.77 263.23 261.36 698.08 700.68 006.13 006.00 023.84 023.55 081.95 082.43 263.65 261.35 699.08 695.18

966
2000 Expected Number of Tests Expected Number of Tests 1500 1000 500 0 Before Reorder After Reorder 2000

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5
2000 Expected Number of Tests Before Reorder After Reorder 1500 1000 500 0 Before Reorder After Reorder

1500 1000 500 0

2

3 4 5 Test Strength (a)

6

2

3 4 5 Test Strength (b)

6

2

3 4 5 Test Strength (c)

6

Fig.5. Expected number of tests for 2 . (a) IPOG. (b) IPOG-F. (c) IPOG-F2.

Expected Number of Tests

Expected Number of Tests

Before Reorder After Reorder 1000

Before Reorder After Reorder 1000

Expected Number of Tests

1500

1500

2000 Before Reorder After Reorder

1500

1000

500

500

500

0

0

1 2 3 Number of Faults (a)

4

0

0

1 2 3 Number of Faults (b)

4

0

0

1 2 3 Number of Faults (c)

4

Fig.6. Expected number of tests, increasing the number of faults in TCAS (4-way). (a) IPOG. (b) IPOG-F. (c) IPOG-F2.

test suite, while relevant, is not the only factor affecting the expected time. Our results suggest that faster IPO implementations remain competitive, and hence that the objective of balancing frequencies of values is a reasonable one to pursue. 6 Conclusions

We identified three main goals in generating a test suite: time to generate the test suite, time to execute the test suite (test suite size), and the rate of fault detection. Our methods focus on reducing the time for generation, without severe negative impact on test suite size and fault detection. We accelerated variants of the IPO method by simplifying the manner in which don't care positions are filled. This results in a consistent improvement in the execution time to construct a test suite, but sacrifices to some extent the algorithm's ability to exploit such positions in repeated horizontal growth phases. This is reflected in our experimental results. While in numerous cases, our modifications find smaller test suites, in the others they do not. This occurs particularly when the number of parameters is large.

Any method to fill don't care positions immediately would be expected to accelerate the methods; however we devised a simple method that strives to balance the frequency of values for each parameter. We argued that such an objective can result in more effective horizontal growth, and that it can permit us to retain effective rates of fault detection. Both of these motivations are borne out by the experimental data. One-test-at-a-time generation methods explicitly aim for good rates of fault detection by covering interactions early in the test suite, while one-parameterat-a-time methods like IPO do not. Nevertheless, we showed that a reordering strategy can be applied to make dramatic improvement on the rate of fault detection. If test suite size is a primary objective, using our methods together with randomized postoptimization[47-48] appears to be worthwhile. If expected time to fault detection is paramount, extending reordering to discover and replace don't care positions appears to be viable. Both merit further study. We suggest that both can benefit from balancing frequencies of values, a fast and simple way to generate useful test suites.

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO

967

References
[1] Birnbaum Z W. On the importance of different components in a multicomponent system. In Multivariate Analysis, Krishnaiah P R (ed.), New York: Academic Press, 1969, pp.591-592. [2] Kuo W, Zhu X. Relations and generalizations of importance measures in reliability. IEEE Trans. Rel., 2012, 61(3): 659674. [3] Kuo W, Zhu X. Some recent advances on importance measures in reliability. IEEE Trans. Rel., 2012, 61(2): 344-360. [4] Anand S, Burke E K, Chen T Y et al. An orchestrated survey of methodologies for automated software test case generation. J. Sys. Software, 2013, 86(8): 1978-2001. [5] Chen T Y, Kuo F C, Liu H et al. Code coverage of adaptive random testing. IEEE Trans. Rel., 2013, 62(1): 226-237. [6] Grindal M, Offutt J, Andler S F. Combination testing strategies: A survey. Softw. Test. Verif. Rel., 2005, 15(3): 167-199. [7] Hao D, Zhang L M, Zhang L et al. A unified test-case prioritization approach. ACM Trans. Soft. Eng. Method, 2014, 24(2): 10:1-10:31. [8] Harman M, McMinn P. A theoretical and empirical study of search-based testing: Local, global, and hybrid search. IEEE Trans. Software Eng., 2010, 36(2): 226-247. [9] Nie C H, Leung H. A survey of combinatorial testing. ACM Comput. Surv., 2011, 43(2): 11:1-11:29. [10] Nebut C, Fleurey F, Le Traon Y et al. Automatic test generation: A use case driven approach. IEEE. Trans. Software Eng., 2006, 32(3): 140-155. [11] Perrouin G, Oster S, Sen S et al. Pairwise testing for software product lines: Comparison of two approaches. Software. Qual. J., 2012, 20(3/4): 605-643. [12] Xie T, Zhang L, Xiao X et al. Cooperative software testing and analysis: Advances and challenges. Journal of Computer Science and Technology, 2014, 29(4): 713-723. [13] Yoo S, Harman M. Regression testing minimization, selection and prioritization: A survey. Softw. Test. Verif. Rel., 2012, 22(2): 67-120. [14] Zhang D M, Xie T. Software analytics: Achievements and challenges. In Proc. the 35th Int. Conf. Software Eng., May 2013, p.1487. [15] Yu K, Lin M, Chen J et al. Towards automated debugging in software evolution: Evaluating delta debugging on real regression bugs from the developers' perspectives. J. Sys. Software, 2012, 85(10): 2305-2317. [16] Bryce R C, Colbourn C J. One-test-at-a-time heuristic search for interaction test suites. In Proc. the 9th Annu. Conf. Genetic and Evolutionary Computation, Jul. 2007, pp.1082-1089. [17] Kuhn D R, Reilly M J. An investigation of the applicability of design of experiments to software testing. In Proc. the 27th Annu. NASA Goddard Workshop on Software Eng., Dec. 2002, pp.91-95. [18] Kuhn D R, Wallace D R, Gallo Jr J A. Software fault interactions and implications for software testing. IEEE Trans. Software Eng., 2004, 30(6): 418-421. [19] Cohen M B, Dwyer M B, Shi J. Constructing interaction test suites for highly-configurable systems in the presence of constraints: A greedy approach. IEEE Trans. Software Eng., 2008, 34(5): 633-650.

[20] Lei Y, Kacker R, Kuhn D R et al. IPOG/IPOG-D: Efficient test generation for multi-way combinatorial testing. Softw. Test. Verif. Rel., 2008, 18(3): 125-148. [21] Tung Y W, Aldiwan W S. Automating test case generation for the new generation mission software system. In Proc. IEEE Aerospace Con., March 2000, pp.431-437. [22] Wallace D R, Kuhn D R. Failure modes in medical device software: An analysis of 15 years of recall data. Int. J. Rel., Quality and Safety Eng., 2001, 8(4): 351-371. [23] Cohen D M, Dalal S R, Kajla A et al. The automatic efficient tests generator (AETG) system. In Proc. the 5th Int. Sympo. Software Rel. Eng., Nov. 1994, pp.303-309. [24] Bryce R C, Colbourn C J. The density algorithm for pairwise interaction testing. Softw. Test. Verif. Rel., 2007, 17(3): 159-182. [25] Bryce R C, Colbourn C J. A density-based greedy algorithm for higher strength covering arrays. Softw. Test. Verif. Rel., 2009, 19(1): 37-53. [26] Lei Y, Tai K C. In-parameter-order: A test generation strategy for pairwise testing. In Proc. the 3rd Int. Symp. HighAssurance Sys. Eng., Nov. 1998, pp.254-261. [27] Lei Y, Kacker R, Kuhn D R et al. IPOG: A general strategy for t-way software testing. In Proc. the 14th Annu. Int. Conf. Worshop. Eng. Computer-Based Sys., March 2007, pp.549-556. [28] Forbes M, Lawrence J, Lei Y, Kacker R N, Kuhn D R. Refining the in-parameter-order strategy for constructing covering arrays. Journal of Research of the National Institute of Standards and Technology, 2008, 113(5): 287-297. [29] Cohen M B, Gibbons P B, Mugridge W B et al. Constructing test cases for interaction testing. In Proc. the 25th Int. Conf. Software Eng., May 2003, pp.38-48. [30] Bryce R C, Colbourn C J. Expected time to detection of interaction faults. J. Combin. Mathematics and Combin. Comput., 2013, 86: 87-110. [31] Rothermel G, Untch R H, Chu C et al. Prioritizing test cases for regression testing. IEEE Trans. Software Eng., 2001, 27(10): 929-948. [32] Qu X, Cohen M B. A study in prioritization for higher strength combinatorial testing. In Proc. the 6th Int. Con. Software Testing, Verification and Validation, the 2nd Int. Workshops on Combinatorial Testing, March 2013, pp.285294. [33] Qu X. Configuration aware prioritization techniques in regression testing. In Proc. the 31st Int. Conf. Software Engineering, Companion Volume, May 2009, pp.375-378. [34] Qu X, Cohen M B, Rothermel G. Configuration-aware regression testing: An empirical study of sampling and prioritization. In Proc. Int. Symp. Software Tesing and Analysis, July 2008, pp.75-86. [35] Bryce R C, Colbourn C J. Test prioritization for pairwise interaction coverage. In Proc. the 1st Int. Workshop on Advances in Model-Based Testing, May 2005. [36] Bryce R C, Colbourn C J. Prioritized interaction testing for pair-wise coverage with seeding and constraints. Inform. Software Tech., 2006, 48(10): 960-970. [37] Huang R, Chen J, Li Z, Wang R, Lu Y. Adaptive random prioritization for interaction test suites. In Proc. the 29th Symp. Appl. Comput., March 2014, pp.1058-1063.

968
[38] Petke J, Yoo S, Cohen M B, Harman M. Efficiency and early fault detection with lower and higher strength combinatorial interaction testing. In Proc. the 12th Joint Meeting on European Software Engineering Conf. and the ACM SIGSOFT Symp. the Foundations of Software Eng. (ESEC/FSE 2013), August 2013, pp.26-36. [39] Qu X, Cohen M B, Woolf K M. Combinatorial interaction regression testing: A study of test case generation and prioritization. In Proc. the 23rd Int. Conf. Software Maintenance, Oct. 2007, pp.255-264. [40] Huang R, Chen J, Zhang T, Wang R, Lu Y. Prioritizing variable-strength covering array. In Proc. the 37th IEEE Annu. Computer Software and Applications Conf., July 2013, pp.502-511. [41] Huang R, Xie X, Towey D, Chen T Y, Lu Y, Chen J. Prioritization of combinatorial test cases by incremental interaction coverage. Int. J. Softw. Eng. Know., 2014, 23(10): 1427-1457. [42] Bryce R C, Memon A M. Test suite prioritization by interaction coverage. In Proc. Workshop on Domain Specific Approaches to Software Test Automation, September 2007, pp.1-7. [43] Lei Y, Kuhn D R. Advanced combinatorial testing suite (ACTS). http://csrc.nist.gov/groups/SNS/acts/index.html, Aug. 2015. [44] Hutchins M, Foster H, Goradia T et al. Experiments of the effectiveness of dataflow and control-flow-based test adequacy criteria. In Proc. the 16th Int. Conf. Software Eng., May 1994, pp.191-200. [45] Kuhn D R, Okun V. Pseudo-exhaustive testing for software. In Proc. the 30th Annu. IEEE/NASA Software Engineering Workshop, April 2006, pp.153-158. [46] Soh Z H C, Abdullah S A C, Zamil K Z. A distributed tway test suite generation using "One-Parameter-at-a-Time" approach. Int. J. Advance Soft Compu. Appl., 2013, 5(3): 91-103. [47] Li X, Dong Z, Wu H et al. Refining a randomized postoptimization method for covering arrays. In Proc. the 7th IEEE Int. Conf. Software Testing, Verification and Validation Workshops (ICSTW), March 31-April 4, 2014, pp.143152. [48] Nayeri P, Colbourn C J, Konjevod G. Randomized postoptimization of covering arrays. Eur. J. Combin., 2013, 34(1): 91-103.

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

Jiang-Hua Lv received her B.S. and Ph.D. degrees in computer science from Jilin University, Changchun, in 1998 and 2003, respectively. Currently she is an assistant professor in the School of Computer Science and Engineering of Beihang University, Beijing. She is a member of the State Key Laboratory of Software Development Environment of Beihang University. Her research focuses on formal theory and technology of software, theory and technology of testing, automatic testing of safety critical systems, and device collaboration. Bing-Lei Du is currently an undergraduate in the School of Computer Science and Engineering of Beihang University, Beijing, and has been an intern in the State Key Laboratory of Software Development Environment of Beihang University since 2013. His research interest is software testing. Charles J. Colbourn earned his Ph.D. degree in computer science from the University of Toronto in 1980, and is a professor of computer science and engineering at Arizona State University. He is the author of The Combinatorics of Network Reliability (Oxford), Triple Systems (Oxford), and 320 refereed journal papers focusing on combinatorial designs and graphs with applications in networking, computing, and communications. In 2004, he was awarded the Euler Medal for Lifetime Research Achievement by the Institute for Combinatorics and its Applications. Shi-Long Ma is currently a professor and doctor tutor of the School of Computer Science and Engineering of Beihang University, Beijing. He is a member of the State Key Laboratory of Software Development Environment of Beihang University. His main research focus is on computation models in networks, logic reasoning and behaviors in network computing, and the theory of automatic testing.

Shi-Wei Gao received his B.S. degree in computer science and technology from Dezhou University, Dezhou, in 2007, and M.S. degree in information science and engineering from Yanshan University, Qinhuangdao, in 2010. He is currently a Ph.D. candidate in the School of Computer Science and Engineering of Beihang University, Beijing. He is a member of the State Key Laboratory of Software Development Environment of Beihang University. His research interests include software testing, software reliability theory, and formal methods.

Locating Arrays: A New Experimental Design for Screening Complex Engineered Systems
Abraham N. Aldaco, Charles J. Colbourn, and Violet R. Syrotiuk
School of Computing, Informatics, and Decision Systems Engineering Arizona State University, Tempe, AZ, U.S.A. 85287-8809

{aaldacog, colbourn, syrotiuk}@asu.edu ABSTRACT
The purpose of a screening experiment is to identify significant factors and interactions on a response for a system. Engineered systems are complex in part due to their size. To apply traditional experimental designs for screening in complex engineered systems requires either restricting the factors considered, which automatically restricts the interactions to those in the set, or restricting interest to main effects, which fails to consider any possible interactions. To address this problem we propose a locating array (LA) as a screening design. Locating arrays exhibit logarithmic growth in the number of factors because their focus is on identification rather than on measurement. This makes practical the consideration of an order of magnitude more factors in experimentation than traditional screening designs. We present preliminary results applying an LA for screening the response of TCP throughput in a simulation model of a mobile wireless network. The full-factorial design for this system is infeasible (over 1043 design points!) yet an LA has only 421 design points. We validate the significance of the identified factors and interactions independently using the statistical software JMP. Screening using locating arrays is viable and yields useful models. Screening: Which factors and interactions are most influential on a response? Confirmation: Is the system currently performing in the same way as it did in the past? Discovery: What happens when new operating conditions, materials, factors, etc., are explored? Robustness: Under what conditions does a response degrade? Stability: How can variability in a response be reduced? Our focus is on screening using techniques from statistical design of experiments (DoE). DoE refers to the process of planning an experiment so that appropriate data are collected and analyzed by statistical methods, in order to result in valid and objective conclusions. Hence any experimental problem includes both the design of the experiment and the statistical analysis of the data. Suppose that there are k factors, F1 , . . . , Fk , and that each factor Fj has a set Lj = {vj,1 , . . . , vj, j }, of j possible levels (or values). A design point is an assignment of a level from Lj to Fj , for each factor j = 1, . . . , k. An experimental design is a collection of design points. When a design has N design points, it can be represented by an N × k array A = (ai,j ) in which each row i corresponds to a design point and each column j to a factor; the entry ai,j gives the level assigned to factor j in the ith design point. When run, a design point results in one or more observable responses. A t-way interaction (or interaction of strength t) in A is a choice of t columns i1 , . . . , it , and the selection of a level ij  Lij for 1  j  t, represented as T = {(ij , ij ) : 1  j  t}. Every interactions of strength t. design point in A covers k t When the objective of experimentation is screening, it is often recommended to keep the number of factors low. It has been considered impractical to experiment with "many" factors; about ten factors is a suggested maximum [23, 31]. Generally, two levels for each factor is considered to work well in screening experiments. Methods for screening seek to reduce the number of design points required because the exhaustive full-factorial design [9, 31] is too large. For k factors each with two levels it has 2k design points. An analysis of variance (ANOVA) allows the significant factors and interactions on the response to be identified.
-p A fractional factorial design 2k is a 21 p fraction of a full factoR rial design with k two-level factors. The design is described by p

Categories and Subject Descriptors
General and reference [Cross-computing tools and techniques]: Experimentation; Mathematics of computing [Discrete mathematics]: Combinatorics

General Terms
Experimentation

Keywords
Screening experiments, Locating arrays

1.

INTRODUCTION

Computer and networked systems are examples of complex engineered systems (CESs). The complexity of an engineered system is not just due to its size, but also arises from its structure, operation (including control and management), evolution over time, and that people are involved in its design and operation [35]. Experimentation is often used to study the performance of CESs. At its most basic, a system may be viewed as transforming some input variables, or factors, into one or more observable output variables, or responses. Some factors of a system are controllable, whereas others are not. Objectives of experimentation include: Copyright is held by the authors.

31

generators, expressions of factors that are confounded; the generators determine the alias structure. A design is of resolution R if no m-factor effect is aliased with another effect containing fewer than R - m factors. A D-optimal design is a popular experimental design among those using optimality criteria. A model to fit, and a bound N on the number of design points, must be specified a priori; this restricts the factors to be analyzed to those in the model. The size of a Doptimal design is bounded by the size of a full-factorial design. Some designs aggregate the factors into groups, e.g., sequential bifurcation [24], to improve design efficiency. Grouping requires care to ensure that factor effects do not cancel. This presents a "chicken and egg" problem: we need to know how to group in order to group. Often, a domain expert is expected to make such grouping decisions. While such experts may have considerable knowledge, it is doubtful whether an expert knows the importance of a specific factor or interaction in a CES. An interaction graph depicts how a change in the level of one factor affects the other factor with respect to a response. Figure 1 shows an interaction graph for the factors of routing and medium access control (MAC) protocol on average delay in a network. The choice of MAC protocol (EDCF or IEEE 802.11) has little impact on the average delay in the AODV routing protocol, while for the DSR routing protocol the impact is very large; see [53]. If MAC protocols were aggregated, this significant interaction would be lost.
-0.2 Log10(Average delay) -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1 AODV DSR Routing protocol EDCF IEEE 802.11

tential to transform experimentation in huge factor spaces such as those found in CESs. The rest of this paper is organized as follows. §2 defines a locating array, and gives an example of how a design is used for location. §3 presents preliminary results applying an LA for screening the response of TCP throughput in a simulation model of a mobile wireless network. The full-factorial design for this system is infeasible -- it has over 1043 design points! Yet there is an LA with only 421 design points. We develop an algorithm using the LA to identify the significant factors and interactions from the data collected, providing a small example. In §4 we validate the significance of the identified factors and interactions independently using the statistical software JMP. Finally, in §5 we summarize, discuss potential threats to our approach, directions for this research, and conclude.

2.

LOCATING ARRAYS

Reducing the number of design points required relies on a sparsity of effects assumption, that interactions of interest involve at most a small, known number t of interacting factors. As one means of reduction, we define locating arrays (LAs) [8]. For a set of factors each taking on a number of levels, an LA permits the identification of a small number of significant interactions among small sets of (factor, level) combinations. LAs differ from standard designed experiments, which are used to measure interactions and to develop a model for the response as a function of these [31]. "Search designs" [17, 48, 49] also attempt to locate interactions of higher strength, but their focus remains on measurement and hence on balanced designs. Rao [20] shows that the number of design points in a balanced design must be at least as large as the number of interactions considered. Thus if t-way interactions among k factors each having v levels are to be examined, balanced designs only reduce the v k exhaustive design points to O(kt ). The selection of few factors from hundreds of candidates by this reduction is not viable. By lessening the requirement from measurement to identification, LAs are not subject to the Rao bound. Fortunately LAs behave more like covering arrays, experimental designs in which every t-way interaction among factors appears in at least one design point. Unlike designed experiments, the number of design points in a covering array for k factors grows as a logarithmic function of k (see [43], for example). In [8], a construction of LAs using covering arrays of higher strength is given, and hence LAs also exhibit this logarithmic growth, making them asymptotically much more efficient than balanced designs. This motivates the consideration of covering arrays, which have been the subject of extensive study [4, 5, 19, 36]. They are used in testing software [10,13,25,26], hardware [46,50], composite materials [3], biological networks [44, 47], and others. Their use to facilitate location of interactions is examined in [29, 56], and measurement in [21, 22]. Covering arrays form the basis for combinatorial methods to learn an unknown classification function using few evaluations -- these arise in computational learning and classification, and hinge on locating the relevant attributes (factors) [11]. Algorithms for generating covering arrays range from greedy (e.g., [2, 16]) through heuristic search (e.g., [38, 52]). However, combinatorial constructions (see [5]) provide the only available deterministic means of producing covering arrays with more than a few hundred factors. A design point, when run, yields one or more responses. For ease of exposition, we classify the responses in two groups, those that

Figure 1: Interaction of routing and MAC protocols on delay [53]. A fractional factorial design is saturated when it investigates k = N - 1 factors in N design points [31]. In a supersaturated design, the number of factors k > N - 1; such designs contain more factors than design points. These designs are only able to estimate a main effects model [27, 31]. Thus they cannot consider possible interactions at all. Even with substantial and detailed domain knowledge, it is imperative not to eliminate or aggregate factors a priori. Our goal, therefore, is an automatic and objective approach to screening. To address this problem we have formulated the definition of a locating array (LA) [8]. Locating arrays exhibit logarithmic growth in the number of factors because their focus is on identification rather than on measurement. This makes practical the consideration of an order of magnitude more factors in experimentation, removing the need for the elimination of factors. As a result, LAs have the po-

32

exceed a specified threshold and those that do not. So we suppose that the outcome of a run of a design point is a single binary response ("pass" or "fail"). A fault is caused by one or more t-way interactions, and is evidenced by a run failing. Given an experimental design and the set of interactions that cause faults, the outcomes can be easily calculated: A run fails exactly when it contains one or more of the faulty interactions, and does not fail otherwise. In order to observe a fault, the interaction must be covered by at least one design point. With no restriction on the interactions that can cause faults, every interaction must be covered. Then the best one can do is to form all k j =1 j possible design points, the exhaustive design. Using sparsity of effects, an upper bound t is placed on the strength of interactions that may be faulty. Then we require that every t-way interaction be covered; in other words, the design is a covering array of strength t. Let A = (ai,j ) be an experimental design, an N × k array where in each row i, levels in the j th column are chosen from a set Lj of size j . For array A and t-way interaction T = {(ij , ij ) : 1  j  t}, define (A, T ) = {r : ar,ij = ij , 1  j  t} as the set of rows of A in which T is covered. For a set T of interactions, (A, T ) = T T (A, T ). Locating faults requires that T be recovered from (A, T ), whenever T is a possible set of faults. Let It be the set of all t-way interactions for an array, and let It be the set of all interactions of strength at most t. Consider an interaction T  It of strength less than t. Any interaction T of strength t that contains T necessarily has (A, T )  (A, T ). In this case, when T is faulty we are unable to determine whether or not T is also faulty. Call a subset T of interactions in It independent if there do not exist T, T  T with T  T . In general, some interactions in It (or perhaps It ) are believed to be faulty, but their number and identity are unknown. The faulty interactions cannot be identified precisely from the outcomes, even if the full factorial design is employed, without some restriction on their number. (Consider the situation in which every design point run fails.) We therefore suppose that a maximum number d of faulty interactions is specified. D EFINITION 2.1 ( [8]). An array A is (d, t)-locating if whenever T1 , T2  It and T1  T2 is independent, |T1 |  d, and |T2 |  d, it holds that (A, T1 ) = (A, T2 )  T1 = T2 . If there is any set of d interactions of strength t that produce exactly the outcomes obtained when using a (d, t)-locating array A to conduct experiments, then there is exactly one such set of interactions. To avoid enumeration of all sets of d interactions of strength t, one can employ a stronger condition that for every interaction T of strength at most T and every set T1  It that does not contain T and for which T1  {T } is independent, it holds that (A, T ) = (A, T1 )  T  T1 . A locating array meeting this stronger condition is termed a detecting array in [8]. When using a detecting array, if there are at most d independent faulty interactions each of strength at most t, they are characterized precisely as the interactions that appear in no run that passes. We typically employ the term locating array to refer to both, but for reasons of computational efficiency the locating arrays that we use are, in fact, detecting arrays. In practice, one does not know a priori how many interactions are faulty, or their strengths. Nevertheless, when responses are contin-

uous, we can select a threshold on the responses so as to limit the number of design points yielding a "fail" outcome to locate those that make the most substantial contribution to the response. We exploit this fact later in §3.2.

2.1

A Small Example

An example is provided to demonstrate fault location, and show the limitations of covering arrays for this purpose. Suppose that we use the experimental design for five binary factors in Table 1. It is a covering array in which each of the 22 5 = 40 two-way 2 interactions is covered. A response for each design point run is listed in the adjacent column. Table 1: Experimental design and response for each run. Factors 3 4 1 1 1 0 0 0 0 1 0 0 0 1

Design Points

1 2 3 4 5 6

1 0 1 0 1 0 1

2 1 0 1 0 0 1

5 1 0 0 1 1 0

Response Fail Pass Fail Pass Pass Pass

First, let us locate faults due to main effects (i.e., the individual factors or one-way interactions). The second design point run passes, so all (factor, level) pairs in it are known not to be faulty. Therefore in Table 2(a), that considers only the second design point, when factor 1 is set to one, the run is not faulty. Similarly, for factors 2, 3, 4, and 5 set to zero, one, zero, and zero, respectively. This is indicated by a check-mark ( ) in the table. Repeating to check coverage of each one-way interaction for each successful run, no single (factor, level) error accounts for the faults; see Table 2(b). Table 2: Locating faults due to main effects. (a) Run 2 Factors 0 1 1 2 3 4 5 (b) All Runs Factors 0 1 1 2 3 4 5

Computing (T ) for every one-way interaction, we obtain the sets in Table 3. Because no two sets are equal, the array is (1, 1)locating and when there is a single faulty one-way interaction it can be located. However, because {1, 3, 5}  {2, 3, 5} = {1, 3, 5}  {1, 2}, when rows 1, 3, and 5 fail and 2, 4, and 6 pass, we cannot determine the two faulty interactions -- the array is not (2, 1)locating. Table 3: (T ) for one-way interactions T = {(c,  )}. c 0 1 1 {1,3,5} {2,4,6} 2 {2,4,5} {1,3,6} 3 {3,4,5,6} {1,2} 4 {2,3,5} {1,4,6} 5 {2,3,6} {1,4,6}

Now, let us try to locate faults due to two-way interactions. Because the second design point run passes, all two-way interactions in it are known not to be faulty; Table 4(a) records the results. Repeating to check for coverage of each two-way interaction for each successful run, those interactions not found to pass in this way in

33

Table 4: Locating faults due to two-way interactions. (a) Run 2 00 01 10 (b) All Runs 00 01 10

Factors 1, 2 1, 3 1, 4 1, 5 2, 3 2, 4 2, 5 3, 4 3, 5 4, 5

11

Factors 1, 2 1, 3 1, 4 1, 5 2, 3 2, 4 2, 5 3, 4 3, 5 4, 5

11

Table 4(b) form a set of candidate faults. In this example, there are nine interactions in the set of candidate faults. Now for the twoway interaction {(1, 0), (2, 1)}, ({(1, 0), (2, 1)}) = {1, 3}, and it is the only two-way interaction for which this holds; and, no oneway interaction T has (T ) = {1, 3}. Hence if there is a single fault, it must be {(1, 0), (2, 1)}, and we have located the fault. Our success for one response is not sufficient, however. Because ({(1, 0), (2, 1)}) = {1} = ({(2, 1), (3, 1)}), if only run 1 fails, there are at least two equally plausible explanations using only a single two-way interaction. Indeed A is not (1, 2)-locating. Thus the ability to locate is more than simply coverage!

of levels for each factor to the desired number, eliminating rows in the process and forming an array C with 143 design points. The resulting array provides coverage of two-way interactions but does not support location. When T and T are interactions, to distinguish them we require that (T ) = (T ), but we ask for more, namely that |(T ) \ (T )|  2 and |(T ) \ (T )|  2; this ensures that for every two interactions of interest, there are at least two design points containing one but not the other. To accomplish this, we formed three copies of C , randomly permuted their symbols within each column, and formed their union (so that every two-way interaction is covered at least three times). The resulting array B with 429 rows turned out to be (1, 2)-detecting. Three rows were selected by a greedy method to ensure the stronger condition that |(T ) \ (T )|  2 for every pair T, T of interactions; then eleven rows were deleted by a greedy algorithm to remove redundant rows, ultimately producing a design with 421 rows. Appendix A gives a pointer to the locating array used as the experimental design. Our objective was not to find the smallest possible array, because a fair evaluation of the efficacy of locating arrays should not rely on substantial additional structure being present. Ten replicates of each design point in the LA are run in ns-2; for each a response of TCP throughput is measured. These are averaged for each design point resulting in a vector with 421 entries of observed average TCP throughput obsT h.

3.2

Screening Algorithm

3.

SCREENING AN ENGINEERED SYSTEM

We now apply locating arrays for screening in a complex engineered system. One example of a CES for which it has been particularly difficult to develop models is a mobile ad hoc network (MANET). A MANET is a collection of mobile wireless nodes that self-organize without the use of any fixed infrastructure or centralized control. We seek to use a locating array to screen for the influential factors and interactions on average transport control protocol (TCP) throughput in a simulation model of a MANET.

We describe an algorithm for screening at a high level to facilitate understanding. In each iteration of the algorithm the most significant main effect or two-way interaction is identified. These terms are accumulated in a screening model of average TCP throughput. However, this screening model is not intended as a predictive model; the quality of its current estimate allows the algorithm to select the next most significant term. The screening model is used only to identify influential main effects and two-way interactions. With its output, a predictive model can be built; see §4. Initially, the screening model has no terms. With no other information, it should estimate the average TCP throughput to be the average of the vector of observed average throughput. This is unlikely to be a very good estimation! Our strategy to identify the most significant factor or interaction as the term to add to the screening model is as follows. Suppose that factor Fj , 1  k  75, has j levels Lj = {vj,1 , . . . vj, j }. For each level , 1   j , of factor Fj iterate through each of the 421 design points of the locating array A. For each design point i, 1  i  421, partition the contribution of the (factor Fj , level vj, ) combination into one of two sets: S or S . If the design point has the factor Fj set to level , i.e., ai,j = vj, , then add the throughput measured for design point i, obsT h[i], to S ; otherwise add obsT h[i] to S . Then, compute the (absolute) difference of the average of sets S and S . (Of course, metrics other than the difference of averages could be used.) Either the difference is zero (i.e., the average TCP throughput collected in the sets S and S is the same), or it is non-zero. If the difference is non-zero, then one possible explanation is that the (factor Fj , level vj, ) combination is responsible for the difference. Our hypothesis is that the (factor Fj , level vj, ) combination over all combinations for which the difference between the sets is the greatest is the most significant one. If this is correct, then a term of the form c · (Fj , vj, ) is added to the screening model. The

3.1

Designing the Experiment

We use the ns-2 simulator [37], version 2.34, for our experimentation. Since our response of interest is average TCP throughput, we select the file transfer protocol (FTP) as our application because it uses TCP for reliability. We select the internet protocol (IP), the Ad hoc On-demand Distance Vector routing protocol (AODV) [42], and IEEE 802.11b direct sequence spread spectrum (DSSS) as protocols at the network, data link, and physical layers of the protocol stack. We also use the mobility, energy, error, and propagation models in ns-2. From these protocols and models we identify 75 controllable factors. The region of interest for each factor, i.e., the range over which the factor is varied, ranges from two to ten levels, with some set according to recommendations in [33]. See Appendix A for a pointer to details of the factors and their levels. The full-factorial design for this factor space is infeasible; it has over 1043 design points! In contrast, the locating array constructed and checked manually has only 421 design points. Except for small locating arrays [51], no general construction methods have been published. We adopted a heuristic approach to construct the LA. Initially we selected a covering array with 75 factors and 10 levels per factor, constructed using a standard product construction [7]. We applied a post-optimization method [34] to reduce the number

34

coefficient c is equal to the difference in average TCP throughput of each set. When this term is added to the screening model, it makes the same estimation for average TCP throughput for sets S and S . In the first iteration of this algorithm, the estimate (i.e., the average of the vector of observed average TCP throughput) is used to determine deviations from each entry in the vector obsT h. We now have a screening model that apparently includes the most significant factor. It is now used to produce a new estimate of average TCP throughput and update the vector of residual throughput. The algorithm can be applied repeatedly to the residuals to identify the next most important factor or interaction. While this algorithm is described for (factor, level) combinations, we actually iterate over all one-way (i.e., all (factor, level) combinations) and all two-way interactions (i.e., all pairs of (factor, level) combinations) to identify the main effect or two-way interaction of highest significance. Any number of stopping conditions may be used to decide when to terminate the model development. We use the R2 , the coefficient of determination, indicating how well data fits a line or curve; when it shows marginal improvement, we stop. The locating array constructed for our CES is a (d = 1, t = 2)locating array, meaning it only guarantees to be able to locate (identify) at most one (d = 1) main effect or two-way (i.e., up to t = 2way) interaction. It is interesting that the LA may be used iteratively to identify subsequent significant main effects or interactions. In this sense, the algorithm uses a "heavy-hitters" approach as in compressive sensing [6].

design point with this initial fitted value. Now, we iterate over each (factor,level) combination. Factor 1 is set 4 to its low level in design points 1­4. Therefore S = 1 1 resT h[i] = 4 8 -134347 1 134344 = - 33586 and S = resT h [ i ] = = 33586. 5 4 4 4 The absolute difference, |S - S | = |-33586 - 33586| = 67172. Repeating for each (factor, level) combination, as well as all twoway interactions, we find that it is a main effect that has highest absolute difference with a value of 131255. It occurs when factor 3 is set to its lowest level, namely when the number of flows at the application layer is only one. Hence we attribute this as the explanation for the largest difference and add the term c · (F3 , v3,0 ) to the model. The method of ordinary least squares (OLS) is used to fit the intercept and coefficient c of the new term. This results in an updated model of T = 12410 + 131255 · (F3 , v3,0 ). Its coefficient of determination is R2 = 0.33. Using this updated model, the residuals can be recomputed as input to the next iteration of the algorithm. Next, we describe some of the obstacles arising in the practical application of the screening algorithm.

3.4

Applying the Screening Algorithm

3.3

Example of the Screening Algorithm

A small example is provided to step through one iteration of the screening algorithm. Suppose that we use the experimental design for four binary factors in Table 5. It is a covering array of strength three and therefore also a (2, 1)-detecting array. Factor 1 corresponds to the distribution function used for introducing errors (uniformly or exponentially distributed), factor 2 to the error rate (10-7 or 10-5 ), factor 3 to the number of flows at the application layer (1 or 18), and factor 4 to the TCP packet size (64 or 2048); the levels are taken as "binary" for this example. All remaining factors are set to their default levels for experimentation. A response of observed TCP throughput for each design point, averaged over ten replicates, is listed in the column obsT h. (All measures are truncated to integers for simplicity.) Table 5: Experimental design and average TCP throughput. Factors 2 3 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1

In applying the screening algorithm to our CES, several obstacles arose. The first is that the measured average TCP throughput is not normally distributed, as Figure 2 shows; this is not uncommon in systems experimentation [12]. The best transformation of the data is a natural logarithm (Figure 3a). From the normal probability plot (Figure 3b), we find that the transformed data are still not normally distributed; nevertheless, we work with this transformation of the data.
72%

300

0.985

0.94
250

0.88 Normal Probability
18% 5% 2% 1% 1% 0% 0% 0%

0.75

200 Frequency

0.5

150

0.25 0.12

100

0.06

50

0.015 0.003
0%

0

50,000

100,000

150,000

200,000

250,000

0

50,000

100,000 obsTH1

150,000

200,000

250,00

obsTH1

(a) Throughput distribution.

(b) Normal probability plot.

Figure 2: Distribution of the original observed average throughput, and corresponding normal probability plot. 4 0 1 1 0 1 0 0 1 obsT h 63339 29860 80801 3804 373866 3879 56656 12095 resT h -14699 -48178 2764 -74234 295828 -74159 -21382 -65943 A much larger problem arises from the fact that the LA does not cover each main effect and two-way interaction the same number of times. Indeed, binary factors are covered much more frequently (some as many as two hundred times in the 421 row LA) compared to two-way interactions of factors with ten levels (only a handful of times). This is unavoidable when one-way and two-way interactions are compared, and when factors have a different numbers of levels. Consider the behaviour of the screening algorithm. For a binary factor the sets S and S have the same or nearly the same size and, as a result, the average of each set has small variance. In the example in §3.3, each (factor, level) combination is covered four times

1 2 3 4 5 6 7 8

1 0 0 0 0 1 1 1 1

The overall mean of the obsT h is 78038. Therefore, the screening model initially estimates this value for average TCP throughput, i.e., T = 78038. The residuals (resT h) are computed in Table 5 by taking the difference of the observed average throughput for each

Design Points

35

26%

0.985

24%

100

0.985

100

0.94
21%

0.94 0.88
80
18%

0.88
Normal Probability
9% 6% 5% 1%

Normal Probability

80

0.75
Frequency

0.75

Frequency

0.5

60
13% 12%

0.5

60

14%

0.25
40

0.25 0.12 0.06

40
8% 7% 6% 5% 5% 4%

0.12 0.06
20

8%

20

0.015 0.003 2 4 6 ln(obsTH1 8 10 12

0.015 0.003 -6
-6 -4 -2 0 2 Residuals of TCP throughput after iteration 1 4

1%

1% 1%

1% 0%

1%

-4

-2

0

2

4

2

4

6 8 ln(obsTH1)

10

12

Residuals of TCP throughput after iteration 1

(a) ln transformation.

(b) Normal probability plot.

(a) Distribution of first residual.

(b) Normal probability plot.

Figure 3: Natural logarithm transformation of the original observed throughput, and corresponding normal probability plot.

Figure 4: Distribution of residuals after the first iteration of the screening algorithm, and corresponding normal probability plot. intercept and i is the coefficient of term i, 1  i  12. Table 6: Screening model with twelve terms. t-Test 52.6 34.5 32.8 -29.1 -11.8 -12.1 -9.3 6.5 6.6 8.4 6.3 5.5 5.2 i 5.6 4.4 4.0 -4.7 -1.6 -1.5 -1.2 0.9 0.7 1.1 1.1 0.7 0.5 Factor or interaction, and level(s) ErrorModel_ranvar_ U nif orm ErrorModel_unit_ pkt) (ErrorModel_ranvar_ U nif orm) * (ErrorModel_unit_ pkt) TCP_packetSize_ 64 MAC_RTSThreshold_ 0 TCP_packetSize_ 128 (TCP_RTTvar_exp_ 2) * (TCP_min_max_RTO_ 0.1) TCP_min_max_RTO_ 0.2 (ErrorModel_unit_ pkt ) * (ErrorModel_rate_ 1.0E -07) (ErrorModel_ranvar_ U nif orm) * (MAC_RTSThreshold_ 0) APP_flows_ 1 RWP_Area_ 8

(each column of the array has four zeros and four ones). However in general, as the number of levels for a factor increases, the size of the sets S and S may become markedly different, and the variance of the average of each set may increase greatly. Returning to the example in §3.3, the two-way interactions are not covered equally. Consider the two-way interaction {(1, 0), (2, 0)}. It is covered in only two rows of the array, namely |({(1, 0), (2, 0)})| = |{1, 2}| = 2 (this is true for all two-way interactions in this example). Even in this small array, the coverage of two-way interactions is unbalanced resulting in S accumulating two values and S accumulating six values. This makes any direct comparison among (factor, level) combinations and/or two-way interactions impossible. To address this problem, factors are grouped according to the number of times each level is covered in the LA; see Appendix A for a pointer to the details on how groups are formed. Now, in each iteration of the screening algorithm, the first step is to select the most significant factor or interaction from each group. Then from these candidates, the most significant factor or interaction overall is selected. The Figure 4 shows the graphical tests for normality of the residuals after the first iteration of the screening algorithm. (Similar behaviour of the residuals is observed after each iteration.) While the figures indicate that the residuals are close to normally distributed, we check using the non-parametric Shapiro-Wilk test. This test indicates that the residuals are still not normally distributed. Hence, we use the Wilcoxon rank sum test and the Mann-Whitney U test [14, 28, 54] to select the most significant factor or two-way interaction within each group. Then, to select the most significant factor or interaction over all groups, the Akaike information criterion (AICC ) [1] is used. We still need to fit the intercept and the coefficients of the terms. For a linear model with the assumptions of expected error of zero and expected variance in the error to be equal, the method of ordinary least squares (OLS) is used. However, if the expected variance in the error is unequal, OLS is no longer appropriate [32]. In this case, the method of weighted least squares (WLS) is used to fit the intercept and coefficients of the terms in the screening model.

The first notable observation about this screening model is that it contains both main effects and two-way interactions. Moreover, it contains factors from across the layers of the protocol stack (application, transport, and MAC) and not just the transport layer; in addition, it includes factors from the error model and the mobility model. Aside from these differences with other models of TCP throughput (such as [15, 18, 30, 39­41, 55, 57, 58]), the screening model includes not just which factors or two-way interactions are significant, but the level at which each is significant. From the statistical point of view, Table 7 shows a strong correlation among the regressors and the response of average TCP throughput. The F statistic indicates that the model is significant to the response. Table 7: Summary statistics of the screening model in Table 6. R2 and Adjusted R2 : 0.84 Standard deviation: 0.92 F statistic: 180.6 on 12 and 408 df, p-value < 7.89e-155 We are encouraged by the factors and interactions identified. This includes how and into what unit errors are introduced (using a uni-

3.4.1

The Resulting Screening Model

Table 6 gives the screening model for average TCP throughput developed in twelve iterations of the screening algorithm; Table 8 lists its unique factors. A Student's t-test was run on each term in the screening model and each was found to be significant; 0 is the

36

form distribution into packets rather than bit errors), and their interaction. Smaller sized packets (64 and 128 bytes) tend to reduce throughput. When RTS/CTS is always on (i.e., the threshold is zero bytes), there is a negative impact on throughput compared to when it is configured to 1500 or 3000 bytes (always off). The retransmission timeout (RTO) and round trip time (RTT) are part of TCP's congestion control mechanism; the RTO infers packet loss by observing duplicate acknowledgements and the RTT is related to the propagation delay. The RTO is significant by itself, and in its interaction with the RTT as they work to correct and prevent network congestion. The synthetic error model of the simulator drops packets comparing them with data from an uniform distribution at a steady-state loss event rate of 1.0E -07; this is the lowest error rate used and naturally it corresponds with higher throughput. Smaller simulation areas also result in higher throughput; a larger area has longer average shortest-hop path lengths and average higher network partition rates both of which negatively affect throughput. The throughput response is higher with fewer flows because increasing the number of flows not only may overload the network but more flows are more challenging to route in a MANET.

Table 9: Partial results of a 29 full-factorial screening experiment using JMP 11.0 on the nine factors in Table 8. Term ErrorModel_ranvar_*ErrorModel_unit_ ErrorModel_ranvar_ ErrorModel_unit_ TCP_packetSize_ APP_flows_ TCP_min_max_RTO_ RWP_Area_ MAC_RTSThreshold_ ErrorModel_unit_*TCP_packetSize_ ErrorModel_rate_ ErrorModel_ranvar_*MAC_RTSThreshold_ APP_flows_*RWP_Area_ ErrorModel_unit_*ErrorModel_rate_ TCP_packetSize_*ErrorModel_rate_ ErrorModel_unit_*MAC_RTSThreshold_ ErrorModel_ranvar_*APP_flows_ APP_flows_*TCP_min_max_RTO_ ErrorModel_unit_*APP_flows_ ErrorModel_ranvar_*TCP_min_max_RTO_ ErrorModel_ranvar_*TCP_packetSize_ TCP_packetSize_*APP_flows_ TCP_min_max_RTO_*RWP_Area_ ErrorModel_ranvar_*RWP_Area_ MAC_RTSThreshold_*ErrorModel_rate_ TCP_min_max_RTO_*ErrorModel_rate_ TCP_min_max_RTO_*TCP_rttvar_exp_ ErrorModel_unit_*TCP_min_max_RTO_ APP_flows_*ErrorModel_rate_ RWP_Area_*MAC_RTSThreshold_ ErrorModel_unit_*RWP_Area_ TCP_rttvar_exp_ TCP_packetSize_*RWP_Area_ APP_flows_*MAC_RTSThreshold_ RWP_Area_*ErrorModel_rate_ ErrorModel_ranvar_*TCP_rttvar_exp_ p-Value <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* <.0001* 0.0001 0.0001 0.0003 0.0006 0.001 0.0012 0.002 0.0116 0.0444 0.0515

4.

VALIDATION AND VERIFICATION

From the 75 controllable factors used in experimentation, nine unique factors are present in the twelve terms in the screening model in Table 6; these are listed in Table 8. Table 8: Unique factors in the screening model in Table 6. Level Factor TCP_RTTvar_exp_ ErrorModel_ranvar_ ErrorModel_unit_ MAC_RTSThreshold_ ErrorModel_rate_ RWP_Area_ TCP_min_max_RTO_ APP_flows_ TCP_packetSize_ Minimum 2 U nif orm pkt 0 1.0E -07 8 0 .1 1 64 Maximum 4 Exponential bit 3000 1.0E -05 40 40 18 2048

In order to validate the factors and interactions identified, we first conduct a full-factorial experiment for these nine factors using the extremes of their region of interest, using the statistical software JMP to analyze the results. From this, we produce a predictive model of average TCP throughput. We then examine the quality of this predictive model by comparing how it performs on random design points (i.e., a design point in which the level of each factor is selected at random). We present our validation results next.

in Table 6. Indeed, both models have the same four most significant terms (though in a different order), and all factors and interactions in Table 6 are a subset of the terms in Table 9. Appendix A gives a pointer to the details of the predictive model for average TCP throughput that was fit using a subset of the significant terms in Table 9. Figure 5 shows the results of evaluating the JMP predictive model as a function of the TCP packet size, for the three levels of error rate. As in the experimentation, all remaining factors are fixed at their default levels. As expected, the results show that the highest TCP throughput is achieved when the error rate is at the lowest level (1.0E -07). For a given error rate the TCP throughput increases as a function of packet size, after which it decreases. An exception is for packet size 1024. Aside from this exception, these results also confirm our intuition of TCP throughput behaviour. The reason for this exception deserves further study but may be related to the default settings used for the other 66 factors not varied in this screening experiment. We now examine the predictive accuracy of the JMP model for random design points.

We conduct an independent 29 full-factorial experiment on the nine factors in Table 8. All remaining 75 - 9 = 66 factors are fixed to their default levels. Ten replicates of each of the 29 design points is run, and TCP throughput measured. The results of the experimentation are input to the JMP statistical software, version 11.0 [45]. The results from the full-factorial screening experiment are given in Table 9. It includes only the main effects and two-way interactions sorted in increasing order by the p-value. The results indicate high commonality with the main effects and two-factor interactions selected by the screening algorithm that formed the screening model

4.1

Full-Factorial Screening in JMP

37

120000 JMP 1.0e-7 JMP 1.0e-6 JMP 1.0e-5

100000 TCP throughput (bps)

factors and two-way interactions, the screening model developed also reflects the actual behaviour well. Despite this, the method aims only to deal with many factors and their interactions to identify the significant ones. We advocate that further experimentation is necessary after the screening is completed, both to confirm the screening results and to build a predictive model. One must be cautious not to over-fit the experimental results and claim unwarranted confidence; confirmation is needed. This is particularly a concern if the stopping criterion chosen locates too many or too few significant interactions; while our choice of R2 appears to have worked well, future effort should address the impact of different stopping criteria. A second concern is the selection criterion for the next factor or interaction to include. Subsequent selections depend upon selections already made, so our method could in principle be misdirected by a bad selection. Our criterion of using the differences between responses for S and those for S has also worked well, but we cannot be certain that such a simple selection suffices in general. Finally, we have employed only a few locating arrays; while they have worked well in our analyses, constructing a suitable locating array remains a challenging problem that merits further research. Certainly further experimentation is needed to assess the merit of screening using LAs, in particular on physical not just simulated complex engineered systems, and draw firm conclusions. What we can conclude is that in a challenging CES arising from a MANET, screening using locating arrays is viable and yields useful models.

80000

60000

40000

20000

0 64 128 256 512 768 1024 Packet size (bytes) 1280 1536 1792 2048

Figure 5: TCP throughput as a function of packet size as predicted the by JMP model; all other factors are at their default levels.

4.2

Predictive Accuracy of JMP Model

In order to test the predictive accuracy of the JMP model, a new experimental design of one hundred random design points is constructed. In constructing each design point, for each of factor Fj , 1  j  75, a random level from Lj is selected. New mobility scenarios are also generated. Ten replicates of each of the random design points are run in the ns-2 simulator, and the TCP throughput measured. In addition, for each experiment in the design, the JMP model is evaluated generating a new data set of fitted TCP throughput. Figure 6 shows the average TCP throughput from simulation, and the fitted throughput from the JMP model corresponding to this random design. The mean TCP throughput from the simulations is 20,892 bps whereas the mean from the JMP model is lower, only 13,946 bps. However, the standard deviation of the results from the JMP model is smaller than the standard deviation from the simulations. Both models exhibit a few outliers. Approximately 94% of the results predicted for TCP throughput from the JMP model are in one standard deviation of the simulation results. Considering the size of the factor space, we conclude that the predicted average TCP throughput of the JMP model is similar to the average TCP throughput measured in simulation.

Acknowledgment
Thanks to Doug Montgomery for his advice on all things statistical. This material is based in part upon work supported by the National Science Foundation under Grant No. 1421058.

6.

REFERENCES

4.3

Predictive Accuracy of Screening Model

While the model developed in applying the screening algorithm based on the LA (Table 6) is not intended to be used as a predictive model, we were curious about its predictive accuracy. Appendix A gives a pointer to a summary of results similar to those in this section for the screening model. To our surprise, the predictive accuracy of the screening model is reasonably good. The screening model does appear to have more variability than the model developed in JMP.

5.

CONCLUSIONS

Locating arrays capture the intuition that in order to see the effect of a main effect or interaction, some design point must cover it; and in order to distinguish it, the responses for the set of design points that cover it must not be equally explained by another small set of main effects or interactions. In a complex engineered system, many main effects and interactions may be significant, but our method identifies them one at a time, iteratively improving a screening model. In this way, an experimental design must be able to repeatedly locate a single "most significant" main effect or interaction. Our results show that using locating arrays for screening appears promising. Indeed while the screening targeted the identification of significant

[1] H. Akaike. A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19(6):716­723, 1974. [2] R. C. Bryce and C. J. Colbourn. A density-based greedy algorithm for higher strength covering arrays. Software Testing, Verification, and Reliability, 19:37­53, 2009. [3] J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE Global Research Technical Report, 29(9):769­781, 2002. [4] C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121­167, 2004. [5] C. J. Colbourn. Covering arrays and hash families. In Information Security and Related Combinatorics, NATO Peace and Information Security, pages 99­136. IOS Press, 2011. [6] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk. Frameproof codes and compressive sensing. In Proc. 48th Annual Allerton Conference on Communication, Control, and Computing, 2010. [7] C. J. Colbourn, S. S. Martirosyan, G. L. Mullen, D. E. Shasha, G. B. Sherwood, and J. L. Yucas. Products of mixed covering arrays of strength two. Journal of Combinatorial Designs, 14(2):124­138, 2006. [8] C. J. Colbourn and D. W. McClary. Locating and detecting arrays for interaction faults. Journal of Combinatorial Optimization, 15:17­48, 2008. [9] C. Croarkin, P. Tobias, J. J. Filliben, B. Hembree,

38

160000 140000 120000 TCP throughput (bps) 100000 80000 60000 40000
+StDev Fitted JMP model

Fitted JMP model Mean fitted JMP model

20000 0 0 10 20
-StDev Fitted JMP model

30

40

50 Random tests

60

70

80

90

100

(a) Predictions by JMP.
160000 140000 120000 TCP throughput (bps) 100000 80000 60000
+StDev Simulated

Simulated Mean Simulated

40000 20000 0
-StDev Simulated

0

10

20

30

40

50 Random tests

60

70

80

90

100

(b) Simulation results.

Figure 6: Predictions by the JMP model and simulation results for random design points.

[10]

[11]

[12]

[13]

[14]

[15]

[16]

W. Guthrie, L. Trutna, and J. Prins, editors. NIST/SEMATECH e-Handbook of Statistical Methods. NIST/SEMATECH, 2012. S. R. Dalal, A. J. N. Karunanithi, J. M. L. Leaton, G. C. P. Patton, and B. M. Horowitz. Model-based testing in practice. In Proc. Intl. Conf. on Software Engineering (ICSE '99), pages 285­294, 1999. P. Damaschke. Adaptive versus nonadaptive attribute-efficient learning. Machine Learning, 41:197­215, 2000. A. B. de Oliveira, S. Fischmeister, A. Diwan, M. Hauswirth, and P. F. Sweeney. Why you should care about quantile regression. In Proc. of the ACM Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS), March 2013. S. Dunietz, W. K. Ehrlich, B. D. Szablak, C. L. Mallows, and A. Iannino. Applying design of experiments to software testing. In Proc. Intl. Conf. on Software Engineering (ICSE '97), pages 205­215, Los Alamitos, CA, 1997. IEEE. M. Fay and M. Proschan. Wilcoxon-Mann-Whitney or t-test? On assumptions for hypothesis tests and multiple interpretations of decision rules. Statistics Surveys, 4:1­39, 2010. S. Floyd, M. Handley, J. Padhye, and J. Widmer. Equation-based congestion control for unicast applications: The extended version. SIGCOMM Computing Communications Review, 30:43­56, 2000. M. Forbes, J. Lawrence, Y. Lei, R. N. Kacker, and D. R.

[17]

[18]

[19]

[20] [21]

[22]

[23]

[24]

Kuhn. Refining the in-parameter-order strategy for constructing covering arrays. J. Res. Nat. Inst. Stand. Tech., 113:287­297, 2008. S. Ghosh and C. Burns. Comparison of four new general classes of search designs. Austral. New Zealand J. Stat., 44:357­366, 2002. K.-J. Grinnemo and A. Brunstrom. A simulation based performance analysis of a TCP extension for best-effort multimedia applications. In Proceedings of the 35th Annual Simulation Symposium, 2002. A. Hartman. Software and hardware testing using combinatorial covering suites. In M. C. Golumbic and I. B.-A. Hartman, editors, Interdisciplinary Applications of Graph Theory, Combinatorics, and Algorithms, pages 237­266. Springer, Norwell, MA, 2005. A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal Arrays. Springer-Verlag, New York, 1999. D. S. Hoskins, C. J. Colbourn, and M. Kulahci. Truncated D-optimal designs for screening experiments. American Journal of Mathematical and Management Sciences, 28:359­383, 2008. D. S. Hoskins, C. J. Colbourn, and D. C. Montgomery. D-optimal designs with interaction coverage. Journal of Statistical Theory and Practice, 3:817­830, 2009. J. P. C. Kleijnen. An overview of the design and analysis of simulation experiments for sensitivity analysis. European Journal of Operational Research, 164:287­300, 2005. J. P. C. Kleijnen, B. Bettonvil, and F. Persson. Screening for

39

[25]

[26]

[27]

[28]

[29]

[30]

[31] [32]

[33]

[34]

[35]

[36] [37] [38]

[39]

[40]

[41]

[42]

[43]

the important factors in large discrete-even simulation models: Sequential bifurcation and its applications. In A. M. Dean and S. M. Lewis, editors, Screening: Methods for Experimentation in Industry, Drug Discovery and Genetics, chapter 13, pages 287­307. Springer-Verlag, 2006. D. Kuhn and M. Reilly. An investigation of the applicability of design of experiments to software testing. In Proc. 27th Annual NASA Goddard/IEEE Software Engineering Workshop, pages 91­95, Los Alamitos, CA, 2002. IEEE. D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault interactions and implications for software testing. IEEE Trans. Software Engineering, 30(6):418­421, 2004. R. Li and D. K. J. Lin. Analysis methods for supersaturated designs: Some comparisons. Journal of Data Science, pages 249­260, 2003. H. B. Mann and D. R. Whitney. On a test of whether one of two random variables is stochastically larger than the other. Annals of Mathematical Statistics, 18:50­60, 1947. C. Martínez, L. Moura, D. Panario, and B. Stevens. Locating errors using ELAs, covering arrays, and adaptive testing algorithms. SIAM J. Discrete Math., 23:1776­1799, 2009/10. M. Mathis, J. Semke, J. Mahdavi, and T. Ott. The macroscopic behavior of the TCP congestion avoidance algorithm. SIGCOMM Comput. Commun. Rev., 27:67­82, 1997. D. C. Montgomery. Design and Analysis of Experiments. John Wiley & Sons, Inc., 8 edition, 2012. D. C. Montgomery, E. A. Peck, and C. G. Vining. Introduction to Linear Regression Analysis. John Wiley & Sons, Inc., 4th edition, 2006. A. Munjal, T. Camp, and W. Navidi. Constructing rigorous MANET simulation scenarios with realistic mobility. In European Wireless Conference (EW), pages 817­824, 2010. P. Nayeri, C. J. Colbourn, and G. Konjevod. Randomized postoptimization of covering arrays. European Journal of Combinatorics, 34:91­103, 2013. Networking and information technology research and development (NITRD) large scale networking (LSN) workshop report on complex engineered networks, 2012. C. Nie and H. Leung. A survey of combinatorial testing. ACM Computing Surveys, 43, 2011. The Network Simulator - ns-2. http://www.isi.edu/nsnam/ns. K. Nurmela. Upper bounds for covering arrays by tabu search. Discrete Applied Mathematics, 138(9):143­152, 2004. J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. Modeling TCP throughput: a simple model and its empirical validation. SIGCOMM Computing Communications Review, 28:303­314, 1998. J. Padhye, V. Firoiu, D. F. Towsley, and J. F. Kurose. Modeling TCP Reno performance: A simple model and its empirical validation. IEEE/ACM Transactions on Networking, 8:133­145, 2000. N. Parvez, A. Mahanti, and C. Williamson. An analytic throughput model for TCP NewReno. IEEE/ACM Transactions on Networking, 18:448­461, 2010. C. E. Perkins and E. M. Royer. Ad hoc on-demand distance vector routing. In Proc. Second IEEE Workshop on Mobile Computing Systems and Applications, pages 90­100, 1999. S. Poljak, A. Pultr, and V. Rödl. On qualitatively independent

[44]

[45] [46]

[47]

[48]

[49]

[50]

[51]

[52]

[53]

[54] [55]

[56]

[57]

[58]

partitions and related problems. Discrete Applied Math., 6:193­205, 1983. A. H. Ronneseth and C. J. Colbourn. Merging covering arrays and compressing multiple sequence alignments. Discrete Applied Mathematics, 157:2177­2190, 2009. JMP statistical software from SAS. http://www.jmp.com. G. Seroussi and N. H. Bshouty. Vector sets for exhaustive testing of logic circuits. IEEE Transactions on Information Theory, 34:513­522, 1988. D. E. Shasha, A. Y. Kouranov, L. V. Lejay, M. F. Chou, and G. M. Coruzzi. Using combinatorial design to study regulation by multiple input signals: A tool for parsimony in the post-genomics era. Plant Physiology, 127:1590­1594, 2001. T. Shirakura, T. Takahashi, and J. N. Srivastava. Searching probabilities for nonzero effects in search designs for the noisy case. Ann. Statist., 24:2560­2568, 1996. J. N. Srivastava. Designs for searching non-negligible effects. In J. N. Srivastava, editor, A Survey of Statistical Design and Linear Models, pages 507­519. North­Holland, 1975. D. T. Tang and C. L. Chen. Iterative exhaustive pattern generation for logic testing. IBM Journal Research and Development, 28:212­219, 1984. Y. Tang, C. J. Colbourn, and J. Yin. Optimality and constructions of locating arrays. J. Stat. Theory Pract., 6(1):20­29, 2012. J. Torres-Jimenez and E. Rodriguez-Tello. New upper bounds for binary covering arrays using simulated annealing. Information Sciences, 185:137­152, 2012. K. K. Vadde and V. R. Syrotiuk. Factor interaction on service delivery in mobile ad hoc networks. IEEE Journal on Selected Areas in Communications, 22:1335­1346, 2004. F. Wilcoxon. Individual comparisons by ranking methods. Biometrics Bulletin, 1:80­83, 1945. I. Yeom and A. L. N. Reddy. Modeling TCP behavior in a differentiated services network. IEEE/ACM Transactions on Networking, 9:31­46, 1999. C. Yilmaz, M. B. Cohen, and A. Porter. Covering arrays for efficient fault characterization in complex configuration spaces. IEEE Transactions on Software Engineering, 31:20­34, 2006. B. Zhou, C. P. Fu, D.-M. Chiu, C. T. Lau, and L. H. Ngoh. A simple throughput model for TCP Reno. In Proceedings of the IEEE International Communications Conference (ICC'06), 2006. M. Zorzi, A. Chockalingam, and R. R. Rao. Throughput analysis of TCP on channels with memory. IEEE Journal on Selected Areas in Communications, 18:1289­1300, 2000.

APPENDIX A. GITHUB REPOSITORY
A GitHub repository provides supplementary material at: https://github.com/locatingarray/screening.git Specifically, it includes the 75 controllable factors in the ns-2 simulator used in experimentation and their levels (§3.1), the 421 × 75 LA used as the experimental design (§3.1), a description of how factors are grouped (§3.4), the JMP model for TCP throughput, along with some statistical analysis (§4.1), and some analysis of the predictive capability of the screening model (§4.3).

40

