arXiv:1507.01922v1 [cs.CR] 7 Jul 2015

Cyber-Deception and Attribution
in Capture-the-Flag Exercises
Eric Nunes, Nimish Kulkarni, Paulo Shakarian

Andrew Ruef, Jay Little

School of Computing, Informatics and
Decision Systems Engineering
Arizona State University
Tempe, AZ 85281, USA
Email: {enunes1, nimish.kulkarni, shak} @asu.edu

Trail of Bits, Inc.
New York, NY 10003, USA
Email: {andrew, jay} @trailofbits.com

Abstractâ€”Attributing the culprit of a cyber-attack is widely
considered one of the major technical and policy challenges
of cyber-security. The lack of ground truth for an individual
responsible for a given attack has limited previous studies. Here,
we overcome this limitation by leveraging DEFCON capture-theflag (CTF) exercise data where the actual ground-truth is known.
In this work, we use various classification techniques to identify
the culprit in a cyberattack and find that deceptive activities
account for the majority of misclassified samples. We also explore
several heuristics to alleviate some of the misclassification caused
by deception.

I.

I NTRODUCTION

Attributing the culprit of a cyber-attack is widely considered one of the major technical and policy challenges of
cyber-security. The lack of ground truth for an individual
responsible for a given attack has limited previous studies. In
this study, we take an important first step toward developing
computational techniques toward attributing the actual culprit
(here hacking group) responsible for a given cyber-attack. We
leverage DEFCON capture-the-flag (CTF) exercise data which
we have processed to be amenable to various machine learning
approaches. Here, we use various classification techniques to
identify the culprit in a cyber-attack and find that deceptive
activities account for the majority of misclassified samples. We
also explore several heuristics to alleviate some of the misclassification caused by deception. Our specific contributions are
as follows:
â€¢

We assemble a dataset of cyber-attacks with ground
truth derived from the traffic of the CTF held at
DEFCON 21 in 2013.

â€¢

We analyze this dataset to identify cyber-attacks where
deception occurred.

â€¢

We frame cyber-attribution as a multi-label classification problem and leverage several machine learning
approaches. We find that deceptive incidents account
for the vast majority of misclassified samples.

explore the deception hypothesis in a cyber-warfare scenario.
When compared to other domains of warfare, there is a much
greater potential for evidence found in the aftermath of cyberattack to be planted by the adversary for purposes of deception.
The policy implications of cyber-attribution have also been
discussed in [9] where the authors point out that anonymity,
ability to launch multi-stage attacks, and attack speed pose
significant challenges to cyber attribution.
In an early survey on cyber-attribution [1], the authors point
out that technical attribution will generally identify machines,
as opposed to a given hacker and his/her affiliations. While
we will use technical information in our approach, we have
ground truth data on the group involved by the nature of
the capture-the-flag data. This will allow our approach to
profile the tactics, techniques, and procedures of a given group
as we have ground-truth information on a hacking group as
opposed to machines. An example of such an approach is the
WOMBAT attribution method [3] which attributes behavior
to IP sources that are potentially linked to some root cause
determined through a clustering technique. Similarly, other
work [8] combines cluster analysis with a component for multicriteria decision analysis and studied an implementation of this
approach using honeypot data â€“ again, this approach lacks any
ground truth of the actual hacker or hacking group.
Concurrently, we have devised a formal logical framework
for reasoning about cyber-attribution [5], [7]. However, we
have not studied how this framework can be instantiated on
a real world dataset and, to date, we have not reported on an
implementation or experiments in the literature. We note that
none of the previous work on cyber-attribution leverages a
data set with ground truth information of actual hacker groups
â€“ which is the main novelty of this paper.
II.

DATASET

We introduce several pruning techniques and show
that they can reduce the effect of deception as well as
provide insight into the conditions in which deception
was employed by the participants of the CTF.

Our dataset consists of events recorded from a Capturethe-flag (CTF) tournament held at DEFCON 21 in 2013.
Briefly, CTF competitions act as educational exercise that
exposes real world attack scenarios to participants. Network
sniffing, analysis of protocols, programming and system level
knowledge, cryptanalysis are some of the instrumental skills
acquired by contestants.

In our text on cyber-warfare [6], we discuss the difficulties
of cyber-attribution and how an intelligence analyst must also

Our data represents attack/defense style, where each team
owns a small network of machines to defend. Teams are judged

â€¢

Robot Mafia

svc

02345

payload hash

2cc03b4e0053cde24400bbd80890446c

time

2013-08-03T23:45:17

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

T-3

T-2

T-1

T-20
T-20

men in black hats

to team

T-19

from team

0

T-18

cmp:12 , svcmi:2, subs:8, movtmi:60 ......

T-17

0Ã—43:245, 0Ã—69:8, 0Ã—3a:9, 0Ã—5d:1, .....

inst hist

T-16

byte hist

200000

T-15

Value

400000

T-14

Field

600000

T-13

TABLE 2: Example event from the dataset

800000

T-12

From this pre-processing of the network data (packets) we
have around 10 million network attacks. There are 20 teams
in the CTF competition. In order to attribute an attack to a
particular team, apart from analyzing the payloads used by the
team, we also need to analyze the behavior of the attacking
team towards his adversary. For this purpose we separate the
network attacks according to the team being targeted. Thus
we have 20 such subsets. We represent the 20 subsets (teams)
as T-i, where i = 1, 2, 3...20. An example of an event in the
dataset is shown in Table 2.

T-11

indicates the date and time of the attack

T-10

indicates the payload used in the attack (md5)

time

T-9

the service that the payload is running

payload hash

T-8

the team being attacked by the payload

svc

Duplicate attacks: A duplicate attack occurs when the same
team uses the same payload to attack a team at different time
instances. Duplicate attacks can be attributed to two reasons.
First when a team is trying to compromise other systems, it
just does not launch a single attack but a wave of attacks with
very little time difference between consecutive attacks. Second,
once a successful payload is created which can penetrate the
defense of other systems, it is used more by the original
attacker as well as the deceptive one as compared to other
payloads. We group duplicates as being non-deceptive and
deceptive. Non-deceptive duplicate are the duplicates of the
team that first initiated the use of a particular payload. On the
other hand deceptive duplicates are all the attacks from the
teams that are being deceptive. Deceptive duplicates form a
large portion of the dataset as seen in Fig. 2.

T-7

the team where the payload originates (attacking team)

to team

Deceptive Attacks

Fig. 1: Unique deceptive attacks directed towards each team.

T-6

histogram of instructions used in the payload

from team

Teams

Unique Attacks

T-5

histogram of byte sequences in the payload

inst hist

0

T-4

Intuition

byte hist

100000

T-3

Field

200000

T-2

TABLE 1: Fields in an instance of network attack

300000

T-1

For each file, we computed an md5 checksum, a byte
histogram, and an ARM instruction histogram. This data was
recorded as a list of tuples (time-stamp, hash, byte-histogram,
instruction-histogram) in a JSON document. These individual
fields of the tuple are listed in Table 1.

400000

Unique Attacks

DEFCON CTF organizers recorded network traffic that
includes network packets generating to and from all participating teams and is available on Internet [4]. Recordings are
stored as archive files of PCAP (packet capture) for each team
(destination as that team) separately. PCAP file contains packet
headers (TCP, SSL, UDP etc.) and respective data as source,
destination, sequence numbers etc. with timestamp having
millisecond precision. Using open source tool tcpflow1 , we
interpreted collection of PCAPs as cumulative data streams.
Tcpflow reconstructs actual data streams from the packets
that proved helpful in protocol analysis and debugging. This
tool produces a file containing the contents of each stream,
representing the data sent between two points in the CTF
system.

attack to a team difficult.
Deception: In the context of this paper we define an attack to
be deceptive when multiple adversaries get mapped to a single
attack pattern. In the current setting we define deception as the
scenario when the same payload is used by multiple teams to
target the same team. Fig. 1 shows the distribution of unique
deception attacks with respect to the total unique attacks in
the dataset based on the target team. These unique deceptive
attacks amount to just under 35% of the total unique attacks.

Total Attacks

based on scores given to attack machines of other teams as well
as defending their own network. Initially, all virtual machines
are configured with specific set of services. These services
are vulnerable to state-of-art hacking techniques. Files can be
considered as form of flag to be captured from other teams or
to be planted to other teams by exploiting those vulnerabilities.

Teams

Non-Deceptive

Deceptive

Total Attacks

Fig. 2: Total attacks and duplicate attacks(Deceptive and
Non-deceptive) directed towards each team

A. Dataset Analysis
We now discuss two important observations from the
dataset, that makes the task of attributing an observed network
1 https://github.com/simsong/tcpflow

III.

BASELINE A PPROACHES

From the dataset, we have the ground truth available for
all the samples. Hence we use supervised machine learning

0.26

Logistic regression (LOG-REG)

0.31

Support vector machine (SVM)

0.30

Random Forest (RF)

0.37

â€¢

Non-deceptive duplicate attacks attributed to one of
the deceptive teams.

â€¢

Deceptive duplicates attributed to some other deceptive team.

â€¢

Payloads that were not encountered during the training
phase.

The first two sources of error make up the majority of
misclassifications, since a given attack can be attributed to any
of the 19 teams.
1.0

0.8

0.6

0.4

0.2

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

0.0

T-3

For our baseline experiments, we separate the attacks based
on the team being targeted. Thus we have 20 subsets. We then
sort the attack according to time. We reserve the first 90% of
the attacks for training and the rest 10% for testing. Attacker
prediction accuracy is used as the performance measure for
the experiment. Accuracy is defined as the fraction of correctly
classified test samples. Fig. 3 shows the accuracy for predicting
the attacker for each target team. Machine learning techniques
significantly outperform random guessing which would have
an average accuracy of choosing 1 out of 19 teams attacking
yielding an accuracy of 0.053. For this experiment random
forest classifier performs better than logistic regression, support vector machine and decision tree for all the target teams.
Table 3 below summarizes the average performance for each
method.

Average Performance

Decision tree (DT)

T-2

A. Experimental Results

Method

T-1

Decision Tree (DT). For baseline comparisons we first implemented a decision tree classifier. We built the decision tree
by finding the attribute that maximizes the information gain at
each split. In order to avoid over-fitting, the terminating criteria
is set to less than 0.1% of total samples.
Random Forest (RF). We use a random forest which combines bagging for each tree with random feature selection at
each node to split the data thus generating multiple decision
tree classifiers.
Support Vector Machine (SVM). Support vector machines
is a popular supervised classification technique that works
by finding a separating margin that maximizes the geometric
distance between classes. We use the popular LibSVM implementation [2] which is publicly available.
Logistic Regression (LOG-REG). Logistic regression classifies samples by computing the odds ratio. The odds ratio
gives the strength of association between the features and the
class. We implement the multinomial logistic regression which
handles multi-class classification.

TABLE 3: Summary of Prediction results averaged across all
Teams

Fraction of Misclassified Samples

approaches to predict the attacking team. The ground truth
corresponds to a team competing in the competition.

Teams

Non-Deceptive Duplicates

Deceptive Duplicates

Unseen payloads

Fig. 4: Sources of error in the misclassified samples.
Fig. 4 shows the distribution of the above mentioned
sources of misclassification for each team. Deceptive duplicates form the majority of misclassifications. This is not
surprising given the fact that deceptive duplicates make up
almost 90% of the total attacks (see Fig. 2).

0.6

IV.

0.5

We explore different pruning techniques to address
misclassification issues with respect to deceptive and nondeceptive duplicates. The pruning techniques are only applied
to the training data, while the test data is maintained at 10%
as mentioned in Section III-A. We use the random forest
classifier for all the pruning techniques.

0.4

Accuracy

P RUNING

0.3
0.2

0.1

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

T-3

T-2

T-1

0

Teams

LOG-REG

RF

SVM

DT

Fig. 3: Team prediction accuracy for LOG-REG, RF, SVM
and DT.

B. Misclassified Samples
Misclassification can be attributed to the following sources,

All-but-majority (P-1): In this pruning technique, for each
payload, we only retain duplicates of the most frequent attacking team and prune the duplicates of all other teams. This
pruned set is then used to train the random forest classifier.
Table 4 shows the classifier performance in comparison with
the baseline method. All-but-majority pruning technique has
better performance on the test set than the baseline approach
for 11 out of 20 teams. Using this pruning technique does
benefit majority of the teams as the prediction accuracy improves for them, but for some teams the performance drops.

The reason for the drop in performance for some teams is
due to the fact that training set gets dominated by a single
team which does not have majority in testing set. Since the
majority team gets represented in most of the leaves of the
random forest classifier, it gets predicted more often leading
to high misclassifications.
All-but-K-majority (P-2): In order to address the issue of
one team dominating in the training set, we use the all-but-Kmajority where we consider the K most frequent teams for a
payload under consideration. After trying out different values
of K we select K = 3, which gives the best performance.
For higher values of K, the pruning behaves like the baseline
approach and for lower values it behaves like All-but-majority.
On average each team gains about 40K samples in the training
set as compared to all-but-majority pruning. Table 4 shows
the classifier performance. In this case also pruning performs
better than baseline in 11 out of 20 teams, but as compared to
all-but-majority the performance for most teams is better.
All-but-earliest (P-3): For this pruning we only retain the
duplicates of the team that initiated the attack using a particular
payload. This pruning technique retains all the non-deceptive
duplicates while getting rid of the deceptive ones. Table 4
shows the classifier performance. This pruning technique performs better than the baseline approach for 8 out of 20 teams.
Comparing this result to all-but-majority (including all-butK-majority) pruning indicates that deceptive duplicates are
informative in attributing an attack to a team and should not
be ignored completely.
All-but-most-recent (P-4): In this pruning we repeat a similar
procedure like All-but-earliest but instead of retaining the
duplicates of the team that initiated an attack, we retain the
duplicates of the team that used the payload last in the training
set. Since the data is sorted according to time, the last attacker
becomes the most recent attacker for the test set. Table 4 shows
the classifier performance.
TABLE 4: Pruning technique performance comparison.
Teams

RF

P-1(RF)

P-2(RF)

P-3(RF)

P-4(RF)

T-1

0.45

0.16

0.46

0.15

0.15

T-2

0.22

0.28

0.30

0.15

0.14

T-3

0.30

0.53

0.29

0.57

0.57

T-4

0.26

0.33

0.27

0.31

0.32

T-5

0.26

0.38

0.45

0.40

0.42

T-6

0.50

0.27

0.24

0.31

0.26

T-7

0.45

0.59

0.58

0.19

0.49

T-8

0.42

0.52

0.52

0.51

0.55

T-9

0.41

0.65

0.68

0.52

0.53

T-10

0.30

0.54

0.34

0.55

0.57

T-11

0.37

0.27

0.35

0.27

0.29

T-12

0.24

0.37

0.37

0.25

0.22

T-13

0.35

0.27

0.37

0.29

0.27

T-14

0.42

0.27

0.40

0.30

0.30

T-15

0.30

0.20

0.27

0.21

0.20

T-16

0.42

0.28

0.22

0.32

0.31

T-17

0.43

0.45

0.35

0.43

0.40

T-18

0.48

0.39

0.43

0.41

0.40

T-19

0.41

0.65

0.58

0.54

0.60

T-20

0.48

0.16

0.16

0.16

0.17

Table 5 gives the summary of the prediction results for
all the pruning techniques in comparison with the random
forest baseline approach. In the pruning techniques All-butK-majority works best with an average accuracy of 0.42.

TABLE 5: Summary of Prediction results averaged across all
Teams
Method

Average Performance

Baseline Approach (RF)

0.37

All-but-majority Pruning (RF)

0.40

All-but-K-majority Pruning (RF)

0.42

All-but-earliest Pruning (RF)

0.34

All-but-most-recent Pruning (RF)

0.36

V.

C ONCLUSION

In this paper, we study cyber-attribution by examining
DEFCON CTF data - which provides us with ground-truth
on the culprit responsible for each attack. We frame cyberattribution as a classification problem and examine it using
several machine learning approaches. We find that deceptive
incidents account for the vast majority of misclassified samples
and introduce heuristic pruning techniques that alleviate this
problem somewhat. Moving forward, we look to employ a
more principled approach to counter deception based on our
previously established theoretical framework for reasoning
about cyber-attribution [5], [7]. In particular we wish to employ
temporal reasoning to tackle the problem of deceptive attacks.
This opens up interesting research questions in particular identifying hacking group from a series of attacks over a period of
time, differentiating between deceptive hacking groups in time
series data. This is a knowledge engineering challenge which
calls for development of efficient and scalable algorithms.
VI.

ACKNOWLEDGMENT

Some of this work was supported by the U.S. Office of
Naval Research and ASU Global Security Initiative (GSI).
R EFERENCES
[1]
[2]

[3]

[4]
[5]

[6]
[7]

[8]

[9]

W. E. Boebert. A survey of challenges in attribution. In Proceedings of
a workshop on Deterring CyberAttacks, pages 41â€“54, 2010.
C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector
machines. ACM Transactions on Intelligent Systems and Technology
(TIST), 2(3):27, 2011.
M. Dacier, V.-H. Pham, and O. Thonnard. The wombat attack attribution
method: some results. In Information Systems Security, pages 19â€“37.
Springer, 2009.
DEFCON. Defcon: Capture the flag. 2013.
S. Jajodia, P. Shakarian, V. S. Subrahmanian, V. Swarup, and C. Wang.
Cyber Warfare: Building the Scientific Foundation. Springer Publishing
Company, Incorporated, 2015.
P. Shakarian, J. Shakarian, and A. Ruef. Introduction to cyber-warfare:
A multidisciplinary approach. Newnes, 2013.
P. Shakarian, G. I. Simari, and M. A. Falappa. Belief revision in
structured probabilistic argumentation. In Foundations of Information
and Knowledge Systems, pages 324â€“343. Springer, 2014.
O. Thonnard, W. Mees, and M. Dacier. On a multicriteria clustering
approach for attack attribution. ACM SIGKDD Explorations Newsletter,
12(1):11â€“20, 2010.
N. Tsagourias. Nicolas politis initiatives to outlaw war and define
aggression, and the narrative of progress in international law. European
Journal of International Law, 23(1):255â€“266, 2012.

Toward Order-of-Magnitude Cascade Prediction
Ruocheng Guo , Elham Shaabani, Abhinav Bhatnagar and Paulo Shakarian
Arizona State University Tempe, AZ Email: {rguosni, shaabani, abhatn, shak}@asu.edu
Abstract--When a piece of information (microblog, photograph, video, link, etc.) starts to spread in a social network, an important question arises: will it spread to "viral" proportions ­ where "viral" is defined as an order-of-magnitude increase. However, several previous studies have established that cascade size and frequency are related through a power-law - which leads to a severe imbalance in this classification problem. In this paper, we devise a suite of measurements based on "structural diversity" ­ the variety of social contexts (communities) in which individuals partaking in a given cascade engage. We demonstrate these measures are able to distinguish viral from non-viral cascades, despite the severe imbalance of the data for this problem. Further, we leverage these measurements as features in a classification approach, successfully predicting microblogs that grow from 50 to 500 reposts with precision of 0.69 and recall of 0.52 for the viral class - despite this class comprising under 2% of samples. This significantly outperforms our baseline approach as well as the current state-of-the-art. Our work also demonstrates how we can tradeoff between precision and recall.

arXiv:1508.03371v1 [cs.SI] 13 Aug 2015

mimics reality. This differs from some previous studies which balance the data before conducting classification. Further, we note that we obtained prediction of order-of-magnitude increases in the size of the cascade - which also differs from other work (i.e. [1]) which focus on identifying cascades that double in size. II. T ECHNICAL P RELIMINARIES

I.

I NTRODUCTION

When a piece of information (microblog, photograph, video, link, etc.) starts to spread in a social network, an important question arises: will it spread to "viral" proportions ­ where "viral" is defined as an order-of-magnitude increase. Several previous studies [1], [2] have established that cascade size and frequency are related through a power-law - which leads to a severe imbalance in this classification problem. In this paper, we devise a suite of measurements based on "structural diversity" that are associated with the growth of a viral cascade in a social network. Structural diversity refers to the variety of social contexts in which an individual engages and is typically instantiated (for social networks) as the number of distinct communities represented in an individual's local neighborhood. Previously, Ugander et al. identified a correlation between structural diversity and influence [?]. We demonstrate these measures are able to distinguish viral from non-viral cascades, despite the severe imbalance of the data for this problem. Further, we leverage these measurements as features in a classification approach, successfully predicting microblogs that grow from 50 to 500 reposts with precision of 0.69 and recall of 0.52 for the viral class (under 2% of the samples). We note that our results on the prediction of cascades rely solely upon the use of our structural diversity based measures for features and limited temporal features - hence the prediction is based on network topology alone (no content information was utilized). We also achieved these results while maintaining the imbalances of the dataset - which we felt better
U.S. Provisional Patent 62/201,517. Contact shak@asu.edu for licensing information

Here we introduce necessary notation and describe our social network data. We represent a social network as a graph G = (V, E ) where V is the set of vertices and E as set of directed edges that have sizes |V |, |E | respectively. The intuition behind edge (v, v ) is that node v can influence v . This intuition stems from how we create the edges in our network: (v, v ) is an edge if during a specified time period there is at least one microblog posted by v that is reposted by v (we leave other thresholds beyond 1 repost to future work). We shall also assume a partition over nodes that specifies a community structure. We shall assume that such a partition is static (based on the same time period from which the edges were derived) and that the partition C consists of k communities: {C1 , C2 , ..., Ck }. There are many possible methods to derive the communities (if user-reported communities are not available). We utilize the Louvain algorithm to identify our communities in this paper due to its ability to scale. Cascades. For a given microblog , we denote the subset of first-m nodes who originally posted or reposted  as Vm and refer to them as adopters (at size m). Likewise, the set of reposting relationships within the same time period m . Taken together, we have a cascade: will be denoted R m m m D = (V , R ). Any valid original microblog  could be treated as a unique identifier for a cascade. Given a microblog , v is the originator at instance t0  , which is defined as the origin time when the originator posted the th repost microblog  and time t is time since t0  . The m m of the microblog  happens at time t . As m increases, a cascade accumulates nodes and edges over time. We shall use N to denote the final size of a cascade while the size of a cascade at any particular instance is the set of nodes present at that instance is simply |Vm |. For a given size m, we shall refer to the frontiers as the outgoing neighbors of the adopters in graph G who are not adopters themselves. Form mally: F = {v  V /Vm s.t. vi  Vm where (vi , v )  E }. For nodes in G that are outside the adopters, we shall use the notation texp (v, , m) to denote the number of time units from the initial post of  before the microblog was reposted by one of v 's incoming neighbors - intuitively the time at which v was exposed to . For a given natural number  (used to specify a time period), we define the  frontiers as a subset of the frontiers that have been exposed to 

no earlier than  time units previously. Formally this set m, m is defined as follows: F = {v  F |texp (v, , m)  }. Finally, the complement of this set are the  non-adopters: ¯ m, = {v  F m |texp (v, , m) > }. F   Sina Weibo Dataset. The dataset we used was provided by WISE 2012 Challenge1 . It included a sample of microblogs posted on Sina Weibo from 2009 to 2012. In this dataset, we are provided with time and user information for each post and subsequent repost which enabled us to derive a corpus of cascades. From this data, we derived our social network G = (V, E ) (with 17.9 M vertices and 52.4 M edges) that was created from reposts that were published during the 3 month period between May 1, 2011 and July 31, 2011. For this network, the average clustering coefficient is 0.107. There are 4974 connected components in the network. Louvain algorithm outputs 379,416 communities with average size of 47.5 for this network. As expected, this network exhibits a power-law degree distrubtion. For this network, the number of active nodes in August (the time period we studied for cascade prediction) is 5,910,608, while 5,664,625 of them at least have one out-neighbor. During the month of August, there were 9,323,294 reposts with 2,252,368 different original microblogs. 1,920,763 (86.6%) of them were written by authors who at least published one microblog during May 1, 2011 to July 31, 2011 (the time period we used to create the underlying network). The average time it took for viral cascades to become viral is approximately 18 hours. The distribution of final size of cascades mimics a power-law distribution which can demonstrate that this dataset is more representative of cascade behavior observed "in the wild". This differs significantly from the previous works which conduct biased sampling to artificially provide balanced classes. We selected  as 30 minutes as 90% of all reposts in the initial 3 month period occurred in under this time. Number of communities. For V  V , the associated communities C (V ) are the communities represented by V . Formally: C (V ) = {Ci  C s.t. V  Ci = }. The cardinality of this set (number of communities) will be denoted K (V ). We measure the number of communities represented by the above three m, ¯ m, ) observed populations of nodes: K (Vm ), K (F ), K (F  at either a given cascade size. Gini impurity. For V  V , the gini impurity, IG (V ) is the probability of a node in V being placed into the incorrect community if assigned a community based on the distribution of communities represented in V . Formally: IG (V ) = |Ci V | 1 - i ( |V | )2 . We study the gini impurity of the adopters,  non-adopters and  frontiers for either a given cascade size m, ¯ m, ). The intuition is to capture m: IG (Vm ), IG (F ), IG (F  a notion of how the communities are distributed amongst the nodes in each of these sets with a single scalar value. We note that the impurity of the adopter set IG (Vm ) behaves similar to the entropy of this set (a measurement introduced in [3]). However, as we will see in the next two sections, we found that the impurity of the  frontiers is a more discriminating feature. Overlap. For Va , Vb  V , the overlap (O(Va , Vb )) is simply the number of shared communities. Formally: O(Va , Vb ) =
1 http://www.wise2012.cs.ucy.ac.cy/challenge.html

|C (Va )  C (Vb )|. We study overlap between adopters and  frontiers, between adopters and  non-adopters, and between m, ¯ m, ),  frontiers and  non-adopters: O(Vm , F ), O(Vm , F  m, ¯ m, and O(F , F ) respectively. The intuition with overlap stems directly from the original structural diversity results of [?] - for instance a high overlap between adopters and  frontiers may indicate that the  frontiers are linked to adopters with inner-community connections and high structural diversity - hence increasing the probability of adoption. Average time to adoption. The average time to adoption for the nodes in the current set of adopters (once the cascade grows m i =1 t to size m): im . We also use average time to adoption as a baseline measure. III. R ESULTS

Here we examine the behavior of the various structural diversity measurements as viral and non-viral cascades progress. We define a cascade as viral if the number of reposts reaches a threshold (denoted T H ) of 500 (in the next section we will explore other settings for T H when describing our classification results). We look at snapshots of the cascades as they progress both in terms of size (denoted m). For m = {10, 30, 50, 100, 200}, the number of samples is {98832, 26733, 13285, 4722, 1324} respectively with 208 of the samples are viral. With each size m we consider the m Cascades with m adopters at some time tm  , t can vary for different . Hence, cascades with final size N < m are ignored in our analysis task. This leads to a decrease in the number of non-viral Cascades as m increases. Average time to adoption. As a baseline measurement, we study the average time to adoption for each size-based stage of the cascade process (Fig. 1i, Fig. 1j). As expected, viral cascades exhibit a faster rate of reposting. While we note that significant differences are present - especially in the early stages of the cascade, the whiskers of the non-viral class indicate a significant proportion of non-viral cascades that exhibit rapid adoption. We believe this is likely due to the fact that certain cascades may have very high appeal to specialized communities. Number of communities. Fig. 1a, Fig. 1b, Fig. 1c and Fig. 1d display how the number of communities K (V ) increases over m, m = {10, 30, 50, 100, 200} for the sets V = Vm , F . m We note that K (V ) (the communities represented in the set of adopters) was shown to be a useful feature in [3] for tasks where the target class had fewer reposts than in this study. Here, we note that while statistically significant differences exist, the average and median values at each of the examined stages are generally similar. On the other hand, the m, communities represented by the set of  frontiers (K (F )) shows viral Cascades have stronger capability than non-viral ones to keep a diverse set of  frontiers. We also noted that the ¯ m, ) (not pictured) shows viral cascades start median of K (F  m, with smaller K (F ). However, it increases faster in viral cascades as nodes in  frontiers becomes  non-adopters. Gini impurity. Cascades in both classes tend to accumulate diversity in the process of collecting more adopters - and we have also noted that a related entropy measure (studied in [3]) performed similarly. We also noted (not pictured) that in the

TABLE I: Features: Cascade Prediction over Time and Size
Group Feature(s) over size
m, ¯ m, ),IG (V m ),IG (F m, ),IG (F ¯ m, ), K (F ),K (F    

Am

m, m, ¯ m, m m ¯ m, O (V , F ),O (V , F ),O (F , F ), m, ¯ m, |, |F |, |F  m ti i=1  m

, m  {30, 50}

10

Bm Cm

Community Features Mentioned in [3] and Cm
m ti i=1  m

Number of Adopters

30

50

100

200

Number of Communities

70 60 50 40 30 20 10 0

M: 8.0 A: 7.7

18.0 17.5

25.0 24.0

35.0 34.9

48.0 47.6
70 60 50 40 30 20 10 0

Number of Communities

M: 8.0 A: 8.1

17.0 17.3

23.0 23.5

34.0 34.0

46.5 46.5

10

, m = 50

Number of Adopters

30

50

100

200

Number of Communities(102 )

Overlap. We found that overlap grows with the number of adopters in the three types of overlap considered. For m ), viral cascades start with a larger initial value O(Vm , F and keep leading non-viral ones in the diffusion process of first 200 nodes (Fig. 1e, Fig. 1f). This may hint that viral cascades also take advantage of the densely linked communities to help ¯ m ) and them become viral. However, in the case of O(Vm , F  m, ¯ m, O(F , F ), viral cascades begin with lower value but grow much faster than non-viral Cascades. Classification Experiments. Here we examine our experiments for predicting whether a cascade becomes viral - when a size threshold (T H ) exceeds 500 adopters given that the cascade has 50 adopters (s = 50). Based on the distribution of final size of cascades in this dataset, this is a binary classification task with two heavily imbalanced classes. Hence, we report performance measurements (precision, recall and F1 score) for only the minority (viral) class. Throughout the course of our experiments, we found that varying threshold (slightly modifying the definition of "viral") for only the training set allows for a trade-off between precision and recall. We study the trend of performance measures in two cases: (1.) The threshold for test set is maintained as T Hts = 500 while the training threshold is varied T Htr = {300, 400, 500, 600, 700}. (2.) The two thresholds are kept as the same T H while we modify this value T H = {300, 400, 500, 600, 700}. Table I shows the groups of features used in our prediction tasks. The features introduced in this paper is group Am . As a baseline method for size-based prediction (feature group Cm ) we used average time to adoption. We also compare our features (Group Am ) with the community features extracted in [3] (Group Bm ). This was the best performing feature set in that paper for a comparable task.2 Additionally, we study the average size of recalled and non-recalled viral cascades by classifiers using features in groups Am . We also investigate the significance and performance of individual and certain combinations of features introduced in this paper. We used ten-fold cross-validation in our experiments to
2 This was their highest-performing set of features for predicting cascades that grew from 50 to 367 and 100 to 417 reposts. We also included the baseline feature in this set as we found it improved the effectiveness of this approach.

10

Number of Adopters

30

50

100

200

Number of Communities(102 )

early stages, viral cascades can show more diversity in  fronm, tiers measured by IG (F ) (m = {10, 30, 50}). But, perhaps most striking, that non-viral Cascades gain more uniformly distributed nodes over communities in  non-adopters, shown ¯ m, ) (Fig. 1g, Fig. 1h). We believe that this is due to by IG (F  non-viral cascades likely have an appeal limited to a relatively small number of communities - hence those not adopting the trend may represent a more diverse set of communities.

(a) Number of communities amongst m )) for non-viral casadopters (K (V cades
M: 7.0 A: 25.7 15.0 39.6 20.0 53.2 27.0 33.0 88.5 111.1

(b) Number of communities amongst m )) for viral cascades adopters (K (V
M: 21.0 A: 24.3 30.0 41.7 30.0 44.4 33.5 78.7 42.5 88.6

1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0

1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0

10

Number of Adopters

30

50

100

200

(c) Number of communities amongst m,  frontiers (K (F )) for non-viral cascades
M: 3.0 A: 3.7 9.0 8.7 12.0 12.3 19.0 18.5 26.0 25.2

(d) Number of communities amongst m,  frontiers (K (F )) for viral cascades
M: 7.0 A: 6.7 13.0 12.7 17.0 16.5 22.5 22.2 31.0 29.5

60 50 40 30 20 10 0

60 50 40 30 20 10 0

Overlap

10

Number of Adopters

30

50

100

200

Overlap

10

Number of Adopters

30

50

100

200

(e) Overlap of adopters and  fronm , F m, )) for non-viral tiers (O(V  cascades
M: 0.8 A: 0.8 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9

(f) Overlap of adopters and  fronm , F m, )) for viral castiers (O(V  cades
M: 0.0 A: 0.4 0.9 0.8 0.9 0.9 0.9 0.9 0.9 0.9

1.0 0.8

1.0 0.8

Gini

0.4 0.2 0.0 10

Gini
30 50 100 200

0.6

0.6

0.4 0.2

Number of Adopters

0.0

10

Number of Adopters

30

50

100

200

(g) Gini impurity of  non-adopters ¯ m, )) for non-viral cascades (IG (F 
M: 865.9 A: 780.3 853.1 804.5 754.6 765.2 790.1 771.0 753.8 759.9

(h) Gini impurity of  non-adopters ¯ m, )) for viral cascades (IG (F 
M: 15.3 A: 40.9 49.7 78.4 168.1 301.1 86.9 129.4 215.8 347.7

1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0

10

Number of Adopters

30

50

100

200

1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0

Average Time(103 )

Average Time(103 )

10

Number of Adopters

30

50

100

200

(i) Non-viral cascades

(j) Viral cascades

Fig. 1: Number of communities, gini impurity, overlap and average time since t0  to adoption for m = {10, 30, 50, 100, 200}

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Precision

Am Bm

Cm

1.0 0.8 0.6 0.4

precision

recall

f1 score

Recall

F1 Score

0.2

300

Fig. 2: Classification results based on groups of features (Am ,Bm ,Cm ) extracted when m = 50 for fixed T Htr = 500, T Hts = 500. Error bars represent one standard deviation. ensure the results do not take any advantage of randomness in picking training and testing sets. First we carried out the prediction tasks with fixed thresholds T Htr = 500, T Hts = 500. Then we modify the training threshold T Htr = {300, 400, 500, 600, 700} to show how this achieves a trade-off between precision and recall. The difference in average final size between correctly classified viral cascades and incorrectly classified ones is also monitored over T Htr = {300, 400, 500, 600, 700} to show the potential to predict exact number of adopters by features. Furthermore, we modify threshold of both training and testing sets T H = {300, 400, 500, 600, 700} to show the robustness of our features on related classification problems. We used the oversampling method SMOTE with random forest classifier to generate synthetic samples for the viral class. Other, lesserperforming classifiers were also examined (including SVM, MLP, and other ensemble methods) and are not reported here. All results shown in this section is a sample mean produced by ten repeated experiments under each combination of variables. Size-based prediction. We studied cascades of size 50 that reached 500 for this task. There are 13,285 cascades that can reach the size m = 50 while 208 out of them reached the size of 500. Maintaining the threshold T H = 500, Fig. 2 shows random forest classifier trained with features in group Am can outperform the other groups. The trade-off between precision and recall can be achieved by changing the training threshold T Htr while maintaining the testing threshold (Fig. 3a). We also note that the average final size of viral cascades recalled by the classifier increases with the training threshold (Fig. 3b). With threshold T H = {300, 400, 500, 600, 700} on both training and testing samples, the features of group Am consistently outperform those previously introduced (Bm ) (Fig. 3c, Fig. 3d). Feature investigation. Here we investigate the importance of each feature in Am . With T Htr = 500 and T Hts = 500, we trained 100 randomized logistic regressions models - each assigning weights to the features in those sets. We then categorized the features with weight larger than 0.01 (on average) into groups such as overlap, gini impurity, etc. Then, we performed classification on the basis of single feature categories or combination of such categories. The average weights assigned are shown in Table II. As shown by these results, overlaps can make significant contribution to cascade prediction. Intuitively, communication between two sets of nodes is more likely to happen in their shared communities which is consistent with the results of [?]. This implies that the larger overlap value, the more influence of one set on the other. For example, we can infer that viral cascades tend to have

400 500 600 Training Threshold

700

1.4 recalled not recalled 1.3 1.2 1.1 1.0 0.9 0.8 0.7 0.6 300 400 500 600 Training Threshold

mean

Size (103 )

700

(a) Results for features in Am with (b) Average final size of viral cascades different T Htr . (recalled, mean and not recalled)
1.0 0.8 0.6 0.4 0.2 0.0 300 400 500 600 700 Training/Testing Threshold
precision recall f1 score

1.0 0.8 0.6 0.4 0.2

precision

recall

f1 score

0.0 300

400 500 600 700 Training/Testing Threshold

(c) Results for features in group Am (d) Results for features in group Bm when T Htr and T Hts change. when T Htr and T Hts change.

Fig. 3: Prediction results for Am when T Htr and T Hts change. Error bars represent one standard deviation.

Name Gini

Features
50, IG (F ) ¯ 50, ) IG (F  ¯ 30, ) IG (F  50 ti i=1  50

Weights 0.02 0.02 0.52 1.00

Name

Features
30, 30 O (V , F ) 30 ¯ 30, O (V , F )

Weights 0.50 0.04 0.23 0.50 0.26

Overlap

Baseline

  30, ¯ 30, O (F , F ) 50, 50 O (V , F ) 50, ¯ 50, O (F , F )

TABLE II: Weights of features assigned by randomized logistic regression models
m, larger O(Vm , F ) value for adopters have larger chance to influence the  frontiers than non-viral cascades. Moverover, the gini impurity of  non-adopters also shows its importance. Intuitively, non-viral cascades are easier to be trapped in a relatively small amount of communities. This means even if they could show up in people's timeline with high structural diversity but can not get them infected.

IV.

ACKNOWLEDGMENT

This work is supported through the AFOSR Young Investigator Program (YIP), grant number FA9550-15-1-0159. R EFERENCES
J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec, "Can cascades be predicted?" in Proceedings of the 23rd international conference on World wide web. International World Wide Web Conferences Steering Committee, 2014, pp. 925­936. [2] P. Shakarian, A. Bhatnagar, A. Aleali, R. Guo, and E. Shaabani, Diffusion in Social Networks. Springer (in press), 2015. [Online]. Available: http://lab.engineering.asu.edu/cysis/diffusion/ [3] L. Weng, F. Menczer, and Y.-Y. Ahn, "Predicting successful memes using network and community structure," in Eighth International AAAI Conference on Weblogs and Social Media, 2014. [1]

arXiv:1507.01922v1 [cs.CR] 7 Jul 2015

Cyber-Deception and Attribution
in Capture-the-Flag Exercises
Eric Nunes, Nimish Kulkarni, Paulo Shakarian

Andrew Ruef, Jay Little

School of Computing, Informatics and
Decision Systems Engineering
Arizona State University
Tempe, AZ 85281, USA
Email: {enunes1, nimish.kulkarni, shak} @asu.edu

Trail of Bits, Inc.
New York, NY 10003, USA
Email: {andrew, jay} @trailofbits.com

Abstractâ€”Attributing the culprit of a cyber-attack is widely
considered one of the major technical and policy challenges
of cyber-security. The lack of ground truth for an individual
responsible for a given attack has limited previous studies. Here,
we overcome this limitation by leveraging DEFCON capture-theflag (CTF) exercise data where the actual ground-truth is known.
In this work, we use various classification techniques to identify
the culprit in a cyberattack and find that deceptive activities
account for the majority of misclassified samples. We also explore
several heuristics to alleviate some of the misclassification caused
by deception.

I.

I NTRODUCTION

Attributing the culprit of a cyber-attack is widely considered one of the major technical and policy challenges of
cyber-security. The lack of ground truth for an individual
responsible for a given attack has limited previous studies. In
this study, we take an important first step toward developing
computational techniques toward attributing the actual culprit
(here hacking group) responsible for a given cyber-attack. We
leverage DEFCON capture-the-flag (CTF) exercise data which
we have processed to be amenable to various machine learning
approaches. Here, we use various classification techniques to
identify the culprit in a cyber-attack and find that deceptive
activities account for the majority of misclassified samples. We
also explore several heuristics to alleviate some of the misclassification caused by deception. Our specific contributions are
as follows:
â€¢

We assemble a dataset of cyber-attacks with ground
truth derived from the traffic of the CTF held at
DEFCON 21 in 2013.

â€¢

We analyze this dataset to identify cyber-attacks where
deception occurred.

â€¢

We frame cyber-attribution as a multi-label classification problem and leverage several machine learning
approaches. We find that deceptive incidents account
for the vast majority of misclassified samples.

explore the deception hypothesis in a cyber-warfare scenario.
When compared to other domains of warfare, there is a much
greater potential for evidence found in the aftermath of cyberattack to be planted by the adversary for purposes of deception.
The policy implications of cyber-attribution have also been
discussed in [9] where the authors point out that anonymity,
ability to launch multi-stage attacks, and attack speed pose
significant challenges to cyber attribution.
In an early survey on cyber-attribution [1], the authors point
out that technical attribution will generally identify machines,
as opposed to a given hacker and his/her affiliations. While
we will use technical information in our approach, we have
ground truth data on the group involved by the nature of
the capture-the-flag data. This will allow our approach to
profile the tactics, techniques, and procedures of a given group
as we have ground-truth information on a hacking group as
opposed to machines. An example of such an approach is the
WOMBAT attribution method [3] which attributes behavior
to IP sources that are potentially linked to some root cause
determined through a clustering technique. Similarly, other
work [8] combines cluster analysis with a component for multicriteria decision analysis and studied an implementation of this
approach using honeypot data â€“ again, this approach lacks any
ground truth of the actual hacker or hacking group.
Concurrently, we have devised a formal logical framework
for reasoning about cyber-attribution [5], [7]. However, we
have not studied how this framework can be instantiated on
a real world dataset and, to date, we have not reported on an
implementation or experiments in the literature. We note that
none of the previous work on cyber-attribution leverages a
data set with ground truth information of actual hacker groups
â€“ which is the main novelty of this paper.
II.

DATASET

We introduce several pruning techniques and show
that they can reduce the effect of deception as well as
provide insight into the conditions in which deception
was employed by the participants of the CTF.

Our dataset consists of events recorded from a Capturethe-flag (CTF) tournament held at DEFCON 21 in 2013.
Briefly, CTF competitions act as educational exercise that
exposes real world attack scenarios to participants. Network
sniffing, analysis of protocols, programming and system level
knowledge, cryptanalysis are some of the instrumental skills
acquired by contestants.

In our text on cyber-warfare [6], we discuss the difficulties
of cyber-attribution and how an intelligence analyst must also

Our data represents attack/defense style, where each team
owns a small network of machines to defend. Teams are judged

â€¢

Robot Mafia

svc

02345

payload hash

2cc03b4e0053cde24400bbd80890446c

time

2013-08-03T23:45:17

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

T-3

T-2

T-1

T-20
T-20

men in black hats

to team

T-19

from team

0

T-18

cmp:12 , svcmi:2, subs:8, movtmi:60 ......

T-17

0Ã—43:245, 0Ã—69:8, 0Ã—3a:9, 0Ã—5d:1, .....

inst hist

T-16

byte hist

200000

T-15

Value

400000

T-14

Field

600000

T-13

TABLE 2: Example event from the dataset

800000

T-12

From this pre-processing of the network data (packets) we
have around 10 million network attacks. There are 20 teams
in the CTF competition. In order to attribute an attack to a
particular team, apart from analyzing the payloads used by the
team, we also need to analyze the behavior of the attacking
team towards his adversary. For this purpose we separate the
network attacks according to the team being targeted. Thus
we have 20 such subsets. We represent the 20 subsets (teams)
as T-i, where i = 1, 2, 3...20. An example of an event in the
dataset is shown in Table 2.

T-11

indicates the date and time of the attack

T-10

indicates the payload used in the attack (md5)

time

T-9

the service that the payload is running

payload hash

T-8

the team being attacked by the payload

svc

Duplicate attacks: A duplicate attack occurs when the same
team uses the same payload to attack a team at different time
instances. Duplicate attacks can be attributed to two reasons.
First when a team is trying to compromise other systems, it
just does not launch a single attack but a wave of attacks with
very little time difference between consecutive attacks. Second,
once a successful payload is created which can penetrate the
defense of other systems, it is used more by the original
attacker as well as the deceptive one as compared to other
payloads. We group duplicates as being non-deceptive and
deceptive. Non-deceptive duplicate are the duplicates of the
team that first initiated the use of a particular payload. On the
other hand deceptive duplicates are all the attacks from the
teams that are being deceptive. Deceptive duplicates form a
large portion of the dataset as seen in Fig. 2.

T-7

the team where the payload originates (attacking team)

to team

Deceptive Attacks

Fig. 1: Unique deceptive attacks directed towards each team.

T-6

histogram of instructions used in the payload

from team

Teams

Unique Attacks

T-5

histogram of byte sequences in the payload

inst hist

0

T-4

Intuition

byte hist

100000

T-3

Field

200000

T-2

TABLE 1: Fields in an instance of network attack

300000

T-1

For each file, we computed an md5 checksum, a byte
histogram, and an ARM instruction histogram. This data was
recorded as a list of tuples (time-stamp, hash, byte-histogram,
instruction-histogram) in a JSON document. These individual
fields of the tuple are listed in Table 1.

400000

Unique Attacks

DEFCON CTF organizers recorded network traffic that
includes network packets generating to and from all participating teams and is available on Internet [4]. Recordings are
stored as archive files of PCAP (packet capture) for each team
(destination as that team) separately. PCAP file contains packet
headers (TCP, SSL, UDP etc.) and respective data as source,
destination, sequence numbers etc. with timestamp having
millisecond precision. Using open source tool tcpflow1 , we
interpreted collection of PCAPs as cumulative data streams.
Tcpflow reconstructs actual data streams from the packets
that proved helpful in protocol analysis and debugging. This
tool produces a file containing the contents of each stream,
representing the data sent between two points in the CTF
system.

attack to a team difficult.
Deception: In the context of this paper we define an attack to
be deceptive when multiple adversaries get mapped to a single
attack pattern. In the current setting we define deception as the
scenario when the same payload is used by multiple teams to
target the same team. Fig. 1 shows the distribution of unique
deception attacks with respect to the total unique attacks in
the dataset based on the target team. These unique deceptive
attacks amount to just under 35% of the total unique attacks.

Total Attacks

based on scores given to attack machines of other teams as well
as defending their own network. Initially, all virtual machines
are configured with specific set of services. These services
are vulnerable to state-of-art hacking techniques. Files can be
considered as form of flag to be captured from other teams or
to be planted to other teams by exploiting those vulnerabilities.

Teams

Non-Deceptive

Deceptive

Total Attacks

Fig. 2: Total attacks and duplicate attacks(Deceptive and
Non-deceptive) directed towards each team

A. Dataset Analysis
We now discuss two important observations from the
dataset, that makes the task of attributing an observed network
1 https://github.com/simsong/tcpflow

III.

BASELINE A PPROACHES

From the dataset, we have the ground truth available for
all the samples. Hence we use supervised machine learning

0.26

Logistic regression (LOG-REG)

0.31

Support vector machine (SVM)

0.30

Random Forest (RF)

0.37

â€¢

Non-deceptive duplicate attacks attributed to one of
the deceptive teams.

â€¢

Deceptive duplicates attributed to some other deceptive team.

â€¢

Payloads that were not encountered during the training
phase.

The first two sources of error make up the majority of
misclassifications, since a given attack can be attributed to any
of the 19 teams.
1.0

0.8

0.6

0.4

0.2

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

0.0

T-3

For our baseline experiments, we separate the attacks based
on the team being targeted. Thus we have 20 subsets. We then
sort the attack according to time. We reserve the first 90% of
the attacks for training and the rest 10% for testing. Attacker
prediction accuracy is used as the performance measure for
the experiment. Accuracy is defined as the fraction of correctly
classified test samples. Fig. 3 shows the accuracy for predicting
the attacker for each target team. Machine learning techniques
significantly outperform random guessing which would have
an average accuracy of choosing 1 out of 19 teams attacking
yielding an accuracy of 0.053. For this experiment random
forest classifier performs better than logistic regression, support vector machine and decision tree for all the target teams.
Table 3 below summarizes the average performance for each
method.

Average Performance

Decision tree (DT)

T-2

A. Experimental Results

Method

T-1

Decision Tree (DT). For baseline comparisons we first implemented a decision tree classifier. We built the decision tree
by finding the attribute that maximizes the information gain at
each split. In order to avoid over-fitting, the terminating criteria
is set to less than 0.1% of total samples.
Random Forest (RF). We use a random forest which combines bagging for each tree with random feature selection at
each node to split the data thus generating multiple decision
tree classifiers.
Support Vector Machine (SVM). Support vector machines
is a popular supervised classification technique that works
by finding a separating margin that maximizes the geometric
distance between classes. We use the popular LibSVM implementation [2] which is publicly available.
Logistic Regression (LOG-REG). Logistic regression classifies samples by computing the odds ratio. The odds ratio
gives the strength of association between the features and the
class. We implement the multinomial logistic regression which
handles multi-class classification.

TABLE 3: Summary of Prediction results averaged across all
Teams

Fraction of Misclassified Samples

approaches to predict the attacking team. The ground truth
corresponds to a team competing in the competition.

Teams

Non-Deceptive Duplicates

Deceptive Duplicates

Unseen payloads

Fig. 4: Sources of error in the misclassified samples.
Fig. 4 shows the distribution of the above mentioned
sources of misclassification for each team. Deceptive duplicates form the majority of misclassifications. This is not
surprising given the fact that deceptive duplicates make up
almost 90% of the total attacks (see Fig. 2).

0.6

IV.

0.5

We explore different pruning techniques to address
misclassification issues with respect to deceptive and nondeceptive duplicates. The pruning techniques are only applied
to the training data, while the test data is maintained at 10%
as mentioned in Section III-A. We use the random forest
classifier for all the pruning techniques.

0.4

Accuracy

P RUNING

0.3
0.2

0.1

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

T-3

T-2

T-1

0

Teams

LOG-REG

RF

SVM

DT

Fig. 3: Team prediction accuracy for LOG-REG, RF, SVM
and DT.

B. Misclassified Samples
Misclassification can be attributed to the following sources,

All-but-majority (P-1): In this pruning technique, for each
payload, we only retain duplicates of the most frequent attacking team and prune the duplicates of all other teams. This
pruned set is then used to train the random forest classifier.
Table 4 shows the classifier performance in comparison with
the baseline method. All-but-majority pruning technique has
better performance on the test set than the baseline approach
for 11 out of 20 teams. Using this pruning technique does
benefit majority of the teams as the prediction accuracy improves for them, but for some teams the performance drops.

The reason for the drop in performance for some teams is
due to the fact that training set gets dominated by a single
team which does not have majority in testing set. Since the
majority team gets represented in most of the leaves of the
random forest classifier, it gets predicted more often leading
to high misclassifications.
All-but-K-majority (P-2): In order to address the issue of
one team dominating in the training set, we use the all-but-Kmajority where we consider the K most frequent teams for a
payload under consideration. After trying out different values
of K we select K = 3, which gives the best performance.
For higher values of K, the pruning behaves like the baseline
approach and for lower values it behaves like All-but-majority.
On average each team gains about 40K samples in the training
set as compared to all-but-majority pruning. Table 4 shows
the classifier performance. In this case also pruning performs
better than baseline in 11 out of 20 teams, but as compared to
all-but-majority the performance for most teams is better.
All-but-earliest (P-3): For this pruning we only retain the
duplicates of the team that initiated the attack using a particular
payload. This pruning technique retains all the non-deceptive
duplicates while getting rid of the deceptive ones. Table 4
shows the classifier performance. This pruning technique performs better than the baseline approach for 8 out of 20 teams.
Comparing this result to all-but-majority (including all-butK-majority) pruning indicates that deceptive duplicates are
informative in attributing an attack to a team and should not
be ignored completely.
All-but-most-recent (P-4): In this pruning we repeat a similar
procedure like All-but-earliest but instead of retaining the
duplicates of the team that initiated an attack, we retain the
duplicates of the team that used the payload last in the training
set. Since the data is sorted according to time, the last attacker
becomes the most recent attacker for the test set. Table 4 shows
the classifier performance.
TABLE 4: Pruning technique performance comparison.
Teams

RF

P-1(RF)

P-2(RF)

P-3(RF)

P-4(RF)

T-1

0.45

0.16

0.46

0.15

0.15

T-2

0.22

0.28

0.30

0.15

0.14

T-3

0.30

0.53

0.29

0.57

0.57

T-4

0.26

0.33

0.27

0.31

0.32

T-5

0.26

0.38

0.45

0.40

0.42

T-6

0.50

0.27

0.24

0.31

0.26

T-7

0.45

0.59

0.58

0.19

0.49

T-8

0.42

0.52

0.52

0.51

0.55

T-9

0.41

0.65

0.68

0.52

0.53

T-10

0.30

0.54

0.34

0.55

0.57

T-11

0.37

0.27

0.35

0.27

0.29

T-12

0.24

0.37

0.37

0.25

0.22

T-13

0.35

0.27

0.37

0.29

0.27

T-14

0.42

0.27

0.40

0.30

0.30

T-15

0.30

0.20

0.27

0.21

0.20

T-16

0.42

0.28

0.22

0.32

0.31

T-17

0.43

0.45

0.35

0.43

0.40

T-18

0.48

0.39

0.43

0.41

0.40

T-19

0.41

0.65

0.58

0.54

0.60

T-20

0.48

0.16

0.16

0.16

0.17

Table 5 gives the summary of the prediction results for
all the pruning techniques in comparison with the random
forest baseline approach. In the pruning techniques All-butK-majority works best with an average accuracy of 0.42.

TABLE 5: Summary of Prediction results averaged across all
Teams
Method

Average Performance

Baseline Approach (RF)

0.37

All-but-majority Pruning (RF)

0.40

All-but-K-majority Pruning (RF)

0.42

All-but-earliest Pruning (RF)

0.34

All-but-most-recent Pruning (RF)

0.36

V.

C ONCLUSION

In this paper, we study cyber-attribution by examining
DEFCON CTF data - which provides us with ground-truth
on the culprit responsible for each attack. We frame cyberattribution as a classification problem and examine it using
several machine learning approaches. We find that deceptive
incidents account for the vast majority of misclassified samples
and introduce heuristic pruning techniques that alleviate this
problem somewhat. Moving forward, we look to employ a
more principled approach to counter deception based on our
previously established theoretical framework for reasoning
about cyber-attribution [5], [7]. In particular we wish to employ
temporal reasoning to tackle the problem of deceptive attacks.
This opens up interesting research questions in particular identifying hacking group from a series of attacks over a period of
time, differentiating between deceptive hacking groups in time
series data. This is a knowledge engineering challenge which
calls for development of efficient and scalable algorithms.
VI.

ACKNOWLEDGMENT

Some of this work was supported by the U.S. Office of
Naval Research and ASU Global Security Initiative (GSI).
R EFERENCES
[1]
[2]

[3]

[4]
[5]

[6]
[7]

[8]

[9]

W. E. Boebert. A survey of challenges in attribution. In Proceedings of
a workshop on Deterring CyberAttacks, pages 41â€“54, 2010.
C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector
machines. ACM Transactions on Intelligent Systems and Technology
(TIST), 2(3):27, 2011.
M. Dacier, V.-H. Pham, and O. Thonnard. The wombat attack attribution
method: some results. In Information Systems Security, pages 19â€“37.
Springer, 2009.
DEFCON. Defcon: Capture the flag. 2013.
S. Jajodia, P. Shakarian, V. S. Subrahmanian, V. Swarup, and C. Wang.
Cyber Warfare: Building the Scientific Foundation. Springer Publishing
Company, Incorporated, 2015.
P. Shakarian, J. Shakarian, and A. Ruef. Introduction to cyber-warfare:
A multidisciplinary approach. Newnes, 2013.
P. Shakarian, G. I. Simari, and M. A. Falappa. Belief revision in
structured probabilistic argumentation. In Foundations of Information
and Knowledge Systems, pages 324â€“343. Springer, 2014.
O. Thonnard, W. Mees, and M. Dacier. On a multicriteria clustering
approach for attack attribution. ACM SIGKDD Explorations Newsletter,
12(1):11â€“20, 2010.
N. Tsagourias. Nicolas politis initiatives to outlaw war and define
aggression, and the narrative of progress in international law. European
Journal of International Law, 23(1):255â€“266, 2012.

Toward Order-of-Magnitude Cascade Prediction
Ruocheng Guo , Elham Shaabani, Abhinav Bhatnagar and Paulo Shakarian
Arizona State University Tempe, AZ Email: {rguosni, shaabani, abhatn, shak}@asu.edu
Abstract--When a piece of information (microblog, photograph, video, link, etc.) starts to spread in a social network, an important question arises: will it spread to "viral" proportions ­ where "viral" is defined as an order-of-magnitude increase. However, several previous studies have established that cascade size and frequency are related through a power-law - which leads to a severe imbalance in this classification problem. In this paper, we devise a suite of measurements based on "structural diversity" ­ the variety of social contexts (communities) in which individuals partaking in a given cascade engage. We demonstrate these measures are able to distinguish viral from non-viral cascades, despite the severe imbalance of the data for this problem. Further, we leverage these measurements as features in a classification approach, successfully predicting microblogs that grow from 50 to 500 reposts with precision of 0.69 and recall of 0.52 for the viral class - despite this class comprising under 2% of samples. This significantly outperforms our baseline approach as well as the current state-of-the-art. Our work also demonstrates how we can tradeoff between precision and recall.

arXiv:1508.03371v1 [cs.SI] 13 Aug 2015

mimics reality. This differs from some previous studies which balance the data before conducting classification. Further, we note that we obtained prediction of order-of-magnitude increases in the size of the cascade - which also differs from other work (i.e. [1]) which focus on identifying cascades that double in size. II. T ECHNICAL P RELIMINARIES

I.

I NTRODUCTION

When a piece of information (microblog, photograph, video, link, etc.) starts to spread in a social network, an important question arises: will it spread to "viral" proportions ­ where "viral" is defined as an order-of-magnitude increase. Several previous studies [1], [2] have established that cascade size and frequency are related through a power-law - which leads to a severe imbalance in this classification problem. In this paper, we devise a suite of measurements based on "structural diversity" that are associated with the growth of a viral cascade in a social network. Structural diversity refers to the variety of social contexts in which an individual engages and is typically instantiated (for social networks) as the number of distinct communities represented in an individual's local neighborhood. Previously, Ugander et al. identified a correlation between structural diversity and influence [?]. We demonstrate these measures are able to distinguish viral from non-viral cascades, despite the severe imbalance of the data for this problem. Further, we leverage these measurements as features in a classification approach, successfully predicting microblogs that grow from 50 to 500 reposts with precision of 0.69 and recall of 0.52 for the viral class (under 2% of the samples). We note that our results on the prediction of cascades rely solely upon the use of our structural diversity based measures for features and limited temporal features - hence the prediction is based on network topology alone (no content information was utilized). We also achieved these results while maintaining the imbalances of the dataset - which we felt better
U.S. Provisional Patent 62/201,517. Contact shak@asu.edu for licensing information

Here we introduce necessary notation and describe our social network data. We represent a social network as a graph G = (V, E ) where V is the set of vertices and E as set of directed edges that have sizes |V |, |E | respectively. The intuition behind edge (v, v ) is that node v can influence v . This intuition stems from how we create the edges in our network: (v, v ) is an edge if during a specified time period there is at least one microblog posted by v that is reposted by v (we leave other thresholds beyond 1 repost to future work). We shall also assume a partition over nodes that specifies a community structure. We shall assume that such a partition is static (based on the same time period from which the edges were derived) and that the partition C consists of k communities: {C1 , C2 , ..., Ck }. There are many possible methods to derive the communities (if user-reported communities are not available). We utilize the Louvain algorithm to identify our communities in this paper due to its ability to scale. Cascades. For a given microblog , we denote the subset of first-m nodes who originally posted or reposted  as Vm and refer to them as adopters (at size m). Likewise, the set of reposting relationships within the same time period m . Taken together, we have a cascade: will be denoted R m m m D = (V , R ). Any valid original microblog  could be treated as a unique identifier for a cascade. Given a microblog , v is the originator at instance t0  , which is defined as the origin time when the originator posted the th repost microblog  and time t is time since t0  . The m m of the microblog  happens at time t . As m increases, a cascade accumulates nodes and edges over time. We shall use N to denote the final size of a cascade while the size of a cascade at any particular instance is the set of nodes present at that instance is simply |Vm |. For a given size m, we shall refer to the frontiers as the outgoing neighbors of the adopters in graph G who are not adopters themselves. Form mally: F = {v  V /Vm s.t. vi  Vm where (vi , v )  E }. For nodes in G that are outside the adopters, we shall use the notation texp (v, , m) to denote the number of time units from the initial post of  before the microblog was reposted by one of v 's incoming neighbors - intuitively the time at which v was exposed to . For a given natural number  (used to specify a time period), we define the  frontiers as a subset of the frontiers that have been exposed to 

no earlier than  time units previously. Formally this set m, m is defined as follows: F = {v  F |texp (v, , m)  }. Finally, the complement of this set are the  non-adopters: ¯ m, = {v  F m |texp (v, , m) > }. F   Sina Weibo Dataset. The dataset we used was provided by WISE 2012 Challenge1 . It included a sample of microblogs posted on Sina Weibo from 2009 to 2012. In this dataset, we are provided with time and user information for each post and subsequent repost which enabled us to derive a corpus of cascades. From this data, we derived our social network G = (V, E ) (with 17.9 M vertices and 52.4 M edges) that was created from reposts that were published during the 3 month period between May 1, 2011 and July 31, 2011. For this network, the average clustering coefficient is 0.107. There are 4974 connected components in the network. Louvain algorithm outputs 379,416 communities with average size of 47.5 for this network. As expected, this network exhibits a power-law degree distrubtion. For this network, the number of active nodes in August (the time period we studied for cascade prediction) is 5,910,608, while 5,664,625 of them at least have one out-neighbor. During the month of August, there were 9,323,294 reposts with 2,252,368 different original microblogs. 1,920,763 (86.6%) of them were written by authors who at least published one microblog during May 1, 2011 to July 31, 2011 (the time period we used to create the underlying network). The average time it took for viral cascades to become viral is approximately 18 hours. The distribution of final size of cascades mimics a power-law distribution which can demonstrate that this dataset is more representative of cascade behavior observed "in the wild". This differs significantly from the previous works which conduct biased sampling to artificially provide balanced classes. We selected  as 30 minutes as 90% of all reposts in the initial 3 month period occurred in under this time. Number of communities. For V  V , the associated communities C (V ) are the communities represented by V . Formally: C (V ) = {Ci  C s.t. V  Ci = }. The cardinality of this set (number of communities) will be denoted K (V ). We measure the number of communities represented by the above three m, ¯ m, ) observed populations of nodes: K (Vm ), K (F ), K (F  at either a given cascade size. Gini impurity. For V  V , the gini impurity, IG (V ) is the probability of a node in V being placed into the incorrect community if assigned a community based on the distribution of communities represented in V . Formally: IG (V ) = |Ci V | 1 - i ( |V | )2 . We study the gini impurity of the adopters,  non-adopters and  frontiers for either a given cascade size m, ¯ m, ). The intuition is to capture m: IG (Vm ), IG (F ), IG (F  a notion of how the communities are distributed amongst the nodes in each of these sets with a single scalar value. We note that the impurity of the adopter set IG (Vm ) behaves similar to the entropy of this set (a measurement introduced in [3]). However, as we will see in the next two sections, we found that the impurity of the  frontiers is a more discriminating feature. Overlap. For Va , Vb  V , the overlap (O(Va , Vb )) is simply the number of shared communities. Formally: O(Va , Vb ) =
1 http://www.wise2012.cs.ucy.ac.cy/challenge.html

|C (Va )  C (Vb )|. We study overlap between adopters and  frontiers, between adopters and  non-adopters, and between m, ¯ m, ),  frontiers and  non-adopters: O(Vm , F ), O(Vm , F  m, ¯ m, and O(F , F ) respectively. The intuition with overlap stems directly from the original structural diversity results of [?] - for instance a high overlap between adopters and  frontiers may indicate that the  frontiers are linked to adopters with inner-community connections and high structural diversity - hence increasing the probability of adoption. Average time to adoption. The average time to adoption for the nodes in the current set of adopters (once the cascade grows m i =1 t to size m): im . We also use average time to adoption as a baseline measure. III. R ESULTS

Here we examine the behavior of the various structural diversity measurements as viral and non-viral cascades progress. We define a cascade as viral if the number of reposts reaches a threshold (denoted T H ) of 500 (in the next section we will explore other settings for T H when describing our classification results). We look at snapshots of the cascades as they progress both in terms of size (denoted m). For m = {10, 30, 50, 100, 200}, the number of samples is {98832, 26733, 13285, 4722, 1324} respectively with 208 of the samples are viral. With each size m we consider the m Cascades with m adopters at some time tm  , t can vary for different . Hence, cascades with final size N < m are ignored in our analysis task. This leads to a decrease in the number of non-viral Cascades as m increases. Average time to adoption. As a baseline measurement, we study the average time to adoption for each size-based stage of the cascade process (Fig. 1i, Fig. 1j). As expected, viral cascades exhibit a faster rate of reposting. While we note that significant differences are present - especially in the early stages of the cascade, the whiskers of the non-viral class indicate a significant proportion of non-viral cascades that exhibit rapid adoption. We believe this is likely due to the fact that certain cascades may have very high appeal to specialized communities. Number of communities. Fig. 1a, Fig. 1b, Fig. 1c and Fig. 1d display how the number of communities K (V ) increases over m, m = {10, 30, 50, 100, 200} for the sets V = Vm , F . m We note that K (V ) (the communities represented in the set of adopters) was shown to be a useful feature in [3] for tasks where the target class had fewer reposts than in this study. Here, we note that while statistically significant differences exist, the average and median values at each of the examined stages are generally similar. On the other hand, the m, communities represented by the set of  frontiers (K (F )) shows viral Cascades have stronger capability than non-viral ones to keep a diverse set of  frontiers. We also noted that the ¯ m, ) (not pictured) shows viral cascades start median of K (F  m, with smaller K (F ). However, it increases faster in viral cascades as nodes in  frontiers becomes  non-adopters. Gini impurity. Cascades in both classes tend to accumulate diversity in the process of collecting more adopters - and we have also noted that a related entropy measure (studied in [3]) performed similarly. We also noted (not pictured) that in the

TABLE I: Features: Cascade Prediction over Time and Size
Group Feature(s) over size
m, ¯ m, ),IG (V m ),IG (F m, ),IG (F ¯ m, ), K (F ),K (F    

Am

m, m, ¯ m, m m ¯ m, O (V , F ),O (V , F ),O (F , F ), m, ¯ m, |, |F |, |F  m ti i=1  m

, m  {30, 50}

10

Bm Cm

Community Features Mentioned in [3] and Cm
m ti i=1  m

Number of Adopters

30

50

100

200

Number of Communities

70 60 50 40 30 20 10 0

M: 8.0 A: 7.7

18.0 17.5

25.0 24.0

35.0 34.9

48.0 47.6
70 60 50 40 30 20 10 0

Number of Communities

M: 8.0 A: 8.1

17.0 17.3

23.0 23.5

34.0 34.0

46.5 46.5

10

, m = 50

Number of Adopters

30

50

100

200

Number of Communities(102 )

Overlap. We found that overlap grows with the number of adopters in the three types of overlap considered. For m ), viral cascades start with a larger initial value O(Vm , F and keep leading non-viral ones in the diffusion process of first 200 nodes (Fig. 1e, Fig. 1f). This may hint that viral cascades also take advantage of the densely linked communities to help ¯ m ) and them become viral. However, in the case of O(Vm , F  m, ¯ m, O(F , F ), viral cascades begin with lower value but grow much faster than non-viral Cascades. Classification Experiments. Here we examine our experiments for predicting whether a cascade becomes viral - when a size threshold (T H ) exceeds 500 adopters given that the cascade has 50 adopters (s = 50). Based on the distribution of final size of cascades in this dataset, this is a binary classification task with two heavily imbalanced classes. Hence, we report performance measurements (precision, recall and F1 score) for only the minority (viral) class. Throughout the course of our experiments, we found that varying threshold (slightly modifying the definition of "viral") for only the training set allows for a trade-off between precision and recall. We study the trend of performance measures in two cases: (1.) The threshold for test set is maintained as T Hts = 500 while the training threshold is varied T Htr = {300, 400, 500, 600, 700}. (2.) The two thresholds are kept as the same T H while we modify this value T H = {300, 400, 500, 600, 700}. Table I shows the groups of features used in our prediction tasks. The features introduced in this paper is group Am . As a baseline method for size-based prediction (feature group Cm ) we used average time to adoption. We also compare our features (Group Am ) with the community features extracted in [3] (Group Bm ). This was the best performing feature set in that paper for a comparable task.2 Additionally, we study the average size of recalled and non-recalled viral cascades by classifiers using features in groups Am . We also investigate the significance and performance of individual and certain combinations of features introduced in this paper. We used ten-fold cross-validation in our experiments to
2 This was their highest-performing set of features for predicting cascades that grew from 50 to 367 and 100 to 417 reposts. We also included the baseline feature in this set as we found it improved the effectiveness of this approach.

10

Number of Adopters

30

50

100

200

Number of Communities(102 )

early stages, viral cascades can show more diversity in  fronm, tiers measured by IG (F ) (m = {10, 30, 50}). But, perhaps most striking, that non-viral Cascades gain more uniformly distributed nodes over communities in  non-adopters, shown ¯ m, ) (Fig. 1g, Fig. 1h). We believe that this is due to by IG (F  non-viral cascades likely have an appeal limited to a relatively small number of communities - hence those not adopting the trend may represent a more diverse set of communities.

(a) Number of communities amongst m )) for non-viral casadopters (K (V cades
M: 7.0 A: 25.7 15.0 39.6 20.0 53.2 27.0 33.0 88.5 111.1

(b) Number of communities amongst m )) for viral cascades adopters (K (V
M: 21.0 A: 24.3 30.0 41.7 30.0 44.4 33.5 78.7 42.5 88.6

1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0

1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0

10

Number of Adopters

30

50

100

200

(c) Number of communities amongst m,  frontiers (K (F )) for non-viral cascades
M: 3.0 A: 3.7 9.0 8.7 12.0 12.3 19.0 18.5 26.0 25.2

(d) Number of communities amongst m,  frontiers (K (F )) for viral cascades
M: 7.0 A: 6.7 13.0 12.7 17.0 16.5 22.5 22.2 31.0 29.5

60 50 40 30 20 10 0

60 50 40 30 20 10 0

Overlap

10

Number of Adopters

30

50

100

200

Overlap

10

Number of Adopters

30

50

100

200

(e) Overlap of adopters and  fronm , F m, )) for non-viral tiers (O(V  cascades
M: 0.8 A: 0.8 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9

(f) Overlap of adopters and  fronm , F m, )) for viral castiers (O(V  cades
M: 0.0 A: 0.4 0.9 0.8 0.9 0.9 0.9 0.9 0.9 0.9

1.0 0.8

1.0 0.8

Gini

0.4 0.2 0.0 10

Gini
30 50 100 200

0.6

0.6

0.4 0.2

Number of Adopters

0.0

10

Number of Adopters

30

50

100

200

(g) Gini impurity of  non-adopters ¯ m, )) for non-viral cascades (IG (F 
M: 865.9 A: 780.3 853.1 804.5 754.6 765.2 790.1 771.0 753.8 759.9

(h) Gini impurity of  non-adopters ¯ m, )) for viral cascades (IG (F 
M: 15.3 A: 40.9 49.7 78.4 168.1 301.1 86.9 129.4 215.8 347.7

1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0

10

Number of Adopters

30

50

100

200

1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0

Average Time(103 )

Average Time(103 )

10

Number of Adopters

30

50

100

200

(i) Non-viral cascades

(j) Viral cascades

Fig. 1: Number of communities, gini impurity, overlap and average time since t0  to adoption for m = {10, 30, 50, 100, 200}

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Precision

Am Bm

Cm

1.0 0.8 0.6 0.4

precision

recall

f1 score

Recall

F1 Score

0.2

300

Fig. 2: Classification results based on groups of features (Am ,Bm ,Cm ) extracted when m = 50 for fixed T Htr = 500, T Hts = 500. Error bars represent one standard deviation. ensure the results do not take any advantage of randomness in picking training and testing sets. First we carried out the prediction tasks with fixed thresholds T Htr = 500, T Hts = 500. Then we modify the training threshold T Htr = {300, 400, 500, 600, 700} to show how this achieves a trade-off between precision and recall. The difference in average final size between correctly classified viral cascades and incorrectly classified ones is also monitored over T Htr = {300, 400, 500, 600, 700} to show the potential to predict exact number of adopters by features. Furthermore, we modify threshold of both training and testing sets T H = {300, 400, 500, 600, 700} to show the robustness of our features on related classification problems. We used the oversampling method SMOTE with random forest classifier to generate synthetic samples for the viral class. Other, lesserperforming classifiers were also examined (including SVM, MLP, and other ensemble methods) and are not reported here. All results shown in this section is a sample mean produced by ten repeated experiments under each combination of variables. Size-based prediction. We studied cascades of size 50 that reached 500 for this task. There are 13,285 cascades that can reach the size m = 50 while 208 out of them reached the size of 500. Maintaining the threshold T H = 500, Fig. 2 shows random forest classifier trained with features in group Am can outperform the other groups. The trade-off between precision and recall can be achieved by changing the training threshold T Htr while maintaining the testing threshold (Fig. 3a). We also note that the average final size of viral cascades recalled by the classifier increases with the training threshold (Fig. 3b). With threshold T H = {300, 400, 500, 600, 700} on both training and testing samples, the features of group Am consistently outperform those previously introduced (Bm ) (Fig. 3c, Fig. 3d). Feature investigation. Here we investigate the importance of each feature in Am . With T Htr = 500 and T Hts = 500, we trained 100 randomized logistic regressions models - each assigning weights to the features in those sets. We then categorized the features with weight larger than 0.01 (on average) into groups such as overlap, gini impurity, etc. Then, we performed classification on the basis of single feature categories or combination of such categories. The average weights assigned are shown in Table II. As shown by these results, overlaps can make significant contribution to cascade prediction. Intuitively, communication between two sets of nodes is more likely to happen in their shared communities which is consistent with the results of [?]. This implies that the larger overlap value, the more influence of one set on the other. For example, we can infer that viral cascades tend to have

400 500 600 Training Threshold

700

1.4 recalled not recalled 1.3 1.2 1.1 1.0 0.9 0.8 0.7 0.6 300 400 500 600 Training Threshold

mean

Size (103 )

700

(a) Results for features in Am with (b) Average final size of viral cascades different T Htr . (recalled, mean and not recalled)
1.0 0.8 0.6 0.4 0.2 0.0 300 400 500 600 700 Training/Testing Threshold
precision recall f1 score

1.0 0.8 0.6 0.4 0.2

precision

recall

f1 score

0.0 300

400 500 600 700 Training/Testing Threshold

(c) Results for features in group Am (d) Results for features in group Bm when T Htr and T Hts change. when T Htr and T Hts change.

Fig. 3: Prediction results for Am when T Htr and T Hts change. Error bars represent one standard deviation.

Name Gini

Features
50, IG (F ) ¯ 50, ) IG (F  ¯ 30, ) IG (F  50 ti i=1  50

Weights 0.02 0.02 0.52 1.00

Name

Features
30, 30 O (V , F ) 30 ¯ 30, O (V , F )

Weights 0.50 0.04 0.23 0.50 0.26

Overlap

Baseline

  30, ¯ 30, O (F , F ) 50, 50 O (V , F ) 50, ¯ 50, O (F , F )

TABLE II: Weights of features assigned by randomized logistic regression models
m, larger O(Vm , F ) value for adopters have larger chance to influence the  frontiers than non-viral cascades. Moverover, the gini impurity of  non-adopters also shows its importance. Intuitively, non-viral cascades are easier to be trapped in a relatively small amount of communities. This means even if they could show up in people's timeline with high structural diversity but can not get them infected.

IV.

ACKNOWLEDGMENT

This work is supported through the AFOSR Young Investigator Program (YIP), grant number FA9550-15-1-0159. R EFERENCES
J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec, "Can cascades be predicted?" in Proceedings of the 23rd international conference on World wide web. International World Wide Web Conferences Steering Committee, 2014, pp. 925­936. [2] P. Shakarian, A. Bhatnagar, A. Aleali, R. Guo, and E. Shaabani, Diffusion in Social Networks. Springer (in press), 2015. [Online]. Available: http://lab.engineering.asu.edu/cysis/diffusion/ [3] L. Weng, F. Menczer, and Y.-Y. Ahn, "Predicting successful memes using network and community structure," in Eighth International AAAI Conference on Weblogs and Social Media, 2014. [1]

MIST: Missing Person Intelligence Synthesis Toolkit
Elham Shaabani, Hamidreza Alvari, and
Paulo Shakarianâˆ—
Arizona State University
Tempe, AZ 85281

J.E. Kelly Snyder
Find Me Group
Chandler, AZ 85249

kelly@findmegroup.org

{shaabani, halvari, shak}@asu.edu

ABSTRACT
Each day, approximately 500 missing persons cases occur
that go unsolved/unresolved in the United States. The nonprofit organization known as the Find Me Group (FMG),
led by former law enforcement professionals, is dedicated to
solving or resolving these cases. This paper introduces the
Missing Person Intelligence Synthesis Toolkit (MIST) which
leverages a data-driven variant of geospatial abductive inference. This system takes search locations provided by a
group of experts and rank-orders them based on the probability assigned to areas based on the prior performance of
the experts taken as a group. We evaluate our approach
compared to the current practices employed by the Find Me
Group and found it significantly reduces the search area leading to a reduction of 31 square miles over 24 cases we
examined in our experiments. Currently, we are using MIST
to aid the Find Me Group in an active missing person case.

Keywords
Geospatial abduction; abductive inference; law enforcement;
missing person

1.

INTRODUCTION

Each day, approximately 500 missing persons cases occur
that go unsolved/unresolved in the United States. The nonprofit organization known as the Find Me Group (FMG),
led by former law enforcement professionals, is dedicated
to solving or resolving these cases. This non-profit operates with limited resources - so it must use its volunteer assets in a highly efficient manner. This paper introduces the
Missing Person Intelligence Synthesis Toolkit (MIST) which
leverages a data-driven variant of geospatial abductive inference [24]. This system takes search locations provided by
a group of experts and rank-orders them based on the probability assigned to areas based on the prior performance of
âˆ—
U.S. Provisional Patent 62/345,193. Contact shak@asu.edu for
licensing information.

the experts taken as a group. We evaluate our approach
compared to the current practices employed by the FMG
and found it significantly reduces the search area. In 24
cases examined in our experiments (on real-world data provided by FMG), we found our approach to be able to reduce
total search area by a total of 31 square miles for standard
searches and by 19 square miles when dog team assets obtain
a detection. This reduction is significant for the following
reasons:
â€¢ Reduction in time to locate missing persons.
In cases where baseline provided 20 square miles or
more (the most difficult cases), we achieved reduction
in search area of 7 to 56 square miles. As 3-5 square
miles are searched on a typical day (terrain dependent), such a reduction can potentially increase the
chance of a missing person being found alive.
â€¢ Reduction in direct costs. During a search, FMG
spends approximately $2200 per day. In all tests, our
approach reduced the search area in the majority of
cases which can be interpreted as a reduction in direct
costs.
â€¢ Reduction in indirect costs. FMG relies extensively on volunteers to augment searches. During searches,
these individuals often lose earnings from their day job
or small business. As many volunteers also perform
consulting or other services to law enforcement, longer
searches lead to loss of revenue and opportunity. In
one case, a volunteer estimated a loss of $15K. Again,
our approach leads to a consistent reduction in search
area - hence reducing these indirect costs.
Specifically, we contribute an extension to geospatial abduction [24] that leverages historical data of individual experts. We also create new algorithms to learn parameters
of a geospatial abduction model from data based on integer
programming. We then evaluate these algorithms on realworld data provided by the FMG under a variety of different
settings. This approach learns pattern of each reporter independently and is able to overcome outliers if any. It also
does well on the limited data. This work has prepared us
in our ongoing deployment of the software. At the time of
this writing, we have provided results of MIST to support
an active case with FMG. Figure 1 shows an example output of MIST where it rank-orders search locations. FMG is
currently using this information to support their operations.
They found the result consistent with their experiences.

Figure 1: Mapping of ordered grids by MIST (green squares)
and current searched area by FMG (red square).
The rest of the paper is organized as follows. In Section 2,
we present the background of the missing person problem.
Next, we provide the technical preliminaries. We discuss
our data-driven extension in Section 4. In Section 5, we
detail our algorithmic approach. We introduce our dataset
and conduct data analysis in Section 6. Next, we discuss
the experimental results in Section 7. We review the related
work in Section 8. We conclude the paper presenting future
research directions.

2.

BACKGROUND

Missing persons cases have been on the rise in the USA
for the past twenty years. Currently, approximately 4000
people go missing each and every day. Approximately 3500
of those cases are solved or resolved (i.e., cases solved by
only providing accurate information to the authorities and
without physical involvement), which leaves an astounding
number of victims that are never located. In the case of
missing adults 13 years of age and older, the police are not
required or obligated to conduct an investigation or search
unless there are extenuating circumstances such as suicide, a
potential for violence, medical reasons, etc. This leaves families and friends without professional assistance in locating
their loved ones. The Find Me Group (FMG) was founded
by retired U.S. Drug Enforcement Agency (DEA) Special
Agent J.E. â€œKellyâ€ Snyder in 2002. The group consists of
current and retired law enforcement officers with a widerange of investigative expertise, including but not limited
to linguistics, handwriting analysis, body language, missing person/homicide experience and search-and-rescue field
management skills. The FMG has trained experts/sources
that provide detailed location information where missing individuals can be found. Many of these experts have the
ability to provide GPS coordinates to locate missing persons
with a varying levels of success. The FMG focus/goal is to
provide accurate location information in a timely manner
and minimize the potential of finding the victim deceased.
Thirty canine handlers certified in tracking, scent and cadaver complements the FMG and has led to many instances
where the person in questions was located.
Equally disturbing nationwide is the rise in human trafficking, which aligns within the missing person category.
This type of crime has long-term and devastating results.
The work of this paper is also the first step toward an allencompassing methodology of identifying locations of missing persons who were victims of human trafficking. Another

important related crime is homicide. Many missing persons
and human trafficking victims are found deceased due to this
crime. This work represents initial progress in aiding toward
crimes of this nature as well.
In this paper, we formulate the problem of â€œfinding missing personâ€ with respect to information provided by FMGâ€™s
experts, formally as a variant of the geospatial abduction
problem (GAP) [22]. To account for the key nuances of
â€œfinding missing personâ€ problem though, we extended the
GAP framework to better suite this domain. In particular,
we extend the GAP formalism with a data driven model accounting for the previous performance of experts aiding in
the missing person cases. We list the unique characteristics
of our framework here. Later in the next section, we provide
our technical approach to each.
1. Explanation Size. One key difference â€œfinding missing personâ€ problem has from other GAP instances, is
that the explanation (the result of a GAP inference algorithm) only consists of a single related location (i.e.,
the location of the missing person) corresponding to
the phenomenon under study. This differs from returning a set of k locations in the previously-introduced
GAP formalisms. Consequently, here, an explanation
will consist of a single point, which in turn led us to
explore a non-deterministic version of the original explanation.
2. Distance Constraints. In the original GAP formalism, each observed geospatial phenomenon is related
to unobserved â€œpartnerâ€ points through a distance constraint - (Î±,Î²) where Î± is the minimum distance between an observation and partner and Î² is the maximum distance. As described, this pair of constraints
was the same for all observations. However, in the
missing persons problem, each observation corresponds
to a different domain expert - and hence has a different (Î±,Î²) constraint pair. Further, we study how this
is best learned from data, as well as â€œsoftenâ€ the constraint - assigning a probability of the partner point
being less than Î±, between distances Î± and Î², and
greater than distance Î² from an observation.
3. Uncertainty. As we learn the (Î±,Î²) distance constraints for each observation and associate corresponding probabilities from historical data, it makes sense
that the inference step is treated probabilistically which differs from the original deterministic GAP framework. Further, this enables us to rank the potential
partner locations (again, as an explanation consists of
one point, ranking search locations is more useful in a
practical sense).
4. Independent Observations. In the original GAP
framework, independence amongst the observations was
not an assumption in the framework. However, FMG
compartmentalizes the information from their law enforcement experts from one another in a manner to
obtain independent reporting. Hence, we make this
assumption in this paper and it is supported by our
experimental results.
FMG currently uses a simple heuristic to rank-order potential search locations for a missing person (we describe this

later in Section 5). Once ranked, FMG leverages a variety of assets. Figure 2 depicts a recently searched area for
a case. It represents a screen shot of the tracks from the
GPS units that the dogs wear as well as the handheld units
that the searchers wear. This shows several dog tracks and
the human tracks. The green, dark blue, magenta represent
three dogs, the grey and red represent two human searchers.
The teal track is a trailing dog, ascertaining a direction of
travel. The straight lines tend to be humans and the rapidly
changing direction lines are dogs as they grid around the humans. Figure 3 shows real-world examples of how the FMG
practices in an undisclosed location.

in [23] and later extended in [24, 21, 20, 19]. More formally, each GAP consists of three major elements [22]: (1)
observations: a set of observations that explain the locations associated with the event under study (e.g., in this
application, the locations reported by the domain experts),
(2) distance constraints: a pair (Î±, Î²) âˆˆ R corresponding to
lower and upper bounds on the distances between observation and partner location and, (3) feasibility predicate: this
allows to specify whether an area on the map is a potential
location for a partner.
Next, we present the notations and definitions used throughout the paper, and review the geospatial abduction framework of [22]. In the next section, we describe specialized
extensions that were necessary to study our problem. First,
without loss of generality, we assume throughout the paper
that a map (resp. space) is represented by a discrete two
dimensional grid of size M Ã— N , defined as follows:
Definition 3.1. (Space). Given natural numbers M ,
N , the space S is the set [1, . . . , M ] Ã— [1, . . . , N ].

Figure 2: Screen shot of the tracks from the GPS units.

3.

TECHNICAL PRELIMINARIES

In this section, we briefly explain geospatial abductive inference [24], and introduce our new (introduced in this paper) data-driven probabilistic extension. We show how this
extension was used to address the unique characteristics of
the missing person location problem.
In general, abduction or abductive inference [12] refers to
a type of logic or reasoning to derive plausible explanations
for a given set of facts [13]. Abduction has been extensively
studied in medicine [13, 14], fault diagnosis [3], belief revision [11], database updates [8, 4] and AI planning [5]. Two
major existing theories of abduction include logic-based abduction [6] and set-covering abduction [2]. Though none of
the above papers takes into account spatial inference, [25]
presents a logical formalism dealing with objectsâ€™ spatial occupancy, while [18] describes the construction of a qualitative spatial reasoning system based on sensor data from a
mobile robot.
Geospatial abduction problem (GAP) [22], on the other
hand, refers to the problem of identifying unobserved partner locations (i.e., the location of a missing person) that
best explain a set of observed phenomenon with known geographic locations. Geospatial abduction was first introduced

(a)

(b)

Figure 3: (a) Picture of the search area taken from the plane.
(b) Search team.

Associated with the space is a distance function d : S Ã—
S â†’ R+ that satisfies the normal distance axioms: d(pi , pi ) =
0, d(pi , pj ) = d(pj , pi ), and d(pi , pj ) â‰¤ d(pi , pq ) + d(pq , pj ).
Note that we use o to represent the observer (source of information) and po to represent the location he/she reported
(which differs slightly from the original framework). From
these observations (reports), the corresponding unobserved
phenomenon is the actual location of the missing person.
In the original framework, the explanation consisted of geographic locations that were located at least distance Î± and
no more than distance Î² away from each observation. In
this work, we generalize this notion by providing Î±,Î² pair
for each observer - denoted Î±o ,Î²o .
Definition 3.2. (Feasibility Function). A feasibility
function feas is defined as feas : S â†’ {True, False}.
A key use for the feasibility function here is for an initial
reduction of the search space by the FMG. This is due to the
fact that missing person reports often span a large area and
an initial reduction is necessary for practical reasons. An
obvious future direction would be to utilize a probabilistic
variant of the feasibility function - which would assign a prior
probability to a location for a missing person. However, in
this application, it is unclear where such a distribution would
come from. Further, as the search space is relatively large
when compared to FMG resources, the deterministic version
of this definition is more appropriate for operational reasons.
Due to resource constraints and the generally large areas
over which reports are spread, FMG typically only searches
areas for which there is a report. As we shall describe in
Section 5, they search a 1 Ã— 1 mile square surrounding a
location reported by an observer. As such is the case, we
shall assume the following feasibility function throughout
this paper:
(
True
if p âˆˆ O
feas(p) =
(1)
False otherwise
Unless otherwise noted, we shall assume the above function is used for feasibility and hence the subset of the space
considered will be the points in O.
We now come to the important definition of an explanation. Intuitively, for a given set of points {p1 , . . . , p|O| }

reported by observers in O, an explanation is a set of points
E such that every point in this set is feasible and for every
observation, there is a point in E that is at least Î± units
away from the observation, but no more than Î² units from
the observation.
Definition 3.3. ((Î±,Î²) Explanation). Suppose O is
the set of observations, E is a finite set of points in S, and
0 â‰¤ Î±, Î² â‰¤ 1 are two real numbers. E is said to be an (Î±, Î²)
explanation of O iff:
â€¢ p âˆˆ E implies that feas(p) = True, i.e., all points in E
are feasible.
â€¢ (âˆ€o âˆˆ O)(âˆƒp âˆˆ E) Î± â‰¤ d(p, o) â‰¤ Î², i.e., every observation is neither too close nor too far from some point
in E.
Thus, an (Î±,Î²) explanation is a set of points. Each point
must be feasible and every observation must have an analogous point in the explanation which is neither too close nor
too far.
Again, we note that here an explanation will consist of a
single point - the location of the missing person. Hence, this
deterministic definition of an explanation will not suffice as in practice there will often not exist an explanation for
a given problem instance. As such is the case, we extended
this framework using a data-driven approach.

4.

DATA-DRIVEN EXTENSIONS

In this section, we describe our data-driven probabilistic
extension to the original GAP formalism. The framework
extensions in this section were not previously introduced and
are new in this paper. In order to do so, we first introduce
some preliminary notation. For point p âˆˆ S, the random
variable Pp denotes that the missing person was found at
point p, so this is either true or false. We will use Pp as
shorthand for Pp = True. For observer o âˆˆ O the random
variable Oo can be assigned to one of the points in p. Based
on this notation, we define an explanation distribution.
Definition 4.1 (Explanation Distribution). Given
a set of observers O and a set of reported locations by each
observer p1 , . . . , po , . . . , p|O| , an explanation distribution
is a probability distribution over all points in S - directly addressing characteristic 3 of this application (see Section 2).
This distribution assigns the probability of a missing person
being located at each point conditioned on the observers reporting V
their respective locations. Formally, it is written as
P r(Pp | oâˆˆO Oo = po ).
The key intuition is that if we are able to compute an explanation distribution, we can then rank-order points in the
space by probability - and hence conserve search resources.
Note that the explanation distribution is over all points implying that there is precisely one location. While generalizations that allow for more than one location are possible
in such a probabilistic framework, we keep the size at one
due to the first characteristic of our problem (as described
in Section 2).
In this paper, we make an assumption of distance primacy
meaning
the distance constraints (Î±o , Î²o ) relate the Pp with
V
oâˆˆO Oo = po . Hence, we introduce another random vari0
o
able, RÎ²p,p
0 which is true if d(p, p ) â‰¤ Î²o and false otherwise.

Note that in the remainder of this section, we will use one
distance constraint (Î²) for sake of brevity - though this idea
can be extended for multiple distance constraints (as per
characteristic 2 from Section 2). In fact, we leverage multiple distance constraints in our optimization procedure for
parameter selection introduced later. Hence, by distance
primacy, we have the following relationships.
^
^ Î²
P r(Pp |
Oo = po ) = P r(Pp |
Rp,po )
(2)
oâˆˆO

oâˆˆO

By Bayesâ€™ Theorem, this is equivalent to the following.
V
P r(Pp ) Ã— P r( oâˆˆO RÎ²p,po |Pp )
(3)
V
P r( oâˆˆO RÎ²p,po )
However, by characteristic 4, we assume that the observers
report information independently, which gives us the following.
Q
P r(Pp ) Ã— oâˆˆO P r(RÎ²p,po |Pp )
(4)
V
P r( oâˆˆO RÎ²p,po )
Due to our application, we will not consider the prior probability P r(Pp ) as each missing person case occurs in a different geographic location - and due to the wide range of
cases that span multiple countries, data supporting a realistic, informed prior is highly sparse. As such, we consider a
uninformed prior. Further, for notational simplicity, we shall
use the notation ÏÎ²o for the quantity P r(RÎ²p,po = True|Pp =
True). Therefore, we can rank points in the space based
on the explanation distribution by simply considering their
log-likelihood computed as follows:
X
X
log(ÏÎ²o ) +
log(1 âˆ’ ÏÎ²o )
(5)
oâˆˆO
d(p,po )â‰¤Î²

oâˆˆO
d(p,po )>Î²

Hence, the inference step for this problem is straightforward provided we know the values Î² and ÏÎ²o for each
observer o âˆˆ O (or similar parameters if considering more
than one distance constraint). If we know the value Î² we
can then compute ÏÎ²o based on a corpus of historical data
concerning the accuracy of reporter o. Given a corpus of
previous cases for the observer Co where the found location
was pc and the location reported by the observer was pco , we
can compute ÏÎ²o as follows:
ÏÎ²o =

|{c âˆˆ Co s.t. d(pc , pco ) â‰¤ Î²}|
|Co |

(6)

Hence, we also adjust ÏÎ²o to account for volume of the reporterâ€™s history to provide the effect of regularization. Considering Î·o as the portion of total number of cases in which
observer o has participated, to the total number of cases,
and  as a non-negative parameter, we define ÏÎ²,
as follows:
o
ÏÎ²,
= ÏÎ²o âˆ’  Ã— (1 âˆ’ Î·o )
o

(7)

The situation is further complicated with multiple distance constraints. We propose an optimization approach to
this problem in the next section.

5.

ALGORITHMIC APPROACH

In this section, we present our algorithmic approach to
special case of geospatial abductive inference. First, we ex-

plain the method that FMG currently uses. Then, we provide our proposed optimization approach to solve the problem.

also minimize the following quantity:
XX X
X h
0
F2 =
Î´Î² (p, pco ) Ã— log ÏoÎ² Ã— Xo,Î²
câˆˆC oâˆˆO pâˆˆ{S\pc } Î²âˆˆ[Î²o ]

5.1

Existing Method

0

The FMG uses the following method to explore the missing person location. Given the reported locations provided
by different observers, FMG initially creates a search area
(grid) as follows. First, they draw building blocks (or boxes)
of size 1Ã—1 mile centered at each reported location (note that
depending on the situation, these boxes may overlap). Then,
they search the entire grid in the following order. First, they
search the larger areas created of the overlapping boxes, and
if the missing person was not found, they explore the remaining boxes in the order of the observersâ€™ history (how
well they did in the past). The whole process is repeated
by extending the size of boxes to 2Ã—2 miles, if the missing
person was not located. Note that, we use the same grid in
our proposed methods.

5.2

Proposed Methods

As described, for simplicity, we first elaborate on the required steps to calculate the best Î²o for each observer. Then,
we extend the idea for multiple distance constraints. Let
[Î²o ] be the set of possible error radii. Note that for Co cases
where observer reported a location, there are at most |Co |
possible values for Î²o . Hence, our goal is to select as a set
of these distance constraints - one for each observer. We do
this through an integer program - where for each observer
o âˆˆ O and each associated distance constraint Î²o âˆˆ [Î²o ]
we have an indicator variable Xo,Î²o that is 1 if we use that
value and zero otherwise. We shall refer to this as the single
constraint integer program. Hence, we find an assignment of
values to these indicator variables in order to maximize the
following quantity:
XX X h
F1 =
Î´Î² (pc , pco ) Ã— log ÏÎ²o Ã— Xo,Î² +

+ (1 âˆ’ Î´Î² (p, pco )) Ã— log(1 âˆ’ ÏoÎ² ) Ã— Xo,Î²

(1 âˆ’ Î´Î² (p

, pco ))

Ã— log(1 âˆ’

ÏÎ²o )

Ã— Xo,Î²

i

(8)

subject to the following constraints:
âˆ€Xo,Î² âˆˆ {0, 1}
âˆ€o,

X

(13)

Therefore, the objective function we seek to optimize is
L1 = max(F1 âˆ’ F2 )

(14)

Theorem 5.1. Number of variables in single-distance constraint integer program is O(avg(|Co |) Â· |O|).
We extend the previous formulation by allowing the objective function to find a pair of distance constraints for each
reporter. We have experimentally found diminishing returns
on performance (and in many cases increased complexity)
with more than two constraints. This will give us the double
distance constraint integer program as follows:
XX X X h
F10 =
Î´Î± (pc , pco ) Ã— log ÏÎ±,
Ã— Xo,Î±,Î²
o
câˆˆC oâˆˆO Î±âˆˆ[Î²o ]Î²âˆˆ[Î²o ]
Î²â‰¥Î±





Î±,
Ã— Xo,Î±,Î² +
+ 1 âˆ’ Î´Î± (pc , pco ) Ã— Î´Î² (pc , pco ) Ã— log ÏÎ²,
o âˆ’ Ïo


i
(1 âˆ’ Î´Î² (pc , pco )) Ã— log 1 âˆ’ ÏÎ²,
Ã— Xo,Î±,Î²
o
subject to the following constraints:

âˆ€o,

âˆ€Xo,Î±,Î² âˆˆ {0, 1}
X
Xo,Î±,Î² â‰¤ 1
Î±,Î²âˆˆ[Î²o ]

Likewise, we use the following objective function, to avoid
bias toward selecting the largest Î²â€™s.
L2 = max(F10 âˆ’ F20 )
where

câˆˆC oâˆˆO Î²âˆˆ[Î²o ]
c

i

F20 =

F20

is defined as follows:
X

XX X X h

(15)

0

Î´Î± (p, pco ) Ã— log ÏoÎ±, Ã— Xo,Î±,Î² +

c
câˆˆC oâˆˆO Î±âˆˆ[Î²o ]Î²âˆˆ[Î²o ] pâˆˆ{S\p }
Î²â‰¥Î±



1 âˆ’ Î´Î± (p, pco ) Ã— Î´Î² (p, pco )Ã—
(9)

Xo,Î² â‰¤ 1

(10)

Xo,Î² = k

(11)

0

0

log(ÏoÎ², âˆ’ ÏoÎ±, ) Ã— Xo,Î±,Î² +




i
0
1 âˆ’ Î´Î² (p, pco ) Ã— log 1 âˆ’ ÏoÎ², Ã— Xo,Î±,Î²

(16)

Î²âˆˆ[Î²o ]

X X
o

Î²âˆˆ[Î²o ]

where k is a cardinality that limits the number of reporters
(which is set to a natural number in the range 1, . . . , |O|),
and Î´Î² (x, y) is defined as:
(
1 if d(x, y) â‰¤ Î²
Î´Î² (x, y) =
(12)
0 if d(x, y) > Î²
However, this equation will result in tendency toward selecting the largest distance constraints. This has the effect of not only maximizing the probability of the locations
where the missing person was found, but also can increase
the probability of other locations. Intuitively, we want to

Theorem 5.2. Number of variables in double distance constraint integer program is O(avg(|Co |)2 Â· |O|).
While we obtained a significant reduction in the area searched
by setting the cardinality constraint k = O, we found that
varying it would often lead to further improvement. We
gradually increased the number of observers from one to the
total number of observers and each time, we learned the
distance constraints for the last added observers. In this
method of optimization, we may choose a specific number
of points in each iteration. The number of points added
with each iteration can be determined based on available
resources.
We also defined two heuristic to discriminate points with
the same probability. In each iteration, we chose the point
with highest probability. If there were more than one point,

Table 1: Description of the dataset

we applied following heuristics: (1) we chose the points
which had most of the reported locations in its 1 Ã— 1 mile.
(2) we chose the point which had the maximum summation
of the priors of the reporters in its 1 Ã— 1 mile.
Algorithm 1 is a specific variant of restricted model. In
this algorithm, in each iteration one point (i.e., representative of a 1 Ã— 1 mile) is selected. Though we note that this
can easily be adjusted in practice. If the area size we are
able to search is larger than number of observers, we sort
the representatives based on their probabilities. Then, we
apply two heuristics to rank them (similar to Lines 11-19 ).

Name
Found Status
Alive
Deceased
Gender
Male
Female
Age
Under 13
13 to 30
30 and older

Algorithm 1 Iterative Search Resource Allocation

6.1

30
25
20
15
10

Overview

6.2

Data Analysis

The dataset consists of cases distributed all over the world.
We split the U.S. based cases into 4 regions, west, midwest,
northeast and south, according to the United States Census
Bureau. We further grouped together all cities outside the
U.S. into one single category, namely, international. The
distribution of cases across different regions is demonstrated
in Figure 4. Though we did not explicitly show in the figure,
the west is dominated by Arizona and California, due to the
large focus of FMG on these two states.
There are several known reasons of disappearance associated with the cases in our dataset including, accidental,

West

South

Midwest

International

Northeast

5

MISSING PERSON DATASET

Our dataset includes cases (i.e., missing persons), found
status (alive/deceased), found location (latitude and longitude), age and reason for disappearance as well as the potential locations (latitude and longitude) associated with the
reporters/experts. The description of this dataset is summarized in Table 1. Note that in some cases, we are aware of
reports, but do not have the found location (pco ). In this
work, we only have 29 cases with the known found locations
used for the experiments. However, for the data analysis,
the entire dataset is applied.

9
39
40

35

Theorem 5.3. The time complexity of the algorithm (1)
is O(|C|Â·avg(|Co |)2 Â· avg(|Oc |)3 ).

In this section, we describe our dataset and briefly discuss
the observation made from our initial data analysis.

41
47

40

0

6.

12
76

bipolar, drowning, foul play, natural, runaway, self-inflicted,
staged and undetermined. According to Figure 5, â€˜foul playâ€™
is the dominant reason for disappearance. There are also different number of reporters for each case. The distribution of
reporters with respect to the number of cases in which they
participated is shown in Figure 6.

Number of cases

1: procedure Opt-Point-By-Point(A, c, S, Ï) . Train
set A, Test case c
2:
List R = âˆ…
. Output
3:
for k âˆˆ [1, |Oc |] do
. k is a constant value of the
constraint
4:
Find assignment of variables that optimize (15)
w.r.t. (9 - 11)
5:
RP â† Order by (5)
. Ranked points RP
6:
RP â† RP \ R
7:
Pick P âŠ† RP with largest probabilities
8:
if P includes one point then
9:
R=RâˆªP
10:
else
11:
p â† Heuristic(P )
12:
R = R âˆª {p}
13:
return R

Value

Regions

Figure 4: Distribution of the cases across different regions
of the US and international.
For the rest of our data analysis, we need to introduce
some preliminary notation. We use the random variable gA
to denote if the missing person is found alive or not, so it is
either true or false. We shall use P r(gA = True|o stated Alive)
to denote the confidence of the observer o in reporting Alive.
This confidence value shows the portion of the cases for
which o has reported the missing person is Alive and he/she
was found Alive, to the total number of cases for which o has
reported Alive. Likewise, we compute the confidence of o in
reporting Deceased. The distribution of the reporters with
respect to their confidence values is demonstrated in Figure 7. According to the figure, most reportersâ€™ confidence
values belong to the ranges of [0.3,0.4) for alive and [0.8,0.9)
for deceased statuses.
We also define the ratio rA as follows:
rA =

P r(gA = True|observer o stated Alive)
P r(gA = True)

(17)

This ratio demonstrates how much the observer o outperformed the prior probability P r(gA = True) on Alive. Similarly, we use rD for Deceased cases. The distributions of the
reporters with respect to rA and rD are shown in Figure 8.
We note that as most are found dead, it is harder for the

16	
Number	of	Reporters	

18	

30
25
20
15
10

Foul play

Undetermined

Accidental

Self-inflicted

Runaway

Natural

Bipolar

0

Staged

5
Drowning

Number of cases

35

14	
12	
10	
8	
6	
4	
2	
0	
0	

1	

2	

3	

4	
rA

5	

6	

7	

8	

180	

Reasons

160	
Number	of	Reporters	

Figure 5: Distribution of the cases with respect to the probable reasons.
45	
Number	of	Reporters	

40	
35	
30	

140	
120	
100	
80	
60	
40	

25	

20	

20	

0	

15	

0.5	

0.7	

10	

0.9	
rD

1.1	

1.3	

5	
0	
1	

2	

3	

4	

5	
6	
7	 10	 13	
Frequency	of	Par=cipa=on	

15	

16	

Figure 8: The distributions of the reporters with respect to
rA and rD .

17	

[0.9,	1]	

[0.8,	0.9)	

[0.7,	0.8)	

[0.6,	0.7)	

[0.5,	0.6)	

[0.4,	0.5)	

[0.3,	0.4)	

[0.2,	0.3)	

alive	
deceased	

[0.1,	0.2)	

18	
16	
14	
12	
10	
8	
6	
4	
2	
0	

[0,	0.1)	

Number	of	Reporters	

Figure 6: Distribution of frequency of participation.

Conï¬dence	

Figure 7: Distribution of all reporters with respect to their
confidence values.

reporters to outperform the prior on Deceased compared to
the Alive.

7.

EXPERIMENTAL RESULTS

This section reports on the experiments conducted to validate our approach. We note that the individual cases themselves are not related - hence we are justified in using leaveone-out cross validation in our experiments. Specifically, for
each case in the experiments, we learn a different model
using all of the other cases. We first compare the methods for restricted (without dog) and unrestricted (with dog)
searches and then discuss the sensitivity of the parameter.

7.1

Area Reduction

In this section, we examine how our approach can be used
to reduce the area searched by the Find Me Group over the
baseline. Figure 9 shows the reduction of area based on our
approach (double distance constraint integer program with
Algorithm 1 and  = 0.1) when compared to the baseline.
We examine this with grid squares of 1Ã—1 miles and 2Ã—2
miles. In the 19 cases where the missing person was located,
our approach achieved area reduction in 11 cases - reducing
the search area by an average by 3 square miles. In the 2
cases where our method caused the search area to increase,
the increase was only 1 square mile in each case. This contrasts with the cases where the area was reduced - reducing
the search area by up to 9 square miles. For the 11 cases
where reduction was experienced, the average reduction was
1.63 miles (t(19) = 1.25, p <0.11).
We also examined cases where the size of the grid squares
was 2Ã—2 miles. In the 19 cases, the area reduction achieved
by our method was in 14 cases, and by an average by 8.5
square miles. Further, in the 6 cases, our method caused
an increase in the search area, however, the increase was 3
square miles on average. Further, for the cases that baseline needs to search areas larger than 20 square miles, our
approach reduced the area from 7 to 56. Our method outperformed the baseline in area reduction with an average of
4.21 mile square (t(20) = 1.19, p <0.13).

7.2

Consideration of Dog Team Detections

The experiments of the previous section illustrated how
our approach could reduce the search area over the baseline
for standard grid settings. However, in the events that a dog

18

18
Baseline
Algorithm 1

14

14

12

12

10
8
6

8
6
4

2

2
0

2

4

6

8

10

12 14
Cases

16

18

20

0

22

(a) Search area with 1 Ã— 1 mile per observation

2

4

6

8

10

12 14
Cases

16

18

20

22

60
Baseline
Algorithm 1

50

Baseline
Algorithm 1

50

Searched Area

40
30
20
10
0

0

(a) Search area with 1 Ã— 1 mile per observation

60

Searched Area

10

4

0

Baseline
Algorithm 1

16

Searched Area

Searched Area

16

40
30
20
10

0

2

4

6

8

10

12 14
Cases

16

18

20

0

22

(b) Search area with 2 Ã— 2 miles per observation

0

2

4

6

8

10

12 14
Cases

16

18

20

22

(b) Search area with 2 Ã— 2 miles per observation

Figure 9: Searched area until the missing person is located
(baseline and Algorithm 1).

Figure 10: Searched area with dogs allowed to explore 1 mile
beyond the grid (baseline and Algorithm 1).

team detects evidence of the missing person, it may lead to a
continued search outside of the assigned grid square. These
searches can lead to FMG personnel examining up to a mile
outside a designated location. In this section, we consider a
grid square settings in the last section, but also allow for an
additional mile outside the square to mimic the effect of the
dog search team following such a lead. Figure 10 demonstrates the reduction of area based on our approach (double
distance constraint integer program with Algorithm 1 and
 = 0.1) when compared to the baseline. We investigate the
area reduction with grid squares of 1Ã—1 miles and 2Ã—2 miles.
According to Figure 10a, in the 22 cases where the missing
person was located, our approach achieved area reduction in
12 cases - reducing the search area by 2 square miles on average. In the 2 cases where our method caused the search area
to increase, the increase was only 3 square miles on average.
This contrasts with the cases where the area was reduced
- reducing the search area by up to 9 square miles. Our
method outperformed the baseline in area reduction with
an average of 0.86 mile square (t(22) = 0.8, p <0.22).
We examined cases where the size of the grid squares was
2Ã—2 miles. In the 24 cases, the area reduction achieved by
our method was in 21 cases, and on average by 8.85 square
miles. In the 3 cases where our method caused the search
area to increase, the increase was 4.3 square miles on aver-

age. This contrasts with the cases with the reduced search
area by up to 56 square miles. Our method outperformed
the baseline in area reduction with an average of 7.2 mile
square (t(24) = 1.95, p <0.05).

7.3

Parameter Sensitivity

We compare different values of  in both double distance
constraint integer programs (iterative search resource allocation and non-iterative program). The impact of changing
the parameter  is shown in Figure 11. To do so, we plot the
fraction of area searched by our method over the baseline,
against the , for both sizes of 1 Ã— 1 and 2 Ã— 2. We note that
while the extreme values of  (i.e. 0.0 and 0.5) negatively
effected the performance of both approaches, we achieved
relatively stable results for intermediate values - noting that
the best performance was to set  equal to 0.1 - which we
used in the experiments.
We also studied the performance of our optimization approach without algorithm 1 (i.e. prioritize locations by equation 5 after selecting the values for Î²o through optimization
of 19 with regards to Lines 9-11). The results are depicted
in Figure 12. The behavior of the algorithm for different
settings of  were similar to that found with Algorithm 1,
the reduction in search area was generally less - and in some

1.4

1.6

Without Dog
With Dog
Fraction of Searched Area

Fraction of Searched Area

1.6

1.2
1.0
0.8
0.6
0.4
0.2
0.0

1.4

Without Dog
With Dog

1.2
1.0
0.8
0.6
0.4
0.2

0.1

0.2

0.3

0.4

0.5

0.0

0.1

0.2

Â²
(a) Search area with 1 Ã— 1 mile per observation (Algorithm 1)

1.4

Without Dog
With Dog

1.6

1.2
1.0
0.8
0.6
0.4
0.2
0.0

0.1

0.2

0.4

0.5

(a) Search area with 1Ã—1 mile per observation not using Algorithm
1

Fraction of Searched Area

Fraction of Searched Area

1.6

0.3

Â²

0.3

0.4

0.5

Â²

1.4

Without Dog
With Dog

1.2
1.0
0.8
0.6
0.4
0.2
0.0

0.1

0.2

0.3

0.4

0.5

Â²
(b) Search area with 2 Ã— 2 miles per observation (Algorithm 1)

Figure 11: Fraction of total area searched across all cases
with the iterative search resource allocation approach over
the baseline.

(b) Search area with 2 Ã— 2 miles per observation not using Algorithm 1

Figure 12: Fraction of total area searched across all cases
by the double distance constraint integer programming approach (not using Algorithm 1) over the baseline.

cases (i.e. 1x1 mile grid square with use of the dogs) it
performed worse.

8.

RELATED WORK

Recently, there has been some work [19, 20, 21, 23, 9] dealing with geospatial abductive inference introduced in [24].
In [19] for example, authors studied the case of geospatial
abduction where there is an explicit adversary who is interested in ensuring that the agent does not detect the partner
locations in an attempt to simulating the real-world scenario of insurgents who conduct IED (improvised explosive
device) attacks. Another work [20], has adopted geospatial
abduction to develop a software tool which applies geospatial
abduction to the environment of Afghanistan, to look for insurgent high-value targets, supporting insurgent operations.
The work of [21] introduced a variant of the GAPs called
region-based GAPs (RGAPs) which deals with the multiple
possible definitions of the subregions of the map. Finally,
spatial cultural abductive reasoning engine which solves spatial abductive problems was developed in [23]. Aside from
introducing GAP, the work of [24] demonstrated the accuracy of proposed framework on real-world dataset of insurgent IED attacks against US forces in Iraq. Further, the
work of [9], proposed a technique to reduce the computational cost of point-based GAPs. They presented an exact
algorithm for the natural optimization problem of pointbased GAPs. Geospatial abduction problems are related
to facility location [26] and sensor placement problems [10]
in that they identify a set of geo-locations to optimize a

cost or reward function. However, there are key differences
amongst these various frameworks that arise from the difference between explanation and optimization. See [22] for
further discussion on this topic.
Similarly, [1] presents a specific aspect of the well-known
qualification problem, namely spatial qualitative reasoning
approach, which aims at investigating the possibility of an
agent being present at a specific location at a certain time
to carry out an action or participate in an event, given
its known antecedents. This work is different from both
above papers and our study, as it takes on purely logical approach to formalizing spatial qualifications, while our work
and other aforementioned studies use geometric and probabilistic techniques. Further, the framework of this paper is
tailored specifically for the missing person problem.
Looking beyond geospatial abduction, recent research has
demonstrated that GPS (positional) data could be used to
learn rich models of human activity [16, 15, 17, 7]. For example, [16, 15, 17], modeled the human interactions and intentions in a fully relational multi-agent setting. They used
raw GPS data from a real-world game of capture the flag and
Markov logic- a statistical-relational language. Whereas [7]
developed a model to simulate the behaviors associated with
insurgent attacks, and their relationship with geographic locations and temporal windows.
At first glance, one may think our work is similar to [10],
in that they identify a set of geo-locations to optimize a

cost or reward function. However, as described, there are
key differences amongst these various frameworks that arise
from the difference between explanation and optimization.
[11]

9.

CONCLUSION

In this paper, we have introduced the Missing Person Intelligence Synthesis Toolkit (MIST) which leverages a datadriven variant of geospatial abductive inference. MIST can
rank-order the set of search locations provided by a group
of experts. The experimental results showed that our approach is able to reduce the total search area by a total of 31
square miles for standard searched and by 19 square miles
when dog team assets obtain a detection. This reduction
will make FMG locating missing persons faster while saving in direct and indirect cost. At the time of this writing,
we have initiated support to FMG with MIST for an active
case. FMG will use MISTâ€™s ranking of search locations for
this ongoing operation.
Our future plans include utilizing a probabilistic variant
of the feasibility function, applying other features such as
missing personâ€™s region, age, gender to the model and extending our toolkit to be able to solve other problems such
as human trafficking.

Acknowledgement

[12]

[13]

[14]

[15]

[16]

[17]

This work was funded by the Find Me Group.

10.

REFERENCES

[1] B. Akinkunmi and P. C. Bassey. A Logic of Spatial
Qualification Using Qualitative Reasoning Approach.
International Journal of Artificial Intelligence &
Applications, 4(2):45, 2013.
[2] T. Bylander, D. Allemang, M. C. Tanner, and J. R.
Josephson. The Computational Complexity of
Abduction. Artif. Intell., 49(1-3):25â€“60, 1991.
[3] L. Console, L. Portinale, and D. T. DupreÌ. Focussing
Abductive Diagnosis. AI Commun., 4(2/3):88â€“97,
1991.
[4] L. Console, M. L. Sapino, and D. T. DupreÌ. The Role
of Abduction in Database View Updating. J. Intell.
Inf. Syst., 4(3):261â€“280, 1995.
[5] S. do Lago Pereira and L. N. de Barros. Planning with
abduction: A logical framework to explore extensions
to classical planning. In Brazilian Symposium on
Artificial Intelligence, pages 62â€“72. Springer, 2004.
[6] T. Eiter and G. Gottlob. The complexity of
logic-based abduction. Journal of the ACM, 42:3â€“42,
1995.
[7] S. George, X. Wang, J. Lin, B. Qu, and J.-C. Liu.
MECH: Algorithms and Tools for Automated
Assessment of Potential Attack Locations. Technical
report, Texas A & M University, College Station, 2015.
[8] A. C. Kakas and P. Mancarella. Database updates
through abduction. In VLDB, volume 90, pages
650â€“661, 1990.
[9] A. Koutsioumpas. Abductive reasoning in 2d
geospatial problems. In Applications of Mathematics
and Informatics in Science and Engineering, pages
333â€“347. Springer, 2014.
[10] A. Krause, J. Leskovec, C. Guestrin, J. Vanbriesen,
and C. Faloutsos. Efficient sensor placement

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]
[26]

optimization for securing large water distribution
networks. Journal of Water Resources Planning and
Management, 2008.
M. Pagnucco. The Role of Abductive Reasoning within
the Process of Belief Revision. PhD thesis, Basser
Department of Computer Science, University of
Sydney, 1996.
C. S. Peirce. Philosophical writings of Peirce, selected
and edited with an introd. by Justus Buchler. Dover
Publications New York, 1955.
Y. Peng and J. Reggia. Abductive inference models for
diagnostic problem-solving. Symbolic computation.
Springer-Verlag, New York, 1990.
Y. Peng and J. A. Reggia. Plausibility of Diagnostic
Hypotheses: The Nature of Simplicity. In Proceedings
of the 5th National Conference on Artificial
Intelligence. Philadelphia, PA, August 11-15, 1986.
Volume 1: Science., pages 140â€“147, 1986.
A. Sadilek and H. Kautz. Modeling Success, Failure,
and Intent of Multi-Agent Activities Under Severe
Noise.
A. Sadilek and H. Kautz. Location-based Reasoning
About Complex Multi-agent Behavior. J. Artif. Int.
Res., 43(1):87â€“133, Jan. 2012.
A. Sadilek and H. A. Kautz. Recognizing multi-agent
activities from gps data. In AAAI, volume 39, page
109, 2010.
P. Santos and M. Shanahan. Hypothesising object
relations from image transitions. In ECAI, pages
292â€“296, 2002.
P. Shakarian, J. P. Dickerson, and V. Subrahmanian.
Adversarial geospatial abduction problems. ACM
Transactions on Intelligent Systems and Technology
(TIST), 3(2):34, 2012.
P. Shakarian, M. K. Nagel, B. E. Schuetzle, and
V. Subrahmanian. Abductive inference for combat:
using scare-s2 to find high-value targets in
afghanistan. Technical report, DTIC Document, 2011.
P. Shakarian and V. Subrahmanian. Region-based
Geospatial Abduction with Counter-IED Applications.
In U. K. Wiil, editor, Counterterrorism and Open
Source Intelligence. Springer, 2010.
P. Shakarian and V. Subrahmanian. Geospatial
Abduction: Principles and Practice. SpringerLink :
BuÌˆcher. Springer New York, 2011.
P. Shakarian, V. Subrahmanian, and M. L. Spaino.
SCARE: A Case Study with Baghdad. In Proceedings
of the Third International Conference on
Computational Cultural Dynamics. AAAI, 2009.
P. Shakarian, V. Subrahmanian, and M. L. Spaino.
GAPs: Geospatial Abduction Problems. ACM
Transactions on Intelligent Systems and Technology,
2010.
M. Shanahan. Noise and the Common Sense
Informatic Situation for a Mobile Robot.
J. F. Stollsteimer. A working model for plant numbers
and locations. Journal of Farm Economics,
45(3):631â€“645, 1963.

A Quantitative Approach to Belief Revision in Structured Probabilistic Argumentation
Gerardo I. Simari · Paulo Shakarian · Marcelo A. Falappa

Received: July 30, 2015/ Accepted: date

Abstract Many real-world knowledge-based systems must deal with information coming from different sources that invariably leads to incompleteness, overspecification, or inherently uncertain content. The presence of these varying levels of uncertainty doesn't mean that the information is worthless ­ rather, these are hurdles that the knowledge engineer must learn to work with. In this paper, we continue work on an argumentation-based framework that extends the well-known Defeasible Logic Programming (DeLP) language with probabilistic uncertainty, giving rise to the Defeasible Logic Programming with Presumptions and Probabilistic Environments (DeLP3E) model. Our prior work focused on the problem of belief revision in DeLP3E, where we proposed a non-prioritized class of revision operators called AFO (Annotation Function-based Operators) to solve this problem. In this paper, we further study this class and argue that in some cases it may be desirable to define revision operators that take quantitative aspects into account, such as how the probabilities of certain literals or formulas of interest change after the revision takes place. To the best of our knowledge, this problem has not been addressed in the argumentation literature to date. We propose the QAFO (Quantitative Annotation Function-based Operators) class of operators, a subclass of AFO, and then go on to study the complexity of several problems related to their specification and application in revising knowledge bases. Finally, we present an algorithm for computing the probability that a literal is warranted in a DeLP3E knowledge base, and discuss how it could be applied towards implementing QAFO-style operators that compute approximations rather than exact operations.
G.I. Simari, M.A. Falappa Dept. of Comp. Sci. and Eng., Universidad Nacional del Sur and CONICET, Argentina E-mail: {gis,mfalappa}@cs.uns.edu.ar P. Shakarian Arizona State University, Tempe, AZ, USA E-mail: shak@asu.edu

2

G.I. Simari, P. Shakarian, and M.A. Falappa

Keywords Structured Argumentation · Belief Revision · Reasoning under Probabilistic Uncertainty

1 Introduction and Motivation Many real-world knowledge-based systems must deal with information coming from different sources that invariably leads to uncertain content, be it from gaps in knowledge (incompleteness), overspecification (inconsistency), or because the knowledge is inherently uncertain (such as weather forecasts or measurements that are necessarily imprecise). Far from considering such uncertain knowledge useless, knowledge engineers face the challenge of putting it to its best possible use when solving a wide range of problems. In particular, one basic problem that needs to be investigated in depth is that of revising such knowledge bases in a principled manner. In this paper, we continue work on that combines the well-known Defeasible Logic Programming (DeLP) language (actually, an extension called PreDeLP to handle presumptions) with probabilistic uncertainty, which led to the development of the DeLP3E model (Defeasible Logic Programming with Presumptions and Probabilistic Environments). In previous work [39, 41], we studied a class of non-prioritized belief revision operators called AFO that allows changes to be made only to probabilistic annotations when addressing the incorporation of an epistemic input into the knowledge base. Here, we propose a subclass called QAFO that takes quantitative aspects into account when performing revisions, such as how the probabilities of certain literals or formulas of interest change after the revision takes place. To the best of our knowledge, this problem has not been addressed in the argumentation literature to date. The main contributions presented in this paper are briefly summarized as follows: ­ As building blocks to be used in our quantitative belief revision operators, we define: (i) warrant probability functions (WPFs), which have as domains either conjunctions or disjunctions of ground literals that are mapped to the probability that they are warranted in the DeLP3E program; and (ii) revision objective functions (ROFs), which take as input two DeLP3E programs I1 , I2 and an epistemic input and return a numeric score that is interpreted as the value of obtaining I2 from I1 when revising it by the epistemic input. ROFs generally include WPFs in their definitions. ­ We propose the QAFO class of operators, a subclass of AFO that allows modifications to the annotation function to be carried out as part of the belief revision operation, but focusing on optimizing a given ROF, as described above. ­ We study the complexity of several problems related to our approach; in particular, we present:

Quantitative Belief Revision in Structured Probabilistic Argumentation

3

(i) A new lower bound on the complexity of deciding the warrant status of a literal or a conjunction/disjunction of literals in a (classical) PreDeLP program; (ii) Computing WPFs in general is #P-hard; (iii) Point (ii) holds even when computing probabilities of worlds in the environmental model can be done in polynomial time; (iv) Computing WPFs in the special case in which Nilsson's probabilistic logic [33] is used is NP-complete; (v) By combining the intuition behind the proof of point (iv) and further conditions, we identify a class of instances for which WPFs can be computed in polynomial time; and (vi) Under the same conditions as point (v), we show that QAFO revisions are NP-complete; furthermore, we show that the problem has the same complexity even when the revision objective function is constrained to be a simple sum of warranting probabilities for atoms in the AM. ­ We present an algorithm for computing the probability that a literal is warranted in a DeLP3E knowledge base, and discuss its application towards implementing QAFO-style operators that compute approximations rather than perform exact operations.

The rest of this paper is organized as follows: Section 2 discusses preliminary concepts and the DeLP3E framework, which was first introduced in [39, 41]. Section 3 motivates the specialization of AFO operators to take into account quantitative aspects, and presents the class QAFO. Section 4 studies the complexity of several problems related to QAFO and the application of such operators in revising DeLP3E knowledge bases. Section 5 studies a novel approach to reasoning about probabilities of literals in DeLP3E, called warranting formulas, and their application in the implementation of QAFO-style operators that address the high computational cost issuess by applying heuristics the trade off optimality for tractability. Finally, Sections 6 and 7 present related work and conclusions, respectively. Throughout the entire paper, we illustrate the presentation via a running example that is inspired on the use of DeLP3E in medical diagnosis, which is the kind of real-world application in which we envision our work being of most use; we have also explored its application in the related scenario of solving the attribution problem1 in cyber security and cyber warfare [40], with encouraging feedback from the community as to its potential impact.
1 Essentially, given a cyber event of interest, the attribution problem involves finding out who was responsible for it. This is especially well suited for argumentation and belief revision due to the ease with which a potential culprit can plant false or misleading clues, hence giving rise to an inconsistent knowledgebase. See [38] for further discussion.

4

G.I. Simari, P. Shakarian, and M.A. Falappa

2 Preliminaries on the DeLP3E Framework The DeLP3E framework is comprised of two distinct, but interrelated models of the world. The first is called the environmental model (referred to from now on as the "EM"), and is used to describe uncertain knowledge about the domain that is subject to probabilistic events. The second one is called the analytical model (referred to as the "AM"), and contains knowledge that is either strict or defeasible, as described below ­ this will be useful in the analysis of competing hypotheses that can account for a given phenomenon (what we will generally call queries). The AM is composed of a classical (that is, non-probabilistic) PreDeLP [32] program in order to allow for contradictory information, giving the system the capability to model competing explanations for a given query. In general, the EM contains knowledge such as evidence, uncertain facts, or knowledge about agents and systems. The AM, on the other hand, contains knowledge that may or may not be strictly valid, yet it does not depend on probabilistic events. Indeed, dividing knowledge between the AM and EM is a knowledge engineering task, and its adequate resolution will call for design decisions to be made; note that this separation also allows for a different kind of uncertainty to be modeled ­ defeasible rules and presumptions can be leveraged when there is no probabilistic information available but we still wish to maintain that a specific portion of the knowledge base is uncertain. In the rest of this section, we formally describe these two models, as well as how knowledge in the AM can be annotated with information from the EM ­ these annotations specify the conditions under which the various statements in the AM can potentially be true. Basic Language. We assume sets of variables and constants, denoted with V and C, respectively. The language also contains a set of n-ary predicate symbols; the EM and AM use separate sets of predicate symbols, denoted with PEM and PAM , respectively ­ the two models can, however, share variables and constants. As usual, a term is composed of either a variable or a constant. Given terms t1 , ..., tn and n-ary predicate symbol p, p(t1 , ..., tn ) is called an atom; if t1 , ..., tn are constants, then the atom is said to be ground. The sets of all ground atoms for EM and AM are denoted with GEM and GAM , respectively; finally, we also use notation LAM to denote the set of all (ground) literals: {a | a  GAM }  {¬a | a  GAM }. Note that even though in general the language allows variables, for simplicity we will assume throughout this paper that all objects are ground (propositional). Example 1 Consider a medical diagnosis scenario2 in which a patient goes to a hospital exhibiting certain symptoms: shortness of breath, sporadic fainting (loss of consciousness for brief periods of time), and some signs of memory
2 This and all related examples in this paper, though inspired by potential real-world applications, make many simplifying assumptions in order to allow for a concise presentation of the key concepts that we wish to illustrate.

Quantitative Belief Revision in Structured Probabilistic Argumentation PEM : anx risk dep risk FN-anx test The patient falls into the category of people at risk of suffering anxiety. The patient falls into the category of people at risk of suffering depression. Event associated with a false negative coming up when performing a test to see whether a patient is showing anxiety-related symptoms. Event associated with a false negative coming up when performing a blood test for the presence of toxic substances. The patient shows signs of memory loss. The patient suffers shortness of breath. The patient suffers short-term loss of consciousness. The patient is diagnosed with an anxiety-related disorder. The patient is diagnosed with a depression-related disorder. Patient diagnosed with misuse of sleeping aids. Test proves absence of anxiety-related disorders. Test proves absence of toxins in blood. Test proves presence of depression-related disorders.

5

FN-tox screen

PAM :

mem loss short breath fainting anxiety depression sleep aid misuse neg anx test neg tox screen pos dep test

Fig. 1 Explanation of the meaning of the predicates used in the running example.

loss. Figure 1 shows the predicates that we will use throughout the paper in the running example. As shown in the figure (and discussed in more detail below), some of these predicates comprise the analytical model (AM), while others are part of the environmental model (EM). For instance, in our example, predicates stating the presence of symptoms such as memory loss and shortness of breath, as well as those representing diagnoses, such as anxiety and depression, are part of the analytical model. On the other hand, the environmental model contains predicates that are associated with uncertain events, such as false negatives coming up when a test is performed or the risk that the patient will be affected by anxiety-related disorders. Note that in the running example we make use of the term "at risk" according to the common use of this expression in the medical domain, i.e., characterized by high risk or susceptibility, such as to a certain disease. Given set of ground atoms, a world is any subset of atoms ­ those that belong to the set are said to be true in the world, while those that do not are false. Therefore, there are 2|GEM | possible worlds in the EM and 2|GAM | worlds in the AM. These sets are denoted with WEM and WAM , respectively. In order to avoid worlds that do not model possible situations given a particular domain, we include integrity constraints of the form oneOf(A ), where A is a subset of ground atoms. Intuitively, such a constraint states that any world

6

G.I. Simari, P. Shakarian, and M.A. Falappa

where more than one of the atoms from set A appears is invalid. We use ICEM and ICAM to denote the sets of integrity constraints for the EM and AM, respectively, and the sets of worlds that conform to these constraints is denoted with WEM (ICEM ) and WAM (ICAM ), respectively. Finally, logical formulas arise from the combination of atoms using the traditional connectives (, , and ¬). As usual, we say that a world  satisfies formula (f ), written  |= f , iff: (i) If f is an atom, then  |= f iff f  ; (ii) if f = ¬f then  |= f iff  |= f ; (iii) if f = f  f then  |= f iff  |= f and  |= f ; and (iv) if f = f  f then  |= f iff  |= f or  |= f . We use the notation formEM , formAM to denote the set of all possible (ground) formulas in the EM and AM, respectively; finally, we use basicAM to denote all possible conjunctions or disjunctions of literals from LAM , which we refer to as basic formulas.

2.1 Environmental Model In this paper, we generalize the approach taken in our previous work [39, 41] for the environmental model ­ here, we simply assume that we have a probabilistic model defined over GEM , which represents a probability distribution over WEM . Definition 1 (Probabilistic Model) Given sets PEM , V, and C, and corresponding sets GEM and WEM , a probabilistic model EM is any function Pr : WEM  [0, 1] such that WEM Pr () = 1. Examples of probabilistic models that can be used are Bayesian networks (BNs) [34], Markov logic networks (MLNs) [37], extensions of first order logic such as Nilsson's probabilistic logic [33], or even ad-hoc representations of function Pr from Definition 1. The following is an example of a BN over the running example. Example 2 Consider the set PEM from Figure 1. The Bayesian network depicted in Figure 2 describes the probability distribution Pr over all possible worlds WEM shown in Figure 3. So, for instance, the probability that false negatives do not arise in any of the two tests, and that the patient is at risk for both anxiety- and depressionrelated disorders (world 4 ) is 0.29808.

2.2 Analytical Model The DeLP3E formalism adopts a structured argumentation framework [36] for the AM. While the EM contains probabilistic information about the state of the world, the AM must allow for a different kind of information; in particular, it must be able to represent contradictory knowledge. This approach allows

Quantitative Belief Revision in Structured Probabilistic Argumentation
anx_risk AR
Pr(AR = T) = 0.4 Pr(AR = F) = 0.6

7

FN_tox_screen FNTS
Pr(FNTS = T) = 0.1 Pr(FNTS = F) = 0.9

dep_risk DR
Pr(DR = T | AR = T) = 0.9 Pr(DR = F | AR = T) = 0.1 Pr(DR = T | AR = F) = 0.2 Pr(DR = F | AR = F) = 0.8

FN_anx_test FNAT
Pr(FNAT = T | AR = T) = 0.08 Pr(FNAT = F | AR = T) = 0.92 Pr(FNAT = T | AR = F) = 0.01 Pr(FNAT = F | AR = F) = 0.99

Fig. 2 Bayesian network used in the EM of the running example. The names of the random variables are simply the abbreviations of their corresponding atoms: AR  anx risk, DR  dep risk, FNAT  FN anx test, and FNTS  FN tox screen.

World 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

anx risk T T T T T T T T F F F F F F F F

dep risk T T T T F F F F T T T T F F F F

FN anx test T T F F T T F F T T F F T T F F

FN tox screen T F T F T F T F T F T F T F T F

Probability 0.00288 0.02592 0.03312 0.29808 0.00032 0.00288 0.00368 0.03312 0.00048 0.00432 0.04752 0.42768 0.00012 0.00108 0.01188 0.10692

Fig. 3 Probability distribution for the worlds in the running example.

for the creation of arguments that may compete with each other in order to reach a conclusion regarding a given query. This is known as a dialectical process, where arguments defeat each other based on a comparison criterion. Resulting from this process, certain arguments are warranted, while others are defeated. Argumentation-based reasoning has been proposed and studied in depth as a natural way to manage a set of inconsistent information ­ its strength lies in the fact that it closely resembles the way humans settle disputes (consider, for instance, how convictions are decided in trials). Another highly desirable characteristic of structured argumentation frameworks is that, once a conclusion is reached, the process also yields an explanation of how we arrived at it, as well as information about why a given argument is warranted. In the following, we first recall the basics of the underlying argumentation framework used, and then go on to introduce the analytical model (AM).

8

G.I. Simari, P. Shakarian, and M.A. Falappa

Defeasible Logic Programming with Presumptions (PreDeLP) PreDeLP, first introduced in [32], is a formalism combining logic programming with defeasible argumentation. We will briefly recall the basics of PreDeLP, and refer the reader to [17, 32] for the complete presentation. The formalism contains several different constructs: facts, presumptions, strict rules, and defeasible rules. Facts are statements that always hold (such as a patient's symptom in our example), while presumptions are statements that may or may not be true (such as a medical diagnosis). Strict rules establish logical consequences (similar to material implication in first order logic, though the semantics is not exactly the same since the contrapositive does not follow from a strict rule). While strict rules, like facts, always hold, defeasible rules specify logical consequences that may be assumed to be true when no contradicting information is available. These components are used in the construction of arguments, and together comprise PreDeLP programs. Formally, we use the notation AM = (, , , ) to denote a PreDeLP program, where: ­  is the set of strict rules of the form L0  L1 , . . . , Ln , where L0 is a ground literal and {Li }i>0 is a set of ground literals; ­  is the set of facts, written simply as atoms; ­  is the set of defeasible rules of the form L0 ­ L1 , . . . , Ln , where L0 is a ground literal and {Li }i>0 is a set of ground literals, and ­  is the set of presumptions, which are written as defeasible without a body. For simplicity, we will sometimes refer to AM as a set corresponding to the union of its components. Recall that all atoms in the AM must be formed with a predicate from the set PAM , and note that in both strict and defeasible rules, strong negation (i.e., classical negation as in first-order logic) is allowed in the head, and thus may be used to represent contradictory knowledge. The following is an example of a PreDeLP program over the running example. Example 3 Consider again the medical diagnosis scenario from our running example; the DeLP3E program in Figure 4 encodes some basic knowledge that the attending physician might use to diagnose their patient. For instance, strict rule 1 states that based on a negative result when administering a test for toxins in the blood we can conclude that the patient is not misusing sleeping aids. On the other hand, defeasible rule 1 states that memory loss and depression can lead to such a misuse. In this example, the set of presumptions is empty. Arguments. Given a query in the form of a ground atom, the goal is to derive arguments for and against its validity ­ derivation follows the same mechanism of logic programming [30], and we denote such derivation with the symbol " ". In the following, we say that such a derivation is "strict" if it only uses facts

Quantitative Belief Revision in Structured Probabilistic Argumentation : 1 = 2 = 3 = 1 2 3 4  1 2 3 4 = = = = sleep aid misuse ­ mem loss, depression anxiety ­ short breath depression ­ mem loss anxiety ­ fainting = = = = mem loss short breath fainting ¬sleep aid misuse  neg tox screen ¬anxiety  neg anx test anxiety  depression depression  pos dep test

9

:

: :

Fig. 4 A ground argumentation framework.

and strict rules; otherwise, we say that the derivation is "defeasible". Likewise, we say that a literal is strictly (defeasible) derived if the derivation is strict (defeasible). Finally, we say that an argument is "factual" if no presumptions are used in it. Since rule heads can contain strong negation, it is possible to defeasibly derive contradictory literals from a program. For the treatment of contradictory knowledge, PreDeLP incorporates a defeasible argumentation formalism that allows the identification of the pieces of knowledge that are in conflict and, through the dialectical process discussed above, decides which information prevails as warranted. This dialectical process involves the construction and evaluation of arguments, formally defined next. Definition 2 (Argument) An argument A, L for a literal L is a pair of the literal and a (possibly empty) set of the AM (A  AM ) that provides a minimal proof for L meeting the following requirements: (i) L is defeasibly derived from A; (ii)     A is not inconsistent; and (iii) A is a minimal subset of    satisfying (i) and (ii), denoted A, L . Literal L is called the conclusion supported by the argument, and A is the support of the argument. An argument B , L is a subargument of A, L if B  A. An argument A, L is presumptive if A   is not empty. We will also use  (A) = A   , (A) = A  , (A) = A  , and (A) = A  . Our definition differs slightly from that of [42], where DeLP is introduced, as we include strict rules and facts as part of arguments. We make this change because in our framework the strict rules and facts used to construct a given argument may only be true in certain worlds in the EM. Hence, the entire set of facts and strict rules need not be consistent in our framework. We shall discuss how portions of the AM are assigned EM worlds in the next section. Example 4 Figure 5 shows example arguments based on the PreDeLP program from Figure 4. Argument A3 uses an additional component not present in the

10 A1 , anxiety A2 , anxiety A3 , ¬sleep aid misuse

G.I. Simari, P. Shakarian, and M.A. Falappa A1 = {1 , 3 , 3 } A2 = {3 , 4 } A3 = {1 }  {neg tox screen ­ }

Fig. 5 Example arguments based on the running example scenario.

original program, and states that if we can assume a negative result for a tox screen, we can conclude that the patient is not misusing sleeping aids. Given an argument A1 , L1 , counter-arguments are arguments that contradict it. Argument A2 , L2 is said to counterargue or attack A1 , L1 at a literal L iff there exists a subargument A, L of A1 , L1 such that the set  (A1 )   (A2 )  (A1 )  (A2 )  {L2 , L } is inconsistent. A proper defeater of an argument A, L is a counter-argument that ­ by some criterion ­ is considered to be better than A, L ; if the two are incomparable according to this criterion, the counterargument is said to be a blocking defeater. An important characteristic of PreDeLP is that the argument comparison criterion is modular, and thus the most appropriate criterion for the domain that is being represented can be selected; the default criterion used in classical defeasible logic programming (from which PreDeLP is derived) is generalized specificity [44], though an extension of this criterion is required for arguments using presumptions [32]. We briefly recall this criterion next ­ the first definition is for generalized specificity, which is subsequently used in the definition of presumption-enabled specificity. Definition 3 (DeLP Argument Preference) Let AM = (, , , ) be a PreDeLP program and let F be the set of all literals that have a defeasible derivation from AM . An argument A1 , L1 is preferred to A2 , L2 , denoted with A1 P S A2 if: (1) For all H  F ,  (A1 )   (A2 )  H is consistent: if there is a derivation for L1 from  (A2 )   (A1 )  (A1 )  H , and there is no derivation for L1 from  (A1 )   (A2 )  H , then there is a derivation for L2 from  (A1 )   (A2 )  (A2 )  H ; and (2) there is at least one set H  F ,  (A1 )   (A2 )  H is consistent, such that there is a derivation for L2 from  (A1 )   (A2 )  H  (A2 ), there is no derivation for L2 from  (A1 )   (A2 )  H , and there is no derivation for L1 from  (A1 )   (A2 )  H  (A1 ). Intuitively, the principle of specificity says that, in the presence of two conflicting lines of argument about a proposition, the one that uses more of the available information is more convincing. The following extension for presumptive arguments was first introduced in [32]. Definition 4 (Presumptive Argument Preference) Given PreDeLP program AM = (, , , ), an argument A1 , L1 is preferred to A2 , L2 , denoted with A1 A2 if any of the following conditions hold: (1) A1 , L1 and A2 , L2 are both factual and A1 , L1 P S A2 , L2 .

Quantitative Belief Revision in Structured Probabilistic Argumentation

11

(2) (3)

A1 , L1 is a factual argument and A2 , L2 is a presumptive argument. A1 , L1 and A2 , L2 are presumptive arguments, and (A2 ) or,
PS

(a) (A1 )

(b) (A1 ) = (A2 ) and A1 , L1

A2 , L2 .

Generally, if A, B are arguments with rules X and Y , resp., and X  Y , then A is stronger than B . This also holds when A and B use presumptions P1 and P2 , resp., and P1  P2 . Note: The specificity criterions used here are not transitive [48], and therefore should not be assumed to define an ordering over arguments. This, however, does not pose a problem for our framework, as the criterion is only ever used to compare pairs of arguments to see which one "wins out". We remind the reader that the comparison criterion in our framework is modular; if transitivity is required, the one proposed in [48] is an option. A sequence of arguments called an argumentation line thus arises from this attack relation, where each argument defeats its predecessor. To avoid undesirable sequences, which may represent circular argumentation lines, in DeLP an argumentation line is acceptable if it satisfies certain constraints (see [17]). A literal L is warranted if there exists a non-defeated argument A supporting L. Clearly, there can be more than one defeater for a particular argument A, L . Therefore, many acceptable argumentation lines could arise from A, L , leading to a tree structure. The tree is built from the set of all argumentation lines rooted in the initial argument. In a dialectical tree, every node (except the root) represents a defeater of its parent, and leaves correspond to undefeated arguments. Each path from the root to a leaf corresponds to a different acceptable argumentation line. A dialectical tree provides a structure for considering all the possible (maximal) acceptable argumentation lines that can be generated for deciding whether an argument is defeated. We call this tree dialectical because it represents an exhaustive dialectical (in the sense of providing reasons for and against a position) analysis for the argument in its root. For a given argument A, L , we denote the corresponding dialectical tree as T ( A, L ). Given a literal L and an argument A, L , in order to decide whether or not a literal L is warranted, every node in the dialectical tree T ( A, L ) is recursively marked as "D" (defeated ) or "U" (undefeated ), obtaining a marked dialectical tree T  ( A, L ) as follows: 1. All leaves in T  ( A, L ) are marked as "U"s, and 2. Let B , Lq be an inner node of T  ( A, L ). Then B , Lq will be marked as "U" iff every child of B , Lq is marked as "D". The node B , Lq will be marked as "D" iff it has at least a child marked as "U". Given an argument A, L obtained from AM , if the root of T  ( A, L ) is marked as "U", then we will say that T  ( A, L ) warrants L and that L is warranted from AM . (Warranted arguments correspond to those in the

12

G.I. Simari, P. Shakarian, and M.A. Falappa

grounded extension of a Dung argumentation system [11].) There is a further requirement when the arguments in the dialectical tree contains presumptions ­ the conjunction of all presumptions used in even (respectively, odd) levels of the tree must be consistent. This can give rise to multiple trees for a given literal, as there can potentially be different arguments that make contradictory assumptions. We can then extend the idea of a dialectical tree to a dialectical forest. For a given literal L, a dialectical forest F (L) consists of the set of dialectical trees for all arguments for L. We shall denote a marked dialectical forest, the set of all marked dialectical trees for arguments for L, as F  (L). Hence, for a literal L, we say it is warranted if there is at least one argument for that literal in the dialectical forest F  (L) that is labeled as "U", not warranted if there is at least one argument for the literal ¬L in the dialectical forest F  (¬L) that is labeled as "U", and undecided otherwise. We shall refer to whether literal L is warranted, not warranted, or undecided as L's "warrant status", and sometimes refer to the "warranted" status as "Yes" and the "not warranted" status as "No". 2.3 The DeLP3E Framework Our framework, originally proposed in [39], is the result of combining the environmental and analytical models (which we denote with EM and AM , respectively). Intuitively, given AM , every element of        only hold in certain worlds in the set WEM ­ i.e., these elements are subject to probabilistic events. Each element of        is thus associated with a formula over GEM (using conjunction, disjunction, and negation, as usual) ­ we use formEM to denote the set of all such formulas. The notion of an annotation function associates elements of        with elements in formEM . Definition 5 (Annotation Function [39]) An annotation function is any function of the form af :         formEM . We use [af ] to denote the set of all annotation functions. We will sometimes denote annotation functions as sets of pairs (f, af(f )) in order to simplify the presentation. Figure 6 shows an example of an annotation function for our running example; for instance, the annotation for rule 2 means that this rule only holds whenever the probabilistic event anx risk is true. If annotations are "True", this means that they hold in all possible worlds. Definition 6 (DeLP3E Program) Given environmental model EM , analytical model AM , and annotation function af , a DeLP3E program is of the form I = (EM , AM , af ). We use notation [I ] to denote the set of all possible programs. In the following, given DeLP3E program I = (EM , AM , af ) and   WEM , we use notation AM () = {f  AM s.t.  |= af (f )}. This gives rise to a decomposed view of DeLP3E programs, as illustrated next.

Quantitative Belief Revision in Structured Probabilistic Argumentation af (1 ) = af (2 ) = af (3 ) = af (1 ) = af (2 ) = af (3 ) = af (4 ) = af (1 ) = af (2 ) = af (3 ) = af (4 ) = True True True True True True True True anx risk dep risk anx risk

13

Fig. 6 An example of an annotation function over the running example. 1 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 ,  1 , 2 , 3 ,  4 } 5 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 ,  1 , 2 , 4 } 9 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 ,  1 , 3 } 13 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 , 1 } 2 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 ,  2 ,  3 ,  4 } 6 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 ,  2 ,  4 } 10 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 ,  3 } 14 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 } 3 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 , 1 ,  2 ,  3 ,  4 } 7 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 , 1 ,  2 ,  4 } 11 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 , 1 ,  3 } 15 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 , 1 } 4 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 ,  2 ,  3 ,  4 } 8 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 ,  2 ,  4 } 12 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 ,  3 } 16 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 }

Fig. 7 A depiction of how the DeLP3E program in the running example can be decomposed into one classical PreDeLP program for each possible EM world (cf. Figure 3 for the definition of worlds 1 ­16 in terms of the random variables in the EM).

Example 5 Consider the different examples presented so far: the EM from Example 2 (with the worlds from Figure 3), AM from Figure 4, the arguments in Figure 5, and the annotation function from Figure 6 ­ these components give rise to a DeLP3E program I = (EM , AM , af ) Figure 7 shows how I can be decomposed into one classical PreDeLP program AM () for each world   WEM . For instance, AM (7 ) contains 1 , 2 , 3 , 1 , 2 , 3 , 4 , and 1 because the annotation function associates condition True to all of these components; it contains 2 and 4 because condition anx risk is true in 7 , and it does not contain 3 because condition dep risk is false in 7 . The most direct way of considering consequences of DeLP3E programs is thus to consider what happens in each world in WEM ; that is, the defeat relationship among arguments depends on the current state of the (EM) world. Definition 7 (Existence of an Argument in a World) Given DeLP3E program I = (EM , AM , af ), argument A, L is said to exist in world   WEM if c  A,  |= af(c).

14

G.I. Simari, P. Shakarian, and M.A. Falappa

The notion of existence is extended to argumentation lines, dialectical trees, and dialectical forests in the expected way (for instance, an argumentation line exists in  iff all arguments that comprise that line exist in ). Example 6 Consider the different examples presented so far: the worlds in Figure 3, AM from Figure 4, the arguments in Figure 5, and the annotation function from Figure 6. Since argument A1 uses defeasible rule 3 , and af (3 ) = dep risk (while the other two components have annotation "True"), we can conclude that this argument exists in worlds in which dep risk is true, i.e.,, 1 ­4 and 9 ­12 . The idea of a dialectical tree is also extended w.r.t. worlds; so, for a given world   WEM , the dialectical (resp., marked dialectical) tree induced by   is denoted with T A, L (resp., T A, L ). We require that all arguments and defeaters in these trees exist in . Likewise, we extend the notion of dialectical  forests in the same manner (denoted with F (L) and F (L), respectively). Based on these concepts, we introduce the notion of warranting scenario. Definition 8 (Warranting Scenario) Given DeLP3E program I = (EM , AM , af ) and literal L formed with a ground atom from GAM , a world   WEM is said to be a warranting scenario for L (denoted  war L) if there is a   (L) in which L is warranted and F (L) exists in . dialectical forest F The idea of a warranting scenario is used to formally define DeLP3E entailment. The set of worlds in the EM where a literal L in the AM must be true is exactly the set of warranting scenarios -- these are the "necessary" worlds: nec(L) = {  WEM | ( war L)}. Now, the set of worlds in the EM where AM literal L can be true is the following -- these are the "possible" worlds: poss(L) = {  WEM |  war ¬L}. In the following we use notation for() = a a  a /  ¬a, which denotes the formula that has  as its only model. We now extend this notation to sets of worlds: for(W ) = W for(). Entailment can then be defined as follows: Definition 9 (DeLP3E Entailment) Given DeLP3E program I = (EM , AM , af ), AM literal L and probability interval p  [ , u], we say that I entails L with probability p  [ , u] if the probability distribution Pr yielded by EM is such that  nec(L) Pr () and poss(L) Pr ()  u. 2.4 Consistency of DeLP3E Programs Finally, one of the central topics of this paper is that of inconsistencies, which can arise in our framework in more than one way [39]. In this paper, we assume that the probabilistic model is consistent, and focus on inconsistencies that arise in the AM. Towards this end, we use the following notion of (classical) consistency: PreDeLP program  is said to be consistent if there does not exist a ground literal a s.t.  a and  ¬a. For DeLP3E programs, the interaction between the annotation function and facts and strict rules may cause conflicts, as defined next.

Quantitative Belief Revision in Structured Probabilistic Argumentation

15

Definition 10 (Consistency of DeLP3E Programs) A DeLP3E program I = (EM , AM , af ), with AM = , , ,  , is consistent if: given probability distribution Pr for EM , if there exists a world   WEM such that x  | |=af(x) {x} is inconsistent, then we have Pr () = 0. The intuition behind this definition is that subsets of facts and strict rules hold under the circumstances specified by the annotation function ­ such circumstances can be expressed as sets of EM worlds. Now, if there exists a world where (at least) two contradictory strict statements are true, then the EM must assign probability zero to this world. Example 7 Let us return to the running example; consider AM from Figure 4, EM from Figure 2, and the annotation function from Figure 6, with the addition of fact 4 = pos dep test with af (4 ) = True and fact 5 = neg anx test with af (5 ) = ¬FN-anx test. It is now clear that the program is inconsistent, since there exists world 3 (among several others) such that x  | 3 |=af(x) {x} warrants both anxiety (via argument with 4 and 3 ) and ¬anxiety (via argument with 5 and 2 ).

3 Revision of DeLP3E Programs based on Quantitative Aspects We have finally arrived at the main problem we address in this paper ­ revising knowledge bases. This problem can be generically stated as: given DeLP3E program I = (EM , AM , af ), with AM =        and a pair (f, af ) where f is either an atom or a rule and af is equivalent to af , except for its expansion to include f 3 , obtain a new program I called the revised knowledge base that addresses the incorporation of the epistemic input (f, af ) into the original program; we denote this operation with the symbol "·" ­ i.e.,, I = I · (f, af ). Now, the problem statement as presented above is quite vague, since we did not give any details as to how the operator "addresses the incorporation" of the epistemic input. There are many approaches in the literature (cf. Section 6) that address this problem quite differently; one of the main properties that characterize revision operators is whether or not they satisfy the Success property, which states that the epistemic input must be a consequence of the revised knowledge base. Here, we will adopt a cautious stance and assume that this property does not hold in general; therefore, we focus on so-called non-prioritized revision operators. The basic issue that revision operators must deal with is inconsistency (we will discuss this in more depth shortly); as we saw in Section 2.4, inconsistency in DeLP3E programs involves worlds that have non-zero probability and an associated PreDeLP program that is inconsistent. In our previous work [39, 41] we identified three basic approaches that can be taken towards solving this problem:
3

That is, af (x) = af (x) for all x  dom(af ), and dom(af ) = dom(af )  {f }.

16

G.I. Simari, P. Shakarian, and M.A. Falappa

­ Modifying the EM: Perhaps the simplest approach is to fix the problem of inconsistency by forcing the probabilistic model to assign probability zero to all worlds that cause inconsistencies to arise. ­ Modifying the AM: Alternatively, for each world in which the corresponding PreDeLP program is inconsistent, we can modify this program to remove the problem. ­ Modifying the annotation function. Finally, as a finer-grained approach compared to the previous one, we can alter the annotation function. In the following, we will assume that epistemic inputs involve only strict components (facts or rules), since defeasible components can always be added without inconsistencies arising. Regarding these three possible approaches, in this paper we will focus on the third one since it is a generalization of the second ­ if we only allow removing elements from the AM, such an operation will have the same effect as not removing the element but modifying the annotation function so that it associates the formula "" to it. The generalization of annotation function-based revision lies in that there is not always the need for such a drastic measure; see [41] for further discussions on this, including examples. Furthermore, operations of the first kind alone do not suffice to perform revisions, as can be seen in the following simple example. Example 8 Consider the following DeLP3E program, where the EM consists of two worlds {a} and {¬a}, each with probability 0.5: 1 : 1 : 2 : 2 : pq ¬p ¬p  q p af (1 ) = a af (1 ) = a af (2 ) = ¬a af (2 ) = ¬a

Now, suppose we wish to revise by formula 3 : q with af (3 ) = True. Since both EM worlds are inconsistent with the formula, it is impossible to change the allocation of the probability mass in order to avoid inconsistencies; therefore, the only option is to reject the input.

3.1 A Recap of Annotation Function-based Belief Revision Since the quantitative approach that we propose in this paper is related to the AFO class of revision operators introduced in [39], in this section we provide a brief summary of the relevant material. After recalling the relevant postulates, we present the construction of AFO operators. 3.1.1 Rationality Postulates The following properties characterize the behavior of operators for revising annotation functions; we briefly discuss them here in an informal manner, and refer the reader to [41] for their formal presentation.

Quantitative Belief Revision in Structured Probabilistic Argumentation

17

­ Inclusion: For any given element g in the AM, the worlds that satisfy g 's annotation after the revision are a subset of the set of worlds satisfying g 's annotation before the revision. That is, the constraints in the revised annotations can only become more restrictive. ­ Vacuity: If simply adding the input to the program does not lead to inconsistencies, then the operator does precisely that. ­ Consistency Preservation: If the original program is consistent, then so is the revised program. ­ Weak Success: As in Vacuity, if adding the input to the program does not cause inconsistencies, then the input must be present "as is" in the revised program. ­ Relevance: Given a specific EM world, if a part of its associated AM knowledge base is removed by the operator, then there exists a superset of the remaining knowledge base that is not consistent with the removed element and the input. That is, removed elements are always "relevant" to an inconsistency. ­ AF Uniformity: If two different inputs are such that the same set of EM worlds lead to inconsistencies in a given AM knowledge base, and it is the case that analogous subsets taken in conjunction with their respective input lead to equivalent consistency/inconsistency, then the models removed from the annotations elements in the AM knowledge base are the same for both inputs. In other words, the operator must behave in the same way when presented with inputs that have an equivalent effect on the knowledge base. We now continue briefly recalling the presentation of the annotation functionbased operator of [39] by discussing its construction. In order to do so, we adopt the following notation: the program to revise is denoted with I = (EM , AM , af ), with AM =       , and the epistemic input is denoted with (f, af ), where f is either an atom or a rule and af is equivalent to af , except for its expansion to include f . We denote the revision as follows: I · (f, af ) = (EM , AM , af ) where af is the revised annotation function. We will slightly abuse notation in order to make the presentation clearer, and use notation to convert sets of worlds to and from formulas. Moreover, we often refer to "removing elements of AM " to refer to changes to the annotation function that cause certain elements of the AM to not have their annotations satisfied in certain EM worlds. As we are looking to change the annotation function for a specific subset of facts and strict rules, we specify these subsets with the following notation: ­ I  (f, af ) to denote I = (EM , AM  {f }, af ). ­ (f, af )  I = (AM , EM , af ) to denote f  AM and af = af .
0 I ­ WEM (I ) = {  WEM | AM () is inconsistent} ­ this set contains all the EM worlds for a given program where the corresponding knowledge base in the AM is classically inconsistent.

18

G.I. Simari, P. Shakarian, and M.A. Falappa

I 0 0 ­ WEM (I ) = {  WEM |Pr () > 0} ­ this is a subset of WEM (I ) containing worlds that are assigned a non-zero probability; i.e., the worlds where inconsistency in the AM arises.

­ wld(f ) = { |  |= f } ­ the set of worlds that satisfy formula f ; and ­ for() = ­
I () AM a

a

a /

¬a ­ the formula that has  as its only model.

= {f     |  |= af(f )} ­ the subset of facts and strict rules in AM whose annotations are true in EM world .

We will make use of this notation in the next section. 3.1.2 Operator Construction The basis of the construction of the class of so-called annotation functionbased operators is that any subset of AM that is associated with a world I in WEM (I  (f, af )) must be modified so that consistency is ensured. For each such world , we make use of the following set of "candidate replacement programs" for AM (): CandP gmaf (, I ) = {AM | AM  AM () s.t. AM is consistent and AM  AM () s.t. AM  AM s.t. AM is consistent}
I The intuition is that each maximal consistent subset of AM () is a candidate for replacing the inconsistent program for that world. To specify the construction, we need to introduce some more notation. Let  : WEM  2[][ ] ; i.e., a function from EM worlds to subsets of the strict components of the AM ­ this function will be used to choose the revised AM for each world. For each component h in the AM there is a formula in AM {f }, where f is part of the epistemic input (i.e., what the operator is revising by). Given these elements, we define:

newFor(h, , I , (f, af )) = af (h) 
I (I(f,af )) | h WEM /  ()

¬f or(i )

Intuitively, newFor provides a new annotation for each component h  AM ; such formula can be seen as the result of selecting a maximally consistent subset of AM () for each EM world . We are finally able to introduce the AFO class of operators: Definition 11 (AF-based Operators [39, 41]) A belief revision operator · is an "annotation function-based" (or af-based) operator (·  AFO) if given program I = (EM , AM , af ) and input (f, af ), the revision is defined as I · (f, af ) = (EM , AM  {f }, af ), where: h, af (h) = newFor(h, , I , (f, af )) where   WEM ,  ()  CandP gmaf (, I  (f, af )).

Quantitative Belief Revision in Structured Probabilistic Argumentation Analytical Model : Annotation Function

19

1 = mem loss True True 2 = short breath 3 = fainting True -----------------------------------------------{¬FN-anx test} 4 = neg anx test 5 = neg tox screen {¬FN-tox screen} 1 2 3 4  1 2 3 4 = = = = sleep aid misuse ­ mem loss, depression anxiety ­ short breath depression ­ mem loss anxiety ­ fainting True {anx risk} {dep risk} {anx risk} = = = = ¬sleep aid misuse  neg tox screen ¬anxiety  neg anx test anxiety  depression depression  pos dep test True True True True

:

: :

Fig. 8 The DeLP3E program from the running example, after adding facts 4 and 5 . The annotation function is provided in a separate column for convenience.

In [39, 41], we provide a representation theorem that states the equivalence between this construction and the operators discussed above. We refer the reader to these articles for a more complete and detailed presentation.

3.2 Towards a Quantitative Approach Traditionally, belief revision has been addressed from a qualitative point of view rather than a quantitative one (cf. Section 6 for a discussion on related work). A simple example of this is the fact that, faced with the option of removing either both atoms a and b or only atom c, classical revision operators typically declare both options to be just as good, since neither is a subset of the other; it could be argued, then, that taking quantitative aspects into account (such as the number of elements removed) may lead to a better solution ­ of course, this may depend on other factors. As we will see, there are different ways in which such quantitative aspects can be incorporated into revision operations. For instance, in our setting, DeLP3E programs can be regarded in a world-by-world manner, and changes made in one world can be compared to those made in another. The AFO operators described in Section 3.1 make decisions for each world independently; we now wish to address the issue of taking into account different kinds of quantitative aspects when revising DeLP3E programs. The following example motivates our approach in our medical diagnosis scenario. Example 9 Consider again the running example, and suppose the physician has decided to carry out as a first step two tests, a toxin screen and an anxiety test, and that both tests yielded negative results. Note that the validity of

20

G.I. Simari, P. Shakarian, and M.A. Falappa

these tests is subject to probabilistic events (in this case, false negatives). The new program is reproduced in Figure 8. Figure 9 shows the world-by-world decomposition of the new program, and the atoms that are warranted in each case. From the information in this figure, we can compute the following probabilities for the hypotheses that the physician is contemplating (depression, anxiety, and misuse of sleeping aids): Literal depression sleep aid misuse ¬sleep aid misuse anxiety ¬anxiety Probability : 0.06672 : 0.05088 : 0.93324 : 0.06992 : 0.92888

Since they all have low probabilities after performing the tests, the doctor decides to test for depression and in this case receives a positive result (atom pos dep test). For the sake of this example, we will assume that the validity of the outcome of this test (unlike the other two) is not subject to probabilistic events ­ thus, we have af (pos dep test) = True. Now, while for the first two tests we were able to simply add the corresponding atoms and extend the annotation function accordingly, simply adding 6 = pos dep test with af (6 ) = True causes inconsistencies to arise in eight of the possible worlds (3 , 4 , 7 , 8 , 11 , 12 , 15 , and 16 ). Essentially, the problems arise because the negative anxiety test allows us to conclude that there is no anxiety, while the positive depression test would allow us to conclude that indeed there is anxiety. Since both derivations only involve strict components, this leads to an inconsistent AM. Example 9 shows an interesting case of belief revision in DeLP3E programs; when presented with new information that is in conflict with existing one, we must find a way to address its incorporation into the existing knowledge ­ non-prioritized operators are very flexible, since they always have the option of ignoring the new information. However, this flexibility also means that ­ in the case of DeLP3E programs ­ there is no guidance with respect to how revisions should be carried out globally, since each world is treated as a separate revision problem. Next, we discuss two kinds of functions that will prove to be useful in addressing this situation. 3.2.1 Two Building Blocks We now introduce warrant probability functions and revision objective functions, which are later used in the definition of our new class of non-prioritized belief revision operators. Warrant Probability Functions. As one of the building blocks to our quantitative approach, given a DeLP3E program I we define warrant probability functions (WPFs, for short).

Quantitative Belief Revision in Structured Probabilistic Argumentation World 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Probability 0.00288 0.02592 0.03312 0.29808 0.00032 0.00288 0.00368 0.03312 0.00048 0.00432 0.04752 0.42768 0.00012 0.00108 0.01188 0.10692 Warranted Literals {1 , 2 , 3 }  {depression, sleep aid misuse, anxiety} {1 , 2 , 3 , 5 }  {depression, ¬sleep aid misuse, anxiety} {1 , 2 , 3 , 4 }  {depression, ¬sleep aid misuse, anxiety} {1 , 2 , 3 , 4 , 5 }  {¬sleep aid misuse, ¬anxiety} {1 , 2 , 3 }  {anxiety} {1 , 2 , 3 , 5 }  {¬sleep aid misuse, anxiety} {1 , 2 , 3 , 4 }  {¬anxiety} {1 , 2 , 3 , 4 , 5 }  {¬sleep aid misuse, ¬anxiety} {1 , 2 , 3 }  {depression, sleep aid misuse, anxiety} {1 , 2 , 3 , 5 }  {depression, ¬sleep aid misuse, anxiety} {1 , 2 , 3 , 4 }  {sleep aid misuse, ¬anxiety} {1 , 2 , 3 , 4 , 5 }  {¬sleep aid misuse, ¬anxiety} {1 , 2 , 3 } {1 , 2 , 3 , 5 }  {¬sleep aid misuse} {1 , 2 , 3 , 4 }  {¬anxiety} {1 , 2 , 3 , 4 , 5 }  {¬anxiety, ¬sleep aid misuse}

21

Fig. 9 Atoms that are warranted in each possible EM world, given the AM and annotation function in Figure 8.

Before introducing these formulas, we need to present a simple extension to the concept of "warrant status", which is up to now defined for literals. The following definition is a simple extension to conjunctions or disjunctions of literals. Definition 12 (Warranting a Conjunction/Disjunction of Literals) Let AM be a ground PreDeLP program and Q be either a conjunction or disjunction of ground literals L1 , . . . , Ln . The warrant status of Q with respect to AM is defined as follows: 1. If Q is a single literal L, then the warrant status of Q is the warrant status of L in AM . 2. If Q = Q1  Q2 then the warrant status of Q is: ­ Yes iff the warrant status of both Q1 and Q2 is Yes; ­ No if the warrant status of either Q1 or Q2 is No; and ­ Undecided whenever neither of the above cases hold. 3. If Q = Q1  Q2 then the warrant status of Q is: ­ Yes iff the warrant status of either Q1 or Q2 is Yes; ­ No if the warrant status of both Q1 and Q2 is No; and ­ Undecided whenever neither of the above cases hold. Using Definition 12, we can easily extend the nec and poss notations (cf. Page 14) to conjunctions and disjunctions of literals. The following result is a consequence of the fact that conflicting literals cannot be warranted in (Pre)DeLP [17]. Proposition 1 Let AM be a ground PreDeLP program and Q = L1  . . .  Ln be a conjunction of ground literals. Then, only one of the following cases holds: (i) P war Q, (ii) P war ¬Q, or (iii) the warrant status of Q is undecided.

22

G.I. Simari, P. Shakarian, and M.A. Falappa

Warrant Probability Functions
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
Original Program Prioritizing depression test Prioritzing anxiety test

Warrant Probability

Potentially Warranted Atoms

Fig. 10 Histogram depiction of the entailment probability functions for the programs of Example 9.

Proof In [17], a corresponding trichotomy result was shown for literals, i.e., the warrant status for any literal is one and only one of Yes, No, and Undecided. Our result is a direct consequence of this and Definition 12. Warrant Probability Functions are then simply defined as partial mappings with signature: I : basicAM  [0, 1] such that for f  basicAM , I (f ) = p if and only if nec(f ) Pr () = p. 4 When the program is clear from context, we drop the subscript and write simply  . In the following, we use notation dom( ) to denote the set of formulas for which  is defined. The table shown in Example 9 is a simple example of a WPF, whose domain is a handful of literals. The following is another example along the same vein. Example 10 Figure 10 shows three examples of WPFs in which the domains are fixed to the set of literals that can be warranted in the input program. These functions are related to the revision described in Example 9: the black bars show the original probabilities, the striped bars give the probabilities yielded by the program obtained by favoring the inclusion of the positive depression test, while the light gray bars depict the probabilities obtained by favoring the negative anxiety test. Figure 11 shows the three revised programs.

Revision Objective Functions. The other building block allows us to effectively quantify how good a revision is considered to be. Towards this end,
4 Note that this definition can easily be extended to deal with probability intervals as well (i.e., using both nec and poss ); here, for simplicity of presentation, we adopt this definition in order to work with point probabilities.

Quantitative Belief Revision in Structured Probabilistic Argumentation

23

we define revision objective functions (ROFs, for short) as functions that take two DeLP3E programs I1 and I2 , along with an epistemic input (f, af ), and returns a positive real number. We keep the definition of ROFs very general in order to allow different kinds of objectives to be specified. The following is a simple example of a ROF over our running example, which makes use of warranting probability functions. Example 11 Let us return once again to the medical diagnosis example. Suppose that we take the three revised programs we presented (Figure 11) ­ call them I1 , I2 , and I3 ­ and that we wish to compare them with respect to the effect of the last revision over the warranted atoms, taking the probabilities yielded by I1 as the baseline. So, we define the following revision objective function:  (I , I , (f, af )) = e- LLAM ,L=f |I (L)-I (L)| where I is the WPF for program I . Intuitively, this function sums up all the differences between the probabilities of literals entailed by the programs, but ignores the input (if it is a literal). In this way, a distance between the original program and the two possible revisions is obtained based on the effects that each revision had on the probabilities with which literals are derived. So, for our revisions, we get:  (I1 , I2 , (pos dep test, af 2 ))  0.0547  (I1 , I3 , (pos dep test, af 3 ))  0.8611 Therefore, we can conclude that the revision yielding I3 is preferred over the one yielding I2 when this ROF is adopted. Note that the function presented in Example 11 is just one possibility; the framework is very flexible and allows the user to express many different functions, depending on the specific way in which they wish to express distances between the original program and a given revised program. 3.2.2 The Class QAFO Given the basic constructs introduced above, we can now define the class of quantitative annotation function-based revision operators. Definition 13 (The Class QAFO) Let I = (EM , AM , af ), with AM =        be a DeLP3E program,  AFO be an annotation function-based belief revision operator, and  be a revision objective function. Operator is said to be a quantitative af-based operator (denoted  QAFO) if: Given an epistemic input (f, af ), we have that if I = I (f, af ) then there does not exist DeLP3E program I = I · (f, af ) such that  (I , I , (f, af )) >  (I , I , (f, af )), where ·  AFO is an arbitrary operator.

24 Analytical Model : 1 2 3 4 5 6 = = = = = = mem loss short breath fainting neg anx test neg tox screen pos dep test ¬sleep aid misuse  neg tox screen ¬anxiety  neg anx test anxiety  depression depression  pos dep test

G.I. Simari, P. Shakarian, and M.A. Falappa af 1 True True True ¬FN-anx test ¬FN-tox screen True af 2 True True True False af 1 (5 ) True af 3 True True True af 1 (4 ) af 1 (5 ) ¬af 1 (4 ) True True True True

:

1 = 2 = 3 = 4 =

True True True True

True True True True

: :

 1 = 2 = 3 = 4 = sleep aid misuse ­ mem loss, depression anxiety ­ short breath depression ­ mem loss anxiety ­ fainting

True {anx risk} {dep risk} {anx risk}

True af 1 (2 ) af 1 (3 ) af 1 (4 )

True af 1 (2 ) af 1 (3 ) af 1 (4 )

Fig. 11 The DeLP3E program from the running example, after performing three revisions: (i) The addition of the 4 and 5 , as discussed in Example 9; (ii) The revision by pos dep test by prioritizing this input; and (iii) The same revision but prioritizing neg anx test.

So, this subclass of AFO simply takes a revision objective function and uses it to obtain the best possible revised program. The following example, based on our previous work on applications of DeLP3E to problems in the cyber security domain [40], shows how QAFO operators can be applied to belief revision problems in real-world scenarios other than the running example. Example 12 Suppose we are modeling a cyber security scenario in which a computer worm has been deployed and has infected millions of computers worldwide ­ by the time the worm is discovered, it is very difficult to reason about the origin and even the intended target of the attack. In this kind of situations, analysts are trying to solve the so-called attribution problem: given a cyber operation of interest, determine the party that was ultimately responsible for carrying it out [38]. Towards this end, we can model all knowledge available by means of a DeLP3E program I = (EM , AM , af ), in which there is one distinguished predicate condOp (A, O) in the AM that is intuitively read as "actor A conducted operation O". Furthermore, if we assume that only one actor is ever responsible for an operation (an assumption that can easily be removed), we have an integrity constraint of the form oneOf(C ), where C is the set of all ground atoms built with the condOp predicate. Given this setup, we can define a WPF with a domain consisting of some formulas of interest that reflect conditions that the analysts would like to remain relatively unaffected when incorporating new information. For instance,

Quantitative Belief Revision in Structured Probabilistic Argumentation

25

suppose we define: dom( ) = ¬condOp (countryA, worm)  ¬condOp (countryB, worm) condOp (countryD, worm) , denoting the fact that neither country A nor country B are responsible for deploying the worm, and that country D is. If we pair this WPF with the ROF from Example 11, the corresponding QAFO operator will prefer revisions that do not affect the conclusions already reached regarding the probabilities assigned to these statements. In other words, this definition of dom( ), with the ROF in question, causes distances to be gauged relative to their effect on the probabilities assigned to the suspicions that (i) neither country A nor country B carried out the attack, and (ii) country D is behind the attack. Thus, such a setup causes the operator to prefer revisions that keep the probabilities assigned to such suspicions as close as possible to the ones yielded by the original program. In the next section, we study the computational complexity associated with this approach to belief revision in the DeLP3E setting. 4 Computational Complexity In this section, we will focus on some of the computational aspects of quantitative af-based belief revision operations. As a first observation, we have that the problem of deciding the warranting status in a (classical) PreDeLP program has not yet been pinpointed. In [4], the authors present a proof for the PSPACE-completeness of the problem of marking a given dialectical tree; PSPACE membership for deciding the warrant status is therefore a direct consequence of this result, since a dialectical tree can be built within this budget. As a step towards finding a lower bound for the complexity of the problem in general, we have the following. Proposition 2 Let AM be a ground PreDeLP program and L be a ground literal. Deciding AM war L is NP-hard. Proof By reduction from 3SAT-CNF; we therefore start with an input formula F with n variables X1 , . . . , Xn and m clauses C1 , . . . , Cm . We produce a PreDeLP program AM with the following predicates: f , x1 , . . . , xn , and c1 , . . . , cm . We then set:  = {f, ¬f, x1 , ¬x1 , . . . , xn , ¬xn }  = {cj ­ xi | Xi = T makes clause Cj true}  {cj ­ ¬xi | Xi = F makes clause Cj true}  {f ­ ci | for each clause Ci } ==

26

G.I. Simari, P. Shakarian, and M.A. Falappa

Next, we set the comparison criterion to specificity (the default in PreDeLP), except for the following exceptions: {xi , cj ­ xi }, cj is always preferred over {¬xi , cj ­ ¬xi }, cj {¬f }, ¬f is preferred over {f }, f Now, we must show that AM war f if and only if there exists a satisfying assumption for formula F . Suppose AM war f ; the only way this can happen is if argument {¬f }, ¬f (the only counterargument to {f }, ) is defeated, which happens if and only if all arguments that defeat this argument are in turn undefeated. Note that the only arguments capable of being in this situation are the ones using the rules with cj in the head. Therefore, if all such arguments are undefeated it must be the case that either xi or ¬xi can be chosen for each variable Xi in such a way that all clauses are satisfied, which holds if and only if there exists a satisfying assumption for F . As a corollary to Proposition 2, we have that deciding our extended notion of warrant status remains within the same complexity bounds. Corollary 1 Let AM be a ground PreDeLP program and Q be either a conjunction or disjunction of ground literals. Deciding AM war Q is NP-hard and in PSPACE. Proof (Sketch) Applying Definition 12, the warrant status of Q can be determined in polynomial time based on the warrant status of its literals; therefore, the same complexity results for determining the warrant status of a single literal apply. Assumption: Since, as stated above, the precise complexity of deciding the warrant status of a literal in a PreDeLP program is not yet known, and with the objective of separating the complexity of this problem from the complexity of the problems inherent to quantitative belief revision in DeLP3E programs, in the following we will make the assumption that classical warranting in PreDeLP is decidable in polynomial time. This is not an unreasonable assumption if we consider the possibility of pre-compiling inferences [3] or having tractable approximation algorithms to address the problem. We call this the polynomialtime warranting (PTW) assumption. Note that, even though this assumption does not hold in general, it is a useful tool in the analysis of the complexity of the problems studied here; it is also with this spirit that we make use of the PTW assumption. Unfortunately, our first result regarding the probabilistic extension of PreDeLP tells us that computing WPFs runs into a computational tractability hurdle. Theorem 1 Under the PTW assumption, computing the warrant probability function for a DeLP3E program is #P-hard.

Quantitative Belief Revision in Structured Probabilistic Argumentation

27

Proof We will prove the statement by reduction from #3CNF-SAT. Given formula F in 3CNF with n variables and m clauses, generate a DeLP3E program as follows: in the AM, there is one atom f , one atom ci for each clause in F and two atoms for each variable V in, which we denote with posV and negV . ^ Y ^ Z ^ , where V ^ denotes either V or For each clause Ci in F of the form X ¬V , we have strict rules: ci  x ^ ci  y ^ ci  z ^ where v ^ denotes posV if V is positive in the clause and negV if it is negative. Next, we have the strict rule: f  c1 , . . . , c m and facts posV and negV for each variable V in the input formula. In the EM we have atoms event-posV and event-negV for each variable V in F . The probability distribution assigns probability 0.5 to each individual event, probability 0 to the conjunction of both events for the same variable, and probability 1 to their disjunction. Finally, the annotation function  assigns formula True to all components of the AM, except the facts, for which we have af (posV ) = event-posV and af (negV ) = event-negV . Now we can see that, with this DeLP3E program, if the warranting probability function  assigns probability p to atom f , we have that:
nec(f )

Pr () = p.

Clearly, from our construction we know that   nec(f ) iff  corresponds to a satisfying assignment for formula F . Therefore, p = 2k n , where k is the number of satisfying assignments for F . Solving for k , we have k = p · 2n . The complexity class #P contains problems related to counting solutions (or, in Turing machine terms, accepting paths) to problems in NP. The decision version of this class is called PP, and contains problems decidable by a probabilistic Turing machine in polynomial time, with error probability less than a certain proportion (say, 1/2). Unfortunately, Toda's theorem [47] tells us that a polynomial-time Turing machine with either a PP or #P oracle can solve all problems in the polynomial hierarchy. Though it might be surmised that the #P-hardness is caused solely by the computation of probabilities (as is the case in many probabilistic formalisms), by analyzing the proof of Theorem 1 we can quickly arrive at the following conclusion. Observation 1 Computing the warrant probability function for a DeLP3E program is #P-hard even in the special case in which probabilities associated with EM worlds can be computed in PTIME.

28

G.I. Simari, P. Shakarian, and M.A. Falappa

Though this intractability holds in general, restricting the EM can soften the impact on complexity. For instance, if we assume that Nilsson's probabilistic logic [33] is used then the complexity is lowered, as we show next; first, we introduce a lemma that will be used in the proof of this result: Lemma 1 ([6, 12]) If a system of m linear equalities and/or inequalities has a nonnegative solution, then it has a nonnegative solution with at most m positive variables. This result was first introduced in [6], and later used in [12] to show that deciding the validity of a formula in their logic is NP-complete. We can now state our result. Proposition 3 Under the PTW assumption, and assuming that Nilsson's probabilistic logic is used in the EM, computing the warrant probability function for a DeLP3E program is NP-complete. Proof Membership: Lemma 1 states that a solution to a linear program is guaranteed to exist where only a number of the variables that is linear in the number of constraints in the EM are set to a non-zero value. This implies the existence of a number of EM worlds with non-zero probability that is linear in the number of statements in the probabilistic model. A witness can therefore be verified in polynomial time. Hardness: NP-hardness is a consequence of the well-known fact that SAT is reducible to computing probabilities in Nilsson logic (cf. [27] for a detailed proof). The previous result gives us a hint towards reaching the next one: if we combine the simplifying assumption that probabilities can be computed tractably with the further assumption that the number of EM worlds that have nonzero probability is bounded by a polynomial (condition 1 below), then we are guaranteed that computing WPFs is also tractable. Corollary 2 Let I = (EM , AM , af ), with AM =       , be a DeLP3E program. If we make the following assumptions: 1. |{ |   WEM and Pr() > 0}|  O(poly(n)), where n represents the size of the input; 2. Pr() can be computed in PTIME for any   WEM ; and 3. the PTW assumption holds, then warrant probability functions for I can also be computed in PTIME. Proof Direct consequence of the assumption that we have access to the polynomially many worlds that have non-zero probability. We thus simply keep an accumulator for each element in the domain of the WPF and iterate through the set of worlds, adding the probability of the world to each formula's accumulator if and only if it is warranted in the AM induced by that world.

Quantitative Belief Revision in Structured Probabilistic Argumentation

29

Unfortunately, the following result states that even in this scenario we still face an intractable hurdle when computing optimal revisions. Theorem 2 Let I = (EM , AM , af ), with AM =       , be a DeLP3E program,  QAFO, and  be a revision objective function that can be computed in polynomial time. If we have that: 1. |{ |   WEM and Pr() > 0}|  O(poly(n)), where n represents the size of the input; 2. Pr() can be computed in PTIME for any   WEM ; and 3. the PTW assumption holds, then deciding if  (I , I (f, af ), (f, af ))  k for some k  R, is NP-complete. Proof Membership: Given I , we can check in polynomial whether each world with non-zero probability induces a maximal consistent subset of I in that world; by construction, AFO operators are only constrained to make such changes in worlds with probability greater than zero. Furthermore, since by hypothesis we know that  can be computed in polynomial time, we can decide whether or not the witness satisfies the constraints. Hardness: By reduction from SUBSET-SUM; we therefore start from an instance of this problem, which consists of a set of n integers x1 , . . . , xn and another integer c; the goal is to verify if there exists a subset of the numbers that add up to c. Lets then create an instance of our problem, starting with a DeLP3E program with one atom numi in the AM for each xi in the input instance, as well as an auxiliary atom p; there will also be a corresponding atom event-numi in the EM for each such atom. The probability distribution associated with these random variables is set to an arbitrary one satisfying the condition that only a number polynomial in n has non-zero probability. Add as facts all atoms numi , with the annotation function defined as af (numi ) = event-numi . Next, add the strict rule:  : ¬numi  p such that af ( ) = True. Set the epistemic input to fact in with af (in ) = event-num1  . . .  event-numn . Finally, set the function to optimize to:   1 if res = {numi | numi  AM , af (numi ) |= } obj(I , I ) = is such that numi res xi = c   0 otherwise. That is, the objective function only considers the revised DeLP3E program, and takes value 1 if and only if the atoms that belong to this program correspond to numbers that, taken together, sum up to c. Thus, all we have to do is check if the optimal revision yields a value of 1 for the objective function in order to decide the given subset-sum instance.

30

G.I. Simari, P. Shakarian, and M.A. Falappa

The reader may note that the construction used in the proof of Theorem 2 uses a very powerful objective function that essentially encodes the NP-hard problem; furthermore, this objective function is not based on WPFs. We now provide an alternative result that proves NP-completeness under the same conditions, but assumes that the objective function is simply the sum of the probabilities assigned by the WPF to the set of ground atoms in the language associated with the AM. Theorem 3 Let I = (EM , AM , af ), with AM =       , be a DeLP3E program,  QAFO, and  be a revision objective function that can be computed in polynomial time. If we have that: 1. |{ |   WEM and Pr() > 0}|  O(poly(n)), where n represents the size of the input; 2. Pr() can be computed in PTIME for any   WEM ; and 3. the PTW assumption holds, then deciding if  (I , I (f, af ), (f, af ))  k for some k  R, is NP-complete even when  is defined as aGAM  (a). Proof Membership: As the ROF can still be computed in polynomial time, the membership result of Theorem 2 still holds. Hardness: We show NP-hardness by reduction from SIMPLE MAX CUT (SMC) [19]. The SMC problem takes as input an undirected graph G = (V, E ) and k  0, and decides if there exist sets V1 , V2  V such that |{(u, v )  E : u  V1 , v  V2 }|  k . Let x be a the only atom in GEM . We will define a simple EM that sets the probability of atom x to 1.0. We specify the atoms in GAM as follows: for each vi  V we create two atoms, set1 (vi ), set2 (vi ). For each edge (vi , vj )  E we create atom edge(vi , vj ) (we assume that each bi-directional edge is specified by one atom an that the order of the arguments for the edge predicate is arbitrary but consistent). We will also have an additional atom query in GAM , which will also act as the formula for the epistemic input. We create AM with the following elements: ­ For each vi  V add the following strict rules: ­ set1 (vi )  query ­ ¬set1 (vi )  query ­ set2 (vi )  ¬set1 (vi ) ­ For each edge (vi , vj )  E add strict rules edge(vi , vj )  set1 (vi ), set2 (vj ) and edge(vi , vj )  set2 (vi ), set1 (vj ) For each element y  AM , we define the annotation function af (y ) = True. For the epistemic input, let af be the extension of af such that af (query) = x. Finally, let the ROF be defined as in the statement of the theorem. Further, for ease of notation, let af  be the annotation function returned after the belief revision operation takes place.

Quantitative Belief Revision in Structured Probabilistic Argumentation

31

Clearly, this construction can be performed in polynomial time. Further, note that the original program is consistent, since none of the rules in AM ever fire since there are no facts. Observations: We notice right away that any valid belief revision operator must return an annotation function af  such that {x} |= af  (query) ­ as this atom is needed to ensure the objective function has a non-zero value (which is clearly possible). Further, any optimal solution where af  (query)  True can be replaced with a solution where af  (query) = x. We also note that in any optimal solution, the size of the set {setX (vi ) |  (setX (vi ) = 1.0} is equal to |V | ­ this is how we capture the notion that for each vertex vi , exactly one of set1 (vi ), set2 (vi ) will be warranted under world {x}); hence, we capture the requirement from the instance of SMC that the sets V1 , V2 is a partition of V . We also note that the annotation function in the solution af  must only modify the return values for strict rules of the form set1 (vi )  query and ¬set1 (vi )  query. Claim 1: Let V1 , V2 be an optimal solution to an instance of SMC. Then, the optimal solution to the corresponding revision problem has an objective function whose value is greater than or equal to |{(u, v )  E : u  V1 , v  V2 }| + |V | +1. This can clearly be achieved by a solution where for each vi  V1 , af  (set1 (vi )  query) = x and for each vi  V2 , af  (¬set1 (vi )  query) = ¬x. Claim 2: Let R be the value of the objective function returned in an optimal solution to the revision problem. Then, the number of edges returned in the corresponding instance of SMC is greater than or equal to R - |V | - 1. By the aforementioned observations, this claim is equivalent to saying that the number of positive literals of the form edge(vi , vj ) that are warranted in the world {x} is less than or equal to the number of edges returned in the optimal solution to the corresponding instance of SMC. Suppose, by way of contradiction, that the number of literals of that form that are warranted under {x} is greater than the number of edges in the optimal solution to the corresponding instance of SMC. By the construction, for each literal of the form edge(vi , vj ), exactly one of the following pairs of literals must also be warranted: set1 (vi ), set2 (vj ) or set2 (vi ), set1 (vj ). Therefore, we can partition the corresponding vertices from SMC into two sets ­ V1 , V2 and the number of edges in the set {(u, v )  E : u  V1 , v  V2 } must then be greater than the number of edges in the optimal solution to the SMC problem. However, this is not possible, as this is also (by construction) the same objective function that is optimized in that problem ­ hence, we reach a contradiction. The proof of hardness follows directly from Claims 1 and 2. So, the proof of Theorem 3 illustrates that the quantified revision problem is still NP-hard when the EM and the number of EM worlds, and (hence) the computation of the WPF is not a source of complexity ­ even when the ROF used is a simple aggregate over WPFs of atoms. Further, as we can embed the Simple Max Cut problem, the ROF ­ even a simple sum over WPFs ­ will not necessarily be monotonic, even when using a revision operator that satisfies

32 Algorithm warrantFormula(F )

G.I. Simari, P. Shakarian, and M.A. Falappa

1. For each tree (V, E )  F with (V, E ) = T  i ( A, L ) do 2. For each v  V with label(v ) = fj A af (fj ) do 3. While |V | > 1 do 4. For each v = A , L  {v  V | children(v )  leaves(V )} do 5. label(v ):= label(v )  ¬ v children(v) ¬label(v ); 6. End for; 7. V := V \ leaves(V ); 8. End while; 9. fi := label root(T  i ( A, L )) ; 10. End for; 11. End for; 12. Return i fi . Fig. 12 An algorithm that takes a classical dialectical forest and computes a logical formula specifying the possible worlds under which a given literal is warranted.

the Inclusion postulate (where the set of worlds satisfying af  (y )  af (y )). This also shows NP-completeness when the belief revision operator performs modifications to AM (by removing elements, as discussed in [41]) as setting af  (y ) = ¬x can be viewed as an operation that is equivalent to removing it from AM . We also note that the related problem of consolidation or contraction by falsum, where we start with an inconsistent program and then must adjust the annotation function to make it consistent, can also be shown to be NPcomplete by a simple modification to the proof: we fix the epistemic input to True, and change the rules of the form set1 (vi )  query, ¬set1 (vi )  query to facts of the form set1 (vi ), ¬set1 (vi ).

5 Warranting Formulas We now focus on an algorithmic approach that can be used to compute approximate solutions and therefore address the computational intractability that we have seen in the results above. In the following, given a dialectical forest F (L) and a node V corresponding to argument A, we will use the notation label(V ) = cA af (c). For a given probabilistic argumentation framework, literal, and dialectical tree, Algorithm warrantFormula in Figure 12 computes the formula describing the set of possible worlds that are warranting scenarios for the literal. Intuitively, this algorithm creates a formula for every dialectical tree in the forest associated with an argument ­ the algorithm iteratively builds a formula associated with the environmental conditions under which the argument in the root of the tree is undefeated. It then returns the disjunction of all such formulas in that forest. We refer to this disjunction as the warranting formula for the literal. The following result states the correctness of the warrantFormula algorithm.

Quantitative Belief Revision in Structured Probabilistic Argumentation

33

v1

1

1 = á{q1, d1, d3}, Lñ
label(v1) = dep_risk  (FN_tox_screen)

v3

3

3 = á{q1, q6, w4, d1}, Lñ
label(v3) = True  (FN_tox_screen)

v2

2

2 = á{q5, w3}, Lñ
label(v2) = (FN_tox_screen)

v4

4

4 = á{q5, w3}, Lñ
label(v4) = (FN_tox_screen)

Fig. 13 Dialectical forest for literal L = sleep aid misuse composed of trees T1 (left) and T2 (right).

Proposition 4 Given forest F  (L),
nec(L) = poss(L) =   WEM |  |= warrantFormula F  (L)   WEM |  |= warrantFormula F  (¬L) .

Proof (Sketch) Claim 1: nec(L)  {  WEM | ( |= warrantFormula(F  (L)))}. To prove this claim, it suffices to show that if T A, L is a valid dialectical tree for L then  |= warrantFormula(F  (L)). Suppose, BWOC, T A, L is such a tree where  |= warrantFormula(F  (L)). We note that this tree shares a root and is a subtree of a tree in F  (L). Also, for each node v in the tree, at line 2, we have  |= label(v ). Hence, we can continue the proof by replacing line 2 with v, label(v ) = f or() and showing that warrantFormula(F  (L)) = f alse. However, we can conclude that warrantFormula(F  (L)) = f or() since the tree has an odd depth by definition ­ this is a contradiction. Claim 2: nec(L)  {  WEM | ( |= warrantFormula(I , L))}. Suppose, by way of contradiction, that the claim is false. Then there must exist a root-sharing subtree of an element of F  (L) such that for each v , at step 4  |= label(v ) and does not exist in . However, this is a contradiction by Definition 7. Claim 3: The second part of the statement follows directly from Claims 1-2 and the fact that poss(L) = WEM \ nec(¬L). Even though warranting formulas are another way of solving the problem of computing probabilities exactly, our main motivation for developing it was to explore options for pursuing tractable algorithms, as discussed next. The following is an example of the warranting formula approach in the setting of our running example. Example 13 Consider the DeLP3E in our running example as shown in Figure 11 with annotation function af 1 . If we run Algorithm warrantFormula for literal sleep aid misuse, we start with the dialectical forest shown in Figure 13. Suppose that the algorithm begins with tree T1 (on the left); the only leaf of this tree corresponds to vertex v2 for argument A2 , and its label remains

34

G.I. Simari, P. Shakarian, and M.A. Falappa

the conjunction of all annotations of elements in the argument ­ label(v2 ) = ¬FN tox screen. The algorithm then moves to the next node up, which is already the root, and updates the label by adding the conjunction with the negation of its child, which yields: label(v1 ) = dep risk  ¬ ¬FN tox screen = dep risk  FN tox screen. Processing tree T2 similarly yields: label(v3 ) = True  ¬ ¬FN tox screen = FN tox screen. Finally, the algorithm outputs the disjunction of these two formulas, which is simply FN tox screen. Outlook: Towards Tractable Computations By applying the warrantFormula algorithm to the dialectical forest for a given literal L, we can obtain the sets nec(L) and poss(L) with a running time proportional to the size of the forest and the annotation formulas ­ though the worst-time complexity has not been determined exactly, it is safe to conjecture that the worst case is intractable. However, the warranting formula approach opens the door to several possibilities for heuristics and approximate computations that either avoid exhaustively enumerating worlds in WEM or working with full forests (or both). When combined with existing heuristics for classical argumentation (the AM) and probabilistic models (the EM), this provides us with a much more efficient way to compute warranting probability functions. Experimental evaluations for such hypotheses are currently underway. The use of the warranting formula approach can have several impacts in the implementation of specific QAFO operators. First, warrant probability functions  in this setting can now be redefined to map elements in their domain to warranting formulas instead of probabilities as in their original formulation. Revision objective functions now have at their disposal formulas instead of raw numbers. This opens up the possibility for specific implementations to leverage optimizations such as applying SAT algorithms to decide whether Pr I1 (L)  Pr I2 (L) (which can be decided via the SAT check I1 (L)  I2 (L)). Such an approach is clearly compatible with heuristic optimizations that may, for instance, sacrifice precision for greater tractability. An alternative class of operators can thus be defined based on the same ideas as QAFO except that approximations are allowed instead of exact computations. There is much work to be done in this direction, which is outside the scope of the current paper. 6 Related Work This paper continues the research line that began with two works on belief revision in structured probabilistic argumentation. In [39], we introduced the

Quantitative Belief Revision in Structured Probabilistic Argumentation

35

DeLP3E formalism (which is called P-PreDeLP in that work) and annotationfunction based belief revision (the class AFO), while in [40] we studied a special case of entailment queries and showed how the framework can be applied to a cyber-attribution problem. As we have seen, the main problem studied in belief revision is the study of how epistemic states should be changed in response to epistemic inputs. Traditionally, epistemic states have taken the form of either belief sets (sets of formulas closed under consequence) [1, 18] or belief bases [24, 23] (which are not closed). Our goal is to ultimately apply our results to real-world domains, and therefore we focus our attention on belief bases. Epistemic states in our case consist of formulas over which argumentation-based reasoning is carried out and to which we couple a general probabilistic model. The relationship between argumentation and belief revision can be traced back to [10]; in this regard, the work that is most closely related to how we approach their combination is that of [14], where explanation-based revision operators are studied. For a discussion on the relationship between the two areas of study, see [15] and [13]. Regarding argumentation systems that feature some quantitative form of reasoning under uncertainty, we point out that the combination of probabilistic reasoning with argumentation systems has been the focus of several works in the recent past. A significant portion of this work, however, has adopted abstract argumentation systems as the basis for the probabilistic extension [29, 45, 25, 16]. Contrary to structured argumentation systems like the one adopted in this work, abstract argumentation is focused on the study of attacks among arguments without inspecting their composition. There are also some works that combine structured argumentation approaches with models for reasoning under uncertainty ­ the first of these was [22]; the work of [28], which was developed even earlier, combines structured argumentation with abstract uncertainty measures but does not explicitly handle probability. Several other works followed; for instance, in [5], the authors develop a possibilistic extension to DeLP, and [26] presents an approach based on probabilistic logic. The main difference between these works and our own is that here we adopt a bipartite knowledge base, where one part models the knowledge that is not inherently probabilistic ­ uncertain knowledge is modeled separately, thus allowing a clear separation of interests between the two kinds of models. This kind of approach is not novel; it has been adopted in several frameworks, such as the Independent Choice Logic [35] or probabilistic ontology languages for the Semantic Web (see [20], and references within). From a quantitative take on belief revision, which is the specific topic of this paper, there hasn't been much work in the precise direction taken here. Perhaps the earliest proposal with such an idea in mind is that of maxichoice revision operators [2], which ensure that minimal changes are made to the knowledge base when revisions are performed; in fact, our class of AFO operators (and therefore also QAFO) perform maxichoice revision operations in each possible EM world. In the related setting of belief contraction operators, David Makinson [31] has defended the use of maxichoice operators, explaining that some counterintuitive behaviors that this approach leads to is

36

G.I. Simari, P. Shakarian, and M.A. Falappa

due to its "misapplication" to belief sets (which, we recall, are closed under consequence). Another quantitative proposal is the one presented in [7], where revisions are carried out according to a notion of distance between worlds, such as the number of propositional symbols that separate one model from another. These notions, however, do not correspond directly with the one adopted here, since in our setting we are pursuing a revision that is optimal from the point of view of its effect on the probabilistic aspect of the consequences of the knowledge base, while the knowledge bases in [2] are non-probabilistic. Another interesting approach to belief revision from a quantitative standpoint is ranking theory [43], which is a normative theory of the dynamics of belief. Again, this approach is not directly related to the one taken here; however, exploring how this well-studied approach can be applied in conjunction with argumentation in the way that probability theory is applied in this work is an interesting avenue for future work. Along the same vein of seeking to minimize information loss, and in the closely related setting of inconsistency management, the work of [21] proposes the notion of preferred repair based on so-called "consistency scores"; our quantitative approach to performing belief revision operations is loosely based on this proposal. Also in the inconsistency management literature, the recent work of [9, 8] proposes to resolve conflicts at a global level to minimize information loss but using incision functions instead. Another work in inconsistency management is that of [46] proposes measures of inconsistency for probabilistic logics. Apart from [21], this is perhaps the closest work in spirit to the one presented here, though the underlying language used (probabilistic conditional logic) is quite different and that work does not address the problem of belief revision ­ their measures, however, could be applied to efforts in line with our own. The adaptation of measures of probabilistic inconsistency such as the ones proposed in [46] to DeLP3E and their use in quantitative belief revision operators is the topic of ongoing and future work.

7 Conclusions and Future Work In this work, we tackled the problem of incorporating a new piece of information to an existing knowledge base; specifically, we adopted the DeLP3E model, which is an extension of the structured argumentation language DeLP with presumptions (PreDeLP) via the incorporation of annotations that refer to events for which we have underlying probabilistic information. The main focus of this paper was to further explore a class of belief revision operators called AFO (for annotation function-based revision) that we proposed recently in [39, 41] by considering operators in this class that have the further requirement of "quantitative optimality" ­ this gave rise to the QAFO class of operators. Though this optimality criterion was kept as general as possible so that knowledge engineers can specify their preferences, we explored the computational complexity of the approach in general, arriving at a host of results that range from intractability for the general case to polynomial-time

Quantitative Belief Revision in Structured Probabilistic Argumentation

37

special cases. finally, we presented an algorithm designed to compute the probability with which a literal is warranted via so-called warranting formulas, and provide some initial discussion regarding how this approach could be applied in the implementation of QAFO operators or approximations of them that trade theoretical guarantees for tractability in practice. Future work along this line of research involves continuing with efforts to bridge the gap between the theoretical developments that have been steadily coming from the belief revision community and practical implementations that can be applied in real-world domains such as medical diagnosis (the topic of our running example) and the related problem of solving the attribution problem in cyber security and cyber warfare, as proposed in [40]. We are also investigating the use of different kinds of belief revision operators, for instance ones that are based on argumentation [14]. Finally, we are currently in the final stages of developing a fully-functional implementation of DeLP3E; incorporating the belief revision operators developed in [39, 41] and in this paper is the next step in the implementation effort.
Acknowledgements This work was supported by UK EPSRC grant EP/J008346/1-- "PrOQAW", ERC grant 246858--"DIADEM", by NSF grant #1117761, by the Army Research Office under the Science of Security Lablet grant (SoSL) and project 2GDATXR042, DARPA project R.0004972.001, and funds provided by CONICET and Universidad Nacional del Sur, Argentina. The opinions in this paper are those of the authors and do not necessarily reflect the opinions of the funders, the U.S. Military Academy, or the U.S. Army.

References
1. Alchourr´ on, C.E., G¨ ardenfors, P., Makinson, D.: On the logic of theory change: Partial meet contraction and revision functions. J. Sym. Log. 50(2), 510­530 (1985) 2. Alchourr´ on, C.E., Makinson, D.: On the logic of theory change: Contraction functions and their associated revision functions. Theoria 48(1), 14­37 (1982) 3. Capobianco, M., Ches~ nevar, C.I., Simari, G.: Argumentation and the dynamics of warranted beliefs in changing environments. Intl. Journal on Autonomous Agents and Multiagent Systems (JAAMAS) 11, 127­151 (2005) 4. Cecchi, L.A., Simari, G.R.: El marcado de un a ´rbol dial´ ectico en DeLP es PSPACEcompleto. In: Proc. of Congreso Argentino de Ciencias de la Computaci´ on (CACIC) (2011) 5. Ches~ nevar, C.I., Simari, G.R., Alsinet, T., Godo, L.: A logic programming framework for possibilistic argumentation with vague knowledge. In: Proc. of UAI 2004, pp. 76­84 (2004) 6. Chv´ atal, V.: Linear Programming. W.H.Freeman, New York (1983) 7. Dalal, M.: Investigations into a theory of knowledge base revision: Preliminary report. In: Proc. of AAAI, pp. 475­479 (1988) 8. Deagustini, C.A.D., Martinez, M.V., Falappa, M.A., Simari, G.R.: Improving inconsistency resolution by considering global conflicts. In: Proc. of SUM. Springer (2014, To Appear) 9. Deagustini, C.A.D., Martinez, M.V., Falappa, M.A., Simari, G.R.: Inconsistency resolution and global conflicts. In: Proc. of ECAI (2014, To Appear) 10. Doyle, J.: A truth maintenance system. Artif. Intell. 12(3), 231­272 (1979) 11. Dung, P.M.: On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artif. Intell. 77, pp. 321­357 (1995)

38

G.I. Simari, P. Shakarian, and M.A. Falappa

12. Fagin, R., Halpern, J.Y., Megiddo, N.: A logic for reasoning about probabilities. Information and Computation 87(1/2), 78­128 (1990) 13. Falappa, M.A., Garcia, A.J., Kern-Isberner, G., Simari, G.R.: On the evolving relation between belief revision and argumentation. The Knowledge Engineering Review 26(01), 35­43 (2011) 14. Falappa, M.A., Kern-Isberner, G., Simari, G.R.: Explanations, belief revision and defeasible reasoning. Artif. Intell. 141(1/2), 1­28 (2002) 15. Falappa, M.A., Kern-Isberner, G., Simari, G.R.: Belief revision and argumentation theory. In: Argumentation in artificial intelligence, pp. 341­360. Springer (2009) 16. Fazzinga, B., Flesca, S., Parisi, F.: On the complexity of probabilistic abstract argumentation. In: Proc. of IJCAI 2013 (2013) 17. Garc´ ia, A.J., Simari, G.R.: Defeasible logic programming: An argumentative approach. TPLP 4(1-2), 95­138 (2004) 18. Gardenfors, P.: Knowledge in flux: modeling the dynamics of epistemic states. MIT Press, Cambridge, Mass. (1988) 19. Garey, M., Johnson, D.: Computers and Intractability: A Guide to the Theory of NPCompleteness. Freeman, New York (1979) 20. Gottlob, G., Lukasiewicz, T., Martinez, M.V., Simari, G.I.: Query answering under probabilistic uncertainty in Datalog+/­ ontologies. AMAI (2013) 21. Gottlob, G., Lukasiewicz, T., Martinez, M.V., Simari, G.I.: Query answering under probabilistic uncertainty in Datalog+/- ontologies. AMAI (2013) 22. Haenni, R., Kohlas, J., Lehmann, N.: Probabilistic argumentation systems. Springer (1999) 23. Hansson, S.: Semi-revision. J. of App. Non-Classical Logics 7(1-2), 151­175 (1997) 24. Hansson, S.O.: Kernel contraction. J. Symb. Log. 59(3), 845­859 (1994) 25. Hunter, A.: Some foundations for probabilistic abstract argumentation. In: Proc. of COMMA 2012, pp. 117­128 (2012) 26. Hunter, A.: A probabilistic approach to modelling uncertain logical arguments. Int. J. Approx. Reasoning 54(1), 47­81 (2013) 27. Khuller, S., Martinez, M.V., Nau, D.S., Sliva, A., Simari, G.I., Subrahmanian, V.S.: Computing most probable worlds of action probabilistic logic programs: scalable estimation for 1030,000 worlds. AMAI 51(2-4), 295­331 (2007) 28. Krause, P., Ambler, S., Elvang-Gørannson, M., Fox, J.: A logic of argumentation for reasoning under uncertainty. Computational Intelligence 11 (1), 113­131 (1995) 29. Li, H., Oren, N., Norman, T.J.: Probabilistic argumentation frameworks. In: Proc. of TAFA, pp. 1­16 (2011) 30. Lloyd, J.W.: Foundations of Logic Programming, 2nd Edition. Springer (1987) 31. Makinson, D.: On the status of the postulate of recovery in the logic of theory change. Journal of Philosophical Logic 16(4), 383­394 (1987) 32. Martinez, M.V., Garc´ ia, A.J., Simari, G.R.: On the use of presumptions in structured defeasible reasoning. In: Proc. of COMMA, pp. 185­196 (2012) 33. Nilsson, N.J.: Probabilistic logic. Artif. Intell. 28(1), 71­87 (1986) 34. Pearl, J.: Probabilistic reasoning in intelligent systems: networks of plausible inference (1988) 35. Poole, D.: The independent choice logic for modelling multiple agents under uncertainty. Artif. Intell. 94(1-2), 7­56 (1997) 36. Rahwan, I., Simari, G.R.: Argumentation in Artificial Intelligence. Springer (2009) 37. Richardson, M., Domingos, P.: Markov logic networks. Machine Learning 62, 107­136 (2006) 38. Shakarian, P., Shakarian, J., Ruef, A.: Introduction to Cyber-Warfare: A Multidisciplinary Approach. Syngress (2013) 39. Shakarian, P., Simari, G.I., Falappa, M.A.: Belief revision in structured probabilistic argumentation. In: Proc. of FoIKS 2014, pp. 324­343 40. Shakarian, P., Simari, G.I., Moores, G., Parsons, S., Falappa, M.A.: An argumentationbased framework to address the attribution problem in cyber-warfare. In: Proc. of Cyber Security 2014 (2014)

Quantitative Belief Revision in Structured Probabilistic Argumentation

39

41. Shakarian, P., Simari, G.I., Moores, G., Paulo, D., Parsons, S., Falappa, M.A., Aleali, A.: Belief revision in structured probabilistic argumentation: Model and application to cyber security. Under review ­ available at: http://www.delp3e.webs.com/Shakarian-etal-DeLP3E.pdf (2014) 42. Simari, G.R., Loui, R.P.: A mathematical treatment of defeasible reasoning and its implementation. Artif. Intell. 53(2-3), 125­157 (1992) 43. Spohn, W.: The laws of belief: Ranking theory and its philosophical applications. Oxford University Press (2012) 44. Stolzenburg, F., Garc´ ia, A., Ches~ nevar, C.I., Simari, G.R.: Computing Generalized Specificity. Journal of Non-Classical Logics 13(1), 87­113 (2003) 45. Thimm, M.: A probabilistic semantics for abstract argumentation. In: Proc. of ECAI 2012, pp. 750­755 (2012) 46. Thimm, M.: Inconsistency measures for probabilistic logics. Artif. Intell. 197, 1­24 (2013) 47. Toda, S.: On the computational power of PP and P. In: Proc. of FOCS, pp. 514­519 (1989) 48. Wirth, C., Stolzenburg, F.: David Poole's specificity revised. In: Proc. of KR (2014)

A Quantitative Approach to Belief Revision in
Structured Probabilistic Argumentation
Gerardo I. Simari Â· Paulo Shakarian Â·
Marcelo A. Falappa

Received: July 30, 2015/ Accepted: date

Abstract Many real-world knowledge-based systems must deal with information coming from different sources that invariably leads to incompleteness,
overspecification, or inherently uncertain content. The presence of these varying levels of uncertainty doesnâ€™t mean that the information is worthless â€“
rather, these are hurdles that the knowledge engineer must learn to work
with. In this paper, we continue work on an argumentation-based framework
that extends the well-known Defeasible Logic Programming (DeLP) language
with probabilistic uncertainty, giving rise to the Defeasible Logic Programming with Presumptions and Probabilistic Environments (DeLP3E) model.
Our prior work focused on the problem of belief revision in DeLP3E, where we
proposed a non-prioritized class of revision operators called AFO (Annotation
Function-based Operators) to solve this problem. In this paper, we further
study this class and argue that in some cases it may be desirable to define revision operators that take quantitative aspects into account, such as how the
probabilities of certain literals or formulas of interest change after the revision
takes place. To the best of our knowledge, this problem has not been addressed
in the argumentation literature to date. We propose the QAFO (Quantitative
Annotation Function-based Operators) class of operators, a subclass of AFO,
and then go on to study the complexity of several problems related to their
specification and application in revising knowledge bases. Finally, we present
an algorithm for computing the probability that a literal is warranted in a
DeLP3E knowledge base, and discuss how it could be applied towards implementing QAFO-style operators that compute approximations rather than
exact operations.
G.I. Simari, M.A. Falappa
Dept. of Comp. Sci. and Eng., Universidad Nacional del Sur and CONICET, Argentina
E-mail: {gis,mfalappa}@cs.uns.edu.ar
P. Shakarian
Arizona State University, Tempe, AZ, USA
E-mail: shak@asu.edu

2

G.I. Simari, P. Shakarian, and M.A. Falappa

Keywords Structured Argumentation Â· Belief Revision Â· Reasoning under
Probabilistic Uncertainty

1 Introduction and Motivation
Many real-world knowledge-based systems must deal with information coming from different sources that invariably leads to uncertain content, be it
from gaps in knowledge (incompleteness), overspecification (inconsistency), or
because the knowledge is inherently uncertain (such as weather forecasts or
measurements that are necessarily imprecise). Far from considering such uncertain knowledge useless, knowledge engineers face the challenge of putting it
to its best possible use when solving a wide range of problems. In particular,
one basic problem that needs to be investigated in depth is that of revising
such knowledge bases in a principled manner. In this paper, we continue work
on that combines the well-known Defeasible Logic Programming (DeLP) language (actually, an extension called PreDeLP to handle presumptions) with
probabilistic uncertainty, which led to the development of the DeLP3E model
(Defeasible Logic Programming with Presumptions and Probabilistic Environments). In previous work [39, 41], we studied a class of non-prioritized belief
revision operators called AFO that allows changes to be made only to probabilistic annotations when addressing the incorporation of an epistemic input
into the knowledge base. Here, we propose a subclass called QAFO that takes
quantitative aspects into account when performing revisions, such as how the
probabilities of certain literals or formulas of interest change after the revision
takes place. To the best of our knowledge, this problem has not been addressed
in the argumentation literature to date.
The main contributions presented in this paper are briefly summarized as
follows:
â€“ As building blocks to be used in our quantitative belief revision operators,
we define: (i) warrant probability functions (WPFs), which have as domains
either conjunctions or disjunctions of ground literals that are mapped to
the probability that they are warranted in the DeLP3E program; and (ii)
revision objective functions (ROFs), which take as input two DeLP3E programs I1 , I2 and an epistemic input and return a numeric score that is
interpreted as the value of obtaining I2 from I1 when revising it by the
epistemic input. ROFs generally include WPFs in their definitions.
â€“ We propose the QAFO class of operators, a subclass of AFO that allows
modifications to the annotation function to be carried out as part of the
belief revision operation, but focusing on optimizing a given ROF, as described above.
â€“ We study the complexity of several problems related to our approach; in
particular, we present:

Quantitative Belief Revision in Structured Probabilistic Argumentation

3

(i) A new lower bound on the complexity of deciding the warrant status of
a literal or a conjunction/disjunction of literals in a (classical) PreDeLP
program;
(ii) Computing WPFs in general is #P-hard;
(iii) Point (ii) holds even when computing probabilities of worlds in the
environmental model can be done in polynomial time;
(iv) Computing WPFs in the special case in which Nilssonâ€™s probabilistic
logic [33] is used is NP-complete;
(v) By combining the intuition behind the proof of point (iv) and further
conditions, we identify a class of instances for which WPFs can be
computed in polynomial time; and
(vi) Under the same conditions as point (v), we show that QAFO revisions
are NP-complete; furthermore, we show that the problem has the same
complexity even when the revision objective function is constrained to
be a simple sum of warranting probabilities for atoms in the AM.
â€“ We present an algorithm for computing the probability that a literal is
warranted in a DeLP3E knowledge base, and discuss its application towards
implementing QAFO-style operators that compute approximations rather
than perform exact operations.

The rest of this paper is organized as follows: Section 2 discusses preliminary concepts and the DeLP3E framework, which was first introduced in [39,
41]. Section 3 motivates the specialization of AFO operators to take into account quantitative aspects, and presents the class QAFO. Section 4 studies
the complexity of several problems related to QAFO and the application of
such operators in revising DeLP3E knowledge bases. Section 5 studies a novel
approach to reasoning about probabilities of literals in DeLP3E, called warranting formulas, and their application in the implementation of QAFO-style
operators that address the high computational cost issuess by applying heuristics the trade off optimality for tractability. Finally, Sections 6 and 7 present
related work and conclusions, respectively.
Throughout the entire paper, we illustrate the presentation via a running
example that is inspired on the use of DeLP3E in medical diagnosis, which
is the kind of real-world application in which we envision our work being
of most use; we have also explored its application in the related scenario of
solving the attribution problem1 in cyber security and cyber warfare [40], with
encouraging feedback from the community as to its potential impact.
1 Essentially, given a cyber event of interest, the attribution problem involves finding
out who was responsible for it. This is especially well suited for argumentation and belief
revision due to the ease with which a potential culprit can plant false or misleading clues,
hence giving rise to an inconsistent knowledgebase. See [38] for further discussion.

4

G.I. Simari, P. Shakarian, and M.A. Falappa

2 Preliminaries on the DeLP3E Framework
The DeLP3E framework is comprised of two distinct, but interrelated models
of the world. The first is called the environmental model (referred to from
now on as the â€œEMâ€), and is used to describe uncertain knowledge about
the domain that is subject to probabilistic events. The second one is called
the analytical model (referred to as the â€œAMâ€), and contains knowledge that
is either strict or defeasible, as described below â€“ this will be useful in the
analysis of competing hypotheses that can account for a given phenomenon
(what we will generally call queries).
The AM is composed of a classical (that is, non-probabilistic) PreDeLP [32]
program in order to allow for contradictory information, giving the system the
capability to model competing explanations for a given query. In general, the
EM contains knowledge such as evidence, uncertain facts, or knowledge about
agents and systems. The AM, on the other hand, contains knowledge that
may or may not be strictly valid, yet it does not depend on probabilistic
events. Indeed, dividing knowledge between the AM and EM is a knowledge
engineering task, and its adequate resolution will call for design decisions to be
made; note that this separation also allows for a different kind of uncertainty
to be modeled â€“ defeasible rules and presumptions can be leveraged when there
is no probabilistic information available but we still wish to maintain that a
specific portion of the knowledge base is uncertain.
In the rest of this section, we formally describe these two models, as well
as how knowledge in the AM can be annotated with information from the EM
â€“ these annotations specify the conditions under which the various statements
in the AM can potentially be true.
Basic Language. We assume sets of variables and constants, denoted with V
and C, respectively. The language also contains a set of n-ary predicate symbols; the EM and AM use separate sets of predicate symbols, denoted with
PEM and PAM , respectively â€“ the two models can, however, share variables
and constants. As usual, a term is composed of either a variable or a constant.
Given terms t1 , ..., tn and n-ary predicate symbol p, p(t1 , ..., tn ) is called an
atom; if t1 , ..., tn are constants, then the atom is said to be ground. The sets
of all ground atoms for EM and AM are denoted with GEM and GAM , respectively; finally, we also use notation LAM to denote the set of all (ground)
literals: {a | a âˆˆ GAM } âˆª {Â¬a | a âˆˆ GAM }. Note that even though in general the
language allows variables, for simplicity we will assume throughout this paper
that all objects are ground (propositional).
Example 1 Consider a medical diagnosis scenario2 in which a patient goes to
a hospital exhibiting certain symptoms: shortness of breath, sporadic fainting
(loss of consciousness for brief periods of time), and some signs of memory
2 This and all related examples in this paper, though inspired by potential real-world
applications, make many simplifying assumptions in order to allow for a concise presentation
of the key concepts that we wish to illustrate.

Quantitative Belief Revision in Structured Probabilistic Argumentation
PEM :

PAM :

anx risk

The patient falls into the category of people at risk of
suffering anxiety.

dep risk

The patient falls into the category of people at risk of
suffering depression.

FN-anx test

Event associated with a false negative coming up when
performing a test to see whether a patient is showing
anxiety-related symptoms.

FN-tox screen

Event associated with a false negative coming up when
performing a blood test for the presence of toxic
substances.

mem loss

The patient shows signs of memory loss.

short breath

The patient suffers shortness of breath.

fainting

The patient suffers short-term loss of consciousness.

anxiety

The patient is diagnosed with an anxiety-related
disorder.

depression

The patient is diagnosed with a depression-related
disorder.

sleep aid misuse

Patient diagnosed with misuse of sleeping aids.

neg anx test

Test proves absence of anxiety-related disorders.

neg tox screen

Test proves absence of toxins in blood.

pos dep test

Test proves presence of depression-related disorders.

5

Fig. 1 Explanation of the meaning of the predicates used in the running example.

loss. Figure 1 shows the predicates that we will use throughout the paper in
the running example.
As shown in the figure (and discussed in more detail below), some of these
predicates comprise the analytical model (AM), while others are part of the
environmental model (EM). For instance, in our example, predicates stating
the presence of symptoms such as memory loss and shortness of breath, as
well as those representing diagnoses, such as anxiety and depression, are part
of the analytical model. On the other hand, the environmental model contains
predicates that are associated with uncertain events, such as false negatives
coming up when a test is performed or the risk that the patient will be affected
by anxiety-related disorders. Note that in the running example we make use
of the term â€œat riskâ€ according to the common use of this expression in the
medical domain, i.e., characterized by high risk or susceptibility, such as to a
certain disease.

Given set of ground atoms, a world is any subset of atoms â€“ those that
belong to the set are said to be true in the world, while those that do not are
false. Therefore, there are 2|GEM | possible worlds in the EM and 2|GAM | worlds
in the AM. These sets are denoted with WEM and WAM , respectively. In
order to avoid worlds that do not model possible situations given a particular
domain, we include integrity constraints of the form oneOf(A0 ), where A0 is a
subset of ground atoms. Intuitively, such a constraint states that any world

6

G.I. Simari, P. Shakarian, and M.A. Falappa

where more than one of the atoms from set A0 appears is invalid. We use
ICEM and ICAM to denote the sets of integrity constraints for the EM and
AM, respectively, and the sets of worlds that conform to these constraints is
denoted with WEM (ICEM ) and WAM (ICAM ), respectively.
Finally, logical formulas arise from the combination of atoms using the
traditional connectives (âˆ§, âˆ¨, and Â¬). As usual, we say that a world Î» satisfies
formula (f ), written Î» |= f , iff: (i) If f is an atom, then Î» |= f iff f âˆˆ Î»; (ii) if
f = Â¬f 0 then Î» |= f iff Î» 6|= f 0 ; (iii) if f = f 0 âˆ§ f 00 then Î» |= f iff Î» |= f 0 and
Î» |= f 00 ; and (iv) if f = f 0 âˆ¨ f 00 then Î» |= f iff Î» |= f 0 or Î» |= f 00 . We use the
notation formEM , formAM to denote the set of all possible (ground) formulas
in the EM and AM, respectively; finally, we use basicAM to denote all possible
conjunctions or disjunctions of literals from LAM , which we refer to as basic
formulas.

2.1 Environmental Model
In this paper, we generalize the approach taken in our previous work [39, 41]
for the environmental model â€“ here, we simply assume that we have a probabilistic model defined over GEM , which represents a probability distribution
over WEM .
Definition 1 (Probabilistic Model) Given sets PEM , V, and C, and corresponding sets GEM and WP
EM , a probabilistic model Î EM is any function
Pr : WEM â†’ [0, 1] such that Î»âˆˆWEM Pr (Î») = 1.
Examples of probabilistic models that can be used are Bayesian networks
(BNs) [34], Markov logic networks (MLNs) [37], extensions of first order logic
such as Nilssonâ€™s probabilistic logic [33], or even ad-hoc representations of
function Pr from Definition 1. The following is an example of a BN over the
running example.
Example 2 Consider the set PEM from Figure 1. The Bayesian network depicted in Figure 2 describes the probability distribution Pr over all possible
worlds WEM shown in Figure 3.
So, for instance, the probability that false negatives do not arise in any of
the two tests, and that the patient is at risk for both anxiety- and depressionrelated disorders (world Î»4 ) is 0.29808.


2.2 Analytical Model
The DeLP3E formalism adopts a structured argumentation framework [36] for
the AM. While the EM contains probabilistic information about the state of
the world, the AM must allow for a different kind of information; in particular,
it must be able to represent contradictory knowledge. This approach allows

Quantitative Belief Revision in Structured Probabilistic Argumentation
FN_tox_screen

anx_risk
AR

dep_risk
DR

7

Pr(AR = T) = 0.4

FNTS

Pr(AR = F) = 0.6

Pr(FNTS = T) = 0.1
Pr(FNTS = F) = 0.9

FN_anx_test
Pr(DR = T | AR = T) = 0.9

FNAT

Pr(DR = F | AR = T) = 0.1

Pr(FNAT = T | AR = T) = 0.08
Pr(FNAT = F | AR = T) = 0.92
Pr(FNAT = T | AR = F) = 0.01

Pr(DR = T | AR = F) = 0.2

Pr(FNAT = F | AR = F) = 0.99

Pr(DR = F | AR = F) = 0.8

Fig. 2 Bayesian network used in the EM of the running example. The names of the random
variables are simply the abbreviations of their corresponding atoms: AR 7â†’ anx risk, DR 7â†’
dep risk, FNAT 7â†’ FN anx test, and FNTS 7â†’ FN tox screen.

World
Î»1
Î»2
Î»3
Î»4
Î»5
Î»6
Î»7
Î»8
Î»9
Î»10
Î»11
Î»12
Î»13
Î»14
Î»15
Î»16

anx risk
T
T
T
T
T
T
T
T
F
F
F
F
F
F
F
F

dep risk
T
T
T
T
F
F
F
F
T
T
T
T
F
F
F
F

FN anx test
T
T
F
F
T
T
F
F
T
T
F
F
T
T
F
F

FN tox screen
T
F
T
F
T
F
T
F
T
F
T
F
T
F
T
F

Probability
0.00288
0.02592
0.03312
0.29808
0.00032
0.00288
0.00368
0.03312
0.00048
0.00432
0.04752
0.42768
0.00012
0.00108
0.01188
0.10692

Fig. 3 Probability distribution for the worlds in the running example.

for the creation of arguments that may compete with each other in order
to reach a conclusion regarding a given query. This is known as a dialectical
process, where arguments defeat each other based on a comparison criterion.
Resulting from this process, certain arguments are warranted, while others
are defeated. Argumentation-based reasoning has been proposed and studied
in depth as a natural way to manage a set of inconsistent information â€“ its
strength lies in the fact that it closely resembles the way humans settle disputes
(consider, for instance, how convictions are decided in trials). Another highly
desirable characteristic of structured argumentation frameworks is that, once a
conclusion is reached, the process also yields an explanation of how we arrived
at it, as well as information about why a given argument is warranted. In the
following, we first recall the basics of the underlying argumentation framework
used, and then go on to introduce the analytical model (AM).

8

G.I. Simari, P. Shakarian, and M.A. Falappa

Defeasible Logic Programming with Presumptions (PreDeLP)
PreDeLP, first introduced in [32], is a formalism combining logic programming
with defeasible argumentation. We will briefly recall the basics of PreDeLP,
and refer the reader to [17, 32] for the complete presentation.
The formalism contains several different constructs: facts, presumptions,
strict rules, and defeasible rules. Facts are statements that always hold (such
as a patientâ€™s symptom in our example), while presumptions are statements
that may or may not be true (such as a medical diagnosis). Strict rules establish logical consequences (similar to material implication in first order logic,
though the semantics is not exactly the same since the contrapositive does not
follow from a strict rule). While strict rules, like facts, always hold, defeasible rules specify logical consequences that may be assumed to be true when
no contradicting information is available. These components are used in the
construction of arguments, and together comprise PreDeLP programs.
Formally, we use the notation Î AM = (Î˜, â„¦, Î¦, âˆ†) to denote a PreDeLP
program, where:
â€“ â„¦ is the set of strict rules of the form L0 â† L1 , . . . , Ln , where L0 is a
ground literal and {Li }i>0 is a set of ground literals;
â€“ Î˜ is the set of facts, written simply as atoms;
â€“ âˆ† is the set of defeasible rules of the form L0 â€“â‰º L1 , . . . , Ln , where L0 is a
ground literal and {Li }i>0 is a set of ground literals, and
â€“ Î¦ is the set of presumptions, which are written as defeasible without a
body.
For simplicity, we will sometimes refer to Î AM as a set corresponding to the
union of its components.
Recall that all atoms in the AM must be formed with a predicate from the
set PAM , and note that in both strict and defeasible rules, strong negation (i.e.,
classical negation as in first-order logic) is allowed in the head, and thus may
be used to represent contradictory knowledge. The following is an example of
a PreDeLP program over the running example.
Example 3 Consider again the medical diagnosis scenario from our running
example; the DeLP3E program in Figure 4 encodes some basic knowledge that
the attending physician might use to diagnose their patient. For instance,
strict rule Ï‰1 states that based on a negative result when administering a
test for toxins in the blood we can conclude that the patient is not misusing
sleeping aids. On the other hand, defeasible rule Î´1 states that memory loss and
depression can lead to such a misuse. In this example, the set of presumptions
is empty.

Arguments. Given a query in the form of a ground atom, the goal is to derive
arguments for and against its validity â€“ derivation follows the same mechanism
of logic programming [30], and we denote such derivation with the symbol â€œ`â€.
In the following, we say that such a derivation is â€œstrictâ€ if it only uses facts

Quantitative Belief Revision in Structured Probabilistic Argumentation
Î˜:

Î¸1 =
Î¸2 =
Î¸3 =

mem loss
short breath
fainting

â„¦:

Ï‰1
Ï‰2
Ï‰3
Ï‰4

=
=
=
=

Â¬sleep aid misuse â† neg tox screen
Â¬anxiety â† neg anx test
anxiety â† depression
depression â† pos dep test

Î¦:

âˆ…

âˆ†:

Î´1
Î´2
Î´3
Î´4

=
=
=
=

sleep aid misuse â€“â‰º mem loss, depression
anxiety â€“â‰º short breath
depression â€“â‰º mem loss
anxiety â€“â‰º fainting

9

Fig. 4 A ground argumentation framework.

and strict rules; otherwise, we say that the derivation is â€œdefeasibleâ€. Likewise,
we say that a literal is strictly (defeasible) derived if the derivation is strict
(defeasible). Finally, we say that an argument is â€œfactualâ€ if no presumptions
are used in it.
Since rule heads can contain strong negation, it is possible to defeasibly
derive contradictory literals from a program. For the treatment of contradictory knowledge, PreDeLP incorporates a defeasible argumentation formalism
that allows the identification of the pieces of knowledge that are in conflict
and, through the dialectical process discussed above, decides which information prevails as warranted. This dialectical process involves the construction
and evaluation of arguments, formally defined next.
Definition 2 (Argument) An argument hA, Li for a literal L is a pair of
the literal and a (possibly empty) set of the AM (A âŠ† Î AM ) that provides
a minimal proof for L meeting the following requirements: (i) L is defeasibly
derived from A; (ii) â„¦ âˆª Î˜ âˆª A is not inconsistent; and (iii) A is a minimal
subset of âˆ† âˆª Î¦ satisfying (i) and (ii), denoted hA, Li.
Literal L is called the conclusion supported by the argument, and A is the
support of the argument. An argument hB, Li is a subargument of hA, L0 i if
B âŠ† A. An argument hA, Li is presumptive if A âˆ© Î¦ is not empty. We will also
use â„¦(A) = A âˆ© â„¦, Î˜(A) = A âˆ© Î˜, âˆ†(A) = A âˆ© âˆ†, and Î¦(A) = A âˆ© Î¦.
Our definition differs slightly from that of [42], where DeLP is introduced, as
we include strict rules and facts as part of arguments. We make this change
because in our framework the strict rules and facts used to construct a given
argument may only be true in certain worlds in the EM. Hence, the entire set
of facts and strict rules need not be consistent in our framework. We shall
discuss how portions of the AM are assigned EM worlds in the next section.
Example 4 Figure 5 shows example arguments based on the PreDeLP program
from Figure 4. Argument A3 uses an additional component not present in the

10

G.I. Simari, P. Shakarian, and M.A. Falappa
hA1 , anxietyi
hA2 , anxietyi
hA3 , Â¬sleep aid misusei

A1 = {Î¸1 , Î´3 , Ï‰3 }
A2 = {Î¸3 , Î´4 }
A3 = {Ï‰1 } âˆª {neg tox screen â€“â‰º }

Fig. 5 Example arguments based on the running example scenario.

original program, and states that if we can assume a negative result for a tox
screen, we can conclude that the patient is not misusing sleeping aids.

Given an argument hA1 , L1 i, counter-arguments are arguments that contradict it. Argument hA2 , L2 i is said to counterargue or attack hA1 , L1 i at a
literal L0 iff there exists a subargument hA, L00 i of hA1 , L1 i such that the set
â„¦(A1 ) âˆª â„¦(A2 ) âˆª Î˜(A1 ) âˆª Î˜(A2 ) âˆª {L2 , L00 } is inconsistent. A proper defeater
of an argument hA, Li is a counter-argument that â€“ by some criterion â€“ is
considered to be better than hA, Li; if the two are incomparable according to
this criterion, the counterargument is said to be a blocking defeater. An important characteristic of PreDeLP is that the argument comparison criterion
is modular, and thus the most appropriate criterion for the domain that is
being represented can be selected; the default criterion used in classical defeasible logic programming (from which PreDeLP is derived) is generalized
specificity [44], though an extension of this criterion is required for arguments
using presumptions [32]. We briefly recall this criterion next â€“ the first definition is for generalized specificity, which is subsequently used in the definition
of presumption-enabled specificity.
Definition 3 (DeLP Argument Preference) Let Î AM = (Î˜, â„¦, Î¦, âˆ†) be
a PreDeLP program and let F be the set of all literals that have a defeasible
derivation from Î AM . An argument hA1 , L1 i is preferred to hA2 , L2 i, denoted
with A1 P S A2 if:
(1) For all H âŠ† F, â„¦(A1 )âˆªâ„¦(A2 )âˆªH is consistent: if there is a derivation for
L1 from â„¦(A2 ) âˆª â„¦(A1 ) âˆª âˆ†(A1 ) âˆª H, and there is no derivation for L1 from
â„¦(A1 ) âˆª â„¦(A2 ) âˆª H, then there is a derivation for L2 from â„¦(A1 ) âˆª â„¦(A2 ) âˆª
âˆ†(A2 ) âˆª H; and
(2) there is at least one set H 0 âŠ† F, â„¦(A1 ) âˆª â„¦(A2 ) âˆª H 0 is consistent, such
that there is a derivation for L2 from â„¦(A1 ) âˆª â„¦(A2 ) âˆª H 0 âˆª âˆ†(A2 ), there is
no derivation for L2 from â„¦(A1 ) âˆª â„¦(A2 ) âˆª H 0 , and there is no derivation for
L1 from â„¦(A1 ) âˆª â„¦(A2 ) âˆª H 0 âˆª âˆ†(A1 ).
Intuitively, the principle of specificity says that, in the presence of two
conflicting lines of argument about a proposition, the one that uses more
of the available information is more convincing. The following extension for
presumptive arguments was first introduced in [32].
Definition 4 (Presumptive Argument Preference) Given PreDeLP program Î AM = (Î˜, â„¦, Î¦, âˆ†), an argument hA1 , L1 i is preferred to hA2 , L2 i, denoted with A1  A2 if any of the following conditions hold:
(1) hA1 , L1 i and hA2 , L2 i are both factual and hA1 , L1 i P S hA2 , L2 i.

Quantitative Belief Revision in Structured Probabilistic Argumentation

11

(2) hA1 , L1 i is a factual argument and hA2 , L2 i is a presumptive argument.
(3) hA1 , L1 i and hA2 , L2 i are presumptive arguments, and
(a) Î¦(A1 ) ( Î¦(A2 ) or,
(b) Î¦(A1 ) = Î¦(A2 ) and hA1 , L1 i P S hA2 , L2 i.
Generally, if A, B are arguments with rules X and Y , resp., and X âŠ‚ Y , then
A is stronger than B. This also holds when A and B use presumptions P1 and
P2 , resp., and P1 âŠ‚ P2 .
Note: The specificity criterions used here are not transitive [48], and therefore
should not be assumed to define an ordering over arguments. This, however,
does not pose a problem for our framework, as the criterion is only ever used to
compare pairs of arguments to see which one â€œwins outâ€. We remind the reader
that the comparison criterion in our framework is modular; if transitivity is
required, the one proposed in [48] is an option.
A sequence of arguments called an argumentation line thus arises from
this attack relation, where each argument defeats its predecessor. To avoid
undesirable sequences, which may represent circular argumentation lines, in
DeLP an argumentation line is acceptable if it satisfies certain constraints
(see [17]). A literal L is warranted if there exists a non-defeated argument A
supporting L.
Clearly, there can be more than one defeater for a particular argument
hA, Li. Therefore, many acceptable argumentation lines could arise from hA, Li,
leading to a tree structure. The tree is built from the set of all argumentation
lines rooted in the initial argument. In a dialectical tree, every node (except
the root) represents a defeater of its parent, and leaves correspond to undefeated arguments. Each path from the root to a leaf corresponds to a different
acceptable argumentation line. A dialectical tree provides a structure for considering all the possible (maximal) acceptable argumentation lines that can
be generated for deciding whether an argument is defeated. We call this tree
dialectical because it represents an exhaustive dialectical (in the sense of providing reasons for and against a position) analysis for the argument in its root.
For a given argument hA, Li, we denote the corresponding dialectical tree as
T (hA, Li).
Given a literal L and an argument hA, Li, in order to decide whether or
not a literal L is warranted, every node in the dialectical tree T (hA, Li) is
recursively marked as â€œDâ€ (defeated ) or â€œUâ€ (undefeated ), obtaining a marked
dialectical tree T âˆ— (hA, Li) as follows:
1. All leaves in T âˆ— (hA, Li) are marked as â€œUâ€s, and
2. Let hB, Lq i be an inner node of T âˆ— (hA, Li). Then hB, Lq i will be marked
as â€œUâ€ iff every child of hB, Lq i is marked as â€œDâ€. The node hB, Lq i will be
marked as â€œDâ€ iff it has at least a child marked as â€œUâ€.
Given an argument hA, Li obtained from Î AM , if the root of T âˆ— (hA, Li)
is marked as â€œUâ€, then we will say that T âˆ— (hA, Li) warrants L and that L
is warranted from Î AM . (Warranted arguments correspond to those in the

12

G.I. Simari, P. Shakarian, and M.A. Falappa

grounded extension of a Dung argumentation system [11].) There is a further
requirement when the arguments in the dialectical tree contains presumptions
â€“ the conjunction of all presumptions used in even (respectively, odd) levels
of the tree must be consistent. This can give rise to multiple trees for a given
literal, as there can potentially be different arguments that make contradictory
assumptions.
We can then extend the idea of a dialectical tree to a dialectical forest. For
a given literal L, a dialectical forest F(L) consists of the set of dialectical trees
for all arguments for L. We shall denote a marked dialectical forest, the set of
all marked dialectical trees for arguments for L, as F âˆ— (L). Hence, for a literal
L, we say it is warranted if there is at least one argument for that literal in
the dialectical forest F âˆ— (L) that is labeled as â€œUâ€, not warranted if there is
at least one argument for the literal Â¬L in the dialectical forest F âˆ— (Â¬L) that
is labeled as â€œUâ€, and undecided otherwise. We shall refer to whether literal
L is warranted, not warranted, or undecided as Lâ€™s â€œwarrant statusâ€, and
sometimes refer to the â€œwarrantedâ€ status as â€œYesâ€ and the â€œnot warrantedâ€
status as â€œNoâ€.
2.3 The DeLP3E Framework
Our framework, originally proposed in [39], is the result of combining the
environmental and analytical models (which we denote with Î EM and Î AM ,
respectively). Intuitively, given Î AM , every element of â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ only
hold in certain worlds in the set WEM â€“ i.e., these elements are subject to
probabilistic events. Each element of â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ is thus associated with
a formula over GEM (using conjunction, disjunction, and negation, as usual)
â€“ we use formEM to denote the set of all such formulas. The notion of an
annotation function associates elements of â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ with elements in
formEM .
Definition 5 (Annotation Function [39]) An annotation function is any
function of the form af : â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ â†’ formEM . We use [af ] to denote the
set of all annotation functions.
We will sometimes denote annotation functions as sets of pairs (f, af(f )) in
order to simplify the presentation. Figure 6 shows an example of an annotation
function for our running example; for instance, the annotation for rule Î´2 means
that this rule only holds whenever the probabilistic event anx risk is true. If
annotations are â€œTrueâ€, this means that they hold in all possible worlds.
Definition 6 (DeLP3E Program) Given environmental model Î EM , analytical model Î AM , and annotation function af , a DeLP3E program is of the
form I = (Î EM , Î AM , af ). We use notation [I] to denote the set of all possible
programs.
In the following, given DeLP3E program I = (Î EM , Î AM , af ) and Î» âˆˆ
WEM , we use notation Î AM (Î») = {f âˆˆ Î AM s.t. Î» |= af (f )}. This gives rise
to a decomposed view of DeLP3E programs, as illustrated next.

Quantitative Belief Revision in Structured Probabilistic Argumentation
af (Î¸1 ) =
af (Î¸2 ) =
af (Î¸3 ) =

True
True
True

af (Ï‰1 ) =
af (Ï‰2 ) =
af (Ï‰3 ) =
af (Ï‰4 ) =

True
True
True
True

af (Î´1 ) =
af (Î´2 ) =
af (Î´3 ) =
af (Î´4 ) =

True
anx risk
dep risk
anx risk

13

Fig. 6 An example of an annotation function over the running example.
Î»1 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰ 2 , Ï‰ 3 , Ï‰ 4 ,
Î´ 1 , Î´2 , Î´3 , Î´ 4 }

Î»2 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰2 , Ï‰3 , Ï‰4 ,
Î´1 , Î´ 2 , Î´ 3 , Î´ 4 }

Î»3 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰ 2 , Ï‰ 3 , Ï‰ 4 ,
Î´1 , Î´ 2 , Î´ 3 , Î´ 4 }

Î»4 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰2 , Ï‰3 , Ï‰4 ,
Î´1 , Î´ 2 , Î´ 3 , Î´ 4 }

Î»5 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰ 2 , Ï‰ 3 , Ï‰ 4 ,
Î´ 1 , Î´2 , Î´4 }

Î»6 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰2 , Ï‰3 , Ï‰4 ,
Î´1 , Î´ 2 , Î´ 4 }

Î»7 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰ 2 , Ï‰ 3 , Ï‰ 4 ,
Î´1 , Î´ 2 , Î´ 4 }

Î»8 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰2 , Ï‰3 , Ï‰4 ,
Î´1 , Î´ 2 , Î´ 4 }

Î»9 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰ 2 , Ï‰ 3 , Ï‰ 4 ,
Î´ 1 , Î´3 }

Î»10 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰2 , Ï‰3 , Ï‰4 ,
Î´1 , Î´ 3 }

Î»11 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰ 2 , Ï‰ 3 , Ï‰ 4 ,
Î´1 , Î´ 3 }

Î»12 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰2 , Ï‰3 , Ï‰4 ,
Î´1 , Î´ 3 }

Î»13 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰ 2 , Ï‰ 3 , Ï‰ 4 ,
Î´1 }

Î»14 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰2 , Ï‰3 , Ï‰4 ,
Î´1 }

Î»15 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰ 2 , Ï‰ 3 , Ï‰ 4 ,
Î´1 }

Î»16 : {Î¸1 , Î¸2 , Î¸3 ,
Ï‰1 , Ï‰2 , Ï‰3 , Ï‰4 ,
Î´1 }

Fig. 7 A depiction of how the DeLP3E program in the running example can be decomposed
into one classical PreDeLP program for each possible EM world (cf. Figure 3 for the definition
of worlds Î»1 â€“Î»16 in terms of the random variables in the EM).

Example 5 Consider the different examples presented so far: the EM from
Example 2 (with the worlds from Figure 3), Î AM from Figure 4, the arguments
in Figure 5, and the annotation function from Figure 6 â€“ these components
give rise to a DeLP3E program I = (Î EM , Î AM , af ) Figure 7 shows how I can
be decomposed into one classical PreDeLP program Î AM (Î») for each world
Î» âˆˆ WEM .
For instance, Î AM (Î»7 ) contains Î¸1 , Î¸2 , Î¸3 , Ï‰1 , Ï‰2 , Ï‰3 , Ï‰4 , and Î´1 because
the annotation function associates condition True to all of these components;
it contains Î´2 and Î´4 because condition anx risk is true in Î»7 , and it does not
contain Î´3 because condition dep risk is false in Î»7 .

The most direct way of considering consequences of DeLP3E programs is
thus to consider what happens in each world in WEM ; that is, the defeat
relationship among arguments depends on the current state of the (EM) world.
Definition 7 (Existence of an Argument in a World) Given DeLP3E
program I = (Î EM , Î AM , af ), argument hA, Li is said to exist in world Î» âˆˆ
WEM if âˆ€c âˆˆ A, Î» |= af(c).

14

G.I. Simari, P. Shakarian, and M.A. Falappa

The notion of existence is extended to argumentation lines, dialectical trees,
and dialectical forests in the expected way (for instance, an argumentation
line exists in Î» iff all arguments that comprise that line exist in Î»).
Example 6 Consider the different examples presented so far: the worlds in
Figure 3, Î AM from Figure 4, the arguments in Figure 5, and the annotation
function from Figure 6.
Since argument A1 uses defeasible rule Î´3 , and af (Î´3 ) = dep risk (while
the other two components have annotation â€œTrueâ€), we can conclude that this
argument exists in worlds in which dep risk is true, i.e.,, Î»1 â€“Î»4 and Î»9 â€“Î»12 . 
The idea of a dialectical tree is also extended w.r.t. worlds; so, for a given
world Î» âˆˆ WEM , the dialectical (resp., marked dialectical) tree induced by Î»
is denoted with TÎ» hA, Li (resp., TÎ»âˆ— hA, Li). We require that all arguments and
defeaters in these trees exist in Î». Likewise, we extend the notion of dialectical
forests in the same manner (denoted with FÎ» (L) and FÎ»âˆ— (L), respectively).
Based on these concepts, we introduce the notion of warranting scenario.
Definition 8 (Warranting Scenario) Given DeLP3E program I = (Î EM ,
Î AM , af ) and literal L formed with a ground atom from GAM , a world Î» âˆˆ
WEM is said to be a warranting scenario for L (denoted Î» `war L) if there is a
dialectical forest FÎ»âˆ— (L) in which L is warranted and FÎ»âˆ— (L) exists in Î».
The idea of a warranting scenario is used to formally define DeLP3E entailment. The set of worlds in the EM where a literal L in the AM must be true
is exactly the set of warranting scenarios â€” these are the â€œnecessaryâ€ worlds:
nec(L) = {Î» âˆˆ WEM | (Î» `war L)}. Now, the set of worlds in the EM where
AM literal L can be true is the following â€” these are the â€œpossibleâ€ worlds:
poss(L) = {Î» âˆˆ WEM | Î» 6`war Â¬L}.
V
V
In the following we use notation for(Î») = aâˆˆÎ» a âˆ§ aâˆˆÎ»
/ Â¬a, which denotes
the formula that has W
Î» as its only model. We now extend this notation to sets
of worlds: for(W ) = Î»âˆˆW for(Î»). Entailment can then be defined as follows:
Definition 9 (DeLP3E Entailment) Given DeLP3E program I = (Î EM ,
Î AM , af ), AM literal L and probability interval p âˆˆ [`, u], we say that I
entails L with probabilityPp âˆˆ [`, u] if the probability
distribution Pr yielded
P
by Î EM is such that ` â‰¤ Î»âˆˆnec(L) Pr (Î») and Î»âˆˆposs(L)Î» Pr (Î») â‰¤ u.
2.4 Consistency of DeLP3E Programs
Finally, one of the central topics of this paper is that of inconsistencies, which
can arise in our framework in more than one way [39]. In this paper, we assume
that the probabilistic model is consistent, and focus on inconsistencies that
arise in the AM. Towards this end, we use the following notion of (classical)
consistency: PreDeLP program Î  is said to be consistent if there does not
exist a ground literal a s.t. Î  ` a and Î  ` Â¬a. For DeLP3E programs, the
interaction between the annotation function and facts and strict rules may
cause conflicts, as defined next.

Quantitative Belief Revision in Structured Probabilistic Argumentation

15

Definition 10 (Consistency of DeLP3E Programs) A DeLP3E program
I = (Î EM , Î AM , af ), with Î AM = hÎ˜, â„¦, Î¦, âˆ†i, is consistent if: given probability
distribution Pr for Î EM , if there exists a world Î» âˆˆ WEM such that
S
xâˆˆÎ˜âˆªâ„¦ | Î»|=af(x) {x} is inconsistent, then we have Pr (Î») = 0.
The intuition behind this definition is that subsets of facts and strict rules hold
under the circumstances specified by the annotation function â€“ such circumstances can be expressed as sets of EM worlds. Now, if there exists a world
where (at least) two contradictory strict statements are true, then the EM
must assign probability zero to this world.
Example 7 Let us return to the running example; consider Î AM from Figure 4, Î EM from Figure 2, and the annotation function from Figure 6, with
the addition of fact Î¸4 = pos dep test with af (Î¸4 ) = True and fact Î¸5 =
neg anx test with af (Î¸5 ) = Â¬FN-anx test. It is now clear that the program
is
S inconsistent, since there exists world Î»3 (among several others) such that
xâˆˆÎ˜âˆªâ„¦ | Î»3 |=af(x) {x} warrants both anxiety (via argument with Î¸4 and Ï‰3 )
and Â¬anxiety (via argument with Î¸5 and Ï‰2 ).


3 Revision of DeLP3E Programs based on Quantitative Aspects
We have finally arrived at the main problem we address in this paper â€“ revising
knowledge bases. This problem can be generically stated as: given DeLP3E
program I = (Î EM , Î AM , af ), with Î AM = â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ and a pair (f, af 0 )
where f is either an atom or a rule and af 0 is equivalent to af , except for its
expansion to include f 3 , obtain a new program I 0 called the revised knowledge
base that addresses the incorporation of the epistemic input (f, af 0 ) into the
original program; we denote this operation with the symbol â€œâ€¢â€ â€“ i.e.,, I 0 =
I â€¢ (f, af 0 ).
Now, the problem statement as presented above is quite vague, since we
did not give any details as to how the operator â€œaddresses the incorporationâ€
of the epistemic input. There are many approaches in the literature (cf. Section 6) that address this problem quite differently; one of the main properties
that characterize revision operators is whether or not they satisfy the Success
property, which states that the epistemic input must be a consequence of the
revised knowledge base. Here, we will adopt a cautious stance and assume
that this property does not hold in general; therefore, we focus on so-called
non-prioritized revision operators.
The basic issue that revision operators must deal with is inconsistency (we
will discuss this in more depth shortly); as we saw in Section 2.4, inconsistency
in DeLP3E programs involves worlds that have non-zero probability and an
associated PreDeLP program that is inconsistent. In our previous work [39,
41] we identified three basic approaches that can be taken towards solving this
problem:
3

That is, af 0 (x) = af (x) for all x âˆˆ dom(af ), and dom(af 0 ) = dom(af ) âˆª {f }.

16

G.I. Simari, P. Shakarian, and M.A. Falappa

â€“ Modifying the EM: Perhaps the simplest approach is to fix the problem of
inconsistency by forcing the probabilistic model to assign probability zero
to all worlds that cause inconsistencies to arise.
â€“ Modifying the AM: Alternatively, for each world in which the corresponding
PreDeLP program is inconsistent, we can modify this program to remove
the problem.
â€“ Modifying the annotation function. Finally, as a finer-grained approach
compared to the previous one, we can alter the annotation function.
In the following, we will assume that epistemic inputs involve only strict
components (facts or rules), since defeasible components can always be added
without inconsistencies arising. Regarding these three possible approaches, in
this paper we will focus on the third one since it is a generalization of the second
â€“ if we only allow removing elements from the AM, such an operation will have
the same effect as not removing the element but modifying the annotation
function so that it associates the formula â€œâŠ¥â€ to it. The generalization of
annotation function-based revision lies in that there is not always the need
for such a drastic measure; see [41] for further discussions on this, including
examples. Furthermore, operations of the first kind alone do not suffice to
perform revisions, as can be seen in the following simple example.
Example 8 Consider the following DeLP3E program, where the EM consists of
two worlds {a} and {Â¬a}, each with probability 0.5:
Ï‰1 :
Î¸1 :
Ï‰2 :
Î¸2 :

pâ†q
Â¬p
Â¬p â† q
p

af (Ï‰1 ) = a
af (Î¸1 ) = a
af (Ï‰2 ) = Â¬a
af (Î¸2 ) = Â¬a

Now, suppose we wish to revise by formula Î¸3 : q with af (Î¸3 ) = True. Since
both EM worlds are inconsistent with the formula, it is impossible to change
the allocation of the probability mass in order to avoid inconsistencies; therefore, the only option is to reject the input.


3.1 A Recap of Annotation Function-based Belief Revision
Since the quantitative approach that we propose in this paper is related to the
AFO class of revision operators introduced in [39], in this section we provide a
brief summary of the relevant material. After recalling the relevant postulates,
we present the construction of AFO operators.
3.1.1 Rationality Postulates
The following properties characterize the behavior of operators for revising
annotation functions; we briefly discuss them here in an informal manner, and
refer the reader to [41] for their formal presentation.

Quantitative Belief Revision in Structured Probabilistic Argumentation

17

â€“ Inclusion: For any given element g in the AM, the worlds that satisfy gâ€™s
annotation after the revision are a subset of the set of worlds satisfying
gâ€™s annotation before the revision. That is, the constraints in the revised
annotations can only become more restrictive.
â€“ Vacuity: If simply adding the input to the program does not lead to
inconsistencies, then the operator does precisely that.
â€“ Consistency Preservation: If the original program is consistent, then so
is the revised program.
â€“ Weak Success: As in Vacuity, if adding the input to the program does
not cause inconsistencies, then the input must be present â€œas isâ€ in the
revised program.
â€“ Relevance: Given a specific EM world, if a part of its associated AM
knowledge base is removed by the operator, then there exists a superset
of the remaining knowledge base that is not consistent with the removed
element and the input. That is, removed elements are always â€œrelevantâ€ to
an inconsistency.
â€“ AF Uniformity: If two different inputs are such that the same set of EM
worlds lead to inconsistencies in a given AM knowledge base, and it is the
case that analogous subsets taken in conjunction with their respective input
lead to equivalent consistency/inconsistency, then the models removed from
the annotations elements in the AM knowledge base are the same for both
inputs. In other words, the operator must behave in the same way when
presented with inputs that have an equivalent effect on the knowledge base.
We now continue briefly recalling the presentation of the annotation functionbased operator of [39] by discussing its construction. In order to do so, we
adopt the following notation: the program to revise is denoted with I =
(Î EM , Î AM , af ), with Î AM = â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦, and the epistemic input is
denoted with (f, af 0 ), where f is either an atom or a rule and af 0 is equivalent
to af , except for its expansion to include f . We denote the revision as follows:
I â€¢ (f, af 0 ) = (Î EM , Î AM , af 00 ) where af 00 is the revised annotation function.
We will slightly abuse notation in order to make the presentation clearer, and
use notation to convert sets of worlds to and from formulas. Moreover, we often refer to â€œremoving elements of Î AM â€ to refer to changes to the annotation
function that cause certain elements of the Î AM to not have their annotations
satisfied in certain EM worlds. As we are looking to change the annotation
function for a specific subset of facts and strict rules, we specify these subsets
with the following notation:
â€“ I âˆª (f, af 0 ) to denote I 0 = (Î EM , Î AM âˆª {f }, af 0 ).
â€“ (f, af 0 ) âˆˆ I = (Î AM , Î EM , af ) to denote f âˆˆ Î AM and af = af 0 .
0
I
â€“ WEM
(I) = {Î» âˆˆ WEM | Î AM
(Î») is inconsistent} â€“ this set contains all the
EM worlds for a given program where the corresponding knowledge base
in the AM is classically inconsistent.

18

G.I. Simari, P. Shakarian, and M.A. Falappa

I
0
0
â€“ WEM
(I) = {Î» âˆˆ WEM
|Pr (Î») > 0} â€“ this is a subset of WEM
(I) containing worlds that are assigned a non-zero probability; i.e., the worlds where
inconsistency in the AM arises.

â€“ wld(f ) = {Î» | Î» |= f } â€“ the set of worlds that satisfy formula f ; and
V
V
â€“ for(Î») = aâˆˆÎ» a âˆ§ aâˆˆÎ»
/ Â¬a â€“ the formula that has Î» as its only model.
I
(Î») = {f âˆˆ Î˜ âˆª â„¦ | Î» |= af(f )} â€“ the subset of facts and strict rules
â€“ Î AM
in Î AM whose annotations are true in EM world Î».

We will make use of this notation in the next section.
3.1.2 Operator Construction
The basis of the construction of the class of so-called annotation functionbased operators is that any subset of Î AM that is associated with a world
I
in WEM
(I âˆª (f, af 0 )) must be modified so that consistency is ensured. For
each such world Î», we make use of the following set of â€œcandidate replacement
programsâ€ for Î AM (Î»):
0
0
0
CandP gmaf (Î», I) = {Î AM
| Î AM
âŠ† Î AM (Î») s.t. Î AM
is consistent and
00
00
0
00
@Î AM
âŠ† Î AM (Î») s.t. Î AM
âŠƒ Î AM
s.t. Î AM

is consistent}
I
The intuition is that each maximal consistent subset of Î AM
(Î») is a candidate
for replacing the inconsistent program for that world. To specify the construction, we need to introduce some more notation. Let Î“ : WEM â†’ 2[Î˜]âˆª[â„¦ ] ; i.e.,
a function from EM worlds to subsets of the strict components of the AM â€“
this function will be used to choose the revised AM for each world. For each
component h in the AM there is a formula in Î AM âˆª{f }, where f is part of the
epistemic input (i.e., what the operator is revising by). Given these elements,
we define:
^
newFor(h, Î“, I, (f, af 0 )) = af 0 (h) âˆ§
Â¬f or(Î»i )
I (Iâˆª(f,af 0 )) | hâˆˆÎ“
Î»âˆˆWEM
/ (Î»)

Intuitively, newFor provides a new annotation for each component h âˆˆ Î AM ;
such formula can be seen as the result of selecting a maximally consistent
subset of Î AM (Î») for each EM world Î». We are finally able to introduce the
AFO class of operators:
Definition 11 (AF-based Operators [39, 41]) A belief revision operator
â€¢ is an â€œannotation function-basedâ€ (or af-based) operator (â€¢ âˆˆ AFO) if given
program I = (Î EM , Î AM , af ) and input (f, af 0 ), the revision is defined as
I â€¢ (f, af 0 ) = (Î EM , Î AM âˆª {f }, af 00 ), where:
âˆ€h, af 00 (h) = newFor(h, Î“, I, (f, af 0 ))
where âˆ€Î» âˆˆ WEM , Î“ (Î») âˆˆ CandP gmaf (Î», I âˆª (f, af 0 )).

Quantitative Belief Revision in Structured Probabilistic Argumentation
Analytical Model

19

Annotation Function

Î˜:

Î¸1 =
mem loss
True
True
Î¸2 =
short breath
Î¸3 =
fainting
True
-----------------------------------------------{Â¬FN-anx test}
Î¸4 =
neg anx test
Î¸5 =
neg tox screen
{Â¬FN-tox screen}

â„¦:

Ï‰1
Ï‰2
Ï‰3
Ï‰4

Î¦:

âˆ…

âˆ†:

Î´1
Î´2
Î´3
Î´4

=
=
=
=

Â¬sleep aid misuse â† neg tox screen
Â¬anxiety â† neg anx test
anxiety â† depression
depression â† pos dep test

True
True
True
True

=
=
=
=

sleep aid misuse â€“â‰º mem loss, depression
anxiety â€“â‰º short breath
depression â€“â‰º mem loss
anxiety â€“â‰º fainting

True
{anx risk}
{dep risk}
{anx risk}

Fig. 8 The DeLP3E program from the running example, after adding facts Î¸4 and Î¸5 . The
annotation function is provided in a separate column for convenience.

In [39, 41], we provide a representation theorem that states the equivalence
between this construction and the operators discussed above. We refer the
reader to these articles for a more complete and detailed presentation.

3.2 Towards a Quantitative Approach
Traditionally, belief revision has been addressed from a qualitative point of
view rather than a quantitative one (cf. Section 6 for a discussion on related
work). A simple example of this is the fact that, faced with the option of removing either both atoms a and b or only atom c, classical revision operators
typically declare both options to be just as good, since neither is a subset
of the other; it could be argued, then, that taking quantitative aspects into
account (such as the number of elements removed) may lead to a better solution â€“ of course, this may depend on other factors. As we will see, there
are different ways in which such quantitative aspects can be incorporated into
revision operations. For instance, in our setting, DeLP3E programs can be regarded in a world-by-world manner, and changes made in one world can be
compared to those made in another. The AFO operators described in Section 3.1 make decisions for each world independently; we now wish to address
the issue of taking into account different kinds of quantitative aspects when
revising DeLP3E programs. The following example motivates our approach in
our medical diagnosis scenario.
Example 9 Consider again the running example, and suppose the physician
has decided to carry out as a first step two tests, a toxin screen and an anxiety
test, and that both tests yielded negative results. Note that the validity of

20

G.I. Simari, P. Shakarian, and M.A. Falappa

these tests is subject to probabilistic events (in this case, false negatives). The
new program is reproduced in Figure 8.
Figure 9 shows the world-by-world decomposition of the new program,
and the atoms that are warranted in each case. From the information in this
figure, we can compute the following probabilities for the hypotheses that the
physician is contemplating (depression, anxiety, and misuse of sleeping aids):
Literal
depression
sleep aid misuse
Â¬sleep aid misuse
anxiety
Â¬anxiety

Probability
: 0.06672
: 0.05088
: 0.93324
: 0.06992
: 0.92888

Since they all have low probabilities after performing the tests, the doctor
decides to test for depression and in this case receives a positive result (atom
pos dep test). For the sake of this example, we will assume that the validity of
the outcome of this test (unlike the other two) is not subject to probabilistic
events â€“ thus, we have af (pos dep test) = True.
Now, while for the first two tests we were able to simply add the corresponding atoms and extend the annotation function accordingly, simply adding
Î¸6 = pos dep test with af (Î¸6 ) = True causes inconsistencies to arise in eight
of the possible worlds (Î»3 , Î»4 , Î»7 , Î»8 , Î»11 , Î»12 , Î»15 , and Î»16 ). Essentially, the
problems arise because the negative anxiety test allows us to conclude that
there is no anxiety, while the positive depression test would allow us to conclude that indeed there is anxiety. Since both derivations only involve strict
components, this leads to an inconsistent AM.

Example 9 shows an interesting case of belief revision in DeLP3E programs;
when presented with new information that is in conflict with existing one, we
must find a way to address its incorporation into the existing knowledge â€“
non-prioritized operators are very flexible, since they always have the option
of ignoring the new information. However, this flexibility also means that â€“
in the case of DeLP3E programs â€“ there is no guidance with respect to how
revisions should be carried out globally, since each world is treated as a separate
revision problem. Next, we discuss two kinds of functions that will prove to
be useful in addressing this situation.
3.2.1 Two Building Blocks
We now introduce warrant probability functions and revision objective functions, which are later used in the definition of our new class of non-prioritized
belief revision operators.
Warrant Probability Functions. As one of the building blocks to our
quantitative approach, given a DeLP3E program I we define warrant probability functions (WPFs, for short).

Quantitative Belief Revision in Structured Probabilistic Argumentation
World
Î»1
Î»2
Î»3
Î»4
Î»5
Î»6
Î»7
Î»8
Î»9
Î»10
Î»11
Î»12
Î»13
Î»14
Î»15
Î»16

Probability
0.00288
0.02592
0.03312
0.29808
0.00032
0.00288
0.00368
0.03312
0.00048
0.00432
0.04752
0.42768
0.00012
0.00108
0.01188
0.10692

21

Warranted Literals
{Î¸1 , Î¸2 , Î¸3 } âˆª {depression, sleep aid misuse, anxiety}
{Î¸1 , Î¸2 , Î¸3 , Î¸5 } âˆª {depression, Â¬sleep aid misuse, anxiety}
{Î¸1 , Î¸2 , Î¸3 , Î¸4 } âˆª {depression, Â¬sleep aid misuse, anxiety}
{Î¸1 , Î¸2 , Î¸3 , Î¸4 , Î¸5 } âˆª {Â¬sleep aid misuse, Â¬anxiety}
{Î¸1 , Î¸2 , Î¸3 } âˆª {anxiety}
{Î¸1 , Î¸2 , Î¸3 , Î¸5 } âˆª {Â¬sleep aid misuse, anxiety}
{Î¸1 , Î¸2 , Î¸3 , Î¸4 } âˆª {Â¬anxiety}
{Î¸1 , Î¸2 , Î¸3 , Î¸4 , Î¸5 } âˆª {Â¬sleep aid misuse, Â¬anxiety}
{Î¸1 , Î¸2 , Î¸3 } âˆª {depression, sleep aid misuse, anxiety}
{Î¸1 , Î¸2 , Î¸3 , Î¸5 } âˆª {depression, Â¬sleep aid misuse, anxiety}
{Î¸1 , Î¸2 , Î¸3 , Î¸4 } âˆª {sleep aid misuse, Â¬anxiety}
{Î¸1 , Î¸2 , Î¸3 , Î¸4 , Î¸5 } âˆª {Â¬sleep aid misuse, Â¬anxiety}
{Î¸1 , Î¸2 , Î¸3 }
{Î¸1 , Î¸2 , Î¸3 , Î¸5 } âˆª {Â¬sleep aid misuse}
{Î¸1 , Î¸2 , Î¸3 , Î¸4 } âˆª {Â¬anxiety}
{Î¸1 , Î¸2 , Î¸3 , Î¸4 , Î¸5 } âˆª {Â¬anxiety, Â¬sleep aid misuse}

Fig. 9 Atoms that are warranted in each possible EM world, given the AM and annotation
function in Figure 8.

Before introducing these formulas, we need to present a simple extension
to the concept of â€œwarrant statusâ€, which is up to now defined for literals.
The following definition is a simple extension to conjunctions or disjunctions
of literals.
Definition 12 (Warranting a Conjunction/Disjunction of Literals)
Let Î AM be a ground PreDeLP program and Q be either a conjunction or
disjunction of ground literals L1 , . . . , Ln . The warrant status of Q with respect to Î AM is defined as follows:
1. If Q is a single literal L, then the warrant status of Q is the warrant status
of L in Î AM .
2. If Q = Q1 âˆ§ Q2 then the warrant status of Q is:
â€“ Yes iff the warrant status of both Q1 and Q2 is Yes;
â€“ No if the warrant status of either Q1 or Q2 is No; and
â€“ Undecided whenever neither of the above cases hold.
3. If Q = Q1 âˆ¨ Q2 then the warrant status of Q is:
â€“ Yes iff the warrant status of either Q1 or Q2 is Yes;
â€“ No if the warrant status of both Q1 and Q2 is No; and
â€“ Undecided whenever neither of the above cases hold.
Using Definition 12, we can easily extend the nec and poss notations (cf.
Page 14) to conjunctions and disjunctions of literals.
The following result is a consequence of the fact that conflicting literals
cannot be warranted in (Pre)DeLP [17].
Proposition 1 Let Î AM be a ground PreDeLP program and Q = L1 âˆ§. . .âˆ§Ln
be a conjunction of ground literals. Then, only one of the following cases holds:
(i) P `war Q, (ii) P `war Â¬Q, or (iii) the warrant status of Q is undecided.

22

G.I. Simari, P. Shakarian, and M.A. Falappa

Warrant Probability Functions

Warrant Probability

1
0.9
0.8
0.7

Original Program

0.6
0.5

Prioritizing depression test
Prioritzing anxiety test

0.4
0.3
0.2
0.1
0

Potentially Warranted Atoms

Fig. 10 Histogram depiction of the entailment probability functions for the programs of
Example 9.

Proof In [17], a corresponding trichotomy result was shown for literals, i.e., the
warrant status for any literal is one and only one of Yes, No, and Undecided.
Our result is a direct consequence of this and Definition 12.

Warrant Probability Functions are then simply defined as partial mappings
with signature:
Î¥I : basicAM â†’ [0, 1]
P
such that for f âˆˆ basicAM , Î¥I (f ) = p if and only if Î»âˆˆnec(f ) Pr (Î») = p. 4
When the program is clear from context, we drop the subscript and write simply Î¥ . In the following, we use notation dom(Î¥ ) to denote the set of formulas
for which Î¥ is defined. The table shown in Example 9 is a simple example of a
WPF, whose domain is a handful of literals. The following is another example
along the same vein.
Example 10 Figure 10 shows three examples of WPFs in which the domains
are fixed to the set of literals that can be warranted in the input program.
These functions are related to the revision described in Example 9: the black
bars show the original probabilities, the striped bars give the probabilities
yielded by the program obtained by favoring the inclusion of the positive
depression test, while the light gray bars depict the probabilities obtained by
favoring the negative anxiety test. Figure 11 shows the three revised programs.


Revision Objective Functions. The other building block allows us to
effectively quantify how good a revision is considered to be. Towards this end,
4 Note that this definition can easily be extended to deal with probability intervals as well
(i.e., using both nec and poss); here, for simplicity of presentation, we adopt this definition
in order to work with point probabilities.

Quantitative Belief Revision in Structured Probabilistic Argumentation

23

we define revision objective functions (ROFs, for short) as functions that take
two DeLP3E programs I1 and I2 , along with an epistemic input (f, af ), and
returns a positive real number. We keep the definition of ROFs very general
in order to allow different kinds of objectives to be specified. The following
is a simple example of a ROF over our running example, which makes use of
warranting probability functions.
Example 11 Let us return once again to the medical diagnosis example. Suppose that we take the three revised programs we presented (Figure 11) â€“ call
them I1 , I2 , and I3 â€“ and that we wish to compare them with respect to the
effect of the last revision over the warranted atoms, taking the probabilities
yielded by I1 as the baseline. So, we define the following revision objective
function:
P
Î¨ (I, I 0 , (f, af 0 )) = eâˆ’ LâˆˆLAM ,L6=f |Î¥I (L)âˆ’Î¥I0 (L)|
where Î¥I is the WPF for program I.
Intuitively, this function sums up all the differences between the probabilities of literals entailed by the programs, but ignores the input (if it is a
literal). In this way, a distance between the original program and the two possible revisions is obtained based on the effects that each revision had on the
probabilities with which literals are derived. So, for our revisions, we get:
Î¨ (I1 , I2 , (pos dep test, af 2 )) â‰ˆ 0.0547
Î¨ (I1 , I3 , (pos dep test, af 3 )) â‰ˆ 0.8611
Therefore, we can conclude that the revision yielding I3 is preferred over the
one yielding I2 when this ROF is adopted.

Note that the function presented in Example 11 is just one possibility; the
framework is very flexible and allows the user to express many different functions, depending on the specific way in which they wish to express distances
between the original program and a given revised program.
3.2.2 The Class QAFO
Given the basic constructs introduced above, we can now define the class of
quantitative annotation function-based revision operators.
Definition 13 (The Class QAFO) Let I = (Î EM , Î AM , af ), with Î AM =
â„¦âˆªÎ˜âˆªâˆ† âˆª Î¦ be a DeLP3E program, ? âˆˆ AFO be an annotation function-based
belief revision operator, and Î¨ be a revision objective function. Operator ? is
said to be a quantitative af-based operator (denoted ? âˆˆ QAFO) if:
Given an epistemic input (f, af 0 ), we have that if I 0 = I ? (f, af 0 )
then there does not exist DeLP3E program I 00 = I â€¢ (f, af 0 ) such that
Î¨ (I, I 00 , (f, af 0 )) > Î¨ (I, I 0 , (f, af 0 )),
where â€¢ âˆˆ AFO is an arbitrary operator.

24

G.I. Simari, P. Shakarian, and M.A. Falappa
Analytical Model
Î˜:

Î¸1
Î¸2
Î¸3
Î¸4
Î¸5
Î¸6

=
=
=
=
=
=

â„¦:

Ï‰1 =
Ï‰2 =
Ï‰3 =
Ï‰4 =

Î¦:

âˆ…

âˆ†:

Î´1 =
Î´2 =
Î´3 =
Î´4 =

af 1

af 2

mem loss
short breath
fainting
neg anx test
neg tox screen
pos dep test

True
True
True
Â¬FN-anx test
Â¬FN-tox screen
True

True
True
True
False
af 1 (Î¸5 )
True

True
True
True
af 1 (Î¸4 )
af 1 (Î¸5 )
Â¬af 1 (Î¸4 )

af 3

Â¬sleep aid misuse â†
neg tox screen
Â¬anxiety â† neg anx test
anxiety â† depression
depression â† pos dep test

True
True
True
True

True
True
True
True

True
True
True
True

sleep aid misuse â€“â‰º
mem loss, depression
anxiety â€“â‰º short breath
depression â€“â‰º mem loss
anxiety â€“â‰º fainting

True
{anx risk}
{dep risk}
{anx risk}

True
af 1 (Î´2 )
af 1 (Î´3 )
af 1 (Î´4 )

True
af 1 (Î´2 )
af 1 (Î´3 )
af 1 (Î´4 )

Fig. 11 The DeLP3E program from the running example, after performing three revisions:
(i) The addition of the Î¸4 and Î¸5 , as discussed in Example 9; (ii) The revision by pos dep test
by prioritizing this input; and (iii) The same revision but prioritizing neg anx test.

So, this subclass of AFO simply takes a revision objective function and uses
it to obtain the best possible revised program. The following example, based
on our previous work on applications of DeLP3E to problems in the cyber
security domain [40], shows how QAFO operators can be applied to belief
revision problems in real-world scenarios other than the running example.
Example 12 Suppose we are modeling a cyber security scenario in which a
computer worm has been deployed and has infected millions of computers
worldwide â€“ by the time the worm is discovered, it is very difficult to reason
about the origin and even the intended target of the attack. In this kind
of situations, analysts are trying to solve the so-called attribution problem:
given a cyber operation of interest, determine the party that was ultimately
responsible for carrying it out [38].
Towards this end, we can model all knowledge available by means of a
DeLP3E program I = (Î EM , Î AM , af ), in which there is one distinguished
predicate condOp(A, O) in the AM that is intuitively read as â€œactor A conducted operation Oâ€. Furthermore, if we assume that only one actor is ever
responsible for an operation (an assumption that can easily be removed), we
have an integrity constraint of the form oneOf(C), where C is the set of all
ground atoms built with the condOp predicate.
Given this setup, we can define a WPF with a domain consisting of some
formulas of interest that reflect conditions that the analysts would like to remain relatively unaffected when incorporating new information. For instance,

Quantitative Belief Revision in Structured Probabilistic Argumentation

25

suppose we define:
n
dom(Î¥ ) = Â¬condOp(countryA, worm) âˆ§ Â¬condOp(countryB, worm)
o
condOp(countryD, worm) ,
denoting the fact that neither country A nor country B are responsible for
deploying the worm, and that country D is. If we pair this WPF with the
ROF from Example 11, the corresponding QAFO operator will prefer revisions
that do not affect the conclusions already reached regarding the probabilities
assigned to these statements.
In other words, this definition of dom(Î¥ ), with the ROF in question, causes
distances to be gauged relative to their effect on the probabilities assigned to
the suspicions that (i) neither country A nor country B carried out the attack,
and (ii) country D is behind the attack. Thus, such a setup causes the operator
to prefer revisions that keep the probabilities assigned to such suspicions as
close as possible to the ones yielded by the original program.

In the next section, we study the computational complexity associated with
this approach to belief revision in the DeLP3E setting.
4 Computational Complexity
In this section, we will focus on some of the computational aspects of quantitative af-based belief revision operations.
As a first observation, we have that the problem of deciding the warranting
status in a (classical) PreDeLP program has not yet been pinpointed. In [4],
the authors present a proof for the PSPACE-completeness of the problem of
marking a given dialectical tree; PSPACE membership for deciding the warrant
status is therefore a direct consequence of this result, since a dialectical tree
can be built within this budget. As a step towards finding a lower bound for
the complexity of the problem in general, we have the following.
Proposition 2 Let Î AM be a ground PreDeLP program and L be a ground
literal. Deciding Î AM `war L is NP-hard.
Proof By reduction from 3SAT-CNF; we therefore start with an input formula F with n variables X1 , . . . , Xn and m clauses C1 , . . . , Cm . We produce
a PreDeLP program Î AM with the following predicates: f , x1 , . . . , xn , and
c1 , . . . , cm . We then set:
Î¦ = {f, Â¬f, x1 , Â¬x1 , . . . , xn , Â¬xn }
âˆ† = {cj â€“â‰º xi | Xi = T makes clause Cj true} âˆª
{cj â€“â‰º Â¬xi | Xi = F makes clause Cj true} âˆª
{f â€“â‰º ci | for each clause Ci }
â„¦=Î˜=âˆ…

26

G.I. Simari, P. Shakarian, and M.A. Falappa

Next, we set the comparison criterion to specificity (the default in PreDeLP),
except for the following exceptions:
h{xi , cj â€“â‰º xi }, cj i is always preferred over h{Â¬xi , cj â€“â‰º Â¬xi }, cj i
h{Â¬f }, Â¬f i is preferred over h{f }, f i
Now, we must show that Î AM `war f if and only if there exists a satisfying
assumption for formula F . Suppose Î AM `war f ; the only way this can happen
is if argument h{Â¬f }, Â¬f i (the only counterargument to h{f }, i) is defeated,
which happens if and only if all arguments that defeat this argument are
in turn undefeated. Note that the only arguments capable of being in this
situation are the ones using the rules with cj in the head. Therefore, if all such
arguments are undefeated it must be the case that either xi or Â¬xi can be
chosen for each variable Xi in such a way that all clauses are satisfied, which
holds if and only if there exists a satisfying assumption for F .

As a corollary to Proposition 2, we have that deciding our extended notion
of warrant status remains within the same complexity bounds.
Corollary 1 Let Î AM be a ground PreDeLP program and Q be either a conjunction or disjunction of ground literals. Deciding Î AM `war Q is NP-hard
and in PSPACE.
Proof (Sketch) Applying Definition 12, the warrant status of Q can be determined in polynomial time based on the warrant status of its literals; therefore,
the same complexity results for determining the warrant status of a single
literal apply.

Assumption: Since, as stated above, the precise complexity of deciding the
warrant status of a literal in a PreDeLP program is not yet known, and with
the objective of separating the complexity of this problem from the complexity
of the problems inherent to quantitative belief revision in DeLP3E programs,
in the following we will make the assumption that classical warranting in PreDeLP is decidable in polynomial time. This is not an unreasonable assumption
if we consider the possibility of pre-compiling inferences [3] or having tractable
approximation algorithms to address the problem. We call this the polynomialtime warranting (PTW) assumption. Note that, even though this assumption
does not hold in general, it is a useful tool in the analysis of the complexity of
the problems studied here; it is also with this spirit that we make use of the
PTW assumption.
Unfortunately, our first result regarding the probabilistic extension of PreDeLP tells us that computing WPFs runs into a computational tractability
hurdle.
Theorem 1 Under the PTW assumption, computing the warrant probability
function for a DeLP3E program is #P-hard.

Quantitative Belief Revision in Structured Probabilistic Argumentation

27

Proof We will prove the statement by reduction from #3CNF-SAT. Given formula F in 3CNF with n variables and m clauses, generate a DeLP3E program
as follows: in the AM, there is one atom f , one atom ci for each clause in F
and two atoms for each variable V in, which we denote with posV and negV .
For each clause Ci in F of the form XÌ‚ âˆ¨ YÌ‚ âˆ¨ ZÌ‚, where VÌ‚ denotes either V or
Â¬V , we have strict rules:
ci â† xÌ‚
ci â† yÌ‚
ci â† zÌ‚
where vÌ‚ denotes posV if V is positive in the clause and negV if it is negative.
Next, we have the strict rule:
f â† c1 , . . . , c m
and facts posV and negV for each variable V in the input formula.
In the EM we have atoms event-posV and event-negV for each variable V
in F . The probability distribution assigns probability 0.5 to each individual
event, probability 0 to the conjunction of both events for the same variable,
and probability 1 to their disjunction.
Finally, the annotation function Î¸ assigns formula True to all components
of the AM, except the facts, for which we have af (posV ) = event-posV and
af (negV ) = event-negV .
Now we can see that, with this DeLP3E program, if the warranting probability function Î¥ assigns probability p to atom f , we have that:
P
Î»âˆˆnec(f ) Pr (Î») = p.
Clearly, from our construction we know that Î» âˆˆ nec(f ) iff Î» corresponds to a
satisfying assignment for formula F . Therefore, p = 2kn , where k is the number
of satisfying assignments for F . Solving for k, we have k = p Â· 2n .

The complexity class #P contains problems related to counting solutions (or,
in Turing machine terms, accepting paths) to problems in NP. The decision
version of this class is called PP, and contains problems decidable by a probabilistic Turing machine in polynomial time, with error probability less than a
certain proportion (say, 1/2). Unfortunately, Todaâ€™s theorem [47] tells us that
a polynomial-time Turing machine with either a PP or #P oracle can solve
all problems in the polynomial hierarchy.
Though it might be surmised that the #P-hardness is caused solely by the
computation of probabilities (as is the case in many probabilistic formalisms),
by analyzing the proof of Theorem 1 we can quickly arrive at the following
conclusion.
Observation 1 Computing the warrant probability function for a DeLP3E
program is #P-hard even in the special case in which probabilities associated
with EM worlds can be computed in PTIME.

28

G.I. Simari, P. Shakarian, and M.A. Falappa

Though this intractability holds in general, restricting the EM can soften
the impact on complexity. For instance, if we assume that Nilssonâ€™s probabilistic logic [33] is used then the complexity is lowered, as we show next; first, we
introduce a lemma that will be used in the proof of this result:
Lemma 1 ([6, 12]) If a system of m linear equalities and/or inequalities has
a nonnegative solution, then it has a nonnegative solution with at most m
positive variables.
This result was first introduced in [6], and later used in [12] to show that
deciding the validity of a formula in their logic is NP-complete. We can now
state our result.
Proposition 3 Under the PTW assumption, and assuming that Nilssonâ€™s
probabilistic logic is used in the EM, computing the warrant probability function
for a DeLP3E program is NP-complete.
Proof
Membership: Lemma 1 states that a solution to a linear program is guaranteed
to exist where only a number of the variables that is linear in the number of
constraints in the EM are set to a non-zero value. This implies the existence of
a number of EM worlds with non-zero probability that is linear in the number
of statements in the probabilistic model. A witness can therefore be verified
in polynomial time.
Hardness: NP-hardness is a consequence of the well-known fact that SAT is
reducible to computing probabilities in Nilsson logic (cf. [27] for a detailed
proof).

The previous result gives us a hint towards reaching the next one: if we combine the simplifying assumption that probabilities can be computed tractably
with the further assumption that the number of EM worlds that have nonzero probability is bounded by a polynomial (condition 1 below), then we are
guaranteed that computing WPFs is also tractable.
Corollary 2 Let I = (Î EM , Î AM , af ), with Î AM = â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦, be a
DeLP3E program. If we make the following assumptions:
1. |{Î» | Î» âˆˆ WEM and Pr(Î») > 0}| âˆˆ O(poly(n)), where n represents the size
of the input;
2. Pr(Î») can be computed in PTIME for any Î» âˆˆ WEM ; and
3. the PTW assumption holds,
then warrant probability functions for I can also be computed in PTIME.
Proof Direct consequence of the assumption that we have access to the polynomially many worlds that have non-zero probability. We thus simply keep an
accumulator for each element in the domain of the WPF and iterate through
the set of worlds, adding the probability of the world to each formulaâ€™s accumulator if and only if it is warranted in the AM induced by that world.


Quantitative Belief Revision in Structured Probabilistic Argumentation

29

Unfortunately, the following result states that even in this scenario we still
face an intractable hurdle when computing optimal revisions.
Theorem 2 Let I = (Î EM , Î AM , af ), with Î AM = â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦, be a
DeLP3E program, ? âˆˆ QAFO, and Î¨ be a revision objective function that can
be computed in polynomial time. If we have that:
1. |{Î» | Î» âˆˆ WEM and Pr(Î») > 0}| âˆˆ O(poly(n)), where n represents the size
of the input;
2. Pr(Î») can be computed in PTIME for any Î» âˆˆ WEM ; and
3. the PTW assumption holds,
then deciding if Î¨ (I, I ? (f, af 0 ), (f, af 0 )) â‰¤ k for some k âˆˆ R, is NP-complete.
Proof
Membership: Given I 0 , we can check in polynomial whether each world with
non-zero probability induces a maximal consistent subset of I in that world;
by construction, AFO operators are only constrained to make such changes in
worlds with probability greater than zero. Furthermore, since by hypothesis
we know that Î¨ can be computed in polynomial time, we can decide whether
or not the witness satisfies the constraints.
Hardness: By reduction from SUBSET-SUM; we therefore start from an instance of this problem, which consists of a set of n integers x1 , . . . , xn and
another integer c; the goal is to verify if there exists a subset of the numbers
that add up to c. Lets then create an instance of our problem, starting with
a DeLP3E program with one atom numi in the AM for each xi in the input
instance, as well as an auxiliary atom p; there will also be a corresponding
atom event-numi in the EM for each such atom. The probability distribution
associated with these random variables is set to an arbitrary one satisfying the
condition that only a number polynomial in n has non-zero probability.
Add as facts all atoms numi , with the annotation function defined as
af (numi ) = event-numi . Next, add the strict rule:
Ï‰ : Â¬numi â† p
such that af (Ï‰) = True. Set the epistemic input to fact Î¸in with af (Î¸in ) =
event-num1 âˆ¨ . . . âˆ¨ event-numn . Finally, set the function to optimize to:
ï£±
00
ï£´
i | numi âˆˆ Î AM , af (numi ) 6|= âŠ¥}
ï£²1 if res = {num
P
obj(I, I 0 ) =
is such that numi âˆˆres xi = c
ï£´
ï£³
0 otherwise.
That is, the objective function only considers the revised DeLP3E program, and
takes value 1 if and only if the atoms that belong to this program correspond
to numbers that, taken together, sum up to c. Thus, all we have to do is check
if the optimal revision yields a value of 1 for the objective function in order to
decide the given subset-sum instance.


30

G.I. Simari, P. Shakarian, and M.A. Falappa

The reader may note that the construction used in the proof of Theorem 2
uses a very powerful objective function that essentially encodes the NP-hard
problem; furthermore, this objective function is not based on WPFs. We now
provide an alternative result that proves NP-completeness under the same
conditions, but assumes that the objective function is simply the sum of the
probabilities assigned by the WPF to the set of ground atoms in the language
associated with the AM.
Theorem 3 Let I = (Î EM , Î AM , af ), with Î AM = â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦, be a
DeLP3E program, ? âˆˆ QAFO, and Î¨ be a revision objective function that can
be computed in polynomial time. If we have that:
1. |{Î» | Î» âˆˆ WEM and Pr(Î») > 0}| âˆˆ O(poly(n)), where n represents the size
of the input;
2. Pr(Î») can be computed in PTIME for any Î» âˆˆ WEM ; and
3. the PTW assumption holds,
then deciding if Î¨ (I, I ? (f,P
af 0 ), (f, af 0 )) â‰¤ k for some k âˆˆ R, is NP-complete
even when Î¨ is defined as aâˆˆGAM Î¥ (a).
Proof
Membership: As the ROF can still be computed in polynomial time, the membership result of Theorem 2 still holds.
Hardness: We show NP-hardness by reduction from SIMPLE MAX CUT
(SMC) [19]. The SMC problem takes as input an undirected graph G = (V, E)
and k â‰¥ 0, and decides if there exist sets V1 , V2 âŠ† V such that |{(u, v) âˆˆ E :
u âˆˆ V1 , v âˆˆ V2 }| â‰¥ k.
Let x be a the only atom in GEM . We will define a simple Î EM that sets the
probability of atom x to 1.0. We specify the atoms in GAM as follows: for each
vi âˆˆ V we create two atoms, set1 (vi ), set2 (vi ). For each edge (vi , vj ) âˆˆ E we
create atom edge(vi , vj ) (we assume that each bi-directional edge is specified
by one atom an that the order of the arguments for the edge predicate is
arbitrary but consistent). We will also have an additional atom query in GAM ,
which will also act as the formula for the epistemic input. We create Î AM
with the following elements:
â€“ For each vi âˆˆ V add the following strict rules:
â€“ set1 (vi ) â† query
â€“ Â¬set1 (vi ) â† query
â€“ set2 (vi ) â† Â¬set1 (vi )
â€“ For each edge (vi , vj ) âˆˆ E add strict rules edge(vi , vj ) â† set1 (vi ), set2 (vj )
and edge(vi , vj ) â† set2 (vi ), set1 (vj )
For each element y âˆˆ Î AM , we define the annotation function af (y) = True.
For the epistemic input, let af 0 be the extension of af such that af 0 (query) = x.
Finally, let the ROF be defined as in the statement of the theorem. Further,
for ease of notation, let af âˆ— be the annotation function returned after the belief
revision operation takes place.

Quantitative Belief Revision in Structured Probabilistic Argumentation

31

Clearly, this construction can be performed in polynomial time. Further,
note that the original program is consistent, since none of the rules in Î AM
ever fire since there are no facts.
Observations: We notice right away that any valid belief revision operator
must return an annotation function af âˆ— such that {x} |= af âˆ— (query) â€“ as this
atom is needed to ensure the objective function has a non-zero value (which is
clearly possible). Further, any optimal solution where af âˆ— (query) â‰¡ True can
be replaced with a solution where af âˆ— (query) = x. We also note that in any
optimal solution, the size of the set {setX (vi ) | Î¥ (setX (vi ) = 1.0} is equal to
|V | â€“ this is how we capture the notion that for each vertex vi , exactly one
of set1 (vi ), set2 (vi ) will be warranted under world {x}); hence, we capture the
requirement from the instance of SMC that the sets V1 , V2 is a partition of
V . We also note that the annotation function in the solution af âˆ— must only
modify the return values for strict rules of the form set1 (vi ) â† query and
Â¬set1 (vi ) â† query.
Claim 1: Let V1 , V2 be an optimal solution to an instance of SMC. Then,
the optimal solution to the corresponding revision problem has an objective
function whose value is greater than or equal to |{(u, v) âˆˆ E : u âˆˆ V1 , v âˆˆ
V2 }|+|V |+1. This can clearly be achieved by a solution where for each vi âˆˆ V1 ,
af âˆ— (set1 (vi ) â† query) = x and for each vi âˆˆ V2 , af âˆ— (Â¬set1 (vi ) â† query) = Â¬x.
Claim 2: Let R be the value of the objective function returned in an optimal
solution to the revision problem. Then, the number of edges returned in the
corresponding instance of SMC is greater than or equal to R âˆ’ |V | âˆ’ 1. By the
aforementioned observations, this claim is equivalent to saying that the number
of positive literals of the form edge(vi , vj ) that are warranted in the world {x}
is less than or equal to the number of edges returned in the optimal solution to
the corresponding instance of SMC. Suppose, by way of contradiction, that the
number of literals of that form that are warranted under {x} is greater than
the number of edges in the optimal solution to the corresponding instance of
SMC. By the construction, for each literal of the form edge(vi , vj ), exactly one
of the following pairs of literals must also be warranted: set1 (vi ), set2 (vj ) or
set2 (vi ), set1 (vj ). Therefore, we can partition the corresponding vertices from
SMC into two sets â€“ V10 , V20 and the number of edges in the set {(u, v) âˆˆ E :
u âˆˆ V10 , v âˆˆ V20 } must then be greater than the number of edges in the optimal
solution to the SMC problem. However, this is not possible, as this is also (by
construction) the same objective function that is optimized in that problem â€“
hence, we reach a contradiction.
The proof of hardness follows directly from Claims 1 and 2.



So, the proof of Theorem 3 illustrates that the quantified revision problem
is still NP-hard when the EM and the number of EM worlds, and (hence) the
computation of the WPF is not a source of complexity â€“ even when the ROF
used is a simple aggregate over WPFs of atoms. Further, as we can embed the
Simple Max Cut problem, the ROF â€“ even a simple sum over WPFs â€“ will not
necessarily be monotonic, even when using a revision operator that satisfies

32

G.I. Simari, P. Shakarian, and M.A. Falappa
Algorithm warrantFormula(F )
1. For each tree (V, E) âˆˆ F with (V, E)V= T âˆ— i (hA, Li) do
2.
For each v âˆˆ V with label(v) = fj âˆˆA0 af (fj ) do
3.
While |V | > 1 do
0 âˆˆ V | children(v 0 ) âŠ† leaves(V )} do
4.
For each v = hA0 , L0 i âˆˆ {vV
5.
label(v):= label(v) âˆ§ Â¬ v0 âˆˆchildren(v) Â¬label(v 0 );
6.
End for;
7.
V := V \ leaves(V );
8.
End while;

9.
fi := label root(T âˆ— i (hA, Li)) ;
10.
End for;
11. End for;W
12. Return i fi .

Fig. 12 An algorithm that takes a classical dialectical forest and computes a logical formula
specifying the possible worlds under which a given literal is warranted.

the Inclusion postulate (where the set of worlds satisfying af âˆ— (y) âŠ† af 0 (y)).
This also shows NP-completeness when the belief revision operator performs
modifications to Î AM (by removing elements, as discussed in [41]) as setting
af âˆ— (y) = Â¬x can be viewed as an operation that is equivalent to removing it
from Î AM .
We also note that the related problem of consolidation or contraction by
falsum, where we start with an inconsistent program and then must adjust
the annotation function to make it consistent, can also be shown to be NPcomplete by a simple modification to the proof: we fix the epistemic input to
True, and change the rules of the form set1 (vi ) â† query, Â¬set1 (vi ) â† query to
facts of the form set1 (vi ), Â¬set1 (vi ).

5 Warranting Formulas
We now focus on an algorithmic approach that can be used to compute approximate solutions and therefore address the computational intractability that we
have seen in the results above.
In the following, given a dialectical forest F(L) and a node
V V corresponding to argument A, we will use the notation label(V ) = câˆˆA af (c). For a
given probabilistic argumentation framework, literal, and dialectical tree, Algorithm warrantFormula in Figure 12 computes the formula describing the set
of possible worlds that are warranting scenarios for the literal. Intuitively, this
algorithm creates a formula for every dialectical tree in the forest associated
with an argument â€“ the algorithm iteratively builds a formula associated with
the environmental conditions under which the argument in the root of the
tree is undefeated. It then returns the disjunction of all such formulas in that
forest. We refer to this disjunction as the warranting formula for the literal.
The following result states the correctness of the warrantFormula algorithm.

Quantitative Belief Revision in Structured Probabilistic Argumentation

v1

v2

ï1

ï2

ï1 = Ã¡{q1, d1, d3}, LÃ±

v3

ï3

ï3 = Ã¡{q1, q6, w4, d1}, LÃ±

label(v1) = dep_risk ïƒ™

label(v3) = True ïƒ™

ïƒ˜(ïƒ˜FN_tox_screen)

ïƒ˜(ïƒ˜FN_tox_screen)

ï2 = Ã¡{q5, w3}, ïƒ˜LÃ±
label(v2) = (ïƒ˜FN_tox_screen)

v4

ï4

33

ï4 = Ã¡{q5, w3}, ïƒ˜LÃ±
label(v4) = (ïƒ˜FN_tox_screen)

Fig. 13 Dialectical forest for literal L = sleep aid misuse composed of trees T1 (left) and
T2 (right).

Proposition 4 Given forest F âˆ— (L),
n
o
Î» âˆˆ WEM | Î» |= warrantFormula F âˆ— (L)
n
o
poss(L) = Î» âˆˆ WEM | Î» 6|= warrantFormula F âˆ— (Â¬L) .
nec(L) =

Proof (Sketch)
Claim 1: nec(L) âŠ† {Î» âˆˆ WEM | (Î» |= warrantFormula(F âˆ— (L)))}. To prove
this claim, it suffices to show that if TÎ» hA, Li is a valid dialectical tree for L
then Î» |= warrantFormula(F âˆ— (L)). Suppose, BWOC, TÎ» hA, Li is such a tree
where Î» 6|= warrantFormula(F âˆ— (L)). We note that this tree shares a root and
is a subtree of a tree in F âˆ— (L). Also, for each node v in the tree, at line 2,
we have Î» |= label(v). Hence, we can continue the proof by replacing line 2
with âˆ€v, label(v) = f or(Î») and showing that warrantFormula(F âˆ— (L)) = f alse.
However, we can conclude that warrantFormula(F âˆ— (L)) = f or(Î») since the tree
has an odd depth by definition â€“ this is a contradiction.
Claim 2: nec(L) âŠ‡ {Î» âˆˆ WEM | (Î» |= warrantFormula(I, L))}. Suppose, by way
of contradiction, that the claim is false. Then there must exist a root-sharing
subtree of an element of F âˆ— (L) such that for each v, at step 4 Î» |= label(v)
and does not exist in Î». However, this is a contradiction by Definition 7.
Claim 3: The second part of the statement follows directly from Claims 1-2
and the fact that poss(L) = WEM \ nec(Â¬L).

Even though warranting formulas are another way of solving the problem of
computing probabilities exactly, our main motivation for developing it was to
explore options for pursuing tractable algorithms, as discussed next.
The following is an example of the warranting formula approach in the
setting of our running example.
Example 13 Consider the DeLP3E in our running example as shown in Figure 11 with annotation function af 1 . If we run Algorithm warrantFormula for
literal sleep aid misuse, we start with the dialectical forest shown in Figure 13.
Suppose that the algorithm begins with tree T1 (on the left); the only leaf
of this tree corresponds to vertex v2 for argument A2 , and its label remains

34

G.I. Simari, P. Shakarian, and M.A. Falappa

the conjunction of all annotations of elements in the argument â€“ label(v2 ) =
Â¬FN tox screen. The algorithm then moves to the next node up, which is
already the root, and updates the label by adding the conjunction with the
negation of its child, which yields:

label(v1 ) = dep risk âˆ§ Â¬ Â¬FN tox screen = dep risk âˆ§ FN tox screen.
Processing tree T2 similarly yields:

label(v3 ) = True âˆ§ Â¬ Â¬FN tox screen = FN tox screen.
Finally, the algorithm outputs the disjunction of these two formulas, which is
simply FN tox screen.

Outlook: Towards Tractable Computations
By applying the warrantFormula algorithm to the dialectical forest for a given
literal L, we can obtain the sets nec(L) and poss(L) with a running time proportional to the size of the forest and the annotation formulas â€“ though the
worst-time complexity has not been determined exactly, it is safe to conjecture
that the worst case is intractable. However, the warranting formula approach
opens the door to several possibilities for heuristics and approximate computations that either avoid exhaustively enumerating worlds in WEM or working
with full forests (or both). When combined with existing heuristics for classical
argumentation (the AM) and probabilistic models (the EM), this provides us
with a much more efficient way to compute warranting probability functions.
Experimental evaluations for such hypotheses are currently underway.
The use of the warranting formula approach can have several impacts in
the implementation of specific QAFO operators. First, warrant probability
functions Î¥ in this setting can now be redefined to map elements in their
domain to warranting formulas instead of probabilities as in their original
formulation. Revision objective functions now have at their disposal formulas instead of raw numbers. This opens up the possibility for specific implementations to leverage optimizations such as applying SAT algorithms to
decide whether Pr I1 (L) â‰¥ Pr I2 (L) (which can be decided via the SAT check
Î¥I1 (L) â‡’ Î¥I2 (L)). Such an approach is clearly compatible with heuristic optimizations that may, for instance, sacrifice precision for greater tractability.
An alternative class of operators can thus be defined based on the same
ideas as QAFO except that approximations are allowed instead of exact computations. There is much work to be done in this direction, which is outside
the scope of the current paper.
6 Related Work
This paper continues the research line that began with two works on belief
revision in structured probabilistic argumentation. In [39], we introduced the

Quantitative Belief Revision in Structured Probabilistic Argumentation

35

DeLP3E formalism (which is called P-PreDeLP in that work) and annotationfunction based belief revision (the class AFO), while in [40] we studied a special
case of entailment queries and showed how the framework can be applied to a
cyber-attribution problem.
As we have seen, the main problem studied in belief revision is the study
of how epistemic states should be changed in response to epistemic inputs.
Traditionally, epistemic states have taken the form of either belief sets (sets of
formulas closed under consequence) [1, 18] or belief bases [24, 23] (which are not
closed). Our goal is to ultimately apply our results to real-world domains, and
therefore we focus our attention on belief bases. Epistemic states in our case
consist of formulas over which argumentation-based reasoning is carried out
and to which we couple a general probabilistic model. The relationship between
argumentation and belief revision can be traced back to [10]; in this regard,
the work that is most closely related to how we approach their combination
is that of [14], where explanation-based revision operators are studied. For a
discussion on the relationship between the two areas of study, see [15] and [13].
Regarding argumentation systems that feature some quantitative form of
reasoning under uncertainty, we point out that the combination of probabilistic reasoning with argumentation systems has been the focus of several works
in the recent past. A significant portion of this work, however, has adopted abstract argumentation systems as the basis for the probabilistic extension [29,
45, 25, 16]. Contrary to structured argumentation systems like the one adopted
in this work, abstract argumentation is focused on the study of attacks among
arguments without inspecting their composition. There are also some works
that combine structured argumentation approaches with models for reasoning
under uncertainty â€“ the first of these was [22]; the work of [28], which was
developed even earlier, combines structured argumentation with abstract uncertainty measures but does not explicitly handle probability. Several other
works followed; for instance, in [5], the authors develop a possibilistic extension to DeLP, and [26] presents an approach based on probabilistic logic. The
main difference between these works and our own is that here we adopt a
bipartite knowledge base, where one part models the knowledge that is not
inherently probabilistic â€“ uncertain knowledge is modeled separately, thus allowing a clear separation of interests between the two kinds of models. This
kind of approach is not novel; it has been adopted in several frameworks, such
as the Independent Choice Logic [35] or probabilistic ontology languages for
the Semantic Web (see [20], and references within).
From a quantitative take on belief revision, which is the specific topic of
this paper, there hasnâ€™t been much work in the precise direction taken here.
Perhaps the earliest proposal with such an idea in mind is that of maxichoice
revision operators [2], which ensure that minimal changes are made to the
knowledge base when revisions are performed; in fact, our class of AFO operators (and therefore also QAFO) perform maxichoice revision operations
in each possible EM world. In the related setting of belief contraction operators, David Makinson [31] has defended the use of maxichoice operators,
explaining that some counterintuitive behaviors that this approach leads to is

36

G.I. Simari, P. Shakarian, and M.A. Falappa

due to its â€œmisapplicationâ€ to belief sets (which, we recall, are closed under
consequence). Another quantitative proposal is the one presented in [7], where
revisions are carried out according to a notion of distance between worlds, such
as the number of propositional symbols that separate one model from another.
These notions, however, do not correspond directly with the one adopted here,
since in our setting we are pursuing a revision that is optimal from the point
of view of its effect on the probabilistic aspect of the consequences of the
knowledge base, while the knowledge bases in [2] are non-probabilistic. Another interesting approach to belief revision from a quantitative standpoint
is ranking theory [43], which is a normative theory of the dynamics of belief.
Again, this approach is not directly related to the one taken here; however,
exploring how this well-studied approach can be applied in conjunction with
argumentation in the way that probability theory is applied in this work is an
interesting avenue for future work.
Along the same vein of seeking to minimize information loss, and in the
closely related setting of inconsistency management, the work of [21] proposes
the notion of preferred repair based on so-called â€œconsistency scoresâ€; our
quantitative approach to performing belief revision operations is loosely based
on this proposal. Also in the inconsistency management literature, the recent
work of [9, 8] proposes to resolve conflicts at a global level to minimize information loss but using incision functions instead. Another work in inconsistency
management is that of [46] proposes measures of inconsistency for probabilistic
logics. Apart from [21], this is perhaps the closest work in spirit to the one
presented here, though the underlying language used (probabilistic conditional
logic) is quite different and that work does not address the problem of belief
revision â€“ their measures, however, could be applied to efforts in line with our
own. The adaptation of measures of probabilistic inconsistency such as the
ones proposed in [46] to DeLP3E and their use in quantitative belief revision
operators is the topic of ongoing and future work.

7 Conclusions and Future Work
In this work, we tackled the problem of incorporating a new piece of information to an existing knowledge base; specifically, we adopted the DeLP3E
model, which is an extension of the structured argumentation language DeLP
with presumptions (PreDeLP) via the incorporation of annotations that refer
to events for which we have underlying probabilistic information.
The main focus of this paper was to further explore a class of belief revision operators called AFO (for annotation function-based revision) that we
proposed recently in [39, 41] by considering operators in this class that have the
further requirement of â€œquantitative optimalityâ€ â€“ this gave rise to the QAFO
class of operators. Though this optimality criterion was kept as general as possible so that knowledge engineers can specify their preferences, we explored
the computational complexity of the approach in general, arriving at a host of
results that range from intractability for the general case to polynomial-time

Quantitative Belief Revision in Structured Probabilistic Argumentation

37

special cases. finally, we presented an algorithm designed to compute the probability with which a literal is warranted via so-called warranting formulas, and
provide some initial discussion regarding how this approach could be applied
in the implementation of QAFO operators or approximations of them that
trade theoretical guarantees for tractability in practice.
Future work along this line of research involves continuing with efforts to
bridge the gap between the theoretical developments that have been steadily
coming from the belief revision community and practical implementations that
can be applied in real-world domains such as medical diagnosis (the topic of our
running example) and the related problem of solving the attribution problem in
cyber security and cyber warfare, as proposed in [40]. We are also investigating
the use of different kinds of belief revision operators, for instance ones that
are based on argumentation [14]. Finally, we are currently in the final stages
of developing a fully-functional implementation of DeLP3E; incorporating the
belief revision operators developed in [39, 41] and in this paper is the next step
in the implementation effort.
Acknowledgements This work was supported by UK EPSRC grant EP/J008346/1â€”
â€œPrOQAWâ€, ERC grant 246858â€”â€œDIADEMâ€, by NSF grant #1117761, by the Army Research Office under the Science of Security Lablet grant (SoSL) and project 2GDATXR042,
DARPA project R.0004972.001, and funds provided by CONICET and Universidad Nacional del Sur, Argentina. The opinions in this paper are those of the authors and do not
necessarily reflect the opinions of the funders, the U.S. Military Academy, or the U.S. Army.

References
1. AlchourroÌn, C.E., GaÌˆrdenfors, P., Makinson, D.: On the logic of theory change: Partial
meet contraction and revision functions. J. Sym. Log. 50(2), 510â€“530 (1985)
2. AlchourroÌn, C.E., Makinson, D.: On the logic of theory change: Contraction functions
and their associated revision functions. Theoria 48(1), 14â€“37 (1982)
3. Capobianco, M., ChesnÌƒevar, C.I., Simari, G.: Argumentation and the dynamics of warranted beliefs in changing environments. Intl. Journal on Autonomous Agents and
Multiagent Systems (JAAMAS) 11, 127â€“151 (2005)
4. Cecchi, L.A., Simari, G.R.: El marcado de un aÌrbol dialeÌctico en DeLP es PSPACEcompleto. In: Proc. of Congreso Argentino de Ciencias de la ComputacioÌn (CACIC)
(2011)
5. ChesnÌƒevar, C.I., Simari, G.R., Alsinet, T., Godo, L.: A logic programming framework
for possibilistic argumentation with vague knowledge. In: Proc. of UAI 2004, pp. 76â€“84
(2004)
6. ChvaÌtal, V.: Linear Programming. W.H.Freeman, New York (1983)
7. Dalal, M.: Investigations into a theory of knowledge base revision: Preliminary report.
In: Proc. of AAAI, pp. 475â€“479 (1988)
8. Deagustini, C.A.D., Martinez, M.V., Falappa, M.A., Simari, G.R.: Improving inconsistency resolution by considering global conflicts. In: Proc. of SUM. Springer (2014, To
Appear)
9. Deagustini, C.A.D., Martinez, M.V., Falappa, M.A., Simari, G.R.: Inconsistency resolution and global conflicts. In: Proc. of ECAI (2014, To Appear)
10. Doyle, J.: A truth maintenance system. Artif. Intell. 12(3), 231â€“272 (1979)
11. Dung, P.M.: On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artif. Intell. 77, pp. 321â€“357
(1995)

38

G.I. Simari, P. Shakarian, and M.A. Falappa

12. Fagin, R., Halpern, J.Y., Megiddo, N.: A logic for reasoning about probabilities. Information and Computation 87(1/2), 78â€“128 (1990)
13. Falappa, M.A., Garcia, A.J., Kern-Isberner, G., Simari, G.R.: On the evolving relation
between belief revision and argumentation. The Knowledge Engineering Review 26(01),
35â€“43 (2011)
14. Falappa, M.A., Kern-Isberner, G., Simari, G.R.: Explanations, belief revision and defeasible reasoning. Artif. Intell. 141(1/2), 1â€“28 (2002)
15. Falappa, M.A., Kern-Isberner, G., Simari, G.R.: Belief revision and argumentation theory. In: Argumentation in artificial intelligence, pp. 341â€“360. Springer (2009)
16. Fazzinga, B., Flesca, S., Parisi, F.: On the complexity of probabilistic abstract argumentation. In: Proc. of IJCAI 2013 (2013)
17. GarcÄ±Ìa, A.J., Simari, G.R.: Defeasible logic programming: An argumentative approach.
TPLP 4(1-2), 95â€“138 (2004)
18. Gardenfors, P.: Knowledge in flux: modeling the dynamics of epistemic states. MIT
Press, Cambridge, Mass. (1988)
19. Garey, M., Johnson, D.: Computers and Intractability: A Guide to the Theory of NPCompleteness. Freeman, New York (1979)
20. Gottlob, G., Lukasiewicz, T., Martinez, M.V., Simari, G.I.: Query answering under
probabilistic uncertainty in Datalog+/â€“ ontologies. AMAI (2013)
21. Gottlob, G., Lukasiewicz, T., Martinez, M.V., Simari, G.I.: Query answering under
probabilistic uncertainty in Datalog+/- ontologies. AMAI (2013)
22. Haenni, R., Kohlas, J., Lehmann, N.: Probabilistic argumentation systems. Springer
(1999)
23. Hansson, S.: Semi-revision. J. of App. Non-Classical Logics 7(1-2), 151â€“175 (1997)
24. Hansson, S.O.: Kernel contraction. J. Symb. Log. 59(3), 845â€“859 (1994)
25. Hunter, A.: Some foundations for probabilistic abstract argumentation. In: Proc. of
COMMA 2012, pp. 117â€“128 (2012)
26. Hunter, A.: A probabilistic approach to modelling uncertain logical arguments. Int. J.
Approx. Reasoning 54(1), 47â€“81 (2013)
27. Khuller, S., Martinez, M.V., Nau, D.S., Sliva, A., Simari, G.I., Subrahmanian, V.S.:
Computing most probable worlds of action probabilistic logic programs: scalable estimation for 1030,000 worlds. AMAI 51(2-4), 295â€“331 (2007)
28. Krause, P., Ambler, S., Elvang-GÃ¸rannson, M., Fox, J.: A logic of argumentation for
reasoning under uncertainty. Computational Intelligence 11 (1), 113â€“131 (1995)
29. Li, H., Oren, N., Norman, T.J.: Probabilistic argumentation frameworks. In: Proc. of
TAFA, pp. 1â€“16 (2011)
30. Lloyd, J.W.: Foundations of Logic Programming, 2nd Edition. Springer (1987)
31. Makinson, D.: On the status of the postulate of recovery in the logic of theory change.
Journal of Philosophical Logic 16(4), 383â€“394 (1987)
32. Martinez, M.V., GarcÄ±Ìa, A.J., Simari, G.R.: On the use of presumptions in structured
defeasible reasoning. In: Proc. of COMMA, pp. 185â€“196 (2012)
33. Nilsson, N.J.: Probabilistic logic. Artif. Intell. 28(1), 71â€“87 (1986)
34. Pearl, J.: Probabilistic reasoning in intelligent systems: networks of plausible inference
(1988)
35. Poole, D.: The independent choice logic for modelling multiple agents under uncertainty.
Artif. Intell. 94(1-2), 7â€“56 (1997)
36. Rahwan, I., Simari, G.R.: Argumentation in Artificial Intelligence. Springer (2009)
37. Richardson, M., Domingos, P.: Markov logic networks. Machine Learning 62, 107â€“136
(2006)
38. Shakarian, P., Shakarian, J., Ruef, A.: Introduction to Cyber-Warfare: A Multidisciplinary Approach. Syngress (2013)
39. Shakarian, P., Simari, G.I., Falappa, M.A.: Belief revision in structured probabilistic
argumentation. In: Proc. of FoIKS 2014, pp. 324â€“343
40. Shakarian, P., Simari, G.I., Moores, G., Parsons, S., Falappa, M.A.: An argumentationbased framework to address the attribution problem in cyber-warfare. In: Proc. of Cyber
Security 2014 (2014)

Quantitative Belief Revision in Structured Probabilistic Argumentation

39

41. Shakarian, P., Simari, G.I., Moores, G., Paulo, D., Parsons, S., Falappa, M.A., Aleali,
A.: Belief revision in structured probabilistic argumentation: Model and application to
cyber security. Under review â€“ available at:
http://www.delp3e.webs.com/Shakarian-etal-DeLP3E.pdf (2014)
42. Simari, G.R., Loui, R.P.: A mathematical treatment of defeasible reasoning and its
implementation. Artif. Intell. 53(2-3), 125â€“157 (1992)
43. Spohn, W.: The laws of belief: Ranking theory and its philosophical applications. Oxford
University Press (2012)
44. Stolzenburg, F., GarcÄ±Ìa, A., ChesnÌƒevar, C.I., Simari, G.R.: Computing Generalized
Specificity. Journal of Non-Classical Logics 13(1), 87â€“113 (2003)
45. Thimm, M.: A probabilistic semantics for abstract argumentation. In: Proc. of ECAI
2012, pp. 750â€“755 (2012)
46. Thimm, M.: Inconsistency measures for probabilistic logics. Artif. Intell. 197, 1â€“24
(2013)
47. Toda, S.: On the computational power of PP and âŠ•P. In: Proc. of FOCS, pp. 514â€“519
(1989)
48. Wirth, C., Stolzenburg, F.: David Pooleâ€™s specificity revised. In: Proc. of KR (2014)

Early Identification of Violent Criminal Gang Members
Elham Shaabani, Ashkan Aleali , and Paulo Shakarian
Arizona State University Tempe, AZ

John Bertetto
Chicago Police Department Chicago, IL

john.bertetto@chicagopolice.org

{shaabani, aleali, shak}@asu.edu

arXiv:1508.03965v1 [cs.SI] 17 Aug 2015

ABSTRACT
Gang violence is a major problem in the United States accounting for a large fraction of homicides and other violent crime. In this paper, we study the problem of early identification of violent gang members. Our approach relies on modified centrality measures that take into account additional data of the individuals in the social network of co-arrestees which together with other arrest metadata provide a rich set of features for a classification algorithm. We show our approach obtains high precision and recall (0.89 and 0.78 respectively) in the case where the entire network is known and out-performs current approaches used by lawenforcement to the problem in the case where the network is discovered overtime by virtue of new arrests - mimicking real-world law-enforcement operations. Operational issues are also discussed as we are preparing to leverage this method in an operational environment. Categories and Subject Descriptors: J.4 [Computer Applications]: Sociology General Terms: Security; Experimentation Keywords: Social Network Analysis; Criminology

a classifier to identify potentially violent individuals. We note that this classification problem is particularly difficult due to not only data imbalances, but also due to the fact that many violent crimes are conducted due to heightened emotions - and hence difficult to identify. Though we augment our network-based features with some additional meta-data from the arrest records, our approach does not leverage features concerning the race, ethnicity, or gender of individuals in the social network. We evaluate our method using realworld offender data from the Chicago Police Department. This paper makes the following contributions: · We discuss how centrality measurements such as degree, closeness, and betweenness when modified to account for metadata about past offenses such as the type of offense and whether the offense was classified as "violent" can serve as robust features for identifying violent offenders. · We show how the network features, combined with other feature categories provide surprisingly robust performance when the entire offender is known in terms of both precision (0.89) and recall (0.78) using cross-fold validation. · We then test our methods in the case where the network is exposed over time (by virtue of new arrests) which mimics an operational situation. Though precision and recall are reduced in this case, we show that our method significantly outperforms the baseline approach currently in use by law-enforcement - on average increasing precision and recall by more than two and three times respectively. In addition to these main results, we also present some side results on the structure and nature of the police dataset we examine. The paper is organized as follows. In Section 2 we motivate this difficult problem within the law-enforcement community. This is followed by a description of our dataset along with technical notation in Section 3. There, we also describe some interesting aspects of the gang arrest dataset and our co-arrestee network. In Section 4 we formally define our problem, describe existing approaches, and then describe the features we use in our approach. Then we present our results in Section 5 for both cases where we assume the underlying network is known and when we discover the network over time (mimicking an operational scenario). Finally, related work is discussed in Section 6.

1.

INTRODUCTION

Gang violence is a major problem in the United States [1, 2] - accounting for 20 to 50 percent of homicides in many major cities [10]. Yet, law enforcement actually has existing data on many of these groups. For example the underlying social network structure is often recorded by lawenforcement and has previously been shown useful in enabling "smart policing" tactics [17] and improving law -enforcement's understanding of a gang's organizational structure [19]. In this paper we look to leverage this gang social network information to create features that allows us to classify individuals as potentially violent. While the results of such a classifier are insufficient to lead to arrests, it is able to provide the police leads to individuals who are likely to be involved in violence, allowing for a more focused policing with respect to patrols and intelligence gathering. Our key aim is to significantly reduce the population of potential violent gang members which will lead to more efficient policing. In this paper, we introduce our method for identifying potentially violent gang members that leverages features derived from the co-arestee social network of criminal gangs in
These authors contributed equally to this work. U.S. Provisional Patent 62/191,086. Contact shak@asu.edu for licensing information.
 

2.

BACKGROUND

A recent study shows that the network for gunshot victimization is denser than previously believed [16]. According to the authors, within the city of Chicago over 70% of all gunshot victims are contained within only 6% of the total population These findings validate what has been considered common knowledge among police for decades: who you hang out with matters, and if you hang out with those who engage in or are victims of violence you are more likely to become an offender or victim yourself. Identifying potential offenders of gun violence has also been a staple practice for most law enforcement agencies as an attempt to curtail future victimization. When gang conflicts get "hot," it's common for law enforcement agents to put together a list of known "shooters": those known gang members with an existing criminal history for gun violence and a predilection for engaging in such illegal activity. Law enforcement agents then attempt to make contact with these individuals with the expectation that such direct contact might prevent violence. For most law enforcement agencies, however, this practice is performed in a very adhoc manner. Identifying these individuals for intervention has relied primarily on the ability of law enforcement agents to remember and identify at-risk individuals. While feasible for small or discreet networks, the ability to recall multiple individuals in large networks that cross large geographic regions and interact with multiple networks becomes increasing difficult. This difficulty increases significantly as relationships between networks change, known individuals leave the network, and new individuals enter it. In particular, the practice is less than idea because it requires officers to attempt to recall criminal history and network association data that varies between network members. For example, a subject who has been arrested on multiple occasions for carrying a gun or has been arrested for shooting another individual is easy to recall, but recalling and quantifying the risk for a subject with multiple arrests for non-gun violence and a direct association with several offenders and victims of gun violence can be much more difficult. In short, identifying a known "shooter" is relatively straightforward: they are known. The approach in this paper synthesizes network connectivity other attributes of the subject to identify those individuals at risk that law enforcement might not yet know. Using this information, law enforcement agents may not only more reliably and consistently identify those individuals most likely to engage in acts of violence or become victims of violence due to their personal associations with it, but also to more effectively manage agency resources. Intervention strategies may include service providers outside law enforcement, such as family members, social service providers, current or former educators, and clergy. This diversity in approach not only delivers a more powerful "stop the violence" message but provides a kind of force multiplier for law enforcement, increasing the number of persons involved in the effort to prevent violence. Identifying specific individuals for intervention also allows for a more targeted effort by law enforcement in terms of personnel and geographic areas needing coverage. Blanketing violence reduction strategies that saturate geographic areas with law enforcement agents and rely on direct contact with large numbers of criminal network members are inefficient and resource consuming. Focusing efforts on those individuals most likely to engage in

violence allows law enforcement to focus on smaller groups of people and smaller geographic areas (those areas within which those individuals identified are known to frequent). Therefore, our approach can significantly improve such efforts to identify violent individuals. In this paper, we see how our method not only out-performs the current social network heuristic used by police, but also that it provides a much smaller and more precise list of potentially violent offenders than simply listing those with a violent criminal record.

3.

GANG CO-OFFENDER NETWORK

In this section, we introduce the necessary basic notation to describe our co-offender network and then provide details of our real-world criminal dataset and study some of its properties.

3.1

Technical Preliminaries

Throughout this paper we shall represent an offender network as an undirected graph G = (V, E ) where the nodes correspond with previous offenders and an undirected edge exists between offenders if they were arrested together. We will use  to denote the set of timepoints (dates). We also have three sets of labels for the nodes: V , S , gang which are the sets of violent crimes, non violent crimes, and gangs. For each time point t and each node v , the binary variable arrt v  {true, false} denotes if v was arrested at time t and t t distrt v , beatv , gangv to denote the district, beat, and gang affiliation of v at time t (we will assume that time is fine-grain enough to ensure that at each time unit an individual is arrested no more than once). If we drop the t superscript for these three symbols, it will denote the most recent district, beat, and gang associated with v in the knowledgebase. We t t shall use the sets Vv and Sv to denote the set of violent and non violent offenses committed by v at time t respectively. t Note if arrt v = false then Vv = . We will drop the superscript t for this symbol to denote the union of labels at any time t in the historical knowledgebase. We also note that the edges in the graph also depend on time, but for sake of readability, we shall state with words the duration of time considered for the edges. For a given violent crime c  VS , we will use the notation t Vct = {v  V s.t. c  Vv } (intuitively, the subset of the population who have committed crime c at time t). Again, we will drop the superscript t if v could have committed crime c at any time in the historical knowledgebase. For a set of labels C  V  S , we will extend this notation: t t VC = {v  V s.t. C  Vv = }. We will slightly abuse notation here: Vt = V . We will use similar notation for denoting a subset of the population that are members of a certain gang. For instance, Vgangv refers to the set of nodes who are in the same gang as node v . Likewise, we shall use the same notation for subgraphs: Gt C is the subgraph of G t containing only nodes in VC and their adjacent edges. We will use the function d : V × V  N to denote the distance between two nodes - which for this paper will be the number of links in the shortest path. For a given node v , the set i Nv = {v  V s.t. d(v, v ) = i} ­ the set of nodes that are whose shortest path is exactly i hops from v . For two nodes v, v , we will use the notation  (v, v ) to be the number of shortest paths between v and v . For nodes u, v, v , u (v, v ) will be the number of shortest paths between v and v that pass through u.

Table 1: Summary of arrest data. Name Value Number of records 64466 Violent offense 4450 Homicide 312 Criminal sexual assault 153 Robbery 1959 Aggravated assault 1441 Aggravated battery 896 Non violent offense 60016

Table 2: Network properties. Name Values Vertices 9373 Edges 17197 Average degree 3.66 Average clustering 0.5 Transitivity 0.62 Connected components 1843 Largest connected component di36 ameter Largest connected component aver12.22 age path length Largest connected component aver0.63 age clustering

Figure 1: The gang co-offender network. Each color corresponds with a different gang. isolated vertices are eliminated due to the lack of structural information. A visualization of the network is depicted in Figure 1 and we have included summary statistics in Table 2. In studying this network, we studied its degree distribution (Figure 2). Unlike the degree distribution for other scale free social networks, the degree distribution for the offender network is exponential rather than power law. However, despite the degree distribution being similar to that of a random (ER) or small world network topology [27], we noticed other characteristics that indicate differently. The co-offender network has a much higher average clustering coefficient than in a random network and does not follow the properties of the small world topology due to the relative high diameter and average shortest path (computed for the largest connected component.)

For a given subgraph G of G, we shall use C(G ) to denote the largest connected component of G and for node v  G , we will use the notation Cv (G ) to denote the connected component of G to which v belongs. If we apply a community finding algorithm to subgraph G , we will use the notation Pv (G ) to denote the partition of G to which v belongs. We will use the notation |·| to denote the size of a set or the number of nodes in a subgraph.

3.2

Overview of Network Data

Number  of  Ver+ces(Hundreds  )  

In this section we describe our police dataset and the associated co-offender network as well as some interesting characteristics that we have noticed. Police Dataset. Our dataset consists of gang-related arrest incidents gathered from August 2011 - August 2014 in Chicago as well as their immediate associates. This data set includes locations, dates, the links between the joint arrests, and the gang affiliation of the offenders. In Table 1, we summarize some of the important characteristics of the dataset. Violent Crimes. In our dataset, the set V consists of the following crimes have been identified by the Chicago Police as violent crimes: homicide (first or second degree murder), criminal sexual assault, robbery, aggravated assault, and aggravated battery. All aforementioned offenses are also FBI "index" crimes as well. A key aspect about the violent crimes is that the dataset is highly imbalanced with much more arrests for non violent crimes vs. arrests for violent crimes (60016 vs. 4450). Network Properties. From the arrest data, we were able to construct the co-offender network. In this network, the

40   35   30   25   20   15   10   5   0   1   3   5   7   9   11   13   15   17   19   21   23   25   27   29   31   33   35   37   39   41   43   45   47   49  

Degree  

Figure 2: Network degree distribution. The exponential function fits to the distribution (R2 = 0.77). Repeat Offenders. There are many instances of repeated offenses from the same offender. Figure 3 shows the distribution of the repeated arrests for each individual in the dataset. This indicates that arrest records have utility in

identifying future offenders.

Number  of  individuals(Thousands  )  

6   5   4   3   2   1   0   2   4   6   8   10   12   14   16   18   20   22   24   26   28   31   38  

In this section, we describe our problem, some of the existing practical approaches used by law-enforcement, and our approach based on supervised learning with features primarily generated by the network topology.

4.1

Problem Statement

Given a co-offender network, G = (V, E ) and for each historical timepoint t   = {1, . . . , tmax } and v  V , we t t have the values of arrt v , distrv , beatv and elements of the t t sets Vv , gangv , we wish to identify set {v  V s.t. t > t tmax where |Vv |> 0}. In other words, we wish to find a set of offenders in our current co-offender network that commit a violent crime in the future.

4.2
Number  of  arrests  

Existing Methods

Figure 3: Repeated arrests. 12866 instances of one-time arrests have been removed. Seasonality of Crime. There is also a higher chance of criminal activities in different months of the year. Figure 4 demonstrates some of these variations. As per police observations, both violent and non-violent crime incidents are lower in the winter months (Dec.-Feb.).
180  

Here we describe two common techniques often used by law-enforcement to predict violent offenders. The first is a simple heuristic based on violent activities in the past. The second is a heuristic that was based on the findings of [17] which was designed to locate future victims of violent crime. Both of these approaches are ad-hoc practical approaches that have become "best practices" for predicting violent offenders. However, we are not aware of any datadriven, formal evaluation of these methods in the literature. Past Violent Activities (PVA). The first ad-hoc approach is quite simple: if an offender has committed a violent crime in the past, we claim that he will commit a violent crime in the future. An obvious variant of this approach is to return the set of violent offenders from the last t days. We note in practice, if police also have records of those who are incarcerated, and such individuals would be removed from the list (due to the different jurisdictions of police and corrections in the Chicago area, we did not have access to incarceration data - however discussed re-arrests observed in the data in the previous section). Two-Hop Heuristic (THH). The two-hop heuristic is based on the result of [17] which investigated a social network of gunshot victims in Boston and found an inverse relationship between the probability of being a gunshot victim and the shortest path distance on the network to the nearest previous gunshot victim. Hence, THH returns all neighbors one and two hops away from previous violent criminals (see Algorithm 1 for details on the version we used in our experiments - which was the best-performing variant for our data). The Chicago Police have adopted a variant of this method to identify potential gang victims using a combination of arrest and victim data - the co-arrestee network of criminal gang members includes many individuals who are also victims of violent crime (this is a direct result of gang conflict). We note that victim information did not offer a significant improvement to our approach, except the trivial case that a homicide victim cannot commit any crime in the future.

Number  of  violent  individuals  

160   140   120   100   80   60   40   20   0   Aug   Sep   Oct   Nov   Dec   Jan   Feb   Mar   Apr   May   Jun   Jul  

Month   Number  of  non  violent  individuals  
2500   2000   1500   1000   500   0   Aug   Sep   Oct   Nov   Dec   Jan   Feb   Mar   Apr   May   Jun   Jul  

Month   2011-2012   2012-2013   2013-2014  

4.3

Supervised Learning Approach

Figure 4: Seasonality of crime.

4.

IDENTIFYING VIOLENT OFFENDERS

We evaluated many different supervised learning approaches including Naive Bayes (NB), Linear Regression (LR), Decision Tree (DT), Random Forest (RF), Neural Network (NN), and support vector machines (SVM) on the same set of features for the nodes in the network that we shall describe in this section. We also explored combining these approaches

Algorithm 1 Two-Hop Heuristic 1: procedure TwoHop(G) Offender network G. 2: R  {} Identified violent offenders. 3: V ICT IM S  {u  G|is homicide victim(u)} 4: for v  V ICT IM S do 1 2 5: N  Nv  Nv Immediate neighbors 6: R  R  {u  N s.t. Vu = } 7: return R

Table 3: Neighborhood-Based Features Description Degree (w.r.t. C ) Fraction of 1-hop neighbors committing a crime in C Fraction of 2-hop neighbors committing a crime in C Majority of 1-hop and 2-hop neighbors committing a crime in C Minority of 1-hop and majority of 2-hop neighbors comitting a crime in C Definition
1 |{u|u  Nv  VC }| 1 1 |{u|u  Nv  VC }|/|Nv |

with techniques for imbalanced data such as SMOTE [4] and Borderline SMOTE [9], however we do not report the results of Borderline SMOTE as it provided no significant difference from SMOTE. We group our features into four categories: (1.) neighborhood-based (having to do with the immediate neighbors of a given node), (2.) network-based (features that require the consideration of more than a nodes immediate and nearby neighbors), (3.) temporal characteristics, and (4.) geographic characteristics.

2 2 |{u|u  Nv  VC }|/|Nv |

majv (C, 1)  majv (C, 2)

¬majv (C, 1)  majv (C, 2)

4.3.1

Neighborhood-Based Features

Neighborhood-based features are the features computed using each node and its first and/or second level neighbors in G ­ often with respect to some C  V . The simplest such measure is the degree of vertex v ­ corresponding to the number of offenders arrested with v . We can easily extend this for some set of crimes of interest (C ) where we look at all the neighbors of v who have committed a crime in C . This generalizes degree (as that is the case where C = ). In our experiments, we found the most useful neighborhood features to be in the case where C = V though standard degree (C = ) was also used. We also found that using combinations of the following booleans based on the below definition also proved to be useful: majv (C, i)
i i = |{u|u  (i Nv )  VC }| 0.5 × |(i Nv )|

Table 4: Network-Based Features (Community) Description Component size when v is removed Largest component size with a violent node after v is removed Group size Relationships within the group Number of violent members in the group Triangles group Transitivity group Group-to-group connections Gang-to-gang connections in of Definition |C(Cv (G) \ {v })|

maxv C(Cv (G){v}VV |Xv | where Xv = Cv (Cv (G){v })

Intuitively, majv (C, i) is true if at least half of the nodes within a network distance of i from node v have committed a crime in C and false otherwise. Using these intuitions, we explored the space of variants of these neighborhood-based features and list those we found to be best-performing in Table 3.

|Pv (Ggangv )| |{(u, v )  Pv (Ggangv )}| E s.t. u, v 

|{v  Pv (Ggangv ) s.t. Vv = }|

4.3.2

Network-Based Features

Network-based features fall into two sub-categories that we shall describe in this section: community-based and pathbased. Network-based community features. We use several notions of a node's community when engineering features: the connected component to which a node belongs, the gang to which a node belongs, and what we will refer to as an individual's group. The connected component is simply based on the overall network structure, while the gang is simply the subgraph induced by the individuals in the network who belong to the same gang (the social network of node v 's gang is denoted Ggangv . A nodes group is defined as the partition he/she belongs to based on a partition of Ggangv found using the Louvain algorithm [5]. We found in our previous work [19] and ensuing experience with the Chicago Police that the groups produced in this method

No. of triangles within subgraph Pv (Ggangv ) No. of triangles in Pv (Ggangv ) No. of ""'s in Pv (Ggangv ) |{u  Pv (Ggangv ) s.t. (u, w)  E where w  / Pv (Ggangv )}| |{u  Ggangv s.t. (u, w)  E where w  / Ggangv }|

were highly relevant operationally. In this work, we also examined other community finding methods (i.e. Infomap, and Spectral Clustering ) and found we obtained the best results by using the Louvain algorithm. We provide our best performing network-based community features that we used in Table 4. Of particular interest, we found for individual

Table 5: Network-Based Features (Path) Description Betweenness (w.r.t. C ) Closeness (w.r.t. C) Shell Number (w.r.t. C ) Propagation (w.r.t. C ) Definition
v (u,w) u,wVC  (u,w)

Table 6: Geographic Features Name District quency FreDefinition
t |{(t, v ) s.t. arrv = true  t t t s.t. distrv = distrv }| t |{(t, v ) s.t. arrv = true  t t s.t. beatv = beatt v }| t t |{(t, v ) s.t. arrv = true  Vv = t t   t s.t. beatv = beatv }| t t |{(t, v ) s.t. arrv = true  Vv = t t   t s.t. distrv = distrv }|

(|VC |-1)/

uVC

d(u, v )

Beat Frequency Beat Violence District Violence

shellC (v ) (see appendix for further details) 1 if v   (VV ), 0 otherwise. (see appendix for further details)

v that features relating to the size of the largest connected component resulting v removal of his/her connected component was useful. Another interesting pair of features we noted for both group and gang were the number of edges from members of that group/gang to a different group or gang. We hypothesize that the utility of these features is a result of conflicts between groups/gangs they are connected to as well as the spread of violence amongst different groups (i.e. if two groups are closely connected, one may conduct violent activities on behalf of the other). Network-based path features. We looked at several features that leveraged the paths in the network by adopting three common node metrics from the literature: betweenness, closeness [6], and shell-number [23] as well as a propagation process based on a deterministic tipping model [8]. The features are listed in Table 5. We examined our modified definitions of closeness, betweenness, and shell number where C was a single element of V , where C = V and where C =  (which provides the standard definitions of these measures). Our intuition was that individuals nearer in the network to other violent individuals would also tend to be more violent - and we found several interesting relationships such as that for closeness (where C = VV ) discussed in section 5.1 when we run the classifier on each feature group. Shell number and the propagation process were used to capture the idea of the spread of violence (as shell number was previously shown to correspond with "spreaders" in various network epidemic models [11]). For the propagation process, we set the threshold () equal to two, three, four, five, and six. Further details on shell number and the propogation process can be found in the appendix.

val month and number of violent groups. Average interval time considers the average time duration of consecutive arrests of the offender. The other feature, which we examine, is number of violent groups appeared over time in the environment. We examined that the number of violent groups has been an important temporal aspect for identifying the violent criminals. The key intuition here is, if at least one member of the offender's groups (formed over time) is violent then we consider the offender as a part of that violent group. For an individual v , we define the partially ordered t t set tv C = {t s.t. arrv = true  VC = } (intuitively the set of the time points where v has committed at least on of the v v crimes in C .) We also define v i (C ) = ti - ti-1 for each v . Considering these definitions, we formally define tv  t i C the temporal features in Table 7. Table 7: Temporal Features Name Average interval time (w.r.t. C ) Number of violent groups Definition
i v v i ( C ) / |t C |

t |{t s.t. arrv = true  t = true  v s.t. arrv t Vv = t v  Nv }|

5.

EXPERIMENTAL RESULTS

4.3.3

Geographic Features

Geographic features capture the information related to the location of a crime incident. The intuition is that the individuals who commit crimes in violent districts are more likely to become violent than the others. We found that the beat the individual has committed a crime in is an important feature for our problem. This is in accordance with previous well known literature in criminology [3, 21] which studies spatio-temporal modeling of criminal behavior. The complete list is shown in Table 6.

4.3.4

Temporal Features

We considered couple of temporal features: average inter-

In this section, we review the results of our experiments. We looked at two types: experiments where the entire cooffender network is known before-hand (Section 5.1) and experiments where the network is discovered over time (Section 5.2). The intuition behind the experiments where the co-offender network is known is that the police often have additional information to augment co-arrestee data. This information can include informant reporting, observed individuals interacting by patrolmen, intelligence reporting, and information discovered on social media and the Internet. In our second type of experiment we discover the network over time in an effort to mimic real-world operations - however, we also show that this makes the problem more difficult as it reduces the power of neighborhood-based and networkbased features. Based on our discussions with the Chicago

Police, we believe that real-world results will most likely fall somewhere between these two experiments. Operationally, we will not have full arrest data, but the aforementioned augmenting data sources are available (even though we did not have access to them for our experiments).

1   0.9   0.8   0.7   0.6   0.5   0.4   0.3  

1   0.9   0.8   0.7   0.6   0.5   0.4   0.3   0.2   0.1   0   Precision   Recall   F1   Precision   Recall   F1  

5.1

Known Co-Offender Network

0.2   0.1   0  

In this experiment we assume that the entire offender network is known. In other words, to compute the features for each vertex v , we assume that the set Vv is unknown while the rest of the network is observable. In here we compared our approach with THH but not with the PVA as we do not utilize time. In each of the experiments described in this section, we conduct 10-fold cross validation. We consider the result of each approach as a set of nodes that the approach considers to be a set of potentially violent individuals. Our primary metrics are precision (fraction of reported violent individual who were actually violent in the dataset), recall (fraction of violent individuals in the dataset reported by the approach), F1 (the harmonic mean of precision and recall) and area under the curve. We conduct two types of experiments: first, we study classification performance using only features within a given category (neighborhood, network, temporal, and geographic), then we study the classification performance when the entire feature set is used but with various different classification algorithms and compare the result to THH. Classification using single feature categories. Here we describe classification results using single feature categories. In this set of experiments, we use a random forest classifier (which we will later show provides the best performance of the classifiers that we examined). Figure 5 shows the performance of RF for the described categories. The network-based features are highly-correlated to violent behavior with average F1 value of 0.72 compared to 0.63 for neighborhood, 0.21 for geographic, and 0.03 for temporal features. In Figure 6, we show the performance of a feature from each category to classify violent vs. non violent crimes; the performance of each example is a good indicator of the performance of its category.
1   0.9   0.8   0.7   0.6   0.5   0.4   0.3   0.2   0.1   0   Neighborhood- Network-Based   Geographic   Based   Features     Temporal  

(a)
1   0.9   0.8   0.7   0.6   0.5   0.4   0.3   0.2   0.1   0   Precision   Recall   F1   1   0.9   0.8   0.7   0.6   0.5   0.4   0.3   0.2   0.1   0   Precision  

(b)

Recall  

F1  

Violent  

Non  violent  

(c)

(d)

Figure 6: Example features from each category. (a) Neighborhood-based: Minority of 1-hop and majority of 2-hop neighbors committing a crime in C . (b) Network-based: Closeness (w.r.t. V ). (c) Geographic: Beat violence. (d) Temporal: Average interval months.
1   0.9   0.8   0.7   0.6   0.5   0.4   0.3   0.2   0.1   0   0   0.2   Network-Based   Geographic   All   0.4   0.6   0.8   1  

True  posi*ve  

False  posi*ve  
Neighborhood-Based     Temporal  

Figure 7: ROC curve for each feature set. Table 8, RF provides the best performance (F1=0.83); we also note that using SMOTE for RF, did not improve this result. Figure 8 shows that our algorithm outperforms THH. The performance of our features are also illustrated in Figure 7. The area under the curve (AUC) of applying all features is 0.98 ­ a higher overall accuracy. The AUC for network-based, neighborhood-based, geographic, and temporal categories are 0.92, 0.91, 0.65, and 0.7 respectively. This indicates the importance of network features for this classification task.

Precision  

Recall  

F1  

5.2
Figure 5: Precision, recall, and F1 comparison between each group of features. Classification comparison. Table 8 shows the performance of different classification algorithms. According to

Co-Offender Network Emerges Over Time

In this section, we present a more difficult experiment where the co-arrestee network is discovered over time (by virtue of arrests). To simulate this phenomenon, we split our data into two disjoint sets: the first set for learning and identification, and the second one for measuring the

Table 8: K-fold cross validation. Method Precision Recall F1 RF 0.89 0.78 0.83 RF w. SMOTE 0.86 0.78 0.82 NB 0.45 0.49 0.47 LR 0.68 0.49 0.57 DT 0.71 0.66 0.68 NN 0.64 0.57 0.6 SVM 0.73 0.2 0.31
1   0.9   0.8   0.7   0.6   0.5   0.4   0.3   0.2   0.1   0   THH   RF  

and number of false positives and display the results in Figures 10 and 11. In FRF (Filtered Random Forest) we filter the offenders who have not committed any crime in the last 200 days. This simple heuristic increase the precision drastically while preserving the recall. The main advantage of our method, besides the high precision, is its ability to significantly reduce the population of potentially violent offenders when compared to PVA - which for each month had between 1813 and 3571 false positives. Figure 11 compares the number of true and false positives instances for all the approaches for each month except PVA (PVA was omitted due to readability because of the large amount of false positives). While the F1 measure for PVA is higher than that of the others, the large number of false positives prevents the law enforcement from using it effectively in practice. Furthermore, as time progresses, PVA likely rises in recall due to the drop in the number of violent criminals to predict.

6.

RELATED WORK

Precision  

Recall  

F1  

Figure 8: Performance comparison between THH and RF in K-fold cross validation. performance. We do monthly split and start from February 2013. To illustrate the difficulty of this test, we show the number of nodes, edges, and violent individuals per month in Figure 9. We note that in the early months, we are missing much of the graphical data (over 40% of nodes and edges in the first two months) - hence making many of our features less effective. However, as the months progress, there are less violent individuals to identify (due to the temporal nature of the dataset) - hence amplifying the data imbalance as time progresses.
1   0.9   0.8   0.7   0.6   0.5   0.4   0.3   0.2   0.1   0  

Though we believe that the prediction of violent offenders using co-offender social networks is new, there has previously been work on both co-offender networks in general as well as crime forecasting. In this section, we briefly review some of the relevant contributions in both of these areas. There has been much previous work on co-offender networks. The earlier work that studied these special social networks primarily came from the criminology literature. For instance, [14] utilizes social network analysis techniques to study several case studies where the social network of the criminal organization was known. In [13], the authors study the stability of these networks change over time. More recently, graphical features derived from networks comprised of both offenders and victims has been shown to be related to the the probability of an individual becoming a victim of a violent crime [17, 16]. Previous work has also looked at the relationship between network structure and geography [18] and has leveraged both network and geographic features to predict criminal relationships [25] as well as influence gang members to dis-enroll [24]. There have also been several software tools developed for conducting a widerange of analysis on co-offender networks including CrimeFighter [20], CrimeLink [22], and ORCA [19]. However, our work departs from this is that we are looking to leverage the network topology and other features to identify violent offenders - which was not studied in any of the previous work. There has also been a large amount of work on crime forecasting (i.e. [7, 12]) though historically, this work has relied on spatio-temporal modeling of criminal behavior [3, 21] or was designed to identify suspects for specific crimes [26, 15]. None of this previous work was designed to identify future violent offenders nor did it leverage social network structure.

Frac%on  of  full  dataset  

Month   Edges   Nodes   Violent  individuals  

7.

CONCLUSION

Figure 9: Number of nodes, edges, and violent individuals over time. More training data, less offenders to identify. In these experiments, we compared our approach using random forests with the full feature set to THH and PVA. We measure precision, recall, F1, number of true positives,

In this paper we explored the problem of identifying repeat offenders who will commit violent crime. We showed a strong relationship between network-based features and whether a criminal will commit a violent offense providing an unbiased F1 score of 0.83 in our cross-validation experiment where we assumed that the underlying network was known. When we moved to the case where the network

0.4  

90  

Number  of  true  posi.ve  

0.35   0.3  

80   70   60   50   40   30   20   10   0  

Recall  

0.25   0.2   0.15   0.1   0.05   0  

13    Ap r-1 3   Ju n- 13    Au g- 13    Oc t-1 3   De c- 13    Fe b- 14    Ap r-1 4   Ju n- 14   

Fe b-

Month  
0.25   1200  

Number  of  false  posi/ve  

0.2  

1000   800   600   400   200   0  

Precision  

0.15   0.1   0.05   0  

13    Ap r-1 3   Ju n- 13    Au g- 13    Oc t-1 3   De c- 13    Fe b- 14    Ap r-1 4   Ju n- 14   

Fe b-

Month  
0.18   0.16   0.14   0.12  

Figure 11: Number of true and false positive instances.

F1  

0.1   0.08   0.06   0.04   0.02   0  

belongs. For a given node v and C  V , we define shellC (v ) as the shell number of node v on the subgraph consisting of v and all nodes v where C  Vv = . We slightly abuse notation and define shell (v ) as the shell number of v on the full network. Propogation Process. For a given node v and the set of activated nodes V , we define v 's active neighbors as follows:
1 actv (V ) = {u|u  Nv V }

13    Ap r-1 3   Ju n- 13    Au g- 13    Oc t-1 3   De c- 13    Fe b- 14    Ap r-1 4   Ju n- 14   

Fe b-

Month   THH   FRF   RF   PVA  

Figure 10: Precision, recall, and F1 over time. was discovered over time, our method significantly outperformed baseline approaches significantly increasing precision and recall. We are currently discussing ways to operationalize this technology with the Chicago Police as well as design strategies to best deploy police assets to areas with higher concentrations of potentially violent offenders. We are also working with the police to identify other sources of data to build a more complete social network of the offenders.

We now define an activation function A that, given an initial set of active nodes, returns a set of active nodes after one time step. A (V ) = V  {v  V s.t. |actv (V )| } We also note that the activation function can be applied iteratively, to model a diffusion process. Hence, we shall use the following notation to signify multiple applications of A (for natural numbers t > 1). At  (V ) = A (V ) -1 A (At  (V )) if t = 1 otherwise

Appendix
Shell Number. For a given graph, the k-core is the largest subgraph where each node has at least degree k. The k-shell is the set of nodes in core k but not in any higher core. A node's shell number is k value of the shell to which that node

t-1 Clearly, when At G, (V ) = AG, (V ) the process has converged. Further, this always converges in no more than |V | steps, since the process must activate at least one new node in each step prior to converging. Based on this idea, we define the function  which returns the set of all nodes activated upon the convergence of the activation function. We

Ap r-1 3   Ju n- 13    Au g- 13    Oc t-1 3   De c- 13    Fe b- 14    Ap r-1 4   Ju n- 14   

Fe b-

13   

13    Ap r-1 3   Ju n- 13    Au g- 13    Oc t-1 3   De c- 13    Fe b- 14    Ap r-1 4   Ju n- 14   

Fe b-

Month   FRF   RF   THH  

Month   FRF   RF   THH  

define  (V ) = A (V ) where t is the least value such that t-1 At  (V ) = A (V ).

8.

REFERENCES

[1] J. Bertetto. Countering criminal street gangs: Lessons from the counterinsurgent battlespace. Law Enforcement Executive Forum, 12(3):43, 2012. [2] A. Braga, D. Hureau, and A. Papachristos. Deterring gang-involved gun violence: Measuring the impact of boston's operation ceasefire on street gang behavior. Journal of Quantitative Criminology, pages 1­27, 2013. [3] P. Brantingham and P. Brantingham. Crime Pattern Theory. In R. Wortley and L. Mazerolle, editors, Enviromental Criminology and Crime Analysis, pages 78­93. 2008. [4] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16(1):321­357, 2002. [5] P. Expert, T. S. Evans, V. D. Blondel, and R. Lambiotte. Uncovering space-independent communities in spatial networks. Proceedings of the National Academy of Sciences, 108(19):7663­7668, 2011. [6] L. C. Freeman. A set of measures of centrality based on betweenness. Sociometry, 40(1):pp. 35­41, 1977. [7] W. Gorr and R. Harries. Introduction to crime forecasting. International Journal of Forecasting, 19(4):551 ­ 555, 2003. [8] M. Granovetter. Threshold models of collective behavior. The American Journal of Sociology, (6):1420­1443. [9] H. Han, W.-Y. Wang, and B.-H. Mao. Borderline-smote: a new over-sampling method in imbalanced data sets learning. In Advances in intelligent computing, pages 878­887. Springer, 2005. [10] J. C. Howell. Gangs in America's Communities. Sage, 2012. [11] M. Kitsak, L. K. Gallos, S. Havlin, F. Liljeros, L. Muchnik, H. E. Stanley, and H. A. Makse. Identification of influential spreaders in complex networks. Nat Phys, 6(11):888­893, Nov. 2010. [12] H. Liu and D. E. Brown. Criminal incident prediction using a point-pattern-based density model. International Journal of Forecasting, 19(4):603­622, Oct. 2003. [13] J. M. Mcgloin, C. J. Sullivan, A. R. Piquero, and S. Bacon. Investigating the stability of co-offending and co-offenders among a sample of youthful offenders. Criminology, 46(1):155­188, 2008. [14] C. Morselli. Inside Criminal Networks. Springer, 2009. [15] C. Overall and G. Day. The Hammer Gang: an exercise in spatial analyis of an armed robbery series using the probability grid method. In S. Chainey and L. Tompson, editors, Crime Mapping Case Studies, pages 55­62. 2008.

[16] A. Papachristos, C. Wildeman, and E. Roberto. Tragic, but not random: The social contagion of nonfatal gunshot injuries. Social Science and Medicine, page 139. [17] B. A. H. D. Papachristos, A. Social networks and the risk of gunshot injury. J. Urban Health, 2012. [18] H. D. B. A. Papachristos, A. The corner and the crew: The influence of geography and social networks on gang violence. American Sociological Review, 2013. [19] D. Paulo, B. Fischl, T. Markow, M. Martin, and P. Shakarian. Social network intelligence analysis to combat street gang violence. In Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM '13, pages 1042­1049, New York, NY, USA, 2013. ACM. [20] R. R. Petersen and U. K. Wiil. Crimefighter investigator: A novel tool for criminal network investigation. In EISIC, pages 197­202. IEEE, 2011. [21] D. K. Rossmo and S. Rombouts. Geographic Profiling. In R. Wortley and L. Mazerolle, editors, Enviromental Criminology and Crime Analysis, pages 136­149. 2008. [22] J. Schroeder, J. Xu, and H. Chen. Crimelink explorer: Using domain knowledge to facilitate automated crime association analysis. In H. Chen, R. Miranda, D. Zeng, C. Demchak, J. Schroeder, and T. Madhusudan, editors, Intelligence and Security Informatics, volume 2665 of Lecture Notes in Computer Science, pages 168­180. Springer Berlin Heidelberg, 2003. [23] S. B. Seidman. Network structure and minimum degree. Social Networks, 5(3):269 ­ 287, 1983. [24] P. Shakarian, J. Salmento, W. Pulleyblank, and J. Bertetto. Reducing gang violence through network influence based targeting of social programs. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14, pages 1829­1836, New York, NY, USA, 2014. ACM. [25] M. A. Tayebi, M. Ester, U. Gl¨ asser, and P. L. Brantingham. Spatially embedded co-offence prediction using supervised learning. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14, pages 1789­1798, New York, NY, USA, 2014. ACM. [26] M. A. Tayebi, M. Jamali, M. Ester, U. Gl¨ asser, and R. Frank. Crimewalker: A recommendation model for suspect investigation. In Proceedings of the Fifth ACM Conference on Recommender Systems, RecSys '11, pages 173­180, New York, NY, USA, 2011. ACM. [27] D. J. Watts and S. H. Strogatz. Collective dynamics of "small-world" networks. nature, 393(6684):440­442, 1998.

Product Offerings in Malicious Hacker Markets
Ericsson Marin, Ahmad Diab and Paulo Shakarian
Arizona State University Tempe, Arizona {ericsson.marin, ahmad.diab, shak}@asu.edu

arXiv:1607.07903v1 [cs.CR] 26 Jul 2016

Abstract--Marketplaces specializing in malicious hacking products - including malware and exploits - have recently become more prominent on the darkweb and deepweb. We scrape 17 such sites and collect information about such products in a unified database schema. Using a combination of manual labeling and unsupervised clustering, we examine a corpus of products in order to understand their various categories and how they become specialized with respect to vendor and marketplace. This initial study presents how we effectively employed unsupervised techniques to this data as well as the types of insights we gained on various categories of malicious hacking products.

not trivial, including effort to answer questions, solve puzzles, mathematical equation or CAPTCHA. Most related work on darkweb markets such as [3] focus on a single market and do not restrict their study to malicious hacking products. Our previous work on markets [4] focused on a game theoretic analysis of a small subset of the data in this paper - and did not attempt to categorize the products for sale. Additionally, there is a complementary lines of work on malicious hacking forums (i.e. [5], [6], [7], [8], [9], [10], [11], [12]) - which is a related but different topic from this paper. II. M ALICIOUS H ACKING P RODUCT C ATEGORIZATION In this section, we describe our malicious hacker product dataset and our use of clustering to identify malicious hacker product categories. We examined 17 malicious hacker marketplaces crawled over a 6 month period. The crawled information was then parsed and stored in a relational database. Relevant tables record the marketplaces themselves, the vendors of the various products and the items/products for sale. Each item is associated with a vendor and a marketplace, allowing for join queries. Some of the more relevant fields for marketplace items include the price, title, description, rating, posting date. In this work, we primarily extract features from the product title/name to generate features. We note that many items are cross-posted and are nearly identical. We show the distribution of vendors who use the same screen-name across multiple marketplaces in Fig. 1(a). To clean our product data, we identify duplicate (cross-posted) products and report on the size of our dataset in Table I. As we collect data from a variety of different sites, there is inconsistency as to how products are categorized on each site - if such non-trivial categorization even exists for a given site. In addition, there is a clear absence of a standardized method for vendors to register their products. As a consequence, the great majority of products are unique when compared with simple matching or regular expression technique. It is valid even in the case where a pair of vendors with different screen names post what a human would determine to be the same product. Fig. 1(b) shows the distribution of products and number of vendors sharing each product. The distribution follows a power-law. Note that about 57% of products are unique by simple comparison methods. Clustering approach. Using product names, we engineer features that represent each product as a vector. A set of pilot experiments suggests that word and character n-grams

I. I NTRODUCTION Websites on the deepweb and darkweb specializing in the sale of malicious hacking products - such as malware platforms, software exploits and botnet rental - have become the venue of choice for online purchase of these items by cyber criminals. In this paper, we leverage unsupervised learning to categorize and study the product offerings of 17 of these online markets. Specifically, we describe how we used manual labeling combined with clustering techniques to identify product categories (Section II), and then we analyze the results both quantitatively and qualitatively (Section III). We identify categories of products that are highly specialized with respect to particular vendors and markets. We also highlight other interesting facets of this ecosystem - for instance, vendors who habitually cross-list products on multiple sites and nearly identical products for sale by multiple vendors. Background and Related Work. The darkweb refers to the anonymous communication provided by crypto-network tools such as "The Onion Router" (Tor), which is free software dedicated to protect the privacy of its users by obscuring traffic analysis as a form of network surveillance [1]. On the other hand, the deepweb refers to sites not indexed by common search engines due to a variety of reasons (e.g. password protections), that not necessarily rely on additional protocols. The sites on the darkweb and deepweb explored in this study comprise marketplaces [2]. In these websites, vendors advertise and sell their goods and services relating to malicious hacking, drugs, pornography, weapons and software services. Products are most often verified before any funds are released to the seller. The main engine of these environments is trust. If a seller is misleading or fails to deliver the appropriate item, he is banned from the site. Similarly, buyers can be banned for not complying with the transaction rules. Basically, all marketplaces in darkweb require a registration and a valid account to get access. Sometimes, this registration process is

TABLE I S CRAPED DATA FROM M ARKETPLACES IN DARKWEB . Marketplaces Products (Total) Products (Distinct) Vendors
60
10000

17 16122 9093 1332

50
1000

30
20 10 0 0 2 4 6

Number of Products

40

100

10

1

Markets
(a)

8

10 12 14 16 18

0 2 4 6 8 10 12 14 16 18 20 22 24

Number of Shared Vendors
(b)

Fig. 1. Distribution of (a) Shared Vendors over Markets. (b) Products over Shared Vendors.

Table II shows the performance of each TF-IDF vectorization using Rand-index and entropy, when K-means starts with the 34 fixed centroids. For Rand-index, character ngrams in the range from 3 to 4, 3 to 5, and 3 to 6, when Kmeans used cosine similarity reached a high best performance (0.986). In addition, we also found the best entropy (0.067) when K-means uses the same specification. This way, K-means configuration with character n-grams in the range from 3 to 6 for vectorization, cosine similarity for distance function and the 34 points for the starting centroids was our natural choice to produce the clusters in the entire dataset. We also examined the performance of our approach using random centroids. As expected, it performs worse than using the centroids derived from products. Additionally, we examined products (from the full dataset) with a cosine similarity of less than 0.1 from the calculated centroids. There were 410 such distinct products (4.51% of the dataset). These were then manually examined and most were found to be irrelevant to our target domain - and we did not consider them further. III. A NALYST I NTERPRETATION OF P RODUCT C LUSTERS

would provide more pure clusters compared with other feature engineering methods, such as meta-data or domain-specific keywords. These features were valued using standard term frequency - inverse document frequency (TF-IDF), after the elimination of stopping words and the execution of steaming. We evaluated word n-gram feature vectors of length up to 1 and up to 2 words and many character n-gram features in the ranges from 3 to 7 and from 4 to 7. This gave us 10 different feature vectors in all. To verify which of these strategies could reach the best performance in our dataset, we evaluated the effect of the different types of feature vectors on the accuracy and purity of clusters produced by the K-means algorithm. To determine the best feature vector, we manually labeled 500 samples using 34 labeled groups (listed in Table III). We used 400 of the samples to determine centroids for each of the 34 groups, and then we evaluated the resulting clustering on the remaining 100 samples. We examined the accuracy of the different approaches when compared to ground truth using the Rand-index method [13]. This method is defined as the number of pairs correctly considered in the same class or correctly considered in different classes divided by n 2 , where n is the number of samples. In addition, we used standard entropy measurements to examine the purity of the clusters. Entropy measures the amount of disorder in a cluster. A zerovalue for this metric means the clusters are formed by just one class. The formal definition is as follows:
k

Number of Shared Vendors

entropy (Di ) = -
i=1

P ri (cj ) log2 P ri (cj ),

(1)

where P ri (cj ) is the proportion of class cj data points in cluster i or Di . The total entropy (considering all clusters) is:
k

entropytotal (D) =
i=1

|Di | x entropy (Di ) |D |

(2)

In this section, we examine the results of clustering based on character n-grams in the range 3 to 6 using initial centroids determined from the labeled data. In order to analyze the information of these clusters, we calculated their entropy with respect to two different criteria: marketplaces and vendors. We also checked in the database the number of distinct marketplaces and vendors inside each cluster. The idea was to understand the diversity of the clusters regarding these two facets. A low marketplace entropy for a given cluster would mean its products were mainly found in a particular marketplace. Similarly, low vendor entropy would mean the cluster's products were mainly sold by a particular vendor. Table III presents the results. As shown in Table III, Links holds the lowest entropy when we analyzed the marketplaces, suggesting the great majority of products come from the same market. In this cluster, 80% of products came from only 2 markets. However, when we check the vendor entropy for this same cluster, we can observe a higher value, suggesting that many vendors are actually offering products related to Links. It is possible that many markets discourage the re-selling of lists of links, as much of this information can be found on darkweb Wiki's for free. Similarly, Hacking Tools holds the lowest entropy for the vendor criteria. This suggest that only a few vendors are present in that cluster. Specifically, only 2 vendors author 416(50%) of this type of products. At first glance, this may be surprising as this appears to be a very general group. However, upon inspection of the contents, we find that many authors of these products are actually organizations. These organizations use similar language in their product description in an effort to brand their wares. This could indicate the presence of hackingcollectives that author products as well as the limitations of our text-based approach - which can potentially cluster products branded in a similar fashion. We also note one of the most

TABLE II K- MEANS E VALUATION (F IXED C ENTROIDS ).
word(1,1) 0.986 0.986 0.075 0.224 word(1,2) 0.985 0.977 0.079 0.110 char(3,4) 0.986 0.976 0.067 0.153 char(3,5) 0.986 0.973 0.067 0.156 Rand-index char(3,6) char(3,7) 0.986 0.985 0.973 0.974 Entropy 0.067 0.075 0.156 0.141 char(4,4) 0.985 0.975 0.072 0.134 char(4,5) 0.985 0.975 0.079 0.134 char(4,6) 0.984 0.977 0.088 0.137 char(4,7) 0.982 0.971 0.088 0.175 Random 0.933 0.933 0.423 0.423

Cosine Euclidean Cosine Euclidean

TABLE III C LUSTERS E NTROPY.
Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 Cluster Name Carding PayPal-related Cashing Credit Cards PGP Netflix-related Hacking Tools - General Dumps - General Linux-related Email Hacking Tools Network Security Tools Ebay-related Amazon-related Bitcoin Links (Lists) Banking Point of Sale VPN Botnet Hacking Groups Invitation RATs Browser-related Physical Layer Hacking Password Cracking Smartphone - General Wireless Hacking Phishing Exploit Kits Viruses/Counter AntiVirus Network Layer Hacking RDP Servers Android-related Keyloggers Windows-related Facebook-related No of Products 1263 1103 867 865 846 825 749 561 547 539 472 456 443 422 384 375 272 257 251 249 249 237 230 223 222 218 218 210 205 191 156 143 119 119 No of Markets 16 16 16 15 14 15 12 16 13 15 15 16 15 12 13 15 12 12 14 15 12 13 13 14 13 13 14 14 14 12 11 13 12 15 Market Entropy 0.320 0.340 0.351 0.347 0.270 0.331 0.289 0.372 0.335 0.366 0.385 0.391 0.360 0.211 0.349 0.384 0.413 0.291 0.387 0.453 0.380 0.408 0.434 0.408 0.389 0.403 0.413 0.413 0.459 0.405 0.429 0.496 0.464 0.501 No of Vendors 315 335 256 203 351 132 280 117 196 117 163 197 201 221 186 181 130 110 143 99 134 122 100 110 56 111 91 60 60 124 60 77 50 67 Vendor Entropy 0.720 0.754 0.738 0.696 0.805 0.516 0.777 0.758 0.738 0.621 0.772 0.825 0.823 0.838 0.840 0.841 0.827 0.796 0.865 0.797 0.857 0.856 0.781 0.816 0.601 0.849 0.795 0.684 0.716 0.895 0.770 0.862 0.717 0.876

hacking products with respect to vendor and marketplace, and finally, we identified several interesting characteristics of how the products were grouped. Currently, we are examining other methods for grouping these products using matrix factorization and supervised techniques. Additionally, we are studying the underlying social network of vendors through relationships based on similar product offerings.
Acknowledgments. Some of the authors were supported by the Office of Naval Research (ONR) Neptune program, the Arizona State University Global Security Initiative (ASU GSI), and CNPq-Brazil.

R EFERENCES
[1] R. Dingledine, N. Mathewson, and P. Syverson, "Tor: The Secondgeneration Onion Router," in Proceedings of the 13th Conference on USENIX Security Symposium - Volume 13, ser. SSYM'04. Berkeley, CA, USA: USENIX Association, 2004, pp. 21­21. [2] V. Ciancaglini, M. Balduzzi, R. McArdle, and M. R¨ osler, "Below the Surface: Exploring the Deep Web," 2015. [3] N. Christin, "Traveling the silk road: A measurement analysis of a large anonymous online marketplace," in Proceedings of the 22Nd International Conference on World Wide Web, ser. WWW '13. New York, NY, USA: ACM, 2013, pp. 213­224. [4] J. Robertson, V. Paliath, J. Shakarian, A. Thart, and P. Shakarian, "Data Driven Game Theoretic Cyber Threat Mitigation," in Proc. 28th Innovative Applications of Artificial Intelligence (IAAI-16), 2016. [5] C. C. Yang, X. Tang, and X. Gong, "Identifying dark web clusters with temporal coherence analysis," in Intelligence and Security Informatics (ISI), 2011 IEEE International Conference on, July 2011, pp. 167­172. [6] V. Benjamin, W. Li, T. Holt, and H. Chen, "Exploring threats and vulnerabilities in hacker web: Forums, irc and carding shops," in Intelligence and Security Informatics (ISI), 2015 IEEE International Conference on, May 2015, pp. 85­90. [7] M. Macdonald, R. Frank, J. Mei, and B. Monk, "Identifying digital threats in a hacker web forum," in Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015, ser. ASONAM '15. New York, NY, USA: ACM, 2015, pp. 926­933. [8] Z. Zhao, G.-J. Ahn, H. Hu, and D. Mahi, "SocialImpact: Systematic Analysis of Underground Social Dynamics." in ESORICS, ser. Lecture Notes in Computer Science, S. Foresti, M. Yung, and F. Martinelli, Eds., vol. 7459. Springer, 2012, pp. 877­894. [9] H. Chen, "Dark web: Exploring and mining the dark side of the web," in Intelligence and Security Informatics Conference (EISIC), 2011 European, Sept 2011, pp. 1­2. [10] J. Shakarian, P. Shakarian, and A. Ruef, "Cyber attacks and public embarrassment: A survey of some notable hacks," Elsevier SciTechConnect, 2015. [11] P. Shakarian and J. Shakarian, "Socio-cultural modeling for cyber threat actors," in AAAI Workshop on Artificial Intelligence and Cyber Security (AICS), 2016. [12] J. Shakarian, A. Gunn, and P. Shakarian, "Exploring malicious hacker forums," in Cyber Deception: Building the Scientific Foundation, S. Jajodia, V. Subrahmanian, V. Swarup, and C. Wang, Eds. Springer, 2016. [13] W. M. Rand, "Objective Criteria for the Evaluation of Clustering Methods," Journal of the American Statistical Association, vol. 66, no. 336, pp. 846­850, Dec. 1971.

prominent vendor in this cluster was itself a marketplace which is also reflected in the low marketplace entropy. In our analysis of the Facebook and Keylogger clusters, we can see that they point to the other direction. They have high values for both entropy, a clear sign about the diversity with respect to both vendors and markets. For example, in cluster Facebook, there were 119 products and 67 vendors, and the most prolific vendor for this cluster authored only 8 products. In the same cluster, products were also spread across 15 markets - and the most well-represented market was associated with 30 products. This analysis also indicates widespread prevalence of keyloggers - which is not surprising as it is a well established hacking technique. However, observing the similar trend for the Facebook cluster could be indicative of an increase in demand for Facebook-directed social media hacking products and information. Conclusion and Future Work. In this paper, we conducted an initial examination of malware products from 17 malicious hacker markets through unsupervised learning. Using manually-labeled data, we studied the effect of feature vector on cluster purity using text-based features. We then analyzed the impurity of clusters in our corpus of over 8, 000 malicious

Toward Order-of-Magnitude Cascade Prediction
Ruocheng Guo , Elham Shaabani, Abhinav Bhatnagar and Paulo Shakarian

arXiv:1508.03371v1 [cs.SI] 13 Aug 2015

Arizona State University
Tempe, AZ
Email: {rguosni, shaabani, abhatn, shak}@asu.edu
Abstractâ€”When a piece of information (microblog, photograph, video, link, etc.) starts to spread in a social network, an
important question arises: will it spread to â€œviralâ€ proportions
â€“ where â€œviralâ€ is defined as an order-of-magnitude increase.
However, several previous studies have established that cascade
size and frequency are related through a power-law - which leads
to a severe imbalance in this classification problem. In this paper,
we devise a suite of measurements based on â€œstructural diversityâ€
â€“ the variety of social contexts (communities) in which individuals
partaking in a given cascade engage. We demonstrate these
measures are able to distinguish viral from non-viral cascades,
despite the severe imbalance of the data for this problem. Further,
we leverage these measurements as features in a classification
approach, successfully predicting microblogs that grow from 50
to 500 reposts with precision of 0.69 and recall of 0.52 for the
viral class - despite this class comprising under 2% of samples.
This significantly outperforms our baseline approach as well as
the current state-of-the-art. Our work also demonstrates how we
can tradeoff between precision and recall.

I.

I NTRODUCTION

When a piece of information (microblog, photograph,
video, link, etc.) starts to spread in a social network, an
important question arises: will it spread to â€œviralâ€ proportions
â€“ where â€œviralâ€ is defined as an order-of-magnitude increase.
Several previous studies [1], [2] have established that cascade
size and frequency are related through a power-law - which
leads to a severe imbalance in this classification problem.
In this paper, we devise a suite of measurements based on
â€œstructural diversityâ€ that are associated with the growth of a
viral cascade in a social network. Structural diversity refers
to the variety of social contexts in which an individual engages and is typically instantiated (for social networks) as the
number of distinct communities represented in an individualâ€™s
local neighborhood. Previously, Ugander et al. identified a
correlation between structural diversity and influence [?]. We
demonstrate these measures are able to distinguish viral from
non-viral cascades, despite the severe imbalance of the data
for this problem. Further, we leverage these measurements as
features in a classification approach, successfully predicting
microblogs that grow from 50 to 500 reposts with precision
of 0.69 and recall of 0.52 for the viral class (under 2% of the
samples).
We note that our results on the prediction of cascades
rely solely upon the use of our structural diversity based
measures for features and limited temporal features - hence
the prediction is based on network topology alone (no content
information was utilized). We also achieved these results while
maintaining the imbalances of the dataset - which we felt better
U.S. Provisional Patent 62/201,517. Contact shak@asu.edu for licensing
information

mimics reality. This differs from some previous studies which
balance the data before conducting classification. Further,
we note that we obtained prediction of order-of-magnitude
increases in the size of the cascade - which also differs from
other work (i.e. [1]) which focus on identifying cascades that
double in size.
II.

T ECHNICAL P RELIMINARIES

Here we introduce necessary notation and describe our
social network data. We represent a social network as a graph
G = (V, E) where V is the set of vertices and E as set
of directed edges that have sizes |V |, |E| respectively. The
intuition behind edge (v, v 0 ) is that node v can influence v 0 .
This intuition stems from how we create the edges in our
network: (v, v 0 ) is an edge if during a specified time period
there is at least one microblog posted by v that is reposted by
v 0 (we leave other thresholds beyond 1 repost to future work).
We shall also assume a partition over nodes that specifies a
community structure. We shall assume that such a partition is
static (based on the same time period from which the edges
were derived) and that the partition C consists of k communities: {C1 , C2 , ..., Ck }. There are many possible methods to
derive the communities (if user-reported communities are not
available). We utilize the Louvain algorithm to identify our
communities in this paper due to its ability to scale.
Cascades. For a given microblog Î¸, we denote the subset
of first-m nodes who originally posted or reposted Î¸ as VÎ¸m
and refer to them as adopters (at size m). Likewise, the
set of reposting relationships within the same time period
will be denoted RÎ¸m . Taken together, we have a cascade:
DÎ¸m = (VÎ¸m , RÎ¸m ). Any valid original microblog Î¸ could
be treated as a unique identifier for a cascade. Given a
microblog Î¸, vÎ¸ is the originator at instance t0Î¸ , which is
defined as the origin time when the originator posted the
microblog Î¸ and time t is time since t0Î¸ . The mth repost
of the microblog Î¸ happens at time tm
Î¸ . As m increases, a
cascade accumulates nodes and edges over time. We shall
use NÎ¸ to denote the final size of a cascade while the size
of a cascade at any particular instance is the set of nodes
present at that instance is simply |VÎ¸m |. For a given size m,
we shall refer to the frontiers as the outgoing neighbors of the
adopters in graph G who are not adopters themselves. Formally: FÎ¸m = {v âˆˆ V /VÎ¸m s.t. âˆƒvi âˆˆ VÎ¸m where (vi , v) âˆˆ E}.
For nodes in G that are outside the adopters, we shall use
the notation texp (v, Î¸, m) to denote the number of time units
from the initial post of Î¸ before the microblog was reposted
by one of vâ€™s incoming neighbors - intuitively the time at
which v was exposed to Î¸. For a given natural number Î»
(used to specify a time period), we define the Î» frontiers
as a subset of the frontiers that have been exposed to Î¸

no earlier than Î» time units previously. Formally this set
is defined as follows: FÎ¸m,Î» = {v âˆˆ FÎ¸m |texp (v, Î¸, m) â‰¤ Î»}.
Finally, the complement of this set are the Î» non-adopters:
FÌ„Î¸m,Î» = {v âˆˆ FÎ¸m |texp (v, Î¸, m) > Î»}.
Sina Weibo Dataset. The dataset we used was provided by
WISE 2012 Challenge1 . It included a sample of microblogs
posted on Sina Weibo from 2009 to 2012. In this dataset, we
are provided with time and user information for each post
and subsequent repost which enabled us to derive a corpus
of cascades. From this data, we derived our social network
G = (V, E) (with 17.9 M vertices and 52.4 M edges) that
was created from reposts that were published during the 3
month period between May 1, 2011 and July 31, 2011. For this
network, the average clustering coefficient is 0.107. There are
4974 connected components in the network. Louvain algorithm
outputs 379,416 communities with average size of 47.5 for
this network. As expected, this network exhibits a power-law
degree distrubtion. For this network, the number of active
nodes in August (the time period we studied for cascade
prediction) is 5,910,608, while 5,664,625 of them at least have
one out-neighbor. During the month of August, there were
9,323,294 reposts with 2,252,368 different original microblogs.
1,920,763 (86.6%) of them were written by authors who at
least published one microblog during May 1, 2011 to July
31, 2011 (the time period we used to create the underlying
network). The average time it took for viral cascades to become
viral is approximately 18 hours. The distribution of final
size of cascades mimics a power-law distribution which can
demonstrate that this dataset is more representative of cascade
behavior observed â€œin the wildâ€. This differs significantly
from the previous works which conduct biased sampling to
artificially provide balanced classes. We selected Î» as 30
minutes as 90% of all reposts in the initial 3 month period
occurred in under this time.
Number of communities. For V 0 âŠ† V , the associated communities C(V 0 ) are the communities represented by V 0 . Formally:
C(V 0 ) = {Ci âˆˆ C s.t. V 0 âˆ© Ci 6= âˆ…}. The cardinality of this set
(number of communities) will be denoted K(V 0 ). We measure
the number of communities represented by the above three
populations of nodes: K(VÎ¸m ), K(FÎ¸m,Î» ), K(FÌ„Î¸m,Î» ) observed
at either a given cascade size.
Gini impurity. For V 0 âŠ† V , the gini impurity, IG (V 0 ) is the
probability of a node in V 0 being placed into the incorrect
community if assigned a community based on the distribution
of communities represented in V 0 . Formally: IG (V 0 ) =
P |Ci âˆ©V 0 |
1 âˆ’ i ( |V 0 | )2 . We study the gini impurity of the adopters,
Î» non-adopters and Î» frontiers for either a given cascade size
m: IG (VÎ¸m ), IG (FÎ¸m,Î» ), IG (FÌ„Î¸m,Î» ). The intuition is to capture
a notion of how the communities are distributed amongst the
nodes in each of these sets with a single scalar value. We note
that the impurity of the adopter set IG (VÎ¸m ) behaves similar
to the entropy of this set (a measurement introduced in [3]).
However, as we will see in the next two sections, we found
that the impurity of the Î» frontiers is a more discriminating
feature.
Overlap. For Va , Vb âŠ‚ V , the overlap (O(Va , Vb )) is simply
the number of shared communities. Formally: O(Va , Vb ) =
1 http://www.wise2012.cs.ucy.ac.cy/challenge.html

|C(Va ) âˆ© C(Vb )|. We study overlap between adopters and Î»
frontiers, between adopters and Î» non-adopters, and between
Î» frontiers and Î» non-adopters: O(VÎ¸m , FÎ¸m,Î» ), O(VÎ¸m , FÌ„Î¸m,Î» ),
and O(FÎ¸m,Î» , FÌ„Î¸m,Î» ) respectively. The intuition with overlap
stems directly from the original structural diversity results
of [?] - for instance a high overlap between adopters and
Î» frontiers may indicate that the Î» frontiers are linked to
adopters with inner-community connections and high structural
diversity - hence increasing the probability of adoption.
Average time to adoption. The average time to adoption for
the nodes in P
the current set of adopters (once the cascade grows
m
tiÎ¸
to size m): i=1
. We also use average time to adoption as
m
a baseline measure.
III.

R ESULTS

Here we examine the behavior of the various structural diversity measurements as viral and non-viral cascades
progress. We define a cascade as viral if the number of
reposts reaches a threshold (denoted T H) of 500 (in the
next section we will explore other settings for T H when
describing our classification results). We look at snapshots of
the cascades as they progress both in terms of size (denoted
m). For m = {10, 30, 50, 100, 200}, the number of samples
is {98832, 26733, 13285, 4722, 1324} respectively with 208 of
the samples are viral. With each size m we consider the
m
Cascades with m adopters at some time tm
Î¸ , tÎ¸ can vary for
different Î¸. Hence, cascades with final size N < m are ignored
in our analysis task. This leads to a decrease in the number of
non-viral Cascades as m increases.
Average time to adoption. As a baseline measurement, we
study the average time to adoption for each size-based stage
of the cascade process (Fig. 1i, Fig. 1j). As expected, viral
cascades exhibit a faster rate of reposting. While we note that
significant differences are present - especially in the early
stages of the cascade, the whiskers of the non-viral class
indicate a significant proportion of non-viral cascades that
exhibit rapid adoption. We believe this is likely due to the fact
that certain cascades may have very high appeal to specialized
communities.
Number of communities. Fig. 1a, Fig. 1b, Fig. 1c and Fig. 1d
display how the number of communities K(V 0 ) n
increases over
o
m = {10, 30, 50, 100, 200} for the sets V 0 = VÎ¸m , FÎ¸m,Î» .
We note that K(VÎ¸m ) (the communities represented in the
set of adopters) was shown to be a useful feature in [3]
for tasks where the target class had fewer reposts than in
this study. Here, we note that while statistically significant
differences exist, the average and median values at each of the
examined stages are generally similar. On the other hand, the
communities represented by the set of Î» frontiers (K(FÎ¸m,Î» ))
shows viral Cascades have stronger capability than non-viral
ones to keep a diverse set of Î» frontiers. We also noted that the
median of K(FÌ„Î¸m,Î» ) (not pictured) shows viral cascades start
with smaller K(FÎ¸m,Î» ). However, it increases faster in viral
cascades as nodes in Î» frontiers becomes Î» non-adopters.
Gini impurity. Cascades in both classes tend to accumulate
diversity in the process of collecting more adopters - and we
have also noted that a related entropy measure (studied in [3])
performed similarly. We also noted (not pictured) that in the

Cm

Pm
i
i=1 tÎ¸
m

, m = 50

early stages, viral cascades can show more diversity in Î» frontiers measured by IG (FÎ¸m,Î» ) (m = {10, 30, 50}). But, perhaps
most striking, that non-viral Cascades gain more uniformly
distributed nodes over communities in Î» non-adopters, shown
by IG (FÌ„Î¸m,Î» ) (Fig. 1g, Fig. 1h). We believe that this is due to
non-viral cascades likely have an appeal limited to a relatively
small number of communities - hence those not adopting the
trend may represent a more diverse set of communities.
Overlap. We found that overlap grows with the number
of adopters in the three types of overlap considered. For
O(VÎ¸m , FÎ¸m ), viral cascades start with a larger initial value
and keep leading non-viral ones in the diffusion process of first
200 nodes (Fig. 1e, Fig. 1f). This may hint that viral cascades
also take advantage of the densely linked communities to help
them become viral. However, in the case of O(VÎ¸m , FÌ„Î¸m ) and
O(FÎ¸m,Î» , FÌ„Î¸m,Î» ), viral cascades begin with lower value but
grow much faster than non-viral Cascades.
Classification Experiments. Here we examine our experiments for predicting whether a cascade becomes viral - when
a size threshold (T H) exceeds 500 adopters given that the
cascade has 50 adopters (s = 50). Based on the distribution
of final size of cascades in this dataset, this is a binary
classification task with two heavily imbalanced classes. Hence,
we report performance measurements (precision, recall and
F1 score) for only the minority (viral) class. Throughout the
course of our experiments, we found that varying threshold
(slightly modifying the definition of â€œviralâ€) for only the training set allows for a trade-off between precision and recall. We
study the trend of performance measures in two cases: (1.) The
threshold for test set is maintained as T Hts = 500 while the
training threshold is varied T Htr = {300, 400, 500, 600, 700}.
(2.) The two thresholds are kept as the same T H while we
modify this value T H = {300, 400, 500, 600, 700}.
Table I shows the groups of features used in our prediction
tasks. The features introduced in this paper is group Am . As
a baseline method for size-based prediction (feature group
Cm ) we used average time to adoption. We also compare our
features (Group Am ) with the community features extracted
in [3] (Group Bm ). This was the best performing feature set
in that paper for a comparable task.2 Additionally, we study
the average size of recalled and non-recalled viral cascades by
classifiers using features in groups Am . We also investigate
the significance and performance of individual and certain
combinations of features introduced in this paper.
We used ten-fold cross-validation in our experiments to
2 This was their highest-performing set of features for predicting cascades
that grew from 50 to 367 and 100 to 417 reposts. We also included the
baseline feature in this set as we found it improved the effectiveness of this
approach.

10

30

50

100

Number of Adopters

48.0
47.6

200

(a) Number of communities amongst
adopters (K(VÎ¸m )) for non-viral cascades

1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0

M: 7.0
A: 25.7

10

15.0
39.6

20.0
53.2

30

50

100

Number of Adopters

200

M: 3.0
A: 3.7

9.0
8.7

12.0
12.3

19.0
18.5

1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0

60

50

50

40

40

30
10
0

30

50

100

Number of Adopters

34.0
34.0

46.5
46.5

30

50

100

200

Number of Adopters

M: 21.0
A: 24.3

10

30.0
41.7

30.0
44.4

33.5
78.7

42.5
88.6

30

50

100

200

Number of Adopters

M: 7.0
A: 6.7

13.0
12.7

17.0
16.5

22.5
22.2

31.0
29.5

30

50

100

200

30
20

M: 0.8
A: 0.8

0.9
0.9

0.9
0.9

0.9
0.9

0

200

10

Number of Adopters

(f) Overlap of adopters and Î» frontiers (O(VÎ¸m , FÎ¸m,Î» )) for viral cascades

0.9
0.9

1.0

0.8

M: 0.0
A: 0.4

0.9
0.8

0.9
0.9

0.9
0.9

0.9
0.9

30

50

100

200

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.0

0.0

10

30

50

100

Number of Adopters

200

(g) Gini impurity of Î» non-adopters
(IG (FÌ„Î¸m,Î» )) for non-viral cascades

1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0

23.0
23.5

10
10

(e) Overlap of adopters and Î» frontiers (O(VÎ¸m , FÎ¸m,Î» )) for non-viral
cascades

1.0

10

17.0
17.3

(d) Number of communities amongst
Î» frontiers (K(FÎ¸m,Î» )) for viral cascades

26.0
25.2

20

M: 8.0
A: 8.1

(b) Number of communities amongst
adopters (K(VÎ¸m )) for viral cascades

27.0 33.0
88.5 111.1

(c) Number of communities amongst
Î» frontiers (K(FÎ¸m,Î» )) for non-viral
cascades

60

70
60
50
40
30
20
10
0

Number of Communities

Community Features Mentioned in [3] and Cm

35.0
34.9

Overlap

, m âˆˆ {30, 50}

Bm

25.0
24.0

Gini

Pm
i
i=1 tÎ¸
m

18.0
17.5

M: 865.9
A: 780.3

10

853.1 804.5 754.6 765.2
790.1 771.0 753.8 759.9

30

50

100

Number of Adopters

(i) Non-viral cascades

200

10

Number of Adopters

(h) Gini impurity of Î» non-adopters
(IG (FÌ„Î¸m,Î» )) for viral cascades

Average Time(103 )

|FÎ¸m,Î» |, |FÌ„Î¸m,Î» |,

Number of Communities

O(VÎ¸m , FÎ¸m,Î» ),O(VÎ¸m , FÌ„Î¸m,Î» ),O(FÎ¸m,Î» , FÌ„Î¸m,Î» ),

Number of Communities(102 )

Am

Overlap

K(FÎ¸m,Î» ),K(FÌ„Î¸m,Î» ),IG (VÎ¸m ),IG (FÎ¸m,Î» ),IG (FÌ„Î¸m,Î» ),

M: 8.0
A: 7.7

Gini

Feature(s) over size

Average Time(103 )

Group

70
60
50
40
30
20
10
0

Number of Communities(102 )

TABLE I: Features: Cascade Prediction over Time and Size

1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0

M: 15.3
A: 40.9

10

49.7 78.4 168.1 301.1
86.9 129.4 215.8 347.7

30

50

100

Number of Adopters

200

(j) Viral cascades

Fig. 1: Number of communities, gini impurity, overlap and average time since t0Î¸ to adoption for m = {10, 30, 50, 100, 200}

Am
Bm

Cm

precision

1.0

recall

f1 score

0.8

Size (103 )

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0 Precision

0.6
0.4
Recall

F1 Score

Fig. 2: Classification results based on groups of features
(Am ,Bm ,Cm ) extracted when m = 50 for fixed T Htr = 500,
T Hts = 500. Error bars represent one standard deviation.
ensure the results do not take any advantage of randomness in picking training and testing sets. First we carried
out the prediction tasks with fixed thresholds T Htr =
500, T Hts = 500. Then we modify the training threshold T Htr = {300, 400, 500, 600, 700} to show how this
achieves a trade-off between precision and recall. The difference in average final size between correctly classified viral
cascades and incorrectly classified ones is also monitored
over T Htr = {300, 400, 500, 600, 700} to show the potential
to predict exact number of adopters by features. Furthermore, we modify threshold of both training and testing sets
T H = {300, 400, 500, 600, 700} to show the robustness of
our features on related classification problems. We used the
oversampling method SMOTE with random forest classifier to
generate synthetic samples for the viral class. Other, lesserperforming classifiers were also examined (including SVM,
MLP, and other ensemble methods) and are not reported here.
All results shown in this section is a sample mean produced by
ten repeated experiments under each combination of variables.
Size-based prediction. We studied cascades of size 50 that
reached 500 for this task. There are 13,285 cascades that can
reach the size m = 50 while 208 out of them reached the size
of 500. Maintaining the threshold T H = 500, Fig. 2 shows
random forest classifier trained with features in group Am can
outperform the other groups. The trade-off between precision
and recall can be achieved by changing the training threshold T Htr while maintaining the testing threshold (Fig. 3a).
We also note that the average final size of viral cascades
recalled by the classifier increases with the training threshold
(Fig. 3b). With threshold T H = {300, 400, 500, 600, 700}
on both training and testing samples, the features of group
Am consistently outperform those previously introduced (Bm )
(Fig. 3c, Fig. 3d).
Feature investigation. Here we investigate the importance of
each feature in Am . With T Htr = 500 and T Hts = 500,
we trained 100 randomized logistic regressions models - each
assigning weights to the features in those sets. We then
categorized the features with weight larger than 0.01 (on
average) into groups such as overlap, gini impurity, etc. Then,
we performed classification on the basis of single feature
categories or combination of such categories. The average
weights assigned are shown in Table II. As shown by these
results, overlaps can make significant contribution to cascade
prediction. Intuitively, communication between two sets of
nodes is more likely to happen in their shared communities which is consistent with the results of [?]. This implies that the
larger overlap value, the more influence of one set on the other.
For example, we can infer that viral cascades tend to have

0.2

300

400
500 600
Training Threshold

700

1.4
recalled
not recalled
1.3
1.2
1.1
1.0
0.9
0.8
0.7
0.6 300
400
500
600
Training Threshold

mean

700

(a) Results for features in Am with (b) Average final size of viral cascades
different T Htr .
(recalled, mean and not recalled)
1.0

precision

recall

f1 score

1.0

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.0 300

400
500 600 700
Training/Testing Threshold

precision

0.0 300

recall

f1 score

400
500 600 700
Training/Testing Threshold

(c) Results for features in group Am (d) Results for features in group Bm
when T Htr and T Hts change.
when T Htr and T Hts change.

Fig. 3: Prediction results for Am when T Htr and T Hts
change. Error bars represent one standard deviation.

Name
Gini

Baseline

Features

Weights

IG (FÎ¸50,Î» )
IG (FÌ„Î¸50,Î» )
IG (FÌ„Î¸30,Î» )
P50
i
i=1 tÎ¸
50

0.02
0.02
0.52

Name

Features

Weights

O(VÎ¸30 , FÎ¸30,Î» )

0.50

O(VÎ¸30 , FÌ„Î¸30,Î» )

0.04

Overlap O(F 30,Î» , FÌ„ 30,Î» )
Î¸
Î¸

1.00

0.23

O(VÎ¸50 , FÎ¸50,Î» )

0.50

O(FÎ¸50,Î» , FÌ„Î¸50,Î» )

0.26

TABLE II: Weights of features assigned by randomized logistic
regression models
larger O(VÎ¸m , FÎ¸m,Î» ) value for adopters have larger chance to
influence the Î» frontiers than non-viral cascades. Moverover,
the gini impurity of Î» non-adopters also shows its importance.
Intuitively, non-viral cascades are easier to be trapped in a
relatively small amount of communities. This means even if
they could show up in peopleâ€™s timeline with high structural
diversity but can not get them infected.
IV.

ACKNOWLEDGMENT

This work is supported through the AFOSR Young Investigator Program (YIP), grant number FA9550-15-1-0159.
R EFERENCES
[1]

J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec,
â€œCan cascades be predicted?â€ in Proceedings of the 23rd international
conference on World wide web.
International World Wide Web
Conferences Steering Committee, 2014, pp. 925â€“936.
[2] P. Shakarian, A. Bhatnagar, A. Aleali, R. Guo, and E. Shaabani,
Diffusion in Social Networks. Springer (in press), 2015. [Online].
Available: http://lab.engineering.asu.edu/cysis/diffusion/
[3] L. Weng, F. Menczer, and Y.-Y. Ahn, â€œPredicting successful memes
using network and community structure,â€ in Eighth International AAAI
Conference on Weblogs and Social Media, 2014.

Belief Revision in
Structured Probabilistic Argumentation
Paulo Shakarian1 , Gerardo I. Simari2 , and Marcelo A. Falappa3

arXiv:1401.1475v1 [cs.LO] 7 Jan 2014

1

2

Department of Electrical Engineering and Computer Science
U.S. Military Academy, West Point, NY, USA
paulo@shakarian.net
Department of Computer Science, University of Oxford, United Kingdom
gerardo.simari@cs.ox.ac.uk
3
Departamento de Ciencias e IngenierÄ±Ìa de la ComputacioÌn
Universidad Nacional del Sur, BahÄ±Ìa Blanca, Argentina
mfalappa@cs.uns.edu.ar

Abstract. In real-world applications, knowledge bases consisting of all
the information at hand for a specific domain, along with the current
state of affairs, are bound to contain contradictory data coming from
different sources, as well as data with varying degrees of uncertainty
attached. Likewise, an important aspect of the effort associated with
maintaining knowledge bases is deciding what information is no longer
useful; pieces of information (such as intelligence reports) may be outdated, may come from sources that have recently been discovered to be
of low quality, or abundant evidence may be available that contradicts
them. In this paper, we propose a probabilistic structured argumentation
framework that arises from the extension of Presumptive Defeasible Logic
Programming (PreDeLP) with probabilistic models, and argue that this
formalism is capable of addressing the basic issues of handling contradictory and uncertain data. Then, to address the last issue, we focus on the
study of non-prioritized belief revision operations over probabilistic PreDeLP programs. We propose a set of rationality postulates â€“ based on
well-known ones developed for classical knowledge bases â€“ that characterize how such operations should behave, and study a class of operators
along with theoretical relationships with the proposed postulates, including a representation theorem stating the equivalence between this class
and the class of operators characterized by the postulates.

1

Introduction and Related Work

Decision-support systems that are part of virtually any kind of real-world application must be part of a framework that is rich enough to deal with several
basic problems: (i) handling contradictory information; (ii) answering abductive queries; (iii) managing uncertainty; and (iv) updating beliefs. Presumptions
come into play as key components of answers to abductive queries, and must be
maintained as elements of the knowledge base; therefore, whenever candidate answers to these queries are evaluated, the (in)consistency of the knowledge base

together with the presumptions being made needs to be addressed via belief
revision operations.
In this paper, we begin by proposing a framework that addresses items (i)â€“
(iii) by extending Presumptive DeLP [1] (PreDeLP, for short) with probabilistic
models in order to model uncertainty in the application domain; the resulting
framework is a general-purpose probabilistic argumentation language that we
will refer to as Probabilistic PreDeLP(P-PreDeLP, for short).
In the second part of this paper, we address the problem of updating beliefs â€“
item (iv) above â€“ in P-PreDeLP knowledge bases, focusing on the study of nonprioritized belief revision operations. We propose a set of rationality postulates
characterizing how such operations should behave â€“ these postulates are based
on the well-known postulates proposed in [2] for non-prioritized belief revision in
classical knowledge bases. We then study a class of operators and their theoretical
relationships with the proposed postulates, concluding with a representation
theorem.

Related Work. Belief revision studies changes to knowledge bases as a response
to epistemic inputs. Traditionally, such knowledge bases can be either belief sets
(sets of formulas closed under consequence) [3, 4] or belief bases [5, 2] (which are
not closed); since our end goal is to apply the results we obtain to real-world
domains, here we focus on belief bases. In particular, as motivated by requirements (i)â€“(iv) above, our knowledge bases consist of logical formulas over which
we apply argumentation-based reasoning and to which we couple a probabilistic model. The connection between belief revision and argumentation was first
studied in [6]; since then, the work that is most closely related to our approach
is the development of the explanation-based operators of [7].
The study of argumentation systems together with probabilistic reasoning
has recently received a lot attention, though a significant part has been in the
combination between the two has been in the form of probabilistic abstract argumentation [8â€“11]. There have, however, been several approaches that combine
structured argumentation with models for reasoning under uncertainty; the first
of such approaches to be proposed was [12], and several others followed, such
as the possibilistic approach of [13], and the probabilistic logic-based approach
of [14]. The main difference between these works and our own is that here we
adopt a bipartite knowledge base, where one part models the knowledge that is
not inherently probabilistic â€“ uncertain knowledge is modeled separately, thus
allowing a clear separation of interests between the two kinds of models. This
approach is based on a similar one developed for ontological languages in the
Semantic Web (see [15], and references within).
Finally, to the best of our knowledge, this is the first paper in which the combination of structured argumentation, probabilistic models, and belief revision
has been addressed in conjunction.

Probabilistic Model (EM)
Analytical Model (AM)
â€œMalware X was compiled on a system
â€œMalware X was compiled on a system in
using the English language.â€
English-speaking country Y.â€
â€œCounty Y and country Z are
â€œCountry Y has a motive to launch a
currently at war.â€
cyber-attack against country Z
â€œMalware W and malware X were created â€œMalware W and malware X are related.
in a similar coding style.â€
Table 1. Examples of the kind of information that could be represented in the two
different models in a cyber-security application domain.

2

Preliminaries

The Probabilistic PreDeLP (P-PreDeLP, for short) framework is composed of
two separate models of the world. The first is called the environmental model
(referred to as â€œEMâ€), and is used to describe the probabilistic knowledge that
we have about the domain. The second one is called the analytical model (referred
to as â€œAMâ€), and is used to analyze competing hypotheses that can account for
a given phenomenon â€“ what we will generally call queries. The AM is composed
of a classical (that is, non-probabilistic) PreDeLP program in order to allow for
contradictory information, giving the system the capability to model competing
explanations for a given query.
Two Kinds of Uncertainty. In general, the EM contains knowledge such as
evidence, uncertain facts, or knowledge about agents and systems. The AM, on
the other hand, contains ideas that a user may conclude based on the information in the EM. Table 1 gives some examples of the types of information that
could appear in each of the two models in a cyber-security application. Note that
a knowledge engineer (or automated system) could assign a probability to statements in the EM column, whereas statements in the AM column can be either
true or false depending on a certain combination (or several possible combinations) of statements from the EM. There are thus two kinds of uncertainty that
need to be modeled: probabilistic uncertainty and uncertainty arising from defeasible knowledge. As we will see, our model allows both kinds of uncertainty to
coexist, and also allows for the combination of the two since defeasible rules and
presumptions (that is, defeasible facts) can also be annotated with probabilistic
events.
In the rest of this section, we formally describe these two models, as well as
how knowledge in the AM can be annotated with information from the EM â€“
these annotations specify the conditions under which the various statements in
the AM can potentially be true.
Basic Language. We assume sets of variable and constant symbols, denoted
with V and C, respectively. In the rest of this paper, we will use capital letters
to represent variables (e.g., X, Y, Z), while lowercase letters represent constants.
The next component of the language is a set of n-ary predicate symbols; the
EM and AM use separate sets of predicate symbols, denoted with PEM , PAM ,

respectively â€“ the two models can, however, share variables and constants. As
usual, a term is composed of either a variable or constant. Given terms t1 , ..., tn
and n-ary predicate symbol p, p(t1 , ..., tn ) is called an atom; if t1 , ..., tn are constants, then the atom is said to be ground. The sets of all ground atoms for EM
and AM are denoted with GEM and GAM , respectively.
Given set of ground atoms, a world is any subset of atoms â€“ those that belong to the set are said to be true in the world, while those that do not are
false. Therefore, there are 2|GEM | possible worlds in the EM and 2|GAM | worlds
in the AM. These sets are denoted with WEM and WAM , respectively. In order to avoid worlds that do not model possible situations given a particular
domain, we include integrity constraints of the form oneOf(Aâ€² ), where Aâ€² is a
subset of ground atoms. Intuitively, such a constraint states that any world where
more than one of the atoms from set Aâ€² appears is invalid. We use ICEM and
ICAM to denote the sets of integrity constraints for the EM and AM, respectively, and the sets of worlds that conform to these constraints is denoted with
WEM (ICEM ), WAM (ICAM ), respectively.
Finally, logical formulas arise from the combination of atoms using the traditional connectives (âˆ§, âˆ¨, and Â¬). As usual, we say a world w satisfies formula
(f ), written w |= f , iff: (i) If f is an atom, then w |= f iff f âˆˆ w; (ii) if f = Â¬f â€²
then w |= f iff w 6|= f â€² ; (iii) if f = f â€² âˆ§ f â€²â€² then w |= f iff w |= f â€² and w |= f â€²â€² ;
and (iv) if f = f â€² âˆ¨ f â€²â€² then w |= f iff w |= f â€² or w |= f â€²â€² . We use the notation
formEM , formAM to denote the set of all possible (ground) formulas in the EM
and AM, respectively.
2.1

Probabilistic Model

The EM or environmental model is largely based on the probabilistic logic of [16],
which we now briefly review.
Definition 1. Let f be a formula over PEM , V, and C, p âˆˆ [0, 1], and Ç« âˆˆ
[0, min(p, 1 âˆ’ p)]. A probabilistic formula is of the form f : p Â± Ç«. A set KEM of
probabilistic formulas is called a probabilistic knowledge base.
In the above definition, the number Ç« is referred to as an error tolerance. Intuitively, probabilistic formulas are interpreted as â€œformula f is true with probability between p âˆ’ Ç« and p + Ç«â€ â€“ note that there are no further constraints over
this interval apart from those imposed by other probabilistic formulas in the
knowledge base. The uncertainty regarding the probability values stems from
the fact that certain assumptions (such as probabilistic independence) may not
be suitable in the environment being modeled.
Example 1. Consider the following set KEM :
f1 = a : 0.8 Â± 0.1
f2 = b : 0.2 Â± 0.1
f3 = c : 0.8 Â± 0.1

f4 = d âˆ§ e
: 0.7 Â± 0.2
f5 = f âˆ§ g âˆ§ h : 0.6 Â± 0.1
f6 = i âˆ¨ Â¬j
: 0.9 Â± 0.1

â€²
Throughout the paper, we also use KEM
= {f1 , f2 , f3 }

f7 = k : 1 Â± 0



A set of probabilistic formulas describes a set of possible probability distributions Pr over the set WEM (ICEM ). We say that
Pprobability distribution Pr
satisfies probabilistic formula f : p Â± Ç« iff: p âˆ’ Ç« â‰¤ wâˆˆWEM (ICEM ) Pr (w) â‰¤ p + Ç«.
We say that a probability distribution over WEM (ICEM ) satisfies KEM iff it
satisfies all probabilistic formulas in KEM .
Given a probabilistic knowledge base and a (non-probabilistic) formula q,
the maximum entailment problem seeks to identify real numbers p, Ç« such that
all valid probability distributions Pr that satisfy KEM also satisfy q : p Â± Ç«, and
there does not exist pâ€² , Ç«â€² s.t. [p âˆ’ Ç«, p + Ç«] âŠƒ [pâ€² âˆ’ Ç«â€² , pâ€² + Ç«â€² ], where all probability
distributions Pr that satisfy KEM also satisfy q : pâ€² Â± Ç«â€² . In order to solve this
problem we must solve the linear program defined below.
Definition 2. Given a knowledge base KEM and a formula q, we have a variable
xi for each wi âˆˆ WEM (ICEM ).
â€“ For each fj : pj Â± Ç«j âˆˆ KEM , there is a constraint of the form:
P
pj âˆ’ Ç«j â‰¤ wi âˆˆWEM (ICEM ) s.t. wi |=fj xi â‰¤ pj + Ç«j .
P
â€“ We also have the constraint: wi âˆˆWEM (ICEM ) xi = 1.
P
â€“ The objective is to minimize the function: wi âˆˆWEM (ICEM ) s.t. wi |=q xi .
We use the notation EP-LP-MIN(KEM , q) to refer to the value of the objective
function in the solution to the EM-LP-MIN constraints.
The next step is to solve the linear program a second time, but instead
maximizing the objective function (we shall refer to this as EM-LP-MAX) â€“ let
â„“ and u be the results of these operations, respectively. In [16], it is shown that
Ç« = uâˆ’â„“
and p = â„“ + Ç« is the solution to the maximum entailment problem.
2
We note that although the above linear program has an exponential number
of variables in the worst case (i.e., no integrity constraints), the presence of
constraints has the potential to greatly reduce this space. Further, there are also
good heuristics (cf. [17, 18]) that have been shown to provide highly accurate
approximations with a reduced-size linear program.
â€²
Example 2. Consider KB KEM
from Example 1 and a set of ground atoms restricted to those that appear in that program; we have the following worlds:

w1 = {a, b, c}
w5 = {b}

w2 = {a, b}
w6 = {a}

w3 = {a, c}
w7 = {c}

w4 = {b, c}
w8 = âˆ…

and suppose we wish to compute the probability for formula q = a âˆ¨ c. For each
formula in KEM we have a constraint, and for each world above we have a variable. An objective function is created based on the worlds that satisfy the query
â€²
formula (in this case, worlds w1 , w2 , w3 , w4 , w6 , w7 ). Solving EP-LP-MAX(KEM
, q)
â€²
and EP-LP-MIN(KEM , q), we obtain the solution 0.9 Â± 0.1.


3

Argumentation Model

For the analytical model (AM), we choose a structured argumentation framework [19] due to several characteristics that make such frameworks highly applicable to many domains. Unlike the EM, which describes probabilistic information about the state of the real world, the AM must allow for competing ideas.
Therefore, it must be able to represent contradictory information. The algorithmic approach we shall later describe allows for the creation of arguments based
on the AM that may â€œcompeteâ€ with each other to answer a given query. In this
competition â€“ known as a dialectical process â€“ one argument may defeat another
based on a comparison criterion that determines the prevailing argument. Resulting from this process, certain arguments are warranted (those that are not
defeated by other arguments) thereby providing a suitable explanation for the
answer to a given query.
The transparency provided by the system can allow knowledge engineers
to identify potentially incorrect input information and fine-tune the models or,
alternatively, collect more information. In short, argumentation-based reasoning
has been studied as a natural way to manage a set of inconsistent information â€“ it
is the way humans settle disputes. As we will see, another desirable characteristic
of (structured) argumentation frameworks is that, once a conclusion is reached,
we are left with an explanation of how we arrived at it and information about why
a given argument is warranted; this is very important information for users to
have. In the following, we first recall the basics of the underlying argumentation
framework used, and then go on to introduce the analytical model (AM).
3.1

Defeasible Logic Programming with Presumptions (PreDeLP)

Defeasible Logic Programming with Presumptions (PreDeLP) [1] is a formalism
combining logic programming with defeasible argumentation; it arises as an extension of classical DeLP [20] with the possibility of having presumptions, as
described below â€“ since this capability is useful in many applications, we adopt
this extended version in this paper. In this section, we briefly recall the basics
of PreDeLP; we refer the reader to [20, 1] for the complete presentation.
The formalism contains several different constructs: facts, presumptions, strict
rules, and defeasible rules. Facts are statements about the analysis that can always be considered to be true, while presumptions are statements that may or
may not be true. Strict rules specify logical consequences of a set of facts or
presumptions (similar to an implication, though not the same) that must always
occur, while defeasible rules specify logical consequences that may be assumed
to be true when no contradicting information is present. These building blocks
are used in the construction of arguments, and are part of a PreDeLP program,
which is a set of facts, strict rules, presumptions, and defeasible rules. Formally,
we use the notation Î AM = (Î˜, â„¦, Î¦, âˆ†) to denote a PreDeLP program, where
â„¦ is the set of strict rules, Î˜ is the set of facts, âˆ† is the set of defeasible rules,
and Î¦ is the set of presumptions. In Figure 1, we provide an example Î AM . We
now define these constructs formally.

Î˜ : Î¸1a = p

Î¸1b = q

Î¸2 = r

â„¦ : Ï‰1a = Â¬s â† t

Ï‰1b = Â¬t â† s

Ï‰2a = s â† p, u, r, v

Î¦ : Ï†1 = y

Ï†2 = v

Ï†3 = Â¬z

â€“â‰º

âˆ† : Î´1a = s â€“â‰º p
Î´4 = u â€“â‰º y

â€“â‰º

Î´1b = t â€“â‰º q
Î´5a = Â¬u â€“â‰º Â¬z

Ï‰2b = t â† q, w, x, v

â€“â‰º

Î´2 = s â€“â‰º u
Î´5b = Â¬w â€“â‰º Â¬n

Î´3 = s â€“â‰º r, v

Fig. 1. An example (propositional) argumentation framework.

Facts (Î˜) are ground literals representing atomic information or its negation,
using strong negation â€œÂ¬â€. Note that all of the literals in our framework must
be formed with a predicate from the set PAM . Note that information in the form
of facts cannot be contradicted. We will use the notation [Î˜] to denote the set
of all possible facts.
Strict Rules (â„¦) represent non-defeasible cause-and-effect information that resembles an implication (though the semantics is different since the contrapositive
does not hold) and are of the form L0 â† L1 , . . . , Ln , where L0 is a ground literal
and {Li }i>0 is a set of ground literals. We will use the notation [â„¦] to denote
the set of all possible strict rules.
Presumptions (Î¦) are ground literals of the same form as facts, except that
they are not taken as being true but rather defeasible, which means that they
can be contradicted. Presumptions are denoted in the same manner as facts,
except that the symbol â€“â‰º is added.
Defeasible Rules (âˆ†) represent tentative knowledge that can be used if nothing
can be posed against it. Just as presumptions are the defeasible counterpart of
facts, defeasible rules are the defeasible counterpart of strict rules. They are of
the form L0 â€“â‰º L1 , . . . , Ln , where L0 is a ground literal and {Li }i>0 is a set of
ground literals. In both strict and defeasible rules, strong negation is allowed in
the head of rules, and hence may be used to represent contradictory knowledge.
Even though the above constructs are ground, we allow for schematic versions
with variables that are used to represent sets of ground rules. We denote variables
with strings starting with an uppercase letter.
Arguments. Given a query in the form of a ground atom, the goal is to derive
arguments for and against itâ€™s validity â€“ derivation follows the same mechanism of logic programming [21]. Since rule heads can contain strong negation,
it is possible to defeasibly derive contradictory literals from a program. For the
treatment of contradictory knowledge, PreDeLP incorporates a defeasible argumentation formalism that allows the identification of the pieces of knowledge
that are in conflict and, through the previously mentioned dialectical process,
decides which information prevails as warranted. This dialectical process involves

hA1 , si
hA3 , si
hA5 , ui
hA7 , Â¬ui

A1
A3
A5
A7

= {Î¸1a , Î´1a }
= {Ï†1 , Î´2 , Î´4 }
= {Ï†1 , Î´4 }
= {Ï†3 , Î´5a }

hA2 , si A2 = {Ï†1 , Ï†2 , Î´4 , Ï‰2a , Î¸1a , Î¸2 }
hA4 , si A4 = {Ï†2 , Î´3 , Î¸2 }
hA6 , Â¬si A6 = {Î´1b , Î¸1b , Ï‰1a }

Fig. 2. Example ground arguments from the framework of Figure 1.

the construction and evaluation of arguments, building a dialectical tree in the
process. Arguments are formally defined next.
Definition 3. An argument hA, Li for a literal L is a pair of the literal and
a (possibly empty) set of the EM (A âŠ† Î AM ) that provides a minimal proof
for L meeting the following requirements: (i) L is defeasibly derived from A;
(ii) â„¦ âˆª Î˜ âˆª A is not contradictory; and (iii) A is a minimal subset of âˆ† âˆª Î¦
satisfying 1 and 2, denoted hA, Li.
Literal L is called the conclusion supported by the argument, and A is the
support of the argument. An argument hB, Li is a subargument of hA, Lâ€² i iff
B âŠ† A. An argument hA, Li is presumptive iff A âˆ© Î¦ is not empty. We will also
use â„¦(A) = A âˆ© â„¦, Î˜(A) = A âˆ© Î˜, âˆ†(A) = A âˆ© âˆ†, and Î¦(A) = A âˆ© Î¦.
Our definition differs slightly from that of [22], where DeLP is introduced, as
we include strict rules and facts as part of arguments â€“ the reason for this will
become clear in Section 4. Arguments for our scenario are shown next.
Example 3. Figure 2 shows example arguments based on the knowledge base
from Figure 1. Note that hA5 , ui is a sub-argument of hA2 , si and hA3 , si.

Given an argument hA1 , L1 i, counter-arguments are arguments that contradict it. Argument hA2 , L2 i is said to counterargue or attack hA1 , L1 i at a
literal Lâ€² iff there exists a subargument hA, Lâ€²â€² i of hA1 , L1 i such that the set
â„¦(A1 ) âˆª â„¦(A2 ) âˆª Î˜(A1 ) âˆª Î˜(A2 ) âˆª {L2 , Lâ€²â€² } is contradictory.
Example 4. Consider the arguments from Example 3. The following are some of
the attack relationships between them: A1 , A2 , A3 , and A4 all attack A6 ; A5
attacks A7 ; and A7 attacks A2 .

A proper defeater of an argument hA, Li is a counter-argument that â€“ by
some criterion â€“ is considered to be better than hA, Li; if the two are incomparable according to this criterion, the counterargument is said to be a blocking
defeater. An important characteristic of PreDeLP is that the argument comparison criterion is modular, and thus the most appropriate criterion for the domain
that is being represented can be selected; the default criterion used in classical
defeasible logic programming (from which PreDeLP is derived) is generalized
specificity [23], though an extension of this criterion is required for arguments
using presumptions [1]. We briefly recall this criterion next â€“ the first definition is for generalized specificity, which is subsequently used in the definition of
presumption-enabled specificity.

Definition 4. Let Î AM = (Î˜, â„¦, Î¦, âˆ†) be a PreDeLP program and let F be
the set of all literals that have a defeasible derivation from Î AM . An argument
hA1 , L1 i is preferred to hA2 , L2 i, denoted with A1 â‰»P S A2 iff:
(1) For all H âŠ† F , â„¦(A1 ) âˆª â„¦(A2 ) âˆª H is non-contradictory: if there is a
derivation for L1 from â„¦(A2 ) âˆª â„¦(A1 ) âˆª âˆ†(A1 ) âˆª H, and there is no derivation
for L1 from â„¦(A1 ) âˆª â„¦(A2 ) âˆª H, then there is a derivation for L2 from â„¦(A1 ) âˆª
â„¦(A2 ) âˆª âˆ†(A2 ) âˆª H; and
(2) there is at least one set H â€² âŠ† F , â„¦(A1 ) âˆª â„¦(A2 ) âˆª H â€² is non-contradictory,
such that there is a derivation for L2 from â„¦(A1 ) âˆª â„¦(A2 ) âˆª H â€² âˆª âˆ†(A2 ), there
is no derivation for L2 from â„¦(A1 ) âˆª â„¦(A2 ) âˆª H â€² , and there is no derivation for
L1 from â„¦(A1 ) âˆª â„¦(A2 ) âˆª H â€² âˆª âˆ†(A1 ).
Intuitively, the principle of specificity says that, in the presence of two conflicting lines of argument about a proposition, the one that uses more of the available information is more convincing. A classic example involves a bird, Tweety,
and arguments stating that it both flies (because it is a bird) and doesnâ€™t fly (because it is a penguin). The latter argument uses more information about Tweety
â€“ it is more specific â€“ and is thus the stronger of the two.
Definition 5 ([1]). Let Î AM = (Î˜, â„¦, Î¦, âˆ†) be a PreDeLP program. An argument hA1 , L1 i is preferred to hA2 , L2 i, denoted with A1 â‰» A2 iff any of the
following conditions hold:
(1) hA1 , L1 i and hA2 , L2 i are both factual arguments and hA1 , L1 i â‰»P S hA2 , L2 i.
(2) hA1 , L1 i is a factual argument and hA2 , L2 i is a presumptive argument.
(3) hA1 , L1 i and hA2 , L2 i are presumptive arguments, and
(a) Î¦(A1 ) ( Î¦(A2 ) or,
(b) Î¦(A1 ) = Î¦(A2 ) and hA1 , L1 i â‰»P S hA2 , L2 i.
Generally, if A, B are arguments with rules X and Y , resp., and X âŠ‚ Y , then A
is stronger than B. This also holds when A and B use presumptions P1 and P2 ,
resp., and P1 âŠ‚ P2 .
Example 5. The following are some relationships between arguments from Example 3, based on Definitions 4 and 5.
A1 and A6 are incomparable (blocking defeaters);
A6 â‰» A2 , and thus A6 defeats A2 ;
A5 and A7 are incomparable (blocking defeaters).



A sequence of arguments called an argumentation line thus arises from this
attack relation, where each argument defeats its predecessor. To avoid undesirable sequences, which may represent circular argumentation lines, in DeLP an
argumentation line is acceptable if it satisfies certain constraints (see [20]). A
literal L is warranted if there exists a non-defeated argument A supporting L.
Clearly, there can be more than one defeater for a particular argument hA, Li.
Therefore, many acceptable argumentation lines could arise from hA, Li, leading to a tree structure. The tree is built from the set of all argumentation lines

rooted in the initial argument. In a dialectical tree, every node (except the root)
represents a defeater of its parent, and leaves correspond to undefeated arguments. Each path from the root to a leaf corresponds to a different acceptable
argumentation line. A dialectical tree provides a structure for considering all
the possible acceptable argumentation lines that can be generated for deciding
whether an argument is defeated. We call this tree dialectical because it represents an exhaustive dialectical4 analysis for the argument in its root. For a given
argument hA, Li, we denote the corresponding dialectical tree as T (hA, Li).
Given a literal L and an argument hA, Li, in order to decide whether or not a
literal L is warranted, every node in the dialectical tree T (hA, Li) is recursively
marked as â€œDâ€ (defeated ) or â€œUâ€ (undefeated ), obtaining a marked dialectical
tree T âˆ— (hA, Li) as follows:
1. All leaves in T âˆ— (hA, Li) are marked as â€œUâ€s, and
2. Let hB, qi be an inner node of T âˆ— (hA, Li). Then hB, qi will be marked as â€œUâ€
iff every child of hB, qi is marked as â€œDâ€. The node hB, qi will be marked as
â€œDâ€ iff it has at least a child marked as â€œUâ€.
Given an argument hA, Li obtained from Î AM , if the root of T âˆ— (hA, Li) is
marked as â€œUâ€, then we will say that T âˆ— (hA, hi) warrants L and that L is warranted from Î AM . (Warranted arguments correspond to those in the grounded
extension of a Dung argumentation system [24].) There is a further requirement
when the arguments in the dialectical tree contains presumptions â€“ the conjunction of all presumptions used in even (respectively, odd) levels of the tree must
be consistent. This can give rise to multiple trees for a given literal, as there can
potentially be different arguments that make contradictory assumptions.
We can then extend the idea of a dialectical tree to a dialectical forest. For
a given literal L, a dialectical forest F (L) consists of the set of dialectical trees
for all arguments for L. We shall denote a marked dialectical forest, the set of
all marked dialectical trees for arguments for L, as F âˆ— (L). Hence, for a literal
L, we say it is warranted if there is at least one argument for that literal in the
dialectical forest F âˆ— (L) that is labeled as â€œUâ€, not warranted if there is at least
one argument for the literal Â¬L in the dialectical forest F âˆ— (Â¬L) that is labeled
as â€œUâ€, and undecided otherwise.

4

Probabilistic PreDeLP

Probabilistic PreDeLP arises from the combination of the environmental and
analytical models (Î EM and Î AM , respectively). Intuitively, given Î AM , every
element of â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ might only hold in certain worlds in the set WEM â€“
that is, they are subject to probabilistic events. Therefore, we associate elements
of â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ with a formula from formEM . For instance, we could associate
formula rainy to fact umbrella to state that the latter only holds when the
probabilistic event rainy holds; since weather is uncertain in nature, it has been
modeled as part of the EM.
4

In the sense of providing reasons for and against a position.

af(Î¸1a ) = af(Î¸1b )
af(Î¸2 )
af(Ï‰1a ) = af(Ï‰1b )
af(Ï‰2a ) = af(Ï‰2b )
af(Ï†1 )
af(Ï†2 )

= k âˆ¨ f âˆ§ h âˆ¨ (e âˆ§ l)
=i
= True
= True
=câˆ¨a
=f âˆ§m



af(Ï†3 )
af(Î´1a ) = af(Î´1b )
af(Î´2 )
af(Î´3 )
af(Î´4 )
af(Î´5a ) = af(Î´5b )

=b
= True
= True
= True
= True
= True

Fig. 3. Example annotation function.

We can then compute the probabilities of subsets of â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ using the
information contained in Î EM , as we describe shortly. The notion of an annotation function associates elements of â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ with elements of formEM .
Definition 6. An annotation function is any function af : â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ â†’
formEM . We shall use [af ] to denote the set of all annotation functions.
We will sometimes denote annotation functions as sets of pairs (f, af(f )) in
order to simplify the presentation. Figure 3 shows an example of an annotation
function for our running example.
We now have all the components to formally define Probabilistic PreDeLP
programs (P-PreDeLP for short).
Definition 7. Given environmental model Î EM , analytical model Î AM , and
annotation function af , a probabilistic PreDeLP program is of the form I =
(Î EM , Î AM , af ). We use notation [I] to denote the set of all possible programs.
Given this setup, we can consider a world-based approach; that is, the defeat
relationship among arguments depends on the current state of the (EM) world.
Definition 8. Let I = (Î EM , Î AM , af ) be a P-PreDeLP program, argument
hA, Li is valid w.r.t. world w âˆˆ WEM iff âˆ€c âˆˆ A, w |= af(c).
We extend the notion of validity to argumentation lines, dialectical trees,
and dialectical forests in the expected way (for instance, an argumentation line
is valid w.r.t. w iff all arguments that comprise that line are valid w.r.t. w).
We also extend the idea of a dialectical tree w.r.t. worlds; so, for a given world
w âˆˆ WEM , the dialectical (resp., marked dialectical) tree induced by w is denoted
with Tw hA, Li (resp., Twâˆ— hA, Li). We require that all arguments and defeaters in
these trees to be valid with respect to w. Likewise, we extend the notion of
dialectical forests in the same manner (denoted with Fw (L) and Fwâˆ— (L), resp.).
Based on these concepts we introduce the notion of warranting scenario.
Definition 9. Let I = (Î EM , Î AM , af ) be a P-PreDeLP program and L be a
literal formed with a ground atom from GAM ; a world w âˆˆ WEM is said to be
a warranting scenario for L (denoted w âŠ¢war L) iff there is a dialectical forest
Fwâˆ— (L) in which L is warranted and Fwâˆ— (L) is valid w.r.t. w.

Hence, the set of worlds in the EM where a literal L in the AM must be true
is exactly the set of warranting scenarios â€“ these are the â€œnecessaryâ€ worlds:
nec(L) = {w âˆˆ WEM | (w âŠ¢war L)}. Now, the set of worlds in the EM where
AM literal L can be true is the following â€“ these are the â€œpossibleâ€ worlds:
poss(L) = {w âˆˆ WEM | w 6âŠ¢war Â¬L}. The probability distribution Pr defined
over the worlds in the EM induces an upper and lower bound on the probability
of literal L (denoted PL,Pr ,I ) as follows:
â„“L,Pr ,I =

X

Pr (w),

uL,Pr ,I =

wâˆˆnec(L)

X

Pr (w)

wâˆˆposs(L)

â„“L,Pr ,I â‰¤ PL,Pr ,I â‰¤ uL,Pr ,I
Since the EM in general does not define a single probability distribution, the
above computations should be done using linear programs EP-LP-MIN and EPLP-MAX, as described above.
4.1

Sources of Inconsistency

We use the following notion of (classical) consistency of PreDeLP programs: Î 
is said to be consistent if there does not exist ground literal a s.t. Î  âŠ¢ a and
Î  âŠ¢ Â¬a. For P-PreDeLP programs, there are two main kinds of inconsistency
that can be present; the first is what we refer to as EM, or Type I, (in)consistency.
Definition 10. Environmental model Î EM is Type I consistent iff there exists
a probability distribution Pr over the set of worlds WEM that satisfies Î EM .
We illustrate this type of consistency in the following example.
Example 6. The following formula is a simple example of an EM for which there
is no satisfying probability distribution:
rain âˆ¨ hail : 0.3 Â± 0;
rain âˆ§ hail : 0.5 Â± 0.1.
A P-PreDeLP program using such an EM gives rise to an example of Type I
inconsistency, as it arises from the fact that there is no satisfying interpretation
for the EM knowledge base.

Assuming a consistent EM, inconsistencies can still arise through the interaction between the annotation function and facts and strict rules. We will refer
to this as combined, or Type II, (in)consistency.
Definition 11. A P-PreDeLP program I = (Î EM , Î AM , af ), with Î AM =
hÎ˜, â„¦, Î¦, âˆ†i, is Type II consistent iff: given any probabilitySdistribution Pr that
satisfies Î EM , if there exists a world w âˆˆ WEM such that xâˆˆÎ˜âˆªâ„¦ | w|=af(x) {x}
is inconsistent, then we have Pr(w) = 0.

Thus, any EM world in which the set of associated facts and strict rules are
inconsistent (we refer to this as â€œclassical consistencyâ€) must always be assigned
a zero probability. The following is an example of this other type of inconsistency.
Example 7. Consider the EM knowledge base from Example 1, the AM presented
in Figure 1 and the annotation function from Figure 3. Suppose the following
fact is added to the argumentation model:
Î¸3 = Â¬p,
and that the annotation function is expanded as follows:
af (Î¸3 ) = Â¬k.
Clearly, fact Î¸3 is in direct conflict with fact Î¸1a â€“ this does not necessarily mean
that there is an inconsistency. For instance, by the annotation function, Î¸1a holds
in the world {k} while Î¸3 does not. However, if we consider the world:
w = {f, h)
Note that w |= af (Î¸3 ) and w |= af (Î¸2 ), which means that, in this world, two
contradictory facts can occur. Since the environmental model indicates that this
world can be assigned a non-zero probability, we have a Type II inconsist program.

Another example (perhaps easier to visualize) in the rain/hail scenario discussed
above, is as follows: suppose we have facts f = umbrella and g = Â¬umbrella,
and annotation function af (f ) = rain âˆ¨ hail and af (g) = wind. Intuitively, the
first fact states that an umbrella should be carried if it either rains or hails,
while the second states that an umbrella should not be carried if it is windy. If
the EM assigns a non-zero probability to formula (rain âˆ¨ hail) âˆ§ wind, then we
have Type II inconsistency.
In the following, we say that a P-PreDeLP program is consistent if and only
if it is both Type I and Type II consistent. However, in this paper, we focus on
Type II consistency and assume that the program is Type I consistent.
4.2

Basic Operations for Restoring Consistency

Given a P-PreDeLP program that is Type II inconsistent, there are two basic
strategies that can be used to restore consistency:
Revise the EM: the probabilistic model can be changed in order to force the
worlds that induce contradicting strict knowledge to have probability zero.
Revise the annotation function: The annotations involved in the inconsistency
can be changed so that the conflicting information in the AM does not become
induced under any possible world.
It may also appear that a third option would be to adjust the AM â€“ this is,
however, equivalent to modifying the annotation function. Consider the presence

of two facts in the AM: a, Â¬a. Assuming that this causes an inconsistency (that
is, there is at least one world in which they both hold), one way to resolve it
would be to remove one of these two literals. Suppose Â¬a is removed; this would
be equivalent to setting af(Â¬a) = âŠ¥ (where âŠ¥ represents a contradiction in the
language of the EM). In this paper, we often refer to â€œremoving elements of
Î AM â€ to refer to changes to the annotation function that cause certain elements
of the Î AM to not have their annotations satisfied in certain EM worlds.
Now, suppose that Î EM is consistent, but that the overall program is Type
II inconsistent. Then, there must exist a set of worlds in the EM where there is
a probability distribution that assigns each of them a non-zero probability. This
gives rise to the following result.
Proposition 1. If there exists a probability distribution PrSthat satisfies Î EM
s.t. there exists a world w âˆˆ WEM where Pr(w) > 0 and xâˆˆÎ˜âˆªâ„¦ | w|=af(x) {x}
is inconsistent (Type II inconsistency), then any change made in order to reâ€²
solve
yields a new EM Î EM
such that
 by modifying only Î EM
V
V this inconsistency
â€²
aâˆˆw
/ Â¬a : 0 Â± 0 is entailed by Î EM .
aâˆˆw a âˆ§
Proposition 1 seems to imply an easy strategy of adding formulas to Î EM
causing certain worlds to have a zero probability. However, this may lead to
â€²
Type I inconsistencies in the resulting model Î EM
. If we are applying an EM-only
strategy to resolve inconsistencies, this would then lead to further adjustments
â€²
to Î EM
in order to restore Type I consistency. However, such changes could
potentially lead to Type II inconsistency in the overall P-PreDeLP program
â€²
(by either removing elements of Î EM
or loosening probability bounds of the
â€²
sentences in Î EM ), which would lead to setting more EM worlds to a probability
of zero. It is easy to devise an example of a situation in which the probability
mass cannot be accommodated given the constraints imposed by the AM and
EM together â€“ in such cases, it would be impossible to restore consistency by
only modifying Î EM . We thus arrive at the following observation:
Observation 1 Given a Type II inconsistent P-PreDeLP program, consistency
cannot always be restored via modifications to Î EM alone.
Therefore, due to this line of reasoning, in this paper we focus our efforts on
modifications to the annotation function only. However, in the future, we intend
to explore belief revision operators that consider both the annotation function
(which, as we saw, captures changes to the AM) along with changes to the EM,
as well as combinations of the two.

5

Revising Probabilistic PreDeLP Programs

Given a P-PreDeLP program I = (Î EM , Î AM , af ), with Î AM = â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦,
we are interested in solving the problem of incorporating an epistemic input
(f, af â€² ) into I, where f is either an atom or a rule and af â€² is equivalent to
af , except for its expansion to include f . For ease of presentation, we assume

that f is to be incorporated as a fact or strict rule, since incorporating defeasible
knowledge can never lead to inconsistency. As we are only conducting annotation
function revisions, for I = (Î EM , Î AM , af ) and input (f, af â€² ) we denote the
â€²
â€²
revision as follows: I â€¢ (f, af â€² ) = (Î EM , Î AM
, af â€²â€² ) where Î AM
= Î AM âˆª {f }
â€²â€²
and af is the revised annotation function.
Notation. We use the symbol â€œâ€¢â€ to denote the revision operator. We also
slightly abuse notation for the sake of presentation, as well as introduce notation
to convert sets of worlds to/from formulas.
â€“ I âˆª (f, af â€² ) to denote I â€² = (Î EM , Î AM âˆª {f }, af â€² ).
â€“ (f, af â€² ) âˆˆ I = (Î AM , Î EM , af ) to denote f âˆˆ Î AM and af = af â€² .
â€“ wld(f ) = {w | w |= f } â€“ the set of worlds that satisfy formula f ; and
V
V
â€“ f or(w) = aâˆˆw a âˆ§ aâˆˆw
/ Â¬a â€“ the formula that has w as its only model.

I
â€“ Î AM
(w) = {f âˆˆ Î˜ âˆª â„¦ | w |= af(f )}

0
I
â€“ WEM
(I) = {w âˆˆ WEM | Î AM
(w) is inconsistent}
I
0
â€“ WEM
(I) = {w âˆˆ WEM
| âˆƒPr s.t. Pr |= Î EM âˆ§ Pr (w) > 0}
I
Intuitively, Î AM
(w) is the subset of facts and strict rules in Î AM whose annota0
tions are true in EM world w. The set WEM
(I) contains all the EM worlds for a
given program where the corresponding knowledge base in the AM is classically
I
inconsistent and WEM
(I) is a subset of these that can be assigned a non-zero
probability â€“ the latter are the worlds where inconsistency in the AM can arise.

5.1

Postulates for Revising the Annotation Function

We now analyze the rationality postulates for non-prioritized revision of belief
bases first introduced in [2] and later generalized in [25], in the context of PPreDeLP programs. These postulates are chosen due to the fact that they are
well studied in the literature for non-prioritized belief revision.

Inclusion: For I â€¢ (f, af â€² ) = (Î EM , Î AM âˆª {f }, af â€²â€² ), âˆ€g âˆˆ Î AM , wld af â€²â€² (g) âŠ†
wld(af â€² (g)).
This postulate states that, for any element in the AM, the worlds that satisfy its
annotation after the revision are a subset of the original set of worlds satisfying
the annotation for that element.
Vacuity: If I âˆª (f, af â€² ) is consistent, then I â€¢ (f, af â€² ) = I âˆª (f, af â€² )
Consistency Preservation: If I is consistent, then I â€¢(f, af â€² ) is also consistent.
Weak Success: If I âˆª (f, af â€² ) is consistent, then (f, af â€² ) âˆˆ I â€¢ (f, af â€² ).
Whenever the simple addition of the input doesnâ€™t cause inconsistencies to arise,
the result will contain the input.
Core Retainment: For I â€¢ (f, af â€² ) = (Î EM , Î AM âˆª {f }, af â€²â€² ), for each w âˆˆ
I
WEM
(I âˆª (f, af â€² )), we have Xw = {h âˆˆ Î˜ âˆª â„¦ | w |= af â€²â€² (h)}; for each g âˆˆ

Î AM (w) \ Xw there exists Yw âŠ† Xw âˆª {f } s.t. Yw is consistent and Yw âˆª {g} is
inconsistent.
For a given EM world, if a portion of the associated AM knowledge base is
removed by the operator, then there exists a subset of the remaining knowledge
base that is not consistent with the removed element and f .
I
Relevance: For I â€¢ (f, af â€² ) = (Î EM , Î AM âˆª {f }, af â€²â€² ), for each w âˆˆ WEM
(I âˆª
â€²
â€²â€²
(f, af )), we have Xw = {h âˆˆ Î˜ âˆª â„¦ | w |= af (h)}; for each g âˆˆ Î AM (w) \ Xw
there exists Yw âŠ‡ Xw âˆª {f } s.t. Yw is consistent and Yw âˆª {g} is inconsistent.
For a given EM world, if a portion of the associated AM knowledge base is
removed by the operator, then there exists a superset of the remaining knowledge
base that is not consistent with the removed element and f .
I
Uniformity 1: Let (f, af â€²1 ), (g, af â€²2 ) be two inputs where WEM
(I âˆª (f, af â€²1 )) =
â€²
â€²
I
I
WEM (I âˆª (g, af 2 )); for all w âˆˆ WEM (I âˆª (f, af )) and for all X âŠ† Î AM (w); if
{x | x âˆˆ X âˆª {f }, w |= af â€²1 (x)} is inconsistent iff {x | x âˆˆ X âˆª {g}, w |= af â€²2 (x)}
is inconsistent, then for each h âˆˆ Î AM , we have that:
I
{w âˆˆ WEM
(I âˆª (f, af â€²1 )) | w |= af â€²1 (h) âˆ§ Â¬af â€²â€²1 (h)} =
I
{w âˆˆ WEM
(I âˆª (g, af â€²2 )) | w |= af â€²2 (h) âˆ§ Â¬af â€²â€²2 (h)}.

If two inputs result in the same set of EM worlds leading to inconsistencies in
an AM knowledge base, and the consistency between analogous subsets (when
joined with the respective input) are the same, then the models removed from
the annotation of a given strict rule or fact are the same for both inputs.
I
Uniformity 2: Let (f, af â€²1 ), (g, af â€²2 ) be two inputs where WEM
(I âˆª (f, af â€²1 )) =
â€²
â€²
I
I
WEM (I âˆª (g, af 2 )); for all w âˆˆ WEM (I âˆª (f, af ))and for all X âŠ† Î AM (w); if
{x | x âˆˆ X âˆª {f }, w |= af â€²1 (x)} is inconsistent iff {x | x âˆˆ X âˆª {g}, w |= af â€²2 (x)}
is inconsistent, then
I
{w âˆˆ WEM
(I âˆª (f, af â€²1 )) | w |= af â€²1 (h) âˆ§ af â€²â€²1 (h)} =
I
{w âˆˆ WEM
(I âˆª (g, af â€²2 )) | w |= af â€²2 (h) âˆ§ af â€²â€²2 (h)}.

If two inputs result in the same set of EM worlds leading to inconsistencies in
an AM knowledge base, and the consistency between analogous subsets (when
joined with the respective input) are the same, then the models retained in the
the annotation of a given strict rule or fact are the same for both inputs.
Relationships between Postulates. There are a couple of interesting relationships among the postulates. The first is a sufficient condition for Core
Retainment to be implied by Relevance.
Proposition 2. Let â€¢ be an operator such that I â€¢ (f, af â€² ) = (Î EM , Î AM âˆª
Iâ€¢(f,af â€² )
I
(w) is a maximal consis{f }, af â€²â€² ), where âˆ€w âˆˆ WEM
(I âˆª (f, af â€² )), Î AM
â€²
Iâˆª(f,af )
(w). If â€¢ satisfies Relevance then it also satisfies Core
tent subset of Î AM
Retainment.

Similarly, we can show the equivalence between the two Uniformity postulates
under certain conditions.
Proposition 3. Let â€¢ be an operator such that I â€¢ (f, af â€² ) = (Î EM , Î AM âˆª
Iâˆª(f,af â€² )
Iâ€¢(f,af â€² )
(w). Operator â€¢ satisfies Unifor(w) âŠ† Î AM
{f }, af â€²â€² ) and âˆ€w, Î AM
mity 1 iff it satisfies Uniformity 2.
Given the results of Propositions 2 and 3, we will not study Core Retainment
and Uniformity 2 with respect to the construction of a belief revision operator
in the next section.
5.2

An Operator for P-PreDeLP Revision

In this section, we introduce an operator for revising a P-PreDeLP program. As
I
stated earlier, any subset of Î AM associated with a world in WEM
(I âˆª (f, af â€² ))
must be modified by the operator in order to remain consistent. So, for such a
world w, we introduce a set of candidate replacement programs for Î AM (w) in
order to maintain consistency and satisfy the Inclusion postulate.
â€²
â€²
â€²
candP gm(w, I) = {Î AM
| Î AM
âŠ† Î AM (w) s.t. Î AM
is consistent and
â€²â€²
â€²â€²
â€²â€²
â€²
s.t. Î AM
is consistent}
âˆ„Î AM âŠ† Î AM (w) s.t. Î AM âŠƒ Î AM

Intuitively, candP gm(w, I) is the set of maximal consistent subsets of Î AM (w).
Coming back to the rain/hail example presented above, we have:
Example 8. Consider the P-PreDeLP program I presented right after Example 7, and the following EM knowledge base:
rain âˆ¨ hail : 0.5 Â± 0.1;
rain âˆ§ hail : 0.3 Â± 0.1;
wind : 0.2 Â± 0.
Given this setup, we have, for instance:
candP gm({rain, hail, wind}, I) =

n

	 
	o
umbrella , Â¬umbrella .

Intuitively, this means that, since the world where rain, hail, and wind are all
true can be assigned a non-zero probability by the EM, we must choose either
umbrella or Â¬umbrella in order to recover consistency.

We now show a series of intermediate results that lead up to the representation theorem (Theorem 1). First, we show how this set plays a role in showing a
necessary and sufficient requirement for Inclusion and Consistency Preservation
to hold together.
Lemma 1. Given program I and input (f, af â€² ), operator â€¢ satisfies Inclusion
and Consistency Preservation iff for I â€¢ (f, af â€² ) = (Î EM , Î AM , af â€²â€² ), for all
I
(I âˆª (f, af â€² )), there exists an element X âˆˆ candP gm(w, I âˆª (f, af â€² ))
w âˆˆ WEM
s.t. {h âˆˆ Î˜ âˆª â„¦ âˆª {f } | w |= af â€²â€² (h)} âŠ† X.

Next, we investigate the role that the set candP gm plays in showing the necessary and sufficient requirement for satisfying Inclusion, Consistency Preservation, and Relevance all at once.
Lemma 2. Given program I and input (f, af â€² ), operator â€¢ satisfies Inclusion,
Consistency Preservation, and Relevance iff for I â€¢ (f, af â€² ) = (Î EM , Î AM , af â€²â€² ),
I
for all w âˆˆ WEM
(I âˆª (f, af â€² )) we have {h âˆˆ Î˜ âˆª â„¦ âˆª {f } | w |= af â€²â€² (h)} âˆˆ
candP gm(w, I âˆª (f, af â€² )).
The last of the intermediate results shows that if there is a consistent program
where two inputs cause inconsistencies to arise in the same way, then for each
world the set of candidate replacement programs (minus the added AM formula)
is the same. This result will be used as a support of the satisfaction of the first
Uniformity postulate.
Lemma 3. Let I = (Î EM , Î AM , af ) be a consistent program, (f1 , af â€²1 ), (f2 , af â€²2 )
I
I
be two inputs, and Ii = (Î EM , Î AM âˆª {fi }, af â€²i ). If WEM
(I1 ) = WEM
(I2 ), then
I
for all w âˆˆ WEM (I1 ) and all X âŠ† Î AM (w) we have that:
1. If {x | x âˆˆ X âˆª {f1 }, w |= af â€²1 (x)} is inconsistent â‡” {x | x âˆˆ X âˆª {f2 }, w |=
af â€²2 (x)} is inconsistent, then {X \ {f1 } | X âˆˆ candP gm(w, I1 )} = {X \
{f2 } | X âˆˆ candP gm(w, I2 )}.
2. If {X \ {f1 } | X âˆˆ candP gm(w, I1 )} = {X \ {f2 } | X âˆˆ candP gm(w, I2 )}
then {x | x âˆˆ X âˆª{f1 }, w |= af â€²1 (x)} is inconsistent â‡” {x | x âˆˆ X âˆª{f2 }, w |=
af â€²2 (x)} is inconsistent.
We now have the necessary tools to present the construction of our nonprioritized belief revision operator.
Construction. Before introducing the construction, we define some preliminary
notation. Let Î¦ : WEM â†’ 2[Î˜]âˆª[â„¦ ] . For each h there is a formula in Î AM âˆª {f },
where f is part of the input. Given these elements, we define:
^
newFor(h, Î¦, I, (f, af â€² )) = af â€² (h) âˆ§
Â¬f or(wi )
I (Iâˆª(f,af â€² )) | hâˆˆÎ¦(w)
wâˆˆWEM
/

The following definition then characterizes the class of operators called AFO
(annotation function-based operators).
Definition 12 (AF-based Operators). A belief revision operator â€¢ is an â€œannotation function-basedâ€ (or af-based) operator (â€¢ âˆˆ AFO) iff given program
I = (Î EM , Î AM , af ) and input (f, af â€² ), the revision is defined as I â€¢ (f, af â€² ) =
(Î EM , Î AM âˆª {f }, af â€²â€² ), where:
âˆ€h, af â€²â€² (h) = newFor(h, Î¦, I, (f, af â€² ))
where âˆ€w âˆˆ WEM , Î¦(w) âˆˆ CandP gmaf (w, I âˆª (f, af â€² )).
As the main result of the paper, we now show that satisfying a key set of
postulates is a necessary and sufficient condition for membership in AFO.

Theorem 1 (Representation Theorem). An operator â€¢ belongs to class AFO
iff it satisfies Inclusion, Vacuity, Consistency Preservation, Weak Success, Relevance, and Uniformity 1.
Proof. (Sketch) (If) By the fact that formulas associated with worlds in the set
I
WEM
(I âˆª(f, af â€² )) are considered in the change of the annotation function, Vacuity and Weak Success follow trivially. Further, Lemma 2 shows that Inclusion,
Consistency Preservation, and Relevance are satisfied while Lemma 3 shows that
Uniformity 1 is satisfied.
(Only-If) Suppose BWOC that an operator â€¢ satisfies all postulates and â€¢ âˆˆ
/
AFO. Then, one of four conditions must hold: (i) it does not satisfy Lemma 2
or (ii) it does not satisfy Lemma 3. However, by those previous arguments,
if it satisfies all postulates, these arguments must be true as well â€“ hence a
contradiction.


6

Conclusions

We have proposed an extension of the PreDeLP language that allows sentences to
be annotated with probabilistic events; such events are connected to a probabilistic model, allowing a clear separation of interests between certain and uncertain
knowledge. After presenting the language, we focused on characterizing belief
revision operations over P-PreDeLP KBs. We presented a set of postulates inspired in the ones presented for non-prioritized revision of classical belief bases,
and then proceeded to study a construction based on these postulates and prove
that the two characterizations are equivalent.
As future work, we plan to study other kinds of operators, such as more
general ones that allow the modification of the EM, as well as others that operate
at different levels of granularity. Finally, we are studying the application of PPreDeLP to real-world problems in cyber security and cyber warfare domains.
Acknowledgments. The authors are partially supported by UK EPSRC grant
EP/J008346/1 (â€œPrOQAWâ€), ERC grant 246858 (â€œDIADEMâ€), ARO project
2GDATXR042, DARPA project R.0004972.001, Consejo Nacional de Investigaciones CientÄ±Ìficas y TeÌcnicas (CONICET) and Universidad Nacional del Sur
(Argentina).
The opinions in this paper are those of the authors and do not necessarily
reflect the opinions of the funders, the U.S. Military Academy, or the U.S. Army.

References
1. Martinez, M.V., GarcÄ±Ìa, A.J., Simari, G.R.: On the use of presumptions in structured defeasible reasoning. In: Proc. of COMMA. (2012) 185â€“196
2. Hansson, S.: Semi-revision. J. of App. Non-Classical Logics 7(1-2) (1997) 151â€“175
3. AlchourroÌn, C.E., GaÌˆrdenfors, P., Makinson, D.: On the logic of theory change:
Partial meet contraction and revision functions. J. Sym. Log. 50(2) (1985) 510â€“530

4. Gardenfors, P.: Knowledge in flux: modeling the dynamics of epistemic states.
MIT Press, Cambridge, Mass. (1988)
5. Hansson, S.O.: Kernel contraction. J. Symb. Log. 59(3) (1994) 845â€“859
6. Doyle, J.: A truth maintenance system. Artif. Intell. 12(3) (1979) 231â€“272
7. Falappa, M.A., Kern-Isberner, G., Simari, G.R.: Explanations, belief revision and
defeasible reasoning. Artif. Intell. 141(1/2) (2002) 1â€“28
8. Li, H., Oren, N., Norman, T.J.: Probabilistic argumentation frameworks. In: Proc.
of TAFA. (2011) 1â€“16
9. Thimm, M.: A probabilistic semantics for abstract argumentation. In: Proc. of
ECAI 2012. (2012) 750â€“755
10. Hunter, A.: Some foundations for probabilistic abstract argumentation. In: Proc.
of COMMA 2012. (2012) 117â€“128
11. Fazzinga, B., Flesca, S., Parisi, F.: On the complexity of probabilistic abstract
argumentation. In: Proc. of IJCAI 2013. (2013)
12. Haenni, R., Kohlas, J., Lehmann, N.: Probabilistic argumentation systems.
Springer (1999)
13. ChesnÌƒevar, C.I., Simari, G.R., Alsinet, T., Godo, L.: A logic programming framework for possibilistic argumentation with vague knowledge. In: Proc. of UAI 2004.
(2004) 76â€“84
14. Hunter, A.: A probabilistic approach to modelling uncertain logical arguments.
Int. J. Approx. Reasoning 54(1) (2013) 47â€“81
15. Gottlob, G., Lukasiewicz, T., Martinez, M.V., Simari, G.I.: Query answering under
probabilistic uncertainty in Datalog+/â€“ ontologies. AMAI (2013)
16. Nilsson, N.J.: Probabilistic logic. Artif. Intell. 28(1) (1986) 71â€“87
17. Khuller, S., Martinez, M.V., Nau, D.S., Sliva, A., Simari, G.I., Subrahmanian, V.S.:
Computing most probable worlds of action probabilistic logic programs: scalable
estimation for 1030,000 worlds. AMAI 51(2-4) (2007) 295â€“331
18. Simari, G.I., Martinez, M.V., Sliva, A., Subrahmanian, V.S.: Focused most probable world computations in probabilistic logic programs. AMAI 64(2-3) (2012)
113â€“143
19. Rahwan, I., Simari, G.R.: Argumentation in Artificial Intelligence. Springer (2009)
20. GarcÄ±Ìa, A.J., Simari, G.R.: Defeasible logic programming: An argumentative approach. TPLP 4(1-2) (2004) 95â€“138
21. Lloyd, J.W.: Foundations of Logic Programming, 2nd Edition. Springer (1987)
22. Simari, G.R., Loui, R.P.: A mathematical treatment of defeasible reasoning and
its implementation. Artif. Intell. 53(2-3) (1992) 125â€“157
23. Stolzenburg, F., GarcÄ±Ìa, A., ChesnÌƒevar, C.I., Simari, G.R.: Computing Generalized
Specificity. Journal of Non-Classical Logics 13(1) (2003) 87â€“113
24. Dung, P.M.: On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artif. Intell. 77
(1995) pp. 321â€“357
25. Falappa, M.A., Kern-Isberner, G., Reis, M., Simari, G.R.: Prioritized and nonprioritized multiple change on belief bases. J. Philosophical Logic 41(1) (2012)
77â€“113

The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence Artificial Intelligence for Cyber Security: Technical Report WS-16-03

Socio-Cultural Modeling for Cyber Threat Actors
Paulo Shakarian and Jana Shakarian
Arizona State University {shak, jshak}@asu.edu

Abstract
In this paper we describe the unique challenges to the important problem of socio-cultural modeling of cyber-threat actors and why they necessitate further advances in artificial intelligence ­ particularly with regard to interdisciplinary efforts with the social sciences.

er communities combined with the goal of automating the collection and analysis of information about the activity of cyber threat actors produces some very unique challenges. In this position paper, we describe some unique characteristics of cyber threat socio-cultural environments and several challenging modeling problems for which various artificial intelligence techniques can be used to help solve.

Introduction
Cyber security is often referred to as "offense dominant" referring to the notion that the domain generally favors the attacker (Lynn, 2010). The reasoning behind this is simple: a successful defense requires total control over all pathways to a system while a successful attack requires only one. As a result, any given cyber-defense based on the hardening of systems will fall prey to a cyber-attack as perpetrators gain knowledge and resources. Solutions have ranged from sophisticated adaptive defense strategies to offensive cyber-operations directed against malicious hackers. However, these methods have various technical shortcomings ­ which range from the technical immaturity of adaptive defenses to consequences of aggressive cyber counter-operations which can lead to undesirable effects such as preemptive and preventative cyber war. A recent trend in the cybersecurity industry has been a move toward "threat intelligence" where various sources of information about potential cyber-attackers are explored with the goal of pre-empting cyber-attacks before they occur. A key source of cyber-threat intelligence lies in the digital communities of the malicious hackers ­ a collection of sites, markets, chatrooms, and social media channels where information is shared, hackers are recruited, and the latest malware and exploits are bought and sold. While artificial intelligence and machine learning techniques for analyzing communities on the Internet are long-established across specialty areas such as data mining, information retrieval, and web science, we argue that the study of hack-

Characteristics of Cyber Threat Socio-Cultural Environments
In our group, we have studied hacker communities from a qualitative standpoint (Shakarian, Shakarian, and Ruef 2015). Throughout this research, we have noted several unique characteristics in the online socio-cultural environments frequented by malicious hackers that make these communities distinct from other groups. Some of these characteristics include the following. · Bounded anonymity. Individuals participating in the malicious hacker community online make efforts to hide their identity. Some however seek to maintain a consistent online persona to gain social status in the hacker meritocracy. · Participation in high-risk behavior. Despite recent arrests for individuals associated with darknet markets as well as suspicions of law-enforcement infiltration, many individuals still participate in discussions about illegal activities in darknet forums, though access controls appear to increase. Likewise, individuals participate in hacktivist operations advertised through social media. A recent lab-based behavioral study has explored some of the potential factors that would lead an individual to participate in risky hacktivism activities (Bodford, 2015). · High incentives to cheat. The existence of marketplaces where malicious hackers sell software and exploits to others is an environment where both parties are highly incentivized to cheat. For instance the sale of a

Copyright © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

193

faulty product and violations of exclusive use agreements can be conducted with relative ease. · Ability to deceive. The anonymous nature of these environments combined with the fact that various aspects of a malicious hacker's digital persona can be forged allow for deceptive activities to occur with relative ease. These characteristics are interesting in several ways. First, from a sociological and behavioral standpoint, the freedom at which individuals in these communities discuss criminal activities as well as share information and code with individuals likely involved with computer related crimes (which itself is also a crime) begs the question how trust is afforded to enable observable social interactions. Second, the characteristics such as anonymity and deception lead to modeling challenges ­ perhaps requiring consideration of latent attributes. Third, aspects such as cheating may actually constrain models to a degree ­ hence leading to model simplifications.

· Emergence and disintegration of trust-based communities. For darknet marketplaces to thrive, populations of individuals have to make decisions to trust both those running the marketplace and many of the vendors. While there are established models for trust among individuals, understanding how the propagation of trust is initiated and spread in anonymous environments ­ which seem to discourage trust ­ remains an open question. By addressing this problem, we can better understand when a given cyber-exploit/malware marketplace will become well established. · Modeling deception hypotheses. In order to properly attribute individual activity on the darknet to that seen in public in cases of cyber-attacks or attributing the author of a given malware or exploit, cyber-security analysts consider the "deception hypotheses" ­ the chance that some or all of the observed evidence can be planted by an adversary. Therefore, for models designed for problems relating to cyber-attribution, we must also consider the deception hypothesis. In some of our ongoing efforts, we are leveraging defeasible logic programming to explicitly consider the deception hypothesis.

Modeling Challenges
In this section, we describe a few major challenges for modeling socio-cultural cyber threat actor communities. Overcoming these challenges will provide new insights into this environment and also aide in higher-level tasks such as predicting cyber-attacks and understanding the development of exploits and malware by this community. · Establishment of social status in an anonymous environment. In order for a malicious hacking community to exist, there must be anonymity, yet actors stand to gain from prestige earned in the hacker meritocracy, such as access to invite-only forums, trust in social interactions in general as opposed to undergoing frequent vetting processes. Modeling the accumulation of this latent quantity with which proxy measurements are challenging in non-anonymous environments ­ and the level of anonymity itself creates even more difficult challenges. However, in addressing these challenges, we can better identify significant cyber threat actors and associate a greater degree of confidence with their actions. Recently, there has been some initial, descriptive work on this topic (Abbasi et al., 2014). · Data-driven modeling of risk taking. The adoption of risky behavior has gained attention in the computational social science literature using model-based approaches (Roos, Carr, and Nau, 2010). However, instantiating models based on data remains largely an open question. The issue is further complicated by limited data on verified activities ­ as not all cyber-attacks are reported in the open. The goals in establishing such models for the study of cyber-threats in determining when certain risky behavior will occur is likely to aide in prediction and preventative cyber defense.

Conclusion and Ongoing Efforts
In this paper, we have discussed some of the unique characteristics of the socio-cultural environment for cyber threat actors and associated modeling problems that are of interest to the artificial intelligence community. We are currently exploring these challenges in support of larger cyber-security goals such as attack prediction and critical infrastructure defense. Acknowledgements. This work was supported by Arizona State University's Global Security Initiative (GSI) as well as the U.S. Office of Naval Research (ONR) NEPTUNE program.

References
Abbasi, A.; Li, W.; V. Benjamin, V.; Hu, S.; Chen, H. (2014) Descriptive Analytics: Examining Expert Hackers in Web Forums. IEEE Joint International Conference on Intelligence and Security Informatics. Bodford, J. (2015) We are Legion: Hacktivism as a Product of Deindividuation, Power, and Social Injustice, Masters Thesis, Arizona State University. Lynn, W. J. 2010. Defending a New Domain: The Pentagon's Cyberstrategy. Foreign Affairs, 89(5). Roos, P.; Carr, R.; Nau, D. 2010. Evolution of state-dependent risk preferences. ACM Transactions on Intelligent Systems and Technology 1(1). Shakarian, J.; Shakarian, P.; Ruef, A. 2015. Cyber Attacks and Public Embarrassment: A Survey of Some Notable Hacks. Elsevier SciTechConnect.

194

A Comparison of Methods for Cascade Prediction
Ruocheng Guo, Paulo Shakarian

arXiv:1606.05730v1 [cs.SI] 18 Jun 2016

Arizona State University
Tempe, AZ
Email: {rguosni,shak}@asu.edu

Abstractâ€” Information cascades exist in a wide variety of
platforms on Internet. A very important real-world problem is
to identify which information cascades can â€œgo viralâ€. A system
addressing this problem can be used in a variety of applications
including public health, marketing and counter-terrorism. As a
cascade can be considered as compound of the social network
and the time series. However, in related literature where methods
for solving the cascade prediction problem were proposed, the
experimental settings were often limited to only a single metric
for a specific problem formulation. Moreover, little attention was
paid to the run time of those methods. In this paper, we first
formulate the cascade prediction problem as both classification
and regression. Then we compare three categories of cascade
prediction methods: centrality based, feature based and point
process based. We carry out the comparison through evaluation
of the methods by both accuracy metrics and run time. The
results show that feature based methods can outperform others
in terms of prediction accuracy but suffer from heavy overhead
especially for large datasets. While point process based methods
can also run into issue of long run time when the model can not
well adapt to the data. This paper seeks to address issues in order
to allow developers of systems for social network analysis to select
the most appropriate method for predicting viral information
cascades.

I. I NTRODUCTION
Identifying when a piece of information goes â€œviralâ€ in social media is an important problem in social network analysis.
This is often referred to as â€œcascade predictionâ€. Recently,
the cascade prediction problem attracted considerable attention from researchers from communities of machine learning,
data mining and statistics. Researchers attempted to predict
the final size of information cascades based on approaches
inspired by knowledge in various areas. Pei et al. [1] measured
influence of the root node by k-shell number and related
heuristics. Weng et al. [2] and Guo et al. [3] uitilized features
describing both structural and temporal properties of earlystage cascades. The work described in [4] and [5] modelled
cascades by one-dimensional point process. However, in this
line of research, the experimental settings varied from paper
to paper. Furthermore, as the cascade prediction problem
can be treated as either classification or regression, most of
previous work only dealt with one or the other and using
just a single evaluation metric.With deployment of a counterextremism messaging system (i.e. an enhanced version of [6])
as one of the primary goals in our group, cascade prediction
can play a crucial role in detection of early-stage extremism
message that is potential to go viral on social network sites.
Other applications include the spread of information following
a disaster, promotion of health behaviors and applications

to marketing. Therefore, it is important to understand how
well the existing methods stemming from different research
area could perform in near real-world experimental settings.
An ideal cascade prediction method for counter-extremism
messaging system should provide acceptable accuracy with
ability to make near real-time prediction.
In this paper, we compare performance of a variety of
cascade prediction methods originating from different research
areas as both classification and regression problems with
multiple evaluation metrics. We also measure the run time
of the tasks required by the methods to complete cascade
prediction â€“ another key deployment concern not explored in
most research.
In this paper, the main contribution can be summarized as:
â€¢ We compare cascade prediction methods in three categories: centrality based, feature based and point process
based, therefore providing comparison between methods
orginating from different research areas.
â€¢ The cascade prediction problem is considered from both
the aspect of regression and classification. we also conduct a comprehensive comparison between methods by
various evaluation metrics.
â€¢ We also compare the run time of tasks needed for the
cascade prediction methods are also measured in a task
by task style.
The rest of this paper is organized as the follows: In Section II,
definitions relevant to the methods considered in this paper are
introduced along with a formal problem statement of cascade
prediction. Section III summarizes the mechanism of the three
categories of cascade prediction methods. Section IV and V
presents the setup of experiments and performance of each
method in terms of both accuracy and run time. Section VI
reviews related work. At last, Section VII concludes the paper
and discusses the main issues of these methods.
II. T ECHNICAL P RELIMINARIES
In this section, related concepts for the three categories of
methods are defined. Then we formulate the cascade prediction
problem as regression and classification respectively.
A. Definitions
Network and Cascade: The social network is a directed
graph G = (V, E) where each node v âˆˆ V represents a
user and each edge eij = (vi , vj ) denotes that user vi is
followed by user vj . Identified by the original message or the
corresponding hashtag, a cascade is a time-variant subgraph of

the social network d(t) = (V (t), E(t)). Each node v âˆˆ V (t)
denotes a user reposted the original message of cascade d(t)
(for the Aminer dataset [7]) or a user posted the hashtag
defining cascade d(t) (for the Twitter dataset [2]) within time
t. The time variable t denotes number of time units since the
microblog including the original message or the hashtag. For
each node v âˆˆ V (t) we record their adoption time of cascade
d(t) as tv . For v âˆˆ V (t), tv â‰¤ t while for v 6âˆˆ V (t) we define
tv = âˆž. Thus we can get an ascendingly sorted vector tv (t)
including all tv â‰¤ t for each cascade, which plays an important
role in both feature based methods and point process based
methods for cascade prediction. The kth element of tv (t) can
be denoted as tv (t)[k]. For convenience, we use tend to denote
the time when the last adoption of a cascade happened.
Besides the cascade d(t) = (V (t), E(t)), the neighborhood
of V (t) also can provide information about the potential of
the cascade. Here we define the out-neighborhood reachable
by any node in V (t) in step i as ith surface Fi (t). To show
how â€™freshâ€™ the cascade is for a node v âˆˆ Fi (t), we define a
function fâˆ†t : v â†’ âˆ†t that maps such a node to the number of
time units since v become a member of first surface to current
time t. As time makes a big difference in social influence and
diffusion, we divide the first surface F1 (t) into two sets of
nodes depends on fâˆ†t (v) for all v âˆˆ F1 (t). With a selected
threshold tÎ» . The first set named as frontiers includes all
nodes v âˆˆ F1 (t) such that fâˆ†t (v) â‰¤ tÎ» and the other set nonadopters consists the other nodes v âˆˆ F1 (t) with fâˆ†t (v) > tÎ» .
In this paper, |x| denotes absolute value of scaler x and |x|
denotes cardinality of set x.
Communities: We can treat a community partition of a social
network as a function fC : V â†’ C which maps a set of
nodes V to a set of communities C. With this function, given
a cascade d(t) = (V (t), E(t)), it enables us to describe the
distribution of nodes over communities by features such as
|fC (V )|, the number of communities among set V .
Point Process: Each adoption in a cascade can be represented
as an event from the aspect of point process as in [4]. Thus,
for cascade prediction, we can use tv (t âˆ’ âˆ†t) to describe the
history of a point process strictly before t. The core of a point
process is the conditional density function Î»(t). Conditioned
on tv (t âˆ’ âˆ†t), the conditional density is the limit of expected
number of adoptions would happen in time interval [t, t + âˆ†t]
by taking âˆ†t â†’ 0+ :
Î»(t) = lim E {|V (t + âˆ†t)| âˆ’ |V (t)|}
âˆ†tâ†’0+

(1)

Given the density function Î»(t) and target prediction time
t0 , the predicted cascade size can be computed by:
Z t0
Ë†
0
|V (t )| = |V (t)| +
Î»(Ï„ )dÏ„
(2)
t

B. Problem Statement
In this paper, we focus on comparison of different methods
which can solve the cascade prediction problem. This problem can be formulated as either a regression problem or a
classification problem:

Regression Problem: Given a early stage cascade d(t) =
(V (t), E(t)) and the corresponding node attribute vector tv (t)
with constraint |V (t)| = n, the target is to predict the final
size of the cascade |V (tend )|.
Classification Problem: A threshold T hres is selected to
label each cascade. For a given cascade if its |V (tend )| â‰¥
T hres, we define it as a viral sample labeled as 1, otherwise,
we label it as non-viral labeled as 0. Then the problem is to
classify a given early-stage cascade d(t) to the viral class or
the non-viral class.
III. M ETHODS
In this section we introduce several recently published
methods for solving the cascade prediction problem. Diffusion
process in social network includes information of time series,
network structure, sometimes with microblog content and node
attributes, therefore, methods originated from knowledge in
various research area like social network analysis, random
point process and non-linear programming can be applied. The
methods can be categorized into: centrality based methods,
feature based methods and point process based methods.
A. Centrality Based Methods
Previous work [1] discovered that the k-shell value of a node
is highly correlated to the average cascade size it initiates.
In this paper, we also consider eigenvector centrality, outdegree and Pagerank of the root node of cascades to deal with
the cascade prediction problem. We refer to centrality based
approaches as method C in this paper.
B. Feature Based Methods
In this paper, we consider two recently proposed methods
[3] and [2] and call them method A and method B respectively
for convenience. The features computed by the two methods
can be categorized into network features, community based
features and temporal features.
Both of the feature based methods require to take advantage
of community detection algorithms. Given the social network,
community detection algorithms such as [8] and [9] can
be applied to it and assign each node to one or multiple
communities. Based on the communities detected, features can
be computed to numerically describe how the nodes that participate in a cascade are distributed over communities. Thus,
we can quantitatively measure structural diversity from [10]
or influence locality from [7] as features.
Network Features: In method B proposed by [2], the authors
consider several types of network features:
â€¢ Neighborhood size, including first surface (|F1 (Vt )|) and
second surface (|F2 (Vt )|).
â€¢ Path length, consisting average step distance and coefficient of variation of it, and diameter of the cascade.
Step distance is the length of shortest path between two
consecutive adopters vi and vi+1 .
Where coefficient of variation is defined as the ratio of the
standard deviation to the mean.

Community Based Features: In both [3] and [2], community
features are extracted and contribute to the predictive methods.
â€¢ Community features for adopters, including the number
of communities (|fC (V (t))|), entropy and gini entropy.
â€¢ Community features for frontiers and non-adopters, including the number of communities (|fC (F1 (t))|), entropy and gini entropy.
â€¢ The number of shared communties between any two
groups of adopters, frontiers and non-adopters.
Temporal Features: In [3], the authors computed average of
tv (t) while average step time and its corresponding coefficient
of variation are calculated in [2] as two features.
C. Point Process Based Methods
To discover patterns in the temporal dynamics of cascades,
authors of both [5] and [4] both consider a cascade as an
instance of one-dimensional point process in time space. They
proposed novel density functions to characterize time series
of cascades. The two methods are quite similar, in terms of
the formulation of conditional density function Î»(t). In both
cases, Î»(t) consists of an element modeling the popularity of
the cascade and another describing the probablity distribution
of an adoption behavior over time.
The Reinforced Poisson Process (RPP) Method: In [5], the
authors consider the density function for a cascade d as a
product of three elements:
Î»d (t) = Î±d fd (t; Î¸d ) |V (t)|

(3)

For cascade d, Î±d denotes the intrinsic attractiveness, fd (t; Î¸d )
is defined as the relaxation function which models how likely
an adoption would happen at time t without considering Î±d
and |V (t)|. For each cascade d, parameters Î±d and Î¸d are
learned by maximization of the likelihood of tv (t). Thus, the
predicted cascade size at time t0 > t can be computed by:
Z t0
|VÌ‚ (t0 )| = |V (t)| +
Î±d fd (Ï„ ; Î¸d ) |V (Ï„ )| dÏ„
(4)
t

The SEISMIC Method: In [4], authors model the density
function as a modified Hawkes Process made up of three elements: infectiousness pt , node degree ni and human reaction
time distribution Ï†(s):
|V (t)|

Î»(t) = pt

X

ni Ï†(t âˆ’ tvi )

(5)

i=1

Where tvi âˆˆ tv (t) is the time when each adoption happens.
Similar to Î±d in the Reinforced Poisson Process model, pt is
computed by maximization of the likelihood function:
|V (t)|âˆ’1

pt = arg max
pt

Y

Î»(tvi ) expâˆ’

R tvi+1
tvi

Î»(Ï„ )dÏ„

(6)

i=0

While the human reaction time distribution Ï†(s) is formulated
as a piece-wise function consists of a constant piece and a
power-law piece with parameter c and Î¸:
(
c
s â‰¤ s0
(7)
Ï†(s) =
s âˆ’(1+Î¸)
s > s0
c( s0 )

TABLE I: Dataset Statistics
Property
Directed
Nodes
Edges
Number of communities
Modularity
Average Out-degree
Average Eigenvector Centrality
Average K-shell
Average Pagerank
Cascades (â‰¥ 50 nodes)

Twitter Dataset

Weibo Dataset

undirected
595,460
7,170,209
24,513
0.7865
47.94
0.001783

directed
1,787,443
216,511,564
2,802
0.5581
231.3381
0.0186

24.6032
1.6794eâˆ’6
14,607

52.3064
5.596eâˆ’7
99,257

As Ï†(s)R is a probability distribution function, with the conâˆž
straint 0 Ï†(s)ds = 1 and power-law decay factor Î¸ estimated by training data, c can be computed. With the density
function Î»(t), the predicted cascade size can be computed by
equation (2).
IV. E XPERIMENTAL S ETUP
For comprehensiveness, we evaluate the performance of
each method by treating cascade prediction problem as both
regression and classification problem. We only consider cascades that end up with at least 50 adopters. Thus we can treat
first 50 nodes of each cascade as its early stage. In this section,
an introduction of the datasets is followed by descriptions
of setup of the classification and regression experiments. All
the experiments are carried out on an Intel(R) Xeon(R) CPU
E5-2620 @ 2.40 GHz machine with 256GB RAM running
Windows 7. All the methods are implemented in Python 2.7.
A. Dataset Description
The statistics of the two datasets used in this paper for
evaluation of the cascade prediction methods are shown in
Table I.
Twitter Dataset: Twitter1 is the most well-known microblog
platform throughout the world. The dataset was used in [2].
This dataset includes a friendship network with undirected
edges, cascades identified by hashtags and corresponding
mentions and retweets.
Weibo Dataset: Sina Weibo2 is the largest Chinese microblog
social network. The dataset was used in [7]. It consists of a
directed followership network and retweet cascades.
B. Regression
For the regression problem, the m Ã— 1 ground truth vector
y is made up of final size of each cascade (|V (tend )|), where
m is the number of cascade. Each regression model is able
to output a m Ã— 1 vector yÌ‚. Thus each element yÌ‚i âˆˆ yÌ‚
is the predicted size of the ith cascade. For point process
models, with different prediction time, the predicted results
can change. Thus, for each early-stage cascade, we set t as
1 https://twitter.com
2 https://weibo.com

the time when we observed the 50th adoption and prediction
time as {2, 4, 6, 8, 10} Ã— tv (t)[50]. To evaluate a method for
the regression problem, the difference between its prediction
results yÌ‚ and the ground truth y can be described by various
error functions. In addition, yÌ‚top10% denotes the set of top
10% cascades in prediction result while ytop10% is the set
top of 10% cascades of ground truth. In this paper we choose
following metrics to compare the prediction made by different
methods, as they are widely used in related literatures such as
[11], [5], [12] and [4]:
Pm |yË†i âˆ’yi |
1
â€¢ APE (average percentage error): m
i=1
yi
â€¢ RMSE (root mean square error):
r Pm
2
i=1 (yË†i âˆ’ yi )
m
â€¢ RMLSE (root mean logrithm square error):
r Pm
2
i=1 (log yË†i âˆ’ log yi )
m


10 
yÌ‚top10% âˆ© ytop10% 
â€¢ Top 10% coverage:
m

C. Classification
For classification, we apply three predetermined thresholds
(50th, 75th and 90th percentiles) to final size of cascades to
assign each of them a class label, which provides the m Ã— 1
ground truth vector L = {l0 , ..., lmâˆ’1 } one for each threshold.
The cascades with size larger than threshold are labelled as
viral class with li = 1. Table II shows the thresholds and
counts of samples for both classes. Then the methods for
solving the classification problem can output predicted label
vector LÌ‚. Comparing L with LÌ‚ results in standard metrics:
precision, recall and F1 score. To examine the effectivess of
the methods, we focus on reporting the metrics on the minority
class (viral) as it is more difficult to do good predictions for
it than the other.
Specially, for point process based mothods, as they are capable to predict the final cascade size without being trained with
class labels (once parameters are determined and prediction
times are selected), we carry out the evaluation on them in this
way: prediction results (by setting different prediction times)
are treated as features for each sample. As the time when each
cascade stop growing is not easy to determine.
D. Run time
We also take the run time of tasks into account for the cascade prediction methods. To understand how computationally
expensive the methods are in terms of run time, it is necessary to analyze the procedure of them. For centrality based
methods, the prediction can be divided into three steps: computation of centrality, training and prediction. Similarly, for
feature based methods, computation of features, training and
prediction are required to be done. In addition, preprocessing
like community detection, computation of shortest path length
are needed, which can be computationally expensive. While
point process based methods require little preprocessing. For
each cascade, parameters are computed independently through

TABLE II: Thresholds for Classification
Percentile

Threshold

Viral samples

Non-viral samples

Twitter Dataset
50%
75%
90%

106
226
587

50%
75%
90%

152
325
688

7,303
3,652
1,461

7,304
10,955
13,146

Weibo Dataset
49,628
24,814
9,925

49,629
74,443
89,332

MLE of the observed time vector tv (t) and properties of the
adopters V (t). Then prediction is made by integral of density
functions. Thus, we consider the following processes one by
one and then combine them to estimate the run time of a
certain method.
Proprecessing: There are three types of proprecessing considered: loading the graph, computation of centralities and
community detection.
Computation of Features: For feature based methods, we
measure the run time of computation of the features , which
takes the product of preprocessing as input.
Training and Prediction: For centrality and feature based
methods, the run time of training and prediction is measured
for ten-folds. For point process based methods, we measure
the run time of parameter estimation and prediction for the
whole batch of data.
V. E XPERIMENTAL R ESULTS
In this section we show the experimental results including
both accuracy of cascade prediction and the run time for each
method. For convenience, we call method of [3], [2] and the
centrality based method as method A, B and C respectively.
For method A, B and C, 10-fold cross-validation is applied.
For results where we compare these three methods, we report
only the best-performing centrality measure amongst outdegree, Pagerank, Shell number and eigenvector centrality as
the method C for each dataset. As shown in Fig. 1, eigenvector
centrality outperforms others in the classification task when
the two classes are imbalanced. Thus we take eigenvector
centrality as the method C. The results for regression is not
shown here for limited space as the difference between results
produced by different centralities is trivial. For the Reinforced
Poisson Process (RPP) method [5], as the parameter estimation
task for each cascade is independent of others, the crossvalidation is skipped and predictions are made by parameters
learned from first 50 nodes of each cascade. For the SEISMIC
method [4], we also skip the 10-fold cross-validation. We set
the cutoff time s0 = 30000(s) for the Twitter dataset and
s0 = 300(s) for the Weibo dataset then fit the parameters
(Î¸, c) for the human reaction time distribution function Ï†(s)
with all samples in the dataset. While in the original paper [4],
the authors set Î¸ and c just by 15 tweets they manually
picked. The power-law fitting is done as per [13], which

Out-degree
Pagerank
Shell Number
Eigenvector

1.0
0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.0

Precision

Recall

Out-degree
Pagerank
Shell Number
Eigenvector

1.0
0.8

0.0

F1 Score

(a) Twitter Dataset: 50th percentile

0.4

0.2

0.2

0.0

0.0

Recall

F1 Score

(c) Twitter Dataset: 90th percentile
Out-degree
Pagerank
Shell Number
Eigenvector

1.0
0.8

200.0%

Recall

0.0%

F1 Score

Out-degree
Pagerank
Shell Number
Eigenvector

(a) Twitter Dataset: APE
2.0
1.5

Precision

Recall

(d) Weibo Dataset: 50th percentile
Out-degree
Pagerank
Shell Number
Eigenvector

1.0
0.8
0.6
0.4

100.0%

0.2

0.2
F1 Score

(e) Weibo Dataset: 75th percentile

0.0

Precision

Recall

F1 Score

SEISMIC
RPP

(b) Twitter Dataset: RMSE

0.30
0.25
0.20

A(SVR)
B(SVR)
C(DTR)

SEISMIC
RPP

0.10
0.05
0.00

(c) Twitter Dataset: RMSLE
200.0%

A(SVR)
B(SVR)
C(DTR)

0.15

0.0

F1 Score

0.4

Recall

SEISMIC
RPP

0.5

0.6

Precision

A(SVR)
B(SVR)
C(DTR)

1.0

150.0%

0.0

3000
2500
2000
1500
1000
500
0

A(SVR)
B(SVR)
C(DTR)
SEISMIC
RPP

50.0%
Precision

0.8

0.4

250.0%

100.0%

1.0

0.6

300.0%

150.0%

(b) Twitter Dataset: 75th percentile

0.6

Precision

Out-degree
Pagerank
Shell Number
Eigenvector

1.0

A(SVR)
B(SVR)
C(DTR)
SEISMIC
RPP

(d) Twitter Dataset: Top 10% Coverage
2000

A(SVR)
B(SVR)
C(DTR)

SEISMIC
RPP

1500
1000

50.0%

500

0.0%

0

(f) Weibo Dataset: 90th percentile

Fig. 1: Classification results of centrality based methods: error
bar stands for one standard deviation.

(e) Weibo Dataset: APE
2.0
1.5

A(SVR)
B(SVR)
C(DTR)

SEISMIC
RPP

1.0

âˆ’5

returns (Î¸, c) = (0.440, 1.018e ) and (0.282, 7.332e
the Twitter dataset and Weibo dataset respectively.

âˆ’4

) for

A. Regression
For centrality based methods, we apply linear regression
with least squared error. We carry out the training and prediction with random forest regressor, SVR and linear regression
model provided by [14] for feature based methods. We only
show the results produced by SVR as it outperformes others.
For the point process based mothods, we only report the best
result among prediction time out of {2, 4, 6, 8, 10}Ã—tv (t)[50].
For the Twitter dataset, Fig. 2a, 2b, 2c and 2d show the
experimental results for the regression problem. Feature based
methods and SEISMIC outperform RPP and method C w.r.t.
APE. Concerning RMSE, method A shows more predictive
power than others. As to RMSLE, feature based methods result
in less error than the other two categories. From the aspect of
Top 10% coverage, RPP, method A are more likely to track
the trending cascades than others.
Fig. 2e, 2f, 2g and 2h show the regression result for the
Weibo dataset, Regarding APE, SEISMIC, method A and B
have comparable performance and outperform others. In terms
of RMSE, method A, B are measured to be more predictive
than the rest. Feature based methods also make predictions
with least RMSLE. For top 10% coverage, RPP is more likely
to detect popular cascades than others.
An interesting observation is that the prediction accuracy
measured by different error metrics can be contrary to each
other. For example, in Fig. 2a, compared to SEISMIC, prediction made by method C results in more error measured by

0.5
0.0

(g) Weibo Dataset: RMSLE

(f) Weibo Dataset: RMSE

0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00

A(SVR)
B(SVR)
C(DTR)

SEISMIC
RPP

(h) Weibo Dataset: Top 10% Coverage

Fig. 2: Regression results

APE, however, comparable error w.r.t. RMSE and less error
regarding RMSLE (See Fig. 2b and 2c). This implies that it
is better for researchers to show more than one type of error
for evaluation of regression results.
B. Classification
We show the precision, recall and F1 score for the viral
class with all the three percentile thresholds. For each dataset,
we choose the 50th, 75th and 90th percentile of the final size
of all cascades as the thresholds for assigning the cascades
into viral or non-viral class. The number of samples in
each class is shown in Table II. Thus we can evaluate the
cascade prediction methods with balanced and imbalanced
classes. For each method, we only show the best result among
those produced by different classifiers or various training
methods. As a result, for feature based methods, random forest
outperforms others. While for point process based methods
we treat cascade size predicted by setting prediction time as
{2, 4, 6, 8, 10}Ã—tv (t)[50] as features. Here we show the results
produced by classifiers trained by these features.
Fig. 3a, 3b and 3c show the classification results for
the Twitter dataset. With all three thresholds, feature based
methods A and B outperform others. In addition, they also

1.0

A(RF)
B(RF)

C(DT)
SEISMIC

RPP

1.0

0.8

0.8

0.6

0.6

0.4

0.4

0.2
0.0

Recall

F1 Score

A(RF)
B(RF)

C(DT)
SEISMIC

RPP

0.0

1.0
0.8

0.6

0.6

0.4

0.4

0.2

0.2
Precision

Recall

F1 Score

(c) Twitter Dataset: 90th percentile
1.0

A(RF)
B(RF)

C(DT)
SEISMIC

RPP

0.0

1.0
0.8

0.6

0.6

0.4

0.4

0.2

Precision

Recall

F1 Score

A(RF)
B(RF)

Precision

C(DT)
SEISMIC

Recall

RPP

F1 Score

(d) Weibo Dataset: 50th percentile

0.8

0.0

RPP

(b) Twitter Dataset: 75th percentile

0.8

0.0

C(DT)
SEISMIC

0.2
Precision

(a) Twitter Dataset: 50th percentile
1.0

A(RF)
B(RF)

A(RF)
B(RF)

C(DT)
SEISMIC

RPP

0.2
Precision

Recall

F1 Score

(e) Weibo Dataset: 75th percentile

0.0

Precision

Recall

F1 Score

(f) Weibo Dataset: 90th percentile

Fig. 3: Classification results: error bar stands for one standard
deviation.

show more robustness than others to imbalance of two classes
in dataset. In terms of point process based methods, SEISMIC
outperforms RPP especially when the two class are imbalanced. RPP suffers from relatively large standard deviation,
as the Newtonâ€™s method is not always able to achieve convergence. Thus the parameters learned through the MLE approach
can vary as a result from random initialization. Method C
(eigenvector centrality) shows little predictive power with any
of the three thresholds for the Twitter dataset, even if it
outperforms other centrality based methods.
For the Weibo dataset, as shown in Fig. 3d, 3e and 3f,
feature based methods outperform others again with all three
thresholds. Regarding point process based methods, contrary
to the results for Twitter dataset, RPP achieves better F1 score
than SEISMIC when threshold value becomes large. Method
C (eigenvector centrality) performs comparably to RPP.
C. Run time
In this subsection, we show the run time of tasks for the
cascade prediction methods considered in this paper. On one
hand, preprocessing, computation of centralities and features
suffer from high overhead as immense amount of data needs
to be loaded. The run time of these tasks are listed in Table III.
On the other hand, training and prediction tasks barely have
the overhead issue.
Preprocessing: We carry out the community detection task by
the java implementation of Louvain algorithm [15] with 10
random start and 10 iterations for each start. For computation
of centralities, we load edgelist of the social networks as a
graph object in igraph-python [16]. As shown in Table III,

community detection, computation of Pagerank and loading
graph are the tasks suffer the most when the size of dataset
increases. Community detection, computation of Pagerank and
loading graph for the Weibo dataset take 80.32, 66.855 and
19.80 times the run time of those for the Twitter dataset
respectively.
Computation of Features: As shown in Table III, for the
feature computation task, it takes method B 12.37 and 8 times
the run time method of A for the Twitter Dataset and the
Weibo Dataset respectively. To explain this observation, an
analysis of what computation is carried out in each iteration for
method A and B. For method A, computation of the features
can be done without loading the graph (a heavy overhead).
Moreover, for each cascade, method B also requires expensive
computation of shortest path length for each pair of nodes in
cascade subgraphs and size of 2-hop neighborhood.
Training and Prediction: The run time of training and prediction is not directly related to the size of the social network.
On one hand, it is correlated to the number of cascades for
training and prediction. On the other hand, it is decided by the
complexity of the method: for example, number of parameters
to be learned, the complexity for learning each paramter and
the comsumption to work out the prediction. Here we only
measure the run time for solving the classification problem.
We run each method with single process, overhead run time
such as graph loading is ignored. For feature based methods
the training and prediction time are also correlated to the
number of features. For centrality based methods, we only
show the run time for k-shell (method C) as all methods in this
category are trained and tested with one feature: the centrality
measure of the root node. Compared to RPP, SEISMIC is a
deterministic method with closed form solution. The run time
for each sample can be distributed with little variance. For the
RPP method, as the log-likelihood function is non-convex, it is
not guaranteed that global maximum can be reached in limited
number of iterations. Therefore, the run time for a sample
running out of the maximum number of iterations can be
thousands times that of another, which reaches the convergence
condition in the first iteration. As the log-likelihood function of
RPP is twice-differentiable, Newtonâ€™s method can be applied.
In our experiments, with the maximum number of iterations
setted as 100, the convergence is more likely to be achieved by
Newtonâ€™s method than gradient descent. Thus we only show
the run time of RPP with Newtonâ€™s method.
Fig. 4 shows the run time for each method to complete
training and prediction tasks for all cascades in the two
datasets. For feature based methods, it shows the run time
needed for random forest (RF), SVM and logistic regression
(LR). For method C, it shows that of decision tree (DT), SVM
and logistic regression (LR).
Concerning the Twitter dataset (See Fig. 4a), taking advantage of decent implementation of classifiers, feature based
methods comparable run time to point process based methods
w.r.t. the training and prediction task with random forest and
SVM (rbf kernel).
For the Weibo dataset, as shown in Fig. 4b, the run time

TABLE III: Run time: Preprocessing & Feature Computation
Type

Task

Twitter Dataset
Louvain
275
Loading Graph
60.033
Degree
0.016
K-shell
2.757
Eigenvector
20.444
Pagerank
26.298
A
267.144
B
3252.7562
Weibo Dataset
Louvain
22087
Loading Graph
1188.486
Degree
0.045
K-shell
139.128
Eigenvector
391.140
Pagerank
1758.164
A
11181.453
B
87651.213

Feature
Computation

Preprocessing

Feature
Computation

Run time (seconds)

10 2

A(RF)
A(LR)
A(SVM)
B(RF)

B(LR)
B(SVM)
C(DT)
C(LR)

C(SVM)
SEISMIC
RPP

10 6
10 5

Run time (seconds)

Preprocessing

10 3

Total time (s)

A(RF)
A(LR)
A(SVM)
B(RF)

Time
per
sample (s)
â€“
â€“
â€“
â€“
â€“
â€“
0.018
0.2227
â€“
â€“
â€“
â€“
â€“
â€“
0.110
0.883
B(LR)
B(SVM)
C(DT)
C(LR)

C(SVM)
SEISMIC
RPP

10 4
10 3

10 1

10 2

10 0

10 1
10 0

10 -1

(a) Twitter Dataset

(b) Weibo Dataset

Fig. 4: Run Time of Trainig and Prediction

feature based methods comsume is comparable to SEISMIC
with random forest. But the SVM with rbf kernel suffers from
the order-of-magnitude increase of the number of training and
testing samples. Thus leads to the observation that the run time
becomes approximately 10 times that of random forest.
Comparing Fig. 4b with Fig. 4a, the run time of RPP method
increases the most. This means it is much more difficult for
the Newtonâ€™s method to converge for samples in the Weibo
datasets. There are two possible reasons to explain this: 1).
the uniform distribution used in random initialization can not
produce good initial values that are closed to local optimal
points; 2). the choice of log-norm distribution as function
fd (t; Î¸d ) can not provide fairly good description of cascades
in this dataset.
VI. R ELATED W ORK
Influence Maximization Since the proposal of Influence maximization problem by Kempe et al. [17], related work emerged,
focusing on estimation of influence for a selected set of nodes
that can be measured by expected number of infectees under
a certain influence model, such as [18] and [19]. Recently,
a scalable randomized algorithm designed by Du et al. [20]
estimates influence initiated by selected source nodes and thus
select seed set with maximum expected influence.

Cascade Prediction Although in [1], k-shell and heuristics
of k-shell were shown to be effective indicator of long-term
influence of nodes, in [21], experimental results showed that
the shell number of the root node is not effectively predictive in
the cascade by cascade scenario. Feature based methods from
Jenders et al. [22] Chen et al. [23] were designed to solve the
cascade prediction problem formulated as binary classification
on balanced dataset, however, these methods are more or less
dependent on content features from specific social media sites.
Ma et al. [24] focused on applying content features to classify
hashtag cascades by how much their size increases. Regarding
to point process based methods, model designed with the
intuition of mutual exciting nature of social influence, Zhou
et al [25] applied multi-dimensional Hawkes process to rank
cascades (memes) by their popularity. Recently, the model
introducted by Yu et al. [11] combined feature engineering
and human reaction time distribution function widely used
in point process based methods to aggregate adoptions in
subcascades for cascade prediction. Besides feature based
methods and point process based methods studied in this
paper, knowledge from related research fields could also be
applied to cascade prediction. Goyal et al. [26] proposed the
credit distribution model to learn pair-wise influence based
on IC model proposed by Kempe et al. [17]. Cui et al. [27]
proposed a feature selection approach for binary classification
of cascades. Wang et al. [28] proposed a model to decouple
the influence measured in a pair-wise way into two latent
vectors representing influence and susceptibility of a node.
This work differs from all the past efforts in that it is the most
thorough comparison of methods general enough to be applied
to different datasets without relying on features specific to a
certain social media site.
VII. D ISCUSSION AND C ONCLUSION
In this paper, we evaluate three categories of recently
proposed methods with both the classification and regression
formulaton of cascade prediction. Feature based methods
generally provide better prediction accuracy for the cascade
prediction problem, no matter it is considered as classification
or regression. However, they suffer from heavy overhead
such as community detection and computation of features.
Random point process based methods enable us to achieve the
prediction with little preprocessing but are shown to be less
accurate than feature based methods. The run time of methods
in this category can also suffer from the situation when the
data can not be well modelled by the proposed density function
Î»(t).
In regression experiments, we find the inconsitancy between
evaluation with different error metrics. A method that performs
well w.r.t. one metric could result in large error measured by
another. A predictive method should be able to perform fairly
well measured by various error metrics.
How to deal with changes in the social network and progress
of cascades to update features is the biggest issue that both
centrality based and feature based methods encounter. The

heavy overhead introduced by preprocessing and computation
of features limits these methods from near real-time prediction.
Point process based methods require little preprocessing
and the training and prediction process are parallelable as
they consider each cascade is indenpendent of others. This
advantage in terms of run time over feature based methods
can also be amplified as the size of the social network and the
number of cascades. Moreover, point process based methods
encounter little cold start problem. These two characteristics of
point process based methods make them more suitable for realtime cascade prediction task. But how to secure the accuracy
of prediction is the biggest issue for them. The point process
based models are faced with two more problems: sensitivity
to scale of time unit and requirement of prediction time as an
input variable. In real-world application, given a early stage
cascade, estimation of when it will stop progressing is a nontrivial problem.
On balance, this paper explored various methods in the
academic literature of predicting viral information cascades
in a more comprehensive manner. Our aim is to provide important insights into which methods based on graph topology
or temporal dynamics performed best - as these results can
generalize to a variety of application domains. In our ongoing
work on developing a deplyable system for identifying viral
extremist messages, this represents an important consideration.
Our next step is to consider microblog content as well - which
tends to be more domain specific.
ACKNOWLEDGMENTS
Some of the authors are supported through the AFOSR
Young Investigator Program (YIP) grant FA9550-15-1-0159,
ARO grant W911NF-15-1-0282, the DoD Minerva program
and the EU RISE program.
R EFERENCES
[1] S. Pei, L. Muchnik, J. S. Andrade Jr, Z. Zheng, and H. A. Makse,
â€œSearching for superspreaders of information in real-world social media,â€ Scientific reports, vol. 4, 2014.
[2] L. Weng, F. Menczer, and Y.-Y. Ahn, â€œPredicting successful memes
using network and community structure,â€ in Eighth International AAAI
Conference on Weblogs and Social Media, 2014.
[3] R. Guo, E. Shaabani, A. Bhatnagar, and P. Shakarian, â€œToward order-ofmagnitude cascade prediction,â€ in Proceedings of the 2015 IEEE/ACM
International Conference on Advances in Social Networks Analysis and
Mining 2015. ACM, 2015, pp. 1610â€“1613.
[4] Q. Zhao, M. A. Erdogdu, H. Y. He, A. Rajaraman, and J. Leskovec,
â€œSeismic: A self-exciting point process model for predicting tweet
popularity,â€ in Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 2015,
pp. 1513â€“1522.
[5] H. Shen, D. Wang, C. Song, and A.-L. BarabaÌsi, â€œModeling and
predicting popularity dynamics via reinforced poisson processes,â€ in
Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.
[6] N. Kim, S. Gokalp, H. Davulcu, and M. Woodward, â€œLookingglass: A
visual intelligence platform for tracking online social movements,â€ in
Advances in Social Networks Analysis and Mining (ASONAM), 2013
IEEE/ACM International Conference on. IEEE, 2013, pp. 1020â€“1027.
[7] J. Zhang, B. Liu, J. Tang, T. Chen, and J. Li, â€œSocial influence locality
for modeling retweeting behaviors.â€ in IJCAI, vol. 13, 2013, pp. 2761â€“
2767.
[8] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, â€œFast
unfolding of communities in large networks,â€ Journal of statistical
mechanics: theory and experiment, vol. 2008, no. 10, p. P10008, 2008.

[9] M. Rosvall and C. T. Bergstrom, â€œMaps of random walks on complex
networks reveal community structure,â€ Proceedings of the National
Academy of Sciences, vol. 105, no. 4, pp. 1118â€“1123, 2008.
[10] J. Ugander, L. Backstrom, C. Marlow, and J. Kleinberg, â€œStructural
diversity in social contagion,â€ Proceedings of the National Academy
of Sciences, vol. 109, no. 16, pp. 5962â€“5966, 2012.
[11] L. Yu, P. Cui, F. Wang, C. Song, and S. Yang, â€œFrom micro to
macro: Uncovering and predicting information cascading process with
behavioral dynamics,â€ arXiv preprint arXiv:1505.07193, 2015.
[12] S. Gao, J. Ma, and Z. Chen, â€œModeling and predicting retweeting
dynamics on microblogging platforms,â€ in Proceedings of the Eighth
ACM International Conference on Web Search and Data Mining. ACM,
2015, pp. 107â€“116.
[13] J. Alstott, E. Bullmore, and D. Plenz, â€œpowerlaw: a python package for
analysis of heavy-tailed distributions,â€ PloS one, vol. 9, no. 1, p. e85777,
2014.
[14] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, â€œScikit-learn: Machine learning in Python,â€ Journal of Machine
Learning Research, vol. 12, pp. 2825â€“2830, 2011.
[15] L. Waltman and N. J. van Eck, â€œA smart local moving algorithm
for large-scale modularity-based community detection,â€ The European
Physical Journal B, vol. 86, no. 11, pp. 1â€“14, 2013.
[16] G. Csardi and T. Nepusz, â€œThe igraph software package for complex
network research,â€ InterJournal, vol. Complex Systems, p. 1695, 2006.
[Online]. Available: http://igraph.org
[17] D. Kempe, J. Kleinberg, and EÌ. Tardos, â€œMaximizing the spread of
influence through a social network,â€ in Proceedings of the ninth ACM
SIGKDD international conference on Knowledge discovery and data
mining. ACM, 2003, pp. 137â€“146.
[18] W. Chen, Y. Wang, and S. Yang, â€œEfficient influence maximization in
social networks,â€ in Proceedings of the 15th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 2009, pp.
199â€“208.
[19] A. Goyal, W. Lu, and L. V. Lakshmanan, â€œCelf++: optimizing the greedy
algorithm for influence maximization in social networks,â€ in Proceedings
of the 20th international conference companion on World wide web.
ACM, 2011, pp. 47â€“48.
[20] N. Du, L. Song, M. Gomez-Rodriguez, and H. Zha, â€œScalable influence
estimation in continuous-time diffusion networks,â€ in Advances in neural
information processing systems, 2013, pp. 3147â€“3155.
[21] P. Shakarian, A. Bhatnagar, A. Aleali, E. Shaabani, and R. Guo,
Diffusion in Social Networks. Springer, 2015.
[22] M. Jenders, G. Kasneci, and F. Naumann, â€œAnalyzing and predicting
viral tweets,â€ in Proceedings of the 22nd international conference
on World Wide Web companion.
International World Wide Web
Conferences Steering Committee, 2013, pp. 657â€“664.
[23] J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec,
â€œCan cascades be predicted?â€ in Proceedings of the 23rd international
conference on World wide web. ACM, 2014, pp. 925â€“936.
[24] Z. Ma, A. Sun, and G. Cong, â€œOn predicting the popularity of newly
emerging hashtags in twitter,â€ Journal of the American Society for
Information Science and Technology, vol. 64, no. 7, pp. 1399â€“1410,
2013.
[25] K. Zhou, H. Zha, and L. Song, â€œLearning social infectivity in sparse lowrank networks using multi-dimensional hawkes processes,â€ in Proceedings of the Sixteenth International Conference on Artificial Intelligence
and Statistics, 2013, pp. 641â€“649.
[26] A. Goyal, F. Bonchi, and L. V. Lakshmanan, â€œA data-based approach to
social influence maximization,â€ Proceedings of the VLDB Endowment,
vol. 5, no. 1, pp. 73â€“84, 2011.
[27] P. Cui, S. Jin, L. Yu, F. Wang, W. Zhu, and S. Yang, â€œCascading outbreak
prediction in networks: a data-driven approach,â€ in Proceedings of the
19th ACM SIGKDD international conference on Knowledge discovery
and data mining. ACM, 2013, pp. 901â€“909.
[28] Y. Wang, H. Shen, S. Liu, and X. Cheng, â€œLearning user-specific latent
influence and susceptibility from information cascades,â€ in Twenty-Ninth
AAAI Conference on Artificial Intelligence, 2015.

The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence
Artificial Intelligence for Cyber Security: Technical Report WS-16-03

Toward Argumentation-Based Cyber Attribution
Eric Nunes and Paulo Shakarian

Gerardo I. Simari

Arizona State University
Tempe, AZ 85281, USA
Email: {enunes1, shak}@asu.edu

Inst. for Computer Science and Engineering
(Univ. Nac. del Surâ€“CONICET), Bahia Blanca, Argentina
E-mail: gis@cs.uns.edu.ar

Abstract

multi-label classification problem and applied several machine learning and pruning techniques; the results of this
work are discussed in (Nunes et al. 2015). Based on our observations, machine learning approaches fail in situations of
deception, where similar attributes point towards multiple
attackers. Standard machine learning approaches treat this
problem as â€œlabeling noiseâ€ leading to a random selection
of attacker. We propose to address this issue using a formal
logical framework. Specific contributions of this paper include,
â€¢ a model for cyber-attribution created in the DeLP argumentation framework;

A major challenge in cyber-threat analysis is combining information from different sources to find the person or the
group responsible for the cyber-attack. It is one of the most
important technical and policy challenges in cyber-security.
The lack of ground truth for an individual responsible for an
attack has limited previous studies. In this paper, we overcome this limitation by building a dataset from the capturethe-flag event held at DEFCON, and propose an argumentation model based on a formal reasoning framework called
DeLP (Defeasible Logic Programming) designed to aid an
analyst in attributing a cyber-attack to an attacker. We build
argumentation-based models from latent variables computed
from the dataset to reduce the search space of culprits (attackers) that an analyst can use to identify the attacker. We show
that reducing the search space in this manner significantly improves the performance of classification-based approaches to
cyber-attribution.

â€¢ experiments demonstrating that using argumentationbased tools can significantly reduce the number of potential culprits in a cyber-attack; and
â€¢ experiments showing that the reduced set of culprits,
used in conjunction with classification, leads to improved
cyber-attribution decisions.

Introduction
A major challenge in cyber-threat analysis is to find the
person or the group responsible for a cyber-attack. This is
known as cyber-attribution (Shakarian, Shakarian, and Ruef
2013) and it is one of the central technical and policy challenges in cyber-security. Oftentimes, the evidence collected
from multiple sources provides a contradictory viewpoint.
This gets worse in cases of deception where either an attacker plants false evidence or the evidence points to multiple actors, leading to uncertainty. In our text on cyberwarfare (Shakarian, Shakarian, and Ruef 2013) we discuss
the difficulties that an intelligence analyst faces in attributing an attack to a perpetrator given that deception might have
occurred, and how the analyst needs to explore deception hypotheses under the given attack scenario.
However, one of the major drawbacks of the study and
evaluation of cyber-attribution models is the lack of datasets
with the ground truth available regarding the individual party
responsible for the attackâ€”this has limited previous studies.
To overcome this, we built and leveraged a dataset from the
capture-the-flag event held at DEFCON (cf. the dataset section for a detailed discussion). In previous work, we used
this dataset to study cyber-attribution, and framed it as a

Related Work
Currently, cyber-attribution is limited to identifying machines (Boebert 2010) as opposed to the hacker or their affiliation to a group or a state organization. An example of
such a technical attribution approach is WOMBAT (Dacier,
Pham, and Thonnard 2009), where a clustering technique
is used to group attacks to common IP sources. A method
that combines information from different sources was proposed in (Walls 2014), where forensic information from diverse sources is considered but inconsistency or uncertainty
due to deception is not considered. A less rigorous mathematical model, known as the Q model (Rid and Buchanan
2015), has been proposed recently; there, the model answers queries from an analyst, both technical (tools used)
and non-technical (environment related), and by combining
these query answers the analyst tries to attribute an attack
to a party. Unfortunately, there are no experimental results
evaluating the effectiveness of this model.
Concurrently, we have devised a formal logical framework for reasoning about cyber-attribution (Shakarian et
al. 2015a; 2015b). This reasoning model explores multiple
competing hypotheses based on the evidence for and against
a particular attacker before it attributes the attack to a specific party. With this approach we get a clear map regarding

c 2016, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

177

tion, we used the open source tool tcpflow1 to process the
network captures into a set of files, with each file representing data sent or received on a particular connection.
This produced a corpus of data that could be searched and
processed with standard UNIX tools, like grep. Further analysis of the game environment provided an indicator of when
a data file contained an exploit. The game stored keys for
services in a standard, hard-coded location on each competitorâ€™s server. By searching for the text of this location in the
data, we identified data files that contained exploits for services.
With these data files identified, we analyzed some of them
by hand using the Interactive Disassembler (IDA) to determine if the data contained shell-code, which in fact was the
case. We used an automated tool to produce a summary of
each data file as a JSON encoded element. Included in this
summary was a hash of the contents of the file and a histogram of the processor instructions contained in the file.
These JSON files were the final output of the low level analysis, transforming hundreds of gigabytes of network traffic into a manageable set of facts about exploit traffic in
the data. Each JSON file is a list of tuples (time-stamp,
hash, byte-histogram, instruction-histogram). The individual
fields of the tuple are listed in Table 1.

the arguments that led to the decision. This paper is the first
to provide experimental results using a formal logical framework to build a reasoning model.
The rest of the paper is organized as follows. We present
the argumentative model based on Default Logic Programming (DeLP) (GarcÄ±Ìa and Simari 2004). This is followed by
a description of our DEFCON capture-the-flag dataset and
an analysis on the use of deception within this data. We then
give a summary of our results from (Nunes et al. 2015) and
discuss how we built our argumentation model for cyberattribution with DeLP. Experimental results are discussed in
the subsequent section.

DEFCON CTF Dataset
The DEFCON security conference sponsors and hosts a
capture-the-flag (CTF) competition every year, held on site
with the conference in Las Vegas, Nevada. The CTF competition can be viewed as a microcosm of the global Internet and the careful game of cat and mouse between hacking
groups and security firms. Teams are free to use different
technical means to discover vulnerabilities: they may reverse
engineer programs, monitor the network data sent to their
services, and dynamically study the effects that network data
has on unpatched services. If a team discovers a vulnerability and uses it against another team, the first team may discover that their exploit is re-purposed and used against them
within minutes.
The organizers of DEFCON CTF capture all of the network traffic sent and received by each team, and publish this
traffic at the end of the competition (DEFCON 2013). This
includes IP addresses for source and destination, as well as
the full data sent and received and the time the data was sent
or received. This data is not available to contestants; depending on the organizersâ€™ choice from year to year, the contestants either have a real time feed but with the IP address
obscured, or a full feed delivered on a time delay ranging
from minutes to hours.
In addition to the traffic captures, copies of the vulnerable services are distributed by the organizers, who usually
do not disclose the vulnerabilities they engineered into each
service; however, competitors frequently disclose this information publicly after the game is finished by preparing technical write-ups.
The full interaction of all teams in the game environment
are captured by this data. We cannot build a total picture of
the game at any point in time, since there is state information
from the servers that is not captured, but any exploit attempt
would have to travel over the network and that would be
observed in the data set.

Table 1: Fields in an instance of network attack
Field

Intuition

byte hist

Histogram of byte sequences in the payload

inst hist

Histogram of instructions used in the
payload

from team

The team where the payload originates
(attacking team)

to team

The team being attacked by the exploit

svc

The service that the payload is running

payload hash

The md5 of the payload used

time

Date and time of the attack

Table 2: Example event from the dataset
Field

Value

byte hist

0Ã—43:245, 0Ã—69:8, 0Ã—3a:9, 0Ã—5d:1,
.....

inst hist

cmp:12, svcmi:2, subs:8, movtmi:60
......

Analysis

from team

Robot Mafia

We use the data from the CTF tournament held at DEFCON 21 in 2013; the dataset is very large, about 170 GB
in compressed format. We used multiple systems with distributed and coordinated processing to analyze the entire
corpusâ€”fortunately, analyzing individual streams is easy to
parallelize. To analyze this data, we identified the TCP ports
associated with each vulnerable service. From this informa-

to team

Blue Lotus

svc

02345

payload hash

2cc03b4e0053cde24400bbd80890446c

time

2013-08-03T23:45:17

1

178

https://github.com/simsong/tcpflow

Number of Attacks

400000

tion, as it must have the capability to reason about inconsistency in cases of deception.
Before describing the argumentation model in detail, we
introduce some necessary notation. Variables and constant
symbols represent items such as the exploits/payloads used
for the attack, and the actors conducting the cyber-attack (in
this case, the teams in the CTF competition). We denote the
set of all variable symbols with V and the set of all constants
with C. For our model we require two subsets of C: Cact , denoting the actors capable of conducting the cyber-operation,
and Cexp , denoting the set of unique exploits used. We use
symbols in all capital letters to denote variables. The running
example is based on a subset of our DEFCON CTF dataset:
Example 1. Consider the following actors and cyberoperations from the CTF data:

300000

200000

100000

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

T-3

T-2

T-1

0

Teams

Unique Attacks

Deceptive Attacks

Figure 1: Number of unique and deceptive attacks directed
towards each team.

Cact

After this pre-processing of the network data packets, we
obtained around 10 million network attacks. There are 20
teams in the CTF competition; in order to attribute an attack to a particular team, apart from analyzing the payloads
used by the team, we also need to analyze the behavior of
the attacking team towards their adversary. For this purpose,
we separate the network attacks according to the team being
targeted. Thus, we have 20 such subsets, which we represent
as T-i, where i âˆˆ {1, 2, 3, ..., 20}. An example of an event in
the dataset is shown in Table 2.
We now discuss two important observations from the
dataset, which make the task of attributing an observed network attack to a team difficult.
Deception: In the context of this paper we define an attack
to be deceptive when multiple adversaries get mapped to a
single attack pattern. In the current setting we define deception as the scenario in which the same exploit is used by
multiple teams to target the same team. Figure 1 shows the
distribution of unique deception attacks with respect to the
total unique attacks in the dataset based on the target team.
These unique deceptive attacks amount to just under 35% of
the total unique attacks.

Cexp

= {bluelotus, robotmafia, apt8}
= {exploit1 , exploit2 , ..., exploitn }


The language also contains a set of predicate symbols that
have constants or variables as arguments, and denote events
that can be either true or false. We denote the set of predicates with P; examples of predicates are shown in Table 3.
For instance, culprit(exploit1 , bluelotus) will either be true
or false, and denotes the event where bluelotus used exploit1
to conduct a cyber-operation.
A ground atom is composed by a predicate symbol and a
tuple of constants, one for each of the predicateâ€™s arguments.
The set of all ground atoms is denoted as G. A literal L
is a ground atom or a negated ground atom; hence, literals
have no variables. Examples of ground atoms formed using
predicates in Table 3 are shown in the following example.
We denote a subset of G with G0 .

Example 2. The following are examples of ground atoms
over the predicates given in Table 3.
G0 :

Duplicate attacks: A duplicate attack occurs when the same
team uses the same payload to attack a team at different
points in time. Duplicate attacks can be attributed to two reasons. First, when a team is trying to compromise anotherâ€™s
system, it does not just launch a single attack but a wave
of attacks with very little time difference between consecutive attacks. Second, once a successful payload is created
that can penetrate the defense of other systems, it is used
more by the original attacker as well as the deceptive one as
compared to other payloads. We group duplicates as either
being non-deceptive or deceptive. Non-deceptive duplicates
are the copies of the attacks launched by the team that first
initiated the use of a particular payload; on the other hand,
deceptive duplicates are all the attacks from the teams that
did not initiate the use.

attack(exploit1 , bluelotus),
deception(exploit1 , apt8),
culprit(exploit1 , apt8)

Table 3: Example Predicates and explanation

Argumentation Model
Our approach relies on a model of the world where we
can analyze competing hypotheses in a cyber-operation scenario. Such a model must allow for contradictory informa-

179

Predicate

Explanation

attack(exploit1 , bluelotus)

exploit1 was targeted towards
the team Blue Lotus.

replay attack(E, Y)

Exploit E was replayed by
team Y.

deception(exploit1 , apt8)

Team apt8 used exploit1 for deception.

time diff(I, Y)

Team Y was deceptive within
the given time interval I.

culprit(exploit1 , apt8)

Team apt8 is the likely culprit
for the attack (using exploit1 on
the target team).



We choose a structured argumentation framework (Rahwan, Simari, and van Benthem 2009) to build our argumentation model, which allows for competing ideas to deal
with contradictory information. Due to such characteristics,
argumentation-based models are favorable for cyber-attack
scenarios. Our approach works by creating arguments (in the
form of a set of rules and facts) that compete with each other
to attribute an attack to a given perpetrator. In this case, arguments are defeated based on contradicting information in
other arguments. This procedure is known as a dialectical
process, where the arguments that are undefeated prevail.
An important result is the set of all the arguments that are
warranted (not defeated) by any other argument, which give
a clear map of not only the set of attackers responsible for
the cyber-operation but also the arguments supporting the
conclusion.
The transparency of the model lets a security analyst not
only add new arguments based on new evidence discovered
in the system, but also get rid of incorrect information and
fine-tune the model for better performance. Since the argumentation model can deal with inconsistent information, it
draws a natural analogy to the way humans settle disputes
when there is contradictory information available (GarcÄ±Ìa
and Simari 2004). The model provides a clear explanation as
to why one argument is chosen over others, which is a desirable characteristic for both the analyst and for organizations
to make decisions and policy changes. We now discuss some
preliminaries for the argumentation model.

Î˜:

Î¸1
Î¸2
Î¸3
Î¸4
Î¸5

=
=
=
=
=

â„¦:

Ï‰1 =
Ï‰2 =

âˆ†:

Î´1 =
Î´2 =
Î´3 =
Î´4 =

attack(exploit1 , bluelotus)
first attack(exploit1 , robotmafia)
last attack(exploit1 , apt8))
time diff(interval, robotmafia)
most frequent(exploit1 , pwnies)
culprit(exploit1 , pwnies) â†
most frequent(exploit1 , pwnies),
replay attack(exploit1 )
Â¬ culprit(exploit1 , robotMafia) â†
last attack(exploit1 , apt8),
replay attack(exploit1 )
replay attack(exploit1 ) -â‰º
attack(exploit1 , bluelotus),
last attack(exploit1 , apt8)
deception(exploit1 , apt8) -â‰º
replay attack(exploit1 ),
first attack(exploit1 , robotmafia)
culprit(exploit1 , apt8) -â‰º
deception(exploit1 , apt8),
replay attack(exploit1 )
Â¬culprit(exploit1 , apt8) -â‰º
time diff(interval, robotmafia)

Figure 2: A ground argumentation framework.
the defeasible counterparts of strict rules; they are of the
form L0 -â‰º L1 , ...., Ln , where L0 , is the ground literal and
{Li }i>0 is a set of ground literals. Strong negation is allowed for both strict and defeasible rules to represent contradictory information.

Defeasible Logic Programming
DeLP is a formalism that combines logic programming
with defeasible argumentation; full details are discussed
in (GarcÄ±Ìa and Simari 2004). The formalism is made up of
several constructs, namely facts, strict rules, and defeasible
rules. Facts represent statements obtained from evidence,
and are always true; similarly, strict rules are logical combinations of facts that always hold. On the contrary, defeasible rules can be thought of as strict rules that may be true in
some situations, but could be false if contradictory evidence
is present. These three constructs are used to build arguments. DeLP programs are sets of facts, strict rules and defeasible rules. We use the usual notation to denote DeLP programs: denoting the knowledge base with Î  = (Î˜, â„¦, âˆ†),
where Î˜ is the set of facts, â„¦ is the set of strict rules, and
âˆ† is the set of defeasible rules. Examples of the three constructs are provided with respect to the dataset in Figure 2
and in the Latent variables section. We now describe the
constructs in detail.
Facts (Î˜) are ground literals that represent atomic information or its negation (Â¬). The facts are always true and cannot
be contradicted.
Strict Rules (â„¦) represent cause and effect information that
is always true. They are built from using a combination of
ground literals and are of the form L0 â† L1 , ...Ln , where
L0 is a ground literal and {Li }i>0 is a set of ground literals.
Defeasible Rules (âˆ†) comprise knowledge that can be true
if no contradictory information is provided. These rules are

When a cyber-attack occurs, the model derives arguments
as to who could have conducted the attack. Derivation follows the same mechanism as logic programming (Lloyd
2012). DeLP incorporates defeasible argumentation, which
decides which arguments are warranted and it blocks arguments that are in conflict.
Figure 2 shows a ground argumentation framework
demonstrating constructs derived from the CTF data. For
instance, Î¸1 indicates the fact that exploit1 was used to target
the team Blue Lotus, and Î¸5 indicates that team pwnies is the
most frequent user of exploit1 . For the strict rules, Ï‰1 says
that for a given exploit1 the attacker is pwnies if it was the
most frequent attacker and the attack exploit1 was replayed.
Defeasible rules can be read similarly; Î´2 indicates that
exploit1 was used in a deceptive attack by APT8 if it was
replayed and the first attacker was not APT8. By replacing
the constants with variables in the predicates we can derive
a non-ground argumentation framework.

Definition 1. (Argument) An argument for a literal L is a
pair hA, Li, where A âŠ† Î  provides a minimal proof for L
meeting the requirements: (1) L is defeasibly derived from
A, (2) Î˜âˆªâ„¦âˆªâˆ† is not contradictory, and (3) A is a minimal
subset of âˆ† satisfying 1 and 2, denoted hA, Li.

180





A1 , replay attack(exploit1 ) 

A2 , deception(exploit1 , apt8)


A3 , culprit(exploit1 , apt8) 
A4 , Â¬culprit(exploit1 , apt8)

A1
A2
A3
A4

= {Î´1 , Î¸1 , Î¸3 }
= {Î´1 , Î´2 , Î¸2 }
= {Î´1 , Î´2 , Î´3 }
= {Î´1 , Î´4 , Î¸3 }

Î˜:

Î¸1 =
Î¸2 =
Î¸3 =

attack (E, X)
first attack (E, Y)
last attack (E, Y)

Figure 4: Facts defined for each test sample.

Figure 3: Example ground arguments from Figure 2.
while the set of facts and strict rules is consistent (noncontradictory), the set of defeasible rules can be inconsistent. We engineer our cyber-attribution framework as a series of defeasible and strict rules whose structure we have
created, but are dependent on values learned from a historical corpus. Then, for a given incident, we instantiate a set
of facts for that situation. This information is then provided
as input into a DeLP implementation that uses heuristics to
generate all arguments for and against every possible culprit
for the cyber attack. Then, the DeLP implementation creates dialectical trees based on these arguments and decides
which culprits are warranted. This results in a reduced set of
potential culprits, which we then use as input into a classifier
to obtain an attribution decision.

Literal L is called the conclusion supported by the argument, and A is the support. An argument hB, Li is a subargument of hA, L0 i iff B âŠ† A.
The following shows arguments for our running example
scenario.
Example 3. Figure 3 shows example arguments based on
the knowledge base from Figure 2. Note that the following
relationship exists:




A1 , replay attack(exploit1 ) is
 a subargument of
A
,
deception(exploit
,
apt8)
and
2
1 


A3 , culprit(exploit1 , apt8) .


Latent Variables

For a given argument there may be counter-arguments that
contradict it. For instance, referring to Figure 3, we can see
that A4 attacks A3 . A proper defeater of an argument hA, Li
is a counter-argument thatâ€”by some criterionâ€”is considered to be better than hA, Li; if the two are incomparable
according to this criterion, the counterargument is said to be
a blocking defeater. The default criterion used in DeLP for
argument comparison is generalized specificity (Stolzenburg
et al. 2003).
A sequence of arguments is referred to as an argumentation line. There can be more than one defeater argument,
which leads to a tree structure that is built from the set of
all argumentation lines rooted in the initial argument. In a
dialectical tree, every child can defeat its parent (except for
the root), and the leaves represent the undefeated arguments.
The tree thus creates a map of all possible argumentation
lines that decide if an argument is defeated


or not.
Given a literal L and an argument A, L , in order to decide whether or not a literal L is warranted, every node in the
dialectical tree T (hA, Li) is recursively marked as â€œDâ€ (defeated) or â€œUâ€ (undefeated), obtaining a marked dialectical
tree T âˆ— (hA, Li) where:

In (Nunes et al. 2015) we leveraged machine learning techniques on the CTF data to identify the attacker. We will now
provide a summary of the results obtained.
The experiment was performed as follows. We divide the
dataset according to the target team, building 20 subsets, and
all the attacks are then sorted according to time. We reserve
the first 90% of the attacks for training and the remaining
10% for testing. The byte and instruction histograms are
used as features to train and test the model. Models constructed using a random forest classifier performed the best,
with an average accuracy of 0.37. Most of the misclassified
samples tend to be deceptive attacks and their duplicates.
When using machine learning approaches it is difficult to
map the reasons why a particular attacker was predicted, especially in cases of deception where multiple attackers were
associated with the same attack. Knowing the arguments that
supported a particular decision would greatly aid the analyst in making better decisions dealing with uncertainty. To
address this issue we now describe how we can form arguments/rules based on the latent variables computed from the
training data, given an attack for attribution.
We use the following notation: let E be the test attack under consideration aimed at target team X, Y represent all the
possible attacking teams, and D be the set of all deceptive
teams (those using the same payload to target the same team)
if the given attack is deceptive in the training set. For nondeceptive attacks, D will be empty. We note that facts cannot have variables, only constants (however, to compress the
program for presentation purposes, we use meta-variables
in facts). To begin, we define the facts described in Figure 4;
fact Î¸1 states that attack E was used to target team X, Î¸2
states that team Y was the first team to use the attack E in the
training data, and similarly Î¸3 states that team Y was the last
team to use the attack E in the training data. The first and
last attacking team may or may not be the same. We study

â€¢ All leaves in T âˆ— (hA, Li) are marked as â€œUâ€s, and
â€¢ Let hB, qi be an inner node of T âˆ— (hA, Li). Then, hB, qi
will be marked as â€œUâ€ iff every child of hB, qi is marked
as â€œDâ€. Node hB, qi will be marked as â€œDâ€ iff it has at
least a child marked as â€œUâ€.
Given argument hA, Li over Î , if the root of T âˆ— (hA, Li) is
marked â€œUâ€, then T âˆ— (hA, hi) warrants L and that L is warranted from Î . (Warranted arguments correspond to those
in the grounded extension of a Dung argumentation system
(Dung 1995).)
In practice, an implementation of DeLP accepts as input sets of facts, strict rules, and defeasible rules. Note that

181

â„¦:

Ï‰2 =
âˆ†:

Î´1 =
Î´2 =
Î´3 =

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

culprit(E, Df ) â† frequent(E, Df ),
deception (E, Di )
Â¬culprit(E, Y) â† first attack(E, Y),
deception(E, X)

0

T-7

Ï‰1 =

200

T-6

deception (E, X)
frequent (E, Df )

400

T-5

Î¸1 =
Î¸2 =

600

Teams

Figure 7: Average time for duplicate attacks for each team.
16000

Average time (sec) to issue deceptive
attack

Î˜:

800

T-1

Figure 5: Defeasible and strict rule for non-deceptive attack.

1000

T-4

replay attack(E) -â‰º attack(E, X),
last attack(E, Y).

1200

T-3

Î´1 =

culprit(E, Y) â† last attack(E, Y),
replay attack(E).

T-2

âˆ†:

Ï‰1 =

Average time (sec) to replay own attacks

â„¦:

replay attack(E) -â‰º attack(E, X),
last attack(E, Y)
deception(E, Di ) -â‰º replay attack(E),
first attack(E, Y)
culprit(E, Di ) -â‰º deception(E, Di ),
first attack(E, Y)

14000
12000
10000
8000
6000
4000
2000

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-9

T-11

Figure 6: Facts and rules for deceptive attacks.

T-10

T-8

T-7

T-6

T-5

T-4

T-3

T-2

T-1

0

Teams

Figure 8: Average time for deceptive attacks for each team.
the following three cases:
Case 1: Non-deceptive attacks. In non-deceptive attacks,
only one team uses the payload to target other teams in the
training data. It is easy to predict the attacker for these cases,
since the search space only has one team. To model this situation, we define the set of defeasible and strict rules shown
in Figure 5. Defeasible rule Î´1 checks whether the attack was
replayed in the training dataâ€”since it is a non-deceptive attack, it can only be replayed by the same team. Strict rule Ï‰1
then puts forth an argument for the attacker (culprit) if the
defeasible rule holds and there is no contradiction for it.

the K nearest neighbors from the training set according to
a simple Euclidean distance between the byte and instruction histograms of the two attacks. In this case we choose
K = 3. For each of the matching attack from the training
data we check if the attack is deceptive or non-deceptive. If
non-deceptive, we follow the procedure for Case 1, otherwise we follow the procedure for Case 2. Since we replace
one unseen attack with three seen attacks, the search space
for the attacker increases for unseen attacks.

Case 2: Deceptive attacks. These attacks form the majority of the misclassified samples in (Nunes et al. 2015). The
set D is not empty for this case; let Di denote the deceptive
teams in D. We also compute the most frequent deceptive
attacker from D. Let the most frequent attacker be denoted
as Df . Figure 6 shows some of the DeLP components that
model this case. Fact Î¸1 indicates if the attack E was deceptive towards the team X and Î¸2 indicates the most frequent
attacker team Df from the deceptive team set D. The strict
rules Ï‰1 states that the attacker should be Df if the attack is
deceptive and Ï‰2 indicates that in case of deception the first
attack team Y is not the attacker. For the defeasible rules, Î´1
deals with the case in which the attack E was replayed, Î´2
deals with the case of deceptive teams from the set D and
Î´3 indicates that all the deceptive teams are likely to be the
attackers in the absence of any contradictory information.

Attacker Time Analysis
The CTF data provides us with time stamps for the attacks
in the competition. We can use this information to come up
with rules for/against an argument for a team being the attacker. We compute the average time for a team to replay its
own attack given that it was the first one to initiate the attack (see Figure 7). It can be observed that teams like more
smoked leet chicken (T-13) and Wowhacker-bios (T-8) are
very quick to replay their own attacks as compared to other
teams.
Figure 8 on the other hand shows the average time for a
team to replay an attack initiated by some other team. This
refers to the time the team takes to commit a deceptive attack. Teams like The European (T-7) and Blue lotus (T-10)
are quick to commit deception, while others take more time.
We use this time information to narrow down our search
space for possible attackers. In particular, for a deceptive
test sample, we compute the time difference between the test
sample and the training sample that last used the same payload. We denote this time difference as 4t, and include it

Case 3: Unseen Attacks. The most difficult attacks to attribute in the dataset are the unseen ones, i.e., attacks encountered in the test set that did not occur in the training set.
To build constructs for this kind of attack we first compute

182

9

as a fact Î¸1 . We then divide the deceptive times from Figure 8 into appropriate intervals; each team is assigned to one
of those time intervals. We then check which time interval
4t belongs to and define a defeasible rule Î´1 that makes a
case for all teams not belonging to the interval to not be the
culprits, as shown in Figure 9.
Î˜:

Î¸1 =

timedifference (E, X)

Î´1 =

For Y âˆˆ
/ interval:
Â¬culprit(E, Y) -â‰º timedifference (E, X).

Average number of Teams

8
7
6

5
4
3
2
1

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

T-3

T-2

Teams

Figure 10: Average Search space after running the query to
compute warranted arguments.

Figure 9: Time facts and rules. Interval indicates a small portion of the entire deceptive time (for instance < 2000 sec.,
> 8000 sec., and so on).

Fraction of test samples with ground
truth

1

Experiments
We use the same experimental setup as in (Nunes et al.
2015). We sort the dataset by time for each target team and
then use the first 90% of the data for training and the rest
10% for testing. We compute the constructs for all test samples based on the cases discussed in the previous section, and
then input these arguments to the DeLP implementation. For
each test sample we query the DeLP system to find all possible attackers (culprits) based on the arguments provided. If
there is no way to decide between contradicting arguments,
these are blocked and thus return no answers. Initially, the
search space for each test sample is 19 teams.
Figure 10 shows the average search space after running
the queries to return set of possible culprits. There is a significant reduction in search space across all target teams. The
average search space is reduced from 19 teams to 5.8 teams
(standard deviation of 1.92). To evaluate how much the reduced search space can aid an analyst in predicting the actual culprit, we compute one more metricâ€”we check if the
reduced search space has the ground truth (actual culprit)
present in it. Figure 11 shows the fraction of test samples
with ground truth present in it. For the majority of the target
teams, we have ground truth present in more than 70% of
the samples with reduced search space. For some teams like
more smoked leet chicken (T-13) and raon ASRT (whois) (T17) the average reduced search space is as low as 1.82 and
2.9 teams, with high ground truth fraction of 0.69 and 0.63
respectively. The reason why there is a large search space
for some teams is the presence of a larger number of unseen
attacks. As discussed earlier, each unseen attack is replaced
by the three most similar attacks from the training set, and
hence the search space becomes larger.
We next perform predictive analysis on the reduced search
space. The experimental setup is similar to the one described
earlier; the only difference is that now instead of having a
19-team search space as in the previous case, the machine
learning approach is allowed to make a prediction from the
reduced search space only. We use random forest as the machine learning approach, which has shown to have the best
performance for CTF data (Nunes et al. 2015). Table 4 gives
a summary of the results.

0.8

0.6

0.4

0.2

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-10

T-9

T-8

T-7

T-6

T-5

T-4

T-3

T-1

0

T-2

âˆ†:

T-1

0

Teams

Figure 11: Fraction of test samples with the ground truth in
the reduced search space.
We report three accuracies for the 20 teams. â€œPrev. Acc.â€
represents the accuracies achieved after running Random
forest without applying the argumentation based techniques,
as reported in (Nunes et al. 2015). â€œAcc. (ground truth)â€ considers only the test samples where ground truth is present
in the reduced search space. With this setting we are able
to correctly predict on average more than 70% of the test
samples, with T-4, T-13 and T-14 having accuracies over
80%. On the other hand, â€œAcc. (all)â€ also takes into account
test samples where ground truth was not present in the reduced search space. It is clear that all those test samples will
be misclassified since the ground truth is not present in the
search space at all. Even accounting for those samples, the
average accuracy is 0.5, which is significantly better than the
average accuracy of 0.37 in (Nunes et al. 2015). The results
are statistically significant (t(20) = 6.83, p < 0.01).

Conclusion
In this paper we showed how an argumentation-based framework (DeLP) can be leveraged to improve cyber-attribution
decisions. We demonstrated this concept using the DeLP
framework applied to CTF data, which reduced the set of
potential culprits allowing for greater accuracy when using
a classifier for cyber-attribution.
We are currently looking at implementing our probabilistic variant of DeLP (Shakarian et al. 2015b), which assigns
probabilities to the arguments, helping in this way to decide
which arguments prevail when contradictory information is

183

Table 4: Summary of Results
Team

Acc. (ground
truth)

Acc. (all)

Prev.
Acc.

T-1

0.69

0.51

0.45

T-2

0.72

0.45

0.22

T-3

0.55

0.40

0.30

T-4

0.81

0.44

0.26

T-5

0.68

0.45

0.26

T-6

0.62

0.49

0.5

T-7

0.70

0.53

0.45

T-8

0.75

0.61

0.42

T-9

0.68

0.50

0.41

T-10

0.77

0.42

0.30

T-11

0.62

0.44

0.37

T-12

0.76

0.43

0.24

T-13

0.85

0.63

0.35

T-14

0.71

0.52

0.42

T-15

0.57

0.38

0.30

T-16

0.68

0.48

0.43

T-17

0.80

0.58

0.42

T-18

0.68

0.50

0.48

T-19

0.70

0.51

0.41

T-20

0.74

0.51

0.48

programming and n-person games. Artificial intelligence
77(2):321â€“357.
GarcÄ±Ìa, A. J., and Simari, G. R. 2004. Defeasible logic programming: An argumentative approach. Theory and practice of logic programming 4(1+ 2):95â€“138.
Lloyd, J. W. 2012. Foundations of logic programming.
Springer Science & Business Media.
Nunes, E.; Kulkarni, N.; Shakarian, P.; Ruef, A.; and Little,
J. 2015. Cyber-deception and attribution in capture-the-flag
exercises. In Proceedings of the 2015 International Symposium on Foundation of Open Source Intelligence and Security Informatics (FOSINT-SI).
Rahwan, I.; Simari, G. R.; and van Benthem, J. 2009. Argumentation in artificial intelligence, volume 47. Springer.
Rid, T., and Buchanan, B. 2015. Attributing cyber attacks.
Journal of Strategic Studies 38(1-2):4â€“37.
Shakarian, P.; Simari, G. I.; Moores, G.; and Parsons, S.
2015a. Cyber attribution: An argumentation-based approach. In Cyber Warfare. Springer. 151â€“171.
Shakarian, P.; Simari, G. I.; Moores, G.; Paulo, D.; Parsons,
S.; Falappa, M.; and Aleali, A. 2015b. Belief revision in
structured probabilistic argumentation. Annals of Mathematics and Artificial Intelligence 1â€“43.
Shakarian, P.; Shakarian, J.; and Ruef, A. 2013. Introduction
to cyber-warfare: A multidisciplinary approach. Newnes.
Stolzenburg, F.; GarcÄ±Ìa, A. J.; Chesnevar, C. I.; and Simari,
G. R. 2003. Computing generalized specificity. Journal of
Applied Non-Classical Logics 13(1):87â€“113.
Walls, R. J. 2014. Inference-based Forensics for Extracting Information from Diverse Sources. Ph.D. Dissertation,
University of Massachusetts Amherst.

present. We are also designing our own CTF event in order
to obtain more realistic data.

Acknowledgments
Authors of this work were supported by the U.S. Department of the Navy, Office of Naval Research, grant N0001415-1-2742 as well as the Arizona State University Global
Security Initiative (GSI) and funds provided by CONICET
and Universidad Nacional del Sur, Argentina. Any opinions,
findings, and conclusions or recommendations expressed in
this material are those of the author(s) and do not necessarily
reflect the views of the Office of Naval Research.

References
Boebert, W. E. 2010. A survey of challenges in attribution.
In Proceedings of a workshop on Deterring CyberAttacks,
41â€“54.
Dacier, M.; Pham, V.-H.; and Thonnard, O. 2009. The wombat attack attribution method: some results. In Information
Systems Security. Springer. 19â€“37.
DEFCON. 2013. Defcon: Capture the flag.
Dung, P. M. 1995. On the acceptability of arguments
and its fundamental role in nonmonotonic reasoning, logic

184

KSGM: Keynode-driven Scalable Graph Matching
Xilun Chen, K. Selçuk Candan
Arizona State University Tempe, AZ, USA {xilun.chen, candan}@asu.edu

Maria Luisa Sapino
University of Torino Torino, Italy marialuisa.sapino@unito.it

Paulo Shakarian
Arizona State University Tempe, AZ, USA shak@asu.edu

ABSTRACT
Understanding how a given pair of graphs align with each other (also known as the graph matching problem) is a critical task in many search, classification, and analysis applications. Unfortunately, the problem of maximum common subgraph isomorphism between two graphs is a well known NP-hard problem, rendering it impractical to search for exact graph alignments. While there are several heuristics, most of these analyze and encode global and local structural information for every node of the graph and then rank pairs of nodes across the two graphs based on their structural similarities. Moreover, many algorithms involve a post-processing (or refinement) step which aims to improve the initial matching accuracy. In this paper 1 we note that the expensive refinement phase of graph matching algorithms is not practical in any application where scalability is critical. It is also impractical to seek structural similarity between all pairs of nodes. We argue that a more practical and scalable solution is to seek structural keynodes of the input graphs that can be used to limit the amount of time needed to search for alignments. Naturally, these keynodes need to be selected carefully to prevent any degradations in accuracy during the alignment process. Given this motivation, in this paper, we first present a structural keynode extraction (SKE) algorithm and then use structural keynodes obtained during off-line processing for keynode-driven scalable graph matching (KSGM). Experiments show that the proposed keynode-driven scalable graph matching algorithms produce alignments that are as accurate as (or better than) the state-of-the-art algorithms, with significantly faster online executions.

Figure 1: Graph matching/alignment problem seeks a maximum common subgraph isomorphism between two input graphs they represent the pairwise relationships between the nodes of the graph. Edges can be directed or undirected, meaning that the relationship can be non-symmetric or symmetric, respectively. Nodes and edges of the graph can also be labeled or non-labeled. The label of an edge, for example, may denote the name of the relationship between the corresponding pair of nodes or may represent other meta-data, such as the certainty of the relationship or the cost of leveraging that relationship within an application. Due to the success of the graph model as a powerful and flexible data representation, graph analysis and search tasks are also increasingly critical in many application domains. In particular, understanding how a given set of graphs align with each other (also known as the graph matching/alignment problem, Figure 1) forms the core task in many search, classification, and analysis applications. Unfortunately, the problem of maximum common subgraph isomorphism between two graphs is a well known NP-hard problem [24], making it impractical to search for exact or maximal graph alignments. As a result, while there are some attempts to improve the performance of exact maximum common subgraph matching solutions [23], most of the recent efforts in the area have focused on seeking approximate/inexact graph alignments [3, 18, 22, 19, 29]. While these algorithms differ in their specific techniques, most of them rely on a four phase process: 1. First, the matching algorithm analyzes and encodes the global structural information (for example a spectral signature [23]) corresponding to the nodes of the graph. 2. Secondly, the algorithm analyzes and encodes the local structural information (such as neighborhood degree distribution [29]) for the nodes of the graph. 3. Once these global and local signatures are encoded, the matching algorithm compares the signatures of pairs of nodes across the given graphs to rank these pairs of nodes (for example using a stable matching algorithm, like the Hungarian algorithm [16]) based on their overall structural similarities.

1.

INTRODUCTION

Graphs have been used to represent a large variety of complex data, from multimedia objects, social networks, hypertext/Web, knowledge graphs (RDF), mobility graphs,to protein interactions. Let D be a set of entities of interest, a graph, G(V, E ), defined over V = D describes the relationships between pairs of objects in D. The elements in the set V are referred to as the nodes or vertices of the graph. The elements of the set E are referred to as the edges and
1 This work is supported by NSF Grants #1339835 and #1318788. This work is also supported in part by NSF grant #0856090.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. CIKM'15, October 19­23, 2015, Melbourne, Australia. c 2015 ACM. ISBN 978-1-4503-3794-6/15/10 ...$15.00. DOI: http://dx.doi.org/10.1145/2806416.2806577.

1101

1.2

Organization of this Paper



Figure 2: Keynode selection problem for scalable graph matching: the nodes marked with "*" are keynodes of the input graphs that can be used to reduce the amount of time needed to search for alignments 4. Finally, a post-processing, or refinement, step (involving, for example, a vertex cover operation) is used to improve the accuracy of the initial matching [29]. Unfortunately, many of these steps result in significant scalability challenges in terms of the matching time needed to compare the pairs of nodes: · In particular, the expensive refinement phase of graph matching algorithms is not practical in applications where scalability of the graph matching operation is critical. · Moreover, especially in very large graphs, it is also impractical to seek pairwise structural similarities for all node pairs during the graph matching process.

The paper is organized as follows: in the next section, we first introduce basic concepts and review existing graph matching algorithms. In Section 3, we provide overviews of the general graph matching process as well as the proposed keynode-driven scalable graph matching (KSGM) algorithm. Then, in Section 4, we present our structural keynode extraction (SKE) algorithm. In Sections 5 and 6, we discuss how to use these structural keynodes for obtaining graph alignments. We discuss the complexity of the proposed algorithms and parallelization opportunities in Section 7. We present experimental evaluations with various real and synthetic data sets in Section 8. These confirm that the proposed approximate graph matching algorithm is highly effective and efficient. Finally, we conclude the paper in Section 9.

2.

BACKGROUND AND RELATED WORK

1.1

Contributions of this Paper

Based on these observations, in this paper, we argue that a more practical and scalable solution would be to seek structural keynodes of the input graphs that can be used to reduce the amount of time needed to search for alignments (Figure 2). Of course, these keynodes must be selected carefully to prevent any degradations in accuracy during the alignment process, especially because, as mentioned above, refinement post-processes are detrimental to scalability of matching algorithms. Given this motivation, in this paper, we first present a highly efficient and effective structural keynode extraction (SKE) algorithm. The SKE algorithm, which is executed off-line, relies on a 3-step process: 1. In the first step, a PageRank algorithm [7] is ran to associate a structural score to each node in the graph. 2. In the second step, a scale-space (based on a difference-ofGaussians (DoG) function defined over different scales of the graph) is constructed. 3. In the third step, keynode candidates are extracted by analyzing the resulting scale-space for extrema of the DoG function and a subset of these candidates are selected as structural keynodes. We then propose a graph matching algorithm that uses these structural keynodes (obtained during off-line processing) for keynodedriven scalable graph matching (KSGM). In particular, KSGM extracts only local signatures and relies on the structural keynodes for fast node-to-node similarity searching. In addition, we also show that this keynode-driven approach not only reduces the number of comparisons that need to be performed online, but it also enables effective matching, even without having to rely on an expensive assignment algorithm, like the Hungarian algorithm (with O(|V |3 ) complexity). Experiment results show that the proposed structural keynode extraction and keynode-driven scalable graph matching algorithms produce alignments that are as accurate as (or better than) the state-of-the-art algorithms, while requiring significantly less online execution time without refinement.

In this section, we review key concepts related to the graph matching problem and discuss the existing algorithms. Graph Isomorphism: Given two graphs G and H , G is isomorphic to H if there exists a bijective mapping from the nodes of G to the nodes H that preserves the edge structure [13]: for any two vertices that are adjacent on G, the vertices they are mapped to are also adjacent on H , and vice versa. Subgraph Isomorphism: Subgraph isomorphism seeks a bijective function, f , such that there is a subgraph G of G and a subgraph H of H , such that G is isomorphic to H , with respect to f . Maximum Common Subgraph Isomorphism: Maximum common subgraph isomorphism seeks the largest subgraph of G isomorphic to a subgraph of H [24]. Intuitively, the larger the maximum common subgraph of two graphs is, the more similar the graphs are to each other. One of the first exact graph matching algorithms was proposed by Ullman [24]. An alternative way to search for a matching between two graphs is to rely on graph edit distance algorithms: given two graphs the corresponding graph edit distance is the least cost sequence of edit operations that transforms G1 into G2 . Commonly used graph edit operations include substitution, deletion, and insertion of graph nodes and edges. Unfortunately, the graph edit distance problem is also known to be NP-complete [24]. In fact, even approximating graph-edit distance is very costly; the edit distance problem is known to be APX-hard [8]. [8] shows that graph isomorphism, subgraph isomorphism, and maximum common subgraph problem are special instances of the graph edit distance computation problem. Many subgraph isomorphism search algorithms have been developed, such as [15, 29, 14]. Approximate Graph Matching: In order to be applicable to large graphs, many heuristic and approximate graph matching algorithms have been proposed. While, as we discussed above, graph matching through edit distance computation is an expensive task, there are various heuristics that have been developed to perform this operation more efficiently. GraphGrep [14] is one such technique, relying on a path-based representation of graphs. GraphGrep takes an undirected, nodelabeled graph and for each node in the graph, it finds all paths that start at this node and have length up to a given, small upper bound, lp . Given a path in the graph, the corresponding id-path is the list of the ids of the nodes on the path. The corresponding label-path is the list of the labels of the nodes on the path. The fingerprint of the graph, then, is a hash table, where each row contains the hash of the label-path and the corresponding number of id-paths in the graph. Irrelevant graphs are filtered out by comparing the numbers of idpaths for each matching hash key and by discarding those graphs which have at least one value in its fingerprint less than the corresponding value in the fingerprint of the query. Matching sub-graphs are found by focusing on the parts of the graph which correspond to the label-paths in the query. After, the relevant id-path sets are

1102

selected and overlapping id-paths are found and concatenated to build matching sub-graphs. A common method to obtain an approximate graph matching is to use the eigenvectors derived from the adjacency matrix of the graph [23]: intuitively, two similar graphs should have similar eigenvectors; moreover, if we construct a |V | × |V | matrix (for example the Laplacian of the graph or a matrix encoding node distances) and decompose it into three matrices of |V | × c, c × c, and c × |V | elements using an eigen-decomposition technique like SVD, the c-length vector corresponding the node v  V can be used as a global-signature corresponding to node v . Once node-tonode similarities are computed, an assignment is usually found using an assignment algorithm, such as the Hungarian algorithm [16], which uses a primal-dual strategy to solve this problem in O(|V |3 ) time. This simple observation, led to several works leveraging different global-signatures for identifying node matches across different graphs [3, 18, 22, 19, 29]. [28] formulates the labeled weighted graph matching problem in the form of a convex-concave program, which searches for appropriate permutation matrices by solving a least-square problem. In addition, feature selection techniques are used for more accurate calculation [11, 12, 20]. In order to improve matching accuracy, [29] proposes to enrich the global-signatures associated to the graph nodes with local-signatures, encoding the properties of the immediate neighborhood of each node.

Algorithm 1 Overview of keynodes based graph matching Input: A set G = {G1 , G2 , ...Gg } of graphs A query graph Gq  G . Output: Rank Gi  G in terms of matching quality Offline process: 1: for all Gi  G (including Gq ) do 2: Perform structural keynode extraction (SKE) for Gi 3: Extract local-signatures for all nodes in Gi 4: (Optional) Extract global-signatures for all nodes in Gi 5: end for Online process: 6: for all Gi  G do 7: Compute local similarities for keynode pairs from Gi and Gq . 8: (Optional) Compute global similarities for keynode pairs from Gi and Gq and combine these with local similarities. 9: Select anchors to obtain a base matching 10: Expand the base matching to obtain Mq,i 11: Compute matching quality, quality (Mq,i ) 12: end for 13: Rank Gi  G in terms of quality (Mq,i )

3.

OVERVIEW OF KEYNODE-DRIVEN GRAPH MATCHING

Given a set G = {G1 , G2 , ..., Gg } of graphs and a query graph Gq  G , in this paper, we seek the maximum graph matching between Gq and all Gi  G (i = q ). Note that the exact solution for this problem is NP-hard [24]. Since we treat scalability as a key constraint, we consider inexact solutions and rely on the matching quality measure proposed in [29] to evaluate the accuracies of the resulting alignments: Let Gq (Vq , Eq ) be a query graph and let Gi (Vi , Ei ) be a graph in G . Let Mq,i (Vq,i , Eq,i ) be a subgraph of both Gq and Gi , returned by an inexact subgraph search algorithm. [29] defines the matching quality function as follows: quality (Mq,i ) = |Eq,i | , min(|Eq |, |Ei |)

Intuitively, the quality function describes how similar the given query graph Gq and known graph Gi are by using the ratio of matched edges and the maximum number of edges that can be possibly matched, which is equal to the minimum number of edges between two graphs. In other words, the larger the number of edges in the graph Mq,i , the better is the quality of the matching (or the more similar the two graphs are).

combines (by multiplying) the global and local similarities of each pair of nodes into a single value, thereby quantifying the overall similarity of the pair. 4. Once the overall similarities for |Vq | × |Vi | pairs of nodes are computed, [29] drops node pairs with small degrees and, then, expands the remaining set of anchor pairs by adding, in an iterative manner, immediate good nearby pairs to this anchor set. 5. When no more pairs can be added to the anchor set, [29] uses the Hungarian algorithm to identify an initial node matching in O(max{|Vq |, |Vi |}3 ) time. 6. Finally, as a post-processing step, [29] applies a vertex cover based refinement, which explores different subsets of the nodes and searches for better alignments than the one initially identified. In particular, the algorithm seeks small vertex covers, which are likely to give the mismatched nodes additional chances to be refined. Note that since the minimum vertex cover problem is known to be NP-hard, the algorithm searches for minimal vertex covers in O(m×n3 ) time, where m = min{|Eq |, |Ei |} and n = max{|Vq |, |Vi |}. This process includes a number of very expensive steps: The first two steps, involving global and local analysis are expensive, but can be performed off-line and indexed for later reuse assuming that the graphs are available ahead of time. The last four steps, however, need to be performed on-line, yet they consist of operations that are quadratic or higher. In particular, the last refinement step, with O(m × n3 ) time cost is impractical for most large data graphs. In this paper, we note that Step 3 can be significantly sped up if the similarity computations are limited to only a small subset of the vertices in Vq and Vi (which we refer to as keynodes of Vq and Vi ). However, the use of keynodes for node similarity computation is not sufficient to reduce the overall complexity as, once the keynodes are identified and the keynode pairs set is expanded, solving the assignment problem needed to return the matching would still take O(max{|Vq |, |Vi |}3 ) time, if we were to apply the Hungarian algorithm on the extracted keynodes. Therefore, we also need to reduce the time complexity of this step significantly. It is especially important that the initial keynode based similarity computation is accurate as we cannot afford a cubic algorithm like Hungarian algorithm to return a high-quality matching.

3.1

Challenges

Given a query graph Gq , our goal is to rank the graphs in G according to their matching similarities against Gq (and eventually return the top few matches to the user). [29] solves this problem by relying on a 6-step process, common to many graph search algorithms: 1. [29], first, analyzes the global structure of each graph through eigen-decomposition of the graph Laplacian matrix and encodes this in the form of a c-length vector associated to each node in the graph. 2. Secondly, [29] encodes the structural information local to each node, vj , in the form of an sj -length degree distribution vector, where sj is the number of nodes in the kneighborhood of the node. 3. Given the global and local signatures of all nodes in Gq and Gi , [29] then computes the global and local similarities for each pair of nodes from the two graphs, in O(|Vq | × |Vi | × c) and O(|Vq | × |Vi | × maxvj (sj )) time, respectively. It then

1103

Outline of KSGM Algorithm 1 illustrates an overview of the keynode-driven scalable graph matching (KSGM) process. In the rest of the paper, we study each step in detail. First, in the next two sections, we focus on the offline steps of KSGM, which involve identifying keynodes and extracting local-signatures. The online steps of the KSGM algorithm are discussed in Section 6. 4. STRUCTURAL KEYNODE EXTRACTION

3.2

4.2

Proposed Solution - Robust Keynode Extraction through Scale Space Analysis

In this section, we propose an off-line structural keynode extraction (SKE) algorithm which identifies % (where  is a user provided parameter) of the nodes in V as the keynode set, K , of a given graph, G(V, E ) to support scalable graph matching. The proposed SKE algorithm has a number of advantages: (a) First of all, the identified keynodes are robust against noise, such as random edge insertion/removal; and (b) the identified nodes represent structural features of the graph of different sizes and complexities (i.e., correspond to neighborhoods of different sizes).

4.1

Naive Solution - Selecting Structural Keynodes based on Node Significance

As described above, the keynodes of the graph need to represent the structural properties of the graph well (i.e., extracted keynodes need to be structurally significant in the graph) to support effective matching. Therefore, the first alternative is to rely on traditional node significance measures. Measures like betweenness [26] and the centrality/cohesion [5], help quantify how significant any node is on a given graph based on the underlying graph topology. The betweenness measure [26], for example, quantifies the number of shortest paths that pass through a given node. The centrality/cohesion [5] measures quantify how close to a clique the given node and its neighbors are. Other authority, prestige, and prominence measures [4, 7, 5] quantify the significance of the node in the graph through eigen-analysis or random walks, which help measure how reachable a node is in the graph. PageRank [7] is one of the most widely-used random-walk based methods for measuring node significance and has been used in a variety of application domains, including web search, biology, and social networks. The basic thesis of PageRank is that a node is important if it is pointed to by other important nodes ­ it takes into account the connectivity of nodes in the graph by defining the score of the node vi  V as the amount of time spent on vi in a sufficiently long random walk on the graph. Given a graph G(V, E ), the PageRank scores are represented as r, where r = TG r + (1 - )t where TG is a transition matrix corresponding to the graph G, t is 1 a teleportation vector (such that t[i] = |V ), and  if the residual | probability (or equivalently, (1 - ) is the so-called teleportation probability). Unless the graph is weighted, the transition matrix, TG , is constructed such that for a node v with k (outgoing) neighbors, the transition probability from v to each of its (outgoing) neighbors will be 1/k. If the graph is weighted, then the transition probabilities are adjusted in a way to account for the relative weights of the edges. Therefore, as the first alternative, we consider a PageRank based keynode selection scheme: in this scheme, given a graph G(V, E ), we would (a) first identify the PageRank scores, p(vi ), of all vi  V , then (b) we would rank the nodes in non-increasing order of PageRank scores, and finally, (c) we would return the top % of the nodes in V as the keynode set, K .

We note that the above alternative has a number of disadvantages: · First of all, many of the structural significance measures, such as PageRank, are not entirely robust against modifications in the graph. The PageRank score of a node, for example, can jump significantly, if a new edge connects the sub-graph in which the node is contained to a high PageRank node in the graph. · Secondly, common structural significance measures, like PageRank, capture the significance of a node in the whole graph and favor nodes that are overall central. However, this may be disadvantageous as there is a possibility that smaller scale, but distinct (and, therefore, useful for matching) structural features of the graph may be missed. We therefore argue that we need a better alternative, which is both robust and multi-scale. We build the proposed SKE algorithm based on three key insights: · Robustness: Even when the PageRank scores of the nodes themselves vary due to graph transformations, such as edge insertions and removals, a given node's PageRank score relative to the scores of the nodes in its neighborhood is likely to be stable. · Structural distinctiveness: A node is structurally distinctive in its neighborhood, if "the relationship between its PageRank score to the PageRank scores of its neighbors" is different from the "relationships between the node's neighbors' PageRank scores and the PageRank scores of their own neighbors". · Multi-scale: Since we do not know the scale of the structurally distinctive features of the graph, we need to search for features of potentially different sizes. It is important to note that similar requirements also exist in other application domains. For example, algorithms for extracting such robust, local features have been developed for 2D images (SIFT [21]), uni-variate time series [9], and multi-variate time series [25]. In this paper, we argue that a similar process can be used to identify keynodes (corresponding to robust, multi-scale structural features) of a graph, if the nodes are annotated with PageRank scores ahead of the time. Let G(V, E, p) be a PageRank-labeled graph, where p() is a mapping from the nodes to the corresponding PageRank scores. What makes the problem of extracting local features from PageRank-labeled graphs challenging is that the concepts of neighborhood, gradient, and smoothing are not welldefined for graphs. Therefore, before we describe the keynode extraction process, we describe how to smooth a PageRank-labeled graph. Intuitively, smoothing the graph with respect to the scores associated to the graph nodes creates versions of the given graph at different resolutions and, thus, helps identify features with different amounts of details.

4.2.1

Gaussian Smoothing of a PageRank-Labeled Graph

1D or 2D data are commonly smoothed by applying a convolution operation with a Gaussian window. For example, if Y = t0 , t1 , . . . , tl is a time series data and  is a smoothing parameter, ~ (t,  ), is obtained through G (t,  )  Y (t) its smoothed version, Y where  is the convolution operation in t and G (t,  ) is the Gaussian function -t2 1 G (t,  ) =  e 22 . 2 Essentially, the Gaussian smoothing process takes a weighted average of values of the points in the vicinity of a given point, t.

1104

The closer a point to t, the higher is the weight. Therefore, in order to implement a similar Gaussian smoothing of the given graph, we first need to define a distance function to measure how close different nodes are to each other. Common applicable definitions of node distance include the hop distance (determined by the shortest edge distance between the nodes on the given graph) or hitting distance [10]. In this paper, we use hop distance to measure how far nodes are to each other: D EFINITION 1 (N ODE D ISTANCE M ATRIX ). Let us be given a graph G(V, E ) with n nodes. The ordering among the nodes is described through a set of node distance matrices, Nj , where · Nj , for j  0, is an n × n 0, 1-valued matrix, where for a given node vi in the graph, G, the ith row in the matrix, Nj is 1 only for nodes that have node distance exactly j from the node vi , and · Nj , for j  0, is an n × n 0, 1-valued matrix, where for a given node vi , the ith column in the matrix, N(j, G) = 1 only for nodes that have distance exactly j on the inverted graph, where all edges are inverted. Intuitively, the cell Nj [v1 , v2 ] = 1 if the node v2 is exactly j hops from v1 . When j is positive the hop-distance is measured following outgoing edges, whereas when j is negative, incoming edges are followed. Given this, we construct multiple scales of the given graph G by using a Gaussian graph smoothing function defined as follows. D EFINITION 2 (G AUSSIAN G RAPH S MOOTHING F UNCTION ). Let us be given a labeled graph G(V, E, x) and let ·  be a smoothing parameter. · X = x(v1 ), x(v2 ), ..., x(vn ) be a vector encoding the labels associated with the nodes, vi  V . Then, if G is a directed graph, the non-normalized Gaussian graph smoothing function, SG, () is defined as
n




    

 



 

        


 

 

   



 

 




    

 


 

 




    

 

Figure 3: Computing the Difference-of-Gaussian (DoG) series of the PageRank values of a graph Intuitively, the smoothing function S applies Gaussian smoothing on the X values (associated with the nodes, vi  V ) based on the hop-distances between nodes and returns a vector SG, (X ) = x ~(v1 ), x ~(v2 ), . . . , x ~(vn ) encoding the smoothed X values associated with the graph nodes. Note that, since at a given hop distance there may be more than one node, all the nodes at the same distance have the same degree of contribution and the degree of contribution gets progressively smaller as we get further away from the node for which the smoothing is performed. Therefore, given a PageRank-labeled graph, G(V, E, p), and a corresponding PageRank vector, P = p(v1 ), p(v2 ), ..., p(vn ) , encoding PageRank scores associated with the nodes, vi  V , the vector SG, (P ) = p ~(v1 ), p ~(v2 ), . . . , p ~(vn ) , encodes the  -smoothing of the PageRank-annotated graph, G(V, E, p). We also say that SG, (P ) encodes the PageRank scores of G at scale  . We next describe how to construct a scale-space for the given graph through an iterative smoothing process leveraging the PageRank vector and the structure of the graph.

SG, (X ) = G (0,  )IX +
j =1 n

G (j,  )Nj X G (j,  )Nj X,
j =1

+

where G (0,  ) is a Gaussian function with zero mean and  standard deviation. If, on the other hand, G is an undirected graph, then the non-normalized Gaussian graph smoothing function is
n

4.2.2

Graph Scale-Space Construction

SG, (X ) = G (0,  )IX +
j =1

2G (j,  )Nj X.

Intuitively, S applies Gaussian-based weighted averaging to the entries of vector X based on the hop-distances2 . However, unlike the basic Gaussian smoothing, during (non-normalized) relationship smoothing, there may be more than one node at the same distance and all such nodes have the same degree of contribution. As a consequence, the sum of all contributions may exceed 1.0. Therefore, the normalized Gaussian graph smoothing function, S (G,  ), discounts weights based on the number of nodes at a given distance: SG, (X ) = SG, X ÷ SG, 1(n) , where 1(n) is an n-vector such that all values are 1 and "÷" is a pairwise division operation.
2 In practice, since the Gaussian function drops fast as we move away from the mean, we need to consider only a small window, w, of hops

The first step in identifying robust graph features is to generate a scale-space representing versions of the given graph with different amounts of details. In particular, building on the observation that features are often located where the differences between neighboring regions (also in different scales) are large, we seek structural features of the given graph at the extrema of the scale space defined by the difference-of-the-Gaussian (DoG) series. More specifically, given · a PageRank-labeled graph, G(V, E, p), · the corresponding vector, P = p(v1 ), p(v2 ), ..., p(vn ) encoding the scores associated with the nodes, vi  V , · a minimum smoothing scale, min , · a maximum smoothing scale, max , · the number, l, of levels of the scale space, then, we compute a difference-of-Gaussians (DoG) series, D(G, P, min , max , l) = {D1 , D2 , ..., Dl }, where each Di encodes the differences of two nearby scales separated by a multiplicative factor k: Di = SG,ki min (P ) - SG,ki-1 min (P ),

1105



    

  

· DoG-neighbors numbered #2 and #7 correspond to the DoG values of the same node at the previous and next levels of the scale space. Therefore, we have N
vi ,j



[2] = Di-1 [j ] and N

vi ,j

[7] = Di+1 [j ].





  



· In contrast, DoG-neighbors #3, #5, and #8 correspond to the (average) DoG values of the forward neighbors of the node vj , at the previous, current, and next levels of the scale space, respectively. Therefore, we have N N
vi ,j vi ,j

Figure 4: Extrema detection where k =
l

[3] = (FDi-1 ) [j ],
vi ,j

[5] = (FDi ) [j ], N

[8] = (FDi+1 ) [j ],

max . min

Figure 3 visualizes the process:

· On the left hand side of the figure, we have the incrementally smoothed versions of the PageRank vector, P . Here, the lowest level, SG,min (P ), corresponds to the most detailed version of the graph (with the least amount of smoothing), whereas SG,max (P ) corresponds to the least detailed (most smoothed) version of the graph. In other words, min determines the sizes of the smallest structural features we can locate and max = kl min determines the sizes of the largest structural features we can identify. In particular, since under Gaussian smoothing, a diameter of 6 would cover  99.73% of the weights, the diameter of the smallest structural feature that can be identified using SKE is  6min hops, whereas the diameter of the largest feature would be  6max hops. The number of levels, l, denotes the number of detail levels (or scales) we explore between min and max . Intuitively, each of these levels corresponds to a different target size for the structural features of the graph. · On the right hand side of the figure, we have the resulting Difference-of-Gaussian (DoG) series, consisting of vectors, D1 through Dl . Note that, intuitively, Di [j ] measures how different the PageRank values of the neighborhood around vj at scale i-1 (= ki-1 min ) are from the PageRank values of the neighborhood around vj at scale i (= ki min ). Therefore, a large Di [j ] value would indicate a major structural change when neighborhoods of different size around vj are considered (e.g., a node with a high PageRank score is included when considering a neighborhood of larger scale). In contrast, a small Di [j ] indicates that there is minimal structural change when considering neighborhoods of different scales.

where, F is a row-normalized adjacency matrix accumulating the (averaged) contributions of the nodes to their neighbors along the forward edges. · Similarly, DoG-neighbors #1, #4, and #6 correspond to the (average) DoG values of the backward neighbors at the previous, current, and next levels of the scale space. Therefore, we have N vi ,j [1] = (BDi-1 ) [j ], N
vi ,j

[4] = (BDi ) [j ], N

vi ,j

[6] = (BDi+1 ) [j ],

where, B is a row-normalized backward-adjacency matrix (where all edges are reversed) accumulating the (averaged) contributions of the nodes to their neighbors along the backward direction of the edges. Given these, the pair vj , i is an extremum (i.e., vj is a keynode candidate at scale i ), iff Di [j ] is a local maximum Di [j ]  M AX N
1h8 vj ,i

[h]

or it is a local minimum Di [j ]  M IN N
1h8 vj ,i

[h].

Intuitively, since Di [j ] measures how different the PageRank values of the neighborhood around vj at scale i are from the PageRank values of the neighborhood around vj at scale i-1 , a local maximum corresponds to a highly scale-sensitive region (amidst relatively scale-insensitive regions), whereas a local minimum corresponds to a scale-insentive region (amidst more scale-sensitive regions), of the graph.

4.2.4

Selecting the Best Keynodes

4.2.3

Identifying Keynode Candidates

As we mentioned earlier, our intuition is that a graph node is structurally distinctive in its neighborhood, if "the relationship between its PageRank score to the PageRank scores of its neighbors" is different from the "relationships between the node's neighbors' PageRank scores and the PageRank scores of their own neighbors, at multiple scales". Therefore, to locate the keynode candidates, we focus on the local extrema of the difference-of-Gaussian (DoG) series D. More specifically, we identify vj , i pairs where the DoG value for node vj at scale, i = ki min , is an extremium (maximum and/or minimum) with respect to the neighbors of vj in the same scale as well as neighbors in the previous and next levels of the scale space. In order to verify if the pair vj , i is an extremium or not, we compare Di [j ] with the values corresponding to eight DoGneighbors in the scale-space, as visualized in Figure 4:

In the final step, we need to rank the keynode candidates and  return the top 100 × |V | of them, where  is a user provided parameter, as the keynode set, K . We propose Extremum Ranking to select the best keynodes. Since keynodes are located at the local extrema of the DoG series, we can rank the keynode candidates based on their extremum score defined as follows: Let the pair vj , i be a local extremum. The corresponding extremum score,  ( vj , i ), is defined as   D [j ] - M AX N    i 1h8    
1h8 vj ,i

[h]

if vj , i is max.

M IN N

vj ,i

[h] - Di [j ]

if vj , i is min.

Intuitively, the higher the extremum score is, the better local extremum (and, thus, a better keynode) is vj , i .

1106

5.

LOCAL NODE SIGNATURES

The next step in the process is to extract the local signatures (to be used to compute local node similarities) for the nodes in the graph. Note that this process is also offline. While there are different local signatures proposed in the literature, in our work we build on the k-neighborhood degree distribution based local signature proposed in [29] (both because it is simple and effective and also because this helps us compare our keynode-driven approach to the approach proposed in [29] more directly). Briefly, for each node vj  V and for a user provided k, [29] first identifies the set, Nk (vj )  V , of nodes that are at most k hops from vj and extracts a subgraph, Gk (vj )  G, induced by vj and its k-hop neighbors. Then the degree sequence, j = [dj,1 , dj,2 , . . . , dj,|Nk (vj )| ] consisting of the degrees of nodes in Gk (vj ) (excluding vj ), sorted in non-increasing order, along with the degree of the node vj and the numbers of vertices and edges in its k-hop neighborhood, form the local signature of node vj : local_signature(vj ) = j , degree(vj ), j , j , where j = |Nk (vj )  {vj }| is the number of nodes in Gk (vj ) and j = |Ek (vj )| is the number of edges. Note that, while we use a local signature similar to that proposed in [29], we extend the node pair ranking function to better account for the node degrees as discussed later in Section 6.1.1. As we see in Section 8, this extension provides a significant boost in accuracy.

nbhd_sim(vi , vj ) proposed by [29] accounts for the alignment between the degree distributions in these neighborhood graphs3 : nbhd_sim(vi , vj ) = where dmin = min{degree(vi ), degree(vj )} nmin = min{i , j } D(vi , vj ) = dmin +
nmin -1 h=1

nmin + D(vi , vj ) , (i + i )(j + j )

min{di,h , dj,h }

2

.

Node Pair Ranking with Extended Similarity.
While the local neighborhood similarity computation we use is similar to the one proposed in [29], we rank pairs of nodes differently. Let vi and vj be two nodes (from two different graphs). In particular, [29] ranks the pair vi , vj of nodes based on their neighborhood similarities, nbhd_sim(vi , vj ). We, however, argue that neighborhood similarity is not sufficient for accounting for how effective the node pair is in supporting expansion. More specifically, we observe that a pair, vi , vj , is likely to be a better anchor for expansion than the pair va , vb if not only (a) the neighborhoods of vi and vj are more similar to each other than the pair, va and vb , but also (b) if vi and vj have degrees that are more aligned with each other than va and vb . Based on this observation, instead of applying a degree threshold, we propose that the pair vi , vj should be ranked based on the ranking function (vi , vj ) = nbhd_sim(vi , vj )× min{degree(vi ), degree(vj )} . max{degree(vi ), degree(vj )}

6.

(KEYNODE-BASED) GRAPH MATCHING

As discussed in Section 3 and outlined in Algorithm 1, once the keynodes are extracted and local signatures are computed offline, the next steps of the algorithm are to · compare the signatures of pairs of nodes across the given graphs to rank these pairs of nodes, · select a set of pairs of keynodes (we refer it as anchor set) that serve as the base matching, and · expand this base matching to obtain Mq,i . We now describe how these steps are implemented in the keynodedriven scalable graph matching (KSGM) algorithm.

Note that [29] simply drops node pairs where the minumum of the two node degrees is smaller than the larger average degree of the two input graphs. We, however, argue that such node pairs may be useful, especially if the degrees in the graph are not uniformly distributed and the maximum matching occurs at the sparse portions of the graph. Therefore, we keep such pairs as long as they rank highly based on (). We evaluate this ranking function in Section 8.

6.1.2

Keynode pair Selection

6.1

Anchor Set Selection

Let G1 (V1 , E1 ) and G2 (V2 , E2 ) be two graphs and let K1  V1 and K2  V2 be the corresponding keynodes identified by the SKE algorithm proposed in Section 4. The next step is to select a subset, A, of the pairs of nodes in K1 × K2 as the anchor set of alignments based on a ranking function (a) evaluating how structurally similar a pair of nodes are and (b) how likely they are to lead to an effective expansion process to discover other alignments.

6.1.1

Node Similarity Matching and Node Pair Ranking

As we discussed in Section 5, KSGM uses a local node signature similar to the one proposed by [29]: j , degree(vj ), j , j , where j is the number of nodes in the neighborhood of vj , j is the number of neighborhood edges, and j = [dj,1 , dj,2 , . . . , dj,|Nk (vj )| ] consists of the degrees of nodes in the k-neighborhood of vj (excluding vj ), sorted in non-increasing order.

[29] uses the Hungarian algorithm to identify an initial node matching in O(n3 ) time, where n = max{|V1 |, |V2 |}. To reduce the execution time, [29] prunes those node pairs for which the similarity is  0.5. Since, instead of considering the node pairs in V1 × V2 , we only need to consider pairs of nodes in K1 × K2 , and since |K1 | |V1 | and |K2 | |V2 |, keynode-driven processing is likely to be faster even without using the threshold. However, the cubic time of the Hungarian algorithm is still prohibitive and impractical for scalable graph matching. Therefore, we propose a greedy anchor selection algorithm, which (as we see in Section 8) performs very well when used along with keynodes selected in Section 4 and the proposed ranking function, (). In particular, we first include all keynode pairs in K1 × K2 into a queue in the order of their ranks based on (), then, until the queue is empty, we remove and consider the keynode pair, v, u at the head of the queue. If neither v nor u has been marked anchored, we include v, u as an anchor and we mark v and u as anchored, otherwise, we drop the pair, v, u . Note that this process has O((|K1 | × |K2 |) × log (|K1 | × |K2 |)) time cost (instead of the cubic cost of the Hungarian algorithm) and,
3 In addition to using local similarities, [29] also extracts global signatures along with the local-signatures to compute node similarities. As we see in Section 8, the proposed keynode-driven graph matching algorithm achieves good results without having to rely on such global-signatures.

Local Neighborhood Similarity.
Let vi and vj be two nodes (from two different graphs) and let Gk (vi ) and Gk (vj ) be the corresponding induced k-neighborhood graphs. Then, local similarity function

1107

as we see in Section 8, performs very well in practice. Furthermore, the nature of Hungarian algorithm, which forces to pair all possible nodes to produce the optimal bipartite matching for the given two sets of nodes, is not guaranteed to provide a better matching in this case. Since the extracted keynodes are not all necessarily perfectly paired with each other, some keynodes can be a unique feature of the given graph, which does not align with other graphs, by forcing them to pair with other keynodes, it in fact introduces a bad initial base matching, and thus expand into an even worse matching. The proposed greedy matching algorithm, however, only consider the highly aligned keynodes, which in practice provides better results than the optimal bipartite matching.

6.2

Matching List Expansion

Because keynodes are inherently sparsely localized, the anchor set, A, is not necessarily a good final matching for graphs G1 and G2 . We therefore need to expand this anchor list. Here, we follow [29]'s recommendation and expand the list incrementally by considering the neighbors (and their neighbors) until no effective expansion is possible (but we use the ranking function () instead of the node similarity function): 1. we first include all node pairs in A into a ranked queue (i.e., max-heap) in the order of their ranks based on the ranking function, (), 2. then, for each node pair v, u  A, we also include the node pairs in neighbors(u) × neighbors(v ) in the same ranked queue 3. then, until the ranked queue is empty, we remove and consider the node pair, v, u at the head of the ranked queue (a) if either v or u has not yet been marked matched, then i. we include the pair, v, u , in the expanded matching list, L, ii. we mark both v and u as matched, and iii. then, the pairs in neighbors(u) × neighbors(v ) are included in the ranked queue (b) otherwise, we drop the pair, v, u Once the anchor list is expanded, [29] relies on a post-process, with time complexity, O(m × n3 ), where m = min{|E1 |, |E2 |} and n = max{|V1 |, |V2 |}. This step is not scalable due to its prohibitive time complexity. Therefore, the proposed keynode-driven scalable graph matching (KSGM) algorithm omits this refinement post-process, due to its high time complexity4 . Instead, the set, L, of node pairs remaining after the expansion process is directly returned as the aligned nodes of the matching, M1,2 , for the input graphs, G1 and G2 .

more efficient randomized algorithms exist [27]. Once the node distances have been computed, we construct the scale-space in O(l × |V | × max_w_nbhd_size), as for each of the l scales, the score of each node needs to be smoothed considering the scores of the vertices in its w-hop neighborhood (w is the Gaussian window size and max_w_nbhd_size is the size of the largest w-hop neighborhood in G). Once the scale-space is constructed, next, we identify the keynode candidates. This involves O(l × |V |) time, because for each of the l scales, each node needs to be compared with a constant number (8) of DoG-neighbors in the scale-space. Finally, we rank the keynode candidates to select the top K =  V many as the keynodes to bootstrap the online matching pro100 cess. Let there be C many keynode candidates. Computing the ranking scores for these takes O(C ) time, because each keynode candidate needs to be compared with a constant number of DoGneighbors and obtaining the top K takes O(C × log (K )) time.

7.1.2

Local Signature Extraction

Since the local signature extraction process needs to extract the k-hop neighborhoods around the nodes, the complexity of this step is O(|V | × max_k_nbhd_size), where max_k_nbhd_size is the size of the largest k-hop neighborhood in G. Note that this step can also leverage the node distance matrix constructed during the offline keynode extraction process.

7.2
7.2.1

Online Time Complexity
Local Similarity Computation for Keynodes

Let G1 (V1 , E1 ) and G2 (V2 , E2 ) be two graphs. The online process includes the following operations. This process has O(|K1 | × |K2 | × compare_length) complexity, where compare_length = min{max_k_nbhd_size1 , max_k_nbhd_size2 }) since signatures (of length are compared for each pair of nodes in the keynode sets K1 and K2 .

7.2.2

Anchor Set Selection

This greedy process has O((|K1 | × |K2 |) × log (|K1 | × |K2 |)) time cost as each pair off nodes among the keynode sets need to be considered only once in ranked order.

7.2.3

Anchor Set Expansion

This has O((|V1 | × |V2 |) × log (|V1 | × |V2 |)) worst case time cost, as in the worst case, all pairs of vertices across the two graphs may need to be considered for expansion in ranked order.

8.

EXPERIMENTS

7. TIME COMPLEXITY ANALYSIS 7.1 Offline Time Complexity
Let G(V, E ) be a graph to be indexed in the database.

In this section, we present experimental evaluations of the proposed keynode-driven scalable graph matching (KSGM) algorithm. In particular, we compare KSGM to the graph matching algorithm presented in [29] in terms of efficiency and accuracy.

8.1
8.1.1

Data Sets
Facebook Data Graph

7.1.1

Structural Keynode Extraction

The first step in structural keynode extraction is to obtain the PageRank scores for the nodes of the two graphs. While, this is an expensive operation (involving a matrix inversion with O(|V |2.373 ) complexity for a graph with |V | nodes), there are many efficient, approximate implementations of PageRank, including sublinear approximations [6]. The second step is the creation of an l-layer scale-space for G. To construct the scale space, we first construct a node distance matrix, which requires an all-pairs shortest path computation, with complexity O(|V |3 ) for a graph with |V | nodes, but Though, in cases where scalability is not critical, this refinement can be implemented without any change in the rest of the algorithm.
4

The first data set we used is the Facebook social circles data graph obtained from the Stanford Large Network Dataset Collection [2]. This is a connected graph with 4039 nodes and 88234 edges. The graph has a diameter of 8 and a 90-percentile effective diameter of 4.7. For the experiments, we constructed 10 subgraphs by uniformly sampling connected subsets, containing 60 - 70% of the original graph nodes. Once the subgraphs are obtained, each of the subgraphs is used as a query against the rest. We report the averages of execution time and accuracy.

8.1.2

Synthetic Graph Data Sets

In addition to the Facebook graph, we also used synthetic graphs, where we controlled the topology, size, and node degree to explore

1108

Table 1: Synthetic graph topologies and configurations
Graph topology Erdos-Renyi (ER) Power law (PL) Number of nodes 5000, 7500 (plus 1 to 10%) 5000, 7500 (plus 1 to 10%) Average degree 4, 8, 16 4, 8, 16

Table 2: Experiment results for the Facebook Graph (default parameters)
Matching time (online, sec.) Accuracy [29] 19.2 35.4% 11.5 KSGM 6.4 38.0% 110.4 PR 7.25 34.12% 0.39 Random 6.0 15.4% -

the advantages and disadvantages of the algorithms under different scenarios. We generated the synthetic graphs using the well known random graph generating tool, NetworkX [1]. We consider two common graph topologies: the Erdos-Renyi (ER) model and the power law topology (PL) under the Barabasi-Albert model. Table 1 lists the number of nodes and average degree settings that we used for assessing our algorithms. For each configuration, we generated 10 graphs. Note that, in addition to the base sizes (of 5000 and 7500), we randomly created an additional 1 to 10% more nodes to ensure that the different graphs in the data set have slightly different numbers of nodes. As before, once the 10 graphs are obtained for each configuration, each of the subgraphs is used as query against the rest. We report the averages of execution time and accuracy.

Extraction time (offline, sec.)

Table 3: Impact of the keynode percentage, , for the Facebook Graph
Matching time (online, sec.) Accuracy 2% 6.5 37.6% 3% 6.4 38.0% 4% 6.3 38.7% 6% 6.4 36.4% 8% 7.2 33.2%

Table 4: Impact of the node-pair ranking function, (), for the Facebook Graph
Matching time (online sec.) Accuracy () 6.4 38.0% w/o Degree 6.0 31.8% with Global 4.0 22.9%

8.2

Evaluation Criteria

Table 5: Impact of the local-signature neighborhood size, k, for the Facebook Graph
Matching time (online, sec.) Accuracy 2 hops (default) 6.4 38.0% 3 hops 5.5 33.9% 4 hops 4.3 23.3%

All experiments were conducted using a 4-core Intel Core i52400, 3.10GHz, machine with 8GB memory, running 64-bit Windows 7 Enterprise. The codes were executed using Matlab 2013b and Visual Studio 2012. To evaluate accuracy, we use the matching quality defined in Section 3.

8.2.1

Execution Time

We report both offline and online execution times. As shown in Algorithm 1 in Section 3, for KSGM, the offline execution includes structural keynode and local-signature extraction steps. Online execution includes similarity computation, anchor selection, and expansion steps. [29] does not perform structural keynode extraction; instead, offline execution includes eigen-decomposition for global signatures. For both KSGM and [29], we omit the refinement step as its complexity is prohibitive for scalable graph matching. For instance, for the Facebook graph for which KSGM takes  6 seconds for online processing for a pair of graphs, refinement takes  30 minutes ­ i.e., it causes a  260× slowdown5 .

8.3

Experiment Parameters

processing, its online matching time is 3× faster than that of [29]. Moreover, the matching accuracy of KSGM is 1.2× better than that of the competitor, through it does not use global signatures, nor it relies on the optimal Hungarian algorithm for anchor selection. The table also lists the performance of KSGM when using top PageRank (PR) scored nodes instead of those returned by the SKE algorithm. As we see here, while the offline process is faster when using PageRank scoring nodes, the runtime performance (both in terms of execution time and accuracy) is worse when using SKE keynodes. In addition, to see whether it is possible to achieve a competitive accuracy if we were to select a similar percentage of node randomly, in Table 2, we also include results where random keynodes are used in the matching online phase. As we can see, the accuracy drops significantly when we use random keynodes instead of using robust structural keynodes extracted by the proposed SKE algorithm6 . These indicate that SKE is indeed effective in extracting structurally distinct and useful keynodes.

The default parameters for the structural keynode extraction (SKE) algorithm are as follows: · PageRank teleportation probability (1 - ) = 0.15 (as is commonly assumed), · least smoothing factor (min ) = 0.275, corresponding to  2-hop neighborhoods, · maximum smoothing factor (max ) = 0.777, corresponding to  5-hop neighborhoods, and · number (l) of smoothing levels = 6. In addition, for KSGM, the default percentage () of keynodes selected from the graph was set to 3%. Also, for all algorithms, local signatures were extracted from 2-hop neighborhoods (i.e., k = 2), as recommended by the authors of [29].

8.4.2

Impact of the Keynode Percentage

Table 3 studies the impact of the percentage, , of the nodes used as keynodes. As we see, up to a point, the more keynodes we use, the more accurate and faster the matching becomes. Beyond that point, however, additional keynodes become disadvantageous. This indicates that top keynodes are the most effective in serving as good starting points and, as expected, below a certain rank they loose distinctiveness, resulting in increased cost and loss in accuracy.

8.4.3

Impact of the Node-Pair Ranking Function

8.4
8.4.1

Results for the Facebook Graph
Default Configuration

Table 2 lists the online and offline processing times and accuracy for the Facebook graph under the default parameter settings. As we see here, while KSGM spends more time in one-time, offline
5 For this data configuration, when using expensive refinement postprocessing, KSGM and [29]'s accuracies are 0.72 and 0.696, respectively.

Table 4 studies the impact of the node-pair ranking function, (). In particular, we compare the performance of the ranking function proposed in Section 6.1.1, to the ranking function without degree extension and ranking function including additional globalsignature similarity as proposed in [29]. As we see here, the proposed node-pair ranking function provides the best expansion opportunities (and thus provides the highest accuracy, with slight expansion time overhead). Also, the "with Global" optional provides a much worse matching. Thus, while the algorithm allows, we encourage the users not to use "with Global" option.
6 The slight time gain when using random keynodes is due to the fact that random keynodes are not good starting points for expansion and, thus, the expansion process ends earlier.

1109

Table 6: Experiment results for the synthetic data sets (avg. degree=4, varying models and number of nodes)
KSGM Online time (sec.) [29] Online time (sec.) KSGM Accuracy [29] Accuracy KSGM Offline time (sec.) [29] Offline time (sec.) PL(5000) 6.9 99.3 45.3% 42.9% 130.0 23.8 PL(7500) 13.9 223.7 45.1% 42.8% 284.7 82.8 ER(5000) 6.5 746.7 45.7% 46.3% 134.8 24.9 ER(7500) 14.5 745.1 51.2% 53.0% 270.8 84.5

gorithm works faster than the state-of-the-art algorithms without refinement, yet produces alignments that are as good or better.

Acknowledgments
We thank the authors of [29] for sharing their source code and data.

10.

REFERENCES

Table 7: Impact of the average node degree (number of nodes=5000, power law model)
KSGM Online time (sec.) [29] Online time (sec.) KSGM Accuracy [29] Accuracy KSGM Offline time (sec.) [29] Offline time (sec.) degree=4 6.9 99.3 45.3% 42.9% 130.0 23.8 degree=8 7.5 116.7 25.2% 25.1% 199.1 27.9 degree=16 9.3 141.2 13.9% 14.1% 396.7 53.2

8.4.4

Impact of the Neighborhood Size, k, for LocalSignatures

Table 5 studies the impact of the neighborhood size, k, for localsignature extraction. As we see in the table, the highest accuracy is at 2 hops7 , increasing the neighborhood size negatively affects the accuracy, indicating that unless locally meaningful signatures are used, the resulting node-pair ranking is not effective for expansion. This shows the keynode matching process is more accurate when keynodes are easy to localize and this requires them to be distinct and locally representative. Large neighborhoods potentially violate both. Note that this is in line with the observation in Table 4.

8.5

Results for the Synthetic Data Sets

In this subsection, we consider the impacts of graph topology, size, and node degree using ER and PL topologies. We omit discussions of the impacts of the other parameters, as they mirror those presented in Tables 3 through 5.

8.5.1

Default Configurations

Table 6 lists the performances of KSGM and [29] for synthetic graphs for different topologies and numbers of nodes under the default parameter settings. As we see here, the online execution time of KSGM is significantly (10× to 115×) faster than that of [29], especially for the ER topology. Moreover, on both Erdos-Renyi (ER) and power law (PL) topologies, the accuracy is highly competitive, with KSGM providing non-negligible accuracy gains for the PL model (where it is relatively easier to identify effective keynodes).

8.5.2

Impact of Average Node Degree

Table 6 studies the impact of average node degree on matching accuracies for the power law graph. As we see in the table, both algorithms see a drop in the matching accuracy with larger node degrees. However, KSGM stays competitive in terms of accuracy, whereas it provides more gains in terms of online execution time.

9.

CONCLUSIONS

Noticing that existing solutions to the graph matching problem face major scalability challenges, we argue that it is impractical to seek alignment among all pairs of nodes. Given these observations, in this paper, we first presented an offline structural keynode extraction (SKE) algorithm and then discussed how to use these structural keynodes in a novel keynode-driven scalable graph matching (KSGM) algorithm. Keynodes are selected carefully especially because a post refinement step is not feasible due to scalability requirements. Experiment results show that the proposed KSGM al7 Coincidentally, this also is the scale at which the SKE algorithm located an overwhelming majority of the keynodes for this graph.

[1] http://networkx.github.io/ [2] http://snap.stanford.edu/index.html [3] X. Bai, H. Yu, and E. R. Hancock. Graph matching using spectrament. ICPR 2004. [4] A. Balmin, et al. ObjectRank: Authority-based keyword search in databases. VLDB, 2004. [5] M.G. Borgatti, et al. Network measures of social capital. Connections 21(2):27-36, 1998. [6] C. Borgs, M. Brautbar, J. T. Chayes, S.-H. Teng. Multiscale Matrix Sampling and Sublinear-Time PageRank Computation. Internet Mathematics 10(1-2): 20-48, 2014. [7] S. Brin, et al. The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems 30: 107-117, 1998. [8] H. Bunke. Error correcting graph matching: On the influence of the underlying cost function. IEEE TPAMI, 21(9):917­922, 1999. [9] K. S. Candan, R. Rossini, M. L. Sapino, X. Wang. sDTW: Computing DTW Distances using Locally Relevant Constraints based on Salient Feature Alignments. PVLDB, 1519-1530, 2012. [10] M. Chen, J. Liu, and X. Tang. Clustering via random walk hitting time on directed graphs. AAAI 2008. [11] Xilun Chen, K. Selcuk Candan. LWI-SVD: Low-rank, Windowed, Incremental Singular Value Decompositions on Time-Evolving Data Sets. KDD 2014. [12] Xilun Chen, K. Selcuk Candan. GI-NMF: Group Incremental Non-Negative Matrix Factorization on Data Streams. CIKM 2014. [13] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms. 2001. [14] R. Giugno and D. Shasha. Graphgrep: A fast and universal method for querying graphs. ICPR, pp. 112-115, 2002. [15] W. S. Han, J. Lee, and J. H. Lee. TurboISO: Towards ultrafast and robust subgraph isomorphism search in large graph databases. SIGMOD 2013 [16] R. Jonker and T. Volgenant.Improving the Hungarian assignment algorithm. Oper. Res. 171-175. 1986. [17] G. Karypis and V. Kumar "A fast and high quality multilevel scheme for partitioning irregular graphs". SIAM Journal on Scientific Computing 20 (1), 1999. [18] D. Knossow, A. Sharma, D. Mateus, and R. Horaud. Inexact matching of large and sparse graphs using laplacian eigenvectors. GbRPR, 2009. [19] W.-J. Lee and R. P. W. Duin. An inexact graph comparison approach in joint eigenspace. In SSPR/SPR, 35-44, 2008. [20] Jundong Li, Xia Hu, Jiliang Tang, Huan Liu. Unsupervised Streaming Feature Selection in Social Media. CIKM 2015 [21] D. G. Lowe. Distinctive Image Features from Scale-Invariant Keypoints. Int. Journal of Computer Vision, 60, 2, 2004. [22] K. Riesen, X. Jiang, and H. Bunke. Exact and inexact graph matching: Methodology and applications. Managing and Mining Graph Data, pages 217-247, 2010. [23] S. Umeyama. An eigen decomposition approach to weighted graph matching problems. IEEE TPAMI, 10(5):695-703, 1988. [24] J. R. Ullman. An algorithm for subgraph isomorphism, JACM Vol. 23, No. 1, pp. 31-42. 1976 [25] X. Wang, K. S. Candan, M. L. Sapino: Leveraging metadata for identifying local, robust multi-variate temporal (RMT) features. ICDE, 2014. [26] White D.R., et al. Betweenness centrality measures for directed graphs. Social Networks, 16, 335-346,1994. [27] R. Williams. Faster all-pairs shortest paths via circuit complexity. STOC, 664-673. 2014. [28] M. Zaslavskiy, F. R. Bach, and J.-P. Vert. A path following algorithm for the graph matching problem. IEEE Trans. Pattern Anal. Mach. Intell., 31(12):2227-2242, 2009. [29] Y. Zhu, L. Qin, J. X. Yu, et al. High Efficiency and Quality: Large Graphs Matching. CIKM, pp. 1755-1764. 2011.

1110

The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence
Artificial Intelligence for Cyber Security: Technical Report WS-16-03

Socio-Cultural Modeling for Cyber Threat Actors
Paulo Shakarian and Jana Shakarian
Arizona State University
{shak, jshak}@asu.edu

er communities combined with the goal of automating the
collection and analysis of information about the activity of
cyber threat actors produces some very unique challenges.
In this position paper, we describe some unique characteristics of cyber threat socio-cultural environments and several challenging modeling problems for which various artificial intelligence techniques can be used to help solve.

Abstract
In this paper we describe the unique challenges to the important problem of socio-cultural modeling of cyber-threat
actors and why they necessitate further advances in artificial
intelligence â€“ particularly with regard to interdisciplinary
efforts with the social sciences.

Introduction
Cyber security is often referred to as â€œoffense dominantâ€
referring to the notion that the domain generally favors the
attacker (Lynn, 2010). The reasoning behind this is simple:
a successful defense requires total control over all pathways to a system while a successful attack requires only
one. As a result, any given cyber-defense based on the
hardening of systems will fall prey to a cyber-attack as
perpetrators gain knowledge and resources. Solutions have
ranged from sophisticated adaptive defense strategies to
offensive cyber-operations directed against malicious
hackers. However, these methods have various technical
shortcomings â€“ which range from the technical immaturity
of adaptive defenses to consequences of aggressive cyber
counter-operations which can lead to undesirable effects
such as preemptive and preventative cyber war.
A recent trend in the cybersecurity industry has been a
move toward â€œthreat intelligenceâ€ where various sources of
information about potential cyber-attackers are explored
with the goal of pre-empting cyber-attacks before they occur. A key source of cyber-threat intelligence lies in the
digital communities of the malicious hackers â€“ a collection
of sites, markets, chatrooms, and social media channels
where information is shared, hackers are recruited, and the
latest malware and exploits are bought and sold. While
artificial intelligence and machine learning techniques for
analyzing communities on the Internet are long-established
across specialty areas such as data mining, information
retrieval, and web science, we argue that the study of hack-

Characteristics of Cyber Threat
Socio-Cultural Environments
In our group, we have studied hacker communities from a
qualitative standpoint (Shakarian, Shakarian, and Ruef
2015). Throughout this research, we have noted several
unique characteristics in the online socio-cultural environments frequented by malicious hackers that make these
communities distinct from other groups. Some of these
characteristics include the following.
â€¢ Bounded anonymity. Individuals participating in the
malicious hacker community online make efforts to
hide their identity. Some however seek to maintain a
consistent online persona to gain social status in the
hacker meritocracy.
â€¢ Participation in high-risk behavior. Despite recent arrests for individuals associated with darknet markets
as well as suspicions of law-enforcement infiltration,
many individuals still participate in discussions about
illegal activities in darknet forums, though access controls appear to increase. Likewise, individuals participate in hacktivist operations advertised through social
media. A recent lab-based behavioral study has explored some of the potential factors that would lead an
individual to participate in risky hacktivism activities
(Bodford, 2015).
â€¢ High incentives to cheat. The existence of marketplaces where malicious hackers sell software and exploits
to others is an environment where both parties are
highly incentivized to cheat. For instance the sale of a

Copyright Â© 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

193

â€¢ Emergence and disintegration of trust-based communities. For darknet marketplaces to thrive, populations of
individuals have to make decisions to trust both those
running the marketplace and many of the vendors. While
there are established models for trust among individuals,
understanding how the propagation of trust is initiated
and spread in anonymous environments â€“ which seem to
discourage trust â€“ remains an open question. By addressing this problem, we can better understand when a given
cyber-exploit/malware marketplace will become well established.
â€¢ Modeling deception hypotheses. In order to properly attribute individual activity on the darknet to that seen in
public in cases of cyber-attacks or attributing the author
of a given malware or exploit, cyber-security analysts
consider the â€œdeception hypothesesâ€ â€“ the chance that
some or all of the observed evidence can be planted by
an adversary. Therefore, for models designed for problems relating to cyber-attribution, we must also consider
the deception hypothesis. In some of our ongoing efforts, we are leveraging defeasible logic programming to
explicitly consider the deception hypothesis.

faulty product and violations of exclusive use agreements can be conducted with relative ease.
â€¢ Ability to deceive. The anonymous nature of these environments combined with the fact that various aspects of a malicious hackerâ€™s digital persona can be
forged allow for deceptive activities to occur with relative ease.
These characteristics are interesting in several ways.
First, from a sociological and behavioral standpoint, the
freedom at which individuals in these communities discuss
criminal activities as well as share information and code
with individuals likely involved with computer related
crimes (which itself is also a crime) begs the question how
trust is afforded to enable observable social interactions.
Second, the characteristics such as anonymity and deception lead to modeling challenges â€“ perhaps requiring consideration of latent attributes. Third, aspects such as cheating may actually constrain models to a degree â€“ hence
leading to model simplifications.

Modeling Challenges
Conclusion and Ongoing Efforts

In this section, we describe a few major challenges for
modeling socio-cultural cyber threat actor communities.
Overcoming these challenges will provide new insights
into this environment and also aide in higher-level tasks
such as predicting cyber-attacks and understanding the
development of exploits and malware by this community.

In this paper, we have discussed some of the unique
characteristics of the socio-cultural environment for cyber
threat actors and associated modeling problems that are of
interest to the artificial intelligence community. We are
currently exploring these challenges in support of larger
cyber-security goals such as attack prediction and critical
infrastructure defense.

â€¢ Establishment of social status in an anonymous environment. In order for a malicious hacking community to
exist, there must be anonymity, yet actors stand to gain
from prestige earned in the hacker meritocracy, such as
access to invite-only forums, trust in social interactions
in general as opposed to undergoing frequent vetting
processes. Modeling the accumulation of this latent
quantity with which proxy measurements are challenging in non-anonymous environments â€“ and the level of
anonymity itself creates even more difficult challenges.
However, in addressing these challenges, we can better
identify significant cyber threat actors and associate a
greater degree of confidence with their actions. Recently, there has been some initial, descriptive work on this
topic (Abbasi et al., 2014).
â€¢ Data-driven modeling of risk taking. The adoption of
risky behavior has gained attention in the computational
social science literature using model-based approaches
(Roos, Carr, and Nau, 2010). However, instantiating
models based on data remains largely an open question.
The issue is further complicated by limited data on verified activities â€“ as not all cyber-attacks are reported in
the open. The goals in establishing such models for the
study of cyber-threats in determining when certain risky
behavior will occur is likely to aide in prediction and
preventative cyber defense.

Acknowledgements. This work was supported by Arizona
State Universityâ€™s Global Security Initiative (GSI) as well
as the U.S. Office of Naval Research (ONR) NEPTUNE
program.

References
Abbasi, A.; Li, W.; V. Benjamin, V.; Hu, S.; Chen, H. (2014)
Descriptive Analytics: Examining Expert Hackers in Web Forums. IEEE Joint International Conference on Intelligence and
Security Informatics.
Bodford, J. (2015) We are Legion: Hacktivism as a Product of
Deindividuation, Power, and Social Injustice, Masters Thesis,
Arizona State University.
Lynn, W. J. 2010. Defending a New Domain: The Pentagonâ€™s
Cyberstrategy. Foreign Affairs, 89(5).
Roos, P.; Carr, R.; Nau, D. 2010. Evolution of state-dependent
risk preferences. ACM Transactions on Intelligent Systems and
Technology 1(1).
Shakarian, J.; Shakarian, P.; Ruef, A. 2015. Cyber Attacks and
Public Embarrassment: A Survey of Some Notable Hacks. Elsevier SciTechConnect.

194

The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence
Artificial Intelligence for Cyber Security: Technical Report WS-16-03

Toward Argumentation-Based Cyber Attribution
Eric Nunes and Paulo Shakarian

Gerardo I. Simari

Arizona State University
Tempe, AZ 85281, USA
Email: {enunes1, shak}@asu.edu

Inst. for Computer Science and Engineering
(Univ. Nac. del Surâ€“CONICET), Bahia Blanca, Argentina
E-mail: gis@cs.uns.edu.ar

Abstract

multi-label classification problem and applied several machine learning and pruning techniques; the results of this
work are discussed in (Nunes et al. 2015). Based on our observations, machine learning approaches fail in situations of
deception, where similar attributes point towards multiple
attackers. Standard machine learning approaches treat this
problem as â€œlabeling noiseâ€ leading to a random selection
of attacker. We propose to address this issue using a formal
logical framework. Specific contributions of this paper include,
â€¢ a model for cyber-attribution created in the DeLP argumentation framework;

A major challenge in cyber-threat analysis is combining information from different sources to find the person or the
group responsible for the cyber-attack. It is one of the most
important technical and policy challenges in cyber-security.
The lack of ground truth for an individual responsible for an
attack has limited previous studies. In this paper, we overcome this limitation by building a dataset from the capturethe-flag event held at DEFCON, and propose an argumentation model based on a formal reasoning framework called
DeLP (Defeasible Logic Programming) designed to aid an
analyst in attributing a cyber-attack to an attacker. We build
argumentation-based models from latent variables computed
from the dataset to reduce the search space of culprits (attackers) that an analyst can use to identify the attacker. We show
that reducing the search space in this manner significantly improves the performance of classification-based approaches to
cyber-attribution.

â€¢ experiments demonstrating that using argumentationbased tools can significantly reduce the number of potential culprits in a cyber-attack; and
â€¢ experiments showing that the reduced set of culprits,
used in conjunction with classification, leads to improved
cyber-attribution decisions.

Introduction
A major challenge in cyber-threat analysis is to find the
person or the group responsible for a cyber-attack. This is
known as cyber-attribution (Shakarian, Shakarian, and Ruef
2013) and it is one of the central technical and policy challenges in cyber-security. Oftentimes, the evidence collected
from multiple sources provides a contradictory viewpoint.
This gets worse in cases of deception where either an attacker plants false evidence or the evidence points to multiple actors, leading to uncertainty. In our text on cyberwarfare (Shakarian, Shakarian, and Ruef 2013) we discuss
the difficulties that an intelligence analyst faces in attributing an attack to a perpetrator given that deception might have
occurred, and how the analyst needs to explore deception hypotheses under the given attack scenario.
However, one of the major drawbacks of the study and
evaluation of cyber-attribution models is the lack of datasets
with the ground truth available regarding the individual party
responsible for the attackâ€”this has limited previous studies.
To overcome this, we built and leveraged a dataset from the
capture-the-flag event held at DEFCON (cf. the dataset section for a detailed discussion). In previous work, we used
this dataset to study cyber-attribution, and framed it as a

Related Work
Currently, cyber-attribution is limited to identifying machines (Boebert 2010) as opposed to the hacker or their affiliation to a group or a state organization. An example of
such a technical attribution approach is WOMBAT (Dacier,
Pham, and Thonnard 2009), where a clustering technique
is used to group attacks to common IP sources. A method
that combines information from different sources was proposed in (Walls 2014), where forensic information from diverse sources is considered but inconsistency or uncertainty
due to deception is not considered. A less rigorous mathematical model, known as the Q model (Rid and Buchanan
2015), has been proposed recently; there, the model answers queries from an analyst, both technical (tools used)
and non-technical (environment related), and by combining
these query answers the analyst tries to attribute an attack
to a party. Unfortunately, there are no experimental results
evaluating the effectiveness of this model.
Concurrently, we have devised a formal logical framework for reasoning about cyber-attribution (Shakarian et
al. 2015a; 2015b). This reasoning model explores multiple
competing hypotheses based on the evidence for and against
a particular attacker before it attributes the attack to a specific party. With this approach we get a clear map regarding

c 2016, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

177

tion, we used the open source tool tcpflow1 to process the
network captures into a set of files, with each file representing data sent or received on a particular connection.
This produced a corpus of data that could be searched and
processed with standard UNIX tools, like grep. Further analysis of the game environment provided an indicator of when
a data file contained an exploit. The game stored keys for
services in a standard, hard-coded location on each competitorâ€™s server. By searching for the text of this location in the
data, we identified data files that contained exploits for services.
With these data files identified, we analyzed some of them
by hand using the Interactive Disassembler (IDA) to determine if the data contained shell-code, which in fact was the
case. We used an automated tool to produce a summary of
each data file as a JSON encoded element. Included in this
summary was a hash of the contents of the file and a histogram of the processor instructions contained in the file.
These JSON files were the final output of the low level analysis, transforming hundreds of gigabytes of network traffic into a manageable set of facts about exploit traffic in
the data. Each JSON file is a list of tuples (time-stamp,
hash, byte-histogram, instruction-histogram). The individual
fields of the tuple are listed in Table 1.

the arguments that led to the decision. This paper is the first
to provide experimental results using a formal logical framework to build a reasoning model.
The rest of the paper is organized as follows. We present
the argumentative model based on Default Logic Programming (DeLP) (GarcÄ±Ìa and Simari 2004). This is followed by
a description of our DEFCON capture-the-flag dataset and
an analysis on the use of deception within this data. We then
give a summary of our results from (Nunes et al. 2015) and
discuss how we built our argumentation model for cyberattribution with DeLP. Experimental results are discussed in
the subsequent section.

DEFCON CTF Dataset
The DEFCON security conference sponsors and hosts a
capture-the-flag (CTF) competition every year, held on site
with the conference in Las Vegas, Nevada. The CTF competition can be viewed as a microcosm of the global Internet and the careful game of cat and mouse between hacking
groups and security firms. Teams are free to use different
technical means to discover vulnerabilities: they may reverse
engineer programs, monitor the network data sent to their
services, and dynamically study the effects that network data
has on unpatched services. If a team discovers a vulnerability and uses it against another team, the first team may discover that their exploit is re-purposed and used against them
within minutes.
The organizers of DEFCON CTF capture all of the network traffic sent and received by each team, and publish this
traffic at the end of the competition (DEFCON 2013). This
includes IP addresses for source and destination, as well as
the full data sent and received and the time the data was sent
or received. This data is not available to contestants; depending on the organizersâ€™ choice from year to year, the contestants either have a real time feed but with the IP address
obscured, or a full feed delivered on a time delay ranging
from minutes to hours.
In addition to the traffic captures, copies of the vulnerable services are distributed by the organizers, who usually
do not disclose the vulnerabilities they engineered into each
service; however, competitors frequently disclose this information publicly after the game is finished by preparing technical write-ups.
The full interaction of all teams in the game environment
are captured by this data. We cannot build a total picture of
the game at any point in time, since there is state information
from the servers that is not captured, but any exploit attempt
would have to travel over the network and that would be
observed in the data set.

Table 1: Fields in an instance of network attack
Field

Intuition

byte hist

Histogram of byte sequences in the payload

inst hist

Histogram of instructions used in the
payload

from team

The team where the payload originates
(attacking team)

to team

The team being attacked by the exploit

svc

The service that the payload is running

payload hash

The md5 of the payload used

time

Date and time of the attack

Table 2: Example event from the dataset
Field

Value

byte hist

0Ã—43:245, 0Ã—69:8, 0Ã—3a:9, 0Ã—5d:1,
.....

inst hist

cmp:12, svcmi:2, subs:8, movtmi:60
......

Analysis

from team

Robot Mafia

We use the data from the CTF tournament held at DEFCON 21 in 2013; the dataset is very large, about 170 GB
in compressed format. We used multiple systems with distributed and coordinated processing to analyze the entire
corpusâ€”fortunately, analyzing individual streams is easy to
parallelize. To analyze this data, we identified the TCP ports
associated with each vulnerable service. From this informa-

to team

Blue Lotus

svc

02345

payload hash

2cc03b4e0053cde24400bbd80890446c

time

2013-08-03T23:45:17

1

178

https://github.com/simsong/tcpflow

Number of Attacks

400000

tion, as it must have the capability to reason about inconsistency in cases of deception.
Before describing the argumentation model in detail, we
introduce some necessary notation. Variables and constant
symbols represent items such as the exploits/payloads used
for the attack, and the actors conducting the cyber-attack (in
this case, the teams in the CTF competition). We denote the
set of all variable symbols with V and the set of all constants
with C. For our model we require two subsets of C: Cact , denoting the actors capable of conducting the cyber-operation,
and Cexp , denoting the set of unique exploits used. We use
symbols in all capital letters to denote variables. The running
example is based on a subset of our DEFCON CTF dataset:
Example 1. Consider the following actors and cyberoperations from the CTF data:

300000

200000

100000

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

T-3

T-2

T-1

0

Teams

Unique Attacks

Deceptive Attacks

Figure 1: Number of unique and deceptive attacks directed
towards each team.

Cact

After this pre-processing of the network data packets, we
obtained around 10 million network attacks. There are 20
teams in the CTF competition; in order to attribute an attack to a particular team, apart from analyzing the payloads
used by the team, we also need to analyze the behavior of
the attacking team towards their adversary. For this purpose,
we separate the network attacks according to the team being
targeted. Thus, we have 20 such subsets, which we represent
as T-i, where i âˆˆ {1, 2, 3, ..., 20}. An example of an event in
the dataset is shown in Table 2.
We now discuss two important observations from the
dataset, which make the task of attributing an observed network attack to a team difficult.
Deception: In the context of this paper we define an attack
to be deceptive when multiple adversaries get mapped to a
single attack pattern. In the current setting we define deception as the scenario in which the same exploit is used by
multiple teams to target the same team. Figure 1 shows the
distribution of unique deception attacks with respect to the
total unique attacks in the dataset based on the target team.
These unique deceptive attacks amount to just under 35% of
the total unique attacks.

Cexp

= {bluelotus, robotmafia, apt8}
= {exploit1 , exploit2 , ..., exploitn }


The language also contains a set of predicate symbols that
have constants or variables as arguments, and denote events
that can be either true or false. We denote the set of predicates with P; examples of predicates are shown in Table 3.
For instance, culprit(exploit1 , bluelotus) will either be true
or false, and denotes the event where bluelotus used exploit1
to conduct a cyber-operation.
A ground atom is composed by a predicate symbol and a
tuple of constants, one for each of the predicateâ€™s arguments.
The set of all ground atoms is denoted as G. A literal L
is a ground atom or a negated ground atom; hence, literals
have no variables. Examples of ground atoms formed using
predicates in Table 3 are shown in the following example.
We denote a subset of G with G0 .

Example 2. The following are examples of ground atoms
over the predicates given in Table 3.
G0 :

Duplicate attacks: A duplicate attack occurs when the same
team uses the same payload to attack a team at different
points in time. Duplicate attacks can be attributed to two reasons. First, when a team is trying to compromise anotherâ€™s
system, it does not just launch a single attack but a wave
of attacks with very little time difference between consecutive attacks. Second, once a successful payload is created
that can penetrate the defense of other systems, it is used
more by the original attacker as well as the deceptive one as
compared to other payloads. We group duplicates as either
being non-deceptive or deceptive. Non-deceptive duplicates
are the copies of the attacks launched by the team that first
initiated the use of a particular payload; on the other hand,
deceptive duplicates are all the attacks from the teams that
did not initiate the use.

attack(exploit1 , bluelotus),
deception(exploit1 , apt8),
culprit(exploit1 , apt8)

Table 3: Example Predicates and explanation

Argumentation Model
Our approach relies on a model of the world where we
can analyze competing hypotheses in a cyber-operation scenario. Such a model must allow for contradictory informa-

179

Predicate

Explanation

attack(exploit1 , bluelotus)

exploit1 was targeted towards
the team Blue Lotus.

replay attack(E, Y)

Exploit E was replayed by
team Y.

deception(exploit1 , apt8)

Team apt8 used exploit1 for deception.

time diff(I, Y)

Team Y was deceptive within
the given time interval I.

culprit(exploit1 , apt8)

Team apt8 is the likely culprit
for the attack (using exploit1 on
the target team).



We choose a structured argumentation framework (Rahwan, Simari, and van Benthem 2009) to build our argumentation model, which allows for competing ideas to deal
with contradictory information. Due to such characteristics,
argumentation-based models are favorable for cyber-attack
scenarios. Our approach works by creating arguments (in the
form of a set of rules and facts) that compete with each other
to attribute an attack to a given perpetrator. In this case, arguments are defeated based on contradicting information in
other arguments. This procedure is known as a dialectical
process, where the arguments that are undefeated prevail.
An important result is the set of all the arguments that are
warranted (not defeated) by any other argument, which give
a clear map of not only the set of attackers responsible for
the cyber-operation but also the arguments supporting the
conclusion.
The transparency of the model lets a security analyst not
only add new arguments based on new evidence discovered
in the system, but also get rid of incorrect information and
fine-tune the model for better performance. Since the argumentation model can deal with inconsistent information, it
draws a natural analogy to the way humans settle disputes
when there is contradictory information available (GarcÄ±Ìa
and Simari 2004). The model provides a clear explanation as
to why one argument is chosen over others, which is a desirable characteristic for both the analyst and for organizations
to make decisions and policy changes. We now discuss some
preliminaries for the argumentation model.

Î˜:

Î¸1
Î¸2
Î¸3
Î¸4
Î¸5

=
=
=
=
=

â„¦:

Ï‰1 =
Ï‰2 =

âˆ†:

Î´1 =
Î´2 =
Î´3 =
Î´4 =

attack(exploit1 , bluelotus)
first attack(exploit1 , robotmafia)
last attack(exploit1 , apt8))
time diff(interval, robotmafia)
most frequent(exploit1 , pwnies)
culprit(exploit1 , pwnies) â†
most frequent(exploit1 , pwnies),
replay attack(exploit1 )
Â¬ culprit(exploit1 , robotMafia) â†
last attack(exploit1 , apt8),
replay attack(exploit1 )
replay attack(exploit1 ) -â‰º
attack(exploit1 , bluelotus),
last attack(exploit1 , apt8)
deception(exploit1 , apt8) -â‰º
replay attack(exploit1 ),
first attack(exploit1 , robotmafia)
culprit(exploit1 , apt8) -â‰º
deception(exploit1 , apt8),
replay attack(exploit1 )
Â¬culprit(exploit1 , apt8) -â‰º
time diff(interval, robotmafia)

Figure 2: A ground argumentation framework.
the defeasible counterparts of strict rules; they are of the
form L0 -â‰º L1 , ...., Ln , where L0 , is the ground literal and
{Li }i>0 is a set of ground literals. Strong negation is allowed for both strict and defeasible rules to represent contradictory information.

Defeasible Logic Programming
DeLP is a formalism that combines logic programming
with defeasible argumentation; full details are discussed
in (GarcÄ±Ìa and Simari 2004). The formalism is made up of
several constructs, namely facts, strict rules, and defeasible
rules. Facts represent statements obtained from evidence,
and are always true; similarly, strict rules are logical combinations of facts that always hold. On the contrary, defeasible rules can be thought of as strict rules that may be true in
some situations, but could be false if contradictory evidence
is present. These three constructs are used to build arguments. DeLP programs are sets of facts, strict rules and defeasible rules. We use the usual notation to denote DeLP programs: denoting the knowledge base with Î  = (Î˜, â„¦, âˆ†),
where Î˜ is the set of facts, â„¦ is the set of strict rules, and
âˆ† is the set of defeasible rules. Examples of the three constructs are provided with respect to the dataset in Figure 2
and in the Latent variables section. We now describe the
constructs in detail.
Facts (Î˜) are ground literals that represent atomic information or its negation (Â¬). The facts are always true and cannot
be contradicted.
Strict Rules (â„¦) represent cause and effect information that
is always true. They are built from using a combination of
ground literals and are of the form L0 â† L1 , ...Ln , where
L0 is a ground literal and {Li }i>0 is a set of ground literals.
Defeasible Rules (âˆ†) comprise knowledge that can be true
if no contradictory information is provided. These rules are

When a cyber-attack occurs, the model derives arguments
as to who could have conducted the attack. Derivation follows the same mechanism as logic programming (Lloyd
2012). DeLP incorporates defeasible argumentation, which
decides which arguments are warranted and it blocks arguments that are in conflict.
Figure 2 shows a ground argumentation framework
demonstrating constructs derived from the CTF data. For
instance, Î¸1 indicates the fact that exploit1 was used to target
the team Blue Lotus, and Î¸5 indicates that team pwnies is the
most frequent user of exploit1 . For the strict rules, Ï‰1 says
that for a given exploit1 the attacker is pwnies if it was the
most frequent attacker and the attack exploit1 was replayed.
Defeasible rules can be read similarly; Î´2 indicates that
exploit1 was used in a deceptive attack by APT8 if it was
replayed and the first attacker was not APT8. By replacing
the constants with variables in the predicates we can derive
a non-ground argumentation framework.

Definition 1. (Argument) An argument for a literal L is a
pair hA, Li, where A âŠ† Î  provides a minimal proof for L
meeting the requirements: (1) L is defeasibly derived from
A, (2) Î˜âˆªâ„¦âˆªâˆ† is not contradictory, and (3) A is a minimal
subset of âˆ† satisfying 1 and 2, denoted hA, Li.

180





A1 , replay attack(exploit1 ) 

A2 , deception(exploit1 , apt8)


A3 , culprit(exploit1 , apt8) 
A4 , Â¬culprit(exploit1 , apt8)

A1
A2
A3
A4

= {Î´1 , Î¸1 , Î¸3 }
= {Î´1 , Î´2 , Î¸2 }
= {Î´1 , Î´2 , Î´3 }
= {Î´1 , Î´4 , Î¸3 }

Î˜:

Î¸1 =
Î¸2 =
Î¸3 =

attack (E, X)
first attack (E, Y)
last attack (E, Y)

Figure 4: Facts defined for each test sample.

Figure 3: Example ground arguments from Figure 2.
while the set of facts and strict rules is consistent (noncontradictory), the set of defeasible rules can be inconsistent. We engineer our cyber-attribution framework as a series of defeasible and strict rules whose structure we have
created, but are dependent on values learned from a historical corpus. Then, for a given incident, we instantiate a set
of facts for that situation. This information is then provided
as input into a DeLP implementation that uses heuristics to
generate all arguments for and against every possible culprit
for the cyber attack. Then, the DeLP implementation creates dialectical trees based on these arguments and decides
which culprits are warranted. This results in a reduced set of
potential culprits, which we then use as input into a classifier
to obtain an attribution decision.

Literal L is called the conclusion supported by the argument, and A is the support. An argument hB, Li is a subargument of hA, L0 i iff B âŠ† A.
The following shows arguments for our running example
scenario.
Example 3. Figure 3 shows example arguments based on
the knowledge base from Figure 2. Note that the following
relationship exists:




A1 , replay attack(exploit1 ) is
 a subargument of
A
,
deception(exploit
,
apt8)
and
2
1 


A3 , culprit(exploit1 , apt8) .


Latent Variables

For a given argument there may be counter-arguments that
contradict it. For instance, referring to Figure 3, we can see
that A4 attacks A3 . A proper defeater of an argument hA, Li
is a counter-argument thatâ€”by some criterionâ€”is considered to be better than hA, Li; if the two are incomparable
according to this criterion, the counterargument is said to be
a blocking defeater. The default criterion used in DeLP for
argument comparison is generalized specificity (Stolzenburg
et al. 2003).
A sequence of arguments is referred to as an argumentation line. There can be more than one defeater argument,
which leads to a tree structure that is built from the set of
all argumentation lines rooted in the initial argument. In a
dialectical tree, every child can defeat its parent (except for
the root), and the leaves represent the undefeated arguments.
The tree thus creates a map of all possible argumentation
lines that decide if an argument is defeated


or not.
Given a literal L and an argument A, L , in order to decide whether or not a literal L is warranted, every node in the
dialectical tree T (hA, Li) is recursively marked as â€œDâ€ (defeated) or â€œUâ€ (undefeated), obtaining a marked dialectical
tree T âˆ— (hA, Li) where:

In (Nunes et al. 2015) we leveraged machine learning techniques on the CTF data to identify the attacker. We will now
provide a summary of the results obtained.
The experiment was performed as follows. We divide the
dataset according to the target team, building 20 subsets, and
all the attacks are then sorted according to time. We reserve
the first 90% of the attacks for training and the remaining
10% for testing. The byte and instruction histograms are
used as features to train and test the model. Models constructed using a random forest classifier performed the best,
with an average accuracy of 0.37. Most of the misclassified
samples tend to be deceptive attacks and their duplicates.
When using machine learning approaches it is difficult to
map the reasons why a particular attacker was predicted, especially in cases of deception where multiple attackers were
associated with the same attack. Knowing the arguments that
supported a particular decision would greatly aid the analyst in making better decisions dealing with uncertainty. To
address this issue we now describe how we can form arguments/rules based on the latent variables computed from the
training data, given an attack for attribution.
We use the following notation: let E be the test attack under consideration aimed at target team X, Y represent all the
possible attacking teams, and D be the set of all deceptive
teams (those using the same payload to target the same team)
if the given attack is deceptive in the training set. For nondeceptive attacks, D will be empty. We note that facts cannot have variables, only constants (however, to compress the
program for presentation purposes, we use meta-variables
in facts). To begin, we define the facts described in Figure 4;
fact Î¸1 states that attack E was used to target team X, Î¸2
states that team Y was the first team to use the attack E in the
training data, and similarly Î¸3 states that team Y was the last
team to use the attack E in the training data. The first and
last attacking team may or may not be the same. We study

â€¢ All leaves in T âˆ— (hA, Li) are marked as â€œUâ€s, and
â€¢ Let hB, qi be an inner node of T âˆ— (hA, Li). Then, hB, qi
will be marked as â€œUâ€ iff every child of hB, qi is marked
as â€œDâ€. Node hB, qi will be marked as â€œDâ€ iff it has at
least a child marked as â€œUâ€.
Given argument hA, Li over Î , if the root of T âˆ— (hA, Li) is
marked â€œUâ€, then T âˆ— (hA, hi) warrants L and that L is warranted from Î . (Warranted arguments correspond to those
in the grounded extension of a Dung argumentation system
(Dung 1995).)
In practice, an implementation of DeLP accepts as input sets of facts, strict rules, and defeasible rules. Note that

181

â„¦:

Ï‰2 =
âˆ†:

Î´1 =
Î´2 =
Î´3 =

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

culprit(E, Df ) â† frequent(E, Df ),
deception (E, Di )
Â¬culprit(E, Y) â† first attack(E, Y),
deception(E, X)

0

T-7

Ï‰1 =

200

T-6

deception (E, X)
frequent (E, Df )

400

T-5

Î¸1 =
Î¸2 =

600

Teams

Figure 7: Average time for duplicate attacks for each team.
16000

Average time (sec) to issue deceptive
attack

Î˜:

800

T-1

Figure 5: Defeasible and strict rule for non-deceptive attack.

1000

T-4

replay attack(E) -â‰º attack(E, X),
last attack(E, Y).

1200

T-3

Î´1 =

culprit(E, Y) â† last attack(E, Y),
replay attack(E).

T-2

âˆ†:

Ï‰1 =

Average time (sec) to replay own attacks

â„¦:

replay attack(E) -â‰º attack(E, X),
last attack(E, Y)
deception(E, Di ) -â‰º replay attack(E),
first attack(E, Y)
culprit(E, Di ) -â‰º deception(E, Di ),
first attack(E, Y)

14000
12000
10000
8000
6000
4000
2000

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-9

T-11

Figure 6: Facts and rules for deceptive attacks.

T-10

T-8

T-7

T-6

T-5

T-4

T-3

T-2

T-1

0

Teams

Figure 8: Average time for deceptive attacks for each team.
the following three cases:
Case 1: Non-deceptive attacks. In non-deceptive attacks,
only one team uses the payload to target other teams in the
training data. It is easy to predict the attacker for these cases,
since the search space only has one team. To model this situation, we define the set of defeasible and strict rules shown
in Figure 5. Defeasible rule Î´1 checks whether the attack was
replayed in the training dataâ€”since it is a non-deceptive attack, it can only be replayed by the same team. Strict rule Ï‰1
then puts forth an argument for the attacker (culprit) if the
defeasible rule holds and there is no contradiction for it.

the K nearest neighbors from the training set according to
a simple Euclidean distance between the byte and instruction histograms of the two attacks. In this case we choose
K = 3. For each of the matching attack from the training
data we check if the attack is deceptive or non-deceptive. If
non-deceptive, we follow the procedure for Case 1, otherwise we follow the procedure for Case 2. Since we replace
one unseen attack with three seen attacks, the search space
for the attacker increases for unseen attacks.

Case 2: Deceptive attacks. These attacks form the majority of the misclassified samples in (Nunes et al. 2015). The
set D is not empty for this case; let Di denote the deceptive
teams in D. We also compute the most frequent deceptive
attacker from D. Let the most frequent attacker be denoted
as Df . Figure 6 shows some of the DeLP components that
model this case. Fact Î¸1 indicates if the attack E was deceptive towards the team X and Î¸2 indicates the most frequent
attacker team Df from the deceptive team set D. The strict
rules Ï‰1 states that the attacker should be Df if the attack is
deceptive and Ï‰2 indicates that in case of deception the first
attack team Y is not the attacker. For the defeasible rules, Î´1
deals with the case in which the attack E was replayed, Î´2
deals with the case of deceptive teams from the set D and
Î´3 indicates that all the deceptive teams are likely to be the
attackers in the absence of any contradictory information.

Attacker Time Analysis
The CTF data provides us with time stamps for the attacks
in the competition. We can use this information to come up
with rules for/against an argument for a team being the attacker. We compute the average time for a team to replay its
own attack given that it was the first one to initiate the attack (see Figure 7). It can be observed that teams like more
smoked leet chicken (T-13) and Wowhacker-bios (T-8) are
very quick to replay their own attacks as compared to other
teams.
Figure 8 on the other hand shows the average time for a
team to replay an attack initiated by some other team. This
refers to the time the team takes to commit a deceptive attack. Teams like The European (T-7) and Blue lotus (T-10)
are quick to commit deception, while others take more time.
We use this time information to narrow down our search
space for possible attackers. In particular, for a deceptive
test sample, we compute the time difference between the test
sample and the training sample that last used the same payload. We denote this time difference as 4t, and include it

Case 3: Unseen Attacks. The most difficult attacks to attribute in the dataset are the unseen ones, i.e., attacks encountered in the test set that did not occur in the training set.
To build constructs for this kind of attack we first compute

182

9

as a fact Î¸1 . We then divide the deceptive times from Figure 8 into appropriate intervals; each team is assigned to one
of those time intervals. We then check which time interval
4t belongs to and define a defeasible rule Î´1 that makes a
case for all teams not belonging to the interval to not be the
culprits, as shown in Figure 9.
Î˜:

Î¸1 =

timedifference (E, X)

Î´1 =

For Y âˆˆ
/ interval:
Â¬culprit(E, Y) -â‰º timedifference (E, X).

Average number of Teams

8
7
6

5
4
3
2
1

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

T-3

T-2

Teams

Figure 10: Average Search space after running the query to
compute warranted arguments.

Figure 9: Time facts and rules. Interval indicates a small portion of the entire deceptive time (for instance < 2000 sec.,
> 8000 sec., and so on).

Fraction of test samples with ground
truth

1

Experiments
We use the same experimental setup as in (Nunes et al.
2015). We sort the dataset by time for each target team and
then use the first 90% of the data for training and the rest
10% for testing. We compute the constructs for all test samples based on the cases discussed in the previous section, and
then input these arguments to the DeLP implementation. For
each test sample we query the DeLP system to find all possible attackers (culprits) based on the arguments provided. If
there is no way to decide between contradicting arguments,
these are blocked and thus return no answers. Initially, the
search space for each test sample is 19 teams.
Figure 10 shows the average search space after running
the queries to return set of possible culprits. There is a significant reduction in search space across all target teams. The
average search space is reduced from 19 teams to 5.8 teams
(standard deviation of 1.92). To evaluate how much the reduced search space can aid an analyst in predicting the actual culprit, we compute one more metricâ€”we check if the
reduced search space has the ground truth (actual culprit)
present in it. Figure 11 shows the fraction of test samples
with ground truth present in it. For the majority of the target
teams, we have ground truth present in more than 70% of
the samples with reduced search space. For some teams like
more smoked leet chicken (T-13) and raon ASRT (whois) (T17) the average reduced search space is as low as 1.82 and
2.9 teams, with high ground truth fraction of 0.69 and 0.63
respectively. The reason why there is a large search space
for some teams is the presence of a larger number of unseen
attacks. As discussed earlier, each unseen attack is replaced
by the three most similar attacks from the training set, and
hence the search space becomes larger.
We next perform predictive analysis on the reduced search
space. The experimental setup is similar to the one described
earlier; the only difference is that now instead of having a
19-team search space as in the previous case, the machine
learning approach is allowed to make a prediction from the
reduced search space only. We use random forest as the machine learning approach, which has shown to have the best
performance for CTF data (Nunes et al. 2015). Table 4 gives
a summary of the results.

0.8

0.6

0.4

0.2

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-10

T-9

T-8

T-7

T-6

T-5

T-4

T-3

T-1

0

T-2

âˆ†:

T-1

0

Teams

Figure 11: Fraction of test samples with the ground truth in
the reduced search space.
We report three accuracies for the 20 teams. â€œPrev. Acc.â€
represents the accuracies achieved after running Random
forest without applying the argumentation based techniques,
as reported in (Nunes et al. 2015). â€œAcc. (ground truth)â€ considers only the test samples where ground truth is present
in the reduced search space. With this setting we are able
to correctly predict on average more than 70% of the test
samples, with T-4, T-13 and T-14 having accuracies over
80%. On the other hand, â€œAcc. (all)â€ also takes into account
test samples where ground truth was not present in the reduced search space. It is clear that all those test samples will
be misclassified since the ground truth is not present in the
search space at all. Even accounting for those samples, the
average accuracy is 0.5, which is significantly better than the
average accuracy of 0.37 in (Nunes et al. 2015). The results
are statistically significant (t(20) = 6.83, p < 0.01).

Conclusion
In this paper we showed how an argumentation-based framework (DeLP) can be leveraged to improve cyber-attribution
decisions. We demonstrated this concept using the DeLP
framework applied to CTF data, which reduced the set of
potential culprits allowing for greater accuracy when using
a classifier for cyber-attribution.
We are currently looking at implementing our probabilistic variant of DeLP (Shakarian et al. 2015b), which assigns
probabilities to the arguments, helping in this way to decide
which arguments prevail when contradictory information is

183

Table 4: Summary of Results
Team

Acc. (ground
truth)

Acc. (all)

Prev.
Acc.

T-1

0.69

0.51

0.45

T-2

0.72

0.45

0.22

T-3

0.55

0.40

0.30

T-4

0.81

0.44

0.26

T-5

0.68

0.45

0.26

T-6

0.62

0.49

0.5

T-7

0.70

0.53

0.45

T-8

0.75

0.61

0.42

T-9

0.68

0.50

0.41

T-10

0.77

0.42

0.30

T-11

0.62

0.44

0.37

T-12

0.76

0.43

0.24

T-13

0.85

0.63

0.35

T-14

0.71

0.52

0.42

T-15

0.57

0.38

0.30

T-16

0.68

0.48

0.43

T-17

0.80

0.58

0.42

T-18

0.68

0.50

0.48

T-19

0.70

0.51

0.41

T-20

0.74

0.51

0.48

programming and n-person games. Artificial intelligence
77(2):321â€“357.
GarcÄ±Ìa, A. J., and Simari, G. R. 2004. Defeasible logic programming: An argumentative approach. Theory and practice of logic programming 4(1+ 2):95â€“138.
Lloyd, J. W. 2012. Foundations of logic programming.
Springer Science & Business Media.
Nunes, E.; Kulkarni, N.; Shakarian, P.; Ruef, A.; and Little,
J. 2015. Cyber-deception and attribution in capture-the-flag
exercises. In Proceedings of the 2015 International Symposium on Foundation of Open Source Intelligence and Security Informatics (FOSINT-SI).
Rahwan, I.; Simari, G. R.; and van Benthem, J. 2009. Argumentation in artificial intelligence, volume 47. Springer.
Rid, T., and Buchanan, B. 2015. Attributing cyber attacks.
Journal of Strategic Studies 38(1-2):4â€“37.
Shakarian, P.; Simari, G. I.; Moores, G.; and Parsons, S.
2015a. Cyber attribution: An argumentation-based approach. In Cyber Warfare. Springer. 151â€“171.
Shakarian, P.; Simari, G. I.; Moores, G.; Paulo, D.; Parsons,
S.; Falappa, M.; and Aleali, A. 2015b. Belief revision in
structured probabilistic argumentation. Annals of Mathematics and Artificial Intelligence 1â€“43.
Shakarian, P.; Shakarian, J.; and Ruef, A. 2013. Introduction
to cyber-warfare: A multidisciplinary approach. Newnes.
Stolzenburg, F.; GarcÄ±Ìa, A. J.; Chesnevar, C. I.; and Simari,
G. R. 2003. Computing generalized specificity. Journal of
Applied Non-Classical Logics 13(1):87â€“113.
Walls, R. J. 2014. Inference-based Forensics for Extracting Information from Diverse Sources. Ph.D. Dissertation,
University of Massachusetts Amherst.

present. We are also designing our own CTF event in order
to obtain more realistic data.

Acknowledgments
Authors of this work were supported by the U.S. Department of the Navy, Office of Naval Research, grant N0001415-1-2742 as well as the Arizona State University Global
Security Initiative (GSI) and funds provided by CONICET
and Universidad Nacional del Sur, Argentina. Any opinions,
findings, and conclusions or recommendations expressed in
this material are those of the author(s) and do not necessarily
reflect the views of the Office of Naval Research.

References
Boebert, W. E. 2010. A survey of challenges in attribution.
In Proceedings of a workshop on Deterring CyberAttacks,
41â€“54.
Dacier, M.; Pham, V.-H.; and Thonnard, O. 2009. The wombat attack attribution method: some results. In Information
Systems Security. Springer. 19â€“37.
DEFCON. 2013. Defcon: Capture the flag.
Dung, P. M. 1995. On the acceptability of arguments
and its fundamental role in nonmonotonic reasoning, logic

184

Darknet and Deepnet Mining for Proactive Cybersecurity Threat Intelligence
Eric Nunes, Ahmad Diab, Andrew Gunn, Ericsson Marin , Vineet Mishra, Vivin Paliath, John Robertson, Jana Shakarian, Amanda Thart, Paulo Shakarian
Arizona State University Tempe, AZ 85281, USA Email: {enunes1, ahmad.diab, andrewgunn, ericsson.marin, vvmishra, vivin.paliath, jj.robertson, jshak, amanda.thart, shak} @asu.edu

arXiv:1607.08583v1 [cs.CR] 28 Jul 2016

Abstract--In this paper, we present an operational system for cyber threat intelligence gathering from various social platforms on the Internet particularly sites on the darknet and deepnet. We focus our attention to collecting information from hacker forum discussions and marketplaces offering products and services focusing on malicious hacking. We have developed an operational system for obtaining information from these sites for the purposes of identifying emerging cyber threats. Currently, this system collects on average 305 high-quality cyber threat warnings each week. These threat warnings include information on newly developed malware and exploits that have not yet been deployed in a cyber-attack. This provides a significant service to cyberdefenders. The system is significantly augmented through the use of various data mining and machine learning techniques. With the use of machine learning models, we are able to recall 92% of products in marketplaces and 80% of discussions on forums relating to malicious hacking with high precision. We perform preliminary analysis on the data collected, demonstrating its application to aid a security expert for better threat analysis.

TABLE 1: Exploit example. Timeline Feb. 2015 Event Microsoft identifies Windows vulnerability MS15010/CVE 2015-0057 for remote code execution. There was no publicly known exploit at the time the vulnerability was released. An exploit for MS15-010/CVE 2015-0057 was found on a darknet market on sale for 48 BTC (around $10,000-15,000). FireEye identified that the Dyre Banking Trojan, designed to steal credit card number, actually exploited this vulnerability1 .

April 2015

July 2015

I.

I NTRODUCTION

Pre-reconnaissance cyber threat intelligence refers to information gathered before a malicious party interacts with the defended computer system. An example demonstrating the importance of cyber threat intelligence is shown in Table 1. A Microsoft Windows vulnerability was identified in Feb. 2015. The release of the vulnerability was essentially Microsoft warning its customers of a security flaw. Note that at this time, there was no publicly known method to leverage this flaw in a cyber-attack (i.e. an available exploit). However, about a month later an exploit was found to be on sale in darknet market. It was not until July when FireEye, a major cybersecurity firm, identified that the Dyre Banking Trojan designed to steal credit cards exploited this vulnerability - the first time an exploit was reported. This vignette demonstrates how threat warnings gathered from the darknet can provide valuable information for security professionals. The average global exposure of the Dyre Banking Trojan was 57.3% along with another banking malware Dridex1 . It means that nearly 6 out of 10 organizations in the world were affected, and this is a significantly high number on a global level. In this paper, we examine how such intelligence can be gathered and analyzed from various social platforms on the In1 https://www.fireeye.com/blog/threat-research/2015/06/evolution

ternet particularly sites on the darknet and deepnet. In doing so, we encounter several problems that we addressed with various data mining techniques. Our current system is operational and actively collecting approximately 305 cyber threats each week. Table 2 shows the current database statistics. It shows the total data collected and the data related to malicious hacking. The vendor and user statistics cited only consider those individuals associated in the discussion or sale of malicious hackingrelated material, as identified by the system. The data is collected from two sources on the darknet/deepnet: markets and forums.
TABLE 2: Current Database Status Total Number Markets Total products Hacking related Vendors Total Number Forums Topics/Posts Hacking related Users 27 11991 1573 434 21 23780/162872 4423/31168 5491

We are providing this information to cyber-security professionals to support their strategic cyder-defense planning to address questions such as, 1) What vendors and users have a presence in multiple darknet/deepnet markets/ forums? 2)What zero-day exploits are being developed by malicious hackers? 3) What vulnerabilities do the latest exploits target?

of dridex.html

Specific contributions of this paper include, 1) Description of a system for cyber threat intelligence gathering from various social platforms from the Internet such as deepnet and darknet websites. 2) The implementation and evaluation of learning models to separate relevant information from noise in the data collected from these online platforms. 3) A series of case studies showcasing various findings relating to malicious hacker behavior resulting from the data collected by our operational system. Background: Many of the individuals behind cyber-operations ­ originating outside of government run labs or military commands ­ rely on a significant community of hackers. They interact through a variety of online forums (as means to both stay anonymous and to reach geographically dispersed collaborators). Darknet and Deepnet Sites: Widely used for underground communication, "The Onion Router" (Tor) is free software dedicated to protect the privacy of its users by obscuring traffic analysis as a form of network surveillance [9]. The network traffic in Tor is guided through a number of volunteeroperated servers (also called "nodes"). Each node of the network encrypts the information it blindly passes on neither registering where the traffic came from nor where it is headed [9], disallowing any tracking. Effectively, this allows not only for anonymized browsing (the IP-address revealed will only be that of the last node), but also for circumvention of censorship2 . Here, we will use "darknet" to denote the anonymous communication provided by crypto-networks like "Tor", which stands in contrast to "deepnet" which commonly refers to websites hosted on the open portion of the Internet (the "Clearnet"), but not indexed by search engines [15]. Markets: Users advertise and sell their wares on marketplaces. Darknet marketplaces provide a new avenue to gather information about the cyber threat landscape. The marketplaces sell goods and services relating to malicious hacking, drugs, pornography, weapons and software services. Only a small fraction of products (13% in our collected data to date) are related to malicious hacking. Vendors often advertise their products on forums to attract attention towards their goods and services. Forums. Forums are user-oriented platforms that have the sole purpose of enabling communication. It provides the opportunity for the emergence of a community of like-minded individuals - regardless of their geophysical location. Administrators set up Darknet forums with communication safety for their members in mind. While structure and organization of Darknet-hosted forums might be very similar to more familiar web-forums, the topics and concerns of the users vary distinctly. Forums addressing malicious hackers feature discussions on programming, hacking, and cyber-security. Threads are dedicated to security concerns like privacy and online-safety - topics which plug back into and determine the structures and usage of the platforms. II. SYSTEM OVERVIEW

were able to find forums and marketplaces populated by malicious hackers. Other platforms were discovered through links posted on forums either on the Tor-network or on the Clearnet. The system consists of three main modules built independently before integration. The system is currently fully integrated and actively collecting cyber threat intelligence. Crawler: The crawler is a program designed to traverse the website and retrieve HTML documents. Topic based crawlers have been used for focused crawling where only webpages of interest are retrieved [17], [6]. More recently, focused crawling was employed to collect forum discussions from darknet [10]. We have designed separate crawlers for different platforms (markets/forums) identified by experts due to the structural difference and access control measures for each platform. In our crawler, we address design challenges like accessibility, unresponsive server, repeating links creating a loop etc. to gather information regarding products from markets and discussions on forums. Parser: We designed a parser to extract specific information from marketplaces (regarding sale of malware/exploits) and hacker forums (discussion regarding services and threats). This well-structured information is stored in a relational database. We maintain two databases, one for marketplaces and the other for forums. Like the crawler, each platform has its own parser. The parser also communicates with the crawler from time to time for collection of temporal data. The parser communicates a list of relevant webpages to the crawler, which are re-crawled to get time-varying data. For markets we collect the following important products fields: {item title, item description, vendor name, shipping details, item reviews, items sold, CVE, items left, transaction details, ratings}. For forums we collect the following fields: {topic content, post content, topic author, post author, author status, reputation, topic interest}. Classifier: We employ a machine learning technique using an expert-labeled dataset to detect relevant products and topics from marketplaces and forums respectively discussed in Section III. These classifiers are integrated into the parser to filter out products and topics relating to drugs, weapons, etc. not relevant to malicious hacking. III. E VALUATION

We consider the classification of identifying relevant products in darknet/deepnet marketplaces and relevant topics on forum post containing communication relevant to malicious hacking in this paper. It is a binary classification problem with the data sample (in this case products/forum topics) being relevant or not. We look at both supervised and semisupervised approaches to address the classification. A. Machine Learning Approaches In this work, we leverage a combination of supervised and semi-supervised methods. Supervised methods include the well-known classification techniques of Naive Bayes (NB), random forest (RF), support vector machine (SVM) and logistic regression (LOG-REG). However, supervised techniques required labeled data, and this is expensive and often requires expert knowledge. Semi-supervised approaches work with limited labeled data by leveraging information from unlabeled data. We discuss popular semi-supervised

Fig. 1 gives the overview of the system. Through search engines and spider services on the Tor network, human analysts
2 See

the Tor Project's official website (https://www.torproject.org/)

Fig. 1: System overview

approaches used in this work. We perform a grid search to find optimal parameters for the learning techniques. Label propagation (LP). The label propagation approach [22] has been widely used for semi-supervised classification task [3], [16], [21], [8]. It estimates the label values based on graph Laplacian [1] where the model is represented by a weighted graph G = (V, E ) , where V indicates the vertices representing the samples, while the edges E are the weights indicating the similarity between points. A subset of these vertices are labeled and these vertices are then used to estimate the labels of the remaining under the assumption that the edges are able to capture the similarity between samples. Hence, the performance of these methods depends on the similarity measure used. The most commonly used similarity measures include k -NN and Gaussian kernel. Co-training (CT). The Co-training approach was proposed by Blum and Mitchell [4]. In this approach, the feature set is divided into two sets (assumed to be independent), and two classifiers are trained using the limited labeled set denoted by L . These trained classifiers are then used to estimate the labels for the unlabeled points. High confidence label estimates from classifier-1 are added to the labeled set L of classifier-2 and vice versa. For the current setting we set the confidence to 70%. Every time the labeled set L is updated, the classifiers are retrained. This procedure repeats until all of the unlabeled points are labeled. It can be viewed as two classifiers teaching each other. B. Experiments: Marketplaces Marketplaces sell goods and services that do not relate to malicious hacking, including drugs, pornography, weapons and software services. Only a small fraction of products (13%) are related to malicious hacking. We thus require a model that can separate relevant products from the non-relevant ones. The data collected from marketplaces is noisy and hence not suitable to use directly as input to a learning model. Hence, the raw information undergoes several steps of automated data cleaning. We now discuss the challenges associated with the dataset obtained and the data processing steps taken to address them. We note that similar challenges occur for forum data. Text Cleaning. Product title and descriptions on marketplaces often have much text that serves as noise to the classifier (e.g. *****SALE*****). To deal with these instances, we first removed all non-alphanumeric characters from the title and

description. This, in tandem with standard stop-word removal, greatly improved classification performance. Misspellings and Word Variations. Misspellings frequently occur on forums and marketplaces, which is an obstacle for the standard bag-of-words classification approach. Additionally, with the standard bag-of-words approach, variations of words are considered separately (e.g. hacker, hack, hackers, etc.). Word stemming mitigates these issue of word variations, but fails to fix the issue of misspellings. To address this we use character n-gram features. As an example of character n-gram features, consider the word "hacker". If we were using tri-gram character features, the word "hacker" would yield the features "hac", "ack", "cke", "ker". The benefit of this being that the variations or misspellings of the word in the forms "hack", "hackz", ""hackker", will all have some common features. We found that using character n-grams in the range (3, 7) outperformed word stemming in our experiments. Large Feature Space. In standard bag-of-words approach, as opposed to the character n-gram approach, the feature matrix gets very large as the number of words increase. As the number of unique words grow, this bloated feature matrix begins to greatly degrade performance. Using n-gram features further increases the already over-sized feature matrix. To address this issue, we leveraged the sparse matrix data structure in the scipy3 library, which leverages the fact that most of the entries will be zero. If a word or n-gram feature is not present in a given sample, there is simply no entry for that feature in the sparse matrix. Preserving Title Feature Context. As the title and description of the product are disjoint, we found that simply concatenating the description to the product title before extracting features led to sub-optimal classification performance. We believe that by doing a simple concatenation, we were losing important contextual information. There are features that should be interpreted differently should they appear in the title versus the description. Initially, we used two separate classifiers: one for the title and one for the description. With this construction, when an unknown product was being classified, we would pass the title to the title classifier and the description to the description classifier. If either classifier returned a positive classification, we would assign the product a positive classification. However, we believe that this again led to the loss of important contextual information. To fix this, we independently extract character n-gram features from the title and description.
3 http://www.scipy.org/

TABLE 3: Markets and Number of products collected. Markets Market-1 Market-2 Market-3 Market-4 Market-5 Products 439 1329 455 4018 876 Markets Market-6 Market-7 Market-8 Market-9 Market-10 Products 497 491 764 2014 600

and time investment can be reduced by applying a semisupervised approach which leverages the unlabeled data to aid in classification. It takes approximately one minute for a human to label 5 marketplace products or 2 topics on forums as relevant or not, highlighting the costliness of manual labeling. The experimental setup is similar to the supervised approach, but this time we also utilize the large unlabeled data from each marketplace (75%) for training.
1

This step yields a title feature vector and a description feature vector. We then horizontally concatenate these vectors, forming a single feature vector which includes separate feature sets for the title and description. Results: We consider 10 marketplaces to train and test our learning model. A summary of these marketplaces is shown in Table 3. Table 4 gives an instance of products defined as being relevant or not. With the help of security experts we label 25% of the products from each marketplace. The experimental setup is as follows. We perform a leave-one-marketplace-out cross-validation. In other words, given n marketplaces we train on n - 1 and test on the remaining one. We repeat this experiment for all the marketplaces. For the supervised experiment, we only use the 25% labeled data from each marketplace. We evaluate the performance based primarily on three metrics: precision, recall and unbiased F1. Precision indicates the fraction of products that were relevant from the predicted ones. Recall is the fraction of relevant products retrieved. F1 is the harmonic mean of precision and recall. The results are averaged and weighted by the number of samples in each market. In this application, a high recall is desirable as we do not want to omit relevant products. In the supervised approaches, SVM with linear kernel performed the best, recalling 87% of the relevant products while maintaining a precision of 85% (Fig. 2). SVM performed the best likely due to the fact it maximizes generality as opposed to minimizing error.
TABLE 4: Example of Products. Product Title 20+ Hacking Tools (Botnets Keyloggers Worms and More!) 5 gm Colombian Cocaine Relevant YES NO

0.9

Average

0.8

0.7

0.6

Precision LP CT-NB

Recall CT-LOG-REG CT-RF

F1
CT-SVM

Fig. 3: Average Precision, Recall and F1 comparisons for LP, CT-NB, CT-LOG-REG, CT-RF and CT-SVM for product classification.

Fig. 3 shows the performance comparison for the semisupervised approaches. For the co-training approach, we divide the feature space into two sets. The two feature sets used are both based on character n-grams. However, the set of words from which the character n-grams are derived are disjoint between the two sets. In this way, the two corresponding feature vectors can be treated as being independent from one another. Hence we get two views of the same sample. Co-training with Linear SVM is able to recall 92% of the relevant products as compared to label propagation and other variants of co-training while maintaining a precision of 82%, which is desirable. In this case, the unlabeled data aided the classification in improving the recall to 92% without significantly reducing the precision. C. Experiment: Forums In addition to the darknet/deepnet marketplaces that we have already discussed, there are also numerous darknet forums on which users discuss malicious hacking related topics. Again, there is the issue that only a fraction of these topics with posts on these forums contain information that is relevant to malicious hacking or the trading of exploits. Hence, we need a classifier to identify relevant topics. This classification problem is very similar to the product classification problem previously discussed, with similar set of challenges. We performed evaluation on two such English forums. The dataset consisted of 781 topics with 5373 posts. Table 5 gives instance of topics defined as being relevant or not. We label 25% of the topics and perform a 10-fold cross validation using supervised methods. We show the results from the top two performing supervised and semi-supervised methods. In the supervised setting, LOG-REG performed the best with 80% precision and 68% recall (Fig. 4). On the other hand, leveraging unlabeled data in a semi-supervised technique improved the recall while maintaining the precision. We note that in this case the 10-fold cross validation was performed only on the labeled points. In the semi-supervised domain

0.9

0.8

Average

0.7

0.6

0.5

Precision NB

Recall LOG-REG RF SVM

F1

Fig. 2: Average Precision, Recall and F1 comparisons for NB, LOG-REG, RF and SVM for product classification.

As stated, only 25% of the data is labeled, as labeling often requires expert knowledge. However, this significant cost

co-training with LOG-REG improved the recall to 80% with precision of 78%.
TABLE 5: Example of Topics. Topic Bitcoin Mixing services Looking for MDE/MDEA shipped to Aus Relevant YES NO

TABLE 6: Example of Zero-day exploits. Zero-day exploit Internet Explorer 11 Remote Code Execution 0day Android WebView 0day RCE Price (BTC) 20.4676 40.8956

0.9

0.8

0.7

0.6

0.5

Precision LOG-REG SVM

Recall CT-LOG-REG CT-SVM

F1

Fig. 4: Average Precision, Recall and F1 comparisons for LOG-REG, SVM, CT-LOGREG, and CT-SVM for English forum topic classification.

IV.

C ASE S TUDIES

We analyze the data with the purpose of answering the questions raised in the Section I. We will be using the following key security terms. Vulnerability is a security flaw that allows an attacker to compromise a software or an operating system. Exploit is a piece of software that takes advantage of a vulnerability in a piece of software or operating system to compromise it. Patch is a piece of software used to improve existing software by fixing vulnerabilities to improve security. We discuss the following case-studies. A. Discovery of Zero-Day Exploits. Over a 4 week period, we detected 16 zero-day exploits from the marketplace data. Zero-day exploits leverage vulnerabilities that are unknown to the vendor. Table 6 shows a sample of zero-day exploits with their selling price in Bitcoin. The Android WebView zero-day affects a vulnerability in the rendering of web pages in Android devices. It affects devices running on Android 4.3 Jelly Bean or earlier versions of the operating system. This comprised of more than 60% of the Android devices in 2015. After the original posting of this zero-day, a patch was released in Android KitKit 4.4 and Lollipop 5.0 which required devices to upgrade their operating system. As not all users have/will update to the new operating system, the exploit continues to be sold for a high price. Detection of these zero-day exploits at an earlier stage can help organizations avoid an attack on their system or minimize the damage. For instance, in this case, an organization may decide to prioritize patching, updating, or replacing certain systems using the Android operating system. B. Users having presence in markets/ forums. Previous studies on darknet crawling [10], [2] explore a single domain, namely forums. We create a social network that includes both types of information studied in this paper: marketplaces and forums. We can thus study and find these

cross-site connections that were previously unstudied. We are able to produce this connected graph using the "usernames" used by vendors and users in each domain. A subgraph of this network containing some of the individuals who are simultaneously selling products related to malicious hacking and publishing in hacking related forums is shown in Fig. 5. In most cases, the vendors are trying to advertise/discuss their products on the forums, demonstrating their expertise. Using these integrated graphic representations, one can visualize the individuals' participation in both domains, making the right associations that lead to a better comprehension of the malicious hacker networks. It is helpful in determining social groups within the forums of user interaction. The presence of users on multiple markets and forums follows a power law. From Fig. 6, majority of users only belong to a single market or forum. We note that there are 751 users that are present in more than two platforms. Fig. 7 considers one such user/vendor. The vendor is active in 7 marketplaces and 1 forum . The vendor offers 82 malicious hacking related products and discusses these products on the forum. The vendor has an average rating of 4.7/5.0, rated by customers on the marketplace with more than 7000 successful transactions, indicating the reliability of the products and the popularity of the vendor.

Average

Fig. 5: Vendor/User network in marketplace and forum.

10000

Number of Users (Log scale)

1000 100 10

1
0 2 4 Number of Platforms 6 8

Fig. 6: Users in multiple markets and forums. Fig. 7: A centric network of a Vendor.

V.

R ELATED W ORK

Web crawling is a popular way of collecting large amounts of data from the Internet. In many applications, researchers

are interested in specific topics for their application. Hence, the need for a topic-based crawler popularly referred to as a focused crawler [6], [5]. Most of the focused crawlers are designed to collect information from the surface web with little concentration on the darknet websites. More recently, a focused crawler concentrating on dark web forums was designed [10]. This research primarily concentrated on forums, collecting data over a period of time and then performing static analysis to study online communities. The authors also describe different data mining techniques for these forums in [7]. We, on the other hand, not only look at darknet forums but also collect information from marketplaces hosting a range of products relating to malicious hacking. Another application of leveraging darknet information to counter human trafficking is developed by DARPA through the Memex program4 - a program with different goals than the work described in this paper. Previous work leverages the exploit information from marketplaces in a game theoretic framework to formulate system configurations that minimize the potential damage of a malicious cyber attack [19]. Work analyzing hacker forums to detect threats that pose great risk to individuals, businesses, and government have been discussed in [2]. It further states that knowledge is distributed in forums. That minimally skilled people could learn enough by simply frequenting such platforms. Studying these hacker communities gives insights in the social relationships. Also, the distribution of information amongst users in these communities based on their skill level and reputation [13], [14], [11]. These forums also serve as markets where malware and stolen personal information are shared / sold [12]. Samtani et al. analyze hacker assets in underground forums [20]. They discuss the dynamics and nature of sharing of tutorials, source code, and "attachments" (e.g. e-books, system security tools, hardware/software). Tutorials appear to be the most common way of sharing resources for malicious attacks. Source code found on these particular forums was not related to specific attacks. Additionally underground (not malicious hacking related) forums have also been analyzed to captures the dynamic trust relationships forged between mutually distrustful parties [18]. VI. C ONCLUSION

[2]

[3]

[4]

[5]

[6]

[7] [8] [9]

[10]

[11]

[12]

[13]

[14] [15]

[16]

In this paper, we implement a system for intelligence gathering related to malicious hacking. Our system is currently operational. We are in the process of transitioning this system to a commercial partner. We consider social platforms on darknet and deepnet for data collection. We address various design challenges to develop a focused crawler using data mining and machine learning techniques. The constructed database is made available to security professionals in order to identify emerging cyber-threats and capabilities.
Acknowledgments: Some of this work is supported by ONR NEPTUNE, ASU GSI, ASU ISSR and CNPq-Brazil.

[17]

[18]

[19] [20]

[21]

R EFERENCES
[1] M. Belkin and P. Niyogi. Using manifold structure for partially labelled classification. In Advances in NIPS, 2002. [22]

V. Benjamin, W. Li, T. Holt, and H. Chen. Exploring threats and vulnerabilities in hacker web: Forums, irc and carding shops. In Intelligence and Security Informatics (ISI), 2015 IEEE International Conference on, pages 85­90. IEEE, 2015. C. M. Bishop and I. Ulusoy. Object recognition via local patch labelling. In Deterministic and Statistical Methods in Machine Learning, pages 1­21, 2004. A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT' 98, pages 92­100, New York, NY, USA, 1998. ACM. S. Chakrabarti, K. Punera, and M. Subramanyam. Accelerated focused crawling through online relevance feedback. In Proceedings of the 11th international conference on World Wide Web, pages 148­159. ACM, 2002. S. Chakrabarti, M. Van den Berg, and B. Dom. Focused crawling: a new approach to topic-specific web resource discovery. Computer Networks, 31(11):1623­1640, 1999. H. Chen. Dark web: Exploring and data mining the dark side of the web, volume 30. Springer Science & Business Media, 2011. H. Cheng, Z. Liu, and J. Y. 0001. Sparsity induced similarity measure for label propagation. In ICCV, pages 317­324. IEEE, 2009. R. Dingledine, N. Mathewson, and P. Syverson. Tor: The secondgeneration onion router. In Proceedings of the 13th Conference on USENIX Security Symposium - Volume 13, SSYM'04, pages 21­21, 2004. T. Fu, A. Abbasi, and H. Chen. A focused crawler for dark web forums. Journal of the American Society for Information Science and Technology, 61(6):1213­1231, 2010. T. J. Holt. Subcultural evolution? examining the influence of on-and offline experiences on deviant subcultures. Deviant Behavior, 28(2):171­ 198, 2007. T. J. Holt and E. Lampke. Exploring stolen data markets online: products and market forces. Criminal Justice Studies, 23(1):33­50, 2010. T. J. Holt, D. Strumsky, O. Smirnova, and M. Kilger. Examining the social networks of malware writers and hackers. International Journal of Cyber Criminology, 6(1):891­903, 2012. T. Jordan and P. Taylor. A sociology of hackers. The Sociological Review, 46(4):757­780, 1998. D. Lacey and P. M. Salmon. It's dark in there: Using systems analysis to investigate trust and engagement in dark web forums. In D. Harris, editor, Engineering Psychology and Cognitive Ergonomics, volume 9174 of Lecture Notes in Computer Science, pages 117­128. Springer International Publishing, 2015. A. Levin, D. Lischinski, and Y. Weiss. A closed form solution to natural image matting. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1, CVPR '06, pages 61­68, Washington, DC, USA, 2006. IEEE Computer Society. F. Menczer, G. Pant, and P. Srinivasan. Topical web crawlers: Evaluating adaptive algorithms. ACM Transactions on Internet Technology (TOIT), 4(4):378­419, 2004. M. Motoyama, D. McCoy, K. Levchenko, S. Savage, and G. M. Voelker. An analysis of underground forums. In Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement conference, pages 71­ 80. ACM, 2011. J. Robertson, V. Paliath, J. Shakarian, A. Thart, and P. Shakarian. Data driven game theoretic cyber threat mitigation. In IAAI, 2016. S. Samtani, R. Chinn, and H. Chen. Exploring hacker assets in underground forums. In Intelligence and Security Informatics (ISI), 2015 IEEE International Conference on, pages 31­36. IEEE, 2015. C. Wang, S. Yan, L. Z. 0001, and H.-J. Zhang. Multi-label sparse coding for automatic image annotation. In CVPR, pages 1643­1650. IEEE, 2009. X. Zhu, J. Lafferty, and Z. Ghahramani. Combining active learning and semi-supervised learning using gaussian fields and harmonic functions. In ICML 2003 workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, pages 58­65, 2003.

4 http://opencatalog.darpa.mil/MEMEX.html

An Empirical Evaluation Of Social Influence
Metrics
Nikhil Kumar, Ruocheng Guo, Ashkan Aleali, Paulo Shakarian

arXiv:1607.00720v3 [cs.SI] 23 Jul 2016

Arizona State University,
Tempe, AZ
Email: {nikhilkumar, rguosni, aleali, shak}@asu.edu

Abstractâ€”Predicting when an individual will adopt a new
behavior is an important problem in application domains such
as marketing and public health. This paper examines the performance of a wide variety of social network based measurements
proposed in the literature - which have not been previously
compared directly. We study the probability of an individual
becoming influenced based on measurements derived from neighborhood (i.e. number of influencers, personal network exposure),
structural diversity, locality, temporal measures, cascade measures, and metadata. We also examine the ability to predict
influence based on choice of classifier and how the ratio of positive
to negative samples in both training and testing affect prediction
results - further enabling practical use of these concepts for social
influence applications.

I. I NTRODUCTION
Predicting when an individual will adopt a new behavior is
an important problem in application domains such as marketing [1], the spread of innovation [2], countering extremism [3],
and public health [4]. As a result, a variety of social network
based measurements have been proposed in the literature and
shown to predict how likely an individual will adopt a new
behavior given information about his immediate social ties.
However, when such measures are proposed, they are often
evaluated under different conditions - making it difficult to
understand which of these measurements should be used in
a real-world application. Further complicating the issue is
that the choice of classification algorithm and the effect of
class imbalance in both training and testing are often not
explored in most research. In our lab, we have the goal
of creating and deploying a system for counter-extremism
messaging. Hence, understanding how influence measurements
work in experimental settings that closely resemble real-world
scenarios is an important first step.
In this paper, we study measurements based on neighborhood (i.e. number of influencers [4], personal network
exposure [2]), structural diversity [5], locality [6], temporal
measures [7], cascade measures [8], and metadata [9]. We
examine the probability of an individual becoming influenced
based on these measurements (probability of adoption). We
also examine the ability to predict influence based on choice
of classifier and how the ratio of positive to negative samples in
both training and testing affect prediction results. Specifically,
we make the following contributions.

1) We review a variety of measurements used to predict
social influence and we group them in six categories
(Section III).
2) We evaluate how these measurements relate to the probability of a user being influenced using real-world microblog data (Section IV).
3) We evaluate how these measurements perform when used
as features in a machine learning approach and compare
performance across a variety of supervised machine learning approaches (Section V).
4) We evaluate how the ratio of positive to negative samples in both training and testing affect predictive results
(Section VI).
We note that contribution 4 is of particular importance, as
(particularly with microblog data) users are exposed to large
number of messages that they do not retweet (negative samples). Hence, in both training and testing, researchers can
increase the negative samples utilized by large amounts - hence
arbitrarily determining the level of class imbalance. As with
this study as a whole, the experiments on data imbalance were
to better understand these previous research results in tests that
better mimicked real-world scenarios.
Related work. Beyond the work that we shall describe
concerning the various measures for social influence we investigate in Section IV, there has been some general work
in the area of social influence that have taken approaches not
necessarily amenable to comparison. For instance, the seminal
work of Kempe et al. [10] describes two popular models for
information cascades which spawned several techniques to
learn the parameters (which also correspond to edge weights
in the graph). For example, Saito et al. [11] assigned such
probabilities based on an expectation-maximization appproach
while Goyal et al. [7] leveraged a variety of simple models
based on ideas such as a empirically-learned probabilities and
similarity measurements. See [12] for a review of some of
this work. There has also been related work on predicting cascades [13], [14], [8] which are more focused on determining
if a trend in social media exceeds a certain size. That said,
some of the ideas from these approaches, such as structural
diversity [5] are examined here (though this paper is focused
on a different problem). Other work such as Myers et al. [15]
studied the external factors influencing information diffusion,
Liu et al. [16] and Tang et al. [17] focused their studies

on topic influence. Jenders et al. [9] studied a combination
of different features including some of the metadata features
like mentions and hashtags, along with latent features like
sentiments and emotional divergence for predicting the virality
of a tweet - many of which we examine in this study as well.
Hong et al. [18] have also considered a wide spectrum of features including structural, content and temporal information.
However, their study focused more on content-based features
and not the structural features considered here - many of which
were introduced after that work.

cascades. We create the social network G from the retweeting
relationships of microblogs published between May 1, 2011
and July 31, 2011. We use the microblogs published in August
2011 to train and test our approach. Table I lists the statistics
of the dataset we used.

II. T ECHNICAL P RELIMINARIES

We found that the network derived from the dataset had
7,668,693 users with 55,381,104 edges between them. For this
network, the number of active users in August (the time period
used to study social influence) is 5,910,608 while 5,664,625 of
them have at least have one out-neighbor. During the month
of August, there were 22,182,703 retweet chains. From this
data, we removed the users who are not present in V ; we
also removed 2,660,421 empty repost chains caused by this
elimination. The dataset does not contain the repost time for
the nodes in the middle of chains. We estimated this time for
each node in the chain based on the original post time and
the final repost time. Table I lists the statistics of this dataset
during the period of study.
Among all the retweeted users we further extract the top
retweeters defined as those who had at least 100 retweets
during the period. This set of high frequency tweeters will be
used as a base for deriving the sample set for our experiments.
For each user in the above mentioned group, an occurrence of
them retweeting a post when they have an active in-neighbor
is considered as a positive instance. If any of their followees
have tweeted and they havenâ€™t retweeted, it is considered as a
negative instance.

Here we introduce the necessary notation and describe our
social network data. We represent a social network as a graph
G = (V, E) where V is the set of vertices and E is the set
of directed edges that have sizes |V |, |E| respectively. The
intuition behind edge (v, v 0 ) is that node v can influence v 0 .
This intuition stems from how we create the edges in our
network: (v, v 0 ) is an edge if during a specified time period
there is at least one microblog posted by v that is reposted
by v 0 . For node v âˆˆ V , the set of in-neighbors is denoted as
Î·vin , and the set of out-neighbors as Î·vout . We use din
v and
to
denote
the
in-degree
and
out-degree
respectively.
We
dout
v
also assume a partition over nodes that specifies a community
structure. We assume that such a partition is static (based on
the same time period from which the edges were derived) and
the function P (V ) : V â†’ C maps the set of nodes (V ) to the
set of communities (C), where C consists of k communities:
{C1 , C2 , ..., Ck }. We utilize the Louvain algorithm [19] to
identify our communities in this paper due to its ability to
scale.
Cascades. For a given microblog Î¸, we define t as the number
of time units from the initial post of Î¸ before the microblog
was reposted by one of vâ€™s incoming neighbors - intuitively
the time at which v was exposed to Î¸. We denote the subset
of nodes who originally posted or reposted Î¸ for time period
t as VÎ¸t . Likewise, the set of reposting relationships within
the same time period will be denoted by RÎ¸t . Taken together,
we have a cascade: DÎ¸t = (VÎ¸t , RÎ¸t ). Any valid original
microblog Î¸ could be treated as a unique identifier for a
cascade. Given a microblog Î¸, vÎ¸ is the originator at instance
t0Î¸ , which is defined as the origin time when the originator
posted the microblog Î¸. We denote the size of a cascade at
any particular time t as |VÎ¸t |. For v âˆˆ VÎ¸t , the set of all active
neighbors with respect to Î¸ is defined as SÎ¸v = VÎ¸t âˆ© Î·vin . We
also define the distance dtÎ¸ (v, u) as the shortest path length
between v and u in DÎ¸t .
Sina Weibo Dataset. The dataset we used was provided by the
WISE 2012 Challenge1 . It included a sample of microblogs
posted on Sina Weibo from 2009 to 2012. In this dataset, we
are provided with time and user information for each post and
the last repost in a chain which enabled us to derive a corpus of
1 http://www.wise2012.cs.ucy.ac.cy/challenge.html

#Users
5,910,608

#Edges
52,472,547

#Reposted tweets
2,238,659

#Reposted Users
394,441

TABLE I: Graph statistics

III. M EASUREMENTS TO P REDICT S OCIAL I NFLUENCE
In this section, we categorize several approaches for predicting social influence.
1) Neighborhood-based measures
2) Structural diversity measures
3) Influence locality
4) Cascade-based measures
5) Temporal measures
6) Metadata
We examine each of these categories in turn.
Neighborhood-based measures. These are the measures computed using each node and its immediate neighbors. These
measures represents the pair wise influence that the neighboring nodes exert on a given node. Retweeting from followees
is the primary mode of tweet visibility in a microblogging
site, as usually a tweet is visible to a user from its followee
subgraph. Specifically, we study the following
Î¸
â€¢ Number of active neighbors. (|Sv |) This represents
the count of active neighbors for a node v. In Damon
Centolaâ€™s notable empirical study [4], he noted that additional â€œsocial signalsâ€ â€“ or active neighbors â€“ significantly

â€¢

â€¢

increased the likelihood of an individual adopting a new
behavior.
Personal Network Exposure (PNE). (|SvÎ¸ |/din
v ) Is a
measure adopted from the social science community (i.e.
see [2] ) and has obtained recent interest (i.e. [20]). As
per [2], PNE quantifies the extent to which a person is
exposed to direct and indirect influence. This value is
defined as the ratio of number of active neighbors to total
number of neighbors. It is a measure of the fraction of
influence an active neighbor u has on v. If v has many
in-neighbors aka followees, then uâ€™s influence is diluted
and PNE represents that dilution.
Average in-neighbor count of active neighbors.
Î¸
(|Î£uâˆˆSvÎ¸ din
u |/|Sv |) This is calculated by averaging the
number of in-neighbors of each active neighbor of a node.
This defines the dilution of the influence path and is
similar to the measure, number of uninfected neighbors
as described in [14]. Other releated studies include Cha
et al. [21], where they studied the effect of a social
network userâ€™s indegree in depth, and observed that high
indegree is not necessarily correlated to influence in terms
of spawning retweets.

Structural diversity measures. This group of measurements
taken into account the structural diversity in the local neighborhood of the node - which refers to the communities present
in the neighborhood.
Ugander et al. [5] introduced structural diversity where
they studied the effect of number of connected components of
a friendship network. Fortunato et al [22] defined communities
as the set of graph vertices which are organized into groups
that seem to live fairly independently of the rest of the graph.
Weng et al. [23] used the community structure to predict the
increase in cascade size. We use the modularity maximization
method [24] for detecting communities in our dataset. The
Louvain Algorithm [19] which comes under this method is
used to derive the communities in this study due to its ability
to scale. We use two community based measures.
â€¢

â€¢

Active community count. (|P (SvÎ¸ )|) This is defined as
the number of adjacent communities of a given user v
with at least one active neighbor of v. The communities
that include active neighbors are more significant in this
context than rest of the adjacent communities. Shakarian
et al. have studied this measure in their book [12]
highlighting the importance of structural diversity.
Active community ratio (|P (SvÎ¸ )|/|P (Î·vin )|) It is calculated as the ratio of the active community count to
the total number of adjacent communities. This is similar
to the personal network exposure [2] and represents the
dilution of the effect of active community count with
respect to other neighboring communities.

Influence locality. We examine the Influence Locality model
known as LRC-Q, introduced by Zhang et al. [6]. LRC-Q
is defined by the influence locality function Q which is a
combination of peer influence factor (g) and structural factor
(f ). Peer influence factor is obtained as a linear combination

of the geometric mean of random walk probabilities of active
neighbors and structural factor as a linear combination of the
number of circles formed by the active neighbors in the ego
network of the user v. These are defined in their paper by the
following equations.
Q = w Ã— g + (1 âˆ’ w) Ã— f
s Y
(tvÎ¸ âˆ’ tvÎ¸i ) Ã— pvi
g = |SvÎ¸ |

(1)
(2)

vi âˆˆSvÎ¸
Î¸

f = a log(|SvÎ¸ | + 1) + beâˆ’Âµ|C(Sv )|

(3)

In the above equations, pvi is the random walk probability
from the active user vi to the given user v, C(Sv ) is the
collection of circles formed by the active neighbors, tvÎ¸ is the
time at which v posted or reposted the microblog Î¸, Âµ is the
decay factor and, a, b and w are balance parameters. For our
experiments we set the value of Âµ as 1 and, a, b and w to be
0.5, as per the parameter settings of [6].
Cascade-based measures.
This group of measurements take into account the various
parameters that are part of a microblog cascade. There has
been many studies in the area of predicting the cascades
including Bakshy et al. [25] , Cheng et al. [13] and more
recently Guo et al. [8]. Unlike our study, there hasnâ€™t been
many attempts to utilize the cascade parameters in predicting
retweet behavior. We study the following measures.
t
â€¢ Cascade size. (|VÎ¸ |) Cascade size is computed as the
count of people who have retweeted a particular microblog Î¸ at time t. This number is usually visible to the
microblog user and can have an impact on their retweet
behavior.
t
â€¢ Path length. (dÎ¸ (v, vÎ¸ )) Path length is the length of a
tweet trace path from the original tweeter to a given user
in the cascade. Watts et al. [26] were the first to study
the path length where they found that many social and
technological networks have small path lengths. Kwak et
al. [27] studied the path length in twitter, and Weng et
al. [23] studied a distance measure called Average step
distance which was based on the path length. Our study
focuses on the path length with respect to a particular
cascade DÎ¸t .
Temporal Measure Temporal measures were given prominence in many of the prior studies either by itself, or as a
factor in combination with other measures. Goyal et al. [7]
utilized the temporal factor and attempted to predict the time
by which an influenced user will perform an action. Hong et al.
[18] studied a variety of temporal measures and observed that
they have a stronger effect on messages with low and medium
volume of retweets, compared to highly popular messages. We
study the following temporal measure.
â€¢ Retweet Time delay. (t) This is defined as the time
delay between the original tweet and the time when
v is exposed to microblog Î¸. The time at which a

tweet was made is another piece of information which
people are exposed to while viewing a tweet. This
can affect their decision to retweet it or not. This is
one of the temporal measures studied by Hong et al. [18].
Metadata. These are simple measures derived from the metadata associated with the tweets. We consider the presence
or absence of links, mentions and hashtags as measures for
our study. Jenders et al. [9] did an extensive analysis of a
wide range of tweet and user features regarding their influence
on the spread of tweets. They considered the number of
mentions and number of hashtags among the obvious tweet
features. They observed that tweets containing both hashtags
and mentions are more likely to be retweeted than those with
out, however as the number of hashtags/mentions in a tweet
grows, the expected number of retweets decreases. In this
study we only consider their presence or absence as a measure
and do not go into any deeper analysis.
â€¢ Presence of a link (hasLink). This is a binary value
which represents whether the original tweet had a link.
Links are usually shown as part of the tweet content. The
measure of Links in tweets is similar to that of mentions
and hashtags, but has not been studied as extensively as
either in the context of social influence.
â€¢ Presence of a mention (hasMention). A binary value
which represents whether the original tweet had a mention. Intuitively, a user might be more willing to retweet
if there is a mention of him/her or someone he/she knows.
Similar to [9], Cha et al. [21] analyzed the effect of the
number of mentions and found that mentions can be an
important measure of an individual influence in the social
network.
â€¢ Presence of a hashtag (hasHashtag). A binary value
which represents whether the original tweet had hashtags.
Hashtags are also means by which tweets become visible
to users and thus are of significance in this regard. A
deeper analysis such as [9], is beyond the scope of this
work and we only focus on how the presence or absence
of a hashtag affects the retweeting behavior.
IV. S OCIAL I NFLUENCE M EASUREMENT S TUDY
Here, we examine the distribution of various measurements
which were defined in Section III. For each of those measures,
the values are put into intervals of equal sizes and the fraction
of positive samples in the interval is plotted as the probability.
The horizontal axis shows the value intervals of the measure,
while the vertical one shows the number of occurrences for
positive instances with respect to the total amount in that
particular interval. The error bar shows twice the standard
deviation of the sample. These are shown in Fig. 1 and Fig. 2.
A detailed analysis of their distribution is given below.
Neighborhood-based measures. Active neighbor count
intuitively has a positive correlation with the influence as
shown in Fig. 1(a). Fig. 1(b) shows the active neighbor count

for the lower values which also shows similar correlation. This
is consistent with the empirical study of [4]. As the number
of retweeters among in-neighbors increases, the probability of
a person retweeting the particular tweet increases. Fig. 1(c)
shows that PNE also exhibits positive correlation like active
neighbor count. This shows the significance of PNE measure
as demonstrated by other studies such as [2] and [20].
Average in-neighbor count of Active Neighbors does not
show a clear correlation in its distribution as seen in Fig. 1(d).
Structural diversity measures. Number of active
communities shows a good positive correlation with the
retweet behavior. This result is consistent with the related
studies such as [23] and [13]. Active community ratio
also demonstrates a reasonable correlation with the positive
instances as this measure represents the dilution of community
influence based on the total number of adjacent communities.
Cascade-based measures. Intuitively, cascade size is an
important influencer in retweet behavior. If a tweet is
reasonably popular it tends to attract further retweets. The
same is revealed from the distribution in Fig. 2(c). This is
consistent with the research of [25] and [13] although they
studied a different problem. The intuition for path length is
that, as the distance from the original tweeter increases a user
is less interested in retweeting the tweet. Our results show
(Fig. 2(d)) that this intuition holds between path length 1 and
2. But, for the remaining intervals, results doesnâ€™t correlate
well. This can be explained by comparing to the results
of [9] where they found similiar pattern while analyzing
mentions and hashtags. Further, the results of [13] indicate
that information cascade depth is related to popularity. Hence,
the microblogs that are far from the original poster may be
inherently popular as the information cascade has proceeded
to a larger depth.
Temporal. Fig. 2(e) shows that retweet time delay has slight
inverse correlation with the influence. Intuitively, the influence
of a tweet decays with time, and as people are exposed to
date/time information in the social network they are less
likely to retweet old tweets. This decay factor has been used
in works like [7], [6] etc. and above result shows the same.
Metadata. Table II shows the conditional probability of
positive instances given the meta measure value of 0 and 1,
respectively. The values from the table shows that presence or
absence of a link doesnâ€™t seem to have much correlation with
the influence. It also shows that, the presence of mentions
seem have slight negative correlation to influence though
there is no actual intuition to base this on. But, this can be
explained by the observation in the paper [9] that as the
number of mentions in a tweet grows, the expected number
of retweets decreases. The presence of hashtag shows an
interesting correlation in Table II. This is consistent with the
study of [9] and illustrates the significance of hashtags in
enhancing the visibility of the tweet and motivating a user to

P (yi = pos |Vi = 1)
0.48
0.45
0.66

1.0
0.8

Probability

P (yi = pos |Vi = 0)
0.51
0.51
0.50

1.0
0.8
0.6
0.4
0.2
0.0

~ is a column of the design matrix corresponding
TABLE II: V
to a certain binary feature, pos represents positive label and i
is the index of the sample.

0

1

2

3

4

5

Active community count
(a)

(b)
1.0

0.2
0.0

0.4
0.2

Active neighbor count

(b)

1.0

0.2

0.2

Path length

(c)

Probability

0.6
0.4
0.2

(d)

0.6
0.4
0.2
0.0

10 99
0
20 0-19
00 99
30 -29
0 99
40 0-39
00 99
50 -49
0 99
60 0-59
00 99
70 -69
0 99
80 0-79
0 99
90 0-89
9
10 00-99 9
00 99
0-1
09
99

0-9

.6)

.5)

[0.

5-0

.4)

[0.

4-0

.3)

[0.

3-0

.2)

[0.

2-0

.1)

1-0

[0.

0-0
[0.

PNE

0 1 2 3 4 5 6 7 8 9

1.0

0.0

0.0

0.2
0.0

Avg. in-neighbor count

010 99
0-1
20 99
0
30 -299
0
40 -399
0
50 -499
0
60 -599
0-6
70 99
0
80 -799
0-8
90 99
0-9
99

Probability

0.4

0.4

0.8

0.8

0.6

0.6

Cascade size

1.0

0.8

Probability

0 1 2 3 4 5 6 7 8 9

Active neighbor count (lower values)

(a)

0.4
0.0

0.0

0-9 10-19 20-29 30-39 40-49 50-59

0.6

040 399
0
80 -799
0
12 -119
0
16 0-15 9
0 9
20 0-19 9
0 9
24 0-23 9
0 9
28 0-27 9
0 9
32 0-31 9
0 9
36 0-35 9
0 9
40 0-39 9
00 99
-43
99

0.4

0.6

0.8

0.8

Probability

0.8

Probability

Probability

0.8
0.6

0.2

Active community ratio

Probability

1.0

0.4
0.0

1.0

1.0

0.6

[0.
0-0
[0. .1)
1-0
[0. .2)
2-0
[0. .3)
3-0
[0. .4)
4-0
[0. .5)
5-0
[0. .6)
6-0
[0. .7)
7-0
.8)

~
V
hasLink
hasMention
hasHashtag

Probability

retweet them.

Time Delay
(e)

(c)

(d)

Fig. 1: Plots of Neighborhood and temporal measures. Error
bars represent two standard deviations.
V. I NFLUENCE P REDICTION
A. Methods
We derive our graph G from the dataset as described
under Section II. We use the microblogs published in August
2011 to extract the instances to train and test our approach.
Positive and negative instances are extracted as described in
Section II, and the measures described in Section III were
extracted as features for each of them. This set is used to
obtain a random sample with 1:1 negative to positive ratio,
which we will use for the classification experiments.
Classification experiments Here we examine our experiments
for predicting whether a user under given conditions will
retweet or not. As this is a binary classification task we report
the performance measurements (precision, recall and unbiased
F1) for only the positive (retweeting) class. We also examine
the classification performances of various learning algorithms.
For each of the experiments we use a training to test set ratio of
70:30 and used a 10 fold cross validation. We use the following
classification algorithms for our experiment.

Fig. 2: Plots of Structural Diversity and Cascade-based measures. Error bars represent two standard deviations.

Random Forest (RF). Random Forest [28] is a popular
ensemble method used for classification and regression.
Ensemble methods use multiple classifier algorithms to obtain
better accuracy than that could be obtained using any of the
individual classifiers. We use random forest algorithm with
bootstrap aggregating, that fits a number of decision trees
on different sub-samples of the dataset. Each decision tree
provides its own predictions which are then merged obtain a
better accuracy.
AdaBoost Classifier (AB). The AdaBoost algorithm [29]
proposed by Yoav Freund and Robert Schapire is one of the
most important ensemble methods. It is prominent among the
boosting techniques [29] which are used in conjuction with
other learning algorithms. In this method, the weak learners
are combined into a final sum representing the boosted
output. We use the particular algorithm called AdaBoostSAMME [30] and use the decision trees as the base estimator.
Logistic Regression (LR). Logistic regression is a generalized
linear model which uses a logistic function to infer the

relationship between a dependent variable and one or more
independent variables. We utilizes the binomial logistic
regression which predicts the probability that an observation
falls into one of the two categories. Logistic regression has
low varience and is less prone to overfitting.
Naive Bayes Classifier (NB). Naive Bayes is a probabilistic
classifier which is based on applying Bayesâ€™ theorem with
independence assumption between every feature pairs. Naive
Bayes classifiers are highly scalable and less prone to the
curse of dimensionality, making it one of the top machine
learning algorithms. We implement the Gaussian Naive Bayes
algorithm for classification where the likelihood of the features
is assumed to be Gaussian.

1.0

Precision

Recall

F1

0.8
0.6
0.4
0.2
0.0

Nbr

Structural Cascade Temporal

Meta

LRC-Q Multi-Meas

(a)

1.0

Precision

Recall

F1

0.8
0.6

B. Measurement Group Comparison

0.4

Here we compare the classification performance of the
various measurement groups described in Section III. Fig. 3
shows the behavior of different feature groups using multiple
classifier algorithms, which provides a better understanding of
this all-important component in a deployed system. Generally
Random Forest provides the best performance among all
the classifier algorithms. Neighborhood-based (Nbr) measures
perform quite well in Random Forest, AdaBoost and Logistic
regression. This is consistent with what we discussed in Section IV. Structural diversity measures show less performance
compared to other groups. This can be attributed to the fact that
it is not often used independently in classification, and usually
this group performs well in conjunction with other measures
such as Neighborhood-based. LRC-Q gives performance measure comparable to the results in [6]. Cascade-based measures
are observed to perform reasonably well in Random Forest,
Logistic Regression and AdaBoost. This once again illustrates
the significance of cascade size and brings into focus the
path length measure. Temporal measure performs well in all
classifiers except Naive Bayes. Although time based measures
are frequently used as a decay factor in conjunction with other
measures ([7], [6]), our results show that it could yield high
predictive power by itself. Metadata measures show good and
consistent performance across all classifiers. As research by
[9] shows, hashtag and mentions have high predictive power
with respect to retweet behaviour and our results confirm the
significance of this measure along with the hasLinks measure.
With an eye toward a deployed system, we also examine
a â€œMulti-Measurement modelâ€ which is a combination of
Neighborhood, Structural, Cascade, Temporal and Metadata
measures. The Multi-Measurement model shows better performance than individual groups generally among Random
Forest, Logistic Regression and AdaBoost classifiers. The
other measures such as neighborhood-based, temporal and
LRC-Q perform reasonably well compared to rest of the individual future groups. The performance of Multi-Measurement
model shows real value in combining the various features and
individual feature groups to improve our ability to predict
retweet behavior in real world datasets.

0.2
0.0

Nbr

Structural Cascade Temporal

Meta

LRC-Q Multi-Meas

(b)

1.0

Precision

Recall

F1

0.8
0.6
0.4
0.2
0.0

Nbr

Structural Cascade Temporal

Meta

LRC-Q Multi-Meas

Meta

LRC-Q Multi-Meas

(c)

1.0

Precision

Recall

F1

0.8
0.6
0.4
0.2
0.0

Nbr

Structural Cascade Temporal
(d)

Fig. 3: Performance with different classifier algorithms. a)
Random Forest b) Logistic Regression c) Naive Bayes d)
AdaBoost.

C. Multi-Measurement Model Compared to Influence Locality
We compare our results with the LRC-Q model described in
[6]. We experimented with multiple classification algorithms
for this task and the best results were obtained using Random
Forest classifier. The results obtained using Random Forest
(RF), Logistic Regression (LR), Naive Bayes (NB) and
AdaBoost (AB) are shown in the Table III. As LRC-Q uses

Precision
0.679
0.95
0.794
0.602
0.764

Recall
0.573
0.947
0.765
0.704
0.285

F1
0.622
0.948
0.784
0.649
0.415

TABLE III: Performance of retweet behavior prediction
VI. VARYING N EGATIVE TO P OSITIVE RATIO
An important question when deploying the aforementioned
methods in a real-world application is how to best train the
model to cope with data imbalance observed in-practice. As
individuals are exposed to an arbitrarily large number of
microblogs that they do not rebroadcast, this is a difficult and unfortunately relatively unstudied problem. Here, we conducted experiments to analyse how classification performance
varies with different negative to positive ratio in both training
and test set. The surface and linear plots in Fig. 4 show the
precision, recall and F1 values obtained using Random Forest
classifier, when negative to positive ratio is varied from 1:1
to 9:1. The ratio was varied in both training set and test
set to observe the effects on overall performance. Precision
is observed to decrease as we increase the size of negative
samples in test set while keeping the ratio in training set
constant. Recall is observed to remain the same with changing
ratio in test set. Change in negative to positive ratio in training
set on the hand, shows slight increase in precision where as
recall decreases. Results for LRC-Q follows a similar pattern
except for the convergence of recall for increased imbalance in

Precision

1.00
0.95
0.90
0.85
0.80
0.75
0.70
N to 9
P tr8a765
0.65
inin432 1 9 8 7 6 5 4 3 2 1
gs
P test set

et

Precision

1
2

N to

N to P for training
3
5
7
4
6
8

9

1.00
0.95
0.90
0.85
0.80
0.75
0.70
0.65
0.60
1 2 3 4 5 6 7 8 9
Negative to positive ratio in test set

(a)

(b)

Recall

0.95
0.90
0.85
0.80
0.75
0.70
0.65
89

1
2

6 7 set
1 2
4 5 test
N to3P tr4 5 6
3
P
aining s7 8
2
et 9 1 N to

N to P for training
3
5
7
4
6
8

9

1.00
0.95
0.90
0.85
0.80
0.75
0.70
0.65
0.60
1 2 3 4 5 6 7 8 9
Negative to positive ratio in test set

(c)

(d)
1
2

0.95
0.90

F10.85

F1

Model
LRC-Q (LR)
Multi-Meas (RF)
Multi-Meas (AB)
Multi-Meas (LR)
Multi-Meas (NB)

training set. From these results, it can be generally observed
that 1:1 is the ideal ratio of negative to positive samples in
training set for an unknown imbalance in test data.

Recall

only a single feature, we only use Logistic Regression for
its evaluation. It can be observed that Multi-Measurement
model outperforms the LRC-Q model in all classifiers except
for Naive Bayes. This can be attributed to the fact that
while LRC-Q takes into account pairwise and structural
influence along with time decay, Multi-Measurement model
incorporates more parameters in addition to the above.
LRC-Q has combined the pairwise and structural factor
into a single feature and uses time measure as a decay
factor. The Multi-Measurement model on the other hand
treats them individually, along with including different
kinds of pairwise influence (such as active neighbor count,
personal network exposure and average in-neighbors of active
neighbors), considering both direct as well as ratio based
measures for structural diversity, and using temporal measure
as an independent feature. In addition to that, this model
also includes cascade and metadata based features giving
it a broader view of the parameters that can influence an
individualâ€™s retweeting behavior. This demonstrates that in
any attempt of retweet prediction, a broader approach is
required, which incorporates multiple measures that are are
closely related (within the measurement groups) and those
that are mutually exclusive (across groups) to obtain the best
prediction in classification.

0.80
1 et
0.75
4 3 2 ng s
1 2 3 4 5 6
7 6 5 aini
N to P test set 7 8 9 9 t8o P tr

N

N to P for training
3
5
7
4
6
8

9

1.00
0.95
0.90
0.85
0.80
0.75
0.70
1 2 3 4 5 6 7 8 9
Negative to positive ratio in test set

(e)

(f)

Fig. 4: Plots for classification on imbalanced data for MultiMeasurement model using Random Forest. a) Precision surface plot b) Precision line plot c) Recall surface plot d) Recall
line plot e) F1 surface plot f) F1 line plot.

VII. C ONCLUSION
In this paper, we examines the performance of a wide
variety of social network based measurements and study the
probability of an individual becoming influenced based on
them. In this study, we grouped those measures under various
measurement groups to understand their group wise predictive
power. We designed these experiments so that they would
move beyond standard research-based experiments used to
evaluate an idea - we designed these experiments to understand
how well these ideas can be used in a deployed system. We
look to use these results in a system that we intend to deploy
or license for real-world influence operations such as counterextremism.

Precision

N to 9
P tr8a765
inin432 1 9 8 7 6 5 4 3 2 1
g se
N to P test set
t

Precision

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

1
2

9

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 2 3 4 5 6 7 8 9
Negative to positive ratio in test set

(a)

(b)

Recall

0.6
0.5
0.4
0.3
0.2
0.1
0.0
9
8

Recall

1
2

6 7 set
1 2
4 5 test
N to3P tr4 5 6
3
P
aining s7 8
2
et 9 1 N to

N to P for training
3
5
7
4
6
8

9

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 2 3 4 5 6 7 8 9
Negative to positive ratio in test set

(c)

(d)

F1

1
2

0.7
0.6
0.5
F1 0.4
0.3
0.2
0.1
0.0

N to P for training
3
5
7
4
6
8

1 et
4 3 2 ng s
1 2 3 4 5 6
7 6 5 aini
N to P test set 7 8 9 9 t8o P tr

N

N to P for training
3
5
7
4
6
8

9

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 2 3 4 5 6 7 8 9
Negative to positive ratio in test set

(e)

(f)

Fig. 5: Plots for classification on imbalanced data for LRCQ using Logistic Regression. a) Precision surface plot b)
Precision line plot c) Recall surface plot d) Recall line plot e)
F1 surface plot f) F1 line plot.

ACKNOWLEDGMENTS
Some of the authors are supported through the AFOSR
Young Investigator Program (YIP) grant FA9550-15-1-0159,
ARO grant W911NF-15-1-0282, the DoD Minerva program
grant N00014-16-1-2015 and the EU RISE program.
R EFERENCES
[1] D. Watts and J. Peretti, â€œViral marketing for the real world,â€ Harvard
Business Review, May 2007.
[2] T. W. Valente, Network models of the diffusion of innovations, ser.
Quantitative methods in communication.
Cresskill, N.J.: Hampton
Press, 1995, thomas W. Valente. Includes bibliographical references (p.
153-163) and indexes.
[3] S. Al-khateeb and N. Agarwal, â€œExamining botnet behaviors for propaganda dissemination: A case study of isilâ€™s beheading videos-based
propaganda,â€ in ICDM Workshops. IEEE, 2015, pp. 51â€“57.
[4] D. Centola, â€œThe Spread of Behavior in an Online Social Network
Experiment,â€ Science, vol. 329, no. 5996, pp. 1194â€“1197, Sep. 2010.
[5] J. Ugander, L. Backstrom, C. Marlow, and J. Kleinberg, â€œStructural
diversity in social contagion,â€ Proceedings of the National Academy
of Sciences, vol. 109, no. 16, pp. 5962â€“5966, 2012.

[6] J. Zhang, B. Liu, J. Tang, T. Chen, and J. Li, â€œSocial influence locality
for modeling retweeting behaviors.â€ in IJCAI, vol. 13, 2013, pp. 2761â€“
2767.
[7] A. Goyal, F. Bonchi, and L. V. Lakshmanan, â€œLearning influence
probabilities in social networks,â€ in Proceedings of the third ACM
international conference on Web search and data mining. ACM, 2010,
pp. 241â€“250.
[8] R. Guo, E. Shaabani, A. Bhatnagar, and P. Shakarian, â€œToward order-ofmagnitude cascade prediction,â€ in Proceedings of the 2015 IEEE/ACM
International Conference on Advances in Social Networks Analysis and
Mining 2015. ACM, 2015, pp. 1610â€“1613.
[9] M. Jenders, G. Kasneci, and F. Naumann, â€œAnalyzing and predicting
viral tweets,â€ in Proceedings of the 22nd international conference
on World Wide Web companion.
International World Wide Web
Conferences Steering Committee, 2013, pp. 657â€“664.
[10] D. Kempe, J. Kleinberg, and EÌ. Tardos, â€œMaximizing the spread of
influence through a social network,â€ in Proceedings of the ninth ACM
SIGKDD international conference on Knowledge discovery and data
mining. ACM, 2003, pp. 137â€“146.
[11] K. Saito, R. Nakano, and M. Kimura, â€œPrediction of information
diffusion probabilities for independent cascade model,â€ in Knowledgebased intelligent information and engineering systems. Springer, 2008,
pp. 67â€“75.
[12] P. Shakarian, A. Bhatnagar, A. Aleali, R. Guo, and E. Shaabani,
Diffusion in Social Networks. Springer, 2015.
[13] J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec,
â€œCan cascades be predicted?â€ in Proceedings of the 23rd international
conference on World wide web. ACM, 2014, pp. 925â€“936.
[14] L. Weng, F. Menczer, and Y.-Y. Ahn, â€œVirality prediction and community
structure in social networks,â€ Scientific reports, vol. 3, 2013.
[15] S. A. Myers, C. Zhu, and J. Leskovec, â€œInformation diffusion and
external influence in networks,â€ in Proceedings of the 18th ACM
SIGKDD international conference on Knowledge discovery and data
mining. ACM, 2012, pp. 33â€“41.
[16] L. Liu, J. Tang, J. Han, M. Jiang, and S. Yang, â€œMining topic-level
influence in heterogeneous networks,â€ in Proceedings of the 19th ACM
international conference on Information and knowledge management.
ACM, 2010, pp. 199â€“208.
[17] J. Tang, J. Sun, C. Wang, and Z. Yang, â€œSocial influence analysis
in large-scale networks,â€ in Proceedings of the 15th ACM SIGKDD
international conference on Knowledge discovery and data mining.
ACM, 2009, pp. 807â€“816.
[18] L. Hong, O. Dan, and B. D. Davison, â€œPredicting popular messages in
twitter,â€ in Proceedings of the 20th international conference companion
on World wide web. ACM, 2011, pp. 57â€“58.
[19] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, â€œFast
unfolding of communities in large networks,â€ Journal of Statistical
Mechanics: Theory and Experiment, vol. 2008, no. 10, p. P10008, 2008.
[20] A. Halavais, K. H. Kwon, S. Havener, and J. Striker, â€œBadges of
friendship: Social influence and badge acquisition on stack overflow,â€
in 2014 47th Hawaii International Conference on System Sciences, Jan
2014, pp. 1607â€“1615.
[21] M. Cha, H. Haddadi, F. Benevenuto, and P. K. Gummadi, â€œMeasuring
user influence in twitter: The million follower fallacy.â€ ICWSM, vol. 10,
no. 10-17, p. 30, 2010.
[22] S. Fortunato, â€œCommunity detection in graphs,â€ Physics reports, vol.
486, no. 3, pp. 75â€“174, 2010.
[23] L. Weng, F. Menczer, and Y.-Y. Ahn, â€œPredicting successful memes
using network and community structure,â€ in Eighth International AAAI
Conference on Weblogs and Social Media, 2014.
[24] M. Chen, K. Kuzmin, and B. K. Szymanski, â€œCommunity detection
via maximization of modularity and its variants,â€ Computational Social
Systems, IEEE Transactions on, vol. 1, no. 1, pp. 46â€“65, 2014.
[25] E. Bakshy, J. M. Hofman, W. A. Mason, and D. J. Watts, â€œEveryoneâ€™s
an influencer: quantifying influence on twitter,â€ in Proceedings of the
fourth ACM international conference on Web search and data mining.
ACM, 2011, pp. 65â€“74.
[26] D. J. Watts and S. H. Strogatz, â€œCollective dynamics of smallworldnetworks,â€ nature, vol. 393, no. 6684, pp. 440â€“442, 1998.
[27] H. Kwak, C. Lee, H. Park, and S. Moon, â€œWhat is twitter, a social
network or a news media?â€ in Proceedings of the 19th international
conference on World wide web. ACM, 2010, pp. 591â€“600.
[28] L. Breiman, â€œRandom forests,â€ Machine learning, vol. 45, no. 1, pp.
5â€“32, 2001.

[29] Y. Freund, R. Schapire, and N. Abe, â€œA short introduction to boosting,â€
Journal-Japanese Society For Artificial Intelligence, vol. 14, no. 771780, p. 1612, 1999.
[30] J. Zhu, H. Zou, S. Rosset, and T. Hastie, â€œMulti-class adaboost,â€
Statistics and its Interface, vol. 2, no. 3, pp. 349â€“360, 2009.

Data Driven Game Theoretic Cyber Threat Mitigation

Penetration testing is regarded as the gold-standard for understanding how well an organization can withstand sophisticated cyber-attacks. However, the recent prevalence of markets specializing in zero-day exploits on the darknet make exploits widely available to potential attackers. The cost associated with these sophisticated kits generally precludes penetration testers from simply obtaining such exploits á‘› so an alternative approach is needed to understand what exploits an attacker will most likely purchase and how to defend against them. In this paper, we introduce a data-driven security game framework to model an attacker and provide policy recommendations to the defender. In addition to providing a formal framework and algorithms to develop strategies, we present experimental results from applying our framework, for various system configurations, on real-world exploit market data actively mined from the dark-net.

Modeling cyber-attacks on industrial control systems
:
Despite the prevalence of markets for malware and exploits and their potential threat to industrial control systems (ICS), existing paradigms for modeling of such cyber-adversarial behavior do not account for the complex nature of ICS systems consisting of multiple interconnected components. This paper takes the first steps toward addressing this need. Here, we introduce a framework that allows for modeling of ICS systems with highly interconnected components and study this model through the lens of lattice theory. We then turn our attention to the problem of determining the optimal/most dangerous for a cyber-adversary with respect to this model and find it to be an NP-Complete problem. To address this complexity, we utilize an A*-based approach and develop admissible heuristics. We provide an implementation and show through a suite of experiments using both simulated and actual vulnerability data that this method performs well in practice for identifying adversarial courses of action in this domain.

Exploring Malicious Hacker Forums
For consumers the increasingly widespread consumer-grade connected (â€œsmartâ€) devices; growing use of cloud-storage and globally still expanding use of Internet and mobile phones; mobile payment options will pose increasing risk of becoming a victim of cyber-attack. For companies and institutions of all kinds, matters regarding the protection of Intellectual Property (IP) and Personally Identifiable Information (PII) from cyber-breaches and -leaks will demand higher financial investment. With the discovery of Stuxnet, offensive and defensive cyber-capabilities have already become an acknowledged tool in military arsenals worldwide and are at the cusp of shifting the global landscape of military power. With the expanding yield of cyber-related activities, understanding the actors creating, manipulating, and distributing malicious code becomes a paramount task. In this chapter we report on the results of an exploration of black hat hacker forums on both the Internet and crypto-networks (in particular those accessed via the Tor-browser). We report on the structure, content, and standards of behavior within these forums. Throughout we highlight how these activity augment the activities of the black hat hackers who participate.

A Comparison of Methods for Cascade Prediction
Ruocheng Guo, Paulo Shakarian

arXiv:1606.05730v1 [cs.SI] 18 Jun 2016

Arizona State University
Tempe, AZ
Email: {rguosni,shak}@asu.edu

Abstractâ€” Information cascades exist in a wide variety of
platforms on Internet. A very important real-world problem is
to identify which information cascades can â€œgo viralâ€. A system
addressing this problem can be used in a variety of applications
including public health, marketing and counter-terrorism. As a
cascade can be considered as compound of the social network
and the time series. However, in related literature where methods
for solving the cascade prediction problem were proposed, the
experimental settings were often limited to only a single metric
for a specific problem formulation. Moreover, little attention was
paid to the run time of those methods. In this paper, we first
formulate the cascade prediction problem as both classification
and regression. Then we compare three categories of cascade
prediction methods: centrality based, feature based and point
process based. We carry out the comparison through evaluation
of the methods by both accuracy metrics and run time. The
results show that feature based methods can outperform others
in terms of prediction accuracy but suffer from heavy overhead
especially for large datasets. While point process based methods
can also run into issue of long run time when the model can not
well adapt to the data. This paper seeks to address issues in order
to allow developers of systems for social network analysis to select
the most appropriate method for predicting viral information
cascades.

I. I NTRODUCTION
Identifying when a piece of information goes â€œviralâ€ in social media is an important problem in social network analysis.
This is often referred to as â€œcascade predictionâ€. Recently,
the cascade prediction problem attracted considerable attention from researchers from communities of machine learning,
data mining and statistics. Researchers attempted to predict
the final size of information cascades based on approaches
inspired by knowledge in various areas. Pei et al. [1] measured
influence of the root node by k-shell number and related
heuristics. Weng et al. [2] and Guo et al. [3] uitilized features
describing both structural and temporal properties of earlystage cascades. The work described in [4] and [5] modelled
cascades by one-dimensional point process. However, in this
line of research, the experimental settings varied from paper
to paper. Furthermore, as the cascade prediction problem
can be treated as either classification or regression, most of
previous work only dealt with one or the other and using
just a single evaluation metric.With deployment of a counterextremism messaging system (i.e. an enhanced version of [6])
as one of the primary goals in our group, cascade prediction
can play a crucial role in detection of early-stage extremism
message that is potential to go viral on social network sites.
Other applications include the spread of information following
a disaster, promotion of health behaviors and applications

to marketing. Therefore, it is important to understand how
well the existing methods stemming from different research
area could perform in near real-world experimental settings.
An ideal cascade prediction method for counter-extremism
messaging system should provide acceptable accuracy with
ability to make near real-time prediction.
In this paper, we compare performance of a variety of
cascade prediction methods originating from different research
areas as both classification and regression problems with
multiple evaluation metrics. We also measure the run time
of the tasks required by the methods to complete cascade
prediction â€“ another key deployment concern not explored in
most research.
In this paper, the main contribution can be summarized as:
â€¢ We compare cascade prediction methods in three categories: centrality based, feature based and point process
based, therefore providing comparison between methods
orginating from different research areas.
â€¢ The cascade prediction problem is considered from both
the aspect of regression and classification. we also conduct a comprehensive comparison between methods by
various evaluation metrics.
â€¢ We also compare the run time of tasks needed for the
cascade prediction methods are also measured in a task
by task style.
The rest of this paper is organized as the follows: In Section II,
definitions relevant to the methods considered in this paper are
introduced along with a formal problem statement of cascade
prediction. Section III summarizes the mechanism of the three
categories of cascade prediction methods. Section IV and V
presents the setup of experiments and performance of each
method in terms of both accuracy and run time. Section VI
reviews related work. At last, Section VII concludes the paper
and discusses the main issues of these methods.
II. T ECHNICAL P RELIMINARIES
In this section, related concepts for the three categories of
methods are defined. Then we formulate the cascade prediction
problem as regression and classification respectively.
A. Definitions
Network and Cascade: The social network is a directed
graph G = (V, E) where each node v âˆˆ V represents a
user and each edge eij = (vi , vj ) denotes that user vi is
followed by user vj . Identified by the original message or the
corresponding hashtag, a cascade is a time-variant subgraph of

the social network d(t) = (V (t), E(t)). Each node v âˆˆ V (t)
denotes a user reposted the original message of cascade d(t)
(for the Aminer dataset [7]) or a user posted the hashtag
defining cascade d(t) (for the Twitter dataset [2]) within time
t. The time variable t denotes number of time units since the
microblog including the original message or the hashtag. For
each node v âˆˆ V (t) we record their adoption time of cascade
d(t) as tv . For v âˆˆ V (t), tv â‰¤ t while for v 6âˆˆ V (t) we define
tv = âˆž. Thus we can get an ascendingly sorted vector tv (t)
including all tv â‰¤ t for each cascade, which plays an important
role in both feature based methods and point process based
methods for cascade prediction. The kth element of tv (t) can
be denoted as tv (t)[k]. For convenience, we use tend to denote
the time when the last adoption of a cascade happened.
Besides the cascade d(t) = (V (t), E(t)), the neighborhood
of V (t) also can provide information about the potential of
the cascade. Here we define the out-neighborhood reachable
by any node in V (t) in step i as ith surface Fi (t). To show
how â€™freshâ€™ the cascade is for a node v âˆˆ Fi (t), we define a
function fâˆ†t : v â†’ âˆ†t that maps such a node to the number of
time units since v become a member of first surface to current
time t. As time makes a big difference in social influence and
diffusion, we divide the first surface F1 (t) into two sets of
nodes depends on fâˆ†t (v) for all v âˆˆ F1 (t). With a selected
threshold tÎ» . The first set named as frontiers includes all
nodes v âˆˆ F1 (t) such that fâˆ†t (v) â‰¤ tÎ» and the other set nonadopters consists the other nodes v âˆˆ F1 (t) with fâˆ†t (v) > tÎ» .
In this paper, |x| denotes absolute value of scaler x and |x|
denotes cardinality of set x.
Communities: We can treat a community partition of a social
network as a function fC : V â†’ C which maps a set of
nodes V to a set of communities C. With this function, given
a cascade d(t) = (V (t), E(t)), it enables us to describe the
distribution of nodes over communities by features such as
|fC (V )|, the number of communities among set V .
Point Process: Each adoption in a cascade can be represented
as an event from the aspect of point process as in [4]. Thus,
for cascade prediction, we can use tv (t âˆ’ âˆ†t) to describe the
history of a point process strictly before t. The core of a point
process is the conditional density function Î»(t). Conditioned
on tv (t âˆ’ âˆ†t), the conditional density is the limit of expected
number of adoptions would happen in time interval [t, t + âˆ†t]
by taking âˆ†t â†’ 0+ :
Î»(t) = lim E {|V (t + âˆ†t)| âˆ’ |V (t)|}
âˆ†tâ†’0+

(1)

Given the density function Î»(t) and target prediction time
t0 , the predicted cascade size can be computed by:
Z t0
Ë†
0
|V (t )| = |V (t)| +
Î»(Ï„ )dÏ„
(2)
t

B. Problem Statement
In this paper, we focus on comparison of different methods
which can solve the cascade prediction problem. This problem can be formulated as either a regression problem or a
classification problem:

Regression Problem: Given a early stage cascade d(t) =
(V (t), E(t)) and the corresponding node attribute vector tv (t)
with constraint |V (t)| = n, the target is to predict the final
size of the cascade |V (tend )|.
Classification Problem: A threshold T hres is selected to
label each cascade. For a given cascade if its |V (tend )| â‰¥
T hres, we define it as a viral sample labeled as 1, otherwise,
we label it as non-viral labeled as 0. Then the problem is to
classify a given early-stage cascade d(t) to the viral class or
the non-viral class.
III. M ETHODS
In this section we introduce several recently published
methods for solving the cascade prediction problem. Diffusion
process in social network includes information of time series,
network structure, sometimes with microblog content and node
attributes, therefore, methods originated from knowledge in
various research area like social network analysis, random
point process and non-linear programming can be applied. The
methods can be categorized into: centrality based methods,
feature based methods and point process based methods.
A. Centrality Based Methods
Previous work [1] discovered that the k-shell value of a node
is highly correlated to the average cascade size it initiates.
In this paper, we also consider eigenvector centrality, outdegree and Pagerank of the root node of cascades to deal with
the cascade prediction problem. We refer to centrality based
approaches as method C in this paper.
B. Feature Based Methods
In this paper, we consider two recently proposed methods
[3] and [2] and call them method A and method B respectively
for convenience. The features computed by the two methods
can be categorized into network features, community based
features and temporal features.
Both of the feature based methods require to take advantage
of community detection algorithms. Given the social network,
community detection algorithms such as [8] and [9] can
be applied to it and assign each node to one or multiple
communities. Based on the communities detected, features can
be computed to numerically describe how the nodes that participate in a cascade are distributed over communities. Thus,
we can quantitatively measure structural diversity from [10]
or influence locality from [7] as features.
Network Features: In method B proposed by [2], the authors
consider several types of network features:
â€¢ Neighborhood size, including first surface (|F1 (Vt )|) and
second surface (|F2 (Vt )|).
â€¢ Path length, consisting average step distance and coefficient of variation of it, and diameter of the cascade.
Step distance is the length of shortest path between two
consecutive adopters vi and vi+1 .
Where coefficient of variation is defined as the ratio of the
standard deviation to the mean.

Community Based Features: In both [3] and [2], community
features are extracted and contribute to the predictive methods.
â€¢ Community features for adopters, including the number
of communities (|fC (V (t))|), entropy and gini entropy.
â€¢ Community features for frontiers and non-adopters, including the number of communities (|fC (F1 (t))|), entropy and gini entropy.
â€¢ The number of shared communties between any two
groups of adopters, frontiers and non-adopters.
Temporal Features: In [3], the authors computed average of
tv (t) while average step time and its corresponding coefficient
of variation are calculated in [2] as two features.
C. Point Process Based Methods
To discover patterns in the temporal dynamics of cascades,
authors of both [5] and [4] both consider a cascade as an
instance of one-dimensional point process in time space. They
proposed novel density functions to characterize time series
of cascades. The two methods are quite similar, in terms of
the formulation of conditional density function Î»(t). In both
cases, Î»(t) consists of an element modeling the popularity of
the cascade and another describing the probablity distribution
of an adoption behavior over time.
The Reinforced Poisson Process (RPP) Method: In [5], the
authors consider the density function for a cascade d as a
product of three elements:
Î»d (t) = Î±d fd (t; Î¸d ) |V (t)|

(3)

For cascade d, Î±d denotes the intrinsic attractiveness, fd (t; Î¸d )
is defined as the relaxation function which models how likely
an adoption would happen at time t without considering Î±d
and |V (t)|. For each cascade d, parameters Î±d and Î¸d are
learned by maximization of the likelihood of tv (t). Thus, the
predicted cascade size at time t0 > t can be computed by:
Z t0
|VÌ‚ (t0 )| = |V (t)| +
Î±d fd (Ï„ ; Î¸d ) |V (Ï„ )| dÏ„
(4)
t

The SEISMIC Method: In [4], authors model the density
function as a modified Hawkes Process made up of three elements: infectiousness pt , node degree ni and human reaction
time distribution Ï†(s):
|V (t)|

Î»(t) = pt

X

ni Ï†(t âˆ’ tvi )

(5)

i=1

Where tvi âˆˆ tv (t) is the time when each adoption happens.
Similar to Î±d in the Reinforced Poisson Process model, pt is
computed by maximization of the likelihood function:
|V (t)|âˆ’1

pt = arg max
pt

Y

Î»(tvi ) expâˆ’

R tvi+1
tvi

Î»(Ï„ )dÏ„

(6)

i=0

While the human reaction time distribution Ï†(s) is formulated
as a piece-wise function consists of a constant piece and a
power-law piece with parameter c and Î¸:
(
c
s â‰¤ s0
(7)
Ï†(s) =
s âˆ’(1+Î¸)
s > s0
c( s0 )

TABLE I: Dataset Statistics
Property
Directed
Nodes
Edges
Number of communities
Modularity
Average Out-degree
Average Eigenvector Centrality
Average K-shell
Average Pagerank
Cascades (â‰¥ 50 nodes)

Twitter Dataset

Weibo Dataset

undirected
595,460
7,170,209
24,513
0.7865
47.94
0.001783

directed
1,787,443
216,511,564
2,802
0.5581
231.3381
0.0186

24.6032
1.6794eâˆ’6
14,607

52.3064
5.596eâˆ’7
99,257

As Ï†(s)R is a probability distribution function, with the conâˆž
straint 0 Ï†(s)ds = 1 and power-law decay factor Î¸ estimated by training data, c can be computed. With the density
function Î»(t), the predicted cascade size can be computed by
equation (2).
IV. E XPERIMENTAL S ETUP
For comprehensiveness, we evaluate the performance of
each method by treating cascade prediction problem as both
regression and classification problem. We only consider cascades that end up with at least 50 adopters. Thus we can treat
first 50 nodes of each cascade as its early stage. In this section,
an introduction of the datasets is followed by descriptions
of setup of the classification and regression experiments. All
the experiments are carried out on an Intel(R) Xeon(R) CPU
E5-2620 @ 2.40 GHz machine with 256GB RAM running
Windows 7. All the methods are implemented in Python 2.7.
A. Dataset Description
The statistics of the two datasets used in this paper for
evaluation of the cascade prediction methods are shown in
Table I.
Twitter Dataset: Twitter1 is the most well-known microblog
platform throughout the world. The dataset was used in [2].
This dataset includes a friendship network with undirected
edges, cascades identified by hashtags and corresponding
mentions and retweets.
Weibo Dataset: Sina Weibo2 is the largest Chinese microblog
social network. The dataset was used in [7]. It consists of a
directed followership network and retweet cascades.
B. Regression
For the regression problem, the m Ã— 1 ground truth vector
y is made up of final size of each cascade (|V (tend )|), where
m is the number of cascade. Each regression model is able
to output a m Ã— 1 vector yÌ‚. Thus each element yÌ‚i âˆˆ yÌ‚
is the predicted size of the ith cascade. For point process
models, with different prediction time, the predicted results
can change. Thus, for each early-stage cascade, we set t as
1 https://twitter.com
2 https://weibo.com

the time when we observed the 50th adoption and prediction
time as {2, 4, 6, 8, 10} Ã— tv (t)[50]. To evaluate a method for
the regression problem, the difference between its prediction
results yÌ‚ and the ground truth y can be described by various
error functions. In addition, yÌ‚top10% denotes the set of top
10% cascades in prediction result while ytop10% is the set
top of 10% cascades of ground truth. In this paper we choose
following metrics to compare the prediction made by different
methods, as they are widely used in related literatures such as
[11], [5], [12] and [4]:
Pm |yË†i âˆ’yi |
1
â€¢ APE (average percentage error): m
i=1
yi
â€¢ RMSE (root mean square error):
r Pm
2
i=1 (yË†i âˆ’ yi )
m
â€¢ RMLSE (root mean logrithm square error):
r Pm
2
i=1 (log yË†i âˆ’ log yi )
m


10 
yÌ‚top10% âˆ© ytop10% 
â€¢ Top 10% coverage:
m

C. Classification
For classification, we apply three predetermined thresholds
(50th, 75th and 90th percentiles) to final size of cascades to
assign each of them a class label, which provides the m Ã— 1
ground truth vector L = {l0 , ..., lmâˆ’1 } one for each threshold.
The cascades with size larger than threshold are labelled as
viral class with li = 1. Table II shows the thresholds and
counts of samples for both classes. Then the methods for
solving the classification problem can output predicted label
vector LÌ‚. Comparing L with LÌ‚ results in standard metrics:
precision, recall and F1 score. To examine the effectivess of
the methods, we focus on reporting the metrics on the minority
class (viral) as it is more difficult to do good predictions for
it than the other.
Specially, for point process based mothods, as they are capable to predict the final cascade size without being trained with
class labels (once parameters are determined and prediction
times are selected), we carry out the evaluation on them in this
way: prediction results (by setting different prediction times)
are treated as features for each sample. As the time when each
cascade stop growing is not easy to determine.
D. Run time
We also take the run time of tasks into account for the cascade prediction methods. To understand how computationally
expensive the methods are in terms of run time, it is necessary to analyze the procedure of them. For centrality based
methods, the prediction can be divided into three steps: computation of centrality, training and prediction. Similarly, for
feature based methods, computation of features, training and
prediction are required to be done. In addition, preprocessing
like community detection, computation of shortest path length
are needed, which can be computationally expensive. While
point process based methods require little preprocessing. For
each cascade, parameters are computed independently through

TABLE II: Thresholds for Classification
Percentile

Threshold

Viral samples

Non-viral samples

Twitter Dataset
50%
75%
90%

106
226
587

50%
75%
90%

152
325
688

7,303
3,652
1,461

7,304
10,955
13,146

Weibo Dataset
49,628
24,814
9,925

49,629
74,443
89,332

MLE of the observed time vector tv (t) and properties of the
adopters V (t). Then prediction is made by integral of density
functions. Thus, we consider the following processes one by
one and then combine them to estimate the run time of a
certain method.
Proprecessing: There are three types of proprecessing considered: loading the graph, computation of centralities and
community detection.
Computation of Features: For feature based methods, we
measure the run time of computation of the features , which
takes the product of preprocessing as input.
Training and Prediction: For centrality and feature based
methods, the run time of training and prediction is measured
for ten-folds. For point process based methods, we measure
the run time of parameter estimation and prediction for the
whole batch of data.
V. E XPERIMENTAL R ESULTS
In this section we show the experimental results including
both accuracy of cascade prediction and the run time for each
method. For convenience, we call method of [3], [2] and the
centrality based method as method A, B and C respectively.
For method A, B and C, 10-fold cross-validation is applied.
For results where we compare these three methods, we report
only the best-performing centrality measure amongst outdegree, Pagerank, Shell number and eigenvector centrality as
the method C for each dataset. As shown in Fig. 1, eigenvector
centrality outperforms others in the classification task when
the two classes are imbalanced. Thus we take eigenvector
centrality as the method C. The results for regression is not
shown here for limited space as the difference between results
produced by different centralities is trivial. For the Reinforced
Poisson Process (RPP) method [5], as the parameter estimation
task for each cascade is independent of others, the crossvalidation is skipped and predictions are made by parameters
learned from first 50 nodes of each cascade. For the SEISMIC
method [4], we also skip the 10-fold cross-validation. We set
the cutoff time s0 = 30000(s) for the Twitter dataset and
s0 = 300(s) for the Weibo dataset then fit the parameters
(Î¸, c) for the human reaction time distribution function Ï†(s)
with all samples in the dataset. While in the original paper [4],
the authors set Î¸ and c just by 15 tweets they manually
picked. The power-law fitting is done as per [13], which

Out-degree
Pagerank
Shell Number
Eigenvector

1.0
0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.0

Precision

Recall

Out-degree
Pagerank
Shell Number
Eigenvector

1.0
0.8

0.0

F1 Score

(a) Twitter Dataset: 50th percentile

0.4

0.2

0.2

0.0

0.0

Recall

F1 Score

(c) Twitter Dataset: 90th percentile
Out-degree
Pagerank
Shell Number
Eigenvector

1.0
0.8

200.0%

Recall

0.0%

F1 Score

Out-degree
Pagerank
Shell Number
Eigenvector

(a) Twitter Dataset: APE
2.0
1.5

Precision

Recall

(d) Weibo Dataset: 50th percentile
Out-degree
Pagerank
Shell Number
Eigenvector

1.0
0.8
0.6
0.4

100.0%

0.2

0.2
F1 Score

(e) Weibo Dataset: 75th percentile

0.0

Precision

Recall

F1 Score

SEISMIC
RPP

(b) Twitter Dataset: RMSE

0.30
0.25
0.20

A(SVR)
B(SVR)
C(DTR)

SEISMIC
RPP

0.10
0.05
0.00

(c) Twitter Dataset: RMSLE
200.0%

A(SVR)
B(SVR)
C(DTR)

0.15

0.0

F1 Score

0.4

Recall

SEISMIC
RPP

0.5

0.6

Precision

A(SVR)
B(SVR)
C(DTR)

1.0

150.0%

0.0

3000
2500
2000
1500
1000
500
0

A(SVR)
B(SVR)
C(DTR)
SEISMIC
RPP

50.0%
Precision

0.8

0.4

250.0%

100.0%

1.0

0.6

300.0%

150.0%

(b) Twitter Dataset: 75th percentile

0.6

Precision

Out-degree
Pagerank
Shell Number
Eigenvector

1.0

A(SVR)
B(SVR)
C(DTR)
SEISMIC
RPP

(d) Twitter Dataset: Top 10% Coverage
2000

A(SVR)
B(SVR)
C(DTR)

SEISMIC
RPP

1500
1000

50.0%

500

0.0%

0

(f) Weibo Dataset: 90th percentile

Fig. 1: Classification results of centrality based methods: error
bar stands for one standard deviation.

(e) Weibo Dataset: APE
2.0
1.5

A(SVR)
B(SVR)
C(DTR)

SEISMIC
RPP

1.0

âˆ’5

returns (Î¸, c) = (0.440, 1.018e ) and (0.282, 7.332e
the Twitter dataset and Weibo dataset respectively.

âˆ’4

) for

A. Regression
For centrality based methods, we apply linear regression
with least squared error. We carry out the training and prediction with random forest regressor, SVR and linear regression
model provided by [14] for feature based methods. We only
show the results produced by SVR as it outperformes others.
For the point process based mothods, we only report the best
result among prediction time out of {2, 4, 6, 8, 10}Ã—tv (t)[50].
For the Twitter dataset, Fig. 2a, 2b, 2c and 2d show the
experimental results for the regression problem. Feature based
methods and SEISMIC outperform RPP and method C w.r.t.
APE. Concerning RMSE, method A shows more predictive
power than others. As to RMSLE, feature based methods result
in less error than the other two categories. From the aspect of
Top 10% coverage, RPP, method A are more likely to track
the trending cascades than others.
Fig. 2e, 2f, 2g and 2h show the regression result for the
Weibo dataset, Regarding APE, SEISMIC, method A and B
have comparable performance and outperform others. In terms
of RMSE, method A, B are measured to be more predictive
than the rest. Feature based methods also make predictions
with least RMSLE. For top 10% coverage, RPP is more likely
to detect popular cascades than others.
An interesting observation is that the prediction accuracy
measured by different error metrics can be contrary to each
other. For example, in Fig. 2a, compared to SEISMIC, prediction made by method C results in more error measured by

0.5
0.0

(g) Weibo Dataset: RMSLE

(f) Weibo Dataset: RMSE

0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00

A(SVR)
B(SVR)
C(DTR)

SEISMIC
RPP

(h) Weibo Dataset: Top 10% Coverage

Fig. 2: Regression results

APE, however, comparable error w.r.t. RMSE and less error
regarding RMSLE (See Fig. 2b and 2c). This implies that it
is better for researchers to show more than one type of error
for evaluation of regression results.
B. Classification
We show the precision, recall and F1 score for the viral
class with all the three percentile thresholds. For each dataset,
we choose the 50th, 75th and 90th percentile of the final size
of all cascades as the thresholds for assigning the cascades
into viral or non-viral class. The number of samples in
each class is shown in Table II. Thus we can evaluate the
cascade prediction methods with balanced and imbalanced
classes. For each method, we only show the best result among
those produced by different classifiers or various training
methods. As a result, for feature based methods, random forest
outperforms others. While for point process based methods
we treat cascade size predicted by setting prediction time as
{2, 4, 6, 8, 10}Ã—tv (t)[50] as features. Here we show the results
produced by classifiers trained by these features.
Fig. 3a, 3b and 3c show the classification results for
the Twitter dataset. With all three thresholds, feature based
methods A and B outperform others. In addition, they also

1.0

A(RF)
B(RF)

C(DT)
SEISMIC

RPP

1.0

0.8

0.8

0.6

0.6

0.4

0.4

0.2
0.0

Recall

F1 Score

A(RF)
B(RF)

C(DT)
SEISMIC

RPP

0.0

1.0
0.8

0.6

0.6

0.4

0.4

0.2

0.2
Precision

Recall

F1 Score

(c) Twitter Dataset: 90th percentile
1.0

A(RF)
B(RF)

C(DT)
SEISMIC

RPP

0.0

1.0
0.8

0.6

0.6

0.4

0.4

0.2

Precision

Recall

F1 Score

A(RF)
B(RF)

Precision

C(DT)
SEISMIC

Recall

RPP

F1 Score

(d) Weibo Dataset: 50th percentile

0.8

0.0

RPP

(b) Twitter Dataset: 75th percentile

0.8

0.0

C(DT)
SEISMIC

0.2
Precision

(a) Twitter Dataset: 50th percentile
1.0

A(RF)
B(RF)

A(RF)
B(RF)

C(DT)
SEISMIC

RPP

0.2
Precision

Recall

F1 Score

(e) Weibo Dataset: 75th percentile

0.0

Precision

Recall

F1 Score

(f) Weibo Dataset: 90th percentile

Fig. 3: Classification results: error bar stands for one standard
deviation.

show more robustness than others to imbalance of two classes
in dataset. In terms of point process based methods, SEISMIC
outperforms RPP especially when the two class are imbalanced. RPP suffers from relatively large standard deviation,
as the Newtonâ€™s method is not always able to achieve convergence. Thus the parameters learned through the MLE approach
can vary as a result from random initialization. Method C
(eigenvector centrality) shows little predictive power with any
of the three thresholds for the Twitter dataset, even if it
outperforms other centrality based methods.
For the Weibo dataset, as shown in Fig. 3d, 3e and 3f,
feature based methods outperform others again with all three
thresholds. Regarding point process based methods, contrary
to the results for Twitter dataset, RPP achieves better F1 score
than SEISMIC when threshold value becomes large. Method
C (eigenvector centrality) performs comparably to RPP.
C. Run time
In this subsection, we show the run time of tasks for the
cascade prediction methods considered in this paper. On one
hand, preprocessing, computation of centralities and features
suffer from high overhead as immense amount of data needs
to be loaded. The run time of these tasks are listed in Table III.
On the other hand, training and prediction tasks barely have
the overhead issue.
Preprocessing: We carry out the community detection task by
the java implementation of Louvain algorithm [15] with 10
random start and 10 iterations for each start. For computation
of centralities, we load edgelist of the social networks as a
graph object in igraph-python [16]. As shown in Table III,

community detection, computation of Pagerank and loading
graph are the tasks suffer the most when the size of dataset
increases. Community detection, computation of Pagerank and
loading graph for the Weibo dataset take 80.32, 66.855 and
19.80 times the run time of those for the Twitter dataset
respectively.
Computation of Features: As shown in Table III, for the
feature computation task, it takes method B 12.37 and 8 times
the run time method of A for the Twitter Dataset and the
Weibo Dataset respectively. To explain this observation, an
analysis of what computation is carried out in each iteration for
method A and B. For method A, computation of the features
can be done without loading the graph (a heavy overhead).
Moreover, for each cascade, method B also requires expensive
computation of shortest path length for each pair of nodes in
cascade subgraphs and size of 2-hop neighborhood.
Training and Prediction: The run time of training and prediction is not directly related to the size of the social network.
On one hand, it is correlated to the number of cascades for
training and prediction. On the other hand, it is decided by the
complexity of the method: for example, number of parameters
to be learned, the complexity for learning each paramter and
the comsumption to work out the prediction. Here we only
measure the run time for solving the classification problem.
We run each method with single process, overhead run time
such as graph loading is ignored. For feature based methods
the training and prediction time are also correlated to the
number of features. For centrality based methods, we only
show the run time for k-shell (method C) as all methods in this
category are trained and tested with one feature: the centrality
measure of the root node. Compared to RPP, SEISMIC is a
deterministic method with closed form solution. The run time
for each sample can be distributed with little variance. For the
RPP method, as the log-likelihood function is non-convex, it is
not guaranteed that global maximum can be reached in limited
number of iterations. Therefore, the run time for a sample
running out of the maximum number of iterations can be
thousands times that of another, which reaches the convergence
condition in the first iteration. As the log-likelihood function of
RPP is twice-differentiable, Newtonâ€™s method can be applied.
In our experiments, with the maximum number of iterations
setted as 100, the convergence is more likely to be achieved by
Newtonâ€™s method than gradient descent. Thus we only show
the run time of RPP with Newtonâ€™s method.
Fig. 4 shows the run time for each method to complete
training and prediction tasks for all cascades in the two
datasets. For feature based methods, it shows the run time
needed for random forest (RF), SVM and logistic regression
(LR). For method C, it shows that of decision tree (DT), SVM
and logistic regression (LR).
Concerning the Twitter dataset (See Fig. 4a), taking advantage of decent implementation of classifiers, feature based
methods comparable run time to point process based methods
w.r.t. the training and prediction task with random forest and
SVM (rbf kernel).
For the Weibo dataset, as shown in Fig. 4b, the run time

TABLE III: Run time: Preprocessing & Feature Computation
Type

Task

Twitter Dataset
Louvain
275
Loading Graph
60.033
Degree
0.016
K-shell
2.757
Eigenvector
20.444
Pagerank
26.298
A
267.144
B
3252.7562
Weibo Dataset
Louvain
22087
Loading Graph
1188.486
Degree
0.045
K-shell
139.128
Eigenvector
391.140
Pagerank
1758.164
A
11181.453
B
87651.213

Feature
Computation

Preprocessing

Feature
Computation

Run time (seconds)

10 2

A(RF)
A(LR)
A(SVM)
B(RF)

B(LR)
B(SVM)
C(DT)
C(LR)

C(SVM)
SEISMIC
RPP

10 6
10 5

Run time (seconds)

Preprocessing

10 3

Total time (s)

A(RF)
A(LR)
A(SVM)
B(RF)

Time
per
sample (s)
â€“
â€“
â€“
â€“
â€“
â€“
0.018
0.2227
â€“
â€“
â€“
â€“
â€“
â€“
0.110
0.883
B(LR)
B(SVM)
C(DT)
C(LR)

C(SVM)
SEISMIC
RPP

10 4
10 3

10 1

10 2

10 0

10 1
10 0

10 -1

(a) Twitter Dataset

(b) Weibo Dataset

Fig. 4: Run Time of Trainig and Prediction

feature based methods comsume is comparable to SEISMIC
with random forest. But the SVM with rbf kernel suffers from
the order-of-magnitude increase of the number of training and
testing samples. Thus leads to the observation that the run time
becomes approximately 10 times that of random forest.
Comparing Fig. 4b with Fig. 4a, the run time of RPP method
increases the most. This means it is much more difficult for
the Newtonâ€™s method to converge for samples in the Weibo
datasets. There are two possible reasons to explain this: 1).
the uniform distribution used in random initialization can not
produce good initial values that are closed to local optimal
points; 2). the choice of log-norm distribution as function
fd (t; Î¸d ) can not provide fairly good description of cascades
in this dataset.
VI. R ELATED W ORK
Influence Maximization Since the proposal of Influence maximization problem by Kempe et al. [17], related work emerged,
focusing on estimation of influence for a selected set of nodes
that can be measured by expected number of infectees under
a certain influence model, such as [18] and [19]. Recently,
a scalable randomized algorithm designed by Du et al. [20]
estimates influence initiated by selected source nodes and thus
select seed set with maximum expected influence.

Cascade Prediction Although in [1], k-shell and heuristics
of k-shell were shown to be effective indicator of long-term
influence of nodes, in [21], experimental results showed that
the shell number of the root node is not effectively predictive in
the cascade by cascade scenario. Feature based methods from
Jenders et al. [22] Chen et al. [23] were designed to solve the
cascade prediction problem formulated as binary classification
on balanced dataset, however, these methods are more or less
dependent on content features from specific social media sites.
Ma et al. [24] focused on applying content features to classify
hashtag cascades by how much their size increases. Regarding
to point process based methods, model designed with the
intuition of mutual exciting nature of social influence, Zhou
et al [25] applied multi-dimensional Hawkes process to rank
cascades (memes) by their popularity. Recently, the model
introducted by Yu et al. [11] combined feature engineering
and human reaction time distribution function widely used
in point process based methods to aggregate adoptions in
subcascades for cascade prediction. Besides feature based
methods and point process based methods studied in this
paper, knowledge from related research fields could also be
applied to cascade prediction. Goyal et al. [26] proposed the
credit distribution model to learn pair-wise influence based
on IC model proposed by Kempe et al. [17]. Cui et al. [27]
proposed a feature selection approach for binary classification
of cascades. Wang et al. [28] proposed a model to decouple
the influence measured in a pair-wise way into two latent
vectors representing influence and susceptibility of a node.
This work differs from all the past efforts in that it is the most
thorough comparison of methods general enough to be applied
to different datasets without relying on features specific to a
certain social media site.
VII. D ISCUSSION AND C ONCLUSION
In this paper, we evaluate three categories of recently
proposed methods with both the classification and regression
formulaton of cascade prediction. Feature based methods
generally provide better prediction accuracy for the cascade
prediction problem, no matter it is considered as classification
or regression. However, they suffer from heavy overhead
such as community detection and computation of features.
Random point process based methods enable us to achieve the
prediction with little preprocessing but are shown to be less
accurate than feature based methods. The run time of methods
in this category can also suffer from the situation when the
data can not be well modelled by the proposed density function
Î»(t).
In regression experiments, we find the inconsitancy between
evaluation with different error metrics. A method that performs
well w.r.t. one metric could result in large error measured by
another. A predictive method should be able to perform fairly
well measured by various error metrics.
How to deal with changes in the social network and progress
of cascades to update features is the biggest issue that both
centrality based and feature based methods encounter. The

heavy overhead introduced by preprocessing and computation
of features limits these methods from near real-time prediction.
Point process based methods require little preprocessing
and the training and prediction process are parallelable as
they consider each cascade is indenpendent of others. This
advantage in terms of run time over feature based methods
can also be amplified as the size of the social network and the
number of cascades. Moreover, point process based methods
encounter little cold start problem. These two characteristics of
point process based methods make them more suitable for realtime cascade prediction task. But how to secure the accuracy
of prediction is the biggest issue for them. The point process
based models are faced with two more problems: sensitivity
to scale of time unit and requirement of prediction time as an
input variable. In real-world application, given a early stage
cascade, estimation of when it will stop progressing is a nontrivial problem.
On balance, this paper explored various methods in the
academic literature of predicting viral information cascades
in a more comprehensive manner. Our aim is to provide important insights into which methods based on graph topology
or temporal dynamics performed best - as these results can
generalize to a variety of application domains. In our ongoing
work on developing a deplyable system for identifying viral
extremist messages, this represents an important consideration.
Our next step is to consider microblog content as well - which
tends to be more domain specific.
ACKNOWLEDGMENTS
Some of the authors are supported through the AFOSR
Young Investigator Program (YIP) grant FA9550-15-1-0159,
ARO grant W911NF-15-1-0282, the DoD Minerva program
and the EU RISE program.
R EFERENCES
[1] S. Pei, L. Muchnik, J. S. Andrade Jr, Z. Zheng, and H. A. Makse,
â€œSearching for superspreaders of information in real-world social media,â€ Scientific reports, vol. 4, 2014.
[2] L. Weng, F. Menczer, and Y.-Y. Ahn, â€œPredicting successful memes
using network and community structure,â€ in Eighth International AAAI
Conference on Weblogs and Social Media, 2014.
[3] R. Guo, E. Shaabani, A. Bhatnagar, and P. Shakarian, â€œToward order-ofmagnitude cascade prediction,â€ in Proceedings of the 2015 IEEE/ACM
International Conference on Advances in Social Networks Analysis and
Mining 2015. ACM, 2015, pp. 1610â€“1613.
[4] Q. Zhao, M. A. Erdogdu, H. Y. He, A. Rajaraman, and J. Leskovec,
â€œSeismic: A self-exciting point process model for predicting tweet
popularity,â€ in Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 2015,
pp. 1513â€“1522.
[5] H. Shen, D. Wang, C. Song, and A.-L. BarabaÌsi, â€œModeling and
predicting popularity dynamics via reinforced poisson processes,â€ in
Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.
[6] N. Kim, S. Gokalp, H. Davulcu, and M. Woodward, â€œLookingglass: A
visual intelligence platform for tracking online social movements,â€ in
Advances in Social Networks Analysis and Mining (ASONAM), 2013
IEEE/ACM International Conference on. IEEE, 2013, pp. 1020â€“1027.
[7] J. Zhang, B. Liu, J. Tang, T. Chen, and J. Li, â€œSocial influence locality
for modeling retweeting behaviors.â€ in IJCAI, vol. 13, 2013, pp. 2761â€“
2767.
[8] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, â€œFast
unfolding of communities in large networks,â€ Journal of statistical
mechanics: theory and experiment, vol. 2008, no. 10, p. P10008, 2008.

[9] M. Rosvall and C. T. Bergstrom, â€œMaps of random walks on complex
networks reveal community structure,â€ Proceedings of the National
Academy of Sciences, vol. 105, no. 4, pp. 1118â€“1123, 2008.
[10] J. Ugander, L. Backstrom, C. Marlow, and J. Kleinberg, â€œStructural
diversity in social contagion,â€ Proceedings of the National Academy
of Sciences, vol. 109, no. 16, pp. 5962â€“5966, 2012.
[11] L. Yu, P. Cui, F. Wang, C. Song, and S. Yang, â€œFrom micro to
macro: Uncovering and predicting information cascading process with
behavioral dynamics,â€ arXiv preprint arXiv:1505.07193, 2015.
[12] S. Gao, J. Ma, and Z. Chen, â€œModeling and predicting retweeting
dynamics on microblogging platforms,â€ in Proceedings of the Eighth
ACM International Conference on Web Search and Data Mining. ACM,
2015, pp. 107â€“116.
[13] J. Alstott, E. Bullmore, and D. Plenz, â€œpowerlaw: a python package for
analysis of heavy-tailed distributions,â€ PloS one, vol. 9, no. 1, p. e85777,
2014.
[14] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, â€œScikit-learn: Machine learning in Python,â€ Journal of Machine
Learning Research, vol. 12, pp. 2825â€“2830, 2011.
[15] L. Waltman and N. J. van Eck, â€œA smart local moving algorithm
for large-scale modularity-based community detection,â€ The European
Physical Journal B, vol. 86, no. 11, pp. 1â€“14, 2013.
[16] G. Csardi and T. Nepusz, â€œThe igraph software package for complex
network research,â€ InterJournal, vol. Complex Systems, p. 1695, 2006.
[Online]. Available: http://igraph.org
[17] D. Kempe, J. Kleinberg, and EÌ. Tardos, â€œMaximizing the spread of
influence through a social network,â€ in Proceedings of the ninth ACM
SIGKDD international conference on Knowledge discovery and data
mining. ACM, 2003, pp. 137â€“146.
[18] W. Chen, Y. Wang, and S. Yang, â€œEfficient influence maximization in
social networks,â€ in Proceedings of the 15th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 2009, pp.
199â€“208.
[19] A. Goyal, W. Lu, and L. V. Lakshmanan, â€œCelf++: optimizing the greedy
algorithm for influence maximization in social networks,â€ in Proceedings
of the 20th international conference companion on World wide web.
ACM, 2011, pp. 47â€“48.
[20] N. Du, L. Song, M. Gomez-Rodriguez, and H. Zha, â€œScalable influence
estimation in continuous-time diffusion networks,â€ in Advances in neural
information processing systems, 2013, pp. 3147â€“3155.
[21] P. Shakarian, A. Bhatnagar, A. Aleali, E. Shaabani, and R. Guo,
Diffusion in Social Networks. Springer, 2015.
[22] M. Jenders, G. Kasneci, and F. Naumann, â€œAnalyzing and predicting
viral tweets,â€ in Proceedings of the 22nd international conference
on World Wide Web companion.
International World Wide Web
Conferences Steering Committee, 2013, pp. 657â€“664.
[23] J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec,
â€œCan cascades be predicted?â€ in Proceedings of the 23rd international
conference on World wide web. ACM, 2014, pp. 925â€“936.
[24] Z. Ma, A. Sun, and G. Cong, â€œOn predicting the popularity of newly
emerging hashtags in twitter,â€ Journal of the American Society for
Information Science and Technology, vol. 64, no. 7, pp. 1399â€“1410,
2013.
[25] K. Zhou, H. Zha, and L. Song, â€œLearning social infectivity in sparse lowrank networks using multi-dimensional hawkes processes,â€ in Proceedings of the Sixteenth International Conference on Artificial Intelligence
and Statistics, 2013, pp. 641â€“649.
[26] A. Goyal, F. Bonchi, and L. V. Lakshmanan, â€œA data-based approach to
social influence maximization,â€ Proceedings of the VLDB Endowment,
vol. 5, no. 1, pp. 73â€“84, 2011.
[27] P. Cui, S. Jin, L. Yu, F. Wang, W. Zhu, and S. Yang, â€œCascading outbreak
prediction in networks: a data-driven approach,â€ in Proceedings of the
19th ACM SIGKDD international conference on Knowledge discovery
and data mining. ACM, 2013, pp. 901â€“909.
[28] Y. Wang, H. Shen, S. Liu, and X. Cheng, â€œLearning user-specific latent
influence and susceptibility from information cascades,â€ in Twenty-Ninth
AAAI Conference on Artificial Intelligence, 2015.

arXiv:1507.01922v1 [cs.CR] 7 Jul 2015

Cyber-Deception and Attribution
in Capture-the-Flag Exercises
Eric Nunes, Nimish Kulkarni, Paulo Shakarian

Andrew Ruef, Jay Little

School of Computing, Informatics and
Decision Systems Engineering
Arizona State University
Tempe, AZ 85281, USA
Email: {enunes1, nimish.kulkarni, shak} @asu.edu

Trail of Bits, Inc.
New York, NY 10003, USA
Email: {andrew, jay} @trailofbits.com

Abstractâ€”Attributing the culprit of a cyber-attack is widely
considered one of the major technical and policy challenges
of cyber-security. The lack of ground truth for an individual
responsible for a given attack has limited previous studies. Here,
we overcome this limitation by leveraging DEFCON capture-theflag (CTF) exercise data where the actual ground-truth is known.
In this work, we use various classification techniques to identify
the culprit in a cyberattack and find that deceptive activities
account for the majority of misclassified samples. We also explore
several heuristics to alleviate some of the misclassification caused
by deception.

I.

I NTRODUCTION

Attributing the culprit of a cyber-attack is widely considered one of the major technical and policy challenges of
cyber-security. The lack of ground truth for an individual
responsible for a given attack has limited previous studies. In
this study, we take an important first step toward developing
computational techniques toward attributing the actual culprit
(here hacking group) responsible for a given cyber-attack. We
leverage DEFCON capture-the-flag (CTF) exercise data which
we have processed to be amenable to various machine learning
approaches. Here, we use various classification techniques to
identify the culprit in a cyber-attack and find that deceptive
activities account for the majority of misclassified samples. We
also explore several heuristics to alleviate some of the misclassification caused by deception. Our specific contributions are
as follows:
â€¢

We assemble a dataset of cyber-attacks with ground
truth derived from the traffic of the CTF held at
DEFCON 21 in 2013.

â€¢

We analyze this dataset to identify cyber-attacks where
deception occurred.

â€¢

We frame cyber-attribution as a multi-label classification problem and leverage several machine learning
approaches. We find that deceptive incidents account
for the vast majority of misclassified samples.

explore the deception hypothesis in a cyber-warfare scenario.
When compared to other domains of warfare, there is a much
greater potential for evidence found in the aftermath of cyberattack to be planted by the adversary for purposes of deception.
The policy implications of cyber-attribution have also been
discussed in [9] where the authors point out that anonymity,
ability to launch multi-stage attacks, and attack speed pose
significant challenges to cyber attribution.
In an early survey on cyber-attribution [1], the authors point
out that technical attribution will generally identify machines,
as opposed to a given hacker and his/her affiliations. While
we will use technical information in our approach, we have
ground truth data on the group involved by the nature of
the capture-the-flag data. This will allow our approach to
profile the tactics, techniques, and procedures of a given group
as we have ground-truth information on a hacking group as
opposed to machines. An example of such an approach is the
WOMBAT attribution method [3] which attributes behavior
to IP sources that are potentially linked to some root cause
determined through a clustering technique. Similarly, other
work [8] combines cluster analysis with a component for multicriteria decision analysis and studied an implementation of this
approach using honeypot data â€“ again, this approach lacks any
ground truth of the actual hacker or hacking group.
Concurrently, we have devised a formal logical framework
for reasoning about cyber-attribution [5], [7]. However, we
have not studied how this framework can be instantiated on
a real world dataset and, to date, we have not reported on an
implementation or experiments in the literature. We note that
none of the previous work on cyber-attribution leverages a
data set with ground truth information of actual hacker groups
â€“ which is the main novelty of this paper.
II.

DATASET

We introduce several pruning techniques and show
that they can reduce the effect of deception as well as
provide insight into the conditions in which deception
was employed by the participants of the CTF.

Our dataset consists of events recorded from a Capturethe-flag (CTF) tournament held at DEFCON 21 in 2013.
Briefly, CTF competitions act as educational exercise that
exposes real world attack scenarios to participants. Network
sniffing, analysis of protocols, programming and system level
knowledge, cryptanalysis are some of the instrumental skills
acquired by contestants.

In our text on cyber-warfare [6], we discuss the difficulties
of cyber-attribution and how an intelligence analyst must also

Our data represents attack/defense style, where each team
owns a small network of machines to defend. Teams are judged

â€¢

Robot Mafia

svc

02345

payload hash

2cc03b4e0053cde24400bbd80890446c

time

2013-08-03T23:45:17

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

T-3

T-2

T-1

T-20
T-20

men in black hats

to team

T-19

from team

0

T-18

cmp:12 , svcmi:2, subs:8, movtmi:60 ......

T-17

0Ã—43:245, 0Ã—69:8, 0Ã—3a:9, 0Ã—5d:1, .....

inst hist

T-16

byte hist

200000

T-15

Value

400000

T-14

Field

600000

T-13

TABLE 2: Example event from the dataset

800000

T-12

From this pre-processing of the network data (packets) we
have around 10 million network attacks. There are 20 teams
in the CTF competition. In order to attribute an attack to a
particular team, apart from analyzing the payloads used by the
team, we also need to analyze the behavior of the attacking
team towards his adversary. For this purpose we separate the
network attacks according to the team being targeted. Thus
we have 20 such subsets. We represent the 20 subsets (teams)
as T-i, where i = 1, 2, 3...20. An example of an event in the
dataset is shown in Table 2.

T-11

indicates the date and time of the attack

T-10

indicates the payload used in the attack (md5)

time

T-9

the service that the payload is running

payload hash

T-8

the team being attacked by the payload

svc

Duplicate attacks: A duplicate attack occurs when the same
team uses the same payload to attack a team at different time
instances. Duplicate attacks can be attributed to two reasons.
First when a team is trying to compromise other systems, it
just does not launch a single attack but a wave of attacks with
very little time difference between consecutive attacks. Second,
once a successful payload is created which can penetrate the
defense of other systems, it is used more by the original
attacker as well as the deceptive one as compared to other
payloads. We group duplicates as being non-deceptive and
deceptive. Non-deceptive duplicate are the duplicates of the
team that first initiated the use of a particular payload. On the
other hand deceptive duplicates are all the attacks from the
teams that are being deceptive. Deceptive duplicates form a
large portion of the dataset as seen in Fig. 2.

T-7

the team where the payload originates (attacking team)

to team

Deceptive Attacks

Fig. 1: Unique deceptive attacks directed towards each team.

T-6

histogram of instructions used in the payload

from team

Teams

Unique Attacks

T-5

histogram of byte sequences in the payload

inst hist

0

T-4

Intuition

byte hist

100000

T-3

Field

200000

T-2

TABLE 1: Fields in an instance of network attack

300000

T-1

For each file, we computed an md5 checksum, a byte
histogram, and an ARM instruction histogram. This data was
recorded as a list of tuples (time-stamp, hash, byte-histogram,
instruction-histogram) in a JSON document. These individual
fields of the tuple are listed in Table 1.

400000

Unique Attacks

DEFCON CTF organizers recorded network traffic that
includes network packets generating to and from all participating teams and is available on Internet [4]. Recordings are
stored as archive files of PCAP (packet capture) for each team
(destination as that team) separately. PCAP file contains packet
headers (TCP, SSL, UDP etc.) and respective data as source,
destination, sequence numbers etc. with timestamp having
millisecond precision. Using open source tool tcpflow1 , we
interpreted collection of PCAPs as cumulative data streams.
Tcpflow reconstructs actual data streams from the packets
that proved helpful in protocol analysis and debugging. This
tool produces a file containing the contents of each stream,
representing the data sent between two points in the CTF
system.

attack to a team difficult.
Deception: In the context of this paper we define an attack to
be deceptive when multiple adversaries get mapped to a single
attack pattern. In the current setting we define deception as the
scenario when the same payload is used by multiple teams to
target the same team. Fig. 1 shows the distribution of unique
deception attacks with respect to the total unique attacks in
the dataset based on the target team. These unique deceptive
attacks amount to just under 35% of the total unique attacks.

Total Attacks

based on scores given to attack machines of other teams as well
as defending their own network. Initially, all virtual machines
are configured with specific set of services. These services
are vulnerable to state-of-art hacking techniques. Files can be
considered as form of flag to be captured from other teams or
to be planted to other teams by exploiting those vulnerabilities.

Teams

Non-Deceptive

Deceptive

Total Attacks

Fig. 2: Total attacks and duplicate attacks(Deceptive and
Non-deceptive) directed towards each team

A. Dataset Analysis
We now discuss two important observations from the
dataset, that makes the task of attributing an observed network
1 https://github.com/simsong/tcpflow

III.

BASELINE A PPROACHES

From the dataset, we have the ground truth available for
all the samples. Hence we use supervised machine learning

0.26

Logistic regression (LOG-REG)

0.31

Support vector machine (SVM)

0.30

Random Forest (RF)

0.37

â€¢

Non-deceptive duplicate attacks attributed to one of
the deceptive teams.

â€¢

Deceptive duplicates attributed to some other deceptive team.

â€¢

Payloads that were not encountered during the training
phase.

The first two sources of error make up the majority of
misclassifications, since a given attack can be attributed to any
of the 19 teams.
1.0

0.8

0.6

0.4

0.2

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

0.0

T-3

For our baseline experiments, we separate the attacks based
on the team being targeted. Thus we have 20 subsets. We then
sort the attack according to time. We reserve the first 90% of
the attacks for training and the rest 10% for testing. Attacker
prediction accuracy is used as the performance measure for
the experiment. Accuracy is defined as the fraction of correctly
classified test samples. Fig. 3 shows the accuracy for predicting
the attacker for each target team. Machine learning techniques
significantly outperform random guessing which would have
an average accuracy of choosing 1 out of 19 teams attacking
yielding an accuracy of 0.053. For this experiment random
forest classifier performs better than logistic regression, support vector machine and decision tree for all the target teams.
Table 3 below summarizes the average performance for each
method.

Average Performance

Decision tree (DT)

T-2

A. Experimental Results

Method

T-1

Decision Tree (DT). For baseline comparisons we first implemented a decision tree classifier. We built the decision tree
by finding the attribute that maximizes the information gain at
each split. In order to avoid over-fitting, the terminating criteria
is set to less than 0.1% of total samples.
Random Forest (RF). We use a random forest which combines bagging for each tree with random feature selection at
each node to split the data thus generating multiple decision
tree classifiers.
Support Vector Machine (SVM). Support vector machines
is a popular supervised classification technique that works
by finding a separating margin that maximizes the geometric
distance between classes. We use the popular LibSVM implementation [2] which is publicly available.
Logistic Regression (LOG-REG). Logistic regression classifies samples by computing the odds ratio. The odds ratio
gives the strength of association between the features and the
class. We implement the multinomial logistic regression which
handles multi-class classification.

TABLE 3: Summary of Prediction results averaged across all
Teams

Fraction of Misclassified Samples

approaches to predict the attacking team. The ground truth
corresponds to a team competing in the competition.

Teams

Non-Deceptive Duplicates

Deceptive Duplicates

Unseen payloads

Fig. 4: Sources of error in the misclassified samples.
Fig. 4 shows the distribution of the above mentioned
sources of misclassification for each team. Deceptive duplicates form the majority of misclassifications. This is not
surprising given the fact that deceptive duplicates make up
almost 90% of the total attacks (see Fig. 2).

0.6

IV.

0.5

We explore different pruning techniques to address
misclassification issues with respect to deceptive and nondeceptive duplicates. The pruning techniques are only applied
to the training data, while the test data is maintained at 10%
as mentioned in Section III-A. We use the random forest
classifier for all the pruning techniques.

0.4

Accuracy

P RUNING

0.3
0.2

0.1

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

T-10

T-8

T-7

T-6

T-5

T-4

T-3

T-2

T-1

0

Teams

LOG-REG

RF

SVM

DT

Fig. 3: Team prediction accuracy for LOG-REG, RF, SVM
and DT.

B. Misclassified Samples
Misclassification can be attributed to the following sources,

All-but-majority (P-1): In this pruning technique, for each
payload, we only retain duplicates of the most frequent attacking team and prune the duplicates of all other teams. This
pruned set is then used to train the random forest classifier.
Table 4 shows the classifier performance in comparison with
the baseline method. All-but-majority pruning technique has
better performance on the test set than the baseline approach
for 11 out of 20 teams. Using this pruning technique does
benefit majority of the teams as the prediction accuracy improves for them, but for some teams the performance drops.

The reason for the drop in performance for some teams is
due to the fact that training set gets dominated by a single
team which does not have majority in testing set. Since the
majority team gets represented in most of the leaves of the
random forest classifier, it gets predicted more often leading
to high misclassifications.
All-but-K-majority (P-2): In order to address the issue of
one team dominating in the training set, we use the all-but-Kmajority where we consider the K most frequent teams for a
payload under consideration. After trying out different values
of K we select K = 3, which gives the best performance.
For higher values of K, the pruning behaves like the baseline
approach and for lower values it behaves like All-but-majority.
On average each team gains about 40K samples in the training
set as compared to all-but-majority pruning. Table 4 shows
the classifier performance. In this case also pruning performs
better than baseline in 11 out of 20 teams, but as compared to
all-but-majority the performance for most teams is better.
All-but-earliest (P-3): For this pruning we only retain the
duplicates of the team that initiated the attack using a particular
payload. This pruning technique retains all the non-deceptive
duplicates while getting rid of the deceptive ones. Table 4
shows the classifier performance. This pruning technique performs better than the baseline approach for 8 out of 20 teams.
Comparing this result to all-but-majority (including all-butK-majority) pruning indicates that deceptive duplicates are
informative in attributing an attack to a team and should not
be ignored completely.
All-but-most-recent (P-4): In this pruning we repeat a similar
procedure like All-but-earliest but instead of retaining the
duplicates of the team that initiated an attack, we retain the
duplicates of the team that used the payload last in the training
set. Since the data is sorted according to time, the last attacker
becomes the most recent attacker for the test set. Table 4 shows
the classifier performance.
TABLE 4: Pruning technique performance comparison.
Teams

RF

P-1(RF)

P-2(RF)

P-3(RF)

P-4(RF)

T-1

0.45

0.16

0.46

0.15

0.15

T-2

0.22

0.28

0.30

0.15

0.14

T-3

0.30

0.53

0.29

0.57

0.57

T-4

0.26

0.33

0.27

0.31

0.32

T-5

0.26

0.38

0.45

0.40

0.42

T-6

0.50

0.27

0.24

0.31

0.26

T-7

0.45

0.59

0.58

0.19

0.49

T-8

0.42

0.52

0.52

0.51

0.55

T-9

0.41

0.65

0.68

0.52

0.53

T-10

0.30

0.54

0.34

0.55

0.57

T-11

0.37

0.27

0.35

0.27

0.29

T-12

0.24

0.37

0.37

0.25

0.22

T-13

0.35

0.27

0.37

0.29

0.27

T-14

0.42

0.27

0.40

0.30

0.30

T-15

0.30

0.20

0.27

0.21

0.20

T-16

0.42

0.28

0.22

0.32

0.31

T-17

0.43

0.45

0.35

0.43

0.40

T-18

0.48

0.39

0.43

0.41

0.40

T-19

0.41

0.65

0.58

0.54

0.60

T-20

0.48

0.16

0.16

0.16

0.17

Table 5 gives the summary of the prediction results for
all the pruning techniques in comparison with the random
forest baseline approach. In the pruning techniques All-butK-majority works best with an average accuracy of 0.42.

TABLE 5: Summary of Prediction results averaged across all
Teams
Method

Average Performance

Baseline Approach (RF)

0.37

All-but-majority Pruning (RF)

0.40

All-but-K-majority Pruning (RF)

0.42

All-but-earliest Pruning (RF)

0.34

All-but-most-recent Pruning (RF)

0.36

V.

C ONCLUSION

In this paper, we study cyber-attribution by examining
DEFCON CTF data - which provides us with ground-truth
on the culprit responsible for each attack. We frame cyberattribution as a classification problem and examine it using
several machine learning approaches. We find that deceptive
incidents account for the vast majority of misclassified samples
and introduce heuristic pruning techniques that alleviate this
problem somewhat. Moving forward, we look to employ a
more principled approach to counter deception based on our
previously established theoretical framework for reasoning
about cyber-attribution [5], [7]. In particular we wish to employ
temporal reasoning to tackle the problem of deceptive attacks.
This opens up interesting research questions in particular identifying hacking group from a series of attacks over a period of
time, differentiating between deceptive hacking groups in time
series data. This is a knowledge engineering challenge which
calls for development of efficient and scalable algorithms.
VI.

ACKNOWLEDGMENT

Some of this work was supported by the U.S. Office of
Naval Research and ASU Global Security Initiative (GSI).
R EFERENCES
[1]
[2]

[3]

[4]
[5]

[6]
[7]

[8]

[9]

W. E. Boebert. A survey of challenges in attribution. In Proceedings of
a workshop on Deterring CyberAttacks, pages 41â€“54, 2010.
C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector
machines. ACM Transactions on Intelligent Systems and Technology
(TIST), 2(3):27, 2011.
M. Dacier, V.-H. Pham, and O. Thonnard. The wombat attack attribution
method: some results. In Information Systems Security, pages 19â€“37.
Springer, 2009.
DEFCON. Defcon: Capture the flag. 2013.
S. Jajodia, P. Shakarian, V. S. Subrahmanian, V. Swarup, and C. Wang.
Cyber Warfare: Building the Scientific Foundation. Springer Publishing
Company, Incorporated, 2015.
P. Shakarian, J. Shakarian, and A. Ruef. Introduction to cyber-warfare:
A multidisciplinary approach. Newnes, 2013.
P. Shakarian, G. I. Simari, and M. A. Falappa. Belief revision in
structured probabilistic argumentation. In Foundations of Information
and Knowledge Systems, pages 324â€“343. Springer, 2014.
O. Thonnard, W. Mees, and M. Dacier. On a multicriteria clustering
approach for attack attribution. ACM SIGKDD Explorations Newsletter,
12(1):11â€“20, 2010.
N. Tsagourias. Nicolas politis initiatives to outlaw war and define
aggression, and the narrative of progress in international law. European
Journal of International Law, 23(1):255â€“266, 2012.

KSGM: Keynode-driven Scalable Graph Matching
Xilun Chen, K. SelÃ§uk Candan

Maria Luisa Sapino

Paulo Shakarian

Arizona State University
Tempe, AZ, USA
{xilun.chen, candan}@asu.edu

University of Torino
Torino, Italy
marialuisa.sapino@unito.it

Arizona State University
Tempe, AZ, USA
shak@asu.edu

ABSTRACT
Understanding how a given pair of graphs align with each other
(also known as the graph matching problem) is a critical task in
many search, classification, and analysis applications. Unfortunately, the problem of maximum common subgraph isomorphism
between two graphs is a well known NP-hard problem, rendering
it impractical to search for exact graph alignments. While there
are several heuristics, most of these analyze and encode global and
local structural information for every node of the graph and then
rank pairs of nodes across the two graphs based on their structural
similarities. Moreover, many algorithms involve a post-processing
(or refinement) step which aims to improve the initial matching
accuracy. In this paper 1 we note that the expensive refinement
phase of graph matching algorithms is not practical in any application where scalability is critical. It is also impractical to seek
structural similarity between all pairs of nodes. We argue that a
more practical and scalable solution is to seek structural keynodes
of the input graphs that can be used to limit the amount of time
needed to search for alignments. Naturally, these keynodes need to
be selected carefully to prevent any degradations in accuracy during the alignment process. Given this motivation, in this paper,
we first present a structural keynode extraction (SKE) algorithm and
then use structural keynodes obtained during off-line processing
for keynode-driven scalable graph matching (KSGM). Experiments
show that the proposed keynode-driven scalable graph matching algorithms produce alignments that are as accurate as (or better than)
the state-of-the-art algorithms, with significantly faster online executions.

1.

!"#$%&'&

!"#$%&(&

Figure 1: Graph matching/alignment problem seeks a maximum common subgraph isomorphism between two input graphs
they represent the pairwise relationships between the nodes of the
graph. Edges can be directed or undirected, meaning that the relationship can be non-symmetric or symmetric, respectively. Nodes
and edges of the graph can also be labeled or non-labeled. The
label of an edge, for example, may denote the name of the relationship between the corresponding pair of nodes or may represent
other meta-data, such as the certainty of the relationship or the cost
of leveraging that relationship within an application.
Due to the success of the graph model as a powerful and flexible data representation, graph analysis and search tasks are also
increasingly critical in many application domains. In particular,
understanding how a given set of graphs align with each other (also
known as the graph matching/alignment problem, Figure 1) forms
the core task in many search, classification, and analysis applications. Unfortunately, the problem of maximum common subgraph
isomorphism between two graphs is a well known NP-hard problem [24], making it impractical to search for exact or maximal
graph alignments. As a result, while there are some attempts to
improve the performance of exact maximum common subgraph
matching solutions [23], most of the recent efforts in the area have
focused on seeking approximate/inexact graph alignments [3, 18,
22, 19, 29].
While these algorithms differ in their specific techniques, most
of them rely on a four phase process:

INTRODUCTION

Graphs have been used to represent a large variety of complex
data, from multimedia objects, social networks, hypertext/Web,
knowledge graphs (RDF), mobility graphs,to protein interactions.
Let D be a set of entities of interest, a graph, G(V, E), defined over
V = D describes the relationships between pairs of objects in D.
The elements in the set V are referred to as the nodes or vertices of
the graph. The elements of the set E are referred to as the edges and
1
This work is supported by NSF Grants #1339835 and #1318788.
This work is also supported in part by NSF grant #0856090.

1. First, the matching algorithm analyzes and encodes the
global structural information (for example a spectral signature [23]) corresponding to the nodes of the graph.
2. Secondly, the algorithm analyzes and encodes the local structural information (such as neighborhood degree distribution [29]) for the nodes of the graph.
3. Once these global and local signatures are encoded, the
matching algorithm compares the signatures of pairs of
nodes across the given graphs to rank these pairs of nodes
(for example using a stable matching algorithm, like the Hungarian algorithm [16]) based on their overall structural similarities.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org.
CIKMâ€™15, October 19â€“23, 2015, Melbourne, Australia.
c 2015 ACM. ISBN 978-1-4503-3794-6/15/10 ...$15.00.

DOI: http://dx.doi.org/10.1145/2806416.2806577.

1101

)&

)&
)&

)&

)&
)&

*&

)&
)&

!"#$%&'&

)&

)&
)&

)&

!"#$%&(&

Figure 2: Keynode selection problem for scalable graph matching: the nodes marked with "*" are keynodes of the input
graphs that can be used to reduce the amount of time needed to
search for alignments
4. Finally, a post-processing, or refinement, step (involving, for
example, a vertex cover operation) is used to improve the
accuracy of the initial matching [29].

2.

BACKGROUND AND RELATED WORK

In this section, we review key concepts related to the graph
matching problem and discuss the existing algorithms.
Graph Isomorphism: Given two graphs G and H, G is isomorphic to H if there exists a bijective mapping from the nodes of G
to the nodes H that preserves the edge structure [13]: for any two
vertices that are adjacent on G, the vertices they are mapped to are
also adjacent on H, and vice versa.
Subgraph Isomorphism: Subgraph isomorphism seeks a bijective
function, f , such that there is a subgraph G0 of G and a subgraph
H 0 of H, such that G0 is isomorphic to H 0 , with respect to f .
Maximum Common Subgraph Isomorphism: Maximum common subgraph isomorphism seeks the largest subgraph of G isomorphic to a subgraph of H [24]. Intuitively, the larger the maximum common subgraph of two graphs is, the more similar the
graphs are to each other.
One of the first exact graph matching algorithms was proposed
by Ullman [24]. An alternative way to search for a matching between two graphs is to rely on graph edit distance algorithms:
given two graphs the corresponding graph edit distance is the least
cost sequence of edit operations that transforms G1 into G2 . Commonly used graph edit operations include substitution, deletion, and
insertion of graph nodes and edges. Unfortunately, the graph edit
distance problem is also known to be NP-complete [24]. In fact,
even approximating graph-edit distance is very costly; the edit distance problem is known to be APX-hard [8]. [8] shows that graph
isomorphism, subgraph isomorphism, and maximum common subgraph problem are special instances of the graph edit distance computation problem. Many subgraph isomorphism search algorithms
have been developed, such as [15, 29, 14].
Approximate Graph Matching: In order to be applicable to large
graphs, many heuristic and approximate graph matching algorithms
have been proposed.
While, as we discussed above, graph matching through edit distance computation is an expensive task, there are various heuristics
that have been developed to perform this operation more efficiently.
GraphGrep [14] is one such technique, relying on a path-based
representation of graphs. GraphGrep takes an undirected, nodelabeled graph and for each node in the graph, it finds all paths that
start at this node and have length up to a given, small upper bound,
lp . Given a path in the graph, the corresponding id-path is the list
of the ids of the nodes on the path. The corresponding label-path is
the list of the labels of the nodes on the path. The fingerprint of the
graph, then, is a hash table, where each row contains the hash of the
label-path and the corresponding number of id-paths in the graph.
Irrelevant graphs are filtered out by comparing the numbers of idpaths for each matching hash key and by discarding those graphs
which have at least one value in its fingerprint less than the corresponding value in the fingerprint of the query. Matching sub-graphs
are found by focusing on the parts of the graph which correspond
to the label-paths in the query. After, the relevant id-path sets are

Unfortunately, many of these steps result in significant scalability
challenges in terms of the matching time needed to compare the
pairs of nodes:
â€¢ In particular, the expensive refinement phase of graph matching algorithms is not practical in applications where scalability of the graph matching operation is critical.
â€¢ Moreover, especially in very large graphs, it is also impractical to seek pairwise structural similarities for all node pairs
during the graph matching process.

1.1

Organization of this Paper

The paper is organized as follows: in the next section, we first
introduce basic concepts and review existing graph matching algorithms. In Section 3, we provide overviews of the general graph
matching process as well as the proposed keynode-driven scalable
graph matching (KSGM) algorithm. Then, in Section 4, we present
our structural keynode extraction (SKE) algorithm. In Sections 5
and 6, we discuss how to use these structural keynodes for obtaining graph alignments. We discuss the complexity of the proposed algorithms and parallelization opportunities in Section 7. We
present experimental evaluations with various real and synthetic
data sets in Section 8. These confirm that the proposed approximate graph matching algorithm is highly effective and efficient.
Finally, we conclude the paper in Section 9.

)&

)&
)&
)&

1.2

)&

Contributions of this Paper

Based on these observations, in this paper, we argue that a more
practical and scalable solution would be to seek structural keynodes of the input graphs that can be used to reduce the amount of
time needed to search for alignments (Figure 2). Of course, these
keynodes must be selected carefully to prevent any degradations in
accuracy during the alignment process, especially because, as mentioned above, refinement post-processes are detrimental to scalability of matching algorithms.
Given this motivation, in this paper, we first present a highly efficient and effective structural keynode extraction (SKE) algorithm.
The SKE algorithm, which is executed off-line, relies on a 3-step
process:
1. In the first step, a PageRank algorithm [7] is ran to associate
a structural score to each node in the graph.
2. In the second step, a scale-space (based on a difference-ofGaussians (DoG) function defined over different scales of the
graph) is constructed.
3. In the third step, keynode candidates are extracted by analyzing the resulting scale-space for extrema of the DoG function and a subset of these candidates are selected as structural
keynodes.
We then propose a graph matching algorithm that uses these structural keynodes (obtained during off-line processing) for keynodedriven scalable graph matching (KSGM). In particular, KSGM extracts only local signatures and relies on the structural keynodes for
fast node-to-node similarity searching. In addition, we also show
that this keynode-driven approach not only reduces the number of
comparisons that need to be performed online, but it also enables
effective matching, even without having to rely on an expensive assignment algorithm, like the Hungarian algorithm (with O(|V |3 )
complexity). Experiment results show that the proposed structural
keynode extraction and keynode-driven scalable graph matching
algorithms produce alignments that are as accurate as (or better
than) the state-of-the-art algorithms, while requiring significantly
less online execution time without refinement.

1102

Algorithm 1 Overview of keynodes based graph matching
Input:
A set G = {G1 , G2 , ...Gg } of graphs
A query graph Gq âˆˆ G.
Output:
Rank Gi âˆˆ G in terms of matching quality
Offline process:
1: for all Gi âˆˆ G (including Gq ) do
2:
Perform structural keynode extraction (SKE) for Gi
3:
Extract local-signatures for all nodes in Gi
4:
(Optional) Extract global-signatures for all nodes in Gi
5: end for
Online process:
6: for all Gi âˆˆ G do
7:
Compute local similarities for keynode pairs from Gi and
Gq .
8:
(Optional) Compute global similarities for keynode pairs
from Gi and Gq and combine these with local similarities.
9:
Select anchors to obtain a base matching
10:
Expand the base matching to obtain Mq,i
11:
Compute matching quality, quality(Mq,i )
12: end for
13: Rank Gi âˆˆ G in terms of quality(Mq,i )

selected and overlapping id-paths are found and concatenated to
build matching sub-graphs.
A common method to obtain an approximate graph matching
is to use the eigenvectors derived from the adjacency matrix of
the graph [23]: intuitively, two similar graphs should have similar eigenvectors; moreover, if we construct a |V | Ã— |V | matrix (for
example the Laplacian of the graph or a matrix encoding node distances) and decompose it into three matrices of |V | Ã— c, c Ã— c,
and c Ã— |V | elements using an eigen-decomposition technique like
SVD, the c-length vector corresponding the node v âˆˆ V can be
used as a global-signature corresponding to node v. Once node-tonode similarities are computed, an assignment is usually found using an assignment algorithm, such as the Hungarian algorithm [16],
which uses a primal-dual strategy to solve this problem in O(|V |3 )
time. This simple observation, led to several works leveraging different global-signatures for identifying node matches across different graphs [3, 18, 22, 19, 29]. [28] formulates the labeled weighted
graph matching problem in the form of a convex-concave program,
which searches for appropriate permutation matrices by solving a
least-square problem. In addition, feature selection techniques are
used for more accurate calculation [11, 12, 20]. In order to improve
matching accuracy, [29] proposes to enrich the global-signatures
associated to the graph nodes with local-signatures, encoding the
properties of the immediate neighborhood of each node.

3.

OVERVIEW OF KEYNODE-DRIVEN
GRAPH MATCHING

combines (by multiplying) the global and local similarities
of each pair of nodes into a single value, thereby quantifying
the overall similarity of the pair.
4. Once the overall similarities for |Vq | Ã— |Vi | pairs of nodes
are computed, [29] drops node pairs with small degrees and,
then, expands the remaining set of anchor pairs by adding,
in an iterative manner, immediate good nearby pairs to this
anchor set.
5. When no more pairs can be added to the anchor set, [29] uses
the Hungarian algorithm to identify an initial node matching
in O(max{|Vq |, |Vi |}3 ) time.
6. Finally, as a post-processing step, [29] applies a vertex cover
based refinement, which explores different subsets of the
nodes and searches for better alignments than the one initially identified. In particular, the algorithm seeks small vertex covers, which are likely to give the mismatched nodes additional chances to be refined. Note that since the minimum
vertex cover problem is known to be NP-hard, the algorithm
searches for minimal vertex covers in O(mÃ—n3 ) time, where
m = min{|Eq |, |Ei |} and n = max{|Vq |, |Vi |}.

Given a set G = {G1 , G2 , ..., Gg } of graphs and a query graph
Gq âˆˆ G, in this paper, we seek the maximum graph matching between Gq and all Gi âˆˆ G (i 6= q). Note that the exact solution for
this problem is NP-hard [24]. Since we treat scalability as a key
constraint, we consider inexact solutions and rely on the matching quality measure proposed in [29] to evaluate the accuracies of
the resulting alignments: Let Gq (Vq , Eq ) be a query graph and let
Gi (Vi , Ei ) be a graph in G. Let Mq,i (Vq,i , Eq,i ) be a subgraph of
both Gq and Gi , returned by an inexact subgraph search algorithm.
[29] defines the matching quality function as follows:
quality(Mq,i ) =

|Eq,i |
,
min(|Eq |, |Ei |)

Intuitively, the quality function describes how similar the given
query graph Gq and known graph Gi are by using the ratio of
matched edges and the maximum number of edges that can be possibly matched, which is equal to the minimum number of edges between two graphs. In other words, the larger the number of edges
in the graph Mq,i , the better is the quality of the matching (or the
more similar the two graphs are).

3.1

This process includes a number of very expensive steps: The first
two steps, involving global and local analysis are expensive, but can
be performed off-line and indexed for later reuse assuming that the
graphs are available ahead of time. The last four steps, however,
need to be performed on-line, yet they consist of operations that
are quadratic or higher. In particular, the last refinement step, with
O(m Ã— n3 ) time cost is impractical for most large data graphs.
In this paper, we note that Step 3 can be significantly sped up
if the similarity computations are limited to only a small subset of
the vertices in Vq and Vi (which we refer to as keynodes of Vq and
Vi ). However, the use of keynodes for node similarity computation
is not sufficient to reduce the overall complexity as, once the keynodes are identified and the keynode pairs set is expanded, solving
the assignment problem needed to return the matching would still
take O(max{|Vq |, |Vi |}3 ) time, if we were to apply the Hungarian
algorithm on the extracted keynodes. Therefore, we also need to reduce the time complexity of this step significantly. It is especially
important that the initial keynode based similarity computation is
accurate as we cannot afford a cubic algorithm like Hungarian algorithm to return a high-quality matching.

Challenges

Given a query graph Gq , our goal is to rank the graphs in G according to their matching similarities against Gq (and eventually
return the top few matches to the user). [29] solves this problem by
relying on a 6-step process, common to many graph search algorithms:
1. [29], first, analyzes the global structure of each graph
through eigen-decomposition of the graph Laplacian matrix
and encodes this in the form of a c-length vector associated
to each node in the graph.
2. Secondly, [29] encodes the structural information local to
each node, vj , in the form of an sj -length degree distribution vector, where sj is the number of nodes in the kneighborhood of the node.
3. Given the global and local signatures of all nodes in Gq and
Gi , [29] then computes the global and local similarities for
each pair of nodes from the two graphs, in O(|Vq | Ã— |Vi | Ã— c)
and O(|Vq | Ã— |Vi | Ã— maxvj (sj )) time, respectively. It then

1103

3.2

Outline of KSGM
Algorithm 1 illustrates an overview of the keynode-driven scalable graph matching (KSGM) process. In the rest of the paper, we
study each step in detail. First, in the next two sections, we focus on
the offline steps of KSGM, which involve identifying keynodes and
extracting local-signatures. The online steps of the KSGM algorithm
are discussed in Section 6.
4.

4.2

We note that the above alternative has a number of disadvantages:
â€¢ First of all, many of the structural significance measures,
such as PageRank, are not entirely robust against modifications in the graph. The PageRank score of a node, for example, can jump significantly, if a new edge connects the
sub-graph in which the node is contained to a high PageRank node in the graph.
â€¢ Secondly, common structural significance measures, like
PageRank, capture the significance of a node in the whole
graph and favor nodes that are overall central. However, this
may be disadvantageous as there is a possibility that smaller
scale, but distinct (and, therefore, useful for matching) structural features of the graph may be missed.
We therefore argue that we need a better alternative, which is both
robust and multi-scale. We build the proposed SKE algorithm based
on three key insights:
â€¢ Robustness: Even when the PageRank scores of the nodes
themselves vary due to graph transformations, such as edge
insertions and removals, a given nodeâ€™s PageRank score relative to the scores of the nodes in its neighborhood is likely
to be stable.
â€¢ Structural distinctiveness: A node is structurally distinctive
in its neighborhood, if "the relationship between its PageRank score to the PageRank scores of its neighbors" is different from the "relationships between the nodeâ€™s neighborsâ€™ PageRank scores and the PageRank scores of their own
neighbors".
â€¢ Multi-scale: Since we do not know the scale of the structurally distinctive features of the graph, we need to search
for features of potentially different sizes.
It is important to note that similar requirements also exist in
other application domains. For example, algorithms for extracting such robust, local features have been developed for 2D images
(SIFT [21]), uni-variate time series [9], and multi-variate time series [25]. In this paper, we argue that a similar process can be used
to identify keynodes (corresponding to robust, multi-scale structural features) of a graph, if the nodes are annotated with PageRank
scores ahead of the time. Let G(V, E, p) be a PageRank-labeled
graph, where p() is a mapping from the nodes to the corresponding PageRank scores. What makes the problem of extracting local features from PageRank-labeled graphs challenging is that the
concepts of neighborhood, gradient, and smoothing are not welldefined for graphs.
Therefore, before we describe the keynode extraction process,
we describe how to smooth a PageRank-labeled graph. Intuitively,
smoothing the graph with respect to the scores associated to the
graph nodes creates versions of the given graph at different resolutions and, thus, helps identify features with different amounts of
details.

STRUCTURAL KEYNODE EXTRACTION

In this section, we propose an off-line structural keynode extraction (SKE) algorithm which identifies Î˜% (where Î˜ is a user
provided parameter) of the nodes in V as the keynode set, K, of
a given graph, G(V, E) to support scalable graph matching. The
proposed SKE algorithm has a number of advantages: (a) First of
all, the identified keynodes are robust against noise, such as random edge insertion/removal; and (b) the identified nodes represent
structural features of the graph of different sizes and complexities
(i.e., correspond to neighborhoods of different sizes).

4.1

Proposed Solution - Robust Keynode Extraction through Scale Space Analysis

Naive Solution - Selecting Structural
Keynodes based on Node Significance

As described above, the keynodes of the graph need to represent
the structural properties of the graph well (i.e., extracted keynodes
need to be structurally significant in the graph) to support effective
matching. Therefore, the first alternative is to rely on traditional
node significance measures.
Measures like betweenness [26] and the centrality/cohesion [5],
help quantify how significant any node is on a given graph based on
the underlying graph topology. The betweenness measure [26], for
example, quantifies the number of shortest paths that pass through
a given node. The centrality/cohesion [5] measures quantify how
close to a clique the given node and its neighbors are. Other authority, prestige, and prominence measures [4, 7, 5] quantify the
significance of the node in the graph through eigen-analysis or random walks, which help measure how reachable a node is in the
graph. PageRank [7] is one of the most widely-used random-walk
based methods for measuring node significance and has been used
in a variety of application domains, including web search, biology,
and social networks. The basic thesis of PageRank is that a node
is important if it is pointed to by other important nodes â€“ it takes
into account the connectivity of nodes in the graph by defining the
score of the node vi âˆˆ V as the amount of time spent on vi in a sufficiently long random walk on the graph. Given a graph G(V, E),
the PageRank scores are represented as ~r, where
~r = Î±TG~r + (1 âˆ’ Î±)~t
where TG is a transition matrix corresponding to the graph G, ~t is
a teleportation vector (such that ~t[i] = |V1 | ), and Î± if the residual
probability (or equivalently, (1 âˆ’ Î±) is the so-called teleportation
probability). Unless the graph is weighted, the transition matrix,
TG , is constructed such that for a node v with k (outgoing) neighbors, the transition probability from v to each of its (outgoing)
neighbors will be 1/k. If the graph is weighted, then the transition probabilities are adjusted in a way to account for the relative
weights of the edges.
Therefore, as the first alternative, we consider a PageRank based
keynode selection scheme: in this scheme, given a graph G(V, E),
we would (a) first identify the PageRank scores, p(vi ), of all vi âˆˆ
V , then (b) we would rank the nodes in non-increasing order of
PageRank scores, and finally, (c) we would return the top Î˜% of
the nodes in V as the keynode set, K.

4.2.1

Gaussian Smoothing of a PageRank-Labeled
Graph

1D or 2D data are commonly smoothed by applying a convolution operation with a Gaussian window. For example, if Y =
ht0 , t1 , . . . , tl i is a time series data and Ïƒ is a smoothing parameter,
its smoothed version, YÌƒ (t, Ïƒ), is obtained through G(t, Ïƒ) âˆ— Y (t)
where âˆ— is the convolution operation in t and G(t, Ïƒ) is the Gaussian function
âˆ’t2
1
G(t, Ïƒ) = âˆš
e 2Ïƒ2 .
2Ï€Ïƒ
Essentially, the Gaussian smoothing process takes a weighted
average of values of the points in the vicinity of a given point, t.

1104

The closer a point to t, the higher is the weight. Therefore, in order
to implement a similar Gaussian smoothing of the given graph, we
first need to define a distance function to measure how close different nodes are to each other. Common applicable definitions of
node distance include the hop distance (determined by the shortest
edge distance between the nodes on the given graph) or hitting distance [10]. In this paper, we use hop distance to measure how far
nodes are to each other:

!"#$%"&'()"*+$,(,-../0$1(23/0(4#$%"&'4!"((

6!"77(6.8(9$)$*(!"
!"#$%"&'()"*+$,(,-../0$1(23/0(4!75(

â€¢ Nj , for j â‰¥ 0, is an n Ã— n 0, 1-valued matrix, where for a
given node vi in the graph, G, the ith row in the matrix, Nj
is 1 only for nodes that have node distance exactly j from the
node vi , and

!#

!"""""#

D EFINITION 1 (N ODE D ISTANCE M ATRIX ). Let us be given
a graph G(V, E) with n nodes. The ordering among the nodes is
described through a set of node distance matrices, Nj , where

6!75(77(6.8(9$)$*(;!75<(

6:(77(6.8(9$)$*(:(

!"#$%"&'()"*+$,(,-../0$1(23/0(45(

65(77(6.8(9$)$*(5(
!"#$%"&'()"*+$,(,-../0$1(23/0(4-3&(

â€¢ Nj , for j â‰¤ 0, is an n Ã— n 0, 1-valued matrix, where for a
given node vi , the ith column in the matrix, N(j, G) = 1
only for nodes that have distance exactly j on the inverted
graph, where all edges are inverted.


Figure 3: Computing the Difference-of-Gaussian (DoG) series
of the PageRank values of a graph

Intuitively, the cell Nj [v1 , v2 ] = 1 if the node v2 is exactly j hops
from v1 . When j is positive the hop-distance is measured following
outgoing edges, whereas when j is negative, incoming edges are
followed. Given this, we construct multiple scales of the given
graph G by using a Gaussian graph smoothing function defined as
follows.

Intuitively, the smoothing function S applies Gaussian smoothing
on the X values (associated with the nodes, vi âˆˆ V ) based on the
hop-distances between nodes and returns a vector
SG,Ïƒ (X) = hxÌƒ(v1 ), xÌƒ(v2 ), . . . , xÌƒ(vn )i

D EFINITION 2 (G AUSSIAN G RAPH S MOOTHING F UNCTION ).
Let us be given a labeled graph G(V, E, x) and let
â€¢ Ïƒ be a smoothing parameter.
â€¢ X = hx(v1 ), x(v2 ), ..., x(vn )i be a vector encoding the labels associated with the nodes, vi âˆˆ V .
Then, if G is a directed graph, the non-normalized Gaussian graph

smoothing function, SG,Ïƒ
() is defined as
n
X

G(j, Ïƒ)Nj X
SG,Ïƒ (X) = G(0, Ïƒ)IX +

encoding the smoothed X values associated with the graph nodes.
Note that, since at a given hop distance there may be more than
one node, all the nodes at the same distance have the same degree
of contribution and the degree of contribution gets progressively
smaller as we get further away from the node for which the smoothing is performed.
Therefore, given a PageRank-labeled graph, G(V, E, p), and a
corresponding PageRank vector, P = hp(v1 ), p(v2 ), ..., p(vn )i,
encoding PageRank scores associated with the nodes, vi âˆˆ V , the
vector
SG,Ïƒ (P ) = hpÌƒ(v1 ), pÌƒ(v2 ), . . . , pÌƒ(vn )i,

j=1

+

n
X

encodes the Ïƒ-smoothing of the PageRank-annotated graph,
G(V, E, p). We also say that SG,Ïƒ (P ) encodes the PageRank
scores of G at scale Ïƒ.
We next describe how to construct a scale-space for the given
graph through an iterative smoothing process leveraging the PageRank vector and the structure of the graph.

G(j, Ïƒ)Nj X,

j=1

where G(0, Ïƒ) is a Gaussian function with zero mean and Ïƒ standard deviation. If, on the other hand, G is an undirected graph, then
the non-normalized Gaussian graph smoothing function is

SG,Ïƒ
(X)

= G(0, Ïƒ)IX +

n
X

4.2.2

Graph Scale-Space Construction

The first step in identifying robust graph features is to generate a
scale-space representing versions of the given graph with different
amounts of details. In particular, building on the observation that
features are often located where the differences between neighboring regions (also in different scales) are large, we seek structural
features of the given graph at the extrema of the scale space defined
by the difference-of-the-Gaussian (DoG) series. More specifically,
given
â€¢ a PageRank-labeled graph, G(V, E, p),
â€¢ the corresponding vector, P = hp(v1 ), p(v2 ), ..., p(vn )i encoding the scores associated with the nodes, vi âˆˆ V ,
â€¢ a minimum smoothing scale, Ïƒmin ,
â€¢ a maximum smoothing scale, Ïƒmax ,
â€¢ the number, l, of levels of the scale space,
then, we compute a difference-of-Gaussians (DoG) series,
D(G, P, Ïƒmin , Ïƒmax , l) = {D1 , D2 , ..., Dl }, where each Di encodes the differences of two nearby scales separated by a multiplicative factor k:

2G(j, Ïƒ)Nj X.

j=1

Intuitively, S  applies Gaussian-based weighted averaging to the
entries of vector X based on the hop-distances2 . However, unlike
the basic Gaussian smoothing, during (non-normalized) relationship smoothing, there may be more than one node at the same distance and all such nodes have the same degree of contribution. As
a consequence, the sum of all contributions may exceed 1.0. Therefore, the normalized Gaussian graph smoothing function, S(G, Ïƒ),
discounts weights based on the number of nodes at a given distance:

 



SG,Ïƒ (X) = SG,Ïƒ
X Ã· SG,Ïƒ
1(n) ,
where 1(n) is an n-vector such that all values are 1 and â€œÃ·â€ is a
pairwise division operation.

2
In practice, since the Gaussian function drops fast as we move
away from the mean, we need to consider only a small window, w,
of hops

Di = SG,ki Ïƒmin (P ) âˆ’ SG,kiâˆ’1 Ïƒmin (P ),

1105

56'
-2'

-3'

â€¢ DoG-neighbors numbered #2 and #7 correspond to the
DoG values of the same node at the previous and next levels of the scale space. Therefore, we have

!"#$%&'(),+'

-4'

Nhvi ,Ïƒj i [2] = Diâˆ’1 [j] and Nhvi ,Ïƒj i [7] = Di+1 [j].

!"#$%&'()'

-0'

7)869'

-+'

-.'

-1'
!"#$%&'()*+'

â€¢ In contrast, DoG-neighbors #3, #5, and #8 correspond to
the (average) DoG values of the forward neighbors of the
node vj , at the previous, current, and next levels of the scale
space, respectively. Therefore, we have

-/'

Nhvi ,Ïƒj i [3] = (FDiâˆ’1 ) [j],

Figure 4: Extrema detection

Nhvi ,Ïƒj i [5] = (FDi ) [j], Nhvi ,Ïƒj i [8] = (FDi+1 ) [j],
where k =

q
l

Ïƒmax
.
Ïƒmin

Figure 3 visualizes the process:

where, F is a row-normalized adjacency matrix accumulating
the (averaged) contributions of the nodes to their neighbors
along the forward edges.

â€¢ On the left hand side of the figure, we have the incrementally smoothed versions of the PageRank vector, P . Here,
the lowest level, SG,Ïƒmin (P ), corresponds to the most detailed version of the graph (with the least amount of smoothing), whereas SG,Ïƒmax (P ) corresponds to the least detailed
(most smoothed) version of the graph. In other words, Ïƒmin
determines the sizes of the smallest structural features we
can locate and Ïƒmax = kl Ïƒmin determines the sizes of the
largest structural features we can identify. In particular, since
under Gaussian smoothing, a diameter of 6Ïƒ would cover
âˆ¼ 99.73% of the weights, the diameter of the smallest structural feature that can be identified using SKE is âˆ¼ 6Ïƒmin
hops, whereas the diameter of the largest feature would be
âˆ¼ 6Ïƒmax hops.

â€¢ Similarly, DoG-neighbors #1, #4, and #6 correspond to
the (average) DoG values of the backward neighbors at the
previous, current, and next levels of the scale space. Therefore, we have
Nhvi ,Ïƒj i [1] = (BDiâˆ’1 ) [j],
Nhvi ,Ïƒj i [4] = (BDi ) [j], Nhvi ,Ïƒj i [6] = (BDi+1 ) [j],
where, B is a row-normalized backward-adjacency matrix
(where all edges are reversed) accumulating the (averaged)
contributions of the nodes to their neighbors along the backward direction of the edges.
Given these, the pair hvj , Ïƒi i is an extremum (i.e., vj is a keynode
candidate at scale Ïƒi ), iff Di [j] is a local maximum

The number of levels, l, denotes the number of detail levels
(or scales) we explore between Ïƒmin and Ïƒmax . Intuitively,
each of these levels corresponds to a different target size for
the structural features of the graph.

Di [j] â‰¥ M AX Nhvj ,Ïƒi i [h]
1â‰¤hâ‰¤8

â€¢ On the right hand side of the figure, we have the resulting
Difference-of-Gaussian (DoG) series, consisting of vectors,
D1 through Dl .

or it is a local minimum
Di [j] â‰¤ M IN Nhvj ,Ïƒi i [h].
1â‰¤hâ‰¤8

Note that, intuitively, Di [j] measures how different the PageRank
values of the neighborhood around vj at scale Ïƒiâˆ’1 (= kiâˆ’1 Ïƒmin )
are from the PageRank values of the neighborhood around vj at
scale Ïƒi (= ki Ïƒmin ).
Therefore, a large Di [j] value would indicate a major structural
change when neighborhoods of different size around vj are considered (e.g., a node with a high PageRank score is included when
considering a neighborhood of larger scale). In contrast, a small
Di [j] indicates that there is minimal structural change when considering neighborhoods of different scales.

4.2.3

Intuitively, since Di [j] measures how different the PageRank values of the neighborhood around vj at scale Ïƒi are from the PageRank values of the neighborhood around vj at scale Ïƒiâˆ’1 , a local
maximum corresponds to a highly scale-sensitive region (amidst
relatively scale-insensitive regions), whereas a local minimum corresponds to a scale-insentive region (amidst more scale-sensitive
regions), of the graph.

4.2.4

Selecting the Best Keynodes

In the final step, we need to rank the keynode candidates and
Î˜
return the top 100
Ã— |V | of them, where Î˜ is a user provided parameter, as the keynode set, K. We propose Extremum Ranking to
select the best keynodes.
Since keynodes are located at the local extrema of the DoG series, we can rank the keynode candidates based on their extremum
score defined as follows: Let the pair hvj , Ïƒi i be a local extremum.
The corresponding extremum score, Î¾(hvj , Ïƒi i), is defined as


ï£±
ï£´
if hvj , Ïƒi i is max.
ï£´ Di [j] âˆ’ M AX Nhvj ,Ïƒi i [h]
ï£´
ï£²
1â‰¤hâ‰¤8


ï£´
ï£´
ï£´
ï£³ M IN Nhv ,Ïƒ i [h] âˆ’ Di [j] if hvj , Ïƒi i is min.
j i

Identifying Keynode Candidates

As we mentioned earlier, our intuition is that a graph node is
structurally distinctive in its neighborhood, if "the relationship between its PageRank score to the PageRank scores of its neighbors"
is different from the "relationships between the nodeâ€™s neighborsâ€™
PageRank scores and the PageRank scores of their own neighbors,
at multiple scales". Therefore, to locate the keynode candidates,
we focus on the local extrema of the difference-of-Gaussian (DoG)
series D. More specifically, we identify hvj , Ïƒi i pairs where the
DoG value for node vj at scale, Ïƒi = ki Ïƒmin , is an extremium
(maximum and/or minimum) with respect to the neighbors of vj in
the same scale as well as neighbors in the previous and next levels
of the scale space.
In order to verify if the pair hvj , Ïƒi i is an extremium or not,
we compare Di [j] with the values corresponding to eight DoGneighbors in the scale-space, as visualized in Figure 4:

1â‰¤hâ‰¤8

Intuitively, the higher the extremum score is, the better local extremum (and, thus, a better keynode) is hvj , Ïƒi i.

1106

5.

LOCAL NODE SIGNATURES

nbhd_sim(vi , vj ) proposed by [29] accounts for the alignment between the degree distributions in these neighborhood graphs3 :

The next step in the process is to extract the local signatures (to
be used to compute local node similarities) for the nodes in the
graph. Note that this process is also offline.
While there are different local signatures proposed in the literature, in our work we build on the k-neighborhood degree distribution based local signature proposed in [29] (both because it is
simple and effective and also because this helps us compare our
keynode-driven approach to the approach proposed in [29] more
directly). Briefly, for each node vj âˆˆ V and for a user provided k,
[29] first identifies the set, Nk (vj ) âŠ† V , of nodes that are at most
k hops from vj and extracts a subgraph, Gk (vj ) âŠ† G, induced by
vj and its k-hop neighbors. Then the degree sequence,

nbhd_sim(vi , vj ) =
where

dmin = min{degree(vi ), degree(vj )}
nmin = min{Î½i , Î½j }
P min âˆ’1
min{di,h , dj,h }
dmin + n
h=1
.
D(vi , vj ) =
2

Node Pair Ranking with Extended Similarity.

Îºj = [dj,1 , dj,2 , . . . , dj,|Nk (vj )| ]

While the local neighborhood similarity computation we use is
similar to the one proposed in [29], we rank pairs of nodes differently. Let vi and vj be two nodes (from two different graphs). In
particular, [29] ranks the pair hvi , vj i of nodes based on their neighborhood similarities, nbhd_sim(vi , vj ). We, however, argue that
neighborhood similarity is not sufficient for accounting for how effective the node pair is in supporting expansion. More specifically,
we observe that a pair, hvi , vj i, is likely to be a better anchor for
expansion than the pair hva , vb i if not only (a) the neighborhoods
of vi and vj are more similar to each other than the pair, va and
vb , but also (b) if vi and vj have degrees that are more aligned with
each other than va and vb . Based on this observation, instead of applying a degree threshold, we propose that the pair hvi , vj i should
be ranked based on the ranking function

consisting of the degrees of nodes in Gk (vj ) (excluding vj ), sorted
in non-increasing order, along with the degree of the node vj and
the numbers of vertices and edges in its k-hop neighborhood, form
the local signature of node vj :
local_signature(vj ) = hÎºj , degree(vj ), Î½j , Îµj i,
where Î½j = |Nk (vj ) âˆª {vj }| is the number of nodes in Gk (vj ) and
Îµj = |Ek (vj )| is the number of edges.
Note that, while we use a local signature similar to that proposed
in [29], we extend the node pair ranking function to better account
for the node degrees as discussed later in Section 6.1.1. As we see
in Section 8, this extension provides a significant boost in accuracy.

6.

(KEYNODE-BASED) GRAPH MATCHING

Ï(vi , vj ) = nbhd_sim(vi , vj )Ã—

As discussed in Section 3 and outlined in Algorithm 1, once the
keynodes are extracted and local signatures are computed offline,
the next steps of the algorithm are to

6.1.2

We now describe how these steps are implemented in the keynodedriven scalable graph matching (KSGM) algorithm.

Keynode pair Selection

[29] uses the Hungarian algorithm to identify an initial node
matching in O(n3 ) time, where n = max{|V1 |, |V2 |}. To reduce
the execution time, [29] prunes those node pairs for which the similarity is â‰¤ 0.5. Since, instead of considering the node pairs in
V1 Ã— V2 , we only need to consider pairs of nodes in K1 Ã— K2 , and
since |K1 |  |V1 | and |K2 |  |V2 |, keynode-driven processing
is likely to be faster even without using the threshold. However,
the cubic time of the Hungarian algorithm is still prohibitive and
impractical for scalable graph matching. Therefore, we propose a
greedy anchor selection algorithm, which (as we see in Section 8)
performs very well when used along with keynodes selected in Section 4 and the proposed ranking function, Ï(). In particular, we first
include all keynode pairs in K1 Ã— K2 into a queue in the order of
their ranks based on Ï(), then, until the queue is empty, we remove
and consider the keynode pair, hv, ui at the head of the queue. If
neither v nor u has been marked anchored, we include hv, ui as
an anchor and we mark v and u as anchored, otherwise, we drop
the pair, hv, ui.
Note that this process has O((|K1 | Ã— |K2 |) Ã— log(|K1 | Ã— |K2 |))
time cost (instead of the cubic cost of the Hungarian algorithm) and,

Anchor Set Selection

Let G1 (V1 , E1 ) and G2 (V2 , E2 ) be two graphs and let K1 âŠ† V1
and K2 âŠ† V2 be the corresponding keynodes identified by the SKE
algorithm proposed in Section 4. The next step is to select a subset,
A, of the pairs of nodes in K1 Ã— K2 as the anchor set of alignments
based on a ranking function (a) evaluating how structurally similar
a pair of nodes are and (b) how likely they are to lead to an effective
expansion process to discover other alignments.

6.1.1

min{degree(vi ), degree(vj )}
.
max{degree(vi ), degree(vj )}

Note that [29] simply drops node pairs where the minumum of the
two node degrees is smaller than the larger average degree of the
two input graphs. We, however, argue that such node pairs may
be useful, especially if the degrees in the graph are not uniformly
distributed and the maximum matching occurs at the sparse portions of the graph. Therefore, we keep such pairs as long as they
rank highly based on Ï(). We evaluate this ranking function in Section 8.

â€¢ compare the signatures of pairs of nodes across the given
graphs to rank these pairs of nodes,
â€¢ select a set of pairs of keynodes (we refer it as anchor set)
that serve as the base matching, and
â€¢ expand this base matching to obtain Mq,i .

6.1

nmin + D(vi , vj )
,
(Î½i + Îµi )(Î½j + Îµj )

Node Similarity Matching and Node Pair
Ranking

As we discussed in Section 5, KSGM uses a local node signature similar to the one proposed by [29]: hÎºj , degree(vj ), Î½j , Îµj i,
where Î½j is the number of nodes in the neighborhood of
vj , Îµj is the number of neighborhood edges, and Îºj =
[dj,1 , dj,2 , . . . , dj,|Nk (vj )| ] consists of the degrees of nodes in the
k-neighborhood of vj (excluding vj ), sorted in non-increasing order.

3
In addition to using local similarities, [29] also extracts global
signatures along with the local-signatures to compute node similarities. As we see in Section 8, the proposed keynode-driven graph
matching algorithm achieves good results without having to rely on
such global-signatures.

Local Neighborhood Similarity.
Let vi and vj be two nodes (from two different graphs)
and let Gk (vi ) and G0k (vj ) be the corresponding induced
kâˆ’neighborhood graphs.
Then, local similarity function

1107

as we see in Section 8, performs very well in practice. Furthermore,
the nature of Hungarian algorithm, which forces to pair all possible
nodes to produce the optimal bipartite matching for the given two
sets of nodes, is not guaranteed to provide a better matching in this
case. Since the extracted keynodes are not all necessarily perfectly
paired with each other, some keynodes can be a unique feature of
the given graph, which does not align with other graphs, by forcing
them to pair with other keynodes, it in fact introduces a bad initial
base matching, and thus expand into an even worse matching. The
proposed greedy matching algorithm, however, only consider the
highly aligned keynodes, which in practice provides better results
than the optimal bipartite matching.

6.2

more efficient randomized algorithms exist [27]. Once the node
distances have been computed, we construct the scale-space in
O(l Ã— |V | Ã— max_w_nbhd_size), as for each of the l scales, the
score of each node needs to be smoothed considering the scores
of the vertices in its w-hop neighborhood (w is the Gaussian window size and max_w_nbhd_size is the size of the largest w-hop
neighborhood in G).
Once the scale-space is constructed, next, we identify the keynode candidates. This involves O(l Ã— |V |) time, because for each
of the l scales, each node needs to be compared with a constant
number (8) of DoG-neighbors in the scale-space.
Finally, we rank the keynode candidates to select the top K =
Î˜
V many as the keynodes to bootstrap the online matching pro100
cess. Let there be C many keynode candidates. Computing the
ranking scores for these takes O(C) time, because each keynode
candidate needs to be compared with a constant number of DoGneighbors and obtaining the top K takes O(C Ã— log(K)) time.

Matching List Expansion

Because keynodes are inherently sparsely localized, the anchor
set, A, is not necessarily a good final matching for graphs G1 and
G2 . We therefore need to expand this anchor list. Here, we follow [29]â€™s recommendation and expand the list incrementally by
considering the neighbors (and their neighbors) until no effective
expansion is possible (but we use the ranking function Ï() instead
of the node similarity function):

7.1.2

1. we first include all node pairs in A into a ranked queue (i.e.,
max-heap) in the order of their ranks based on the ranking
function, Ï(),
2. then, for each node pair hv, ui âˆˆ A, we also include the node
pairs in neighbors(u) Ã— neighbors(v) in the same ranked
queue

7.2

Online Time Complexity

Let G1 (V1 , E1 ) and G2 (V2 , E2 ) be two graphs. The online process includes the following operations.

3. then, until the ranked queue is empty, we remove and consider the node pair, hv, ui at the head of the ranked queue

7.2.1

Local Similarity Computation for Keynodes

This process has O(|K1 | Ã— |K2 | Ã— compare_length) complexity, where
compare_length = min{max_k_nbhd_size1 , max_k_nbhd_size2 })
since signatures (of length are compared for each pair of nodes in
the keynode sets K1 and K2 .

(a) if either v or u has not yet been marked matched, then
i. we include the pair, hv, ui, in the expanded matching list, L,
ii. we mark both v and u as matched, and
iii. then, the pairs in neighbors(u) Ã— neighbors(v)
are included in the ranked queue
(b) otherwise, we drop the pair, hv, ui

7.2.2

Anchor Set Selection

This greedy process has O((|K1 | Ã— |K2 |) Ã— log(|K1 | Ã— |K2 |))
time cost as each pair off nodes among the keynode sets need to be
considered only once in ranked order.

Once the anchor list is expanded, [29] relies on a post-process,
with time complexity, O(m Ã— n3 ), where m = min{|E1 |, |E2 |}
and n = max{|V1 |, |V2 |}. This step is not scalable due to its prohibitive time complexity. Therefore, the proposed keynode-driven
scalable graph matching (KSGM) algorithm omits this refinement
post-process, due to its high time complexity4 . Instead, the set,
L, of node pairs remaining after the expansion process is directly
returned as the aligned nodes of the matching, M1,2 , for the input
graphs, G1 and G2 .

7.2.3

Anchor Set Expansion

This has O((|V1 | Ã— |V2 |) Ã— log(|V1 | Ã— |V2 |)) worst case time
cost, as in the worst case, all pairs of vertices across the two graphs
may need to be considered for expansion in ranked order.

8.

EXPERIMENTS

In this section, we present experimental evaluations of the proposed keynode-driven scalable graph matching (KSGM) algorithm.
In particular, we compare KSGM to the graph matching algorithm
presented in [29] in terms of efficiency and accuracy.

7. TIME COMPLEXITY ANALYSIS
7.1 Offline Time Complexity

8.1

Let G(V, E) be a graph to be indexed in the database.

7.1.1

Local Signature Extraction

Since the local signature extraction process needs to extract the
k-hop neighborhoods around the nodes, the complexity of this step
is O(|V | Ã— max_k_nbhd_size), where max_k_nbhd_size is the
size of the largest k-hop neighborhood in G. Note that this step
can also leverage the node distance matrix constructed during the
offline keynode extraction process.

8.1.1

Structural Keynode Extraction

Data Sets
Facebook Data Graph

The first data set we used is the Facebook social circles data
graph obtained from the Stanford Large Network Dataset Collection [2]. This is a connected graph with 4039 nodes and 88234
edges. The graph has a diameter of 8 and a 90-percentile effective
diameter of 4.7. For the experiments, we constructed 10 subgraphs
by uniformly sampling connected subsets, containing 60 âˆ’ 70% of
the original graph nodes. Once the subgraphs are obtained, each
of the subgraphs is used as a query against the rest. We report the
averages of execution time and accuracy.

The first step in structural keynode extraction is to obtain the
PageRank scores for the nodes of the two graphs. While, this is an
expensive operation (involving a matrix inversion with O(|V |2.373 )
complexity for a graph with |V | nodes), there are many efficient,
approximate implementations of PageRank, including sublinear approximations [6].
The second step is the creation of an l-layer scale-space for
G. To construct the scale space, we first construct a node distance matrix, which requires an all-pairs shortest path computation, with complexity O(|V |3 ) for a graph with |V | nodes, but

8.1.2

4

Synthetic Graph Data Sets

In addition to the Facebook graph, we also used synthetic graphs,
where we controlled the topology, size, and node degree to explore

Though, in cases where scalability is not critical, this refinement
can be implemented without any change in the rest of the algorithm.

1108

Table 1: Synthetic graph topologies and configurations
Graph topology
Erdos-Renyi (ER)
Power law (PL)

Number of nodes
5000, 7500 (plus 1 to 10%)
5000, 7500 (plus 1 to 10%)

the advantages and disadvantages of the algorithms under different
scenarios.
We generated the synthetic graphs using the well known random
graph generating tool, NetworkX [1]. We consider two common
graph topologies: the Erdos-Renyi (ER) model and the power law
topology (PL) under the Barabasi-Albert model. Table 1 lists the
number of nodes and average degree settings that we used for assessing our algorithms. For each configuration, we generated 10
graphs. Note that, in addition to the base sizes (of 5000 and 7500),
we randomly created an additional 1 to 10% more nodes to ensure
that the different graphs in the data set have slightly different numbers of nodes. As before, once the 10 graphs are obtained for each
configuration, each of the subgraphs is used as query against the
rest. We report the averages of execution time and accuracy.

8.2

Table 2: Experiment results for the Facebook Graph (default
parameters)

Average degree
4, 8, 16
4, 8, 16

KSGM
6.4
38.0%

PR
7.25
34.12%

Random
6.0
15.4%

Extraction time (offline, sec.)

11.5

110.4

0.39

-

Matching time (online, sec.)
Accuracy

2%
6.5
37.6%

3%
6.4
38.0%

4%
6.3
38.7%

6%
6.4
36.4%

8%
7.2
33.2%

Table 4: Impact of the node-pair ranking function, Ï(), for the
Facebook Graph
Matching time (online sec.)
Accuracy

Ï()
6.4
38.0%

w/o Degree
6.0
31.8%

with Global
4.0
22.9%

Table 5: Impact of the local-signature neighborhood size, k, for
the Facebook Graph

Evaluation Criteria

Matching time (online, sec.)
Accuracy

2 hops (default)
6.4
38.0%

3 hops
5.5
33.9%

4 hops
4.3
23.3%

processing, its online matching time is 3Ã— faster than that of [29].
Moreover, the matching accuracy of KSGM is 1.2Ã— better than that
of the competitor, through it does not use global signatures, nor it
relies on the optimal Hungarian algorithm for anchor selection.
The table also lists the performance of KSGM when using top
PageRank (PR) scored nodes instead of those returned by the SKE
algorithm. As we see here, while the offline process is faster when
using PageRank scoring nodes, the runtime performance (both in
terms of execution time and accuracy) is worse when using SKE
keynodes. In addition, to see whether it is possible to achieve a
competitive accuracy if we were to select a similar percentage of
node randomly, in Table 2, we also include results where random
keynodes are used in the matching online phase. As we can see, the
accuracy drops significantly when we use random keynodes instead
of using robust structural keynodes extracted by the proposed SKE
algorithm6 . These indicate that SKE is indeed effective in extracting
structurally distinct and useful keynodes.

Execution Time

We report both offline and online execution times. As shown in
Algorithm 1 in Section 3, for KSGM, the offline execution includes
structural keynode and local-signature extraction steps. Online execution includes similarity computation, anchor selection, and expansion steps. [29] does not perform structural keynode extraction;
instead, offline execution includes eigen-decomposition for global
signatures.
For both KSGM and [29], we omit the refinement step as its complexity is prohibitive for scalable graph matching. For instance, for
the Facebook graph for which KSGM takes âˆ¼ 6 seconds for online
processing for a pair of graphs, refinement takes âˆ¼ 30 minutes â€“
i.e., it causes a âˆ¼ 260Ã— slowdown5 .

8.3

[29]
19.2
35.4%

Table 3: Impact of the keynode percentage, Î˜, for the Facebook
Graph

All experiments were conducted using a 4-core Intel Core i52400, 3.10GHz, machine with 8GB memory, running 64-bit Windows 7 Enterprise. The codes were executed using Matlab 2013b
and Visual Studio 2012. To evaluate accuracy, we use the matching
quality defined in Section 3.

8.2.1

Matching time (online, sec.)
Accuracy

Experiment Parameters

The default parameters for the structural keynode extraction
(SKE) algorithm are as follows:

8.4.2

Impact of the Keynode Percentage

Table 3 studies the impact of the percentage, Î˜, of the nodes used
as keynodes. As we see, up to a point, the more keynodes we use,
the more accurate and faster the matching becomes. Beyond that
point, however, additional keynodes become disadvantageous. This
indicates that top keynodes are the most effective in serving as good
starting points and, as expected, below a certain rank they loose
distinctiveness, resulting in increased cost and loss in accuracy.

â€¢ PageRank teleportation probability (1âˆ’Î±) = 0.15 (as is commonly assumed),
â€¢ least smoothing factor (Ïƒmin ) = 0.275, corresponding to âˆ¼
2-hop neighborhoods,
â€¢ maximum smoothing factor (Ïƒmax ) = 0.777, corresponding
to âˆ¼ 5-hop neighborhoods, and
â€¢ number (l) of smoothing levels = 6.

8.4.3

In addition, for KSGM, the default percentage (Î˜) of keynodes selected from the graph was set to 3%. Also, for all algorithms, local
signatures were extracted from 2-hop neighborhoods (i.e., k = 2),
as recommended by the authors of [29].

Impact of the Node-Pair Ranking Function

Table 2 lists the online and offline processing times and accuracy for the Facebook graph under the default parameter settings.
As we see here, while KSGM spends more time in one-time, offline

Table 4 studies the impact of the node-pair ranking function,
Ï(). In particular, we compare the performance of the ranking
function proposed in Section 6.1.1, to the ranking function without
degree extension and ranking function including additional globalsignature similarity as proposed in [29]. As we see here, the proposed node-pair ranking function provides the best expansion opportunities (and thus provides the highest accuracy, with slight expansion time overhead). Also, the "with Global" optional provides
a much worse matching. Thus, while the algorithm allows, we encourage the users not to use "with Global" option.

5
For this data configuration, when using expensive refinement postprocessing, KSGM and [29]â€™s accuracies are 0.72 and 0.696, respectively.

6
The slight time gain when using random keynodes is due to the
fact that random keynodes are not good starting points for expansion and, thus, the expansion process ends earlier.

8.4
8.4.1

Results for the Facebook Graph
Default Configuration

1109

gorithm works faster than the state-of-the-art algorithms without
refinement, yet produces alignments that are as good or better.

Table 6: Experiment results for the synthetic data sets (avg.
degree=4, varying models and number of nodes)
KSGM Online time (sec.)
[29] Online time (sec.)
KSGM Accuracy
[29] Accuracy

PL(5000)
6.9
99.3
45.3%
42.9%

PL(7500)
13.9
223.7
45.1%
42.8%

ER(5000)
6.5
746.7
45.7%
46.3%

ER(7500)
14.5
745.1
51.2%
53.0%

KSGM Offline time (sec.)
[29] Offline time (sec.)

130.0
23.8

284.7
82.8

134.8
24.9

270.8
84.5

Acknowledgments
We thank the authors of [29] for sharing their source code and data.

10.

Table 7: Impact of the average node degree (number of
nodes=5000, power law model)
KSGM Online time (sec.)
[29] Online time (sec.)
KSGM Accuracy
[29] Accuracy

degree=4
6.9
99.3
45.3%
42.9%

degree=8
7.5
116.7
25.2%
25.1%

degree=16
9.3
141.2
13.9%
14.1%

KSGM Offline time (sec.)
[29] Offline time (sec.)

130.0
23.8

199.1
27.9

396.7
53.2

8.4.4

Impact of the Neighborhood Size, k, for LocalSignatures

Table 5 studies the impact of the neighborhood size, k, for localsignature extraction. As we see in the table, the highest accuracy is
at 2 hops7 , increasing the neighborhood size negatively affects the
accuracy, indicating that unless locally meaningful signatures are
used, the resulting node-pair ranking is not effective for expansion.
This shows the keynode matching process is more accurate when
keynodes are easy to localize and this requires them to be distinct
and locally representative. Large neighborhoods potentially violate
both. Note that this is in line with the observation in Table 4.

8.5

Results for the Synthetic Data Sets

In this subsection, we consider the impacts of graph topology,
size, and node degree using ER and PL topologies. We omit discussions of the impacts of the other parameters, as they mirror those
presented in Tables 3 through 5.

8.5.1

Default Configurations

Table 6 lists the performances of KSGM and [29] for synthetic
graphs for different topologies and numbers of nodes under the default parameter settings. As we see here, the online execution time
of KSGM is significantly (10Ã— to 115Ã—) faster than that of [29], especially for the ER topology. Moreover, on both Erdos-Renyi (ER)
and power law (PL) topologies, the accuracy is highly competitive, with KSGM providing non-negligible accuracy gains for the PL
model (where it is relatively easier to identify effective keynodes).

8.5.2

Impact of Average Node Degree

Table 6 studies the impact of average node degree on matching
accuracies for the power law graph. As we see in the table, both
algorithms see a drop in the matching accuracy with larger node
degrees. However, KSGM stays competitive in terms of accuracy,
whereas it provides more gains in terms of online execution time.

9.

REFERENCES

[1] http://networkx.github.io/
[2] http://snap.stanford.edu/index.html
[3] X. Bai, H. Yu, and E. R. Hancock. Graph matching using
spectrament. ICPR 2004.
[4] A. Balmin, et al. ObjectRank: Authority-based keyword search in
databases. VLDB, 2004.
[5] M.G. Borgatti, et al. Network measures of social capital.
Connections 21(2):27-36, 1998.
[6] C. Borgs, M. Brautbar, J. T. Chayes, S.-H. Teng. Multiscale
Matrix Sampling and Sublinear-Time PageRank Computation.
Internet Mathematics 10(1-2): 20-48, 2014.
[7] S. Brin, et al. The anatomy of a large-scale hypertextual Web
search engine. Computer Networks and ISDN Systems 30:
107-117, 1998.
[8] H. Bunke. Error correcting graph matching: On the influence of
the underlying cost function. IEEE TPAMI, 21(9):917â€“922, 1999.
[9] K. S. Candan, R. Rossini, M. L. Sapino, X. Wang. sDTW:
Computing DTW Distances using Locally Relevant Constraints
based on Salient Feature Alignments. PVLDB, 1519-1530, 2012.
[10] M. Chen, J. Liu, and X. Tang. Clustering via random walk hitting
time on directed graphs. AAAI 2008.
[11] Xilun Chen, K. Selcuk Candan. LWI-SVD: Low-rank,
Windowed, Incremental Singular Value Decompositions on
Time-Evolving Data Sets. KDD 2014.
[12] Xilun Chen, K. Selcuk Candan. GI-NMF: Group Incremental
Non-Negative Matrix Factorization on Data Streams. CIKM 2014.
[13] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.
Introduction to Algorithms. 2001.
[14] R. Giugno and D. Shasha. Graphgrep: A fast and universal
method for querying graphs. ICPR, pp. 112-115, 2002.
[15] W. S. Han, J. Lee, and J. H. Lee. TurboISO: Towards ultrafast and
robust subgraph isomorphism search in large graph databases.
SIGMOD 2013
[16] R. Jonker and T. Volgenant.Improving the Hungarian assignment
algorithm. Oper. Res. 171-175. 1986.
[17] G. Karypis and V. Kumar "A fast and high quality multilevel
scheme for partitioning irregular graphs". SIAM Journal on
Scientific Computing 20 (1), 1999.
[18] D. Knossow, A. Sharma, D. Mateus, and R. Horaud. Inexact
matching of large and sparse graphs using laplacian eigenvectors.
GbRPR, 2009.
[19] W.-J. Lee and R. P. W. Duin. An inexact graph comparison
approach in joint eigenspace. In SSPR/SPR, 35-44, 2008.
[20] Jundong Li, Xia Hu, Jiliang Tang, Huan Liu. Unsupervised
Streaming Feature Selection in Social Media. CIKM 2015
[21] D. G. Lowe. Distinctive Image Features from Scale-Invariant
Keypoints. Int. Journal of Computer Vision, 60, 2, 2004.
[22] K. Riesen, X. Jiang, and H. Bunke. Exact and inexact graph
matching: Methodology and applications. Managing and Mining
Graph Data, pages 217-247, 2010.
[23] S. Umeyama. An eigen decomposition approach to weighted
graph matching problems. IEEE TPAMI, 10(5):695-703, 1988.
[24] J. R. Ullman. An algorithm for subgraph isomorphism, JACM
Vol. 23, No. 1, pp. 31-42. 1976
[25] X. Wang, K. S. Candan, M. L. Sapino: Leveraging metadata for
identifying local, robust multi-variate temporal (RMT) features.
ICDE, 2014.
[26] White D.R., et al. Betweenness centrality measures for directed
graphs. Social Networks, 16, 335-346,1994.
[27] R. Williams. Faster all-pairs shortest paths via circuit complexity.
STOC, 664-673. 2014.
[28] M. Zaslavskiy, F. R. Bach, and J.-P. Vert. A path following
algorithm for the graph matching problem. IEEE Trans. Pattern
Anal. Mach. Intell., 31(12):2227-2242, 2009.
[29] Y. Zhu, L. Qin, J. X. Yu, et al. High Efficiency and Quality: Large
Graphs Matching. CIKM, pp. 1755-1764. 2011.

CONCLUSIONS

Noticing that existing solutions to the graph matching problem
face major scalability challenges, we argue that it is impractical to
seek alignment among all pairs of nodes. Given these observations,
in this paper, we first presented an offline structural keynode extraction (SKE) algorithm and then discussed how to use these structural keynodes in a novel keynode-driven scalable graph matching
(KSGM) algorithm. Keynodes are selected carefully especially because a post refinement step is not feasible due to scalability requirements. Experiment results show that the proposed KSGM al7
Coincidentally, this also is the scale at which the SKE algorithm
located an overwhelming majority of the keynodes for this graph.

1110

A Non-Parametric Learning Approach to Identify
Online Human Trafficking
Hamidreza Alvari
and Paulo Shakarian

arXiv:1607.08691v2 [cs.LG] 2 Aug 2016

Arizona State University
Tempe, Arizona
Email: {halvari,shak}@asu.edu

Abstractâ€”Human trafficking is among the most challenging
law enforcement problems which demands persistent fight against
from all over the globe. In this study, we leverage readily
available data from the website â€œBackpageâ€â€“ used for classified
advertisementâ€“ to discern potential patterns of human trafficking
activities which manifest online and identify most likely trafficking related advertisements. Due to the lack of ground truth,
we rely on two human analysts â€“one human trafficking victim
survivor and one from law enforcement, for hand-labeling the
small portion of the crawled data. We then present a semisupervised learning approach that is trained on the available
labeled and unlabeled data and evaluated on unseen data with
further verification of experts.

I. I NTRODUCTION
Human trafficking has received increased national and societal concern over the past decade [1]. According to the
United Nation [2], human trafficking is defined as the modern
slavery or the trade of humans mostly for the purpose of
sexual exploiting and forced labor, via different improper ways
including force, fraud and deception. Human trafficking is
among the challenging problems facing the law enforcementâ€“
it is difficult to identify victims and counter traffickers.
Before the advent of the Internet, pimps were under the risks
of being arrested by law enforcement, while advertising their
victims on the streets [3]. However, the move to the Internet,
has made it easier and less dangerous for both sex buyers and
sellers, especially for the pimps [4] as they no longer needed
to advertise on the streets. There are now plethora of websites
that host and provide sexual services, under categories of
escort, adult entertainment, massage services, etc., which help
pimps, traffickers and sex buyers (a.k.a. â€œjohnsâ€), maintain
their anonymity. Though some services such as Craiglistâ€™s
adult section and myredbook.com were shut down recently,
still there are many websites such as Backpage.com that
provide such services and many new are frequently created.
Traffickers even use dating and social networking websites,
including Twitter, Facebook, Instagram and Tinder to reach
out to the johns and their other followers. Although Internet
has presented new trafficking related challenges for law enforcement, it has also provided readily and publicly available
rich source of information which could be gleaned from online
sex advertisements for fighting this crime [5].

J.E. Kelly Snyder
Find Me Group
Tempe, Arizona
Email: kelly@findmegroup.org

Although, the Internet is being used for many other activities
including attracting the victims, communicating with costumers and rating the escort services, here we only focus on the
online advertisements. In this study, we use data crawled from
the adult entertainment section of the website Backpage.com
and propose a non-parametric learning approach to identify the
most likely human trafficking related online advertisements out
of the escort advertisements. To the best of our knowledge,
this is the first study that employs both data mining and
semi-supervised machine learning techniques to identify the
potential human trafficking related advertisements given only
a small portion of labeled data. We thus make the following
contributions.
1) We collected real posts from the U.S. cities represented
on Backpage.com. The data was then preprocessed and
cleaned.
2) Based on the literature, we created different groups
of features that capture the characteristics of potential
human trafficking activities. The less likely human trafficking related posts were then filtered out using these
features.
3) Due to the lack of ground truth, we relied on human
analysts for hand-labeling small portion of the filtered
data.
4) We trained a semi-supervised learner on labeled and
unlabeled data and sent back the identified highly human trafficking related advertisements to the experts for
further verification. We then validated our approach on
unseen data with further verification of experts.
The rest of the paper is organized as follows. In Section II,
we briefly provide the background of the problem of human
trafficking. Next, we review the prior studies on human trafficking in Section III. Then in Section IV, we explain our data
preparation and feature extraction scheme. Our unsupervised
filtering and expert assisted labeling are explained in Sections
V and VI, respectively. We detail our non-parametric learning
approach in Sections VII. We conclude the paper by providing
future research directions in Section VIII.
II. BACKGROUND
The United Statesâ€™ Trafficking Victim Protection Act of
2000 (TVPA 2000) [6], was the first U.S. legislation passed

against human trafficking. According to TVPA 2000, sex
trafficking is a severe form of trafficking, where force, fraud
or corecion are primary ways of inducing commercial sex
act. Human Trafficking is a crime against humanity and is
one of the most atrocious crimes of global magnitude. It is
a $150 billion industry of exploitation of children and young
adults, utilizing humans for forced labor and sex trafficking
worldwide. No country is immune and the problem is rapidly
growing with little to no law enforcement addressing the issue
and approximately 161 countries affected. Human trafficking
is considered to be a form of modern day slavery. Humans
are controlled, exploited, abused, forced into prostitution and
labor of servitude in some form and all under the threat of
punishment if they do not perform their required duties.
The Find Me Group (FMG) was founded by retired DEA
Special Agent Jerry â€œKellyâ€ Snyder in 2002 primarily to
locate missing persons. The natural evolution of the group
in locating missing persons was to allocate resources for
locating victims in human trafficking, as well as identifying
the persons responsible and reporting these organizations to
law enforcement. The FMG consists of current and retired
law enforcement agents and officers with a wide-range of
investigative expertise, including but not limited to linguistics,
handwriting analysis, body language, missing persons and
homicide. The search and rescue component of the FMG is
also comprised of current and retired law enforcement officers
and agents with 28 years of field management skills in locating
missing persons. The FMG has an additional advantage by
using trained experts/sources that provide detailed location
information of human trafficking victims.
The ultimate goal of the current project is to identify
missing persons which are connected to human trafficking
organizations. This can be done by identifying their locations,
utilizing logistical methodology with an additional focus on
their financial status and reporting assets to worldwide law
enforcement.
III. R ELATED W ORK
Recently, several studies have examined the role of the Internet and related technology in facilitating human trafficking[7],
[8], [9]. For example, the work of [7] studied how closely sex
trafficking is intertwined with new technologies. According
to [8], â€œThe sexual exploitation of women and children is
a global human rights crisis that is being escalated by the
use of new technologiesâ€. Researchers have studied the relationship between new technologies and human trafficking and
advantages of the Internet for sex traffickers. For instance,
according to [9], findings from a group of experts from the
Council of Europe demonstrate that the Internet and sex
industry are closely interlinked and the volume and content
of the material on the Internet promoting human trafficking
are unprecedented.
One of the earliest works which leveraged data mining
techniques for online human trafficking was [9], where the
authors conducted an analysis of data on the adult section
of the website Backpage.com. Their findings confirmed that

the female escort post frequency would increase in Dallas,
Texas, leading up to Super Bowl 2011 event. In a similar
attempt, other studies [10], [11] have investigated the impact
of large public events such as Super Bowl on sex trafficking
by exploring advertisement volume, trends and movement of
advertisements along with the scope and volume of demand
associated with such events. The work of [10], for instance,
concludes that in large events like Super Bowl which attract
significant amount of concentration of people in a relatively
short period of time and in a confined urban area, could be
a desirable location for sex traffickers to bring their victims
for commercial sexual exploitation. Similarly, the data-driven
approach of [11] shows that in some but not all events, one
can see a correlation between the occurrence of the event and
statistically significant evidence of an influx of sex trafficking
activity. Also, certain studies [12] have tried to build large
distributed systems to store and process the available online
human trafficking data in order to perform entity resolution
and create ontological relations between the entities.
Beyond these works, the work of [13], studied the problem
of isolating sources of human trafficking from online advertisements with a pairwise entity resolution approach. Specifically,
they trained a classifier to predict if two ads are from the
same source, using phone numbers as a strong feature. Then,
this classifier was used to perform entity resolution using a
heuristically learned value for the score of classifier. Another
work of [5] used Backpage.com data and extracted most likely
human trafficking spatio-temporal patterns with the help of law
enforcement. Note that unlike our method, this work did not
employ any machine learning methodologies for automatically
identifying the human trafficking related advertisements. The
work of [14] also used machine learning techniques by training
a supervised learning classifier on labeled data (based on the
phone numbers of known traffickers) provided by a victim
advocacy group, for the ad-classification problem. We note that
while phone numbers can provide very precise set of positive
labeled data, there are clearly many posts with previously
unseen phone numbers. In contrast, we do not solely rely on
the phone numbers for labeling the data. Instead, our experts
analyze the whole postâ€™s content to identify whether it is
human trafficking related or not. Indeed, we first filter out
the most likely advertisements using several feature groups
and pass a small sample to the experts for hand-labeling.
Then, we train semi-supervised learner on both labeled and
unlabeled data which in turn let us evaluate our approach on
the new coming (unseen) data as well. We note that our semisupervised approach can also be used as a complementary
method to procedures such as those described in [14] as we can
significantly expand the training set for use with supervised
learning.
IV. DATA C OLLECTION E FFORT
We collected about 20K publicly available listings from
the U.S. posted on Backpage.com in March, 2016. Each post
includes a title, description, time stamp, the posterâ€™s age,
posterâ€™s ID, location, image, video and sometimes audio. The

description usually lists the attributes of the individual(s) and
contact phone numbers. In this work we only focus on the
textual component of the data. This free-text data required
significant cleaning due to a variety of issues common to
textual analytics (i.e. misspellings, format of phone numbers,
etc.). We also acknowledge that the information in data could
be intentionally inaccurate, such as the posterâ€™s name, age and
even physical appearance (i.e. bra cup size, weight). Figure 1
shows an actual post from Backpage.com. To illustrate the
geographic diversity of the listings, we also plot the phone
distribution with respect to the different states in Figure 2.
Note that for brevity, we only show those with a frequency
greater than 5.
Fig. 1.

Fig. 3. An evidence of human trafficking. The boxes and numbers in red,
indicate the features and their corresponding group numbers (see also Table I).

A real post from Backpage.

TABLE I
D IFFERENT GROUPS OF FEATURES USED IN OUR WORK .
No.
1
2
3
4
5
6

Fig. 2.

Phone distribution by different states.

Feature Group
Advertisement Language Pattern
Words and Phrases of Interest
Countries of Interest
Multiple Victims Advertised
Victim Weight
Reference to Website or Spa Massage Therapy

Ref.
[5], [15], [16]
[17], [18], [19]
[1]
[5]
[6], [20]
[5]

for multiple escorts with the first individual coming from Asia
and very young. In the followings, we discuss such common
properties of human trafficking related advertisements, in more
details.
Inspired from literature, we define and extract 6 groups of
features from advertisements, shown in Table I, which could
be amongst the strong indicators of the human trafficking. In
what follows, we briefly describe each group of features used
in our work. Each feature listed is treated as a binary variable.
1) Advertisement Language Pattern: The first group consists of different language related features. For the first and
second features, we identify posts which has third person
language (more likely to be written by someone other than the
escort) and posts which contain first person plural pronouns
such as â€˜weâ€™ and â€˜ourâ€™ (more likely to be an organization) [5].

Next, we explain the most important characteristics of
potential human trafficking advertisements captured by our
feature groups.
A. Feature Engineering
Though many advertisements on Backpage.com are posted
by posters selling their own services without coercion and
intervention of traffickers, some do exhibit many common
trafficking triggers. For example, in contrast to the previous advertisements, Figure 3 shows an advertisement that could be an
evidence of human trafficking. This advertisement has several
potential properties of human trafficking including advertising

To ensure their anonymity, traffickers would deploy techniques to generate diverse information and hence make their
posts look more complicated. They usually do this to avoid being identified by either human analyst or automated programs.
Thus, to obtain the third feature, we take an approach from
complexity theory, namely Kolmogorov complexity which is
defined as the length of the shortest program to reproduce the
advertisement content on a universal machine such as Turing
Machine [15]. We approximate the Kolmogorov complexity of
an advertisementâ€™s content, by simply computing the Entropy
of the content [15] as follows. Let X denote the content and
xi be a given word in the content. We use the following

equation [21] to calculate the Entropy of the content.
H(X) = âˆ’

n
X

P (xi ) log2 P (xi )

evidence of human traffickingâ€“ this is described in the next
section.
(1)

i=1

We expect higher values of the Entropy correspond to
human trafficking. Finally, we discretize the result by using
the threshold of 4 which was found empirically in our experiments.
Next, we use word-level n-grams to find the common
language patterns of the advertisements, as the character-level
n-grams have already shown to be useful in detecting
unwanted content for Spam detection [16]. We set n = 4
and compute the normalized n-grams (using TF-IDF) of the
advertisementâ€™s content and use threshold of 0.5 to binarize
their values. This gives us 6 more features to include into
our feature set. Overall, we have 9 features related to the
language of the advertisement.
2) Words and Phrases of Interest: Despite the fact that
advertisements on Backpage.com do not directly mention
sex with children, costumers who prefer children, know to
look for words and phrases such as â€œsweet, candy, fresh,
new in town, new to the gameâ€ [17], [18], [19]. We thus
investigate within the posts to see if they contain such words as
they could be highly related with human trafficking in general.

V. U NSUPERVISED F ILTERING
Having detailed our feature set, we now construct feature
vectors for each instance by creating a vector of 15 binary
features that correspond to the important characteristics of
human trafficking related posts.
We obtain 999 instances from our dataset by filtering out
samples that do not posses any of the binary features. We
will refer to this as our filtered dataset. In Figure 4, we
visualize 500 of the 999 samples and an additional 500
samples outside of the filtered dataset (i.e., from the remainder
of the samples) we studied in a 2-D projection (using the tSNE transformation [22]). We clustered the visualized samples
into 2 clusters (using K-means) and found the clusters to be
purely either inside or outside of the sampled data (100% of
samples in cluster 1 were from the identified listings and 100%
of samples in cluster 2 were from outside this group).
Fig. 4. Two clusters of a portion of the filtered data set combined with
random samples from the remainder of the samples, in the trasformed feature
space.

3) Countries of Interest: We identify if the individual
being escorted is coming from other countries such as those
in Southeast Asia (especially from China, Vietnam, Korea
and Thailand, as we observed in our data) [1].
4) Multiple Victims Advertised: Some advertisements
advertise for multiple women at the same time. We consider
the presence of more than one girl as a potential evidence of
organized human trafficking [5].
5) Victim Weight: We take into account weight of the
individual being escorted as a feature (if it is available).
This information is particularly useful assuming that for
the most part, lower body weights (under 110 lbs) correlate
with smaller and underage girls [6], [20] and thereby human
trafficking.
6) Reference to Website or Spa Massage Therapy: The
presence of a link in the advertisement, either referencing
to an outside website (especially infamous ones) or spa
massage therapy, could be an indicator of more elaborate
organization [5]. In case of spa therapy, we observed many
advertisements interrelated with advertising for young Asian
girls and their erotic massage abilities. Therefore, the last
group has two binary features for the presence of both website
and spa.
In order to extract these features, we first clean the original
data and conduct preprocessing. Then we draw 999 instances
out of our dataset for further analysis, as they might be

We then create a second feature space that is used through
the remainder of the paper. Using Latent Drichlet Allocation
(LDA) [23] topic modeling from Python package gensim [24],
we identify 25 most representative topics out of the filtered
dataset. This allows us to uncover the hidden thematic structure in the data. Further, we rely on the document-topic
distribution given by the LDA (here each document is seen as
a mixture of topics) to distinguish the normal advertisements
(outliers) from highly human trafficking related ones. More
specifically, we treat each listing in the filtered data as a vector
of 25 probabilistic values provided by LDAâ€™s document-topic
distributionâ€“ this feature space is used in the next step.
Moreover, since we lack ground truth for our data, we rely
on human analysts (experts) for labeling the listings as either
human trafficking or not. In the next section, we select a
smaller yet finer grain subset of this data to be sent to the
experts. This alleviates the burden of the tedious work of handlabeling.

TABLE III
VALIDATED RESULTS ON UNLABELED DATA FOR BOTH KERNELS .

VI. E XPERT A SSISTED L ABELING
We first obtain a sample of 150 listings from the filtered
dataset. This set of listings was labeled by two human experts:
a previous human trafficking victim and a law enforcement
officer who specialized in this type of crime. From this subset,
a law enforcement professional and human trafficking victim
identified 38 and 139 instances (respectively) to be human
trafficking related instances. Among them, there were 31
records for which both experts agreed were highly related to
human trafficking. Thus, we now have 31 confirmed positive
samples, but still have large amounts of unlabeled examples
(849 instances) in our dataset. We summarize the data statistics
in Table II. Any sample for which at least one expert labeled
as negative, we treated as a negative sample.
TABLE II
D ESCRIPTION OF THE DATASET.
Name
Raw
Filtered
Unlabeled
Labeled
Positive
Negative

Expert 1
38
112

Value
20,822
999
849
Expert 2 Intersection
139
31
11
4

Name
Kernel
RBF (Union)
RBF (Intersection)
KNN (Union)
KNN (Intersection)

Positive
(Learner)
145
848
188
849

Value
Negative
Positive
(Learner) (Experts)
704
134
1
661
170
0
-

Precision
(Positive)
92.41%
90.42%
-

and 170 labels out of 145 and 188 positive instances and
achieved precision of 92.41% and 90.42%, respectively. We
further demonstrate the word clouds for the positive instances
assigned by RBF and KNN, in Figure 5 and Figure 6,
respectively.
Fig. 5.

Word cloud for the positive instances assigned by RBF.

Fig. 6.

Word cloud for the positive instances assigned by KNN.

Union
146
119

In the next section, we explain how we deploy a nonparametric learning approach to identify the labels of the rest
of the data to be sent for further expert verification.
VII. N ON -PARAMETRIC L EARNING
We use the Python package scikit-learn [25] for training
semi-supervised learner on the filtered dataset. There are
two label propagation semi-supervised (non-parametric) based
models in this package, namely, LabelPropagation and LabelSpreading [26]. These models rely on the geometry of
the data induced by both labeled and unlabeled instances as
opposed to the supervised models which only use the labeled
data [26]. This geometry is usually represented by a graph
G = (V, E), with the nodes V represent the training data
and edges E represent the similarity between them [26] in the
form of weight matrix W. Given the graph G, a basic approach
for semi-supervised learning is through propagating labels on
the graph [26]. Due to the higher performance achieved, we
chose to use LabelSpreading model. We conducted experiment
with the two built-in kernels radial basis function (RBF) and
K-nearest neighbor (KNN) in label propagation models and
report the results in Table III. Note that we only reported the
precision when 119 negative samples (labeled by either of the
experts) were used in the learning process. We did so because
of the reasonable number of the positive labels assigned by
either of the kernels in presence of these negative instances
(our experts had limited time to validate the labels of the data).
As we see from this table, out of 849 unlabeled data, our
learner with RBF and KNN kernels assigned positive labels
to the 145 and 188 instances, respectively. Next, we pass the
identified positive labels to the experts for further verification.
Our approach with RBF and KNN correctly identified 134

VIII. C ONCLUSION
Readily available online data from escort advertisements
could be leveraged in favor of fighting against the human
trafficking. In this study, having focused on textual information
from available data crawled from Backpage.com, we identified if an escort advertisement can be reflective of human
trafficking activities. More specifically, we first propose an
unsupervised filtering approach to filter out the data which
are more likely involved in trafficking. We then trained a

semi-supervised learner on small portion of such data, handlabeled by human trafficking experts, to identify the labels for
unseen data. The results suggest our non-parametric approach
is successful at identifying the potential human trafficking
related advertisements.
In future work we seek to extract the underlying network of
the data to find interesting patterns such as the most influential
nodes as they might indicate the known pimps and traffickers.
We would also like to replicate the study by integrating
more features, especially those supported by the criminology
literature.
ACKNOWLEDGMENT
This work was funded by the Find Me Group, a 501(c)3
dedicated to bring resolution and closure to families of missing
persons. https://www.findmegroup.org/
R EFERENCES
[1] â€œTrafficking in Persons Report,â€ July 2015.
[2] â€œUNODC on human trafficking and migrant smuggling,â€ 2011.
[3] C. Desplaces, â€œPolice Run â€˜Prostitutionâ€™ Sting; 19 Men Arrested,
Charged in Fourth East Dallas Operation.â€ Nov 1992.
[4] K. Nicholas D., â€œHow Pimps Use the Web to Sell Girls.â€ Jan 2012.
[5] E. Kennedy, â€œPredictive patterns of sex trafficking online,â€ 2012.
[6] â€œTrafficking Victims Protection Act of 2000,â€ 2000.
[7] D. M. Hughes et al., â€œThe demand for victims of sex trafficking,â€
Womens Studies Program, University of Rhode Island, 2005.
[8] D. M. Hughes, â€œThe Use of New Communications and Information
Technologies for Sexual Exploitation of Women and Children,â€ Hastings
Womenâ€™s Law Journal, vol. 13, no. 1, pp. 129â€“148, 2002.
[9] M. Latonero, â€œHuman trafficking online: The role of social networking
sites and online classifieds,â€ Available at SSRN 2045851, 2011.
[10] D. Roe-Sepowitz, J. Gallagher, K. Bracy, L. Cantelme, A. Bayless,
J. Larkin, A. Reese, and L. Allbee, â€œExploring the Impact of the Super
Bowl on Sex Trafficking,â€ Feb. 2015.
[11] K. Miller, E. Kennedy, and A. Dubrawski, â€œDo Public Events Affect
Sex Trafficking Activity?â€ ArXiv e-prints, Feb. 2016.
[12] P. A. Szekely, C. A. Knoblock, J. Slepicka, A. Philpot, A. Singh,
C. Yin, D. Kapoor, P. Natarajan, D. Marcu, K. Knight, D. Stallard, S. S.
Karunamoorthy, R. Bojanapalli, S. Minton, B. Amanatullah, T. Hughes,
M. Tamayo, D. Flynt, R. Artiss, S.-F. Chang, T. Chen, G. Hiebel, and
L. Ferreira, â€œBuilding and Using a Knowledge Graph to Combat Human
Trafficking.â€ in International Semantic Web Conference (2), ser. Lecture
Notes in Computer Science, vol. 9367. Springer, 2015, pp. 205â€“221.
[13] C. Nagpal, K. Miller, B. Boecking, and A. Dubrawski, â€œAn Entity
Resolution approach to isolate instances of Human Trafficking online,â€
ArXiv e-prints, Sep. 2015.
[14] A. Dubrawski, K. Miller, M. Barnes, B. Boecking, and E. Kennedy,
â€œLeveraging publicly available data to discern patterns of humantrafficking activity,â€ Journal of Human Trafficking, vol. 1, no. 1, pp.
65â€“85, 2015.
[15] M. Li and P. M. Vitnyi, An Introduction to Kolmogorov Complexity and
Its Applications, 3rd ed. Springer Publishing Company, Incorporated,
2008.
[16] I. Kanaris, K. Kanaris, and E. Stamatatos, â€œSpam Detection Using
Character N-Grams.â€ in SETN, ser. Lecture Notes in Computer Science,
vol. 3955. Springer, 2006, pp. 95â€“104.
[17] K. Hetter, â€œFighting sex trafficking in hotels, one room at a time,â€ March
2012.
[18] R. Lloyd, â€œAn Open Letter to Jim Buckmaster,â€ April 2012.
[19] J. Dickinson Goodman and M. Holmes, â€œCan We Use RSS to Catch
Rapists,â€ 2011.
[20] â€œAverage Height to Weight Chart - Babies to Teenagers.â€
[Online]. Available: http://www.disabled-world.com/artman/publish/
height-weight-teens.shtml
[21] C. E. Shannon, â€œA mathematical theory of communication,â€ ACM
SIGMOBILE Mobile Computing and Communications Review, vol. 5,
no. 1, pp. 3â€“55, 2001.

[22] L. van der Maaten and G. Hinton, â€œVisualizing High-Dimensional Data
Using t-SNE,â€ 2008.
[23] D. M. Blei, A. Y. Ng, and M. I. Jordan, â€œLatent dirichlet allocation,â€
the Journal of machine Learning research, vol. 3, pp. 993â€“1022, 2003.
[24] R. RÌŒehuÌŠrÌŒek and P. Sojka, â€œSoftware Framework for Topic Modelling
with Large Corpora,â€ in Proceedings of the LREC 2010 Workshop on
New Challenges for NLP Frameworks. Valletta, Malta: ELRA, May
2010, pp. 45â€“50, http://is.muni.cz/publication/884893/en.
[25] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, â€œScikit-learn: Machine learning in Python,â€ Journal of Machine
Learning Research, vol. 12, pp. 2825â€“2830, 2011.
[26] Y. Bengio, O. Delalleau, and N. Le Roux, In Semi-Supervised Learning.
MIT Press, 2006.

Argumentation Models for Cyber Attribution
Eric Nunes, Paulo Shakarian
Arizona State University Tempe, AZ 85281, USA Email: {enunes1, shak} @asu.edu

Gerardo I. Simari
Inst. for CS and Eng. (CONICET­UNS) DCIC, UNS, Bahia Blanca, Argentina Email: gis@cs.uns.edu.ar

Andrew Ruef
Trail of Bits, Inc. New York, NY 10003, USA Email: andrew@trailofbits.com

arXiv:1607.02171v1 [cs.AI] 7 Jul 2016

Abstract--A major challenge in cyber-threat analysis is combining information from different sources to find the person or the group responsible for the cyber-attack. It is one of the most important technical and policy challenges in cyber-security. The lack of ground truth for an individual responsible for an attack has limited previous studies. In this paper, we take a first step towards overcoming this limitation by building a dataset from the capture-the-flag event held at DEFCON, and propose an argumentation model based on a formal reasoning framework called DeLP (Defeasible Logic Programming) designed to aid an analyst in attributing a cyber-attack. We build models from latent variables to reduce the search space of culprits (attackers), and show that this reduction significantly improves the performance of classification-based approaches from 37% to 62% in identifying the attacker.

·

·

experiments demonstrating that using argumentationbased tools can significantly reduce the number of potential culprits that need to be considered in the analysis of a cyber-attack; and experiments showing that the reduced set of culprits, used in conjunction with classification, leads to improved cyber-attribution decisions.

I. I NTRODUCTION A major challenge in cyber-threat analysis is to find the person or the group responsible for a cyber-attack. This is known as cyber-attribution [17] and it is one of the central technical and policy challenges in cyber-security. Oftentimes, the evidence collected from multiple sources provides a contradictory viewpoint. This gets worse in cases of deception where either an attacker plants false evidence or the evidence points to multiple actors, leading to uncertainty. In the text on cyber-warfare [17] the authors discuss the difficulties that an intelligence analyst faces in attributing an attack to a perpetrator given that deception might have occurred, and how the analyst needs to explore deception hypotheses under the given attack scenario. However, one of the major drawbacks of the study and evaluation of cyber-attribution models is the lack of datasets with the ground truth available regarding the individual party responsible for the attack--this has limited previous studies. To overcome this, we built and leveraged a dataset from the capture-the-flag event held at DEFCON. In previous work, this dataset was used to study cyber-attribution, framing it as a multi-label classification problem to predict the attacker [13]. Machine learning approaches struggle in situations of deception, where similar attributes point towards multiple attackers--we propose to address this issue using a formal logical framework. Specific contributions of this paper include:
·

Related work: Adversarial machine learning is an emerging field of study. It uses effective machine learning techniques to identify or defend against an adversary's opponents. Understanding the limits of adversary's knowledge and capabilities is crucial for coming up with countermeasures, as discussed in [9]. Here the authors propose models to study these limitations to come up with evasion techniques. On the contrary, Lowd and Meek [12] explore the problem from an adversarial point of view. They propose strategies that an adversary can use to reverse engineer a classifier so that his attacks are undetected by the classifier. They use a real world application in spam filtering to demonstrate their method, which they call adversarial classifier evasion. In a spam filtering setting an example of such a technique is replacing feature words that raise a red flag with their synonyms to evade detection. This feature cross substitution technique is discussed in [10]. Here the authors offer a simple heuristic method based on mixedinteger linear programming with constraint generation to make the classifier robust to cross substitution techniques. There is research that looks at modeling the interaction between the learner (adversary) and the classifier in terms of a competition using Stackelberg games [4], [3]. Most adversarial machine learning applications deal with modeling classifiers to be robust against evasive techniques in real world applications like malware detection and spam filtering. Cyber-attribution falls in the domain of adversarial learning, but looks at analyzing the evidence in the aftermath of an attack to discover the attacker. Currently, cyber-attribution is limited to identifying machines [2] as opposed to the hacker or their affiliation to a group or a state. An example of such a technical attribution approach is WOMBAT [5], where a clustering technique is used to group attacks to common IP sources. A method that combines information from different sources was proposed by Walls [22], who considered forensic information from diverse sources but did not account for inconsistency or uncertainty due to deception. A less rigorous mathematical model, known as the Q model [15], was proposed recently; the model answers

description of how a model for cyber-attribution can be designed and implemented in the DeLP structured argumentation framework;

queries from an analyst, and by combining these answers the analyst attributes an attack to a party. Unfortunately, there are no experimental evaluations of its effectiveness. Argumentation has been used for cyber reasoning [1] by leveraging arguments to deal with incomplete and contradictory data, allowing to derive big-picture conclusions to keep systems secure and online in case of an attack. This is a different application than the one we are addressing. In [20], a tool was presented to support human decisions, focusing on how user trust in the evidence influences the process; a user study demonstrating the hypotheses was presented in [16]. Concurrently, a formal logical framework for reasoning about cyber-attribution has been devised [18], [19]; it explores multiple competing hypotheses based on the evidence for and against a particular attacker to help analysts decide on an attribution, providing a map of the reasoning that led to the decision. The rest of the paper is organized as follows. We present a description of our DEFCON capture-the-flag dataset and an analysis on the occurrence of deception within this data in Section II. This is followed by the argumentative model based on [8] in Section III. We then summarize results from [13] and discuss how we built our baseline argumentation model along with two other extended baseline models for cyber-attribution with DeLP in Section V and Section VI, with a discussion of the experimental results obtained with each of these models. Conclusions are discussed in Section VII. II. DEFCON CTF DATASET The DEFCON security conference sponsors and hosts a capture the flag (CTF) competition every year, held on site with the conference in Las Vegas, Nevada. DEFCON CTF is one of the oldest and best-known competitions. The ctftime.org site provides a ranking for CTF teams and CTF competitions, and in this system DEFCON CTF has the highest average weight of all other CTF competitions. CTF competitions can be categorized by what role the competitors play in the competition: either red team, blue team, or a combination. In a blue team focused CTF the competitors harden their systems against a red team played by the organizers of the CTF. In a combined red/blue team CTF every team plays both blue and red team simultaneously. The NCCDC and CDX competitions are examples of a blue team CTF, while DEFCON CTF is a combined red/blue team. Each team is simultaneously responsible for hardening and defending their systems as well as identifying vulnerabilities and exploiting them in other teams' systems. The game environment is created primarily by the DEFCON CTF organizers. The game focuses around programs (known in the game as services) written by the organizers. These services are engineered to contain specific vulnerabilities. The binary image of the service is made available to each team at the start of the game, but no other information about the service is released. Part of the challenge of the game is identifying the purpose of each service as well as the vulnerabilities present in the service. Identification of vulnerabilities serves both a

defensive and offensive goal. Once a vulnerability has been identified, a team may patch this vulnerability in the binary program. Additionally, the teams may create exploits for that vulnerability and use them to attack other teams and capture digital flags from those teams' systems. Each team is also provided with a server running the services, which contains the digital flags to be defended. To deter defensive actions such as powering off the server or stopping the services, the white team (a third team, played by the organizers) conducts periodic availability tests of the services running on each team's server. A team's score is the sum of the value of the flags they have captured, minus the sum of the flags that have been captured from that team, multiplied by an availability score determined by how often the white team was able to test that team's services. This scoring model incentivizes teams to keep their server online, identify the vulnerabilities in services and patch them quickly, and exploit other teams services to capture their flags. It disincentivizes teams' from performing host-level blocking and shutting down services, as this massively impacts their final score. This game environment can be viewed as a microcosm of the global Internet and the careful game of cat and mouse between hacking groups and companies. Teams are free to use different technical means to discover vulnerabilities. They may use fuzzing and reverse engineering on their own programs, or, they may monitor the network data sent to their services and dynamically study the effects that network data has on unpatched services. If a team discovers a vulnerability and uses it against another team, the first team may discover that their exploit is re-purposed and used against them within minutes. The organizers of DEFCON CTF capture all of the network traffic sent and received by each team, and publish this traffic at the end of the competition [6]. This includes IP addresses for source and destination, as well as the full data sent and received and the time the data was sent or received. This data is not available to contestants; depending on the organizers' choice from year to year, the contestants either have a real time feed but with the IP address obscured, or a full feed delivered on a time delay of minutes to hours. Analysis: We use the data from the CTF tournament held at DEFCON 21 in 2013. The CTF data set is very large, about 170 GB in compressed format. We used multiple systems with distributed and coordinated processing to analyze the entire dataset--fortunately, analyzing individual streams is easy to parallelize. To analyze this data, we identified the TCP ports associated with each vulnerable service. From this information, we used the open source tool tcpflow to process the network captures into a set of files, with each file representing data sent or received on a particular connection. With these data files identified, we analyzed some of them by hand using the Interactive Disassembler (IDA) to determine if the data contained shell-code, which in fact was the case. We used an automated tool to produce a summary of each data file as a JSON encoded element. Included in this summary was a hash of the contents of the file and a histogram of the processor instructions contained in the file. These JSON files

TABLE 1: Fields in an instance of network attack
Field byte hist inst hist from team to team time Intuition Histogram of byte sequences in the payload Histogram of instructions used in the payload The team where the payload originates (attacking team) The team being attacked by the exploit Indicates the date and time of the attack Value 0×43:245, 0×69:8, 0×3a:9, ..... cmp:12, subs:8, movtmi:60 ...... Blue Lotus Robot Mafia 2013-08-03T23:45:17

be deceptive when multiple adversaries get mapped to a single attack pattern; deception is thus a scenario in which the same exploit is used by multiple teams to target the same team. The number of unique deceptive attacks amount to just under 35% of the total unique attacks in our dataset--clearly, deception is a heavily-used technique in this domain. Duplicate attacks: A duplicate attack occurs when the same team uses the same payload to attack the same team at different points in time. We group duplicates as either being nondeceptive or deceptive. Non-deceptive duplicates are the copies of the attacks launched by the team that first initiated the use of a particular payload; on the other hand, deceptive duplicates are all the attacks from the teams that did not initiate the use. III. A RGUMENTATION M ODEL Our approach relies on a model of the world where we can analyze competing hypotheses in a cyber-operation scenario. Such a model should allow for contradictory information so it can handle inconsistency in cases of deception. Before describing the argumentation model in detail, we introduce some necessary notation. Variables and constant symbols represent items such as the exploits/payloads used for the attack, and the actors conducting the cyber-attack (in this case, the teams in the CTF competition). We denote the set of all variable symbols with V and the set of all constants with C. For our model we require two subsets of C: Cact , denoting the actors capable of conducting the cyber-operation, and Cexp , denoting the set of unique exploits used. We use symbols in all capital letters to denote variables. In the running example, we use a subset of our DEFCON CTF dataset. Example 1. Actors and cyber-operations from the CTF data: Cact = {bluelotus, robotmafia, apt8}, Cexp = {exploit1 , exploit2 , ..., exploitn }. The language also contains a set of predicate symbols that have constants or variables as arguments, and denote events that can be either true or false. We denote the set of predicates with P; examples of predicates are shown in Table 2. For instance, culprit(exploit1 , apt8) will either be true or false, and denotes the event where apt8 used exploit1 to conduct a cyberoperation. TABLE 2: Example predicates and explanation
Predicate attack(exploit1 , bluelotus) replay attack(E , Y) deception(exploit1 , apt8) time diff(I, Y) culprit(exploit1 , apt8) Explanation exploit1 was targeted towards the team Blue Lotus. Exploit E was replayed by team Y. Team apt8 used exploit1 for deception. Team Y was deceptive within the given time interval I . Team apt8 is the likely culprit for the attack (using exploit1 on the target team).

were the final output of the low-level analysis, transforming hundreds of gigabytes of network traffic into a manageable set of facts about exploit traffic in the data. Each JSON file is a list of tuples (time-stamp, byte-histogram, instruction-histogram, attack team and target team). The individual fields of the tuple are listed in Table 1. The pre-processing can be summarized in the following steps: · Un-tarring the archives available from the organizers. The archives produce a large number of pcap-ng formatted files that contain the traffic captures. · Conversion of the pcap-ng files to tcpdump format capture using the editcap utility. This is to allow tcpflow to process the data. · Use of xargs and GNU parallel to run tcpflow on each pcap. This is a time-consuming process, and produced a directory structure with files for data sent and received on host-port socket pairs. This step of processing allows file-based tools to process the network data. · A tool to process each file containing data sent or received by network ports associated with CTF challenges. These tools produced summary statistics for each data stream, to include a byte histogram, overall size, a hash, and an ARM instruction histogram (we ran a linear sweep with the Capstone instruction decoder to produce this). This data was saved via JSON. After this pre-processing of the network data packets, we have around 10 million network attacks consisting of around 1 million unique exploits built and used by 20 teams in the competition. In order to attribute an attack to a particular team, apart from analyzing the payloads used by the team, we also need to analyze the behavior of the attacking team towards their adversary. For this purpose, we divide the attacks according to the team being targeted. Thus, we have 20 such subsets, which we represent as T-i, where i  {1, 2, 3, ..., 20}. The processed dataset is publicly available 1 . We now discuss two important observations from the dataset, which make the task of attributing an observed network attack to a team difficult. Deception: In the context of this paper we define an attack to
1 http://lab.engineering.asu.edu/cysis/cyber-attribution/

A ground atom is composed by a predicate symbol and a tuple of constants, one for each argument. The set of all

ground atoms is denoted as G. A ground literal L is a ground atom or a negated ground atom; hence, ground literals have no variables. An example of a ground atom for our running example is attack(exploit1 , bluelotus). We denote a subset of G with G . We choose a structured argumentation framework [14] for our model; our approach works by creating arguments (in the form of a set of rules and facts) that compete with each other to attribute an attack to a given perpetuator. In this case, arguments are defeated based on contradicting information in other arguments. This procedure is known as a dialectical process, where the arguments that are undefeated prevail. An important result is the set of all the arguments that are warranted (not defeated) by any other argument, which give a clear map supporting the conclusion. Such transparency lets a security analyst not only add new arguments based on new evidence discovered in the system, but also get rid of incorrect information and fine-tune the model for better performance. Since the argumentation model can deal with inconsistent information, it draws a natural analogy to the way humans settle disputes when there is contradictory information available. Having a clear explanation of why one argument is chosen over others is a desirable characteristic for both the analyst and for organizations to make decisions and policy changes. We now briefly discuss some preliminaries on DeLP. Defeasible Logic Programming: DeLP is a formalism that combines logic programming with defeasible argumentation; full details are discussed in [8]. The formalism is made up of several constructs, namely facts, strict rules, and defeasible rules. Facts represent statements obtained from evidence, and are always true; similarly, strict rules are logical combinations of elements (facts or other inferences) that can always be performed. On the contrary, defeasible rules can be thought of as strict rules that may be true in some situations, but could be false if contradictory evidence is present. These three constructs are used to build arguments, and DeLP programs are sets of facts, strict rules and defeasible rules. We use the usual notation for DeLP programs, denoting the knowledge base with  = (, , ), where  is the set of facts,  is the set of strict rules, and  is the set of defeasible rules. Examples of the three constructs are provided with respect to the dataset in Fig. 1. We now describe the constructs in detail. Facts () are ground literals that represent atomic information or its (strong) negation (¬). Strict Rules () represent cause and effect information; they are of the form L0  L1 , ...Ln , where L0 is a literal and {Li }i>0 is a set of literals. Defeasible Rules () are weaker versions of strict rules, and are of the form L0 - L1 , ...., Ln , where L0 , is the literal and {Li }i>0 is a set of literals. When a cyber-attack occurs, the model can be used to derive arguments as to who could have conducted the attack. Derivation follows the same mechanism as logic programming [11]. DeLP incorporates defeasible argumentation, which decides which arguments are warranted and it blocks arguments that

:

1 2 3 4 5

= = = = =

attack(exploit1 , bluelotus) first attack(exploit1 , robotmafia) last attack(exploit1 , apt8)) time diff(interval, robotmafia) most frequent(exploit1 , pwnies) culprit(exploit1 , pwnies)  most frequent(exploit1 , pwnies), replay attack(exploit1 ) ¬ culprit(exploit1 , robotMafia)  last attack(exploit1 , apt8), replay attack(exploit1 ) replay attack(exploit1 ) - attack(exploit1 , bluelotus), last attack(exploit1 , apt8) deception(exploit1 , apt8) - replay attack(exploit1 ), first attack(exploit1 , robotmafia) culprit(exploit1 , apt8) - deception(exploit1 , apt8), replay attack(exploit1 ) ¬culprit(exploit1 , apt8) - time diff(interval, robotmafia)

:

1 = 2 =

:

1 = 2 = 3 = 4 =

Fig. 1: A ground argumentation framework.
A1 , A2 , A3 , A4 , replay attack(exploit1 ) deception(exploit1 , apt8) culprit(exploit1 , apt8) ¬culprit(exploit1 , apt8) A1 A2 A3 A4 = {1 , 1 , 3 } = {1 , 2 , 2 } = {1 , 2 , 3 } = {1 , 4 , 3 }

Fig. 2: Example ground arguments from Figure 1.

are in conflict and a winner cannot be determined. Fig. 1 shows a ground argumentation framework demonstrating constructs derived from the CTF data. For instance, 1 indicates the fact that exploit1 was used to target the team Blue Lotus, and 5 indicates that team pwnies is the most frequent user of exploit1 . For the strict rules, 1 says that for a given exploit1 the attacker is pwnies if it was the most frequent attacker and the attack exploit1 was replayed. Defeasible rules can be read similarly; 2 indicates that exploit1 was used in a deceptive attack by APT8 if it was replayed and the first attacker was not APT8. By replacing the constants with variables in the predicates we can derive a non-ground argumentation framework. Definition 1. (Argument) An argument for a literal L is a pair A, L , where A   provides a minimal proof for L meeting the requirements: (1) L is defeasibly derived from A2 , (2)      is not contradictory, and (3) A is a minimal subset of  satisfying 1 and 2, denoted A, L . Literal L is called the conclusion supported by the argument, and A is the support. An argument B , L is a subargument of A, L iff B  A. The following examples show arguments for our scenario. Example 2. Fig. 2 shows example arguments based on the
2 This means that there exists a derivation consisting of a sequence of rules that ends in L--that possibly includes defeasible rules.

KB from Fig. 1; here, A1 , replay attack(exploit1 ) is a subargument of A2 , deception(exploit1 , apt8) and A3 , culprit(exploit1 , apt8) . For a given argument there may be counter-arguments that contradict it. For instance, referring to Fig. 2, we can see that A4 attacks A3 . A proper defeater of an argument A, L is a counter-argument that--by some criterion--is considered to be better than A, L ; if the two are incomparable according to this criterion, the counterargument is said to be a blocking defeater. The default criterion used in DeLP for argument comparison is generalized specificity [21]. A sequence of arguments is called an argumentation line. There can be more than one defeater argument, which leads to a tree structure that is built from the set of all argumentation lines rooted in the initial argument. In this dialectical tree, every child can defeat its parent (except for the root), and the leaves represent unchallenged arguments; this creates a map of all possible argumentation lines that decide if an argument is defeated or not. Arguments that either have no attackers or all attackers have been defeated are said to be warranted. Given a literal L and an argument A, L , in order to decide whether or not a literal L is warranted, every node in the dialectical tree T ( A, L ) is recursively marked as "D" (defeated) or "U" (undefeated), obtaining a marked dialectical tree T  ( A, L ) where:  · All leaves in T ( A, L ) are marked as "U"s, and  · Let B , q be an inner node of T ( A, L ). Then, B , q will be marked as "U" iff every child of B , q is marked as "D". Node B , q will be marked as "D" iff it has at least one child marked as "U". Given argument A, L over , if the root of T  ( A, L ) is marked "U", then T  ( A, h ) warrants L and that L is warranted from . (Warranted arguments correspond to those in the grounded extension of a Dung argumentation system [7].) In practice, an implementation of DeLP accepts as input sets of facts, strict rules, and defeasible rules. Note that while the set of facts and strict rules is consistent (non-contrdictory), the set of defeasible rules can be inconsistent. We engineer our cyber-attribution framework as a set of defeasible and strict rules whose structure was created manually, but are dependent on values learned from a historical corpus of data. Then, for a given incident, we instantiate a set of facts for that situation. This information is then provided as input into a DeLP implementation that uses heuristics to generate all arguments for and against every possible culprit for the cyber attack. Dialectical trees based on these arguments are analyzed, and a decision is made regarding which culprits are warranted. This results in a reduced set of potential culprits, which we then use as input into a classifier to obtain an attribution decision. IV. BASELINE A RGUMENTATION M ODEL (BM) In [13] machine learning techniques were leveraged on the CTF data to identify the attacker. We will now provide a sum-

1 = 1 =

culprit(E , Y)  last attack(E , Y), replay attack(E ). replay attack(E ) - attack(E , X), last attack(E , Y).

Fig. 3: Defeasible and strict rule for non-deceptive attack.

mary of the results obtained. The experiment was performed as follows. The dataset was divided according to the target team, building 20 subsets, and all the attacks were then sorted according to time. The first 90% of the attacks were reserved for training and the remaining 10% for testing. The byte and instruction histograms were used as features to train and test the model. Models constructed using a random forest classifier performed the best, with an average accuracy of 0.37. Most of the misclassified samples tend to be deceptive attacks and their duplicates. When using machine learning approaches it is difficult to map the reasons why a particular attacker was predicted, especially in cases of deception where multiple attackers were associated with the same attack. Knowing the arguments that supported a particular decision would greatly aid the analyst in making better decisions dealing with uncertainty. To address this issue we now describe how we can form arguments/rules based on the latent variables computed from the training data, given an attack for attribution. We use the following notation: let E be the test attack under consideration aimed at target team X, Y represent all the possible attacking teams, and D be the set of all deceptive teams (those using the same payload to target the same team) if the given attack is deceptive in the training set. For nondeceptive attacks, D will be empty. We note that facts cannot have variables, only constants (however, to compress the program for presentation purposes, we use meta-variables in facts). To begin, we define the facts: 1 = attack (E , X), 2 = first attack (E , Y), 3 = last attack (E , Y); 1 states that attack E was used to target team X, 2 states that team Y was the first team to use the attack E in the training data, and similarly 3 states that team Y was the last team to use the attack E in the training data. The first and last attacking team may or may not be the same. We study the following three cases: Case 1: Non-deceptive attacks. In non-deceptive attacks, only one team uses the payload to target other teams in the training data. It is easy to predict the attacker for these cases, since the search space only has one team. To model this situation, we define a set of defeasible and strict rules. In Fig. 3, defeasible rule 1 checks whether the attack was replayed in the training data. Since it is a non-deceptive attack, it can only be replayed by the same team. The strict rule 1 then puts forth an argument for the attacker (culprit) if the defeasible rule holds and there is no contradiction for it. Case 2: Deceptive attacks. These attacks form the majority of the misclassified samples in [13]. The set D is not empty for this case; let Di denote the deceptive teams in D. We also compute the most frequent attacker from the training data given a deceptive attack. Let the most frequent deceptive

1 = decep (E , X), 2 = frequent (E , F ) 1 = 2 = 1 = 2 = 3 = ¬culprit(E , Y)  first attack(E , Y), decep(E , X) culprit(E , F )  frequent(E , F ), deception (E , Di ) replay attack(E ) - attack(E , X), last attack(E , Y) deception(E , Di ) - replay attack(E ), first attack(E , Y) culprit(E , Di ) - deception(E , Di ), first attack(E , Y)
Average time (sec) - Log Scale

10000

1000

100

10

Fig. 4: Facts and rules for deceptive attacks.

1

T-5

T-14

T-1

T-2

T-3

T-4

T-6

T-7

T-8

T-9

T-10

T-11

T-12

T-13

T-15

T-16

T-17

T-18

T-19

Deceptive

Replay

attacker be denoted as F . The DeLP components that model this case are shown in Figure 4; fact 1 indicates if the attack E was deceptive towards the team X and 2 indicates the most frequent attacker team F from the training set. The strict rule 1 indicates that in case of deception the first team to attack (Y) is not the attacker, 2 states that the attacker should be F if the attack is deceptive and F was the most frequent deceptive attacker. For the defeasible rules, 1 deals with the case in which the attack E was replayed, 2 deals with the case of deceptive teams from the set D, 3 indicates that all the deceptive teams are likely to be the attackers in the absence of any contradictory information. and 4 states that the attacker should be F if the attack is deceptive and F was the most frequent attacker. Case 3: Previously Unseen Attacks. The most difficult attacks to attribute in the dataset are the unseen ones, i.e. attacks first encountered in the test set and thus did not occur in the training set. To build constructs for this kind of attack we first compute the k nearest neighbors from the training set according to a simple Euclidean distance between the byte and instruction histograms of the two attacks. In this case we choose k = 3. For each of the matching attacks from the training data we check if the attack is deceptive or nondeceptive. If non-deceptive, we follow the procedure for Case 1, otherwise we follow the procedure for Case 2. Since we replace one unseen attack with three seen attacks, the search space for the attacker increases for unseen attacks. Attacker Time Analysis: The CTF data provides us with time stamps for the attacks in the competition. We can use this information to come up with rules for/against an argument for a team being the attacker. We compute the average time for a team to replay its own attack given that it was the first one to deploy the attack (see Fig. 5). It can be observed that teams like more smoked leet chicken (T-13) and Wowhacker-bios (T8) are very quick to replay their own attacks as compared to other teams. Fig. 5 also shows the average time for a team to perform a deceptive attack. Teams like The European (T-7) and Blue lotus (T-10) are quick to commit deception, while others take more time. We use this time information to narrow down our search space for possible attackers. In particular, for a deceptive test sample, we compute the time difference between the test sample and the training sample that last used the same payload. We denote this time difference as t, and include it as a

Fig. 5: Average time for team to perform a deceptive attack and replay its own attack (Log-scale).
: : 1 = 1 = timedifference (E , X) For Y  / interval: ¬culprit(E , Y) - timedifference (E , X).

Fig. 6: Time facts and rules. Interval indicates a small portion of the entire deceptive time (for instance < 2000 sec, > 8000 sec and so on).

fact 1 . We then divide the deceptive times from Fig. 5 into appropriate intervals; each team is assigned to one of those time intervals. We then check which time interval t belongs to and define a defeasible rule 1 that makes a case for all teams not belonging to the interval to not be the culprits, as shown in Fig. 6. We now provide a summary of the experimental results-- the setup is similar to [13]: the dataset is sorted by time for each target team, the first 90% of the data is used for training and the remaining 10% for testing. The constructs for all test samples based on the cases discussed in the previous section are computed, and these arguments are used as input to the DeLP implementation. For each test sample the DeLP system is queried to find all possible attackers (culprits) based on the arguments provided. If there is no way to decide between contradicting arguments, these are blocked and thus return no answers. Initially, the search space for each test sample is 19 teams (all except the one being attacked). After running the queries to return the set of possible culprits, the average search space across all target teams is 5.85 teams. This is a significant reduction in search space across all target teams; to gauge how much the reduced search space can aid an analyst in predicting the actual culprit, a metric is computed that checks if the reduced search space contains the ground truth (actual culprit). For all the target teams, the ground truth is present on average in almost 66% of the samples with reduced search space. For some teams like more smoked leet chicken (T-13) and raon ASRT (whois) (T-17) the average reduced search space is as low as 1.82 and 2.9 teams, with high ground truth fraction of 0.69 and 0.63, respectively. Predictive analysis is then performed on the reduced search

T-20

space. The experimental setup is similar to the one described earlier; the only difference this time is instead of having a 19 team search space as in [13], the machine learning approach is allowed to make a prediction from the reduced search space only; a random forest is used for learning, which has been shown to have the best performance for CTF data [13]. We report the following average accuracies across 20 target teams; the accuracy achieved after running Random forest without applying the argumentation-based techniques, as reported in [13], is 0.37. This was the best performing approach using standard machine learning techniques. The baseline model achieves an average accuracy of 0.5, which is significantly better than the average accuracy of 0.37 in [13]. V. E XTENDED BASELINE M ODEL I (EB1) Previously Unseen attacks make up almost 20% of the test samples for each target team. On analyzing the misclassification from the baseline argumentation model, we observe that the majority of the previously unseen attacks get misclassified (>80%). The misclassifications can be attributed to two reasons: (i) the reduced search space is not able to capture the ground truth for unseen attacks, leading the learning model to a wrong prediction; and (ii) we represent each unseen attack by the 3 most similar attacks in the training data; this leads to an increase in the search space and many choices for the learning model. We address these issues by proposing two sets of defeasible rules. First, for each target team we compute from the training set the top 3 teams that come up with the most unique exploits, as these teams are more likely to launch an unseen attack in the test set. The intuition behind this rule is the fact that not all teams write their own exploits, most teams just capture a successful exploit launched by other teams and repackage it and use it as their own (deception). The second set of rules is proposed to avoid addition of less similar teams to the reduced search space. In the baseline model we use 3-nearest neighbors to represent an unseen attack. In this extended version we consider only the nearest neighbors that are less than a particular threshold value T , which is decided for each target team separately. So, each attack will be represented by k  3 teams depending upon the threshold requirement. In addition to the baseline model rules, we propose the following rules for deceptive attacks. Let U denote the set of teams with the three highest numbers of unique attacks in the training data. Also, let N denote the set of three most similar culprits for the given unseen attack. The extended model is shown in Fig. 7; the fact 1 indicates the teams present in N and whose similarity is less than a particular threshold T , and 2 indicates if the team ui was one of most unique attackers from set U . For the defeasible rules, 1 makes use of the fact 1 stating that the teams in N that satisfy the threshold condition are likely to be the culprits, and 2 indicates that if ui is a unique attacker then it can be the culprit unless contradictory information is available. U is independent of the test samples and will be the same for all unseen attacks given a target team.

:

1 = 2 =

For (ni  N and sim < T ): threshold(E , T ) For ui in U : unique(E , ui ) culprit(E , ui ) - threshold(E , T ) For ui  U : culprit(E , ui ) - unique(E , ui )

:

1 = 2 =

Fig. 7: Rules for unseen attacks.
: : 1 = 1 = timedifference (E , X) For Y  interval: culprit(E , Y) - timedifference (E , X).

Fig. 8: Time facts and rules. Interval indicates a small portion of the entire deceptive time (for instance < 2000 sec, > 8000 sec and so on).

On the contrary, for each of the similar payloads (three or fewer) computed from the training data we check if the attack is deceptive or non-deceptive. If non-deceptive, we follow the procedure for Case 1, otherwise we follow the procedure for Case 2 stated in the baseline argumentation model. Experiment: We evaluate EB1 using an experimental setup similar to the one for the baseline argumentation model. We report the average reduced search space and prediction accuracy for both EB1 and baseline model to provide a comparison. EB1 performs better than the baseline with an average accuracy of 0.53 vs. 0.50, and significantly better than the machine learning model without argumentation that has an average accuracy of 0.37. The improvement in performance is due to the larger fraction of reduced search spaces with ground truth present in them. Also, the search space reduced from on average 6.07 teams to 5.025 (less teams to consider). The results are reported in Table 3 along with a comparison to the second extended baseline argumentation model (EB2). VI. E XTENDED BASELINE M ODEL II (EB2) Another source of misclassification in the baseline argumentation model is the presence of unseen deceptive teams and their duplicates. These refer to teams that did not use the exploit in the training set but started using it in the test set. It is difficult for a machine learning approach to predict such a team as being the culprit if it has not encountered it using the exploit in the training set. In our dataset these attacks comprise 15% of the total, and up to 20% for some teams. In order to address this issue we propose an extension of EB1, where we group together teams that have similar deceptive behavior based on the time information available to us from the training set; for instance teams that are deceptive within a certain interval of time (e.g., less than 2,000 secs.) after the first attack has been played are grouped together. For a given test attack we compute the time difference between the test attack and the last time the attack was used in the training set. We then assign this time difference to a specific

group based on which interval the time difference falls in. In order to fine tune the time intervals, instead of using the average deceptive times averaged across all target teams (as used in the baseline model), we compute and use deceptive times for each target team separately. We model the time rules as stated in Fig. 8; fact 1 states the time difference between the test sample and the last training sample to use that attack, defeasible rule 1 on the other hand states that teams belonging to that interval (in which the time difference lies) are likely to be the culprits unless a contradiction is present. It is clear that this rule will increase the search space for the test sample, as additional teams are now being added as likely culprits. We observe that for EB2 the search space is increased by an average of almost 2.5 teams per test sample from EB1; at the same time the presence of ground truth in the reduced search space increased to 0.78, which is a significant improvement over 0.68. Experiment: We evaluate EB2 using an experimental setup similar to the one discussed in the baseline argumentation model. We report the prediction accuracies for each of the proposed baseline argumentation models for each of the target teams and compare it with the previous accuracy reported in [13], denoted as ML. In Table 3 the second extended baseline model (EB2) performs the best with an average prediction accuracy of 62% as compared to other proposed methods. The additions of teams based on time rules not only benefits detection of unseen deceptive teams but it also helps in predicting attackers for unseen attacks. The major reason for the jump in performance is that for most unseen deceptive team samples, the time rules proposed in the baseline model block all deceptive teams from being the culprit, leading to an empty set of culprits. The new set of rules proposed in EB2 adds similar-behaving teams to this set based on time information; the learning algorithm can then predict the right one from this set. VII. C ONCLUSION In this paper we demonstrated how an argumentationbased framework (DeLP) can be leveraged to improve cyberattribution decisions by building DeLP programs based on CTF data; this affords a reduction of the set of potential culprits and thus greater accuracy when using a classifier for cyber attribution. We are currently looking at implementing a probabilistic variant of DeLP [19], as well as designing our own CTF event in order to better mimic real-world scenarios. Our new CTF will encourage deceptive behavior among the participants, and we are also enhancing our instrumentation of the CTF, allowing for additional data collection (host data is of particular interest). VIII. ACKNOWLEDGMENTS Authors of this work were supported by the U.S. Department of the Navy, Office of Naval Research, grant N00014-151-2742 as well as the Arizona State University Global Security Initiative (GSI) and by CONICET and Universidad Nacional del Sur, Argentina.

TABLE 3: Results Summary
Team T-1 T-2 T-3 T-4 T-5 T-6 T-7 T-8 T-9 T-10 T-11 T-12 T-13 T-14 T-15 T-16 T-17 T-18 T-19 T-20 ML [13] 0.45 0.22 0.30 0.26 0.26 0.5 0.45 0.42 0.41 0.30 0.37 0.24 0.35 0.42 0.30 0.43 0.42 0.48 0.41 0.48 BM 0.51 0.45 0.40 0.44 0.45 0.49 0.53 0.61 0.50 0.42 0.44 0.43 0.63 0.52 0.38 0.48 0.58 0.50 0.51 0.51 EB1 0.52 0.38 0.47 0.42 0.45 0.55 0.56 0.58 0.53 0.41 0.5 0.36 0.64 0.53 0.55 0.55 0.58 0.52 0.56 0.64 EB2 0.60 0.43 0.66 0.44 0.56 0.7 0.66 0.74 0.76 0.41 0.73 0.52 0.75 0.67 0.64 0.65 0.68 0.65 0.68 0.71

R EFERENCES
[1] A. Applebaum, K. Levitt, Z. Li, S. Parsons, J. Rowe, and E. Sklar. Cyber reasoning with argumentation: Abstracting from incomplete and contradictory evidence. In Proc. of MILCOM, 2015. [2] W. E. Boebert. A survey of challenges in attribution. In Proc. of a workshop on Deterring CyberAttacks, pages 41­54, 2010. [3] M. Br¨ uckner, C. Kanzow, and T. Scheffer. Static prediction games for adversarial learning problems. The Journal of Machine Learning Research, 13(1):2617­2654, 2012. [4] M. Br¨ uckner and T. Scheffer. Stackelberg games for adversarial prediction problems. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 547­555. ACM, 2011. [5] M. Dacier, V.-H. Pham, and O. Thonnard. The wombat attack attribution method: some results. In Information Systems Security, pages 19­37. Springer, 2009. [6] DEFCON. DEFCON: Capture the flag. https://media.defcon.org/, 2013. [Online; accessed January-2015]. [7] P. M. Dung. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artificial intelligence, 77(2):321­357, 1995. [8] A. J. Garc´ ia and G. R. Simari. Defeasible logic programming: An argumentative approach. Theory and practice of logic programming, 4(1+ 2):95­138, 2004. [9] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar. Adversarial machine learning. In Proceedings of the 4th ACM workshop on Security and artificial intelligence, pages 43­58. ACM, 2011. [10] B. Li and Y. Vorobeychik. Feature cross-substitution in adversarial classification. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2087­2095. Curran Associates, Inc., 2014. [11] J. W. Lloyd. Foundations of logic programming. Springer Science & Business Media, 2012. [12] D. Lowd and C. Meek. Adversarial learning. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 641­647. ACM, 2005.

[13] E. Nunes, N. Kulkarni, P. Shakarian, A. Ruef, and J. Little. Cyberdeception and attribution in capture-the-flag exercises. In Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2015, Paris, France, August 25 - 28, 2015, pages 962­965, 2015. [14] I. Rahwan, G. R. Simari, and J. van Benthem. Argumentation in artificial intelligence, volume 47. Springer, 2009. [15] T. Rid and B. Buchanan. Attributing cyber attacks. Journal of Strategic Studies, 38(1-2):4­37, 2015. [16] J. Salvit, Z. Li, S. Perumal, H. Wall, J. Mangels, S. Parsons, and E. I. Sklar. Employing argumentation to support human decision making: A user study. In AAMAS Workshop on Argumentation in Multiagent Systems, 2014. [17] P. Shakarian, J. Shakarian, and A. Ruef. Introduction to cyber-warfare: A multidisciplinary approach. Elsevier, 2013. [18] P. Shakarian, G. I. Simari, G. Moores, and S. Parsons. Cyber attribution: An argumentation-based approach. In Cyber Warfare: Building the Scientific Foundation, pages 151­171. Springer, 2015. [19] P. Shakarian, G. I. Simari, G. Moores, D. Paulo, S. Parsons, M. Falappa, and A. Aleali. Belief revision in structured probabilistic argumentation. Annals of Mathematics and Artificial Intelligence, pages 1­43, 2015. [20] E. I. Sklar, S. Parsons, Z. Li, J. Salvit, S. Perumal, H. Wall, and J. Mangels. Evaluation of a trust-modulated argumentation-based interactive decision-making tool. Autonomous Agents and Multi-Agent Systems, pages 1­38, 2015. [21] F. Stolzenburg, A. J. Garc´ ia, C. I. Chesnevar, and G. R. Simari. Computing generalized specificity. Journal of Applied Non-Classical Logics, 13(1):87­113, 2003. [22] R. J. Walls. Inference-based Forensics for Extracting Information from Diverse Sources. PhD thesis, University of Massachusetts Amherst, 2014.

MIST: Missing Person Intelligence Synthesis Toolkit
Elham Shaabani, Hamidreza Alvari, and
Paulo Shakarianâˆ—
Arizona State University
Tempe, AZ 85281

J.E. Kelly Snyder
Find Me Group
Chandler, AZ 85249

kelly@findmegroup.org

{shaabani, halvari, shak}@asu.edu

ABSTRACT
Each day, approximately 500 missing persons cases occur
that go unsolved/unresolved in the United States. The nonprofit organization known as the Find Me Group (FMG),
led by former law enforcement professionals, is dedicated to
solving or resolving these cases. This paper introduces the
Missing Person Intelligence Synthesis Toolkit (MIST) which
leverages a data-driven variant of geospatial abductive inference. This system takes search locations provided by a
group of experts and rank-orders them based on the probability assigned to areas based on the prior performance of
the experts taken as a group. We evaluate our approach
compared to the current practices employed by the Find Me
Group and found it significantly reduces the search area leading to a reduction of 31 square miles over 24 cases we
examined in our experiments. Currently, we are using MIST
to aid the Find Me Group in an active missing person case.

Keywords
Geospatial abduction; abductive inference; law enforcement;
missing person

1.

INTRODUCTION

Each day, approximately 500 missing persons cases occur
that go unsolved/unresolved in the United States. The nonprofit organization known as the Find Me Group (FMG),
led by former law enforcement professionals, is dedicated
to solving or resolving these cases. This non-profit operates with limited resources - so it must use its volunteer assets in a highly efficient manner. This paper introduces the
Missing Person Intelligence Synthesis Toolkit (MIST) which
leverages a data-driven variant of geospatial abductive inference [24]. This system takes search locations provided by
a group of experts and rank-orders them based on the probability assigned to areas based on the prior performance of
âˆ—
U.S. Provisional Patent 62/345,193. Contact shak@asu.edu for
licensing information.

the experts taken as a group. We evaluate our approach
compared to the current practices employed by the FMG
and found it significantly reduces the search area. In 24
cases examined in our experiments (on real-world data provided by FMG), we found our approach to be able to reduce
total search area by a total of 31 square miles for standard
searches and by 19 square miles when dog team assets obtain
a detection. This reduction is significant for the following
reasons:
â€¢ Reduction in time to locate missing persons.
In cases where baseline provided 20 square miles or
more (the most difficult cases), we achieved reduction
in search area of 7 to 56 square miles. As 3-5 square
miles are searched on a typical day (terrain dependent), such a reduction can potentially increase the
chance of a missing person being found alive.
â€¢ Reduction in direct costs. During a search, FMG
spends approximately $2200 per day. In all tests, our
approach reduced the search area in the majority of
cases which can be interpreted as a reduction in direct
costs.
â€¢ Reduction in indirect costs. FMG relies extensively on volunteers to augment searches. During searches,
these individuals often lose earnings from their day job
or small business. As many volunteers also perform
consulting or other services to law enforcement, longer
searches lead to loss of revenue and opportunity. In
one case, a volunteer estimated a loss of $15K. Again,
our approach leads to a consistent reduction in search
area - hence reducing these indirect costs.
Specifically, we contribute an extension to geospatial abduction [24] that leverages historical data of individual experts. We also create new algorithms to learn parameters
of a geospatial abduction model from data based on integer
programming. We then evaluate these algorithms on realworld data provided by the FMG under a variety of different
settings. This approach learns pattern of each reporter independently and is able to overcome outliers if any. It also
does well on the limited data. This work has prepared us
in our ongoing deployment of the software. At the time of
this writing, we have provided results of MIST to support
an active case with FMG. Figure 1 shows an example output of MIST where it rank-orders search locations. FMG is
currently using this information to support their operations.
They found the result consistent with their experiences.

Figure 1: Mapping of ordered grids by MIST (green squares)
and current searched area by FMG (red square).
The rest of the paper is organized as follows. In Section 2,
we present the background of the missing person problem.
Next, we provide the technical preliminaries. We discuss
our data-driven extension in Section 4. In Section 5, we
detail our algorithmic approach. We introduce our dataset
and conduct data analysis in Section 6. Next, we discuss
the experimental results in Section 7. We review the related
work in Section 8. We conclude the paper presenting future
research directions.

2.

BACKGROUND

Missing persons cases have been on the rise in the USA
for the past twenty years. Currently, approximately 4000
people go missing each and every day. Approximately 3500
of those cases are solved or resolved (i.e., cases solved by
only providing accurate information to the authorities and
without physical involvement), which leaves an astounding
number of victims that are never located. In the case of
missing adults 13 years of age and older, the police are not
required or obligated to conduct an investigation or search
unless there are extenuating circumstances such as suicide, a
potential for violence, medical reasons, etc. This leaves families and friends without professional assistance in locating
their loved ones. The Find Me Group (FMG) was founded
by retired U.S. Drug Enforcement Agency (DEA) Special
Agent J.E. â€œKellyâ€ Snyder in 2002. The group consists of
current and retired law enforcement officers with a widerange of investigative expertise, including but not limited
to linguistics, handwriting analysis, body language, missing person/homicide experience and search-and-rescue field
management skills. The FMG has trained experts/sources
that provide detailed location information where missing individuals can be found. Many of these experts have the
ability to provide GPS coordinates to locate missing persons
with a varying levels of success. The FMG focus/goal is to
provide accurate location information in a timely manner
and minimize the potential of finding the victim deceased.
Thirty canine handlers certified in tracking, scent and cadaver complements the FMG and has led to many instances
where the person in questions was located.
Equally disturbing nationwide is the rise in human trafficking, which aligns within the missing person category.
This type of crime has long-term and devastating results.
The work of this paper is also the first step toward an allencompassing methodology of identifying locations of missing persons who were victims of human trafficking. Another

important related crime is homicide. Many missing persons
and human trafficking victims are found deceased due to this
crime. This work represents initial progress in aiding toward
crimes of this nature as well.
In this paper, we formulate the problem of â€œfinding missing personâ€ with respect to information provided by FMGâ€™s
experts, formally as a variant of the geospatial abduction
problem (GAP) [22]. To account for the key nuances of
â€œfinding missing personâ€ problem though, we extended the
GAP framework to better suite this domain. In particular,
we extend the GAP formalism with a data driven model accounting for the previous performance of experts aiding in
the missing person cases. We list the unique characteristics
of our framework here. Later in the next section, we provide
our technical approach to each.
1. Explanation Size. One key difference â€œfinding missing personâ€ problem has from other GAP instances, is
that the explanation (the result of a GAP inference algorithm) only consists of a single related location (i.e.,
the location of the missing person) corresponding to
the phenomenon under study. This differs from returning a set of k locations in the previously-introduced
GAP formalisms. Consequently, here, an explanation
will consist of a single point, which in turn led us to
explore a non-deterministic version of the original explanation.
2. Distance Constraints. In the original GAP formalism, each observed geospatial phenomenon is related
to unobserved â€œpartnerâ€ points through a distance constraint - (Î±,Î²) where Î± is the minimum distance between an observation and partner and Î² is the maximum distance. As described, this pair of constraints
was the same for all observations. However, in the
missing persons problem, each observation corresponds
to a different domain expert - and hence has a different (Î±,Î²) constraint pair. Further, we study how this
is best learned from data, as well as â€œsoftenâ€ the constraint - assigning a probability of the partner point
being less than Î±, between distances Î± and Î², and
greater than distance Î² from an observation.
3. Uncertainty. As we learn the (Î±,Î²) distance constraints for each observation and associate corresponding probabilities from historical data, it makes sense
that the inference step is treated probabilistically which differs from the original deterministic GAP framework. Further, this enables us to rank the potential
partner locations (again, as an explanation consists of
one point, ranking search locations is more useful in a
practical sense).
4. Independent Observations. In the original GAP
framework, independence amongst the observations was
not an assumption in the framework. However, FMG
compartmentalizes the information from their law enforcement experts from one another in a manner to
obtain independent reporting. Hence, we make this
assumption in this paper and it is supported by our
experimental results.
FMG currently uses a simple heuristic to rank-order potential search locations for a missing person (we describe this

later in Section 5). Once ranked, FMG leverages a variety of assets. Figure 2 depicts a recently searched area for
a case. It represents a screen shot of the tracks from the
GPS units that the dogs wear as well as the handheld units
that the searchers wear. This shows several dog tracks and
the human tracks. The green, dark blue, magenta represent
three dogs, the grey and red represent two human searchers.
The teal track is a trailing dog, ascertaining a direction of
travel. The straight lines tend to be humans and the rapidly
changing direction lines are dogs as they grid around the humans. Figure 3 shows real-world examples of how the FMG
practices in an undisclosed location.

in [23] and later extended in [24, 21, 20, 19]. More formally, each GAP consists of three major elements [22]: (1)
observations: a set of observations that explain the locations associated with the event under study (e.g., in this
application, the locations reported by the domain experts),
(2) distance constraints: a pair (Î±, Î²) âˆˆ R corresponding to
lower and upper bounds on the distances between observation and partner location and, (3) feasibility predicate: this
allows to specify whether an area on the map is a potential
location for a partner.
Next, we present the notations and definitions used throughout the paper, and review the geospatial abduction framework of [22]. In the next section, we describe specialized
extensions that were necessary to study our problem. First,
without loss of generality, we assume throughout the paper
that a map (resp. space) is represented by a discrete two
dimensional grid of size M Ã— N , defined as follows:
Definition 3.1. (Space). Given natural numbers M ,
N , the space S is the set [1, . . . , M ] Ã— [1, . . . , N ].

Figure 2: Screen shot of the tracks from the GPS units.

3.

TECHNICAL PRELIMINARIES

In this section, we briefly explain geospatial abductive inference [24], and introduce our new (introduced in this paper) data-driven probabilistic extension. We show how this
extension was used to address the unique characteristics of
the missing person location problem.
In general, abduction or abductive inference [12] refers to
a type of logic or reasoning to derive plausible explanations
for a given set of facts [13]. Abduction has been extensively
studied in medicine [13, 14], fault diagnosis [3], belief revision [11], database updates [8, 4] and AI planning [5]. Two
major existing theories of abduction include logic-based abduction [6] and set-covering abduction [2]. Though none of
the above papers takes into account spatial inference, [25]
presents a logical formalism dealing with objectsâ€™ spatial occupancy, while [18] describes the construction of a qualitative spatial reasoning system based on sensor data from a
mobile robot.
Geospatial abduction problem (GAP) [22], on the other
hand, refers to the problem of identifying unobserved partner locations (i.e., the location of a missing person) that
best explain a set of observed phenomenon with known geographic locations. Geospatial abduction was first introduced

(a)

(b)

Figure 3: (a) Picture of the search area taken from the plane.
(b) Search team.

Associated with the space is a distance function d : S Ã—
S â†’ R+ that satisfies the normal distance axioms: d(pi , pi ) =
0, d(pi , pj ) = d(pj , pi ), and d(pi , pj ) â‰¤ d(pi , pq ) + d(pq , pj ).
Note that we use o to represent the observer (source of information) and po to represent the location he/she reported
(which differs slightly from the original framework). From
these observations (reports), the corresponding unobserved
phenomenon is the actual location of the missing person.
In the original framework, the explanation consisted of geographic locations that were located at least distance Î± and
no more than distance Î² away from each observation. In
this work, we generalize this notion by providing Î±,Î² pair
for each observer - denoted Î±o ,Î²o .
Definition 3.2. (Feasibility Function). A feasibility
function feas is defined as feas : S â†’ {True, False}.
A key use for the feasibility function here is for an initial
reduction of the search space by the FMG. This is due to the
fact that missing person reports often span a large area and
an initial reduction is necessary for practical reasons. An
obvious future direction would be to utilize a probabilistic
variant of the feasibility function - which would assign a prior
probability to a location for a missing person. However, in
this application, it is unclear where such a distribution would
come from. Further, as the search space is relatively large
when compared to FMG resources, the deterministic version
of this definition is more appropriate for operational reasons.
Due to resource constraints and the generally large areas
over which reports are spread, FMG typically only searches
areas for which there is a report. As we shall describe in
Section 5, they search a 1 Ã— 1 mile square surrounding a
location reported by an observer. As such is the case, we
shall assume the following feasibility function throughout
this paper:
(
True
if p âˆˆ O
feas(p) =
(1)
False otherwise
Unless otherwise noted, we shall assume the above function is used for feasibility and hence the subset of the space
considered will be the points in O.
We now come to the important definition of an explanation. Intuitively, for a given set of points {p1 , . . . , p|O| }

reported by observers in O, an explanation is a set of points
E such that every point in this set is feasible and for every
observation, there is a point in E that is at least Î± units
away from the observation, but no more than Î² units from
the observation.
Definition 3.3. ((Î±,Î²) Explanation). Suppose O is
the set of observations, E is a finite set of points in S, and
0 â‰¤ Î±, Î² â‰¤ 1 are two real numbers. E is said to be an (Î±, Î²)
explanation of O iff:
â€¢ p âˆˆ E implies that feas(p) = True, i.e., all points in E
are feasible.
â€¢ (âˆ€o âˆˆ O)(âˆƒp âˆˆ E) Î± â‰¤ d(p, o) â‰¤ Î², i.e., every observation is neither too close nor too far from some point
in E.
Thus, an (Î±,Î²) explanation is a set of points. Each point
must be feasible and every observation must have an analogous point in the explanation which is neither too close nor
too far.
Again, we note that here an explanation will consist of a
single point - the location of the missing person. Hence, this
deterministic definition of an explanation will not suffice as in practice there will often not exist an explanation for
a given problem instance. As such is the case, we extended
this framework using a data-driven approach.

4.

DATA-DRIVEN EXTENSIONS

In this section, we describe our data-driven probabilistic
extension to the original GAP formalism. The framework
extensions in this section were not previously introduced and
are new in this paper. In order to do so, we first introduce
some preliminary notation. For point p âˆˆ S, the random
variable Pp denotes that the missing person was found at
point p, so this is either true or false. We will use Pp as
shorthand for Pp = True. For observer o âˆˆ O the random
variable Oo can be assigned to one of the points in p. Based
on this notation, we define an explanation distribution.
Definition 4.1 (Explanation Distribution). Given
a set of observers O and a set of reported locations by each
observer p1 , . . . , po , . . . , p|O| , an explanation distribution
is a probability distribution over all points in S - directly addressing characteristic 3 of this application (see Section 2).
This distribution assigns the probability of a missing person
being located at each point conditioned on the observers reporting V
their respective locations. Formally, it is written as
P r(Pp | oâˆˆO Oo = po ).
The key intuition is that if we are able to compute an explanation distribution, we can then rank-order points in the
space by probability - and hence conserve search resources.
Note that the explanation distribution is over all points implying that there is precisely one location. While generalizations that allow for more than one location are possible
in such a probabilistic framework, we keep the size at one
due to the first characteristic of our problem (as described
in Section 2).
In this paper, we make an assumption of distance primacy
meaning
the distance constraints (Î±o , Î²o ) relate the Pp with
V
oâˆˆO Oo = po . Hence, we introduce another random vari0
o
able, RÎ²p,p
0 which is true if d(p, p ) â‰¤ Î²o and false otherwise.

Note that in the remainder of this section, we will use one
distance constraint (Î²) for sake of brevity - though this idea
can be extended for multiple distance constraints (as per
characteristic 2 from Section 2). In fact, we leverage multiple distance constraints in our optimization procedure for
parameter selection introduced later. Hence, by distance
primacy, we have the following relationships.
^
^ Î²
P r(Pp |
Oo = po ) = P r(Pp |
Rp,po )
(2)
oâˆˆO

oâˆˆO

By Bayesâ€™ Theorem, this is equivalent to the following.
V
P r(Pp ) Ã— P r( oâˆˆO RÎ²p,po |Pp )
(3)
V
P r( oâˆˆO RÎ²p,po )
However, by characteristic 4, we assume that the observers
report information independently, which gives us the following.
Q
P r(Pp ) Ã— oâˆˆO P r(RÎ²p,po |Pp )
(4)
V
P r( oâˆˆO RÎ²p,po )
Due to our application, we will not consider the prior probability P r(Pp ) as each missing person case occurs in a different geographic location - and due to the wide range of
cases that span multiple countries, data supporting a realistic, informed prior is highly sparse. As such, we consider a
uninformed prior. Further, for notational simplicity, we shall
use the notation ÏÎ²o for the quantity P r(RÎ²p,po = True|Pp =
True). Therefore, we can rank points in the space based
on the explanation distribution by simply considering their
log-likelihood computed as follows:
X
X
log(ÏÎ²o ) +
log(1 âˆ’ ÏÎ²o )
(5)
oâˆˆO
d(p,po )â‰¤Î²

oâˆˆO
d(p,po )>Î²

Hence, the inference step for this problem is straightforward provided we know the values Î² and ÏÎ²o for each
observer o âˆˆ O (or similar parameters if considering more
than one distance constraint). If we know the value Î² we
can then compute ÏÎ²o based on a corpus of historical data
concerning the accuracy of reporter o. Given a corpus of
previous cases for the observer Co where the found location
was pc and the location reported by the observer was pco , we
can compute ÏÎ²o as follows:
ÏÎ²o =

|{c âˆˆ Co s.t. d(pc , pco ) â‰¤ Î²}|
|Co |

(6)

Hence, we also adjust ÏÎ²o to account for volume of the reporterâ€™s history to provide the effect of regularization. Considering Î·o as the portion of total number of cases in which
observer o has participated, to the total number of cases,
and  as a non-negative parameter, we define ÏÎ²,
as follows:
o
ÏÎ²,
= ÏÎ²o âˆ’  Ã— (1 âˆ’ Î·o )
o

(7)

The situation is further complicated with multiple distance constraints. We propose an optimization approach to
this problem in the next section.

5.

ALGORITHMIC APPROACH

In this section, we present our algorithmic approach to
special case of geospatial abductive inference. First, we ex-

plain the method that FMG currently uses. Then, we provide our proposed optimization approach to solve the problem.

also minimize the following quantity:
XX X
X h
0
F2 =
Î´Î² (p, pco ) Ã— log ÏoÎ² Ã— Xo,Î²
câˆˆC oâˆˆO pâˆˆ{S\pc } Î²âˆˆ[Î²o ]

5.1

Existing Method

0

The FMG uses the following method to explore the missing person location. Given the reported locations provided
by different observers, FMG initially creates a search area
(grid) as follows. First, they draw building blocks (or boxes)
of size 1Ã—1 mile centered at each reported location (note that
depending on the situation, these boxes may overlap). Then,
they search the entire grid in the following order. First, they
search the larger areas created of the overlapping boxes, and
if the missing person was not found, they explore the remaining boxes in the order of the observersâ€™ history (how
well they did in the past). The whole process is repeated
by extending the size of boxes to 2Ã—2 miles, if the missing
person was not located. Note that, we use the same grid in
our proposed methods.

5.2

Proposed Methods

As described, for simplicity, we first elaborate on the required steps to calculate the best Î²o for each observer. Then,
we extend the idea for multiple distance constraints. Let
[Î²o ] be the set of possible error radii. Note that for Co cases
where observer reported a location, there are at most |Co |
possible values for Î²o . Hence, our goal is to select as a set
of these distance constraints - one for each observer. We do
this through an integer program - where for each observer
o âˆˆ O and each associated distance constraint Î²o âˆˆ [Î²o ]
we have an indicator variable Xo,Î²o that is 1 if we use that
value and zero otherwise. We shall refer to this as the single
constraint integer program. Hence, we find an assignment of
values to these indicator variables in order to maximize the
following quantity:
XX X h
F1 =
Î´Î² (pc , pco ) Ã— log ÏÎ²o Ã— Xo,Î² +

+ (1 âˆ’ Î´Î² (p, pco )) Ã— log(1 âˆ’ ÏoÎ² ) Ã— Xo,Î²

(1 âˆ’ Î´Î² (p

, pco ))

Ã— log(1 âˆ’

ÏÎ²o )

Ã— Xo,Î²

i

(8)

subject to the following constraints:
âˆ€Xo,Î² âˆˆ {0, 1}
âˆ€o,

X

(13)

Therefore, the objective function we seek to optimize is
L1 = max(F1 âˆ’ F2 )

(14)

Theorem 5.1. Number of variables in single-distance constraint integer program is O(avg(|Co |) Â· |O|).
We extend the previous formulation by allowing the objective function to find a pair of distance constraints for each
reporter. We have experimentally found diminishing returns
on performance (and in many cases increased complexity)
with more than two constraints. This will give us the double
distance constraint integer program as follows:
XX X X h
F10 =
Î´Î± (pc , pco ) Ã— log ÏÎ±,
Ã— Xo,Î±,Î²
o
câˆˆC oâˆˆO Î±âˆˆ[Î²o ]Î²âˆˆ[Î²o ]
Î²â‰¥Î±





Î±,
Ã— Xo,Î±,Î² +
+ 1 âˆ’ Î´Î± (pc , pco ) Ã— Î´Î² (pc , pco ) Ã— log ÏÎ²,
o âˆ’ Ïo


i
(1 âˆ’ Î´Î² (pc , pco )) Ã— log 1 âˆ’ ÏÎ²,
Ã— Xo,Î±,Î²
o
subject to the following constraints:

âˆ€o,

âˆ€Xo,Î±,Î² âˆˆ {0, 1}
X
Xo,Î±,Î² â‰¤ 1
Î±,Î²âˆˆ[Î²o ]

Likewise, we use the following objective function, to avoid
bias toward selecting the largest Î²â€™s.
L2 = max(F10 âˆ’ F20 )
where

câˆˆC oâˆˆO Î²âˆˆ[Î²o ]
c

i

F20 =

F20

is defined as follows:
X

XX X X h

(15)

0

Î´Î± (p, pco ) Ã— log ÏoÎ±, Ã— Xo,Î±,Î² +

c
câˆˆC oâˆˆO Î±âˆˆ[Î²o ]Î²âˆˆ[Î²o ] pâˆˆ{S\p }
Î²â‰¥Î±



1 âˆ’ Î´Î± (p, pco ) Ã— Î´Î² (p, pco )Ã—
(9)

Xo,Î² â‰¤ 1

(10)

Xo,Î² = k

(11)

0

0

log(ÏoÎ², âˆ’ ÏoÎ±, ) Ã— Xo,Î±,Î² +




i
0
1 âˆ’ Î´Î² (p, pco ) Ã— log 1 âˆ’ ÏoÎ², Ã— Xo,Î±,Î²

(16)

Î²âˆˆ[Î²o ]

X X
o

Î²âˆˆ[Î²o ]

where k is a cardinality that limits the number of reporters
(which is set to a natural number in the range 1, . . . , |O|),
and Î´Î² (x, y) is defined as:
(
1 if d(x, y) â‰¤ Î²
Î´Î² (x, y) =
(12)
0 if d(x, y) > Î²
However, this equation will result in tendency toward selecting the largest distance constraints. This has the effect of not only maximizing the probability of the locations
where the missing person was found, but also can increase
the probability of other locations. Intuitively, we want to

Theorem 5.2. Number of variables in double distance constraint integer program is O(avg(|Co |)2 Â· |O|).
While we obtained a significant reduction in the area searched
by setting the cardinality constraint k = O, we found that
varying it would often lead to further improvement. We
gradually increased the number of observers from one to the
total number of observers and each time, we learned the
distance constraints for the last added observers. In this
method of optimization, we may choose a specific number
of points in each iteration. The number of points added
with each iteration can be determined based on available
resources.
We also defined two heuristic to discriminate points with
the same probability. In each iteration, we chose the point
with highest probability. If there were more than one point,

Table 1: Description of the dataset

we applied following heuristics: (1) we chose the points
which had most of the reported locations in its 1 Ã— 1 mile.
(2) we chose the point which had the maximum summation
of the priors of the reporters in its 1 Ã— 1 mile.
Algorithm 1 is a specific variant of restricted model. In
this algorithm, in each iteration one point (i.e., representative of a 1 Ã— 1 mile) is selected. Though we note that this
can easily be adjusted in practice. If the area size we are
able to search is larger than number of observers, we sort
the representatives based on their probabilities. Then, we
apply two heuristics to rank them (similar to Lines 11-19 ).

Name
Found Status
Alive
Deceased
Gender
Male
Female
Age
Under 13
13 to 30
30 and older

Algorithm 1 Iterative Search Resource Allocation

6.1

30
25
20
15
10

Overview

6.2

Data Analysis

The dataset consists of cases distributed all over the world.
We split the U.S. based cases into 4 regions, west, midwest,
northeast and south, according to the United States Census
Bureau. We further grouped together all cities outside the
U.S. into one single category, namely, international. The
distribution of cases across different regions is demonstrated
in Figure 4. Though we did not explicitly show in the figure,
the west is dominated by Arizona and California, due to the
large focus of FMG on these two states.
There are several known reasons of disappearance associated with the cases in our dataset including, accidental,

West

South

Midwest

International

Northeast

5

MISSING PERSON DATASET

Our dataset includes cases (i.e., missing persons), found
status (alive/deceased), found location (latitude and longitude), age and reason for disappearance as well as the potential locations (latitude and longitude) associated with the
reporters/experts. The description of this dataset is summarized in Table 1. Note that in some cases, we are aware of
reports, but do not have the found location (pco ). In this
work, we only have 29 cases with the known found locations
used for the experiments. However, for the data analysis,
the entire dataset is applied.

9
39
40

35

Theorem 5.3. The time complexity of the algorithm (1)
is O(|C|Â·avg(|Co |)2 Â· avg(|Oc |)3 ).

In this section, we describe our dataset and briefly discuss
the observation made from our initial data analysis.

41
47

40

0

6.

12
76

bipolar, drowning, foul play, natural, runaway, self-inflicted,
staged and undetermined. According to Figure 5, â€˜foul playâ€™
is the dominant reason for disappearance. There are also different number of reporters for each case. The distribution of
reporters with respect to the number of cases in which they
participated is shown in Figure 6.

Number of cases

1: procedure Opt-Point-By-Point(A, c, S, Ï) . Train
set A, Test case c
2:
List R = âˆ…
. Output
3:
for k âˆˆ [1, |Oc |] do
. k is a constant value of the
constraint
4:
Find assignment of variables that optimize (15)
w.r.t. (9 - 11)
5:
RP â† Order by (5)
. Ranked points RP
6:
RP â† RP \ R
7:
Pick P âŠ† RP with largest probabilities
8:
if P includes one point then
9:
R=RâˆªP
10:
else
11:
p â† Heuristic(P )
12:
R = R âˆª {p}
13:
return R

Value

Regions

Figure 4: Distribution of the cases across different regions
of the US and international.
For the rest of our data analysis, we need to introduce
some preliminary notation. We use the random variable gA
to denote if the missing person is found alive or not, so it is
either true or false. We shall use P r(gA = True|o stated Alive)
to denote the confidence of the observer o in reporting Alive.
This confidence value shows the portion of the cases for
which o has reported the missing person is Alive and he/she
was found Alive, to the total number of cases for which o has
reported Alive. Likewise, we compute the confidence of o in
reporting Deceased. The distribution of the reporters with
respect to their confidence values is demonstrated in Figure 7. According to the figure, most reportersâ€™ confidence
values belong to the ranges of [0.3,0.4) for alive and [0.8,0.9)
for deceased statuses.
We also define the ratio rA as follows:
rA =

P r(gA = True|observer o stated Alive)
P r(gA = True)

(17)

This ratio demonstrates how much the observer o outperformed the prior probability P r(gA = True) on Alive. Similarly, we use rD for Deceased cases. The distributions of the
reporters with respect to rA and rD are shown in Figure 8.
We note that as most are found dead, it is harder for the

16	
Number	of	Reporters	

18	

30
25
20
15
10

Foul play

Undetermined

Accidental

Self-inflicted

Runaway

Natural

Bipolar

0

Staged

5
Drowning

Number of cases

35

14	
12	
10	
8	
6	
4	
2	
0	
0	

1	

2	

3	

4	
rA

5	

6	

7	

8	

180	

Reasons

160	
Number	of	Reporters	

Figure 5: Distribution of the cases with respect to the probable reasons.
45	
Number	of	Reporters	

40	
35	
30	

140	
120	
100	
80	
60	
40	

25	

20	

20	

0	

15	

0.5	

0.7	

10	

0.9	
rD

1.1	

1.3	

5	
0	
1	

2	

3	

4	

5	
6	
7	 10	 13	
Frequency	of	Par=cipa=on	

15	

16	

Figure 8: The distributions of the reporters with respect to
rA and rD .

17	

[0.9,	1]	

[0.8,	0.9)	

[0.7,	0.8)	

[0.6,	0.7)	

[0.5,	0.6)	

[0.4,	0.5)	

[0.3,	0.4)	

[0.2,	0.3)	

alive	
deceased	

[0.1,	0.2)	

18	
16	
14	
12	
10	
8	
6	
4	
2	
0	

[0,	0.1)	

Number	of	Reporters	

Figure 6: Distribution of frequency of participation.

Conï¬dence	

Figure 7: Distribution of all reporters with respect to their
confidence values.

reporters to outperform the prior on Deceased compared to
the Alive.

7.

EXPERIMENTAL RESULTS

This section reports on the experiments conducted to validate our approach. We note that the individual cases themselves are not related - hence we are justified in using leaveone-out cross validation in our experiments. Specifically, for
each case in the experiments, we learn a different model
using all of the other cases. We first compare the methods for restricted (without dog) and unrestricted (with dog)
searches and then discuss the sensitivity of the parameter.

7.1

Area Reduction

In this section, we examine how our approach can be used
to reduce the area searched by the Find Me Group over the
baseline. Figure 9 shows the reduction of area based on our
approach (double distance constraint integer program with
Algorithm 1 and  = 0.1) when compared to the baseline.
We examine this with grid squares of 1Ã—1 miles and 2Ã—2
miles. In the 19 cases where the missing person was located,
our approach achieved area reduction in 11 cases - reducing
the search area by an average by 3 square miles. In the 2
cases where our method caused the search area to increase,
the increase was only 1 square mile in each case. This contrasts with the cases where the area was reduced - reducing
the search area by up to 9 square miles. For the 11 cases
where reduction was experienced, the average reduction was
1.63 miles (t(19) = 1.25, p <0.11).
We also examined cases where the size of the grid squares
was 2Ã—2 miles. In the 19 cases, the area reduction achieved
by our method was in 14 cases, and by an average by 8.5
square miles. Further, in the 6 cases, our method caused
an increase in the search area, however, the increase was 3
square miles on average. Further, for the cases that baseline needs to search areas larger than 20 square miles, our
approach reduced the area from 7 to 56. Our method outperformed the baseline in area reduction with an average of
4.21 mile square (t(20) = 1.19, p <0.13).

7.2

Consideration of Dog Team Detections

The experiments of the previous section illustrated how
our approach could reduce the search area over the baseline
for standard grid settings. However, in the events that a dog

18

18
Baseline
Algorithm 1

14

14

12

12

10
8
6

8
6
4

2

2
0

2

4

6

8

10

12 14
Cases

16

18

20

0

22

(a) Search area with 1 Ã— 1 mile per observation

2

4

6

8

10

12 14
Cases

16

18

20

22

60
Baseline
Algorithm 1

50

Baseline
Algorithm 1

50

Searched Area

40
30
20
10
0

0

(a) Search area with 1 Ã— 1 mile per observation

60

Searched Area

10

4

0

Baseline
Algorithm 1

16

Searched Area

Searched Area

16

40
30
20
10

0

2

4

6

8

10

12 14
Cases

16

18

20

0

22

(b) Search area with 2 Ã— 2 miles per observation

0

2

4

6

8

10

12 14
Cases

16

18

20

22

(b) Search area with 2 Ã— 2 miles per observation

Figure 9: Searched area until the missing person is located
(baseline and Algorithm 1).

Figure 10: Searched area with dogs allowed to explore 1 mile
beyond the grid (baseline and Algorithm 1).

team detects evidence of the missing person, it may lead to a
continued search outside of the assigned grid square. These
searches can lead to FMG personnel examining up to a mile
outside a designated location. In this section, we consider a
grid square settings in the last section, but also allow for an
additional mile outside the square to mimic the effect of the
dog search team following such a lead. Figure 10 demonstrates the reduction of area based on our approach (double
distance constraint integer program with Algorithm 1 and
 = 0.1) when compared to the baseline. We investigate the
area reduction with grid squares of 1Ã—1 miles and 2Ã—2 miles.
According to Figure 10a, in the 22 cases where the missing
person was located, our approach achieved area reduction in
12 cases - reducing the search area by 2 square miles on average. In the 2 cases where our method caused the search area
to increase, the increase was only 3 square miles on average.
This contrasts with the cases where the area was reduced
- reducing the search area by up to 9 square miles. Our
method outperformed the baseline in area reduction with
an average of 0.86 mile square (t(22) = 0.8, p <0.22).
We examined cases where the size of the grid squares was
2Ã—2 miles. In the 24 cases, the area reduction achieved by
our method was in 21 cases, and on average by 8.85 square
miles. In the 3 cases where our method caused the search
area to increase, the increase was 4.3 square miles on aver-

age. This contrasts with the cases with the reduced search
area by up to 56 square miles. Our method outperformed
the baseline in area reduction with an average of 7.2 mile
square (t(24) = 1.95, p <0.05).

7.3

Parameter Sensitivity

We compare different values of  in both double distance
constraint integer programs (iterative search resource allocation and non-iterative program). The impact of changing
the parameter  is shown in Figure 11. To do so, we plot the
fraction of area searched by our method over the baseline,
against the , for both sizes of 1 Ã— 1 and 2 Ã— 2. We note that
while the extreme values of  (i.e. 0.0 and 0.5) negatively
effected the performance of both approaches, we achieved
relatively stable results for intermediate values - noting that
the best performance was to set  equal to 0.1 - which we
used in the experiments.
We also studied the performance of our optimization approach without algorithm 1 (i.e. prioritize locations by equation 5 after selecting the values for Î²o through optimization
of 19 with regards to Lines 9-11). The results are depicted
in Figure 12. The behavior of the algorithm for different
settings of  were similar to that found with Algorithm 1,
the reduction in search area was generally less - and in some

1.4

1.6

Without Dog
With Dog
Fraction of Searched Area

Fraction of Searched Area

1.6

1.2
1.0
0.8
0.6
0.4
0.2
0.0

1.4

Without Dog
With Dog

1.2
1.0
0.8
0.6
0.4
0.2

0.1

0.2

0.3

0.4

0.5

0.0

0.1

0.2

Â²
(a) Search area with 1 Ã— 1 mile per observation (Algorithm 1)

1.4

Without Dog
With Dog

1.6

1.2
1.0
0.8
0.6
0.4
0.2
0.0

0.1

0.2

0.4

0.5

(a) Search area with 1Ã—1 mile per observation not using Algorithm
1

Fraction of Searched Area

Fraction of Searched Area

1.6

0.3

Â²

0.3

0.4

0.5

Â²

1.4

Without Dog
With Dog

1.2
1.0
0.8
0.6
0.4
0.2
0.0

0.1

0.2

0.3

0.4

0.5

Â²
(b) Search area with 2 Ã— 2 miles per observation (Algorithm 1)

Figure 11: Fraction of total area searched across all cases
with the iterative search resource allocation approach over
the baseline.

(b) Search area with 2 Ã— 2 miles per observation not using Algorithm 1

Figure 12: Fraction of total area searched across all cases
by the double distance constraint integer programming approach (not using Algorithm 1) over the baseline.

cases (i.e. 1x1 mile grid square with use of the dogs) it
performed worse.

8.

RELATED WORK

Recently, there has been some work [19, 20, 21, 23, 9] dealing with geospatial abductive inference introduced in [24].
In [19] for example, authors studied the case of geospatial
abduction where there is an explicit adversary who is interested in ensuring that the agent does not detect the partner
locations in an attempt to simulating the real-world scenario of insurgents who conduct IED (improvised explosive
device) attacks. Another work [20], has adopted geospatial
abduction to develop a software tool which applies geospatial
abduction to the environment of Afghanistan, to look for insurgent high-value targets, supporting insurgent operations.
The work of [21] introduced a variant of the GAPs called
region-based GAPs (RGAPs) which deals with the multiple
possible definitions of the subregions of the map. Finally,
spatial cultural abductive reasoning engine which solves spatial abductive problems was developed in [23]. Aside from
introducing GAP, the work of [24] demonstrated the accuracy of proposed framework on real-world dataset of insurgent IED attacks against US forces in Iraq. Further, the
work of [9], proposed a technique to reduce the computational cost of point-based GAPs. They presented an exact
algorithm for the natural optimization problem of pointbased GAPs. Geospatial abduction problems are related
to facility location [26] and sensor placement problems [10]
in that they identify a set of geo-locations to optimize a

cost or reward function. However, there are key differences
amongst these various frameworks that arise from the difference between explanation and optimization. See [22] for
further discussion on this topic.
Similarly, [1] presents a specific aspect of the well-known
qualification problem, namely spatial qualitative reasoning
approach, which aims at investigating the possibility of an
agent being present at a specific location at a certain time
to carry out an action or participate in an event, given
its known antecedents. This work is different from both
above papers and our study, as it takes on purely logical approach to formalizing spatial qualifications, while our work
and other aforementioned studies use geometric and probabilistic techniques. Further, the framework of this paper is
tailored specifically for the missing person problem.
Looking beyond geospatial abduction, recent research has
demonstrated that GPS (positional) data could be used to
learn rich models of human activity [16, 15, 17, 7]. For example, [16, 15, 17], modeled the human interactions and intentions in a fully relational multi-agent setting. They used
raw GPS data from a real-world game of capture the flag and
Markov logic- a statistical-relational language. Whereas [7]
developed a model to simulate the behaviors associated with
insurgent attacks, and their relationship with geographic locations and temporal windows.
At first glance, one may think our work is similar to [10],
in that they identify a set of geo-locations to optimize a

cost or reward function. However, as described, there are
key differences amongst these various frameworks that arise
from the difference between explanation and optimization.
[11]

9.

CONCLUSION

In this paper, we have introduced the Missing Person Intelligence Synthesis Toolkit (MIST) which leverages a datadriven variant of geospatial abductive inference. MIST can
rank-order the set of search locations provided by a group
of experts. The experimental results showed that our approach is able to reduce the total search area by a total of 31
square miles for standard searched and by 19 square miles
when dog team assets obtain a detection. This reduction
will make FMG locating missing persons faster while saving in direct and indirect cost. At the time of this writing,
we have initiated support to FMG with MIST for an active
case. FMG will use MISTâ€™s ranking of search locations for
this ongoing operation.
Our future plans include utilizing a probabilistic variant
of the feasibility function, applying other features such as
missing personâ€™s region, age, gender to the model and extending our toolkit to be able to solve other problems such
as human trafficking.

Acknowledgement

[12]

[13]

[14]

[15]

[16]

[17]

This work was funded by the Find Me Group.

10.

REFERENCES

[1] B. Akinkunmi and P. C. Bassey. A Logic of Spatial
Qualification Using Qualitative Reasoning Approach.
International Journal of Artificial Intelligence &
Applications, 4(2):45, 2013.
[2] T. Bylander, D. Allemang, M. C. Tanner, and J. R.
Josephson. The Computational Complexity of
Abduction. Artif. Intell., 49(1-3):25â€“60, 1991.
[3] L. Console, L. Portinale, and D. T. DupreÌ. Focussing
Abductive Diagnosis. AI Commun., 4(2/3):88â€“97,
1991.
[4] L. Console, M. L. Sapino, and D. T. DupreÌ. The Role
of Abduction in Database View Updating. J. Intell.
Inf. Syst., 4(3):261â€“280, 1995.
[5] S. do Lago Pereira and L. N. de Barros. Planning with
abduction: A logical framework to explore extensions
to classical planning. In Brazilian Symposium on
Artificial Intelligence, pages 62â€“72. Springer, 2004.
[6] T. Eiter and G. Gottlob. The complexity of
logic-based abduction. Journal of the ACM, 42:3â€“42,
1995.
[7] S. George, X. Wang, J. Lin, B. Qu, and J.-C. Liu.
MECH: Algorithms and Tools for Automated
Assessment of Potential Attack Locations. Technical
report, Texas A & M University, College Station, 2015.
[8] A. C. Kakas and P. Mancarella. Database updates
through abduction. In VLDB, volume 90, pages
650â€“661, 1990.
[9] A. Koutsioumpas. Abductive reasoning in 2d
geospatial problems. In Applications of Mathematics
and Informatics in Science and Engineering, pages
333â€“347. Springer, 2014.
[10] A. Krause, J. Leskovec, C. Guestrin, J. Vanbriesen,
and C. Faloutsos. Efficient sensor placement

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]
[26]

optimization for securing large water distribution
networks. Journal of Water Resources Planning and
Management, 2008.
M. Pagnucco. The Role of Abductive Reasoning within
the Process of Belief Revision. PhD thesis, Basser
Department of Computer Science, University of
Sydney, 1996.
C. S. Peirce. Philosophical writings of Peirce, selected
and edited with an introd. by Justus Buchler. Dover
Publications New York, 1955.
Y. Peng and J. Reggia. Abductive inference models for
diagnostic problem-solving. Symbolic computation.
Springer-Verlag, New York, 1990.
Y. Peng and J. A. Reggia. Plausibility of Diagnostic
Hypotheses: The Nature of Simplicity. In Proceedings
of the 5th National Conference on Artificial
Intelligence. Philadelphia, PA, August 11-15, 1986.
Volume 1: Science., pages 140â€“147, 1986.
A. Sadilek and H. Kautz. Modeling Success, Failure,
and Intent of Multi-Agent Activities Under Severe
Noise.
A. Sadilek and H. Kautz. Location-based Reasoning
About Complex Multi-agent Behavior. J. Artif. Int.
Res., 43(1):87â€“133, Jan. 2012.
A. Sadilek and H. A. Kautz. Recognizing multi-agent
activities from gps data. In AAAI, volume 39, page
109, 2010.
P. Santos and M. Shanahan. Hypothesising object
relations from image transitions. In ECAI, pages
292â€“296, 2002.
P. Shakarian, J. P. Dickerson, and V. Subrahmanian.
Adversarial geospatial abduction problems. ACM
Transactions on Intelligent Systems and Technology
(TIST), 3(2):34, 2012.
P. Shakarian, M. K. Nagel, B. E. Schuetzle, and
V. Subrahmanian. Abductive inference for combat:
using scare-s2 to find high-value targets in
afghanistan. Technical report, DTIC Document, 2011.
P. Shakarian and V. Subrahmanian. Region-based
Geospatial Abduction with Counter-IED Applications.
In U. K. Wiil, editor, Counterterrorism and Open
Source Intelligence. Springer, 2010.
P. Shakarian and V. Subrahmanian. Geospatial
Abduction: Principles and Practice. SpringerLink :
BuÌˆcher. Springer New York, 2011.
P. Shakarian, V. Subrahmanian, and M. L. Spaino.
SCARE: A Case Study with Baghdad. In Proceedings
of the Third International Conference on
Computational Cultural Dynamics. AAAI, 2009.
P. Shakarian, V. Subrahmanian, and M. L. Spaino.
GAPs: Geospatial Abduction Problems. ACM
Transactions on Intelligent Systems and Technology,
2010.
M. Shanahan. Noise and the Common Sense
Informatic Situation for a Mobile Robot.
J. F. Stollsteimer. A working model for plant numbers
and locations. Journal of Farm Economics,
45(3):631â€“645, 1963.

Malware Task Identification: A Data Driven
Approach
Eric Nunes, Casey Buto, Paulo Shakarian

arXiv:1507.01930v1 [cs.CR] 7 Jul 2015

School of Computing, Informatics and
Decision Systems Engineering
Arizona State University
Tempe, AZ 85281, USA
Email: {enunes1, cbuto, shak} @asu.edu

Christian Lebiere,
Stefano Bennati,
Robert Thomson
Carnegie Mellon University
Pittsburgh, PA 15218
Email: {cl@cmu.edu ,
{sbennati, thomsonr} @andrew.cmu.edu}

Abstractâ€”Identifying the tasks a given piece of malware was
designed to perform (e.g. logging keystrokes, recording video,
establishing remote access, etc.) is a difficult and time-consuming
operation that is largely human-driven in practice. In this paper,
we present an automated method to identify malware tasks.
Using two different malware collections, we explore various
circumstances for each - including cases where the training
data differs significantly from test; where the malware being
evaluated employs packing to thwart analytical techniques; and
conditions with sparse training data. We find that this approach
consistently out-performs the current state-of-the art software for
malware task identification as well as standard machine learning
approaches - often achieving an unbiased F1 score of over 0.9.
In the near future, we look to deploy our approach for use by
analysts in an operational cyber-security environment.

I.

Earlier work has sought to classify malware by similar
â€œfamiliesâ€ which has been explored as a supervised classification problem [2], [15], [16]. However, differences over â€œground
truthâ€ for malware families (e.g. Symantec and MacAfee
cluster malware into families differently) and the tendency
for automated approaches to primarily succeed at â€œeasy to
classifyâ€ samples [19], [23] are two primary drawbacks of
malware family classification. More recently, there has been
work on directly inferring the tasks a malware was designed to
perform [12]. This approach leverages static malware analysis
(i.e. analysis of the malware sample conducted without execution, such as decompilation) and a comparison with a crowdsource database of code snippets using a proprietary machine
U.S. Provisional Patent 62/182,006. Contact shak@asu.edu for licensing
information.

Sentar Inc.
Huntsville, AL 35805
Email: holger.jaenisch@sentar.com

leaning approach. However, a key shortcoming of the static
method is that it is of limited value when the malware authors
encrypt part of their code â€“ as we saw with the infamous Gauss
malware [14]. This work builds upon recent developments in
the application of cognitive models to intelligence analysis
tasks [18] and our own preliminary studies on applying cognitive models to identify the tasks a piece of malware was
designed to perform [17], [27]. Specifically, the contributions
of this paper include,
â€¢

I NTRODUCTION

Identifying the tasks a given piece of malware was designed
to perform (e.g. logging keystrokes, recording video, establishing remote access, etc.) is a difficult and time consuming task
that is largely human-driven in practice [24]. The complexity
of this task increases substantially when you consider that
malware is constantly evolving, and that how each malware
instance is classified may be different based on each cybersecurity expertâ€™s own particular background. However, automated solutions are highly attractive for this problem as it can
significantly reduce the time it takes to conduct remediation
in the aftermath of a cyber-attack.

Holger Jaenisch

â€¢

â€¢

â€¢

Experimental results illustrating consistent and significant performance improvements (in terms of precision, recall, and F1) of the instance-based cognitive
model approach when compared with various standard
machine learning approaches (including SVM, logistic regression and random forests) for two different
sandboxes and for two different datasets.
Experimental results showing a consistent and significant performance improvement of the instance-based
cognitive model and several other machine learning
approaches when compared to the current state-of-theart commercial technology (which is based on static
analysis).
Experiments where we study cases where the malware
samples are mutated, encrypted, and use different
carriers - providing key insights into how our approach
will cope with operational difficulties.
Experimental results illustrating that a cognitivelyinspired intermediate step of inferring malware families provides improved performance in the machine
learning and rule-based cognitive model (though no
significant change to the instance-based cognitive
model).

This paper is organized as follows. In Section II we state
the technical preliminaries used in the paper. In Section III
we introduce our cognitive-based approaches, describing the
algorithms and explaining our selection of parameter settings.
This is followed by a description of the baseline approaches
that we studied in our evaluation in Section IV-A and a
description of the two different dynamic malware sandbox
environments we used in Section IV-B. In Section V we present
our suite of experimental results which include experiments
involving samples discovered by Mandiant, Inc. in their APT1
report [21] and samples created using the GVDG [11] tool. Fi-

nally, related work and conclusion are discussed in Section VI
and Section VII respectively.
II.

T ECHNICAL P RELIMINARIES

Throughout this paper, we shall assume that we have a set
of malware samples that comprise a historical corpus (which
we shall denote M) and each sample i âˆˆ M is associated with
a set of tasks (denoted tasks(i)) and a set of attributes (denoted
attribs(i)). Attributes are essentially binary features associated
with a piece of malware that we can observe using dynamic
and/or static analysis while the tasks - which tell us the higherlevel purpose of the malware - must be determined by a human
reviewing the results of such analysis. As M comprises our
historical knowledge, we assume that for each i âˆˆ M both
tasks(i) and attribs(i) are known. For a new piece of malware,
we assume that we only know the attributes. We also note that
throughout the paper, we will use the notation | Â· | to denote
the size of a given set. Tables 1 and 2 provide an example of
the attributes and tasks based on the malware samples from
the Mandiant APT1 dataset (created from samples available
at [22], see also [21]). A full description of this dataset is
presented in Section V.
TABLE 1: Attributes extracted through automated malware
analysis

review some of the major concepts of the ACT-R framework
that are relevant to these models and provide a description of
both approaches.
We leveraged features of the declarative memory and
production system of the ACT-R architecture to complete
malware task identification. These systems store and retrieve
information that correspond to declarative and procedural
knowledge, respectively. Declarative information is the knowledge that a person can attend to, reflect upon, and usually
articulate in some way (e.g., by declaring it verbally or by
gesture). Conversely, procedural knowledge consists of the
skills we display in our behavior, generally without conscious
awareness.
Declarative Knowledge. Declarative knowledge is represented
formally in terms of chunks. Chunks have an explicit type, and
consist of an ordered list of slot-value pairs of information.
Chunks are retrieved from declarative memory by an activation
process, and chunks are each associated with an activation
strength which in turn is used to compute an activation
probability. In this paper, chunks will typically correspond to
a malware family. In the version of ACTR-IB where we do
not leverage families, the chunks correspond with samples in
the training data.
For a given chunk i, the activation strength Ai is computed

Attribute

Intuition

usesDLL(X)

Malware uses a library X

regAct(K)

Malware conducts an activity in the registry, modifying key K.

fileAct(X)

Malware conducts an activity on certain file X

proAct

Malware initiates or terminates a process

TABLE 2: Sample of malware tasks
Task

Intuition

beacon

Beacons back to the adversaryâ€™s system

enumFiles

Designed to enumerate files on the target

serviceManip

Manipulates services running on the target

takeScreenShots

Takes screen shots

upload

Designed to upload files from the target

Throughout the paper, we will also often consider malware
families, using the symbol F to denote the set of all families.
Each malware sample will belong to exactly one malware
family, and all malware samples belonging to a given family
will have the same set of tasks. Hence, we shall also treat each
element of F as a subset of M.
III.

ACT-R BASED A PPROACHES

We propose two models built using the mechanisms of
the ACT-R (Adaptive Control of Thought-Rational) cognitive
architecture [1]. These models leverage the work on applying
this architecture to intelligence analysis problems [18]. In particular, we look to leverage our recently-introduced instancebased (ACTR-IB) and rule-based (ACTR-R) models [17], [27].
Previous research has argued the ability of instance-based
learning in complex dynamic situations making it appropriate
for sensemaking [10]. On the other hand the rule-based learning is a more compact representation of associating samples
in memory with their respective families. In this section, we

as,
Ai = Bi + Si + Pi

(1)

where, Bi is the base-level activation, Si is the spreading
activation, and Pi is the partial matching score. We describe
each of these in more detail as follows.
Base-Level Activation (Bi ): The base-level activation for
chunk i reflects the frequency of samples belonging to a
particular family in memory . More important, base-level is
set to the log of the prior probability (i.e., the fraction of
samples associated with the chunk) in ACTR-R; for instancebased (ACTR-IB), we set it to a base level constant Î²i .
Spreading Activation (Si ): The spreading activation for chunk
i is based on a strength of association between chunk i and
the current test malware sample being considered. The strength
of association is computed differently in both approaches and,
in some cognitive model implementations, is weighted (as is
done in ACTR-R of this paper).
Partial Matching (Pi ): A partial matching mechanism computes the similarity between two samples. In this work, it
is only relevant to the instance-based approach. Given a test
sample j, its similarity with a sample i in memory is computed
as a product of the mismatch penalty (mp, a parameter of the
system) and the degree of mismatch Mji . We define the value
of Mji to be between 0 and âˆ’1; 0 indicates complete match
while âˆ’1 complete mismatch.
As common with models based on the ACT-R framework,
we shall discard chunks whose activation strength is below a
certain threshold (denoted Ï„ ). All the chunks with activation
greater than Ï„ are denoted as Aj . Once the activation strength,
Ai , is computed for a given chunk, we can then calculate
the activation probability, Pri . This is the probability that
the cognitive model will recall that chunk and is computed
using the Boltzmann(softmax) equation [25], which we provide

below.

B. ACT-R Rule-Based Model
Ai
s

(e )
P ri = P Aj
s )
j (e

(2)

Here, e is the base of the natural logarithm and s is momentary
noise inducing stochasticity by simulating background neural
activation (this is also a parameter of the system).

A. ACT-R Instance-Based Model
The instance based model is an iterative learning method
that reflects the cognitive process of accumulating experiences
(in this case the knowledge base of training samples) and
using them to predict the tasks for unseen test samples. Each
malware instance is associated with a set of attributes. When
a new malware sample is encountered, the activation strength
of that sample with each sample in memory is computed
using Equation 1. The spreading activation is a measure of
the uniqueness of the attributes between a test sample i and
a sample j in memory. To compute the spreading activation
we compute the f an for each attribute a (f an(a) finds all
instances in memory with the attribute a) of the test sample
i. The Partial matching is computed as explained above. The
degree of mismatch is computed as the intersection between
the attribute vector of the given malware and each sample
in memory normalized using the Euclidean distance between
the two vectors. The retrieval probability of each sample j
in memory with respect to the test sample i is then computed
using Equation 2. This generates a probability distribution over
families. The tasks are then determined by summing up the
probability of the families associated with that task with an
appropriately set threshold (we set that threshold at 0.5, based
on rationality).
Algorithm 1 ACT-R Instance-based Learning
INPUT: New malware sample i, historical malware corpus
M.
OUTPUT: Set of tasks associated with sample i.
for query malware sample i do
for all j in M do
Bj = Î²j
Pj = mp Ã— âˆš|attribs(i)âˆ©attribs(j)|

In this version of ACT-R model we classify samples based
on simple rules computed during the training phase. Given a
malware training sample with its set of attributes a, along with
the ground truth value family, we compute pair of conditional
probabilities p(a|f ) and p(a|Â¬f ) for an attribute in a piece
of malware belonging (or not belonging) to family f . These
probabilistic rules (conditional probabilities) are used to set
the strength of association of the attribute with a family
(sa,f ). We use empirically determined Bayesian priors p(f )
to set the base-level of each family as opposed to using a
constant base-level for instance based. Only two components
of the activation function in Equation 1 are used, namely
base-level and spreading activation. Given the attributes for
current malware , we calculate the probability of the sample
belonging to each family according to Equation 2, generating
a probability distribution over families. The task are then
determined in a similar way to that of instance-based model.
Algorithm 2 ACT-R Rule-based Learning
INPUT: New malware sample i, historical malware corpus
M.
OUTPUT: Set of tasks associated with new sample i.
TRAINING:
S
Let X = jâˆˆM attrib(j)
for all a in X do
Compute the set of rules p(a|f ) and p(a|Â¬f )
(where p(a|f ) = |{iâˆˆMâˆ©f s.t.|f |aâˆˆattrib(i)}|
s.t. aâˆˆattrib(i)}|
and p(a|Â¬f ) = |{iâˆˆMâˆ’f |M|âˆ’|f
)
|
end for
TESTING:
for all f âˆˆ F do
|f |
)
Bf = log(p(f )) (where p(f ) = |M|
sa,f = 0.0
for all a âˆˆ attrib(i) do
wÃ—sa,f
p(a|f )
sa,f = log( p(a|Â¬f
) ); Sf =+ |attribs(i)|
end for
Af = Bf + Sf
end for
Calculate Prf as per Equation 2
tp = {t âˆˆ T |pf â‰¥ 0.5}

|attribs(i)|Ã—|attribs(j)|

sij = 0.0
for a âˆˆ attribs(i) do
if a âˆˆ attribs(j) then
sij += log( |f|M|
an(a) |)
else
1
sij += log( |M|
)
end if
end for
P
sij
Sj = j |attribs(i)|
Calculate Aj as per Equation 1
end for
Calculate
P Prj as per Equation 2
Prf = jâˆˆf s.t. Aj â‰¥Ï„ Prj
tp = {t âˆˆ T | Prf â‰¥ 0.5}
end for

C. Model Parameter Settings
The two proposed models leverage separate components of
the activation function. Table 3 provides a list of parameters
used for both the ACT-R models - we use standard ACT-R
parameters that have been estimated from a wide range of
previous ACT-R modeling studies from other domains [28] and
which are also suggested in the ACT-R reference manual [3].
The intuition behind these parameters is as follows. The
parameter s injects stochastic noise in the model. It is used to
compute the variance of the noise distribution and to compute
the retrieval probability of each sample in memory. The
mismatch penalty parameter mp is an architectural parameter
that is constant across samples, but it multiplies the similarity
between the test sample and the samples in knowledge base.
Thus, with a large value it penalizes the mismatch samples

more. It typically trades off against the value of the noise s
in a signal-to-noise ratio manner: larger values of mp lead
to more consistent retrieval of the closest matching sample
whereas larger values of s leads to more common retrieval of
poorer matching samples.The activation threshold Ï„ determines
which samples will be retrieved from memory to make task
prediction decisions. The base level constant Î² is used to avoid
retrieval failures which might be caused due to high activation
threshold. The source activation w is assigned to each retrieval
to avoid retrieval failures for rule-based models.
TABLE 3: Parameters for the Cognitive models
Model

Parameters

Instance Based Learning

Î² = 20 (base-level constant)
s = 0.1 (stochastic noise parameter)
Ï„ = -10 (activation threshold)
mp = 20(mismatch penalty)

Rule Based learning

s = 0.1 (stochastic noise parameter)
w = 16 (source activation)

IV.

predictors used in combination to classify new unseen samples.
We use a random forest which combines bagging for each
tree with random feature selection at each node to split the
data thus generating multiple decision tree classifiers [4]. Each
decision tree gives its own opinion on test sample classification
which are then merged to generate a probability distribution
over families.
Support Vector Machine (SVM). Support vector machines
(SVM) was proposed by Vapnik [7]. SVMâ€™s work by finding
a separating margin that maximizes the geometric distance
between classes. The separating margin is termed as hyperplane. We use the popular LibSVM implementation [5] which
is publicly available.
Logistic Regression (LOG-REG). Logistic regression classifies samples by computing the odds ratio. The odds ratio
gives the strength of association between the attributes and
the family like simple rules used in the ACT-R rule based
learning. We implement the multinomial logistic regression
which handles multi-class classification.

E XPERIMENTAL S ETUP

A. Baseline Approaches

B. Dynamic Malware Analysis

We compare the proposed cognitive models against a
variety of baseline approaches - one commercial package and
five standard machine learning techniques. For the machine
learning techniques, we generate a probability distribution
over families and return the set of tasks associated with a
probability of 0.5 or greater while the commercial software
was used as intended by the manufacturer. Parameters for all
baseline approaches were set in a manner to provide the best
performance.
Commercial Offering: Invencia Cynomix. Cynomix is a malware analysis tool made available to researchers by Invencia
industries [12] originally developed under DARPAâ€™s Cyber
Genome project. It represents the current state-of-the-art in
the field of malware capability detection. Cynomix conducts
static analysis of the malware sample and uses a proprietary
algorithm to compare it to crowd-sourced identified malware
components where the functionality is known.
Decision Tree (DT). Decision tree is a hierarchical recursive
partitioning algorithm. We build the decision tree by finding
the best split attribute i.e. the attribute that maximizes the
information gain at each split of a node. In order to avoid
over-fitting, the terminating criteria is set to less than 5% of
total samples. Malware samples are tested by the presence and
absence of the best split attribute at each level in the tree till
it reaches the leaf node.
Naive Bayes Classifier (NB). Naive Bayes is a probabilistic
classifier which uses Bayes theorem with independent attribute
assumption. During training we compute the conditional probabilities of a given attribute belonging to a particular family.
We also compute the prior probabilities for each family i.e.
fraction of the training data belonging to each family. Naive
Bayes assumes that the attributes are statistically independent
hence the likelihood for a sample S represented with a
set of attributes a associated
with a family f is given as,
Qd
Pr(f |S) = P (f ) Ã— i=1 Pr(ai |f ), where d is the number
of attributes in a.
Random Forest (RF). Ensemble methods are popular classification tools. It is based on the idea of generating multiple

Dynamic analysis studies a malicious program as it executes on the host machine. It uses tools like debuggers,
function call tracers, machine emulators, logic analyzers, and
network sniffers to capture the behavior of the program. We
use two publicly available malware analysis tools to generate
attributes for each malware sample. These tools make use of
a sandbox which is a controlled environment to run malicious
software.
Anubis Sandbox. Anubis [13] is an online sandbox which
generates an XML formated report for a malware execution in
a remote environment. It generates detailed static analysis of
the malware but provides less details regarding the behavior of
the malware on the host machine. Since it is hosted remotely
we cannot modify its settings.
Cuckoo Sandbox. Cuckoo [6] is a standalone sandbox implemented using a dedicated virtual machine and more importantly can be customized to suit our needs. It generates
detailed reports for both static as well as behavior analysis by
watching and logging the malware while its running on the
virtual machine. These behavior analysis prove to be unique
indicators for a given malware for the experiments.
C. Performance Evaluation
In our tests, we evaluate performance based primarily
on four metrics: precision, recall, unbiased F1, and family
prediction accuracy. For a given malware sample being tested,
precision is the fraction of tasks the algorithm associated with
the malware that were actual tasks in the ground truth. Recall,
for a piece of malware, is the fraction of ground truth tasks
identified by the algorithm. The unbiased F1 is the harmonic
mean of precision and recall. In our results, we report the
averages for precision, recall, and unbiased F1 for the number
of trials performed. Our measure of family accuracy - the
fraction of trials where the most probable family was the
ground truth family of the malware in question - is meant
to give some insight into how the algorithm performs in the
intermediate steps.

R ESULTS

All experiments were run on Intel core-i7 operating at 3.2 GHz
with 16 GB RAM. Only one core was used for experiments.
All experimental results presented in this section are new and
have not been previously introduced.
A. Mandiant Dataset
Our first set of experiments uses a dataset based on the
the T1 cyber espionage group as identified in the popular
report by Mandiant Inc [21]. This dataset consisted of 132
real malware samples associated with the Mandiant report
that were obtained from the Contagio security professional
website [22]. Each malware sample belonged to one of 15
families including BISCUIT, NEWSREELS, GREENCAT and
COOKIEBAG. Based on the malware family description [21],
we associated a set of tasks with each malware family (that
each malware in that family was designed to perform). In
total, 30 malware tasks were identified for the given malware
samples (see Table 2). On average, each family performed 9
tasks.
We compared the four machine learning approaches with
the rule based and instance-based ACT-R models (ACTR-R
and ACTR-IB respectively). We also submitted the samples
to the Cynomix tool for automatic detection of capabilities.
These detected capabilities were then manually mapped to
the tasks from the Mandiant report. Precision and recall
values were computed for the inferred adversarial tasks. On
average the machine learning approaches predicted 9 tasks per
sample, ACTR-R predicted 9 tasks per sample and ACTR-IB
predicted 10 tasks. On the other hand Cynomix was able to
detect on average only 4 tasks.

Average

Leave one out Cross-Validation(LOOCV)
In leave one out cross validation, for n malware samples,
we train on n âˆ’ 1 samples and test on the remaining one.
This procedure was repeated for all samples and the results
were averaged. We performed this experiment using both
sandboxes and compared the results (see Fig. 1).
1

1

0.9
0.8
0.7
0.6
0.5
Precision

LOG-REG

SVM

Recall

RF

ACTR-R

F1

ACTR-IB

Family
Prediction
INVINCEA

Fig. 2: Average Precision, Recall, F1 and Family prediction
comparisons for LOG-REG, RF, SVM, ACTR-R, ACTR-IB
and INVINCEA.
Fig. 2 compares the performance of the five best performing methods from Fig. 1 and compares it with the Cynomix
tool of Invincea industries. ACTR-IB outperformed LOGREG, SVM, RF and ACTR-R; average F1 = 0.97 vs 0.85 (t
(132) = 7.85, p < .001), 0.9 (t (132) = 4.7, p < .001), 0.89
(t (132) = 5.45, p < .001) and 0.88 (t (132) = 5.2, p < .001)
respectively. Both the proposed cognitive models and machine
learning techniques significantly outperformed the Cynomix
tool in detecting the capabilities (tasks).
These three approaches (LOG-REG, SVM, RF) were
also evaluated with respect to predicting the correct family
(before the tasks were determined). ACTR-IB outperformed
LOG-REG, SVM, RF and ACTR-R; average family prediction
= 0.93 vs 0.84 (t (132) = 3.22, p < .001), 0.86 (t (132) =
3.13, p < .001), 0.86 (t (132) = 3.13, p < .001) and 0.89 (t
(132) = 2.13, p = .03) respectively.

0.6
0.2
F1

Family Prediction

Anubis Sandbox

Average

RF (t (132) = 0.56, p = 0.57), SVM (t (132) = 1.95, p = 0.05),
LOG-REG (t (132) = 1.82, p = 0.07), NB (t (132) = 1.79, p
= 0.08) and DT (t (132) = 0.83, p = 0.4). But the significant
improvement was in the family prediction values with ACTRIB improving by 0.12 from 0.81 to 0.93 (t (132) = 3.86, p
< .001) and ACTR-R by 0.15 from 0.72 to 0.87 (t (132) =
3.78, p < .001) outperforming all other methods. Since having
behavior analysis helps in better task prediction as seen from
the comparison experiment, we use cuckoo sandbox for rest
of our experiments.

Average

V.

1
0.6
0.2
F1

Family Prediction

Cuckoo Sandbox
DT

NB

LOG-REG

SVM

RF

ACTR-R

ACTR-IB

Fig. 1: Average F1 and Family prediction comparisons for DT,
NB, LOG-REG, SVM, RF, ACTR-IB and ACTR-R for Anubis
(top) and Cuckoo (bottom).
The average F1 increases by 0.03 when we use the attributes generated by the Cuckoo sandbox instead of Anubis.
The statistical significance results are as follows: for ACTR-IB
(t (132) = 1.94, p = 0.05), ACTR-R (t (132) = 1.39, p = 0.16),

Task Prediction without inferring families:
In the proposed models we infer the malware family first and
then predict the tasks associated with that family. However,
differences over â€œground truthâ€ for malware families in the
cyber-security community calls for a direct inference of tasks
without dependence on family prediction. In this section we
adapt the models to predict tasks directly without inferring
the family.
Fig. 3 shows the performance of the cognitive and machine
learning models without inferring the families. There is no
difference in the performance of ACTR-IB and ACTR-R
approaches as compared to Fig. 2 where we use families. On
the other hand direct task prediction reduces the F1 measure
of machine learning techniques on average by almost 0.1. This
is due to the fact, now instead of having a single classifier
for each family we have multiple classifiers for each task
that a malware sample is designed to perform. This not only
degrades the performance but also adds to the training time

Average

1

0.9

0.8

0.7
Precision
LOG-REG

SVM

Recall
RF
ACTR-R

F1
ACTR-IB

Fig. 3: Average Precision, Recall, and F1 comparisons for
LOG-REG, RF, SVM, ACTR-R and ACTR-IB without inferring families.

200

Training time (sec)

Training time (sec)

for these methods. We compare the training time with increase
in training data for task prediction with/without inferring
families. Inferring families first reduces the training time
(see Fig. 4 (a)). On the other hand predicting tasks directly
significantly increases the training time for the machine
learning methods along for the rule-based ACT-R approach
(Fig. 4 (b)). Due to the issues with respect to performance
and training time, we consider inferring families first for rest
of the experiments. An important point to note is this has no
effect on the Instance-based model for both performance and
computation time.
150
100
50

0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
LOG-REG SVM RF ACTR-R

(a) (a)

200
150
100
50

0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
LOG-REG SVM RF ACTR-R

(b) (b)

Fig. 4: Training time for LOG-REG, SVM, RF and ACTR-R
with(a) / without(b) inferring families.

Fig. 5 shows the GVDG user interface used for the
generation of malware samples. We can select the carrier
type and the tasks that we want the malware sample to
perform on the host machine. The tasks are represented as
payloads, while carrier is a functional template of specific
behavior which are the operational framework supporting and
enabling the task activity. In generating datasets with GVDG,
we specify families based on sets of malware with the same
tasks. Whether or not a family consists of malware with the
same carrier depends on the experiment. Further, GVDG also
has an option to increase â€œmutationâ€ or variance among the
samples. We perform experiments analyzing the performance
of the proposed methods when the generated samples belong
to different carrier and same carrier types, as well as when
the samples are encrypted and mutated making task prediction
difficult. In all the experiments we consider 60% of the data
for training and 40% for testing. The results are averaged
across 10 trials. The Cynomix tool from Invencia was unable
to detect any tasks for the GVDG dataset, primarily due to
to its inability to find public source documents referencing
GVDG samples and also unable to generalize from similar
samples.
Different Carriers:
In this experiment, we generated 1000 samples for each
carrier type with low mutation. On average each carrier type
performs 7 tasks(payloads). Hence each carrier represents one
family for this experiment. Both random forest and ACTR-IB
model were able to predict the tasks and family with F1
measure of 1.0 outperforming LOG-REG 1 vs 0.91 , SVM
1 vs 0.95 and ACTR-R 1 vs 0.95. All results are statistical
significant with (t (1998) â‰¥ 8.93, p < .001)(Fig. 6). Also for
family prediction ACTR-IB and RF outperformed LOG-REG
1 vs 0.92, SVM 1 vs 0.92 and ACTR-R 1 vs 0.95 (t (1998)
â‰¥ 8.93, < .001). These results are not surprising given that
different carrier(family) types have high dissimilarity between
them. Also, samples belonging to the same carrier have on
average 60% of similar attributes.
1

Average

B. GVDG Dataset

0.9

0.8
Precision

LOG-REG

Recall

SVM

RF

F1

ACTR-R

Family
Prediction
ACTR-IB

Fig. 6: Average Precision, Recall, F1 and Family prediction
comparisons for LOG-REG,SVM, RF, ACTR-R and ACTRIB for different carrier samples.
Fig. 5: GVDG User Interface
GVDG is a malware generation tool designed for the study
of computer threats [11]. It is capable of generating following
malware threats,
â€¢
â€¢
â€¢
â€¢
â€¢

File-virus
Key-Logger
Trojan-Extortionist
USB-Worm
Web Money-Trojan

Different Carriers-Mutation:
For this case, we generate the same samples as in the previous
experiment but with maximum mutation between samples
belonging to the same carrier. We generated 1000 samples for
each carrier with maximum mutation. In this case ACTR-IB
had an average F1 of 1 outperforming LOG-REG 1 vs 0.83,
SVM 1 vs 0.88 , RF 1 vs 0.96 and ACTR-R 1 vs 0.92 (t (1998)
â‰¥ 7, p < .001)(Fig. 7). Also for family prediction ACTR-IB
outperformed LOG-REG 1 vs 0.85, SVM 1 vs 0.88 , RF 1 vs

1

0.95 and ACTR-R 1 vs 0.92 (t (1998) â‰¥ 7, p < .001).

Average

Average

1

0.9

0.9

0.8

0.8

Precision
0.7

LOG-REG
Precision

LOG-REG

Recall

SVM

F1

RF

Family
Prediction
ACTR-IB

ACTR-R

Fig. 7: Average Precision, Recall, F1 and Family prediction
comparisons for LOG-REG,SVM, RF, ACTR-R and ACTRIB.
High mutation induces high variance between samples
associated with the same carrier making the classification task
difficult. High mutation samples belonging to same carrier have
only 20% of common attributes as compared to 60% for low
mutation.

RF

ACTR-R

Family
Prediction
ACTR-IB

outperforming LOG-REG 0.95 vs 0.84, SVM 0.95 vs 0.87, RF
0.95 vs 0.90 and ACTR-R 0.95 vs 0.92 (t (678) â‰¥ 1.52 , p â‰¤
0.13). Since each family performs exactly one task the family
prediction is similar to F1. Using the same carrier for each
payload makes the task difficult as they have high similarity
between them.
Same Carrier-Encryption:
The GVDG tool provides the option for encrypting the
malware samples for the File-virus carrier type. We use
this option to generate 100 encrypted malware samples
for each task(payload) and use them as test data with the
unencrypted versions from the same carrier experiment as
training samples. From Fig. 10 ACTR-IB had an average F1
of 0.9 outperforming LOG-REG 0.9 vs 0.8, SVM 0.9 vs 0.8,
RF 0.9 vs 0.74 and ACTR-R 0.9 vs 0.88 (t (1698) â‰¥ 2.36
, p â‰¤ 0.02). Encrypting malware samples morphs the task
during execution making it difficult to detect during analysis.
Hence the drop in performance as compared to non-encrypted
samples. We note that SVM performs better than RF likely
because it looks to maximize generalization.

0.5
0.4
0.3
0.2
0.1

1

0.9

File-virus

Key-logger

Trojan-E

USB-worm

Web-T

0.5

Average

SVM

F1

Fig. 9: Average Precision, Recall, F1 and Family prediction
comparisons for LOG-REG,SVM, RF, ACTR-R and ACTRIB.

0.4

Average

F1

Leave one carrier out cross-validation:
To see how the models generalize to unseen malware family(carrier), we performed a leave-one-carrier-out comparison
(using high mutation samples), where we test the models
against one previously unseen malware carrier. ACTR-IB performs better or on par with all other baseline approaches for
all the carriers. It clearly outperforms all the approaches in
recalling most of the actual tasks (40%) (see Figure 8). ACTRIB has shown to generalize for unseen malware families [17].
This case is difficult given the fact that the test family is
not represented during training, hence task prediction depends
on associating the test family with the training families that
perform similar tasks.

Recall

0.8
0.7

0.3
0.6

0.2

Precision

0.1
Precision

LOG-REG

Recall

SVM

RF

ACTR-R

F1

ACTR-IB

Fig. 8: Average F1 values for 5 malware carriers (above) and
the average precision, recall and F1 across all carriers (below)
for LOG-REG, SVM, RF, ACTR-R and ACTR-IB..
Same Carrier:
As seen in the previous experiments, different carrier types
makes the task easier because of less similarity between them.
We now test the performance, on same carrier type performing
exactly one task. Since there are 17 tasks in the GVDG tool,
we generate 100 samples for each task for carrier type Filevirus. In this experiment each task represents one family.
Thus in total we have 1700 samples. We do the 60-40 split
experiment. From Fig. 9 ACTR-IB had an average F1 of 0.95

LOG-REG

Recall

SVM

RF

F1

ACTR-R

Family
Prediction
ACTR-IB

Fig. 10: Average Precision, Recall, F1 and Family prediction
comparisons for LOG-REG,SVM, RF, ACTR-R and ACTRIB.
VI. R ELATED W ORK
Identification of malicious software. The identification of
whether or not binary is malicious [8], [26] is important and
can be regarded as a â€œfirst stepâ€ in the analysis of suspicious
binaries in the aftermath of a cyber-attack. However, we note
that as many pieces of malware are designed to perform
multiple tasks, that successful identification of a binary as
malicious does not mean that the identification of its associated
tasks will be a byproduct of the result - and hence this is
normally the case, which has led to some of the other related
work described in this section.
Malware family classification. There is a wealth of existing

work on malware family identification [2], [15], [16]. The
intuition here is that by identifying the family of a given piece
of malware, an analyst can then more easily determine what
it was designed to do based on previously studied samples
from the same family. However, malware family classification
has suffered from two primary draw-backs: (1) disagreement
about malware family ground truth as different analysts (e.g.
Symantec and MacAfee) cluster malware into families differently; and (2) previous work has shown that some of these
approaches mainly succeed in â€œeasy to classifyâ€ samples [19],
[23], where â€œeasy to classifyâ€ is a family that is agreed upon
by multiple malware firms. In this paper, we infer the specific
tasks a piece of malware was designed to carry out. While we
do assign malware to a family as a component of our approach,
to avoid the two aforementioned issues as the family partition
is done so probabilistically and the result ground truth is the
focus of our comparison (though we show family prediction
results as a side-result). Further, we also describe and evaluate
a variant of our instance-based method that does not consider
families and yields a comparable performance to our instancebased method that does consider families.
Malware task identification. With regard to direct inference
of malware tasks, the major related work include the software
created by the firm Invincea [12] for which we have included
a performance comparison. Additionally, some of the ideas
in this paper were first introduced in [17], [27]. However, that
work primarily focused on describing the intuitions behind the
cognitive modeling techniques and only included experimental
evaluation on one dataset (the Mandiant APT1 dataset) and
one sandbox environment (Anubis) with a comparison amongst
only the instance based approach, the rule-based cognitive
model, the standard decision tree, and the naive Bayes reasoner. The experimental evaluation in this paper was designed
to be much more thorough to pave the way toward deployment
of the approach for use by cyber-security analysts.
VII.

[2]
[3]
[4]
[5]

[6]
[7]
[8]

[9]
[10]
[11]
[12]

[13]
[14]
[15]
[16]

[17]

[18]

C ONCLUSION

In this paper, we introduced an automated method that
combines dynamic malware analysis with cognitive modeling
to identify malware tasks. This method obtains excellent precision and recall - often achieving an unbiased F1 score of over
0.9 - in a wide variety of conditions over two different malware
sample collections and two different sandbox environments outperforming a variety of baseline methods. Currently, our
future work has three directions. First, we are looking to
create a deployed version of our approach to aid cyber-security
analysts in the field. Second, we look to enhance our malware
analysis to also include network traffic resulting from the
sample by extending the capabilities of the sandbox. Finally,
we also look to address cases of highly-sophisticated malware
that in addition to using encryption and packing to limit static
analysis, also employ methods to â€œshut downâ€ when run in a
sandbox environment [20]. We are exploring multiple methods
to address this such as the recently introduced technique of
â€œspatial analysisâ€ [9] that involves direct analysis of a malware
binary.

[19]
[20]

[21]
[22]
[23]
[24]

[25]
[26]

[27]

R EFERENCES
[1]

J. R. Anderson, D. Bothell, M. D. Byrne, S. Douglass, C. Lebiere, and
Y. Qin. An integrated theory of mind. PSYCHOLOGICAL REVIEW,
111:1036â€“1060, 2004.

[28]

U. Bayer, P. M. Comparetti, C. Hlauschek, C. Kruegel, and E. Kirda.
Scalable, behavior-based malware clustering, 2009.
D. Bothell. Act-r 6.0 reference manual. http:// act-r.psy.cmu.edu/ actr6/
reference-manual.pdf , 2004.
L. Breiman. Random forests. Machine Learning, 45(1):5â€“32, 2001.
C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector
machines. ACM Trans. Intell. Syst. Technol., 2(3):27:1â€“27:27, May
2011.
J. B. M. S. Claudio Guarnieri, Alessandro Tanasi. Cuckoo sandbox.
http:// www.cuckoosandbox.org/ , 2012.
C. Cortes and V. Vapnik. Support-vector networks. pages 273â€“297,
1995.
I. Firdausi, C. lim, A. Erwin, and A. S. Nugroho. Analysis of machine
learning techniques used in behavior-based malware detection. In
Proceedings of the 2010 Second International Conference on ACT, ACT
â€™10, pages 201â€“203, Washington, DC, USA, 2010. IEEE Computer
Society.
D. Giametta and A. Potter. Shmoomcon 2014:there and back again:a
critical analysis of spatial analysis, 2014.
C. Gonzalez, J. F. Lerch, and C. Lebiere. Instance-based learning in
dynamic decision making. Cognitive Science, 27(4):591 â€“ 635, 2003.
GVDG. Generator malware gvdg. 2011.
Invencia. Crowdsource: Crowd trained machine learning model for
malware capability detection. http:// www.invincea.com/ tag/ cynomix/ ,
2013.
ISEC-Lab. Anubis: Analyzing unknown binaries. http:// anubis.iseclab.
org/ , 2007.
Kaspersky. Gauss: Abnormal distribution, 2012.
J. Kinable and O. Kostakis. Malware classification based on call graph
clustering. J. Comput. Virol., 7(4):233â€“245, Nov. 2011.
D. Kong and G. Yan. Discriminant malware distance learning on structural information for automated malware classification. In Proceedings
of the 19th ACM SIGKDD, KDD â€™13, pages 1357â€“1365, New York,
NY, USA, 2013. ACM.
C. Lebiere, S. Bennati, R. Thomson, P. Shakarian, and E. Nunes.
Functional cognitive models of malware identification. In Proceedings
of ICCM, ICCM 2015, Groningen, The Netherlands, April 9-11, 2015,
2015.
C. Lebiere, P. Pirolli, R. Thomson, J. Paik, M. Rutledge-Taylor,
J. Staszewski, and J. R. Anderson. A functional model of sensemaking
in a neurocognitive architecture. Intell. Neuroscience, 2013:5:5â€“5:5,
Jan. 2013.
P. Li, L. Liu, and M. K. Reiter. On challenges in evaluating malware
clustering, 2007.
M. Lindorfer, C. Kolbitsch, and P. Milani Comparetti. Detecting
environment-sensitive malware. In Proceedings of the 14th International Conference on RAID, RAIDâ€™11, pages 338â€“357, Berlin, Heidelberg, 2011. Springer-Verlag.
Mandiant. Apt1:exposing one of chinaâ€™s cyber espionage units. http:
// intelreport.mandiant.com/ , 2013.
Mandiant. Mandiant APT1 samples categorized by malware families.
Contagio Malware Dump, 2013.
R. Perdisci and ManChon. Vamo: towards a fully automated malware
clustering validity analysis. In ACSAC, pages 329â€“338. ACM, 2012.
M. Sikorski and A. Honig. Practical Malware Analysis: The Hands-On
Guide to Dissecting Malicious Software. No Starch Press, 1 edition,
2012.
R. S. Sutton and A. G. Barto. Introduction to Reinforcement Learning.
MIT Press, Cambridge, MA, USA, 1st edition, 1998.
A. Tamersoy, K. Roundy, and D. H. Chau. Guilt by association: Large
scale malware detection by mining file-relation graphs. In Proceedings
of the 20th ACM SIGKDD, KDD â€™14, pages 1524â€“1533. ACM, 2014.
R. Thomson, C. Lebiere, S. Bennati, P. Shakarian, and E. Nunes. Malware identification using cognitively-inspired inference. In Proceedings
of BRIMS, BRIMS 2015, Washington DC, March 31-April 3, 2015, 2015.
T. J. Wong, E. T. Cokely, and L. J. Schooler. An online database of
act-r parameters: Towards a transparent community-based approach to
model development. 2010.

Malware Task Identification: A Data Driven
Approach
Eric Nunes, Casey Buto, Paulo Shakarian

arXiv:1507.01930v1 [cs.CR] 7 Jul 2015

School of Computing, Informatics and
Decision Systems Engineering
Arizona State University
Tempe, AZ 85281, USA
Email: {enunes1, cbuto, shak} @asu.edu

Christian Lebiere,
Stefano Bennati,
Robert Thomson
Carnegie Mellon University
Pittsburgh, PA 15218
Email: {cl@cmu.edu ,
{sbennati, thomsonr} @andrew.cmu.edu}

Abstractâ€”Identifying the tasks a given piece of malware was
designed to perform (e.g. logging keystrokes, recording video,
establishing remote access, etc.) is a difficult and time-consuming
operation that is largely human-driven in practice. In this paper,
we present an automated method to identify malware tasks.
Using two different malware collections, we explore various
circumstances for each - including cases where the training
data differs significantly from test; where the malware being
evaluated employs packing to thwart analytical techniques; and
conditions with sparse training data. We find that this approach
consistently out-performs the current state-of-the art software for
malware task identification as well as standard machine learning
approaches - often achieving an unbiased F1 score of over 0.9.
In the near future, we look to deploy our approach for use by
analysts in an operational cyber-security environment.

I.

Earlier work has sought to classify malware by similar
â€œfamiliesâ€ which has been explored as a supervised classification problem [2], [15], [16]. However, differences over â€œground
truthâ€ for malware families (e.g. Symantec and MacAfee
cluster malware into families differently) and the tendency
for automated approaches to primarily succeed at â€œeasy to
classifyâ€ samples [19], [23] are two primary drawbacks of
malware family classification. More recently, there has been
work on directly inferring the tasks a malware was designed to
perform [12]. This approach leverages static malware analysis
(i.e. analysis of the malware sample conducted without execution, such as decompilation) and a comparison with a crowdsource database of code snippets using a proprietary machine
U.S. Provisional Patent 62/182,006. Contact shak@asu.edu for licensing
information.

Sentar Inc.
Huntsville, AL 35805
Email: holger.jaenisch@sentar.com

leaning approach. However, a key shortcoming of the static
method is that it is of limited value when the malware authors
encrypt part of their code â€“ as we saw with the infamous Gauss
malware [14]. This work builds upon recent developments in
the application of cognitive models to intelligence analysis
tasks [18] and our own preliminary studies on applying cognitive models to identify the tasks a piece of malware was
designed to perform [17], [27]. Specifically, the contributions
of this paper include,
â€¢

I NTRODUCTION

Identifying the tasks a given piece of malware was designed
to perform (e.g. logging keystrokes, recording video, establishing remote access, etc.) is a difficult and time consuming task
that is largely human-driven in practice [24]. The complexity
of this task increases substantially when you consider that
malware is constantly evolving, and that how each malware
instance is classified may be different based on each cybersecurity expertâ€™s own particular background. However, automated solutions are highly attractive for this problem as it can
significantly reduce the time it takes to conduct remediation
in the aftermath of a cyber-attack.

Holger Jaenisch

â€¢

â€¢

â€¢

Experimental results illustrating consistent and significant performance improvements (in terms of precision, recall, and F1) of the instance-based cognitive
model approach when compared with various standard
machine learning approaches (including SVM, logistic regression and random forests) for two different
sandboxes and for two different datasets.
Experimental results showing a consistent and significant performance improvement of the instance-based
cognitive model and several other machine learning
approaches when compared to the current state-of-theart commercial technology (which is based on static
analysis).
Experiments where we study cases where the malware
samples are mutated, encrypted, and use different
carriers - providing key insights into how our approach
will cope with operational difficulties.
Experimental results illustrating that a cognitivelyinspired intermediate step of inferring malware families provides improved performance in the machine
learning and rule-based cognitive model (though no
significant change to the instance-based cognitive
model).

This paper is organized as follows. In Section II we state
the technical preliminaries used in the paper. In Section III
we introduce our cognitive-based approaches, describing the
algorithms and explaining our selection of parameter settings.
This is followed by a description of the baseline approaches
that we studied in our evaluation in Section IV-A and a
description of the two different dynamic malware sandbox
environments we used in Section IV-B. In Section V we present
our suite of experimental results which include experiments
involving samples discovered by Mandiant, Inc. in their APT1
report [21] and samples created using the GVDG [11] tool. Fi-

nally, related work and conclusion are discussed in Section VI
and Section VII respectively.
II.

T ECHNICAL P RELIMINARIES

Throughout this paper, we shall assume that we have a set
of malware samples that comprise a historical corpus (which
we shall denote M) and each sample i âˆˆ M is associated with
a set of tasks (denoted tasks(i)) and a set of attributes (denoted
attribs(i)). Attributes are essentially binary features associated
with a piece of malware that we can observe using dynamic
and/or static analysis while the tasks - which tell us the higherlevel purpose of the malware - must be determined by a human
reviewing the results of such analysis. As M comprises our
historical knowledge, we assume that for each i âˆˆ M both
tasks(i) and attribs(i) are known. For a new piece of malware,
we assume that we only know the attributes. We also note that
throughout the paper, we will use the notation | Â· | to denote
the size of a given set. Tables 1 and 2 provide an example of
the attributes and tasks based on the malware samples from
the Mandiant APT1 dataset (created from samples available
at [22], see also [21]). A full description of this dataset is
presented in Section V.
TABLE 1: Attributes extracted through automated malware
analysis

review some of the major concepts of the ACT-R framework
that are relevant to these models and provide a description of
both approaches.
We leveraged features of the declarative memory and
production system of the ACT-R architecture to complete
malware task identification. These systems store and retrieve
information that correspond to declarative and procedural
knowledge, respectively. Declarative information is the knowledge that a person can attend to, reflect upon, and usually
articulate in some way (e.g., by declaring it verbally or by
gesture). Conversely, procedural knowledge consists of the
skills we display in our behavior, generally without conscious
awareness.
Declarative Knowledge. Declarative knowledge is represented
formally in terms of chunks. Chunks have an explicit type, and
consist of an ordered list of slot-value pairs of information.
Chunks are retrieved from declarative memory by an activation
process, and chunks are each associated with an activation
strength which in turn is used to compute an activation
probability. In this paper, chunks will typically correspond to
a malware family. In the version of ACTR-IB where we do
not leverage families, the chunks correspond with samples in
the training data.
For a given chunk i, the activation strength Ai is computed

Attribute

Intuition

usesDLL(X)

Malware uses a library X

regAct(K)

Malware conducts an activity in the registry, modifying key K.

fileAct(X)

Malware conducts an activity on certain file X

proAct

Malware initiates or terminates a process

TABLE 2: Sample of malware tasks
Task

Intuition

beacon

Beacons back to the adversaryâ€™s system

enumFiles

Designed to enumerate files on the target

serviceManip

Manipulates services running on the target

takeScreenShots

Takes screen shots

upload

Designed to upload files from the target

Throughout the paper, we will also often consider malware
families, using the symbol F to denote the set of all families.
Each malware sample will belong to exactly one malware
family, and all malware samples belonging to a given family
will have the same set of tasks. Hence, we shall also treat each
element of F as a subset of M.
III.

ACT-R BASED A PPROACHES

We propose two models built using the mechanisms of
the ACT-R (Adaptive Control of Thought-Rational) cognitive
architecture [1]. These models leverage the work on applying
this architecture to intelligence analysis problems [18]. In particular, we look to leverage our recently-introduced instancebased (ACTR-IB) and rule-based (ACTR-R) models [17], [27].
Previous research has argued the ability of instance-based
learning in complex dynamic situations making it appropriate
for sensemaking [10]. On the other hand the rule-based learning is a more compact representation of associating samples
in memory with their respective families. In this section, we

as,
Ai = Bi + Si + Pi

(1)

where, Bi is the base-level activation, Si is the spreading
activation, and Pi is the partial matching score. We describe
each of these in more detail as follows.
Base-Level Activation (Bi ): The base-level activation for
chunk i reflects the frequency of samples belonging to a
particular family in memory . More important, base-level is
set to the log of the prior probability (i.e., the fraction of
samples associated with the chunk) in ACTR-R; for instancebased (ACTR-IB), we set it to a base level constant Î²i .
Spreading Activation (Si ): The spreading activation for chunk
i is based on a strength of association between chunk i and
the current test malware sample being considered. The strength
of association is computed differently in both approaches and,
in some cognitive model implementations, is weighted (as is
done in ACTR-R of this paper).
Partial Matching (Pi ): A partial matching mechanism computes the similarity between two samples. In this work, it
is only relevant to the instance-based approach. Given a test
sample j, its similarity with a sample i in memory is computed
as a product of the mismatch penalty (mp, a parameter of the
system) and the degree of mismatch Mji . We define the value
of Mji to be between 0 and âˆ’1; 0 indicates complete match
while âˆ’1 complete mismatch.
As common with models based on the ACT-R framework,
we shall discard chunks whose activation strength is below a
certain threshold (denoted Ï„ ). All the chunks with activation
greater than Ï„ are denoted as Aj . Once the activation strength,
Ai , is computed for a given chunk, we can then calculate
the activation probability, Pri . This is the probability that
the cognitive model will recall that chunk and is computed
using the Boltzmann(softmax) equation [25], which we provide

below.

B. ACT-R Rule-Based Model
Ai
s

(e )
P ri = P Aj
s )
j (e

(2)

Here, e is the base of the natural logarithm and s is momentary
noise inducing stochasticity by simulating background neural
activation (this is also a parameter of the system).

A. ACT-R Instance-Based Model
The instance based model is an iterative learning method
that reflects the cognitive process of accumulating experiences
(in this case the knowledge base of training samples) and
using them to predict the tasks for unseen test samples. Each
malware instance is associated with a set of attributes. When
a new malware sample is encountered, the activation strength
of that sample with each sample in memory is computed
using Equation 1. The spreading activation is a measure of
the uniqueness of the attributes between a test sample i and
a sample j in memory. To compute the spreading activation
we compute the f an for each attribute a (f an(a) finds all
instances in memory with the attribute a) of the test sample
i. The Partial matching is computed as explained above. The
degree of mismatch is computed as the intersection between
the attribute vector of the given malware and each sample
in memory normalized using the Euclidean distance between
the two vectors. The retrieval probability of each sample j
in memory with respect to the test sample i is then computed
using Equation 2. This generates a probability distribution over
families. The tasks are then determined by summing up the
probability of the families associated with that task with an
appropriately set threshold (we set that threshold at 0.5, based
on rationality).
Algorithm 1 ACT-R Instance-based Learning
INPUT: New malware sample i, historical malware corpus
M.
OUTPUT: Set of tasks associated with sample i.
for query malware sample i do
for all j in M do
Bj = Î²j
Pj = mp Ã— âˆš|attribs(i)âˆ©attribs(j)|

In this version of ACT-R model we classify samples based
on simple rules computed during the training phase. Given a
malware training sample with its set of attributes a, along with
the ground truth value family, we compute pair of conditional
probabilities p(a|f ) and p(a|Â¬f ) for an attribute in a piece
of malware belonging (or not belonging) to family f . These
probabilistic rules (conditional probabilities) are used to set
the strength of association of the attribute with a family
(sa,f ). We use empirically determined Bayesian priors p(f )
to set the base-level of each family as opposed to using a
constant base-level for instance based. Only two components
of the activation function in Equation 1 are used, namely
base-level and spreading activation. Given the attributes for
current malware , we calculate the probability of the sample
belonging to each family according to Equation 2, generating
a probability distribution over families. The task are then
determined in a similar way to that of instance-based model.
Algorithm 2 ACT-R Rule-based Learning
INPUT: New malware sample i, historical malware corpus
M.
OUTPUT: Set of tasks associated with new sample i.
TRAINING:
S
Let X = jâˆˆM attrib(j)
for all a in X do
Compute the set of rules p(a|f ) and p(a|Â¬f )
(where p(a|f ) = |{iâˆˆMâˆ©f s.t.|f |aâˆˆattrib(i)}|
s.t. aâˆˆattrib(i)}|
and p(a|Â¬f ) = |{iâˆˆMâˆ’f |M|âˆ’|f
)
|
end for
TESTING:
for all f âˆˆ F do
|f |
)
Bf = log(p(f )) (where p(f ) = |M|
sa,f = 0.0
for all a âˆˆ attrib(i) do
wÃ—sa,f
p(a|f )
sa,f = log( p(a|Â¬f
) ); Sf =+ |attribs(i)|
end for
Af = Bf + Sf
end for
Calculate Prf as per Equation 2
tp = {t âˆˆ T |pf â‰¥ 0.5}

|attribs(i)|Ã—|attribs(j)|

sij = 0.0
for a âˆˆ attribs(i) do
if a âˆˆ attribs(j) then
sij += log( |f|M|
an(a) |)
else
1
sij += log( |M|
)
end if
end for
P
sij
Sj = j |attribs(i)|
Calculate Aj as per Equation 1
end for
Calculate
P Prj as per Equation 2
Prf = jâˆˆf s.t. Aj â‰¥Ï„ Prj
tp = {t âˆˆ T | Prf â‰¥ 0.5}
end for

C. Model Parameter Settings
The two proposed models leverage separate components of
the activation function. Table 3 provides a list of parameters
used for both the ACT-R models - we use standard ACT-R
parameters that have been estimated from a wide range of
previous ACT-R modeling studies from other domains [28] and
which are also suggested in the ACT-R reference manual [3].
The intuition behind these parameters is as follows. The
parameter s injects stochastic noise in the model. It is used to
compute the variance of the noise distribution and to compute
the retrieval probability of each sample in memory. The
mismatch penalty parameter mp is an architectural parameter
that is constant across samples, but it multiplies the similarity
between the test sample and the samples in knowledge base.
Thus, with a large value it penalizes the mismatch samples

more. It typically trades off against the value of the noise s
in a signal-to-noise ratio manner: larger values of mp lead
to more consistent retrieval of the closest matching sample
whereas larger values of s leads to more common retrieval of
poorer matching samples.The activation threshold Ï„ determines
which samples will be retrieved from memory to make task
prediction decisions. The base level constant Î² is used to avoid
retrieval failures which might be caused due to high activation
threshold. The source activation w is assigned to each retrieval
to avoid retrieval failures for rule-based models.
TABLE 3: Parameters for the Cognitive models
Model

Parameters

Instance Based Learning

Î² = 20 (base-level constant)
s = 0.1 (stochastic noise parameter)
Ï„ = -10 (activation threshold)
mp = 20(mismatch penalty)

Rule Based learning

s = 0.1 (stochastic noise parameter)
w = 16 (source activation)

IV.

predictors used in combination to classify new unseen samples.
We use a random forest which combines bagging for each
tree with random feature selection at each node to split the
data thus generating multiple decision tree classifiers [4]. Each
decision tree gives its own opinion on test sample classification
which are then merged to generate a probability distribution
over families.
Support Vector Machine (SVM). Support vector machines
(SVM) was proposed by Vapnik [7]. SVMâ€™s work by finding
a separating margin that maximizes the geometric distance
between classes. The separating margin is termed as hyperplane. We use the popular LibSVM implementation [5] which
is publicly available.
Logistic Regression (LOG-REG). Logistic regression classifies samples by computing the odds ratio. The odds ratio
gives the strength of association between the attributes and
the family like simple rules used in the ACT-R rule based
learning. We implement the multinomial logistic regression
which handles multi-class classification.

E XPERIMENTAL S ETUP

A. Baseline Approaches

B. Dynamic Malware Analysis

We compare the proposed cognitive models against a
variety of baseline approaches - one commercial package and
five standard machine learning techniques. For the machine
learning techniques, we generate a probability distribution
over families and return the set of tasks associated with a
probability of 0.5 or greater while the commercial software
was used as intended by the manufacturer. Parameters for all
baseline approaches were set in a manner to provide the best
performance.
Commercial Offering: Invencia Cynomix. Cynomix is a malware analysis tool made available to researchers by Invencia
industries [12] originally developed under DARPAâ€™s Cyber
Genome project. It represents the current state-of-the-art in
the field of malware capability detection. Cynomix conducts
static analysis of the malware sample and uses a proprietary
algorithm to compare it to crowd-sourced identified malware
components where the functionality is known.
Decision Tree (DT). Decision tree is a hierarchical recursive
partitioning algorithm. We build the decision tree by finding
the best split attribute i.e. the attribute that maximizes the
information gain at each split of a node. In order to avoid
over-fitting, the terminating criteria is set to less than 5% of
total samples. Malware samples are tested by the presence and
absence of the best split attribute at each level in the tree till
it reaches the leaf node.
Naive Bayes Classifier (NB). Naive Bayes is a probabilistic
classifier which uses Bayes theorem with independent attribute
assumption. During training we compute the conditional probabilities of a given attribute belonging to a particular family.
We also compute the prior probabilities for each family i.e.
fraction of the training data belonging to each family. Naive
Bayes assumes that the attributes are statistically independent
hence the likelihood for a sample S represented with a
set of attributes a associated
with a family f is given as,
Qd
Pr(f |S) = P (f ) Ã— i=1 Pr(ai |f ), where d is the number
of attributes in a.
Random Forest (RF). Ensemble methods are popular classification tools. It is based on the idea of generating multiple

Dynamic analysis studies a malicious program as it executes on the host machine. It uses tools like debuggers,
function call tracers, machine emulators, logic analyzers, and
network sniffers to capture the behavior of the program. We
use two publicly available malware analysis tools to generate
attributes for each malware sample. These tools make use of
a sandbox which is a controlled environment to run malicious
software.
Anubis Sandbox. Anubis [13] is an online sandbox which
generates an XML formated report for a malware execution in
a remote environment. It generates detailed static analysis of
the malware but provides less details regarding the behavior of
the malware on the host machine. Since it is hosted remotely
we cannot modify its settings.
Cuckoo Sandbox. Cuckoo [6] is a standalone sandbox implemented using a dedicated virtual machine and more importantly can be customized to suit our needs. It generates
detailed reports for both static as well as behavior analysis by
watching and logging the malware while its running on the
virtual machine. These behavior analysis prove to be unique
indicators for a given malware for the experiments.
C. Performance Evaluation
In our tests, we evaluate performance based primarily
on four metrics: precision, recall, unbiased F1, and family
prediction accuracy. For a given malware sample being tested,
precision is the fraction of tasks the algorithm associated with
the malware that were actual tasks in the ground truth. Recall,
for a piece of malware, is the fraction of ground truth tasks
identified by the algorithm. The unbiased F1 is the harmonic
mean of precision and recall. In our results, we report the
averages for precision, recall, and unbiased F1 for the number
of trials performed. Our measure of family accuracy - the
fraction of trials where the most probable family was the
ground truth family of the malware in question - is meant
to give some insight into how the algorithm performs in the
intermediate steps.

R ESULTS

All experiments were run on Intel core-i7 operating at 3.2 GHz
with 16 GB RAM. Only one core was used for experiments.
All experimental results presented in this section are new and
have not been previously introduced.
A. Mandiant Dataset
Our first set of experiments uses a dataset based on the
the T1 cyber espionage group as identified in the popular
report by Mandiant Inc [21]. This dataset consisted of 132
real malware samples associated with the Mandiant report
that were obtained from the Contagio security professional
website [22]. Each malware sample belonged to one of 15
families including BISCUIT, NEWSREELS, GREENCAT and
COOKIEBAG. Based on the malware family description [21],
we associated a set of tasks with each malware family (that
each malware in that family was designed to perform). In
total, 30 malware tasks were identified for the given malware
samples (see Table 2). On average, each family performed 9
tasks.
We compared the four machine learning approaches with
the rule based and instance-based ACT-R models (ACTR-R
and ACTR-IB respectively). We also submitted the samples
to the Cynomix tool for automatic detection of capabilities.
These detected capabilities were then manually mapped to
the tasks from the Mandiant report. Precision and recall
values were computed for the inferred adversarial tasks. On
average the machine learning approaches predicted 9 tasks per
sample, ACTR-R predicted 9 tasks per sample and ACTR-IB
predicted 10 tasks. On the other hand Cynomix was able to
detect on average only 4 tasks.

Average

Leave one out Cross-Validation(LOOCV)
In leave one out cross validation, for n malware samples,
we train on n âˆ’ 1 samples and test on the remaining one.
This procedure was repeated for all samples and the results
were averaged. We performed this experiment using both
sandboxes and compared the results (see Fig. 1).
1

1

0.9
0.8
0.7
0.6
0.5
Precision

LOG-REG

SVM

Recall

RF

ACTR-R

F1

ACTR-IB

Family
Prediction
INVINCEA

Fig. 2: Average Precision, Recall, F1 and Family prediction
comparisons for LOG-REG, RF, SVM, ACTR-R, ACTR-IB
and INVINCEA.
Fig. 2 compares the performance of the five best performing methods from Fig. 1 and compares it with the Cynomix
tool of Invincea industries. ACTR-IB outperformed LOGREG, SVM, RF and ACTR-R; average F1 = 0.97 vs 0.85 (t
(132) = 7.85, p < .001), 0.9 (t (132) = 4.7, p < .001), 0.89
(t (132) = 5.45, p < .001) and 0.88 (t (132) = 5.2, p < .001)
respectively. Both the proposed cognitive models and machine
learning techniques significantly outperformed the Cynomix
tool in detecting the capabilities (tasks).
These three approaches (LOG-REG, SVM, RF) were
also evaluated with respect to predicting the correct family
(before the tasks were determined). ACTR-IB outperformed
LOG-REG, SVM, RF and ACTR-R; average family prediction
= 0.93 vs 0.84 (t (132) = 3.22, p < .001), 0.86 (t (132) =
3.13, p < .001), 0.86 (t (132) = 3.13, p < .001) and 0.89 (t
(132) = 2.13, p = .03) respectively.

0.6
0.2
F1

Family Prediction

Anubis Sandbox

Average

RF (t (132) = 0.56, p = 0.57), SVM (t (132) = 1.95, p = 0.05),
LOG-REG (t (132) = 1.82, p = 0.07), NB (t (132) = 1.79, p
= 0.08) and DT (t (132) = 0.83, p = 0.4). But the significant
improvement was in the family prediction values with ACTRIB improving by 0.12 from 0.81 to 0.93 (t (132) = 3.86, p
< .001) and ACTR-R by 0.15 from 0.72 to 0.87 (t (132) =
3.78, p < .001) outperforming all other methods. Since having
behavior analysis helps in better task prediction as seen from
the comparison experiment, we use cuckoo sandbox for rest
of our experiments.

Average

V.

1
0.6
0.2
F1

Family Prediction

Cuckoo Sandbox
DT

NB

LOG-REG

SVM

RF

ACTR-R

ACTR-IB

Fig. 1: Average F1 and Family prediction comparisons for DT,
NB, LOG-REG, SVM, RF, ACTR-IB and ACTR-R for Anubis
(top) and Cuckoo (bottom).
The average F1 increases by 0.03 when we use the attributes generated by the Cuckoo sandbox instead of Anubis.
The statistical significance results are as follows: for ACTR-IB
(t (132) = 1.94, p = 0.05), ACTR-R (t (132) = 1.39, p = 0.16),

Task Prediction without inferring families:
In the proposed models we infer the malware family first and
then predict the tasks associated with that family. However,
differences over â€œground truthâ€ for malware families in the
cyber-security community calls for a direct inference of tasks
without dependence on family prediction. In this section we
adapt the models to predict tasks directly without inferring
the family.
Fig. 3 shows the performance of the cognitive and machine
learning models without inferring the families. There is no
difference in the performance of ACTR-IB and ACTR-R
approaches as compared to Fig. 2 where we use families. On
the other hand direct task prediction reduces the F1 measure
of machine learning techniques on average by almost 0.1. This
is due to the fact, now instead of having a single classifier
for each family we have multiple classifiers for each task
that a malware sample is designed to perform. This not only
degrades the performance but also adds to the training time

Average

1

0.9

0.8

0.7
Precision
LOG-REG

SVM

Recall
RF
ACTR-R

F1
ACTR-IB

Fig. 3: Average Precision, Recall, and F1 comparisons for
LOG-REG, RF, SVM, ACTR-R and ACTR-IB without inferring families.

200

Training time (sec)

Training time (sec)

for these methods. We compare the training time with increase
in training data for task prediction with/without inferring
families. Inferring families first reduces the training time
(see Fig. 4 (a)). On the other hand predicting tasks directly
significantly increases the training time for the machine
learning methods along for the rule-based ACT-R approach
(Fig. 4 (b)). Due to the issues with respect to performance
and training time, we consider inferring families first for rest
of the experiments. An important point to note is this has no
effect on the Instance-based model for both performance and
computation time.
150
100
50

0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
LOG-REG SVM RF ACTR-R

(a) (a)

200
150
100
50

0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
LOG-REG SVM RF ACTR-R

(b) (b)

Fig. 4: Training time for LOG-REG, SVM, RF and ACTR-R
with(a) / without(b) inferring families.

Fig. 5 shows the GVDG user interface used for the
generation of malware samples. We can select the carrier
type and the tasks that we want the malware sample to
perform on the host machine. The tasks are represented as
payloads, while carrier is a functional template of specific
behavior which are the operational framework supporting and
enabling the task activity. In generating datasets with GVDG,
we specify families based on sets of malware with the same
tasks. Whether or not a family consists of malware with the
same carrier depends on the experiment. Further, GVDG also
has an option to increase â€œmutationâ€ or variance among the
samples. We perform experiments analyzing the performance
of the proposed methods when the generated samples belong
to different carrier and same carrier types, as well as when
the samples are encrypted and mutated making task prediction
difficult. In all the experiments we consider 60% of the data
for training and 40% for testing. The results are averaged
across 10 trials. The Cynomix tool from Invencia was unable
to detect any tasks for the GVDG dataset, primarily due to
to its inability to find public source documents referencing
GVDG samples and also unable to generalize from similar
samples.
Different Carriers:
In this experiment, we generated 1000 samples for each
carrier type with low mutation. On average each carrier type
performs 7 tasks(payloads). Hence each carrier represents one
family for this experiment. Both random forest and ACTR-IB
model were able to predict the tasks and family with F1
measure of 1.0 outperforming LOG-REG 1 vs 0.91 , SVM
1 vs 0.95 and ACTR-R 1 vs 0.95. All results are statistical
significant with (t (1998) â‰¥ 8.93, p < .001)(Fig. 6). Also for
family prediction ACTR-IB and RF outperformed LOG-REG
1 vs 0.92, SVM 1 vs 0.92 and ACTR-R 1 vs 0.95 (t (1998)
â‰¥ 8.93, < .001). These results are not surprising given that
different carrier(family) types have high dissimilarity between
them. Also, samples belonging to the same carrier have on
average 60% of similar attributes.
1

Average

B. GVDG Dataset

0.9

0.8
Precision

LOG-REG

Recall

SVM

RF

F1

ACTR-R

Family
Prediction
ACTR-IB

Fig. 6: Average Precision, Recall, F1 and Family prediction
comparisons for LOG-REG,SVM, RF, ACTR-R and ACTRIB for different carrier samples.
Fig. 5: GVDG User Interface
GVDG is a malware generation tool designed for the study
of computer threats [11]. It is capable of generating following
malware threats,
â€¢
â€¢
â€¢
â€¢
â€¢

File-virus
Key-Logger
Trojan-Extortionist
USB-Worm
Web Money-Trojan

Different Carriers-Mutation:
For this case, we generate the same samples as in the previous
experiment but with maximum mutation between samples
belonging to the same carrier. We generated 1000 samples for
each carrier with maximum mutation. In this case ACTR-IB
had an average F1 of 1 outperforming LOG-REG 1 vs 0.83,
SVM 1 vs 0.88 , RF 1 vs 0.96 and ACTR-R 1 vs 0.92 (t (1998)
â‰¥ 7, p < .001)(Fig. 7). Also for family prediction ACTR-IB
outperformed LOG-REG 1 vs 0.85, SVM 1 vs 0.88 , RF 1 vs

1

0.95 and ACTR-R 1 vs 0.92 (t (1998) â‰¥ 7, p < .001).

Average

Average

1

0.9

0.9

0.8

0.8

Precision
0.7

LOG-REG
Precision

LOG-REG

Recall

SVM

F1

RF

Family
Prediction
ACTR-IB

ACTR-R

Fig. 7: Average Precision, Recall, F1 and Family prediction
comparisons for LOG-REG,SVM, RF, ACTR-R and ACTRIB.
High mutation induces high variance between samples
associated with the same carrier making the classification task
difficult. High mutation samples belonging to same carrier have
only 20% of common attributes as compared to 60% for low
mutation.

RF

ACTR-R

Family
Prediction
ACTR-IB

outperforming LOG-REG 0.95 vs 0.84, SVM 0.95 vs 0.87, RF
0.95 vs 0.90 and ACTR-R 0.95 vs 0.92 (t (678) â‰¥ 1.52 , p â‰¤
0.13). Since each family performs exactly one task the family
prediction is similar to F1. Using the same carrier for each
payload makes the task difficult as they have high similarity
between them.
Same Carrier-Encryption:
The GVDG tool provides the option for encrypting the
malware samples for the File-virus carrier type. We use
this option to generate 100 encrypted malware samples
for each task(payload) and use them as test data with the
unencrypted versions from the same carrier experiment as
training samples. From Fig. 10 ACTR-IB had an average F1
of 0.9 outperforming LOG-REG 0.9 vs 0.8, SVM 0.9 vs 0.8,
RF 0.9 vs 0.74 and ACTR-R 0.9 vs 0.88 (t (1698) â‰¥ 2.36
, p â‰¤ 0.02). Encrypting malware samples morphs the task
during execution making it difficult to detect during analysis.
Hence the drop in performance as compared to non-encrypted
samples. We note that SVM performs better than RF likely
because it looks to maximize generalization.

0.5
0.4
0.3
0.2
0.1

1

0.9

File-virus

Key-logger

Trojan-E

USB-worm

Web-T

0.5

Average

SVM

F1

Fig. 9: Average Precision, Recall, F1 and Family prediction
comparisons for LOG-REG,SVM, RF, ACTR-R and ACTRIB.

0.4

Average

F1

Leave one carrier out cross-validation:
To see how the models generalize to unseen malware family(carrier), we performed a leave-one-carrier-out comparison
(using high mutation samples), where we test the models
against one previously unseen malware carrier. ACTR-IB performs better or on par with all other baseline approaches for
all the carriers. It clearly outperforms all the approaches in
recalling most of the actual tasks (40%) (see Figure 8). ACTRIB has shown to generalize for unseen malware families [17].
This case is difficult given the fact that the test family is
not represented during training, hence task prediction depends
on associating the test family with the training families that
perform similar tasks.

Recall

0.8
0.7

0.3
0.6

0.2

Precision

0.1
Precision

LOG-REG

Recall

SVM

RF

ACTR-R

F1

ACTR-IB

Fig. 8: Average F1 values for 5 malware carriers (above) and
the average precision, recall and F1 across all carriers (below)
for LOG-REG, SVM, RF, ACTR-R and ACTR-IB..
Same Carrier:
As seen in the previous experiments, different carrier types
makes the task easier because of less similarity between them.
We now test the performance, on same carrier type performing
exactly one task. Since there are 17 tasks in the GVDG tool,
we generate 100 samples for each task for carrier type Filevirus. In this experiment each task represents one family.
Thus in total we have 1700 samples. We do the 60-40 split
experiment. From Fig. 9 ACTR-IB had an average F1 of 0.95

LOG-REG

Recall

SVM

RF

F1

ACTR-R

Family
Prediction
ACTR-IB

Fig. 10: Average Precision, Recall, F1 and Family prediction
comparisons for LOG-REG,SVM, RF, ACTR-R and ACTRIB.
VI. R ELATED W ORK
Identification of malicious software. The identification of
whether or not binary is malicious [8], [26] is important and
can be regarded as a â€œfirst stepâ€ in the analysis of suspicious
binaries in the aftermath of a cyber-attack. However, we note
that as many pieces of malware are designed to perform
multiple tasks, that successful identification of a binary as
malicious does not mean that the identification of its associated
tasks will be a byproduct of the result - and hence this is
normally the case, which has led to some of the other related
work described in this section.
Malware family classification. There is a wealth of existing

work on malware family identification [2], [15], [16]. The
intuition here is that by identifying the family of a given piece
of malware, an analyst can then more easily determine what
it was designed to do based on previously studied samples
from the same family. However, malware family classification
has suffered from two primary draw-backs: (1) disagreement
about malware family ground truth as different analysts (e.g.
Symantec and MacAfee) cluster malware into families differently; and (2) previous work has shown that some of these
approaches mainly succeed in â€œeasy to classifyâ€ samples [19],
[23], where â€œeasy to classifyâ€ is a family that is agreed upon
by multiple malware firms. In this paper, we infer the specific
tasks a piece of malware was designed to carry out. While we
do assign malware to a family as a component of our approach,
to avoid the two aforementioned issues as the family partition
is done so probabilistically and the result ground truth is the
focus of our comparison (though we show family prediction
results as a side-result). Further, we also describe and evaluate
a variant of our instance-based method that does not consider
families and yields a comparable performance to our instancebased method that does consider families.
Malware task identification. With regard to direct inference
of malware tasks, the major related work include the software
created by the firm Invincea [12] for which we have included
a performance comparison. Additionally, some of the ideas
in this paper were first introduced in [17], [27]. However, that
work primarily focused on describing the intuitions behind the
cognitive modeling techniques and only included experimental
evaluation on one dataset (the Mandiant APT1 dataset) and
one sandbox environment (Anubis) with a comparison amongst
only the instance based approach, the rule-based cognitive
model, the standard decision tree, and the naive Bayes reasoner. The experimental evaluation in this paper was designed
to be much more thorough to pave the way toward deployment
of the approach for use by cyber-security analysts.
VII.

[2]
[3]
[4]
[5]

[6]
[7]
[8]

[9]
[10]
[11]
[12]

[13]
[14]
[15]
[16]

[17]

[18]

C ONCLUSION

In this paper, we introduced an automated method that
combines dynamic malware analysis with cognitive modeling
to identify malware tasks. This method obtains excellent precision and recall - often achieving an unbiased F1 score of over
0.9 - in a wide variety of conditions over two different malware
sample collections and two different sandbox environments outperforming a variety of baseline methods. Currently, our
future work has three directions. First, we are looking to
create a deployed version of our approach to aid cyber-security
analysts in the field. Second, we look to enhance our malware
analysis to also include network traffic resulting from the
sample by extending the capabilities of the sandbox. Finally,
we also look to address cases of highly-sophisticated malware
that in addition to using encryption and packing to limit static
analysis, also employ methods to â€œshut downâ€ when run in a
sandbox environment [20]. We are exploring multiple methods
to address this such as the recently introduced technique of
â€œspatial analysisâ€ [9] that involves direct analysis of a malware
binary.

[19]
[20]

[21]
[22]
[23]
[24]

[25]
[26]

[27]

R EFERENCES
[1]

J. R. Anderson, D. Bothell, M. D. Byrne, S. Douglass, C. Lebiere, and
Y. Qin. An integrated theory of mind. PSYCHOLOGICAL REVIEW,
111:1036â€“1060, 2004.

[28]

U. Bayer, P. M. Comparetti, C. Hlauschek, C. Kruegel, and E. Kirda.
Scalable, behavior-based malware clustering, 2009.
D. Bothell. Act-r 6.0 reference manual. http:// act-r.psy.cmu.edu/ actr6/
reference-manual.pdf , 2004.
L. Breiman. Random forests. Machine Learning, 45(1):5â€“32, 2001.
C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector
machines. ACM Trans. Intell. Syst. Technol., 2(3):27:1â€“27:27, May
2011.
J. B. M. S. Claudio Guarnieri, Alessandro Tanasi. Cuckoo sandbox.
http:// www.cuckoosandbox.org/ , 2012.
C. Cortes and V. Vapnik. Support-vector networks. pages 273â€“297,
1995.
I. Firdausi, C. lim, A. Erwin, and A. S. Nugroho. Analysis of machine
learning techniques used in behavior-based malware detection. In
Proceedings of the 2010 Second International Conference on ACT, ACT
â€™10, pages 201â€“203, Washington, DC, USA, 2010. IEEE Computer
Society.
D. Giametta and A. Potter. Shmoomcon 2014:there and back again:a
critical analysis of spatial analysis, 2014.
C. Gonzalez, J. F. Lerch, and C. Lebiere. Instance-based learning in
dynamic decision making. Cognitive Science, 27(4):591 â€“ 635, 2003.
GVDG. Generator malware gvdg. 2011.
Invencia. Crowdsource: Crowd trained machine learning model for
malware capability detection. http:// www.invincea.com/ tag/ cynomix/ ,
2013.
ISEC-Lab. Anubis: Analyzing unknown binaries. http:// anubis.iseclab.
org/ , 2007.
Kaspersky. Gauss: Abnormal distribution, 2012.
J. Kinable and O. Kostakis. Malware classification based on call graph
clustering. J. Comput. Virol., 7(4):233â€“245, Nov. 2011.
D. Kong and G. Yan. Discriminant malware distance learning on structural information for automated malware classification. In Proceedings
of the 19th ACM SIGKDD, KDD â€™13, pages 1357â€“1365, New York,
NY, USA, 2013. ACM.
C. Lebiere, S. Bennati, R. Thomson, P. Shakarian, and E. Nunes.
Functional cognitive models of malware identification. In Proceedings
of ICCM, ICCM 2015, Groningen, The Netherlands, April 9-11, 2015,
2015.
C. Lebiere, P. Pirolli, R. Thomson, J. Paik, M. Rutledge-Taylor,
J. Staszewski, and J. R. Anderson. A functional model of sensemaking
in a neurocognitive architecture. Intell. Neuroscience, 2013:5:5â€“5:5,
Jan. 2013.
P. Li, L. Liu, and M. K. Reiter. On challenges in evaluating malware
clustering, 2007.
M. Lindorfer, C. Kolbitsch, and P. Milani Comparetti. Detecting
environment-sensitive malware. In Proceedings of the 14th International Conference on RAID, RAIDâ€™11, pages 338â€“357, Berlin, Heidelberg, 2011. Springer-Verlag.
Mandiant. Apt1:exposing one of chinaâ€™s cyber espionage units. http:
// intelreport.mandiant.com/ , 2013.
Mandiant. Mandiant APT1 samples categorized by malware families.
Contagio Malware Dump, 2013.
R. Perdisci and ManChon. Vamo: towards a fully automated malware
clustering validity analysis. In ACSAC, pages 329â€“338. ACM, 2012.
M. Sikorski and A. Honig. Practical Malware Analysis: The Hands-On
Guide to Dissecting Malicious Software. No Starch Press, 1 edition,
2012.
R. S. Sutton and A. G. Barto. Introduction to Reinforcement Learning.
MIT Press, Cambridge, MA, USA, 1st edition, 1998.
A. Tamersoy, K. Roundy, and D. H. Chau. Guilt by association: Large
scale malware detection by mining file-relation graphs. In Proceedings
of the 20th ACM SIGKDD, KDD â€™14, pages 1524â€“1533. ACM, 2014.
R. Thomson, C. Lebiere, S. Bennati, P. Shakarian, and E. Nunes. Malware identification using cognitively-inspired inference. In Proceedings
of BRIMS, BRIMS 2015, Washington DC, March 31-April 3, 2015, 2015.
T. J. Wong, E. T. Cokely, and L. J. Schooler. An online database of
act-r parameters: Towards a transparent community-based approach to
model development. 2010.

Belief Revision in
Structured Probabilistic Argumentation
Paulo Shakarian1 , Gerardo I. Simari2 , and Marcelo A. Falappa3

arXiv:1401.1475v1 [cs.LO] 7 Jan 2014

1

2

Department of Electrical Engineering and Computer Science
U.S. Military Academy, West Point, NY, USA
paulo@shakarian.net
Department of Computer Science, University of Oxford, United Kingdom
gerardo.simari@cs.ox.ac.uk
3
Departamento de Ciencias e IngenierÄ±Ìa de la ComputacioÌn
Universidad Nacional del Sur, BahÄ±Ìa Blanca, Argentina
mfalappa@cs.uns.edu.ar

Abstract. In real-world applications, knowledge bases consisting of all
the information at hand for a specific domain, along with the current
state of affairs, are bound to contain contradictory data coming from
different sources, as well as data with varying degrees of uncertainty
attached. Likewise, an important aspect of the effort associated with
maintaining knowledge bases is deciding what information is no longer
useful; pieces of information (such as intelligence reports) may be outdated, may come from sources that have recently been discovered to be
of low quality, or abundant evidence may be available that contradicts
them. In this paper, we propose a probabilistic structured argumentation
framework that arises from the extension of Presumptive Defeasible Logic
Programming (PreDeLP) with probabilistic models, and argue that this
formalism is capable of addressing the basic issues of handling contradictory and uncertain data. Then, to address the last issue, we focus on the
study of non-prioritized belief revision operations over probabilistic PreDeLP programs. We propose a set of rationality postulates â€“ based on
well-known ones developed for classical knowledge bases â€“ that characterize how such operations should behave, and study a class of operators
along with theoretical relationships with the proposed postulates, including a representation theorem stating the equivalence between this class
and the class of operators characterized by the postulates.

1

Introduction and Related Work

Decision-support systems that are part of virtually any kind of real-world application must be part of a framework that is rich enough to deal with several
basic problems: (i) handling contradictory information; (ii) answering abductive queries; (iii) managing uncertainty; and (iv) updating beliefs. Presumptions
come into play as key components of answers to abductive queries, and must be
maintained as elements of the knowledge base; therefore, whenever candidate answers to these queries are evaluated, the (in)consistency of the knowledge base

together with the presumptions being made needs to be addressed via belief
revision operations.
In this paper, we begin by proposing a framework that addresses items (i)â€“
(iii) by extending Presumptive DeLP [1] (PreDeLP, for short) with probabilistic
models in order to model uncertainty in the application domain; the resulting
framework is a general-purpose probabilistic argumentation language that we
will refer to as Probabilistic PreDeLP(P-PreDeLP, for short).
In the second part of this paper, we address the problem of updating beliefs â€“
item (iv) above â€“ in P-PreDeLP knowledge bases, focusing on the study of nonprioritized belief revision operations. We propose a set of rationality postulates
characterizing how such operations should behave â€“ these postulates are based
on the well-known postulates proposed in [2] for non-prioritized belief revision in
classical knowledge bases. We then study a class of operators and their theoretical
relationships with the proposed postulates, concluding with a representation
theorem.

Related Work. Belief revision studies changes to knowledge bases as a response
to epistemic inputs. Traditionally, such knowledge bases can be either belief sets
(sets of formulas closed under consequence) [3, 4] or belief bases [5, 2] (which are
not closed); since our end goal is to apply the results we obtain to real-world
domains, here we focus on belief bases. In particular, as motivated by requirements (i)â€“(iv) above, our knowledge bases consist of logical formulas over which
we apply argumentation-based reasoning and to which we couple a probabilistic model. The connection between belief revision and argumentation was first
studied in [6]; since then, the work that is most closely related to our approach
is the development of the explanation-based operators of [7].
The study of argumentation systems together with probabilistic reasoning
has recently received a lot attention, though a significant part has been in the
combination between the two has been in the form of probabilistic abstract argumentation [8â€“11]. There have, however, been several approaches that combine
structured argumentation with models for reasoning under uncertainty; the first
of such approaches to be proposed was [12], and several others followed, such
as the possibilistic approach of [13], and the probabilistic logic-based approach
of [14]. The main difference between these works and our own is that here we
adopt a bipartite knowledge base, where one part models the knowledge that is
not inherently probabilistic â€“ uncertain knowledge is modeled separately, thus
allowing a clear separation of interests between the two kinds of models. This
approach is based on a similar one developed for ontological languages in the
Semantic Web (see [15], and references within).
Finally, to the best of our knowledge, this is the first paper in which the combination of structured argumentation, probabilistic models, and belief revision
has been addressed in conjunction.

Probabilistic Model (EM)
Analytical Model (AM)
â€œMalware X was compiled on a system
â€œMalware X was compiled on a system in
using the English language.â€
English-speaking country Y.â€
â€œCounty Y and country Z are
â€œCountry Y has a motive to launch a
currently at war.â€
cyber-attack against country Z
â€œMalware W and malware X were created â€œMalware W and malware X are related.
in a similar coding style.â€
Table 1. Examples of the kind of information that could be represented in the two
different models in a cyber-security application domain.

2

Preliminaries

The Probabilistic PreDeLP (P-PreDeLP, for short) framework is composed of
two separate models of the world. The first is called the environmental model
(referred to as â€œEMâ€), and is used to describe the probabilistic knowledge that
we have about the domain. The second one is called the analytical model (referred
to as â€œAMâ€), and is used to analyze competing hypotheses that can account for
a given phenomenon â€“ what we will generally call queries. The AM is composed
of a classical (that is, non-probabilistic) PreDeLP program in order to allow for
contradictory information, giving the system the capability to model competing
explanations for a given query.
Two Kinds of Uncertainty. In general, the EM contains knowledge such as
evidence, uncertain facts, or knowledge about agents and systems. The AM, on
the other hand, contains ideas that a user may conclude based on the information in the EM. Table 1 gives some examples of the types of information that
could appear in each of the two models in a cyber-security application. Note that
a knowledge engineer (or automated system) could assign a probability to statements in the EM column, whereas statements in the AM column can be either
true or false depending on a certain combination (or several possible combinations) of statements from the EM. There are thus two kinds of uncertainty that
need to be modeled: probabilistic uncertainty and uncertainty arising from defeasible knowledge. As we will see, our model allows both kinds of uncertainty to
coexist, and also allows for the combination of the two since defeasible rules and
presumptions (that is, defeasible facts) can also be annotated with probabilistic
events.
In the rest of this section, we formally describe these two models, as well as
how knowledge in the AM can be annotated with information from the EM â€“
these annotations specify the conditions under which the various statements in
the AM can potentially be true.
Basic Language. We assume sets of variable and constant symbols, denoted
with V and C, respectively. In the rest of this paper, we will use capital letters
to represent variables (e.g., X, Y, Z), while lowercase letters represent constants.
The next component of the language is a set of n-ary predicate symbols; the
EM and AM use separate sets of predicate symbols, denoted with PEM , PAM ,

respectively â€“ the two models can, however, share variables and constants. As
usual, a term is composed of either a variable or constant. Given terms t1 , ..., tn
and n-ary predicate symbol p, p(t1 , ..., tn ) is called an atom; if t1 , ..., tn are constants, then the atom is said to be ground. The sets of all ground atoms for EM
and AM are denoted with GEM and GAM , respectively.
Given set of ground atoms, a world is any subset of atoms â€“ those that belong to the set are said to be true in the world, while those that do not are
false. Therefore, there are 2|GEM | possible worlds in the EM and 2|GAM | worlds
in the AM. These sets are denoted with WEM and WAM , respectively. In order to avoid worlds that do not model possible situations given a particular
domain, we include integrity constraints of the form oneOf(Aâ€² ), where Aâ€² is a
subset of ground atoms. Intuitively, such a constraint states that any world where
more than one of the atoms from set Aâ€² appears is invalid. We use ICEM and
ICAM to denote the sets of integrity constraints for the EM and AM, respectively, and the sets of worlds that conform to these constraints is denoted with
WEM (ICEM ), WAM (ICAM ), respectively.
Finally, logical formulas arise from the combination of atoms using the traditional connectives (âˆ§, âˆ¨, and Â¬). As usual, we say a world w satisfies formula
(f ), written w |= f , iff: (i) If f is an atom, then w |= f iff f âˆˆ w; (ii) if f = Â¬f â€²
then w |= f iff w 6|= f â€² ; (iii) if f = f â€² âˆ§ f â€²â€² then w |= f iff w |= f â€² and w |= f â€²â€² ;
and (iv) if f = f â€² âˆ¨ f â€²â€² then w |= f iff w |= f â€² or w |= f â€²â€² . We use the notation
formEM , formAM to denote the set of all possible (ground) formulas in the EM
and AM, respectively.
2.1

Probabilistic Model

The EM or environmental model is largely based on the probabilistic logic of [16],
which we now briefly review.
Definition 1. Let f be a formula over PEM , V, and C, p âˆˆ [0, 1], and Ç« âˆˆ
[0, min(p, 1 âˆ’ p)]. A probabilistic formula is of the form f : p Â± Ç«. A set KEM of
probabilistic formulas is called a probabilistic knowledge base.
In the above definition, the number Ç« is referred to as an error tolerance. Intuitively, probabilistic formulas are interpreted as â€œformula f is true with probability between p âˆ’ Ç« and p + Ç«â€ â€“ note that there are no further constraints over
this interval apart from those imposed by other probabilistic formulas in the
knowledge base. The uncertainty regarding the probability values stems from
the fact that certain assumptions (such as probabilistic independence) may not
be suitable in the environment being modeled.
Example 1. Consider the following set KEM :
f1 = a : 0.8 Â± 0.1
f2 = b : 0.2 Â± 0.1
f3 = c : 0.8 Â± 0.1

f4 = d âˆ§ e
: 0.7 Â± 0.2
f5 = f âˆ§ g âˆ§ h : 0.6 Â± 0.1
f6 = i âˆ¨ Â¬j
: 0.9 Â± 0.1

â€²
Throughout the paper, we also use KEM
= {f1 , f2 , f3 }

f7 = k : 1 Â± 0



A set of probabilistic formulas describes a set of possible probability distributions Pr over the set WEM (ICEM ). We say that
Pprobability distribution Pr
satisfies probabilistic formula f : p Â± Ç« iff: p âˆ’ Ç« â‰¤ wâˆˆWEM (ICEM ) Pr (w) â‰¤ p + Ç«.
We say that a probability distribution over WEM (ICEM ) satisfies KEM iff it
satisfies all probabilistic formulas in KEM .
Given a probabilistic knowledge base and a (non-probabilistic) formula q,
the maximum entailment problem seeks to identify real numbers p, Ç« such that
all valid probability distributions Pr that satisfy KEM also satisfy q : p Â± Ç«, and
there does not exist pâ€² , Ç«â€² s.t. [p âˆ’ Ç«, p + Ç«] âŠƒ [pâ€² âˆ’ Ç«â€² , pâ€² + Ç«â€² ], where all probability
distributions Pr that satisfy KEM also satisfy q : pâ€² Â± Ç«â€² . In order to solve this
problem we must solve the linear program defined below.
Definition 2. Given a knowledge base KEM and a formula q, we have a variable
xi for each wi âˆˆ WEM (ICEM ).
â€“ For each fj : pj Â± Ç«j âˆˆ KEM , there is a constraint of the form:
P
pj âˆ’ Ç«j â‰¤ wi âˆˆWEM (ICEM ) s.t. wi |=fj xi â‰¤ pj + Ç«j .
P
â€“ We also have the constraint: wi âˆˆWEM (ICEM ) xi = 1.
P
â€“ The objective is to minimize the function: wi âˆˆWEM (ICEM ) s.t. wi |=q xi .
We use the notation EP-LP-MIN(KEM , q) to refer to the value of the objective
function in the solution to the EM-LP-MIN constraints.
The next step is to solve the linear program a second time, but instead
maximizing the objective function (we shall refer to this as EM-LP-MAX) â€“ let
â„“ and u be the results of these operations, respectively. In [16], it is shown that
Ç« = uâˆ’â„“
and p = â„“ + Ç« is the solution to the maximum entailment problem.
2
We note that although the above linear program has an exponential number
of variables in the worst case (i.e., no integrity constraints), the presence of
constraints has the potential to greatly reduce this space. Further, there are also
good heuristics (cf. [17, 18]) that have been shown to provide highly accurate
approximations with a reduced-size linear program.
â€²
Example 2. Consider KB KEM
from Example 1 and a set of ground atoms restricted to those that appear in that program; we have the following worlds:

w1 = {a, b, c}
w5 = {b}

w2 = {a, b}
w6 = {a}

w3 = {a, c}
w7 = {c}

w4 = {b, c}
w8 = âˆ…

and suppose we wish to compute the probability for formula q = a âˆ¨ c. For each
formula in KEM we have a constraint, and for each world above we have a variable. An objective function is created based on the worlds that satisfy the query
â€²
formula (in this case, worlds w1 , w2 , w3 , w4 , w6 , w7 ). Solving EP-LP-MAX(KEM
, q)
â€²
and EP-LP-MIN(KEM , q), we obtain the solution 0.9 Â± 0.1.


3

Argumentation Model

For the analytical model (AM), we choose a structured argumentation framework [19] due to several characteristics that make such frameworks highly applicable to many domains. Unlike the EM, which describes probabilistic information about the state of the real world, the AM must allow for competing ideas.
Therefore, it must be able to represent contradictory information. The algorithmic approach we shall later describe allows for the creation of arguments based
on the AM that may â€œcompeteâ€ with each other to answer a given query. In this
competition â€“ known as a dialectical process â€“ one argument may defeat another
based on a comparison criterion that determines the prevailing argument. Resulting from this process, certain arguments are warranted (those that are not
defeated by other arguments) thereby providing a suitable explanation for the
answer to a given query.
The transparency provided by the system can allow knowledge engineers
to identify potentially incorrect input information and fine-tune the models or,
alternatively, collect more information. In short, argumentation-based reasoning
has been studied as a natural way to manage a set of inconsistent information â€“ it
is the way humans settle disputes. As we will see, another desirable characteristic
of (structured) argumentation frameworks is that, once a conclusion is reached,
we are left with an explanation of how we arrived at it and information about why
a given argument is warranted; this is very important information for users to
have. In the following, we first recall the basics of the underlying argumentation
framework used, and then go on to introduce the analytical model (AM).
3.1

Defeasible Logic Programming with Presumptions (PreDeLP)

Defeasible Logic Programming with Presumptions (PreDeLP) [1] is a formalism
combining logic programming with defeasible argumentation; it arises as an extension of classical DeLP [20] with the possibility of having presumptions, as
described below â€“ since this capability is useful in many applications, we adopt
this extended version in this paper. In this section, we briefly recall the basics
of PreDeLP; we refer the reader to [20, 1] for the complete presentation.
The formalism contains several different constructs: facts, presumptions, strict
rules, and defeasible rules. Facts are statements about the analysis that can always be considered to be true, while presumptions are statements that may or
may not be true. Strict rules specify logical consequences of a set of facts or
presumptions (similar to an implication, though not the same) that must always
occur, while defeasible rules specify logical consequences that may be assumed
to be true when no contradicting information is present. These building blocks
are used in the construction of arguments, and are part of a PreDeLP program,
which is a set of facts, strict rules, presumptions, and defeasible rules. Formally,
we use the notation Î AM = (Î˜, â„¦, Î¦, âˆ†) to denote a PreDeLP program, where
â„¦ is the set of strict rules, Î˜ is the set of facts, âˆ† is the set of defeasible rules,
and Î¦ is the set of presumptions. In Figure 1, we provide an example Î AM . We
now define these constructs formally.

Î˜ : Î¸1a = p

Î¸1b = q

Î¸2 = r

â„¦ : Ï‰1a = Â¬s â† t

Ï‰1b = Â¬t â† s

Ï‰2a = s â† p, u, r, v

Î¦ : Ï†1 = y

Ï†2 = v

Ï†3 = Â¬z

â€“â‰º

âˆ† : Î´1a = s â€“â‰º p
Î´4 = u â€“â‰º y

â€“â‰º

Î´1b = t â€“â‰º q
Î´5a = Â¬u â€“â‰º Â¬z

Ï‰2b = t â† q, w, x, v

â€“â‰º

Î´2 = s â€“â‰º u
Î´5b = Â¬w â€“â‰º Â¬n

Î´3 = s â€“â‰º r, v

Fig. 1. An example (propositional) argumentation framework.

Facts (Î˜) are ground literals representing atomic information or its negation,
using strong negation â€œÂ¬â€. Note that all of the literals in our framework must
be formed with a predicate from the set PAM . Note that information in the form
of facts cannot be contradicted. We will use the notation [Î˜] to denote the set
of all possible facts.
Strict Rules (â„¦) represent non-defeasible cause-and-effect information that resembles an implication (though the semantics is different since the contrapositive
does not hold) and are of the form L0 â† L1 , . . . , Ln , where L0 is a ground literal
and {Li }i>0 is a set of ground literals. We will use the notation [â„¦] to denote
the set of all possible strict rules.
Presumptions (Î¦) are ground literals of the same form as facts, except that
they are not taken as being true but rather defeasible, which means that they
can be contradicted. Presumptions are denoted in the same manner as facts,
except that the symbol â€“â‰º is added.
Defeasible Rules (âˆ†) represent tentative knowledge that can be used if nothing
can be posed against it. Just as presumptions are the defeasible counterpart of
facts, defeasible rules are the defeasible counterpart of strict rules. They are of
the form L0 â€“â‰º L1 , . . . , Ln , where L0 is a ground literal and {Li }i>0 is a set of
ground literals. In both strict and defeasible rules, strong negation is allowed in
the head of rules, and hence may be used to represent contradictory knowledge.
Even though the above constructs are ground, we allow for schematic versions
with variables that are used to represent sets of ground rules. We denote variables
with strings starting with an uppercase letter.
Arguments. Given a query in the form of a ground atom, the goal is to derive
arguments for and against itâ€™s validity â€“ derivation follows the same mechanism of logic programming [21]. Since rule heads can contain strong negation,
it is possible to defeasibly derive contradictory literals from a program. For the
treatment of contradictory knowledge, PreDeLP incorporates a defeasible argumentation formalism that allows the identification of the pieces of knowledge
that are in conflict and, through the previously mentioned dialectical process,
decides which information prevails as warranted. This dialectical process involves

hA1 , si
hA3 , si
hA5 , ui
hA7 , Â¬ui

A1
A3
A5
A7

= {Î¸1a , Î´1a }
= {Ï†1 , Î´2 , Î´4 }
= {Ï†1 , Î´4 }
= {Ï†3 , Î´5a }

hA2 , si A2 = {Ï†1 , Ï†2 , Î´4 , Ï‰2a , Î¸1a , Î¸2 }
hA4 , si A4 = {Ï†2 , Î´3 , Î¸2 }
hA6 , Â¬si A6 = {Î´1b , Î¸1b , Ï‰1a }

Fig. 2. Example ground arguments from the framework of Figure 1.

the construction and evaluation of arguments, building a dialectical tree in the
process. Arguments are formally defined next.
Definition 3. An argument hA, Li for a literal L is a pair of the literal and
a (possibly empty) set of the EM (A âŠ† Î AM ) that provides a minimal proof
for L meeting the following requirements: (i) L is defeasibly derived from A;
(ii) â„¦ âˆª Î˜ âˆª A is not contradictory; and (iii) A is a minimal subset of âˆ† âˆª Î¦
satisfying 1 and 2, denoted hA, Li.
Literal L is called the conclusion supported by the argument, and A is the
support of the argument. An argument hB, Li is a subargument of hA, Lâ€² i iff
B âŠ† A. An argument hA, Li is presumptive iff A âˆ© Î¦ is not empty. We will also
use â„¦(A) = A âˆ© â„¦, Î˜(A) = A âˆ© Î˜, âˆ†(A) = A âˆ© âˆ†, and Î¦(A) = A âˆ© Î¦.
Our definition differs slightly from that of [22], where DeLP is introduced, as
we include strict rules and facts as part of arguments â€“ the reason for this will
become clear in Section 4. Arguments for our scenario are shown next.
Example 3. Figure 2 shows example arguments based on the knowledge base
from Figure 1. Note that hA5 , ui is a sub-argument of hA2 , si and hA3 , si.

Given an argument hA1 , L1 i, counter-arguments are arguments that contradict it. Argument hA2 , L2 i is said to counterargue or attack hA1 , L1 i at a
literal Lâ€² iff there exists a subargument hA, Lâ€²â€² i of hA1 , L1 i such that the set
â„¦(A1 ) âˆª â„¦(A2 ) âˆª Î˜(A1 ) âˆª Î˜(A2 ) âˆª {L2 , Lâ€²â€² } is contradictory.
Example 4. Consider the arguments from Example 3. The following are some of
the attack relationships between them: A1 , A2 , A3 , and A4 all attack A6 ; A5
attacks A7 ; and A7 attacks A2 .

A proper defeater of an argument hA, Li is a counter-argument that â€“ by
some criterion â€“ is considered to be better than hA, Li; if the two are incomparable according to this criterion, the counterargument is said to be a blocking
defeater. An important characteristic of PreDeLP is that the argument comparison criterion is modular, and thus the most appropriate criterion for the domain
that is being represented can be selected; the default criterion used in classical
defeasible logic programming (from which PreDeLP is derived) is generalized
specificity [23], though an extension of this criterion is required for arguments
using presumptions [1]. We briefly recall this criterion next â€“ the first definition is for generalized specificity, which is subsequently used in the definition of
presumption-enabled specificity.

Definition 4. Let Î AM = (Î˜, â„¦, Î¦, âˆ†) be a PreDeLP program and let F be
the set of all literals that have a defeasible derivation from Î AM . An argument
hA1 , L1 i is preferred to hA2 , L2 i, denoted with A1 â‰»P S A2 iff:
(1) For all H âŠ† F , â„¦(A1 ) âˆª â„¦(A2 ) âˆª H is non-contradictory: if there is a
derivation for L1 from â„¦(A2 ) âˆª â„¦(A1 ) âˆª âˆ†(A1 ) âˆª H, and there is no derivation
for L1 from â„¦(A1 ) âˆª â„¦(A2 ) âˆª H, then there is a derivation for L2 from â„¦(A1 ) âˆª
â„¦(A2 ) âˆª âˆ†(A2 ) âˆª H; and
(2) there is at least one set H â€² âŠ† F , â„¦(A1 ) âˆª â„¦(A2 ) âˆª H â€² is non-contradictory,
such that there is a derivation for L2 from â„¦(A1 ) âˆª â„¦(A2 ) âˆª H â€² âˆª âˆ†(A2 ), there
is no derivation for L2 from â„¦(A1 ) âˆª â„¦(A2 ) âˆª H â€² , and there is no derivation for
L1 from â„¦(A1 ) âˆª â„¦(A2 ) âˆª H â€² âˆª âˆ†(A1 ).
Intuitively, the principle of specificity says that, in the presence of two conflicting lines of argument about a proposition, the one that uses more of the available information is more convincing. A classic example involves a bird, Tweety,
and arguments stating that it both flies (because it is a bird) and doesnâ€™t fly (because it is a penguin). The latter argument uses more information about Tweety
â€“ it is more specific â€“ and is thus the stronger of the two.
Definition 5 ([1]). Let Î AM = (Î˜, â„¦, Î¦, âˆ†) be a PreDeLP program. An argument hA1 , L1 i is preferred to hA2 , L2 i, denoted with A1 â‰» A2 iff any of the
following conditions hold:
(1) hA1 , L1 i and hA2 , L2 i are both factual arguments and hA1 , L1 i â‰»P S hA2 , L2 i.
(2) hA1 , L1 i is a factual argument and hA2 , L2 i is a presumptive argument.
(3) hA1 , L1 i and hA2 , L2 i are presumptive arguments, and
(a) Î¦(A1 ) ( Î¦(A2 ) or,
(b) Î¦(A1 ) = Î¦(A2 ) and hA1 , L1 i â‰»P S hA2 , L2 i.
Generally, if A, B are arguments with rules X and Y , resp., and X âŠ‚ Y , then A
is stronger than B. This also holds when A and B use presumptions P1 and P2 ,
resp., and P1 âŠ‚ P2 .
Example 5. The following are some relationships between arguments from Example 3, based on Definitions 4 and 5.
A1 and A6 are incomparable (blocking defeaters);
A6 â‰» A2 , and thus A6 defeats A2 ;
A5 and A7 are incomparable (blocking defeaters).



A sequence of arguments called an argumentation line thus arises from this
attack relation, where each argument defeats its predecessor. To avoid undesirable sequences, which may represent circular argumentation lines, in DeLP an
argumentation line is acceptable if it satisfies certain constraints (see [20]). A
literal L is warranted if there exists a non-defeated argument A supporting L.
Clearly, there can be more than one defeater for a particular argument hA, Li.
Therefore, many acceptable argumentation lines could arise from hA, Li, leading to a tree structure. The tree is built from the set of all argumentation lines

rooted in the initial argument. In a dialectical tree, every node (except the root)
represents a defeater of its parent, and leaves correspond to undefeated arguments. Each path from the root to a leaf corresponds to a different acceptable
argumentation line. A dialectical tree provides a structure for considering all
the possible acceptable argumentation lines that can be generated for deciding
whether an argument is defeated. We call this tree dialectical because it represents an exhaustive dialectical4 analysis for the argument in its root. For a given
argument hA, Li, we denote the corresponding dialectical tree as T (hA, Li).
Given a literal L and an argument hA, Li, in order to decide whether or not a
literal L is warranted, every node in the dialectical tree T (hA, Li) is recursively
marked as â€œDâ€ (defeated ) or â€œUâ€ (undefeated ), obtaining a marked dialectical
tree T âˆ— (hA, Li) as follows:
1. All leaves in T âˆ— (hA, Li) are marked as â€œUâ€s, and
2. Let hB, qi be an inner node of T âˆ— (hA, Li). Then hB, qi will be marked as â€œUâ€
iff every child of hB, qi is marked as â€œDâ€. The node hB, qi will be marked as
â€œDâ€ iff it has at least a child marked as â€œUâ€.
Given an argument hA, Li obtained from Î AM , if the root of T âˆ— (hA, Li) is
marked as â€œUâ€, then we will say that T âˆ— (hA, hi) warrants L and that L is warranted from Î AM . (Warranted arguments correspond to those in the grounded
extension of a Dung argumentation system [24].) There is a further requirement
when the arguments in the dialectical tree contains presumptions â€“ the conjunction of all presumptions used in even (respectively, odd) levels of the tree must
be consistent. This can give rise to multiple trees for a given literal, as there can
potentially be different arguments that make contradictory assumptions.
We can then extend the idea of a dialectical tree to a dialectical forest. For
a given literal L, a dialectical forest F (L) consists of the set of dialectical trees
for all arguments for L. We shall denote a marked dialectical forest, the set of
all marked dialectical trees for arguments for L, as F âˆ— (L). Hence, for a literal
L, we say it is warranted if there is at least one argument for that literal in the
dialectical forest F âˆ— (L) that is labeled as â€œUâ€, not warranted if there is at least
one argument for the literal Â¬L in the dialectical forest F âˆ— (Â¬L) that is labeled
as â€œUâ€, and undecided otherwise.

4

Probabilistic PreDeLP

Probabilistic PreDeLP arises from the combination of the environmental and
analytical models (Î EM and Î AM , respectively). Intuitively, given Î AM , every
element of â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ might only hold in certain worlds in the set WEM â€“
that is, they are subject to probabilistic events. Therefore, we associate elements
of â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ with a formula from formEM . For instance, we could associate
formula rainy to fact umbrella to state that the latter only holds when the
probabilistic event rainy holds; since weather is uncertain in nature, it has been
modeled as part of the EM.
4

In the sense of providing reasons for and against a position.

af(Î¸1a ) = af(Î¸1b )
af(Î¸2 )
af(Ï‰1a ) = af(Ï‰1b )
af(Ï‰2a ) = af(Ï‰2b )
af(Ï†1 )
af(Ï†2 )

= k âˆ¨ f âˆ§ h âˆ¨ (e âˆ§ l)
=i
= True
= True
=câˆ¨a
=f âˆ§m



af(Ï†3 )
af(Î´1a ) = af(Î´1b )
af(Î´2 )
af(Î´3 )
af(Î´4 )
af(Î´5a ) = af(Î´5b )

=b
= True
= True
= True
= True
= True

Fig. 3. Example annotation function.

We can then compute the probabilities of subsets of â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ using the
information contained in Î EM , as we describe shortly. The notion of an annotation function associates elements of â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ with elements of formEM .
Definition 6. An annotation function is any function af : â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦ â†’
formEM . We shall use [af ] to denote the set of all annotation functions.
We will sometimes denote annotation functions as sets of pairs (f, af(f )) in
order to simplify the presentation. Figure 3 shows an example of an annotation
function for our running example.
We now have all the components to formally define Probabilistic PreDeLP
programs (P-PreDeLP for short).
Definition 7. Given environmental model Î EM , analytical model Î AM , and
annotation function af , a probabilistic PreDeLP program is of the form I =
(Î EM , Î AM , af ). We use notation [I] to denote the set of all possible programs.
Given this setup, we can consider a world-based approach; that is, the defeat
relationship among arguments depends on the current state of the (EM) world.
Definition 8. Let I = (Î EM , Î AM , af ) be a P-PreDeLP program, argument
hA, Li is valid w.r.t. world w âˆˆ WEM iff âˆ€c âˆˆ A, w |= af(c).
We extend the notion of validity to argumentation lines, dialectical trees,
and dialectical forests in the expected way (for instance, an argumentation line
is valid w.r.t. w iff all arguments that comprise that line are valid w.r.t. w).
We also extend the idea of a dialectical tree w.r.t. worlds; so, for a given world
w âˆˆ WEM , the dialectical (resp., marked dialectical) tree induced by w is denoted
with Tw hA, Li (resp., Twâˆ— hA, Li). We require that all arguments and defeaters in
these trees to be valid with respect to w. Likewise, we extend the notion of
dialectical forests in the same manner (denoted with Fw (L) and Fwâˆ— (L), resp.).
Based on these concepts we introduce the notion of warranting scenario.
Definition 9. Let I = (Î EM , Î AM , af ) be a P-PreDeLP program and L be a
literal formed with a ground atom from GAM ; a world w âˆˆ WEM is said to be
a warranting scenario for L (denoted w âŠ¢war L) iff there is a dialectical forest
Fwâˆ— (L) in which L is warranted and Fwâˆ— (L) is valid w.r.t. w.

Hence, the set of worlds in the EM where a literal L in the AM must be true
is exactly the set of warranting scenarios â€“ these are the â€œnecessaryâ€ worlds:
nec(L) = {w âˆˆ WEM | (w âŠ¢war L)}. Now, the set of worlds in the EM where
AM literal L can be true is the following â€“ these are the â€œpossibleâ€ worlds:
poss(L) = {w âˆˆ WEM | w 6âŠ¢war Â¬L}. The probability distribution Pr defined
over the worlds in the EM induces an upper and lower bound on the probability
of literal L (denoted PL,Pr ,I ) as follows:
â„“L,Pr ,I =

X

Pr (w),

uL,Pr ,I =

wâˆˆnec(L)

X

Pr (w)

wâˆˆposs(L)

â„“L,Pr ,I â‰¤ PL,Pr ,I â‰¤ uL,Pr ,I
Since the EM in general does not define a single probability distribution, the
above computations should be done using linear programs EP-LP-MIN and EPLP-MAX, as described above.
4.1

Sources of Inconsistency

We use the following notion of (classical) consistency of PreDeLP programs: Î 
is said to be consistent if there does not exist ground literal a s.t. Î  âŠ¢ a and
Î  âŠ¢ Â¬a. For P-PreDeLP programs, there are two main kinds of inconsistency
that can be present; the first is what we refer to as EM, or Type I, (in)consistency.
Definition 10. Environmental model Î EM is Type I consistent iff there exists
a probability distribution Pr over the set of worlds WEM that satisfies Î EM .
We illustrate this type of consistency in the following example.
Example 6. The following formula is a simple example of an EM for which there
is no satisfying probability distribution:
rain âˆ¨ hail : 0.3 Â± 0;
rain âˆ§ hail : 0.5 Â± 0.1.
A P-PreDeLP program using such an EM gives rise to an example of Type I
inconsistency, as it arises from the fact that there is no satisfying interpretation
for the EM knowledge base.

Assuming a consistent EM, inconsistencies can still arise through the interaction between the annotation function and facts and strict rules. We will refer
to this as combined, or Type II, (in)consistency.
Definition 11. A P-PreDeLP program I = (Î EM , Î AM , af ), with Î AM =
hÎ˜, â„¦, Î¦, âˆ†i, is Type II consistent iff: given any probabilitySdistribution Pr that
satisfies Î EM , if there exists a world w âˆˆ WEM such that xâˆˆÎ˜âˆªâ„¦ | w|=af(x) {x}
is inconsistent, then we have Pr(w) = 0.

Thus, any EM world in which the set of associated facts and strict rules are
inconsistent (we refer to this as â€œclassical consistencyâ€) must always be assigned
a zero probability. The following is an example of this other type of inconsistency.
Example 7. Consider the EM knowledge base from Example 1, the AM presented
in Figure 1 and the annotation function from Figure 3. Suppose the following
fact is added to the argumentation model:
Î¸3 = Â¬p,
and that the annotation function is expanded as follows:
af (Î¸3 ) = Â¬k.
Clearly, fact Î¸3 is in direct conflict with fact Î¸1a â€“ this does not necessarily mean
that there is an inconsistency. For instance, by the annotation function, Î¸1a holds
in the world {k} while Î¸3 does not. However, if we consider the world:
w = {f, h)
Note that w |= af (Î¸3 ) and w |= af (Î¸2 ), which means that, in this world, two
contradictory facts can occur. Since the environmental model indicates that this
world can be assigned a non-zero probability, we have a Type II inconsist program.

Another example (perhaps easier to visualize) in the rain/hail scenario discussed
above, is as follows: suppose we have facts f = umbrella and g = Â¬umbrella,
and annotation function af (f ) = rain âˆ¨ hail and af (g) = wind. Intuitively, the
first fact states that an umbrella should be carried if it either rains or hails,
while the second states that an umbrella should not be carried if it is windy. If
the EM assigns a non-zero probability to formula (rain âˆ¨ hail) âˆ§ wind, then we
have Type II inconsistency.
In the following, we say that a P-PreDeLP program is consistent if and only
if it is both Type I and Type II consistent. However, in this paper, we focus on
Type II consistency and assume that the program is Type I consistent.
4.2

Basic Operations for Restoring Consistency

Given a P-PreDeLP program that is Type II inconsistent, there are two basic
strategies that can be used to restore consistency:
Revise the EM: the probabilistic model can be changed in order to force the
worlds that induce contradicting strict knowledge to have probability zero.
Revise the annotation function: The annotations involved in the inconsistency
can be changed so that the conflicting information in the AM does not become
induced under any possible world.
It may also appear that a third option would be to adjust the AM â€“ this is,
however, equivalent to modifying the annotation function. Consider the presence

of two facts in the AM: a, Â¬a. Assuming that this causes an inconsistency (that
is, there is at least one world in which they both hold), one way to resolve it
would be to remove one of these two literals. Suppose Â¬a is removed; this would
be equivalent to setting af(Â¬a) = âŠ¥ (where âŠ¥ represents a contradiction in the
language of the EM). In this paper, we often refer to â€œremoving elements of
Î AM â€ to refer to changes to the annotation function that cause certain elements
of the Î AM to not have their annotations satisfied in certain EM worlds.
Now, suppose that Î EM is consistent, but that the overall program is Type
II inconsistent. Then, there must exist a set of worlds in the EM where there is
a probability distribution that assigns each of them a non-zero probability. This
gives rise to the following result.
Proposition 1. If there exists a probability distribution PrSthat satisfies Î EM
s.t. there exists a world w âˆˆ WEM where Pr(w) > 0 and xâˆˆÎ˜âˆªâ„¦ | w|=af(x) {x}
is inconsistent (Type II inconsistency), then any change made in order to reâ€²
solve
yields a new EM Î EM
such that
 by modifying only Î EM
V
V this inconsistency
â€²
aâˆˆw
/ Â¬a : 0 Â± 0 is entailed by Î EM .
aâˆˆw a âˆ§
Proposition 1 seems to imply an easy strategy of adding formulas to Î EM
causing certain worlds to have a zero probability. However, this may lead to
â€²
Type I inconsistencies in the resulting model Î EM
. If we are applying an EM-only
strategy to resolve inconsistencies, this would then lead to further adjustments
â€²
to Î EM
in order to restore Type I consistency. However, such changes could
potentially lead to Type II inconsistency in the overall P-PreDeLP program
â€²
(by either removing elements of Î EM
or loosening probability bounds of the
â€²
sentences in Î EM ), which would lead to setting more EM worlds to a probability
of zero. It is easy to devise an example of a situation in which the probability
mass cannot be accommodated given the constraints imposed by the AM and
EM together â€“ in such cases, it would be impossible to restore consistency by
only modifying Î EM . We thus arrive at the following observation:
Observation 1 Given a Type II inconsistent P-PreDeLP program, consistency
cannot always be restored via modifications to Î EM alone.
Therefore, due to this line of reasoning, in this paper we focus our efforts on
modifications to the annotation function only. However, in the future, we intend
to explore belief revision operators that consider both the annotation function
(which, as we saw, captures changes to the AM) along with changes to the EM,
as well as combinations of the two.

5

Revising Probabilistic PreDeLP Programs

Given a P-PreDeLP program I = (Î EM , Î AM , af ), with Î AM = â„¦ âˆª Î˜ âˆª âˆ† âˆª Î¦,
we are interested in solving the problem of incorporating an epistemic input
(f, af â€² ) into I, where f is either an atom or a rule and af â€² is equivalent to
af , except for its expansion to include f . For ease of presentation, we assume

that f is to be incorporated as a fact or strict rule, since incorporating defeasible
knowledge can never lead to inconsistency. As we are only conducting annotation
function revisions, for I = (Î EM , Î AM , af ) and input (f, af â€² ) we denote the
â€²
â€²
revision as follows: I â€¢ (f, af â€² ) = (Î EM , Î AM
, af â€²â€² ) where Î AM
= Î AM âˆª {f }
â€²â€²
and af is the revised annotation function.
Notation. We use the symbol â€œâ€¢â€ to denote the revision operator. We also
slightly abuse notation for the sake of presentation, as well as introduce notation
to convert sets of worlds to/from formulas.
â€“ I âˆª (f, af â€² ) to denote I â€² = (Î EM , Î AM âˆª {f }, af â€² ).
â€“ (f, af â€² ) âˆˆ I = (Î AM , Î EM , af ) to denote f âˆˆ Î AM and af = af â€² .
â€“ wld(f ) = {w | w |= f } â€“ the set of worlds that satisfy formula f ; and
V
V
â€“ f or(w) = aâˆˆw a âˆ§ aâˆˆw
/ Â¬a â€“ the formula that has w as its only model.

I
â€“ Î AM
(w) = {f âˆˆ Î˜ âˆª â„¦ | w |= af(f )}

0
I
â€“ WEM
(I) = {w âˆˆ WEM | Î AM
(w) is inconsistent}
I
0
â€“ WEM
(I) = {w âˆˆ WEM
| âˆƒPr s.t. Pr |= Î EM âˆ§ Pr (w) > 0}
I
Intuitively, Î AM
(w) is the subset of facts and strict rules in Î AM whose annota0
tions are true in EM world w. The set WEM
(I) contains all the EM worlds for a
given program where the corresponding knowledge base in the AM is classically
I
inconsistent and WEM
(I) is a subset of these that can be assigned a non-zero
probability â€“ the latter are the worlds where inconsistency in the AM can arise.

5.1

Postulates for Revising the Annotation Function

We now analyze the rationality postulates for non-prioritized revision of belief
bases first introduced in [2] and later generalized in [25], in the context of PPreDeLP programs. These postulates are chosen due to the fact that they are
well studied in the literature for non-prioritized belief revision.

Inclusion: For I â€¢ (f, af â€² ) = (Î EM , Î AM âˆª {f }, af â€²â€² ), âˆ€g âˆˆ Î AM , wld af â€²â€² (g) âŠ†
wld(af â€² (g)).
This postulate states that, for any element in the AM, the worlds that satisfy its
annotation after the revision are a subset of the original set of worlds satisfying
the annotation for that element.
Vacuity: If I âˆª (f, af â€² ) is consistent, then I â€¢ (f, af â€² ) = I âˆª (f, af â€² )
Consistency Preservation: If I is consistent, then I â€¢(f, af â€² ) is also consistent.
Weak Success: If I âˆª (f, af â€² ) is consistent, then (f, af â€² ) âˆˆ I â€¢ (f, af â€² ).
Whenever the simple addition of the input doesnâ€™t cause inconsistencies to arise,
the result will contain the input.
Core Retainment: For I â€¢ (f, af â€² ) = (Î EM , Î AM âˆª {f }, af â€²â€² ), for each w âˆˆ
I
WEM
(I âˆª (f, af â€² )), we have Xw = {h âˆˆ Î˜ âˆª â„¦ | w |= af â€²â€² (h)}; for each g âˆˆ

Î AM (w) \ Xw there exists Yw âŠ† Xw âˆª {f } s.t. Yw is consistent and Yw âˆª {g} is
inconsistent.
For a given EM world, if a portion of the associated AM knowledge base is
removed by the operator, then there exists a subset of the remaining knowledge
base that is not consistent with the removed element and f .
I
Relevance: For I â€¢ (f, af â€² ) = (Î EM , Î AM âˆª {f }, af â€²â€² ), for each w âˆˆ WEM
(I âˆª
â€²
â€²â€²
(f, af )), we have Xw = {h âˆˆ Î˜ âˆª â„¦ | w |= af (h)}; for each g âˆˆ Î AM (w) \ Xw
there exists Yw âŠ‡ Xw âˆª {f } s.t. Yw is consistent and Yw âˆª {g} is inconsistent.
For a given EM world, if a portion of the associated AM knowledge base is
removed by the operator, then there exists a superset of the remaining knowledge
base that is not consistent with the removed element and f .
I
Uniformity 1: Let (f, af â€²1 ), (g, af â€²2 ) be two inputs where WEM
(I âˆª (f, af â€²1 )) =
â€²
â€²
I
I
WEM (I âˆª (g, af 2 )); for all w âˆˆ WEM (I âˆª (f, af )) and for all X âŠ† Î AM (w); if
{x | x âˆˆ X âˆª {f }, w |= af â€²1 (x)} is inconsistent iff {x | x âˆˆ X âˆª {g}, w |= af â€²2 (x)}
is inconsistent, then for each h âˆˆ Î AM , we have that:
I
{w âˆˆ WEM
(I âˆª (f, af â€²1 )) | w |= af â€²1 (h) âˆ§ Â¬af â€²â€²1 (h)} =
I
{w âˆˆ WEM
(I âˆª (g, af â€²2 )) | w |= af â€²2 (h) âˆ§ Â¬af â€²â€²2 (h)}.

If two inputs result in the same set of EM worlds leading to inconsistencies in
an AM knowledge base, and the consistency between analogous subsets (when
joined with the respective input) are the same, then the models removed from
the annotation of a given strict rule or fact are the same for both inputs.
I
Uniformity 2: Let (f, af â€²1 ), (g, af â€²2 ) be two inputs where WEM
(I âˆª (f, af â€²1 )) =
â€²
â€²
I
I
WEM (I âˆª (g, af 2 )); for all w âˆˆ WEM (I âˆª (f, af ))and for all X âŠ† Î AM (w); if
{x | x âˆˆ X âˆª {f }, w |= af â€²1 (x)} is inconsistent iff {x | x âˆˆ X âˆª {g}, w |= af â€²2 (x)}
is inconsistent, then
I
{w âˆˆ WEM
(I âˆª (f, af â€²1 )) | w |= af â€²1 (h) âˆ§ af â€²â€²1 (h)} =
I
{w âˆˆ WEM
(I âˆª (g, af â€²2 )) | w |= af â€²2 (h) âˆ§ af â€²â€²2 (h)}.

If two inputs result in the same set of EM worlds leading to inconsistencies in
an AM knowledge base, and the consistency between analogous subsets (when
joined with the respective input) are the same, then the models retained in the
the annotation of a given strict rule or fact are the same for both inputs.
Relationships between Postulates. There are a couple of interesting relationships among the postulates. The first is a sufficient condition for Core
Retainment to be implied by Relevance.
Proposition 2. Let â€¢ be an operator such that I â€¢ (f, af â€² ) = (Î EM , Î AM âˆª
Iâ€¢(f,af â€² )
I
(w) is a maximal consis{f }, af â€²â€² ), where âˆ€w âˆˆ WEM
(I âˆª (f, af â€² )), Î AM
â€²
Iâˆª(f,af )
(w). If â€¢ satisfies Relevance then it also satisfies Core
tent subset of Î AM
Retainment.

Similarly, we can show the equivalence between the two Uniformity postulates
under certain conditions.
Proposition 3. Let â€¢ be an operator such that I â€¢ (f, af â€² ) = (Î EM , Î AM âˆª
Iâˆª(f,af â€² )
Iâ€¢(f,af â€² )
(w). Operator â€¢ satisfies Unifor(w) âŠ† Î AM
{f }, af â€²â€² ) and âˆ€w, Î AM
mity 1 iff it satisfies Uniformity 2.
Given the results of Propositions 2 and 3, we will not study Core Retainment
and Uniformity 2 with respect to the construction of a belief revision operator
in the next section.
5.2

An Operator for P-PreDeLP Revision

In this section, we introduce an operator for revising a P-PreDeLP program. As
I
stated earlier, any subset of Î AM associated with a world in WEM
(I âˆª (f, af â€² ))
must be modified by the operator in order to remain consistent. So, for such a
world w, we introduce a set of candidate replacement programs for Î AM (w) in
order to maintain consistency and satisfy the Inclusion postulate.
â€²
â€²
â€²
candP gm(w, I) = {Î AM
| Î AM
âŠ† Î AM (w) s.t. Î AM
is consistent and
â€²â€²
â€²â€²
â€²â€²
â€²
s.t. Î AM
is consistent}
âˆ„Î AM âŠ† Î AM (w) s.t. Î AM âŠƒ Î AM

Intuitively, candP gm(w, I) is the set of maximal consistent subsets of Î AM (w).
Coming back to the rain/hail example presented above, we have:
Example 8. Consider the P-PreDeLP program I presented right after Example 7, and the following EM knowledge base:
rain âˆ¨ hail : 0.5 Â± 0.1;
rain âˆ§ hail : 0.3 Â± 0.1;
wind : 0.2 Â± 0.
Given this setup, we have, for instance:
candP gm({rain, hail, wind}, I) =

n

	 
	o
umbrella , Â¬umbrella .

Intuitively, this means that, since the world where rain, hail, and wind are all
true can be assigned a non-zero probability by the EM, we must choose either
umbrella or Â¬umbrella in order to recover consistency.

We now show a series of intermediate results that lead up to the representation theorem (Theorem 1). First, we show how this set plays a role in showing a
necessary and sufficient requirement for Inclusion and Consistency Preservation
to hold together.
Lemma 1. Given program I and input (f, af â€² ), operator â€¢ satisfies Inclusion
and Consistency Preservation iff for I â€¢ (f, af â€² ) = (Î EM , Î AM , af â€²â€² ), for all
I
(I âˆª (f, af â€² )), there exists an element X âˆˆ candP gm(w, I âˆª (f, af â€² ))
w âˆˆ WEM
s.t. {h âˆˆ Î˜ âˆª â„¦ âˆª {f } | w |= af â€²â€² (h)} âŠ† X.

Next, we investigate the role that the set candP gm plays in showing the necessary and sufficient requirement for satisfying Inclusion, Consistency Preservation, and Relevance all at once.
Lemma 2. Given program I and input (f, af â€² ), operator â€¢ satisfies Inclusion,
Consistency Preservation, and Relevance iff for I â€¢ (f, af â€² ) = (Î EM , Î AM , af â€²â€² ),
I
for all w âˆˆ WEM
(I âˆª (f, af â€² )) we have {h âˆˆ Î˜ âˆª â„¦ âˆª {f } | w |= af â€²â€² (h)} âˆˆ
candP gm(w, I âˆª (f, af â€² )).
The last of the intermediate results shows that if there is a consistent program
where two inputs cause inconsistencies to arise in the same way, then for each
world the set of candidate replacement programs (minus the added AM formula)
is the same. This result will be used as a support of the satisfaction of the first
Uniformity postulate.
Lemma 3. Let I = (Î EM , Î AM , af ) be a consistent program, (f1 , af â€²1 ), (f2 , af â€²2 )
I
I
be two inputs, and Ii = (Î EM , Î AM âˆª {fi }, af â€²i ). If WEM
(I1 ) = WEM
(I2 ), then
I
for all w âˆˆ WEM (I1 ) and all X âŠ† Î AM (w) we have that:
1. If {x | x âˆˆ X âˆª {f1 }, w |= af â€²1 (x)} is inconsistent â‡” {x | x âˆˆ X âˆª {f2 }, w |=
af â€²2 (x)} is inconsistent, then {X \ {f1 } | X âˆˆ candP gm(w, I1 )} = {X \
{f2 } | X âˆˆ candP gm(w, I2 )}.
2. If {X \ {f1 } | X âˆˆ candP gm(w, I1 )} = {X \ {f2 } | X âˆˆ candP gm(w, I2 )}
then {x | x âˆˆ X âˆª{f1 }, w |= af â€²1 (x)} is inconsistent â‡” {x | x âˆˆ X âˆª{f2 }, w |=
af â€²2 (x)} is inconsistent.
We now have the necessary tools to present the construction of our nonprioritized belief revision operator.
Construction. Before introducing the construction, we define some preliminary
notation. Let Î¦ : WEM â†’ 2[Î˜]âˆª[â„¦ ] . For each h there is a formula in Î AM âˆª {f },
where f is part of the input. Given these elements, we define:
^
newFor(h, Î¦, I, (f, af â€² )) = af â€² (h) âˆ§
Â¬f or(wi )
I (Iâˆª(f,af â€² )) | hâˆˆÎ¦(w)
wâˆˆWEM
/

The following definition then characterizes the class of operators called AFO
(annotation function-based operators).
Definition 12 (AF-based Operators). A belief revision operator â€¢ is an â€œannotation function-basedâ€ (or af-based) operator (â€¢ âˆˆ AFO) iff given program
I = (Î EM , Î AM , af ) and input (f, af â€² ), the revision is defined as I â€¢ (f, af â€² ) =
(Î EM , Î AM âˆª {f }, af â€²â€² ), where:
âˆ€h, af â€²â€² (h) = newFor(h, Î¦, I, (f, af â€² ))
where âˆ€w âˆˆ WEM , Î¦(w) âˆˆ CandP gmaf (w, I âˆª (f, af â€² )).
As the main result of the paper, we now show that satisfying a key set of
postulates is a necessary and sufficient condition for membership in AFO.

Theorem 1 (Representation Theorem). An operator â€¢ belongs to class AFO
iff it satisfies Inclusion, Vacuity, Consistency Preservation, Weak Success, Relevance, and Uniformity 1.
Proof. (Sketch) (If) By the fact that formulas associated with worlds in the set
I
WEM
(I âˆª(f, af â€² )) are considered in the change of the annotation function, Vacuity and Weak Success follow trivially. Further, Lemma 2 shows that Inclusion,
Consistency Preservation, and Relevance are satisfied while Lemma 3 shows that
Uniformity 1 is satisfied.
(Only-If) Suppose BWOC that an operator â€¢ satisfies all postulates and â€¢ âˆˆ
/
AFO. Then, one of four conditions must hold: (i) it does not satisfy Lemma 2
or (ii) it does not satisfy Lemma 3. However, by those previous arguments,
if it satisfies all postulates, these arguments must be true as well â€“ hence a
contradiction.


6

Conclusions

We have proposed an extension of the PreDeLP language that allows sentences to
be annotated with probabilistic events; such events are connected to a probabilistic model, allowing a clear separation of interests between certain and uncertain
knowledge. After presenting the language, we focused on characterizing belief
revision operations over P-PreDeLP KBs. We presented a set of postulates inspired in the ones presented for non-prioritized revision of classical belief bases,
and then proceeded to study a construction based on these postulates and prove
that the two characterizations are equivalent.
As future work, we plan to study other kinds of operators, such as more
general ones that allow the modification of the EM, as well as others that operate
at different levels of granularity. Finally, we are studying the application of PPreDeLP to real-world problems in cyber security and cyber warfare domains.
Acknowledgments. The authors are partially supported by UK EPSRC grant
EP/J008346/1 (â€œPrOQAWâ€), ERC grant 246858 (â€œDIADEMâ€), ARO project
2GDATXR042, DARPA project R.0004972.001, Consejo Nacional de Investigaciones CientÄ±Ìficas y TeÌcnicas (CONICET) and Universidad Nacional del Sur
(Argentina).
The opinions in this paper are those of the authors and do not necessarily
reflect the opinions of the funders, the U.S. Military Academy, or the U.S. Army.

References
1. Martinez, M.V., GarcÄ±Ìa, A.J., Simari, G.R.: On the use of presumptions in structured defeasible reasoning. In: Proc. of COMMA. (2012) 185â€“196
2. Hansson, S.: Semi-revision. J. of App. Non-Classical Logics 7(1-2) (1997) 151â€“175
3. AlchourroÌn, C.E., GaÌˆrdenfors, P., Makinson, D.: On the logic of theory change:
Partial meet contraction and revision functions. J. Sym. Log. 50(2) (1985) 510â€“530

4. Gardenfors, P.: Knowledge in flux: modeling the dynamics of epistemic states.
MIT Press, Cambridge, Mass. (1988)
5. Hansson, S.O.: Kernel contraction. J. Symb. Log. 59(3) (1994) 845â€“859
6. Doyle, J.: A truth maintenance system. Artif. Intell. 12(3) (1979) 231â€“272
7. Falappa, M.A., Kern-Isberner, G., Simari, G.R.: Explanations, belief revision and
defeasible reasoning. Artif. Intell. 141(1/2) (2002) 1â€“28
8. Li, H., Oren, N., Norman, T.J.: Probabilistic argumentation frameworks. In: Proc.
of TAFA. (2011) 1â€“16
9. Thimm, M.: A probabilistic semantics for abstract argumentation. In: Proc. of
ECAI 2012. (2012) 750â€“755
10. Hunter, A.: Some foundations for probabilistic abstract argumentation. In: Proc.
of COMMA 2012. (2012) 117â€“128
11. Fazzinga, B., Flesca, S., Parisi, F.: On the complexity of probabilistic abstract
argumentation. In: Proc. of IJCAI 2013. (2013)
12. Haenni, R., Kohlas, J., Lehmann, N.: Probabilistic argumentation systems.
Springer (1999)
13. ChesnÌƒevar, C.I., Simari, G.R., Alsinet, T., Godo, L.: A logic programming framework for possibilistic argumentation with vague knowledge. In: Proc. of UAI 2004.
(2004) 76â€“84
14. Hunter, A.: A probabilistic approach to modelling uncertain logical arguments.
Int. J. Approx. Reasoning 54(1) (2013) 47â€“81
15. Gottlob, G., Lukasiewicz, T., Martinez, M.V., Simari, G.I.: Query answering under
probabilistic uncertainty in Datalog+/â€“ ontologies. AMAI (2013)
16. Nilsson, N.J.: Probabilistic logic. Artif. Intell. 28(1) (1986) 71â€“87
17. Khuller, S., Martinez, M.V., Nau, D.S., Sliva, A., Simari, G.I., Subrahmanian, V.S.:
Computing most probable worlds of action probabilistic logic programs: scalable
estimation for 1030,000 worlds. AMAI 51(2-4) (2007) 295â€“331
18. Simari, G.I., Martinez, M.V., Sliva, A., Subrahmanian, V.S.: Focused most probable world computations in probabilistic logic programs. AMAI 64(2-3) (2012)
113â€“143
19. Rahwan, I., Simari, G.R.: Argumentation in Artificial Intelligence. Springer (2009)
20. GarcÄ±Ìa, A.J., Simari, G.R.: Defeasible logic programming: An argumentative approach. TPLP 4(1-2) (2004) 95â€“138
21. Lloyd, J.W.: Foundations of Logic Programming, 2nd Edition. Springer (1987)
22. Simari, G.R., Loui, R.P.: A mathematical treatment of defeasible reasoning and
its implementation. Artif. Intell. 53(2-3) (1992) 125â€“157
23. Stolzenburg, F., GarcÄ±Ìa, A., ChesnÌƒevar, C.I., Simari, G.R.: Computing Generalized
Specificity. Journal of Non-Classical Logics 13(1) (2003) 87â€“113
24. Dung, P.M.: On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artif. Intell. 77
(1995) pp. 321â€“357
25. Falappa, M.A., Kern-Isberner, G., Reis, M., Simari, G.R.: Prioritized and nonprioritized multiple change on belief bases. J. Philosophical Logic 41(1) (2012)
77â€“113

Noname manuscript No. (will be inserted by the editor)

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks
Ruocheng Guo · Elham Shaabani · Abhinav Bhatnagar · Paulo Shakarian

Received: date / Accepted: date

arXiv:1608.02646v1 [cs.SI] 8 Aug 2016

Abstract When a piece of information (microblog, photograph, video, link, etc.) starts to spread in a social network, an important question arises: will it spread to "viral" proportions ­ where "viral" can be defined as an order-ofmagnitude increase. However, several previous studies have established that cascade size and frequency are related through a power-law - which leads to a severe imbalance in this classification problem. In this paper, we devise a suite of measurements based on "structural diversity" ­ the variety of social contexts (communities) in which individuals partaking in a given cascade engage. We demonstrate these measures are able to distinguish viral from non-viral cascades, despite the severe imbalance of the data for this problem. Further, we leverage these measurements as features in a classification approach, successfully predicting microblogs that grow from 50 to 500 reposts with precision of 0.69 and recall of 0.52 for the viral class - despite this class comprising under 2% of samples. This significantly outperforms our baseline approach as well as the current state-of-the-art. We also show this approach also performs well for identifying if cascades observed for 60 minutes will grow to 500 reposts as well as demonstrate how we can tradeoff between precision and recall. Keywords Cascade Prediction · Information Diffusion · Social Network Analysis · Diffusion in Social Networks

U.S. provisional patent 62/201,517. A non-provisional patent is currently being filed. Some of the authors of this paper are supported by by AFOSR Young Investigator Program (YIP) grant FA9550-15-1-0159, ARO grant W911NF-15-1-0282, and the DoD Minerva program. Ruocheng Guo, Elham Shaabani, Abhinav Bhatnagar, Paulo Shakarian Arizona State University E-mail: {rguosni, shaabani, abhatn, shak}@asu.edu

2

Ruocheng Guo et al.

1 Introduction When a piece of information (microblog, photograph, video, link, etc.) starts to spread in a social network, an important question arises: will it spread to "viral" proportions ­ where "viral" is defined as a significant (i.e. order-ofmagnitude) increase in the number of individuals re-posting the information. However, several previous studies (Bakshy et al, 2011; Cheng et al, 2014) have established that cascade size and frequency are related through a power-law - which leads to a severe imbalance in this classification problem. In this paper, we devise a suite of measurements based on "structural diversity" that are associated with the growth of a viral cascade in a social network. Structural diversity refers to the variety of social contexts in which an individual engages and is typically instantiated (for social networks) as the number of distinct communities represented in an individual's local neighborhood (Ugander et al, 2012; Zhang et al, 2013; Shakarian et al, 2014; Li et al, 2015). Previously, Ugander et al. identified a correlation between structural diversity and influence (Ugander et al, 2012). We demonstrate these measures are able to distinguish viral from non-viral cascades, despite the severe imbalance of the data for this problem. Further, we leverage these measurements as features in a classification approach, successfully predicting microblogs that grow to 500 reposts from 50 (size-based experiments) or the first-hour observations (time-based experiments). The main contributions of the paper are as follows: ­ We develop a suite of structural diversity-based measurements that are indicative of cascade growth. ­ We are able to identify cascades of 50 reposts that grow to 500 reposts with a precision of 0.69 and recall of 0.52 for the viral class (200 out of 13,285 samples). ­ We are able to identify cascades that have advanced for 60 minutes that will reach 500 reposts with a precision of 0.65 and recall of 0.53 for the viral class (200 out of 3,444 samples). ­ We demonstrate how to trade-off between precision and recall for the above-mentioned problems. For instance, to predict cascades that reach 500 nodes, we can obtain precision of 0.78 or recall of 0.71 at the expense of the other. ­ We demonstrate that our approach is stable for alternative definitions of "viral" (i.e. microblogs that grow to sizes above or below 500 reposts). We note that our results on the prediction of cascades rely solely upon the use of our structural diversity based measures for features and limited temporal features - hence the prediction is based on network topology alone (no content information was utilized). We also achieved these results while maintaining the imbalance of the dataset - where we leave the ratio of 'viral' and 'non-viral' samples as it is. This differs from some previous studies (i.e. (Jenders et al, 2013)) which balance the data before conducting classification experiments. Further, we note that we obtained prediction of order-of-magnitude increases in the size of the cascade - which also differs from other work (i.e. (Cheng

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

3

et al, 2014)) which focus on identifying cascades that double in size. The remainder of the paper is organized as follows. In Section 2 we introduce our notation and describe the dataset used in this paper. This is followed by an introduction of our structural diversity measurements for cascades in Section 3. Then we describe our experimental results where we examined both the behavior of these measurements and the performance of classifiers built using these measurements in Section 5. Finally, we discuss related work in Section 6.

2 Technical Preliminaries Here we introduce necessary notation and describe our social network data. We represent a social network as G = (V, E ) where V is the set of nodes and E as a set of directed edges with sizes |V |, |E | respectively. The intuition behind edge (v, v ) is that it is possible that v repost a microblog from v since v did this previously. This intuition stems from how we create the edges in our network: (v, v ) is an edge if v reposted from v once or more during a specified time period (for our experiments, May 1 to July 31, 2011). We also assume a partition over nodes that specifies a community structure. We assume that such a partition is static (based on the same time period from which the edges were derived) and that the partition C consists of k communities: {C1 , C2 , ..., Ck }, each is a set of nodes. There are many possible methods to derive the communities (if user-reported communities are not available) - for instance: the Louvain algorithm (Blondel et al, 2008), Infomap (Rosvall and Bergstrom, 2008), Smart Local Move (SLM) (Waltman and van Eck, 2013) and Label Propagation (Raghavan et al, 2007). Previous work such as (Weng et al, 2014; Grabowicz et al, 2012) showed the effectiveness of communities detected by these algorithms for different applications. In this paper, We utilize the Louvain algorithm, Infomap algorithm and SLM algorithm to identify communities in the social network G due to their scalability for large social network. For these algorithms, the number of communities is not an argument as input but rather produced as part of the output of these algorithms. Note that we require C to be a partition over nodes - hence we disallow for overlapping communities. This is consistent with the community structure derivations from previous, related work (Ugander et al, 2012; Zhang et al, 2013; Shakarian et al, 2014; Li et al, 2015) which also required a partition over nodes such as strongly connected components. As such, we leave the study of structural diversity in the case of overlapping communities to future efforts. Cascades. A cascade  = (U, R) consists of all nodes (U ) who posted or reposted a certain original microblog and the reposting relationships between them, treated as edges (R). Naturally, any cascade is a subgraph of the social network G. In order to predict the final size, snapshots of a cascade can be taken by different time since adoption of the seed adopter (denoted by t). Then a snapshot of cascade  introduces a subset t = (Ut , Rt ) of  . We refer to

4

Ruocheng Guo et al.

Ut as adopters. Moreover, we also call the out-neighbors of adopters in G but not among the adopters as exposed users and denote them as NG (Ut ). For each node v  NG (Ut ), we define the adopters who exposes the cascade to v as its exposers. For convenience, we also define function uea : v  u to return the earliest adopter u among exposers of v  NG (Ut ). For size-based experiments, the time t for taking snapshot of a cascade is decided by a given cascade size m. We use t(m) to denote the smallest t such that |Ut |= m is true for a certain cascade. Accordingly, to get the corresponding order number n of an adopter u  Ut , we define function Index : u  n where n  [1, |Ut |]. To maintain a unique order of reposts, a very small random number is added to each t(n) for all integers n  [1, |Ut (m)|]. We have not found this to be a significant issue in this dataset. For convenience and simplicity, we use t to stand for both t(m) in size-based and t in time-based experiments later. For a given snapshot t = (Ut , Rt ), then we want to divide the set NG (Ut ) into two sets, namely recently exposed users (Ft ) and past exposed users (Nt ). Intuitively, this division is done based on how long it is since v  NG (Ut ) is true (when it is possible for v to make a repost) till the snapshot t is taken. Formally, given a node v  NG (Ut ), we decide whether it is a recently or past exposed user: texpose (v ) = t - t(Index(uea (v ))) (1) As defined before, t(n) denotes the earliest time t when |Ut |= n is true. Then the value of texpose (v ) is the number of time periods since the earliest adoption among its exposers till when the snapshot of cascade is taken. A positive constant  is set as a threshold on texpose (v ) (we will discuss how this constant is set in the last paragraph of 2), the recently exposed users and past exposed users are defined as follows: Ft = {v  NG (Ut ) s.t. texpose (v )  } Nt = NG (Ut ) \ Ft (2) (3)

Sina Weibo Dataset. The dataset we used was provided by WISE 2012 Challenge1 . It included a sample of microblogs posted on Sina Weibo2 from 2009 to 2012. In this dataset, we are provided with time and user information for each microblog and subsequent repost which enabled us to derive a corpus of cascades. For every repost in this dataset, the reposting relationship is provided as uid: v tab v which indicates this message is a reposted from user v by v . From this data, we derived our social network G = (V, E ) that was created from microblogs (including original posts and reposts) published during May 1, 2011 to July 31, 2011 (the 3-month period). For this network, the number of active nodes in August (the time period we studied for cascade prediction) is 5,910,608, while 5,664,625 of them at least have one out-neighbor. During
1 2

http://www.wise2012.cs.ucy.ac.cy/challenge.html http://weibo.com

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

5

Table 1: Properties of the Social Network and Cascades
Network Properties Vertices (Nodes) Edges Average degree Average clustering coefficient Connected components Number Average Number Average Number Average Number Number Number Average of communities (Louvain) size of communities (Louvain) of communities (Infomap) size of communities (Infomap) of communities (SLM) size of communities (SLM) of cascades of viral cascades of active nodes in cascades time to become viral Value 17,996,803 52,472,547 5.83 0.107 4974 379,416 47.5 39,922 450.799 380,854 47.3 Value 2,113,405 208 5,910,608 18 (h)

Cascade Properties

the month of August, there were 22,182,704 microblogs. Of these, 9,323,294 are reposts. 2,252,368 different of original posts succeeded to make at least one user repost, while 1,920,763 (86.6%) of them were written by authors who at least published one microblog during the 3-month period mentioned before. For this dataset, although different from a power-law noted previoulsly in (Bakshy et al, 2011; Cheng et al, 2014), the histogram of final cascade size (see Figure 1a) still shows that only quite few cascades went 'viral'. Therefore, we could demonstrate that this dataset is more representative of cascade behavior observed in real world than work like (Jenders et al, 2013) which conducted biased sampling to artificially provide balanced classes. We select the threshold constant  as 30 minutes since vast majority of all the reposts in May-July, 2011 occurred within 30 minutes since adoption of the seed adopter (see Figure 1b). To justify this selection, knowing that  is a threshold on texpose (v ) which is upper bounded by t, the proportion of exposed users became adopter with texpose (v )  30(min) should be more than that of those did the repost with t  30(min). This implies why it is necessary to distinguish recently and past exposed users due to the significant difference in probability to adopt. In Figure 1c, we show distribution of how long it takes for viral cascades to reach 500 nodes - note that the average value here is approximately 18 hours (which is significantly greater than what we study in our time-based classification problem).

6

Ruocheng Guo et al.

107 106 105 104 103 102 101 100 0

Frequency

200 400 600 800 1000 1200 1400 Final Size of Cascades

(a) The histogram of cascade size for August, 2011

10 0 cumulative probability 10 -1 10 -2 10 -3
-4 1010 0 10 1 10 4 10 5 10 2 10 3 time since the adoption of the seed adopter (minutes)

(b) CDF of adoption time since adoption of the seed adopter (minutes) for May-July, 2011

10 2 Frequency

10 1

10 0 0 200 400 600 800 1000 1200 1400 Time taken to become viral (>500) in minutes
(c) Histogram of time (minutes) 'viral' cascades took to reach size of 500.

Fig. 1: Network Dataset Statistics

3 Structural Diversity Measurements in Real Information Cascades Found by (Ugander et al, 2012), an individual is more likely to be infected by a `social contagion' if his/her `infected' in-neighbors are distributed over more connected components of social network users. For example, as shown

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

7

in Figure 2, although the man on the left has more infected in-neighbors, the woman on the right is more likely to be infected by the social contagion. As in-neighbors of her are showing higher structural diversity (from two communities). Translated into the terminologies introduced in this paper, they showed that an exposed user is more likely to become an adopter with exposers of high structural diversity. If this effect is aggregated over all the exposed users of a cascade, the significance to measure the relationship between structural diversity of adopters would be revealed. Moreover, we also extend our experiments to measure that of exposed users. Instead of connected components, we consider structural diversity described by communities. In this section we introduce a suite of various structural diversity measurements. We study these measurements as cascades progress in Section 4 and then leverage them as features for our classification problem in Section 5. We introduce these measurements as follows. Number of communities. For a given set of node S  {Ut , Ft , Nt } we can retrieve the associated communities C (S ) by the partition of the social network C (G). Formally: C (S ) = {Ci  C (V ) s.t. S  Ci = } where C (V ) is the partition of the social network G, introduced in Section 2. We measure the number of communities represented by |C (S )| for S  {Ut , Ft , Nt }. Gini impurity. For S  {Ut , Ft , Nt } in a cascade t , the gini impurity IG (S ) proposed by (Breiman et al, 1984) for splitting samples in decision tree, intuitively, is a scalar describing how much the distribution of nodes in S over communities in C (S ) differs from the uniform distribution. Here the uniS| form distribution stands for the situation where |Ci |= |C|( S )| for all Ci  C (S ). To show the extreme values, IG (S )  1 means the nodes are uniformly distributed over a large quantity of communities while IG (S )  0 implies most of the nodes in S are from the few 'dominant' communities. Formally, we define gini impurity as follows: IG (S ) = 1 - (
Ci C (S )

|Ci | 2 ) |S |

(4)

We study the gini impurity IG (S ) for S  {Ut , Ft , Nt } for each cascade. We note that the impurity of the adopter set IG (Ut ) behaves similar to the entropy of this set (a measurement introduced in (Weng et al, 2014)). However, as we will see in the next two sections, we found that the impurity of the recently exposed users is a more discriminating feature. Overlap. For {Sa , Sb }  {Ut , Ft , Nt }, the overlap (O(Sa , Sb )) is simply the number of shared communities between the sets of nodes Sa and Sb . Formally: O(Sa , Sb ) = |C (Sa )  C (Sb )| (5)

8

Ruocheng Guo et al.

Fig. 2: An example for structural diversity: Although the man on the left has more infected in-neighbors, the woman on the right is more likely to be infected by the social contagion. As in-neighbors of her are showing higher structural diversity (from two communities).

The intuition behind overlap stems directly from the original structural diversity results of the related work (Ugander et al, 2012) - for instance a large overlap value O(Ut , Ft ) is likely to indicate that the local neighborhoods of many of the recently exposed users will exhibit high structural diversity hence increasing the probability to become adopters in the future. Baseline measures. In addition to the aforementioned structural diversity measurements, we also examine two baseline measurements dealing with time and size. Average time to adoption. The average time to adoption for adopters in the m 1 cascade snapshot of size m: m i=1 t(m). Number of nodes. The cardinality of adopters, recently and past exposed users |Ut |,|Ft |,|Nt |.

4 Structural Diversity Measurement Study Here we examine the behavior of the various structural diversity measurements as viral and non-viral cascades progress. In this section, we define a cascade as viral if the number of reposts eventually reaches a threshold (denoted T H ) of 500 (in the next section we will explore various values for T H ). Only the distributions of feature values computed based on Louvain algorithm are exhibited in this section as it provides best results in both size-based and time-based classification tasks (See Section 5). All the measurements are computed by cascade

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

9

Table 2: Number of samples analyzed in different stages
m 10 30 50 100 200 t (min) 40 60 100 150 300 Samples 98,832 26,733 13,285 4,722 1,324 Samples 2,234 3,444 5,767 8,349 15,350 Viral Samples (%) 0.2% 0.7% 1.5% 4.2% 15% Viral Samples (%) 7% 5% 3% 2% 1%

snapshots with five populations of nodes with m = {10, 30, 50, 100, 200} (or t(m) accordingly) and five values of time since adoption of the seed adopter with t = {40, 60, 100, 150, 300}. Table 2 shows the number of samples our analysis covers in both classes for each value of m and t. For each time t we perform analysis on measurements for those cascade snapshots with no less than 5 adopters at the time so that the enough information can be provided from Ut ,Ft and Nt for the prediction task. For each size m, we consider the cascades with |Ut |= m adopters at the corresponding time t(m), t(m) can vary for different cascades. Hence, cascades with final size less than m are ignored in our analysis. This leads to that the number of non-viral cascades decreases as m increases. We examined a total of 24 measurements discussed in the previous section (12 for size-based and 12 for time-based analysis, listed as Am and At respectively in Table 3). For each measurement, for each m and t describing the diffusion process, we attempted to identify statistically significant difference between viral and non-viral classes. For this, we performed KS tests for each pair of measurements. In every test, p  10-13 , so the null hypothesis is rejected for all cases, which means each pair of the distributions are significantly different. We choose KS test over T test and Chi-square test as it is sensitive to both the location and shape of the distribution as well as it does not require each distribution to cover all possible values of the other. As notations of the box plots in the following subsections, A and M denotes mean and median for each box plot respectively.

4.1 Size Progression Average time to adoption. As a baseline measurement, we study the average time to adoption for each m of the cascade process (Figure 3). As expected, viral cascades exhibit shorter average time since adoption of the seed adopter till each later adoption. While we note that significant differences are present - especially in the early stages of the cascade, the whiskers of the non-viral class indicate a significant proportion of non-viral cascades that exhibit rapid

10

Ruocheng Guo et al.

adoption. We believe this is likely due to the fact that certain cascades may have very high appeal to specialized communities. Number of communities. Figure 4 displays how the number of communities |C (S )| increases over m = {10, 30, 50, 100, 200} for the sets S = {Ut , Ft }. We note that |C (Ut )| (the communities represented in the set of adopters) was shown to be a useful feature in (Weng et al, 2014) for tasks where the target class had fewer reposts than in this study. Here, we note that while statistically significant differences exist, the average and median values at each of the examined stages are generally similar. On the other hand, the communities represented by the set of rencently exposed users (Ft ) shows viral cascades have stronger capability to keep set of rencently exposed users with many communities than non-viral ones. We also noted that the median of |C (Nt )| shows viral cascades start with smaller |C (Nt )|. However, it increases faster in viral cascades as nodes in rencently exposed users become past exposed users (not pictured) as m increases. Gini impurity. Cascades in both classes tend to accumulate diversity in the process of collecting more adopters - and we have also noted that a related entropy measure (studied in (Weng et al, 2014)) performed similarly. We also observed that viral cascades can show larger gini impurity in recently exposed users measured by IG (Ft ) in early stages (m = {10, 30, 50}). However, perhaps most striking, non-viral cascades gain more uniformly distributed nodes over communities in non-adopters, shown by IG (Nt ) (Figure 5). We believe that this is due to non-viral cascades likely have an appeal limited to a relatively small number of communities - hence those not adopting the trend may represent a distribution of nodes over communities which is more different from a uniform distribution. Overlap. We found that overlap grows with the number of adopters in the three types of overlap considered. For O(Ut , Ft ), viral cascades start with a larger initial value and keep leading non-viral ones in the diffusion process of first 200 nodes (Figure 6). We consider that viral cascades also take advantage of the densely linked communities to help them become viral. However, in the case of O(Ut , Nt ) and O(Ft , Nt ), viral cascades begin with lower value but grow much faster than non-viral cascades. 4.2 Time Progression Number of adopters. As a baseline measurement, we study the number of adopters at regular time intervals and, as expected, found a clear difference between the two classes. Figure 7 shows how |Ut | changes over 40, 60, 100, 150 and 300 minutes. Although there is an obvious difference in early stages (40-60 minutes) between the two distributions, we will see in the next section that this alone does not provide adequate performance for our prediction task (see Section 5).

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

11

1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 10

M: 865 853 804 754 765 A: 780 790 770 753 759

Number of Adopters

30

50 100 200

1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 10

M: 15.3 49.7 78.4 168 301 A: 40.9 86.9 129 215 347

Average Time(103 )

Average Time(103 )

Number of Adopters

30

50 100 200

(a) Non-viral cascades

(b) Viral cascades

Fig. 3: Average time (minutes in 103 ) since adoption of the seed adopter to each later adoption
M: 8.0 18.0 25.0 35.0 48.0 A: 7.7 17.5 24.0 34.9 47.6 70 60 50 40 30 20 10 0 10 30 50 100 200 M: 8.0 17.0 23.0 34.0 46.5 A: 8.1 17.3 23.5 34.0 46.5 70 60 50 40 30 20 10 0 10 30 50 100 200

Number of Communities

Number of Adopters

Number of Communities Number of Communities(102 )

Number of Adopters

(a) Number of communities amongst (b) Number of communities amongst adopters (|C (Ut )|) for non-viral cascades adopters (|C (Ut )|) for viral cascades

Number of Communities(102 )

M: 7.0 15.0 20.0 27.0 33.0 A: 25.7 39.6 53.2 88.5 111 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 10 30 50 100 200

Number of Adopters

M: 21.0 30.0 30.0 33.5 42.5 A: 24.3 41.7 44.4 78.7 88.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 10 30 50 100 200

Number of Adopters

(c) Number of communities amongst recently (d) Number of communities amongst recently exposed users (|C (Ft )|) for non-viral cascades exposed users (|C (Ft )|) for viral cascades

Fig. 4: Number of communities for m = {10, 30, 50, 100, 200}

Number of communities. Figure 8 shows how |C (S )| for S  {Ut , Ft , Nt } changes over time. The value of |C (S )| increases over time for Ut and Nt but decreases for Ft . Here, the differences are somewhat more pronounced than for the size-progression measurements (compare with Figure 4). Viral cascades are more likely to have more communities in any one of Ut , Ft , Nt than non-viral ones. For adopters and non-adopters, |C (Ut )| and |C (Nt )| value of viral cascades increases faster than that of non-viral ones over time. While for recently exposed users, |C (Ft )| of non-viral cascades decreases more than viral ones in the same amount of time.

12

Ruocheng Guo et al.

1.0 0.8 0.6 0.4 0.2 0.0 10

M: 0.7 0.8 0.9 0.9 0.9 A: 0.7 0.8 0.8 0.8 0.9

Number of Adopters

30

50 100 200

1.0 0.8 0.6 0.4 0.2 0.0 10

M: 0.9 0.9 0.9 0.9 0.9 A: 0.8 0.9 0.9 0.9 0.9

Gini

Gini

Number of Adopters

30

50 100 200

(a) Gini impurity of recently exposed users (b) Gini impurity of recently exposed users (IG (Ft )) for non-viral cascades (IG (Ft )) for viral cascades

M: 0.8 0.9 0.9 0.9 0.9 A: 0.8 0.9 0.9 0.9 0.9 1.0 0.8 0.6 0.4 0.2 0.0 10 30 50 100 200

Number of Adopters

M: 0.0 0.9 0.9 0.9 0.9 A: 0.4 0.8 0.9 0.9 0.9 1.0 0.8 0.6 0.4 0.2 0.0 10 30 50 100 200

Gini

Gini

Number of Adopters

(c) Gini impurity of past exposed users (d) Gini impurity of past exposed users (IG (Nt )) for non-viral cascades (IG (Nt )) for viral cascades

Fig. 5: Gini impurity for m = {10, 30, 50, 100, 200} Gini impurity. It takes less than  = 30 minutes for a considerable portion of viral cascades to reach size m = 30. This explains the difference between size-based and time-based gini impurity values in initial-stage cascades (compare Figure 5 and Figure 9). In terms of size-based gini impurity of the nonadopters (IG (Nt )), the values of viral cascades are smaller than those of nonviral cascades when m is small. However, when t is small, larger gini impurity (IG (Nt )) amongst non-adopters are shown in viral cascades. Furthermore, as m increases, although no significant difference is shown by the median and average of IG (Nt ), Figure 5 shows non-viral cascades are more likely to have a value smaller than the lower whisker to become outliers. Overlap. By definition, overlap is the number of shared communities between two sets of nodes. We found that overlap O(Ut , Ft ), O(Ut , Nt ) and O(Ut , Nt ) manifest obvious difference between viral and non-viral cascades by values and trend over time. For instance, in Figure 10, we see growth of O(Ut , Ft ) for the viral cascades compared to the non-viral class. In fact, over time, this value decreases for non-viral cascades as the set of recently exposed users fades away for non-viral cascades with time.

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

13

60 50 40 30 20 10 0 10

M: 3.0 9.0 12.0 19.0 26.0 A: 3.7 8.7 12.3 18.5 25.2

Number of Adopters

30

50 100 200

60 50 40 30 20 10 0 10

M: 7.0 13.0 17.0 22.5 31.0 A: 6.7 12.7 16.5 22.2 29.5

Overlap

Overlap

Number of Adopters

30

50 100 200

(a) Overlap of adopters and recently exposed (b) Overlap of adopters and recently exposed users (O(Ut , Ft )) for non-viral cascades users (O(Ut , Ft )) for viral cascades

M: 6.0 16.0 23.0 34.0 46.0 A: 5.9 15.6 22.3 33.3 46.1 70 60 50 40 30 20 10 0 10 30 50 100 200

Number of Adopters

M: 0.0 14.0 22.0 32.0 45.0 A: 2.4 13.3 21.1 32.2 45.0 70 60 50 40 30 20 10 0 10 30 50 100 200

Overlap

Overlap Overlap

Number of Adopters

(c) Overlap of adopters and past exposed (d) Overlap of adopters and past exposed users (O(Ut , Nt )) for non-viral cascades users (O(Ut , Nt )) for viral cascades

M: 3.0 11.0 16.0 23.0 30.0 A: 4.5 12.1 17.1 24.8 31.9 70 60 50 40 30 20 10 0 10 30 50 100 200

Number of Adopters

M: 0.0 14.5 21.5 28.0 38.0 A: 2.5 16.0 23.5 30.0 38.1 70 60 50 40 30 20 10 0 10 30 50 100 200

Overlap

Number of Adopters

(e) Overlap of recently and past exposed (f) Overlap of recently and past exposed users (O(Ft , Nt )) for non-viral cascades users (O(Ft , Nt )) for viral cascades

Fig. 6: Overlap for m = {10, 30, 50, 100, 200}
M: 7.0 7.0 8.0 9.0 A: 9.5 10.4 11.8 13.1 3.0 2.5 2.0 1.5 1.0 0.5 0.0 40 60 100 150 9.0 15.3 M: 16.0 22.0 34.0 50.0 A: 22.2 31.4 49.7 71.9 3.0 2.5 2.0 1.5 1.0 0.5 0.0 40 60 100 150 109 139

Number of Nodes(102 )

Time Since the Original Post

300

Number of Nodes(102 )

Time Since the Original Post

300

(a) Number of adopters (|Ut |) for non-viral (b) Number of adopters (|Ut |) for viral cascascades cades

Fig. 7: Number of adopters for t  {40, 60, 100, 150, 300} (min)

14

Ruocheng Guo et al.

Number of Communities

80 70 60 50 40 30 20 10 0 40

Time Since the Original Post

60 100 150

300

Number of Communities

M: 6.0 6.0 6.0 7.0 A: 7.0 7.4 8.0 8.6

7.0 9.3

M: 12.0 14.0 19.0 24.0 A: 12.7 15.7 20.3 24.9 80 70 60 50 40 30 20 10 0 40 60 100 150

34.0 35.2

Time Since the Original Post

300

(a) Number of communities amongst (b) Number of communities amongst adopters (|C (Ut )|) for non-viral cascades adopters (|C (Ut )|) for viral cascades

Number of Communities

Time Since the Original Post

300

Number of Communities

M: 15.0 11.0 7.0 3.0 A: 79.8 61.0 49.3 35.8 100 80 60 40 20 0 40 60 100 150

0.0 15.2

M: 29.5 28.5 27.0 26.0 A: 66.7 44.5 57.4 46.3 100 80 60 40 20 0 40 60 100 150

25.0 38.2

Time Since the Original Post

300

(c) Number of communities amongst recently (d) Number of communities amongst recently exposed users (|C (Ft )|), non-viral cascades exposed users (|C (Ft )|) for viral cascades

Number of Communities(102 )

2.0 1.5 1.0

Number of Communities(102 )

M: 4.0 11.0 17.0 20.0 A: 22.1 43.5 73.4 105

23.0 140

M: 14.0 27.0 48.0 61.5 A: 22.1 63.1 95.1 146 2.0

92.5 261

1.5 1.0

0.5

0.5

0.0 40

Time Since the Original Post

60 100 150

300

0.0 40

Time Since the Original Post

60 100 150

300

(e) Number of communities amongst past ex- (f) Number of communities amongst past exposed users (|C (Nt )|) for non-viral cascades posed users (|C (Nt )|) for viral cascades

Fig. 8: Number of communities for t = {40, 60, 100, 150, 300} (min)

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

15

1.0 0.8 0.6 0.4 0.2 0.0 40

M: 0.8 0.8 0.7 0.5 A: 0.8 0.7 0.6 0.4

0.0 0.3

Time Since the Original Post

60 100 150

300

1.0 0.8 0.6 0.4 0.2 0.0 40

M: 0.9 0.9 0.9 0.9 A: 0.9 0.8 0.8 0.8

0.9 0.8

Gini

Gini

Time Since the Original Post

60 100 150

300

(a) Gini impurity of recently exposed users (b) Gini impurity of recently exposed users (IG (Ft )) for non-viral cascades (IG (Ft )) for viral cascades

M: 0.6 0.8 0.8 0.9 A: 0.5 0.7 0.8 0.8 1.0 0.8 0.6 0.4 0.2 0.0 40 60 100 150

0.9 0.8

Time Since the Original Post

300

M: 0.8 0.9 0.9 0.9 A: 0.7 0.8 0.9 0.9 1.0 0.8 0.6 0.4 0.2 0.0 40 60 100 150

0.9 0.9

Gini

Gini

Time Since the Original Post

300

(c) Gini impurity of past exposed users (d) Gini impurity of past exposed users (IG (Nt )) for non-viral cascades (IG (Nt )) for viral cascades

Fig. 9: Gini impurity for t = {40, 60, 100, 150, 300} (min)
M: 5.0 4.0 3.0 2.0 A: 5.7 4.8 3.8 2.9 60 50 40 30 20 10 0 40 60 100 150 0.0 1.9 M: 10.0 11.0 13.0 14.0 A: 11.5 12.6 14.4 15.8 60 50 40 30 20 10 0 40 60 100 150 18.0 19.2

Overlap

Time Since the Original Post

300

Overlap

Time Since the Original Post

300

(a) Overlap of adopters and recently exposed (b) Overlap of adopters and recently exposed users (O(Ut , Ft )) for non-viral cascades users (O(Ut , Ft )) for viral cascades

Fig. 10: Overlap for t = {40, 60, 100, 150, 300} (min)

16

Ruocheng Guo et al.

5 Classification Experiments Here we examine our experiments for predicting whether a cascade becomes viral - when the number of adopters exceeds a size threshold (T H = 500) given that either the cascade has 50 adopters (m = 50) or has progressed for an hour (t = 60). We shall refer to these as size-based and time-based prediction problems. Based on the distribution of final size of cascades in this dataset (see Figure 1a), as shown in Table 2, this binary classification task deals with two heavily imbalanced classes. Hence, we report performance measurements (precision, recall and F1 score) for only the minority (viral) class. Throughout the course of our experiments, we found that varying threshold (slightly modifying the definition of "viral") for only the training set allows for a trade-off between precision and recall. We study the trend of performance metrics in two cases: ­ The threshold for test set is maintained as T Hts = 500 while the training threshold is varied T Htr  {300, 400, 500, 600, 700}. ­ The two thresholds are kept as the same T H while we modify this value T H  {300, 400, 500, 600, 700}. Table 3 shows the groups of features used in our prediction tasks. The features introduced in this paper are groups Am (size-based) and At (timebased). We compare our features (Group Am , At ) with the community features extracted in (Weng et al, 2014) (Group Bm ,Bt ) and nodal features of the seed adopter (Group Cm and Ct ). Here nodal measures of the seed adopter refer to k-shell number, out-degree, in-degree, pagerank and eigenvector, which are computed based on the social network G. In previous work (Pei et al, 2014), kshell number of the seed adopter node is shown to be correlated to the average size of cascades. However, cascades from the seed adopter nodes with the same k-shell number can end up with quite different size (Shakarian et al, 2015). As baseline methods, average time to adoption (group Dm ) is applied to the sizebased experiment while cascade size at time t (group Dt ) is evaluated for timebased prediction. We extracted each group of community-based features (Am , At , Bm , Bt ) with all the three community detection algorithms mentioned in Section 2: Louvain, Infomap and SLM. Therefore, for both size-based and time-based prediction, there are 8 groups of features. Among them, Bm and Bt were the best performing feature set in the paper (Weng et al, 2014) for a comparable task.3 Additionally, we study the average size of correctly classified viral cascades and the other viral samples using features in groups Am and At . We also investigate the significance and performance of individual and certain combinations of features introduced in this paper.
3 This was their highest-performing set of features for predicting cascades that grew from 50 to 367 and 100 to 417 reposts. We also included the baseline feature in this set as we found it improved the effectiveness of this approach.

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

17

Table 3: Features for prediction tasks (size-based and time-based): Am , Bm , At and Bt are computed based on three community detection algorithms (Louvain, Infomap and SLM)
Name Am Feature(s) over size |C (Ft )|,|C (Nt )|, IG (Ut ),IG (Ft ),IG (Nt ), O(Ut , Ft ),O(Ut , Nt ),O(Ft , Nt ), m 1 |Ft |,|Nt |, m i=1 t(i) where t stands for t(m) with m  {30, 50} Community Features Mentioned in (Weng et al, 2014) m 1 and m i=1 t(i), m = 50 Nodal
1 m 1 m m i=1 m i=1

Name At

Feature(s) over time |C (Ft )|,|C (Nt )|, IG (Ut ),IG (Ft ),IG (Nt ), O(Ut , Ft ),O(Ut , Nt ),O(Ft , Nt ), |Ut |,|Ft |,|Nt | for time t  {40, 60}(min) Community Features Mentioned in (Weng et al, 2014) and |Ut |, t = 60(min) Nodal Features and |Ut |, t = 60(min) |Ut |, t = 60(min)

Bm

Bt

Cm Dm

Features t(i), m = 50 t(i), m = 50

and

Ct Dt

5.1 Cascade Prediction Results We split cascades into training set and testing set using ten-fold cross-validation. All classification experiments are repeated for 10 times to ensure the results do not take any advantage of randomness in picking training and testing sets. First we carried out the prediction tasks with fixed thresholds for both training and testing T Htr = 500, T Hts = 500. Then we modify the training threshold T Htr  {300, 400, 500, 600, 700} to show how this achieves a tradeoff between precision and recall. The difference in average final size between correctly classified viral cascades and incorrectly classified ones is also monitored over T Htr  {300, 400, 500, 600, 700} to show the potential to predict exact number of adopters by features in Am and At . Furthermore, we modify threshold of both training and testing sets T H  {300, 400, 500, 600, 700} to show the robustness of our features on related classification problems. We used the oversampling method SMOTE (Chawla et al, 2002) with random forest classifier to generate synthetic samples for the viral class. Other, lesser-performing classifiers were also examined (including SVM, MLP, and other ensemble methods) and are not reported here. All results shown in this section is a sample mean produced by repeated experiments (10 times) under each combination of variables. Error bars represent one standard deviation. Size-based prediction. We studied cascades of size 50 that reached 500 for this task. There are 13,285 cascades that can reach the size m = 50 while only 200 out of them reached the size of 500. Maintaining the threshold T H = 500, Figure 11 shows random forest classifier trained with features in group Am can outperform the other groups with any of the three community detection algorithms. The tradeoff between precision and recall can be achieved by changing the training threshold T Htr while maintaining the testing thresh-

18

Ruocheng Guo et al.

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Precision

Classification Results: TH = 500

Louvain Am Infomap Am SLM Am Louvain Bm Infomap Bm SLM Bm Cm Dm

Recall

F1 Score

Fig. 11: Classification results based on groups of features (Am ,Bm ,Cm ,Dm ) extracted with three community detection algorithms (Louvain, Infomap and SLM) when m = 50 for fixed T Htr = 500, T Hts = 500. Error bars represent one standard deviation.

old T Hts = 500 (see Figure 12). We also note that the average final size of viral cascades correctly classified by the classifier increases with the training threshold. With threshold T H  {300, 400, 500, 600, 700} on both training and testing samples, the features introduced in this paper (Am ) consistently outperform those previously introduced (Bm ) ­ see Figure 13. The fact that features in Bm are not able to maintain their predictability over different T H can be explained as that they only count the number of users on recently exposed users instead of taking the community structure of them or the decay of probability to repost over time into consideration. As shown in Figure 12a, 12c and 12e, while the trends relating to this tradeoff are similar among the various community detection algorithms, the Louvain algorithm led to superior performance for precision and F1. Infomap and SLM generally outperformed Louvain in terms of recall for both feature sets. We also note that our features outperform those of Weng et al. regardless of the testing/training thresholds and the selected community finding algorithm. Time-based prediction. As shown in Table 2, there are 3,444 cascades in our dataset reached the size of m = 5 within 60 (min) with only 5% from the minority class. When the threshold is kept as T H = 500 for both training set and testing set, we obtain the results shown in Figure 14 again showing that the features introduced in this paper (At ) outperform the other feature sets in terms of recall, precision and F1 score, no matter which community detection algorithm is used. By modifying threshold for training samples only, two phenomenon are discovered. First, a tradeoff between precision and recall can be manipulated by controlling the training threshold (T Htr ). This is shown in Figure 15a, 15c, 15e. Second, as shown in Figure 15b, 15d, 15f, with T Htr increasing, the average final size of correctly classified viral cascades also grows. Furthermore, we modify the threshold for training and testing sets together

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

19

1.2 1.0 0.8 0.6 0.4 0.2

precision recall

f1 score

1.4 1.3 1.2 1.1 1.0 0.9 0.8 0.7

correct incorrect mean

Size (103 )
300 400 500 600 700 Training Threshold

300 400 500 600 700 Training Threshold

(b) Average final size of viral cascades (cor(a) Precision, recall, and F1 score for different rectly classified, mean and incorrectly classitraining thresholds, using Louvain algorithm. fied), using Louvain algorithm.

1.2 1.0 0.8 0.6 0.4 0.2

1.2
precision recall f1 score

1.1 Size (103 ) 1.0 0.9 0.8 0.7

correct incorrect mean

300 400 500 600 700 Training Threshold

300 400 500 600 700 Training Threshold

(d) Average final size of viral cascades (cor(c) Precision, recall, and F1 score for different rectly classified, mean and incorrectly classitraining thresholds, using Infomap algorithm. fied), using Infomap algorithm.

1.2 1.0 0.8 0.6 0.4 0.2

1.2
precision recall f1 score

1.1 Size (103 ) 1.0 0.9 0.8 0.7

correct incorrect mean

300 400 500 600 700 Training Threshold

300 400 500 600 700 Training Threshold

(f) Average final size of viral cascades (cor(e) Precision, recall, and F1 score for different rectly classified, mean and incorrectly classitraining thresholds, using SLM algorithm. fied), using SLM algorithm.

Fig. 12: Prediction results when T Htr = {300, 400, 500, 600, 700} for Am (Louvain, Infomap and SLM). Error bars represent one standard deviation.

20

Ruocheng Guo et al.

0.8 0.7 0.6 0.5 0.4 0.3 precision f1 score 0.2 recall 0.1 300 400 500 600 700 Training/Testing Threshold

0.8 precision f1 score recall 0.7 0.6 0.5 0.4 0.3 0.2 0.1 300 400 500 600 700 Training/Testing Threshold

(a) Classification results for features in group (b) Classification results for features in group Am (Louvain) Bm (Louvain)

0.8 precision f1 score recall 0.7 0.6 0.5 0.4 0.3 0.2 0.1 300 400 500 600 700 Training/Testing Threshold

0.8 precision f1 score recall 0.7 0.6 0.5 0.4 0.3 0.2 0.1 300 400 500 600 700 Training/Testing Threshold

(c) Classification results for features in group (d) Classification results for features in group Am (Infomap) Bm (Infomap)

0.8 precision f1 score recall 0.7 0.6 0.5 0.4 0.3 0.2 0.1 300 400 500 600 700 Training/Testing Threshold

0.8 precision f1 score recall 0.7 0.6 0.5 0.4 0.3 0.2 0.1 300 400 500 600 700 Training/Testing Threshold

(e) Classification results for features in group (f) Classification results for features in group Am (SLM) Bm (SLM)

Fig. 13: Prediction results based on groups of features extracted for m = 50 when T H = {300, 400, 500, 600, 700}. Error bars represent one standard deviation.

to show the reliability of features in group At is better than ones in Bt (See Figure 16). Here, we noted similar trends with regard to both feature sets and community finding algorithms as found in the size-based tests.

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

21

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Precision

Classification Results: TH = 500

Louvain At Infomap At SLM At Louvain Bt Infomap Bt SLM Bt Ct Dt

Recall

F1 Score

Fig. 14: Classification results based on groups of features (At ,Bt ,Ct ,Dt ) extracted with three community detection algorithms (Louvain, Infomap and SLM) when t = 60 for fixed T Htr = 500, T Hts = 500. Error bars represent one standard deviation.

5.2 Feature Investigation

Here we investigate the importance of each feature in Am (Louvain) and Am (Louvain) as communities detected by Louvain algorithm achieves the best classification results out of the three. With T Htr = 500 and T Hts = 500, we trained 200 randomized logistic regressions models (100 for Am and 100 for At ) - with each assigning weights to the features in those sets. We then categorized the features with weight larger than 0.01 (on average) into groups such as overlap, gini impurity, etc. Then, we performed classification on the basis of single feature group or combination of such groups. The average weights assigned are shown in Table 4 while classification results (by random forest with SMOTE) are depicted in Figure 17 for groups and combinations of them. As shown, overlaps can make significant contribution to the prediction tasks. Intuitively, communication between two sets of nodes is more likely to happen in their shared communities - which is consistent with the results of (Ugander et al, 2012). This implies that the larger overlap value, the more likely one set would repost from the otherFor example, we can infer that viral cascades tend to have larger O(Ut , Ft ) value therefore adopters in them have larger chance to motivate the recently exposed users to repost than non-viral cascades. Figure 6 and Figure 10 provide evidence of this phenomenon.

22

Ruocheng Guo et al.

1.2 1.0 0.8 0.6 0.4 0.2

precision recall

f1 score

1.4 1.3 1.2 1.1 1.0 0.9 0.8 0.7

correct incorrect mean

Size (103 )
300 400 500 600 700 Training Threshold

300 400 500 600 700 Training Threshold

(b) Average final size of viral cascades (cor(a) Precision, recall, and F1 score for different rectly classified, mean and incorrectly classitraining thresholds, using Louvain algorithm. fied), using Louvain algorithm.

1.2 1.0 0.8 0.6 0.4 0.2

1.2
precision recall f1 score

1.1 Size (103 ) 1.0 0.9 0.8 0.7

correct incorrect mean

300 400 500 600 700 Training Threshold

300 400 500 600 700 Training Threshold

(d) Average final size of viral cascades (cor(c) Precision, recall, and F1 score for different rectly classified, mean and incorrectly classitraining thresholds, using Infomap algorithm. fied), using Infomap algorithm.

1.2 1.0 0.8 0.6 0.4 0.2

1.2
precision recall f1 score

1.1 Size (103 ) 1.0 0.9 0.8 0.7

correct incorrect mean

300 400 500 600 700 Training Threshold

300 400 500 600 700 Training Threshold

(f) Average final size of viral cascades (cor(e) Precision, recall, and F1 score for different rectly classified, mean and incorrectly classitraining thresholds, using SLM algorithm. fied), using SLM algorithm.

Fig. 15: Prediction results when T Htr  {300, 400, 500, 600, 700} for At (Louvain, Infomap and SLM). Error bars represent one standard deviation.

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

23

0.8 0.7 0.6 0.5 0.4 0.3 precision f1 score 0.2 recall 0.1 300 400 500 600 700 Training/Testing Threshold

0.8 precision f1 score recall 0.7 0.6 0.5 0.4 0.3 0.2 0.1 300 400 500 600 700 Training/Testing Threshold

(a) Classification results for features in group (b) Classification results for features in group At (Louvain) Bt (Louvain)

0.8 precision f1 score recall 0.7 0.6 0.5 0.4 0.3 0.2 0.1 300 400 500 600 700 Training/Testing Threshold

0.8 precision f1 score recall 0.7 0.6 0.5 0.4 0.3 0.2 0.1 300 400 500 600 700 Training/Testing Threshold

(c) Classification results for features in group (d) Classification results for features in group At (Infomap) Bt (Infomap)

0.8 precision f1 score recall 0.7 0.6 0.5 0.4 0.3 0.2 0.1 300 400 500 600 700 Training/Testing Threshold

0.8 precision f1 score recall 0.7 0.6 0.5 0.4 0.3 0.2 0.1 300 400 500 600 700 Training/Testing Threshold

(e) Classification results for features in group (f) Classification results for features in group At (SLM) Bt (SLM)

Fig. 16: Prediction results based on groups of features extracted for t = 60(min) for T H  {300, 400, 500, 600, 700}. Error bars represent one standard deviation.
0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Precision
ol at gini ol+at gini+at ol+gini ol+at+gini

Recall

F1 Score

0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Precision

ol cs gini ol+cs

gini+cs ol+gini ol+cs+gini

Recall

F1 Score

(a) Classification results for subsets of Am (Louvain): ol means overlap, gini means gini impurity, at represents average time to 50 1 adoption ( 50 i=1 t(i)).

(b) Classification results for subsets of At (Louvain): ol means overlap, gini means gini impurity, cs represents number of adopters (|U60 |).

Fig. 17: Classification results (random forest with SMOTE) based on subsets of features from Am and At (by Louvain algorithm)

24

Ruocheng Guo et al.

Group Name

Features(Am (Louvain)) IG (Ft(50) )

Weights 0.020 0.021 0.521 0.503 0.037 0.227 0.500 0.257

Features(At (Louvain)) IG (U60 ) IG (U40 ) IG (F40 ) O(U60 , F60 ) O(U60 , N60 ) O(F60 , N60 ) O(U40 , F40 ) O(U40 , N40 ) O(F40 , N40 )

Weights 0.039 0.049 0.331 0.500 0.538 0.409 0.628 0.509 0.288 0.072

Gini Impurity

IG (Nt(50) ) IG (Nt(30) ) O(Ut(30) , Ft(30) ) O(Ut(30) , Nt(30) )

Overlap

O(Ft(30) , Nt(30) ) O(Ut(50) , Ft(50) ) O(Ft(50) , Nt(50) )
1 50 50 i=1

Baseline

t(i)

1.0

|U60 |

Table 4: Weights of features assigned by randomized logistic regression models

6 Related Work Early works about popularity prediction with data driven approach simplified the problem of cascade prediction as modeling one step information propagation Galuba et al (2010); Bian et al (2014); Zhang et al (2013) or as predicting the near term popularity Gupta et al (2012). As the real pioneer of cascade prediction, the work (Bakshy et al, 2011) devised a regression model for this task and was one of the first attempts to explore this problem. They noted that the severe imbalance of the data due to a power-law relationship between cascade size and frequency (which we also observed) hindered the creation of useful model - they obtained an R2 value of only 0.3 for their regression model. The later work of (Jenders et al, 2013) also studies the problem, again taking a machine learning approach and identify several useful features to obtain relatively high precision and recall. However, in their evaluation, they artificially balance the dataset - they ensure that each fold had equal amounts of viral and non-viral tweets. The work of (Cheng et al, 2014) predicts "viral" cascades with high precision and recall, but defines "viral" as cascades that can double in size (which also has the effect of balancing the classes in the dataset). The very recent work of (Weng et al, 2014) also looks at predicting viral cascades and does leverage some community-based features, some of which are also inspired by structural diversity - though their structural diversity features are more limited than in this study - we perform a comparison with their structural diversity method (see previous section). In a nutshell, there are two main points differing our work from the ones mentioned in this section: (1). the method proposed by this paper does not need the content of microblogs or the underlying topology based on friendship relationships (2). this method is able to provide a reliable performance in prediction of order-of-

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

25

magnitude increase of cascade size. In a conference version of this paper (Guo et al, 2015) we described the basics of tis approach. However that work did not include time-based results, examination of various underlying community finding algorithms and how each sub-group of features performs in independent classification experiments. In addition to the work on cascades, there is much related work on structural diversity. This concept was first studied in (Ugander et al, 2012) and later explored in the work of (Zhang et al, 2013; Shakarian et al, 2014; Li et al, 2015; Bao et al, 2013a,b; Huang et al, 2013). However, these papers leverage structural diversity for a variety of other social network applications including the creation of new diffusion models, the study of peer influence, identifying influential nodes, and ranking communities. Finally, we note that the popular work on diffusion in the areas of computer science (Kempe et al, 2003), physics (Gallos et al, 2010), and biology (Lieberman et al, 2005) have led to a ground swell of research on this topic over the past decade, please see (Shakarian et al, 2015) for a review of major results.

7 Conclusion In this paper, we explored the effect of structural diversity on a diffusion process which allowed us to predict viral cascades. Moving forward, we look to integrate our structural-diversity approach with content information (which we believe will further increase performance) as well as study how to best operationalize this method in a system to detect viral cascades in near-real time.

8 Acknowledgment Some of the authors of this paper are supported by by AFOSR Young Investigator Program (YIP) grant FA9550-15-1-0159, ARO grant W911NF-15-10282, and the DoD Minerva program. Portions this work were also disclosed in U.S. provisional patent 62/201,517. A non-provisional patent is currently being filed.

References Alstott J, Bullmore E, Plenz D (2014) powerlaw: a python package for analysis of heavy-tailed distributions Bakshy E, Hofman JM, Mason WA, Watts DJ (2011) Everyone's an Influencer: Quantifying Influence on Twitter. In: Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, ACM, New York, NY, USA, WSDM '11, pp 65­74, DOI 10.1145/1935826.1935845, URL http://dx.doi.org/10.1145/1935826.1935845

26

Ruocheng Guo et al.

Bao P, Shen HW, Chen W, Cheng XQ (2013a) Cumulative effect in information diffusion: empirical study on a microblogging network. PloS one 8(10):e76,027 Bao Q, Cheung WK, Zhang Y (2013b) Incorporating structural diversity of neighbors in a diffusion model for social networks. In: Web Intelligence (WI) and Intelligent Agent Technologies (IAT), 2013 IEEE/WIC/ACM International Joint Conferences on, IEEE, vol 1, pp 431­438 Bian J, Yang Y, Chua TS (2014) Predicting trending messages and diffusion participants in microblogging network. In: Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval, ACM, pp 537­546 Blondel VD, Guillaume JL, Lambiotte R, Lefebvre E (2008) Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment 2008(10):P10,008 Breiman L, Friedman J, Stone CJ, Olshen RA (1984) Classification and regression trees. CRC press Chawla NV, Bowyer KW, Hall LO, Kegelmeyer WP (2002) Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research 16(1):321­357 Cheng J, Adamic L, Dow PA, Kleinberg JM, Leskovec J (2014) Can cascades be predicted? In: Proceedings of the 23rd international conference on World wide web, International World Wide Web Conferences Steering Committee, pp 925­936 Gallos L, Havlin S, Kitsak M, Liljeros F, Makse H, Muchnik L, Stanley H (2010) Identification of influential spreaders in complex networks. Nature Physics 6(11):888­893 Galuba W, Aberer K, Chakraborty D, Despotovic Z, Kellerer W (2010) Outtweeting the twitterers-predicting information cascades in microblogs. In: Proceedings of the 3rd conference on Online social networks, vol 39, p 3^ aAS3 Grabowicz PA, Ramasco JJ, Moro E, Pujol JM, Eguiluz VM, et al (2012) Social features of online networks: The strength of intermediary ties in online social media. PloS one 7(1):e29,358 Guo R, Shaabani E, Bhatnagar A, Shakarian P (2015) Toward order-ofmagnitude cascade prediction. In: Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015, ACM, pp 1610­1613 Gupta M, Gao J, Zhai C, Han J (2012) Predicting future popularity trend of events in microblogging platforms. Proceedings of the American Society for Information Science and Technology 49(1):1­10 Huang X, Cheng H, Li RH, Qin L, Yu JX (2013) Top-k structural diversity search in large networks. Proceedings of the VLDB Endowment 6(13):1618­ 1629 Jenders M, Kasneci G, Naumann F (2013) Analyzing and predicting viral tweets. In: Proceedings of the 22Nd International Conference on World Wide Web Companion, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, WWW '13 Compan-

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

27

ion, pp 657­664, URL http://dl.acm.org/citation.cfm?id=2487788.2488017 Kempe D, Kleinberg J, Tardos E (2003) Maximizing the spread of influence through a social network. In: Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, New York, NY, USA, KDD '03, pp 137­146, DOI 10.1145/956750.956769, URL http://doi.acm.org/10.1145/956750.956769 Li RH, Qin L, Yu JX, Mao R (2015) Influential community search in large networks. Proceedings of the VLDB Endowment 8(5) Lieberman E, Hauert C, Nowak MA (2005) Evolutionary dynamics on graphs. Nature 433(7023):312­316, DOI 10.1038/nature03204 Pei S, Muchnik L, Andrade Jr JS, Zheng Z, Makse HA (2014) Searching for superspreaders of information in real-world social media. Scientific reports 4 Raghavan UN, Albert R, Kumara S (2007) Near linear time algorithm to detect community structures in large-scale networks. Physical Review E 76(3):036,106 Rosvall M, Bergstrom CT (2008) Maps of random walks on complex networks reveal community structure. Proceedings of the National Academy of Sciences 105(4):1118­1123 Shakarian P, Gerdes L, Lei H (2014) Circle-based tipping cascades in social networks. In: WSDM Workshop on Diffusion Networks and Cascade Analytics Shakarian P, Bhatnagar A, Aleali A, Guo R, Shaabani E (2015) Diffusion in Social Networks. Springer (in press) Ugander J, Backstrom L, Marlow C, Kleinberg J (2012) Structural diversity in social contagion. Proceedings of the National Academy of Sciences 109(16):5962­5966 Waltman L, van Eck NJ (2013) A smart local moving algorithm for large-scale modularity-based community detection. The European Physical Journal B 86(11):1­14 Weng L, Menczer F, Ahn YY (2014) Predicting successful memes using network and community structure. In: Eighth International AAAI Conference on Weblogs and Social Media Zhang J, Liu B, Tang J, Chen T, Li J (2013) Social influence locality for modeling retweeting behaviors. In: Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, AAAI Press, pp 2761­2767

Product Offerings in Malicious Hacker Markets
Ericsson Marin, Ahmad Diab and Paulo Shakarian

arXiv:1607.07903v1 [cs.CR] 26 Jul 2016

Arizona State University
Tempe, Arizona
{ericsson.marin, ahmad.diab, shak}@asu.edu

Abstractâ€”Marketplaces specializing in malicious hacking
products - including malware and exploits - have recently become
more prominent on the darkweb and deepweb. We scrape 17
such sites and collect information about such products in a
unified database schema. Using a combination of manual labeling
and unsupervised clustering, we examine a corpus of products
in order to understand their various categories and how they
become specialized with respect to vendor and marketplace. This
initial study presents how we effectively employed unsupervised
techniques to this data as well as the types of insights we gained
on various categories of malicious hacking products.

not trivial, including effort to answer questions, solve puzzles,
mathematical equation or CAPTCHA.
Most related work on darkweb markets such as [3] focus
on a single market and do not restrict their study to malicious
hacking products. Our previous work on markets [4] focused
on a game theoretic analysis of a small subset of the data in
this paper - and did not attempt to categorize the products for
sale. Additionally, there is a complementary lines of work on
malicious hacking forums (i.e. [5], [6], [7], [8], [9], [10], [11],
[12]) - which is a related but different topic from this paper.

I. I NTRODUCTION
Websites on the deepweb and darkweb specializing in
the sale of malicious hacking products - such as malware
platforms, software exploits and botnet rental - have become
the venue of choice for online purchase of these items by cyber
criminals. In this paper, we leverage unsupervised learning
to categorize and study the product offerings of 17 of these
online markets. Specifically, we describe how we used manual labeling combined with clustering techniques to identify
product categories (Section II), and then we analyze the results
both quantitatively and qualitatively (Section III). We identify
categories of products that are highly specialized with respect
to particular vendors and markets. We also highlight other
interesting facets of this ecosystem - for instance, vendors
who habitually cross-list products on multiple sites and nearly
identical products for sale by multiple vendors.
Background and Related Work. The darkweb refers to the
anonymous communication provided by crypto-network tools
such as â€The Onion Routerâ€ (Tor), which is free software
dedicated to protect the privacy of its users by obscuring traffic
analysis as a form of network surveillance [1]. On the other
hand, the deepweb refers to sites not indexed by common
search engines due to a variety of reasons (e.g. password
protections), that not necessarily rely on additional protocols.
The sites on the darkweb and deepweb explored in this
study comprise marketplaces [2]. In these websites, vendors
advertise and sell their goods and services relating to malicious
hacking, drugs, pornography, weapons and software services.
Products are most often verified before any funds are released
to the seller. The main engine of these environments is trust.
If a seller is misleading or fails to deliver the appropriate item,
he is banned from the site. Similarly, buyers can be banned
for not complying with the transaction rules. Basically, all
marketplaces in darkweb require a registration and a valid
account to get access. Sometimes, this registration process is

II. M ALICIOUS H ACKING P RODUCT C ATEGORIZATION
In this section, we describe our malicious hacker product
dataset and our use of clustering to identify malicious hacker
product categories. We examined 17 malicious hacker marketplaces crawled over a 6 month period. The crawled information
was then parsed and stored in a relational database. Relevant
tables record the marketplaces themselves, the vendors of the
various products and the items/products for sale. Each item
is associated with a vendor and a marketplace, allowing for
join queries. Some of the more relevant fields for marketplace
items include the price, title, description, rating, posting date.
In this work, we primarily extract features from the product
title/name to generate features.
We note that many items are cross-posted and are nearly
identical. We show the distribution of vendors who use the
same screen-name across multiple marketplaces in Fig. 1(a).
To clean our product data, we identify duplicate (cross-posted)
products and report on the size of our dataset in Table I.
As we collect data from a variety of different sites, there is
inconsistency as to how products are categorized on each site
- if such non-trivial categorization even exists for a given site.
In addition, there is a clear absence of a standardized method
for vendors to register their products. As a consequence, the
great majority of products are unique when compared with
simple matching or regular expression technique. It is valid
even in the case where a pair of vendors with different screen
names post what a human would determine to be the same
product. Fig. 1(b) shows the distribution of products and
number of vendors sharing each product. The distribution
follows a power-law. Note that about 57% of products are
unique by simple comparison methods.
Clustering approach. Using product names, we engineer
features that represent each product as a vector. A set of
pilot experiments suggests that word and character n-grams

TABLE I
S CRAPED DATA FROM M ARKETPLACES IN DARKWEB .
Marketplaces
Products (Total)
Products (Distinct)
Vendors
60

17
16122
9093
1332

Number of Shared Vendors

10000

50
1000

Number of Products

40

100

30
20
10

10

1

0
0

2

4

6

8

10 12 14 16 18

0 2 4 6 8 10 12 14 16 18 20 22 24

Markets

Number of Shared Vendors

(a)

(b)

Fig. 1. Distribution of (a) Shared Vendors over Markets. (b) Products over
Shared Vendors.

Table II shows the performance of each TF-IDF vectorization using Rand-index and entropy, when K-means starts
with the 34 fixed centroids. For Rand-index, character ngrams in the range from 3 to 4, 3 to 5, and 3 to 6, when Kmeans used cosine similarity reached a high best performance
(0.986). In addition, we also found the best entropy (0.067)
when K-means uses the same specification. This way, K-means
configuration with character n-grams in the range from 3 to
6 for vectorization, cosine similarity for distance function and
the 34 points for the starting centroids was our natural choice
to produce the clusters in the entire dataset.
We also examined the performance of our approach using
random centroids. As expected, it performs worse than using
the centroids derived from products. Additionally, we examined products (from the full dataset) with a cosine similarity
of less than 0.1 from the calculated centroids. There were 410
such distinct products (4.51% of the dataset). These were then
manually examined and most were found to be irrelevant to
our target domain - and we did not consider them further.
III. A NALYST I NTERPRETATION OF P RODUCT C LUSTERS

would provide more pure clusters compared with other feature
engineering methods, such as meta-data or domain-specific
keywords. These features were valued using standard term
frequency - inverse document frequency (TF-IDF), after the
elimination of stopping words and the execution of steaming.
We evaluated word n-gram feature vectors of length up to 1
and up to 2 words and many character n-gram features in the
ranges from 3 to 7 and from 4 to 7. This gave us 10 different
feature vectors in all. To verify which of these strategies could
reach the best performance in our dataset, we evaluated the
effect of the different types of feature vectors on the accuracy
and purity of clusters produced by the K-means algorithm.
To determine the best feature vector, we manually labeled
500 samples using 34 labeled groups (listed in Table III). We
used 400 of the samples to determine centroids for each of
the 34 groups, and then we evaluated the resulting clustering
on the remaining 100 samples. We examined the accuracy
of the different approaches when compared to ground truth
using the Rand-index method [13]. This method is defined as
the number of pairs correctly considered in the sameclass or
correctly considered in different classes divided by n2 , where
n is the number of samples. In addition, we used standard
entropy measurements to examine the purity of the clusters.
Entropy measures the amount of disorder in a cluster. A zerovalue for this metric means the clusters are formed by just one
class. The formal definition is as follows:
entropy(Di ) = âˆ’

k
X

P ri (cj ) log2 P ri (cj ),

(1)

i=1

where P ri (cj ) is the proportion of class cj data points in
cluster i or Di . The total entropy (considering all clusters) is:
entropytotal (D) =

k
X
|Di |
i=1

|D|

x entropy(Di )

(2)

In this section, we examine the results of clustering based
on character n-grams in the range 3 to 6 using initial centroids
determined from the labeled data. In order to analyze the
information of these clusters, we calculated their entropy with
respect to two different criteria: marketplaces and vendors.
We also checked in the database the number of distinct
marketplaces and vendors inside each cluster. The idea was
to understand the diversity of the clusters regarding these
two facets. A low marketplace entropy for a given cluster
would mean its products were mainly found in a particular
marketplace. Similarly, low vendor entropy would mean the
clusterâ€™s products were mainly sold by a particular vendor.
Table III presents the results.
As shown in Table III, Links holds the lowest entropy when
we analyzed the marketplaces, suggesting the great majority
of products come from the same market. In this cluster, 80%
of products came from only 2 markets. However, when we
check the vendor entropy for this same cluster, we can observe
a higher value, suggesting that many vendors are actually
offering products related to Links. It is possible that many
markets discourage the re-selling of lists of links, as much of
this information can be found on darkweb Wikiâ€™s for free.
Similarly, Hacking Tools holds the lowest entropy for the
vendor criteria. This suggest that only a few vendors are
present in that cluster. Specifically, only 2 vendors author
416(50%) of this type of products. At first glance, this may be
surprising as this appears to be a very general group. However,
upon inspection of the contents, we find that many authors of
these products are actually organizations. These organizations
use similar language in their product description in an effort to
brand their wares. This could indicate the presence of hackingcollectives that author products as well as the limitations of our
text-based approach - which can potentially cluster products
branded in a similar fashion. We also note one of the most

TABLE II
K- MEANS E VALUATION (F IXED C ENTROIDS ).

Cosine
Euclidean

word(1,1)
0.986
0.986

word(1,2)
0.985
0.977

char(3,4)
0.986
0.976

char(3,5)
0.986
0.973

Cosine
Euclidean

0.075
0.224

0.079
0.110

0.067
0.153

0.067
0.156

Rand-index
char(3,6)
char(3,7)
0.986
0.985
0.973
0.974
Entropy
0.067
0.075
0.156
0.141

TABLE III
C LUSTERS E NTROPY.
Rank
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34

Cluster Name
Carding
PayPal-related
Cashing Credit Cards
PGP
Netflix-related
Hacking Tools - General
Dumps - General
Linux-related
Email Hacking Tools
Network Security Tools
Ebay-related
Amazon-related
Bitcoin
Links (Lists)
Banking
Point of Sale
VPN
Botnet
Hacking Groups Invitation
RATs
Browser-related
Physical Layer Hacking
Password Cracking
Smartphone - General
Wireless Hacking
Phishing
Exploit Kits
Viruses/Counter AntiVirus
Network Layer Hacking
RDP Servers
Android-related
Keyloggers
Windows-related
Facebook-related

No of
Products
1263
1103
867
865
846
825
749
561
547
539
472
456
443
422
384
375
272
257
251
249
249
237
230
223
222
218
218
210
205
191
156
143
119
119

No of
Markets
16
16
16
15
14
15
12
16
13
15
15
16
15
12
13
15
12
12
14
15
12
13
13
14
13
13
14
14
14
12
11
13
12
15

Market
Entropy
0.320
0.340
0.351
0.347
0.270
0.331
0.289
0.372
0.335
0.366
0.385
0.391
0.360
0.211
0.349
0.384
0.413
0.291
0.387
0.453
0.380
0.408
0.434
0.408
0.389
0.403
0.413
0.413
0.459
0.405
0.429
0.496
0.464
0.501

No of
Vendors
315
335
256
203
351
132
280
117
196
117
163
197
201
221
186
181
130
110
143
99
134
122
100
110
56
111
91
60
60
124
60
77
50
67

Vendor
Entropy
0.720
0.754
0.738
0.696
0.805
0.516
0.777
0.758
0.738
0.621
0.772
0.825
0.823
0.838
0.840
0.841
0.827
0.796
0.865
0.797
0.857
0.856
0.781
0.816
0.601
0.849
0.795
0.684
0.716
0.895
0.770
0.862
0.717
0.876

prominent vendor in this cluster was itself a marketplace which is also reflected in the low marketplace entropy.
In our analysis of the Facebook and Keylogger clusters, we
can see that they point to the other direction. They have high
values for both entropy, a clear sign about the diversity with
respect to both vendors and markets. For example, in cluster
Facebook, there were 119 products and 67 vendors, and the
most prolific vendor for this cluster authored only 8 products.
In the same cluster, products were also spread across 15
markets - and the most well-represented market was associated
with 30 products. This analysis also indicates widespread
prevalence of keyloggers - which is not surprising as it is
a well established hacking technique. However, observing the
similar trend for the Facebook cluster could be indicative of
an increase in demand for Facebook-directed social media
hacking products and information.
Conclusion and Future Work. In this paper, we conducted
an initial examination of malware products from 17 malicious hacker markets through unsupervised learning. Using
manually-labeled data, we studied the effect of feature vector
on cluster purity using text-based features. We then analyzed
the impurity of clusters in our corpus of over 8, 000 malicious

char(4,4)
0.985
0.975

char(4,5)
0.985
0.975

char(4,6)
0.984
0.977

char(4,7)
0.982
0.971

Random
0.933
0.933

0.072
0.134

0.079
0.134

0.088
0.137

0.088
0.175

0.423
0.423

hacking products with respect to vendor and marketplace, and
finally, we identified several interesting characteristics of how
the products were grouped. Currently, we are examining other
methods for grouping these products using matrix factorization
and supervised techniques. Additionally, we are studying the
underlying social network of vendors through relationships
based on similar product offerings.
Acknowledgments. Some of the authors were supported by the
Office of Naval Research (ONR) Neptune program, the Arizona State
University Global Security Initiative (ASU GSI), and CNPq-Brazil.

R EFERENCES
[1] R. Dingledine, N. Mathewson, and P. Syverson, â€œTor: The Secondgeneration Onion Router,â€ in Proceedings of the 13th Conference on
USENIX Security Symposium - Volume 13, ser. SSYMâ€™04. Berkeley,
CA, USA: USENIX Association, 2004, pp. 21â€“21.
[2] V. Ciancaglini, M. Balduzzi, R. McArdle, and M. RoÌˆsler, â€œBelow the
Surface: Exploring the Deep Web,â€ 2015.
[3] N. Christin, â€œTraveling the silk road: A measurement analysis of a
large anonymous online marketplace,â€ in Proceedings of the 22Nd
International Conference on World Wide Web, ser. WWW â€™13. New
York, NY, USA: ACM, 2013, pp. 213â€“224.
[4] J. Robertson, V. Paliath, J. Shakarian, A. Thart, and P. Shakarian,
â€œData Driven Game Theoretic Cyber Threat Mitigation,â€ in Proc. 28th
Innovative Applications of Artificial Intelligence (IAAI-16), 2016.
[5] C. C. Yang, X. Tang, and X. Gong, â€œIdentifying dark web clusters with
temporal coherence analysis,â€ in Intelligence and Security Informatics
(ISI), 2011 IEEE International Conference on, July 2011, pp. 167â€“172.
[6] V. Benjamin, W. Li, T. Holt, and H. Chen, â€œExploring threats and
vulnerabilities in hacker web: Forums, irc and carding shops,â€ in
Intelligence and Security Informatics (ISI), 2015 IEEE International
Conference on, May 2015, pp. 85â€“90.
[7] M. Macdonald, R. Frank, J. Mei, and B. Monk, â€œIdentifying digital
threats in a hacker web forum,â€ in Proceedings of the 2015 IEEE/ACM
International Conference on Advances in Social Networks Analysis and
Mining 2015, ser. ASONAM â€™15. New York, NY, USA: ACM, 2015,
pp. 926â€“933.
[8] Z. Zhao, G.-J. Ahn, H. Hu, and D. Mahi, â€œSocialImpact: Systematic
Analysis of Underground Social Dynamics.â€ in ESORICS, ser. Lecture
Notes in Computer Science, S. Foresti, M. Yung, and F. Martinelli, Eds.,
vol. 7459. Springer, 2012, pp. 877â€“894.
[9] H. Chen, â€œDark web: Exploring and mining the dark side of the
web,â€ in Intelligence and Security Informatics Conference (EISIC), 2011
European, Sept 2011, pp. 1â€“2.
[10] J. Shakarian, P. Shakarian, and A. Ruef, â€œCyber attacks and public embarrassment: A survey of some notable hacks,â€ Elsevier SciTechConnect,
2015.
[11] P. Shakarian and J. Shakarian, â€œSocio-cultural modeling for cyber threat
actors,â€ in AAAI Workshop on Artificial Intelligence and Cyber Security
(AICS), 2016.
[12] J. Shakarian, A. Gunn, and P. Shakarian, â€œExploring malicious hacker
forums,â€ in Cyber Deception: Building the Scientific Foundation, S. Jajodia, V. Subrahmanian, V. Swarup, and C. Wang, Eds. Springer, 2016.
[13] W. M. Rand, â€œObjective Criteria for the Evaluation of Clustering
Methods,â€ Journal of the American Statistical Association, vol. 66, no.
336, pp. 846â€“850, Dec. 1971.

Early Identification of Violent Criminal Gang Members
Elham Shaabaniâˆ—, Ashkan Alealiâˆ— , and
Paulo Shakarianâ€ 
Arizona State University
Tempe, AZ

John Bertetto
Chicago Police Department
Chicago, IL

john.bertetto@chicagopolice.org

arXiv:1508.03965v1 [cs.SI] 17 Aug 2015

{shaabani, aleali, shak}@asu.edu

ABSTRACT
Gang violence is a major problem in the United States accounting for a large fraction of homicides and other violent crime. In this paper, we study the problem of early
identification of violent gang members. Our approach relies on modified centrality measures that take into account
additional data of the individuals in the social network of
co-arrestees which together with other arrest metadata provide a rich set of features for a classification algorithm. We
show our approach obtains high precision and recall (0.89
and 0.78 respectively) in the case where the entire network
is known and out-performs current approaches used by lawenforcement to the problem in the case where the network
is discovered overtime by virtue of new arrests - mimicking real-world law-enforcement operations. Operational issues are also discussed as we are preparing to leverage this
method in an operational environment.
Categories and Subject Descriptors: J.4 [Computer
Applications]: Sociology
General Terms: Security; Experimentation
Keywords: Social Network Analysis; Criminology

1.

INTRODUCTION

Gang violence is a major problem in the United States [1,
2] - accounting for 20 to 50 percent of homicides in many
major cities [10]. Yet, law enforcement actually has existing data on many of these groups. For example the underlying social network structure is often recorded by lawenforcement and has previously been shown useful in enabling â€œsmart policingâ€ tactics [17] and improving law
-enforcementâ€™s understanding of a gangâ€™s organizational structure [19]. In this paper we look to leverage this gang social
network information to create features that allows us to classify individuals as potentially violent. While the results of
such a classifier are insufficient to lead to arrests, it is able
to provide the police leads to individuals who are likely to
be involved in violence, allowing for a more focused policing
with respect to patrols and intelligence gathering. Our key
aim is to significantly reduce the population of potential violent gang members which will lead to more efficient policing.
In this paper, we introduce our method for identifying
potentially violent gang members that leverages features derived from the co-arestee social network of criminal gangs in
âˆ—

These authors contributed equally to this work.
U.S. Provisional Patent 62/191,086. Contact shak@asu.edu for
licensing information.
â€ 

a classifier to identify potentially violent individuals. We
note that this classification problem is particularly difficult
due to not only data imbalances, but also due to the fact that
many violent crimes are conducted due to heightened emotions - and hence difficult to identify. Though we augment
our network-based features with some additional meta-data
from the arrest records, our approach does not leverage features concerning the race, ethnicity, or gender of individuals
in the social network. We evaluate our method using realworld offender data from the Chicago Police Department.
This paper makes the following contributions:
â€¢ We discuss how centrality measurements such as degree, closeness, and betweenness when modified to account for metadata about past offenses such as the
type of offense and whether the offense was classified
as â€œviolentâ€ can serve as robust features for identifying
violent offenders.
â€¢ We show how the network features, combined with
other feature categories provide surprisingly robust performance when the entire offender is known in terms of
both precision (0.89) and recall (0.78) using cross-fold
validation.
â€¢ We then test our methods in the case where the network is exposed over time (by virtue of new arrests)
which mimics an operational situation. Though precision and recall are reduced in this case, we show that
our method significantly outperforms the baseline approach currently in use by law-enforcement - on average increasing precision and recall by more than two
and three times respectively.
In addition to these main results, we also present some side
results on the structure and nature of the police dataset we
examine. The paper is organized as follows. In Section 2 we
motivate this difficult problem within the law-enforcement
community. This is followed by a description of our dataset
along with technical notation in Section 3. There, we also
describe some interesting aspects of the gang arrest dataset
and our co-arrestee network. In Section 4 we formally define our problem, describe existing approaches, and then describe the features we use in our approach. Then we present
our results in Section 5 for both cases where we assume the
underlying network is known and when we discover the network over time (mimicking an operational scenario). Finally,
related work is discussed in Section 6.

2.

BACKGROUND

A recent study shows that the network for gunshot victimization is denser than previously believed [16]. According
to the authors, within the city of Chicago over 70% of all
gunshot victims are contained within only 6% of the total
population These findings validate what has been considered common knowledge among police for decades: who you
hang out with matters, and if you hang out with those who
engage in or are victims of violence you are more likely to
become an offender or victim yourself.
Identifying potential offenders of gun violence has also
been a staple practice for most law enforcement agencies
as an attempt to curtail future victimization. When gang
conflicts get â€œhot,â€ itâ€™s common for law enforcement agents
to put together a list of known â€œshootersâ€: those known
gang members with an existing criminal history for gun violence and a predilection for engaging in such illegal activity. Law enforcement agents then attempt to make contact
with these individuals with the expectation that such direct
contact might prevent violence. For most law enforcement
agencies, however, this practice is performed in a very adhoc manner. Identifying these individuals for intervention
has relied primarily on the ability of law enforcement agents
to remember and identify at-risk individuals. While feasible
for small or discreet networks, the ability to recall multiple
individuals in large networks that cross large geographic regions and interact with multiple networks becomes increasing difficult. This difficulty increases significantly as relationships between networks change, known individuals leave
the network, and new individuals enter it. In particular,
the practice is less than idea because it requires officers to
attempt to recall criminal history and network association
data that varies between network members. For example,
a subject who has been arrested on multiple occasions for
carrying a gun or has been arrested for shooting another individual is easy to recall, but recalling and quantifying the
risk for a subject with multiple arrests for non-gun violence
and a direct association with several offenders and victims
of gun violence can be much more difficult. In short, identifying a known â€œshooterâ€ is relatively straightforward: they
are known. The approach in this paper synthesizes network
connectivity other attributes of the subject to identify those
individuals at risk that law enforcement might not yet know.
Using this information, law enforcement agents may not
only more reliably and consistently identify those individuals
most likely to engage in acts of violence or become victims
of violence due to their personal associations with it, but
also to more effectively manage agency resources. Intervention strategies may include service providers outside law enforcement, such as family members, social service providers,
current or former educators, and clergy. This diversity in
approach not only delivers a more powerful â€œstop the violenceâ€ message but provides a kind of force multiplier for
law enforcement, increasing the number of persons involved
in the effort to prevent violence. Identifying specific individuals for intervention also allows for a more targeted effort by
law enforcement in terms of personnel and geographic areas
needing coverage. Blanketing violence reduction strategies
that saturate geographic areas with law enforcement agents
and rely on direct contact with large numbers of criminal
network members are inefficient and resource consuming.
Focusing efforts on those individuals most likely to engage in

violence allows law enforcement to focus on smaller groups
of people and smaller geographic areas (those areas within
which those individuals identified are known to frequent).
Therefore, our approach can significantly improve such efforts to identify violent individuals. In this paper, we see
how our method not only out-performs the current social
network heuristic used by police, but also that it provides
a much smaller and more precise list of potentially violent
offenders than simply listing those with a violent criminal
record.

3.

GANG CO-OFFENDER NETWORK

In this section, we introduce the necessary basic notation
to describe our co-offender network and then provide details of our real-world criminal dataset and study some of
its properties.

3.1

Technical Preliminaries

Throughout this paper we shall represent an offender network as an undirected graph G = (V, E) where the nodes
correspond with previous offenders and an undirected edge
exists between offenders if they were arrested together. We
will use Ï„ to denote the set of timepoints (dates). We also
have three sets of labels for the nodes: V, S, gang which
are the sets of violent crimes, non violent crimes, and gangs.
For each time point t and each node v, the binary variable
arrtv âˆˆ {true, false} denotes if v was arrested at time t and
distrtv , beattv , gangtv to denote the district, beat, and gang affiliation of v at time t (we will assume that time is fine-grain
enough to ensure that at each time unit an individual is arrested no more than once). If we drop the t superscript for
these three symbols, it will denote the most recent district,
beat, and gang associated with v in the knowledgebase. We
shall use the sets Vvt and Svt to denote the set of violent and
non violent offenses committed by v at time t respectively.
Note if arrtv = false then Vvt = âˆ…. We will drop the superscript t for this symbol to denote the union of labels at any
time t in the historical knowledgebase. We also note that
the edges in the graph also depend on time, but for sake of
readability, we shall state with words the duration of time
considered for the edges.
For a given violent crime c âˆˆ VâˆªS, we will use the notation
Vct = {v âˆˆ V s.t. c âˆˆ Vvt } (intuitively, the subset of the
population who have committed crime c at time t). Again,
we will drop the superscript t if v could have committed
crime c at any time in the historical knowledgebase. For
a set of labels C âŠ† V âˆª S, we will extend this notation:
VCt = {v âˆˆ V s.t. C âˆ© Vvt 6= âˆ…}. We will slightly abuse
notation here: Vâˆ…t = V . We will use similar notation for
denoting a subset of the population that are members of a
certain gang. For instance, Vgangv refers to the set of nodes
who are in the same gang as node v. Likewise, we shall use
the same notation for subgraphs: GtC is the subgraph of G
containing only nodes in VCt and their adjacent edges. We
will use the function d : V Ã— V â†’ N to denote the distance
between two nodes - which for this paper will be the number
of links in the shortest path. For a given node v, the set
Nvi = {v 0 âˆˆ V s.t. d(v, v 0 ) = i} â€“ the set of nodes that are
whose shortest path is exactly i hops from v. For two nodes
v, v 0 , we will use the notation Ïƒ(v, v 0 ) to be the number of
shortest paths between v and v 0 . For nodes u, v, v 0 , Ïƒu (v, v 0 )
will be the number of shortest paths between v and v 0 that
pass through u.

Table 1: Summary of arrest data.
Name
Value
Number of records
64466
Violent offense
4450
Homicide
312
Criminal sexual assault
153
Robbery
1959
Aggravated assault
1441
Aggravated battery
896
Non violent offense
60016

For a given subgraph G0 of G, we shall use C(G0 ) to denote the largest connected component of G0 and for node
v âˆˆ G0 , we will use the notation Cv (G0 ) to denote the connected component of G0 to which v belongs. If we apply a
community finding algorithm to subgraph G0 , we will use
the notation Pv (G0 ) to denote the partition of G0 to which
v belongs. We will use the notation |Â·| to denote the size of
a set or the number of nodes in a subgraph.

3.2

Overview of Network Data

In this section we describe our police dataset and the associated co-offender network as well as some interesting characteristics that we have noticed.
Police Dataset. Our dataset consists of gang-related arrest incidents gathered from August 2011 - August 2014 in
Chicago as well as their immediate associates. This data set
includes locations, dates, the links between the joint arrests,
and the gang affiliation of the offenders. In Table 1, we summarize some of the important characteristics of the dataset.
Violent Crimes. In our dataset, the set V consists of the
following crimes have been identified by the Chicago Police
as violent crimes: homicide (first or second degree murder),
criminal sexual assault, robbery, aggravated assault, and aggravated battery. All aforementioned offenses are also FBI
â€œindexâ€ crimes as well. A key aspect about the violent crimes
is that the dataset is highly imbalanced with much more arrests for non violent crimes vs. arrests for violent crimes
(60016 vs. 4450).
Network Properties. From the arrest data, we were able
to construct the co-offender network. In this network, the

Figure 1: The gang co-offender network. Each color corresponds with a different gang.
isolated vertices are eliminated due to the lack of structural
information. A visualization of the network is depicted in
Figure 1 and we have included summary statistics in Table 2.
In studying this network, we studied its degree distribution
(Figure 2). Unlike the degree distribution for other scale free
social networks, the degree distribution for the offender network is exponential rather than power law. However, despite
the degree distribution being similar to that of a random (ER) or small world network topology [27], we noticed other
characteristics that indicate differently. The co-offender network has a much higher average clustering coefficient than in
a random network and does not follow the properties of the
small world topology due to the relative high diameter and
average shortest path (computed for the largest connected
component.)

40	 Â 

Number	 Â of	 Â Ver+ces(Hundreds	 Â )	 Â 

Table 2: Network properties.
Name
Values
Vertices
9373
Edges
17197
Average degree
3.66
Average clustering
0.5
Transitivity
0.62
Connected components
1843
Largest connected component di36
ameter
Largest connected component aver12.22
age path length
Largest connected component aver0.63
age clustering

35	 Â 
30	 Â 
25	 Â 
20	 Â 
15	 Â 
10	 Â 
5	 Â 
0	 Â 
1	 Â  3	 Â  5	 Â  7	 Â  9	 Â  11	 Â 13	 Â 15	 Â 17	 Â 19	 Â 21	 Â 23	 Â 25	 Â 27	 Â 29	 Â 31	 Â 33	 Â 35	 Â 37	 Â 39	 Â 41	 Â 43	 Â 45	 Â 47	 Â 49	 Â 

Degree	 Â 

Figure 2: Network degree distribution. The exponential
function fits to the distribution (R2 = 0.77).
Repeat Offenders. There are many instances of repeated
offenses from the same offender. Figure 3 shows the distribution of the repeated arrests for each individual in the
dataset. This indicates that arrest records have utility in

6	 Â 

In this section, we describe our problem, some of the existing practical approaches used by law-enforcement, and our
approach based on supervised learning with features primarily generated by the network topology.

5	 Â 

4.1

Number	 Â of	 Â individuals(Thousands	 Â )	 Â 

identifying future offenders.

4	 Â 
3	 Â 
2	 Â 
1	 Â 

4.2

0	 Â 
2	 Â  4	 Â  6	 Â  8	 Â  10	 Â  12	 Â  14	 Â  16	 Â  18	 Â  20	 Â  22	 Â  24	 Â  26	 Â  28	 Â  31	 Â  38	 Â 

Number	 Â of	 Â arrests	 Â 

Figure 3: Repeated arrests. 12866 instances of one-time
arrests have been removed.
Seasonality of Crime. There is also a higher chance of
criminal activities in different months of the year. Figure 4
demonstrates some of these variations. As per police observations, both violent and non-violent crime incidents are
lower in the winter months (Dec.-Feb.).

Number	 Â of	 Â violent	 Â individuals	 Â 

180	 Â 
160	 Â 
140	 Â 
120	 Â 
100	 Â 
80	 Â 
60	 Â 
40	 Â 
20	 Â 
0	 Â 
Aug	 Â  Sep	 Â  Oct	 Â  Nov	 Â  Dec	 Â  Jan	 Â  Feb	 Â  Mar	 Â  Apr	 Â  May	 Â  Jun	 Â  Jul	 Â 

Number	 Â of	 Â non	 Â violent	 Â individuals	 Â 

Month	 Â 
2500	 Â 
2000	 Â 
1500	 Â 
1000	 Â 
500	 Â 
0	 Â 
Aug	 Â  Sep	 Â  Oct	 Â  Nov	 Â  Dec	 Â  Jan	 Â  Feb	 Â  Mar	 Â  Apr	 Â  May	 Â  Jun	 Â  Jul	 Â 

Month	 Â 
2011-Â­â€2012	 Â 

2012-Â­â€2013	 Â 

2013-Â­â€2014	 Â 

Figure 4: Seasonality of crime.

4.

Problem Statement

Given a co-offender network, G = (V, E) and for each
historical timepoint t âˆˆ Ï„ = {1, . . . , tmax } and v âˆˆ V , we
have the values of arrtv , distrtv , beattv and elements of the
sets Vvt , gangtv , we wish to identify set {v âˆˆ V s.t. âˆƒt >
tmax where |Vvt |> 0}. In other words, we wish to find a set
of offenders in our current co-offender network that commit
a violent crime in the future.

IDENTIFYING VIOLENT OFFENDERS

Existing Methods

Here we describe two common techniques often used by
law-enforcement to predict violent offenders. The first is
a simple heuristic based on violent activities in the past.
The second is a heuristic that was based on the findings of
[17] which was designed to locate future victims of violent
crime. Both of these approaches are ad-hoc practical approaches that have become â€œbest practicesâ€ for predicting
violent offenders. However, we are not aware of any datadriven, formal evaluation of these methods in the literature.
Past Violent Activities (PVA). The first ad-hoc approach is quite simple: if an offender has committed a violent crime in the past, we claim that he will commit a
violent crime in the future. An obvious variant of this approach is to return the set of violent offenders from the last
âˆ†t days. We note in practice, if police also have records of
those who are incarcerated, and such individuals would be
removed from the list (due to the different jurisdictions of
police and corrections in the Chicago area, we did not have
access to incarceration data - however discussed re-arrests
observed in the data in the previous section).
Two-Hop Heuristic (THH). The two-hop heuristic is
based on the result of [17] which investigated a social network of gunshot victims in Boston and found an inverse relationship between the probability of being a gunshot victim
and the shortest path distance on the network to the nearest
previous gunshot victim. Hence, THH returns all neighbors
one and two hops away from previous violent criminals (see
Algorithm 1 for details on the version we used in our experiments - which was the best-performing variant for our
data). The Chicago Police have adopted a variant of this
method to identify potential gang victims using a combination of arrest and victim data - the co-arrestee network of
criminal gang members includes many individuals who are
also victims of violent crime (this is a direct result of gang
conflict). We note that victim information did not offer a
significant improvement to our approach, except the trivial
case that a homicide victim cannot commit any crime in the
future.

4.3

Supervised Learning Approach

We evaluated many different supervised learning approaches
including Naive Bayes (NB), Linear Regression (LR), Decision Tree (DT), Random Forest (RF), Neural Network (NN),
and support vector machines (SVM) on the same set of features for the nodes in the network that we shall describe in
this section. We also explored combining these approaches

Algorithm 1 Two-Hop Heuristic
1: procedure TwoHop(G)
. Offender network G.
2:
R â† {}
. Identified violent offenders.
3:
V ICT IM S â† {u âˆˆ G|is homicide victim(u)}
4:
for v âˆˆ V ICT IM S do
5:
N â† Nv1 âˆª Nv2
. Immediate neighbors
6:
R â† R âˆª {u âˆˆ N s.t. Vu = âˆ…}
7:
return R

with techniques for imbalanced data such as SMOTE [4] and
Borderline SMOTE [9], however we do not report the results
of Borderline SMOTE as it provided no significant difference
from SMOTE. We group our features into four categories:
(1.) neighborhood-based (having to do with the immediate neighbors of a given node), (2.) network-based (features
that require the consideration of more than a nodes immediate and nearby neighbors), (3.) temporal characteristics,
and (4.) geographic characteristics.

4.3.1

Neighborhood-Based Features

Neighborhood-based features are the features computed
using each node and its first and/or second level neighbors
in G â€“ often with respect to some C âŠ† V. The simplest such
measure is the degree of vertex v â€“ corresponding to the
number of offenders arrested with v. We can easily extend
this for some set of crimes of interest (C) where we look at
all the neighbors of v who have committed a crime in C.
This generalizes degree (as that is the case where C = âˆ…).
In our experiments, we found the most useful neighborhood
features to be in the case where C = V though standard
degree (C = âˆ…) was also used. We also found that using
combinations of the following booleans based on the below
definition also proved to be useful:
majv (C, i)

= |{u|u âˆˆ (âˆªi Nvi ) âˆ© VC }|â‰¥ 0.5 Ã— |(âˆªi Nvi )|

Intuitively, majv (C, i) is true if at least half of the nodes
within a network distance of i from node v have committed
a crime in C and false otherwise. Using these intuitions, we
explored the space of variants of these neighborhood-based
features and list those we found to be best-performing in
Table 3.

4.3.2

Network-Based Features

Network-based features fall into two sub-categories that
we shall describe in this section: community-based and pathbased.
Network-based community features. We use several
notions of a nodeâ€™s community when engineering features:
the connected component to which a node belongs, the gang
to which a node belongs, and what we will refer to as an individualâ€™s group. The connected component is simply based
on the overall network structure, while the gang is simply the subgraph induced by the individuals in the network who belong to the same gang (the social network of
node vâ€™s gang is denoted Ggangv . A nodes group is defined
as the partition he/she belongs to based on a partition of
Ggangv found using the Louvain algorithm [5]. We found
in our previous work [19] and ensuing experience with the
Chicago Police that the groups produced in this method

Table 3: Neighborhood-Based Features
Description

Definition

Degree (w.r.t. C)

|{u|u âˆˆ Nv1 âˆ© VC }|

Fraction of 1-hop
neighbors committing a crime
in C

|{u|u âˆˆ Nv1 âˆ© VC }|/|Nv1 |

Fraction of 2-hop
neighbors committing a crime
in C

|{u|u âˆˆ Nv2 âˆ© VC }|/|Nv2 |

Majority of 1-hop
and 2-hop neighbors committing
a crime in C

majv (C, 1) âˆ§ majv (C, 2)

Minority of 1-hop
and majority of
2-hop neighbors
comitting a crime
in C

Â¬majv (C, 1) âˆ§ majv (C, 2)

Table 4: Network-Based Features (Community)
Description

Definition

Component
size when v is
removed

|C(Cv (G) \ {v})|

Largest component size with a
violent node after
v is removed

maxv0 âˆˆC(Cv (G){v}âˆ©VV |Xv0 |
where Xv0 = Cv0 (Cv (G){v})

Group size

|Pv (Ggangv )|

Relationships
within the group

|{(u, v)
âˆˆ
Pv (Ggangv )}|

Number of violent members in
the group

|{v 0 âˆˆ Pv (Ggangv ) s.t. Vv 6= âˆ…}|

Triangles
group

in

Transitivity
group

of

No. of triangles within subgraph Pv (Ggangv )
No. of triangles in Pv (Ggangv )
No. of â€œâˆ¨â€â€™s in Pv (Ggangv )

E s.t. u, v

âˆˆ

Group-to-group
connections

|{u âˆˆ Pv (Ggangv ) s.t. âˆƒ(u, w) âˆˆ
E where w âˆˆ
/ Pv (Ggangv )}|

Gang-to-gang
connections

|{u âˆˆ Ggangv s.t. âˆƒ(u, w) âˆˆ E
where w âˆˆ
/ Ggangv }|

were highly relevant operationally. In this work, we also
examined other community finding methods (i.e. Infomap,
and Spectral Clustering) and found we obtained the best results by using the Louvain algorithm. We provide our best
performing network-based community features that we used
in Table 4. Of particular interest, we found for individual

Table 6: Geographic Features

Table 5: Network-Based Features (Path)
Description
Betweenness
(w.r.t. C)

Name

Definition
P
Ïƒv (u,w)
u,wâˆˆVC

District
quency

Ïƒ(u,w)

P

Closeness (w.r.t.
C)

(|VC |âˆ’1)/

Shell
Number
(w.r.t. C)

shellC (v) (see appendix for further details)

Propagation
(w.r.t. C)

1 if v âˆˆ Î“Îº (VV ), 0 otherwise.
(see appendix for further details)

uâˆˆVC

d(u, v)

v that features relating to the size of the largest connected
component resulting v 0 removal of his/her connected component was useful. Another interesting pair of features we
noted for both group and gang were the number of edges
from members of that group/gang to a different group or
gang. We hypothesize that the utility of these features is a
result of conflicts between groups/gangs they are connected
to as well as the spread of violence amongst different groups
(i.e. if two groups are closely connected, one may conduct
violent activities on behalf of the other).
Network-based path features. We looked at several features that leveraged the paths in the network by adopting
three common node metrics from the literature: betweenness, closeness [6], and shell-number [23] as well as a propagation process based on a deterministic tipping model [8].
The features are listed in Table 5. We examined our modified definitions of closeness, betweenness, and shell number
where C was a single element of V, where C = V and where
C = âˆ… (which provides the standard definitions of these
measures). Our intuition was that individuals nearer in the
network to other violent individuals would also tend to be
more violent - and we found several interesting relationships
such as that for closeness (where C = VV ) discussed in section 5.1 when we run the classifier on each feature group.
Shell number and the propagation process were used to capture the idea of the spread of violence (as shell number was
previously shown to correspond with â€œspreadersâ€ in various
network epidemic models [11]). For the propagation process,
we set the threshold (Îº) equal to two, three, four, five, and
six. Further details on shell number and the propogation
process can be found in the appendix.

4.3.3

Geographic Features

Geographic features capture the information related to
the location of a crime incident. The intuition is that the
individuals who commit crimes in violent districts are more
likely to become violent than the others. We found that the
beat the individual has committed a crime in is an important feature for our problem. This is in accordance with
previous well known literature in criminology [3, 21] which
studies spatio-temporal modeling of criminal behavior. The
complete list is shown in Table 6.

4.3.4

Temporal Features

We considered couple of temporal features: average inter-

Definition
Fre-

|{(t, v 0 ) s.t. arrvt 0 = true âˆ§
0
âˆƒt0 s.t. distrvt 0 = distrvt }|

Beat Frequency

|{(t, v 0 ) s.t. arrvt 0 = true âˆ§
0
âˆƒt0 s.t. beattv0 = beattv }|

Beat Violence

|{(t, v 0 ) s.t. arrvt 0 = true âˆ§ Vvt 0 6=
0
âˆ… âˆ§ âˆƒt0 s.t. beattv0 = beattv }|

District Violence

|{(t, v 0 ) s.t. arrvt 0 = true âˆ§ Vvt 0 6=
0
âˆ… âˆ§ âˆƒt0 s.t. distrvt 0 = distrvt }|

val month and number of violent groups. Average interval
time considers the average time duration of consecutive arrests of the offender. The other feature, which we examine,
is number of violent groups appeared over time in the environment. We examined that the number of violent groups
has been an important temporal aspect for identifying the
violent criminals. The key intuition here is, if at least one
member of the offenderâ€™s groups (formed over time) is violent then we consider the offender as a part of that violent
group. For an individual v, we define the partially ordered
set tvC = {t s.t. arrtv = true âˆ§ VCt 6= âˆ…} (intuitively the set
of the time points where v has committed at least on of the
crimes in C.) We also define âˆ†vi (C) = tvi âˆ’ tviâˆ’1 for each
tvi âˆˆ tvC . Considering these definitions, we formally define
the temporal features in Table 7.
Table 7: Temporal Features
Name
Average interval
time (w.r.t. C)
Number of violent groups

Definition
P v
v
i âˆ†i (C)/|tC |
|{t s.t. arrvt = true âˆ§
âˆƒv 0 s.t. arrvt 0 = true âˆ§
Vvt 6= âˆ… âˆ§
v 0 âˆˆ Nvt }|

5.

EXPERIMENTAL RESULTS

In this section, we review the results of our experiments.
We looked at two types: experiments where the entire cooffender network is known before-hand (Section 5.1) and experiments where the network is discovered over time (Section 5.2). The intuition behind the experiments where the
co-offender network is known is that the police often have
additional information to augment co-arrestee data. This
information can include informant reporting, observed individuals interacting by patrolmen, intelligence reporting, and
information discovered on social media and the Internet. In
our second type of experiment we discover the network over
time in an effort to mimic real-world operations - however,
we also show that this makes the problem more difficult as
it reduces the power of neighborhood-based and networkbased features. Based on our discussions with the Chicago

Police, we believe that real-world results will most likely fall
somewhere between these two experiments. Operationally,
we will not have full arrest data, but the aforementioned
augmenting data sources are available (even though we did
not have access to them for our experiments).

5.1

Known Co-Offender Network

1	 Â 

1	 Â 

0.9	 Â 

0.9	 Â 

0.8	 Â 

0.8	 Â 

0.7	 Â 

0.7	 Â 

0.6	 Â 

0.6	 Â 

0.5	 Â 

0.5	 Â 

0.4	 Â 

0.4	 Â 

0.3	 Â 

0.3	 Â 

0.2	 Â 

0.2	 Â 

0.1	 Â 

In this experiment we assume that the entire offender network is known. In other words, to compute the features for
each vertex v, we assume that the set Vv is unknown while
the rest of the network is observable. In here we compared
our approach with THH but not with the PVA as we do not
utilize time. In each of the experiments described in this section, we conduct 10-fold cross validation. We consider the
result of each approach as a set of nodes that the approach
considers to be a set of potentially violent individuals. Our
primary metrics are precision (fraction of reported violent
individual who were actually violent in the dataset), recall
(fraction of violent individuals in the dataset reported by the
approach), F1 (the harmonic mean of precision and recall)
and area under the curve. We conduct two types of experiments: first, we study classification performance using only
features within a given category (neighborhood, network,
temporal, and geographic), then we study the classification
performance when the entire feature set is used but with
various different classification algorithms and compare the
result to THH.

0.1	 Â 

0	 Â 

0	 Â 
Precision	 Â 

Recall	 Â 

F1	 Â 

Precision	 Â 

(a)

Recall	 Â 

F1	 Â 

(b)

1	 Â 

1	 Â 

0.9	 Â 

0.9	 Â 

0.8	 Â 

0.8	 Â 
0.7	 Â 

0.7	 Â 

0.6	 Â 

0.6	 Â 

0.5	 Â 

0.5	 Â 

0.4	 Â 

0.4	 Â 

0.3	 Â 

0.3	 Â 

0.2	 Â 

0.2	 Â 

0.1	 Â 
0	 Â 

0.1	 Â 

Precision	 Â 

0	 Â 
Precision	 Â 

Recall	 Â 

Recall	 Â 

Violent	 Â 

F1	 Â 

(c)

F1	 Â 

Non	 Â violent	 Â 

(d)

Figure 6: Example features from each category. (a)
Neighborhood-based: Minority of 1-hop and majority of 2-hop neighbors committing a crime in C.
(b) Network-based: Closeness (w.r.t. V). (c) Geographic: Beat violence. (d) Temporal: Average
interval months.

True	 Â posi*ve	 Â 

Classification using single feature categories. Here
we describe classification results using single feature categories. In this set of experiments, we use a random forest
classifier (which we will later show provides the best performance of the classifiers that we examined). Figure 5 shows
the performance of RF for the described categories. The
network-based features are highly-correlated to violent behavior with average F1 value of 0.72 compared to 0.63 for
neighborhood, 0.21 for geographic, and 0.03 for temporal
features. In Figure 6, we show the performance of a feature
from each category to classify violent vs. non violent crimes;
the performance of each example is a good indicator of the
performance of its category.

1	 Â 
0.9	 Â 
0.8	 Â 
0.7	 Â 
0.6	 Â 
0.5	 Â 
0.4	 Â 
0.3	 Â 
0.2	 Â 
0.1	 Â 
0	 Â 
0	 Â 

0.2	 Â 

0.4	 Â 

0.6	 Â 

0.8	 Â 

1	 Â 

False	 Â posi*ve	 Â 
Network-Â­â€Based	 Â 

Neighborhood-Â­â€Based	 Â 	 Â 

Geographic	 Â 

Temporal	 Â 

All	 Â 

Figure 7: ROC curve for each feature set.

1	 Â 
0.9	 Â 
0.8	 Â 
0.7	 Â 
0.6	 Â 
0.5	 Â 
0.4	 Â 
0.3	 Â 
0.2	 Â 
0.1	 Â 
0	 Â 
Neighborhood-Â­â€ Network-Â­â€Based	 Â  Geographic	 Â 
Based	 Â 
Features	 Â 	 Â 

Precision	 Â 

Recall	 Â 

Temporal	 Â 

Table 8, RF provides the best performance (F1=0.83); we
also note that using SMOTE for RF, did not improve this
result. Figure 8 shows that our algorithm outperforms THH.
The performance of our features are also illustrated in Figure 7. The area under the curve (AUC) of applying all
features is 0.98 â€“ a higher overall accuracy. The AUC for
network-based, neighborhood-based, geographic, and temporal categories are 0.92, 0.91, 0.65, and 0.7 respectively.
This indicates the importance of network features for this
classification task.

F1	 Â 

5.2
Figure 5: Precision, recall, and F1 comparison between
each group of features.
Classification comparison. Table 8 shows the performance of different classification algorithms. According to

Co-Offender Network Emerges Over Time

In this section, we present a more difficult experiment where the co-arrestee network is discovered over time (by
virtue of arrests). To simulate this phenomenon, we split
our data into two disjoint sets: the first set for learning
and identification, and the second one for measuring the

Table 8: K-fold cross validation.
Method
Precision Recall F1
RF
0.89
0.78
0.83
RF w. SMOTE 0.86
0.78
0.82
NB
0.45
0.49
0.47
LR
0.68
0.49
0.57
DT
0.71
0.66
0.68
NN
0.64
0.57
0.6
SVM
0.73
0.2
0.31
1	 Â 
0.9	 Â 
0.8	 Â 
0.7	 Â 
0.6	 Â 

and number of false positives and display the results in Figures 10 and 11. In FRF (Filtered Random Forest) we filter the offenders who have not committed any crime in the
last 200 days. This simple heuristic increase the precision
drastically while preserving the recall. The main advantage
of our method, besides the high precision, is its ability to
significantly reduce the population of potentially violent offenders when compared to PVA - which for each month had
between 1813 and 3571 false positives. Figure 11 compares
the number of true and false positives instances for all the
approaches for each month except PVA (PVA was omitted
due to readability because of the large amount of false positives). While the F1 measure for PVA is higher than that of
the others, the large number of false positives prevents the
law enforcement from using it effectively in practice. Furthermore, as time progresses, PVA likely rises in recall due
to the drop in the number of violent criminals to predict.

0.5	 Â 
0.4	 Â 

6.

0.3	 Â 
0.2	 Â 
0.1	 Â 
0	 Â 
THH	 Â 

RF	 Â 

Precision	 Â 

Recall	 Â 

F1	 Â 

Figure 8: Performance comparison between THH and RF
in K-fold cross validation.

Frac%on	 Â of	 Â full	 Â dataset	 Â 

performance. We do monthly split and start from February
2013. To illustrate the difficulty of this test, we show the
number of nodes, edges, and violent individuals per month in
Figure 9. We note that in the early months, we are missing
much of the graphical data (over 40% of nodes and edges in
the first two months) - hence making many of our features
less effective. However, as the months progress, there are less
violent individuals to identify (due to the temporal nature of
the dataset) - hence amplifying the data imbalance as time
progresses.
1	 Â 
0.9	 Â 
0.8	 Â 
0.7	 Â 
0.6	 Â 
0.5	 Â 
0.4	 Â 
0.3	 Â 
0.2	 Â 
0.1	 Â 
0	 Â 

RELATED WORK

Though we believe that the prediction of violent offenders
using co-offender social networks is new, there has previously
been work on both co-offender networks in general as well
as crime forecasting. In this section, we briefly review some
of the relevant contributions in both of these areas.
There has been much previous work on co-offender networks. The earlier work that studied these special social
networks primarily came from the criminology literature.
For instance, [14] utilizes social network analysis techniques
to study several case studies where the social network of the
criminal organization was known. In [13], the authors study
the stability of these networks change over time. More recently, graphical features derived from networks comprised
of both offenders and victims has been shown to be related
to the the probability of an individual becoming a victim
of a violent crime [17, 16]. Previous work has also looked
at the relationship between network structure and geography [18] and has leveraged both network and geographic
features to predict criminal relationships [25] as well as influence gang members to dis-enroll [24]. There have also
been several software tools developed for conducting a widerange of analysis on co-offender networks including CrimeFighter [20], CrimeLink [22], and ORCA [19]. However, our
work departs from this is that we are looking to leverage
the network topology and other features to identify violent
offenders - which was not studied in any of the previous
work.
There has also been a large amount of work on crime forecasting (i.e. [7, 12]) though historically, this work has relied
on spatio-temporal modeling of criminal behavior [3, 21] or
was designed to identify suspects for specific crimes [26, 15].
None of this previous work was designed to identify future
violent offenders nor did it leverage social network structure.

Month	 Â 
Edges	 Â 

Nodes	 Â 

Violent	 Â individuals	 Â 

Figure 9: Number of nodes, edges, and violent individuals
over time. More training data, less offenders to identify.
In these experiments, we compared our approach using
random forests with the full feature set to THH and PVA.
We measure precision, recall, F1, number of true positives,

7.

CONCLUSION

In this paper we explored the problem of identifying repeat offenders who will commit violent crime. We showed
a strong relationship between network-based features and
whether a criminal will commit a violent offense providing
an unbiased F1 score of 0.83 in our cross-validation experiment where we assumed that the underlying network was
known. When we moved to the case where the network

90	 Â 

Number	 Â of	 Â true	 Â posi.ve	 Â 

0.4	 Â 
0.35	 Â 

Recall	 Â 

0.3	 Â 
0.25	 Â 
0.2	 Â 
0.15	 Â 
0.1	 Â 

80	 Â 
70	 Â 
60	 Â 
50	 Â 
40	 Â 
30	 Â 
20	 Â 
10	 Â 
0	 Â 

13
	 Â 
Ap
r-Â­â€1
3	 Â 
Ju
n-Â­â€
13
	 Â 
Au
g-Â­â€
13
	 Â 
Oc
t-Â­â€1
3	 Â 
De
c-Â­â€
13
	 Â 
Fe
b-Â­â€
14
	 Â 
Ap
r-Â­â€1
4	 Â 
Ju
n-Â­â€
14
	 Â 

0.05	 Â 

13
	 Â 
Ap
r-Â­â€1
3	 Â 
Ju
n-Â­â€
13
	 Â 
Au
g-Â­â€
13
	 Â 
Oc
t-Â­â€1
3	 Â 
De
c-Â­â€
13
	 Â 
Fe
b-Â­â€
14
	 Â 
Ap
r-Â­â€1
4	 Â 
Ju
n-Â­â€
14
	 Â 

Fe
b-Â­â€

0	 Â 

Fe
b-Â­â€

Month	 Â 

Month	 Â 

FRF	 Â 

0.2	 Â 
0.15	 Â 
0.1	 Â 
0.05	 Â 

1000	 Â 
800	 Â 
600	 Â 
400	 Â 
200	 Â 

13
	 Â 

Ap
r-Â­â€1
3	 Â 
Ju
n-Â­â€
13
	 Â 
Au
g-Â­â€
13
	 Â 
Oc
t-Â­â€1
3	 Â 
De
c-Â­â€
13
	 Â 
Fe
b-Â­â€
14
	 Â 
Ap
r-Â­â€1
4	 Â 
Ju
n-Â­â€
14
	 Â 

0	 Â 

Month	 Â 

Fe
b-Â­â€

13
	 Â 
Ap
r-Â­â€1
3	 Â 
Ju
n-Â­â€
13
	 Â 
Au
g-Â­â€
13
	 Â 
Oc
t-Â­â€1
3	 Â 
De
c-Â­â€
13
	 Â 
Fe
b-Â­â€
14
	 Â 
Ap
r-Â­â€1
4	 Â 
Ju
n-Â­â€
14
	 Â 

0	 Â 

Fe
b-Â­â€

Precision	 Â 

THH	 Â 

1200	 Â 

Number	 Â of	 Â false	 Â posi/ve	 Â 

0.25	 Â 

RF	 Â 

Month	 Â 

FRF	 Â 

0.18	 Â 

RF	 Â 

THH	 Â 

Figure 11: Number of true and false positive instances.

0.16	 Â 
0.14	 Â 

F1	 Â 

0.12	 Â 

belongs. For a given node v and C âŠ† V, we define shellC (v)
as the shell number of node v on the subgraph consisting
of v and all nodes v 0 where C âˆ© Vv 6= âˆ…. We slightly abuse
notation and define shellâˆ… (v) as the shell number of v on the
full network.

0.1	 Â 
0.08	 Â 
0.06	 Â 
0.04	 Â 
0.02	 Â 

Fe
b-Â­â€

13
	 Â 
Ap
r-Â­â€1
3	 Â 
Ju
n-Â­â€
13
	 Â 
Au
g-Â­â€
13
	 Â 
Oc
t-Â­â€1
3	 Â 
De
c-Â­â€
13
	 Â 
Fe
b-Â­â€
14
	 Â 
Ap
r-Â­â€1
4	 Â 
Ju
n-Â­â€
14
	 Â 

0	 Â 

Month	 Â 
THH	 Â 

FRF	 Â 

RF	 Â 

PVA	 Â 

Figure 10: Precision, recall, and F1 over time.
was discovered over time, our method significantly outperformed baseline approaches significantly increasing precision
and recall. We are currently discussing ways to operationalize this technology with the Chicago Police as well as design
strategies to best deploy police assets to areas with higher
concentrations of potentially violent offenders. We are also
working with the police to identify other sources of data to
build a more complete social network of the offenders.

Appendix
Shell Number. For a given graph, the k-core is the largest
subgraph where each node has at least degree k. The k-shell
is the set of nodes in core k but not in any higher core. A
nodeâ€™s shell number is k value of the shell to which that node

Propogation Process. For a given node v and the set of
activated nodes V 0 , we define vâ€™s active neighbors as follows:
actv (V 0 ) = {u|u âˆˆ Nv1 âˆ© V 0 }
We now define an activation function A that, given an initial
set of active nodes, returns a set of active nodes after one
time step.
AÎº (V 0 ) = V 0 âˆª {v âˆˆ V s.t. |actv (V 0 )|â‰¥ Îº}
We also note that the activation function can be applied
iteratively, to model a diffusion process. Hence, we shall use
the following notation to signify multiple applications of A
(for natural numbers t > 1).

AÎº (V 0 )
if t = 1
t
0
AÎº (V ) =
0
AÎº (Atâˆ’1
otherwise
Îº (V ))
0
Clearly, when AtG,Îº (V 0 ) = Atâˆ’1
G,Îº (V ) the process has converged. Further, this always converges in no more than |V |
steps, since the process must activate at least one new node
in each step prior to converging. Based on this idea, we
define the function Î“ which returns the set of all nodes activated upon the convergence of the activation function. We

define Î“Îº (V 0 ) = AÎº (V 0 ) where t is the least value such that
0
AtÎº (V 0 ) = Atâˆ’1
Îº (V ).

8.

REFERENCES

[1] J. Bertetto. Countering criminal street gangs: Lessons from
the counterinsurgent battlespace. Law Enforcement Executive
Forum, 12(3):43, 2012.
[2] A. Braga, D. Hureau, and A. Papachristos. Deterring
gang-involved gun violence: Measuring the impact of bostonâ€™s
operation ceasefire on street gang behavior. Journal of
Quantitative Criminology, pages 1â€“27, 2013.
[3] P. Brantingham and P. Brantingham. Crime Pattern Theory.
In R. Wortley and L. Mazerolle, editors, Enviromental
Criminology and Crime Analysis, pages 78â€“93. 2008.
[4] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.
Kegelmeyer. Smote: synthetic minority over-sampling
technique. Journal of artificial intelligence research,
16(1):321â€“357, 2002.
[5] P. Expert, T. S. Evans, V. D. Blondel, and R. Lambiotte.
Uncovering space-independent communities in spatial
networks. Proceedings of the National Academy of Sciences,
108(19):7663â€“7668, 2011.
[6] L. C. Freeman. A set of measures of centrality based on
betweenness. Sociometry, 40(1):pp. 35â€“41, 1977.
[7] W. Gorr and R. Harries. Introduction to crime forecasting.
International Journal of Forecasting, 19(4):551 â€“ 555, 2003.
[8] M. Granovetter. Threshold models of collective behavior. The
American Journal of Sociology, (6):1420â€“1443.
[9] H. Han, W.-Y. Wang, and B.-H. Mao. Borderline-smote: a new
over-sampling method in imbalanced data sets learning. In
Advances in intelligent computing, pages 878â€“887. Springer,
2005.
[10] J. C. Howell. Gangs in Americaâ€™s Communities. Sage, 2012.
[11] M. Kitsak, L. K. Gallos, S. Havlin, F. Liljeros, L. Muchnik,
H. E. Stanley, and H. A. Makse. Identification of influential
spreaders in complex networks. Nat Phys, 6(11):888â€“893, Nov.
2010.
[12] H. Liu and D. E. Brown. Criminal incident prediction using a
point-pattern-based density model. International Journal of
Forecasting, 19(4):603â€“622, Oct. 2003.
[13] J. M. Mcgloin, C. J. Sullivan, A. R. Piquero, and S. Bacon.
Investigating the stability of co-offending and co-offenders
among a sample of youthful offenders. Criminology,
46(1):155â€“188, 2008.
[14] C. Morselli. Inside Criminal Networks. Springer, 2009.
[15] C. Overall and G. Day. The Hammer Gang: an exercise in
spatial analyis of an armed robbery series using the probability
grid method. In S. Chainey and L. Tompson, editors, Crime
Mapping Case Studies, pages 55â€“62. 2008.

[16] A. Papachristos, C. Wildeman, and E. Roberto. Tragic, but
not random: The social contagion of nonfatal gunshot injuries.
Social Science and Medicine, page 139.
[17] B. A. H. D. Papachristos, A. Social networks and the risk of
gunshot injury. J. Urban Health, 2012.
[18] H. D. B. A. Papachristos, A. The corner and the crew: The
influence of geography and social networks on gang violence.
American Sociological Review, 2013.
[19] D. Paulo, B. Fischl, T. Markow, M. Martin, and P. Shakarian.
Social network intelligence analysis to combat street gang
violence. In Proceedings of the 2013 IEEE/ACM
International Conference on Advances in Social Networks
Analysis and Mining, ASONAM â€™13, pages 1042â€“1049, New
York, NY, USA, 2013. ACM.
[20] R. R. Petersen and U. K. Wiil. Crimefighter investigator: A
novel tool for criminal network investigation. In EISIC, pages
197â€“202. IEEE, 2011.
[21] D. K. Rossmo and S. Rombouts. Geographic Profiling. In
R. Wortley and L. Mazerolle, editors, Enviromental
Criminology and Crime Analysis, pages 136â€“149. 2008.
[22] J. Schroeder, J. Xu, and H. Chen. Crimelink explorer: Using
domain knowledge to facilitate automated crime association
analysis. In H. Chen, R. Miranda, D. Zeng, C. Demchak,
J. Schroeder, and T. Madhusudan, editors, Intelligence and
Security Informatics, volume 2665 of Lecture Notes in
Computer Science, pages 168â€“180. Springer Berlin Heidelberg,
2003.
[23] S. B. Seidman. Network structure and minimum degree. Social
Networks, 5(3):269 â€“ 287, 1983.
[24] P. Shakarian, J. Salmento, W. Pulleyblank, and J. Bertetto.
Reducing gang violence through network influence based
targeting of social programs. In Proceedings of the 20th ACM
SIGKDD International Conference on Knowledge Discovery
and Data Mining, KDD â€™14, pages 1829â€“1836, New York, NY,
USA, 2014. ACM.
[25] M. A. Tayebi, M. Ester, U. GlaÌˆsser, and P. L. Brantingham.
Spatially embedded co-offence prediction using supervised
learning. In Proceedings of the 20th ACM SIGKDD
International Conference on Knowledge Discovery and Data
Mining, KDD â€™14, pages 1789â€“1798, New York, NY, USA,
2014. ACM.
[26] M. A. Tayebi, M. Jamali, M. Ester, U. GlaÌˆsser, and R. Frank.
Crimewalker: A recommendation model for suspect
investigation. In Proceedings of the Fifth ACM Conference on
Recommender Systems, RecSys â€™11, pages 173â€“180, New York,
NY, USA, 2011. ACM.
[27] D. J. Watts and S. H. Strogatz. Collective dynamics of
â€œsmall-worldâ€ networks. nature, 393(6684):440â€“442, 1998.

An Empirical Evaluation Of Social Influence
Metrics
Nikhil Kumar, Ruocheng Guo, Ashkan Aleali, Paulo Shakarian

arXiv:1607.00720v3 [cs.SI] 23 Jul 2016

Arizona State University,
Tempe, AZ
Email: {nikhilkumar, rguosni, aleali, shak}@asu.edu

Abstractâ€”Predicting when an individual will adopt a new
behavior is an important problem in application domains such
as marketing and public health. This paper examines the performance of a wide variety of social network based measurements
proposed in the literature - which have not been previously
compared directly. We study the probability of an individual
becoming influenced based on measurements derived from neighborhood (i.e. number of influencers, personal network exposure),
structural diversity, locality, temporal measures, cascade measures, and metadata. We also examine the ability to predict
influence based on choice of classifier and how the ratio of positive
to negative samples in both training and testing affect prediction
results - further enabling practical use of these concepts for social
influence applications.

I. I NTRODUCTION
Predicting when an individual will adopt a new behavior is
an important problem in application domains such as marketing [1], the spread of innovation [2], countering extremism [3],
and public health [4]. As a result, a variety of social network
based measurements have been proposed in the literature and
shown to predict how likely an individual will adopt a new
behavior given information about his immediate social ties.
However, when such measures are proposed, they are often
evaluated under different conditions - making it difficult to
understand which of these measurements should be used in
a real-world application. Further complicating the issue is
that the choice of classification algorithm and the effect of
class imbalance in both training and testing are often not
explored in most research. In our lab, we have the goal
of creating and deploying a system for counter-extremism
messaging. Hence, understanding how influence measurements
work in experimental settings that closely resemble real-world
scenarios is an important first step.
In this paper, we study measurements based on neighborhood (i.e. number of influencers [4], personal network
exposure [2]), structural diversity [5], locality [6], temporal
measures [7], cascade measures [8], and metadata [9]. We
examine the probability of an individual becoming influenced
based on these measurements (probability of adoption). We
also examine the ability to predict influence based on choice
of classifier and how the ratio of positive to negative samples in
both training and testing affect prediction results. Specifically,
we make the following contributions.

1) We review a variety of measurements used to predict
social influence and we group them in six categories
(Section III).
2) We evaluate how these measurements relate to the probability of a user being influenced using real-world microblog data (Section IV).
3) We evaluate how these measurements perform when used
as features in a machine learning approach and compare
performance across a variety of supervised machine learning approaches (Section V).
4) We evaluate how the ratio of positive to negative samples in both training and testing affect predictive results
(Section VI).
We note that contribution 4 is of particular importance, as
(particularly with microblog data) users are exposed to large
number of messages that they do not retweet (negative samples). Hence, in both training and testing, researchers can
increase the negative samples utilized by large amounts - hence
arbitrarily determining the level of class imbalance. As with
this study as a whole, the experiments on data imbalance were
to better understand these previous research results in tests that
better mimicked real-world scenarios.
Related work. Beyond the work that we shall describe
concerning the various measures for social influence we investigate in Section IV, there has been some general work
in the area of social influence that have taken approaches not
necessarily amenable to comparison. For instance, the seminal
work of Kempe et al. [10] describes two popular models for
information cascades which spawned several techniques to
learn the parameters (which also correspond to edge weights
in the graph). For example, Saito et al. [11] assigned such
probabilities based on an expectation-maximization appproach
while Goyal et al. [7] leveraged a variety of simple models
based on ideas such as a empirically-learned probabilities and
similarity measurements. See [12] for a review of some of
this work. There has also been related work on predicting cascades [13], [14], [8] which are more focused on determining
if a trend in social media exceeds a certain size. That said,
some of the ideas from these approaches, such as structural
diversity [5] are examined here (though this paper is focused
on a different problem). Other work such as Myers et al. [15]
studied the external factors influencing information diffusion,
Liu et al. [16] and Tang et al. [17] focused their studies

on topic influence. Jenders et al. [9] studied a combination
of different features including some of the metadata features
like mentions and hashtags, along with latent features like
sentiments and emotional divergence for predicting the virality
of a tweet - many of which we examine in this study as well.
Hong et al. [18] have also considered a wide spectrum of features including structural, content and temporal information.
However, their study focused more on content-based features
and not the structural features considered here - many of which
were introduced after that work.

cascades. We create the social network G from the retweeting
relationships of microblogs published between May 1, 2011
and July 31, 2011. We use the microblogs published in August
2011 to train and test our approach. Table I lists the statistics
of the dataset we used.

II. T ECHNICAL P RELIMINARIES

We found that the network derived from the dataset had
7,668,693 users with 55,381,104 edges between them. For this
network, the number of active users in August (the time period
used to study social influence) is 5,910,608 while 5,664,625 of
them have at least have one out-neighbor. During the month
of August, there were 22,182,703 retweet chains. From this
data, we removed the users who are not present in V ; we
also removed 2,660,421 empty repost chains caused by this
elimination. The dataset does not contain the repost time for
the nodes in the middle of chains. We estimated this time for
each node in the chain based on the original post time and
the final repost time. Table I lists the statistics of this dataset
during the period of study.
Among all the retweeted users we further extract the top
retweeters defined as those who had at least 100 retweets
during the period. This set of high frequency tweeters will be
used as a base for deriving the sample set for our experiments.
For each user in the above mentioned group, an occurrence of
them retweeting a post when they have an active in-neighbor
is considered as a positive instance. If any of their followees
have tweeted and they havenâ€™t retweeted, it is considered as a
negative instance.

Here we introduce the necessary notation and describe our
social network data. We represent a social network as a graph
G = (V, E) where V is the set of vertices and E is the set
of directed edges that have sizes |V |, |E| respectively. The
intuition behind edge (v, v 0 ) is that node v can influence v 0 .
This intuition stems from how we create the edges in our
network: (v, v 0 ) is an edge if during a specified time period
there is at least one microblog posted by v that is reposted
by v 0 . For node v âˆˆ V , the set of in-neighbors is denoted as
Î·vin , and the set of out-neighbors as Î·vout . We use din
v and
to
denote
the
in-degree
and
out-degree
respectively.
We
dout
v
also assume a partition over nodes that specifies a community
structure. We assume that such a partition is static (based on
the same time period from which the edges were derived) and
the function P (V ) : V â†’ C maps the set of nodes (V ) to the
set of communities (C), where C consists of k communities:
{C1 , C2 , ..., Ck }. We utilize the Louvain algorithm [19] to
identify our communities in this paper due to its ability to
scale.
Cascades. For a given microblog Î¸, we define t as the number
of time units from the initial post of Î¸ before the microblog
was reposted by one of vâ€™s incoming neighbors - intuitively
the time at which v was exposed to Î¸. We denote the subset
of nodes who originally posted or reposted Î¸ for time period
t as VÎ¸t . Likewise, the set of reposting relationships within
the same time period will be denoted by RÎ¸t . Taken together,
we have a cascade: DÎ¸t = (VÎ¸t , RÎ¸t ). Any valid original
microblog Î¸ could be treated as a unique identifier for a
cascade. Given a microblog Î¸, vÎ¸ is the originator at instance
t0Î¸ , which is defined as the origin time when the originator
posted the microblog Î¸. We denote the size of a cascade at
any particular time t as |VÎ¸t |. For v âˆˆ VÎ¸t , the set of all active
neighbors with respect to Î¸ is defined as SÎ¸v = VÎ¸t âˆ© Î·vin . We
also define the distance dtÎ¸ (v, u) as the shortest path length
between v and u in DÎ¸t .
Sina Weibo Dataset. The dataset we used was provided by the
WISE 2012 Challenge1 . It included a sample of microblogs
posted on Sina Weibo from 2009 to 2012. In this dataset, we
are provided with time and user information for each post and
the last repost in a chain which enabled us to derive a corpus of
1 http://www.wise2012.cs.ucy.ac.cy/challenge.html

#Users
5,910,608

#Edges
52,472,547

#Reposted tweets
2,238,659

#Reposted Users
394,441

TABLE I: Graph statistics

III. M EASUREMENTS TO P REDICT S OCIAL I NFLUENCE
In this section, we categorize several approaches for predicting social influence.
1) Neighborhood-based measures
2) Structural diversity measures
3) Influence locality
4) Cascade-based measures
5) Temporal measures
6) Metadata
We examine each of these categories in turn.
Neighborhood-based measures. These are the measures computed using each node and its immediate neighbors. These
measures represents the pair wise influence that the neighboring nodes exert on a given node. Retweeting from followees
is the primary mode of tweet visibility in a microblogging
site, as usually a tweet is visible to a user from its followee
subgraph. Specifically, we study the following
Î¸
â€¢ Number of active neighbors. (|Sv |) This represents
the count of active neighbors for a node v. In Damon
Centolaâ€™s notable empirical study [4], he noted that additional â€œsocial signalsâ€ â€“ or active neighbors â€“ significantly

â€¢

â€¢

increased the likelihood of an individual adopting a new
behavior.
Personal Network Exposure (PNE). (|SvÎ¸ |/din
v ) Is a
measure adopted from the social science community (i.e.
see [2] ) and has obtained recent interest (i.e. [20]). As
per [2], PNE quantifies the extent to which a person is
exposed to direct and indirect influence. This value is
defined as the ratio of number of active neighbors to total
number of neighbors. It is a measure of the fraction of
influence an active neighbor u has on v. If v has many
in-neighbors aka followees, then uâ€™s influence is diluted
and PNE represents that dilution.
Average in-neighbor count of active neighbors.
Î¸
(|Î£uâˆˆSvÎ¸ din
u |/|Sv |) This is calculated by averaging the
number of in-neighbors of each active neighbor of a node.
This defines the dilution of the influence path and is
similar to the measure, number of uninfected neighbors
as described in [14]. Other releated studies include Cha
et al. [21], where they studied the effect of a social
network userâ€™s indegree in depth, and observed that high
indegree is not necessarily correlated to influence in terms
of spawning retweets.

Structural diversity measures. This group of measurements
taken into account the structural diversity in the local neighborhood of the node - which refers to the communities present
in the neighborhood.
Ugander et al. [5] introduced structural diversity where
they studied the effect of number of connected components of
a friendship network. Fortunato et al [22] defined communities
as the set of graph vertices which are organized into groups
that seem to live fairly independently of the rest of the graph.
Weng et al. [23] used the community structure to predict the
increase in cascade size. We use the modularity maximization
method [24] for detecting communities in our dataset. The
Louvain Algorithm [19] which comes under this method is
used to derive the communities in this study due to its ability
to scale. We use two community based measures.
â€¢

â€¢

Active community count. (|P (SvÎ¸ )|) This is defined as
the number of adjacent communities of a given user v
with at least one active neighbor of v. The communities
that include active neighbors are more significant in this
context than rest of the adjacent communities. Shakarian
et al. have studied this measure in their book [12]
highlighting the importance of structural diversity.
Active community ratio (|P (SvÎ¸ )|/|P (Î·vin )|) It is calculated as the ratio of the active community count to
the total number of adjacent communities. This is similar
to the personal network exposure [2] and represents the
dilution of the effect of active community count with
respect to other neighboring communities.

Influence locality. We examine the Influence Locality model
known as LRC-Q, introduced by Zhang et al. [6]. LRC-Q
is defined by the influence locality function Q which is a
combination of peer influence factor (g) and structural factor
(f ). Peer influence factor is obtained as a linear combination

of the geometric mean of random walk probabilities of active
neighbors and structural factor as a linear combination of the
number of circles formed by the active neighbors in the ego
network of the user v. These are defined in their paper by the
following equations.
Q = w Ã— g + (1 âˆ’ w) Ã— f
s Y
(tvÎ¸ âˆ’ tvÎ¸i ) Ã— pvi
g = |SvÎ¸ |

(1)
(2)

vi âˆˆSvÎ¸
Î¸

f = a log(|SvÎ¸ | + 1) + beâˆ’Âµ|C(Sv )|

(3)

In the above equations, pvi is the random walk probability
from the active user vi to the given user v, C(Sv ) is the
collection of circles formed by the active neighbors, tvÎ¸ is the
time at which v posted or reposted the microblog Î¸, Âµ is the
decay factor and, a, b and w are balance parameters. For our
experiments we set the value of Âµ as 1 and, a, b and w to be
0.5, as per the parameter settings of [6].
Cascade-based measures.
This group of measurements take into account the various
parameters that are part of a microblog cascade. There has
been many studies in the area of predicting the cascades
including Bakshy et al. [25] , Cheng et al. [13] and more
recently Guo et al. [8]. Unlike our study, there hasnâ€™t been
many attempts to utilize the cascade parameters in predicting
retweet behavior. We study the following measures.
t
â€¢ Cascade size. (|VÎ¸ |) Cascade size is computed as the
count of people who have retweeted a particular microblog Î¸ at time t. This number is usually visible to the
microblog user and can have an impact on their retweet
behavior.
t
â€¢ Path length. (dÎ¸ (v, vÎ¸ )) Path length is the length of a
tweet trace path from the original tweeter to a given user
in the cascade. Watts et al. [26] were the first to study
the path length where they found that many social and
technological networks have small path lengths. Kwak et
al. [27] studied the path length in twitter, and Weng et
al. [23] studied a distance measure called Average step
distance which was based on the path length. Our study
focuses on the path length with respect to a particular
cascade DÎ¸t .
Temporal Measure Temporal measures were given prominence in many of the prior studies either by itself, or as a
factor in combination with other measures. Goyal et al. [7]
utilized the temporal factor and attempted to predict the time
by which an influenced user will perform an action. Hong et al.
[18] studied a variety of temporal measures and observed that
they have a stronger effect on messages with low and medium
volume of retweets, compared to highly popular messages. We
study the following temporal measure.
â€¢ Retweet Time delay. (t) This is defined as the time
delay between the original tweet and the time when
v is exposed to microblog Î¸. The time at which a

tweet was made is another piece of information which
people are exposed to while viewing a tweet. This
can affect their decision to retweet it or not. This is
one of the temporal measures studied by Hong et al. [18].
Metadata. These are simple measures derived from the metadata associated with the tweets. We consider the presence
or absence of links, mentions and hashtags as measures for
our study. Jenders et al. [9] did an extensive analysis of a
wide range of tweet and user features regarding their influence
on the spread of tweets. They considered the number of
mentions and number of hashtags among the obvious tweet
features. They observed that tweets containing both hashtags
and mentions are more likely to be retweeted than those with
out, however as the number of hashtags/mentions in a tweet
grows, the expected number of retweets decreases. In this
study we only consider their presence or absence as a measure
and do not go into any deeper analysis.
â€¢ Presence of a link (hasLink). This is a binary value
which represents whether the original tweet had a link.
Links are usually shown as part of the tweet content. The
measure of Links in tweets is similar to that of mentions
and hashtags, but has not been studied as extensively as
either in the context of social influence.
â€¢ Presence of a mention (hasMention). A binary value
which represents whether the original tweet had a mention. Intuitively, a user might be more willing to retweet
if there is a mention of him/her or someone he/she knows.
Similar to [9], Cha et al. [21] analyzed the effect of the
number of mentions and found that mentions can be an
important measure of an individual influence in the social
network.
â€¢ Presence of a hashtag (hasHashtag). A binary value
which represents whether the original tweet had hashtags.
Hashtags are also means by which tweets become visible
to users and thus are of significance in this regard. A
deeper analysis such as [9], is beyond the scope of this
work and we only focus on how the presence or absence
of a hashtag affects the retweeting behavior.
IV. S OCIAL I NFLUENCE M EASUREMENT S TUDY
Here, we examine the distribution of various measurements
which were defined in Section III. For each of those measures,
the values are put into intervals of equal sizes and the fraction
of positive samples in the interval is plotted as the probability.
The horizontal axis shows the value intervals of the measure,
while the vertical one shows the number of occurrences for
positive instances with respect to the total amount in that
particular interval. The error bar shows twice the standard
deviation of the sample. These are shown in Fig. 1 and Fig. 2.
A detailed analysis of their distribution is given below.
Neighborhood-based measures. Active neighbor count
intuitively has a positive correlation with the influence as
shown in Fig. 1(a). Fig. 1(b) shows the active neighbor count

for the lower values which also shows similar correlation. This
is consistent with the empirical study of [4]. As the number
of retweeters among in-neighbors increases, the probability of
a person retweeting the particular tweet increases. Fig. 1(c)
shows that PNE also exhibits positive correlation like active
neighbor count. This shows the significance of PNE measure
as demonstrated by other studies such as [2] and [20].
Average in-neighbor count of Active Neighbors does not
show a clear correlation in its distribution as seen in Fig. 1(d).
Structural diversity measures. Number of active
communities shows a good positive correlation with the
retweet behavior. This result is consistent with the related
studies such as [23] and [13]. Active community ratio
also demonstrates a reasonable correlation with the positive
instances as this measure represents the dilution of community
influence based on the total number of adjacent communities.
Cascade-based measures. Intuitively, cascade size is an
important influencer in retweet behavior. If a tweet is
reasonably popular it tends to attract further retweets. The
same is revealed from the distribution in Fig. 2(c). This is
consistent with the research of [25] and [13] although they
studied a different problem. The intuition for path length is
that, as the distance from the original tweeter increases a user
is less interested in retweeting the tweet. Our results show
(Fig. 2(d)) that this intuition holds between path length 1 and
2. But, for the remaining intervals, results doesnâ€™t correlate
well. This can be explained by comparing to the results
of [9] where they found similiar pattern while analyzing
mentions and hashtags. Further, the results of [13] indicate
that information cascade depth is related to popularity. Hence,
the microblogs that are far from the original poster may be
inherently popular as the information cascade has proceeded
to a larger depth.
Temporal. Fig. 2(e) shows that retweet time delay has slight
inverse correlation with the influence. Intuitively, the influence
of a tweet decays with time, and as people are exposed to
date/time information in the social network they are less
likely to retweet old tweets. This decay factor has been used
in works like [7], [6] etc. and above result shows the same.
Metadata. Table II shows the conditional probability of
positive instances given the meta measure value of 0 and 1,
respectively. The values from the table shows that presence or
absence of a link doesnâ€™t seem to have much correlation with
the influence. It also shows that, the presence of mentions
seem have slight negative correlation to influence though
there is no actual intuition to base this on. But, this can be
explained by the observation in the paper [9] that as the
number of mentions in a tweet grows, the expected number
of retweets decreases. The presence of hashtag shows an
interesting correlation in Table II. This is consistent with the
study of [9] and illustrates the significance of hashtags in
enhancing the visibility of the tweet and motivating a user to

P (yi = pos |Vi = 1)
0.48
0.45
0.66

1.0
0.8

Probability

P (yi = pos |Vi = 0)
0.51
0.51
0.50

1.0
0.8
0.6
0.4
0.2
0.0

~ is a column of the design matrix corresponding
TABLE II: V
to a certain binary feature, pos represents positive label and i
is the index of the sample.

0

1

2

3

4

5

Active community count
(a)

(b)
1.0

0.2
0.0

0.4
0.2

Active neighbor count

(b)

1.0

0.2

0.2

Path length

(c)

Probability

0.6
0.4
0.2

(d)

0.6
0.4
0.2
0.0

10 99
0
20 0-19
00 99
30 -29
0 99
40 0-39
00 99
50 -49
0 99
60 0-59
00 99
70 -69
0 99
80 0-79
0 99
90 0-89
9
10 00-99 9
00 99
0-1
09
99

0-9

.6)

.5)

[0.

5-0

.4)

[0.

4-0

.3)

[0.

3-0

.2)

[0.

2-0

.1)

1-0

[0.

0-0
[0.

PNE

0 1 2 3 4 5 6 7 8 9

1.0

0.0

0.0

0.2
0.0

Avg. in-neighbor count

010 99
0-1
20 99
0
30 -299
0
40 -399
0
50 -499
0
60 -599
0-6
70 99
0
80 -799
0-8
90 99
0-9
99

Probability

0.4

0.4

0.8

0.8

0.6

0.6

Cascade size

1.0

0.8

Probability

0 1 2 3 4 5 6 7 8 9

Active neighbor count (lower values)

(a)

0.4
0.0

0.0

0-9 10-19 20-29 30-39 40-49 50-59

0.6

040 399
0
80 -799
0
12 -119
0
16 0-15 9
0 9
20 0-19 9
0 9
24 0-23 9
0 9
28 0-27 9
0 9
32 0-31 9
0 9
36 0-35 9
0 9
40 0-39 9
00 99
-43
99

0.4

0.6

0.8

0.8

Probability

0.8

Probability

Probability

0.8
0.6

0.2

Active community ratio

Probability

1.0

0.4
0.0

1.0

1.0

0.6

[0.
0-0
[0. .1)
1-0
[0. .2)
2-0
[0. .3)
3-0
[0. .4)
4-0
[0. .5)
5-0
[0. .6)
6-0
[0. .7)
7-0
.8)

~
V
hasLink
hasMention
hasHashtag

Probability

retweet them.

Time Delay
(e)

(c)

(d)

Fig. 1: Plots of Neighborhood and temporal measures. Error
bars represent two standard deviations.
V. I NFLUENCE P REDICTION
A. Methods
We derive our graph G from the dataset as described
under Section II. We use the microblogs published in August
2011 to extract the instances to train and test our approach.
Positive and negative instances are extracted as described in
Section II, and the measures described in Section III were
extracted as features for each of them. This set is used to
obtain a random sample with 1:1 negative to positive ratio,
which we will use for the classification experiments.
Classification experiments Here we examine our experiments
for predicting whether a user under given conditions will
retweet or not. As this is a binary classification task we report
the performance measurements (precision, recall and unbiased
F1) for only the positive (retweeting) class. We also examine
the classification performances of various learning algorithms.
For each of the experiments we use a training to test set ratio of
70:30 and used a 10 fold cross validation. We use the following
classification algorithms for our experiment.

Fig. 2: Plots of Structural Diversity and Cascade-based measures. Error bars represent two standard deviations.

Random Forest (RF). Random Forest [28] is a popular
ensemble method used for classification and regression.
Ensemble methods use multiple classifier algorithms to obtain
better accuracy than that could be obtained using any of the
individual classifiers. We use random forest algorithm with
bootstrap aggregating, that fits a number of decision trees
on different sub-samples of the dataset. Each decision tree
provides its own predictions which are then merged obtain a
better accuracy.
AdaBoost Classifier (AB). The AdaBoost algorithm [29]
proposed by Yoav Freund and Robert Schapire is one of the
most important ensemble methods. It is prominent among the
boosting techniques [29] which are used in conjuction with
other learning algorithms. In this method, the weak learners
are combined into a final sum representing the boosted
output. We use the particular algorithm called AdaBoostSAMME [30] and use the decision trees as the base estimator.
Logistic Regression (LR). Logistic regression is a generalized
linear model which uses a logistic function to infer the

relationship between a dependent variable and one or more
independent variables. We utilizes the binomial logistic
regression which predicts the probability that an observation
falls into one of the two categories. Logistic regression has
low varience and is less prone to overfitting.
Naive Bayes Classifier (NB). Naive Bayes is a probabilistic
classifier which is based on applying Bayesâ€™ theorem with
independence assumption between every feature pairs. Naive
Bayes classifiers are highly scalable and less prone to the
curse of dimensionality, making it one of the top machine
learning algorithms. We implement the Gaussian Naive Bayes
algorithm for classification where the likelihood of the features
is assumed to be Gaussian.

1.0

Precision

Recall

F1

0.8
0.6
0.4
0.2
0.0

Nbr

Structural Cascade Temporal

Meta

LRC-Q Multi-Meas

(a)

1.0

Precision

Recall

F1

0.8
0.6

B. Measurement Group Comparison

0.4

Here we compare the classification performance of the
various measurement groups described in Section III. Fig. 3
shows the behavior of different feature groups using multiple
classifier algorithms, which provides a better understanding of
this all-important component in a deployed system. Generally
Random Forest provides the best performance among all
the classifier algorithms. Neighborhood-based (Nbr) measures
perform quite well in Random Forest, AdaBoost and Logistic
regression. This is consistent with what we discussed in Section IV. Structural diversity measures show less performance
compared to other groups. This can be attributed to the fact that
it is not often used independently in classification, and usually
this group performs well in conjunction with other measures
such as Neighborhood-based. LRC-Q gives performance measure comparable to the results in [6]. Cascade-based measures
are observed to perform reasonably well in Random Forest,
Logistic Regression and AdaBoost. This once again illustrates
the significance of cascade size and brings into focus the
path length measure. Temporal measure performs well in all
classifiers except Naive Bayes. Although time based measures
are frequently used as a decay factor in conjunction with other
measures ([7], [6]), our results show that it could yield high
predictive power by itself. Metadata measures show good and
consistent performance across all classifiers. As research by
[9] shows, hashtag and mentions have high predictive power
with respect to retweet behaviour and our results confirm the
significance of this measure along with the hasLinks measure.
With an eye toward a deployed system, we also examine
a â€œMulti-Measurement modelâ€ which is a combination of
Neighborhood, Structural, Cascade, Temporal and Metadata
measures. The Multi-Measurement model shows better performance than individual groups generally among Random
Forest, Logistic Regression and AdaBoost classifiers. The
other measures such as neighborhood-based, temporal and
LRC-Q perform reasonably well compared to rest of the individual future groups. The performance of Multi-Measurement
model shows real value in combining the various features and
individual feature groups to improve our ability to predict
retweet behavior in real world datasets.

0.2
0.0

Nbr

Structural Cascade Temporal

Meta

LRC-Q Multi-Meas

(b)

1.0

Precision

Recall

F1

0.8
0.6
0.4
0.2
0.0

Nbr

Structural Cascade Temporal

Meta

LRC-Q Multi-Meas

Meta

LRC-Q Multi-Meas

(c)

1.0

Precision

Recall

F1

0.8
0.6
0.4
0.2
0.0

Nbr

Structural Cascade Temporal
(d)

Fig. 3: Performance with different classifier algorithms. a)
Random Forest b) Logistic Regression c) Naive Bayes d)
AdaBoost.

C. Multi-Measurement Model Compared to Influence Locality
We compare our results with the LRC-Q model described in
[6]. We experimented with multiple classification algorithms
for this task and the best results were obtained using Random
Forest classifier. The results obtained using Random Forest
(RF), Logistic Regression (LR), Naive Bayes (NB) and
AdaBoost (AB) are shown in the Table III. As LRC-Q uses

Precision
0.679
0.95
0.794
0.602
0.764

Recall
0.573
0.947
0.765
0.704
0.285

F1
0.622
0.948
0.784
0.649
0.415

TABLE III: Performance of retweet behavior prediction
VI. VARYING N EGATIVE TO P OSITIVE RATIO
An important question when deploying the aforementioned
methods in a real-world application is how to best train the
model to cope with data imbalance observed in-practice. As
individuals are exposed to an arbitrarily large number of
microblogs that they do not rebroadcast, this is a difficult and unfortunately relatively unstudied problem. Here, we conducted experiments to analyse how classification performance
varies with different negative to positive ratio in both training
and test set. The surface and linear plots in Fig. 4 show the
precision, recall and F1 values obtained using Random Forest
classifier, when negative to positive ratio is varied from 1:1
to 9:1. The ratio was varied in both training set and test
set to observe the effects on overall performance. Precision
is observed to decrease as we increase the size of negative
samples in test set while keeping the ratio in training set
constant. Recall is observed to remain the same with changing
ratio in test set. Change in negative to positive ratio in training
set on the hand, shows slight increase in precision where as
recall decreases. Results for LRC-Q follows a similar pattern
except for the convergence of recall for increased imbalance in

Precision

1.00
0.95
0.90
0.85
0.80
0.75
0.70
N to 9
P tr8a765
0.65
inin432 1 9 8 7 6 5 4 3 2 1
gs
P test set

et

Precision

1
2

N to

N to P for training
3
5
7
4
6
8

9

1.00
0.95
0.90
0.85
0.80
0.75
0.70
0.65
0.60
1 2 3 4 5 6 7 8 9
Negative to positive ratio in test set

(a)

(b)

Recall

0.95
0.90
0.85
0.80
0.75
0.70
0.65
89

1
2

6 7 set
1 2
4 5 test
N to3P tr4 5 6
3
P
aining s7 8
2
et 9 1 N to

N to P for training
3
5
7
4
6
8

9

1.00
0.95
0.90
0.85
0.80
0.75
0.70
0.65
0.60
1 2 3 4 5 6 7 8 9
Negative to positive ratio in test set

(c)

(d)
1
2

0.95
0.90

F10.85

F1

Model
LRC-Q (LR)
Multi-Meas (RF)
Multi-Meas (AB)
Multi-Meas (LR)
Multi-Meas (NB)

training set. From these results, it can be generally observed
that 1:1 is the ideal ratio of negative to positive samples in
training set for an unknown imbalance in test data.

Recall

only a single feature, we only use Logistic Regression for
its evaluation. It can be observed that Multi-Measurement
model outperforms the LRC-Q model in all classifiers except
for Naive Bayes. This can be attributed to the fact that
while LRC-Q takes into account pairwise and structural
influence along with time decay, Multi-Measurement model
incorporates more parameters in addition to the above.
LRC-Q has combined the pairwise and structural factor
into a single feature and uses time measure as a decay
factor. The Multi-Measurement model on the other hand
treats them individually, along with including different
kinds of pairwise influence (such as active neighbor count,
personal network exposure and average in-neighbors of active
neighbors), considering both direct as well as ratio based
measures for structural diversity, and using temporal measure
as an independent feature. In addition to that, this model
also includes cascade and metadata based features giving
it a broader view of the parameters that can influence an
individualâ€™s retweeting behavior. This demonstrates that in
any attempt of retweet prediction, a broader approach is
required, which incorporates multiple measures that are are
closely related (within the measurement groups) and those
that are mutually exclusive (across groups) to obtain the best
prediction in classification.

0.80
1 et
0.75
4 3 2 ng s
1 2 3 4 5 6
7 6 5 aini
N to P test set 7 8 9 9 t8o P tr

N

N to P for training
3
5
7
4
6
8

9

1.00
0.95
0.90
0.85
0.80
0.75
0.70
1 2 3 4 5 6 7 8 9
Negative to positive ratio in test set

(e)

(f)

Fig. 4: Plots for classification on imbalanced data for MultiMeasurement model using Random Forest. a) Precision surface plot b) Precision line plot c) Recall surface plot d) Recall
line plot e) F1 surface plot f) F1 line plot.

VII. C ONCLUSION
In this paper, we examines the performance of a wide
variety of social network based measurements and study the
probability of an individual becoming influenced based on
them. In this study, we grouped those measures under various
measurement groups to understand their group wise predictive
power. We designed these experiments so that they would
move beyond standard research-based experiments used to
evaluate an idea - we designed these experiments to understand
how well these ideas can be used in a deployed system. We
look to use these results in a system that we intend to deploy
or license for real-world influence operations such as counterextremism.

Precision

N to 9
P tr8a765
inin432 1 9 8 7 6 5 4 3 2 1
g se
N to P test set
t

Precision

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

1
2

9

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 2 3 4 5 6 7 8 9
Negative to positive ratio in test set

(a)

(b)

Recall

0.6
0.5
0.4
0.3
0.2
0.1
0.0
9
8

Recall

1
2

6 7 set
1 2
4 5 test
N to3P tr4 5 6
3
P
aining s7 8
2
et 9 1 N to

N to P for training
3
5
7
4
6
8

9

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 2 3 4 5 6 7 8 9
Negative to positive ratio in test set

(c)

(d)

F1

1
2

0.7
0.6
0.5
F1 0.4
0.3
0.2
0.1
0.0

N to P for training
3
5
7
4
6
8

1 et
4 3 2 ng s
1 2 3 4 5 6
7 6 5 aini
N to P test set 7 8 9 9 t8o P tr

N

N to P for training
3
5
7
4
6
8

9

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 2 3 4 5 6 7 8 9
Negative to positive ratio in test set

(e)

(f)

Fig. 5: Plots for classification on imbalanced data for LRCQ using Logistic Regression. a) Precision surface plot b)
Precision line plot c) Recall surface plot d) Recall line plot e)
F1 surface plot f) F1 line plot.

ACKNOWLEDGMENTS
Some of the authors are supported through the AFOSR
Young Investigator Program (YIP) grant FA9550-15-1-0159,
ARO grant W911NF-15-1-0282, the DoD Minerva program
grant N00014-16-1-2015 and the EU RISE program.
R EFERENCES
[1] D. Watts and J. Peretti, â€œViral marketing for the real world,â€ Harvard
Business Review, May 2007.
[2] T. W. Valente, Network models of the diffusion of innovations, ser.
Quantitative methods in communication.
Cresskill, N.J.: Hampton
Press, 1995, thomas W. Valente. Includes bibliographical references (p.
153-163) and indexes.
[3] S. Al-khateeb and N. Agarwal, â€œExamining botnet behaviors for propaganda dissemination: A case study of isilâ€™s beheading videos-based
propaganda,â€ in ICDM Workshops. IEEE, 2015, pp. 51â€“57.
[4] D. Centola, â€œThe Spread of Behavior in an Online Social Network
Experiment,â€ Science, vol. 329, no. 5996, pp. 1194â€“1197, Sep. 2010.
[5] J. Ugander, L. Backstrom, C. Marlow, and J. Kleinberg, â€œStructural
diversity in social contagion,â€ Proceedings of the National Academy
of Sciences, vol. 109, no. 16, pp. 5962â€“5966, 2012.

[6] J. Zhang, B. Liu, J. Tang, T. Chen, and J. Li, â€œSocial influence locality
for modeling retweeting behaviors.â€ in IJCAI, vol. 13, 2013, pp. 2761â€“
2767.
[7] A. Goyal, F. Bonchi, and L. V. Lakshmanan, â€œLearning influence
probabilities in social networks,â€ in Proceedings of the third ACM
international conference on Web search and data mining. ACM, 2010,
pp. 241â€“250.
[8] R. Guo, E. Shaabani, A. Bhatnagar, and P. Shakarian, â€œToward order-ofmagnitude cascade prediction,â€ in Proceedings of the 2015 IEEE/ACM
International Conference on Advances in Social Networks Analysis and
Mining 2015. ACM, 2015, pp. 1610â€“1613.
[9] M. Jenders, G. Kasneci, and F. Naumann, â€œAnalyzing and predicting
viral tweets,â€ in Proceedings of the 22nd international conference
on World Wide Web companion.
International World Wide Web
Conferences Steering Committee, 2013, pp. 657â€“664.
[10] D. Kempe, J. Kleinberg, and EÌ. Tardos, â€œMaximizing the spread of
influence through a social network,â€ in Proceedings of the ninth ACM
SIGKDD international conference on Knowledge discovery and data
mining. ACM, 2003, pp. 137â€“146.
[11] K. Saito, R. Nakano, and M. Kimura, â€œPrediction of information
diffusion probabilities for independent cascade model,â€ in Knowledgebased intelligent information and engineering systems. Springer, 2008,
pp. 67â€“75.
[12] P. Shakarian, A. Bhatnagar, A. Aleali, R. Guo, and E. Shaabani,
Diffusion in Social Networks. Springer, 2015.
[13] J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec,
â€œCan cascades be predicted?â€ in Proceedings of the 23rd international
conference on World wide web. ACM, 2014, pp. 925â€“936.
[14] L. Weng, F. Menczer, and Y.-Y. Ahn, â€œVirality prediction and community
structure in social networks,â€ Scientific reports, vol. 3, 2013.
[15] S. A. Myers, C. Zhu, and J. Leskovec, â€œInformation diffusion and
external influence in networks,â€ in Proceedings of the 18th ACM
SIGKDD international conference on Knowledge discovery and data
mining. ACM, 2012, pp. 33â€“41.
[16] L. Liu, J. Tang, J. Han, M. Jiang, and S. Yang, â€œMining topic-level
influence in heterogeneous networks,â€ in Proceedings of the 19th ACM
international conference on Information and knowledge management.
ACM, 2010, pp. 199â€“208.
[17] J. Tang, J. Sun, C. Wang, and Z. Yang, â€œSocial influence analysis
in large-scale networks,â€ in Proceedings of the 15th ACM SIGKDD
international conference on Knowledge discovery and data mining.
ACM, 2009, pp. 807â€“816.
[18] L. Hong, O. Dan, and B. D. Davison, â€œPredicting popular messages in
twitter,â€ in Proceedings of the 20th international conference companion
on World wide web. ACM, 2011, pp. 57â€“58.
[19] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, â€œFast
unfolding of communities in large networks,â€ Journal of Statistical
Mechanics: Theory and Experiment, vol. 2008, no. 10, p. P10008, 2008.
[20] A. Halavais, K. H. Kwon, S. Havener, and J. Striker, â€œBadges of
friendship: Social influence and badge acquisition on stack overflow,â€
in 2014 47th Hawaii International Conference on System Sciences, Jan
2014, pp. 1607â€“1615.
[21] M. Cha, H. Haddadi, F. Benevenuto, and P. K. Gummadi, â€œMeasuring
user influence in twitter: The million follower fallacy.â€ ICWSM, vol. 10,
no. 10-17, p. 30, 2010.
[22] S. Fortunato, â€œCommunity detection in graphs,â€ Physics reports, vol.
486, no. 3, pp. 75â€“174, 2010.
[23] L. Weng, F. Menczer, and Y.-Y. Ahn, â€œPredicting successful memes
using network and community structure,â€ in Eighth International AAAI
Conference on Weblogs and Social Media, 2014.
[24] M. Chen, K. Kuzmin, and B. K. Szymanski, â€œCommunity detection
via maximization of modularity and its variants,â€ Computational Social
Systems, IEEE Transactions on, vol. 1, no. 1, pp. 46â€“65, 2014.
[25] E. Bakshy, J. M. Hofman, W. A. Mason, and D. J. Watts, â€œEveryoneâ€™s
an influencer: quantifying influence on twitter,â€ in Proceedings of the
fourth ACM international conference on Web search and data mining.
ACM, 2011, pp. 65â€“74.
[26] D. J. Watts and S. H. Strogatz, â€œCollective dynamics of smallworldnetworks,â€ nature, vol. 393, no. 6684, pp. 440â€“442, 1998.
[27] H. Kwak, C. Lee, H. Park, and S. Moon, â€œWhat is twitter, a social
network or a news media?â€ in Proceedings of the 19th international
conference on World wide web. ACM, 2010, pp. 591â€“600.
[28] L. Breiman, â€œRandom forests,â€ Machine learning, vol. 45, no. 1, pp.
5â€“32, 2001.

[29] Y. Freund, R. Schapire, and N. Abe, â€œA short introduction to boosting,â€
Journal-Japanese Society For Artificial Intelligence, vol. 14, no. 771780, p. 1612, 1999.
[30] J. Zhu, H. Zou, S. Rosset, and T. Hastie, â€œMulti-class adaboost,â€
Statistics and its Interface, vol. 2, no. 3, pp. 349â€“360, 2009.

arXiv:1607.02171v1 [cs.AI] 7 Jul 2016

Argumentation Models for Cyber Attribution
Eric Nunes, Paulo Shakarian

Gerardo I. Simari

Andrew Ruef

Arizona State University
Tempe, AZ 85281, USA
Email: {enunes1, shak} @asu.edu

Inst. for CS and Eng. (CONICETâ€“UNS)
DCIC, UNS, Bahia Blanca, Argentina
Email: gis@cs.uns.edu.ar

Trail of Bits, Inc.
New York, NY 10003, USA
Email: andrew@trailofbits.com

Abstractâ€”A major challenge in cyber-threat analysis is combining information from different sources to find the person or
the group responsible for the cyber-attack. It is one of the most
important technical and policy challenges in cyber-security. The
lack of ground truth for an individual responsible for an attack
has limited previous studies. In this paper, we take a first step
towards overcoming this limitation by building a dataset from
the capture-the-flag event held at DEFCON, and propose an
argumentation model based on a formal reasoning framework
called DeLP (Defeasible Logic Programming) designed to aid
an analyst in attributing a cyber-attack. We build models from
latent variables to reduce the search space of culprits (attackers),
and show that this reduction significantly improves the performance of classification-based approaches from 37% to 62% in
identifying the attacker.

I. I NTRODUCTION
A major challenge in cyber-threat analysis is to find the
person or the group responsible for a cyber-attack. This is
known as cyber-attribution [17] and it is one of the central
technical and policy challenges in cyber-security. Oftentimes,
the evidence collected from multiple sources provides a contradictory viewpoint. This gets worse in cases of deception
where either an attacker plants false evidence or the evidence
points to multiple actors, leading to uncertainty. In the text
on cyber-warfare [17] the authors discuss the difficulties that
an intelligence analyst faces in attributing an attack to a
perpetrator given that deception might have occurred, and how
the analyst needs to explore deception hypotheses under the
given attack scenario.
However, one of the major drawbacks of the study and
evaluation of cyber-attribution models is the lack of datasets
with the ground truth available regarding the individual party
responsible for the attackâ€”this has limited previous studies.
To overcome this, we built and leveraged a dataset from the
capture-the-flag event held at DEFCON. In previous work,
this dataset was used to study cyber-attribution, framing it
as a multi-label classification problem to predict the attacker [13]. Machine learning approaches struggle in situations
of deception, where similar attributes point towards multiple
attackersâ€”we propose to address this issue using a formal
logical framework.
Specific contributions of this paper include:
â€¢

description of how a model for cyber-attribution can
be designed and implemented in the DeLP structured
argumentation framework;

â€¢

â€¢

experiments demonstrating that using argumentationbased tools can significantly reduce the number of potential culprits that need to be considered in the analysis
of a cyber-attack; and
experiments showing that the reduced set of culprits,
used in conjunction with classification, leads to improved
cyber-attribution decisions.

Related work: Adversarial machine learning is an emerging
field of study. It uses effective machine learning techniques to
identify or defend against an adversaryâ€™s opponents. Understanding the limits of adversaryâ€™s knowledge and capabilities
is crucial for coming up with countermeasures, as discussed
in [9]. Here the authors propose models to study these limitations to come up with evasion techniques. On the contrary,
Lowd and Meek [12] explore the problem from an adversarial
point of view. They propose strategies that an adversary can
use to reverse engineer a classifier so that his attacks are
undetected by the classifier. They use a real world application
in spam filtering to demonstrate their method, which they call
adversarial classifier evasion. In a spam filtering setting an
example of such a technique is replacing feature words that
raise a red flag with their synonyms to evade detection. This
feature cross substitution technique is discussed in [10]. Here
the authors offer a simple heuristic method based on mixedinteger linear programming with constraint generation to make
the classifier robust to cross substitution techniques. There is
research that looks at modeling the interaction between the
learner (adversary) and the classifier in terms of a competition
using Stackelberg games [4], [3]. Most adversarial machine
learning applications deal with modeling classifiers to be robust against evasive techniques in real world applications like
malware detection and spam filtering. Cyber-attribution falls in
the domain of adversarial learning, but looks at analyzing the
evidence in the aftermath of an attack to discover the attacker.
Currently, cyber-attribution is limited to identifying machines [2] as opposed to the hacker or their affiliation to a
group or a state. An example of such a technical attribution
approach is WOMBAT [5], where a clustering technique is
used to group attacks to common IP sources. A method that
combines information from different sources was proposed by
Walls [22], who considered forensic information from diverse
sources but did not account for inconsistency or uncertainty
due to deception. A less rigorous mathematical model, known
as the Q model [15], was proposed recently; the model answers

queries from an analyst, and by combining these answers the
analyst attributes an attack to a party. Unfortunately, there
are no experimental evaluations of its effectiveness. Argumentation has been used for cyber reasoning [1] by leveraging
arguments to deal with incomplete and contradictory data,
allowing to derive big-picture conclusions to keep systems
secure and online in case of an attack. This is a different
application than the one we are addressing.
In [20], a tool was presented to support human decisions, focusing on how user trust in the evidence influences
the process; a user study demonstrating the hypotheses was
presented in [16]. Concurrently, a formal logical framework
for reasoning about cyber-attribution has been devised [18],
[19]; it explores multiple competing hypotheses based on the
evidence for and against a particular attacker to help analysts
decide on an attribution, providing a map of the reasoning that
led to the decision.
The rest of the paper is organized as follows. We present
a description of our DEFCON capture-the-flag dataset and an
analysis on the occurrence of deception within this data in
Section II. This is followed by the argumentative model based
on [8] in Section III. We then summarize results from [13] and
discuss how we built our baseline argumentation model along
with two other extended baseline models for cyber-attribution
with DeLP in Section V and Section VI, with a discussion of
the experimental results obtained with each of these models.
Conclusions are discussed in Section VII.
II. DEFCON CTF DATASET
The DEFCON security conference sponsors and hosts a
capture the flag (CTF) competition every year, held on site
with the conference in Las Vegas, Nevada. DEFCON CTF is
one of the oldest and best-known competitions. The ctftime.org
site provides a ranking for CTF teams and CTF competitions,
and in this system DEFCON CTF has the highest average
weight of all other CTF competitions.
CTF competitions can be categorized by what role the
competitors play in the competition: either red team, blue
team, or a combination. In a blue team focused CTF the
competitors harden their systems against a red team played
by the organizers of the CTF. In a combined red/blue team
CTF every team plays both blue and red team simultaneously.
The NCCDC and CDX competitions are examples of a blue
team CTF, while DEFCON CTF is a combined red/blue team.
Each team is simultaneously responsible for hardening and
defending their systems as well as identifying vulnerabilities
and exploiting them in other teamsâ€™ systems.
The game environment is created primarily by the DEFCON
CTF organizers. The game focuses around programs (known in
the game as services) written by the organizers. These services
are engineered to contain specific vulnerabilities. The binary
image of the service is made available to each team at the
start of the game, but no other information about the service
is released. Part of the challenge of the game is identifying the
purpose of each service as well as the vulnerabilities present
in the service. Identification of vulnerabilities serves both a

defensive and offensive goal. Once a vulnerability has been
identified, a team may patch this vulnerability in the binary
program. Additionally, the teams may create exploits for that
vulnerability and use them to attack other teams and capture
digital flags from those teamsâ€™ systems.
Each team is also provided with a server running the
services, which contains the digital flags to be defended. To
deter defensive actions such as powering off the server or
stopping the services, the white team (a third team, played
by the organizers) conducts periodic availability tests of the
services running on each teamâ€™s server. A teamâ€™s score is the
sum of the value of the flags they have captured, minus the sum
of the flags that have been captured from that team, multiplied
by an availability score determined by how often the white
team was able to test that teamâ€™s services. This scoring model
incentivizes teams to keep their server online, identify the
vulnerabilities in services and patch them quickly, and exploit
other teams services to capture their flags. It disincentivizes
teamsâ€™ from performing host-level blocking and shutting down
services, as this massively impacts their final score.
This game environment can be viewed as a microcosm of
the global Internet and the careful game of cat and mouse
between hacking groups and companies. Teams are free to use
different technical means to discover vulnerabilities. They may
use fuzzing and reverse engineering on their own programs,
or, they may monitor the network data sent to their services
and dynamically study the effects that network data has on
unpatched services. If a team discovers a vulnerability and uses
it against another team, the first team may discover that their
exploit is re-purposed and used against them within minutes.
The organizers of DEFCON CTF capture all of the network
traffic sent and received by each team, and publish this traffic
at the end of the competition [6]. This includes IP addresses
for source and destination, as well as the full data sent and
received and the time the data was sent or received. This data
is not available to contestants; depending on the organizersâ€™
choice from year to year, the contestants either have a real time
feed but with the IP address obscured, or a full feed delivered
on a time delay of minutes to hours.
Analysis: We use the data from the CTF tournament held at
DEFCON 21 in 2013. The CTF data set is very large, about
170 GB in compressed format. We used multiple systems
with distributed and coordinated processing to analyze the
entire datasetâ€”fortunately, analyzing individual streams is
easy to parallelize. To analyze this data, we identified the
TCP ports associated with each vulnerable service. From
this information, we used the open source tool tcpflow to
process the network captures into a set of files, with each file
representing data sent or received on a particular connection.
With these data files identified, we analyzed some of them
by hand using the Interactive Disassembler (IDA) to determine
if the data contained shell-code, which in fact was the case. We
used an automated tool to produce a summary of each data
file as a JSON encoded element. Included in this summary
was a hash of the contents of the file and a histogram of the
processor instructions contained in the file. These JSON files

TABLE 1: Fields in an instance of network attack
Field

Intuition

Value

byte hist

Histogram of byte sequences
in the payload

0Ã—43:245, 0Ã—69:8,
0Ã—3a:9, .....

inst hist

Histogram of instructions
used in the payload

cmp:12,
subs:8,
movtmi:60 ......

from team

The team where the payload
originates (attacking team)

Blue Lotus

to team

The team being attacked by
the exploit

Robot Mafia

time

Indicates the date and time of
the attack

2013-08-03T23:45:17

were the final output of the low-level analysis, transforming
hundreds of gigabytes of network traffic into a manageable set
of facts about exploit traffic in the data. Each JSON file is a list
of tuples (time-stamp, byte-histogram, instruction-histogram,
attack team and target team). The individual fields of the tuple
are listed in Table 1.
The pre-processing can be summarized in the following
steps:
â€¢ Un-tarring the archives available from the organizers. The
archives produce a large number of pcap-ng formatted
files that contain the traffic captures.
â€¢ Conversion of the pcap-ng files to tcpdump format capture using the editcap utility. This is to allow tcpflow to
process the data.
â€¢ Use of xargs and GNU parallel to run tcpflow on each
pcap. This is a time-consuming process, and produced a
directory structure with files for data sent and received
on host-port socket pairs. This step of processing allows
file-based tools to process the network data.
â€¢ A tool to process each file containing data sent or received
by network ports associated with CTF challenges. These
tools produced summary statistics for each data stream,
to include a byte histogram, overall size, a hash, and an
ARM instruction histogram (we ran a linear sweep with
the Capstone instruction decoder to produce this). This
data was saved via JSON.
After this pre-processing of the network data packets, we
have around 10 million network attacks consisting of around
1 million unique exploits built and used by 20 teams in the
competition. In order to attribute an attack to a particular
team, apart from analyzing the payloads used by the team,
we also need to analyze the behavior of the attacking team
towards their adversary. For this purpose, we divide the attacks
according to the team being targeted. Thus, we have 20 such
subsets, which we represent as T-i, where i âˆˆ {1, 2, 3, ..., 20}.
The processed dataset is publicly available 1 .
We now discuss two important observations from the
dataset, which make the task of attributing an observed network attack to a team difficult.
Deception: In the context of this paper we define an attack to
1 http://lab.engineering.asu.edu/cysis/cyber-attribution/

be deceptive when multiple adversaries get mapped to a single
attack pattern; deception is thus a scenario in which the same
exploit is used by multiple teams to target the same team. The
number of unique deceptive attacks amount to just under 35%
of the total unique attacks in our datasetâ€”clearly, deception
is a heavily-used technique in this domain.
Duplicate attacks: A duplicate attack occurs when the same
team uses the same payload to attack the same team at different
points in time. We group duplicates as either being nondeceptive or deceptive. Non-deceptive duplicates are the copies
of the attacks launched by the team that first initiated the use
of a particular payload; on the other hand, deceptive duplicates
are all the attacks from the teams that did not initiate the use.
III. A RGUMENTATION M ODEL
Our approach relies on a model of the world where we can
analyze competing hypotheses in a cyber-operation scenario.
Such a model should allow for contradictory information so it
can handle inconsistency in cases of deception.
Before describing the argumentation model in detail, we
introduce some necessary notation. Variables and constant
symbols represent items such as the exploits/payloads used
for the attack, and the actors conducting the cyber-attack (in
this case, the teams in the CTF competition). We denote the
set of all variable symbols with V and the set of all constants
with C. For our model we require two subsets of C: Cact ,
denoting the actors capable of conducting the cyber-operation,
and Cexp , denoting the set of unique exploits used. We use
symbols in all capital letters to denote variables. In the running
example, we use a subset of our DEFCON CTF dataset.
Example 1. Actors and cyber-operations from the CTF
data: Cact = {bluelotus, robotmafia, apt8}, Cexp =
{exploit1 , exploit2 , ..., exploitn }.
The language also contains a set of predicate symbols that
have constants or variables as arguments, and denote events
that can be either true or false. We denote the set of predicates
with P; examples of predicates are shown in Table 2. For
instance, culprit(exploit1 , apt8) will either be true or false, and
denotes the event where apt8 used exploit1 to conduct a cyberoperation.
TABLE 2: Example predicates and explanation
Predicate

Explanation

attack(exploit1 , bluelotus)

exploit1 was targeted towards the
team Blue Lotus.

replay attack(E, Y)

Exploit E was replayed by team Y.

deception(exploit1 , apt8)

Team apt8 used exploit1 for deception.

time diff(I, Y)

Team Y was deceptive within the
given time interval I.

culprit(exploit1 , apt8)

Team apt8 is the likely culprit for
the attack (using exploit1 on the
target team).

A ground atom is composed by a predicate symbol and
a tuple of constants, one for each argument. The set of all

ground atoms is denoted as G. A ground literal L is a ground
atom or a negated ground atom; hence, ground literals have
no variables. An example of a ground atom for our running
example is attack(exploit1 , bluelotus). We denote a subset of
G with G0 .
We choose a structured argumentation framework [14] for
our model; our approach works by creating arguments (in
the form of a set of rules and facts) that compete with each
other to attribute an attack to a given perpetuator. In this case,
arguments are defeated based on contradicting information in
other arguments. This procedure is known as a dialectical
process, where the arguments that are undefeated prevail.
An important result is the set of all the arguments that are
warranted (not defeated) by any other argument, which give
a clear map supporting the conclusion. Such transparency
lets a security analyst not only add new arguments based
on new evidence discovered in the system, but also get rid
of incorrect information and fine-tune the model for better
performance. Since the argumentation model can deal with
inconsistent information, it draws a natural analogy to the way
humans settle disputes when there is contradictory information
available. Having a clear explanation of why one argument is
chosen over others is a desirable characteristic for both the
analyst and for organizations to make decisions and policy
changes. We now briefly discuss some preliminaries on DeLP.
Defeasible Logic Programming: DeLP is a formalism that
combines logic programming with defeasible argumentation;
full details are discussed in [8]. The formalism is made up
of several constructs, namely facts, strict rules, and defeasible
rules. Facts represent statements obtained from evidence, and
are always true; similarly, strict rules are logical combinations
of elements (facts or other inferences) that can always be
performed. On the contrary, defeasible rules can be thought
of as strict rules that may be true in some situations, but
could be false if contradictory evidence is present. These three
constructs are used to build arguments, and DeLP programs
are sets of facts, strict rules and defeasible rules. We use the
usual notation for DeLP programs, denoting the knowledge
base with Î  = (Î˜, â„¦, âˆ†), where Î˜ is the set of facts, â„¦ is
the set of strict rules, and âˆ† is the set of defeasible rules.
Examples of the three constructs are provided with respect to
the dataset in Fig. 1. We now describe the constructs in detail.
Facts (Î˜) are ground literals that represent atomic information
or its (strong) negation (Â¬).
Strict Rules (â„¦) represent cause and effect information; they
are of the form L0 â† L1 , ...Ln , where L0 is a literal and
{Li }i>0 is a set of literals.
Defeasible Rules (âˆ†) are weaker versions of strict rules, and
are of the form L0 -â‰º L1 , ...., Ln , where L0 , is the literal and
{Li }i>0 is a set of literals.
When a cyber-attack occurs, the model can be used to derive
arguments as to who could have conducted the attack. Derivation follows the same mechanism as logic programming [11].
DeLP incorporates defeasible argumentation, which decides
which arguments are warranted and it blocks arguments that

Î˜:

Î¸1
Î¸2
Î¸3
Î¸4
Î¸5

=
=
=
=
=

â„¦:

Ï‰1 =
Ï‰2 =

âˆ†:

Î´1 =
Î´2 =
Î´3 =
Î´4 =

attack(exploit1 , bluelotus)
first attack(exploit1 , robotmafia)
last attack(exploit1 , apt8))
time diff(interval, robotmafia)
most frequent(exploit1 , pwnies)
culprit(exploit1 , pwnies) â†
most frequent(exploit1 , pwnies),
replay attack(exploit1 )
Â¬ culprit(exploit1 , robotMafia) â†
last attack(exploit1 , apt8),
replay attack(exploit1 )
replay attack(exploit1 ) -â‰º
attack(exploit1 , bluelotus),
last attack(exploit1 , apt8)
deception(exploit1 , apt8) -â‰º
replay attack(exploit1 ),
first attack(exploit1 , robotmafia)
culprit(exploit1 , apt8) -â‰º
deception(exploit1 , apt8),
replay attack(exploit1 )
Â¬culprit(exploit1 , apt8) -â‰º
time diff(interval, robotmafia)

Fig. 1: A ground argumentation framework.
hA1 ,
hA2 ,
hA3 ,
hA4 ,

replay attack(exploit1 ) i
deception(exploit1 , apt8) i
culprit(exploit1 , apt8)i
Â¬culprit(exploit1 , apt8)i

A1
A2
A3
A4

= {Î´1 , Î¸1 , Î¸3 }
= {Î´1 , Î´2 , Î¸2 }
= {Î´1 , Î´2 , Î´3 }
= {Î´1 , Î´4 , Î¸3 }

Fig. 2: Example ground arguments from Figure 1.

are in conflict and a winner cannot be determined. Fig. 1 shows
a ground argumentation framework demonstrating constructs
derived from the CTF data. For instance, Î¸1 indicates the fact
that exploit1 was used to target the team Blue Lotus, and Î¸5
indicates that team pwnies is the most frequent user of exploit1 .
For the strict rules, Ï‰1 says that for a given exploit1 the attacker
is pwnies if it was the most frequent attacker and the attack
exploit1 was replayed. Defeasible rules can be read similarly;
Î´2 indicates that exploit1 was used in a deceptive attack by
APT8 if it was replayed and the first attacker was not APT8.
By replacing the constants with variables in the predicates we
can derive a non-ground argumentation framework.
Definition 1. (Argument) An argument for a literal L is a
pair hA, Li, where A âŠ† Î  provides a minimal proof for L
meeting the requirements: (1) L is defeasibly derived from A2 ,
(2) Î˜ âˆª â„¦ âˆª âˆ† is not contradictory, and (3) A is a minimal
subset of âˆ† satisfying 1 and 2, denoted hA, Li.
Literal L is called the conclusion supported by the argument, and A is the support. An argument hB, Li is a
subargument of hA, L0 i iff B âŠ† A. The following examples
show arguments for our scenario.
Example 2. Fig. 2 shows example arguments based on the
2 This means that there exists a derivation consisting of a sequence of rules
that ends in Lâ€”that possibly includes defeasible rules.




KB from Fig. 1;
 here, A1 , replay attack(exploit
 1 ) 
is a
subargument of A2 , deception(exploit1 , apt8) and A3 ,
culprit(exploit1 , apt8) .
For a given argument there may be counter-arguments that
contradict it. For instance, referring to Fig. 2, we can see that
A4 attacks A3 . A proper defeater of an argument hA, Li is
a counter-argument thatâ€”by some criterionâ€”is considered to
be better than hA, Li; if the two are incomparable according
to this criterion, the counterargument is said to be a blocking
defeater. The default criterion used in DeLP for argument
comparison is generalized specificity [21].
A sequence of arguments is called an argumentation line.
There can be more than one defeater argument, which leads to
a tree structure that is built from the set of all argumentation
lines rooted in the initial argument. In this dialectical tree,
every child can defeat its parent (except for the root), and the
leaves represent unchallenged arguments; this creates a map
of all possible argumentation lines that decide if an argument
is defeated or not. Arguments that either have no attackers or
all attackers have been defeated are said 
to be warranted.
Given a literal L and an argument A, L , in order to
decide whether or not a literal L is warranted, every node
in the dialectical tree T (hA, Li) is recursively marked as â€œDâ€
(defeated) or â€œUâ€ (undefeated), obtaining a marked dialectical
tree T âˆ— (hA, Li) where:
âˆ—
â€¢ All leaves in T (hA, Li) are marked as â€œUâ€s, and
âˆ—
â€¢ Let hB, qi be an inner node of T (hA, Li). Then, hB, qi
will be marked as â€œUâ€ iff every child of hB, qi is marked
as â€œDâ€. Node hB, qi will be marked as â€œDâ€ iff it has at
least one child marked as â€œUâ€.
Given argument hA, Li over Î , if the root of T âˆ— (hA, Li)
is marked â€œUâ€, then T âˆ— (hA, hi) warrants L and that L is
warranted from Î . (Warranted arguments correspond to those
in the grounded extension of a Dung argumentation system
[7].)
In practice, an implementation of DeLP accepts as input sets
of facts, strict rules, and defeasible rules. Note that while the
set of facts and strict rules is consistent (non-contrdictory),
the set of defeasible rules can be inconsistent. We engineer
our cyber-attribution framework as a set of defeasible and
strict rules whose structure was created manually, but are
dependent on values learned from a historical corpus of data.
Then, for a given incident, we instantiate a set of facts for
that situation. This information is then provided as input
into a DeLP implementation that uses heuristics to generate
all arguments for and against every possible culprit for the
cyber attack. Dialectical trees based on these arguments are
analyzed, and a decision is made regarding which culprits are
warranted. This results in a reduced set of potential culprits,
which we then use as input into a classifier to obtain an
attribution decision.
IV. BASELINE A RGUMENTATION M ODEL (BM)
In [13] machine learning techniques were leveraged on the
CTF data to identify the attacker. We will now provide a sum-

Ï‰1 =

culprit(E, Y) â† last attack(E, Y), replay attack(E).

Î´1 =

replay attack(E) -â‰º attack(E, X), last attack(E, Y).

Fig. 3: Defeasible and strict rule for non-deceptive attack.

mary of the results obtained. The experiment was performed
as follows. The dataset was divided according to the target
team, building 20 subsets, and all the attacks were then sorted
according to time. The first 90% of the attacks were reserved
for training and the remaining 10% for testing. The byte and
instruction histograms were used as features to train and test
the model. Models constructed using a random forest classifier
performed the best, with an average accuracy of 0.37. Most
of the misclassified samples tend to be deceptive attacks and
their duplicates.
When using machine learning approaches it is difficult to
map the reasons why a particular attacker was predicted,
especially in cases of deception where multiple attackers were
associated with the same attack. Knowing the arguments that
supported a particular decision would greatly aid the analyst in
making better decisions dealing with uncertainty. To address
this issue we now describe how we can form arguments/rules
based on the latent variables computed from the training data,
given an attack for attribution.
We use the following notation: let E be the test attack
under consideration aimed at target team X, Y represent all
the possible attacking teams, and D be the set of all deceptive
teams (those using the same payload to target the same team)
if the given attack is deceptive in the training set. For nondeceptive attacks, D will be empty. We note that facts cannot
have variables, only constants (however, to compress the
program for presentation purposes, we use meta-variables in
facts). To begin, we define the facts: Î¸1 = attack (E, X), Î¸2 =
first attack (E, Y), Î¸3 = last attack (E, Y); Î¸1 states that
attack E was used to target team X, Î¸2 states that team Y
was the first team to use the attack E in the training data, and
similarly Î¸3 states that team Y was the last team to use the
attack E in the training data. The first and last attacking team
may or may not be the same. We study the following three
cases:
Case 1: Non-deceptive attacks. In non-deceptive attacks, only
one team uses the payload to target other teams in the training
data. It is easy to predict the attacker for these cases, since
the search space only has one team. To model this situation,
we define a set of defeasible and strict rules.
In Fig. 3, defeasible rule Î´1 checks whether the attack was
replayed in the training data. Since it is a non-deceptive attack,
it can only be replayed by the same team. The strict rule Ï‰1
then puts forth an argument for the attacker (culprit) if the
defeasible rule holds and there is no contradiction for it.
Case 2: Deceptive attacks. These attacks form the majority
of the misclassified samples in [13]. The set D is not empty
for this case; let Di denote the deceptive teams in D. We
also compute the most frequent attacker from the training
data given a deceptive attack. Let the most frequent deceptive

Î¸1 = decep (E, X), Î¸2 = frequent (E, F )

Attacker Time Analysis: The CTF data provides us with time
stamps for the attacks in the competition. We can use this
information to come up with rules for/against an argument for
a team being the attacker. We compute the average time for a
team to replay its own attack given that it was the first one to
deploy the attack (see Fig. 5). It can be observed that teams
like more smoked leet chicken (T-13) and Wowhacker-bios (T8) are very quick to replay their own attacks as compared to
other teams. Fig. 5 also shows the average time for a team
to perform a deceptive attack. Teams like The European (T-7)
and Blue lotus (T-10) are quick to commit deception, while
others take more time.
We use this time information to narrow down our search
space for possible attackers. In particular, for a deceptive
test sample, we compute the time difference between the test
sample and the training sample that last used the same payload.
We denote this time difference as 4t, and include it as a

T-20

T-19

T-18

T-17

T-16

T-15

T-14

T-13

T-12

T-11

T-9

Deceptive

attacker be denoted as F . The DeLP components that model
this case are shown in Figure 4; fact Î¸1 indicates if the attack
E was deceptive towards the team X and Î¸2 indicates the most
frequent attacker team F from the training set. The strict rule
Ï‰1 indicates that in case of deception the first team to attack
(Y) is not the attacker, Ï‰2 states that the attacker should be
F if the attack is deceptive and F was the most frequent
deceptive attacker. For the defeasible rules, Î´1 deals with the
case in which the attack E was replayed, Î´2 deals with the
case of deceptive teams from the set D, Î´3 indicates that all the
deceptive teams are likely to be the attackers in the absence of
any contradictory information. and Î´4 states that the attacker
should be F if the attack is deceptive and F was the most
frequent attacker.
Case 3: Previously Unseen Attacks. The most difficult
attacks to attribute in the dataset are the unseen ones, i.e.
attacks first encountered in the test set and thus did not occur
in the training set. To build constructs for this kind of attack
we first compute the k nearest neighbors from the training set
according to a simple Euclidean distance between the byte
and instruction histograms of the two attacks. In this case
we choose k = 3. For each of the matching attacks from
the training data we check if the attack is deceptive or nondeceptive. If non-deceptive, we follow the procedure for Case
1, otherwise we follow the procedure for Case 2. Since we
replace one unseen attack with three seen attacks, the search
space for the attacker increases for unseen attacks.

T-10

T-8

T-7

T-6

1

T-1

Fig. 4: Facts and rules for deceptive attacks.

10

T-5

Î´3 =

100

T-4

replay attack(E) -â‰º attack(E, X), last attack(E, Y)
deception(E, Di ) -â‰º replay attack(E),
first attack(E, Y)
culprit(E, Di ) -â‰º deception(E, Di ), first attack(E, Y)

T-3

Î´1 =
Î´2 =

1000

T-2

Â¬culprit(E, Y) â† first attack(E, Y), decep(E, X)
culprit(E, F ) â† frequent(E, F ), deception (E, Di )

Average time (sec) - Log Scale

Ï‰1 =
Ï‰2 =

10000

Replay

Fig. 5: Average time for team to perform a deceptive attack
and replay its own attack (Log-scale).
Î˜:

Î¸1 =

timedifference (E, X)

âˆ†:

Î´1 =

For Y âˆˆ
/ interval:
Â¬culprit(E, Y) -â‰º timedifference (E, X).

Fig. 6: Time facts and rules. Interval indicates a small portion
of the entire deceptive time (for instance < 2000 sec, > 8000
sec and so on).

fact Î¸1 . We then divide the deceptive times from Fig. 5 into
appropriate intervals; each team is assigned to one of those
time intervals. We then check which time interval 4t belongs
to and define a defeasible rule Î´1 that makes a case for all
teams not belonging to the interval to not be the culprits, as
shown in Fig. 6.
We now provide a summary of the experimental resultsâ€”
the setup is similar to [13]: the dataset is sorted by time for
each target team, the first 90% of the data is used for training
and the remaining 10% for testing. The constructs for all test
samples based on the cases discussed in the previous section
are computed, and these arguments are used as input to the
DeLP implementation. For each test sample the DeLP system
is queried to find all possible attackers (culprits) based on
the arguments provided. If there is no way to decide between
contradicting arguments, these are blocked and thus return no
answers. Initially, the search space for each test sample is 19
teams (all except the one being attacked).
After running the queries to return the set of possible
culprits, the average search space across all target teams is
5.85 teams. This is a significant reduction in search space
across all target teams; to gauge how much the reduced search
space can aid an analyst in predicting the actual culprit, a
metric is computed that checks if the reduced search space
contains the ground truth (actual culprit). For all the target
teams, the ground truth is present on average in almost 66%
of the samples with reduced search space. For some teams
like more smoked leet chicken (T-13) and raon ASRT (whois)
(T-17) the average reduced search space is as low as 1.82 and
2.9 teams, with high ground truth fraction of 0.69 and 0.63,
respectively.
Predictive analysis is then performed on the reduced search

space. The experimental setup is similar to the one described
earlier; the only difference this time is instead of having a 19
team search space as in [13], the machine learning approach is
allowed to make a prediction from the reduced search space
only; a random forest is used for learning, which has been
shown to have the best performance for CTF data [13].
We report the following average accuracies across 20
target teams; the accuracy achieved after running Random
forest without applying the argumentation-based techniques,
as reported in [13], is 0.37. This was the best performing
approach using standard machine learning techniques. The
baseline model achieves an average accuracy of 0.5, which is
significantly better than the average accuracy of 0.37 in [13].
V. E XTENDED BASELINE M ODEL I (EB1)
Previously Unseen attacks make up almost 20% of the test
samples for each target team. On analyzing the misclassification from the baseline argumentation model, we observe that
the majority of the previously unseen attacks get misclassified
(>80%). The misclassifications can be attributed to two reasons: (i) the reduced search space is not able to capture the
ground truth for unseen attacks, leading the learning model to
a wrong prediction; and (ii) we represent each unseen attack
by the 3 most similar attacks in the training data; this leads
to an increase in the search space and many choices for the
learning model.
We address these issues by proposing two sets of defeasible
rules. First, for each target team we compute from the training
set the top 3 teams that come up with the most unique exploits,
as these teams are more likely to launch an unseen attack in
the test set. The intuition behind this rule is the fact that not
all teams write their own exploits, most teams just capture a
successful exploit launched by other teams and repackage it
and use it as their own (deception). The second set of rules
is proposed to avoid addition of less similar teams to the
reduced search space. In the baseline model we use 3-nearest
neighbors to represent an unseen attack. In this extended
version we consider only the nearest neighbors that are less
than a particular threshold value T , which is decided for each
target team separately. So, each attack will be represented by
k â‰¤ 3 teams depending upon the threshold requirement. In
addition to the baseline model rules, we propose the following
rules for deceptive attacks. Let U denote the set of teams with
the three highest numbers of unique attacks in the training
data. Also, let N denote the set of three most similar culprits
for the given unseen attack.
The extended model is shown in Fig. 7; the fact Î¸1 indicates
the teams present in N and whose similarity is less than a
particular threshold T , and Î¸2 indicates if the team ui was
one of most unique attackers from set U. For the defeasible
rules, Î´1 makes use of the fact Î¸1 stating that the teams in N
that satisfy the threshold condition are likely to be the culprits,
and Î´2 indicates that if ui is a unique attacker then it can be
the culprit unless contradictory information is available. U is
independent of the test samples and will be the same for all
unseen attacks given a target team.

Î˜:

Î¸1 =
Î¸2 =

âˆ†:

Î´1 =
Î´2 =

For (ni âˆˆ N and sim < T ):
threshold(E, T )
For ui in U:
unique(E, ui )
culprit(E, ui ) -â‰º threshold(E, T )
For ui âˆˆ U:
culprit(E, ui ) -â‰º unique(E, ui )

Fig. 7: Rules for unseen attacks.
Î˜:

Î¸1 =

timedifference (E, X)

âˆ†:

Î´1 =

For Y âˆˆ interval:
culprit(E, Y) -â‰º timedifference (E, X).

Fig. 8: Time facts and rules. Interval indicates a small portion
of the entire deceptive time (for instance < 2000 sec, > 8000
sec and so on).

On the contrary, for each of the similar payloads (three or
fewer) computed from the training data we check if the attack
is deceptive or non-deceptive. If non-deceptive, we follow the
procedure for Case 1, otherwise we follow the procedure for
Case 2 stated in the baseline argumentation model.
Experiment: We evaluate EB1 using an experimental setup
similar to the one for the baseline argumentation model.
We report the average reduced search space and prediction
accuracy for both EB1 and baseline model to provide a
comparison. EB1 performs better than the baseline with an
average accuracy of 0.53 vs. 0.50, and significantly better than
the machine learning model without argumentation that has an
average accuracy of 0.37. The improvement in performance
is due to the larger fraction of reduced search spaces with
ground truth present in them. Also, the search space reduced
from on average 6.07 teams to 5.025 (less teams to consider).
The results are reported in Table 3 along with a comparison
to the second extended baseline argumentation model (EB2).
VI. E XTENDED BASELINE M ODEL II (EB2)
Another source of misclassification in the baseline argumentation model is the presence of unseen deceptive teams
and their duplicates. These refer to teams that did not use the
exploit in the training set but started using it in the test set. It
is difficult for a machine learning approach to predict such a
team as being the culprit if it has not encountered it using the
exploit in the training set. In our dataset these attacks comprise
15% of the total, and up to 20% for some teams.
In order to address this issue we propose an extension
of EB1, where we group together teams that have similar
deceptive behavior based on the time information available to
us from the training set; for instance teams that are deceptive
within a certain interval of time (e.g., less than 2,000 secs.)
after the first attack has been played are grouped together. For
a given test attack we compute the time difference between
the test attack and the last time the attack was used in the
training set. We then assign this time difference to a specific

group based on which interval the time difference falls in.
In order to fine tune the time intervals, instead of using the
average deceptive times averaged across all target teams (as
used in the baseline model), we compute and use deceptive
times for each target team separately. We model the time rules
as stated in Fig. 8; fact Î¸1 states the time difference between
the test sample and the last training sample to use that attack,
defeasible rule Î´1 on the other hand states that teams belonging
to that interval (in which the time difference lies) are likely to
be the culprits unless a contradiction is present. It is clear that
this rule will increase the search space for the test sample,
as additional teams are now being added as likely culprits.
We observe that for EB2 the search space is increased by an
average of almost 2.5 teams per test sample from EB1; at the
same time the presence of ground truth in the reduced search
space increased to 0.78, which is a significant improvement
over 0.68.
Experiment: We evaluate EB2 using an experimental setup
similar to the one discussed in the baseline argumentation
model. We report the prediction accuracies for each of the
proposed baseline argumentation models for each of the target
teams and compare it with the previous accuracy reported
in [13], denoted as ML. In Table 3 the second extended
baseline model (EB2) performs the best with an average
prediction accuracy of 62% as compared to other proposed
methods. The additions of teams based on time rules not only
benefits detection of unseen deceptive teams but it also helps
in predicting attackers for unseen attacks. The major reason
for the jump in performance is that for most unseen deceptive
team samples, the time rules proposed in the baseline model
block all deceptive teams from being the culprit, leading to
an empty set of culprits. The new set of rules proposed in
EB2 adds similar-behaving teams to this set based on time
information; the learning algorithm can then predict the right
one from this set.
VII. C ONCLUSION
In this paper we demonstrated how an argumentationbased framework (DeLP) can be leveraged to improve cyberattribution decisions by building DeLP programs based on
CTF data; this affords a reduction of the set of potential
culprits and thus greater accuracy when using a classifier for
cyber attribution. We are currently looking at implementing a
probabilistic variant of DeLP [19], as well as designing our
own CTF event in order to better mimic real-world scenarios.
Our new CTF will encourage deceptive behavior among the
participants, and we are also enhancing our instrumentation of
the CTF, allowing for additional data collection (host data is
of particular interest).
VIII. ACKNOWLEDGMENTS
Authors of this work were supported by the U.S. Department of the Navy, Office of Naval Research, grant N00014-151-2742 as well as the Arizona State University Global Security
Initiative (GSI) and by CONICET and Universidad Nacional
del Sur, Argentina.

TABLE 3: Results Summary
Team

ML [13]

BM

EB1

EB2

T-1

0.45

0.51

0.52

0.60

T-2

0.22

0.45

0.38

0.43

T-3

0.30

0.40

0.47

0.66

T-4

0.26

0.44

0.42

0.44

T-5

0.26

0.45

0.45

0.56

T-6

0.5

0.49

0.55

0.7

T-7

0.45

0.53

0.56

0.66

T-8

0.42

0.61

0.58

0.74

T-9

0.41

0.50

0.53

0.76

T-10

0.30

0.42

0.41

0.41

T-11

0.37

0.44

0.5

0.73

T-12

0.24

0.43

0.36

0.52

T-13

0.35

0.63

0.64

0.75

T-14

0.42

0.52

0.53

0.67

T-15

0.30

0.38

0.55

0.64

T-16

0.43

0.48

0.55

0.65

T-17

0.42

0.58

0.58

0.68

T-18

0.48

0.50

0.52

0.65

T-19

0.41

0.51

0.56

0.68

T-20

0.48

0.51

0.64

0.71

R EFERENCES
[1] A. Applebaum, K. Levitt, Z. Li, S. Parsons, J. Rowe, and E. Sklar.
Cyber reasoning with argumentation: Abstracting from incomplete and
contradictory evidence. In Proc. of MILCOM, 2015.
[2] W. E. Boebert. A survey of challenges in attribution. In Proc. of a
workshop on Deterring CyberAttacks, pages 41â€“54, 2010.
[3] M. BruÌˆckner, C. Kanzow, and T. Scheffer. Static prediction games
for adversarial learning problems. The Journal of Machine Learning
Research, 13(1):2617â€“2654, 2012.
[4] M. BruÌˆckner and T. Scheffer. Stackelberg games for adversarial prediction problems. In Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 547â€“555.
ACM, 2011.
[5] M. Dacier, V.-H. Pham, and O. Thonnard. The wombat attack attribution
method: some results. In Information Systems Security, pages 19â€“37.
Springer, 2009.
[6] DEFCON. DEFCON: Capture the flag. https://media.defcon.org/, 2013.
[Online; accessed January-2015].
[7] P. M. Dung. On the acceptability of arguments and its fundamental role
in nonmonotonic reasoning, logic programming and n-person games.
Artificial intelligence, 77(2):321â€“357, 1995.
[8] A. J. GarcÄ±Ìa and G. R. Simari. Defeasible logic programming: An
argumentative approach. Theory and practice of logic programming,
4(1+ 2):95â€“138, 2004.
[9] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar.
Adversarial machine learning. In Proceedings of the 4th ACM workshop
on Security and artificial intelligence, pages 43â€“58. ACM, 2011.
[10] B. Li and Y. Vorobeychik. Feature cross-substitution in adversarial
classification. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence,
and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2087â€“2095. Curran Associates, Inc., 2014.
[11] J. W. Lloyd. Foundations of logic programming. Springer Science &
Business Media, 2012.
[12] D. Lowd and C. Meek. Adversarial learning. In Proceedings of
the eleventh ACM SIGKDD international conference on Knowledge
discovery in data mining, pages 641â€“647. ACM, 2005.

[13] E. Nunes, N. Kulkarni, P. Shakarian, A. Ruef, and J. Little. Cyberdeception and attribution in capture-the-flag exercises. In Proceedings
of the 2015 IEEE/ACM International Conference on Advances in Social
Networks Analysis and Mining, ASONAM 2015, Paris, France, August
25 - 28, 2015, pages 962â€“965, 2015.
[14] I. Rahwan, G. R. Simari, and J. van Benthem. Argumentation in artificial
intelligence, volume 47. Springer, 2009.
[15] T. Rid and B. Buchanan. Attributing cyber attacks. Journal of Strategic
Studies, 38(1-2):4â€“37, 2015.
[16] J. Salvit, Z. Li, S. Perumal, H. Wall, J. Mangels, S. Parsons, and E. I.
Sklar. Employing argumentation to support human decision making:
A user study. In AAMAS Workshop on Argumentation in Multiagent
Systems, 2014.
[17] P. Shakarian, J. Shakarian, and A. Ruef. Introduction to cyber-warfare:
A multidisciplinary approach. Elsevier, 2013.
[18] P. Shakarian, G. I. Simari, G. Moores, and S. Parsons. Cyber attribution:
An argumentation-based approach. In Cyber Warfare: Building the
Scientific Foundation, pages 151â€“171. Springer, 2015.
[19] P. Shakarian, G. I. Simari, G. Moores, D. Paulo, S. Parsons, M. Falappa,
and A. Aleali. Belief revision in structured probabilistic argumentation.
Annals of Mathematics and Artificial Intelligence, pages 1â€“43, 2015.
[20] E. I. Sklar, S. Parsons, Z. Li, J. Salvit, S. Perumal, H. Wall, and J. Mangels. Evaluation of a trust-modulated argumentation-based interactive
decision-making tool. Autonomous Agents and Multi-Agent Systems,
pages 1â€“38, 2015.
[21] F. Stolzenburg, A. J. GarcÄ±Ìa, C. I. Chesnevar, and G. R. Simari.
Computing generalized specificity. Journal of Applied Non-Classical
Logics, 13(1):87â€“113, 2003.
[22] R. J. Walls. Inference-based Forensics for Extracting Information from
Diverse Sources. PhD thesis, University of Massachusetts Amherst,
2014.

Darknet and Deepnet Mining for Proactive
Cybersecurity Threat Intelligence
Eric Nunes, Ahmad Diab, Andrew Gunn, Ericsson Marin , Vineet Mishra,
Vivin Paliath, John Robertson, Jana Shakarian, Amanda Thart, Paulo Shakarian

arXiv:1607.08583v1 [cs.CR] 28 Jul 2016

Arizona State University
Tempe, AZ 85281, USA
Email: {enunes1, ahmad.diab, andrewgunn, ericsson.marin, vvmishra,
vivin.paliath, jj.robertson, jshak, amanda.thart, shak} @asu.edu

Abstractâ€”In this paper, we present an operational system
for cyber threat intelligence gathering from various social platforms on the Internet particularly sites on the darknet and
deepnet. We focus our attention to collecting information from
hacker forum discussions and marketplaces offering products and
services focusing on malicious hacking. We have developed an
operational system for obtaining information from these sites for
the purposes of identifying emerging cyber threats. Currently, this
system collects on average 305 high-quality cyber threat warnings
each week. These threat warnings include information on newly
developed malware and exploits that have not yet been deployed
in a cyber-attack. This provides a significant service to cyberdefenders. The system is significantly augmented through the use
of various data mining and machine learning techniques. With
the use of machine learning models, we are able to recall 92%
of products in marketplaces and 80% of discussions on forums
relating to malicious hacking with high precision. We perform
preliminary analysis on the data collected, demonstrating its
application to aid a security expert for better threat analysis.

I.

I NTRODUCTION

Pre-reconnaissance cyber threat intelligence refers to information gathered before a malicious party interacts with
the defended computer system. An example demonstrating the
importance of cyber threat intelligence is shown in Table 1.
A Microsoft Windows vulnerability was identified in Feb.
2015. The release of the vulnerability was essentially Microsoft
warning its customers of a security flaw. Note that at this time,
there was no publicly known method to leverage this flaw in a
cyber-attack (i.e. an available exploit). However, about a month
later an exploit was found to be on sale in darknet market. It
was not until July when FireEye, a major cybersecurity firm,
identified that the Dyre Banking Trojan designed to steal credit
cards exploited this vulnerability - the first time an exploit
was reported. This vignette demonstrates how threat warnings
gathered from the darknet can provide valuable information
for security professionals. The average global exposure of the
Dyre Banking Trojan was 57.3% along with another banking
malware Dridex1 . It means that nearly 6 out of 10 organizations
in the world were affected, and this is a significantly high
number on a global level.
In this paper, we examine how such intelligence can be
gathered and analyzed from various social platforms on the In1 https://www.fireeye.com/blog/threat-research/2015/06/evolution

of dridex.html

TABLE 1: Exploit example.
Timeline

Event

Feb. 2015

Microsoft identifies Windows vulnerability MS15010/CVE 2015-0057 for remote code execution. There
was no publicly known exploit at the time the vulnerability was released.

April 2015

An exploit for MS15-010/CVE 2015-0057 was found
on a darknet market on sale for 48 BTC (around
$10,000-15,000).

July 2015

FireEye identified that the Dyre Banking Trojan, designed to steal credit card number, actually exploited
this vulnerability1 .

ternet particularly sites on the darknet and deepnet. In doing so,
we encounter several problems that we addressed with various
data mining techniques. Our current system is operational and
actively collecting approximately 305 cyber threats each week.
Table 2 shows the current database statistics. It shows the total
data collected and the data related to malicious hacking. The
vendor and user statistics cited only consider those individuals
associated in the discussion or sale of malicious hackingrelated material, as identified by the system. The data is
collected from two sources on the darknet/deepnet: markets
and forums.
TABLE 2: Current Database Status

Markets

Forums

Total Number

27

Total products

11991

Hacking related

1573

Vendors

434

Total Number

21

Topics/Posts

23780/162872

Hacking related

4423/31168

Users

5491

We are providing this information to cyber-security
professionals to support their strategic cyder-defense planning
to address questions such as, 1) What vendors and users
have a presence in multiple darknet/deepnet markets/ forums?
2)What zero-day exploits are being developed by malicious
hackers? 3) What vulnerabilities do the latest exploits target?

Specific contributions of this paper include, 1) Description
of a system for cyber threat intelligence gathering from various
social platforms from the Internet such as deepnet and darknet
websites. 2) The implementation and evaluation of learning
models to separate relevant information from noise in the
data collected from these online platforms. 3) A series of
case studies showcasing various findings relating to malicious
hacker behavior resulting from the data collected by our
operational system.
Background: Many of the individuals behind cyber-operations
â€“ originating outside of government run labs or military
commands â€“ rely on a significant community of hackers.
They interact through a variety of online forums (as means
to both stay anonymous and to reach geographically dispersed
collaborators).
Darknet and Deepnet Sites: Widely used for underground
communication, â€œThe Onion Routerâ€ (Tor) is free software
dedicated to protect the privacy of its users by obscuring
traffic analysis as a form of network surveillance [9]. The
network traffic in Tor is guided through a number of volunteeroperated servers (also called â€œnodesâ€). Each node of the
network encrypts the information it blindly passes on neither
registering where the traffic came from nor where it is headed
[9], disallowing any tracking. Effectively, this allows not
only for anonymized browsing (the IP-address revealed will
only be that of the last node), but also for circumvention
of censorship2 . Here, we will use â€œdarknetâ€ to denote the
anonymous communication provided by crypto-networks like
â€œTorâ€, which stands in contrast to â€œdeepnetâ€ which commonly
refers to websites hosted on the open portion of the Internet
(the â€œClearnetâ€), but not indexed by search engines [15].
Markets: Users advertise and sell their wares on marketplaces.
Darknet marketplaces provide a new avenue to gather information about the cyber threat landscape. The marketplaces
sell goods and services relating to malicious hacking, drugs,
pornography, weapons and software services. Only a small
fraction of products (13% in our collected data to date) are
related to malicious hacking. Vendors often advertise their
products on forums to attract attention towards their goods
and services.
Forums. Forums are user-oriented platforms that have the
sole purpose of enabling communication. It provides the opportunity for the emergence of a community of like-minded
individuals - regardless of their geophysical location. Administrators set up Darknet forums with communication safety
for their members in mind. While structure and organization
of Darknet-hosted forums might be very similar to more
familiar web-forums, the topics and concerns of the users
vary distinctly. Forums addressing malicious hackers feature
discussions on programming, hacking, and cyber-security.
Threads are dedicated to security concerns like privacy and
online-safety - topics which plug back into and determine the
structures and usage of the platforms.
II.

SYSTEM OVERVIEW

Fig. 1 gives the overview of the system. Through search
engines and spider services on the Tor network, human analysts
2 See

the Tor Projectâ€™s official website (https://www.torproject.org/)

were able to find forums and marketplaces populated by
malicious hackers. Other platforms were discovered through
links posted on forums either on the Tor-network or on the
Clearnet. The system consists of three main modules built
independently before integration. The system is currently fully
integrated and actively collecting cyber threat intelligence.
Crawler: The crawler is a program designed to traverse the
website and retrieve HTML documents. Topic based crawlers
have been used for focused crawling where only webpages of
interest are retrieved [17], [6]. More recently, focused crawling
was employed to collect forum discussions from darknet [10].
We have designed separate crawlers for different platforms
(markets/forums) identified by experts due to the structural
difference and access control measures for each platform. In
our crawler, we address design challenges like accessibility,
unresponsive server, repeating links creating a loop etc. to
gather information regarding products from markets and discussions on forums.
Parser: We designed a parser to extract specific information
from marketplaces (regarding sale of malware/exploits) and
hacker forums (discussion regarding services and threats).
This well-structured information is stored in a relational
database. We maintain two databases, one for marketplaces
and the other for forums. Like the crawler, each platform
has its own parser. The parser also communicates with
the crawler from time to time for collection of temporal
data. The parser communicates a list of relevant webpages
to the crawler, which are re-crawled to get time-varying
data. For markets we collect the following important products fields: {item title, item description, vendor name, shipping details, item reviews, items sold, CVE, items left, transaction details, ratings}. For forums we collect the following
fields: {topic content, post content, topic author, post author,
author status, reputation, topic interest}.
Classifier: We employ a machine learning technique using an
expert-labeled dataset to detect relevant products and topics
from marketplaces and forums respectively discussed in Section III. These classifiers are integrated into the parser to filter
out products and topics relating to drugs, weapons, etc. not
relevant to malicious hacking.
III.

E VALUATION

We consider the classification of identifying relevant products in darknet/deepnet marketplaces and relevant topics on
forum post containing communication relevant to malicious
hacking in this paper. It is a binary classification problem
with the data sample (in this case products/forum topics)
being relevant or not. We look at both supervised and semisupervised approaches to address the classification.
A. Machine Learning Approaches
In this work, we leverage a combination of supervised
and semi-supervised methods. Supervised methods include
the well-known classification techniques of Naive Bayes
(NB), random forest (RF), support vector machine (SVM)
and logistic regression (LOG-REG). However, supervised
techniques required labeled data, and this is expensive and
often requires expert knowledge. Semi-supervised approaches
work with limited labeled data by leveraging information
from unlabeled data. We discuss popular semi-supervised

Fig. 1: System overview

approaches used in this work. We perform a grid search to
find optimal parameters for the learning techniques.

description. This, in tandem with standard stop-word removal,
greatly improved classification performance.

Label propagation (LP). The label propagation approach
[22] has been widely used for semi-supervised classification
task [3], [16], [21], [8]. It estimates the label values based
on graph Laplacian [1] where the model is represented by a
weighted graph G = (V, E) , where V indicates the vertices
representing the samples, while the edges E are the weights
indicating the similarity between points. A subset of these
vertices are labeled and these vertices are then used to estimate
the labels of the remaining under the assumption that the edges
are able to capture the similarity between samples. Hence,
the performance of these methods depends on the similarity
measure used. The most commonly used similarity measures
include k-NN and Gaussian kernel.

Misspellings and Word Variations. Misspellings frequently
occur on forums and marketplaces, which is an obstacle for the
standard bag-of-words classification approach. Additionally,
with the standard bag-of-words approach, variations of words
are considered separately (e.g. hacker, hack, hackers, etc.).
Word stemming mitigates these issue of word variations, but
fails to fix the issue of misspellings. To address this we use
character n-gram features. As an example of character n-gram
features, consider the word â€œhackerâ€. If we were using tri-gram
character features, the word â€œhackerâ€ would yield the features
â€œhacâ€, â€œackâ€, â€œckeâ€, â€œkerâ€. The benefit of this being that the
variations or misspellings of the word in the forms â€œhackâ€,
â€œhackzâ€, â€œâ€hackkerâ€, will all have some common features.
We found that using character n-grams in the range (3, 7)
outperformed word stemming in our experiments.

Co-training (CT). The Co-training approach was proposed
by Blum and Mitchell [4]. In this approach, the feature set is
divided into two sets (assumed to be independent), and two
classifiers are trained using the limited labeled set denoted by
L . These trained classifiers are then used to estimate the labels
for the unlabeled points. High confidence label estimates from
classifier-1 are added to the labeled set L of classifier-2 and
vice versa. For the current setting we set the confidence to
70%. Every time the labeled set L is updated, the classifiers
are retrained. This procedure repeats until all of the unlabeled
points are labeled. It can be viewed as two classifiers teaching
each other.
B. Experiments: Marketplaces
Marketplaces sell goods and services that do not relate to
malicious hacking, including drugs, pornography, weapons and
software services. Only a small fraction of products (13%)
are related to malicious hacking. We thus require a model
that can separate relevant products from the non-relevant ones.
The data collected from marketplaces is noisy and hence not
suitable to use directly as input to a learning model. Hence,
the raw information undergoes several steps of automated data
cleaning. We now discuss the challenges associated with the
dataset obtained and the data processing steps taken to address
them. We note that similar challenges occur for forum data.
Text Cleaning. Product title and descriptions on marketplaces
often have much text that serves as noise to the classifier
(e.g. *****SALE*****). To deal with these instances, we first
removed all non-alphanumeric characters from the title and

Large Feature Space. In standard bag-of-words approach, as
opposed to the character n-gram approach, the feature matrix
gets very large as the number of words increase. As the number
of unique words grow, this bloated feature matrix begins to
greatly degrade performance. Using n-gram features further
increases the already over-sized feature matrix. To address
this issue, we leveraged the sparse matrix data structure in
the scipy3 library, which leverages the fact that most of the
entries will be zero. If a word or n-gram feature is not present
in a given sample, there is simply no entry for that feature in
the sparse matrix.
Preserving Title Feature Context. As the title and description
of the product are disjoint, we found that simply concatenating
the description to the product title before extracting features
led to sub-optimal classification performance. We believe that
by doing a simple concatenation, we were losing important
contextual information. There are features that should be
interpreted differently should they appear in the title versus the
description. Initially, we used two separate classifiers: one for
the title and one for the description. With this construction,
when an unknown product was being classified, we would
pass the title to the title classifier and the description to the
description classifier. If either classifier returned a positive
classification, we would assign the product a positive classification. However, we believe that this again led to the loss of
important contextual information. To fix this, we independently
extract character n-gram features from the title and description.
3 http://www.scipy.org/

TABLE 3: Markets and Number of products collected.
Markets

Products

Markets

Products

Market-1

439

Market-6

497

Market-2

1329

Market-7

491

Market-3

455

Market-8

764

Market-4

4018

Market-9

2014

Market-5

876

Market-10

600

and time investment can be reduced by applying a semisupervised approach which leverages the unlabeled data to
aid in classification. It takes approximately one minute for a
human to label 5 marketplace products or 2 topics on forums as
relevant or not, highlighting the costliness of manual labeling.
The experimental setup is similar to the supervised approach,
but this time we also utilize the large unlabeled data from each
marketplace (75%) for training.
1

Results: We consider 10 marketplaces to train and test our
learning model. A summary of these marketplaces is shown
in Table 3. Table 4 gives an instance of products defined as
being relevant or not. With the help of security experts we label
25% of the products from each marketplace. The experimental
setup is as follows. We perform a leave-one-marketplace-out
cross-validation. In other words, given n marketplaces we
train on n âˆ’ 1 and test on the remaining one. We repeat
this experiment for all the marketplaces. For the supervised
experiment, we only use the 25% labeled data from each
marketplace. We evaluate the performance based primarily
on three metrics: precision, recall and unbiased F1. Precision
indicates the fraction of products that were relevant from the
predicted ones. Recall is the fraction of relevant products
retrieved. F1 is the harmonic mean of precision and recall.
The results are averaged and weighted by the number of
samples in each market. In this application, a high recall is
desirable as we do not want to omit relevant products. In the
supervised approaches, SVM with linear kernel performed the
best, recalling 87% of the relevant products while maintaining
a precision of 85% (Fig. 2). SVM performed the best likely due
to the fact it maximizes generality as opposed to minimizing
error.

0.9

Average

This step yields a title feature vector and a description feature
vector. We then horizontally concatenate these vectors, forming
a single feature vector which includes separate feature sets for
the title and description.

0.8

0.7

0.6

Precision
LP
CT-NB

Recall
CT-LOG-REG
CT-RF

F1
CT-SVM

Fig. 3: Average Precision, Recall and F1 comparisons for LP, CT-NB, CT-LOG-REG,
CT-RF and CT-SVM for product classification.

Fig. 3 shows the performance comparison for the semisupervised approaches. For the co-training approach, we divide
the feature space into two sets. The two feature sets used are
both based on character n-grams. However, the set of words
from which the character n-grams are derived are disjoint
between the two sets. In this way, the two corresponding
feature vectors can be treated as being independent from
one another. Hence we get two views of the same sample.
Co-training with Linear SVM is able to recall 92% of the
relevant products as compared to label propagation and other
variants of co-training while maintaining a precision of 82%,
which is desirable. In this case, the unlabeled data aided
the classification in improving the recall to 92% without
significantly reducing the precision.
C. Experiment: Forums

TABLE 4: Example of Products.
Product Title

Relevant

20+ Hacking Tools (Botnets Keyloggers Worms and More!)

YES

5 gm Colombian Cocaine

NO

0.9

Average

0.8

0.7

0.6

0.5

Precision

Recall
NB

LOG-REG

RF

F1
SVM

Fig. 2: Average Precision, Recall and F1 comparisons for NB, LOG-REG, RF and SVM
for product classification.

As stated, only 25% of the data is labeled, as labeling
often requires expert knowledge. However, this significant cost

In addition to the darknet/deepnet marketplaces that we
have already discussed, there are also numerous darknet forums on which users discuss malicious hacking related topics.
Again, there is the issue that only a fraction of these topics with
posts on these forums contain information that is relevant to
malicious hacking or the trading of exploits. Hence, we need a
classifier to identify relevant topics. This classification problem
is very similar to the product classification problem previously
discussed, with similar set of challenges.
We performed evaluation on two such English forums.
The dataset consisted of 781 topics with 5373 posts. Table 5
gives instance of topics defined as being relevant or not.
We label 25% of the topics and perform a 10-fold cross
validation using supervised methods. We show the results
from the top two performing supervised and semi-supervised
methods. In the supervised setting, LOG-REG performed the
best with 80% precision and 68% recall (Fig. 4). On the other
hand, leveraging unlabeled data in a semi-supervised technique
improved the recall while maintaining the precision. We note
that in this case the 10-fold cross validation was performed
only on the labeled points. In the semi-supervised domain

co-training with LOG-REG improved the recall to 80% with
precision of 78%.

TABLE 6: Example of Zero-day exploits.

TABLE 5: Example of Topics.
Topic

Relevant

Bitcoin Mixing services

YES

Looking for MDE/MDEA shipped to Aus

NO

0.9

Average

0.8

0.7

0.6

0.5

Precision
LOG-REG

Recall
SVM

CT-LOG-REG

F1
CT-SVM

Fig. 4: Average Precision, Recall and F1 comparisons for LOG-REG, SVM, CT-LOGREG, and CT-SVM for English forum topic classification.

IV.

C ASE S TUDIES

We analyze the data with the purpose of answering the
questions raised in the Section I. We will be using the following key security terms. Vulnerability is a security flaw that
allows an attacker to compromise a software or an operating
system. Exploit is a piece of software that takes advantage of
a vulnerability in a piece of software or operating system to
compromise it. Patch is a piece of software used to improve
existing software by fixing vulnerabilities to improve security.
We discuss the following case-studies.

Zero-day exploit

Price (BTC)

Internet Explorer 11 Remote Code Execution 0day

20.4676

Android WebView 0day RCE

40.8956

cross-site connections that were previously unstudied. We are
able to produce this connected graph using the â€œusernamesâ€
used by vendors and users in each domain. A subgraph of
this network containing some of the individuals who are
simultaneously selling products related to malicious hacking
and publishing in hacking related forums is shown in Fig. 5.
In most cases, the vendors are trying to advertise/discuss their
products on the forums, demonstrating their expertise. Using
these integrated graphic representations, one can visualize
the individualsâ€™ participation in both domains, making the
right associations that lead to a better comprehension of the
malicious hacker networks. It is helpful in determining social
groups within the forums of user interaction. The presence
of users on multiple markets and forums follows a power law.
From Fig. 6, majority of users only belong to a single market or
forum. We note that there are 751 users that are present in more
than two platforms. Fig. 7 considers one such user/vendor. The
vendor is active in 7 marketplaces and 1 forum . The vendor
offers 82 malicious hacking related products and discusses
these products on the forum. The vendor has an average rating
of 4.7/5.0, rated by customers on the marketplace with more
than 7000 successful transactions, indicating the reliability of
the products and the popularity of the vendor.

A. Discovery of Zero-Day Exploits.

Fig. 5: Vendor/User network in marketplace and forum.

10000

Number of Users (Log scale)

Over a 4 week period, we detected 16 zero-day exploits
from the marketplace data. Zero-day exploits leverage vulnerabilities that are unknown to the vendor. Table 6 shows a
sample of zero-day exploits with their selling price in Bitcoin.
The Android WebView zero-day affects a vulnerability in the
rendering of web pages in Android devices. It affects devices
running on Android 4.3 Jelly Bean or earlier versions of the
operating system. This comprised of more than 60% of the
Android devices in 2015. After the original posting of this
zero-day, a patch was released in Android KitKit 4.4 and
Lollipop 5.0 which required devices to upgrade their operating
system. As not all users have/will update to the new operating
system, the exploit continues to be sold for a high price.
Detection of these zero-day exploits at an earlier stage can help
organizations avoid an attack on their system or minimize the
damage. For instance, in this case, an organization may decide
to prioritize patching, updating, or replacing certain systems
using the Android operating system.

1000
100
10

1
0

2

4
Number of Platforms

6

8

Fig. 6: Users in multiple markets and
forums.
Fig. 7: A centric network of a Vendor.

B. Users having presence in markets/ forums.
Previous studies on darknet crawling [10], [2] explore a
single domain, namely forums. We create a social network
that includes both types of information studied in this paper:
marketplaces and forums. We can thus study and find these

V.

R ELATED W ORK

Web crawling is a popular way of collecting large amounts
of data from the Internet. In many applications, researchers

are interested in specific topics for their application. Hence,
the need for a topic-based crawler popularly referred to as
a focused crawler [6], [5]. Most of the focused crawlers are
designed to collect information from the surface web with
little concentration on the darknet websites. More recently,
a focused crawler concentrating on dark web forums was
designed [10]. This research primarily concentrated on forums,
collecting data over a period of time and then performing
static analysis to study online communities. The authors also
describe different data mining techniques for these forums
in [7]. We, on the other hand, not only look at darknet forums
but also collect information from marketplaces hosting a range
of products relating to malicious hacking. Another application
of leveraging darknet information to counter human trafficking
is developed by DARPA through the Memex program4 - a
program with different goals than the work described in this
paper.
Previous work leverages the exploit information from
marketplaces in a game theoretic framework to formulate
system configurations that minimize the potential damage of
a malicious cyber attack [19]. Work analyzing hacker forums
to detect threats that pose great risk to individuals, businesses,
and government have been discussed in [2]. It further states
that knowledge is distributed in forums. That minimally skilled
people could learn enough by simply frequenting such platforms. Studying these hacker communities gives insights in
the social relationships. Also, the distribution of information
amongst users in these communities based on their skill level
and reputation [13], [14], [11]. These forums also serve as markets where malware and stolen personal information are shared
/ sold [12]. Samtani et al. analyze hacker assets in underground
forums [20]. They discuss the dynamics and nature of sharing
of tutorials, source code, and â€œattachmentsâ€ (e.g. e-books,
system security tools, hardware/software). Tutorials appear to
be the most common way of sharing resources for malicious
attacks. Source code found on these particular forums was
not related to specific attacks. Additionally underground (not
malicious hacking related) forums have also been analyzed
to captures the dynamic trust relationships forged between
mutually distrustful parties [18].
VI.

[2]

[3]

[4]

[5]

[6]

[7]
[8]
[9]

[10]

[11]

[12]

[13]

[14]
[15]

[16]

C ONCLUSION

In this paper, we implement a system for intelligence
gathering related to malicious hacking. Our system is currently
operational. We are in the process of transitioning this system
to a commercial partner. We consider social platforms on
darknet and deepnet for data collection. We address various
design challenges to develop a focused crawler using data
mining and machine learning techniques. The constructed
database is made available to security professionals in order
to identify emerging cyber-threats and capabilities.
Acknowledgments: Some of this work is supported by ONR NEPTUNE, ASU GSI, ASU ISSR and CNPq-Brazil.

[17]

[18]

[19]
[20]

[21]

R EFERENCES
[1]

M. Belkin and P. Niyogi. Using manifold structure for partially labelled
classification. In Advances in NIPS, 2002.

4 http://opencatalog.darpa.mil/MEMEX.html

[22]

V. Benjamin, W. Li, T. Holt, and H. Chen. Exploring threats and
vulnerabilities in hacker web: Forums, irc and carding shops. In
Intelligence and Security Informatics (ISI), 2015 IEEE International
Conference on, pages 85â€“90. IEEE, 2015.
C. M. Bishop and I. Ulusoy. Object recognition via local patch labelling.
In Deterministic and Statistical Methods in Machine Learning, pages
1â€“21, 2004.
A. Blum and T. Mitchell. Combining labeled and unlabeled data with
co-training. In Proceedings of the Eleventh Annual Conference on
Computational Learning Theory, COLTâ€™ 98, pages 92â€“100, New York,
NY, USA, 1998. ACM.
S. Chakrabarti, K. Punera, and M. Subramanyam. Accelerated focused
crawling through online relevance feedback. In Proceedings of the 11th
international conference on World Wide Web, pages 148â€“159. ACM,
2002.
S. Chakrabarti, M. Van den Berg, and B. Dom. Focused crawling: a new
approach to topic-specific web resource discovery. Computer Networks,
31(11):1623â€“1640, 1999.
H. Chen. Dark web: Exploring and data mining the dark side of the
web, volume 30. Springer Science & Business Media, 2011.
H. Cheng, Z. Liu, and J. Y. 0001. Sparsity induced similarity measure
for label propagation. In ICCV, pages 317â€“324. IEEE, 2009.
R. Dingledine, N. Mathewson, and P. Syverson. Tor: The secondgeneration onion router. In Proceedings of the 13th Conference on
USENIX Security Symposium - Volume 13, SSYMâ€™04, pages 21â€“21,
2004.
T. Fu, A. Abbasi, and H. Chen. A focused crawler for dark web
forums. Journal of the American Society for Information Science and
Technology, 61(6):1213â€“1231, 2010.
T. J. Holt. Subcultural evolution? examining the influence of on-and offline experiences on deviant subcultures. Deviant Behavior, 28(2):171â€“
198, 2007.
T. J. Holt and E. Lampke. Exploring stolen data markets online:
products and market forces. Criminal Justice Studies, 23(1):33â€“50,
2010.
T. J. Holt, D. Strumsky, O. Smirnova, and M. Kilger. Examining the
social networks of malware writers and hackers. International Journal
of Cyber Criminology, 6(1):891â€“903, 2012.
T. Jordan and P. Taylor. A sociology of hackers. The Sociological
Review, 46(4):757â€“780, 1998.
D. Lacey and P. M. Salmon. Itâ€™s dark in there: Using systems analysis
to investigate trust and engagement in dark web forums. In D. Harris,
editor, Engineering Psychology and Cognitive Ergonomics, volume
9174 of Lecture Notes in Computer Science, pages 117â€“128. Springer
International Publishing, 2015.
A. Levin, D. Lischinski, and Y. Weiss. A closed form solution to natural
image matting. In Proceedings of the 2006 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition - Volume 1,
CVPR â€™06, pages 61â€“68, Washington, DC, USA, 2006. IEEE Computer
Society.
F. Menczer, G. Pant, and P. Srinivasan. Topical web crawlers: Evaluating
adaptive algorithms. ACM Transactions on Internet Technology (TOIT),
4(4):378â€“419, 2004.
M. Motoyama, D. McCoy, K. Levchenko, S. Savage, and G. M. Voelker.
An analysis of underground forums. In Proceedings of the 2011 ACM
SIGCOMM conference on Internet measurement conference, pages 71â€“
80. ACM, 2011.
J. Robertson, V. Paliath, J. Shakarian, A. Thart, and P. Shakarian. Data
driven game theoretic cyber threat mitigation. In IAAI, 2016.
S. Samtani, R. Chinn, and H. Chen. Exploring hacker assets in
underground forums. In Intelligence and Security Informatics (ISI),
2015 IEEE International Conference on, pages 31â€“36. IEEE, 2015.
C. Wang, S. Yan, L. Z. 0001, and H.-J. Zhang. Multi-label sparse
coding for automatic image annotation. In CVPR, pages 1643â€“1650.
IEEE, 2009.
X. Zhu, J. Lafferty, and Z. Ghahramani. Combining active learning and
semi-supervised learning using gaussian fields and harmonic functions.
In ICML 2003 workshop on The Continuum from Labeled to Unlabeled
Data in Machine Learning and Data Mining, pages 58â€“65, 2003.

A Non-Parametric Learning Approach to Identify
Online Human Trafficking
Hamidreza Alvari
and Paulo Shakarian

arXiv:1607.08691v2 [cs.LG] 2 Aug 2016

Arizona State University
Tempe, Arizona
Email: {halvari,shak}@asu.edu

Abstractâ€”Human trafficking is among the most challenging
law enforcement problems which demands persistent fight against
from all over the globe. In this study, we leverage readily
available data from the website â€œBackpageâ€â€“ used for classified
advertisementâ€“ to discern potential patterns of human trafficking
activities which manifest online and identify most likely trafficking related advertisements. Due to the lack of ground truth,
we rely on two human analysts â€“one human trafficking victim
survivor and one from law enforcement, for hand-labeling the
small portion of the crawled data. We then present a semisupervised learning approach that is trained on the available
labeled and unlabeled data and evaluated on unseen data with
further verification of experts.

I. I NTRODUCTION
Human trafficking has received increased national and societal concern over the past decade [1]. According to the
United Nation [2], human trafficking is defined as the modern
slavery or the trade of humans mostly for the purpose of
sexual exploiting and forced labor, via different improper ways
including force, fraud and deception. Human trafficking is
among the challenging problems facing the law enforcementâ€“
it is difficult to identify victims and counter traffickers.
Before the advent of the Internet, pimps were under the risks
of being arrested by law enforcement, while advertising their
victims on the streets [3]. However, the move to the Internet,
has made it easier and less dangerous for both sex buyers and
sellers, especially for the pimps [4] as they no longer needed
to advertise on the streets. There are now plethora of websites
that host and provide sexual services, under categories of
escort, adult entertainment, massage services, etc., which help
pimps, traffickers and sex buyers (a.k.a. â€œjohnsâ€), maintain
their anonymity. Though some services such as Craiglistâ€™s
adult section and myredbook.com were shut down recently,
still there are many websites such as Backpage.com that
provide such services and many new are frequently created.
Traffickers even use dating and social networking websites,
including Twitter, Facebook, Instagram and Tinder to reach
out to the johns and their other followers. Although Internet
has presented new trafficking related challenges for law enforcement, it has also provided readily and publicly available
rich source of information which could be gleaned from online
sex advertisements for fighting this crime [5].

J.E. Kelly Snyder
Find Me Group
Tempe, Arizona
Email: kelly@findmegroup.org

Although, the Internet is being used for many other activities
including attracting the victims, communicating with costumers and rating the escort services, here we only focus on the
online advertisements. In this study, we use data crawled from
the adult entertainment section of the website Backpage.com
and propose a non-parametric learning approach to identify the
most likely human trafficking related online advertisements out
of the escort advertisements. To the best of our knowledge,
this is the first study that employs both data mining and
semi-supervised machine learning techniques to identify the
potential human trafficking related advertisements given only
a small portion of labeled data. We thus make the following
contributions.
1) We collected real posts from the U.S. cities represented
on Backpage.com. The data was then preprocessed and
cleaned.
2) Based on the literature, we created different groups
of features that capture the characteristics of potential
human trafficking activities. The less likely human trafficking related posts were then filtered out using these
features.
3) Due to the lack of ground truth, we relied on human
analysts for hand-labeling small portion of the filtered
data.
4) We trained a semi-supervised learner on labeled and
unlabeled data and sent back the identified highly human trafficking related advertisements to the experts for
further verification. We then validated our approach on
unseen data with further verification of experts.
The rest of the paper is organized as follows. In Section II,
we briefly provide the background of the problem of human
trafficking. Next, we review the prior studies on human trafficking in Section III. Then in Section IV, we explain our data
preparation and feature extraction scheme. Our unsupervised
filtering and expert assisted labeling are explained in Sections
V and VI, respectively. We detail our non-parametric learning
approach in Sections VII. We conclude the paper by providing
future research directions in Section VIII.
II. BACKGROUND
The United Statesâ€™ Trafficking Victim Protection Act of
2000 (TVPA 2000) [6], was the first U.S. legislation passed

against human trafficking. According to TVPA 2000, sex
trafficking is a severe form of trafficking, where force, fraud
or corecion are primary ways of inducing commercial sex
act. Human Trafficking is a crime against humanity and is
one of the most atrocious crimes of global magnitude. It is
a $150 billion industry of exploitation of children and young
adults, utilizing humans for forced labor and sex trafficking
worldwide. No country is immune and the problem is rapidly
growing with little to no law enforcement addressing the issue
and approximately 161 countries affected. Human trafficking
is considered to be a form of modern day slavery. Humans
are controlled, exploited, abused, forced into prostitution and
labor of servitude in some form and all under the threat of
punishment if they do not perform their required duties.
The Find Me Group (FMG) was founded by retired DEA
Special Agent Jerry â€œKellyâ€ Snyder in 2002 primarily to
locate missing persons. The natural evolution of the group
in locating missing persons was to allocate resources for
locating victims in human trafficking, as well as identifying
the persons responsible and reporting these organizations to
law enforcement. The FMG consists of current and retired
law enforcement agents and officers with a wide-range of
investigative expertise, including but not limited to linguistics,
handwriting analysis, body language, missing persons and
homicide. The search and rescue component of the FMG is
also comprised of current and retired law enforcement officers
and agents with 28 years of field management skills in locating
missing persons. The FMG has an additional advantage by
using trained experts/sources that provide detailed location
information of human trafficking victims.
The ultimate goal of the current project is to identify
missing persons which are connected to human trafficking
organizations. This can be done by identifying their locations,
utilizing logistical methodology with an additional focus on
their financial status and reporting assets to worldwide law
enforcement.
III. R ELATED W ORK
Recently, several studies have examined the role of the Internet and related technology in facilitating human trafficking[7],
[8], [9]. For example, the work of [7] studied how closely sex
trafficking is intertwined with new technologies. According
to [8], â€œThe sexual exploitation of women and children is
a global human rights crisis that is being escalated by the
use of new technologiesâ€. Researchers have studied the relationship between new technologies and human trafficking and
advantages of the Internet for sex traffickers. For instance,
according to [9], findings from a group of experts from the
Council of Europe demonstrate that the Internet and sex
industry are closely interlinked and the volume and content
of the material on the Internet promoting human trafficking
are unprecedented.
One of the earliest works which leveraged data mining
techniques for online human trafficking was [9], where the
authors conducted an analysis of data on the adult section
of the website Backpage.com. Their findings confirmed that

the female escort post frequency would increase in Dallas,
Texas, leading up to Super Bowl 2011 event. In a similar
attempt, other studies [10], [11] have investigated the impact
of large public events such as Super Bowl on sex trafficking
by exploring advertisement volume, trends and movement of
advertisements along with the scope and volume of demand
associated with such events. The work of [10], for instance,
concludes that in large events like Super Bowl which attract
significant amount of concentration of people in a relatively
short period of time and in a confined urban area, could be
a desirable location for sex traffickers to bring their victims
for commercial sexual exploitation. Similarly, the data-driven
approach of [11] shows that in some but not all events, one
can see a correlation between the occurrence of the event and
statistically significant evidence of an influx of sex trafficking
activity. Also, certain studies [12] have tried to build large
distributed systems to store and process the available online
human trafficking data in order to perform entity resolution
and create ontological relations between the entities.
Beyond these works, the work of [13], studied the problem
of isolating sources of human trafficking from online advertisements with a pairwise entity resolution approach. Specifically,
they trained a classifier to predict if two ads are from the
same source, using phone numbers as a strong feature. Then,
this classifier was used to perform entity resolution using a
heuristically learned value for the score of classifier. Another
work of [5] used Backpage.com data and extracted most likely
human trafficking spatio-temporal patterns with the help of law
enforcement. Note that unlike our method, this work did not
employ any machine learning methodologies for automatically
identifying the human trafficking related advertisements. The
work of [14] also used machine learning techniques by training
a supervised learning classifier on labeled data (based on the
phone numbers of known traffickers) provided by a victim
advocacy group, for the ad-classification problem. We note that
while phone numbers can provide very precise set of positive
labeled data, there are clearly many posts with previously
unseen phone numbers. In contrast, we do not solely rely on
the phone numbers for labeling the data. Instead, our experts
analyze the whole postâ€™s content to identify whether it is
human trafficking related or not. Indeed, we first filter out
the most likely advertisements using several feature groups
and pass a small sample to the experts for hand-labeling.
Then, we train semi-supervised learner on both labeled and
unlabeled data which in turn let us evaluate our approach on
the new coming (unseen) data as well. We note that our semisupervised approach can also be used as a complementary
method to procedures such as those described in [14] as we can
significantly expand the training set for use with supervised
learning.
IV. DATA C OLLECTION E FFORT
We collected about 20K publicly available listings from
the U.S. posted on Backpage.com in March, 2016. Each post
includes a title, description, time stamp, the posterâ€™s age,
posterâ€™s ID, location, image, video and sometimes audio. The

description usually lists the attributes of the individual(s) and
contact phone numbers. In this work we only focus on the
textual component of the data. This free-text data required
significant cleaning due to a variety of issues common to
textual analytics (i.e. misspellings, format of phone numbers,
etc.). We also acknowledge that the information in data could
be intentionally inaccurate, such as the posterâ€™s name, age and
even physical appearance (i.e. bra cup size, weight). Figure 1
shows an actual post from Backpage.com. To illustrate the
geographic diversity of the listings, we also plot the phone
distribution with respect to the different states in Figure 2.
Note that for brevity, we only show those with a frequency
greater than 5.
Fig. 1.

Fig. 3. An evidence of human trafficking. The boxes and numbers in red,
indicate the features and their corresponding group numbers (see also Table I).

A real post from Backpage.

TABLE I
D IFFERENT GROUPS OF FEATURES USED IN OUR WORK .
No.
1
2
3
4
5
6

Fig. 2.

Phone distribution by different states.

Feature Group
Advertisement Language Pattern
Words and Phrases of Interest
Countries of Interest
Multiple Victims Advertised
Victim Weight
Reference to Website or Spa Massage Therapy

Ref.
[5], [15], [16]
[17], [18], [19]
[1]
[5]
[6], [20]
[5]

for multiple escorts with the first individual coming from Asia
and very young. In the followings, we discuss such common
properties of human trafficking related advertisements, in more
details.
Inspired from literature, we define and extract 6 groups of
features from advertisements, shown in Table I, which could
be amongst the strong indicators of the human trafficking. In
what follows, we briefly describe each group of features used
in our work. Each feature listed is treated as a binary variable.
1) Advertisement Language Pattern: The first group consists of different language related features. For the first and
second features, we identify posts which has third person
language (more likely to be written by someone other than the
escort) and posts which contain first person plural pronouns
such as â€˜weâ€™ and â€˜ourâ€™ (more likely to be an organization) [5].

Next, we explain the most important characteristics of
potential human trafficking advertisements captured by our
feature groups.
A. Feature Engineering
Though many advertisements on Backpage.com are posted
by posters selling their own services without coercion and
intervention of traffickers, some do exhibit many common
trafficking triggers. For example, in contrast to the previous advertisements, Figure 3 shows an advertisement that could be an
evidence of human trafficking. This advertisement has several
potential properties of human trafficking including advertising

To ensure their anonymity, traffickers would deploy techniques to generate diverse information and hence make their
posts look more complicated. They usually do this to avoid being identified by either human analyst or automated programs.
Thus, to obtain the third feature, we take an approach from
complexity theory, namely Kolmogorov complexity which is
defined as the length of the shortest program to reproduce the
advertisement content on a universal machine such as Turing
Machine [15]. We approximate the Kolmogorov complexity of
an advertisementâ€™s content, by simply computing the Entropy
of the content [15] as follows. Let X denote the content and
xi be a given word in the content. We use the following

equation [21] to calculate the Entropy of the content.
H(X) = âˆ’

n
X

P (xi ) log2 P (xi )

evidence of human traffickingâ€“ this is described in the next
section.
(1)

i=1

We expect higher values of the Entropy correspond to
human trafficking. Finally, we discretize the result by using
the threshold of 4 which was found empirically in our experiments.
Next, we use word-level n-grams to find the common
language patterns of the advertisements, as the character-level
n-grams have already shown to be useful in detecting
unwanted content for Spam detection [16]. We set n = 4
and compute the normalized n-grams (using TF-IDF) of the
advertisementâ€™s content and use threshold of 0.5 to binarize
their values. This gives us 6 more features to include into
our feature set. Overall, we have 9 features related to the
language of the advertisement.
2) Words and Phrases of Interest: Despite the fact that
advertisements on Backpage.com do not directly mention
sex with children, costumers who prefer children, know to
look for words and phrases such as â€œsweet, candy, fresh,
new in town, new to the gameâ€ [17], [18], [19]. We thus
investigate within the posts to see if they contain such words as
they could be highly related with human trafficking in general.

V. U NSUPERVISED F ILTERING
Having detailed our feature set, we now construct feature
vectors for each instance by creating a vector of 15 binary
features that correspond to the important characteristics of
human trafficking related posts.
We obtain 999 instances from our dataset by filtering out
samples that do not posses any of the binary features. We
will refer to this as our filtered dataset. In Figure 4, we
visualize 500 of the 999 samples and an additional 500
samples outside of the filtered dataset (i.e., from the remainder
of the samples) we studied in a 2-D projection (using the tSNE transformation [22]). We clustered the visualized samples
into 2 clusters (using K-means) and found the clusters to be
purely either inside or outside of the sampled data (100% of
samples in cluster 1 were from the identified listings and 100%
of samples in cluster 2 were from outside this group).
Fig. 4. Two clusters of a portion of the filtered data set combined with
random samples from the remainder of the samples, in the trasformed feature
space.

3) Countries of Interest: We identify if the individual
being escorted is coming from other countries such as those
in Southeast Asia (especially from China, Vietnam, Korea
and Thailand, as we observed in our data) [1].
4) Multiple Victims Advertised: Some advertisements
advertise for multiple women at the same time. We consider
the presence of more than one girl as a potential evidence of
organized human trafficking [5].
5) Victim Weight: We take into account weight of the
individual being escorted as a feature (if it is available).
This information is particularly useful assuming that for
the most part, lower body weights (under 110 lbs) correlate
with smaller and underage girls [6], [20] and thereby human
trafficking.
6) Reference to Website or Spa Massage Therapy: The
presence of a link in the advertisement, either referencing
to an outside website (especially infamous ones) or spa
massage therapy, could be an indicator of more elaborate
organization [5]. In case of spa therapy, we observed many
advertisements interrelated with advertising for young Asian
girls and their erotic massage abilities. Therefore, the last
group has two binary features for the presence of both website
and spa.
In order to extract these features, we first clean the original
data and conduct preprocessing. Then we draw 999 instances
out of our dataset for further analysis, as they might be

We then create a second feature space that is used through
the remainder of the paper. Using Latent Drichlet Allocation
(LDA) [23] topic modeling from Python package gensim [24],
we identify 25 most representative topics out of the filtered
dataset. This allows us to uncover the hidden thematic structure in the data. Further, we rely on the document-topic
distribution given by the LDA (here each document is seen as
a mixture of topics) to distinguish the normal advertisements
(outliers) from highly human trafficking related ones. More
specifically, we treat each listing in the filtered data as a vector
of 25 probabilistic values provided by LDAâ€™s document-topic
distributionâ€“ this feature space is used in the next step.
Moreover, since we lack ground truth for our data, we rely
on human analysts (experts) for labeling the listings as either
human trafficking or not. In the next section, we select a
smaller yet finer grain subset of this data to be sent to the
experts. This alleviates the burden of the tedious work of handlabeling.

TABLE III
VALIDATED RESULTS ON UNLABELED DATA FOR BOTH KERNELS .

VI. E XPERT A SSISTED L ABELING
We first obtain a sample of 150 listings from the filtered
dataset. This set of listings was labeled by two human experts:
a previous human trafficking victim and a law enforcement
officer who specialized in this type of crime. From this subset,
a law enforcement professional and human trafficking victim
identified 38 and 139 instances (respectively) to be human
trafficking related instances. Among them, there were 31
records for which both experts agreed were highly related to
human trafficking. Thus, we now have 31 confirmed positive
samples, but still have large amounts of unlabeled examples
(849 instances) in our dataset. We summarize the data statistics
in Table II. Any sample for which at least one expert labeled
as negative, we treated as a negative sample.
TABLE II
D ESCRIPTION OF THE DATASET.
Name
Raw
Filtered
Unlabeled
Labeled
Positive
Negative

Expert 1
38
112

Value
20,822
999
849
Expert 2 Intersection
139
31
11
4

Name
Kernel
RBF (Union)
RBF (Intersection)
KNN (Union)
KNN (Intersection)

Positive
(Learner)
145
848
188
849

Value
Negative
Positive
(Learner) (Experts)
704
134
1
661
170
0
-

Precision
(Positive)
92.41%
90.42%
-

and 170 labels out of 145 and 188 positive instances and
achieved precision of 92.41% and 90.42%, respectively. We
further demonstrate the word clouds for the positive instances
assigned by RBF and KNN, in Figure 5 and Figure 6,
respectively.
Fig. 5.

Word cloud for the positive instances assigned by RBF.

Fig. 6.

Word cloud for the positive instances assigned by KNN.

Union
146
119

In the next section, we explain how we deploy a nonparametric learning approach to identify the labels of the rest
of the data to be sent for further expert verification.
VII. N ON -PARAMETRIC L EARNING
We use the Python package scikit-learn [25] for training
semi-supervised learner on the filtered dataset. There are
two label propagation semi-supervised (non-parametric) based
models in this package, namely, LabelPropagation and LabelSpreading [26]. These models rely on the geometry of
the data induced by both labeled and unlabeled instances as
opposed to the supervised models which only use the labeled
data [26]. This geometry is usually represented by a graph
G = (V, E), with the nodes V represent the training data
and edges E represent the similarity between them [26] in the
form of weight matrix W. Given the graph G, a basic approach
for semi-supervised learning is through propagating labels on
the graph [26]. Due to the higher performance achieved, we
chose to use LabelSpreading model. We conducted experiment
with the two built-in kernels radial basis function (RBF) and
K-nearest neighbor (KNN) in label propagation models and
report the results in Table III. Note that we only reported the
precision when 119 negative samples (labeled by either of the
experts) were used in the learning process. We did so because
of the reasonable number of the positive labels assigned by
either of the kernels in presence of these negative instances
(our experts had limited time to validate the labels of the data).
As we see from this table, out of 849 unlabeled data, our
learner with RBF and KNN kernels assigned positive labels
to the 145 and 188 instances, respectively. Next, we pass the
identified positive labels to the experts for further verification.
Our approach with RBF and KNN correctly identified 134

VIII. C ONCLUSION
Readily available online data from escort advertisements
could be leveraged in favor of fighting against the human
trafficking. In this study, having focused on textual information
from available data crawled from Backpage.com, we identified if an escort advertisement can be reflective of human
trafficking activities. More specifically, we first propose an
unsupervised filtering approach to filter out the data which
are more likely involved in trafficking. We then trained a

semi-supervised learner on small portion of such data, handlabeled by human trafficking experts, to identify the labels for
unseen data. The results suggest our non-parametric approach
is successful at identifying the potential human trafficking
related advertisements.
In future work we seek to extract the underlying network of
the data to find interesting patterns such as the most influential
nodes as they might indicate the known pimps and traffickers.
We would also like to replicate the study by integrating
more features, especially those supported by the criminology
literature.
ACKNOWLEDGMENT
This work was funded by the Find Me Group, a 501(c)3
dedicated to bring resolution and closure to families of missing
persons. https://www.findmegroup.org/
R EFERENCES
[1] â€œTrafficking in Persons Report,â€ July 2015.
[2] â€œUNODC on human trafficking and migrant smuggling,â€ 2011.
[3] C. Desplaces, â€œPolice Run â€˜Prostitutionâ€™ Sting; 19 Men Arrested,
Charged in Fourth East Dallas Operation.â€ Nov 1992.
[4] K. Nicholas D., â€œHow Pimps Use the Web to Sell Girls.â€ Jan 2012.
[5] E. Kennedy, â€œPredictive patterns of sex trafficking online,â€ 2012.
[6] â€œTrafficking Victims Protection Act of 2000,â€ 2000.
[7] D. M. Hughes et al., â€œThe demand for victims of sex trafficking,â€
Womens Studies Program, University of Rhode Island, 2005.
[8] D. M. Hughes, â€œThe Use of New Communications and Information
Technologies for Sexual Exploitation of Women and Children,â€ Hastings
Womenâ€™s Law Journal, vol. 13, no. 1, pp. 129â€“148, 2002.
[9] M. Latonero, â€œHuman trafficking online: The role of social networking
sites and online classifieds,â€ Available at SSRN 2045851, 2011.
[10] D. Roe-Sepowitz, J. Gallagher, K. Bracy, L. Cantelme, A. Bayless,
J. Larkin, A. Reese, and L. Allbee, â€œExploring the Impact of the Super
Bowl on Sex Trafficking,â€ Feb. 2015.
[11] K. Miller, E. Kennedy, and A. Dubrawski, â€œDo Public Events Affect
Sex Trafficking Activity?â€ ArXiv e-prints, Feb. 2016.
[12] P. A. Szekely, C. A. Knoblock, J. Slepicka, A. Philpot, A. Singh,
C. Yin, D. Kapoor, P. Natarajan, D. Marcu, K. Knight, D. Stallard, S. S.
Karunamoorthy, R. Bojanapalli, S. Minton, B. Amanatullah, T. Hughes,
M. Tamayo, D. Flynt, R. Artiss, S.-F. Chang, T. Chen, G. Hiebel, and
L. Ferreira, â€œBuilding and Using a Knowledge Graph to Combat Human
Trafficking.â€ in International Semantic Web Conference (2), ser. Lecture
Notes in Computer Science, vol. 9367. Springer, 2015, pp. 205â€“221.
[13] C. Nagpal, K. Miller, B. Boecking, and A. Dubrawski, â€œAn Entity
Resolution approach to isolate instances of Human Trafficking online,â€
ArXiv e-prints, Sep. 2015.
[14] A. Dubrawski, K. Miller, M. Barnes, B. Boecking, and E. Kennedy,
â€œLeveraging publicly available data to discern patterns of humantrafficking activity,â€ Journal of Human Trafficking, vol. 1, no. 1, pp.
65â€“85, 2015.
[15] M. Li and P. M. Vitnyi, An Introduction to Kolmogorov Complexity and
Its Applications, 3rd ed. Springer Publishing Company, Incorporated,
2008.
[16] I. Kanaris, K. Kanaris, and E. Stamatatos, â€œSpam Detection Using
Character N-Grams.â€ in SETN, ser. Lecture Notes in Computer Science,
vol. 3955. Springer, 2006, pp. 95â€“104.
[17] K. Hetter, â€œFighting sex trafficking in hotels, one room at a time,â€ March
2012.
[18] R. Lloyd, â€œAn Open Letter to Jim Buckmaster,â€ April 2012.
[19] J. Dickinson Goodman and M. Holmes, â€œCan We Use RSS to Catch
Rapists,â€ 2011.
[20] â€œAverage Height to Weight Chart - Babies to Teenagers.â€
[Online]. Available: http://www.disabled-world.com/artman/publish/
height-weight-teens.shtml
[21] C. E. Shannon, â€œA mathematical theory of communication,â€ ACM
SIGMOBILE Mobile Computing and Communications Review, vol. 5,
no. 1, pp. 3â€“55, 2001.

[22] L. van der Maaten and G. Hinton, â€œVisualizing High-Dimensional Data
Using t-SNE,â€ 2008.
[23] D. M. Blei, A. Y. Ng, and M. I. Jordan, â€œLatent dirichlet allocation,â€
the Journal of machine Learning research, vol. 3, pp. 993â€“1022, 2003.
[24] R. RÌŒehuÌŠrÌŒek and P. Sojka, â€œSoftware Framework for Topic Modelling
with Large Corpora,â€ in Proceedings of the LREC 2010 Workshop on
New Challenges for NLP Frameworks. Valletta, Malta: ELRA, May
2010, pp. 45â€“50, http://is.muni.cz/publication/884893/en.
[25] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, â€œScikit-learn: Machine learning in Python,â€ Journal of Machine
Learning Research, vol. 12, pp. 2825â€“2830, 2011.
[26] Y. Bengio, O. Delalleau, and N. Le Roux, In Semi-Supervised Learning.
MIT Press, 2006.

Noname manuscript No.
(will be inserted by the editor)

Toward Early and Order-of-Magnitude Cascade
Prediction in Social Networks
Ruocheng Guo Â· Elham Shaabani Â·
Abhinav Bhatnagar Â· Paulo Shakarian

arXiv:1608.02646v1 [cs.SI] 8 Aug 2016

Received: date / Accepted: date

Abstract When a piece of information (microblog, photograph, video, link,
etc.) starts to spread in a social network, an important question arises: will it
spread to â€œviralâ€ proportions â€“ where â€œviralâ€ can be defined as an order-ofmagnitude increase. However, several previous studies have established that
cascade size and frequency are related through a power-law - which leads to a
severe imbalance in this classification problem. In this paper, we devise a suite
of measurements based on â€œstructural diversityâ€ â€“ the variety of social contexts (communities) in which individuals partaking in a given cascade engage.
We demonstrate these measures are able to distinguish viral from non-viral
cascades, despite the severe imbalance of the data for this problem. Further,
we leverage these measurements as features in a classification approach, successfully predicting microblogs that grow from 50 to 500 reposts with precision
of 0.69 and recall of 0.52 for the viral class - despite this class comprising under
2% of samples. This significantly outperforms our baseline approach as well
as the current state-of-the-art. We also show this approach also performs well
for identifying if cascades observed for 60 minutes will grow to 500 reposts as
well as demonstrate how we can tradeoff between precision and recall.
Keywords Cascade Prediction Â· Information Diffusion Â· Social Network
Analysis Â· Diffusion in Social Networks

U.S. provisional patent 62/201,517. A non-provisional patent is currently being filed.
Some of the authors of this paper are supported by by AFOSR Young Investigator Program
(YIP) grant FA9550-15-1-0159, ARO grant W911NF-15-1-0282, and the DoD Minerva program.
Ruocheng Guo, Elham Shaabani, Abhinav Bhatnagar, Paulo Shakarian
Arizona State University
E-mail: {rguosni, shaabani, abhatn, shak}@asu.edu

2

Ruocheng Guo et al.

1 Introduction
When a piece of information (microblog, photograph, video, link, etc.) starts
to spread in a social network, an important question arises: will it spread to
â€œviralâ€ proportions â€“ where â€œviralâ€ is defined as a significant (i.e. order-ofmagnitude) increase in the number of individuals re-posting the information.
However, several previous studies (Bakshy et al, 2011; Cheng et al, 2014) have
established that cascade size and frequency are related through a power-law
- which leads to a severe imbalance in this classification problem. In this paper, we devise a suite of measurements based on â€œstructural diversityâ€ that
are associated with the growth of a viral cascade in a social network. Structural diversity refers to the variety of social contexts in which an individual
engages and is typically instantiated (for social networks) as the number of distinct communities represented in an individualâ€™s local neighborhood (Ugander
et al, 2012; Zhang et al, 2013; Shakarian et al, 2014; Li et al, 2015). Previously, Ugander et al. identified a correlation between structural diversity and
influence (Ugander et al, 2012). We demonstrate these measures are able to
distinguish viral from non-viral cascades, despite the severe imbalance of the
data for this problem. Further, we leverage these measurements as features
in a classification approach, successfully predicting microblogs that grow to
500 reposts from 50 (size-based experiments) or the first-hour observations
(time-based experiments). The main contributions of the paper are as follows:
â€“ We develop a suite of structural diversity-based measurements that are
indicative of cascade growth.
â€“ We are able to identify cascades of 50 reposts that grow to 500 reposts
with a precision of 0.69 and recall of 0.52 for the viral class (200 out of
13,285 samples).
â€“ We are able to identify cascades that have advanced for 60 minutes that
will reach 500 reposts with a precision of 0.65 and recall of 0.53 for the
viral class (200 out of 3,444 samples).
â€“ We demonstrate how to trade-off between precision and recall for the
above-mentioned problems. For instance, to predict cascades that reach
500 nodes, we can obtain precision of 0.78 or recall of 0.71 at the expense
of the other.
â€“ We demonstrate that our approach is stable for alternative definitions of
â€viralâ€ (i.e. microblogs that grow to sizes above or below 500 reposts).
We note that our results on the prediction of cascades rely solely upon the use
of our structural diversity based measures for features and limited temporal
features - hence the prediction is based on network topology alone (no content
information was utilized). We also achieved these results while maintaining the
imbalance of the dataset - where we leave the ratio of â€™viralâ€™ and â€™non-viralâ€™
samples as it is. This differs from some previous studies (i.e. (Jenders et al,
2013)) which balance the data before conducting classification experiments.
Further, we note that we obtained prediction of order-of-magnitude increases
in the size of the cascade - which also differs from other work (i.e. (Cheng

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

3

et al, 2014)) which focus on identifying cascades that double in size. The
remainder of the paper is organized as follows. In Section 2 we introduce
our notation and describe the dataset used in this paper. This is followed
by an introduction of our structural diversity measurements for cascades in
Section 3. Then we describe our experimental results where we examined both
the behavior of these measurements and the performance of classifiers built
using these measurements in Section 5. Finally, we discuss related work in
Section 6.

2 Technical Preliminaries
Here we introduce necessary notation and describe our social network data.
We represent a social network as G = (V, E) where V is the set of nodes and
E as a set of directed edges with sizes |V |, |E| respectively. The intuition behind edge (v, v 0 ) is that it is possible that v 0 repost a microblog from v since
v 0 did this previously. This intuition stems from how we create the edges in
our network: (v, v 0 ) is an edge if v 0 reposted from v once or more during a
specified time period (for our experiments, May 1 to July 31, 2011). We also
assume a partition over nodes that specifies a community structure. We assume that such a partition is static (based on the same time period from which
the edges were derived) and that the partition C consists of k communities:
{C1 , C2 , ..., Ck }, each is a set of nodes. There are many possible methods to
derive the communities (if user-reported communities are not available) - for
instance: the Louvain algorithm (Blondel et al, 2008), Infomap (Rosvall and
Bergstrom, 2008), Smart Local Move (SLM) (Waltman and van Eck, 2013)
and Label Propagation (Raghavan et al, 2007). Previous work such as (Weng
et al, 2014; Grabowicz et al, 2012) showed the effectiveness of communities
detected by these algorithms for different applications. In this paper, We utilize the Louvain algorithm, Infomap algorithm and SLM algorithm to identify
communities in the social network G due to their scalability for large social
network. For these algorithms, the number of communities is not an argument
as input but rather produced as part of the output of these algorithms. Note
that we require C to be a partition over nodes - hence we disallow for overlapping communities. This is consistent with the community structure derivations
from previous, related work (Ugander et al, 2012; Zhang et al, 2013; Shakarian et al, 2014; Li et al, 2015) which also required a partition over nodes such
as strongly connected components. As such, we leave the study of structural
diversity in the case of overlapping communities to future efforts.
Cascades. A cascade Ï„ = (U, R) consists of all nodes (U ) who posted or reposted a certain original microblog and the reposting relationships between
them, treated as edges (R). Naturally, any cascade is a subgraph of the social
network G. In order to predict the final size, snapshots of a cascade can be
taken by different time since adoption of the seed adopter (denoted by t). Then
a snapshot of cascade Ï„ introduces a subset Ï„t = (Ut , Rt ) of Ï„ . We refer to

4

Ruocheng Guo et al.

Ut as adopters. Moreover, we also call the out-neighbors of adopters in G but
not among the adopters as exposed users and denote them as NG (Ut ). For
each node v âˆˆ NG (Ut ), we define the adopters who exposes the cascade to v
as its exposers. For convenience, we also define function uea : v â†’ u to return
the earliest adopter u among exposers of v âˆˆ NG (Ut ).
For size-based experiments, the time t for taking snapshot of a cascade
is decided by a given cascade size m. We use t(m) to denote the smallest
t such that |Ut |= m is true for a certain cascade. Accordingly, to get the
corresponding order number n of an adopter u âˆˆ Ut , we define function Index :
u â†’ n where n âˆˆ [1, |Ut |]. To maintain a unique order of reposts, a very small
random number is added to each t(n) for all integers n âˆˆ [1, |Ut (m)|]. We have
not found this to be a significant issue in this dataset. For convenience and
simplicity, we use t to stand for both t(m) in size-based and t in time-based
experiments later.
For a given snapshot Ï„t = (Ut , Rt ), then we want to divide the set NG (Ut )
into two sets, namely recently exposed users (Ft ) and past exposed users (Nt ).
Intuitively, this division is done based on how long it is since v âˆˆ NG (Ut ) is
true (when it is possible for v to make a repost) till the snapshot Ï„t is taken.
Formally, given a node v âˆˆ NG (Ut ), we decide whether it is a recently or past
exposed user:
texpose (v) = t âˆ’ t(Index(uea (v)))
(1)
As defined before, t(n) denotes the earliest time t when |Ut |= n is true.
Then the value of texpose (v) is the number of time periods since the earliest
adoption among its exposers till when the snapshot of cascade is taken.
A positive constant Î» is set as a threshold on texpose (v) (we will discuss
how this constant is set in the last paragraph of 2), the recently exposed users
and past exposed users are defined as follows:
Ft = {v âˆˆ NG (Ut ) s.t. texpose (v) â‰¤ Î»}

(2)

Nt = NG (Ut ) \ Ft

(3)

Sina Weibo Dataset. The dataset we used was provided by WISE 2012 Challenge1 . It included a sample of microblogs posted on Sina Weibo2 from 2009
to 2012. In this dataset, we are provided with time and user information for
each microblog and subsequent repost which enabled us to derive a corpus of
cascades. For every repost in this dataset, the reposting relationship is provided as uid: v 0 tab v which indicates this message is a reposted from user v by
v 0 . From this data, we derived our social network G = (V, E) that was created
from microblogs (including original posts and reposts) published during May
1, 2011 to July 31, 2011 (the 3-month period). For this network, the number
of active nodes in August (the time period we studied for cascade prediction)
is 5,910,608, while 5,664,625 of them at least have one out-neighbor. During
1
2

http://www.wise2012.cs.ucy.ac.cy/challenge.html
http://weibo.com

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

5

Table 1: Properties of the Social Network and Cascades
Network Properties
Vertices (Nodes)
Edges
Average degree
Average clustering coefficient
Connected components
Number
Average
Number
Average
Number
Average

of communities (Louvain)
size of communities (Louvain)
of communities (Infomap)
size of communities (Infomap)
of communities (SLM)
size of communities (SLM)

Cascade Properties
Number
Number
Number
Average

of cascades
of viral cascades
of active nodes in cascades
time to become viral

Value
17,996,803
52,472,547
5.83
0.107
4974
379,416
47.5
39,922
450.799
380,854
47.3
Value
2,113,405
208
5,910,608
18 (h)

the month of August, there were 22,182,704 microblogs. Of these, 9,323,294
are reposts. 2,252,368 different of original posts succeeded to make at least
one user repost, while 1,920,763 (86.6%) of them were written by authors who
at least published one microblog during the 3-month period mentioned before. For this dataset, although different from a power-law noted previoulsly
in (Bakshy et al, 2011; Cheng et al, 2014), the histogram of final cascade size
(see Figure 1a) still shows that only quite few cascades went â€™viralâ€™. Therefore, we could demonstrate that this dataset is more representative of cascade
behavior observed in real world than work like (Jenders et al, 2013) which
conducted biased sampling to artificially provide balanced classes.
We select the threshold constant Î» as 30 minutes since vast majority of
all the reposts in May-July, 2011 occurred within 30 minutes since adoption
of the seed adopter (see Figure 1b). To justify this selection, knowing that
Î» is a threshold on texpose (v) which is upper bounded by t, the proportion
of exposed users became adopter with texpose (v) â‰¤ 30(min) should be more
than that of those did the repost with t â‰¤ 30(min). This implies why it is
necessary to distinguish recently and past exposed users due to the significant
difference in probability to adopt. In Figure 1c, we show distribution of how
long it takes for viral cascades to reach 500 nodes - note that the average value
here is approximately 18 hours (which is significantly greater than what we
study in our time-based classification problem).

Ruocheng Guo et al.

Frequency

6

107
106
105
104
103
102
101
100 0

200 400 600 800 1000 1200 1400
Final Size of Cascades

(a) The histogram of cascade size for August, 2011

cumulative probability

10 0
10 -1
10 -2
10 -3
-4
1010
0
10 1
10 4
10 5
10 2
10 3
time since the adoption of the seed adopter (minutes)

(b) CDF of adoption time since adoption of the seed adopter (minutes) for May-July, 2011

Frequency

10 2

10 1

10 0 0 200 400 600 800 1000 1200 1400
Time taken to become viral (>500) in minutes
(c) Histogram of time (minutes) â€™viralâ€™ cascades took to reach size of 500.

Fig. 1: Network Dataset Statistics

3 Structural Diversity Measurements in Real Information Cascades
Found by (Ugander et al, 2012), an individual is more likely to be infected
by a â€˜social contagionâ€™ if his/her â€˜infectedâ€™ in-neighbors are distributed over
more connected components of social network users. For example, as shown

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

7

in Figure 2, although the man on the left has more infected in-neighbors, the
woman on the right is more likely to be infected by the social contagion. As
in-neighbors of her are showing higher structural diversity (from two communities). Translated into the terminologies introduced in this paper, they showed
that an exposed user is more likely to become an adopter with exposers of
high structural diversity. If this effect is aggregated over all the exposed users
of a cascade, the significance to measure the relationship between structural
diversity of adopters would be revealed. Moreover, we also extend our experiments to measure that of exposed users. Instead of connected components, we
consider structural diversity described by communities. In this section we introduce a suite of various structural diversity measurements. We study these
measurements as cascades progress in Section 4 and then leverage them as
features for our classification problem in Section 5. We introduce these measurements as follows.
Number of communities. For a given set of node S âˆˆ {Ut , Ft , Nt } we can
retrieve the associated communities C(S) by the partition of the social network
C(G). Formally:
C(S) = {Ci âˆˆ C(V ) s.t. S âˆ© Ci 6= âˆ…}
where C(V ) is the partition of the social network G, introduced in Section 2. We
measure the number of communities represented by |C(S)| for S âˆˆ {Ut , Ft , Nt }.
Gini impurity. For S âˆˆ {Ut , Ft , Nt } in a cascade Ï„t , the gini impurity IG (S)
proposed by (Breiman et al, 1984) for splitting samples in decision tree, intuitively, is a scalar describing how much the distribution of nodes in S over
communities in C(S) differs from the uniform distribution. Here the uni|S|
for all Ci âˆˆ C(S).
form distribution stands for the situation where |Ci |= |C(S)|
To show the extreme values, IG (S) â‰ˆ 1 means the nodes are uniformly distributed over a large quantity of communities while IG (S) â‰ˆ 0 implies most of
the nodes in S are from the few â€™dominantâ€™ communities. Formally, we define
gini impurity as follows:
IG (S) = 1 âˆ’

X

(

Ci âˆˆC(S)

|Ci | 2
)
|S|

(4)

We study the gini impurity IG (S) for S âˆˆ {Ut , Ft , Nt } for each cascade. We
note that the impurity of the adopter set IG (Ut ) behaves similar to the entropy
of this set (a measurement introduced in (Weng et al, 2014)). However, as we
will see in the next two sections, we found that the impurity of the recently
exposed users is a more discriminating feature.
Overlap. For {Sa , Sb } âŠ‚ {Ut , Ft , Nt }, the overlap (O(Sa , Sb )) is simply the
number of shared communities between the sets of nodes Sa and Sb . Formally:
O(Sa , Sb ) = |C(Sa ) âˆ© C(Sb )|

(5)

8

Ruocheng Guo et al.

Fig. 2: An example for structural diversity: Although the man on the left has
more infected in-neighbors, the woman on the right is more likely to be infected
by the social contagion. As in-neighbors of her are showing higher structural
diversity (from two communities).

The intuition behind overlap stems directly from the original structural diversity results of the related work (Ugander et al, 2012) - for instance a large
overlap value O(Ut , Ft ) is likely to indicate that the local neighborhoods of
many of the recently exposed users will exhibit high structural diversity hence increasing the probability to become adopters in the future.
Baseline measures. In addition to the aforementioned structural diversity measurements, we also examine two baseline measurements dealing with time and
size.
Average time to adoption. ThePaverage time to adoption for adopters in the
m
1
cascade snapshot of size m: m
i=1 t(m).
Number of nodes. The cardinality of adopters, recently and past exposed users
|Ut |,|Ft |,|Nt |.

4 Structural Diversity Measurement Study
Here we examine the behavior of the various structural diversity measurements
as viral and non-viral cascades progress. In this section, we define a cascade as
viral if the number of reposts eventually reaches a threshold (denoted T H) of
500 (in the next section we will explore various values for T H). Only the distributions of feature values computed based on Louvain algorithm are exhibited
in this section as it provides best results in both size-based and time-based classification tasks (See Section 5). All the measurements are computed by cascade

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

9

Table 2: Number of samples analyzed in different stages
m
10
30
50
100
200

Samples
98,832
26,733
13,285
4,722
1,324

Viral Samples (%)
0.2%
0.7%
1.5%
4.2%
15%

t (min)
40
60
100
150
300

Samples
2,234
3,444
5,767
8,349
15,350

Viral Samples (%)
7%
5%
3%
2%
1%

snapshots with five populations of nodes with m = {10, 30, 50, 100, 200} (or
t(m) accordingly) and five values of time since adoption of the seed adopter
with t = {40, 60, 100, 150, 300}. Table 2 shows the number of samples our
analysis covers in both classes for each value of m and t. For each time t we
perform analysis on measurements for those cascade snapshots with no less
than 5 adopters at the time so that the enough information can be provided
from Ut ,Ft and Nt for the prediction task. For each size m, we consider the
cascades with |Ut |= m adopters at the corresponding time t(m), t(m) can
vary for different cascades. Hence, cascades with final size less than m are
ignored in our analysis. This leads to that the number of non-viral cascades
decreases as m increases. We examined a total of 24 measurements discussed
in the previous section (12 for size-based and 12 for time-based analysis, listed
as Am and At respectively in Table 3). For each measurement, for each m and
t describing the diffusion process, we attempted to identify statistically significant difference between viral and non-viral classes. For this, we performed
KS tests for each pair of measurements. In every test, p â‰¤ 10âˆ’13 , so the null
hypothesis is rejected for all cases, which means each pair of the distributions
are significantly different. We choose KS test over T test and Chi-square test
as it is sensitive to both the location and shape of the distribution as well as
it does not require each distribution to cover all possible values of the other.
As notations of the box plots in the following subsections, A and M denotes
mean and median for each box plot respectively.

4.1 Size Progression
Average time to adoption. As a baseline measurement, we study the average
time to adoption for each m of the cascade process (Figure 3). As expected,
viral cascades exhibit shorter average time since adoption of the seed adopter
till each later adoption. While we note that significant differences are present
- especially in the early stages of the cascade, the whiskers of the non-viral
class indicate a significant proportion of non-viral cascades that exhibit rapid

10

Ruocheng Guo et al.

adoption. We believe this is likely due to the fact that certain cascades may
have very high appeal to specialized communities.
Number of communities. Figure 4 displays how the number of communities
|C(S)| increases over m = {10, 30, 50, 100, 200} for the sets S = {Ut , Ft }. We
note that |C(Ut )| (the communities represented in the set of adopters) was
shown to be a useful feature in (Weng et al, 2014) for tasks where the target
class had fewer reposts than in this study. Here, we note that while statistically
significant differences exist, the average and median values at each of the
examined stages are generally similar. On the other hand, the communities
represented by the set of rencently exposed users (Ft ) shows viral cascades
have stronger capability to keep set of rencently exposed users with many
communities than non-viral ones. We also noted that the median of |C(Nt )|
shows viral cascades start with smaller |C(Nt )|. However, it increases faster in
viral cascades as nodes in rencently exposed users become past exposed users
(not pictured) as m increases.
Gini impurity. Cascades in both classes tend to accumulate diversity in the
process of collecting more adopters - and we have also noted that a related
entropy measure (studied in (Weng et al, 2014)) performed similarly. We also
observed that viral cascades can show larger gini impurity in recently exposed
users measured by IG (Ft ) in early stages (m = {10, 30, 50}). However, perhaps
most striking, non-viral cascades gain more uniformly distributed nodes over
communities in non-adopters, shown by IG (Nt ) (Figure 5). We believe that this
is due to non-viral cascades likely have an appeal limited to a relatively small
number of communities - hence those not adopting the trend may represent a
distribution of nodes over communities which is more different from a uniform
distribution.
Overlap. We found that overlap grows with the number of adopters in the
three types of overlap considered. For O(Ut , Ft ), viral cascades start with a
larger initial value and keep leading non-viral ones in the diffusion process of
first 200 nodes (Figure 6). We consider that viral cascades also take advantage
of the densely linked communities to help them become viral. However, in the
case of O(Ut , Nt ) and O(Ft , Nt ), viral cascades begin with lower value but
grow much faster than non-viral cascades.
4.2 Time Progression
Number of adopters. As a baseline measurement, we study the number of
adopters at regular time intervals and, as expected, found a clear difference
between the two classes. Figure 7 shows how |Ut | changes over 40, 60, 100,
150 and 300 minutes. Although there is an obvious difference in early stages
(40-60 minutes) between the two distributions, we will see in the next section
that this alone does not provide adequate performance for our prediction task
(see Section 5).

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

Average Time(103 )

1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0 10

30

50 100 200

Number of Adopters

M: 15.3 49.7 78.4 168 301
A: 40.9 86.9 129 215 347

1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0 10

Average Time(103 )

M: 865 853 804 754 765
A: 780 790 770 753 759

11

(a) Non-viral cascades

30

50 100 200

Number of Adopters

(b) Viral cascades

Fig. 3: Average time (minutes in 103 ) since adoption of the seed adopter to
each later adoption

Number of Adopters

M: 8.0 17.0 23.0 34.0 46.5
A: 8.1 17.3 23.5 34.0 46.5
70
60
50
40
30
20
10
0 10
30 50 100 200

Number of Communities

Number of Communities

M: 8.0 18.0 25.0 35.0 48.0
A: 7.7 17.5 24.0 34.9 47.6
70
60
50
40
30
20
10
0 10
30 50 100 200

Number of Adopters

(a) Number of communities amongst (b) Number of communities amongst
adopters (|C(Ut )|) for non-viral cascades
adopters (|C(Ut )|) for viral cascades

Number of Adopters

M: 21.0 30.0 30.0 33.5 42.5
A: 24.3 41.7 44.4 78.7 88.6
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0 10
30 50 100 200

Number of Communities(102 )

Number of Communities(102 )

M: 7.0 15.0 20.0 27.0 33.0
A: 25.7 39.6 53.2 88.5 111
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0 10
30 50 100 200

Number of Adopters

(c) Number of communities amongst recently (d) Number of communities amongst recently
exposed users (|C(Ft )|) for non-viral cascades exposed users (|C(Ft )|) for viral cascades

Fig. 4: Number of communities for m = {10, 30, 50, 100, 200}

Number of communities. Figure 8 shows how |C(S)| for S âˆˆ {Ut , Ft , Nt }
changes over time. The value of |C(S)| increases over time for Ut and Nt
but decreases for Ft . Here, the differences are somewhat more pronounced
than for the size-progression measurements (compare with Figure 4). Viral
cascades are more likely to have more communities in any one of Ut , Ft , Nt
than non-viral ones. For adopters and non-adopters, |C(Ut )| and |C(Nt )| value
of viral cascades increases faster than that of non-viral ones over time. While
for recently exposed users, |C(Ft )| of non-viral cascades decreases more than
viral ones in the same amount of time.

12

Ruocheng Guo et al.

Gini

1.0
0.8
0.6
0.4
0.2
0.0 10

M: 0.9 0.9 0.9 0.9 0.9
A: 0.8 0.9 0.9 0.9 0.9

1.0
0.8
0.6
0.4
0.2
0.0 10

Gini

M: 0.7 0.8 0.9 0.9 0.9
A: 0.7 0.8 0.8 0.8 0.9

30

50 100 200

Number of Adopters

30

50 100 200

Number of Adopters

(a) Gini impurity of recently exposed users (b) Gini impurity of recently exposed users
(IG (Ft )) for non-viral cascades
(IG (Ft )) for viral cascades

Number of Adopters

M: 0.0 0.9 0.9 0.9 0.9
A: 0.4 0.8 0.9 0.9 0.9
1.0
0.8
0.6
0.4
0.2
0.0 10
30 50 100 200

Gini

Gini

M: 0.8 0.9 0.9 0.9 0.9
A: 0.8 0.9 0.9 0.9 0.9
1.0
0.8
0.6
0.4
0.2
0.0 10
30 50 100 200

Number of Adopters

(c) Gini impurity of past exposed users (d) Gini impurity of past exposed users
(IG (Nt )) for non-viral cascades
(IG (Nt )) for viral cascades

Fig. 5: Gini impurity for m = {10, 30, 50, 100, 200}
Gini impurity. It takes less than Î» = 30 minutes for a considerable portion
of viral cascades to reach size m = 30. This explains the difference between
size-based and time-based gini impurity values in initial-stage cascades (compare Figure 5 and Figure 9). In terms of size-based gini impurity of the nonadopters (IG (Nt )), the values of viral cascades are smaller than those of nonviral cascades when m is small. However, when t is small, larger gini impurity
(IG (Nt )) amongst non-adopters are shown in viral cascades. Furthermore, as
m increases, although no significant difference is shown by the median and
average of IG (Nt ), Figure 5 shows non-viral cascades are more likely to have
a value smaller than the lower whisker to become outliers.
Overlap. By definition, overlap is the number of shared communities between
two sets of nodes. We found that overlap O(Ut , Ft ), O(Ut , Nt ) and O(Ut , Nt )
manifest obvious difference between viral and non-viral cascades by values and
trend over time. For instance, in Figure 10, we see growth of O(Ut , Ft ) for the
viral cascades compared to the non-viral class. In fact, over time, this value
decreases for non-viral cascades as the set of recently exposed users fades away
for non-viral cascades with time.

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

M: 3.0 9.0 12.0 19.0 26.0
A: 3.7 8.7 12.3 18.5 25.2

M: 7.0 13.0 17.0 22.5 31.0
A: 6.7 12.7 16.5 22.2 29.5

60
50
40
30
20
10
0 10

Overlap

Overlap

60
50
40
30
20
10
0 10

13

30

50 100 200

Number of Adopters

30

50 100 200

Number of Adopters

(a) Overlap of adopters and recently exposed (b) Overlap of adopters and recently exposed
users (O(Ut , Ft )) for non-viral cascades
users (O(Ut , Ft )) for viral cascades

M: 0.0 14.0 22.0 32.0 45.0
A: 2.4 13.3 21.1 32.2 45.0
70
60
50
40
30
20
10
0 10
30 50 100 200

Overlap

Overlap

M: 6.0 16.0 23.0 34.0 46.0
A: 5.9 15.6 22.3 33.3 46.1
70
60
50
40
30
20
10
0 10
30 50 100 200

Number of Adopters

Number of Adopters

(c) Overlap of adopters and past exposed (d) Overlap of adopters and past exposed
users (O(Ut , Nt )) for non-viral cascades
users (O(Ut , Nt )) for viral cascades

M: 0.0 14.5 21.5 28.0 38.0
A: 2.5 16.0 23.5 30.0 38.1
70
60
50
40
30
20
10
0 10
30 50 100 200

Overlap

Overlap

M: 3.0 11.0 16.0 23.0 30.0
A: 4.5 12.1 17.1 24.8 31.9
70
60
50
40
30
20
10
0 10
30 50 100 200

Number of Adopters

Number of Adopters

(e) Overlap of recently and past exposed (f) Overlap of recently and past exposed
users (O(Ft , Nt )) for non-viral cascades
users (O(Ft , Nt )) for viral cascades

Fig. 6: Overlap for m = {10, 30, 50, 100, 200}
9.0
15.3

300

Time Since the Original Post

M: 16.0 22.0 34.0 50.0
A: 22.2 31.4 49.7 71.9
3.0
2.5
2.0
1.5
1.0
0.5
0.0 40 60 100 150

Number of Nodes(102 )

Number of Nodes(102 )

M: 7.0 7.0 8.0 9.0
A: 9.5 10.4 11.8 13.1
3.0
2.5
2.0
1.5
1.0
0.5
0.0 40 60 100 150

109
139

300

Time Since the Original Post

(a) Number of adopters (|Ut |) for non-viral (b) Number of adopters (|Ut |) for viral cascascades
cades

Fig. 7: Number of adopters for t âˆˆ {40, 60, 100, 150, 300} (min)

Ruocheng Guo et al.

M: 6.0 6.0 6.0 7.0
A: 7.0 7.4 8.0 8.6

7.0
9.3

60 100 150

300

80
70
60
50
40
30
20
10
0 40

Time Since the Original Post

M: 12.0 14.0 19.0 24.0
A: 12.7 15.7 20.3 24.9
80
70
60
50
40
30
20
10
0 40 60 100 150

Number of Communities

Number of Communities

14

34.0
35.2

300

Time Since the Original Post

(a) Number of communities amongst (b) Number of communities amongst
adopters (|C(Ut )|) for non-viral cascades
adopters (|C(Ut )|) for viral cascades

0.0
15.2

300

Time Since the Original Post

M: 29.5 28.5 27.0 26.0
A: 66.7 44.5 57.4 46.3
100
80
60
40
20
0 40 60 100 150

Number of Communities

Number of Communities

M: 15.0 11.0 7.0 3.0
A: 79.8 61.0 49.3 35.8
100
80
60
40
20
0 40 60 100 150

25.0
38.2

300

Time Since the Original Post

2.0

M: 4.0 11.0 17.0 20.0
A: 22.1 43.5 73.4 105

23.0
140

1.5
1.0

0.5

M: 14.0 27.0 48.0 61.5
A: 22.1 63.1 95.1 146
2.0

Number of Communities(102 )

Number of Communities(102 )

(c) Number of communities amongst recently (d) Number of communities amongst recently
exposed users (|C(Ft )|), non-viral cascades
exposed users (|C(Ft )|) for viral cascades

92.5
261

1.5
1.0

0.5

0.0 40

60 100 150

300

Time Since the Original Post

0.0 40

60 100 150

300

Time Since the Original Post

(e) Number of communities amongst past ex- (f) Number of communities amongst past exposed users (|C(Nt )|) for non-viral cascades posed users (|C(Nt )|) for viral cascades

Fig. 8: Number of communities for t = {40, 60, 100, 150, 300} (min)

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

0.0
0.3

60 100 150

300

Gini

1.0
0.8
0.6
0.4
0.2
0.0 40

M: 0.9 0.9 0.9 0.9
A: 0.9 0.8 0.8 0.8

0.9
0.8

60 100 150

300

1.0
0.8
0.6
0.4
0.2
0.0 40

Gini

M: 0.8 0.8 0.7 0.5
A: 0.8 0.7 0.6 0.4

15

Time Since the Original Post

Time Since the Original Post

(a) Gini impurity of recently exposed users (b) Gini impurity of recently exposed users
(IG (Ft )) for non-viral cascades
(IG (Ft )) for viral cascades

M: 0.8 0.9 0.9 0.9
A: 0.7 0.8 0.9 0.9
1.0
0.8
0.6
0.4
0.2
0.0 40 60 100 150

0.9
0.9

Gini

0.9
0.8

Gini

M: 0.6 0.8 0.8 0.9
A: 0.5 0.7 0.8 0.8
1.0
0.8
0.6
0.4
0.2
0.0 40 60 100 150

300

Time Since the Original Post

300

Time Since the Original Post

(c) Gini impurity of past exposed users (d) Gini impurity of past exposed users
(IG (Nt )) for non-viral cascades
(IG (Nt )) for viral cascades

Fig. 9: Gini impurity for t = {40, 60, 100, 150, 300} (min)
M: 10.0 11.0 13.0 14.0
A: 11.5 12.6 14.4 15.8
60
50
40
30
20
10
0 40 60 100 150

18.0
19.2

Overlap

0.0
1.9

Overlap

M: 5.0 4.0 3.0 2.0
A: 5.7 4.8 3.8 2.9
60
50
40
30
20
10
0 40 60 100 150

300

Time Since the Original Post

300

Time Since the Original Post

(a) Overlap of adopters and recently exposed (b) Overlap of adopters and recently exposed
users (O(Ut , Ft )) for non-viral cascades
users (O(Ut , Ft )) for viral cascades

Fig. 10: Overlap for t = {40, 60, 100, 150, 300} (min)

16

Ruocheng Guo et al.

5 Classification Experiments
Here we examine our experiments for predicting whether a cascade becomes
viral - when the number of adopters exceeds a size threshold (T H = 500)
given that either the cascade has 50 adopters (m = 50) or has progressed for
an hour (t = 60). We shall refer to these as size-based and time-based prediction problems. Based on the distribution of final size of cascades in this dataset
(see Figure 1a), as shown in Table 2, this binary classification task deals with
two heavily imbalanced classes. Hence, we report performance measurements
(precision, recall and F1 score) for only the minority (viral) class. Throughout
the course of our experiments, we found that varying threshold (slightly modifying the definition of â€œviralâ€) for only the training set allows for a trade-off
between precision and recall. We study the trend of performance metrics in
two cases:
â€“ The threshold for test set is maintained as T Hts = 500 while the training
threshold is varied T Htr âˆˆ {300, 400, 500, 600, 700}.
â€“ The two thresholds are kept as the same T H while we modify this value
T H âˆˆ {300, 400, 500, 600, 700}.
Table 3 shows the groups of features used in our prediction tasks. The
features introduced in this paper are groups Am (size-based) and At (timebased). We compare our features (Group Am , At ) with the community features
extracted in (Weng et al, 2014) (Group Bm ,Bt ) and nodal features of the seed
adopter (Group Cm and Ct ). Here nodal measures of the seed adopter refer
to k-shell number, out-degree, in-degree, pagerank and eigenvector, which are
computed based on the social network G. In previous work (Pei et al, 2014), kshell number of the seed adopter node is shown to be correlated to the average
size of cascades. However, cascades from the seed adopter nodes with the same
k-shell number can end up with quite different size (Shakarian et al, 2015). As
baseline methods, average time to adoption (group Dm ) is applied to the sizebased experiment while cascade size at time t (group Dt ) is evaluated for timebased prediction. We extracted each group of community-based features (Am ,
At , Bm , Bt ) with all the three community detection algorithms mentioned
in Section 2: Louvain, Infomap and SLM. Therefore, for both size-based and
time-based prediction, there are 8 groups of features. Among them, Bm and
Bt were the best performing feature set in the paper (Weng et al, 2014) for a
comparable task.3
Additionally, we study the average size of correctly classified viral cascades
and the other viral samples using features in groups Am and At . We also investigate the significance and performance of individual and certain combinations
of features introduced in this paper.
3 This was their highest-performing set of features for predicting cascades that grew from
50 to 367 and 100 to 417 reposts. We also included the baseline feature in this set as we
found it improved the effectiveness of this approach.

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

17

Table 3: Features for prediction tasks (size-based and time-based): Am , Bm ,
At and Bt are computed based on three community detection algorithms (Louvain, Infomap and SLM)
Name

Feature(s) over size

Name

Feature(s) over time

Am

|C(Ft )|,|C(Nt )|,
IG (Ut ),IG (Ft ),IG (Nt ),
O(Ut , Ft ),O(U
t , Nt ),O(Ft , Nt ),
1 Pm
|Ft |,|Nt |, m
i=1 t(i)
where t stands for t(m) with
m âˆˆ {30, 50}

At

|C(Ft )|,|C(Nt )|,
IG (Ut ),IG (Ft ),IG (Nt ),
O(Ut , Ft ),O(Ut , Nt ),O(Ft , Nt ),
|Ut |,|Ft |,|Nt |
for time t âˆˆ {40, 60}(min)

Bm

Community Features Mentioned P
in (Weng et al, 2014)
m
1
and m
i=1 t(i), m = 50

Bt

Community Features Mentioned in (Weng et al, 2014)
and |Ut |, t = 60(min)

Cm

Nodal
1 Pm

Ct

Nodal Features and |Ut |, t =
60(min)

Dt

|Ut |, t = 60(min)

Dm

m
1
m

i=1

Pm

i=1

Features
t(i), m = 50
t(i), m = 50

and

5.1 Cascade Prediction Results
We split cascades into training set and testing set using ten-fold cross-validation.
All classification experiments are repeated for 10 times to ensure the results
do not take any advantage of randomness in picking training and testing sets.
First we carried out the prediction tasks with fixed thresholds for both training
and testing T Htr = 500, T Hts = 500. Then we modify the training threshold
T Htr âˆˆ {300, 400, 500, 600, 700} to show how this achieves a tradeoff between
precision and recall. The difference in average final size between correctly
classified viral cascades and incorrectly classified ones is also monitored over
T Htr âˆˆ {300, 400, 500, 600, 700} to show the potential to predict exact number of adopters by features in Am and At . Furthermore, we modify threshold
of both training and testing sets T H âˆˆ {300, 400, 500, 600, 700} to show the
robustness of our features on related classification problems. We used the oversampling method SMOTE (Chawla et al, 2002) with random forest classifier to
generate synthetic samples for the viral class. Other, lesser-performing classifiers were also examined (including SVM, MLP, and other ensemble methods)
and are not reported here. All results shown in this section is a sample mean
produced by repeated experiments (10 times) under each combination of variables. Error bars represent one standard deviation.
Size-based prediction. We studied cascades of size 50 that reached 500 for this
task. There are 13,285 cascades that can reach the size m = 50 while only 200
out of them reached the size of 500. Maintaining the threshold T H = 500,
Figure 11 shows random forest classifier trained with features in group Am
can outperform the other groups with any of the three community detection algorithms. The tradeoff between precision and recall can be achieved by
changing the training threshold T Htr while maintaining the testing thresh-

18

Ruocheng Guo et al.

Classification Results: TH = 500

0.7
0.6
0.5
0.4

Louvain Am
Infomap Am
SLM Am
Louvain Bm
Infomap Bm
SLM Bm
Cm
Dm

0.3
0.2
0.1
0.0

Precision

Recall

F1 Score

Fig. 11: Classification results based on groups of features (Am ,Bm ,Cm ,Dm )
extracted with three community detection algorithms (Louvain, Infomap and
SLM) when m = 50 for fixed T Htr = 500, T Hts = 500. Error bars represent
one standard deviation.

old T Hts = 500 (see Figure 12). We also note that the average final size of
viral cascades correctly classified by the classifier increases with the training threshold. With threshold T H âˆˆ {300, 400, 500, 600, 700} on both training
and testing samples, the features introduced in this paper (Am ) consistently
outperform those previously introduced (Bm ) â€“ see Figure 13. The fact that
features in Bm are not able to maintain their predictability over different T H
can be explained as that they only count the number of users on recently exposed users instead of taking the community structure of them or the decay of
probability to repost over time into consideration. As shown in Figure 12a, 12c
and 12e, while the trends relating to this tradeoff are similar among the various community detection algorithms, the Louvain algorithm led to superior
performance for precision and F1. Infomap and SLM generally outperformed
Louvain in terms of recall for both feature sets. We also note that our features
outperform those of Weng et al. regardless of the testing/training thresholds
and the selected community finding algorithm.
Time-based prediction. As shown in Table 2, there are 3,444 cascades in our
dataset reached the size of m = 5 within 60 (min) with only 5% from the
minority class. When the threshold is kept as T H = 500 for both training
set and testing set, we obtain the results shown in Figure 14 again showing
that the features introduced in this paper (At ) outperform the other feature
sets in terms of recall, precision and F1 score, no matter which community
detection algorithm is used. By modifying threshold for training samples only,
two phenomenon are discovered. First, a tradeoff between precision and recall
can be manipulated by controlling the training threshold (T Htr ). This is shown
in Figure 15a, 15c, 15e. Second, as shown in Figure 15b, 15d, 15f, with T Htr
increasing, the average final size of correctly classified viral cascades also grows.
Furthermore, we modify the threshold for training and testing sets together

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

1.0

precision
recall

f1 score

Size (103 )

1.2
0.8
0.6
0.4
0.2

1.4
1.3
1.2
1.1
1.0
0.9
0.8
0.7

300 400 500 600 700
Training Threshold

19

correct
incorrect
mean

300 400 500 600 700
Training Threshold

(b) Average final size of viral cascades (cor(a) Precision, recall, and F1 score for different rectly classified, mean and incorrectly classitraining thresholds, using Louvain algorithm. fied), using Louvain algorithm.

1.0

1.2
precision
recall

1.1

f1 score

Size (103 )

1.2
0.8
0.6
0.4

1.0

correct
incorrect
mean

0.9
0.8
0.7

0.2

300 400 500 600 700
Training Threshold

300 400 500 600 700
Training Threshold

(d) Average final size of viral cascades (cor(c) Precision, recall, and F1 score for different rectly classified, mean and incorrectly classitraining thresholds, using Infomap algorithm. fied), using Infomap algorithm.

1.0

1.2
precision
recall

1.1

f1 score

Size (103 )

1.2
0.8
0.6
0.4

1.0

correct
incorrect
mean

0.9
0.8
0.7

0.2
300 400 500 600 700
Training Threshold

300 400 500 600 700
Training Threshold

(f) Average final size of viral cascades (cor(e) Precision, recall, and F1 score for different rectly classified, mean and incorrectly classitraining thresholds, using SLM algorithm.
fied), using SLM algorithm.

Fig. 12: Prediction results when T Htr = {300, 400, 500, 600, 700} for
Am (Louvain, Infomap and SLM). Error bars represent one standard deviation.

20

Ruocheng Guo et al.

0.8
0.7
0.6
0.5
0.4
0.3
precision
f1 score
0.2
recall
0.1 300 400 500 600 700
Training/Testing Threshold

0.8
precision
f1 score
recall
0.7
0.6
0.5
0.4
0.3
0.2
0.1 300 400 500 600 700
Training/Testing Threshold

(a) Classification results for features in group (b) Classification results for features in group
Am (Louvain)
Bm (Louvain)

0.8
precision
f1 score
recall
0.7
0.6
0.5
0.4
0.3
0.2
0.1 300 400 500 600 700
Training/Testing Threshold

0.8
precision
f1 score
recall
0.7
0.6
0.5
0.4
0.3
0.2
0.1 300 400 500 600 700
Training/Testing Threshold

(c) Classification results for features in group (d) Classification results for features in group
Am (Infomap)
Bm (Infomap)

0.8
precision
f1 score
recall
0.7
0.6
0.5
0.4
0.3
0.2
0.1 300 400 500 600 700
Training/Testing Threshold

0.8
precision
f1 score
recall
0.7
0.6
0.5
0.4
0.3
0.2
0.1 300 400 500 600 700
Training/Testing Threshold

(e) Classification results for features in group (f) Classification results for features in group
Am (SLM)
Bm (SLM)

Fig. 13: Prediction results based on groups of features extracted for m =
50 when T H = {300, 400, 500, 600, 700}. Error bars represent one standard
deviation.

to show the reliability of features in group At is better than ones in Bt (See
Figure 16). Here, we noted similar trends with regard to both feature sets and
community finding algorithms as found in the size-based tests.

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

Classification Results: TH = 500

0.7
0.6
0.5
0.4

21

Louvain At
Infomap At
SLM At
Louvain Bt
Infomap Bt
SLM Bt
Ct
Dt

0.3
0.2
0.1
0.0

Precision

Recall

F1 Score

Fig. 14: Classification results based on groups of features (At ,Bt ,Ct ,Dt ) extracted with three community detection algorithms (Louvain, Infomap and
SLM) when t = 60 for fixed T Htr = 500, T Hts = 500. Error bars represent
one standard deviation.

5.2 Feature Investigation

Here we investigate the importance of each feature in Am (Louvain) and Am
(Louvain) as communities detected by Louvain algorithm achieves the best
classification results out of the three. With T Htr = 500 and T Hts = 500, we
trained 200 randomized logistic regressions models (100 for Am and 100 for
At ) - with each assigning weights to the features in those sets. We then categorized the features with weight larger than 0.01 (on average) into groups such
as overlap, gini impurity, etc. Then, we performed classification on the basis
of single feature group or combination of such groups. The average weights
assigned are shown in Table 4 while classification results (by random forest
with SMOTE) are depicted in Figure 17 for groups and combinations of them.
As shown, overlaps can make significant contribution to the prediction tasks.
Intuitively, communication between two sets of nodes is more likely to happen
in their shared communities - which is consistent with the results of (Ugander
et al, 2012). This implies that the larger overlap value, the more likely one set
would repost from the otherFor example, we can infer that viral cascades tend
to have larger O(Ut , Ft ) value therefore adopters in them have larger chance to
motivate the recently exposed users to repost than non-viral cascades. Figure 6
and Figure 10 provide evidence of this phenomenon.

22

Ruocheng Guo et al.

1.2
1.0

precision
recall

f1 score

Size (103 )

0.8
0.6
0.4
0.2

1.4
1.3
1.2
1.1
1.0
0.9
0.8
0.7

300 400 500 600 700
Training Threshold

correct
incorrect
mean

300 400 500 600 700
Training Threshold

(b) Average final size of viral cascades (cor(a) Precision, recall, and F1 score for different
rectly classified, mean and incorrectly classitraining thresholds, using Louvain algorithm.
fied), using Louvain algorithm.

1.2
1.0

1.2
precision
recall

f1 score

Size (103 )

0.8

1.1

0.6
0.4

1.0

correct
incorrect
mean

0.9
0.8
0.7

0.2

300 400 500 600 700
Training Threshold

300 400 500 600 700
Training Threshold

(d) Average final size of viral cascades (cor(c) Precision, recall, and F1 score for different
rectly classified, mean and incorrectly classitraining thresholds, using Infomap algorithm.
fied), using Infomap algorithm.

1.2
1.0

1.2
precision
recall

f1 score

Size (103 )

0.8

1.1

0.6
0.4

1.0

correct
incorrect
mean

0.9
0.8
0.7

0.2
300 400 500 600 700
Training Threshold

300 400 500 600 700
Training Threshold

(f) Average final size of viral cascades (cor(e) Precision, recall, and F1 score for different
rectly classified, mean and incorrectly classitraining thresholds, using SLM algorithm.
fied), using SLM algorithm.

Fig. 15: Prediction results when T Htr âˆˆ {300, 400, 500, 600, 700} for At (Louvain, Infomap and SLM). Error bars represent one standard deviation.

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

0.8
0.7
0.6
0.5
0.4
0.3
precision
f1 score
0.2
recall
0.1 300 400 500 600 700
Training/Testing Threshold

23

0.8
precision
f1 score
recall
0.7
0.6
0.5
0.4
0.3
0.2
0.1 300 400 500 600 700
Training/Testing Threshold

(a) Classification results for features in group (b) Classification results for features in group
At (Louvain)
Bt (Louvain)

0.8
precision
f1 score
recall
0.7
0.6
0.5
0.4
0.3
0.2
0.1 300 400 500 600 700
Training/Testing Threshold

0.8
precision
f1 score
recall
0.7
0.6
0.5
0.4
0.3
0.2
0.1 300 400 500 600 700
Training/Testing Threshold

(c) Classification results for features in group (d) Classification results for features in group
At (Infomap)
Bt (Infomap)

0.8
precision
f1 score
recall
0.7
0.6
0.5
0.4
0.3
0.2
0.1 300 400 500 600 700
Training/Testing Threshold

0.8
precision
f1 score
recall
0.7
0.6
0.5
0.4
0.3
0.2
0.1 300 400 500 600 700
Training/Testing Threshold

(e) Classification results for features in group (f) Classification results for features in group
At (SLM)
Bt (SLM)

Fig. 16: Prediction results based on groups of features extracted for t =
60(min) for T H âˆˆ {300, 400, 500, 600, 700}. Error bars represent one standard
deviation.
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0 Precision

ol
at
gini
ol+at

Recall

gini+at
ol+gini
ol+at+gini

F1 Score

(a) Classification results for subsets of
Am (Louvain): ol means overlap, gini means
gini impurity,P
at represents average time to
50
1
adoption ( 50
i=1 t(i)).

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0 Precision

ol
cs
gini
ol+cs

Recall

gini+cs
ol+gini
ol+cs+gini

F1 Score

(b) Classification results for subsets of
At (Louvain): ol means overlap, gini means
gini impurity, cs represents number of
adopters (|U60 |).

Fig. 17: Classification results (random forest with SMOTE) based on subsets
of features from Am and At (by Louvain algorithm)

24

Ruocheng Guo et al.

Group Name

Features(Am (Louvain))

Weights

Features(At (Louvain))

Weights

IG (Ft(50) )

0.020

IG (U60 )

0.039

IG (Nt(50) )

0.021

IG (U40 )

0.049

IG (Nt(30) )

0.521

IG (F40 )

0.331

O(Ut(30) , Ft(30) )

0.503

O(U60 , F60 )

0.500

O(Ut(30) , Nt(30) )

0.037

O(U60 , N60 )

0.538

O(Ft(30) , Nt(30) )

0.227

O(F60 , N60 )

0.409

O(Ut(50) , Ft(50) )

0.500

O(U40 , F40 )

0.628

O(Ft(50) , Nt(50) )

0.257

O(U40 , N40 )

0.509

O(F40 , N40 )

0.288

|U60 |

0.072

Gini Impurity

Overlap

Baseline

1
50

P50

i=1

t(i)

1.0

Table 4: Weights of features assigned by randomized logistic regression models

6 Related Work
Early works about popularity prediction with data driven approach simplified
the problem of cascade prediction as modeling one step information propagation Galuba et al (2010); Bian et al (2014); Zhang et al (2013) or as predicting
the near term popularity Gupta et al (2012). As the real pioneer of cascade
prediction, the work (Bakshy et al, 2011) devised a regression model for this
task and was one of the first attempts to explore this problem. They noted
that the severe imbalance of the data due to a power-law relationship between
cascade size and frequency (which we also observed) hindered the creation of
useful model - they obtained an R2 value of only 0.3 for their regression model.
The later work of (Jenders et al, 2013) also studies the problem, again taking a machine learning approach and identify several useful features to obtain
relatively high precision and recall. However, in their evaluation, they artificially balance the dataset - they ensure that each fold had equal amounts of
viral and non-viral tweets. The work of (Cheng et al, 2014) predicts â€œviralâ€
cascades with high precision and recall, but defines â€œviralâ€ as cascades that
can double in size (which also has the effect of balancing the classes in the
dataset). The very recent work of (Weng et al, 2014) also looks at predicting viral cascades and does leverage some community-based features, some of
which are also inspired by structural diversity - though their structural diversity features are more limited than in this study - we perform a comparison
with their structural diversity method (see previous section). In a nutshell,
there are two main points differing our work from the ones mentioned in this
section: (1). the method proposed by this paper does not need the content of
microblogs or the underlying topology based on friendship relationships (2).
this method is able to provide a reliable performance in prediction of order-of-

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

25

magnitude increase of cascade size. In a conference version of this paper (Guo
et al, 2015) we described the basics of tis approach. However that work did not
include time-based results, examination of various underlying community finding algorithms and how each sub-group of features performs in independent
classification experiments.
In addition to the work on cascades, there is much related work on structural diversity. This concept was first studied in (Ugander et al, 2012) and
later explored in the work of (Zhang et al, 2013; Shakarian et al, 2014; Li
et al, 2015; Bao et al, 2013a,b; Huang et al, 2013). However, these papers
leverage structural diversity for a variety of other social network applications
including the creation of new diffusion models, the study of peer influence,
identifying influential nodes, and ranking communities. Finally, we note that
the popular work on diffusion in the areas of computer science (Kempe et al,
2003), physics (Gallos et al, 2010), and biology (Lieberman et al, 2005) have
led to a ground swell of research on this topic over the past decade, please see
(Shakarian et al, 2015) for a review of major results.

7 Conclusion
In this paper, we explored the effect of structural diversity on a diffusion
process which allowed us to predict viral cascades. Moving forward, we look to
integrate our structural-diversity approach with content information (which
we believe will further increase performance) as well as study how to best
operationalize this method in a system to detect viral cascades in near-real
time.

8 Acknowledgment
Some of the authors of this paper are supported by by AFOSR Young Investigator Program (YIP) grant FA9550-15-1-0159, ARO grant W911NF-15-10282, and the DoD Minerva program. Portions this work were also disclosed
in U.S. provisional patent 62/201,517. A non-provisional patent is currently
being filed.

References
Alstott J, Bullmore E, Plenz D (2014) powerlaw: a python package for analysis
of heavy-tailed distributions
Bakshy E, Hofman JM, Mason WA, Watts DJ (2011) Everyoneâ€™s an Influencer: Quantifying Influence on Twitter. In: Proceedings of the Fourth
ACM International Conference on Web Search and Data Mining, ACM, New
York, NY, USA, WSDM â€™11, pp 65â€“74, DOI 10.1145/1935826.1935845, URL
http://dx.doi.org/10.1145/1935826.1935845

26

Ruocheng Guo et al.

Bao P, Shen HW, Chen W, Cheng XQ (2013a) Cumulative effect in information diffusion: empirical study on a microblogging network. PloS one
8(10):e76,027
Bao Q, Cheung WK, Zhang Y (2013b) Incorporating structural diversity of
neighbors in a diffusion model for social networks. In: Web Intelligence (WI)
and Intelligent Agent Technologies (IAT), 2013 IEEE/WIC/ACM International Joint Conferences on, IEEE, vol 1, pp 431â€“438
Bian J, Yang Y, Chua TS (2014) Predicting trending messages and diffusion
participants in microblogging network. In: Proceedings of the 37th international ACM SIGIR conference on Research & development in information
retrieval, ACM, pp 537â€“546
Blondel VD, Guillaume JL, Lambiotte R, Lefebvre E (2008) Fast unfolding
of communities in large networks. Journal of Statistical Mechanics: Theory
and Experiment 2008(10):P10,008
Breiman L, Friedman J, Stone CJ, Olshen RA (1984) Classification and regression trees. CRC press
Chawla NV, Bowyer KW, Hall LO, Kegelmeyer WP (2002) Smote: synthetic
minority over-sampling technique. Journal of artificial intelligence research
16(1):321â€“357
Cheng J, Adamic L, Dow PA, Kleinberg JM, Leskovec J (2014) Can cascades
be predicted? In: Proceedings of the 23rd international conference on World
wide web, International World Wide Web Conferences Steering Committee,
pp 925â€“936
Gallos L, Havlin S, Kitsak M, Liljeros F, Makse H, Muchnik L, Stanley H
(2010) Identification of influential spreaders in complex networks. Nature
Physics 6(11):888â€“893
Galuba W, Aberer K, Chakraborty D, Despotovic Z, Kellerer W (2010) Outtweeting the twitterers-predicting information cascades in microblogs. In:
Proceedings of the 3rd conference on Online social networks, vol 39, p 3aÌ‚AS3
Grabowicz PA, Ramasco JJ, Moro E, Pujol JM, Eguiluz VM, et al (2012)
Social features of online networks: The strength of intermediary ties in online
social media. PloS one 7(1):e29,358
Guo R, Shaabani E, Bhatnagar A, Shakarian P (2015) Toward order-ofmagnitude cascade prediction. In: Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
2015, ACM, pp 1610â€“1613
Gupta M, Gao J, Zhai C, Han J (2012) Predicting future popularity trend of
events in microblogging platforms. Proceedings of the American Society for
Information Science and Technology 49(1):1â€“10
Huang X, Cheng H, Li RH, Qin L, Yu JX (2013) Top-k structural diversity
search in large networks. Proceedings of the VLDB Endowment 6(13):1618â€“
1629
Jenders M, Kasneci G, Naumann F (2013) Analyzing and predicting viral
tweets. In: Proceedings of the 22Nd International Conference on World Wide
Web Companion, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, WWW â€™13 Compan-

Toward Early and Order-of-Magnitude Cascade Prediction in Social Networks

27

ion, pp 657â€“664, URL http://dl.acm.org/citation.cfm?id=2487788.2488017
Kempe D, Kleinberg J, Tardos E (2003) Maximizing the spread of influence
through a social network. In: Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM,
New York, NY, USA, KDD â€™03, pp 137â€“146, DOI 10.1145/956750.956769,
URL http://doi.acm.org/10.1145/956750.956769
Li RH, Qin L, Yu JX, Mao R (2015) Influential community search in large
networks. Proceedings of the VLDB Endowment 8(5)
Lieberman E, Hauert C, Nowak MA (2005) Evolutionary dynamics on graphs.
Nature 433(7023):312â€“316, DOI 10.1038/nature03204
Pei S, Muchnik L, Andrade Jr JS, Zheng Z, Makse HA (2014) Searching for
superspreaders of information in real-world social media. Scientific reports
4
Raghavan UN, Albert R, Kumara S (2007) Near linear time algorithm to
detect community structures in large-scale networks. Physical Review E
76(3):036,106
Rosvall M, Bergstrom CT (2008) Maps of random walks on complex networks
reveal community structure. Proceedings of the National Academy of Sciences 105(4):1118â€“1123
Shakarian P, Gerdes L, Lei H (2014) Circle-based tipping cascades in social
networks. In: WSDM Workshop on Diffusion Networks and Cascade Analytics
Shakarian P, Bhatnagar A, Aleali A, Guo R, Shaabani E (2015) Diffusion in
Social Networks. Springer (in press)
Ugander J, Backstrom L, Marlow C, Kleinberg J (2012) Structural diversity in social contagion. Proceedings of the National Academy of Sciences
109(16):5962â€“5966
Waltman L, van Eck NJ (2013) A smart local moving algorithm for large-scale
modularity-based community detection. The European Physical Journal B
86(11):1â€“14
Weng L, Menczer F, Ahn YY (2014) Predicting successful memes using network and community structure. In: Eighth International AAAI Conference
on Weblogs and Social Media
Zhang J, Liu B, Tang J, Chen T, Li J (2013) Social influence locality for modeling retweeting behaviors. In: Proceedings of the Twenty-Third international
joint conference on Artificial Intelligence, AAAI Press, pp 2761â€“2767

