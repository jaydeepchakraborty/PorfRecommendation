A Quantitative Approach to Belief Revision in Structured Probabilistic Argumentation
Gerardo I. Simari · Paulo Shakarian · Marcelo A. Falappa

Received: July 30, 2015/ Accepted: date

Abstract Many real-world knowledge-based systems must deal with information coming from different sources that invariably leads to incompleteness, overspecification, or inherently uncertain content. The presence of these varying levels of uncertainty doesn't mean that the information is worthless ­ rather, these are hurdles that the knowledge engineer must learn to work with. In this paper, we continue work on an argumentation-based framework that extends the well-known Defeasible Logic Programming (DeLP) language with probabilistic uncertainty, giving rise to the Defeasible Logic Programming with Presumptions and Probabilistic Environments (DeLP3E) model. Our prior work focused on the problem of belief revision in DeLP3E, where we proposed a non-prioritized class of revision operators called AFO (Annotation Function-based Operators) to solve this problem. In this paper, we further study this class and argue that in some cases it may be desirable to define revision operators that take quantitative aspects into account, such as how the probabilities of certain literals or formulas of interest change after the revision takes place. To the best of our knowledge, this problem has not been addressed in the argumentation literature to date. We propose the QAFO (Quantitative Annotation Function-based Operators) class of operators, a subclass of AFO, and then go on to study the complexity of several problems related to their specification and application in revising knowledge bases. Finally, we present an algorithm for computing the probability that a literal is warranted in a DeLP3E knowledge base, and discuss how it could be applied towards implementing QAFO-style operators that compute approximations rather than exact operations.
G.I. Simari, M.A. Falappa Dept. of Comp. Sci. and Eng., Universidad Nacional del Sur and CONICET, Argentina E-mail: {gis,mfalappa}@cs.uns.edu.ar P. Shakarian Arizona State University, Tempe, AZ, USA E-mail: shak@asu.edu

2

G.I. Simari, P. Shakarian, and M.A. Falappa

Keywords Structured Argumentation · Belief Revision · Reasoning under Probabilistic Uncertainty

1 Introduction and Motivation Many real-world knowledge-based systems must deal with information coming from different sources that invariably leads to uncertain content, be it from gaps in knowledge (incompleteness), overspecification (inconsistency), or because the knowledge is inherently uncertain (such as weather forecasts or measurements that are necessarily imprecise). Far from considering such uncertain knowledge useless, knowledge engineers face the challenge of putting it to its best possible use when solving a wide range of problems. In particular, one basic problem that needs to be investigated in depth is that of revising such knowledge bases in a principled manner. In this paper, we continue work on that combines the well-known Defeasible Logic Programming (DeLP) language (actually, an extension called PreDeLP to handle presumptions) with probabilistic uncertainty, which led to the development of the DeLP3E model (Defeasible Logic Programming with Presumptions and Probabilistic Environments). In previous work [39, 41], we studied a class of non-prioritized belief revision operators called AFO that allows changes to be made only to probabilistic annotations when addressing the incorporation of an epistemic input into the knowledge base. Here, we propose a subclass called QAFO that takes quantitative aspects into account when performing revisions, such as how the probabilities of certain literals or formulas of interest change after the revision takes place. To the best of our knowledge, this problem has not been addressed in the argumentation literature to date. The main contributions presented in this paper are briefly summarized as follows: ­ As building blocks to be used in our quantitative belief revision operators, we define: (i) warrant probability functions (WPFs), which have as domains either conjunctions or disjunctions of ground literals that are mapped to the probability that they are warranted in the DeLP3E program; and (ii) revision objective functions (ROFs), which take as input two DeLP3E programs I1 , I2 and an epistemic input and return a numeric score that is interpreted as the value of obtaining I2 from I1 when revising it by the epistemic input. ROFs generally include WPFs in their definitions. ­ We propose the QAFO class of operators, a subclass of AFO that allows modifications to the annotation function to be carried out as part of the belief revision operation, but focusing on optimizing a given ROF, as described above. ­ We study the complexity of several problems related to our approach; in particular, we present:

Quantitative Belief Revision in Structured Probabilistic Argumentation

3

(i) A new lower bound on the complexity of deciding the warrant status of a literal or a conjunction/disjunction of literals in a (classical) PreDeLP program; (ii) Computing WPFs in general is #P-hard; (iii) Point (ii) holds even when computing probabilities of worlds in the environmental model can be done in polynomial time; (iv) Computing WPFs in the special case in which Nilsson's probabilistic logic [33] is used is NP-complete; (v) By combining the intuition behind the proof of point (iv) and further conditions, we identify a class of instances for which WPFs can be computed in polynomial time; and (vi) Under the same conditions as point (v), we show that QAFO revisions are NP-complete; furthermore, we show that the problem has the same complexity even when the revision objective function is constrained to be a simple sum of warranting probabilities for atoms in the AM. ­ We present an algorithm for computing the probability that a literal is warranted in a DeLP3E knowledge base, and discuss its application towards implementing QAFO-style operators that compute approximations rather than perform exact operations.

The rest of this paper is organized as follows: Section 2 discusses preliminary concepts and the DeLP3E framework, which was first introduced in [39, 41]. Section 3 motivates the specialization of AFO operators to take into account quantitative aspects, and presents the class QAFO. Section 4 studies the complexity of several problems related to QAFO and the application of such operators in revising DeLP3E knowledge bases. Section 5 studies a novel approach to reasoning about probabilities of literals in DeLP3E, called warranting formulas, and their application in the implementation of QAFO-style operators that address the high computational cost issuess by applying heuristics the trade off optimality for tractability. Finally, Sections 6 and 7 present related work and conclusions, respectively. Throughout the entire paper, we illustrate the presentation via a running example that is inspired on the use of DeLP3E in medical diagnosis, which is the kind of real-world application in which we envision our work being of most use; we have also explored its application in the related scenario of solving the attribution problem1 in cyber security and cyber warfare [40], with encouraging feedback from the community as to its potential impact.
1 Essentially, given a cyber event of interest, the attribution problem involves finding out who was responsible for it. This is especially well suited for argumentation and belief revision due to the ease with which a potential culprit can plant false or misleading clues, hence giving rise to an inconsistent knowledgebase. See [38] for further discussion.

4

G.I. Simari, P. Shakarian, and M.A. Falappa

2 Preliminaries on the DeLP3E Framework The DeLP3E framework is comprised of two distinct, but interrelated models of the world. The first is called the environmental model (referred to from now on as the "EM"), and is used to describe uncertain knowledge about the domain that is subject to probabilistic events. The second one is called the analytical model (referred to as the "AM"), and contains knowledge that is either strict or defeasible, as described below ­ this will be useful in the analysis of competing hypotheses that can account for a given phenomenon (what we will generally call queries). The AM is composed of a classical (that is, non-probabilistic) PreDeLP [32] program in order to allow for contradictory information, giving the system the capability to model competing explanations for a given query. In general, the EM contains knowledge such as evidence, uncertain facts, or knowledge about agents and systems. The AM, on the other hand, contains knowledge that may or may not be strictly valid, yet it does not depend on probabilistic events. Indeed, dividing knowledge between the AM and EM is a knowledge engineering task, and its adequate resolution will call for design decisions to be made; note that this separation also allows for a different kind of uncertainty to be modeled ­ defeasible rules and presumptions can be leveraged when there is no probabilistic information available but we still wish to maintain that a specific portion of the knowledge base is uncertain. In the rest of this section, we formally describe these two models, as well as how knowledge in the AM can be annotated with information from the EM ­ these annotations specify the conditions under which the various statements in the AM can potentially be true. Basic Language. We assume sets of variables and constants, denoted with V and C, respectively. The language also contains a set of n-ary predicate symbols; the EM and AM use separate sets of predicate symbols, denoted with PEM and PAM , respectively ­ the two models can, however, share variables and constants. As usual, a term is composed of either a variable or a constant. Given terms t1 , ..., tn and n-ary predicate symbol p, p(t1 , ..., tn ) is called an atom; if t1 , ..., tn are constants, then the atom is said to be ground. The sets of all ground atoms for EM and AM are denoted with GEM and GAM , respectively; finally, we also use notation LAM to denote the set of all (ground) literals: {a | a  GAM }  {¬a | a  GAM }. Note that even though in general the language allows variables, for simplicity we will assume throughout this paper that all objects are ground (propositional). Example 1 Consider a medical diagnosis scenario2 in which a patient goes to a hospital exhibiting certain symptoms: shortness of breath, sporadic fainting (loss of consciousness for brief periods of time), and some signs of memory
2 This and all related examples in this paper, though inspired by potential real-world applications, make many simplifying assumptions in order to allow for a concise presentation of the key concepts that we wish to illustrate.

Quantitative Belief Revision in Structured Probabilistic Argumentation PEM : anx risk dep risk FN-anx test The patient falls into the category of people at risk of suffering anxiety. The patient falls into the category of people at risk of suffering depression. Event associated with a false negative coming up when performing a test to see whether a patient is showing anxiety-related symptoms. Event associated with a false negative coming up when performing a blood test for the presence of toxic substances. The patient shows signs of memory loss. The patient suffers shortness of breath. The patient suffers short-term loss of consciousness. The patient is diagnosed with an anxiety-related disorder. The patient is diagnosed with a depression-related disorder. Patient diagnosed with misuse of sleeping aids. Test proves absence of anxiety-related disorders. Test proves absence of toxins in blood. Test proves presence of depression-related disorders.

5

FN-tox screen

PAM :

mem loss short breath fainting anxiety depression sleep aid misuse neg anx test neg tox screen pos dep test

Fig. 1 Explanation of the meaning of the predicates used in the running example.

loss. Figure 1 shows the predicates that we will use throughout the paper in the running example. As shown in the figure (and discussed in more detail below), some of these predicates comprise the analytical model (AM), while others are part of the environmental model (EM). For instance, in our example, predicates stating the presence of symptoms such as memory loss and shortness of breath, as well as those representing diagnoses, such as anxiety and depression, are part of the analytical model. On the other hand, the environmental model contains predicates that are associated with uncertain events, such as false negatives coming up when a test is performed or the risk that the patient will be affected by anxiety-related disorders. Note that in the running example we make use of the term "at risk" according to the common use of this expression in the medical domain, i.e., characterized by high risk or susceptibility, such as to a certain disease. Given set of ground atoms, a world is any subset of atoms ­ those that belong to the set are said to be true in the world, while those that do not are false. Therefore, there are 2|GEM | possible worlds in the EM and 2|GAM | worlds in the AM. These sets are denoted with WEM and WAM , respectively. In order to avoid worlds that do not model possible situations given a particular domain, we include integrity constraints of the form oneOf(A ), where A is a subset of ground atoms. Intuitively, such a constraint states that any world

6

G.I. Simari, P. Shakarian, and M.A. Falappa

where more than one of the atoms from set A appears is invalid. We use ICEM and ICAM to denote the sets of integrity constraints for the EM and AM, respectively, and the sets of worlds that conform to these constraints is denoted with WEM (ICEM ) and WAM (ICAM ), respectively. Finally, logical formulas arise from the combination of atoms using the traditional connectives (, , and ¬). As usual, we say that a world  satisfies formula (f ), written  |= f , iff: (i) If f is an atom, then  |= f iff f  ; (ii) if f = ¬f then  |= f iff  |= f ; (iii) if f = f  f then  |= f iff  |= f and  |= f ; and (iv) if f = f  f then  |= f iff  |= f or  |= f . We use the notation formEM , formAM to denote the set of all possible (ground) formulas in the EM and AM, respectively; finally, we use basicAM to denote all possible conjunctions or disjunctions of literals from LAM , which we refer to as basic formulas.

2.1 Environmental Model In this paper, we generalize the approach taken in our previous work [39, 41] for the environmental model ­ here, we simply assume that we have a probabilistic model defined over GEM , which represents a probability distribution over WEM . Definition 1 (Probabilistic Model) Given sets PEM , V, and C, and corresponding sets GEM and WEM , a probabilistic model EM is any function Pr : WEM  [0, 1] such that WEM Pr () = 1. Examples of probabilistic models that can be used are Bayesian networks (BNs) [34], Markov logic networks (MLNs) [37], extensions of first order logic such as Nilsson's probabilistic logic [33], or even ad-hoc representations of function Pr from Definition 1. The following is an example of a BN over the running example. Example 2 Consider the set PEM from Figure 1. The Bayesian network depicted in Figure 2 describes the probability distribution Pr over all possible worlds WEM shown in Figure 3. So, for instance, the probability that false negatives do not arise in any of the two tests, and that the patient is at risk for both anxiety- and depressionrelated disorders (world 4 ) is 0.29808.

2.2 Analytical Model The DeLP3E formalism adopts a structured argumentation framework [36] for the AM. While the EM contains probabilistic information about the state of the world, the AM must allow for a different kind of information; in particular, it must be able to represent contradictory knowledge. This approach allows

Quantitative Belief Revision in Structured Probabilistic Argumentation
anx_risk AR
Pr(AR = T) = 0.4 Pr(AR = F) = 0.6

7

FN_tox_screen FNTS
Pr(FNTS = T) = 0.1 Pr(FNTS = F) = 0.9

dep_risk DR
Pr(DR = T | AR = T) = 0.9 Pr(DR = F | AR = T) = 0.1 Pr(DR = T | AR = F) = 0.2 Pr(DR = F | AR = F) = 0.8

FN_anx_test FNAT
Pr(FNAT = T | AR = T) = 0.08 Pr(FNAT = F | AR = T) = 0.92 Pr(FNAT = T | AR = F) = 0.01 Pr(FNAT = F | AR = F) = 0.99

Fig. 2 Bayesian network used in the EM of the running example. The names of the random variables are simply the abbreviations of their corresponding atoms: AR  anx risk, DR  dep risk, FNAT  FN anx test, and FNTS  FN tox screen.

World 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

anx risk T T T T T T T T F F F F F F F F

dep risk T T T T F F F F T T T T F F F F

FN anx test T T F F T T F F T T F F T T F F

FN tox screen T F T F T F T F T F T F T F T F

Probability 0.00288 0.02592 0.03312 0.29808 0.00032 0.00288 0.00368 0.03312 0.00048 0.00432 0.04752 0.42768 0.00012 0.00108 0.01188 0.10692

Fig. 3 Probability distribution for the worlds in the running example.

for the creation of arguments that may compete with each other in order to reach a conclusion regarding a given query. This is known as a dialectical process, where arguments defeat each other based on a comparison criterion. Resulting from this process, certain arguments are warranted, while others are defeated. Argumentation-based reasoning has been proposed and studied in depth as a natural way to manage a set of inconsistent information ­ its strength lies in the fact that it closely resembles the way humans settle disputes (consider, for instance, how convictions are decided in trials). Another highly desirable characteristic of structured argumentation frameworks is that, once a conclusion is reached, the process also yields an explanation of how we arrived at it, as well as information about why a given argument is warranted. In the following, we first recall the basics of the underlying argumentation framework used, and then go on to introduce the analytical model (AM).

8

G.I. Simari, P. Shakarian, and M.A. Falappa

Defeasible Logic Programming with Presumptions (PreDeLP) PreDeLP, first introduced in [32], is a formalism combining logic programming with defeasible argumentation. We will briefly recall the basics of PreDeLP, and refer the reader to [17, 32] for the complete presentation. The formalism contains several different constructs: facts, presumptions, strict rules, and defeasible rules. Facts are statements that always hold (such as a patient's symptom in our example), while presumptions are statements that may or may not be true (such as a medical diagnosis). Strict rules establish logical consequences (similar to material implication in first order logic, though the semantics is not exactly the same since the contrapositive does not follow from a strict rule). While strict rules, like facts, always hold, defeasible rules specify logical consequences that may be assumed to be true when no contradicting information is available. These components are used in the construction of arguments, and together comprise PreDeLP programs. Formally, we use the notation AM = (, , , ) to denote a PreDeLP program, where: ­  is the set of strict rules of the form L0  L1 , . . . , Ln , where L0 is a ground literal and {Li }i>0 is a set of ground literals; ­  is the set of facts, written simply as atoms; ­  is the set of defeasible rules of the form L0 ­ L1 , . . . , Ln , where L0 is a ground literal and {Li }i>0 is a set of ground literals, and ­  is the set of presumptions, which are written as defeasible without a body. For simplicity, we will sometimes refer to AM as a set corresponding to the union of its components. Recall that all atoms in the AM must be formed with a predicate from the set PAM , and note that in both strict and defeasible rules, strong negation (i.e., classical negation as in first-order logic) is allowed in the head, and thus may be used to represent contradictory knowledge. The following is an example of a PreDeLP program over the running example. Example 3 Consider again the medical diagnosis scenario from our running example; the DeLP3E program in Figure 4 encodes some basic knowledge that the attending physician might use to diagnose their patient. For instance, strict rule 1 states that based on a negative result when administering a test for toxins in the blood we can conclude that the patient is not misusing sleeping aids. On the other hand, defeasible rule 1 states that memory loss and depression can lead to such a misuse. In this example, the set of presumptions is empty. Arguments. Given a query in the form of a ground atom, the goal is to derive arguments for and against its validity ­ derivation follows the same mechanism of logic programming [30], and we denote such derivation with the symbol " ". In the following, we say that such a derivation is "strict" if it only uses facts

Quantitative Belief Revision in Structured Probabilistic Argumentation : 1 = 2 = 3 = 1 2 3 4  1 2 3 4 = = = = sleep aid misuse ­ mem loss, depression anxiety ­ short breath depression ­ mem loss anxiety ­ fainting = = = = mem loss short breath fainting ¬sleep aid misuse  neg tox screen ¬anxiety  neg anx test anxiety  depression depression  pos dep test

9

:

: :

Fig. 4 A ground argumentation framework.

and strict rules; otherwise, we say that the derivation is "defeasible". Likewise, we say that a literal is strictly (defeasible) derived if the derivation is strict (defeasible). Finally, we say that an argument is "factual" if no presumptions are used in it. Since rule heads can contain strong negation, it is possible to defeasibly derive contradictory literals from a program. For the treatment of contradictory knowledge, PreDeLP incorporates a defeasible argumentation formalism that allows the identification of the pieces of knowledge that are in conflict and, through the dialectical process discussed above, decides which information prevails as warranted. This dialectical process involves the construction and evaluation of arguments, formally defined next. Definition 2 (Argument) An argument A, L for a literal L is a pair of the literal and a (possibly empty) set of the AM (A  AM ) that provides a minimal proof for L meeting the following requirements: (i) L is defeasibly derived from A; (ii)     A is not inconsistent; and (iii) A is a minimal subset of    satisfying (i) and (ii), denoted A, L . Literal L is called the conclusion supported by the argument, and A is the support of the argument. An argument B , L is a subargument of A, L if B  A. An argument A, L is presumptive if A   is not empty. We will also use  (A) = A   , (A) = A  , (A) = A  , and (A) = A  . Our definition differs slightly from that of [42], where DeLP is introduced, as we include strict rules and facts as part of arguments. We make this change because in our framework the strict rules and facts used to construct a given argument may only be true in certain worlds in the EM. Hence, the entire set of facts and strict rules need not be consistent in our framework. We shall discuss how portions of the AM are assigned EM worlds in the next section. Example 4 Figure 5 shows example arguments based on the PreDeLP program from Figure 4. Argument A3 uses an additional component not present in the

10 A1 , anxiety A2 , anxiety A3 , ¬sleep aid misuse

G.I. Simari, P. Shakarian, and M.A. Falappa A1 = {1 , 3 , 3 } A2 = {3 , 4 } A3 = {1 }  {neg tox screen ­ }

Fig. 5 Example arguments based on the running example scenario.

original program, and states that if we can assume a negative result for a tox screen, we can conclude that the patient is not misusing sleeping aids. Given an argument A1 , L1 , counter-arguments are arguments that contradict it. Argument A2 , L2 is said to counterargue or attack A1 , L1 at a literal L iff there exists a subargument A, L of A1 , L1 such that the set  (A1 )   (A2 )  (A1 )  (A2 )  {L2 , L } is inconsistent. A proper defeater of an argument A, L is a counter-argument that ­ by some criterion ­ is considered to be better than A, L ; if the two are incomparable according to this criterion, the counterargument is said to be a blocking defeater. An important characteristic of PreDeLP is that the argument comparison criterion is modular, and thus the most appropriate criterion for the domain that is being represented can be selected; the default criterion used in classical defeasible logic programming (from which PreDeLP is derived) is generalized specificity [44], though an extension of this criterion is required for arguments using presumptions [32]. We briefly recall this criterion next ­ the first definition is for generalized specificity, which is subsequently used in the definition of presumption-enabled specificity. Definition 3 (DeLP Argument Preference) Let AM = (, , , ) be a PreDeLP program and let F be the set of all literals that have a defeasible derivation from AM . An argument A1 , L1 is preferred to A2 , L2 , denoted with A1 P S A2 if: (1) For all H  F ,  (A1 )   (A2 )  H is consistent: if there is a derivation for L1 from  (A2 )   (A1 )  (A1 )  H , and there is no derivation for L1 from  (A1 )   (A2 )  H , then there is a derivation for L2 from  (A1 )   (A2 )  (A2 )  H ; and (2) there is at least one set H  F ,  (A1 )   (A2 )  H is consistent, such that there is a derivation for L2 from  (A1 )   (A2 )  H  (A2 ), there is no derivation for L2 from  (A1 )   (A2 )  H , and there is no derivation for L1 from  (A1 )   (A2 )  H  (A1 ). Intuitively, the principle of specificity says that, in the presence of two conflicting lines of argument about a proposition, the one that uses more of the available information is more convincing. The following extension for presumptive arguments was first introduced in [32]. Definition 4 (Presumptive Argument Preference) Given PreDeLP program AM = (, , , ), an argument A1 , L1 is preferred to A2 , L2 , denoted with A1 A2 if any of the following conditions hold: (1) A1 , L1 and A2 , L2 are both factual and A1 , L1 P S A2 , L2 .

Quantitative Belief Revision in Structured Probabilistic Argumentation

11

(2) (3)

A1 , L1 is a factual argument and A2 , L2 is a presumptive argument. A1 , L1 and A2 , L2 are presumptive arguments, and (A2 ) or,
PS

(a) (A1 )

(b) (A1 ) = (A2 ) and A1 , L1

A2 , L2 .

Generally, if A, B are arguments with rules X and Y , resp., and X  Y , then A is stronger than B . This also holds when A and B use presumptions P1 and P2 , resp., and P1  P2 . Note: The specificity criterions used here are not transitive [48], and therefore should not be assumed to define an ordering over arguments. This, however, does not pose a problem for our framework, as the criterion is only ever used to compare pairs of arguments to see which one "wins out". We remind the reader that the comparison criterion in our framework is modular; if transitivity is required, the one proposed in [48] is an option. A sequence of arguments called an argumentation line thus arises from this attack relation, where each argument defeats its predecessor. To avoid undesirable sequences, which may represent circular argumentation lines, in DeLP an argumentation line is acceptable if it satisfies certain constraints (see [17]). A literal L is warranted if there exists a non-defeated argument A supporting L. Clearly, there can be more than one defeater for a particular argument A, L . Therefore, many acceptable argumentation lines could arise from A, L , leading to a tree structure. The tree is built from the set of all argumentation lines rooted in the initial argument. In a dialectical tree, every node (except the root) represents a defeater of its parent, and leaves correspond to undefeated arguments. Each path from the root to a leaf corresponds to a different acceptable argumentation line. A dialectical tree provides a structure for considering all the possible (maximal) acceptable argumentation lines that can be generated for deciding whether an argument is defeated. We call this tree dialectical because it represents an exhaustive dialectical (in the sense of providing reasons for and against a position) analysis for the argument in its root. For a given argument A, L , we denote the corresponding dialectical tree as T ( A, L ). Given a literal L and an argument A, L , in order to decide whether or not a literal L is warranted, every node in the dialectical tree T ( A, L ) is recursively marked as "D" (defeated ) or "U" (undefeated ), obtaining a marked dialectical tree T  ( A, L ) as follows: 1. All leaves in T  ( A, L ) are marked as "U"s, and 2. Let B , Lq be an inner node of T  ( A, L ). Then B , Lq will be marked as "U" iff every child of B , Lq is marked as "D". The node B , Lq will be marked as "D" iff it has at least a child marked as "U". Given an argument A, L obtained from AM , if the root of T  ( A, L ) is marked as "U", then we will say that T  ( A, L ) warrants L and that L is warranted from AM . (Warranted arguments correspond to those in the

12

G.I. Simari, P. Shakarian, and M.A. Falappa

grounded extension of a Dung argumentation system [11].) There is a further requirement when the arguments in the dialectical tree contains presumptions ­ the conjunction of all presumptions used in even (respectively, odd) levels of the tree must be consistent. This can give rise to multiple trees for a given literal, as there can potentially be different arguments that make contradictory assumptions. We can then extend the idea of a dialectical tree to a dialectical forest. For a given literal L, a dialectical forest F (L) consists of the set of dialectical trees for all arguments for L. We shall denote a marked dialectical forest, the set of all marked dialectical trees for arguments for L, as F  (L). Hence, for a literal L, we say it is warranted if there is at least one argument for that literal in the dialectical forest F  (L) that is labeled as "U", not warranted if there is at least one argument for the literal ¬L in the dialectical forest F  (¬L) that is labeled as "U", and undecided otherwise. We shall refer to whether literal L is warranted, not warranted, or undecided as L's "warrant status", and sometimes refer to the "warranted" status as "Yes" and the "not warranted" status as "No". 2.3 The DeLP3E Framework Our framework, originally proposed in [39], is the result of combining the environmental and analytical models (which we denote with EM and AM , respectively). Intuitively, given AM , every element of        only hold in certain worlds in the set WEM ­ i.e., these elements are subject to probabilistic events. Each element of        is thus associated with a formula over GEM (using conjunction, disjunction, and negation, as usual) ­ we use formEM to denote the set of all such formulas. The notion of an annotation function associates elements of        with elements in formEM . Definition 5 (Annotation Function [39]) An annotation function is any function of the form af :         formEM . We use [af ] to denote the set of all annotation functions. We will sometimes denote annotation functions as sets of pairs (f, af(f )) in order to simplify the presentation. Figure 6 shows an example of an annotation function for our running example; for instance, the annotation for rule 2 means that this rule only holds whenever the probabilistic event anx risk is true. If annotations are "True", this means that they hold in all possible worlds. Definition 6 (DeLP3E Program) Given environmental model EM , analytical model AM , and annotation function af , a DeLP3E program is of the form I = (EM , AM , af ). We use notation [I ] to denote the set of all possible programs. In the following, given DeLP3E program I = (EM , AM , af ) and   WEM , we use notation AM () = {f  AM s.t.  |= af (f )}. This gives rise to a decomposed view of DeLP3E programs, as illustrated next.

Quantitative Belief Revision in Structured Probabilistic Argumentation af (1 ) = af (2 ) = af (3 ) = af (1 ) = af (2 ) = af (3 ) = af (4 ) = af (1 ) = af (2 ) = af (3 ) = af (4 ) = True True True True True True True True anx risk dep risk anx risk

13

Fig. 6 An example of an annotation function over the running example. 1 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 ,  1 , 2 , 3 ,  4 } 5 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 ,  1 , 2 , 4 } 9 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 ,  1 , 3 } 13 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 , 1 } 2 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 ,  2 ,  3 ,  4 } 6 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 ,  2 ,  4 } 10 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 ,  3 } 14 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 } 3 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 , 1 ,  2 ,  3 ,  4 } 7 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 , 1 ,  2 ,  4 } 11 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 , 1 ,  3 } 15 : {1 , 2 , 3 , 1 ,  2 ,  3 ,  4 , 1 } 4 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 ,  2 ,  3 ,  4 } 8 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 ,  2 ,  4 } 12 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 ,  3 } 16 : {1 , 2 , 3 , 1 , 2 , 3 , 4 , 1 }

Fig. 7 A depiction of how the DeLP3E program in the running example can be decomposed into one classical PreDeLP program for each possible EM world (cf. Figure 3 for the definition of worlds 1 ­16 in terms of the random variables in the EM).

Example 5 Consider the different examples presented so far: the EM from Example 2 (with the worlds from Figure 3), AM from Figure 4, the arguments in Figure 5, and the annotation function from Figure 6 ­ these components give rise to a DeLP3E program I = (EM , AM , af ) Figure 7 shows how I can be decomposed into one classical PreDeLP program AM () for each world   WEM . For instance, AM (7 ) contains 1 , 2 , 3 , 1 , 2 , 3 , 4 , and 1 because the annotation function associates condition True to all of these components; it contains 2 and 4 because condition anx risk is true in 7 , and it does not contain 3 because condition dep risk is false in 7 . The most direct way of considering consequences of DeLP3E programs is thus to consider what happens in each world in WEM ; that is, the defeat relationship among arguments depends on the current state of the (EM) world. Definition 7 (Existence of an Argument in a World) Given DeLP3E program I = (EM , AM , af ), argument A, L is said to exist in world   WEM if c  A,  |= af(c).

14

G.I. Simari, P. Shakarian, and M.A. Falappa

The notion of existence is extended to argumentation lines, dialectical trees, and dialectical forests in the expected way (for instance, an argumentation line exists in  iff all arguments that comprise that line exist in ). Example 6 Consider the different examples presented so far: the worlds in Figure 3, AM from Figure 4, the arguments in Figure 5, and the annotation function from Figure 6. Since argument A1 uses defeasible rule 3 , and af (3 ) = dep risk (while the other two components have annotation "True"), we can conclude that this argument exists in worlds in which dep risk is true, i.e.,, 1 ­4 and 9 ­12 . The idea of a dialectical tree is also extended w.r.t. worlds; so, for a given world   WEM , the dialectical (resp., marked dialectical) tree induced by   is denoted with T A, L (resp., T A, L ). We require that all arguments and defeaters in these trees exist in . Likewise, we extend the notion of dialectical  forests in the same manner (denoted with F (L) and F (L), respectively). Based on these concepts, we introduce the notion of warranting scenario. Definition 8 (Warranting Scenario) Given DeLP3E program I = (EM , AM , af ) and literal L formed with a ground atom from GAM , a world   WEM is said to be a warranting scenario for L (denoted  war L) if there is a   (L) in which L is warranted and F (L) exists in . dialectical forest F The idea of a warranting scenario is used to formally define DeLP3E entailment. The set of worlds in the EM where a literal L in the AM must be true is exactly the set of warranting scenarios -- these are the "necessary" worlds: nec(L) = {  WEM | ( war L)}. Now, the set of worlds in the EM where AM literal L can be true is the following -- these are the "possible" worlds: poss(L) = {  WEM |  war ¬L}. In the following we use notation for() = a a  a /  ¬a, which denotes the formula that has  as its only model. We now extend this notation to sets of worlds: for(W ) = W for(). Entailment can then be defined as follows: Definition 9 (DeLP3E Entailment) Given DeLP3E program I = (EM , AM , af ), AM literal L and probability interval p  [ , u], we say that I entails L with probability p  [ , u] if the probability distribution Pr yielded by EM is such that  nec(L) Pr () and poss(L) Pr ()  u. 2.4 Consistency of DeLP3E Programs Finally, one of the central topics of this paper is that of inconsistencies, which can arise in our framework in more than one way [39]. In this paper, we assume that the probabilistic model is consistent, and focus on inconsistencies that arise in the AM. Towards this end, we use the following notion of (classical) consistency: PreDeLP program  is said to be consistent if there does not exist a ground literal a s.t.  a and  ¬a. For DeLP3E programs, the interaction between the annotation function and facts and strict rules may cause conflicts, as defined next.

Quantitative Belief Revision in Structured Probabilistic Argumentation

15

Definition 10 (Consistency of DeLP3E Programs) A DeLP3E program I = (EM , AM , af ), with AM = , , ,  , is consistent if: given probability distribution Pr for EM , if there exists a world   WEM such that x  | |=af(x) {x} is inconsistent, then we have Pr () = 0. The intuition behind this definition is that subsets of facts and strict rules hold under the circumstances specified by the annotation function ­ such circumstances can be expressed as sets of EM worlds. Now, if there exists a world where (at least) two contradictory strict statements are true, then the EM must assign probability zero to this world. Example 7 Let us return to the running example; consider AM from Figure 4, EM from Figure 2, and the annotation function from Figure 6, with the addition of fact 4 = pos dep test with af (4 ) = True and fact 5 = neg anx test with af (5 ) = ¬FN-anx test. It is now clear that the program is inconsistent, since there exists world 3 (among several others) such that x  | 3 |=af(x) {x} warrants both anxiety (via argument with 4 and 3 ) and ¬anxiety (via argument with 5 and 2 ).

3 Revision of DeLP3E Programs based on Quantitative Aspects We have finally arrived at the main problem we address in this paper ­ revising knowledge bases. This problem can be generically stated as: given DeLP3E program I = (EM , AM , af ), with AM =        and a pair (f, af ) where f is either an atom or a rule and af is equivalent to af , except for its expansion to include f 3 , obtain a new program I called the revised knowledge base that addresses the incorporation of the epistemic input (f, af ) into the original program; we denote this operation with the symbol "·" ­ i.e.,, I = I · (f, af ). Now, the problem statement as presented above is quite vague, since we did not give any details as to how the operator "addresses the incorporation" of the epistemic input. There are many approaches in the literature (cf. Section 6) that address this problem quite differently; one of the main properties that characterize revision operators is whether or not they satisfy the Success property, which states that the epistemic input must be a consequence of the revised knowledge base. Here, we will adopt a cautious stance and assume that this property does not hold in general; therefore, we focus on so-called non-prioritized revision operators. The basic issue that revision operators must deal with is inconsistency (we will discuss this in more depth shortly); as we saw in Section 2.4, inconsistency in DeLP3E programs involves worlds that have non-zero probability and an associated PreDeLP program that is inconsistent. In our previous work [39, 41] we identified three basic approaches that can be taken towards solving this problem:
3

That is, af (x) = af (x) for all x  dom(af ), and dom(af ) = dom(af )  {f }.

16

G.I. Simari, P. Shakarian, and M.A. Falappa

­ Modifying the EM: Perhaps the simplest approach is to fix the problem of inconsistency by forcing the probabilistic model to assign probability zero to all worlds that cause inconsistencies to arise. ­ Modifying the AM: Alternatively, for each world in which the corresponding PreDeLP program is inconsistent, we can modify this program to remove the problem. ­ Modifying the annotation function. Finally, as a finer-grained approach compared to the previous one, we can alter the annotation function. In the following, we will assume that epistemic inputs involve only strict components (facts or rules), since defeasible components can always be added without inconsistencies arising. Regarding these three possible approaches, in this paper we will focus on the third one since it is a generalization of the second ­ if we only allow removing elements from the AM, such an operation will have the same effect as not removing the element but modifying the annotation function so that it associates the formula "" to it. The generalization of annotation function-based revision lies in that there is not always the need for such a drastic measure; see [41] for further discussions on this, including examples. Furthermore, operations of the first kind alone do not suffice to perform revisions, as can be seen in the following simple example. Example 8 Consider the following DeLP3E program, where the EM consists of two worlds {a} and {¬a}, each with probability 0.5: 1 : 1 : 2 : 2 : pq ¬p ¬p  q p af (1 ) = a af (1 ) = a af (2 ) = ¬a af (2 ) = ¬a

Now, suppose we wish to revise by formula 3 : q with af (3 ) = True. Since both EM worlds are inconsistent with the formula, it is impossible to change the allocation of the probability mass in order to avoid inconsistencies; therefore, the only option is to reject the input.

3.1 A Recap of Annotation Function-based Belief Revision Since the quantitative approach that we propose in this paper is related to the AFO class of revision operators introduced in [39], in this section we provide a brief summary of the relevant material. After recalling the relevant postulates, we present the construction of AFO operators. 3.1.1 Rationality Postulates The following properties characterize the behavior of operators for revising annotation functions; we briefly discuss them here in an informal manner, and refer the reader to [41] for their formal presentation.

Quantitative Belief Revision in Structured Probabilistic Argumentation

17

­ Inclusion: For any given element g in the AM, the worlds that satisfy g 's annotation after the revision are a subset of the set of worlds satisfying g 's annotation before the revision. That is, the constraints in the revised annotations can only become more restrictive. ­ Vacuity: If simply adding the input to the program does not lead to inconsistencies, then the operator does precisely that. ­ Consistency Preservation: If the original program is consistent, then so is the revised program. ­ Weak Success: As in Vacuity, if adding the input to the program does not cause inconsistencies, then the input must be present "as is" in the revised program. ­ Relevance: Given a specific EM world, if a part of its associated AM knowledge base is removed by the operator, then there exists a superset of the remaining knowledge base that is not consistent with the removed element and the input. That is, removed elements are always "relevant" to an inconsistency. ­ AF Uniformity: If two different inputs are such that the same set of EM worlds lead to inconsistencies in a given AM knowledge base, and it is the case that analogous subsets taken in conjunction with their respective input lead to equivalent consistency/inconsistency, then the models removed from the annotations elements in the AM knowledge base are the same for both inputs. In other words, the operator must behave in the same way when presented with inputs that have an equivalent effect on the knowledge base. We now continue briefly recalling the presentation of the annotation functionbased operator of [39] by discussing its construction. In order to do so, we adopt the following notation: the program to revise is denoted with I = (EM , AM , af ), with AM =       , and the epistemic input is denoted with (f, af ), where f is either an atom or a rule and af is equivalent to af , except for its expansion to include f . We denote the revision as follows: I · (f, af ) = (EM , AM , af ) where af is the revised annotation function. We will slightly abuse notation in order to make the presentation clearer, and use notation to convert sets of worlds to and from formulas. Moreover, we often refer to "removing elements of AM " to refer to changes to the annotation function that cause certain elements of the AM to not have their annotations satisfied in certain EM worlds. As we are looking to change the annotation function for a specific subset of facts and strict rules, we specify these subsets with the following notation: ­ I  (f, af ) to denote I = (EM , AM  {f }, af ). ­ (f, af )  I = (AM , EM , af ) to denote f  AM and af = af .
0 I ­ WEM (I ) = {  WEM | AM () is inconsistent} ­ this set contains all the EM worlds for a given program where the corresponding knowledge base in the AM is classically inconsistent.

18

G.I. Simari, P. Shakarian, and M.A. Falappa

I 0 0 ­ WEM (I ) = {  WEM |Pr () > 0} ­ this is a subset of WEM (I ) containing worlds that are assigned a non-zero probability; i.e., the worlds where inconsistency in the AM arises.

­ wld(f ) = { |  |= f } ­ the set of worlds that satisfy formula f ; and ­ for() = ­
I () AM a

a

a /

¬a ­ the formula that has  as its only model.

= {f     |  |= af(f )} ­ the subset of facts and strict rules in AM whose annotations are true in EM world .

We will make use of this notation in the next section. 3.1.2 Operator Construction The basis of the construction of the class of so-called annotation functionbased operators is that any subset of AM that is associated with a world I in WEM (I  (f, af )) must be modified so that consistency is ensured. For each such world , we make use of the following set of "candidate replacement programs" for AM (): CandP gmaf (, I ) = {AM | AM  AM () s.t. AM is consistent and AM  AM () s.t. AM  AM s.t. AM is consistent}
I The intuition is that each maximal consistent subset of AM () is a candidate for replacing the inconsistent program for that world. To specify the construction, we need to introduce some more notation. Let  : WEM  2[][ ] ; i.e., a function from EM worlds to subsets of the strict components of the AM ­ this function will be used to choose the revised AM for each world. For each component h in the AM there is a formula in AM {f }, where f is part of the epistemic input (i.e., what the operator is revising by). Given these elements, we define:

newFor(h, , I , (f, af )) = af (h) 
I (I(f,af )) | h WEM /  ()

¬f or(i )

Intuitively, newFor provides a new annotation for each component h  AM ; such formula can be seen as the result of selecting a maximally consistent subset of AM () for each EM world . We are finally able to introduce the AFO class of operators: Definition 11 (AF-based Operators [39, 41]) A belief revision operator · is an "annotation function-based" (or af-based) operator (·  AFO) if given program I = (EM , AM , af ) and input (f, af ), the revision is defined as I · (f, af ) = (EM , AM  {f }, af ), where: h, af (h) = newFor(h, , I , (f, af )) where   WEM ,  ()  CandP gmaf (, I  (f, af )).

Quantitative Belief Revision in Structured Probabilistic Argumentation Analytical Model : Annotation Function

19

1 = mem loss True True 2 = short breath 3 = fainting True -----------------------------------------------{¬FN-anx test} 4 = neg anx test 5 = neg tox screen {¬FN-tox screen} 1 2 3 4  1 2 3 4 = = = = sleep aid misuse ­ mem loss, depression anxiety ­ short breath depression ­ mem loss anxiety ­ fainting True {anx risk} {dep risk} {anx risk} = = = = ¬sleep aid misuse  neg tox screen ¬anxiety  neg anx test anxiety  depression depression  pos dep test True True True True

:

: :

Fig. 8 The DeLP3E program from the running example, after adding facts 4 and 5 . The annotation function is provided in a separate column for convenience.

In [39, 41], we provide a representation theorem that states the equivalence between this construction and the operators discussed above. We refer the reader to these articles for a more complete and detailed presentation.

3.2 Towards a Quantitative Approach Traditionally, belief revision has been addressed from a qualitative point of view rather than a quantitative one (cf. Section 6 for a discussion on related work). A simple example of this is the fact that, faced with the option of removing either both atoms a and b or only atom c, classical revision operators typically declare both options to be just as good, since neither is a subset of the other; it could be argued, then, that taking quantitative aspects into account (such as the number of elements removed) may lead to a better solution ­ of course, this may depend on other factors. As we will see, there are different ways in which such quantitative aspects can be incorporated into revision operations. For instance, in our setting, DeLP3E programs can be regarded in a world-by-world manner, and changes made in one world can be compared to those made in another. The AFO operators described in Section 3.1 make decisions for each world independently; we now wish to address the issue of taking into account different kinds of quantitative aspects when revising DeLP3E programs. The following example motivates our approach in our medical diagnosis scenario. Example 9 Consider again the running example, and suppose the physician has decided to carry out as a first step two tests, a toxin screen and an anxiety test, and that both tests yielded negative results. Note that the validity of

20

G.I. Simari, P. Shakarian, and M.A. Falappa

these tests is subject to probabilistic events (in this case, false negatives). The new program is reproduced in Figure 8. Figure 9 shows the world-by-world decomposition of the new program, and the atoms that are warranted in each case. From the information in this figure, we can compute the following probabilities for the hypotheses that the physician is contemplating (depression, anxiety, and misuse of sleeping aids): Literal depression sleep aid misuse ¬sleep aid misuse anxiety ¬anxiety Probability : 0.06672 : 0.05088 : 0.93324 : 0.06992 : 0.92888

Since they all have low probabilities after performing the tests, the doctor decides to test for depression and in this case receives a positive result (atom pos dep test). For the sake of this example, we will assume that the validity of the outcome of this test (unlike the other two) is not subject to probabilistic events ­ thus, we have af (pos dep test) = True. Now, while for the first two tests we were able to simply add the corresponding atoms and extend the annotation function accordingly, simply adding 6 = pos dep test with af (6 ) = True causes inconsistencies to arise in eight of the possible worlds (3 , 4 , 7 , 8 , 11 , 12 , 15 , and 16 ). Essentially, the problems arise because the negative anxiety test allows us to conclude that there is no anxiety, while the positive depression test would allow us to conclude that indeed there is anxiety. Since both derivations only involve strict components, this leads to an inconsistent AM. Example 9 shows an interesting case of belief revision in DeLP3E programs; when presented with new information that is in conflict with existing one, we must find a way to address its incorporation into the existing knowledge ­ non-prioritized operators are very flexible, since they always have the option of ignoring the new information. However, this flexibility also means that ­ in the case of DeLP3E programs ­ there is no guidance with respect to how revisions should be carried out globally, since each world is treated as a separate revision problem. Next, we discuss two kinds of functions that will prove to be useful in addressing this situation. 3.2.1 Two Building Blocks We now introduce warrant probability functions and revision objective functions, which are later used in the definition of our new class of non-prioritized belief revision operators. Warrant Probability Functions. As one of the building blocks to our quantitative approach, given a DeLP3E program I we define warrant probability functions (WPFs, for short).

Quantitative Belief Revision in Structured Probabilistic Argumentation World 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Probability 0.00288 0.02592 0.03312 0.29808 0.00032 0.00288 0.00368 0.03312 0.00048 0.00432 0.04752 0.42768 0.00012 0.00108 0.01188 0.10692 Warranted Literals {1 , 2 , 3 }  {depression, sleep aid misuse, anxiety} {1 , 2 , 3 , 5 }  {depression, ¬sleep aid misuse, anxiety} {1 , 2 , 3 , 4 }  {depression, ¬sleep aid misuse, anxiety} {1 , 2 , 3 , 4 , 5 }  {¬sleep aid misuse, ¬anxiety} {1 , 2 , 3 }  {anxiety} {1 , 2 , 3 , 5 }  {¬sleep aid misuse, anxiety} {1 , 2 , 3 , 4 }  {¬anxiety} {1 , 2 , 3 , 4 , 5 }  {¬sleep aid misuse, ¬anxiety} {1 , 2 , 3 }  {depression, sleep aid misuse, anxiety} {1 , 2 , 3 , 5 }  {depression, ¬sleep aid misuse, anxiety} {1 , 2 , 3 , 4 }  {sleep aid misuse, ¬anxiety} {1 , 2 , 3 , 4 , 5 }  {¬sleep aid misuse, ¬anxiety} {1 , 2 , 3 } {1 , 2 , 3 , 5 }  {¬sleep aid misuse} {1 , 2 , 3 , 4 }  {¬anxiety} {1 , 2 , 3 , 4 , 5 }  {¬anxiety, ¬sleep aid misuse}

21

Fig. 9 Atoms that are warranted in each possible EM world, given the AM and annotation function in Figure 8.

Before introducing these formulas, we need to present a simple extension to the concept of "warrant status", which is up to now defined for literals. The following definition is a simple extension to conjunctions or disjunctions of literals. Definition 12 (Warranting a Conjunction/Disjunction of Literals) Let AM be a ground PreDeLP program and Q be either a conjunction or disjunction of ground literals L1 , . . . , Ln . The warrant status of Q with respect to AM is defined as follows: 1. If Q is a single literal L, then the warrant status of Q is the warrant status of L in AM . 2. If Q = Q1  Q2 then the warrant status of Q is: ­ Yes iff the warrant status of both Q1 and Q2 is Yes; ­ No if the warrant status of either Q1 or Q2 is No; and ­ Undecided whenever neither of the above cases hold. 3. If Q = Q1  Q2 then the warrant status of Q is: ­ Yes iff the warrant status of either Q1 or Q2 is Yes; ­ No if the warrant status of both Q1 and Q2 is No; and ­ Undecided whenever neither of the above cases hold. Using Definition 12, we can easily extend the nec and poss notations (cf. Page 14) to conjunctions and disjunctions of literals. The following result is a consequence of the fact that conflicting literals cannot be warranted in (Pre)DeLP [17]. Proposition 1 Let AM be a ground PreDeLP program and Q = L1  . . .  Ln be a conjunction of ground literals. Then, only one of the following cases holds: (i) P war Q, (ii) P war ¬Q, or (iii) the warrant status of Q is undecided.

22

G.I. Simari, P. Shakarian, and M.A. Falappa

Warrant Probability Functions
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
Original Program Prioritizing depression test Prioritzing anxiety test

Warrant Probability

Potentially Warranted Atoms

Fig. 10 Histogram depiction of the entailment probability functions for the programs of Example 9.

Proof In [17], a corresponding trichotomy result was shown for literals, i.e., the warrant status for any literal is one and only one of Yes, No, and Undecided. Our result is a direct consequence of this and Definition 12. Warrant Probability Functions are then simply defined as partial mappings with signature: I : basicAM  [0, 1] such that for f  basicAM , I (f ) = p if and only if nec(f ) Pr () = p. 4 When the program is clear from context, we drop the subscript and write simply  . In the following, we use notation dom( ) to denote the set of formulas for which  is defined. The table shown in Example 9 is a simple example of a WPF, whose domain is a handful of literals. The following is another example along the same vein. Example 10 Figure 10 shows three examples of WPFs in which the domains are fixed to the set of literals that can be warranted in the input program. These functions are related to the revision described in Example 9: the black bars show the original probabilities, the striped bars give the probabilities yielded by the program obtained by favoring the inclusion of the positive depression test, while the light gray bars depict the probabilities obtained by favoring the negative anxiety test. Figure 11 shows the three revised programs.

Revision Objective Functions. The other building block allows us to effectively quantify how good a revision is considered to be. Towards this end,
4 Note that this definition can easily be extended to deal with probability intervals as well (i.e., using both nec and poss ); here, for simplicity of presentation, we adopt this definition in order to work with point probabilities.

Quantitative Belief Revision in Structured Probabilistic Argumentation

23

we define revision objective functions (ROFs, for short) as functions that take two DeLP3E programs I1 and I2 , along with an epistemic input (f, af ), and returns a positive real number. We keep the definition of ROFs very general in order to allow different kinds of objectives to be specified. The following is a simple example of a ROF over our running example, which makes use of warranting probability functions. Example 11 Let us return once again to the medical diagnosis example. Suppose that we take the three revised programs we presented (Figure 11) ­ call them I1 , I2 , and I3 ­ and that we wish to compare them with respect to the effect of the last revision over the warranted atoms, taking the probabilities yielded by I1 as the baseline. So, we define the following revision objective function:  (I , I , (f, af )) = e- LLAM ,L=f |I (L)-I (L)| where I is the WPF for program I . Intuitively, this function sums up all the differences between the probabilities of literals entailed by the programs, but ignores the input (if it is a literal). In this way, a distance between the original program and the two possible revisions is obtained based on the effects that each revision had on the probabilities with which literals are derived. So, for our revisions, we get:  (I1 , I2 , (pos dep test, af 2 ))  0.0547  (I1 , I3 , (pos dep test, af 3 ))  0.8611 Therefore, we can conclude that the revision yielding I3 is preferred over the one yielding I2 when this ROF is adopted. Note that the function presented in Example 11 is just one possibility; the framework is very flexible and allows the user to express many different functions, depending on the specific way in which they wish to express distances between the original program and a given revised program. 3.2.2 The Class QAFO Given the basic constructs introduced above, we can now define the class of quantitative annotation function-based revision operators. Definition 13 (The Class QAFO) Let I = (EM , AM , af ), with AM =        be a DeLP3E program,  AFO be an annotation function-based belief revision operator, and  be a revision objective function. Operator is said to be a quantitative af-based operator (denoted  QAFO) if: Given an epistemic input (f, af ), we have that if I = I (f, af ) then there does not exist DeLP3E program I = I · (f, af ) such that  (I , I , (f, af )) >  (I , I , (f, af )), where ·  AFO is an arbitrary operator.

24 Analytical Model : 1 2 3 4 5 6 = = = = = = mem loss short breath fainting neg anx test neg tox screen pos dep test ¬sleep aid misuse  neg tox screen ¬anxiety  neg anx test anxiety  depression depression  pos dep test

G.I. Simari, P. Shakarian, and M.A. Falappa af 1 True True True ¬FN-anx test ¬FN-tox screen True af 2 True True True False af 1 (5 ) True af 3 True True True af 1 (4 ) af 1 (5 ) ¬af 1 (4 ) True True True True

:

1 = 2 = 3 = 4 =

True True True True

True True True True

: :

 1 = 2 = 3 = 4 = sleep aid misuse ­ mem loss, depression anxiety ­ short breath depression ­ mem loss anxiety ­ fainting

True {anx risk} {dep risk} {anx risk}

True af 1 (2 ) af 1 (3 ) af 1 (4 )

True af 1 (2 ) af 1 (3 ) af 1 (4 )

Fig. 11 The DeLP3E program from the running example, after performing three revisions: (i) The addition of the 4 and 5 , as discussed in Example 9; (ii) The revision by pos dep test by prioritizing this input; and (iii) The same revision but prioritizing neg anx test.

So, this subclass of AFO simply takes a revision objective function and uses it to obtain the best possible revised program. The following example, based on our previous work on applications of DeLP3E to problems in the cyber security domain [40], shows how QAFO operators can be applied to belief revision problems in real-world scenarios other than the running example. Example 12 Suppose we are modeling a cyber security scenario in which a computer worm has been deployed and has infected millions of computers worldwide ­ by the time the worm is discovered, it is very difficult to reason about the origin and even the intended target of the attack. In this kind of situations, analysts are trying to solve the so-called attribution problem: given a cyber operation of interest, determine the party that was ultimately responsible for carrying it out [38]. Towards this end, we can model all knowledge available by means of a DeLP3E program I = (EM , AM , af ), in which there is one distinguished predicate condOp (A, O) in the AM that is intuitively read as "actor A conducted operation O". Furthermore, if we assume that only one actor is ever responsible for an operation (an assumption that can easily be removed), we have an integrity constraint of the form oneOf(C ), where C is the set of all ground atoms built with the condOp predicate. Given this setup, we can define a WPF with a domain consisting of some formulas of interest that reflect conditions that the analysts would like to remain relatively unaffected when incorporating new information. For instance,

Quantitative Belief Revision in Structured Probabilistic Argumentation

25

suppose we define: dom( ) = ¬condOp (countryA, worm)  ¬condOp (countryB, worm) condOp (countryD, worm) , denoting the fact that neither country A nor country B are responsible for deploying the worm, and that country D is. If we pair this WPF with the ROF from Example 11, the corresponding QAFO operator will prefer revisions that do not affect the conclusions already reached regarding the probabilities assigned to these statements. In other words, this definition of dom( ), with the ROF in question, causes distances to be gauged relative to their effect on the probabilities assigned to the suspicions that (i) neither country A nor country B carried out the attack, and (ii) country D is behind the attack. Thus, such a setup causes the operator to prefer revisions that keep the probabilities assigned to such suspicions as close as possible to the ones yielded by the original program. In the next section, we study the computational complexity associated with this approach to belief revision in the DeLP3E setting. 4 Computational Complexity In this section, we will focus on some of the computational aspects of quantitative af-based belief revision operations. As a first observation, we have that the problem of deciding the warranting status in a (classical) PreDeLP program has not yet been pinpointed. In [4], the authors present a proof for the PSPACE-completeness of the problem of marking a given dialectical tree; PSPACE membership for deciding the warrant status is therefore a direct consequence of this result, since a dialectical tree can be built within this budget. As a step towards finding a lower bound for the complexity of the problem in general, we have the following. Proposition 2 Let AM be a ground PreDeLP program and L be a ground literal. Deciding AM war L is NP-hard. Proof By reduction from 3SAT-CNF; we therefore start with an input formula F with n variables X1 , . . . , Xn and m clauses C1 , . . . , Cm . We produce a PreDeLP program AM with the following predicates: f , x1 , . . . , xn , and c1 , . . . , cm . We then set:  = {f, ¬f, x1 , ¬x1 , . . . , xn , ¬xn }  = {cj ­ xi | Xi = T makes clause Cj true}  {cj ­ ¬xi | Xi = F makes clause Cj true}  {f ­ ci | for each clause Ci } ==

26

G.I. Simari, P. Shakarian, and M.A. Falappa

Next, we set the comparison criterion to specificity (the default in PreDeLP), except for the following exceptions: {xi , cj ­ xi }, cj is always preferred over {¬xi , cj ­ ¬xi }, cj {¬f }, ¬f is preferred over {f }, f Now, we must show that AM war f if and only if there exists a satisfying assumption for formula F . Suppose AM war f ; the only way this can happen is if argument {¬f }, ¬f (the only counterargument to {f }, ) is defeated, which happens if and only if all arguments that defeat this argument are in turn undefeated. Note that the only arguments capable of being in this situation are the ones using the rules with cj in the head. Therefore, if all such arguments are undefeated it must be the case that either xi or ¬xi can be chosen for each variable Xi in such a way that all clauses are satisfied, which holds if and only if there exists a satisfying assumption for F . As a corollary to Proposition 2, we have that deciding our extended notion of warrant status remains within the same complexity bounds. Corollary 1 Let AM be a ground PreDeLP program and Q be either a conjunction or disjunction of ground literals. Deciding AM war Q is NP-hard and in PSPACE. Proof (Sketch) Applying Definition 12, the warrant status of Q can be determined in polynomial time based on the warrant status of its literals; therefore, the same complexity results for determining the warrant status of a single literal apply. Assumption: Since, as stated above, the precise complexity of deciding the warrant status of a literal in a PreDeLP program is not yet known, and with the objective of separating the complexity of this problem from the complexity of the problems inherent to quantitative belief revision in DeLP3E programs, in the following we will make the assumption that classical warranting in PreDeLP is decidable in polynomial time. This is not an unreasonable assumption if we consider the possibility of pre-compiling inferences [3] or having tractable approximation algorithms to address the problem. We call this the polynomialtime warranting (PTW) assumption. Note that, even though this assumption does not hold in general, it is a useful tool in the analysis of the complexity of the problems studied here; it is also with this spirit that we make use of the PTW assumption. Unfortunately, our first result regarding the probabilistic extension of PreDeLP tells us that computing WPFs runs into a computational tractability hurdle. Theorem 1 Under the PTW assumption, computing the warrant probability function for a DeLP3E program is #P-hard.

Quantitative Belief Revision in Structured Probabilistic Argumentation

27

Proof We will prove the statement by reduction from #3CNF-SAT. Given formula F in 3CNF with n variables and m clauses, generate a DeLP3E program as follows: in the AM, there is one atom f , one atom ci for each clause in F and two atoms for each variable V in, which we denote with posV and negV . ^ Y ^ Z ^ , where V ^ denotes either V or For each clause Ci in F of the form X ¬V , we have strict rules: ci  x ^ ci  y ^ ci  z ^ where v ^ denotes posV if V is positive in the clause and negV if it is negative. Next, we have the strict rule: f  c1 , . . . , c m and facts posV and negV for each variable V in the input formula. In the EM we have atoms event-posV and event-negV for each variable V in F . The probability distribution assigns probability 0.5 to each individual event, probability 0 to the conjunction of both events for the same variable, and probability 1 to their disjunction. Finally, the annotation function  assigns formula True to all components of the AM, except the facts, for which we have af (posV ) = event-posV and af (negV ) = event-negV . Now we can see that, with this DeLP3E program, if the warranting probability function  assigns probability p to atom f , we have that:
nec(f )

Pr () = p.

Clearly, from our construction we know that   nec(f ) iff  corresponds to a satisfying assignment for formula F . Therefore, p = 2k n , where k is the number of satisfying assignments for F . Solving for k , we have k = p · 2n . The complexity class #P contains problems related to counting solutions (or, in Turing machine terms, accepting paths) to problems in NP. The decision version of this class is called PP, and contains problems decidable by a probabilistic Turing machine in polynomial time, with error probability less than a certain proportion (say, 1/2). Unfortunately, Toda's theorem [47] tells us that a polynomial-time Turing machine with either a PP or #P oracle can solve all problems in the polynomial hierarchy. Though it might be surmised that the #P-hardness is caused solely by the computation of probabilities (as is the case in many probabilistic formalisms), by analyzing the proof of Theorem 1 we can quickly arrive at the following conclusion. Observation 1 Computing the warrant probability function for a DeLP3E program is #P-hard even in the special case in which probabilities associated with EM worlds can be computed in PTIME.

28

G.I. Simari, P. Shakarian, and M.A. Falappa

Though this intractability holds in general, restricting the EM can soften the impact on complexity. For instance, if we assume that Nilsson's probabilistic logic [33] is used then the complexity is lowered, as we show next; first, we introduce a lemma that will be used in the proof of this result: Lemma 1 ([6, 12]) If a system of m linear equalities and/or inequalities has a nonnegative solution, then it has a nonnegative solution with at most m positive variables. This result was first introduced in [6], and later used in [12] to show that deciding the validity of a formula in their logic is NP-complete. We can now state our result. Proposition 3 Under the PTW assumption, and assuming that Nilsson's probabilistic logic is used in the EM, computing the warrant probability function for a DeLP3E program is NP-complete. Proof Membership: Lemma 1 states that a solution to a linear program is guaranteed to exist where only a number of the variables that is linear in the number of constraints in the EM are set to a non-zero value. This implies the existence of a number of EM worlds with non-zero probability that is linear in the number of statements in the probabilistic model. A witness can therefore be verified in polynomial time. Hardness: NP-hardness is a consequence of the well-known fact that SAT is reducible to computing probabilities in Nilsson logic (cf. [27] for a detailed proof). The previous result gives us a hint towards reaching the next one: if we combine the simplifying assumption that probabilities can be computed tractably with the further assumption that the number of EM worlds that have nonzero probability is bounded by a polynomial (condition 1 below), then we are guaranteed that computing WPFs is also tractable. Corollary 2 Let I = (EM , AM , af ), with AM =       , be a DeLP3E program. If we make the following assumptions: 1. |{ |   WEM and Pr() > 0}|  O(poly(n)), where n represents the size of the input; 2. Pr() can be computed in PTIME for any   WEM ; and 3. the PTW assumption holds, then warrant probability functions for I can also be computed in PTIME. Proof Direct consequence of the assumption that we have access to the polynomially many worlds that have non-zero probability. We thus simply keep an accumulator for each element in the domain of the WPF and iterate through the set of worlds, adding the probability of the world to each formula's accumulator if and only if it is warranted in the AM induced by that world.

Quantitative Belief Revision in Structured Probabilistic Argumentation

29

Unfortunately, the following result states that even in this scenario we still face an intractable hurdle when computing optimal revisions. Theorem 2 Let I = (EM , AM , af ), with AM =       , be a DeLP3E program,  QAFO, and  be a revision objective function that can be computed in polynomial time. If we have that: 1. |{ |   WEM and Pr() > 0}|  O(poly(n)), where n represents the size of the input; 2. Pr() can be computed in PTIME for any   WEM ; and 3. the PTW assumption holds, then deciding if  (I , I (f, af ), (f, af ))  k for some k  R, is NP-complete. Proof Membership: Given I , we can check in polynomial whether each world with non-zero probability induces a maximal consistent subset of I in that world; by construction, AFO operators are only constrained to make such changes in worlds with probability greater than zero. Furthermore, since by hypothesis we know that  can be computed in polynomial time, we can decide whether or not the witness satisfies the constraints. Hardness: By reduction from SUBSET-SUM; we therefore start from an instance of this problem, which consists of a set of n integers x1 , . . . , xn and another integer c; the goal is to verify if there exists a subset of the numbers that add up to c. Lets then create an instance of our problem, starting with a DeLP3E program with one atom numi in the AM for each xi in the input instance, as well as an auxiliary atom p; there will also be a corresponding atom event-numi in the EM for each such atom. The probability distribution associated with these random variables is set to an arbitrary one satisfying the condition that only a number polynomial in n has non-zero probability. Add as facts all atoms numi , with the annotation function defined as af (numi ) = event-numi . Next, add the strict rule:  : ¬numi  p such that af ( ) = True. Set the epistemic input to fact in with af (in ) = event-num1  . . .  event-numn . Finally, set the function to optimize to:   1 if res = {numi | numi  AM , af (numi ) |= } obj(I , I ) = is such that numi res xi = c   0 otherwise. That is, the objective function only considers the revised DeLP3E program, and takes value 1 if and only if the atoms that belong to this program correspond to numbers that, taken together, sum up to c. Thus, all we have to do is check if the optimal revision yields a value of 1 for the objective function in order to decide the given subset-sum instance.

30

G.I. Simari, P. Shakarian, and M.A. Falappa

The reader may note that the construction used in the proof of Theorem 2 uses a very powerful objective function that essentially encodes the NP-hard problem; furthermore, this objective function is not based on WPFs. We now provide an alternative result that proves NP-completeness under the same conditions, but assumes that the objective function is simply the sum of the probabilities assigned by the WPF to the set of ground atoms in the language associated with the AM. Theorem 3 Let I = (EM , AM , af ), with AM =       , be a DeLP3E program,  QAFO, and  be a revision objective function that can be computed in polynomial time. If we have that: 1. |{ |   WEM and Pr() > 0}|  O(poly(n)), where n represents the size of the input; 2. Pr() can be computed in PTIME for any   WEM ; and 3. the PTW assumption holds, then deciding if  (I , I (f, af ), (f, af ))  k for some k  R, is NP-complete even when  is defined as aGAM  (a). Proof Membership: As the ROF can still be computed in polynomial time, the membership result of Theorem 2 still holds. Hardness: We show NP-hardness by reduction from SIMPLE MAX CUT (SMC) [19]. The SMC problem takes as input an undirected graph G = (V, E ) and k  0, and decides if there exist sets V1 , V2  V such that |{(u, v )  E : u  V1 , v  V2 }|  k . Let x be a the only atom in GEM . We will define a simple EM that sets the probability of atom x to 1.0. We specify the atoms in GAM as follows: for each vi  V we create two atoms, set1 (vi ), set2 (vi ). For each edge (vi , vj )  E we create atom edge(vi , vj ) (we assume that each bi-directional edge is specified by one atom an that the order of the arguments for the edge predicate is arbitrary but consistent). We will also have an additional atom query in GAM , which will also act as the formula for the epistemic input. We create AM with the following elements: ­ For each vi  V add the following strict rules: ­ set1 (vi )  query ­ ¬set1 (vi )  query ­ set2 (vi )  ¬set1 (vi ) ­ For each edge (vi , vj )  E add strict rules edge(vi , vj )  set1 (vi ), set2 (vj ) and edge(vi , vj )  set2 (vi ), set1 (vj ) For each element y  AM , we define the annotation function af (y ) = True. For the epistemic input, let af be the extension of af such that af (query) = x. Finally, let the ROF be defined as in the statement of the theorem. Further, for ease of notation, let af  be the annotation function returned after the belief revision operation takes place.

Quantitative Belief Revision in Structured Probabilistic Argumentation

31

Clearly, this construction can be performed in polynomial time. Further, note that the original program is consistent, since none of the rules in AM ever fire since there are no facts. Observations: We notice right away that any valid belief revision operator must return an annotation function af  such that {x} |= af  (query) ­ as this atom is needed to ensure the objective function has a non-zero value (which is clearly possible). Further, any optimal solution where af  (query)  True can be replaced with a solution where af  (query) = x. We also note that in any optimal solution, the size of the set {setX (vi ) |  (setX (vi ) = 1.0} is equal to |V | ­ this is how we capture the notion that for each vertex vi , exactly one of set1 (vi ), set2 (vi ) will be warranted under world {x}); hence, we capture the requirement from the instance of SMC that the sets V1 , V2 is a partition of V . We also note that the annotation function in the solution af  must only modify the return values for strict rules of the form set1 (vi )  query and ¬set1 (vi )  query. Claim 1: Let V1 , V2 be an optimal solution to an instance of SMC. Then, the optimal solution to the corresponding revision problem has an objective function whose value is greater than or equal to |{(u, v )  E : u  V1 , v  V2 }| + |V | +1. This can clearly be achieved by a solution where for each vi  V1 , af  (set1 (vi )  query) = x and for each vi  V2 , af  (¬set1 (vi )  query) = ¬x. Claim 2: Let R be the value of the objective function returned in an optimal solution to the revision problem. Then, the number of edges returned in the corresponding instance of SMC is greater than or equal to R - |V | - 1. By the aforementioned observations, this claim is equivalent to saying that the number of positive literals of the form edge(vi , vj ) that are warranted in the world {x} is less than or equal to the number of edges returned in the optimal solution to the corresponding instance of SMC. Suppose, by way of contradiction, that the number of literals of that form that are warranted under {x} is greater than the number of edges in the optimal solution to the corresponding instance of SMC. By the construction, for each literal of the form edge(vi , vj ), exactly one of the following pairs of literals must also be warranted: set1 (vi ), set2 (vj ) or set2 (vi ), set1 (vj ). Therefore, we can partition the corresponding vertices from SMC into two sets ­ V1 , V2 and the number of edges in the set {(u, v )  E : u  V1 , v  V2 } must then be greater than the number of edges in the optimal solution to the SMC problem. However, this is not possible, as this is also (by construction) the same objective function that is optimized in that problem ­ hence, we reach a contradiction. The proof of hardness follows directly from Claims 1 and 2. So, the proof of Theorem 3 illustrates that the quantified revision problem is still NP-hard when the EM and the number of EM worlds, and (hence) the computation of the WPF is not a source of complexity ­ even when the ROF used is a simple aggregate over WPFs of atoms. Further, as we can embed the Simple Max Cut problem, the ROF ­ even a simple sum over WPFs ­ will not necessarily be monotonic, even when using a revision operator that satisfies

32 Algorithm warrantFormula(F )

G.I. Simari, P. Shakarian, and M.A. Falappa

1. For each tree (V, E )  F with (V, E ) = T  i ( A, L ) do 2. For each v  V with label(v ) = fj A af (fj ) do 3. While |V | > 1 do 4. For each v = A , L  {v  V | children(v )  leaves(V )} do 5. label(v ):= label(v )  ¬ v children(v) ¬label(v ); 6. End for; 7. V := V \ leaves(V ); 8. End while; 9. fi := label root(T  i ( A, L )) ; 10. End for; 11. End for; 12. Return i fi . Fig. 12 An algorithm that takes a classical dialectical forest and computes a logical formula specifying the possible worlds under which a given literal is warranted.

the Inclusion postulate (where the set of worlds satisfying af  (y )  af (y )). This also shows NP-completeness when the belief revision operator performs modifications to AM (by removing elements, as discussed in [41]) as setting af  (y ) = ¬x can be viewed as an operation that is equivalent to removing it from AM . We also note that the related problem of consolidation or contraction by falsum, where we start with an inconsistent program and then must adjust the annotation function to make it consistent, can also be shown to be NPcomplete by a simple modification to the proof: we fix the epistemic input to True, and change the rules of the form set1 (vi )  query, ¬set1 (vi )  query to facts of the form set1 (vi ), ¬set1 (vi ).

5 Warranting Formulas We now focus on an algorithmic approach that can be used to compute approximate solutions and therefore address the computational intractability that we have seen in the results above. In the following, given a dialectical forest F (L) and a node V corresponding to argument A, we will use the notation label(V ) = cA af (c). For a given probabilistic argumentation framework, literal, and dialectical tree, Algorithm warrantFormula in Figure 12 computes the formula describing the set of possible worlds that are warranting scenarios for the literal. Intuitively, this algorithm creates a formula for every dialectical tree in the forest associated with an argument ­ the algorithm iteratively builds a formula associated with the environmental conditions under which the argument in the root of the tree is undefeated. It then returns the disjunction of all such formulas in that forest. We refer to this disjunction as the warranting formula for the literal. The following result states the correctness of the warrantFormula algorithm.

Quantitative Belief Revision in Structured Probabilistic Argumentation

33

v1

1

1 = á{q1, d1, d3}, Lñ
label(v1) = dep_risk  (FN_tox_screen)

v3

3

3 = á{q1, q6, w4, d1}, Lñ
label(v3) = True  (FN_tox_screen)

v2

2

2 = á{q5, w3}, Lñ
label(v2) = (FN_tox_screen)

v4

4

4 = á{q5, w3}, Lñ
label(v4) = (FN_tox_screen)

Fig. 13 Dialectical forest for literal L = sleep aid misuse composed of trees T1 (left) and T2 (right).

Proposition 4 Given forest F  (L),
nec(L) = poss(L) =   WEM |  |= warrantFormula F  (L)   WEM |  |= warrantFormula F  (¬L) .

Proof (Sketch) Claim 1: nec(L)  {  WEM | ( |= warrantFormula(F  (L)))}. To prove this claim, it suffices to show that if T A, L is a valid dialectical tree for L then  |= warrantFormula(F  (L)). Suppose, BWOC, T A, L is such a tree where  |= warrantFormula(F  (L)). We note that this tree shares a root and is a subtree of a tree in F  (L). Also, for each node v in the tree, at line 2, we have  |= label(v ). Hence, we can continue the proof by replacing line 2 with v, label(v ) = f or() and showing that warrantFormula(F  (L)) = f alse. However, we can conclude that warrantFormula(F  (L)) = f or() since the tree has an odd depth by definition ­ this is a contradiction. Claim 2: nec(L)  {  WEM | ( |= warrantFormula(I , L))}. Suppose, by way of contradiction, that the claim is false. Then there must exist a root-sharing subtree of an element of F  (L) such that for each v , at step 4  |= label(v ) and does not exist in . However, this is a contradiction by Definition 7. Claim 3: The second part of the statement follows directly from Claims 1-2 and the fact that poss(L) = WEM \ nec(¬L). Even though warranting formulas are another way of solving the problem of computing probabilities exactly, our main motivation for developing it was to explore options for pursuing tractable algorithms, as discussed next. The following is an example of the warranting formula approach in the setting of our running example. Example 13 Consider the DeLP3E in our running example as shown in Figure 11 with annotation function af 1 . If we run Algorithm warrantFormula for literal sleep aid misuse, we start with the dialectical forest shown in Figure 13. Suppose that the algorithm begins with tree T1 (on the left); the only leaf of this tree corresponds to vertex v2 for argument A2 , and its label remains

34

G.I. Simari, P. Shakarian, and M.A. Falappa

the conjunction of all annotations of elements in the argument ­ label(v2 ) = ¬FN tox screen. The algorithm then moves to the next node up, which is already the root, and updates the label by adding the conjunction with the negation of its child, which yields: label(v1 ) = dep risk  ¬ ¬FN tox screen = dep risk  FN tox screen. Processing tree T2 similarly yields: label(v3 ) = True  ¬ ¬FN tox screen = FN tox screen. Finally, the algorithm outputs the disjunction of these two formulas, which is simply FN tox screen. Outlook: Towards Tractable Computations By applying the warrantFormula algorithm to the dialectical forest for a given literal L, we can obtain the sets nec(L) and poss(L) with a running time proportional to the size of the forest and the annotation formulas ­ though the worst-time complexity has not been determined exactly, it is safe to conjecture that the worst case is intractable. However, the warranting formula approach opens the door to several possibilities for heuristics and approximate computations that either avoid exhaustively enumerating worlds in WEM or working with full forests (or both). When combined with existing heuristics for classical argumentation (the AM) and probabilistic models (the EM), this provides us with a much more efficient way to compute warranting probability functions. Experimental evaluations for such hypotheses are currently underway. The use of the warranting formula approach can have several impacts in the implementation of specific QAFO operators. First, warrant probability functions  in this setting can now be redefined to map elements in their domain to warranting formulas instead of probabilities as in their original formulation. Revision objective functions now have at their disposal formulas instead of raw numbers. This opens up the possibility for specific implementations to leverage optimizations such as applying SAT algorithms to decide whether Pr I1 (L)  Pr I2 (L) (which can be decided via the SAT check I1 (L)  I2 (L)). Such an approach is clearly compatible with heuristic optimizations that may, for instance, sacrifice precision for greater tractability. An alternative class of operators can thus be defined based on the same ideas as QAFO except that approximations are allowed instead of exact computations. There is much work to be done in this direction, which is outside the scope of the current paper. 6 Related Work This paper continues the research line that began with two works on belief revision in structured probabilistic argumentation. In [39], we introduced the

Quantitative Belief Revision in Structured Probabilistic Argumentation

35

DeLP3E formalism (which is called P-PreDeLP in that work) and annotationfunction based belief revision (the class AFO), while in [40] we studied a special case of entailment queries and showed how the framework can be applied to a cyber-attribution problem. As we have seen, the main problem studied in belief revision is the study of how epistemic states should be changed in response to epistemic inputs. Traditionally, epistemic states have taken the form of either belief sets (sets of formulas closed under consequence) [1, 18] or belief bases [24, 23] (which are not closed). Our goal is to ultimately apply our results to real-world domains, and therefore we focus our attention on belief bases. Epistemic states in our case consist of formulas over which argumentation-based reasoning is carried out and to which we couple a general probabilistic model. The relationship between argumentation and belief revision can be traced back to [10]; in this regard, the work that is most closely related to how we approach their combination is that of [14], where explanation-based revision operators are studied. For a discussion on the relationship between the two areas of study, see [15] and [13]. Regarding argumentation systems that feature some quantitative form of reasoning under uncertainty, we point out that the combination of probabilistic reasoning with argumentation systems has been the focus of several works in the recent past. A significant portion of this work, however, has adopted abstract argumentation systems as the basis for the probabilistic extension [29, 45, 25, 16]. Contrary to structured argumentation systems like the one adopted in this work, abstract argumentation is focused on the study of attacks among arguments without inspecting their composition. There are also some works that combine structured argumentation approaches with models for reasoning under uncertainty ­ the first of these was [22]; the work of [28], which was developed even earlier, combines structured argumentation with abstract uncertainty measures but does not explicitly handle probability. Several other works followed; for instance, in [5], the authors develop a possibilistic extension to DeLP, and [26] presents an approach based on probabilistic logic. The main difference between these works and our own is that here we adopt a bipartite knowledge base, where one part models the knowledge that is not inherently probabilistic ­ uncertain knowledge is modeled separately, thus allowing a clear separation of interests between the two kinds of models. This kind of approach is not novel; it has been adopted in several frameworks, such as the Independent Choice Logic [35] or probabilistic ontology languages for the Semantic Web (see [20], and references within). From a quantitative take on belief revision, which is the specific topic of this paper, there hasn't been much work in the precise direction taken here. Perhaps the earliest proposal with such an idea in mind is that of maxichoice revision operators [2], which ensure that minimal changes are made to the knowledge base when revisions are performed; in fact, our class of AFO operators (and therefore also QAFO) perform maxichoice revision operations in each possible EM world. In the related setting of belief contraction operators, David Makinson [31] has defended the use of maxichoice operators, explaining that some counterintuitive behaviors that this approach leads to is

36

G.I. Simari, P. Shakarian, and M.A. Falappa

due to its "misapplication" to belief sets (which, we recall, are closed under consequence). Another quantitative proposal is the one presented in [7], where revisions are carried out according to a notion of distance between worlds, such as the number of propositional symbols that separate one model from another. These notions, however, do not correspond directly with the one adopted here, since in our setting we are pursuing a revision that is optimal from the point of view of its effect on the probabilistic aspect of the consequences of the knowledge base, while the knowledge bases in [2] are non-probabilistic. Another interesting approach to belief revision from a quantitative standpoint is ranking theory [43], which is a normative theory of the dynamics of belief. Again, this approach is not directly related to the one taken here; however, exploring how this well-studied approach can be applied in conjunction with argumentation in the way that probability theory is applied in this work is an interesting avenue for future work. Along the same vein of seeking to minimize information loss, and in the closely related setting of inconsistency management, the work of [21] proposes the notion of preferred repair based on so-called "consistency scores"; our quantitative approach to performing belief revision operations is loosely based on this proposal. Also in the inconsistency management literature, the recent work of [9, 8] proposes to resolve conflicts at a global level to minimize information loss but using incision functions instead. Another work in inconsistency management is that of [46] proposes measures of inconsistency for probabilistic logics. Apart from [21], this is perhaps the closest work in spirit to the one presented here, though the underlying language used (probabilistic conditional logic) is quite different and that work does not address the problem of belief revision ­ their measures, however, could be applied to efforts in line with our own. The adaptation of measures of probabilistic inconsistency such as the ones proposed in [46] to DeLP3E and their use in quantitative belief revision operators is the topic of ongoing and future work.

7 Conclusions and Future Work In this work, we tackled the problem of incorporating a new piece of information to an existing knowledge base; specifically, we adopted the DeLP3E model, which is an extension of the structured argumentation language DeLP with presumptions (PreDeLP) via the incorporation of annotations that refer to events for which we have underlying probabilistic information. The main focus of this paper was to further explore a class of belief revision operators called AFO (for annotation function-based revision) that we proposed recently in [39, 41] by considering operators in this class that have the further requirement of "quantitative optimality" ­ this gave rise to the QAFO class of operators. Though this optimality criterion was kept as general as possible so that knowledge engineers can specify their preferences, we explored the computational complexity of the approach in general, arriving at a host of results that range from intractability for the general case to polynomial-time

Quantitative Belief Revision in Structured Probabilistic Argumentation

37

special cases. finally, we presented an algorithm designed to compute the probability with which a literal is warranted via so-called warranting formulas, and provide some initial discussion regarding how this approach could be applied in the implementation of QAFO operators or approximations of them that trade theoretical guarantees for tractability in practice. Future work along this line of research involves continuing with efforts to bridge the gap between the theoretical developments that have been steadily coming from the belief revision community and practical implementations that can be applied in real-world domains such as medical diagnosis (the topic of our running example) and the related problem of solving the attribution problem in cyber security and cyber warfare, as proposed in [40]. We are also investigating the use of different kinds of belief revision operators, for instance ones that are based on argumentation [14]. Finally, we are currently in the final stages of developing a fully-functional implementation of DeLP3E; incorporating the belief revision operators developed in [39, 41] and in this paper is the next step in the implementation effort.
Acknowledgements This work was supported by UK EPSRC grant EP/J008346/1-- "PrOQAW", ERC grant 246858--"DIADEM", by NSF grant #1117761, by the Army Research Office under the Science of Security Lablet grant (SoSL) and project 2GDATXR042, DARPA project R.0004972.001, and funds provided by CONICET and Universidad Nacional del Sur, Argentina. The opinions in this paper are those of the authors and do not necessarily reflect the opinions of the funders, the U.S. Military Academy, or the U.S. Army.

References
1. Alchourr´ on, C.E., G¨ ardenfors, P., Makinson, D.: On the logic of theory change: Partial meet contraction and revision functions. J. Sym. Log. 50(2), 510­530 (1985) 2. Alchourr´ on, C.E., Makinson, D.: On the logic of theory change: Contraction functions and their associated revision functions. Theoria 48(1), 14­37 (1982) 3. Capobianco, M., Ches~ nevar, C.I., Simari, G.: Argumentation and the dynamics of warranted beliefs in changing environments. Intl. Journal on Autonomous Agents and Multiagent Systems (JAAMAS) 11, 127­151 (2005) 4. Cecchi, L.A., Simari, G.R.: El marcado de un a ´rbol dial´ ectico en DeLP es PSPACEcompleto. In: Proc. of Congreso Argentino de Ciencias de la Computaci´ on (CACIC) (2011) 5. Ches~ nevar, C.I., Simari, G.R., Alsinet, T., Godo, L.: A logic programming framework for possibilistic argumentation with vague knowledge. In: Proc. of UAI 2004, pp. 76­84 (2004) 6. Chv´ atal, V.: Linear Programming. W.H.Freeman, New York (1983) 7. Dalal, M.: Investigations into a theory of knowledge base revision: Preliminary report. In: Proc. of AAAI, pp. 475­479 (1988) 8. Deagustini, C.A.D., Martinez, M.V., Falappa, M.A., Simari, G.R.: Improving inconsistency resolution by considering global conflicts. In: Proc. of SUM. Springer (2014, To Appear) 9. Deagustini, C.A.D., Martinez, M.V., Falappa, M.A., Simari, G.R.: Inconsistency resolution and global conflicts. In: Proc. of ECAI (2014, To Appear) 10. Doyle, J.: A truth maintenance system. Artif. Intell. 12(3), 231­272 (1979) 11. Dung, P.M.: On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artif. Intell. 77, pp. 321­357 (1995)

38

G.I. Simari, P. Shakarian, and M.A. Falappa

12. Fagin, R., Halpern, J.Y., Megiddo, N.: A logic for reasoning about probabilities. Information and Computation 87(1/2), 78­128 (1990) 13. Falappa, M.A., Garcia, A.J., Kern-Isberner, G., Simari, G.R.: On the evolving relation between belief revision and argumentation. The Knowledge Engineering Review 26(01), 35­43 (2011) 14. Falappa, M.A., Kern-Isberner, G., Simari, G.R.: Explanations, belief revision and defeasible reasoning. Artif. Intell. 141(1/2), 1­28 (2002) 15. Falappa, M.A., Kern-Isberner, G., Simari, G.R.: Belief revision and argumentation theory. In: Argumentation in artificial intelligence, pp. 341­360. Springer (2009) 16. Fazzinga, B., Flesca, S., Parisi, F.: On the complexity of probabilistic abstract argumentation. In: Proc. of IJCAI 2013 (2013) 17. Garc´ ia, A.J., Simari, G.R.: Defeasible logic programming: An argumentative approach. TPLP 4(1-2), 95­138 (2004) 18. Gardenfors, P.: Knowledge in flux: modeling the dynamics of epistemic states. MIT Press, Cambridge, Mass. (1988) 19. Garey, M., Johnson, D.: Computers and Intractability: A Guide to the Theory of NPCompleteness. Freeman, New York (1979) 20. Gottlob, G., Lukasiewicz, T., Martinez, M.V., Simari, G.I.: Query answering under probabilistic uncertainty in Datalog+/­ ontologies. AMAI (2013) 21. Gottlob, G., Lukasiewicz, T., Martinez, M.V., Simari, G.I.: Query answering under probabilistic uncertainty in Datalog+/- ontologies. AMAI (2013) 22. Haenni, R., Kohlas, J., Lehmann, N.: Probabilistic argumentation systems. Springer (1999) 23. Hansson, S.: Semi-revision. J. of App. Non-Classical Logics 7(1-2), 151­175 (1997) 24. Hansson, S.O.: Kernel contraction. J. Symb. Log. 59(3), 845­859 (1994) 25. Hunter, A.: Some foundations for probabilistic abstract argumentation. In: Proc. of COMMA 2012, pp. 117­128 (2012) 26. Hunter, A.: A probabilistic approach to modelling uncertain logical arguments. Int. J. Approx. Reasoning 54(1), 47­81 (2013) 27. Khuller, S., Martinez, M.V., Nau, D.S., Sliva, A., Simari, G.I., Subrahmanian, V.S.: Computing most probable worlds of action probabilistic logic programs: scalable estimation for 1030,000 worlds. AMAI 51(2-4), 295­331 (2007) 28. Krause, P., Ambler, S., Elvang-Gørannson, M., Fox, J.: A logic of argumentation for reasoning under uncertainty. Computational Intelligence 11 (1), 113­131 (1995) 29. Li, H., Oren, N., Norman, T.J.: Probabilistic argumentation frameworks. In: Proc. of TAFA, pp. 1­16 (2011) 30. Lloyd, J.W.: Foundations of Logic Programming, 2nd Edition. Springer (1987) 31. Makinson, D.: On the status of the postulate of recovery in the logic of theory change. Journal of Philosophical Logic 16(4), 383­394 (1987) 32. Martinez, M.V., Garc´ ia, A.J., Simari, G.R.: On the use of presumptions in structured defeasible reasoning. In: Proc. of COMMA, pp. 185­196 (2012) 33. Nilsson, N.J.: Probabilistic logic. Artif. Intell. 28(1), 71­87 (1986) 34. Pearl, J.: Probabilistic reasoning in intelligent systems: networks of plausible inference (1988) 35. Poole, D.: The independent choice logic for modelling multiple agents under uncertainty. Artif. Intell. 94(1-2), 7­56 (1997) 36. Rahwan, I., Simari, G.R.: Argumentation in Artificial Intelligence. Springer (2009) 37. Richardson, M., Domingos, P.: Markov logic networks. Machine Learning 62, 107­136 (2006) 38. Shakarian, P., Shakarian, J., Ruef, A.: Introduction to Cyber-Warfare: A Multidisciplinary Approach. Syngress (2013) 39. Shakarian, P., Simari, G.I., Falappa, M.A.: Belief revision in structured probabilistic argumentation. In: Proc. of FoIKS 2014, pp. 324­343 40. Shakarian, P., Simari, G.I., Moores, G., Parsons, S., Falappa, M.A.: An argumentationbased framework to address the attribution problem in cyber-warfare. In: Proc. of Cyber Security 2014 (2014)

Quantitative Belief Revision in Structured Probabilistic Argumentation

39

41. Shakarian, P., Simari, G.I., Moores, G., Paulo, D., Parsons, S., Falappa, M.A., Aleali, A.: Belief revision in structured probabilistic argumentation: Model and application to cyber security. Under review ­ available at: http://www.delp3e.webs.com/Shakarian-etal-DeLP3E.pdf (2014) 42. Simari, G.R., Loui, R.P.: A mathematical treatment of defeasible reasoning and its implementation. Artif. Intell. 53(2-3), 125­157 (1992) 43. Spohn, W.: The laws of belief: Ranking theory and its philosophical applications. Oxford University Press (2012) 44. Stolzenburg, F., Garc´ ia, A., Ches~ nevar, C.I., Simari, G.R.: Computing Generalized Specificity. Journal of Non-Classical Logics 13(1), 87­113 (2003) 45. Thimm, M.: A probabilistic semantics for abstract argumentation. In: Proc. of ECAI 2012, pp. 750­755 (2012) 46. Thimm, M.: Inconsistency measures for probabilistic logics. Artif. Intell. 197, 1­24 (2013) 47. Toda, S.: On the computational power of PP and P. In: Proc. of FOCS, pp. 514­519 (1989) 48. Wirth, C., Stolzenburg, F.: David Poole's specificity revised. In: Proc. of KR (2014)

