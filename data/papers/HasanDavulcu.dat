A Layered Architecture for Querying Dynamic Web Content
Hasan Davulcu*

Juliana Freire

Michael Kifert

I.V. Ramakrishnad

University at Stony Brook
davulcu@cs.sunysb.edu

Bell Laboratories
juliana@research.bell-labscorn

University at Stony Brook
kifer@cs.sunysb.edu

University at Stony Brook
ram@cs.sunysb.edu

Abstract

filling out HTML forms (e.g., to retrieve product information
at a vendor’s site or classified ads in newspaper sites). This
process can become rather tedious when users need to make
complex queries against information at multiple sites, e.g.,

The design of webbases, databasesystems for supporting Webbasedapplications, is currently an active area of research. In this
paper,we proposea 3-layer architecture for designing and implementing webbasesfor querying dynamic Web content (i.e., data
that can only be extracted by filling out multiple forms). The lowest layer, virtual physical layer, provides navigation independence
by shielding the user from the complexities associatedwith retrieving data from raw Web sources.Next, the traditional logical layer
supportssite independence. The top layer is analogousto the external schema layer in traditional databases.
Within this architectural framework we addresstwo problems
unique to webbases- retrieving dynamic Web content in the
virtual physical layer and querying of the external schemaby the
end user. The layered architecture makesit possible to automate
data extraction to a much greaterdegreethan in existing proposals.
Wrappers for the virtual physical schemacan be created semiautomatically, by asking the webbasedesignerto navigate through
the sites of interest - we call this approachmapping by example.
Thus, the webbasedesignerneednot haveexpertisein the language
that maps the physical schema to the raw Web (this should be
contrastedto other approaches,which require expertise in various
Web-enabledflavors of SQL). For the external schemalayer, we
propose a semantic extension of the universal relation interface.
This interface provides powerful, yet reasonably simple, ad hoc
querying capabilities for the end user comparedto the currently
prevailing “canned” form-based interfaces on the one hand or
complex Web-enabling extensions of SQL on the other. Finally,
we discussthe implementation of the proposedarchitecture.

1

make a list of used Jaguars advertised in New York City area,
such that each car is a 1993 or later model, has good safety
ratings, and its selling price is less than its Blue Book value.

Answering such complex queries is quite involved, requiring
the user to visit several related sites, follow a number of
links and fill out several HTML forms. Thus the problem
of developing tools and techniques for creating Web-based
applications that allow end users to shop around for products
and services on the Web without having to tediously fill out
multiple forms manually, is both interesting and challenging.
It is also of considerable importance in view of a recent
survey that contends that 80% of all the data in the Web can
only be accessed via forms [ 181.
Not surprisingly, the design of database systems for managing and querying data on the Web, called webbases (e.g.,
in [25]), is an active area of current database research. A significant body of research covering a broad spectrum of topics
including modeling and querying the Web, information extraction and integration continues to be developed (see [8]
for a survey). Nevertheless research on the design of tools
and techniques for managing and querying the dynamic Web
content (i.e., data that can only be extracted by filling out
one or more forms) is still in a nascent stage.
There are several problems in designing webbases for
dealing with dynamic Web content. Firstly, there is the
problem of navigation complexity. For instance, while there
has been a number of works that propose query languages
for Web navigation [27,17,16,4], they are only beginning to
address the difficult problem of querying sites in which most
of the information is dynamically generated. Navigating
such complex sites requires repeated filling out of forms
many of which themselves are dynamically generated by
CGI scripts as a result of previous user inputs. Furthermore,
the decision regarding which form to fill out next and how,
or which link to follow might depend on the contents of a
dynamically generated page.
Secondly, given the dynamic nature of the Web, in
order to build a practical tool to retrieve dynamic content

Introduction

The trend of using the World Wide Web as the medium
for electronic commerce continues to grow. Web users
need to obtain information in ways that cannot be directly
accomplished by the current generation of Web search
engines. It is typical for a user to obtain information by
*This work was done while the author was at Bell Laboratories.
t Supported in part by NSF grant IRI-9404629.
tsupported in part by NSF grants CCR-9705998,9711386.
Permission to make digital or hard topics of all or part of this work fog
personal or classroom use is granted without fee provided that copies
are not made or distributed Ibr profit or commercial
advantage and that
copies hear this notice and the l’ull citation on the first page. To copy
othcrwisc, to republish, to post on servers or to rcdistrihutc to lists.
requires prior specific permission andior a t&z.

SIGMOD
Copyright

‘99 Philadelphia
ACM

PA

1999 l-58113-084-8/99/05...$5.00

491

raw Web requires a calculus or algebra of some sort to
specify navigation expressions that “populate” the schema
with data. This part is not new as other projects attempted
the same (see e.g., [5]).
However, these approaches
have shortcomings. The webbase designer is required to
have expertise in the underlying calculus, which is usually
some Web-enabling extension of SQL or relational algebra.
Reported experiments [26] suggest that users resist this idea,
because the underlying navigation languages are hard to
master. In addition, given that Web sites change frequently,
maintaining manually generated navigation expressions (can
be an arduous task.
What is different in our approach is that by separating
the virtual physical layer from the logical layer we (can
create navigation expressions semi-automatically,
through
an interactive process that does not require the user to have
any expertise in the formalism underlying the navigation
calculus, and the webbase designer does not even need to
see what the navigation expressions look like. To support
such degree of automation and be able to represent complex
navigation processes, the underlying formalism must h,ave
these properties:

from Web sites, one needs to devise automatic ways to
extract and maintain navigation processes from the site
structure. Lastly, once navigation processes have been
derived, one needs to query the information they represent.
Although traditional ‘databases also provide sophisticated
query languages, such as SQL or QBE, these interfaces
are rarely exposed to the casual user, since they are still
considered too complex. Naive users are usually given
canned queries needed to perform a set of specific tasks.
These canned interfaces served well in the case of fairly
structured corporate environments, but they are too limiting
for the wide audience of Web users. A webbase would
certainly benefit from a query language that is flexible
enough to support interesting types of ad-hoc querying and
yet is simple and natural to use.
To address these problems, we propose a layered architecture, analogous to the traditional layering of database systems, for designing and implementing webbases for querying dynamic Web content. In our architecture, the lowest
layer, which we call the virtual physical layer, provides navigation independence because it shields the user from the
complexities associated with retrieving data from raw Web
sources. Next up, the logical layer, which is akin to the traditional logical database layer, provides site independence.
Finally, the external schema layer is functionally analogous
to the corresponding layer in traditional databases.
This analogy in teirms of layering allows us to focus
on developing techni’ques for problems that are unique
to webbases, and for problems that are common to both
webbases and traditional databases we can directly use the
already known techniques. Based on the databases analogy,
we can readily identify that the problem of mapping the
logical to the physical layer in traditional databases is similar
to what needs to be done in webbases with respect to the
corresponding logical and the virtual physical layer. Thus
all of the techniques developed in traditional databases for
this mapping, such as schema integration and mediators, can
all be directly applied 1.0webbases.
On the other hand, retrieving the dynamic Web content in
the virtual physical layer is a problem unique to webbases.
Unlike the physical layer in traditional databases, we have
no control over the data sources in the Web. Automating retrieval of data from such sources, especially those generated
by forms, is difficult. Similarly, there are important differences at the external sc:hemalayer. Indeed, Web users form
a far larger audience a:nd generally with much wider variation of skill levels than corporate databases users. For them,
traditional query languages such as SQL are too complex.
At the same time, the diverse nature of the audience makes
it difficult to prepare satisfactory canned queries in many areas. Also, preparing canned interfaces for each domain can
be expensive. Thus, it is desirable to have a query interface
that permits both ad ho,c querying and is simple to use.
In brief, our approach to both of the above problems
is as follows.
Mapping the relational schema onto the

It must be high level and declarative, as it is much easie:rto
create high-level specifications of navigation processes.
l It must be compatible with the formalism that underlies
databases query languages (i.e., with relational calculus), so
that it is possible to compose user queries with navigation
expressions in order to create a single expression that would
ultimately fetch the desired answer to the query. This is akin
to the process of answering queries against views, where
view definition is substituted into the query. If the resulting
expression is still part of some declarative formalism, then
the entire query can be optimized using techniques that are
akin to relational algebra transformations (but we do :not
discuss such techniques here).
l Due to the nature of the processes being modeled, .the
navigation calculus must support procedural and declarative
in the same formalism. For instance, at a high level, lthe
calculus should support statements such as “do this after
doing that” or “do this provided that”.
l The high-level
specification formalism must be objectoriented. Web navigation has to deal with complex structures such as Web pages and forms in a declarative environment, and these structures are best represented as objects.
l

o Navigation calculus expressions should be executable specifications themselves.
In our system, we chose a subset of Transaction Flogic [12], which to the best of our knowledge, is the only
language that supports all the above features in a uniform
fashion. Transaction F-logic is an amalgamation of two
other well-known formalisms: F-logic [ 141 and Transaction
Logic [6]. Although our navigation calculus is much more
powerful (and complex) than other proposed languages for
Web navigation, the Web designer does not need to know

492

External

Schema

CJiews)

External

Schema

is complete). While the role of the physical layer in
databases is to describe data storage, the role of VPS in
webbases is to specify how to navigate to the various sources
of information in the Web. In this way, VPS provides
navigation independence for webbase systems and presents
a database view of the Web to the upper layers of the
webbase. In this paper, we use the relational model to
represent data in webbases. More details on the VPS layer
appear in Sections 3 and 4.
We remark that, since the main focus in this paper is
querying and navigation, we do not discuss updates and
methods for data extraction from HTML pages. To the best
of our knowledge, the former issue has not received much
attention, while the latter has been researched extensively.
At the VPS layer, data collected from different sources
resides in different relations, thus semantic and representational discrepancies are likely to exist between these relations. For instance, prices could be represented using different currencies and semantically identical attributes can have
different names. These differences are smoothed out at the
logical layer of the webbase architecture, which provides
site independence, i.e., independence from the specifics of
the data sources that supply data to the webbase. Further details on the logical schema are presented in Section 5. We
should note that resolution of semantic and representational
differences between sites is not the subject of this paper.
There is a vast body of research dedicated to this topic, and
we could use the techniques developed there.
The top level in the webbase architecture is the external
schema layer, which targets specific application domains
(e.g., used car ads, computer equipment, etc.) and is
supported by a user interface that permits a high degree of
ad hoc querying by naive Web users. As mentioned earlier,
for such users traditional query interfaces are either too
complex (SQL, QBE) or too rigid (canned and form-based).
Thus, we need a query language that is flexible enough
to support interesting types of ad-hoc querying and yet is
simple to use. In search of such a language, we resurrected
the Universal Relation (VR) query interface. The details of
our implementation of the UR are presented in Section 6.
The following example illustrates the distinctions among
different levels of abstraction in a dynamic webbase.
Example 2.1 (Used Cars) A webbase for used car shopping in the metropolitan New York area might access the
several sites, such as newspapers (Newsday’ and New York
Times), new car buying services (Car Point and Auto Web),
blue book price references (Kelly’s), reliability information
(Car and Driver) and finance (Car Finance). We present a
possible set of VPS relations that can be extracted from these
sites.2 To make the tables more compact, we use Car as a
shorthand for the attributes Make, Model, Year.

OJiews)

SQL, QBE.

Logical

Schema

Lc~lcal

Schema

I

Relational algebra
I

I

Physical

Schema

Virtual

1

Physical

r

Traditional Database Architecture

Figure 1: Traditional
architecture

Schema

. Navigation calculus
-Data exlratim
.WC?bserVM

Wehbase Architecture

database architecture vs. webbase

anything about it. Our approach makes it possible to create
all necessary wrappers for the virtual physical schema semiautomatically, by simply asking the webbase designer to
navigate through the sites of interest. We call this approach
mapping by example.
The virtual physical layer and the
navigation calculus are described in Sections 3 and 4.
For the external schema layer, we propose a semantic
extension of the universal relation interface [24, 231, which
we call structured universal relation. We argue that this
interface provides powerful, yet reasonably simple ad hoc
querying capabilities for the end user (e.g., a Web shopper)
compared to the currently prevailing canned, form-based
interfaces on the one hand and complex Web-enabled
extensions of SQL on the other. The external schema layer
is described in Section 6.
Apart from the aforesaid sections, Section 2 introduces
our layered architecture. Section 5 discusses the problems
associated with the logical layer of a webbase; our implementation effort of the proposed architecture is described in
Section 7; related work appears in Section 8; and concluding
remarks in Section 9.

2

Architecture

for the WebBase

The most significant difference between a webbase and a
database is the absence of the physical level in the traditional
sense. Indeed, actual data is the exclusive domain of the Web
server, and the only way the webbase can access the data is
through filing requests to the server by following links or by
filling out forms.
Therefore, we introduce the notion of the virtual physical
database schema (W’S), which represents all the data there
is to see by filing requests to the server. In many cases,
the VPS layer cannot be constructed completely (or we
might never know whether the known part of the VPS

‘Newsday is a regional newspaper with circulation in Long Island and
New York City.
2Although this example describes these sites fairly accurately. for
illustration purposes we introduce simplifications as well as bnng in
features found in other sites.

493

Level Relations
I1 VPS- -~~~
1 neusday(Car,Price,Contact,Url),
neusdayCarFeatures(Url,Features,Picture)
nyTimes(Car,Features,Price,Contact)
carPoint(Car,Price,Features,ZipCode,Contact)
Dealer Cars
autoWeb(Car,Price,Features,ZipCode,Contact)
t- Blue Book Prices kellvs(Car,Condition,BBPrice)
carAndJ&ver(Car,Safhy)
'
carFinauce(Car,ZipCode,Duration,Rate)
Iftion

t%%&;kds

I

1

1

Table 1: VPS Level Relations
R in the VPS layer, there is a quadruple, called a handle,
represented as follows:
H = (mandatory-attrs, selection-attrs, R, expression)
The set of mandatory attributes specifies the minimum information that the handle needs in order to invoke the navigation calculus expression (the fourth component) and retrieve
the requisite data. The set of selection attributes specifies the
additional attributes that might be also specified. These aidditional attributes are used by the expression and are eventually passed to the various Web servers who, presumably, use
these attributes to return more specific answers. For convenience, we assume that mandatory-attrs C selection-attrs.
There can be several handles for the same relation.
Different handles for the same relation must use different
sets of mandatory attributes. However, different handles
can have the same sets of selection attributes and the same
navigation expression (for instance, the same HTML form
might have two alternative sets of attributes; at least one of
them must be filled in order to get a result).
We assume that all handles for the same relation agree
with each other: if Hr = (Ml, 5’1, R, El) and HZ = (Mi,
S’s, R, Ez) are two handles for the same relation and we
specify concrete values for a set of attributes S such that
Ml U MZ C S C Sl fl Ss. then handles HI and Hz return
the same result.
Table 3 shows the sets of mandatory and selection
attributes for some relations in the VPS of Example 2.1. The
first column in the table lists relation schemas, the second
column shows mandatory attributes for each schema, and
the third shows the optional attributes (= selection-attrs mandatory-attrs).

Table 1 shows examples of VPS relations for various Web
sites. The first line in the table illustrates that data for the
Newsday’s site might be presented in multiple hyper-linked
pages, and depending on the user’s request, data extraction
might require navigating multiple pages. e.g., new&y and
newsdayCarFeutures.
The logical level relations for our webbase and their
associated relational schemas are presented in Table 2, along
with the corresponding mappings to the VPS layer.
The external schema layer is represented by the following
universal relation, Used.CarUR, which contains the union of
all the attributes of the logical layer:
UsedCarUR(Car,Price,Features,Contact,
BBPrice,Safety,ZipCode,Duration,Rate)

The mapping between external and the logical layer in
the Universal Relation model is a rather subtle issue. In
Section 6, we show that the known approaches (e.g., [23])
are not suitable for Web applications and discuss a possible
solution.
Now, the query posed in Section 1, “make a list of used
Jaguars advertised in New York City area sites such that
each car is a 1993 or later model, has good safety ratings,
and its selling price is less than its Blue Book value”, can be
expressed against our webbase as follows:
UsedCarUR(jaguar,Mdl,Year,Price,Featrs,
Contact,BBPrice,good,ZipCode,Duration,Rate),
Year >1993,13BPrice
> Price

3

0

Virtual Physical Schema

An important difference between webbases and traditional
databases is that webbases do not control the physical
data and there are limited ways in which this data can be
retrieved. Given a virtual physical schema (VPS) for a
relation, the corresponding data can usually be obtained only
by filling out a form, which requires that the user specify
values for a certain selection of attributes, some of which
might be mandatory and some optional. In fact, there might
be several alternative sets of optional/mandatory attributes
per relation that limit the scope of data to be retrieved.
In addition, we must specify the navigation process that
needs to be executed in order to get the data. This process is
represented using Navigation Calculus, which is described
in the next section. Tiherefore, for each relation schema

4

Navigation Calculus

Navigation maps. The basic data structure that enables
automated access to virtual relations residing in the VPS of a
webbase are the navigation maps for the participating sites.
Intuitively, a navigation map codifies all possible acce.ss
paths that a site presents for populating a virtual relation.
A navigation map is a labeled directed graph (see Figure 2)
where the nodes represent the structure of static or dynamic
Web pages, and the labeled edges represent possible actions
(i.e., following a link or filling out a form) that can be
executed from a dynamic page. Our navigation maps are

494

1 Lo&Cal Level Relations
classifieds(Car,Price,Contact,Features)
I

1 Definitions
~car,Pr~c~,cont~ct,P~atur~~
(newsday W
newsdayCarFeatures)

U
dealers(Car,Price,Contact,Features)

(nyTimes)

BCsr,Pric~,Contact,F*at~roa

~car,Pris.,contact,s..tur.,(carpoint)
U ‘ITCu,Pr~c.,contact,~~atur~s (autoWeb)

blue-price(Car,Condition,BBPrice)
reliability(Car,Safety)
interest(Car,ZipCode,Duration,Rate)

~Car,Condition,BBPric*(kellys)

carAndDriver
carFinance

Table 2: Logical Level Relations
VPS
newsday(Make,Model,Price,Contact,Url)
newsda&rFeatures(Url,Features,kc&re)
nyTimes(Make,Model,Features,Price,Contact)
kellys(Make,
Model, Condition,
BBPrice)

) Mandatov
) Make
Url
Make

) Optional
] Model

I
Condition

Make, Model

Table 3: Virtual Physical Schema
closely related to the Web schemes of the Araneus project
[25, 51, but our modeling of the Web is process-oriented,
which facilitates creation of the navigation expressions from
navigation maps.
Mapping the virtual physical schema onto the raw Web requires a calculus of some sort. One obvious candidate would
be the relational calculus or algebra, extended with Webspecific primitives (and some other known extensions, like
the unnesting operator of Ulixes [5]). The Araneus and the
Ariadne projects [5, 151 take this approach. However, these
formalisms are not powerful enough to express complex navigation processes on the Web. For instance, as shown in Figure 2, a navigation process to access the used car ads in the
classified section at the site www.newsday.com requires following a link (linkfauto)), filling out a form @rmfl(make)),
then making an if-then-else choice depending on the resulting page - if the page is not a data page, another form (form
fl(model,$eatrs)) will have to be filled out. The length of the
sequence is not fixed. It is usually one or two, depending
on the number of answers that match the initial query. Once
the final data page is reached, an iteration to collect data is
needed (repeatedly hitting the “More” button).
Examples like this and our experience with other, more
complex, sites shows that navigation processes are best
represented using a calculus that allows recursion and has
the notion of ordering of events. In addition, the calculus
must deal with complex structures, such as Web pages,
forms, etc., which are best represented as objects.3
Unlike other projects that deal with navigation processes
on the Web, we do not invent yet another, new navigation
algebra or calculus. The calculus that satisfies all the
requirements stated above is actually well-known: it is a

newsday

UsedC

new-car-dealer

‘Q

collectible-cars

sport-utility

carPg
link(more)
(3

link(llsting)

link(car f eatures)

i

newdayCarFeatures(features, picture)

Figure 2: Navigation map for Newsday Classified Car Ads
subset of serial-Horn Transaction F-logic [ 121, a natural
cross between Transaction Logic [6] and F-logic [14]. In
fact, the Florid system [9], based on F-logic, has proved
to be very successful for Web applications. Because Florid
lacks the Transaction Logic component, it is not suitable to
be used as a calculus for encoding navigation processes.
The object model. F-logic extends classical logic by
making it possible to represent complex objects on a par with
traditional flat relations. A navigation map is a collection of
F-logic objects, such as the following object that represents
one of the forms to be filled out at the Newsday’s site:
submit-form

: action[f

"/cgi

orm + f ormOl[cgi

- bin/nclassyNDD.x/";

method + “post.“;
mandatory--i+{make,model};

30bserve that the user-level view of the database is represented using
the relational model. However, the underlying navigation process (which is
invisible to the end user) is based on the object model, since it has to deal
with Web pages and other objects, which are not part of the user view.

optional+{year}];
source

495

+ www.newsday.com]

+

currentUrl(pid,

url)

action[
object~{link,form}
source*url;
targets=%%web-page;
doi?&JattrValPair+web-page]
submit-form
follow-link

: action
: action

Current URL of browsing process PID
Declaration of Class Action
Action can apply to a form or a link
Page where the action belongs
Where this could lead us
Method to execute action
Form fillout is an action
Following a link is an action

web-page[
address+url;
titlejstring;
contents*string;
act,ions=D{action}]

Declaration of Class WebPage
URL of page
Title of the page
HTML contents of page
List of actions found in the page

data-page :: web-page
data-page[extract+relation]

The class of data Web pages is a subclass of web-page
Data pages have a data extraction method

link[
name*string;
add.ress*url]

Declaration of Class Link
Name of link
URL of link

form[
cgi.*url;
methodjmeth;
mandatorywattribute;
opt ionalwattribute;
sta.teWattrValPair]

Declaration of Class Form
CGI script’s URL associated with this form
CGI invocation method
Mandatory attributes of this form
Optional attributes of this form
State of form (set of attribute-value pairs)

attrValPair[
attrlame-sstring;
type+widget;
default+Object;
value+Object]

Declaration of Class AttrValPair
Name of the attribute part
Checkbox, select, radio, text etc.
Default value of the attribute
The value part

Figure 3: Common WWW Data Structures Represented in Navigation Calculus
newsday(Make, Model,Price,Contact,Url)
+newsdayPg.actions
: follow-link[object
+ link(auto);
doitO()+UsedCarPg]
@3UsedCarPg.actions
:submit-form[object
+form(fl);
doit@(Make)-XarPgl]
8 (CarlPgl : data-page[extract+tuple(Contact,Price,Url)]
V(CarPgl.actions
: submit_form[object-+form(f2);
doit@(Model)+CarPgZ]
@ CarPg2 :data-page[extract+tuple(Contact,Price,

Find car ads at Newsday:
Follow link(auto)
to used car ads;

WI

Fill form(f1) using M&e;
Either extract data,
or fill form@)
using Model,
then extract data

Figure 4: The Navigation Process of Retrieving Used Car Advertisements from Newsday Site
In the first line, submit-form:action
says that the object
submit-form
belongs to class action. It has attributes
form and source. The attribute form has the value formO1,
which represents the form to be filled out. form01 is itself
a complex object with four attributes: cgi and method are
single-valued, and mandatory and optional are multi-valued.
The attribute source of the object submitform represents the
page to which the action belongs. In addition, the object has
a method, doit (defined below), whose purpose is to execute
the action.
Figure 3 presents the schemas (called signatures) of some
of the objects we use to model navigation maps. The doubleshafted arrows =+ and =++ (as opposed to + and + in the

previous example) signify that these expressions declare the
types of the attributes and methods rather than their states.
Navigation expressions. F-logic provides a declarative
calculus for representing complex objects on the Web, but
to model navigation processes one needs a formalism for the
representation and sequencing of actions. These facilities
are provided by Transaction Logic [6], a conservative extension of classical logic, which is suitable for representing
complex declarative processes that both query the underl,ying database state and update it. The following subset of
serial-Horn Transaction Logic complements the subset of Flogic described above and provides an expressive navigation

496

builder, needs to ever see these expressions. It is easy to see
that the above expressions closely mimic the structure of the
navigation map in Figure 2 and, in fact, they can be derived
automatically directly from that map in linear time in the
size of the map. Due to space limitation, we do not present
the translation algorithm, because this would require that we
spell out the structure of the navigation maps in much greater
detail. Details are given in the full version of this paper.4
It is important to realize that the translation from the map
to the calculus expression has been greatly facilitated by:

calculus for enabling access to raw Web data.
For the purpose of this paper, it suffices to explain the
informal, procedural reading of some of the connectives of
Transaction Logic. Underlying the logic and its semantics
is a set of database states and a collection of paths. A path
is a finite sequence of database states. For instance, if ~1,
s2, “‘, s,, are database states, then (~1, ~2, . . . . s,) is a path
of length 7~. Just as in classical logic, Transaction Logic
formulas assume truth values. However, unlike classical
logic, the truth of these formulas is determined over paths,
not at states. If a formula, 4, is true over a path (~1, . .. . sn),
it means that $ can execute starting at state ~1. During the
execution, the current state will change to ~2, ss, .... etc., and
the execution terminates at state sn.
Procedurally, a Transaction Logic formula can be understood as a transaction or a query (depending on whether it
changes the database state or not). Semantically (and procedurally) these formulas have several common attributes of
database transactions, such as atomicity and isolation. With
this in mind, the intended meaning of the new connectives
of Transaction Logic can be summarized as follows:
l
l

our process-oriented object model, whose objects correspond to nodes and links of the navigation map;
l the fact that the F-logic
component of our navigation
calculus naturally supports this object model; and
l the Transaction Logic component that represents the process structure encoded in the navigation map.

l

Finally, once the translation is done, the resulting navigation
expressions can be directly executed by a Transaction Flogic interpreter when user queries posed against the external
schema level of the webbase eventually turn into queries
against the VPS layer.

I$ @ 1c,means: execute 4 then execute $.
q5V $ means: execute 4 or execute 11,non-deterministically.
This connective is useful for specifying alternative execution
branches in a navigation process.

5

The VPS layer provides a relational view of data that can
be retrieved from a Web site, thereby hiding navigation
details. In contrast to this, we use a logical layer to provide a
uniform interface to data arriving from multiple sources. By
separating these layers, we achieve site independence. This
means both independence from the differences in vocabulary
and representation used by different sites as well as complete
transparency with respect to where the data is coming from.
Table 2 shows a possible mapping of Logical relations
into VPS relations. While VPS layer has eight relations
that shield the user from navigation details, the five logical
relations in the example show a view of the Web data that
is completely transparent with respect to the location of the
data source.
In this paper, we are not concerned with the issues
pertaining the mapping of the logical layer onto VPS. This
mapping can be done using conventional techniques (e.g.,
relational algebra, or Datalog rules) or we could use more
advanced techniques, which might offer certain advantages
in the Web environment (e.g., [8, 191).
However, one issue related to this mapping must be
addressed. The problem is that unlike traditional databases,
VPS relations can only be accessed by supplying values for
certain sets of mandatory attributes. Since logical relations
are mapped onto the physical ones, it is clear that they
also can be accessed only by providing values for certain
attributes. The process of determining these sets of attributes
is called binding propagation (because, in abstract terms,

Figure 4 shows the navigation process to extract car ads
from the Newsday site. Here we use path expressions
as shortcuts for longer F-logic expressions, as described
in [14, 131. For the benefit of the reader who is not
fluent in Transaction F-logic, we annotated each clause in
Figure 4, so the meaning of the navigation expression should
be self-explanatory. Navigation expressions do not always
need to be that complex. For instance, navigating to the
NewsdayCarFeatures page, which is part of our VPS, can
be achieved using the much simpler expression:
newsdayCarFeatures(Url,Features,Picture)
(DataPg : data-page).actions[object
+
link(-)[name
-+ “CarFeatures”];
doit@I()
-+ -[address
+ Url;
extract
+ tuple(Features,
Picture)]]

Logical Layer

+

This expression says that to get to a car features page,
we must first get to a data page (DataPg) that has a link
called “Car Features”, follow this link, and then extract the
features from the page. Of course, in a bigger system, we
would have to qualify this initial page even further to avoid
mis-navigation. The interesting point here is that the page
denoted as DataPg is not an entry point to any navigation
process that a regular Web user might perform. Indeed, as
seen in Figure 4, one has fill out one or two forms to reach
this page. However, this is not a concern at the VPS layer. It
is a job of the logical layer, described in Section 5, to order
joins in such a way that the relation newsday of Figure 4 is
computed first (to retrieve the desired page, DataPg).
Even if the above Transaction F-logic expressions look
a bit complex to the reader, the most important aspect of
our webbase architecture is that nobody, except the system

4The full-version of this paper available at http://www-db.reseah.belllabs.com/users/juliaa.

497

sets of mandatory attri’butes in HTML forms correspond to
variable bindings in programming languages).
The problem of binding propagation has been wellstudied in the literature (see e.g., 17,291). In the following,
we propose a much simpler description, which also differs
from other works in two respects: (1) it handles not only
conjunctive queries, but also all relational algebraic queries;
and (2) instead of deri.ving bindings for a given query on
the fly, it statically determines all allowed bindings for each
logical relation.
Let CYdenote a relational algebraic expression over VPS
relations, and we need1to determine the bindings (or sets
of mandatory attributes) for the resulting relation of this
expression. The binding propagation algorithm can be
described by the following rules, each corresponding to one
of the allowed re1ationa.l operators:
Let (I! = V, where V is a VPS relation, if M is a binding
for V, then M is also a binding for CX.
l Leta = El U Ez or (2 = El - Ez, where El and E2 are
relational expressions over VPS relations, if Ml is a binding
for El and Mz is a bind.ing for Ez, then MI UMz is a binding
for (Y.~
l Let (Y = XX (E) or Q = a+(E),
if M is a binding for E
then M is also a bindiqg for CY.
l Let CS= E>l W E2, if MI,
MZ are bindings for El, Ez,
respectively, then Ml U (Mz - (El n Ez)) and M2 U (Ml (El fl E2)) are both bindings for cr. Here El FUEz denotes
the set of common attributes of the relation schemas for El
and Ez.
l

6

External Schema

Casual users query the webbase through the external schema.
Traditionally, end users have been given access to limited interfaces that allow only a fixed set of canned queries. These
canned interfaces served well in the case of fairly structured
business environments, but, as remarked earlier, they are too
limiting for a casual Web user. On the other hand, more flexible query languages, such as SQL or QBE, are too complex.
In search of a suitable query interface for webbases, we resurrected the idea of the Universal Relation (UR) [24].
The basic idea is simple and appealing. The user is
presented with a list of all attributes that might be of interest
for a particular application domain. To pose a query, the
user simply points to a set of output attributes and imposes
conditions on some other attributes. This is it: no joins, sheer
simplicity. Of course, to realize such an agenda, the system
(and the user) must know what such a query exactly means,
and the understanding of that meaning by both the system
and the user must coincide.
Simplistically, the semantics of a universal relation query
is explained as a natural join of the underlying relations at
the logical layer, which cover the output and the selection
attributes specified in the query. Moreover, the join must be
lossless. Losslessness is required because this is a formal
analog of the common sense idea of connections between
concepts that “make sense”.
Underlying this idea are two basic assumptions:
1. The unique relationship assumption: The relationshlir.
between any given subset of attributes in the universal
relation schema is unambiguous and unique; and
2. The unique role assumption: The name of an attribute
unambiguously determines the role of that attribute.

From these rules, it is also easy to derive an algorithm for
join ordering under the given set of bindings, i.e., an ordering
RI, . .. . R, that guarantees that for each i (1 5 i 5. n), all
mandatory attributes of Ri belong to the union Uizi Rj.
Clearly, the existence of such an ordering is necessary
and sufficient for a join to be computable under the given
set of mandatory attributes. However, in the presence of
multiple sets of mandatory attributes per VPS relation, such
an algorithm would be exponential. In fact, [29] shows that
the problem is NP complete in this case.
To illustrate the above binding propagation algorithm,
consider the logical level relation classzjieds from Example 2.1. Since Make is the only mandatory attribute of
the relation newsday and Url is the only mandatory attribute of newsdayCarFeatures,
by the join rule above,
{Make} turns out also to be the only mandatory binding for
newsday W newsdayCarFeatures.
Similarly, Make is
the only mandatory attribute of nyTimes. Therefore, by the
union and projection rules, {Make} is the only mandatory
binding for classifieds.

The first problem arises even in very simple schemas
that contain just four attributes. For instance, a customer
and a bank might be connected because the customer has
an account in the bank, a loan, or both. Which one
did the user have in mind when she selected Bank and
Customer as output attributes? A number of solutions were
proposed to address the first problem, which range from
restricting the topology of the underlying logical schema
(e.g., acyclicity [2 11) to additional layers of semantics (e.g.,
Maximal Objects, Window Functions [23,22]).
Unfortunately, on the Web, we cannot assume the very
basic lossless join semantics for UR, since we cannot even
assume any dependencies (join, functional, or multivalued)
on which the very idea of losslessness is based. Nor can
we use most of the approaches to enforcing or relaxing the
unique relationship assumption, because these approaches
rely heavily on the use of constraints.
The second problem, the unique role assumption, was
assumed to be solvable by simple renaming of attributes.
However, this solution was never thought to be practical
and may have been responsible for the general lack of
enthusiasm for the UR approach.

‘Here we assume that the user wants all available answers to the query.
If the user is willing to accept only some available answers because she does
not want or care to fill out all the required attributes in a form, then we could
define a relared union. In a relaxed union, both A41 and A42 (separately)
would be acceptable bindings for a.

498

In our attempt to adapt the UR as a Web interface, we kept
the basic idea of a simple query interface, but rejected the
lossless join semantics and the two uniqueness assumptions.
We call this approach structured universal relation.
The
basic idea is to replace losslessness and constraints with
compatibility
rules. A compatibility rule has either the form
Rk+R
or the form RI, . . . . R~+TR.
In the first
RI, . .. .
case, the rule says that if you already joined RI, . . . . Rk
then joining with R also “makes sense”. This is our
“poor man’s lossless join requirement”. The second rule is
really a constraint. It says that if we have already joined
RI, ..-, Rk, then joining with R would create an incorrect
relationship (in the UR model, such connections are known
as “navigation traps”).
With these constraints, we can formulate the semantics of
a query as follows: Let Q be a query that mentions the set
of attributes A = Al, . .. . A,. Then the semantics of this
query is said to be the join R = RI C4 . . . W R,, where
Rl , . . .. R, is a minimal (with respect to inclusion) subset of
logical relations that satisfy the compatibility rules, and R
contains all attributes in A.6 This is essentially our analogue
of the maximal objects approach [23]. If there are several
maximal objects covering the query attributes then we take
the union of results obtained from each object. Depending
on the exact structure of the compatibility rules, algorithms
with various efficiency can be constructed. For instance, if
the rules are of the form R+Q, then we have a restricted
join-ordering problem mentioned in Section 5.
To address the problem of unique name assumption, we
propose to organize the attributes in the UR into a hierarchy
of concepts. Each concept is a relation schema whose
attributes are concepts of a lower layer. As shown in
Figure 5, the top layer in this hierarchy is the universal
relation itself, and the concepts are the attributes of that
relation.

full or liability coverage.
0
The idea behind concept hierarchies is that the user
starts by selecting top-level concepts and then proceeds
to subconcepts. This makes it possible to build queries
incrementally, by restricting the search to various subconcepts and to specific ranges for attributes at the leaf
level. The unique name assumption is not an issue here for the user or the system - since both can see the entire
concept hierarchy to which the attributes belong, and the
relationships among concepts and attributes are defined by
the compatibility rules.
We believe that webbases will be designed for application
domains (such as cars, jobs, houses) by the experts in those
domains, and designing concept hierarchies and compatibility constraints is a feasible task for them. We illustrate these
ideas with an example, leaving out the details due to space
limitation.
Example 6.2 (Structured UR in Action)
The following
compatibility constraints specify the meaningful connections for the UsedCarUR of Example 6.1.

Full

Using compatibility constraints, our algorithm generates the
following maximal objects and the corresponding relational
expressions:

Dealers

Classitieds

UsedCariCar,

Lease + Full-Coverage
Used-Cars + 7 TradeIn-Value

advertised in New York City area sites such that each car’s
monthly payments are less than 1,000 dollars, and its
selling price is less than its Blue Book price. This query can

be expressed as:
Used-Car-UR(jaguar, Model, Year, Price, BBPrice,
Rate, Ins-Cost), Price < BBPrice,
(Price X Rate + Ins-Cost) + 12 < $1000

Retail

BlueBwk(Car.

( Car. Price, BBF’rice,

Rate, Contact.

Semantics
We cannot lease a car
from its owner
Leased cars have to be
1 fully insured
1 Trade-in values are not
applicable

Consider the following query: make a list of used Jaguars

Lease

Price, Contact)

UsedCuUR

Compatibility Constraints
Classifieds
+ 7 Lease

Dealers W Lease W Full W Retail-Val
U Dealers W Loan W Full W Retail-Val
U Dealers W Loan W Liability
W Retail-Val
U Classif ieds W Loan W Liability
W Retail-Val
U Classif ieds W Loan W Full W Retail-Val
0

BBPrice)

Cost)

Figure 5: Concept Hierarchy for the Used Cars UR

Now assuming the existence of a mapping function from
external schema relations to the logical level, maximal
objects made up from the UR relations can be translated
into conjunctive queries over logical level relations. Once
translated, these queries can be optimized and evaluated by
standard query evaluation techniques.

Example 6.1 (Concept Hierarchy)
The concept hierarchy describes the following: (1) A used car is either advertised at a dealer site or it is in the classified section of a newspaper site; (2) The blue book price of a car can either be its
trade-in price or its selling price; (3) The interest rate for a
used car depends on whether it will be financed or leased;
and (4) the insurance rate depends on whether it provides

7

Implementation

and Experiences

We have implemented the most essential components of two
of the modules in our webbase architecture: the navigation
map builder and the query evaluator.
In what follows we
describe the ideas underlying our implementation.

‘%ompatible meansthat for every 1 < i 5 n, there is a rule Left-t R;
such that Left E {RI , . . . . Ri-1); and there is no rule Left-t-R
such
that LeftU {R} E {RI,...,&}.

499

attribute. It is worth pointing out that in many instances our
parser is able to find these links by considering their HTML
environment (e.g., a table), or the user can provide additional
hints. We are cuirently building a graphical user interface to
simplify the input of such information by the designer.
We used our initial prototype of the map builder to map
various sites. To give an idea of the degree of automation
achieved, for the Newsday site depicted in Figure 2, all
objects that describe the navigation map (85 objects with
over 600 attributes in total) were automatically extracted.
Less than 5% of the information in the map was added
manually, which consisted of 10 to 12 facts to standardize
attribute and domain value names. For other sites such as
New York Times and Daily News, the ratio was similar. The
process of mapping each of these sites took on average 30
minutes. It is worth pointing out that the main problem we
face while mapping sites is the presence of faulty HTML, in
which case the parser needs to be able to recover from the
ill-formed documents.
Some points are worthy of note with respect to the
maintenance of such maps. Modifications to Web sites
can be automatically detected by periodically comparing
the navigation map against its corresponding site, or when
the corresponding navigation process fails. Whereas certain
structural changes such as the addition of a new form
attribute require manual intervention, others can be applied
automatically (e.g., the addition of a cell in a selection list).
Since we first built navigation maps for car-related sites,
we have noticed quite a few changes to these sites. For
example, in Kelly’s Blue Book (www.kbb.com) new links
with information about 1999 cars have been added. In order
to update navigation map, we only had to navigate through
the modified pages, a process that took a few minutes.

Navigation Map Builder.
We use the methodology of
mapping by example to extract the navigation maps from
Web sites. The main idea behind mapping by example is
to discover the structure (or schema) of a site while the
webbase designer moves from page to page, filling forms
and following links. There are two key components to this
methodology: (1) discovery of access paths to the data of
interest; and (2) extraction of action objects (see Figure 2).
In order to build a practical tool, there are two important
requirements: the mapping process should be as transparent as possible to the webbase designer (its operation should
closely mimic the browsing experience); and the mapping
tool must be portable (e.g., it should not require modifications to the browser).
The navigation map builder achieves these goals by using
JavaScript events to ciipture browsing actions. Actions are
dynamically intercepted by JavaScript handlers (inserted
into the retrieved pages by the map builder), and are added
as edges of the navigation map. When a new page is
loaded into the browser, it is parsed, and a new node
corresponding to the page is inserted into the navigation
map.7 In order to guide the designer, an applet displays a
graphical representation of the navigation map as it is being
constructed, highlighting in the map the node corresponding
to the page displayed in the browser.
The map builder parses an HTML page and generates
a set of F-logic objects (as detailed in Section 4). It
extends PiLLoW’, a publicly available Prolog-based system,
to extract all necessary information for following links and
submitting forms found inside the page. Since not all
information is stored in the HTML object structure (e.g.,
labels denoting the domain values of some attributes and
attributes defined through a set of links) we take advantage
of HTML tags and anchors and other structuring primitives
(e.g., tables, enumeration) to extract such information. For
forms, the extractor is also able to infer which attributes
are mandatory from .their widget (i.e., if an attribute is
represented by a radio button we can safely assume it
is mandatory), as well as other information such as the
domain of attributes (e.g., from the values of a selection
list), maximum length (e.g., for a text field), default value,
to name a few. For data pages, as described in Figure 3, we
assume that the design,er provides an extraction script.
Of course there are instances where input from the
designer is needed. For instance, the designer has to indicate
whether a text field is mandatory. Also, it is not uncommon
in forms for attributes to have rather cryptic symbolic names
- in these cases (to fa,cilitate subsequent querying) the user
might want to provide a more informative name. There are
also instances where attributes are implicitly defined through
a set of links (e.g., a list of links with car models). Since this
kind of attributes is not part of a form, the designer has to
specify a name as well as the set of links that relate to this

Query Evaluator.
As described in Section 4, once a map
is built, navigation expressions are automatically generated.
This process requires a simple traversal of the navigation
map, and thus can be done in linear time in the size of the
map. Individually, each expression can be seen as a shortcut
to retrieve data from a Web site. Instead of filling forms and
following links, one can simply specify a set of attributes
and execute the appropriate navigation expression (e.g., for
the query SELECT make,model,yeal;price,contact WHERE
make=ford AND model=escort), execute newsday{ford,estort, Year;Price, Contact) (described in Figure 4). It is worth
pointing out that as a byproduct, the process of retriev.ing
such data is made faster since during the execution of a
navigation process no extraneous objects such as figures and
Java animations are retrieved.
Navigation expressions are processed by the Transact.lon
F-logic interpreter, which translates them into logic programs that are executed by a deductive engine, the XSB
system.g On top of XSB, we use the HTTP library provicled
by PiLLoW to follow links, submit forms and retrieve documents from the Web.

‘Since building maps is anincremental process, our tool checks whether
actions and Web page object,5 are new before adding them to a map.
8h~p://www.clip.dia.fi.upm.es/Software/pillow/pillow.htm~

500

even greater degree of programming expertise, it is not
designed to be used by a programmer. Instead, navigation
expressions are generated automatically from the map.
Web information integration systems [20, 5, 2, lo] arc
more closely related to our work in that they try to present
the Web through a unified database interface. The Araneus
project [5] provides a rich model (ADM) to describe both
the topology and the contents of Web sites. Their concept
of the ADM scheme is analogous in many respects to our
navigation maps. Navigation processes to populate database
views are expressed in a newly developed declarative algebra, called Ulixes. Ulixes is intended to be used by a
database designer to create Web views for the end user. In
contrast to this, we use a well-known, existing formalism
(Transaction F-logic [12]), which functionally is a superset of Ulixes. However, as mentioned earlier, it is not intended to be used by a designer or an end user. Instead,
navigation expressions that use this language are generated
automatically from the navigation map. The interpreter than
simply executes these expressions when user queries need
to be evaluated. This is possible in our architecture due to
the clear separation between the VPS and the logical layers of the database and also due to the use of our processoriented object model. It is worth pointing out that maintenance of navigation expressions in our approach is much
simpler, since the navigation maps from which the processes
are generated, can be updated semi-automatically (through
mapping by example).
Ariadne [15] is a system for extracting and integrating
data from semi-structured Web sources. Ariadne has two
foci: data extraction from unstructured Web pages and what
in our architecture amounts to mapping from the logical
layer to the virtual physical layer. Both of these issues
are orthogonal to our work. For instance, Ariadne’s data
extraction facilities as well as the body of techniques for
extracting information from semi-structured data [31] could
be used in our system.
From the perspective of our architecture, the focus of
the work in the Information Manifold (IM) [20, 191 project
can be viewed as mapping the logical layer to VPS. IM
approaches the problem by first specifying the reverse
(physical-to-logical) mapping, which they call source description. The required logical-to-physical mapping is then
generated automatically. The benefit of this indirect approach is claimed to be the ease of maintenance of the
logical-to-physical mapping in view of adding or deleting
the Web sources. In this way, IM is complementary to our
work, since our focus is on building the VPS and the conceptual layers of the webbase.
There is a large body of work on information mediators,
such as TSIMMIS [IO], Hermes [I] and Garlic [30], which
help smooth the semantic and syntactic differences between
heterogeneous information sources. Techniques developed
for information integration systems such as these can be used
in our architecture for semantic integration of VPS relations

In order to combine information from different sites
(or maps), the attribute names and their domains must be
standardized. In our current implementation, one must
manually specify these mappings. If a mapping is not
provided for a certain attribute name, we employ fuzzy
matching techniques, which evidently are not full-proof and
may lead to errors. We intend to incorporate techniques from
mediator systems such as [ 10,301 to address this problem.
We have built navigation maps for a number of sites.
To give an idea of the complexity of the sites and query
execution times, below we show the number of pages
navigated and (some of the best) evaluation times for the
query SELECT make,model,yeal;price WHERE make=ford
AND model= escort over 10 car-related sites. These
timings lo indicate that to ensure acceptable response times
when querying a large number of sites, we may need to use
techniques such as parallelization and caching. It is worth
pointing out that a significant portion of the time in querying
is spent not only in fetching, but also parsing the Web pages.
We believe these times can be greatly improved if a faster
uarser is used.
I

AutoConnect
Newsdav
YahooCars
Kelly’s

8

I

,

3
I 4
5
5

1 7.26
I 1.01
3.12
2.54

1 14.70
I 4.77
10.48
7.63

Related Work

The problem of retrieving data from and querying Web
sources has received considerable attention in the database
literature (see [8] for a survey). Managing information on
the Web encompasses several tasks that include locating interesting data, modeling Web sites, extracting and integrating related information from multiple sites.
Web query languages such as W3QL [16], WebSQL [3],
WebLog [17], and Florid [9] address the problem of finding
and retrieving data from the Web. They improve on search
engines by combining textual retrieval with structure and
topology-based queries. These languages view the Web
as a collection of unstructured documents organized as a
graph, and users can declaratively express how to navigate
portions of the Web to find documents with certain features.
Conceptually, these languages are equivalent to various
More importantly,
subsets of our navigation calculus.
however, these are fairly sophisticated query interfaces
designed to be used by a fairly sophisticated user. In
contrast, even though our navigation calculus requires an
loThe times were collected on a Sun Ultra workstation, with dual 330
MHz processors, I GB of memory, and Solaris 5.6 operating system.

501

-I91- J. Frohn. R. Himmeroeder, P.-Th. Kandzia. G. Lausen.

that come from different sources. On the other hand, these
systems could use our VPS automation techniques to gain
access to dynamic Welb content.
Finally, ‘we should lnote the growing commercial interest
in integration of information from diverse Web sources (e.g.,
Junglee, Center Stage [11,28]). Techniques described in this
paper can facilitate rapid development of such services.

9

and C. Schlepphorst.
Florid - a prototype for F-logic:
In 12th German Workshop on Logic Programming, 1997.
http:Nwww.informatik.uni-freiburg.de/idbis/floridf.
[lo] H. Garcia-Molina, Y. Papakonstantinou, D. Quass, A. Rajaraman, Y. Sagiv, J. D. Ullman, V. Vassalos, and J. Widom.
The tsimmis approach to mediation: Data models and languages. Journal of Intelligent Information Systems, 8(2): 117132, 1997.
[l l] http://www.junglee.com. Junglee Corporation.
[ 121 M. Kifer. Deductive and object-oriented data languages: A
quest for integration. In Proc. of DOOD, pages 187-212,
1995.
131 M. Kifer, W. Kim, and Y. Sagiv. Querying object-oriented
databases.In Proc. of SIGMOD, pages 393-402, 1992.
141 M. Kifer, G. Lausen, and J. Wu. Logical foundations of
object-oriented and frame-based languages. Journal of ACM,
42:741-843, July 1995.
[15] C.A. Knoblock, S. Minton, J.L. Ambite, N. Ashish, P.J. Modi,
I. Muslea, A.G. Philpot, and S. Tejada. Modeling web sources
for information integration. In Proc. of AAAI, 1998.
[16] D. Konopnicki and 0. Shmueli. W3QS: A query system for
the World-Wide Web. In Proc. of VLDB, pages 54-65, 1995.
[17] L.V.S. Lakshmanan, F. Sadri, and IN. Subramanian. A
declarative language for querying and restructuring the WEB.
In Workshop on Research Issues in Data Engineering, pa.ges
12-21, 1996.
[18] S. Lawrence and C.L. Giles. Searching the world wide web.
Science, 280(4):98-100, 1998.
[19] A.Y. Levy, A. Rajaraman, and J.J. Ordille. Query-answering
algorithms for information agents. In Proc. of AAAI, pa.ges
40-47, 1996.
[20] A.Y. Levy, A. Rajaraman, and J.J. Ordille.
Querying
heterogeneous information sources using source descriptions.
In Proc. of VLDB, pages 251-262, 1996.
[21] D. Maier. The Theory of Relational Databases. Computer
Science Press, 1983.
[22] D. Maier, D. Rozenshtein, and D.S. Warren. Windows on the
world. In Proc. of SIGMOD, pages 68-78, 1983.
[23] D. Maier and J.D. Ullman.
Maximal objects and the
semantics of universal relation databases. ACM TO.DS,
8(1):1-14, 1983.
[24] D. Maier, J.D. Ullman, and M.Y. Vardi. On the foundations
of the universal relation model. ACM TODS, 9(2):283-3’08,
1984.
[25] G. Mecca, P. Atzeni, A. Masci, P. Merialdo, and G. Sindoni.
The araneus web-base management system. In Proc. of
SIGMOD, pages 544-546, 1998.
[26] G. Mecca, P. Atzeni, A. Masci, P. Merialdo, and G. Sindoni.
From databases to web-bases: The araneus experience.
Technical Report n. 34-1998, May 1998.
[27] A.O. Mendelzon, G.A. Mihaila, and T. Milo. Querying the
World Wide Web. International Journal on Digital Libraries,
1(1):54-67, 1997.
[28] http://www.ondisplay.com. OnDisplay Corporation.
[29] A. Rajaraman, Y. Sagiv, and J.D. Ullman. Answering queries
using templates with binding patterns. In Proc. of PODS,
pages 105-l 12, 1995.
[30] M.T. Roth, M. Arya, L.M. Haas, M.J. Carey, W.F. Cody,
R. Fagin, P.M. Schwarz, J. Thomas II, and E.L. Wimm~ers.
The garlic project. In Proc. of SIGMOD, page 557, 1996.
[31] D. Suciu, editor.
Proc. of the SIGMOD Workshop on
Management of Semistructured Data, 1997.

Conclusions and Future Directions

In this paper we described a layered architecture for designing webbases. The separation of layers, which is analogous to traditional databases, simplifies the creation, maintenance, and use of webibasesfor retrieving information available on the Web. We have implemented the main components of a prototype implementation of our architecture and
reported on some preliminary experimental results.
We have shown that navigation maps can be created semiautomatically as the webbase designer browses sites, and
that navigation expressions can be automatically derived
from these maps. These expressions are executed when
evaluating a query, and thus optimizing such expressions is
an important problem that needs to be studied.
Our experiments suggest that parallelization of query
evaluation is crucial for obtaining acceptable response times.
Finally, while the idea of structured UR as a query interface
seems to be promising in the context of webbases, more experimental work needs to be done to evaluate the practicality
of the idea.
Acknowledgements: We would like to thank various people that contributed to this work: Vinod Anupam for suggestions on how to imple.ment navigation by example; Daniel
Lieuwen and C.R. Ramakrishnan, for carefully reading this
manuscript; and Narain Gehani, David Warren and Guizhen
Yang for valuable discussions.

References
[l] S. Adali, K. Canda-n, Y. Papakonstantinou, and VS. Subrahmanian. Query caching and optimization in distributed mediator systems. In Proc. of SIGMOD, pages 137-148, 1996.
[2] J.L. Ambite, N. AshJsh, G. Barish, CA. Knoblock, S. Minton,
P.J. Modi, I. Muslea, A. Philpot, and S. Tejada. Ariadne:
A system for constructing mediators for intemet sources. In
Proc. of SIGMOD, 1998.
[3] G. Arocena, A. Mendelzon, and G. Mihaila. Applications of a
Web query language. In Proceedings of the 6th International
WWW Conference, April 1997.
[4] P. Atze.ni, G. Mecca, and P. Merialdo. Semistructured und
structured data in the web: Going back and forth. SIGMOD
Record, 26(4):16-23, 1997.
[S] P. Atzeni, G. Mecca, and P. Merialdo. To weave the web. In
Proc. of VLDB, pages 206-215, 1997.
[6] A.J. Bonner and M. Kifer. An overview of transaction logic.
Theoretical Computer Science, 133:205-265, October 1994.
[7] O.M. Duschka and A.Y. Levy. Recursive plans for information gathering. In Proc. of IJCAI, 1997.
[8] D. Florescu, A.Y. Levy, and A.O. Mendelzon. Database
techniques for the world-wide web: A survey. SIGMOD
Record, 27(3):59-74, 1998.

502

WWW 2012 – Demos Track

April 16–20, 2012, Lyon, France

Partisan Scale
Sedat Gokalp

Hasan Davulcu

Computer Science
Arizona State University

Computer Science
Arizona State University

Sedat.Gokalp@asu.edu

Hasan.Davulcu@asu.edu

ABSTRACT

ber of ’Yea’ and ’Nay’ votes are recorded, except for the roll
call votes. According to The Library of Congress1 ,

US Senate is the venue of political debates where the federal bills are formed and voted. Senators show their support/opposition along the bills with their votes. This information makes it possible to extract the polarity of the
senators. We use signed bipartite graphs for modeling debates, and we propose an algorithm for partitioning both
the senators, and the bills comprising the debate into binary opposing camps. Simultaneously, our algorithm scales
both the senators and the bills on a univariate scale. Using
this scale, a researcher can identify moderate and partisan
senators within each camp, and polarizing vs. unifying bills.
We applied our algorithm on all the terms of the US Senate
to the date for longitudinal analysis and developed a web
based interactive user interface www.PartisanScale.com
to visualize the analysis.

A roll call vote guarantees that every Member’s
vote is recorded, but only a minority of bills receive a roll call vote.
The current political party system in the United States
is a two-party system, which suggests a bipolar nature for
both the senators and the bills; such that, there exists two
polarized camps of senators that oppose each others views,
and two sets of bills that polarize the senators. It can be
presumed that these camps would purely split according to
the political parties of the senators, or the political parties
of the sponsors of the bills. Although this is true to a certain
extent, our analysis show that the actual behaviours can be
different for a minority.
Senators show their support/opposition along the bills
with their votes. This information makes it possible to extract the polarity of the senators. We use signed bipartite
graphs for modeling the opposition, and we used our previous work ANCO-HITS algorithm for partitioning both the
senators, and the bills into two polarized camps. Simultaneously, our algorithm scales both the senators and the bills
on a univariate scale. Using this scale, a researcher can identify moderate and partisan2 senators within each camp, and
polarizing vs. unifying bills.
Partitioning and scaling help a researcher to better understand the structure of political debates in the Senate.
While partisan ends of a scale may represent senators with
irreconcilable viewpoints, moderate senators may represent
viewpoints that are more amenable to engage in a constructive dialog through a set of unifying issues. Moderates may
sympathize with some of the claims and grievances of the
other side. Longitudinal analysis using our proposed algorithms could reveal interesting dynamics, such as, moderates
from opposing camps could be in the process of forming a
coalition by making the necessary compromises to reach a
consensus.
Major contributions of this paper are: (1) a modification
of our previous algorithm ANCO-HITS, to propagate the
scores on a signed bipartite graph to solve the partitioning and scaling problems described above; (2) applying the
algorithm on 112 terms of the US Senate for longitudinal
analysis; (3) developing a web based interactive user interface to visualize the analysis.

Categories and Subject Descriptors
H.2.8 [Database applications]: Data mining;
H.3.3 [Information Search and Retrieval]: Clustering

General Terms
Algorithms

Keywords
Community discovery, Link Analysis, Partitioning, Ranking,
Scaling, HITS, Signed Bipartite Graphs, Spectral Clustering

1.

INTRODUCTION

The United States has a bicameral legislature that comprises the US Senate as the upper house, and the US House
of Representatives. The terms of the US Senate last for two
years, and the senators serve three terms (six years) each.
The terms are staggered in such a way that approximately
one-third of the seats are up for election every two years.
The Senate meets in the United States Capitol in Washington, D.C. to form and debate on motions, or bills. When
debates conclude, the bill in question is put to a vote, where
senators respond either ’Yea’ (in favor of the bill) or ’Nay’
(against the bill). For most of the bills, only the total numCopyright is held by the International World Wide Web Conference Committee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012 Companion, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1230-1/12/04.

1

http://thomas.loc.gov/home/rollcallvotes.html
Partisanship can be defined as being devoted to or biased
in support of a party.
2

349

WWW 2012 – Demos Track

u1 u2 u3

0

April 16–20, 2012, Lyon, France

um-22um-1 um

X

vn-2 vn-1 vn Y

v 1 v2 v3

Figure 3: Vote matrix for the 111th US Senate after
scaling with ANCO-HITS

Figure 1: Perfectly polarized bipartite graph
u1

0

u2

0

X

Y

X

ui and a bill vj represents support, and a red dashed line
represents opposition.
Figure 2 shows an example of two senators; u1 being extreme and u2 being more moderate. u1 supports the bills
of same polarity, and opposes the vertices of the opposite
polarity. However, u2 has mixed support and opposition.
Same relation holds between polarizing and unifying bills.
Although partitioning algorithms can be utilized to detect
the polarity of senators and bills, it is not possible to distinguish partisans from moderates. Scaling overcomes this
problem and makes it possible to compare two senators of
same polarity. In this paper, we are not only able to compare pairs of senators, but also provide the exact locations
on the scale, therefore providing valuable information about
the shape of the distribution as well.

Y

Figure 2: Partisan vs. Moderate senators

2.

PROBLEM FORMULATION

There are many applications [5, 4, 7, 1, 3] for recognizing
political orientation, and bipartite graphs [2, 6, 8] have been
widely used to represent relationships between two sets of
entities. We use bipartite graphs to model the relationships
between the senators and the bills. We use signed edges to
represent the votes, where positive edges denote support,
and negative edges denote opposition on a bill by a senator.
Given

3.

ANCO-HITS

In this study, we used a modified version of our previous
work ANCO-HITS. Algorithm 1 describes the steps of the
ANCO-HITS algorithm for the co-scaling problem.

• G = (U ∪ V, A) is a bipartite graph consisting of senators U and bills V , and a signed vote matrix A
• U = {u1 , u2 , . . . , um }, a set of m senators

Data: Signed vote matrix A
Result: Scale vectors X and Y
Initiate X <0> = (1, 1, . . . , 1) ;
Initiate Y <0> = (1, 1, . . . , 1) ;
repeat
Update X;
Update Y ;
until X vector converges;
Algorithm 1: Iterative update procedure for ANCO-HITS

• V = {v1 , v2 , . . . , vn }, a set of n bills
• A ∈ Rm×n , where aij represents the vote of senator ui
on bill vj
Find
• X = (x1 , x2 , . . . , xm ), where xi ∈ R is the assigned
value of the senator ui
• Y = (y1 , y2 , . . . , yn ), where yj ∈ R is the assigned value
of the bill vj

This research uses a different normalization scheme than
the original ANCO-HITS algorithm. The update functions
for X and Y are modified such that the vectors X and Y
would converge not only in direction, but also in value.

such that
• xi value for a senator ui should be closer to the yj
values of the bills that he supports, and further away
from the yk values of the bills that he opposes. The
magnitude of xi denotes the partisanship of the senators ui , and the magnitude of yj denote how polarizing
the bill vj is. i.e. magnitudes closer to 0 meaning more
moderate and larger magnitudes meaning more partisan.

n
P

x<k>
i

=

j=1
n
P

|aij yj<k−1> |

j=1

m
P

aij yj<k−1>
yj<k>

=

aij x<k>
i

i=1
m
P

(1)

|aij x<k>
|
i

i=1

The convergence values for X and Y vectors will satisfy
−1 ≤ xi , yj ≤ +1.
Figure 3 represents the bipartite graph of the 111th US
Senate data after scaling both the senate and the bills with
ANCO-HITS. The light green colored edges represent ’Yea’
votes, and dark red represents ’Nay’ votes. Similar to our
motivating Figure 1, this figure also shows partisan behavior
in the 111th US Senate.

Figure 1 depicts a perfectly polarized bipartite graph. The
two axes X and Y represent the univariate scale for the
senators and bills. The vertices to the right of zero have
positive values, and the vertices to the left have negative
values on the scale. A green solid line between a senator

350

WWW 2012 – Demos Track

April 16–20, 2012, Lyon, France

Figure 4: A screenshot from PartisanScale.com showing the partisanship history for a senator

4.

INTERACTIVE USER INTERFACE

The US Congress has been collecting data since the very
first congress of the US history. This data has been encoded as XML files and publicly shared through the govtrack.us project3 . We collected the roll call votes of the
US Senate for the terms 1 through 112, covering the years
1789-2011. We ran the ANCO-HITS algorithm for each individual term. The sign of the ANCO-HITS values are arbitrary; therefore, we aligned consecutive terms by mirroring
the scale if necessary. By analyzing more than 3,000,000
votes, we produced the web based interactive user interface
www.PartisanScale.com that allows the users to navigate
through the history of the US Senate.
Figure 4 shows a screenshot of the user interface. Each
term of the senate is shown as a column in the figure. The
top row shows the terms and the years for each senate with
the incumbent US president shown below. The senators are
represented by boxes which are colored according to their
political parties.
The vertical axis of the scale represents the bipolar nature
of the US Senate. The polarity of each senator is represented
by the location of each box. The dashed line shows the
zero point. Senators around this point are calculated to be
moderate, and the senators away from the dashed line are
calculated to be more polarized. Hovering along these boxes
will show the picture, the political party, and the amount of
partisanship for the senator in focus. Clicking on the scale
will further filter the figure to show the partisanship history.
This filtering can also be done with the quick search tool on
the top right corner. The auto-completion feature will help
the users easily select the senator.
For example, Figure 4 shows a senator that is calculated
to be moderate for the 110th term. It can be seen that this
senator was first elected in 1981 and served for 15 terms until
the year 2010. It also shows us that after 12 terms of service
as a republican, he switches membership to the Democratic
Party for the last 3 terms of his service.
3

Figure 5: Longevity of service
An introductory screencast video that shows the usage of
the system can be found on the website.

5.

STATISTICS

Figure 5 shows the histogram for the number of terms each
senator served. The average number of terms the senators
served is 4.68, and the longest run is 26 terms.
Figure 6 shows the partisanship displacement distribution
for three ∆T values on a semi-log scale. Partisanship displacement is defined as the absolute distance of partisan
scale values for a senator between two terms T1 and T2 .
C∆T (d) is the number of displacements ≥ d between any
two terms T1 and T2 satisfying ∆T = T1 − T2 .
This figure shows three plots of C∆T values for ∆T = 1,
∆T = 2 and ∆T = 3. It can be clearly seen that the plots
on the semi-log scale form a linear function, which suggests
an exponential distribution.
Figure 7 aggregates the party polarities. The mean partisanship values of the senators from each party is shown as a
solid line. The shaded areas show 1 standard deviation along
the mean for each term. This figure is helpful to identify the
times of partisan politics within the US Senate.

http://www.govtrack.us/data

351

WWW 2012 – Demos Track

April 16–20, 2012, Lyon, France

Figure 7: Aggregated Party Partisanship

[3]

[4]

[5]

Figure 6: Partisanship displacement distribution

6.

[6]

CONCLUSIONS

In this paper, we introduced a measure for partisanship,
and applied it on 112 terms of the US Senate for longitudinal
analysis. We further developed an interactive user interface
www.PartisanScale.com to visualize the analysis. The
data set and the algorithm in source code are available online.

7.

[7]

[8]

ACKNOWLEDGMENTS

This research was supported in part by US DOD Minerva
Research Initiative grant N00014-09-1-0815.

8.

REFERENCES

[1] M. Bansal, C. Cardie, and L. Lee. The power of
negative thinking: Exploiting label disagreement in the
min-cut classification framework. Proceedings of
COLING: Companion volume: Posters, pages 13–16,
2008.
[2] H. Deng, M. Lyu, and I. King. A generalized co-hits
algorithm and its application to bipartite graphs. In
Proceedings of the 15th ACM SIGKDD international

352

conference on Knowledge discovery and data mining,
pages 239–248. ACM, 2009.
W. Lin and A. Hauptmann. Are these documents
written from different perspectives?: a test of different
perspectives based on statistical distribution
divergence. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 1057–1064. Association for
Computational Linguistics, 2006.
R. Malouf and T. Mullen. Graph-based user
classification for informal online political discourse.
2007.
T. Mullen and R. Malouf. A preliminary investigation
into sentiment analysis of informal political discourse.
In AAAI symposium on computational approaches to
analysing weblogs (AAAI-CAAW), pages 159–162, 2006.
M. Rege, M. Dong, and F. Fotouhi. Co-clustering
documents and words using bipartite isoperimetric
graph partitioning. In Data Mining, 2006. ICDM’06.
Sixth International Conference on, pages 532–541.
IEEE, 2006.
M. Thomas, B. Pang, and L. Lee. Get out the vote:
Determining support or opposition from congressional
floor-debate transcripts. In In Proceedings of EMNLP,
pages 327–335, 2006.
H. Zha, X. He, C. Ding, H. Simon, and M. Gu.
Bipartite graph partitioning and data clustering. In
Proceedings of the tenth international conference on
Information and knowledge management, pages 25–32.
ACM, 2001.

METEOR: Metadata and Instance Extraction from Object
Referral Lists on the Web
Hasan Davulcu, Srinivas Vadrevu, Saravanakumar Nagarajan, Fatih Gelgi
Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85287-5406, USA
{hdavulcu,svadrevu,nrsaravana,fagelgi}@asu.edu

ABSTRACT

instance links to an individual faculty home page with detailed information. We denote these type of groups as object referral lists
(ORL). Examples of ORLs are faculty listings in the universities,
job listings in company sites, course listings in online schools, hotel
and hospital listings in directories etc. Object referral lists follow a
highly regular linkage pattern; first they list their instances under an
informative label such as jobs, f aculty, hotels etc. and then each
instance links to an individual detailed object page. The individual
object page presents the detailed attributes of an object as shown
in the second page of Figure 1. Sometimes the individual object
page may present its attribute information by linking to an object
attribute page as shown in the third page of Figure 1. In this paper
we present algorithms that can interpret any given ORL to navigate
to its instances and extract their attributes and values.

The Web has established itself as the largest public data repository
ever available. Even though the vast majority of information on the
Web is formatted to be easily readable by the human eye, “meaningful information” is still largely inaccessible for the computer
applications. In this paper we present the METEOR system which
utilizes various presentation and linkage regularities from referral
lists of various sorts to automatically separate and extract metadata
and instance information. Experimental results for the university
domain with 12 computer science department Web sites, comprising 361 individual faculty and course home pages indicate that the
performance of the metadata and instance extraction averages 85%,
88% F-measure respectively. METEOR achieves this performance
without any domain specific engineering requirement.
Categories and Subject Descriptors: H.4.m [Information Systems]: Miscellaneous; I.2.6 [Artificial Intelligence]: Learning–
Knowledge Acquisition

2.

General Terms: Algorithms, Performance, Experimentation
Keywords: Web, Semantic, Metadata, Object, Instance, Extraction

1.

OVERVIEW OF THE APPROACH

The METEOR system utilizes Semantic Partitioner that infers
hierarchical relationships among the leaf nodes of the DOM (Document Object Model) tree of a Web page, where all the document
content is stored. Semantic partitioner achieves this through a sequence of two operations: hierarchical grouping and promotion.
The hierarchical grouping is based on a regular pattern mining algorithm which yields a hierarchy of groups (G) and their instances
(I). After hierarchical grouping, all the content of the Web page is
still at the leaf nodes of the hierarchical group tree and hence promotion of some of the leaf nodes is necessary in order to organize
them into a semantic hierarchy. The promotion algorithm identifies
those leaf nodes that should be promoted above their siblings.
Next the METEOR system interprets these semantic structures
by utilizing linkage regularities that exist within the context of an
ORL in order to separate and extract their metadata and instances.
The extracted metadata and object instances are represented as Flogic [2] facts. The interpretation proceeds for the group structures
in the individual object page by providing the appropriate context.
Whenever, the object instance pages present their attributes using a
link group, each group structure within the object attribute pages is
interpreted and corresponding value types are extracted.

INTRODUCTION

Scalable information retrieval [1] based search engine technologies have achieved wide spread adoption and commercial success
towards enabling access to the Web. However, since they are based
on an unstructured representation of the Web documents their performance in making sense of the available information is also limited.
Thanks to the HTML format, unlike plain text documents, Web
pages organize and present their content within nested hierarchies
of HTML structures. In this paper we present an algorithm that
can detect various HTML regularities [3] and utilize them to structure the Web page content itself into hierarchical group structures
which contains blocks of highly regularly presented instances.
Furthermore, many Web pages present their information in the
form of labeled lists and tables of various sorts. Consider the first
page in the example shown in Figure 1 that lists the faculty instances in a computer science department. Each of these faculty

3.

AN ILLUSTRATIVE EXAMPLE

We illustrate the inner-workings of the Hierarchical
Partitioning
Algorithm
using
the
sequence
aaabcdef ef f ghijhikhijhilhijhikmnnonmpnnqrq
from
the individual object page in Figure 1 and explain the process.
The HiearchicalGrouping algorithm attempts to standardize
the path sequence as a regular expression which can be used to
parse the original sequence into hierarchical group structures,
presented in Figure 2. It utilizes the maximize subroutine that

Copyright is held by the author/owner.
WWW 2005, May 10–14, 2005, Chiba, Japan.
ACM 1-59593-051-5/05/0005.

1180

Figure 1: An example of the object referral list page, that links individual object pages. The figure shows an individual object page and
one of its object attribute pages. The labels in individual object page and the object attribute page are marked with corresponding
path identifier symbols.
structure presents the members of the ‘Faculty’ concept and its
value types are found by the Hierarchical Grouping algorithm, the
following F-logic statements are extracted from the ORL group G.
‘Regular Faculty’ : concept.
‘Alex Aiken’ : ‘Regular Faculty’.
‘Daniel Moore’ : ‘Regular Faculty’. ...
‘Regular Faculty’[‘Name’ ⇒
⇒ {‘Alex Aiken’,...}].
‘Alex Aiken’[‘Name’ →
→ ‘Alex Aiken’].
‘Regular Faculty’[‘Phone’ ⇒
⇒ {‘5-3359’, ‘3-3334’, ...}].
‘Alex Aiken’[‘Phone’ →
→ ‘5-3359’]. ...
Figure 2: The group structures for the individual object
page and the object attribute page in Figure 1. The groups
G1 , G2 , G3 , G4 , G5 , G6 correspond to the navigation bar, affiliations, publications, address, the telephone information and the
courses respectively.

4.

CONCLUSIONS AND FUTURE WORK

In this paper, we presented the METEOR system that can automatically separate and extract metadata and instance information
from object referral lists. The experimental results indicate that the
METEOR system was able to extract the metadata and the instance
information with high accuracy. In our future work, we propose
to develop automated algorithms for finding all the ORL structures
within any Web site.

finds the maximum number of consecutive atoms that can be
appended to the current atom. For example, initially the subpattern
(a)∗ is computed using the maximize subroutine by recursively
invoking the HieararchichalGrouping algorithm. This pattern
corresponds to the regularly presented structure, the navigation
bar of the individual object page shown as G1 in Figure 2. Then
the algorithm continues to find the subpatterns bcd(e(f )∗)∗,
g(hij(hik|hil|hik))∗ that corresponds to the affiliations and the
publications presented in the page shown as G2 and G3 , and
appends to the previous subpattern (a)∗. The algorithm eventually
generates nested group structures presented in Figure 2 from the
final pattern. The complexity of the algorithm is O(n3 ) where n is
the length of the input string.
For example, consider the ORL group structure G in Figure 1
that lists instances of the Regular Faculty concept. As the group

5.

REFERENCES

[1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information
Retrieval. Addison Wesley, 1999.
[2] M. Kifer, G. Lausen, and J. Wu. Logical foundations of
object-oriented and frame-based languages. Journal of the
ACM, 42(4):741–843, July 1995.
[3] G. Yang, W. Tan, S. Mukherjee, I.V.Ramakrishnan, and
H. Davulcu. On the power of semantic partitioning of web
documents. In Workshop on Information Integration on the
Web, Acapulco, Mexico, 2003.

1181

IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust

ImpactRank: A Study on News Impact
Forecasting
Sukru Tikves
Computing, Informatics, and Decision Systems Eng.
Arizona State University
Tempe, Arizona 85281
Email: Sukru.Tikves@asu.edu

Hasan Davulcu
Computing, Informatics, and Decision Systems Eng.
Arizona State University
Tempe, Arizona 85281
Email: Hasan.Davulcu@asu.edu

Abstract
In this paper we developed a framework and a measure for news impact
forecasting. We proved the viability of our impact forecasting approach
using a SVM based forecaster on six months of NYT corpus - consisting of
16,852 articles. We experimented with different feature selection and ranking
algorithms including standard frequency based methods, as well as a new
method named ImpactRank. Our ImpactRank based forecaster performed
as the best feature ranking technique while providing a graph suitable
for browsing and identifying the most influential topics, entities and interrelationships going into its impact predictions.

Fig. 1. Snapshot of an Example Event Thread

1. Introduction
and (ii) Topic Tracking to relate incoming news stories with
the related past stories. In Figure 1 we present a snapshot of
the event thread of “train bombings in London” which contains
the initial and follow-up coverage.
In Section 3 we propose a definition for an impact value
measure for news articles, providing us with a baseline to (i)
assign partial impact values to all articles up to a certain date,
(ii) experiment with different feature selection and ranking
algorithms to generate feature vectors for news articles, (iii)
train a predictive classifier, based on Support Vector Machines
utilizing feature vectors of articles and their partial impact
scores, and (iv) generate a gold standard of impact measures
(with look ahead) to measure the overall accuracy of the SVM
classifier [12], [13] for predicting the impact of incoming news
based on information gleaned from the past.
We experimented with different feature selection and ranking algorithms (including standard frequency-based tf, tf-idf
methods), as well as a new ImpactRank network model,
which is based on the TermRank algorithm introduced by
Gelgi [14]. Based on our experiments, this eigen-vector based
new measure performed as the best feature ranking technique,
and the corresponding ImpactRank network model served as
the best method for identifying influential topics, entities and
inter-relationships mentioned within an article to explain its
predicted impact.
Next Section presents the overall design of the impact
estimation procedure. Section 3 formally defines the impact
measure. Section 4 provides the details of the SVM based
impact forecasting. Section 5 introduces and examines the
ImpactRank network model and the TermRank algorithm for

In this paper we worked on measuring and forecasting the
impact of news events as they occur on a timeline. An “event”
is something that happens at some specific time and at a
place[1], e.g. “train bombing in London on July 7th”.
In order to forecast the impact of incoming news events, we
rely on a model based on partial impact calculations of past
news. Partial impact calculations are based on similarity relationships among news, and the events and entities mentioned
within them.
Relevant research on ranking of the impact of conference
papers rely on citation information among scholarly works.
Unlike news, citation information among scholarly publications might be readily available or can be extracted from their
text[2].
Bergstrom introduced eigenfactor, which calculates impact
factor scores for journals and other scholarly publications[3]
based on their citation graphs. This followed Garfield’s early
work on impact factors[4], which was criticized by [5], [6], [7],
since it only relied on one-level immediate citations. ArnetMiner group used a hybrid model for their ranking system[8].
Similarly, in order to measure the quality of the scientific
output of an individual researcher, Hirsch[9] proposed the
popular h index.
While citation information is readily available in the scholarly publications domain, in the news domain we needed to
identify events mentioned in news and rely on similarities
among their topics and entities to identify their impact. Fortunately, previous TDT research[10], [1], [11] provides us with
the tools and techniques for both (i) First Story Detection
(FSD) to identify if a news article is talking about a new story,
978-0-7695-4211-9/10 $26.00 © 2010 IEEE
DOI 10.1109/SocialCom.2010.77

488

1 MONTH

ESTIMATION
2 WEEKS

TRAINING DATA
1 MONTH

+1
WEEK

TRAINING DATA
1 MONTH

+1
WEEK

ESTIMATION
2 WEEKS

+1
WEEK

Towards an impact measure, first we start with a measure
of follow up length, which will be used in the definition of
the impact itself:
Definition 1: Follow up length of an article is defined as
the number of news articles that (i) chronologically follow
that article, and (ii) lies within a predetermined perimeter of
similarity.
Although the above definition is generic, in this paper we
will use cosine similarity[15] as the distance metric, and an
experimentally determined similarity threshold discussed in
Section 6.1.3.
Using the follow up length, it’s now possible to define the
impact score of a news article:
Definition 2: The impact score of a news article is defined
as the logarithm of the follow up length of that article, in base
10, restricted into the [0, 3] range.
For an article a:

EVALUATION

ESTIMATION
2 WEEKS

EVALUATION

EVALUATION

...
TRAINING DATA
1 MONTH

+1
WEEK

+1
WEEK

… REST

ESTIMATION
2 WEEKS

EVALUATION

Fig. 3. Continuation of the Evaluation Loop

feature ranking. Section 6 describes the experimental setup,
and in Section 7 we present experimental results. Section 8
concludes the paper.

Impact(a) = min(log10 (F ollowU p(a) + 1), 3)

(1)

Employing follow up length allows us to have a measurable
follow up calculation for impact definition. The choice of logarithmic scale introduces a means resistant to small variations
in larger follow up configurations. It builds upon order of
magnitude, where impact score around 1 expressing a follow
up in tens-of-articles, 2 in hundreds, and 0 indicating singleton
events. A limit of 3 in impact score assumes all events having
follow up size of thousand or more articles implies highest
impact.

2. Impact Estimation Procedure
Our architecture for investigating the news impact problem
consists of two main components: The first component, runs
offline through the entire sequence of news articles, computes,
and stores the gold standard values of their impact scores.
(in Section 6.1.1). The second component simulates a real
time loop, by continuously testing various impact estimation
methods with increasing amount of data, and enables us to
assess their accuracies.
In the initial step of the loop, the estimator is supplied
with an input of one month of news data and asked to train
itself using partial impact scores calculated only using this
one month input. After the training is complete it’s then asked
to estimate the impact scores of the articles for the following
two weeks of data. The process for this step is illustrated in
Figure 2.
After the initial step, the rest of the dataset is added into
the input steam in weekly increments, the partial impact
scores are updated, the estimation model is re-trained with the
updated partial impact values, and the estimator performance
is evaluated for the next two following weeks of news data.
These progressive steps are illustrated in Figure 2.
Our choice of 1 month of initial training, 2 weeks of
estimations, and weekly increments of the data are not fixed,
but are chosen for practical results. Any other arbitrary choice
of ranges are possible.

4. SVM Based Forecasting
Support vector machines (SVM) have been effectively employed in text processing tasks[12], and they are the primary
choice in our system as the estimation algorithms, due to their
performance and versatility.
In our system, we used term frequency based feature vectors
as a baseline. In addition, better feature selection and ranking
methods will be presented in Section 4.1.
After each document was tokenized into a stream of words,
stop words were removed from that stream, along with any
one or two character long entries. The resulting list was then
converted into lower case, and the standard TF-formula (2)
was used to generate the document term vectors.
ni, j
tfi,j = P
k nk,j

(2)

Since we utilized SVM as a discrete learner, target values
for the impact scores were binned into 7 categories; each
representing a rounding to the nearest multiplier of 0.5. It
should be noted that, in case of the uniform distribution of
impact scores, this will introduce an expected minimum error
around 0.21 even for a perfect estimator.
Our preliminary experiments with standard frequency based
term-vectors demonstrated that the support vector machine
was indeed capable of learning a model of news impacts, and
improve incrementally during additional iterations.

3. Definition of the Impact Measure
The first challenge in our research is identifying a meaningful definition for measuring the impact of news articles. Since,
we don’t have explicit references between news articles, the
measure should rely on implicit relationships. Additionally, it
should be robust, and resistant to small variations.

489

Estimated
Scores

ESTIMATION ALGORITHM

ESTIMATION
PERIOD

TRAINING PERIOD

Performance
Evaluation

GOLD STANDARD w/
LOOK AHEAD

Actual Scores

Fig. 2. Initial Step of the Evaluation Loop

4.1. News Feature Selection and Ranking

tions. The main advantage of TermRank over traditional tdidf based ranking methods is its ability to distinguish among
discriminative, ambiguous and common terms by detecting
their various contexts and rank them accordingly as described
below:
• Discriminative terms typically strongly relate to a specific
high impact context. Topical keywords such as “bombing”, “scandals”, “beheadings” and named entities such
as “Saddam Hussein” belong to this category.
• Ambiguous terms tend to appear in many contexts, however their impact might vary depending on the strength
of their association with a certain context. For instance,
keywords such as “force” and “increase” may be found in
many contexts. Yet, whenever they are strongly associated
with a certain context, their impact should evolve along
with the impact of the context.
• Common terms usually appear in many contexts. Therefore, unlike ambiguous terms, common terms only have
weak connections with their contexts. Some examples of
common terms are “Washington” and “American”.
TermRank algorithm is based on a variation of
PageRank[16], [17]. However unlike web graphs, where
there are explicit directed links between nodes corresponding
to documents, TermRank works on textual term data,
where nodes correspond to terms, and by extracting their
relationships (i.e. co-occurrence) from the text. PageRank
operates on a directed graph where edges have no weights.
Whereas, TermRank works on undirected graphs with
weighted edges. Hence, all edges are considered to be both
incoming and outgoing. Since there are no rank sinks in
undirected graphs, a decay factor is not included in the
TermRank formula. Given a relationship graph G, TermRank
is calculated as follows:

In addition to the basic term frequency representation, which
was described in section 4, the following list of methods are
evaluated in our study:
• SVM-Basic The basic method in Section 4, which uses
term-frequency for representation, without ranking (i.e.
using all terms).
• SVM-TF An evolution of this method, which selects only
top K ranked keywords.
• SVM-TF (EV) Uses keywords as well as recognized
named entities, and term-frequency.
• SVM-TF-IDF (EV) An iteration of previous method,
which uses tf-idf in Formula (3), instead of termfrequency.
tf − idfi,j = tfi,j × log
•

|D|
|{d : d ∈ D ∧ ti ∈ d}|

(3)

SVM-Stable Feature ranking according to the inverse
of the variance of per-term impact values, as shown in
Formula (6), with term-frequency based vectors. The intuition for this method is based on the fact that 1/variance
captures the stability of the impact value of a term within
the document corpus.
Di0 = {d : d ∈ D, ti ∈ d}
P
d∈Di0 Impact(d)
impacti =
|Di0 |
|Di0 |
inv − vari = P
2
d∈D 0 (Impact(d) − impacti )

(4)
(5)
(6)

i

•

TermRank The TermRank algorithm is presented in the
following section.

T R(i) =

5. Term Rank Algorithm

X
j∈N (i)

T R(j).wij
P
k∈N (j) wjk

(7)

where N (i) represents the set of neighbors of the node i. The
essential difference in the formulas can be summarized as the

TermRank is an eigen-vector based ranking algorithm introduced by Gelgi[14] for use on web based document collec-

490

summation of the weights of edges instead of the number of
links.

1:
2:
3:
4:
5:
6:

5.1. ImpactRank: TermRank for Impact
We define a variation of TermRank graph that operates on
news articles’ impact values as follows:
ImpactRank = T ermRank(G(N, E))

7:
8:
9:
10:
11:

(8)

Where G is a graph with node set N corresponding to the
terms in the document corpus, and a weighted edge set E,
corresponding to term co-occurrence frequencies.
For the TermRank based feature ranking method, a combination of filtered keywords, and recognized named entities
were used as the nodes. Additionally sentence boundaries
determined the co-occurrence windows to establish the edges.
The normalized term co-occurrence frequencies were used
to set edge weights. Node weights were initialized with the
average impacts of the corresponding terms.
The evolution of the highest ranked terms in the ImpactRank
graph can be observed in Table 1. The first column corresponds
to the initial ordering of the top nine terms with respect to
their average impact scores, according to Formula (6), for
the network extracted for January 14, 2007. Terms starting
with the text “LOC/”, “ORG/” and “MSC/” refer to recognized named entities. The second column correspond to the
output of the TermRank iteration. It can be observed that,
many common and ambiguous terms have been replaced with
discriminative, and ambiguous but strongly associated terms.
The ORG/Congress term remains as a stable discriminative
term enjoying persistent highest ranking throughout 1/14 1/18 period. After each daily iteration, common terms such
as LOC/Washington and American gradually loses their initial
higher rankings (i.e. due to their initial higher frequency)
against ambiguous but strongly impact associated terms such
as troop and increase.

Fig. 4. Impact Score Calculation
for all incoming article ai do
f ollowupi ← 0
for all previous article aj , j < i do
if Distance(ai , aj ) < T hreshold then
f ollowupj ← f ollowupj + 1
end if
end for
end for
for all articles ai do
impacti = min(log(f ollowupi + 1, 10), 3)
end for

6.1.2. Online Evaluator. For each estimation method tested,
a simulated real time iteration is executed, providing us with
an evaluation mechanism. Initially, the forecasters are trained
with a month of news data. Then, in following iterations
another week of news data is introduced. In each iteration, the
forecasters are retrained, and they are asked to forecast the
impact scores of the following two weeks of articles. Then,
their predictive accuracy is compared against the gold standard
impact values and recorded.
A sample estimator implementation is shown figure 5. This
average based estimator is used as one of the baseline methods.
6.1.3. Similarity Metrics. The articles were first converted
into a representation of mixed term vectors, which go through
basic cleanup, and then enhanced with the inclusion of recognized named entities. As a similarity metric between two
vectors, cosine similarity[15] was used with an experimentally
determined threshold of 0.80.
6.1.4. Evaluation Metric. We employed a mean square error
based evaluation metric. This allows penalizing higher losses,
while minimizing the impact of small variations. Since the
system was evaluated empirically, residual sum of squares
divided by the number of articles were calculated according
to the Equation (9) as an approximation of the actual MSE
values.

6. Experimental Setup
6.1. Evaluation Process
The evaluation process was performed in two stages. In
the first stage an offline application was utilized to calculate
the impact scores with access to the entire data, generating a
gold standard for the news impact evaluation with complete
follow up information. The second stage simulated a real time
execution, by incrementally feeding weekly data to various
forecasting algorithms, and recording their performance for
plotting the evaluation charts presented in Section 7.

mse ≈

X

(Impact(a) − Estimate(a))2 /|A|

(9)

a∈A

6.2. Experiment Data
Our experimental data was extracted from The New York
Times (NYT) Annotated Corpus[18]. The corpus itself consists
of over 1.8 million articles from January of 1987 to June 2007,
for a total of 20.5 years.
In our experiments, we used the news corpus for the first
6 months of 2007. In order to focus on events, articles were
filtered using the category information found within the NYT
corpus, allowing us to remove irrelevant categories, such as
“Obituaries” and “Reviews”. The resulting evaluation dataset
consisted of 16,852 articles.

6.1.1. Offline Calculation. In order to provide a test data set,
a version of the algorithm presented in Figure 4 was executed
with access to the entire corpus. Lines 1-8 iterates through all
articles, and updates the follow up lengths. The last three lines
performs a second pass on articles, filling in the impact scores
by applying Formula (1). The results are then recorded as the
gold standard news impact scores.

491

TABLE 1. Evolution of the highest ranked ImpactRank nodes.
01/14 Initial: average term impact
LOC/Deep Blue Highway
MSC/Safe Port Act

01/14
ORG/Congress
republican

01/15
ORG/Congress
republican

01/16
ORG/Congress
nation

01/17
ORG/Congress
troop

01/18
ORG/Congress
republican

shipping

scandal

nation

troop

republican

senate

commerce
MSC/Year To Keep
90s
forth
masterwork
ORG/Democrats

LOC/Washington
force
senate
nation
Saddam Hussein
complete

LOC/Washington
force
troop
senate
leader
American

senate
American
LOC/Washington
republican
increase
force

increase
nation
senate
American
leader
rush

increase
nation
troop
leader
American
LOC/Washington

Fig. 5. Second Baseline Estimator: Average
procedure A DD T RAINING DATA(article a)
Add a to set A
end procedure
procedure T RAIN
P(partial impact scores impact)
Average ← a∈A impacta /|A|
end procedure
function E STIMATE(article a)
Estimate ← Average
end function

SVM - Basic

Random

3.00
2.50
2.00
1.50
1.00
0.50
0.00
1/31 2/14 2/28 3/14 3/28 4/11 4/25 5/9 5/23 6/6

6.3. Baseline Methods
The first choice as a baseline is random score generation,
which randomly assigns scores in the [0,3] range to any article,
without ever looking at the training dataset.
The second, and stronger baseline is based on using the
average score of the articles seen so far for all newer articles.
As presented in Figure 5, the algorithm uses the partial impact
scores for calculating this average. It should be noted that since
those scores are not generated with access to the entire dataset,
partial scores might significantly vary from the actual ones.

Fig. 6. Comparison of the MSE values for SVM-Basic
and Random estimators

SVM - Basic

SVM - TF

SVM - TF (EV)

2.50
2.00
1.50

6.4. Software Setup

1.00
0.50

The evaluation system utilized Stanford NER library[19]
for named entity recognition and sentence boundary detection, Apache Cassandra1 server for caching and data storage,
LibSVM[13] for Support Vector Machine implementation,
AT&T graphviz software[20] for visualization.

0.00
01/31/07

02/28/07

03/31/07

04/30/07

05/31/07

Fig. 7. Comparison of the MSE values for Term Frequency based methods

7. Experiment Results
The first set of experiments conducted proved the viability
of the SVM based method, by comparing its mean average
error performance against the Random baseline algorithm. As
seen on Figure 6, the SVM-Basic method overtakes Random
with four months of training data.
After determining the viability of the SVM-Basic estimator,
we evaluated the performance of the various feature selection

and ranking schemes. Figure 7 shows the term vector only,
and term vector plus named entity recognition based features,
demonstrating the benefit of including named entities in the
feature vectors. Figure 6 includes SVM-TF-IDF (EV) and
Average (baseline) against the previously mentioned methods,
showing the need for 5 months of training data for SVM
to properly surpass the performance of the simple moving
Average based baseline, and the poor performance of the tf

1. http://cassandra.apache.org

492

SVM - Basic

SVM - TF (EV)

SVM - TF-IDF (EV)

Average

forecasting (i.e. yearly, monthly, weekly, daily, hourly etc.)
and work with multiple news sources.

2.50

Acknowledgment

2.00
1.50

This research was supported by US DoDs Minerva Research
Initiative Grant Award: N00014-09-1-0815, Project leader:
Prof. Mark Woodward, Arizona State University, and the
project name is “Finding Allies for the War of Words: Mapping the Diffusion and Influence of Counter-Radical Muslim
Discourse”.

1.00
0.50
0.00
01/31/07

02/28/07

03/31/07

04/30/07

05/31/07

References

Fig. 8. Comparison of MSE values for SVM-TF (EV),
SVM-TF-IDF (EV) features, and the Average baseline

SVM - Stable

TermRank

[1] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. Archibald, and X. Liu,
“Learning approaches for detecting and tracking news events,” Intelligent Systems and their Applications, IEEE, vol. 14, no. 4, pp. 32 –43,
1999.
[2] C. L. Giles, K. D. Bollacker, and S. Lawrence, “Citeseer: an automatic
citation indexing system,” in DL ’98: Proc. of the third ACM conference
on Digital libraries. New York, NY, USA: ACM, 1998, pp. 89–98.
[3] C. Bergstrom, “Eigenfactor: Measuring the value and prestige of scholarly journals,” C&RL News, vol. 68, no. 5, 2007.
[4] E. Garfield, “The impact factor,” Current Contents, vol. 34, no. 25, pp.
3–7, 1994.
[5] F. Hecht, B. K. Hecht, and A. A. Sandberg, “The journal ’impact
factor’: A misnamed, misleading, misused measure,” Cancer Genetics
and Cytogenetics, vol. 104, no. 2, pp. 77 – 81, 1998.
[6] L. L. Lange, “The impact factor as a phantom: Is there a self-fulfilling
prophecy effect of impact?” Journal of Documentation, 2002.
[7] B. D. Ogden TL, “The ups and downs of journal impact factors,” The
Annals of occupational hygiene, 2008.
[8] ArnetMiner, “Arnetminer conference rank,” online, 2008. [Online].
Available: http://www.arnetminer.org
[9] J. E. Hirsch, “An index to quantify an individual’s scientific research
output,” in Proc. Natl Acad. Sci. USA, Nov. 2005, pp. 16 569–16 572,
published as Proc. Natl Acad. Sci. USA, volume 102, number 46.
[10] J. Allan, J. Carbonell, G. Doddington, J. Yamron, Y. Yang, and
U. Amherst, “Topic detection and tracking pilot study final report,”
Jul. 28 1998. [Online]. Available: http://citeseer.ist.psu.edu/425405.html
[11] E. M. Voorhees, “Overview of TREC 2002,” in TREC, 2002.
[12] T. Joachims, “Text categorization with support vector machines: Learning with many relevant features,” Universität Dortmund, Dortmund,
Germany, Tech. Rep. LS VIII-Report, 1997.
[13] C. C. Chang and C. J. Lin, LIBSVM: a library for support vector
machines. Online, 2001. [Online]. Available: http://www.csie.ntu.edu.
tw/∼cjlin/libsvm
[14] F. Gelgi, H. Davulcu, and S. Vadrevu, “Term ranking for clustering web
search results,” in WebDB, 2007.
[15] G. Salton and C. Buckley, “Term-weighting approaches in automatic
text retrieval,” in INFORMATION PROCESSING AND MANAGEMENT,
1988, pp. 513–523.
[16] L. Page, S. Brin, R. Motwani, and T. Winograd, “The pagerank
citation ranking: Bringing order to the web,” Stanford Digital
Library Technologies Project, Tech. Rep., 1998. [Online]. Available:
citeseer.ist.psu.edu/page98pagerank.html
[17] M. Gori and A. Pucci, “Itemrank: A random-walk based scoring
algorithm for recommender engines,” in IJCAI, 2007, pp. 2766–2771.
[18] E. Sandhaus, “The new york times annotated corpus,” Linguistic
Data Consortium, Philadelphia, 2008. [Online]. Available: http:
//www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2008T19
[19] J. R. Finkel, T. Grenager, and C. D. Manning, “Incorporating non-local
information into information extraction systems by gibbs sampling,” in
ACL. The Association for Computer Linguistics, 2005.
[20] AT&T, “Graphviz,” open Source Graph Drawing Software. [Online].
Available: http://www.research.att.com/sw/tools/graphviz/

Average

2.50
2.00
1.50
1.00
0.50
0.00
01/31/07

02/28/07

03/31/07

04/30/07

05/31/07

Fig. 9. Comparison of MSE values for final set of estimators
and tf-idf based feature selection and ranking methods against
the same baseline. Final set of evaluation among the simple
moving Average based baseline, SVM-Stable and TermRank
methods are presented in Figure 9. These results confirm our
previous intuition about the appropriateness of the inverseof-the-variance as a stable feature ranker. TermRank based
feature ranking methods can outperform all other rankers in the
longer run - within 6 months of training data. It also performs
on the same level of the SVM-Basic with all features, with
significantly lesser number of features.

8. Conclusion
In this paper we developed a framework and a measure
for news impact forecasting. We proved the viability of our
impact forecasting approach using a SVM based forecaster
on six months of NYT corpus subset - consisting of 16,852
articles. We reported the results of our experiments with six
different feature selection and ranking algorithms alongside
two baseline methods, as well as results of a new ImpactRank
technique. In our future work we plan to experiment with
longer time-frames at multiple granularities of training and

493

2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

Perspective Analysis for Online Debates
Sukru Tikves, Sedat Gokalp, Mhamed Temkit, Sujogya Banerjee, Jieping Ye, Hasan Davulcu
Arizona State University, P.O. Box 87-8809, Tempe, AZ, 85281 USA
{Sukru.Tikves, Sedat.Gokalp, Mhamed.Temkit, Sujogya.Banerjee, Jieping.Ye, Hasan.Davulcu}@asu.edu
Tel:(602) 206-8641, Fax: (480) 965-2751

Abstract—Internet and social media devices created a new
public space for online debate on political and social topics.
A debate is deﬁned as a formal discussion on a set of related
topics in a public meeting, in which opposing perspectives and
arguments are put forward. In this paper, we develop automated
perspective discovery techniques which would contribute to the
understanding of features (i.e. social, political, cultural, religious
beliefs, goals, and practices) shared by each side of the debate.
Secondly, we show that, compared to a semi-automated process,
our perspective discovery algorithms not only identify larger
number of relevant features, but they also yield a higher accuracy
scaling of moderate to extreme organizations on both sides of a
debate.

In [10], we utilized a simple term frequency - inverse document frequency (TF-IDF) [12] based technique to generate a
large candidate list of topics and perspectives for inclusion in
scaling analysis. Top 100 n-grams from each organization’s
web site were collected into a list of candidate keywords.
Next, we asked social scientists to scan this list manually, and
identify all signiﬁcant keywords belonging to social, political,
economic, and religious perspectives. During this process,
social scientists on our team assessed a total of 790 candidate
keywords; of which 29 and 26 were selected by experts for
inclusion in the radical and counter-radical scaling analysis
respectively.
Upon analyzing the results of this study, we have identiﬁed
that automatically generating the items of the radical and
counter-radical scales would be an important contribution to
the research. For example, among the included scale items
were phrases like “religious education”. However, reaching
that item from a seed topic (like “education”), instead of
manual selection would be desirable. This would not only
decrease the expert intervention in scale generation, it would
also provide us with useful perspective of organizations on
these topics aligned with the underlying scale. In order to
explore this idea, we have developed methods for perspective
analysis built upon previous ﬁndings of the scaling research.
In this paper, our primary contribution is the development
of automated perspective discovery techniques which would
contribute to the understanding of features (i.e. social, political, cultural, religious beliefs, goals, and practices) shared by
one side of a debate, and by those opposing them. Secondly,
we show that, our perspective discovery algorithms not only
identify larger number of relevant features - compared to the
semi-automated process, but also yield a higher accuracy scale
of radicalism vs. counter-radicalism.

I. I NTRODUCTION
Internet and social media devices created a new public space
for debate on political and social topics [1], [2]. Hotly debated
issues span all spheres of human activity; from liberal vs.
conservative politics, to radical vs. counter-radical religious
debate, to climate change debate in scientiﬁc community.
Many prominent ’camps’ have emerged within Internet debate
rhetoric and practice [3]. There are many applications [4]–[8]
for recognizing politically-oriented sentiment in texts.
A debate is deﬁned as a formal discussion on a set of related
topics in a public meeting, in which opposing arguments
are put forward. Initially, we observe that given a certain
topic, each organization’s web site mostly discusses their own
perspectives related to that topic, and occasionally discusses
others’ perspectives, relating them back to their own perspectives. As a case study of an ongoing large scale online debate,
we utilize the discourse found in the web sites of 10 radical,
and 13 counter-radical Indonesian religious organizations comprising a total of 37,000 articles dating from 2001 to 2011.
Radicalism [9] is the ideological conviction that it is acceptable
and in some cases obligatory to use violence to effect profound
political, cultural and religious transformations and change the
existing social order. Counter-radicals oppose violent social
and political movements.
In our prior work [10] we showed that both counter-radical
and radical movements in Muslim societies exhibit distinct
combinations of perspectives on various social, political, and
religious issues, and those perspectives can be mapped to
a latent linear continuum, or a scale. The resulting model
allowed us to measure the distance between organizations
and movements over the underlying scale. It also facilitates
tracking the ways in which movements and organizations
change over time and space [11].
978-0-7695-4799-2/12 $26.00 © 2012 IEEE
DOI 10.1109/ASONAM.2012.161

II. P ROBLEM D EFINITION
In this study, we would like to automatically discover the
perspectives of organizations on a given set of topics, and
extract the underlying discourse scale. Here, a scale is a social
science model to measure the positioning of organizations on
a latent dimension. We would like this generated scale to ﬁt
both the theoretical model, and also be able to mimic expert
level deduction in the domain.
For our speciﬁc instance, we have utilized Rasch model [13]
as the probabilistic version of a Guttman scale, and web
mining techniques for producing response tables from organizations’ corpora. We have utilized the opinion of our experts
930
898

Guttman scales. Speciﬁcally, in the simple Rasch model, the
probability of a positive response (yes) is modeled as a logistic
function of the difference between the subject and item’s
parameters. Item parameters pertain to the difﬁculty of items
while subject parameters pertain to the ability of subjects
who are assessed. A subject of higher ability relative to the
difﬁculty of an item, has higher probability to respond to a
question afﬁrmatively. In this paper Rasch models are used to
assess the organizations degree of being radical or counterradical based on the religio-social keywords (items) appearing
in their rhetoric.
Rasch model maps the responses of the subjects to the items
in binary or dichotomous format , i.e., 1 or 0. Let Bernoulli
variable Xvi denotes the response of a subject v to the item
i, variable θv denotes the parameter of “ability” of the subject
v and βi denotes the parameter of “difﬁculty” of an item i.
According to the Rasch model the probability that subject v
responds 1 for item i is given by

on Indonesian Islamic religious organizations as the target gold
standard of orderings of these organizations.
III. M ETHODS AND T HEORY
A. Overall System Model
We devised an end-to-end pipeline of methods that would
generate two sets of perspectives for each of the polarities of
a scale, starting from a mined web corpus, and a list of topics
provided by experts.
Overall ﬂow of the system consists of data gathering from
experts, and web mining, manual topic selection, perspective
analysis, and scale generation.
B. Scaling
In social science scaling is a process of measuring and
ordering entities such as subjects based on their qualitative
attributes called items. In general, subjects respond to surveys
in form of interviews or questionnaires, where items are
presented to the subjects in form of questions. Some of the
widely followed scaling procedures in social science are Likert
scale [14], Thurstone scale [15], and Guttman scale [16].
Guttman scaling procedure orders both the subjects and the
items simultaneously with respect to some underlying latent
cumulative continuum. In this study, items tend to have a
natural total ordering to partial ordering, since an organization
support for keyword such as “Sharia” will most likely imply
their support for the keyword “Quran”, we used the Guttman
scaling to rank the organizations based on their response on
the radical and counter-radical keywords.
A Guttman [17] scale presents a number of items to which
each subject is requested to provide a dichotomous response,
e.g. agree/disagree, yes/no, or 1/0. This scaling procedure is
based on the premise that the items have strict orders (i.e., the
items are presented to the subjects ranked according to the
level of the item’s difﬁculty). An item “A” is said to be “more
difﬁcult” than an item “B” if any subject answering “yes”
on item “A” implies that the subject will also answer “yes”
on item “B”. A subject who responds to an item positively
is expected to respond positively to all the items of lesser
difﬁculty.
Guttman scale is a deterministic process and the score of
a subject depends on the number of afﬁrmative responses he
has made on the items. Scores in Guttman scale can also be
interpreted as the “ability” of a subject in answering questions
sorted in increasing order of “difﬁculty”. These scores when
presented on an underlying scale, give us an ordering of the
subjects based on their “ability” too.
The objective of our paper is to order the Indonesian
Islamic organizations based on their views on religio-social
keywords which have an inherent ordering. An organization
supporting “Sharia” will also likely to believe in “Quran”. So
it makes sense to use Guttman scaling procedure to rank the
organizations and their beliefs and practices. One drawback of
Guttman scale is that it is deterministic and assumes a strict
ordering of the items. We used Rasch [18] model to overcome
this drawback, by providing a probabilistic framework for

P (Xvi = 1|θv , βi ) =

exp(θv − βi )
1 + exp(θv − βi )

The maximum likelihood method is used to provide estimates for subject and item parameters. We can also assess
whether the data ﬁts the model by looking at goodness of
ﬁt indices, such as the Andersen’s likelihood ratio test (LRtest) [19]. A p-value, returned by the test, indicates the
goodness of ﬁt and a p-value higher than 0.05 indicates no
presence of lack of ﬁt. We used the eRm [20] package to run
the Rasch models
C. Implementing Rasch Model in the Text Mining Domain
Other works in text-mining domain such as sentiment
analysis, have used Rasch model in their analysis [21]. In
our application, Rasch model subjects correspond to a group
of religious organizations, and items correspond to a set
of keywords for socio-cultural, political, religious radical
and counter-radical beliefs, and practices. An organization
responding “yes” to a feature means the organization exhibits
that feature in its narrative, while an organization responding
“no” to a feature indicates that the organization does not
exhibit such a feature. Difﬁculty of an item translates to
strength of the corresponding attitude in deﬁning radical or
counter-radical ideology of any organization. Similarly ability
of a subject in this case means the degree of radicalism or
counter-radicalism exhibited by an organization’s rhetoric.
D. Our Initial Work on Scale Generation
Our initial work [10] depended on more direct interaction
with experts’ opinion to build a model that can capture the
underlying dynamics of the scale. The experts both provided
a set of target organizations, and also directly selected the
items that would make up the scale, from a machine generated
candidate list. The candidate list consisted of the union of top100 n-grams from each organization’s individual corpus, which
were a total of 790 items. The resulting scale has utilized a
total of 55 of keywords selected by experts.

899
931

E. Debates and Perspective Analysis
Upon inspecting the keywords selected by our team of experts we observed that, some of these keywords correspond to
differing perspectives on a set of topics that are debated within
these web sites. Deﬁnition of debate is “a formal discussion on
a particular topic in a public meeting or legislative assembly, in
which opposing arguments are put forward.”1 . During a debate
on a particular topic, like education, both radical and counterradical organizations discuss different perspectives – such as
“secular multi-cultural education” vs. “sharia based religious
education”.
During the design of an automated perspective detection
algorithm, we made the following simplifying assumptions:
1) Organizations will mostly discuss their own perspective
in a debate;
2) Organizations will occasionally mention others’ perspectives, however, then relate them back to their own
perspective.
In our upcoming work [11], we present a mathematical
formulation of the perspective keyword generation problem
for a given topic, and provide an NP-Completeness proof of
this problem, and design an exact solution through an ILP
(integer linear programming) based solver.
The input to this algorithm also takes the polarity suggestion
from experts into consideration, for automatically identifying
the discriminating perspectives of those organizations from
opposite sides of a debate.
However, due to the algorithmic complexity and the strict
constraints of the exact model, the ILP based solver was not
always able to produce acceptable solutions. Namely, for larger
debated topics, the runtime requirements2 exceeded acceptable
limits of the study, and for more intervened debates, none of
the possible item sets could satisfy strict constraints of the ILP
deﬁnition.
In order to resolve this, in our current version of the system,
we have worked with a feature selection framework, SLEP.
The discussion of the implementation of SLEP is discussed in
the next section.

min
x

m


wi log(1 + exp(−yi (xT ai + c))

(1)

i=1

+ λ|x|1
ρ
+ ||x||22
2

(2)
(3)

where Di is the document i and Fj is the feature (word) j.
A is the term × document matrix with all Aij ≥ 0, yi ∈ y is
the class of each document Di coded as +1 for Radical (R)
and -1 for Counter-Radical (CR) and xj is the weight for each
feature Fj . Let us explain further the three terms involved in
the convex optimization problem.
m
T
•
i=1 wi log(1 + exp(−yi (x ai + c)), this ﬁrst term is
related to the logistic classiﬁcation error. We set the
weights wi values to be all 1 so that all documents have
the same weight.
• λ|x|1 , this term involving the L1 norm deals with the
sparsity of the solution vector x. We experienced with
several lambda values which resulted with an x vector of
various sparsity.
ρ
2
• 2 ||x||2 , this last term deals with the ridge regression,
which is an extra level of shrinkage. We set the weight
of this term ρ = 0 as we were mainly driven by sparsity.
• We used the M ATLAB implementation of the SLEP package3 which utilizes gradient descent approach to solve
the aforementioned optimization problem. This package
can handle matrices of 20M entries within a couple of
seconds on a machine with standard conﬁguration.
• The features with non-zero values on the x vector are the
candidate discriminants. Let FR , where xj > 0 be the
discriminant for the R class. Similarly, let FCR , where
xj < 0 be the discriminant for the CR class due to
the coding schema in step 3. Given that the optimized
formulation resulted with a sparse x vector, most of the
words Fj had xj = 0 and hence were not included in
either FR or FCR .
Note that the sets of features FR and FCR may not satisfy the
Guttman pattern. These sets needed to be further ﬁltered such


⊆ FR and FCR
⊆ FCR would satisfy the Guttman
that FR
pattern.

F. SLEP: A Sparse Learning Package
In order to address the scalability problem encountered in
ILP we resorted to SLEP [22], again with the underlined
motivation to select a subset of discriminating features that can
(a) classify and (b) satisfy Guttman scale [16]. The following
steps describe our algorithm:
1) For each topic, calculate the frequency of the words occurring within a ﬁxed size window of the topic keyword
2) Filter the term × document matrix to include only the
most frequent 1000 words from each camp
3) Formulate the problem in a general sparse learning frame
[22]. Logistic formulation ﬁts our application, since it is
a dichotomous classiﬁcation problem

IV. S YSTEM A RCHITECTURE
A. Data gathering
Initially, social scientists were invited to use their domain
and area expertise to identify a set of organizations, and
hypothesize any number of unipolar or bipolar scales that
could explain the variance among their beliefs and practices.
Next, a set of web crawling scripts were created for extraction
of articles from those organizations’ web sites. For each organization’s corpus we extracted their original text in Indonesian,
and stored the corpus in a ZIP ﬁle.

1 Oxford

Online Dictionary
data volume projections, we have estimated an upper bound of one
hour runtime restriction per topic. For this paper, we have run the cplex ILP
solver several hours for each topic before a timeout.
2 Given

3 http://http://www.public.asu.edu/œjye02/Software/SLEP

900
932

B. Feature Extraction

Fig. 1.

After identifying the features for the analysis, we iterate
over the documents in the corpus of each organization for the
matching items. This yields a feature-document matrix.
This feature extraction task was performed in a simple
three step procedure; initially the occurrence frequencies of
particular features were counted within each organization’s
corpus, then a threshold matrix was calculated from these initial values, and ﬁnally a binary response matrix was generated
by applying these thresholds to the initial values.
The frequency metric is shown in formula 4, where k is the
keyword, o is the organization, and Do is the document set
pertaining to that particular organization.
fo,k =

|{d | k ∈ d, d ∈ Do }|
|Do |

procedure G REEDY-S ELECTION(I, C, )
initial features I
candidate features C
comparison function 

2:
3:
4:

S←I
repeat
m ← S OLVE(S)
N ←∅
for all c ∈ C \ S do
p ← S OLVE(S ∪ {c})
if p  m then
N ← N ∪ {c}
end if
end for
if N = ∅ then
S ←S∪N
end if
until N = ∅
return S
end procedure

5:
6:
7:
8:
9:
10:

(4)

11:
12:
13:
14:
15:

A threshold value for each keyword is calculated from the
values in the related column. And then, each element was
converted into a binary value by comparing it to the column’s
threshold.

16:
17:

C. Model Fitting
We ﬁt the Rasch model on two datasets - (1) radical
organizations with radical keywords and (2) counter-radical
organizations with counter-radical keywords. We used the eRm
package in R, an open source statistical software package4 , to
ﬁt a Rasch model to the dataset, and obtain the organizations’
scores on the latent scale, which are the subject parameter
estimates (θv ) discussed in previous section. The eRm package5 ﬁts Rasch models and provides subjects or organizations
parameter estimates.

Feature set expansion algorithm

1:

subset (line 4), and then identiﬁes each not yet selected feature
that provides a performance increase (lines 7 – 8), and ﬁnally
collects them into the selected feature set for the next iteration
(lines 5, and 8 – 10). When it can no longer include any new
features, the algorithm will terminate.
Another performance trade-off was done using the natural
grouping of the features. Since the features in our problem are
grouped by topic, we decided to keep these natural groupings,
thus making each c in set C a collection of features.
Here the  comparison function will assume greater-thanor-equal-to semantics. This is because, while we want to have
the best possible scoring features as possible, we also want to
be able to have a larger set of perspectives that can be used
to explain the underlying latent scale.
In order to be able to handle the case of an empty initial
feature set (I), we expanded the algorithm as shown in Fig. 2.
This modiﬁcation (to lines 6 – 11 in the original algorithm)
will choose the best available features in the ﬁrst iteration that
are within a score difference of δ of each other. This also
assumes S OLVE function will return a sensible upper limit
value when an empty set is given as its input.

D. Feature Expansion Algorithm
We have observed that including all of the newly discovered
features in the scale resulted a poor performance. This is
because, they neither provided the desired Guttman pattern,
nor the resulting scale aligned with the expert opinion. However, exhaustively enumerating all possible subsets to ﬁnd an
optimal one would also be undesirable due to time complexity.
Thus we have devised a greedy expansion based algorithm to
select the items that make up the scale. It chooses a sufﬁciently
optimal subset of these features by expanding an initial set,
incrementally adding features that offer a higher performance.
One possible implementation is shown in the algorithm in
Fig. 1. This greedy algorithm will start from an initial set of
features I, and iteratively select the features that increase the
performance of the solution. The performance of a solution
is evaluated by the S OLVE function, which takes a candidate
input, and returns the performance value according to expert
agreement.
Each iteration of the loop (lines 3 – 15) tries to iteratively
expand the current set of selected features (lines 12 – 14).
First, it evaluates the performance of the currently selected

E. eRm Iterative Item Elimination Algorithm
While the eRm Rasch analysis package already does trivial
eliminations in the model (for example, ignoring full/empty
1/0 responses), it also provides an algorithm to clean up
a model from features that do not adhere to the Rasch
model/Guttman pattern.
The overall idea of the algorithm is summarized in the algorithm in Fig. 3. While our greedy feature selection algorithm
worked by expanding a set of features, this algorighm works
by going the opposite direction, and reducing the feature set

4 http://cran.r-project.org/
5 http://r-forge.r-project.org/projects/erm/

901
933

Fig. 4.

Fig. 2.
Feature set expansion algorithm modiﬁcation, enabling special
handling of the empty initial set of features
1: for all c ∈ C do
2:
p ← S OLVE(S ∪ {c})
3:
if p  m then
4:
if S = ∅ ∧ p − m < δ then
5:
N ← {c}
6:
7:
8:
9:
10:
11:

m←p
else
N ← N ∪ {c}
end if
end if
end for

Fig. 3. Feature elimination algorithm provided by the eRm package
1: procedure S TEP W ISE I T (m, E VAL )

on collective representations [23], Simmel’s work on conﬂict
and social differentiation [24], Wallace’s writings on revitalization movements [25], and Tilly and Bayat’s studies on
contemporary social movement theory [26] [27]. Our team
has also developed, and is currently testing a theoretically
based class model comprised of continuous latent scales. The
ﬁrst pair of scales focus on distinctions between the goals and
methods of counter-radical and radical discourse, and capture
the degree to which individuals, groups, and behaviors aim
to inﬂuence the social order (Change Orientation) and the
methods by which they attempt to do so (Change Strategies).
Quadrants model (see Figure 4) captures multiple social
trends in four quadrants A, B, C, and D, and it makes
the signiﬁcant distinction between violent and not-violent
dimensions of both radicalisms and counter radicalisms. Using
the quadrants model, a researcher can locate organizations,
individuals, and discourses in broader categories while still
considering subtle differences between groups within categories. A researcher can document movement and trends from
category to category, and identify points where movement is
likely.

Rasch Model (RM) m
Evaluation Function E VAL
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

The quadrants model

r←m
repeat
e ← E VAL(r)
if not F ITS(e) then
i ← L OWEST R ANKED F EATURE(e)
x ← r$x \ {i}
r ← RM(x)
end if
until Max # of Steps, or F ITS(e)
return r
end procedure

in each step. Here $ is the R member access operator, where
r$x is the feature set of rasch model object r, and \ is set
difference. Functions E VAL, and L OWEST R ANKED F EATURE
are references to eRm provided facilities to evaluate, and ﬁnd
the worst contributing item of Rasch models.

C. Expert Opinion and Gold Standard of Rankings

V. E XPERIMENTAL E VALUATION

We collaborated with three area experts, who collectively
possess 35 years of scholarly expertise on Indonesia and
Islam. We utilized a homegrown graphical drag-and-drop user
interface to collect their opinion to build the gold standard of
the rankings. A screenshot of this tool is shown in Figure 5.
Each expert separately evaluated and ranked the organizations in the dataset according to a two dimensional scale
of radical / counter-radical (R/CR) and violent / non-violent
(V/NV) axis. The consensus among the experts was high; since
per item standard deviations among the experts’ scores along
the R/CR axis over a range of [−10, 10], across all organizations were 2.75. The individual scores for each organization
were combined and averaged to obtain the consensus gold
standard rankings along the hypothesized R/CR scale.
In this paper, we used two measures for evaluating the difference between two separate rankings, based on Spearman’s
footrule and Spearman’s correlation coefﬁcient. The original
work utilized a mean displacement based measure as follows:

In order to measure the relation of the generated perspectives to the underlying scale, we have performed a series of
experiments designed to compare their scaling capabilities to
the gold standard ordering done by the experts.
A. Indonesian Corpus
The corpus domain is the online articles published by the
web sites of the 23 religious organizations identiﬁed in Indonesia, in the Indonesian language. These sources are the web
sites or blogs of the identiﬁed think tanks and organizations.
As discussed in the introduction, each source was classiﬁed
as either radical or counter-radical by the area experts. We
downloaded a total of 37,000 Indonesian articles published in
these 23 web sites, dating from 2001 to 2011.
B. The Quadrants Model
Our project leverages the results of our previous work,
which relied on social theory including Durkheim’s research

902
934

Fig. 5. The visual interface of the expert opinion collector for manually
placing the organizations on the two dimensional scale

In order to have a baseline for comparison of the automatically generated items, we have opted to use this scale in our
current study.
E. Candidate Perspectives
We have run both the ILP, and the SLEP based feature
generators on all the 50 topics that has been identiﬁed. ILP was
able to identify perspectives for 18 of the topics, while failed
for the rest, due to either ﬁnding no viable exact solution, or
timeouts. This resulted in a total of 2869 perspectives, with
159 average on each topic. Since these exact features also
included items with very low support, we have ﬁltered these
results to include only the ones with higher frequency in the
corpus. The ﬁnal set contained a total of 227 perspectives on
all 18 solved topics. On the other hand, SLEP was able to
successfully generate candidate perspective on every 50 topic,
totaling 1065 perspectives, with an average of 21 on each
topic.

Given two discrete ordering functions G, and R, on the
organization set O, the normalized displacement of a single
organization is given as:
disp(G, R, O, o) =

|G(o) − R(o)|
|O|

F. Aligning Perspectives with the Scales
In order to identify the perspectives that make up the
theoretical scale we are working on (R/CR bi-polar scales on
Indonesian Islamic religious organizations), we have devised
a set of experiments that measure their relation to the Rasch
model, and the expectation of ﬁeld experts.
Initially, as a baseline, we have re-run the original scale with
the expert selected features, with the new evaluation metrics.
The mean displacement of the features was 0.1172, while
the mean square displacement score was 0.0287. (The slight
difference with the original paper is due to the handling of the
missing items, discussed in Section V-C).
In order to observe the effect of the S TEP W ISE I T, we have
run the elimination algorithm on the original set of features.
The mean displacement was decreased to 0.1115, while the
mean square displacement stayed the same. The algorithm has
eliminated 15 features to reach this score. The summary of
these experiments can be seen in Table I

(5)

Here, O is the set of organizations, G and R are one to one
mapping functions of rankings from set O to range [1, |O|].
Then overall error measure for a given set of rankings was
then deﬁned as:
error(G, R, O) =

 disp(G, R, O, o)
|O|

(6)

o∈O

In addition to this measure, we have also opted to include another measurement to take stability of the items into
consideration. Based on the L2 − N orm of the normalized
displacement function, the msd measure can be deﬁned as
the following:
msd(G, R, O) =

 disp(G, R, O, o)2
|O|

TABLE I
E XPERIMENTAL RESULTS FOR THE ORIGINAL EXPERT SELECTED FEATURE

(7)

o∈O

BASED SCALES

Since our initial work, we have also modiﬁed the evaluation
of the missing items. Speciﬁcally, for empty/full response
patterns, the Rasch model would not be able to make any
inference. Since we experimented with dynamic features, and
the missing items varied in each test, we have opted to position
them in their neutral places. This change has introduced a
slight difference from the experimental results of our original
study.

Original
Original + S TEP W ISE I T

error
0.1172
0.1115

msd
0.0287
0.0287

run time
14s
53s

G. The Initial Experiments with Feature Expansion
After establishing the baseline, we evaluated the perspective
based features discovered by the ILP solver. First we built a
model including all the candidate features proposed by the
solver. This resulted in an mean displacement of 0.1323 and
mean square displacement of 0.0284. While the performance
was near the expert level, the hand selected features performed
(13%) better than this initial run.
Then the features were reﬁned with S TEP W ISE I T, and our
G REEDY-S ELECTION algorithms. The S TEP W ISE I T failed to
provide better results, and actually performed worse, with

D. Baseline Performance
In our previous study [10] we have automatically generated
a Rasch model from the organizational corpus data, and the
expert selected items. We have observed that, against several
baseline algorithms, including score sorting, and principal
component analysis, the Rasch model was able to demonstrate
the best available performance, and was ranked at expert level.

903
935

TABLE II
T HE TOPICS CHOSEN BE THE G REEDY-S ELECTION ALGORITHM FROM THE
CANDIDATE PERSPECTIVES OF THE ILP SOLUTION .
Iteration
1

Fig. 6. Runtime performance of the Rasch model ﬁtting algorithm in the
eRm package. The x axis corresponds to the number of items, while the y axis
represents the runtime length in seconds. Notice that the scatter plot shows
ﬁtness to the x2 polynomial prediction line.

Topics
kufur
disbelief

2

kdrt, kekaﬁran, kesetaraan, konstitusional,
multikultural, sekularisme, tabligh, toleransi
(domestic violence, inﬁdelity, equality, constitutional,
multicultural, secularism, tabligh, tolerance)

3

bunuh, gender, homoseksual, musyrikin, syirik
(suicide, gender, homosexuals, idolaters, paganism)

mean displacement of 0.1632, and mean square displacement
of 0.0386, while failing the LR−test for Rasch model ﬁtness.
The likely reason for this is that S TEP W ISE I T performs item
eliminated locally based on individual item ﬁtness, but the
sparse nature causes loss of global Guttman pattern.
When we built an optimum item set from scratch using
the G REEDY-S ELECTION algorithm, we were able to identify
14 topics that contributed with better ﬁtting perspectives.
The expanding topic sets can be seen in Table II. The ﬁnal
solution had a mean displacement of 0.1020, with a mean
square displacement of 0.0189. An additional cleanup using
the S TEP W ISE I T algorithm over this existing solution did not
produce better results.
The summary of these experiments can be seen in Table III.

TABLE V
T HE TOPICS CHOSEN BE THE G REEDY-S ELECTION ALGORITHM FROM THE
CANDIDATE PERSPECTIVES OF THE SLEP SOLUTION .
Iteration
1
2

error
0.1323
0.1632
0.1020
0.1122

msd
0.0284
0.0386
0.0182
0.189

beragama, bunuh, dakwah, demokrasi, jihad,
kaﬁr, kristen, liberal, multikultural, pluralisme,
politik, sipil, syariat, syirik
(religion, kill, propaganda, democracy, jihad,
inﬁdel, Christian, liberal, multicultural, pluralism,
political, civil, Sharia, polytheism)

perspectives were closely related to the underlying scale. The
expanding topic set can be seen in Table V. The best mean
displacement achieved was 0.0982, with a corresponding mean
square displacement of 0.0189.
The main reason that this table does not share a signiﬁcant
amount of topics with the ILP based topic set, is that the ILP
solver could not provide results for the great majority of the
topics selected by SLEP. The common ones, like “multikultural”, “syirik” were selected in both, while similar topics (like
“politik”/”konstitusional”) were chosen when available.

TABLE III
S CALING EXPERIMENTS WITH THE ILP SOLVER BASED DATA

ILP
ILP + StepWiseIt
Greedy(ILP)
Greedy(ILP) + StepWiseIt

Topics
manusia
(human)

run time
3m:38s
56m:47s

H. SLEP Based Features
I. Sample perspectives

In addition to the ILP based exact features, we also ran
separate experiments for the SLEP output. These yielded a
total of 449 features on counter radical, and 616 features on
the radical scales. The overall runtime duration was 6 hours
and 4 minutes. The resulting scales had a mean displacement
of 0.1398 and mean square displacement of 0.0312. We opted
not to run the S TEP W ISE I T on this particular case, since the
expected runtime would be in the order of weeks, which would
not be practical for the real life conditions of the project.

A set of sample perspectives selected by the ILP solver are
displayed in Figure 7. Here the columns represent individual
topics, while two rows correspond to radical, and counterradical perspectives on these topics. The items have been
machine translated from Indonesian into English.
VI. C ONCLUSION
In this paper, we have implemented a perspective identiﬁcation algorithm, with exact, and approximate solutions, and
also tested the viability of this algorithm, by trying to apply
the discovered perspectives on an underlying bi-polar social
scale.
In relation to our previous work, we have increased the
automation in the scale generation process, by abstracting the
hand selected items to automatically discovered perspectives
from topics, and also increased the overall efﬁciency of the
system, by producing lower distance to the expert agreement.

TABLE IV
S CALING EXPERIMENTS WITH THE SLEP SOLVER BASED DATA

SLEP
Greedy(SLEP)

error
0.1398
0.0982

msd
0.0312
0.0189

run time
6h:04m

Like the ILP based candidates, we also ran the G REEDYS ELECTION algorithm on the SLEP input (Table IV). Over two
iterations, the algorithm was able to identify 15 topics, whose

904
936

Fig. 7. A sample set of perspectives generated by the ILP based solver. Here each row represents a debate topic, while the linear scales represent the
locations of the perspectives. The left side items are the counter-radical, and the right side items are the radical perspectives in each of these topics.

The theorized greedy growth based algorithm demonstrated
the best available performance in our experiments. Additionally both ILP, and SLEP based perspective generation
techniques provided features that ﬁt the underlying bi-polar
scale.
Our future work includes reducing the expert interaction
further by automatically discovering debate topics, and investigating a possible use of these perspective analysis techniques
in the related ﬁeld of sentiment analysis.

[11] S. Tikves, S. Banerjee, H. Temkit, S. Gokalp, H. Davulcu, A. Sen,
S. Corman, M. Woodward, S. Nair, I. Rohmaniyah, and A. Amin, “A
system for ranking organizations using social scale analysis,” Social
Network Analysis and Mining (SNAM), Accepted for publication.
[12] J. Hartigan and M. Wong, “Algorithm as 136: A k-means clustering
algorithm,” Journal of the Royal Statistical Society. Series C (Applied
Statistics), vol. 28, no. 1, pp. 100–108, 1979.
[13] G. Rasch, “On general laws and the meaning of measurement in
psychology,” in Proceedings of the Fourth Berkeley Symposium on
Mathematical Statistics and Psychology, 4, 1961, p. 332.
[14] R. Likert, “A technique for the measurement of attitudes,” Archives of
Psychology, vol. 140, pp. 1–55, 1932.
[15] L. L. Thurstone, “Attitudes can be measured,” American Journal of
Sociology, vol. 33, pp. 529–554, 1928.
[16] J. McIver and E. Carmines, Unidimensional Scaling. Sage Publications,
Inc, 1981, vol. 24.
[17] L. Guttman, “The basis for scalogram analysis,” Measurement and
prediction, vol. 4, pp. 60–90, 1950.
[18] D. Andrich, Rasch models for measurement. Sage, 1988.
[19] D. Hessen, “Likelihood ratio tests for special rasch models,” Journal of
Educational and Behavioral Statistics, vol. 35, no. 6, p. 611, 2010.
[20] P. Mair and R. Hatzinger, “Extended rasch modeling: The erm package
for the application of irt models in r,” 2007.
[21] D. Drehmer, J. Belohlav, and R. Coye, “An exploration of employee
participation using a scaling approach,” Group & Organization Management, vol. 25, no. 4, p. 397, 2000.
[22] J. Liu, J. Chen, and J. Ye, “Large-scale sparse logistic regression,” in
Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 2009, pp. 547–556.
[23] E. Durkheim, “The cultural logic of collective representations,” Social
theory the multicultural and classic readings, Wesleyan University:
Westview Press, 2004.
[24] G. Simmel, Sociological Theory. New York: McGraw-Hill, 2008.
[25] A. Wallace, “Revitalization movements,” American Anthropologist,
vol. 58, pp. 264–281, 1956.
[26] C. Tilly, Social Movements. Boulder, CO, USA: Paradigm Publishers,
2004.
[27] A. Bayat, Making Islam Democratic: Social Movements and the PostIslamist Turn. Stanford University Press, 2007.

R EFERENCES
[1] Z. Papacharissi, “The virtual sphere: the internet as a public sphere,”
New Media Society, vol. 4, no. 1, pp. 9–27, Mar. 2002. [Online].
Available: http://nms.sagepub.com/cgi/content/abstract/4/1/9
[2] I. Himelboim, “Civil society and online political discourse: The network
structure of unrestricted discussions,” Communication Research, Oct.
2010. [Online]. Available: http://dx.doi.org/10.1177/0093650210384853
[3] L. Dahlberg, “The internet and democratic discourse: Exploring the
prospects of online deliberative forums extending the public sphere,”
pp. 615–633, Dec. [Online]. Available: http://www.ingentaconnect.com/
content/routledg/rics/2001/00000004/00000004/art00007
[4] T. Mullen and R. Malouf, “A preliminary investigation into sentiment
analysis of informal political discourse,” in AAAI symp. on computational approaches to analysing weblogs (AAAI-CAAW), 2006, pp. 159–
162.
[5] R. Malouf and T. Mullen, Graph-based user classiﬁcation for informal
online political discourse, 2007.
[6] M. Thomas, B. Pang, and L. Lee, “Get out the vote: Determining
support or opposition from congressional ﬂoor-debate transcripts,” in
In Proceedings of EMNLP, 2006, pp. 327–335.
[7] M. Bansal, C. Cardie, and L. Lee, “The power of negative thinking:
Exploiting label disagreement in the min-cut classiﬁcation framework,”
Proceedings of COLING: Companion volume: Posters, pp. 13–16, 2008.
[8] W. Lin and A. Hauptmann, “Are these documents written from different
perspectives?: a test of different perspectives based on statistical distribution divergence,” in Proc. of the 21st Int. Conf. on Computational
Linguistics and the 44th annual meeting of the ACL. ACL, 2006, pp.
1057–1064.
[9] M. Woodward, I. Rohmaniyah, A. Amin, and D. Coleman, “Muslim
education, celebrating islam and having fun as counter-radicalization
strategies in indonesia,” Perspectives on Terrorism, vol. 4, no. 4, 2010.
[10] S. Tikves, S. Banerjee, H. Temkit, S. Gokalp, H. Davulcu, A. Sen,
S. Corman, M. Woodward, I. Rochmaniyah, and A. Amin, “A system
for ranking organizations using social scale analysis,” in EISIC. IEEE,
2011, pp. 308–313.

905
937

2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology

Directional Prediction of Stock Prices using
Breaking News on Twitter
Hana Alostad

Hasan Davulcu

School of Computing, Informatics and
Decision Systems Engineering
Arizona State University
Tempe, Arizona 85287–8809
Email: hana.alostad@asu.edu

School of Computing, Informatics and
Decision Systems Engineering
Arizona State University
Tempe, Arizona 85287–8809
Email: HasanDavulcu@asu.edu
show that the breaking news based system indeed yields a
statistically signiﬁcant boost in directional prediction accuracy
compared to the one using all news.
The rest of the paper is organized as follows. Section II
presents related work. Section III presents formal problem
deﬁnition. The system architecture is presented in Section IV.
Section V presents the experimental data we used, and quantitative 10-fold cross validation results for the two experiments
that we performed. The last section concludes the paper and
discusses future work.
II. R ELATED W ORK
Stock price prediction problem has been studied for several
years, several research papers have been published with a
goal to increase the accuracy of prediction. Table I contains a
summary of previous research results related to stock price
or direction prediction, the input data sets used, the timeframes for prediction, the length of the period of collected
data, prediction algorithms used, and the overall accuracies.
These systems have different prediction time-frames and
goals. Some of them predict stock price for the intended timeframe like [10], [14] and [19]. Time frames vary as daily or
next 20 minutes. Others such as [16], [12], [13], [15], [18]
, [8], and [17] predict stock price direction for the next day.
[11] aims to predict the price direction every 2-hours, and [9]
aims to predict monthly direction.
Related systems collected their input data from various
sources and exchanges: [14], [18], and [19] collected stock
news, tweets and price charts related to S&P 500 companies.
[17] collected tweets and stock price data related to Nasdaq
stocks, [16] collected tweets and stock price charts related
to Dow Jones Industrial Average (DJIA), [13] collected one
year of data related to Microsoft company. [9] collected stock
price charts from Shenzhen Development Stock A (SDSA)
exchange. [11] collected currency price and news data related
to foreign exchange market (Forex). [8] collected stock price
data from CNX Nifty, S&P BSE Sensex exchanges and ﬁnally
[10] collected thirteen years of stock price charts data related
to Goldman Sachs Group Inc.
[9], [8], and [10] used only stock price as input to predict
stock price or direction with accuracies varying between 83%
and 90%. [12], [13], [15], and [11] are examples of papers
which utilize news as well as stock prices to predict price
direction with varying accuracies between 51% and 83%.
[18] made correlation analysis between the stock price
and the tweet volume, and used it to predict stock market

Abstract—Stock market news and investing tips are popular
topics in Twitter. In this paper, ﬁrst we utilize a 5-year ﬁnancial
news corpus comprising over 50,000 articles collected from the
NASDAQ website for the 30 stock symbols in Dow Jones Index
(DJI) to train a directional stock price prediction system based
on news content. Then we proceed to prove that information
in articles indicated by breaking Tweet volumes leads to a
statistically signiﬁcant boost in the hourly directional prediction
accuracies for the prices of DJI stocks mentioned in these articles.
Secondly, we show that using document-level sentiment extraction
does not yield to a statistically signiﬁcant boost in the directional
predictive accuracies in the presence of other 1-gram keyword
features.
Keywords—stock prediction; text mining; breaking news; twitter
analysis; twitter volume spike; stock trading

I. I NTRODUCTION
Online social networks, like Twitter, are enabling people
who are passionate about trading and investing to break critical
ﬁnancial news faster and they also go deep into relevant areas
of research and sources leading to interesting insights. Recently Twitter has been used to detect and forecast civil unrest
[1], criminal incidents [2], box-ofﬁce revenues of movies [3],
and seasonal inﬂuenza [4].
Stock market news and investing tips are popular topics
in Twitter. In this paper, ﬁrst we utilize a 5-year ﬁnancial
news corpus comprising over 50,000 articles collected from
the NASDAQ website for the 30 stock symbols in Dow Jones
Index (DJI) to train a directional stock price prediction system
based on news content. Next we collect over 750,000 tweets
during a 6 month period in 2014 that mention at least one of
the 30 DJI stock symbols. We utilize the 68-95-99.7 rule, also
known as the three-sigma rule or empirical rule [5], to deﬁne a
simple method for detecting hourly stock symbol related tweet
volume breakouts. Then we proceed to test our hypothesis
to determine if “information in articles indicated by breaking
Tweet volumes will lead to a statistically signiﬁcant boost in
the hourly directional prediction accuracies for the prices of
DJI stocks mentioned in these articles”.
The contributions of the paper can be summarized as follows: Firstly, we show that sparse logistic regression [6] for
this text based classiﬁcation task with 1-gram keyword features
ﬁltered by a Chi2 [7] feature selection algorithm leads to the
best overall directional prediction accuracy. Secondly, we show
that using document-level sentiment extraction does not yield
to a statistically signiﬁcant boost in the predictive accuracies
in the presence of other 1-gram keyword features. Thirdly, we
978-1-4673-9618-9/15 $31.00 © 2015 IEEE
DOI 10.1109/WI-IAT.2015.82

523

TABLE I.
Online News

Tweets

[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]

Data set
Stock Price

Reference








































S UMMARY OF P REVIOUS R ESEARCH R ESULTS

Time-frame

Period

Prediction

Algorithm

Accuracy

Daily
Monthly
Daily
2 Hrs
Daily
Daily
20 Min
Daily
Daily
Daily
Daily
Daily

9 Yrs
2 Yrs
13 Yrs
4 Yrs
14 Yrs
1 Year
1 Mo
1 Year
10 Mos
2 Mos
3 Mos
1 Year

Direction
Direction
Price
Direction
Direction
Direction
Price
Direction
Direction
Direction
Direction
Price

Naive Bayes
Logistic Regression
Linear Regression
SVM
SVM
SVM
SVR
Neural Network
Neural Network
Decision Tree
Liner regression
Bayesian

90%
83%
2.54 (RMSE)
83%
79%
61%
51%,
3.70 (RMSE)
88%
77%
68%
0.3% (daily)

direction with 68% accuracy. Following work by [19] analyzed
tweet spikes in combination with price action based technical
indicators such as price breakout direction as an input to
a Bayesian classiﬁer for stock price prediction, yielding a
daily average gain of approximately 0.3% during a period of
55 days generating a total gain of 15%. [16] used extracted
sentiment information from Twitter data and a neural network
classiﬁer to predict Dow Jones Industrial average (DJIA) daily
price direction with 88% accuracy. [17] also used sentiment
information extracted from Twitter as input to a decision
tree classiﬁer to predict price direction for four companies
in NASDAQ stock exchange with average accuracy of 77%
distributed as APPL at 77%, GOOG at 77%, MSFT at 69%
and AMZN at 85% during a two months period of evaluations.

previous 20 hour periods. We consider a breakout as an
indication that traders or technical analysts are sharing some
exciting or important new information regarding the company
or a group of companies. Next, we collect the URL links
mentioned within the breaking-news hour of tweets and we
designed a pair of experiments to test our hypothesis whether
“information in news indicated by breaking tweet volumes
will lead to statistically signiﬁcant boost in the directional
prediction accuracy for the prices of the related stock symbols
mentioned in these articles”.
Our system has the following characteristics:
1) Input data: hourly stock price charts of the 30 stocks
comprising the Dow Jones Index (DJI), online stock news
articles for a 5 year period spanning 2010 and 2014 from
NASDAQ1 news website, the tweets related the 30 stock
symbols collected from Twitter Streaming API2 spanning
a 6 months period between March 2014 and September
2014, and online news articles mentioned in tweets during
breaking news hours.
2) Prediction time-frame: The collected data is analyzed and
predictions are made on hourly bases.
3) Prediction goal: To predict the hourly price direction for
the stocks mentioned in tweets during breaking news
hours.
The distinguishing features of our system compared to
systems mentioned in our related work are: (1) [19] used
Tweeter volume spikes alongside stock price-based technical
indicators for stock price turning point prediction where as our
system utilizes textual content of the news mentioned in tweets
during breaking Twitter volume hours to predict the hourly
direction of the stock price following a breakout period. (2)
[16] and [17] used extracted sentiment information alongside
stock price-based technical indicators to see if sentiment
information leads to a boost in the predicted direction accuracy.
Our system primarily relies on textual content of the news from
breaking tweet volume hours to predict price direction. We also
experimented with extracted sentiment as an additional feature
to see if it leads to a boost in overall accuracy. Unlike [16] and
[17], our system did not experience a statistically signiﬁcant
boost in predictive accuracies as a result of including sentiment
information alongside other textual content features. [16]’s

III. P ROBLEM D EFINITION
The correction effect of online news articles covering company related events, announcements and technical analyst
reports on the stock price may take some time to show. Depending on the severity and impact of the news announcement
this period may vary between few minutes to an hour, and
the effect may sometimes determine the trend direction of the
ﬁnancial instrument for upcoming weeks or months.
One way to measure the impact of news on a stock price is to
analyze the trading volume following the news announcement.
Another indicator of news impact is the diffusion rates and
volumes of messages on social media containing the stock
symbol and news links of interest.
Twitter provides a suitable platform to investigate properties
of such information diffusion. Diffusion analysis can harness
social media to investigate “viral tweets” to create earlywarning indicators that can signal if a breakout started to
emerge in its nascent stages. In this paper, we utilize the
68-95-99.7 rule to deﬁne a simple method of tweet volume
breakouts. In statistics, the 68-95-99.7 rule, also known as the
three-sigma rule or empirical rule [5], states that nearly all
values lie within three standard deviations (σ) of the mean
(μ) in a normal distribution. We utilize a ﬁxed sized sliding
window (of length 20 hour intervals that was determined
experimentally), to compute a running average and standard
deviation for the hourly volumes of tweets that mention a
stock symbol. Then, we identify breakout signals within a
time-series of hourly tweet volumes for each stock symbol
whenever its hourly volume exceeds (μ(20) + 2σ) of the

1 http://www.nasdaq.com/symbol/ibm/news-headlines
2 https://dev.twitter.com/streaming/overview

524

Fig. 1.

System Architecture

Fig. 2.

Illustration of System Architecture of Experiment-2

accuracy is not comparable to ours since they are reporting the
daily directional prediction accuracy for the Dow Jones Index
Average (DJIA). Compared to predictive accuracies for four
companies listed in [17], we have only one stock in common
with their experiments, i.e. MSFT, where their system reported
a daily directional predictive accuracy of 69% and our system
reports an hourly directional accuracy of 82%.

experimental results and evaluations are presented in Section
V.C.
A. Experiment-1: Hourly Price Direction Prediction using
Online News
The following is a detailed description of each step used in
Experiment-1:
1) One Hour Stock Chart: We collected hourly stock ﬁnancial price charts for all the companies comprising the Dow
Jones Index (DJI) using an API from ActiveTick 3 . For
each trading hour the price direction was calculated based
on the hourly Open and Close prices according to the
Formula 1 below, where d represents the trading date and
h represents the trading hour:

1 if Open(d, h) ≤ Close(d, h) )
Dir(d, h) =
−1 otherwise
(1)
2) Hourly News: We used Web Content Extractor4 to collect
online news articles from NASDAQ website. We stored
all metadata information related to the articles like their
title, url, date, time, and source in a database table. After
that we fetched the news content using their urls and
performed content extraction using Boilerpipe5 .
3) Feature Extraction:
• N-gram Features from News: R for text mining tm6
package was used to extract keyword features from
the news corpus. First all whitespaces, stop words,
numbers, punctuation were removed from the documents, then all the terms were converted to lowercase
and stemmed into their root words. Next features were
recorded in a document-term matrix. For each stock
symbol we created a pair of document-term matrices:
one with 1-gram features and another with 2-gram features represented in a binary form. We used R.matlab7
package to create Matlab format ﬁles that corresponds
to these matrices.
• Sentiment Features: To detect sentiment in news content we used a Java version of SentiStrength library8 ,

IV. S YSTEM A RCHITECTURE
In order to test the hypothesis that “information in news
indicated by breaking tweet volumes will lead to statistically
signiﬁcant boost in the directional prediction accuracy for
the prices of the relevant stock symbols mentioned in such
articles”, we designed two experiments. In the ﬁrst experiment
we trained a classiﬁer using all stock news articles for a 5 year
period spanning 2010 and 2014 from NASDAQ news website. Figure 1 illustrates the system architecture used for the
ﬁrst experiment. For comparison purposes we experimented
with three different types of features extracted from text: 1gram keywords, 2-gram phrases, and bi-polar sentiment (i.e.
positive and negative) extracted from text. We grouped news
hourly, and categorized each hourly collection as one of two
categories: (1) those that led to a increased stock price or (2)
those that led to a price reduction during the next hour. Next,
we applied a feature selection method to reduce the number
of features to only relevant ones. The details of these steps
are presented in the Section IV.A. Finally we experimented
with two types of text classiﬁers and evaluated their directional
predictive accuracy using 10-fold cross validation. The results
of the ﬁrst experiment utilizing all stock news for all 30
company stocks are reported in Section V.B. In our second
experiment, we tested the directional predictive accuracy of
our classiﬁer (i.e. trained in the ﬁrst experiment above) using
only online articles collected during hourly breaking tweet
volume periods. Figure 2 illustrates the system architecture
used for our second experiment. Steps involved in the second
experiment were hourly proﬁling of the tweets mentioning a
stock symbol, detection of tweet volume breakout periods,
collection of online news mentioned in tweets during the
breaking hours, feature extraction from news, and running of
the classiﬁer to predict the stock price direction of the next
hour following a breaking hour. We compared the accuracies
of the classiﬁers in both ﬁrst and second experiments to test the
validity of our hypothesis. The details of the steps involved in
the second experiment are explained in Section IV.B, and the

3 http://www.activetick.com/
4 http://www.newprosoft.com/
5 http://code.google.com/p/boilerpipe/
6 http://cran.r-project.org/web/packages/tm/index.html
7 http://cran.r-project.org/web/packages/R.matlab/index.html
8 http://sentistrength.wlv.ac.uk/

525

Fig. 3.

An Illustration of News Labeling

SentiStrength is a classiﬁer that uses predeﬁned sentiment word list with human polarity and strength judgments, then it applies some rules to detect sentiment
in short text [20]. [21] showed that using general
word lists for sentiment analysis of large ﬁnancial
text leads into mis-classifying of common words in
ﬁnancial domain, so based on our SentiStrenght initial
testing results and the ﬁndings of [21] we decided
to use Loughran and McDonald Financial Sentiment
Dictionaries 9 instead of the general sentiment word
list supplied with SentiStrenght. Besides using different
sentiment word list we needed to get the sentiment
for each document. We used OpenNLP10 Sentence
Detector to extract sentences from each document,
and then we applied the SentiStrenght classiﬁer on
each sentence. We determined the majority polarity
for the sentences contained in a document and used
the majority polarity (i.e. positive or negative) as the
sentiment for the document.
4) Feature Selection: Feature selection in text mining reduces the number of features to only relevant and discriminative set of features. We used Chi2 [7] feature selection
algorithm from a feature selection package 11 . Chi2 is
a two phase general algorithm that selects automatically
a proper critical value for statistical χ2 test and then it
removes all irrelevant and redundant features [7].
5) News Labeling: Figure 3 is an illustration of the news
labeling step. In this phase we used the stock price direction of the following hour to categorize the directionality
of the hourly collections of news articles. In order to align
the news article hours with the stock chart hours we had
to standardize and adjust their time zones. Formula 2 is
used to label the news articles where d represents the
publishing date, and h represents the publishing hour.
Label(d, h) = Dir(d, N ext(h))

during the next hour. Formula 2 applies to all the news
articles published during ofﬁcial trading hours which
starts at 9AM and ends on 3PM in EST time zone.
For articles that are published during the last trading hour,
or after trading hours, or during holidays and weekends
we assumed that their effect will be seen on the direction
of the ﬁrst trading hour of the next trading day. For this
case Formula 3 is used to label those news articles.
Label(d, h) = Dir(N ext(d), F irst(h))

6) Classiﬁer: We formulate price direction prediction problem as a classiﬁcation problem in a general structured
sparse learning framework [6]. In particular, the logistical
regression formulation presented below ﬁts this application, since it is a dichotomous classiﬁcation problem
(e.g. upwards vs. downwards price correction), In the
formula 4, ai is the vector representation of the news
during the ith hour, wi is the weight assigned to the ith
document (wi =1/m by default), and A=[a1 , a2 , , am ] is
the document n-gram matrix, yi is the directionality of
each hour based upon the stock price action of the next
hour, and the unknown xj , the j-th element of x, is the
weight for each n-gram feature, λ > 0 is a regularization
parameter that controls the sparsity of the solution, |x|1
= Σ|xi | is 1-norm of the x vector. We used the SLEP
[6] sparse learning package that utilizes gradient descent
approach to solve the above convex and non-smooth
optimization problem. The n-grams with non-zero values
on the sparse x vector yield the discriminant factors
for classifying a news collection as leading to upwards
or downwards price correction. n-grams with positive
polarity correspond to upward direction indicators, and
those with negative polarity correspond to downward
direction indicators.
n

minx
wi log(1 + exp(1 + yi (xt ai + c))) + λ|x| (4)

(2)

In this paper we are assuming that the effect of published
news articles will be reﬂected on the stock price direction
9 http://www3.nd.edu/∼mcdonald/Word

(3)

i=1

We also utilized an SVM classiﬁer during our experiments
using LIBSVM12 .

Lists.html

10 https://opennlp.apache.org/
11 http://featureselection.asu.edu/software.php

12 http://www.csie.ntu.edu.tw/∼cjlin/libsvm/

526

Fig. 4.

An Illustration of Breaking Tweets

7) 10-fold cross validation: We run a total of 8 experiments
for each stock symbol where we experimented: (1) with
SVM and sparse logistic regression classiﬁers, (2) with
1-gram and 2-gram features, and (3) with and without
extracted sentiment features. After the training phase
of the classiﬁer, we validated the accuracies using 10fold cross validation. The evaluation results for the ﬁrst
experiment are presented in Section V.B.


Breakout =

T rue
F alse

if N (d, h) ≥ μ[20](d, h) + 2σ(d, h))
otherwise
(5)

In Formula 5 N represents tweet volume on speciﬁc
date d, and hour h, μ[20] is 20-hour simple moving
average applied on tweets’ volume, μ[20](d, h)+2σ(d, h)
represents the upper band for simple moving average - a
20-hour moving average plus 2-times standard deviation.
If the volume of hourly tweets N exceeds the upper band
value, this would indicate a volume breakout. Otherwise
the tweet volume is non-breaking. In Fig. 4, the pair
of dotted arrows shows two instances of tweet volume
breakouts at 9/5/2014 at 9AM and 9/5/2014 at 2PM,
where the corresponding articles from these hours will
be used to predict the price directions of the mentioned
stocks at following hours.
4) News From Breaking Tweets: In this step the news
content of URLs found in the tweets during the breaking hours are downloaded and their textual content is
extracted using the following steps:
a) For each breaking hour of a speciﬁc stock symbol we
fetch the URLs found in tweets during the breaking
hour, i.e. Breakout = True. In some cases the URLs
were mentioned in their short URL forms, so before
fetching them, they were converted to their long forms.
b) Fetch the URL links’ content and perform content
extraction from the HTML documents using the jsoup
HTML parser 13 .
5) Classiﬁer: After extracting the hourly breaking news and
their 1-gram features we utilized the logistic regression
classiﬁer to predict the price direction for the next hour.
6) Evaluation: The predictive accuracies of the classiﬁer for
news following breaking hours are presented in Section
V.C.

B. Experiment-2: Hourly Price Direction Prediction using
Breaking News
We selected the classiﬁer with the best performance emerging from Experiment-1 to use in Experiment-2. Experiment-2
was designed to test if the online news indicated by breaking
tweet volumes would lead to a statistically signiﬁcant boost
in the directional prediction accuracy for the prices of the
relevant stock symbols mentioned in such news. The system
architecture ﬁgure in Fig. 2 shows the steps used in this
experiment. The following is a detailed description of each
step:
1) Twitter Stock Symbol Feed: Twitter streaming API was
used to collect tweets related to companies in the Dow
Jones Index (DJI). In order to collect relevant tweets we
used a keyword ﬁlter made from the stock symbols, either
preﬁxed by a dollar sign ($) or preﬁxed by “NYSE:”
or “NASDAQ:”. For example, the keyword ﬁlter for
Microsoft Corp. is $MSFT or NYSE:MSFT. For each
matching tweet we stored the stock symbol, tweet text,
date, time, and the set of URLs mentioned in the tweet. If
the tweet text contained more than one stock symbol then
we stored the same tweet information for each mentioned
stock symbol.
2) Hourly Tweets Volume Proﬁling: We utilize a ﬁxed sized
sliding window (of length 20 hour intervals) where the
20 hour intervals was determined by conducting several
experiments with different intervals, to compute a running
average μ[20] and standard deviation σ for the hourly
volumes of tweets that mention a stock symbol.
3) Tweets Volume Breakout Hour: We identify breakout
signals within a time-series of hourly tweet volumes for
each stock symbol using Formula 5.

13 http://jsoup.org/

527

TABLE II.

C OUNTS OF C OLLECTED N EWS A RTICLES , T WEETS , AND C ALCULATED B REAKING T WEETS FOR 30 D OW J ONES S TOCK S YMBOLS

Stock Symbol

News

Tweets

Breaking
Tweets

$AXP
$BA
$CAT
$CSCO
$CVX
$DD
$DIS
$GE
$GS
$HD
$IBM
$INTC
$JNJ
$JPM
$KO

1614
2006
1842
1984
2168
1553
1870
2260
1878
1743
2188
2157
2232
1543
1899

15251
19041
19303
26611
15897
11218
25014
31336
52888
18459
80412
30724
18236
33658
22688

155
207
188
285
202
63
199
274
246
164
248
273
224
274
208

TABLE III.

Avg
URLs per
Breaking
Tweets
10
15
18
19
13
11
17
21
40
13
50
23
16
27
21

Stock Symbol

News

Tweets

Breaking
Tweets

$MCD
$MMM
$MRK
$MSFT
$NKE
$PFE
$PG
$T
$TRV
$UNH
$UTX
$V
$VZ
$WMT
$XOM

1879
1183
1573
1733
1080
1841
1781
1784
968
1133
1278
1683
2194
2216
2378

21419
11438
28882
59469
18206
50859
15097
30128
8858
11224
10872
19174
20896
30448
22433

228
122
199
271
196
292
192
263
52
144
68
190
252
236
248

Avg
URLs per
Breaking
Tweets
17
9
39
54
18
57
12
24
7
10
9
13
16
27
15

ACCURACY R ESULTS OF E XPERIMENT 2: H OURLY P RICE D IRECTION P REDICTION USING B REAKING N EWS
Stock
Symbol
$AXP
$BA
$CAT
$CSCO
$CVX
$DD
$DIS
$GE
$GS
$HD
$IBM
$INTC
$JNJ
$JPM
$KO

Experiment-1
Accuracy
67.84%
68.20%
65.37%
67.70%
66.60%
67.93%
65.60%
66.60%
68.30%
68.70%
66.10%
69.60%
64.40%
67.50%
67.90%

Experiment-2
Accuracy
69.66%
79.00%
68.39%
69.55%
65.22%
66.67%
64.75%
66.67%
71.60%
75.00%
80.80%
67.30%
69.20%
74.00%
68.60%

Stock
Symbol
$MCD
$MMM
$MRK
$MSFT
$NKE
$PFE
$PG
$T
$TRV
$UNH
$UTX
$V
$VZ
$WMT
$XOM

V. E VALUATION OF E XPERIMENTS R ESULTS
A. Collected Data
We collected online news articles and stock price charts
related to 30 stock symbols in Dow Jones Index for the period
between October, 2009 and September, 2014. We also collected tweets matching stock symbols for the period between
March, 2014 and September, 2014. The total number of news
articles collected from the NASDAQ website for the 30 stock
symbols in Dow Jones Index is 53,641. The total number of
collected tweets matching stock symbols is 780,139. Table II
shows the counts of news articles, total number of collected
tweets, and the number of hourly breakout periods for each
symbol.

Experiment-1
Accuracy
67.80%
70.60%
69.30%
72.60%
70.30%
70.70%
70.80%
75.80%
65.40%
68.40%
67.80%
71.20%
65.30%
66.60%
65.20%

Experiment-2
Accuracy
65.30%
76.50%
64.80%
81.80%
71.30%
59.50%
72.70%
82.90%
75.00%
75.20%
58.60%
71.00%
72.80%
71.00%
67.30%

and LogisticR experiments. Also, the experimental setup with
the LogisticR classiﬁer using 1-Gram features where the sentiment features were excluded led to the maximal accuracies
in 19 out of 30 cases. The second best experimental setup that
achieved the maximal accuracies was also with the LogisticR
classiﬁer with 1-gram features integrated with the sentiment
feature.
Hence, in order to determine the utility of extracted sentiment features we formulated the following hypotheses and
applied Non-parametric sign test [22] at conﬁdence level
95% to test if sentiment features would yield a statistically
signiﬁcant boost in overall prediction accuracies:
1) Null Hypothesis (h0): 1-gram LogisticR classiﬁer without
sentiment features accuracies’ mean = 1-gram LogisticR
classiﬁer with sentiment accuracies’ mean, indicating that
they are at the same level of performance.
2) Alternative Hypothesis (h1): 1-gram LogisticR classiﬁer
without sentiment features accuracies’ mean = 1-gram
LogisticR classiﬁer with sentiment features accuracies’
mean, indicating that they are not at the same level of
performance.
The p-value of the sign test to compare 1-gram LogisticR
classiﬁer without sentiment features with 1-gram LogisticR
classiﬁer with sentiment features at signiﬁcance level 0.05
equals to 0.1221, which leads to the acceptance of the null
hypothesis h0 and the rejection of the alternative hypothesis h1,
concluding that using sentiment would not yield a statistically

B. Experiment-1: Hourly Price Direction Prediction using
Online News
We executed the steps described in Figure 1 on data sets
collected for the 30 Dow Jones Index companies. In order
to identify the best set of text features and the best classiﬁer
we had to perform several experiments. We run a total of 8
experiments for each stock symbol where we experimented: (1)
with SVM and sparse logistic regression classiﬁers, (2) with
1-gram and 2-gram keyword features, and (3) with and without
extracted sentiment for documents. After the training phase of
the classiﬁer, we validated the accuracies using 10-fold cross
validation.
The evaluations show that 1-gram features led to higher
overall accuracies compared to 2-gram features for both SVM
528

TABLE IV.

E XPERIMENT 2 F INANCIAL E VALUATION

$AXP
$BA
$CAT
$CSCO
$CVX
$DD
$DIS
$GE
$GS
$HD
$IBM
$INTC
$JNJ
$JPM
$KO
$MCD
$MMM
$MRK
$MSFT
$NKE
$PFE
$PG
$T
$TRV
$UNH
$UTX
$V
$VZ
$WMT
$XOM

Gross Proﬁt
Points
$14.53
$67.77
$30.67
$13.00
$22.96
$2.62
$21.80
$9.47
$39.45
$21.79
$75.25
$14.54
$27.08
$24.12
$12.63
$23.98
$23.96
$24.67
$26.87
$25.80
$19.17
$24.97
$14.81
$11.26
$22.34
$5.05
$84.70
$16.01
$19.38
$34.84

Gross Loss
Points
-$7.99
-$18.79
-$15.51
-$4.96
-$14.74
-$1.66
-$9.65
-$3.26
-$13.30
-$5.82
-$14.28
-$5.96
-$12.63
-$9.69
-$6.37
-$14.28
-$9.36
-$14.13
-$6.21
-$8.95
-$10.24
-$8.80
-$2.76
-$1.73
-$9.23
-$3.34
-$28.25
-$4.97
-$8.63
-$14.86

Total Proﬁt
Points
$6.543
$48.99
$15.16
$8.03
$8.22
$0.96
$12.15
$6.21
$26.15
$15.97
$60.97
$8.58
$14.45
$14.43
$6.26
$9.70
$14.60
$10.54
$20.66
$16.85
$8.93
$16.18
$12.05
$9.53
$13.11
$1.71
$56.45
$11.03
$10.75
$19.98

Average

$25.85

-$9.68

$16.17

Stock Symbol

signiﬁcant boost in the overall prediction accuracy.

155
207
188
285
202
63
199
274
246
164
248
273
224
274
208
228
122
199
271
196
292
192
263
52
144
68
190
252
236
248

Long Position
(won%)
71%
74%
66%
69%
76%
67%
59%
76%
74%
73%
76%
73%
65%
67%
67%
72%
71%
67%
88%
62%
60%
67%
85%
71%
68%
50%
75%
76%
75%
72%

Short Position
(won%)
68%
87%
81%
71%
58%
67%
83%
64%
70%
85%
93%
58%
84%
94%
75%
62%
96%
63%
77%
87%
59%
94%
82%
86%
91%
86%
68%
71%
68%
62%

205

70%

76%

(ROI%)

Total Trades

7%
47%
16%
39%
7%
1%
16%
27%
18%
22%
37%
35%
15%
29%
17%
10%
11%
20%
65%
25%
35%
22%
41%
21%
19%
1%
31%
26%
15%
22%
23%

tested on the hourly stock chart for its trade entries and exists
whenever: (1) there was a hourly Tweet volume breakout for
Tweets matching a stock’s symbol, with (2) a trade ﬁred in the
direction (e.g. a long/buy or short/sell trade) indicated by our
classiﬁer at the beginning of the following hour for a duration
of 1 hour. For each company, we calculated the amounts
for the total number of trades, gross proﬁt points, gross loss
points, and the total proﬁt points alongside percentage of long
positions (won%), and percentage of short positions (won%).
Table IV shows the results of the simulated back-test evaluations. The results shows that during this period our system was
proﬁtable overall on its recommended hourly trades with each
stock symbol. For stocks, one point loss or gain equals one
dollar loss or gain. In Table IV, the total value of gross proﬁt
points is $775.49 and the average value of gross proﬁt points
is $25.85. Similarly, the total value of gross loss points is $290.35, and the average value of gross loss points is -$9.68.
The total value of proﬁt points is $485.14 and the average
value of total proﬁt points is $16.17.
Since each stock has a different stock price, we also performed simulated trading using a diversiﬁed portfolio based
on equal exposure to risk or gains from each stock in order to
calculate the total and monthly average return on investment
(ROI%) using this trading system. The simulated trades show
that, this system would trigger a total of 6,163 trades during
these 6 months period with a winning ratio of 70% for
its long/buy directional trades and 76% winning ratio for
its short/sell directional trades. On the average this system
triggers 35 monthly trades per stock. Trading with an equally
diversiﬁed portfolio yields a total (ROI%) of 23% for 6 months,
indicating an average monthly (ROI%) of 3.83%. The highest
total (ROI%) achieved was 65% by executing 271 trades with

C. Experiment-2: Hourly Price Direction Prediction using
Breaking News
In Experiment-2 we applied steps outlined in Figure 2 to
30 stock symbols in Dow Jones Index using breaking news
periods only as trade triggers. Table III shows that Experiment2 led to a boost in predictive accuracies for 70% of the
stock symbols (i.e. 21 out of 30 cases). In order to prove
that Experiment-2 yields a statistically signiﬁcant boost in
prediction accuracy compared to Experiment-1 we applied
Sign test at conﬁdence level 95%. We formulated the following
hypothesis:
1) Null Hypothesis (h0): Experiment-1 accuracies mean =
Experiment-2 accuracies mean, indicating that they are at
the same level of performance.
2) Alternative Hypothesis (h1): Experiment-1 accuracies
mean = Experiment-2 accuracies mean, indicating that
they are not at the same level of performance.
The p-value of the sign test to compare Experiment-1 with
Experiment-2 at signiﬁcance level 0.05 equals to 0.0357, which
leads to the rejection of the null hypothesis h0 and the accepting of the alternative hypothesis h1 thus conﬁrming that using
1-gram based LogisticR classiﬁer with breaking news yields a
statistically signiﬁcant boost in directional prediction accuracy
for 30 DJI stocks compared to using the same classiﬁer with
all of the stock news every hour.
Beside evaluating directional accuracies, we also performed
a simulated ﬁnancial evaluation of the proposed trading system
by back-testing its trades and accounting for its return on
investment (ROI%) for a period of 6 months, between March
2014 and September 2014. In this simulation it assumed that no
commissions or fees are charged in each trade. The system was
529

Microsoft ($MSFT), and the lowest total (ROI%) was 1% by
executing 63 trades with DuPont ($DD) and 68 trades with
United Technologies ($UTX).

[12]

VI. C ONCLUSION AND F UTURE W ORK
In this paper we proposed a system to predict the hourly
stock price direction based on the textual analysis of news
articles’ content mentioning a stock symbol. Firstly, we showed
that using LogisticR classiﬁer with 1-gram keyword features
leads to the best overall directional prediction accuracy. Secondly, we showed that using extracted document-level sentiment features does not yield to a statistically signiﬁcant
boost in directional predictive accuracies in the presence
of 1-gram keyword features selected by a Chi2 [7] feature
selection algorithm. Thirdly, we show that the breaking news
based system indeed yields a statistically signiﬁcant boost
in prediction accuracy compared to the one using all news
indiscriminately. As future work, we will perform several
experiments by varying time frames (i.e. 1 minute, 5 minutes,
15 minutes, 30 minutes, 1 hours, 4 hours and daily) for
both grouping tweets and detecting their breakouts, and for
determining the best time-frame on stock price charts that leads
to the highest predictive accuracies. Furthermore, inspired by
related work which utilize technical stock price indicators, we
also propose to utilize indicators such as price-level breakouts,
moving averages and candle-stick patterns to test if they would
led to a statistically signiﬁcant boost in predictive accuracies.
We also plan to experiment with different categories of stocks
(i.e. large cap, mid cap and small cap) to determine their
sensitivities to online news and social-media breakouts.

[13]

[1]
[2]

[3]

[4]

[5]
[6]
[7]
[8]

[9]

[10]
[11]

[14]
[15]

[16]
[17]
[18]

[19]
[20]
[21]
[22]

R EFERENCES
R. Compton, C. Lee, J. Xu, L. Artieda-Moncada, T.-C. Lu, L. Silva,
and M. Macy, “Using publicly visible social media to build detailed
forecasts of civil unrest,” Security Informatics, vol. 3, no. 1, 2014.
X. Wang, D. Brown, and M. Gerber, “Spatio-temporal modeling of
criminal incidents using geographic, demographic, and twitter-derived
information,” in International Conference on Intelligence and Security
Informatics, ser. Lecture Notes in Computer Science, IEEE Press. IEEE
Press, 2012.
S. Asur and B. A. Huberman, “Predicting the future with social media,”
in Proceedings of the 2010 IEEE/WIC/ACM International Conference
on Web Intelligence and Intelligent Agent Technology - Volume 01, ser.
WI-IAT ’10. Washington, DC, USA: IEEE Computer Society, 2010,
pp. 492–499.
H. Achrekar, A. Gandhe, R. Lazarus, S. Yu, and B. Liu, “Twitter improves seasonal inﬂuenza prediction,” in HEALTHINF 2012 Proceedings of the International Conference on Health Informatics,
Vilamoura, Algarve, Portugal, 1 - 4 February, 2012., 2012, pp. 61–70.
F. Pukelsheim, “The three sigma rule,” The American Statistician,
vol. 48, no. 2, pp. 88–91, 1994.
J. Liu, S. Ji, and J. Ye, “Slep: Sparse learning with efﬁcient projections,”
Arizona State University, vol. 6, p. 491, 2009.
H. Liu and R. Setiono, “Feature selection via discretization,” IEEE
Transactions on Knowledge and Data Engineering, vol. 9, no. 4, pp.
642–645, 1997.
J. Patel, “Predicting stock market index using fusion of machine
learning techniques,” Expert Systems with Applications, vol. 42, no. 4,
pp. 2162; 2162–2172; 2172, 2015, doi: 10.1016/j.eswa.2014.10.031
pmid:.
J. Gong and S. Sun, “A new approach of stock price prediction based on
logistic regression model,” in New Trends in Information and Service
Science, 2009. NISS ’09. International Conference on, 2009, pp. 1366–
1371, iD: 1.
S. S. Roy, D. Mittal, A. Basu, and A. Abraham, “Stock market
forecasting using lasso linear regression model,” in Afro-European
Conference for Industrial Advancement. Springer, 2015, pp. 371–381.
A. K. Nassirtoussi, S. Aghabozorgi, T. Y. Wah, and D. C. L. Ngo, “Text
mining of news-headlines for forex market prediction: A multi-layer
dimension reduction algorithm with semantics and sentiment,” Expert
Systems with Applications, vol. 42, no. 1, pp. 306–324, 1 2015.

530

M. Hagenau, L. Michael, and D. Neumann, “Automated news reading:
Stock price prediction based on ﬁnancial news using context-capturing
features,” vol. 55, no. 3, /2013 06, pp. 685; 685–697; 697, doi:
10.1016/j.dss.2013.02.006 pmid:.
M. Y. Kaya and M. E. Karsligil, “Stock price prediction using ﬁnancial
news articles,” in Information and Financial Engineering (ICIFE), 2010
2nd IEEE International Conference on. IEEE, 2010, pp. 478–482.
R. Schumaker and H. Chen, “Textual analysis of stock market prediction
using ﬁnancial news articles,” AMCIS 2006 Proceedings, p. 185, 2006.
S. Lauren and S. D. Harlili, “Stock trend prediction using simple moving
average supported by news classiﬁcation,” in Advanced Informatics:
Concept, Theory and Application (ICAICTA), 2014 International Conference of. IEEE, 2014, pp. 135–139.
J. Bollen, H. Mao, and X. Zeng, “Twitter mood predicts the stock
market,” Journal of Computational Science, vol. 2, no. 1, pp. 1–8, 2011.
T.-T. Vu, S. Chang, Q. T. Ha, and N. Collier, “An experiment in
integrating sentiment features for tech stock prediction in twitter,” 2012.
Y. Mao, W. Wei, B. Wang, and B. Liu, “Correlating s&p 500 stocks with
twitter data,” in Proceedings of the First ACM International Workshop
on Hot Topics on Interdisciplinary Social Networks Research. ACM,
2012, pp. 69–72.
Y. Mao, W. Wei, and B. Wang, “Twitter volume spikes: analysis and
application in stock trading,” in Proceedings of the 7th Workshop on
Social Network Mining and Analysis. ACM, 2013, p. 4.
M. Thelwall, K. Buckley, and G. Paltoglou, “Sentiment strength detection for the social web,” Journal of the American Society for Information
Science and Technology, vol. 63, no. 1, pp. 163–173, 2012.
T. Loughran and B. McDonald, “When is a liability not a liability?
textual analysis, dictionaries, and 10ks,” The Journal of Finance, vol. 66,
no. 1, pp. 35–65, 2011.
E. L. Lehmann, Nonparametrics :statistical methods based on ranks.
San Francisco: Holden-Day, e. L. Lehmann, with the special assistance
of H. J. M. D’Abrera.; ;24 cm; Includes bibliographical references and
index.

Situation-Awareness for Adaptive Coordination
in Service-based Systems
S. S. Yau, D. Huang, H. Gong, and H. Davulcu
Arizona State University,
Tempe, AZ 85287-8809, USA
{yau, dazhi.huang, haishan.gong, hasan.davulcu}@asu.edu

Abstract
Service-based systems have many applications,
including collaborative research and development, ebusiness, health care, environmental control, military
applications, and homeland security. Service coordination
is required for these systems to coordinate distributed
activities. To achieve adaptive service coordination under
changing environment and workload, situation-awareness
is needed. In this paper, a model is presented for situationawareness (SAW) requirements in service-based systems.
Based on this model, SAW agents are developed to
incorporate
situation-awareness
and
adaptive
coordination in service-based systems.
Keywords: Situation-awareness requirements, serviceoriented architecture, adaptive service coordination,
situation-awareness agents, service-based systems.

1. Introduction
Service-based systems have the major advantage of
enabling rapid composition of distributed applications,
regardless of the programming languages and platforms
used in developing and running the applications. ServiceOriented Architecture [1] has been adopted in many
distributed systems, such as Grid and Global Information
Grid (GIG) [2], in various application domains, including
collaborative research and development, e-business, health
care, environmental control, military applications and
homeland security. In these systems, various capabilities
are provided by different organizations as services and
interconnected by various types of networks. We consider
a service as a software/hardware entity with well-defined
interfaces to provide certain capability over wired or
wireless networks using standard protocols, such as HTTP
and SOAP (Simple Object Access Protocol). The services
can be integrated following specific workflows, which are
series of cooperating and coordinated activities designed to
achieve users’ goals. Service coordination is required to
ensure the correctness of workflow execution. Service
coordination is a process of locating participant services,

monitoring their status, invoking proper services, and
propagating necessary information among them to ensure
the correct results obtained from the coordinated
participant services. In service-based systems, service
coordination needs to be adaptive because (1) services
may be unavailable or cannot provide desirable QoS due to
distributed denial-of-service attacks, system failures or
system overload, (2) workflows may need to be adapted
when the situation changes in order to satisfy the
requirements, and (3) new workflows may be generated in
runtime to fulfill users’ new requirements.
To achieve adaptive service coordination, situationawareness (SAW), which is the capability of being aware
of situations and adapting the system’s behavior
accordingly [3, 4], is needed for checking whether a
service can be invoked and adapting new workflows. A
situation is a set of contexts in the application over a
period of time that affects future system behavior [3, 4]. A
context is any instantaneous, detectable, and relevant
property of the environment, the system, or users, such as
location, available bandwidth and a user’s schedule.
In this paper, we will present a model for SAW in
service-based systems. Based on this model, we will
develop SAW agents to incorporate SAW and adaptive
coordination in service-based systems.

2. Current State of the Art
Situation-awareness has been studied in artificial
intelligence [5], human-computer interactions [6] and data
fusion community [7]. Situation Calculus and its
extensions [8-11] were developed for describing and
reasoning how actions and other events affecting the
world. A situation is considered as a complete state of the
world and cannot be fully described, which leads to the
well-known frame problem and ramification problem [9].
A core SAW ontology [12, 13] refers a situation as a
collection of situation objects, including objects, relations
and other situations. However, it does not address how to
verify the specification and perform situation analysis.

Proceedings of the 29th Annual International Computer Software and Applications Conference (COMPSAC’05)
0730-3157/05 $20.00 © 2005 IEEE

Substantial work has been done on service coordination
[14-18]. Industrial standards, such as WS-Coordination
[14] and WS-CF [15], aim at providing standard and
extensible coordination frameworks to support coordinated
workflows on web services, but do not provide techniques
for achieving adaptive service coordination. To coordinate
distributed systems, a formal specification framework [16]
was developed for modeling dynamically changing
contexts and rules in reactive systems. MARS [17]
promotes context dependent coordination by incorporating
programmable coordination media in distributed systems.
EgoSpaces [18] introduced a coordination model and a
middleware for specifying and managing agent-centered
contexts to facilitate easy application development in
mobile ad hoc environments. These approaches only use
current contexts in service coordination and do not
consider variations of contexts over a period of time,
which are important information for service coordination.

3. Our Approach
In this section, we will present our approach to
incorporating SAW for adaptive coordination in servicebased systems. It consists of two major parts:
(1) Modeling and specifying SAW for adaptive service
coordination.
(2) Developing SAW agents for adaptive service
coordination.
It is assumed that there is a mission planner (MP) [20,
21] or its equivalent in a service-based system, which
accepts goals specified by users and generates execution
plans based on available services and current situation.
The generated execution plan is a series of service
compositions to be executed in order to fulfill the overall
goal. A step (service invocation) in the execution plan may
have certain dependencies on situations, i.e., a step can be
executed only when a certain situation is detected. The
execution plan can be decomposed [19] and delivered to
SAW agents for execution. Figure 1 depicts the
Specify

Mission
Goal

Mission Planner (MP)

y
er
Situations /
Qu
Execution Results

Execution Plan

interactions among services, SAW agents, and the MP.
An MP must be able to perform planning based on
partial domain information, i.e. the MP does not know all
the information related to the execution environment and
services at the time of planning. This requirement for MP
is needed because there is usually no central control on
adding/removing services in service-based systems and
services may be unavailable without notifying users. This
requirement makes traditional planners, such as TAL
planner [20] unsuitable. Instead, planners with CTR-S [21]
or other adaptive workflow synthesis techniques [22]
should be used. However, a consequence of planning with
partial domain information is that although the generated
workflows are always sound at the time of planning, these
workflows might be non-executable during their execution
due to dependency violations in a service invocation
caused by situation changes by uncontrollable external
agents. Due to this difficulty, we develop SAW agents to
coordinate the services in the execution plan. In the
following subsections, we will present our model for SAW
and the design of SAW agents.

3.1. Modeling and Specifying SAW for Adaptive
Service Coordination
We consider a service as a process, which can accept
inputs from other processes and produce outputs. Hence, a
service-based system can be considered as a collection of
parallel processes, each of which can send/retrieve data
to/from other processes. Consequently, the service
coordination in such a system becomes the coordination of
these parallel processes.
Before we present the SAW agents, we need to model
SAW for adaptive service coordination, which includes
Trigger

Is a

Directories

Query

Re
gis
te
r

Atom ic
Situation

Situation
Operator

compose

(2)

Composite
Situation

compose

compose

(3)

compose

Argument

Context
Operator

has

Context
Type

Defined on has

value of

constant

value of
Context
Instance

instance of
has

Context
Name

has
Context
Value

in

has

has

Time
Stamp

Location
Identifier

in

Context Value
Domain

Services

Figure 1. SAW agents for adaptive
service coordination

Is a

Situation

location of

Monitor /
Coordinate

Change

Prefer

Context

SituationAwareness
Agents (SAA)

(1)

Process

Precondition of

Figure 2. A conceptual view of our model for
SAW for adaptive service coordination

Proceedings of the 29th Annual International Computer Software and Applications Conference (COMPSAC’05)
0730-3157/05 $20.00 © 2005 IEEE

two aspects: (1) modeling situations, and (2) modeling the
relations between situations and processes.
Figure 2 shows a conceptual view of our model. Since
context acquisition and operations on contexts are highly
domain-specific and often involve low-level system
processes, our model will not include the ways contexts
are collected and the semantics of operations on contexts.
Instead, we assume that each context is collected
periodically by invoking at least one service in a servicebased system, and that a service that can collect a context
also implement operations for preprocessing this context.
Def. 1: A context is a measurable property of the
environment, the system or users:
x ci is the unique name of a context
x Wi = contextType(ci), the context type of ci.
x Di = domainOf(Wi), the value domain of Wi.
Def. 2: A context instance Iix of the context ci is a
quintuple (ci, Wj, vk, tx, lm), where Wj = contextType(ci), vk 
domainOf(Wj), tx is a timestamp, and lm is a location
identifier. Given Iix = (ci, Wj, vk, tx, lm), the following two
functions are defined: valOfIns(ci, tx) = vk, which returns
the value of a context at a particular time, and locOfIns(ci,
tx) = lm, which returns where the context is measured.
Def. 3: An argument arg is one of the following:
x a constant value in a context value domain Di
x a variable ranging over a context value domain Di
x valOfIns(ci, t), in which t is a time variable
x locOfIns(ci, t), in which t is a time variable
An arg is bounded if it is a constant, a variable with a
value vx ( Di) assigned to it, or the return value of
valOfIns(ci, t) or locOfIns(ci, t) at a given time stamp t.
Def. 4: Given a set of arguments, {arg1, …, argn}  Di,
two types of context operators are defined as follows:
x Boolean operators: opi(arg1, …, argn) = true | false
x Value operators: opj(arg1, …, argn) = v  Di
Def. 5: A term is either an application of a context
operator, op(arg1, …, argn), or a nested application of
context operators, op(term1 | arg1, …, termn | argn).
Def. 6: An atomic situation, aSi is a term which returns
boolean values. It can be expressed as follows: aSi(x1, …,
xm) { op(arg1, …, argn) | op(term1 | arg1, …, termn | argn),
where op is a boolean operator, and x1, …, xm are all
unbounded arguments of op.
Def. 7: A composite situation, cSi, is defined as follows:
1) cSi { aSx; 2) cSi {  cSx | cSx  cSy | cSx  cSy; 3) cSi {
P(cSx, Z, H): cSx was true sometime within [now-Z, nowZ+H]; 4) cSi { H(cSx, Z, H): cSx was always true within
[now-Z, now-Z+H]; 5) cSi { Know(cSx, M): The process M
knows that cSx is true; 6) All composite situations are
defined by recursively applying (1) – (5). , , , P, H,
and Know are situation operators.

Def. 8: Let s0 and s1 be situations, and V and M processes.
Five basic relations between situations and processes are
defined as follows:
1) precondition(M, s0): s0 must be true when M can be
executed. 2) do(M, s0, s1): The execution of M makes s1 true
when s0 is true. 3) trigger(V, M, s0): When V knows that s0
is true, V triggers M. This relation models the reactive
behaviors of processes. 4) tell(V, M, s0): V sends M the
information about s0. This relation models the knowledge
sharing between processes. 5) prefer(s0, V, M): When s0 is
true, it is preferable to use V instead of M. This relation
models the preferences on the usage of processes.
Definition 9: A model M for SAW in a service-based
system is a tuple (C, T, L, S, ), R, CH), where C is the set
of definitions of the contexts in the system, T is the set of
timestamps appeared in the system since the system started
to run, L is the set of possible location identifiers in the
system, S is the set of definitions of the situations in the
system, ) is the set of processes in the system, R is the set
of relations between situations and processes defined in
the system, and CH is the context history, which consists
of instances of the contexts in the system.
M is an abstract model since the actual representation
of T and L, and the size of CH depends on the system to be
modeled.
Our model for SAW in service-based systems has
strong expressive power because of the following reasons:
 Based on Defs 4 and 7, our model can capture
temporal relations among instances of contexts.
 Based on Def. 8, our model allows service providers
and developers to define the situations that trigger,
allow or prohibit the execution of processes in the
service-based system.
 Based on Def. 8, our model allows users to express
their preferences on the usage of services.
 Using the five basic relations in Def. 8, our model
allows modeling control structures, which are
commonly used in service coordination.
 Our model can be used to express the situation that
timestamped common knowledge [23], which is very
important for the coordination of distributed
processes, is attained among distributed processes.
The following are the properties of our model M, which
are useful in developing the SAW agents:
P1) Given M for SAW in a service-based system, the
definition of a situation s0, and a timestamp t0 in T, the
question “Does M satisfy s0 at t0?” is decidable.
P2) Given M for SAW in a service-based system, and the
definitions of situations s0 and s1, the question “Does
s0 implies s1?” is decidable.
P3) Given M for SAW in a service-based system, the
definition of a situation s0, and the fact that M has not
satisfied s0 since the system started to run, the
question “Is it possible that M will satisfy s0 sometime

Proceedings of the 29th Annual International Computer Software and Applications Conference (COMPSAC’05)
0730-3157/05 $20.00 © 2005 IEEE

in the future?” is decidable if the contexts involved in
the definition of s0 have finite context value domains.
Based on our model, a formal specification language
can be derived to specify SAW in service-based systems.
The specifications will be used by the MP to generate
suitable execution plans. As a proof-of-concept, we will
show an example in Section 4 using Transaction F-Logic
[24] to specify SAW requirements.

3.2 Design of SAW Agents
SAW agents are distributed autonomous software
entities, which should have the following capabilities to
support situation analysis and service coordination:
C1) Participant service management. SAW agents
should be able to accept “join” or “leave” requests
from participant services, monitor their status, select
and invoke appropriate services when needed, and
report their status to other agents or the MP.
C2) Agent discovery. An SAW agent should advertise its
existence and allow other agents or MP to query its
configuration, including participant services managed
by the agent, and context and situation information
provided by the agent. This is necessary for the
cooperation of multiple SAW agents and the MP to
support situation analysis and service coordination.
C3) Context acquisition and situation analysis. An
SAW agent should collect contexts from its
participant
services
and
analyze
situations
continuously based on its configuration.
With these capabilities, SAW agents can adaptively
coordinate services in execution plans as follows:
(1) In each step of execution, SAW agents check whether
all the dependencies on situations are satisfied.
(2) If the dependency on a situation is not satisfied, SAW
agents will check whether the step can be undone.
(3) If the step is undoable, SAW agents will first undo the
step and then search for an alternative service.
(4) If an alternative service is found, SAW agents will
resume the execution using the alternative service.
Otherwise, SAW agents will notify MP to do the replanning to find an alternative workflow.
Multiple SAW agents in a service-based system will
form a hierarchy to process situation information and
coordinate workflow execution. In this hierarchy, the
agents in lower levels often do not have direct interactions
with users, and perform some simple tasks, such as
collecting contexts, recognizing atomic situations, or
controlling the invocation of one or several services. The
agents in higher levels usually have more interactions with
users and need to perform more complex tasks, such as
recognizing composite situations, discovering other agents
and services for coordinating the workflow execution, and
adapting workflows. Hence, agents at higher levels need to
be reconfigured more often than the agents in lower levels.

Based on this, two types of SAW agents are used in the
system to achieve a balance between reconfigurability and
performance:
(I) SAW agents with internal knowledge bases. An
SAW agent with an internal knowledge base stores the
definitions of contexts and situations, and the relations
between situations and processes in its knowledge base.
Such an SAW agent performs situation analysis, and
determines processes to be triggered by reasoning with
knowledge in its knowledge base. Depending on the
formal language based on our model, a corresponding
inference engine can be used to provide reasoning support.
For example, if we choose Transaction F-Logic to specify
SAW in a service-based system based on our model,
Flora-2 [25] can be used.
This type of SAW agents can be easily reconfigured by
updating its internal knowledge base, and can support the
adaptation of workflows. However, such SAW agents are
heavyweight and it is difficult to maintain consistency of
the knowledge bases of multiple agents, especially when
the number of agents is large. Hence, we only use this type
of SAW agents in higher levels of the hierarchy, where
only a few high level agents with more frequent
reconfiguration are expected.
(II) SAW agents as finite state machines (FSM). In [26],
agents are modeled as FSMs, and they are used to monitor
certain variables, change their states based on the update
values of monitored variables, and output some variables
monitored by other agents during the transition from one
state to another. It can be easily shown that situations
defined by our model can be detected by FSMs:
o An atomic situation can be detected by a two-state
A two-state machine M0 for an atomic situation aS0 = op(arg1, ? argn)
arg1

op/n is false

...
...

argn

op/n is true

op/n is true
aS0 is true/false

False

True
op/n is false

Figure 3. A state machine for detecting an
atomic situation
machine as shown in Figure 3.
o A composite situation can be detected by a more
complex FSM generated by combining FSMs for
detecting the atomic situations.
This type of SAW agents is lightweight, does not
require maintenance of their internal knowledge bases, and
can be automatically generated from the model of SAW.
However, the reconfiguration of such agents is difficult. In
order to improve the performance of the system, we use
this type of SAW agents in lower levels of the hierarchy,
where less reconfiguration is expected.
There is no clear boundary in the hierarchy of SAW
agents indicating which types of agents should be used at
what level. Further investigation is needed to analyze the

Proceedings of the 29th Annual International Computer Software and Applications Conference (COMPSAC’05)
0730-3157/05 $20.00 © 2005 IEEE

tradeoff between reconfigurability and performance, and
develop design methods for generating optimal or suboptimal designs of service-based systems using SAW
agents for adaptive service coordination.

4. An Example
To illustrate our approach, consider the following “ship
rescue” example: A passenger ship (BS) with 200
passengers on board has trouble and may be sinking. There
are two nearby ports P1 and P2, and each port has a
hospital. There are two rescue ships, A and B at P1 and P2
respectively. Both ships have enough capacity to hold 200
passengers, but only B has a helicopter H on-board. A
rescue center RC is responsible for coordinating rescue
operations. Five services are involved in this example:
x SBS: the service on BS for sending out SOS signal with
various context data, including the number of lifethreatening injuries.
x SH: the rescue service which controls the helicopter H
on B to perform rescue operations.
x SA: the rescue service which controls A to perform
rescue operations.
x SB: the rescue service which controls B to perform
rescue operations. SB may invoke SH.
x SRC: the service at the rescue center RC which receives
information from other services, analyzes the received
information and plans rescue operations.
Upon detecting the situation “An SOS signal is detected
from ship BS”, SRC automatically generates a plan to
perform necessary rescue operations for BS. At first, there
is no injury report about the passengers from BS. SRC
discovers the nearby rescue services (SA and SB), and
selects SA to perform the rescue mission since A is closer
to BS. However, after A is on the way to BS, another report
from BS comes indicating several life-threatening injuries
caused by an explosion just happened on BS. A is unable
to reach BS and return to P1 fast enough to save the
injured passengers. Hence, SRC finds SB and SH to rescue
the injured passengers on BS.

We will use Transaction F-Logic to specify the SAW
requirements in the example. Due to the limited space,
only parts of the specifications are shown here:
/* ontology for context classes*/
ship[status(t)*=>string, shipLoc(t)*=>location].
passengerShip::ship[ life_threaten(t)=>integer,
passengers=>> passenger,
lifeThreaten_passsengers=>>passenger].
rescueShip::ship[helicop=>helicopter,
load(passengers)=>boolean, …].
……
/* Rules for situations*/
/* BShip is at helicopter’s reachable range*/
withinRange(RShip, PShip, Port, Time):RShip:rescueShip, PShip:passengerShip,
RShip[helicopĺH], H[rangeĺD],
distance(RShip, PShip, Time, D1),
distance(PShip, Port, Time, D2), D1 + D2 < D.
……
/* Facts */
BS:passengerShip [status(0)ĺ"Normal", injury(0)ĺ0,
life_threaten(0)ĺ0, … ].
……
BS:passengerShip [status(68)ĺ"SOS ", injury(68)ĺ38,
life_threaten(68)ĺ5, … ].
……

 Situation-triggered mission planning
Two SAW agents, a rescue center agent (RC agent) and
a ship agent were developed to monitor SRC and ships (A,
B, BS) respectively. Each of them maintains its internal
knowledge base storing collected contexts. The RC agent
periodically checks the situation “An SOS signal is
detected from a passenger ship” by querying
detectSOS(PShip, Now) from its knowledge base.
Whenever the result is true, the RC agent will send the
situation and PShip’s information to the MP to trigger a
workflow planning process.
We assume that the MP uses CTR-S for workflow
planning as shown in Figure 4. The CTR-S formula F1
states that a rescue ship service (rescueShip) that satisfies
all dependencies (Dependencies) and the goal (GOAL)
should be identified and
(F1) rescued(passengerShip) m rescueShip  Dependencies  GOAL
scheduled. In this example,
Depedencies : [detectSOS(passengerShip,Now)] o A  B
two rescue ship services (A
 [lifeThreaten(passengerShip,Now)] o B
and B) are available.
 [withinRange(rescueShip,passengerShip,port, Now)] o B.helicop
F1 is used for workflow
[detectSOS(passengerShip,Now)] o rescueShip.load(passengerShip.passengers)
GOAL :
generation. F2, F3 and F4 are
 [lifeThreaten(passengerShip,Now)] o (rescueShip.load(passengerShip.passengers)
the generated workflows. The
 rescueShip.helicop.load(passengerShip.lifeThreaten_passengers))
Dependencies in F1 indicates
(F2) wf1 m A.moveTo(BS.shipLoc)  (([lifeThreathen(BS, Now)]  A.load(BS.passengers))
that when SOS signal is
 ([lifeThreaten(BS, Now)]  rescueShip))
detected, either A or B could
(F3) wf new m wf1  A
be used to rescue the passenger
(F4) wf2 m B. moveTo(BS.shipLoc) | ([withinRange(B, BS, P1, Now)] o B. helicop.flyTo(BS.shipLoc))
ship. If there are passengers
 B.helicop.load(BS.lifeThreaten_passengers)  B.load(BS.passengers)
have life threatening juries, B
Figure 4. CTR-S formulas for workflow generation (F1)
should be used. Helicopter onand generated workflows (F2-F4)
board could only be sent out

Proceedings of the 29th Annual International Computer Software and Applications Conference (COMPSAC’05)
0730-3157/05 $20.00 © 2005 IEEE

when BS is within H’s reachable range. The GOAL
indicates that use a rescue ship to load passengers if no life
threatening injury is detected. Otherwise, use a rescue ship
with helicopter on-board to load injured passengers.
F2 shows the initial workflow, in which A needs to
move to the location of BS first, and load the passengers
on BS if no passenger has life threatening injury.
Otherwise, another rescue ship (with helicopter on-board)
needs to be used.
 Workflow execution
Workflow defined by F2 is sent to the ship agent,
which monitors the two rescue ship services (A and B).
The ship agent finds A and invokes its moveTo action. At
the same time, the ship agent periodically queries whether
there are life threatening injuries detected in BS.
 Workflow adaptation
When a ship agent detects that several people have life
threatening injuries, the ship agent will undo the A’s
moveTo action and try to adapt the workflow without the
help of MP by finding an alterative service that can be
used under current situation and resuming the execution of
the remaining workflow. F3 will be used by the ship agent
to find such an alternative service. F4 corresponds to such
an adapted workflow: B will move to BS. When B gets into
its H’s reachable range, H will fly to BS and load the lifethreatening passengers. B will load the remaining
passengers after reaches BS. If such an alternative service
cannot be found, the ship agent will notify MP with the
situation violation, and MP will do re-planning from the
current situation.

5. Conclusions and Future Work
In this paper we have presented an approach to
incorporating SAW for adaptive coordination in servicebased systems. A model for SAW and the design of SAW
agents based on the model to enable adaptive coordination
in service-based systems have been presented. Future work
in this direction includes incorporation of other QoS, such
as security and real-time, in the service coordination, and
development of software tools for SAW agent
specification, verification, generation and deployment, as
well as support for agent mobility.

Acknowledgment
This work is supported by the DoD/ONR under the
Multidisciplinary Research Program of the University
Research Initiative, Contract No. N00014-04-1-0723.

References
[1] Web Services Architecture. Available at: http://www.
w3.org/TR/2004/NOTE-ws-arch-20040211/.
[2] U.S. Department of Defense Directive (DODD) 8100.1:
“Global Information Grid (GIG) Overarching Policy,” The
Pentagon, Washington D.C., 2002.
[3] S. S. Yau, Y. Wang and F. Karim, “Development of
Situation-Aware Application Software for Ubiquitous Computing

Environments”, Proc. 26th IEEE Int'l Computer Software and
Applications Conf., 2002, pp. 233-238.
[4] S. S. Yau, et al, “Reconfigurable Context-Sensitive
Middleware for Pervasive Computing,” IEEE Pervasive
Computing, vol. 1(3), 2002, pp. 33-40.
[5] S. Russell, P. Norvig, Artificial Intelligence: A modern
Approach, 2nd ed., Prentice Hall, 2003.
[6] S. Card, T. Moran, A. Newell, The Psychology of HumanComputer Interaction, Lawrence Erlbraum Associates, 1983
[7] David L. Hall, James Llina, Handbook of Multisensor Data
Fusion, CRC Press, 2001
[8] J. McCarthy and P. J. Hayes, “Some Philosophical Problems
from the Standpoint of Artificial Intelligence”, Machine
Intelligence 4, 1969, pp. 463-502.
[9] J. A. Pinto, Temporal Reasoning in the Situation Calculus,
PhD Thesis, University of Toronto, 1994.
[10] J. McCarthy., “Situation Calculus with Concurrent Events
and Narrative”, http://wwwformal.stanford.edu/jmc/narrative/
narrative.html, 2000.
[11] D. Plaisted “A Hierarchical Situation Calculus”, J.
Computing Research Repository (CoRR), 2003.
[12] C. J. Matheus, M. M. Kokar, and K. Baclawski, “A Core
Ontology for Situation Awareness”, Proc. 6th Int’l Conf. on
Information Fusion, 2003, pp. 545 –552.
[13] C. J. Matheus, et al, “Constructing RuleML-Based Domain
Theories on top of OWL Ontologies”, Proc. 2nd Int’l Workshop
on Rules and Rule Markup Languages for the Semantic Web,
2003, pp. 81–94.
[14] Web Services Coordination (WS-Coordination), Available
at: http://www-106.ibm.com/developerworks/library/ws-coor/
[15] Web Services Coordination Framework (WS-CF),
http://www.oracle.com/technology/tech/webservices/htdocs/spec/
WS-CF.pdf
[16] P. Braioneand G. P. Picco, “On Calculi for Context-Aware
Coordination”, Proc. 6th Coordination Conf., 2004, pp. 38-54
[17] G. Cabri, L. Leonardi and F. Zambonelli, “Engineering
Mobile
Agent
Applications
via
Context-Dependent
Coordination”, IEEE Tran. On Software Engineering, vol.
28(11), 2002, pp. 1039-1055.
[18] C. Julien and G. Roman, “Egocentric context-aware
programming in ad hoc mobile environments”, Proc. 10th Int’l.
Symp. On the Foundations of Software Eng., 2002, pp. 21-30.
[19] M. P. Singh, Distributed Scheduling of Workflow
Computations, Technical Report TR-96-18, North Carolina State
University,
1996.
http://www.csc.ncsu.edu/
faculty/
mpsingh/papers/databases/wfscheduling.ps
[20] P. Doherty, J. Kvarnstrom, “TALplanner: A temporal logicbased planner”, AI Magazine, vol. 22(3), 2001, pp.95-102.
[21] H. Davulcu, M. Kifer, I.V. Ramakrishnan, “CTR-S: A Logic
for Specifying Contracts in Semantic Web Services”, Proc. 13th
Int’l World Wide Web Conf., 2004, pp.144-153.
[22] S. S. Yau, et al, "Adaptable Situation-Aware Secure Service
Based Systems", Proc. 8th IEEE Int'l Symp. on Object-oriented
Real-time distributed Computing, to appear.
[23] J. Halpern and Y. Moses, “Knowledge and common
knowledge in a distributed enviroment,” J. ACM, vol. 37(3),
1990, pp. 549-587.
[24] M. Kifer, “Deductive and object-oriented data languages: A
quest for integration,” Proc. DOOD, 1995, pp. 187-212.
[25] FLORA-2 website, http://flora.sourceforge.net/
[26] R. Bharadwaj, "A Framework for the Formal Analysis of
Multi-Agent Systems," Proc. Formal Approaches to Multi-Agent
Systems, 2003.

Proceedings of the 29th Annual International Computer Software and Applications Conference (COMPSAC’05)
0730-3157/05 $20.00 © 2005 IEEE

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

Co-Clustering Signed 3-Partite Graphs
Sefa Şahin Koç

İsmail Hakkı Toroslu

Hasan Davulcu

Radar and Electronic Warfare Systems Computer Engineering Department Computer Science and Engineering Department
Arizona State University, Tempe, AZ
Middle East Technical University
Business Sector, ASELSAN Inc.
Email: hdavulcu@asu.edu
Ankara, Turkey
Ankara, Turkey
Email: toroslu@ceng.metu.edu.tr
Email: sskoc@aselsan.com.tr

Abstract—In this paper, we propose a new algorithm, called
ST RI C LUSTER, to find tri-clusters from signed 3-partite graphs.
The dataset contains three different types of nodes. Hyperedges
connecting three nodes from three different partitions represent
either positive or negative relations among those nodes. The
aim of our algorithm is to find clusters with strong positive
relations among its nodes. Moreover, negative relations up to
a certain threshold is also allowed. Also, the clusters can have
no overlapping hyperedges. We show the effectiveness of our
algorithm via several experiments.

I. I NTRODUCTION
A hyperedge in a tri-partite graph represents the relationship
among the three nodes it connects. For example, in a social
tagging system, which contains three types of nodes (users,
tags, resources), a hyperedge means that a user annotates a
resource with a tag [1]. A tripartite cluster of these hyperedges
may give many information such as users' attitudes to multiple
resources or users with common interests. As another example,
in a biological analysis system, a level of gene in a sample at
a particular time can be represented as a tripartite hyperedge.
By mining tripartite clusters, genes showing common characteristics in samples at common time slots could be extracted
[2].
Finding biclusters with maximum size from a bipartite graph
is proven to be NP-hard, as well as discovering tripartite
clusters with maximum size [3]. Therefore, works in literature
[2], [4], [1] apply heuristics to determine clusters. As a common strategy, tri-clusters are generated by first constructing
biclusters between each pair of three partitions [4]. Then, each
bicluster is matched with two others in order to construct triclusters. Since this approach is very costly, as an alternative,
first, two partitions are selected, and, then, biclusters of these
bipartite graphs are constructed. After that, by iterating each
one of these biclusters on the third partition, tripartite clusters
can be constructed [2]. However, since the first two partitions
are fixed, this approach has bias against the third partition.
As another approach, tripartite clusters focus on one-to-one
correspondence among the nodes [5], [6]. However, the realworld data is usually more complex. For example, in a social
tagging system, a group of users may tag multiple sources
with the same set of tags, which corresponds to many-to-many
relationship.
In this paper, we present an effective algorithm which
generates tri-clusters from tripartite hyperedges with positive

signs. Our method has the following properties: 1) A minimum
threshold for positive signed hyperedge density ratio over
all possible hyperedges among tri-partitions of the cluster is
defined, and, it must be satisfied by clusters. 2) A simple
greedy approach is used in order to trim the hyperedges from
tri-clusters with negative signs to increase the positive density
ratio of the cluster. 3) In order to prevent constructing very
small clusters, both negative signed hyperedges and triples
with no connections are also allowed as long as they satisfy
user defined density threshold constraints. 4) Clusters are not
allowed to have overlaps in terms of hyperedges. A simple
heuristic is used to mark hyperedges in order to prevent
hyperedge overlaps among clusters, and fast termination of
the algorithm while searching potentially maximal clusters. 5)
The effectiveness of our approach is shown using a coveragebased metric.
To the best of our knowledge this is the first work that
attempts to find co-clusters with potentially overlapping nodes
from signed tri-partite graphs. In our work, the clusters
are constructed considering the hyperedges, and thus, it is
possible to generate clusters with common nodes from the
same dimension. A typical problems that motivates this work
is finding co-clusters from sentiments of tweets on issues.
The three dimensions of this problems are people who write
tweets, the selected set of issues (or named entities) and the
chosen sentiment words by the users on these issues. The
sign of the sentiment words also sepresent the sign of the
hyperedge between the three nodes of these dimenstions. It is
very likely that same sentiment words are used by people with
different clusters corresponding to different camps. Similarly
more than one camp may have similar sentiments towards the
same issue as well. Therefore, clusters generated on sentiment
words and on issues which corresponds to positive feelings
of different camps may have many common items. Even on
people dimension, it is likely to generate clusters with several
common people, which may be interpreted as these people
being close to more than one different political camps.
The rest of the paper is organized as follows. Section
II introduces ST RI C LUSTER algorithm. Section III presents
experiments and section IV concludes the paper.
II. T HE ST RI C LUSTER A LGORITHM
In this paper, we use the notations given in Table 1.
ST RI C LUSTER algorithm takes a set of hyperedges, Γ as an

IEEE/ACM ASONAM 2016, August 18-21, 2016, San Francisco CA, USA
978-1-5090-2846-7/16/$31.00 © 2016 IEEE

945

In order to prevent constructing very small clusters λi is
defined, such that:
Li ≥ λi ,
(3)

TABLE I
S YMBOL TABLE
Symbol
Γ
Γv/iv
α
p
n
λi
Li
<
h+/−
Ui
Eir
Uir
Si

Meaning
set of hyperedges
set of valid/invalid hyperedges
a tripartite cluster
minimum ratio of h+ in a cluster
maximum ratio of h− in a cluster
minimum size for type i in a cluster
number of nodes for type i in a cluster (size of type i)
set of tripartite clusters
a hyperedge with positive/negative label
set of nodes for type i
affectiveness value for node r of type i
minimum Eir in Ui , which belongs to node r
maximum number of h in which a node from type i can be

for 1 ≤ i ≤ 3, and this constraint should also be satisfied by
every cluster.

input, such that each hyperedge h connects three nodes from
three different types U = (U1 , U2 , U3 ). Figure 1 illustrates
hyperedges given as 3D matrix. These hyperedges have either
positive or negative labels which are also represented by green
and red colors respectively in Figure 1. Remaining entries
(white cells) corresponds to node triples without connecting
hyperedges. In the example, nodes are {{A,B}, {a,b,c,d,e},
{1,2,3,4,5}} from types U1 , U2 , U3 respectively.
A

1

a

3

-

b

-

c

+

d
e

2

+

+

4

5

B

+

-

a

+

-

+

2

3

+

-

4

ST RI C LUSTER algorithm (Algorithm 1) starts by generating
a potential cluster α which contains all hyperedges in Γ. For
the example input data in Figure 1, α initially is equal to the
whole graph. If there are invalid hyperedges (used to prevent
hyperedge overlaps), they will be removed from α (Section
2.B). After invalid hyperedges are removed, if α does not
satisfy the condition (3), (i.e., β is FALSE), the algorithm
terminates.

5
+

b

+

+

+

c

+

+

-

-

+

d

-

+

+

+

e

+

+

-

1

Algorithm 1 STriCluster Algorithm
1: procedure ST RI C LUSTER (Γ, p , n , λ1 , λ2 , λ3 )
2:
loop
3:
generate α from Γ
4:
β = C LEAN I NVALIDS(Γ, Γiv , α, λ1 , λ2 , λ3 )
5:
if not β then
6:
return <
7:
end if
8:
D ENSITY C HECKING(α, p , n , λ1 , λ2 , λ3 )
9:
if α * f ormula (3) then
. if α satisfies
10:
<←<⊕α
. ⊕ means appending
11:
end if
12:
for each h in α do
. h is a hyperedge in α
13:
Γiv ← Γiv ⊕ h
14:
end for
15:
end loop
16: end procedure

+

Fig. 1. Input Data

The aim of ST RI C LUSTER is to find tripartite clusters of
hyperedges with highly positive labels. To be a valid tripartite
cluster, it has to satisfy threshold values for both density and
size. The density threshold values are p and n , such that
0 ≤ p , n ≤ 1, (p + n ) ≤ 1. The former one represents the
minimum ratio density of positive hyperedges (h+ ) among all
possible hyperedges (i.e., there may be L1 × L2 × L3 number
of possible hyperedges for a cluster with size (L1 , L2 , L3 ),
where Li is number of nodes with Ui type in the cluster). If
Cp is the number of h+ , then:
p ≤

Cp
,
L1 × L2 × L3

(1)

If p = 1, generated tripartite clusters become tripartite cliques
as well. n is the value to control the density of negatively
signed hyperedges (h− ). If Cn represnts the number of h− ,
then:
Cn
n ≥
,
(2)
L1 × L2 × L3
shows maximum allowed tolerance of h− in a cluster if n 6=
0.

A

1

a

3

-

b

-

c

+

d
e

2

+

+

4

5

B

+

-

a

+

-

+

2

3

+

-

4

5
+

b

+

+

+

c

+

+

-

-

+

d

-

+

+

+

e

+

+

-

1

+

Fig. 2. Removing Node (3) From a Potential Cluster

After a potential cluster α is generated, density check
operation is applied on α (Section 2.A). This operation aims
to get α to satisfy conditions (1), (2), and (3). There are three
possible cases that can happen: In the first case (case I), if
conditions (1) or (2) are not satisfied, the least useful node is
removed from α (Figure 2) iteratively, until both constraints
are satisfied. For the example in Figure 1, this step will remove
nodes {{a,d,e}, {3,4,5}} from types U2 , U3 respectively from
α. As the second case (case II), if the removal of a node
from α violates the constraint (3), the process stops. In this
case, one h− in α is labeled as invalid. This prevents the

946

construction of exactly the same potential tripartite cluster
again, because of C LEAN I NVALIDS operation (Line 4) of the
algorithm. As the third case (case III), α satisfies conditions
(1), (2), and (3). Then, D ENSITY C HECKING operation returns
α (as a reference parameter). In the example, returned cluster
α contains nodes {{A,B}, {b,c}, {1,2}} shown in Figure 3. In
this case, α is added to the cluster list < (Line 7). After that,
all hyperedges in α are labeled as invalid and added into Γiv
which is the list of invalid hyperedges (Line 13). The following
sub-section describes the details of density checking procedure
which handles these operations.

A

1

2

B

1

2

b

-

+

b

+

+

c

+

c

+

+

TABLE II
N UMBER OF POSSIBLE HYPEREDGES FOR
Type
S1
S2
S3

EACH NODE TYPE

Value
L2 × L3
L1 × L3
L1 × L2

reach to case III satisfying all three conditions (1), (2), and (3)
and, then returns α which contains nodes {A, B, b, c, 1, 2}.
If α does not satisfy conditions (1) and (2) and if node
removal results the violation of condition (3), it means that
D ENSITY C HECKING is in case II. In this case, the procedure
marks the first h− as invalid.
B. Clean Invalids

(a) Matrix Representation

(b) Cluster Representation

Fig. 3. A Tripartite Cluster Mined in Given Input

A. Density Checking
If given cluster α does not satisfy conditions (1) and (2),
the density checking algorithm searches for nodes to exclude
until α satisfies these constraints. If a node is connected by
high number of h+ , it should be less likely to be removed.
We define hyperedge’s usefullness as follows:
(
2
if h has positive label
val(h) =
(4)
−1
if h has negative label.
Then, the effectiveness of a node (r) is determined with the
following formula where Si represents the maximum number
of hyperedges which contains that node in Ui :
(
P
val(h)
if r ∈ h
h∈α
0
otherwise
Eir =
.
(5)
Si
Then, for each partition, all nodes are checked to find a node
with minimum effectiveness value:
X
Uir = min(
Eir ).
(6)
r∈Ui

At the end of this stage, there will be three Uir values
which are U1a , U2b , U3c corresponding to partitions U1 , U2 , U3
respectively. Minimum of them will be the effectiveness value
Eix of node x. This node will be the one to be removed from
type i in α in this iteration.
For the input in Figure 1, when D ENSITY C HECKING operation applied on α, node 3 will be removed in the first iteration
(Figure 2). Node 3 has the lowest effectiveness value, E33 ,
5 = 0.1 uscompared to others. E33 is obtained as 2+2−1−1−1
2×
ing formula (5). In following iterations, nodes {a, d, e, 3, 4, 5}
will be removed from α. Then, D ENSITY C HECKING will

Invalid hyperedges include the hyperedges of all tripartite
clusters previously generated as well as all edges marked
as invalid by D ENSITY C HECKING. In order to remove a
hyperedge one of the nodes from this hyperedge should be
removed. To do this, C LEAN I NVALIDS procedure picks a node
to remove and it repeats the same action until no invalid
hyperedge is left in α. While selecting a node, it uses a
heuristic that reduces the cluster size as minimum as possible.
In order to do this, we first determine the number of invalid
hyperedges connected to each node. If the ratio of this number
to Si is high, that node is more likely to be removed. This ratio,
called as θir for node r from type i. Then, we calculate the
effectiveness for all the valid nodes of α, which is called as
v
Eir
. These two values values are combined with the following
formula:
θir
(7)
γir =
v .
0.9 + Eir
Among all nodes, the one which has highest γ value is the
one to be removed.
As a constraint, if removing node x will result violation
of condition (3), C LEAN I NVALIDS procedure returns FALSE.
If there is no invalid hyperedge left in α, C LEAN I NVALIDS
returns TRUE.
For the input data in Figure 1, ST RI C LUSTER algorithm
finds the cluster in Figure 3 in the first iteration. Then,
hyperedges of this newly generated cluster are labeled as
invalid. In the next iteration, new potential cluster α (Figure 4-a) is generated from Γ. But α contains some invalid
hyperedges (colored with blue in Figure 4-a). Therefore, α
is passed to C LEAN I NVALIDS procedure to be cleaned from
invalid hyperedges. First, node c is removed since γ2c is
3 ÷ 0.9 = 3.33, is the maximum among γ values. Then, nodes
2 and 1 are selected and removed respectively (Figure 4-b,
4-c). This will result a clean α (Figure 4-d) and the procedure
terminates.
III. E XPERIMENTS
In order to evaluate our algorithm, we have generated data
sets with varying sizes. We have done all the experiments on
MacBook Pro Mid 2015 (Intel i7 2,5 GHz, 16GB memory).

947

Time

a
b
c

3

-

+

4
+

+

+

-

+

+

d
e

2

+

5

B

-

1

2

a
b

+
+

+

3

4

-

5

A

+

b

c

+

+

-

-

+

d

-

+

+

+

e

+

1

3

a
b

4

5

B

+

-

a

-

+

+

+

-

+

d
e

b
+

1

3

4

+

d

-

e

+

e

+

3

-

4

5

+

-

+

+

-

+

B
a
b

-

+

+

1

2

3

+

-

+

+

d

-

+

e

+

4

5

5

A

+

a
b

+

3

4

5

B

3

+

-

a

-

+

+

-

+

d
e

b
+

d

Cover

1024 sn

35K

768 sn

26K

512 sn

18K

256 sn

9K

+
+

+

+

(b) Removing Second Node

+
+

-

d

(a) Removing First Node

A

2

a

+

+

1

4

5

Number of Hyperedges

1

Time

A

+
+

+

+

0 sn

e

0.2-0.4

0.2-0.2

0.4-0.4

0.4-0.2

0.6-0.2

0K

Density Ratio Values

(c) Removing Third Node

(d) A Clean Tripartite Cluster
Fig. 6. Test Scenario Depending on Density Ratios in Input Data

Fig. 4. A Scenario of C LEAN I NVALIDS Procedure

In the first set of experiments, we have fixed h+ and h−
density ratios while changing input sizes. Other parameters
are also fixed as p = 0.75, n = 0.10, λi = (2,2,2). In this
test, we have generated 6 sample datasets. Each one contains
positive hyperedges with 60%, negative hyperedges with 20%,
and 20% is empty. (L1 × L2 × L3 ) values for these samples
are (31.25K, 62.5K, 125K, 250K, 500K, 1M) respectively.
Figure 5 presents the results.
Cover

IV. C ONCLUSION

7000 sn

300K

5250 sn

225K

3500 sn

150K

1750 sn

75K

0 sn

31.25K

62.5K

125K

250K

500K

1M

In this paper, we have proposed a new method, called
ST RI C LUSTER, to mine tripartite clusters of positively labeled
hyperedges. The input data is composed of three dimensions.
Each hyperedge connects three nodes from each dimension.
Clusters are generated depending on density ratio of positively
(minimum) and negatively (maximum) labeled hyperedges.
We have showed the effectiveness of our approach using
both syntetic and real data sets.

Number of Hyperedges

Time

Time

which also represent the sign of the hyperedge connecting
these three items. We have large datasets with 10K users, 45K
sentiment words and 20 different issues. This 3-dimensional
data is very sparse with only 280K non-empty entries. So far,
we have applied our algorithm to a fraction of this dataset
which corresponds to randomly select few percentages of it.
We have obtained fairly large and overlapping clusters in all
three dimensions. Some largest clusters have as many items as
the 10% of the nodes of its corresponding dimensions, even
for user or sentiment words dimensions.

0K

Input Size

ACKNOWLEDGMENT
This research was supported partially by and USAF Grant
FA9550-15-1-0004 and ASELSAN Inc.

Fig. 5. Test Scenario Depending on Input Size

In the second test, we have generated 5 datasets. In this test,
we have fixed the size as (L1 × L2 × L3 ) = 125K and we
have varying density ratios for (h+ , h− ) pairs as {(0.2,0.4),
(0.2,0.2), (0.4,0.2), (0.4,0.4), (0.6,0.2)}. The results are shown
in Figure 6.
The figures show both execution times and the number of
hyperedges included in the constructed clusters. We prefer
most (positive) hyperedges to be included in clusters while
clusters being non-trivial. The results show that we have
achieved very high coverage in that sense, since constructed
clusters include almost as many hyperedges as the half of the
number of positively signed hyperedges.
We have also tested our approach using real data set, which
corresponds to the tweets of users on selected issues. These
tweets are processed in order to determine the sentiments of
users towards these issues. From these tweets, a three dimensional data set is generated. These dimensions are users, issues,
and sentiment words chosen by the users on these issues,

R EFERENCES
[1] C. Lu, X. Chen, and E. Park, “Exploit the tripartite network of social
tagging for web clustering,” in Proceedings of the 18th ACM conference
on Information and knowledge management. ACM, 2009, pp. 1545–
1548.
[2] L. Zhao and M. J. Zaki, “Tricluster: an effective algorithm for mining
coherent clusters in 3d microarray data,” in Proceedings of the 2005 ACM
SIGMOD international conference on Management of data. ACM, 2005,
pp. 694–705.
[3] M. Dawande, P. Keskinocak, J. M. Swaminathan, and S. Tayur, “On
bipartite and multipartite clique problems,” Journal of Algorithms, vol. 41,
no. 2, pp. 388–403, 2001.
[4] L. Zhu, A. Galstyan, J. Cheng, and K. Lerman, “Tripartite graph clustering for dynamic sentiment analysis on social media,” in Proceedings
of the 2014 ACM SIGMOD international conference on Management of
data. ACM, 2014, pp. 1531–1542.
[5] X. Liu and T. Murata, “Detecting communities in tripartite hypergraphs,”
arXiv preprint arXiv:1011.1043, 2010.
[6] Y.-R. Lin, J. Sun, P. Castro, R. Konuru, H. Sundaram, and A. Kelliher,
“Metafac: community discovery via relational hypergraph factorization,”
in Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 2009, pp. 527–536.

948

I n f o r m a t i o n

I n t e g r a t i o n

OntoMiner:
Bootstrapping and
Populating Ontologies
from Domain-Specific
Web Sites
Hasan Davulcu, Srinivas Vadrevu, and Saravanakumar Nagarajan, Arizona State University
I.V. Ramakrishnan, State University of New York at Stony Brook

S

emantic Web documents use metadata to express the meaning of the content encapsulated within them. Such metadata facilitates machine understanding of Web

data. Although RDF and XML are becoming widely recognized as the standard vehicle

Key to the Semantic

for describing metadata,1 an enormous amount of semantic data is still being encoded in

Web idea are

HTML documents, which are designed primarily for
human consumption. The presence of such legacy
documents makes embracing the Semantic Web
vision difficult.2 Thus, we need scalable solutions to
automatically transform legacy HTML to Semantic
Web documents.
Recent work describes algorithms that automatically annotate HTML documents with semantic
labels.3 Unfortunately, constructing the domain
ontologies that drive these algorithms is human intensive. Bootstrapping and populating large, rich, and
up-to-date domain ontologies that organize the most
relevant concepts, their relationships, and instances
(which correspond to members of concepts) can considerably enhance the scalability of transforming
legacy HTML into Semantic Web documents.
Our system, OntoMiner, offers automated techniques for creating such ontologies based on a small
collection of relevant Web sites. Users can then
employ the ontologies to create a rich set of labeled
examples that a supervised machine-learning system
such as WebKB can use.4

ontologies that can
transform legacy
HTML documents into
Semantic Web
documents and encode
domain knowledge to
facilitate automated
reasoning. The
techniques presented
here can help
bootstrap and populate

Semantic partitioning

specialized domain
ontologies.
24

OntoMiner constructs specialized domain ontologies by organizing and mining a set of user-supplied,
taxonomy-directed Web sites (10 to 15 relevant
URLs). A Web site is said to be taxonomy-directed

if it contains at least one taxonomy for organizing its
contents and it presents the instances belonging to a
concept in a regular fashion. Neither the taxonomy’s
presentation across different pages nor the instances’
presentation among different concepts must be regular to classify a Web site as taxonomy directed.
Almost all scientific, news, financial, travel, shopping, search, and community portals that we know of
are indeed taxonomy directed.
Next, using a semantic partitioning algorithm,
OntoMiner detects the HTML regularities in the chosen sites’ Web documents and turns them into hierarchical structures encoded as XML. Our tree-mining
algorithms identify key domain concepts selected
from within the homepages’ directories. By selectively crawling through the links corresponding to
key concepts, OntoMiner then expands the mined
concept taxonomy with subconcepts. It also has algorithms that can identify the logical regions in Web
documents that contain links to instance pages.
OntoMiner can accurately separate human-oriented
decoration such as navigational panels and advertisement bars from real data instances. Also,
OntoMiner tries to distinguish the data instances
from their labels found within the vicinity of the
extracted data; it can accurately annotate the
extracted data with their labels whenever they are
available.

1094-7167/03/$17.00 © 2003 IEEE
Published by the IEEE Computer Society

IEEE INTELLIGENT SYSTEMS

Body
table
tr

B1

td
table
tr ....
tr
td

a
a
a

td

a

table
tr

table

tr

Real Estate ...
International
National ...

td

B2
td

Job Market

td

B3

a
a
a

Electronic Edition
Media Kit
Community Affairs ...

center
a
Supreme Court
font
By LINDA
font
6:22 PM ET

table
B3

tr
td
a

B2

B4

IMG
center
a

B5

Jumpers

(a)

(b)

Figure 1. (a) The New York Times homepage; (b) DOM tree view of the same page. The “B” tags identify corresponding segments.
(Copyright © 2003 by The New York Times. Reprinted with permission.)

OntoMiner employs a two-phase semantic
partitioning algorithm made up of flat and hierarchical partitioning, which we present here.
Flat partitioning
Our Flat Partitioner (FP) algorithm detects
various logical segments within a Web page.
For example, for the homepage of www.
nytimes.com, we marked the logical segments in boxes B1 through B5 in Figure 1a.
The boundaries of segments B2 and B3 correspond to the dotted lines shown in the
DOM (document object model) tree of the
Web page in Figure 1b.
Intuitively, FP groups similar contiguous
substructures in the Web pages into logical
segments by detecting a high concentration
of neighboring nodes with similar root-toleaf tag paths. First, FP initializes the segment boundary to be the first leaf node in the
DOM tree. Next, it links any two leaf nodes
in the tree with a similarity link if they share
the same path from the tree’s root and if all
SEPTEMBER/OCTOBER 2003

the leaf nodes in between have different
paths. Then, it calculates the ratio of cardinality of similarity links crossing the current
candidate boundary to the cardinality of similarity links inside the current segment. If this
ratio is less than a threshold d, FP marks the
current node as a segment boundary. Otherwise, it adds the current node to the current
segment and considers the next node a segment boundary. The process terminates when
the algorithm reaches the last leaf node. The
tree view in Figure 1b illustrates the FP algorithm listed in Figure 2. The arrows in the
tree view in Figure 1b denote the similarity
links between the leaf nodes. Let’s assume
the threshold d is set to 60 percent. Then,
when the current node is Job Market, the total
number of outgoing unique similarity links
(out in the algorithm’s line 10 in Figure 2) is
1, and the total number of unique similarity
links (total in line 11) is 1. So, the ratio of out
to total is 100 percent, which is greater than
the threshold. Thus, the next leaf node
computer.org/intelligent

becomes the current candidate boundary
(current in line 6). At node International, out
becomes 1 and total is also 1. So, the ratio is
still greater than the threshold. When current
reaches the Community Affairs node, out becomes
0 and total is 1, so the ratio is less than threshold d. Now, Community Affairs (B2 in Figure 1b)
is added to the set of Segment_Boundaries in line
13 and Partition_Nodes is reinitialized in line 14.
The same boundary detection condition is
satisfied once again when the algorithm
reaches 6.22 PM ET, where out becomes 1 and
total is 3. So, 6.22 PM ET (B3 in Figure 1b) is
also added to Segment_Boundaries.
Hierarchical partitioning
The Hierarchical Partitioner (HP) algorithm infers the hierarchical relationships
among the HTML parse tree’s leaf nodes,
where all the page content is stored. HP
achieves this through a sequence of three
operations: binary semantic partitioning,
grouping, and promotion. Unlike early pio25

I n f o r m a t i o n

I n t e g r a t i o n

0

mini ≤ k < j Cost( Li , Lk )

Cost Li , L j = 
+ Cost Lk +1, L j +

Grouping _ Cost Li ... k , Lk +1... j


Flat Partitioner
Input: T: DOM Tree
Output: {b1, b2, … bk}: Flat Boundaries, set of leaf nodes
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

(

PIT := PathIndexTree(T)
current := first leaf node of T
Partition_Nodes := φ
Segment_Boundaries := φ
For each lNode in Leaf_Nodes(T) Do
current := lNode → next
If n = PIT.next_similar(current) exists Then
Partition_Nodes := Partition_Nodes ∪ {n}
End if
out = {path(m)m ∈ Partition_Nodes and m > current}
total = {path(m)m ∈ Partition_Nodes}
If out/total < δ Then
Segment_Boundaries := Segment_Boundaries ∪ {current}
Partition_Nodes := φ
End if
End for
Return Segment_Boundaries

d1 + d2
.
2 * max_depth

CPSIM(A, B ) = 1 Similarity between Paths P1 and P2 .
max(d1 + d2)
CSTSIM(A, B) = 1 – max(Separation, Overlap ).
where

Separation =

| {S1 – S2}  {S2 – S1} |
|S1  S2|

and

Overlap =

|S1  S2| .
|S1  S2|

CORD(A, B ) = 1 – Sim (A, B ) ,
where

Sim(A, B) =

Number of paths similar in order in subtrees of A and B .
Maximum number of paths in subtrees of A and B

Figure 3. Definitions of the four cost factors.

neering systems,5–7 OntoMiner does not
make any assumptions about the roles and
usage patterns of Web pages’ HTML tags.
Binary semantic partitioning. The binary
semantic partitioning of a Web page is based
on a dynamic programming algorithm that
26

(

)

(

if i = j

)}

if i < j ,

where Li and Lj are any two leaf nodes in the
HTML parse tree.
The cost function calculates the measure
of dissimilarity between two internal or leaf
nodes—that is, a high value of cost indicates
that these two nodes’ subtrees are highly dissimilar. Thus, the dynamic programming
algorithm finds the lowest cost among the
various possible binary groupings of nodes
and parenthesizes them into a binary tree,
where similar nodes are grouped. The cost
for grouping two consecutive subtrees is calculated as the sum of four cost factors. Let A
and B be the lowest common ancestors
(LCA) of nodes Li to Lk and Lk+1 to Lj, respectively. Then,

Figure 2. The Flat Partitioner algorithm.

CLCA (A, B) =

)

{

employs the following cost function. By finding the grouping with the minimal cost, the
dynamic programming algorithm determines
the neighboring sibling nodes that must be
grouped together. We recursively define the
cost for grouping any two nodes in the DOM
tree as
computer.org/intelligent

Grouping_Cost (Li…k, Lk+1…j) =
Grouping_Cost (A, B) =
sum of the distances of A and B to their
LCA +
similarity of the paths from A and B
to their LCA +
similarity of the paths in the subtrees
of A and B +
order similarity of the paths in the
subtrees of A and B
Grouping_Cost (A, B) = CLCA(A, B) +
CPSIM(A, B) +
CSTSIM(A, B) + CORD(A, B)
The first cost factor CLCA(A, B) calculates
how far the two nodes are from their LCA.
CPSIM(A, B) determines the cost for similarity between paths to the LCA. CSTSIM(A, B)
and CORD(A, B) compute the cost for similarity in the subtrees rooted at A and B: the
former computes similarity in the paths, the
latter computes the shared ordering of paths
in the subtree.
Let S1 be the set of all paths in the subtree
of A, S2 be the set of all paths in the subtree
of B, d1 be the number of tags on the path
from the LCA to A, d2 be the number of tags
on the path from the LCA to B, and
max_depth be the maximum depth of the
DOM tree. The cost factors are defined in
Figure 3.
For example, in Figure 4a,
• a, b, c is the sequence of tags along the path
from the LCA to A
IEEE INTELLIGENT SYSTEMS

• a, b, d is the sequence of tags along the path
from the LCA to B
• <P1, P2, P3> is the sequence of paths in
A’s subtree
• <P1, P2, P4> is the sequence of paths in
B’s subtree

S1 = {P1, P2, P3}, S2 = {P1, P2, P4}

LCA

d1

c

b

a

a
P1

P2

A

1
CPSIM(A, B ) = 1– | {a, b, c }  {a, b, d } | = —
3
{P1, P2, P3, P4}

b d
d 2

Overlap =
S2

(L1 P1 P2 P3(Lk )

(a)

(b)

Figure 4. (a) A sample domain object model tree; (b) calculating this case’s cost factors.

A

d

d

S1

M1) (M2

S2

S1
A

S2
(LM

P1

B

S1

d
,
max _ depth

LCA

B

For Case 1,

CLCA

| {P1, P2} |
1
=—
| {P1, P2, P3, P4} | 2

1
CSTSIM(A, B ) = 1 – max(Separation, Overlap ) = —
2
| {P1, P2} |
1
Sim(A, B ) =
=—
2
max (|S1|, |S2|)

(Lk+1 P1 P2 P4 (Lj )

P

For Case 2,

| {P3}  {P4} |
1
=—
| {P1, P2, P3, P4} | 2

1
CORD(A, B ) = 1 – Sim(A, B ) = —
3

In these three cases, the second and fourth cost
factors are irrelevant, so the algorithm ignores
them. For Case 3, the algorithm ignores the
first cost factor. Accordingly, we modify the
first and third cost factors as follows.

CSTSIM = 1 − max( Separation, Overlap) .

Separation(A, B ) =

B

S1

CLCA =

3+ 3
2 ∗ max_depth

CLCA (A, B ) =

So, if d1 = | {a, b, c} | = 3, d2 = | {a, b, d} | = 3,
S1 = {P1, P2, P3}, and S2 = {P1, P2, P4},
then OntoMiner would calculate the cost factors as shown in Figure 4b.
We need to adjust the four cost functions
to fit three other cases that might arise during
the cost function evaluation (see Figure 5):
• Case 1. The LCA of the two nodes that the
algorithm is grouping into one partition is
one of the nodes itself, and the other node
is not a leaf node.
• Case 2. The LCA of the two nodes, which
are to be grouped into one partition, is one
of the nodes itself, and the other node is a
leaf node.
• Case 3. The LCA nodes for the ranges are
identical.

d1 = | {a, b, c} | = 3, d2 = | {a, b, d} | = 3

If

R)

(LM)

Case 1

(M2

RM) (LM

M1)

Case 2

(M2

RM)

Case 3

Figure 5. Three different cases during cost function evaluation.

S1 ∩ S2
d
=
, CSTSIM = 1 − S ∪ S .
max _ depth
1
2

For Case 3,
CSTSIM = 1 − max( Separation, Overlap) .
In Figure 6a, Column 1 represents part of the
DOM tree of the New York Times homepage,
and Column 2 (Figure 6b) represents the
binary semantic partition tree. You can see,
for example, that the algorithm groups nodes
68 through 82 into one partition, which has
internal binary partitions.
Grouping. Next, we group similar binary
SEPTEMBER/OCTOBER 2003

partitions into Group nodes. Intuitively, the
grouping step creates Group nodes made up of
multiple similar Instance nodes as its children.
The grouping algorithm first initializes the
type of leaf node in the binary partition tree
as Simple. While traversing the tree in postorder, if it finds two Simple sibling nodes and
if the cost for grouping these two nodes is
less than a threshold d (according to the cost
factor discussed earlier), it marks these nodes
as Instance nodes and their parent as a Group
node. For example, Figures 6b and 6c illustrate the conversion of a binary partition tree
shown in Column 2 into a Group tree shown
in Column 3. In Column 2, the nodes Sports
computer.org/intelligent

and Health are sibling nodes, and the cost for
grouping these is less than d. Similarly, if it
finds two sibling nodes marked Group and if
the cost for grouping their instances is less
than d, it marks the parent of these sibling
nodes as a Group node and merges their
instances. For example, it has already marked
the parent of nodes Health and Sports and the
parent of Science and Technology as Group nodes,
and is marked as a Group node. Then, if the
cost for grouping all the Technology through
Sports instances is also less than d, the algorithm marks the grandparent of these
instances as a Group node and merges their
instances (see Column 3). Alternatively, if
27

I n f o r m a t i o n

(a)

I n t e g r a t i o n

(b)

(c)

(d)

Figure 6. Dynamic programming: (a) Part of the domain object model tree of the New York Times homepage; (b) its binary partition
semantic tree; (c) the converted Group tree; (d) the final hierarchical partition tree after promotion.

one of the sibling nodes is Simple and the other
is Group and the cost for grouping the Simple
node with the instances of the Group node is
also less than threshold d, the algorithm
changes the type of Simple node to an Instance
and merges it with the instances of the Group
node. This operation continues until it
reaches the root of the binary partition tree.
Promotion. The final step in hierarchical partitioning is promotion. The promotion algorithm identifies the leaf nodes that should be
promoted above their siblings. A node is considered for promotion if it satisfies one of the
following rules.

the first child of its parent, and if the parent
is not marked as a Group node.
Rule 2. A node is promoted if it satisfies
all the following conditions:
• It is the first child of its parent.
• Its parent is not marked as a Group node.
• Its only sibling is a Group node.

and recall for the generated HP trees, we
manually generate ideal hierarchical semantic partitioned trees for every page and then
generate transitive closure of all parent-child
relationships implied by each tree. We calculate precision and recall as

{ R} ∩ { R′}
{ R}
{ R} ∩ { R′}
,
{ R′}

Precision =

• There is a Bold tag such as <B> or <bold>
along its path from the root node.
• A Bold tag value is defined for this node’s
class attributes.
• The node’s labeled text is fully capitalized.

The nodes satisfying the Bold condition are
marked (B) in Column 3 (Figure 6c). Upon
promotion, the Bold node replaces its parent
node. If the promotion rules reapply, the Bold
node is promoted again. As an illustration,
Column 3 in Figure 6c represents the Group
tree, and Column 4 (Figure 6d) represents the
final hierarchical partition tree after promotion. The News node in Column 3 is marked
Bold, and it is the first child of its parent. So,
using Rule 1, it is promoted above all its sibling nodes (International through Corrections). The
nodes Opinion and Features are also promoted
using the same rule.

Taxonomy mining

A node can be promoted if it is Bold, if it is

Experimental results. To calculate precision

Taxonomy mining involves several tasks,
including

Rule 1. A node is marked Bold if it satisfies
one of the following conditions:

28

computer.org/intelligent

Recall =

where R is the set of relationships in the hierarchical tree and R′ is the set of relationships
in the ideal tree. Table 1 shows the experimental results for the 13 homepages on which
the semantic partitioning algorithm was
applied. You can access all the experimental
data for semantic partitioning at www.public.
asu.edu/~snagaraj/OntoMiner/Semantic.

IEEE INTELLIGENT SYSTEMS

Table 1. Precision and recall for the Semantic Partitioning algorithm.
Domain

Precision

Recall

www.abcnews.go.com

0.79

0.86

www.ninemsn.com

0.75

0.72

www.cbc.ca

0.94

0.73

www.cnn.com

0.81

0.75

www.foxnews.com

0.83

0.82

www.msnbc.com

0.80

0.86

www.nytimes.com

0.83

0.88

www.time.com

0.80

0.86

www.timesonline.co.uk

0.84

0.92

www.un.org

0.79

0.86

www.usnews.com

0.85

0.64

www.washingtonpost.com

0.77

0.71

www.washingtontimes.com

0.75

0.73

Average

0.81

0.80

• Separating important concepts (the categories that define the context) from
instances (the content that are members of
each concept) and from human-oriented
decoration such as navigational panels and
advertisement bars
• Mining relationships among the concepts
Our goal is to automatically mine a domain’s
taxonomy given 10 to 15 domain-relevant
Web sites. To demonstrate the algorithms’
efficacy, we implemented and tested our
approach with two separate domains: News
and Hotels.
Frequency-based mining
We first preprocess the homepages using
semantic partitioning to generate semistructured XML documents, which we use to
mine the taxonomy. Different Web sites often
repeat important concepts in a given domain,
so our system mines for frequent labels in the
input XML documents. By using an experimentally determined threshold for support—
the minimum number of times a label should
occur to be considered frequent—we separate concepts from the rest of the document.
For example, in the News domain, our system identifies Business, Sports, Politics,
Technology, Health, and Entertainment as
important concepts.
Candidate label extraction phase
Our simple frequency-based mining algorithm might miss some labels that are releSEPTEMBER/OCTOBER 2003

vant but infrequent. For example, in
www.washtimes.com, our system identified
Entertainment to be a frequent label but
missed Civil War and Culture. To find such
relevant labels, our system learns attributed
tag paths of the frequent labels and then
applies them within the corresponding logical segments to retrieve more labels. An
attributed tag path is a regular path query in
XPath syntax. For example, the attributed tag
path for the Entertainment label in www.
washtimes.com is /HTML/BODY[@bgColor]/
TABLE[@cellpadding=0 and @cellspacing=0 and
@width=760]/…/A/#TEXT(). Upon querying the
logical segment of the Entertainment label
with this path query, we identify more labels.
Abridgment phase
As we extract candidate labels, we might
identify some labels that are irrelevant to the
domain—for example, NYT Store in http://
nytimes.com. To eliminate such labels, we
ignore a label if
• It does not have a URL.
• The URL refers to another Web site
domain.
• The page it refers to doesn’t have any new
frequent labels or valid instances (which
we describe later).
Grouping the labels into concepts
During the phases already discussed, we
collected the important labels (keywords).
But the same label might appear differently
computer.org/intelligent

is-a Miner
Input: C: set of concepts, S: set of
semantically partitioned Web pages,
Sup: support
Output: Tree representing the hierarchy of
concepts
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:

T := S
R0 := φ
d := max(depth(s ∈ S))
For i = 1 to d Do
Ri ← Ri–1 bag i – related(T)
FRi := frequent(Ri)
NFRi:= non-frequent(Ri)
T ← NFRi
End for
Returndi= 1 FRi

Figure 7. An algorithm for mining is-a
relationships.

in various documents, and this introduces
duplicates. To eliminate duplicates, we group
them according to their lexicographic similarity. First, OntoMiner stems the labels
using the Porter stemming algorithm.8 Then
it applies Jaccard’s coefficient,9 calculated
as |X ∩ Y| / |X ∪ Y|, where X and Y are sets
of stemmed words in two different labels, to
organize them into groups of equivalent
labels. We denote each collection of labels as
a concept. This simple similarity measure
groups labels that are lexicographically related
(such as “Sport” and “Sports”) but not ones
that are semantically related (such as “World”
and “International”). The issue of identifying and grouping semantically related labels
is an interesting problem we plan to pursue in
the future.
Mining parent-child relationships
from semantically partitioned
Web pages
The concepts we obtain from the grouping phase are flat. To organize them into a
taxonomy, we need is-a relationships among
them. Using the algorithm outlined in Figure
7, we mine these relationships from the
semantically partitioned Web pages. In this
algorithm, FR refers to the bag (a collection
of objects whose members need not be distinct) of frequent relationships and NFR
refers to the bag of infrequent (“nonfrequent”) relationships. Two concepts a and b
are i-related if a is an ancestor of b and i
nodes connect them. We first mine 1-related
29

I n f o r m a t i o n

I n t e g r a t i o n

FR1 = φ, and a-b ∈ NFR1. In the second iteration, we find that a-b ∈ 2-related and the
bag R2 contains a-b twice, so FR2 = {a-b}.
Given the collection of concepts and the hierarchical partition trees corresponding to the
relevant homepages, this algorithm produces
the concept taxonomy.
Expanding the taxonomy beyond
homepages
The taxonomy obtained from the previous phase corresponds to the one we mined
from Web site homepages. To expand the
domain taxonomy, we follow the links corresponding to every concept c and expand
the taxonomy depth-wise by repeating the
earlier phases, thus identifying subconcepts

Table 2. Alignment of segments
corresponding to links in the candidate
segments in Figure 9. “M” indicates a
segment match and “X” indicates a
mismatch. Subscripts correspond to parts
of a document: 1, to the header bar; 2,
the sidebar; 3, the article text; 4, the
image, among other things; 5, video for
some of the articles.
M

M

X

X

X

a1

a2

a3

a4

—

b1

b2

b3

b4

b5

c1

c2

c3

c4

—

Instance extraction

A taxonomy schema is made up of
a set of is-a relationships
between concepts. To populate a
taxonomy, OntoMiner must identify
the concept instances.

Figure 8. A fragment of the taxonomy
obtained for the News domain.

pairs, which are direct parent-child relationships, and find the frequent relationships.
Next, we follow the same procedure for the
union of infrequent 1-related pairs and all 2related pairs to find more is-a relationships.
We repeat this procedure until we reach the
maximum depth of the input trees available
for mining. For example, suppose a-b and ac-b are paths from two trees. The maximum
depth would be 2. If the necessary support
for frequent pairs is 2, then a-b ∈ 1-related,
30

of c. For example, Sports is a concept in the
taxonomy obtained from the previous phase.
If we follow all the links corresponding to
Sports and repeat the procedure, we get a
subtaxonomy for Sports that contains concepts such as Baseball, Tennis, and Horse
Racing. Following the links from all concepts and mining them yield the final taxonomy for the domain.
Experimental results for ontology
mining
We evaluate the mined ontology the same
way we did the semantic partitioning algorithm. We manually create an ideal ontology
and determine the transitive closure of parent-child relationships for both ontologies.
Precision for the mined News taxonomy is
75 percent, and recall is 92 percent.
Figure 8 shows the taxonomy we mined
from the News domain. You can access our
experimental results related to taxonomy
mining at www.public.asu.edu/~snagaraj/
OntoMiner/Taxonomy.
computer.org/intelligent

A taxonomy schema is made up of a set of
is-a relationships between concepts. To populate a taxonomy, OntoMiner must identify
the concept instances. As mentioned earlier,
instances correspond to members of concepts. OntoMiner’s instance extraction algorithm can extract relational instances made
up of attribute-value pairs as well as complex, semistructured instances from a collection of domain-relevant Web sites. Whenever they are available, it also attempts to
extract the attributes’ labels.
Identifying instance segments
To identify instance segments (the segments in the Web documents that contain
links to concept instances), OntoMiner tries
to find the segment that contains a majority
of links referring to pages with templatedriven instances. To find instances in a collection of pages, the algorithm first creates
flat partitions of the HTML documents into
logical segments. For example, in Figure 9,
given the candidate segment s, it flat-partitions the HTML documents referred by segment s into segments labeled ai, bi, and ci.
Next, it aligns the segments using the Levenstein distance measure (of minimum edit
distance) and Jaccard’s coefficient and finds
the dissimilar segments. In Table 2, “M” indicates a segment match and “X” indicates a
mismatch where an insertion, deletion, or
replacement has occurred in the segment
alignment. Next, it extracts the attributed
root-to-leaf tag paths from the dissimilar segments and finds the paths that appear in the
majority of the pages referred by the candidate segment. This set of paths, called template paths, represents the instance template
hidden in these pages for that segment. For
IEEE INTELLIGENT SYSTEMS

a1

a4
Candidate
segment S

a2

b1

a3

b4

c1
b2

c2
b3

b5

c3
Figure 9. Identifying instance segments from an HTML document. Blue and green boxes denote similar segments among instance
pages; the rest denote mismatch segments. (figure courtesy of Arizona State University)

each segment in the concept page, we compute the ratio of the template paths to the set
of all paths to identify the main instance
segment that points to most of the templatedriven instances. For example, in Figure 9,
our system identifies the candidate segment
s to be the main instance segment. The template paths of the main instance segment
make up the signature paths for the concept
instances. To collect all the instances, the system applies these signature paths to all the
pages referred by links in the concept pages.
Instance extraction for labeled
and unlabeled attributes
After identifying the instance signature
paths from the concept Web page, the system
finds the logical segments in the instance
pages and invokes the HP algorithm to generate the hierarchical structures corresponding to every instance. Next, the system uses
similar techniques described in the taxonomy
SEPTEMBER/OCTOBER 2003

mining algorithm to separate the attribute
labels from their values and to identify the
hierarchy among frequent labels within the
instances. For example, in Figure 10a, the segment within solid lines is an instance segment.
Mining frequent labels among such segments
reveals that “GUEST ROOMS & AMENITIES,” “HOTEL SERVICES,” “MEETING
& EVENT FACILITIES,” “RESTAURANTS
& LOUNGES,” and “LOCAL AREA” are the
frequent attribute labels. Upon following links
from frequent labels and by using the hierarchy among them, the system also discovers
the labels and instances of complex-valued
attributes such as LOCAL AREA, whose values are related
to other objects through LOCAL ATTRACTIONS. We fill the
values for all attributes by finding the children
of frequent labels in the hierarchical instance
structures. We then organize these labeled
attributes, along with the hierarchy among
them and their instances, in XML format (see
Figure 10b).
computer.org/intelligent

Some of the data instances might not
reveal any frequent labels. For example, in
our experiments with the News domain, we
found no frequent labels across News
instance segments. In such cases, whenever
the attributes’ labels (such as the article’s
title, body, author, or city) are missing, our
algorithm groups data values with their associated signature paths. That is, we maintain
a hierarchy of attributes where internal labels
correspond to the signature paths and leaf
nodes correspond to atomic instance values.
The issue of labeling these missing attribute
names is an interesting future research problem that we plan to investigate.
Experimental results
Table 3 presents some of the experiment’s
precision and recall values. You can access the
experimental data for instance extraction from
the News and Hotels domains at www.public.
asu.edu/~snagaraj/OntoMiner/Instances.
31

I n f o r m a t i o n

I n t e g r a t i o n

Table 3. Precision and recall values.
Task

Precision

Recall

Extracting instances
from concept pages
of Hotel pages

85%

84%

Extracting unlabeled
attributes for News
instances

81%

97%

Identifying the correct
labels from the Hotel
data instances

80%

91%

O

(a)
- <node label="ROOT (A)">
- <node label="GUEST ROOMS &
AMENITIES (A)">
<node label="High Speed Internet Access
in All Rooms (Charge) (V)"/>
<node label="Air-Conditioned Rooms
(V)"/>
<node label="Alarm Clock (V)"/>
<node label="Balcony (V)"/>
<node label="Cable Channels (V)"/>
<node label="SOME OF THE AMENITIES
ABOVE MAY NOT BE AVAILABLE IN
ALL ROOMS. (A)">
</node>
- <node label="HOTEL SERVICES (A)">
<node label="24-Hour Front Desk (V)"/>
<node label="Business Center/Services
(V)"/>
<node label="Car Rental Service (V)"/>
<node label="Concierge Service (V)"/>
<node label="Fitness Facility (V)"/>
</node>
+ <node label="MEETING & EVENT
FACILITIES (A)"/>
- <node label="RESTAURANTS & LOUNGES
(A)">
<node label="Ring's Steaks, Seafood and

Chops (V)"/>
<node label="Ring's Club (V)"/>
<node label="HOURS OF OPERATION:
(A)">
</node>
- <node label="LOCAL AREA (A)">
<node label="Cisco Systems (V)"/>
<node label="Lucent Technologies (V)"/>
<node label="The Great Mall of the Bay
Area (V)"/>
<node label="San Jose Municipal Golf
Course (V)"/>
<node label="Paramount's Great America
Theme Park (V)"/>
<node label="DRIVING DIRECTIONS (V)"/>
- <node label="LOCAL ATTRACTIONS (A)">
<node label="The Great Mall of the Bay
Area - 2 Miles (V)"/>
<node label="San Jose Municipal Golf
Course - 3 Miles (V)"/>
<node label="Paramount's Great America
Theme Park - 4 Miles (V)"/>
<node label="Spring Valley Golf Course 4 Miles (V)"/>
</node>
</node>
</node>

(b)
Figure 10. (a) A sample hotel page from our experiments; (b) the attributes we
extracted from it encoded as XML. The attribute name labels are capitalized in the
XML file to distinguish them from attribute value labels.
32

computer.org/intelligent

verall, OntoMiner attains a precision
of 80 percent and recall of more than
90 percent for taxonomies, labels, and values of their instances. Currently, the Hierarchical Partitioner algorithm does not handle
Web pages that have data formatted in
HTML tables. We plan to augment HP with
the ability to structure HTML tables using
existing solutions for detecting and wrapping
these structures.10 Also, the heuristic rules
used for promotion are not consistent among
the different kinds of Web pages. Similarly,
the four cost factors do not always perform
well for all kinds of Web pages. We plan to
investigate techniques that combine syntactic as well as semantic regularities,3 and we
will try to iteratively repair hierarchical partitions with bootstrapped ontologies to yield
better precision and recall.

References
1. T. Berners-Lee, J. Hendler, and Ora Lassila,
“The Semantic Web,” Scientific American,
May 2001, pp. 34–43.
2. S. Haustein and J. Pleumann, “Is Participation in the Semantic Web Too Difficult?”
Proc. 1st Int’l Semantic Web Conf., LNCS
2,342, Springer-Verlag, 2002, pp. 448–453.
3. S. Mukherjee, G. Yang, and I. Ramakrishnan,
“Annotating Content-Rich Web Documents:
Structural and Semantic Analysis,” Proc. Int’l
Semantic Web Conf., Springer-Verlag, 2003,
pp. 245-249.
4. M. Craven et al., “Learning to Extract Symbolic Knowledge from the World Wide Web,”
Proc. 15th Nat’l Conf. Artificial Intelligence,
AAAI Press, 1998, pp. 509–516.
5. A. Arasu and H. Garcia-Molina, “Extracting
Structured Data from Web Pages,” Proc. 2003
ACM SIGMOD Int’l Conf. Management of
IEEE INTELLIGENT SYSTEMS

T h e
Data (SIGMOD 03), ACM Press, 2003, pp.
337–348.

A u t h o r s
Hasan Davulcu is an assistant professor in the Computer Science and Engi-

6. C.Y. Chung, M. Gertz, and N. Sundaresan,
“Reverse Engineering for Web Data: From
Visual to Semantic Structures,” Proc.18th
Int’l Conf. Data Eng. (ICDE 02), IEEE CS
Press, 2002, pp. 53–63.

neering Department at Arizona State University, where he founded the Cognitive Information Processing Systems Lab. His research interests include
ontology-directed information extraction, the Semantic Web, data and text
mining, and information integration. He is a member of the IEEE and the
ACM. He received his PhD from the State University of New York, Stony
Brook. Contact him at the Dept. of Computer Science and Eng., Arizona State
Univ., Box 875406, Tempe, AZ 85287-5406; hdavulcu@asu.edu.

7. V. Crescenzi, G. Mecca, and P. Merialdo,
“Roadrunner: Towards Automatic Data
Extraction from Large Web Sites,” Proc. 27th
Int’l Conf. Very Large Data Bases, Morgan
Kaufmann, 2001, pp. 109–118.

Srinivas Vadrevu is a PhD student in the Computer Science and Engineering Department at Arizona State University. His research interests include
data mining, Web mining, machine learning, and bioinformatics. He received
his MS in computer science from the University of Minnesota Duluth. Contact him at the Dept. of Computer Science and Eng., Arizona State Univ.,
Tempe, AZ 85287-5406; svadrevu@asu.edu.

8. M. Porter, “An Algorithm for Suffix Stripping,” Program, vol. 14, no. 3, 1980, pp.
130–137.

Saravanakumar Nagarajan is pursuing his MS in the Computer Science and

9. R. Korfhage, Information Storage and
Retrieval, John Wiley, 1999.
10. W. Cohen, M. Hurst, and L. Jensen, “A Flexible Learning System for Wrapping Tables
and Lists in HTML Documents,” Proc. 11th
Int’l World Wide Web Conf., ACM Press,
2002, pp. 232–241.

For more information on this or any other computing topic, please visit our Digital Library at
http://computer.org/publications/dlib.

Engineering Department at Arizona State University. His research interests
include Semantic Web mining, bioinformatics, and software engineering. He
received his BE in computer science from the University of Madras, India.
Contact him at the Dept. of Computer Science and Eng., Arizona State Univ.,
Tempe, AZ 85287-5406; nrsaravana@asu.edu.

I.V. Ramakrishnan is a full professor in the Department of Computer Sci-

ence at the State University of New York at Stony Brook. He has done extensive research in logic programming and intelligent Web agents. Recently he
has been conducting research and technology development for extracting
information from semistructured and unstructured data sources using machine
learning and the Semantic Web. Contact him at the Dept. of Computer Science,
1421 Computer Science, State Univ. of New York, Stony Brook, NY 117944400; ram@cs.sunysb.edu.

Shhh. Pervasive Computing,

Pass it on…
IEEE Pervasive Computing delivers the latest
developments in pervasive, mobile, and
ubiquitous computing. With content that’s
accessible and useful today, the quarterly
publication acts as a catalyst for realizing the
vision of pervasive (or ubiquitous) computing
Mark Weiser described nearly a decade ago —
the creation of environments saturated
with computing and wireless
communication yet gracefully
integrated with human users.
Editor in Chief: M. Satyanarayanan
Carnegie Mellon Univ. and Intel Research
Pittsburgh

UPCOMING ISSUES:
✔ Sensor and Actuator
Networks
✔ Art, Design &
Entertainment
✔ Handheld Computing

Associate EICs: Roy Want, Intel Research; Tim
Kindberg, HP Labs; Deborah Estrin, UCLA; Gregory
Abowd, GeorgiaTech.; Nigel Davies, Lancaster
University and Arizona University
MOBILE AND UBIQUITOUS SYSTEMS

SUBSCRIBE NOW! http://computer.org/pervasive/subscribe.htm

2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking
(SocialCom), Sustainable Computing and Communications (SustainCom)

Story Forms Detection in Text through Concept-based Co-clustering
Sultan Alzahrani∗ , Betul Ceran∗ , Saud Alashri∗ , Scott W. Ruston† , Steven R. Corman† and Hasan Davulcu∗
of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287-8809
Email: {ssalzahr, betul, salashri, hdavulcu}@asu.edu
† Hugh Downs School of Human Communication, Arizona State University, Tempe, AZ 85287-1205
Email: {scott.ruston, steve.corman}@asu.edu

∗ School

extremist narratives and discover their underlying story
forms. The framework streamlines the narrative analysis by
providing bi-clusters of stories and their underlying characteristic features. Another contribution of this framework is
the improved clustering performance obtained through the
utilization of concept-based features compared to widelyused standard keyword-based features.
The recurring themes in extremist narratives can be
categorized into general story forms. These story forms are
characterized by archetypes and their actions. We aim to
reveal information about the story forms in our dataset via
clustering analysis. We observe that generalized concepts
derived from extracted overlapping subject - verb - object relationships are better suited to be used as features
in clustering since they provide information regarding the
underlying semantic structure of these story forms.
Clustering is an essential step towards the analysis of
data without prior labels or categories. Two critical aspects
of clustering are; i) semantic quality of resulting clusters
and ii) their descriptive features; i.e. clusters should be
self-descriptive in order to present a meaning to the user.
Conventional clustering methods provide ample ways to
group data. However, they do not automatically yield descriptive features for the groups without further processing
(i.e. through a classifier). Co-clustering, on the other hand,
identifies both the underlying groups, out of the data, along
with their characteristic features. It simultaneously clusters
the rows and columns of an input matrix generating a subset
of instances which exhibit high similarity across a subset of
features, called bi-clusters. Since descriptions of the clusters
are produced simultaneously with clustering, co-clustering
presents an advantage over conventional clustering methods
for our application.
The initial efforts in co-clustering text data relied on
term-document matrices and lexical features, mainly ngrams. In this paper, we perform co-clustering of stories using two different types of features: standard unigrams/bigrams and generalized concepts that rely on extracted linguistic roles. We employ the model developed in
Ceran et. al and Alashri et. al. [1], [2] to produce generalized concept-based representations of extremist stories. We
show that the residual error of factorization with conceptbased features is lower than the residual error with standard
keyword-based features. Qualitative evaluations also suggest

Abstract—A story is defined as actors taking actions that
culminate in resolutions. In this paper, we extract subject - verb
- object relationships from paragraphs and generalize them into
semantic conceptual representations. Overlapping generalized
concepts and relationships correspond to archetypes/targets
and actions that characterize story forms. We present an
analytic framework which implements co-clustering based on
generalized conceptual relationships to automatically detect
such story forms. Co-clustering can help in identifying similarities that exist in low-dimensional sub-spaces of sparse data
such as textual paragraphs. Through co-clustering, we detect
not only the clusters themselves but also their characteristic
features which can be useful in describing and summarizing
their contents. We perform co-clustering of stories using two
different types of features: standard unigrams/bigrams and
generalized concepts. We show that the residual error of factorization with concept-based features is significantly lower than
the error with standard keyword-based features. Qualitative
evaluations also suggest that concept-based features yield more
coherent, distinctive and interesting story forms compared to
those produced by using standard keyword-based features.
Index Terms—Story forms, Narrative analysis, Non-negative
matrix factorization, Co-clustering.

1. Introduction
A key component of spreading an ideology is the utilization of cultural narratives tailored to specific target audiences. For example, extremists are known to adopt historically deeply rooted narratives from the cultural heritage
of their target audience in order to gain their attention.
Narratives are systems of stories that are linked by common
archetypes, forms and themes. A story is defined as an
actor(s) taking an action(s) that culminates in a resolution(s).
The actors, actions and resolutions in these stories form
the template of a strategic message regarding the current
events which is used by extremists to justify their actions and
policies, persuade their target audience and gain followers.
Hence identifying and countering the story forms found
in these messages is an essential part of counter violent
extremism efforts. This paper presents a framework to help
subject matter experts rapidly analyze large collections of
978-1-5090-3936-4/16 $31.00 © 2016 IEEE
DOI 10.1109/BDCloud-SocialCom-SustainCom.2016.48

257
258

Figure 1. System Architecture

the labeled benchmark Reuters data set. The concepts used
in [9] are named entities which are extracted using an external information source (Wikipedia), whereas the conceptbased features that we use are in the form of generalized
relational semantic triples which are produced by processing
the document set itself.

that concept-based features yield more coherent, distinctive
and interesting story forms compared to those produced by
utilizing standard keyword-based features.
The rest of the paper is organized as follows.
§ 2 presents related works. The system architecture is
presented in § 3. § 4 describes the extraction of lexical and
linguistic features and co-clustering algorithms. In § 5, we
present experimental evaluations and results. § 6 concludes
the paper.

3. System Architecture

2. Related Work

The components of our framework are presented in
Figure 1. Each component is briefly described below, and
the technical details are presented in the following sections.

Various types of co-clustering algorithms based on matrix factorization, probabilistic and geometric models have
been developed in literature [3] and they have been applied
in many different domains such as text, bioinformatics, and
image analysis.
One of the pioneering works in this area is Dhillon
et. al. [4] which introduces spectral co-clustering of documents and their terms by leveraging the singular value
decomposition of the term-document matrix. Non-negative
Matrix Factorization (NMF) has also been adapted for coclustering [5]. Different versions of NMF for co-clustering
such as [6] have been developed to improve the initial
model.
Kok and Domingos [7] proposed a clustering framework
to produce generalized concepts and relations similar to the
concept-based features utilized in this paper. Their algorithm
is purely statistical and relies on second order Markov Logic.
They compared their results with other algorithms on a goldstandard clustering scheme created by a human. Kang et
al. [8] used tensor decomposition in order to obtain contextual synonyms, and generalized concepts/relationships;
however, they do not present any formal evaluations of their
results. We generate the concept-based features using the
method proposed in Ceran et. al. [1] and and Alashri et. al.
[2].
Jing et.al. [9] studied co-clustering text along with
term and concept features, which were generated using
Wikipedia. They present a higher-order co-clustering framework where they improve the performance of conventional
clustering. They present an evaluation of their work on

•

•

•

•
•

•

259
258

The input document set consisting of stories. We
analyze the data at the paragraph level, i.e. each
document contains a single story paragraph. We
apply pre-processing in order to clean and prepare
the paragraphs for feature extraction. (Steps 1 and
2)
Three different feature sets (unigrams, bigrams and
generalized concepts/relations) are generated from
the story paragraphs. Concepts/relations are obtained
using the method proposed in [1]. Triplet generation
step has been modified according to [2]. (Steps 3, 4
and 5)
Unigrams and bigrams are ranked based on their
TF-IDF values, by initially generating Vector Space
Model (VSM) corresponding to each feature set
and then followed by constructing the DocumentFeature-Matrices (DFMs), for short, feature matrices
(Steps 6 and 7).
A binary feature matrix is also created with concept/relational features. (Step 8)
Co-clustering algorithm is run on unigram, bigram
and concept-based feature matrices to produce biclusters of story forms and their associated features.
(Steps 9, 10 and 11)
Quantitative and qualitative evaluations are performed on the resulting bi-clusters, § 5.

Figure 2. Generation of concepts/relations form hsubjects, verbs, objectsi triplets

4. Methodology

In [1], we utilize both syntactic and semantic corpusbased merging criteria to generalize triplets into concepts. A
pair of hsubjecti-hverbsi-hobjectsi triplets is merged further
only if (i) they share a common context among their corresponding terms (i.e. syntactic criteria) and (ii) they satisfy
corpus-based support and similarity measure of “contextual synonymy” (i.e. semantic criteria) between their newly
added terms and existing terms. Next, a hierarchical bottomup merging algorithm allows information to propagate between clusters of related subjects, verbs and objects leading
to a set of generalized concepts.

4.1. Problem Definition
For a given a set of documents comprising stories
{D1 , . . . , DN } where N denotes the number of documents,
we generate two sets of features: n-grams features and
the generalized concepts. Our main objective is to identify
which type of features yield better bi-clustering of stories
into story-forms. We evaluate the quality, initially quantitatively by employing NMF residual error measure presented
in § 5.3.2 and qualitatively in collaboration with a subject
matter expert in § 5.4

Figure 2 illustrates an instance of how syntactic and
semantic criteria are applied on a sample set of triplets
extracted from our story corpus. Initially, syntactic criterion is satisfied between the pair of triplets: hBoko Haram,
fight, Christiansi and hBoko Haram, fight, infidelsi since
they share a common (subject, verb) context (Boko Haram,
fight). Hence, this pair becomes a candidate for merging if
a “contextual synonymy” relationship exists between their
newly added and existing terms (i.e. Christians and infidels).
Contextual synonyms are not synonyms in the traditional
dictionary sense, but they are phrases that may occur in
similar semantic roles and associated with similar contexts.
In the next step the resulting generalized concept hBoko
Haram, fight, {Christians, infidels}i can be merged with
hJama’atu Ahlis Sunna, fight, infidelsi due to their shared
(verb, object) context: (fight, infidels) meeting the syntactic
criteria, and due to the existence of contextual synonymy relationship between Boko Haram and Jama’atu Ahlis Sunna.
Syntactic criterion is applied iteratively to identify candidate
concepts for merging in combination with the application
of semantic criterion to screen for the introduction of new
topics that could cause a generalized concept to drift from
it original meaning. Let us consider an additional pair of
candidates for merging based on syntactic criteria: h{Boko
Haram, Jama’atu Ahlis Sunna} fight, {Christians, infidels}i
and hJama’atu Ahlis Sunna, {fight, wage war}, {infidels,
democracy}i. First, a core component is created using only
the intersections of the subject, verb, object sets of these
two concepts: hJama’atu Ahlis Sunna, fight, infidelsi. The
remaining words from the two candidates can be added
to the core concept only if they are among the closest

4.2. N-gram Features
We extracted highest ranked unigrams and bigrams by
utilizing term frequency - inverse document frequency (TFIDF), a simple form of cross entropy and a popular technique used in informational retrieval tasks.

4.3. Generalized Concept-based Features
Subject - verb - object triplet extraction is the basic
building block towards generalized concepts. We first process the story corpus to resolve its co-references using stateof-the-art coreference resolvers [10], [11], [12], [13]. Previously, Ceran et. al. [14] utilized ClearNLP [15] to extract
triplets. However, using this triplet extractor alone resulted
in poor recall. Alashri et al [2] proposed an enhanced
approach that utilized additional triplet extractors: AlchemyAPI [16], Everest [17], Reverb [18] and implemented a
Cartesian product of the atomized phrases in all argument
positions to double the production of extracted triplets.
Ceran et. al [1] utilized generalized triplets and compared
their performance with keywords in a classification model
to show that the triplets yield a significant 36% boost in
performance for the story detection task. However, although
triplets as features carried more semantic information, they
were showing high sparsity during matching across the
document corpus. Hence, we moved to a generalized triplet
representation by suitably “merging triplets” into generalized concepts without a drift.
260
259

contextual synonyms of at least one of the already existing
members in the core item. For example, the algorithm
would permit the addition of Boko Haram, wage war and
Christians to the resulting set since the newly added terms
are among the closest contextual synonyms of Jama’atu
Ahlis Sunna, fight and infidels in their respective argument
positions. However, democracy would be left out of the
object argument position in the resulting generalized concept
since it is not among the contextual synonyms of neither
infidels nor the Christians according to the corpus based
definition of “contextual synonymy” (i.e. semantic criteria).

be used as the number of clusters and it has to satisfy
K < min{M, N }. The mathematical formulation of the
optimization problem is written as follows:
n

minimize
U,V

1
λX
1
2
2
kX − U V kF + kU kF +
kvi k1
2
2
2 i=1

subject to U, V ≥ 0
The above optimization problem is a modified version
of NMF proposed by [6] since the standard form of NMF
has shortcomings of non-unique and scale-variance outputs.
Kim et al. [6] enhanced sparseness degree of basis vectors
by introducing regularizations as well as alternating negative
constraints update technique based on the multiplicative
update. The multiplicative update, in the standard NMF,
does not necessarily yield sparse basis vectors. The sparse
optimization problem can be solved using non-negative
quadratic programming (NNQP). The modified NMF compares different feature sets by looking into the residual error
E after factorization, where E , shown in Equation 1, is the
error term after decomposing A matrix into U and V . Lower
E values indicate better underlying structure detection in A.
We used the software package in [20] for the implementation
of the Sparse NMF.

4.4. Co-Clustering
Clustering is an unsupervised learning technique that
tries to draw inferences from given data where data labels
(i.e. classes) are concealed or unknown, as in our case). This
approach is adopted to assist in benchmarking unigrams,
bigrams and the generalized concepts as features when used
for story forms detection in a story corpus. We aim to
investigate which feature set will provide us with the highest
quality bi-clustering results. Comparing different feature sets
while applying co-clustering will not only allow us to determine which feature set can quantitatively perform better but
also, it can prompt us about which feature set could provide
more coherent, distinctive and interesting story forms as
clusters.
To formalize the co-clustering problem, let’s assume
our story corpus contains M documents and N features
provided as the matrix A = (aij )M ×N such that aij
represents the entry value of i-th story document and j th feature. The A feature-term-matrix can be also written as
A = (R, C) ∈ <M ×N where R = {1, 2, . . . , M } denotes
row indices, and C = {1, 2, . . . , N } denotes column indices.
Here, our objective is to find set of sub-matrices or biclusters, say Bk (Xk , Yk ), such that X = M1 , . . . , Ma ⊆ R
and Y = N1 , . . . , Nb ⊆ C as separate subsets of R and
C . This task is an NP-hard problem [19], but an optimization approach with a greedy iterative search utilizing Nonnegative Matrix Factorization (NMF) has been shown to
produce effective results.

A = UV + E

(1)

5. Experimental Results
5.1. Corpus
Our story corpus consists of 6, 856 paragraphs which are
pulled from a database of Islamist extremist texts. Texts are
collected from online sources websites, blogs and other news
sources that are known to be outlets of extremist groups such
as Al-Qaeda, ISIS or their followers who sympathize with
their cause and methods. Extremists’ texts are not entirely
composed of stories. After the crawling process, subject
matter experts annotated the paragraphs based on a coding
system, consisting of eight mutually-exclusive categories:
story, exposition, imperative, question, supplication, verse,
annotation, and other. A paragraph is labeled as a story if
it tells a sequence of related events, leading to a resolution
or projected resolution. In our experiments, we work on the
paragraphs which are coded as stories.

4.4.1. Non-negative Matrix Factorization (NMF). Coclustering is an NP-hard problem, yet many different optimization based approximation algorithms have been developed in the literature. One of those is the non-negative
matrix factorization or decomposition based method which
factorizes a given matrix into multiple matrices revealing
substructure patterns within the matrix. This method has
been widely used in many applications such as bioinformatics, image processing, and text mining. NMF can be
used to factorize our A ∈ <M ×N feature-term-matrix into
a pair matrices U ∈ <M ×K and V ∈ <K×N having nonnegative elements, such that A ' U V constructing an approximation, as shown in Equation 1. U represents the basis
vectors (or factors), and V represents the coefficients on the
linear combination of the factors that allows construction
of the original A feature-document-matrix. K variable can

5.2. Number of Clusters
There is no ground truth of story forms available for
our story corpus therefore, we resort to additional analysis
to determine the number of clusters before we present the
results to subject matter experts for qualitative evaluation.
Determining the right number of clusters has been a challenging problem in clustering and various techniques have
been suggested in the literature to solve this problem. First,
we obtain an embedding of stories × concepts feature
261
260

Residual error is computed by using the formula shown in
Equation 1. The error decreases as the number of clusters
increases since the number of clusters also represents the
dimensionality of the resulting approximation matrices. In
residual error plots, it can be observed that the concept-based
features consistently yield lower residual errors compared to
both the unigram and bigram based features.

matrix into 2-D. We use t-Distributed Stochastic Neighbor Embedding (t-SNE) [21] technique to reduce the data
dimension and visualize the block diagonal sub-structures.
Next, we use an external measure from literature, CalinskiHarabasz index [22], to measure the quality of a clustering
across different numbers of clusters. Calinski-Harabasz index or variance ratio criterion (VRC) is proportional to the
ratio of the overall between-cluster variance and the overall
within-cluster variance. In this scheme, the higher corresponding VRC value, the better the clustering performance.
Figure 3 (a) shows a plot of VRC values across a number of
clusters ranging from 2 to 14. The rule of thumb suggested
in the literature is to discern the values which cause a sharp
spike in the VRC plots. In Figure 3 (b), we can see that there
is a sharp spike at 6 clusters. This indicates that setting the
number of clusters to 6 is a plausible choice in order to
obtain a good clustering scheme. Figure 3 (b) shows the
scatter plot of 6 clusters obtained using K-means after 2D
embedding. The cluster centroids are also marked alongside
their error ellipses representing their covariance matrices.

5.4. Qualitative Evaluation
To determine if the clusters generated by the conceptrelations technique yielded a valuable analytic tool and an
improvement over other clustering methods for the anticipated use case (rapidly analyzing large amounts of story text
to determine themes and overarching narratives to benefit
strategic communication and counter-messaging activities),
a subject matter expert (SME) conducted a qualitative evaluation.
Six clusters were generated using the concept-relations
technique discussed here and six other clusters were created
using bi-gram co-clustering techniques- Tables 1 and 2
provide a summary for each clustering scheme based on
two feature sets. The SME read the stories drawn from
each of the twelve clusters (i.e. 6 for each) without cluster
identification noting narratively significant features such as
the protagonists and antagonists, types of actions taken,
and evident and implied resolutions. Subsequently, the SME
also conducted an evaluation of the feature sets of each
clustering method, looking for patterns and indicators of
meaning useful to a communication analyst. These efforts
were synthesized to draw conclusions about the clusters.
5.4.1. Concept Clusters vs. Bigram Clusters. In general,
both clustering methods produced some distinct clusters
that make sense under qualitative evaluation. Notably, the
dataset is dominated by stories with an overall structure
described by previous analysis as a “victorious battle story”.
In this story form, a protagonist (member of some extremist
group) takes some form of military action killing or injuring
antagonists (US forces or police) [23]. The prevalence of this
basic story form within the dataset complicates identifying
robustly distinct clusters in terms of narrative significance.
This is because the characteristics that distinguish groups
of stories tend to be the terms used for protagonists and
antagonists and the settings, whereas the general meaning
(successful attack by insurgent forces) remains relatively
constant.
However, despite that limitation, the concept clustering method produced meaningful clusters with notable distinctions and with useful implications for communication
analysis. The bigram cluster method produced clusters with
less distinctiveness and significance in terms of overall
meaning. For example, bigram clusters 2 and 3 are nearly
impossible to distinguish, involving similar stories, nearly
identical actions, and having a wide range of protagonists
and antagonists. In the set of concept clusters, clusters 1,
2, 5 and 6 were the most distinctive clusters, especially 5
and 6. The stories in cluster 5 are very similar: mujahidin in

Figure 3. Analysis to determine number of clusters

5.3. Quantitative Evaluation
5.3.1. Block diagonal sub-structure. Figure 4 shows the
block diagonal sparsity structures of the feature matrices
after clustering, in which, the bi-clusters (unigrams vs.
stories bi-clusters in (a), bigrams vs. stories in (b), concepts
vs. stories in (c)) are represented along the diagonal blocks.
Block diagonal plots are obtained by reordering the indices
of stories and their characteristic features in each row and
column for each cluster to show their groupings.
5.3.2. NMF residual error. Figure 4 (d) show the residual
error of non-negative matrix factorization of the three different feature matrices across different numbers of clusters.
262
261

Figure 4. Feature matrices after co-clustering for all feature sets - block diagonal substructure is more coherent for the concept stories biclusters as in (c)

in western Kirkuk. The explosion resulted in destroying a
specialized vehicle and killing or wounding all those on
board. Praise be to God, the Lord of all creation.” Like
Concept Cluster 2’s dispassionate, news-style reporting of
operations, this cluster’s emphasis on the successful results
of the attacks convey the meaning that the insurgents are a
strong force, a strong champion and are winning the conflict. Importantly, this meaning is contained in the semantic
combination of attack and result, but these words are often
separated by dependent clauses or in completely separate
sentences. This association of attack and result would not
be detected and clustered by bigram clustering techniques,
giving concept-based features an edge as it is capable of
extending the features into a higher semantic level.
Concept Cluster 1 has an additional significant feature: the antagonists are almost exclusively referred to by
derogatory epithets (“apostates”, “pagan army”, “safavids”,
“Crusaders”). This rhetorical technique dehumanizes these
groups and assigns unsavory and immoral characteristics
to them, thereby emphasizing the threat to the Ummah by
their very existent. Violence by the community’s champion
is therefore justified against these groups that threaten the
community. Identifying the rhetorical techniques is the first
step to defusing their inflammatory and radicalizing power,
and thus a technique that can distill these constructions from
a body of text data is valuable.

Afghanistan attack US and Afghan forces with improvised
explosives. The stories consistently refer to the Afghan
forces as “puppets”.
5.4.2. Notable clusters. While the variations across the
dataset are subtle (as noted above), the concept clustering
method did usefully identify some meaningful clusters. Concept Cluster 2, for example, contains stories with a particular
subject-verb construction as this example illustrates: “Another martyr operation was carried out by Mujahid Abdul
Wali, who carried out the attacks on military bases of puppet
Afghan soldiers that were still in the same district with
the first martyr operations.” This construction contributes to
a semi-objective news-narrative, belying the propagandistic
content. This format contributes to the positioning of the
mujahidin as champions of Islam defending the Ummah- the
Islamic Community, and also conveys that they are winning
the war [24].
Concept Cluster 1 exhibits another very consistent formulation that contributes to strategic communication goals.
Stories in this cluster are dominated by the verb construction
of attack-result in which the stories describe an attack and
include the results such as the example: “At 11:45 [ 08:45
GMT] on 5 August, one of our combat groups detonated
a guided explosive device against a patrol belonging to
the Crusader occupation forces on Kirkuk-Al-Riyad Road
263
262

TABLE 1. B IGRAMS - BASED C LUSTERS
Cluster
1

2

3
4
5
6

Key narrative features
Protagonists: often unspecified, mujahidin; Actions: attacking
with emphasis on bombs and vehicles, Note: name of Taliban
spokesman frequent; emphasis is Afghanistan;
Protagonists: lions of ansar; Antagonists: inconsistent names, locations action: wide range, with most frequent being detonation
destroying vehicles, with praise and gratitude to God. Emphasis
on date
Actors: security detachment (presumed subject/attacker) Action:
emphasis on attack/result, detonation, and killing result; strong
emphasis on date
Action: attacks in Afghanistan; against Actors: US, NATO, invaders and puppets, vehicle/military base
Actors: Shield of,Islam Brigades, mujahidin, AQIM; Iraqi National
Guard, Mahdi army, and police Action: detonate explosive
Actors: US, ISIS, God, Bin Laden, Shabaab, mujahidin, messenger; Frequent invocations of god, god’s grace and praise; action:
frequent construction ‘carried out’ operation

Notes and significance
Protagonists and antagonists are not consistent; stories contain
repeated phrases; mention of spokesman name a distinguishing
feature
No significant difference in action between cluster 1, 2 and 3
Only difference between cluster 2 and cluster 3 is frequent actor
“security detachment”
Wide variety of actions within general category of ‘attack’; clear
focus on Afghanistan;
Similar to clusters 1-3, but with greater emphasis on claims of
attacks, potentially indicating purpose/intent of story
No consistency to the locations or actors in this cluster; variety
points of view (POV)

TABLE 2. C ONCEPT- BASED C LUSTERS
Cluster
1
2

3

4
5
6

Key narrative features
Protagonists either unspecified or “Lions of Islam”; frequent construction of attack-result; antagonists always labeled with epithets
(apostates, pagans, safavids, crusaders)
Protagonists: Lions or mujahidin; actions: attacks carried out; news
format
Protagonists: mujahidin, Shabaab, Lions; Antagonists: US forces,
apostates, Federal Police; Action: detonation of IEDs, car bombs,
landmines and other explosives; settings: Afghanistan, Iraq, Somalia
Significant variation in protagonists, antagonists, settings and actions; Minor emphasis on the killing of women and children (by
US/allies)
Protagonist: mujahidin; antagonists: US and puppets; action: bomb
blast, detonation
Protagonists: Lions of Ansar Islam; actions: plant or detonate
bombs; antagonists: US, apostates, crusaders

6. Conclusion

Notes and significance
Function: justify the threat to Muslims by ‘others’ who are not
to be respected
Function: legitimize the insurgent/extremist actions by formatting in a news report format; convey the extremists are winning
the war and are champions of Islam
Similar to cluster 5, but with much more variation; Highlights
the vulnerability of adversary forces and highlights effectiveness
across Muslim regions
Very loose cluster with no discernable patterns
Very tight cluster of stories of IED attacks against US and
Afghan forces (puppets) set primarily in Afghanistan
Another very tight cluster, analogous to Cluster 5 but set in Iraq;
illustrates geographically specific epithet

presents a better semantic pattern in terms of strategic
communication as opposed to the repetitive and scattered
content of the bigram clusters as outlined by the SME.

Narratives are systems of stories that construct meaning [25]. That meaning is constructed in part by the patterns
of relationships created by the actors and actions that make
up the constituent stories [26]. In order to analyze the
narratives circulating within a discursive environment, the
ability to distill stories from a larger corpus of information, and then cluster those stories into meaningful groups
of story forms is necessary. The clustering method must
account for patterns of relationships of actors and actions.
In this paper, we show that the concept-based co-clustering
method described here, with its attention to subjects and
objects (actors) and verbs (actions) makes a step towards
a robust method that meets this strategic communication
analysis goal. Concept-based features yield a better clustering scheme compared to bigram features both quantitatively
and qualitatively. The lower residual error in NMF achieved
by concept-based features shows that concept-based features
can capture more information than high level noisy outcomes compared to the other feature sets. Additionally, the
content of the clusters produced by concept-based features

Acknowledgment
This research was partially supported by DoD Minerva
Research Initiative Grant N00014-15-1-2821.

References

264
263

[1]

B. Ceran, N. Kedia, S. R. Corman, and H. Davulcu, “Story detection
using generalized concepts and relations,” in Proceedings of the 2015
IEEE/ACM International Conference on Advances in Social Networks
Analysis and Mining 2015. ACM, 2015, pp. 942–949.

[2]

S. Alashri, S. Alzahrani, J.-Y. Tsai, S. R. Corman, and H. Davulcu,
“Climate change frames detection and categorization based on generalized concepts,” International Journal of Semantic Computing,
vol. 10, no. 02, pp. 1–20, 2016.

[3]

H. Zhao, A. Wee-Chung Liew, D. Z Wang, and H. Yan, “Biclustering
analysis for pattern discovery: current techniques, comparative studies
and applications,” Current Bioinformatics, vol. 7, no. 1, pp. 43–55,
2012.

[4]

I. S. Dhillon, “Co-clustering documents and words using bipartite
spectral graph partitioning,” in Proceedings of the Seventh ACM
SIGKDD International Conference on Knowledge Discovery and
Data Mining. ACM, 2001, pp. 269–274.

[15] J. D. Choi, “Optimization of natural language processing components
for robustness and scalability,” Ph.D. dissertation, University of Colorado Boulder, 2014.

[5]

C. Ding, T. Li, W. Peng, and H. Park, “Orthogonal nonnegative
matrix t-factorizations for clustering,” in Proceedings of the 12th
ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining. New York, NY, USA: ACM, 2006, pp. 126–135.

[16] (2015) Alchemy api language features. AlchemyAPI, Inc. [Online].
Available: http://www.alchemyapi.com/products/alchemylanguage

[6]

H. Kim and H. Park, “Sparse non-negative matrix factorizations via
alternating non-negativity-constrained least squares for microarray
data analysis,” Bioinformatics, vol. 23, no. 12, pp. 1495–1502, 2007.

[7]

S. Kok and P. Domingos, “Extracting semantic networks from text via
relational clustering,” in Proceedings of the 2008 European Conference on Machine Learning and Knowledge Discovery in Databases
- Part I, 2008, pp. 624–639.

[8]

U. Kang, E. Papalexakis, A. Harpale, and C. Faloutsos, “Gigatensor:
Scaling tensor analysis up by 100 times - algorithms and discoveries,”
in Proceedings of the 18th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM, 2012, pp. 316–
324.

[9]

[17] (2013) Everest triplet extraction. Next Century Corporation. [Online].
Available: https://github.com/NextCenturyCorporation/ EVERESTTripletExtraction
[18] A. Fader, S. Soderland, and O. Etzioni, “Identifying relations for
open information extraction,” in Proceedings of the Conference on
Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, 2011, pp. 1535–1545.
[19] S. Busygin, O. Prokopyev, and P. M. Pardalos, “Biclustering in data
mining,” Computers & Operations Research, vol. 35, no. 9, pp. 2964–
2987, 2008.
[20] Y. Li and A. Ngom, “The non-negative matrix factorization toolbox
for biological data mining,” Source code for biology and medicine,
vol. 8, no. 1, p. 1, 2013.
[21] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne,”
Journal of Machine Learning Research, vol. 9, no. 2579-2605, p. 85,
2008.

L. Jing, J. Yun, J. Yu, and J. Huang, “High-order co-clustering
text data on semantics-based representation model,” in Advances in
Knowledge Discovery and Data Mining. Springer, 2011, pp. 171–
182.

[10] K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers, M. Surdeanu,
D. Jurafsky, and C. Manning, “A multi-pass sieve for coreference
resolution,” in Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing. Association for Computational Linguistics, 2010, pp. 492–501.

[22] T. Caliński and J. Harabasz, “A dendrite method for cluster analysis,”
Communications in Statistics-theory and Methods, vol. 3, no. 1, pp.
1–27, 1974.
[23] C. Lundry, S. R. Corman, R. B. Furlow, and K. W. Errickson, “Cooking the books: Strategic inflation of casualty reports by extremists in
the afghanistan conflict,” Studies in Conflict & Terrorism, vol. 35,
no. 5, pp. 369–381, 2012.

[11] H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu, and
D. Jurafsky, “Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task,” in Proceedings of the Fifteenth
Conference on Computational Natural Language Learning: Shared
Task. Association for Computational Linguistics, 2011, pp. 28–34.

[24] S. Corman, S. Ruston, and M. Fisk, “A pragmatic framework for
studying extremists’ use of cultural narrative,” in 2nd International
Conference on Cross-Cultural Decision Making: Focus 2012, 2012,
pp. 21–25.

[12] H. Lee, A. Chang, Y. Peirsman, N. Chambers, M. Surdeanu, and
D. Jurafsky, “Deterministic coreference resolution based on entitycentric, precision-ranked rules,” Computational Linguistics, vol. 39,
no. 4, pp. 885–916, 2013.

[25] J. R. Halverson, S. R. Corman, and H. Goodall Jr, Master narratives
of Islamist extremism. Palgrave Macmillan, 2011.

[13] M. Recasens, M.-C. de Marneffe, and C. Potts, “The life and death
of discourse entities: Identifying singleton mentions.” in HLT-NAACL,
2013, pp. 627–633.

[26] S. W. Ruston, “More than just a story: Narrative insights into comprehension, ideology and decision-making,” in Modeling sociocultural
influences on decision making: Understanding conflict, enabling stability, J. V. Cohn, S. Schatz, H. Freeman, and D. J. Y. Combs, Eds.
CRC Press.

[14] B. Ceran, R. Karad, A. Mandvekar, S. R. Corman, and H. Davulcu,
“A semantic triplet based story classifier,” in Proceedings of the 2012
IEEE/ACM International Conference on Advances in Social Networks
Analysis and Mining (ASONAM). IEEE, 2012, pp. 573–580.

265
264

Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)

MUVIR: Multi-View Rare Category Detection
Dawei Zhou, Jingrui He, K. Seluk Candan, Hasan Davulcu
Arizona State University
Tempe, Arizona
{dzhou23,jingrui.he,candan,hdavulcu}@asu.edu

Abstract

In many real-world applications, the data consists of multiple views, or features from multiple information sources.
For example, in synthetic ID detection, we aim to distinguish
between the true identities and the fake ones generated for
the purpose of committing fraud. Each identity is associated
with information from various aspects, such as demographic
information, online social behaviors, banking behaviors. Another example is insider threat detection, where the goal is to
detect malicious insiders in a large organization, by collecting various types of information regarding each employee’s
daily behaviors. To detect the rare categories in these applications, simply concatenating all the features from multiple
views may lead to sub-optimal performance in terms of increased number of label requests, as it ignores the relationship
among the multiple views. Furthermore, among the multiple
information sources, some may generate features irrelevant
to the identification of the rare examples, thus deteriorates
the performance of rare category detection.
To address this problem, in this paper, we propose a novel
framework named MUVIR for detecting the initial examples from the minority classes in the presence of multi-view
data. The key idea is to integrate view-specific posterior
probabilities of the example coming from the minority class
given features from each view, in order to obtain the estimate of the overall posterior probability given features from
all the views. In particular, the view-specific posterior probabilities can be inferred from the scores computed using
a variety of existing techniques [He and Carbonell, 2007;
He et al., 2008]. Furthermore, MUVIR can be generalized
to handle problems where the exact priors of the minority
classes are unknown. To the best of our knowledge, this paper is the first principled effort on rare category detection in
the presence of multiple views. Compared with existing techniques, the main advantages of MUVIR can be summarized
as follows.
1. Effectively leveraging the relationship among multiple
views to improve the performance of rare category detection;
2. Robustness to irrelevant views;
3. Flexibility in terms of the base algorithm used for generating view-specific posterior probabilities.
The rest of this paper is organized as follows. After a brief
review of the related work in Section 2, we introduce the pro-

Rare category detection refers to the problem
of identifying the initial examples from underrepresented minority classes in an imbalanced data
set. This problem becomes more challenging in
many real applications where the data comes from
multiple views, and some views may be irrelevant
for distinguishing between majority and minority
classes, such as synthetic ID detection and insider
threat detection. Existing techniques for rare category detection are not best suited for such applications, as they mainly focus on data with a single
view.
To address the problem of multi-view rare category
detection, in this paper, we propose a novel framework named MUVIR. It builds upon existing techniques for rare category detection with each single
view, and exploits the relationship among multiple
views to estimate the overall probability of each example belonging to the minority class. In particular, we study multiple special cases of the framework with respect to their working conditions, and
analyze the performance of MUVIR in the presence
of irrelevant views. For problems where the exact
priors of the minority classes are unknown, we generalize the MUVIR algorithm to work with only an
upper bound on the priors. Experimental results on
both synthetic and real data sets demonstrate the effectiveness of the proposed framework, especially
in the presence of irrelevant views.

1

Introduction

In contrast to the large amount of data being generated and
used everyday in a variety of areas, it is usually the case that
only a small percentage of the data might be of interest to
us, which form the minority class. However, without initial
labeled examples, the minority class might be very difficult
to detect with random sampling due to the imbalance nature
of the data, and the limited budget for requesting labels from
a labeling oracle. Rare category detection has been proposed
to address this problem, so that we are able to identify the
very first examples from the minority class, by issuing a small
number of label requests to the labeling oracle.

4098

Rare Category Detection
Rare category analysis has also been studied for years. Up
to now, many methods have been approached to address this
problem. In this paper, we mainly review the following two
existing works on rare category detection. The first one is
[He and Carbonell, 2007], in which algorithm NNDM is proposed standing on two assumptions: (i) data sets have little
knowledge about labels (ii) there is no separability or nearseparability between majority and minority classes. Both assumptions exactly meet the setting of the problem we want
to figure out. The probability distribution function (pdf) of
the majority class tends to be locally smooth, while the pdf of
minority class tends to be a more compact cluster. In general,
the algorithm measures the changes of local density around
a certain point. NNDM gives a score to each example, and
the score is the maximum difference of local density between
one item and all of its neighboring points. By querying the
examples with the largest score, it is able to hit the region of
minority class with the largest probability.
Another work about rare category detection is [He et al.,
2008], the authors provided an upgraded algorithm GRADE
based on NNDM. In this algorithm, they took the consideration of the manifold structure in minority class. For example,
two examples from the same minority class on the manifold
may be far away in Euclidean distance. In this case, they generate a global similarity matrix embedded all of the examples
from the original feature space. The items of minority class
are made to form a more compact cluster for each minority
class. Based on global similarity matrix, they measure the
changes of local density for each example. The changes of local density, to some extent, has been enlarged, and made the
minority classes easier to be discovered. Furthermore, they
provided an approximating algorithm to manage rare category detection with less information about priors of minority classes. In this paper, our proposed framework MUVIR
is generic in the sense that it can leverage multiple existing
RCD methods, such as GRADE, NNDM and etc., to analyze
the problem in the multi-view version. To the best of our
knowledge, this is the first effort on rare category detection
with multiple views.

posed framework for multi-view rare category detection in
Section 3. In Section 4, we test our model on both synthetic
data sets and real data sets. Finally, we conclude this paper in
Section 5.

2

Related Work

Multi-view Learning
Multi-view learning targets problems where the features naturally come from multiple information sources, or multiple
views. It has been studied extensively in the literature. Cotraining [Blum and Mitchell, 1998] is one of the earliest efforts in this area, where the authors proved that maximizing
the mutual consistency of two independent views could be
used to learn the pattern based on a few labeled and many unlabeled examples. Since then, multi-view learning has been
studied in multiple aspects during these years. A portion of
the researchers focus on the study of independent assumption for co-training, which is essential in the real world application. [Abney, 2002] refined the analysis of co-training
and gave a theoretical justification that their algorithm could
work on a more relax independence scenario rather than cotraining. [Balcan et al., 2004] proposed an independence
expansion and proved that it could guarantee the success of
co-training. Another line of work has been devoted to the
construction of multiple views and how to combine multiple views. In [Ho, 1998], they apply random sampling
algorithm called RSM, which perform bootstrapping in the
feature space to separate the views. [Chen et al., 2011]
transform the feature decomposition task into an optimization problem, which could automatically divide the feature
space into two exclusive subsets. While, in the aspect of how
to combine multiple views and learn models, we can separate
it into the problems of supervised learning, semi-supervised
learning and unsupervised learning. In the category of supervised and semi-supervised learning, [Muslea et al., 2003;
2006] designed a robust semi-supervised algorithm which
combined co-learning with active learning. CoMR [Sindhwani and Rosenberg, 2008] proposed a multi-view learning
algorithm based on a reproducing kernel Hilbert space with a
data-dependent co-regularization norm. In [Yu et al., 2011],
author proposed a co-training Bayesian graph model, which is
more reliable in handling the case of missing views. SMVC
[Günnemann et al., 2014] proposed a Bayesian framework
for modeling multiple clusterings of data by multiple mixture distributions. In the category of unsupervised learning,
[Long et al., 2008] introduced a general model for unsupervised multiple view learning and demonstrate it in various
types of unsupervised learning on various types of multiple
view data. The authors of [Song et al., 2013] developed
a kernel machine for learning in multi-view latent variable
models, which also allows mixture components to be nonparametric and to learn data in an unsupervised fashion.
Different from existing work on multi-view learning, in
this paper, we start de-novo, i.e., we do not have any labeled
examples to start with, but we are able to query the oracle for
the labels of selected examples until at least one example has
been detected from each minority class.

3

The Proposed Framework

In this section, we introduce the proposed framework MUVIR for multi-view rare category detection. Notice that similar as existing techniques designed to address this problem
for single-view data, we target the more challenging setting where the support regions of the majority and minority
classes overlap with each other, which makes MUVIR widely
applicable to a variety of real problems.

3.1

Notation

Suppose that we are given a set of unlabeled examples S =
{x1 , · · · , xn }, which come from m distinct classes, i.e. yi ∈
{1, · · · , m}. Without loss of generality, assume that yi = 1
corresponds to the majority class with prior p1 , and the remaining classes are minority classes with prior pc . Furthermore, each example xi is described by features from V views,
i.e., xi = [(x1i )T , . . . , (xVi )T ]T , where xvi ∈ Rdv , and dv is

4099

the dimensionality of the v th view. In our proposed model, we
repeatedly select examples to be labeled by an oracle, and the
goal is to discover at leaset one example from each minority
class by requesting as few labels as possible.

3.2

Proof. Notice that when the features from multiple views are
conditionally independent given the class label, we have
P (x|y = 2) =

P (xv |y = 2)

v=1

Multi-View Fusion

The rest of the proof follows by changing the inequality in
Equation 2 to equality.

In this section, for the sake of exposition, we focus on the binary case, i.e., m = 2, and the minority class corresponds to
yi = 2, although the analysis can be generalized to multiple
minority classes. As reviewed in Section 2, existing techniques for rare category detection with single-view data essentially compute the score for each example according to the
change in the local density, and select the examples with the
largest scores to be labeled by the oracle. Under mild conditions [He et al., 2008; He and Carbonell, 2007], these scores
reflect P (x, y = 2), thus are in proportion to the conditional
probability P (y = 2|x).
For data with multi-view features, running these algorithms [He et al., 2008; He and Carbonell, 2007] on each
view will generate scores in proportion to P (y = 2|xv ),
v = 1, . . . , V . Next, we establish the relationship between
these probabilities and the overall probability P (y = 2|x).

Based on the above analysis, in MUVIR, we propose to assign the score for each example as follows.
!d
QV
V
v
Y
v
v
v=1 P (x )
s(x) =
(3)
s (x )
P (x)
v=1
where sv (xv ) denotes the score obtained based on the v th
view using existing techniques such as NNDM [He and Carbonell, 2007] or GRADE [He et al., 2008]; and d ≥ 0 is a
parameter that controls the impact of the term related to the
marginal probability of the features. In particular, we would
like to discuss two special cases of Equation 3.
Case 1. If the features from multiple views are conditionally
independent given the class label, and they are marginally inQV
dependent, i.e., P (x) = v=1 P (xv ), then Corollary 1 indicates that d = 0;
Case 2. If the features from multiple views are conditionally
independent given the class label, then Corollary 1 indicates
that d = 1.
In Section 4, we study the impact of the parameter d on
the performance of MUVIR, and show that in general, d ∈
(0, 1.5] will lead to reasonable performance.
Notice that the proposed score in Equation 3 is robust to
irrelevant views in the data, i.e., the views where the examples from the majority and minority classes cannot be effectively distinguished. This is mainly due to the first part
QV
v
v
v=1 s (x ) on the right hand side of Equation 3. For example, assume that view 1 is irrelevant such that the distribution of the majority class (P (x|y = 1)) is the same as
the minority class (P (x|y = 2)). In this case, the viewspecific score s1 (x1 ), which reflects the conditional probability P (y = 2|x), would be the same for all the examples.
Therefore, when integrated with the scores from the other relevant views, view 1 will not impact the relative score of all
the examples, thus it will not degrade the performance of the
proposed framework.

Theorem 1. If the features from multiple views have weak
dependence given the class label yi = 2 [Abney, 2002], i.e.,
QV
P (x|y = 2) ≥ α v=1 P (xv |y = 2), α > 0, then
!
QV
V
v
Y
v
v=1 P (x )
P (y = 2|x) ≥ C(
P (y = 2|x )) ×
P (x)
v=1
(1)
where C = (p2 )αV −1 is a constant.
Proof.
P (y = 2)P (x|y = 2)
P (x)
QV
P (y = 2)α v=1 P (xv |y = 2)
≥
P (x)
QV P (y=2|xv )P (xv )
P (y = 2) v=1
P (y=2)
=α
P (x)
QV
P (y = 2|xv )P (xv )
= α v=1
P (x)(P (y = 2))V −1
QV
V
Y
P (xv )
α
v
= 2 V −1
P (y = 2|x ) v=1
(p )
P (x)
v=1

V
Y

P (y = 2|x) =

(2)

3.3 MUVIR Algorithm
The proposed MUVIR algorithm is described in Algorithm 1.
It takes as input the multi-view data set, the priors of all the
classes (p1 , p2 , . . . , pm ), as well as some parameters, and outputs the set of selected examples together with their labels.
MUVIR works as follows. In Step 2, we compute the viewspecific score for each example, which can be done using
any existing techniques for rare category detection. In Step
3, we estimate the view-specific density using kernel density estimation; whereas in Step 5, we estimate the overall density by pooling the features from all the views together. Finally, Steps 6 to 16 aim to select candidates according to P (y = c|x). To be specific, in Step 7, we skip

As a special case of Theorem 1, when the features from
multiple view are conditionally independent given the class
label, i.e., α = 1, we have the following corollary.
Corollary 1. If the features from multiple views are conditionally independent given the class label, then Inequality 1
becomes equality, and C = (p2 )1V −1 .

4100

class c if examples from this class have already been identified in the previous iterations. Step 10 implements the feedback loop by excluding any examples close to the labeled
ones from being selected in future iterations. Notice that
the threshold  depends on the algorithm used to obtain the
view-specific scores. For example, it is set to the smallest
k-nearest neighbor distance in NNDM [He and Carbonell,
2007], and the largest k-nearest neighbor global similarity in
GRADE [He et al., 2008]. Step 11 updates the view-specific
score for each example with enlarged neighborhood for computing the change in local density [He and Carbonell, 2007;
He et al., 2008]. In Step 13, we compute the overall score
based on Equation 3, and select the example with the maximum overall score to be labeled by the oracle in Step 14. In
Step 15, if the labeled example is from the target class in this
iteration, we proceed to the next class; otherwise, we mark
the class of this examples as labeled.

VIR, MUVIR-LI is more suitable in real world applications.
MUVIR-LI is described in Algorithm 2. It works as follows. Step 2 calculates the specific score sv for each example.
The only difference from MUVIR is that here we use upper
bound p to calculate sv , which is a less accurate measurement of changing local density than in MUVIR. The same as
MUVIR, we estimate the view specific density and the overall
density by applying kernel density estimation in Step 3 and
Step 5. The while loop from Step 6 to Step 16 is the query
processing. We calculate the overall score for each example
and select the examples with the largest overall score to be
labeled by oracle. We end the loop until all the classes has
been discovered.
Algorithm 2 MUVIR-LI Algorithm
Input:
Unlabeled data set S with features from V views, p, d, .
Output:
The set I of selected examples and the set L of their labels.
1: for v = 1 : V do
2:
Compute the view-specific score sv (xvi ) for all the examples using existing techniques for rare category detection, such as GRADE-LI [He et al., 2008];
3:
Estimate P (xvi ) using kernel density estimation;
4: end for;
5: Estimate P (xi ) using kernel density estimation;
6: while not all the classes have been discovered do
7:
for t = 2 : n do
8:
for v = 1 : V do
9:
For each xi that has been labeled by the oracle,
∀i, j = 1, . . . , n, i 6= j,, if kxvi , xvj k2 ≤ , then
sv (xvj ) = −∞;
10:
Update the view-specific score sv (xvi ) using existing techniques such as GRADE-LI [He et al.,
2008];
11:
end for;
12:
Compute the overall score for each example s(xi )
based on Equation 3;
13:
Query the label of the example with the maximum
s(xi )
14:
Mark the class that x belongs to as discovered.
15:
end for;
16: end while

Algorithm 1 MUVIR Algorithm
Input: Unlabeled data set S with features from V views,
p1 , . . . , pm , d, .
Output: The set I of selected examples and the set L of their
labels.
1: for v=1 : V do
2:
Compute the view-specific score sv (xvi ) for all the examples using existing techniques for rare category detection, such as GRADE [He et al., 2008];
3:
Estimate P (xvi ) using kernel density estimation;
4: end for
5: Estimate P (xi ) using kernel density estimation on all the
features combined;
6: for c=2 : m do
7:
If class c has been discovered, continue;
8:
for t = 2 : n do
9:
for v = 1 : V do
10:
For each xi that has been labeled by the oracle,
∀i, j = 1, . . . , n, i 6= j,, if kxvi , xvj k2 ≤ , then
sv (xvj ) = −∞;
11:
Update the view-specific score sv (xvi ) using existing techniques such as GRADE [He et al.,
2008];
12:
end for
13:
Compute the overall score for each example s(xi )
based on Equation 3;
14:
Query the label of the example with the maximum
s(xi )
15:
If the label of xi is from class c, break; otherwise,
mark the class of xi as labeled.
16:
end for
17: end for

4

Experimental Results

In this section, we will present the results of our algorithm on
both synthetic data sets and real data sets in multiple special
scenarios, such as data sets with different number of irrelevant
features, data sets with multiple classes and data sets with
very rare categories, such as class proportion of 0.02%.

3.4 MUVIR with Less Information (MUVIR-LI)
In many real applications, it may be difficult to obtain the priors of all the minority classes. Therefore, In this subsection,
we introduce MUVIR-LI, a modified version of Algorithm 1,
which replaces the requirement for the exact priors with an
upper bound p for all minority classes. Compared with MU-

4.1

Synthetic Data Sets

Binary Class Data Sets
For binary classes, we perform experiment on 3600 synthetic
data sets, and each scenario has independent 100 data sets.

4101

In Majority Class Center

300
250

450

400

# of selected examples

# of selected examples

350

Near Majority Class Center

450

Random Sampling
GRADE
d=0
d=0.5
d=1
d=1.5

400

200
150
100

350
300
250
200
150
100

50

50

0

0

1

2

0

3

Partly Separated from Majority Class

400

# of selected examples

450

350
300
250
200
150
100
50

0

# of irrelevant features

1

2

0

3

0

# of irrelevant features

1

2

3

# of irrelevant features

Figure 1: Prior of minority class is 0.5%
In Majority Class Center

150

100

50

0

0

1

2

250

# of selected examples

200

Near Majority Class Center

250

Random Sampling
GRADE
d=0
d=0.5
d=1
d=1.5

# of selected examples

# of selected examples

250

200

150

100

50

0

3

0

# of irrelevant features

1

2

200

150

100

50

0

3

Partly Separated from Majority Class

0

# of irrelevant features

1

2

3

# of irrelevant features

Figure 2: Prior of minority class is 1%
In Majority Class Center

70
60
50
40
30
20

70
60
50
40
30
20
10

0

0

1

2

# of irrelevant features

3

Partly Separated from Majority Class

90

80

10
0

100

90

# of selected examples

# of selected examples

80

Near Majority Class Center

100

Random Sampling
GRADE
d=0
d=0.5
d=1
d=1.5

90

# of selected examples

100

80
70
60
50
40
30
20
10

0

1

2

0

3

# of irrelevant features

0

1

2

3

# of irrelevant features

Figure 3: Prior of minority class is 2%
We consider the following three special conditions: (i) different number of irrelevant features, i.e. from 0 to 3 irrelevant
features; (ii) different priors for minority class, i.e. 0.5%,
1%, 2%; (iii) different levels of correlation between majority
class and minority class, ie. minority class stays in the center of majority class, minority class stays around the center of
majority class, minority class stays at the boundary of majority class. Besides, as the distribution of majority class tends
to be more scattered and the distribution of minority class is
more compact, we set each data set with 5000 examples and
σmajority : σminority = 40 : 1.
In the experiment, we compare MUVIR with GRADE [He
et al., 2008] and random sampling. Fig. 1 shows the results when the prior of minority class is 0.5%. Using random sampling, we need to label 200 examples on average to
identify the minority class. In most cases, other approaches
outperform random sampling. However, the learning model
generated by GRADE algorithm performs worse with the increasing of irrelevant features. In contrast, MUVIR is more
efficient and stable rather GRADE. The experiment with minority proportions of 1% and 2% are represented in Fig. 2
and Fig. 3. In these two experiment, MUVIR outperforms
GRADE and random sampling in each condition with any
setting of d. Comparing these three figures, we have the following observations for binary class data sets: (i) MUVIR is
more reliable especially when dealing with data sets containing irrelevant features. (ii) In the case of data sets with no
irrelevant features, the performance of MUVIR with different

values of d are roughly the same. (iii) In the case of data
sets with irrelevant features, MUVIR with d = 1 outperforms
other methods.

# of selected examples

Multi-classes Data Sets with Imprecise Prior
300
250
200

Random Sampling
GRADE
GRADE-LI
MUVIR, d=1
MUVIR-LI, d=1

150
100
50
0

0

1

2

3

# of irrelevant features

Figure 4: Multi-class data sets
For multi-class data sets, we compare the performances
among different approaches. In particular, GRADE-LI [He
et al., 2008] and MUVIR-LI are only provided with an upper
bound p on the proportion of all the minority classes. The
multi-class data sets consisting of 9000 examples correspond
to majority class, and the other 1000 examples correspond to
4 minority classes. The proportions of minority classes are
4%, 3%, 2%, 1%. Similar to previous experiments, we will
discuss the scenario data sets contain different number of irrelevant features. Each value we represented in the figure
is the median value of results from 100 same scenario data
sets. From Fig. 4, we can have the following conclusions: (i)
MUVIR outperforms all other algorithms in multi-class data

4102

Views
relevant view 1
relevant view 2
relevant view 3
relevant view 4
irrelevant view 1
irrelevant view 2

sets; (ii) GRADE only performs good when data sets have 1
or 0 irrelevant feature; (iii) MUVIR-LI is more reliable than
GRADE-LI in all scenarios. The reason that our models have
better performance is that both MUVIR and MUVIR-LI are
capable to exploit the relationship among multiple views and
extract useful information to make predictions.

Table 1: Relevant and irrelevant views in Adult Data set.
algorithms, we have preprocessed both data sets in order to
keep each feature component has mean 0 and standard deviation 1. In the following experiments, we will compare MUVIR and MUVIR-LI with the following algorithms: GRADE,
GRADE-LI and random sampling.
70

# of selected examples

# of selected examples

Parameter Analysis
From previous experiments, we found different parameter
settings may result in different outcomes. In this experiment,
we will focus on analyzing the impact from degree d and upper bound prior p. To measure the impact of these parameters,
we generate 400 data sets with minority class proportion 1%.
The number of irrelevant features varies from 0 to 3, and each
case has 100 data sets. In Fig. 5, the X axis represents different values of degree d, and Y axis represents the number of
selected examples on average. From Fig. 5, we can see that
MUVIR performs better when d ∈ (0, 1.5]. In the following
experiments, we will focus on studying the performance of
our algorithm with d in this certain area.
250
200
150

With 0 Irrelevant
With 1 Irrelevant
With 2 Irrelevant
With 3 Irrelevant

Features
Features
Features
Features

Without Irrelevant features
With Irrelevant features

60
50
40
30
20
10
0

GRADE

MUVIR, d=0

MUVIR, d=0.5

MUVIR, d=1

MUVIR, d=1.5

100

Figure 7: Adult

50
0
0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

5.5

6

Value of d

Adult data set contains 48842 instances and 14 features of
each example. It is a binary classes data sets. Considering the
original prior of minority class in data sets is around 24.93%.
To better test the performance of our model, we keep majority class the same and down sample the minority class to 500
examples. In this way, we generate 24 data sets with minority
prior of 1.3%. And we select relevant and irrelevant views
based on correlation analysis. Noticed that all the views are
fed to all the algorithms without information regarding their
relevance. The details about relevant and irrelevant views are
represented in Tab.1. Fig. 7 shows the comparison results on
real data by applying 5 different approaches. In this experiment, we have not included MUVIR-LI, it is because MUVIRLI is mainly developed for multi-class cases and Adult is a
binary class data sets. By using random sampling, the average number of selected examples is 76. With irrelevant views,
GRADE needs 69 requests, MUVIR with d = 0 needs 60 requests, MUVIR with d 6= 0 needs around 30 to 40 requests.
The results totally meet our intuition that when dealing data
sets with irrelevant views, MUVIR with d 6= 0 outperforms
MUVIR with d = 0, and MUVIR with d = 0 outperforms
GRADE. However, when dealing with data sets without irrelevant views, GRADE needs less labeling requests than MUVIR with d = 0, but more labeling requests than MUVIR with
d around 1.
Different from Adult, Statlog contains 58000 examples and
7 classes. Among 7 classes, there are 6 minority classes, with
priors varying from 0.02% to 15%. In this experiment, we
compare the following 4 methods: GRADE, GRADE-LI with
c
upper bound p = maxm
c=2 p , MUVIR with d = 1, MUVIR-LI
m
with d = 1 and p = maxc=2 pc . From Fig. 8, we can see that
MUVIR outperforms all other algorithms for finding all the
minority class. With the same upper bound prior, GRADE-

Figure 5: Learning curves with different degree d

# of selected examples

Features
education, education years, work class
age, hours per week, occupation
martial status, relationship, sex
race, native country
final weight
capital loss, capital gain

500
400

With 1 Irrelevant Features
Without Irrelevant Features
Random Sampling

300
200
100
0

2%

4%

6%

8%

10%

12%

14%

16%

Upper bound p

Figure 6: Learning curves with different prior upper bound
With the same data sets, we studied the learning curves of
labeling requests by applying MUVIR-LI with different upper
bound p. In Fig. 6, the X axis represents different values of
upper bound proportion and Y axis represents the number of
labeling requests. The red line represents the average number of labeling requests by using random sampling. When
data sets without irrelevant features, MUVIR-LI works well
even with upper bound p changing from 1% to 12%. When
data sets with irrelevant features, MUVIR-LI can still outperforms random sampling with upper bound p changing from
1% to 8.5%. However, when the upper bound exceeds a certain level, the algorithm tends to be random sampling. This
might be due to the reason that when the bound is very loose,
e.g. the exact proportion of the minority class is 1% and the
given upper bound is 10%, the performance of our proposed
algorithm may be greatly affected by the introduced noise.

4.2

Real Data Sets

In this subsection, we will demonstrate our algorithm on two
real data sets Statlog and Adult. Noted that, before we run our

4103

Percentage of Classes Discovered

LI needs 272 labeling requests while MUVIR-LI only needs
168 labeling requests to discover all the classes. If we apply random sampling, it may needs around 5000 labeling request to only identify the smallest minority class. Compared
with Adult, we have better results on Statlog. It is because
the distribution of majority class and minority classes are not
meshed together as in Adult. Thus, to identify the minority
classes in Statlog is a much easier case.

Proceedings of the eleventh annual conference on Computational learning theory, pages 92–100. ACM, 1998.
[Chen et al., 2011] Minmin Chen, Yixin Chen, and Kilian Q
Weinberger. Automatic feature decomposition for single
view co-training. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages 953–
960, 2011.
[Günnemann et al., 2014] Stephan Günnemann, Ines Färber,
Matthias Rüdiger, and Thomas Seidl. Smvc: semisupervised multi-view clustering in subspace projections.
In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 253–262. ACM, 2014.

1
GRADE
GRADE-LI
MUVIR
MUVIR-LI

0.8

0.6

0.4

[He and Carbonell, 2007] Jingrui He and Jaime G Carbonell.
Nearest-neighbor-based active learning for rare category
detection. In Advances in neural information processing
systems, pages 633–640, 2007.

0.2

0
0

50

100

150

200

250

300

# of Selected Examples

Figure 8: Statlog

5

[He et al., 2008] Jingrui He, Yan Liu, and Richard
Lawrence. Graph-based rare category detection. In
Data Mining, 2008. ICDM’08. Eighth IEEE International
Conference on, pages 833–838. IEEE, 2008.

Conclusion

In this paper, we have proposed a multi-view based method
for rare category detection named MUVIR. Based on MUVIR,
we also provided a modified version MUVIR-LI for dealing
with real applications with less prior information. Different
from existing methods, our methods exploit the relationship
among multiple views and measure the probability belonging
to target class for all examples. Our algorithm works well
with multiple special cases: data sets with irrelevant features,
data sets with multiple minority class and various correlation
levels between minority class and majority class. The effectiveness of our proposed methods is guaranteed by theoretical
justification and extensive experiments results on both synthetic and real data sets, especially in the presence of irrelevant views.

[Ho, 1998] Tin Kam Ho. The random subspace method for
constructing decision forests. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20(8):832–844,
1998.
[Long et al., 2008] Bo Long,
S Yu Philip,
and
Zhongfei (Mark) Zhang. A general model for multiple view unsupervised learning. In SDM, pages 822–833.
SIAM, 2008.
[Muslea et al., 2003] Ion Muslea, Steven Minton, and
Craig A Knoblock. Active learning with strong and weak
views: A case study on wrapper induction. In IJCAI, volume 3, pages 415–420, 2003.

Acknowledgment

[Muslea et al., 2006] Ion Muslea, Steven Minton, and
Craig A Knoblock. Active learning with multiple views.
Journal of Artificial Intelligence Research, pages 203–
233, 2006.

The authors gratefully acknowledge the support from the
National Science Foundation under Grant Numbers IIP1430144. Any opinions, findings, and conclusions expressed
in this material are those of the authors and do not necessarily
reflect the views of the National Science Foundation.

[Sindhwani and Rosenberg, 2008] Vikas Sindhwani and
David S Rosenberg. An rkhs for multi-view learning
and manifold co-regularization. In Proceedings of the
25th international conference on Machine learning, pages
976–983. ACM, 2008.

References
[Abney, 2002] Steven P. Abney. Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics, July 6-12, 2002, Philadelphia,
PA, USA., pages 360–367, 2002.
[Balcan et al., 2004] Maria-Florina Balcan, Avrim Blum,
and Ke Yang. Co-training and expansion: Towards bridging theory and practice. In Advances in neural information
processing systems, pages 89–96, 2004.
[Blum and Mitchell, 1998] Avrim Blum and Tom Mitchell.
Combining labeled and unlabeled data with co-training. In

[Song et al., 2013] Le Song, Animashree Anandkumar,
Bo Dai, and Bo Xie. Nonparametric estimation of
multi-view latent variable models.
arXiv preprint
arXiv:1311.3287, 2013.
[Yu et al., 2011] Shipeng Yu, Balaji Krishnapuram, Rómer
Rosales, and R Bharat Rao. Bayesian co-training. The
Journal of Machine Learning Research, 12:2649–2680,
2011.

4104

2007 IEEE/WIC/ACM International Conference on Web Intelligence

Baum-Welch Style EM Approach
on Simple Bayesian Models for Web Data Annotation
Fatih Gelgi, Hasan Davulcu
Department of Computer Science and Engineering
Arizona State University
{fagelgi, hdavulcu}@asu.edu

Abstract
In this paper, our focus will be on weakly annotated data
(WAD) which is typically generated by a (semi) automated
information extraction system from the Web documents. The
extracted information has a certain level of accuracy which
can be surpassed by using statistical models that are capable of contextual reasoning such as Bayesian models. Our
contribution is an EM algorithm that operates on simple
Bayesian models to re-annotate WAD. EM estimates the parameters, i.e., the prior and conditional probabilities by iterating Bayesian model on the given Web data. In the expectation step, Bayesian classifier is trained from current
annotations, and in the maximization step, the roles of all
the labels are re-annotated to find the best fitting annotation
with the current model then the probabilities are re-adjusted
from the new annotations. Our experiments show that EM
increases the Web data annotation accuracies up to 8%. We
use Baum-Welch methodology in our EM approach.
Keywords: Weakly annotated data, Baum-Welch,
Expectation-Maximization, Bayesian Models.

1. Introduction
Information extraction (IE) systems have been quite successful in extracting and annotating information given in
Web pages that are essentially unstructured. Some examples are (meta) data extraction from template driven Web
sites using semi-automated techniques [9] and ontologydriven automated data extraction techniques [7] from text.
And also, some work exists on fully automated techniques
for extracting (meta) data from data rich segments of Web
pages [4, 10, 16].
In this paper, our focus will be on weakly annotated
data (WAD) which is typically generated by a (semi) automated information extraction (IE) system from the Web
documents. In WAD, annotations correspond to ontolog-

0-7695-3026-5/07 $25.00 © 2007 IEEE
DOI 10.1109/WI.2007.12

ical role assignments such as Concept, Attribute, Value or
Noise. WAD has two major problems; (i) it might contain
incorrect role assignments, and (ii) might have many missing attribute labels between its various entities.
We will use the Web pages in Figure 1 to illustrate
WAD that might be extracted using an IE algorithm such
as [4, 16]. Each of these pages presents a single instance
of the ‘Digital Camera’ concept. In Figure 1(a), attributes
such as ‘storage media’, and values such as ‘sd memory
card’ have uniform and distinct presentation. However, for
an automated system it would be extremely difficult to differentiate the ‘storage media type’ label as an attribute and
‘sd secure digital’ as its value due to their uniform presentation in Figure 1(b). On the other hand, in Figure 1(c) the
attribute ‘storage media’ does not even exist, but only its
value ‘sd memory card’ has been reported.
Automated IE systems mostly make local reasoning, i.e.,
they use single Web pages, or a couple of instances of
template-based Web pages. They are lack of using global
statistics of the domain incorporating contextual information. Hence, the extracted information has a certain level of
accuracy which can be surpassed by using statistical models
that are capable of contextual reasoning such as Bayesian
models [8]. For example, consider the label ‘storage media’ marked in Figure 1. A collection of Web pages such
as those in Figure 1(a), would yield a strong association between ‘canon sd200’ as an object and ‘storage media’ as
an attribute. Whereas, the incorrect annotation, extracted
from Figure 1(b) would yield a weak association between
‘canon a520’ as an object and the ‘storage media’ as a value.
Hence, a Bayesian classifier would be able to re-assign the
attribute role to the ‘storage media’ label by using the domain statistics within its context. Similarly, the ‘storage media’ attribute of the ‘sd card’ value which is missing in Figure 1(c), could be inferred by utilizing its context and the
model.
Bayesian models are capable of making contextual inference on Web data. Meanly, given the context of a label,
one can use Bayesian models to infer the role of that label

736

(a)

(b)

(c)

Figure 1. The instance pages for Canon digital camera from three different Web sites. Digital camera
specifications are marked by dashed boxes in each page. In page (a), attributes are explicitly given
as bold whereas in (b) they are not obvious. On the other hand, the attributes are altogether missing
in (c).

using the statistics of the entire data and the context of the
label. The two problems of Bayesian models are structure
and parameter estimation. In our problem, we work on simple Bayesian models, thus the structure is already known.
However, parameters, i.e., prior and conditional probabilities can not be correctly estimated due to the partial observability and uncertainty of the Web data. Partial observability and uncertainty is due to the reasons; noisy and incorrect
annotations of IE systems, and missing labels and relations
between labels. The partial observability and uncertainty of
the data lead inaccurate inference of Bayesian models due to
the hidden distribution. That’s why, the problem can be considered as a good match for an expectation-maximization
(EM) algorithm.
In this paper, our contributions are; an EM algorithm
that (i) operates on simple Bayesian models to re-annotate
WAD, and (ii) follows Baum-Welch methodology. EM estimates the conditional probabilities by iterating Bayesian
model on the given Web data. In the expectation step, prior
and conditional probability distributions for all roles of labels are computed. In the maximization step, the roles of all
the labels are re-annotated using the new Bayesian model to
find the best fitting annotation with the current model.
In the rest of the paper, the related work is given in Section 2. Section 3 explains the EM model in details and Section 4 presents the experiments. Section 5 concludes the
paper.

2. Related Work
EM models [6, 12, 5] became popular in recent years due
to their high performance in many problems with partial

observable data. In their recent work, Lowd and Domingos [11] argue that EM Naive Bayes is successful in general probabilistic learning and inference. Furthermore, it is
comparable to Bayesian networks with context-specific independence.
Nigam et. al.’s work [13] is one of the main papers that
fires the EM naive Bayes research on text and Web documents. Their EM model trains a naive Bayes classifier in
E-step and uses it for labeling the unlabeled documents in
M-step.
[15] extends the EM naive Bayes model and applies to
classification of Web search results. Salvatierra states the
incorrectness of the “bag of words” universe of the data assumed in previous work and uses pre-defined ontologies to
develop a contextual model. Salvatierra similarly trains a
classifier from labeled Web pages and labels the unlabeled
ones using EM.
Our work differs at three points: (1) our data is fully
annotated and unreliable Web data, (2) we use contextual
models based on term associations without predefined ontologies, and (3) we train the simple Bayesian classifiers using the entire unreliable annotation which is similar to the
Hidden Markov Model (HMM) training method in BaumWelch algorithm.

3. Expectation-Maximization Approach
In this section, we first give a brief summary of BaumWelch algorithm as the background information. Next,
the statistical model of the domain which is called relational graph is explained, then the probabilistic framework
is given in details.

737

3.1. Background
Traditional HMMs have three parameters λ = (π, A, B)
where π is the initial probability distribution of the states,
A is the probability distribution of state transitions and B is
the symbol emission probabilities of states [14]. Input data
is the observation sequence O = O1 O2 . . . OT and each
observation is considered to be produced from the corresponding state sequence Q = q1 q2 . . . qT . Note that the
observations are assumed to be statistically independent.
Our problem is related with 3. problem for HMMs
given in [14]: “How do we adjust the model parameters
λ = (π, A, B) to maximize P (O|λ) ?”. P (O|λ) can be
obtained by summing the joint P
probability over all possible
state sequences Q: P (O|λ) = Q P (O, Q|λ).
Solving this problem means to optimize the parameters
of HMM so that the model describes the generation of observation sequence best. This method is used when the observation sequences and their corresponding states are given
as noisy or uncertain data in partially observable environments. Some of symbols in the sequence might be missing
and the corresponding states might be incorrect. Hence the
only difference of input data is being contextual instead of
sequential in our case.
Baum-Welch algorithm [3] is a typical EM algorithm
that uses the entire data, i.e., observation sequences and
their corresponding states, as training data. It re-estimates
the parameters and maximizes the number of correct individual states in the data.
We can summarize Baum-Welch algorithm as follows:
probabilities are calculated for different annotations in Estep, and the observation sequence is re-annotated in the
way that number of correct individual states is maximized
and the parameters π, A and B are re-estimated in M-step.
Initialization of parameters in the first E-step can be done
by using the initial state sequences corresponding to the observations.

3.2. Relational Graph and Formalization
We assume that the IE system we used annotates the labels of a given individual Web page and transforms it into
an XML/RDF-like hierarchical structure. The annotations
correspond to four ontological roles:
• Concept (C): A concept defines a category or a class
of similar items. E.g., ‘books’ and ‘digital camera’ are
some of the concepts in the shopping domain.
• Attribute (A): An attribute is a property of an instance
or a concept. E.g., ‘storage media’ is an attribute of
the ‘canon powershot sd200’ instance and the ‘digital
camera’ concept.

Figure 2. A fragment of the relational graph
for the Shopping domain is shown. Each
node is composed of a hlabel, rolei pair. The
thickness of an edge is proportional to the association strength between its nodes.

• Value (V): A value is a label that provides the value
information for an attribute of a certain concept or an
instance of a concept. E.g., ‘storage media’ attribute
of the ‘canon powershot sd200’ instance has the value
‘sd memory card’.
• Noise (N): Any label that does not have any of the
above ontological roles are assigned to be noise. For
example, some of the labels in headers, footers or navigation aids, such as ‘back to’ shown in Figure 1(c)
could be annotated as noise.
From these hierarchical structures of the Web pages, we
generate the relational graph which is a statistical domain
model that shows how closely the terms in the domain are
related with each other. It has been used to make contextual inference by Bayesian models as demonstrated in the
example above. The relational graph is an undirected graph
composed of hlabel, rolei terms as nodes and their associations as edges. The weight of a node is the count of the corresponding term, and the weight of an edge is the number
of parent-child relations of the corresponding nodes in the
XML/RDF hierarchies in the entire collection. Formally,
assuming wij as the weight between the terms i and j, and
wi as the weight of the node i, wij = wji = |i ↔ j| and
wi = |i| where |i ↔ j| and |i| denote the number of times
the edge (i, j) and label i appeared in the entire domain respectively. Note that the edges are undirected since association strength between labels is a bi-directional measure. A
fragment of the relational graph corresponding to Shopping
domain is depicted in Figure 2.
For the rest of the paper, we use the following notations:
• A label is a word or a phrase in a Web page.

738

• R denotes the set of roles used for annotating the labels in Web pages.
• An annotation is to assign a role to a label.
• A term is a pair hO, qi composed of a label O and a
role q ∈ R. In other words, terms are annotated labels
in the Web pages.
• In this setting, we consider all the labels in each
Web page are tagged with roles, hence we define
an annotated Web page to be a list of its terms.
Formally, assuming m labels in a Web page W =
[hO1 , q1 i, hO2 , q2 i, . . . , hOm , qm i].
• The context of a label Oj in an annotated Web page
Wi is the set of all the terms in that Web page ex(i)
cluding the terms that contain Oj . That is, Cj =
S
S
(i)
T ∈W (i) {T }\ q∈R {hOj , qi} .

3.3. Probabilistic Framework
We propose a Baum-Welch style EM model on simple Bayesian models. To demonstrate the methodology,
we build EM model on two Bayesian models explained in
[8]: the simple Bayesian classifier (SBC) and the naive
Bayes classifier (NBC). These Bayesian classifiers have
been used to re-annotate the labels obtained from an IE system given in [16]. Simple Bayesian model structure is already known1 ; EM is used to estimate the prior and conditional probabilities of the Bayesian models in partially observable data.
NBC-EM described here can be considered as a modification of [13]. In this one, NBC classifies the labels in the
Web pages instead on Web pages themselves. Additionally,
the entire data is used for training the classifier whereas in
[13] only the labeled data is used.
For the convenience of the reader, we use a similar notation to [14] for EM to easily relate our methodology to
Baum-Welch. In our problem, input data D is the set of
Web pages with annotations. The parameter λ is the role
probability distribution of the Bayesian model. The outputs
are the corrected annotations of the input data.
By default in EM data, the observations are assumed to
be independent from each other. For the Web data, we have
two more assumptions as discussed in [8].
Assumption 1 The role of a label Oj is unique in a context.

The first assumption states that in a specific context, a
label can not have more than one roles. The second one is
for SBC to reduce the effect of the dominant terms in the
collection.
3.3.1

Parameters

The probability of the current data P (D|λ) which is composed of K Web pages is:
P (D|λ) =

K
Y

P (O(i) |λ).

(1)

i=1

By the independence assumption, the probabilities of the
labels in each Web page can be calculated independently.
Given the Web page W (i) ,
X
P (O(i) |λ) =
P (O(i) , Q|λ)
(2)
Q

Now, we will explain how to calculate the probability
P (O(i) , Q|λ) for any given annotation Q.
(i)

(i)
P (O(i) , Q|λ) = P ([O1 , . . . , Om
], [q1 , . . . , qm ]|λ) (3)
(i)

In this formula, qj is the corresponding annotation for Oj .
Thus, we write the same formula with the following representation for our convenience:
(i)

(i)
P (hO1 , q1 i, . . . , hOm
, qm i|λ)

(4)

Due to the independence assumption, it is equal to the product of the probabilities of individual terms,
m
Y

(i)

P (hOj , qj i|λ).

(5)

j=1
(i)

In this formula, the probability P (hOj , qj i|λ) is calculated
by a simple Bayesian model with context dependent rea(i)
soning that corresponds to E-step. Each term hOj , qj i is
conditioned on the context C:
X
(i)
(i)
P (hOj , qj i|λ) =
P (hOj , qj i|C, λ).
(6)
C
(i)

(i)

Since the context of a label Oj is only Cj
(i)
P (hOj , qj i|C

and fixed in

(i)
Cj , λ)

our model,
6=
= 0. That makes,
X
(i)
(i)
(i)
P (hOj , qj i|C, λ) = P (hOj , qj i|Cj , λ).
(7)
C

Assumption 2 The prior probabilities of all the roles of a
label Oj are uniform.
1 Recall that in Baum-Welch algorithm, the Markovian structure is also
assumed to be known which is an HMM and only the parameters are estimated.

Again, by the independence assumption, SBC and NBC
given in [8] calculate this probability respectively as follows:
Q
(i)
(i) P (hOj , qj i|T )
T ∈Cj
(8)
Z
739

Q


(i)
(i)
P (T |hOj , qj i) P (hOj , qj i)

(i)

T ∈Cj

(9)

Z0
where Z and Z are normalization factors. From Equation
8, the parameters for SBC to be estimated are in the form:
0

(i)

λij (q) = P (hOj , qi|T )

(10)

λij (q) =
3.3.2

λ̄ij = P (hOj , q̄j i|T )

(11)

Similarly, conditional probabilities of NBC to be estimated
are:
λ̄ij = [P (T |hOj , q̄j i), P (hOj , q̄j i)]
(18)

E-step

In the expectation step, the role probability distribution
for each label in a given context is calculated. Formally,
(i)
(i)
P (hOj , qi|Cj , λ) in Equation 7 is calculated for any
q ∈ R. This probability computation directly follows from
Equations 8 and 9 for SBC and NBC respectively.
3.3.3

(i)

(i)

where q ∈ R is an arbitrary annotation for the label Oj .
Similarly, from Equation 9, the parameters for NBC are:
(i)
(i)
[P (hT |Oj , qi), P (hOj , qi)]

As emphasized before, these equations maximize the
correct number of individual roles in the data. After the role
inferences, all node and edge weights in the relational graph
are re-adjusted according to the re-annotations. Next, parameter λ is maximized using the new statistics of the data as
follows in SBC:

Following the methodology in the association rules [1,
2], conditional probabilities are calculated from the current
statistics of the entire data:

M-step

The maximization step reduces to maximizing the number
of correct individual annotations of the terms which is similar to maximizing the number of correct individual states as
in the Baum-Welch algorithm. Likelihood of the data can
be maximized by annotating the labels with their maximum
likely roles. Then, the new label-role annotations are used
to maximize the parameter λ. For each Web page W (i) ,
Y
(i)
(i)
max P (O(i) , Q|λ) =
max P (hOj , qi|Cj , λ) (12)
Q

q

j=1

from the independence of the observation. From Equations
(i)
(i)
8 and 9, the probability maxq P (hOj , qi|Cj , λ) is:
maxq

(i)

Q

(i)

T ∈Cj

P (hOj , qi|T )
(13)

Z
maxq

Q

(i)
T ∈Cj


(i)
(i)
P (T |hOj , qi) P (hOj , qi)

(14)
Z0
for SBC and NBC respectively. Since maximization step,
(i)
chooses the role q̄j for a label Oj , the annotation that maximizes the likelihood is:
Y
(i)
q̄j = argmax
P (hOj , qi|T ).
(15)
q

(i)

T ∈Cj

wT 0 T
P (T 0 , T )
wT
=
, P (T ) =
P (T )
wT
Z

(19)

P (T |T 0 ) =

P (T, T 0 )
wT T 0
wT 0
=
, P (T 0 ) = 0
P (T 0 )
wT 0
Z

(20)

(i)

q∈R

The initialization of parameter λ comes from the initial
annotations as in the maximization step using Equations 19
and 20. Recall that Baum-Welch similarly uses entire initial
state sequences to initialize the parameters in the beginning.

4. Experiments
We used the same two sources and experimental setup
in [8] for our data sets to evaluate the efficacy of our algorithms: TAP and CIPS. The first one is a synthetic source
whereas the second one is natural Web data.
• TAP Dataset: The data set is Stanford TAP Knowledge Base 2 [9] containing the categories AirportCodes, CIA, FasMilitary, GreatBuildings, IMDB, MissileThreat, RCDB, TowerRecords and WHO. These
categories alone comprise RDF files of 9,068 individual Web pages.



 Y

(i)
(i)
q̄j = argmax 
P (T |hOj , qi) P (hOj , qi).
q

P (T 0 |T ) =

where T 0 = hOj , qj i, wT 0 T and wT are the weight of the
association between terms T 0 and T , and the weight of the
term T respectively. Z and Z 0 are normalization factors.
These weights are retrieved from the relational graph of the
current annotation.
(i)
Given the label Oj , the stochastic constraint of the
model which is the summation of the probabilities of all
role annotations in a context has to be 1:
X
(i)
(i)
= P (hOj , qi|Cj , λ) = 1.
(21)

For NBC, it is:


(17)

(i)

T ∈Cj

(16)

740

• CIPS Dataset: This data set is composed of faculty,
course home pages, shopping and sports Web pages
consisting of 225 Web sites and more than 20,000 individual pages. The computer science department Web
sites are meta-data-driven, i.e., they present similar
meta-data information across different departments.
Shopping and sports are some popular attribute rich
domains.
Figure 3. Performance of EM on different
Bayesian Models.

In the rest of this section, experimental setup will be explained in details and the results will be discussed in terms
of performance of EM, its convergence and the effect of initialization method.

4.1. Experimental Setup
TAP is a synthetic and a template-driven data set
that is used to measure accuracy of our algorithm.
In the data set, RDF files have been converted into
(Concept, Attribute, V alue) triples first, then distorted to
obtain the inputs. Distortion are applied by deleting the labels and changing the roles of the labels with certain rates
in the triples. Two sets of data is prepared from TAP with
(15%, 15%) and (35%, 35%) distortions where the percentages are the random deletion and role change ratios respectively. Each set consists of a mixture of categories. The
first set has initially 80% accuracy and the second has 40%
accuracy.
In the CIPS data set, we used the semantic partitioner
[16] to obtain initial annotations of the labels from Web
pages. The semantic partitioner is an IE system that transforms a given Web page into an XML-like structure by separating and extracting its meta-data and associated data. For
the evaluations, we created a smaller data set containing
randomly selected 160 Web pages from each domain. We
divided samples into 4 groups with 40 pages from each category, and each group is evaluated by a non-project member
computer science graduate student.
In our experiments, data has been preprocessed using
simple regular expressions to identify the common data
types of values such as percentage, dates, numbers etc.
which is a traditional method in Web mining. The reported accuracies are measured according to the formula
annotations
Accuracy = ##ofofcorrect
total annotations .
In the rest of this section, experimental setup will be explained in details and the results will be discussed in terms
of performance of EM, its convergence and the effect of initialization method.

4.2. Results and Discussions
Our EM model is evaluated with three experiments. The
first one demonstrates the quality performance of EM. The

final accuracies of EM models have been compared with
simple Bayesian models without EM. The results are given
in 3. NBC-EM and SBC-EM are the EM models of NBC
and SBC. The effect of EM on SBC is in the interval of
[2%, 7%] whereas in NBC it is [3%, 8%]. EM has been
well performed especially in Sports and TAP(35%,35%)
data sets since their initial annotations have lower accuracy
which means more instability compared to the other data
sets.
Convergence of EM is tested in the second experiment.
Figure 4 shows the convergence performance of EM on the
same data sets. For both EM models, 7 iterations have been
sufficient for convergence. As expected, the big leap has
been in the first iteration and models have stably converged
following logarithmic curves which is the usual characteristics of EM models.
The third experiment evaluates the effect of initialization of EM models. In [14], it is indicated that good initial
estimates of parameters effects the performance. Figure 5
compares random initialization to the initialization with an
IE system. In Figure 5(a), the accuracy of EM with random initialization is the average of 20 runs. The variation
in the initial and final accuracies is limited in the interval
of 7%. Although, there is not marginal difference in the
accuracy increase, the final accuracies are substantially different. Since the likelihood function has many local maximums, EM with random initialization has been stuck in one
of the local maximum which has very low accuracy; less
than 50% for all data sets.

5. Conclusion and Future Work
In this work, we present a EM methodology on simple
Bayesian models and exemplify on a prior work [8]. Our
work is inspired from the Baum-Welch algorithm which is
essentially an EM model on HMMs to estimate its parameters. As we demonstrate in the experiments, EM increases
the accuracy of the data from 2% to 8%. As the future
work, we consider generalizing the approach for Bayesian
networks with arbitrary structures.

741

(a)

(b)
Figure 4. Convergence of EM.

(a)

(b)

Figure 5. Effect of the initialization method: (a) random initialization, (b) initialization with Semantic
Partitioner.

References
[1] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets of items in large databases. In ACM
SIGMOD, pages 207–216, Washington, D.C., 1993.
[2] E. Alpaydin. Introduction to Machine Learning, chapter 3,
pages 39–59. MIT Press, 2004.
[3] L. E. Baum, T. Petrie, G. Soules, and N. Weiss. A maximization technique occurring in statistical analysis of probabilistic functions in markov chains. The Annals of Mathematical
Statistics, 41(1):164–171, 1970.
[4] V. Crescenzi and G. Mecca. Automatic information extraction from large web sites. Journal of ACM, 51(5):731–779,
2004.
[5] F. Dellaert. The expectation maximization algorithm. Technical Report GIT-GVU-02-20, Georgia Institute of Technology, 2002.
[6] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood
from incomplete data via the em algorithm. Journal of the
Royal Statistical Society, Series B, 39.
[7] S. Dill, N. Eiron, D. Gibson, D. Gruhl, R. Guha, A. Jhingran,
T. Kanungo, K. S. McCurley, S. Rajagopalan, A. Tomkins,
J. A. Tomlin, and J. Y. Zien. A case for automated largescale semantic annotation. Journal of Web Semantics,
1(1):115–132, 2003.

[8] F. Gelgi, S. Vadrevu, and H. Davulcu. Fixing weakly annotated web data using relational models. In ICWE, 2007.
[9] R. Guha and R. McCool. TAP: A semantic web toolkit. Semantic Web Journal, 2003.
[10] K. Lerman, L. Getoor, S. Minton, and C. Knoblock. Using the structure of web sites for automatic segmentation of
tables. In ACM SIGMOD, pages 119–130, Paris, France,
2004.
[11] D. Lowd and P. Domingos. Naive bayes models for probability estimation. In ICML, pages 529–536, 2005.
[12] T. Minka. Expectation-maximization as lower bound maximization, 1998.
[13] K. Nigam, A. K. McCallum, S. Thrun, and T. M. Mitchell.
Text classification from labeled and unlabeled documents
using EM. Machine Learning, 39(2/3):103–134, 2000.
[14] L. R. Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the
IEEE, 77(2):257–286, 1989.
[15] S. M. Salvatierra. Using unlabeled data to improve classification in the naive bayes approach: Application to web
searches. Journal of Computational Methods in Science and
Engineering, 4(1-2):65–74, 2004.
[16] S. Vadrevu, F. Gelgi, and H. Davulcu. Semantic partitioning
of web pages. In WISE, 2005.

742

220	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016

Exploring Evolving Media Discourse Through Event Cueing
Yafeng Lu, Michael Steptoe, Sarah Burke, Hong Wang, Jiun-Yi Tsai,
Hasan Davulcu, Douglas Montgomery, Steven R. Corman, Ross Maciejewski, Senior Member, IEEE

Fig. 1: Overview of the event cueing visual analytics framework. The map view provides a geographical visual analytics environment to enable exploration of frames and entities over space and time. The detailed view to the right of the map switches
between entity wordles and list-based displays. The time series view contains a hierarchical frame analysis visualization. Each
line visualizes significant events and the sentiment associated with a media frame or a frame class in the expanded leaf nodes. The
control pane, which consists of the top left donuts, shows the distribution of frames and events and is used to filter categorical
variables in the linked views.
Abstract— Online news, microblogs and other media documents all contain valuable insight regarding events and responses to
events. Underlying these documents is the concept of framing, a process in which communicators act (consciously or unconsciously)
to construct a point of view that encourages facts to be interpreted by others in a particular manner. As media discourse evolves, how
topics and documents are framed can undergo change, shifting the discussion to different viewpoints or rhetoric. What causes these
shifts can be difficult to determine directly; however, by linking secondary datasets and enabling visual exploration, we can enhance
the hypothesis generation process. In this paper, we present a visual analytics framework for event cueing using media data. As
discourse develops over time, our framework applies a time series intervention model which tests to see if the level of framing is
different before or after a given date. If the model indicates that the times before and after are statistically significantly different, this
cues an analyst to explore related datasets to help enhance their understanding of what (if any) events may have triggered these
changes in discourse. Our framework consists of entity extraction and sentiment analysis as lenses for data exploration and uses two
different models for intervention analysis. To demonstrate the usage of our framework, we present a case study on exploring potential
relationships between climate change framing and conflicts in Africa.
Index Terms—Media Analysis, Time Series Analysis, Event Detection

1

I NTRODUCTION

Recently, the visual analytics community has begun developing a variety of tools for analyzing media collections. These tools tend to focus
on event detection from text streams [39], correlation analysis [28],
• Yafeng Lu, Michael Steptoe, Sarah Burke, Hong Wang, Jiun-Yi Tsai,
Hasan Davulcu, Douglas Montgomery, Steven R. Corman, and Ross
Maciejewski, are with Arizona State University. E-mail:
{lyafeng,msteptoe,seburke2,hxwang,jtsai8,HasanDavulcu,
doug.montgomery,steve.corman, rmacieje}@asu.edu.
Manuscript received
received 31
31 Mar.
Mar. 2015;
2015;accepted
accepted 11Aug.
Aug.2015;
2015;date
dateof
of
Manuscript
publication xx
20 Aug.
Aug. 2015;
2015; date
date of
of current
current version
version 25
25 Oct.
Oct. 2015.
For information on obtaining reprints of this article,
article, please
please send
send
e-mail to: tvcg@computer.org.
Digital Object Identifier no. 10.1109/TVCG.2015.2467991

and topic evolution [15]. These tools are often concerned with understanding an ongoing narrative from structured text and focus on
enabling the user to place news stories within the context of other
ongoing events. However, very few tools [11, 12, 13] explore how
media is being framed, and, to our knowledge, none have explored
changes in frames over time and space. In studying public communications, framing is the use of rhetorical devices (e.g., words, phrases,
metaphors, images) to encourage one interpretation of a set of facts
and discourage other interpretations [16]. Examples include efforts by
U.S. conservatives in the 1990s to reframe the estate tax as a “death
tax”, and competing frames of the Occupy Wall Street protests, “the
99%” (vs. the 1%) as opposed to “makers vs. takers”. Framing affects
the attitudes and behavior of audiences [9], and it is also regarded as
a key media effect, in that media “actively set the frames of reference
that readers or viewers use to interpret and discuss public events” [35].

1077-2626 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

LU ET AL.: EXPLORING EVOLVING MEDIA DISCOURSE THROUGH EVENT CUEING

Understanding framing in the media is vital as it influences the way
people interpret the topic under analysis. Framing is also critical to
the success of social movements and can be a driver for change or
stagnation [4]. What is of interest is how these frames are applied and
how they evolve over time in the context of other events. However, it
can be quite difficult to determine when changes in framing occur and
what events may have contributed to changing attitudes.
In this work, we present a visual analytics framework for event cueing from media. For a given collection of documents (related by topic
and coded with frames), we enable analysts to explore ongoing media
discourse with respect to the overall framing and related sentiment of
the narratives. In order to understand when and how framing about a
topic has shifted, we employ intervention models for time series analysis. Such models examine how a measure changes over time and how
this measure is affected by some external event, or intervention, at a
given time t. If the measure is significantly different before and after the intervention, then one can hypothesize that an intervention is
associated with a change in the measurement. By highlighting these
statistically significant intervention points, we can cue analysts to time
periods of interest. Then, by linking the media data source with secondary sources of information relevant to the topic, an analyst can
explore the frame evolution within the context of ongoing events.
This work is directly related to previous works, such as Narratives [18] and EventRiver [27], which focus on placing media stories
into their historical and social context by allowing people to explore
topics and keywords and associate them with other ongoing stories
and events. Unlike previous work, our framework utilizes intervention modeling strategies and multisource data. Our goal is to enable
analysts to cue to important dates in the dataset. Media can then
be explored in the context of the changing sentiment of the framed
documents as well as linked to concurrent events that may have impacted the media discourse. While previous work from Diakopolous
et al. [11, 12, 13] developed tools for frame analysis, their work provided no support for entity extraction, sentiment analysis, or linking
multisource data. Our contributions include:
1) An ensemble of intervention modeling techniques for event cueing and hypothesis generation,
2) The application of visual analytics for media framing in the context of entities, sentiment, geography and multisource data.
2 R ELATED W ORK
As media sources have broadened from network news coverage to microblogs, Twitter, etc., a variety of tools and techniques have been
developed to analyze and explore such data sources. Given that media
data generates events over time in unstructured text, the majority of
tools and techniques developed have focused on temporal visualizations, topic analysis, and pattern and anomaly detection.
2.1 Time Series Visual Analytics
Visualization has been successfully applied to analyze time-oriented
data, most commonly through the use of line graphs and their variations [17], as well as calendar views and clock views for periodical or seasonal patterns [3, 22, 37]. A variety of enhancements to
these techniques have been proposed over the years to enable better
sensemaking of events and records. For example, LifeFlow [42] combines a list-based display for intra-record pattern analysis and an aggregated overview display for inter-record trends analysis to visualize
time-point based event sequences. EventFlow [30] extends LifeFlow
to handle interval events and explore the relationship between event
sequences and associated outcomes. Another extension of LifeFlow,
Outflow [41] aggregates multiple event sequences, visualizes them as
the pathways through different event states, and connects the pathways to their associated outcomes so that users can explore progression paths and results. Of interest to our work is that OutFlow also
incorporates external factors which may influence the event sequence.
Our work differs in that we focus on cueing analysts to events in time
series datasets through the use of intervention models. These models
enable users to find sequences in the data that appear to have deviated.

221

Our framework then links these deviations to external data sources to
identify potential causes to these deviations.
The incorporation of statistical techniques into time series visualization has led to the development of a variety of visual analytics solutions. A typical example is the visual analytics process proposed
in Bogl et al. [5] where visualization is used to guide domain experts in statistical model parameter selection. Their prototype system,
TiMoVA, is developed to facilitate the process of parameter settings
in autoregressive integrated moving average (ARIMA) and seasonal
ARIMA models. A probabilistic decision tree learner is used in the
e-transaction time-series visual analytics system VAET in [43] to explore transaction patterns among multiple users in a temporal context.
These tools focus on enabling users to visually develop statistical models of the data. In contrast, our work focuses primarily on using statistical models for cueing analysts to events of interest in the time series.
Our proposed type of cueing is similar to work in event detection, and visual analytics has posed a variety of solutions for anomaly
and event detection [8]. Classification-based event detection methods
have been applied in many visualization systems. For example, Scatterblogs [6, 36] is a scalable system enabling analysts to find quantitative information and detect spatiotemporal anomalies within a large
volume of geo-located microblog messages. Work by Chae et al. [7]
utilizes a seasonal-trend-decomposition method to determine anomalous changes in topics in social media. Gotz et al. developed DecisionFlow [20], which integrates interactive multi-view visualizations and
ad hoc statistical models to support the analysis of high-dimensional
temporal event sequence data. While a variety of statistical methods
have been applied for visual analytics of temporal data, these methods
typically focus on anomalous behavior. In our framework, we focus
on the concept of an external intervention causing the system to deviate. This framework requires different statistical analysis and also
needs to integrate multi-source data for analysis. To our knowledge,
this approach is the first such application in visual analytics to explore
time series data in the context of interventions.
2.2

Media Visual Analytics

While applicable to a variety of domains, our focus is specifically on
media data, such as online news and microblogs. Recently, much attention has been paid to this domain area in the visual analytics community, with techniques focusing on knowledge expression, topic extraction, pattern analysis, and storytelling [14, 19, 21, 23, 24]. CloudLines [23] focuses on the detection of visual clusters in a compressed
view of multiple time series to enable the scalable analysis of media streams. To improve sensemaking, LeadLine [14] explores named
entities, locations, and bursts of topic related events by visualizing
the shift of topic volume for different time streams and emphasizing detected events. Contextifier [21] is designed for contextualizing visualization by providing customized annotations for the stock
timeline graph with reference to the content in a news article. StoryTracker [24] combines interactive visualization and text mining techniques to facilitate the analysis of similar topics that split and merge
over time. NewsViews [19] is a novel automated news visualization
system that creates thematic maps automatically for news articles. It
leverages text mining to identify key concepts and locations discussed
in articles. TopicPanorama [25] visualizes the full picture of relevant
topics from different sources to analyze common and distinctive topics. Similar to previous works, we also leverage text mining techniques on media articles. Our system extracts entities and their associated geolocations. Unlike previous works on media frame visual
analytics [11, 12, 13], which typically focus on topic analysis and cooccurring words, our system focuses on frame analysis in conjunction
with multisource data. We focus on a single topic and explore how it
is being discussed (i.e., framed), rather than focusing on multiple topics. In this work, frames are organized into a hierarchical set and the
change in how documents are framed (with respect to space and time)
can be explored. By visualizing statistical results together with the hierarchical frames, we can enhance the hypothesis generation process.

222	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016

(a) Choropleth

(b) Cluster Pie

(c) Weighted Category Choropleth

Fig. 2: Categorical data spatial distribution visualization view. View (a) shows the default choropleth map which colors each country based
on the density of all frames. View (b) shows pie glyphs on the map displaying the proportional distribution of different frame categories in
each cluster. View (c) shows a weighted choropleth map which colors each country based on the weighted frame density. The weights on each
category can be changed interactively by the analysts.
3 E VENT C UEING E NVIRONMENT
The goal of our visual analytics framework is to facilitate the hypothesis generation process by linking multisource data through statistical
event cueing in the form of intervention models. Our framework consists of three main views: 1) the spatial view (top left Figure 1), which
visualizes the geographic location of media steams and events coded in
secondary data sources; 2) the detail view (top right Figure 1), which
provides a lens into the media text and detailed descriptions of events
from secondary data sources chosen by the analyst, and; 3) the time series view (bottom Figure 1), which shows a hierarchical frame-coded,
time-orientated media stream with sentiment and intervention analysis. All views are linked by the overview timeline shown in the middle
of Figure 1 which displays the trend of a secondary dataset.
3.1 Task Characterization
The basis for this work is founded on an interdisciplinary collaboration between computer science and communication. Partners from the
Hugh Downs School of Human Communication at Arizona State University are interested in applying their knowledge of framing to issues
of national security risks related to climate change. Their work focuses
on exploring the framing of climate change research in Africa and how
(if at all) this is impacted by ongoing conflicts in the region. They posit
that, in order to understand how the media reflect different organizations’ interests in addressing climate change as a social problem, it is
necessary to supplement the social movement focus on resource mobilization to framing processes of collective action problems. To do this,
they developed a nuanced typology for studying climate change framing and its adequacy for supporting social movement that would be
necessary to overcome the collective action problem. They apply this
framework to examine framing of climate change in media and social
media texts collected from the Niger Basin region over eight months
from August 2014 to March 2015, applying a novel coding technique
to assess diagnostic, prognostic and motivational framing as the keys
to effective social movements. While the datasets and examples given
in this application focus on media with regards to climate change and
social unrest, our techniques can be adopted to any multi-source data
in which analysts are looking for changes in media frames due to associated events (for example, severe flooding, prolonged droughts). We
have identified three major intents of the communication scientists in
the context of media analysis:
1) Analysts would like to know how frames are spatially distributed
to understand the international context of framing;
2) Analysts would like to know when the distribution of frames
change and quickly be able to explore events that may have impacted this change in media framing;
3) Analysts would like to know what people, locations and organizations are being discussed in the media before, during and after
changes in framing occur.
As such, our framework has been designed to support the spatiotemporal analysis of frames and cue analysts to when the distribution of

frames has changed. These cues then suggest time windows in which
to explore links to secondary datasets.
3.2

Datasets

To illustrate our framework, we use a climate change media dataset
and the Armed Conflict Location & Event Data Project (ACLED)
dataset [1] as an example. However the proposed framework is flexible
for analyzing any media data.
Media: The media dataset is composed of RSS feeds from 122 English language news outlets in the Niger basin countries since August
2014. RSS feeds were scanned hourly and filtered for relevance in
a two-stage process. First, news texts were matched against a set of
222 keywords developed from the Intergovernmental Panel on Climate
Change (IPCC) report and supplemented by project experts. Subsequently, texts passing the keyword test were analyzed by a machine
classifier, trained on a set of 1,000 texts classified by coders as relevant or irrelevant to social discourse of climate change. News articles passing both tests were placed into the database for analysis.
The RSS news dataset collected 1245 relevant articles with 9070 sentences. For this study, each sentence was coded by trained coders
into one (or none) of 25 categories comprising four classes (cause,
problems/threat, solution, motivation) that represent different types of
framing for climate change. Then each article was represented by a
vector of frame counts normalized by the number of sentences coded.
The average Krippendorff α reliability of the coders on a set of training documents was 0.81 and judged to be acceptable. Future work will
use trained classifiers for frame extraction.
ACLED: The ACLED dataset contains information on the dates and
locations of all reposted political violence events in over 50 developing
countries, with a focus on Africa. Each event record contains information on date, location, event type and actors involved. From August to
December 2014, it contains approximately 6500 events.
3.3

Media Data Processing

Media messages contain large amounts of information which can be
complex to effectively analyze. Our framework applies a variety of
automatic data preprocessing techniques including entity, geolocation,
and sentiment extraction.
Entity Extraction: Entities, such as person names, locations, and organizations, inform much of the underlying media discourse. A variety
of named entity recognition methods have been proposed for different
contexts in natural language processing. We applied the well-known
natural language processing tool CoreNLP [29] to a streaming RSS
news dataset and the secondary dataset (ACLED in our example) to
extract named entities. For the 1245 articles from August to December
2014, we have 19,756 entities in which there are 2107 persons, 5791
locations and 3146 organizations extracted from the RSS dataset. The
same entity recognition process was performed on the ACLED dataset
for the notes in each record, extracting 367 persons, 998 locations, 286
organizations.
Geolocation Extraction: An article may have location attributes, either based on where the article is posted or the region the article

LU ET AL.: EXPLORING EVOLVING MEDIA DISCOURSE THROUGH EVENT CUEING

223

Fig. 3: Entity lens on the map shows the most frequently appearing entities recognized from documents that are geo-located in the lens’ area.
The left figure shows the named entities for the RSS news dataset, the middle figure shows the actors given in the ACLED dataset,and the right
shows a comparison lens with the RSS news’ entities on the left and ACLED events’ actors on the right. The example shown in this Figure
covers the data from Oct. 11th to Oct. 27th for all the ACLED event type and problem frames in the RSS news.
discusses; however, this information is not always explicitly coded.
Given that framing may differ by geographic region, our framework
preprocesses the media stream to extract geographic locations. We use
the Data Science Toolkit [2] to extract and geocode this information.
Sentiment Extraction: Media data encapsulates information about
events, responses, and reviews. In exploring media data, sentiment
analysis can provide a quick overview of the attitude a media document’s author might have with regards to the underlying story. To
extract the sentiment embedded in the media data, three sentiment
analysis classifiers are applied at the per sentence level. Details on
the classifiers are provided in Section 3.6.
3.4 Geographical View
Both media data and event sequences from the secondary dataset have
geolocation information. The geographical view is built to explore the
distribution of frames and compare entities between media data and
other data sources.
3.4.1 Exploring Spatial Distribution of Frames
Previous work on frame visualization focused on document keywords.
In this work, we want to allow users to explore frames by country, entities, and sentiment. To analyze the spatial distribution of different
frames, we created a categorical spatial data distribution visualization
view, Figure 2. To show the cumulative frame distribution of a dataset,
Figure 2(a) displays a choropleth map colored by the density of frames
in each country. Users can select any subset of frame categories to analyze. If only one class of frames are selected, the map color matches
the color of the class, otherwise it uses gray. A drawback of this visualization is that only one variable/feature of the underlying data can be
represented, even though there are multiple categories of frames in the
data. To allow for multivariate encoding, we also use a symbol map
with a pie chart, where each segment of the pie represents the number
of sentences of a given frame (Figure 2(b)), and a weighted category
choropleth map (Figure 2(c)) where colors correspond to a multivariate criteria function. Additionally, a tooltip displaying the histogram
of different categories within a country is enabled to help better explore frame distributions.
3.4.2 Exploring Geo-located Entities
Our framework considers two types of entities in the data. One is recognized name entities, which are people, locations and organizations.
The other is the predefined entities that may exist in the structured
datasets that an analyst wishes to explore in the context of media discourse. We created an entity lens to explore geo-located text data.
The geocoding of the entities derives from a sentence’s geocoding for
the RSS news dataset and the reported geolocation from the ACLED
dataset. The entity lens is shown in Figure 3, where the most frequently referenced entities within the lens’s area are extracted and organized around the lens. The most frequent entities are mapped closest
to the lens’s circumference based on available canvas space. The font
size is dependent on the entities’ frequency within the lens’s circumference. The more frequent an entity is, the larger the text.

To link different datasets and find relationships between them, this
entity lens has three modes: media data entity mode which shows only
the RSS news entities (Figure 3(Left)), secondary data entity mode
which shows only the ACLED actors (Figure 3(Middle)), and the combination mode which is a two-sided lens to encode entities for multisource data (Figure 3(Right)). The combination mode shows the top
entities from the RSS news dataset on the left of the lens and ACLED
actors on the right with a dashed line in the middle to separate one
from the other. All modes are also enabled in a coordinated view in
which the lens can move over the map and update the wordles.
3.5

Hierarchical Frames Timeline View

The previous views are necessary to allow overview and detail views;
however, the major contribution of this paper is the event cueing which
is enabled through the hierarchical frames timeline. Previously mentioned techniques enable exploratory data analysis, the problem is that
purely exploratory techniques put the burden of analysis completely on
the analyst. Our goal is to cue the analyst to events that are statistically
interesting in the data. To enable this, we begin with the timeline view
showing the relative volume of frames per day. Specifically, each document has a number of sentences that are encoded with a single frame.
The percent of framing of a document is the number of sentences in
a document associated with a given frame divided by the total number of sentences framed. The frame volume can be visualized by the
average document percent per day, the average number of sentences
encoded with a frame across all documents in a day, or a variety of
other metrics.
To detect possible interventions, we applied two time series analysis
models and visualized the results on the timeline to cue analysts’ exploration. In addition, sentiment information associated to the underlying data is also revealed by a two-side uncertainty-based stack river.
Figure 4 shows our hierarchical timeline view of the media frames.
Here an analyst expanded the cause frame to explore sentences that
frame the cause of climate change to be due to human, natural effects,
policies, or one uncertain of the cause.
Our approach is centered around the concept of an intervention.
We assume that there may be events intervening with media reactions
that cause a shift in how frames are distributed. We use statistical
hypothesis test to detect the intervention dates. For each day in the
dataset, we assume an intervention may have occurred. If a date under
test indicates that the times before and after are statistically different,
this then cues an analyst to explore related datasets to help enhance
their understanding of what (if any) events may have triggered these
changes in discourse.
Intervention Modeling Intervention models are used to explore what
(if any) impact there is between an event and some secondary measure,
for example, the impact that 9-11 had on George Bush’s approval rating. In this case, we consider our events to be armed conflicts and
the measure to be the amount of sentences that are framed in a document with respect to one of the 25 climate categories. Note that such
models can be used for any event and measure dataset combination.

224	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016

Fig. 4: Hierarchical timeline view showing intervention modeling results, Before-During-After analysis results and sentiment river for each
expanded frame or frame category are shown. The frame structure is displayed as a dendrogram on the left. Clicking on the node can expand/collapse its children. The timeline associated with each leaf node is shown on the right.
Mathematically, when a time series model is affected by another input
time series, a transfer function-noise model can be used to improve the
model. The general form of this type of model is:
yt = v(B)xt + Nt

(1)

where yt is the time series of interest, v(B) is an autoregressive, integrated, moving average (ARIMA) model for the time series yt , xt is
the input time series, and Nt is a noise process [31]. A specific case
of transfer function-noise models is an intervention model, where the
input time series is an indicator variable that specifies whether some
event has taken place at time t. Such an event may have a temporary
(or permanent) effect on the level or mean of the time series of interest.
An intervention model can model the effect of a known event on
the time series. However, another common application of intervention
models is to identify outliers in the time series. In this case, we do
not know the exact time period in which the event (outlier) has taken
place. The transfer-function model for this application then becomes:
(t ∗ )

(t ∗ )

yt = v(B)εt + ωIT , where It

=

 1 if
0 if

t = t∗
t = t ∗

(2)

where ω is the change in the mean of the time series at time t ∗ and
(t ∗ )
It is an indicator function assuming that the effect of the outlier is
temporary and only occurs at time period t ∗ . Other models can be used
to model the case where an outlier may have a lasting impact on the
mean of the time series. An iterative procedure is used to identify multiple outliers in the time series. In this scenario, multiple intervention
(t ∗ )
models are fit, updating It for t ∗ = 1, . . . , N for a time series with N
time periods.
For the media data explored in this paper, intervention models were
used to detect outliers, i.e. cues to events that may be of interest to
the analyst, for each of the 25 frames over the time period of August
2 to December 31, 2014. For our intervention model, Figure 4 shows
the trend of several frame categories. A black dot represents a statistically significant shift in frames between the week before and after this
date. Users can then use the coordinated views to explore events that
occurred at this time and begin forming hypotheses on the impact that
events may have had on the media framing. Note that this is for event

cueing and hypothesis generation. Events and frames of interest found
require further investigation. Initial analysis of each time series (each
frame) indicated that there was no significant autocorrelation present.
Therefore, the intervention model can be simplified to:
(t ∗ )

yt = µ + ωIt

+ εt

(3)

where µ represents the overall mean of the time series and εt
represents the error. Outliers at time t ∗ ,t ∗ = 1, . . . , N, can be identified
by comparing the estimated value of ω, ω̂, to its standard error [31].
A significance level of α = 0.05 was used to determine whether the
value of the frame at time t ∗ was an outlier. The presence of an
outlier cues the analyst to investigate what caused this change in the
frame distribution. Although the intervention model is simplified
because the frame time series were not autocorrelated, this approach
is still valid for time series data that does have autocorrelation and
Equation 2 would be used in such cases. Such models are sensitive
to the time period under exploration. In this case, our analysts were
exploring short term changes (1 week prior to the event, 1 week after
the event). As such, the results of the intervention model tend to
highlight peaks in the data; however, this is likely an artifact of the
chosen window sizes. Future work will explore visual representations
for exploring interventions under varying window sizes.
Before-During-After Analysis Since there was no autocorrelation
in the data, a secondary model which requires an assumption of data
independence, can be applied. The second intervention test defines
a Before, During, and After period (where the during period can be
seen as the intervention) and tests their location based on the data
distributions. We let t denote the start time of the During period,
and the time windows for the Before, During and After segments are
represented by WB ,WD , and WA respectively. In this manner, the three
time segments cover the following time periods: Be f ore : (t −WB ,t),
During : (t,t + WD ), and A f ter : (t + WD ,t + WD + WA ). The data
samples for the three segments are denoted as DB = {x1 , x2 , . . . , xnB },
DD = {y1 , y2 , . . . , ynD }, DA = {z1 , z2 , . . . , znA } and they may vary
in length. Each data sample is the percentage of the frame in one
document. Because there was no significant autocorrelation present
in our underlying dataset, each sample is assumed to be independent
and identically distributed (i.i.d.) where Di ∼ N(µi , σi2 ). Therefore

LU ET AL.: EXPLORING EVOLVING MEDIA DISCOURSE THROUGH EVENT CUEING

225

we form the problem to be tested as follows:
A1 : µD is not significantly different than µB
A2 : µD is not significantly different than µA
H0 : µD is not associated with an intervention (A1 ∩ A2 )
H1 : µD is associated with an intervention (A¯1 ∪ A¯2 )
We test H0 by testing A1 and A2 . To test A1 and A2 , we applied a two-sample location test, Welch’s t-test [40], on DB , DD and
DD , DA individually with significance level α. In these two t-tests,
the statement is the null hypothesis. Because of the multiple comparisons problem (in our case we have two tests one for DB and DD ,
and another for DD and DA ), and based on the Bonferroni inequality
P(A1 ∩A2 ) ≥ 1−2α, we applied Bonferroni correction and set the significance level for each test according to the following equation [32],
α=

αF
,
#test

(4)

where α is the significance level for each two-sized t-test, αF is the
family significance level for the multiple comparison for each During time period, and #test is the number of tests applied at each time
period. In our case, #test equals 2 (the tests of A1 and A2 ). We set
αF = 0.05, which guarantees that the overall significance level for
the 2 hypothesis tests at each frame period is 0.05. To guarantee
αF = 0.05, we set α = 0.025 for each single test on the pair of consequent segments. Given the test result and the estimated µ, we can
form 9 types of volume change patterns listed in Table 1. The 9 types
are visualized in different color blocks on the time line for each frame,
as shown in Figure 4 and Figure 5.
The color scheme also denotes the group of patterns as increasing,
decreasing and oscillating. To change the interval length of each test
time period, the user can change the size of the three windows for
Before During and After using the spinners on the left. To better focus on a particular Before-During-After pattern, the user can click on
the pattern legend to gray out options. In addition to knowing the intervention time point from the results of the intervention model, the
Before-During-After analysis provides an adjustable window size and
shows any significant changes.
The statistical tests’ results are visualized in our timeline view for
each frame and frame categories, shown in Figure 4 and Figure 5.
The intervention modeling result is a set of binary indicators denoting
the statistically significant intervention points. This result is represented as a black dot on our timeline view. The Before-During-After
analysis’s result is a set of patterns describing statistically significant
changes in frame distribution over time. In both cases, the analyst can
adjust the before, after and intervention (during) periods using the controls seen in Figure 1 (lower left). In our case study, the analysts were
interested in a single day intervention with a 7 day news cycle.
Table 1: The pattern summary for Before-During-After analysis. Each
pattern is associated with a unique hue as shown in the lower lefthand
legend of Figure 1
pattern
B=D=A

sketch

description
no significant change

B=D<A
B<D=A

increase

B<D<A
. B=D>A

(blues)

B>D=A

decrease

B>D>A

(greens)

B>D<A

oscillating

B<D>A

(oranges)

Fig. 5: Sentiment stacked area chart on bi-side of the time series view.
The blue area represents positive sentiment and the red area river represents negative sentiment. The darker the area color is the more certain the label is for those sentences’ sentiment class.
3.6

Frame Sentiment Visual Analytics

The underlying sentiment of the media and its relation to the framing
can also provide insight. Sentiment analysis visualization has been
successfully applied across a variety of domains, such as political election analysis [38], and merchandise reviews [33]. However, most classifiers are text context sensitive and need to be trained on a particular
domain’s data to boost performance. Furthermore, the limitation of
sentiment classification accuracy is a problem in sentiment analysis
and is subject to uncertainty [10]. In our visual analytics framework,
we employ anl entropy-based sentiment river to reveal the uncertainty
of sentiment over time using an ensemble voting scheme from multiple
classifiers to determine the final sentiment label [26].
In our previous work [26], the uncertainty was visualized in each
time period along the entropy sentiment river. However, the time information associated with RSS media data is not as precise as online
social media data, such as Twitter. In general, the time parsed out
from the RSS news is at the granularity of a day. In one day, there
can be multiple articles collected relating to the target topic and each
article also contains several frame coded sentences. Instead of exploring only the change of the certainty over the media stream, the volume
of certain and uncertain sentiment labels is also explored. To enhance
the understanding of the volume change for both certain and uncertain sentiment labels, a stacked area graph is used to represent each
uncertain level with a stacked area and low uncertain area is stacked
at bottom. Figure 5 shows this view, where the positive sentiment is
colored in blue on top of the time series, while the negative sentiment
is colored in red on bottom of the time series. The volume of relatively
certain sentiment values are shown with a darker color and the uncertain volume with a lighter color. The height of the stacked area graph
shows the average volume of sentences per document in each sentiment polarity over time as well as the portion of uncertainty. In this
way, we can explore the positive and negative sentiment of the media
documents in conjunction with their underlying frames.
3.7

Detail View

The detailed view, Figure 6, contains two modes, the entity wordle
display and the list-based summary display. The data under analysis
for this view changes along with the time period selection, the subset
data selection for both media data and the secondary data, and the
geospatial selection. When a user is only exploring the frame class
‘Problem’ which is colored in red, only the RSS articles containing at
least one sentence being framed as ‘Problem’ will be displayed in the
detail view. For a geospatial selection, e.g. the user selects a country
to explore, the data displayed in the detail view updates to filter for
only the articles and ACLED events related to this country.
In the entity wordle view, the most frequently named entities extracted from the two datasets are displayed in two wordles. Based on
the entity’s class, which is either person, location, or organization, the
word is colored in red, green, or blue respectively. The actors in the
ACLED dataset, being entities as well, are colored in black. The size
of those entities displayed here is also proportional to its frequency.
In the list-based summary view, the RSS news article is summarized
by showing the title and a list of colored squares, where each square
represents each framed sentence colored by its frame class’s color. The
ACLED data, being the secondary data here, displays its notes for each
event in the selected time period. The background color of each note
matches the color for its event type. To analyze events containing

226	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016

(a) Detail view in entity wordle display

(b) Detail view in list-based summary display

Fig. 6: The detail view showing the most frequently named entities in a wordle display and document summary information in a list-based
display. Here we show data from Oct.11 to Oct 27. View (a) is the entity wordle display in which user can choose three classes of entities
(person, location, organization) to show. Black text in the ACLED wordle indicates an actor in the events. View (b) is the list-based summary
display in which the title of media articles and the summary information of the secondary dataset are listed in order of time. The frame
information of each article is summarized into colored squares (the color of the square matches the frame class) in the sentence order from the
article. In this example, the ACLED event notes are filtered by clicking on the entity text ‘Boko Haram’.
a particular entity of interest, a user can click on a particular entity
shown in the wordle display and information containing that entity
will show up in the summary display. Users may also filter by location
by selecting a country in the geographical view.
4

C ASE S TUDY: C LIMATE C HANGE F RAMING AND A RMED
C ONFLICT IN A FRICA
In this section, we demonstrate our work by applying the methods described so far to the RSS news dataset collected on Climate Change
from African countries and the ACLED data set, which focuses on
armed conflicts and political violence in Africa. Collaborators were
interested in linking these two data sets based on previous articles and
reports that discussed the impacts of climate change on armed conflicts [34]. Their goal was to explore the framing of news stories related to climate change and see what, if any, armed conflict events may
be linked to that discourse. In this manner, social scientists can begin
to develop models and theories about how framing can help drive political change, or conversely, how armed conflict is driving discourse.
4.1 Exploring Problem Frames in Africa
The analyst begins with an overview of the system and explores the
distribution of frames over the entire time period of data collection.
The main points of interest are the spatial and temporal distributions of
frames, Figure 7. First, the analyst explores the spatial distribution of
frames, looking at the weighted majority choropleth map. The analyst
notes that most regions are discussing climate change either in terms
of problems (red) or solutions (green). Only a few countries, such
as the Republic of Guinea-Bissau and the Republic of Côte d’Ivoire,
have a majority distribution related to causes of climate change, and
Congo has more motivation frames. The analyst drills down into the
data by selecting a country and quickly learns that only one document
has geographic information related to these countries. Thus the analyst
determines that these outliers are of little interest.
Given that the discourse seems to focus heavily on both problems
and solutions, the analyst decides to explore the temporal view with a
focus on problems. The analyst searches the top level problem hierarchy looking for significant events found in both the intervention model
and Before-During-After model. The analyst finds a time period in
late October (highlighted as circle a in Figure 7) with several points
of interest, and highlights this time period for inspection. The analyst then expands the tree and explores the leaf node problem frames,
Figure 7(bottom). The analyst notes that there are significant interventions in many categories, but very few frames regarding security
threats and water problems in this time period. The analyst further
comments on the lack of water framing in the documents noticing that
climate change is often associated with extreme weather, including
drought, yet there is little discussion in Africa about problems related

to water. The analyst does notice that there are many documents discussing problems with food.
The analyst decides to first focus on the food problem frame and
the events leading to this change in the frame distribution. The analyst narrows the time period to October 11th to October 28th, and
then filters for RSS news articles containing frame category ‘ProblemThreat’ and ACLED events Riots and Battles. The analyst wants
to explore what geographic regions are seeing large amounts of armed
conflict during this time period. The analyst selects the most prevalent
ACLED events (Riots-yellow and Battles-red) using the donut control.
The analyst notes that the largest amount of conflicts are occurring in
Nigeria, Sudan and Somalia. Given the importance of the Niger River
Basin in the area, the analyst chooses to explore events in Nigeria that
may be driving the discourse on climate change. The analyst notes
that it is interesting that there is a clear separation between the riots (in
the south) and the battles (in the north). The analyst selects Nigeria
to filter the detailed view to only RSS documents and ACLED events
that are geocoded to Nigeria.
Looking at the RSS articles’ titles, the analyst finds many reports
talking about the problem of food security and famine in Africa. While
exploring the ACLED events in the same time period, the analyst locates several riots and battles discussing the impact of Boko Haram
on farmers, where militarists are killing farmers and forcing them to
flee their homes, exacerbating the food problem. Example articles and
events are shown in Figure 8(Left). What is interesting to the analyst
is that articles are already discussing the famine problems that Africa
will face due to climate change. If this is further exacerbated by wars,
the problem cycle may become more prevalent resulting in displacement, migration, and potential social unrest. From a social science
perspective, our analysts are interested in how to model such phenomena. By cueing them to such events, they are able to begin looking at
how ongoing events could be modeled to predict future problems.
After discussing the events surrounding the food frame cue, the analyst then decides to also explore the ProbThreatHealth frame (problems associated with health) next. The analyst is interested in the
two significant events that occurred between October 11th and October 28th. Again, the analyst begins exploring related ACLED events
during this time period, and quickly finds several riot/protest events
related to the mistreatment of healthcare workers in the region. The
analyst again noted their interest in these articles and the fact that the
event cueing was able to narrow down their search to potentially relevant information. While there are some obvious links between food
security, armed conflicts and riots (for example, Boko Haram displacing farmers), subtle social issues involved with riots may be harder to
spot. Furthermore, given that such riots are taking place at this time
and there is a shift in frames, the analyst hypothesized that this could
represent a shift in the discourse in the hopes to alleviate concerns

LU ET AL.: EXPLORING EVOLVING MEDIA DISCOURSE THROUGH EVENT CUEING

227

Fig. 7: Exploring the whole time period on the RSS news dataset spatially and temporally. The spatial map shows a weighted choropleth map
with all frame class equally weighted. The add-on histogram shows the frame volume and distribution of the Republic of Côte d’Ivoire. The top
timeline view shows the level of four frame classes and two black circles highlight the time period of interest in ProblemThreat and Motivation.
The bottom timeline view shows the expanded timelines in the ProblemThreat frame class and the time period of interest is highlighted.
from the general population. While no definitive conclusions could
be made at this time, this example further illustrates how our framework can enhance the hypothesis generation process. By specifically
cueing an analyst to a time of interest, we can dramatically cut their exploratory analysis time. For example, there are over 40 ACLED events
per day, each with an associated set of documents. Uncued analysis of
such work would be an extremely laborious process.
4.2

Exploring Motivation Frames in Africa

The analyst concentrates on examining press coverage between
November 1st and November 14th, and identifies events accounting
for notable intervention points on November 6th based on the BeforeDuring-After model. Results indicate an increasing trend in the media
discourse on calling for policy actions on November 2nd with a negative tone. The statistically significant interventions and the burst of
the sentiment can be found in the Figure 7(highlighted in circle b).
The changing pattern is predominantly associated with the launch of
an updated synthesis report by the UN’s Intergovernmental Panel on
Climate Change (IPCC) on November 2nd. Several articles reporting
IPCC can be easily found and shown in Figure 9. As the most comprehensive assessment that attracts worldwide attention, the new IPCC
report summarizes alarming evidence detailing severe impacts of climate change. Adverse impacts range from increased risks of extreme
weather events, food shortages, and violent conflicts. The alarming
messages, circulated by several media outlets, were framed in mostly

negative words (e.g. serious impacts, severe impact, dangerous, catastrophic). In addition, analysts find prevalent explicit statements calling
for international governments to take actions now. The following sentences describe examples of motivational framing.
• “Massive cuts to greenhouse gas emissions are needed in the
coming decades to curb temperature rises to no more than 2C, the
level at which it is thought dangerous impact of climate change
will be felt.”
• “Leaders must act.”
• “There is cause for hope if governments take action.”
• “A binding meaning and enforceable framework is needed to
limit the consequences of global warming.”’
• “The world’s largest polluters, the United States and China,
should take the lead in reducing emissions.”
Conversely, there are noticeable spikes of positive sentiment values
between November 9th and November 12th. The pattern is largely
associated with favorable coverage of U.S. and China announcing a
historical climate change agreement on November 11 when President
Obama visited Beijing for the Asia Pacific Economic Cooperation
(APEC) summit. Together, motivation frames in West Africa reflect
a focus of relying on international actors to drive policy negotiation.

228	

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 1, JANUARY 2016

Fig. 8: The geographical and detail view for exploring RSS news and ACLED data. This figure shows the analyzing time from October 11th to
October 28th. The geographical view color each country by the majority frame class and displays riots (orange dots) to represent the ACLED
events. The detail view lists the Riots events related to health problem within and outside Nigeria. The left side detail view shows examples of
RSS articles discussing food problem in Africa and the ACLED events are riots and battles expressing problem of food supply.
Results of analysis on motivational frames should be viewed in
light of limitations. In the 1,245 relevant articles collected from West
African news media and twitter links, there is little evidence of motivation framing, as less than 10% of a news story contained statements calling for definitive courses of actions, That is, motivational
frames are very uncommon compared to other three frame classes
(cause, problem/threat, and solution). When a set of news stories highlighted explicit calls for actions to solve climate change issues within
the same time period, it is highly possible that the consistent pattern in
press coverage was statistically significantly different than before and
after in the time series analysis. Despite the low presence of motivational statements in the current dataset, the visualization tool allows
researchers, analysts, and policy makers to explore the potential underlying mechanisms linking adverse impacts of climate change and
increased risk of political conflicts.
4.3

Analyst Feedback

Our case study involved two analysts from the Department of Communication at Arizona State University. Feedback on the system was
positive with analysts indicating that the event cueing features were extremely useful in providing a starting point for searching linked data.
Case Study 1 was done as a paired analysis demonstrating the tool
with the computer scientists manipulating the controls and discussing
how the system worked. Case Study 2 was done at the communication
lab with no assistance from the computer science group (the tool is
web-deployed).
Overall feedback was positive with the analysts stating that they
were “fascinated by the visualization tool’s ability to map out temporal and spatial components of media discourse”. In addition, the
analysts also mentioned that this tool can help to tackle co-occurrence
patterns of conflicting events, limiting the possibility of bridging distinct lines of scholarship together–media research, climate change and
conflicts. However, there were suggestions for future work and improvement. Specifically, the analysts were interested in the difference
between the change models and their disagreements. For example, in
Figure 1, there is an intervention marker (black dot) near October 5th
for motivation, but no colored squares from the before-during-after
analysis. The relationship between these two models required more
explanation and future work will explore creating a single ensemble
metric. Along with the intervention model, the analysts also requested
the ability to reconfigure layouts for improved storytelling. They indicated that they would be able to better explore relationships with a
series of small multiples and better alignment between the temporal
components of the unrest data and the framing data.

Fig. 9: Example RSS articles and the entity wordle for the time period of Oct. 28th to Nov. 11th exploring motivation frame. The left
side article summaries show examples of news report relating to the
IPCC and the right side wordle emphasizes the most frequent entities
appearing in those articles, such as IPCC, Obama, and China.

F UTURE W ORK

enables users to explore more complex hypotheses that can enable
analysts to link potential cues between disparately collected sources.
While several visual analytics methods [11, 12, 13] have explored
frames in the context of comparing corpora of text and topical terms
within these text, our framework enables sentiment analysis and intervention modeling which can provide different insights than previous
work.
Our framework was evaluated through collaboration with domain
experts from the School of Communication and findings from their
exploration have prompted new questions and directions to explore.
While our examples focused on climate change and conflicts in Africa,
the tools developed are applicable for a variety of media sources. Furthermore, it is important to note that our intervention strategy can be
applied to any temporal variable, and, by utilizing multiple models,
we are able to strengthen the analysts’ confidence in the findings. This
was particularly evident in the exploration process. Anomaly detection methods, intervention models and others often have a large false
positive rate. By using an ensemble of models, one can begin defining uncertainty. Future work will focus on a combination of anomaly
models and intervention models as well as a weighted output for defining uncertainty in the detection, similar to our sentiment modeling approach. We also plan to explore a combination of sentiment analysis, frames and clustering for defining geo-political regions that share
common framing strategies. We believe that such methods can further
enable multisource data exploration and provide new cues to analysts
who are developing hypotheses and exploring the evolution of topics,
events and discourse both locally and globally.

In this paper, we have demonstrated a framework for event cueing that
enables the exploration of evolving media discourse. Our framework
focuses on both the spatial and temporal distribution of frames, and
allows experts to quickly explore spatial trends in the underlying discourse. By linking multisource data for exploration, our framework

ACKNOWLEDGEMENT
Some of the material presented here was sponsored by Department of
Defense and is approved for public release, case number 15-365 and
upon work supported by the NSF under Grant No. 1350573.

5

C ONCLUSIONS

AND

LU ET AL.: EXPLORING EVOLVING MEDIA DISCOURSE THROUGH EVENT CUEING

R EFERENCES
[1] Armed conflict location & event data project. http://http://www.
acleddata.com/. Accessed: 2015-03-28.
[2] Data science toolkit. http://www.datasciencetoolkit.org.
Accessed: 2015-03-18.
[3] W. Aigner, S. Miksch, H. Schumann, and C. Tominski. Visualization of
time-oriented data. Springer Science & Business Media, 2011.
[4] R. D. Benford and D. A. Snow. Framing processes and social movements:
An overview and assessment. Annual Review of Sociology, pages 611–
639, 2000.
[5] M. Bogl, W. Aigner, P. Filzmoser, T. Lammarsch, S. Miksch, and A. Rind.
Visual analytics for model selection in time series analysis. IEEE Transactions on Visualization and Computer Graphics, 19(12):2237–2246,
2013.
[6] H. Bosch, D. Thom, M. Worner, S. Koch, E. Puttmann, D. Jackle, and
T. Ertl. Scatterblogs: Geo-spatial document analysis. In Proceedings of
IEEE Conference on Visual Analytics Science and Technology (VAST),
pages 309–310. IEEE, 2011.
[7] J. Chae, D. Thom, H. Bosch, Y. Jang, R. Maciejewski, D. S. Ebert, and
T. Ertl. Spatiotemporal social media analytics for abnormal event detection and examination using seasonal-trend decomposition. In Proceedings of IEEE Conference on Visual Analytics Science and Technology
(VAST), pages 143–152. IEEE, 2012.
[8] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey.
ACM Computing Surveys (CSUR), 41(3):15, 2009.
[9] D. Chong and J. N. Druckman. Framing theory. Annual Review of Political Science, 10:103–126, 2007.
[10] N. F. da Silva, E. R. Hruschka, and E. R. Hruschka. Tweet sentiment
analysis with classifier ensembles. Decision Support Systems, 66:170–
179, 2014.
[11] N. Diakopoulos, D. Elgesem, A. Salway, A. Zhang, and K. Hofland.
Compare clouds: Visualizing text corpora to compare media frames. In
Proceedings of IUI Workshop on Visual Text Analytics, 2015.
[12] N. Diakopoulos, A. Zhang, D. Elgesem, and A. Salway. Identifying and
analyzing moral evaluation frames in climate change blog discourse. In
Proceedings of International Conference on Weblogs and Social Media
(ICWSM), 2014.
[13] N. Diakopoulos, A. X. Zhang, and A. Salway. Visual analytics of media frames in online news and blogs. In Proceedings of IEEE InfoVis
Workshop on Text Visualization, 2013.
[14] W. Dou, X. Wang, D. Skau, W. Ribarsky, and M. X. Zhou. Leadline:
Interactive visual analysis of text data through event identification and
exploration. In IEEE Conference on Visual Analytics Science and Technology (VAST), pages 93–102. IEEE, 2012.
[15] W. Dou, L. Yu, X. Wang, Z. Ma, and W. Ribarsky. Hierarchicaltopics:
Visually exploring large text collections using topic hierarchies. IEEE
Transactions on Visualization and Computer Graphics, 19(12):2002–
2011, Dec 2013.
[16] R. M. Entman. Framing: Toward clarification of a fractured paradigm.
Journal of Communication, 43(4):51–58, 1993.
[17] P. Federico, S. Hoffmann, A. Rind, W. Aigner, and S. Miksch. Qualizon
graphs: Space-efficient time-series visualization with qualitative abstractions. In Proceedings of the 2014 International Working Conference on
Advanced Visual Interfaces, pages 273–280. ACM, 2014.
[18] D. Fisher, A. Hoff, G. Robertson, and M. Hurst. Narratives: A visualization to track narrative events as they develop. In IEEE Symposium on
Visual Analytics Science and Technology, pages 115–122. IEEE, 2008.
[19] T. Gao, J. R. Hullman, E. Adar, B. Hecht, and N. Diakopoulos.
Newsviews: An automated pipeline for creating custom geovisualizations
for news. In Proceedings of the 32nd annual ACM conference on Human
Factors in Computing Systems, pages 3005–3014. ACM, 2014.
[20] D. Gotz and H. Stavropoulos. Decisionflow: Visual analytics for highdimensional temporal event sequence data. IEEE Transactions on Visualization and Computer Graphics, 20(12):1783–1792, 2014.
[21] J. Hullman, N. Diakopoulos, and E. Adar. Contextifier: Automatic generation of annotated stock visualizations. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, CHI ’13, pages
2707–2716, New York, NY, USA, 2013. ACM.
[22] S. Ko, S. Afzal, S. Walton, Y. Yang, J. Chae, A. Malik, Y. Jang, M. Chen,
and D. Ebert. Analyzing high-dimensional multivariate network links
with integrated anomaly detection, highlighting and exploration. Proceedings of IEEE Conference on Visual Analytics Science and Technol-

229

ogy, pages 83–92, 2014.
[23] M. Krstajic, E. Bertini, and D. A. Keim. Cloudlines: Compact display of
event episodes in multiple time-series. IEEE Transactions on Visualization and Computer Graphics, 17(12):2432–2439, 2011.
[24] M. Krstajić, M. Najm-Araghi, F. Mansmann, and D. A. Keim. Story
tracker: Incremental visual text analytics of news story development. Information Visualization, 12(3-4):308–323, 2013.
[25] S. Liu, X. Wang, J. Chen, J. Zhu, and B. Guo. Topicpanorama: A full
picture of relevant topics. In Proceedings of IEEE Conference on Visual
Analytics Science and Technology, pages 183–192. IEEE, 2014.
[26] Y. Lu, X. Hu, F. Wang, S. Kumar, H. Liu, and R. Maciejewski. Visualizing social media sentiment in disaster scenarios. In Proceedings of the
24th international conference on World Wide Web companion. International World Wide Web Conferences Steering Committee, 2015.
[27] D. Luo, J. Yang, M. Krstajic, W. Ribarsky, and D. Keim. Eventriver: Visually exploring text collections with temporal references. IEEE Transactions on Visualization and Computer Graphics, 18(1):93–105, 2012.
[28] A. Malik, R. Maciejewski, N. Elmqvist, Y. Jang, D. S. Ebert, and
W. Huang. A correlative analysis process in a visual analytics environment. In Proceedings of IEEE Conference on Visual Analytics Science
and Technology, pages 33–42. IEEE, 2012.
[29] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and
D. McClosky. The Stanford CoreNLP natural language processing
toolkit. In Proceedings of 52nd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations, pages 55–60, 2014.
[30] M. Monroe, K. Wongsuphasawat, C. Plaisant, B. Shneiderman, J. Millstein, and S. Gold. Exploring point and interval event patterns: Display
methods and interactive visual query. University of Maryland Technical
Report, 2012.
[31] D. C. Montgomery, C. L. Jennings, and M. Kulahci. Introduction to Time
Series Analysis and Forecasting. Hoboken, NJ: John Wiley & Sons, 2008.
[32] J. Neter, M. H. Kutner, C. J. Nachtsheim, and W. Wasserman. Applied
linear statistical models, 5th edition, volume 4. Irwin Chicago, 1996.
[33] D. Oelke, M. Hao, C. Rohrdantz, D. Keim, U. Dayal, L. Haug, and
H. Janetzko. Visual opinion analysis of customer feedback data. In IEEE
Symposium on Visual Analytics Science and Technology, pages 187–194,
2009.
[34] J. OLoughlin, A. M. Linke, and F. D. Witmer. Effects of temperature and precipitation variability on the risk of violence in sub-saharan
africa, 1980–2012. Proceedings of the National Academy of Sciences,
111(47):16712–16717, 2014.
[35] D. A. Scheufele. Framing as a theory of media effects. Journal of Communication, 49(1):103–122, 1999.
[36] D. Thom, H. Bosch, S. Koch, M. Worner, and T. Ertl. Spatiotemporal anomaly detection through visual analysis of geolocated twitter messages. In Pacific Visualization Symposium, pages 41–48. IEEE, 2012.
[37] J. J. Van Wijk and E. R. Van Selow. Cluster and calendar based visualization of time series data. In IEEE Symposium on Information Visualization,
pages 4–9. IEEE, 1999.
[38] F. Wanner, C. Rohrdantz, F. Mansmann, D. Oelke, and D. A. Keim. Visual sentiment analysis of RSS news feeds featuring the US presidential
election in 2008. In Workshop on Visual Interfaces to the Social and the
Semantic Web, 2009.
[39] F. Wanner, A. Stoffel, D. Jäckle, B. Kwon, A. Weiler, D. Keim, K. E.
Isaacs, A. Giménez, I. Jusufi, T. Gamblin, et al. State-of-the-art report
of visual analysis for event detection in text data streams. In Computer
Graphics Forum, volume 33, 2014.
[40] B. L. Welch. The generalization of ‘student’s’ problem when several
different population variances are involved. Biometrika, pages 28–35,
1947.
[41] K. Wongsuphasawat and D. Gotz. Exploring flow, factors, and outcomes
of temporal event sequences with the outflow visualization. IEEE Transactions on Visualization and Computer Graphics, 18(12):2659–2668,
2012.
[42] K. Wongsuphasawat, J. A. Guerra Gómez, C. Plaisant, T. D. Wang,
M. Taieb-Maimon, and B. Shneiderman. Lifeflow: Visualizing an
overview of event sequences. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, pages 1747–1756. ACM, 2011.
[43] C. Xie, W. Chen, X. Huang, Y. Hu, S. Barlowe, and J. Yang. VAET: A visual analytics approach for e-transactions time-series. IEEE Transactions
on Visualization and Computer Graphics, 20(12):1743–1752, 2014.

OntoMiner: Bootstrapping Ontologies
From Overlapping Domain Specific Web sites
Hasan Davulcu

Srinivas Vadrevu

Department of CSE
Arizona State University
Tempe, AZ 85287-5406, USA

Department of CSE
Arizona State University
Tempe, AZ 85287-5406, USA

hdavulcu@asu.edu

svadrevu@asu.edu

Saravanakumar
Nagarajan
Department of CSE
Arizona State University
Tempe, AZ 85287-5406, USA

nrsaravana@asu.edu

ABSTRACT

Almost all scientific, news, financial, travel, shopping and
search/community portals that we are aware of are indeed
In this paper, we present automated techniques for boot”taxonomy directed”.
strapping and populating specialized domain ontologies by
The user of the OntoMiner system only need to provide
organizing and mining a set of relevant overlapping Web
the
system with the URLs of the Home Pages of 10 to 15
sites provided by the user. We develop algorithms that detaxonomy-directed
overlapping domain specific Web sites
tect and utilize HTML regularities in the Web documents to
that characterizes her domain of interest. A pair of Web sites
turn them into hierarchical semantic structures encoded as
are said to be overlapping if their taxonomies share some
XML. Next, we present tree-mining algorithms that idenconcept labels. Next, OntoMiner system detects and utitify key domain concepts and their taxonomical relationlizes the HTML regularities within every Web document and
ships. We also extract semi-structured concept instances
turns them into hierarchical semantic structures encoded as
annotated with their labels whenever they are available. ExXML by utilizing a hierarchical partition algorithm. Onperimental evaluation for the News, Travel, and Shopping
toMiner uses tree-mining algorithms to identify most imdomains indicates that our algorithms can bootstrap and
portant key domain concepts and relationships among them
populate domain specific ontologies with high precision and
selected from within the directories of the Home Pages. Onrecall.
toMiner proceeds with expanding the mined concept taxonomy with sub-concepts by selectively crawling through the
Categories and Subject Descriptors
links corresponding to key concepts. OntoMiner also has
H.4.m [Information Systems]: Miscellaneous; I.2.6 [Artificial algorithms that can identify the logical regions within Web
documents that contains links to instance pages.
Intelligence]: Learning—Knowledge Acquisition
A key characteristic of OntoMiner is that, unlike the early
pioneering systems described in [1, 2, 3] which attempt to exGeneral Terms
tract data values alone, OntoMiner extracts the labels correAlgorithms, Performance, Experimentation
sponding to categories and attribute names along with their
data values and organizes them into an ontology.

Keywords
2.

Web Mining, Semantic Web, Ontology, Data Mining

1.

ARCHITECTURE & ALGORITHM

The architecture of OntoMiner is as shown in Figure 1.
As illustrated in the figure, OntoMiner analyzes a collection of overlapping domain specific Web sites and generates
a taxonomy of important concepts and their associated instances. As the Crawler fetches the Web pages, they are fed
to Semantic Partitioner. Semantic Partitioner examines the
structure of the HTML Web page and converts them into
semantic trees inferring the hierarchy presented in the visual
presentation among various labels in the HTML Web page.
Taxonomy Miner examines the semantic trees generated by
Semantic Partitioner, finds the frequent labels and groups
them into concepts C. Next, it and mines the taxonomy of
concepts, T using frequent tree mining algorithms. For each
concept in the mined taxonomy, Instance Miner identifies its
potential instance Web pages and mines instances, I consisting of labeled and unlabeled attributes and their values.
It extracts the attribute labels A and their values by working with two documents, aligning the content in them and
extracting the dissimilar content.

INTRODUCTION

In order to enable widespread usability for the Semantic
Web there is a need to bootstrap large, rich and up-to-date
domain ontologies that organizes most relevant concepts,
their relationships and instances. In this paper, we present
automated techniques for bootstrapping and populating specialized domain ontologies by organizing and mining a set of
relevant overlapping taxonomy-directed Web sites provided
by the user. A Web site is said to be ”taxonomy-directed” if
it contains at least one taxonomy for organizing its key concepts and it presents the instances belonging to each concept
in a regular fashion. Notice that, neither the presentation of
the taxonomy among different pages, nor the presentation
of instances among for different concepts need to be regular for a Web site to be classified as ”taxonomy-directed”.
Copyright is held by the author/owner(s).
WWW2004, May 17–22, 2004, New York, New York, USA.
ACM 1-58113-912-8/04/0005.

500

Figure 2: (a) A snapshot of New York Times Front Page with logical segmentation illustrated; (b) Output
of the Semantic Partitioner, showing the document object model of the html page and the final hierarchical
tree; (c) A fragment of the mined taxonomy of concepts from a collection of News Web sites.
uses taxonomy mining algorithm on these trees to mine the
taxonomy of important concepts. The taxonomy mining
algorithm uses frequent tree pattern mining techniques to
mine is-a relationships. Figure 2c shows a fragment of the
mined taxonomy from a collection of News Web sites.

4.

Figure 1: Architecture of OntoMiner

3.

CONCLUSIONS AND FUTURE WORK

We present algorithms to logically partition a Web page
and infer the hierarchy among the labels in the Web page,
mine the taxonomy of important concepts from overlapping
domain specific Web sites, extract attributed instances and
their values corresponding to each concept in the taxonomy.
OntoMiner only requires a collection of domain specific sites
in building the taxonomy of concepts as it uses frequency
based techniques to mine the is-a relationships. OntoMiner
can find complex semi-structured instances for attributed
concepts and can extract labels for attributes whenever they
are available. Experimental results for semantic partitioning, taxonomy mining and instance extraction can be found
at http://www.public.asu.edu/∼snagaraj/OntoMiner.
Some of the future directions for our work include investigating techniques that combine syntactic as well as semantic regularitiesto repair hierarchical partitions with bootstrapped ontologies to yield higher precision and recall.

AN ILLUSTRATIVE EXAMPLE

The example shown in Figure 2 demonstrates the working
of the OntoMiner algorithm in several phases. The semantic
partitioning algorithm works with single Web page at a time
to extract hierarchy among the labels in the Web page. It
works by first flat partitioning the Web page into logical
segments as shown in Figure 2a. Next, it uses dynamic
programming techniques to organize the labels in the Web
page into groups that exploit the structural similarities in
the Document Object Model (DOM) tree. Finally it uses
promotion rules that are based on the presentation and the
format of the Web page to promote the emphasized labels
on top of certain groups. Figure 2b shows the segment of
the DOM tree of the Web page and the final hierarchical
tree. OntoMiner employs semantic partitioning algorithm
to obtain hierarchical trees on a collection of Web sites and

5.

REFERENCES

[1] Valter Crescenzi, Giansalvatore Mecca, and Paolo
Merialdo. Roadrunner: Towards automatic data
extraction from large web sites. In Proceedings of 27th
International Conference on Very Large Data Bases,
pages 109–118, 2001.
[2] A. Arasu and H. Garcia-Molina. Extracting structured
data from web pages. In ACM SIGMOD, 2003.
[3] Christina Yip Chung, Michael Gertz, and Neel
Sundaresan. Reverse engineering for web data: From
visual to semantic structures. In Intl. Conf. on Data
Engineering, 2002.

501

Online Information Review
Gathering meta-data and instances from object referral lists on the web
Srinivas Vadrevu, Fatih Gelgi, Saravanakumar Nagarajan, Hasan Davulcu,

Article information:

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

To cite this document:
Srinivas Vadrevu, Fatih Gelgi, Saravanakumar Nagarajan, Hasan Davulcu, (2006) "Gathering meta‐
data and instances from object referral lists on the web", Online Information Review, Vol. 30 Issue: 3,
pp.278-296, https://doi.org/10.1108/14684520610675807
Permanent link to this document:
https://doi.org/10.1108/14684520610675807
Downloaded on: 10 June 2017, At: 17:07 (PT)
References: this document contains references to 32 other documents.
To copy this document: permissions@emeraldinsight.com
The fulltext of this document has been downloaded 304 times since 2006*

Access to this document was granted through an Emerald subscription provided by emerald-srm:352589 []

For Authors
If you would like to write for this, or any other Emerald publication, then please use our Emerald for
Authors service information about how to choose which publication to write for and submission guidelines
are available for all. Please visit www.emeraldinsight.com/authors for more information.

About Emerald www.emeraldinsight.com
Emerald is a global publisher linking research and practice to the benefit of society. The company
manages a portfolio of more than 290 journals and over 2,350 books and book series volumes, as well as
providing an extensive range of online products and additional customer resources and services.
Emerald is both COUNTER 4 and TRANSFER compliant. The organization is a partner of the Committee
on Publication Ethics (COPE) and also works with Portico and the LOCKSS initiative for digital archive
preservation.
*Related content and download information correct at time of download.

The current issue and full text archive of this journal is available at
www.emeraldinsight.com/1468-4527.htm

OIR
30,3

Gathering meta-data and
instances from object referral
lists on the web

278

Srinivas Vadrevu, Fatih Gelgi, Saravanakumar Nagarajan and
Hasan Davulcu
Department of Computer Science and Engineering, Arizona State University,
Tempe, Arizona, USA
Abstract

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

Purpose – The purpose of this research is to automatically separate and extract meta-data and
instance information from various link pages in the web, by utilizing presentation and linkage
regularities on the web.
Design/methodology/approach – Research objectives have been achieved through an information
extraction system called semantic partitioner that automatically organizes the content in each web
page into a hierarchical structure, and an algorithm that interprets and translates these hierarchical
structures into logical statements by distinguishing and representing the meta-data and their
individual data instances.
Findings – Experimental results for the university domain with 12 computer science department
web sites, comprising 361 individual faculty and course home pages indicate that the performance of
the meta-data and instance extraction averages 85, 88 percent F-measure, respectively. Our METEOR
system achieves this performance without any domain specific engineering requirement.
Originality/value – Important contributions of the METEOR system presented in this paper are: it
performs extraction without the assumption that the object instance pages are template-driven; it is
domain independent and does not require any previously engineered domain ontology; and by
interpreting the link pages, it can extract both meta-data, such as concept and attribute names and
their relationships, as well as their instances with high accuracy.
Keywords Worldwide web, Information retrieval, Data handling
Paper type Research paper

Online Information Review
Vol. 30 No. 3, 2006
pp. 278-296
q Emerald Group Publishing Limited
1468-4527
DOI 10.1108/14684520610675807

1. Introduction
Scalable, information retrieval-based search engine technologies have achieved
widespread adoption and commercial success, enabling access to the web (Baeza-Yates
and Ribeiro-Neto, 1999). However, since they are based on an unstructured
representation of web documents, their performance in making sense of the
available information is also limited. Hence, in order to make the web more accessible,
with database style querying and automated reasoning, we need scalable techniques
that can identify various types of entities, their attributes and relationships.
In this paper, we present the METEOR system, which utilizes various presentation
and linkage regularities within web pages (Yang et al., 2003b) from referral lists of
various sorts to automatically extract meta-data and instance information. The
METEOR system differs from earlier approaches (Davulcu et al., 2003a, b, 2005;
Vadrevu et al., 2005a, b) in that it gathers and extracts meta-data and instance
information from object lists on the web; whereas the previous techniques worked with

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

a collection of domain-specific web sites to mine a domain taxonomy and its instances.
The METEOR system is also capable of extracting logical facts from web pages.
Thanks to the HTML format, unlike plain text documents, web pages organize
and present their content within nested hierarchies of HTML structures. In this
paper we present an algorithm that can detect various HTML regularities and
utilize them to structure the web page content into hierarchical group structures,
which contain blocks of regularly presented instances. Our algorithm is based on
the observation that most group instances are usually presented together
consecutively, and are also presented with consistent HTML formatting. Our
algorithm is robust in a well-defined sense – it can accurately identify all group
instances even in the presence of certain irregularities.
Furthermore, many web pages present their information in the form of labeled lists
and tables of various sorts. Consider the first page in the example shown in Figure 1,
which lists the faculty instances in a computer science department. Each of these
faculty instances links to an individual faculty home page that contains detailed
information. We denote these types of groups as object referral lists (ORL). Examples
of ORLs are: faculty listings in universities, job listings in company sites, course
listings in online schools, hotel and hospital listings in directories, etc. Object referral
lists follow a highly regular linkage pattern: first they list their instances under an
informative label such as jobs, faculty, hotels, etc. and then each instance links to an
individual object page. The individual object page presents the detailed attributes of an
object as shown in the second page of Figure 1. Sometimes, the individual object page
may present its attribute information by linking to an object attribute page as shown in
the third page of Figure 1.
The figure shows an individual object page and one of its object attribute pages.
The labels in the individual object page and the object attribute page are marked with
corresponding path identifier symbols.
In this paper, we present algorithms that can interpret any given ORL, navigate to
its instances and extract its attributes and values. For a faculty ORL this algorithm
would typically extract attributes such as publications, students, address and
telephone.

Gathering metadata and
instances
279

Figure 1.
An example of the object
referral list page that links
individual object pages

OIR
30,3

280

Important characteristics of the METEOR approach:
.
It performs extraction without the assumption that the object instance pages are
template-driven. For example, we present experimental results with computer
science faculty home pages.
.
It is domain independent and it does not require previously engineered domain
ontology.
.
By interpreting ORL structures, it can extract both meta-data, such as concept
and attribute names and their relationships, as well as their instances with high
accuracy.

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

The rest of the paper is organized as follows: Section 6 gives an overview of the related
work; Section 3 presents the grouping algorithm for web pages; Section 4 presents the
interpreter algorithm for the ORL structures; Section 5 presents experimental results;
and Section 7 concludes the paper.
2. Background
Information extraction is the process of extracting relevant information from text,
databases, semi-structured and multimedia documents. An information extraction
system performs this extraction process either automatically or with human
intervention by using techniques from various areas like machine learning, data
mining, pattern mining, and grammar induction. Since, the worldwide web is
transforming itself into the largest information source ever available, the process of
information extraction from the web is both a challenging and interesting problem.
An example of an information task is shown in Figure 2. The example shows a
fragment of a web page (Figure 2(a)) from the United States Government Bookstore,
and the pertinent information to be extracted from this web page. The web page
contains three books, with each book having five attributes: title, publisher,
description, year/pages, and price. These attributes can also be referred to as
meta-data, since they describe the data. Of the five meta-data labels, the label title is not
presented in the web page and therefore has to be annotated manually. The data is also
presented showing information about three different books. The goal of an information
extraction system in this task is to be able to extract the data associated with all the
books along with their meta-data labels as shown in Figure 2(b).
The information extraction system described in this paper uses pattern mining
techniques to utilize the regularities within a web page and extract the meta-data labels
and the associated data. It also identifies the boundaries among instances, and extracts
information from each of the individual instances. Pattern mining is the process of
discovering and isolating existing patterns by exposing the regularities. Pattern
mining has been successfully applied to several applications, including analysis of
biological sequence, analysis of web access patterns, consumer shopping data, and
natural distastes data.
3. Semantic partitioning
The semantic partitioner is an information extraction system that infers hierarchical
relationships among the leaf nodes of the DOM (Document Object Model, www.w3.org/
TR/DOM-Level-2-Core/introduction.html) tree of a web page, where all the document

Gathering metadata and
instances

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

281

Figure 2.
An example of an
information extraction
task

content is stored. A DOM tree is a logical tree structure that organizes the HTML code
of a web page into a hierarchical model. Semantic partitioner achieves this through a
sequence of two operations: hierarchical grouping and promotion.
3.1 Hierarchical grouping
During the pre-processing step, the web page is parsed and its DOM tree is generated.
Next, each leaf node is labeled by its attributed root-to-leaf HTML tag path. Each path
is assigned a unique path identifier, thus yielding a sequence of path identifies for each
web page. The hierarchical grouping is based on a regular pattern mining algorithm
that yields a hierarchy of groups (G) and their instances (I). The hierarchical grouping
algorithm in Algorithm 1 (Figure 3) identifies all the consecutive group instances and
their boundaries. The algorithm works by detecting approximate repeating substrings
of a sequence.

OIR
30,3

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

282

Figure 3.
Algorithm 1 –
Hierarchical grouping
algorithm

Our approach is based on a matching criterion that identifies groupings of repeating
patterns, which requires that for each matching item there should be another
compatible matching item within the tandem repeat sequence. It finds the approximate
tandem repeats by maximizing the pair-wise compatibility among the patterns that
contribute to the tandem repeat. The standard algorithms to identify repeating
instances within sequences either require large sets of labeled examples (Berwick and
Pilato, 1987) or their assumptions are too limiting for the web page instances. For
example, the k-mismatch tandem repeat algorithm (Gusfield, 1997) requires a
pre-estimate of k for each grouping in the web pages. Since, the seminal work of Gold
(1978) showed that the problem of inferring a deterministic finite automata (DFA) of
minimum size from positive examples is NP-complete, we have chosen to develop a set
of web page independent assumptions that hold for a large variety of groups and their
instances on the web. Next, we present our assumptions and a grouping algorithm that
correctly identifies all ideal groupings from each web page automatically, whenever
our assumptions hold.
We use the individual object page shown in Figure 1 to demonstrate the inner
workings of our hierarchical grouping algorithm. The path sequence corresponding to
that example as marked in the Figure is aaabcdefeffghijhikhijhilhijhmnnonmpnnqrq. In

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

this path sequence, the substrings aaa, bcdefeff, ghijhikhijhilhijh, mnnonmpnnqrq
correspond to the navigation bar, affiliations, publications, and the contact
information, respectively.
In order to state our assumptions, we need the following definitions:
(1) Atom. Given a sequence and a header identifier, an atom is defined as any
substring that starts with an occurrence of a header and extends until its next
occurrence. In the above example, given a header symbol a, the sequence aaa
yields the atoms a, a and a. Similarly, given header symbol h, the substring
hijhikhijhilhijh yields the atoms hij, hik, hij, hil, hij, h.
(2) Atom compatibility. Two atoms A and A0 are atom compatible, denoted by
A ; A0 if their pattern signature aligns with only insertions from either one to
the other. The pattern signature of atoms is computed as regular expressions in
a bottom-up fashion using our algorithm. For example, the atoms for the path
sequence abcbgcbgcdabcbhcbcdabcd for the header a are abcbgcbgcd, abcbhcbcd,
abcd and their corresponding pattern signatures would be abc(bgc) *d,
a(bc) *bhcd and abcd. Hence, the first and second atoms are compatible with
the third atom. However, the first and second atoms are not compatible.
(3) Instance. An instance is defined as a sequence of one or more contiguous atoms.
(4) Instance compatibility. An instance I is compatible with another instance I0
denoted by I ; I0 if the following three conditions are satisfied:
(1) The total number of atoms in both instances I and I0 are equal (’ [ ;).
(2) ;Ai [ I, ’Ai0 [ I such that Ai ; Ai0.
(3) The alignments of corresponding Ai and Ai0 should be consistent in that
either atoms of I insert into corresponding atoms of I0 or vice versa. For
example, the instances hijhik and hijhil are not instance compatible since
their second atoms hik and hil are not atom compatible. However, the
instances abcbgcbgcd, abcbhcbcd are both instance compatible with abcd.
Now we can state our assumptions as follows:
.
Instance header assumption. All instances of a group and their sub-group
instances always begin with the same header. For the example mentioned above,
the header h corresponding to the instances hijhik\hijhil\hijh for the substring
hijhikhijhilhijh satisfies our assumption.
.
Group assumption. A group is defined as a sequence of two or more contiguous
instances such that ;I [ G, ’I0 [ G, such that I ; I0 . The two possible
groupings for the substring hijhikhijhilhijh are ijhikhijhilhijh and hijhikhijhilhijh.
Our group assumption relies on the intuition that a sequence of mismatching instances
can still be made into a group, if for each instance there exists another compatible
instance anywhere in the group that can act as its witness. Hence, our group definition
is a robust one in that it can accurately identify all group instances even in the presence
of irregularities, whenever each type of irregular instance is witnessed by at least
another instance.

Gathering metadata and
instances
283

OIR
30,3

284

Since, any pair-wise compatible atom grouping constitutes a grouping, the
following measure enables our algorithm to prefer larger multi-atom instances over
simple atom groupings:
.
Ideal grouping. A grouping is said to be an ideal grouping if it maximizes the
pair-wise compatibility among its instances after all the pair-wise
incompatibilities are accounted for. The degree of compatibility for a group is
defined as the difference between the total number of pair-wise compatible and
incompatible instances.
X
j{ðI i ; I j ÞjI i ; I j } 2 j{I i ; I j ÞjI i – I j }j
PairwiseCompatibilityðGÞ ¼
I i ;I j [G;i–j

For example, the pair-wise compatibilities for some of the possible groupings of the
sequence hijhikhijhilhijh are:
Downloaded by ASU Library At 17:07 10 June 2017 (PT)

PairwiseCompatibilityðhijhikjhijhiljhijhÞ ¼ 2
and
PairwiseCompatibilityðhijjhikjhijjhiljhijhÞ ¼ 24:
Hence, the first grouping is preferred, which identifies the correct instances of
publications in the individual object page in Figure 1.
3.2 Demonstration of the algorithm with an example
In this section, we illustrate the inner-workings of Algorithm 1 using the sequence
aaabcdefeffghijhikhijhilhijhmnnonmpnnqrq from the individual object page in Figure 1,
and briefly explain the process.
The HierarchicalGrouping algorithm attempts to standardize the path sequence as a
regular expression, which can be used to parse the original sequence into hierarchical
group structures (shown in Figure 4). It utilizes the maximize subroutine that finds the
maximum number of consecutive atoms that can be appended to the current atom. It
also marks the candidate atom as an instance boundary, as long as another compatible
instance exists starting with any of the other compatible atoms.
For example, initially the subpattern (a) * is computed using the maximize
subroutine by recursively invoking the HierarchicalGrouping algorithm. This pattern

Figure 4.
The group structures for
the individual object page
and the object attribute
page in Figure 1

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

corresponds to the regularly presented structure: the navigation bar of the individual
object page shown as G1 in Figure 4. Then the algorithm continues to find the
subpatterns bcd(e( f ) *) * g(hij(hikjhiljh)) * that corresponds to the affiliations and the
publications presented in the page shown as G2 and G3, and appends to the previous
subpattern (a) *. The algorithm resumes from m until the end of the string and
generates the final pattern. The algorithm eventually generates nested group
structures shown in Figure 4 from the final pattern. The complexity of the algorithm is
O(n 3), where n is the length of the input string.
3.3 Promotion
The final step in semantic partitioning is promotion. After hierarchical grouping, all
the content of the web page is still at the leaf nodes of the hierarchical group tree, hence
promotion of some of the leaf nodes is necessary in order to organize them into a
semantic hierarchy. The promotion algorithm identifies those leaf nodes that should be
promoted above their siblings. A label is promoted if it satisfies one of the following
rules:
.
a (sub)group structure can be labeled with its nearest preceding emphasized node
if there is no other (sub)group structure between them; and
.
a value instance can be labeled with its previous emphasized node.
A node label is said to be emphasized if it satisfies one of the following conditions:
.
its labeled text is fully capitalized; or
.
there is a bold tag (such as , b. , bold . , h1. etc.) on its DOM tree
tag-path; or
.
its immediate consecutive group structure corresponds to an HTML list
structure ( , ul . or , ol . ).
In the individual object page in Figure 1, the nodes satisfying the emphasized condition
are Daniel Moore, publications, address, and telephone. Since, all of them satisfy the
pre-conditions for promotion, they are all promoted as group headers as shown Figure 4.
Once the hierarchical grouping and promotion steps are completed, we obtain the
final semantic partitioning for the given web page. In the next section, we present an
algorithm to separate and extract meta-data and instance information from such
semantic structures.
4. Interpreting the semantic structures
The semantic partitioning algorithm presented in Section 3 utilizes presentation
regularities encoded with the HTML markup of the web pages to transform them into
hierarchical group structures and identify their instances. In this section, we describe
how these semantic structures can be interpreted by utilizing linkage regularities that
exist within the context of an ORL, in order to separate and extract their meta-data and
instances.
4.1 Context management
In order to devise an accurate procedure to separate and extract the meta-data and
instance information from group structures originating from ORLs, we need to identify

Gathering metadata and
instances
285

OIR
30,3

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

286

and manage certain context information for each group. For a group structure G, we
formally define the context C as a 4-tuple, C ¼ , StrType, ValueTypes, GrpConcept,
GrpObject . where StrType is the structural type of the group; ValueTypes denotes
the set of ranges of the instances of the group; GrpConcept provides the conceptual
context for interpreting the group; and whenever present, the GrpObject provides the
object identifier for extracting attribute information from the group. In the following
section, we will present an algorithm for managing the group context.
4.2 Structural type of a group
The structural type of a group is defined to be one of the following three types: (we will
be using examples from Figure 4, which shows the corresponding group structures
extracted from the individual object and object attribute pages shown in Figure 1.)
(1) Singleton group. Whenever each instance of the group contains a single value,
the group is the singleton type. The group structure G1 in Figure 4 is a singleton
group.
(2) Simple group. Whenever the group has an instance with more than one value,
the group is the simple type. The group structure G3 in Figure 4 is a simple
group.
(3) A complex group. Any group structure with a subgroup is the complex type.
The group structure G6 in the object attribute page in Figure 4 is a complex
group.
In the following sections, we describe how to retrieve and organize the value types, and
provide an algorithm that can infer meta-data and their instances from an ORL.
In the individual object page, the groups G1, G2, G3, G4, G5 correspond to the
navigation bar, affiliations, publications, address and the telephone information,
respectively. In the object attribute page, the group G6 corresponds to the courses,
where each instance corresponds to separate courses, and the leaf nodes correspond to
the course semesters.
4.3 Extracting and classifying the value types
The second component of a group context is the collection of its atomic value types.
The atomic value types of a group may denote attribute names, attribute ranges,
sub-concepts of a concept, or member objects of a concept. For example, the simple
group structure G3 of the object page in Figure 3 contains different types of values that
correspond to the ranges of various attributes of a publication, such as its author, title,
conference, page number and location. Next, we present retrieval mechanisms for
populating the group value types and techniques for classifying them as attribute
range, objects, attribute names or concepts.
Retrieval of value types. Value types are retrieved from the atomic leaf nodes of the
group structures by utilizing a segment of their HTML path information. Let G be a
group structure and vi be a value that belongs to an instance I of G. Let L be the lowest
common ancestor of all of the values of G in the corresponding HTML tree. For each vi
its path index pi is defined to be its order-indexed HTML tag path from L to vi. The
value types are then obtained by grouping all atomic values, vi’s of a group structure,
by their corresponding path indexes. Figure 5 shows some of the value types obtained
from groups G1 and G3 of the individual object page of Figure 1.

Gathering metadata and
instances

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

287

Figure 5.
Extraction of value types
from group structures

Classification of value types. The value types are classified into three different
categories based on their linkage information. The category of a value type is
determined by the majority of their members’ link status (whether they are HTML
links or not) and the role they play in their corresponding target pages.
(1) Attribute range type. If there is no link associated with any of the values, then
the value type is classified as attribute range. For example, the Value Type3
and Value Type5 in Figure 5 are ranges of a yet unknown attribute.
(2) Meta type. If the majority of the values of a type are links, and they are headers
of group structures in their target pages, then the value type is classified as a
meta type. A meta-typed value type will be interpreted as either a collection of
attribute names or as sub-concepts of another concept, depending on their
context. For example, the Value Type1 in Figure 5 is classified as meta type
and, upon interpretation, its instances turn out to be some of the attributes of the
faculty concept.
(3) Object type. If the majority of values of a type are links and they are not headers
of any group structures in their target pages, then the value type is classified as
an object type. For example, both Value Type2 and Value Type4 in Figure 5 are
classified as object type.
4.4 The semantic structure interpreter
This section provides an algorithm to interpret any given group structure originating
from an ORL. For convenience, the extracted meta-data and object instances are
represented as F-logic facts (Kifer et al., 1995). F-logic provides a means to represent
both the meta-data information and objects in a concise way, and it allows reasoning
with rules (Yang and Kifer, 2000; Yang et al., 2003a). It is also very easy to transform

OIR
30,3

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

288

extracted F-logic facts into the semantic web standard RDF and RDF-S to enable
sharing. The following discussion briefly introduces the subset of the F-logic syntax
that is used in our algorithm.
The F-logic alphabet that we use consists of the sets F (function symbols), and C
(constants). First-order terms over F and C are called Id-terms, and are used to name
objects, methods, and classes. Ground Id-terms (i.e. terms with no variables)
correspond to logical object identifiers O, also called object names. An atomic formula
in F-logic can be one of the following types:
.
If for an object identifier, O, an attribute, A yields a set of objects or constant
symbols {V 1; . . .Vn}; then the formula O½A – .. {V 1; . . . ; Vn} is used to
express such a relationship. For example, john[children – . . {bob, mary}].
.
The formula C[A ¼ ¼ gt; . T ] denotes that the attribute A, when applied to
objects that belong to class C, must yield objects that belong to class T. For
example, person[children ¼ ¼ . . person ].
.
The instance-of relationship between an object O and a class C is represented by
O: C and subclass relationships between two classes C and D is represented by
C < D. For example, john: person and man < person.
The algorithm for interpreting the group structures originating from an ORL is
presented in Algorithm 2 (Figure 6). We will illustrate this algorithm by using the
example in Figure 1. The algorithm is initially provided with a group structure G that
corresponds to an ORL, and its context is initialized to be C ¼ , StrType(G),
ValueTypes(G), Header(G), none . . The algorithm interprets the current ORL group
and computes the corresponding context for all of its instances. It then navigates to the
individual object pages and potentially makes another access to their object attribute
pages to extract all additional attribute and value type information stored in their
corresponding group structures.
In the example in Figure 1, the ORL group structure G lists the instances of the
regular faculty concept. The GroupStructureInterpreter is invoked by G and the initial
context C ¼ , Simple, ValueType(G), ‘Regular Faculty’ none . . The following F-logic
statements are extracted from the ORL group G:
‘Regular Faculty’: concept.
‘Alex Aiken’: ‘Regular Faculty’.
‘Daniel Moore’: ‘Regular Faculty’ . . .
‘Regular Faculty’[‘Name’ ¼ ¼ . . {‘Alex Aiken’ ‘Daniel Moore’ . . .}].
‘Alex Aiken’[‘Name’ – . . ‘Alex Aiken’].
‘Regular Faculty’[‘Phone’ ¼ ¼ . . {‘5-3359’ ‘3-3334’ . . .}].
‘Alex Aiken’[‘Phone’ – . . ‘5-3359’] . . .
Lines 4-6 of the ValueTypeInterpreter subroutine recursively invoke the
GroupStructureInterpreter for each group structure G0 within the individual object
pages with the appropriate context. Interpreting the group instances of the individual
object page for the DanielMoore object using the context C0 ¼ , StrType(G0 ),
ValueType(G 0 ), ‘Regular Faculty’ ‘Daniel Moore’ . yields the following facts:
‘Regular Faculty’[‘Publications’.Attr1 ¼ ¼ . . vt2].

Gathering metadata and
instances

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

289

Figure 6.
Algorithm 2 – Infering
meta-data and its
instances from an ORL

‘Daniel Moore’[‘Publications’.Attr1 – . . vt2].
vt2:valuetype[values – . . {‘Survey on Semantic Web Services’ ‘An efficient
way for providing Web services’ ‘Semantic Tagging of elements in Web
sources’}].
‘Regular Faculty’[‘Publications’.Attr2 ¼ ¼ . . vt3].
‘Daniel Moore’[‘Publications’.Attr2 – . . vt3].
vt3:valuetype[values – . . {‘Proceedings of SIGMOD 2004’ ‘Proceedings of
WWW 2004’ ‘Proceedings of APWeb 2003’}]. . .
Whenever the object instance pages present their attributes using a link group, lines
21-24 of the ValueTypeInterpreter subroutine recursively invokes the

OIR
30,3

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

290

GroupStructureInterpreter for each group structure G00 within the object attribute
pages with the appropriate context. Interpreting the group instances of the object
attribute page for the DanielMoore object using the context C00 ¼ , StrType(G00 ),
ValueType(G00 ), ‘Regular Faculty’ ‘Daniel Moore’ . yields the following facts:
‘Regular Faculty’[‘Courses’.Attr1 ¼ ¼ . . vt4].
‘Daniel Moore’[‘Courses’.Attr1 – . . vt4].
vt4:valuetype[values – . . {‘Ten ideas in data mining and machine learning’
‘Semantic Web mining’ ‘Design and Analysis of Algorithms’ ‘Introduction to
Data Mining’ ‘Randomized Approximation Algorithms’}].
‘Regular Faculty’[‘Courses’.‘CSE421’ ¼ ¼ . . vt5].
‘Daniel Moore’[‘Courses’.‘CSE421’ – . . vt5].
vt5:valuetype[values – . . {‘Fall 03’ ‘Fall 99’}]. . .
5. Experimental results
In this section, we present the experimental results for the semantic partitioning and
the ORL interpretation algorithms.
5.1 Experimental setup
The data set used to test our algorithms consists of ORL pages for the faculty and
course domains in 12 computer science department web sites, comprising 228
individual faculty and 133 individual course pages. The faculty and course home pages
vary greatly in presentation and layout of content, and it was easy for us to verify the
performance of the algorithms for these domains. For each web site, we identified the
corresponding faculty and course ORL pages that present link groups that link to
individual faculty and course home pages in the university. We provide the
experimental results for our algorithms on the ORL pages themselves, individual
object pages and their object attribute pages.
5.2 Evaluation
Table I presents experimental results for the semantic partitioner, and the
interpretation algorithm for separating and extracting the meta-data and the value
types for faculty and course collections. The results are produced for all web pages
reachable from the ORLs. For each web site, a number of pages were sampled and used
to compute the precision, recall and F-measure values by comparing the obtained
results to the gold-standard data, which was obtained manually by inspecting each
web page.
The precision and recall of the HierarchicalGrouping algorithm for a given web
page is calculated by comparing the transitive closure of parent-child relationships
inferred by the algorithmically generated hierarchies, with those implied by the
gold-standard hierarchies. The gold-standard hierarchy (for each page) and taxonomy
was created manually by an evaluator, and then the transitive closure of all
parent-child relationships was materialized. The precision and recall is calculated by
the following formulas:
Precision; P ¼

{T} > {T 0 }
;
j{T}j

Recall; R ¼

{T} > {T‘}
{T‘}

Courses

Faculty

cs.wisc.edu/
cs.cmu.edu/
cs.unc.edu/
cs.umd.edu/
cs.Stanford.edu/
cs.Washington.edu/
Average
cs.wisc.edu/
cs.uiuc.edu/
eecs.mit.edu/
cs.bu.edu/
cs.princeton.edu/
cs.rpi.edu/
Average

Category Web site

26
41
12
23
18
13

32
33
41
44
38
40

No of
pages
88
65
100
86
86
68
83
73
87
91
69
74
83
80

83
98
71
68
80
77
75
93
76
88
67
82
77
81

85
78
83
76
83
72
80
82
86
82
71
78
80
80

Semantic partitioner
P
R
F
(percent) (percent) (percent)
71
89
94
100
92
73
87
91
89
100
93
100
71
91

80
79
96
89
75
83
84
87
84
60
78
85
71
78

74
81
94
85
81
77
84
89
91
75
85
92
83
86

Meta-data extraction
P
R
F
(percent) (percent) (percent)

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

83
79
90
91
85
82
85
91
84
83
85
88
83
86

95
77
100
97
97
90
93
80
91
100
85
100
83
90

88
76
94
94
87
85
87
85
88
90
85
94
83
88

Value types extraction
P
R
F
(percent) (percent) (percent)

Gathering metadata and
instances
291

Table I.
Experimental results for
semantic partitioner,
meta-data extraction, and
value types extraction for
faculty and courses
domains from computer
science department
web sites

OIR
30,3

292

where T and T 0 are the sets of transitive closure of parent-child relationships implied
by the algorithmically generated and gold-standard hierarchies, respectively.
For a given page, the precision and recall values for meta-data and instance
extraction algorithms are calculated as:
Precision; P ¼

TP
;
TP þ FP

Recall; R ¼

TP
;
TP þ FN

where TP is the number of correctly extracted meta-data labels or value type sets; FN
is the number of meta-data labels or value types that were missed; and FP is the
number of meta-data labels or value type sets that were mis-tagged. The F-measure
value is computed as:

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

F 2 Measure ¼

2PR
;
P þR

where P is the precision and R is the recall.
5.3 Discussion
It can be observed from the experimental results that the performance of the semantic
partitioner, meta-data and value types extraction algorithms averaged 80, 84, and 87
percent F-measure for faculty domain; and 80, 86, and 88 percent F-measure for the
course domain, respectively. The semantic partitioner algorithm was able to perform
well on many individual object pages where there was regularity in the presentation
and layout of the content. However, the algorithm is still sensitive to certain types of
irregularities in the data sources. Many of the object pages on the web do not explicitly
provide the attribute label information. In such cases, the semantic partitioner
algorithm would still be able to find the repetitive instances of the group, but would not
be able to find any label to promote as the group header. Also, whenever there are
single valued attributes without any repetitive structure, the algorithm might not be
able to identify and extract their information.
On the other hand, the interpretation algorithm was able to perform with high
accuracy whenever the corresponding semantic partitioner algorithm was successfully
able to identify the appropriate group structures along with their header labels. In our
collection, as the content in course home pages was more regularly presented than the
faculty home pages, the results were slightly better for the course domain. Overall, the
value type extraction performed better than the meta-data extraction, since the
attribute labels were either nonexistent in the pages or were not promoted. In general,
the METEOR system was able to extract the meta-data and their instances from ORLs
whenever the content was regularly presented.
6. Related work
Information extraction from the web is a well-studied problem, and related work can be
categorized as:
.
Wrapper development tools. Wrappers (Hammer et al., 1997; Arocena and
Mendelzon, 1998) are scripts that are created either manually or
semi-automatically after analyzing the location of the data in the HTML

.

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

.

.

.

.

pages. Wrappers tend to be brittle against variations and require maintenance
when the underlying web sites change.
Semi-automated wrapper learning. Wrapper induction systems (Kushmerick
et al., 1997; Muslea et al., 1999) generate extraction rules from a given set of
labeled training examples. These extraction rules can be applied on other new
pages to extract the data. These systems utilize the natural language processing
techniques and HTML tags to infer extraction patterns. Our METEOR approach
does not require any training examples when performing extraction.
Template based automated algorithms. RoadRunner (Crescenzi et al., 2001) works
with two documents from a collection of template generated web pages to infer a
template for the collection using union-free regular expressions. ExAlg (Arasu
and Garcia-Molina, 2003) is another system that can extract data from template
generated web pages. ExAlg uses equivalence classes (sets of items that occur
with the same frequency in every page) to build the template for the pages by
recursively constructing the page template, starting from the root equivalence
class. Table segmentation work presented in Lerman et al. (2004) proposes
methods to efficiently segment HTML tables and lists into records by mining the
page templates. Our work differs from such approaches as it performs extraction
without the assumption that the object instance pages should be template-driven.
Ontology based approaches. Ontology based approaches (Embley et al., 1998; Dill
et al., 2003; Hong and Clark, 2001) require fully developed domain ontology, and
utilize matching and disambiguation algorithms to locate and extract the entities
and relationships of interest. On the other hand, KnowItAll system (Etzioni et al.,
2004) requires only a set of class and relation names, and a small set of generic
rule templates with an assessor algorithm to extract and rank facts from free text
segments. Our METEOR approach is domain independent and does not require
previously engineered domain ontology.
Grammar induction based algorithms. Grammar induction based systems
employ a strong bias on the type and expected presentation of items within web
pages to extract instances. Such assumptions like “product descriptions should
reside on a single line” (Doorenbos et al., 1997) and “items may not have missing
or repeating attributes” (Yang et al., 2001; Doorenbos et al., 1997) do not apply for
most of the web sites.
Hierarchical structuring based approaches. The approaches proposed by Yang
and Zhang (2001) and Chung et al. (2002) are similar to our approach. However,
Yang and Zhang (2001) view HTML as a sequence of HTML tags and text, and
build semantic structures by detecting maximal patterns that identify the
boundaries between various logical sections of a web page. They utilize a
distance function, which measures the similarity between various sequences of
HTML tag paths to discover the recurring patterns in the document. This
similarity function must be hand-coded for different web pages for various
domains of interest. Also, they do not address the problem of labeling the
semantic groups in the document. Their goal is similar to ours; however, they
focus on topic specific HTML documents from a domain of interest, whereas our
approach does not require any tuning or domain specific knowledge.

Gathering metadata and
instances
293

OIR
30,3

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

294

.

Another category of ontology mining work (Rilo and Shepherd, 1997; Maedche
and Staab, 2000; Navigli et al., 2003) uses free-text corpus and various natural
language processing and data mining techniques to bootstrap ontologies.
However, the METEOR system utilizes only presentation regularities of
semi-structured data embedded within web documents to organize them into a
partial bootstrapped ontology. Our system detects free-text HTML leaf nodes
and does not process them any further.

7. Conclusions and future work
In this paper, we presented the METEOR system, which can automatically separate
and extract meta-data and instance information from object referral lists. The
METEOR system can extract the meta-data and its instances from ORLs whenever the
content is regularly presented. The experimental results with the faculty and course
domains from 12 computer science department web sites, indicate that our algorithms
were able to extract the meta-data and instance information with high accuracy.
In our future work, we propose to develop automated algorithms for finding all ORL
structures within any web site. We plan to investigate the utility of the METEOR
system itself in extracting meta-data information for each group structure on the web,
and use a clustering-based algorithm to identify those group structures with highly
overlapping meta-data as ORLs. We believe that such a system would identify any
referred object on the web, and its significant category and attribute-value information.
Such a repository would be a valuable resource for scalable ad hoc querying,
integration and mediation over the web.
References
Arasu, A. and Garcia-Molina, H. (2003), “Extracting structured data from web pages”, paper
presented at ACM SIGMOD Conference on Management of Data, San Diego, CA.
Arocena, G.O. and Mendelzon, A.O. (1998), “Weboql: restructuring documents, databases, and
webs”, paper presented at International Conference on Data Engineering.
Baeza-Yates, R. and Ribeiro-Neto, B. (1999), Modern Information Retrieval, Addison-Wesley,
Reading, MA.
Berwick, R.C. and Pilato, S. (1987), “Learning syntax by automata induction”, Machine Learning,
Vol. 2, pp. 9-38.
Crescenzi, V., Mecca, G. and Merialdo, P. (2001), “Roadrunner: towards automatic data extraction
from large web sites”, Proceedings of 27th International Conference on Very Large Data
Bases, pp. 109-18.
Chung, C.Y., Gertz, M. and Sundaresan, N. (2002), “Reverse engineering for web data: from visual
to semantic structures”, ICDE.
Davulcu, H., Vadrevu, S. and Nagarajan, S. (2003a), “OntoMiner: bootstrapping and populating
ontologies from domain specific web sites”, paper presented at First International
Workshop on Semantic Web and Databases, Berlin, September.
Davulcu, H., Vadrevu, S. and Nagarajan, S. (2003b), “OntoMiner: bootstrapping and populating
ontologies from domain specific web sites”, IEEE Intelligent Systems, Vol. 18 No. 5.
Davulcu, H., Vadrevu, S. and Nagarajan, S. (2005), “OntoMiner: automated metadata and
instance extraction from news websites”, International Journal of Web and Grid Services,
Vol. 1 No. 2, pp. 196-221.

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

Dill, S., Eiron, N., Gibson, D., Gruhl, D., Guha, R., Jhingran, T.A., Rajagopalan, S., Tomkins, A.
and Tomlin, J.A. (2003), “Semtag and seeker: bootstrapping the semantic web via
automated semantic annotation”, paper presented at WWW Conference.
Doorenbos, R.B., Etzioni, O. and Weld, D.S. (1997), “A scalable comparison-shopping agent for
the world-wide web”, in Johnson, W.L. and Hayes-Roth, B. (Eds), Proceedings of the First
International Conference on Autonomous Agents (Agents’97), ACM Press, Marina del
Rey, CA, USA.
Embley, D.W., Campbell, D.M., Smith, R.D. and Liddle, S.W. (1998), “Ontology-based extraction
and structuring of information from data-rich unstructured documents”, paper presented
at Intl. Conference on Knowledge Management.
Etzioni, O., Cafarella, M., Downey, D., Kok, S., Popescu, A., Shaked, S.T., Weld, D.S. and Yates, A.
(2004), “Web-scale information extraction in knowitall (preliminary results)”, paper
presented at WWW Conference.
Gold, E.M. (1978), “Complexity of automaton identification from given sets”, Information and
Control, Vol. 37, pp. 302-20.
Gusfield, D. (1997), Algorithms on Strings, Tree, and Sequences, Cambridge University Press,
Cambridge.
Hammer, J., Garcia-Molina, H., Nestorov, S., Yerneni, R., Breunig, M.M. and Vassalos, V. (1997),
“Template-based wrappers in the tsimmis system”, paper presented at ACM SIGMOD
Conference on Management of Data.
Hong, T.W. and Clark, K.L. (2001), “Using grammatical inference to automate information
extraction from the web”, Principles of Knowledge Discovery and Data Mining.
Kifer, M., Lausen, G. and Wu, J. (1995), “Logical foundations of object-oriented and frame-based
languages”, Journal of the ACM, Vol. 42 No. 4, pp. 741-843.
Kushmerick, N., Weld, D.S. and Doorenbos, R.B. (1997), “Wrapper induction for information
extraction”, paper presented at International Joint Conference on Artificial Intelligence,
pp. 729-37.
Lerman, K., Getoor, L., Minton, S. and Knoblock, C. (2004), “Using the structure of web sites for
automatic segmentation of tables”, paper presented at ACM SIGMOD Conference on
Management of Data.
Maedche, A. and Staab, S. (2000), “Discovering conceptual relations from text”, Proceedings of
ECAI-00, IOS Press, Amsterdam, pp. 321-5.
Muslea, I., Minton, S. and Knoblock, C. (1999), “A hierarchical approach to wrapper induction”,
Proceedings of the Third International Conference on Autonomous Agents (Agents’99),
pp. 190-7.
Navigli, R., Velardi, P. and Gangemi, A. (2003), “Ontology learning and its application to
automated terminology translation”, IEEE Intelligent Systems, Vol. 18 No. 1.
Rilo, E. and Shepherd, J. (1997), “A corpus-based approach for building semantic lexicons”, in
Cardie, C. and Weischedel, R. (Eds), Proceedings of the 2nd Conference on Empirical
Methods in Natural Language Processing, Association for Computational Linguistics,
Somerset, NJ.
Vadrevu, S., Gelgi, F. and Davulcu, H. (2005a), Semantic partitioning web pages, paper presented
at the 6th International Conference on Web Information Systems Engineering (WISE).
Vadrevu, S., Nagarajan, S., Gelgi, F. and Davulcu, H. (2005b), “Automated metadata and instance
extraction from news web sites”, paper presented at The IEEE/WIC/ACM International
Conference on Web Intelligence, September.

Gathering metadata and
instances
295

OIR
30,3

Downloaded by ASU Library At 17:07 10 June 2017 (PT)

296

Yang, G. and Kifer, M. (2000), “Flora: implementing an efficient dood system using a tabling logic
engine”, paper presented at International Conference on Deductive and Object-Oriented
Databases, London.
Yang, G., Kifer, M. and Zhao, C. (2003a), Flora-2: A Rule-Based Knowledge Representation and
Inference Infrastructure for the Semantic Web, Catania, Sicily.
Yang, G., Tan, W., Mukherjee, S., Ramakrishnan, I.V. and Davulcu, H. (2003b), “On the power of
semantic partitioning of web documents”, paper presented at the Workshop on
Information Integration on the Web, Acapulco, Mexico.
Yang, J., Seo, H. and Choi, J. (2001), “MORPHEUS: a customized comparison-shopping agent”,
paper presented at 5th International Conference on Autonomous Agents (Agents-2001),
Montreal, Canada, pp. 63-4.
Yang, Y. and Zhang, H. (2001), “Html page analysis based on visual cues”, ICDAR.
Further reading
Manola, F. and Miller, E. (2004), Rdf Primer, W3C Recommendation, available at: www.w3.org/
TR/rdf-primer/
Corresponding author
Srinivas Vadrevu can be contacted at: svadrevu@asu.edu

To purchase reprints of this article please e-mail: reprints@emeraldinsight.com
Or visit our web site for further details: www.emeraldinsight.com/reprints

IEEE International Conference on Bioinformatics and Biomedicine

Extracting Protein-Protein Interactions from MEDLINE Using Syntactic
Roles
Syed Toufeeq Ahmed, Hasan Davulcu, Chitta Baral.
School of Computing and Informatics
Arizona State University
Tempe, AZ 85287
{toufeeq, hdavulcu, chitta}@asu.edu
Abstract

and Romacker 2002) and then to finally recognize the
relationship between interaction events.

With rapid growth in genomics research in last
decade, amount of information a biomedical
researcher has to keep track of and understand has
increased tremendously. We present a fully automated
information extraction system to aid these researchers
to identify and locate gene and protein interactions in
biomedical text. Our extraction system handles
complex sentences and extracts multiple and nested
interactions specified in these sentences. Experimental
evaluations with two other state of the art extraction
systems indicate that the IntEx system achieves better
performance without the labor intensive pattern
engineering requirement.

We present a fully automated extraction approach to
identify gene and protein interactions in natural
language text with the help of biomedical and
linguistic ontology. The novel aspects of our system
are its ability to handle complex sentence structures
and to extract multiple and nested interactions
specified in a sentence.

2. Related Work
Information extraction is the extraction of salient
facts about pre-specified types of events, entities
(Bunescu, Ge et al. 2003) or relationships from free
text. Information extraction from free-text utilizes
shallow-parsing techniques (Daelemans, Buchholz et
al. 1999 ), noun and verb phrase chunking (Mikheev
and Finch 1997), verb subject and object relationships
(Daelemans, Buchholz et al. 1999), and learned (Califf
and Mooney 1998) or hand-build patterns to automate
the creation of specialized databases.

1. Introduction
Genomic research in the last decade has resulted in
the production of a large amount of textual data in the
form of micro-array experiments, sequence
information and publications discussing the
discoveries. Though scientists in the field are aided by
many online databases of biochemical interactions,
currently a majority of these are curated labor
intensively by domain experts. Information extraction
from text has therefore been pursued actively as an
attempt to extract knowledge from published material
and to speed up the curation process significantly.

Manual pattern engineering approaches employ
shallow parsing with patterns to extract the
interactions. The SUISEKI system of Blaschke
(Blaschke, Andrade et al. 1999) also uses regular
expressions, with probabilities that reflect the
experimental accuracy of each pattern to extract
interactions into predefined frame structures. GENIES
(Friedman, Kra et al. 2001) utilizes a grammar based
NLP engine for information extraction. It has been
extended as GeneWays (Rzhetsky, Iossifov et al.
2004). The BioRAT system (Corney, Buxton et al.
2004) uses manually engineered
templates that
combine lexical and semantic information to identify

In the biomedical context, the first step towards
information extraction is to recognize the names of
proteins (Fukuda, Tsunoda et al. 1998), genes, drugs,
diseases and other molecules. The next step is to
recognize interaction events between such entities
(Blaschke, Andrade et al. 1999; Hunter 2000; Hahn

978-0-7695-3452-7/08 $25.00 © 2008 IEEE
DOI 10.1109/BIBM.2008.62

473

Figure 1: Syntactic Role based Extraction
protein interactions. The GeneScene system(Leroy,
Chen et al. 2003) extracts interactions using frequent
preposition-based templates.

that match a biologically significant action between
two gene/protein names are labeled as ‘interaction
words’. Our gazetteer for interaction words is derived
from UMLS and WordNet4.

Grammar engineering approaches, on the other hand
use manually generated specialized grammar rules
(Rinaldi, Schneider et al. 2004) that perform a deep
parse of the sentences. The PathwayAssist system uses
an NLP system, MedScan (Novichkova, Egorov et al.
2003). Recently, extraction systems have also used link
grammar to identify interactions between proteins
(Ding, Berleant et al. 2003).

3.3. Link Grammar and the Link grammar
parser
Link grammar (LG) introduced by Sleator and
Temperley (Sleator and Temperley 1991) is a
dependency based grammatical system. Fig.2 below
shows the parse for the sentence "The dog chased a
cat".

3. Syntactic Role based Extraction
3.1. Pronoun Resolution
Interactions are often specified through pronominal
references to entities in the discourse, or through co
references where, a number of phrases are used to refer
to the same entity. Our pronoun resolution module
identifies the noun phrases referred by the pronouns in
a sentence based on the number of the pronoun
(singular or plural) and the proximity of the noun
phrase. The first noun phrase that matches the number
of the pronoun is considered as the referred phrase.

Figure 2. Link grammar representation of a
sentence
Our algorithms utilize this property of LG where
certain link types allow us to extract the constituents of
sentences irrespective of the tense.

4. Interaction Extraction

3.2. Entity Tagging

Interaction Extractor (IE) extracts interactions from
simple sentence clauses produced by the complex
sentence processor. The highly technical terminology
and the complex grammatical constructs that are
present in the biomedical abstracts make the extraction
task difficult, Even a simple sentence with a single
verb can contain multiple and/or nested interactions.
That’s why our IE system is based on a deep parse tree

The protein name dictionaries for the entity tagging
are derived from various biological sources such as
UMLS1, Gene Ontology2 and Locuslink3 database.
Regular expressions are also used to mark the names
that do not have a match in the dictionaries. The words
1

http://www.nlm.nih.gov/research/umls/
http://www.geneontology.org/
3
http://www.ncbi.nlm.nih.gov/LocusLink/
2

4

474

http://www.cogsci.princeton.edu/~wn/

structure presented by the LG and it considers a
thorough case based analysis of contents of various
syntactic roles of the sentences like their subjects (S),
verbs (V), objects (O) and modifying phrases (M) as
well as their linguistically significant and meaningful
combinations like S-V-O, S-O, S-V-M or S-M,
illustrated in Figure 3, for finding and extracting
protein-protein interactions.

S-V-O

S-VM

Subject (S)

Table 2. Recall comparison of IntEx and
BioRAT from 229 abstracts when compared
with DIP database.

SM

SO

S

The first evaluation for IntEx system was performed
on the same dataset 5 that was used for the BioRAT
evaluation. For BioRAT evaluation, authors identified
389 interactions from the DIP database such that both
proteins participating in the interaction had SwissProt
entries. These interactions correspond to 229 abstracts
from the PubMed. The BioRAT system was evaluated
using these 229 abstracts. The interactions extracted by
the system were then manually examined by a domain
expert for precision and recall.

O

Object
(O)

Recall
Results

Modifying
(M)

Match
No
Match
Totals

Phrase

385
527

Precision
Results

Table 1. Syntactic Role Type Matcher

Complete

142

79

Percent
(%)
20.31

73.06

310

79.67

100.00

389

100.00

Cases

Table 3. Precision comparison of IntEx and
BioRAT from 229 abstracts.

For each syntactic constituent of the sentence, the
role type matcher identifies the type of each role as
either ‘Elementary’, ‘Partial’ or ‘Complete’ based on
its matching content, as presented in Table 1.

Partial

BioRAT
Percent
(%)
26.94

Cases

M

Figure 3. Composition and analysis of
various syntactic roles.

Role Type
Elementary

IntEx

Description
If the role contains a Protein
name or an interaction word.
If the role has a Protein name and
an interaction word.
If the role has at least two Protein
names and an interaction word.

IntEx

BioRAT

Cases

Percent
(%)

Cases

Percent
(%)

Correct

262

65.66

239

55.07

Incorrect

137

34.33

195

44.93

Totals

399

100.00

434

100.00

We have also limited our protein name dictionary to
the SwissProt entries. Tables 2 and 3 present the
evaluation results as compared with the BioRAT
system. DIP contains protein interactions from both
abstracts and full text. Since our extraction system was
tested only on the abstracts, the system missed out on
some interactions that were only present in the full text
of the abstract.

IntEx interaction extractor works as follows. The
input to IE is the preprocessed and typed simple clause
structures. The IE algorithm progresses bottom up,
starting from each syntactic role S, V or M, and
expanding them using the lattice provided in Figure 3
until all ‘Complete’ singleton or composite role types
are obtained.

Second evaluation for the IntEx system was done to
test its recall performance using an article6 that was
also used by the GeneWays (Rzhetsky, Iossifov et al.

5. Evaluation & discussion
We have evaluated the performance of our system
with two state of the art systems - BioRAT (Corney,
Buxton et al. 2004) and GeneWays (Rzhetsky, Iossifov
et al. 2004).

5
Dataset was obtained from Dr. David Corney by personal
communication.
6
Dataset was obtained from Dr. Andrew Rzhetsky by personal
communication.

475

Conference on Tools with
(ICTAI'03): 467.

2004) system. Both systems performance was tested
using the full text of the article (Friedman, Kra et al.
2001). GeneWays system achieves a recall of 65%
where as IntEx extracted a total of 44 interactions
corresponding to a recall measure of 66 %.

Artificial Intelligence

[7] Friedman, C., P. Kra, et al. (2001). GENIES: a naturallanguage processing system for the extraction of molecular
pathways from journal articles. Proceedings of the
International Confernce on Intelligent Systems for
Molecular Biology: 574-82.

6. Conclusion

[8] Fukuda, K., T. Tsunoda, et al. (1998). "Toward
information extraction: Identifying protein names from
biological papers." PSB 1998,: 705-716.

With the amount of information available in textual
form today an automated information extraction
system is much needed to understand and peruse that
knowledge. IntEx system presented does automated
extraction of gene and protein interactions from
biomedical text. It handles complex sentences and can
extracts multiple nested interactions specified there in.
Experimental evaluations and comparison of the IntEx
system with the state of the art semi-automated
systems -- the BioRAT and GeneWays datasets
indicates that our system performs better without the
labor intensive rule engineering requirement.

[9] Hahn, U. and M. Romacker (2002). "Creating knowledge
repositories from biomedical reports: The medsyndikate text
mining system." Pacific Symposium on Biocomputing 2002:
338-349.
[10] Hunter, R. T. a. C. R. a. J. (2000). "Extracting
Molecular Binding Relationships from Biomedical Text." In
Proceedings of the ANLP-NAACL 000,Association for
Computational Linguistics: pages 188-195.
[11] Leroy, G., H. Chen, et al. (2003). Genescene:
biomedical text and data mining. Proceedings of the third
ACM/IEEE-CS joint conference on Digital libraries: 116-118.

7. Acknowledgements
We would like to thank to both Dr. David Corney
and Dr. Andrey Rzhetsky for sharing their evaluation
datasets and results.

[12] Mikheev, A. and S. Finch (1997). "A workbench for
finding structure in texts." Proceedings of Applied Natural
Language Processing (ANLP-97).
[13] Novichkova, S., S. Egorov, et al. (2003). "MedScan, a
natural language processing engine for MEDLINE abstracts."
Bioinformatics 19(13): 1699-1706.
[22] Quinlan, J. R. (1990). "Learning Logical Definitions
from Relations." Mach. Learn. 5(3): 239--266.

8. References
[1] Blaschke, C., M. A. Andrade, et al. (1999). "Automatic
extraction of biological information from scientific text:
Protein-protein interactions." Proceedings of International
Symposium on Molecular Biology: 60-67.

[14] Rinaldi, F., G. Schneider, et al. (2004). Mining relations
in the GENIA corpus. Proceedings of the Second European
Workshop on Data Mining and Text Mining
for
Bioinformatics: 61 - 68.

[2] Bunescu, R., R. Ge, et al. (2005). Comparative
Experiments on Learning Information Extractors for Proteins
and their Interactions. Artificial Intelligence in Medicine.

[15] Rzhetsky, A., I. Iossifov, et al. (2004). "GeneWays: a
system for extracting, analyzing, visualizing, and integrating
molecular pathway data." J. of Biomedical Informatics 37(1):
43--53.

[3] Califf, M. E. and R. J. Mooney (1998). "Relational
learning of pattern-match rules for information extraction."
Working Notes of AAAI Spring Symposium on Applying
Machine Learning to Discourse Processing: 6-11.

[16] Sleator, D. and D. Temperley (1991). Parsing English
with a Link Grammar.Carnegie Mellon University Computer
Science technical report CMU-CS-91-196, Carnegie Mellon
University.

[4] Corney, D. P. A., B. F. Buxton, et al. (2004). "BioRAT:
extracting biological information from full-length papers."
Bioinformatics 20(17): 3206-3213.
[5] Daelemans, W., S. Buchholz, et al. (1999). "Memorybased shallow parsing." Proceedings of CoNLL.
[6] Ding, J., D. Berleant, et al. (2003). Extracting
Biochemical Interactions from MEDLINE Using a Link
Grammar Parser. Proceedings of the 15th IEEE International

476

WinAgent: A System for Creating and Executing Personal
Information Assistants Using a Web Browser 1
Nikeeta Julasana, Akshat Khandelwal, Anupama Lolage, Prabhdeep Singh, Priyanka Vasudevan,
Hasan Davulcu, I.V. Ramakrishnan
Department of Computer Science, SUNY Stony Brook, Stony Brook, NY 11794

ram@cs.sunysb.edu

ABSTRACT

3. BUILDING PIAs : AN ILLUSTRATION

WinAgent is a software system for creating and executing
Personal Information Assistants (PIAs). These are software robots
that can locate and extract targeted data buried deep within a web
site. They do so by automatically navigating to relevant sites,
locating the correct Web pages (which can be either directly
accessed by traversing appropriate links or by filling out HTML
forms), and extracting, structuring, and organizing data of interest
from these pages into XML. The primary thrust of WinAgent
technology effort was to make these tools easy-to-use by users
who are not necessarily trained in computing. In particular users
create and execute PIAs through a Web Browser.

Suppose the user wishes to populate an XML database with
Domestications product data on comforters. To begin with, the
user instructs WinAgent builder to record all his actions from now
on by clicking the record button in the toolbar.

Categories and Subject Descriptors
H.5.4 [Information Interfaces and Presentation]:
Hypertext/Hypermedia – navigation.

General Terms
Algorithms, Design, Human Factors

Keywords
Machine Learning, XML, Web, Agents, Browser

1. INTRODUCTION
The system consists of a builder through which a user creates a
navigation map for a web site. This map encodes information
about how to navigate to pages of interest in a Web site and how
to extract data needed from these pages. At run time the map
interpreter automatically navigates to the web site, follows
specified links, fills out forms and extracts targeted data from the
pages specified in the map. The extracted data is presented in
XML, which can then be transformed into any user-specified
presentation format, such as comma-separated records, HTML,
VoiceXML, WML etc.

Figure 1 a)
He highlights an example of a comforter item to be extracted from
the page (the shaded item shown circled in Fig.1(a)). WinAgent
learns an XPath expression, applies it to the DOM tree of this
page and displays all the matched items (see the display panel
labeled “WinAgent Results”). The user next instructs WinAgent
that the items in the 1st column are all links to be followed. He
follows one such link that leads to a page that describes a
comforter item in detail (Fig.1(b)). He highlights the item number
and the prices (the shaded region shown circled in Fig.1(b)) for
extraction. WinAgent learns another 1XPath expression and
displays the data matching the expression. Observe that it is the
desired data. The user is done with building the Domestications
PIA. The recording session ends, the navigation map is created
and saved in the repository. The user launches the PIA using the
Play button, the result being that the PIA follows all of the links
(in Fig.1(a)). Each link leads to a product page (Fig.1(b)) from

2. OVERVIEW
The builder is embedded as a tool bar in Internet Explorer (see the
horizontal bar beginning with the label “WinAgent” below the
URL address box in Fig. 1(a)). Users interact with the system
through the five buttons (“Get Item”, “Get Region”, “Record”,
“Stop”, “Play”) resembling a VCR, for creating and interpreting
navigation maps. The maps are created semi-automatically based
on our early work on mapping-by-examples described in [1,2]
and learning algorithms such as those described in [4].

1

Copyright is held by the author/owner(s).
IUI’04, Jan. 13–16, 2004, Madeira, Funchal, Portugal.
ACM 1-58113-815-6/04/0001.

356

Research supported by NSF grants CCR-0205376, 0311512, IIS0072927 & US Army Medical Research Acquisition Activity (Award
number: DAMD17-03-1-0520

which the PIA pulls out the item numbers and prices and populate
the XML database with Domestications data on comforters.

Figure 2
XWRAP’s search interface extraction allows users to compose
GET and POST requests, while WinAgent has full-fledged
capability for navigation including form processing. TrIAs [6],
presents an approach to cooperative problem solving which
requires the user to intervene with the system from time to time
and provide rules. In WinAgent the amount of interaction of the
user with the system is minimal.

Figure 1 b)
The output of the extraction system is in the form of a nested
XML structure (see Figure 2). Such a nested XML can be
transformed into one of many desired presentation formats, such
as comma-separated records, VoiceXML, HTML, WML etc. Such
transformations on XML can be expressed using XSLT
stylesheets. WinAgent provides a generic GUI framework for
rapidly creating such XSLT’s, with an interface for users to plug
in specific formatting modules for transformation.

5. REFERENCES
[1] H. Davulcu, J. Freire, M. Kifer, I.V. Ramakrishnan. A
Layered Architecture for Querying Dynamic Web Content,
ACM Conference on Management of Data (SIGMOD),
Philadelphia, PA, June, 1999.
[2] H. Davulcu, M. Kifer, G. Yang, I.V. Ramakrishnan. Design
and Implementation of the Physical Layer in WebBases: The
XRover Experience, 6th International Conference in Rules
and Objects in Databases (DOOD), London, UK, July 2000.

4. RELATED WORK
W4F [5] and XWRAP [3] are two systems whose objectives come
quite close to those of WinAgent. W4F allows specification of
retrieval, extraction and mapping rules. It produces an agent that
can be executed as a Java program. It doesn’t support true
navigation by extracting and exploring links from within a page.
Once retrieved, extraction rules are applied in the form of paths in
the document hierarchy. However the user has to then create the
rules by observing these “document paths” and trying to
generalize them if possible. WinAgent does this by its own
learning algorithm. In XWRAP, objects and elements of interest
on a page are derived automatically by heuristics. Different pre-set
heuristics may be used and can be further fine-tuned by modifying
characteristics such as element data types, objects sizes and
element tag separators. In WinAgent, all the user needs to do is
identify one sample object and the object and element extraction
rules are then accurately generated.

[3] Ling Liu, Calton Pu, Wei Han. `` XWRAP: An XMLenabled Wrapper Construction System for Web Information
Sources”, Proceedings of the 16th International Conference
on Data Engineering(ICDE’2000), San Diego CA (February
28 - March 3, 2000).
[4] M. Perkowitz, R.B. Doorenbos, O. Etzioni, and D.S. Weld.
Learning to understand information on the internet: An
example-based approach, Journal of Intelligent Information
Systems, 8(2):133--153, March 1997.
[5] Arnaud Sahuguet, Fabien Azavant. “Building light-weight
wrappers for legacy Web data-sources using W4F “,
International Conference on Very Large Databases (VLDB)
(1999)
[6] Bauer, M. and D. Dengler: 1999, `TrIAs: Trainable
Information Assistants for Cooperative Problem Solving’. In:
Proceedings of the Third Annual Conference on
Autonomous Agents. pp. 260—267.

357

2008 IEEE International Conference on Services Computing

A RISK REDUCTION FRAMEWORK FOR DYNAMIC WORKFLOWS
Prabhdeep Singh, Fatih Gelgi, Hasan Davulcu, Stephen S. Yau
Dept. of CSE
Arizona State University, AZ USA
Prabhdeep.Singh, fagelgi, hdavulcu, yau@asu.edu

Supratik Mukhopadhyay
Dept. of CSE
Utah State University, UT USA
supratik@cc.usu.edu

Abstract
Workflows tend to fail in real-world scenarios due to
the uncertain/unreliable sensory information which sometimes needs to be updated during the execution of workflows. In a logic based framework, these dynamic predicates that can be updated are called non-monotonic predicates (NMPs). In this paper, we focus on reducing the risk
of a given workflow due to the NMPs in that workflow. The
main idea is to synthesize a backup workflow by augmenting the main workflow without introducing new NMPs. The
backup workflow is generated by using expected values of
NMPs if necessary instead of given values. The expected
values are calculated from the execution history or provided
by a domain expert. It is argued that total risk reduces to
the square root of the main workflow itself.
Figure 1. An accident-response scenario

1. Introduction

2. The CAR, FE and AMB go to location L.

A workflow is a series of cooperating and coordinated
activities. Since service-based systems usually consist of
thousands of services, it is desirable that the systems can allow users to declaratively specify their goals, and automate
the service composition based on the specified goals.
Consider a service-based system that connects the Police Departments (PD), Fire Departments (FD), and Ambulance Services (AMS) for coordinating various first responders (PD, FD and AMS) in handling traffic accident situations. PD, FD and AMS provide various capabilities as services in the system. The Accident Response Scenario given
in Figure 1 illustrates an automatically generated workflow
for coordinating field rescue operations according to the following dependencies:

3. Upon arriving at L, the police officers set up a perimeter to secure the accident site, and inform the FE and
AMB that the perimeter has been set up.

1. A 911 call center gets a report specifying an accident at
location L on a road, and informs a nearby Police Patrol Car (CAR), a Fire Engine (FE) and an Ambulance
(AMB) about the accident. According to the initial report there are two injured passengers.

When we consider real-world scenarios, such as the accident scenario above, workflows tend to fail due to uncertain or unreliable information mostly based on their sensing
actions. For instance, during the search for passengers, it
might be discovered that there are four injured passengers

978-0-7695-3283-7/08 $25.00 © 2008 IEEE
DOI 10.1109/SCC.2008.76

4. Upon arriving at L, the firefighters start to search for
passengers involved in the accident, and rescue the
passengers if they are trapped in the damaged vehicles.
5. Once the passengers are out of the damaged vehicles,
the paramedics on the AMB assess the status of the
injured passengers to decide the appropriate medical
care for them.
6. After assessing the status of the injured passengers, the
AMB takes the injured passengers to a nearby hospital.

381

instead of two. In the original workflow since there is only
one ambulance allocated, it would not be possible to rescue
all the injured passenger unless another available ambulance
can be found and deployed to the accident location.

Figure 2. Overall goal for the accident response scenario service composition depicted as a control flow graph.
Figure 2 depicts a control flow graph representing a template of the service composition, which should be satisfied
when responding to an accident. The control flow graph
depicts the requirements that whenever the call center receives an accident report, it should notify a CAR and a FE,
and then an AMB to rescue any injured passengers involved
in an accident. If the information is updated to be four injured passengers then the system can activate the backup
workflow to send another AMB’ to the accident location.
In this paper, the focus will be on the non-monotonic
predicates (NMPs) which might be updated during the execution of the workflow such as number of injured passengers. Updating NMPs is one of the major reasons for
workflow failures. Main contribution of this paper is a Risk
Management System for workflows that given a workflow
it synthesizes a backup workflow which supports the main
workflow and reduces the risk of failure to its square root.
Next section presents the architectural overview of the
risk management system. The foundations for logic based
modeling of workflows is presented in Section 3. Risk Management System is presented in Section 4 and its effects are
discussed in Section 5. Related work is given in Section 6,
and conclusion and future work is in Section 7.

Figure 3. An overview of the architecture of
the Risk Management System.

be obtained either from the execution history of the
workflows or provided by a domain expert.
• Backup Workflow Synthesis: This module augments
the main workflow W F without introducing any new
NMPs, and generates a backup workflow to support it.
The main idea to synthesize the backup workflow is
to use expected values of NMPs if their current values
are risky. Similarly, the risk and the expected values of
NMPs are obtained either from the execution history
of the workflows or determined by a domain expert.
Hence the backup workflow is at most as risky as the
main workflow which is one of the main contributions
of this paper.

2 Architectural Overview

• Resource Reservation: The backup resources which
are not in the main workflow but in the backup workflow is determined by this module. Then reservations are requested to make the resources ready to use
in case of a failure in the main workflow due to the
NMPs. That resource reservations make it available to
switch to the backup workflow if one of the NMPs fail
in the main workflow.

A common architecture for the Risk Management System
is presented in Figure 3. The Workflow Synthesis module
generates the candidate workflows for Risk Management
System to select the best work flow. Risk Management System is composed of three modules:
• Risk Calculation: Calculates the risk of each candidate workflow and returns the one which has minimum
risk (denoted as W F ∗ in the figure). The risk calculation of a workflow is based on the probability of the
workflow to fail due to it’s NMPs. Probabilities might

The details of the modules, calculations and algorithms
will be presented in Section 4.

382

3 Logic Based Modeling of Workflows

• Composite dependency: Atomic situational constraints
can be composed with conjunction (∧) and disjunction
(∨).

In this section, we will present α-logic – a high level
modal logic – which is expressive enough for modeling dynamic workflows. α-logic is a hybrid normal modal logic
[2] for specifying service-based systems. The logic has both
temporal modalities for expressing situation information as
well as modalities for expressing communication, knowledge, and service invocation. Since α-logic is not the focus
of this paper, we only intriduce parts of the syntax, semantics and proof system, which will be used in this paper here,
and provide some intuition explanations for the logic.

3.1

• Existential dependency: Situational constraints can be
composed with implication (→) to enforce insertion of
additional services.
• Ordering dependency: Situational constraints can be
composed with ordering (≺) operator to specify that a
pair of services should be invoked one after another.
For example, the following situational constraint related
to the accident scenario captures the first situational requirement that in low visibility situation, upon arriving at the accident location, the police officers PD should first set up a
perimeter to secure the accident site and then notify that the
perimeter has been setup.
serv(L, T ; F E sent, low; P D; 911CC) →
♦serv(L, T ; setup; P D; 911CC) ≺ ♦ < setup > P D

Modeling Control Flow Graphs

We assume that every variable x has a type. Intuitively,
the nominals act as identifiers to processes. The modality
serv(x; u; σ; ϕ) indicates that a process invokes service σ
with parameter x, receives the individual named u as the
result, and then satisfies ϕ. The formula huiϕ describes the
behavior of a process after sending out the individual named
u.
Since α-logic has connectives for services invocation,
eventuality (♦), parallel composition (||) and disjunction
(∨) as well as atomic constraints it is straightforward to
model services composition goals as control flow graphs
into α-logic statements. For example, the control flow
graph in Figure 2 can be represented in α-logic as follows:

Similarly the following ordering situational constraint
captures the requirement that if there are critically injured
passengers then a helicopter HELI should be requested by
the AMB.
serv(L, T ;0 critical0 ; U ; AM B) →
♦serv(L, T ;0 helicopter sent0 , HeliID; U, AM B)

3.3

♦serv(L, T ;0 accident0 ; U ; 911Callcenter)
≺ ♦serv(L, T ;0 CAR sent0 , CarID; X; P DAgent)
≺ (♦serv(L, T ;0 F E sent0 , F eID; Y ; F EAgent)∧
♦serv(L, T ;0 AM B sent0 , AmbID; Y ; AM BAgent))
≺ ♦serv(L, T ;0 rescued0 ; Z; 911CallCenter)

3.2

Services Composition with α-Logic

In this section, we limit our attention to synthesis of noniterative services compositions without loops. In general,
the problem of constrained services composition and execution given a control flow graph and a set of dependencies requires that services are composed at design time and
constraints are checked and enforced at run time alongside
the execution of the control-flow graph. This may lead to
unnecessary backtracking since each constraint violation at
run-time requires that a new alternative execution path must
be tried out. Also, such a strategy can not detect unsatisfiable requirements leading to plenty work done towards an
achievable goal at run-time.
In this paper we propose a more efficient proof theory
for the above classes of control flow graphs and depedencies that allows us to find executable services composition
at design time.
Given an α-Logic formula G, describing a control flow
graph and a set of dependencies C, as inputs our synthesis
algorithm, called Enf orce produces a conjunction-free executable control flow specification GC through a series of
transformations. Our algorithm includes a proof procedure
based on forward-chaining natural deduction presented in
detail in the Background section to enforce existential constraints and a procedure to enforce ordering and other constraints. In order to enforce situational constraints at design

Modeling Dependencies

α-logic can express a wide variety of temporal constraints. But here we focus on a relatively simple algebra of
constraints for expressing the situational constraints identified in Section 4. Our algebra is as expressive as Singh’s
Event Algebra (Singh, 1995). These constraints are believed to be sufficient for the needs of services composition and workflow synthesis tasks, and their expressivity far
beyond of the capabilities of the currently available commercial systems.
• Atomic dependency: ♦serv(In; Out; Callee; Caller)
indicates that a certain type of a service should eventually be invoked.
• Negative atomic dependency:
¬♦serv(In; Out; Callee; Caller) indicates that a
service type should not be used during the rest of the
composition.

383

4.1

time, during each step of the forward chaining proof procedure, the left-hand side (or the body) of each situational
constraint is evaluated using the current sequence of service
composition. If the body of a constraint is evaluated to be
true, the right-hand side (or the head) of the constraint is
enforced using the procedure presented below.

Case-based Analysis for the types of
NMPs

We can categorize the variables of predicates into two
forms:
• Enumerated values such as ambulance, helicopter.

• Atomic dependency. If M is new,
Enforce(♦serv(I; O; M ; A), G) ≡
G||♦serv(I; O; M ; A)

• Non-enumerated values such as number of injured
passengers, average speed of an ambulance.

• Composite dependencies.
Enforce(C1 ∧ C2, G) ≡
Enforce(C2, Enforce( C1, G))

Based on the category of the variables and the situation
of being instantiated or un-instantiated, different problems
may happen. In the following, we will give examples for
each situation based on the accident-response scenario described above:

Enforce(C1 ∨ C2, G) ≡
Enforce(C2, G) ∨ Enforce(C1, G)
• Ordering dependency.
Enforce(♦serv(I1 ; O1 ) ≺ ♦serv(I2 ; O2 ), G) ≡
Synch(Enforce(♦ serv(I2;O2),Enforce(♦serv(I1; O1),
G)))

• Case 1: Enumerated, instantiated. In the scenario,
suppose there are two injured passengers and their
conditions are critical. During the rush hour, it is
almost impossible for an ambulance to get to the
accident location. Hence, it is more reasonable to
send a helicopter instead of an ambulance. Here,
the discrete instantiated non-monotonic predicate is
send(Ambulance). But, we need to re-instantiate it
with ‘Helicopter’; send(Helicopter).

where Synch injects a send signal after every occurrence of ♦serv(I1 ; O1 ) and a receive signal before every occurrence of ♦serv(I2 ; O2 )
• Existential dependency.
Enforce(C1 → C2, Goal) ≡
Enforce(C1, Goal) ` Enforce(C2, Goal)

• Case 2: Enumerated, un-instantiated. Suppose the
deductive proof includes the un-instantiated predicate
send(X). We know that X is a vehicle, but the question is, which vehicle is to be sent to the accident location.

• Negative atomic dependency.
Enforce(¬♦serv(I; O; M ; A), G) ≡
Enforce(♦serv(I; O; M ; A) →⊥, G)

4 Risk Management for NMPs

• Case 3: Non-enumerated, instantiated. Suppose the
capacity of an ambulance is 2 injuries and the number
of injured passengers is initially reported as 2. By the
constraints;

The general framework for risk management system has
three phases:

capacity(Ambulance) = 2

• Generate candidate backup workflows with the expected values of NMPs by augmenting the main workflow.

injured(Y ), send(Ambulance, X),
X ≥ Y /capacity(Ambulance)

• Calculate the risk of each candidate backup workflow,
and select the best one as discussed in Section 4.2.

a workflow planner may generate the result;

• Always do resource reservation in such a way that
when the main workflow fails during execution, the
scheduler can switch to the backup workflow utilizing
the backup resources.

that is, only one ambulance is sent to the location.
However, if the information of injured passengers is
updated to 4, one ambulance won’t be enough.

injured(2) ⇒ send(Ambulance, 1)

• Case 4: Non-enumerated, un-instantiated. In the scenario, another case is what we should do if the number
of injured passengers is not known. In the urgent case,
we have to do planning using the un-instantiated nonmonotonic predicate injured(X).

For the scenarios in the domains of our interest such as
accident scenario, we consider the risk of the workflow as
the probability of the workflow not to succeed. We will
investigate the risk of the workflows in more detail below.

384

4.2

Workflow Risk Calculation

4.2.2 Conditional Workflows

To evaluate the risk of a workflow, first we assume that
there exists a workflow execution history or a domain expert. Our idea is to calculate the probability of the workflow
to be unsuccessful by using history as the statistics.
For each NMP, we obtain a probability distribution over
the values either discrete or continuous. We also assume
that the characteristic of the probability distribution for each
NMP is provided by the domain expert such as normal distribution, uniform distribution etc. Then the risk of each
non-monotonic instantiated predicate in a workflow can be
calculated. The source of the information is also very important in risk calculations. For instance, in the accident
scenario, the police report must be much more accurate and
has more probability to be correct than an anonymous report.

Figure 4. An instance of a conditional workflow.
For conditional workflows, consider the workflow π in
Figure 4. The workflow starts at S and finishes at G. D is a
decision point that workflow splits into two branches, and A
is the reunion point1 . N1 , N2 , N3 and N4 are the fragments
of the workflow. N1 is the fragment between start and the
decision point. N2 and N3 are the branches from decision
point to the reunion point. Lastly, N4 is the fragment between the reunion point and the goal. We have two cases
for the decision point D; D might contain non-monotonic
predicates or not.
Case 1: D does not have any non-monotonic predicates,
that is, the probability of ‘D happens’ is equal to ‘D does
not happen’. From Theorem 4.1, we can calculate the risk
of each fragment by multiplying the risks of non-monotonic
predicates in that fragment. Let Ri be the risk of the fragment i ∈ {1, 2, 3, 4}, then,

Definition 4.1. Let π be a workflow, and Nπ be the set
of non-monotonic predicates in π. The risk of the nonmonotonic predicate n(X, {S}) ∈ N with source S is defined as,
Rn(X,{S}) = 1 − P [n(X, {S})]
where and P [n(X)] is the probability of n with the value
X in its distribution. Note that the argument S is presented
when S is applicable.
We will examine the total risk of the workflow through
analysis of four cases;
1. Unconditional workflows: Simple workflows, only a
sequence of actions.

Ri = 1 −

2. Conditional workflows: Workflows that contain decision points / conditional actions.

Y

1 − Rn(X,{S}) .

n(X,{S})∈Ni

Considering each branch as an event, the branches are
mutually exclusive since either one of the branches is executed. By the independence assumption and the equal probability of the decision point, the total risk of the branches,
Rd will be,
Rd = (R2 + R3 )/2.

3. Alternating workflows: Workflows that contain alternative branches to accomplish the goal.
4. Parallel workflows: All the branches in the workflow
have to be accomplished.
4.2.1 Unconditional Workflows

We can generalize the above result by,

Analogous to the intersection of the risks, we consider the
total risk of the workflow as the product of the risks of the
non-monotonic predicates in that workflow. We assume independence among non-monotonic predicates for simplicity, efficiency and scalability.

Theorem 4.2. Let a workflow π has a decision point that
yields k branches and reunions at the same point. Let Ni be
the set of non-monotonic predicates in the branch i. Then
the total risk of all the branches, Rd is,

Theorem 4.1. Let π be a workflow, and Nπ be the set
of non-monotonic predicates in π. The risk of the overall
workflow Rπ is defined as,
Y
Rπ = 1 −
1 − Rn(X,{S}) .

Rd =

k
1X
Ri
k i=1

1 Note that the reunion point has to exist for conditional workflows; at
worst it would be the goal state since the goal is fixed in a workflow and
the branches have to lead to the same goal

n(X,{S})∈Nπ

385

Proof. Workflow fail means the probability of all the
branches to fail. That is,
Rd =

k
X
1 − P (Ni )

k

i=1

4.2.4 Parallel Workflows
(analogous to the intersection of the risks) For the same figure above, consider D is the splitting point for the parallel
workflows. Since both of the branches have to be successful
the total risk will be,

k
1X
=
Ri
k i=1

where P (Ni ) is the probability of Ni to succeed.

Rd = 1 − (1 − R2 )(1 − R3 ).

¤

From the result we can infer,

Case 2: D does have any non-monotonic predicates, that
is, the probability of ‘D happens’ may not be equal to ‘D
does not happen’. Let Rc is the risk of D, then the only
difference with the previous case is, we add the risk of the
branch that D happens with 1 − Rc and the other branch
with Rc .

Theorem 4.4. Let a workflow π has a decision point that
yields k branches and reunions at the same point. Let Ni be
the set of non-monotonic predicates in the branch i. Then
the total risk of all the branches, Rd is,

Rd = 1 − (1 − Rc )(1 − R2 ) − Rc (1 − R3 ).

Rd = 1 −

k
Y

1 − Ri .

i=1

4.2.3 Alternating Workflows

Proof. The probability of at least one branch to fail is,
Rd = 1 −

k
Y

1 − Ri = 1 −

i=1

k
Y

P (Ni ).

i=1

¤
In short, the idea is to multiply the probabilities in serial
branches, and to take the intersection of the risks in parallel
branches analogous to the set intersection and union operations. We can also use combinations of the above workflow
types and calculate their risks.
Since we would like to generate safe workflows, all the
non-monotonic predicates in the generated workflow should
also satisfy a certain pre-defined confidence level. It may be
either discrete, continuous, instantiated or un-instantiated,
for each non-monotonic predicate, we identify a confidence
interval based on the calculated probability distribution and
the characteristics of distribution.

Figure 5. An instance of a alternating workflow.
Consider the workflow π in Figure 5 which has an alternating point at D, and A is the reunion point. Either N2 or
N3 has to be successfully executed for workflow to succeed.
By independence assumption, the total risk of the branches,
Rd will be,
Rd = (1 − P (N2 )).(1 − P (N3 )) = R2 R3

Definition 4.2. Let Cn be the confidence level for the nonmonotonic predicate n, and the confidence interval for n be
In = [a, b]. Then the following equation holds,
X
P (n(X)) ≥ Cn

where P (Ni ) is the probability of Ni to succeed. We can
generalize the above result by,
Theorem 4.3. Let a workflow has an alternating point that
yields k branches and reunions at the same point. Let Ni be
the set of non-monotonic predicates in the branch i. Then
the total risk of all the branches, Rd is,
Rd =

k
Y

X

for enumerating and discrete variables,
Z b
P (n(X))dx ≥ Cn

Ri .

a

i=1

for continuous variables.

Proof. The probability of all the branches to fail is,
Rd =

k
Y
i=1

1 − P (Ni ) =

k
Y

If a non-monotonic predicate in a workflow is instantiated, and below the confidence interval, then we consider
that the workflow is not safe, hence not qualified even it has
very low overall risk. The crucial reason to eliminate the

Ri .

i=1

¤

386

workflow is that, the risk of one of the non-monotonic predicates might be too high to cause to fail the workflow. To
make the idea of confidence level clear, consider the scenario: there is an accident with 2 or 3 passengers injured.
According to the statistics the probability of injured passengers to be 2 and 3 is 0.86 and 0.99 respectively. AMS has
two ambulances with the distances of 10 and 5 miles to the
accident location, and the capacities of 3 and 2 passengers
respectively. The accident is known to be serious, and the
ambulance has to arrive at the location in 5 minutes. That
means the first ambulance must drive with the average speed
of 120 mph and the second must have 60 mph. The probabilities of the given speeds for the ambulances are 0.75 and
0.95. Assume that we can use only one of the ambulances.
Rule: arrivalT ime = distance(X)/avgSpeed(X)
Constraint: arrivalT ime ≤ 5min

W Fb :
injured(4), send(Ambulance2), send(Ambulance1).
When the main workflow fails due to the number of injured passengers, we can switch to the backup workflow by
just sending Ambulance2 to the accident location.
The idea of backup workflow is to generate a workflow
that is executable in parallel with the main workflow. Let d
be a given NMP and d0 is the NMP of d with its expected
value. Let Bef ore and Af ter be functions that returns the
set of predicates that precedes and succeeds d in the original
workflow:
• Bef ore(d, W F ) ≺ d0 ,
• d0 ≺ Af ter(d, W F ),
We can then use the Enf orce method presented earlier
to synthesize the backup workflow. Here ≺ is a transitive
relation, i.e, if (a ≺ b) ∧ (b ≺ c) → (a ≺ c). In other
words, upon its synthesis d0 shouldn’t violate any ordering
constraints of d. Let D be the dependency set of the main
workflow. Then the dependency set of backup workflow D0
should augment the dependencies of the main workflow by
inheriting the ordering dependencies of d for d0 . That is,

W F1 : injured(3), send(Ambulance1),
arrivalT ime = 5min
Risk1

= 1 − (1 − Rinjured(3) )
×(1 − RavgSpeed(Ambulance1)=120 )
= 1 − 0.99 × 0.75 = 0.2575

W F2 : injured(2), send(Ambulance2),
arrivalT ime = 5min
Risk2

D0 = D ∪ {x ≺ d0 |x ≺ d ∈ D} ∪ {d0 ≺ x|d ≺ x ∈ D}

= 1 − (1 − Rinjured(2) )
×(1 − RavgSpeed(Ambulance2)=60 )
= 1 − 0.86 × 0.95 = 0.2604

The next step is to find the sub-workflow W Fs that is related to d0 which has the root as the least common ancestor
of the predicates that appears together in any dependency of
d0 . All the descendants of that root is in W Fs . Then,

According to overall risk calculations, a rescue workflow
using the first ambulance will have lower risk than the sending the second ambulance. However, it is not likely that the
first ambulance can travel at the speed it is required yielding
a more probable failure. On the other hand, each NMP of
W F2 has low risk factor which makes the overall workflow
safer.

4.3

Enf orce(d ∧ d0 , W Fs )
generates candidate the backup workflow one of which
will be selected based on risk calculations as given in the
next section. The given method is sound since the generated
backup workflow is always consistent with the dependencies and does not introduce new NMPs into the workflow.

Backup Workflow Synthesis

5 Effect of Risk Management

Due to the unreliability of the non-monotonic predicates,
a good idea is to generate an alternative workflow to backup
the main workflow. Assuming that the execution history
gives us enough statistics, we synthesize a backup workflow
by using the expected values of all the non-monotonic uninstantiated predicates and the non-monotonic predicates
that are below the confidence levels with their given values. To keep the backup workflow consistent with the
main workflow, the execution of the workflow is monitored.
Whenever it is inconsistent with the backup workflow, the
backup workflow is re-generated.
Consider the previous example and the main workflow is
W F2 . Suppose the expected number of injured passengers
is 4 and the backup workflow is,

In this section, we will show that risk management with
backup workflow synthesis is an effective method. Our risk
management system reduces the risk of the main workflow
to its square root.
Theorem 5.1. Let πa and πb be the main workflow and the
backup workflow respectively. Then
Rπa + Rπb ≤ Rπ2 a
Proof. From backup workflow synthesis, πb contains at
most same amount of non-monotonic predicates with πa .
The risk of each non-monotonic predicate in πb is less than

387

or equal to the ones in πa since we use expected values or
the value in πa if it is better. Hence, Rπb ≤ Rπa . That
leads,
Rπa ∧πb = Rπa .Rπb ≤ Rπ2 a .

in which the uncertain evidence is acquired, a rather unBayesian looking effect. In [7] Hawthorne explores three
models of sequential updating, the usual extension and two
alternatives. He establishes necessary and sufficient conditions for order-independent updating, and shows that extended rigidity is closely related to these conditions.

¤
Consider the example below;
W Fa : injured(2), send(Ambulance2)
W Fb : injured(4), send(Ambulance2),
send(Ambulance1)

7 Conclusion and Future Work
In this paper, we presented a risk management system for
a given workflow that synthesizes a backup workflow to reduce the risk of the main workflow to at least its square root.
A modal logic based framework is presented for managing
the dependencies of the system.

Suppose RW Fa = 0.05 and RW Fb = 0.03
Then the overall risk reduces to,
2
RW Fa ∧W Fb = 0.05 × 0.03 = 0.0015 ≤ RW
Fa .

Acknowledgments

6 Related Work

This work is supported by the DoD/ONR under the Multidisciplinary Research Program of the University Research
Initiative, Contract No. N00014-04-1-0723.

Three most common frameworks for specifying service
composition requirements are control flow graphs [10, 8],
triggers [3] (also known as event-condition-action rules),
and situational temporal constraints [1, 4]. The control flow
graph is most appropriate for depicting the local execution
dependencies between the key activities in a service composition. It is a good way to visualize the overall flow of control between the milestones that a workflow should satisfy.
Control flow graphs are the primary specification means
in most commercial implementations of workflow management systems. A typical graph specifies the initial and the final activities in a workflow, the successor-activities for each
activity in the graph, and whether these successors must all
be executed concurrently, or it suffices to execute just one
branch non-deterministically.
[9] is the seminal paper of non-monotonic planning. A
theory of planning that uses non-monotonic reasoning on
the modal quantification logic Z is developed. It does forward reasoning and backward planning to reach the goal.
The proposed theory uses frame axioms and the modal
quantification logic Z to propagate the facts from the current situation to the next situation. The method states the
most obvious consequences of each action, and it can not
use domain knowledge or history to handle failures due to
the non-monotonic predicates.
The idea of confidence level and degree of belief goes
back to the papers [5, 6, 7]. In [5], Hawthorne described a
range of non-monotonic conditionals that behave like conditional probability functions at various levels of probabilistic support. These conditionals were defined as semantic relations on an object language for sentential logic.
Hawthorne extends the work to the most prominent family
of these conditionals to a language for predicate logic in [6].
When the evidence is uncertain, the resulting degrees of belief in Bayesian updating appear to be sensitive to the order

References
[1] P. C. Attie, M. P. Singh, A. P. Sheth, and M. Rusinkiewicz.
Specifying and enforcing intertask dependencies. In Proceedings of the 19th Conference on Very Large Databases,
pages 134–145, 1993.
[2] P. Blackburn, M. deRijke, and Y. Venema. Modal Logic.
Cambridge University Press, 2003.
[3] U. Dayal, M. Hsu, and R. Ladin. Organizing long-running
activities with triggers and transactions. In Proceedings of
the 1990 ACM SIGMOD International Conference on Management of Data, pages 204–214, New York, NY, USA,
1990. ACM Press.
[4] E. A. Emerson. Temporal and modal logic. pages 995–1072,
1990.
[5] J. Hawthorne. On the logic of nonmonotonic conditionals and conditional probabilities. Journal of Philosophical
Logic, 25(2):185–218, 1996.
[6] J. Hawthorne. On the logic of nonmonotonic conditionals
and conditional probabilities: Predicate logic. Journal of
Philosophical Logic, 27(1):1–34, 1998.
[7] J. Hawthorne. Three models of sequential belief updating on uncertain evidence. Journal of Philosophical Logic,
33(1):89–123, 2004.
[8] H. Hsu. Special issue on workflow and extended transaction
systems. Bulletin of the IEEE Technical Committee on Data
Engineering, 16(2), 1993.
[9] S. S. Hundal and F. M. Brown. A theory of nonmonotonic
planning. In CSC ’91: Proceedings of the 19th annual conference on Computer Science, pages 247–254, New York,
NY, USA, 1991. ACM Press.
[10] WFMC.
Workflow management coalition. terminology and glossary.
Retrieved December 4,
2005
from
http://www.wfmc.org/standards/docs/TC1011 term glossary v3.pdf, 1996.

388

YellowPager: A Tool for Ontology-based Mining of Service
Directories from Web Sources 
Prashant Choudhari
Akshay More

Hasan Davulcu
Abhishek Joglekar
Saikat Mukherjee
Supriya Patil
I.V. Ramakrishnan
Dept. of Computer Science
SUNY Stony Brook
Stony Brook, NY 11794, USA

fcprashan,davulcu,abhishek,amore,saikat,supriya,ramg@cs.sunysb.edu

The web has established itself as the dominant medium for
doing electronic commerce. Realizing that its global reach
provides signicant market and business opportunities, service providers, both large and small are advertising their
services on the web. A number of them operate their own
web sites promoting their services at length while others
are merely listed in a referral site. Aggregating all of the
providers into a queriable service directory makes it easy for
customers to locate the one most suited for his/her needs.
YellowPager is a tool for creating service directories by
mining web sources. Service directories created by YellowPager have several merits compared to those generated by
existing practices, which typically require participation by
service providers (e.g. Verizon's SuperYellowPages.com).
Firstly, the information content will be rich. Secondly since
the process is automated and repeatable the content can
always be kept current. Finally the same process can be
readily adapted to dierent domains.
YellowPager builds service directories by mining the web
through a combination of keyword-based search engines,web
agents, text classiers and novel extraction algorithms.The
extraction is driven by a services ontology consisting of a
taxonomy of service concepts and their associated attributes
(such as names and addresses) and type descriptions for the
attributes. In addition the ontology also associates an extractor function with each attribute. Applying the function
to a web page will identify all the occurrences of the attribute in that page.
YellowPager 's mining algorithm consists of a training step
followed by classication and extraction steps. In the training step a classier is trained to identify web pages relevant
to the service of interest. The classication step proceeds
by doing a search for the particular service of interest us-

ing a keyword based web search engine and retrieves all the
matching web pages. From these pages the relevant ones are
identied using the classier. The nal step is extraction
of attribute values, associated with the service, from these
pages. Each web page is parsed into a DOM tree and the
extractor functions are applied. All of the attributes corresponding to a service provider are then correctly aggregated.
This can pose diÆculties especially in the presence of multiple service providers in a page. Using a novel concept of
scoring and con
ict resolution to prevent erroneous associations of attributes with service provider entities in the page,
the algorithm aggregates all the attribute occurrences correctly. The extractor function may not be complete in the
sense that it cannot always identify all the attributes in a
page. By exploiting the regularity of the sequence in which
attributes occurr in referral pages, the mining algorithm automatically learns generalized patterns to locate attributes
that the extractor function misses. The distinguishing aspects of YellowPager 's extraction algorithm are: (i) it is
unsupervised, and (ii) the attribute values in the pages are
extracted independent of any page-specic relationships that
may exist among the markup tags.
YellowPager has been used by a large pet food producer
to build a directory of veterinarian service providers in the
United States. The resulting database was found to be much
larger and richer than that found in Vetquest, Vetworld, and
the Super Yellow pages.
YellowPager is implemented in JAVA and is interfaced to
Rainbow, a library utility in C that is used for classication.
The tool will demonstrate the creation of a service directory
for any service domain by mining web sources.

 This work was supported by industry and university grant
{ NSF IIS0072927

Copyright is held by the author/owner.
SIGIR’02, August 11-15, 2002, Tampere, Finland.
ACM 1-58113-561-0/02/0008.

458

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

A Clustering Technique
for Mining Data from
Text Tables∗
Hasan Davulcu† , Saikat Mukherjee‡ , and I.V. Ramakrishnan§
Abstract
Considerable quantities of valuable data about product information and ﬁnancial
statements is often available in sources and formats that are not amenable for
querying using traditional database techniques. One such important source is text
documents. In such documents these kinds of data often appear in tabular form. A
data item in these text tables may span several words (e.g. product description).
Furthermore items supposedly within the same column do not necessarily begin or
end at the same position. Thus the absence of any regularity in column separators
makes it diﬃcult to automatically mine, i.e. extract data items from text tables.
Nevertheless an interesting characteristic often exhibited by these tables is that
intra-column items are “closer” to each other than inter- column items. We exploit this observation to develop a clustering-based technique to extract data items
from these tables. In contrast to previous appproaches, a unique and important
aspect of using clustering is that it makes the technique robust in the presence of
misalignments. We provide a characterization theorem for text tables on which
this technique will always produce a correct extraction. We discuss the design and
implementation of a system for extracting tabular data based on this clustering
technique. We present experimental evidence of its eﬀectiveness and usability on
real industrial data.
∗ This

work was supported by industry and university grant – NSF IIS0072927.
Inc., Suite 115, Nassau Hall, Stony Brook, NY 11790. davulcu@cs.sunysb.edu
‡ XSB, Inc., Suite 115, Nassau Hall, Stony Brook, NY 11790 and Department of Computer
Science, SUNY Stony Brook, Stony Brook, NY 11794. saikat@cs.sunysb.eud.
§ XSB, Inc., Suite 115, Nassau Hall, Stony Brook, NY 11790. Department of Computer Science,
SUNY Stony Brook, Stony Brook, NY 11794. ram@cs.sunysb.eud.
† XSB,

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

315

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

1

Introduction

A wealth of information relevant for electronic commerce often appears in text form.
This includes speciﬁcation and performance data sheets of products created by manufacturers, ﬁnancial statements published by brokerage houses, product oﬀerings by
vendors, etc. Typically these types of product and ﬁnancial data are published in
tabular form. Figure 1 is an illustrative example of a ﬁnancial statement presented
as a table1 in text form, which we will refer to as the text table.
123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9

SECURITIES
----------

RATE
------

MATURITY
--------

Carson City Nev Sch Dist.......
7.700
2001
Trans Authority in the state of Delaware.... 7.100
2001
Harrison Cnty MS Sch Dist......
7.000
2002
NYS Environ.Fac Corp.........
7.300
2002
Lewisville TX Indpt Sch Dist.....
7.500
2002
Hamilton Twp NJ Sch Dist.....
7.000
2002
Lincoln, MI Cons Sch Dist.....
7.000
2003
Tempe, AZ.......
6325
2003
Rowlett, TX.......
6.000
2004
Gladstone MI Pub Schedule in Michigan USA... 6.500
2005
NYS Dorm Auth.....
6.500
2005
FL St Brd Mand SKG.....
7.500
2005
Metro Pier and Expo.....
6.250
2005
Metro Pier and Exploration of oil and gas...
6.500
2005
Jackson Miss Pub Sch.....
6.250
2005
NY Dorm Auth......
7.800
2005

AMOUNT
-----------

BASIS
---------

265,000
200,000
70,000
500,000
300,000
260,000
325,000
210,000
170,000
100,000
250,000
20,000
100,000
310,000
130,000
40,000

267,100
201,981
72,19
507,985
3,137
270,868
345,241
221,770
177,535
109,035
26,806
20,644
108,603
3,373
137,765
41,610

Figure 1. A Finance Data Table
One would like to query the information available in text tables as in Figure 1
above. However this information which is in unstructured text form, is not readily
queriable using traditional database technology such as SQL. One way to make it
amenable to standard database querying techniques is to mine, i.e. extract the
data items in the tables and populate a database out of the extracted data. But
extracting text data from tables that are “irregular ” as in the example above poses
some diﬃculties. In broad terms, irregularity is characterized by variable length
data items (perhaps spanning multiple words) that possibly overlap with items in
neighboring columns. For example in Figure 1 the item Gladstone MI Pub Schedule
in Michigan USA in the ﬁrst column overlaps with items (7.700, 7.000 and 7.000
from the 1 st, 2 nd and 7 th rows respectively) in the second column. In fact this
table was one of over several thousand text tables sent to us by a ﬁnancial data
aggregation company that had tasked us to develop an extractor for these tables.
Let us now examine some possible approaches to extraction from such tables.
Assume that every character (including white spaces) in a row is assigned a unique
1 The topmost row denotes the column position of each character and the ﬁrst column in each
row denotes the row number in the table.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

316

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

position in that row. Since an item in the table is a string, we can further suppose
that every item i in a row is associated with a pair < li , ri > where li , ri are
the positions where the item begins and ends respectively. A simple approach for
extracting items is to ﬁnd ﬁxed separators between successive columns. Intuitively,
a ﬁxed separator is a unique position that distinguishes items occurring in a pair of
neighboring columns. In particular if p is a ﬁxed separator for columns k and k + 1
then ∀ items i ∈ k, ri < p and ∀ items j ∈ k + 1, lj > p, i.e. all the items in column
k occur before p while those in k + 1 occur after p. In fact a recent work [1] uses
such an approach for extracting data from tables. But observe in Figure 1 that we
cannot always ﬁnd ﬁxed separators (as in columns 1 and 2 in Figure 1). Even if
ﬁxed separators exist it is unclear how they can unambiguously separate columns
that have multiword items (e.g. Column 1 in Figure 1). Another technique that
is generally used for extracting data from text is based on regular expressions [6].
Regular expressions specify patterns that occur in text and a regular expression
processing engine extracts pieces of text that match the speciﬁed patterns [4].
Although regular expression based extractors are powerful when dealing with text
processing in general, they are quite cumbersome and diﬃcult to use in the presence
of tables consisting of items that span several words and overlap with items in other
columns. The problem we address in this paper is how do we extract data from
text tables such as those in Figure 1.
Overview of Approach and Summary of Results Observe in Figure 1 that although ﬁxed separators between every pair of adjacent columns do not exist, by
visual inspection a casual observer can still correctly associate every item to its
corresponding column. This is because all the items belonging to a column, despite having irregular alignments, appear clustered more “closely” to each other
than to items in diﬀerent columns. Whereas such clusters can be clearly discerned
by a human observer, making them machine recognizable is the key to automated
extraction of data items from text tables.
• In this paper we develop a clustering technique for extracting items from
irregular text tables. We assume that the tables have headers corresponding
to each column. We ﬁrst formalize the notion of “closeness” of data items.
Using this notion we develop an iterative algorithm to partition all the data
items into clusters and associate each cluster with a distinct header.
• We also formalize the notion of a correct extraction for a table and provide
a syntactic characterization theorem for tables on which our algorithm will
always produce a correct extraction (see Section 2).
• We exploit the characterization to engineer a practical system that can be
conﬁgured to produce a high extraction yield as well as facilitate identiﬁcation
of incorrect extractions (see Section 3).
Research Contributions
• A number of clustering algorithms have been developed in the past for applications in image processing, machine learning and data mining [9, 3]. We

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

317

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

use clustering for the purpose of extracting all the column items from a text
table. To the best of our knowledge applying a clustering technique to this
problem has not yet been explored in the research literature.
• Clustering enables us to make associations between items in a column based
not merely on examining items in adjacent rows but across all the rows in
the table. This means that even though two items in adjacent rows may
not appear to be in the same column when examined in isolation (due to
misalignments), but when viewed in the context of all the rows they can be
correctly associated with the same column. In contrast to previous techniques
[1, 8, 5, 6, 7] a clustering-based algorithm is robust in the presence of misalignments.
• We have developed the ﬁrst formal treatment of the problem. The formalization yielded the characterization theorem which from a theoretical perspective,
can serve as the basis for comparing the power of diﬀerent approaches. On
the practical side it can provide valuable information for improving extraction yield by examining the incorrectly extracted tables and appropriately
reconﬁguring and rerunning the system on these tables (see Section 3).
The rest of the paper is organized as follows. In Section 2 we develop the
concept of clustering pertinent to the problem of table extraction as well as the
notion of a correct extraction. Based on this clustering concept, we describe an
algorithm to extract items from a table. Characterization of tables on which this
algorithm always yields a correct extraction also appears in this section. In Section 3 we describe an implementation of our extraction system based on clustering
and present experimental evidence of its eﬀectiveness on real industrial data. Our
algorithm assumes that every column is associated with a unique header. However these headers may span multiple lines. We brieﬂy discuss how to detect such
headers and uniquely associate them with their corresponding columns. Related
work appears in Section 4. Discussions and concluding remarks appear in Section 5
where we mention how similar clustering ideas can be extended for detecting tables
embedded in arbitrary text documents.

2

A Clustering Technique for Table Extraction

The problem we address is this: Given the rows of a table embedded in text as the
input, create an algorithm that associates the items in the table with their corresponding columns.
We develop a clustering-based algorithm for the above problem. We will require the concept of a cluster appropriate for the above problem. Also observe that
the result produced by the algorithm is an association between the items in the table with columns. We therefore need the notion of a correct association to evaluate
its output. To formalize all these concepts we ﬁrst develop the technical machinery
required to describe the algorithm and its properties.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

318

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

2.1

Formalization

We assume that a line in a text table is made up of characters. Each character in a
line is associated with a unique position. We will use lines and rows interchangeably
Each row has a unique integer index called its row index. A line is made up of tokens
deﬁned as:
Deﬁnition 1 (token). : A token is a contiguous sequence of characters until
either a blank space or a newline character is encountered.
A token t is characterized as a four tuple (start, end, center, row) where :
• start(t) is the position of its ﬁrst character.
• end(t) is the position of its last character.
• center(t)=  start + end 	 / 2.
• row(t) is the index of the row in which it occurs.
In Figure 1, the item “Delaware....” in row 5 is a token whose start, end,
center and row index are 35, 46, 41 and 5 respectively.
Deﬁnition 2 (cluster). : Let S be a set of tokens. S is a cluster if ∀tk ∈ S ∃tk ∈ S
such that: ∀tk ∈ S |center(tk ) - center(tk )| ≤ |center(tk - center(tk )|.
We can partition the entire set of tokens into clusters as shown in Figure 2. In
the ﬁgure, tokens t1 , t4 , t5 and t6 belong to cluster C1 whereas tokens t2 , t3 , t7 and
t8 belong to cluster C2 . It can be veriﬁed that the two clusters satisfy deﬁnition 2.

4

10

20

t1

25
t2

23
1

5 6
t4

9 10
t5

13
t6

18
t7

22

28
t3
25
t8

28

Figure 2. Example of Clusters

Observe in Figure 2 that the positions of tokens in C1 precede those in C2 .
We can hence impose a linear order on the set of all clusters in a partition. This
ordering will be based on certain “extremal” tokens in the clusters. We now set up
the concepts necessary to deﬁne this order. First observe in Figure 2 that we can
identify certain tokens as being at the boundaries of a cluster.
In the following deﬁnitions t, tl and tr are tokens belonging to the same cluster
Ci .

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

319

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Deﬁnition 3 (boundary tokens). : Let Ci denote a cluster and j be some row.
Then,

 undef ined,
t,
border(i, j) =

tl , tr ,

if there are no tokens in the row.
if there is only one token t in the row.
s.t.  ∃ tokens tl , tr ∈ Ci ∧ row j with start(tl ) > start(tl )∧ end(tr ) < end(tr ).

In Figure 2, the boundary tokens for C1 are t1 , t4 and t6 . We can classify all
the tokens in a table unambiguously as either boundary or non-boundary tokens.
Since every token in a row has unique start and end positions we can deﬁne the
rightmost and leftmost token of a row in a cluster as:
Deﬁnition 4. Let Ci denote a cluster and j denote a row index. Then,

undef ined,
 ∃ a border token t with row(t) = j.
rightmost(i, j) =
t,
t ∈ border(i, j) ∧  ∃t ∈ border(i, j) s.t. end(t ) > end(t).

lef tmost(i, j) =

undef ined,
t,

 ∃ a border token t with row(t) = j.
t ∈ border(i, j) ∧  ∃t ∈ border(i, j) s.t. start(t ) < start(t).

For cluster C1 in Figure 2, the rightmost (leftmost) tokens are t1 (t1 ) for the
ﬁrst row, is undeﬁned for the second row and is t6 (t4 ) for the third row. Note,
the rightmost(i, j) or the leftmost(i, j) may not be the spatially rightmost token or
leftmost token for the j th row of Ci . The additional requirement is that they be
border tokens. We now deﬁne the extremal tokens of a cluster.
Deﬁnition 5 (extremal tokens). : For a cluster Ci the rightmost token is deﬁned as:
rightmost(i) = rightmost(i, j) ∧  ∃j  s.t. rightmost(i, j  ) ∧ end(rightmost(i, j) <
end(rightmost(i, j  )).
leftmost(i) = leftmost(i, j) ∧  ∃j  s.t. leftmost(i, j  ) ∧ start(leftmost(i, j) >
start(leftmost(i, j  )).
In Figure 2, t6 is the rightmost token of C1 and t4 is its leftmost token. We can
pick one of the extremal tokens consistently (say the leftmost) from every cluster
and use their start positions to linearly order all the clusters in the partition. We will
henceforth assume that tokens are partitioned into clusters C1 , C2 , . . . , Cn where if
i < j then Ci occurs before Cj in the order. In Figure 2 we can pick the leftmost
tokens in the two clusters and use their start positions to place C1 before C2 .
We say that Ci and Cj are adjacent if there is no other cluster between
them. The span of a cluster Ci , denoted span(Ci ), is the sequence of positions
<start(leftmost(i)), start(leftmost(i)) + 1, ...., end(rightmost(i))>. In Figure 2,
span(C1 ) is <1,2,...,13>.
We are now ready to deﬁne the important notion of “closeness” between clusters. Our notion is based on “averaging” the length of inter-cluster gaps between
adjacent clusters using their boundary tokens.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

320

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

We associate a gap between Ci and Ci−1 , denoted gap on left(i) and a gap between Ci and Ci+1 , denoted as gap on right(i). The gap on right and the gap on left
are deﬁned in terms of intrarow gaps as follows.
Deﬁnition 6 (intrarow sep).
The gap between cluster Ci and Ci+1 in the j th row, denoted,

start(lef tmost(i + 1, j)) − end(rightmost(i, j)),




if both leftmost(i + 1, j) and rightmost(i, j) are deﬁned.




 max(0, start(lef tmost(i + 1, j)) − end(rightmost(i))),
if rightmost(i, j) is undeﬁned but leftmost(i + 1, j ) is deﬁned.
intrarow sepr (i, j) =


max(0,
start(lef
tmost(i + 1)) − end(rightmost(i, j))),




if
leftmost(i
+ 1, j) is undeﬁned but rightmost(i, j) is deﬁned.



0,
if both are undeﬁned.
The gap between cluster Ci and Ci−1 in the j th row, denoted,

start(lef tmost(i, j)) − end(rightmost(i − 1, j)),




if both leftmost(i, j) and rightmost(i - 1, j) are deﬁned.




 max(0, start(lef tmost(i, j)) − end(rightmost(i − 1))),
if rightmost(i - 1, j) is undeﬁned but leftmost(i, j ) is deﬁned.
intrarow sepl (i, j) =


max(0,
start(lef
tmost(i)) − end(rightmost(i − 1, j))),




if
leftmost(i,
j) is undeﬁned but rightmost(i - 1, j) is deﬁned.



0,
if both are undeﬁned.
Informally, the intrarow gap between Ci and Ci+1 refers to the gap between a
pair of extremal tokens that occur in the same row. We have to take into account
the cases where both tokens exist in both the clusters, only one of them exists in
one of the clusters and neither exist. In the last case the gap is considered to be
undeﬁned. In the case when only one of them exists the closest token is chosen.
This choice is made to ensure that the gap is always minimal. Note that no gaps
exist between overlapping tokens.
Deﬁnition 7 (gaps). 
gap on right(i) is ( row gap on right(i, j)) / M, where M is the number 
of
rows of the cluster where row gap right(i, j) > 0. Similarly, gap on left(i) is (
row gap on left(i, j)) / M.
In Figure 2, let us consider the three clusters C1 consisting of the token t4 ,
C2 consisting of the tokens t1 and t5 and C3 consisting of the token t6 . For C2 ,
the intrarow sepl (2, 1) = max(0, start(t1 ) - end(t4 )) = 0, the intrarow sepl (2, 2)
= 0 (both undeﬁned) and intrarow sepl (2, 3) = start(t5 ) - end(t4 ) = 1. Thus, the
average inter cluster gap between C1 and C2 , gap on left(2) = intrarow sepl (2, 3)
) / 1 = 1. The gap on right for any cluster is also calculated in a similar way.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

321

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

2.2

Building blocks

We sketch a high-level overview of our clustering algorithm. It has very close parallels to iterative partition reﬁnement algorithms. Starting with an initial partitioning
of the set of tokens into clusters, the partition gets reﬁned in every iteration. Reﬁnement amounts to creating larger clusters by merging adjacent ones based on
inter-cluster gaps. The algorithm terminates when no more reﬁnement is possible.
We will now construct the essential building blocks for our algorithm, namely,
creating the initial partition, merging of clusters in an iteration and termination.
Initial partitioning The initial partition puts all tokens with a high-degree of overlap into one cluster. Intuitively what this means is that all such tokens belong to
the same column. Formally:
Deﬁnition 8 (overlapping tokens). : Tokens ti and tj overlap whenever one of
the following holds:
1. start(ti ) ≤ start(tj ) ≤ end(ti ).
2. start(ti ) ≤ end(tj ) ≤ end(ti ).
3. start(tj ) ≤ start(ti ) and end(tj ) ≥ end(ti )
In Figure 2, tokens t2 and t3 overlap. We use this notion to deﬁne:
Deﬁnition 9 (high degree of overlap between tokens). : Tokens ti and tj ,
with centers ci and cj respectively, have a high degree of overlap whenever:

 cj
ci = cj + 1

cj − 1
In Figure 2, tokens t1 and t5 have a high degree of overlap between them since
center(t1 ) = 7 and center(t5 ) = 8. From deﬁnition 9 it is easy to see that the centers
of tokens with a high-degree of overlap occupy consecutive positions. To create the
initial set of clusters, we add tokens whose centers are on consecutive positions to
the same set. It is easy to see that each such set is a cluster.
Merging clusters We say that adjacent clusters Ci and Ci−1 are “mutually close”
whenever (gap on left(i) < gap on right(i)) ∧ (gap on right(i - 1) < gap on left(i 1)). We only merge mutually close clusters into a larger cluster. Once we merge
the pair of clusters into a bigger cluster we will have to associate gaps with this
bigger cluster. If Ci is merged with Ci−1 into the cluster C then gap on left(i-1)
and gap on right(i) become C’s left and right gaps respectively. On the other hand
if Ci is merged with Ci+1 then gap on left(i) and gap on right(i+1) become left and
right gaps respectively of cluster C.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

322

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

However even when adjacent clusters are mutually close to each other we do
not merge them if they belong to diﬀerent columns. How do we identify such a case
since we are not provided with any information about columns? We use headers
for this purpose. We assume that every column is associated with a distinct header
string. The span of a header, analogous to that of a cluster, is the sequence of the
positions associated with the characters in its string. In Figure 1, the span of the
header “SECURITIES” is <3, 4, 5,..., 12 >. We say that a cluster is distinctly
associated with a header when their spans overlap. We do not merge mutually close
clusters whenever they are uniquely associated with their headers. In such a case
we say that the clusters belong to diﬀerent columns.
Termination Note that in a iteration we may fail to merge a pair of adjacent
clusters because:
1. They are not mutually close.
2. They belong to diﬀerent columns.
If in an iteration we are unable to merge any pair of clusters the algorithm terminates.

2.3

Putting it All Together

We present here Algorithm Extract Columns using the building blocks described
above. Extract Columns scans the rows of the table as it’s input. It will produce
as the result an association of the items in the table with columns. Recall that two
adjacent clusters are merged when they are mutually close. To facilitate merger we
maintain a variable link with each cluster Ci . During the iteration, link(Ci ) points
to Ci−1 (Ci+1 ) if Ci is closer to Ci−1 (Ci+1 ). In the algorithm, we use Gi to denote
the gap between Ci and Ci+1 i.e. Gi = gap on right(Ci ) = gap on left(Ci+1 ). The
procedure Initial Clusters creates the initial partition of tokens into a set of clusters.
After that clusters overlapping with the same header are merged together. The procedure Inter Cluster Gaps computes the gaps between adjacent clusters using the
initial partitions. The procedure Merge Clusters merges mutually close clusters
subject to the condition that their spans do not overlap with two diﬀerent headers.
The merged cluster produced contains the union of the tokens of the two clusters
and if either of them is associated with a header then the resulting cluster is also
associated with the same header.
For example, in Figure 2 the initial set of clusters are C1 = t4 , C2 =t1 , t5 , C3
= t6 , C4 = t7 , C5 = t2 , C6 = t3 , t8 . Thus, G1 = 1, G2 = 1, G3 = 5, G4 = 0 and
G5 = 0
In the ﬁrst iteration, link(C1 ) = C2 , link(C2 ) = C1 , link(C3 ) = C2 , link(C4 ) = C5 ,
link(C5 ) = C4 and link(C6 ) = C5 . So we merge clusters C1 and C2 and clusters
C4 and C5 . Our new set of clusters are C1 = merge(C1 , C2 ), C2 = C3 , C3 =
merge(C4 , C5 ) and C4 = C6 . Our new set of intercluster gaps are G1 = G2 , G2
= G3 , G3 = G5 . In the second iteration, link(C1 ) = C2 , link(C2 ) = C1 , link(C3 )

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

323

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

= C4 and link(C4 ) = C3 . Thus, we merge clusters C1 and C2 and clusters C3
and C4 . Our new set of clusters are C1 = merge(C1 , C2 ) and C2 = merge(C3 ,
C4 ). Our new intercluster gap is G1 = G3 . In the third iteration we reach a ﬁxpoint since we cannot merge these two clusters as they overlap with two diﬀerent
headers. Thus, the algorithm reaches a ﬁxpoint and gives the correct set of clusters.
AlgorithmExtractColumns
begin

1. Invoke Initial Clusters to form the set of initial clusters C = {C1 , C2 , ...., Cn }
2. Invoke Inter
 Cluster
 Gaps to compute the gaps between the initial clusters, Gap = {G1 , G2 , ...., Gn−1 }
3. while(|
|=
 | C |) do
(* a ﬁxpoint check is done *)
C

4
C =
C
5
Gap = Gap

6.
forall i, 1 ≤ i ≤ | C | do
7.
link(Ci ) = nil
8.
end

9.
forall i, 1 ≤ i | C | - 2 do
(* for each cluster the gaps between *)
10.
if(Gi ≤ Gi+1 ) then
(* it’s neighbors are compared *)
11.
link(Ci+1 ) = Ci
(* and the closeness of that cluster is determined *)
12.
else
13.
link(Ci+1 ) = Ci+2
14.
endif
15.
end
16.
link(C1 ) = C2
17.
link(C| C | ) = C| C |−1
18.
j=1

19.
forall i, 1 ≤ i ≤ | C | - 1 do
(* mutually close clusters are merged *)
20.
if(link(Ci ) = right & link(Ci+1 ) = left) then
(* together to form a single cluster *)
21.
Cj = Merge Clusters(Ci , Ci+1 )
(* if they do not overlap with diﬀerent headers *)
22.
Gj = Gi+1
23.
i=i+1
24.
else
25.
Cj = Ci
26.
Gj = Gi+1
27.
endif


28.
(* the new set of clusters are created *)
C =
C ∪ Cj
29.
Gap = Gap ∪ Gj
(* gaps are associated with this new set *)
30.
j=j+1
31.
end
32. endwhile

33. return C
end

2.4

Algorithmic Properties

The objective of the underlying algorithm ExtractColumns is to correctly associate
items with a column. Can it always do so? Herein we address the question of a
correct extraction of columns w.r.t. a table whose columns have been already demarcated by a user. This demarcation represents the “correct extraction” expected
by the user. The formalization proceeds as follows.
The span of a column is the contiguous sequence of positions from the start of
it’s leftmost token to the end of it’s rightmost token. One can conceptually think

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

324

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

of the span as two imaginary lines such that the span of no token in the column
extends beyond these two lines. Span of a column is a purely visual concept. Using
columns we deﬁne:
Deﬁnition 10 (table). : A table is a partition of the tokens into a set of columns
Col1 , Col2 , ..., Coln such that there is a distinct header associated with every
column (i.e. their spans overlap) and a token t ∈ Coli ⇐⇒ center(t) is contained
in the span of Coli .
Note that this deﬁnition allows spans of adjacent columns to overlap so long
as the centers of the shared tokens belong to only one of the columns. In Figure 1, spans of columns 1,2,3,4 and 5 are < 3,4,...,46 >, < 45,..,54 >, < 55,..,64 >,
< 68,69,...,80 > and < 83,84,...,92 > respectively. The headers “SECURITIES”,
“RATE”, “MATURITY”, “AMOUNT” and “BASIS” are associated with Columns
1,2,3,4 and 5 respectively.
In the following, we will assume that T denotes a table whose columns have
been demarcated by the user. Suppose the rows of a table T are supplied as input
to ExtractColumns and it produces the clusters C1 , C2 , . . . , Cn as it’s output upon
termination. We say that this extraction is correct w.r.t. T iﬀ every Ci is uniquely
associated with a header and there is a bijective mapping between the columns of
T and the extracted clusters. We can guarantee the following:
Theorem 1. If in every iteration ExtractColumns only merges mutually close
clusters belonging to the same column of T then it will always produce a correct
extraction of the table T.
We now characterize tables for which ExtractColumns will always yield a
correct extraction. We will base this characterization on the gaps between nonoverlapping tokens in a column. In the deﬁnitions below let p,q,q denote tokens.
Deﬁnition 11 (token gaps). The gap between token p and some other token q
(both p and q in the same row j) denoted,

start(q) − end(p), start(q) > end(p) ∧  ∃q in j s.t. start(q) < start(q).
intra row gap(p) =
undef ined,
otherwise.
The gap between token p and some other token q not in the same row denoted,

 start(q) − end(p), start(q) > end(p) ∧ intra row gap(p) is undeﬁned
∧  ∃q s.t. end(p) < start(q) < start(q).
inter row gap(p) =

undef ined,
otherwise.
Let columngap(i) denote the maximum over all intra row gaps and inter row gaps
of tokens in column i. Furthurmore, let min columngap denote the smallest inter
column gap.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

325

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

H1

H2

t1

t4

t7

t2

t5

t8

t3

t6

t9

C2

C1

C1’

C2’
Figure 3. Illustration of Theorem 2

Theorem 2 (Characterization of tables). For every column i in T, if columngap(i) <
min columngap then ExtractColumns will produce a correct extraction for T.
Proof sketch: By contradiction. Assume ExtractColumns terminates with an
incorrect partitioning. This means that in some iteration, a cluster Ci was merged
with Ci+1 while it should have been merged with Ci−1 . Let the gap between Ci and
Ci−1 be di−1 and the gap between Ci and Ci+1 be di+1 . Since, Ci is (incorrectly)
merged with Ci+1 this means that di+1 < di−1 . However, since Ci and Ci−1 actually
belong to the same column di−1 ≤ columngap(i). Also, since in reality Ci and Ci+1
belong to diﬀerent columns di+1 ≥ min columngap. Using our characterization
theorem we can derive, di−1 ≤ columngap(i) < minc olumngap ≤ di+1 . Thus,
di−1 < di+1 and we get a contradiction. Therefore our assumption is incorrect and
we always produce a correct extraction.
For example, in Figure 3, if the user deﬁnes table T  with columns C1 and

C2 then our characterization enables us to decide a priori that Extract Columns
will not generate a correct extraction w.r.t. T  . This is because the gap between
tokens t2 and t5 (both belonging to column C1 ) is more than the gap between C1
and C2 violating the condition of Theorem 2. If however the user deﬁnes table T
with columns C1 and C2 then our output will be a correct extraction w.r.t T.
Note that characterization of tables on which the algorithm is guaranteed to
produce a correct extraction is based on the column spans. This information can
only be supplied externally as it is not computed by our algorithm. So it may
appear that such a characterization may not be useful. However we describe how
it can be used in practice especially where the tables are generated automatically
such as ﬁnancial and product data. In these cases all of the tables are “structurally
similar”. We can hence sample a few tables and determine whether they satisfy our
characterization. In such a case we can run the algorithm over all such tables in

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

326

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

a batch knowing a priori that the extraction will be correct. In fact this has been
validated in our work with the data provided to us by a ﬁnancial company and
also tables containing electronic part descriptions supplied to us by an electronic
component catalog manufacturer. We elaborate on this idea in the next section.

3

The Extraction System based on Clustering

We implemented a extraction system for text tables based on our clustering algorithm. It is written in Java and the entire system is approximately 3000 lines of
code.
The clustering algorithm assumes that each column is associated with a unique
header. Each header is a string consisting of one or more words. Typically the text
table can be logically separated into two consecutive regions, namely the header
region consisting of all the headers followed by the data region. The header region
may span multiple lines and the two regions are separated by special tokens which
may include blank lines. We assume that the user supplies the list of keywords
appearing in the headers as well as the separator tokens. Our system must ﬁrst
discover the headers associated with the columns. This can become quite subtle
when the separator tokens are interspersed with the headers. We have developed
an algorithm to do header discovery whose details are beyond the scope of this
paper.
Below we now describe some of the key features of our system:
Look and Feel: The system can be set up to do automatic extraction over a
collection of text tables. It has a graphical user interface which is divided into four
panels. To use this system, an initial setup prior to extraction is required. This
consists of:
1. Loading the input ﬁles containing the tables. There is an input panel in the
GUI where the user can load up a set of ﬁles each containing a text table or
an entire directory containing all the ﬁles.
2. Specifying the destination directory where the output tables will be generated.
There is an output panel in the GUI where the user can specify this location.
3. The user speciﬁes the headers of columns to be extracted by their header
name. We have a panel in our GUI for loading up these header names. The
user can provide a list of all the header names that can possibly occur across
all tables. The user can also enter phrases as keywords besides providing the
union of all the keywords.
4. Tokens which serve as separators between headers and data, like a blank line
(newline) or special characters like ” ”, ”-”, etc, are provided as separator
keywords between the headers and the data. There is a panel where the user
enters the separators.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

327

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Error Handling: During an extraction run the system partitions all the text ﬁles
into two categories, namely, those it considers have been correctly extracted and
those in which errors were detected. It can readily detect certain types of errors.
These are:
1. Header mismatch: This error occurs whenever either the column corresponding to a header is empty or the conﬁguration parameter is underestimated.
The latter will prevent clusters consisting of items belonging to the same
column from being merged thereby creating an erroneous number of columns.
2. Failure of header discovery: The algorithm for identifying headers cannot
detect the boundary separating the header and data regions in the text table.
3. Absence of any user supplied headers: The text table does not contain any of
the headers supplied by the user.
During a run the system further partitions tables in which errors were detected
into one of the above three sub-categories.
Conﬁguration: A run of the system amounts to doing extraction over a collection
of text tables. For correct extraction we need to know the minimum column gap (see
Theorem 2). We discuss now how we can get an estimate of this parameter. The
system is used interactively by the user to sample a few text tables and estimate the
minimum column gap. The estimated gap becomes a conﬁguration parameter of the
system. The algorithm uses this parameter when merging clusters. In particular it
will not merge adjacent clusters if the gap between them is larger than the estimated
minimum column gap. (The basis for this step is Theorem 2.) The system will use
this parameter during a run. It partitions the results of the run into correct and
erroneous sub-categories (see “error handling” describe above). At the end of a
run the user can examine the text tables on which the extraction failed, identify
if it was the caused by an erroneous estimate, collect all such cases, readjust the
parameter and start a new run on them. This process can help the user obtain a
high extraction yield. Finally we remark that the estimated minimum column gap
cannot be used as a ﬁxed separator since this would determine whether data items
belonged to the same column based only on examining the items on adjacent rows,
making it brittle to misalignments.
Performance Measurements We have tested our system using text tables consisting of ﬁnancial and electronics parts data. A total of 516 input ﬁles, each consisting
of a text table, were loaded up and the algorithm was run in batch over this entire
collection. Out of these, 426 tables were properly extracted representing a yield
of 83% while 74 ﬁles suﬀered from the header mismatch problem (14%), 11 ﬁles
suﬀered from the failure of header discovery problem (2%) and 5 ﬁles didn’t have
the headers supplied by the user (1%). The set of input ﬁles was automatically
partitioned into the correct and incorrect categories and the latter was further partitioned into the three sub-categories. The system took a total of 15 minutes to
complete the extraction over this data set. A sample the output generated by our

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

328

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

system (loaded in Microsoft Excel) on the table in Figure 1 is shown in Figure 4.
The system provides a facility to generate diﬀerent output formats from an intermediate representation of the table.

4

Related Work

The importance of table extraction from documents has been recognized in [2].
The work in [5] proposes a text block detection methodology that depends not
only on the spatial layout of documents but its linguistic content as well. The text
block merging algorithm in that work is based on setting parameters such as the
minimum distance between blocks to merge. It is mentioned that such settings
can be a source of errors during column extraction. Our approach is essentially
syntactic. We use a single user-speciﬁed parameter, the minimum column gap,
which can be adjusted based on automatically identiﬁed incorrect extractions and
the system rerun on them to get higher yield. The work of [7] is based on feature
extraction and adaptation of learning techniques for automatically recognizing table
boundaries and its rows and columns. The algorithm requires training from a set
of sample documents and yields a specialized table extractor with high accuracy on
similar documents. Firstly this approach does not yield a generic table extractor
such as ours. Secondly, since columns are extracted by detecting certain vertical
lines as column separators, it would fail to extract correctly when columns overlap.
In contrast our clustering technique can handle overlapping columns. The extraction
framework of [6], depends on manual construction of a domain dependent lattice of
regular expressions. The algorithm depends on various empirical thresholds which
are not easy to discover. Moreover the approach does not appear robust especially
when columns can have elements missing in some of its rows. Further it is not clear
how regular expressions can handle column elements that span multiple words and
data values of diﬀerent types. Work that can be most closely related to our system
are Nodose and Tintin. Nodose is an interactive system for semi-automated data
extraction from documents [1]. It utilizes features like beginning and ending marker
keywords or ﬁxed oﬀsets to generate extraction wrappers. As far as table extraction
is concerned it can only deal with tables which have ﬁxed column separators. Tintin
is a system for retrieving text tables [8]. Its main focus appears to be querying and
indexing of the extracted data. In contrast we assume that the extracted data will be
used to populate a traditional database system which can be queried using standard
database techniques. As far as table detection and column extraction is concerned,
Tintin uses heuristics to extract both the table and its column data. Speciﬁcally
for column extraction it tries to discover a gap which runs across all the rows of
the table. This approach suﬀers from two drawbacks. Firstly it can erroneously
split a column consisting of multi-word items into two columns. Secondly it is
brittle in the sense that it will not be able to detect column boundaries when such
a gap does not exist due to misalignments. While we do not address the question
of table detection as is done in the Tintin system, we discuss in the next section
how clustering ideas can be used for developing robust table detection algorithms

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

329

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Figure 4. Excel Output of Extract Column on the example table in Figure 1

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

330

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

also. Clustering algorithms have been extensively investigated in machine learning,
image processing and data mining disciplines. While clustering algorithms in these
areas can be broadly viewed as an optimization technique, our algorithm is geared
towards doing a correct extraction of table which is a novel application of clustering.
It will be interesting to investigate the connections between optimizing the measures
we have used and correct extraction.

5

Concluding Remarks

We have proposed a clustering technique for extracting tabular data from text tables. We have obtained a extraction yield of over 80% on over 500 tables containing
ﬁnancial and parts data on the very ﬁrst run. We were able to increase the yield
to over 90% by readjusting the conﬁguration parameter, i.e. the estimated column
gap and re-running the extraction process on those tables which in the ﬁrst run
exhibited header mismatch because of underestimating of the gap value.
On the implementation side it will be useful to integrate an automated table
detection algorithm into our system. It is possible to extend the clustering ideas
described in this paper to detect tables embedded in arbitrary text. The main idea
is to deﬁne a closeness metric between rows using the spatial location of gaps in the
rows. Based on this metric and the observation that rows in a table are clustered
together, we can use our iterative clustering algorithm to detect tables also.
Finally, note that we have used a static measure of gaps between clusters. It
would be interesting to investigate if we can identify a dynamic measure of gaps
and merge clusters based upon that. Such a measure might yield correct extractions
over a larger class of tables.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

331

Downloaded 06/10/17 to 149.169.125.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Bibliography
[1] Brad Adelberg. Nodose - a tool for semi-automatically extracting semistructured data from text documents. In ACM SIGMOD Conference on Management of Data, pages 283–294, 1998.
[2] Douglas Appelt and David Israel. Tutorial notes on building information extraction systems. In Fifth Conference on Applied Natural Language Processing,
1997.
[3] Daniel Fasulo. An analysis of recent work on clustering algorithms. 1999.
[4] Jeﬀrey Friedl. Mastering Regular Expressions. O Reilly, 1997.
[5] Matthew Hurst. Layout and language: An eﬃcient algorithm for detecting text
blocks based on spatial and linguistic evidence. In Document Recognition and
Retrieval VIII, 2001.
[6] Stephen W. Liddle, Douglas M. Campbell, and Chad Crawford. Automatically
extracting structure and data from business reports. In Proceedings of the Eighth
International Conference on Knowledge Management, pages 86–93, 1999.
[7] H. Ng, C. Lim, and J. Koo. Learning to recognize tables in free text. In
Proceedings of the 37th Annual Meeting of ACL, pp. 443-450., 1999.
[8] Pallavi Pyreddy and W. Bruce Croft. Tintin: A system for retrieval in text
tables. In ACM Digital Libaries Conference, 1997.
[9] Ian H. Witten and Eibe Frank. Data Mining. Morgan Kaufmann Publishers,
1999.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

332

Boosting Item Keyword Search with Spreading Activation
Dipti Aswath, Syed Toufeeq Ahmed, James D’cunha, Hasan Davulcu
Dept. of Computer Science, Arizona State University, Tempe, AZ, USA.
{dipti, toufeeq, james.dcunha, hdavulcu}@asu.edu
Abstract
Most keyword search engines return directly matching
keyword phrases. However, publishers cannot anticipate
all possible ways in which users would search for the
items in their documents. In fact, many times, there may
be no direct keyword match between a keyword search
phrase and descriptions of relevant items that are perfect
matches for the search. We present an automated, high
precision-based information retrieval solution to boost
item find-ability by bridging the semantic gap between
item information and popular keyword search phrases.
Our solution achieves an average of 80% F-measure for
various boosted matches for keyword search phrases in
various categories.
KEYWORDS:
E-commerce, Data Mining, Web Data, Information
Retrieval, Information Extraction, Spreading Activation,
Support Vector Machines.

1. Introduction
Most search engines do their text query and retrieval
using keywords. Recent research [1] found that 40
percent of companies rate their search tools as “not very
useful” or “only somewhat useful.” Further, a review of
89 sites [1] found that 75 percent have keyword search
engines that fail to retrieve important information and put
results in order of relevance; 92 percent fail to provide
guided search interfaces to help offset keyword
deficiencies [1], and seven out of 10 web shoppers were
unable to find products using the search engine, even
when the items were stocked and available.
Problem Definition: Search engines or web-publishers
cannot anticipate all possible ways in which users search
for the items in their documents. In fact, many times,
there may be no direct keyword match between a search
phrase and descriptions of items that are perfect “hits” for
the search. For example, if a shopper uses “motorcycle
jacket” then, unless the publisher or search engine knows
that every “leather jacket” is a “motorcycle jacket”, it
cannot produce all matches for user’s search. Thus, for
certain phrases, there is a semantic gap between the
search phrase used and the way the corresponding
matching items are described. A serious consequence of

this gap is that it results in unsatisfied customers. Thus
there is a critical need to boost item find-ability [3] by
bridging the semantic gap that exists between search
phrases and item information. Closing this gap has the
strong potential, for example, to translate web search
traffic into higher conversion rates and more satisfied
customers.
Issues in Bridging the Semantic Gap: We denote a
search phrase to be a “target search phrase” or a “hot
phrase”, if does not directly match certain relevant item
descriptions [3]. The semantics of items matching such
phrases is implicit in their descriptions. For phrases with
fixed meanings i.e. their connotations do not change such
as in “animal print comforter”, it is possible to close the
gap by extracting their meaning with a thesaurus [9] and
relating it to product descriptions, such as “zebra print
comforter” or “leopard print bedding” etc. Where they
pose a more interesting challenge is when their meaning
is subjective, driven by perceptions, and hence their
connotations change over time as in the case of
“fashionable handbag”. The concept of a fashionable
handbag is based on trends, which change over time.
Bridging the semantic gap therefore is in essence the
problem of inferring the meaning of search phrases in all
its nuances.
Our Approach: A two level spreading activation
network activates and identifies strong positive and
negative phrases related to the matches of a given
keyword search phrase, which in turn activates other
potentially relevant products in addition to those that are
exact keyword matches for the search term itself. Next,
we identify all products that do not match any of the
highly activated phrases and use them as strongly
negative mismatches. A SVM classifier is trained, using
these strong positive and negative matches of a search
phrase, to separate the rest of the matches from
mismatches.
In next section we discuss related work. In Section 3,
the Spreading Activation and Classification framework is
presented. In Section 4, we present the experimental
results and evaluation method and Section 5 concludes
the paper.

Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI’05)
0-7695-2415-X/05 $20.00 © 2005 IEEE

2. Related Work
In [7] linguistic analysis is employed to mine the
descriptions of phrases. This approach restricting
question types to descriptions of noun phrases, fails to
work well with domains where the target phrase does not
associate itself within the context of the above domains.
As feature selection strategies are critical in
classification tasks, [12] shows that with two-sided
metrics such as Information Gain (IG) and Chi-Square
(CHI), the values of positive features are not necessarily
comparable to those of the negative ones and hence
cannot ensure an optimal combination.
The framework presented in Section 3 is similar to
that used by the Positive Example Based Learning
(PEBL) algorithm for classification of web pages,
through the use of positive and unlabeled examples [11].
[5] proposes a method for measuring semantic similarity
between words, with the similarity being computed on a
semantic network constructed systematically from a
subset of the English dictionary.

3. Spreading Activation and Classification
The proposed algorithm runs in two stages and
identifies additional relevant matches in a 2-level
spreading activation stage and a classification stage. In
the spreading activation stage, a network is activated as a
set of nodes representing product descriptions or phrases
(i.e., indexing terms of the product descriptions) with
relationships between the nodes specified by labeled
links. The 2-level node activation process starts in one
direction placing an initial activation weight on the hot
phrase node, activating product description and phrase
nodes relevant to the hot phrase. In the other direction,
the network originating with the hot phrase node activates
its synonym nodes that in turn activate their relevant
product and phrase nodes. Relevant phrases thus defining
the query phrase identify strong positive and negative
phrases together with positive and negative instances. In
the classification stage, a trained SVM classifier
classifies activated relevant product nodes as either
positives or negatives. Positives thus classified; serve as
additional relevant “hits” for the query phrase.

and relevant terms or items that are matches to the
original query itself.

Figure.1. Two Level Spreading Activation Network,
illustrating a feed-forward network since activation always
spreads in the increasing order of levels below and above
the hot phrase. Circular notation denotes activated phrase
nodes and the box indicates activated product nodes.

Typically the activation weight of a node is computed
as a function of the weighted sum of the inputs to that
node from directly connected nodes. If aj is the original
activation weight of node j, and wij is the link weight
between nodes i and j, representing the influence of node
j on node i, the new activation weight ai on node i is
computed as [6]:

Our 2-level spreading activation network model Ø1
activates phrase sets relevant to the query phrase Q and
its synonyms, Pf1 and Pf2. Activation spreads activating
product descriptions R relevant to Pf1 and Pf2; and hence
related to Q.A 2-level network was selected since an
initial experimental evaluation returned no product
descriptions common between immediate products
activated by the hot phrase and those activated by its
synonyms.

3.1. 2-level Spreading Activation Network (Ø1)
In information retrieval literature there has been an
extensive research in applying spreading activation
models to the IR problems [2]. This approach can be
distinguished from the other approaches in IR by the fact
that it represents queries, terms, documents, and their
relationships as a network of interconnected nodes, thus
expanding the matches of a query through new matching

Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI’05)
0-7695-2415-X/05 $20.00 © 2005 IEEE

Activation through the hot phrase: Level0 contains a
set of product description nodes activated by the keyword
query phrase. Product nodes in turn activate the phrase
nodes Pf1 indexing them at Level1, using Z-score
normalized activation weights.
Activation through the synonyms of the hot phrase:
Similarly synonym sets Pf2 for a given query phrase are
obtained from the WordNet [10] library and activated by
product nodes from Level2 to identify the most relevant
synonyms.

3.2. Extraction of Strong Positive / Negative
Instances and Phrases
Identification of positive product instances Pp is
carried out by extracting all product descriptions from the
given product descriptions set, that are exact matches of
the stemmed variations of the query phrase Q. We can
identify the strong positive phrases from U by checking
the frequency of the hot-phrase and synonym activated
phrase nodes Pf1and Pf2 obtained as a result of the 2-level
spreading activation model within the positive product
instances Pp as shown in figure 2. An experimentally
determined threshold is used in this selection of strong
positive phrases Pf from Pf1and Pf2. Negative product
descriptions Np in U are those that do not contain any of
the positive phrases Pf. A 1-level spreading activation
model commencing with Np activates negative phrase
nodes. Those nodes with activation weights exceeding an
experimentally determined threshold comprise the list of
strong negative phrases Nf.

3.3.1.

Rule based classification require the use of mutually
exclusive and exhaustive rules. Indirect method of
extraction of classification rules from models such as
decision trees, fail to retrieve such rules as test conditions
involve a single categorical attribute and hence decision
boundaries produced are “parallel” to the coordinate axes
and not maximal. Decision boundaries with large margins
tend to have better generalization errors than those with
small margins, as with small margins any slight
perturbations to the decision boundary can have a
significant impact on its classification [8]. Hence, it is
desirable to build linear classifiers e.g., linear SVM’s that
maximize the margins of their decision boundaries in
order to achieve better generalization performance.
Training phase of SVM involves estimating the
parameters w and b of the decision boundary such that the
following condition is met and the margin of its decision
boundary is maximum.
yi ( w . xi + b )  1, i = 1, 2 ,..n. [4, 8]
We train a SVM classifier from Pf, Nf (strong
positive and negative phrase sets) and Pp and Np using the
Radial Basis Kernels (RBF) with a fixed J . It has been
shown in [11] that Gaussian (RBF) kernels perform the
best as they draw flexible boundaries to fit a mixture
model by implicit transformation of the feature space as
against the polynomial kernels which may cause over fitting of training data as the degree of the polynomial
kernels grow. Through the use of Gaussian kernels, we
transform our data-sets into its higher dimensional space
using a transformation function Ø(x)[4]. With the
transformation, a linear decision boundary w. Ø(x) +b is
used to separate our test data set of activated relevant
products R into positive and negative matches for the
query phrase Q. Positive matches thus classified are
returned as perfect “hits” for Q.

4.

Figure2. Identification and subsequent activation algorithm

Ø1’ of Pf and Nf.

3.3. Learning from strong positive / negative
product instances and phrases with SVM
Support Vector Machines are used for classifying
activated relevant product descriptions R as either
positive “hits” for the given query phrase Q or as
negative products that would not be retrieved as matches
for Q.

Need for an SVM Classifier

Experiments and Evaluations

In this section, we present our spreading activation
and classification experimental results on hot phrases
coming from the three categorical datasets: shoes, rugs
and beddings. We report our results on our two
classification tasks, with the precision-recall metrics. The
performance of the SVM classifier improves with
datasets that are well defined. SVM classifier performed
exceptionally well on the category shoes as instances in
were easily identifiable as either sports shoes, formal
leather shoes or women’s sandals. Fairly good results
were obtained from rugs and beds categories.

Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI’05)
0-7695-2415-X/05 $20.00 © 2005 IEEE

Table1. Precision and Recall values for search phrases
related to shoes, rugs and bedding datasets.
Category Shoes

Running
Trail
Walking
Hiking
Casual
Fashion
Tennis
Basketball

Precision

Recall (%)

(TP)/
(Total AH)

%

326 / 387
468 / 534
1481 / 1810
66 / 80
1100 / 1309
1360 / 1625
58 / 79
45 / 63

84.23
87.64
81.82
82.50
84.03
83.69
73.41
71.48

Category Rugs

Precision
(TP)/
(Total AH)

Floral
Traditional
Classic
Modern
Contemporary
Unique

Category Beds

358 / 457
390 / 459
325 / 452
187 / 228
182 / 228
346 / 453

Crib
Toddler

50 / 58
12 / 14

70 / 100
73 / 86
59 / 77
16 / 21
232 / 289
288 / 289
33 / 42
47 / 72

6. References

%

70.00
84.88
76.77
76.19
80.27
99.00
78.57
65.27

Recall
%

78.33
84.96
81.90
82.01
79.82
76.34

Precision
(TP)/
(Total AH)

(TKP)/
(Total KP)

(TKP)/
(Total KP)

155 / 213
78 / 106
94 / 148
168 / 171
217 / 228
110 / 125

%

72.76
73.59
63.51
98.24
95.17
88.00

Recall
%

(TKP)/
(Total KP)

86.20 350 / 450
85.75 69 / 112

rugs” that should only match their occurrences.
Identifying such phrases is our future work.

%

83.35
61.20

TP: True Positive, Total AH: Total Activated Hits, Total
KP: Total Positives without Keywords, TKP: True
Positives without Keyword.

The evaluation of the presented algorithm on "adidas
shoes" and "shaw rugs" (brand names for shoes and rugs
category respectively), classify almost every relevant
product as false since every brand seemed to have its own
unique set of phrases, and those product instances that are
activated to be related to adidas shoes belong to different
brands and hence classify as false. For "red shoes"
almost every activated product is classified as true. We
even get a “heel shoe” and “sandal shoe” classified as
true as most of the strong positive features that appear in
“red shoes” would also appear in a “heel shoe” or a
“sandal shoe”.

5. Conclusion
Experimental results indicate that our techniques is
able to pick up robust strongly positive and negative
matches for a product and an SVM classifier is trained
using these strong items to retrieve more relevant
matches. However, our technique currently cannot flag
certain technical phrases such as “plastic boots” or “red

[1] W. Andrews, “Gartner Report: Visionaries Invade the 2003
Search Engine Magic Quadrant”, April 2003.
[2] F. Crestani, “Application of spreading activation techniques
in information retrieval”, Artificial Intelligence Review 11,
6(1997), Kluwer Academic Publishers Norwell, MA, USA, pp.
453-482.
[3] H. Davulcu, H. V. Nguyen and V. Ramachandran,“Boosting
Item findability : Bridging the semantic gap between search
phrases and item information”, ICEIS 2005.
[4] T. Joachims, “Text Categorization with Support Vector
Machines: Learning with Many Relevant Features”,
Proceedings 10th European Conference Machine Learning
(ECML ’98), 1998, pp. 137-142.
[5] H. Kozima, and T. Furogori, “Similarity between words
computed by Spreading Activation on an English Dictionary”,
Proceedings of the sixth conference on European chapter of the
Association for Computational Linguistics, Association for
Computational Linguistics Morristown, NJ, USA, 1993, pp.
232-239.
[6] J. Lee, and D. Dubin, “Context-Sensitive vocabulary
mapping with a spreading activation network”, Proceedings of
the 22nd annual international ACM SIGIR conference on
Research and development in information retrieval, ACM
Press, NY,USA, 1999, pp. 198-205.
[7] H. Nguyen, P. Velamuru, D. Kollipakkam, H. Davulcu, H.
Liu, and M. Ates, “Mining “hidden” phrase definitions from the
web”, APWeb 2003, pp. 156-165.
[8] Tan, P., M. Steinbach, and V. Kumar, Introduction to Data
Mining, Chapter 5. Support Vector Machines. In Publication.
Jan 21, 2005.
[9] E. M. Voorhees. Using WordNet for Text Retrieval. In
WordNet: An Electronic Lexical Database, Edited by Christiane
Fellbaum, MIT Press, May 1998.
[10] WordNet
bin/webwn

library.

http://wordnet.princeton.edu/cgi-

[11] H. Yu, J. Han, and K.C. Chang, “PEBL: Positive-Example
Based Learning for Web Page Classification Using SVM”,
Proceedings Eighth I’ntl Conf. Knowledge Discovery and Data
Mining (KDD), 2002, pp. 239-248.
[12] Z. Zheng, X. Wu, and R. Srihari, “Feature Selection for
Text Categorization on Imbalanced data”, ACM SIGKDD
Explorations Newsletter, Vol. 6(1), ACM Press, June 2004, NY,
USA, pp. 80- 89.

Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI’05)
0-7695-2415-X/05 $20.00 © 2005 IEEE

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

Functional “AJAX” in Secure Synchronous Programming
Supratik Mukhopadhyay
Louisiana State University
Baton Rouge, LA, 70803
supratik@csc.lsu.edu

Ramesh Bharadwaj
Naval Research Laboratory
4555 Overlook Avenue
Washington DC
ramesh@itd.nrl.navy.mil

Abstract
AJAX (Asynchronous Javascript and XML) is a confederation of technologies aimed at providing improved
user interaction with web-based applications.
While AJAX provides an improved user experience,
it also comes with its baggage of problems. The lack
of formal semantics makes AJAX applications difficult
to build, debug, understand, and validate. Different
component technologies of AJAX (e.g., XMLHttpRequest
or Javascript) are browser-sensitive and have different implementations and provide distinct functionalities.
Source code is downloaded and run on the clients machines, raising security concerns.
In this paper, we present an “AJAX”-like framework
in an event-driven secure synchronous programming environment. Our framework is supported by a formal
operational semantics. Applications written in our language can be verified using formal static analysis techniques such as theorem proving. The applications are
compiled and run on the SINS (Secure Infrastructure for
Networked Systems) infrastructure jointly developed in
collaboration with the Naval Research Laboratory.

1

Introduction

AJAX (Asynchronous Javascript and XML) is a collection of technologies aimed at providing improved
user interaction with web-based applications. Based
on the XMLHttpRequest API, AJAX includes an engine that invokes services asynchronously while the application interacts with the users. While AJAX provides an improved user experience, it also comes with
its baggage of problems. The lack of formal semantics makes AJAX applications difficult to build, debug,
understand, and validate. Different component technologies of AJAX (e.g., XMLHttpRequest or Javascript)
are browser-sensitive and have different implementa-

Hasan Davulcu
Arizona State University
Tempe, AZ
hdavulcu@asu.edu

tions and provide distinct functionalities. Source code is
downloaded and run on the clients’ machines, raising security concerns. Due to these problems, existing AJAX
frameworks are unsuitable for deployment in missioncritical enterprise applications.
In this paper, we present an “AJAX”-like “formal”
framework on an event-driven synchronous programming [3] language (a la LUSTRE [13], SCR [6], and
Esterel [4]). The components of our framework include the Secure Operations Language (SOL), a reliable
distributed infrastructure, called the Secure Infrastructure for Networked Systems (SINS), for running programs in SOL, and automated theorem provers (SALSA
[9]) for statically checking the correctness of SOL programs. We extend the synchronous programming language SOL with capabilities to handle asynchronous
service invocations and strong typing to ensure enforcement of information flow and security policies. In the
synchronous programming paradigm, the programmer
is provided with an abstraction that respects the synchrony hypothesis, i.e., one may assume that an external event is processed completely by the system before
the arrival of the next event. One might wonder how
a synchronous programming language such as SOL execute efficiently on widely networked systems where
there is inherent asynchrony. The answer may seem surprising to some, but perfectly reasonable to others: We
have shown elsewhere [10] that under certain sufficient
conditions (which are met in our case) the synchronous
semantics of a SOL application are preserved when it
is deployed on an asynchronous, distributed infrastructure. Whereas our framework incorporates “AJAX”-like
functionality in a synchronous programming language,
unlike AJAX, it is not a conglomeration of disparate
technologies. It is supported by formal operational semantics (see Section 5.2). The individual modules follow a “publish-subscribe” pattern of interaction while
asynchronous service invocations akin to XMLHttpRequest are given a continuation-passing-based semantics.
The design of SOL was influenced by SAL (the SCR

1530-1605/11 $26.00 © 2011 IEEE

1

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

Abstract Language), a specification language based on
SCR [14]. Applications written in SOL can be verified
by formal static analysis tools such as theorem provers
or model checkers. We provide a static type system
to ensure respectively (1) static type soundness and (2)
to ensure type soundness in the presence of third party
(possibly Commercial-off-the-shelf or COTS) services
that may undergo reconfiguration during runtime due
to network faults, system failures, or malicious attacks.
SOL programs are compiled to Java and run unmodified on SINS – an infrastructure developed in collaboration the Naval Research Laboratory that provides a
reliable messaging service on the top of a distributed
storage scheme that implements transactions, provides
eventual consistency guarantees, and is resilient to network faults. SINS can be interfaced with any Javacompatible browser; this makes our framework browserindependent as opposed to existing ones. A typical
SINS infrastructure comprises SINS Virtual Machines
(SVMs) running on multiple disparate hosts, each of
which is responsible for managing a set of modules on
that host. SINS provides the required degree of trust
for the modules, in addition to ensuring compliance of
modules with a set of requirements, including security
policies. The contribution of this paper is to bring an
“AJAX”-like pattern under a formal framework. This
will enable the deployment of formally verified interactive web applications in mission-critical scenarios.
The rest of the paper is organized as follows. Section 2 presents related work. Section 3 describes our
framework including the SOL language and its extensions for handling asynchronous service invocations.
Section 5 describes a static type system for SOL. Section 5.2 describes the operational semantics of SOL. The
paper is concluded in Section 6 that includes future research directions.

2

Related Work

AJAX provides an engine that acts as a client-side
brokerage and orchestration point for thick-client web
services and provides support for the XMLHttpRequest
API, XSLT, DOM and Javascript. Calls to the services
are handled asynchronously using the XMLHttpRequest API. Responses from the server are handled by
Javascript code running at the client end. Libraries for
AJAX are available for platforms such as .NET. In our
case, the SOL agents compiled to Java can be directly
deployed on the SINS platform that acts as a coordination point for the different agents and the services. We
have already outlined the shortcomings of the conventional AJAX framework in the Introduction and have
also outlined how our framework ameliorates them.
The nesC [12] programming language at U.C. Berke-

ley has been designed for programming networked embedded systems. It supports asynchronous calls to components using events to signify the completion of a
call. In the polyphonic C# [16] programming language,
asynchronous method calls are supported using queues.
A set of methods at the server end defines a “chord”.
A method call is delayed until all methods in the corresponding chord are invoked. The asynchronous service
invocation framework in our approach is reminiscent
of the “programming with futures” paradigm adopted
in languages like E [1], even though E adheres to the
capability-based computing paradigm rather than synchronous programming.
The communicating concurrent processes, the dominant paradigm for distributed application development,
has remained unchallenged for almost 40 years. Not
only is this model difficult to use for the average developer, but in addition it fails as a paradigm for designing
applications that must satisfy critical requirements such
as real-time guarantees [15]. Therefore, applications
developed using conventional programming models are
vulnerable to deadlocks, livelocks, starvation, and synchronization errors. Moreover, such applications are
vulnerable to catastrophic failures in the event of hardware or network malfunctions. Here we present an alternative approach. We embed an “AJAX”-like framework
in an event-driven synchronous programming environment (a’ la’ LUSTRE [13], SIGNAL [2] SCR [6], and
Esterel [4]). As opposed to other synchronous programming languages like ESTEREL, LUSTRE and SIGNAL,
SOL is a synchronous programming language for distributed web-based applications.
Preliminary versions of SOL and SINS have been introduced in [8, 5, 7]. The current paper extends those
versions by providing asynchronous service-invocation
management functionalities, type systems for safe information down grading and secure information flow. Besides, it provides operational semantics for the SOL language.

3

A Formal “AJAX”-like Framework

In this Section we develop the different components
of our framework. We start by describing SOL and its
extensions for handling asynchronous service invocations. We then briefly describe the SINS infrastructure.
The formal semantics of SOL are described in the next
section.

3.1

SOL: The Secure Operations Language

A module is the unit of specification in SOL and
comprises type definitions, flow control rules, unit declarations, unit conversion rules, variable declarations,

2

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

service declarations, assumptions, guarantees, and definitions. A module in SOL may include one or more
attributes. The attribute deterministic declares
the module as being free of nondeterminism (which is
checked by the SOL compiler). Attribute reactive
declares that the module will not cause a state change
or invoke a method unless its (visible) environment initiates an event by changing state or invoking a method
(service); moreover, the module’s response to an environmental event will be immediate; i.e., in the next step.
As defined previously, an agent is an instance of a SOL
module. In the sequel, we use module and agent interchangeably.
The module definition comprises a sequence of sections, all of them optional, each beginning with one or
more keywords. User-defined types are defined in the
type definitions section. Each entry in this section consists of an identifier for the type, followed by its
definition, which may be in terms of the built-in types,
their subranges, or enumerated types.
“Integer”, “Real”, “Boolean”, and “String” are the
built-in data types in SOL. User-defined types as well
as enumerated types can be defined in the type definitions section. Besides, this section allows the user to
declare “secrecy” types (e.g., secret, classified, unclassified etc.) in order to enforce information flow policies and prevent unwanted downgrading of sensitive information from “secret” variables to “public” variables.
The flow control rules section provides rules that govern
the downgrading/flow of information between variables
of different “secrecy” types (e.g., unclassified => classified, signifies that a variable of type unclassified can
be assigned to a variable of type classified, i.e., information flow from an unclassified to a classified variable
is allowed, while the rule unclassified + classified =>
classified, denotes that the “secrecy” type of an expression denoting the sum of an unclassified variable and a
classified variable is classified). The flow control rules
can be used to compute the secrecy types of expressions
from those of its constituent variables. If not specified
in the flow control section, information flow between
variables/expressions with different secrecy types is allowed only in the presence of explicit coercion provided
by the programmer. These policies are enforced statically by a static type system. The unit declaration section declares units for the physical quantities that the
module monitors and manipulates (e.g., lb, kg, centigrade etc.). This section provides conversion (coercion)
rules between the different units (e.g., kg=2.2 lb). Units
of expressions can be computed from the units of their
constituent variables. The variables of a module are declared under three sections – “monitored” variables are
variables in the environment that the module can read,
“controlled” variables are variables in the environment

that the module can control, and “internal” variables that
are introduced to make the module specification concise. A variable declaration can specify the units (declared in the unit declaration section) of the physical
quantity that its value represents (e.g., int weight unit
lb;). Assignment of a variable/expression with a unit U
to a variable with unit V is allowed only if this conversion is specified in the unit conversion rules section.
If such is the case, the value of the variable/expression
is converted to unit V using the corresponding conversion rule before being assigned to the corresponding
variable. The service declaration section specifies the
methods that may be invoked within the module along
with the services providing them. Also specified for
each method is the precondition that is to be satisfied
before the invocation of the method as well as the post
condition that the return value(s) from the method is/are
supposed to satisfy. The assumptions section includes assumptions upon which correct operation of the
agent depends. Execution aborts when any of these assumptions are violated by the environment resulting in
the failure variable corresponding to that agent to be set
to true. The required safety properties of the agent are
specified in the guarantees section. Variable definitions, provided as functions or more generally relations
in the definitions section, specify values of internal
and controlled variables. In this paper, we often distinguish between monitored variables, i.e., variables whose
values are specified by the environment, and dependent
variables, i.e., variables whose values are computed by
a SOL module using the values of the monitored variables. Dependent variables of a SOL module include
the controlled variables and internal variables. SOL also
provides type constructors such as arrays and tuples. In
this paper, we shall not elaborate on the tuple and array
constructs of SOL (see [17] for details).

3.2

Events

SOL borrows from SCR the notion of events [14].
Informally, an SCR event denotes a change of state, i.e.,
an event is said to occur when a state variable changes
value. SCR systems are event-driven and the SCR
model includes a special notation for denoting them.
The notation @T(c) denotes the event “condition c became true,” @F(c) denotes “condition c became false,”
and @C(x) the event “the value of expression x has
changed.” These constructs are explained below. In the
sequel, PREV(x) denotes the value of expression x in
the previous state.
@T(c)
@F(c)
@C(c)

def

=

¬PREV(c) ∧ c

def

=

PREV(c) ∧ ¬c

def

PREV(c) = c

=

3

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

Events may be triggered predicated upon a condition by
including a “when” clause. Informally, the expression
following the keyword when is “aged” (i.e., evaluated
in the previous state) and the event occurs only when
this expression has evaluated to true. Formally, a conditioned event, defined as
def

@T(c) when d = ¬PREV(c) ∧ c ∧ PREV(d),
denotes the event “condition c became true when condition d was true in the previous state”. Conditioned
events involving the @F and @C constructs are defined
along similar lines. Events trigger the evaluation of
guarded commands in a module; simultaneous events
can concurrently trigger multiple guarded commands.
Each controlled and internal variable of a module
has one and only one definition which determines when
and how the variable gets updated. All definitions of
a module m implicitly specify a dependency relation
Dm such that a variable a depends on variable b (i.e.,
(a, b) ∈ Dm ) if and only if b appears in the definition
of a. Note that variable a may depend on the previous
values of other variables (including itself) which has no
effect on the dependency relation. A dependency graph
may be inferred from the dependency relation by taking
each variable in the module to be a node and including
an edge from a to b if a depends on b1 . It is required
that the dependency graph of each module is acyclic.
Intuitively, the execution of a SOL program proceeds
as a sequence of steps, each initiated by an event (known
as the triggering event). Each step of a SOL module
comprises a set of variable updates and service invocations that are consistent with the dependency relation
Dm of that module. Computation of each step of a module proceeds as follows: the module or its environment
nondeterministically initiates a triggering event; each
module in the system responds to this event by updating
all its dependent (i.e., internal, service, and controlled)
variables. In the programmer’s view all updates and service invocations of the system are assumed to be synchronous (similar to the Synchrony Hypothesis of languages such as Esterel, LUSTRE, etc. [13]) – it is assumed that the response to a triggering event is completed in one step, i.e, all updates to dependent variables
and all method calls are performed by the modules of
the system before the next triggering event. Moreover,
all updates are performed in an order that is consistent
with the partial order imposed by the dependency graph.

1 The

notion of a dependency relation is easily extended to the entire system.

3.3 An Automated Therapeutic Drug Monitoring System in SOL
In this subsection we present a (part of a) skeleton in
SOL of a distributed automated therapeutic drug monitoring system in a hospital. We will use this as a running
example in this paper. A scenario of the operation of the
system is depicted in Figure 1. A sensor (can be a nurse
sitting at a terminal) at a patient’s bed in a hospital monitors the patient’s vital data (e.g., saturation, heartbeat,
blood pressure etc.). As soon as the vital data indicate
that the patient’s condition is critical, the sensor reports
the vital data to the central hospital server along with a
report that the patient’s condition is critical. The central hospital server contacts the patient’s doctor (e.g., by
sending a message to her palmpilot) with the patient’s
vital data and the report (critical) from the sensor. The
doctor can look up a drug appropriate for the patient’s
condition and invoke a service provided by the pharmaceutical company (producing the drug) with the vital inf!
ormation of the patient that computes the correct dosage
corresponding to the patient’s current state. Further, if
the patient’s saturation is below a certain threshold, the
doctor can order her to be put on oxygen. The doctor
communicates her response (dosage, oxygen) to the central hospital server which in turn communicates it to the
nurse (patient sensor and actuator) that attends the patient by administering the required dosage of the drug
or by putting her on oxygen. The patient sensor reports
to the hospital service whenever the state of the patient
changes (e.g., turns from critical to noncritical) which
in turn reports to the doctor for appropriate action. Due
to space limitations, we show here only the SOL module running on the doctor’s palmpilot in Figure 2. Interested readers may refer to [13] to compare SOL with
other synchronous programming languages such as Esterel, Argos, LUSTRE, and SIGNAL.
The doctor module is implemented as a deterministic reactive module. We identify four monitored
variables – heartrate, pressure (unit lb/sqinch),
saturation and p cond corresponding to the vital data heart rate, blood pressure and saturation of
the patient and condition of the patient (critical or
noncritical) that the module obtains from the hospital server, a service variable c dosage (unit mg)
that is defined by invoking the pharmaceutical service, a continuation variable that cont that is passed
as a continuation while invoking the service, and two
controlled variables output dosage (unit cc) and
oxygen that correspond respectively to the dosage
and the decision whether to put the patient on oxygen or not sent back to the hospital server that listens
to these variables. We also identify a service invocation pharmserv:compute dosage that invokes the

4

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

compute dosage method of the pharmaceutical service named (and addressed) pharmserv with the vital data of the patient as arguments and the variable c!
ont being passed as a continuation. The service invocation is used to obtain the required dosage of the patient and defines the service variable c dosage. The
preconditions for invoking the service provided in the
services section specify that the types of all the three
formal parameters x, y and z should be integer while the
postcondition is true. The return value from the service
invocation should be of type dosage. The unit conversion rules section defines an mg to to be equal to a cc so
that the value of the variable c dosage can be directly
assigned to the controlled variable output dosage.
The module doctor responds to a triggering event2
by updating its dependent variables in compliance
with the dependency (partial) order. One possible
order is oxygen → saturation, c dosage→
pressure,
heartrate,
c dosage→
c dosage→ saturation, c dosage→ cont
, and output dosage → c dosage.

deterministic reactive module doctor {
units
lb_per_sqinch, mg, cc;
unit conversion rules
mg=cc;
type definitions
dosage = integer;
patient_condition={critical,not_critical};
services
dosage pharmserv:compute_dosage(integer x, y, z) unit mg,
continuation dosage_computed, pre = true, post = true;
monitored variables
integer heartrate;
integer pressure unit lb_per_sqinch;
integer saturation;
patient_condition p_cond;
controlled variables
dosage output_dosage unit cc;
boolean oxygen;
definitions
c_dosage = initially null then
if{
[] @C(p_cond) && @C(heartrate) && @C(pressure)
-> pharmserv:
compute_dosage(heartrate,pressure,saturation)
ˆdosage_computed;
}// service invocation
output_dosage= initially null then
if{
[] @T(dosage_computed.done)-> c_dosage;
} //update of controlled variable
oxygen= initially false then
if{
[] @T(saturation<65) -> true;
[] @T(saturation>90) -> false;
}
}

Figure 2. Doctor module in SOL.

Figure 1. Automated therapeutic drug
monitoring scenario

3.4 SOL Definitions
The definitions section is at the heart of a SOL
module. This section determines how each internal and
controlled variable of the module is updated in response
2 Since doctor is reactive, all triggering events are external to

the module.

5

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

to events (i.e., state changes) generated either internally
or by the module’s environment.
A variable definition is either a one-state or a twostate definition. A one-state definition, of the form x =
expr (where expr is an expression), defines the value
of variable x in terms of the values of other variables in
the same state. A two-state variable definition, of the
form x = initially init then expr (where expr is
a two-state expression), requires the initial value of x to
equal expression init; the value of x in each subsequent
state is determined in terms of the values of variables in
that state as well as the previous state (specified using
operator PREV or by a when clause).
A conditional expression, consisting of a sequence
of branches “[] guard → expression”, is introduced
by the keyword “if” and enclosed in braces ("{" and
"}"). A guard is a boolean expression. The informal
semantics of the conditional expression if { []g1 →
expr1 []g2 → expr2 . . . } is defined along the lines of
Dijkstra’s guarded commands [11] – in a given state, its
value is equivalent to expression expri whose associated
guard gi is true. If more than one guard is true, the expression is nondeterministic. It is an error if none of the
guards evaluates to true, and execution aborts setting
the failure variable corresponding to that module to true.
The case expression case expr { []v1 → expr1 []v2 →
expr2 . . . } is equivalent to the conditional expression
if { [](expr == v1 ) → expr1 [](expr == v2 ) →
expr2 . . . }. The conditional expression and the case
expression may optionally have an otherwise clause
with the obvious meaning.
Expressions may include service invocations of the
form A:B(var list)ĉont where the identifier A is
the name/URL of the service, B is the name of the
method invoked, var list is the list of variables
passed as arguments to the method, and, cont is the
associated continuation variable. For each service invocation site in a module, a distinct continuation variable
should be used. Upon successful completion of a service invocation, the value of the field “done” of the corresponding continuation variable is set to “true.”
When the agent doctor defining the service variable c dosage is executed, the agent environment invokes the service by sending it a message. The preparation of this message involves marshaling the arguments as well as the continuation, which includes information about the channel Chan on which the result
of the service invocation is to be returned. Once the
service returns the result, the guard @Rec(Chan) in
the continuation module associated with the continuation variable becomes true. This event results in the
controlled variablec dosage (in the continuation module) being set the value received on Chan as the response for the service invocation and @Comp(cont)

in the environment being set to true. In module
doctor, this in turn sets the value of the service variable c dosage to the value received as response from
the service (i.e., the value of the controlled variable
c dosage of the continuation module cont) and triggers the event @Comp(cont). The triggering of the
event @Comp(cont) in the doctor module results
in the controlled variable output dosage being assigned the value of c dosage which at that point is
the value returned as a response to the service invocation. Note that the invocation of the service can be
asynchronous, i.e., the response from the service may
not arrive instantaneously. Computations that do not depend on the response received from the service invocation (i.e., definitions of dependent variables that do not
depend on the service variable receiving the response
from the service invocation) are not blocked in order
to wait for the response from the service. For example, in Figure 2, the decision whether to put the patient
on oxygen can be made without waiting for the pharmaceutical service to return the required dosage. Hence
the definition of the variable oxygen can be executed
while waiting for the response from the pharmaceutical service if one of the events @T(saturation<65)
or @T(saturation>90) is triggered. Computations
dependent on the result of the service invocation must be
guarded by @Comp(cont), where cont is the variable passed as continuation in the service invocation,
so that they wait until the result of the service invocation is available (signaled by the triggering of the
@Comp(cont) event). The asynchronous nature of the
service invocations create the effect of the XMLHttpRequest API in AJAX-like applications.
The asynchronous nature of the service invocations
can be used to define a timer. We assume the existence of
a timer method provided by a time service that when invoked with a (integer or real) delay provides a response
after the delay specified by the argument. If the variable cont is passed as a continuation while invoking
the service, the triggering of the event @Comp(cont)
signifies passage of the delay. The implementation of
the timer is illustrated in the example below.
deterministic reactive module delay{
...
services
String time:timer(x),
pre=x::integer && x>0 -- post = true;
controlled variables
integer x;
monitored variables
boolean clock;
service variables
String t;
continuation variables

6

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

continuation cont;
...
definitions
...
t=initially null then
if{
[] @T(clock)->time:timer(10);
}
x=initially null then
if{
[] @Comp(cont)-> ...
}
}
The module delay ensures that the controlled x is output 10 time units after the arrival of a clock pulse i.e.,
there is a delay of 10 time units between an input event
(arrival of a clock pulse) and the corresponding output.

3.5 Failure Handling
Benign failures in the environment are handled by
program transformations incorporated in the SOL compiler that automatically transform a SOL module based
on the failure handling information provided in the monitored variable declaration section. Given the declaration failure boolean I in the monitored variable
section of a failure variable signifying the (benign) failure of a module I in the environment and the declaration integer x on I y of a monitored variable x
(y is also a monitored variable), the SOL compiler transforms each two-state definition z=initially null
then expr, where z is a dependent variable and
expr is an expression in which x occurs, to

in other words, we have the expressiveness of the full
language in these clauses. This does not have a detrimental effect on the proof tools, since most commonly
encountered theorems about SOL programs are decidable.

4

SOL agents execute on a distributed run-time infrastructure called SINS (see Figure 3). A typical SINS implementation comprises one or more SINS Virtual Machines (SVMs), each of which is responsible for a set of
agents on a given host. SVMs on disparate hosts communicate using the Agent Control Protocol (ACP) [18]
for exchanging agent and control information. An ancillary protocol, termed the Module Transfer Protocol
(MTP) manages all aspects of code distribution including digital signatures, authentication, and code integrity.
Agents in SOL are allowed access to local resources of
each host in compliance with locally enforced security
policies. An inductive theorem prover is used to statically verify compliance of an agent with certain local
security policies. Other safety properties and security requirements are enforced by observer agents (termed “security agents”) that monitor the execution of applicationspecific agents and take remedial action when a violation
is detected.
Host

z= initially null then

3.6

Agents
E
n
c
SINS Virtual
r
Machine (SVM) y
p
t
Encrypt

Host

if{
[] I -> expr[y/x]
}
where expr[y/x] is the expression obtained by replacing each occurrence of the variable x by the variable
y. One-state definitions are transformed similarly.

SINS

Encrypt
SINS Virtual
Machine (SVM)

Host
Agents
E
n
c
r
y
p
t

SINS Virtual
Machine (SVM)

E
n
c
r
y
p
t

Agents

Figure 3. Architecture of SINS.

5

A Static Type System for SOL

Assumptions and Guarantees

The assumptions of a module, which are typically assumptions about the environment of the subsystem being
defined, are included in the assumptions section. It
is up to the user to make sure that the set of assumptions
is not inconsistent, i.e., a logical contradiction. Users
specify the module invariants in the guarantees section, which is automatically verified by a tool such as
Salsa. The syntax for specifying module assumptions
and guarantees is identical to that of module definitions,

In this section, we provide the formal semantics of
SOL. We first present a static type system that enforces
the information flow policies ensuring safe downgrading
of information.

5.1 Static Type Checking for Information Flow
Let S denote a typing environment, x, y range over
the variables of a module, expr over the set of expressions in the module, s, t over the set of types defined in

7

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

the type definition section of the module, and u, v over
the set of units defined in the unit definition section of
the module. A typing environment S is defined as

S ::= ∅ | S ∪ {x → t unit u}

where x → t unit u denotes that x is of type t an unit u.
Here the unit qualifier is optional. Let us define S(x) =
t if x → t unit u ∈ S or x → t ∈ S, and Sunit (x) = u
if x → t unit u ∈ S. We will write S 	 defn if the definition defn is well-typed under the typing environment
S. The typing rules for the static type system for SOL is
given in Figure 4. The judgements [type] and [unit] are
obvious. The judgement [expr] is infers the secrecy type
of an expression from those of its subexpressions (op
is a binary operator/relation symbol). If under the typing environment S, the secrecy types of the expressions
expr1 and expr2 are t and t respectively, and t → t
is a flow conversion rule (i.e., belongs to FlowRules),
then the secrecy type of the expression expr1 op expr2
is t . Informally, the rule states that, if binary operation/relation is applied on values, one of which is classified and the other unclassified, then the secrecy type
of the result is still classified. The judgements [expru1],
[PREV1], [PREV2], and [if] are straightforward. In [if],
if (expr, expr1 , expr2 ) denotes the if expression if
[]expr -> expr1 otherwise -> expr2 . The
judgement [expru2] states that if under the typing environment S, the expressions expr1 and expr2 have units
u and v respectively, then a binary operation can be applied on the expressions if there exists a conversion rule
from the unit u to the unit v (or viceversa) declared in the
unit conversion rules section of the module (here e(v)
is an expression containing v). In case u is defined in
terms of v, the unit of the resultant expression will be v.
The judgements [odeft], [odefu], [tdeft], and [tdefu] provide the type and unit checking rules for one-state definitions and two-state definitions respectively. We explain
[odeft]; the others are similar. Intuitively the rule [odeft]
states that the value of an unclassified expression can be
assigned to a variable declared as classified. More formally, under the typing environment S, the value of an
expression of type t can be assigned to a variable of
type t only if it is permitted by a rule in the flow conversion section. Finally, the judgements [onecast] and
[twocast] state that an assignment of an expression of
type t to a variable of type t is allowed if explicitly coerced by the programmer. A module m typechecks if
decl ∪ UnitRules ∪ FlowRules 	 m where decl is the
set of declarations in the module, FlowRules is the set of
flow control rules and UnitRules is the set of unit conversion rules. A module m is secure if it typechecks.

5.2

Formal Operational Semantics

In this section, we provide (a part of) the formal operational semantics of SOL. Let Γ be an environment.
Let Types be the set of all types in a SOL program. We
let val range over values of type T for T ∈ TYPES .
Let x, y range over the variables in SOL program. An
environment Γ is defined as
Γ ::= ∅ | Γ ∪ {x → val} | Γ ∪ {PREV (x) → val}
where x → val denotes that x assumes value val. We
will write Γ 	 x → val if x → val ∈ Γ. We will denote by dom(Γ) the set {x | {x → . . .} ⊆ Γ}. Let us
write Γunit (x) = u if u ∈ dom(Γ) and the unit of x
is u. For a module m, we denote by Γm the restriction
of Γ to the variables in m. The judgements for the operational semantics of SOL are given in Figure 5. For
sake of brevity, we do not include the full operational
semantics; rather we only provide a sampling of some
of the more informative rules. The first judgement [no
action] states that for a module m if no monitored variable changes, then no computation is done. Here MV m
denotes the set of all monitored variables of m. The second judgement [unit conv] shows how unit conversion
is done automatically at runtime. If the unit of an expression is v, under the current environment its value is
val, and a variable x of unit u is assigned the value of
the expression, then the value val is first transformed
to unit u using the rules in the unit conversion section
of concerned module before assignment to x. The third
judgement [CV] states that if x is a controlled variable
in the module m and a monitored variable in the module
n and if x has value val under the environment Γm then
it has the same value under the environment Γn (here
CV m is the set of controlled variables of the module
m). The judgement [@Comp] describes the @Comp
event. Assume that x is an internal or a controlled variable defined by an expression. Assume also that the
definition is guarded by @Comp(cont) where cont is a
continuation variable. If under the current environment
Γ, the expression in the definition evaluates to val and
@Comp(cont) is true, then the variable x evaluates to
val and the environment turns off @Comp(cont). The
other rules that we do not present here deal with checking the preconditions of a service before a service invocation, checking the postconditions after a service has
responded and dealing with the external events. Note
that at runtime the preconditions and postconditions of
a service invocation (including types) are checked to ensure soundness in the presence of third party (possibly
COTS) component services that may undergo reconfigurations at runtime due to network faults or malicious
attacks.

8

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

[type]
[unit]
[expr]

[if]

[tdeft]
[tdefu]

S 	 x#u

if S(x) = t
Sunit (x) = u

S 	 expr2 :: t
S 	 expr1 : t
t ⇒ t ∈ FlowRules
S 	 expr1 op expr2 :: t
[expru1]

[expru2]

S 	 x :: t

S 	 expr1 #u
S 	 expr2 #u
S 	 expr1 op expr2 #u

S 	 expr1 #u
S 	 expr2 #v
u = e(v) ∈ UnitRules
S 	 expr1 op expr2 #v
[PREV1]

S 	 expr#u
S 	 PREV (expr)#u

[PREV2]

S 	 expr :: t
S 	 PREV (expr) :: t

S 	 expr2 :: t
S 	 expr1 :: t
t ⇒ t ∈ FlowRules
S 	 if (expr, expr1 , expr2 ) :: t

[odeft]

S 	 x :: t
S 	 expr :: t 
t ⇒ t ∈ FlowRules
S 	 defn(x, expr)

[odefu]

S 	 x#u
S 	 expr#v
u = e(v) ∈ UnitRules
S 	 defn(x, expr)

S 	 x :: t
S 	 init :: t , S 	 expr : t 
t ⇒ t, t ⇒ t ∈ FlowRules
S 	 twodefn(x, init, expr)
S 	 x#u
S 	 init#w, S 	 expr#v
u = e(v), u = f (w) ∈ UnitRules
S 	 twodefn(x, init, expr)

[onecast]

S 	 x :: t
S 	 defn(x, (t)expr)

[twocast]

S 	 x :: t
S 	 twodefn(x, (t)init, (t)expr)

Figure 4. A static type system for SOL

6

Conclusions

We have provided a formal framework for developing
interactive AJAX-like thick-client applications deployed
in mission-critical scenarios. The framework is based
on a synchronous programming language SOL. SOL has
formal operational semantics helping build, understand,
and debug programs. Static type checking helps eliminate security and information flow errors in SOL programs while automatic theorem provers can be used to
verify the correctness. SOL programs compile to Java
and run unmodified on the SINS infrastructure. Being
based on Java, our framework is browser-agnostic; it interfaces with any Java-compatible browser.

References
[1] http://www.ERights.org.

[2] http://www.irisa.fr/espresso/
Polychrony/document/tutorial.pdf.
[3] A. Benveniste, P. Caspi, S. A. Edwards, N. Halbwachs,
P. L. Guernic, and R. de Simone. The synchronous
languages 12 years later. Proceedings of the IEEE,
91(1):64–83, 2003.
[4] G. Berry and G. Gonthier. The Esterel synchronous
programming language: Design, semantics, implementation. Sci. of Computer Prog., 19, 1992.
[5] R. Bharadwaj. Development of dependable componentbased distributed applications. Technical report, Naval
Research Laboratory, 2005.
[6] R. Bharadwaj and C. Heitmeyer. Model checking complete requirements specifications using abstraction. Automated Softw. Engg., 6(1), Jan. 1999.
[7] R. Bharadwaj and S. Mukhopadhyay. A formal approach
to developing reliable event-driven service-oriented systems. In In Procedings of COMPSAC, 2008.
[8] R. Bharadwaj, S. Mukhopadhyay, and N. Padh. Service composition in a secure agent-based architecture.

9

Proceedings of the 44th Hawaii International Conference on System Sciences - 2011

[no action]
[unit conv]

Γ 	 expr → val

Γ 	 ∀x ∈ MV m PREV (x) = x

Γunit (expr) = v, Γunit (x) = u
u = e(v) ∈ UnitRules, def (x, expr) ∈ m
Γ 	 x → e(val)
[CV]

[@Comp]

∀contΓ 	 @Comp(cont)

Γm 	 x → val
x ∈ CVm
x ∈ MVn
Γn 	 x → val

Γ 	 expr → val
Γ 	 @Comp(cont) → true,
def (Comp(cont), x, expr) ∈ m
Γ[@Comp(cont) := f alse] 	 x → val
Figure 5. Operational Semantics for SOL

[9]

[10]
[11]
[12]

[13]

[14]

[15]
[16]

[17]

[18]

In Proceedings of the IEEE International Conference on
E-Technologies, E-commerce and E-Service (EEE’05),
pages 787–788, 2005.
R. Bharadwaj and S. Sims. Salsa: Combining constraint
solvers with BDDs for automatic invariant checking. In
Proc. 6th International Conference on Tools and Algorithms for the Construction and Analysis of Systems
(TACAS’2000), ETAPS 2000, Berlin, Mar. 2000.
R. Bharadwaj and S.Mukhopadhyay. From synchrony to
sins. Technical report, West Virginia University, 2005.
E. W. Dijkstra. A Discipline of Programming. PrenticeHall, 1976.
D. Gay, P. Levis, J. R. von Behren, M. Welsh, E. A.
Brewer, and D. E. Culler. The nesc language: A holistic approach to networked embedded systems. In PLDI,
pages 1–11, 2003.
N. Halbwachs. Delay analysis in synchronous programs.
In C. Courcoubetis, editor, the International Conference
on Computer-Aided-Verification, volume 697 of LNCS,
pages 333–346. Springer-Verlag, 1993.
C. L. Heitmeyer, R. D. Jeffords, and B. G. Labaw. Automated consistency checking of requirements specifications. ACM Transactions on Software Engineering and
Methodology, 5(3):231–261, April–June 1996.
E. A. Lee. Absolutely positively on time: What would it
take? Computer, 38(7):85–87, 2005.
G. Neumann and U. Zdun. Pattern-based design and implementation of an xml and rdf parser and interpreter: A
case study. In ECOOP, pages 392–414, 2002.
F. Rocheteau and N. Halbwachs. POLLUX: A Lusture
based hardware design environment. In P. Quinton and
Y. Robert, editors, Proc. Conf. on Algorithms and Parallel VLSI Arch. II, Chateau de Bonas, June 1991.
E. Tressler. Inter-agent protocol for distributed SOL processing. Technical Report To Appear, Naval Research
Laboratory, Washington, DC, 2002.

10

3154

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

VOL. 28,

NO. 12,

DECEMBER 2016

A Generalized Hierarchical Multi-Latent Space
Model for Heterogeneous Learning
Pei Yang, Member, IEEE, Hasan Davulcu, Member, IEEE, Yada Zhu, Member, IEEE,
and Jingrui He, Member, IEEE
Abstract—In many real world applications such as image annotation, gene function prediction, and insider threat detection, the data
collected from heterogeneous sources often exhibit multiple types of heterogeneity, such as task heterogeneity, view heterogeneity, and
label heterogeneity. To address this problem, we propose a Hierarchical Multi-Latent Space (HiMLS) learning framework to jointly
model the triple types of heterogeneity. The basic idea is to learn a hierarchical multi-latent space by which we can simultaneously
leverage the task relatedness, view consistency and the label correlations to improve the learning performance. We first propose a
multi-latent space approach to model the complex heterogeneity, which is then used as a building block to stack up a multi-layer
structure in order to learn the hierarchical multi-latent space. In such a way, we can gradually learn the more abstract concepts in the
higher level. We present two instantiated models of the generalized framework using different divergence measures. The two-phase
learning algorithms are used to train the multi-layer models. We drive the multiplicative update rules for pre-training and fine-tuning in
each model, and prove the convergence and correctness of the update methods. The effectiveness of the proposed approach is
verified on various data sets.
Index Terms—Heterogeneous learning, multi-task learning, multi-view learning, multi-label learning, matrix tri-factorization

Ç
1

I

INTRODUCTION

the era of big data, a large amount of information collected from heterogeneous sources are correlated with
each other. It is of great importance to mine such hidden
correlations in the presence of multiple types of heterogeneity for many real world applications, such as web news classification, gene function prediction, insider threat detection,
image annotation, etc. In this paper, we focus on triple types
of heterogeneity, i.e., task heterogeneity, view heterogeneity, and label heterogeneity. For example, for the satellite
image analysis problems, task heterogeneity refers to the
images collected from different satellites following from different distributions; view heterogeneity refers to various
types of features such as color histogram, edge distribution
histogram, and bag of visual words; label heterogeneity
refers to the multiple tags associated with each image.
The major challenge for learning with the triple types of
heterogeneity is how to effectively mine the hidden correlations among the heterogeneous data. Such correlations
should reflect the key assumptions underlying each type of
heterogeneity, including the task relatedness assumption [7],
the view consistency assumption [16], as well as the label
correlation assumption [34]. To the best of our knowledge,
we are the first to jointly model the triple heterogeneity.
N



P. Yang, H. Davulcu, and J. He are with Arizona State University, Tempe,
AZ 85281.
E-mail: {cs.pyang, jingrui.he}@gmail.com, HasanDavulcu@asu.edu.
 Y. Zhu is with IBM Research, Yorktown Heights, NY 10598.
E-mail: yzhu@us.ibm.com.
Manuscript received 22 Dec. 2015; revised 26 July 2016; accepted 5 Sept.
2016. Date of publication 20 Sept. 2016; date of current version 2 Nov. 2016.
Recommended for acceptance by X. Zhu.
For information on obtaining reprints of this article, please send e-mail to:
reprints@ieee.org, and reference the Digital Object Identifier below.
Digital Object Identifier no. 10.1109/TKDE.2016.2611514

To tackle this problem, we propose a Hierarchical MultiLatent Space (HiMLS) framework for heterogeneous learning. The goal is to maximally leverage the rich correlations
among heterogeneous data to improve the performance. To
this end, we first present a multi-latent space model, which
characterizes task relatedness, view consistency, and label
correlation in a principled framework. It is formulated as a
regularized non-negative matrix tri-factorization problem,
aiming to simultaneously minimize the reconstruction loss
on the instance-feature data and the classification loss on the
instance-label data, while maximizing the similarity among
the co-latent spaces. Furthermore, the proposed multi-latent
space model is used as a building block to establish a multilayer structure. It aims to build a hierarchical multi-latent
space to gradually learn the more abstract concepts in the
higher layer. The proposed HiMLS approach is motivated
from two streams of work in machine learning. One is multiway clustering (or co-clustering) [3] which improves the
quality of clustering by intertwining both row and column
information that are inter-related. Another is multi-layer
models [19] which obtains better data representations by
automatically extracting the hierarchical concepts from data.
Our multi-latent space model employs multi-way clustering
on the instances, features, and labels to capture the correlations among the heterogeneous data, while the hierarchical
multi-latent space model takes advantage of multi-layer
structure to learn the hierarchical concepts from data. Both of
them help extract the rich correlations among heterogeneous
data, leading to better performance.
Based on this generalized framework, we present two
instantiated models using different distance metrics, i.e.,
least squares loss function and the generalized KullbackLeibler divergence. For each model, we develop an iterative
updating algorithm to solve the optimization problem. The

1041-4347 ß 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

YANG ET AL.: A GENERALIZED HIERARCHICAL MULTI-LATENT SPACE MODEL FOR HETEROGENEOUS LEARNING

proposed algorithms consist of two phases. First, each layer
is pre-trained in a greedy layer-wise way. Then, it fine-tunes
the weights of all the layer to reduce the total reconstruction
loss and the classification loss. It is worth noting that the
proposed approach is a generalized framework to learn
from complex heterogeneity, which subsumes some popular methods on learning from a single heterogeneity.
The main contributions of this paper can be summarized
as:


A novel learning problem which simultaneously
models triple types of heterogeneity;
 A generalized framework to learn the hierarchical
multi-latent space from complex heterogeneity;
 Two alternative models and the corresponding optimization algorithms;
 Generalization of some previous work on learning
from single heterogeneity;
 Experimental results on various data sets showing
the effectiveness of the proposed approach.
The rest of the paper is organized as follows. After a review
of the related work in Section 2, we present the proposed generalized framework in Section 3, and two alternative models
and their corresponding optimization algorithms in Sections 4
and 5, respectively. Some case studies are discussed in
Section 6. Section 7 shows the experimental results. Finally,
we conclude the paper in Section 8.

2

RELATED WORK

Since we make use of matrix factorization techniques to
model the complex heterogeneity, we review the related
work on both heterogeneous learning and non-negative
matrix factorization.

2.1 Heterogeneous Learning
Heterogeneous learning aims to leverage different types of
heterogeneity, such as task heterogeneity, view heterogeneity, and label heterogeneity, to improve the learning performance. Most of the previous work were focused on
modeling a single or dual types of heterogeneity.
In multi-task learning, the goal is to leverage the small
amount of labeled data from multiple related tasks to
improve the learner for each task. Among others, alternating structure optimization [1] decomposed the model into
the task-specific and task-shared feature mapping; multitask feature learning [2] assumed that multiple related tasks
share a low-dimensional representation; clustered multitask learning [47] assumed that multiple tasks follow a clustered structure. Some recent multi-task learning methods
dealt with irrelevant tasks by assuming that the model can
be decomposed into a shared feature structure that captures
task relatedness, and a group-sparse structure that detects
outliers [17].
In multi-view learning, the features from multiple sources form natural views. The goal is to leverage the complementary information among different views to improve the
performance. Co-Training [4] is one of the earliest algorithms for multi-view learning. More recent work includes:
SVM-2K [16] which combined KCCA with SVM in an optimization framework; the information-theoretic framework
for multi-view learning [30]; the CoMR method [29] based

3155

on a data-dependent Reproducing Kernel Hilbert Space
(RKHS); the large-margin framework for multi-view data
based on a latent space Markov network [8]; the convex
multi-view subspace learning method MSL [36] which
enforced conditional independence among the multiple
views while reducing dimensionality, etc.
In multi-label learning, each instance is associated with a
set of labels [34], [46]. The key issue is how to exploit the
correlations or dependencies among multiple labels. To
name a few, ML-kNN [45] converted the multi-label learning into a number of independent binary classification problems; Rank-SVM [15] solved the label ranking problem
under the large margin framework; LEAD [44] employed
Bayesian network to encode the conditional dependencies
of the labels; LS-ML [22] assumed that a common subspace
is shared among multiple labels; HG [31] constructed a
hypergraph to exploit the correlation information among
different labels; LEML [41] learned the latent label space
under a generic empirical risk minimization framework
with trace-norm regularization. In addition, MLLOC [21]
assumed that the label correlation may be shared by a subset of instances only rather than all the instances; the boosting based method MAHR [20] aimed to discover the label
relationship by using a hypothesis reuse mechanism; the
transductive approach TRAM [23] leveraged the information from unlabeled data to estimate the optimal label concept compositions.
More recently, researchers begin to study problems with
dual types of heterogeneity. For problems with both task
and view heterogeneity, a variety of techniques have been
proposed to model task relatedness in the presence of multiple views, e.g., the transductive method IteM 2 [18], the
inductive method regMVMT [43], the Bayesian method
NOBLE [37], and the graph-based method M 2 LID [39]. For
the problems with both label and view heterogeneity, the
L2 F method proposed in [40] modeled both the view consistency and the label correlations in a graph-based framework. For the more complex setting with all three types of
heterogeneity, these techniques cannot be readily applied
without disregarding the useful information from a certain
type of heterogeneity, except for our recent work [38] on
modeling the triple heterogeneity. This paper extends [38]
substantially by providing the generalized learning framework, the alternative optimization algorithms, and the theoretical analysis regarding the optimal solutions, as well as
the comprehensive empirical evaluations.

2.2 Non-Negative Matrix Factorization
Non-negative matrix factorization (NMF) [24] aims to
extract data-dependent non-negative basis functions, which
has been given much attention due to its part-based and
easy interpretable representation. Non-negative matrix factorization [25] has been widely used in data mining, biomedical, chemometrics, signal processing, computer vision,
neuroscience, graph analysis, etc [10]. Incorporating extra
constraints such as sparseness [28], smoothness [5], or
orthogonality [14] was shown to improve the decomposition and provide the better representation. Various extensions and variations of NMF have been proposed, such as
Semi-NMF [12], Convex-NMF [12], multi-layer NMF [10],
[33], weighed NMF [35], Tri-NMF [14], etc.

3156

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

NMF has connections to many other techniques in data
mining. For example, under some mild conditions, NMF
with the least squares loss function is equivalent to a relaxed
K-means clustering [11], while NMF with the generalized
Kullback-Leibler (KL) divergence loss function is equivalent
to probabilistic latent semantic indexing [13].

3

THE PROPOSED GENERALIZED FRAMEWORK
FOR HETEROGENEOUS LEARNING

We first present the multi-latent space framework to model
the complex heterogeneity, which is then used as a building
block to stack up a multi-layer structure in order to learn
the hierarchical multi-latent space.

3.1 Notations and Problem Statements
Suppose we are given the multi-label data with multiple
views in different tasks. Let T be the number of tasks, V the
number of views, m the number of labels. Each instance is
described from V views, and associated with multiple
labels. For the ith task and jth view, denote the number of
instances and features by ni and dj , respectively. Let


~ ij = X ij
2 Rni dj be the instance-feature matrix for the
X
Xiju
ith task and jth view, where Xij is the training data and Xiju


Yi
~
2 Rni m be the instance-label
is the test data. Let Y i ¼
Yiu
matrix for the ith task, where Yi and Yiu are for training and
test data respectively. The instance-label matrix can be
either a binary or a real matrix, such as the user-item matrix
of either preference or rating scores in a recommender system. The goal is to build a model to predict the instancelabel matrix for the test data by leveraging the rich information among heterogeneous data.
Some math symbols used in this paper are introduced as
follows. For two matrices X and Y , let X  Y , X  Y , and X
Y
be the Hadamard product (or entrywise product), Kronecker product, and Hadamard division, respectively. Let
x ¼ vecðXÞ be the matrix vectorization of X into a vector x.
3.2 Multi-Latent Space Learning
We propose a multi-latent space learning framework to
jointly model the task relatedness, view consistency, and
label correlations in a principled way.
Motivated by the success of multi-way clustering [3] in
leveraging the inter-correlations among data to improve
clustering quality, we do multi-way clustering on heterogeneous data to learn the multi-latent space. It simultaneously
clusters instances, features

and labels into the correspondR
i
~i ¼
2 Rni p be the instance encoding clusters. Let R
Rui
ing matrix where p is the dimensionality of instance latent
space, Ri and Rui are for training and test data, respectively.
Let Cj 2 Rdj q be the feature encoding matrix, C Y 2 Rmq
the label encoding matrix where q is the dimensionality of
~i (or Cj , CY )
feature (or label) latent space. Each row in R
represents the coeffecients of the instance (or feature, label)
associated with the instance (or feature, label) clusters.
Denote M ij 2 Rpq ; M iY 2 Rpq as the co-latent space

VOL. 28,

NO. 12,

DECEMBER 2016

matrices. We try to reconstruct the instance-feature matrix
~ ij  R
~i Mij C T and
and instance-label matrix by letting X
j
Yi  Ri MiY CYT respectively, where 1  i  T and 1  j  V .
Note that Mij models the correlations between instance
clusters and feature clusters, while MiY models the correlations between instance clusters and label clusters.
The multi-latent space model is formulated as a regularized non-negative matrix triple factorization problem, which
simultaneously decomposes the instance-feature and
instance-label matrices, while enforcing the task relatedness,
view consistency, and label correlations on the data. The
objective is to simultaneously minimize the reconstruction
loss on the instance-feature data (1st term) and the classification loss on the instance-label data (2nd term), while maximizing the similarity among the co-latent spaces (3rd term):
min

fR;M;Cg > 0

T X
V


X
~ ij ; R
~i M ij C T
L X
j
i¼1 j¼1

T
T X
V
X
X




þa
L Y i ; Ri M iY CYT þ b
L M ij ; M iY
i¼1

(1)

i¼1 j¼1

where LðX; Y Þ is the distance metric between X and Y . a and
b are the non-negative parameters. The non-negative constraints allow for the multi-way clustering interpretation.
The multi-latent space model can be interpreted from the
perspective of constrained multi-way clustering. By constraining the multi-way clustering procedures, we model
the task relatedness by requiring the features across different
tasks to share the same feature clustering coefficients,
enhance the view consistency by requiring the instances to
share the same instance clustering coefficients across different views, characterize the label correlations by requiring the
labels to share the same label clustering coefficients across
different tasks. Fig. 1a shows an illustrative example about
the proposed multi-latent space model. Specifically, the
multi-latent space model encodes multiple types of correlations among the heterogeneous data as follows:
Task Relatedness. For the jth view, the decompositions of
the instance-feature data Xij ð1  i  T Þ in different tasks
share the same feature encoding matrix C j .
Label Correlation. The labels share the same label encoding matrix C Y across different tasks.
View Consistency. For the ith task, the decompositions of
the instance-feature data Xij ð1  j  V Þ in different views
share the same instance encoding matrix Ri .
Correlations Among Feature-Instance-Label. For the ith task,
the decompositions of instance-feature data X ij and
instance-label data Y i share the same instance encoding
matrix Ri .
Correlations Among Co-Latent Spaces. Since the instances,
features, and labels may share the latent semantic concepts,
we hope the learned co-latent spaces, Mij and MiY , are similar to each other.
The intuition of enhancing the correlations among colatent spaces is as follows. Take webpage classification as an
example. The words (1st view) on the webpage, the hyperlinks (2nd view) pointing to the webpage, and categories
(labels) of webpage could be linked by the latent semantic
topics (bridges) of the webpage. Therefore, we hope that the

YANG ET AL.: A GENERALIZED HIERARCHICAL MULTI-LATENT SPACE MODEL FOR HETEROGENEOUS LEARNING

3157

Fig. 1. An illustrative example about the proposed approach. In (a), without loss of generality, suppose there are two tasks and two views. The view
consistency is modeled by sharing the instance encoding matrix R1 (or R2 ) across different views. The task relatedness is modeled by sharing the
feature encoding matrices C1 (or C2 ) across different tasks. The label correlation is modeled by sharing the label encoding matrix CY across different
ð1Þ
ð1Þ
ð1Þ
ðl1Þ
tasks. In (b), the input data matrix Xij ð1  i  T; 1  j  V Þ is decomposed into three matrices, Ri , Mij , and Cj . Then, the co-latent space Mij
ðlÞ

is further decomposed to learn its own co-latent space Mij where 2  l  L. In such a way, the multi-latent space model can be used as a building
block to stack up a multi-layer architecture in order to learn the hierarchical multi-latent space.

co-latent spaces Mij ð1  j  V Þ learned in the feature spaces
from multiple views are as similar as possible to the colatent space MiY learned from label spaces, which acts as a
bridge to link the labels with the features from multiple
views in the latent spaces. Note that maximizing the correlations between co-latent spaces is equivalent to minimizing
the distance between them.

3.3 Hierarchical Multi-Latent Space Model
Motivated by the success of multi-layer models [19] in automatically extracting the hierarchical concepts from data, we
use the multi-latent space model as a building block to stack
up a multi-layer architecture. It aims to learn the hierarchical multi-latent space from complex heterogeneity.
The co-latent spaces Mij and MiY can be viewed as the
~ ij and
compact representations for the original input data X
Yi . Let L be the number of layers. For the co-latent space
ðl1Þ
ðl1Þ
(or MiY ) where lð2  l  LÞ represents the layer,
Mij
ðlÞ

we hope to further learn its own co-latent space Mij (or
ðlÞ

MiY ) in a higher level, i.e.,
ðl1Þ

 Ri Mij Cj

ðl1Þ

 Ri MiY CY

Mij

MiY

ðlÞ

ðlÞ

ðlÞT

ðlÞ

ðlÞ

ðlÞT

In such a way, we can gradually learn the factor matrices in
ðLÞ
each layer. Based on the learned co-latent spaces Mij and
ðLÞ

MiY in the highest layer L, we hope to recover the original
~ ij and Yi , in the first layer as accurately as posinput data, X
sible. Thus, the objective for the multi-layer architecture is
as follows:
min

fR;M;Cg > 0

XT XV
i¼1



~ ij ;
L X
j¼1

~ð1:LÞ M ðLÞ C ð1:LÞT
R
i
ij
j



ð1:LÞ
ðLÞ ð1:LÞT
L
Y
;
R
M
C
i
i
iY
Y
i¼1


XT XV
ðLÞ
ðLÞ
L
M
;
M
þb
ij
iY
i¼1
j¼1
þa

XT



Q
where Aðs:tÞ ¼ tl¼s AðlÞ if s  t, and Aðs:tÞ ¼ I otherwise for
any matrix A. I is an identity
For the simplicity of
 matrix.

R
ð1:LÞ
ð2:LÞ
i
~
Ri .
notation, we denote R
¼
i
Rui
Fig. 1b shows an illustrative example for the proposed
hierarchical multi-latent space model. Take the webpage
classification or image annotation as the examples. In each
layer, we do multi-way clustering on the instances, features
and labels. Since the instances, features and labels may usually have hierarchical latent structures, they can be clustered
into sub-categories, and further into high-level sub-categories, until the top categories. In such a way, we can gradually learn the more abstract semantic concepts in a higher
layer.

3.4 Prediction
Note that the proposed hierarchical multi-latent space
model works in a transductive fashion since the first term of
Eqs. (1) or (2) involves both training and test data in building the model.
After the model training, we can obtain the instance
encoding matrices Rui for test data, Ri for training data, and
ðlÞ
Ri ð2  l  LÞ shared by both training and test data. Then,
we can use the factor matrices to predict the instances in the
test data. The final prediction is the weighted sum of predicðl1Þ
tions resulting from each layer. We have MiY 
ðlÞ
ðlÞ ðlÞT
Ri MiY CY ð2  l  LÞ, and try to approximate Yiu by using
ð1Þ ð1ÞT
Rui MiY CY . Therefore, the predicted instance-label matrix
for the test data in ith ð1  i  T Þ task can be computed as
follows:
Fi ¼

L
X
l¼1

(2)

ðlÞ

wl Fi ¼

L
X

ð2:lÞ

wl Rui Ri

ðlÞ

ð1:lÞT

MiY CY

;

(3)

l¼1

where wl controls the weight for lth layer. A na€ıve way is to
set the weights based on the reconstruction loss in each
layer. In our experiments, we simply use the equal weight
for each layer.

3158

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

If the input instance-label matrix is a binary matrix, we
can transform the predicted matrix Fi into a binary one by
using 0.5 as the classification decision threshold.

3.5 Distance Metric
Various distance metric LðX; Y Þ can be used in our proposed model to measure the similarity between X and
Y . In this paper, we focus on two divergence measures
widely used in NMF models. One is the least squares
loss function,
X 
2
X ij  Y ij :
kX  Y k2F ¼
i;j

db ðxjyÞ ¼

 log xy  1
x log xy  x þ y

Lemma 1. For any non-negative matrices M; Xi ; Ri ; Ci ; Pj and
Kj , the objective function,
J ðM Þ ¼ a

J ðM Þ ¼ a
¼ atr

i

F

T 

T X
V 

X
X





Y i  Ri M iY C T 
2 þ b

M ij  M iY 
2 :
a
Y F
F
i¼1

(4)

i¼1 j¼1

The objective function defined in Eq. (2) for hierarchical
multi-latent space can be instantiated as follows,
min

fR;M;Cg > 0

þa


2


ð1:LÞ
ðLÞ ð1:LÞT 


Y i  Ri MiY CY



F

i¼1

þb

 2M RTi X i C i
T



i
Xh
M T MP j PjT  2M T K j PjT þ const:

9
8 T
2
½Ri Ri M ðtÞ CiT C i 	uv 
Muv
>
>
>
>

=
<
ðtÞ
XX
Muv
ðtÞ

	
¼a
G M; M

>
>  T
>
ðtÞ
uv
i u;v >
;
: 2 Ri Xi C i uv Muv
1 þ ln MðtÞ
Muv
h
i
9
8
2
>
>
M ðtÞ P j PjT

Muv
>
>
>
>
uv
=

XX<
ðtÞ
Muv
;
þb

	
i
>
> h
>
j u;v >
>
>
M
ðtÞ
uv
;
: 2 K j PjT Muv
1 þ ln ðtÞ




uv

Muv

is an auxiliary function of J ðM Þ due to the facts:

and
(5)

T X
V 


2
X

 ðLÞ
ðLÞ 


Mij  MiY 
 :
i¼1 j¼1

RTi Ri MCiT C i

GðM; M Þ ¼ J ðM Þ;

F

i¼1 j¼1

M

j
T

Let t be the index of iteration. Similar to [14], we can
show that


2
T X
V 

X

~
~ð1:LÞ M ðLÞ C ð1:LÞT 


Xij  R


i
ij
j

T 

X

i

j

T X
V 


2
X

~
~i M ij C T 


X ij  R
j 
 þ
i¼1 j¼1

X

X






Xi  Ri MC T 
2 þ b

MP j  K j 
2
i F
F

X

þ btr

In this section, we introduce the two-phase optimization
algorithm for HiMLS.
When using least squares loss function, the objective
defined in Eq. (1) for multi-latent space can be instantiated as
follows,

fR;M;Cg > 0

(7)

where a and b are the non-negative parameters.

OPTIMIZATION ALGORITHM FOR HIMLS

min

j

sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
P
a i RTi Xi C i þ b j K j PjT
P
P
;
M¼M
a i RTi Ri MCiT C i þ b j MP j PjT

Next, we propose the optimization algorithm HiMLS
based on least squares loss function in Section 4, and
HiMLSD based on generalized Kullback-Leibler divergence
in Section 5.

4

X

X






Xi  Ri MC T 
2 þ b

MP j  K j 
2 ;
i F
F

Proof. We make use of auxiliary function approach [25] to
derive the update rules for Eq. (6) and prove its
convergence.
The objective function for M is rewritten into:

b 2 Rnf0; 1g:

bðb1Þ

greedy layer-wise manner, then fine-tune the weights of all
layers to reduce the total reconstruction loss and classification loss.
To derive the multiplicative update rules for pre-training
(in Theorem 3) and fine-tuning (in Theorem 4) in HiMLS,
we first derive Lemma 1. This lemma provides a generic
method to derive the update rules for all of R; M; C in both
pre-training and fine-tuning.

is non-increasing under the update rule:

b¼0
b¼1

>
>
: ðxb þðb1Þyb bxyb1 Þ

DECEMBER 2016

(6)

It
P reducesP to the Kullback-Leibler divergence when
i;j X ij ¼
i;j Y ij ¼ 1.
Note that both the least squares (b ¼ 2) and generalized
KL divergence (b ¼ 1) are the special cases of b-divergence:
x
y

NO. 12,

i

Another is the generalized Kullback-Leibler divergence,
	
X 
Xij
DðXjj Y Þ ¼
X
log

X
þ
Y
ij
ij
ij :
i;j
Y ij

8
>
>
<

VOL. 28,

F

Following the tactics successfully used in deep learning [19], we adopt a two-phase procedure to train the multilayer model. We first pre-train the weights of each layer in a



G M; M ðtÞ  J ðM Þ:
The minimum is obtained by setting the derivative to
zero:

@ 
G M; M ðtÞ ¼ 0:
@M

YANG ET AL.: A GENERALIZED HIERARCHICAL MULTI-LATENT SPACE MODEL FOR HETEROGENEOUS LEARNING

vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u PT ~ T ~
u
i¼1 Xij Ri M ij
C j ¼ C j  tPT
~T R
~i M ij
CjM T R

Then, we get the update rule as follows:
sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
P
a i RTi Xi C i þ b j K j PjT
P
P
M¼M
:
a i RTi Ri MCiT C i þ b j MP j PjT






Since
J M ðtÞ ¼ G M ðtÞ ; M ðtÞ  min G M; M ðtÞ




M
¼ G M ðtþ1Þ ; M ðtÞ  J M ðtþ1Þ , the objective function
J ðM Þ is non-increasing under the above update rule. t
u

j

 trðLM Þ;
T

where LðL  0Þ is the Lagrangian multiplies matrix. The
zero gradient condition gives

X
T
T
R
R
MC
C
þ
b
MP j PjT
i
i
i
i
i
j
X
X
A¼a
RT X i C i þ b
K j PjT :
i i
j

:

(15)

Also, the limiting solutions of the update rules satisfy the KKT
condition.

Proof. The convergence of the update follows from
Lemma 1. According to Lemma 2, we can prove that the
limiting solutions satisfy the KKT condition.
u
t
ð1:LÞ
ðLÞ ð1:LÞT
~
, Vij ¼
For simplicity, denote Vij ¼ Ri Mij Cj
ð1:LÞ
ðLÞ ð1:LÞT
ð1:LÞ
ðLÞ ð1:LÞT
~
, ViY ¼ Ri M C
, and FðAÞ ¼
Ri Mij Cj
Y

Theorem 4 (Convergence of Fine-tuning). The objective
function in Eq. (5) is non-increasing under the update rules:

According to the complementary slackness condition, we
have

vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ



ﬃ
u PV
ðLÞT
u j¼1 F Xij Cjð1:LÞ MijðLÞT þ aF Y i CYð1:LÞ MiY
u
ðlÞ




¼ Ri  tP
ð1:LÞ
ðLÞT
ð1:LÞ
ðLÞT
V
þ aF ViY CY MiY
Mij
j¼1 F Vij Cj

ðlÞ

Ri

(8)

Next, we verify that the limiting solution of the update
rule in Eq. (7) satisfies the above equation. When it converges, M ð1Þ ¼ M ðtþ1Þ ¼ M ðtÞ ¼ M where t is the number
of iteration, we have

(16)
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u
PV
ðLÞT ð2:LÞT
u
u ð1:LÞ
Mij Ri
u
j¼1 Xij Cj
u
u
Ri ¼ Ri  tP
V
u ð2:LÞ M ðLÞ C ð1:LÞT C ð1:LÞ M ðLÞT Rð2:LÞT
ij
j
j
ij
i
j¼1 Ri Ri

ðM  MÞ  B ¼ ðM  MÞ  A ) ðB  AÞ  ðM  MÞ ¼ 0:
(9)
The equivalence between Eqs. (8) and (9) completes the
proof.
u
t
Theorem 3 shows the multiplicative update rules for the
multi-latent space model defined in Eq. (4), and demonstrates its convergence and correctness.

Theorem 3 (Convergence of Pre-training). The objective
function in Eq. (4) is non-increasing under the update rules:
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
PV
u
T
T
u
j¼1 X ij C j Mij þ aY i C Y MiY
t
Ri ¼ Ri  PV
T
T
T
T
j¼1 Ri M ij Cj C j Mij þ aRi M iY CY C Y MiY
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u PV
u
T
u
j¼1 Xij C j Mij
Rui ¼ Rui  tPV
u
T
T
j¼1 Ri M ij Cj C j Mij

aRTi Ri M iY CYT C Y þ bVM iY

for any matrix A.
Theorem 4 shows the multiplicative update rules for the
hierarchical multi-latent space model defined in Eq. (5), and
demonstrates its convergence and correctness.

X

L  M ¼ 0 ) ðB  AÞ  M ¼ 0:

M iY ¼ M iY 

sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
aRTi Y i C Y þ b Vj¼1 M ij

(14)

ð1:l1ÞT
ðlþ1:LÞT
Ri
ARi

where
B¼a

(13)

sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
~T X
~ ij C j +bM iY
R
i
M ij ¼ M ij 
T
~
~
Ri Ri M ij CjT C j +bM ij

iY

@LðMÞ
¼ 0 ) L ¼ B  A;
@M

(12)

i

sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
PT
Y T Ri M iY
C Y ¼ C Y  PT i¼1 iT T
i¼1 C Y MiY Ri Ri M iY

Lemma 2. The limiting solution of the update rule in Eq. (7) satisfies the KKT condition.

i

ij

i¼1

Lemma 2 shows that the iterative update method in
Lemma 1 will converge to the stationary point.

Proof. For the function JðMÞ in Eq. (6) with non-negative
constraint, we introduce the Lagrangian function
X

X






X i  Ri MC T 
2 þ b

MP j  K j 
2
LðM Þ ¼a
i F
F

3159

(10)
(11)

(17)
ðlÞ

Cj

vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
uPT
~T R
~ð1:LÞ M ðLÞ C ðlþ1:LÞT
u i¼1 Cjð1:l1ÞT X
ij i
ij
j
ðlÞ
¼ Cj  tPT
ð1:l1ÞT ~ T ~ð1:LÞ
ðLÞ ðlþ1:LÞT
V R
C
M C
i¼1

ðlÞ

CY

j

ij

i

ij

vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u PT
ð1:l1ÞT T ð1:LÞ
ðLÞ ðlþ1:LÞT
u
CY
Yi Ri MiY CY
ðlÞ
¼ CY  tPTi¼1 ð1:l1ÞT
ð1:LÞ
ðLÞ ðlþ1:LÞT
VTiY Ri MiY CY
i¼1 CY

ðLÞ

Mij

vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u ~ð1:LÞT ~ ð1:LÞ
ðLÞ
uRi
þ bMiY
Xij Cj
ðLÞ
¼ Mij  t ð1:LÞT
~ ij C ð1:LÞ þ bM ðLÞ
~
V
R
i

ðLÞ

ðLÞ

MiY ¼ MiY 

j

ð1:LÞT

ð1:LÞ

ViY CY

(19)

(20)

ij

vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u ð1:LÞT
P
ð1:LÞ
ðLÞ
uaRi
Y i CY þ b Vj¼1 Mij
t
aRi

(18)

j

ðLÞ

þ bVMiY

;

(21)

where 1  l  L. Also, the limiting solutions of the update
rules satisfy the KKT condition.

3160

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

ðlÞ

After obtaining Ri

ðlÞ

ðlÞ

and Cj , we can update Mij and

 l < LÞ by using the update rule got in the pretraining phase.
Based on Theorems 3 and 4, we summarize the optimization algorithm for HiMLS in Algorithm 1. There are two
training phases including pre-training and fine-tuning in
Algorithm 1. As shown in Steps 1-9, the pre-training phase
goes forward from the first layer to the highest layer, and
each layer is trained in a greedy layer-wise manner. In contrast, the fine-tuning phase shown in Steps 10-17 moves in
an opposite direction, and the weights of all the layers will
be updated. The convergence of the HiMLS algorithm is
guaranteed by Theorems 3 and 4.

Algorithm 1. HiMLS Algorithm
~ ij ð1  i  T; 1  j  V Þ,
Input: Instance-feature matrices X
instance-label matrices for train data Y i ð1  i  T Þ, a; b,
number of layers L.
Output: Predicted instance-label matrices Fi ð1  i  T Þ for test
data.
1: for l ¼ 1 : L do
~ðlÞ ð1  i  T Þ, C ðlÞ and C ðlÞ ð1  j  V Þ by
2: Initialize R
i
j
Y
clustering instances, labels, and features using
probabilistic latent semantic analysis, respectively;
3:

ðlÞ

ðlÞy

ðl1Þ

ðlÞy

ðlÞ

ðlÞy

ðl1Þ

ðlÞy

Initialize M ij ¼ Ri Mij C j , M iY ¼ Ri M iY CY

1

1
where Ry ¼ RT R RT and C y ¼ C C T C . Note that
ð0Þ
~ ij , and M ð0Þ ¼ Yi ;
M ¼X
ij

iY

4:
5:

repeat
ðlÞ
Update Ri ð1  i  T Þ and Rui by Eq. (10) and Eq. (11);

6:

Update Cj ð1  j  V Þ and CY by Eq. (12) and Eq. (13);

ðlÞ

ðlÞ
Mij

ðlÞ

ðlÞ
MiY

12:
13:

Update
and
where 1  i  T; 1  j  V by
Eq. (14) and Eq. (15);
until converged
end for;
repeat
ðLÞ
ðLÞ
Update Mij and MiY where 1  i  T; 1  j  V by
Eq. (20) and Eq. (21);
for l ¼ L : 1 do
ðlÞ
Update Ri ð1  i  T Þ and Rui by Eq. (16) and Eq. (17);

14:

Update Cj ð1  j  V Þ and CY by Eq. (18) and Eq. (19);

7:
8:
9:
10:
11:

ðlÞ

ðlÞ
Mij

ðlÞ

ðlÞ
MiY

Update
and
where 1  i  T; 1  j  V; l 6¼ L
by Eq. (14) and Eq. (15);
16: end for;
17: until converged
18: return Predictions for the test data using Eq. (3).
15:

Time Complexity. Similar to other matrix factorization
methods based on multiplicative update rules [14], [25], a
nice property of the proposed HiMLS algorithm is that most
of the computations are matrix multiplications and can be
computed efficiently. Lemma 5 shows the complexity of the
algorithm. The proof is omitted for brevity.

Lemma 5 (Complexity). The time complexity for the multiplicative update rules in Theorem 3 are as follows:

NO. 12,

DECEMBER 2016

XV


2
OðRi Þ ¼ OðRui Þ ¼ O
n
N
pq
þ
q
þ
d
q
þ
mq
i
j
j¼1
XT


d N ni p þ pq þ p2
OðCj Þ ¼ O
i¼1 j
XT


2
mN
n
p
þ
pq
þ
p
OðCY Þ ¼ O
i
i¼1
 

OðMij Þ ¼ O pN ni dj þ qdj þ pq þ q2
 

OðMiY Þ ¼ O pN ni m þ qm þ pq þ q2 ;

Proof. According to Lemma 1, we can prove the convergence of the updating. According to Lemma 2, we can
prove that the limiting solutions satisfy the KKT
condition.
u
t
ðlÞ
MiY ð1

VOL. 28,

where 1  i  T; 1  j  V and N is the number of iteration
until convergence.
Note that the dimensions of the latent spaces are usually far smaller than the ones in the original spaces, i.e.,
p  ni and q  dj . Lemma 5 shows that the multiplicative update rules for pre-training are scalable to the
problem sizes. Likewise, we can obtain the time complexity of the update rules for fine-tuning, which are
omitted for brevity.

5

OPTIMIZATION ALGORITHM FOR HIMLSD

In this section, we introduce the optimization algorithm for
HiMLSD, which is the counterpart of HiMLS.
HiMLSD adopts the generalized Kullback-Leibler divergence (see Section 3.5) as loss metric. Therefore, the objective function defined in Eq. (1) for multi-latent space is
instantiated as,
min

fR;M;Cg > 0

þa

T
X

T X
V


X
~ ij jjR
~i M ij C T
D X
j
i¼1 j¼1

T X
V
X




D Y i jjRi M iY CYT þ b
D M ij jjM iY :

i¼1

(22)

i¼1 j¼1

The objective function defined in Eq. (2) for hierarchical
multi-latent space can be instantiated as,
min

fR;M;Cg > 0

þa

T
X

T X
V


X
~ ij jjR
~ð1:LÞ M ðLÞ C ð1:LÞT
D X
i
ij
j
i¼1 j¼1



ð1:LÞ
ðLÞ ð1:LÞT
D Y i jjRi MiY CY

(23)

i¼1

þb

T X
V


X
ðLÞ
ðLÞ
D Mij jjMiY :
i¼1 j¼1

Next, we derive Lemma 6, which is a generic method to
derive the multiplicative update rules for R; M; C in both
pre-training and fine-tuning of HiMLDS.

Lemma 6. For any non-negative matrices H; X; R; C and P , the
function


F ðH Þ ¼ aD XjjRHC T þ bDðHjjP Þ;

(24)

is non-increasing under the update:

H


H  aRT
bE



X
C
RHC T
T
þ aR EC

þ bP

;

(25)

YANG ET AL.: A GENERALIZED HIERARCHICAL MULTI-LATENT SPACE MODEL FOR HETEROGENEOUS LEARNING

where a and b are non-negative parameters. E is a unit matrix
whose dimensions are set wherever appropriate.

Proof. The function F ðHÞ can be rewritten as,

where W ¼ C  R. The key issue is to design an auxiliary
function for F ðhÞ. Denote

X
X


G h; ht ¼ a
xi log xi  xi þ
W
h
ij
j
j
i

W ij htj
W ij htj
P
a
xi P
log
W
h

log
ij
j
t
t
k W ik hk
k W ik hk
i;j
!
X
hj
hj log  hj þ pj :
þb
pj
j

X

W ij hj  

j

X
j

s:t: cj  0;

X

cj log

Setting cj ¼ P

k

log

X
j

W ik htk

W ij hj  

j

W ij hj
cj

cj ¼ 1:

P
W ij htj
W ij hj k W ik htk
P
:
t log
W ij htj
k W ik hk

!


X
xi W ij htj
dG h; ht
hj
P
þ b log
¼ a
¼ 0:
t  W ij
h
W
h
pj
dhj
j
ik
k
k
i
Since log x  1  1=x with x ! 1 [6], the above equation
can be approximated as:
a

i

xi W ij htj
P
 W ij
hj k W ik htk

!


	
pj
þb 1
¼ 0:
hj

u
t
Similar to the proof in Lemma 6, we can derive the
following lemma.
X



ak D X k jjRk HCkT ;

(27)

is non-increasing under the update rule:

	
P
Xk
T
H
k ak Rk Rk HC T C k þ bP
k
P
H
;
bE þ k ak RTk E k C k

(28)

where b and ak are non-negative parameters. E (or Ek ) is a
unit matrix whose dimensions are set wherever appropriate.
Lemma 8 shows that the iterative update method in
Lemma 6 will converge to the stationary point.

Lemma 8. The limiting solution of the update rule in Eq. (25)
satisfies the KKT condition.



From this inequality it follows
that G h; ht  F ðhÞ.


The minimum of G h; ht with respect to h is determined by setting the gradient to zero:

X

aW T 
 1 þ b 
 1
 
  X 
ht  a C T  RT vec RHC
þ bp
T
 T

¼
a 
 vec R EC þ b 
 1


t
X
h  vec aRT RHC
T C þ bvecðP Þ


¼
a 
 vec RT EC þ bvecðE Þ


X
H  aRT RHC
C
þ bP
T
)H
:
T
aR EC þ bE

k

, we obtain,

X

Paxi W ij t þ bpj
W ik h
k
P k
b þ a i W ij


x
þ bp
ht  aW T Wh
i

J ðH Þ ¼ bDðHjjP Þ+

j
W ij htj

P

Lemma 7. The function

!



To show that G h; ht is an auxiliary function ofF ðhÞ,
we need to prove: (1) Gðh; hÞ ¼ F ðhÞ; (2) G h; ht
 F ðhÞ. The first equation is straightforward. To prove
the latter inequality, we use the convexity of log
function:
 log

htj

)h¼

¼ aDðvecðXÞjjðC  RÞvecðH ÞÞ+bDðvecðH ÞjjvecðP ÞÞ
¼ aDðxjjWhÞ+bDðhjjpÞ
!
X
X
xi
¼a
xi log P
 xi þ
W ij hj
j
j W ij hj
i
!
X
hj
hj log  hj þ pj ;
þb
pj
j

X

According to Eq. (26), we have,
hj ¼



F ðhÞ ¼ aD XjjRHC T +bDðHjjP Þ



¼ aD vecðX Þjjvec RHC T þ bDðvecðHÞjjvecðP ÞÞ

3161

(26)

Proof. For the function F ðHÞ in Eq. (24) with non-negative
constraint, we introduce the Lagrangian function
!
X
X
xi
xi log P
 xi þ
W ij hj
LðhÞ ¼a
j
j W ij hj
i
!
X
hj
hj log  hj þ pj  trðLhT Þ;
þb
pj
j
where LðL  0Þ is the Lagrangian multiplies vector. The
zero gradient condition gives

	
X
@LðhÞ
xi
hj
þ b log :
¼ 0 ) Lj ¼ a
Wij 1  P
@hj
W
h
pj
ik k
k
i
According to the complementary slackness condition
Lj  hj ¼ 0, we have
"
#

	
X
xi
hj
þ b log

 hj ¼ 0:
a
Wij 1  P
(29)
pj
k Wik hk
i
Likewise, when x ! 1, log x  1  1=x, the above equation can be approximated as:

3162

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

"
a

X
i


	

	#
xi
pj
þb 1

 hj ¼ 0:
Wij 1  P
hj
k Wik hk

ðLÞ
Mij

(30)

ðLÞ

Mij

ðtþ1Þ
hj

verges,
¼
iteration, we have
hj b þ a

X

ðtÞ
hj

¼

Wij


ðLÞ
ð1:LÞT
MiY  aRi

ðLÞ

¼ hj

i

MiY

X axi Wij
P
þ bpj :
k Wik hk
i

M iY

Yi
Ri M iY CYT

CY þ b

M ij
j¼1 M iY


ðlÞ
ð1:l1ÞT
Ri  aRi

ð lÞ
Ri

Ri

P
X
T
Ri  a R MY i C T C Y MiY
þ Vj¼1 R M ijC T C j MijT
i iY Y
i ij j
PV
T þ
T
aE i C Y MiY
j¼1 E ij C j Mij

Rui

CjT

CYT

Rui 

PV

u
Xij
j¼1 Ru M ij C T
j
i

ð1:l1ÞT

aRi

T
j¼1 E ij C j Mij

~
P
~T Xij T
CjT  Ti¼1 MijT R
i R
~i M ij C
j
PT
TR
~T E ij
M
ij i
i¼1

P
T
CYT  Ti¼1 MiY
RTi R MY i C T
i iY Y
:
PT
T
T
i¼1 MiY Ri E ij

ðLÞ

ð1:LÞ

þb

ð1:LÞT CY

MiY CY

Yi
T
ViY ð0Þ ViY ðlÞ

E i VTiY ðlÞ þ

Rui 

Rui

P

PV
j¼1

ðLÞ

Mij

	

ðLÞ

MiY

þ bV 
 E

þ
PV

PV

ð1:l1ÞT

j¼1

Ri

ð1:l1ÞT

Ri

j¼1



X ij
T
Vij ð0Þ Vij ðlÞ

E ij VTij ðlÞ

ðlÞT

ðlÞT

Cj



PT
i¼1

PV

QTij ðlÞ

PT

Cj

i¼1

ðlÞT
CY

CY



PT
i¼1

i¼1

(33)

(41)

E ij VTij ð1Þ
~ ij
X

~ð1:LÞ M ðLÞ C ð1:LÞT
R
i
ij
j

ð1:l1Þ

Cj

(42)

ð1:l1Þ

QTij ðlÞE ij Cj

QTiY ðlÞ

PT



u
Xij
V
T
j¼1 Ru Vij ð1Þ Vij ð1Þ
i

j¼1

ðlÞT

ð1:LÞ

Ri

Yi

ðLÞ ð1:LÞT

MiY CY

ð1:l1Þ

QTiY ðlÞE i CY

ð1:l1Þ

CY

;

(43)

where 1  l  L. Also, the limiting solutions of the update
rules satisfy the KKT condition.

	
(34)

C j MijT

PV

ð1:LÞ

Ri

Yi

(40)

	

aRTi E i C Y þ bV 
 E


ðLÞ

þ bMiY

(39)

Theorem 9 (Convergence of Pre-training). The objective
function in Eq. (22) is non-increasing under the update rules:

	
~
~T Xij T C j þ bM iY
M ij  R
i R
~i M ij C
j
(32)
M ij
~T E ij C j þ bE
R
i
M iY  aRTi

ð1:LÞ
C
~ð1:LÞ M ðLÞ C ð1:LÞT j
R
i
ij
j

ð1:LÞT
ð1:LÞ
aRi
E i CY

Next we derive the multiplicative update rules for pretraining and fine-tuning in HiMLDS. Theorem 9 shows the
multiplicative update rules for Eq. (22), and demonstrates
its convergence and correctness.

PV

!

~ ij
X

(31)

The equivalence between Eqs. (30) and (31) completes the
proof.
u
t



DECEMBER 2016

(38)

¼ hj where t is the index of

!

NO. 12,

~ð1:LÞT E ij C ð1:LÞ þ bE
R
i
j

Next, we verify that the limiting solution of the update
rule in Eq. (25) satisfies the above equation. When it conð1Þ
hj



~ð1:LÞT
R
i

VOL. 28,

(35)

Proof. According to Lemma 7, we can prove the convergence of the updating. According to Lemma 8, we can
prove that the limiting solutions satisfy the KKT
condition.
u
t
Likewise, we can obtain the algorithm for HiMLSD and
its time complexity, which are omitted for brevity.

6
(36)

(37)

Proof. According to Lemma 7, we can prove the convergence of the updating. According to Lemma 8, we can
prove that the limiting solutions satisfy the KKT
condition.
u
t
ðlþ1:LÞ
ðLÞ ð1:LÞT
ðlþ1:LÞ
ðLÞ
Mij Cj
, ViY ðlÞ ¼ Ri
MiY
Define Vij ðlÞ ¼ Ri
ð1:LÞT
~ð1:LÞ M ðLÞ C ðlþ1:LÞT , and QiY ðlÞ ¼ Rð1:LÞ M ðLÞ
CY
, Qij ðlÞ ¼ R
i
ij
j
i
iY
ðlþ1:LÞT
CY
.

Theorem 10 shows the multiplicative update rules for
Eq. (23), and demonstrates its convergence and correctness.

Theorem 10 (Convergence of Fine-tuning). The objective
function in Eq. (23) is non-increasing under the update rules:

THE SPECIAL CASES OF HIMLS

The proposed model is a generalized framework for learning complex heterogeneity. It is widely applicable to multiple types of heterogeneous learning problems.
A special case of HiMLS is to learn the common co-latent
space M shared among all the tasks, view, and labels, i.e.,
Mij ¼ MiY ¼ Mð1  i  T; 1  j  V Þ. And by using the
training data only, Eq. (4) can be specialized as
min

R;M;C


2
T X
V 

T 

X
X







Y i  Ri MC T 
2 :

Xij  Ri MCjT 
 þ a
Y F
i¼1 j¼1

F

i¼1

(44)
It is worth noting that Eq. (44) is not a trivial special case.
Theorem 11 shows that some popular methods for learning
from single heterogeneity can be viewed as the special cases
of our proposed model, such as the multi-view learning
method MSL [36] and the multi-label learning method
LS-CCA [32]. Both MSL and LS-CCA are closely related to
canonical correlation analysis (CCA), while MSL is an unsupervised learning method aiming to learn the subspace

YANG ET AL.: A GENERALIZED HIERARCHICAL MULTI-LATENT SPACE MODEL FOR HETEROGENEOUS LEARNING

from multiple views, and LS-CCA is a supervised learning
method for the multi-label problem when one of the views
used in CCA is derived from the labels.

Theorem 11. The multi-view learning method MSL [36] and the
multi-label learning method LS-CCA [32] can be viewed as the
special cases of HiMLS.
Proof. Consider two special cases of HiMLS for learning
from a single heterogeneity as follows:
1) Unsupervised multi-view learning: By letting
T ¼ 1, V ¼ 2, and a ¼ 0, Eq. (44) can be rewritten into:



2 


2
min 
X1  RMC1T 
F þ
X2  RMC2T 
F :

R;M;C

(45)

where Xj ðj ¼ 1; 2Þ is the instance-feature matrix for the
jth view.
2) Supervised multi-label learning: By letting T ¼ 1,
V ¼ 1, and a ¼ 1, Eq. (44) can be rewritten into:



2 


2
min 
X  RMC1T 
F þ
Y  RMC2T 
F ;

R;M;C

(46)

where X and Y are the instance-feature matrix and
instance-label matrix, respectively.
Both Eqs. (45) and (46) have the same form as follows:



2
min 
½X; Y 	  HC T 
F ;

(47)

H;C



where H ¼ RM and C T ¼ C1T ; C2Th . Consider the normali
1

1

ized data matrix defined as Z ¼ ðXX T Þ2 X; ðYY T Þ2 Y .

When imposing the orthogonal constraint C T C ¼ I,
Eq. (47) can be rewritten into:



2
min 
Z  HC T 
F :

H;C T C¼I

(48)

Let f ðH; C Þ denote the objective function for Eq. (48),
which can be transformed into:


f ðH; C Þ ¼ tr Z T Z  2CH T Z þ H T H :
When fixing C, we have:
rH f ðH; C Þ ¼ 2ZC þ 2H ¼ 0 ) H ¼ ZC:
By substituting H ¼ ZC into Eq. (48), we have



2
min 
Z  HC T 
F
T
C C¼I


¼ min tr Z T Z  2CC T Z T Z þ C T Z T XC
T
C C¼I




¼ tr Z T Z  max tr C T Z T ZC :

(49)

C T C¼I

The optimal solution for C is given by the top k eigenvectors of Z T Z. According to [36], Eq. (49) has the same
optimal solution with CCA which aims to optimize:


s:t: U T XXT U ¼ V T YY T V ¼ I:
max tr U T XY T V
U;V

Therefore, the first special case of HiMLS is equivalent to
applying CCA to the instance-feature matrices from multiple views [36]. The second special case of HiMLS is

3163

equivalent to applying CCA to both the instance-feature
matrix and instance-label matrix [32].
u
t

7

EXPERIMENTS

In this section, we demonstrate the effectiveness of the proposed algorithms on various data sets in comparison with
different heterogeneous learning methods.

7.1 Data Sets and Setup
Four real data sets from different domains are used for evaluation, including text, image, and manufacturing data.
The first data set is the Reuters Corpus Volume I
(RCV1V2)1 data set [26], which is a collection of over
800,000 newswire stories. There are three category sets of
data: Topics (i.e., major subject of a story), Industry Codes
(i.e., type of business discussed), and Regions (i.e., geographic locations). Each of these category sets has a hierarchical structures. It is usually common to use several
subsets of this data, each containing 6,000 data instances on
average and with a total number of 101 class labels.
EUR-Lex [27] is a text data set containing European
Union official laws in practice, different kinds of treaties
and agreements, parliamentary journals. This data set contains nearly 20,000 text documents classified according to
three different schemas: i) subject matter (e.g., agriculture),
ii) official classification hierarchy called the directory codes
(e.g., a document belonging to a class also belongs to all its
parent classes), and iii) EUROVOC, a multilingual thesaurus maintained by the Office for Official Publications of the
European Communities. Each of these category sets forms a
hierarchical structures.
NUS-WIDE2 [9] is the a real-world web image data set
comprising over 269,000 images with over 5,000 user-provided tags, and ground-truth of 81 concepts with a hierarchical structures. There are several types of low-level visual
features such as 64-D color histogram in LAB color space,
144-D color correlogram in HSV color space, 73-D edge distribution histogram, and 500-D bag of visual words. We use
the light version of NUS-WIDE.
In these data sets, the label refer to the multiple categories each instance belonging to. For the NUS-WIDE data,
the view refers to different types of low-level visual feature.
For either RCV1V2 or EUR-Lex data sets, similar to [42], the
data are described from two views: one corresponds to the
TF-IDF features; another corresponds to the latent topics
obtained by applying probabilistic latent semantic analysis3
on the term counts. The task refers to classify the instances
belonging to different sub-categories, which follow different
but related distributions [18].
The last data set AL-SMELT is related to manufacturing
process. AL-SMELT is collected from Aluminum smelting
process. This data set corresponds to an electrolytic process
with 174 process variables that forms 4 views based on the
process control practice: power and resistance, noise control, feed control, and chemicals. It is concurrently running
in 245 smelters, which can be classified into 5 groups (tasks)
based on their design and generation. The 174 process
1. http://mulan.sourceforge.net/datasets-mlc.html
2. http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm
3. http://lear.inrialpes.fr/people/verbeek/code

3164

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

TABLE 1
Statistics of Different Data Sets
Data set
RCV1V2_1
RCV1V2_2
RCV1V2_3
RCV1V2_4
EUR-Lex
NUS-WIDE
AL-SMELT

47,236
47,236
47,229
47,236
5,000
708
174

101
101
101
101
412
81
2

2.880
2.634
2.614
2.484
1.292
1.869
0.986

0.029
0.026
0.026
0.025
0.003
0.023
0.493

1,028
954
939
816
1,615
18,430
4

TABLE 2
Comparison Among HiMLS Variants on RCV1V2_1 Data Set
Algorithm

F1 -score

Accuracy

HiMLS
MLS
MLS-T
MLS-V
MLS-S

.906
.006
.868
.023
.864
.008
.857
.009
.847
.018

.885
.006
.829
.028
.822
.009
.813
.010
.805
.020

NO. 12,

DECEMBER 2016

TABLE 3
Comparison Among HiMLS Variants on RCV1V2_2 Data Set

Instances Features Labels Cardinality Density Diversity
6,000
6,000
6,000
6,000
19,348
55,615
6,468

VOL. 28,

Hamming loss
.064
.003
.102
.018
.106
.006
.107
.005
.116
.011

variables are collected automatically at daily level via sensors. Two other important control variables, temperature
and Alumina Fluoride, are collected every other day manually. Here, the goal is to predict the change direction
(increase or decrease) of these 2 variables (labels) when they
are not collected. The prediction fills in the information gap
and enables feedback control in a finer granularity.
Table 1 shows the properties of different data sets. Label
cardinality is the average number of labels per instance.
Accordingly, label density normalizes label cardinality by
the the number of labels. Label diversity is the number of
distinct label combinations observed in the data set [46].

7.2 Evaluation Metrics
In order to comprehensively investigate the performance of
the proposed method, we use F1 -score, accuracy and Hamming loss on the test data as the evaluation metrics.
F1 -score [46] is the harmonic mean of precision and recall
where precision is the proportion of predicted correct labels
to the total number of actual labels, recall is the proportion
of predicted correct labels to the total number of predicted
labels, averaged over all instances. Note that the larger
value of F1 -score is indicating the better performance.
Accuracy [46] for each instance is defined as the proportion of the predicted correct labels to the total number of
labels for that instance. Overall accuracy is the average
across all instances. Note that the larger value of accuracy is
indicating the better performance.
Hamming Loss [46] reports how many times on average,
the relevance of an instance to a class label is incorrectly predicted. Therefore, hamming loss takes into account the prediction error (an incorrect label is predicted) and the
missing error (a relevant label not predicted), normalized
over total number of classes and total number of instances.
Note that the smaller the value of Hamming loss, the better
the performance of the learning algorithm.
7.3 Effectiveness of HiMLS Components
First of all, we aim to verify the effectiveness of each
component in the proposed model, and demonstrate the

Algorithm
HiMLS
MLS
MLS-T
MLS-V
MLS-S

F1 -score

Accuracy

.903
.006
.880
.006
.872
.003
.856
.026
.842
.011

.872
.006
.846
.008
.828
.005
.818
.030
.814
.014

Hamming loss
.073
.003
.089
.004
.098
.005
.102
.017
.100
.007

TABLE 4
Comparison Among HiMLS Variants on RCV1V2_3 Data Set
Algorithm
HiMLS
MLS
MLS-T
MLS-V
MLS-S

F1 -score

Accuracy

.900
.006
.878
.011
.871
.019
.860
.003
.854
.018

.869
.006
.844
.012
.841
.022
.820
.004
.814
.024

Hamming loss
.076
.003
.096
.007
.091
.012
.103
.001
.106
.013

advantages of simultaneously modeling the multiple heterogeneity in one framework. Therefore, we compare
HiMLS with its four special cases: 1) multi-task multi-view
variant MLS; 2) multi-task single-view variant MLS-T; 3)
multi-view single-task variant MLS-V; 4) single-task singleview variant MLS-S. Each of these four variants has only
one layer.
HiMLS and MLS are input with multi-task and multiview data. For the single-view setting, the features from
all the views are concatenated into one single view. For
the single-task setting, the instances in all the tasks are
pooled into one single task. For HiMLS, we set the number of layers L ¼ 2, and the numbers of latent topics ½p; q	
for the instances and features(or labels) to ½200; 100	,
½40; 20	 in the first and second layer, respectively. For
all the other methods with only one layer, we set
½p; q	 ¼ ½40; 20	.
The classification performances of HiMLS and its variants on RCV1V2 data sets are shown on Tables 2, 3, 4, and
5. Based on these comparison results, we have the following
findings:






Both MLS-T and MLS-V perform better than MLS-S
in most cases by incorporating either task relatedness or view consistency. It suggests that simply
concatenating the features from different views is
not the best way to model the view heterogeneity;
likewise, simply pooling the instances of all tasks
into one single task is not the best way to model the
task heterogeneity.
MLS perform better than either MLS-T or MLS-V in
most cases. It suggests that jointly modeling multiple
types of heterogeneity can gain performance improvement upon single-heterogeneity learning.
HiMLS performs better than MLS. It indicates that the
learned hierarchical multi-latent space helps build a
more robust and discriminative classifier. One possible reason to account for this is that the multi-layer
structure helps find the more accurate local optimum
by gradually learning the abstract concepts. In contrast, the single-layer methods may suffer from the
local optimal solution in lower quality.

YANG ET AL.: A GENERALIZED HIERARCHICAL MULTI-LATENT SPACE MODEL FOR HETEROGENEOUS LEARNING

TABLE 5
Comparison Among HiMLS Variants on RCV1V2_4 Data Set
Algorithm

F1 -score

Accuracy

HiMLS
MLS
MLS-T
MLS-V
MLS-S

.894
.002
.874
.010
.864
.019
.859
.016
.851
.013

.860
.002
.835
.012
.825
.023
.816
.020
.807
.016

Hamming loss
.082
.002
.097
.008
.103
.014
.106
.013
.113
.011

TABLE 8
Classification Performance on RCV1V2_3
Algorithm

F1 -score

Accuracy

HiMLS
HiMLSD
L2 F
ML-kNN
LS-ML
TRAM

.900
.006
.889
.007
.837
.008
.764
.005
.816
.010
.873
.006

.869
.006
.860
.007
.788
.010
.738
.006
.785
.006
.846
.005

TABLE 6
Classification Performance on RCV1V2_1
Algorithm

F1 -score

Accuracy

HiMLS
HiMLSD
L2 F
ML-kNN
LS-ML
TRAM

.906
.006
.889
.007
.847
.011
.803
.092
.821
.021
.888
.003

.885
.006
.858
.007
.802
.015
.775
.094
.789
.019
.857
.004

F1 -score

Accuracy

HiMLS
HiMLSD
L2 F
ML-kNN
LS-ML
TRAM

.903
.006
.911
.004
.884
.005
.772
.009
.828
.019
.874
.004

.872
.006
.881
.004
.850
.005
.751
.008
.799
.016
.848
.004

Hamming loss
.076
.003
.080
.005
.120
.005
.115
.001
.107
.003
.081
.003

TABLE 9
Classification Performance on RCV1V2_4

Hamming loss
.064
.003
.082
.003
.110
.009
.102
.038
.109
.005
.082
.003

Algorithm

F1 -score

Accuracy

HiMLS
HiMLSD
L2 F
ML-kNN
LS-ML
TRAM

.894
.002
.913
.002
.858
.005
.754
.005
.831
.017
.870
.004

.860
.002
.866
.004
.816
.005
.728
.007
.801
.015
.851
.006

TABLE 7
Classification Performance on RCV1V2_2
Algorithm

3165

7.4 Performance Comparison
The second experiment is to compare the proposed method
with various heterogeneous learning algorithms. In this
work, we focus on improving the performance of multilabel learning by leveraging the multiple type of heterogeneity. To the best of our knowledge, there is no previous
work for learning from the triple heterogeneity. Therefore,
we compare our proposed approach with a variety of multilabel learning methods which learn from single or dual heterogeneity. The comparison approaches includes: 1) multiview multi-label learning methods L2 F [40]; 2) graph-based
multi-label approach ML-kNN [45]; 3) multi-label method
based on subspace learning LS-ML [22]; 4) transductive
multi-label learning approach TRAM [23].
In addition, we compare the two alternative algorithms
of our proposed approach, i.e., HiMLS and HiMLSD, to
examine their performance differences. Note that HiMLS is
based on least squares loss function, while HiMLSD is based
on generalized KL divergence. In order to conduct a fair
comparison between them, the same initializations are used
for HiMLS and HiMLSD.
HiMLS (or HiMLSD) is input with multi-task and multiview data. For the other algorithms, the instances of all the
tasks are pooled together. L2 F method is given the multiview features, whereas the other methods are given the
concatenated features from all the views. The parameters
are tuned for each algorithm using cross-validation on the
training data. We repeat the experiments ten times for each
data set and report the average performances and the standard deviations.

.082
.002
.077
.002
.106
.005
.118
.004
.104
.004
.075
.004

TABLE 10
Classification Performance on EUR-Lex

Hamming loss
.073
.003
.071
.002
.083
.003
.103
.001
.100
.004
.079
.002

Hamming loss

Algorithm

F1 -score

Accuracy

HiMLS
HiMLSD
L2 F
ML-kNN
LS-ML
TRAM

.749
.009
.740
.009
.713
.020
.498
.029
.664
.013
.667
.016

.719
.011
.707
.008
.680
.020
.472
.027
.631
.013
.635
.016

Hamming loss
.033
.002
.034
.001
.033
.003
.043
.002
.088
.006
.040
.002

TABLE 11
Classification Performance on NUS-WIDE
Algorithm

F1 -score

Accuracy

HiMLS
HiMLSD
L2 F
ML-kNN
LS-ML
TRAM

.675
.007
.649
.008
.700
.001
.589
.004
.628
.021
.684
.007

.645
.006
.634
.009
.615
.002
.582
.003
.618
.020
.676
.008

Hamming loss
.187
.003
.192
.005
.204
.002
.215
.002
.190
.008
.166
.003

Tables 6, 7, 8, and 9 show the classification performances
of different methods on RCV1V2. The performances on
EUR-Lex, NUS-WIDE, and AL-SMELT are shown in
Tables 10, 11, and 12, respectively.
The results show that both HiMLS and HiMLSD perform
better than the other algorithms in most cases. LS-ML [22]
learns a common subspace shared among multiple labels,
which helps improve the learning performance for the
multi-label data. However, since its objective function is
non-convex, the performance of LS-ML may be limited by
the local optimum problem. TRAM [23] is a transductive
multi-label learning method which tries to exploit the information from unlabeled data to estimate the optimal label
concept compositions. The results show that unlabeled
data can provide helpful information to build the multilabel classifier. For ML-kNN [45], since it ignores the correlation among multiple labels, its performance on these data

3166

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

TABLE 12
Classification Performance on AL-SMELT
Algorithm

F1 -score

Accuracy

HiMLS
HiMLSD
L2 F
ML-kNN
LS-ML
TRAM

.848
.003
.873
.006
.773
.001
.845
.001
.852
.000
.383
.001

.847
.003
.871
.006
.772
.001
.842
.000
.850
.001
.381
.001

Hamming loss
.140
.003
.115
.006
.214
.000
.139
.001
.131
.001
.605
.000

sets is not comparable with the other methods in most cases.
Different from these methods for learning from single heterogeneity, both HiMLS (or HiMLSD) and L2 F [40] model
the feature and label heterogeneity and gain performance
improvement by enhancing the view consistency. It suggests that treating the features from different views in a discriminative and complementary way is usually better than
just concatenating all the features into one view. Likewise,
treating the instances in different tasks discriminatively is
usually better than just pooling all the instances together.
The performance superiority of the proposed method over
the comparison methods verifies the effectiveness of the
proposed approach to model the complex heterogeneity in
a principled framework. Another important competency of
the proposed method is that its multi-layer structure helps
build a robust classifier by gradually finding the more highlevel concepts in the deep structures.
TRAM performs a little better than HiMLS (or HiMLSD)
on NUS-WIDE data set. It indicates that NUS-WIDE may be
consistent with the smoothness assumption, and TRAM is
able to effectively leverage this assumption. However,
TRAM shows relative poor performance on AL-SMELT
data suggesting that the transductive method may be misled by the unlabeled information.
The results show that the performances of HiMLS and
HiMLSD are comparable. Each of them wins on three out of
seven data sets. It suggests that they adapt to different data
set. Both of them provide the alternative methods to model
the heterogeneous data.

7.5 Parameter Sensitivity
We study the parameter sensitivity on the RCV1V2_1 data
set. a and b are tuned on the grid 10½3:1:3	 . The results are
shown in Figs. 2a, and 2b. a is used to balance the importance of classification loss. The algorithm performs worse as
a approaches 0. When a ¼ 0, it means that no label information is used for training. The optimal performance is
achieved at a ¼ 1. Nevertheless, the performance is quite
robust over a wide range of values of a. b is used to control

VOL. 28,

NO. 12,

DECEMBER 2016

TABLE 13
Performance Varies with Number of Layers
L

F1 -score

Accuracy

1
2
3
4
5

.868
.023
.874
.002
.884
.003
.891
.003
.906
.006

.829
.028
.842
.003
.855
.005
.864
.003
.885
.006

Hamming loss
.102
.018
.092
.001
.083
.002
.079
.002
.064
.003

the importance of regularization. The result shown in
Fig. 2b indicates that setting appropriate weight to the regularization term can lead to better performance. As a result,
we tune the parameters, a and b, for each data set by crossvalidation on the training data.
We empirically study the convergence of HiMLS on the
RCV1V2_1 data set. The result is shown in Fig. 2c. From this
figure, we can see that HiMLS converges fast and its performance becomes stable after a few iterations. Thus, we terminate the algorithm after a maximum of 50 iterations.

7.6 Impact of Layers
It is interesting to investigate how the number of layers L
affects the performance of the proposed approach (e.g.,
HiMLS). We set L ¼ 1; 2; 3; 4; 5, and the numbers of latent
topics in each layer are 40,100,400,1000,4000, respectively.
We set p ¼ q here. Table 13 shows the results on the
RCV1V2_1 data set. We can see that the performances
(F1 -score, accuracy, and Hamming loss) are consistently
improved when the number of layers increased from 1 to 5.
It demonstrates that the multi-layer structure improves the
performance by learning the hierarchical abstract concepts
from data. When L keeps increased from 5, we have not
observed the significant improvement of performance. Our
conjecture is that the algorithm may have approached the
local optimum. Therefore, we empirically set L ¼ 5.

8

CONCLUSION

We propose a multi-layer framework to jointly model triple
heterogeneity. In each layer, it learns a multi-latent space
shared among the heterogeneous data. Then the multi-latent
model is used as a building block to stack up a multi-layer
structure so as to gradually learn the more abstract concepts.
Based on this generalized framework, we present two alternative models using different divergence measures. A deep
learning algorithm is proposed to solve the optimization problem in each model, which first pre-trains each layer and then
fine-tunes the whole multi-layer structure by using the multiplicative update rules. The comparison experiments with various heterogeneous learning methods demonstrate the
effectiveness of the proposed model.

Fig. 2. From left to right: a) F1 -score versus a (log10 scale); b) F1 -score versus b (log10 scale); c) F1 -score versus iteration.

YANG ET AL.: A GENERALIZED HIERARCHICAL MULTI-LATENT SPACE MODEL FOR HETEROGENEOUS LEARNING

ACKNOWLEDGMENTS
This work is supported by the NSF research grant IIS1552654, ONR Research grant N00014-15-1-2821, IBM Faculty Award, and NSFC research grant 61473123. The views
and conclusions are those of the authors and should not be
interpreted as representing the official policies of the funding agencies or the governments.

REFERENCES
[1]
[2]
[3]

[4]
[5]
[6]
[7]
[8]
[9]

[10]

[11]
[12]
[13]

[14]

[15]
[16]
[17]
[18]
[19]
[20]
[21]

R. K. Ando and T. Zhang, “A framework for learning predictive
structures from multiple tasks and unlabeled data,” J. Mach. Learning Res., vol. 6, pp. 1817–1853, 2005.
A. Argyriou, T. Evgeniou, and M. Pontil, “Multi-task feature
learning,” in Proc. Adv. Neural Inf. Process. Syst., 2006, pp. 41–48.
A. Banerjee, I. S. Dhillon, J. Ghosh, S. Merugu, and D. S. Modha,
“A generalized maximum entropy approach to Bregman co-clustering and matrix approximation,” J. Mach. Learning Res., vol. 8,
pp. 1919–1986, 2007.
A. Blum and T. Mitchell, “Combining labeled and unlabeled data
with co-training,” in Proc. 11th Annu. Conf. Comput. Learning Theory, 1998, pp. 92–100.
D. Cai, X. He, J. Han, and T. S. Huang, “Graph regularized nonnegative matrix factorization for data representation,” IEEE Trans.
Pattern Anal. Mach. Intell., vol. 33, no. 8, pp. 1548–1560, Aug. 2011.
D. Cai, X. He, X. Wang, H. Bao, and J. Han, “Locality preserving
nonnegative matrix factorization,” in IJCAI, 2009, pp. 1010–1015.
R. Caruana, “Multitask learning,” Mach. Learning, vol. 28, no. 1,
pp. 41–75, 1997.
N. Chen, J. Zhu, and E. P. Xing, “Predictive subspace learning for
multi-view data: a large margin approach,” in Proc. Advances
Neural Inf. Process. Syst., 2010, pp. 361–369.
T. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng,
“NUS-WIDE: A real-world web image database from national
university of singapore,” in Proc. ACM Int. Conf. Image Video
Retrieval, 2009, Art. no. 48.
A. Cichocki, R. Zdunek, A. H. Phan, and S. ichi Amari, Nonnegative Matrix and Tensor Factorizations - Applications to Exploratory
Multi-Way Data Analysis and Blind Source Separation, Hoboken, NJ,
USA: Wiley, 2009.
C. H. Q. Ding and X. He, “On the equivalence of nonnegative
matrix factorization and spectral clustering,” in Proc. SIAM Int.
Conf. Data Mining, 2005, pp. 606–610.
C. H. Q. Ding, T. Li, and M. I. Jordan, “Convex and seminonnegative matrix factorizations,” IEEE Trans. Pattern Anal.
Mach. Intell., vol. 32, no. 1, pp. 45–55, Jan. 2010.
C. H. Q. Ding, T. Li, and W. Peng, “On the equivalence between
non-negative matrix factorization and probabilistic latent semantic indexing,” Comput. Statistics Data Anal., vol. 52, no. 8, pp. 3913–
3927, 2008.
C. H. Q. Ding, T. Li, W. Peng, and H. Park, “Orthogonal nonnegative matrix t-factorizations for clustering,” in Proc. 12th ACM
SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2006, pp. 126–
135.
A. Elisseeff and J. Weston, “A kernel method for multi-labelled
classification,” in Proc. Adv. Neural Inf. Process. Syst., 2001,
pp. 681–687.
J. D. R. Farquhar, D. R. Hardoon, H. Meng, J. Shawe-Taylor, and S.
Szedm
ak, “Two view learning: SVM-2K, theory and practice,” in
Proc. Adv. Neural Inf. Process. Syst., 2005, pp. 355–362.
P. Gong, J. Ye, and C. Zhang, “Robust multi-task feature
learning,” in Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data
Mining, 2012, pp. 895–903.
J. He and R. Lawrence, “A graph-based framework for multi-task
multi-view learning,” in Proc. 28th Int. Conf. Mach. Learning, 2011,
pp. 25–32.
G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of data with neural networks,” Science, vol. 313, no. 5786,
pp. 504–507, 2006.
S.-J. Huang, Y. Yu, and Z.-H. Zhou, “Multi-label hypothesis
reuse,” in Proc. ACM SIGKDD Int. Conf. Knowl. Discovery Data
Mining, 2012, pp. 525–533.
S.-J. Huang and Z.-H. Zhou, “Multi-label learning by exploiting label
correlations locally,” in Proc. AAAI Conf. Artif. Intell., 2012, pp. 1–7.

3167

[22] S. Ji, L. Tang, S. Yu, and J. Ye, “Extracting shared subspace for
multi-label classification,” in Proc. ACM SIGKDD Int. Conf. Knowl.
Discovery Data Mining, 2008, pp. 381–389.
[23] X. Kong, M. K. Ng, and Z.-H. Zhou, “Transductive multilabel
learning via label set propagation,” IEEE Trans. Knowl. Data Eng.,
vol. 25, no. 3, pp. 704–719, Mar. 2013.
[24] D. D. Lee and H. S. Seung, “Learning the parts of objects by nonnegative matrix factorization,” Nature, vol. 401, pp. 788–791, 1999.
[25] D. D. Lee and H. S. Seung, “Algorithms for non-negative matrix
factorization,” in Proc. Adv. Neural Inf. Process. Syst., pp. 556–562, 2000.
[26] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li, “RCV1: A new benchmark collection for text categorization research,” J. Mach. Learning
Res., vol. 5, pp. 361–397, 2004.
[27] E. L. Mencıa and J. F€
urnkranz, “Efficient pairwise multilabel
classification for large-scale problems in the legal domain,” in
Proc. Eur. Conf. ECML-PKDD, 2008, pp. 126–135.
[28] A. D. Pascual-Montano, J. M. Carazo, K. Kochi, D. Lehmann, and
R. D. Pascual-Marqui, “Nonsmooth nonnegative matrix factorization (nsNMF),” IEEE Trans. Pattern Anal. Mach. Intell., vol. 28,
no. 3, pp. 403–415, Mar. 2006.
[29] V. Sindhwani and D. S. Rosenberg, “An RKHS for multi-view
learning and manifold co-regularization,” in Proc. 25th Int. Conf.
Mach. Learning, 2008, pp. 976–983.
[30] K. Sridharan and S. M. Kakade, “An information theoretic framework for multi-view learning,” in Proc. Annu. Conf. Comput. Learning Theory, 2008, pp. 403–414.
[31] L. Sun, S. Ji, and J. Ye, “Hypergraph spectral learning for multilabel classification,” in Proc. 14th ACM SIGKDD Int. Conf. Knowl.
Discovery Data Mining, 2008, pp. 668–676.
[32] L. Sun, S. Ji, and J. Ye, “Canonical correlation analysis for multilabel classification: A least-squares formulation, extensions, and
analysis,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 1,
pp. 194–200, Jan. 2011.
[33] G. Trigeorgis, K. Bousmalis, S. Zafeiriou, and B. W. Schuller, “A
deep semi-NMF model for learning hidden representations,” in
Proc. 31st Int. Conf. Mach. Learning, 2014, pp. 1692–1700.
[34] G. Tsoumakas and I. Katakis, “Multi-label classification: An overview,” Int. J. Data Warehousing Mining, vol. 3, no. 3, pp. 1–13, 2007.
[35] D. Wang, T. Li, and C. H. Q. Ding, “Weighted feature subset non-negative matrix factorization and its applications to document understanding,” in Proc. IEEE 10th Int. Conf. Data Mining, 2010, pp. 541–550.
[36] M. White, Y. Yu, X. Zhang, and D. Schuurmans, “Convex multiview subspace learning,” in Proc. Adv. Neural Inf. Process. Syst.,
2012, pp. 1682–1690.
[37] H. Yang and J. He, “Learning with dual heterogeneity: A nonparametric bayes model,” in Proc. 20th ACM SIGKDD Int. Conf. Knowl.
Discovery Data Mining, 2014, pp. 582–590.
[38] P. Yang and J. He, “Model multiple heterogeneity via hierarchical
multi-latent space learning,” in Proc. 21th ACM SIGKDD Int. Conf.
Knowl. Discovery Data Mining, 2015, pp. 1375–1384.
[39] P. Yang, J. He, and J.-Y. Pan, “Learning complex rare categories
with dual heterogeneity,” in Proc. SIAM Int. Conf. Data Mining,
2015, pp. 523–531.
[40] P. Yang, J. He, H. Yang, and H. Fu, “Learning from label and feature heterogeneity,” in Proc. 14th IEEE Int. Conf. Data Mining,
2014, pp. 1079–1084.
[41] H.-F. Yu, P. Jain, P. Kar, and I. S. Dhillon, “Large-scale multi-label
learning with missing labels,” in Proc. 31st Int. Conf. Mach. Learning, 2014, pp. 593–601.
[42] D. Zhang, J. He, and R. D. Lawrence, “MI2LS: Multi-instance learning from multiple information sources,” in Proc. 19th ACM SIGKDD
Int. Conf. Knowl. Discovery Data Mining, 2013, pp. 149–157.
[43] J. Zhang and J. Huan, “Inductive multi-task learning with multiple view data,” in Proc. 18th ACM SIGKDD Int. Conf. Knowl.
Discovery Data Mining, 2012, pp. 543–551.
[44] M.-L. Zhang and K. Zhang, “Multi-label learning by exploiting
label dependency,” in Proc. 16th ACM SIGKDD Int. Conf. Knowl.
Discovery Data Mining, 2010, pp. 999–1008.
[45] M.-L. Zhang and Z.-H. Zhou, “ML-KNN: A lazy learning
approach to multi-label learning,” Pattern Recognit., 2007,
pp. 2038–2048.
[46] M.-L. Zhang and Z.-H. Zhou, “A review on multi-label learning
algorithms,” IEEE Trans. Knowl. Data Eng., vol. 26, no. 8, pp. 1819–
1837, Aug. 2014.
[47] J. Zhou, J. Chen, and J. Ye, “Clustered multi-task learning via
alternating structure optimization,” in Proc. Adv. Neural Inf.
Process. Syst., 2011, pp. 702–710.

3168

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,

Pei Yang is a postdoctoral researcher in the Statistical Learning Lab (STAR Lab), Arizona State
University. His research focuses on statistical
machine learning and data mining such as heterogeneous learning, semi-supervised learning,
transfer learning, rare category analysis, and
functional data analysis, with applications in web
mining, big data analytics, bioinformatics, healthcare, etc. He has published more than 20 research
articles on referred journals and conference proceedings. He is a member of the IEEE.
Hasan Davulcu received the PhD degree in computer science from State University of New York,
Stony Brook, New York. He is an associate professor in the School of Computing, Informatics
and Decision Systems Engineering, Arizona
State University. He has done research in data
mining and information assurance. His previous
works in data and services integration were published at prestigious ACM and the IEEE conferences. He is currently the PI for an US National
Science Foundation partnership for Innovation:
Building Innovation Capacity (PFI:BIC) grant focusing on financial fraud
detection via visual analytics. He is a member of the IEEE.

VOL. 28,

NO. 12,

DECEMBER 2016

Yada Zhu is a research staff member with the IBM
T. J. Watson Research Center. Her research interests include big data analytics, survival analysis,
statistical data mining and machine learning with
applications to ecommerce, advanced manufacturing, and energy and utilities. She has published
more than 30 research articles on referred journals,
books, and conference proceedings. Her work has
been acknowledged by IBM innovation awards and
IBM research accomplishment awards. She has
served as an associated editor of the International
Journal QTQM. She is a member of the IEEE.
Jingrui He received the PhD degree in computer
science from Carnegie Mellon University. She is
an assistant professor in the School of Computing, Informatics and Decision Systems Engineering, Arizona State University (ASU). She joined
ASU in 2014 and directs the Statistical Learning
Lab (STAR Lab). Her research focuses on heterogeneous machine learning, rare category
analysis, semi-supervised learning, and active
learning, with applications in healthcare, social
network analysis, semiconductor manufacturing,
etc. She received the NSF CAREER Award in 2016, IBM Faculty Award
in 2015 and 2014, respectively, and has published more than 60 refereed
articles. She has served on the organizing committee/senior program
committee of many conferences, including ICML, KDD, IJCAI, SDM,
ICDM, etc. She is also the author of the book Analysis of Rare Categories (Springer-Verlag, 2012). She is a member of the IEEE.
" For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

Rank-One Matrix Pursuit for Matrix Completion

Zheng Wang∗
ZHENGWANG @ ASU . EDU
Ming-Jun Lai†
MJLAI @ MATH . UGA . EDU
Zhaosong Lu‡
ZHAOSONG @ SFU . CA
Wei Fan§
DAVID . FANWEI @ HUAWEI . COM
Hasan Davulcu¶
HASANDAVULCU @ ASU . EDU
Jieping Ye∗¶
JIEPING . YE @ ASU . EDU
∗
The Biodesign Institue, Arizona State University, Tempe, AZ 85287, USA
†
Department of Mathematics, University of Georgia, Athens, GA 30602, USA
‡
Department of Mathematics, Simon Fraser University, Burnaby, BC, V5A 156, Canada
§
Huawei Noah’s Ark Lab, Hong Kong Science Park, Shatin, Hong Kong
¶
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85281, USA

Abstract
Low rank matrix completion has been applied
successfully in a wide range of machine learning applications, such as collaborative filtering,
image inpainting and Microarray data imputation. However, many existing algorithms are not
scalable to large-scale problems, as they involve
computing singular value decomposition. In this
paper, we present an efficient and scalable algorithm for matrix completion. The key idea is to
extend the well-known orthogonal matching pursuit from the vector case to the matrix case. In
each iteration, we pursue a rank-one matrix basis generated by the top singular vector pair of
the current approximation residual and update
the weights for all rank-one matrices obtained
up to the current iteration. We further propose
a novel weight updating rule to reduce the time
and storage complexity, making the proposed algorithm scalable to large matrices. We establish
the linear convergence of the proposed algorithm.
The fast convergence is achieved due to the proposed construction of matrix bases and the estimation of the weights. We empirically evaluate the proposed algorithm on many real-world
large-scale datasets. Results show that our algorithm is much more efficient than state-of-theart matrix completion algorithms while achieving
similar or better prediction performance.
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

1. Introduction
Low rank matrix learning has attracted significant attention
in the machine learning community due to its wide range
of applications, such as collaborative filtering (Koren et al.,
2009; Srebro et al., 2005), compressed sensing (Candès
& Recht, 2009), multi-class learning and multi-task learning (Argyriou et al., 2008; Negahban & Wainwright, 2010;
Dudı́k et al., 2012). In this paper, we consider the general
form of low rank matrix completion: given a partially observed real-valued matrix Y ∈ <n×m , the low rank matrix
completion problem is to find a matrix X ∈ <n×m with
minimum rank such that PΩ (X) = PΩ (Y), where Ω includes the index pairs (i, j) of all observed entries, and PΩ
is the orthogonal projector onto the span of matrices vanishing outside of Ω. As it is intractable to minimize the
matrix rank exactly in the general case, the trace norm or
nuclear norm is widely used as a convex relaxation of the
matrix rank (Candès & Recht, 2009). It is defined by the
Schatten p-norm with p = 1. ForP
matrix X with rank r, its
r
Schatten p-norm is defined by ( i=1 σip )1/p , where {σi }
are the singular values of X. Thus, the trace norm
Prof X is
the `1 norm of the matrix spectrum as ||X||∗ = i=1 |σi |.
Solving the standard low rank or trace norm problem is
computationally expensive for large matrices, as it involves
computing singular value decomposition (SVD). How to
solve these problems efficiently and accurately has attracted much attention in recent years (Avron et al., 2012;
Srebro et al., 2005; Cai et al., 2010; Balzano et al., 2010;
Keshavan & Oh, 2009; Toh & Yun, 2010; Ji & Ye, 2009;
Ma et al., 2011; Mazumder et al., 2010; Mishra et al.,
2011; Wen et al., 2010; Lee & Bresler, 2010; Recht &
Ré, 2013). Most of these methods still involve the computation of SVD or truncated SVD iteratively, which is

Rank-One Matrix Pursuit for Matrix Completion

not scalable to large-scale problems (Cai et al., 2010; Keshavan & Oh, 2009; Toh & Yun, 2010; Ma et al., 2011;
Mazumder et al., 2010; Lee & Bresler, 2010). Several
methods approximate the trace norm using its variational
characterizations (Mishra et al., 2011; Srebro et al., 2005;
Wen et al., 2010; Recht & Ré, 2013), and proceed by alternating optimization. The linear convergence rate is established theoretically for properly designed alternating optimization algorithm under appropriate initialization (Jain
et al., 2013). However, its computational complexity depends on the square of the rank of the estimated matrix.
Thus in practical problems, especially for large matrices, it
requires the rank of the estimated matrix to be very small,
which sacrifices the estimation accuracy.
Recently, the coordinate gradient descent method has been
demonstrated to be efficient in solving sparse learning
problems in the vector case (Friedman et al., 2010; ShalevShwartz & Tewari, 2009). The key idea is to solve a very
simple one-dimensional problem (for one coordinate) in
each iteration. One natural question is whether and how
such method can be applied to solve the matrix completion problem. Some progress has been made recently along
this direction (Jaggi & Sulovský, 2010; Tewari et al., 2011;
Shalev-Shwartz et al., 2011; Dudı́k et al., 2012; Zhang
et al., 2012). These algorithms proceed in two main steps
in each iteration. The first step involves computing the
top singular vector pair, and the second step refines the
weights of the rank-one matrices formed by all top singular vector pairs obtained up to the current iteration. The
main differences among these algorithms lie in how they
refine the weights. The Jaggi’s algorithm (JS) (Jaggi &
Sulovský, 2010) directly applies the Hazan’s algorithm,
which adapts the Frank-Wolfe algorithm to the matrix case
(Hazan, 2008). It updates the weights with a small step size
and does not consider further refinement. It does not use
all information in each step, which leads to a slow convergence rate. Similar to JS, Tewari et al. (Tewari et al., 2011)
use a small update step size for a general structure constrained problem. A more efficient Frank-Wolfe type algorithm is to fully refine the weights, which is claimed to be
equivalent to orthogonal matching pursuit (OMP) in a wide
range of l1 ball constrained convex optimization problems
(Jaggi, 2013). The greedy efficient component optimization (GECO) (Shalev-Shwartz et al., 2011) applies a similar approach, which optimizes the weights by solving another time consuming optimization problem. It empirically
reduces the number of iterations without theoretical guarantees. However, the sophisticated weight refinement leads
to a higher total computational cost. The lifted coordinate
gradient descent algorithm (Lifted) (Dudı́k et al., 2012) updates the rank-one matrix basis with a constant weight in
each iteration, and conducts a lasso type algorithm (Tibshirani, 1994) to fully correct the weights. The weights for

the basis update are difficult to tune: a large value leads to
divergence; a small value makes the algorithm slow (Zhang
et al., 2012). The matrix norm boosting approach (Boost)
(Zhang et al., 2012) learns the update weights and designs
a local refinement step by a non-convex optimization problem which is solved by alternating optimization. It has a
sub-linear convergence rate.
In this paper, we present a simple and efficient algorithm
to solve the low rank matrix completion problem. The key
idea is to extend the orthogonal matching pursuit procedure
(Pati et al., 1993) from the vector case to the matrix case. In
each iteration, a rank-one basis matrix is generated by the
left and right top singular vectors of the current approximation residual. In the standard algorithm, we fully update
the weights for all rank-one matrices in the current basis
set at the end of each iteration by performing an orthogonal projection of the observation matrix onto their spanning
subspace. The most time-consuming step of the proposed
algorithm is to calculate the top singular vector pair of a
sparse matrix, which costs O(|Ω|) operations in each iteration. An appealing feature of the proposed algorithm is that
it has a linear convergence rate. This is quite different from
traditional orthogonal matching pursuit or weak orthogonal
greedy algorithms, whose convergence rate for sparse vector recovery is sub-linear as shown in (Liu & Temlyakov,
2012). See also (Tropp, 2004) for an extensive study on
various greedy algorithms. With this rate of convergence,
we only need O(log(1/)) iterations for achieving an accuracy solution. One drawback of the standard algorithm
is that it needs to store all rank-one matrices in the current
basis set for full weight updating, which contains r|Ω| elements in the r-th iteration. This makes the storage complexity of the algorithm dependent on the number of iterations, which restricts the approximation rank especially
for large matrices. To tackle this problem, we propose an
economic weight updating rule for this algorithm. In this
economic algorithm, we only track two matrices in each iteration. One is the current estimated matrix and the other
one is the pursued rank-one matrix. When restricted to the
observations in Ω, each has |Ω| nonzero elements. Thus
the storage requirement, i.e., 2|Ω|, keeps the same in different iterations, which is the same as the greedy algorithms
(Jaggi & Sulovský, 2010; Tewari et al., 2011). Interestingly, we show that using this economic updating rule we
still retain the linear convergence rate. To the best of our
knowledge, our proposed algorithms are the fastest among
all related methods. We verify the efficiency of our algorithms empirically on large-scale matrix completion problems.
The main contributions of our paper are:
• We propose a computationally efficient and scalable
algorithm for matrix completion, which extends the

Rank-One Matrix Pursuit for Matrix Completion

orthogonal matching pursuit from the vector case to
the matrix case.
• We theoretically prove the linear convergence rate of
our algorithm. As a result, we only need O(log(1/))
steps to obtain an -accuracy solution, and in each step
we only need to compute the top singular vector pair,
which can be computed efficiently.
• We further reduce the storage complexity of our algorithm based on an economic weight updating rule
while retaining the linear convergence rate. This algorithm has constant storage complexity which is independent of the approximation rank and is more practical for large-scale problems.
• Our proposed algorithm is free of tuning parameter,
except for the accuracy of the solution. And it is guaranteed to converge, i.e., no risk of divergence.
Notations: Let Y = (y1 , · · · , ym ) ∈ <n×m be an n × m
real matrix, and Ω ⊂ {1, · · · , n} × {1, · · · , m} denote the
indices of the observed entries of Y. PΩ is the projection
operator onto the space spanned by the matrices vanishing outside of Ω so that the (i, j)-th component of PΩ (Y)
equals to Yi,j for (i, j) ∈ Ω and zero otherwise.
The
qP
2
Frobenius norm of Y is defined as ||Y||F =
i,j Yi,j .
T T
) denote a vector reshaped
Let vec(Y) = (y1T , · · · , ym
from matrix Y by concatenating all its column vectors. Let
ẏ = vec(PΩ (Y)) be the vector by concatenating all observed entries in Y. The inner product of two matrices X
and Y is defined as hX, Yi = hvec(X), vec(Y)i. Given a
matrix A ∈ <n×m , we denote PΩ (A) by AΩ . For any two
n×m
matrices A,
, we definephA, BiΩ = hAΩ , BΩ i,
pB ∈ <
kAkΩ = hA, AiΩ and kAk = hA, Ai.

2. Rank-One Matrix Pursuit
It is well-known that any matrix X ∈ <n×m can be written
as a linear combination of rank-one matrices, that is,
X
X = M(θ) =
θi Mi ,
(1)
i∈I

where {Mi : i ∈ I} is the set of all n × m rank-one
matrices with unit Frobenius norm. Clearly, θ is an infinite
dimensional vector. Such a representation can be obtained
from the standard SVD of X.
The original low rank matrix approximation problem aims
to minimize the zero-norm of θ subject to the constraint:
min ||θ||0
θ

s.t.

PΩ (M(θ)) = PΩ (Y),

(2)

where ||θ||0 denotes the cardinality of the number of
nonzero elements of θ.

If we reformulate the problem as
min ||PΩ (M(θ)) − PΩ (Y)||2F
θ

s.t.

||θ||0 ≤ r,

(3)

we could solve it by an orthogonal matching pursuit type
greedy algorithm using rank-one matrices as the basis. If
the dictionary {Mi : i ∈ I} is known and finite, this is
equivalent to the compressed sensing problem. However,
in our formulation, the size of the dictionary is infinite and
the bases are to be constructed during the basis pursuit process. In particular, we are to find a suitable subset with
over-complete rank-one matrix coordinates, and learn the
weight for each coordinate. This is achieved by executing
two steps alternatively: one is to construct the basis, and
the other one is to learn the weights of the basis.
Suppose that after the (k-1)-th iteration, the rank-one basis matrices M1 , . . . , Mk−1 and their current weight θ k−1
are already computed. In the k-th iteration, we are to
pursue a new rank-one basis matrix Mk with unit Frobenius norm, which is mostly correlated with the current observed regression residual Rk = PΩ (Y) − Xk−1 , where
Pk−1
Xk−1 = (M(θ k−1 ))Ω = i=1 θik−1 (Mi )Ω . Therefore,
Mk can be chosen to be an optimal solution of the following problem:
max{hM, Rk i : rank(M) = 1, kMkF = 1}.
M

(4)

Notice that each rank-one matrix M with unit Frobenius
norm can be written as the product of two unit vectors,
namely, M = uvT for some u ∈ <n and v ∈ <m with
kuk = kvk = 1. We then see that problem (4) can be
equivalently reformulated as
max{uT Rk v : kuk = kvk = 1}.
u,v

(5)

Clearly, the optimal solution (u∗ , v∗ ) of problem (5) is
a pair of top left and right singular vectors of Rk . It
can be efficiently computed by the power method (Jaggi
& Sulovský, 2010; Dudı́k et al., 2012). The new rankone basis matrix Mk is then readily available by setting
Mk = u∗ v∗T .
After finding the new rank-one basis matrix Mk , we update the weights θ k for all currently available basis matrices {M1 , · · · , Mk } by solving the following least squares
regression problem:
min ||

θ∈<k

k
X

θi Mi − Y||2Ω .

(6)

i=1

By reshaping the matrices (Y)Ω and (Mi )Ω into vectors ẏ
and ṁi , we can easily see that the optimal solution θ k of
(6) is given by
θ k = (M̄Tk M̄k )−1 M̄Tk ẏ,

(7)

Rank-One Matrix Pursuit for Matrix Completion

where M̄k = [ṁ1 , · · · , ṁk ] is the matrix formed by all
reshaped basis vectors. The row size of matrix M̄k is the
total number of observed entries. It is computationally expensive to directly calculate the matrix multiplication. An
incremental update rule can be applied to solve this step
efficiently (Wang et al., 2014).
We run the above two steps iteratively until some desired stopping condition is satisfied. We can terminate the
method based on the rank of the estimated matrix or the approximation residual. In particular, one can choose a preferred rank of the approximate solution matrix. Alternatively, one can stop the method once the residual kRk k is
less than a tolerance parameter ε. The main steps of RankOne Matrix Pursuit (R1MP) are given in Algorithm 1.
Remark In our algorithm, we adapt orthogonal matching
pursuit on the observed part of the matrix. This is similar to the GECO algorithm. However, GECO constructs
the estimated matrix by projecting the observation matrix
onto a much larger subspace, which is a product of two
subspaces spanned by all left singular vectors and all right
singular vectors obtained up to the current iteration. So
it has much higher computational complexity. Lee et al.
(Lee & Bresler, 2010) recently propose the ADMiRA algorithm, which is also a greedy approach. In each step it first
chooses 2r components by top-2r truncated SVD and then
uses another top-r truncated SVD to obtain a rank-r matrix.
Thus, the ADMiRA algorithm is computationally more expensive than the proposed algorithm. The main difference
between the proposed algorithm and ADMiRA is somewhat similar to the difference between the OMP (Pati et al.,
1993) for learning sparse vectors and CoSaMP (Needell &
Tropp, 2010). In addition, the performance guarantees (including recovery guarantee and convergence property) of
ADMiRA rely on strong assumptions, i.e., the matrix involved in the loss function satisfies a rank-restricted isometry property, which is not satisfied in matrix completion
(Lee & Bresler, 2010). Lee et al. sketch a similar idea as
the standard verion of our algorithm in Remark 2.3 without
any further analysis, and their theoretical results cannot be
easily extended to our algorithm. Another contribution of
our work is that we further propose an economic version of
the algorithm and analyze its convergence property.

3. Convergence Analysis
In this section, we will show that our proposed rank-one
matrix pursuit algorithm achieves a linear convergence rate.
This main result is given in the following theorem.
Theorem 3.1. The rank-one matrix pursuit algorithm satisfies
||Rk || ≤ γ k−1 kYkΩ , ∀k ≥ 1.
γ is a constant in [0, 1).

Algorithm 1 Rank-One Matrix Pursuit (R1MP)
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed residual matrix Rk =
YΩ − Xk−1 and set Mk = uk (vk )T .
Step 2: Compute the weight θ k using the closed form
least squares solution θ k = (M̄Tk M̄k )−1 M̄Tk ẏ.
Pk
Step 3: Set Xk = i=1 θik (Mi )Ω and k ← k + 1.
until stopping criterion is satisfied
Pk
Output: Constructed matrix Ŷ = i=1 θik Mi .

Before proving Theorem 3.1, we need to establish some
useful and preparatory properties of Algorithm 1. The first
property says that Rk+1 is perpendicular to all previously
generated Mi for i = 1, · · · , k.
Property 3.2. hRk+1 , Mi i = 0 for i = 1, · · · , k.
Proof. Recall that θ k is the optimal solution of problem (6). By the first-order optimality condition, one has
Pk
hY − j=1 θjk Mj , Mi iΩ = 0 for i = 1, · · · , k, which toPk
gether with Rk = YΩ − Xk−1 and Xk = j=1 θjk (Mj )Ω
implies that hRk+1 , Mi i = 0 for i = 1, · · · , k.
The following property shows that as the number of rankone basis matrices Mi increases during our learning process, the residual kRk k does not increase.
Property 3.3. kRk+1 k ≤ kRk k for all k ≥ 1.
Proof. We observe that for all k ≥ 1,
Pk
kRk+1 k2 = min {kY − i=1 θi Mi k2Ω }
θ∈<k
Pk−1
≤ min {kY − i=1 θi Mi k2Ω } = kRk k2 ,
θ∈<k−1

and hence the conclusion holds.
We next establish that {(Mi )Ω }ki=1 is linearly independent
unless kRk k = 0. It follows that formula (7) is welldefined and hence θ k is uniquely defined before the algorithm stops.
Property 3.4. Suppose that Rk 6= 0 for some k ≥ 1. Then,
M̄i has a full column rank for all i ≤ k.
Proof. Using Property 3.3 and the assumption Rk 6= 0
for some k ≥ 1, we see that Ri 6= 0 for all i ≤ k.
We now prove this statement by induction on i. Indeed,
since R1 6= 0, we clearly have M̄1 6= 0. Hence the conclusion holds for i = 1. We now assume that it holds
for i − 1 < k and need to show that it also holds for
i ≤ k. By the induction hypothesis, M̄i−1 has a full column rank. Suppose for contradiction that M̄i does not have
a full column rank. Then, there exists α ∈ <i−1 such that

Rank-One Matrix Pursuit for Matrix Completion

Pi−1
(Mi )Ω = j=1 αj (Mj )Ω , which together with Property
3.2 implies that hRi , Mi i = 0. It follows that σmax (Ri ) =
uTi Ri vi = hRi , Mi i = 0, and hence Ri = 0, which contradicts the fact that Rj 6= 0 for all j ≤ k. Therefore, M̄i
has a full column rank and the conclusion holds.

Substituting M̄k = QU into (10), and using QT Q = I
and (11), we obtain that

We next build a relationship between two consecutive
residuals kRk+1 k and kRk k.

= [0, · · · , 0, hMk , Rk i] U−1 U−T [0, · · · , 0, hMk , Rk i]

For convenience, define θkk−1 = 0 and let θ k = θ k−1 +η k .
In view of (6), one can observe that
η k = arg min ||
η

Let
Lk =

k
X

ηi Mi − Rk ||2Ω .

(8)

i=1
k
X

kLk k2
= ṙTk M̄k (UT U)−1 M̄Tk ṙk
T

= hMk , Rk i2 /(Ukk )2 ≥ hMk , Rk i2 .
The last equality follows since U is upper triangular and
the last inequality is due to |Ukk | ≤ 1.
We are now ready to prove Theorem 3.1.
Proof. Using the definition of Mk , we have

ηik (Mi )Ω .

(9)

hMk , Rk i = huk (vk )T , Rk i = σ∗ (Rk ),

i=1

By the definition of Xk , one can also observe that Xk =
Xk−1 + Lk and Rk+1 = Rk − Lk .
Property 3.5. ||Rk+1 ||2 = ||Rk ||2 −||Lk ||2 and ||Lk ||2 ≥
hMk , Rk i2 , where Lk is defined in (9).
P
Proof. Since Lk = i≤k ηik (Mi )Ω , it follows from Property 3.2 that hRk+1 , Lk i = 0. Thus,
||Rk+1 ||2
= ||Rk − Lk ||2 = ||Rk ||2 − 2hRk , Lk i + ||Lk ||2
= ||Rk ||2 − 2hRk+1 + Lk , Lk i + ||Lk ||2
= ||Rk ||2 − 2hLk , Lk i + ||Lk ||2
= ||Rk ||2 − ||Lk ||2
We next bound kLk k2 from below. If Rk = 0, ||Lk ||2 ≥
hMk , Rk i2 clearly holds. We now suppose throughout
the remaining proof that Rk 6= 0. It then follows from
Property 3.4 that M̄k has a full column rank. Using this
−1 T
fact and (8), we have η k = M̄Tk M̄k
M̄k ṙk , where
ṙk is the
reshaped
residual
vector
of
R
.
Invoking that
k
P
Lk = i≤k ηik (Mi )Ω , we then obtain
||Lk ||2 = ṙTk M̄k (M̄Tk M̄k )−1 M̄Tk ṙk .

(10)

Let M̄k = QU be the QR factorization of M̄k , where
QT Q = I and U is a k × k nonsingular upper triangular
matrix. One can observe that (M̄k )k = ṁk , where (M̄k )k
denotes the k-th column of the matrix M̄k and ṁk is the reshaped vector of (Mk )Ω . Recall that kMk k = kuk vkT k =
1. Hence, k(M̄k )k k ≤ 1. Due to QT Q = I, M̄k = QU
and the definition of U, we have
0 < |Ukk | ≤ kUk k = k(M̄k )k k ≤ 1.
In addition, by Property 3.2, we have
T

M̄Tk ṙk = [0, · · · , 0, hMk , Rk i] .

(11)

where σ∗ (Rk ) is the maximum singular value of the residual matrix Rk . Using this inequality and Property 3.5, we
obtain that
||Rk+1 ||2

= ||Rk ||2 − ||Lk ||2
≤ ||Rk ||2 − hMk , Rk i2


σ∗2 (Rk )
= 1 − kR
||Rk ||2 .
2
k
k

In view of this relation and the fact that kR1 k = kYk2Ω ,
we easily conclude that
s
k−1
Y
σ 2 (Ri )
||Rk || ≤ kYkΩ
1− ∗ 2 .
kRi k
i=1
1
∗ (Ri )
As for each step we have 0 < rank(R
≤ σkR
≤ 1,
i)
ik
there must exist 0 ≤ γ < 1 that satisfies ||Rk || ≤
γ k−1 kYkΩ . This completes the proof.
2

ik
Remark In practice, the value of σkR
that controls the
2
∗ (Ri )
convergence speed is much less than min(m, n). We will
emprically verify this in the experiments.

Remark If Ω is the entire set of all indices of {(i, j), i =
1, · · · , m, j = 1, · · · , n}, our rank-one matrix pursuit algorithm equals to standard SVD using the power method.
Remark This convergence is obtained for the optimization
residual in the low rank matrix completion problem. We
further extend our algorithm to solve the more general matrix sensing problem and analyze the corresponding statistical convergence behavior under mild conditions, such as
the rank-restricted isometry property (Lee & Bresler, 2010;
Jain et al., 2013). Details are provided in the longer version
of this paper (Wang et al., 2014).

Rank-One Matrix Pursuit for Matrix Completion

4. Economic Rank-One Matrix Pursuit

5. Experiments

The proposed R1MP algorithm has to track all pursued
bases and save them in the memory. It demands O(r|Ω|)
storage complexity to obtain a rank-r estimated matrix. For
large-scale problems, such storage requirement is not negligible and restricts the rank of the matrix to be estimated.
To adapt our algorithm to large-scale problems with a large
approximation rank, we simplify the orthogonal projection
step by only tracking the estimated matrix Xk−1 and the
rank-one update matrix Mk . In this case, we only need to
estimate the weights for these two matrices in Step 2 of our
algorithm by solving the following least squares problem:

In this section, we compare our rank-one matrix pursuit algorithms R1MP and ER1MP with state-of-the-art matrix
completion algorithms. The competing algorithms include:
singular value projection (SVP) (Jain et al., 2010), singular value thresholding (SVT) (Candès & Recht, 2009),
Jaggi’s fast algorithm for trace norm constraint (JS) (Jaggi
& Sulovský, 2010), spectral regularization algorithm (SoftImpute) (Mazumder et al., 2010), low rank matrix fitting
(LMaFit) (Wen et al., 2010), alternating minimization (AltMin) (Jain et al., 2013), boosting type accelerated matrixnorm penalized solver (Boost) (Zhang et al., 2012) and
greedy efficient component optimization (GECO) (ShalevShwartz et al., 2011). The general greedy method (Tewari
et al., 2011) is not included in our comparison, as it includes JS and GECO (included in our comparison) as special cases for matrix completion. The lifted coordinate descent method (Lifted) (Dudı́k et al., 2012) is not included in
our comparison, as it is similar to Boost proposed in (Zhang
et al., 2012), but more sensitive to the parameters.

αk = arg

min

α={α1 ,α2 }

||α1 Xk−1 + α2 Mk − Y||2Ω . (12)

This still corrects all weights of the existed bases, though
the correction is sub-optimal. If we write the estimated matrix
of the bases, we have Xk =
Pk as ak linear combination
k
θ
(M
)
with
θ
=
α2k and θik = θik−1 α1k , for
i Ω
k
i=1 i
i < k. The detailed procedure of this simplified method
is given in Algorithm 2.

The code for most of these algorithms is available online:
Algorithm 2 Economic Rank-One Matrix Pursuit (ER1MP)
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed residual matrix Rk =
YΩ − Xk−1 and set Mk = uk (vk )T .
Step 2: Compute the optimal weights αk for Xk−1
and Mk by solving:
arg min ||α1 Xk−1 + α2 (Mk )Ω − YΩ ||2 .
α

Step 3: Set Xk = α1k Xk−1 + α2k (Mk )Ω ; θkk = α2k
and θik = θik−1 α1k for i < k; k ← k + 1.
until stopping criterion is satisfied
Pk
Output: Constructed matrix Ŷ = i=1 θik Mi .
The proposed economic rank-one matrix pursuit algorithm
(ER1MP) uses the same amount of storage as the greedy
algorithms (Jaggi & Sulovský, 2010; Tewari et al., 2011),
which is significantly smaller than that required by R1MP
algorithm. Interestingly, we can show that the ER1MP algorithm still retains the linear convergence rate. The main
result is given in the following theorem, and the proof is
provided in the long version of this paper (Wang et al.,
2014).
Theorem 4.1. The economic rank-one matrix pursuit algorithm satisfies
||Rk || ≤ γ̃ k−1 kYkΩ , ∀k ≥ 1.
γ̃ is a constant in [0, 1).

• SVP:
http://www.cs.utexas.edu/∼pjain/svp/
• SVT:
http://svt.stanford.edu/
• SoftImpute:
http://www-stat.stanford.edu/∼rahulm/software.html
• LMaFit:
http://lmafit.blogs.rice.edu/
• Boost:
http://webdocs.cs.ualberta.ca/∼xinhua2/boosting.zip
• GECO:
http://www.cs.huji.ac.il/∼shais/code/geco.zip
We compare these algorithms in two problems, including
image recovery and collaborative filtering. The data size
for image recovery is relatively small, and the recommendation problem is in large-scale. In the experiments, we follow the recommended settings of the parameters for competing algorithms. If no recommended parameter value
is available, we choose the best one from a candidate set
using cross validation. For our R1MP and ER1MP algorithms, we only need a stopping criterion. For simplicity, we stop our algorithms after r iterations. In this
way, we approximate the ground truth using a rank-r matrix. We present the experimental results using root-meansquare error (RMSE) (Jaggi & Sulovský, 2010; ShalevShwartz et al., 2011). The experiments are implemented
in MATLAB1 . They call some external packages for fast
1
GECO is written in C++ and we call its executable file in
MATLAB.

Rank-One Matrix Pursuit for Matrix Completion
Table 1. Image recovery results measured in terms of the RMSE: the value below is the actual value times 100 (mean ± std).
Image

Lenna
Barbara
Clown
Crowd
Girl
Man

SVT

SVP

SoftImpute

LMaFit

AltMin

JS

R1MP

ER1MP

3.86 ± 0.02
4.48 ± 0.02
3.72 ± 0.03
4.48 ± 0.02
3.36 ± 0.02
4.49 ± 0.03

5.31 ± 0.14
5.60 ± 0.08
10.97 ± 0.17
7.62 ± 0.13
4.45 ± 0.16
5.52 ± 0.10

4.60 ± 0.02
5.22 ± 0.01
4.48 ± 0.03
5.35 ± 0.02
4.10 ± 0.01
5.16 ± 0.03

7.45 ± 0.63
5.16 ± 0.28
4.65 ± 0.67
4.91 ± 0.05
4.12 ± 0.48
5.31 ± 0.13

4.47 ± 0.10
5.05 ± 0.06
5.49 ± 0.46
4.87 ± 0.02
5.07 ± 0.50
5.19 ± 0.11

5.48 ± 0.72
6.52 ± 0.88
7.30 ± 2.32
7.38 ± 1.41
4.42 ± 0.46
6.25 ± 0.54

3.90 ± 0.02
4.63 ± 0.01
3.85 ± 0.03
4.89 ± 0.03
3.09 ± 0.02
4.66 ± 0.03

3.97 ± 0.02
4.73 ± 0.02
3.91 ± 0.03
4.96 ± 0.03
3.12 ± 0.02
4.76 ± 0.03

computation of SVD2 and sparse matrix computations. The
experiments are run in a PC with WIN7 system, Intel 4 core
3.4 GHz CPU and 8G RAM.
5.1. Image Recovery
In the image recovery experiments, we use the following
benchmark test images: Lenna, Barbara, Clown, Crowd,
Girl, Man3 . The size of each image is 512 × 512. For each
experiment, we present the average RMSE and the corresponding standard derivation of 10 different runs for each
competing algorithm. In each run, we randomly exclude
50% of the pixels in the image, and the remaining ones
are used as the observations. As the image matrix is not
guaranteed to be low rank, we use the rank 200 for the estimation matrix for each experiment. The JS algorithm does
not explicitly control the rank, thus we fix its number of
iterations to 2000. The numerical results are listed in Table 1. The results show that SVT, our R1MP and ER1MP
achieve the best numerical performance. However, our algorithm is much faster and more stable than SVT. For each
image, ER1MP uses around 3.5 seconds, but SVT consumes around 400 seconds. Image recovery needs a relatively higher approximation rank; GECO and Boost fail
to find a good recovery in some cases, so we do not include
them in the table.

Lens datasets were collected from the MovieLens website4 .
They contain anonymous ratings of the movies on this
web made by its users. For MovieLens100K and MovieLens1M, there are 5 rating scores (1–5), and for MovieLens10M there are 10 levels of scores with a step size 0.5
in the range of 0.5 to 5. In the following experiments, we
randomly split the ratings into training and test sets. Each
set contains 50% of the ratings. We compare the prediction results from different methods. In the experiments, we
use 100 iterations for the JS algorithm, and for other algorithms we use the same rank for the estimated matrices; the
values of the rank are {10, 10, 5, 10, 10, 20} for the six corresponding datasets. The results in terms of the RMSE is
given in Table 3. We also show the running time of different
methods in Table 4. We can observe from the above experiments that our ER1MP algorithm is the fastest among all
competing methods to obtain satisfactory results.
Table 2. Characteristics of the recommendation datasets.
Dataset
Jester1
Jester2
Jester3
MovieLens100k
MovieLens1M
MovieLens10M

# row
24983
23500
24938
943
6040
69878

# column
100
100
100
1682
3706
10677

# rating
106
106
6×105
105
106
107

5.2. Recommendation
In the following experiments, we compare the different
matrix completion algorithms using large recommendation datasets: Jester (Goldberg et al., 2001) and MovieLens (Miller et al., 2003). We use six datasets including:
Jester1, Jester2, Jester3, MovieLens100K, MovieLens1M,
and MovieLens10M. The statistics of these datasets are
given in Table 2. The Jester datasets were collected from
a joke recommendation system. They contain anonymous
ratings of 100 jokes from the users. The ratings are
real values ranging from −10.00 to +10.00. The Movie2

PROPACK is used in SVP, SVT, SoftImpute and Boost. It is
an efficient SVD package, which can be downloaded from http:
//soi.stanford.edu/˜rmunk/PROPACK/
3
Images are downloaded from http://www.utdallas.
edu/˜cxc123730/mh_bcs_spl.html

5.3. Convergence and Efficiency
We present the residual curves on the Lenna image in logarithmic scale for our R1MP and ER1MP algorithms in Figure 1. The results show that our algorithms reduce the approximation error in a linear rate. This is consistent with
our theoretical analysis. The empirical results verify the
linear convergence property of our proposed algorithms.

6. Conclusion
In this paper, we propose an efficient and scalable low rank
matrix completion algorithm. The key idea is to extend orthogonal matching pursuit method from the vector case to
the matrix case. We also propose a novel weight updating
4

http://movielens.umn.edu

Rank-One Matrix Pursuit for Matrix Completion
Table 3. Recommendation results measured in terms of the RMSE. Boost fails on the MovieLens10M.
Dataset
Jester1
Jester2
Jester3
MovieLens100K
MovieLens1M
MovieLens10M

SVP
4.7311
4.7608
8.6958
0.9683
0.9085
0.8611

SoftImpute
5.1113
5.1646
5.4348
1.0354
0.8989
0.8534

LMaFit
4.7623
4.7500
9.4275
1.2308
0.9232
0.8625

AltMin
4.8572
4.8616
9.7482
1.0042
0.9382
0.9007

Boost
5.1746
5.2319
5.3982
1.1244
1.0850
–

JS
4.4713
4.5102
4.6866
1.0146
1.0439
0.8728

GECO
4.3680
4.3967
5.1790
1.0243
0.9290
0.8668

R1MP
4.3418
4.3649
4.9783
1.0168
0.9595
0.8621

ER1MP
4.3384
4.3546
5.0145
1.0261
0.9462
0.8692

Table 4. The running time (measured in seconds) of all methods on all recommendation datasets.
Dataset
Jester1
Jester2
Jester3
MovieLens100K
MovieLens1M
MovieLens10M

SVP
18.35
16.85
16.58
1.32
18.90
> 103

SoftImpute
161.49
152.96
10.55
128.07
59.56
> 103

Lenna
−1

RMSE

10

−2

10

AltMin
11.14
10.47
12.23
3.23
68.77
310.82

Boost
93.91
261.70
245.79
2.87
93.91
–

JS
29.68
28.52
12.94
2.86
13.10
130.13

GECO
> 104
> 104
> 103
10.83
> 104
> 105

R1MP
1.83
1.68
0.93
0.04
0.87
23.05

ER1MP
0.99
0.91
0.34
0.04
0.54
13.79

7. Acknowledgments

Lenna

−1

10

RMSE

LMaFit
3.68
2.42
8.45
2.76
30.55
154.38

This work was supported in part by China 973 Fundamental
R&D Program (No.2014CB340304), NIH (LM010730),
and NSF (IIS-0953662, CCF-1025177).

−2

10

References
−3

10

0

−3

50

100

150

rank

200

250

300

10

0

50

100

150

200

250

300

rank

Figure 1. Illustration of the linear convergence of the proposed
rank-one matrix pursuit algorithms on the Lenna image: the xaxis is the iteration, and the y-axis is the RMSE in logarithmic
scale. The curves are the results for R1MP and ER1MP respectively.

rule under this framework to reduce the storage complexity
and make it independent of the approximation rank. Our algorithms are computationally inexpensive for each matrix
pursuit iteration, and find satisfactory results in a few iterations. Another advantage of our proposed algorithms is
they have only one tunable parameter, which is the rank. It
is easy to understand and to use by the user. This becomes
especially important in large-scale learning problems. In
addition, we rigorously show that both algorithms achieve
a linear convergence rate, which is significantly better than
the previous known results (a sub-linear convergence rate).
We also empirically compare the proposed algorithms with
state-of-the-art matrix completion algorithms, and our results show that the proposed algorithms are more efficient
than competing algorithms while achieving similar or better prediction performance. We plan to generalize our theoretical and empirical analysis to other loss functions in the
future.

Argyriou, A., Evgeniou, T., and Pontil, M. Convex multitask feature learning. Machine Learning, 73(3):243–
272, 2008.
Avron, H., Kale, S., Kasiviswanathan, S., and Sindhwani,
V. Efficient and practical stochastic subgradient descent
for nuclear norm regularization. In ICML, 2012.
Balzano, L., Nowak, R., and Recht, B. Online identification and tracking of subspaces from highly incomplete
information. In Allerton, 2010.
Cai, J., Candès, E. J., and Shen, Z. A singular value thresholding algorithm for matrix completion. SIAM Journal
on Optimization, 20(4):1956–1982, 2010.
Candès, E. J. and Recht, B. Exact matrix completion
via convex optimization. Foundations of Computational
Mathematics, 9(6):717–772, 2009.
Dudı́k, M., Harchaoui, Z., and Malick, J. Lifted coordinate
descent for learning with trace-norm regularization. In
AISTATS, 2012.
Friedman, J. H., Hastie, T., and Tibshirani, R. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22,
2010.

Rank-One Matrix Pursuit for Matrix Completion

Goldberg, K., Roeder, T., Gupta, D., and Perkins, C. Eigentaste: A constant time collaborative filtering algorithm.
Information Retrieval, 4(2):133–151, 2001.

Negahban, S. and Wainwright, M.J. Estimation of (near)
low-rank matrices with noise and high-dimensional scaling. In ICML, 2010.

Hazan, E. Sparse approximate solutions to semidefinite
programs. In LATIN, 2008.

Pati, Y. C., Rezaiifar, R., Rezaiifar, Y. C. Pati R., and Krishnaprasad, P. S. Orthogonal matching pursuit: Recursive
function approximation with applications to wavelet decomposition. In Asilomar SSC, 1993.

Jaggi, M. Revisiting Frank-Wolfe: Projection-free sparse
convex optimization. In ICML, 2013.
Jaggi, M. and Sulovský, M. A simple algorithm for nuclear
norm regularized problems. In ICML, 2010.
Jain, P., Meka, R., and Dhillon, I. S. Guaranteed rank minimization via singular value projection. In NIPS, 2010.
Jain, P., Netrapalli, P., and Sanghavi, S. Low-rank matrix
completion using alternating minimization. In STOC,
2013.
Ji, S. and Ye, J. An accelerated gradient method for trace
norm minimization. In ICML, 2009.
Keshavan, R. and Oh, S. Optspace: A gradient descent
algorithm on grassmann manifold for matrix completion.
http://arxiv.org/abs/0910.5260, 2009.
Koren, Y., Bell, R., and Volinsky, C. Matrix factorization
techniques for recommender systems. Computer, 2009.
Lee, K. and Bresler, Y. Admira: atomic decomposition for
minimum rank approximation. IEEE Transactions on
Information Theory, 56(9):4402–4416, 2010.
Liu, E. and Temlyakov, T. N. The orthogonal super
greedy algorithm and applications in compressed sensing. IEEE Transactions on Information Theory, 58:
2040–2047, 2012.
Ma, S., Goldfarb, D., and Chen, L. Fixed point and
bregman iterative methods for matrix rank minimization.
Mathematical Programming, 128(1-2):321–353, 2011.
Mazumder, R., Hastie, T., and Tibshirani, R. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 99:2287–
2322, August 2010.
Miller, B. N., Albert, I., Lam, S. K., Konstan, J. A., and
Riedl, J. Movielens unplugged: experiences with an
occasionally connected recommender system. In IUI,
2003.
Mishra, B., Meyer, G., Bach, F., and Sepulchre,
R. Low-rank optimization with trace norm penalty.
http://arxiv.org/abs/1112.2318, 2011.
Needell, D. and Tropp, J. A. Cosamp: iterative signal recovery from incomplete and inaccurate samples. Communications of the ACM, 53(12):93–100, 2010.

Recht, B. and Ré, C. Parallel stochastic gradient algorithms
for large-scale matrix completion. Mathematical Programming Computation, 5(2):201–226, 2013.
Shalev-Shwartz, S. and Tewari, A. Stochastic methods for
`1 regularized loss minimization. In ICML, 2009.
Shalev-Shwartz, S., Gonen, A., and Shamir, O. Largescale convex minimization with a low-rank constraint.
In ICML, 2011.
Srebro, N., Rennie, J., and Jaakkola, T. Maximum margin
matrix factorizations. In NIPS, 2005.
Tewari, A., Ravikumar, P., and Dhillon, I. S. Greedy algorithms for structurally constrained high dimensional
problems. In NIPS, 2011.
Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society, Series B,
58:267–288, 1994.
Toh, K. and Yun, S. An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems. Pacific Journal of Optimization, 6:615 – 640,
2010.
Tropp, J. A. Greed is good: algorithmic results for sparse
approximation. IEEE Transactions on Information Theory, 50:2231–2242, 2004.
Wang, Z., Lai, M., Lu, Z., Fan, W., Davulcu, H., and Ye, J.
Orthogonal rank-one matrix pursuit for low rank matrix
completion. http://arxiv.org/abs/1404.1377, 2014.
Wen, Z., Yin, W., and Zhang, Y. Low-rank factorization
model for matrix completion by a non-linear successive
over-relaxation algorithm. Rice CAAM Tech Report 1007, University of Rice, 2010.
Zhang, X., Yu, Y., and Schuurmans, D. Accelerated
training for matrix-norm regularization: A boosting approach. In NIPS, 2012.

Tracking Terrorism News Threads by Extracting
Event Signatures
Syed Toufeeq Ahmed, Ruchi Bhindwale, and Hasan Davulcu
School of Computing and Informatics
Arizona State University
Tempe, Arizona
{toufeeq, rbhindwa, hdavulcu}@asu.edu
it belongs to known topic. 2) Story Segmentation – segment a
news stream into topically cohesive stories, this task mainly
applies to audio or TV news (e.g., transcribed speech), since
web news articles/feeds are supplied in segmented form. 3)
Topic Tracking – track events of interest based on sample news
stories, and associate incoming news stories with the related
stories, which were already discussed before.

Abstract- With the humongous amount of news stories published
daily and the range of ways (RSS feeds, blogs etc) to disseminate
them, even an expert at tracking new developing stories can feel
the information overload. At most times, when a user is reading a
news story, she would like to know “what happened before this?”
or “how things progressed after this incident?”. In this paper, we
present a novel real-time yet simple method to detect and track
new events related to violence and terrorism in news streams
through their life over a time line. We do this by first extracting
signature of the event, at microscopic level rather than topic or
macroscopic level, and then tracking and linking this event with
mentions of same event signature in other incoming news articles.
There by forming a thread that links all the news articles that
describe this specific event, with no training data used or
machine learning algorithms employed. We also present our
experimental evaluations conducted with Document Understand
Conference (DUC) datasets that validate our observations and
methodology.

II. RELATED WORK
One of main problem tackled in text data mining research is
to extract meaningful structure from document streams that
arrive continuously over time, [6] presents an approach to
model “burst of activity” in a such document stream. Seminal
work [7, 8] in event detection and tracking explored both
retrospective detection and online detection approaches and
clustering algorithms like agglomerative clustering, augmented
Group Average Clustering [8] were used. Well-known idfweighted cosine coefficient metric method [11] was also used
to detect and track, with good results in tracking but not
encouraging results in detection task.

Keywords- Named Entity Recognition; Event Detection; News
Threads Extraction; First Story Detection;

I.
INTRODUCTION
The ever-growing amount of electronically disseminated
news stories available for daily peruse is almost overwhelming.
To pay detailed human attention to every developing story and
to track these stories over a time period is an insurmountable
task. Imagine an intelligence expert who needs answers for the
question “What happened during Greece Student Riots and
how the story unfolded?” she has to search news paper
websites and feeds to find the articles she could read about this
story and arrange, organize and link these stories mentally in
the fashion they would have unfolded, in order to understand
what happened and to get the bigger picture.

Retrospective detection is defined as the discovery of
previously unidentified events in historical news corpus [10].
In their work, [10] used both contents and time information of
news articles. Online (real-time) detection strives to identity
the onset of new events from live news feeds in real-time [8].
A real-time news event extraction system [9] extracts violence
and disaster events by processing the news article using
extraction grammars on each document in the cluster.

As defined in Topic Detection and Tracking (TDT) domain,
we present some definitions [1] and also define Event
Signature (ES). A “story” is a news article delivering some
information to the users. A “topic” is a set of news stories
strongly connected by a seminal event, whereas an “event” is
something that happens at some specific time and place [2].
For example “Chinese airplane crash in Korea in April 2002” is
an event. Topic is more general, for example news stories about
“Mars Probe Phoenix” or “Hurricane Katrina”. TDT research is
focused on three main tracks [3]: 1) First Story Detection
(FSD) - identify if a news article is talking about a new story or

We like to capture all the pertinent information that
describes an event (like people involved, organizations
mentioned, locations, dates, event describing phrases etc.) We
extract this information from the text of the news articles and
also try to capture the uniqueness about the event (example
shown in figure 1). Figure 2 shows an example of an Event
Thread, a chain/thread of news articles about a particular event
sorted based on the date published. We remove duplicate
articles which have exact same text, as it is a common
occurrence to get the story from a news wire service (like
Reuters, or AFP).

978-1-4244-4173-0/09/$25.00 ©2009 IEEE

III.

EXTRACTING E VENT SIGNATURES TO TRACK E VENT
THREADS
A. Event Signature

182

ISI 2009, June 8-11, 2009, Richardson, TX, USA

Figure 1. Extracting an Event Signature from a news article. Example article
shown is describing “Daniel Pearl’s Kidnapping and Beheading”. (Source:
CNN.com)
Figure 3. Extracting Event Threads from RSS news feeds.

event or incident in few words (like “Daniel Pearl’s
Kidnapping”, “9/11 attacks” or “Greece Student Riots”). We
first extract candidate phrases from the news articles by
dropping stop words and collecting remaining words as
phrases. Next step is to filter these phrases through heuristics
rules that keep only those phrases that match these rules (e.g.
“… [Named-Entity]…..<Violence-Type word>….”). Then, we
select the defining phrase that best matches the whole article
using vector cosine similarity measure.

Figure 2. An Event Thread unfolding over a timeline. Example articles
shown are about “July 7 th bombing in London”. (Source:
CNN.com/BBC.co.uk)

E. Thread Signature
In order to speed up event tracking, instead of comparing
the new incoming article with every article previously seen,
we maintain thread signatures for every thread and efficiently
compare thread signatures to new article event signature.

B. Extracting Named Entities
The field of Named Entity Recognition (NER) has matured
considerably over last few years and systems based on
Conditional Random Fields [4] have shown very good results.
We used Stanford NER1 system for labeling three entity class
types: PERSON, ORGANIZATION, and LOCATION. To
recognize DATE mentions in the text, we used regular
expressions to recognize different date formats and also
calendar months, days and years.

Input: (RSS) Stream of News Articles
Output: Event Threads
1. For each input article, extract its Event Signature (ESk)
2. Compare ESk similarity with current list of Thread
Signatures (TSk ).
3. Add the article to the thread with highest Similarity score,
Jaccard-Sim (ESk ,TSk ) > ζ, where ζ is min. first story
threshold. Recalculate TSk = ESk φ TSk
4. If article’s JaccardSim(ESk ,TSk ) ≤ ζ, then create a new
thread, and assign TSk = ESk

C. Extracting Violence Type Words (Action Words)
Recognizing the action words that describe a violent
activity (like bombing, kidnapping, blast, shot, killed, burnt) in
the text, is first step to classify whether the document (or
paragraph) may be describing a violent event that happened at
some location at a certain time. We recognize violence type
words using a hash table built as ontology of these words,
which we extracted using an initial (around 240) seed root
words describing violence (e.g., kill, shot, burn, bomb) and
recursively extracting synonyms from WordNet 2 using
synonym sets (synsets). To match words, say “burnt” and
“burned” as same words in the given context, we stem these
words using Porter Stemmer [5] before matching process.

Figure 4. Algorithm to compute Event Signature similarity with exisiting
threads.

F. Online Extraction of Event Threads
Algorithm (in figure 4) describes Event Signature similarity
computation with Thread Signatures. Figure 3 shows the thread
extraction process. Event Threads are extracted directly from
live RSS feeds.

D. Extracting Event Defining Phrases
To fully capture the signature of the event, we also need to
extract phrases that usually end up defining or describing the
1
2

IV. EVALUATION AND EXPERIMENTS
An evaluation of event thread extraction performance was
carried on Document Understanding Conference (DUC) 3

http://nlp.stanford.edu/software/CRF-NER.shtml
http://wordnet.princeton.edu/

3

183

http://duc.nist.gov/

datasets, from year 2004 to 2006. Datasets (DUC2004,
DUC2005 and DUC2006) each consists of 50 folders (clusters)
of news articles from Associated Press and New York Times.
Each cluster contains 25 news articles about a particular topic
or event. For our experiments we take each cluster as a thread
about that event or topic. We observed that in most articles, the
primary event talked about is mentioned in the title and first
two paragraphs. If we leverage this observation, we can save
valuable real-time while doing expensive NER and also reduce
noise in the signature extracted. To validate this, we conducted
experiments on both long (full) articles and short (title and first
two paragraphs) articles. To study the effects of length of the
thread (number of articles in the thread), we conducted
experiments for thread length of 6 and 10 articles in each
cluster respectively. And to study the effects of increasing
number of threads, we conducted experiments with datasets
with 10 threads and 25 thread collections. So, a dataset (see
Figure 5 and 6) labeled “25-10-S” means, 25 total threads,
with 10 short articles in each thread cluster. For each dataset,
we collected all the articles from all clusters into one cluster
and sorted them randomly, and tried to extract threads back
from this collection. We evaluated event thread extraction
performance by these criteria.

Figure 7. F-measure for different datasets as mentioned in figure 5 and 6.

V. CONCLUSIONS
In this paper, we presented a novel real-time yet simple
method to detect and track new events related to violence and
terrorism in news streams with no training data used or
machine learning algorithms employed. We see that event
thread extraction using only title and first two paragraphs of the
article performed better than full articles. In future work, we
need to handle splitting threads into multiple threads as the
stories diverge into multiple stories.

Number of threads extracted: (How many threads were
extracted from initial dataset of 25 threads?). Figure 5 and
figure 6 show results for 10 thread and 25 thread datasets
respectively. We got almost comparable results for both short
and long articles, short performing little better. We got better
results for DUC2006 dataset as it has cleaner clusters than
other two datasets.

REFERENCES
[1]

Nallapati, R., Feng, A., Peng, F., and Allan, J. 2004. Event threading
within news topics. In Proceedings of the Thirteenth ACM international
Conference on information and Knowledge Management, CIKM '04.
ACM, New York, NY, 446-453.
[2] Yang, Y., Carbonell, J. G., Brown, R. D., Pierce, T., Archibald, B. T.,
and Liu, X. 1999. Learning Approaches for Detecting and Tracking
News Events. IEEE Intelligent Systems 14, 4 (Jul. 1999), 32-43.
[3] Chung, S., Jun, J., and McLeod, D. Incremental Mining from News
Streams. Encyclopedia of Data Warehousing and Mining, Idea Group
Inc. 2004.
[4] J. R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Nonlocal Information into Information Extraction Systems by Gibbs
Sampling. Proceedings of the 43nd Annual Meeting of the Association
for Computational Linguistics (ACL 2005), pp. 363-370.
[5] Porter, M. F. (1997). "An algorithm for suffix stripping." Progam, vol.
14, no. 3, July 1980: 313--316.
[6] Kleinberg, J. 2002. Bursty and hierarchical structure in streams. In
Proceedings of the Eighth ACM SIGKDD international Conference on
Knowledge Discovery and Data Mining, KDD '02. ACM, New York,
NY, 91-101.
[7] Allan, J., Papka, R., and Lavrenko, V. 1998. On-line new event detection
and tracking. In Proceedings of the 21st Annual international ACM
SIGIR Conference on Research and Development in information
Retrieval , SIGIR '98. ACM, New York, NY, 37-45.
[8] Yang, Y., Pierce, T., and Carbonell, J. 1998. A study of retrospective
and on-line event detection. In Proceedings of the 21st Annual
international ACM SIGIR Conference, SIGIR '98. ACM, New York,
NY, 28-36.
[9] Tanev, H., Piskorski, J., and Atkinson, M. 2008. Real-Time News Event
Extraction for Global Crisis Monitoring. Lecture Notes In Computer
Science, vol. 5039. Springer-Verlag, Berlin, Heidelberg, 207-218.
[10] Li, Z., Wang, B., Li, M., and Ma, W. 2005. A probabilistic model for
retrospective news event detection. In Proceedings of the 28th Annual
international ACM SIGIR Conference on Research and Development in
information Retrieval, SIGIR '05. ACM, New York, NY, 106-113.
[11] J. Michael Schultz, Mark Liberman, "Topic Detection and Tracking
using idf-weighted Cosine Coefficient," DARPA Broadcast News
Workshop Proceedings, 1999.

F-measure: (The weighted harmonic mean of precision and
recall), F-measure scores for each dataset are shown in figure 7.
We see that short articles out performs long articles by 6.5%
and again DUC2006 performing best in three datasets.

Figure 5. Number of event threads extracted from 10 thread datasets.

Figure 6. Number of event threads extracted from 25 thread datasets.

184

Computational Aspects of Resilient Data Extraction
from Semistructured Sources
(Extended Abstract)
Hasan Davulcu

Guizhen Yang

Michael Kifer

I.V. Ramakrishnan

Department of Computer Science
SUNY at Stony Brook
Stony Brook, NY 11794-4400, USA

fda vulcu,guizyang,kifer,ramg@cs.sunysb.edu
ABSTRACT

1. INTRODUCTION

Automatic data extraction from semistructured sources such
as HTML pages is rapidly growing into a problem of signican t importance, spurred by the gro wing popularity of the so
called "shopbots" that enable end users to compare prices of
goods and other services at various web sites without having
to manually browse and ll out forms at each one of these
sites.
The main problem one has to con tend with when designing data extraction techniques is that the contents of a w eb
page c hanges frequently, either because its data is generated
dynamically, in response to lling out a form, or because of
changes to its presentation format. This makes the problem
of data extraction particularly challenging, since a desirable
requirement of any data extraction technique is that it be
"resilien t", i.e., using it we should always be able to locate
the object of interest in a page (such as a form or an element
in a table generated by a form ll-out) in spite of changes
to the page's content and la yout.
In this paper we propose a formal computation model for developing resilient data extraction techniques from semistructured sources. Specically we formalize the problem of data
extraction as one of generating unambiguous extraction expressions, whic hare regular expressions with some additional structure. The problem of resilience is then formalized
as one of generating a maximal extraction expression of this
kind. We presen t characterization theorems for maximal
extraction expressions, complexity results for testing them,
and algorithms for synthesizing them.

The World Wide Web is becoming the dominant medium for
information delivery and electronic commerce. The number
of users who routinely use the web to buy goods and services con tinuesto increase at a rapid pace. In response,
softw are robots (called \shopbots") that allow consumers to
quic kly nd out the best prices for comparable goods and
services are beginning to emerge. Information about prices
and other attributes of products are typically obtained by
lling out forms at a vendor's site. Soft w are robots retrieve
such information by automatically na vigating to relevan t
sites, locating the correct forms, lling them out and extracting the data of interest from web pages returned as the
result. (Junglee and Jango [17] are tw oexamples of such
shopbots.) Currently almost all web information is stored
as semistructured data, mostly as HTML pages, and shopbots need to retrieve data from such sources. Hence automatic data extraction from semistructured data sources is a
problem of signicant importance especially in the context
of w eb-based electronic commerce.
This problem has attracted a lot of research atten tion recently. The techniques proposed so far, are by and large
centered around creating a wr app er[3, 6, 9, 12, 13, 15, 18,
14, 7, 21, 22], that parses an HTML source and maps it into
a set of structured or semistructured database objects that
can be readily queried and manipulated by applications.
The central diÆculty in designing data extraction techniques
is the v olatile nature of HTML pages, in the sense that they
change very frequently. Changes occur either because they
ha ve to accommodate new services and content oerings or
because they are dynamically generated in response to formbased queries. Such variations can give rise to \brittleness"
in data extractors, i.e. they may no longer be able to locate
the objects of interest in the page (such as a form or a table
element). Th us dev eloping data extraction tec
hniques that
are resilient to changes in the data source structure is both
desirable and important. T o the best of our knowledge this
problem has not yet been fully explored.

 Work supported in part by the NSF grants CCR-9711386
and EIA-9705998.
Permission to make digital or hard copies of part or all of this
work or personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or
commercial advantage and that copies bear this notice and the
full citation on the first page. To copy otherwise, to republish, to
post on servers, or to redistribute to lists, requires prior specific
permission and/or a fee.
POD 2000, Dallas, T X USA
© ACM 2000 1-58113-218-x/00/05 . . .$5.00

In this paper, we make the rst step tow ards developing a
formal framework for creating resilient data extraction wrappers for semistructured sources. As an example, Web pages

136

can be represented as sequences of tokens (HTML tags and
strings), and the extraction problem is usually reduced to
the problem of parsing using regular grammars [9, 12, 13,
21, 18, 22, 14, 15], context-free grammars [6, 3] or specialized languages [7].

a query language [11, 2] is one way to extract data. This
technique is applicable when the structure of the data and
the location of the desired object is known to a large degree
and is not subject to drastic changes.
Wrapper based extraction techniques from semistructured
data include [3, 6, 9, 13, 15, 18, 22, 14, 7, 21]. The works
[14, 15] provide a powerful framework for manual extraction of complex data objects based on the application of
sequences of patterns. An expressive specialized language
for data extraction is proposed in [7]. The language is based
on searches using a subset of regular expressions and other
page restructuring instructions. The works in [3, 9, 13, 20,
22] propose various graphical tools and heuristics for semiautomatic derivation of regular extraction expressions. Such
techniques could be used at the initial learning stage of our
framework (see Sections 3 and 7). However, none of these
works addresses the problems inherent in resilient data extraction.

We propose the notion of extraction expressions, which are
tag-marked regular expressions, as a formalization of the
informal concept of the \target object that can be identied
by its local or global context."
The initial extraction expression is usually derived from a set
of examples (i.e., of HTML pages and target objects) using
one of the known heuristic approaches [13, 21, 22, 9], or one
of the learning algorithms [18, 4, 5], or even manually [14,
15, 12].
We rst introduce the notion of \unambiguity" as a consistency requirement for extraction expressions. Unambiguous extraction expressions must identify the target objects
uniquely within any page. We show that ambiguity of any
given extraction expression is decidable in polynomial time.

The works [18, 21] describe fully automated wrapper generation techniques that use machine learning induction algorithms (similar to [8, 4, 5]). Such techniques are useful
in our framework since they could supply us with initial
extraction expressions that could then be generalized using
our techniques. A limitation of these approaches, however,
is that the extracted data must be representable as a set of
tuples. An ontology-based approach to extracting data is
presented in [12]. This technique requires that suitable ontologies of context keywords, relationships, and regular expressions for lexicons must be constructed in advance (and
manually). In [6], a heuristic algorithm for automatic generation of context-free grammar for data extraction purposes
is presented. However this work does not address the issue
robustness of the wrappers in the presence of variations in
source documents.

If an extraction expression can correctly identify the target
object in many variations of the same document, we shall
call such an expression resilient to changes. Unambiguous
extraction expressions can be ordered according to the languages which they parse. The larger the language | the
more resilient the expression. We thus reduce the problem
of resilience against variations to the task of synthesizing
maximal unambiguous extraction expressions.
We show that some unambiguous expressions can be maximized in several dierent | even innite number (!) of |
ways and the problem of deciding maximality is PSPACEcomplete. We do not know whether every unambiguous extraction expression can be maximized (although we conjecture that this is possible). However, we propose algorithms
for maximizing a large, non-trivial class of extraction expressions that arise frequently in practical situations. Our
preliminary experiments with the maximization algorithms
proposed here indicate that they are suÆcient to provide
resilient extraction capabilities for a web-based information
harvesting system that we have developed recently.

Overall, with the exception of [6], all data extraction techniques we are aware of rely on regular expressions. Hence,
our results enhance these techniques by providing both the
objective criteria for sorting out the good expressions from
the bad ones, and techniques for making the good extraction
expressions more robust.
3. MOTIVATING EXAMPLE

The remainder of the paper is organized as follows. Section 2 surveys the related work. In Section 3, we motivate
the problem of resilient extraction and our approach using
a concrete and realistic example. Section 4 introduces the
main denitions, including the notions of ambiguity and resilience. In Section 5, we resolve the complexity of various
decision problems related to resilient extraction. In Section 6, we present our maximization algorithms and prove
their correctness. Section 7 illustrates how the techniques
developed in this paper apply to the example of Section 3.
Section 8 concludes our paper. Due to space limitation, we
cannot present the proofs here. However, all proofs can be
found in a technical report at
http://www.cs.sunysb.edu/

2.

One of the main problems with data extraction from Web
documents is that the structure of the data might change
due to the dynamic nature of the document or because the
document was redesigned manually and, thus, the location
of the object of interest might be hard to pinpoint. The most
typical changes are insertion or deletion of HTML elements
before or after the object of interest and embedding of the
object inside some other HTML element.
To illustrate, consider the documents in Figures 1 and 2,
which represent the main elements of a typical catalog shopping site. The user can ll out a form to search for a product
by keywords or part number and out come the results. The
top document consists of some header information and a
form. The bottom document is similar, but the form is now
embedded in a table and a new table row, a link to customer
service, is added. If we are building a web robot that goes
around and compares prices, the most likely object of inter-



guizyang/papers/extraction.ps

RELATED WORK

Semi-structured data [10, 1] has recently emerged as an important area of study. Querying semistructured data through

137

<P>
<H1><IMG SRC="supplier.gif">
Virtual Supplier, Inc.
</H1>
<P>
<FORM METHOD="POST" ACTION="search.cgi">
<INPUT TYPE="image" ALIGN=left
SRC="search.gif">
<INPUT TYPE="text" SIZE="15"
NAME="VALUE"> <BR>
<P>
<INPUT TYPE="radio" NAME="ATTR"
VALUE="1" checked>
Keywords<BR>
<INPUT TYPE="radio" NAME="ATTR"
VALUE="2">
Manufacturer Part#
</FORM>

<TABLE>
<TR> <TD><IMG SRC="supplier.gif"></TD>
<TD><H1>Virtual Supplier, Inc.
</H1>
</TD>
</TR>
<TR> <TD><IMG SRC="cust.gif"></TD>
<TD><A HREF="cust.html">
Customer Service
</A><BR></TD>
</TR>
<FORM METHOD="POST" ACTION="search.cgi">
<TR> <TD><INPUT TYPE="image"
SRC="search.gif">
</TD>
<TD><INPUT TYPE="text"
SIZE="15" NAME="VALUE">
</TD>
<TD> <INPUT TYPE="radio" NAME="ATTR"
VALUE="1" checked>
Keywords<BR>
<INPUT TYPE="radio" NAME="ATTR"
VALUE="2">
Manufacturer Part#
</TD>
</TR>
</FORM>
</TABLE>

Virtual Supplier, Inc.

Keywords
Part#

Virtual Supplier, Inc.

Figure 1: Original Page

Customer Service

est would be the form. Suppose now that we train our robot
to nd the needed form using the top document. We want
to make sure that the robot does not fail even if the site administrator reorganizes the document as shown in Figure 2,
if more rows are added to the second document before or
after the form, and (hopefully) if other forms or tables are
added to that document.

Keywords
Part#

Figure 2: Rearranged Page

symbols (e.g., to incorporate tag attributes). Each of the
above strings is an abstract representation of a semistructured Web document, but alone they do not yet specify the
objects of interest. To tell the robot which object we want,
we can enclose it in angle brackets. To indicate that we are
interested in the second INPUT-element of the form, we can
write the following:

To begin, we can try to represent our documents as strings of
objects. For instance, the top document can be represented
as
P H1 /H1 P FORM /FORM

P H1 /H1 P FORM INPUT <INPUT> P INPUT INPUT /FORM

In this representation, we assume that the contents of the
objects H1 and FORM is of no interest. If the insides of, say,
the form are of interest (usually they are), we could expand
the representation as follows:

and
TABLE TR TD /TD TD /TD /TR TR TD /TD TD /TD /TR
FORM TR TD INPUT /TD TD <INPUT> /TD TD INPUT /TD /TR
/FORM /TABLE

P H1 /H1 P FORM INPUT INPUT P INPUT INPUT /FORM

Likewise, the second document in the gure can be represented as:

Since the above expressions are obviously dierent and each
matches only one of our candidate documents, they are still
not very useful to our robot. However, we can try to generalize them, say, using the syntax of regular expressions:

TABLE TR TD /TD TD /TD /TR TR TD /TD TD /TD /TR
FORM TR TD INPUT /TD TD INPUT /TD TD INPUT /TD /TR
/FORM /TABLE

(T ags - FORM)* FORM (T ags - INPUT)* INPUT
(T ags - INPUT)* <INPUT> T ags*

The exact details of this representations is of no signicance
here. It is easy to enrich this model to take the tag attributes
into account, and so on. The point is that documents can
be represented as sequences of tags plus some additional

In the above expression, T ags is a regular expression that
matches any HTML tag, (T ags FORM) matches any tag ex-

138

of the fact that our goal is to nd generalizations that are as
resilient as possible. It turns out that extraction expressions
of the form E1 <p>E2 can be ordered according to how vast
the regular languages L(E1 ) and L(E2 ) are. The bigger they
are | the more resilient the extraction expression is. Ideally,
we would like to nd generalizations that are maximal with
respect to the above order among the unambiguous expressions that match the object of interest on our sample pages.
We show that maximality of any given unambiguous regular expressions is decidable, albeit it is a PSPACE-complete
problem. Surprisingly, the question of whether every unambiguous extraction expression has a maximal generalization
seems to be a hard problem, and it remains open at this
point. However, we shall see that maximal generalizations
do exist for large classes of extraction expressions.

cept FORM), and \*" says that the corresponding expression
can be repeated zero or more times.
Since this regular expression matches both documents and
identies the object of interest correctly, we could give it to
our robot and it will extract the information properly even
if the document changes as shown in Figure 2.
Regular expressions with a marked occurrence of a symbol
will be called extraction expressions in this paper. If an extraction expression can correctly identify the desired object
in many variations of the same document, we shall say that
such an expression is resilient to changes.
The above discussion suggests the following strategy for nding resilient extraction expressions. In the rst stage, a small
number of sample variants of the desired document can be
obtained by lling out the same form in dierent ways. If
the document is static but might change as a result of a
redesign, heuristics can be used to automatically create several perturbations of the document. Next, the variations of
the document in question are represented as strings and the
object of interest is marked in each document (this object
must be of the same kind in each case, since we assumed
that they are perturbations of the same document), which
leaves us with a small number of very rigid extraction expressions. Finally, these expressions are generalized into a
single extraction expression that matches all the instances
of our document.

4. EXTRACTION EXPRESSIONS,
UNAMBIGUITY, AND MAXIMALITY

The previous section provided the intuition behind the notions of extraction expressions, ambiguity, and resilience.
We are now going to dene these concepts formally and
prove several of their important properties.
Denition 4.1. (Extraction Expression) Given a nite alphabet , p 2 , and regular expressions E1 and E2
over , E1 hpiE2 is called an extraction expression over .
In other words, an extraction expression is simply an ordinary regular expression of a special form E1 hpiE2 , with one
marked occurrence, hpi, of an alphabet symbol. 2

Once the problem of data extraction is reduced to the problem of generalizing marked regular expressions, many interesting questions arise. The rst of these is: how far should
(and can) we generalize before the robot gets confused? Indeed, the original extraction expressions were not resilient
because they matched only one document. However, when
they did match, the object of interest was identied correctly
and uniquely. Can it happen that a generalized extraction
expression might match two or more objects and thus confuse our robot (because it will not know which is the true
object of interest)? We do not need to think hard to nd
such a generalization: T ags* <INPUT> T ags*.

The language parsed by E1 hpiE2 is dened as L(E1 hpiE2 )
def
= fj 2 L(E1  p  E2 )g. Given a string  2  , we say
that E1 hpiE2 parses  if  2 L(E1 hpiE2). We also say that
E1 hpiE2 extracts p from  if there exist ;  2  , such that
 =   p   and  2 L(E1 );  2 L(E2 ).
Suppose  is a string. We can use E1 hpiE2 to extract information from  as follows: First we can try to split  into
a prex , followed by p, followed by a suÆx  . If  is
recognized by E1 and  is recognized by E2 , then p is the
extracted object. We try such splits until we either succeed
on some split or fail on all candidates.

This leads to the question of ambiguity of extraction expressions. For instance, the expression ((qp)*|(T ags
p)*)<p>p*, where \|" denotes an alternative (union of regular languages) and (T ags p) matches anything but p, is
unambiguous in the sense that whenever it matches a string,
the marked occurrence of the symbol p falls into a unique
place on the string (even though the prex ((qp)*|(T ags
p)*) can match the prex of a string in more than one way).
In contrast, (qp)*p*<p>p* is an ambiguous expression, because there are many ways to match the marked occurrence
of p on some strings. For instance, consider qppp. The prex (qp)*p* can match qp and qpp, so the marked occurrence
of p can match the second or the third occurrence of p.

As illustrated in Section 3, the expression E1 hpiE2 might
be ambiguous, i.e., there might be a string,
, where two
dierent splits can succeed. For instance, p hpiq parses pppq,
but any one of three p's in pppq can be returned as the
extracted object.
Denition 4.2. (Unambiguous Extraction Expression)
Expression E1 hpiE2 is unambiguous i for all ; 0 2 L(E1 )
and ;  0 2 L(E2 ), if   p   = 0  p   0 then  = 0 and
 = 0. 2

We believe that (un)ambiguity is an important property of
extraction expressions that places limits on how much we
can generalize from a set of examples. We shall see that
ambiguity can be decided in polynomial time.

Example 4.3. (Ambiguous and Unambiguous Extraction Expressions) (pq ) hpi and (pjpp)hpi(pjpp) are
ambiguous extraction expressions, whereas (qp)hpi and
(pjpp)hpi(pjppp) are unambiguous extraction expressions. For

While unambiguity is important, we should not loose sight

139

instance, pqpq can be parsed by (pq)hpi as   p  qpq and
as pq  p  q. Likewise, (pjpp)hpi(pjpp) can parse pppp in two
dierent ways. On the other hand, it can be proved that
the last two expressions always parse their matching strings
uniquely. 2

and

((qp( p))j(( p) q))hpi
(The latter expression is obtained using Algorithm 6.2 in
Section 6, when qphpi is used as input.) In fact, it can be
shown that the above expression has an innite number of
maximal expressions that generalize it. 2

Since, as explained in Section 3, we are mostly interested in
unambiguous extraction expressions, from now on the term
\extraction expression" will refer to unambiguous expressions, unless explicitly specied otherwise.

5. COMPLEXITY OF THE AMBIGUITY
AND MAXIMALITY PROBLEMS
Denition 5.1. (Prex and SuÆx Factoring) Given
a pair of regular expressions E1 and E2 over a nite alphabet , the prex factorization of E1 by E2 is dened
as E2nE1 def
= fj9 2 L(E2 );    2 L(E1 )g. The suÆx factorization of E1 by E2 is E1=E2 def
= fj9 2 L(E2 );    2
L(E1 )g. 2

Denition 4.4. (Partial Order among Extraction Expressions) F1 hpiF2  E1 hpiE2 i L(F1 )  L(E1 ) and
L(F2 )  L(E2 ). We shall also say that E1 hpiE2 generalizes
F1 hpiF2 . 2

Informally, extraction expressions that are \larger" with respect to  are more resilient because they can uniquely
parse larger sets of strings. Moreover if F1 hpiF2  E1 hpiE2
then the two expressions parse the strings in L(F1 hpiF2 ) the
same way. It is easy to see that  is re
exive, antisymmetric and transitive. Therefore,  is a partial order on the set
of all unambiguous extraction expressions.

It is known that E2nE1 and E1=E2 are regular languages, if
both E1 and E2 are regular expressions [19]. Thus factors
can be represented as regular expressions. Since every regular language corresponds to a regular expression and vice
versa, we shall use E and L(E ) interchangeably to denote
the regular language recognized by E .

Note that F1 hpiF2  E1 hpiE2 implies L(F1 hpiF2 ) 
L(E1 hpiE2 ), but not the other way around! Indeed, the
two expressions phpippp and pphpipp parse exactly the same
language, but they extract dierent objects from that language: phpippp extracts the second occurrence of p, while
pphpipp extracts the third.

Lemma 5.2. Given regular expressions E1 and E2 over a
nite alphabet , the regular expressions corresponding to
E2nE1 or E1=E2 can be computed in polynomial time in the
size of E1 and E2 .
Lemma 5.3. E1 hpiE2 is ambiguous i there exist ; ; 
 2
 such that   p  
  p   2 L(E1 hpiE2 ), ;   p  
 2 L(E1 )
and ; 
  p   2 L(E2 ).

Denition 4.5. (Maximal Extraction Expression)
An unambiguous extraction expression E over a nite alphabet  is maximal i for any unambiguous extraction expression E 0 over , if E E 0 then L(E ) = L(E 0 ). 2

Proposition 5.4. (Necessary and SuÆcient Condition for Unambiguity) An extraction expression E1 hpiE2
over a nite alphabet  is unambiguous i
(E1 p)nE1 \ E2=(pE2 ) = 

For the following examples, we use the expression E1 E2 to
represent the regular expression that recognizes the regular
set L(E1 ) L(E2 ).

Theorem 5.5. (Complexity of Testing Ambiguity)
Let E1 hpiE2 be an extraction expression over a nite alphabet . Then testing whether E1 hpiE2 is ambiguous can be
done in time quadratic in the size of E1 hpiE2 .

Example 4.6. (Maximal Extraction Expressions)

Although it might not be immediately obvious, both
( p)hpi and (qp)  (( p) q)hpi are maximal
extraction expressions. 2

Proposition 5.6. (Necessary and SuÆcient Condition for Maximality) An unambiguous extraction expression E1 hpiE2 over a nite alphabet  is maximal i
(E1  p  E2 )=(pE2 ) = 
and

(E1 p)n(E1  p  E2 ) = 

Example 4.7. (Unambiguity and Maximality) Given

a non-maximal unambiguous extraction expression E over
, if there exists a maximal extraction expression E 0 over 
such that E  E 0,0 we say that extraction expression E can
be maximized to E .
It is not known whether every unambiguous extraction expression E can be maximized. However, even when maximization is known to exist then it might not be unique. For
example, qphpi can be maximized to
( p)  p  ( p)hpi

Lemma 5.7. Given a regular expression E over a nite
alphabet , the problem of testing whether L(E ) =  is
PSPACE-complete.

140

Proof: See [16] 2

Left-Filtering Maximization.

Lemma 5.8. For any regular expression E over a nite
alphabet , the extraction expression ( p) hpiE is unambiguous.
Proof: Because ( p) pn( p) = , we know that
n( p) \ E=(pE) = 
( p) p
for any E . Therefore, ( p)hpiE is unambiguous by Propo-

sition 5.4. 2

Proposition 5.9. For any regular expression E over a
nite alphabet , the extraction expression ( p) hpiE is
maximal i L(E ) =  .




 p) p)n(( p)  p   ) = 

((

and
(( p)  p   )=(p ) = ( p) [ (( p)  p   ) = 
it follows from Corollary 5.6 that ( p)hpi is maximal.
Thus, again by Lemma 5.8,
we conclude that ( p) hpiE
is maximal i L(E ) =  . 2
Theorem 5.10. (Complexity of Testing Maximality) For any extraction expression E1 hpiE2 over a nite alphabet , the problem of testing whether E1 hpiE2 is maximal
is PSPACE-complete.

From Lemma 5.7 and Proposition 5.9, we know
that the problem is PSPACE-hard. Testing whether E1 hpiE2
is unambiguous only takes polynomial time according to
Theorem 5.5. Then by Corollary 5.6, to test if E1 hpiE2 is
maximal it suÆces to test whether (E1  p  E2 )=(pE2 ) = 
and (E1 p)n(E1  p  E2 ) =  . According to Lemma 5.2 both
(E1  p  E2 )=(pE2 ) and (E1 p)n(E1  p  E2 ) can be computed in
polynomial time. After applying Lemma 5.7 again, we conclude that the problem of testing maximality is in PSPACE.
2
Proof:

6.

Denition 6.1. (Finite Sequence Filtering Operator)
Given a regular expression E over a nite alphabet , a
symbol p 2 , and an integer n  0, the nite sequence ltering operator E kpn is dened as follows:
E kpn def
= E \ ( p)  p|  ( p) {z  p  ( p)}
n
where the suÆx p  ( p) repeats n times. 2

Proof: First ( p) hpi is unambiguous by Lemma 5.8.

Because

Consider an extraction expression E1 hpiE2 over a nite alphabet . If (E1 p)nE1 = , then by Proposition 5.4 we can
replace E2 with  and obtain a more general
unambiguous extraction expression: E1 hpiE2 E1 hpi . (Similarly,
if E2=(pE2 ) =  then we can generalize E1 hpiE2 to  hpiE2.)
The problem is that E1 hpi might not be maximal, so we
must do some work on E1 to make our expression maximal. The algorithm, below, is one way to maximize such an
extraction expression.

Informally, E kpn consists of exactly those strings recognized
by E that contain precisely n occurrences of the symbol p.
Since the intersection of two regular expressions can be computed in polynomial time, E kpn can be computed in polynomial time.
Algorithm 6.2.

(Left-Filtering Maximization)

an extraction expression E hpi , where 
is a finite alphabet and E=(p ) kpn = ,
for some n  0.
Output: a maximal, unambiguous extraction expression
E 0 hpi that generalizes E hpi .
BEGIN
1
F := E=(p ) ;
2
S := ( p) (F kp0 );
3
n := 0;
p
4
while F kn 6=  f
5
S := S + (F kpn  p  ( p) F kpn+1 );
6
n := n + 1;
7
g0
8
E := E + S ;
9
return E 0 hpi
END
Input:

Proposition 6.3. (Correctness of Left-Filtering Maximization Algorithm) Let E hpi be an unambiguous extraction expression over a nite alphabet , such that
E=(p ) kpn =  for some n  0. Then the extraction expression E 0 hpi computed by Algorithm 6.2 is a maximal
and unambiguous generalization of E hpi .

SYNTHESIZING MAXIMAL
EXTRACTION EXPRESSIONS

In this section we rst propose an algorithm, called leftltering maximization, that can maximize a large class of
unambiguous extraction expressions. Then we develop a
pivot maximization framework, which can be used to enhance maximization algorithms. In particular, when applied
to the left-ltering maximization algorithm, it yields a much
more powerful maximization technique.

Pivot Maximization Framework.

Consider an extraction expression E hpi and suppose we
can nd an equivalent representation for E of the form:
E1  q1  E2  q2    En  qn  En+1
(1)
such that

141

 E hq i ;    ; En hqn i ; En hpi are all unam1

1

+1

biguous extraction expressions; and

 E hq i ; 0  ; En hqn i ; E0 n hpi 0can be maximized to E hq i ;    ; En hqn i ; En hpi , re1

1

spectively.

+1

1

1

+1

In such a case, we shall call each qi a pivot and say that
E hpi is pivot-maximizable. It turns out (by Proposition 6.6)

that given a pivot-maximizable expression as above, the expression
(E10  q1  E20  q2    En0  qn  En0 +1 )hpi
(2)

is a maximal and unambiguous generalization of E hpi .

Suppose the claim is true for n = k; k  0. Then for n =
k + 1, we have:
E hpi = (E1  q1    Ek  qk  Ek+1  qk+1  Ek+2 )hpi (3)
Let F be F = E1  q1    Ek 1  qk 1  Ek  qk  Ek+1. Since
the number of pivots in F is k, by induction hypothesis we
know that
(E10  q1    Ek0 1  qk 1  Ek0  qk  Ek0 +1 )hqk+1 i (4)
is a maximal and unambiguous generalization of F hqk+1 i .
Because Ek0 +2hpi is a maximal expression that generalizes Ek+2hpi (due to the assumption that E is pivotmaximizable), Proposition 6.50 implies that the result of composing Expression (4) and Ek+2hpi :
(E10  q1  E20  q2    Ek0 +1  qk+1  Ek0 +2 )hpi
(5)
is unambiguous and maximal. Clearly, Expression (5) generalizes Expression (3), since Expression (3) is
(F  qk+1  Ek+2)hpi
and Expression (4) generalizes F hqk+1 i . 2

Pivot maximization is a powerful framework for harnessing
the various specialized maximization algorithms. For instance, the left-ltering maximization algorithm can be used
in this framework as follows. Suppose E can be represented
as Expression (1), whereq E1 matches only a bounded number
of q1 's (i.e., E1=(q1  ) kn1 =  for some n  0), E2 matches
only a bounded number of q2 's, etc., and En+1 matches only
a bounded number of p's. Then conditions of left-ltering
maximization apply to each of the Ei 's, so we can maximize
the corresponding extraction expressions using that method.
By Proposition 6.6, Expression (2) is a maximal generalization of the original expression. Note that this technique is
strictly more powerful than plain
left-ltering maximization.
Indeed, to maximize E hpi through left-ltering, it must
be the case that E matches only a bounded number of p's. In
contrast, pivot maximization requires that only En+1 must
match a bounded number of p's, so E itself can potentially
match an unbounded number of p's.

7. PUTTING IT ALL TOGETHER

We are now going to revisit our motivating example of Section 3 and apply the tools and techniques we developed in
Sections 4 through 6.
Consider the two HTML pages from Sections 4, in their tagsequence representation. In both cases, we are interested in
the second INPUT-element of the form. The corresponding
extraction expressions (each one works only for one of the
two pages) are as follows:

The proof of correctness of pivot maximization relies on
Proposition 6.5 (and indirectly on Proposition 6.4).

P H1 /H1 P FORM INPUT <INPUT> P INPUT INPUT /FORM

Proposition 6.4. (Composition of Unambiguous Extraction Expressions) If E1 hq i and E2 hpi are unambiguous extraction expressions over a nite alphabet ,
where q; p 2  (and q = p is possible), then the extraction
expression (E1  q  E2 )hpi is unambiguous over .

and
TABLE TR TD /TD TD /TD /TR TR TD /TD TD /TD /TR
FORM TR TD INPUT /TD TD <INPUT> /TD TD INPUT /TD /TR
/FORM /TABLE

Theorem 6.6. (Correctness of Pivot Maximization)
If an extraction expression E hpi over a nite alphabet ,
represented as in Expression (1), is pivot-maximizable, then
Expression (2) is a maximal and unambiguous generalization of E hpi .

Our strategy is to rst generalize these strings into an extraction expression. Learning techniques of [18, 3, 8, 4, 5]
could be utilized at this stage. For the sake of this example, we will use a simple left-to-right merging heuristic,
which tries to nd a sequence of tags common to the two
strings and takes the union of everything in-between. This
yields the extraction expression below, where we replaced
TR TD /TD TD /TD /TR TR TD /TD TD /TD /TR with TR ... /TR,
to save space:
((P H1 /H1 P) + (TABLE TR ... /TR)) FORM
(6)
(TR TD)? INPUT (/TD TD)? <INPUT> T ags*
In this expression, the symbol \?" means that the corresponding subexpression can occur zero or more times, and
\+" stands for the union, as before.

By induction on n, the number of pivots in E .
The case of n = 0 is trivial.

By Proposition 5.4, this expression is unambiguous, but it
is not maximal. If none of the heuristics succeeds in producing an unambiguous expression, then the algorithm fails. An

Proposition 6.5. (Composition of Maximal Extraction Expressions) If E1 hq i and E2 hpi are maximal
unambiguous extraction expressions over a nite alphabet ,
where q; p 2  (and q = p is possible), then the extraction
expression (E1  q  E2 )hpi is maximal and unambiguous
over .

Proof:

142

interesting problem is to develop heuristics for guiding the
disambiguation process for extraction expressions, as mentioned in Section 8.

to generate suitable queries automatically. We believe that
the techniques presented here along with the works on learning regular expressions discussed in Section 2 can provide a
simpler solution. However, XML can make this task simpler
and more reliable. One interesting issue here is using DTDs
to guide the learning algorithms.

The left-merging heuristics used for the above example is
geared towards the pivot maximization framework. In our
case, we can use the symbols FORM and INPUT as pivots.
It turns out that the conditions for pivot maximization are
satised and that both of the three expressions

Finally, we should mention that, like all extraction techniques that are based on regular expressions, our framework
has limitations. For instance, we cannot learn or generalize extraction expressions that can be expressed only using context-free grammars. A typical example here is extracting the middle row from dynamically generated tables.
Indeed, the training set for such a learning system would
consist of extraction expressions of the form TR < TR > TR,
TR TR < TR > TR TR, etc. The desired pattern to learn here
is TRn < TR > TRn , but the language recognized by this expression is not regular, so this extraction problem cannot be
solved using regular expression based techniques. It would
therefore be interesting to extend our framework to include
more general patterns.It would then be possible to apply our
results to works like [6] and, thus, enhance their results.

((P H1 /H1 P) + (TABLE TR ... /TR)) <FORM> T ags*
(TR TD)? <INPUT> T ags* (/TD TD)? <INPUT> T ags*

can be maximized using the left-ltering algorithm (Algorithm 6.2). The result is the following maximal and unambiguous extraction expression:
(T ags- FORM)* FORM (T ags- INPUT)* INPUT
(T ags- INPUT)* <INPUT> T ags*

It is worth noting that Expression (6) can also be maximized
by a direct application of Algorithm 6.2. However, this will
produce a dierent (much larger) extraction expression. The
semantics of the two expressions will also be dierent: while
the above expression always nds the second INPUT-element
in the rst form, the expression produced by a direct application of Algorithm 6.2 would be looking for a second
INPUT-element on the page, even if the rst and the second
INPUT-element come from dierent forms.
8.

Acknowledgment.

The authors would like to thank C.R. Ramakrishnan for the
helpful comments and suggestions.
9. REFERENCES

[1] S. Abiteboul. Querying semi-structured data. In Int'l
Conference on Database Theory, volume 1186, pages
1{18, Delphi, Greece, 1997. Springer.

CONCLUSION AND FUTURE WORK

In this paper, we have made the rst few steps towards a
theory of resilient data extraction from semistructured documents. To this end, we dened the notion of extraction
expression, provided a correctness criterion for it (unambiguity), and formalized the intuitive notion of such an expression being \robust" in the presence of changes in the source
document (maximality). We provided complexity results for
deciding ambiguity and maximality and proposed powerful
algorithms that can maximize very large, practical classes
of unambiguous extraction expressions.

[2] Serge Abiteboul, Dallan Quass, Jason McHugh,
Jennifer Widom, and Janet L. Wiener. The lorel query
language for semistructured data. Int. J. on Digital
Libraries, 1(1):68{88, 1997.
[3] B. Adelberg. NoDoSe: A tool for semi-automatically
extracting structured and semi-structured data from
text documents. In ACM SIGMOD Conference on
Management of Data, pages 283{294, Washington,
1998. ACM.

Several problems still remain. First, it is still unknown
whether the general problem of maximization is decidable.
Second, there still is a need for learning techniques that generate good initial unambiguous expressions that could be
used by our maximization algorithms. The works discussed
in Section 2 do not address this issue. One way how the
existing algorithms can help the task of learning unambiguous extraction expressions is as follows: we can use them to
generate ambiguous expressions rst. Then we could feed
this expression to a \disambiguation procedure" along with
a number of counterexamples. Developing such disambiguation techniques is a topic for future research. The third line
of work is to explore classes of regular expressions that can
be maximized with lower computational complexity.

[4] D. Angluin. Finding patterns common to a set of
strings. In ACM Symposium on Theory of Computing,
pages 130{141, 1979.
[5] D. Angluin. Inductive inference of formal languages
from positive data. Information and Control,
45:117{135, 1980.
[6] N. Ashish and C. Knoblock. Wrapper generation for
semi-structured internet sources. ACM SIGMOD
Record, 26(4):8{15, 1997.
[7] P. Atzeni and G. Mecca. Cut & paste. In ACM
Symposium on Principles of Database Systems, pages
117{121, Arizona, June 1997. ACM.

Another interesting issue is to explore data extraction from
XML. Although XML documents are generally better structured than HTML, automatic extraction from such documents calls for the creation of ontologies in order to be able

[8] R.C. Berwick and S. Pilato. Learning syntax by
automata induction. Machine Learning, 2:9{38, 1987.

143

[22] A. Sahuguet and F. Azavant. Web Ecology: Recycling
HTML pages as XML documents using W4F. In ACM

[9] B.Ribeiro-Neto, A.H.L. Laender, and A.S. da Silva.
Extracting semi-structured data through examples. In

SIGMOD Workshop on Database the Web and
Databases (WebBD'99), pages 31{35, Philadelphia,

Proceedings of the International Conference on
Knowledge Management, November 1999.

Pennsylvania, June 1999.

[10] P. Buneman. Semistructured data. In ACM

Symposium on Principles of Database Systems, pages

117{121, Tucson, Arizona, June 1997.
[11] P. Buneman, S. Davidson, G. Hillebrand, and
D. Suciu. A query language and optimization
techniques for unstructured data. In ACM SIGMOD
Conference on Management of Data, Montreal,
Canada, 1996. ACM.
[12] D.W. Embley, D.M. Campbell, Y.S. Jiang,
S.W. Liddle aand D.W. Lonsdale, Y.-K. Ng, and R.D.
Smith. Conceptual-model-based data extraction from
multiple-record web pages. Journal of Data and
Knowledge Engineering, November 1999.
[13] Jean-Robert Gruser, L. Raschid, M. E. Vidal, and
L. Bright. Wrapper generation for web accessible data
sources. In Proceedings of the Third International
Conference on Cooperative Information Systems
(CoopIS98), pages 14{23, New York City, New York,

[14]

[15]

[16]
[17]
[18]

[19]
[20]

[21]

1998.
J. Hammer, H. Garcia-Molina, J. Cho, A. Crespo, and
R. Aranha. Extracting semistructured information
from the web. In In Proceedings of the Workshop on
Management of Semistructured Data, pages 18{25,
Tucson, Arizona, May 1997.
J. Hammer, Hector Garcia-Molina, S. Nestorov,
Ramana Yerneni, Markus M. Breunig, and Vasilis
Vassalos. Template-based wrappers in the tsimmis
system. In ACM SIGMOD Conference on
Management of Data, pages 532{535. ACM, 1997.
J.E. Hopcroft and J.D. Ullman. Introduction to
Automata Theory, Languages, and Computation.
Addison-Wesley, Reading, MA, 1979.
http://www.jango.com. Jango Corporation.
N. Kushmerick, D. S. Weld, and R. B. Doorenbos.
Wrapper induction for information extraction. In Int'l
Joint Conference on Articial Intelligence, volume 1,
pages 729{737, Nagoya, Japan, 1997.
H.R. Lewis and C.H. Papadimitriou. Elements of the
Theory of Computation. Prentice Hall, Englewood
Clis, NJ, 1981.
Seung-Jin Lim and Yiu-Kai Ng. An automated
approach for retrieving hierarchical data from html
table. In Proceedings of the International Conference
on Knowledge Management, November 1999.
M. Perkowitz, R.B. Doorenbos, O. Etzioni, and D.S.
Weld. Learning to understand information on the
internet: An example-based approach. Journal of
Intelligent Information Systems, 8(2):133{153, March
1997.

144

SocialCom/PASSAT/BigData/EconCom/BioMedCom 2013

K–partitioning of Signed or Weighted Bipartite
Graphs
Nurettin B. Omeroglu, Ismail H. Toroslu
Middle East Technical University, Dep. of Computer
Engineering, Ankara, Turkey
{omeroglu, toroslu}@ceng.metu.edu.tr

Sedat Gokalp, Hasan Davulcu
Arizona State University, Sch of Computing, Inf. and
Decision Sys. Eng., Tempe, AZ, USA
{sedat.gokalp, hasan.davulcu}@asu.edu

Abstract— In this work, K-partitioning of signed or weighted
bipartite graph problem has been introduced, which appears as a
real life problem where the partitions of bipartite graph
represent two different entities and the edges between the nodes
of the partitions represent the relationships among them. A
typical example is the set of people and their opinions, whose
strength is represented as signed numerical values. Using the
weights on the edges, these bipartite graphs can be partitioned
into two or more clusters. In political domain, a cluster
represents strong relationship among a group of people and a
group of issues. In the paper, we formally define the problem and
compare different heuristics, and show through both real and
simulated data the effectiveness of our approaches.
Keywords—
Partitioning

Social

I.

Networks,

Bipartite

Graphs,

3. The sum of the weights of the negative edges within
clusters is minimized,
4. The sum of the weights of the negative edges between
clusters is maximized.
In Fig. 1, sets and are partitioned into clusters from 1
to k, where clustering is done simultaneously. Simultaneous
(i.e., vertical) partitioning is a little bit confusing; some may
think that there are
clusters in the figure instead of
clusters. To clarify the key point of simultaneous partitioning,
we can say that
and
jointly forms
like shown in
Figure 2.
Each line in Fig. 1 represents the sum of the weights of the
to ,
where label N (i.e., red lines)
edges from
denotes negative and label P (i.e., green lines) denotes positive
signs. Note that some of the lines are thicker than the others.
Thick lines represent higher values than the thin ones.

Graph

INTRODUCTION

One very common form of a social network is a simple
bipartite graph where one partition U represents actors (e.g.,
people, organizations) and the other partition V represents a set
of issues (e.g., political issues, beliefs). One of the earliest
definitions of this problem is given in [1]. An edge between a
person and an issue represents the opinion of that person on
that issue. This opinion expressed with a sign, as positive or
negative, (no edge between a person-issue pair expresses “no
opinion”), and, a numerical value representing the strength of
the opinion of person.

Fig. 1. Partitioning of A to A1, A2, .. Ak and B to B1, B2, .. Bk

The inputs of k-way partitioning of signed bipartite graph
problems are bipartite graph
, label function
and partition count . Label of an edge can be
positive or negative real value. In most cases the range of the
mapping is either a small subset of integers of real values. For
this modeling, we assume that a positive edge from
to
and
means that
supports , and a
where
negative edge implies that is against . The goal of the
and
into
partitioning problem is to divide the sets
) and (
) simultaneously to form
(
clusters (
, such
disjoint max
that,

Fig. 2. Nodes are distributed among blocks
In Figure 2, six nodes from and four nodes from are
distributed into three blocks. In this example, separators (i.e.,
thick vertical red lines) cut negatively weighted edges between
blocks and divide nodes into 3 partitions. For illustration, all
nodes are distributed ideally so that edges within blocks are all
has positive and edges between blocks are all has negative sign.
As one would predict, most of the time, partitioning may not be
perfect, meaning that there can be negative edges within
clusters and positive edges between clusters.

1. The sum of the weights of the positive edges within
clusters is maximized,
2. The sum of the weights of the positive edges between
clusters is minimized,
978-0-7695-5137-1/13 $26.00 © 2013 IEEE
DOI 10.1109/SocialCom.2013.122

815

To our knowledge, no efficient algorithm was presented for
k-way partitioning of signed bipartite graph problem before our
study, as stated in [2]. This study is the extension of the work
in [2] in two folds: One of the extensions is k-way partitioning
of bipartite graphs, and the other one is the reduction of the
execution time almost by half using a simple observation about
the hill-climbing algorithm.

Subject to

The objective function (1) gives the maximized L value as
the objective value, by the help of constraints (2). ’s
and ’s expressed in (1) and (2) are the variables of these
equations. As seen in the above formulation (1), L value can be
obtained by subtracting the sum of edges across clusters (let’s
say “O”-out) (i.e., right part) from the sum of edges within
clusters (let’s say “I”-in) (i.e., left part). Clearly, we can find O
by subtracting I from the total sum of edges (let’s say T), since
T = I + O. Thus, the above formulation (max L= I - O) can be
rewritten as follows (max L= 2I - T):

The rest of the paper is organized as follows; mathematical
model of the problem given in Section 2. Generic and movebased heuristic are presented in Section 3 and 4 respectively.
The results obtained from real datasets are presented at the 4th
Section. Finally, the last Section contains the conclusions and
the future work.
II.

MATHEMATICAL MODEL

Let
and
partitioning of the nodes of bipartite graph
and let

be
,

As the right part of the formulation (T) is constant, to
maximize L we need to maximize the left part of the
formulation (let’s say );

,
be indicator vectors for
Thus,

and

respectively,

.

It is not possible to convert this problem into linear
programming (LP) problem as it is and solve with a LP solver,
since there are nonlinear terms (i.e.,
) in (3).

Clearly,
and

III.

and
and

GENERIC ALGORITHMS

Two well-known generic heuristics, namely genetic
algorithms and simulated annealing, have been applied to solve
k-way partitioning of signed bipartite graphs problem.

.

A genetic algorithm (GA) [4] is a heuristic algorithm,
inspired by evolutionary processes of ecological systems, that
finds optimal (or near-optimal) solutions to complex
optimization problems. In GAs, possible solutions to the
problem are coded in chromosomes. A “chromosome” (or
“individual”) can be designed as a string, binary digit or other
symbols that corresponds to a solution of the problem at hand.
The fitness function of GA analyzes “genes” in the
chromosomes, makes some qualitative assessment and
provides a meaningful and comparable fitness value for that
solution.

Fig. 3. Illustrative Example
represents the adjacency matrix for the
Let
. The sum of all edges in the
bipartite graph
clusters is given by [5].

A typical GA works as follows:
x Construct an initial population of chromosomes by
generating randomly attempted solutions to a problem
x Do the following until a satisfactory fitness level has
been reached or run out of time:
─ Evaluate each fitness of the solutions
─ Keep a subset of these solutions (using different
heuristics)
─ Use these solutions to generate a new population by
using the crossover and mutation operators.

The mathematical programming formulation can be written
as follows:

816

x

There are many different techniques which a genetic
algorithm can use to select the individuals to be copied over
into the next generation [4].
There are two basic reproduction strategies, which are
crossover and mutation. Crossover is a reproduction technique
to generate two offspring from two selected parents. The
chromosomes of the two parents are recombined according to
some techniques to form offspring. Mutation is a reproduction
mechanism, which generates new offspring from single parent.
Each binary digit of the chromosome is subject to inversion
under a given probability (most of the time small).

In order to measure the quality of clustering of bipartite
graphs, as in [2], we have defined a gain function that
recalculates the gains of all nodes as the vertices are placed into
blocks. The gain calculation is done as follows:

Simulated annealing (SA) is a generic probabilistic metaalgorithm used to find an approximate solution to global
optimization problems, which was introduced by Kirkpatrick
[5]. It is inspired by annealing in metallurgy which is a
technique of controlled cooling of material to reduce defects.

x

If both vertices are in the same block, and the edge
between them is positively weighted, then moving
either one will reduce the gain.

x

Similarly, if the vertices are in different blocks and the
edge between them is negatively weighted, then
putting them into the same block will also reduce the
gain.

x

If the vertices are in the same block, but the edge
between them is negative weighted, then, moving one
of them to a different block will increase the gain.

x

Finally, if two vertices are in different blocks, but the
edge between them are positively weighted, then
moving them into the same block will increase the
gain.

A typical SA algorithm works as follows:
x Initialize temperature , epsilon , alpha
x Generate a random initial solution as current solution
x Do the following till

or run out of time

x While stopping criteria not met do
─ Find the neighbor of the current solution
(i.e.,
: fitness
o Compute
function)
o Randomly generate a real number from 0 to 1
then
o If
─ Reduce T by multiplying with

Move-Based Heuristic (MBH) Algorithm
Input : Graph:
Number of clusters: , Number of iterations: .
Output: Maximal value and partitions of nodes.
1:
2: while
3: Initially, place each node into block 1 to randomly
4:
5: do
6:
7:
Compute gains of all nodes (Refer to Figure 4)
8:
do
9:
nod1 select the unlocked node with max gain
10:
blck1 select the best block for nod1
11:
place the nod1 into blck1
12:
update gains of nod1’s neighbors
13:
New RESULT
14:
lock nod1
15:
until all nodes are locked
16: while
17:
18: end while
19: print

SA starts with some solution that is totally random, and
changes it to another solution that is similar to the previous
one. Newly generated solutions are generally chosen
randomly, though more sophisticated methods can be applied.
If this solution is a better solution, it will replace the current
solution. If it is a worse one, it may be chosen to replace the
current solution with a probability that depends on the
temperature (i.e., cooling process, decreases with time) and
the distance (i.e., difference between new (worse) solution
and the old one) parameters. As the algorithm progresses, the
temperature parameter decreases by multiplying , giving
worse solutions a lesser chance of replacing the current
solution.
IV.

MOVE-BASED HEURISTIC

Move based heuristic (MBH) is a typical hill-climbing
algorithm. For k-partitioning of signed bipartite graphs, movebased heuristics work as follows:
x

Nodes are randomly placed into the blocks at the
beginning.

x

Then, through iterations, the node with the highest
gain value is selected and moved to the block that
maximizes the gain. After each move, that node is
locked. Until all the nodes are locked, the iteration
continues.

After all nodes are locked, the change in the result
value L (the objective value which is defined in
Section 2) is checked. If the completed iteration
increases the value of L, a new iteration starts by
configuring the initial state with the best state found in
the previous iteration. Otherwise, iterations end and
best solution is returned.

In MBH Algorithm, which is given above, L represents the
result of the objective function, and K is the number of clusters,
which is given as an input. Hill-climbing algorithms usually
improve the result, but it is always possible to strike at local
maximum. In order to avoid this problem we repeat the process

817

been reached. This is done simply by comparing the current
objective value,
, with the one that is already been
obtained, . If the trend of the current value is a decrease, then,
the rest of the traversing the nodes has been abandoned. Below
is the extension on MBH Algorithm for this addition. This
addition should be made to the end of the inner loop between
the lines 8-15 of MBH Algorithm.

several times. Therefore, we use one more parameter, R, to
randomly start the process more than once in order to be able to
avoid local minimum.
While we were analyzing the outputs of MBH, we have
observed that there have been some unnecessary moves in the
process of MBH algorithm. These redundant moves become
clear by displaying a sample run of the algorithm as in Figure
5. This figure has been obtained from questionnaire dataset
(which will be explained in the following section) experiment
for K=9. In this experiment, the R value is 3, and therefore the
MBH algorithm has been applied 3 times with 3 random initial
solutions.

Optimized MBH Algorithm
At each iteration, we set the
…
1: if
2:
3: else if
then
4: ; // Do Nothing
5: else
6:
if
then
7:
else
8: end if
9: if
then
10:
exit loop
11: end if
…

Fig. 4. Questionnaire Experiment (K=9), MBH Stats

V.

then

EXPERIMENTAL RESULTS

The methods expressed in this work are all implemented in
C++, using the Visual Studio 2005 development environment.
Tests are done on a commodity computer having Windows 7
x86 OS, Intel Core 2 Duo 2.00 GHz CPU, and 3 GB RAM.
1st dataset contains questionnaire with 48 questions, which
are applied to 7572 people. The questions were ranked between
-5 and +5. 2nd dataset corresponds to US Congress (SENATE)
dataset which is published publicly in www.govrack.us. From
this site, we have used the roll call votes for the 111th US
Congress Senate that covers the years 2009-2010. The 111th
Senate data contains information about 108 senators and their
votes on 696 bills. We have constructed a signed bipartite
graph as in [3] based on the votes of the senators on the bills.

Fig. 5. Questionnaire Experiment (K=9), Opt-MBH Stats
In Figure 4, we see that objective values are increasing and
decreasing in a systematic way. Decreasing parts are not
necessary for our solution, since we have been trying to find
the global maximum. Furthermore, as the chart presents,
calculation of the descending values is really time consuming.
Therefore, we wanted to remove the declining parts from the
execution (chart) to reduce the total execution time. This is
done by detecting when the values start to fall below the local
maxima. In this way, we have managed to cut the unnecessary
parts of the computation as shown in Figure 5. Note that the
elapsed time has fallen below by half (approximately 57000 to
20000 ms).

A. Senator Experiment
Opt-MBH algorithm had clustered 108 senators and 696
bills into 3 clusters. That is, the gain has increased when the
cluster size is increased from 2 to 3, but, there were no increase
afterwards. Figure 6 shows the 2-way and the 3-way
partitioning of the bipartite graphs. In these figures columns
correspond to the bills and the rows correspond to the senators.
The colors (green and red) correspond to the votes of senators
on the bills (favor or against). Notice that blue lines have been
inserted into these figures in order to make clusters more
visible.

The standard version of MBH locks all of the nodes
(traverses all of the nodes) in each iteration even after a local
maximum has been reached, which causes the decline of the
total gain. In order to cut this wasted time we have modified
the algorithm just to detect whether the local maximum has

Fig. 6. Partitioning of Senators and Bills with K = 2 (left) and K = 3 (right)

818

Table 1. Comparison of Results (K=2, K=3, K=9)
Best Results

K=2
GA
642282
46066

SA
642291
46066

1
2

GA
923707
33924

SA
7454
1406

1
2

GA
1115816
271807

SA
12321
2136

1
2
Time to Find
the Best

MBH
64221
46066

OptMBH
64221
46066

GA
685281
46066

SA
678842
46422

MBH
746
14

OptMBH
437
11

GA
1071273
33924

SA
10518
2321

MBH
41116
878

OptMBH
11023
289

GA
1105766
271807

SA
14602
3451

K=2

Total
Exec. Time

x

OptMBH
686261
46422

GA
682912
46376

SA
682900
46422

OptMBH
1407
15

GA
1079663
191094

SA
17068
4607

OptMBH
19580
401

GA
1090272
269835

SA
23489
7193

K=3

K=2

MBH
4855
16

MBH
57415
1033

K=9
MBH
695053
46422

OptMBH
694831
46422

K=9

K=3

The US Senate has 2-party system (with 2 independents,
mostly inclined to Democrats), with 100 members. During the
2 years of 111th Senate, the numbers of the members of both
parties have changed due to different circumstances. Therefore,
the total number of senators has also increased to 108. In two
clustering, the clusters were roughly representing the party
lines. During 111th Senate, the number of Republicans was 39
in its minimum level, and one of the clusters our system has
obtained exactly had that many senators. Of course, there are
several Senators voting quite independently from their
respective parties. However, even in 3-cluster structure, it has
been observed that senators were not clustered forming a 3rd
group. Only, a small number of bills have been discovered,
which are mostly been rejected by the senators of both parties.
The structures of 2 and 3 clusters are as follows:
x

K=3
MBH
686261
46422

MBH
61346
39

OptMBH
32776
34

K=9
MBH
143526
2002

OptMBH
54711
948

C. Comparing MBH and Optimized-MBH
In the final experiment, MBH and Opt-MBH have been
compared with inputs of not only the same datasets but also the
same randomly generated initial solutions. “Questionnaire
dataset with K=9” and “Senator dataset with K=8” results are
displayed in the figures below. As can be seen from the
minimum values in these graphics, R=3 has been used in these
experiments. Figure 8 contains two graphics which emphasize
total execution time of the algorithms and the cut points (i.e.,
vertical lines) in Opt-MBH for the questionnaire dataset. In
Figures 9 and 10, senator dataset has been used and
approximate saved times are also shown.

in 2-way, 39 senators and 257 bills formed one cluster
and 69 senators and 439 bills formed the other one,
in 3-way, the number of senators were the same for the
first two clusters, and the third cluster had 0 senators,
however, 7 bills from the first cluster, and 4 bills from the
second cluster had been moved into the third one making
it with 0 senators and 11 bills.

B. Questionnaire Dataset
The data size was very large and dimensions were
disproportional, we could not print the results in a figure
similar to the one that we have done for the Senate experiment.
Opt-MBH algorithm had partitioned this weighted bipartite
graph into 9 clusters, as the best clustering structure. We tried
all the cluster sizes from 2 to 9. We have discovered that the
objective value increased for each cluster size as it can be seen
from Figure 7. Similar to the Senate experiment, some clusters
had only vertices from one of the partitions of the bipartite
graph

Fig. 8. Execution times of Questionnaire Dataset with MBH
and Opt-MBH

Fig. 7. Results of Questionnaire Experiment with Moving
Average Trendline

819

Fig. 9. Execution times of Senator Dataset with MBH

VI.

Fig.10. Execution times of Senator Dataset with Opt-MBH

REFERENCES

CONCLUSION AND FUTURE WORK
[1]

This work extends previous work on 2-way clustering of
signed bipartite graphs [2] to k-way clustering of signed or
weighted bipartite graphs. This problem appears in social
networks in many different forms.

[2]

In this study, for k-way partitioning of the signed bipartite
graphs problem, mathematical methods, generic algorithms
and various move-based heuristics have been developed. We
have shown that our approaches are quite effective through
experiments on real world data. Our experiments show that
optimized move-based heuristic algorithm produces the best
result and has the best execution time.

[3]
[4]
[5]

VII. ACKNOWLEDGMENT
This research was supported by US DoDs Minerva
Research Initiative Grant N00014-09-1-0815, Project leader:
Prof. Mark Woodward, Arizona State University, and the
project name is “Finding Allies for the War of Words:
Mapping the Diffusion and Influence of Counter-Radical
Muslim Discourse”.

820

Andrej, M., and Doreian, P. Partitioning signed two-mode networks.
Journal of Mathematical Sociology 33, pages 196–221, 2009.
Banerjee, S., Sarkar, K., Gokalp, S., Sen, A., and Davulcu, H.
Partitioning Signed Bipartite Graphs for Classification of Individuals
and Organizations. Social Computing, Behavioral - Cultural Modeling
and Prediction Lecture Notes in Computer Science Volume 7227, 2012,
pp 196-204
Bansal, N., Blum, A., and Chawla, S. Correlation clustering. In Machine
Learning, pp. 238–247, 2002.
Holland, J. H. Adaption in Natural and Artificial Systems. University of
Michigan Press, 1975.
Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P. Optimization by
simulated annealing. Science 220, 671-680, 1983.

Logic Based Modeling and Analysis
(Extended Abstract)

*

Hasan Davulcu

Michael Kifer

SUNY ot Stony Brook

SUNY at Stony Brook

SUNY at Stony Brook

SUNY at Stony Brook

davulcuQcs.sunysb.edu

kifer@cs.sunysb.edu

cram@cs.sunysb.edu

ram@cs.sunysb.edu

CR. Ramakrishnan

WC propose Concurrent Transaction Logic (C7X) as the language for specifying, analyzing, and scheduling of workflows.
We show that both local and global properties of worktlows
can be naturally represented as C7X formulas and reasoning
can be done with the use of the proof theory and the semantics of this logic, We describe a transformation that leads to
an eilicicnt algorithm for scheduling worldlows in the presencc of global temporal constraints, which leads to decision
proccdurcs for dealing with several safety related properties
such as whether every valid execution of the workflow satisfits a particular property or whether a worlcfiow execution is
consistent with some given global constraints on the ordering of events in a workflow. We also provide tight complexity
results on the running times of these algorithms.

1 Introduction
A wark,/low is a collection of cooperating, coordinated activities designed to carry out a well-defined complex process,
such as trip planning, graduate student registration procedure, or a business process in a large enterprise. An activity
in a workflow might be performed by a human, a device, or
a program. Workflow management &ems
provide a framework for capturing the interaction among the activities in
a workfiow and are recognized as a new paradigm for integrating disparate systems, including legacy systems [20, 81.
Ideally, they should also help the user in analysis and reasoning about complex business processes.
It has been realized that analysis and reasoning about
workflows requires a formal specification model with a well
defined semantics [18, 21. In this paper, we develop a novel
framework for specifying, analyzing and executing workflows
based on Transaction Logic [5,4, 6, 71.

Workflow representation frameworks.
picts three most common frameworks

I.V. Ramakrishnan

flows: control flow graph, triggers (ale0 known BS euentcondition-action rules), and temporal constraints.
The control flow graph is most appropriate for depicting
the local execution dependencies of the activities in a workflow; it is a good way to visualize the overall flow of control.
Control flow graphs are the primary specification means in
most commercial implementations of worktlow management
systems. A typical graph specifies the initial and the final
activity in a workflow, the successor-activities for each activity in the graph, and whether these successors must 021
be executed concurrently, or it suflices to execute just one
branch non-det erministically.
In Figure 1, all successors of
activity a must be executed, which is indicated with the
“AND”-label.
In contrast, “OR” indicates that when b is
finished, there is a choice of executing d, h, then j or e then
j. Successful execution of any one of these branches should
sufiice for the overall success of the worldlow.
Arcs in a control flow graph can be labeled with tmnsition conditions. The condition applies to the current state
of the worlctlow (which, in a broad sense, may include the
current state of the underlying database, the output of the
completed tasks, the current time, etc.). When the task at
the tail of an arc completes, the task at the head can begin
only if the corresponding transition condition evaluates to
true.
The Worldlow Management Coalition [lo] identifies additional
controls, such as loops and sub-worldlows. Various
researchers have also suggested other types of controls, including alternative execution and compensation for failed
activities [12, 17, 15, 25, 28, 1, 131. However, control flow
graphs have one obvious limitation:
they cannot be used to
specify gIoba2 dependencies between worldlow tasks, such as
those expressed as global constraints on the right-hand side
of Figure 1.
Defming workflows using triggers is yet another possibility [ll]. However, this method is not as general as control
flow graphs. For instance, like the graphs, triggers cannot
be used to specify global task dependencies, and they are
not sufliciently expressive when it comes to representing alternatives
in worldlow execution (depicted as “OR” nodes
in Figure 1). In fact, it follows from a result in [7] that triggers with so-called “immediate” execution semantics can be
represented using control flow graphs, and this result can be
adapted to triggers with the “eventual” execution semantics
as well. Since triggers can be %ompiled into” the control
flow graph, we shall be treating triggers as part of the control flow graph.
Other researchers proposed frameworks that rely exclusively on constraints to specify both local and global prop-

Abstract

.

of Workflows

Figure 1 defor specifying work-

‘Tlth
work is partially supported by a DLA/DARPA
contract
nnd by the NSF grants IRI-9404629, CCR-9705998,9711386,9510072
9404921, CDA-9504276,9303181, INT-9600598

25

Control

Flow

Triggers

Graph

If event d occurs then
If cm& holds then
execute activity p
If event g occurs then
If cendr hoIds then
execute activity q

Global

Constraints

If d is ever executed then
g must also be executed
If

e and i arc both executed
then e must happen before i

If g and j are both executed
then j must happen before g

Figure 1: Frameworks for Specifying WorkfIo~7s
1. Apply transformation

crtics of workflows. In [26, 271, Singh describes an algebra
of temporal constraints, which is believed to cover all useful
global dependencies that might arise in workflow systems.
For instance, this algebra includes Klein’s constraints [22],
which commonly occur in workflow specifications ‘. This
algebra is sufficiently expressive for modeling control flo17
grnphs that have no transition conditions attached to arcs.
However, such constraints cannot be used to model workflows that query the intermediate state of the workflow and
make scheduling decisions based on the outcome. More importantly, algebraic approaches are not part of a larger reasoning framework and so it is unclear how they apply to
more expressive specification languages (e.g., to represent
oub-workflows, failure and compensation, etc.) and how
they could be used for reasoning and verifying the correctncnn of workflows.

enables us to determine:

(a) Whether every legal execution of a given workflow specification satisfies a particular property.
Moreover, if some execution does not satisfy the
property, then the verification procedure returns
a counter example \7bich
is the most general execution of the workflow that violates the property
in question.
(b) Whether the specitlcation made up of the control
flow
graph and global constraints is consistent;
and
(c) Whether some of the specified constraints
dundant.

are re-

2. The transformation

eliminates the parts of the control
graph that are inconsistent with the constraints, which
facilitates scheduling of events at run-time.

Logic-based formalism. In this paper, we base our apnroach on Concurrent Transaction Logic 161 (abbr., C7%).
Th crc arc many reasons for this choic;. I?&& control flow
graphs with transition conditions can be easily and naturally represented in C’I%. Second, [7] shows that triggers
ore easy to represent as well, Finally, [53 contains an extensive discussion of the temporal capabilities of C’T’R. In
particulnr, the entire algebra of constraints described in [26]
is isomorphic to a small subset of the propositional Transaction Logic. Hence C’lX provides us with a unifying formalism that subsumes all of the three popular worktlow specificntion frameworks described above. Furthermore, being a
full-blown logic, C’7% allows not only scheduling workflows,
but nlso reasoning about their properties. Finally, like in
logic programming systems, the proof theory of C’I% is also
a run-time environment for ezecuting workflows.

3. The separation of control flo17 graph and global constraints in the workilow specifications leads to tighter
complexity results for the veriiication problem.
Our results also contribute to the theory of Transaction
Logic itself. Here, we essentially extend the efficient SLDstyle proof procedure of C7R from so called concurrentHorn goals to a larger class of formulas, which incorporates
temporal constraints (a more precise formulation appears in
Section 2).
2

An Overview of Concurrent

Transaction

Logic

This section provides a short summary of the CTR syntax,
which is used in this paper to represent workflows. Due to
space limitation, we cannot discuss the model theory of the
logic or its proof theory. Instead, we rely on the procedural
reading of CTR. statements? A thorough treatment of the
main aspects of Transaction Logic appears in [6, 5, 41. A
fairly detailed, yet informal introduction
can be found in

Summary of resdt~.
Our approach is based on a transformation procedure, named Apply, which accepts a workflow specification that includes a control flow graph 9, triggers (viewed as part of G), and a set of temporal constraints
C, and constructs an equivalent specification in C7X that
rcpreeents only those executions of G where C holds. The
rcoulting specification can be directly used to execute the
workflow. Thus, Apply can be viewed as a compilotionprocess that facilitates verification of workflows and optimizes
run-time scheduling.

[W

C’TR is a conservative extension of the classical predicate
logic in the sense that both its proof theory and the model
theory reduce to those of the classical logic for formulas that
do not cause state transitions (but only query the current
state).

‘Klein constraints arc of the form: (1) if events a and b both
occur, then a occurs earlier than b; or (2) if event a ever occurs then
b mnst occur as well (before or after a).

‘This is analogous to the procedural reading of Datalog programs.

26

Basic syntax. The atomic formulas of CT’R are identical
to those of the classical logic, i.e., they are expressions of
the form p(h, , . . , t,,), where p is a predicate symbol and
the t[‘s are function terms. More complex formulas are built
with the help of connectives and quantifiers.
Apart from the classical V, A, 7, V, and 3, C’J’R has two
additional connectives, Q (serial conjunction) and ] (concurrent conjunction), and two modal operators, 0 (esecutional possibility) and 0 (isolated execution). For instance,
;y(-Ll
0 q(X)) 1 (VY(r(Y) V 8(X, Y))) is a we&formed
0
,
Underlying the logic and its semantics
Informal
scmentice.
is a set of database states and a collection of naths. For the
purpose of this paper, the reader can think of the states as
just a set of relational databases, but the logic does not rely
on the exact nature of the states - it can deal with a wide
variety of them.
A path is a finite sequence of states. For instance, if
51, 5s,,,,, By are database states, then (51), (sl,s5), and
(51,5s, ,.,, 5,,) are paths of length 1, 2, and n, respectively.
Just as in classical logic, C’j?. formulas assume truth
values, However, unlpnl classical logic, the truth of C’IX
formulae is determined over paths, not at states. If a formula, 4, is true over a path (51, ....s.,), it means that I$ can
efficcute starting at state 51. During the execution, the current state will change to 52, 55, .... etc., and the execution
.
terminates at state 8*,
With this in mind, the intended meaning of the C73~
connectives can be summarized as follows:
l

l

execute 4 then execute 11. Or, modeltheoretically, 4 @$ is true over a path (51, .... 5”) if 4
is true over a prefix of that path (say, (51, .... 5;)) and
$ is true over the s&ix (i.e., (s;, .... sn)). In terms
of control flow graphs (cf. Figure l), this connective
represents arcs connecting adjacent activities.
d, @ qb means:

a0 (

condl0b0((d0cond30h)Ve)0j)

( cod

1

0 c 0 ((f 0 i 0 conda) v (g 0 conds))) > 0 k

0)
Expressions of the above form are called concurrent-Horn
goals. FormalIy:
l

l

any atomic formula is a concurrent-Horn goab
4 @$,4 1 11,and 4 V $ are concurrent-Horn goals, if
soare4and$;
@!Jand 04 are concurrent-Horn goals, if so is 4.

It should be clear from the above example how control flow
graphs translate into concurrent-Horn goals.
A concurrent-Horn
rub
is a CTR formula of the form
head t body, where head is an atomic formula and body is
a concurrent-Horn goal.
In this paper, we limit our attention to non-iterative
workflows, which means that we do not allow recursive concurrent rules. Section 7 discussesto what extent our present
results apply to recursively defined workflows.
From the workflow point of view, the primary use for the
rules is to represent sub-workflows. Indeed, since workflows
and sub-workflows can be described using concurrent-Horn
goals, me can use the rules of the form 8ub WorkFlowName
t
sub WorkFlowDejinition
to define sub-workflows. For
instance, sub WorkFlowName can be used in workflow specifications as if it were a regular activity, thereby completely
hiding the underlying structure of the activity from top-level
specifications.
Observe that the definition of concurrent-Horn rules
and goals doe8 not include the connective A. In general,
A represents constrained ezecution, which is usually hard
to implement, since constraints must be checked at every step of the execution. If a constraint violation is detected, a new execution path must be tried out. In contrast,
the concurrent-Horn fragment of C’TR is efficiently implementable, and there exist an SLD-style proof procedure that
proves concurrent-Horn formulas and esecutes them at the
same time [6].
The efficiency gap between concurrent-Horn execution
and constrained execution is the main motivation for our
results. In logical terms, we shoh7 that, for a large class of

q5I$ means: (p and $ must both execute concurrently,
in an interleaved fashion. This connective corresponds
to the “AND”-nodes in control flow graphs.

dame path, In practical terms, this is best understood
in terms of constraints on the execution. For instance,
4 can be thought of as a transaction and JI as a constraint on the execution of 4. It is this feature of the
logic that let5 us specify temporal constraints as part
of workfiovf specifications.
meana: execute 4 or execute J, nondeterministicaUy. This connective corresponds to the
t~OR”-nodes in control flow graphs.

b 4 V fj

l

0~~5means: check if4 is executable at the current state.
Section 7 discusses the role of the possibility operator
0 in workflow modeling.

Concurrent-Horn
subset of CTR. Next, we define the implication, p t q, as p V 74. The form and the purpose of
the implication in CTR is similar to that of Datalog: p can
be thought of as the name of a procedure and q as the definition of that procedure. However, unlike Datalog, both p
and q assume truth values on execution paths, not at states.
More precisely, p t q means: if q can execute along a
path (81, .... s,,), then so can p. Ifp is viewed as a subroutine
name, then the mea&g can be re-phrased as: one way to
execute p is to execute q, the definition of p.
Having provided the intuition behind the logical connectives, it is now easy to see how control flow graphs are represented in Ci%. For instance, the graph in Figure 1 is
represented as:

l

(I q5A $ means: 4 and $ must both execute along the

l

l

76) meana: execute in any way, provided that this will
not be a valid execution of 4. There are many uses
for this feature. One is that, just as in classical logic,
the negation lets us define deductive rules which, in
terms of the workflows, correspond to sub-workSow
definitions, Negation is also an important component
in temporal constraint specifications.

@#Jmeans: execute 4 in isolation, i.e., without interleaving with other concurrently running activities.
This
operator enables us to specify the transactional
parts of workflovf
specifications.

27

constrointe, formulas of the form ConcurrentHornGoaZ A
Conetraints
have an equivalent concurrent-Horn
form (
which, therefore, does not use the connective A). In practical terme, therefore, this means that there is au efficient
workflow scheduling strategy and, moreover, this strategy
can be determined at “design time” of the workflow (as oppoeed to run-time scheduling of [27]).

them directly into the control flow graph in appropriate
spots. The temporal constraints on workflow execution can
then be expressed in terms of these events. Without loss of
generality (as far as workBow modeling goes), we make the
following assumptions:

No significant event occurs twice during the execution
Indeed, we can always rename tierent
occurrences of
the same type of event.
Each significant event is represented as an elementary
update that applies in every state.
This assumption is appropriate since, typically, a
significant event amounts to nothing more than
forcing a suitable record into the system log.

Elcmcntory updates. We complete our informal introduction to Cr7E. by explaining how execution of (some) formulas
may actually change the underlying database state. Most
of the machinery has already been introduced (albeit very
informally).
What is missing is the notion of eZementov
updatea,

(2)

In Cm, elementary updates are represented by ordinary
ntomic, variable-free formulas. Syntactically,
C’TR does not
distinguish elementary updates in any way, but the user may
want to do 50 by adopting a syntactic convention (e.g., a convention could be that ine.p(t) represents the act of insertion
of tuple t into the relation p).
What distinguishes elementary updates is their semantics, Through some black magic, called transition oracle,
CJR arranges 50 that each elementary updates is always
true along certnin arcs, i.e., paths of the form (81,s~). Informally, one can think of an elementary update as a binary
relation over states. For instance, if (51,s2) belongs to the
relation corresponding to an elementary update u, it means
that u cnn cause a transition from state 31 to state 32. Note
thnt an update can be non-deterministic (any one of a number of alternative state transition5 might be possible) and
it is possible for an update to be inapplicable in certain
stntes but it is also possible for an update to apply in every
state), 5
This mechanism ie very general. It accounts for a wide
variety of elementary state changes: from simple tuple iusertione and deletions, to relational assignments, to updates
performed by legacy programs, to whatever workflow activities might do. The connectives of CTR. are then used
to build more complex updates from the elementary ones
nnd then to combine these complex updates into even more
compIex update programs. This process of building CTR
programs from the ground up is very natural and powerful.
The render is referred to [4, 5, 6] for concrete examples.
Now we can explain how the various workfiow activities (e.g., the symbols a, b, c, etc., in (1)) appear to C’TR.
Namely, each activity ie encoded as a variable-free atomic
formula, q, that represents either a sub-worHow defined by
a eet of concurrent-Horn
rules, or it can represent an ordinary activities, in which case IJ is an elementary update.
The latter is appropriate, since individua1 activities appear
to workflow management systems as “bIack boxes” that perform state changes in ways that are (at best) only partially
specified.

3

The first assumption

translates into the following unique
limits the kinds of concurrent-Horn
goals that we shall consider in this paper:

event property,

~7hich

Definition 9.1 (Unique Event Property). A concurrentHorn goal 9 has the unique event property if and only if
every significant event 0ccm-s at most once in any execution
of 9. In such cases, we shall also say that zi is a unique-event
0
goal.
Unique-event goals can be recognized in linear time in
the size of the goal, but we shall not present this algorithm
here. Instead, we mention some obvious, yet useful properties of such goals, 17&h
stices for our purposes. Let a be
a significant event. Then:
l

l

l

If G = & @ & is a unique-event goal and a occurs
in I& then it cannot occur in &.
IfG=&)i&isaunique-eventgoalandaoccursin
& then it cannot occur in &.
If G = El V & then G is a unique-event goal if and
only if so are both & and Ea.

(3)
In the rest of this paper, all concurrent-Horn goals are assumed to have the unique event property.
Transaction Logic can express a wide variety of temporal
constraints [S], but here we focus on a relatively simple alge
bra of constraints, which we denote by Co~srx.
CONST?Zis
as expressive as Singh’s Event Algebra [Z’]. Using these constraints we can specify that one task must start before some
other task, that the execution of one task causes some other
task to be executed or not executed, etc. These constraints
are believed to be suflicieut for the needs of workflow management systems, and they are far beyond of the capabilities
of the currently available commercial systems.
We specify all sign&ant
events in the system as propositions drawn from a set, denoted by EVEAV. In addition,
we introduce one special proposition, path, which is defined
as 4 V 14, for any CT’R formula. This means that path is
true on all possible execution paths.’

Events and Temporal Constraints

Definition
3.2 (Constraints).
The basic building blocks of
Co~sra are formulas of the form path 8 e 8 path, where
e E &VEAV. To save space, we shall use a shortcut for such
formulas: V4 E path@ 4 @path, by definition. Then the
followiug constraints form the constraint algebra COMST~:

In workflow systems, tasks are typically modeled in terms of
their significant, externally observable events, such as start,
commit, or abort. For the purpose of control flow, we can
represent these event5 as regular activities and incorporate
‘An cxnmplc of the first kind is an update that deletes p(t) only if
p(t) is trua in the current state. An example of the second update is
dclction of p(t) rogardlcas of whether p(t) is true. If p(t) is not true
In nomc stntc, 8, then no state transition takes place, but the update
will atill bc true over the arc (~,a).

‘This is one of the counterparts of “true” in classical logic. In
CT77, one can define other propositions that express various truths.
For instance, we can express the proposition stab*, which ie true
precisely on paths of length 1, Le., at states. It is also possible to
express formulas that are true precisely on arcs, etc.

28

Primitive constraints: If e E Evenrr then me (event
c must happen) and YVe (event e must not happen)
me primitive
constraints in Co~sra.
The constraint
Vc is n poaitiue primitive constraint. and -Ge is a negative primitive constraint.
.
.
Sorrel constramts: If 81, ... . Bn E COMSTU are positiuc primitive cor&aints,
then s18-e @s,, E COAUTR
is a aerial constraint. For convenience, primitive constroints are also viewed as serial constraints.

The previous results lead to the following normal form
for the members of CONST~L:

Complex

Corollary

constraints:

Since YTVe is equivalent to Ve, we only need to show that
s = -(Ve1 Q - - - 8 Ve,) is equivalent to some constraint in
CONSTU.
By Proposition 3.3, we can assume that n < 3. If n = 1,
then a = -Vel is a negative primitive constraint. If 12= 2,
then s = -(Vel @ Vez), which is equivalent (under the
assumptions (2)) to +Jel V yVe2 V (Vez 8 Vel).
0

If Cl, CZ E CONSTU then so
0

Nothing else is in CO,~ST~L.

To get u better grasp of the capabilities of COAYST~,here
are a few typical constraints and their real-world interpretation:

VeAVf
order);
1VCV7Vf
together.

-

Lemma 3.4 helps express certain constraints
easily. For instance:

it is not, possible for e and f to happen

l

if event, e occurs, then f must
-vc v (ve 8 Vf) occur Borne time later;

4

if event f has occurred, then
IVf v (ve al Vf) cvcnt e must have occurred some time prior to that;

Note fhat Definition 3.2 does not explicitly state that
Cu,~orn is closed under negation. Nevertheless, we can show
that it i5,

Serial Constraints).

Under the aaaumptions (2), any serial constraint is equivalent to a A-conjunction of aerial constraints, each composed
of no more than two primitive constraints.

and Scheduling

Problems for

Determine

whether g ia conaiatent with C.

Verification:

Determine whether every Zegal ezecution of
the workpow aatiajies some property G E CCWSTX.

Scheduling:

Find an execution path (or all paths) in 9
where C hoZds.

In C’JX, the consistency problem is tantamount to the existence of an execution for the formula 9 h C.
The verification problem is a special case of the consistency problem. Indeed, every legal execution of the workfiow
satisfies @ iff g AcA-di cannot execute (i.e., g is inconsistent
with CA d).
The verification problem also subsumes the redundancy
problem: ip E C is redundant iff every legal execution of
9 A (C - {a}) satisfies 9.
In this paper, we solve the verifkation problem constructively by transforming the fornxda 9 AC into an equivalent
concurrent-Horn
formula G’, which is always executable;
or if this is impossible, 9 A C reduces to -path - a nonexecutable transaction, which is the CTR analog of the classical false. Our algorithm is exponential in the size of C (in
the worst, case), which turns out to be inherent to the verlfication problem:

A serial constraint of the form Va @ VP is called an order
constraint; it says that cy and /3 must both occur and a must
occur before p, (Note that this is somewhat. stronger than
Klein’s order constraint mentioned earlier.)

Negation).

Than CO,VOTU haa a constraint
a0rtheaaaumptiona (2).

Consistency, Verification,
Workflows

Consistency:

Proof, Consider a positive serial constraint composed of
more than two primitive constraints: Vel @ ~e2 0 s, where
D is a serial constraint. We can show that this is equivalent
cl
to (VCl 8 Flea) A (vez c3 8).

Lemma 3.4 (Constraint

+Ve@Vf)
- it is not possible for f to occur after
e (and for e before f).

This section and the next assumes that the control flow
graphs do not have transition conditions on the arcs and
that the speciiication does not, contain concurrent-Horn rules
that define sub-workflows. In Section 7, we discuss how our
results apply to graphs that include these features.
Let. 9 be a concurrent-Horn goal with unique event prop
erty (Definition 3.1), which represents a control flow graph,
and let, C c Co~sra be a set of constraints.
The three
central problems in workflow management systems can be
formulated as follows:

IVCVVf
- if event, e occurs, then f must also occur
(before or after e). This is known as Klein’s existence
constraint [22].

3.3 (Splitting

much more

if e happens and then f does,
b +Ve 8 Of 0 Vg) the event. g cannot happen later.

- if both e and f occur, then
-6JeV+7fV(Ve@Vf)
c must come before f. This is known as Klein’s order
constraint [22].

Proposition

Form for Constraints).

Proof. Follows from Proposition 3.3, Lemma 3.4, and the
fact that, as in classical logic, V distributes through A and
cl
vice versa.

events e and f must both occur (in some

-

3.5 (Normal

Every constraint in CONS-R ia equivalent to a constraint of
where each serialConstr,~ is
the form Vi(AjseridCo~t~,j)
either a primitive constraint or a serial constraint composed
of two positive primitive constrainta.

nrc Cl ACa, and Cl V Ca.

Let C E COMSTU.
that is equivalent to 4 un-

Proof, WC can push negation down to the serial constraints
in (I using the classical De Morgan’s laws for A, V, and 7,
which arc valid al50 in C’j’%

29

graph. In contrast, the event scheduler of [27] has quadratic
complexity.
Thus, while expanding the effort on consistency checking
(which needs to be done anyway), we compile the original
specifications into a form that lets us find allowable schedules much more efficiently than with the passive approaches
of [27,3,19] (It should b e noted that, these latter algorithms
do not do consistency checks).

Proposition
4.1 (Complexity
of Verification).
Let 9
bc a concurrent goal and C C COJVSTU be a set of constraints.
Then determining whether g/\C is executable in CTR is NPcomplctc.

The NP-hardness proof is by reduction to satisfiabiity
of propositional logic [16]. That the decision problem is in
NP follows from the fact that given an arbitrary sequenceof
events the satisfiability of a set of constraints and a uniqueevent control flow graph is decidable in polynomial time.
h similar result has been previously obtained in [24]. Howcvcr, their NP-completeness result is based on aynchronizerconstraints, Each synchronizer corresponds to a combmation of an existence conatraintG and an order constraint6 in
our formalism. We tighten their complexity result by showing that synchronization per se is not a culprit: the problem
is NP-complete even in the presence of just the existence
constraints, In fact, it follows from our solution to the consistency problem that for order constraints the verification
problem can be solved in polynomial time.
The scheduling problem need5 more explanation. Workflow literature distinguishes two approaches to the problem:
paaaive and pro-active.
Poseivc schedulers receive sequences of events from an
external source, euch as a workflow or a transaction mannger, and validate that these sequences satisfy all global
constraints (possibly after reordering some events in the sequences). Several such schedulers are described in [26,3,19].
To validate a particular sequence of events, each of these
schedulers takes at least quadratic time in the number of
events.’ However, in passive scheduling environments, it
is left to an unspecified external system to do consistency
checking, to ensure the liveness of the scheduling strategy
and to select the event sequencesfor validation. The known
nlgorithms for these tasks are worst-case exponential.
In contrast to passive scheduling, our approach is proactive. In particular, we do not rely on any external system.
Instead, we construct a “compressed” explicit representation
of 011nllowcd executions (i.e., executions that are known to
satisfy all constraints). This representation can be used to
enumerate all allowed executions at linear time per execution path (linear in the size of the path). In this way, at each
stage in the execution of a workflow, the scheduler knows all
events that are eligible to start. There is no need to validate
constraints at run time, since the constraints are “compiled
into” the structure .
More precisely, our solution to the scheduling problem
capitalizes on the solution to the consistency problem. Fit,
we verify that the specifications are consistent by transforming g A C into an equivalent concurrent-Horn goal g’, a5 explnincd above. The formula si’ plays the role of the aforesaid
explicit representation for the set of all allowed execution5
of g A c.
If the transformation succeeds(i.e., the specifications are
consistent), enumerating all execution paths of 9’ takes time
linear in SI per path (note: linear in the original graph, not
in the much larger graph G’!). This means that after the
compilation, we can pick a legal schedule for workflow activities in time linear in the size of the original control flow

5

Compiling

Constraints

into the Control Flow Graph

We define the process of compiling the constraints in
Co~sra into unique-event goals by starting with simple
events and extending the transformation to more complex
ones. The unique-event property assumption is crucial for
the correctness of the results in this section.

Compiling primitive

constraints.

The following transformation takes a primitive constraint
of the form Va or +Ja and a control flow graph (expressed
as a concurrent unique-event goal) and returns a concurrentHorn goal whose execution5 are precisely those executions
of the original graph that satisfy the constraint. Intuitively,
this means that the contraint is compiled into the graph.
Definition 5.1 (Primitive
Let a,/3 E EweMr. Then:
Apply (Va, a)
Apply (Va, 0)
Apply (-Va, a)
APP~B (-Vat
P)

=

a

=
=
=

-path
-path
P

Constraint

Compilation).

ifa#P
ifa#P

Let T and K be concurrent-Horn goals and let cr stand for
Va or -rva. Then:

= (Apply(Va, T) @K) V

&ply(Va, T QDK)
Apply(+Ja,T @ K)

(T~APP~Y (Va, K))
= Apply(+a,T)
@Apply(+Ja,K)

Apply&% T I K)
fwly(+Ja,T
I K)

(Apply(Va, T) 1 K) V
= (T I APPLY(Va, K))
= Apply(yVa,T) ] Apply(TVa,K)

APP~Y(O,

0 T)

APP~Y(~,

T V K)

= @(APP~Y(~,T))
= APP~Y(U, T) V APPLY (0, K)

0

Observe that, due to the properties given in (3), the
above transformation preserves the unique-event property of
concurrent-Horn goals. For example, ifT is r@(avflvr&S,
then:
Apply(Va, T)
Awly(+Ja, T)

=
=

vOac36
r@(DVtl)cQ,s

Proposition
5.2 (Primitive
Constraint
Compilation).
If T is a concurrent-Horn goal and CTis a primitive constraint, then Apply(a, T) G T A u.

Compiling order constraints. Next we extend Apply
to work with order constraints, i.e., constraints of the form
Va 8 VP.

%iatcncc constraintsform the subset of COMST~L obtained from
prlmitivc conetrnints by combining them with A and v only.
‘Order constraints form the subset of Cotis~a
that does not use

Definition 5.3 (Order Compilation).
Let a,/3 E Eve~r
and let T be a concurrent-Horn goal. Then:

V,
‘TWO of tbcec schedulers arc actually exponential in the size of
the lnrgcst global constraint, but it is reasonable to assume that in
prnctlce thin sire is bound by a small constant.

Apply(Va 8 VP, 2’) =
s&a
< PJpply(Va, APP~Y(VP,T)))

30

If the result is not -path, this still does not mean that we
have a directly executable workflow speciflcation. The reason is that the send/receive synchronization primitives may
cause a “deadlock” . In model-theoretic terms, this means
that such a formula is C7Xequivalent
to -path, and in
proof-theoretic terms this means that the proof procedure
would halt and declare that no execution exists. In this case,
we rewrite $7~into -path.
Also, when the proof procedure declares a failure, it produces a concurrent Horn god, 9fai1, which in a sense is the
smallest subpart of the original workflow that is inconsistent
with the constraints. In this way, the workffow designers can
be given a feedback that might help them find the bug in
their specifications.
Even if the proof procedure of C77?. does find a proof
and thus 9c is an executable workflow specification, & may
have sub-formulas where the send/receive primitives cause
a cyclic wait, which we call knots.
The problem with lmots is that, when they exist, finding
an execution path in Bc may not be a linear task (despite
what we have promised in Section 4). Fortunately, it is easy
to show that a variant of the proof theory of C’J’R can be
used to remove all loots from !& in time linear in the size
of 9~. This procedure, which we call Excise, yields either a
knot-free concurrent-Horn goal equivalent to &, or -path,
if Gc is inconsistent.
We illustrate the Excise process with the fO~OWhg example.

The transformation
sync is designed to synchronize
events in the desired order. It is defined as follows:
sync(a < P,T)

= T’

where T’ is like T, except that every occurrence of event a
io replaced with a @Isend([) and every occurrence of event
/I is replaced with receive(t) @p, where E is a new constant.
The actions send and receive are easily expressed in C’TR
(see [O]) and their semantics is what one would expect of such
synchronization primitives: receive([) is true if and only if
send(t) has been previously executed. In this way, fi cannot
cl
atort before a is done.
It io easy to verify that, due to (3), the above transformation preserves the unique-event property of concurrent-Horn
gonlo, The following examples illustrate the transformation:
APPR+x
receive(t)

@ VP, 7 V (P Q a)) =
0 /3 0 a @ send(t)

Apply(Va

0 VP, a I P I PI I .. . I PA =

(4)

(a 0 .wnd(~)) I (receive(t) S/3) I PI I ... I pn
Proposition
5.4 (Order Compilation).
Let
concurrent-Horn goal and a,@ E Evewr. Then
Apply(Va 8 VP, T) E T A (Va 8 VP).

T

be

a

Compiling general constraints.

We are now ready to
extend Apply to handle the general constraints in Co~sra.

Example 5.7 (Knots).
fietthegraph(3ber@(nV(aI
:‘V] Vi vn$ let the constramts be as follows:
c2 S -7aTf~~~i
a
c2 3 -V/3 V (VP 0 Vq),
va). The constraint cl says, If a takes place, then fl must
also happen afterwards. The other constraints have similar
interpretation.
Omitting some intermediate steps, we have:

Definition 6.6 (Compiling
General Constraints).
Let
T be a concurrent-Horn
goal. We assume that workflows
are specified by a set of constraints C and each individual
conetraint is represented in the normal form of Corollary 3.5.
Therefore, C can be written as a single dependency of the

form

Apply(cr , 9) = Apply(TVa,

8) V Apply(Va

69 VP, 9)

=&VA,
61 Ad2

A...A6,

where 91 E 7 8 (a @Isend(&)
andGz=TYV

(5)

where ench 61 is in the normal form. In particular, all serial
constraints are assumed to have been split into simpler order
constraints. To extend Apply to such constraints, we only
need to define:
$i;[Z

:: 2:g

eApply(C1,
T) V Apply(C2,
= APP~Y(G, APP~Y(~z, T))

Apply(cz,~l

Ap;ply(c3,G3

T)

@/3

'.'92)=

Ba V !&a,

where & 3 y 8 (receive(&) @ a Q send(&) 1
receiue(&)
@ /3 @ send(&) 1receive(t)
@q
@send(&))

•I

V Excise(g2).
Finally, Excise(&
V A) = Excise(&)
The proof procedure of CTR finds no knots in G2, so
Excise(G2) = G2. On the other hand, it detects a knot
in g4 as follows.
First, the proof procedure %xecutes” 7 and deletes it
from 94. This results in a goal where each concurrent conjunct starts with a receive
and the corresponding send’s
are slated to occur only later in the execution.
Therefore, the proof procedure halts and we declare a knot in
= -path. Hence, Excise(Apply(cl
A
&. Thus Excise(&)

constraints C into the graph
be done. First, the result of
literals of the form ypath so,
concurrent-Horn goal. HowC7X tautologies to simplify

c2 AQ,~))=

E -path
ypath 0 4 E 4 @-path
-pathI 4 E I# I Tpath E Tpath
Ipath V 4 5 #V-path
E 9
The result would be either a concurrent-Horn

1receive(&)

App~y(c2,9l)VApply(c2,92)

where G2 3 y @ (a 8 send(&)
0send(b) 1receive(b) 0 tl)

Proposition
6.6 (Compiling
General Constraints).
Lot T be a concurrent-Horn goal and let 6 be a constraint
of the form (5). Then Apply(~!, T) E T A 6.
After compiling the
9, several things still need to
the compilation, gc, can have
strictly speaking, gc is not a
ever, we can use the following
9c:

8 P I ‘I)

=hVi72,

As before, it is easy to see that the above transformation
preserves the unique-event property.

)$nots.

VA)=

I receiue(b)

A.

Main results.

cl

We are now ready to summarize how the
APP~V and Excise transformations
heln solve the consist&y,
verification, and related problems.
Theorems 5.8
through 5.10 assume that every activity in the workflow (except for the receive primitive) always succeeds. Without this

goal or ypath.

31

assumption, only the “if”-part of Theorem 5.8 holds and its
corollaries, Theorems 5.9 and 5.10, must be adjusted accordingly,

contrast, using process algebras to model database state is
awkward and impractical.
Fourth, after verification, the proof theory of C~?Z can
schedule workflows at time linear in the size of the original
graph, but exponential in the size of the constraint set. In
contrast, process scheduling using the standard toolkit of
process algebras and temporal logic requires automata that
are exponential in the size of the original graph.

Thcoccm 5.8 (Consistency Checking). Given a worldflow specification 9 A C, it ia inconsistent iff
Exciao(Apply(C, 8)) = -path.
Proof. Follows from Proposition 5.6 and the soundness and
0
completeness of C’T7t proof theory.

Proof, 9 is satisfied by every execution of the worlcfIo\v if
and only if Excise Apply(~@ A C,B)) = lpath. Other-

Workflow modeling.
We have already discussed formalisms
used for passive workflow scheduling [26,27,3,19]. In addition, general purpose process specification formalisms such
as Petri-nets, state charts [29], temporal logic [14] or process
algebras [23] can also be used for modeling workflows. However, we believe that our results show that CTR provides a
much simpler and uniform way to both describe and reason
about workflows.

wise, Exciao(Apply t 1% AC, 9)) rewrites to the most genert
counter example where !E fails to hold.

7

Theorem

5.9 (Property

Verification).

Given a
workflow apecijication f2 A C and a property 9 E COJJSTZ,
there is a constructive way of verifying whether every execution of the workflow aatisjies Q;.

5.10 (Redundancy
Elimination).
Given a
workflow specification Q AC and a constraint @ E C, we can
uerifv whether @ is redundant.

Theorem

Thcorcm

We presented a logic-based formalism for specifying, verifying, and esecuting workflows. We developed an algorithm

for consistency checking of workflows and for their property
verification. The algorithm compiles global constraints on
workflow execution into the control flow graph. In addition
to solving the consistency and ve-riflcation problems, this
compilation technique helps optimize the run-time scheduling of workflow events.
Our work can be extended to handle additional features,
such as the following:

6.11 (Complexity).

Let 9 be a control flow
graph g and C C CO,VSTU be a set of global constraints in
the normal form of Corollary 3.5. Let 191 denote the size of
9, N bc the number of constrainta in C, and d be the largest
number of diajuncta in a constraint in C. Then
l
l

The worst-case

size

of Apply(C,g)

is O(dN x 191).

T}te worst-case time complexity of applying Excise
proportional to the size of Apply(C, 9).

Conclusion

is

!I’ransition conditions. As a compilation technique, our algorithm cannot lily account for control flow graphs
with transition conditions that query the database.
For such graphs, our algorithm is sound but not complete for consistency and verification problems. However, if additional semantic information about the data
base is available, it can be incorporated into our framework to yield more accurate results.

A simple corollary of Theorem 5.11 is: If C consists of serial
constraints only, then d = 1 and the size of Apply(C,G) is
proportional to Igl.
G Related Formalisms

It, is straightforward to augment our compilation technique to handle sub-workflows defined via
concurrent-Horn rules. Furthermore, when global dependencies do not span sub-workflow boundaries, the
complexity reported in Theorem 5.11 can be
reduced. Indeed, it can be shown that, if M is the
largest number of dependencies in a sub-workflow,
then the size of Apply(C, 9) is O(d” x ISI).

Sub-workflows.

Workflow verification. Process algebras / temporal logic
suites have been used for modeling concurrent systems (akin
to workflows) for over a decade now, and model checking is
a pretty standard mechanism for verifying such systems.
However, the salient benefits of using CTR over process
algebras and related formalisms are very tangible. Fit,
CT’,%is one uniform formalism in which workflows can be
specified, verified and scheduled. This should be contrasted
with the use of the algebras and temporal logic for specifying workflows, model-checking for their verification, and
automata for scheduling.
Second, the use of CT72 has enabled us to tid more efficient verification algorithms. Indeed, standard model checking techniques [9] used for verification are worst-case exponential in the size of the control flow graph. This is often
referred to as the atate-explosionproblem.
In contrast, Apply
is linear in the size of the graph - it is exponential only
in the size of the constraint set (Theorem 5.11), which is a
much smaller object. In a sense,Apply (aIong with the proof
theory of C’7X) can be viewed as specialized, more efficient
model checker for the problem at hand.
Third, C7-R integrates “process oriented” and “data oriented” features, which makes it easy to model processesthat
perform complex transformations over the database. In fact,
extending our techniques to workflows that query the underlying database state is the next logical step for our work. In

Loops and iteration. Loops in control flow graph can be ex-

pressed using recursive CTR. rules. Our techniques assumes the unique-event property for worktlow graphs.
Hence this property has to be relaxed to handle workflows with loops.
Failure semantics. Failure atomicity is built into CTR se-

mantics. However, more complex workflows require
more advanced failure semantics, such as compensation [15]. Some such semantics can be expressed using the possibility operator of CT’& 0. Work is in
progress on extending our framework to handle other
failure semantics.

Acknowledgements.
The authors would like to thank
Tony Banner for the helpm comments on a draft of this
Pap-.
32

__-.----_ _._

[15] H. Garcia-Molina and K. Salem. Sagas. In Intl. Conference on Very Large Data Bases, pages 249-259, May
1987.

Rofcrcncee

[l] G. Alonso, D. Agrawal, A. El Abbadi, M. Kamath,
R, Giinthar, and-C. Mohan. Advanced transaction
models in workflow contexts. In IntemationaZ Confcr~ncc on Data Engineering, New Orleans, Louisiana,
February 1996.

[16] M.R. Garey and D.S. Johnson.

Computers and
Intractability:
A Guide to the Theory of NPCompleteness. Freeman and Company, San Francisco,
CA, 1978.

PI G Alonso, D. Agrawal, A. El Abbadi, and C. Mohan.
Functionality and limitations of current workflow manngement systems. In IEEE-Bxpert (to appear in 4 3peda1 iaaue on Cooperative Information
Systems), 1997.

[17] D. Georgakopoulos, M. Hornick, P. Krychniak,

and
F. Manola. Specification and management of extended
transactions in a programmable transaction environment. In International Conference on Data Engineering, Houston, TX, February 1994.

PI P, Attic,

M. Singh, A. Sheth, and M. Rusinkiewicz.
Specifying and enforcing intertask dependencies. In
Intl, Conference on Very Large Data Bases, 1993.

[18] D. Georgakopoulos, M. Hornick, and A. Sheth. An
overview of workflow management: From process modeling to infrastructure for automation. JournaZ on Distributed and Parallel Database Systems, 3(2):119-153,
April 1995.

PI A,J,

Bonner and M. Kifer. An overview of transaction
logic, !%eoretical Computer Science, 133:205-265, October 1994.

PI A,J.

Bonner and M. Kifer. Transaction logic programming (or a logic of declarative and procedural
knowledge). Technical Report CSRI-323, University of
Toronto, November 1995. Unpublished manuscript.

[19] R. Gunthor. Extended transaction processing based
on dependency rules. In Proceedings of the RIDE&KS
Workshop, 1993.
[26] M. Hsu, Ed. Special issue on workflow systems. Bulletin

PI A,J.

Bonncr and M. Kifer. Concurrency and communication in transaction logic. In Joint Intl. Conference
and Symposium on Logic Programming, pages 142-156,
Bonn, Germany, September 1996. MIT Press.

of the Technical Committee on Data Engineering
Computer Society), 18(l), March 1995.

[21] M. Kiier. Transaction logic for the busy workflow professional. Unpublished manuscript, August 1996. -

Bonner, M. Kifer, and M. Consens. Database programming in transaction logic. In A. Ohori C. Beeri and
D.E, Shasha, editors, Proceeding3 of the International
Workshop on Database Programming Languages, Workshops in Computing, pages 399-337. Springer-Verlag,
February 1994. Workshop held on Aug 30-Sept 1,1993,
New York City, NY.

[71 A.J,

PI

P21J. Klein. Advanced rule-driven transaction management. In IEEE COMPCON.

[23l R. Milner.

Databases-An

International

[ill

U, Dayal, M. Hsu, and R. Ladin. Organizing longrunning activities with triggers and transactions. In
ACM SIGMOD
1990.

PI

[25l M. Rusinkiewics and A. Sheth. Specification and exe-

Edmund M. Clarke, E. Allen Emerson, and A. Prasad
Sistla, Automatic verification of finite-state concurrent
systems using temporal logic specifications. In ACM
!!%anaactionaon Programming Language3 and Systems
(TOPLAS), pages 244-263, 1986.
Workflow Management Coalition. Terminology and
glossary, Technical Report (WFMC-TC-loll),
Workflow Management Coalition, Brussels, 1996.

Conference on Management

cution of transactional workflows. In W. Kim, editor,
In Modern Database Systems: The Object Model, Interoperability, and Beyond. ACM Press, 1994.

WI

of Data,

@I H. Wachter and A. Reuter. The Con-act
[13], chapter 7, pages 229-263. 1992.

Litwin,
A,
Elmagarmid,
Y.
Leu,
W.
and M, Rusinkiewcz. A multi - database transaction
model for interbase. In Intl. Conference on Very Large

WI

A,K,
1131

Elmagarmid, editor. Database Transaction Models for Advanced Applications. Morgan-Kaufmann, San
Mnteo, CA, 1992.

1

M.P. Singh. Semantical considerations on workflows:
An algebra for intertask dependencies. In Proceedingsof
the International
Workshop on Database Programming
Language3, Gubbio, Umbria, Italy, September 6-8 1995.

[271 M.P. Singh. Synthesizing distributed constrained events
from transactional workSow specifications. In Proceedings of 1%th IEEE Intt. Conference on Data Engineering, pages 616-623, New Orleans, LA, February 1996.

Data Bases, 1990.

D41

Prentice

Verification problems in conceptual workflow speciScations. In Intl. Conference on Conceptual Modelling, volume 1157 of Lecture Note3 in Computer Science, Cottbus, Germany, 1996. Springer-Verlag.

Journal,

PO1

and Concurrency.

[24] M.E. Orlowska, J. Rajapakse, and A.H.M. ter Hofstede.

3(2), April 1995.
PI

Communication

IEEE, 1991.

Hall, 1989.

0. Bukbres and E. Kueshn, Eds. Special issue on software support for work flow management. Distributed
and Parallel

(IEEE

E,A. Emerson. Temporal and modal logic. In Handbook
of Theoretical Computer Science, pages 997-1672,1996.
33

model. In

Diik Wodtke and Gerhard Weikum. A formal foundation for distributed workflow execution based on state
charts. In Intl. Conference on Database Theory, pages
230-246, 1997.

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

Story Detection Using Generalized Concepts and
Relations
∗ School

Betul Ceran∗ , Nitesh Kedia∗ , Steven R. Corman† and Hasan Davulcu∗
of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287-8809
Email: {betul, nitesh.kedia, hdavulcu}@asu.edu
† Hugh Downs School of Human Communication, Arizona State University, Tempe, AZ 85287-1205
Email: steve.corman@asu.edu

victims) as different objects, potentially missing common
narrative patterns. We address this problem by discovering
“contextual synonyms” [1] which are verb and noun phrases
that occur in similar contexts. After revealing contextual
similarity, we generalize such references to a common node
in a semantic network.
We developed an unsupervised and domain-independent
framework which extracts high-level information from text
as relationships and concepts forming a semantic network.
It first uses a semantic role labeler to obtain ground facts as
semantic triplets from text, and then proceeds to generalize
them through a bottom-up agglomerative clustering algorithm. Semantic role labeling, i.e. shallow semantic parsing,
is a task in natural language processing which recognizes
the predicate or verb phrases in a sentence along with its
semantic arguments and classifies them into their specific
roles as subjects and objects. For example, we would like
to merge extracted triplets such as hmujahidin→kill→kafiri
and hISIS →demolish→shrinesi into high level generalized
concepts and relations, such as h{ISIS, mujahidin}→{kill,
demolish}→{kafir, shrines}i by discovering contextual synonyms such as {ISIS, mujahidin}, {kafir, shrines} and {kill,
demolish}. Note that contextual synonyms are not synonyms
in the traditional dictionary sense, but they are phrases that
may occur in similar semantic roles and associated with
similar contexts.

Abstract—A major challenge in automated text analysis is that
different words are used for related concepts. Analyzing text
at the surface level would treat related concepts (i.e. actors,
actions, targets, and victims) as different objects, potentially
missing common narrative patterns. Shallow parsers reveal
semantic roles of words leading to subject-verb-object triplets.
We developed a novel algorithm to extract information from
triplets by clustering them into generalized concepts by utilizing syntactic criteria based on common contexts and semantic
corpus-based statistical criteria based on “contextual synonyms”. We show that generalized concepts representation of
text (1) overcomes surface level differences (which arise when
different keywords are used for related concepts) without drift,
(2) leads to a higher-level semantic network representation of
related stories, and (3) when used as features, they yield a
significant 36% boost in performance for the story detection
task.

1. Introduction
Extremist groups use stories to frame contemporary
events and persuade audiences to adopt their extremist ideology. Foreign policy of most countries in the twenty first
century is centrally influenced by perceptions, attitudes and
beliefs of societies. Therefore, it’s of critical importance
to fully understand the means by which extremist groups
leverage cultural narratives in support of their ideological
agenda. Understanding the structure of extremist discourse
will provide better intelligence on what kinds of narrative
and persuasive appeals they are making, allowing both better
detection of trends and better knowledge of which themes
might best be countered and how this might be accomplished. The research presented in this paper mainly focuses
on extracting high-level relations and concepts which are
then utilized for detecting stories and themes embedded in
longer messages of extremist discourse.
A major challenge facing automated discourse analysis
is that word usage can differ between two texts even though
they are talking about the same thing. For example, violent
extremists may use words such as “brothers”, “mujahidin”,
“mujahedeen” and even “lions of Islam” to refer to the
same group of people. Analyzing text at the surface level
would treat related concepts (i.e. actors, actions, targets, and

ASONAM '15, August 25-28, 2015, Paris, France
© 2015 ACM. ISBN 978-1-4503-3854-7/15/08 $15.00
DOI: http://dx.doi.org/10.1145/2808797.2809312

mujahidin → kill → kafir


{ISIS, mujahidin}




↓
and
{kill, demolish}


↓


ISIS → demolish → shrines
{kafir, shrines}
Triplets extracted with semantic role labeling are noisy
and sparse. We develop a hierarchical bottom-up merging
algorithm that generalizes triplets into meaningful high level
relationships. We achieve this by employing syntactic and
semantic corpus-based criteria. Syntactic criteria are developed to merge a pair of subjects-verbs-objects only if they
share common context related to their different arguments
(i.e. a pair of different subjects are merged only if they cooccur with an identical verbs-objects context). The details
of the syntactic criteria are presented in Section 5.1. Furthermore, a corpus-based semantic criterion is developed

942

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

story detection task. We aim to to develop a story classifier
that can discriminate between stories and non-stories. A
story is defined as an actor(s) taking action(s) that culminates in a resolution(s), e.g. “Mujahedeen Imarah Islam
Afghanistan attacked a military base in Hisarak district
of Nangarhar province with heavy weapons on Tuesday.
Reports indicate about 22 mortars landed on the base and
causing a fatal loss enemy side.” A non-story paragraph is
one in which there is no explicit resolution but hypothetical
ones, e.g. “Praised be God. We praise Him and seek His
help and forgiveness. God save us from our evils and bad
deeds. Whoever is guided by God, no one can mislead him,
and whoever deviates no one can guide him.”
We use a corpus of 39, 642 paragraphs where 9, 058
paragraphs coded as stories, and 30, 584 paragraphs coded
as non-stories by domain experts. We experiment with (i)
standard keyword-based features, (ii) triplet-based features
which generate sets of subjects and sets of objects associated
with distinct verbs as features [4], and (iii) generalized
concept/relation based features developed in this paper. Previously in [4], we obtained a precision of 73%, recall of
56% and F-measure of 63% for the detection of minority
class (i.e. stories) by using triplet-based features, which
provided a 161% boost in recall, and an overall 90% boost
in F-measure over keyword-based features. In this paper,
we show that when we utilize generalized concepts/relations
extracted from the entire corpus of stories and non-stories as
features, we obtain new highs in story detection accuracies
as 86% precision, 82% recall and 85% F-measure. Generalized concepts/relations as features yield a 50% boost
in recall at higher precision, and an overall 36% boost
in story detection accuracy over verb-based triplet features
developed earlier.
The contributions of this paper are: (i) a generalized concept/relationship representation of text that overcomes surface level differences (which arise when different keywords
are used for related concepts) without drift (ii) a higher-level
semantic network representation of related stories, and (iii)
a 36% boost in the challenging automated story detection
[5] task.

Figure 1. A sample semantic network learned from stories

for subjects, verbs and objects based on their shared verbobject, subject-object and subject-verb contexts correspondingly. The details of the semantic criterion are presented in
Section 5.2. A hierarchical bottom-up merging algorithm,
similar to the one employed in [2], allows information to
propagate between clusters of relations and clusters of objects and subjects as they are created. Each cluster represents
a high-level relation or concept. A concept cluster can be
viewed as a node in a graph, and a relation cluster can be
viewed as a link between the concept clusters that it relates.
Our proposed algorithm utilizes both syntactic and semantic corpus-based merging criteria. A pair of hSubject,
Verb, Objecti triplets is merged only if (i) they share a
common context among their corresponding terms (i.e. syntactic criteria) and (ii) they satisfy corpus-based support
and similarity measure thresholds (i.e. semantic criteria).
A corpus-based measure of “contextual synonymy” will be
defined based on their shared contexts of subjects, verbs and
objects. We observed that this combination of criteria helps
to generalize triplets into meaningful high-level concepts
without drift. For example, Table 1 shows top ten contextual
synonyms identified for three keywords selected from our
extremist discourse corpus.
Generalized concept and relation clusters define a semantic network [3]. Collections of co-related contextual
synonyms can be used to construct meta-nodes and links
in a network describing the semantic space of the underlying texts. Components of the graph reveal networks
of generalized concepts expressed as different groups of
actors (subjects) performing various sets of actions (verbs)
on different groups of targets/victims (objects). A sample
network extracted from stories that mention Afghanistan and
Iraq is shown in Figure 1. This technique contributes to the
detection of narratives used by extremist groups to convey
their ideology.
We evaluate the utility of generalized concepts by comparing their predictive accuracy when used as features in a

2. Related Work
Our paper has contributions in two distinct areas; unsupervised relation extraction and story detection. We present
related work in these two areas separately.

2.1. Unsupervised Relation Extraction
Unsupervised learning of concepts and relations has become very popular in the last decade. One of the pioneering
studies in the field, by Hasegawa et al. [6], clusters pairs of
named entities according to the similarity of context words
(predicates) in between. Each cluster represents a relation,
and a pair of objects can appear in at most one cluster
(relation). Our framework does not depend on the use of
a Named Entity Recognition (NER) system and it allows
subject and objects to appear in more than one relation.

943

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

Bank et al. [7] build soft clusters of named entities however
their system require an external knowledge-base/ontology of
relations to operate.
Kok and Domingos presented a similar framework to
ours in their 2008 paper [2], which extracts concepts and
relations together from ground facts also learning a semantic
network. They use a purely statistical model based on second
order Markov Logic and report performance in comparison
with other clustering algorithms based on a manually created
gold-standard. Our evaluation strategy compares the efficacy
of concepts/relations as features with other feature sets on
story detection task.
Recently, the focus of unsupervised information extraction has moved on to large web data sets creating the need
for more scalable approaches. Kang et al. [1] deals with
this problem using a parallel version of tensor decomposition. They learn contextual synonyms and generalized concepts/relations simultaneously, however they do not present
any formal evaluation of their concepts/relations.
Another problem of dealing with web-scale discovery
is polysemy and synonymy of verbs. Polysemy becomes
a problem when the two occurrences of the same word
which have different meanings are placed into the same
cluster. Min et al. [8] addresses this problem by incorporating various semantic resources such as hyponymy relations,
coordination patterns, and HTML structures. They observe
that the best performance is achieved when various resources
are combined together. We address word sense disambiguation by incorporating features from words’ context. The
contextual information flow via alternating merging of nouns
and verbs handles the problems due to polysemy.

2)

3)

4)

5)
6)

7)
8)

Paragraphs are loaded into a SRL component. First,
we apply co-reference resolution. Then, we use a
shallow NLP parser and a post-processing step on
the parse-tree in order to obtain the semantic role
labels for hSubject, V erb, Objecti triplets found in
sentences. (See section 3.1).
Using the triplets, we create three separate pairwise
contextual similarity matrices for subjects, verbs
and objects based on their co-occurrences with
verb-object, subject-object, and subject-verb pairs
respectively. (See section 4).
Triplets and contextual similarity matrices are used
as inputs to our clustering engine, which selectively
merges and grows combined clusters of related
subjects, verbs and objects. (See section 5).
Step 4 yields a number of concept (i.e. subject/object) clusters linked by relation (i.e. verb)
clusters. (See section 5).
We further experiment with expanding these concepts and generalized relations with word-sense disambiguated dictionary look-ups in WordNet [12].
(See section 6.2).
We further expand these concepts and relations
with word-sense disambiguated dictionary lookups. (See section 6.2).
Both original and expanded concepts/relations are
tested as features for the story/non-story classification task using ten-fold cross validation. (See
section 6.4).

3.1. Semantic Role Labeler
2.2. Story Detection
We study the problem of predicting whether or not a
given paragraph tells a story. A story can be defined as
“a sequence of events leading to a resolution or projected
resolution”. We perform supervised learning using a training
set of stories and non-stories annotated by domain experts.
Gordon et al. has published related work about story detection in conversational speech [9] and weblogs [10]. They
use a confidence-weighted linear classifier with a variety of
lexical features to classify weblog posts in the ICWSM 2009
Spinn3r Dataset and obtained the best performance [11]
using unigram features with precision 66%, recall = 48%,
F-score = 55%.

We use the Stanford Deterministic Coreference Resolution System [13], [14], [15], [16] as a pre-processing step.
Next, each paragraph is processed by ClearNLP shallow
parser [17], which assigns a semantic role label to each
word in a sentence. There are more than 40 possible labels1
provided by ClearNLP. Currently, we are only interested
in subjects, predicates and objects. In the final step, we
apply post-processing on the output of ClearNLP to handle
complex sentences with multiple verbs or some considerations for active or passive voice in order to extract related
subject-verb-object triplets expressed in the sentence. Our
framework can be adapted for languages other than English,
provided that a semantic role labeler exists for that language.

3. System Architecture

4. Contextual Synonyms

The main components of our system architecture are
shown in Figure 2. The numbers on the top left corner of
each box represent the order in which these processes are
executed. Each process is briefly described below, while the
details are presented in following sections.

We observe that a meaningful measure of pairwise similarity for subjects, verbs and objects can be obtained based
on their shared verb-object, subject-object and subject-verb
contexts, respectively. Therefore, we adapted the standard
bag-of-words approach [18] to be used with triplets rather
than regular text. For example, the similarity between a

1)

Paragraphs in our dataset are annotated by human
experts as Story and Non-Story. We treat each
paragraph as a single data item to be classified.

1

944

https://github.com/clir/clearnlp-guidelines/blob/master/md/dependency/
dependency guidelines.md

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

1:
2:
3:
4:

5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

F IND CONCEPTS W / UNIQUE PAIRS(T , C 0 )
X , Y, Z ← ∅
for all hsi , vj , ok i ∈ T do
Find and add unique pairs to:
X ← X ∪ {hsi , vj i}
Y ← Y ∪ {hvj , ok i}
Z ← Z ∪ {hsi , ok i}
end for
for all hsi , vj i ∈ X and hsi , vj , ok i ∈ T do
C 0 ← C 0 ∪ {hsi , vj , Oi} where ok ∈ O.
end for
for all hvj , ok i ∈ Y and hsi , vj , ok i ∈ T do
C 0 ← C 0 ∪ {hS, vj , ok i} where si ∈ S .
end for
for all hsi , ok i ∈ Z and hsi , vj , ok i ∈ T do
C 0 ← C 0 ∪ {hsi , V, ok i} where vj ∈ V .
end for
end
Figure 3. Algorithm: Find concepts with unique pairs

Let S, V and O be the set of all unique subjects, verbs
and objects in our data set, respectively. And let T be the
set of all hs, v, oi extracted triplets from our corpus, where
s ∈ S , v ∈ V , o ∈ O denote a single subject, verb
and object respectively. We calculate pairwise contextual
similarity matrices (SS ) for subjects, (SV ) for verbs and
(SO ) for objects using the algorithms described in Figures 3
and 4. Throughout the rest of this paper, we refer to both
noun and verb clusters as ‘concepts’ for simplicity.
Initially, we create concepts comprised of distinct pairs
of subjects, verbs or objects with common context. We will
name this initial set of concepts C 0 in order to avoid confusion with the resulting set of concepts. Set C 0 is composed of
concepts c, each of which has a set of subjects (S ), verbs (V )
and objects (O), which co-occur with unique hverb, objecti,
hsubject,objecti and hsubject,verbi pairs respectively. The
pseudo-code given in Figure 3 describes this procedure.
In the first for-loop (lines 3–5, we iterate over all the
hs, v, oi triplets and create a list of unique hsubject,verbi,
hverb,objecti and hsubject,objecti pairs. In the subsequent
three for-loops, we grow our concept set at each iteration
by adding a unique pair along with a set of all co-occurring
words. Lines 6–8 perform this operation for hsubject,verbi
pairs, lines 9–11 for hverb,objecti pairs and lines 12–14 for
hsubject,objecti pairs.
After producing concepts with unique pairs, we proceed
to calculate pairwise contextual similarity for subjects, verbs
and objects. Let ns = |S|, nv = |V| and no = |O|
be the number of all unique subjects, verbs and objects
in our corpus, respectively. We create similarity matrices
SS ∈ Rns ×ns for subjects, SV ∈ Rnv ×nv for verbs, and
SO ∈ Rno ×no for objects. The algorithm in Figure 4 is
used to fill in the similarity matrices. The similarity between
a pair of words is defined as the number of common cooccurring unique contexts, i.e. if any of the two subjects,
verbs or objects appear with the same verb-object, subject-

Figure 2. System Architecture
TABLE 1. T OP TEN CONTEXTUAL SYNONYMS FOR mujahedeen, attack
AND base
mujahedeen

attack

base

mujahidin

storm

area

group

hit

house

soldier

seize

area

force

loot

home

lion

raid

station

hero

shoot

center

fighter

ambush

checkpoint

mujahid

assassinate

headquarters

brigade

bomb

land

mujahedeen

capture

location

detachment

disrupt

region

pair of subjects is determined by the frequency of their cooccurrences with the same verb-object pairs. In our preliminary experiments, we applied various clustering algorithms
comparing different similarity measures such as euclidean
and cosine however the contextual similarity measure defined in Figure 4 provides the most meaningful results. For
example, in Table 1, lion is indeed among the most similar
words for mujahedeen based on the contextual similarity
measure, whereas none of the other standard similarity
measures are able to retrieve this keyword.

945

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

C ALCULATE CONTEXTUAL SIMILARITY(C 0 )
SS , SV , SO ← 0
for all c ∈ C 0 do
if c = hS, v, oi then
SS (i, j) ← SS (i, j) + 1, ∀si , sj ∈ S .
else if c = hs, V, oi then
SV (i, j) ← SV (i, j) + 1, ∀vi , vj ∈ V .
else if c = hs, v, Oi then
SO (i, j) ← SO (i, j) + 1, ∀oi , oj ∈ O.
end if
end for
end

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

Figure 4. Algorithm: Calculate contextual similarity

object or subject-verb pair respectively, then we increase
the similarity count between those two words by one. In
Figure 4, lines 4–5 calculate pairwise similarities between
subjects, lines 6–7 for verbs and lines 8–9 for objects.

C LUSTER C ONCEPTS(T , SS , SV , SO , C 0 )
C ← C0
while f lag = 1 do
f lag ← 0
for all c ∈ C 0 do
Find matching concepts M using Syntactic Criteria
if |M| ≥ 1 then
f lag ← 1
for all m ∈ M do
{c} ← {c} ∪ {m}
Prune c using Semantic Criteria.
C ← C ∪ {c}
end for
end if
end for
end while
end
Figure 5. Bottom-Up Agglomerative Clustering Algorithm

to c2 ’s subject set, we also require that the intersection of
c1 and c2 ’s verb and object sets, {v1 } and {o1 }, should be
non-empty as well. Since these conditions are satisfied in
this case, we can merge c1 and c2 into the same concept
provided they satisfy the semantic criteria discussed in the
next section.
On the other hand, let us consider c1 = h{s1 , s2 }, v1 , o1 i
and c2 = hs1 , {v1 , v2 }, o2 i. If we merge these concepts,
the new concept will be c3 = h{s1 , s2 }, {v1 , v2 }, {o1 , o2 }i.
Since c3 adds a new object, o2 , to c1 , we require that the
intersection of c1 and c2 ’s subject and verb sets, {s1 } and
{v1 }, should be non-empty, which is the case. c3 would
also add a new verb, v2 , to c1 , hence we require that the
intersection of c1 and c2 ’s subject and object sets should
be non-empty as well, which is not the case. There is a
common subject but objects are totally distinct. Therefore
we should not merge these concepts into the same one since
there is not enough common context to justify the merged
concept. We express these conditions in a more formal way,
as follows.
Let C1 = hS1 , V1 , O1 i and C2 = hS2 , V2 , O2 i be two
concepts. We merge C1 and C2 if they meet all of the
following conditions:

5. Concept and Relation Clustering
We follow a bottom-up agglomerative merging approach
in order to populate our noun and verb clusters. The pseudocode for the algorithm is as shown in Figure 5. We start
with the initial concept set, C 0 , that we created in Figure 3
and iteratively expand each element. First, each element
of C 0 is compared with the rest in order to create a set
of candidates for merging based on the syntactic criteria
(lines 5–6) described in Section 6.1. Next, we process each
candidate and eliminate the words which fail the semantic
criteria (lines 9–10) described in Section 6.2. We grow our
candidate concepts by adding the elements which pass both
tests (line 12). The main while-loop, beginning at line 3,
continues to iterate until there are no more candidates suitable for merging. We explain the details of these syntactic
and semantic criteria in the following two sections.

5.1. Syntactic Criteria
One of the major challenges in obtaining information
via generalization is to maintain meaningful concepts as
they grow. We address this problem by merging concepts
only if they have a common context in all three semantic arguments (i.e. subject, verb, object). Given a generalized concept, h{s1 , s2 , ...}, {v1 , v2 , ...}, {o1 , o2 , ...}i ∈ C ,
we maintain that all subjects (si ), verbs (vj ) and objects
(ok ) are “ contextually synonymous” among themselves and
can be used interchangeably to generate meaningful triplets.
Let c1 = h{s1 , s2 }, v1 , o1 i and c2 = hs1 , v1 , {o1 , o2 }i be
two concepts with unique pairs, i.e. c1 , c2 ∈ C 0 . Consider
merging these concepts into a more generalized concept
c3 = h{s1 , s2 }, v1 , {o1 , o2 }i. Since c3 adds a new object,
o2 , to c1 , we require that c1 and c2 have a common context
in order to justify the merge, i.e. the intersection of c1 and
c2 ’s subject and verb sets, {s1 } and {v1 }, should be nonempty. Similarly, since we are adding a new subject, s2 ,

•
•
•

S1 6= S2 ⇒ {V1 ∩ V2 6= ∅ and O1 ∩ O2 =
6 ∅}
V1 6= V2 ⇒ {S1 ∩ S2 6= ∅ and O1 ∩ O2 =
6 ∅}
O1 6= O2 ⇒ {S1 ∩ S2 6= ∅ and V1 ∩ V2 =
6 ∅}

5.2. Semantic Criteria
While the syntactic criteria ensure inter-relatedness of
distinct members of concepts to their contexts, we also
utilize a secondary measure to establish intra-relatedness
between the distinct members of concepts in each argument
position. We utilize the contextual similarity measure (defined in Figure 4) that relates subjects, verbs, and objects
among themselves. The semantic test requires that only the

946

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

6.3. Feature Matrix Generation

most similar candidate keywords can be added to a concept.
We use these criteria to grow the concepts without drift. We
formally present semantic criteria as follows.
Let C1 = hS1 , V1 , O1 i and C2 = hS2 , V2 , O2 i be two
concepts which passes the syntactic criteria and let C3 be
the new concept after merging. Semantic criteria are applied
as follows:
•
•

•

•

We report the results of story detection task using five
different feature sets: (i) keywords, (ii) verb-based features
extracted from triplets [4], (iii) concepts-based features
(Tier 1) developed in this paper, (iv) concepts expanded
with contextual similarity index (Tier 1 + Similarity) and
(v) concepts expanded with WordNet (Tier 1 + WordNet).

Define Sint = S1 ∩S2 , Vint = V1 ∩V2 , Oint = O1 ∩O2 .
Define
Sdiff = (S1 \ S2 ) ∪ (S2 \ S1 )
Vdiff = (V1 \ V2 ) ∪ (V2 \ V1 )
Odiff = (O1 \ O2 ) ∪ (O2 \ O1 )

6.3.1. Verb-based Features. In our previous paper [4], we
followed a standard verb-based approach to extract simple
subject, object and preposition clauses associated with verbs
found in story and non-story paragraphs. For each verb (V)
mentioned in a story (S), and non-story (NS), we generated
following set-valued features by using the training data:

∗
∗
∗
Define Sint
, Vint
and Oint
to be the sets composed
of the closest contextual synonyms of all words in
Sint , Vint and Oint , respectively. In this step, we use
the contextual similarity metric from the algorithm
presented in Figure 4.
Initially, C3 contains only the intersections of C1 and
C2 , i.e. C3 = hSint , Vint , Oint i . We grow C3 by adding
words from the difference sets of C1 and C2 only if
they are among the closest contextual synonyms of
the words in the intersections. Formally,

•
•

Argument list for S.V.Subjects, S.V.Objects,
S.V.Prepositions for each verb V and story S.
Argument list for NS.V.Subjects, NS.V.Objects,
NS.V.Prepositions for each verb V and non-story
NS.

For each test paragraph P, for each verb V in P, we
extracted its typed argument lists P.V.Subjects, P.V.Objects
and P.V.Prepositions. Then, we matched them to the argument lists of the same verb V. A match succeeds if the
overlap between a feature’s argument list (e.g. S.V.Subjects,
or NS.V.Subjects) covers the majority of the test paragraph’s
corresponding verb argument list (e.g. P.V.Subjects).

∗
C3 = h (Sdiff ∩ Sint
) ∪ (S1 ∩ S2 ),
∗
(Vdiff ∩ Vint ) ∪ (V1 ∩ V2 ),
∗
(Odiff ∩ Oint
) ∪ (O1 ∩ O2 ) i.

6. Experimental Evaluation

6.3.2. Concepts-based Features. In this paper, first, we
generate the concepts for story and non-story paragraphs by
using the training data. Next, we process each test paragraph
P, and generate its semantic triplets, hs, v, oi. A binary
feature matrix is created by checking if any of the semantic
triplets of P matches a concept, hS, V, Oi, where S, V and
O are related sets of subjects, verbs and objects respectively.
A match succeeds if s ∈ S, v ∈ V and o ∈ O.

6.1. Data Set
We use a corpus of 39, 642 paragraphs where 9, 058 are
coded as stories and 30, 584 as non-stories by domain experts. Text is collected from websites, blogs and other news
sources that are known to be outlets of extremist groups
such as Al-Qaeda, ISIS or their followers who sympathize
with their cause and methods.

6.4. Cross Validation for Detecting Stories

6.2. Expansion of Concepts with Dictionary-based
Synonyms

We evaluate the quality of generalized concepts and
relations by their performance as features in story detection.
The goal is to improve the predictive accuracy of story/nonstory classifier through the use of these new features. We
experiment with several different supervised learning packages including SVM [19], decision trees [20] and SLEP [21]
concluding that SLEP outperforms others for this task. We
use the MATLAB implementation of SLEP package [22]
and obtained the best results using LogisticR model. Training and testing are performed using ten-fold cross validation
and repeated with random shuffling over multiple iterations.
The results are averaged over all iterations. We report the
predictive performance of SLEP classifier using various
feature sets for story and non-story categories in Tables 2
and 3.
The feature sets we used are keywords, verb-based features [4] (Triplets), concepts and relations (Tier 1), concepts/relations expanded with contextual similarity index

After the Bottom-Up Agglomerative Clustering procedure (in Figure 5) terminates, we obtain high-level concepts
and relations, which we refer to as ‘Tier 1’. In order to
expand the concepts further with keywords that are missing
in the training corpus, we experiment with adding sensedisambiguated WordNet [12] synonyms to ‘Tier 1’ obtaining
‘Tier 1 + WordNet’. Alternatively, we also utilize contextual similarity index to create ‘Tier 1 + Similarity’ as
follows. For each concept c = h{s1...m }, {v1...n }, {o1...l }i ∈
C, where m, n, l > 1, we create a set of candidates to merge
by picking the synonyms of each subject, (si , 1 ≤ i ≤ m),
verb (vj , 1 ≤ j ≤ n) and object (ok , 1 ≤ k ≤ l). Without
loss of generality, we add w, synonym of si to cluster c, only
if there is at least one triplet in our database, hs, v, oi ∈ T
such that w = s, v ∈ {v1...n } and o ∈ {o1...l }.

947

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

TABLE 2. P ERFORMANCE OF CLASSIFIER FOR S TORIES
Method

Precision

Recall

F-Measure

Keywords

0.81

0.20

0.33

Triplets

0.73

0.55

0.62

Tier 1

0.87

0.78

Tier 1 + Similarity

0.86

Tier 1 + WordNet

0.87

TABLE 3. P ERFORMANCE OF CLASSIFIER FOR N ON -S TORIES
Method

Precision

Recall

F-Measure

Keywords

0.90

0.98

0.94

Triplets

0.89

0.99

0.92

0.83

Tier 1

0.80

0.89

0.84

0.82

0.84

Tier 1 + Similarity

0.83

0.86

0.84

0.80

0.83

Tier 1 + WordNet

0.82

0.88

0.85

(Tier 1 + Similarity) and concepts/relations expanded with
WordNet (Tier 1 + WordNet). The feature sets produced
by the bottom-up agglomerative clustering algorithm outperform others in the story detection category. We gained
7% boost in precision, 50% boost in recall and 36% boost
in F-Measure over the best performance of keyword and
triplet features (see Table 2). We observe that high-level
concepts/relations have far more discriminative power compared to other features. A key reason is that they are able
to eliminate dependent features by generalization. There is
not a big difference in performance among the original and
expanded concept-based feature sets and we can clearly see
that WordNet expansion did not contribute to the performance of concepts expanded by contextual similarity. This
finding presents another strong point in favor of our framework since adding information from an external knowledgebase was not able to provide a boost.
In the non-story category (Table 3), concept-based features are lagging behind in performance. This may be due
to the structural diversity of non-story paragraphs since
there are several different sub categories among them [23].
Another observation is that concept-based features help
overcome the performance bias between story and non-story
categories due to the imbalance in the number of training
samples. Overall, concepts/relations deliver a 36% boost in
performance for story detection.

7. Conclusion

6.5. Sensitivity Analysis

References

We assess concept/relation based features against the
possibility of over-fitting since they are highly dependent
on the training corpus. We explore this issue by using the
regularization parameter, λ in SLEP’s optimization formulation. We can pin-point the optimal number of features
and avoid over-fitting by observing the performance of
the system as the value of λ changes. The plots given
in Figure 6 display the change in λ versus the number
of features (middle row) and the performance (precision,
recall and F-Measure) for story (top row) and non-story
(bottom row) categories. In both cases, we can observe that
there is a sharp drop in the number of features (12, 000
to 2, 000) around 10−5 ≤ λ ≤ 10−4 while the precision,
recall and F-Measure are preserved. The data cursor box in
the middle plot mark the point of optimal value for λ and
the corresponding number of features. Experimentally, we
identified the optimal number of features as 7, 563 which
prevents over-fitting of the model and preserves the gains in
performance.

[1]

U. Kang, E. Papalexakis, A. Harpale, and C. Faloutsos, “Gigatensor:
Scaling tensor analysis up by 100 times - algorithms and discoveries,”
in Proceedings of the 18th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM, 2012, pp. 316–
324.

[2]

S. Kok and P. Domingos, “Extracting semantic networks from text via
relational clustering,” in Proceedings of the 2008 European Conference on Machine Learning and Knowledge Discovery in Databases
- Part I, 2008, pp. 624–639.

[3]

M. R. Quillian, “Semantic memory,” in Semantic Information Processing, 1968, pp. 227–270.

[4]

B. Ceran, R. Karad, A. Mandvekar, S. R. Corman, and H. Davulcu, “A
semantic triplet based story classifier,” 2012 IEEE/ACM International
Conference on Advances in Social Networks Analysis and Mining
(ASONAM 2012), vol. 0, pp. 573–580, 2012.

[5]

J. Allan, V. Lavrenko, and H. Jin, “First story detection in tdt is hard,”
in Proceedings of the Ninth International Conference on Information
and Knowledge Management, ser. CIKM ’00, 2000, pp. 374–381.

[6]

T. Hasegawa, S. Sekine, and R. Grishman, “Discovering relations
among named entities from large corpora,” in Proceedings of the
42Nd Annual Meeting on Association for Computational Linguistics,
ser. ACL ’04, 2004.

We presented an algorithm for discovering generalized
concept/relationship representation of a collection of related
documents that overcomes surface level differences which
arise when different keywords are used for related concepts.
This representation provides a 36% boost in the challenging
automated story detection task and a higher-level semantic
network representation of related stories. In future work, we
plan to gauge the utility of generalized concepts in document
clustering tasks. We plan to use a bi-clustering approach
which can point to subsets of stories and associated generalized concepts/relations as their themes. Since clustering is
unsupervised, we need to rely on domain expert knowledge
to evaluate the quality of the detected clusters and their
themes. We also plan to develop visualization tools for
exploring document collections and their clusterings through
their high-level semantic network representations.

Acknowledgment
This research was supported by an Office of Naval
Research grants N00014-09-1-0872 and N00014-14-1-0477
performed at Arizona State University. Some of the material
presented here was sponsored by Department of Defense and
is approved for public release, case number:15-467.

948

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

Figure 6. Sensitivity Analysis

[7]

M. Banko and O. Etzioni, “Strategies for lifelong knowledge extraction from the web,” in Proceedings of the 4th International
Conference on Knowledge Capture, ser. K-CAP ’07, 2007, pp. 95–
102.

[15] H. Lee, A. Chang, Y. Peirsman, N. Chambers, M. Surdeanu, and
D. Jurafsky, “Deterministic coreference resolution based on entitycentric, precision-ranked rules,” Comput. Linguist., vol. 39, no. 4, pp.
885–916, Dec. 2013.

[8]

B. Min, S. Shi, R. Grishman, and C.-Y. Lin, “Ensemble semantics
for large-scale unsupervised relation extraction,” in Proceedings of
the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.
Association for Computational Linguistics, 2012, pp. 1027–1037.

[16] M. Recasens, M. C. de Marneffe, and C. Potts, “The life and death of
discourse entities: Identifying singleton mentions,” in Proceedings of
the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
2013, pp. 627–633.

[9]

A. S. Gordon and K. Ganesan, “Automated story capture from conversational speech,” in K-CAP ’05: Proceedings of the 3rd international
conference on Knowledge capture, 2005, pp. 145–152.

[17] J. D. Choi, “Optimization of natural language processing components
for robustness and scalability,” Ph.D. dissertation, University of Colorado at Boulder, 2012.
[18] N. Ide and J. Véronis, “Introduction to the special issue on word sense
disambiguation: The state of the art,” Comput. Linguist., vol. 24, pp.
2–40, 1998.

[10] A. Gordon, Q. Cao, and R. Swanson, “Automated story capture from
internet weblogs,” in Proceedings of the 4th international conference
on Knowledge capture, 2007, pp. 167–168.

[19] C. Chang and C. Lin, “Libsvm: a library for support vector machines,”
ACM Transactions on Intelligent Systems and Technology (TIST),
vol. 2, no. 3, p. 27, 2011.

[11] A. Gordon and R. Swanson, “Identifying personal stories in millions
of weblog entries,” in Third International Conference on Weblogs and
Social Media, Data Challenge Workshop, 2009.

[20] (2013) Matlab statistics and machine learning toolbox. The
MathWorks Inc. [Online]. Available: http://www.mathworks.com/
help/stats/classification-trees-and-regression-trees-1.html

[12] (2010) About wordnet. Princeton University. [Online]. Available:
http://wordnet.princeton.edu

[21] J. Liu, J. Chen, and J. Ye, “Large-scale sparse logistic regression,” in
Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 2009, pp. 547–556.

[13] K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers, M. Surdeanu,
D. Jurafsky, and C. Manning, “A multi-pass sieve for coreference
resolution,” in Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing. Association for Computational Linguistics, 2010, pp. 492–501.

[22] J. Liu, S. Ji, and J. Ye. (2009) Slep: Sparse learning with
efficient projections. Arizona State University. [Online]. Available:
http://www.public.asu.edu/∼jye02/Software/SLEP

[14] H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu, and
D. Jurafsky, “Stanfords multi-pass sieve coreference resolution system
at the conll-2011 shared task,” CoNLL 2011, p. 28, 2011.

[23] H. L. Halverson, J. R. Goodall and S. R. Corman, Master Narratives
of Islamist Extremism. New York: Palgrave Macmillan, 2011.

949

c The British Computer Society 2015. All rights reserved.

For Permissions, please email: journals.permissions@oup.com
doi:10.1093/comjnl/bxv075

Advance Access publication on 11 September 2015

Predicting the Location and Time
of Mobile Phone Users by Using
Sequential Pattern Mining Techniques
Mert Ozer1 , Ilkcan Keles2 , Hakki Toroslu3 , Pinar Karagoz3∗ and
Hasan Davulcu1
1 School of Computing, Informatics, Decision Systems Engineering, Arizona State University,

Phoneix, AZ, USA
2 Department of Computer Science, Aalborg University, Aalborg, Denmark
3 Computer Engineering Department, METU, Ankara, Turkey
∗Corresponding author: karagoz@ceng.metu.edu.tr

In recent years, using cell phone log data to model human mobility patterns became an active research
area. This problem is a challenging data mining problem due to huge size and non-uniformity of
the log data, which introduces several granularity levels for the specification of temporal and spatial
dimensions. This paper focuses on the prediction of the location of the next activity of the mobile
phone users. There are several versions of this problem. In this work, we have concentrated on the
following three problems: predicting the location and the time of the next user activity, predicting
the location of the next activity of the user when the location of the user changes, and predicting
both the location and the time of the activity of the user when the user’s location changes. We have
developed sequential pattern mining-based techniques for these three problems and validated the
success of these methods with real data obtained from one of the largest mobile phone operators in
Turkey. Our results are very encouraging, since we were able to obtain quite high accuracy results
under small prediction sets.
Keywords: human mobility patterns; mobile phone user; sequence mining; location and time prediction
Received 1 March 2015; revised 13 July 2015
Handling editor: Yannis Manolopoulos

1.

INTRODUCTION

Since the introduction of the first mobile phones, especially
after 1990s, mobile phones quickly became indispensable
devices for ordinary people. Nowadays almost 95% of the people in the world use mobile phones. Mobile phone usages of
people generate huge amount of data for mobile phone operators. These data are mainly used for generating customer
invoice.
However, in addition to the information used for generating
invoice such as caller and callee information, the time and the
duration of the call, these data also contain location information
of both the caller and the callee. This location information is
not precise since the mobile phone operators only keep/know
the base station id of both users, not exact locations. Although
exact locations of mobile phone users can be determined, it is

typically not obtained by mobile phone operators, since it is not
feasible. Some operators use coarse location data to improve
their service quality, and some of them exploit these data to
create new forms of businesses such as generating appropriate
advertisement messages to users selected according to their
predicted movements [1–4].
One of the most well-known problems related to the user
location information is the prediction of the next location of
the mobile phone user. Users’ navigation behavior patterns are
important knowledge for mobile phone operators, so that they
can calculate potential next location of individual users in order
to be able to optimize their advertisement strategies. There are
also other potential usages of user behavior patterns in terms
of mass people movement modeling, such as city planning and
traffic optimization.

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

Predicting the Location and Time of Mobile Phone Users
In this paper, we focus on predicting individual mobile phone
user’s next location using her previous log data. User log data,
also named as call detail record (CDR), contain the base station
identifiers (and their locations) for the caller and callee and the
time of the activity (such as voice call, sending SMS or use of
internet). This historical data can be processed using sequential
pattern mining and time series analysis techniques in order to
predict the time and the location of the next event for users. The
main challenges of this problem are due to the huge size and
the non-uniformity of the data. User events do not come with
uniform distribution in time or spatial dimensions. Sometimes
events are very rare and sometimes are very often. Similar
non-uniform pattern can also be observed in terms of location
distribution of the data. However, a simple analysis also shows
that 80% of the users’ location of next activity is the same as
their current location (in terms of the base station identifiers
their mobile phones are connected to). Only 20% of the two
consecutive events are at different locations.
Although the prediction of the next location of the mobile
phone users seems like a well-defined problem, since it contains different parameters, several different variations of it can
be defined. In this paper, we have investigated three versions
of the next location prediction problem, which are listed as
follows:
(i) determining the location and the time of the next user
activity, regardless of whether the location of the user
changes or not,
(ii) predicting the location of the next activity of the user
when the location of the user changes,
(iii) predicting both the location and the time of the activity
of the user when the user’s location changes.
In this study, we have utilized CDR data obtained from one
of the largest mobile phone operators in Turkey. Typically, each
mobile phone activity is associated with the closest base station. Therefore, each base station can be assumed to be defining
a region covering the activities in that region. In CDR data, the
exact time of each activity is recorded. However, in the time
prediction of user activity, exact time is not very informative.
Therefore, we have divided a day into time intervals in our
process.
Also, we have clustered base stations according to their locations into regions and aimed to predict the region of the next
activity of the user in terms of these regions. In the first problem,
we have tried to predict both the region and the time interval of
the next activity.
For the second problem, we have focused on only predicting
the location of the next activity of the user in case the user’s location changes. Since for 80% of the activities the location of the
activity is same as the location of the previous activity, the location change problem is important.
Finally, in the last problem we have tried to predict both the
time and the location of the user’s next activity when the location
of the user changes. This is a kind of the extended version of

909

the second problem. Basically, we aimed to show that using time
information, in terms of time intervals, in addition to the location
change information, increases the accuracy of the predicting the
changed location and the time of the change.
We have made an extensive set of experiments to measure
the applicability and the accuracies of these approaches using
real data of more than 1 million mobile phone users for a period
of 1 month for a region of roughly 25 000 km2 . Usually, there
is a typical tradeoff in this kind of prediction problems such
that in order to increase the accuracy of a prediction it might be
necessary to make a large number of alternative suggestions.
When the suggestion or prediction set gets smaller, usually the
accuracy of the prediction quickly drops. Our results are very
encouraging, since for each problem, high accuracy values are
obtained by making only a very small number of alternative
suggestions. Our solutions for the first and second problem
with limited experimental analysis were also included in our
previous works [5,6]. A shorter version of our solution for the
third problem was also studied in [7].
The rest of the paper is organized as follows. Section 2 introduces previous work on location prediction. Section 3 presents
the details of the data and the problem definition. Section 4
introduces the proposed solutions for the problem defined.
Section 5 contains the experimental results of our proposed
methods. Section 6 concludes our work and points out possible
further studies.

2.

RELATED WORK

In recent years, variety of location prediction schemes on human
mobility have been studied in various dimensions [2,5,8–22].
Interesting findings about the human mobility habits and its
predictability are reported in [14,15]. In [14], de Montjoye et al.
propose a method using both Voronoi diagrams involving base
stations and spatial and temporal properties of users’ movement
data to find the minimum number of points enough to uniquely
identify individuals. They show that, almost for all users, distinct sequences with four spatio-temporal points exist in CDR
data. Therefore, such short sequences are sufficient to uniquely
identify 95% of the users, while sequences of length two characterize more than 50%.
In [15], Song et al. analyze the limits of predictability in
human mobility. They used the data collected from 50 000
mobile phone users for 3 months. They propose three entropy
measures which are believed to be the most fundamental quantities to analyze the limits of predictability, the random entropy,
the temporal-uncorrelated entropy and the actual entropy. They
also use a probability measure for correctly predicting user’s
future movements. They find that there is a 93% potential predictability in user mobility at best and 80% at worst for any
user [15].
In [23], Zheng and Ni investigate human mobility from
mobile data both in individual and group behavior aspects.

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

910

M. Ozer et al.

They use probabilistic, unsupervised approach for uncovering
the behavior similarity among users and for clustering individual behaviors.
There are other methods to use for location prediction problem rather than sequential pattern mining such as Markov
models and expectation maximization algorithms. In [11],
Thanh and Phuong make use of Gaussian distribution and
expectation maximization algorithm to learn the model parameters. Then, mobility patterns, where each is characterized by
a combination of common trajectory and a cell-residence time
model, are used for making predictions. They use Gaussian
mixture models to find similarities in cell-residence times of
mobile users. They outperform the methods that ignore temporal characteristics of user movements. However, they are in
need of studying their method in real data.
In [12], Gao et al. use both spatial and temporal data to predict users’ location. They propose ten different models which
can be categorized as spatial-based, temporal-based and spatiotemporal-based. They make use of Bayes’ rules for their prediction models which use historical data while predicting the next
location. They also make use of Markov models to build two
of their models. For the best model named as HPY Prior Hour–
Day Model, they managed to predict user locations with an
accuracy rate of 50%. They do not use any social network
information together with spatio-temporal patterns.
In [13], Gidófalvi and Dong propose a method which use
both spatial and temporal GPS data for building Markov model
which is used for next location and time prediction of user. In
other words, they both predict the change of location and the
time of this change. They use an Inhomogeneous ContinuousTime Markov (ICTM) model since the prediction depends on
the previous locations and time. They use both spatial and temporal information for building the model. Their ICTM model
predicts the departure time correctly with the 45 min error and
the next region correctly 67% of the cases.
Similar to our work, in [5,18–20], the authors propose
sequential pattern mining techniques for the location prediction problem. In [18], Yavas et al. propose an AprioriAll-based
algorithm which is similar to our three methods. They extract
frequent user trajectories which they name user mobility patterns (UMP) from a user move database and predict the user’s
next movement accordingly. However, they do not use any
spatial or temporal information while extracting UMPs or generating predictions. The rules consist of only cell ids rather than
any spatial attribute. They introduce alignment parameters on
the length of the sequences and maximum number of predictions as ours. They show that they get higher accuracies for
mobility prediction than previously proposed methods using
transition matrices.
In [19], Giannotti et al. propose methods to solve different trajectory pattern mining problems. They define spatio-temporal
sequences as the pairs of spatial attribute and the time that user
has spent in there. They also try to detect the popular regions.
The difference of this technique from the conventional sequence

pattern mining technique is the use of trajectories (T-patterns)
rather than itemsets.
In [20], Cao et al. introduces a method for discovery of periodic patterns in spatio-temporal sequences. They also make use
of an AprioriAll-based algorithm for extraction of periodic patterns. The distinctive feature of these periodic patterns is that
they are not frequent in the whole time span but in some time
interval, so they change their support definition accordingly.
There are various works that aim to further increase the prediction accuracies by the help of social networks. In [8], Cho
et al. propose that general human mobility does not have a high
degree of freedom and variation as it is believed. They work on
three features of human mobility; geographic movement, temporal dynamics and the social network. Social network is used
since human mobility is partly driven by our social relationships, e.g. we move to visit our friends. They use three main
data sources, where two of them are popular online locationbased social networks, Gowalla and Brightkite and the other
is a trace of 2 million mobile phone user’s phone activity in
Europe. They find that social relationships can explain ∼10%
of human movement in cell phone data and 30% of movement
in location-based social networks. However, periodic movement behaviour explains ∼50–70% of it. They reach 40%
accuracy while predicting user’s location at any time.
In [9], Boldrini and Passarella propose a model that integrates three main properties believed to be fundamental for
human mobility. First, user mobility largely depends on their
social relationships. Secondly, users are disposed to spend their
most of time in a few number of locations. Thirdly, users mostly
move shorter distances rather than the longer ones. The main
novelty of their model named Home-cell Community-based
Mobility Model (HCMM) is to integrate these three features.
They incrementally improved HCMM starting with a pure
social-based model and mathematically justifying the need for
extending the features. Finally, they claim that HCMM is able
to regenerate the main properties of human movement patterns.
In [10], Zhang et al. further improve the user mobility models
of [8,9] by amplifying the effect of social network information
in location prediction. They also claim that call patterns are
strongly related with co-locate patterns and mainly affect user’s
short-time mobility. They further propose a method named
NextMe which takes social interplay into consideration as well.
However this time, when the social interplay will affect social
mobility is identified and used accordingly. They validate their
scores with the MIT Reality Mining dataset. They reach up
to 60% accuracy levels for the prediction with their NextMe
method.
Rather than using social relationships or networks of the
user, in [16,17] distinctive features of spatial attribute in the
data are made use of. In [16], Zheng et al. aim to extract interesting locations such as culturally significant places, shopping
malls, city centers etc., and travel sequences from multiple
users’ GPS logs. They used tree-based hierarchical graph to
model user’s historical movement patterns then introduce a

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

Predicting the Location and Time of Mobile Phone Users

911

TABLE 1. List of attributes for CDR data.
Attributes

Description

Base station id #1
Phone number #1
Province code of phone number #1
Base station id #2
Phone number #2
Province code of the phone
number #2
call time
CDR type
URL
Duration
Call date

Unique integer representing the base station that caller, SMS sender or GPRS user connected to. For example,
17083
Unique string representing the caller, SMS sender or GPRS user. Due to the privacy reasons, it is not a regular
phone number, e.g. 7bcfc0259b9c8a4af95177a7e79bcd28
An integer that represents the province user started a call or a GPRS connection, or sent an SMS. e.g. 06
Unique integer representing the base station which callee or SMS receiver is connected to. It is null if the
type of the record is GPRS connection, e.g. 17083
Unique string that represents the callee or SMS receiver. Due to the privacy reasons, it is not a regular phone
number. It is null if the type of the record is GPRS connection, e.g. 28119ffa652d31607a3bb573bd3d594b
An integer that represents the province callee or SMS receiver is in, e.g. 06
The time that action started in a ‘hhmmss’ format, e.g. 170251
It can be one of the following: voice caller, voice callee SMS sender, SMS receive, GPRS connection
It is used only for GPRS data. It represents the URL that user tries to get.
An integer that represents the duration of the call. It is null for SMS, e.g. 47
The date that action is performed in a ‘yyyymmdd’ format, e.g. 20120907

Hypertext-Induced Topic Search-based inference model, which
represents one of the users’ travel to a location as a vertex. The
weight of the vertex is defined by user’s experience. Location’s
interest is also defined by user’s experience as well as the number of user’s visit. They claim that such a model can be used for
location recommendation like a mobile tourist guidance. They
evaluated their method with the GPS data of the 107 users of a
1-year period.
In [17], Ying et al. propose an algorithm which uses semantic
labels for locations rather than just using spatial attributes. They
explore semantic trajectories of the users and predict the next
location of the user accordingly. Rather than using sequential
pattern mining techniques, they use clustering methods for next
location prediction. They group users hierarchically according to their semantic trajectories by using Maximal Semantic
Trajectory Pattern Similarity (MSTP-Similarity) which they
define. It was the first work which combines the semantic tags
for location and spatial attributes for next location prediction
problem and their proposed location prediction model has a
high performance.

3. DATA AND PROBLEM DEFINITION
3.1. CDR data
The data used in this study are provided by one of the largest
mobile phone operators in Turkey. The CDR data contain more
than 1 million user’s mobile phone records corresponding to
a period of 1 month. The area corresponding to the calls is
around 25 000 km2 and the population of the area is almost
5 million. Two-thirds of the population lives in a large urban
area, corresponding to <30% of the whole area and the rest of

the population is scattered in small towns and villages. Due to
this population distribution, most of the 13 000 base stations
are located in densely populated areas of the region. In rural
areas, the distances among base stations reach tens of kilometers, while in the downtown area sometimes these distances are
as small as hundred meters.
Each record in data represents one of the following mobile
user activities: voice caller, voice callee, SMS sender, SMS
receiver and GPRS connection. Besides these cases, no record
exists in the CDR data. These records consist of 11 attributes.
For both the caller (i.e. #1) and callee (i.e. #2), base station
id, phone number, province code of the phone number are
included. In addition, call time, CDR type, URL, duration, call
date also exist in these records. Definition of these attributes
and example record attributes are presented in Table 1.
3.2.

Problem definition

Due to the content of our data set, the location of user activity
corresponds to the location of the base station s/he is connected
to. In some dense areas, the base station locations are very close
to each other, and sometimes users are not connected to the
nearest base stations due to load balancing. Therefore, we have
grouped base stations into larger regions and aimed to obtain
possible region of the user.
It is possible to construct next location and time prediction
model for each user separately from her/his CDR records.
Typical weekday and holiday patterns of most users can be
constructed using statistical methods if sufficient amount of
CDR data (at least a couple of months) is available. Travels,
insufficient action records, and heterogeneity of user actions
are main drawbacks for constructing models for each user

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

912

M. Ozer et al.

separately. However, in our model we wanted to determine frequent common user patterns from daily user patterns, regardless
of which users daily activities support them. Thus, for example,
a frequent common pattern may be supported by some of the
weekday activities of a number of users. Therefore, this frequent pattern may just correspond to a few hours of a day. This
way, when a new user sequence is given, it may be compared
against existing sequences with determine if it matches with
one frequent pattern, and that frequent pattern can be used to
predict potential next location and the time for that user. In
this approach, the number of frequent patterns becomes much
smaller compared to user based model generation. Also considering users switching mobile phone operators more frequently
nowadays, the cold start problem for many users are overcome
with this approach.
One month of CDR data of almost one million users
are used in this study. At the preprocessing phase of raw
CDR data, each activity record is converted to a triple as
(User_Id, Action_Location, Action_Time). The first field,
namely user id, is used only in order to generate a sequence of
location and time tuples for each user. We have used these daily
sequences as the main input of our sequential pattern mining
methods, which are defined as follows:
Definition (DUAS: daily user activity sequence): < (L1 ,
T1 ), . . . (Ln , Tn ) > is a DUAS obtained from the 1 day activities of one of the users, in which each (Li , Ti ) pair represents
the location and the time of an activity of the selected user.
All the problems that are discussed in this work are based
on finding frequent patterns obtained basically from DUAS.
A DUAS can contribute (i.e. increase) to the frequency of a
pattern only once as a contiguous sequence, even if that pattern
occurs more than once in that DUAS, which may occur only for
problems that do not include the time information. Matching
between a pattern and a DUAS is done in terms of substring
matching with some tolerance, whenever it is defined. This is
inherently enforced by the time dimension whenever it is used
by converting exact times into time intervals.
Since the exact activity time for most activities do not have
any significance, we have used simple abstraction approach and
divided each day into a predefined number of time intervals. If
no action has been recorded in a given time interval, then it is
dropped from the DUAS. If more than one action is recorded in
a given time interval, then the most frequent location is selected
as the location information. As a result, DUAS is converted into
a sequence of daily user location-time pairs obtained under the
time abstraction, which is defined:
Definition (DUS-LT: daily user sequence with location and
time): < (L1 , T1 ), . . . (Ln , Tn ) > is a daily user location–time
sequence obtained from DUAS, in which each Ti corresponds
to the beginning of predefined time intervals and each Li corresponds to the location of the most of the activities occurred for
a selected user in that time interval.
The first problem is defined on DUS-LT to determine the location and the time of the next activity of the given user. Support

count of a DUS-LT is the number of occurrences of it in DUAS.
A frequent DUS-LT is a DUS-LT whose support count is over a
given minimum threshold.
Definition (FDUS-LT: frequent DUS-LT): {D1 , D2 , . . . Dm } is
a set of DUS-LT such that each Di is a frequent DUS-LT under
user defined tolerance on time intervals.
Problem 1. Predicting the Location and the Time for the
Next Activity in the following time interval (LTNA): in this
problem, for a given user sequence, which is a DUS-LT, such
as u = (Lu1 , Tu1 ), . . . (Lun , Tun ) all frequent patterns matching (under some tolerance) are determined from FDUS-LT, and
a set of potential next location–time pairs are predicted from
these patterns.
For the next problem, time information is not used, and DUSLT are converted into a sequence of locations, which is defined
as below:
Definition (DUS-NL: daily user sequence with non-repeating
location): L1 , L2 , . . . Ln  is a daily user location sequence
obtained from DUS-LT by dropping time attribute and replacing successively repeated locations with a single one. Thus, for
all successive locations Li , Li+1, Li = Li+1 .
Definition (FDUS-NL: frequent DUS-NL): {D1 , D2 , . . . Dm }
is a set of DUS-NL such that each Di is a frequent DUS-NL.
The problem is defined as follows.
Problem 2. Predicting Location for the first Successive
Activity, which has a different location from the current location (LSA): in this problem, for a given user sequence, which
is a DUS-NL, such as u = Lu1 , . . . Lun , a frequent pattern is
searched from FDUS-NL, and if a matching (under some tolerance) has been found, the next location is predicted for that
user sequence from the matching frequent pattern.
In the third problem, the location and the time of the first
successive activity is predicted which has a different location
than from the current location. Therefore, DUS-LTs are converted into sequences without successively repeating locations,
which is defined:
Definition (DUS-NLT: daily user sequence with nonrepeating location and time): (L1 , T1 ), . . . is a daily user
non-repeating location and time sequence obtained from
DUS-LT, in which each Ti corresponds to the beginning of
predefined time intervals and Li s corresponds to the location
of the most of the activities occurred for a selected user in that
time interval, and successively repeated locations are replaced
by a single one. Thus, for all successive locations Li , Li+1 ,
Li = Li+1 .
Definition (FDUS-NLT: frequent DUS-NLT): {D1 , D2 , . . . ,
Dm } is a set of DUS-NLT such that each Di is a frequent DUSNLT generated under some tolerance on time intervals for
pre-selected locations. The problem definition is given.

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

Predicting the Location and Time of Mobile Phone Users

913

Problem 3. Predicting Location and Time for the first
Successive Activity, which has a different location from the current location (LTSA): in this problem, for a given user sequence,
which is DUS-NLT, such as u = (Lu1 , Tu1 ), . . . (Lun , Tun ) a
frequent pattern is searched from FDUS-NLT, and if a matching
(under some tolerance) has been found, the next location–time
pair is predicted for that user sequence from the matching
frequent pattern.

4.

PROPOSED METHODS

In all three problems, since it is almost impossible to predict
the exact location of the next activity, we tried to determine the
region of the activity. The regions are created by combining
the areas corresponding to base stations. Using the location
coordinates of base stations, they have been clustered by using
k-means method. More than 13 000 base stations are clustered
in varying number of regions. Then, the CDR data have been
processed to replace base station identifiers with their corresponding region identifiers. Due to uneven distribution of
the base stations, when we constructed 100 regions, we have
obtained clusters containing between 6 and 656 base stations.
Figures 1–3 show the regions with different zoom levels.
As in almost all big data problems, CDR data also contain a lot
of irrelevant information and therefore it has to be preprocessed
before it can be utilized. In this preprocessing phase, irrelevant
fields such as province code, anonymized callee number etc. are
removed and date and time fields are joined together and then all
the records are sorted according to caller id in temporal order.
Afterwards, for each user, each day’s activity is converted into
DUAS format, as described in Section 3.2. Table 3 shows a small
sample of this process applied to the data shown in Table 2.

FIGURE 2. Regions in Zoom Level 2.

FIGURE 3. Regions in Zoom Level 3.

4.1.

FIGURE 1. Regions in Zoom Level 1.

Predicting the location and the time for the next
activity in the following time interval (LTNA)

4.1.1. Extracting frequent patterns
The main aim of this problem is to predict the common frequent
daily navigation patterns of mobile phone users. Therefore, the
input of the problem is just a huge set of daily user sequences
of region–time interval pairs. The actual day information
or even the user identifier is not important. From this set of
sequences, we would like to determine frequent patterns. These

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

M. Ozer et al.

914

TABLE 2. Sample CDR data before preprocessing.
R91
R91
R55
R55
R55
R55

phone #1
phone #1
phone #1
phone #1
phone #1
phone #1

06
06
06
06
06
06

R91
R21
R27
R27
R91
R3

phone #2
phone #3
phone #4
phone #4
phone #5
phone #6

06
06
06
06
06
06

2012/09/07
2012/09/07
2012/09/07
2012/09/07
2012/09/07
2012/09/07

01:02:51
07:10:08
09:22:31
11:15:40
14:43:32
17:03:04

mmo
mmo
mmo
mmo
mmo
mmo

47
3
11
8
14
12

TABLE 3. Sample CDR data of Table 2 after preprocessing.
R91, 01:02

R91, 07:10

R55, 09:22

frequent patterns are determined according to the following
parameters:
(i) pattern length, which describes the length of the desired
frequent pattern,
(ii) minimum support, which describes the minimum
ratio of the pattern to occur in order to be identified as
frequent,
(iii) time interval length, which is used to discretize the time
of the day, and defines the length of each interval.
The final parameter has somehow similar effect as defining
regions on spatial dimension. Since it is not feasible to process
the data with exact times, we have used this parameter to define
varying length time intervals to discretize the time dimension.
As a result, each day is divided into a predefined number of
equivalent length time intervals, and, each exact time data are
replaced with the beginning time of the interval it falls into.
This may generate sequences of region–time interval pairs with
potentially more than one region for the same time interval
value. Such pairs are reduced into a single region-time interval
pair by choosing the most frequent region for that interval in
the sequence.
Our frequent pattern extraction algorithm is a variant of classical AprioriAll algorithm. The standard AprioriAll algorithm
consists of two phases, namely; candidate generation and elimination phases. In the standard algorithm, k-length candidates
are generated from (k − 1)-length patterns. Since this bottomup process is very costly and since we are only interested in patterns of a given length, this phase has been modified and our
algorithm generates all candidates with predefined fixed length
by making a single pass on the data. During this pass the counts
of all fixed length patterns are determined, and those that do not
satisfy the required minimum support are eliminated. Table 2
depicts a sample with three frequent patterns for pattern length
4. In the table, the pairs, constituting region id and the start of
the time interval, separated by comma represent region id and
discretized time of the day.

R55, 11:15

R55, 14:43

R55, 17:03

4.1.2. Prediction
The prediction phase works as follows: after a navigation
pattern of a current user’s region–time interval pair length
(k − 1) sequence has been obtained, we try to predict the kth
region–time interval pair of the user by comparing it with the
existing frequent patterns. In order to do this, simply, previously constructed frequent patterns are searched to find if
there is a matching between the current user’s length (k − 1)
sequence and the prefixes of length (k − 1) of the existing frequent sequences. If such a matching is found, the kth item of
the existing sequence can be predicted as the next region–item
pair of the current user. Since there can be more than one such
predictions, a set of predictions can be generated, which are
sorted according to the decreasing support values.
Due to the difficulty of finding exact matches between
the current user navigation sequence and existing frequent
sequences, we have added a tolerance parameter in time dimension in order to be able to make more flexible predictions.
This tolerance parameter allows time intervals to match if they
overlap.
Assume that we have a user, with the following navigation
sequence:
(R91, 1015), (R95, 1230), (R45, 1630)
However, there is no frequent pattern starting exactly with the
same sequence, but we have the following frequent pattern:
(R91, 1000), (R95, 1245), (R45, 1630), (R52, 1700)
This frequent pattern and the above user navigation pattern
have only 15 min time difference. We can assume that, the
current user’s navigation pattern is very similar to this existing frequent pattern, and therefore we can predict the next
region–time interval pair of this user as the last pair in the
frequent pattern as:
(R52, 1700)
In order to be able to produce these kind of results, our method
uses time-tolerance parameter. In this example, it should be set
to 15 min or larger in order to be able to accept these matchings.

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

Predicting the Location and Time of Mobile Phone Users

915

TABLE 4. Sample frequent patterns.
Frequent pattern (sequence)

Support

< (R91, 1000), (R95, 1215), (R45, 1615), (R48, 1800) >
< (R91, 1000), (R95, 1215), (R45, 1615), (R70, 1900) >
< (R91, 1000), (R95, 1215), (R45, 1615), (R55, 1915) >

4.02 × 10−6
3.68 × 10−6
2.53 × 10−6

In general, more than one frequent pattern’s prefix may
match with the current user’s navigation pattern. When such
case occurs, as a simple solution, the kth pair of the frequent
pattern with the highest support value may be returned as a
prediction. We may prefer to have more than one prediction in
order to increase the accuracy of the prediction. However, it is
not feasible to produce a large set of prediction just to increase
the accuracy. This trade-off has been handled by our system
with the introduction of the multi-prediction limit parameter.
This parameter works as follows: all frequent patterns starting
with given user’s traversal sequence are sorted in decreasing
order of the support values and then the sum of the support
values are normalized to 1. After that, the prediction set is
generated by adding kth elements of frequent patterns one by
one in support-sorted order, until sum of the normalized support values reach to the multi-prediction limit for the selected
sequences. The details are given in the following section.
For example, consider the following user sequence:

determine frequent user sequences corresponding to region
changes. To achieve this, firstly as a preprocessing, for all
user sequences, successively repeated regions are eliminated
from each daily sequence. After that, standard frequent pattern
mining algorithm has been applied on these sequences, and,
as a result, frequent patterns corresponding to users’s region
changes are obtained.

(R91, 1000), (R95, 1215), (R45, 1615)

R77, R91, R95, R16, R22, R41.

In this sequence, there are three frequent patterns with length
4 as given in Table 4. For this sequence, if the frequent pattern
with the highest support value is used to make the prediction,
(R48, 1800), will be predicted. If the multi-prediction limit is
set to 0.5, again only the same prediction will be made. However, if the prediction limit is increased to 0.8, then, the first two
frequent sequences are going to be used, and two predictions,
which are (R48, 1800) and (R70, 1900), going to be produced.
4.2. Predicting location for the first successive activity,
which has a different location from the current
location (LSA)
In this method, each record, which is structured as a sequence
of region ids, represents a user’s daily location change pattern.
An example sequence, which is obtained from the sample data
given in Table 2 is R91, R55.
4.2.1. Extracting frequent patterns
Since in this problem, we are interested in the change of the
regions of the mobile phone users, the frequent pattern generation phase is slightly different from the first problem, in which
all frequent patterns as pairs of regions and time intervals are
determined. In this problem, time information is not used and
only temporal relations of regions are considered in order to

4.2.2. Prediction
The prediction method used in this problem is very similar to
the first problem. Since time information is not used, there is
no need for the time-tolerance parameter. Instead, a new tolerance parameter has been introduced in order to be able to match
patterns with different lengths, as a simple alignment operation
between sequences. We have not used standard alignment algorithms since in our problem the sequences are very short, and
therefore, the amount of tolerance needed is very small. As an
example, consider that we have a user sequence as follows:

Although there is no exact matching frequent sequence, let us
assume that we have a frequent sequence starting with:
R77, R95, R16, R22, R41
or
R77, R95, R35, R16, R22, R41
In this case, we may tolerate one additional region (R91) in
the user sequence or one additional region (R35) in the frequent
sequence in the matching process and predict the next region of
the frequent pattern as a potential next region of the user.
Since potentially the lengths of frequent patterns and user
sequences are quite small, our tests have shown that except
for length tolerances of 1 or 2 the quality of predictions using
general alignment methods sharply drops.
4.3.

Predicting location and time for the first successive
activity, which has a different location from the
current location (LTSA)

For this method, user’s daily sequence contains not only spatial attribute but also temporal attribute. For example, sequence
which is obtained from the sample data given in Table 2 will be
(R91, 01 : 02), (R55, 09 : 22).

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

M. Ozer et al.

916

4.3.1. Extracting frequent patterns
Basic intuition behind the extraction method is nearly the same
as that of the first proposed method. In this approach, the patterns are generated in order to keep only the change of region
ids in a single day. The difference with the second method is the
use of temporal information. This time user’s daily sequences
have pairs of region id and time information as in the first
method. Thus, pairs having the same region id as in the previous pair are eliminated. This guarantees that there will be no
successive repetition of region ids in one frequent pattern, and
predictions never have the same region id with the last region
id of traversal instance.
4.3.2. Prediction
In this method, we use both tolerance parameters, time tolerance and tolerance in pattern length for prediction. Apart from
this difference, the prediction algorithm works similar to the
first method.

5.

EVALUATION AND EXPERIMENTAL RESULTS

5.1.

Problem parameters

In the evaluation process we have measured the qualities of the
proposed solutions for the three problems by using the following
problem parameters:
(i) Pattern length (l): defines the length of the patterns
that are constructed at the frequent pattern construction
phase. During the prediction phase whenever a user pattern of length (l − 1) has been reached, it is compared
against the frequent patterns in order to be able to find
matching patterns and then use the last items of those
patterns to predict the lth item of the user sequence.
(ii) Length tolerance (lt): defines the amount of alignment
tolerance for matching two patterns. If lt is 1 then two
sequences with length n and (n + 1) matches with each
other if n of their items are same.
(iii) Minimum support threshold (s): defines the minimum
number of occurrences of a sequence of a given length
in daily user sequences in order to mark that sequence
as frequent sequence, which is specified in terms of the
percentage of the size of the daily user sequences.
(iv) Multi-prediction limit (p): defines, in terms of percentages, how to construct prediction set from all frequent
patterns that match with the given user sequence using
the supports of these frequent patterns, whose values
are normalized as the summation of them is 100. This
is done as follows: First, all frequent patterns matching
with the given users sequence are sorted in decreasing
order of the support values, and the sum of their support
values are normalized to 100, and the supports of the
frequent patterns are also normalized accordingly. After
that, the prediction set is populated by choosing the last

items of the first k frequent patterns until the normalized
summation of the support values of the chosen frequent
patterns reach to the specified multi-prediction limit p.
(v) Region/cluster count (r): defines the number of regions/
clusters which are generated using the coordinates of the
base stations via clustering.
(vi) Time interval length (t): defines the length of time intervals in terms of minutes, which is used to divide one day
(24 h) into same size time intervals.
(vii) Time tolerance (tt): defines the amount of the tolerance
time in terms of minutes, that two time parameters can
match. For example if both tt and t is 15, then an activity
that occurred at 13:10 (which is converted to 13:00, after
mapping it to start time of the time interval it is in) can
match with an activity which occurred in a time interval
(12:45–13:15).
Varying values of the above parameters are used in the evaluation of the three problems introduced in the previous sections in
order to determine the qualities of the solutions:
(i) LTNA: only pattern length and minimum support threshold are used.
(ii) LSA: pattern length, minimum support, length tolerance
and multi prediction limit parameters are used.
(iii) LTSA: all of the above parameters, namely pattern
length, length tolerance, minimum support threshold, multi prediction limit, region/cluster count, time
interval length and time tolerance parameters are used.
5.2. Evaluation process
In order to assess the quality of the predictions made by the
methods proposed in the previous section, we have used 5-fold
cross validation on a real CDR data set that has been introduced
earlier. Training phase of the evaluation process consists of
applying the frequent pattern extraction steps of the proposed
methods on the training data, in order to generate frequent
patterns.
The testing phase works as follows: in step one, the test data
are processed as in the training phase to extract all sequential
patterns, except this time with no minimum support, in order to
generate all traversal patterns. For each one of the traversal patterns, prediction algorithm introduced in the previous section
has been applied to predict the last elements of these patterns.
The result of the prediction is compared against the actual
last element of the traversal pattern. These results are used in
the calculations of the evaluation metrics which is introduced
below.
5.3. Evaluation metrics
In order to measure the qualities of the proposed methods,
we have introduced three new metrics, namely p-accuracy,
g-accuracy and prediction count.

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

917

Predicting the Location and Time of Mobile Phone Users
g-accuracy (general accuracy) is the ratio of number of true
predictions to the number of all patterns with the same length in
the test set.
g-accuracy =

|Correctly Predicted Instances|
|Test Set|

p-accuracy (predictions’ accuracy) is the ratio of the number
of true predictions to the number of all predictions we are able
to make.
p-accuracy =

|Correctly Predicted Instances|
|Predicted Instances|

The reason for using two different accuracy calculation is due
to the fact that the proposed algorithm may not be able to generate prediction for each one of the test instances, if there is no
matching frequent pattern found for the queried instance. In the
first form of accuracy calculation, the accuracy result superficially drops for such cases.
In addition to the accuracy, the quality of the results obtained
also depends on the size of the prediction set.
Prediction count metric is required because of the multiprediction limit parameter. It quantifies the size of the prediction
set when correct prediction result is in the prediction set.
5.4. Experimental results
In our experiments, we fix all the related problem parameters
except the one which we measure the effect on the performance.
5.4.1. Results for Problem 1 (LTNA)
In the first set of experiments, we analyze the effect of length of
the frequent patterns and support threshold using the following
parameter values:
(i)
(ii)
(iii)
(iv)
(v)
(vi)

pattern length is 6
time tolerance is 75 min,
time interval is 15 min,
minimum support is 10−6 ,
cluster count is 100,
multi-prediction support limit is 1.0 (which means
allowing to use all frequent patterns matching with test
set patterns).

From the experiments the following results have been
obtained:
(i) The effect of pattern length: as it can be seen in Fig. 4,
when the pattern length increases, prediction g-accuracy
decreases. This is due to the fact that the number of
longer frequent patterns is much fewer than the number
of shorter frequent patterns. The number of frequent
patterns for various pattern lengths are given in Table 5.
When we have analyzed the number of predictions
made with multi-prediction method as a potential next
region we have observed that these numbers are quite
high as presented in Table 6.

FIGURE 4. The effect of pattern length on g-accuracy for LTNA.
TABLE 5. Number of frequent patterns for different pattern
lengths for LTNA.
Pattern length

Number of frequent patterns

2
3
4
5
6
7
8
9
10
11
12

1 777 423
1 706 778
1 186 798
796 505
539 586
381 818
281 931
214 897
168 218
134 827
110 334

TABLE 6. Prediction counts for different pattern lengths for
LTNA.
Pattern length
2
3
4

Prediction count
59.79
11.82
6.92

(ii) The effect of support threshold: Fig. 5 shows that, when
minimum support threshold value increases, prediction
g-accuracy drops. The reason for this result is that as
minimum support threshold increases the number of
generated frequent patterns decreases.
An important observation in these result is that using
multi-prediction, a very high g-accuracy has been obtained
for patterns with length smaller than 5. However, when the
total number of regions, which is 100 in our case, are considered, the number of predictions obtained from multi-prediction
method is not practical and useful for real cases. For example,

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

M. Ozer et al.

918

FIGURE 5. The effect of minimum support on g-accuracy for LTNA.
FIGURE 6. The effect of pattern length on g-accuracy, p-accuracy
and prediction count for LSA.

for length 2, the size of the prediction is almost 60 on average. This explains the superficially high g-accuracy values for
patterns shorter than five.
The most remarkable result that we found in this analysis is
the ratio of the number of the patterns (any length n) that have
the same region id for nth and (n − 1)th time interval to the number of all patterns. It holds for almost 80% of patterns having
lengths >4. This causes prediction for test set pattern to be the
last element of the matching key in frequent pattern, in other
words causes to predict one person’s next location as the current
location for 80% of the test data. Since our first motivation was
change of location problem, we did not evolve this method and
do not elaborate on further results of this method.
5.4.2. Results for Problem 2 (LSA)
In the experiments for this problem, we analyze the effect of the
pattern length, support threshold, length tolerance and the multiprediction limit in terms of accuracy and prediction count using
the following values of parameters:
(i)
(ii)
(iii)
(iv)
(v)

pattern length is 5
multi-prediction limit is 0.8,
the length tolerance is 2
cluster count is 100,
the minimum support is 4 × 10−7 .

For the length tolerance experiment, three parameters are
set to different values, namely pattern length is set to 7, multiprediction limit is set to 0.5 and the minimum support value is
set to 0.0001.
The following results have been obtained from the experiments on this problem:
(i) The effect of pattern length: as it can be seen in Fig. 6,
when the pattern length increases, prediction g-accuracy
drops. It is because of the decreasing number of frequent patterns as the pattern length increases. We
did not include patterns shorter than 5 since for patterns with length 4, multi-prediction method generates

7 alternatives on average. For pattern length 5, our
method under multi-prediction limit 0.8 generated 2.3
predictions on average for successful prediction, which
is reasonable value for the number of generated predictions. Figure 6 also shows the relationship between
pattern length and p-accuracy. Since p-accuracy is the
ratio of true predictions to the number of predictions
made (instead of the total number of test patterns), it is
not expected to have a similar behavior when pattern
length increases. The reason for the lower g-accuracies
of higher pattern lengths in Fig. 6 is the non-predicted
instances in test data. However, we do not include
non-predicted patterns in p-accuracy. Prediction count
has been positively affected with the increase in pattern
lengths, as can be seen in Fig. 6.
(ii) The effect of support threshold: Fig. 7 shows that
when minimum support value increases, prediction
g-accuracy drops as in our first problem. Similarly, this
is due to the fact that as the minimum support increases,
the number of generated frequent patterns decreases.
For the selected parameter set, prediction count is stable. This experiment shows that multi-prediction limit
outweighs the effect of minimum support on prediction
count.
(iii) The effect of length tolerance: as given in Table 7,
g-accuracy values are lower than the first problem,
since minimum support used in this set of experiments
is 0.0001. As it can be seen in the table, when the
length tolerance increases, prediction g-accuracy also
increases.
(iv) The effect of multi-prediction limit: Fig. 8 depicts that
when multi-prediction limit increases, both prediction
g-accuracy and p-accuracy also increase, as expected.
Nevertheless, Fig. 8 shows that with the increase in
multi-prediction limit prediction count also increases.

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

Predicting the Location and Time of Mobile Phone Users

919

TABLE 7. Length tolerance vs g-accuracy for LSA.
Length tolerance
0
1
2

g-accuracy
0.20
0.23
0.29

FIGURE 9. The effect of pattern length on g-accuracy, p-accuracy
and prediction count for LTSA.

1 237 313), and increment both true and false predictions biased
to true predictions.
Similar to the previous experiments, as can be seen in Fig. 7,
p-accuracy does not seem to be effected with the increase in the
support value, and there is a very small increase.
FIGURE 7. The effect of support threshold on g-accuracy, p-accuracy
and prediction count for LSA.

5.4.3. Results for Problem 3 (LTSA):
For this problem, we analyze the effect of all seven paramaters
on the accuracy and prediction count using the following values
of parameters:
(i)
(ii)
(iii)
(iv)
(v)
(vi)
(vii)

pattern length is 5
length tolerance is 2,
time interval length is 60,
time tolerance is 120,
multi-prediction limit is 0.8,
cluster count is 100,
minimum support is 4 × 10−7 .

In the time interval experiment, time tolerance is set to 0.
The following results have been obtained from the experiments on LTSA:
FIGURE 8. The effect of multi-prediction limit on g-accuracy,
p-accuracy, prediction count for LSA.

When compared with the first problem, it can be seen that
g-accuracy values are much higher in the second problem.
There are two reasons for it; length tolerance and eliminating
successively repetitive region ids. Length tolerance gives the
ability to search test set pattern throughout different lengths
of frequent patterns. Eliminating repetitive region ids gives
less variety in frequent patterns. These factors reduce the number of non-predicted patterns as expected (from 2 214 700 to

(i) The effect of pattern length and support threshold:
when pattern length/minimum support increases, gaccuracy decreases and p-accuracy is almost stable
(Figs. 9 and 10). On the other hand, the prediction count
drops.
(ii) The effect of length tolerance and multi-prediction limit:
when length tolerance/multi-prediction limit increases,
both g-accuracy and p-accuracy increase (Figs. 11
and 12). Increasing length tolerance/multi-prediction
limit makes some unpredicted test sequences predictable which increases the g-accuracy. Unfortunately,
as it can be seen in the figures, this leads to larger
prediction sets.

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

920

M. Ozer et al.

FIGURE 10. The effect of support threshold on g-accuracy,
p-accuracy and prediction count for LTSA.

FIGURE 11. The effect of length tolerance on g-accuracy, p-accuracy
and prediction count for LTSA.

(iii) The effect of number of regions: In this problem, we
have also analyzed the effect of the region sizes. As it
can be seen in Fig. 13, when cluster count, i.e. number
of base station regions, increases g-accuracy decreases
slightly. It is because of the unpredicted test sequences
rather than false predictions since increasing cluster
count makes frequent patterns harder to extract. However, Fig. 13 also shows that, p-accuracy increases
slightly. This is due to the fact that when cluster count
increases movement patterns of users can be defined
more precisely which makes frequent patterns harder to
find but more accurate ones. Therefore, usually correct
predictions are generated when compared with fewer
numbers of clusters. It also eventually decreases the
prediction count, which can be seen in Fig. 13.

FIGURE 12. The effect of multi-prediction limit on g-accuracy,
p-accuracy and prediction count for LTSA.

FIGURE 13. The effect of number of regions on g-accuracy,
p-accuracy and prediction count for LTSA.

(iv) The effect of time interval length: when time interval length increases, both g-accuracy and p-accuracy
increase, as shown in Fig. 14. Although there is a sharp
increase in g-accuracy, the increase in p-accuracy is
limited. Larger time interval means more similar daily
sequences and eventually higher number of frequent
patterns. Thus, larger time intervals lead to increase
in the accuracy. We can say that as prediction count
increases in general, as it can be seen in Fig. 14, the time
interval length increases. However, for time interval
length 360, there is a small drop.
(v) The effect of time tolerance: similarly, when time tolerance increases, both g-accuracy and p-accuracy
increase slightly, as it can be seen in Fig. 15. Moreover,
also Fig. 15 shows that, when time tolerance increases,
prediction count slightly decreases.

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

Predicting the Location and Time of Mobile Phone Users

921

most valuable prediction results for these three methods:

FIGURE 14. The effect of time interval length on g-accuracy,
p-accuracy and prediction count for LTSA.

(i) For the spatio-temporal next location prediction, it does
not make sense to present the results below or ∼80%
accuracy since 80% of the user’s next location is their
current location.
(ii) For the spatial next location change prediction,
g-accuracies differ between 48 and 84% for the
prediction counts 2.4 and 14 for 100 regions while
p-accuracies differ between 74 and 99% for the same
prediction counts. These values show that our proposed model for this problem can generate successful
accuracy values with acceptable prediction counts.
(iii) For the spatio-temporal next location change and
time prediction, while it predicts nearly half of the
test sequences, p-accuracies reach up to 93% for 14
prediction count for possible 9600 ([24 × 1 hour time
interval] × 400 clusters) spatio-temporal prediction combination. Moreover, it generates 87% paccuracy for 3.44 prediction count for possible 153 600
([24 × 1 hour time interval] × 6400 clusters) prediction
combination.
As a future work, we plan to enlarge our problem space into
the following directions: next location change prediction using
spatio-temporal data, next action time prediction using temporal data, location and time prediction of the next action using
spatio-temporal data.

FUNDING

FIGURE 15. The effect of time tolerance on g-accuracy, p-accuracy
and prediction count for LTSA.

This research was supported by Ministry of Science,
Industry and Technology of Turkey with project number
01256.STZ.2012-1 and title ‘Predicting Mobile Phone Users’
Movement Profiles’.

REFERENCES
6. DISCUSSION AND CONCLUSION
In this work, we applied sequence pattern mining techniques for
location prediction problem domain. We used one of the largest
mobile phone operator companies’ CDR data. We focused on
three different subproblems in the location prediction problem
space, namely, next location and time prediction using spatiotemporal data, next location change prediction using spatial
data, next location change and time prediction using spatiotemporal data. The main novelties are time prediction and
spatio-temporal alignments for the prediction task. In the
experiments, we have evaluated our model’s prediction quality
with respect to g-accuracy, p-accuracy and prediction count
and further analyzed the effects of change of minimum support,
multi-prediction limit, length tolerance, pattern length, cluster
count, time interval length and time tolerance on prediction
accuracies and count. Here are the some basic findings and

[1] Tseng, V.S. and Lin, K.W. (2006) Efficient mining and prediction
of user behavior patterns in mobile web systems. Inform. Softw.
Technol., 48, 357–369.
[2] Gonzalez, M.C., Hidalgo, C.A. and Barabasi, A.-L. (2008)
Understanding individual human mobility patterns. Nature, 453,
779–782.
[3] Phithakkitnukoon, S., Horanont, T., DiLorenzo, G., Shibasaki,
R. and Ratti, C. (2010) Activity-Aware Map: Identifying Human
Daily Activity Pattern using Mobile Phone Data. In Salah, A.,
Gevers, T., Sebe, N., and Vinciarelli, A. (eds), Human Behavior
Understanding, Lecture Notes in Computer Science 6219,
pp. 14–25. Springer, Berlin, Heidelberg.
[4] Zhu, Y., Zhang, Y., Shang, W., Zhou, J. and Ying, C. (2009)
Trajectory Enabled Service Support Platform for Mobile users’
Behavior Pattern Mining. 6th Annual Int. Conf. Mobile and
Ubiquitous Systems: Networking Services, MobiQuitous, 2009.
MobiQuitous’09, July, pp. 1–10.

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

922

M. Ozer et al.

[5] Keles, I., Ozer, M., Toroslu, I.H. and Karagoz, P. (2014) Location
Prediction of Mobile Phone Users Using Apriori-based Sequence
Mining with Multiple Support Thresholds. Proc. 3rd Workshop
on New Frontiers in Mining Complex Patterns, NFMCP 2014,
pp. 2–13.
[6] Ozer, M., Keles, I., Toroslu, I.H. and Karagoz, P. (2013) Predicting the Change of Location of Mobile Phone users. Proc. 2nd
ACM SIGSPATIAL Int. Workshop on Mobile Geographic Information Systems, MobiGIS’13, New York, NY, USA , pp. 43–50.
ACM.
[7] Ozer, M., Keles, I., Toroslu, I.H., Karagoz, P. and Ergut, S.
(2014) Predicting the Next Location Change and Time of
Change for Mobile Phone Users. Proc. 3rd ACM SIGSPATIAL
Int. Workshop on Mobile Geographic Information Systems,
MobiGIS’14, New York, NY, USA, pp. 51–59. ACM.
[8] Cho, E., Myers, S.A. and Leskovec, J. (2011) Friendship and
Mobility: User Movement in Location-Based Social Networks.
KDD’11 Proc. 17th ACM SIGKDD Int. Conf. Knowledge
Discovery and Data Mining, pp. 1082–1090. ACM,
[9] Boldrini, C. and Passarella, A. (2010) Hcmm: modelling spatial
and temporal properties of human mobility driven by users social
relationships. Comput. Commun., 33, 1056–1074.
[10] Zhang, D., Vasilakos, A.V. and Xiong, H. (2012) Predicting location using mobile phone calls. SIGCOMM Comput. Commun.
Rev., 42, 295–296.
[11] Thanh, N. and Phuong, T.M. (2007) A gaussian mixture model
for mobile location prediction. 9th Int. Conf. Advanced Communication Technology, 2, 914–919.
[12] Gao, H., Tang, J. and Liu, H. (2012) Mobile Location Prediction in Spatio-Temporal Context. Proc. Mobile Data Challenge
by Nokia Workshop at the 10th Int. Conf. Pervasive Computing,
June. Nokia.
[13] Gidófalvi, G. and Dong, F. (2012) When and Where Next:
Individual Mobility Prediction. Proc. 1st ACM SIGSPATIAL
Int. Workshop on Mobile Geographic Information Systems,
MobiGIS’12, New York, NY, USA, pp. 57–64. ACM.

[14] de Montjoye, Y.-A., Hidalgo, C.A., Verleysen, M. and Blondel,
V.D. (2013) Unique in the crowd: the privacy bounds of human
mobility. Sci. Rep., 3.
[15] Song, C., Qu, Z., Blumm, N. and Barabsi, A.-L. (2010) Limits of
predictability in human mobility. Science, 327, 1018–1021.
[16] Zheng, Y., Zhang, L., Xie, X. and Ma, W.-Y. (2009) Mining
Interesting Locations and Travel Sequences from gps Trajectories. Proc. 18th Int. Conf. World Wide Web, WWW’09, New
York, NY, USA, pp. 791–800. ACM.
[17] Ying, J. J.-C., Lee, W.-C., Weng, T.-C. and Tseng, V.S. (2011)
Semantic Trajectory Mining for Location Prediction. Proc.
19th ACM SIGSPATIAL Int. Conf. Advances in Geographic
Information Systems, GIS’11, New York, NY, USA, pp. 34–43.
ACM.
[18] Yavas, G., Katsaros, D., Ulusoy, O. and Manolopoulos, Y. (2005)
A data mining approach for location prediction in mobile environments. Data Knowl. Eng., 54, 121–146.
[19] Giannotti, F., Nanni, M., Pinelli, F. and Pedreschi, D. (2007)
Trajectory Pattern Mining. Proc. 13th ACM SIGKDD Int. Conf.
Knowledge Discovery and Data Mining, KDD’07, New York,
NY, USA, pp. 330–339. ACM.
[20] Cao, H., Mamoulis, N. and Cheung, D.W. (2007) Discovery
of periodic patterns in spatiotemporal sequences. IEEE Trans.
Knowl. Data Eng., 19, 453–467.
[21] Candia, J., Wang, P., Schoenharl, T., Madey, G. and Barabasi,
A.-L. (2008) Uncovering individual and collective human
dynamics from mobile phone records. J. Phys. A: Math. Theor.,
41, 1–11.
[22] Ryder, J., Longstaff, B., Reddy, S. and Estrin, D. (2009) Ambulation: A tool for monitoring mobility patterns over time using
mobile phones. Int. Conf. Computational Science and Engineering 2009, CSE’09, August, pp. 927–931.
[23] Zheng, J. and Ni, L.M. (2012) An Unsupervised Framework for
Sensing Individual and Cluster Behavior Patterns from Human
Mobile Data. Proc. 2012 ACM Conf. Ubiquitous Computing,
pp. 153–162. ACM.

SECTION C: COMPUTATIONAL INTELLIGENCE, MACHINE LEARNING AND DATA ANALYTICS
THE COMPUTER JOURNAL, VOL. 59 NO. 6, 2016

IEEE International Conference on Social Computing / IEEE International Conference on Privacy, Security, Risk and Trust

Analyzing Sentiment Markers Describing Radical
and Counter-Radical Elements in Online News
Hasan Davulcu, Syed Toufeeq Ahmed, Sedat Gokalp

M’hamed H Temkit, Tom Taylor

Dept. of Computer Science and Engineering
Arizona State University, Tempe, AZ
hdavulcu@asu.edu, toufeeq@asu.edu, sgokalp@asu.edu

Department of Mathematics
Arizona State University, Tempe, AZ
htemkit@mathpost.asu.edu, tom.taylor@asu.edu

Mark Woodward

Ali Amin

Department of Religious Studies and CSRC
Arizona State University, Tempe, AZ
mataram@asu.edu

Center for Religious and Cross Cultural Studies
Gadjah Mada University, Yogyakarta, Indonesia
aleejtr77@yahoo.com

Abstract— In this study, we aim to obtain “natural groupings” of
151 local non-government organizations and institutions
mentioned in a news archive of 77,000 articles spanning a decade
(May 1999 to Jan 2010) from Indonesia. One of our goals is to
enhance our understanding of counter-radical movements in
critical locations in the Muslim world. We present information
extraction techniques to recognize entities, and their beliefs and
practices in text as a step towards identifying socially significant
scales with explanatory power. Then, we proceed to cluster
organizations based on these scales. We present experimental
results, and discuss challenges in reasoning with the complex
interactions of many simultaneous beliefs, practices and attitudes
held by the leaders and followers of various organizations.

and practices their leaders and followers are presented in
Section 3. Section 2 presents related work. Section 4 presents
our efforts for dimensionality reduction of attributes in order to
(i) overcome the sparsity problem encountered while text
processing, and (ii) group markers into meaningful and socially
significant categories with some explanatory power. In
Sections 4 and 5, we present various similarity metrics and
clustering techniques we considered and, analysis of the
clustering results. Social scientists on our team observed that
only four out of the eight clusters we identified correspond to
pure groups of radical or counter-radical organizations. Pure
radical clusters were easily identified due to high similarity
among their violent practices. Similarly, pure moderate clusters
were identified due their strong reactionary opposition to
violent practices through protests and rhetoric. We had four
other mixed clusters. Upon analyzing the profiles of the
organizations found within these mixed (radical and moderate)
clusters, we identified that attempting a binary labeling as
radical or counter-radical as a measure of cluster purity do not
capture the overlap, movement and interactivity among these
organizations. Specifically, counter-radicalism can manifest in
many forms; very few are secular or pro-Western, most support
flexible interpretations of Islamic texts, most support religious
tolerance, most support the empowerment of women, most
consider democracy to be a permissible form of government
and most reject religious practices that are strongly associated
with radical Islam. However, it is difficult to categorize many
organizations due to differences of opinion and practices
among their various factions and due to the utilization of nonviolent, but still radicalizing (non-secular, repressive, and nontolerant) practices and rhetoric of some others.

Keywords-component; Web information extraction, markers,
spectral clustering, scales, hierarchical clustering, organizations.

I.

INTRODUCTION

Many social network analysis [22] tools depend on a single
kind of affinity relationship, such as friendship, kinship, or
warfare among its actors. However, sociologists [23] assume
that, until proven otherwise, actors’ alliances are shaped by
complex interaction of many simultaneous beliefs, practices
and attitudes. In this study, we attempt to obtain “natural
groupings” of 151 non-government local organizations and
institutions mentioned in a news archive from Indonesia
spanning a decade (May 1999 to Jan 2010). One of our goals is
to enhance our understanding of counter-radical movements in
critical locations in the Muslim world. By design our study
leaves the meaning of moderate or counter-radical openended—except for a baseline understanding that moderates
reject violence as a means to political or social objectives. In
consultation with social scientists on our team, first we
identified a preliminary collection of 357 social markers as
significant attributes of an organization’s reported beliefs and
activities. These markers correspond to most frequently
occurring terror and violence related keywords (such as
bombings, kidnappings, and other violent crimes), keywords
related to social, legal, political activities (such as call for
reforms, protests, criminal and corruption related activities) and
lists of religious sects, beliefs and practices. The information
extraction methods for identifying organizations and sentiments
978-0-7695-4211-9/10 $26.00 © 2010 IEEE
DOI 10.1109/SocialCom.2010.55

II.

RELATED WORK

Large-scale information and sentiment extraction from web
is an active research area. Web information extraction systems
such as KnowItAll [1] extract facts and relationships from text
using set of extraction rules for each class and relation based on
a set of generic, domain independent templates. Pasca et al. [2]
utilized generalized contextual extraction patterns to extract
large amount of facts (of type person-born-in-year) from web.
Turney [3] used large number of co-occurrences of entities on
335

entities in unstructured text. Named Entity Recognition
(NER)[10] is a well researched area, and tools using latest
machine learning algorithms like Conditional Random Fields
have shown high accuracy in identifying named entities in
text. Typically in news articles, entities of interest are dates,
people, places, and organizations. For our task, we choose
Open-Calais 2 web API, one of the state-of-the-art NER
systems. Let us consider an example sentence3:

web as a measure to recognize synonyms. Seminal work [4],
[5] in event detection and tracking explored clustering
algorithms like agglomerative clustering augmented Group
Average Clustering. Well-known idf-weighted cosine
coefficient metric method [6] was also used to detect and track
topics, [7] used both text content and date information of news
articles. A real-time news event extraction system [8] extracts
violence and disaster events by processing the news article
using extraction grammars on each document. By first
extracting signature of the event, [10] tracked and attempted to
link related events with mentions of same event signature in
other incoming news articles, thereby forming a thread that
links all the news articles referring to specific events.
III.

Example sentence S1:
National Police chief Gen. Bambang Hendarso Danuri on
Tuesday said all Aceh-based fugitives were on the police’s
wanted list, for their possible involvement in several bombings
in Jakarta and Bali.

EXTRACTING SENTIMENT FEATURES FROM NEWS

Wealth of information locked in online daily news media
makes it a valuable resource to mine associations, and
competing sentiments and rhetoric about any reported entity or
topic. Web text mining presents a useful tool analyze these
associations at a large scale and quantify significant patterns
for further study or observations.. In our task of mining
sentiment on social markers, we implemented a robust and
scalable web text mining pipeline illustrated in Figure 1. Steps
of this pipeline are explained below.

The sentence after Named Entity Recognition:
National Police chief <PERSON>Gen. Bambang Hendarso
Danuri </PERSON> on Tuesday said all Aceh-based fugitives
were on the police’s wanted list, for their possible involvement
in several bombings in <LOCATION>Jakarta</LOCATION>
and<LOCATION> Bali</LOCATION>.
Tags <PERSON> marks the beginning of the entity name and
</PERSON> marks the end of the entity name. In this study
we focused on three types of entity names <PERSON>,
<LOCATION> and <ORGANIZATION>.
OpenCalais also provides co-reference resolution. Resolving
pronouns and anaphora in an article is crucial, since after the
first occurrence of a proper noun (say “John Doe”), the entity
name is usually referred to by its pronouns (“He”) or
anaphora (“Mr. Doe”). For the above example, if we follow
few sentences in the same article we find a sentence where a
pronoun “his” and a first name is used to refer to the person
entity “Gen. Bambang Hendarso Danuri”, as shown below:
Example sentence S2:
“Some of [the terrorist suspects] were also involved in the
Indonesia Stock Exchange [BEI] and Bali bombings,”
Bambang said during his visit to Aceh Besar.
After co-reference resolution:
“Some of [the terrorist suspects] were also involved in the
Indonesia Stock Exchange [BEI] and Bali bombings,” Gen.
Bambang Hendarso Danuri said during Gen. Bambang
Hendarso Danuri visit to Aceh Besar.

Figure1. Entity-marker association and sentiment extraction pipeline.

A. Data Collection and pre-processing
We crawled archives of the premier English online news
source in Indonesia, the Jakarta Post1 from May 1999 to Jan
2010, and collected 77,000 articles listed under the following
categories picked by our social scientists: National, Jakarta,
Bali, Archipelago, Islam, Opinion, and Violence. These
articles were converted to plain text by stripping HTML tags.

This co-resolution step makes it possible to extract further
associations from all the sentences and their clauses.
C. Extracting (collocation) associations around the entities
For every entity name (and its co-referent) we extracted their
associated markers corresponding to various beliefs and
practices. Using a windowing technique of size of seven words

B. Recognizing entities and practices in text
Before we can extract entity-marker association information
and sentiment (i.e. whether an organization is for or against a
certain practice or belief), we need to first identity named

2

OpenCalais: www.opencalais.com
Example article: www.thejakartapost.com/news/2010/03/16/acehterrorists-involved-hotel-bombings-says-national-police.html

3
1

The Jakarta Post: www.thejakartapost.com

336

of two categorical variables, the name of the organizations and
a preliminary labeling of the organizations by the social
scientists as either primarily radical(R) or counter-radical(C).
According to this labeling there are 67 radicals and 84 counter
radical organizations in our dataset. The 357 markers for each
organization were filled with integer values based on a simple
formula assessing the difference between the number of
supportive associations between an organization and a marker
and the number of opposing associations. This formula yielded
with 0’s to denote no recorded associations, positive values
indicating support and negative values indicating opposition.
We observed that the eventual 151 by 357 matrix between
organizations and markers had a high frequency of 0’s, which
became a challenge during the analysis. Next we discuss
dimensionality reduction through clustering of the markers to
obtain new data scales and subsequently clustering of the
organizations based on these scales.

to the left and right side of the entity name, close occurrence
information is used to establish connections between named
entities and markers. If there is a connection, then we associate
the marker with the closest entity. Alongside <PERSON>,
<LOCATION> and <ORGANIZATION> types, we also
created a list of keywords for <DEMOGRAPHIC> groups,
such as students, farmers, workers, police forces etc. The list
for demographic groups was created starting with a seed list of
keywords provided by social scientists on our team and
expanding the list through simple text patterns and relaxation
labeling techniques [23] suitable for entity identification in
text. Our final list for demographics currently includes 1,528
phrases. Let us consider the example sentence S1 again with
tagged markers and demographics:
Sentence S1:
National Police chief Gen. Bambang Hendarso Danuri on
Tuesday said all Aceh-based
<DEMOGRAPHICS>fugitives</DEMOGRAPHICS> were
on the<DEMOGRAPHICS>police’s </DEMOGRAPHICS>
wanted list, for their possible involvement in several
<MARKER>bombings </MARKER> in Jakarta and Bali.

B. Spectral clustering of the markers
Spectral clustering [15] has become one of the most modern
clustering algorithms and has very often outperformed
traditional clustering algorithms such as K-means [20].
Spectral clustering is based on similarity graphs, the graph
Laplacians and K-means algorithm to cluster the observations
or points represented in the new space formed by the specified
number of eigenvalues obtained from the graph Laplacian
derived from the graph similarity. Spectral clustering is very
well suited when the data is sparse and suitable for detecting
clusters in non-convex regions. After using spectral clustering
[16] on the 357 markers and looking at their plot of withincluster-sum-of-squares versus the potential number of
predicted clusters we observed that 20 clusters form the best
number of groups of markers. We further investigated the
quality of these 20 groups of markers by using graphical
methods such as scatter plots, their correlation to social
scientists’ classifications in the linear sense (Pearson[12]) and
in the general sense according to (Spearman[13]). Finally we
also used the Cronbach alpha coefficient [11] to assess the
internal consistency of these groups as scales. As a rule of
thumb a Cronbach alpha coefficient >= 0.70 is indication of a
good scale. Out of 20 clusters we had 11 good scales
according to Cronbach alpha coefficient. Next we shared these
20 groups of markers with social scientists. Social scientists
were able to organize 19 clusters containing 151 markers into
9 core clusters with very few (only 7) edits. We also had one
big group of markers with 206 entries which we did not have
many matches in our corpus and they were aggregated
together. The nine categories identified by social scientists
were related to: religiosity/piety, politics, war, corruption,
terror, violent crimes, protest and moderate attitudes. We
named these nine clusters of markers as our golden scales.
Figure 2 below shows a sample of the organization of the 20
marker clusters (integers corresponding to cluster id’s of each
marker) into 9 scales (capitalized) and the suggested edits (red
keywords highlighted in yellow) by domain experts.

An example sentence showing a practice:
A recent survey also found <MARKER>
polygamy</MARKER> was a significant factor behind the
country's rising divorce rate.
D. Sentiment Extraction
We also extracted sentiment toward a belief or practice by the
associated named entity. Starting with two seed lists of handbuild lexicon for support and dissent indicating phrases, we
expanded our lists by finding their synonyms/antonyms in
WordNet 4 and recursively adding them into corresponding
categories. Our final lists of support/dissent indicators include
2,457 and 2,735 keywords correspondingly. We extracted
sentiment polarity either as support, dissent or no data based on
the co-occurrence of a sentiment phrase within close proximity
of an entity-marker association. Consider the following
sentence:
“The <DEMOGRAPHICS> residents </DEMOGRAPHICS>,
who <DISSENT> opposed </DISSENT> the
<MARKER>eviction plan</MARKER>, said it would be an
insult for Muslims who see the place as sacred as it used to be
the grave of the ….”
IV.

CLUSTERING ORGANIZATIONS USING MARKERS

The main objectives of this paper are twofold: (i) to reduce
the large number of markers corresponding to various beliefs
and behaviors by constructing socially significant scales with
explanatory power, and (ii) utilizing these scales for clustering
organizations to identify their common traits and groupings.
A. Methods
The final dataset or Excel spreadsheet was comprised of 151
organizations by 357 markers. The columns were comprised
4

WordNet: wordnet.princeton.edu

337

+
+
0
+
+
+

0
0
0
0
0
0

religious

0
0
0
0
0
0

moderat
es

0
0
0
0
0
0

radical

protest

+
0
+
0
0
+

politics

terror

R
C
R
C
C
R

vio_crim
e
corrupti
on

war

ORGS
Org 1
Org 2
Org 3
Org 4
Org 5
Org 6

A RE-CODED SUBSET OF THE DATA WITH GOLDEN SCALES

CATEGO
RY

TABLE I.

0
0
0
0
0

0
0
0
0
0
0

0
0
0
+
0
0

0
0
0
0
0
0

D. Results
In Figure 4, we provide the dendogram resulting from the
hierarchical clustering of the categorical coded scale, using
minowski distance with p=1 and the ward method. In the leaf
nodes of the dendogram we notice the presence of several runs
of C and R type organizations. To determine the optimal
number of clusters we used the within cluster sum of squares
criterion or, cluster ratio of within sum of squares to between
sum of squares and looked at its plot against the potential
number of clusters in Figure 3 below. Upon analyzing Figure 3,
we can see that the within and between sum of squares ratio has
a significant drop at 8 clusters.

Figure2. Golden Scale organization of markers suggested by domain experts

This process reduced the dimensionality of the data and
aggregating the corresponding marker scores into golden scale
scores by adding them up, rendered the data less sparse by
deflating the rate of 0 scores.
C. Hierarchical Clustering and clustering the organizations
To meet our second objective, we tried clustering the
organizations through several metric distances and assessed
the quality of the resulting clustering by checking their purity
against the a priori labeling provided by the domain experts.
Purity measures how pure is the cluster i by weighting each
cluster proportional to its size and according to the numbers of
different organizations in it belonging to different categories.
In our clustering analyses we tried several metrics based on
the original data and recoded data to ordinal measurement {(negative sentiment), 0(neutral sentiment), +(positive
sentiment)} which was one of the ways to address the
discrepancy in ranges between the scales, as well as the use of
normalizing techniques on the original scores. The clustering
analyses used the package Hclust in R[18], which is based on
hierarchical clustering based on several choices of distances
such as Hamming, Euclidean and Minkowski and several
clustering methods such as single, complete or ward[19]. The
clustering results using Minkowski distance with p=1 and
ward method using the ordinal data fared better than other
distances and methods. Hamming distance did not take into
account the ordinal nature of the data and Euclidean distance
was more sensitive to outliers.

Figure 3. Within/Between Ratio vs. Number of Clusters

We provide the purity of the clustering based on the 8 clusters
identified in the form of a confusion matrix. This is presented
in the contingency table in Figure 4 by counting the numbers of
R and C type organizations found within each cluster.

338

Dist
Golden Scales

Cluster 3

Cluster 6

Cluster 8

Cluster 4

Cluster 7

Cluster 5

Cluster 2

Cluster 1

1

0

11

7

9

10

17

12

Radicals
Counter-Radicals
War
Terror
Protest
Violence/Crime
Corruption
Politics
Radical
Moderate
Religious

-

22
0
0
0
0
0
0
0
0
0

+
+
+
+
+
+
+
+
+

-

17
0
0
0
0
0
0
0
0
0

+
+
+
+
+
+
+
+
+

-

12
0
0
0
0
0
0
0
0
0

+
+
+
+
+
+
+
+
+

Figure 4. Dendogram of the hierarchical clustering of organizations

V.

-

7
0
0
0
0
0
0
0
0
0

+
+
+
+
+
+
+
+
+

-

8
0
0
0
0
0
0
0
0
0

+
+
+
+
+
+
+
+
+

-

18
0
0
0
0
0
0
0
0
0

+
+
+
+
+
+
+
+
+

-

0
0
0
0
0
0
0
0
0
0

+
+
+
+
+
+
+
+
+

-

0
0
0
0
0
0
0
0
0
0

+
+
+
+
+
+
+
+
+

Cluster 2: Radical
Quantitative Characteristics: Organizations in Cluster 2
support war/fighting and they support radical markers.
Semantic Characteristic: This cluster has extremist
militant and political forces.
Cluster 3: Counter-Radical
Quantitative Characteristics: Organizations in Cluster 3
have negative sentiment on war, no sentiment on
violence/crime scale.
Semantic Characteristics: This cluster hosts peaceful
counter-radical organizations addressing social issues.
Cluster 4: Mixed
Quantitative Characteristics: Mixed
Semantic Characteristics: Mostly political organizations
Cluster 5: Mixed
Quantitative Characteristics: Organizations in Cluster 5
share an opposing sentiment on terror and radical scales.
Semantic Characteristics: This cluster has less active
and/or localized smaller groups of organizations of intellectual
and elite classes.
Cluster 6: Counter-Radical
Quantitative Characteristics: Organizations in Cluster 6
have negative sentiment on war, opposing sentiments against
violent crimes.

EXPERIMENTAL ANALYSIS

In this section, we will explain the characteristics of each
cluster by analyzing the common traits of the organizations
found with a cluster, and also by analyzing the discriminatory
scales that would improve the purity of each cluster based on
the polarity of the sentiments {-(opposition), 0 (zero) or
+(support)} found within each cluster.
The table in Figure 4 highlights the discriminatory features of
the clusters. Bold polarities correspond to sentiments shared
by many members of a cluster. For example, the organizations
in Cluster 3 all share a negative sentiment on war related
markers.
We hope to shed more light on the purity and its lack thereof
within the eight clusters identified, here:
Cluster 1: Radical
Quantitative Characteristics: Organizations in Cluster 1
have supportive sentiment on war, mixed sentiments on terror
related activities and they support radical beliefs and practices.
Semantic Characteristics: This cluster contains extremely
radical and rebellious organizations with military capabilities
including some guerilla fighters that are listed in the US
terrorist blacklist.

339

[2]

Semantic Characteristics: This cluster contains counterradicals with more conservative views and a social issues
agenda with emphasis on human rights.
Cluster 7: Mixed
Quantitative Characteristics: Organizations in Cluster 7
have positive sentiment on war, non-positive (opposing)
sentiment on protest and radical scales.
Semantic Characteristics: This cluster has organizations
that are taking action against terror, violence and corruption. It
also has groups that are promoting tolerance in Islam as well
as some radical organizations.
Cluster 8: Mixed
Quantitative Characteristics: Organizations in Cluster 8
have are pro-war, pro-protest tendencies and mixed sentiments
on radical scales.
Semantic Characteristics: This cluster contains student
organizations or organizations with youth concentration taking
action for social rights with some mixed militarist tendencies.
VI.

[3]
[4]

[5]

[6]

[7]

[8]

[9]

CONCLUSIONS AND FUTURE WORK
[10]

In this paper we presented the results of information extraction
and data clustering techniques to obtain “natural groupings” of
151 local non-government organizations and their beliefs and
practices identified in a news archive of 77,000 articles
spanning a decade (May 1999 to Jan 2010) from Indonesia.
The study results indicate both challenges and opportunities.
Our information extraction algorithms mostly rely on
collocation statistics and hence they are language independent
for extracting organization-marker associations and
support/opposition sentiment polarities. Our study also points
to the critical role that social scientists play in guiding and
gauging the efforts of computer scientists in social computing
investigations. Most of the computational steps presented in
this paper, such as the generation of seed lists for various
information extraction tasks, and generation of scales for
clustering and accessing the quality of the clustering results,
were supervised or semi-supervised based on the input from
social scientists. Social scientists have great insights for
reasoning with the challenging dynamics of the social
organizations involving complex interaction of many evolving
beliefs and practices.

[11]
[12]

[13]
[14]

[15]

[16]
[17]

[18]
[19]
[20]

ACKNOWLEDGEMENT

[21]

This research was supported by US DoD’s Minerva Research
Initiative Grant Award: N00014-09-1-0815, Project leader:
Prof. Mark Woodward, Arizona State University, and the
project name is “Finding Allies for the War of Words:
Mapping the Diffusion and Influence of Counter-Radical
Muslim Discourse”.

[22]
[23]

[24]

REFERENCES
[1]

Etzioni, O., Cafarella, M., Downey, D., Kok, S., Popescu, A., Shaked,
T., Soderland, S., Weld, D. S., and Yates, A. 2004. Web-scale
information extraction in knowitall: (preliminary results). In Proc. of
WWW 2004, ACM, New York, NY, 100-110.

340

M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. Organizing and
searching the world wide web of facts - step one: the one-million fact
extraction challenge. In Proc. of AAAI-2006, 2006.
Turney, P. D. 2001. Mining the Web for Synonyms: PMI-IR versus LSA
on TOEFL. In Proc. ECML 2001, (September 05 - 07, 2001).
J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang, “Topic
detection and tracking pilot study: Final report,” in Proc. of the DARPA
broadcast news transcription and understanding workshop, vol.1998.
Citeseer, 1998.
Y. Yang, T. Pierce, and J. Carbonell, “A study of retrospective and onlineevent detection,” in Proc. of ACM SIGIR 1998 , New York, NY,
USA, 1998, pp. 28–36.
J. Schultz and M. Liberman, “Topic detection and tracking using
idfweighted cosine coefficient,” in Broadcast News Workshop’99
Proceedings. Morgan Kaufmann, 1999, p. 189.
Z. Li, B. Wang, M. Li, and W. Ma, “A probabilistic model for
retrospective news event detection,” in Proc. of ACM SIGIR 2005.
ACM New York, NY, USA, 2005, pp. 106–113.
H. Tanev, J. Piskorski, and M. Atkinson, “Real-Time News Event
Extraction for Global Crisis Monitoring,” in Proc. of NLDB 2008,
London, UK. Springer, 2008.
J. R. Finkel, T. Grenager, and C. D. Manning, “Incorporating non-local
information into information extraction systems by gibbs sampling.” In
ACL. The Association for Computer Linguistics, 2005.
S. T. Ahmed, R. Bhindwale, and H. Davulcu, “Tracking Terrorism News
Threads by Extracting Event Signatures ,” in Proc. Of ISI-2009. Dallas,
Texas, USA, 2009
Cronbach, L. J. (1951). Coefficient alpha and the internal structure of
tests. Psychometrika. 16, 297-334
Fisher, R.A. (1915). "Frequency distribution of the values of the
correlation coefficient in samples from an indefinitely large population".
Biometrika 10 (4): 507–521.
Hollander, D.A. Wolfe, "Nonparametric statistical methods" , Wiley
(1973)
4- Jianbo Shi and Jitendra Malik, "Normalized Cuts and Image
Segmentation", IEEE Transactions on Pattern Analysis and Machine
Intelligence, 22(8), 888-905, August 2000
Andrew Y. Ng, Michael I. Jordan, Yair Weiss, “On Spectral Clustering:
Analysis and an Algorithm”, Neural Information Processing Symposium
2001
http://bm2.genes.nig.ac.jp/RGM2/R_current/library/kernlab/man/specc.h
tml
Murtagh, F. (1985). “Multidimensional Clustering Algorithms”, in
COMPSTAT Lectures 4. Wuerzburg: Physica-Verlag (for algorithmic
details of algorithms used).
http://stat.ethz.ch/R-manual/R-patched/library/stats/html/hclust.html
Ward, J. 1963. Hierarchical grouping to optimize an objective function.
Journal of the American Statistical Association, 58:236-244.
Hartigan, J. A. and Wong, M. A. (1979). A K-means clustering
algorithm. Applied Statistics 28, 100–108.
Wasserman, S. and Faust, K. (1994) Social Network Analysis: Methods
and Applications (Structural Analysis in the Social Sciences).
Cambridge University Press.
Hanneman R. A., and Riddle M., Introduction to Social Network
Methods. Online textbook.
Shen, W., Li, X., and Doan, A. Constraint-Based Entity Matching.
Proceedings of the National Conference on Artificial Intelligence
(AAAI) (2005)
M. Rosell, V. Kann, and J.-E. Litton. Comparing comparisons:
Document clustering evaluation using two manual classifications. In
ICON, 2004.

2016 IEEE Tenth International Conference on Semantic Computing

“Climate Change” Frames Detection and
Categorization Based on Generalized Concepts
Saud Alashri

Jiun-Yi Tsai

Sultan Alzahrani

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, Arizona
Email: salashri@asu.edu

Hugh Downs School of
Human Communication
Arizona State University
Tempe, Arizona
Email: jtsai8@asu.edu

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, Arizona
Email: ssalzahr@asu.edu

Steven R. Corman

Hasan Davulcu

Hugh Downs School of
Human Communication
Arizona State University
Tempe, Arizona
Email: steve.corman@asu.edu

School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University
Tempe, Arizona
Email: hdavulcu@asu.edu

and metaphors). Increasingly, governments and international
communities are concerned about the security implications
of climate change as empirical research has documented that
climate change is linked to increased risk of violent conﬂict
[4]. For example, in May 2015, U.S. President Barack Obama
suggested that extreme weather is a threat to national security
and elevates the risk of global instability and conﬂict. Some
popular press adopted security threat frame to gain public attention. Therefore, systematic detection of news frames related
to climate change offers better understanding of stakeholders
and their competing perspectives.
Politicians have used framing on hotly debated issues to
shift public opinion, gain support and pursue their agenda.
A frame is the bundling of a component of oratory to urge
certain perceptions and to dishearten others [5]. Framing is
accomplished when a choice of words, expressions, subjects
and other logical gadgets support one understanding of an
arrangement of realities, and debilitate other interpretations.
One of those framed issues is climate change. Internet created
a public space for politicians and stakeholders to frame climate
change and related issues to push for their agenda. Online
tools such as blogsphere, microblogging and social media
streams have increased the availability of data on climate
change related debate and made it feasible for researchers to
analyze them.
Framing research requires qualitative analysis of a number
of texts by subject matter experts to identify and code a set
of frames. This is a time consuming process that does not
scale well. In order to address the scalability problem, machine
learning techniques can be utilized to detect and classify
frames. In this paper we propose a system for automatic
detection of frames in sentences in a climate change related
corpus, and map them to one of four expert identiﬁed frame

Abstract— The subliminal impact of framing of social,
political and environmental issues such as climate change
has been studied for long time in political science and
communications research. Media framing offers “interpretative
package” for average citizens on how to make sense of climate
change and its consequences to their livelihoods, how to deal
with its negative impacts, and which mitigation or adaptation
policies to support. A line of related work has used bag of
words and word-level features to detect frames automatically
in text. Such works face limitations since standard keyword
based features may not generalize well to accommodate surface
variations in text when different keywords are used for similar
concepts. In this paper, we develop a new type of textual
features that generalize (subject,verb,object) triplets extracted
from text, by clustering them into high-level concepts. We utilize
these concepts as features to detect frames in text. Our corpus
comprises more than 45,000 climate change related sentences.
Expert coders annotated those sentences as frame/non-frame and
framed sentences were mapped into one of four general frame
categories: solution, problem threat, cause, and motivation.
Compared to unigram and bigram based models, classiﬁcation
using our generalized concepts yielded better discriminating
features and a higher accuracy classiﬁer with a 12% boost (i.e.
from 74% to 83% in f-measure) for frame/no frame detection.
Keywords—Text mining, Frames Detection, Concepts, Big
Data, Climate Change, Natural Language Processing

I. I NTRODUCTION
Climate change has provoked heated debates on the global
political and media arenas. Media framing offers “interpretative package” for average citizens on how to make sense
of climate change and its consequences to their livelihoods,
how to deal with its negative impacts, and which mitigation
or adaptation policies to support [1], [2], [3]. News frames
encourage salient interpretation of debated issues through the
usage of rhetorical devices (e.g. words, repetitive phrases,
978-1-5090-0662-5/16 $31.00 © 2016 IEEE
DOI 10.1109/ICSC.2016.14

277

Fig. 1.

Example of merging two related concepts

[12].
Media representation of climate change plays a vital role
in shaping ongoing policy discourse, public perception and
attitudes. [14] suggests that prominent political actors frame
climate risk for their own purposes, and align frames with their
interests and perspectives through media feedback processes
of representing climate change risk. Studies have shown that
the lay people learn about climate change mainly through
consuming mainstream media news [15]. Consequently, [2]
argued news media framing can catalyze public engagement
and help trigger collective concern of climate change. Put
differently, media framing is a powerful tool to highlight
different aspects of the policy options, and promote speciﬁc
interpretations or evaluations that inﬂuence decision making
[16].
Existing typologies of climate change framing, focusing
on dichotomous categories, are limited by their inability to
link framing processes with movement interaction. We argue
that, in order to understand how the media reﬂect different
organizations interests in addressing climate change as a social
problem, it is necessary to supplement the social movement
focus on resource mobilization to framing processes of collective action problems. To do that, this study develops a nuanced
typology for studying climate change framing and its adequacy
for supporting a social movements that would be necessary to
overcome the collective action problem. Our typology provides
a holistic map to evaluate how climate change media framing
can enable appropriate social and policy actions that ultimately
can mitigate risks of social unrest. We apply this framework to
examine framing of climate change in media and social media
texts collected from the Niger Basin region over seven months
from August 2014 to February 2015, using a novel coding
technique to assess diagnostic, prognostic, and motivational
framing described by [17] as the keys to effective social
movements.

categories: solution, problem threat, cause, and motivation.
Our problem here can be described as a multi-level multi-class
classiﬁcation problem where we ﬁrst classify each sentence as
Frame or Non-Frame. Then, the Frame sentences are further
classiﬁed into one of four predeﬁned frame categories. In
particular, we show that generalized concepts and relations [6]
as features outperform classical textual features (e.g. uni-grams
and bi-grams) while detecting and categorizing Frame/NonFrame sentences. We experimented with SVM [7], Random
Forests [8] and sparse logistic regression [9] classiﬁers, and
identiﬁed sparse logistic regression as the best performing
classiﬁer for these tasks.
Generalized concepts approach extracts high-level information from text as relationships and concepts forming a
semantic network. It ﬁrst uses shallow semantic parser to generate POS tags to obtain semantic triplets (subject,verb,object)
from text. Next, it utilizes a bottom-up agglomerative clustering approach to merge and generalize those triplets into
concepts. In NLP, shallow parsing is the task of extracting the subjects, predicates or verb phrases, and objects.
Figure 1 shows how two related triplets could be merged
into a higher level generalized concept. In this ﬁgure, two
extracted triplets: action plan→build→sustainability and
policy →consolidate→sustainability are merged to form a
high level generalized concept and relationship as: {action
plan, policy}→{build, consolidate}→{sustainability} by discovering contextual synonyms such as {action plan, policy}
and {build, consolidate}. Here the deﬁnition of contextual
synonyms is not based on the one in the traditional dictionary.
Rather, they correspond to phrases that may occur in similar
semantic roles and associated with similar contexts. In Figure
1 the two triplets share the same object {sustainability} and
semantically similar verbs; hence, we can merge their subjects
{action plan,policy} as contextual synonyms.
II. R ELATED W ORK

B. Framing Research in Computer Science

A. Media Framing

[18] examined twitter stream on extracted frames and
pointed out a strong ties between frames collected from news
with the public opinions expressed in tweeter feeds. [19] went
further to distill agenda from news and link them to action.
Content analysis of frames in news is performed either by (1)
manual frame coding, that is done by trained coders, which is
costly as well as not scalable, or by (2) frame identiﬁcation
by using machine learning techniques that overcome human

Mainstream media serve as the main arena where international governments, social and political actors, scientists,
social movement organizations interact and make competing
claims about climate change issues [10]. Communication
surrounding climate change can inhibit or support science
and policy interactions, propagate consensus or disagreements
[11], and ultimately facilitate social change [12], [13], depending on how messages about climate change have been framed
278

Threat, Cause, Motivation}. Figure 2 shows our multi-level
multi-class classiﬁcation problem for a given sentence.
IV. M ETHODOLOGY
A. Overall System Model
The overall system consists of documents collected from
nearly 100 RSS feeds that are related to climate change in
the Niger Delta region. We also perform sentence splitting of
documents, identiﬁcation of key frames and their categories
by the coders, feature extraction (uni-grams, bi-grams, and
generalized concepts), identiﬁcation of discriminative features,
and a predictive model to detect and identify the frame
categories for sentences containing frame references.
B. Climate Change Corpus
Fig. 2.

Our climate change corpus is comprised of nearly 45, 054
sentences extracted from news and social media websites, that
are related to climate change topics in Niger Basin region
over a seven months period from August 2014 to February
2015. There are 16, 050 sentences coded as frame sentences
and 29, 004 as non-frame sentences by domain experts. Frame
sentences are further categorized into one of four categories:
Solution, Problem Threat, Cause, and Motivation.

Multi-level multi-class classiﬁcation

limitations by automatically detecting frames after training a
classiﬁer [20]. Many studies have addressed media framing
as a document classiﬁcation problem by building a learning
model to classify documents or paragraphs by utilizing different features. Aside from document level, [21], [22] examined
the classiﬁcation task at the sentence level and even at the
phrase level. Previous work on sentence level classiﬁcation
has focused on experimenting with different classiﬁers and
different features. [23] examined: bag of words, n-grams,
and topic models to classify news articles and map them to
a set of frames. Others, [24] employed POS-tags [25] and
named entities [26] as features to detect and classify frames.
Ceran et al [27] experimented with {subject,verb,object} based
features and benchmarked “ paragraph level” classiﬁer for
story detection against standard keyword based features, which
showed signiﬁcant improvement in classiﬁcation accuracy.
More advanced conceptual features engineering was developed
in [6] as they showed how generalized concepts performed
better in detecting stories in paragraphs. We utilize their generalized concepts as features to detect and categorize frames.
Our paper works on sentence level classiﬁcation compared to
their paragraph level. Also, our task is a multi-level multiclass classiﬁcation task where we ﬁrst examine if a sentence
contains a frame, and then we identify which one of four
frame categories it belongs to. Moreover, we developed tripleextraction techniques where we can extract more features
and incorporate a larger percentage of sentences into the
classiﬁcation model (i.e. 80% of sentences compared to 40%).

C. N-gram Features
We experimented with both uni-gram and bi-gram features.
We run a simple term frequency - inverse document frequency
(TF-IDF) [28] based technique on the entire corpus to generate
a large ranked list of, stopword eliminated, uni-grams and bigrams, and we experimented with them separately as features
in our classiﬁcation models.
D. Generalized Concepts Features
In [6], they extracted concepts from paragraphs where only
40% of the paragraphs generated concepts. In this paper, since
we are working on sentence level, we improved the concept
extraction approach, by extracting more triplets by utilizing
a larger number of triplet extractors and pre-processing their
output to include about 80% of the sentences in our experimental evaluations.
1) Triplets
Extraction:
In
order
to
extract
Subject,Verb,Object triplets, ﬁrst we run a pronoun resolver
[29], [30], [31], [32]. Since triplets extraction is an ongoing
research topic in NLP, we proceeded to use four state-ofthe-art triplets extraction tools: ClearNLP [33], Reverb [34],
Everest [35], AlchemyAPI [36] as complementary systems.
Additionally, any triplet slots with phrases were segmented
into keywords, stemmed, stop-word removed and their
cartesian product were produced as additional triplets.
2) Concepts Generation: Triplets extraction algorithms typically produce noisy and sparse triplets. Therefore, we apply
a hierarchical bottom-up clustering algorithm that generalizes
triplets into more meaningful relationships. To do so, we
employ both syntactic and semantic criteria that are based
on the corpus to generalize triplets into high level concepts

III. P ROBLEM D EFINITION
Given a set of documents {D1 ,...,DM } where each document contains one or more paragraphs. First, we split documents into sentences {S1 ,...,SN }. Next, using sentences as
data points, we aim to resolve whether a sentence Si contains
a frame or not. And, if the sentence contains a frame, then
we aim to identify its category, as one of: {Solution, Problem

279

without drift. In syntactic criteria, a pair of subjects-verbsobjects are merged only if they share common context related
to their different arguments (i.e. a pair of different subjects
are merged only if they co-occur with an identical verb-object
context).
Additionally, we capture contextual synonyms for subjects,
verbs and objects by deﬁning a semantic criterion which is
based on our corpus as well as WordNet [37]. Corpus-based
contextual synonyms for subjects, verbs and objects is based
on their common verb-object, subject-object and subject-verb
contexts respectively. Also, we capture contextual synonyms
that are not derivable from our corpus by applying WordNet
synonyms and hyponyms on the memebers of the concepts to
further expand and generalize them.
In order for the information to propagate between clusters of
subjects/objects and clusters of relations, we apply a hierarchical bottom-up clustering algorithm [38]. High level concepts
and relations are merged to form clusters. Each cluster is represented by graph of nodes and edges where nodes represent
concepts and edges represent relations between concepts. The
details of the above criteria and the generalization algorithm
are available in [6].

and non-smooth optimization problem. The features with nonzero values on the sparse x vector yield the discriminant
factors for classifying a sentence.
V. E XPERIMENTAL E VALUATION
A. Sentence Annotation
Our experts developed four categories of climate change
related frames as follows:
• Solution framing (prognostic): Covering the prognostic
function of deﬁning what should be done about problems,
solution framing refers to actions taken to prevent further
impact of climate change effects or further impact of
the causes of climate change such as greenhouse gas
emissions. Solutions can also emphasize ongoing measures to deal with existing effects of climate change.
Six frames capture an array of mitigation and adaptation
efforts conservation, education, investment, infrastructure
and development, creation or implementation of policy
and programs, and goal.
• Problem Threat framing (diagnostic): This diagnostic
framing class stresses on how climate change or outcomes
of climate change impact various actors, industries, human health, and the environment, Eight codes capture
negative consequences and threats brought by climate
change, including environmental systems and ecosystem,
public health, economic development, food security, water scarcity, national security, social unrest, and general or
multiple impacts. Both cause framing and problem/threat
framing comprise the diagnostic function in deﬁning
social problems.
• Cause framing: This group of diagnostic frames focus on
attributing the blame for causing climate change to either
human activity, natural variation or other reasons. Six
subcategories captured different explanations for causal
attribution of climate change: (a) human activity, (b)
natural variation, (c) scientiﬁc uncertainty, (d) policy
causes, (e) insufﬁcient actions, and (f) human disruption
to mitigate climate change impact.
• Motivation framing (motivational): Motivational framing refers to statements that explicit call for deﬁnitive
course(s) of action and explain why the audience should
make an effort to enact solutions [17]. In other words,
motivational frames elaborate on the rationale for action
that goes beyond diagnosis and prognosis, and include
vocabularies of severity, urgency, efﬁcacy, and propriety
[40]. We added a general category to analyze statements
that call for actions without providing readers with abovementioned reasons.
We assigned sentence annotation to three different expert
coders where we break ties by using the majority vote.

E. Frame Classiﬁcation
To classify each sentence as Frame/Non-Frame and identify
its relevant frame category we utilize sparse learning framework [9], with the underlined motivation to select a subset
of discriminating concepts that can (1) identify sentences
containing frame references and (b) classify a sentence into
a frame category. The following steps describe our algorithm:
1) Generate features from the entire corpus
2) Filter the features × sentences matrix to include only
resultant generalized concepts/features
3) Formulate the problem in a general sparse learning
framework [9]. In particular, the logistical regression
formulation presented below ﬁts this application, since it
is a dichotomous frame classiﬁcation problem (e.g. each
sentence classiﬁed as Frame/Non-Frame), and multiclass classiﬁcation problem (e.g. each Frame sentence
is further classiﬁed as one of four frame {Solution,
Problem Threat, Cause, and Motivation}):

minx

m


wi log(1 + exp(−yi (xt ai + c))) + λ|x|

(1)

i=1

In formula (1), ai is the vector representation of the ith
sentence, wi is the weight assigned to the ith sentence
(wi = 1/m by default), and A = [a1 , a2 , . . . , am ] is the
features × sentences matrix, yi is the label of each sentence,
and the xj , the j th element of x, is the unknown weight for
each feature, (λ > 0) is a regularization
 parameter that controls
|xi | is 1-norm of the x
the sparsity of the solution, |x|1 =
vector. We used the SLEP [39] sparse learning package that
utilizes gradient descent approach to solve the above convex

B. Quantitaive Evaluation
Once sentences are labeled as Frame/Non-Frame and categorized with their corresponding frame category, we utilize
uni-gram keywords, bi-gram terms, and generalized concepts
separately as features and the sparse logistical regression

280

TABLE I
F RAME /N ON -F RAME C LASSIFICATION
Method

Concepts

Bi-grams

Class Label

Precision

Recall

F-measure

Frame

0.80

0.88

0.84

Non Frame

0.87

0.77

0.82

Method

Concepts

Frame Category

Precision

Recall

F-measure

Solution

0.75

0.93

0.83

Problem Threat

0.77

0.84

0.79

Average

0.83

0.83

0.83

Cause

0.85

0.77

0.80

Frame

0.75

0.42

0.54

Motivation

0.89

0.62

0.73

0.82

0.79

0.79

Non Frame

0.74

0.92

0.82

Average

Average

0.74

0.67

0.68

Solution

0.87

0.77

0.81

0.59

Problem Threat

0.84

0.77

0.80

Cause

0.86

0.73

0.76

Motivation

0.90

0.58

0.71

Average

0.87

0.71

0.77

Solution

0.78

0.87

0.82

Problem Threat

0.81

0.81

0.81

Cause

0.83

0.62

0.82

Motivation

0.85

0.57

0.64

Average

0.82

0.72

0.77

Frame
Uni-grams

TABLE II
F RAME C LASSIFICATION INTO FOUR CATEGORIES

Non Frame
Average

0. 75
0.76
0.75

0.48
0.91
0.70

Bi-grams

0.89
0.74

classiﬁer SLEP [39] to identify weighted discriminative features and classify sentences. We experimented with three
different classiﬁers (SVM [7], Random Forests [8]) and found
that SLEP outperformed both these other classiﬁers. Using
different types of features generated from the entire corpus, we
perform ten-fold cross-validation for measuring the classiﬁer’s
predictive accuracy to detect Frame/Non-Frame sentences.
Next, using features generated from frame sentences only, we
train a multi-class model to classify sentences into their corresponding frame category. We report precision, recall, and Fmeasure as quantitative evaluation metrics. Qualitative analysis
of the identiﬁed discriminating concepts is also presented in
the next section.
Table 1 presents the accuracies for detecting Frame/NonFrame sentences using different features. Using generalized
concepts approach as features, the resultant average accuracy
(F-measure of 83%) outperforms both accuracies with unigrams (74%) and bi-grams (68%) features by 12% and 22%
respectively.
Table 2 shows the accuracies for identifying the corresponding frame category. Using generalized concepts, these
accuracies vary between 73% and 83% (F-measure) for different categories. In this table, utilizing generalized concepts
yields slightly better performance compared to both uni-grams
and bi-grams with an overall average accuracy (F-measure) of
79%.

Uni-grams

associated global warming with carbon dioxide emissions
using the following triplets to construct a cohesive story:
• Scientiﬁc research indicate that atmospheric carbon dioxide increase at a large level.
• Cars and trucks were major sources of air pollution and
carbon dioxide emissions, which directly increased local
temperature.
2) Problem Threat Framing: Next, we turned our attention
to identify the dominant concepts representing the problem
and threat framing of climate change. Media texts tended
to highlight devastating environmental impacts caused by
climate change, such as ﬂoods, prolonged drought, loss of
landmass and soil, desertiﬁcation, sea-level rise, storm surge,
heat waves, and more. Flooding, in particular, is a severe
concern as nine out of sixteen triplets of high weigh values
explicitly mentioned the negative impacts of heavy rainfall
or torrential rain. Consequently, economic condition and food
insecurity were inﬂuenced, infrastructure was damaged, and
health diseases were exacerbated with the increased intensity
and frequency of ﬂoods.
3) Solution Framing: The most representative discourse of
solution framing is discussed next in Section D.
4) Motivation Framing: When discussing motivation for
why policy actors and citizens should act upon, the most
salient concepts emphasized that international communities
(e.g. U.S., EU, and China) should negotiate a legal agreement
to reduce greenhouse gas emissions at the end of 2015.
There is little attention to stating speciﬁc reasons for offering localized adaptation strategies that people can undertake.
Although the awareness of climate change impacts among
African government ofﬁcials was generally high, the prevailing
generalized concept of calling for international actions on
mitigation from mainstream media discourse reﬂected a lack

C. Qualitative Analysis of Resultant Concepts
Table III shows top ﬁve discreminative concepts for each
frame category. Our team of experts explored the highly
signiﬁcant generalized concepts germane to four-class framing
in media discourse surrounding climate change across West
African RSS feeds and provided qualitative evaluations as
follows:
1) Cause Framing: Causal responsibility of climate change
and its effects was often attributed to anthropogenic activities, particularly man-made greenhouse gas emissions, humaninduced pollution, and fossil fuel use. Carbon dioxide and
greenhouse gas emission emerged as highly signiﬁcant concepts, as indicated by high weigh value. Media texts often

281

TABLE III
T OP FIVE GENERATED CONCEPTS FOR EACH FRAME CATEGORY
Cause

Problem Threat

Solution

Motivation

{Greenhouse,Emissions,Gases}
↓
{Cause,Attribute to}
↓
{Global warming}

{Flood}
↓
{Associate,Create}
↓
{Poverty,Disease}

{Action plan,Policy}
↓
{Build,Consolidate}
↓
{Sustainability,Resilience
future}

{International,Community}
↓
{Urge,Warn}
↓
{Threat}

{Industry,Anthropogenic}
↓
{Raise}
↓
{Earth temperature,CO2,CO5}

{Heavy rainfall, Torrential rain}
↓
{Create,Bring,Increase}
↓
{Flooding,Disaster,Landslide}

{Development,
Sustainability,National
program}
↓
{Enhance}
↓
{Community}

{Agreement,Leaders,World}
↓
{Help}
↓
{Future,Hope}

{Fossil fuel}
↓
{Impact,Harm}
↓
{Planet,Environment,Weather}

{Drought}
↓
{Cause,Impact,Reduce}
↓
{Food-shortage,Foodproduction,Crop}

{Brown}
↓
{Sign}
↓
{Local legislation, CA
groundwater,Management
framework}

{USA,EU,China}
↓
{Recognize,Reduce}
↓
{Emissions}

{Coal
combustion,Diesel,Man-Made}
↓
{Create}
↓
{Extreme
weather,Temperature-up}

{Sea-level rise}
↓
{Result in,Cause}
↓
{Tsunami,Damage,Flood}

{Sustainability,Energy}
↓
{Can help,Improve}
↓
{Food security,Households}

{Truck,Car}
↓
{Rise}
↓
{Carbon pollution,Pollute}

{Extreme Weather, Hailstorm}
↓
{Cause,Affect}
↓
{Mudslide,Floods,Farming}

{Smart agriculture,Africa
countries}
↓
{Meet,Breathe}
↓
{Life}

of effective national and local polices.

{Africa}
↓
{Need,Implement}
↓
{Policy,Awareness,Partnership}

{Nigerian}
↓
{Apply,Take}
↓
{Measures,Renewable
Energy,Policy}

actions) can enhance local community resilience. According to
the IPCC (Intergovernmental Panel on Climate Change) report,
majority of rural communities rely on rain-fed agriculture
to sustain their livelihoods in West Africa, the region worst
affected by climate change. With changing rainfall patterns,
prolonged droughts and ﬂooding, sustainable system of developing agriculture-smart technologies can help improve food
security at the household level. Interestingly, the African media
discussed that California Governor Jerry Brown has signed the
most signiﬁcant framework for regulating underground water
resources to achieve sustainable development in September,
2014.

D. Visualizing Concepts
To visualize the generalized concept and relation clusters,
we utilize a semantic network [41] of nodes (V) and edges
(E) to describe the semantic space of the underlying texts.
Circle nodes represent subjects/objects and square nodes represent verbs. Edges represent relations between concepts. In
such a network, distinct combinations of actors (subjects)
perform or recommend various sets of actions (verbs) on
distinct combinations of targets (objects). The sample semantic
network in Figure 3 (next page) illustrates how sustainability
emerges as a concept that is central to addressing climate
change impacts. The semantic network represents the contextual relationships between generalized triplets relating to
strategies for sustainable adaptation. In the media discourse,
sustainable adaptation is predominantly framed as an effective
solution to reduce impacts of climate change and contribute to
social, economic, and environmental development. As shown
in Figure 3, developing sustainable national programs (or

VI. C ONCLUSION AND F UTURE W ORK
Climate change framing has pervasive inﬂuence, and this paper presents a new computational approach based on generalized concepts to identify popular media frames and map them
to different categories: solution, problem threat, cause, and
motivation. A line of related work has used bag of words and
word-level features to detect frames automatically in text. Such

282

Fig. 3.

A sample semantic network of frame concepts

work face limitations since standard keyword based features
may not generalize well to accommodate surface variations in
text when different keywords are used for similar concepts. In
this paper, we developed a new type of textual features that
generalize (subject,verb,object) triplets extracted from text,
by clustering them into high-level concepts. Compared to
unigram and bigram based models, frame classiﬁcation using
our generalized concepts yielded better discriminating features
with a 12% boost in accuracy (i.e. from 74% to 83% in fmeasure) for frame/no frame detection. In our future work, we
plan to utilize discriminating generalized concepts indicating
actor-action-target sequences to infer causal chains of events,
frames, and actions that might lead to better indicators of
climate-change related social unrest.

[4] J. Barnett and W. N. Adger, “Climate change, human security
and violent conﬂict,” Political Geography, vol. 26, no. 6, pp. 639
– 655, 2007, climate Change and Conﬂict. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S096262980700039X
[5] S. Alashri, S. Alzahrani, L. Bustikova, D. Siroky, and H. Davulcu, “What
animates political debates? analyzing ideological perspectives in online
debates between opposing parties,” in Proceedings of the ASE/IEEE
International Conference on Social Computing (SocialCom-15), 2015.
[6] B. Ceran, N. Kedia, S. Corman, and H. Davulcu, “Story detection using
generalized concepts and relations,” in Proceedings of International
Symposium on Foundation of Open Source Intelligence and Security
Informatics (FOSINT-SI), in conj. with IEEE ASONAM, 2015.
[7] C. Cortes and V. Vapnik, “Support-vector networks,” Machine learning,
vol. 20, no. 3, pp. 273–297, 1995.
[8] L. Breiman, “Random forests,” Mach. Learn., vol. 45, no. 1, pp.
5–32, Oct. 2001. [Online]. Available: http://dx.doi.org/10.1023/A:
1010933404324
[9] J. Liu, J. Chen, and J. Ye, “Large-scale sparse logistic regression,” in
Proceedings of the 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, ser. KDD ’09. NY, USA:
ACM, 2009, pp. 547–556. [Online]. Available: http://doi.acm.org/10.
1145/1557019.1557082
[10] S. Hilgartner and C. L. Bosk, “The rise and fall of social problems: A
public arenas model,” American journal of Sociology, pp. 53–78, 1988.
[11] M. Hulme, Why we disagree about climate change: Understanding
controversy, inaction and opportunity. Cambridge University Press,
2009.
[12] M. T. Boykoff, Who speaks for the climate?: Making sense of media
reporting on climate change. Cambridge University Press, 2011.
[13] S. C. Moser and L. Dilling, Creating a climate for change. Cambridge
University Press, 2006.
[14] A. Carvalho, “Ideological cultures and media discourses on scientiﬁc
knowledge: re-reading news on climate change,” Public understanding
of science, vol. 16, no. 2, pp. 223–243, 2007.
[15] R. J. Brulle, J. Carmichael, and J. C. Jenkins, “Shifting public opinion on
climate change: an empirical assessment of factors inﬂuencing concern
over climate change in the us, 2002–2010,” Climatic change, vol. 114,
no. 2, pp. 169–188, 2012.

ACKNOWLEDGMENT
Some of the material presented here was sponsored by
Department of Defense and it is approved for public release,
case number:15-467.
R EFERENCES
[1] D. Chong and J. N. Druckman, “A theory of framing and
opinion formation in competitive elite environments,” Journal of
Communication, vol. 57, no. 1, pp. 99–118, 2007. [Online]. Available:
http://dx.doi.org/10.1111/j.1460-2466.2006.00331.x
[2] M. C. Nisbet, “Communicating climate change: Why frames matter for
public engagement,” Environment: Science and Policy for Sustainable
Development, vol. 51, no. 2, pp. 12–23, 2009. [Online]. Available:
http://dx.doi.org/10.3200/ENVT.51.2.12-23
[3] A. Shehata and D. N. Hopmann, “Framing climate change,” Journalism
Studies, vol. 13, no. 2, pp. 175–192, 2012. [Online]. Available:
http://dx.doi.org/10.1080/1461670X.2011.646396

283

[16] R. M. Entman, “Framing: Towards clariﬁcation of a fractured paradigm,”
McQuail’s reader in mass communication theory, pp. 390–397, 1993.
[17] R. D. Benford and D. A. Snow, “Framing processes and social movements: An overview and assessment,” Annual review of sociology, pp.
611–639, 2000.
[18] S. M. Jang and P. S. Hart, “Polarized frames on climate change and
global warming across countries and states: evidence from twitter big
data,” Global Environmental Change, vol. 32, pp. 11–17, 2015.
[19] K. Stalpouskaya and C. Baden, “To do or not to do: the role of agendas
for action in analyzing news coverage of violent conﬂict,” ACL-IJCNLP
2015, p. 21, 2015.
[20] B. Burscher, D. Odijk, R. Vliegenthart, M. de Rijke, and C. H. de Vreese,
“Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis,” Communication
Methods and Measures, vol. 8, no. 3, pp. 190–206, 2014.
[21] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,”
arXiv preprint arXiv:1408.5882, 2014.
[22] T. Wilson, J. Wiebe, and P. Hoffmann, “Recognizing contextual polarity
in phrase-level sentiment analysis,” in Proceedings of the conference on
human language technology and empirical methods in natural language
processing. Association for Computational Linguistics, 2005, pp. 347–
354.
[23] D. Odijk, B. Burscher, R. Vliegenthart, and M. De Rijke, “Automatic
thematic content analysis: Finding frames in news,” in Social Informatics. Springer, 2013, pp. 333–345.
[24] E. Baumer, E. Elovic, Y. Qin, F. Polletta, and G. Gay, “Testing and
comparing computational approaches for identifying the language of
framing in political news.” in HLT-NAACL, 2015.
[25] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer, “Featurerich part-of-speech tagging with a cyclic dependency network,” in
Proceedings of the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on Human Language
Technology-Volume 1. Association for Computational Linguistics, 2003,
pp. 173–180.
[26] J. Finkel, T. Grenager, and C. Manning, “Incorporating non-local information into information extraction systems by gibbs sampling,” in
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, 2005,
pp. 363–370.
[27] B. Ceran, R. Karad, A. Mandvekar, S. R. Corman, and H. Davulcu,
“A semantic triplet based story classiﬁer,” in Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International
Conference on. IEEE, 2012, pp. 573–580.
[28] J. A. Hartigan and M. A. Wong, “Algorithm AS 136: A k-means
clustering algorithm,” Applied Statistics, vol. 28, no. 1, pp. 100–108,
1979. [Online]. Available: http://dx.doi.org/10.2307/2346830
[29] K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers, M. Surdeanu,
D. Jurafsky, and C. Manning, “A multi-pass sieve for coreference
resolution,” in Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational
Linguistics, 2010, pp. 492–501.
[30] H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu, and
D. Jurafsky, “Stanfords multi-pass sieve coreference resolution system
at the conll-2011 shared task,” CoNLL 2011, p. 28, 2011.
[31] H. Lee, A. Chang, Y. Peirsman, N. Chambers, M. Surdeanu, and
D. Jurafsky, “Deterministic coreference resolution based on entitycentric, precision-ranked rules,” Comput. Linguist., vol. 39, no. 4, pp.
885–916, Dec. 2013.
[32] M. Recasens, M. C. de Marneffe, and C. Potts, “The life and death of
discourse entities: Identifying singleton mentions,” in Proceedings of the
2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, 2013, pp.
627–633.
[33] J. D. Choi, “Optimization of natural language processing components for
robustness and scalability,” Ph.D. dissertation, University of Colorado at
Boulder, 2012.
[34] A. Fader, S. Soderland, and O. Etzioni, “Identifying relations for open
information extraction,” in Proceedings of the Conference of Empirical
Methods in Natural Language Processing (EMNLP ’11), Edinburgh,
Scotland, UK, July 27-31 2011.
[35] (2013) Everest triplet extraction. Next Century Corporation. [Online]. Available: https://github.com/NextCenturyCorporation/
EVEREST-TripletExtraction

[36] (2015) Alchemyapi language features. AlchemyAPI, Inc. [Online].
Available: http://www.alchemyapi.com/products/alchemylanguage
[37] (2010) About wordnet. Princeton University. [Online]. Available:
http://wordnet.princeton.edu
[38] S. Kok and P. Domingos, “Extracting semantic networks from text via
relational clustering,” in Proceedings of the 2008 European Conference
on Machine Learning and Knowledge Discovery in Databases - Part I,
2008, pp. 624–639.
[39] J. Liu, S. Ji, and J. Ye, SLEP: Sparse Learning with Efﬁcient
Projections, Arizona State University, 2009. [Online]. Available:
http://www.public.asu.edu/∼jye02/Software/SLEP
[40] R. D. Benford, “” you could be the hundredth monkey”: Collective
action frames and vocabularies of motive within the nuclear disarmament
movement,” Sociological Quarterly, pp. 195–216, 1993.
[41] M. R. Quillian, “Semantic memory,” in Semantic Information Processing, 1968, pp. 227–270.

284

SocialCom/PASSAT/BigData/EconCom/BioMedCom 2013

Spatio-Temporal Signal Recovery from Political
Tweets in Indonesia
Anisha Mazumder, Arun Das, Nyunsu Kim, Sedat Gokalp, Arunabha Sen, Hasan Davulcu
School of Computing, Informatics and Decision Systems Engineering
Arizona State University
Tempe, Arizona - 85287
Email: {Anisha.Mazumder, adas22, nkim30, Sedat.Gokalp, asen, hdavulcu}@asu.edu
time. For this analysis a region corresponds to a province of
Indonesia. Finally, from the Radicalization Index and Location
Index of individuals, Heat Index of a region , which is a
composite measure of the number of radical tweeters of that
region and their ‘degree of radicalism’, is computed.
In our model we have a set of tweeters (or users), U =
{U1 , U2 , . . . , Un }. Each user Ui , 1 ≤ i ≤ n creates a set of
tweets Ti = {Ti,1 , Ti,2 , . . . , Ti,t
in}. The set of all tweets by
all users is denoted by T = i=1 Ti . The geographic area
from where the tweets originate is divided into a set of regions
R = {R1 , R2 , . . . , Rm }. In our study m is equal to thirty four,
the number of provinces and special administrative regions of
Indonesia. Each user Ui , 1 ≤ i ≤ n has a home location
HLi , 1 ≤ i ≤ n associated with her, which may or may not
be declared. Each tweet Ti,k , 1 ≤ i ≤ n, 1 ≤ k ≤ ti has a
geo-location GLi,k , 1 ≤ i ≤ n, 1 ≤ k ≤ ti associated with
it. However, GLi,k for some tweets Ti,k may not be known
as the user Ui might turn her GPS off. Accordingly, we can
divide the set of users in four different classes:
(i) Class 1: user Ui whose home location is declared and
geo-location of at least one tweet is known,
(ii) Class 2: Ui whose home location is not declared and
geo-location of at least one tweet is known,
(iii) Class 3: Ui whose home location is declared and geolocation of none of the tweets are known, and
(iv) Class 4 : Ui whose home location is not declared and
geo-location of none of the tweets are known.
From the input data set (U, T, R ), we compute, (i) Location
Index, Li of each user Ui , 1 ≤ i ≤ n, (ii) Radicalization Index,
RDi of each user Ui , 1 ≤ i ≤ n, and ﬁnally, combining Li
and RDi , we compute (iii) Heat Index, Hj of each region
Rj , 1 ≤ j ≤ m. It may be noted that whereas RDi , 1 ≤ i ≤ n
is a scalar value, Li is a vector of size m, (Li,1 , . . . , Li,m ),
where Li,j indicates the probability of user Ui being located
in region Rj i.e. Li,j indicates the probability of the Actual
home location of Ui being Rj . Finally, the HeatIndex Hj of
n
region Rj , 1 ≤ j ≤ m is computed as Hj = i=1 RDi ×
Li,j , ∀j, 1 ≤ j ≤ m. We thus provide a generic technique for
generating time-varying political Heat Maps of a geographical
region based on the Twitter data analysis. Throughout this
paper we have used ‘region ’ and ‘location’ interchangeably
to mean an ‘Indonesian Province’. It is to be noted that for
our calculations, we have considered all Indonesian provinces

Abstract—Online social network community now provides an
enormous volume of data for analyzing human sentiment about
people, places, events and political activities. It is becoming
increasingly clear that analysis of such data can provide great
insights on the social, political and cultural aspects of the participants of these networks. As part of the Minerva project, currently
underway at Arizona State University, we have analyzed a large
volume of Twitter data to understand radical political activity in
the provinces of Indonesia. Based on analysis of radical/counter
radical sentiments expressed in tweets by Twitter users, we
create a Heat Map of Indonesia which visually demonstrates the
degree of radical activities in various provinces of Indonesia.
We create the Heat Map of Indonesia by computing (i) the
Radicalization Index and (ii) the Location Index of each Twitter
user from Indonesia, who has expressed some radical sentiment
in her tweets. The conclusions derived from our analysis matches
signiﬁcantly with the analysis of Wahid Institute, a leading
political think tank of Indonesia, thus validating our results.
Index Terms—radical, tweet, Radicalization Index, Location
Index, Heat Map

I. I NTRODUCTION
The sheer popularity of online social media nowadays is
reﬂected by the immense amount of data being fed every
second by people from all over the world. It is becoming
increasingly evident that analysis of this huge online dataset
can provide great insights on the social, political and cultural
aspects of the Twitter users and possibly the non-Twitter
users as well. In this study, we have developed a tool for
recovering spatio-temporal signals from tweets generated in
Indonesia. Our interest in analyzing tweets from Indonesia
developed in the context of the Minerva1 project, currently
underway at Arizona State University. The goal of this project
is to increase the understanding of movements within Muslim
communities towards radicalism or counter radicalism. Based
on the support and opposition of certain beliefs and practices
of an individual (as expressed in her tweet), we can assign a
Radicalization Index to that individual. In addition, from the
self declared home location of a Twitter user and the locations
of her tweets, we can compute a distribution of Location
Index for that user. The map of Indonesia is divided up into
a set of regions and the Location Index of a user provides the
probability of the user to be in a speciﬁc region at a speciﬁc
1 This research was supported in part by US DOD Minerva Research
Initiative grant N00014-09-1-0815.

978-0-7695-5137-1/13 $26.00 © 2013 IEEE
DOI 10.1109/SocialCom.2013.46

280

including special administrative regions such as Yogyakarta
and special capital region such as Jakarta and we have ignored
Class 4 users.

Fig. 1: The ﬂow diagram of our Heat Map computation technique. The Web data mentioned here refers to the documents
generated by crawling the web pages of radical and counter
radical organizations of Indonesia.

II. R ELATED W ORK
Identiﬁcation of the location of users using Twitter data has
been quite a focus of recent research ([11], [12]). [4], [5]
combine location information and text from social-network
data history to infer user preferences and provide recommendations. However, we do not rely on any ‘checking in’
information for our computations. ‘Geo-coding’ (the use of
gazetteers) is applicable to our problem since we employ the
notion of regions. Following [3], we too argue that location
estimates are multi-modal probability distributions, rather than
particular points or regions. However, it may be noted that in
contrast to [3], our estimate of the location of the user must
be the probability of each Indonesian province as the Actual
home location of the user under consideration, rather than the
probability of the user being located anywhere on the surface
of the earth. In this study, we have developed a simple yet
effective means of computing the geo-location of the user as
compared to other more complex methodologies such as Topic
Detection Techniques [16], [17].
Human mobility is modeled as a stochastic process in [6].
In [1], the authors study the manner in which the movements
of human beings are related to time of the day, geography
as well as social ties. Similar problems have been studied
by [7], [8]. However, in our problem, there is no notion of
prediction of location of users involved. Besides, we consider
categorical distribution but we apply the concept of mixture
of distributions in the lines of [1]. Another line of research
which focuses on location estimation by content-analysis of
the tweets of a user has been studied by [9], [10]. However, in
this current work we rely on the geo-location containing tweets
of users and their declared home location to obtain the location
distribution of the users. In [13], the authors analyze tweets
generated during the United Kingdom 2010 General Election
to infer the political afﬁliation of a user based on her tweets.
We also study a similar problem, however our goal is not to
identify the political afﬁliations of users, rather we compute
the ‘degree of radicalism’ of the user. Besides, unlike them,
we apply a very simple yet effective term-frequency analysis
of tweets and leverage heavily on our team of domain experts.
The work in [30] which is followed by [14] is very relevant
to our technique of Radicalization Index assignment to users.
These works speciﬁcally focus on presenting a framework for
combining entity matching techniques for detecting extremist
behavior on discussion boards. Identiﬁcation and analysis of
such weak signals of radicalism by the ‘lone wolf terrorists’
through the use of topic-ﬁltered web harvesting as well as
application of natural language processing techniques, thereby
fusing aliases for identifying the person form the basis of the
works of [30]. Their work is fundamentally different from ours
because we deal speciﬁcally with the users’ publicly available
tweets only - this eliminates the availability of the vital background information such as characteristic ( ‘radical internet

forum’, ‘capability internet forum’ ) annotation of particular
discussion boards that is leveraged in [30] .Furthermore, [30]
and [14] do not deal with location proﬁling of users which is
one of the two major goals of our work.
III. M OTIVATION AND D ISTINGUISHING F EATURES OF
OUR WORK

The goal of our research is to create a visual description
of the spatio-temporal distribution of the radical population
of Indonesia by recovering political signals from Twitter data.
A ﬂow diagram of our methodology is provided in Figure
1. In [2], the authors retrieve road-kill signals from Twitter
data using human beings as sensors. Like [2], we too use
human beings as sensors to the extent that we use tweets
of Indonesian people to infer radicalism Heat Indices of the
provinces of Indonesia. However, unlike [2], ﬁrst we intend to
ﬁnd the distribution of (radical) individuals, so we should not
factor in any ‘human population bias’ i.e variation of densities
of people across the different provinces of Indonesia. Second,
our problem is much more complex because we not only need
to know from which location have the radical tweets come
in greater number, but also the ‘degree of radicalism’ of the
tweets - so we need to comprehend the sentiment of the tweets.
So, questions of interest for us are(Qs1) the ‘degree of radicalism’ of tweet tw
(Qs2) the originating location of tweet tw
Thus, Heat Index of a region factors in both the count of
the radical tweets from the region as well as the ‘degree of
radicalism’ of the tweets. However, there are certain challenges
in answering these questions. As for Qs1, a tweet can at most
be 140 characters long. This is indeed too little information
to ascertain the ‘degree of radicalism’ of tweets on individual
basis. Thus, we go one level up the hierarchy and consider
individual users instead of individual tweets. We collect all
the tweets from individual users and assign the ‘degree of
radicalism’ to the user based on her tweets. Now, Qs2 would
have been easy to answer with respect to individual tweets if
all the tweets had geo-co-ordinate information because Twitter
API2 provides geo-location information of tweets if the user
had chosen to reveal her location at the time of tweeting.
2 https://dev.twitter.com/docs/streaming-apis
and
ter.com/docs/platform-objects/tweets have been used

281

https://dev.twit-

Home Location matrix gCHL, from the entire dataset barring
the timespan (month in our case) for which the Heat Map is
being generated. The created matrix gCHL is an m×m matrix
where the entry gCHLa,b , 1 ≤ a ≤ m, 1 ≤ b ≤ m, is the
conditional probability of the Actual home location of a user
being region Rb , when her Declared Home Location is region
Ra , as learnt from the dataset. The gCHL matrix is computed
using the following three steps provided in Algorithm 1. Thus,
gCHLa,b is given by:

Algorithm 1 Counting Algorithm for computation of the
general Computed Home Location gCHL
•
•

•

Step 1: Initialize gCHLa,b = 0, 1 ≤ a ≤ m, 1 ≤ b ≤ m
Step 2: For each tweet tw in T , increment gCHLa,b if
Declared home location of the author of tw and the geolocation of tw are Ra and Rb respectively
Step 3: Make each row gCHLa of gCHL matrix row
stochastic, 1 ≤ a ≤ m

gCHLa,b = X
Y
However, there are certain problems with this approach - ﬁrst,
the tweets containing geo-location information are very scarce
(such tweets constitute less than 1% of our dataset). Second,
when we consider individual users, it is unjustiﬁed to assume
that all her tweets containing geo-location information will
point to a single region, even if all her tweets contained geolocation information. Thus, the best estimate of the location
of the user is the probability distribution of the user’s location
over the Indonesian provinces.
We consider categorical distribution of the users into the
thirty four provinces of Indonesia. The motivation behind
employing categorical distribution, instead of say the more
popular Gaussian distribution over the entire landscape of
Indonesia, is that we wish to obtain a political Heat Map
of Indonesia with the granularity level of a province. Our
technique of Location Index computation is discussed in
further details in the following section.
Sentiment Analysis using social media data has been attempted by works such as [26] which tries to exploit patterns
in online social media communication and also by [27] which
uses background lexical information and reﬁning of the same
for speciﬁc domains by supervised learning techniques. However, we have computed Radicalization Indices using simpler
text regression techniques similar to [28], [29]. Our technique
of Radicalization Index computation, which is veriﬁed to be
quite accurate is discussed in further details in Section V.
In summary, individual Twitter users are our chosen level of
granularity. We characterize a user not only on the radicalization scale but we also obtain a location distribution of the user
over the regions of Indonesia. Hence, there is no prediction
of the location of the user involved as in [1]. It is to be noted
that we consider only the users classiﬁed as radical by our
Radicalization Index computation method.

where,
X = The number of tweets in T such that the author of the
tweet has Declared Home Location as Ra and geo-location of
the tweet is Rb
Y = The number of tweets in T such that the author of the
tweet has Declared Home Location as Ra
Let the ath row of the gCHL matrix be denoted by gCHLa .
Now Computed Home Location vector for the user Ui denoted
by CHLi is assigned the value of gCHLa if DHLi is region
Ra . It is to be noted that the gCHL matrix is general (and not
user speciﬁc) and is computed using the entire Twitter data set
comprising all users.
From those tweets Ti,k , 1 ≤ k ≤ ti of user Ui , that contain the
geo-location information GLi,k , we compute the Computed
Geo Location vector CGLi of length m, where CGLi,j , 1 ≤
j ≤ m, is the probability of the Actual home location of
user Ui being region Rj , as learnt from the tweets of Ui . The
CGLi,j is computed in the following way:
A
CGLi,j = B

where,
A = The number of tweets in Ti whose geo-location is
Rj and
B = The number of tweets in Ti whose geo-location is
known
We thus obtain two pieces of information about the Actual
home location of Ui in the form of two distributions: CHLi
and CGLi , where CGLi is completely user-speciﬁc. However,
CHLi is partially user-speciﬁc - it does depend on Ui because
CHLi is based on her Declared Home Location, but it
also depends on the general distribution which depends on
the entire population mass. It is evident that both CHLi
and CGLi are categorical distributions over the thirty four
Indonesian provinces. Now, a mixture of discrete distributions
over any ﬁnite number of categories is just another distribution
over those categories. In order to combine CGLi and CHLi
we obtain a convex combination of the two to obtain Li,j in
the following way:

IV. L OCATION I NDEX C OMPUTATION
As discussed earlier, each user Ui , 1 ≤ i ≤ n has a home
location HLi , 1 ≤ i ≤ n associated with her, which may or
may not be declared. Each tweet Ti,k , 1 ≤ i ≤ n, 1 ≤ k ≤ ti
has a geo-location GLi,k , 1 ≤ i ≤ n, 1 ≤ k ≤ ti associated
with it. However, GLi,k for some tweets Ti,k may not be
known as the user Ui might turn her GPS off. Even when
user Ui has a Declared Home Location DHLi , it may not be
accurate. User Ui might intentionally or inadvertently misstate
her location. Accordingly, we do not accept the DHLi at its
face value as the Actual home location of Ui . Instead, we
compute a matrix, which we term as the general Computed

Li,j = (1 − ωi ) ∗ CHLi,j + ωi ∗ CGLi,j

(1)

Now, the mixture weights ωi for Ui is learnt from the data
itself and is calculated as ωi = |Ti |/|Ti |.
Li,j essentially is given by




Li,j = |Ti | ∗ CHLi,j + |Ti | ∗ CGLi,j

282

(2)



label each organization as radical or counter radical based on
these organizations beliefs and practices. Using web crawling
tools, we download a large number of documents from the web
sites of these organizations. We use the term ‘vocabulary’ to
mean the set of all unique terms that appear in all documents
from all organizations. All the documents of an organization
are assigned the same Radicalization Index as that assigned
to the organization by the domain experts in our team. This
set of documents together with their Radicalization Indices
form the training dataset for our model. After that we use the
model to assign a Radicalization Index to the document Di
created from the tweets of user Ui . This Radicalization Index
of document Di is taken to be RDi of user Ui .

which gives equation (1) when normalized by |Ti | = |Ti | +

|Ti | i.e the total number of tweets posted by the user Ui ,
where
Ti = set of tweets produced by user Ui
Ti = subset of Ti and represents the set of tweets by Ui
that contains geo-location information
Ti = subset of Ti and represents the set of tweets by Ui
that do not contain geo-location information
The motivation behind this deﬁnition of the mixture weight

is that for the Ti tweets which contain geo-location information, we consider the user-speciﬁc location distribution
information inferred from the particular user’s geo-location

containing tweets. However, for the tweets of Ti , we have no
location information except for the general information that
given a Declared Home Location for any user Uv in our dataset
as Ra , CHLv for Uv is gCHLa . Thus, if DHLi of Ui is
given to be Ra , we consider CHLi = gCHLa . This simple
formulation of Li,j also captures the fact that we rely more
on CGLi than on CHLi when the number of tweets with
geo-location information, generated by Ui is high - however
if that count is low ( or even absent), instead of discarding
Ui ’s information, we obtain the location distribution of Ui
from her DHLi . We experimented by using only geo-location
containing tweets and we saw that the results are far more
accurate if we included users of Type 3 - this is intuitively
correct because the geo-location containing tweets form less
than 1% of the entire dataset. We compute Li,j for users
belonging to Classes 1-3 (deﬁned previously) using equation
(1). For the users belonging to Class 3, we obtain ωi to be
zero, as we do not have any geo-location data from the tweets
to compute ωi .

A. Problem Formulation:
We formulate the problem in a general sparse learning
framework and solve the following optimization problem (3)
using the techniques from [22] . This is indeed a sparse learning problem because the vocabulary is very large compared to
the number of words used in a document.
1
ρ
2
2
min Ax − y2 + x2 + λ x1
x 2
2

(3)

where A ∈ Rs×p , y ∈ Rs×1 , and x ∈ Rp×1
In our application, we have
a) A is Document × Term matrix which is constructed as
follows: The set of terms (t1 , . . . tp ) includes all the terms from
all the documents by all the organizations, barring the stop
words. The size of the vocabulary in this case is p. If data
is collected by crawling web sites of different organization
(O1 , . . . , Oq ) and documents (di,1 , . . . , d1,ri ) are collected
from the web site of organization Oi , 1 ≤ i ≤ q, the total
number of rows of the matrix A is s = Σqi=1 ri . Thus, Aij =
term frequency of the j th term in the ith document such that
Aij ≥ 0, 1 ≤ i ≤ s, 1 ≤ j ≤ p.
b) yi ∈ {+1, −1} is the class of each document Di , 1 ≤
i ≤ s. The Radicalization Index of a document is the same
the Radicalization Index of the organization that created that
document. Thus, when an organization is labeled as radical
(or counter radical) by the domain experts, all the documents
pertaining to that organization is marked as +1 (or −1). Thus
yi = +1 (or −1) if Di , 1 ≤ i ≤ s belongs to an organization
marked as radical (or counter radical) by the experts.
c) xj is the weight for each term tj , 1 ≤ j ≤ p. This is the
parameter estimated by optimizing the objective function (3).
The xj ’s thus form the predictor variables of the model.
Let us further clarify the three terms involved in the convex
optimization problem:
2
a) 12 Ax − y2 - this ﬁrst term is related to the sum of
the squared errors to ﬁt a straight line to a set of data points.
The objective function (3) thus is the optimization problem of
minimizing this sum of squared-errors.
2
b) ρ2 x2 - this term deals with the ridge regression, which
is an extra level of shrinkage. We set ρ = 0 as we were mainly
driven by sparsity.

V. R ADICALIZATION I NDEX C OMPUTATION
We intend to assign a Radicalization Index RDi to Ui based
on the content of her tweets. We collect tweets from users
over a period of time (a month-in our case) and for each
user Ui we create a document Di that contains all the tweets
of that user, during that period of time. As there exists a
one-to-one correspondence between Ui and Di , by assigning
a Radicalization Index to Di , we essentially assign RDi
to Ui . Classical predictive model Multiple Linear regression
[18], [19], [20] ﬁts our application, since it is a dichotomous
classiﬁcation problem with multiple predictor variables, where
the predictor variables are the terms of our ‘vocabulary’.
Classical classiﬁcation methods such as Logistic Regression
which has applications in a wide variety of domains can
also be used for document classiﬁcation [21]. Thus, Logistic
Regression can also be applied for our problem. However,
Linear Regression was selected instead of Logistic Regression
because it out-performed the Logistic one through 10-fold
cross validation. Linear Regression showed around 98%
accuracy, but Logistic Regression showed 83-85% of accuracy.
The implementation of our approach proceeds in the following
way: First, we identify a set of Indonesian political organizations. Next, social scientists in our Minerva team, who are
domain experts for Indonesia, hypothesize a classiﬁcation to

283

TABLE I: The table provides the top 5 province or special region names based on their computed Heat Index values (also
mentioned alongwith) for October 10 - November 10, November 11 - December 10, December 11- January 10
Province Name
Jakarta
East Java
West Java
Yogyakarta
Central Java

Heat Index
5.48
2.95
2.68
1.74
1.68

Province Name
Jakarta
East Java
Yogyakarta
Central Java
West Java

c) λ x1 - this term involving the L1 norm deals with the
sparsity of the solution vector x. For different values of λ
we obtain a solution vector x which represents the weights
associated with each term tj , 1 ≤ j ≤ p ( the same terms
which are considered in the A matrix). Some of these weights
are positive, some negative (values can be very close to 0). The
terms with positive (or negative) weights are the radical (or
counter radical) words. The top (ones with weights having high
magnitude) radical and counter radical words are presented to
the experts for validation. We experiment with several λ values
resulting in x vectors of various sparsity until the list of top
radical and counter radical words are approved by the ﬁeld
experts.
We use the Matlab implementation of the SLEP package
[23] that utilizes gradient descent approach to solve the
optimization problem (3). This package can handle matrices
of 20M entries within a couple of seconds on a machine with
standard conﬁguration. The input to the SLEP package are the
values of A, λ, and y. The SLEP model outputs the weight
vector x.

Heat Index
16.16
12.33
4.53
3.7
3.39

Province Name
Jakarta
Yogyakarta
West Java
East Java
Central Java

Heat Index
4.71
1.82
1.25
1.20
0.69

VI. H EAT I NDEX C OMPUTATION
Once we have obtained the Location Indices Li , 1 ≤ i ≤ n
and Radicalization Indices RDi , 1 ≤ i ≤ n, for all the users
Hj of region Rj , 1 ≤ j ≤ m
Ui , 1 ≤ i ≤ n , the Heat
Index
n
RD
is computed as Hj =
i × Li,j , ∀j, 1 ≤ j ≤ m.
i=1
The Heat Index Hj for a region Rj indicates the degree of
prevalence of radical ideologies among the people of Rj by
taking into account both the number of radical tweeters living
in Rj and also their ‘degree of radicalism’.
VII. DATA C OLLECTION
Since our model requires the computation of both RDi as well
as Li for each user Ui , we followed a two step data collection
procedure described as follows:
i) For the purpose of collecting the training data set for computing the Radicalization Index, we crawled the websites of
36 well-known Indonesian organizations which are classiﬁed
as radical or counter radical by our ﬁeld experts. A few of the
organizations are mentioned in Table II. We crawled the websites of all these different organizations and collected a total
of 78,135 documents which after pre-processing and ﬁltering
resulted into 49,250 documents. The reason for this reduction
in numbers is that many of the crawled documents did not have
any relevant information (for example documents having only
advertisements). Each of the documents on a average contained
280 words i.e on an average 2880 characters. All documents
pertaining to an organization were labeled as radical or counter
radical depending on the outlook professed by the organization
itself. These were then used for ﬁtting our Radicalization Index
computation model.
ii) For our study on recovery of political signals pertaining to
trend of radical activities in Indonesia, we chose Twitter as
the data collection platform as Indonesia accounts for 19.0%
to 20.8% of Twitter’s total reach by country (Dec 2010)3 .
No other publicly available portal offers access to opinions
posted online by the Indonesian populace on a similar scale
as does Twitter. For gathering tweets, we use Twitter’s Stream
API to access Twitter’s global stream of publicly available
tweet data. Since our goal is to recover ‘political signals’, we
setup a keyword ﬁlter on the Stream API to gather tweets that
relate to radical and counter radical ideologies. The keywords
used for this ﬁltration have been identiﬁed by the social
scientists in our Minerva project team and are considered to

B. Assignment of Radicalization Index:
For each time period (in our case one month), each user
Ui will be assigned an RDi based on their tweets within that
period. This is done as follows:
a) As mentioned earlier, from the tweets of each user Ui we
form a User Document Di . It is to be noted here that many
users choose to tweet quite infrequently, hence even if we
collect tweets for one month, a user might have tweeted only
once or twice during the entire one month which defeats the
purpose of collecting tweets for a month. Hence, we further
apply the constraint that we consider only those users who
have tweeted at least seven times in a month. The value of this
threshold has been arrived at empirically after experimentation
with various values of the threshold.
b) With the help of the model that has been ﬁtted using the
organization documents, we classify the Di ’s. Let each Di
which is a term frequency row be denoted by the row vector
tc of count of terms from our ‘vocabulary’.
c) Each user Ui receives
ap ‘score’ which we refer to as RDi
given by RDi = tc .x = j=1 tcj xj .
This provides us a time-series of RDi values for the users.
This makes it possible to analyze the transition dynamics of
each user. Evidently, a high positive RDi indicates that Ui is
highly radical whereas a high negative RDi indicates that Ui
is highly counter radical.

3 http://www.billhartzer.com/pages/comscore-twitter-latin-america-usage/

http://www.comscoredatamine.com/2011/02/the-netherlands-leads-globalmarkets-in-twitter-reach/

284

TABLE II: Table showing some of the well-known radical and
counter radical organizations of Indonesia
Radical Organizations
AbuJibriel
PKS
Arrahmah
EraMuslim
HizbutTahrir

Fig. 2: Figure showing the number of tweets collected over
our observation period

Counter radical Organizations
NU
Interﬁdei
IslamLiberal
PPIM
LKIS

TABLE III: Keyword markers used for ﬁltering Twitter Stream
API
Keyword
‘penegakan syariah’
‘jihad majelis’
‘mati syahid’
‘ajaran islam’
‘pendidikan agama di sekolah’
‘demokrasi yang’

Interpretation
enforcement of Sharia
jihad assemblies
martyrdom
the teaching of Islam
religious education in schools
democracy

VIII. E XPERIMENTAL R ESULTS
We created Heat Maps of Indonesia on a monthly basis. We
computed the RDi of each user U i for each month from
October 10 to January 10, as long as Ui tweeted at least 7
times in that month Again, for each user Ui we computed
the Location Index Li by considering all her tweets over
the period of the month. For that we computed the general
Computed Home Location gCHL matrix. The gCHL matrix
provides interesting insights on the Indonesian population. We
computed the gCHL matrix on all possible doublets among
the three months of observation period. i.e for each month for
calculating the Location Indices Li of users, we have generated
the gCHL matrix using the other two months of data. Thus,
in each case, we had training data of two months and test
data of one month. We observed that people with Declared
Home Locations in various different provinces from all around
Indonesia such as Bangka Belitung, Banten, Maluku, West
Nusa Tenggara, East Nusa Tenggara and Papua have a very
strong tendency to have high probability of having Actual
home location in Jakarta (as observed from our results over
three months). This is very intuitive because Jakarta being the
Capital Region must have attracted people from different parts
of Indonesia for prospective settlement. We further made an
observation that people with Declared Home Location of East
Kalimantan have considerable geo-location containing tweets
from Central Kalimantan.

be signiﬁcant markers of radical and counter radical ideologies
in the Indonesian context. A few such markers are listed in
Table III.
We collected tweet data for a three-month interval and
gathered a total of 12,152,874 tweets from October 10, 2012
to January 10, 2013 ( Figure 2) that matched the keyword
ﬁltration criteria. In this research, we are interested in the
probability distribution Location Index Li of user Ui over
the thirty four provinces of Indonesia, thus we focus only on
users from Indonesia. The keywords used are in Indonesian
language and narrows down the tweets we obtained from the
Twitter API. Thus, the geo-code in majority of cases indicated
a location in Indonesia. However, not all geo-codes are from
Indonesia. We ignore those tweets in the current work. Thus,
out of these 12 million tweets, 110,063 tweets contained
geo-locations that mapped to regions within Indonesia. To
apply this reverse geo-coding, we used the OpenStreetMap
API4 . A user repository was constructed by including only
those users whose Declared Home Locations matched with an
identiﬁable Indonesian city or province. We found that many
users have put texts such as ’Dark side of the moon’ or ‘Here’
or ‘infront of my laptop’ as home location and hence, there is
a need for pre-processing of the text. Also, the users provided
location information to varied degrees of granularity ranging
from continents to towns. However we are interested in the
ﬁxed granularity level of Indonesian provinces and the special
regions such as Jakarta and Yogyakarta. Hence we manually
created a database of towns and cities of all of the Indonesian
provinces. Each of the provinces were annotated with 42 cities/
towns on an average with Papua being the highest which
was annotated with 70 cities/towns. Using this database we
then assigned a legitimate Declared Home Location to as
many users as possible. The ﬁnal user repository consisted
of 959,911 unique users.

The Heat Indices values for the thirty four Indonesian
provinces are computed using our approach for three months
of our observation period - namely October 10 - November 10,
November 11 - December 10, December 11 -January 10. We
found a drastic change in the heat indices during the interval
of November 10 – December 10. But we could not discern any
particular event which could have triggered the same. Among
all Indonesian provinces the top ﬁve provinces and special
regions along with their Heat Index values are presented in
Table I for the three months. Color maps of Indonesia with
Heat Indices is shown in Figure 3, where darker colors indicate
a higher level of radical tweeting, and lighter colors indicate a
lower level of radical tweeting. It may be seen from Figure 3
that the area around Jakarta and the Java provinces are highly
active in radical tweet creation. According to our Twitter data
analysis, the provinces Jakarta, East Java, Yogyakarta and
Central Java, along with West Java are the top provinces that

4 The relevant information about the API could be found at http://wiki.openstreetmap.org/wiki/Nominatim

285

(a) Heat Map for October 10 to November 10

(b) Heat Map for November 11 to December 10

(c) Heat Map for December 11 to January 10

(d) Radicalism
Scale

Index

Fig. 3: Heat Maps of Indonesia
generate a high level of radical activities.
may be mentioned here that ﬁeld studies7 in January 2012
by Setara Institute8 , a well-known Indonesian NGO, showed
that the strong radicalism of the young muslim population
in Yogyakarta and Central Java are making them hot targets
to be recruited as Jihadists. In May 2012, there was a mob
attack by Indonesian Mujahidin Council in Yogyakarta and in
September 2012, there has been arrests of potential terrorists
from Yogyakarta9 . Because, Wahid Institute has mentioned
about Indonesian provinces only, it might be expected that
Jakarta and Yogyakarta, being special administrative regions,
are missing from their list - however, we do not have access to
their full report. The high radicalism of the Java provinces are
also corroborated by reports of the Setara Institute. The only
radically active province that shows up in the Wahid Institute
report but does not appear at the top of our list is Aceh, located
at the north west corner of Indonesia. It is worth mentioning
here that Aceh was completely devastated by the 2004 Indian
Ocean Tsunami and is still recovering from its effects. Aceh
is also one of the least economically developed provinces
of Indonesia. We believe that due to the lack of economic
advancement in Aceh, the level of Internet penetration in Aceh
is fairly small and not many people from Aceh are active
tweeters. This may explain the reason for Aceh not showing
up among our list of top radically active provinces.

IX. VALIDATION
For the purpose of validation of the Radicalization Index (not
the ‘degree of radicalism’), we computed the Radicalization
Indices of some well-known counter radical leaders of Indonesia for the months that they had tweeted for more than
7 times which we consider as our threshold. Our classiﬁer
gave perfect accuracy. By accuracy of the classiﬁcation we
mean the percentage of time the leaders who are thus known
to be counter radical were classiﬁed as counter radical by our
classiﬁer. We did not validate the Location Index computation
technique because of the lack of the ground truth of the
Actual home location of users. However, our results of Heat
Index are validated by the ﬁndings of the Indonesia-based
Wahid Institute5 . Wahid Institute promotes a moderate version
of Islam through dialogue events, publications, and public
advocacies. According to the Wahid Institute’s Annual Report
of 20126 , the top four provinces of Indonesia where radical
activities are most observable are West Java, Aceh, East Java,
and Central Java. It may be noted here, that three out of the
four most radical provinces identiﬁed by the Wahid Institute,
also appear at the very top of our list. Also, our ﬁeld experts
have conﬁrmed Jakarta to be a center of radical activities. It
5 http://berkleycenter.georgetown.edu/resources/organizations/wahidinstitute
6 Released on December 28, 2012

7 http://www.setara-institute.org/en/content/study-shows-how-youngradical-indonesian-muslims-become-terrorists
8 http://www.setara-institute.org/
9 http://www.washingtontimes.com/multimedia/image/indonesia-terrorjpg/

286

X. C ONCLUSION
We have developed a generic technique for recovering
signals pertaining to a geographical area such as a country
using Twitter Data. We have applied our technique to our
Indonesian dataset and have observed high accuracy. The
goal of our work is the generation of a political Heat Map
of Indonesia which highlights the Indonesian provinces with
prominent radical narrative. Thus, we have analyzed tweets
made by a user Ui in a month to assign a Radicalization
Index RDi to Ui where RDi indicates how radical is Ui
in her political outlook. Also, by mining the tweets in our
database we assign a Location Index Li to Ui . For computation
of the Location Index, we use not only the geo-location
tagging as provided by Twitter API but also the user declared
home location information from her proﬁle (after considerable
amount of pre-processing and cleansing). We have combined
these two sources of information for inferring the probability
distribution of the location of the users among the provinces
of Indonesia. Thus, by considering the RDi ’s in conjunction
with the Li ’s for all the users over a period of one month
we generate the political Heat Maps of the likes of Figure 3.
Such Heat Maps can prove to be very useful in studying the
spatio-temporal dynamics of the people of Indonesia so far as
their political outlook is concerned

[12] Cheng, Z., Caverlee, J., & Lee, K. (2010, October). You are where
you tweet: a content-based approach to geo-locating twitter users. In
Proceedings of the 19th ACM international conference on Information
and knowledge management (pp. 759-768). ACM.
[13] Boutet, A., Kim, H., & Yoneki, E. (2012, August). What’s in Twitter: I
Know What Parties are Popular and Who You are Supporting Now!. In
Advances in Social Networks Analysis and Mining (ASONAM), 2012
IEEE/ACM International Conference on (pp. 132-139). IEEE.
[14] Dahlin, J., Johansson, F., Kaati, L., Martenson, C., & Svenson, P.
(2012, August). Combining Entity Matching Techniques for Detecting
Extremist Behavior on Discussion Boards. In Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International
Conference on (pp. 850-857). IEEE.
[15] Eisenstein, J., Ahmed, A., & Xing, E. P. (2011, June). Sparse additive
generative models of text. In International Conference on Machine
Learning (ICML).
[16] Hong, L., Ahmed, A., Gurumurthy, S., Smola, A. J., & Tsioutsiouliklis,
K. (2012, April). Discovering geographical topics in the twitter stream.
In Proceedings of the 21st international conference on World Wide Web
(pp. 769-778). ACM.
[17] Yin, Z., Cao, L., Han, J., Zhai, C., & Huang, T. (2011, March).
Geographical topic discovery and comparison. In Proceedings of the
20th international conference on World wide web (pp. 247-256). ACM.
[18] Zhang, T. (2009). Some sharp performance bounds for least squares
regression with L1 regularization. The Annals of Statistics, 37(5A),
2109-2144.
[19] S.Kim, K.Koh, M.Lustig, S.Boyd and D. Gorinevsky, An Interior-Point
Method for Large-Scale l1 - Regularized Least Squares Journal of
seleccted topics in Signal Processing, Vol. 1, No. 4, pages 606 - 617,
Dec 2007
[20] Kolter, J. Z., & Ng, A. Y. (2009, June). Regularization and feature
selection in least-squares temporal difference learning. In Proceedings
of the 26th Annual International Conference on Machine Learning (pp.
521-528). ACM.
[21] Brzezinski, J. R., & Knaﬂ, G. J. (1999). Logistic regression modeling for context-based classiﬁcation. In Database and Expert Systems
Applications, 1999. Proceedings. Tenth International Workshop on (pp.
755-759). IEEE.
[22] Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for
generalized linear models via coordinate descent. Journal of statistical
software, 33(1), 1.
[23] J. Liu, S. Ji and Jieping Ye. SLEP: Sparse Learning with Efﬁcient
Projections, Arizona State University (2009)
[24] Lee, S. I., Lee, H., Abbeel, P., & Ng, A. Y. (2006, July). Efﬁcient
L˜ 1 Regularized Logistic Regression. In Proceedings of the National
Conference on Artiﬁcial Intelligence (Vol. 21, No. 1, p. 401). Menlo
Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.
[25] T. P. Minka, A comparison of numerical optimizaers for logistic regression, Technical report, 2007
[26] Thelwall, M., Buckley, K., Paltoglou, G., Skowron, M., Garcia, D.,
Gobron, S., ... & Holyst, J. A. (2013). Damping Sentiment Analysis
in Online Communication: Discussions, Monologs and Dialogs. In
Computational Linguistics and Intelligent Text Processing (pp. 1-12).
Springer Berlin Heidelberg.
[27] Melville, P., Gryc, W., & Lawrence, R. D. (2009, June). Sentiment analysis of blogs by combining lexical knowledge with text classiﬁcation.
In Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining (pp. 1275-1284). ACM.
[28] Joshi, M., Das, D., Gimpel, K., & Smith, N. A. (2010, June). Movie
reviews and revenues: An experiment in text regression. In Human
Language Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational Linguistics (pp.
293-296). Association for Computational Linguistics.
[29] Kogan, S., Levin, D., Routledge, B. R., Sagi, J. S., & Smith, N. A.
(2009, May). Predicting risk from ﬁnancial reports with regression.
In Proceedings of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Association for
Computational Linguistics (pp. 272-280). Association for Computational
Linguistics
[30] Brynielsson, J., Horndahl, A., Johansson, F., Kaati, L., Martenson, C.,
& Svenson, P. (2012, August). Analysis of weak signals for detecting
lone wolf terrorists. In Intelligence and Security Informatics Conference
(EISIC), 2012 European (pp. 197-204). IEEE.

R EFERENCES
[1] Cho, E., Myers, S. A., & Leskovec, J. (2011, August). Friendship
and mobility: user movement in location-based social networks. In
Proceedings of the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining (pp. 1082-1090). ACM.
[2] Xu, J. M., Bhargava, A., Nowak, R., & Zhu, X. (2012). Socioscope:
spatio-temporal signal recovery from social media. In Machine Learning
and Knowledge Discovery in Databases (pp. 644-659). Springer Berlin
Heidelberg.
[3] Priedhorsky, R., Culotta, A., & Del Valle, S. Y. (2013). Inferring the
Origin Locations of Tweets with Quantitative Conﬁdence. arXiv preprint
arXiv:1305.3932.
[4] Yang, D., Zhang, D., Yu, Z., & Wang, Z. (2013, May). A sentimentenhanced personalized location recommendation system. In Proceedings
of the 24th ACM Conference on Hypertext and Social Media (pp.
119-128). ACM.
[5] Li, Y., Steiner, M., Wang, L., Zhang, Z. L., & Bao, J. (2012, December).
Dissecting foursquare venue popularity via random region sampling.
In Proceedings of the 2012 ACM conference on CoNEXT student
workshop (pp. 21-22). ACM.
[6] Gonzalez, M. C., Hidalgo, C. A., & Barabasi, A. L. (2008). Understanding individual human mobility patterns. Nature, 453(7196), 779-782.
[7] Sadilek, A., Kautz, H., & Bigham, J. P. (2012, February). Finding your
friends and following them to where you are. In Proceedings of the
ﬁfth ACM international conference on Web search and data mining (pp.
723-732). ACM.
[8] Noulas, A., Scellato, S., Lambiotte, R., Pontil, M., & Mascolo, C.
(2012). A tale of many cities: universal patterns in human urban mobility.
PloS one, 7(5), e37027.
[9] Chang, H. W., Lee, D., Eltaher, M., & Lee, J. (2012, August). @
Phillies Tweeting from Philly? Predicting Twitter User Locations with
Spatial Word Usage. In Advances in Social Networks Analysis and
Mining (ASONAM), 2012 IEEE/ACM International Conference on (pp.
111-118). IEEE.
[10] Mahmud, J., Nichols, J., & Drews, C. (2012). Where is this tweet from?
inferring home locations of twitter users. Proc AAAI ICWSM, 12.
[11] Eisenstein, J., O’Connor, B., Smith, N. A., & Xing, E. P. (2010,
October). A latent variable model for geographic lexical variation. In
Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing (pp. 1277-1287). Association for Computational
Linguistics.

287

2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

LookingGlass: A Visual Intelligence Platform for
Tracking Online Social Movements
Nyunsu Kim, Sedat Gokalp, Hasan Davulcu, Mark Woodward
School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University
Tempe, USA
{nkim30, Sedat.Gokalp, hdavulcu, Mark.Woodward}@asu.edu

Abstract— We propose a multi-scale text mining methodology
and develop a visual intelligence platform for tracking the
diffusion of online social movements. The algorithms utilize large
amounts of text collected from a wide variety of organizations’
media outlets to discover their hotly debated topics, and their
discriminative perspectives voiced by opposing camps organized
into multiple scales. We utilize discriminating perspectives to
classify and map individual Tweeter's message content to social
movements based on the perspectives expressed in their weekly
tweets. We developed a visual intelligence platform, named
LookingGlass, to track the geographical footprint, shifting
positions and flows of individuals, topics and perspectives
between groups.
Keywords—Text mining; Multi-scaling; Social movements

I. INTRODUCTION
We propose a multi-scaling based methodology and a
visual intelligence platform that represents an important step
change in how we might observe and analyze radical social
movements. Rather than placing external forms of analysis that
color and tautological define what is ‘radical’ or not from an
external perspective, we propose a more ontologically oriented
approach. We seek to generate a methodology to allow the
orientations of these movements to define themselves via their
own discourse within their own universe and understanding of
actions, rather than an external and potentially poorly
calibrated analysis of what constitutes radical. Without this
kind of fundamental reorientation to research of religiously or
politically inspired social movements, we get the poor
assumption based analysis that (incorrectly) predicts and
champions ill-defined relationships between certain religious
or political sects and violence, for example. With our
reorientation of approach, we are more fundamentally able to
examine such relationships in a way that should allow analysts
to take other kinds of nuance and understanding into account.
Current technology for monitoring social media tracks
keyword matching documents for names of known groups,
individuals, and places. However, they cannot find the
proverbial “needles in a haystack” corresponding to those
individuals with radical or extremist ideas, connect the dots to
identify their relationships, and their socio-cultural, political,
economic drivers. Raw data in multiple modalities (e.g. tweets,
blogs, and newswires) gushes like an uncapped oil well but
existing technologies fail to provide comprehensive tools for

ASONAM'13, August 25-29, 2013, Niagara, Ontario, CAN
Copyright 2013 ACM 978-1-4503-2240-9 /13/08 ...$15.00

making-sense of the data and for seeing the bigger picture.
LookingGlass is designed for real-time contextual analysis of
complex socio-political situations that are rife with volatility
and uncertainty. It is able to rapidly recognize radical hot-spots
of networks, narratives and activities, and their socio-cultural
economic, political drivers. Also, it is informed by highly
trained area experts on the ground with social science and
subject matter expertise as well as local cultural knowledge.
II. PROBLEM DEFINITION
One of the fundamental issues with interpretative and
qualitative data collection and analysis has been the researchers’
bias while conducting the research. Goertz [1, 2] makes the
crucial point that, in their enthusiasm for reifying complex
sociological or political concepts, theorists and empiricists
often focus too much on what a concept is, rather than on
identifying the concept on a continuum, in order to assess when
a concept is present versus when it is absent. In the social
sciences, scaling is the process of measuring and ordering
actors (subjects) with respect to quantitative attributes or traits
(items). In the context of our project, both social movements
(subjects) and their socio-economic, political, or religious
beliefs, goals and practices (items) are mapped simultaneously
on a set of scales via expert inputs and text mining algorithms.
Recently we developed algorithms [3, 4, 5, 6] that utilize large
amounts of multilingual text collected from a wide variety of
organizations’ media outlets (e.g. web sites, blogs, news, RSS
feeds, tweets, leaders’ speeches etc.) to (i) discover hotly
debated topics relevant for a scale, and discriminative topicspecific perspectives voiced by opposing camps, (ii) next, we
identify a subset of these discriminating perspectives with a
very specific statistical pattern (probabilistic Guttman pattern
[7]) that can be used to classify and rank any actor’s polarity
and neutral-to-extreme position on a continuous scale at area
expert-level accuracy, and finally, (iii) we utilize spatialtemporal analysis of real-time textual message feeds, from
organizational RSS feeds and individuals’ Tweets to track the
geographical footprint, hot spots, shifting positions and flows
of individuals between groups.

1020

2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
Proponents of this position maintain that to determine the
meaning of a scriptural passage appropriate for a particular
time, place, and culture, both the context of revelation and
the context of exegesis must be considered.

III. MULTI-SCALE MODELING OF SOCIAL MOVEMENTS
Our modeling leverages social theory including
Durkheim’s research on collective representations [8],
Simmel’s work on conflict and social differentiation [9],
Wallace’s writings on revitalization movements [10], and Tilly
and Bayat’s studies on contemporary social movement theory
[11, 12] to understand features shared by violent social and
political movements and by those opposing them. Radicalism
is the ideological conviction that it is acceptable and sometimes
obligatory to use violence to effect profound political, cultural
and religious transformations and to change the existing social
order fundamentally. Radical movements have complex origins
and depend on diverse factors that enable the translation of
their radical ideology into social, political and religious
movements [13, 14]. Crelinsten [15] states, “both violence and
terrorism possess a logic and grammar that must be understood
if we are to prevent or control them.” Binary labeling as
counter-radical vs. radical does not capture the overlaps,
movement and interactivity among these actors. We observe
[6] that both counter-radical and radical movements in Muslim
societies exhibit distinct combinations of discrete states
comprising various social, political, and religious beliefs,
goals, attitudes and practices, and their discriminating
perspectives can be semi-automatically identified and mapped
onto latent linear continuums or scales.
The model described below defines a seven dimensional
possibility space within which diverse organizations, social
movements, and individuals can be located. The variables are
treated as continuous bipolar scales. Each scale is measured
independently of the others. Social movements exhibit distinct
combinations of discrete social, political, and religious beliefs,
attitudes, and practices that can be mapped onto these latent
linear scales. Scaling is a method for measuring and ordering
entities based on their qualitative attributes.
The choice of scales relies on the work of a combination of
American, European, African, and Southeast Asian scholars
and the literature on similar movements in various regions. The
variables are generalizations based on ethnographic research
that involved observation of public events, extended interviews
and informal conversations with leaders and rank and file
members of organizations and movements, and discourse
analysis. Field work findings and conventional discourse
analysis provided input for automated discourse analysis
conducted using methods relying on key words associated with
each of the variables (described elsewhere). These automated
methods analyzed more than fifty thousand local language
documents obtained with the use of web-mining technology.
The scales used in LookingGlass tools for characterizing
Radical-Islamist and Counter-Radical Islamist movements are:
x Epistemology: This refers to the ways in which religious
groups interpret core texts. Foundationalism is at one end
of a continuum. It fixes meaning in invariant, “literal”
readings of core religious texts. Foundationalists claim that
their readings are ahistorical and not influenced by cultural
considerations. Constructivism is at the other end of the
scale. It acknowledges that all variants of a religious
tradition are constructed in historical, social, and cultural
contexts and they can, and indeed must, change over time.

x Religious Diversity Tolerance: Exclusivists, who insist on
universal adherence to their own beliefs and social norms
and who claim exclusive possession of complete truth, are
at one end. Pluralists, who understand difference as a
social and religious good or theological pluralism, are at
the other. An entity at the extreme pluralist end of the
tolerance scale holds the view that all religions should be
tolerated and that all are based on truths that transcend
confessional and sectarian differences.
x Change Orientation: Change orientation aims to capture
the degree to which an entity wishes to effect social,
political, and/or religious change. It is also a measure of
the degree to which an individual or group attempts to
influence others. Revitalization movements that seek to
destroy the world as it is and rebuild it from scratch are at
one end of the scale. Defenders of the social, political, and
religious status quo are at the other.
x Violence: Violence is defined broadly to include more than
killing, inflicting physical injury, and destruction of
property. Symbolic and discursive violence are included in
this scale because they are often steps leading toward
physical violence. They can cause havoc, especially when
the manipulation of symbols and discourse is purposively
articulated to provoke adversaries, demonize opponents,
incite mobs to action, or to provide justifications for the
“necessity of violence.” Unlike physical violence that can
be seen and clearly understood for what it is, symbolic and
discursive violence are not necessarily self-evident; hence
both require knowledge of their contexts to identify them
and assess their real and potential danger.
Dehumanization, demonization, and the desecration of
sacred places and objects are among the most common and
provocative forms of symbolic violence committed in
contexts of ethnic and religious conflict.
x Violence Ideology scale represents the degree to which an
entity supports or rejects violence as a matter of principle.
Though some of the movements scaled rely on reasoned
argumentation appealing to concepts of justice and
oppression in addition to, or in place of narratives. At one
end are those who would support any type of violence; at
the other are pacifists who are ideologically committed to
nonviolence. A lack of violent rhetoric is insufficient to
classify an organization as pacifist if the organization is
silent in the face of others’ violence violent acts and
violent rhetoric.
x Violence Engagement scale measures the degree to which
an entity engages in any type of violence including
symbolic or discursive violence. At one end of the
continuum are those who have explicitly claimed
responsibility for violent acts. At the other end are those
who have never engaged in any type of violence.

1021

2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

Figure 1. Graphical Scaling Tool
்
௫ σ௠
௜ୀଵ ‫ݓ‬௜ ሺͳ ൅ ൫െ‫ݕ‬௜ ሺ‫ܽ ݔ‬௜ ൅ ܿሻ൯ ൅ ߣȁ‫ݔ‬ȁଵ

IV. LOOKINGGLASS TOOLS AND METHODS
A. Graphical Scaling Tool
We designed an intuitive, easy-to-use graphical tool for
defining multiple scales, so that area experts may populate
them with polarities and positions of known social movements
from a certain region. A snapshot of such a graphical scaling
tool is shown in Figure 1, which marks the positions and
polarities of 23 groups from Indonesia on the seven scales
described above.
B. Multi-Lingual Topic Detection
We utilized on-line topic detection [16] in text corpus. We
asked area-experts to scan detected topics and associate them
with the scales that they are relevant. For example, a partial list
of relevant topics for a Political Change scale for Indonesia
include: {sharia, family law, corruption, democracy, election,
secularism, state, constitution, justice}.
C. Discriminative Perspective Mining
The more challenging aspect of textual analysis is
discriminating perspective mining in debates between the
opposing camps on a scale. A debate is a formal discussion on
a set of related topics in a forum, in which opposing arguments
are put forward. For example, a debate on education might
comprise opposing perspectives, such as “secular, multicultural education” vs. “religious, sharia based education”. In
this step, our focus is the development of an automated
perspective mining algorithm, which would contribute to the
understanding of features (i.e. social, political, cultural,
religious beliefs, goals, and practices) shared by one side of a
debate, and by those opposing them. We formulate the
perspective mining problem in a general structured sparse
learning framework [17]. In particular, the logistic regression
formulation fits our application, since it is a dichotomous
classification problem:

In the formula above, ai is the vector representation of the
ith document, wi is the weight assigned to the ith document (wi
=1/m by default), and A=[a1, a2, …, am] is the document
keyword matrix, yi is the polarity of each document based upon
the scale polarity of the actor that the document belongs to, and
the unknown xj , the j-th element of x, is the weight for each
keyword, λ>0 is a regularization parameter that controls the
sparsity of the solution, |x|1= Σ|xi| is 1-norm of the x vector. We
use the SLEP1 sparse learning package that utilizes gradient
descent approach to solve the above convex and non-smooth
optimization problem. The keyword phrases with non-zero
values on the sparse x vector yield the discriminant
perspectives based on their polarity (positive or negative).
Following table displays radical perspectives for fivetopics.

Figure 2. Radical Perspectives for Topics

1

1022

http://www.public.asu.edu/~jye02/Software/SLEP/

2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
D. Response Tables of Social Movements
A response table is calculated based on the normalized
frequency with which actors mention various perspectives, as
indicated by keywords. The median frequency of each
perspective is selected as a threshold. Actors and normalized
perspective frequencies are used to build a dichotomous [0/1]
response matrix. A sample partial response table for the
Violence Ideology scale is presented in Figure 3.

Figure 3. Response Table of Radical Actors on Violence Ideology perspectives

E. Rasch Modeling
A true Guttman scale is deterministic, i.e. if an actor
subscribes to a certain perspective, then it must also agree
with all lower order perspectives on the scale. Of course,
perfect order is rare in the social world. The Rasch [18] model
provides a probabilistic framework for Guttman scales to
accommodate incomplete observations and measurement
error. We employ the Rasch model to measure the abilities of
actors on a latent scale, alongside the difficulties of
perspectives on the same scale. By definition, the location of
an item (difficulty) on a scale corresponds to the actor location
(ability) at which there is a 0.5 probability of a response. Once
item locations are scaled, then actor locations are measured on
the scale using the EM-method for joint Maximum Likelihood
Estimation [19]. The following figure (Figure 4) is the radical
actor-perspective map displaying the location of
discriminating perspective (items) as well as the distribution
of actors (subjects) along a Violence Ideology scale:

to one polarity or another polarity of a scale (e.g. radical or
not-radical), but also if a message or a messenger classifies as
a follower of one of the known social movement’s rhetoric or
ideology. In our 10-fold cross-validation-based experiments
with 37,000 web pages downloaded from web sites of 23
Islamic organizations (10 radical and 13 non-radical) from
Indonesia, we observed that the linear formulation-based
message classifier, with discriminative perspectives as its
features, achieves over 98% accuracy for predicting the
corresponding polarity of documents. Furthermore, we
observed that logistic formulation-based classifier achieves
over 83% accuracy for predicting the corresponding source (a
particular Islamic organization) of a document. Combined
with longitudinal analysis of an individual’s messages (such as
those that can be observed on Twitter, message boards, blogs,
or in chat rooms), we can determine (i) shifts of individuals
from the status of unaffiliated to affiliated of one of the known
social movements (SM), (ii) growth and shrinkage drivers (i.e.
types of events and narratives) of SMs, and (iii) influential
followers of SMs. In the following section we describe the
design of a real-time dashboard, named LookingGlass (see
Figure. 5), to display all types of flows and hot spots of
Tweeters between radical (red) and non-radical (green) SMs,
their popular keywords, hash tags, event mentions, and media
sources driving their weekly shifts.

Figure 4. A Violence Ideology Scale for Radical Actors

F. Micro Level Analysis of Groups and Individuals
The utilization of the logistic regression formulation
presented in Section IV. C provides a classification model [20]
between different polarities of a scale -- by checking the
polarity of (xT.ai) where xT corresponds to the weights of the
discriminant perspectives relevant to a scale, and ai is the
keyword vector of each Tweeter’s weekly message content.
Using the discriminating perspectives, we can not only detect
if a message or a collection of messages from a Tweeter maps

Figure 5. A Real-Time Dashboard for Visualizing Shifts and their Flows

1023

2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
V. LOOKINGGLASS DESIGN AND ARCHITECTURE
The following diagram in Figure 6 shows the major
components of the LookingGlass real-time dashboard.

K. Chord Diagram
User-Group mappings are rendered using d3 2 chord
diagram. All user and group information is indexed by Apache
Solr 3 server, supporting keyword and parametric search.
Google map API4 was used to display users’ and organizations’
geographical footprint.
VI. LOOKINGGLASS DEPLOYMENT
We collected 7 weeks of matching Twitter messages from
Indonesia, Malaysia and Singapore starting on October 10,
2012. Weekly, the average number of matching tweets was
786,484, average number of unique tweeters was 348,227.
Weekly statistics of number of tweets and unique tweeters are
shown below in Figure 7 and Figure 8.

Figure 6. LookingGlass system architecture.

Initially we downloaded all documents from web sites of
organizations, followed by topic detection and discriminant
perspective mining. Next, we utilized Twitter Streaming API to
collect all tweets matching topics and perspectives. We
collected tweets from each user and apply the polarity and
group-level classifiers to map tweeters to groups. We display
shifts and flows among groups by utilizing a chord diagram.
We extracted the geographic locations of tweeters from their
GPS coded tweets, home pages and rendered the information
on a heat map.

Figure 7. Total number of weekly Indonesian tweets matching topics and
perspectives.

G. Data Collecting
To bootstrap the process, we asked area experts with field
and domain expertise to create a list of radical and non-radical
organizations. Next, they determined the polarity and scaled
them using the Graphical Scaling Tool.
H. Perspective Analysis
We utilized multi-lingual topic detection and topic mapping
tools described in IV. B, and discriminative perspective mining
algorithm described in IV. C to determine topics and
discriminative perspectives (n-grams) to train linear regression
based polarity level (radical or non) classifier, and logistic
regression based group level classifier.
I. Twitter Stream
We subscribed to Twitter streaming API and collected all
messages matching topics and topic-specific perspectives.
J. User Classification
Collected raw Tweet data is aggregated weekly by user and
we applied classifiers to map users to groups based on topics
and perspectives mentioned in their tweets.

Figure 8. Total number of weekly Indonesian tweeters.

Based on Tweeter classification results (Section V. D),
around 10% of Tweeters’ messages were predicted to match
the perspectives of radical groups and rest of the users’
messages matched the perspectives of non-radical groups.
Figure 9 below shows the percentages of weekly polarization
between radical and non-radical.

2

http://d3js.org

3

http://lucene.apache.org/solr/

4

https://developers.google.com/maps/

1024

2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

Figure 9. Weekly radical and non-radical polarities of Tweeters.

Figure 11.Weekly radicalized / counter radicalized shifts - percentages.

In order to detect shifts, we utilized the Rasch Model
scaling of organizations discussed in Section IV. E. Figure 10
below shows the polarities of the radical and non-radical lists
of organizations we analyzed, and their rankings from neutralto-extreme positions on both sides. For instance, if a tweeter’s
messages were classified to a radical organization during a
previous week, and following week the same tweeter’s
messages were classified to a non-radical organization, then we
labeled the user’s shift as “counter-radicalized”. Similarly, if a
tweeter’s messages were classified to a non-radical group, or to
a radical group of a lower caliber during an earlier week, and
then the same user’s messages were classifier to a radical
group of a higher caliber during the following week then we
labeled the user’s shift as “radicalized”. The polarity and
rankings of organizations, and two possible shift directions are
as shown below in Figure 10.

radical groups by the group level classifier, then we label that
user as “Unaffiliated Radical”. A similar reasoning also
applies to detect “Unaffiliated Counter-Radicals”.
a) An event was among the most popular tweets of 370
Unaffiliated Radicals between Oct.10.2012 and Oct.17.2012.
b) The event was a salafi student protest and the violent
reaction by the security forces against them at North Sulawesi,
Indonesia.
c) This “radicalizing” event can be detected by selecting
the “Unaffiliated Radical” segment on the chord diagram and
checking the popular URL’s mentioned in their tweets.
Clicking on the corresponding URL pops-up the article titled
“Demo Cagub, Polisi Hajar Mahasiswa” from its original
media source http://sindikasi.inilah.com/. (Figure 12)

Figure 10. Weekly opinion shifts in tweets.

Weekly opinion shift trends for 7 weeks are shown in Figure
11. An average percentage of radicalized opinion shifts was
around 15%, average percentage of counter radicalized opinion
shifts was 70%, and approximately 15% of users preserved
their previous weekly positions. During weeks 6 and 7
percentage of radicalized shifts jumped above 20% due to
corruption related debates.
VII. LOOKINGGLASS USE SCENARIOS
In this section, we present three scenarios to demonstrate
different features of the LookingGlass platform. All scenarios
are drawn from real social movements and Tweeter streams.
1) A Protest Event highlighted by Non-Affiliated Radicals.
If a Tweeter’s weekly message content gets classified as
“Radical”, but the user does not classify to one of the known

Figure 12. Event based users’ opinion tracking

2) A Radical Group’s Local Followers.
a) Clicking on one of the radical groups on the chord
diagram shows the geographic footprint of their followers.
b) A group of followers were detected at “The Tun
Dr.Ismail International School of Johor”, in Padang Tengku,
Malaysia.

1025

2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

Figure 13. A Radical Group’s local followers

c) We show the Tweeter home pages of two followers
in Figure 13. One of tweeters on the followers list, is the
official twitter account of the organization itself highlighing
their Khilafah goals. Another one of the tweeters on the
followers list, talks about protests, jihad, mujahideen and
displays the image of a militia group on the background image
of the Tweeter page.
3) A Radical Group’s Transnational Followers.
a) Selecting another radical group on the chord diagram,
reveals a group of followers exchanging messages from
Singapore, Malaysia, and Indonesia.
b) One of the popular URLs shared among these
transnational group of tweeters is an article on a missile attack
into Israeli territory by a terrorist organization.
c) Following figure (Figure 14) shows the selected
organization on the Chord Diagram, one of the popular articles
exchanged among followers, and its orginal media source.

Figure 14. A Radical Group’s transnational followers.

ACKNOWLEDGMENT
This research was supported by US DoDs Minerva
Research Initiative Grant N00014-09-1-0815, Project leader:
Prof. Mark Woodward, Arizona State University, and the
project name is “Finding Allies for the War of Words:
Mapping the Diffusion and Influence of Counter-Radical
Muslim Discourse”.

REFERENCES
[1]

G. Goertz, Social science concepts : a user’s guide, Princeton, NJ:
Princeton University Press., 2006.
[2] G. Goertz and J. Mahoney, "Two-Level Theories and Fuzzy-Set
Analysis," Sociological Methods Research, vol. 33, pp. 497-538, 2005.
[3] H. Davulcu, S. Ahmed, S. Gokalp, H. Temkit, T. Taylor, M. Woodward
and A. Amin, “Analyzing Sentiment Markers Describing Radical and
Counter-Radical Elements in Online News,” in IEEE Symposium on
Social Intelligence and Networking (SIN-10), pp. 335-340, 2010.
[4] S. Tikves, S. Banerjee, H. Temkit, S. Gokalp, H. Davulcu, A. Sen, S.
Corman, M. Woodward, R. I. and A. Amin, “A System for Ranking
Organizations using Social Scale Analysis,” in Proc. of the International
Symposium on Open Source Intelligence & Web Mining (OSINT-WM),
2011.
[5] S. Tikves, S. Gokalp, M. Temkit, S. Banerjee, J. Ye and H. Davulcu,
"Perspective Analysis for Online Debates," Proceedings of International
Symposium on Foundation of Open Source Intelligence and Security
Informatics (FOISINT-SI), 2012.
[6] S. Tikves, S. Banerjee, H. Temkit, S. Gokalp, H. Davulcu, A. Sen, S.
Corman, M. Woodward, I. Rohmaniyah and A. Amin, " A System for
Ranking Organizations Using Social Scale Analysis," Social Network
Analysis and Mining Journal, Vols. ISSN (Print) 1869-5450 - ISSN
(Online) 1869-5469, pp. 1-16, 2012.
[7] L. Guttman, “The basis for scalogram analysis,” Measurement and
prediction, vol. 4, pp. 60-90, 1950.
[8] E. Durkheim, “The cultural logic of collective representations,” in
Social theory the multicultural and classic readings , Wesleyan
University: Westview Press, 2004, pp. 90-99.
[9] G. Simmel, Sociological Theory, New York: McGraw–Hill, 2008.
[10] A. Wallace, “Revitalization Movements,” American Anthropologist, vol.
58, pp. 264-281, 1956.

1026

2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
[11] A. Bayat, Making Islam Democratic: Social Movements and the PostIslamist Turn, Stanford University Press, 2007.
[12] C. Tilly, Social Movements, Boulder, Colorado, USA: Paradigm
Publishers, 2004.
[13] G.-M. J., "The rhetoric and reality: radicalization and political
discourse," International Political Science Review, vol. 33, no. 5, pp.
556-567, 2012.
[14] G.-M. J., "The rhetoric and reality: radicalization and political
discourse," International Political Science Review, vol. 33, no. 5, pp.
556-567, 2012.
[15] R. Crelinsten, "Analysing terrorism and counter-terrorism: A
communication model," Terrorism and Political Violence, vol. 14, p.
77–122, 2002.

[16] L. AlSumait, D. Barbara and C. Domeniconi, "On-line LDA: Adaptive
Topic Models for Mining Text Streams with Applications to Topic
Detection and Tracking," in Eighth IEEE International Conference on
Data Mining (ICDM '08), 2008.
[17] J. Liu, J. Chen and J. Ye, "Large-scale sparse logistic regression," in
Proceedings of the 15th ACM SIGKDD international conference on
Knowledge Discovery and Data Mining, 2009.
[18] D. Andrich, Rasch models for measurement, Sage, 1988.
[19] Y. Pawitan, In all likelihood: statistical modelling and inference using
likelihood., Oxford University Press, 2001.
[20] R. Tibshirani, "Regression shrinkage and selection via the lasso,"
Journal of the Royal Statistical Society Series B, vol. 58, no. 1, pp. 267288, 1996.

1027

Modeling and Analysis of Interactions
in Virtual Enterprises 
Hasan Davulcu, Michael Kifer, L. Robert Pokorny
C.R. Ramakrishnan, I.V. Ramakrishnan
Dept. of Computer Science
SUNY at Stony Brook
Stony Brook, NY 11794.

Abstract
Advances in computer networking technology and open
system standards are making the creation and management
of virtual enterprises feasible. A virtual enterprise is a
temporary consortium of autonomous, diverse, and possibly geographically dispersed organizations that pool their
resources to meet short-term objectives and exploit fastchanging market trends. For a virtual enterprise to succeed,
its business processes must be automated, and its startup
costs must be minimized.
In this paper we describe a formal framework for modeling and reasoning about interactions in a virtual enterprise.
Such a framework will form the basis for tools that provide
automated support for creation and operation of virtual enterprises.

Steven Dawson
Computer Science Laboratory
SRI International
333 Ravenswood Avenue,
Menlo Park, CA 94025.

Because virtual enterprises are composed of autonomous
entities and created for short-term objectives, their business
processes must be automated, and their startup costs must
be minimized. Therefore, the very process of creating a
virtual enterprise must be automated as much as possible.
In this paper we propose a formal framework based on
Concurrent Transaction Logic (CT R) for modeling and reasoning about interactions in a virtual enterprise. This framework will form the basis for a Virtual Enterprise Management Systems (VEMS). A VEMS is envisioned as a tool that
will enable declarative modeling and automatic enactment
of virtual enterprises. Moreover, since the proposed framework is rooted in logic, it will permit reasoning about the
intended behavior of virtual enterprises and support verification to ensure that virtual enterprises function as specified. For illustration purposes we focus on task coordination
and information interchange in a virtual enterprise.

1. Introduction

2. Example of a Virtual Enterprise

Advances in computer networking technology and open
system standards have made it practically feasible to create
and manage virtual enterprises. A virtual enterprise [6, 7] is
a temporary consortium of autonomous, diverse, and possibly geographically dispersed organizations that pool their
resources to meet short-term objectives and exploit fastchanging market trends. Upon realizing the objective, the
enterprise can possibly disband. For a virtual enterprise to
succeed, it must coordinate many varied tasks and facilitate data sharing among heterogeneous information systems
without compromising the proprietary information assets of
any individual organization. Network-based virtual enterprise is a powerful paradigm that has the potential to profoundly impact a wide range of business practices.

The following scenario, drawn from our CASP project1
experience illustrates a typical situation that would benefit
from the creation of a virtual enterprise. A maintenance
crew is doing a routine inspection of a rescue helicopter.
A defect is discovered in the strut assembly of the landing
gear. This component is an assembly of parts that was designed to last for the life of the aircraft and is not available
from the maintenance parts depot. The original supplier of
the assembly is no longer available to provide the assembly,
so an alternate source must be found. Currently this process
can easily take more than a year. To expedite the repair,

 This work is partially supported by a DLA/DARPA contract and by
the NSF grants IRI-9404629, CCR-9705998, 9711386, 9510072 9404921,
CDA-9504275, 9303181, INT-9600598

1 CASP, the Center for Agile Sources of Parts, manages a consortium
of more than 100 manufacturers with experience building military parts.
The Defense Logistics Agency uses CASP to identify and contract with
these manufacturers for parts that are no longer available from their original sources. At SUNY Stony Brook, we have developed tools to support
this identification process [12].

a request for the assembly is made to the Defense Logistics Agency’s (DLA) On-Demand Manufacturing Program
(ODM). To rapidly supply the needed assembly, the ODM
management team assembles a virtual enterprise made up
of several collaborating manufacturers, DLA engineers, auditors, and assorted information providers.
The ODM management team first identifies potential
manufacturers by matching their capabilities, previously inferred and stored in DLA’s knowledge bases, with the technical characteristics of the needed part. Bids are then solicited from the selected manufacturers. Some bids may
propose changes to part design to reflect current technology trends. Evaluation of such bids requires assembling
an engineering team with competence in the appropriate
technology. The engineering evaluation may itself require
searching heterogeneous data sources for information on
parts built previously using similar technology. Each bid
is analyzed and a contract is awarded to the group that best
meets the goals of price and timeliness. After the contract is
awarded, the ODM management team interacts continually
with the manufacturing group to identify and work around
problems that may arise as the assembly is being produced.
Observe from the scenario above that in a virtual enterprise, the interactions between entities are inherently complex, since the autonomy of the entities precludes any simplification of the interrelationships. Given the complexity,
diversity and short-term nature of virtual enterprises, automation of coordination and information interchange is essential. Without automation, creation and operation of a
virtual enterprise is extremely labor intensive and on a large
scale is well nigh impossible. To make automation possible
we need:

 A formal specification language to specify the structure of entities, processes and their interactions at a
high-level.
 Verification methods to ascertain that the virtual enterprise possesses certain key properties that are essential for it to function correctly. For example, in the
ODM virtual enterprise above, a key property is: Any
part produced using new technology must always be
approved by the engineering team.
 Techniques for automatically deriving coordination
and information interchange mechanisms from the formal description.
Analogous to a Data Base Management System (DBMS)
that provides tools for modeling and manipulating large collections of structured data, we envision a Virtual Enterprise
Management System (VEMS), providing a comprehensive
set of tools for modeling, analysis, and operation of a virtual
enterprise. A VEMS, based on a formal framework as out-

lined above, enables the creation of virtual enterprises that
meet their design specifications.

2.1. Enabling Technologies
Several core technologies are needed for the creation and
operation of a virtual enterprise. Internet technology and the
evolving standards for interoperability are important technologies that support the communication fabric of a virtual
enterprise. Workflow Management technologies serve the
coordination needs, and Mediation technologies address the
information needs of a virtual enterprise.
We will first examine the issues underlying coordination
in a virtual enterprise through the following example drawn
from our CASP project.
Example 2.1 (Bid Evaluation). Consider the workflow
represented in Figure 1, which represents a simplified
process of evaluating a single bid received in response to
DLA’s request for bids on the landing gear strut assembly.
The workflow represented by the graph consists of two
major parts: market evaluation of the bid (nodes B and C)
and technical evaluation of the bid (nodes D, E, F, G, H).
When both evaluations are completed (as indicated by the
AND-node A), a decision is made (node I).
The technical evaluation of a bid can be done either by an
internal team of engineers (nodes E, F), by a consultant firm
(nodes G, H), or both the internal team and the consultant
may need to be involved. These alternatives are indicated
by the OR-node D.
Coordinating such a workflow would have been quite
straightforward if not for the dependencies that cannot be
easily captured by graphs. Specifically, the following constraints, expressed informally, might be applicable in the bid
evaluation process:
1. if B:cost > $1000 then prefer E over G
This constraint says that if the cost of the part is above
$1000, then task E or task G must be executed (i.e.,
technical analysis should be performed), but E (internal evaluation) is preferred over G (hiring a consultant).
2. if B:cost < $1000 then not G
In other words, do not hire a consultant if the cost of a
part is not very high.
3. if occurs(E ) ^ occurs(G) then G before F
If it turns out to be necessary to do both the internal
evaluation and hire a consultant, then the consultant
should finish work before risk analysis gets into full
swing.
4. if E:recommendation = consultant then G
If our internal investigation concludes that a consultant
is needed, then hire one.

Financial
Analysis

Contractor
Evaluation

B

C

Receive
Bid

A

AND

D
Technical
Evaluation

Internal
Technical
Evaluation

Risk
Analysis

E

F

G

H

Hire
Consultant

Consultant
Billing

Make Decision
on Bid

I

OR

Figure 1. Bid Evaluation Workflow

5. if C:contractorRating = low then not G
If the contractor’s overall rating was determined to be
low, then spending additional funds on a consultant is
not justified.
6. if occurs(E ) then E before C
If internal technical evaluation must take place, then
do it before evaluating the contractor.
7. if occurs(G) then C before G
Do not hire a consultant before the contractor is fully
evaluated.

2
3. Logic-based Framework for Virtual Enterprises
As can be seen from Example 2.1, even in small workflows it might be difficult to fully comprehend all the consequences of the specifications. For instance, are the above
constraints consistent? If they are, are they consistent with
the task precedence order implied by the graph in Figure 1?
Can it happen that certain activities can never be executed
(which probably indicates a bug)? Apart from the consistency issue, it is important to be able to reason about the
properties of the workflows. For instance, to assess the correctness of our specifications, we might need to verify properties, such as “is it true that E (the internal technical evaluation of the bid) is always done?” Yet another issue is the
efficiency of the coordination process. Of course, we can
always select a possible execution and then check if it satisfies the constraints. If it does not, then we can try to find
another execution. Unfortunately, this process might force

the workflow scheduler to do exponential amount of work
and thus is inefficient.
Our contention is that representation of complex enterprises can and should be done using logic as a unifying principle. Furthermore, the very same language should be used
for modeling, reasoning, and coordination of these enterprises. We have already shown in [5] that a logic-based
formalism for workflows is as expressive as any current
method. The advantage of using a common logical framework is that scheduling of activities in a virtual enterprise
as well as verifying its operational properties naturally reduce to one and the same problem — logical unsatisfiability. Consequently, there is no need to devise distinct, complicated algorithms to deal with these seemingly unrelated
tasks.
Example 2.1 illustrates the potential of using powerful
logical formalisms, such as CT R, for workflow management. A number of formal approaches have been proposed
[14, 15, 11, 2, 8, 1, 4], but, unfortunately, most are incomplete: one approach might be appropriate for modeling
workflows, another might be able to reason, and yet another
one to schedule. The problem is that it is not easy to get the
different approaches to work together (see [5] for detailed
comparisons).
It is therefore desirable to find a formalism where all
three tasks can be done in a uniform way. For instance,
we would like to be able to represent the graph in Figure 1
and the constraints of Example 2.1 as a formula, and then
use the semantics and the logic’s proof theory to decide the
properties of the workflow.
An Overview of Concurrent Transaction Logic This
section provides a short summary of the CT R syntax, which

is used in this paper to represent workflows. Due to space
limitation, we cannot discuss the model theory of the logic
or its proof theory. Instead, we rely on the procedural reading of CT R statements.
Underlying the logic and its semantics is a set of database
states and a collection of paths. A path is a finite sequence
of database states. For instance, if s1 ; s2 ; :::; sn are database
states, then hs1 ; s2 ; :::; sn i is a paths of length n. Just as in
classical logic, CT R formulas assume truth values. However, unlike classical logic, the truth of CT R formulas is
determined over paths, not at states. If a formula, , is true
over a path hs1 ; :::; sn i, it means that  can execute starting at state s1 . During the execution, the current state will
change to s2 , s3 , ..., etc., and the execution terminates at
state sn . With this in mind, the intended meaning of the
CT R connectives can be summarized as follows:

2:
3:
4:
5:
6:
7:

O[B:cost < $1000] ! :OG
OE ^ OG ! OG 
 OF
O[E:recomm = consultant] ! OG
O[C:contractor = low] ! :OG
OE ! OE 
 OC
OG ! OC 
 OG

However, expressing constraint #1 is more involved
and requires the necessity modality, “2,” and the nonmonotonic aspects of CT R. In modal terms, 2 means that
 is the only transaction that can succeed from the present
state. The following CT R expression represents the preference constraint #1.








j

cute E or, if this is not possible, execute G.
Once constraints and the graph are specified in the logic,
the entire workflow can be represented as:



^

Bid Eval ^ (^7=1 Constr )



means: execute  then execute . In terms of
control flow graphs (cf. Figure 1), this connective represents arcs connecting adjacent tasks.

means that action E occurs somewhere on the execution
path. (O is not a new operator in Transaction Logic; it can
be expressed through other logical connectives).

means:  and must both execute concurrently,
in an interleaved fashion. This connective corresponds
to the “AND”-nodes in control flow graphs.
means:  and must both execute along the
same path. In practical terms, this is best understood in
terms of constraints on the execution. For instance, 
can be thought of as a transaction and as a constraint
on the execution of . It is this feature of the logic that
lets us specify temporal constraints as part of workflow
specifications.



_
means: execute  or execute
nondeterministically. This connective corresponds to the
“OR”-nodes in control flow graphs.

 : means: execute in any way, provided that this will
not be a valid execution of . There are many uses
for this feature. One is that, just as in classical logic,
the negation lets us define deductive rules which, in
terms of the workflows, correspond to sub-workflow
definitions. Negation is also an important component
in temporal constraint specifications.
Example 3.1 (Re-visiting Bid Evaluation). The following
is a representation of the control graph in Figure 1 in the
^ B means that one of the two
language of CT R, where A_
actions represented by A must execute (or, perhaps, both
must execute together, in parallel.

Bid ,Eval
,

A 
 (B 
 C ) j D 
 (E 
 F )_^ (G 
 H ) 
 I

(1)

The following is a representation of the coordination dependencies #2 to #7 in the language of CT R,where OE

1: O[B:cost > $1000] ! (OE _ (2:OE 
 OG))
Formally, this means: if [B:cost > $1000] then either exe-

i

i

(2)

Representing workflow control structure as logical formulas opens up a host of possibilities. For instance, it is now
possible to prove formally that the specifications are consistent (i.e., there is at least one valid schedule), that event E
(internal technical evaluation) always occurs, and that if it is
necessary to hire a consultant (i.e., G occurs), then contractor evaluation must finish before doing risk analysis (i.e., C
must happen before F ). An even more important question
is, will every bid evaluation workflow reach a decision stage
(node I)? It is not immediately obvious that the latter is not
guaranteed!2
Another interesting consequence of the logical representation of workflows is that there is a close relationship between proving a formula like (2) and run-time scheduling
of the corresponding workflow. Namely, valid schedules
are by-products of proving such formulas. As a result, there
is a direct relationship between the complexity of finding a
proof and the run-time cost of finding a schedule!
This leads to the following schedule optimization. Suppose we can transform the workflow representation (2) into
an equivalent formula but one that has a more efficient
proof. Then, in view of the above discussion, we can obtain
a more efficient run-time scheduling algorithm for (2). This
line of research was pursued in [5], where it was shown that
(2) can be transformed into equivalent formula of the form
2 Indeed,

suppose that the bid prices the job under $1000 and activity
However, the latter is prevented by
Constraint 4.

E recommends hiring a consultant.

	 ^ Constr1 , where 	 has a much more efficient proof than
Bid Eval ^ (^7=2 Constr ).
i

i

In other words, constraints 2 to 7 can be “compiled
away” and never need to be checked by the workflow scheduler at run time. Constraint 1, which expresses a preference
relation, is more difficult to handle and is a subject of further
research. 2

4. Information Interchange
So far we have illustrated our approach by modeling and
reasoning about coordination requirements in a virtual enterprise. Note though that there are other types of interactions in a virtual enterprise, notably interchange of information which is the topic of this section.
As discussed in Section 2.1, the information needs of a
virtual enterprise are best addressed by mediation technologies. Although much work has been done in the development of mediation techniques for heterogeneous information systems, there are particular issues in both security and
semantic mediation that arise in virtual enterprises, which
remain to be addressed. Below we describe these issues
and outline our approach to resolving them.
paragraphSecurity Mediation in Virtual Enterprises A
key aspect of information interchange in a virtual enterprise
is the need for entities to share possibly sensitive or proprietary information without compromising the security of
other information. For an information consumer in the enterprise, the main concern is to have timely access to all
required information. At the same time, the information
provider3 is most concerned with shielding its proprietary
information from unauthorized access. The following example illustrates these issues.
Example 4.1 (Component Manufacturing). Consider the
manufacturing phase of the ODM scenario (Section 2). The
bid process has been completed, and the virtual enterprise
is now engaged in producing the replacement landing gear.
One manufacturer, Entity A is responsible for producing a
component strut, which will be integrated into a larger strut
assembly by another manufacturer, Entity B. The manufacturing plan calls for Entity B to certify to the management
team that the entire strut assembly meets its specifications.
To make this certification, Entity B requires the quality assurance report on the component strut from Entity A. Entity A’s quality report details critical properties and testing
results for the strut at various stages in the manufacturing
process. While this information is needed by Entity B to
certify the strut assembly, the report also reveals details of a
proprietary manufacturing process used by Entity A. To satisfy Entity B’s requirements while protecting its own pro3 Note that a single entity in a virtual enterprise may act as both an
information consumer and an information provider.

prietary information, Entity A agrees to give Entity B access
to the quality report, provided that Entity B does not disclose proprietary information from the report to third parties. 2
Thus, the primary goal of security mediation in a virtual enterprise is to ensure that all and only the information needed by other entities be made available to them. To
achieve this goal, we must first cope with the fact that a virtual enterprise is composed of autonomous entities that may
employ a wide range of incompatible security mechanisms.
More specifically, an effective solution for security mediation in a virtual enterprise must address the following areas
of heterogeneity:

 Security interfaces: Different organizations use different mechanisms for communication, identification,
and authentication.
 Security policies: Different organizations may formulate their security policies in terms of different authorization models and enforce access control at varying
levels of granularity.
Moreover, a virtual enterprise brings an added dimension
in that security considerations become intertwined with the
flow of information throughout the enterprise, as the nondisclosure requirement in Example 4.1 indicates. Indeed, it
is the interaction between security policies and information
dissemination in a virtual enterprise that is the focus of our
proposed research in security mediation.
Approach to Security Mediation Given that the autonomy of entities in a virtual enterprise precludes internal reorganization, the best option for integrating them for secure
information interchange is to wrap them. Such a wrapper
will provide a bridge between the system-specific interfaces
of individual entities and a uniform external interface that
enables interaction with other wrapped entities. The capabilities of a virtual enterprise wrapper specific to security
mediation will include:

 A uniform interface for secure communications (e.g.,
SSL), identification, and authentication.
 Mapping between entity-specific security models and
an enterprise-level security model.
 Support for fine-grained access control.
Furthermore, virtual enterprise wrappers will be specified
at a high level in a uniform logical framework that supports
their automated generation.
The idea of using wrappers (and even wrapper generators) to integrate heterogeneous systems is not new; it has
been explored previously, e.g., in [10, 9]. In addition, our

ongoing research effort at SRI on Secure Access Wrappers
(SAW) involves the development of wrapping techniques
for integrating multilevel secure (MLS) databases in highassurance information systems. The novel aspects of wrappers in a VEMS context are: (1) the specification of wrappers that manage both security and semantic heterogeneity
in a uniform logical framework; and (2) support for automated generation of such wrappers that are guaranteed to
meet their specifications.
Security Constraints in a Virtual Enterprise At a high
level, information interchange in a virtual enterprise involves three distinct considerations: (1) semantic interrelationships among the data of different entities, (2) the local
security policies, and (3) the requirements of information
dissemination among the entities. Often considerations (2)
and (3) will conflict. Detecting and resolving such conflicts
is crucial to proper functioning of a virtual enterprise and
requires analysis of the interactions among all three of the
above considerations.
For illustration, consider again the situation in Example 4.1. Since Entity B depends, in part, on Entity A’s
quality report for certification of the strut assembly, Entity B’s information model includes a semantic relationship
that captures the inclusion of information from Entity A’s
quality report in Entity B’s own certification report. In addition, Entity B’s workflow specifies the transmission of its
certification report to the management team. In the absence
of other information, it can be deduced that Entity B will
forward (parts of) Entity A’s quality report to the management team. Note that this release of information would be
in conflict with Entity A’s nondisclosure constraint on Entity B. Detecting this conflict requires knowledge of the semantic relationship, the control and information flow, and
Entity A’s security policy. Resolving the conflict requires
either a relaxation of Entity A’s nondisclosure constraint, or
a new constraint on Entity B’s release of its certification report (to remove the proprietary portions of Entity A’s quality
report).
Recall that the primary goal of security mediation in a
virtual enterprise is to permit all and only required information to be shared. Implicit in this goal is the need to limit
exposure of any entity’s security policy (since otherwise it
may be possible to draw inferences pertaining to sensitive
or proprietary information). This need to protect individual
security policies further implies that detection and resolution of security conflicts in the virtual enterprise, which is
inherently global (enterprise-wide), should be carried out
locally and in a distributed manner.
We briefly outline an approach for solving this problem.
During the formation of a virtual enterprise, each entity
develops a specification of its information holdings, information requirements, security policy, and workflow. From

this specification, a set of access requirements is automatically deduced, detailing what access to information (from
other entities) will be needed. A negotiation phase ensues
in which each entity requests the needed access rights from
others. Each request for an access right is either approved or
denied by the target entity, based on its own security policy
and access rights granted by others. Observe that a solution
to this problem will likely involve an intricate protocol that
must be verified. A logical framework will greatly facilitate
the specification and verification of such a protocol.
As with workflow coordination, automated support for
specification, reasoning, and enforcement of security constraints in a virtual enterprise is essentially a problem of
logical inference. Moreover, since security policies interact with process rules, reasoning about these interactions
within the same logical framework proposed for workflow
coordination is the best approach to security mediation in a
virtual enterprise environment.

5. Towards a Prototype Virtual Enterprise
Management System (VEMS)
As a proof of concept, we are currently building a prototype VEMS tool kit for modeling and enacting virtual enterprises. A primary purpose of this effort is to verify our
ideas in practice and use it CASP, which is an on-demand
manufacturing venture.
We envision our prototype to have a user-friendly graphical design tool, which would permit the user to specify
workflows and mediators at a high level. The graphical tool
will be structured such that graphical and textual specifications can be intermixed (e.g., complex temporal and transition constraints, or complex semantic mappings between
data sources could be specified textually). We have already
begun implementing a cross-platform workflow design tool.
At present, this tool can specify complex control flows, and
we are now working on the specification of a protocol that
will enable it to communicate with the logical subsystem at
the semantic level.
The virtual enterprise system infrastructure will be supported by XSB, the logic-based deductive engine that implements CT R. The XSB system is a deductive engine
developed here at Stony Brook. XSB is our choice for
several reasons: it is currently known as the most efficient implementation of deductive databases that outperforms other similar systems by one to two orders of magnitude [13]; it extends logic programming with higher-order
programming (HiLog [3]); it provides support for nonmonotonic reasoning (through its support for well-founded
semantics for negation); and it incorporates special indexing structures that considerably simplify the implementation of CT R. Furthermore, XSB is well-integrated into the
overall computing infrastructure. It runs on most platforms

(including Windows and the various flavors of Unix), it interfaces to database systems through ODBC drivers, has a
Perl interface, and Java interface is currently under development. XSB has been installed in over a thousand sites
around the world. More information on XSB can be found
at http://www.cs.sunysb.edu/˜ sbprolog.

References
[1] N. Adam, V. Atluri, and W. Huang. Modeling and analysis
of workflows using petri nets. Journal of Intelligent Information Systems, 10(2):131–158, March 1998.
[2] P. Attie, M. Singh, A. Sheth, and M. Rusinkiewicz. Specifying and enforcing intertask dependencies. In Intl. Conference on Very Large Data Bases, 1993.
[3] W. Chen, M. Kifer, and D. Warren. HiLog: A foundation
for higher-order logic programming. Journal of Logic Programming, 15(3):187–230, February 1993.
[4] P. Chrysanthis and K. Ramamritham. ACTA: A framework
for specifying and reasoning about transaction structure and
behavior. In ACM SIGMOD Conference on Management of
Data, pages 194–203, May 1990.
[5] H. Davulcu, M. Kifer, C. Ramakrishnan, and I. Ramakrishnan. Logic based modeling and analysis of workflows. In
ACM Symposium on Principles of Database Systems, pages
25–33, Seattle, Washington, June 1998.
[6] A. Dewey et. al. The impact of NIIIP virtual enterprise technology on next generation manufacturing. In Proceedings of
Conference on Agile and Intelligent Manufacturing Systems,
1996.
[7] C. Gilman, M. Aparicio, J. Barry, T. Durniak, H. Lam, and
R. Ramnath. Integration of design and manufacturing in a
virtual enterprise using enterprise rules, intelligent agents,
STEP, and workflow. In Proceedings of SPIE Vol. 3203,
pages 160–171, 1997.
[8] R. Gunthor. Extended transaction processing based on dependency rules. In Proceedings of the RIDE-IMS Workshop,
1993.
[9] L. Haas, D. Kossmann, E. Wimmers, and J. Yang. Optimizing queries across diverse data sources. In Proceedings
of the 23rd International Conference on Very Large Data
Bases (VLDB ’97), 1997.
[10] J. Hammer, H. Garcia-Molina, S. Nestorov, R. Yerneni,
M. Breunig, and V. Vassalos. Template-based wrappers in
the TSIMMIS system. In Proceedings ACM SIGMOD International Conference on Management of Data (SIGMOD
’97), pages 532–535, 1997.
[11] M. Orlowska, J. Rajapakse, and A. ter Hofstede. Verification problems in conceptual workflow specifications. In Intl.
Conference on Conceptual Modelling, volume 1157 of Lecture Notes in Computer Science, Cottbus, Germany, 1996.
Springer-Verlag.
[12] R.Hopkins, D.Warren, A.Kahn, I. Ramakrishnan, J.Jones,
M.Kifer, T.Swift, and L. Pokorny. CASP: A virtual parts
supplier. In Proceedings of International Conference on Agile Manufacturing, 1997.

[13] K. Sagonas, T. Swift, and D. Warren. XSB as an efficient
deductive database engine. In ACM SIGMOD Conference
on Management of Data, pages 442–453, New York, May
1994. ACM.
[14] M. Singh. Semantical considerations on workflows: An
algebra for intertask dependencies. In Proceedings of the
International Workshop on Database Programming Languages, Gubbio, Umbria, Italy, September 6–8 1995.
[15] M. Singh. Synthesizing distributed constrained events from
transactional workflow specifications. In Proceedings of 12th IEEE Intl. Conference on Data Engineering, pages 616–
623, New Orleans, LA, February 1996.

Definitions
Descriptions

Mining Search-Phrase

from

Item

Hung V. Nguyen and Hasan Davulcu
Department of Computer Science and Engineering, Arizona State University
Tempe, AZ 85287, USA
hung,hdavulcu@asu.edu

Abstract- In this paper, we develop a model for representing
term dependence based on Markov Random Fields and present
an approach based on Markov Chain Monte Carlo technique
for generating phrase definitions. This approach can use a
small corpus of keyword matching and a random sample of
other product descriptions for an advertiser's search-phrase to
effectively mine and rank alternative but highly relevant searchphrase definitions. These definitions, which are search-phrases
themselves, can then be provided as alternative phrases to an
advertiser.
I. INTRODUCTION

The World Wide Web has made a dramatic transition from
its early beginnings as a distributed repository of browsable information into a dominant medium for conducting ecommerce. In particular, it has become a mainstream advertising medium for retail goods with online advertising reaching
$16 billion in revenue in 2006. Because of this immense
commercial power of the Web, the number of vendors, both
large and small, who are setting up online presence and
subscribing to search advertising continues to proliferate.
In search advertising, vendors subscribe to triplets of the
form < searchphrase, product -url, bid >. For example,
a shoe store may subscribe to advertise its "NIKE Airmax
180" product as "running shoes" by specifying the triplet
< runntingshoes, NIKEAirmaxl8O, $.50 >. This triplet
indicates that whenever a web search phrase mentions "running shoes" or a web site contains "running shoes" related
information, this vendor would like to list its "NIKE Airmax
180" product and agrees to pay 50 cents per click.
Nevertheless, vendors can not anticipate all possible ways
in which to advertise for their products and shoppers can not
guess all possible ways to search and find a product. Many
times, user's search query may not be a perfect description of
their information needs. Even when the information is somewhat well described, a search engine or information retrieval
system may not be able to retrieve documents matching the
query as stated. We call this phenomenon is "semantic gap"
between search phrases and item's information. For example,
if a web page mentions "stable lightweight shoes", a smart
algorithm should be able to detect the close relationship
between "stable lightweight shoes" and "running shoes".
Our work attempts to bridge this gap, in the context of
highly descriptive and data rich product information, such as
"Nike stable lightweight shoes ..." and two or three word long
popular search phrases such as "running shoes".

978-1-4244-1837-4/08/$25.00 (© 2008 IEEE

In order to bridge this gap, we need to substitute original
search phrase, such as "running shoes" with a definition that
contains other parameters corresponding to technical characteristics of matching product descriptions.
Previous studies on mining definitions have focused on
query substitution [1], advertising-page ranking by using
genetic programming [2], or syntactic improvements such
as spelling changes, synonym substitutions, taxonomy based
generalizations and specialization of search-phrases [3]. A
different approach adopted in recent studies [4], [5], [6] is
to utilize frequent item-set mining algorithms to identify alternative definitions. However, such algorithms generate large
numbers of frequent item-sets as possible definitions and we
observe the need for more robust filtering algorithms that
can use both keyword-matching and a sample of product
descriptions to rank the candidate definitions and enlist the
most promising definition phrases.
In this paper, we develop a model for representing the
dependence among phrases and present a robust automated
algorithm that can use a small corpus of keyword matching
and a random sample of other product descriptions for a
popular search-phrase to effectively mine and rank alternative
but highly relevant search-phrase definitions.
Basically, we reduce the problem significantly by modeling
the joint distribution as a product of conditional distributions,
modeled as a Markov Random Field. We observe that, in
textual product descriptions, the critical statistical relationship
among terms is the co-occurrence. This kind of correlation is
undirected in nature. Moreover, it is hard to determine whether
some term causes the presence of another term in a description
in general. Hence, we propose an undirected graphical model
that can be succinctly used to model the dependence among

objects.

Hence we propose an undirected graphical model that can
be succinctly used to model the dependence among objects
and it is also suitable for modeling and reasoning with the
term dependencies within highly descriptive data-rich product
descriptions.
In our approach, an undirected graphical model is a graph
G(V, E) in which V is the set of random variables, E is
the set of edges connecting pairs of random variables. In
our context, the set of random variables corresponds to the
set of terms. The rule in the form of target phrase <term,Aterm2 ... Aterm, is said to be a Phrase Definition rule

1346

ICDE 2008

if given G and a real number T, (0 < T < 1), the conditional
probability P(target phrase terml, term2,..., term,) >
T. The target phrase is the phrase to be defined using other
terms on the right hand side of the rule.
Our experimental results illustrate that our technique yields
high recall (above 90% on average) as well as high precision
(above 84 % on average).
Our contributions are: (1) Define a model for estimating
conditional probabilities via sampling true joint distribution
among phrases, (2) Develop an algorithm that utilizes these
conditional probabilities to find relevant definitions of search
phrases that in turn can serve as alternative highly relevant
definitions.
II. PROBLEM DESCRIPTION

have their own different ways to describe the products that
they consider as "luxury bedding". We can view each isolated
description as a sample of that distribution. It is important
to note that we may not need to make any assumption
about this distribution. Finding the true joint distribution
helps us compute correct conditional probabilities of the form
p(luxury bedding unknown features). These unknown
features that we are interested in are the terms that can be
used to describe luxury bedding. If these terms are identified,
they can be used to annotate relevant web pages or relevant
records to increase the chance of getting more hits (of the
pages or records). This also helps customers find more relevant
product pages given their queries. Given these observations,
we need to take into account these hypotheses when we
formulate the problem in the next section: 1) Term dependency
model does not need to incorporate the information about the
order of terms and several context features since the product
descriptions, as observed above, are free-style and no context
features can entirely capture correctly the product information.
We argue that if the model is simple and works, then let it be.
Otherwise, we may need to encode more features into the
model to increase its efficiency. 2) However, the model may
need to take into account the distance among terms within a
document or a product description.
In order to explore the above type of descriptions, we model
the dependencies among terms in the databases of product
descriptions. In the next section, we discuss an undirected
graphical model known as Markov random fields (MRF) or
Markov network to model our problem and develop techniques
for mining phrase definition rules.

In this section, we intuitively motivate the use of probabilistic inference techniques to solve our problem. Given a
search phrase, the descriptive information about that phrase
is an important clue for efficiently finding the relevant items
(item descriptions in catalog databases) that match the search
phrase. We adopt the standard statistical IR assumption that
inter-term relevance is reflected in the co-occurrence statistics
over the corpus and use it to generate definition rules in the
above format.
We motivate the modeling using an example as follows.
The product catalogs of a general web-based vendor such as
Amazon or Ebay contain hundreds of thousands of records
of all kinds of products ranging from books to video games,
apparel or electronics. These products can be listed by many
other online sellers and individuals. The descriptions of the
products can be very diverging and may not have a uniform
III. MODEL
structure even for the same kind of products or products from
similar categories. In order to match the relevant items to
A Markov Random Field (MRF) has several components:
search phrases, the use of the classification technique as a a set V = {1, ...,m} of site v; a neighborhood system N =
filter does not suffice. For example, although "running shoes" {Nv Cv V} in which each Nv is a subset of sites in V
and "dress shoes" are in the same category "men shoes", these describing the neighbors of v. These two sets form a undirected
two types of shoes can be described by totally different sets graph called G; a field (or set) of random variables is denoted
of terms. Another example is that "running shoes" and "tennis as X = {Xv v C V}. In our problem, given a set of terms,
shoes" are in a narrower category which is "athletic shoes" the set V of vertices in G is the set of the terms. Two terms
and yet, their descriptions are still different. Furthermore, each present two neighbor nodes in the MRF if the co-occurrence
seller or individual can have his or her own way to describe of these two terms in the whole collection passes a threshold
the features as well as the functions and the condition of 6. Each random variable Xv takes a value xv in some set
the products. We may experience this using Ebay. This web L = {li, ...l1r} of the possible labels. When sampling method
site has a "user feedback" solution that offers shoppers to is used to recover the joint distribution among terms, for each
browse different product descriptions with or without specific node in the MRF (i.e. each term), the corresponding random
keywords/terms. But in the scenario of online advertising, variable can only be 0 or 1 (whether that term appears in
an accurate automated technique is a must. Even the set of a specific sample). Therefore, L = {0, 1}; a set of potential
terms used to describe a specific product is finite, the order functions ~0k (also called factors or clique potentials) one for
and the form of terms can be different in different product each clique k in G. Note that a clique is a maximal clique
descriptions given by different describers. This inconsistency if it is not contained within any other clique. Each ~0k maps
is due to the fact that there are too many ways that a product from all possible joint assignments (to the elements to k) to
can be described and this heterogeneity causes difficulties for non-negative real numbers. The joint probability of all random
advertisers. Irrelevant or missing annotation leads to a loss variables in the MRF denoted as Pr(X = x) is computed as:
of revenue for advertisers. Let us assume that there exists
implicitly an underlying true joint distribution of features that
Pr(X = x) = z I| k(xk)
are used to describe what luxury bedding is. Different vendors
kCC(G)

1347

of the ad, the authors use various ad sections (bid phrase, title,
body) as a basis for the ad vector. In a follow-up work [2],
the authors use genetic programming paradigm to develop
the ranking algorithm for ads. The results show that genetic
Running
programming finds matching functions that significantly improve the matching compared to the best method (without
page expansion) reported in [8]. In a more recent work by
Durbl
lightweight
Broder et. al. [9], the authors introduce the class taxonomy
to classify ads. This phase acts as a filter before conducting
the phrase extraction process and this technique shows a clear
Fig.
1.
Sample MRF with {running,cushion,durablej, improvement. In these previous studies, the accuracy or the
{running,cushion,stable} are maximal 3-cliques and {lightweight,stable} is relevance of the ads is compared with documents using cosine
a maximal 2-clique
theta which is different from our study where we find the
alternative definitions for the search phrase. The alternative
query terms are matched against the product description to
where C(G) is the set of cliques in G, Xk is the values of judge the relevance by domain experts.
random variables in kth clique, Z = Exx HMcC(G) f(c) iS
V. CONCLUSIONS
the normalized factor of the distribution. For each clique in
G, we define potential function ~Ok as:
We performed an extensive evaluation of our rule miner syswhich relies on MRF for term dependency and sampling
tem,
=lo df (ql, .;,i)
~Ok =log
technique to estimate the goodness of the candidate alternative
queries (the terms in the right high side of the definition rules).
We
do not report our results because of space constraints. As
where df(q,..., qi) denotes the number of times the terms
a summary of our conclusions, our approach produced rules
qi, ..., qi occur together in the collection and IC is the number
of documents (product descriptions) in the collection. By that yield high recall as well high precision. As expected, our
saying "terms ql, ..., qi occur together", it means these terms approach also avoids placing irrelevant items in high ranked
appear together in a description but they must appear in the positions. This criterium is very important in the context of
same sentence. However, this distance constraint is relaxed for Content Targeted Advertising computing.
the target phrase. More specifically, the target phrase can be
VI. ACKNOWLEDGMENT
anywhere in the description. Other terms must appear in the
The first author is funded in part by a grant from the
same sentence to be considered as co-occurring terms.
Vietnam Education Foundation (VEF). The opinions, findings,
As stated in Section I, in order to learn the phrase definition and conclusions stated herein are those of the authors and do
rules target phrase <- term,Aterm2...Atermn, we want to not necessarily reflect those of VEF
find the set of terms terml, term2, ..., termn that maximize
REFERENCES
the probability P(target phrase termj, term2, ..., termn).
More specifically, we find the set of variables {X1, ..., Xn } so [1] R. Jones, B. Rey, 0. Madani, and Greiner, "Generating query substitutionss," in Proceedings of the 15th WWW Conference, 2006, pp. 387-396.
that
Cushion

Stable

f(X) Pr(XXi, ..., Xn)= Pr(X A, Xn)
=

(1)

is maximized. Here, X is the random variable in the MRF and
presents the target phrase, Xis are the random variables in the
MRF and present other terms and A=(X1, ..., Xn- 1) In order
to compute the above type of probability, we do the inference
in the MRFs by developing an inference algorithm utilizing
Markov Chain Monte Carlo technique [7] and derive another
algorithm to mine the rules with highest probability values.
The mined rules are ranked based on the probability values of
corresponding conditional probabilities in the form 1.
IV. RELATED WORKS
Ribeiro-Neto et. al. [8] study various strategies to match
pages to ads based on extracted keywords. Their approach
employs the vector space model to represent ads and pages.
The first five strategies proposed in their study match the pages
and the ads based on the cosine theta of the angle between the
ad vector and the page vector. To identify the important part

[2] A. Lacerda, M. Cristo, M. A. Goncalves, W. Fan, N. Ziviani, and
B. Ribeiro-Neto, "Learning to advertise," in SIGIR '06. New York,
NY, USA: ACM Press, 2006, pp. 549-556.
[3] E. Terra and C. L. Clarke, "Scoring missing terms in information retrieval
tasks," in CIKM '04. New York, NY, USA: ACM Press, 2004, pp. 50-58.
[4] H. Davulcu, H. V. Nguyen, and V. Ramachandran, "Boosting item
findability: Bridging the semantic gap between search phrases and item

information," Enterprise Information Systems VII, vol. VII, pp. 215-222,

2006.
[5] B. Liu, C. W. Chin, and H. T. Ng, "Mining topic-specific concepts and
definitions on the web," in WWW '03. New York, NY, USA: ACM Press,
2003, pp. 251-260.
[6] H. V. Nguyen, H. Davulcu, and V. Ramachandran, "Boosting item
findability: Bridging the semantic gap between search phrases and item
description," International Journal of Intelligent Information Technologies, vol. 2, pp. 1-20, 2006.
[7] C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan, "An introduction
to mcmc for machine learning," Machine Learning, vol. 50, pp. 5-43,
2003.
[8] B. Ribeiro-Neto, M. Cristo, P. B. Golgher, and E. S. de Moura,
"Impedance coupling in content-targeted advertising," in SIGIR '05. New
York, NY, USA: ACM Press, 2005, pp. 496-503.
[9] A. Broder, M. Fontoura, V. Josifovski, and L. Riedel, "A semantic
approach to contextual advertising," in SIGIR '07. New York, NY, USA:
ACM Press, 2007, pp. 559-566.

1348

Clustering and Mapping Related News about
Violence Events on their Time-lines
Syed Toufeeq Ahmed, Sukru Tikves, and Hasan Davulcu
Department of Computer Science and Engineering, Arizona State University,
{toufeeq, sukru, hdavulcu}@asu.edu
Abstract—Keeping track of news stories and events as they
progress can be a tedious job, but as every day routine most
of the web users read and follow many stories and events in
news. If an analyst in her area has to follow and map all
these according to the time-line they happen, the task quickly
becomes overwhelming. We present an online tool which attempts
to ease the analyst’s task of ﬁnding all news articles about
an event, and sorting and mapping them on a time-line. We
implemented an incremental clustering algorithm working on
real-time incoming news, experimenting with different feature
sets, including named entities and sentence overlap methods.
We evaluated these approaches using Document Understand
Conference (DUC) datasets.

I. I NTRODUCTION AND R ELATED W ORK
To understand and extract meaningful structure from document continuously arriving news streams, [1] presented a
“burst of activity” model. Seminal work [2], [3] in event detection and tracking explored clustering algorithms like agglomerative clustering, augmented Group Average Clustering. Wellknown idf-weighted cosine coefﬁcient metric method [4] was
also used to detect and track topics. A real-time news event
extraction system [5] extracts violence and disaster events by
processing the news article using extraction grammars on each
document in the cluster.
II. O NLINE T OOL FOR V ISUALIZING AND M APPING N EWS
C LUSTERS AND NAMED E NTITIES
Online system works in three steps: 1) Data Scraping from
online sources, pre-processing and Named Entity Recognition
(NER); 2) Detecting new violent events and then clustering of
related items to a thread. 3) Sorting and mapping these related
events on a timeline and location visualizer. For recognizing
the violent events (like bombing or kidnapping) in the text,
we built an ontology by recursively extracting synonym sets
using an initial seed (around 240) from WordNet1 . Next
step is incremental clustering of news articles using different
feature sets (Term vector, Named Entity vector, Mixed (term
+ named entities) and Sentence Overlap). Our preliminary
experiments with sentence overlap method have shown that
looking for similar or exact sentences in news articles produces
almost perfect precision. However, in order to improve the
overall recall, we also utilized a secondary metric based on
cosine similarity between document term vectors. Last step is
sorting and mapping the articles on an time-line and location

map. The online system utilizes MIT’s SIMILE library for
time-line visualization. The cluster of related articles, and
related entities are shown as ﬁlter-able components, and the
mentioned locations are displayed on an interactive map using
the Bing maps API.
III. E VALUATION
The system was evaluated using Document Understanding
Conference (DUC)2 datasets for years 2004, 2005, and 2006.
For each year of data, 100 articles spanning 10 different news
events have been randomly selected. The evaluation has been
done using F1-Measure, which summarizes both precision and
recall of the algorithm. Table I summarizes the performance
results of different approaches. Entity based vectors significantly under performed due to loss of information. We’ve
also experimented combining entities and terms in a single
representation as Mixed vector.
TABLE I
F1-M EASURE PERFORMANCE .

Method
Term Vector
Entity Vector
Mixed Vector
Sentence Overlaps

DUC’04
0.96
0.90
0.95
0.95

DUC’05
0.71
0.46
0.59
0.75

DUC’06
0.67
0.34
0.52
0.73

Avg
0.78
0.57
0.69
0.81

The online web site can be accessed at: http://code.azcips.
info/DemoSite/. Currently, it hosts 474 event threads and
corresponding named entities in the news articles ﬁltered from
January 2007 subset of New York Times annotated corpus3 .
R EFERENCES
[1] J. Kleinberg, “Bursty and hierarchical structure in streams,” Data Mining
and Knowledge Discovery, vol. 7, no. 4, pp. 373–397, 2003.
[2] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang, “Topic
detection and tracking pilot study: Final report,” in Proc. of the DARPA
broadcast news transcription and understanding workshop, vol. 1998.
[3] Y. Yang, T. Pierce, and J. Carbonell, “A study of retrospective and on-line
event detection,” in Proc. of 21st ACM SIGIR. ACM, NY, USA, 1998,
pp. 28–36.
[4] J. Schultz and M. Liberman, “Topic detection and tracking using idfweighted cosine coefﬁcient,” in Broadcast News Workshop’99 Proceedings. Morgan Kaufmann, 1999, p. 189.
[5] H. Tanev, J. Piskorski, and M. Atkinson, “Real-Time News Event Extraction for Global Crisis Monitoring,” in Proc. of the 13th (NLDB 2008),
London, UK, 2008, pp. 24–27.
2 Document

Understanding Conference: http://duc.nist.gov/
Corpus: http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2008T19
3 NYT

1 WordNet:

wordnet.princeton.edu/

978-1-4244-6446-3/10/$26.00 © 2010 IEEE

175

ISI 2010, May 23-26, 2010, Vancouver, BC, Canada

SocialCom/PASSAT/BigData/EconCom/BioMedCom 2013

Partitioning and Scaling Signed Bipartite Graphs
for Polarized Political Blogosphere
Sedat Gokalp

M’hamed Temkit

Hasan Davulcu

I. Hakki Toroslu

Computer Science
Arizona State University
Sedat.Gokalp@asu.edu

Mathematical Statistics
Arizona State University
Mhamed.Temkit@asu.edu

Computer Science
Arizona State University
Hasan.Davulcu@asu.edu

Computer Engineering
Middle East Technical University
Toroslu@ceng.metu.edu.tr

Abstract—Blogosphere plays an increasingly important role
as a forum for public debate. In this paper, given a mixed set
of blogs debating a set of political issues from opposing camps,
we use signed bipartite graphs for modeling debates, and we
propose an algorithm for partitioning both the blogs, and the
issues (i.e. topics, leaders, etc.) comprising the debate into binary
opposing camps. Simultaneously, our algorithm scales both the
blogs and the underlying issues on a univariate scale. Using
this scale, a researcher can identify moderate and extreme blogs
within each camp, and polarizing vs. unifying issues. Through
performance evaluations we show that our proposed algorithm
provides an effective solution to the problem, and performs
much better than existing baseline algorithms adapted to solve
this new problem. In our experiments, we used both real data
from political blogosphere and US Congress records, as well
as synthetic data which were obtained by varying polarization
and degree distribution of the vertices of the graph to show the
robustness of our algorithm.

I.

1)
2)

Using this scale, a researcher can identify both the moderate and extreme blogs within each camp, and the polarizing
vs. unifying issues. Partitioning and scaling help a researcher
to better understand the structure of a social, political or
economic debate, or even the details of an emerging geopolitical conﬂict in the world. While extremist ends of a scale,
may represent blogs with irreconcilable viewpoints, in some
cases, moderate blogs may represent viewpoints that are more
amenable to engage in a constructive dialog through a set
of unifying issues. Moderates may sympathize with some
of the claims and grievances of the other side. Longitudinal
analysis using our proposed algorithms could reveal interesting
dynamics, such as, moderates from opposing camps could be
in the process of forming a coalition by making the necessary
compromises to reach a consensus. All the while, moderates
may be alienating extremists in their own camps who may
choose to focus on polarizing issues only, and lash out violent
or demonizing rhetoric on everyone else who do not share their
exclusivist viewpoints.

I NTRODUCTION

Blogosphere plays an increasingly important role [1] as
a forum of public debate, with knock-on consequences for
the media, politics, and policy. Hotly debated issues span
all spheres of human activity; from liberal vs. conservative
politics, to extremist vs. counter-extremist religious debate, to
climate change debate in scientiﬁc community, to globalization
debate in economics, and to nuclear disarmament debate in
security. There are many applications [2], [3], [4], [5], [6]
for recognizing politically-oriented sentiment in texts. Previous
work [7] studied linking patterns and discussion topics of
political bloggers by measuring the degree of interaction
between liberal and conservative blogs, and to uncover their
differences. In this paper, given a mixed set of blogs debating
a set of related issues from two opposing camps, we propose
an algorithm to determine (i) which blog lies in which camp,
(ii) what are the contested issues, and, (iii) who are mentioned
as the key individuals within each camp.

To the best of our knowledge, simultaneous scaling and
partitioning on signed weighted bipartite graphs has not been
studied in the literature, and this paper is the ﬁrst attempt to
introduce the problem and provide an effective solution and
evaluation strategies.
Major contributions of this paper are: an iterative algorithm,
named Alternatingly Normalized CO-HITS (ANCO-HITS), to
propagate the scores on a signed bipartite graph to solve
the partitioning and scaling problems described above; and
performance evaluations of blog/issue partitioning and scaling
algorithms using synthetic data sets which were obtained by
varying polarization and degree distribution of signed bipartite
graphs, and analysis with two real data sets: (i) partitioning
and scaling of Republicans/Democrats and their roll call votes1
based on the 111th US Congress voting records, and (iii)
partitioning and scaling of the top 22 liberal and conservative
blogs, and the most inﬂuential individuals mentioned in these

Bipartite graphs [8], [9], [10] have been widely used to
represent relationships between two sets of entities. We use
bipartite graphs to model the relationships between blogs
and issues (i.e. topics, individuals, etc.) mentioned within
blogs. We use signed weighted edges to represent opinion
strengths, where positive edges denote support, and negative
edges denote opposition between a blog and an issue.
We develop algorithms to solve the following problems on
signed bipartite graphs modeling blog debates:
978-0-7695-5137-1/13 $26.00 © 2013 IEEE
DOI 10.1109/SocialCom.2013.32

Partitioning of both the blogs, and the underlying
issues mentioned in blogs, into two opposing camps;
Scaling of both the blogs and the underlying issues
on a univariate scale such that the position of a
vertex is closer to the positions of the vertices it
is connected with positive edges, and further away
from the positions of the vertices it is connected with
negative edges.

1 http://thomas.loc.gov/home/rollcallvotes.html

168

blogs. In our experiments, variance in polarization relates to the
distributions of the ratio of vertices corresponding to extremes
vs. moderates.

cluster are more similar than those in distinct clusters. Spectral
clustering [21], [11], [12] is a powerful clustering method that
is able to outperform K-means clustering [22], which has a
major drawback of not being able to separate clusters that
are non-linearly separable in input space [21]. The method is
based on computing the eigenvalues of the normalized version
of the graph Laplacian, and has theoretical connections with
the normalized cut of the graph. In particular when clustering a
bipartite graph into two balanced clusters, the second smallest
positive eigenvalue [12] is the solution to the normalized cut of
the graph. In recent years, several authors have used spectral
clustering to analyze bipartite graphs [10]. Furthermore, some
work has been done to take into account a signed adjacency
matrix by using an augmented adjacency matrix [23]. In this
paper, spectral clustering was also used as one of our baseline
methods for partitioning and scaling signed bipartite graphs.

Alongside our proposed ANCO-HITS, we also evaluated
two baseline algorithms, namely CO-HITS [8] and spectral
clustering [11]. Although Co-HITS was designed for scaling
unsigned bipartite graphs, it can be directly applied for scaling signed bipartite graphs, and partitioning by considering
the signs of vertex values. Spectral clustering algorithm was
designed for partitioning of graphs, and it can also produce
a scale by using the component values of the eigenvector
associated with the second smallest positive eigenvalue of the
graph Laplacian [12], [13]. Our experiments showed that the
ANCO-HITS algorithm is the only robust algorithm in the
presence of variance in polarization and vertex degrees.
The rest of this paper is organized as follows. We review
related work in Section 2. Section 3 presents the problem
formulation. In Sections 4 and 5, we present the spectral clustering and CO-HITS algorithms as the baselines. In Section 6,
we present our proposed ANCO-HITS algorithm. We describe
and report experimental evaluations in Section 7. Finally, we
present conclusions and future work in Section 8.
II.

III.

P ROBLEM F ORMULATION

A. CoScaling for signed bipartite graphs
Given
•

G = (U ∪ V, A) is a bipartite graph consisting of
two disjoint sets of vertices U and V , and a signed
adjacency matrix A

•

U = {u1 , u2 , . . . , um }, a set of m vertices

•

V = {v1 , v2 , . . . , vn }, a set of n vertices

•

A ∈ Rm×n , where aij represents the signed edge
between ui and vj

R ELATED W ORK

Scaling vertices of a graph based on the network structure
rather than individual properties has been of great interest for
more than a decade. Two most well-known algorithms are
the PageRank [14] and the HITS [15] algorithms. They were
designed to rank the vertices of graphs with positive weighted
edges. Spectral analysis show that both PageRank and HITS
algorithms converge. An important distinction between the two
algorithms is that; the HITS algorithm provides two different types of rankings corresponding to hubs and authorities,
whereas PageRank provides only a single ranking.

Find

Many data types from data mining applications can be
modeled as bipartite graphs, examples include terms and
documents in a text corpus, customers and items purchased
in market basket analysis and bloggers writing about current
issues.

•

X = (x1 , x2 , . . . , xm ), where xi ∈ R is the assigned
value of the vertex ui

•

Y = (y1 , y2 , . . . , yn ), where yi ∈ R is the assigned
value of the vertex vi

such that

Based on variations of HITS and PageRank, many researchers have proposed algorithms. In [8], the authors propose
a modiﬁcation of the HITS algorithm to work on bipartite
graphs called CO-HITS. The main difference between HITS
and CO-HITS is that; HITS provides two scores for each
vertex, whereas CO-HITS provides one score for each type
of vertex. In this paper, we use CO-HITS as one of the
baseline algorithms, and in order to overcome its deﬁciencies,
we extend it with normalization steps.

•

sgn(xi ) and sgn(yi ) shall determine the polarity of
the vertices i.e. −1 and +1 as the opposing polarities

•

xi value for a vertex ui should be closer to the
yj values of the vertices that it supports (connects
positively), and further away from the yk values of
the vertices that it opposes (connects negatively). The
magnitudes of xi and yj denote the extremity of the
nodes ui and vj . i.e. magnitudes closer to 0 meaning
more moderate and larger magnitudes meaning more
extreme.

The clustering coefﬁcient was ﬁrst introduced in [16] to
measure how much multiplicative transitivity property the
graph exhibits, which reﬂects the tendency of the vertices to
form small groups. In [17], authors deﬁne a new coefﬁcient using the multiplicative transitivity for signed graphs to measure
structural equilibrium.

Figure 1 depicts a perfectly polarized bipartite graph. The
two axes X and Y represent the univariate scale for the nodes
in U and V . The vertices to the right of zero have positive
values, and the vertices to the left have negative values on
the scale. A green solid line between the nodes ui and vj
represents support, and a red dashed line represents opposition.

Data mining methods such as clustering have been used
quite extensively for exploratory data mining applications [18],
[19]. Clustering analysis [20] provides a partitioning of the
data into subsets, called clusters such that the objects in a

Figure 2 shows an example of two vertices; u1 being
extreme and u2 being more moderate. u1 supports the vertices
of same polarity, and opposes the vertices of the opposite
polarity. However, u2 has mixed support and opposition.
169

0

u1 u2 u3

um-2um-1 um

u1

0

X

X

u2

0

Y

vn-2 vn-1 vn Y

v 1 v2 v3

Fig. 1.

0

X

u2

0

Y

Extremity vs Degree

Spectral clustering uses an adjacency matrix with all positive entries. However, our problem assumes a signed adjacency
matrix. One of the common techniques to circumvent this
problem is to augment the matrix into a bigger matrix [10],
[25], [24], such that all entries are positive. The ﬁrst half of
the augmented matrix is reserved for the entries with positive
values, and the second half is reserved for the entries with
negative values.

X

Y

Extreme vs. Moderate vertices

V.

Although partitioning algorithms can be utilized to detect
the polarity of vertices, it is not possible to distinguish extremes from moderates. Scaling overcomes this problem and
makes it possible to compare two vertices of same polarity.
In this paper, we are not only able to compare pairs of
vertices, but also provide the exact locations on the scale,
therefore providing valuable information about the shape of
the distribution as well.

CO-HITS

In [8], the authors modify the well-known HITS [15]
algorithm and propose the CO-HITS algorithm which is used
to rank vertices of a bipartite graph. Even though the adjacency
matrix has only positive values in the original HITS paper, the
theory still holds for adjacency matrices with signed entries.
Algorithm 1 describes the steps of the CO-HITS algorithm
for the co-scaling problem.

To solve this co-scaling problem, we present two baseline
methods. The ﬁrst one is a common modiﬁcation [24], [10],
[25] of the well-known Spectral Clustering approach to work
on graphs with signed edges. The second one is the COHITS [8] algorithm, that is a modiﬁcation of the well-known
HITS algorithm, designed for bipartite graphs.

Data: Adjacency matrix A
Result: Scale vectors X and Y
Initiate X <0> = (1, 1, . . . , 1) ;
Initiate Y <0> = (1, 1, . . . , 1) ;
repeat
Update X;
Update Y ;
until X vector converges;
Algorithm 1: Iterative update procedure for CO-HITS

Finally, we compare these baseline methods with a novel
algorithm we developed for co-scaling problem, named Alternatingly Normalized CO-HITS (ANCO-HITS).
IV.

Y

the components of the eigenvector associated with the smallest
positive eigenvalue of the Laplacian.

Perfectly polarized bipartite graph

u1

Fig. 2.

Fig. 3.

X

S PECTRAL C LUSTERING

The update functions for X and Y are deﬁned as follows:
n
m


<k−1>
<k>
x<k>
=
a
y
y
=
aij x<k>
ij
i
j
i
j

Spectral clustering [12] uses spectral graph theory, which
is the study of graphs using linear algebra methods. In this
context, for a given graph, its edge set is represented by
an adjacency matrix. The eigenvectors of the normalized
Laplacian of the adjacency matrix are used to partition the
graph into clusters, where objects in a cluster are more similar
than those in distinct clusters. Spectral clustering incorporates
the properties of a graph via the adjacency matrix and is able to
outperform K-means clustering in many situations, especially
in the presence of non-convex groups of data. This method
has close connections with the normalized cut [11] of the
graph. In particular when clustering a bipartite graph into two
balanced clusters, the second smallest positive eigenvalue of
the Laplacian matrix [21] is the solution to the problem of
minimizing the normalized cut of the graph.

j=1

i=1

and convergence is achieved when

 <k>
X
− X <k−1> 2 < 
with  a small positive value.
The drawback of this method is its sensitivity to the vertex
degrees. Two vertices with same polarities but different degrees
will result with the higher degree vertex having a higher score
on the scale.
For example, let us consider two vertices u1 and u2 with
u1 having a smaller degree than u2 . However, let u1 be more
polarized than u2 as shown in Figure 3. In this scenario,
the corresponding scale values for u1 and u2 should satisfy
|x1 | > |x2 |. But, this will not be the case with the CO-HITS.
This suggests a better algorithm that accounts for the negative
impact of degree variation through some normalization mechanism.

Spectral clustering embeds the data into the subspace of
the eigenvectors of the Laplacian. In this paper, we will use
spectral clustering to partition both types of vertices of a signed
bipartite graph into two polarities and provide a scale for the
vertices. We will do this by using the sign and the value of
170

VI.

A LTERNATINGLY N ORMALIZED
CO-HITS (ANCO-HITS)

111th US Senate

According to our problem formulation, the values of the
vertices on the scale shall not be sensitive to their degrees, but
rather be sensitive to what kind of relations they have with the
other set of vertices.

Fig. 4. Vote matrix of the 111th US Senate after scaling with ANCO-HITS

For this purpose, we propose ANCO-HITS algorithm,
which introduces a normalization mechanism to address the
issue of degree sensitivity of CO-HITS. The proposed method
uses the same iteration procedure described in Algorithm 1.
The update functions for X and Y are modiﬁed such that they
are normalized as follows:
n
m


aij yj<k−1>
aij x<k>
i
j=1
i=1
<k>
<k>
xi
=
= 
yj
n
m

|aij |
|aij |
j=1

i=1

Fig. 5. Bipartite graph of the 111th US Senate after scaling with ANCO-HITS

VII.

E XPERIMENTS & E VALUATIONS

To validate our algorithm, we have used two different
datasets that are US Congress and political blogosphere. In
addition to real data, we introduced a model to generate
synthetic data to analyze the performance of the algorithms
for various parameters.

amount of partisanship. The ﬁrst two columns of Table I
provide information about this data as well as the partitioning
accuracies of the algorithms.
We analyzed the congressmen that have been assigned to
be moderate by each algorithm. We observed that the baseline
algorithms tend to have the congressmen with less number of
votes (i.e. lesser degree) to be moderate regardless of their
partisanship. On the other hand, when we queried the names
assigned to be most moderates by the ANCO-HITS, for both
Democrats and Republicans, we were able to identify a number
of supporting articles matching the ANCO-HITS scaling [26],
[27], [28], [29].

A. US Congress
The US Congress has been collecting data since the very
ﬁrst congress of the US history. This data has been encoded
as XML ﬁles and publicly shared through the govtrack.us
project2 .From various types of data available at the project
site, we collected the roll call votes for the 111th US Congress
which includes The Senate and The House of Representatives
and covers the years 2009-2010. According to The Library of
Congress3 ,

Figure 4 depicts the vote matrices of the 111th US
Congress, with rows representing the congressmen and the
columns representing the bills. The light green color represents
’Yea’ votes, and dark red represents ’Nay’ votes. Scaling these
graphs leads to a re-ordering of the rows and columns where
congressmen and bills are co-clustered together.

A roll call vote guarantees that every Member’s vote
is recorded, but only a minority of bills receive a roll
call vote.
The 111th Senate has 1084 senators and the data contains
their votes on 696 bills, and The 111th House has 451 representatives and the data contains their votes on 1655 bills.

Figure 5 represents the bipartite graph of the 111th US
Congress data after scaling both the congressmen and the bills
with ANCO-HITS. The light green colored edges represent
’Yea’ votes, and dark red represents ’Nay’ votes. Similar to our
motivating Figure 1, this ﬁgure also shows partisan behavior
in the 111th US Congress.

We extracted the adjacency matrix A ∈ {−1, 0, 1}|U |×|V | ,
with U vertices representing the congressmen, and the V
vertices representing the bills. The values aij are 1 if the congressman ui votes ‘Yea’ for the bill vj , -1 if the congressman
votes ‘Nay’, and 0 if he did not attend the session.

B. Political Blogosphere

The aforementioned scaling algorithms will scale both the
congressmen and the bills. In presence of partisanship5 in the
Congress, the sign of the scale values for the congressmen
should correspond to the Democrat and Republican parties,
and the magnitude of the scale values should represent the

As Web 2.0 platforms gained popularity, it became easy
for web users to be a part of the web and express their
opinions, mostly through blogs. Most blogs are maintained
by individuals, whereas there are also professional blogs with
a group of authors. In this study, we focus on a set of popular
political liberal and conservative blogs that have clearly declared positions. These blogs contain discussions about social,
political, economic issues and related key individuals. They
express positive sentiment towards individuals whom they
share ideologies with, and negative sentiment towards others.
In these blogs, it is common to see criticism of people within
the same camp, or support for people from the other camp.

2 http://www.govtrack.us/data
3 http://thomas.loc.gov/home/rollcallvotes.html
4 Normally, each congress has 100 senators (2 from each state), however in
many of the congresses, there are unexpected changes on the seats caused by
displacements or deaths.
5 Partisanship can be deﬁned as being devoted to or biased in support of a
party.

171

In this experiment, we collected a list of 22 most popular
liberal and conservative blogs from the Technorati6 rankings.
For each blog, we fetched the posts for the period of 6 months
before the 2008 US presidential elections (May - October,
2008) due to the intensity of the debates and discussions.

distributions for varying the degree distributions of vertices.
Degree distributions were obtained by N (μ = 30, σ = 2),
N (μ = 30, σ = 5) and N (μ = 30, σ = 10) in order to
evaluate the effect of degree variance on the performance of
the algorithms. We also experimented with different μ values
of 10, 30, and 50 in order to measure the effect of density
variations of the graph on the performance of the algorithms,
which did not show any signiﬁcant impact.

We use AlchemyAPI7 to run a named entity tagger to
extract people names from the posts, and entity-level sentiment
analysis which provided us with weighted sentiment (positive
values indicating support, and negative indicating opposition)
for each person. This information was used to synthesize a
signed bipartite graph, where the blogs and people correspond
to the two sets of vertices U and V . The aij values of the
adjacency matrix A are the cumulative sum of sentiment values
for each mention of the person vj by the blog ui .

For each polarization and degree distribution we tested
the performance of two baseline algorithms and our proposed
algorithm, and made the following observations:

To get a gold standard list of the most inﬂuential liberal and
conservative people, we used The Telegraph List8 for 2007.
The third column of Table I provides information about this
data as well as the partitioning accuracies of the algorithms.

•

Across all polarizations, as the vertex degree variance
increases, overall errors for baseline algorithms increase due to their sensitivity to vertex degrees.

•

Between the baseline algorithms, spectral clustering
consistently outperforms CO-HITS.

•

Even though spectral clustering performs almost as
good as our proposed ANCO-HITS for bimodal and
uniform polarization distributions, when the polarization is high, as in the other two distributions, its
performance degrades.

•

As polarization increases, ANCO-HITS performance
also increases. In case of perfect polarization, ANCOHITS has almost no error.

C. Synthetic Data
The actual partitioning information for the real datasets
were available, which made it possible to check the partitioning
accuracy of the algorithms. However, to thoroughly check the
scaling accuracy of the algorithms, we developed a method to
generate random bipartite graphs with the following properties:
•

The degrees and the scores for the vertices in U
and V follow independent probability distribution with
varying parameters and shapes.

Overall, in every single case our proposed ANCO-HITS
algorithm outperforms the baselines.
VIII.

Following procedure describes the method to generate
random graphs.

In this paper, we introduced a new problem for scaling and
partitioning signed weighted bipartite graphs. We adapted two
existing algorithms, and proposed a new algorithm to solve this
problem. We used both real data from political blogosphere
and US Congress records, as well as synthetic data to evaluate
these algorithms. Our experiments showed that our proposed
algorithm is very effective and outperforms the two other
baselines. The algorithms in source code and the test data is
available online at http://www.PartisanScale.com/paperdata

RandomGraph(m, n, Ddegree , Dscale )
Let A, U , V , X and Y be deﬁned as in Deﬁnition 1.
1)
2)
3)
4)
5)

C ONCLUSIONS & F UTURE W ORK

Create m vertices for U , and n vertices for V
Independently assign degrees 1 ≤ d(ui ) ≤ m and
1 ≤ d(vj ) ≤ n with probability distribution Ddegree
Independently assign −1 ≤ xi , yj ≤ +1 values with
probability distribution Dscale .
Generate an adjacency list of pairs (ui , vj ), where
each node ui and vj occur in the list d(ui ) and d(vj )
times respectively.
For each adjacency pair (ui , vj ), assign the entry in
the adjacency matrix aij with value
a) sgn(xi ) × sgn(yj ), with probability 1 − (1 −
|xi |)(1 − |yj |)
b) −sgn(xi ) × sgn(yj ), with probability (1 −
|xi |)(1 − |yj |)

In real world graphs, it is rarely the case that all the nodes
have the same degree. Some bloggers have more extensive
coverage than others. Similarly, some senators miss or abstain
on more votes than others. Partisanship shall not be affected
by the variance in node degrees. A marginal example is the
case when a senator supersedes a deceased one through the
end of the term. All the baseline algorithms assign this low
degree senator to a moderate location, even though he or she
can be extremely partisan. ANCO-HITS manages the degree
bias by a normalizing scheme, and places this senator to an
appropriate location.

In our experiments, the number of vertices of the graph is
m = n = 100. We used four different distributions for varying
polarization. These were perfectly polarized, Beta, bimodal
and uniform distributions [30]. Perfectly polarized distribution
was obtained by mapping all vertices to the extremes of both
sides with equal probability. We used three different normal

In order to analyze longitudinal voting patterns for the
US Congresses, we downloaded all voting records since the
1st US Congress, executed ANCO-HITS, and produced an
interactive visualization system as described in [31]. This
system is accessible online at www.PartisanScale.com

6 http://technorati.com/

Our future work involves developing techniques for detecting and presenting both friendly and unfriendly neighborhoods
of a blog or an issue, and their agreements and disagreements.

7 http://www.alchemyapi.com/
8 http://www.telegraph.co.uk/news/uknews/1435447/The-top-US-

conservatives-and-liberals.html

172

TABLE I.

D ESCRIPTIVE SUMMARIES OF THE GRAPHS FOR EACH DATASET WITH THE PARTITIONING ACCURACIES FOR EACH ALGORITHM

Vertices in U
Vertices in V
Graph Density

111th US Senate
64 Democrat + 42 Republican
Senators
696
Bills
88.36%

111th US House
268 Democrat + 183 Republican
Representatives
1655
Bills
91.23%

Political Blogosphere
13 Liberal + 9 Conservative
Blogs
20 Liberal + 14 Conservative
People
39.04%

100.00%
100.00%
100.00%

99.11%
99.56%
99.56%

75.39%
98.21%
98.21%

Spectral Clustering
CO-HITS
ANCO-HITS

ACKNOWLEDGMENTS

[17]

J. Kunegis, A. Lommatzsch, and C. Bauckhage, “The slashdot zoo:
mining a social network with negative edges,” in Proceedings of the
18th international conference on World wide web. ACM, 2009, pp.
741–750.
[18] I. Dhillon, J. Fan, and Y. Guan, “Efﬁcient clustering of very large
document collections,” Data mining for scientiﬁc and engineering
applications, pp. 357–381, 2001.
[19] N. Slonim and N. Tishby, “Document clustering using word clusters via
the information bottleneck method,” in Proceedings of the 23rd annual
international ACM SIGIR conference on Research and development in
information retrieval. ACM, 2000, pp. 208–215.
[20] P. Berkhin, “Survey of clustering data mining techniques,” Grouping
Multidimensional Data: Recent Advances in Clustering, pp. 25–71,
2006.
[21] I. Dhillon, Y. Guan, and B. Kulis, “Kernel k-means: spectral clustering
and normalized cuts,” in Proceedings of the tenth ACM SIGKDD
international conference on Knowledge discovery and data mining.
ACM, 2004, pp. 551–556.
[22] J. Hartigan and M. Wong, “Algorithm as 136: A k-means clustering
algorithm,” Journal of the Royal Statistical Society. Series C (Applied
Statistics), vol. 28, no. 1, pp. 100–108, 1979.
[23] J. Kunegis, S. Schmidt, A. Lommatzsch, J. Lerner, E. De Luca,
and S. Albayrak, “Spectral analysis of signed graphs for clustering,
prediction and visualization,” in Proc SDM. Citeseer, 2010.
[24] I. Dhillon, “Co-clustering documents and words using bipartite spectral
graph partitioning,” in Proceedings of the seventh ACM SIGKDD
international conference on Knowledge discovery and data mining.
ACM, 2001, pp. 269–274.
[25] X. Fern and C. Brodley, “Solving cluster ensemble problems by bipartite
graph partitioning,” in Proceedings of the twenty-ﬁrst international
conference on Machine learning. ACM, 2004, p. 36.
[26] S. T. Dennis, “Senate moderates look for more inﬂuence,”
http://www.rollcall.com/issues/56 90/-203808-1.html.
[27] J. Newton-Small, “Can ben nelson get a bipartisan stimulus win,”
http://www.time.com/time/politics/article/0,8599,1877535,00.html.
[28] “Factions
in
the
republican
party
(united
states),”
http://en.wikipedia.org/wiki/Factions in the Republican Party (United States).

We would like to thank Dananjayan Thirumalai for helping
us collect the political blogosphere data. This research was
supported in part by US DOD Minerva Research Initiative
grant N00014-09-1-0815.
R EFERENCES
[1]
[2]

[3]
[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]
[12]

[13]

[14]
[15]
[16]

D. Drezner and H. Farrell, “The power and politics of blogs,” Public
Choice, vol. 134, pp. 15–30, 2008.
T. Mullen and R. Malouf, “A preliminary investigation into sentiment
analysis of informal political discourse,” in AAAI symposium on computational approaches to analysing weblogs (AAAI-CAAW), 2006, pp.
159–162.
R. Malouf and T. Mullen, Graph-based user classiﬁcation for informal
online political discourse, 2007.
M. Thomas, B. Pang, and L. Lee, “Get out the vote: Determining
support or opposition from congressional ﬂoor-debate transcripts,” in
In Proceedings of EMNLP, 2006, pp. 327–335.
M. Bansal, C. Cardie, and L. Lee, “The power of negative thinking:
Exploiting label disagreement in the min-cut classiﬁcation framework,”
Proceedings of COLING: Companion volume: Posters, pp. 13–16, 2008.
W. Lin and A. Hauptmann, “Are these documents written from different
perspectives?: a test of different perspectives based on statistical distribution divergence,” in Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. Association for Computational
Linguistics, 2006, pp. 1057–1064.
L. A. Adamic and N. Glance, “The political blogosphere and the 2004
u.s. election: divided they blog,” in Proceedings of the 3rd international
workshop on Link discovery, ser. LinkKDD ’05. New York, NY, USA:
ACM, 2005, pp. 36–43.
H. Deng, M. Lyu, and I. King, “A generalized co-hits algorithm and
its application to bipartite graphs,” in Proceedings of the 15th ACM
SIGKDD international conference on Knowledge discovery and data
mining. ACM, 2009, pp. 239–248.
M. Rege, M. Dong, and F. Fotouhi, “Co-clustering documents and words
using bipartite isoperimetric graph partitioning,” in Data Mining, 2006.
ICDM’06. Sixth International Conference on. IEEE, 2006, pp. 532–
541.
H. Zha, X. He, C. Ding, H. Simon, and M. Gu, “Bipartite graph partitioning and data clustering,” in Proceedings of the tenth international
conference on Information and knowledge management. ACM, 2001,
pp. 25–32.
U. V. Luxburg, “A tutorial on spectral clustering,” 2007.
A. Ng, M. Jordan, and Y. Weiss, “On spectral clustering: Analysis and
an algorithm,” in Advances in Neural Information Processing Systems
14: Proceeding of the 2001 Conference, 2001, pp. 849–856.
J. Shi and J. Malik, “Normalized cuts and image segmentation,” Pattern
Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8,
pp. 888 –905, aug 2000.
L. Page, S. Brin, R. Motwani, and T. Winograd, “The pagerank citation
ranking: Bringing order to the web.” 1999.
J. Kleinberg, “Authoritative sources in a hyperlinked environment,”
Journal of the ACM (JACM), vol. 46, no. 5, pp. 604–632, 1999.
D. Watts and S. Strogatz, “Collective dynamics of small-world networks,” Nature, vol. 393, no. 6684, pp. 440–442, 1998.

[29]
[30]
[31]

173

“Blue dog coalition,” http://ross.house.gov/BlueDog/Members/.
V. Rohatgi and A. Saleh, An introduction to probability and statistics.
Wiley-India, 2008.
S. Gokalp and H. Davulcu, “Partisan scale,” in Proceedings of the 21st
international conference companion on World Wide Web. ACM, 2012,
pp. 349–352.

World Wide Web (2007) 10:157–179
DOI 10.1007/s11280-007-0021-1

Information Extraction from Web Pages Using
Presentation Regularities and Domain Knowledge
Srinivas Vadrevu · Fatih Gelgi · Hasan Davulcu

Received: 2 May 2006 / Revised: 19 September 2006 /
Accepted: 8 January 2007 / Published online: 2 March 2007
© Springer Science + Business Media, LLC 2007

Abstract World Wide Web is transforming itself into the largest information resource making the process of information extraction (IE) from Web an important
and challenging problem. In this paper, we present an automated IE system that
is domain independent and that can automatically transform a given Web page
into a semi-structured hierarchical document using presentation regularities. The
resulting documents are weakly annotated in the sense that they might contain
many incorrect annotations and missing labels. We also describe how to improve the
quality of weakly annotated data by using domain knowledge in terms of a statistical
domain model. We demonstrate that such system can recover from ambiguities in the
presentation and boost the overall accuracy of a base information extractor by up to
20%. Our experimental evaluations with TAP data, computer science department
Web sites, and RoadRunner document sets indicate that our algorithms can scale up
to very large data sets.
Keywords information extraction · web · page segmentation · grammar induction ·
pattern mining · semantic partitioner · metadata · domain knowledge ·
statistical domain model

S. Vadrevu (B) · F. Gelgi · H. Davulcu
Department of Computer Science and Engineering, Arizona State University,
Tempe, AZ 85287, USA
e-mail: svadrevu@asu.edu
F. Gelgi
e-mail: fagelgi@asu.edu
H. Davulcu
e-mail: hdavulcu@asu.edu

158

World Wide Web (2007) 10:157–179

1 Introduction
The vast amount of data on the World Wide Web poses many challenges in devising
effective methodologies to search, access and integrate information. Even though
the current Web represents a large collection of heterogeneous sources, their data
presentation and domain specific metadata adheres to certain regularities.
In this paper we present automated algorithms for gathering metadata and their
instances from collections of domain specific and attribute rich Web pages by using
the presentation regularities that are present within the Web pages and by utilizing
domain knowledge. Our system works without the requirements that (a) the Web
pages need to share a similar presentation template or (b) that they need to share
the same set of metadata among each other. Hence, we cannot readily use previously
developed wrapper induction techniques [4, 16, 18] which require that the item pages
should be template driven or the ontology driven extraction techniques [8, 10, 11]
which require that an ontology of concepts, relationships and their value types is
provided apriori in order to find matching information.
Our system proceeds in three phases. In the first phase, we use the presentation
regularities that are present in the Web pages to organize the content in a hierarchical
XML-like structure and annotate the labels in the Web page with their semantic
roles. In the second phase, we build a statistical domain model from the hierarchical
content structures, exploiting the domain knowledge. In the third phase, we utilize
the statistical domain model to improve the semantic role annotations of the labels
within each of the Web pages.
Unlike plain text documents, Web pages organize and present their content using
hierarchies of HTML structures [23]. Different logical blocks of content, such as
taxonomies, navigation aids, headers and footers as well as various other information
blocks such as indexes, are usually presented using different HTML structures.
Furthermore, whenever an information block contains a list of items, these items
themselves are presented consecutively and regularly using repeating similar HTML
substructures. In this paper, we present an information extraction (IE) system,
called Semantic Partitioner that uses these presentation regularities to transform a
given Web page into a semi-structured document, with the labels in the Web page
annotated with their semantic roles. However the labels may be weakly annotated as
the automated IE system is prone to make certain mistakes due to the ambiguity in
presentation.
In addition to the presentation regularities present in the Web, there are also
regularities in the metadata that can be exposed to refine the structured information
extracted from the automated systems. Sometimes the presentation regularities
alone are not sufficient to automatically structure the content in a Web page and
we need to utilize the domain knowledge in order to organize the content. To
obtain such domain knowledge we combine the weakly annotated data extracted
from individual Web pages by automated IE systems into a statistical domain
model. The performance of IE systems can be enhanced by the use of such domain
knowledge. The extracted knowledge is probabilistic because the extracted data may
be unreliable and it may change depending upon the context. In this paper, we
present how to extract such domain knowledge in terms of a statistical domain model
from the weakly annotated data and how to utilize the domain knowledge to improve
the performance of automated IE systems.

World Wide Web (2007) 10:157–179

159

The key contributions and innovations of our system can be summarized as
follows:
–
–
–

A domain independent IE system that organizes the content in a Web page into
a weakly annotated semi-structured document using presentation regularities.
A statistical domain model that is built from the weakly annotated data obtained
from an automated IE system.
A domain model based IE system that uses domain knowledge and presentation
regularities to further improve the annotations made by the statistical domain
model.

We currently do not align the extracted metadata or instance information with any
available ontology or knowledge structure. Various existing approaches developed
for mapping and merging ontologies [20] can be utilized for this purpose. We also
currently do not process plain text inside the Web pages, i.e., any text fragments that
contain modal verbs. However, we hypothesize that the metadata extracted from the
attribute rich segments of the Web pages can be used to extract information from
text with the help of natural language processing techniques [15].
The rest of the paper is organized as follows. Section 2 presents the background
material about information extraction and tagging Web pages. Section 3 presents the
Semantic Partitioner system that performs IE on Web pages using the presentation
regularities. In the following Section 4, we explain how to utilize the hierarchical
structures obtained by Semantic Partitioner and build a statistical domain model.
Next, we describe how to improve the semantic role annotations by using the domain
model in Section 5. Section 6 presents complexity analysis of our algorithms and
Section 7 briefly discusses the related work. We provide the experimental results
with various data sets in Section 8 and conclude the paper in Section 9 with some
future directions of our work.

2 Background on information extraction
Information extraction is the process of extracting pertinent or relevant information
from text, databases, semi-structured and multimedia documents in a structured
format. An information extraction system performs this task either automatically or
with human intervention by using techniques from various areas like machine learning, data mining, pattern mining, and grammar induction. Since, the World Wide
Web is transforming itself into the largest information source ever available, the
process of information extraction from the Web is both challenging and interesting
problem, which is the primary focus of this paper.
An example of an information extraction task is shown in Figure 1. The example
shows the fragment of a faculty home page (Figure 1a), and the structured information extracted from this Web page (Figure 1b). The Web page contains three
publications, with each publication having at most five attributes: title, authors,
conference, address, and presentation. These attributes can also be referred to as
metadata as they describe the data. The Web page does not contain the metadata
labels themselves, but it contains values from these metadata attributes. In addition,

160

World Wide Web (2007) 10:157–179

Figure 1 Example of an information extraction task.

each of the publication record may not contain values for all the five attributes and
some of the values may be missing. The goal of the information extraction system in
this task is to be able to extract the data associated with all their metadata labels as
shown in Figure 1b.

3 Information extraction using presentation regularities
In this section we discuss the semantic partitioning algorithm that works by exploiting
the presentation regularities in Web pages. Our semantic partitioning algorithm can
detect repeating HTML substructures and organize the content into a hierarchical
content structure consisting of groups of instances, while skipping HTML blocks that
do not constitute a group. The semantic partitioning algorithm requires no training
and works automatically on each Web page.
The semantic partitioning algorithm works in four phases: page segmentation,
grouping, promotion and semantic role annotation.
3.1 Page segmentation
A Web page usually contains several pieces of information [5] and it is necessary
to partition a Web page into several segments (or information blocks) before
organizing the content into hierarchical groups. In this section we describe our page
segmentation algorithm that partitions a given Web page into flat segments.

World Wide Web (2007) 10:157–179

161

The page segmentation algorithm relies on the DOM tree representation of the
HTML Web page and traverses it in a top–down fashion in order to segment the
content of the page, which lies at the leaf nodes. In the HTML DOM tree of a Web
page, the formatting information about a label can be revealed from its root-to-leaf
path in the DOM tree. For example, a root-to-leaf path html.body.table.td.b.a of a
label l reveals that l is bold and has a link. We utilize this information to identify the
presentation of a label and distinguish the presentation formatting of one label with
that of another label in the Web page.
We define a segment as a contiguous set of leaf nodes within a Web page.
The algorithm aims to find homogeneous segments, where the presentation of the
content within each segment is uniform. The algorithm employs a split-traverse based
approach that treats all uniform set of nodes as homogeneous segments and further
splits non-uniform nodes into smaller segments until each segment is homogeneous.
The algorithm relies on the concept of entropy in order to determine whether a
segment is homogeneous.
We define the notion of entropy of a node in a DOM tree in terms of the
uncertainty in the root-to-leaf paths under the node. Our algorithm is based on the
observation that a well organized or homogeneous system will have low entropy.
We use this principle while traversing the DOM tree from root to leaf nodes in a
breadth-first fashion, and split the nodes until each and every segment in the DOM
tree is homogeneous.
Entropy is generally used to measure the uncertainty in the system. Hence if any
random variable has low entropy, then there is less uncertainty in predicting the
possible values that the random variable can take. In our case, we view each node as
a random variable in terms of the root-to-leaf paths Pi s under the node. We define a
set of nodes in the DOM tree to be homogeneous if the paths in the random variable
are uniformly distributed, and it is easy to predict the next path within the set.
Definition 1 The path entropy H P (N) of a node N in the DOM tree can be defined
as
H P (N) = −

k


p(i) log p(i),

i

where p(i) is the probability of path Pi appearing under the node N.
We use the concept of path entropy to partition the DOM tree into segments.
The algorithm to partition a given Web page into segments is given in Algorithm 1.
This algorithm is initialized with a vector of nodes containing just the root node of
the DOM tree of the Web page. The MedianEntropy is calculated as the median
of the path entropies of all the nodes in the DOM tree. Essentially we assume the
nodes whose path entropy is less than the median entropy of all the nodes in the
DOM tree to be homogeneous and output it as a pure segment. We use median
entropy because it acts as a representative sample for an average value in the set. If
a node is not homogeneous, then we traverse the children of the nodes in order to
find the homogeneous segments. Our page segmentation algorithm is able to identify
four segments in the Faculty home page as shown in Figure 2a. Please note that we
currently ignore any text fragments in the Web page that contains modal verbs as
they add to the noise in identifying the patterns.

162

World Wide Web (2007) 10:157–179

Algorithm 1 Page Segmentation Algorithm
PageSegmenter
Input: Nodes[], a node in the DOM tree
Output: A set of segments
1: for Each Subset S of Nodes[] do
2:
H P (S) := Average Path Entropy of all nodes in S
3:
if H P (S) ≤ MedianEntropy then
4:
Output all the leaf nodes under N as a new segment PS
5:
else
6:
PageSegmenter(Children(S))
7:
end if
8: end for

3.2 Inferring group hierarchy
Even though the page segmentation algorithm organizes the content in a Web page
in terms of segments that present the content in a uniform fashion, each segment just
contains a flat representation of the labels. In many scenarios, the content is usually
represented as a hierarchy of labels and we need to infer the hierarchy among the
labels in order to properly depict the content structure. We define a group to be a

Figure 2 a Shows an example of a faculty home page. The labels in the page are marked with
their corresponding path identifier symbols. The page segments in the web page are also marked
as segments 1 to 4. b Shows the sequence of path identifiers, the regular expression inferred from it,
and the corresponding group tree for each segment.

World Wide Web (2007) 10:157–179

163

contiguous collection of instances that are presented together in a Web page, where
an instance is a repeating element of a group. Such a group hierarchy is helpful in
organizing the content and determining the most general concept in the page and its
attributes.
One possible way to achieve such hierarchical organization of the content is to
work directly on the DOM tree in a top–down fashion [6]. But such approaches suffer
from handling the noise in the leaf nodes and in successfully detecting the boundaries
as look-ahead searching is expensive. An efficient alternative approach is to utilize
the presentation information (embedded in their root-to-leaf tag path in the DOM
tree) of the labels in order to infer a hierarchy among them. We transform each
segment into a sequence of path identifiers of root-to-leaf paths of the leaf nodes
and infer regular expressions from this sequence in order to extract patterns from
them. For example, the path sequence corresponding to the Segment 3 as marked in
Figure 2b is ghijhikhijhilhijhik.
After the segment is transformed into a sequence of path identifiers, we extract
patterns from it by inferring a path regular expression from the sequence of path
identifiers. We use the standard regular language operators:
–

–
–

Kleene star (*): The kleene star operator conveys the repeating presentation
patterns within the Web page. For example, if the Web page presents the names
of the courses that a faculty member teaches, they are usually presented with
similar presentation template and the kleene star operator would identify this
pattern.
Optional (?) and Union (|): The optional and the union operators allow the
patterns to accommodate noise.
Concatenation (.): The concatenation operator allows a contiguous sequence to
be formed from its sub-patterns.

We build the path regular expression for a given sequence by incrementally
building it using bottom-up parsing. We go through the sequence several times, each
time folding it using one of the four operators, until there are no more patterns left.
The algorithm for building the path regular expression from a sequence is described
in Algorithm 2. The InferRegEx method is initialized for each segment in the page
with its corresponding path sequence.
Once the path regular expression is inferred from the sequence of path identifiers,
every kleene star in the path regular expression is treated as a group and its members
are treated as instances of the group. For example, in Segment 3 of the Web page
in Figure 2b, the path regular expression (hijhi(k|l))∗ is identified as a group which
corresponds to the publications of the corresponding faculty member. The nested
kleene star symbols are transformed into nested group hierarchies from the path
sequence as shown in Figure 2b.
3.3 Promotion
After grouping, the content of the Web page is organized into hierarchical group
structures, but each of these group structures do not have any label. The labels for
the groups, which we call as group headers, are important as they play a role in
connecting the repeating set of instances with the corresponding concept or attribute
label. For example, in a news Web page such as CNN, a group structure containing all

164

World Wide Web (2007) 10:157–179

Algorithm 2 Inferring Path Regular Expressions
InferRegEx(S)
Input: S, a sequence of symbols
Output: S, a new regex sequence of symbols
1: patternFound = true;
2: repeat
3:
patternFound = false;
4:
for len = 1:length(S)/2 do
5:
for i = 0:length(S) do
6:
currPattern := subseq(j, j+i);
7:
if ExtendPattern(currPattern, S, j+i+1) = true then
8:
ReplacePattern(currPattern, S);
9:
patternFound = true;
10:
end if
11:
end for
12:
end for
13: until patternFound = true
14: return S;
End of InferRegEx
ExtendPattern(P, S, startIndex)
Input: P, current pattern; S, a sequence of symbols; startIndex, the start index to look
patterns for
Output: boolean, indicating whether the pattern is extended
1: for i = startIndex:length(S) do
2:
consensusString := IsMatch(currPattern, subseq(i,i+length(currPattern));
3:
if consensusString  = null then
4:
currPattern := consensusString;
5:
ExtendPattern(currPattern, S, i+length(currPattern))
6:
end if
7: end for
End of ExtendPattern
IsMatch(P1 , P2 )
Input: P1 , first pattern; P2 , second pattern
Output: consensusString obtained by alining P1 and P2 or null
MaxLength(P1 ,P2 )
3

1: if EditDistance(P1 , P2 ) ≤
2:
return Consensus(P1 , P2 )
3: else
4:
return null
5: end if

then

End of IsMatch
ReplacePattern(P, S)
Input: P, current pattern; S, a sequence of symbols
Output: none
1: replace all occurrences of the pattern P in the sequence S by a new symbol P .
End of ReplacePattern

World Wide Web (2007) 10:157–179

165

the scientific articles cannot be of much use unless it is labeled as Science or Sci/Tech
for indexing and searching purposes.
Bootstrapping the Promotion: In the grouping phase, all the leaf nodes that appear
before the group are identified as candidate group headers and the goal of the
promotion algorithm is to select the appropriate group header from these candidates
for the group. These group headers are bootstrapped by promoting the label as the
header whenever there is only one candidate for a particular group.
Frequent Label based Promotion: Whenever similar Web pages from the same
domain are available, we identify all the frequent labels in the domain from these
similar pages and promote the closest frequent label that is present in the candidate
headers of a group as the label for the group.
Naive Bayes based Promotion: When many similar Web pages obtained from
similar domains and from the same context are available, the candidate group
headers can be used as a training data when deciding on a label to promote as a group
header for a group. In such scenarios, we use the words in the instances as features to
train a Naive Bayes classifier and compute the likelihood of every candidate group
header with a set of instances. Later we promote the closest one as the header for the
group.
Path Consistent Annotation: Typically similar presentation templates are used to
represent similar labels within the same Web page. Hence if one of those similarly
presented labels is promoted with the above rules, then we promote all the other
labels within the same Web page with the same presentation template on top of the
next groups, whenever applicable.
Figure 3 shows the final hierarchical content structure after promotion for the
Web page shown in Figure 2. It can be noticed that the labels ‘Daniel Moore,’

Figure 3 The complete hierarchical structure of the web page shown in Figure 1 after promotion.
The group structures G1 through G6 with their respective promoted labels are shown in each page
segment. The parts of the Web page that do not correspond to any group structure are shown with
dotted triangles. The figure also illustrates meta semantic role annotation by annotating the labels
with their corresponding semantic roles (C – concepts, A – attributes, V – values).

166

World Wide Web (2007) 10:157–179

‘Publications,’ ‘Address,’ and ‘Phone Number’ are promoted over the adjacent
groups G2, G3, G5, and G6 as group headers. The groups G1 and G4 do not have
any group headers as they did not have any candidate labels to promote over them.
3.4 Annotation with semantic roles
After the group hierarchy has been found and the appropriate labels have been
promoted on the groups, we have a powerful content structure that organizes the
labels in the Web page in a uniform fashion. We utilize this content structure to
annotate the labels in the Web page with metadata tags.
Our annotation framework involves the following four semantic roles:
–

–

–

–

Concept (C): A concept defines an abstract or symbolic category or a class of
similar items. For example, ‘Faculty’ and ‘Computer Science Department’ are
some of the concepts in the academic domain.
Attribute (A): An attribute is a key property of an object or a concept or the
name of the relationship. For example, ‘publications,’ ‘address’ and ‘telephone’
are some of the attributes of the ‘Faculty’ concept in Figure 2a.
Value (V): A value is a string that provides the value information for an attribute
of a certain object or a concept. For example, ‘(424) 435-3897’ and ‘(434) 7874671’ are the values of the attributes ‘Office Telephone’ and ‘Fax Telephone’
attributes in Figure 2a.
Noise (N): A label that does not belong to any of the above semantic role is
assigned to be noise. For example, some of the labels in headers, footers or
navigational aids could be annotated as noise.

Figure 3 shows an example of the assignment of tags for the Web page shown in
Figure 2a. Within the context of a certain concept, we interpret all the group headers
as attribute labels and the all the instance values as value labels of the corresponding
attribute. Please note that this assignment of roles may not always be robust; for
example, the label ‘Daniel Moore’ has been tagged as an attribute, but it is actually
the name of the object that belongs the concept ‘Faculty.’ Therefore we call this data
as weakly annotated because some of the annotations may be incorrect and some
labels may not be annotated at all.
3.5 Discussion on weakly annotated data
Typically presentation regularities are sufficient to organize the content of an HTML
Web page and extract information from its data rich segments. However, when the
presentation in a Web page does not correlate with the semantic organization of its
content, then the performance of extraction systems [3, 9, 19, 22] detoriates.
Figure 4 shows a typical shopping Web page where the product categories and
featured list of products are presented. Even though the presentation of the labels is
mostly regular, there are some noisy labels, the ones that are circled, that makes it
difficult to skip and extract the repeating item structure. Some of these presentation
irregularities are categorized as follows:
–

Presentation Inconsistency: The HTML formatting does not correlate with the
logical organization of the page content. For example, in the shopping Web page

World Wide Web (2007) 10:157–179

167

Figure 4 A sample products Web page that highlights presentation irregularities. Presentation
outliers, labels that do not adhere to the adjacent formatting, and Insertions/Deletions, labels that
only present in some of instances in a similarly presented group, are circled.

–

–

–

shown in Figure 4, the labels such as ‘$199.99’ and ‘$299.99’ are presented with a
bold emphasized formatting that does not match their value semantic roles.
Metadata/Data Ambiguity: Labels with distinct semantic roles are presented with
identical formatting. For example in the left hand part of the Web page in
Figure 4, the category names such as ‘Action,’ ‘Comedy,’ ‘Drama’ under the
section ‘DVDs’ and the labels such as ‘-more’ are presented using identical
formatting and there is no way to distinguish them with presentation information
alone.
Presentation Outliers: This occurs when a group of similarly presented items
contains some irregularly presented items. For example, in Figure 4, the circled
labels under the ‘BROWSE BY CATEGORY’ taxonomy represent outlier
concept labels with irregular presentation.
Insertions/Deletions: Within a group of similar items, some label types are optional. For example in the ‘FEATURED PRODUCTS’ section of the Web page
in Figure 4, the labels ‘Last Day to Save!!’ or ‘Qualifies for FREE SHIPPING’
are optional labels.

These presentation irregularities may lead to misinterpretations of the content by
IE systems, thus producing weakly annotated data. Such weakly annotated data can
be corrected by using the domain knowledge. In the next section, we explain how to

168

World Wide Web (2007) 10:157–179

incorporate the domain knowledge into an IE system by using a statistical domain
model.

4 Extracting domain knowledge
In this section, we model the domain knowledge from the hierarchical structures
given above. The crucial idea is to identify the degree of relationship between
the labels and construct a relational graph which is easy to generate probability
distributions of the roles for each label in the domain. Such a graph would capture
the global statistics of the labels and their associations within a domain. We first
describe the statistical domain model and then discuss how to obtain the probability
distributions.
4.1 Statistical domain model
Before we proceed to the details about our algorithms, we define the notation that
we use in the rest of the paper as follows:
–

The ontological roles R is the set of Concept, Attribute, Values or Noise. Formally,

R = {C, A, V, N}.
–

–

A term is a pair l, r composed of a label l and a role r ∈ R. In other words, terms
are tagged labels in the Web pages. Each label in a Web page is assumed to be
tagged with only one of the given ontological roles above.
In this setting, we consider all the labels in each Web page are tagged with roles,
hence we define a Web page to be a vector of its terms. Formally, assuming m
labels in the Web page W ;

W = {l1 , r1 , l2 , r2 , . . . , lm , rm }.
–

–

–

The statistical domain model G is a weighted undirected graph where the nodes
are terms in the domain, and the weights on the edges represent the association
strength between terms.
The semantic role distribution Dl of a label l is a probability distribution of the
four roles {Pc , Pa , Pv , Pn }, where Pc , Pa , Pv , Pn are the probabilities of l being a
concept, attribute, value and noise, respectively within a certain context – which
represents the Web document of the label. That is, the role distribution of a label
might vary in different Web pages.
In our framework, the context of a label l ∈ L in a Web page W is the Web page
W itself.

Statistical domain model is a relational graph G generated from automatically
extracted data. The nodes in G denote the labels with their semantic roles and the
edges denote the association strengths between the annotated labels. Node weights
are initialized as the counts of the corresponding terms and the edge weights are the
counts of the corresponding edges in the hierarchies. Formally, assuming wij as the
weight between the terms i and j, and wi as the weight of the node i, wij = w ji =
|i ↔ j| and wi = |i| where |i ↔ j| and |i| denote the number of times the edge (i, j)

World Wide Web (2007) 10:157–179

169

Figure 5 A fragment of the relational graph obtained by aggregating several products Web pages,
that is relevant to the page shown in Figure 4. The thickness of the line represents the strength of the
association between two labels. Each label is also annotated with its semantic role.

appeared in the hierarchies and label i appeared in the entire domain respectively.
Note that the edges are undirected since association strength between labels is a bidirectional measure.
A fragment of the statistical domain model obtained for the shopping domain
from a collection of Web pages is presented in Figure 5. The nodes are the tagged
labels such as ‘Product:C’ which specifies the concept node of the ‘Product’ label. The
thicker the edge, the stronger the relationship is. For instance, the value ‘V7 W1PS
LCD Display’ is more related with the attribute ‘Price’ than the attribute ‘Rebates’
in the graph.
4.2 Role distribution calculation
The probability calculations are briefly introduced below.
Definition 2 For a given Web page W , the probability of a label l tagged with a role
r ∈ R is Pr (l|W ).
In order to reduce the complexity and to utilize the context, the probabilities are
calculated using a simple Bayesian model with the following assumptions:
Assumption 1 All the terms in G are independent from each other but the given
term l, r.
Assumption 2 The prior probabilities of all the roles of a label l are uniform.
Here, the first assumption is the well-known “naive” assumption of the simple
Bayesian models. Note that we only utilize the first order relationships of a term
in its context, i.e., neighbors of the term in G . One can easily extend the model

170

World Wide Web (2007) 10:157–179

for higher order relationships. Assumption 2 states that the role distribution of a
label shouldn’t depend on its frequency but only its context. Otherwise when a label
which is frequent in the domain, the role probability distribution will be strongly
biased towards its frequent role in the domain, and will dominate the contextual
information. Now, given the two assumptions above we can state the following
theorem:
Theorem 1 Let W = {t1 , t2 , . . . , tm }. Then the normalized probability of a label l
tagged with the role r is,
m
Pr (l|ti )
m
Pr (l|W ) =  i=1
.
(1)
k∈R
i=1 Pk (l|ti )
Proof By Bayes’s rule,
Pr (l|W ) = Pr (l|t1 , t2 , . . . , tm ) =

Pr (t1 , t2 , . . . , tm |l)Pr (l)
.
P(t1 , t2 , . . . , tm )

Using the independence assumption,
m
Pr (ti |l)Pr (l)
m
= i=1
i=1 P(ti )
Again using Bayes’s rule,
m
m
Pr (l|ti )
Pr (l)
i=1 Pr (l|ti )P(ti )
=
. m
= i=1 m−1
m
Pr (l)
Pr (l)
i=1 P(ti )
m−1
This is the unnormalized probability. Since Pr (l)
is 
constant by Assumption 2,
m
we can remove it and add the normalization factor k∈R i=1
Pk (l|ti ) in the denominator. That is,
m
Pr (l|ti )
m
.
Pr (l|W ) =  i=1
k∈R
i=1 Pk (l|ti )




A conditional probability such as Pr (l|ti ) depends on the association strength
r (l,ti )
=
between the terms l, r and ti in the relational graph G . That is, Pr (l|ti ) = PP(t
i)
wl,rti
by Bayes’s rule where wl,rti is the weight of the edge (l, r, ti ) and wti is
wti
the weight of the node ti . Our probability model is based on the methodology of
association rules [1]. Hence, the initialization for the above conditional probabilities
is defined analogous to Pr (l|ti ) ≡ Conf idence(ti → l, r) [2]. This formulation is
consistent with Assumption 2 since it is independent from the prior, Pr (l). For more
details, interested reader can refer to the technical report [13].
The domain knowledge for a given Web page is represented in terms of semantic
role distributions Dl for each label l in the Web page. Pr in the role distributions of
a label l in a Web page is the shorthand notation for Pr (l|W ). As the output, these
distributions are presented to the IE system.

World Wide Web (2007) 10:157–179

171

5 Integrating the domain knowledge into IE system
The key phases in the IE system presented in Section 3 are grouping and promotion.
In this section we describe how to accommodate the domain knowledge into these
phases.
5.1 Grouping
To accommodate the domain knowledge in the IE system, the grouping algorithm
described in Section 3.2 is extended to include the role distributions of the labels. The
only change in the new grouping algorithm occurs in the similarity measure between
two candidate sequences of labels. In Algorithm 2, the similarity measure between
two sequences of labels is computed using the minimum edit distance between their
path regular expressions. In the new grouping algorithm, we add a new similarity
measure that takes the similarity of the role assignments of the label sequences into
account when computing the similarity measure.
Let l1 and l2 be two label sequences. Let S p be the path similarity measure between
l1 and l2 computed using the edit distance between their path regular expressions.
The role similarity measure Sr between l1 and l2 is computed by calculating Pearson
Correlation Coefficient [21] between the role distributions Dl1 and Dl2 of the two
labels. The role similarity measure ensures agreement in the role assignment of labels
in the sequences. The total similarity measure between l1 and l2 is computed by a
weighted sum of S p and Sr . Therefore two label sequences are said to be similar if
and only if there is an agreement in both their path regular expressions and their
semantic role assignments of individual labels.
The rest of the grouping algorithm proceeds in a similar fashion as described
in Section 3.2 in that a regular expression is inferred from the sequence of labels
and each Kleene star is interpreted as a group and the members of Kleene star are
interpreted as instances.
5.2 Promotion with semantic roles
The promotion algorithm is also slightly altered from the one described in Section 3.3
to bias towards metadata labels in choosing the headers for the group. The group
headers are obtained by identifying the closest metadata label, with role assignment
of a concept or an attribute, as identified from the role distributions. The group
header is chosen by the following simple formula:
GroupHeader(G) = arg max {di ∗ (Pa + Pc )}.
li

(2)

where li is the label and di is the distance of li from the group structure.
We also promote metadata labels inside the instances of the groups over the
next sequence of values with similar path identifiers. This promotion within the
instances assists in extracting the attribute-value relationships that are present within
the instances of group structures.
5.3 Significance of domain knowledge
Figure 6a shows a fragment of a course Web page. The labels in the page are
annotated with the HTML root-to-leaf path identifiers that highlight the presentation

172

World Wide Web (2007) 10:157–179

Figure 6 a Shows a fragment of a course Web page. For each label in the fragment, its path identifier
symbol and its semantic role are marked. b Shows the group structures extracted by semantic
partitioner algorithm that relies solely on presentation regularities. c Shows the group structures
extracted by using a domain model.

formatting and their semantic roles. An automated IE system such as semantic partitioning algorithm described in Section 3 that relies solely on the presentation would
not be able to distinguish the ‘Place’ attribute of the ‘Professor’ concept and ‘Office
hours’ attribute of the ‘Teaching Fellow’ concept from their respective values ‘Room
MCS 123’ and ‘Tuesday, 4–6 ...’ since their presentation formatting is identical. Also
such a system cannot associate the attributes ‘E-mail’ and ‘Office hours’ with their
corresponding concepts ‘Professor’ and ‘Teaching Fellow’. Similarly, it would fail to
associate these attributes with their respective values, due to various presentation
irregularities that would hide the repetitive sub-structures and their boundaries.
Figure 6b shows the output of the semantic partitioning algorithm that relies solely
on the presentation.
If the system is aware of the domain knowledge, it can more accurately separate metadata from data, and discover the relationships between them. For example, in Figure 6, if the system is aware of the fact that the labels ‘E-mail,’
‘Office hours,’ ‘Place’ and ‘Phone’ are attributes, and the labels ‘jones@cs.univ.edu,’
‘Monday 11–12 ...,’ ‘Room MCS 123,’ ‘adam@cs.univ.edu,’ ‘Tuesday, 4–6 ...,’ and
‘123-456-7891’ are values, it can discover correct groupings for G2 and G3 for the
concepts ‘Professor’ and ‘Teaching Fellow.’ The accurate group structures obtained
by using such a domain knowledge as well as presentation regularities is shown in
Figure 6c.

6 Complexity analysis
The page segmentation algorithm works directly on the DOM tree of the Web page
in a top-down fashion and its complexity is O(n lg n), where n is the total number
of nodes in the DOM tree. The group hierarchy inference phase iteratively goes
through the path sequence in a bottom-up fashion until no more regular expressions
are found. Assuming there are k nodes in the segment, the worst complexity of this
phase is O(k3 ). Since k is considerably smaller than the number of leaf-nodes in

World Wide Web (2007) 10:157–179

173

the Web page, this is reasonable for the Web. The promotion and labeling phases
are linear in the number of nodes in the corresponding group. The complexity of
generating the role distributions from the statistical model is O(m + p) where m and
p are the total number of labels and Web pages in the collection respectively [13].
7 Related work
In this section, we discuss the related work from several areas and show how our
system is different from them.
Template based algorithms: RoadRunner [9] works with a pair of documents
from a collection of template generated Web pages to infer a grammar for the
collection using union-free regular expressions. ExAlg [3] is another system that
can extract data from template generated Web pages. ExAlg uses equivalence
classes (sets of items that occur with the same frequency in every page) to build the
template for the pages by recursively constructing the page template starting from
the root equivalence class. TAP [14] is a system that extracts RDF triplets from
template driven Web sites in order to generate a huge knowledge base that has a
Web searchable interface. These algorithms are based on the assumption that the
input Web pages are template driven in their presentation and are typically driven
by standard metadata. Our approach differs from all these approaches in that it
does not require that the input Web pages are template driven and it can effectively
handle noise.
Grammar induction based algorithms: Grammar induction based systems employ
a strong bias on the type and expected presentation of items within Web pages to
extract instances. XTRACT [12] is such a system that can automatically extract
Document Type Descriptors (DTDs) from a set of XML documents. It transforms
each XML document to a sequence of identifiers and infers a common regular
expression that serves as a DTD, using the Minimum Description Length (MDL)
principle. Our pattern mining algorithm is different from these approaches and
parses the given sequence in a bottom-up fashion and infers the grammar on-the-fly
as it goes through the sequence multiple number of times.
Page Segmentation algorithms: VIPS algorithm [5] is a vision-based page segmentation algorithm that is based on HTML heuristics relying on specific tags such as font
size and <HR> to partition the page into information blocks. Our page segmentation
algorithm is similar to the VIPS algorithm in traversing the DOM tree in top-down
fashion, but our algorithm uses well-defined information theoretic methods in order
to measure the homogeneity of the segment, whereas the VIPS algorithm is based on
HTML heuristics.
KnowItAll [11] and C-PANKOW [7] systems extract facts from a collection of
Web pages starting with a seed set of factual patterns that are either manually
specified or (semi)automatically engineered. The Semtag and Seeker system [10]
uses the domain ontologies extracted from automatic wrappers [16] to annotate Web
pages. Such systems extract relationships and facts that match these patterns from the
natural language text segments of the Web pages. Hence, they are complementary to
our systems capabilities that processes the data rich segments of Web pages.

174

World Wide Web (2007) 10:157–179

8 Experimental results
In this section, we describe the data we used in our experiments and provide the
results and discussions for our experiments.
8.1 Experimental setup
We used three different data sets in order to evaluate the efficacy of our algorithms.
In the first two data sets, we show how our algorithms work with template-driven
and non-template-driven Web pages. In the third data set, we compare our approach
with another IE system, RoadRunner [9].
The first data set consists of TAP KB,1 containing the categories AirportCodes,
CIA, FasMilitary, GreatBuildings, IMDB, MissileThreat, RCDB, TowerRecords
and WHO. These categories alone comprise 9,068 individual attribute-rich Web
pages. We provide experimental results for this data set with our algorithms and
compare them against the relations obtained by TAP.
As our second data set, we prepared CSEDepts data set which is composed of
individual Web pages from Faculty and Courses domains, consisting of 125 Web sites
and more than 15,000 individual Web pages. To demonstrate the performance of
our semantic role annotation algorithms, we created a smaller data set containing
randomly chosen 120 Web pages from each of the faculty and course categories. We
provide experimental results for this data set with our algorithms.
As the third data set, we selected the RoadRunner2 [9] data. We compare our
extraction of data values with the RoadRunner system in this experiment.
The experimental results are obtained by comparing the data annotations of the
algorithms to manually annotated data by eight human volunteers who are nonproject member computer science graduate students. The inter-human agreement
on manual annotation was 87%, which indicates that the data annotations can be
ambiguous and can be interpreted differently in various contexts.
8.2 Experiments with the TAP data set
The Table 1 shows the experimental results for the TAP data set using semantic
partitioning algorithm that relies solely on presentation regulariteis. The algorithm
achieves 100% F-Measure with annotating the labels with the concept label. Since
the TAP data set contains only one concept per page, the algorithm is able to easily
identify the label. However, the algorithm suffers from low recall with annotating
as attribute labels because some of the attribute labels are single valued and there
is no group associated with them, and they are labeled as values. Nevertheless, the
algorithm is able to tag with the attribute labels correctly whenever it does, as the
precision is above 91%. As expected, the recall and precision numbers for the value
label annotation are exactly opposite for the same reasons that many attribute labels
are labeled as values.

1 TAP

Home page is located at http://tap.stanford.edu.

2 RoadRunner

experimental results can be found at http://www.dia.uniroma3.it/db/roadRunner/
experiments.html.

World Wide Web (2007) 10:157–179

175

Table 1 Experimental results with TAP data set using semantic partitioner algorithm.
DomainName

P(C) R(C) F(C) P(A) R(A) F(A) P(V) R(V) F(V) Avg F-Measure

AirportCodes
CIA
FAS Military
Great buildings
Missile threat
IMDB
Roller coster
Database (RCDB)
Tower records
WHO
Overall

100
100
100
100
100
100
100

100
100
100
100
100
100
100

100
100
100
100
100
100
100

94
96
96
95
96
72
78

83
54
76
61
63
56
52

88
69
85
74
76
63
46

86
88
84
88
66
63
84

98
99
99
99
99
51
91

92
92
91
93
79
56
88

93
87
92
89
85
73
78

100
100
100

100
100
100

100
100
100

75
100
91

69
94
67

72
97
74

62
85
79

55
100
92

58
92
85

77
96
86

P(C), R(C), F(C) denote the precision, recall and F-measure of the concept annotation. Similar
notation is used for attributes (A), and values (V).

For the TAP data set, the statistical domain model and corresponding semantic
role distributions for each Web page are generated from automatic wrapper based
IE systems. The extracted domain knowledge is fed to our algorithm that extracts
the relational facts and also improves the semantic role annotations of labels. We
compare the performances of the semantic partitioning algorithm with and without
utilizing the statistical domain model.
The experimental results for semantic role annotations are shown in Figure 7.
The graphs show the F-Measure values of concept labeling, attribute labeling, value
labeling and overall labeling. The semantic partitioning algorithm that relies solely
on presentation regularities and does not utilize the statistical domain model was not
able to identify the attribute names correctly in some cases because the presentation
does not distinguish the attribute name and its values. But the algorithm that makes
use of the statistical domain model in terms of semantic role distributions overcomes
such irregularities in presentation. As it can be seen from the results, the accuracies
for some categories in the TAP data set are lower than others. The low F-Measure
value occurs when the statistical domain model cannot recover from unreliable
information extracted from automated systems.

Figure 7 The role annotation F-measure values from the TAP data set for semantic partitioning
algorithm, with and without using the domain knowledge.

176

World Wide Web (2007) 10:157–179

Figure 8 The role annotation F-measure values from CSEDepts data set for semantic partitioning
algorithm, with and without using the domain knowledge.

8.3 Experiments with the CSEDepts data set
Figure 8 shows and compares the F-measure values for semantic partitioning algorithm with and without utilizing the domain knowledge. It shows that the semantic
partitioning algorithm based on presentation regularities alone achieves about 90%
F-measure, and the semantic partitioning algorithm that uses the statistical domain
model achieves more than 95% F-measure. The F-measure for the concepts in the
initial semantic partitioning algorithm is low because the number of concept labels
in the domain are very few and they are ambiguously located along with the other
attribute labels. However, the statistical domain model is helpful in disambiguating
these concept values and boost the F-measure. The algorithm is able to perform fairly
accurately in annotating the attribute and value labels in both cases, which is very
vital because, these labels are the most frequent on the Web. The F-measure for
value annotation is the highest among all role annotations, as the semantic partitioner
is able to correctly identify the groups and thus identify the instances of the concept
and attribute labels accurately.
8.4 Experiments with RoadRunner data set
Table 2 shows the number of Web pages in each of the ten categories from which
the IE systems, RoadRunner and semantic partitioner systems, with and without
utilizing the statistical domain model were able to extract the relevant information.
By exposing the regularities in the extraction patterns, the semantic partitioner
system with the statistical domain model was able to extract relevant information
from 8 out of 10 categories, whereas the original system was only able to extract
data from 6 categories. The resulting semantic partitioner system with the statistical
domain model was also able to perform better than the RoadRunner system in
one category (package directory) as the RoadRunner system requires two Web
pages to infer the presentation template. The ‘uefa’ data is organized in terms of
complex tables, RoadRunner was able to infer the template by using two sample
pages whereas the semantic partitioner (both initial and modified) was unable to
extract from such tables using a single page. The results show that the performance
of the semantic partitioner with the statistical domain model is comparable to that of
RoadRunner system. The statistical domain model for this data set is learnt from over

World Wide Web (2007) 10:157–179

177

Table 2 Comparison of the performance the RoadRunner algorithm with semantic partitioner
system, before and after utilizing the statistical domain model.
Classes
Site

amazon.com
amazon.com
buy.com
buy.com
rpmfind.net
rpmfind.net
rpmfind.net
rpmfind.net
uefa.com
uefa.com

Description

Cars by brand
Music bestsellers
by style
Product information
Product subcategories
Packages by name
Packages by
distribution
Single package
Package directory
Clubs by country
Players in the national
team

Comparative results
No. of
pages

Metadata

RoadRunner Sempart Sempart
(before) (after)

21

Yes

21

–

21

20
10
20
30

No
Yes
Yes
No

–
10
20
10

–
10
20
10

–
10
20
10

20
18
20
20
20

No
No
No
No
No

20
18
–
20
20

20
18
20
–
–

20
18
20
20
–

100 shopping and sports Web sites. The precision and recall of metadata extraction
from the RoadRunner data set are 89 and 94%. Please note that our system is able
to achieve this performance without the requirement of template-drivenness unlike
RoadRunner system.

9 Conclusions and future work
In this paper, we have presented an IE system, Semantic Partitioner, that can
automatically structure the content of a Web page into a semi-structured hierarchical
document, with the labels in the Web page weakly annotated with their semantic
roles. We also presented details about how to build a statistical domain model that
can be utilized as domain knowledge to enhance the performance of an automated
IE system. We demonstrate that automated IE coupled with an automatically
constructed domain model can recover from ambiguities in the presentation and
improve the accuracy of the base information extractor. Our experimental evaluations with TAP, computer science department Web sites, and RoadRunner data
sets indicate that our algorithms can scale up to large data sets in terms of running
time complexity. Hence, we conclude that current automated IE systems can benefit
from using the domain regularities in order to extract information from all kinds
of data rich Web documents. Such structured information also enables browsing of
information in order to see interesting connections among a set of objects.
In our future work, we intend to identify the missing attribute labels of values in a
given Web page by using aggregated statistical domain model which may obtain the
labels from different Web pages. This research also involves merging of labels that
refer to the same entities which is closely related with the area of schema matching
and merging. Another dimension is to extend our work to perform information

178

World Wide Web (2007) 10:157–179

extraction from text blobs of the Web pages with the help of natural language
processing techniques.
Acknowledgements This work was partially supported by the Office of Naval Research (ONR)
under its Multidisciplinary Research Program of the University Research Initiative (MURI) under
Grant No. N00014-04-1-0723.

References
1. Agrawal, R., Imielinski, T., Swami, A.N.: Mining association rules between sets of items in large
databases. In: ACM SIGMOD Conference on Management of Data, pp. 207–216. Washington,
D.C. (1993)
2. Alpaydin, E.: Introduction to Machine Learning, chapter 3, pp. 39–59. MIT Press, Cambridge,
MA (2004)
3. Arasu, A., Garcia-Molina, H.: Extracting structured data from web pages. In: ACM SIGMOD
Conference on Management of Data, San Diego, USA (2003)
4. Ashish, N., Knoblock, C.A.: Semi-automatic wrapper generation for internet information
sources. In: Conference on Cooperative Information Systems, pp. 160–169 (1997)
5. Cai, D., Yu, S., Wen, J.-R., Ma, W.-Y.: Vips: a vision-based page segmentation algorithm.
Technical Report MSR-TR-2003-79, Microsoft Technical Report (2003)
6. Chkrabarti, S.: Integrating the document object model with hyperlinks for enhanced topic distillation and information extraction. In: International World Wide Web (WWW) Conference
(2001)
7. Cimiano, P., Ladwig, G., Staab, S.: Gimme’ the context: context-driven automatic semantic
annotation with c-pankow. In: The 14th International World Wide Web (WWW) Conference
(2005)
8. Ciravegna, F., Chapman, S., Dingli, A., Wilks, Y.: Learning to harvest information for the semantic web. In: Proceedings of the 1st European Semantic Web Symposium, Heraklion, Greece
(2004)
9. Crescenzi, V., Mecca, G.: Automatic information extraction from large web sites. J. Artists’
Choice Mus. 51(5), 731–779 (2004)
10. Dill, S., Eiron, N., Gibson, D., Gruhl, D., Guha, R., Jhingran, A., Kanungo, T., McCurley, K.S.,
Rajagopalan, S., Tomkins, A., Tomlin, J.A., Zien, J.Y.: A case for automated large-scale semantic
annotation. Journal of Web Semantics 1(1), 115–132 (2003)
11. Etzioni, O., Cafarella, M., Downey, D., Kok, S., Popescu, A.-M., Shaked, T., Soderland, S., Weld,
D.S., Yates, A.: Web-scale information extraction in knowitall. In: International World Wide
Web (WWW) Conference (2004)
12. Garofalakis, M., Gionis, A., Rastogi, R., Seshadri, S., Shim, K.: XTRACT: a system for extracting
document type descriptors from xml documents. In: ACM SIGMOD Conference on Management of Data (2000)
13. Gelgi, F., Vadrevu, S., Davulcu, H.: Automatic extraction of relational models from the web data.
Technical Report ASU-CSE-TR-06-009, Arizona State University, April (2006)
14. Guha, R., McCool, R.: TAP: a semantic web toolkit. Semantic Web Journal (2003)
15. Hearst, M.A.: Untangling text data mining. In: Association for Computational Linguistics (1999)
16. Kushmerick, N.: Wrapper induction: efficiency and expressiveness. Artif. Intell. 118(1–2), 15–68
(2000)
17. Kushmerick, N., Weld, D.S., Doorenbos, R.B.: Wrapper induction for information extraction. In:
Intl. Joint Conference on Artificial Intelligence (IJCAI), pp. 729–737 (1997)
18. Liu, L., Pu, C., Han, W.: Xwrap: an xml-enabled wrapper construction system for web information sources. In: International Conference on Data Engineering (2000)
19. Muslea, I., Minton, S., Knoblock, C.: Stalker: learning extraction rules for semistructured. In:
Workshop on AI and Information Integration (1998)
20. Noy, N., Musen, M.: Prompt: algorithm and tool for automated ontology merging and alignment.
In: Proceedings of the 17th Conference of the American Association for Artificial Intelligence
(AAAI). AAAI Press, Menlo Park, CA (2000)
21. Pearson, K.: On the coefficient of racial likeliness. Biometrica 18, 105–117 (1926)

World Wide Web (2007) 10:157–179

179

22. Vadrevu, S., Gelgi, F., Davulcu, H.: Semantic partitioning web pages. In: The 6th International
Conference on Web Information Systems Engineering (WISE) (2005)
23. Yang, G., Tan, W., Mukherjee, S., Ramakrishnan, I.V., Davulcu, H.: On the power of semantic
partitioning of web documents. In: Workshop on Information Integration on the Web, Acapulco,
Mexico (2003)

2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

A Semantic Triplet Based Story Classiﬁer
Betul Ceran∗ , Ravi Karad∗ , Ajay Mandvekar∗ , Steven R. Corman† and Hasan Davulcu∗
∗
School of Computing, Informatics and Decision Systems Engineering
Arizona State University, Tempe, AZ 85287-8809.
Email:{betul, rkarad, amandvek, hdavulcu}@asu.edu
†
Hugh Downs School of Human Communication,
Arizona State University, Tempe, AZ 85287-1205.
Email:steve.corman@asu.edu
Abstract

to illustrate cultural differences, as well as to illustrate
how telling their own stories serves to recruit and
assimilate outsiders into local political groups and
extremist organizations. But the problem with analysis
of extremist text is that it needs many human annotators to extract stories and non stories from different
sources. The main purpose of developing an automated
story classiﬁer is to reduce the human dependency to
annotate story and non-stories.
A story is comprised of three components. First,
there must be an actor or actors telling the story
implicitly or explicitly. This can include politicians,
mujahedeen, everyday people and so on. Second, the
actors must be performing actions. This can include
ﬁghting, preparing for a battle, talking to others and so
on. Third, the actors actions must result in a resolution.
Resolutions can include a new state of affairs, a new
equilibrium created, a previous equilibrium restored,
victory and so on. Besides, stories usually have story
worlds, or worlds where the stories are taking place.
Story worlds are not ﬁctional universes, but rather
environments in which the story takes place.
Story example: “They have planted your remains in
the sands like a ﬂag. To motivate the people morning
and night, woe unto them, they have raised a beacon of
blood To inspire tomorrow’s generation with hate and
dislike”. A non-story paragraph is one, among the categories Exposition, Supplication, Question, Annotation,
Imperative, Verse or Other. Non-Story example: “Let
the soldiers of this Administration go to hell. Petraeus
and Bush are trying to convince the Americans that
their salvation will begin six weeks from next July.
In fact even if Bush keeps all his forces in Iraq until
doomsday and until they go to hell, they will face only
defeat and incur loss, God willing.” This paragraph
is coded as “Non-Story” because there is no explicit
resolution. There are only hypothetical resolutions.

A story is deﬁned as “an actor(s) taking action(s)
that culminates in a resolution(s).” In this paper, we investigate the utility of standard keyword based features,
statistical features based on shallow-parsing (such as
density of POS tags and named entities), and a new
set of semantic features to develop a story classiﬁer.
This classiﬁer is trained to identify a paragraph as
a “story,” if the paragraph contains mostly story(ies).
Training data is a collection of expert-coded story and
non-story paragraphs from RSS feeds from a list of
extremist web sites. Our proposed semantic features
are based on suitable aggregation and generalization
of <Subject, Verb, Object> triplets that can be extracted using a parser. Experimental results show that
a model of statistical features alongside memory-based
semantic linguistic features achieves the best accuracy
with a Support Vector Machine (SVM) classiﬁer.

1. Introduction
Personal narratives are powerful sources of persuasion, none more so than stories that cultural heroes
tell about their own lives [1]. Whether their account
retells the story of a great athlete or actor or celebrity
or terrorist, fans are drawn to these accounts as moths
to bright lights. In part this is because the stories
themselves can be quite interesting, and in part because
readers often closely want to in some way identify
their own lives with the life stories of their heroes [2].
An investigation of terrorist narrative communication
through an in-depth examination of extremists published autobiographies and interviews can be helpful in
understanding mindsets and motivation behind terrorist
activities. In addition, the analysis of terrorist narratives across geographical regions holds the potential
978-0-7695-4799-2 2012
U.S. Government Work Not Protected by U.S. Copyright
DOI 10.1109/ASONAM.2012.97

572
573

In this paper, we utilize a corpus of 16, 930 paragraphs where 3, 301 paragraphs coded as stories, and
13, 629 paragraphs coded as non-stories by domain
experts to develop a story classiﬁer. Training data
is a collection of Islamist extremist texts, speeches,
video transcripts, forum posts, etc., collected in open
source. We investigate the utility of standard keyword
based features, statistical features that can be extracted
using shallow-parsing (such as density of POS tags and
density of named entities), and a new set of semantic
features in development of a story classiﬁer. Our study
is motivated by the observation [3] that interrelated
stories that work together as a system are fundamental
building blocks of (meta-) narrative analysis.
We focus on discriminating between stories, and
non-stories. The main contribution of this paper is the
introduction of a new set of semantic features based on
related linguistic subject, verb, object categories that
we named as triplet based verb features which are
motivated by the deﬁnition of “story” as “actors taking
actions that culminate in resolutions.”. Our proposed
semantic features are based on suitable aggregation
and generalization of <Subject, Verb, Object> triplets
that can be extracted using a shallow-parser. Experimental results (see Table 4) show that a combination
of statistical part-of-speech (POS) and named-entity
(NE) features, with semantic triplet-based features
achieves highest accuracy with a Support Vector Machine (SVM) based classiﬁer. We obtained precision of
73%, recall of 56% and F-measure of 0.63 for minority
class (i.e. stories) which indicates a 161% boost in
recall, and an overall 90% boost in F-measure with
negligible reduction in precision through the utility
of triplet based features over standard keyword based
features.

categorize the transcribed text of a speech into story
and non-story categories. Using word-level unigram
and bigram frequency counts as feature vectors, they
reported results for the classiﬁcation of a speech as
a story with 53.0% precision, 62.9% recall and 0.575
F-measure. For weblogs, in [8], they incorporated techniques for automatically detecting sentence boundaries
to their previously used text features to train a Support
Vector Machine classiﬁer. After smoothing the conﬁdence values with a Gaussian function, they achieved
46.4% precision, 60.6% recall and 0.509 F-measure.
In Gordon and Swanson’s most recent work on story
classiﬁcation [9], they used a conﬁdence-weighted
linear classiﬁer with a variety of lexical features, and
obtained the best performance with unigrams. They
applied this classiﬁer to classify weblog posts in the
ICWSM 2009 Spinn3r Dataset, and they obtained 66%
precision, 48% recall, and F-measure of 0.55.

3. System Architecture

Figure 1. System Architecture

3.1. Data Collection
Our corpus is comprised of 16, 930 paragraphs from
extremist texts collected in open source. Stories were
drawn from a database of Islamist extremist texts. Texts
were selected by subject matter experts who consulted
open source materials, including opensource.gov, private collection/dissemination groups, and known Islamist extremist web sites and forums. Texts come
from groups including al-Qaeda, its afﬁliates, and
groups known to sympathize with its cause and methods. The subject matter experts selected texts which
they believe contained or were likely to contain stories,
deﬁned as a sequence of related events, leading to a
resolution or projected resolution.
Extremists’ texts are rarely, if ever, composed of
100% stories, and indeed the purpose of this project
is to enable the detection of portions of texts that are
stories. Accordingly, we developed a coding system
consisting of eight mutually-exclusive and exhaustive
categories: story, exposition, imperative, question, supplication, verse, annotation, and other along with

2. Related Work
Computational models of stories have been studied
for many different purposes. R.E. Hoffman et al.
(2011)[4] modeled stories using an artiﬁcial neural network. After the learning stage, they compare the storyrecall performance of the neural network with that of
schizophrenic patients as well as normal controls in
order to derive a computational model which matches
the illness mechanism. The most common form of
classiﬁcation applied for stories tackles the problem
of mapping a set of stories to predeﬁned categories.
One of the popular applications is the classiﬁcation of
news stories to their topics [5], [6].
Gordon investigated the problem of detecting stories
in conversational speech [7] and weblogs [8] and [9].
In [7], the authors train a Naive Bayes classiﬁer to

574
573

Table 1. NER Tagger F-measures

deﬁnitions and examples on which coders could be
trained. After training, coders achieved reliability of
Cohen’s Kappa = 0.824 (average across eleven randomly sampled texts). Once reliability of the coders
and process was established, single coders coded the
remainder of the texts, with spot-check double coding
to ensure reliability was maintained.
The Cohen’s Kappa measure represents how two
observers agrees on sorting items into different categories. The observers can be human or machine. The
range of Cohen’s Kappa varies between 0 and 1. Fleiss
[10] characterizes Kappa range over 0.75 as excellent,
range between 0.40 to 0.75 as fair to good, and less
than 0.40 as poor. Hence coders’ reliability of 0.824
falls into the range of excellent. After training and
testing with ten-fold cross-validation, we calculated the
agreement between classiﬁer algorithm and the human
coders as Cohen’s Kappa = 0.48, which falls into the
range of fair to good.

Text-ID
1
2
3
4
5
6
Average

DNERT
0.592
0.567
0.652
0.837
0.720
0.505
0.644

Stanford
0.355
0.587
0.627
0.867
0.686
0.446
0.594

Illinois
0.463
0.549
0.574
0.867
0.483
0.651
0.597

Open Calasis
0.312
0.164
0.247
0.591
0.459
0.416
0.364

3) If the phrase is classiﬁed by only one tagger,
then assign it as the ﬁnal tag.
4) If all taggers disagree on the category of a
phrase, then we pick the ﬁnal category according
to the accuracy of the taggers as follows:
• Illinois NER has the highest accuracy for
Locations and Organizations;
• Stanford NER has the highest accuracy for
Persons.
5) If two out of three NER taggers agree on the
predicted category of a phrase, then the ﬁnal
category is determined by majority agreement.
Within the six documents, specialist annotated 308
organization names, 259 location names, and 127
person names. Table 1 summarizes the accuracies of
the software libraries, as well as the accuracy of our
simple democratic NER tagger (DNERT) which relias
on all of them. Overall, our democratic NERT achieves
the highest performance compared to individual NER
taggers.

3.2. Data Preprocessing
3.2.1. Named Entity Recognition Tagger. Named
entity recognition (NER) [11] (also known as entity
identiﬁcation or entity extraction) is a subtask of
information extraction that seeks to locate and classify
atomic elements in text into predeﬁned categories such
as persons, organizations, locations. Research indicates
that even state-of-the-art NER systems are brittle,
meaning that NER systems developed for one domain
do not typically perform well on other domains [12].
For the purpose of annotating the entities found within
the texts belonging to extremist narratives, we used
the most popular publicly available NER libraries.
We evaluated three libraries: Stanford Named Entity
Recognizer [13], Illinois Named Entity Tagger [14] and
an online web service provided by OpenCalais [15].
A set of six documents belonging to extremist
narratives were manually annotated by a specialist as
a person, a location or an organization. Next, the same
set of documents was annotated using NER libraries in
order to determine their accuracy. F-measure was used
to measure their accuracy as shown in Table 1.
We also developed a consensus analysis based algorithm, which we named as ‘democratic NER tagger’,
using the output of all three taggers as follows:

3.2.2. Named Entity Standardization. The extremist texts under consideration are collected from various social media sources (i.e. blogs, organizations’
websites and RSS feeds). Since these are all human
generated documents, they are rife with misspellings
and aliases of named entities. For example, the person
entity ‘Osama Bin Laden’ is sometimes referred to
as ‘Bin Laden’, ‘Sheikh Osama’, or it is sometimes
misspelled as ‘Osamma Bin Laden’ or ‘Usama’. In
order to standardize the usage of the entities and
improve the accuracy of classiﬁer dependent on the
entity features we have came up with a two step named
entity standardization process:
• Misspelling Correction Step
The named entity spelling is corrected using a
lookup with the Google’s ‘Did u mean?’ feature.
This process enables us to identify the most
correct spelling of a named entity. E.g. An entity named ‘Osamma bin laden’ is corrected as
‘Osama bin laden’.
• Standardization for Aliases
We query the RDF data stores of DBpedia[16]

1) For a particular text document to be annotated
for named-entity tags, invoke each NER tagger
(Stanford, Illinois and Open Calais).
2) For an annotated entity/word determine the category (Person, Organization, Location) assigned
by each NER tagger.

575
574

Table 2. Named-Entity Correction and
Standardization Results
Occurrences
Total
Distinct
Standardized
Accuracy

Person
5015
332
72
65 (90.3%)

Organization
2456
200
26
24 (92.3%)

3.4. Feature Extraction
Above observations made in Section 3.3 motivates
following standard keyword based features, statistical
features based on shallow-parsing, and a new set
of semantic features for the development of a story
classiﬁer:
• Keywords: TF/IDF measure [17] is calculated
for each word contained in the whole paragraph
set. Then a certain number of terms, in our case
20, 000, with the top TF/IDF values are selected
as features. Then term-document frequency matrix is created out these keyword features.
• Density of POS Tags: Part of Speech (POS) Tag
Ratios [18] for each document is calculated with
respect to numbers of tokens.
• Density of Named Entities: Named Entity (NE)
Tag frequency [13] per document is calculated.
The tags are Person, Location and Organization.
• Density of Stative Verbs: Some other statistical
features are also included in all experiments,
such as the number of valid tokens and the ratio
between observed stative verbs and total number
of verbs in a paragraph.
• Semantic Triplets Extraction: We present our
semantic triplet extraction methods in Section 3.
We also discuss how triplets from stories and nonstories are aggregated and generalized to form
memory-based features for verbs.

Location
4279
290
30
28 (93.3%)

in order to ﬁnd a standardized name for all
location, organization and person entities. DBpedia data set has a public SPARQL endpoint available at: http://DBpedia.org/sparql. The
DBpedia OWL ontology for named entity
types have following properties where known
aliases are stored: dbpprop:alternativeNames,
dbpprop:name, foaf:name. By querying the alternative names we are able to obtain the standard
name for each entity.
Table 2 summarizes the results for the named entity
correction and standardization algorithm. First row
shows the total number of occurrences of each type
of named entity in our document corpus. Second row
shows the counts of distinct entities by their type.
Third row shows the number of distinct named entity
corrections made through above spelling correction
and alias standardization procedure. The changes were
manually evaluated by a human annotator to verify
their accuracy. As seen in the ﬁnal row of the table,
out of 72 person name changes, 65 were accurately
standardized by the algorithm, providing 90% accuracy
for person entities. Similarly, for organization entities
the correction and standardization accuracy is around
92%, and for location entities it is around 93%.

3.5. Support Vector Machine (SVM) Classiﬁer
SVM [19] is a supervised learning technique which
makes use of a hyperplane to separate the data into
two categories. SVM is originally proposed as a linear
classiﬁer [20] but later improved by the use of kernel
functions to detect nonlinear patterns underlying the
data [21].There are various types of kernel functions
available [22]. In this study, we use RBF kernel deﬁned
as K(xi , xj ) = exi −xj  , where xi,j are data points
[23].

3.3. Human Annotation: Story vs. Non-Story
Coding
Stories are differentiated from non-stories as follows: Because they describe actions, stories will have
a lower proportion of stative verbs than non-stories.
Stories will include more named entities, especially
person names, than non-stories. Stories will use more
personal pronouns than non-stories. Stories may include more past tense verbs (i.e., X resulted in Y, X
succeeded in doing Y, etc.) than non-stories. Stories
may repeat similar nouns. For example, “mujahedeen”
may be mentioned in the beginning of the story and
then again at the end of the story. Paragraphs with
stories in them have different sentence lengths than
paragraphs without stories in them.

3.6. Training and Testing
The corpus contains 1,256 documents containing
both story and non-story paragraphs. There are a
total of 16,930 paragraphs, where 13,629 paragraphs
classiﬁed reliably as non-stories, and 3,301 paragraphs
classiﬁed as stories by domain experts. In our evaluations, we performed 10 fold cross validation with the
document ﬁles as follows: we break documents into 10
sets of size n/10, where n is total number of documents

576
575

resolution module [26], [27] uses a heuristic approach
to identify the noun phrases referred by the pronouns
in a sentence. The heuristic is based on the number of
the pronoun (singular or plural) and the proximity of
the noun phrase. The closest earlier mentioned noun
phrase that matches the number of the pronoun is
considered as the referred phrase.

4.2. Semantic Role Labeler (SRL) Parser
SRL parser [28] is the key component of our triplet
extractor. To extract the subject-predicate-object from
an input sentence, important step is identifying these
elements in a sentence and parse it. SRL parser does
exactly the same. SRL is propriety software developed
by Illinois research group and its shallow semantic
parser. The goal of the semantic role labeling task is
to discover the predicate-argument structure of each
predicate that ﬁll a semantic role and to determine their
role (Agent, Patient, Instrument etc). As shown in the
following example, SRL is robust in identifying verbs,
and their arguments and argument types accurately in
the presence of syntactic variations.
Numbered arguments (A0-A5, AA): Arguments deﬁne verb-speciﬁc roles. They depend on the verb in a
sentence. The most frequent roles are A0 and A1 and,
commonly, A0 stands for the agent and A1 corresponds
to the patient or theme of the proposition.
Adjuncts (AM-): General arguments that any verb
may take optionally. There are 13 types of adjuncts:
AM-ADV - general-purpose, AM-MOD - modal verb,
AM-CAU - cause, AM-NEG - negation marker, AMDIR - direction, AM-PNC - purpose, AM-DIS - discourse marker, AM-PRD - predication, AM-EXT extent, AM-REC - reciprocal, AM-LOC - location,
AM-TMP - temporal, AM-MNR - manner.
References (R-): Arguments representing arguments
realized in other parts of the sentence. The label is an
R- tag preﬁxed to the label of the referent, e.g. [A1
The pearls] [R-A1 which] [A0 I] [V left] [A2 to my
daughter-in-law] are fake.
SRL System Architecture: SRL works in fourstages, starting with pruning of irrelevant arguments,
identifying relevant arguments, classifying arguments
and inference of global meaning.
Pruning - Used to ﬁlter out simple constituents that
are very unlikely to be arguments.
Argument Identiﬁcation - Utilizes binary classiﬁcation to identify whether a candidate is an argument or
not. The classiﬁers are applied on the output from the
pruning stage. A simple heuristic is employed to ﬁlter
out some candidates that are obviously not arguments.
Argument Classiﬁcation - This stage assigns labels

Figure 2. Triplet Extraction Pipeline

(1,256). During the training phase, both story and nonstory paragraphs from 9/10 documents are used as the
training set, their features are extracted, and a classiﬁer
is trained. During the testing phase, the remaining
1/10th of the documents are used; the features for
both stories and non-stories are extracted, and matched
to the features extracted during the training phase.
Doing this evaluation, we are ensuring that training
and test data features are in fact coming from different
documents. We calculate precision, recall for each
iteration of the 10 fold cross validation and we report
mean precision, recall for both both stories and nonstories.

4. Semantic Triplet Extraction
We follow a standard verb-based approach to extract
the simple clauses within a sentence. A sentence is
identiﬁed to be complex if it contains more than one
verb. A simple sentence is identiﬁed to be one with
a subject, a verb, with objects and their modifying
phrases. A complex sentence involves many verbs. We
deﬁne a triplet in a sentence as a relationship between
a verb, its subject and object(s). Extraction of triplets
[24], [25] is the process of ﬁnding who (subject),
is doing what (verb) with/to whom (direct objects),
when and where (indirect objects/and prepositions).
Our triplet extraction utilizes the information extraction
pipeline shown in Figure (2).

4.1. Pronoun Resolution
Interactions are often speciﬁed through pronominal
references to entities in the discourse, or through co
references where, a number of phrases are used to
refer to the same entity. Hence, a complete approach to
extracting information from text should also take into
account the resolution of these references. Our pronoun

577
576

argument A1 with a nested event (E1), and recursively
process A1 with our triplet extraction rules. We achieve
this nested processing through a bottom-up algorithm
that (i) detects simple verb occurrences (i.e. verbs
with non-verb arguments) in the SRL parse tree, (ii)
extracts triplets for those simple verb occurrences using
the following Triplet Matching Rules, (iii) replaces
simple verb clauses with an event identiﬁer, thus
turning all complex verb occurrences into simple verb
occurrences with either non-verb or event arguments,
and applies the following Triplet Matching Rules.

to the argument candidates identiﬁed in the previous
stage.
Inference - In the previous stages, decisions were
always made for each argument independently, ignoring the global information across arguments in the
ﬁnal output. The purpose of the inference stage is to
incorporate such information, including both linguistic
and structural knowledge. This knowledge is useful to
resolve any inconsistencies of argument classiﬁcation
in order to generate ﬁnal legitimate predictions.

4.3. Triplet Extraction

4.3.2. Triplet Matching Rules. We list four matching
rules below to turn simple SRL columns into triplets:
• A0, V, A1: <SUBJECT, VERB, DIRECT
OBJECT>
• A0,
V,
A2:
<SUBJECT,
VERB,
PREPOSITION>, if direct object A1 not
present in column.
• A0, V, A1, A2-AM-LOC: <SUBJECT, VERB,
DIRECT OBJECT, location (PREPOSITION)>
• A1,
V, A2: <DIRECT OBJ, VERB,
PREPOSITION>

Our triplet extraction algorithm processes SRL output. The SRL output has a speciﬁc multi-column
format. Each column represents one verb (predicate)
and its arguments (A0, A1, R-A1, A2, etc) potentially
forming many triplets. For a simple sentence, we can
read one column and extract a triplet. For complex
sentences with many verbs, we developed a bottom-up
extraction algorithm for detecting and tagging nested
events. We will illustrate our approach using the following example.
“America
commissioned
Example Paragraph:
Musharraf with the task of taking revenge on the
border tribes, especially the valiant and lofty Pashtun
tribes, in order to contain this popular support for
jihad against its crusader campaign. So he began
demolishing homes, making arrests, and killing
innocent people. Musharraf, however, pretends to
forget that these tribes, which have defended Islam
throughout its history, will not bow to US”.
Our algorithm produces the following triplets for the
example paragraph below:

4.3.3. Triplet Extraction Accuracy. The triplet extraction accuracy is based on SRL accuracy. SRL has
precision of 82.28%, recall of 76.78% and f-measure
79.44% [28].
4.3.4. Triplet Based Feature Extraction. For each
verb (V) mentioned in a story (S), or non-story (NS)
we stemmed and aggregated its arguments corresponding to its SUBJECTs, OBJECTs and PREPOSITIONs
to generate following set-valued “semantic verb features” by using the training data:
• Argument list for S.V.Subjects, S.V.Objects,
S.V.Prepositions for each verb V and story S.
• Argument list for NS.V.Subjects, NS.V.Objects,
NS.V.Prepositions for each verb V and Non-Story
NS.
For each test paragraph P, for each verb V in
P, we extract its typed argument lists P.V.Subjects,
P.V.Objects and P.V.Prepositions. Then, we match them
to the argument lists of the same verb V. A match
succeeds if the overlap between a feature’s argument
list (e.g. S.V.Subjects, or NS.V.Subjects) covers the
majority of the test paragraph’s corresponding verb
argument list (e.g. P.V.Subjects).

Table 3. Extracted Triplets
Event

Subject

Verb

Object

E1
E2
E2

America
America
Musharraf
Musharraf
Musharraf
Musharraf
Musharraf
tribes
tribes

commission
take
demolish
make
kill
pretend
forget
defend
not bow

Musharraf
revenge
homes
arrests
innocent people
E1
E2
Islam
to US

4.3.1. Bottom-Up Event Tagging Approach. In the
example above, consider the triplet <Musharraf, pretend, E1>. Here the object column of the verb
pretend has an A1 argument including three other
verbs (forget, defend and bow). That is, argument A1
is itself complex, comprising other triplets. So we tag

5. Generalized Verb Features
Generalization and reduction of features is an important step in classiﬁcation process. Reduced feature representations not only reduce computing time but they

578
577

may also yield to better discriminatory behavior. Owing to the generic nature of the curse of dimensionality
it has to be assumed that feature reduction techniques
are likely to improve classiﬁcation algorithm.
Our training data had 750 and 1, 754 distinct verbs
in stories and non-stories, yielding 750 ∗ 3 = 2, 250
and, 1, 754 ∗ 3 = 5, 262 verb features for stories and
non-stories respectively, and total of 7, 512 features.
VerbNet (VN) [29] is the largest on-line verb lexicon currently available for English. It is a hierarchical domain-independent, broad-coverage verb lexicon.
VerbNet index has 5, 879 total verbs represented, and
these verbs are mapped into 270 total VerbNet main
classes. For example, the verbs mingle, meld, blend,
combine, decoct, add, connect all share the same
meaning (i.e. to bring together or combine), and hence
they map to verb class “mix” numbered 22.1. With the
help of VerbNet and SRL argument types of the verbs,
we mapped all occurrences of our verbs in stories and
non-stories to one of these 270 VerbNet main classes.
This mapping enabled us to reduce our verb features to
268 ∗ 6 = 1, 608 verb features. The number 6 is used
in the preceding equation since each verb class can
lead to at most 6 features as V.Subject, V.Object and
V.Preposition for its story and non-story occurrences.
We started with 7, 512 verb features, and after mapping
these verb features to their verb category features we
ended up with 1, 608 features only.
In the generalization process, we faced a problem
of verb sense disambiguation. There are some verbs
which can be mapped to different senses, and each
sense belongs to a different verb class. For example,
the verb “add” can be used with the sense ‘mix’
(22.1) or ‘categorize’ (29.2) or ‘say’ (25.3). To solve
this problem, we used argument types extracted using
SRL for the ambiguous verbs. Then, we performed
a look-up for each verb in the PropBank database
to identify the matching verb sense with same type
of arguments, and its verb class. PropBank [30] is a
corpus that is annotated with verbal propositions, and
their arguments - a “proposition bank”. In the look-up
process, there is a chance that we may encounter more
than one verb sense for the input verb matching the
corresponding argument types. In this case, we picked
the ﬁrst matching verb sense listed in PropBank.

matching is implemented using JAVA and classiﬁcation
is performed using LIBSVM [22] in MATLAB.
Table 4. Classiﬁer Performance for Stories
Feature Set
POS
Keyword
Keyword + POS + NE
Triplet
Triplet + POS + NE

Precision
0.133
0.821
0.750
0.798
0.731

Recall
0.066
0.205
0.214
0.515
0.559

F-measure
0.088
0.329
0.333
0.626
0.634

Table 5. Classiﬁer Performance for Non-Stories
Feature Set
POS
Keyword
Keyword + POS + NE
Triplet
Triplet + POS + NE

Precision
0.887
0.903
0.904
0.886
0.905

Recall
0.944
0.994
0.991
0.998
0.939

F-measure
0.914
0.946
0.945
0.938
0.923

6.1. Effectiveness of Semantic Features
The baseline performance for a dummy classiﬁer
which would assign all instances to the majority class
(non-story) would achieve 80.5% precision and 100%
recall for the non-story category however, its precision
and recall would be null for the stories. Hence, not
useful at all for detecting stories.
Our proposed model makes use of triplets to incorporate both semantic and structural information available in stories and non-stories. In Table (4), we report
the performance of SVM classiﬁcation with various
feature sets. SVM with POS, NE and generalized
triplet based features outperforms other combinations
of standard categories of features in terms of recall
and F-measure. Table (4) shows 151.2% boost in
recall and 90% boost in F-measure for keyword based
(second row) vs triplet based (fourth row) features.
After adding POS and NE features (Keyword + POS
+ NE based, third row) vs (Triplets + POS + NE, ﬁfth
row), we obtained 161.2% boost in recall and 90%
boost in F-measure.

7. Conclusion
This paper proposes a model with semantic triplet
based features for story classiﬁcation. The effectiveness of the model is demonstrated against other traditional features used in the literature for text classiﬁcation tasks. Future work includes more detailed
evaluations, and also experiments with appropriate
generalizations of nouns, adjectives and other types of
keywords found in verb arguments.

6. Experimental Evaluations
In this section, we evaluate the the utility of standard
keyword based features, statistical features based on
shallow-parsing (such as density of POS tags and
named entities), and a new set of semantic features
to develop a story classiﬁer. Feature extraction and

579
578

Acknowledgment
[15]

This research was supported by an Ofﬁce of Naval
Research grant (N00014-09-1-0872) to the Center for
Strategic Communication at Arizona State University.

[16]

References

[17]

[1] J. Bruner and S. Weisser, “Autobiography and the
construction of self,” 1992.
[2] C. Joseph, The hero with a thousand faces. Princeton
University Press, 1949.
[3] H. L. Halverson, J. R. Goodall and S. R. Corman,
Master Narratives of Islamist Extremism. New York:
Palgrave Macmillan, 2011.
[4] R. Hoffman, U. Grasemann, R. Gueorguieva, D. Quinlan, D. Lane, and R. Miikkulainen, “Using computational patients to evaluate illness mechanisms in
schizophrenia,” Biological psychiatry, 2011.
[5] B. Masand, G. Linoff, and D. Waltz, “Classifying news
stories using memory based reasoning,” in Proceedings
of the 15th annual international ACM SIGIR conference
on Research and development in information retrieval.
ACM, 1992, pp. 59–65.
[6] D. Billsus and M. Pazzani, “A hybrid user model
for news story classiﬁcation,” Lectures-International
Centre for Mechanical Sciences, pp. 99–108, 1999.
[7] A. S. Gordon and K. Ganesan, “Automated story
capture from conversational speech,” in K-CAP ’05:
Proceedings of the 3rd international conference on
Knowledge capture, ACM.
Banff, Canada: ACM,
2005, p. 145–152.
[8] A. Gordon, Q. Cao, and R. Swanson, “Automated story
capture from internet weblogs,” in Proceedings of the
4th international conference on Knowledge capture.
ACM, 2007, pp. 167–168.
[9] A. Gordon and R. Swanson, “Identifying personal stories in millions of weblog entries,” in Third International Conference on Weblogs and Social Media, Data
Challenge Workshop, San Jose, CA, 2009.
[10] J. Fleiss, “Measuring nominal scale agreement among
many raters.” Psychological bulletin, vol. 76, no. 5, p.
378, 1971.
[11] E. F. Tjong Kim Sang and F. De Meulder, “Introduction
to the conll-2003 shared task: Language-independent
named entity recognition,” in Proceedings of CoNLL2003, W. Daelemans and M. Osborne, Eds. Edmonton,
Canada, 2003, pp. 142–147.
[12] T. Poibeau and L. Kosseim, “Proper name extraction
from non-journalistic texts,” Language and Computers,
vol. 37, no. 1, pp. 144–157, 2001.
[13] J. Finkel, T. Grenager, and C. Manning, “Incorporating
non-local information into information extraction systems by gibbs sampling,” in Proceedings of the 43rd
Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics,
2005, pp. 363–370.
[14] L. Ratinov and D. Roth, “Design challenges and misconceptions in named entity recognition,” in CoNLL
’09: Proceedings of the Thirteenth Conference on Computational Natural Language Learning. Morristown,

[18]

[19]

[20]

[21]
[22]

[23]
[24]

[25]
[26]

[27]

[28]

[29]

[30]

580
579

NJ, USA: Association for Computational Linguistics,
2009, pp. 147–155.
“Calais.” [Online]. Available: http://www.opencalais.
com/
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, and
Z. Ives, “Dbpedia: A nucleus for a web of open data,”
in In 6th Intl Semantic Web Conference, Busan, Korea.
Springer, 2007, pp. 11–15.
S. Robertson, “Understanding inverse document frequency: on theoretical arguments for idf,” Journal of
Documentation, vol. 60, no. 5, pp. 503–520, 2004.
E. Brill, “A simple rule-based part of speech tagger,”
in Proceedings of the workshop on Speech and Natural
Language. Association for Computational Linguistics,
1992, pp. 112–116.
T. Joachims, “A statistical learning learning model
of text classiﬁcation for support vector machines,” in
Proceedings of the 24th annual international ACM
SIGIR conference on Research and development in
information retrieval. ACM, 2001, pp. 128–136.
B. Boser, I. Guyon, and V. Vapnik, “A training algorithm for optimal margin classiﬁers,” in Proceedings of
the ﬁfth annual workshop on Computational learning
theory. ACM, 1992, pp. 144–152.
C. Cortes and V. Vapnik, “Support-vector networks,”
Machine learning, vol. 20, no. 3, pp. 273–297, 1995.
C. Chang and C. Lin, “Libsvm: a library for support
vector machines,” ACM Transactions on Intelligent
Systems and Technology (TIST), vol. 2, no. 3, p. 27,
2011.
S. Keerthi and C. Lin, “Asymptotic behaviors of support
vector machines with gaussian kernel,” Neural computation, vol. 15, no. 7, pp. 1667–1689, 2003.
D. Rusu, L. Dali, B. Fortuna, M. Grobelnik, and
D. Mladenić, “Triplet extraction from sentences,” Proceedings of the 10th International Multiconference Information Society-IS, pp. 8–12, 2007.
D. Hooge Jr, “Extraction and indexing of triplet-based
knowledge using natural language processing,” Ph.D.
dissertation, University of Georgia, 2007.
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu, and D. Jurafsky, “Stanfords multi-pass sieve
coreference resolution system at the conll-2011 shared
task,” CoNLL 2011, p. 28, 2011.
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning, “A multipass sieve for coreference resolution,” in Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing. Association for Computational
Linguistics, 2010, pp. 492–501.
V. Punyakanok, D. Roth, and W. Yih, “The importance
of syntactic parsing and inference in semantic role
labeling,” Computational Linguistics, vol. 34, no. 2, pp.
257–287, 2008.
K. Kipper, A. Korhonen, N. Ryant, and M. Palmer, “A
large-scale classiﬁcation of english verbs,” Language
Resources and Evaluation, vol. 42, no. 1, pp. 21–40,
2008.
M. Palmer, D. Gildea, and P. Kingsbury, “The proposition bank: An annotated corpus of semantic roles,”
Computational Linguistics, vol. 31, no. 1, pp. 71–106,
2005.

ORTHOGONAL RANK-ONE MATRIX PURSUIT
FOR LOW RANK MATRIX COMPLETION
ZHENG WANG∗ , MING-JUN LAI† , ZHAOSONG LU‡ , WEI FAN§ , HASAN DAVULCU∗ ,

arXiv:1404.1377v2 [cs.LG] 16 Apr 2014

AND JIEPING YE∗

Abstract. In this paper, we propose an efficient and scalable low rank matrix completion
algorithm. The key idea is to extend orthogonal matching pursuit method from the vector case to
the matrix case. We further propose an economic version of our algorithm by introducing a novel
weight updating rule to reduce the time and storage complexity. Both versions are computationally
inexpensive for each matrix pursuit iteration, and find satisfactory results in a few iterations. Another
advantage of our proposed algorithm is that it has only one tunable parameter, which is the rank.
It is easy to understand and to use by the user. This becomes especially important in large-scale
learning problems. In addition, we rigorously show that both versions achieve a linear convergence
rate, which is significantly better than the previous known results. We also empirically compare the
proposed algorithms with several state-of-the-art matrix completion algorithms on many real-world
datasets, including the large-scale recommendation dataset Netflix as well as the MovieLens datasets.
Numerical results show that our proposed algorithm is more efficient than competing algorithms while
achieving similar or better prediction performance.
Key words. Low rank, singular value decomposition, rank minimization, matrix completion,
matching pursuit

1. Introduction. Recently, low rank matrix learning has attracted significant
attentions in machine learning and data mining due to its wide range of applications, such as collaborative filtering, dimensionality reduction, compressed sensing,
multi-class learning and multi-task learning. See [1, 2, 3, 7, 9, 23, 34, 40, 37] and
the references therein. In this paper, we consider the general form of low rank matrix completion: given a partially observed real-valued matrix Y ∈ ℜn×m , the low
rank matrix completion problem is to find a matrix X ∈ ℜn×m with minimum rank
that best approximates the matrix Y on the observed elements. The mathematical
formulation is given by
min

rank(X)

s.t.

PΩ (X) = PΩ (Y),

X∈ℜn×m

(1.1)

where Ω is the set of all index pairs (i, j) of observed entries, and PΩ is the orthogonal
projector onto the span of matrices vanishing outside of Ω.
1.1. Related Works. As it is intractable to minimize the matrix rank exactly
in the general case, many approximate solutions have been proposed to attack the
problem (1.1) (cf., e.g. [7, 24, 28]). A widely used convex relaxation of matrix rank is
the trace norm or nuclear norm [7]. The matrix trace norm is defined by the Schatten
p-norm
with p = 1. For matrix X with rank r, its Schatten p-norm is defined by
P
( ri=1 σip )1/p , where {σi } are the singular values of X and without loss of generality
we assume they are sorted in descending order. Thus, the trace norm of X is the ℓ1
∗ School of Computing, Informatics, and Decision Systems Engineering, Arizona State University,
Tempe, Arizona 85287, USA (zhengwang@asu.edu, jieping.ye@asu.edu).
† Department of Mathematics, The University of Georgia, Athens, GA 30602, USA
(mjlai@math.uga.edu).
‡ Department of Mathematics, Simon Frasor University, Burnaby, BC, V5A 156, Canada
(zhaosong@sfu.ca).
§ Huawei Noah’s Ark Lab Hong Kong (wei.fan@gmail.com).

1

2

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion

norm of the matrix spectrum as ||X||∗ =
problem (1.1) is given by
min

X∈Rn×m

s.t.

Pr

i=1

|σi |. Then the convex relaxation for

||X||∗

PΩ (X) = PΩ (Y).

(1.2)

Cai et al. [6] propose an algorithm based on soft singular value thresholding (SVT).
Keshavan et al. [21] and Meka et al. [18] develop more efficient algorithms by using
the top-k singular pairs.
Many other algorithms have been developed to solve the trace norm penalized
problem:
min

X∈Rn×m

||PΩ (X) − PΩ (Y)||2F + λ||X||∗ .

(1.3)

Ji et al. [20], Liu et al. [27] and Toh et al. [44] independently propose to employ the
proximal gradient algorithm to improve the algorithm of [6] by significantly
√ reducing the number of iterations. They obtain an ǫ-accurate solution in O(1/ ǫ) steps.
More efficient soft singular vector thresholding algorithms are proposed in [29, 30]
by investigating the factorization property of the estimated matrix. Each step of the
algorithms requires the computation of a partial SVD for a dense matrix. In addition,
several methods approximate the trace norm using its variational characterizations
[32, 40, 46, 37], and proceed by alternating optimization. However these methods lack
global convergence guarantees.
Solving these low rank or trace norm problems is computationally expensive for
large matrices, as it involves computing singular value decomposition (SVD). Most
of the methods above involve the computation of SVD or truncated SVD iteratively,
which is not scalable to large-scale problems. How to solve these problems efficiently
and accurately for large-scale problems attracts much attention in recent years.
Recently, the coordinate gradient descent method has been demonstrated to be
efficient in solving sparse learning problems in the vector case [11, 39, 47, 48]. The
key idea is to solve a very simple one-dimensional problem (for one coordinate) in
each iteration. One natural question is whether and how such method can be applied
to solve the matrix completion problem. Some progress has been made recently along
this direction. Dudı́k et al. [9] propose a coordinate gradient descent solution for
the trace norm penalized problem. They recast the non-smooth objective in problem
(1.3) as a smooth one in an infinite dimensional rank-one matrix space, then apply the
coordinate gradient algorithm on the collection of rank-one matrices. Zhang et al. [49]
further improve the efficiency using the boosting method, and the improved algorithm
guarantees an ǫ-accuracy within O(1/ǫ) iterations. Although these algorithms need
slightly more iterations than the proximal methods, they are more scalable as they
only need to compute the top singular vector pair in each iteration. Note that the
top singular vector pair can be computed efficiently by the power method or Lanczos
iterations [13]. Jaggi et al. [17] propose an algorithm which achieves the same iteration
complexity as the algorithm in [49] by directly applying the Hazan’s algorithm [15].
Tewari et al. [42] solve a more general problem based on a greedy algorithm. ShalevShwartz et al. [38] further reduce the number of iterations based on a heuristic without
theoretical guarantees.
Most methods based on the top singular vector pair include two main steps in
each iteration. The first step involves computing the top singular vector pair, and the
second step refines the weights of the rank-one matrices formed by all top singular

3
vector pairs obtained up to the current iteration. The main differences among these
algorithms lie in how they refine the weights. The Jaggi’s algorithm (JS) [17] directly applies the Hazan’s algorithm [15], which relied on the Frank-Wolfe algorithm
[10]. It updates the weights with a small step size and does not consider further
refinement. It does not use all information in each step, which leads to a slow convergence rate. Similar to JS, Tewari et al. [42] use a small update step size for a
general structure constrained problem. The greedy efficient component optimization
(GECO) [38] optimizes the weights by solving another time consuming optimization
problem. It involves a smaller number of iterations than the JS algorithm. However,
the sophisticated weight refinement leads to a higher total computational cost. The
lifted coordinate gradient descent algorithm (Lifted) [9] updates the rank-one matrix
basis with a constant weight in each iteration, and conducts a LASSO type algorithm
[43] to fully correct the weights. The weights for the basis update are difficult to
tune: a large value leads to divergence; a small value makes the algorithm slow [49].
The matrix norm boosting approach (Boost) [49] learns the update weights and designs a local refinement step by a non-convex optimization problem which is solved
by alternating optimization. It has a sub-linear convergence rate.
Let us summarize their common drawbacks as follows:
• The weight refinement steps are inefficient, resulting in a slow convergence
rate. The current best convergence rate is O(1/ǫ). Some refinement steps
themselves contain computationally expensive iterations [9, 49], which do not
scale to large-scale data.
• They have heuristic-based tunable parameters which are not easy to use.
However, these parameters severely affect their convergence speed and the
approximation result. In some algorithms, an improper parameter even makes
the algorithm diverge [6, 9].
In this paper, we present a simple and efficient algorithm to solve the low rank
matrix completion problem. The key idea is to extend the orthogonal matching pursuit (OMP) procedure [35] from the vector case to the matrix case. In each iteration,
a rank-one basis matrix is generated by the left and right top singular vectors of the
current approximation residual. In the standard version of the proposed algorithm,
we fully update the weights for all rank-one matrices in the current basis set at the
end of each iteration by performing an orthogonal projection of the observation matrix onto their spanning subspace. The most time-consuming step of the proposed
algorithm is to calculate the top singular vector pair of a sparse matrix, which costs
O(|Ω|) operations in each iteration. An appealing feature of the proposed algorithm
is that it has a linear convergence rate. This is quite different from traditional orthogonal matching pursuit or weak orthogonal greedy algorithms, whose convergence
rate for sparse vector recovery is sub-linear as shown in [26]. See also [8], [41], [45] for
an extensive study on various greedy algorithms. With this rate of convergence, we
only need O(log(1/ǫ)) iterations for achieving an ǫ-accuracy solution.
One drawback of the standard algorithm is that it needs to store all rank-one
matrices in the current basis set for full weight updating, which contains r|Ω| elements
in the r-th iteration. This makes the storage complexity of the algorithm dependent on
the number of iterations, which restricts the approximation rank especially for largescale matrices. To tackle this problem, we propose an economic weight updating rule
for this algorithm. In this economic version of the proposed algorithm, we only track
two matrices in each iteration. One is the current estimated matrix and the other one
is the pursued rank-one matrix. When restricted to the observations in Ω, each has |Ω|

4

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion

nonzero elements. Thus the storage requirement, i.e., 2|Ω|, keeps the same in different
iterations, which is the same as the greedy algorithms [17, 42]. Interestingly, we show
that using this economic updating rule we still retain the linear convergence rate. To
the best of our knowledge, our proposed algorithms are the fastest among all related
methods in the literature. We verify the efficiency of our algorithms empirically on
large-scale matrix completion problems, such as MovieLens [31] and Netflix [4, 5], see
§7.
The main contributions of our paper are:
• We propose a computationally efficient and scalable algorithm for matrix
completion, which extends the orthogonal matching pursuit from the vector
case to the matrix case.
• We theoretically prove the linear convergence rate of our algorithm. As a
result, we only need O(log(1/ǫ)) steps to obtain an ǫ-accuracy solution, and
in each step we only need to compute the top singular vector pair, which can
be computed efficiently.
• We further reduce the storage complexity of our algorithm based on an economic weight updating rule while retaining the linear convergence rate. This
version of our algorithm has a constant storage complexity which is independent of the approximation rank and is more practical for large-scale matrices.
• Both versions of our algorithm have only one free parameter, i.e., the rank
of the estimated matrix. The proposed algorithm is guaranteed to converge,
i.e., no risk of divergence.
1.2. Notations and Organization. Let Y = (y1 , · · · , ym ) ∈ ℜn×m be an
n× m real matrix, and Ω ⊂ {1, · · · , n} × {1, · · · , m} denote the indices of the observed
entries of Y. PΩ is the projection operator onto the space spanned by the matrices
vanishing outside of Ω so that the (i, j)-th component of PΩ (Y) equals to Yi,j for
(i,
j) ∈ Ω and zero otherwise. The Frobenius norm of Y is defined as ||Y||F =
qP
T
T T
2
i,j Yi,j . Let vec(Y) = (y1 , · · · , ym ) denote a vector reshaped from matrix Y
by concatenating all its column vectors. Let ẏ be the vector by concatenating all
observed entries in Y, which is composed by keeping the observed elements in the
vector vec(PΩ (Y)). The Frobenius inner product of two matrices X and Y is defined
as hX, Yi = trace(XT Y), which also equals to the component-wise inner product of
the corresponding vectors as hvec(X), vec(Y)i. Given a matrix A ∈ ℜn×m , we denote
PΩ (A) by AΩ . For any two matrices A, B ∈ ℜn×m , we define
hA, BiΩ = hAΩ , BΩ i,
p
kAkΩ =
hA, AiΩ . Without further declaration, thepmatrix norm refers to the
Frobenius norm, which could also be written as kAk = hA, Ai.
The rest of the paper is organized as follows: we present our algorithm in Section
2; Section 3 analyzes the convergence rate of the standard version of our algorithm; we
further propose an economic version of our algorithm and prove its linear convergence
rate in Section 4; Section 5 extends the proposed algorithm to a more general matrix
sensing case, and presents its guarantee of finding the optimal solution under rankrestricted-isometry-property condition; in Section 6 we analyze the stability of both
versions of our algorithms; empirical evaluations are presented in Section 7 to verify
the efficiency and effectiveness of our algorithms. We finally conclude our paper in
Section 8.

5
2. Orthogonal Rank-One Matrix Pursuit. It is well-known that any matrix
X ∈ ℜn×m can be written as a linear combination of rank-one matrices, that is,
X
X = M(θ) =
θi M i ,
(2.1)
i∈I

where {Mi : i ∈ I} is the set of all n × m rank-one matrices with unit Frobenius
norm. Clearly, there is an infinitely many choice of Mi ’s. Such a representation can
be obtained via the standard SVD decomposition of X.
The original low rank matrix approximation problem aims to minimize the zeronorm of θ subject to the constraint:
min ||θ||0
θ

s.t.

PΩ (M(θ)) = PΩ (Y),

(2.2)

where ||θ||0 denotes the number of nonzero elements of vector θ.
If we reformulate the problem as
min

||PΩ (M(θ)) − PΩ (Y)||2F

s.t.

||θ||0 ≤ r,

θ

(2.3)

we could solve it by an orthogonal matching pursuit type algorithm using rank-one
matrices as the basis. In particular, we are to find a suitable subset with overcomplete rank-one matrix coordinates, and learn the weight for each coordinate. This
is achieved by executing two steps alternatively: one is to pursue the basis, and the
other one is to learn the weights of the basis.
Suppose that after the (k-1)-th iteration, the rank-one basis matrices M1 , . . . , Mk−1
and their current weight θ k−1 are already computed. In the k-th iteration, we are to
pursue a new rank-one basis matrix Mk with unit Frobenius norm, which is mostly
correlated with the current observed regression residual Rk = PΩ (Y) − Xk−1 , where
Xk−1 = (M(θ

k−1

))Ω =

k−1
X

θik−1 (Mi )Ω .

i=1

Therefore, Mk can be chosen to be an optimal solution of the following problem:
max{hM, Rk i : rank(M) = 1, kMkF = 1}.
M

(2.4)

Notice that each rank-one matrix M with unit Frobenius norm can be written as the
product of two unit vectors, namely, M = uvT for some u ∈ ℜn and v ∈ ℜm with
kuk = kvk = 1. We then see that problem (2.4) can be equivalently reformulated as
max{uT Rk v : kuk = kvk = 1}.
u,v

(2.5)

Clearly, the optimal solution (u∗ , v∗ ) of problem (2.5) is a pair of top left and right
singular vectors of Rk . It can be efficiently computed by the power method [17, 9].
The new rank-one basis matrix Mk is then readily available by setting Mk = u∗ v∗T .
After finding the new rank-one basis matrix Mk , we update the weights θ k for
all currently available basis matrices {M1 , · · · , Mk } by solving the following least
squares regression problem:
min ||

θ∈ℜk

k
X
i=1

θi Mi − Y||2Ω .

(2.6)

6

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion

By reshaping the matrices (Y)Ω and (Mi )Ω into vectors ẏ and ṁi , we can easily see
that the optimal solution θ k of (2.6) is given by
θ k = (M̄Tk M̄k )−1 M̄Tk ẏ,

(2.7)

where M̄k = [ṁ1 , · · · , ṁk ] is the matrix formed by all reshaped basis vectors. The
row size of matrix M̄k is the total number of observed entries. It is computationally
expensive to directly calculate the matrix multiplication. We simplify this step by an
incremental process, and give the implementation details in Appendix.
We run the above two steps iteratively until some desired stopping condition is
satisfied. We can terminate the method based on the rank of the estimated matrix
or the approximation residual. In particular, one can choose a preferred rank of the
approximate solution matrix. Alternatively, one can stop the method once the residual
kRk k is less than a tolerance parameter ε. The main steps of Orthogonal Rank-One
Matrix Pursuit (OR1MP) are given in Algorithm 1.
Algorithm 1 Orthogonal Rank-One Matrix Pursuit (OR1MP)
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0, θ 0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed
residual matrix Rk = YΩ − Xk−1 and set Mk = uk (vk )T .
Step 2: Compute the weight θ k using the closed form least squares solution
θ k = (M̄Tk M̄k )−1 M̄Tk ẏ.
P
Step 3: Set Xk = ki=1 θik (Mi )Ω and k ← k + 1.
until stopping criterion is satisfiedP
k
Output: Constructed matrix Ŷ = i=1 θik Mi .

Remark 2.1. In our algorithm, we adapt orthogonal matching pursuit on the observed part of the matrix. This is similar to the GECO algorithm. However, GECO
constructed the estimated matrix by projecting the observation matrix onto a much
larger subspace, which is a product of two subspaces spanned by all left singular vectors and all right singular vectors obtained up to the current iteration. So it has much
higher computational complexity. Lee et al. [25] recently proposed the ADMiRA algorithm, which is also a greedy approach. In each step it first chose 2r components by
top-2r truncated SVD and then uses another top-r truncated SVD to obtain a rank-r
matrix. Thus, the ADMiRA algorithm is computationally more expensive than the
proposed algorithm. The difference between the proposed algorithm and ADMiRA is
somewhat similar to the difference between the OMP [35] for learning sparse vectors
and CoSaMP [33]. In addition, the performance guarantees (including recovery guarantee and convergence property) of ADMiRA rely on strong assumptions, i.e., the
matrix involved in the loss function satisfies a rank-restricted isometry property [25].

3. Convergence Analysis of Algorithm 1. In this section, we will show that
Algorithm 1 is convergent and achieves a linear convergence rate. This result is given
in the following theorem.
Theorem 3.1. The orthogonal rank-one matrix pursuit algorithm satisfies
s
!k−1
1
1−
||Rk || ≤
kY kΩ , ∀k ≥ 1.
min(m, n)

7

Before proving Theorem 3.1, we need to establish some useful and preparatory
properties of Algorithm 1. The first property says that Rk+1 is perpendicular to all
previously generated Mi for i = 1, · · · , k.
Property 3.2. hRk+1 , Mi i = 0 for i = 1, · · · , k.
Proof. Recall that θ k is the optimal solution of problem (2.6). By the first-order
optimality condition, one has
hY −

t
X
i=1

θik Mi , Mi iΩ = 0 for i = 1, · · · , k,

Pk
which together with Rk = YΩ −Xk−1 and Xk = i=1 θik (Mi )Ω implies that hRk+1 , Mi i =
0 for i = 1, · · · , k.
The following property shows that as the number of rank-one basis matrices Mi
increases during our learning process, the residual kRk k does not increase.
Property 3.3. kRk+1 k ≤ kRk k for all k ≥ 1.
Proof. We observe that for all k ≥ 1,
kRk+1 k2

=
≤
=

min {kY −

θ∈ℜk

Pk

min {kY −

θ∈ℜk−1

kRk k2 ,

2
i=1 θi Mi kΩ }

Pk−1
i=1

θi Mi k2Ω }

and hence the conclusion holds.
We next establish that {(Mi )Ω }ki=1 is linearly independent unless kRk k = 0. It
follows that formula (2.7) is well-defined and hence θ k is uniquely defined before the
algorithm stops.
Property 3.4. Suppose that Rk 6= 0 for some k ≥ 1. Then, M̄i has a full
column rank for all i ≤ k.
Proof. Using Property 3.3 and the assumption Rk 6= 0 for some k ≥ 1, we see
that Ri 6= 0 for all i ≤ k. We now prove the statement of this lemma by induction
on i. Indeed, since R1 6= 0, we clearly have M̄1 6= 0. Hence the conclusion holds for
i = 1. We now assume that it holds for i − 1 < k and need to show that it also holds
for i ≤ k. By the induction hypothesis, M̄i−1 has a full column rank. Suppose for
contradiction that M̄i does not have a full column rank. Then, there exists α ∈ ℜi−1
such that
(Mi )Ω =

i−1
X

αj (Mj )Ω ,

j=1

which together with Property 3.2 implies that hRi , Mi i = 0. It follows that
σ1 (Ri ) = uTi Ri vi = hRi , Mi i = 0,
and hence Ri = 0, which contradicts the fact that Rj 6= 0 for all j ≤ i. Therefore,
M̄i has a full column rank and the conclusion holds for general i.
We next build a relationship between two consecutive residuals kRk+1 k and kRk k.
For convenience, define θkk−1 = 0 and let
θ k = θ k−1 + η k .

8

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion

In view of (2.6), one can observe that
η k = arg min ||
η

k
X
i=1

ηi Mi − Rk ||2Ω .

(3.1)

Let
Lk =

k
X

ηik (Mi )Ω .

(3.2)

i=1

By the definition of Xk , one can also observe that
Xk = Xk−1 + Lk ,
Rk+1 = Rk − Lk .
Property 3.5. ||Rk+1 ||2 = ||Rk ||2 − ||Lk ||2 and ||Lk ||2 ≥ hMk , Rk i2 , where Lk
is defined in (3.2).
P
Proof. Since Lk = i≤k ηik (Mi )Ω , it follows from Property 3.2 that hRk+1 , Lk i =
0. We then have
||Rk+1 ||2

= ||Rk − Lk ||2

= ||Rk ||2 − 2hRk , Lk i + ||Lk ||2

= ||Rk ||2 − 2hRk+1 + Lk , Lk i + ||Lk ||2

= ||Rk ||2 − 2hLk , Lk i + ||Lk ||2

= ||Rk ||2 − ||Lk ||2 .

We next bound kLk k2 from below. If Rk = 0, ||Lk ||2 ≥ hMk , Rk i2 clearly holds.
We now suppose throughout the remaining proof that Rk 6= 0. It then follows from
Property 3.4 that M̄k has a full column rank. Using this fact and (3.1), we have
η k = M̄Tk M̄k

−1

M̄Tk ṙk ,

where ṙk is the reshaped residual vector of Rk . Invoking that Lk =

P

i≤k

then obtain
||Lk ||2 = ṙTk M̄k (M̄Tk M̄k )−1 M̄Tk ṙk .

ηik (Mi )Ω , we

(3.3)

Let M̄k = QU be the QR factorization of M̄k , where QT Q = I and U is a k × k
nonsingular upper triangular matrix. One can observe that (M̄k )k = ṁk , where
(M̄k )k denotes the k-th column of the matrix M̄k and ṁk is the reshaped vector of
(Mk )Ω . Recall that kMk k = kuk vkT k = 1. Hence, k(M̄k )k k ≤ 1. Due to QT Q = I,
M̄k = QU and the definition of U, we have
0 < |Ukk | ≤ kUk k = k(M̄k )k k ≤ 1.
In addition, by Property 3.2, we have
T

M̄Tk ṙk = [0, · · · , 0, hMk , Rk i] .

(3.4)

9
Substituting M̄k = QU into (3.3), and using QT Q = I and (3.4), we obtain that
kLk k2 = ṙTk M̄k (UT U)−1 M̄Tk ṙk

= [0, · · · , 0, hMk , Rk i] U−1 U−T [0, · · · , 0, hMk , Rk i]

T

= hMk , Rk i2 /(Ukk )2 ≥ hMk , Rk i2 ,

where the last equality follows since U is upper triangular and the last inequality is
due to |Ukk | ≤ 1.
We are now ready to prove Theorem 3.1.
Proof. [ of Theorem 3.1] Using the definition of Mk , we have
hMk , Rk i = huk (vk )T , Rk i = σ1 (Rk )
qP 2
q
q
kRk k2
kRk k2
i σi (Rk )
≥
=
≥
rank(Rk )
rank(Rk )
min(m,n) .

Using this inequality and Property 3.5, we obtain that
||Rk+1 ||2

= ||Rk ||2 − ||Lk ||2 ≤ ||Rk ||2 − hMk , Rk i2
≤ (1 −

1
2
min(m,n) )||Rk || .

In view of this relation and the fact that kR1 k = kYk2Ω , we easily conclude that
||Rk || ≤

s

1
1−
min(m, n)

!k−1

kYkΩ .

This completes the proof.
Remark 3.6. If Ω is the entire set of all indices of {(i, j), i = 1, · · · , m, j =
1, · · · , n}, our orthogonal rank-one matrix pursuit algorithm equals to standard singular value decomposition using the power method. In particular, when Ω is the set
of all indices while the given entries are noisy values of an exact matrix, our OR1MP
algorithm can help remove the noises.
Remark 3.7. In a standard study of the convergence rate of the Orthogonal
Matching Pursuit (OMP) or Orthogonal Greedy Algorithm (OGA), one can only get
|hMk , Rk i| ≥ kRk k2 , which leads a sub-linear convergence. Our Mk is a data dependent construction which is based on the top left and right singular vectors of the
residual matrix Rk . It thus has a better estimate which gives us the linear convergence.
4. An Economic Orthogonal Rank-One Matrix Pursuit Algorithm. The
proposed OR1MP algorithm has to track all pursued bases and save them in the
memory. It demands O(r|Ω|) storage complexity to obtain a rank-r estimated matrix.
For large scale problems, such storage requirement is not negligible and restricts the
rank of the matrix to be estimated. To adapt our algorithm to large scale problems
with a large approximation rank, we simplify the orthogonal projection step by only
tracking the estimated matrix Xk−1 and the rank-one update matrix Mk . In this case,
we only need to estimate the weights for these two matrices by solving the following
least squares problem:
αk = arg

min

α={α1 ,α2 }

||α1 Xk−1 + α2 Mk − Y||2Ω .

(4.1)

10

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion

Algorithm 2 Economic Orthogonal Rank-One Matrix Pursuit (EOR1MP)
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0, θ 0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed
residual matrix Rk = YΩ − Xk−1 and set Mk = uk (vk )T .
Step 2: Compute the optimal weights αk for Xk−1 and Mk by solving:
arg min ||α1 Xk−1 + α2 (Mk )Ω − YΩ ||2 .
α

Step 3: Set Xk = αk1 Xk−1 + αk2 (Mk )Ω ; θkk = αk2 and θik = θik−1 αk1 for i < k;
k ← k + 1.
until stopping criterion is satisfiedP
Output: Constructed matrix Ŷ = ki=1 θik Mi .

This still fully corrects all weights of the existed bases, though the correction is suboptimal. If P
we write the estimated matrix as a linear combination of the bases, we
k
have Xk = i=1 θik (Mi )Ω with θkk = αk2 and θik = θik−1 αk1 , for i < k. The detailed
procedure of this simplified method is given in Algorithm 2.
The proposed economic orthogonal rank-one matrix pursuit algorithm (EOR1MP)
uses the same amount of storage as the greedy algorithms [17, 42], which is significantly smaller than that required by our OR1MP algorithm, Algorithm 1. Interestingly, we can show that the EOR1MP algorithm is still convergent and retains the
linear convergence rate. The main result is given in the following theorem.
Theorem 4.1. Algorithm 2, the economic orthogonal rank-one matrix pursuit
algorithm satisfies
s
!k−1
1
1−
kY kΩ , ∀k ≥ 1.
||Rk || ≤
min(m, n)
Before proving Theorem 4.1, we present several useful properties of our Algorithm 2. The first property says that Rk+1 is perpendicular to matrix Xk−1 and
matrix Mk .
Property 4.2. hRk+1 , Xk−1 i = 0 and hRk+1 , Mk i = 0.
Proof. Recall that αk is the optimal solution of problem (4.1). By the first-order
optimality condition according to Xk−1 and Mk , one has
hY − αk1 Xk−1 − αk2 Mk , Xk−1 iΩ = 0,
and
hY − αk1 Xk−1 − αk2 Mk , Mk iΩ = 0,
which together with Rk = YΩ −Xk−1 implies that hRk+1 , Xk−1 i = 0 and hRk+1 , Mk i =
0.
Property 4.3. kRk+1 k2 = kYΩ k2 − kXk k2 for all k ≥ 1.
Proof. We observe that for all k ≥ 1,
kYΩ k2

=

kRk+1 + Xk k2

=

kRk+1 k2 + kXk k2 + 2hRk+1 , Xk i

=

kRk+1 k2 + kXk k2

11
as hRk+1 , Xk i = αk1 hRk+1 , Xk−1 i + αk2 hRk+1 , Mk i = 0, and hence the conclusion
holds.
The following property shows that as the number of rank-one basis matrices Mi
increases during our iterative process, the residual kRk k decreases.
Property 4.4. kRk+1 k ≤ kRk k for all k ≥ 1.
Proof. We observe that for all k ≥ 1,
kRk k2

=
=
≥
=

min kY − α1 Xk−2 − α2 Mk−1 k2Ω

α∈ℜ2

kY − (α1k−1 Xk−2 + α2k−1 Mk−1 )k2Ω
min kY − α1 (α1k−1 Xk−2 + α2k−1 Mk−1 ) − α2 Mk k2Ω

α∈ℜ2

min kY − α1 Xk−1 − α2 Mk k2Ω

α∈ℜ2

kRk+1 k2 ,

=

and hence the conclusion holds.
Let
Ak =

BTk Bk

"
#
hXk−1 , Xk−1 i hXk−1 , Mk i

=

hMk , Xk−1 i

hMk , Mk iΩ

T
and Bk = [vec(Xk−1 ), vec((Mk )Ω )]. The solution of problem (4.1) is αk = A−1
k Bk vec(YΩ ).
We next establish that vec(Xk−1 ) and vec((Mk )Ω ) are linearly independent unless
kRk k = 0. It follows that Ak is invertible and hence αk is uniquely defined before
the algorithm stops.
Property 4.5. If Xk−1 = β(Mk )Ω for some β 6= 0, then kRk+1 k = kRk k.
Proof. If Xk−1 = β(Mk )Ω with nonzero β, we get

kRk+1 k2

=
=

min kY − α1 Xk−1 − α2 Mk k2Ω

α∈ℜ2

min kY − (α1 + α2 /β)Xk−1 k2Ω

α∈ℜ2

=

min kY − γXk−1 k2Ω

=

min kY − γα1k−1 Xk−2 − γα2k−1 Mk−1 k2Ω

≥

γ∈ℜ

γ∈ℜ

min kY − γ1 Xk−2 − γ2 Mk−1 k2Ω

γ∈ℜ2

=

kY − Xk−1 k2Ω

=

kRk k2 .

and hence the conclusion holds with kRk k2 ≥ kRk+1 k2 given in Property 4.4.
Property 4.6. Let σ1 (Rk ) be the maximum singular value of Rk . hMk , Rk i =
for all k ≥ 1.
σ1 (Rk ) ≥ √ kRk k
min(m,n)

Proof. The optimum Mk in our algorithm satisfies
hMk , Rk i =

Using the fact that
the conclusion.

max

hM, Rk i = σ1 (Rk ).

rank(M)=1

p
rank(Rk )σ1 (Rk ) ≥ kRk k and rank(Rk ) ≤ min(m, n), we get

12

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion

Property 4.7. Suppose that Rk 6= 0 for some k ≥ 1. Then, Xk−1 6= β(Mk )Ω
for all β 6= 0.
Proof. If Xk−1 = β(Mk )Ω with β 6= 0, we have
kRk+1 k2

= kY − Xk k2Ω
=
=

min kY − α1 Xk−1 − α2 Mk k2Ω

α∈ℜ2

min kY − (α1 + α2 /β)Xk−1 k2Ω

α∈ℜ2

= min kY − γXk−1 k2Ω
γ∈ℜ

= kY − γ k Xk−1 k2Ω
= kRk k2
= kY − Xk−1 k2Ω .
As Rk 6= 0, we have (Mk )Ω 6= 0 and Xk−1 6= 0. Then from the above equality,
we conclude that γ k = 1 is the unique optimal solution of the minimization in terms
of γ, thus we obtain its first-order optimality condition: hXk−1 , Rk i = 0. However,
this contradicts with
hXk−1 , Rk i = βhMk , Rk i = βσ1 (Rk ) 6= 0.
The complete the proof.
We next build a relationship between two consecutive residuals kRk+1 k and kRk k.
σ2 (R )
Property 4.8. kRk+1 k2 ≤ kRk k2 − hM1k ,Mkk iΩ .
Proof.
kRk+1 k2

=
≤
=

min kY − α1 Xk−1 − α2 Mk k2Ω

α∈ℜ2

min kY − Xk−1 − α2 Mk k2Ω

α2 ∈ℜ

min kRk − α2 Mk k2Ω .

α2 ∈ℜ

This has a closed form solution as α∗2 =
into the formulation, we get
kRk+1 k2

hRk ,Mk i
hMk ,Mk iΩ .

Plugging this optimum α∗2 back

hRk ,Mk i
2
hMk ,Mk i Mk kΩ

≤

kRk −

=

kRk k2 −

hRk ,Mk i2
hMk ,Mk iΩ

=

kRk k2 −

σ12 (Rk )
hMk ,Mk iΩ .

This completes the proof.
We are now ready to prove Theorem 4.1.
Proof. [ of Theorem 4.1] Using the definition of Mk with its normalization property hMk , Mk iΩ ≤ 1, Property 4.8 and Property 4.6, we obtain that
||Rk+1 ||2

σ2 (R )

≤ ||Rk ||2 − hM1k ,Mkk iΩ ≤ ||Rk ||2 − σ12 (Rk )


1
||Rk ||2 .
≤
1 − min(m,n)

13
In view of this relation and the fact that kR1 k = kYk2Ω , we easily conclude that
s
!k−1
1
kYkΩ .
||Rk || ≤
1−
min(m, n)
This completes the proof.
5. An Extension to the Matrix Sensing Problem and Its Convergence
Analysis. In this section, we extend our algorithms to deal with the following matrix
sensing problem (cf. [36, 25, 18, 19]):
min

X∈ℜn×m

rank (X) : A(X) = A(Y),

(5.1)

where Y is a target low rank matrix and A is a linear operator, e.g., A consists
of vector pairs (fi , gi ), i = 1, · · · , d such that fi⊤ Xgi = fi⊤ Ygi , i = 1, · · · , d are given
constraints. Clearly, the matrix completion studied in the previous sections is a special
case of the above problem by setting the linear operator A to be the observation
operator PΩ .
Let us explain how to use our algorithms to solve this matrix sensing problem (5.1). Recall a linear operator vec which maps a matrix X of size n × m to
a vector vec(X) of size mn × 1. We now define an inverse operator matmn which
converts a vector v of size mn × 1 to a matrix V = matmn (v) of size n × m. Note that
when X is vectorized into vec(X), the linear operator A can be expressed in terms of
matrix A. That is, A(X) = A(Y) can be rewritten as A(vec(X)) = A(vec(Y)). For
convenience, we can write A = Avec. It is clear that A is a matrix of size d × mn.
Certainly, one can find its pseudo inverse A† which is A⊤ (AA⊤ )−1 as we have assumed that A is of full row rank. We note that since d << mn, AA† = Id while
A† A 6= Imn , where Id and Imn are the identity matrices of size d × d and mn × mn,
respectively. For convenience, we let A−1 = matmn (A† ) which satisfies
AA−1 b = b
for any vector b of size d × 1. We are now ready to tackle the matrix sensing problem (5.1) as follows: Let b = A(Y) = Avec(Y) and R0 := A−1 (b) be the given
matrix. We apply Algorithm 3 to obtain M(θ (k) ) in k ≥ r steps:
We shall show that M(θ (k) ) converges to the exact rank r matrix Y. First of all,
Algorithm 3 can be also proved to be linearly converged using the same procedure
as the proof of Theorem 3.1 in the main paper. We thus have the following theorem
without presenting the detail of proof.
Theorem 5.1. Each step in Algorithm 3 satisfies
s
!k−1
1
1−
||Rk ||F ≤
kA−1 (b)kF , ∀k ≥ 1.
min(m, n)
holds for all matrices X of rank at most r.
We now show M(θ (k) ) approximates the exact matrix Y as k large. In the setting
of matrix sensing, we are able to use the rank-RIP condition. Let us recall
Definition 5.2. Let A be a linear map on linear space of matrices of size m × n
with m ≤ n. For every integer r with 1 ≤ r ≤ m, let the rank r restricted isometry
constant be the smallest number δr (A) such that
(1 − δr (A))kXk2F ≤ kA(X)k22 ≤ (1 + δr (A))kXk2F

14

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion

Algorithm 3 Rank-One Matrix Pursuit for Matrix Sensing (R1MP4MS)
Input: R0 and stopping criterion.
Initialize: Set X0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the residual
matrix Rk by solving a least squares problem using power method and set Mk =
uk (vk )T .
Step 2: Compute the weight vector θ (k) using the closed form least squares
approximation of R0 using the best rank-one matrices Mi , i = 1, · · · , k:
θ (k) := min kR0 −
θ1 ,··· ,θk

k
X
i=1

θi A−1 A(Mi )k2F .

Pk
(k)
Step 3: Set M(θ (k) ) = i=1 θi Mi , Rk+1 = R0 − A−1 A(M(θ (k) )) and set
k ← k + 1.
until stopping criterion is satisfied
Output: the constructed matrix Ŷ = M(θ (k) ).
holds for all matrices X of rank at most r.
It is known that some random matries A = Avec satisfies the rank-RIP condition with high probability [36]. Armed with the rank-RIP condition, we are able to
establish the following convergence result:
Theorem 5.3. Let Y be a matrix of rank r. Suppose the measurement mapping
A(X) satisfies rank-RIP for rank-r0 with δr0 = δr0 (A) < 1 with r0 ≥ 2r. The output
matrix M(θ (k) ) from Algorithm 3 approximates the exact matrix Y in the following
sense: there is a positive constant τ < 1 such that
C
τ k,
kM(θ (k) ) − YkF ≤ p
1 − δr 0

for all k = 1, · · · , r0 − r, where C > 0 is a constant dependent on A.
Proof. Using the definition of δr0 , for k + r ≤ r0 , we have
(1 − δr0 )kM(θ k ) − Yk2F

≤ kA(M(θ k )) − A(Y)k22

= kA(Rk )k22 ≤ kAk22 kRk k2F ≤ kAk22 τ 2k kA−1 (b)k2F .
q
1
by using Theorem 5.1, where τ = 1 − min{m,n}
. It follows
kM(θ k ) − Yk2F ≤

kAk2 τ 2k
kA−1 (b)k2F .
1 − δr 0

Therefore, we have the desired result.
Similarly we can extend our economic algorithm to the setting of matrix sensing.
We leave it to the interested readers.
6. Effect of Inexact Top Singular Vectors. In our rank-one matrix pursuit
algorithms, we need to calculate the top singular vector pair of the residual matrix in
each iteration. We rewrite it here as
max{uT Rk v : kuk = kvk = 1}.
u,v

(6.1)

15
We solve this problem efficiently by the power method, which is an iterative method.
In practice, we obtain a solution with approximation error less than a small tolerance
δk ≥ 0, that is
ũT Rk ṽ ≥ (1 − δk )

{uT Rk v}.

max

kuk=kvk=1

(6.2)

We show that the proposed algorithms still retain the linear convergence rate when
the top singular pair computed at each iteration satisfies (6.2) for 0 ≤ δk < 1. This
result is given in the following theorem.
Theorem 6.1. Assume that there is a tolerance parameter 0 ≤ δ < 1, such that
δk ≤ δ for all k. Then the orthogonal rank-one matrix pursuit algorithms achieve a
linear convergence rate
s

q2
1−
min(m, n)

||Rk || ≤

!k−1

kYkΩ ,

where q = 1 − δ satisfies 0 < q ≤ 1.
Proof. In Step 1 of our algorithms, we iteratively solve the problem (6.1) using
the power method. In this method, we stop the iteration such that
ũTk Rk ṽk ≥ (1 − δk )

{uT Rk v} ≥ 0,

max

kuk=1,kvk=1

with 0 ≤ δk ≤ δ < 1. Denote M̃k = ũk ṽkT as the generated basis. Next, we show that
the following holds for both OR1MP and EOR1MP:
kRk+1 k2 ≤ kRk k2 − hM̃k , Rk i2 .
For OR1MP algorithm, we have
kRk+1 k2

=
≤
=

min kY −

θ∈ℜk

Pk

2
i=1 θi M̃i kΩ

min kY − Xk−1 − θk M̃k k2Ω

θk ∈ℜ

min kRk − θk M̃k k2Ω .

θk ∈ℜ

For EOR1MP algorithm, we have
kRk+1 k2

=
≤
=

min kY − α1 Xk−1 − α2 M̃k k2Ω

α∈ℜ2

min kY − Xk−1 − α2 M̃k k2Ω

α2 ∈ℜ

min kRk − α2 M̃k k2Ω .

α2 ∈ℜ

hRk ,M̃k i
. Plugging the optiIn both cases, we obtain closed form solutions as hM̃
k ,M̃k iΩ
mum solution into the corresponding formulations, we get

kRk+1 k2

hRk ,M̃k i
M̃k k2Ω
hM̃k ,M̃k i

≤

kRk −

=

kRk k2 −

≤

kRk k2 − hRk , M̃k i2 ,

hRk ,M̃k i2
hM̃k , M̃k iΩ
hM̃k ,M̃k i2Ω

16

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion

as hM̃k , M̃k iΩ ≤ 1. It follows from Property 4.5 and Property 4.6 that
kRk k
hRk , M̃k i ≥ (1 − δk )σ1 (Rk ) ≥ (1 − δk ) p
.
rank(Rk )

Combining the above two results, we get


(1 − δk )2
2
kRk+1 k ≤ 1 −
kRk k2 .
min(m, n)

In view of this relation and the fact that kR1 k = kYk2Ω , we conclude that
||Rk || ≤

s

q2
1−
min(m, n)

!k−1

kYkΩ ,

where q = 1 − δ ≤ inf(1 − δk ) = 1 − sup δk , and is a constant between (0, 1]. This
completes the proof.
7. Experiments. In this section, we compare the two versions of our algorithms,
e.g. OR1MP and EOR1MP, with several state-of-the-art matrix completion methods in the literature. The competing algorithms include: singular value projection
(SVP) [18], singular value thresholding (SVT) [7], Jaggi’s fast algorithm for trace
norm constraint (JS) [17], spectral regularization algorithm (SoftImpute) [30], low
rank matrix fitting (LMaFit) [46], boosting type accelerated matrix-norm penalized
solver (Boost) [49], atomic decomposition for minimum rank approximation (ADMiRA) [25] and greedy efficient component optimization (GECO) [38]. The first
three solve trace norm constrained problems; the next three solve trace norm penalized problems; the last two directly solves the low rank constrained problem. The
general greedy method [42] is not included in our comparison, as it includes JS and
GECO (included in our comparison) as special cases for matrix completion. The lifted
coordinate descent method (Lifted) [9] is not included in our comparison as it is very
sensitive to the parameters and is less efficient than Boost proposed in [49].
The code for most of these methods are available online:
• singular value projection (SVP):
http://www.cs.utexas.edu/∼pjain/svp/
• singular value thresholding (SVT):
http://svt.stanford.edu/
• spectral regularization algorithm (SoftImpute):
http://www-stat.stanford.edu/∼rahulm/software.html
• low rank matrix fitting (LMaFit):
http://lmafit.blogs.rice.edu/
• boosting type solver (Boost):
http://webdocs.cs.ualberta.ca/∼xinhua2/boosting.zip
• greedy efficient component optimization (GECO):
http://www.cs.huji.ac.il/∼shais/code/geco.zip
We compare these algorithms in two settings: one is image recovery and the other
one is collaborative filtering or recommendation problem. The data size for image
recovery is relatively small, and the recommendation problem is in large-scale. All the
competing methods are implemented in MATLAB1 and call some external packages
1 GECO

is written in C++ and we call its executable file in MATLAB.

17
Algorithm 4 Forward Rank-One Matrix Pursuit (FR1MP)
Input: YΩ and stopping criterion.
Initialize: Set X0 = 0, θ 0 = 0 and k = 1.
repeat
Step 1: Find a pair of top left and right singular vectors (uk , vk ) of the observed
residual matrix Rk = YΩ − Xk−1 and set Mk = uk (vk )T .
Step 2: Set θk = (uTk Rk vk )/kMk kΩ .
Step 3: Set Xk = Xk−1 + θk (Mk )Ω ; k ← k + 1.
until stopping criterion is satisfiedP
Output: Constructed matrix Ŷ = ki=1 θik Mi .
for fast computation of SVD2 and sparse matrix computations. The experiments are
run in a PC with WIN7 system, Intel 4 core 3.4 GHz CPU and 8G RAM.
In the following experiments, we follow the recommended settings of the parameters for competing algorithms. If no recommended parameter value is available, we
choose the best one from a candidate set using cross validation. For our OR1MP
and EOR1MP algorithms, we only need a stopping criterion. For simplicity, we stop
our algorithms after r iterations. In this way, we approximate the ground truth using a rank-r matrix. We present the experimental results using three metrics, peak
signal-to-noise ratio (PSNR) [16] and root-mean-square error (RMSE) [22]. PSNR is
a test metric specific for images. A higher value in PSNR generally indicates a better
quality [16]. RMSE is a general metric for prediction. It measures the approximation
error of the corresponding result.
7.1. Convergence and Efficiency. Before we present the numerical results
from these comparison experiments, we shall include another algorithm called the
forward rank-one matrix pursuit algorithm (FR1MP), which extends the matching
pursuit method from the vector case to the matrix case. The detailed procedure of
this method is given in Algorithm 4.
In FR1MP algorithm, we add the pursued rank-one matrix with an optimal weight
in each iteration, which is similar to the forward selection rule [14]. This is a standard
algorithm to find SVD of any matrix Y if all of its entries are given. In this case,
the FR1MP algorithm is more efficient in finding SVD of the matrix than our two
proposed algorithms. However, when only partial entries are known, the FR1MP
algorithm will not be able to find the best low rank solution. The computational step
to find θk in both proposed algorithms is necessary.
The empirical results for convergence efficiency of our proposed algorithms are
reported in Figure 7.1 and Figure 7.2. They are based on an image recovery experiment as well as an experiment of completing recommendation dataset, Netflix
[22, 4, 5]. The Netflix dataset has 108 ratings of 17,770 movies by 480,189 Netflix3
customers. This is a large-scale dataset, and most of the competing methods are not
applicable for this dataset. In Figure 7.1, we present the convergence characteristics
of the proposed OR1MP algorithm. As the memory demanded is increasing w.r.t.
the iterations, we can only run it for about 40 iterations on the Netflix dataset. The
EOR1MP algorithm has no such limitation. The results in Figure 7.2 show that our
2 PROPACK is used in SVP, SVT, SoftImpute and Boost. It is an efficient SVD package, which
is implemented in C and Fortran. It can be downloaded from http://soi.stanford.edu/~ rmunk/
PROPACK/
3 http://www.netflixprize.com/

18

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion

EOR1MP algorithm rapidly reduces the approximation error. We also present the
same residual curves in logarithmic scale with relatively large number of iterations in
Figure 7.3, which verify the linear convergence property of our algorithms, and this
is consistent with our theoretical analysis.
Lenna

Lenna
8
7
6

Time (seconds)

0.04
0.035

RMSE

0.03
0.025
0.02
0.015

5
4
3

0.01

2

0.005

1

0
0

100

200

0
0

300

50

100

150

200

250

300

350

rank

rank
Netflix

Netflix
8000

0.02

7000

Time (seconds)

RMSE

0.018
0.016
0.014
0.012

6000
5000
4000
3000
2000
1000

0.01
0

10

20

rank

30

40

0
0

10

20

30

40

rank

Fig. 7.1. Illustration of convergence of the proposed OR1MP algorithm on the Lenna image
and the Netflix dataset: the x-axis is the rank, the y-axis is the RMSE (left column), and the running
time is measured in seconds (right column).

In the convergence analysis, we derive the upper bound for the convergence speed
of our proposed algorithms. From Theorem 3.1 and Theorem 4.1, the convergence
2
speed is controlled by the value of kRk k2F /σk,∗
, where σk,∗ is the maximum singular
value of the residual matrix Rk in the k-th iteration. A smaller value indicates a
faster convergence of our algorithms. Though it has a worst case upper bound of
2
≤ rank(Rk ) ≤ min(m, n), in the following experiments, we empirically
kRk k2F /σk,∗
verify that its value is much smaller than the theoretical worst case. Thus the convergence speed of our algorithms is much better than the theoretical worst case. We
2
at different iterations on the Lenna image and the
present the values of kRk k2F /σk,∗
MovieLens1M dataset for both of our algorithms in Figure 7.4. The results show that
2
is much smaller than min(m, n).
the quantity kRk k2F /σk,∗
In the following experiments, we plot the residual curves over iterations for different rank-one matrix pursuit algorithms, including our OR1MP algorithm, our
EOR1MP algorithm and the forward rank-one matrix pursuit algorithm (FR1MP).
The evaluations are conducted on the Lenna image and the MovieLens1M dataset,
which are given in Figure 7.5. The results show that among the three algorithms,
EOR1MP and OR1MP perform better than the forward pursuit algorithm. It is interesting to note that EOR1MP achieves similar performance as OR1MP, while it
demands much less computational cost.

19
Lenna

Lenna

0.04

1
0.9

0.035

0.8
0.03
Time (seconds)

0.7
RMSE

0.025
0.02
0.015

0.6
0.5
0.4
0.3

0.01
0.2
0.005
0
0

0.1
50

100

150
200
Iteration

250

0
0

300

50

100

150
200
Iteration

4500

0.019

4000

0.018

3500
Time (seconds)

0.017
0.016
RMSE

300

Netflix

Netflix
0.02

0.015
0.014
0.013

3000
2500
2000
1500
1000

0.012

500

0.011
0.01
0

250

20

40

60
Iteration

80

0
0

100

20

40

60
Iteration

80

100

Fig. 7.2. Illustration of convergence of the proposed EOR1MP algorithm on the Lenna image
and the Netflix dataset: the x-axis is the rank, the y-axis is the RMSE (left column), and the running
time is measured in seconds (right column).

Netflix

Lenna

−1

RMSE

RMSE

10

−2

10

−2

10

−3

10

0

50

100

150

200

250

0

5

10

15

300

Lenna

−1

20

25

30

35

40

rank

rank

Netflix

RMSE

RMSE

10

−2

10

−3

10

0

−2

50

100

150

rank

200

250

300

10

20

40

60

80

100

rank

Fig. 7.3. Illustration of the linear convergence of different rank-one matrix pursuit algorithms
on the Lenna image and the MovieLen1M dataset: the x-axis is the iteration, the y-axis is the RMSE
in log scale. The curves in the first row are the results for OR1MP and the curves in the second
row are the results for EOR1MP.

20

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion
OR1MP on MovieLens1M

OR1MP on Lenna
5000
||R||2 / σ2

||R||2 / σ2

*

600

min(m,n)

*

min(m,n)

4000

500
3000

400
300

2000

200
1000
100
0
0

20

40
60
Iteration

80

0
0

100

EOR1MP on Lenna

10

20
30
Iteration

40

50

EOR1MP on MovieLens1M
5000
2

600

2

2

2

||R|| / σ*

||R|| / σ*

min(m,n)

min(m,n)

4000

500
3000

400
300

2000

200
1000
100
0
0

20

40
60
Iteration

80

0
0

100

10

20
30
Iteration

40

50

Fig. 7.4. Illustration of the values of kRk2 /σ∗2 at different iterations and the value of min(m, n)
on the Lenna image and MovieLens1M for both R1MP and ER1MP algorithms: the x-axis is the
iteration number; the y-axis is the value.
Lenna

MovieLen1M

0.04
OR1MP
EOR1MP
FR1MP

0.035

0.017

0.03

0.0165
0.016

0.025

RMSE

RMSE

OR1MP
EOR1MP
FR1MP

0.0175

0.02

0.0155
0.015
0.0145

0.015

0.014
0.01
0.005
0

0.0135
20

40

60
Iteration

80

100

0.013
0

20

40

60
Iteration

80

100

Fig. 7.5. Illustration of convergence speed of different rank-one matrix pursuit algorithms on
the Lenna image and the MovieLen1M dataset: the x-axis is the iteration, the y-axis is the RMSE.

7.2. Inexact Top Singular Vectors. We empirically analyze the performance
of our algorithms with inexact singular vector computation. In the experiments, we
control the total number of iterations in the power method for computing the top
singular vector pairs. The numbers of iterations are set as {1, 2, 5, 10, 20}. And we
plot the learning curves for OR1MP and EOR1MP algorithms on the MovieLen1M
dataset in Figure 7.6. The results show that the linear convergence speed is preserved
for different iteration numbers. However, the results under the same outer iterations
depend on the accuracy of the power methods. This verifies our theoretical results.
Our empirical results also suggest that in practice we need to run more than 5 iterations in the power method, as the learning curves for 5, 10 and 20 power method
iterations are close to each other but are far away from the other two curves, especially

21
for EOR1MP algorithm.
OR1MP on MovieLen1M

EOR1MP on MovieLen1M
iteration = 1
iteration = 2
iteration = 5
iteration = 10
iteration = 20

0.018

0.017
RMSE

RMSE

0.017

0.016

0.016

0.015

0.015

0.014

0.014

0.013
0

iteration = 1
iteration = 2
iteration = 5
iteration = 10
iteration = 20

0.018

20

40

60
Iteration

80

100

0.013
0

20

40

60
Iteration

80

100

Fig. 7.6. Illustration of convergence property of the proposed algorithms with different iteration
numbers in the power method on the MovieLen1M dataset: the x-axis is the outer iteration number;
the y-axis is the RMSE.

Table 7.1
Image recovery results measured in terms of the peak signal-to-noise ratio (PSNR).

Data Set
Barbara
Cameraman
Clown
Couple
Crowd
Girl
Goldhill
Lenna
Man
Peppers

SVT
26.9635
25.6273
28.5644
23.1765
26.9644
29.4688
28.3097
28.1832
27.0223
25.7202

SVP
25.2598
25.9444
19.0919
23.7974
22.2959
27.5461
16.1256
25.4586
25.3246
26.0223

SoftImpute
25.6073
26.7183
26.9788
26.1033
25.4135
27.7180
27.1516
26.7022
25.7912
26.8475

LMaFit
25.9589
24.8956
27.2748
25.8252
26.0662
27.4164
22.4485
23.2003
25.7417
27.3663

ADMiRA
23.3528
26.7645
25.7019
25.6260
24.0555
27.3640
26.5647
26.2371
24.5223
25.8934

JS
23.5322
24.6238
25.2690
24.4100
18.6562
26.1557
25.9706
24.5056
23.3060
24.0979

OR1MP
26.5314
27.8565
28.1963
27.0707
26.0535
30.0878
28.5646
28.0115
26.5829
28.0781

7.3. Image Recovery. In the image recovery experiments, we use the following
benchmark test images: Barbara, Cameraman, Clown, Couple, Crowd, Girl, Goldhill,
Lenna, Man, Peppers4 . The size of each image is 512 × 512. We randomly exclude
50% of the pixels in the image, and the remaining ones are used as the observations.
As the image matrix is not guaranteed to be low rank, we use the rank 50 for the
estimation matrix for each experiment. In our OR1MP and EOR1MP algorithms, we
stop the algorithms after 150 iterations. The JS algorithm does not explicitly control
the rank, thus we fix its number of iterations to 2000. The numerical results in terms
of the PSNR are listed in Table 7.1. We also present the images recovered by different
algorithms for Lenna in Figure 7.7. The results show SVT, our OR1MP and EOR1MP
achieve the best numerical performance. However, our algorithm is much better than
SVT for Cameraman, Couple, Peppers, but only slightly worse than SVT for Lenna,
Barbara and Clown. Besides, our algorithm is much faster and more stable than SVT
(SVT easily diverges). For each image, EOR1MP uses around 3.5 seconds, but SVT
consumes around 400 seconds. Image recovery needs a relatively higher approximation
4 Images

are downloaded from http://www.utdallas.edu/~ cxc123730/mh_bcs_spl.html

EOR1MP
26.4413
27.8283
28.2052
27.0310
26.0510
30.0565
28.5101
27.9643
26.5049
28.0723

22

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion

Original

SVT

SVP

SoftImpute

LMafit

ADMiRA

JS

OR1MP

EOR1MP

Fig. 7.7. The original image and images recovered by different methods on the Lenna image.

rank; both GECO and Boost fail to find a good recovery in most cases, so we do not
include them in the result tables.
Table 7.2
Characteristics of the recommendation datasets.

Data Set
Jester1
Jester2
Jester3
MovieLens100k
MovieLens1M
MovieLens10M

# row
24983
23500
24983
943
6040
69878

# column
100
100
100
1682
3706
10677

# rating
106
106
6×105
105
106
107

23
Table 7.3
The running time (measured in seconds). Boost fails on MovieLens10M.

Data Set
Jester1
Jester2
Jester3
MovieLens100K
MovieLens1M
MovieLens10M

SVP
18.3495
16.8519
16.5801
1.3237
18.9020
> 103

SoftImpute
161.4941
152.9600
1.5450
128.0658
59.5600
> 103

LMaFit
3.6756
2.4237
8.4513
2.7613
30.5475
154.3760

Boost
93.9142
261.7005
245.7895
2.8669
93.9142
–

JS
29.6751
28.5228
12.9441
2.8583
13.0972
130.1343

GECO
> 104
> 104
> 103
10.8300
> 104
> 105

OR1MP
1.8317
1.6769
0.9264
0.0418
0.8714
23.0513

EOR1MP
0.9924
0.9082
0.3415
0.0358
0.5397
13.7935

Table 7.4
Recommendation results measured in terms of the RMSE.

Data Set
Jester1
Jester2
Jester3
MovieLens100K
MovieLens1M
MovieLens10M

SVP
4.7311
4.7608
8.6958
0.9683
0.9085
0.8611

SoftImpute
5.1113
5.1646
5.4348
1.0354
0.8989
0.8534

LMaFit
4.7623
4.7500
9.4275
1.0238
0.9232
0.8971

Boost
5.1746
5.2319
5.3982
1.1244
1.0850
–

JS
4.4713
4.5102
4.6866
1.0146
1.0439
0.8728

GECO
4.3680
4.3967
5.1790
1.0243
0.9290
0.8668

OR1MP
4.3418
4.3649
4.9783
1.0168
0.9595
0.8621

EOR1MP
4.3384
4.3546
5.0145
1.0261
0.9462
0.8692

7.4. Recommendation. In the following experiments, we compare different
matrix completion algorithms using large recommendation datasets: Jester [12] and
MovieLens [31]. We use six datasets including: Jester1, Jester2, Jester3, MovieLens100K, MovieLens1M, and MovieLens10M. The statistics of these datasets are
given in Table 7.2. The Jester datasets were collected from a joke recommendation
system. They contain anonymous ratings of 100 jokes from the users. The ratings are
real values ranging from −10.00 to +10.00. The MovieLens datasets were collected
from the MovieLens website5 . They contain anonymous ratings of the movies on this
web made by its users. For MovieLens100K and MovieLens1M, there are 5 rating
scores (1–5), and for MovieLens10M there are 10 levels of scores with a step size 0.5
in the range of 0.5 to 5. In the following experiments, we randomly split the ratings
into training and test sets. Each set contains 50% of the ratings. We compare the
running time and the prediction result from different methods. In the experiments,
we use 100 iterations for the JS algorithm, and for other algorithms we use the same
rank for the estimated matrices; the values of the rank are {10, 10, 5, 10, 10, 20} for the
six corresponding datasets. We first show the running time of different methods in
Table 7.3. The reconstruction results in terms of the RMSE are given in Table 7.4. We
can observe from the above experiments that our EOR1MP algorithm is the fastest
among all competing methods to obtain satisfactory results.
8. Conclusion. In this paper, we propose an efficient and scalable low rank
matrix completion algorithm. The key idea is to extend orthogonal matching pursuit method from the vector case to the matrix case. We also propose a novel weight
updating rule under this framework to reduce the storage complexity and make it independent of the approximation rank. Our algorithms are computationally inexpensive
for each matrix pursuit iteration, and find satisfactory results in a few iterations. Another advantage of our proposed algorithms is they have only one tunable parameter,
5 http://movielens.umn.edu

24

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion

which is the rank. It is easy to understand and to use by the user. This becomes
especially important in large-scale learning problems. In addition, we rigorously show
that both algorithms achieve a linear convergence rate, which is significantly better
than the previous known results (a sub-linear convergence rate). We also empirically
compare the proposed algorithms with state-of-the-art matrix completion algorithms,
and our results show that the proposed algorithms are more efficient than competing algorithms while achieving similar or better prediction performance. We plan to
generalize our theoretical and empirical analysis to other loss functions in the future.
Appendix A. Inverse Matrix Update. In our OR1MP algorithm, we use the
least squares solution to update the weights for the rank-one matrix bases. In this step,
we need to calculate (M̄k M̄k )−1 . To directly compute this inverse is computationally
expensive, as the matrix M̄k has large row size. We implement this efficiently using
an incremental method. As
M̄Tk M̄k = [M̄k−1 , ṁk ]T [M̄k−1 , ṁk ],
its inverse can be written in block matrix form
 T
M̄k−1 M̄k−1
T
−1
(M̄k M̄k ) =
ṁTk M̄Tk−1

M̄Tk−1 ṁk
ṁTk ṁk

−1

.

Then it is calculated by blockwise inversion as


A + dAbbT A −dAb
−dbT A
d
where A = (M̄Tk−1 M̄k−1 )−1 is the corresponding inverse matrix in the last step, b =
M̄Tk−1 ṁk is a vector with |Ω| elements, and d = (bT b−bT Ab)−1 = 1/(bT b−bT Ab)
is a scalar.
M̄Tk ẏ is also calculated incrementally by [M̄Tk−1 ẏ, ṁTk ẏ], as ẏ is fixed.
REFERENCES
[1] A. Argyriou, T. Evgeniou, and M. Pontil, Convex multi-task feature learning, Machine
Learning, 73 (2008), pp. 243–272.
[2] F. Bach, Consistency of trace norm minimization, Journal of Machine Learning Research, 9
(2008), pp. 1019–1048.
[3] L. Balzano, R. Nowak, and B. Recht, Online identification and tracking of subspaces from
highly incomplete information, in Proceedings of the Allerton Conference on Communication, Control and Computing, 2010.
[4] R. Bell and Y. Koren, Lessons from the netflix prize challenge, SIGKDD Explorations, 9
(2007).
[5] J. Bennett and S. Lanning, The netflix prize, in Proceedings of KDD Cup and Workshop,
2007.
[6] J.-F. Cai, E. J. Candès, and Z. Shen, A singular value thresholding algorithm for matrix
completion, SIAM Journal on Optimization, 20 (2010), pp. 1956–1982.
[7] E. J. Candès and B. Recht, Exact matrix completion via convex optimization, Foundations
of Computational Mathematics, 9 (2009), pp. 717–772.
[8] R. A. DeVore and V. N. Temlyakov, Some remarks on greedy algorithms, Advances in
computational Mathematics, 5 (1996), pp. 173–187.
[9] M. Dudı́k, Z. Harchaoui, and J. Malick, Lifted coordinate descent for learning with tracenorm regularization, in Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS), 2012.
[10] M. Frank and P. Wolfe, An algorithm for quadratic programming, Naval Research Logistics
Quarterly, 3 (1956), pp. 95–110.

25
[11] J. H. Friedman, T. Hastie, and R. Tibshirani, Regularization paths for generalized linear
models via coordinate descent, Journal of Statistical Software, 33 (2010), pp. 1–22.
[12] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins, Eigentaste: A constant time collaborative filtering algorithm, Information Retrieval, 4 (2001), pp. 133–151.
[13] G. H. Golub and C. F. V. Loan, Matrix computations (3rd ed.), The Johns Hopkins University
Press, 1996.
[14] T. Hastie, R. Tibshirani, and J. H. Friedman, The elements of statistical learning: data
mining, inference, and prediction, New York: Springer-Verlag, 2009.
[15] E. Hazan, Sparse approximate solutions to semidefinite programs, in Proceedings of the 8th
Latin American conference on Theoretical informatics, 2008.
[16] Q. Huynh-Thu and M. Ghanbari, Scope of validity of psnr in image/video quality assessment,
Electronics Letters, 44 (2008), pp. 800–801.
[17] M. Jaggi and M. Sulovský, A simple algorithm for nuclear norm regularized problems, in
Proceedings of the 27th International Conference on Machine Learning (ICML), 2010,
pp. 471–478.
[18] P. Jain, R. Meka, and I. S. Dhillon, Guaranteed rank minimization via singular value projection, in Advances in Neural Information Processing Systems (NIPS) 22, 2010, pp. 937–945.
[19] P. Jain, P. Netrapalli, and S. Sanghavi, Low-rank matrix completion using alternating
minimization, in Proceedings of the 45th Annual ACM Symposium on Symposium on
Theory of Computing (STOC), 2013, pp. 665–674.
[20] S. Ji and J. Ye, An accelerated gradient method for trace norm minimization, in Proceedings
of the 26th International Conference on Machine Learning (ICML), 2009, pp. 457–464.
[21] R. Keshavan and S. Oh, Optspace: A gradient descent algorithm on grassmann manifold for
matrix completion, http://arxiv.org/abs/0910.5260, (2009).
[22] Y. Koren, Factorization meets the neighborhood: a multifaceted collaborative filtering model,
in Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery
and data mining (KDD), 2008.
[23] Y. Koren, R. Bell, and C. Volinsky, Matrix factorization techniques for recommender
systems, Computer, (2009).
[24] M.-J. Lai, Y. Xu, and W. Yin, Improved iteratively reweighted least squares for unconstrained
smoothed ℓq minimization,, SIAM Journal on Numerical Analysis, (2012).
[25] K. Lee and Y. Bresler, Admira: atomic decomposition for minimum rank approximation,
IEEE Transactions on Information Theory, 56 (2010), pp. 4402–4416.
[26] E. Liu and T. N. Temlyakov, The orthogonal super greedy algorithm and applications in
compressed sensing, IEEE Transactions on Information Theory, 58 (2012), pp. 2040–2047.
[27] Y.-J. Liu, D. Sun, and K.-C. Toh, An implementable proximal point algorithmic framework
for nuclear norm minimization, Mathematical Programming, 133 (2012), pp. 399–436.
[28] Z. Lu and Y. Zhang, Penalty decomposition methods for rank minimization,
http://arxiv.org/abs/0910.5260, (2010).
[29] S. Ma, D. Goldfarb, and L. Chen, Fixed point and bregman iterative methods for matrix
rank minimization, Mathematical Programming, 128 (2011), pp. 321–353.
[30] R. Mazumder, T. Hastie, and R. Tibshirani, Spectral regularization algorithms for learning
large incomplete matrices, Journal of Machine Learning Research, 99 (2010), pp. 2287–
2322.
[31] B. N. Miller, I. Albert, S. K. Lam, J. A. Konstan, and J. Riedl, Movielens unplugged:
experiences with an occasionally connected recommender system, in Proceedings of the 8th
international conference on Intelligent user interfaces, 2003, pp. 263–266.
[32] B. Mishra, G. Meyer, F. Bach, and R. Sepulchre, Low-rank optimization with trace norm
penalty, http://arxiv.org/abs/1112.2318, (2011).
[33] D. Needell and J. A. Tropp, Cosamp: iterative signal recovery from incomplete and inaccurate samples, Communications of the ACM, 53 (2010), pp. 93–100.
[34] S. Negahban and M. Wainwright, Estimation of (near) low-rank matrices with noise and
high-dimensional scaling, in Proceedings of the 27th International Conference on Machine
Learning (ICML), 2010.
[35] Y. C. Pati, R. Rezaiifar, Y. C. P. R. Rezaiifar, and P. S. Krishnaprasad, Orthogonal
matching pursuit: Recursive function approximation with applications to wavelet decomposition, in Proceedings of the 27th Annual Asilomar Conference on Signals, Systems, and
Computers, 1993, pp. 40–44.
[36] B. Recht, M. Fazel, and P. A. Parrilo, Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization, SIAM Review, 52 (2010), pp. 471–501.
[37] B. Recht and C. Ré, Parallel stochastic gradient algorithms for large-scale matrix completion,
Mathematical Programming Computation, 5 (2013), pp. 201–226.

26

Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion

[38] S. Shalev-Shwartz, A. Gonen, and O. Shamir, Large-scale convex minimization with a lowrank constraint, in Proceedings of the 28th International Conference on Machine Learning
(ICML), 2011, pp. 329–336.
[39] S. Shalev-Shwartz and A. Tewari, Stochastic methods for l1 regularized loss minimization,
in Proceedings of the 26th International Conference on Machine Learning (ICML), 2009,
pp. 929–936.
[40] N. Srebro, J. Rennie, and T. Jaakkola, Maximum margin matrix factorizations, in Advances in Neural Information Processing Systems (NIPS) 17, 2005.
[41] V. N. Temlyakov, Greedy approximation, Acta Numerica, 17 (2008), pp. 235–409.
[42] A. Tewari, P. Ravikumar, and I. S. Dhillon, Greedy algorithms for structurally constrained
high dimensional problems, in Advances in Neural Information Processing Systems (NIPS)
23, 2011.
[43] R. Tibshirani, Regression shrinkage and selection via the lasso, Journal of the Royal Statistical
Society, Series B, 58 (1994), pp. 267–288.
[44] K.-C. Toh and S. Yun, An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems, Pacific Journal of Optimization, 6 (2010), pp. 615 – 640.
[45] J. A. Tropp, Greed is good: algorithmic results for sparse approximation, IEEE Trans. Inform.
Theory, 50 (2004), pp. 2231–2242.
[46] Z. Wen, W. Yin, and Y. Zhang, Low-rank factorization model for matrix completion by a
non-linear successive over-relaxation algorithm, Rice CAAM Tech Report 10-07, University
of Rice, 2010.
[47] T. T. Wu and K. Lange, Coordinate descent algorithms for lasso penalized regression, Annals
of Applied Statistics, 2 (2008), pp. 224–244.
[48] S. Yun and K.-C. Toh, A coordinate gradient descent method for l1-regularized convex minimization, Computational Optimization and Applications, (2011).
[49] X. Zhang, Y. Yu, and D. Schuurmans, Accelerated training for matrix-norm regularization:
A boosting approach, in Advances in Neural Information Processing Systems (NIPS) 24,
2012.

Exploiting Agent and Database Technologies for
Biological Data Collection
Hasan Davulcu, Zoé Lacroix, Kaushal Parekh
Arizona State University

I. V. Ramakrishnan, Nikeeta Julasana
State University of New York – Stony Brook

{HasanDavulcu, zoe.lacroix, kaushal}@asu.edu

{ram, Nikeeta}@cs.sunysb.edu

Abstract
Web data sources constitute an important
resource for Biological research. A simple tool
that can retrieve information from different Web
sites through a single interface and store the
extracted data in a standardized format for
efficient future use is critical to scientific
discovery. In this paper we discuss an approach
that combines agent and database technologies
for biological data integration. To illustrate this,
we employ two software tools: WinAgent, for
building agents, and dbXML, for XML data
management. WinAgent learns from its users by
recording a browsing session on Web sites and
successive data extraction from regions of
interest on retrieved Web pages. The results are
stored in a XML document and can be managed,
queried and updated using a native XML database
system such as dbXML. This approach is
currently being evaluated at the Brain Tumor
Cancer Unit of the Translational Genomics
Research Institute (TGen), Phoenix, Arizona.

explore and control their data collection process
step by step. However, most of the process could
be automated. First the scientist designs the data
collection protocol that is a succession of steps
including Web data source selection, data
extraction, and selection of links between retrieved
entries to follow. When the data collection steps
are designed, the process of data collection
consists in repeating the designed paths to collect
successively similar data following the same
protocol. The last phase of data collection consists
of the integration the retrieved data.
We propose to use agents combined with
database technology to support biological data
collection. Our proposed approach improves
significantly current data collection processes by:
• Allowing scientists to design their own data
collection protocols as they are used to
(browsing the Web resources);
• Automating the data collection process
itself by exploiting the WinAgent tool; and
• Storing retrieved data in a database to
allow
further
transformations
and
integrations.

2. WinAgent specifications
1. Introduction
The Internet constitutes a major resource for
biologists. Web sites such as the National Center
for Biotechnology Information (NCBI) provide
various biological repositories and applications.
Scientists spend a lot of time and energy
searching the Web for useful resources,
accessing resources adequate to their needs,
extracting information of interest, and filtering
relevant data from multiple heterogeneous Web
sites to support the data collection needs for their
complex scientific pipelines. This tedious collection
process is typically performed manually as
available technology does not allow scientists to

WinAgent can be considered as a Personal
Information Assistant (PIA) [1] or a Software Robot
that browses the Web for the user and extracts
desired information from respective Web sites.
Personal, because it provides the users the
flexibility to design different agents depending on
their information needs. The agent needs to “learn”
the navigation path to extract the data of interest. It
does this by recording the actions of the user
browsing through Web pages, filling up forms with
keywords and highlighting text regions. Once the
recording is done, the agent can be executed
repeatedly to perform the same actions for
different data sources or different keywords. The

Proceedings of the 15th International Workshop on Database and Expert Systems Applications (DEXA’04)
1529-4188/04 $ 20.00 IEEE

agent then saves the results in a XML document.
Before discussing the details of the agent building
process and the learning mechanism, it is
important to discuss the requirements of a
successful biological data collection system and
how WinAgent’s specifications meet these
requirements.

2.1. User profile
In general, scientists or biologists are not
programmers. A recent survey [2] indicated that
only about a tenth of the biologists using computer
systems could actually program themselves. Hence
we can expect a large proportion of the user profile
to include people with only minimal programming
skills. Also scientists will not spend too much time
learning new software. WinAgent is ideal in this
respect. It sits as an add-on toolbar (Fig. 1) in the
Internet browser and has buttons with intuitive
functions (e.g., Record, Play, etc.) that make it
simple and easy to use. WinAgent empowers the
biologists to create customized agents without
having to code at all.

Figure 1. WinAgent Toolbar in IE
Another challenge is that a biologist’s data
requirements change very frequently while the data
sources also change quite often. Frequent
maintenance of data collection systems will require
either excellent programming skills or external IT
support. But neither can guarantee that the new
version will be available on time before the
requirements or sources change again. WinAgent
is well suited for this task as it learns directly from
the user and provides the flexibility to immediately
adjust to the new situation by discarding the old
agent and creating a new one.

2.2. Desired functionality
It is also important to discuss what functionality
biologists expect. These include the ability to:

1. Design the collection process by browsing
manually, selecting the Web resources,
extracting the data of interest, and following
the links;
2. Reproduce the data collection process on
similar collection queries; and
3. Store retrieved data in a repository that
allow further querying, transformation,
integration, and annotation.
The first functionality requires the approach to
mimic manual browsing, implementing various
navigational and extraction queries. Such queries
can be quite complex, spanning different levels
within a Web site, or jumping from one Web
source to another, thus integrating multiple
heterogeneous resources. Consider the following
“simple” case: A scientist may chose to search
OMIM1 for genes related to a disease and then use
the references in each record to reach PubMed2
citations related to those diseases. Such a
biological data collection scenario involves two
Web resources OMIM and PubMed, but a more
complex one may involve many intermediate
resources to collect citations linked to specific
diseases [3]. A Perl script could automate such a
data collection protocol, but browsing the Web site
is much simpler and more intuitive than writing a
script. In contrast, the WinAgent tool allows users
to build the collection agent by recording the
designed process as it is performed manually, thus
avoiding programming. When an agent is built, it
can be reused as long as the resources involved in
the data collection protocol have not changed
dramatically. This is because WinAgent uses a
semi-structured approach i.e., the data does not
follow a strict format, thus if certain data items from
a Web site are missing or new ones are added, it
is still successfully able to collect the remaining, as
long as the overall structure remains the same.
The third requirement involves several aspects
of data management. Collected data need to be
stored in a repository for curation and future use.
Also scientists need to integrate data collected with
different protocols, they need to annotate their data
and query them exploiting the full range of their
complexity (thus with a query language rather than
a full-text search). Database technology fulfills
1

OMIM™ is the Online Mendelian Inheritance in Man authored
and edited by Dr. Victor A. McKusick and his colleagues at
Johns Hopkins available at http://www.ncbi.nlm.nih.gov/
2
PubMed is a service of the National Library of Medicine
available at http://www.ncbi.nlm.nih.gov/

Proceedings of the 15th International Workshop on Database and Expert Systems Applications (DEXA’04)
1529-4188/04 $ 20.00 IEEE

these needs but it suffers from drawbacks that
scientists do not “speak DB”, that is they have
difficulties to express their complex scientific
pipelines with queries in SQL, CPL, etc. [4] and
that adjusting to changes involves a significant
delay. Using WinAgent with a database would
solve this problem. As WinAgent collects data in
XML format, we propose to use a XML-native
database to store retrieved data.

2.3. Technical issues
Platform independence plays a major role in the
success of any software tool. Currently, WinAgent
requires Microsoft Internet Explorer 6 on a
Windows Operating System and hence is highly
platform dependant. But with increased use, it can
be ported to other platforms and browsers.
The system should be easily scalable to handle
large amounts, types and sources of data, which is
a characteristic of biomedical information. As
noted before, WinAgent can handle any data that
are available on a Web site. Unlike most data
collection systems, the approach offers great
flexibility to address changes in existing sources
and only a small effort is needed to create a new
agent.
Another important issue is the efficiency with
respect to data storage, communication overhead
and data integration overhead. The user will usually
query the local copy of the collected data hence
there is no communication overhead except when
the database is updated with the latest information
or it is being populated for the first time. The
database management component also takes care
of the other requirements.

3. Motivation example

After installing the WinAgent toolbar, the first
step is to visit the NCBI Web site
(www.ncbi.nlm.nih.gov) and press the ‘Record’
button on the WinAgent toolbar. WinAgent
monitors and records the user’s actions during the
period between ‘Record’ and ‘Stop’ events. Next
the user searches OMIM for a disease condition
(say ‘brain tumor’) and clicks on ‘Go’ to begin the
search.
Because a form was filled, the form processing
wizard pops up (Fig. 2) and the user has to set the
type of the form entries as constants or variables.
Here the user wishes to use the agent later with
other keywords (thus ‘term’ is selected as a
variable), but always starting from the data source
OMIM (‘db’ is assigned the constant value ‘OMIM’).
Clicking on ‘Submit’ takes the user to the OMIM
results page (The option ‘Ignore’ goes directly to
the results page despite the form, which is useful
in some cases).

Figure 2. Setting form element types
The user then selects a sample of OMIM
records, to show the region of interest from which
data is to be extracted (Fig. 3) and clicks on ‘Get
Region’. Next he highlights a single record and
presses the ‘Get Item’ button to show the agent
what types of items are to be retrieved from the
region.

We illustrate our approach navigating from
OMIM to PubMed. The task is to perform a
keyword search on OMIM at the NCBI Web site,
follow the links for each gene record to get to the
detailed description and then extract PubMed
citations from the references. The next section
briefly describes the agent recording process. For
a
video
demonstration,
please
visit
http://bioinformatics.eas.asu.edu/winagent.htm.

3.1 Creating the agent

Figure 3. Region selection using ‘Get Region’

Proceedings of the 15th International Workshop on Database and Expert Systems Applications (DEXA’04)
1529-4188/04 $ 20.00 IEEE

PubMed Ids from the references of each OMIM
record. This is done by a similar procedure of ‘Get
Region’ on the references (Fig. 6), ‘Get Item’ on
an individual PubMed Id and then making
appropriate attribute selection. The agent has now
been shown the data extraction protocol and can
‘Stop’. It is then saved in a file with a ‘.agt’
extension.

3.2 Executing the agent
Figure 4. Selecting attributes of interest
The agent shows all the records broken up into
their attributes (Fig. 4). One has to select and
rename only the desired attributes in each record
and drop the rest. To follow hyperlinks (e.g.,
OMIMid) to the next level, the ‘Follow Link’ option
for that attribute has to be selected.

Agents can be run using the ‘Play’ button. The
user selects the input file containing the previously
saved agent and creates a new XML file for saving
the results.

Figure 7. Input the variable form element
Figure 5. Confirm results of selection
On pressing ‘Submit’, WinAgent shows the
results (Fig. 5) of these selections for confirmation.
Clicking on ‘Close’ accepts the results. One can
also ‘Cancel’ and repeat the process again if the
results are not as desired.

The Agent checks if it has to fill any forms with
variable elements while navigating. If so, it will ask
the user for the values of those variables, in this
case, the name of the disease condition (Fig. 7).
Automatic browsing begins on pressing ‘Submit’.

Figure 8. XML output displayed

Figure 6. Selecting references for ‘Get Region’
The next step is to navigate a level deeper by
following the links in OMIM Ids and then retrieve

When the agent is done visiting the appropriate
data sources and extracting information, the
collected data is saved and presented in the XML
format (Fig. 8). The same procedure can be
repeated for different keywords (diseases). This
can also be done automatically by specifying an
input text file containing disease names from which

Proceedings of the 15th International Workshop on Database and Expert Systems Applications (DEXA’04)
1529-4188/04 $ 20.00 IEEE

WinAgent can retrieve keywords. The results for all
these keywords are stored in a single output file.
This has an advantage over typing in keywords
each time and managing multiple output files for
each keyword.

4. WinAgent learning algorithm
This section provides a brief description of the
most important part of the Personal Information
Assistant, the algorithm that extracts information
from the Web pages. The learning algorithm,
which is explained in more detail in [5], uses the
DOM (Document Object Model) tree of the HTML
Web page and XPath in the DOM to reach the
region of interest. Elements in HTML form nodes
and leaves of the DOM tree. The next few
paragraphs explain how the XPath query of the
desired object is determined.
When the user specifies the smallest region
containing all items of interest using ‘Get Region’,
a ‘regionLCA’ (Least Common Ancestor of all the
HTML nodes in the selected region) is identified in
the DOM tree. This means that all items of interest
are members of the subtree rooted at regionLCA.
Similarly, based on the highlighted region for ‘Get
Item’, the itemLCA is identified. Next, the algorithm
learns or constructs two XPath expressions, the
isolatorXPath that identifies all itemLCA nodes and
attributeXPath that identifies all attributes of an
item.
In the first phase of the algorithm, the
isolatorXPath is constructed iteratively. Initially the
isolatorXPath is set to be the itemLCA node. Now
there will be similar nodes throughout the DOM tree
of the HTML. The isolatorXPath is then specialized
by incorporating attributes or attribute value pairs
of the itemLCA into the isolatorXPath, which
eliminates some nodes outside the subtree under
regionLCA. This process is repeated till all the
nodes outside the subtree are eliminated.
In the second phase, the attributeXPath(s) are
generated to retrieve all attributes of the item of
interest. From these XPaths a site navigation map
is constructed which itself is in the XML format.
This is actually the ‘.agt’ file recorded (Fig. 9). The
map determines WinAgent’s navigation of the Web
sites and the information extracted.

Figure 9. Part of the map for OMIM example

5. XML Database Integration
Scientists should be able to easily query,
update, modify or combine the data they collected
using WinAgent. Databases are perfectly suited
for this task. It thus becomes necessary to
integrate XML documents with a DBMS or
transform the XML documents into a relational
database so that they can be managed and
queried in the same way as databases.
The XPath and XQuery XML query languages
attempt to provide similar functionality for XML as
SQL does for relational databases. Native XML
database systems, which can be used to manage
XML data, can use these languages to query XML
documents. We use dbXML [6], which is an open
source native XML database system, for this
purpose. The system features include an XPath
query engine, XUpdate implementation for updates
and an API implementation for developing
applications. An example of an XPath query
expressed against the data stored in dbXML is
given below. The simple query gets all PubMed Id’s
that were cited in the OMIM record ‘*601607’ from
the XML file generated by the agent described
previously.
<dbxml:xpath xmlns:dbxml="http://www.dbxml.com/db/query">
//Object2[OMIMid="*601607"]/Object3/PMid
</dbxml:xpath>

Compared to XQuery, XPath can be used to
express queries of only a limited complexity but
dbXML currently supports only XPath. Future
versions of dbXML are expected to include support
for the more sophisticated XQuery standard.
Similar queries can be written using XUpdate to
update the XML database.

Proceedings of the 15th International Workshop on Database and Expert Systems Applications (DEXA’04)
1529-4188/04 $ 20.00 IEEE

6. Advantages
Several biological data integration approaches
have been discussed in [7]. On comparison,
WinAgent has many advantages when used as a
data integration tool; some of the important ones
are outlined below.
• The first advantage is its simplicity. The fact
that it is just a toolbar in an Internet browser
makes it very attractive and easy to use. The
only computer skills required are the ability to
browse Web sites and highlight regions of
interest with the mouse;
• The output is highly personalized;
• The output is in XML, a standard and a
versatile format, which makes it easy to
exchange and integrate with existing data
sources;
• For adapting to modifications in existing
sources or incorporating new ones, one just
needs to create a new agent for the Web site.
Compare this to learning the new API and
modifying the original scripts or modules of the
existing system.

7. Conclusions and Future work
WinAgent combined with a XML native database
shows great promise as a Web biological
information collection and integration tool. In the
future, we will couple our approach with existing
relational databases as laboratories typically use
systems such as IBM DB2 Information Integrator
(formerly known as DiscoveryLink) or SYBASE to
store their data. Accessing XML data from a
relational database has its own advantage. Many
scientists use relational databases to efficiently
manage their data. Currently many relational
database products support XML based on the
SQL/XML standard. For example, IBM’s
Information Integrator comes with a relational
wrapper for XML that allows users to query XML
documents using SQL just like a relational table.
Once the XML data have been wrapped into virtual
tables, operations like updates and joins become
possible. The increasing support for XML in most
data management systems makes the XML format
for output in WinAgent a wise choice.
Previous research [4] has shown that scientists
cannot be expected to use and query XML
databases as described in Section 5. Future work

will also include a scientist-friendly querying
interface that enables scientists to express their
complex data collection protocols with a workflowlike interface and exploit combinations of agents
and XML queries.
WinAgent is currently limited to textual data. A
large amount of biological information is also
graphical in nature such as genome maps, gene
expression patterns, linkage maps, etc. Hence
WinAgent needs to be adapted to be able to
extract information from such complex data
sources.

8. Acknowledgements
We greatly appreciate the cooperation of
Michael Berens, and his team, the Brain Tumor
Cancer Unit at TGen. In particular Anna Joy,
Dominique Hoelzinger, Jessica Rennert, and Nhan
Tran are thanked for their enthusiasm testing
WinAgent.

9. References
[1] N. Julasana, A. Khandelwal, A. Lolage, P. Singh, P.
Vasudevan, H. Davulcu and I. V. Ramakrishnan, “WinAgent: A
System for Creating and Executing Personal Information
Assistants Using a Web Browser”, 9th International Conference
on Intelligent User Interfaces, Madeira, Portugal, 2004, pp 356357.
[2] Z. Lacroix and S. Ganta, “Analysis of Scientific Tasks: How
do Scientists exploit Scientific Resources”, Technical Report,
Scientific Data Management Lab, Arizona State University,
2004.
[3] Z. Lacroix, H. Murthy, F. Naumann, and L. Raschid, “Links
and Paths through Life Science Data Sources'', First
International Workshop on Data Integration in the Life Sciences,
Leipzig, Germany, March 2004, pp 203-211.
[4] B. Eckman, K. Deutsch, M. Janer, Z. Lacroix and L.
Raschid, “A query language to support scientific discovery”, The
Second International IEEE Computer Society Computational
Systems Bioinformatics Conference, Stanford, California,
August 2003, pp 388-390.
[5] N. Julasana, A. Khandelwal, A. Lolage, P. Singh, P.
Vasudevan, H. Davulcu and I. V. Ramakrishnan, “Creating
personal Information Assistants for Targeted Navigation and
Extraction via a Web Browser”, Technical Report
(www.lmc.cs.sunysb.edu/~winagent/WinAgentPaper.pdf)
[6] dbXML Native XML Database (http://www.dbxml.com)
[7] Z. Lacroix, T. Critchlow, “Bioinformatics: Managing Scientific
Data”, Morgan Kaufmann Publishers (ISBN: 1-55860-829-X),
July 2003.

Proceedings of the 15th International Workshop on Database and Expert Systems Applications (DEXA’04)
1529-4188/04 $ 20.00 IEEE

2011 European Intelligence and Security Informatics Conference

A System for Ranking Organizations using Social
Scale Analysis
Inayah Rochmaniyah, Ali Amin

Sukru Tikves, Sujogya Banerjee, Hamy Temkit, Sedat Gokalp,
Hasan Davulcu, Arunaba Sen, Steven Corman, Mark Woodward

Center for Religious and Cross Cultural Studies
Gadjah Mada University, Yogyakarta, Indonesia

Arizona State University, P.O. Box 87-8809, Tempe, AZ, 85281 USA

{rochmaniyah, aleejtr77}@yahoo.com

{stikves, sujogya, mtemkit, sgokalp, hdavulcu, asen, scorman, mataram}@asu.edu
Tel:(602) 206-8641, Fax: (480) 965-2751

Abstract—In this paper we utilize feature extraction and model
fitting techniques to process the rhetoric found in the web sites
of 23 Indonesian religious organizations – comprising a total of
37,000 articles dating from 2005 to 2011 – to profile their ideology
and activity patterns along a hypothesized radical/counter-radical
scale. We rank these organizations by assigning them to probable
positions on the scale. We show that the developed Rasch model
fits the data using Andersen’s LR-test. We create a gold standard
of the ranking of these organizations through an expertise
elicitation tool. We compute expert-to-expert agreements, and
we present experimental results comparing the performance of
three different baseline methods to show that the Rasch model
not only outperforms our baseline methods, but it is also the only
system that performs at expert-level accuracy.

radical organizations into pure clusters. Pure radical clusters
were easily identified due to high similarity among their
support for violent practices. Pure counter-radical clusters
were identified due to their strong reactionary opposition to
violent practices through protests and rhetoric. But the rest of
the groupings were mixed. We realized that binary labeling
as counter-radical or radical does not capture the overlap,
movement and interactivity among these organizations. In this
paper we hypothesize that both counter-radical and radical
movements in Muslim societies exhibit distinct combinations
of discrete states comprising various social, political, and
religious beliefs, attitudes and practices, that can be mapped
to a latent linear continuum or a scale. Using such a scale, an
analyst can determine where exactly along the spectrum any
particular group lies, and also potentially where it is heading
with its rhetoric and activity.

I. I NTRODUCTION
Being able to asses information on radical and moderate actors in a geographic area is an important research topic for our
national security. Radicalism is the ideological conviction that
it is acceptable and in some cases obligatory to use violence to
effect profound political, cultural and religious transformations
and change the existing social order fundamentally. Muslim
radical movements have complex origins and depend on diverse factors that enable translation of their radical ideology
into social, political and religious movements. In [1] Crelinsten
states that “both violence and terrorism possess a logic and
grammar that must be understood if we are to prevent or control them”. Therefore, analysis of Muslim radical and counterradical movements requires attention to the global, national
and local social, economic and political contexts in which
they are located. Similarly, in the Islamic context, counterradical discourse takes various different forms; discursive and
narrative refutations of extremist claims, symbolic action such
as ritual and other religious and cultural practices, and Islamic
arguments for pluralism, peaceful relations with non-Muslims,
democracy, etc. The most effective counter-radicals are likely
to be religiously conservative Muslims. Effective containment
and defeat of radicalism depends on our ability to recognize
various levels of radicalization, and detection of counterradical voices.
In our previous work [2], we attempted a clustering approach to obtain “natural groupings” of a number of local
non-government religious social movements and organizations
in Indonesia. Social scientists on our team observed that
clustering was not fully able to separate all counter-radical or
978-0-7695-4406-9/11 $26.00 © 2011 IEEE
DOI 10.1109/EISIC.2011.37

Given the complex nature of the task, such as regional
differences in local cultures, beliefs and practices, and in
the absence of readily available high accuracy parsers, highly
structured religio-social ontologies, and information extraction
systems; we decided to devise a multi-lingual non-linguistic
text processing pipeline that relies on only statistical modeling
of keyword frequency and co-occurrence information.
We worked with social scientists on our team to come
up with an orthogonal model comprising of two primary dimensions. Both dimensions, (i) radical/counter-radical and (ii)
violent/non-violent, are characterized as latent, partial orders
of discrete beliefs and practices based on a generalization of
item order in Guttman scaling [3] using a Rasch model [4].
A true Guttman scale is a deterministic process, i.e. if a
social movement subscribes to a certain belief or practice,
than it must also agree with all lower order practices and
beliefs on the scale. Of course, such perfect order is rare in
the social world. The Rasch model provides a probabilistic
framework for Guttman scales to accommodate for incomplete
observations and measurement errors.
In this paper we present feature extraction and model fitting
techniques to process the rhetoric found in the web sites of
23 religious Indonesian organizations – comprising a total of
37,000 articles dating from 2005 to 2011. We aim to profile
their ideology and activity patterns along the hypothesized
radical/counter-radical scale, and rank them by assigning them
308

is expected to respond positively to all the items of lesser
difficulty. For example, in order to find out how extreme a
subject’s view is on Guttman scale, the subject is presented
with the following series of items in question form: (1) Are
you willing to permit immigrants to live in your country?
(2) Are you willing to permit immigrants to live in your
community? (3) Are you willing to permit immigrants to
live in your neighborhood? (4) Are you willing to permit
immigrants to live to your next door? and (5) Are you willing
to permit your child to marry an immigrant? If the items form
a Guttman scale, any subject agreeing with any item in this
series will also agree with other items of lower rank-order
in this series. Guttman scale is a deterministic process and
the score of a subject depends on the number of affirmative
responses he has made on the items. So, a score of 2 for
a subject in the above Guttman scale not only means he
has given affirmative response to two of the questions or
items, but also indicates that he agrees with two particular
questions, namely the first and second. Scores in Guttman
scale can also be interpreted as the “ability” of a subject in
answering questions sorted in increasing order of “difficulty”.
These scores when presented on an underlying scale, give us
an ordering of the subjects based on their “ability” too.
The objective of our paper is to order the Indonesian
Islamic organizations based on their views on religio-social
keywords which have an inherent ordering. For example, two
such keywords are “Quran” and “Sharia”. An organization
supporting “Sharia” will also likely to “believe in Quran”. So
it makes sense to use Guttman scaling procedure to rank the
organizations and their beliefs and practices. One drawback of
Guttman scale is that it is deterministic and assumes a strict
ordering of the items. In real world, it is difficult to order all
the items in such a strict level of increasing difficulty, therefore
perfect scales are not often observed in practice. Furthermore,
many times, the order of the items are not known since they are
not straightforwardly comparable. Also measurement errors
might lead to responses that do not strictly fit the ordering.
As a result we can no longer conclude deterministically that
if a subject answers a question affirmative, whether she will
be able to give affirmative answers to other questions of lower
order in the same questionnaire. We use Rasch model to
overcome this drawback by taking into account measurement
error.

to probable positions on this scale [5]. We used the eRm1
package to fit the Rasch model on this data set, and identify
organizations’ positions based on maximum likelihood estimation [6]. The automated ordering of organizations is formed
by ranking the organizations according to their estimates on
the latent scale. We show that the model fits the data using the
Andersen’s LR-test[7]. We also created a gold standard of the
ranking of these organizations through an expert opinion elicitation tool, and through the opinions of three ethnographers
on our team who collectively possess 35 years of scholarly
expertise on Indonesia and Islam. We computed expert-to-gold
standard agreements, as well as compared the performance of
three different baseline computational methods to show that
the Rasch model presented here not only performs the best
among the baseline methods, but that it also performs at an
expert level of accuracy.
A. Organization of the paper
Next section provides an introduction to the theory of
Guttman Scaling and Rash Models. Section III defines the
problem, presents the system architecture, and the methods
used to solve the problem. Section IV describes the Indonesian
corpus, expert opinion elicitation tool, baseline computational
methods, and experimental evaluations. Section V concludes
the paper.

II. I NTRODUCTION OF G UTTMAN S CALING AND R ASCH
M ODEL
In social science scaling is a process of measuring and
ordering entities called subjects based on their qualitative
attributes called items. In general, subjects are requested to respond to surveys conducted by means of structured interviews
or questionnaires. Items are presented to the subjects in form
of questions. Statistical analysis of the response of the subjects
on the questions about items are used in scaling the subjects.
Some of the widely followed scaling procedure in social
science surveys are Likert scale [8], Thurnstone scale [9], and
Guttman scale [10]. Guttman scaling procedure orders both
the subjects and the items simultaneously with respect to some
underlying cumulative continuum. In this paper we follow the
Guttman scaling process to rank the organizations based on
their response on the radical and counter-radical keywords.

B. Rasch Model

A. Guttman Scaling

Rasch model [4] provides a probabilistic framework for
Guttman scales. In Rasch model, the probability of a specified
binary response (e.g. a subject agreeing or disagreeing to an
item) is modeled as a function of subject’s and item’s parameters. Specifically, in the simple Rasch model, the probability
of a positive response (yes) is modeled as a logistic function
of the difference between the subject and item’s parameters.
Item parameters pertain to the difficulty of items while subject
parameters pertain to the ability of subjects who are assessed.
A subject of higher ability relative to the difficulty of an item,
has higher probability to respond to a question affirmatively. In

A Guttman [3] scale presents a number of items to which
each subject is requested to provide a dichotomous response,
e.g. agree/disagree, yes/no, or 1/0. This scaling procedure is
based on the premise that the items have strict orders (i.e., the
items are presented to the subjects ranked according to the
level of the item’s difficulty). An item “A” is said to be “more
difficult” than an item “B” if any subject answering “yes”
on item “A” implies that the subject will also answer “yes”
on item “B”. A subject who responds to an item positively
1 http://r-forge.r-project.org/projects/erm/

309

this paper Rasch models are used to assess the organizations
degree of being radical or counter-radical based on the religiosocial keywords (items) appearing in their rhetoric.
Rasch model also maps the responses of the subjects to
the items in binary or dichotomous format , i.e., 1 or 0. Let
Bernoulli variable Xvi denotes the response of a subject v to
the item i, variable θv denotes the parameter of “ability” of
the subject v and βi denotes the parameter of “difficulty” of
an item i. According to simple Rasch model the probability
that subject v responds 1 for item i is given by
P (Xvi = 1|θv , βi ) =

and counter-radical keywords. These keywords represent the
radical and counter-radical beliefs and practices of the organizations. An organization responding “yes” to a feature means
the organization exhibits that feature while an organization
responding “no” to a feature indicates that the organization
does not exhibit such a feature. Difficulty of an item translates
to strength of the corresponding attitude in defining radical or
counter-radical ideology of any organization. Similarly ability
of a subject in this case means degree of radicalism or counterradicalism exhibited by an organization’s rhetoric. Details of
keyword selection is presented in the next section.

exp(θv − βi )
1 + exp(θv − βi )

III. M ETHOD
A. Problem Definition

Rasch model assumes that the data under analysis have the
following properties
1) Unidimensionality: P (xvi = 1|θv , βi , α) = P (xvi =
1|θv , βi ), i.e., the response probability does not depend
on other variable
2) Sufficiency: sum of responses contains all information
on ability of a subject, regardless which item it has
responded
3) Conditional independence: for a fixed subject there is
no correlation between any two items
4) Monotonicity: response probability increases with higher
values of θ, i.e., subject’s ability
Pn
Items with si = v xvi value of 0 or n, and subjects with
Pk
rv = i xvi value of 0 or k are removed prior to estimation,
where n is the total number of subjects and k is the total
number of items. Running Rasch model on the data gives us
an Item parameter estimate or a score for each item. Generally
the estimation of βi or score for a item i is calculated through
Conditional Maximum Likelihood (CML) estimation [11]. The
conditional likelihood function for measuring item parameter
estimate is defined as
Y
exp(−βi si )
Lc =
P (xvi |rv ) = Q P
r
x|r exp(−βi xvi )
v

The goal of this study is to build a semi-automated method
to rank religious organizations from a certain geographical
region on a scale of radicalism vs. counter-radicalism using
their web sites. The efficacy of the generated model is evaluated by comparing it against baseline methods and expert level
performance.
B. System Architecture

where r represents the sum over all combinations of r items.
Similarly maximum likelihood is used to calculate subject
parameter estimation θv or score for each subject.
In order to evaluate the quality of these measurements we
run Anderson Likelihood Ratio test (LR-test) [7] on the set of
data. The test gives us a goodness of fit of the data in Rasch
model, i.e., it tells us whether the data follows the assumptions
of Rasch model. A p-value, returned by the test, indicates the
goodness of fit and a p-value2 higher than 0.05 indicates no
presence of lack of fit.

Fig. 1.

A model of the system architecture.

The system architecture is shown in Figure 1. Here the flow
of the processes and data can be seen as interactions between
experts and automated modules. The system works as follows:
• Initially, social scientists use their technical and area
expertise to identify a set of organizations, and hypothesize any number of unipolar or bipolar scales that could
explain the variance among the beliefs and practices of
the organizations. In this paper, we primarily focus on
the construction and validation of the bipolar radical vs.
counter-radical (R/CR) scale, however the techniques can

C. Implementing in Text Mining Domain
In this paper, we use Guttman scaling and Rasch model to
find a ranking of some political organizations based on how
extreme their views are on radicalism and counter-radicalism.
In our project, model subjects are a group of religious organizations and items are a set of socio-religious radical
2 http://en.wikipedia.org/wiki/P-value

310

•

•

•

be readily applied to the construction and validation of
any other relevant scale.
Next, we crawl and download the web sites of the
organizations, and the system automatically extracts the
top-k candidate keywords for consideration in the hypothesized scale. Social scientists screen the list of extracted
keywords, and select the relevant ones for inclusion in
further analysis.
The system builds response tables; a pair of tables for
a bipolar scale (such as R/CR), or a single table for a
unipolar scale, by thresholding the occurrence frequencies of the selected keywords in the organizations’ web
corpus. See Fig. 3 and Fig. 4 for the response tables for
the R/CR scale. The response tables are fed as input to the
Rasch Model building algorithm. The algorithm produces
a metric to validate the fitness of the model, and rankings
of the organizations and keywords.
In parallel to this, two types of other information are collected for evaluation purposes. First, expert rankings of
the organizations, using a graphical drag-and-drop expert
opinion elicitation tool shown in Fig. 2). Expert rankings
are merged into a consensus gold standard of rankings.
Next, two other computational baseline methods; one
based on simple sorting, and another based on principal
component analysis [12], are used to generate alternative
computational rankings shown in Fig. 5.

This task was performed in a simple three step procedure;
initially the occurrence frequencies of particular keywords
were counted within each organization’s corpus, then a threshold matrix was calculated from the initial values, and finally
a binary response matrix was generated by applying these
thresholds to the initial values.
The frequency metric is shown in formula (1), where k is
the keyword, o is the organization, and Do is the document
set pertaining to that particular organization.
fo,k =

|{d | k ∈ d, d ∈ Do }|
|Do |

(1)

A threshold value for each keyword is calculated by taking
the median of the values in the related column. Median was
preferred over mean as a threshold, since the distribution of the
values did not fit Gaussian distribution, yet median empirically
proved to be a better measure.
Finally, each element was converted into a binary value by
comparing it to the column’s threshold. English translations
of the keywords is presented for clarity in Fig. 3 and Fig.
4, additionally names of the corresponding organizations are
anonymized consistently in all figures and tables.
E. Model Fitting
We fit the Rasch model on two datasets - (1) radical
organizations with radical keywords and (2) counter-radical
organizations with counter-radical keywords. We used the eRm
package in R, an open source statistical software package3 , to
fit a Rasch model to the dataset, and obtain the organizations’
scores on the latent scale, which are the the subject parameter
estimates (θv ) discussed in previous section. The eRm package4 fits Rasch models and provide subjects or organizations
parameter estimates based on maximum likelihood estimation.
The automated scale of the organizations is formed by
ranking the organizations according to their estimates on
the latent scale. Not only we can provide the organization
estimates but we can also assess whether the model fits the
data by looking at several goodness of fit indices, such as the
Andersen’s LR-test.

C. Keyword Selection
In order to identify candidate keywords, one option was
to translate the documents into English and apply readily
available keyword extraction methods [13]. However it was
preferable to preserve the original expression of the phrases in
the original language. Therefore, we utilized a non-linguistic
technique that relies only on statistical occurrence and frequency information.
Within each document, the words were separated by whitespace or punctuation marks. We considered each keyword to
be an n-gram of one to three words. We treated each organization as one document and calculated the term frequency
- inverse document frequency (TF-IDF) [14] values for every
single n-gram mentioned by these organizations. The n-grams
with highest TF-IDF value gave us the topics that each of
the organization discusses most. Then the top 100 n-grams
from each organization were made into a list of candidate
keywords. Finally, belief and practice keywords that belong to
one of the following categories {social, politics, economics,
religion} were manually identified by the experts as relevant
for inclusion. This process assessed a total of 790 candidate
keywords; of which 29 and 26 were selected by experts for
inclusion in the radical and counter-radical scales respectively.

IV. E XPERIMENTAL E VALUATION
A. Indonesian Corpus
The corpus domain is the online articles published by the
web sites of the 23 religious organizations identified in Indonesia, in the Indonesian language. These sources are the web
sites or blogs of the identified think tanks and organizations.
As discussed in the introduction, each source was classified
as either radical or counter-radical by the area experts. We
downloaded a total of 37,000 Indonesian articles published in
these 23 web sites, dating from 2005 to 2011. For each web
site, a specific REGEX filter was used to strip off the headers,
footers, advertising sections and to extract the plain text from
the HTML code.

D. Feature Extraction
After identifying the keywords for the analysis, we needed
to search the web site corpus of the organizations for the
matching items. This yielded a term-document matrix.

3 http://cran.r-project.org/
4 http://r-forge.r-project.org/projects/erm/

311

Fig. 3. Radical subset of organizations and keywords, sorted according to
aggregate row values.

Fig. 4.
Counter-Radical subset of organizations and keywords, sorted
according to aggregate row values.

Fig. 2. The visual interface of the expert opinion collector for manually
placing the organizations on the two dimensional scale

For two exactly matching rankings, the error(G, R) will be
zero, whereas for two inversely sorted rankings it is expected
to be 0.5 (when the size of O is even). Also a random ranking
is expected to have a error of 0.375.

B. Expert Opinion and Gold Standard of Rankings
We collaborated with three area experts, who collectively
possess 35 years of scholarly expertise on Indonesia and
Islam. In order to build a gold standard of orderings of
the organizations, we built a graphical drag-and-drop user
interface tool to collect the opinions of each of the area experts.
A screenshot of the tool is shown in Fig. 2.
Each expert separately evaluated and ranked the organizations in the dataset according to a two dimensional scale of
radical/counter-radical (R/CR) and violent/non-violent (V/NV)
axis. The consensus among the experts was high; since per
item standard deviations among the experts’ scores along the
R/CR axis over a range of [−10, 10], across all organizations
were 2.75. Also, 90% of the items have less than 22.6%
difference in their rankings. The individual scores for each
organization were combined and averaged to obtain the consensus gold standard rankings along the hypothesized R/CR
scale.

D. Expert-to-Gold Standard Error
We calculated the error between each expert’s ranking and
their consensus gold standard of rankings. The first expert’s
error measure is 0.06, and the second and third expert’s errors
are 0.12 and 0.14 correspondingly as shown in the last row of
the table in Fig. 5. The average error of our experts against
their gold standard ranking is 0.11.
E. Baseline - Sorting with Aggregate Score
The first baseline we used was constructed by sorting the
organizations according to the number of different keywords
observed in their corpus. While this provided a pattern similar to a Guttman Scale, and orderings of the organizations
matched to a certain degree with the gold standard as shown
in Fig. 5, the error for this baseline was 0.19, which is higher
than the average expert’s performance.

C. Computationally Generated Scale
The ranking discovered by the Rasch model fitting the
corpus has been evaluated against the gold standard rankings
of the organizations provided by the experts. The difference
between two separate rankings have been calculated by using
the following misplacement error measure in Equation 2.
P
|G(o)−R(o)|
error(G, R) =

o∈O

|O|

|O|

F. Baseline - Principal Component Analysis
A stronger baseline was built by employing principal component analysis [12], and sorting the organizations according
to their projections in the first principal component of the
term-document matrix. Since experts selected the R/CR scale
relevant keywords only, it was expected that the first principal
component would reflect the corresponding scale. PCA proved
to be performing better than the aggregate score sorting, with
an error measure of 0.18. However, this error rate is still
higher than the error rate of each expert.

(2)

Here, O is the set of organizations, G and R are one to one
mapping functions of rankings from set O to range [1, |O|].

312

G. Performance of the Rasch Model Ranking System
The p-values from the Anderson LR goodness of fit test
from model (1) and model (2) (mentioned in section III-E)
are 0.85 and 0.669 respectively, suggesting no evidence of
lack of fit. The Rasch models allow us to get a natural order of
the organizations, according to their “abilities”, i.e.: radicalism
and counter-radicalism in this case. This system had an error
measure of 0.10, which actually provided a higher ranking
performance than the average performance of our experts’ –
performing better than the majority of our area experts.
H. Evaluations
Our experiments showed that the hypothesized compatibility
of the R/CR scale for the Indonesian corpus is valid. Not only
the Rasch model was statistically fitting the response matrix,
but also the generated ranking performance was better than
the average expert performance. Among our computational
baseline methods, the Rasch Model was the only method
producing expert-level performance as shown in Fig. 5. This
preliminary analysis with the R/CR scale shows that when
experts assist the system with keyword selection, the web
corpus of organizations provides rich enough information
and patterns to enable a computational method to rank them
accurately.

Fig. 5.

Computational and expert rankings

R EFERENCES
[1] R. Crelinsten, “Analysing terrorism and counter-terrorism: A communication model,” Terrorism and Political Violence, vol. 14, pp. 77–122,
2002.
[2] H. Davulcu, S. T. Ahmed, S. Gokalp, M. H. Temkit, T. Taylor,
M. Woodward, and A. Amin, “Analyzing sentiment markers describing
radical and counter-radical elements in online news,” in Proceedings of
the 2010 IEEE Second International Conference on Social Computing,
ser. SOCIALCOM ’10. IEEE Computer Society, 2010, pp. 335–340.
[3] L. Guttman, “The basis for scalogram analysis,” Measurement and
prediction, vol. 4, pp. 60–90, 1950.
[4] D. Andrich, Rasch models for measurement. Sage, 1988, no. 68.
[5] R. D. McPhee and S. Corman, “An activity-based theory of communication networks in organizations, applied to the case of a local church,”
Communication Monographs, vol. 62, pp. 1–20, 1995.
[6] L. Le Cam, “Maximum likelihood an introduction,” ISI Review, vol. 58,
no. 2, pp. 153–171, 1990.
[7] D. Hessen, “Likelihood ratio tests for special rasch models,” Journal of
Educational and Behavioral Statistics, vol. 35, no. 6, p. 611, 2010.
[8] R. Likert, “A technique for the measurement of attitudes,” Archives of
Psychology, vol. 140, pp. 1–55, 1932.
[9] L. L. Thurstone, “Attitudes can be measured,” American Journal of
Sociology, vol. 33, pp. 529–554, 1928.
[10] J. McIver and E. Carmines, Unidimensional Scaling. Sage Publications,
Inc, 1981, vol. 24.
[11] Y. Pawitan, In all likelihood: statistical modelling and inference using
likelihood. Oxford University Press, USA, 2001.
[12] I. Jolliffe, Principal Component Analysis. Springer Series in Statistics,
2002.
[13] W. Michael and J. Kogan, Text Mining: Applications and Theory. Wiley,
2010.
[14] G. Salton and C. Buckley, “Term-weighting approaches in automatic
text retrieval,” in Information Processing and Management, 1988, pp.
513–523.
[15] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—but
is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the Conference on Empirical Methods in
Natural Language Processing, ser. EMNLP ’08. Stroudsburg, PA, USA:
Association for Computational Linguistics, 2008, pp. 254–263. [Online].
Available: http://portal.acm.org/citation.cfm?id=1613715.1613751
[16] A. W. James and L. M. John, “Algebraic representations of beliefs
and attitudes: Partial order models for item responses,” Sociological
Methodology, vol. 29, pp. 113–146, 2002.

V. C ONCLUSIONS AND F UTURE W ORK
In our experiments, not only did the data show fitness with
the Rasch Model for the R/CR scale, but also the Rasch
rankings of the organizations are better than the output of the
other baseline computational methods, and they are at expert
level performance when compared with the consensus gold
standard rankings.
Rasch Model also provided us with another output, namely
the ranking of selected keywords (items) on the R/CR scale.
Although preliminary observations indicates that this can be
a valuable asset by itself, we plan to further investigate the
quality and utility of this ranking as future work.
While the model has been demonstrated to fit on the R/CR
scale, two major expansion points can be investigated in
the future work, namely the violent/non-violent scale, and
enhancement of feature selection. Although our experts have
identified a second dimension, evaluating its correlation to
R/CR axis, or existence of other significant ones could be
beneficial. Additionally, the features can be enhanced by
experimenting with the significance of the radical keywords
in the counter-radical organization corpuses, and vice-versa.
Other interesting work includes making our expert opinion
elicitation tool available online to a wider and more geographically distributed audience to crowdsource [15] the needed
expertise for making lists of local organizations, identifying
their web sources, and overcome the complex task of construction and validation of significant and fitting scales. Another
interesting dimension is to look at synthesis and analysis of
scales that do have a strict hierarchy of keywords, but adhere
to more flexible partial order models [16].

313

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

A Dynamic Modularity Based Community Detection
Algorithm for Large-scale Networks: DSLM
Riza Aktunc and Ismail Hakki Toroslu

Mert Ozer and Hasan Davulcu

Computer Engineering Department
Middle East Technical University
Ankara, Turkey 06530
Email: {riza.aktunc, toroslu}@ceng.metu.edu.tr

School of Computing, Informatics,
Decision Systems Engineering
Arizona State University
Tempe, USA 85287
Email: {mozer, hdavulcu}@asu.edu

Abstract—In this work, a new fast dynamic community
detection algorithm for large scale networks is presented. Most
of the previous community detection algorithms are designed
for static networks. However, large scale social networks are
dynamic and evolve frequently over time. To quickly detect communities in dynamic large scale networks, we proposed dynamic
modularity optimizer framework (DMO) that is constructed
by modifying well-known static modularity based community
detection algorithm. The proposed framework is tested using
several different datasets. According to our results, community
detection algorithms in the proposed framework perform better
than static algorithms when large scale dynamic networks are
considered.

I.

I NTRODUCTION

In the last decade, the notion of social networking is
emerged and produced very large graphs that consist of the
information of their users. These graphs generally consist of
the nodes that represent the users; and edges that represent the
relations among users. The nodes in these graphs generally
tend to get together and construct communities of their own.
Thus, it can be stated that social networks commonly have
a community structure. These networks can be divided into
groups of nodes that have denser connections inside the group;
but fewer connections to the outside of the group. For example,
in a GSM network, a group of users who call each other more
densely than they call other users may construct their own
community. In this case, the nodes represent the users and the
edges represent the calls that users made. The detection of
communities in these large networks is a problem in this area;
therefore a lot of community detection algorithms such as [1],
[2], [3], [4], [5], [6], [7], [8], [9] proposed in the literature.
Almost all these community detection algorithms are static
and designed for static networks.
However, most of the social networks are not static because
they evolve in many ways. They may gain or lose users that
are represented as nodes in the graphs over time. The users of
these social networks may lose contact from each other or there
can be new connections among users. In other words, some
edges in the graphs may be removed or new edges may be
added to the graph over time. All these processes may happen
in a very small amount of time in a social network if it has
a lot of active users. This kind of a social network may be
called as highly dynamic. For example, popular social sites
such as Facebook, Twitter, LinkedIn and so on have highly
dynamic social networks. Moreover, most GSM networks have
ASONAM '15, August 25-28, 2015, Paris, France
© 2015 ACM. ISBN 978-1-4503-3854-7/15/08 $15.00
DOI: http://dx.doi.org/10.1145/2808797.2808822

1177

millions of users and hundreds of calls made in seconds;
therefore, they can also be labeled as highly dynamic networks.
Addition or deletion of an edge or a node from a network
which has millions of edges might seem insignificant; but when
this additions or deletions of an edge or a node happen very
frequently, they begin to change the community structure of
the whole network and become very important. This change in
the community structure raises the need of re-identification of
communities in the network. This need arises frequently and
creates a new problem in the community detection research
area. This new problem requires somehow fast detection of
communities in dynamic networks.
The first solution that comes to mind for community
detection in large dynamic networks problem is the execution
of static community detection algorithms already defined in
the literature all over again to detect the new community
structure whenever the network is modified. Nevertheless, this
solution takes too much time in every modification of the large
networks since it runs the community detection algorithm from
scratch each time. A much efficient and less time consuming
solution is to run the community detection algorithms not
from scratch but from a point in the history of the network
by storing and using the historical results of executions of
the algorithms whenever network is evolved. In other words,
updating previously discovered community structure instead of
trying to find communities from scratch each time the network
evolves consumes much less time and thus much efficient. This
solution method for the problem of detecting communities in
large dynamic networks is the main focus of our study in this
paper.
In this paper, we modified the smart local moving (SLM)
algorithm defined by Waltman & Van Eck [9] so that it would
detect the communities in rapidly growing large networks dynamically and efficiently. As a result, we propose the dynamic
SLM (dSLM) algorithm that dynamically detects communities
in large networks by optimizing modularity and using its
own historical results. We tested our proposed approach on
several different datasets. We demonstrated the effects of our
contribution to the SLM algorithm in two ways. One of
them is the change in modularity value which determines
the quality of the community structure of the network. The
other one is the change in running time that determines the
pace of the algorithm. The latter is more significant than the
former because the community structure of the network must
be quickly identified at the given timestamp before the next

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
timestamp is reached. We realized that dSLM improved SLM
by decreasing its running time incredibly. Moreover, there
are some experiments where modularity value increases while
running time decreases.
The rest of the paper is organized as follows. Section II
introduces previous researches done in the area. Section III
explains the modularity. The proposed solution for dynamic
community detection in large networks called as dSLM and
its static version SLM are described in Section IV. In Section
V, the results of the experiments of SLM and dSLM are
demonstrated. Finally, the paper is concluded in Section VI.

II.

R ELATED W ORK

The idea of modularity-based community detection is to
try to assign each vertex of the given network to a community
such that it maximizes the modularity value of the network.
Optimizing modularity is an NP-hard problem. [10] Exact
algorithms that maximize modularity such as [11], [10], [12]
can be used only for small networks.
For large-scale modularity optimization, heuristic algorithms are proposed. We basically focus on three well known
algorithms, namely; CNM, Louvain and SLM. The first one is
Clauset et al.’s [13] CNM algorithm. It is a greedy modularity
maximization algorithm that searches for best community
assignment for each node. The second one is referred as
Louvain algorithm and proposed by Blondel et al. [7] in 2008.
By considering each community as a single node, it further
searches for new community merges after the local optimum
satisfied using CNM. The last one is called as Smart Local
Moving (SLM) algorithm that is proposed by Waltman and
Jan van Eck in 2013. [9] SLM algorithm is explained in detail
in chapter III.
Due to the dynamic features of many social networks
[14], the need for detecting communities dynamically in the
large networks is emerged in the latest years. There have
been many community detection algorithms proposed in the
literature to fulfill this need. Xu et al. divides the current
research on community evolution into the following categories.
Parameter estimation methods and probabilistic models have
been proposed in the literature. [15], [16] A methodology
that tries to find an optimal cluster sequence by detecting
a cluster structure at each timestamp that optimizes the incremental quality can be classified as evolutionary clustering.
[17], [18] Furthermore, tracking algorithms based on similarity
comparison have also been studied in order to be able to
describe the change of communities on the time axis. [19], [20]
Apart from these algorithms that are focused on the evolution
procedures of communities, community detection in dynamic
social networks aims to detect the optimal community structure
at each timestamp. For this purpose, incremental versions of
both CNM and Louvain algorithm are proposed by Dinh et
al.[21] and Aynaud et al. [22]. To the best of our knowledge,
this is the first work considering the incremental version of
Smart Local Moving algorithm in literature. Our algorithm
can be classified as the last mentioned category which aims
to detect optimal community structure at each timestamp with
minimum running time.

1178

III.

M ODULARITY

Modularity is a function that is used for measuring the
quality of the results of community detection algorithms.
If the modularity value of a partitioned network is high,
it means that the network is partitioned well. Apart from
quality measurement, modularity is used as the basis of some
community detection algorithms. These algorithms try to detect
communities (partitions) in a network by trying to maximize
the modularity value of the network. Thus, modularity is
a function that is used for both quality measurement and
community detection.
Modularity is based on the idea that a randomly created
graph is not expected to have community structure, so comparing the graph at hand with a randomly created graph would
reveal the possible community structures in the graph at hand.
This comparison is done through comparing the actual density
of edges in a subgraph and the expected edge density in the
subgraph if the edges in the subgraph were created randomly.
This expected edge density depends on how random the edges
created. This dependency is tied to a rule that defines how
to create the randomness and called as null model. A null
model is a copy of an original graph and it keeps some
of this original graphs structural properties but not reflects
its community structure. There can be multiple null models
for a graph such that each of them keeps different structural
properties of the original graph. Using different null models for
the calculation of the modularity leads to different modularity
calculation methods and values. The most common null model
that is used for modularity calculation is the one that preserves
the degree of each vertex of the original graph. With this null
model, modularity is calculated as the fraction of edges that
fall in the given communities minus such fraction in the null
model. [23], [24] The formula of modularity can be written as
in Equation 1

Q=

1 X
(Aij − Pij )δ(Ci , Cj )
2m ij

(1)

m represents the total number of edges of the graph. Sum
iterates over all vertices denoted as i and j. Aij is the number
of edges between vertex i and vertex j in the original graph.
Pij is the expected number of edges between vertex i and
vertex j in the null model. The δ function results as 1 if the
vertex i and vertex j are in the same community (Ci = Cj ),
0 otherwise. The null model can be created by cutting the
edges between vertices; thus, creating stubs (half edges) and
rewiring them to random vertices. Thus, it obeys the rule of
keeping degrees of vertices unchanged. Cutting edges into half,
creates m ∗ 2 = 2m stubs. In the null model, a vertex could be
attached to any other vertex of the graph and the probability
that vertices i and j, with degrees ki and kj , are connected,
can be calculated. The probability pi to pick a ramdom stub
ki
connection for vertex i is 2m
, as there are ki stubs of i out of a
total of 2m stubs. The probability of vertex i and vertex j being
connected is pi pj , since stubs are connected independently
of each other. Since there are 2m stubs, there are 2mpi pj
expected number of edges between vertex i and vertex j. [24]
This yields to equation 2

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining

Pij = 2mpi pj = 2m

ki kj
ki kj
=
2
4m
2m

(2)

By placing equation 2 into equation 1, modularity function
is presented as in equation 3.
Q=

1 X
ki kj
(Aij −
)δ(Ci , Cj )
2m ij
2m

(3)

The resulting
 values of this modularity function lie in the
range −1
,
1
. It would be positive if the number of edges
2
within subgraphs is more than the number of expected edges
in the subgraphs of null model. Higher values of the modularity
function mean better community structures. [9]
This modularity function also applies to weighted networks. [9], [25] The modularity function for the weighted
graphs can be calculated as in equation 4.
Qw =

1 X
si sj
(Wij −
)δ(Ci , Cj )
2W ij
2W

(4)

There are three differences. The first difference is that in
the case of a weighted network Wij , instead of Aij , may take
not just 0 or 1 but any non-negative value that represents the
weight of the edge. The second one is that instead of m, which
is total number of edges, W , which is the sum of the weights
of all edges is used in the equation. The last one is that si and
sj which represents the sum of the weights of edges adjacent
to vertex i and vertex j respectively is used in the equation
instead of ki and kj which means the degree of vertex i and
vertex j respectively. [24]
Apart from weighted networks, the modularity function
defined in 3 has been extended in order to be also applicable
to directed networks. [26], [27] When the edges are directed,
stubs will also be directed and it changes the possibility of
rewiring stubs and connecting edges. The calculation of this
possibility in the directed case depends on the in- and outdegrees of the end vertices. For instance, there are two vertices
A and B. A has a high in-degree and low out-degree. B has a
low in-degree and high out-degree. Thus, in the null model of
modularity, an edge will be much more likely to point from
B to A than from A to B. [24] Therefore, the expression of
modularity for directed graphs can be written as in equation 5
Qd =

kiout kjin
1 X
(Aij −
)δ(Ci , Cj )
m ij
m

There have been a few proposals of modified version of the
modularity functions defined above as alternative modularity
functions. These modified, extended versions for instance offer
a resolution parameter that makes it possible to customize
the granularity level at which communities are detected and
to mitigate the resolution limit problem defined by Fortunato
and Barthlemy [28]. [29] Moreover, there are modularity
functions with a somewhat modified mathematical structure
in the literature such as Reichardt & Bornholdt, 2006; Traag,
Van Dooren, & Nesterov, 2011; Waltman, Van Eck, & Noyons,
2010. [9], [28], [30], [31]
IV.

A. SLM Algorithm
SLM is a community detection algorithm that is evolved
from Louvain algorithm. Louvain algorithm is a large scale
modularity based community detection algorithm that is proposed by Blondel et al in 2008. [7] The quality of detected
communities by Louvain algorithm is measured by the method
called modularity. The modularity of a network is a value
that is between -1 and 1. This value presents the density of
links inside communities over the density of links between
communities. [23] When this value is close to 1, then the
measured network can be called as modular network. In
the case of weighted networks, modularity function can take
weights into consideration and measure the quality of detected
communities. Louvain algorithm uses modularity function as
not only a measurement function but also an objective function
to optimize.
Louvain algorithm is a recursive algorithm which has two
steps running in each recursive call. Before the recursion starts,
the algorithm assigns a different community to each node of
the network whose communities are going to be detected.
Therefore, in the initial case each node has its own community.
In each recursive call the following steps are run:
1)

(5)
2)

The sum of the in-degrees (out-degrees) equals m not
2m as in the case of undirected graph. Therefore, the factor
2 in the denominator of the first and second summand has
been dropped. In order to get the modularity function to be
applicable to directed weighted networks, the equations 4 and
5 can be merged; thus, equation 6 can be constructed as the
most general expression of modularity. [24]
Qdw =

in
sout
1 X
i sj
(Wij −
)δ(Ci , Cj )
W ij
W

(6)

1179

SLM AND D SLM A LGORITHMS

It runs a local moving heuristic in order to obtain
an improved community structure. This heuristic basically moves each node from its own community
to its neighbors’ community and run the modularity
function. If the result of the modularity function,
which means quality, increased, the node would be
kept in the new community; else, the node would be
moved back to its previous community. This process
is applied to each node for its each neighbor in
random order and thereby heuristically the quality is
tried to be increased.
The algorithm constructs a reduced network whose
nodes are the communities that are evolved in the
first step. Moreover, the weights of the edges in this
reduced network are given by the sum of weights
of the links between the nodes which reside in
the corresponding two communities. Links between
nodes of the same community in the old network are
presented as self-links for the node that represents
that community in the new reduced network. When
this reduced network is fully constructed, then algorithm calls itself recursively and first step is applied
to this reduced network.

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
The algorithm keeps recursing until no further improvement
in modularity is measured and thereby there are no changes in
the community structure. [7]
Louvain algorithm detects community structures whose
modularity values are locally optimal with respect to community merging, but not necessarily locally optimal with respect
to individual node movements. Since the Louvain algorithm
applies local moving heuristic in the beginning of its recursive
block and merges communities by reducing network in the
end of its recursive block, calling it iteratively ensures that
the resulting community structure cannot be improved further
either by merging communities or by moving individual nodes
from one community to another. Like the iterative variant
of these algorithms SLM algorithm constructs community
structures that are locally optimal with respect to both individual node movements and community merging. Besides
these capabilities, SLM also tries to optimize modularity by
splitting up communities and moving sets of nodes between
communities. This is done by changing the way that local
moving heuristic and network reduction runs.[9]
Louvain algorithm runs local moving heuristic algorithm
on the present network as the first step, and then construct
the reduced network as the second step. However, the SLM
algorithm changes the reduced network construction step by
applying following processes:
1)

2)
3)

It iterates over all communities that are formed by the
first step. It copies each community and constructs a
subnetwork that contains only the specific community’s nodes.
It then runs the local moving heuristic algorithm on
each subnetwork after assigning each node in the
subnetwork to its own singleton community.
After local moving heuristic constructs a community
structure for each subnetwork, the SLM algorithm
creates the reduced network whose nodes are the
communities detected in subnetworks. The SLM algorithm initially defines a community for each subnetwork. Then, it assigns each node to the community
that is defined for the node’s subnetwork. Thus,
there is a community defined for each subnetwork
and detected communities in subnetworks are placed
under these defined communities as nodes in the
reduced network.

Procedure: Initialize Communities
Input: Old Communities, Old Network, New Network
Output: New Communities
1: j = 0
2: for i = 0 → Old Communities.size do
3:
N ew Communities = ReadCommunitiesF ile()
4: end for
5: Delta N etwork = N ew N etwork − Old N etwork
6: for j = i → Delta N etwork.size do
7:
new communities[j] = Delta N etwork[j − i]
8: end for
Fig. 1.

Initialize Communities Procedure

•

Existing communities are read from file as New Communities.

•

If exists, the extensions to the network has been
determined.

•

For each new node, singleton new communities are
constructed and added to New Communities.

The effects of other changes in the network, such as adding
new edges and deletions of nodes and edges, are handled
while executing standard SLM procedure. The new dSLM is
available at https://github.com/mertozer/dSLM.
Since after some iterations, the increase of modularity
drops to very small values, it might make sense to stop the
iterations using either the amount of changes or by setting
a target modularity value. We have implemented the second
option, which is called dSLMEVS in the experiments. We
have made the tests by setting the modularity values as the one
obtained for SLM in order to be able to observe the differences
in the execution times for exactly the same modularity values.
C. Running Example

This is the way that the SLM algorithm constructs the reduced
network. After these processes, the SLM algorithm gives the
reduced network to the recursive call as input and all the
processes starts again for the reduced network. The recursion
continues until a network is constructed that cannot be reduced
further. To sum up, the SLM algorithm has more freedom in
trying to optimize the modularity by having the ability to move
sets of nodes between communities which cannot be done by
Louvain algorithm. [9]
B. Dynamic Smart Local Moving Algorithm

Fig. 2.

SLM algorithm initially assigns each node to a different
community, so each node has its own singleton community. In
order convert SLM to dynamic form, we replace that operation
with a newly defined procedure, called initialize communities
which is given in Figure 1. This procedure works as follows:

1180

Network in time t (analyzed by SLM)

Let the sample network depicted in Figure 2 to be a
network that changes in time and needs to be analyzed
continuously in each time frame. So, the network is analyzed
and communities are detected in time t. Figure 2 presents the

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
beginning and end states of the community structure of the
network analyzed by SLM algorithm in time t. Solid rectangles
present the initial community structure; whereas the colors
of the nodes (and dashed rectangles) present the resulting
community structure. From time t to time t+1, a node which
is numbered as 10 and an edge between this new node and
the node which is numbered as 9 are added to the network.
This evolved network in time t+1 can be seen in Figure 3 and
Figure 4. Figure 3 presents the community detection process

structure from scratch by trying and finding node movements
that maximize the modularity of the network. However, dSLM
needs to try only one node movement which is to move newly
added node from its singleton community to its only neighbor
(blue) community. Since it appears to increase the modularity
of the network, dSLM places the new node to blue community
and that is it. Because the initial community structure is known
to be the one that maximizes the modularity of the network,
there is no other node movement trying that can increase
modularity. By this way, the dSLM runs faster than SLM .
This run time difference between SLM and dSLM gets much
greater while the network size increases.
V.

E XPERIMENTS & R ESULTS

We evaluate our proposed approach dSLM on five realworld datasets which are the arXiv citation dataset, the GSM
calls dataset, Google Plus, Twitter and Youtube user network
datasets..
The arXiv1 citation dataset is published in the KDD Cup
2003. It contains approximately 29,000 papers and their citation graph. In this graph, each vertex represents a paper
and each edge represents the citation between its connected
vertexes. There are around 350,000 edges which represent
citations in this graph.

Fig. 3.

Network in time t+1 (analyzed by SLM)

of the network in time t+1 performed by SLM algorithm in
the same way as Figure 2. As the difference of Figure 2
and Figure 3, a new node and a new edge are only seen in
Figure 3. Since they both demonstrate the SLM process, the
initial communities are singleton. Figure 4 demonstrates the

We have used call detail record (CDR) dataset, obtained
from one of the largest GSM operators in Turkey. This
produced GSM calls dataset contains 12,521,352 nodes and
44,768,912 edges. These two datasets are used for edge
deletion and addition experiments purposes.
The Google Plus and Twitter user network data is collected
by Stanford Network Analysis Project2 . The Google Plus
data consists of 107,614 nodes and 13,673,453 edges. The
Twitter data consists of 81,306 nodes and 1,768,149 edges.
The Youtube user network data is provided by Mislove et
al. [32]. It consists of 1,134,890 nodes and 2,987,624 edges.
The users of the Youtube are the nodes, and the friendships
are represented by edges. We used these 3 datasets for node
deletion and addition experiments purposes.
In the first set of experiments we have assumed the dynamic
feature is in the form of edge insertions and deletions. Table I
and II gives the results for these experiements. In the second
set, on the other hand, node insertions and deletions represent
the dynamic feature. Table III and IV presents the results for
these experiments.
TABLE I.

Fig. 4.

Network in time t+1 (analyzed by dSLM)

dSLM process of the network in time t+1. In this process, the
community structure of the network in time t is used as the
initial states of communities which can be seen as rectangles
in Figure 4. Both SLM and dSLM algorithms place the newly
added node in blue community. SLM constructs the community

1181

T HE EFFECT OF D SLM

Algorithm

Dataset

Base (#
of Edges)

dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS

arxiv
arxiv
GSM
GSM
GSM
GSM
GSM
GSM
GSM
GSM

300,000
300,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000

#
of
Edges
Added
1,000
10,000
1,000
10,000
100,000
1,000,000
1,000
10,000
100,000
1,000,000

FOR EDGE INSERTIONS

Change in Modularity Value
0.02% increased
0.15% increased
no change
no change
no change
0.02% increased
no change
no change
no change
no change

1 http://www.cs.cornell.edu/projects/kddcup/datasets.html
2 http://snap.stanford.edu/data

Decrease
in Running
Time
26%
26%
27%
29%
20%
17%
91%
63%
64%
80%

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
TABLE II.

T HE EFFECT OF D SLM FOR

Algorithm

Dataset

Base (#
of Edges)

dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS

arxiv
arxiv
GSM
GSM
GSM
GSM
GSM
GSM
GSM
GSM

300,000
300,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000
10,000,000

TABLE III.

T HE EFFECT OF D SLM

Algorithm

Dataset

Base (#
of Nodes)

dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS

Twitter
Twitter
Twitter
Twitter
GPlus
GPlus
GPlus
GPlus
Youtube
Youtube
Youtube
Youtube
Twitter
Twitter
Twitter
Twitter
GPlus
GPlus
GPlus
GPlus
Youtube
Youtube
Youtube
Youtube

81,296
81,206
80,306
71,306
107,604
107,514
106,614
97,614
1157728
1156828
1147828
1057828
81,296
81,206
80,306
71,306
107,604
107,514
106,614
97,614
1,157,728
1,156,828
1,147,828
1,057,828

TABLE IV.

#
of
Edges
Deleted
1,000
10,000
1,000
10,000
100,000
1,000,000
1,000
10,000
100,000
1,000,000

#
of
Nodes
Added
10
100
1,000
10,000
10
100
1,000
10,000
100
1000
10000
100000
10
100
1,000
10,000
10
100
1,000
10,000
100
1000
10000
100000

EDGE DELETIONS

Change in Modularity Value
0.08% increased
0.04% increased
no change
no change
0.01% increased
0.01% increased
no change
no change
no change
no change

Decrease
in Running
Time
32%
7%
38%
27%
24%
16%
92%
61%
91%
80%

FOR NODE INSERTIONS

Change in Modularity Value
no change
no change
no change
0.04% increased
0.02% decreased
no change
no change
0.97% decreased
1.77% increased
1.36% increased
0.03% increased
0.12% increased
no change
no change
no change
no change
0.02% decreased
no change
no change
0.04% increased
0.04% increased
0.13% increased
0.08% increased
0.15% increased

Decrease
in Running
Time
67%
90%
89%
70%
72%
53%
54%
11%
73%
65%
77%
41%
82%
90%
79%
75%
70%
83%
83%
30%
92%
99%
98%
98%

T HE EFFECT OF D SLM FOR NODE DELETIONS

Algorithm

Dataset

Base (#
of Nodes)

dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLM
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS
dSLMEVS

Twitter
Twitter
Twitter
Twitter
GPlus
GPlus
GPlus
GPlus
Youtube
Youtube
Youtube
Youtube
Twitter
Twitter
Twitter
Twitter
GPlus
GPlus
GPlus
GPlus
Youtube
Youtube
Youtube
Youtube

81,306
81,306
81,306
81,306
107,614
107,614
107,614
107,614
1,157,828
1,157,828
1,157,828
1,157,828
81,306
81,306
81,306
81,306
107,614
107,614
107,614
107,614
1,157,828
1,157,828
1,157,828
1,157,828

#
of
Nodes
Added
10
100
1,000
10,000
10
100
1,000
10,000
100
1000
10000
100000
10
100
1,000
10,000
10
100
1,000
10,000
100
1000
10000
100000

Change in Modularity Value
0.06% increased
no change
0.02% increased
0.03% decreased
0.35% decreased
0.37% decreased
0.35% decreased
0.35% decreased
0.28% decreased
0.03% decreased
0.21% decreased
0.09% decreased
0.05% increased
0.02% decreased
0.02% increased
0.05% decreased
0.36% decreased
0.35% decreased
0.36% decreased
0.34% decreased
0.29% decreased
0.01% decreased
0.19% decreased
0.07% decreased

Decrease
in Running
Time
85%
79%
87%
28%
90%
78%
75%
73%
88%
87%
73%
73%
92%
81%
87%
28%
90%
78%
84%
69%
99%
99%
99%
98%

1182

In general, dSLM does not decrease the number iterations
of convergence of SLM, however, it decreases the number
of node movements needed in each iteration of SLM. This
indicates that each iteration of dSLM runs faster than each
iteration of SLM. Therefore, overall running time of dSLM is
less than SLM’s overall execution time. The overall results can
be seen in Table I, II, III and IV.
In order to be able to decrease the overall running time
of dSLM algorithm even more, we added another parameter
called expected modularity value that enables the algorithm
stop when it is reached. We named this kind of new algorithm
as dSLMEVS and made same experiments on it with this
new parameter set to the modularity value resulted from SLM
algorithm. By this new algorithm and parameter, we aimed to
decrease running time as much as possible while keeping the
modularity value unchanged or increased. We reached our aim
and decreased running time drastically and keep modularity
value unchanged or increased as seen in all of the tables.
VI.

C ONCLUSION

Waltman & Van Eck proposed and implemented the SLM
algorithm in order to detect communities in large networks.
We extended their implementation to define the community
structure in a dynamic rather than static way. We made use of
the past calculation results of the SLM algorithm in order to
calculate the current networks community structure. This usage
is the main extension and contribution to the SLM algorithm.
In the basics, it is what extends the SLM to be dSLM.
To sum up, we extended SLM to be incremental and
dynamic by using the historical results of community detection
algorithms for the initial community assignments of the nodes.
Thus, the number of node movement actions tried to maximize
the modularity value is decreased. This led to decrease in
running time of the algorithms. Moreover, it can lead to
decrease in number of iterations to converge. Thus, if the
algorithms run with a constant number of iterations parameter,
the modularity value may result as increased.
ACKNOWLEDGMENT
This research was supported partially by USAF Grant
FA9550-15-1-0004.
R EFERENCES
[1]

[2]

[3]

[4]

[5]

A. Clauset, M. Newman, and C. Moore, “Finding community
structure in very large networks,” Physical Review E, vol. 70,
p. 066111, 2004. [Online]. Available: http://www.citebase.org/cgibin/citations?id=oai:arXiv.org:cond-mat/0408187
R. Guimera, M. Sales-Pardo, and L. Amaral, “Modularity from fluctuations in random graphs and complex networks,” Physical Review E,
vol. 70, no. 2, p. 025101, 2004.
J.
Duch
and
A.
Arenas,
“Community
detection
in
complex networks using extremal optimization,” Physical
Review E, vol. 72, p. 027104, 2005. [Online]. Available:
http://www.citebase.org/abstract?id=oai:arXiv.org:cond-mat/0501368
M. Newman, “Finding community structure in networks using the
eigenvectors of matrices,” Physical Review E, vol. 74, no. 3, p. 36104,
2006.
S. Lehmann and L. K. Hansen, “Deterministic modularity optimization,”
The European Physical Journal B, vol. 60, no. 1, pp. 83–88, 2007.
[Online]. Available: http://dx.doi.org/10.1140/epjb/e2007-00313-2

2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining
[6]
[7]
[8]

[9]

[10]

[11]

[12]

[13]

[14]
[15]

[16]
[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]
[25]

[26]

[27]

J. Lee, S. P. Gross, and J. Lee, “Mod-csa: Modularity optimization by
conformational space annealing,” CoRR, vol. abs/1202.5398, 2012.
V. Blondel, J. Guillaume, R. Lambiotte, and E. Mech, “Fast unfolding
of communities in large networks,” J. Stat. Mech, p. P10008, 2008.
R. Rotta and A. Noack, “Multilevel local search algorithms for modularity clustering.” ACM Journal of Experimental Algorithmics, vol. 16,
2011.
L. Waltman and N. J. van Eck, “A smart local moving algorithm
for large-scale modularity-based community detection.” CoRR, vol.
abs/1308.6604, 2013.
U. Brandes, D. Delling, M. Gaertler, R. Goerke, M. Hoefer,
Z. Nikoloski, and D. Wagner, “On modularity clustering,” IEEE Transactions on Knowledge and Data Engineering, vol. 20, no. 2, pp. 172–
188, 2008.
D. Aloise, S. Cafieri, G. Caporossi, P. Hansen, L. Liberti, and S. Perron,
“Column generation algorithms for exact modularity maximization in
networks,” Physical Review E, vol. 82, no. 4, article, pp. –, jan 2010.
G. Xu, S. Tsoka, and L. G. Papageorgiou, “Finding community
structures in complex networks using mixed integer optimisation,” The
European Physical Journal B, vol. 60, no. 2, pp. 231–239, 2007.
[Online]. Available: http://dx.doi.org/10.1140/epjb/e2007-00331-0
A. Clauset, M. E. J. Newman, and C. Moore, “Finding
community structure in very large networks,” Phys. Rev.
E, vol. 70, p. 066111, Dec 2004. [Online]. Available:
http://link.aps.org/doi/10.1103/PhysRevE.70.066111
P. Holme and J. Saramäki, “Temporal networks,” Physics Reports, vol.
519, no. 3, pp. 97–125, 2012.
T. Yang, Y. Chi, S. Zhu, Y. Gong, and R. Jin, “Detecting communities
and their evolutions in dynamic social networks - a bayesian approach.”
Machine Learning, vol. 82, no. 2, pp. 157–189, 2011.
X. Tang and C. C. Yang, “Dynamic community detection with temporal
dirichlet process.” in SocialCom/PASSAT. IEEE, 2011, pp. 603–608.
D. Chakrabarti, R. Kumar, and A. Tomkins, “Evolutionary clustering,”
in Proceedings of the 12th ACM SIGKDD international conference
on Knowledge discovery and data mining, ser. KDD ’06. New
York, NY, USA: ACM, 2006, pp. 554–560. [Online]. Available:
http://doi.acm.org/10.1145/1150402.1150467
M.-S. Kim and J. Han, “A particle-and-density based evolutionary
clustering method for dynamic networks.” PVLDB, vol. 2, no. 1, pp.
622–633, 2009.
D. Greene, D. Doyle, and P. Cunningham, “Tracking the evolution of
communities in dynamic social networks.” in ASONAM, N. Memon and
R. Alhajj, Eds. IEEE Computer Society, 2010, pp. 176–183.
P. Brodka, S. Saganowski, and P. Kazienko, “Group evolution discovery
in social networks,” in Advances in Social Networks Analysis and
Mining (ASONAM), 2011 International Conference on, 2011, pp. 247–
253.
T. Dinh, Y. Xuan, and M. Thai, “Towards social-aware routing in
dynamic communication networks,” in Performance Computing and
Communications Conference (IPCCC), 2009 IEEE 28th International,
Dec 2009, pp. 161–168.
T. Aynaud and J.-L. Guillaume, “Static community detection algorithms
for evolving networks,” in Modeling and Optimization in Mobile, Ad
Hoc and Wireless Networks (WiOpt), 2010 Proceedings of the 8th
International Symposium on, May 2010, pp. 513–519.
M. Newman, “Modularity and community structure in networks,” Proceedings of the National Academy of Sciences, vol. 103, no. 23, pp.
8577–8582, 2006.
S. Fortunato, “Community detection in graphs,” Physics Reports, vol.
486, pp. 75–174, 2010.
M. E. J. Newman, “Analysis of weighted networks,” Phys. Rev.
E, vol. 70, no. 5, p. 056131, Nov. 2004. [Online]. Available:
http://pre.aps.org/abstract/PRE/v70/i5/e056131
A. Arenas, J. Duch, A. Fernandez, and S. Gmez, “Size reduction of complex networks preserving modularity,” CoRR, vol.
abs/physics/0702015, 2007.
E. A. Leicht and M. E. J. Newman, “Community structure in directed
networks,” Phys. Rev. Lett., vol. 100, no. 11, p. 118703, Mar. 2008.
[Online]. Available: http://prl.aps.org/abstract/PRL/v100/i11/e118703

1183

[28]

[29]
[30]

[31]

[32]

S. Fortunato and M. Barthlemy, “Resolution limit in community detection,” Proceedings of the National Acadamy of Sciences of the United
States of America (PNAS), vol. 104, no. 1, pp. 36–41, 2007.
J. Reichardt and S. Bornholdt, “Statistical mechanics of community
detection,” Arxiv preprint cond-mat/0603718, 2006.
V. A. Traag, P. Van Dooren, and Y. Nesterov, “Narrow scope for
resolution-limit-free community detection,” Physical Review E, vol. 84,
no. 1, p. 016114, 2011.
L. Waltman, N. J. van Eck, and E. C. Noyons, “A unified
approach to mapping and clustering of bibliometric networks,”
Journal of Informetrics, vol. 4, no. 4, pp. 629–635, 2010. [Online]. Available: http://www.sciencedirect.com/science/article/B83WV50RFN28-1/2/809f89176e8076cac3862b6589bc6fd5
A. Mislove, M. Marcon, K. P. Gummadi, P. Druschel, and B. Bhattacharjee, “Measurement and Analysis of Online Social Networks,” in
Proceedings of the 5th ACM/Usenix Internet Measurement Conference
(IMC’07), San Diego, CA, October 2007.

SocialCom/PASSAT/BigData/EconCom/BioMedCom 2013

MultiScale Modeling of Islamic Organizations in UK
Nyunsu Kim∗ , Sukru Tikves∗ , Zheng Wang∗ , Jonathan Githens-Mazer† and Hasan Davulcu∗
∗ CIDSE,

Arizona State University, Tempe, AZ, USA
of Politics, University of Exeter, Exeter, UK

† Department

Abstract—In this paper we utilize an efﬁcient sparse inverse
covariance matrix (precision matrix) estimation technique to
identify a set of highly correlated discriminative perspectives. We
develop a ranking system that utilizes ranked perspectives to map
26 UK Islamic organizations on a set of socio-cultural, political
and behavioral scales based on their web corpus. We create a gold
standard ranking of these organizations through an expertise
elicitation tool. We compute expert-to-expert agreements, and
we present experimental results comparing the performance of
the QUIC based scaling system to another baseline method. The
QUIC based algorithm not only outperforms the baseline method,
but it is also the only system that consistently performs at area
expert-level accuracies for all scales.

I.

use a combination of ethnographic, discourse analysis, and
computational methods as well as a case study involving 26
Islamic groups from the United Kingdom (UK). The model
presented here aims to broaden the base of discussion and
analysis, recapture and build upon previous observations, and
establish a general framework within which critically needed
comparative studies looking at both violent and non-violent
groups can be conducted.
One of the fundamental issues with interpretative and
qualitative data collection and analysis of groups and social
movements has been the researchers’ bias while conducting
the research. Goertz [3] makes the crucial point that, in
their enthusiasm for reifying complex sociological, cultural or
political concepts, theorists and empiricists often focus too
much on what a concept is, rather than on identifying the
concept on a continuum, in order to assess when a concept
is present versus when it is absent.

I NTRODUCTION

We propose a multi-scaling based methodology that represents an important step change in how we might observe
and analyze radical and counter-radical Islamic groups in any
speciﬁc region. Rather than placing external forms of analysis
that color and tautological deﬁne what is radical or not, we
propose a more ontologically oriented approach. We seek to
develop a methodology to allow the orientations of these
groups to deﬁne themselves via their own discourse within
their own universe and understanding of actions, rather than
an external and potentially poorly calibrated analysis of what
constitutes radical. Without this kind of fundamental reorientation to research of religiously or politically inspired groups,
we get the poor assumption based analysis that (incorrectly)
predicts and champions ill-deﬁned relationships between certain religious or political sects and violence, for example.
With our reorientation of approach, we are more fundamentally
able to examine such relationships in a way that should allow
researchers to take other kinds of nuance and understanding
into account.

In the social sciences, scaling is the process of measuring
and ordering actors (subjects) with respect to quantitative
attributes or traits (items). In this paper, we present graphical tools and computational techniques so that both social
movements (subjects) and their socio-economic, political, or
religious beliefs, goals and practices (items) can be mapped
simultaneously on a set of continuous scales via expert inputs
and also via algorithms.
A Guttman scale [4] utilizes a number of items, corresponding to socio-cultural, political beliefs, goals and
practices, and each group’s dichotomous response, e.g.
agree/disagree. Guttman scaling procedure is based on the
premise that items can be ranked in some order so that, for a
rational respondent, the response pattern can be captured by a
single index on the ordered scale. The Guttman pattern appears
on the response tables of groups (subjects) when perspectives
(items) can be arranged in an order on a scale so that an
organization who voices a particular perspective also voices
most of the other perspectives of lower rank-order. In order
to synthesize high accuracy multi-scaling models that can
process large document collections (tens of thousands) from
a large number of organizations’ web sites we need scalable
algorithms (i) to rapidly identify highly correlated subsets of
discriminant perspectives and (ii) to rank both discriminant
perspectives (items) and organizations (subjects) according to
neutral-to-extreme positions on any scale accurately.

In the case of Islamic social movements, Edward Said [1]
observed, the boundary between political rhetoric and scholarship concerning Islam is often blurred. The problem is particularly acute when it comes to the study of violent forms of political Islam and others deemed to be potentially violent. Much
of the analytic and policy oriented literature relies on binary
distinctions such as “radical/moderate”, “modern/traditional”,
“conservative/progressive” etc. Binary models map enormous
diversity into ill-deﬁned categories that often measure a mix
of attitudes about democracy, secularism, attitudes about the
West and proclivity to violence.

The contributions of this paper can be summarized as
follows:

This paper explores these problems and presents a multiscale model based on more precise and objective criteria
that can be used to evaluate and compare movements in
diverse cultural, historical, and political contexts and how they
change over time. The analysis and modeling efforts build on
previous studies [2] of change oriented social movements and
978-0-7695-5137-1/13 $26.00 © 2013 IEEE
DOI 10.1109/SocialCom.2013.8

•

13

First we utilize an efﬁcient sparse inverse covariance
matrix (precision matrix) estimation [5] technique
to identify a sorted subset of perspectives that are
likely to reveal a Guttman pattern in the corpus of

organizations, and hence suitable for utilization as
items during scaling. The QUIC algorithm presented
in Section III-B has superlinear convergence - it uses
O(log(1/e)) iterations for error e, which makes it
suitable for large-scale problems.
•

•

that all variants of a religious tradition are constructed in
historical, social, and cultural contexts and they can, and
indeed must, change over time. Proponents of this position
maintain that to determine the meaning of a scriptural passage
appropriate for a particular time, place, and culture, both the
context of revelation and the context of exegesis must be
considered.

Second, we provide experimental results showing that
for a corpus of nearly 10,000 documents downloaded
from 26 UK Islamic organizations’ web sites, the
QUIC algorithm consistently identiﬁes subsets of discriminant perspectives that reveal the Guttman pattern
by showing that the corresponding Rasch models ﬁts
the data using the Andersens LR-test [6].

Religious Diversity Tolerance: Exclusivists, who insist
on universal adherence to their own beliefs and social norms
and who claim exclusive possession of complete truth, are
at one end. Pluralists, who understand difference as a social
and religious good or theological pluralism, are at the other.
An entity at the extreme pluralist end of the tolerance scale
holds the view that all religions should be tolerated and that all
are based on truths that transcend confessional and sectarian
differences.

Third, we show that a heuristic ranking technique
based on the QUIC algorithm performs at higher
accuracy than Rasch model itself while performing
at area expert-level accuracies in ranking 26 British
Islamic organizations on all six socio-cultural political
and behavioral scales presented below in Section II.

Change Orientation: Change orientation aims to capture
the degree to which an entity wishes to effect social, political,
and/or religious change. It is also a measure of the degree
to which an individual or group attempts to inﬂuence others.
Revitalization movements [9] that seek to destroy the world
as it is and rebuild it from scratch are at one end of the scale.
Defenders of the social, political, and religious status quo are
at the other end.

Rest of the paper is organized as follows: Section II
presents related social theory and a multi-scale model of
Islamic organizations developed in collaboration with social
scientists on our team. Section III provides brief descriptions
of the techniques utilized in our item selection and scaling algorithms. Section IV describes the overall system architecture.
Section V presents the UK case study, experimental design and
evaluations.
II.

Violence Ideology: Violence is deﬁned broadly to include
more than killing, inﬂicting physical injury, and destruction
of property. Symbolic and discursive violence are included
in this scale because they are often steps leading toward
physical violence. They can cause havoc, especially when
the manipulation of symbols and discourse is purposively
articulated to provoke adversaries, demonize opponents, incite
mobs to action, or to provide justiﬁcations for the necessity
of violence. Unlike physical violence that can be seen and
clearly understood for what it is, symbolic and discursive
violence are not necessarily self-evident; hence both require
knowledge of their contexts to identify them and assess their
real and potential danger. Dehumanization, demonization, and
the desecration of sacred places and objects are among the
most common and provocative forms of symbolic violence
committed in contexts of ethnic and religious conﬂict.

M ULTI -S CALE M ODELING OF S OCIAL M OVEMENTS

Radicalism is the ideological conviction that it is acceptable
and sometimes obligatory to use violence to effect profound
political, cultural and religious transformations and to change
the existing social order fundamentally. Radical movements
have complex origins and depend on diverse factors that
enable the translation of their radical ideology into social,
political and religious movements [7]. Crelinsten [8] states,
“both violence and terrorism possess a logic and grammar that
must be understood if we are to prevent or control them.”
Binary labeling does not capture the overlaps, movement
and interactivity among these actors. The model described
below deﬁnes a six dimensional possibility space within which
diverse organizations, social movements, and individuals can
be located. The variables are treated as continuous bipolar
scales. Each scale is measured independently of the others.

III.

M ULTI -S CALE M ODELING T ECHNIQUES

A. Guttman Pattern Detection
We summarize the process we used to automatically select
a subset of discriminant perspectives that can (a) well classify
the two different classes of the documents corresponding
to different polarities of each scale and (b) approximately
satisfy Guttman scaling requirements [10]. The following steps
describe our implementation:

The choice of scales relies on the work of a combination
of American, European, African, and Southeast Asian scholars
and the literature on similar movements in various regions. The
variables are generalizations based on ethnographic research
that involved observation of public events, extended interviews
and informal conversations with leaders and rank and ﬁle members of organizations and movements, and online discourse
analysis. The scales used in our model for characterizing
diverse Islamic movements are:

1)

Epistemology: This refers to the ways in which religious
groups interpret core texts. Foundationalism is at one end of a
continuum. It ﬁxes meaning in invariant, “literal” readings of
core religious texts. Foundationalists claim that their readings
are ahistorical and not inﬂuenced by cultural considerations.
Constructivism is at the other end of the scale. It acknowledges

2)

3)

14

For each topic relevant to a scale, calculate the frequency of the keywords co-occurring with the topic
phrase in a document.
Use a sparse regression method with logistic loss
discussed in previous section to learn the discriminant
perspectives for each class using SLEP logistic sparse
learning function [11].
Use the identiﬁed perspectives to create a document
x perspective matrix. Use QUIC algorithm presented

4)

5)
6)

which is convex but non-differentiable.

below to learn a sparse inverse covariance matrix of
the perspectives. In this matrix, the non-zero terms
indicate that the corresponding pairs of perspectives
are conditionally dependent.
Threshold the elements of the inverse covariance
matrix with a small value (0.05 was used in our
experiments): If the absolute value of the element is
smaller than this value, substitute it with 0, otherwise
substitute it with 1.
Rank the selected perspectives, showing 1’s in their
respective rows, in descending order by the number
of their conditionally dependent perspectives.
Rank the organizations by the number of perspectives
observed in their response tables.

In the (t + 1)-th step, as we have obtained the estimate Θt ,
the log determinant of (Θt + Δ) can be approximated as
log det(Θt + Δ)

−1
− 12 trace(Θ−1
t ΔΘt Δ).

Let Wt = Θ−1
t . Deﬁne the second-order approximation of
g(Θ) = g(Θt + Δ) as ḡΘt (Δ). It is written as
ḡΘt (Δ)

Then the Newton direction Dt for the entire objective f (Θ)
can be written as the solution of the following problem:
Dt = arg min ḡΘt (Δ) + h(Θt + Δ).

By assuming the data are independently distributed according to Gaussian distribution N (0, Σ), a zero in an offdiagonal element of Σ−1 corresponds to a pair of variables that
are conditionally independent given all other variables [12],
[13]. For example, if Σ−1 (i, j) = 0, then the variable i and
variable j are conditionally independent. Therefore, we use the
inverse covariance matrix of the perspectives to represent the
concurrent relationships among the corresponding perspective
pairs. Quadratic Approximation Method for Sparse Inverse
Covariance Learning (QUIC) [5] is a very efﬁcient method
to estimate a sparse inverse covariance matrix for the features
of a given sample set.

Δ

i=1

log √

∝ log det(Σ

1
e
(2π)p |Σ|
−1

To compute the Newton direction is an 1 regularized
least squares problem, which is also called Lasso [11]. It is
time consuming for directly solving (2). QUIC adapts the
coordinate descent and a screening heuristic to accelerate this
optimization procedure [5]. It is proved that QUIC has superlinear convergence, which is suitable for large-scale problems.
The implementation of this algorithm is available online1 .
C. Rasch Model
Rasch model [14] provides a probabilistic framework for
Guttman scaling to accommodate incomplete observations and
measurement errors. In Rasch model, the probability of a
speciﬁed binary response (e.g. a subject agreeing or disagreeing with an item) is modeled as a function of subject’s and
item’s parameters. Speciﬁcally, in the simple Rasch model,
the probability of a positive response (yes) is modeled as a
logistic function of the difference between the subject and
item’s parameters. Item parameters pertain to the difﬁculty
of items while subject parameters pertain to the ability of
subjects who are assessed. A subject of higher ability relative
to the difﬁculty of an item, has higher probability to respond
to an item afﬁrmatively. In this paper Rasch models are used
to assess the organizations’ degree of radicalism or counterradicalism on six scales based on the religio-social perspectives
(items) appearing in their online rhetoric.

xi Σ−1 xT
i
−
2

) − trace(SΣ−1 ),

where S = cov(X) = XT X/n is the empirical covariance
matrix for the samples X. QUIC uses the maximum likelihood
principle to estimate the inverse covariance matrix Θ = Σ−1 ,
with an extra sparse regularization term as follows:
min − log det(Θ) + trace(SΘ) + λ|Θ|1 ,
Θ0

(2)

QUIC computes the Newton direction iteratively with a proper
step-size, which is selected by Armijo-rule, until it ﬁnds a
satisfactory estimate of Θ.

Given the samples X from Gaussian distribution N (0, Σ),
the log likelihood of these data is
n
log P (X) = i=1 log P (xi )
n

−1
= trace((S − Wt )Δ) + 12 trace(Θ−1
t ΔΘt Δ)

− log det(Θt ) + trace(SΘt ).

B. QUIC: QUadratic Inverse Covariance

=

≈ log det(Θt ) + trace(Θ−1
t Δ)

(1)

λ is a nonnegative tunable parameter which controls the
sparsity of the matrix Σ.
QUIC solve the problem in (1) iteratively based on Newton method, by using the second-order information. In each
iteration, it uses a quadratic approximation for the objective
function around the current estimated matrix Θt , and ﬁnds
the Newton direction Dt for the next estimate by solving a
regularized quadratic program.

D. Applying Rasch Model in the Text Mining Domain
In this paper, we use Guttman scaling with Rasch model to
ﬁnd rankings of 26 Islamic organizations in the UK based on
extremity of their perspectives on six bipolar scales presented
in Section II. In our application, Rasch model subjects correspond to a group of religious organizations, and items correspond to a set of conditionally dependent discriminant perspectives on socio-cultural, political, religious beliefs, goals and
practices. An organization responding “yes” to a perspective
means the organization exhibit that perspective prominently
in its narrative, while an organization responding “no” to a

We denote the objective function as
f (Θ) = − log det(Θ) + trace(SΘ) + λ|Θ|1 .
It contains two parts f (Θ) = g(Θ) + h(Θ), where
g(Θ) = − log det(Θ) + trace(SΘ),
which is twice differentiable and strictly convex, and

1 http://www.cs.utexas.edu/∼sustik/QUIC/

h(Θ) = λ|Θ|1 ,
15

perspective indicates that the organization does not exhibit that
perspective prominently. In our model difﬁculty of an item
corresponds to the strength of the corresponding attitude in
deﬁning neutral-to-extreme position of any organization on a
scale. Similarly ability of a subject, in this case, means the
degree of polarization exhibited by an organization’s rhetoric
on a continuous scale.
IV.

Fig. 3.

A sample guttman pattern identiﬁed by the QUIC method.

S YSTEM A RCHITECTURE
C. Scaling with QUadratic Inverse Covariance (QUIC)

Fig. 1.

We would like to identify the perspective characteristics of
varying degrees of polarization, from neutral to more extreme
positions, on either side of each scale. A Guttman scale [4]
presents a number of items, corresponding to socio-cultural,
political beliefs, goals and practices, if these items can be
ranked in some order so that, for a rational respondent, the
response pattern can be captured by a single index on that
ordered scale. In other words, on a Guttman scale, items can
be arranged in an order so that an organization who voices a
particular item also voices most of the other items of lower
rank-order. We utilize the sparse inverse covariance estimation
technique presented in Section III-B to identify the candidatesorted subset of perspectives that are likely to reveal a Guttman
pattern and hence are suitable for utilization as reliable items
in Guttman scaling. A sample probabilistic Guttman pattern
discovered by the QUIC algorithm in the response table
of the UK organizations for the Violence Ideology scale is
shown above in Figure 3 – where rows correspond to sorted
organizations, and columns correspond to sorted items, and
each dot represents an afﬁrmative response of an organization
for an item.

The system architecture.

The experimental and evaluation data consists of positions
of 26 UK based Islamic religious organizations on six scales
scaled by three independent experts, topic-to-scale mapping
information provided by experts, and an online web corpus
of nearly 10,000 documents downloaded from the web sites
of these organizations. The steps for processing and scaling
these 26 UK Islamic organizations is summarized in Figure 1.
1)
2)
3)
4)
5)

Currently six scales are used in this study.
Experts identify the relevant list of organizations.
Web sites of these organizations are downloaded.
A text mining system identiﬁes top 100 n-grams as
candidate topics from each web site.
Experts map relevant topics to scales.

D. Response Tables of Organizations
A response table is calculated based on the normalized
frequency with which organizations voice various perspectives
in their web sites. The median frequency of each perspective
is used as a threshold. Organizations’ normalized perspective
frequencies and the threshold of each perspective are used to
build a dichotomous [0/1] response matrix as the organizations’
response table.

A. Graphical Scaling Tool
We built a graphical scaling tool to collect and record the
opinions of area experts to be used for mining discriminant
perspectives and for evaluating our scaling algorithms (Section II). Each expert independently classiﬁed and ranked all the
UK organizations relative to each other on all six linear bipolar
scales described in Section II. The positions of each individual
organization provided by three experts are then averaged to
generate the gold standard of rankings of all organizations on
all scales.

E. Ranking with Rasch Modeling
A true Guttman scale is deterministic, i.e. if an actor
subscribes to a certain perspective, then it must also agree
with all lower order perspectives on the scale. But, perfect
order is rare in the real world. The Rasch [14] model provides
a probabilistic framework for Guttman scales to accommodate
incomplete observations and measurement error. We employed
the Rasch model presented in Section III-C to rank both the
organizations (subjects) and corresponding perspectives (items)
on each scale as an alternative ranking algorithm alongside the
QUIC algorithm. Rasch Modeling algorithm2 also produces
a metric [6] to validate the ﬁtness of the model. A p-value,
returned by the test, indicates the goodness of ﬁt and a pvalue3 higher than 0.05 indicates no presence of lack of ﬁt.

B. Perspective Mining
A debate on a topic is a formal discussion in which
opposing perspectives are put forward. In this step, our focus is
the determination of discriminant topic-speciﬁc perspectives,
which would contribute to understanding of features (i.e.
social, political, cultural, religious beliefs, goals, and practices)
shared by one side of a debate, and by those opposing
them. We formulate the perspective mining problem in a general structured sparse learning framework as an optimization
problem. The keyword phrases with non-zero values on the
minimized solution vector yields the discriminant perspectives.
Figure 4 on the next page shows radical-Islamist and counter
radical-Islamist perspectives identiﬁed by the algorithm for ﬁve
sample topics of the Violence Ideology scale.

2 http://r-forge.r-project.org/projects/erm/
3 http://en.wikipedia.org/wiki/P-value

16

Fig. 2.

A sample set of radical and counter-radical perspectives for ﬁve different topics on the Violence Ideology scale.

V.

C. Computationally Generated Scales

E XPERIMENTAL E VALUATIONS

The organizational rankings discovered by both the Rasch
model and the QUIC algorithm have been evaluated against the
gold standard rankings of the experts by using the following
displacement error measure deﬁned in Equation 3.

A. UK Corpus
The experimental corpus comprises articles published online by the 26 Islamic organizations identiﬁed in the UK.
Online sources correspond to web sites, RSS and Tweet
feeds, and blogs of known leaders of these organizations.
We downloaded a total of 10,521 articles published by these
organizations. For HTML pages, the boilerpipe toolkit4 was
used to clean the headers, footers and extract plain text.


error(G, R) =

o∈O

|G(o)−R(o)|
|O|

|O|

(3)

Here, O is the set of organizations, G and R are one to one
mapping functions of rankings from set O to range [1, |O|]. For
two exactly matching rankings, the error(G, R) will be zero,
whereas for two inversely sorted rankings it will be 0.5 (when
the size of O is even). A random ranking is expected to have
an error measure of 0.375.

B. Expert Opinion and Gold Standard of Rankings
We collaborated with three highly trained area experts with
social science and British and Islamic cultural knowledge. In
order to build a gold standard of rankings of these organizations, each expert independently used a graphical scaling tool
to rank the organizations relative to every other organization.

D. Scale Evaluation
We calculated the displacement error between each expert’s
ranking and the consensus gold standard of rankings. For the
epistemology scale, the ﬁrst expert’s displacement error is
0.127, and the second and third experts’ displacement errors
are 0.319 and 0.148 correspondingly as shown in the last
row of the table in Figure 4. The average error of all three
experts against the gold standard ranking is 0.198. The Rasch
model shows a displacement error of 0.299, which is better
than random ranking. The QUIC algorithm performed like an
experts’ ranking with a displacement error measure of 0.175,
which beats the ranking performance of both the Rasch model
and the average displacement error of our three experts.

Each expert independently ranked the organizations that
they are familiar with according to six socio-cultural, political
and ideological scales. The individual scores for each organization by each expert were combined and averaged to obtain the
consensus gold standard rankings on each of the six scales. A
random ordering would theoretically amount to an error rate of
0.375 according to the displacement error measure we deﬁned
in Equation 3. The consensus rankings among our three experts
was high; since their average error rate compared to the gold
standard of all organizations were 0.198 for the epistemology
scale, 0.146 for the religious diversity tolerance scale, 0.145
for the political change scale, 0.127 for the religious change
scale, 0.113 for the social change scale, and 0.127 for the
violence ideology scale.

For the Political Change scale, the ﬁrst expert’s displacement error is 0.068, and the second and third experts’ displacement errors are 0.324 and 0.041 correspondingly. The average
error of all three experts against the gold standard ranking is
0.144. The Rasch model shows a displacement error of 0.225,
which is better than random ranking. The QUIC algorithm
performed like an expert’s ranking with a displacement error
measure of 0.198 falling within the experts’ error range of

The version of the graphical scaling tool that our
experts used is online at: http://www.minerva-project.org/
DataCollector.
4 https://code.google.com/p/boilerpipe/

17

average error of all three experts against the gold standard
ranking is 0.127. The Rasch model shows a displacement
error of 0.214, which is better than random ranking. The
QUIC algorithm performed like an expert’s ranking with a
displacement error measure of 0.213 falling within the experts’
error range.
VI.

C ONCLUSION

Scaling with the QUIC algorithm consistently performs
at area expert-level accuracies for all the evaluated scales
used for modeling the UK Islamic organizations. This preliminary analysis with all six scales show that when experts
can bootstrap the system with a list of organizations and
assist it with topic-to-scale mapping, then the web corpus of
these organizations provides sufﬁcient information to enable
a computational method to rank and model organizations at
area expert-level accuracies. Our future work includes investigations on automated discovery of new and emerging
groups, as well as utilization of clustering techniques using the
inverse covariance matrix to automatically synthesize scales
representing highly correlated sets of topics.
Fig. 4.

Computational and expert rankings of epistemology scale

ACKNOWLEDGMENT
The authors would like to thank our three area experts: Our
co-author Professor Jonathan Githens-Mazer, a political scientist at Institute of Arab and Islamic Studies and Department
of Politics at University of Exeter, UK. Dr. Zacharias Pieri,
a post-doc at University of South Florida, USA. Dr. Sajjad
Rizvi, Professor of Islamic Intellectual History, and Director
of Education at University of Exeter, UK.

0.041 and 0.324. It also beats the ranking performance of the
Rasch model.
For the Religious Change scale, the ﬁrst expert’s displacement error is 0.089, and the second and third experts’
displacement errors are 0.256 and 0.036 correspondingly. The
average error of all three experts against the gold standard
ranking is 0.127. The Rasch model shows a displacement
error of 0.186, which is better than random ranking. The
QUIC algorithm performed like an expert’s ranking with a
displacement error measure of 0.183 falling within the experts’
error range of 0.041 and 0.324. QUIC also beats the ranking
performance of the Rasch model.

R EFERENCES
[1]
[2]
[3]

For the Social Change scale, the ﬁrst expert’s displacement
error is 0.068, and the second and third experts’ displacement
errors are 0.195 and 0.077 correspondingly. The average
error of all three experts against the gold standard ranking
is 0.113. The Rasch model shows a displacement error of
0.163, which is better than random ranking. Both the QUIC
algorithm and the Rasch model performed like an expert’s
ranking performance with a displacement error measure of
0.163 falling within the experts’ error range of 0.068 and 0.195
for this scale.

[4]
[5]

[6]
[7]
[8]

For the Religious Diversity Tolerance scale, the ﬁrst expert’s displacement error is 0.092, and the second and third
experts’ displacement errors are 0.219 and 0.127 correspondingly. The average error of all three experts against the gold
standard ranking is 0.143. The Rasch model shows a displacement error of 0.194, which is better than random ranking. The
QUIC algorithm performed like an expert’s ranking with a
displacement error measure of 0.169 falling within the experts’
error range. QUIC also beats the ranking performance of the
Rasch model.

[9]
[10]
[11]

[12]
[13]

For the Violence Ideology scale, the ﬁrst expert’s displacement error is 0.062, and the second and third experts’
displacement errors are 0.248 and 0.071 correspondingly. The

[14]

18

E. Said, Orientalism. Pantheon Books, 1978.
A. Wallace and R. Grumet, Revitalizations and Mazeways: Essays on
Culture Change, Volume 1. University of Nebraska Press, 2003.
G. Goertz, Social science concepts : a users guide.
Princeton
University Press, 2006.
L. Guttman, “The basis for scalogram analysis,” Measurement and
prediction, vol. 4, pp. 60–90, 1950.
C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar, “Sparse
inverse covariance matrix estimation using quadratic approximation,”
vol. 24, 2011, pp. 2330–2338.
E. B. Andersen, “A goodness of ﬁt test for the rasch model,” Psychometrika, vol. 38, pp. 123–140, 1973.
J. Githens-Mazer, “The rhetoric and reality: radicalization and political
discourse,” vol. 33, no. 5, 2012, pp. 556–567.
R. Crelinsten, “Analysing terrorism and counter-terrorism: A communication model,” Terrorism and Political Violence, vol. 14, pp. 77–122,
2002.
A. Wallace, “Revitalization movements,” American Anthropologist,
vol. 58, pp. 264–281, 1956.
J. McIver and E. Carmines, Unidimensional Scaling. Sage Publications,
Inc, 1981, vol. 24.
J. Liu, J. Chen, and J. Ye, “Large-scale sparse logistic regression,” in
Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 2009, pp. 547–556.
M. Yuan and Y. Lin, “Model selection and estimation in the gaussian
graphical model,” Biometrika, vol. 94, pp. 19–35, 2007.
J. Friedman, T. Hastie, and R. Tibshirani, “Sparse inverse covariance
estimation with the graphical lasso,” Biostatistics, vol. 9, pp. 432–441,
2008.
D. Andrich, Rasch models for measurement. Sage, 1988.

Adaptable Situation-Aware Secure Service-Based (AS3) Systems
1

S. S. Yau, 1H. Davulcu, 2S. Mukhopadhyay, 1D. Huang and 1Y. Yao
1
Arizona State University, Tempe, AZ
2
West Virginia University, Morgantown, WV
1
{yau, hasan.davulcu, dazhi.huang, yisheng.yao}@asu.edu
2
supratik.mukhophadyay@mail.wvu.edu
Abstract

Service-oriented systems are distributed systems
which have the major advantage of enabling rapid
composition of distributed applications, regardless of
the programming languages and platforms used in
developing and running different components of the
applications. In these systems, various capabilities are
provided by different organizations as services
interconnected by various types of networks. The
services can be integrated following a specific
workflow to achieve a mission goal for users. For
large-scale service-based systems involving multiple
organizations, high confidence and adaptability are of
prime concern in order to ensure that users can use
these systems anywhere, anytime with various devices,
knowing that their confidentiality and privacy are well
protected and the systems will adapt to satisfy their
needs in various situations. Hence, these systems must
be adaptable, situation-aware and secure. In this
paper, an approach to rapid development of adaptable
situation-aware secure service-based (AS3) systems is
presented. Our approach enables users to rapidly
generate, discover, compose services into processes to
achieve their goals based on the situation and adapt
these processes when situation changes.
Keywords: Service-based systems, adaptive workflow
planning, situation-awareness, distributed trust
management, specification language, verification.

1. Introduction
Service-based systems are distributed computing
systems which have the major advantage of SOA is its
capability of enabling rapid composition of distributed
applications, regardless of programming languages and
platforms used in developing and running different
components of the applications. These systems are
being adopted in many large-scale distributed systems,
such as Grid and Global Information Grid (GIG) [2],

for various applications including collaborative
scientific and engineering work, e-business, healthcare,
military, and homeland security. In service-based
systems, various capabilities are provided by different
organizations as services, which are software/hardware
entities with well-defined interfaces to provide certain
capability over wired or wireless networks. The
services can be integrated following a specific
workflow, which is a series of cooperating and
coordinated activities designed to carry out a welldefined process to achieve a mission goal for users. For
large-scale service-based systems, high confidence and
adaptability are of prime concern. It is very important
to ensure that users can use these systems anywhere,
any time using various devices (ranging from handheld
devices to PCs), knowing that their confidentiality and
privacy are well protected and the systems will adapt to
their needs in various situations. Therefore, these
systems must have the following properties:
(1) Adaptability. In these systems, services may
become unavailable due to distributed denial-of-service
attacks or system failures, and new processes may be
created in runtime to fulfill users’ new mission goals.
Hence, the systems must have the capabilities to
change their configurations to provide continuous
availability, or to adapt their behavior to satisfy the
new goals in dynamic environments.
(2) Situation-awareness (SAW). SAW is the
capability of being aware of situations and adapting the
system’s behavior based on situation changes [3, 4]. A
situation is a set of context attributes over a period of
time that is relevant to future system behavior [3, 4]. A
context is any instantaneous, detectable, and relevant
property of the environment, the system or users, such
as time, location, wind velocity, temperature, available
bandwidth, invocation of action, and a user’s schedule
[3, 4]. SAW is essential for high confidence and
adaptable service-based systems since it is needed for
determining adaptive processes to achieve users’ goal,
and enforcing flexible security policies.

Proceedings of the Eighth IEEE International Symposium on Object-Oriented Real-Time Distributed Computing (ISORC’05)
0-7695-2356-0/05 $ 20.00 IEEE

(3) Security. To provide high confidence to users, these
systems must have the capabilities of authenticating
users and service providers, verifying the integrity of
services, protecting the confidentiality of information,
controlling the access to services based on security
policies, and detecting malicious services and users.
Although various techniques have been proposed to
improve the security and dynamic service composition
of service-based systems [5-9], so far there are no
effective enabling techniques for developing Adaptable
Situation-aware Secure Service-based (AS3) Systems.
In this paper, we will first discuss the adaptability,
SAW, and security requirements of service-based
systems and outline our approach to achieving the two
objectives: (1) Enable rapid development of and
provide runtime support for hierarchical SAW and
distributed security policy management in AS3
systems. (2) Enable users to rapidly discover, contract
with and compose reliable and unreliable services into
adaptive processes to achieve their goals based on the
situation. An example will be given to illustrate the
requirements of AS3 systems and the rationale of our
approach.

2. Rationale and Requirements
2.1. Rationale of AS3 Systems
Consider a typical global service-based system
which connects various units, such as aircrafts, ships,
offices, etc, through wired and wireless networks. Each
unit may provide certain services to allow other units
to utilize some of its capabilities. The following “ship
rescue” example using such a global service-based
system shows that the requirements of such an
application naturally can be satisfied by an AS3 system.
Consider the following example: There is a broken
passenger ship (BS) with 200 people on board. Two
nearby ports, P1 and P2, are in two different countries,
and there is a hospital in each port. Two rescue ships,
A and B, are at P1 and P2 respectively. Both rescue
ships have enough capacity to hold 200 people, but
only B has a helicopter H on-board. A rescue center
RC is responsible for coordinating the rescue
operations. The following services are involved in this
example:
x SBS: the service on BS for sending out SOS signal
with various context data, including the number of
life-threatening injuries.
x SH: the rescue service for controlling the helicopter
H on B to perform rescue operations.
x SA: the rescue service for controlling A to perform
rescue operations.
x SB: the rescue service for controlling B to perform
rescue operations. SB may invoke SH.

x SRC: the service at the rescue center RC for receiving
and analyzing information from other services, and
coordinating rescue operations.
Upon detecting the situation “An SOS signal is
detected from ship BS”, SRC automatically generate a
plan to perform the necessary rescue operations for BS.
At the beginning, there is no injury report about the
people from BS. The SRC dynamically discovers
nearby rescue services (SA and SB), and selects SA to
perform the rescue mission since BS is closer to P1.
However, after A is on the way to BS, another report
from BS is received reporting that several people on
BS have life-threatening injuries caused by an
explosion just happened on BS. A is unable to reach
BS and return to P1 fast enough to save the injured
people on BS. Hence, SRC finds SB and SH to rescue the
injured people from BS.
To develop the above example application on a
global service-based system for ensuring success in the
rescue operations, the following requirements need to
be satisfied:
x Adaptability. The system needs to dynamically
discover the nearby rescue services (SA and SB), and
plan a workflow to perform the rescue operations.
When situation changes, the system needs to
adaptively find other rescue services and re-plan or
modify the workflow to rescue people on BS.
x Context-acquisition. The system needs to consider
the following contexts:
o The locations of A, B, BS, P1 and P2
o The number of injured people on BS
o The speed of each of A, B and H
o The maximum flying distance of H
x SAW. The system needs to automatically recognize
and respond to the following situations based on the
acquired context data:
o SRC detects a broken ship that needs help
o There is no injury on BS
o There are life-threatening injuries on BS
x Security. For the security of services, the system
needs to enforce the following security policies:
o Since the distances between P1 and BS and
between P2 and BS are greater than the flight
radius of H (i.e., H cannot fly to BS and return to
P1 or P2 without refueling), SH can be called for
rescuing BS only when B is close enough to BS.
o SA and SB can only be called by SRC.
Although various techniques have been proposed to
address some of these requirements individually, such
as security for Web services [6-9] and composition of
web services [5] (see Section 3 for details), so far there
is no effective enabling technique to systematically
build such a service-based system rapidly to satisfy all
these requirements. The objective of the Adaptable

Proceedings of the Eighth IEEE International Symposium on Object-Oriented Real-Time Distributed Computing (ISORC’05)
0-7695-2356-0/05 $ 20.00 IEEE

Situation-aware Secure Service-based (AS3) Systems is
to naturally satisfy all these requirements.

2.2. Requirements for Building AS3 Systems
To enable rapid development of AS3 systems, we
will need the following support:
R1) Support for formally specifying the semantics of
services, and requirements for SAW and security.
R2) Support for verifying workflow, SAW and security
specifications. Tools and techniques are needed for
verifying the following properties of the workflow,
SAW and security specifications: (i) The consistency
of specifications for SAW and security policy. (ii) The
accessibility of the system, i.e. the specified policies
for an AS3 system should not deny all possible access
requests. (iii) The execution of a workflow satisfies all
SAW and security requirements.
R3) Adaptive workflow planning. AS3 systems need to
dynamically compose distributed services into
processes based on the goals of users, situations and
relevant security policies and to adapt a process so that
it can still achieve the users’ goals when unexpected
situations (e.g., services fail, security policies change)
are detected.
For example, the SRC needs to dynamically discover
the nearby rescue services (SA and SB), and plan a
workflow to perform the rescue operations. When
situation changes, SRC needs to adaptively find other
rescue units and re-plan or modify the workflow to
rescue people on ship BS.
R4) Distributed situation-aware and secure workflow
scheduling. AS3 systems need to decompose
workflows composed by distributed services into
distributed processes and schedule these processes on a
given set of resources efficiently without violating the
relevant security policies.
In AS3 systems, multiple workflows may be
generated together and the execution process of a
generated workflow may overlap with others. Since
invoking services during workflow execution will
cause some situation changes, which will change the
security policies to be enforced, the execution of the
generated workflows must be properly scheduled so
that all of them can be executed with all dependencies
on situations and security policies of these workflows
satisfied. Furthermore, the execution of the generated
workflows must be scheduled to satisfy the timing
requirements and utilize resources efficiently.
R5) SAW for adaptable service coordination.
In an AS3 system, the distributed processes, each of
which contains a sequence of service invocations, need
to be coordinated to ensure correct execution of the
workflow under varying situations. Hence, AS3
systems need to have the capabilities to recognize

situations, invoke appropriate services and enforce
relevant security policies when situations change. For
the above “rescue ship” example, the system needs to
recognize the specified situations and respond
promptly to the situation changes. For example, when
the situation of “there are life-threatening injuries on
ship BS” becomes true, SRC needs to do re-planning to
find SB and SH to rescue the injured people from BS.
To enforce security policies, SAW is often desired [11,
12]. For example, in the policy of “SH can be called for
rescuing BS only when B is close enough to BS”, the
situation of “B is close enough to BS” needs to be
recognized to ensure the success of the rescue
operations and the security of H.
R6) Distributed security policy enforcement. AS3
systems need to enforce distributed security policies
that are specified by different service providers and
users in different security domains. Each security
domain can have one or more security policy
repositories. The specified security policies of each
security domain are stored in its corresponding security
policy repository.
For example, the provider of SH requires to enforce
a security policy like “SH can be called for rescuing
BS when B is close enough to BS” to ensure the
security of SH. The SRC may require that “Only SRC can
access SA and SB when there is a broken ship detected
by the system”. As services are dynamically composed
into processes, it is necessary to enforce these
distributed security policies during the execution of
these processes to protect the security of AS3 systems.

3. Current State of the Art
Currently, much research has focused on the
following five aspects related to some of the above
requirements.

3.1. Web Service Specification
Several specification languages have been
developed for specifying Web services, among which
Web Services Description Language (WSDL) [13] and
Ontology Web Language for Web Services (OWL-S)
[14] are most popular. WSDL is an XML-formatted
language for describing a Web service's capabilities as
collections of communication endpoints capable of
exchanging messages. WSDL provides a basic and
simple abstraction of Web services. OWL-S provides
primitives for service descriptions in semantic web. Its
overall structure includes three main parts: the service
profile for advertising and discovering services; the
process model, which gives a detailed description of a
service's operation; and the grounding, which provides
details on how to interoperate with a service, via

Proceedings of the Eighth IEEE International Symposium on Object-Oriented Real-Time Distributed Computing (ISORC’05)
0-7695-2356-0/05 $ 20.00 IEEE

messages. However, formalisms for expressing
conditions, such as situational and security conditions
are not defined in OWL-S.

3.2. Service Contraction and Composition
Several methodologies have been developed to
model processes of Web services [10, 15, 16]. Earlier
work [17] showed that Concurrent Transaction Logic
(CTR) [18] is a natural logical formalism for
representing workflow control graphs, for reasoning
with temporal and causality constraints on workflow
execution, and for scheduling workflows in the
presence of those constraints. Recently, we have
substantially expanded this work to allow modeling of
workflows to include non-trustworthy or adversarial
services with the introduction of CTR-S [19]. CTR-S is
in the intersection of the areas of contracting and multiparty workflow modeling, and provides a formal
framework for designing, modeling, and reasoning
about the run-time properties of workflows made up of
a collection of controllable and adversarial services.

3.3. SAW
SAW has been studied in artificial intelligence,
human-computer interaction and data fusion
communities [20-25]. In [20], a Situation Calculus was
introduced for reasoning on various facts of a situation
without explicitly specifying what constitutes a
situation. Situation Calculus was later extended in [21]
to support temporal reasoning in Situation Calculus. In
[22], formal notation and semantics for situations were
presented, and situation is considered as a part of
reality that can be observed and reasoned about. In
[23], a formal framework for SAW formalizing the
data fusion process was presented. This formal
framework can be expressed in various languages,
including UML, DAML and the Slang methods
language [23]. A core SAW ontology using OWL was
presented in [24], and used to construct RuleML-based
domain theories in [25]. However, most of these
research efforts are focused on modeling SAW and
situation analysis without considering the support to
development of situation-aware systems.

3.4. Security Policy Specification and Analysis
One important aspect in distributed security
management is how to specify security policies
representing the security requirements of users.
Industrial standards have been proposed to specifying
security policies for Web services, such as WSSecurity [6], WS-SecurityPolicy [7], eXtensible Access
Control Markup Language (XACML) [8], and Security

Assertion Markup Language (SAML) [9]. WS-Security
provides an XML framework for exchanging
authentication and authorization information to secure
the interaction among Web services and service
invokers. WS-SecurityPolicy is used to describe the
security policies in terms of their characteristics and
supported features, such as required encryption
algorithms and privacy rules. SAML has the same
purpose as WS-Security, but requires a third-party or
services themselves to gather the evidence needed for
policy decision. XACML is an XML framework for
specifying access control policies for Web-based
resources and can potentially be applied to secure
service-based systems. Although XML-based security
policy specification languages provide powerful
expressiveness, these languages have no support for
formal analysis on security policies.
Logic-based policy specification languages have
gained more interests for their unambiguous semantics,
flexible expression formats and various types of
reasoning support. Default logic [26] is a very flexible
policy specification language that incorporates the
Bell-LaPadula model [27] and the complexity of
decision evaluation is in quadratic time. Temporal
Authorization Bases (TABs) [28] labels each
authorization policy with a periodic temporal
expression. Although with periodic constraints TABs is
still decidable, it becomes undecidable if we add more
flexible temporal constructs since time instance is
unbounded. Flexible Authorization Framework [29]
introduces the idea of hierarchical structure and allows
users to specify how to resolve policy conflicts.
Description logic (DL) [30] is used to specify security
policies for its clean structure and powerful reasoning
support. However, to make the reasoning bounded,
many restrictions are introduced, and hence security
policy cannot be easily written in an intuitive way.
Note that none of these approaches can address
temporal
constraints,
situation-awareness
and
hierarchical structure at the same time. Note that none
of these approaches can systematically address SAW in
the distributed security enforcement, which is very
important for AS3 systems.

3.5. Distributed Security Policy Enforcement
Much research has been done on distributed trust
management, which is related to our adaptable security
framework. PolicyMaker [31] and KeyNote [32]
provide a policy enforcement framework, which uses
local policies, credentials, and action strings as input to
determine whether the request should be granted or
denied. However, PolicyMaker and KeyNote have no
support for credentials discovery, negative assertions,
policy analysis, and policy composition. REFEREE

Proceedings of the Eighth IEEE International Symposium on Object-Oriented Real-Time Distributed Computing (ISORC’05)
0-7695-2356-0/05 $ 20.00 IEEE

[33] places all critical operations under policy control
rather than making arbitrary decisions, which may
cause certain dangerous operations to occur. In [34], a
role-based trust management system, called RT, based
on the semantics of constraint Datalog was presented.
RT supports distributed credential discovery, which can
process access requests with an incomplete set of
credentials. However, RT only considers access control
constraints, and provides no support for other aspects
in a trust management system. In these systems, trust
relationship solely depends on credential verification,
and hence it is relatively static.

4. Our Approach
In this section, we will first present our conceptual
view of AS3 systems, and then present our approach to
developing, deploying and operating AS3 systems, to
satisfy the six requirements discussed in Section 2.2.

4.1. The Conceptual View of AS3 Systems
Figure 1 shows the top-level conceptual view of
AS3 systems. An AS3 system is a collection of entities
acting in response to certain situations, and the
interactions among the entities restricted by security
policies. The entities of an AS3 system include
services, users, processes, and computation/
communication (comp/comm) resources. Users use
processes to achieve their goals. A process may be
composed of a set of services or implemented by a
single service. The usage of services will consume a
certain amount of computing and communicating
(comp/comm) resources. Each entity relates to some
situations. For example, a service may be invoked
under certain situations and change the current
situation after it is invoked; a process may contain
different execution paths that need to be selected in
different situations; a mission goal that a user wants to
achieve is a desired situation in the AS3 system that the

Figure 1. The top level conceptual view of
an AS3 System

user wants to have by using a process; the status of
services or comp/comm resources may be captured by
certain situations of the AS3 system. Each entity may
have its own security policies. For example, a service
or comp/comm resource may have security policies
that restrict the access to the service or the resource,
and specify the obligations for using the service or the
resource; a process may have security policies that
control the delegations among the users and services
involved in the process; a user may have security
policies that defines what authorities he/she may have.
Security policies in an AS3 system are situation-aware,
i.e., different security policy may need to be enforced
under different situations.

4.2. Our Approach to Developing, Deploying
and Operating AS3 systems
Our overall approach to developing, deploying and
operating AS3 systems consists of the following three
steps:
Step (1) To satisfy R1, we develop a formal model
based on the conceptual view of AS3 systems discussed
in Section 4.1, and a formal specification language for
AS3 systems based on the developed formal model.
Based on our conceptual view of AS3 systems, the
following primitives are provided in the formal model:
(i) Primitives for describing SAW, including the
primitives for defining contexts, situations and their
relations to future actions. (ii) Primitives for describing
service semantics. These primitives are needed for
adaptive workflow planning. (iii) Primitives for
defining security policies, including the primitives for
defining authentication, authorization and delegation
policies.
To make the formal model useful for AS3 systems,
the following issues need to be considered:
a) Usability of the model. The formal model must be
easily understood and used by various serviceproviders or developers of AS3 systems.
b)Tractability of the model. The formal model must be
tractable to allow interesting queries about service
semantics, situation changes, and security policies, to
be answered efficiently.
c) Expressiveness of the model. The formal model
should have strong expressiveness sufficient to support
SAW, service semantics and security policies.
In order to realize the models from the requirements
of AS3 systems automatically, we specify the
requirements of an AS3 system along with the QoS
goals in an AS3 logic, which is a decidable modal logic
based on the ambient logic [35, 36]. So far we use this
logic to formally describe the service theory, which
includes input-output descriptions of the services,
specification and recognition of situations, and access

Proceedings of the Eighth IEEE International Symposium on Object-Oriented Real-Time Distributed Computing (ISORC’05)
0-7695-2356-0/05 $ 20.00 IEEE

control policies for enforcing security. A model
realizing these requirements can be synthesized from
its proof from the service theory. We are continuing to
work on modeling authentication policies, delegation
policies and auditing policies.
Step (2) To satisfy R2, we develop formal methods for
verifying the consistency, completeness and
realizability of specifications for AS3 systems. In
addition, we also develop other tools for specification
analysis, such as suspension analysis.
Step (3) To satisfy R3-6, we develop the following
techniques based on the formal model developed in
Step (1):
3a) Adaptive workflow planning (R3). In AS3
systems, workflows may fail due to execution time
problems; which may be due to incomplete domain
information or due to unpredictable situation changes.
We are developing domain-specific workflow
adaptation techniques, where we provide (semi)automated algorithms for mining successful and
unsuccessful execution traces of workflows in order to
identify and compose combinations of tasks into highlevel tasks, and engineer domain specific hierarchies of
tasks. Such hierarchies may contain tasks such as
“move a rescue ship within the range of the broken
ship” or “perform all rescue operations on the broken
ship”. Online planning with high-level tasks, where we
plan some and then execute some, enables the
synthesis of adaptive workflows that can take
advantage of domain-specific knowledge as the
situation evolves during the execution. Candidates for
domain-specific high-level tasks can be identified by
mining workflow execution traces using a sequence
mining algorithm, such as SPADE [37]. In the above
“ship rescue” example, the mining algorithm may
detect that, a rescue ship frequently needs the
assistance of a satellite to track the location of a broken
ship, and hence the algorithm can suggest to group the
satellite service and rescue ship into a high-level task
of “move a rescue ship within the range of the broken
ship” upon approval of a domain expert. The next step
involves determining the preconditions and effects of a
new high-level task. We are exploring the use of
inductive logic programming [38] techniques for
learning the definitions of the precondition and effect
predicates by mining the workflow execution log itself,
and hence incorporating all the situation information as
well as success and failure patterns into these predicate
specifications. In addition, we are exploring techniques
for on-line synthesis of a new plan for adapting the
original plan for emerging new situations during the
execution.
3b) Distributed situation-aware and secure workflow
scheduling (R4). To schedule workflow with
dependencies on situations and security policies

efficiently, we are developing a distributed scheduling
approach inspired by event algebra [39], Concurrent
Constraint Transaction Logic (CCTR) [40] and
GridFlow [41]. Our approach consists of the following
major steps: (i) Identify required resources based on a
generated workflow. (ii) Decompose the generated
workflow into distributed processes, and compute the
guards (conditions) for the execution of the distributed
processes based on relevant security policies,
situational constraints and timing constraints. If no
feasible decomposition can be found, notify the
planner to perform re-planning. (iii) Check whether
any situational, security and timing constraints of other
workflows already running in the system are violated if
the distributed processes generated in (ii) are deployed
and executed. (iv) If no constraint is violated, search
for an optimal resource allocation for the distributed
processes based on the resource utilization
requirements and availability of the required
comm/comp resources; otherwise, repeat (ii) with
additional constraints generated from the distributed
processes that cause the violations. (v) Deploy and
execute the workflow.
3c) SAW agents for adaptable service coordination.
(R5). Since it is very expensive (in terms of
computation) to enumerate all possible situation
changes during the planning and scheduling process,
we use distributed SAW agents to adaptively
coordinate the execution of a generated workflow to
ensure that all situational dependencies of the
workflow are satisfied (security agents for enforcing
security policies will be discussed in 3d). A SAW
agent works as follows: (i) Accept a workflow from the
workflow scheduler or other agents; (ii) discover other
SAW agents for cooperative situation analysis and/or
service coordination if context and situation
information and/or required services in the workflow
are not available locally; (iii) Acquire contexts and
analyze situations; (iv) Select and invoke the
appropriate services monitored by the SAW agent
based on the workflow and the current situation, and
send partial workflow to other SAW agents if
necessary; (v) Adapt the workflow by searching
alternative services or performing re-planning when a
dependency on situations cannot be satisfied.
3d) Agent-based
distributed
security
policy
management (R6). To improve the enforcement
performance, we use distributed agents to enforce all
these distributed security policies in service-based
systems. Our approach works as follows: (i)
Decompose security policies into different sets of
security policies by analyzing the targets of each policy
(the targets of a policy are determined by the entities in
the policy). (ii) Synthesize the decomposed security
policies to generate security agents. (iii) Deploy the

Proceedings of the Eighth IEEE International Symposium on Object-Oriented Real-Time Distributed Computing (ISORC’05)
0-7695-2356-0/05 $ 20.00 IEEE

security agents on an agent platform. (iv) Discover and
activate related security agents to evaluate and enforce
security policies at runtime. During the enforcement of
security policies, the security agents may need to
communicate with SAW agents to enforce situationaware security policies.
Mission
Goal

Discovery
Services
(DS)

Mission Planning
Services (MP)

Workflow
Security
Agents
(SeA)

Directories

SituationAwareness
Agents
(SAA)

Workflow
Scheduling
Services (WS)

Services

Various
Capabilities

Figure 2. A high-level architecture of
AS3 Systems
Figure 2 illustrates the high-level architecture of an
AS3 system. It integrates the components implementing
the techniques developed in Step (3) with the protocols
that determine the way a component interacts with
others. The services are used for publishing various
capabilities provided by organizations. The
specification language that we are currently developing
will be used to describe the semantics, and the related
situations and security policies of services in AS3
systems. Similar to normal service-based systems, the
description of services will be published in the
Directories. The following agents and services are
being developed to provide runtime support for AS3
systems:
x SAW Agents (SAA) which collect context data of
interest, analyze the collected data to determine the
situation, execute workflows and adapt workflows
when relevant situation changes occur.
x Security Agents (SeA) which discover and enforce
relevant security policies in a distributed manner.
x Discovery Services (DS) which perform semanticbased situation-aware service discovery.
x Mission Planning Services (MP) which generate
workflows to fulfill certain mission goals based on
security policies, situations and available services.
x Workflow Scheduling Services (WS) which
decompose the generated workflows, and generate,
deploy and initialize SAAs and SeAs to execute
workflows in such a way that all relevant security
policies and resource constraints are satisfied.
An AS3 system operates as follows:
S1) A user (either a human user or an application)
specifies the mission goal and additional security or
situational constraints that need to be satisfied when

the AS3 system achieves the mission goal, and sends
the information to the MP.
S2) The MP will first use available DSs to identify
services that may be needed to achieve the mission
goal, and then start the planning process to generate a
workflow that can achieve the specified mission goal
using available services with all relevant security
policies and situational constraints satisfied.
S3) The generated workflow will then be decomposed
by a WS to generate SeAs and SAAs for executing the
workflow and enforcing security policies.
S4) During the execution of the workflow, the SAAs
can adapt the workflow by selecting alternative
services or invoke the MP to perform partial or
complete re-planning on the occurrence of certain
situation changes that make the execution of the
workflow to violate the situational or security
constraints of some services used in the workflow.
S5) For the partial re-planning, the SAAs will notify
the MP of the completed portion of the initial
workflow, and the MP will generate a new workflow to
achieve the mission goal based on the partial result
generated from the completed portion of the initial
workflow. In case such a new workflow cannot be
found, the MP will re-plan the entire workflow without
using the services that cause the failure. However,
there is no guarantee that a new workflow can be found
without using the failed services. If this is the case, the
user will be informed that the system is currently
unable to provide the necessary services for achieving
the user’s mission goal and the user will need to
modify the mission goal.

5. Conclusion
In this paper, we have presented the rationale and
requirements of rapid development of AS3 systems,
and an overview of our approach to satisfying these
requirements. Challenging research issues arising in
our approach for developing new techniques to address
these issues are briefly discussed. Currently, we are
developing these techniques and a demonstration
system based on Secure Infrastructure for Networked
Systems (SINS) [42] developed by US Naval Research
Laboratory (NRL).

Acknowledgment
This work is supported by the Department of
Defense/Office of Naval Research under the
Multidisciplinary Research Program of the University
Research Initiative, Contract No. N00014-04-1-0723.
We would like to thank Ramesh Bharadwaj of NRL for

Proceedings of the Eighth IEEE International Symposium on Object-Oriented Real-Time Distributed Computing (ISORC’05)
0-7695-2356-0/05 $ 20.00 IEEE

his constructive
discussions.

comments

and

many

helpful

References
[1]
W3C,
“Web
Services
Architecture,”
http://www.w3.org/TR/2004/NOTE-ws-arch-20040211/
[2] U.S. Department of Defense Directive (DODD) 8100.1:
“Global Information Grid (GIG) Overarching Policy,” The
Pentagon, Washington D.C., September 2002.
[3] S. S. Yau, et al., “Development of Situation-Aware
Application
Software
for
Ubiquitous
Computing
Environments,” Proc. 26th Ann. Int’l Computer Software and
Applications Conf. (COMPSAC 2002), 2002, pp. 233-238.
[4] S. S. Yau, et al., “Reconfigurable Context-Sensitive
Middleware for Pervasive Computing,” IEEE Pervasive
Computing, vol. 1(3), 2002, pp. 33-40.
[5] Business Process Execution Language for Web Services.
http://www-128.ibm.com/developerworks/library/specification/ws-bpel/
[6] WS-Security. http://www-106.ibm.com/developerworks/
webservices/library/ws-secure/
[7] WS Security Policy. http://www-106.ibm.com/developer
works/library/ws-secpol/
[8] OASIS eXtensible Access Control Markup Language
(XACML) TC. http://www.oasis-open.org/committees/
tc_home.php?wg_abbrev=xacml.
[9] OASIS Security Services TC, “Security Assertion
Markup Language (SAML),” http://www.oasis-open.org/
committees/tc_home.php?wg_abbrev=security.
[10] P. C. Attie, et al., “Scheduling Workflows by Enforcing
Intertask Dependencies,” Distributed Systems Engineering J.,
vol. 3(4), 1996, pp. 222-238.
[11] S. S. Yau, et al., “Situation-Aware Access Control for
Service-Oriented Autonomous Decentralized Systems,” 7th
Int’l Symp. on Autonomous Decentralized Systems, to appear.
[12] S. S. Yau, et al., “An Adaptable Security Framework for
Service-based Systems,” 10th IEEE Int’l Workshop on
Object-oriented Real-time Dependable Systems, to appear.
[13] W3C, “Web Services Description Language (WSDL)
1.1,” http://www.w3.org/TR/wsdl
[14] W3C, “OWL-S: Semantic Markup for Web Services,”
http://www.daml.org/services/owl-s/1.1/overview/
[15] R. Gunthor, “Extended transaction processing based on
dependency rules”, Proc. RIDE-IMS Workshop, 1993, pp.
207-214.
[16] C. Schlenoff, et al., “The Essence of the Process
Specification Language,” Trans. of the Society for Computer
Simulation Int’l, vol. 16(4), 2000, pp. 204-216.
[17] H. Davulcu, et al., “Logic based modeling and analysis
of workflows”, ACM Symp. on Principles of Database
Systems (PODS), 1998, pp.25-33.
[18] A.J. Bonner and M. Kifer, “Concurrency and
Communication in Transaction Logic”, Proc. Joint Int’l Conf.
and Symp. on Logic Programming, 1996, pp. 142-156.
[19] H. Davulcu, et al., “CTR-S: A Logic for Specifying
Contracts in Semantic Web Services”, Proc. 13th Int’l WWW
Conf., 2004, pp.144-153.
[20] J. McCarthy and P. J. Hayes, “Some Philosophical
Problems from the Standpoint of Artificial Intelligence”,
Machine Intelligence 4, 1969, pp. 463-502.

[21] J. A. Pinto, “Temporal Reasoning in the Situation
Calculus”, PhD Thesis, University of Toronto, 1994.
[22] J. Barwise, “Scenes and Other Situations”, J. Philosophy,
vol. 77, 1981, pp. 369-397.
[23] K. Baclawski, et al., “Formalization of Situation
Awareness,” Proc. 11th OOPSLA Workshop on Behavioral
Semantics, 2002, pp. 1-15.
[24] C. J. Matheus, et al., “A Core Ontology for Situation
Awareness”, Proc. 6th Int’l Conf. on Information Fusion,
2003, pp. 545 –552.
[25] C. J. Matheus, et al., “Constructing RuleML-Based
Domain Theories on top of OWL Ontologies”, Proc. 2nd Int’l
Workshop on Rules and Rule Markup Languages for the
Semantic Web, 2003, pp. 81–94.
[26] R. Reiter, “A Logic for Default Reasoning,” Artificial
Intelligence, vol. 13, 1980, pp.81-132.
[27] D. Bell and L. Lapadula, "Secure Computer System:
Unified Exposition and Multics Interpretation." Technical
Report MTR-1997, MITRE, 1976.
[28] E. Bertino, et al., “Temporal Authorization Bases: From
Specification to Integration,” J. Computer Security, vol. 8(4),
2000.
[29] S. Jajodia, et al., “Flexible Supporting for Multiple
Access Control Policies,” ACM Trans. on Database Systems,
vol. 26(2), 2001, pp. 214-260.
[30] F. Baader, et al., editors, “The Description Logic
Handbook,” Cambridge University Press, 2003
[31] M. Blaze, et al., “Decentralized Trust Management,”
Proc. IEEE Symposium on Privacy and Security, Oakland,
1996, pp. 164-173.
[32] M. Blaze, et al., “The KeyNote Trust Management
System (version 2),” RFC2704, 1999.
[33] Y. Chu, et al., “REFEREE: Trust Management for Web
Applications.” World Wide Web J., vol.2(3),1997,pp.127-139.
[34] N. Li and J. Mitchell. “RT: A Role-Based Trust
Management Framework,” Proc. 3rd DARPA Information
Survivability Conf. and Exposition (DISCEX '03), Apr. 2003,
[35] L. Cardelli and A .D. Gordon, “Anytime, Anywhere:
Modal Logics for Mobile Ambients”, Proc. 27th ACM Symp.
on Principles of Programming Languages, 2000, pp 365-377.
[36] W. Charatonik, et al., “Model Checking Mobile
Ambients”, Theoretical Computer Science, vol. 308(1-3),
2003, pp. 277-331.
[37] M. J. Zaki. “SPADE: An Efficient Algorithm for Mining
Frequent Sequences,” Machine Learning, 2001.
[38] N. Lavrac and S. Dzeroski. "Inductive Logic
Programming: Techniques and Applications", Ellis Horwood,
New York, 1994.
[39] Munindar P. Singh, et al., “An Event Algebra for
Specifying and Scheduling Workflows,” Proc. 4th Int’l Conf.
on Database Systems for Advanced Applications
(DASFAA’95), 1995, pp. 53-60.
[40] P. Senkul, et al., “A Logical Framework for Scheduling
Workflows under Resource Allocation Constraints,” Proc.
28th Int’l Conf. on Very Large Data Bases, 2002, pp. 694-705.
[41] J. Cao, et al., “GridFlow: Workflow Management for
Grid Computing,” Proc. 3rd IEEE/ACM Int’l Symp. On
Cluster Computing and the Grid, 2003, pp. 198-205.
[42] R. Bharadwaj, "A Framework for the Formal Analysis
of Multi-Agent Systems," Proc. Formal Approaches to
Multi-Agent Systems (FAMAS), 2003.

Proceedings of the Eighth IEEE International Symposium on Object-Oriented Real-Time Distributed Computing (ISORC’05)
0-7695-2356-0/05 $ 20.00 IEEE

CTR-S: A Logic for Specifying Contracts
in Semantic Web Services
∗

Hasan Davulcu

Michael Kifer

I.V. Ramakrishnan

Department of CSE
Arizona State University
Box 875406,
Tempe, AZ 85287-5406

Department of Computer
Science
Stony Brook University
Stony Brook, NY 11794

Department of Computer
Science
Stony Brook University
Stony Brook, NY 11794

hdavulcu@asu.edu

kifer@cs.stonybrook.edu

ram@cs.stonybrook.edu

ABSTRACT

and thus is in the intersection of the areas of contracting and process modeling. Since CT R-S is an extension of the classical firstorder logic, it is well-suited for modeling of the static aspects of
contracting as well. If object-oriented representation is desired, Flogic [14] (and an adaptation of CT R-S) can be used instead.
The idea of using a logic to model processes is not new [12, 2,
23, 8, 21]. These methodologies are commonly based on the classical first-order logic, temporal logic [9], and Concurrent Transaction Logic [5]. A distinctive aspect of contracting in Web services,
which is not captured by these formalisms, is that contracting involves multi-party processes, which can be adversarial in nature.
One approach to deal with this situation could be to try and extend a multi-modal logic of knowledge [10]. However, we found
it more expedient to extend Concurrent Transaction Logic [5] (or
CT R), which has been proven a valuable tool for modeling and reasoning about processes [8, 4, 17]. The extension is called CT R-S
and is designed to model the adversarial situation that arises in service contracting. This is achieved by extending the model theory of
CT R with certain concepts borrowed from the Game Theory [18,
13, 19]. In this paper we also develop a proof theory for CT R-S
and illustrate the use of this logic for modeling and reasoning about
Web service contracts.
A typical situation in contracting where different parties may
sometimes have conflicting goals is when a buyer interacts with a
seller and a delivery service. The buyer needs to be assured that the
goods will either be delivered (using a delivery service) or money
will be returned. The seller might need assurance that if the buyer
breaks the contract then part of the down-payment can be kept as
compensation. We thus see that services can be adversarial to an
extent. Reasoning about such services is an unexplored research
area and is the topic of this paper.

A requirements analysis in the emerging field of Semantic Web Services (SWS) (see http://daml.org/services/swsl/requirements/) has
identified four major areas of research: intelligent service discovery, automated contracting of services, process modeling, and service enactment. This paper deals with the intersection of two of
these areas: process modeling as it pertains to automated contracting. Specifically, we propose a logic, called CT R-S, which captures the dynamic aspects of contracting for services. Since CT R-S
is an extension of the classical first-order logic, it is well-suited to
model the static aspects of contracting as well. A distinctive feature
of contracting is that it involves two or more parties in a potentially
adversarial situation. CT R-S is designed to model this adversarial
situation through its novel model theory, which incorporates certain game-theoretic concepts. In addition to the model theory, we
develop a proof theory for CT R-S and demonstrate the use of the
logic for modeling and reasoning about Web service contracts.

Categories and Subject Descriptors
H.4.m [Information Systems]: Miscellaneous; I.1.3 [Computing
Methodologies ]: Symbolic and algebraic manipulation—Languages and Systems

General Terms
Algorithms, Languages, Verification

Keywords
Web Services, Services Composition, Contracts

1.

INTRODUCTION

A Web service is a process that interacts with the client and other
services to achieve a certain goal. A requirements analysis in the
emerging field of Semantic Web Services (SWS)1 has identified four
major areas of research: intelligent service discovery, automated
contracting of services, process modeling, and service enactment.
It is generally agreed that Semantic Web Services should be based
on a formalism with a well-defined model-theoretic semantics, i.e.,
on some sort of a logic. In this paper we propose a logic, called
CT R-S, which captures the dynamics of contracting for services

Overview and summary of results. We introduce game-theoretic
aspects into CT R using a new connective, the opponent’s conjunction. This connective represents the choice of action that can be
made by a party other than the reasoner. The reasoner here can
be the client of a Web service who needs to verify that her goals are
met or a service that needs to make sure that its business rules are
satisfied no matter what the other parties (the clients and other services) do. Actors other than the reasoner are collectively referred
to as the opponent. We then develop a model theory for CT R-S
and show how this new logic can be used to specify executions of
services that may be non-cooperating and have potentially conflicting goals. We also discuss reasoning about a fairly large class of
temporal and causality constraints.
In CT R-S, a contract is modeled as a workflow that represents
the various possibilities for the service and the outside actors (or the

∗
M. Kifer and I.V. Ramakrishnan were supported in part by the
NSF grants CCR-0311512 and IIS-0072927.
1
See http://daml.org/services/swsl/requirements/

Copyright is held by the author/owner(s).
WWW2004, May 17–22, 2004, New York, New York, USA.
ACM 1-58113-912-8/04/0005.

144

φ∨¬ψ. The usual dualities φ∨ψ ≡ ¬(¬φ∧¬ψ) and ∃ φ ≡ ¬∀¬φ
also hold. The opponent’s conjunction has a dual connective, 	, but
we will not discuss it in this paper.
As mentioned in the introduction, we model the dynamics of
service contracts using the abstraction of a 2-party workflow, where
the first party is the reasoner and the other represents the rest of
the players involved in the contract. In general, if several parties
need to be able to reason about the same contract, the contract can
be represented as several 2-party workflows, each representing the
contract from the point of view of a different reasoner.

client and other services). The CT R-S model theory characterizes
all possible outcomes of a formula W that represents such a workflow. A constraint, Φ, represents executions of the contract with
certain desirable properties. For instance, from the client’s point of
view, a desirable property might be that either the good is delivered
or the payment is refunded. The formula W ∧Φ characterizes those
executions of the contract that satisfy the constraint Φ. If W ∧ Φ is
satisfiable, i.e., there is at least one execution in its model, then we
say that the constraint Φ is enforcible in the workflow formula W.
We describe a synthesis algorithm that converts declarative specifications, such as W ∧ Φ, into equivalent CT R-S formulas that
can be executed more efficiently and without backtracking. The
transformation also detects unsatisfiable specifications, which are
contracts that the reasoner cannot force to execute with desirable
outcomes. In game-theoretic terms, the result of such a transformation can be thought of as a concise representation of all winning
strategies, i.e., all the ways for the reasoner to achieve the desired
outcome, regardless of what the rest of the system does, if all the
parties obey the terms of the contract.
Finally, since CT R-S is a natural generalization of CT R, a
pleasing aspect of this work is that our earlier results in [8] become
special cases of the new results.
The rest of the paper is organized as follows. In Section 2, we
introduce CT R-S and discuss its use for modeling workflows and
contracts. In Section 3, we introduce the model theory of CT R-S.
Section 4 discusses the proof theory. Section 5 introduces causal
and temporal workflow constraints that can be used to specify goals
of the participants in a contract. In Section 6, we present an algorithm for solving these constraints and discuss its complexity. Section 7 concludes with a discussion of related formalisms.

2.

Definition 1. (Workflows) A CT R-S goal is recursively defined as either an atomic formula or an expression of the form φ⊗ψ,
φ | ψ, φ ∨ ψ, or φ  ψ, where φ and ψ are CT R-S goals. A rule
is of the form head←body, where head is an atomic formula and
body a CT R-S goal. A workf low control specif ication
consists of a CT R-S goal and a (possibly empty) set of rules.
Note that the connective ∧ is not allowed in workflow control specifications, but is used to specify constraints.

2.2 Modeling Contract Dynamics in CT R-S
Example 1. (Procurement Contract) Consider a procurement
application that consists of a buyer interacting with three services,
sell, f inance, and deliver. We assume that the buyer is the reasoner in this example.
Services are modeled in terms of their signif icant events.
For instance, the buy service begins when the significant event
pay escrow occurs. When pay escrow is finished, a concurrent
execution of the sell and f inance services begins.
Thus, at a high level, the buy service can be represented as:

CT R-S AND THE DYNAMICS OF SERVICE CONTRACTS

pay escrow ⊗ (f inance | sell)
The connective ⊗ represents succession of events or actions: when
the above expression “executes,” the underlying database state is
first changed by the execution of the formula pay escrow and then
by the execution of f inance | sell. The connective | represents
concurrent, interleaved execution of the two sequences of actions.
Intuitively, this means that a legal execution of (f inance | sell)
is a sequence of database states where the initial subsequence corresponds, say, to a partial execution of the subformula f inance;
the next subsequence of states corresponds to an execution of
sell; the following subsequence is a continuation of the execution of f inance; and so on. The overall execution sequence of
(f inance | sell) is a merge of an execution sequence for the left
subformula and an execution sequence for the right subformula.
Execution has precise meaning in the model and proof theories
of CT R. Truth of CT R formulas is established not over database
states, as in classical logic, but over sequences of states; it is interpreted as an execution of that formula in which the initial database
state of the sequence is successively changed to the second, third,
etc., state. The database ends up in the final state of the sequence
when the execution terminates.3
Workflow formulas can be modularized with the help of rules.
The intuitive meaning of a rule, head←body, where head is an
atomic formula and body is a CT R-S goal, is that head is an invocation interface to body, where body is viewed as a subroutine.
This is because according to the semantics described in Section 3,

Familiarity with CT R [5] can help understanding of CT R-S and
its relationship to workflows and contracts. However, this paper is
self-contained and includes all the necessary definitions. We first
describe the syntax of CT R-S and then supply intuition to help the
reader make sense out of the formal definitions that follow.

2.1 Syntax
The atomic f ormulas of CT R-S are expressions of the
form p(t1 , ..., tn ), where p is a predicate symbol and ti are terms,
i.e., they are the same as in classical logic. Complex formulas are
built with the help of connectives and quantifiers: if φ and ψ are
CT R-S formulas, then so are φ ∧ ψ, φ ∨ ψ, φ ⊗ ψ, φ | ψ, ¬φ,
φ  ψ, (∃X)φ, and (∀X)φ, where X is a variable. Intuitively, the
formula φ ⊗ ψ means: execute φ and then execute ψ. The connective ⊗ is called serial conjunction. The formula φ | ψ
denotes an interleaved execution of two games φ and ψ. The connective | is called concurrent conjunction. The formula
φ  ψ means that the opponent chooses whether to execute φ or
ψ, and therefore  is called opponent s conjunction. The
meaning of φ ∨ ψ is similar, except the reasoner makes the decision. In CT R this connective is called classical disjunction
but because of its interpretation as reasoner’s choice we will also
refer to it as reasoner  s disjunction. Finally, the formula
φ ∧ ψ denotes execution of φ constrained by ψ (or ψ constraint by
φ). It is called classical conjunction.2
As in classical logic, we introduce φ←ψ as an abbreviation for

Space limitation does not permit us to compare CT R to other
logics that on the surface might appear to address the same issues
(e.g., temporal logics, process and dynamic logics, the situation
calculus, etc.). We refer the reader to the extensive comparisons
provided in [5, 6].
3

2
The meaning of ∧ is all but classical. However, its semantic definition looks very much like that of a conjunction in predicate calculus. This similarity is the main reason for the name.

145

show how such goals can be represented and that they can be enforced under this contract even in adversarial situations.

such a rule is true if every legal execution of body must also be a
legal execution of head. Combined with the minimal model semantics this gives the desired effect [6]. With this in mind, we can now
express the above procurement workflow as follows:
buy

← pay escrow

We shall see that a large class of temporal and causality constraints can be represented as CT R-S formulas. If Φ represents
such a formula for the above example, then finding a strategy to
enforce the constraints under the rules of the contract is tantamount
to checking whether buy ∧ Φ is satisfiable in CT R-S.
Before going on, we should clear up one possible doubt: why
is there only one opponent? The answer is that this is sufficient
for a vast majority of practical cases, especially those that arise in
Web services. Indeed, even when multiple independent actors are
involved, we can view each one of them (or any group that decides
to cooperate) as the reasoner and all the rest as the opponent. Any
such actor or a group can then use CT R-S to verify that its goals
(specified as a condition Φ) are indeed enforcible.

⊗ (sell | f inance)

Next, we search for matching services for the sell service using
a service directory to discover the following rules.
sell ← reserve item ⊗ (deliver ∨ keep escrow)
deliver ← insured ∨ uninsured
The ∨ connective in the definition of sell represents alternative executions. For instance, a legal execution of insured ∨ uninsured
is either a legal execution of insured or of uninsured. Similarly,
a legal execution of sell involves the execution of reserve item
and then, an execution of either deliver or recv escrow.
The above definition of sell also requires compliance with the
following contract requirements between the buyer and the seller:

3. MODEL THEORY
In this section we define a model theory for our logic. The importance of a model theory is that it provides the exact semantics
for the behavioral aspects of service contracts and thus serves as a
yardstick of correctness for the algorithms in Section 6.

− if buyer cancels, then seller keeps the escrow
− if buyer pays, then seller must deliver
Thus, the connective ∨ represents a choice. The question is
whose choice is it: the reasoner’s or that of the opponent? In an
environment where workflow activities might not always cooperate, we need a way to distinguish these two kinds of choices. For
instance, the contract may say that the outcomes of the actions of
the delivery agent are that the goods might be delivered or lost.
This alternative is clearly not under the control of the buyer, who
is the reasoner here. On the other hand, the choice of whether to
use insured or uninsured delivery is made by the buyer, i.e., the
reasoner. With this understanding, the insured and uninsured
services can be defined as follows:

3.1 Sets of Multipaths
A path is a sequence of database states, d1 ...dn . Informally, a
database state can be a collection of facts or even more complex formulas, but for this paper we can think of them as simply
symbolic identifiers for various collections of facts.
In CT R [5], which allows concurrent, interleaved execution, the
semantics is based on sequences of paths, π = 
p1 , ..., pm , where
each pi is a path. Such a sequence is called a multipath, or
an m-path [5]. For example, 
d1 d2 , d3 d4 d5  is an m-path that
consists of two paths: one having two database states and the other
three (note that a comma separates paths, not states in a path). As
explained in Example 1, multipaths capture the idea of an execution of a transaction that interleaves with executions of other transactions. Thus, an m-path can be viewed as an execution that is
broken into segments, such that other transactions could execute
in-between the segments.
CT R-S further extends this idea by recognizing that in the presence of other parties, the reasoner cannot be certain which execution (or “play”) will actually take place, due to the lack of information about the actual moves that the opponent will make. However,
the reasoner can have a strategy to ensure that regardless of what
the opponent does the resulting execution will be contained within
a certain set of plays. If every play in the set satisfies the properties that the reasoner wants, then the strategy will achieve the
reasoner’s objectives. Such a set of plays is called an outcome
of the game. Thus, while truth values of formulas in CT R are
determined on m-paths, CT R-S formulas get their truth values on
sets of m-paths. Each such set, S, is interpreted as an outcome
of the game in the above sense, and saying that a CT R-S formula,
φ, is true on S is tantamount to saying that S is an outcome of φ.
In particular, two games are considered to be equivalent if and only
if they have the same sets of outcomes.

insured ← (delivered ⊗ satisf ied)  (lost ⊗ satisf ied)
uninsured ← (delivered ⊗ satisf ied)  lost
The connective  here represents the choice made by the actors
other than the reasoner (the buyer). If the buyer uses insured delivery then she is guaranteed satisfaction if the item is delivered or
lost (in the latter case the buyer presumably gets the money back).
If the buyer uses uninsured delivery then she can get satisfaction
only if the item is delivered. Whether the item is delivered or
lost is outside of the control of the buyer.
Next, we identify the following matching service for f inance:
f inance

← (approve

⊗ (make payment ∨ cancel))
 (reject ⊗ cancel)

Note that approval or rejection of the financing request is an opponent’s (the servicing agent’s) choice. However, if financing is
approved the choice of whether to proceed and make payment
or to cancel depends on the reasoner (the buyer). In addition, the
financing agent might require the following clause in the contract:
− if financing is approved and buyer does not cancel
then delivery should satisfy
Details of how to express the above contract requirement in CT R-S
will be given in Section 5.
The buyer and the services involved might have specific goals
with respect to the above contract. For instance, the buyer wants
that if financing is approved then she has a strategy to ensure that
she is satisfied (either by receiving the goods or by getting the
money back). The seller might want to have the peace of mind
knowing that if the buyer cancels the contract after receiving financing then the seller can keep the escrow. In Section 6 we will

3.2 Satisfaction on Sets of Multipaths
The following definitions make the above discussion precise.
Definition 2. (m-Path Structure [5])
An m-path
structure is a triple of the form 
U, IF , Ipath , where U
is the domain, IF is an interpretation function for constants and
function symbols (exactly like in classical logic), and Ipath is a

146

• or I, R |=ν ψ and for all Tπ , I, Tπ |=ν φ

mapping such that if π is an m-path, then Ipath (π) is a first-order
semantic structure (as commonly used in classical predicate logic).

Here πTπ denotes the set of all m-paths that are obtained by interleaving π with some m-path in Tπ . For instance, if π = 
d1 d2 , d3 d4  and 
d5 d6 , d7 d8 d9  ∈ Tπ then
one interleaving is 
d1 d2 , d5 d6 , d3 d4 , d7 d8 d9 , another is

d1 d2 , d5 d6 , d7 d8 d9 , d3 d4 , etc.

For a CT R formula, φ, and an m-path, π, the truth of φ on π
with respect to an m-path structure is determined by the truth values of the components of φ on the appropriate sub-m-paths of π.
In a well-defined sense, establishing the truth of a formula, φ, over
an m-path, π = 
p1 , ..., pn , corresponds to the possibility of executing φ along π where the gaps between p1 , ..., pn are filled with
executions of other formulas [5].
The present paper extends this notion to CT R-S by defining truth
of a formula φ over sets of m-paths, where each such set represents
a possible outcome of the game corresponding to φ. The new definition reduces to CT R’s for formulas that have no ’s.

7. Universal Quantification: I, π |=ν ∀X.φ if and only
if I, π |=µ φ for every variable assignment µ that assigns
the same value as ν to all variables except X. Existential
quantification, ∃X.φ, is a shorthand for ¬∀X¬φ.
Example 2. (Database Transactions) Consider the following
formula, where st means “start,” ab means “abort,” cm is “commit,”
cp means “compensate,” and no stands for a noop. Further assume
that each elementary update em in the following formula denotes
an insert(em) operation which satisfies {
d d ∪ {em}} |=
insert(em) where d is a set of ground atomic formulas.

Definition 3. (Satisfaction) Let I = 
U, IF , Ipath  be an mpath structure, π be an arbitrary m-path. Let S, T , S1 , S2 , etc.,
denote non-empty sets of m-path, and let ν be a variable assignment, which assigns an element of U to each variable. We define
the notion of satisf action of a formula, φ, in I on S by structural induction on φ:

φ = st ⊗ (ab  cm) ⊗ (cp ∨ no)
Then the possible outcomes for φ can be computed from the outcomes of its components as follows:

1. Base Case:
I, {π} |=ν
p(t1 , . . . , tn )
iff
p(t1 , . . . , tn ). Here {π} is a set of mIpath (π) |=classic
ν
paths that contains only one m-path, π, and p(t1 , . . . , tn )
is an atomic formula. Recall that Ipath (π) is a usual firstorder semantic structure, so |=classic here denotes the usual,
classical first-order entailment.

1. By (3) in Definition 3: {
∅ {cp}} |= (cp ∨ no),
and {
∅ {no}} |= (cp ∨ no)
2. By (5) in Definition 3: {
∅ {ab}, 
∅ {cm}} |= (ab  cm)
3. By (6) in Definition 3:
{
∅ {st} {st, ab}, 
∅ {st} {st, cm}} |= st ⊗ (ab  cm)

Typically, p(t1 , ..., tn ) is either defined via rules (as in Example 1) or is a “built-in,” such as insert(q(a, b)), with a
fixed meaning. For instance, in case of insert(q(a, b)) the
meaning would be that I, {π} |=ν insert(q(a, b)) iff π is
an m-path of the form 
d1 d2 , which consists of a single
path, and d2 = d1 ∪ {q(a, b)}.4 These built-ins are called
elementary updates and constitute the basic building
blocks from which more complex actions, such as those at
the end of Example 1, are constructed.

4. By (6) in Definition 3: Hence there are four possible
outcomes for φ;
{
∅{st}{st, ab}{st, ab, cp}, 
∅{st}{st, cm}{st, cm, cp}}
{
∅{st}{st, ab}{st, ab, cp}, 
∅{st}{st, cm}{st, cm, no}}
{
∅{st}{st, ab}{st, ab, no}, 
∅{st}{st, cm}{st, cm, cp}}
{
∅{st}{st, ab}{st, ab, no}, 
∅{st}{st, cm}{st, cm, no}}
Definition 4. (Playset) As in classical logic, φ ∨ ¬φ is a tautology for any φ, i.e., it is true on every set of m-paths. We denote this
tautology with a special proposition Playset, an analogue of true
in classical logic.

2. Negation: I, S |=ν ¬φ iff it is not the case that I, S |=ν φ.
3. Reasoner’s Disjunction: I, S |=ν φ ∨ ψ iff I, S |=ν φ or
I, S |=ν ψ. We define φ ∧ ψ as a shorthand for ¬(¬φ ∨ ¬ψ).

By definition, I, S |= Playset for any m-path structure and any
set of m-paths. Therefore, ¬Playset is unsatisfiable. Intuitively,
Playset is the game in which all outcomes are possible, while
¬Playset is a game with no outcomes.

4. Opponent’s Conjunction: I, S |= ν φ  ψ iff S =
S1 ∪ S2 , for some pair of m-path sets, such that I, S1 |=ν φ ,
and I, S2 |=ν ψ. The dual connective, 	, also exists, but is
not used in this paper.

4. PROOF THEORY

5. Serial Conjunction: I, S |=ν φ ⊗ ψ iff there
S is a set R
of m-paths, such that S can be represented as π∈R π ◦ Tπ ,
where each Tπ is a set of m-paths, I, R |=ν φ , and for each
Tπ , I, Tπ |=ν ψ.
Here π◦T = {π ◦ δ | δ ∈ T }, where π ◦ δ is an m-path obtained by appending the m-path δ to the end of the m-path π.
(For instance, if π = 
d1 d2 , d3 d4  and δ = 
d5 d6 , d7 d8 d9 
then π ◦ δ = 
d1 d2 , d3 d4 , d5 d6 , d7 d8 d9 .) In other words,
R is a set of prefixes of the m-paths in S.

4.1 Execution as Entailment
We now define executional entailment, a concept that connects
model theory with the execution of a certain strategy of the reasoner.
Definition 5. (Executional Entailment) Let φ be a CT R-S
goal and W a set of rules that define services (see Definition 1).
Let D0 , ..., Dn be a sequence of database states. A path tree
with a shared pref ix D0 , ..., Dn is a set of paths where each
begins with this sequence of states. We define

6. Concurrent Conjunction:
I, S |= ν φ | ψ iff there
is aSset R of m-paths, such that S can be represented
as π∈R πTπ , where each Tπ is a set of m-paths, and

D0 , ..., Dn --- |= (∃)φ

• either I, R |=ν φ and for all Tπ , I, Tπ |=ν ψ;

(4.1)

to mean that for every model M of φ there is a path tree T such
that M, T |= (∃)φ and 
D0 , ..., Dn  is a shared prefix of T . Here
(∃) indicates that all variables in φ are quantified existentially.

4

Formally, the semantics of such built-ins is defined using the notion of the transition oracle [6].

147

1. Applying transaction definitions: Let b←β be a rule in
P, and assume that its variables have been renamed so
that none are shared with ψ. If a and b unify with mgu
(most general unifier) σ then

Intuitively, (4.1) means that the reasoner playing the game φ can
ensure that the execution will begin with the database state D0 and
continue with D1 , ..., Dn .
Observe that executional entailment is defined over path trees,
not over arbitrary outcomes. Hence, when we talk about execution
of a game we are only interested in enforcible outcomes that can
reduce to a path tree. We call these executable outcomes.
We are interested in these outcomes because ultimately we want
to obtain strategires that contain complete plays—plays that represent movements of all the players involved. Such a play must be
represented by a path, not m-path, because m-paths are incomplete
plays—they contain gaps, which must be filled by external players.
The plays in an outcome that represents a strategy must form a
path tree because all the plays in an outcome of a real game start
with the same initial state D0 . Thus, finding out if a winning strategy exists in state D0 is tantamount to proving W, D0 --- |= (∃)φ.

P, D ---  (∃) ψ  σ
where ψ  is ψ where a hot ocP, D ---  (∃) ψ
currence of a is replaced by β.
For instance, if ψ = (c  e) | (a ⊗ f ) | (d ∨ h) and
the hot component in question is a in the middle subformula, then ψ = (c  e) | (β ⊗ f ) | (d ∨ h).
2. Querying the database: If Od (D) |=c (∃)aσ; aσ and
ψ  σ share no variables; then
P, D ---  (∃) ψ  σ
where ψ  is obtained from ψ
P, D ---  (∃) ψ
by deleting a hot occurrence of a.

4.2 Inference Rules

For instance, if ψ = (c  e) | (a ⊗ f ) | (d ∨ h) and
the hot component is a in the middle subformula, then
ψ  = (c  e) | f | (d ∨ h).

We now develop an inference system for proving statements of
the form W, D0 --- |= (∃)φ, where W is a set of rules and φ is
a CT R-S goal. The system manipulates expressions of the form
W, D0 ---  φ, called sequents.
First we need the notion of the hot component of a formula;
it is a generalization of a similar notion from [5]: hot(φ) is a set
of subformulas of φ, defined by induction on the structure of φ as
follows:

3. Executing elementary updates: If Ot (D1 , D2 ) |=c
(∃)aσ; aσ and ψ σ share no variables, then
P, D2 ---  (∃) ψ  σ
where ψ  is obtained from
P, D1 ---  (∃) ψ
ψ by deleting a hot occurrence of a.

1. hot(φ) = {φ}, if φ is an atomic formula

For instance, if ψ = (c  e) | (a ⊗ f ) | (d ∨ h) and
the hot component is a in the middle subformula, then
ψ  = (c  e) | f | (d ∨ h).
Note that in this rule the current state changes from D2
to D1 .

2. hot(φ ⊗ ψ) = hot(φ)
3. hot(φ | ψ) = hot(φ) ∪ hot(ψ)
4. hot(φ ∨ ψ) = {φ ∨ ψ}

4. Reasoner’s move: Let ψ be a formula with a hot component η of the form α∨β. Then we have the following
pair of inference rules, which can lead to two independent possible derivations.

5. hot(φ  ψ) = {φ  ψ}.
Note that in cases of ∨ and , the hot component is a singleton set
that contains the very formula that is used as an argument to hot.
Here are some examples of hot components:
a⊗b⊗c
(a ⊗ b) | (c ⊗ d)
(a  b) ⊗ c
(a  b) | (c ∨ d)
((a  b) ⊗ c) ∨ ((f | g) ⊗ h)

P, D ---  (∃) ψ 
P, D ---  (∃) ψ

{a}
{a, c}
{a  b}
{a  b, c ∨ d}
{((a  b) ⊗ c) ∨ ((f | g) ⊗ h)}

P, D ---  (∃) ψ 
P, D ---  (∃) ψ

Here ψ  is obtained from ψ by replacing the hot component η with α and ψ is obtained from ψ by replacing
η with β.
For instance, if ψ = (c  e) | (a ⊗ f ) | (d ∨ h) and the
hot component η is d∨h, then ψ = (ce) | (a⊗f ) | d
and ψ = (c  e) | (a ⊗ f ) | h.

Note that a hot component represents a particular occurrence of
a subformula in a bigger formula. For instance, hot(a ⊗ b ⊗ a)
is {a}, where a corresponds to the first occurrence of this subformula in a ⊗ b ⊗ a and not the second one. This point is important
because in the inference rules, below, we will sometime say that
ψ  is obtained from ψ by deleting a hot occurrence of a (or some
other subformula). Thus, in the above, deleting the hot component
a leaves us with b ⊗ a, not a ⊗ b.
The inference rules are as follows:

5. Opponent’s move: Let ψ be a formula with the hot
component, τ , of the form γ  δ. Then we have the
following inference rule:
and
P, D ---  (∃) ψ 
P, D ---  (∃) ψ 
P, D ---  (∃) ψ
where ψ  is obtained from ψ by replacing the hot component τ with γ and ψ is obtained from ψ by replacing
τ with δ.
For instance, if ψ = (c  e) | (a ⊗ f ) | (d ∨ h) and the
hot component τ is ce, then ψ = c | (a⊗f ) | (d∨h)
and ψ = e | (a ⊗ f ) | (d ∨ h).
Note that unlike in the reasoner’s case when we have
two inference rules, the opponent’s case is a single inference rule. It says that in order to prove
P, D ---  (∃) ψ (i.e., to execute ψ on a set S of

Axiom: P, D ---  ( ), for any D.
Here ( ) is the empty CT R-S goal; it represents a game that
starts and stops in the same state without making any moves.
The axiom says that such a game can be played in any state.
Inference Rules: In Rules 1–4 below, σ is a substitution, ψ and
ψ  are concurrent serial conjunctions, and a is a formula in
hot(ψ).

148

constraints, denoted P RIMIT IVE . Item 4 defines the set C ON ST R
of all constraints.

paths emanating from D) we need to be able to execute
ψ  on a set S1 of paths emanating from D and ψ on
another (possibly the same) set S2 of paths emanating
from D such that S = S1 ∪ S2 .

1. Elementary primitive constraints: If e ∈ E VEN T is an
event, then ∗e and ∗(¬e) are primitive constraints. Informally, the constraint ∗e is true on a set S of m-paths in an
m-path structure I = (U, IF , Ipath ) iff e occurs on every mpath in S. Similarly, ∗(¬e) is true on S iff e does not occur
on any m-path in S.

T HEOREM 1 (S OUNDNESS OF I NFERENCE S YSTEM ). Under the Horn conditions, the entailment P, D --- |= (∃) φ holds
if there is a deduction of the sequent P, D ---  φ in the above
inference system.

Formally, ∗e says that every execution of the contract, i.e.,
every m-path 
p1 , ..., pi , ..., pn  ∈ S, includes a path, pi , of
the form d1 ...dk dk+1 ...dm , such that for some pair of adjacent states, dk and dk+1 , the event e occurs at dk and causes
a state transition to dk+1 , i.e., Ipath (
dk dk+1 ) |=classic e
(see Definition 3). The constraint ∗(¬e) means that e does
not occur on any m-path in S.

We conjecture that the above proof theory is also complete for
workflow control specifications. The importance of the proof theory in CT R-S is that it can be used to execute workflow specifications. When these specifications represent a service contract, the
inference system will be able to execute the contract.

5.

CONSTRAINTS ON CONTRACT
EXECUTION

2. Disjunctive and Conjunctive Primitive constraints: Any
∨, ∧, or ¬ combination of propositions from E VEN T is allowed under the scope of ∗. For instance, ∗(e1 ∨ e2 ) and
∗(e1 ∨ ¬e2 ) are allowed. The former is true on a set of mpaths, S, if either e1 or e2 occurs on every m-path in S. The
latter is true if, for every m-path in S, the occurrence of e1 on
the m-path implies that e2 occurs on the same m-path. For
∗(e1 ∨ ¬e2 ), we will use the abbreviation ∗(e2 → e1 ).

In [8], we have shown how a large class of constraints on workflow execution can be expressed in CT R. In CT R-S we are interested in finding a similar class of constraints, which could be
used to denote the desirable properties of a contract, as explained
at the end of Sections 1 and 2.2. In this context, verification of
a constraint against a contract means that the reasoner has a way
of executing the contract so that the constraint will hold no matter
what the other parties do (for instance, that the goods are delivered
or the payment is refunded regardless). Our verification algorithm
requires that behavioral specifications of contracts have no loops
in them and that they have the unique event property defined below.5 The no-loops requirement is captured by the restriction that
the workflow rules are non-recursive (so having rules is just a matter of convenience, which does not increase the expressive power).
We assume that there is a subset of propositions, E VEN T , which
represents the “interesting” events that occur in the course of workflow execution. These events are the building blocks of both workflows and constraints. In terms of Definition 3, these propositions
would be defined as built-in elementary updates.

3. Serial primitive constraints: If e1 , ..., en ∈ E VEN T then
∗(e1 ⊗ ... ⊗ en ) is a primitive constraint. It is true on a set
of m-paths S iff e1 occurs before e2 before ... before en on
every path in S.
4. Complex constraints: The set of all constraints, C ON ST R,
consists of all Boolean combinations of primitive constraints
(i.e., constraints defined by Items 1–3) using the connectives
∨ and ∧: φ ∧ ψ (resp. φ ∨ ψ) is satisfied by a set of m-paths
S iff S satisfies φ and (resp. or) ψ.
It can be shown that under the unique event assumption any serial
primitive constraint can be decomposed into a conjunction of binary serial constraints. For instance, ∗(e1 ⊗ e2 ⊗ e3 ) is equivalent
to ∗(e1 ⊗ e2 ) ∧ ∗(e2 ⊗ e3 ). Here are some typical constraints in
C ON ST R and their real-world meaning:
∗e − event e should always eventually happen;
∗e ∧ ∗f − events e and f must always both occur (in some order);
∗(e ∨ f ) − always either event e or event f or both must occur;
∗e ∨ ∗f − either always event e occurs or always event f occurs;
∗(¬e ∨ ¬f ) − it is not possible for e and f to happen together;
∗(e → f ) − if event e occurs, then f must also occur.

Definition 6. (Unique-event property) A CT R-S workflow W
has the unique event property if and only if every proposition in E VEN T can execute at most once in any execution of W.
Formally, this can be defined both model-theoretically and syntactically. The syntactic definition is that for every proposition
e ∈ E VEN T :
If W is W1 ⊗ W2 or W1 | W2 and e occurs in W1
then it cannot occur in W2 , and vice versa.
For workflows with no loops, we can always rename different occurrences of the same type of event to satisfy the above property.
We shall call such workflows unique event workf lows.

Example 3. (Contract Goals) The actors in the procurement
workflow of Example 1 may want to ensure that they have a way to
reach their goals within the scope of the contract. We express these
goals using the following set of constraints:

Definition 7. (Constraints) Let φ be a -free formula. Then
∗φ denotes a formula that is true on a set of m-paths, S, if and only
if φ is true on every m-path in S. The operator ∗ can be expressed
using the basic machinery of CT R-S.
Our constraints on workflow execution, defined below, will all
be of the form ∗φ because, intuitively, the most common thing that
a reasoner wants is to make sure that every execution in the outcome has certain desirable properties. Items 1–3 define primitive

∗(approve ∧ ¬cancel → satisf ied)
if financing is approved and buyer does not cancel
then she wants to be satisfied
∗(cancel → keep escrow)
if buyer cancels, then seller keeps the escrow
∗(make payment → deliver)
if buyer pays, then seller must deliver

5
This assumption is made by virtually all formal approaches to
workflow modeling (e.g., [2, 22]) and even such specification languages as WSFL — IBM’s proposal for a Web service specification
language that was one of the inputs to BPEL4WS [15].

149

6.

(α)∗α = α, and to compute (W1  W2 )Φ it suffices to compute
(W1 )Φ  (W2 )Φ .
Enforcing serial constraints. Next we deal with constraints of
the form ∗(α ⊗ β). Let α, β ∈ E VEN T and let W be a CT R-S
workflow. We define W∗(α⊗β) as

ENFORCEMENT OF EXECUTION
CONSTRAINTS

Given a contract represented as a CT R-S workflow, W, and a
reasoner’s goal specified as a constraint, Φ ∈ C ON ST R, we would
like to construct another workflow, WΦ , that represents a class of
strategies that enforce Φ within the rules of the contract. “Enforcement” here means that as long as the opponent plays by the rules of
the contract (i.e., chooses only the alternatives specified by the connective), the reasoner can still ensure that all the plays belong to
an outcome that satisfies the constraints. In CT R-S this amounts to
computing W ∧ Φ — the formula that represents the collection of
all reasoner’s strategies for finding the outcomes of W that satisfy
the constraint Φ.
We must clarify what we mean by “computing” W ∧ Φ. After
all, W ∧ Φ already is a representation of all the winning strategies
for the reasoner and all outcomes of W ∧ Φ satisfy Φ. However,
this formula is not an explicit set of instructions to the workflow
scheduler. In fact, it is not even a workflow specification in the
sense of Definition 1. In contrast, the formula WΦ that we are after
is a workflow specification. It does not contain the ∧-connective,
and it can be directly executed by the CT R-S proof theory. This
means that the proof theory can be used to execute the contract W
in such a way that the reasoner’s objectives Φ will be satisfied. The
precise relationship between W ∧ Φ and WΦ will be established in
Definition 8. Informally speaking, the two formulas are equivalent
in the sense that they have exactly the same executions modulo
certain synchronization acts.
We now develop an algorithm for computing WΦ through a series of transformations.

sync(α < β, (∗α ∧ (∗β ∧ W)))
where sync is a transformation that synchronizes events in the
right order. It uses elementary updates send(ξ) and receive(ξ)
and is defined as follows: sync(α < β, W) = W  , where W 
is like W, except that every occurrence of event α is replaced with
α⊗send(ξ) and every occurrence of β with receive(ξ)⊗β, where
ξ is a new constant.
The primitives send and receive can be defined as
insert(channel(ξ)) and channel(ξ) ⊗ delete(channel(ξ)),
respectively, where ξ is a new proposition symbol. The point here
is that these two primitives can be used to provide synchronization:
receive(ξ) can become true if and only if send(ξ) has been
executed previously. In this way, β cannot start before α is done.
The following examples illustrate the definition of W∗(α⊗β) :
(β ⊗ α)∗(α⊗β) = receive(ξ) ⊗ β ⊗ α ⊗ send(ξ)
(α | β | ρ1 | ... | ρn )∗(α⊗β) =
(α ⊗ send(ξ)) | (receive(ξ) ⊗ β) | ρ1 | ... | ρn )
Note that W∗(α⊗β) is not logically equivalent to W ∧ ∗(α ⊗ β),
but these two formulas are behaviorally equivalent as defined next.
Definition 8. (Behavioral Equivalence) A CT R-S formula φ
behaviorally entails ψ iff for every m-path structure I =
(U, IF , Ipath ) and a set on m-paths S, if I, S |= φ then there is
another set S  such that I, S  |= ψ, where S  and S are congruent
modulo synchronization; namely, these sets of m-paths reduce to
the same set of m-paths under the following operations:

Enforcing complex constraints. Let ∗C1 , ∗C2 ∈ C ON ST R, W
be a CT R-S workflow, then
(∗C1 ∨ ∗C2 ) ∧ W ≡ (∗C1 ∧ W) ∨ (∗C2 ∧ W)
(∗C1 ∧ ∗C2 ) ∧ W ≡ (∗C1 ∧ (∗C2 ∧ W))

• Remove all synchronization messages inserted by the
send and receive primitives (i.e., the atoms of the form
channel(ξ)) from all database states in S  and S.

Thus, we can compute W(∗C1 ∨∗C2 ) as W∗C1 ∨ W∗C2 and
W(∗C1 ∧∗C2 ) as W∗C1 ∧ W∗C2 .
Enforcing elementary constraints. The following transformation
takes an elementary primitive constraint Φ of the form ∗α or ∗¬α
and a CT R-S unique-event workflow W, and returns a CT R-S
workflow that is equivalent to W ∧ Φ. Let α, β ∈ E VEN T and
W1 , W2 be CT R-S workflows. Then:
∗α ∧ α
∗α ∧ β

≡ α
≡ ¬Playset

∗¬α ∧ α
∗¬α ∧ β

• Eliminate adjacent duplicate states from all m-paths (i.e., if
π is an m-path of the form 
..., d1 ...dk dk+1 ..., ... and dk
is the same state as dk+1 then delete dk+1 from π. (After the
previous step, such adjacent duplicate states normally correspond to state transitions that occur due to the execution of
send and receive.)

≡ ¬Playset
≡ β if α = β

∗α ∧ (W1 ⊗ W2 ) ≡ (∗α ∧ W1 ) ⊗ W2 ∨ W1 ⊗ (∗α ∧ W2 )
∗¬α ∧ (W1 ⊗ W2 ) ≡ (∗¬α ∧ W1 ) ⊗ (∗¬α ∧ W2 )
∗α ∧ (W1 | W2 ) ≡ (∗α ∧ W1 ) | W2 ∨ W1 | (∗α ∧ W2 )
∗¬α ∧ (W1 | W2 ) ≡ (∗¬α ∧ W1 ) | (∗¬α ∧ W2 )

φ and ψ are behaviorally equivalent if each behaviorally
entails the other.
Proposition 1. (Enforcing Elementary and Serial Constraints) The above transformations compute a CT R-S workflow
that is behaviorally equivalent to W ∧ Φ, where Φ is an elementary
or serial constraint and W is a unique event workflow.

(Recall that Playset was introduced in Definition 4.) These logical equivalences are identical to those used for workflows of cooperating tasks in [8]. The first equivalence below is specific to
CT R-S. Here we use Φ to denote ∗α or ∗¬α:

Enforcing conjunctive primitive constraints. To enforce a primitive constraint of the form ∗(Φ1 ∧ ... ∧ Φm ), where all Φi are elementary, we utilize the logical equivalence ∗(Φ1 ∧ ... ∧ Φm ) ≡ ∗
Φ1 ∧ ... ∧ ∗Φm (and the earlier equivalences for enforcing complex
constraints).
Enforcing disjunctive primitive constraints. These constraints
have the form ∗(Φ1 ∨ ... ∨ Φn ), where all Φi are elementary constraints. Enforcement of such constraints relies on the following
lemma.

Φ ∧ (W1  W2 ) ≡ (Φ ∧ W1 )  (Φ ∧ W2 )
Φ ∧ (W1 ∨ W2 ) ≡ (Φ ∧ W1 ) ∨ (Φ ∧ W2 )
For example, if W is abort  prepare ⊗ (abort ∨ commit), then:
∗abort ∧ W ≡ abort  (prepare ⊗ abort)
∗¬abort ∧ W ≡ ¬Playset
The above equivalences enable us to decompose the problem of computing WΦ into simpler problems. For instance,

150

buy

Lemma 1. (Disjunctive Primitive Constraints) Let Φi be elementary constraints. Then

n3
pay_escrow

∗(Φ1 ∨ ... ∨ Φn ) ≡ (∗¬Φ2 ∧ ... ∧ ∗¬Φn → ∗Φ1 )  ...
 (∗¬Φ1 ∧ ... ∧ ∗¬Φi−1 ∧ ∗¬Φi+1 ∧ ... ∧ ∗¬Φn → ∗Φi )  ...
 (∗¬Φ1 ∧ ... ∧ ∗¬Φn−1 → ∗Φn )

sell
n4

finance
n1

This equivalence allows us to decompose the set of all plays in an
outcome into subsets that satisfy the different implications shown
in the lemma. Unfortunately, enforcing such implications is still not
easy. Unlike the other constraints in this section, enforcement of the
implicational constraints cannot be described by a series of simple
equivalences. Instead, we have to resort to equivalence transformations defined with the help of parse trees of CT R-S formulas that
represent unique event workflows.

n2
reserve

n6
approve

reject
cancel

deliver

n5
make_payment
n10
cancel

n8
keep_escrow
n7
uninsured
n9

n11

insured
lost

Definition 9. (Maximal guarantee for an event) Let ∗Φ be an
elementary constraint (i.e., Φ is e or ¬e), W be a formula for a
unique event workflow, and ϕ be a subformula of W. Then ϕ is
said to be a maximal guarantee for ∗Φ iff

delivered
delivered
satisfied

lost

satisfied

satisfied

1. (W ∧ (ϕ | Playset)) |= ∗ Φ
2. ϕ is a maximal subformula of W that satisfies (1)

Figure 1: Workflow parse tree and workflow graph for Example 3

Intuitively, a maximal guarantee for ∗e is any maximal subformula,
φ, of W such that e occurs in every execution of φ. A maximal
guarantee for ∗¬e is such a maximal subformula, φ, of W that
e does not occur in any execution of φ. The set of all maximal
guarantees for an elementary event ∗Φ is denoted by GS∗Φ (W).

GS∗¬e (W): Let T be the set of nodes in the parse tree of W that
belong to any of the subformulas ϕ ∈ GS∗e (W) or ψ ∈
coExec(W, ϕ). Then, η ∈ GS∗¬e (W) iff it is a subformula
of W such that its subtree contains no nodes in T and η is a
maximal subformula with such a property.

Definition 10. (Co-executing sub-formulas) Let W be a formula for a unique event workflow and ψ, ϕ be a pair of subformulas of W. We say that ψ coexecutes with ϕ in W, denoted
ψ ∈ coExec(W, ϕ), iff

Example 4. (Computation of maximal guarantees and coexecution) The workflow parse tree for Example 3 is shown in
Figure 1. For the event approve, the highest node that can be
reached by the aforesaid marking procedure is n1 and there are
no other such nodes. Therefore, GS∗approve (W) is {approve ⊗
(make payment∨cancel)}. The set of co-executing subformulas
for n1, coExec(W, n1), consists of two formulas that correspond
to the nodes n3 and n4 in the figure. The only maximal guarantee
for ¬approve is the subformula reject ⊗ cancel, which corresponds to node n2.

1. (W ∧ (ϕ | Playset)) |= (ψ | Playset),
2. φ and ψ are disjoint subformulas in W, and
3. ψ is a maximal subformula in W satisfying (1) and (2)
Intuitively members of coExec(W, ϕ) correspond to maximal
sub-formulas of W that must be executed whenever ϕ executes as
part of the workflow W.
Proposition 2. (Computing GS∗Φ (W) and coExec(W, ϕ))
and
The
following
procedures
compute
GS∗Φ (W)
coExec(W, ϕ). They operate on the parse tree of W, which
is defined as usual: the inner nodes correspond to composite
subformulas and the leaves to atomic subformulas. Thus, the
leaves are labeled with their corresponding atomic subformulas,
while the inner nodes are labeled with the main connective of the
corresponding composite subformula.

procedure Compute W(∗Φ1 ∧...∧∗Φn →∗Φ)
1. if W |= ∗Φ1 ∧ ... ∧ ∗Φn then Compute W∗Φ
2. else Guard(g) := ∅ for all g ∈ subf ormulas(W)
3.
for each i such that W |= ∗ Φi do
4.
for each f ∈ GS∗¬Φi (W) do
5.
if ∃h ∈ coExec(W, f ) and (h ∧ ∗Φ) is satisfiable
6.
then Rewrite f to send(ξ) ⊗ f and
7.
for every g ∈ GS∗¬Φ (h) set
8.
Guard(g) := Guard(g) ∪ {receive(ξ)}
9.
else Compute sibling(f )(∗Φ1 ∧...∧∗Φn →∗Φ)
10.
for each g ∈ subf ormulas(W) do
11.
if Guard(g) = ∅ then
W
12.
rewrite g to ( receive(ξ)∈Guard(g) receive(ξ)) ⊗ g

GS∗e (W): The set of subformulas that correspond to the nodes in
the parse tree of W that are closest to the root of the tree and
can be reached by the following marking procedure: (i) mark
all the leaves labeled with e; (ii) mark any node corresponding to a subformula of W of the form (ϕ ⊗ ψ) or (ϕ | ψ) if
either the node for ϕ or for ψ is marked; (iii) mark any node
corresponding to a subformula of the form (ϕ∨ψ) or (ϕψ)
if both the node for ϕ and the node for ψ are marked.

Figure 2: Computation of W(∗Φ1 ∧...∧∗Φn →∗Φ)

coExec(W, ϕ): Consider a subformula of W of the form ψ1 | ψ2 ,
ψ2 | ψ1 , ψ1 ⊗ ψ2 , or ψ2 ⊗ ψ1 , where φ is a subformula of
ψ1 . Suppose that ψ2 is a maximal subformula with such a
property, i.e., W has no subformula of the form ψ1 | ψ2 ,
ψ2 | ψ1 , ψ1 ⊗ ψ2 , or ψ2 ⊗ ψ1 , respectively, such that φ is a
subformula of ψ1 and ψ2 is a subformula of ψ2 (ψ2 = ψ2 ).
Then, ψ2 ∈ coExec(W, ϕ).

Recall that, according to Lemma 1, in order to enforce a disjunctive constraint we need to learn how to enforce implicational constraints of the form ∗Φ1 ∧ ... ∧ ∗Φn → ∗Φ, where Φ and the Φi s
are elementary. This is done using the algorithm in Figure 2, which
computes a workflow that is equivalent to (∗Φ1 ∧ ... ∧ ∗Φn →

151

∗Φ) ∧ W. If the antecedent of the constraint is true during an
execution, then (in line 1) ∗Φ must be enforced on W. Otherwise,
for every ∗Φi that is not true everywhere, we identify the subformulas f ∈ GS∗¬Φi (lines 2-3). Note that, whenever subformulas
in GS∗¬Φi are executed the constraint ∗Φ1 ∧ ... ∧ ∗Φn → ∗Φ is
vacuously true. In lines 4-6, we identify the subformulas h of W
that co-execute with the formulas f ∈ GS∗¬Φi . If ∗Φ is enforcible
in any of these subformulas h, i.e., h ∧ ∗Φ is satisfied (there can
be at most one such subformula h per f , due to the unique event
property, Definition 6), then we enforce the above constraint by delaying executions of those subformulas in h that violate ∗Φ (these
are exactly the g’s in line 7) until it is guaranteed that the execution
moves into f ∈ GS∗¬Φi , because once f is executed our constraint
becomes satisfied. This delay is achieved by synchronizing the executions of f to occur before the executions of g by rewriting f into
send(ξ) ⊗ f (in line 6) and by adding receive(ξ) to the guard for
g (in line 8). Otherwise, if no such h exists, in line 9, we explicitly
enforce the constraint on the sibling nodes (in the parse tree of W)
of the formulas f ∈ GS∗¬Φi (because an outcome that satisfies
∗Φi might exist in a sibling). Finally, in lines 10-12, we make sure
that the execution of every g that has a non-empty guard is conditioned on receiving of a message from at least one f with which g
is synchronized.

Proposition 3. (Enforcing disjunctive primitive constraints)
The above algorithm for enforcing disjunctive primitive constraints
computes a CT R-S workflow WΦ that is behaviorally equivalent
to W ∧ ∗(Φ1 ∨ ... ∨ Φn ) where Φi are elementary constraints.
T HEOREM 2 (D ISJUNCTIVE C ONSTRAINTS ). Let W be a
control flow graph and ∗Φ ∈ P RIMIT IVE be a disjunctive primitive constraint. Let |W| denote the size of W, and d be the
number of elementary disjuncts in ∗Φ. Then the worst-case size
of WΦ is O(d × |W|), and the time complexity is O(d × |W|2 ).
V
Enforcement of arbitrary constraints. If Φ = N (∨j P rim)
where P rim ∈ P RIMIT IVE , we compute WΦ by applying the
above transformations for complex and elementary constraints.
Each transformation is either a logical equivalence or a behavioral
equivalence. Therefore, WΦ is behaviorally equivalent to W ∧ Φ.
T HEOREM 3 (A RBITRARY C ONSTRAINTS ). Let W be a
R be a set of global
control flow graph W and Φ ⊂ C ON ST V
constraints in the conjunctive normal form N (∨j P rim) where
P rim ∈ P RIMIT IVE . Let |W| denote the size of W, N be
the number of constraints in Φ, and d be the largest number of
disjuncts in a primitive constraint in Φ. Then the worst-case size
of WΦ is O(dN ×|W|), and the time complexity is O(dN ×|W|2 ).

Example 5. (Procurement Workflow, contd.) The algorithm in
Figure 2 creates the following workflow by applying the constraints
in Example 3 to the procurement workflow in Example 1. Refer to
the parse tree for that workflow in Figure 1.

Cycle detection and removal. We can still improve the above
transformation by eliminating certain “useless” parts of WΦ —the
parts that will never be executed. The problem is that even though
WΦ is an executable workflow specification, WΦ may have subformulas where the send/receive primitives cause a cyclic wait.
This means that those parts of WΦ can never be involved in an
execution. Fortunately, we can show that all cyclic waits can be
removed from W in time O(|W|3 ).

• To enforce (∗cancel → ∗keep escrow) we first compute GS∗¬cancel (buy) = {n5}, and coExec(buy, n5) =
{n3, n4, n6}. Of these, n4 (substituted for h) satisfies the
conditions on line 5 of the algorithm in Figure 2. Since
GS∗¬keep escrow (n4) = {n7}, we insert a synchronization
from node n5 to n7 shown in Figure 1 as a dotted line. This
ensures that if the buyer cancels the procurement workflow,
the seller collects the escrow.

Example 6. (Cyclic Wait Removal) Let W be (a ∨ b) ⊗ (c  d)
and Φ be (∗c → ∗a). Our algorithm transforms W ∧ Φ into (a ∨
receive(ξ)⊗b)⊗(c(send(ξ)⊗d)). Now, if the reasoner chooses
b, a deadlock occurs. However, we can rewrite this formula into a
behaviorally equivalent formula a ⊗ (c  d) and avoid the problem.

• To enforce (∗approve ∧ ∗ ¬cancel → ∗satisf ied),
we compute GS∗¬approve(buy) = {n2} and notice that
n4 ∈ coExec(buy, n2)} satisfies the conditions in line 5
of the algorithm in Figure 2. Since GS∗¬satisf ied (n4) =
{n8, n9}, we insert a synchronization from node n2 to n8
and n9 which yields the dotted edges in Figure 1. We
also compute GS∗cancel (buy) = {n10, n2} and notice
that n4 ∈ coExec(buy, n2)}, n4 ∈ coExec(buy, n10)},
and n4 satisfies the conditions in line 5 of the algorithm.
Since GS∗¬satisf ied (n4) = {n8, n9}, we insert a synchronization from the nodes n10 and n2 to n8 and n9,
which yields the dotted edges in Figure 1. This synchronization ensures that if buyer’s financing is approved and he
chooses to make the payment and buy the item then delivery must use the insured method. Also, once the constraint
(∗make payment → ∗deliver) is enforced too, the seller
can no longer pocket the escrow. The resulting strategy is:

7. CONCLUSION AND RELATED WORK

We presented a novel formalism, CT R-S, for modeling the dynamics of service contracts. CT R-S is a logic in which service contracts are represented as formulas that specify the various choices
that are allowed for the parties to the contracts. The logic permits
the reasoner to state the desired outcomes of the contract execution
and verify that a desired outcome can be achived no matter what
the other parties do as long as they obey the rules of the contract.
There is a body of preliminary work trying to formalize the representation of Web service contracts [20, 11], but none deals with
the dynamics of such contracts, which is the main subject of this
paper. Technically, the works closest to ours come from the fields
of model checking and game logics.
Process algebras and alternating temporal logic [7, 1] have been
used for modeling open systems with game semantics. Model
buy ← pay escrow ⊗ (f inance | sell)
checking is a standard mechanism for verifying temporal properf inance ← (approve ⊗ ((send(ξ1 ) ⊗ make payment) ∨
ties of such systems and deriving automata for scheduling. In [16],
((send(ξ3 ) ⊗ cancel)))  (send(ξ2 ) ⊗ (reject ⊗ cancel))
the complexity and size of computing the winning strategies for
sell ← reserve item ⊗ ((receive(ξ1 ) ⊗ deliver)
infinite games played on finite graphs are explored. A result anal∨ ((receive(ξ2 ) ∨ receive(ξ3 )) ⊗ keep escrow))
ogous to ours is obtained for infinite games: assuming the size of
deliver ← insured ∨ ((receive(ξ2 ) ∨ receive(ξ3 )) ⊗ uninsured) the graph is Q and the size of the winning condition is W , the cominsured ← (delivered ⊗ satisf ied)  (lost ⊗ satisf ied)
plexity of computing winning strategies is exponential in the size
uninsured ← (delivered ⊗ satisf ied)  lost
of W and polynomial in the size of the set Q.

152

The use of CT R-S has enabled us to find a more efficient verification algorithm than what one would get using model checking.
Indeed, standard model checking techniques [7, 1] are worst-case
exponential in the size of the entire formula and the corresponding
scheduling automata are also exponential. This is often referred to
as the state-explosion problem. In contrast, the size of our solver
is linear in the size of the original workflow specification and exponential only in the size of the constraint set (Theorem 3), which
is a much smaller object. In a sense, our solver can be viewed as
a specialized and more efficient model checker for the problem at
hand. It accepts high level specifications of workflows and yields
strategies and schedulers in the same high level language.
Logic games have been proposed before in other contexts [13,
19]. As in CT R-S, validity of a statement in such a logic means
that the reasoner has a winning strategy against the opponent. In
CT R-S however, games, winning conditions, and strategies are
themselves logical formulas (rather than modal operators). Logical equivalence in CT R-S is a basis for constructive algorithms
for solving games and synthesizing strategies, which are in turn
executable by the proof theory of CT R-S. Related game logic formalisms, such as [13, 19], only deal with assertions about games
and their winning strategies. In these logics, games are modalities
rather than executable specifications, so they can only be used for
reasoning about Web service contracts, but not for modeling and
executing them.
Related work in planning, where goals are expressed as temporal
formulas, includes [3]. In [3], plans are generated using a forward
chaining engine that generates finite linear sequences of actions. As
these linear sequences are generated, the paths are incrementally
checked against the temporal goals. This approach is sound and
complete. However, in the worst case it performs an exhaustive
search of the model similar to the model checking approaches.
For the future work, we are planning to extend our results to allow contracts that include iterative behaviour. Such contracts can
already be specified in CT R-S. However, iteration requires new
verification algorithms to enable reasoning about the desired outcomes of such contracts.

8.

[7] E.M. Clarke, E.A. Emerson, and A.P. Sistla. Automatic
verification of finite-state concurrent systems using temporal
logic specifications. In ACM Transactions on Programming
Languages and Systems (TOPLAS), pages 244–263, 1986.
[8] H. Davulcu, M. Kifer, C.R. Ramakrishnan, and I.V.
Ramakrishnan. Logic based modeling and analysis of
workflows. In ACM Symposium on Principles of Database
Systems, pages 25–33, Seattle, Washington, June 1998.
[9] E.A. Emerson. Temporal and modal logic. In Handbook of
Theoretical Computer Science, pages 997–1072. Elsevier
and MIT Press, 1990.
[10] R. Fagin, J. Y. Halpern, Y. Moses, and Moshe Y. Vardi.
Reasoning About Knowledge. MIT Press, 1994.
[11] B.N. Grosof and T.C. Poon. Sweetdeal: representing agent
contracts with exceptions using XML rules, ontologies, and
process descriptions. In Proceedings of the twelfth
international conference on World Wide Web, pages
340–349, May 2003.
[12] R. Gunthor. Extended transaction processing based on
dependency rules. In Proceedings of the RIDE-IMS
Workshop, 1993.
[13] J. Hintikka. Logic, Language Games, and Information.
Oxford Univ. Press, Clarendon, Oxford, 1973.
[14] M. Kifer, G. Lausen, and J. Wu. Logical foundations of
object-oriented and frame-based languages. Journal of ACM,
pages 741–843, July 1995.
[15] F. Leymann. Web services flow language (wsfl 1.0).
Technical report, IBM, 2001. http://www4.ibm.com/software/solutions/webservices/pdf/WSFL.pdf.
[16] R. McNaughton. Infinite games played on finite graphs.
Annals of Pure and Applied Logic, 65:149–184, 1993.
[17] S. Mukherjee, H. Davulcu, M. Kifer, G. Yang, and P. Senkul.
Survey of logic based approaches to workflow modeling. In
J. Chomicki, R. van der Meyden, and G. Saake Springer,
editors, Logics for Emerging Applications of Databases,
LNCS. Springer-Verlag, October 2003.
[18] M. Osborne and A. Rubinstein. A Course in Game Theory.
The MIT Press, 1998.
[19] Rohit Parikh. Logic of games and its applications. In Annals
of Discrete Mathematics, volume 24, pages 111–140.
Elsevier Science Publishers, March 1985.
[20] D.M. Reeves, M.P. Wellman, and B.N. Grosof. Automated
negotiation from declarative contract descriptions. In J.P.
Müller, E. Andre, S. Sen, and C. Frasson, editors,
Proceedings of the Fifth International Conference on
Autonomous Agents, pages 51–58, Montreal, Canada, 2001.
ACM Press.
[21] C. Schlenoff, M. Gruninger, M. Ciocoiu, and J. Lee. The
essence of the process specification language. Transactions
of the Society for Computer Simulation International,
16(4):204–216, 2000.
[22] M.P. Singh. Semantical considerations on workflows: An
algebra for intertask dependencies. In Proceedings of the
International Workshop on Database Programming
Languages, Gubbio, Umbria, Italy, September 6–8 1995.
[23] M.P. Singh. Synthesizing distributed constrained events from
transactional workflow specifications. In Proceedings of
12-th IEEE Intl. Conference on Data Engineering, pages
616–623, New Orleans, LA, February 1996.

REFERENCES

[1] R. Alur, T.A. Henzinger, and O. Kupferman.
Alternating-time temporal logic. In Intl. Conference on
Foundations of Computer Science, pages 100–109, 1997.
[2] P.C. Attie, M.P. Singh, E.A. Emerson, A.P. Sheth, and
M. Rusinkiewicz. Scheduling workflows by enforcing
intertask dependencies. Distributed Systems Engineering
Journal, 3(4):222–238, December 1996.
[3] F. Bacchus and F. Kabanza. Planning for temporally
extended goals. In Proceedings of the Thirteenth National
Conference on Artificial Intelligence (AAAI-96), pages
1215–1222, Portland, Oregon, USA, 1996. AAAI Press / The
MIT Press.
[4] A.J. Bonner. Workflow, transactions, and datalog. In ACM
Symposium on Principles of Database Systems, Philadelphia,
PA, May/June 1999.
[5] A.J. Bonner and M. Kifer. Concurrency and communication
in transaction logic. In Joint Intl. Conference and Symposium
on Logic Programming, pages 142–156, Bonn, Germany,
September 1996. MIT Press.
[6] A.J. Bonner and M. Kifer. A logic for programming database
transactions. In J. Chomicki and G. Saake, editors, Logics for
Databases and Information Systems, chapter 5, pages
117–166. Kluwer Academic Publishers, March 1998.

153

2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)

Community Detection in Political Twitter Networks
using Nonnegative Matrix Factorization Methods
Mert Ozer

Nyunsu Kim

Hasan Davulcu

School of Computing, Informatics,
Decision Systems Engineering
Arizona State University
Tempe, USA 85287
mozer@asu.edu

School of Computing, Informatics,
Decision Systems Engineering
Arizona State University
Tempe, USA 85287
nkim30@asu.edu

School of Computing, Informatics,
Decision Systems Engineering
Arizona State University
Tempe, USA 85287
hdavulcu@asu.edu

mentions always indicate endorsement in Twitter [7]. However
in the political sub-domain of Twitter, it has been shown that
retweets tend to happen between like-minded users rather than
between members of opposing camps [8].
Using both connectivity and content information for community detection in social networks has been a popular approach among many researchers’ prior works [4], [5], [6], [3].
In [4], Tang et al. propose a general framework for integrating
multiple heterogenous data sources for community detection.
Tang’s work does not pay attention to identifying the endorsement subgraph of the connectivity graph. In [5] Sachan et al.
propose an LDA-like social interaction model by representing
user connectivity as a document alongside message content.
This approach also does not discriminate between positive
or negative user engagement. In [6], Ruan et al. propose to
use a filtered graph to eliminate ambiguous interactions by
checking content similarity in the user’s neighborhood. In
this formulation, only local content patterns are taken into
consideration whereas in our formulations we incorporate the
global content patterns into our optimization framework.
The contributions of this paper can be summarized as
follows:

Abstract—Community detection is a fundamental task in social
network analysis. In this paper, first we develop an endorsement
filtered user connectivity network by utilizing Heider’s structural
balance theory and certain Twitter triad patterns. Next, we
develop three Nonnegative Matrix Factorization frameworks to
investigate the contributions of different types of user connectivity
and content information in community detection. We show that
user content and endorsement filtered connectivity information
are complementary to each other in clustering politically motivated users into pure political communities. Word usage is
the strongest indicator of users’ political orientation among all
content categories. Incorporating user-word matrix and word
similarity regularizer provides the missing link in connectivityonly methods which suffer from detection of artificially large
number of clusters for Twitter networks.

I. I NTRODUCTION
Twitter has become one of the main stages of political
activity both among politicians and partisan crowds. We have
seen huge political mobilizations over Twitter in recent uprisings such as the Arab Spring and the Gezi protests [1].
Since then, politicians have been engaging in using Twitter to
attract supporters and people have been using it to express their
political views and opinions on various leaders and issues.
Community detection is a fundamental task in social network analysis [2]. A community [3] can be defined as a group
of users that (1) interact with each other more frequently than
with those outside the group and (2) are more similar to each
other than to those outside the group. Utilizing community detection algorithms to detect online political camps has attracted
many researchers [4], [5], [6]. In this work, we propose three
nonnegative matrix factorization frameworks to exploit both
user connectivity and content information in Twitter to find
ideologically pure communities in terms of their members’
political orientations.
Twitter presents three types of connectivity information
between users: follow, retweet and user mention. In this paper,
we do not use follow information since follow relationships
correspond to longer-term structural bonds [10] and it remains
challenging to determine if a follow relationship between a
pair of users indicate political support or opposition. Furthermore, it has been observed that neither user retweets nor user

•

•

IEEE/ACM ASONAM 2016, August 18-21, 2016, San Francisco, CA, USA
978-1-5090-2846-7/16/$31.00 ©2016 IEEE

81

We start with retweets without edits as indicators of
positive endorsements between users and utilize Heider’s
P-O-X triad balance theory [11] to incorporate selected
”structurally balanced” edited retweets and user mentions
into a weighted undirected connectivity graph as additional indicators of positive endorsements.
We develop algorithms which incorporate users’ content
information in our community detection frameworks to
overcome the sparse nature of Twitter connectivity networks. We break down Twitter message content into
three categories; words, hashtags and urls, and design
experiments to measure the performance contributions
of each category. Proposed Nonnegative Matrix Factorization (NMF) algorithms use user-word, user-hashtag
and user-domain frequency matrices to be factorized into
lower rank user vector representations while regularizing
over user connectivity and content similarity to map users
into their respective communities.

Pei et al. in [3] also model the problem as nonnegative
matrix tri-factorization problem which factorizes user-word,
tweet-word and user-user matrices into lower rank representations of users and tweets while regularizing it with user interaction and message similarity matrices. They build user-user
connectivity matrix by utilizing the structural follow relationships which do not capture dynamic political context-sensitive
engagement. They treat all user mentions and retweets identically and without any discrimination for endorsement. Their
framework also lacks word similarity regularization.
We develop and experiment with three nonnegative matrix factorization frameworks: MultiNMF, TriNMF, DualNMF,
which incorporate connectivity alongside different types of
content information as regularizers. After experimenting with
different dimensions of user content and different types of
induced connectivity networks we discovered that incorporating more information does not necessarily yield higher
clustering performance. Highest quality clustering is achieved
through endorsement filtered connectivity based on methods
we develop in Section III alongside user-word matrix based
content regularization. Our DualNMF framework gives purity
scores around 88%, adjusted rand index around 75% and NMI
around 67%. It improves all of the other baseline methods
significantly as presented in Section V and it also improves
over the NMTF framework developed recently by Pei et al.
[3] by 8% in purity, 47% in ARI and by up to 60% in
NMI metrics. Proposed endorsement filtered sub-graph of user
mentions and retweets also improves all baseline methods in
almost all of the experimental setups by up to 109% in NMI,
71% in ARI and 17% in purity.
The rest of the paper is organized as follows. Section
II briefly surveys related work. In Section III, we present
Heider’s theory of P-O-X structural balance of triads and
its application to retweet and mention graphs to identify
endorsement filtered user connectivity networks. In Section IV,
we introduce our three nonnegative matrix factorization frameworks for community detection. In Section V, we present our
experimental design, evaluation metrics and results. Section
VI concludes the paper and discusses future work.

problems. Cai et al. [29] introduced GNMF algorithm to
incorporate Laplacian graph regularization to the standard
NMF algorithm which assumes data points are sampled from
a Euclidian space which is not the case usually for real-world
applications. Gu et al. [31] further incorporate local learning regularization to NMF which assumes that geometrically
neighboring data points are similar to each other, and should be
in the same cluster. For co-clustering purposes Ding et al. [28]
propose nonnegative matrix tri-factorization with orthogonality
constraints. Shang et al. introduce graph dual regularized NMF
algorithm in [27] by claiming that not only observed data but
also features lie on a manifold. Yao et al. [24] apply the same
logic for collaborative filtering domain and propose a dual
regularized one-class collaborative filtering method.
III. T HE S TRUCTURAL BALANCE OF R ETWEET AND
M ENTION G RAPH
Since P-O-X triad balance theory proposed by Heider in
[11], structural balance of signed networks has been studied
extensively. Heider proposed that in a signed triad, only two
combinations of eight possible sign configurations are possible
for a triad to be structurally balanced. Those are the following
cases;
1) three positive edges,
2) one positive and a pair of negative edges.
In other words, there cannot be any structurally balanced triad
having only one negative edge. We adopt this social theory
for the Twitter user connectivity networks, by assuming that
”retweets without edits” imply political endorsement or an
unambiguous positive edge [12]. However, when a retweet is
edited, it has already been shown that [13], it does not necessarily mean endorsement anymore. Moreover, user mentions
do not imply endorsements either. For these reasons, we only
consider retweets without edits as positive edges. For the rest
of the user actions, corresponding to retweets with edits and
users mentions, it is hard to detect positivity or negativity of
the edges.
In certain triad configurations, retweets with edits and user
mentions can be identified as positive edges with the help
of Heider’s triad structural balance (TSB) rules. Since we
do not have unambiguous negative edges, the second case
is not applicable. However, since we have some positive
edges to begin with, we can employ Heider’s first case (i.e.
three positive edges), to infer that in the presence of a triad
with a pair of positive edges, the third edge can also be
labeled as positive. An example configuration with a pair of
positive edges is shown in Figure 1. In this case, TSB rule is
applicable and would allow us to infer that any user mention
or retweet with an edit edge connecting the lower pair of
users in the triad is indeed a positive edge. By employing
this inference mechanism we identify the endorsement filtered
user connectivity network.

II. R ELATED W ORK
A. Community Detection
Since the introduction of the modularity metric by Newman
in [16], plenty of modularity based community detection
methods have been proposed in the literature [17], [18], [19],
[20]. We employ Blondel et al. [18] and Clauset et al. [19]
works as baseline algorithms to compare with ours due to their
wide popularity among practitioners. A general drawback of
these algorithms, when they are applied to Twitter networks,
is that due to the sparse nature of the connectivity they end
up with an artificially large number of communities.
B. Nonnegative Matrix Factorization
Nonnegative Matrix Factorization(NMF) algorithms by Lee
et al. [22] and Lin et al. [23] have been extensively used
and extended for different variations of community detection

82

TABLE I
N OTATION
Xuw
Xuh

user x word
user x hashtag

Xud

user x domain

R

user x user

M

user x user

∆M

user x user

∆Mw

user x user

C

user x user

Hsim
Dsim
Wsim
α
γ
θ
β
U
H
D
W

hashtag x hashtag
domain x domain
word x word
number
number
number
number
user x cluster
hashtag x cluster
domain x cluster
word x cluster

Fig. 1. An example application of TSB Rule

IV. P ROPOSED M ETHODS
We propose three methods for clustering politically motivated users in Twitter namely; MultiNMF, TriNMF and
DualNMF. For MultiNMF method we use document term
representation of user-word, user-hashtag and user-domain matrices to be factorized and regularize the factorization problem
with the user connectivity graph, cosine similarity matrices of
words, domains and hashtag co-occurence matrix. For TriNMF
method we use only user-word and one of user-hashtag or
user-domain matrices and regularize over user connectivity
and cosine domain similarity or hashtag co-occurence matrix.
For DualNMF method we factorize user-word matrix into two
nonnegative lower rank matrices while regularizing it with user
connectivity and cosine word similarity. Before going into the
details of the three algorithms we present notation in Table I. In
this work, instead of using only full user retweet and mention
network we offer three types of user connectivity regularizers
as follows;
• R + M: It is the adjacency matrix of the full retweet and
mention graph. If there exists both retweet and mention
edges between two users, weights are summed up.
• R + ∆M: It is the adjacency matrix of the union of
retweet and mention graphs in which mention edges and
retweet with edits either complete a missing link in a
triad of retweet without edit or already correspond to a
retweet without edit edge. ∆M can be formally defined
as;
PN
∆M = {(i, j, Mij ) | Rij > 0 ∨ k=1 Rik Rkj > 0}
• R + ∆Mw : It is the adjacency matrix of the union of
retweet and mention graphs in which mention edges and
retweet with edits either complete a missing link in a triad
of retweet without edit or already correspond to a retweet
without edit edge. The ones that complete a missing link
in a triad of retweet without edit are weighted by the
multiplication of the weights of two retweet without edit
edges in the triad. ∆Mw can be defined formally as;
∆Mw = {(i, j, Mij (Rij +

PN

k=1

cos(θ) =

frequencies of words used by users
frequencies of hashtags used by users
frequencies of distinct domains used
by users
adjacency matrix of
retweet without edit graph
adjacency matrix of
mention and retweet with edit graph
adjacency matrix of mentions
and retweet with edits completing
retweet without edit triads
adjacency matrix of mentions
and retweet with edits completing
retweet without edit triads
weighted by retweet without edit edges
any combination of user connectivity
graphs
hashtag co-occurence matrix
domain similarity matrix
word similarity matrix
user connectivity regularizer parameter
hashtag similarity regularizer parameter
domain similarity regularizer parameter
word similarity regularizer parameter
cluster assignment matrix of users
cluster assignment matrix of hashtags
cluster assignment matrix of domains
cluster assignment matrix of words

vi · vj
k vi k ∗ k vj k

where vi is the user usage frequency vector of ith word or
domain. For hashtag similarity we build similarity matrix by
making use of co-occurences of hashtags in tweets. If two
hashtags occur in the same tweet, we assume that those two
hashtags are similar.
A. MultiNMF with multi regularizers
To incorporate usage of both hashtags and domains of
shared url links by users, we propose an NMF framework
which has the following objective function;
JU,H,D,W =k Xuw − UWT k2F + k Xuh − UHT k2F
+ k Xud − UDT k2F +αT r(UT LC U)
+ γT r(HT LHsim H) + θT r(DT LDsim D)

(1)

T

+ βT r(W LWsim W)
s.t. U ≥ 0, H ≥ 0, D ≥ 0, W ≥ 0
where LC is the Laplacian matrix of adjacency matrix of user
connectivity graph defined as DC − C and DC is the matrix
which contains the degree of each user node in its diagonals.
LHsim , LDsim and LWsim follow the same definition for
hashtags and words. Due to the very fuzzy multi-class nature
of words, hashtags and domain names, we do not include
orthogonality constraints for matrices U, H, D, W, which
usually result in more precise clusters for co-clustering tasks.
It is easy to see that the proposed objective function is not

Rik Rkj ))}

For word similarity and domain similarity regularizers
we make use of cosine similarity. It can be formally defined as;

83

v
u T
u Xuw U + βL−
Wsim W
W ←Wt
T
WU U + βL+
Wsim W

convex for U, H, D and W, hence we develop an iterative
algorithm which tries to find a local minima by updating each
matrix iteratively as follows;
s
Xuw W + Xuh H + Xud D + αL−
CU
(2)
U←U
UWT W + UHT H + UDT D + αL+
CU
v
u T
u Xuh H + γL−
Hsim H
H←Ht
(3)
T
HU U + γL+
Hsim H
v
u T
u Xud D + θL−
Dsim D
D←Dt
T
DU U + θL+
Dsim D
v
u T
u Xuw U + βL−
Wsim W
W ←Wt
T
WU U + βL+
Wsim W

Note that this update rules can be obtained by setting D, Dsim
and θ equal to 0 in Equations 2, 3, 5. Complexity of the method
can be calculated by omitting the costs of operations done over
matrices Xud , D and LDsim . The complexity of the method
is O(i(uwk + uhk + u2 k + h2 k + w2 k)).
C. DualNMF
To use only user word matrix as user content and regularize
factorization with user connectivity and keyword similarity,
inspired by [24], we present DualNMF objective function as;

(4)

JU,W =k Xuw − UWT k2F +αT r(UT LC U)
+ βT r(WT LWsim W)
s.t. U ≥ 0, W ≥ 0

(5)

Complexity of the method can be inferred as O(i(uwk+u2 k+
w2 k)) after omitting the extra operations done over matrices
Xuh , H and DHsim in the previous method. The general
Algorithm 1 NMF Algorithms
Input:{Xuw , Xuh , Xud , C, Hsim , Dsim , Wsim , α, β, θ, γ}
Output: U
1: Initialize U, H, D, W > 0
2: while ∆residual > threshold do
3:
Update U by using one of Equations 2, 7, 11
4:
Update H by using one of Equations 3, 8
5:
Update D by using Equation 4
6:
Update W by using one of Equations 5, 9, 12
7: end while
8: Assign user i to community j where j = argmaxj Uij .

B. TriNMF with three regularizers
To incorporate usage of hashtags or domains of shared url
links solely, we propose a new NMF framework which has
the following objective function.
JU,H,W =k Xuw − UWT k2F + k Xuh − UHT k2F
+ βT r(WT LWsim W)
s.t. U ≥ 0, H ≥ 0, W ≥ 0

(10)

After following the same procedure introduced in Section
IV-A, we can get the update rules for U and W as;
s
Xuw W + αL−
CU
(11)
U←U
T
UW W + αL+
CU
v
u T
u Xuw U + βL−
Wsim W
W ←Wt
(12)
T
WU U + βL+
Wsim W

−
where L+
ij = (|Lij | + Lij )/2 and Lij = (|Lij | − Lij )/2.
[·]
 represents element-wise multiplication and
represents
[·]
element-wise division. Derivation of update rules can be seen
in Appendix A. Complexity of the method can be inferred
as O(i(uwk + uhk + udk + u2 k + h2 k + d2 k + w2 k))
when complexity of multiplying any X matrix with any of
U, H, D, W is considered to be O(uwk), O(uhk), O(udk)
and multiplying any of Laplacian matrices L with any of
U, H, D, W is taken as O(u2 k), O(h2 k), O(d2 k) or O(w2 k)
where i is the number of iterations, u is number of users, h is
the number of hashtags, d is the number of domains, w is the
number of words and k is the number of clusters. The general
algorithmic framework is given at the end of methodology in
Algorithm 1.

+ αT r(UT LC U) + γT r(HT LHsim H)

(9)

(6)

algorithm can be summarized as the application of the related
update rules to the matrices U, H, D, W. For MultiNMF with
multi regularizers method, equations 2, 3, 4, 5 are applied.
For TriNMF with three regularizers method, equations 7, 8,
9 are applied and D matrix is not included in calculations.
For DualNMF method, equations 11 and 12 are applied and
H and D matrices are not included in calculations.

where LC is the Laplacian matrix of user connectivity defined
as DC − C and DC is a diagonal matrix which contains the
degree of each user in its diagonals. LHsim and LWsim follows
the same definition for hashtags and words. After applying the
same procedure followed in Section IV-A, we get updating
rules as follows.
s
Xuw W + Xuh H + αL−
CU
U←U
(7)
T
T
UW W + UH H + αL+
CU
v
u T
u Xuh U + γL−
Hsim H
H←Ht
(8)
T
HU U + γL+
Hsim H

V. E XPERIMENTS AND R ESULTS
A. Data Description
We make use of a pair of publicly available1 political Twitter
datasets to evaluate our methods. These datasets are user lists
1 Users’
Twitter
id
lists
http://mlg.ucd.ie/aggregation/index.html

84

can

be

obtained

from

of 419 British political figures from four major political parties
in the UK, namely; Conservative and Unionist Party, Labour
Party, Scottish National Party, Liberal Democrats and others,
and 349 major Irish political figures from seven political
parties; Fianna Fil, Fine Gael, Green Party, Sinn Fin, United
Left Alliance, Independents. Several statistics for the datasets
are shown in Table II.

pairs which belong to both different ground-truth classes and
identified communities. It evaluates the similarity of groundtruth class labels and clustering result.
Normalized Mutual Information can be formally defined
as;
 P (j, i) 
P|l| P|C|
j=1
i=1 P (j, i)log
P (i)P (j)
p
NMI =
H(l)H(C)

TABLE II
DATA ATTRIBUTES

#
#
#
#
#
#
#
#

of
of
of
of
of
of
of
of

Tweets
Retweets
Mentions
Words
Hashtags
URL Domains
Users
Baseline Communities

UK

Ireland

19, 947
1, 566
4, 956
10, 766
945
946
233
5

14, 656
7, 088
22, 072
7, 973
986
634
258
7

where, H(l) and H(C) are the entropy of class and community
assignments of l and C. P (j, i) is the probability that randomly
picked user has class label j and belongs to the community i
while P (j) gives the probability of randomly picked user to
be in class j and similarly P (i) to be in community i.
C. Baseline Algorithms
As a baseline to evaluate the performance of using both
connectivity and content information, we design experiments
with connectivity-only and content-only clustering methods.
For connectivity-only method, we use Louvain [18] and
CNM [19] algorithms utilizing modularity optimization over
user adjacency matrix. Modularity is defined as:
ki kj
1 X
(Aij −
)δ(ci , cj )
(13)
Q=
2m ij
2m

For the UK and Ireland data, we crawl all of the tweets sent
from the accounts of given user id lists. In order not to be
heavily influenced by the extremely polarized election season,
we only used tweets dated after May, 7 2015, which was the
election day in the UK. To balance the share of number of
tweets from each user we limit the number of tweets to 200
per user.
For each dataset, same preprocessing method is followed.
First, words occurring less than 20 times and stop words
are eliminated. After eliminating word features, users and
tweets that lack content are also eliminated. Hashtags and
domains that appear only once are not taken into consideration
either. Statistics shown in Table II show the numbers after
preprocessing.

where δ(ci , cj ) is the Kronecker delta symbol, ci is the label
of the community to which node i is assigned, and ki is the
degree of node i.
For content-only approach, we experiment with kmeans[21] and conventional non-negative matrix factorization
algorithm [22].
For approaches employing both connectivity and content
information of users, we test GNMF [29] and NMTF [3]
algorithms besides proposed methods. GNMF algorithm is
introduced by Cai et al. to incorporate intrinsic geometric
similarity of users. We feed previously defined three types
of user connectivity graphs’ adjacency matrices as graph
regularization terms to the GNMF algorithm.
Pei et al. work in [3] applies nonnegative matrix trifactorization with regularization to Twitter data. It makes use
of user similarity, [tweet x word] and [user x word] matrices
and regularize the objective function with tweet similarity and
user connectivity matrices. Complexity of the algorithm is
O(rk(mn + mw + nw + m3 + n2 )) where r is the iteration
times. m, n, k, and w denote the number of users, messages,
features and communities.

B. Evaluation Metrics
To evaluate the methods, we make use of three well known
clustering quality metrics, namely; purity, adjusted rand index
and normalized mutual information.
Purity can be formally defined as;
k

P urity =

1X
maxj |Ci ∩ lj |
n i=1

where k is the number of communities found, n is the number
of instances, lj is the set of instances which belong to the class
j, and Ci is the set of instances that are members of community
i.
Adjusted Rand Index [14] can be formally defined as;
ARI =

D. Experimental Design
First set of experiments test the performance of using
connectivity-only information for community detection, labeled as the Experiment Set 1. We test Louvain and CNM
algorithms on three different types of connectivity graphs.
Second set of experiments test the performance of contentonly methods, labeled as Experiment Set 2. We test k-means
and NMF methods. Third set of experiments test the performance of methods utilizing both connectivity and content

RI − E[RI]
max(RI) − E[RI]

where
RI =

s + s0

n
2

s is the number of pairs which belong to both same groundtruth class and identified community. s0 is the number of

85

information, labeled as Experiment Set 3. We test GNMF and
NMTF frameworks proposed by [3] as baseline algorithms,
alongside our proposed MultiNMF, TriNMF and DualNMF
methods. In user content dimension, we use DualNMF method
to test the experiment design that only uses user-word content.
We use TriNMF method to test the experiment design that
uses user-hashtag or user-domain information in combination
with the user-word information. We use MultiNMF method
to test the experiment design that uses all of user-word, userhashtag and user-domain contents. We label these experiments
as Experiment Set 3.1, 3.2 and 3.3 respectively.

TABLE IV
UK E XPERIMENT S ET 1 R ESULTS
Algorithm

User Graph

k

Purity

ARI

NMI

Louvain

R+M
R + ∆M
R + ∆Mw

20
42
42

.9313
.9613
.9484

.4661
.3691
.4291

.5854
.5916
.5916

CNM

R+M
R + ∆M
R + ∆Mw

17
41
41

.8498
.9700
.9700

.5656
.6150
.6150

.5257
.6496
.6496

TABLE V
I RELAND E XPERIMENT S ET 1 R ESULTS

E. Experimental Results
First, we present statistics of retweets without edits and user
mentions on the full and endorsement filtered user connectivity
graphs. Table III shows that retweeting without edits indeed
occurs mostly inside like-minded political camps, rather than
cross-camps. Roughly 97% of retweets in the UK data, and
88% of retweets in the Ireland data occur inside like-minded
groups, while these percentages are much lower for users mentions. Our endorsement filtered connectivity network boosts
the percentage of inner group user mentions from 83% to
97% in the UK data and from 59% to 87% in the Ireland
data evidencing that TSB rule in fact identifies positive user
mentions and retweets with edits with high accuracy.

Ireland

962
28

1, 652
216

Inner Group Retweet + Mention Links
Inter Group Retweet + Mention Links

1, 986
398

3, 056
2, 092

Inner Group Retweet + ∆Mention Links
Inter Group Retweet + ∆Mention Links

1, 456
40

2, 820
432

Inner Group Retweet Links
Inter Group Retweet Links

•

k

Purity

ARI

NMI

Louvain

R+M
R + ∆M
R + ∆Mw

13
31
31

.8720
.9186
.9224

.7277
.7453
.7536

.6849
.7393
.7518

CNM

R+M
R + ∆M
R + ∆Mw

10
29
29

.7016
.8333
.8333

.4509
.6426
.6426

.4720
.6381
.6381

TABLE VI
UK E XPERIMENT S ET 2 R ESULTS
Algorithm

User Content

Purity

ARI

NMI

k-Means
NMF

user x word
user x word

.6738
.6395

.2378
.1541

.2018
.1709

TABLE VII
I RELAND E XPERIMENT S ET 2 R ESULTS

We run each experiment 20 times for every method and pick
the maximum score achieved for reporting. Each regularizer
parameter (α, γ, θ, β) are experimented with values 1, 10,
100 and 1000. Best accuracies are usually reached with
experiments in which α and β equal to 10 or 100 while
γ and θ equal to 1. This shows the contribution of user
connectivity and word similarity regularizers, and considerably
lower contributions of hashtag and domain name regularizers
towards overall performance of the algorithms.
Major findings for Experiment Set 1 can be summarized as
follows:
•

User Graph

using full user connectivity graph. There is a pattern of
weighted graph approach outperforming the others.

TABLE III
DATA ATTRIBUTES
UK

Algorithm

Algorithm

User Content

Purity

ARI

NMI

k-Means
NMF

user x word
user x word

.4651
.4186

.0488
.0434

.1672
.1139

Experiment Set 2 indicates that word usage-only based
clustering yields considerably lower accuracies compared to
user connectivity-only based clustering.
Major findings from Experiment Set 3 can be summarized
as follows;
• Regardless of the experiment set and algorithms used,
endorsement filtered user connectivity graph yields higher
accuracy clustering performance compared to using the
full connectivity graph. Usually weighted graph approach
outperforms the others.
• DualNMF method which factorizes user-word matrix
alongside user connectivity and word similarity regularizers yields the highest accuracy clustering performance.
• We get much higher scores of clustering accuracy in
Experiment Set 3 compared to Experiment Set 2. Regularizing content-only methods with user connectivity

Relatively larger clustering scores occur due to artificially
large number of clusters that are found. Considering the
number of users in both datasets, the number of clusters
identified in Experiment Set 1 are not practical for use
(e.g. 29 clusters in Ireland data for 7 political parties).
Using endorsement filtered user connectivity graph usually gives better clustering performance compared to

86

TABLE VIII
UK E XPERIMENT S ET 3 R ESULTS

TABLE X
C OMPARISON OF M ETHODS FOR E XPERIMENT S ET 3

Algorithm

User Graph

User Content

Purity

ARI

NMI

user x word

GNMF[29]

R+M
R + ∆M
R + ∆Mw

.7854
.8069
.8326

.4955
.6099
.6469

.4120
.4922
.5461

NMTF[3]

R+M
R + ∆M
R + ∆Mw

user x word,
tweet x word

.8197
.8112
.8412

.6448
.5657
.5331

.2593
.2471
.3751

TriNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x domain

.7597
.7940
.8283

.3707
.5566
.6375

.3158
.4595
.5006

TriNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x hashtag

.7897
.8112
.7768

.5232
.4640
.5001

.4320
.3780
.3837

MultiNMF

R+M
R + ∆M
R + ∆Mw

user x word
user x domain,
user x hashtag

.7554
.8112
.8112

.4025
.5726
.6108

.3343
.4404
.4978

R+M
R + ∆M
R + ∆Mw

user x word

DualNMF

.8326
.8927
.8970

.5674
.7291
.7616

.5146
.6086
.6380

Content

User Graph

Content

Purity

ARI

NMI

R+M
R + ∆M
R + ∆Mw

user x word

GNMF[29]

.5543
.6279
.8178

.2447
.4557
.6978

.2881
.4652
.6399

NMTF[3]

R+M
R + ∆M
R + ∆Mw

user x word,
tweet x word

.5969
.6860
.7597

.3119
.3986
.5198

.2144
.2384
.4469

TriNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x domain

.7209
.7946
.8101

.5051
.6045
.6807

.5237
.5313
.6372

TriNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x hashtag

.6938
.7016
.8062

.4202
.5300
.6784

.4431
.4224
.6885

MultiNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x domain,
user x hashtag

.7481
.6744
.8178

.4777
.4597
.6953

.4938
.4219
.6411

R+M
R + ∆M
R + ∆Mw

user x word

DualNMF

.7364
.7597
.8721

.5561
.6292
.7536

.5397
.6029
.7096

•

•

TriNMF

3TriNMF

MultiNMF

3MultiNMF

domain usage information (i.e. TriNMF and MultiNMF)
do not contribute to the overall clustering quality.
VI. C ONCLUSION
In Twitter, content and endorsement filtered connectivity
are complementary to each other in clustering politically
motivated users into pure political communities. Word usage is
the strongest indicator of user’s political orientation among all
content categories. Incorporating user-word matrix and word
similarity regularizer provides the missing link in connectivityonly methods which suffers from detection of artificially large
number of clusters in sparse Twitter networks. Our future work
includes parallel distributed evolutionary community detection
and identification of emerging coalitions and conflicts among
communities.

TABLE IX
I RELAND E XPERIMENT S ET 3 R ESULTS
Algorithm

3word
word,
hashtag or domain
word,
hashtag and domain

Connectivity
R+M
3R + ∆Mw
DualNMF
33DualNMF

ACKNOWLEDGMENT
This research was supported by ONR Grants N00014-16-12386 and N00014-15-1-2722.
R EFERENCES
[1] Howard, Philip N., and Aiden Duffy, Deen Freelon, Muzammil Hussain,
Will Mari, and Marwa Mazaid. Opening Closed Regimes: What Was the
Role of Social Media During the Arab Spring? Project on Information
Technology and Political Islam Data Memo 2011.1. Seattle: University
of Washington, 2011.
[2] Girvan,M., Newman M.E.J. (2002). Community structure in social and
biological networks, Proceedings of the National Academy of Sciences,
99(12), pp.7821-7826.
[3] Yulong Pei, Nilanjan Chakraborty, and Katia Sycara. 2015. Nonnegative
matrix tri-factorization with graph regularization for community detection
in social networks. In Proceedings of the 24th International Conference on
Artificial Intelligence (IJCAI’15), Qiang Yang and Michael Wooldridge
(Eds.). AAAI Press 2083-2089.
[4] J. Tang, X. Wang, and H. Liu. Integrating Social Media Data for
Community Detection. In Modeling and Mining Ubiquitous Social Media,
2012.
[5] Mrinmaya Sachan, Danish Contractor, Tanveer A. Faruquie, and L.
Venkata Subramaniam. 2012. Using content and interactions for discovering communities in social networks. In Proceedings of the 21st
international conference on World Wide Web (WWW ’12). ACM, New
York, NY, USA, 331-340.
[6] Y. Ruan, D. Fuhry, and S. Parthasarathy. Efficient community detection
in large networks using content and links. In WWW 13, 2013.
[7] Tufekci, Z. (2014). Big Questions for Social Media Big Data: Representativeness, Validity and Other Methodological Pitfalls. In: International
AAAI Conference on Weblogs and Social Media.
[8] M. D. Conover, B. Goncalves, J. Ratkiewicz, M. Francisco, A. Flammini,
and F. Menczer, Political polarization on Twitter, in Proceedings of the
5th InternationalConference on Weblogs and Social Media, 2011
[9] Sandra Gonzlez-Bailn, Ning Wang, Alejandro Rivero, Javier BorgeHolthoefer, Yamir Moreno, Assessing the bias in samples of large online
networks, Social Networks, Volume 38, July 2014, Pages 16-27, ISSN
0378-8733, http://dx.doi.org/10.1016/j.socnet.2014.01.004.

graphs(GNMF [29]), dramatically increases the quality
of the clustering. DualNMF which incorporates keyword
similarity regularization to GNMF further boosts the
quality of clustering.
Compared to DualNMF method, including tweet messages for NMTF method proposed in [3] does not help to
further improve the clustering quality, while it increases
complexity dramatically. DualNMF provides 9% additional purity, 46% additional ARI score while doubling
the NMI score compared to the baseline NMTF method
of Pei et al. in [3].
Compared to DualNMF method, utilizing hashtag and/or

87

[10] S. A. Myers, A. Sharma, P. Gupta, and J. Lin. Information network
or social network? The structure of the Twitter follow graph. WWW
Companion, 2014
[11] Heider F. The psychology of interpersonal relations. New York: Wiley,
1958. 322 p.
[12] Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, and Mung Chiang.
2013. Quantifying political leaning from tweets and retweets. In Proceedings of the International AAAI Conference on Weblogs and Social Media
(ICWSM).
[13] D. Boyd, S. Golder and G. Lotan, ”Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter,” System Sciences (HICSS),
2010 43rd Hawaii International Conference on, Honolulu, HI, 2010, pp.
1-10. doi: 10.1109/HICSS.2010.412
[14] Hubert, L., Arabie, P. (1985). Comparing partitions. Journal of Classification, 2, 193218.
[15] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, Cambridge, 2004.
[16] M. Newman. 2006. Modularity and community structure in networks.
Proceedings of the National Academy of Sciences, vol. 103, no. 23, pp.
85778582.
[17] S. Fortunato. 2010. Community detection in graphs. Physics Reports,
vol. 486, pp. 75174
[18] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne
Lefebvre. Fast unfolding of communities in large networks. Journal of
Statistical Mechanics: Theory and Experiment 2008 (10), P10008 (12pp)
doi: 10.1088/1742-5468/2008/10/P10008.
[19] A. Clauset, M. Newman, and C. Moore, Finding community structure in very large networks, Physical Review E, vol. 70, p. 066111, 2004. [Online]. Available:
http://www.citebase.org/cgibin/citations?id=oai:arXiv.org:condmat/0408187
[20] Waltman, L., Van Eck, N. J.. 2013. A smart local moving algorithm
for large-scale modularity-based community detection. European Physical
Journal B, 86, 471.
[21] Lloyd., S. P. (1982). ”Least squares quantization in PCM”
(PDF). IEEE Transactions on Information Theory 28 (2): 129137.
doi:10.1109/TIT.1982.1056489
[22] Daniel D. Lee, H. Sebastian Seung. 2000. Algorithms for Non-negative
Matrix Factorization. In Neural Information Processing Systems (NIPS),
Vol. 13 , pp. 556-562.
[23] C. J. Lin. On the Convergence of Multiplicative Update Algorithms for Nonnegative Matrix Factorization. In IEEE Transactions on
Neural Networks, vol. 18, no. 6, pp. 1589-1596, Nov. 2007. doi:
10.1109/TNN.2007.895831
[24] Yuan Yao, Hanghang Tong, Guo Yan, Feng Xu, Xiang Zhang,
Boleslaw K. Szymanski, and Jian Lu. 2014. Dual-Regularized OneClass Collaborative Filtering. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge
Management (CIKM ’14). ACM, New York, NY, USA, 759-768.
DOI=http://dx.doi.org/10.1145/2661829.2662042
[25] Linhong Zhu, Aram Galstyan, James Cheng, and Kristina Lerman.
2014. Tripartite graph clustering for dynamic sentiment analysis on
social media. In Proceedings of the 2014 ACM SIGMOD International
Conference on Management of Data (SIGMOD ’14). ACM, New York,
NY, USA, 1531-1542. DOI=http://dx.doi.org/10.1145/2588555.2593682
[26] Quanquan Gu and Jie Zhou. 2009. Co-clustering on manifolds. In
Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining (KDD ’09). ACM, New York,
NY, USA, 359-368. DOI=http://dx.doi.org/10.1145/1557019.1557063
[27] Fanhua Shang, L. C. Jiao, and Fei Wang. 2012. Graph
dual regularization non-negative matrix factorization for coclustering. Pattern Recogn. 45, 6 (June 2012), 2237-2250.
DOI=http://dx.doi.org/10.1016/j.patcog.2011.12.015
[28] Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006. Orthogonal
nonnegative matrix t-factorizations for clustering. In Proceedings of the
12th ACM SIGKDD international conference on Knowledge discovery
and data mining (KDD ’06). ACM, New York, NY, USA, 126-135.
DOI=http://dx.doi.org/10.1145/1150402.1150420
[29] D. Cai, X. He, J. Han and T. S. Huang, Graph Regularized Nonnegative
Matrix Factorization for Data Representation, in IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 33, no. 8, pp. 1548-1560,
Aug. 2011. doi: 10.1109/TPAMI.2010.231
[30] Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based
on non-negative matrix factorization. In Proceedings of the 26th annual

international ACM SIGIR conference on Research and development in
informaion retrieval (SIGIR ’03). ACM, New York, NY, USA, 267-273.
DOI=http://dx.doi.org/10.1145/860435.860485
[31] Zhou, Quanquan Gu Jie. Local learning regularized nonnegative matrix
factorization. IJCAI 2009, Proceedings of the 21st International Joint
Conference on Artificial Intelligence, Pasadena, California, USA, July
11-17, 2009
[32] Linhong Zhu, Aram Galstyan, James Cheng, and Kristina Lerman. Tripartite graph clustering for dynamic sentiment analysis on social media.
In Proceedings of the 2014 ACM SIGMOD International Conference on
Management of Data, pages 1531-1542. ACM, 2014.

A PPENDIX
D ERIVATION OF E QUATIONS 2, 3, 4, 5
To follow the conventional theory of constrained optimization we rewrite objective function 1 as;
JU,H,D,W = T r((Xuw − UWT )(Xuw − UWT )T )
+ T r((Xuh − UHT )(Xuh − UHT )T )
+ T r((Xud − UDT )(Xud − UDT )T )
+ αT r(UT LC U) + γT r(HT LHsim H)
+ θT r(DT LDsim D) + βT r(WT LWsim W)
JU,H,D,W = T r(Xuw XTuw ) − 2T r(Xuw WUT )
+ T r(UWT WUT ) + T r(Xuh XTuh )
− 2T r(Xuh HUT ) + T r(UHT HUT )
+ T r(Xud XTud ) − 2T r(Xud DUT ) + T r(UDT DUT )
+ αT r(UT LC U) + γT r(HT LHsim H)
+ θT r(DT LDsim D) + βT r(WT LWsim W)
Let Φ, η, Ω and Ψ be the Lagrangian multipliers for constraints
U, H, D, W > 0 respectively. So the Lagrangian function L
becomes;
L = T r(Xuw XTuw ) − 2T r(Xuw WUT ) + T r(UWT WUT )
+ T r(Xuh XTuh ) − 2T r(Xuh HUT ) + T r(UHT HUT )
+ T r(Xud XTud ) − 2T r(Xud DUT ) + T r(UDT DUT )
+ αT r(UT LC U) + γT r(HT LHsim H) + θT r(DT LDsim D)
+ βT r(WT LWsim W) + T r(ΦUT ) + T r(ηHT )
+ T r(ΩDT ) + T r(ΨWT )
The partial derivatives of Lagrangian function L with respect
to U, H, D, W are as follows;
∂L
= −2Xuw W + 2UWT W − 2Xuh H + 2UHT H−
∂U
2Xud D + 2UDT D + 2αLC U + Φ
∂L
= −2XTuh H + 2UHT H + 2γLHsim H + η
∂H
∂L
= −2XTud H + 2UDT D + 2θLDsim D + Ω
∂D
∂L
= −2XTuw U + 2WUT U + 2βLWsim W + Ψ
∂W
Setting derivatives equal to zero and using KKT complementarity conditions [15] of nonnegativity of matrices
U, H, D, W, ΦU = 0, ηH = 0, ΩD = 0 and ΨW = 0,
we get the update rules given in Equations 2, 3, 4, 5.

88

Automated Metadata and Instance Extraction from News Web Sites
Srinivas Vadrevu, Saravanakumar Nagarajan, Fatih Gelgi, Hasan Davulcu
Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ, 85287, USA
{svadrevu, nrsaravana, fagelgi, hdavulcu}@asu.edu
Abstract
Over the past few years World Wide Web has established
as a vital resource for news. With the continuous growth
in the number of available news Web sites and the diversity in their presentation of content, there is an increasing
need to organize the news related information on the Web
and keep track of it. In this paper, we present automated
techniques for extracting metadata instance information by
organizing and mining a set of news Web sites. We develop algorithms that detect and utilize HTML regularities
in the Web documents to turn them into hierarchical semantic structures encoded as XML. The tree-mining algorithms
that we present identify key domain concepts and their taxonomical relationships. We also extract semi-structured concept instances annotated with their labels whenever they are
available. We report experimental evaluation for the news
domain to demonstrate the efﬁcacy of our algorithms.

1

Introduction

The problem of extracting, managing and organizing the
data from unstructured and semi-structured Web pages is an
important problem, investigated by several researchers [1,
2, 3]. Critical information such as metadata and attribute
labels is usually unlabeled and difﬁcult to locate. It is also
presented in various incompatible formats in different Web
sources. This data must be digested into an organized into a
uniform manner such that it can be used for scalable ad-hoc
querying, automatic summarization, integration and mediation over the Web.
There are a plethora of techniques that explore information extraction from semi-structured and unstructured Web
sources. For example, schema learning [7] and automatic
data extraction [2, 1, 5] methods work on structured Web
sites to extract the schema and reconstruct the template of
the Web pages. These approaches have rigid requirements
on the input Web pages that they need to be template-driven
and regularly structure their content in an uniform manner.

In order to develop efﬁcient techniques to extract the
metadata and instance information from Web pages in an
automated manner, it is usually helpful to exploit speciﬁc
characteristics of the domain of interest. One such domain
of interest is that of on-line newspapers and news portals
on the Web, which have become one of the most important
sources of up-to-date information. There are indeed thousands of sites that provide daily news in a very distinct formats and there is a growing need for tools that will allow
individuals to access and keep track of this information in
an automatic manner.
In this paper, we present techniques for automatically
extracting the metadata and instance information by organizing and mining a set of news Web sites. We extract a
common news taxonomy that organizes the important concepts and individual news articles for these concepts with
their attribute information.
OntoMiner differs from the earlier information extraction methods in a way that it works in a completely automated manner without any human intervention, it does not
require any labeled training examples, and it does not assume anything about the presentation template of the input
Web pages. The main contributions of OntoMiner system
are threefold, described as following:
• A semantic partitioning algorithm that logically segments the page and groups and organizes the content
in an HTML Web page.
• A taxonomy mining algorithm that organizes important concepts in a set of overlapping Web sites.
• An instance mining algorithm that extracts individual
instances with their attribute labels from Web pages
that belong to the same category.
Section 2 presents the semantic partitioning algorithm to
segment a Web page and organize its content into groups
and instances. Sections 3 and 4 describe the taxonomy mining and the news instance extraction algorithms. Section 5
presents the experimental results and section 6 concludes
the paper and describes the future directions of this work.

Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI’05)
0-7695-2415-X/05 $20.00 © 2005 IEEE

Figure 1. Snapshot of New York Times Home Page

2

Semantic Partitioning

OntoMiner employs a two-phase semantic partitioning
algorithm made up of ﬂat partitioning, and hierarchical partitioning.

2.1

Flat partitioning

Our Flat Partitioner (FP) algorithm detects various logical segments within a Web page. For example, for the
homepage of www.nytimes.com, we marked the logical segments in boxes B1 through B5 in Figure 1a. The
boundaries of segments B2 and B3 correspond to the dotted
lines shown in the DOM tree of the Web page in Figure 1b.
FP groups similar contiguous substructures in the Web
pages into logical segments by detecting a high concentration of neighboring nodes with similar root-to-leaf tag
paths. First, FP initializes the segment boundary to be the
ﬁrst leaf node of the DOM tree. Next, it connects a pair
of leaf nodes with a “similarity-link” if they share the same
root-to-leaf path and if all other leaf nodes in between have
different paths. Then, it calculates the ratio of cardinality
of similarity-links crossing the current candidate boundary
to that of those within the current segment. If this ratio is
less than an experimentally determined threshold δ, set to
0.34, then FP marks the current node as a segment boundary. Otherwise, it adds the current node to the current segment and considers the next node as a segment boundary.
The process terminates when the algorithm reaches the last
leaf node. The tree view in Figure 1b illustrates the FP algorithm.

2.2

Hierarchical Partitioning

Hierarchical Partitioning (HP) algorithm infers hierarchical relationships among the leaf nodes of the DOM tree
of an HTML page, where all the document content is stored.
HP achieves this through a sequence of three operations: binary semantic partitioning, grouping, and promotion.
The binary hierarchical partitioning of a Web page is
based on a dynamic programming algorithm that employs
the following cost function. It basically creates an hierarchical binary parenthesization of all the leaf nodes yielding
a binary partition tree. We recursively deﬁne the cost for
grouping any two nodes in the DOM tree as follows:

Cost(Li , Lj ) =

0, if i = j
mini≤k<j {Cost(Li , Lk ) + Cost(Lk+1 , Lj )
+Grouping Cost(Li...k , Lk+1...j )}, if i < j

where Li , Lj are any two leaf nodes in the DOM tree.
The cost function calculates the measure of dissimilarity
between two internal or leaf nodes, that is, a high value
of cost indicates that these two nodes’ subtrees are highly
dissimilar. Thus, the dynamic programming algorithm
ﬁnds the lowest cost among all possible binary groupings
of nodes and parenthesizes them into a binary tree, where
similar nodes are grouped together. The cost for grouping
two consecutive subtrees is calculated using various similarity measures. For example, if A and B are the lowest
common ancestors (LCA) of nodes Li to Lk and Lk+1 to
Lj , then Grouping Cost(Li...k , Lk+1...j ) =
Grouping Cost(A, B) =
Sum of distances of A and B to their LCA +
Similarity of the paths from A and B to their LCA +
Similarity of the paths in the sub trees of A and B +
Order similarity of the paths in the sub trees of A and B +

Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI’05)
0-7695-2415-X/05 $20.00 © 2005 IEEE

Similarity of nodes in the sub trees of A and B
Grouping: Next, we identify and group a sequence of similar binary partitions under Group nodes. The Group nodes
are made up of multiple similar Instance nodes as its children. The grouping algorithm ﬁrst initializes the type of
the leaf nodes in the binary partition tree as “simple”. The
algorithm traverses the tree in post-order and merges the
neighboring “simple” sibling nodes if the cost for grouping
these two nodes is less than an experimentally determined
threshold 0.33. It marks the merged nodes as “Group” node
and the nodes as “Instances”. The algorithm proceeds to
traverse the tree recursively until no two adjacent “simple”
or “Group” nodes can be merged.
Promotion: The ﬁnal step in semantic partitioning is promotion. The promotion algorithm identiﬁes those leaf nodes
that should be promoted above their siblings. Each group
structure is labeled with its nearest preceding emphasized
node and a value instance is labeled with its previous emphasized node. A node label is emphasized if its labeled
text is fully capitalized, or if the html tag of the node has a
bold tag.

3

Figure 2. Identifying the template of instances from a
concept Web page. Solid boxes denote the content of the
news articles among instance pages and dashed boxes denote mismatch segments that may correspond to “humanoriented decoration”. OntoMiner extracts the individual attributes of the news article from these solid boxes.

Taxonomy Mining
The taxonomy mining involves several tasks, including
• Separating important concepts (the categories that deﬁne the context) from instances (the members of each
concept), and human-oriented decoration, and
• Mining taxonomical relationships among the concepts

Various phases involved in taxonomy mining are explained
in the following subsections.
Mapping Labels to Concepts: Initially the frequent labels
across the input Web sites are obtained and they are preprocessed to eliminate invalid labels (e.g., with no link, and
if it points outside the domain, etc.). During this phase,
similar concept labels are grouped together based on their
lexicographic similarity. The words are stemmed using the
Porter’s stemming algorithm [8]. Next, Jaccard’s similarity
∩ Y|
coefﬁcient [4], calculated as |X
|X ∪ Y | , where X and Y are
sets of stemmed words from two different labels, is used to
group similar labels. We denote each collection of labels to
be a concept.
Mining Taxonomical Relationships: The concepts obtained from the mapping phase are ﬂat. To organize them
into a taxonomy, we need to infer hierarchical relationships
among them. Two concepts a and b in a tree are i-related if
a is an ancestor of b and i nodes connect them in the tree.
We ﬁrst mine 1-related pairs, which are direct parent-child
relationships and ﬁnd the frequent relationships. Next, we
follow the same procedure for the union of infrequent 1related pairs and all 2-related pairs to ﬁnd more frequent

is-a relationships. We repeat this procedure until we reach
the maximum depth of the input trees available for mining.
Given the collections of concepts and the hierarchical partition trees corresponding to the relevant home pages, this
algorithm produces the concept taxonomy.
Expanding the Taxonomy beyond Home Pages: The taxonomy obtained from the previous steps still corresponds to
the Home Pages. To expand the domain taxonomy deeper,
we follow the links corresponding to every concept label,
and expand the taxonomy by repeating the earlier phases,
thus identifying sub-concepts. For example, “Sports” is a
concept in the taxonomy obtained from the Home Pages.
If we follow all the links corresponding to “Sports” concept and repeat the above steps, the sub-taxonomy containing sub-concepts such as “Baseball”, “Tennis”, and “Horse
Racing” is obtained. Following the links from all known
concepts and mining them yields the ﬁnal taxonomy for the
domain.

4

News Instance Extraction

A taxonomy schema is made up of a set of hierarchical
relationships between concepts. To populate a taxonomy,
OntoMiner must identify the concept instances. Instances
correspond to members of concepts. In the news domain,
the concept instances are the individual news articles and
their attributes are title, place, text, place, date, etc.

Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI’05)
0-7695-2415-X/05 $20.00 © 2005 IEEE

Domain
www.abcnews.go.com
www.cbc.ca
www.cbsnews.com
www.cnn.com
www.foxnews.com
www.msnbc.com
www.news.bbc.co.uk
www.nytimes.com
www.reuters.com
www.time.com
www.timesonline.co.uk
www.usatoday.com

Number
of Pages
43
54
36
74
32
48
36
64
37
55
47
29

Semantic Partitioner
Precision
Recall
86%
89%
89%
73%
86%
89%
76%
84%
93%
90%
98%
87%
83%
87%
88%
83%
88%
83%
91%
90%
90%
73%
73%
71%

News Articles Extraction
Precision
Recall
82%
87%
88%
74%
86%
89%
77%
79%
92%
93%
91%
90%
92%
89%
97%
84%
86%
85%
94%
95%
88%
71%
75%
79%

Table 1. Experimental results from a sample 12 Web sites for semantic partitioner and news articles extraction algorithms.
Instance extraction procedure ﬁrst identiﬁes the “news
article template” from the concept Web page and uses this
template to extract the individual news articles by following
all the links of that page. To extract the template, we use the
ﬂat partitioning algorithm described in Section 2.1 to partition any two individual news article pages and identify the
segments that are lexicographically dissimilar. We identify
the similar segments from individual news article pages as
“human-oriented decoration”, and the dissimilar segments
as the instance information. An illustration of extracting
the news article template is demonstrated in Figure 2.

6

Conclusions and Future Work

In this paper, we presented techniques to automatically
extract metadata and news instances from news Web sites.
The experimental results indicate that our algorithms were
able to perform well on various news Web sites. In our future work, we propose to improve the cost factors in order
that they can work well for any Web page. We also plan
to investigate techniques that combine syntactic as well as
semantic regularities [6].

References
5

Experimental Results

We tested our algorithms on 31 different news Web sites
with a total number of 3216 individual Web pages. We ﬁrst
present metrics for evaluating the performance of these algorithms and then present the results for the news taxonomy extraction, and news articles extraction algorithms on
a sample 12 news Web sites.
The experimental results for the semantic partitioning
and the individual news extraction are as shown in Table 1.
The semantic partitioner performs with overall precision of
87% and recall of 83%, and the instance extraction algorithm performs with 87% precision and 85% recall respectively. The recall of web pages can be improved by incorporating the Web site speciﬁc information so that correct
boundaries between segments is detected. The precision
and recall values for the taxonomy mining algorithm are
91% and 79% respectively. The experimental results show
that the taxonomy mining algorithm is able to identify the
relevant concepts and their labels. The precision and recall
for all the algorithms is calculated by manually calculating
the “gold-standard” data and comparing to the “algorithmically generated” data.

[1] A. Arasu and H. Garcia-Molina. Extracting structured data
from web pages. In ACM SIGMOD, 2003.
[2] V. Crescenzi, G. Mecca, and P. Merialdo. Roadrunner: Towards automatic data extraction from large web sites. In Intl.
Conf. on Very Large Data Bases, 2001.
[3] M. Garofalakis, A. Gionis, R. Rastogi, S. Seshadri, and
K. Shim. Xtract: A system for extracting document type descriptors from xml documents. In ACM SIGMOD, 2000.
[4] R. Korfhage. Information Storage and Retrieval. John Wiley
Computer Publications, New York, 1999.
[5] K. Lerman, L. Getoor, S. Minton, and C. Knoblock. Using the
structure of web sites for automatic segmentation of tables. In
ACM SIGMOD, 2004.
[6] S. Mukherjee, G. Yang, and I. Ramakrishnan. Annotating
content-rich web documents: Structural and semantic analysis. In International Semantic Web Conference, 2003.
[7] S. Nestorov, S. Abiteboul, and R. Motwani. Extracting
schema from semistructured data. In SIGMOD, pages 295–
306, New York, NY, USA, 1998. ACM Press.
[8] M. Porter. An algorithm for sufﬁx stripping. Program,
14(3):130–137, 1980.

Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI’05)
0-7695-2415-X/05 $20.00 © 2005 IEEE

DataRover: A Taxonomy Based Crawler for Automated
Data Extraction from Data-Intensive Websites
H. Davulcu, S. Koduri, S. Nagarajan
Department of Computer Science and Engineering
Arizona State University,
Tempe, AZ, 85287, USA

{hdavulcu, koduri, nrsaravana}@asu.edu
also presents its content as a sequence of taxonomy pages that
eventually leads to a list-of-products page, which leads to singleproduct pages.

ABSTRACT
The advent of e-commerce has created a trend that brought
thousands of catalogs online. Most of these websites are
“taxonomy-directed”. A Web site is said to be ``taxonomydirected'' if it contains at least one taxonomy for organizing its
contents and it presents the instances belonging to a category in a
regular fashion. This paper describes the DataRover system,
which can automatically crawl and extract products from
taxonomy-directed online catalogs. DataRover utilizes heuristic
rules to discover the structural regularities among: taxonomy
segments, list-of-product and single-product pages and it uses
these regularities to turn the online catalogs into a database of
categorized products without the need for user interaction or the
wrapper maintenance burden. We provide experimental results to
demonstrate the efficacy of the DataRover and point to its current
limitations.

For example if you go to http://store.yahoo.com/shoedini, all the
top level categories are listed in the homepage, see Figure 1, and
if you take any of these top level URLs, it leads to a list of
products page, similar to one in Figure 6. Figure 8 shows samples
of single products pages reachable from list of product pages. This
kind of structure is characteristic of most online catalogs.

Categories and Subject Descriptors
H.m [Information Systems]: Miscellaneous.

General Terms: Algorithms, Design.

Taxonomy

Keywords: Web Data Extraction, Web Annotation, Web Data
Integration

1. INTRODUCTION
The advent of e-commerce has created a trend that brought
thousands of catalogs online. Most of these websites are
“taxonomy-directed”. A Web site is said to be ``taxonomydirected'' if it contains at least one taxonomy for organizing its
products and it presents the instances belonging to a category in a
regular fashion.
Notice that, neither the presentation of the taxonomy among
different pages, nor the presentation of instances among for
different concepts needs to be regular for a Web site to be
classified as “taxonomy-directed”. A taxonomy-directed web site

Figure 1 Home page of [16] with taxonomies
DataRover is a specialized domain-specific crawler that uses
domain-specific heuristics and the regularities within taxonomydirected online catalogs to turn these web sites into product
databases. Such product databases enable construction of domainspecific search engines, such as Froogle.com [10].

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and
that copies bear this notice and the full citation on the first page. To
copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior specific permission and/or a fee.
WIDM’03, November 7–8, 2003, New Orleans, Louisiana, USA.
Copyright 2003 ACM 1-58113-725-7/03/0011…$5.00.

DataRover uses a Page Segmentation Algorithm that takes a DOM
tree of the web page as input and finds the logical segments in it.
Intuitively, this algorithm groups contiguous similar structures in
the Web pages into segments by detecting a high concentration of

9

The Page Segmentation Algorithm takes a DOM tree of the web
page as input and finds the segments in it. Intuitively, this
algorithm groups contiguous similar structures in the Web pages
into segments by detecting a high concentration of neighboring
repeated nodes, with similar root-to-leaf tag-paths. First, the
segment boundary is initialized to the first leaf node in the DOM
tree. Two leaf nodes in the tree are said to have a “similarity link”
between them if they share the same path from the root of the tree
and the leaf nodes in between have different paths. It then counts
the number of similarity links that crosses the boundary and finds
the ratio of number of similarity links that cross the boundary to
the total number of similarity links inside the current segment. If
this ratio is less than threshold δ, the previous node is marked as
the segment boundary. Otherwise, current node is added to the
current segment and next node is considered as the segment
boundary. The above process is stopped when the last element in
the list is reached. A Path Index is built from the DOM tree, from
which the similarity links between the leaf nodes can be
determined. The Path Index Tree (PIT) is a trie data structure,
which has all unique tag-paths in the tree. In its leaf nodes, PIT
also stores the list of leaf nodes in DOM tree that share the same
root to leaf tag-path.

neighboring repeated nodes, with similar root-to-leaf tag-paths.
Next, DataRover applies taxonomy-tests to identify segments
corresponding to taxonomies, crawls the taxonomies recursively
until no more sub-taxonomies are detected. Next, it applies list-ofproducts-test to page segments to see if any qualifies as a list of
products. If a segment qualifies as a list-of-products segment,
DataRover fetches two distinct single product pages, segments
and aligns them to learn the root to leaf html tag paths to product
data. Once it learns these paths, it applies them to all the pages
from the list-of-products page to extract and collect all product
data organized by their root-to-leaf tag-paths.
The early method for information extraction from web sites was to
use wrappers. A wrapper is a program script that is created using
extraction expressions to locate the information of interest in an
HTML page. But, wrappers are site-specific, require user
interaction to create and maintain them whenever the web site
changes. Second kind of approach is to learn and use a template.
A template is generated from the set of input pages and this
template is used to do the extraction
The organization of the rest of the paper is as follows. Section 2
outlines the architecture of the DataRover. Section 3 describes
the Page Segmentation algorithm. Section 4 presents the data
extraction algorithms. Section 5 presents an approach for data
labeling. Section 6 presents experimental results. Section 7
presents related work and section 8 concludes the paper.

B1

2. ARCHITECTURE of DATAROVER

B2

The architecture of the system is as shown in the Fig. 2. The input
to the system is a homepage of interest and output is a set of
records. The components are explained in detail in the following
sections.

B3

B4
B5

B6
Figure 3: Snapshot of a web page and its logical segments
Figure 2 Architecture of the DataRover
Figure 4 illustrates the Page Segmentation Algorithm, presented
in Figure 5, on the HTML DOM tree. The Arrows in the Figure 4
denotes the “similarity links” between the leaf nodes. The
threshold δ is set as 60%. For example, when the current node is
320, the total number of outgoing unique similarity links (out) is
1(line 10) and total number of unique similarity links (total) is 1
(line 11). Hence the ratio of out to total is 100% which is greater
than threshold (condition in line 12 fails). Hence current node
becomes the next leaf node (line 6). At node 341, out becomes 2
and total is also 2. Hence the ratio is still greater than 60%. When
node 345 is reached, out becomes 1 whereas total is 2 and hence

3. PAGE SEGMENTATION
Page Segmentation component creates segments of the given web
page i.e. it detects the various logical sections of the web page.
Consider the product page from
http://store.yahoo.com/shoedini/007850.html
Various logical sections of the web page are marked with boxes in
Fig 3. The function of Page Segmentation Component can be
understood by considering the parse tree view of the given web
page. The Page Segmentation component finds the boundaries
indicated by dotted lines as shown in the Fig 4.

10

the ratio is less than threshold δ (line 12). Now, node 345 (B1 in
Figure 4) is added to the segment_boundaries (line 13) and all the
similarity links are removed from the segment_nodes (line14).
The same condition is satisfied when the algorithm reaches the
node 360 where out becomes 1 and total is 2. Hence the node 360
(B2 in Figure 4) is also added to the segment_boundaries.

4.1 Finding Taxonomies
First step in the data extraction phase is finding the taxonomies or
top level categories. Taxonomy is identified using the following
algorithm. The inputs to this algorithm are the HTML DOM tree
corresponding to the set of segments identified using the page
segmentation for a given page and a set of URLs previously
recorded in that website while crawling.
INPUT: <S1, …, Sk> : Set of segment DOM Trees,
SeenURLs: Hash table of URLs found in previous pages
OUTPUT: List of taxonomy segments
BEGIN
1)

Find sequences, <U1, U2, …, Uk>, where Ui is a list of
consecutive links within Si

2)

Find those Ui’s where the sequence of URL’s are
“similar”, <U1’, U2’, …, Ul’>

3)

IF (|U1’| + .. +|Ul’|) / |Si| > d THEN Si is a Taxonomy
segment. Return Si

END
Two URL’s are “similar” if they differ only by the values of their
page arguments. The d is an experimentally determined threshold.
The algorithm measures if the sequences of similar consecutive
URLs form the majority of the segment. If this test succeeds then
the segment is a taxonomy segment and DataRover crawls from
these URLs.

Figure 4: Parse Tree View of the web page

Figure 6: An example ‘list of products page’

Figure 5 Page Segmentation Algorithm

4.2 Identifying List of Products Segments

4. DATA EXTRACTION

INPUT: DOM Tree of a segment

Data extraction mainly consists of three phases.
1.

Finding sequence of taxonomies from the Home Page

OUTPUT: Product Nodes (if segment is a ‘list of products’
segments else null)

2.

Identifying list of products segment

BEGIN

3.

Extracting data from single product pages

11

1.
2.
3.

4.
5.

two sample product pages and gives them to the Segmentation
Component, which segments these two pages as explained in
Section 4.3.1. Next, Segment Aligner takes this sequence of
segments and aligns them as explained in Section 4.3.2. All the
data from the aligned dissimilar segments are considered as
Product Information. At this stage, we also learn the root to leaf
tag-paths of Product Information nodes in the HTML tree and
apply these tag-paths iteratively on the remaining Product Pages
to collect and organize all the product information.

Use a regular expression for PRICE ($ followed by
digits) to match leaf nodes of the HTML tree and mark
those with PRICE=true.
Mark the leaf nodes corresponding to HTML anchors
with URL=true.
Propagate the PRICE or URL to ancestors as long as
there are no conflicts. A conflict occurs when an
ancestor has two children nodes such that (i)
PRICE=true and (ii) URL=true with two distinct urls.
Mark the children of all such “conflict” nodes as
Product Nodes.
IF the number of Product Nodes within a segment is
greater than two then the segment is a list-of-products
THEN return Product Nodes ELSE return null.

4.3.1 Segment Aligner:
Segment Aligner takes two sequences of segments and aligns
them based on the content similarity in the segments. This
procedure helps in separating the product data from the template.
Segments that are aligned and dissimilar are considered as product
data where as the segments that are aligned and similar are
discarded as page template.

END

To align the two segments we use minimum edit distance
algorithm and to check the similarity between two segments we
use Jaccard coefficient [14], which is calculated as ratio of the
common words in the two segments to the total number of words
present in the two segments.
For example from Figure 8,
P1 = <S1, S2, S3, S4>, where S1, S2, S3, S4
are segments
P2 = <S1’, S2’, S3’, S4’>, where S1’, S2’,
S3’, S4’ are segments. These two sets of
segments are aligned as
M
X
X
M
<S1 S2 S3 S4>

Figure 7: DOM Tree for the list-of-products and markings
In Figure 7, u1, .., u6 are the URL markings and p1, …, p6 are the
PRICE markings. When we propagate these markings to td[I]
nodes in Line 3, still no conflicts are detected. Upon propagating
one more level up, to tr nodes, we detect “conflict” since the
markings at the tr[1] = {u1, u2, p1, p2} which contains two td’s,
td[1] and td[2] with two distinct urls, u1 and u2. Hence, the
children td[1] and td[2] are marked as Product Nodes. Similar
reasoning marks the remaining td’s as Product Nodes too.

<S1’ S2’ S3’ S4>
In this example, S1 aligns with S1’ with a content match (M), S2
aligns with S2’ with content mismatch (X), S3 aligns with S3’
with content mismatch (X), and S4 aligns with S4’ with content
match (M). Hence, we identify segments S2, S3 and S2’, S3’ to
be the product content regions. After finding the product content
regions, we learn the root to leaf HTML paths and apply these on
the remaining single product pages obtained from the list of
products.

4.3 Extracting Data from Single-Product Page
When the ‘list of products’ page identifier succeeds and returns
the list of Product Nodes then the Extractor component retrieves

S1

S1’

S2

S2’

S3

S3’

S4’

S4

Figure 8: Two single-product pages with segments.
12

Another approach [1, 6] is to learn and use a template. A
template is generated from the set of input pages and this template
is used to do actual extraction. This approach of learning the
template may sometimes require too many examples to effectively
learn the schema. Also, user is involved in selecting the pages that
shares the same template. Instead, our information extraction
technique learns information paths using just a few page instances
that are identified automatically. Other difference between our
system and [6] is that we also extract the products categories from
the taxonomies along with the other information.

5. DATA LABELING
The objective of the data labeler is to annotate the columns
obtained from the data extraction phase. When we get the data, we
manually label the attributes of a few tables or use techniques
such as [2] to extract the labels from the web page itself, and then,
train column classifiers based upon the syntactic properties of the
data values. Syntactic features identified include string length,
word counts, number of words and special characters such as
comma, dot, hyphen etc. Using these features, we build a decision
tree classifier for classifying data values to columns similar to
techniques used in [4]. This way, we can automate labeling of
product data columns from hundreds of vendors. In our
experiments, we used the decision tree classifier package C4.5
from [18].

8. CONCLUSIONS AND FUTURE WORK
This paper describes the DataRover system, which can
automatically crawl and extract products from taxonomy-directed
online catalogs. DataRover utilizes heuristic rules to discover the
structural regularities among: taxonomy segments, list-of-product
segments and single-product pages and uses these regularities to
turn the online catalogs into a database of categorized products
without the need for user interaction or the wrapper maintenance
burden.

6. EXPERIMENTAL RESULTS
We run the DataRover on the following sites and obtained the
results as illustrated in the following table. The table illustrates
the performance of DataRover tested against 9 online catalogs.
Any cell marked with YES means that DataRover successfully
identified all the segments corresponding to a taxonomy, list-ofproducts or single product and extracted the corresponding data

Our future work includes:

Table 1 Experimental Results
Taxon
omy

List - of

Single

Products

Product

Finder

Finder

Finder

www.zappos.com

YES

YES

YES

www.onestepahead.com

YES

YES

YES

www.Homevisions.com

YES

YES

YES

www.walgreens.com

YES

YES

YES

Web Catalog

www.smartbargains.com

YES

YES

YES

store.yahoo.com/shoedini

YES

YES

YES

www.overstock.com

YES

YES

YES

www.walmart.com

YES

NO

YES

www.officedepot.com

YES

NO

YES

•

Performing extensive experiments and finding the exact
precision and recall values.

•

Incorporating more robust techniques for record
boundary detection.

•

Currently our system utilizes built-in heuristics for the
product catalogs domain. We plan to investigate domain
independent techniques and algorithms.

9. REFERENCES
[1] A. Arasu, H. Garcia-Molina, “Extracting structured data
from web pages”, Fourth International Workshop on the Web
and Databases, WebDB 2001.

[2] L. Arlotta, V. Crescenzi, G. Mecca, and P. Merialdo.
“Automatic annotation of data extracted from large web
sites”, WebDB 2003, San Diego, CA.

[3] S. Brin, “Extracting patterns and relations from the world
wide web”, In WebDB Workshop at 6th International
Conference on Extending Database Technology, EDBT’98,
1998.

For two web sites, our list-of-products test failed to find the item
boundaries since the children of conflicting parent node do not
correspond to single product nodes but sequences of nodes that
make up a single product itself. Hence, more sophisticated
techniques for product record boundary detection, such as [9], is
necessary. The extracted product data for the above experiments
are presented at http://www.public.asu.edu/~skoduri/datarover

[4] B. Chidlovskii. “Automatic repairing of web wrappers.”
WIDM 2001.

[5] C. Y. Chung, M. Gertz, and N. Sundaresan, “Reverse
engineering for web data: From visual to semantic
structures”, In Intl. Conf. on Data Engineering, 2002.

[6] V. Crescenzi, G. Mecca, and P. Merialdo,
“Roadrunner:Towards automatic data extraction from large
web sites”, In Proc. of the 2001 Intl. Conf. on Very Large
Data Bases, 2001.

7. RELATED WORK
Information extraction from the web is a well-studied problem.
But most of the work is wrapper based. A wrapper is a small
program that extracts the data from the web sites. A wrapper is
created either manually or semi-automatically after analyzing the
location of the data in the HTML pages. The wrapper-based
approaches are explained in [11, 13, 15, 17]. But, wrappers are
site-specific, require user interaction to create and maintain them
whenever the web site changes

[7] DOM tree, http://www.w3.org/DOM.
[8] R. Doorenbos, O. Etzioni, and D. Weld, “A Scalable
Comparison-Shopping Agent for the World-Wide Web”, In
Proceedings of the First International Conference on
Autonomous Agents, pp. 39–48, 1997.

13

[9] D. W. Embley ,Y. Jiang and Y.-K. Ng, “Record-boundary

[15] I. Muslea, S. Minton, and C. A. Knoblock, “A hierarchical

discovery in Web documents”, ACM SIGMOD International
Conference on Management of Data, 1999.

approach to wrapper induction”, In Proceedings of third
International Conference on Autonomous Agents, Seattle,
WA, 1999.

[10] Froogle homepage, www.froogle.com
[11] J. Hammer, H. Garcia-Molina, J. Cho, A. Crespo, and R.

[16] Shoedini Website, http://store.yahoo.com/shoedini.
[17] S. Soderland. “Learning extraction rules for semi- structured

Aranha, “Extracting semi-structure information from the
web”, In Proceedings of the Workshop on Management of
Semistructured Data,1997.

and free text”, Machine Learning, 34:233–272, 1999

[18] I. H. Witten, E. Frank, Data Mining: Practical Machine

[12] JTIDY parser, http://lempinen.net/sami/jtidy/.
[13] N. Kushmerick, D. Weld, and R. Doorenbos, “Wrapper

Learning Tools and Techniques with Java Implementations,
Morgan Kaufmann, October 1999.

[19] X Path, http://www.w3.org/TR/xpath.

induction for information extraction”, In Proc. of the 1997
Intl. Joint Conf. on Artificial Intelligence, 1997.

[14] R. R. Korfhage, Information Storage and Retrieval, John
Wiley Computer Publications, New York, 1999.

14

Hindawi Publishing Corporation
Advances in Bioinformatics
Volume 2012, Article ID 509126, 12 pages
doi:10.1155/2012/509126

Research Article
BioEve Search: A Novel Framework to Facilitate Interactive
Literature Search
Syed Toufeeq Ahmed,1 Hasan Davulcu,2 Sukru Tikves,2
Radhika Nair,2 and Zhongming Zhao1, 3
1 Department

of Biomedical Informatics, Vanderbilt University, Nashville, TN 37232, USA
of Computer Science and Engineering, Arizona State University, Tempe, AZ 85281, USA
3 Department of Cancer Biology, Vanderbilt University School of Medicine, Nashville, TN 37232, USA
2 Department

Correspondence should be addressed to Syed Toufeeq Ahmed, syed.t.ahmed@vanderbilt.edu
Received 15 November 2011; Revised 7 March 2012; Accepted 28 March 2012
Academic Editor: Jin-Dong Kim
Copyright © 2012 Syed Toufeeq Ahmed et al. This is an open access article distributed under the Creative Commons Attribution
License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly
cited.
Background. Recent advances in computational and biological methods in last two decades have remarkably changed the scale
of biomedical research and with it began the unprecedented growth in both the production of biomedical data and amount of
published literature discussing it. An automated extraction system coupled with a cognitive search and navigation service over
these document collections would not only save time and eﬀort, but also pave the way to discover hitherto unknown information
implicitly conveyed in the texts. Results. We developed a novel framework (named “BioEve”) that seamlessly integrates Faceted
Search (Information Retrieval) with Information Extraction module to provide an interactive search experience for the researchers
in life sciences. It enables guided step-by-step search query refinement, by suggesting concepts and entities (like genes, drugs, and
diseases) to quickly filter and modify search direction, and thereby facilitating an enriched paradigm where user can discover
related concepts and keywords to search while information seeking. Conclusions. The BioEve Search framework makes it easier
to enable scalable interactive search over large collection of textual articles and to discover knowledge hidden in thousands of
biomedical literature articles with ease.

1. Background
Human genome sequencing marked the beginning of the
era of large-scale genomics and proteomics, leading to large
quantities of information on sequences, genes, interactions,
and their annotations. In the same way that the capability
to analyze data increases, the output by high-throughput
techniques generates more information available for testing
hypotheses and stimulating novel ones. Many experimental findings are reported in the -omics literature, where
researchers have access to more than 20 million publications,
with up to 4,500 new ones per day, available through to
the widely used PubMed citation index and Google Scholar.
This vast increase in available information demands novel
strategies to help researchers to keep up to date with recent
developments, as ad hoc querying with Boolean queries is
tedious and often misses important information.

Even though PubMed provides an advanced keyword
search and oﬀers useful query expansion, it returns hundreds
or thousands of articles as result; these are sorted by
publication date, without providing much help in selecting
or drilling down to those few articles that are most relevant
regarding the user’s actual question. As an example of both
the amount of available information and the insuﬃciency
of naı̈ve keyword search, the name of the protein p53 occurs
in 53,528 PubMed articles, and while a researcher interested
specifically in its role in cancer and its interacting partners
might try the search “p53 cancer interaction” to narrow down
the results, this query still yields 1,777 publications, enough
for months of full-time reading [1]. Nonetheless, PubMed is
a very widely used free service and is providing an invaluable
service to the researchers around the world. In March 2007,
PubMed served 82 million (statistics of Medline searches:
http://www.nlm.nih.gov/bsd/medline growth.html) query

2
searches and the usage is ever increasing. A few commercial
products are currently available that provide additional
services, but they also rely on basic keyword search, with
no real discovery or dynamic faceted search. Examples are
OvidSP and Ingenuity Answers, both of which support bookmarking as one means of keeping track of visited citations.
Research tools such as EBIMed (EBIMed: http://www.ebi.ac
.uk/Rebholz-srv/ebimed/index.jsp) [2] and AliBaba (AliBaba:
http://alibaba.informatik.hu-berlin.de)
[3]
provide
additional cross-referencing of entities to databases
such as UniProt or to the GeneOntology. They also try
to identify relations between entities, such as protein-protein
interactions, functional protein annotations, or gene-disease
associations.
Search tools should provide dedicated and intuitive
strategies that help to find relevant literature, starting
with initial keyword searches and drilling down results via
overviews enriched with autogenerated suggestions to refine
queries. One of the first steps in biomedical text mining is to
recognize named entities occurring in a text, such as genes
and diseases. Named entity recognition (NER) is helpful
to identify relevant documents, index a document collection, and facilitate information retrieval (IR) and semantic
searches [4]. A step on top of NER is to normalize each entity
to a base form (also called grounding and identification); the
base form often is an identifier from an existing, relevant
database; for instance, protein names could be mapped to
UniProt IDs [5, 6]. Entity normalization (EN) is required
to get rid of ambiguities such as homonyms, and map synonyms to one and the same concept. This further alleviates
the tasks of indexing, IR, and search. Once named entities
have been identified, systems aim to extract relationships
between them from textual evidences; in the biomedical
domain, these include gene-disease associations and proteinprotein interactions. Such relations can then be made available for subsequent search in relational databases or used for
constructing particular pathways and entire networks [7].
Information extraction (IE) [8–11] is the extraction of
salient facts about prespecified types of events, entities [12],
or relationships from free text. Information extraction from
free text utilizes shallow-parsing techniques [13], part-ofspeech tagging [14], noun and verb phrase chunking [15],
predicate-subject and object relationships [13], and learned
[8, 16, 17] or hand-build patterns [18] to automate the creation of specialized databases. Manual pattern engineering
approaches employ shallow parsing with patterns to extract
the interactions. In the system presented in [19], sentences
are first tagged using a dictionary-based protein name
identifier and then processed by a module which extracts
interactions directly from complex and compound sentences
using regular expressions based on part of speech tags. IE
systems look for entities, relationships among those entities,
or other specific facts within text documents. The success of
information extraction depends on the performance of the
various subtasks involved.
The Suiseki system of Blaschke et al. [20] also uses regular
expressions, with probabilities that reflect the experimental
accuracy of each pattern to extract interactions into predefined frame structures. Genies [21] utilizes a grammar-based

Advances in Bioinformatics
natural language processing (NLP) engine for information
extraction. Recently, it has been extended as GeneWays [22],
which also provides a Web interface that allows users to
search and submit papers of interest for analysis. The BioRAT
system [23] uses manually engineered templates that combine lexical and semantic information to identify protein
interactions. The GeneScene system [24] extracts interactions using frequent preposition-based templates.
Over the last years, a focus has been on the extraction
of protein-protein interactions in general, recently including
extraction from full text articles, relevance ranking of
extracted information, and other related aspects (see, for
instance, the BioCreative community challenge [25]). The
BioNLP’09 Shared Task concentrated on recognition of more
fine-grained molecular events involving proteins and genes
[26]. Both papers give overviews over the specific tasks and
reference articles by participants.
One of the first eﬀorts to extract information on biomolecular events was proposed by Yakushiji et al. [27]. They
implemented an argument structure extractor based on full
sentence parses. A list of target verbs have specific argument
structures assigned to each. Frame-based extraction then
searches for filler of each slot required according to the particular arguments. On an small in-house corpus, they found
that 75% of the errors can be attributed to erroneous parsing
and another 7% to insuﬃcient memory; both causes might
have less impact on recent systems due to more accurate
parsers and larger memory.
Ding et al. [28] studied the extraction of protein-protein
interactions using the Link Grammar parser. After some
manual sentence simplification to increase parsing eﬃciency,
their system assumed an interaction whenever two proteins
were connected via a link path; an adjustable threshold
allowed to cut oﬀ too long paths. As they used the original
version of Link Grammar, Ding et al. [28] argued that
adaptations to the biomedical domain would enhance the
performance.
An information extraction application analyzes texts
and presents only the specific information from them that
the user is interested in [29]. IE systems are knowledge
intensive to build and are to varying degrees tied to particular
domains and scenarios such as target schema. Almost all IE
applications start with fixed target schema as a goal and are
tuned to extract information from unstructured text that will
fit the schema. In scenarios where target schema is unknown,
open information extraction systems [30] like KnowItNow
[31] and TextRunner [32] allow rules to be defined easily
based on the extraction need. An hybrid application (IR +
IE) that leverages the best of information retrieval (ability
to relevant texts) and information extraction (analyze text
and present only specific information user is interested in)
would be ideal in cases when the target extraction schema is
unknown. An iterative loop of IR and IE with user’s feedback
will be potentially useful. For this application, we will
need main components of IE system (like parts-of-speech
tagger, named entity taggers, shallow parsers) preprocesses
the text before being indexed by a custom-built augmented
index that helps retrieve queries of the type “Cities such
as ProperNoun(Head(NounPhrase)).” Cafarella and Etzioni

Advances in Bioinformatics
[33] have done work in this direction to build a search
engine for natural language and information extraction
applications.
Exploratory search [34] is a topic that has grown from the
fields of information retrieval and information seeking but
has become more concerned with alternatives to the kind of
search that has received the majority of focus (returning the
most relevant documents to a Google-like keyword search).
The research is motivated by questions like “what if the user
does not know which keywords to use?” or “what if the user
is not looking for a single answer?”. Consequently, research
began to focus on defining the broader set of information
behaviors in order to learn about situations when a user
is—or feels—limited by having only the ability to perform a
keyword search (source: http://en.wikipedia.org/wiki/Exploratory search). Exploratory search can be defined as specialization of information exploration which represents the
activities carried out by searchers who are either [35]:
(1) unfamiliar with the domain of their goal (i.e., need to
learn about the topic in order to understand how to
achieve their goal);
(2) unsure about the ways to achieve their goals (either
the technology or the process); or even
(3) unsure about their goals in the first place.
A faceted search system (or parametric search system)
presents users with key value metadata that is used for
query refinement [36]. By using facets (which are metadata
or class labels for entities such as genes or diseases), users
can easily combine the hierarchies in various ways to refine
and drill down the results for a given query; they do not
have to learn custom query syntax or to restart their search
from scratch after each refinement. Studies have shown
that users prefer faceted search interfaces because of their
intuitiveness and ease of use [37]. Hearst [38] shares her
experience, best practices, and design guidelines for faceted
search interfaces, focusing on supporting flexible navigation,
seamless integration with directed search, fluid alternation
between refining and expanding, avoidance of empty results
sets, and most importantly making users at ease by retaining
a feeling of control and understanding of the entire search
and navigation process. To improve web search for queries
containing named entities [39], automatically identify the
subject classes to which a named entity might refer to and
select a set of appropriate facets for denoting the query.
Faceted search interfaces have made online shopping
experiences richer and increased the accessibility of products
by allowing users to search with general keywords and
browse and refine the results until the desired sub-set is
obtained (SIGIR’2006 Workshop on Faceted Search (CFP):
http://sites.google.com/site/facetedsearch/). Faceted navigation delivers an experience of progressive query refinement
or elaboration. Furthermore, it allows users to see the
impact of each incremental choice in one facet on the
choices in other facets. Faceted search combines faceted
navigation with text search, allowing users to access (semi)
structured content, thereby providing support for discovery

3
and exploratory search, areas where conventional search falls
short [40].

2. Approach
In an age of ever increasing published research documents
(available in search-able textual form) containing amounts
of valuable information and knowledge that are vital to
further research and understanding, it becomes imperative
to build tools and systems that enable easier and quick access
to right information the user is seeking for, and this has
already become an information overload problem in diﬀerent domains. Information Extraction (IE) systems provide
an structured output by extracting nuggets of information
from these text document collections, for a defined schema.
The output schema can vary from simple pairwise relations
to a complex, nested multiple events.
Faceted search and navigation is an eﬃcient way to
browse and search over a structured data/document collection, where the user is concerned about the completeness of
the search, not just top ranked results. Faceted search system
needs structured input documents, and IE systems extract
structured information from text documents. By combining
these two paradigms, we are able to provide faceted search
and navigation over unstructured text documents, and, with
this fusion, we are also able to leverage real utility of information extraction, that is, finding hidden relationships as the
user goes through a search process, and to help refine the
query to more satisfying and relevant level, all while keeping
user feel incontrol of the whole search process.
We developed BioEve Search (http://www.bioeve.org/)
framework to provide fast and scalable search service, where
users can quickly refine their queries and drill down to the
articles they are looking for in a matter of seconds, corresponding to a few number of clicks. The system helps identify
hidden relationships between entities (like drugs, diseases,
and genes), by highlighting them using a tag cloud to give
a quick visualization for eﬃcient navigation. In order to
have suﬃcient abstraction between various modules (and
technologies used) in this system, we have divided this framework into four diﬀerent layers (refer to Figure 1) and they
are (a) Data Store layer, (b) Information Extraction layer,
(c) Faceting layer, and (d) Web Interface layer. Next sections
explain each layer of this framework in more details.
2.1. Data Store Layer. The Data Store layer preprocesses
and stores the documents in an indexed data store to make
them eﬃciently accessible to the modules of upper layer
(information extraction layer). Format conversion is needed
sometimes (from ASCII to UTF-8 or vice versa), or XML
documents need to be converted to text documents before
being passed to next module. After the documents are in the
required format and cleansed, they are stored in a indexed
data store for eﬃcient and fast access to either individual
documents or the whole collections. The data store can be
implemented using an Indexer service like (Apache Lucene
(Lucene: http://lucene.apache.org/) or any database like
MySQL). The Medline dataset is available as zipped XML

Advances in Bioinformatics

Data import
(XML)

Solr schema
.XML

Web API
(JSON, HTTP/XML)

Faceting engine
(Apache Solr)

Discovery interface
(AJAX and Javascript)

Web service
(Apache Tomcat)

BioEve event

CRF classifier

Extraction engine
(biomolecular events)

ABNER

Data store
layer

Information
extraction
layer

Faceting layer

Web interface
layer

4

OSCAR3
MeSH

Preprocessing
(XML2text-SAX
parser)
MEDLINE

Apache Lucene (data store)

Figure 1: BioEve search framework architecture.

files that needed XML2 text conversion, after which we could
ingest them into an indexer, Apache Lucene in our case. Such
an indexer allows for faster access and keyword-based text
search to select a particular subset of abstracts for further
processing.

once an article is classified and annotated with diﬀerent
entity types, it does not need to be processed again for each
search query. This step can be done preindexing and as a
batch process.
2.3. Faceting Layer

2.2. Information Extraction Layer. For recognizing diﬀerent
gene/protein names, DNA, RNA, cell line, and cell types,
we leveraged ABNER [41], A Biomedical Named Entity
Recognizer. We used OSCAR3 (Oscar3: http://sourceforge
.net/projects/oscar3-chem/) (Open Source Chemistry Analysis Routines) to identify chemical names and chemical structures. To annotate disease names, symptoms, and causes,
we used a subset of the Medical Subject Heading (MeSH)
dataset (MeSH: http://www.nlm.nih.gov/mesh/).
2.2.1. Annotating Biomolecular Events in the Text. A first
step towards bio-event extraction is to identify phrases in
biomedical text which indicate the presence of an event.
The labeled phrases are classified further into nine event
types (based on the Genia corpus (BioNLP’09 Shared Task 1:
http://www.nactem.ac.uk/tsujii/GENIA/SharedTask/)). The
aim of marking such interesting phrases is to avoid looking
at the entire text to find participants, as deep parsing of sentences can be a computationally expensive process, especially
for the large volumes of text. We intend to mark phrases in
biomedical text, which could contain a potential event, to
serve as a starting point for extraction of event participants.
Section 6.1 gives more details about our experimentations
with classification and annotation of biomedical entities.
All the classification and annotation were done oﬄine
before the annotated articles are indexed for the search as

2.3.1. Faceting Engine. To provide faceted classification and
navigation over these categories (facets), many oﬀ-the-shelf
systems are available such as in academia; Flamenco project
(Flamenco: http://flamenco.berkeley.edu/) (from University
of California Berkeley) and mspace (mspace: http://mspace
.fm/) (University of Southampton) and in enterprise area;
Apache Solr (Apache Solr: http://lucene.apache.org/solr/)
and Endeca (Endeca: http://www.endeca.com/). We used the
Apache Solr library for faceted search, which also provides an
enterprise quality full-text search.
2.3.2. Shared Schema between IE Layer and Faceting Layer. In
order to facilitate indexing and faceting over the extracted
semi-structured text articles, both IE layer and faceting layer
needs to share a common schema. A sample of shared schema
used for enabling interaction between these layers is shown in
Scheme 1.
2.4. Web Interface Layer. With the advent of Web 2.0 technologies, web-based interfaces have undergone delightful
improvements and now provide rich dynamic experiences.
Key component in this layer is a user interface that connects
the user with the web service from the faceting layer and
provides features that allow search, selection of facet/values,
refinement, query restart, and dynamic display of a result

Advances in Bioinformatics

<field
<field
<field
<field
<field
<field

5

name="pmid" type="string" indexed="true" stored="true" required="true"/>
name="text" type="text" indexed="true" stored="true" multiValued="true"/>
name="title" type="text" indexed="true" stored="true"/>
name="gene" type="string" indexed="true" stored="true" multiValued="true"/>
name="drug" type="string" indexed="true" stored="true" multiValued="true"/>
name="disease" type="string" indexed="true" stored="true" multiValued="true"/>
Scheme 1

set as user interacts and navigates. It also provides the bulk
import of data for further analysis of the faceting/extraction.
The web interface provides following features for interactive search and navigation. The interface presents a number
of entities types (on the left panel) along with the specific
instances/values, from previous search results, and the
current query. Users can choose any of the highlighted values
of these entity types to interactively refine the query (add
new values/remove any value from the list with just one
click) and thereby drill down to the relevant articles quickly
without actually reading the entire abstracts. Users can easily
remove any of the previous search terms, thus widening
the current search. We implemented the BioEve user interface using AJAX (AJAX: http://evolvingweb.github.com/ajaxsolr/), Javascript, and JSON to provide rich dynamic experience. The web interface runs on an Apache Tomcat server.
Next section explains about navigation aspect of the user
interface.

3. User Interface: A Navigation Guide
Search interface is divided into left and right panels, see
Figure 2, basically displaying enriched keywords and results,
respectively.
Left panel: it oﬀers suggestions and insights (based on
cooccurrence frequency with the query terms) for diﬀerent
entities types, such as genes, diseases, and drugs/chemicals.
(i) Left panel shows navigation/refinement categories
(genes, diseases, and drugs); users can click on any of
the entity names (in light blue) to refine the search.
By clicking on an entity, the user adds that entity
to the search and the results on the right panel are
refreshed on the y to reflect the refined results.
(ii) Users can add or remove any number of refinements
to the current search query until they reach the
desired results set (shown in the right panel).
Right panel: it shows the user’s current search results
and is automatically refreshed based on user’s refinement and
navigation choices on the left panel.
(i) The top of the panel shows users current query terms
and navigation so far. Here, users can also deselect
any of the previously selected entities or even all of
them by single click on “remove all.” By deselecting
any entities, user is essentially expanding the search
and the results in the right panel are refreshed on
the fly to remaining query entities to oﬀer a dynamic
navigation experience.

(ii) Abstracts results on this panel show “title” of the
abstract (in light red), full abstract text (in black, if
abstract text is available).
(iii) Below the full abstract text, the list of entities mentioned in that abstracts (in light blue) is shown. These
entities names are clickable and will start a new search
for that entity name, with a single click.
(iv) A direct URL is also provided to the abstract page on
http://pubmed.gov in case the user wants to access
additional information such as authors, publication
type, or links to a full-text article.

4. Interactive Search and Navigation:
A Walk through and behind the Scenes
Let us start an example search process, say with the query
“cholesterol” and the paragraph titled “behind-the-scenes”
gives details of the computational process behind the action.
(1) The autocomplete feature helps in completion of the
name while typing if the word is previously mentioned in the
literature, which is the case here with “cholesterol.”
Behind-the-scenes: as user starts typing, the query is
tokenized (in case of multiple words) and search is made to
retrieve word matches (and not the result rows yet) using the
beginning with the characters user has already typed, and this
loop continues. Technologies at play are jQuery, AJAX, and
faceting feature of Apache Solr. Once the query is submitted
by the user, the results rows also contain the annotated entity
names and these are used to generate tag clouds, using the
faceting classification entity frequency count.
The search results in 27177 articles hits (Figure 3). Those
are a lot of articles to read. How about narrowing down these
results with some insights given by BioEve Search?
(2) In left panel, “hepatic lipase” is highlighted; let
us click on that as it shows some important relationship
between “cholesterol” and “hepatic lipase.” The search
results are now narrowed down to 195 articles from 27177
(Figure 4). That is still a lot of articles to read this afternoon,
how about some insights on diseases.
Behind-the-scenes: once user click on a highlighted entity
name in tag cloud, this term (gene: “hepatic lipase”) is added
to the search filter and the whole search process and tag could
be generated again for the new query.
You can see disease “hyperthyroidism” highlighted in
Figure 5.
(3)Selecting “hyperthyroidism” drills results down to 3,
as can be seen in Figure 6.

6

Advances in Bioinformatics

Figure 2: A sample screen shot of the main search screen. Left panel shows clickable top relevant entities, which if selected refines the query
and results dynamically. User can deselect any of the previously selected entities to refine query more, and the results are updated dynamically
to reflect the current selected list of entities.

Figure 3: A sample result set with the query “cholesterol.”

The top result is about “Treatment of hyperthyroidism:
eﬀects on hepatic lipase, lipoprotein lipase, LCAT and plasma
lipoproteins”. With few clicks user can refine search results to
more relevant articles.

5. Initial User Reviews and Feedback
We asked three life science researchers to review and provide
feedback on ease of search and novelty of the system, and

shown below is their feedback (paraphrased). Their names
and other details are removed for privacy purposes.
5.1. Researcher One, P.h.D, Research Fellow, Microbiology,
University of California, Berkeley
“ I am impressed by ease of its use.” “When I
have the confidence that BioEve is indexing all
the data without missing any critical article, I

Advances in Bioinformatics

7

Figure 4: “Hepatic-lipase” selected.

Figure 5: “Hyperthyroidism” highlighted.

Figure 6: Final refined search results.

8

Advances in Bioinformatics
will be compelled to use this search tool. I believe
a finished product will be immensely useful and
could become a popular tool for life science
researchers.”

5.2. Researcher Two, P.h.D, Investigator and Head, Molecular
Genetics Laboratory
“You have a powerful search. Synchronize this
with MEDLINE. Connect with more databases,
OMIM, Entrez Gene . . .. You can get cell line
database from ATCC.org.”

Table 1: Classification approaches used: Naı̈ve Bayes classifier
(NBC), NBC + Expectation Maximization (EM), Maximum
Entropy (MaxEnt), Conditional Random Fields (CRFs).
Granularity
Single label,
Sentence level

Features

Classifier

Bag-of-words (BOW)

NBC

BOW + gene names boosted
BOW + trigger words boosted
BOW + gene names and trigger
words boosted

Multiple labels

BOW

NBC +
EM

5.3. Researcher Three, P.h.D, Postdoc Researcher, Faculty of
Kinesiology, University of Calgary
“I particularly like the idea of having larger fonts
for the more relevant terms highlighting what is
researched more often.”

Sentence level
Event trigger
phrase labeling

MaxEnt
BOW +

CRFs

3-gram and 4-gram
prefixes and suﬃxes +
orthographic features +
trigger phrase dictionary

6. Methods
6.1. Information Extraction: Annotating Sentences with
Biomolecular Event Types. The first step towards bioevent
extraction is to identify phrases in biomedical text which
indicate the presence of an event. The aim of marking such
interesting phrases is to avoid looking at the entire text to find
participants. We intend to mark phrases in biomedical text,
which could contain a potential event, to serve as a starting
point for extraction of event participants. We experimented
with well-known classification approaches, from a naı̈ve
Bayes classifier to the more sophisticated machine classification algorithms Expectation Maximization, Maximum
Entropy, and Conditional Random Fields. Overview of
diﬀerent classifiers applied at diﬀerent levels of granularity
and the features used by these classifiers is shown in Table 1.
For naı̈ve Bayes classifier implementation, we utilized
WEKA (WEKA: http://www.cs.waikato.ac.nz/ml/weka/)
library, a collection of machine learning algorithms for
data mining tasks, for identifying single label per sentence
approach. WEKA does not support multiple labels for the
same instance. Hence, we had to include a tradeoﬀ here by
including the first encountered label in the case where the
instance had multiple labels. For Expectation Maximization
(EM) and Maximum Entropy (MaxEnt) algorithms, we used
classification algorithms from MALLET library (MALLET:
http://mallet.cs.umass.edu/index.php). Biomedical abstracts
are split into sentences. For training purposes, plain text
sentences are transformed into training instances as required
by MALLET.
6.1.1. Feature Selection for Naı̈ve Bayes, EM, and MaxEnt
Classifiers. For the feature sets mentioned below, we used the
TF-IDF representation. Each vector was normalized based
on vector length. Also, to avoid variations, words/phrases
were converted to lowercase. Based on WEKA library token
delimiters, features were filtered to include those which
had an alphabet as a prefix, using regular expressions.

For example, features like −300 bp were filtered out, but
features like p55, which is a protein name, were retained.
We experimented with the list of features described below,
to understand how well each feature suits the corpus under
consideration.
(i) Bag-of-words model: this model classified sentences
based on word distribution.
(ii) Bag-of-words with gene names boosted: the idea was
to give more importance to words, which clearly
demarcate event types. To start with, we included
gene names provided in the training data. Next, we
used the ABNER (ABNER: http://pages.cs.wisc.edu/
∼bsettles/abner/), a gene name tagger, to tag gene
names, apart from the ones already provided to us.
We boosted weights for renamed feature “protein”, by
2.0.
(iii) Bag-of-words with event trigger words boosted: we
separately tried boosting event trigger words. The list
of trigger words was obtained from training data.
This list was cleaned to remove stop words. Trigger
words were ordered in terms of their frequency of
occurrence with respect to an event type, to capture
trigger words which are most discriminative.
(iv) Bag-of-words with gene names and event trigger
words boosted: the final approach was to boost both
gene names and trigger words together. Theoretically,
this approach was expected to do better than previous
two feature sets discussed. Combination of discriminative approach of trigger words and gene name
boosting was expected to train the classifier better.
6.1.2. Evaluation of Sentence Level Classification Using Naı̈ve
Bayes Classifier. This approach assigns a single label to

Advances in Bioinformatics

9

Table 2: Single label, sentence level results.
Classifier Feature set
Bag-of-words
Bag-of-words + gene name boosting
NBC
Bag-of-words + trigger word boosting
Bag-of-words + trigger word boosting +
Gene name boosting
Bag-of-POS tagged words

Precision
62.39%
50.00%
49.92%
49.77%
43.30%

each sentence. For evaluation purposes, the classifier is
tested against GENIA development data. For every sentence,
evaluator process checks if the event type predicted is the
most likely event in that sentence. In case a sentence has more
than one event with equal occurrence frequency, classifier
predicted label is compared with all these candidate event
types. The intent of this approach was to just understand
the features suitable for this corpus. Classifier evaluated was
NaiveBayesMultinomial classifier from Weka (http://www.cs
.waikato.ac.nz/ml/weka/) library, which is a collection of
machine learning algorithms for data mining tasks. Table 2
shows precision results for NBC classifier with diﬀerent
feature sets for single label per sentence classification.
6.2. Conditional Random Fields Based Classifier. Conditional
Random fields (CRFs) are undirected statistical graphical
models, a special case of which is a linear chain that
corresponds to a conditionally trained finite-state machine
[41]. CRFs in particular have been shown to be useful in
parts-of-speech tagging [42] and shallow parsing [42]. We
customized ABNER which is based on MALLET, to suit our
needs. ABNER employs a set of orthographic and semantic
features.
6.2.1. Feature Selection for CRF Classifier. The default model
included the training vocabulary (provided as part of the
BIONLP-NLPBA 2004 shared task) in the form of 17
orthographic features based on regular expressions [41].
These include upper case letters (initial upper case letter, all
upper case letters, mix of upper and lower case letters), digits
(special expressions for single and double digits, natural
numbers, and real numbers), hyphen (special expressions
for hyphens appearing at the beginning and end of a
phrase), other punctuation marks, Roman and Greek words,
and 3-gram and 4-gram suﬃxes and prefixes. ABNER uses
semantic features that are provided in the form of handprepared (Greek letters, amino acids, chemical elements,
known viruses, abbreviations of all these) and databasereferenced lexicons (genes, chromosome locations, proteins,
and cell lines).
6.3. Evaluation of Sentence Classification Approaches. The
framework is designed for large-scale extraction of molecular
events from biomedical texts. To assess its performance, we
evaluated the underlying components on the GENIA event
dataset made available as part of BioNLP’09 Shared Task

[26]. This data consists of three diﬀerent sets: the training
set consists of 800 PubMed abstracts (with 7,499 sentences),
the development set has 150 abstracts (1,450 sentences),
and the test set has 260 abstracts (2,447 sentences). We
used the development set for parameter optimization and
fine tuning and evaluated the final system on the test set.
Employed classifiers were evaluated based on precision and
recall. Precision indicates the correctness of the system,
by measuring number of samples correctly classified in
comparison to the total number of classified sentences. Recall
indicates the completeness of the system, by calculating the
number of results which actually belong to the expected set of
results. Sentence level single label classification and sentence
level multilabel classification approaches were evaluated
based on how well the classifier labels a given sentence from
a test set with one of the nine class labels. Phrase level
classification using CRF model was evaluated based on how
well the model tags trigger phrases. Evaluating this approach
involved measuring the extent to which the model identifies
that a phrase is a trigger phrase and how well it classifies a
tagged trigger phrase under one of the nine predefined event
types. Retrieved trigger phrases refer to the ones which are
identified and classified by the CRF sequence tagger. Relevant
trigger phrases are the ones which are expected to be tagged
by the model. Retrieved and relevant trigger words refer to
the tags which are expected to be classified and which are
actually classified by the CRF model. All the classifiers are
trained using BioNLP shared task training data and tested
against BioNLP shared task development abstracts.
We compare the above three approaches for classification
in Table 3. CRF has a good tradeoﬀ as compared to Maximum Entropy classifier results. As compared to multiple
labels, sentence level classifiers, it performs better in terms
of having a considerably good accuracy for most of the event
types with a good recall. It not only predicts the event types
present in the sentence, but also localizes the trigger phrases.
There are some entries where ME seems to perform better
than CRF; for example, in case of positive regulation, where
the precision is as high as 75%. However, in this case, the
recall is very low (25%). The reason noticed (in training
examples) was that, most of the true example sentences of
positive regulation or negative regulation class type were
misclassified as either phosphorylation or gene expression.
The F1-score for CRF indicates that, as compared to the
other approaches, CRF predicts 80% of the relevant tags, and,
among these predicted tags, 68% of them are correct.
6.3.1. Evaluation of Phrase Level Labeling. Evaluation of
this approach was focused more on the overlap of phrases
between the GENIA annotated development and CRF tagged
labels. The reason being for each abstract in the GENIA
corpus, there is generally a set of biomedical entities present
in it. For the shared task, only a subset of these entities was
considered in the annotations, and accordingly only events
concerning these annotated entities were extracted. However,
based on the observation of the corpus, there was a probable
chance of other events involving entities not selected for the
annotations. So we focused on the coverage, where both the
GENIA annotations and CRF annotations agree upon. CRF

10

Advances in Bioinformatics

Table 3: Summary of classification approaches: test instances (marked events) for each class type in test dataset. Precision, recall, and F1score in percentage. Compared to NB + EM and CRF, Maximum Entropy based classifier had better average precision, but CRF has best
recall and good precision, giving it best F-Measure of the three well-known classifiers.
Event type

NB + EM

Test instances
Total: 942

P

R

MaxEnt
F1

P

R

CRF
F1

P

R

F1

Phosphorylation

38

62

42

50

97

73

83

80

83

81

Protein catabolism

17

60

47

53

97

73

83

85

86

85

Gene expression

200

60

41

49

88

58

70

75

81

78

Localization

39

39

47

43

61

69

65

67

79

72

Transcription

60

24

52

33

49

80

61

57

78

66

Binding

153

56

63

59

65

62

63

65

81

72

Regulation

90

47

69

55

52

67

58

62

73

67

Positive regulation

220

70

27

39

75

25

38

55

74

63

Negative regulation

125

42

46

44

54

38

45

68

82

74

51

48

47

71

61

63

68

80

73

Average

7. Discussion and Conclusions

Table 4: CRF sequence labeling results.
Type of evaluation
Exact boundary matching
Soft boundary matching

Coverage %
79%
82%

performance was evaluated on two fronts in terms of this
overlap.
(i) Exact boundary matching: this involves exact label
matching and exact trigger phrase match.
(ii) Soft boundary matching: this involves exact label
matching and partial trigger phrase match, allowing
1-word window on either side of the actual trigger
phrase.
Checking of the above constraints was a combination
of template matching and manually filtering of abstracts.
Table 4 gives an estimate of the coverage. Soft boundary
matching increases the coverage by around 3%. Table 3
gives the overall evaluation of CRF with respect to GENIA
corpus. With regards to the CRF results, accuracy for positive
regulation is comparatively low. Also, the test instances for
positive regulation were more than any other event type. So
this reduced average precision to some extent.
A detailed analysis of the results showed that around 3%
tags were labeled incorrectly in terms of the event type. There
were some cases where it was not certain whether an event
should be marked as regulation or positive regulation. Some
examples include “the expression of LAL-mRNA,” where
“LAL-mRNA” refers to a gene. As per examples seen in the
training data, the template of the form “expression of <gene
name>” generally indicates presence of a Gene expression
event. Hence, more analysis may be need to exactly filter out
such annotations as true negatives or deliberately induced
false positives.

PubMed is one of the most well known and used citation
indexes for the Life Sciences. It provides basic keyword
searches and benefits largely from a hierarchically organized
set of indexing terms, MeSH, that are semi-automatically
assigned to each article. PubMed also enables quick searches
for related publications given one or more articles deemed
relevant by the user. Some research tools provide additional
cross-referencing of entities to databases such as UniProt
or to the GeneOntology. They also try to identify relations
between entities of the same or diﬀerent types, such as
protein-protein interactions, functional protein annotations,
or gene-disease associations. GoPubMed [43] guides users
in their everyday searches by mapping articles to concept
hierarchies, such as the Gene Ontology and MeSH. For each
concept found in abstracts returned by the initial user query,
GoPubMed computes a rank based on occurrences of that
concept. Thus, users can quickly grasp which terms occur
frequently, providing clues for relevant topics and relations,
and refine subsequent queries by focusing on particular
concepts, discarding others.
In this paper, we presented BioEve Search framework,
which can help identify important relationships between
entities such as drugs, diseases, and genes by highlights
them during the search process. Thereby, allowing the
researcher not only to navigate the literature, but also to see
entities and the relations they are involved in immediately,
without having to fully read the article. Nonetheless, we
envision future extensions to provide a more complete and
mainstream service and here are few of these next steps.
Keeping the search index up-to-date and complete: we
are adding a synchronization module that will frequently
check with Medline for supplement articles as they are
published; these will typically be in the range of 2500–4500
new articles per day. Frequent synchronization is necessary
to keep BioEve abreast with Medline collection and give users
the access to the most recent articles.

Advances in Bioinformatics
Normalizing and grounding of entity names: as the same
gene/protein can be referred by various names and symbols
(e.g., the TRK-fused gene is also known as TF6; TRKT3;
FLJ36137; TFG), a user searching for any of these names
should find results mentioning any of the others. Removal
of duplicates and cleanup of nonbiomedical vocabulary
that occurs in the entity tag clouds will further improve
navigation and search results.
Cross-referencing with biomedical databases: we want
to cross-reference terms indexed with biological databases.
For example, each occurrence of a gene could be linked to
EntrezGene and OMIM; cell lines can be linked and enriched
with ATCC.org’s cell line database; we want to crossreference disease names with UMLS and MeSH to provide
access to ontological information. To perform this task of
entity normalization, we have previously developed Gnat [6],
which handles gene names. Further entity classes that exhibit
relatively high term ambiguity with other classes or within
themselves are diseases, drugs, species, and GeneOntology
terms (“Neurofibromatosis 2” can refer to the disease or
gene).

Conflict of Interests
To the authors knowledge, there is no conflict of interest with
name “BioEve” or with any trademarks.

Acknowledgments
The authors like to thank Jeorg Hakenberg, Chintan Patel,
and Sheela P. Kanwar for valuable discussions, ideas, and
help with writing this paper. They also wish to thank the
researchers who provided an initial user review and gave
them valuable feedback.

References
[1] S. Pyysalo, A dependency parsing approach to biomedical text
mining, Ph.D. thesis, 2008.
[2] D. Rebholz-Schuhmann, H. Kirsch, M. Arregui, S. Gaudan, M.
Riethoven, and P. Stoehr, “EBIMed—text crunching to gather
facts for proteins from Medline,” Bioinformatics, vol. 23, no. 2,
pp. e237–e244, 2007.
[3] C. Plake, T. Schiemann, M. Pankalla, J. Hakenberg, and U.
Leser, “ALIBABA: pubMed as a graph,” Bioinformatics, vol. 22,
no. 19, pp. 2444–2445, 2006.
[4] U. Leser and J. Hakenberg, “What makes a gene name? Named
entity recognition in the biomedical literature,” Briefings in
Bioinformatics, vol. 6, no. 4, pp. 357–369, 2005.
[5] H. Xu, J. W. Fan, G. Hripcsak, E. A. Mendonça, M. Markatou,
and C. Friedman, “Gene symbol disambiguation using knowledge-based profiles,” Bioinformatics, vol. 23, no. 8, pp. 1015–
1022, 2007.
[6] J. Hakenberg, C. Plake, R. Leaman, M. Schroeder, and G.
Gonzalez, “Inter-species normalization of gene mentions with
GNAT,” Bioinformatics, vol. 24, no. 16, pp. i126–i132, 2008.
[7] K. Oda, J. D. Kim, T. Ohta et al., “New challenges for
text mining: mapping between text and manually curated
pathways,” BMC Bioinformatics, vol. 9, supplement 3, article
S5, 2008.

11
[8] M. E. Caliﬀ and R. J. Mooney, “Relational learning of patternmatch rules for information extraction,” in Working Notes
of AAAI Spring Symposium on Applying Machine Learning to
Discourse Processing, pp. 6–11, AAAI Press, Menlo Park, Calif,
USA, 1998.
[9] N. Kushmerick, D. S. Weld, and R. B. Doorenbos, “Wrapper
induction for information extraction,” in Proceedings of the
International Joint Conference on Artificial Intelligence (IJCAI
’97), pp. 729–737, 1997.
[10] L. Schubert, “Can we derive general world knowledge from
texts?” in Proceedings of the 2nd International Conference on
Human Language Technology Research, pp. 94–97, Morgan
Kaufmann, San Francisco, Calif, USA, 2002.
[11] M. Friedman and D. S. Weld, “Eﬃciently executing information-gathering plans,” in Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI ’97), pp.
785–791, Nagoya, Japan, 1997.
[12] R. Bunescu, R. Ge, R. J. Kate et al., “Comparative experiments
on learning information extractors for proteins and their
interactions,” Artificial Intelligence in Medicine, vol. 33, no. 2,
pp. 139–155, 2005.
[13] W. Daelemans, S. Buchholz, and J. Veenstra, “Memory-based
shallow parsing,” in Proceedings of the Conference on Natural
Language Learning (CoNLL ’99), vol. 99, pp. 53–60, 1999.
[14] E. Brill, “A simple rule-based part-of-speech tagger. In Proceedings of ANLP-92,” in Proceedings of the 3rd Conference
on Applied Natural Language Processing, pp. 152–155, Trento,
Italy, 1992.
[15] A. Mikheev and S. Finch, “A workbench for finding structure
in texts,” in Proceedings of the Applied Natural Language
Processing (ANLP ’97), Washington, DC, USA, 1997.
[16] M. Craven and J. Kumlien, “Constructing biological knowledge bases by extracting information from text sources,” in
Proceedings of the 7th International Conference on Intelligent
Systems for Molecular Biology, pp. 77–86, AAAI Press, 1999.
[17] K. Seymore, A. McCallum, and R. Rosenfeld, “Learning hidden markov model structure for information extraction,” in
Proceedings of the AAAI Workshop on Machine Learning for
Information Extraction, 1999.
[18] L. Hunter, Z. Lu, J. Firby et al., “OpenDMAP: an open source,
ontology-driven concept analysis engine, with applications
to capturing knowledge regarding protein transport, protein
interactions and cell-type-specific gene expression,” BMC
Bioinformatics, vol. 9, article 78, 2008.
[19] T. Ono, H. Hishigaki, A. Tanigami, and T. Takagi, “Automated
extraction of information on protein-protein interactions
from the biological literature,” Bioinformatics, vol. 17, no. 2,
pp. 155–161, 2001.
[20] C. Blaschke, M. A. Andrade, C. Ouzounis, and A. Valencia,
“Automatic extraction of biological information from scientific text: protein-protein interactions,” AAAI, pp. 60–67.
[21] C. Friedman, P. Kra, H. Yu, M. Krauthammer, and A. Rzhetsky,
“GENIES: a natural-language processing system for the extraction of molecular pathways from journal articles,” Bioinformatics, vol. 17, no. 1, pp. S74–S82, 2001.
[22] A. Rzhetsky, I. Iossifov, T. Koike et al., “GeneWays: a system for
extracting, analyzing, visualizing, and integrating molecular
pathway data,” Journal of Biomedical Informatics, vol. 37, no.
1, pp. 43–53, 2004.
[23] D. P. A. Corney, B. F. Buxton, W. B. Langdon, and D. T. Jones,
“BioRAT: extracting biological information from full-length
papers,” Bioinformatics, vol. 20, no. 17, pp. 3206–3213, 2004.

12
[24] G. Leroy, H. Chen, and J. D. Martinez, “A shallow parser based
on closed-class words to capture relations in biomedical text,”
Journal of Biomedical Informatics, vol. 36, no. 3, pp. 145–158,
2003.
[25] M. Krallinger, F. Leitner, C. Rodriguez-Penagos, and A.
Valencia, “Overview of the protein-protein interaction annotation extraction task of BioCreative II,” Genome Biology, vol.
9, no. 2, article S4, 2008.
[26] J. D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii, “Overview
of BioNLP’09 shared task on event extraction,” in Proceedings
of the Workshop Companion Volume for Shared Task (BioNLP
’09), pp. 1–9, Association for Computational Linguistics,
Boulder, Colo, USA, 2009.
[27] A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii, “Event extraction from biomedical papers using a full parser,” Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing,
pp. 408–419, 2001.
[28] J. Ding, D. Berleant, J. Xu, and A. W. Fulmer, “Extracting Biochemical Interactions from MEDLINE Using a Link Grammar
Parser,” in Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence, pp. 467–471,
November 2003.
[29] H. Cunningham, Information Extraction, Automatic, Encyclopedia of Language and Linguistics, 2nd edition, 2005.
[30] O. Etzioni, M. Cafarella, D. Downey et al., “Methods for
domain-independent information extraction from the web: an
experimental comparison,” in Proceedings of the 19th National
Conference on Artificial Intelligence (AAAI ’04), pp. 391–398,
AAAI Press, Menlo Park, Calif, USA, July 2004.
[31] M. Cafarella, D. Downey, S. Soderland, and O. Etzioni,
“KnowItNow: fast, scalable information extraction from the
web,” in Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pp. 563–570, Association for
Computational Linguistics, Morristown, NJ, USA, 2005.
[32] O. Etzioni, M. Banko, S. Soderland, and D. S. Weld, “Open
information extraction from the web,” Communications of the
ACM, vol. 51, no. 12, pp. 68–74, 2008.
[33] M. Cafarella and O. Etzioni, “A search engine for natural
language applications,” in Proceedings of the International
Conference on World Wide Web (WWW ’05), pp. 442–452,
ACM, New York, NY, USA, 2005.
[34] R. White, B. Kules, and S. Drucker, “Supporting exploratory
search, introduction, special issue, communications of the
ACM,” Communications of the ACM, vol. 49, no. 4, pp. 36–39,
2006.
[35] W. T. Fu, T. G. Kannampallil, and R. Kang, “Facilitating
exploratory search by model-based navigational cues,” in
Proceedings of the 14th ACM International Conference on
Intelligent User Interfaces (IUI ’10), pp. 199–208, ACM, New
York, NY, USA, February 2010.
[36] J. Koren, Y. Zhang, and X. Liu, “Personalized interactive
faceted search,” in Proceedings of the 17th International Conference on World Wide Web (WWW ’08), pp. 477–485, ACM,
April 2008.
[37] V. Sinha and D. R. Karger, “Magnet: supporting navigation
in semistructured data environments,” in Proceedings of the
ACM SIGMOD International Conference on Management of
Data (SIGMOD ’05), pp. 97–106, ACM, June 2005.
[38] M. Hearst, “Design recommendations for hierarchical faceted
search interfaces,” in Proceedings of the ACM Workshop on
Faceted Search (SIGIR ’06), 2006.
[39] S. Stamou and L. Kozanidis, “Towards faceted search for
named entity queries,” Advances in Web and Network Technologies, and Information Management, vol. 5731, pp. 100–112,
2009.

Advances in Bioinformatics
[40] D. Tunkelang, Faceted Search, Morgan & Claypool, 2009.
[41] B. Settles, “ABNER: an open source tool for automatically
tagging genes, proteins and other entity names in text,”
Bioinformatics, vol. 21, no. 14, pp. 3191–3192, 2005.
[42] J. Laﬀerty and F. Pereira, “Conditional random fields: probabilistic models for segmenting and labeling sequence data,”
in Proceedings of the 18th International Conference on Machine
Learning (ICML ’01), 2001.
[43] A. Doms and M. Schroeder, “GoPubMed: exploring PubMed
with the gene ontology,” Nucleic Acids Research, vol. 33, no. 2,
pp. W783–W786, 2005.

International Journal of

Peptides

BioMed
Research International
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Advances in

Stem Cells
International
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Virolog y
Hindawi Publishing Corporation
http://www.hindawi.com

International Journal of

Genomics

Volume 2014

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Journal of

Nucleic Acids

Zoology

 International Journal of

Hindawi Publishing Corporation
http://www.hindawi.com

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Volume 2014

Submit your manuscripts at
http://www.hindawi.com
The Scientific
World Journal

Journal of

Signal Transduction
Hindawi Publishing Corporation
http://www.hindawi.com

Genetics
Research International
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Anatomy
Research International
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Enzyme
Research

Archaea
Hindawi Publishing Corporation
http://www.hindawi.com

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Volume 2014

Hindawi Publishing Corporation
http://www.hindawi.com

Biochemistry
Research International

International Journal of

Microbiology
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

International Journal of

Evolutionary Biology
Volume 2014

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Molecular Biology
International
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Advances in

Bioinformatics
Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Journal of

Marine Biology
Volume 2014

Hindawi Publishing Corporation
http://www.hindawi.com

Volume 2014

Semantic Classification and Dependency Parsing enabled
Automated Bio-Molecular Event Extraction from Text
∗

Syed Toufeeq Ahmed , Radhika Nair, Chintan Patel, Sheela P. Kanwar,
Jörg Hakenberg, and Hasan Davulcu
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287, USA

ABSTRACT

Text mining systems have been devised over the last decade to extract information relevant to various fields in the biomedical domain from scientific literature. A large arsenal is covered in widely
used citations indexes such as PubMed and also Google Scholar.
PubMed currently contains around 20 million citations, more than
9 million with abstracts, and over 2.5 million linked to open-access
full-text articles. Text mining aims to extract various kinds of information from these sources and ultimately bring them into a structured and thus queryable form. Fundel and Zimmer [4] estimated
in 2007 that PubMed abstracts since 1990 contained 150,000 PPIs
(including duplicates). Most of these PPIs are results from smallscale and thus reliable experiments; in contrast, expertly curated
databases draw most of their data from high-throughput screens:
more than 70% of IntAct [3] are large-scale data; about 50,000 are
from small-scale experiments. Thus, PubMed abstracts can be a
good starting point to gather more small-scale data, once they become accessible to curators; more data can certainly be found in
the full text articles.

The last two decades of invigorating research in the area of human genome sequencing marked the beginning of large-scale data
collection. Much of the valuable knowledge gained is found in
published articles, and thus in un-structured textual form. To aid
in searching and extracting knowledge from textual sources, we
present BioEve, a fully automated system to extract bio-molecular
events from Medline abstracts. BioEve first semantically classifies
each sentence to the class type of the event mentioned in the sentence, and then using high coverage, class-specific, hand-crafted
rules, it extracts the participants of that event. An online version of
BioEve is available at http://bioeve.org/.

Categories and Subject Descriptors
J.3 [Life and Medical Sciences]: Biology and genetics; I.2.7 [Artificial Intelligence]: Natural Language Processing; I.5.2 [Design
Methodology]: Classifier design and evaluation

Keywords

One of the first steps in biomedical text mining is to recognize
named entities occurring in a text, such as genes and diseases.
Named entity recognition (NER) is helpful to identify relevant documents, index a document collection, and facilitate information retrieval (IR) and semantic searches. A step on top of NER is to
normalize each entity to a base form (also called grounding and
identification); the base form often is an identifier from an existing,
relevant database: protein names can be mapped to UniProt IDs.
Entity normalization (EN) is required to get rid of ambiguities such
as homonyms, and map synonyms to one and the same concept.
This further alleviates the tasks indexing, IR, and search. Once
named entities have been identified, systems aim to extract relationships between them from textual evidences; in the biomedical domain, these include gene–disease associations and protein–protein
interactions. Relations can then be made available in RDBMS or
used for constructing particular pathways and entire networks [7].

Bio-Molecular Event Extraction, Information Extraction, Semantic
Classification, Dependency Parsing

1.

INTRODUCTION

Human genome sequencing marked the beginning of the era of
large-scale genomics and proteomics, leading to large quantities
of information on sequences, interactions, and their annotations.
Most of this information is contained in the -omics literature, in the
form of semi-structured text. Expert database curators, such as for
protein-protein interaction databases, try to keep up with this vast
amount of data and bring it into structured form to allow querying and data integration. Despite policies that include, for instance,
journal shadowing to include data immediately after publication,
or direct author submissions, legacy data from millions of relevant
articles published since the 1960s need to be curated, demanding
huge efforts.

To compare text mining methods, benchmark sets have been created that cover a variety of tasks. Apart from protein–protein interactions [6], the extraction of molecular events have been addressed
recently. The BioNLP’09 Shared Task [5], which provided the
training and test data that we use in this paper, consisted of three
tasks to detect events from nine different classes (gene expression,
binding, phosphorylation, regulation, and so on; see Table 1; recognizing event participants and arguments; and detection of negation
and speculation concerning such events. A molecular event could
be described as a change in the state of a molecule or molecules.
An example of an event is shown in Figure 1. Each event in the
BioNLP’09 Shared Task data consists of an event trigger term, in-

∗Corresponding author is Syed Toufeeq Ahmed and his email address is Toufeeq@asu.edu
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first page.
To copy otherwise, to republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee.
ACM-BCB 2010, Niagara Falls, NY, USA
Copyright © 2010 ACM ISBN 978-1-4503-0438-2... $10.00

370

IL-10 can directly inhibit|B-Negative_regulation STATdependent early response gene|B-Gene_expression
expression|I-Gene_expression induced|B-Positive_regulation ..

dicating the event class (but ambiguities exist, as some terms can
refer to multiple event classes, and some mentions of a term do not
refer to a relevant event), and a theme involved in the event (which
is a protein in the example shown, but can be another event as well,
such as in “regulation of gene expression”). In the remainder of this
paper, we denote terms and phrases referring to trigger terms and
events in quotation marks (“phoshorylation”), and to event classes
in italics (Phosphorylation).

Figure 3: CRF Training Instance
applied along with information about the context in which selected
trigger phrases may result in false positives. An example of a training instance is shown in Figure 3, using the BIO format for tagging: a tag gets the prefix B- when it belongs to the first token in
a phrase (Begin), and the prefix I- when it belongs to any other token Inside the phrase. For named phrases with only a single token,
the format is <phrase>|B-<class-label>. In the example,
the trigger phrase “gene expression” is a multi-word phrase with
label Gene_expression; the tag for the token “gene” gets the prefix
B-, and the tag for “expression” the prefix I-. The training data labeling is based on the dictionary created using BioNLP’09 Shared
Task data. Template matching was applied further to get event trigger phrase annotations.

Figure 1: Example describing a phosphorylation event.
In BioEve, each biomedical abstracts gets first split into sentences,
before being sent to our sentence classifier. To help our event extraction module, each sentence is then semantically labeled with
event class types; see Section 2. These labeled sentences are parsed
using a dependency parser to identify argument–predicate roles;
see Section 3. For each event class type, we hand crafted high coverage extraction rules to identify all event participants, where the
rules were similar to the ones applied by Fundel and Zimmer for
protein-protein interactions [4].

2.

2.2

SENTENCE LEVEL CLASSIFICATION
AND SEMANTIC LABELING

A first step towards bio-event extraction is to identify phrases in
biomedical text which indicate the presence of an event. The labeled phrases are classified further in to nine event types, see Table 1. The aim of marking such interesting phrases is to avoid looking at the entire text to find participants. Full parsing of biomedical
literature would be very expensive especially for large volumes of
text. We intend to mark phrases in biomedical text, which could
contain a potential event, to serve as a starting point for extraction
of event participants.

2.1

3.

EVENT EXTRACTION USING DEPENDENCY PARSING

The sentences, after being class labeled and tagged, are parsed using a dependency parser (the Stanford lexicalized parser1 ) to identify argument-predicate roles. Words in the sentence and the relationships between these words form the dependency parse tree of
the sentence. One problem encountered during initial testing stages
was due to the gene and protein names. These names are not a part
of the standard English dictionary and as a result, the dependency
parses of the sentences gives unexpected results. To remedy the
situation, each mention is substituted by a unique identifier. For
example, PU.1 would be substituted by T7, depending on its occurrence in the text. The annotations are not part of the standard English dictionary either, but they do not cause the dependency parser
to parse the sentence incorrectly and also, searching for them in the
dependency tree can be simplified by simple regular expressions.

Labeling Event Type Phrases

Not just multiple event types present in a sentence, it would be more
useful if we can identity their location in the text, that is phrases
that mention the event type. In this approach, we considered event
trigger phrase classification as a sequence segmentation problem,
where each word is a token in a sequence to be assigned a label [8].
Taking context in to consideration was important while labeling
event type phrases. Figure 2 gives examples of Transcription and
Phosphorylation events.
· · · leading to NF-kappaB nuclear translocation and
transcription of E-selectin and IL-8 · · ·
· · · . Ligation of CD3 also induces the tyrosine
phosphorylation of HS1 · · ·

For our system, we used typed-dependency representation output
format from Stanford parser which is a simple tuple, reln(gov, dep),
where reln is the dependency relation, gov is the governor word and
dep is the dependent word. Consider the following example:

Figure 2: Example event phrases and context around them.

2.1.1

Bio-Entities and Trigger Phrases

To identify candidate participants of a bio-molecular event, we utilized ABNER [8] to label different entity types including protein
names, DNA, RNA, cell line and cell types. Abstracts also contains
other entities such as drug and chemical names, which could also
participate in an event. We used OSCAR3 (Open Source Chemistry
Analysis Routines) to identify chemical names and chemical structures. The events are annotated with the event trigger phrase which
indicates presence of an event. Event annotation records an event
based on trigger phrase annotation ID and IDs of the participants
involve it; reason being same trigger word could be associated with
same or different entities at different locations in the text. Each of
these events needs to be captured uniquely.

Conditional Random Fields Based Classifier

Conditional Random fields (CRFs) are undirected statistical graphical models, a special case of which is a linear chain that corresponds to a conditionally trained finite-state machine. CRFs in particular have been shown to be particularly useful in named entity
recognition [8]. For training the CRF classifier, the trigger for the
event type-mapping dictionary (explained in next sub-section) was

We investigated whether PU.1 binds and activates the
M-CSF receptor promoter.
After this sentence is labeled with event class type and entities:
1

371

Stanford Parser: http://nlp.stanford.edu/software/lex-parser.shtml

We investigated whether T7 binds/ BINDING and
activates/ POSITIVE_REGULATION the T8 promoter.

converted to its dependency tree, and the rule matcher then works
on the dependency tree to extract an event and its participants.

The tagged sentence is parsed to obtain dependency relations:

3.2

nsubj(investigated-2, We-1)
complm(binds-5, whether-3)
nsubj(binds-5, T7-4)
ccomp(investigated-2, binds-5)
conj_and(binds-5, activates-7)
det(promoter-10, the-8)
nn(promoter-10, T8-9)
dobj(binds-5, promoter-10)

4.

EXPERIMENTS AND EVALUATION

BioEve is designed for large scale extraction of molecular events
from biomedical texts. To assess its performance, we evaluated the
underlying components on the GENIA event data made available as
part of BioNLP’09 Shared Task [5]. This data consists of three different sets: the training set consists of 800 PubMed abstracts (with
7,499 sentences), the development set has 150 abstracts (1,450 sentences), and the test set has 260 abstracts (2,447 sentences). We use
the development set for parameter optimization and fine tuning, and
evaluate the final system on the test set; this is the same scenario
used to assess all participating systems.

Figure 4: Dependency Parse tree, and event binding and its participants are shown.
This sentence mentions two separate events, Binding and Positive
regulation. Let us consider extracting the Binding event and its participants. Figure 4 shows the parse tree representation and the part
of the tree that needs to be identified for extracting event Binding.
The rule matcher starts from the root node of the dependency parse
tree of the sentence. The module traverses the tree in a breadth-first
fashion, looking for event trigger words. On finding a trigger word,
it marks the node in the tree and activates the rule matcher for the
corresponding event class on that node. The matcher searches the
tree for participants of the event and on finding them successfully,
creates a record in the result set corresponding to the event.

4.1

Evaluation on BioNLP Shared Task data

We participated in the BioNLP’09 Shared Task 1 [5] with a previous version of BioEve [2]. Task 1 aims at the extraction of events
concerning given genes and proteins; it involved event trigger detection (e.g., a keyword that hints on the event), event classification
(into one of nine event classs), and primary argument recognition
(i.e., a gene or protein; sometimes another event). BioEve participation results on BioNLP’09 data (Task 1, classification and event
recognition) are shown in Table 1.
We have developed our system further (see previous two sections)
to improve BioEve’s performance over our results during the Shared
Task evaluation. Table 1 also contains our recent results. The performance micro-average over all nine event classes increased from
20.7% to 27.5% in f-score (recall: +3.5%; precision: +9.8%). For
individual classes, the observed increase was between 5% (such as
for Localization events) and 32% in f-score (for Protein catabolism
events). Phosphorylation is the only event class for which the current system performs worse (f-score -9.3%; balanced loss in precision and recall) than the previous one.

In Figure 4, “binds” is a trigger word for a Binding event. The
extraction module fires a signal on detecting its corresponding node
in the parse tree. It then marks the node and loads the rule matcher
for Binding events. This matcher searches for the participants of
the Binding event as per the rules created for it. It finds T7 and T8
in the tree and reports them back. This results in the creation of
a binding event, with the trigger word “binds” and participants T7
and T8 dereferenced to “PU.1” and “M-CSF receptor”.

3.1

Two-Fold Extraction for Nested Events

Nested events mentioned in complex sentences have other events
or their products as their participants, apart from entities. These
kind of events are more difficult to detect and extract. An example
of a nested event is “However, LPS stimulated VCAM-1 expression
in HUAECs.” The trigger words in the sentence are “stimulated”,
denoting a Positive regulation and “expression”, denoting a Gene
expression. The Gene expression event is catalyzed by the Positive regulation event. Nested events occur as participants for most
regulation events. This caused the extraction to give lower recall
and even lesser precision numbers. To aid this situation, two-fold
extraction was used. The precedence order of events is essentially
kept the same as single-fold extraction. The difference is that the
extraction is done using two passes over each parse tree, the first
pass for non-regulation events and the second pass exclusively for
regulation events. At the end of the first pass, we replace event
trigger terms with event annotations, so that they can be picked as
themes in the second pass, if a rule matches.

Single-Fold Extraction

For each event class type we crafted class-specific rules (for details,
see [2]), taking the theme of the event, the number of participants,
and their interactions into consideration. By knowing the class type
from the classification process, the job of crafting rules becomes
manageable, facilitating less complex and high coverage rules, as
shown in RelEx [4] and IntEx [1]. All the abstracts are iterated
once, their text is split into constituent sentences, each sentence is

Overall, performance for extracting event of the classes Gene expression, Protein catabolism, and Phosphorylation is higher than
for the classes Transcription, Regulation, Positive regulation, and
Negative regulation. We attribute this discrepancy in large part to
misclassification of detected events: most of the Positive and Negative regulation events were predicted as either Phosphorylation or
Gene expression events.

372

Event Class
Localization
Binding
Gene expression
Transcription
Protein catabolism
Phosphorylation
EVT-TOTAL
Regulation
Positive regulation
Negative regulation
REG-TOTAL
ALL-TOTAL

Official F1
30.91
22.35
42.36
12.59
37.21
59.50
36.06
8.13
9.48
11.63
9.66
20.74

Current F1
36.29
28.36
49.43
28.90
69.23
50.22
41.08
15.08
15.90
19.77
16.64
27.47

5.

Table 1: Evaluation on BioNLP’09 Shared Task 1 using approximate span matching; balanced f-scores in %. Official F1
refers to the balanced f-score achieved during the task; current F1 includes the improvements described in this paper.
Events are grouped by ‘simple’ (EVT) and ‘complex’ regulatory (REG) events. Total numbers refer to micro-averaged F1.

4.2

We evaluated the BioEve system on BioNLP’09 Shared Task data.
We showed significantly improved f-score of our classification and
labeling module (+24%), due to using a Conditional Random Fieldsbased classifier instead of näive Bayes [2]. We were able to improve f-score of the event participant extraction module by 14%
using the two-fold parse tree traversal and rule matching. Both improvements lead to an increase of the BioEve system performance
by 7%. Even after these significant improvements in classification and extraction tasks, there is still some room for further improvements. We want to look into particular event types Transcription, Regulation and Positive regulation to see if we can bring in
more discriminative features based on context, such as presence of
a protein or gene name or DNA/RNA types; all four are currently
grouped together into one entity category. Whereas to improve the
extraction module, we want to further improve our extraction rules
especially for Regulation and Binding event types. An online version of BioEve is available at http://bioeve.org/.

Evaluation of Event Extraction

The extraction module is evaluated using the training data, which
has all the entities annotated. Table 2 shows the comparison between single- and two-fold extraction. Event extraction for the
classes Gene expression, Protein catabolism and Phosphorylation
performed better compared to Transcription, Regulation, Positive
and Negative regulation. The reason noticed (in training examples)
was that, most of the true example sentences of positive regulation or Negative regulation class type were misclassified as either
Phosphorylation or Gene expression. Improvement in the classification of the semantic labels might help improve the extraction
results. On the extraction side, the rules used by the system were
simple considering the language versatility. Nested events were responsible for the relatively poor numbers for the regulation events.
Significant improvement was obtained due to two-fold extraction.
The numbers for non-regulation events remained relatively constant, whereas the regulation events showed a large improvement.

6.

Localization
Binding
Gene expression
Transcription
Protein catabolism
Phosphorylation

Single-fold F1

Two-fold F1

70.93
54.24
72.53
71.91
76.38
80.00

77.15
57.48
74.75
75.39
81.16
80.40

EVT-TOTAL

68.53

71.47

Regulation
Positive regulation
Negative regulation

17.34
23.45
18.01

42.24
45.27
43.39

ALL-TOTAL

42.05

56.33

REFERENCES

[1] S. T. Ahmed, H. Davulcu, and C. Baral. Extracting
Protein-Protein Interactions from MEDLINE Using Syntactic
Roles. In IEEE Int Conf on Bioinformatics and Biomedicine
(BIBM), pages 473–476, 2008.
[2] S. T. Ahmed, R. Nair, C. Patel, and H. Davulcu. BioEve:
Bio-Molecular Event Extraction from Text Using Semantic
Classification and Dependency Parsing. In BioNLP 2009,
pages 99–102, 2009.
[3] B. Aranda, P. Achuthan, Y. Alam-Faruque, I. Armean,
A. Bridge, and et al. The IntAct molecular interaction
database in 2010. Nucl. Acids Res., 38:D525–D531, 2010.
[4] K. Fundel, R. Küffner, and R. Zimmer. RelEx—Relation
extraction using dependency parse trees. Bioinformatics,
23(3):365–371, 2007.
[5] J.-D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
Overview of BioNLP’09 Shared Task on Event Extraction. In
Proc BioNLP 2009 Workshop Companion Volume for Shared
Task, pages 1–9, Boulder, Colorado, June 2009. Association
for Computational Linguistics.
[6] M. Krallinger, A. Morgan, L. Smith, F. Leitner, L. Tanabe,
J. Wilbur, L. Hirschman, and A. Valencia. Evaluation of
text-mining systems for biology: overview of the Second
BioCreative community challenge. Gen Biol, 9(S2):S1, 2008.
[7] K. Oda, J.-D. Kim, T. Ohta, D. Okanohara, T. Matsuzaki,
Y. Tateisi, and J. Tsujii. New challenges for text mining:
mapping between text and manually curated pathways. BMC
Bioinformatics, 9 Suppl 3:S5, 2008.
[8] B. Settles. ABNER: an open source tool for automatically
tagging genes, proteins and other entity names in text, 2005.

Using the two-fold extraction component in BioEve led to an increase in recall by 13% and precision by 15.5% over single-fold
passes. This improvement was largely due to better performance on
the complex regulatory event classes (f-score +25% in total), performance for the simple event classes improved by 3% in f-score,
see Table 2.
Event class

CONCLUSIONS

In this paper, we presented a fully automated system to extract biomolecular events from bio-medical abstracts. These events cover
gene expression, transcription, binding, protein catabolism, protein
localization, and regulatory events. By semantically classifying
each sentence to the class type of the event, and then using highcoverage rules, BioEve extracts the participants of that event. To
extract event participants we use dependency parse information to
generate a parse tree; we are able to extract participants by applying hand-crafted high-coverage rules. The BioEve extraction module can extract multiple events from sentence containing multiple
but disjoint events, or nested multiple events from a single sentence
using a two-pass algorithm.

Table 2: Single-fold and two-fold event extraction results.

373

Automating the Biological Data Collection Process with Agents
Zoé Lacroix, Kaushal Parekh, Hasan Davulcu
Arizona State University
{HasanDavulcu, zoe.lacroix,kaushal}@asu.edu
Abstract
Scientists spend significant amount of time
accessing Web resources, extracting information
of interest, filtering, and integrating relevant
data from multiple heterogeneous Web sites to
support their data collection needs. This tedious
collection process is typically performed
manually as available technology does not allow
scientists to explore and control their data
collection process step by step. However, most of
the process can be automated. While scripts
(e.g., Perl) may be written to retrieve, parse and
extract data of interest, many scientists are not
programmers and do not have IT support. In
contrast we propose an approach based on
Personal Information Agents (PIA) that provide
scientists a user-friendly mechanism to automate
their data collection processes without the need
of any programming. This approach is currently
being evaluated at the Brain Tumor Cancer Unit
of the Translational Genomics Research Institute
(TGen), Phoenix, Arizona.

I.V. Ramakrishnan, Nikeeta Julasana
State University of New York
{ram, Nikeeta}@cs.sunysb.edu
variables. Here the user wishes to use the agent
later with other keywords (thus ‘term’ is selected
as a variable), but always starting from the data
source OMIM (‘db’ is assigned the constant
value ‘OMIM’). Clicking on ‘Submit’ takes the
user to the OMIM results page.

Figure 1. Setting form element types
The user then selects a sample of OMIM records,
to show the region of interest from which data is
to be extracted (Fig. 2) and clicks on ‘Get
Region’. Next he highlights a single record and
presses the ‘Get Item’ button to show the agent
what types of items are to be retrieved from the
region.

1. Introduction
We illustrate our approach retrieving citations
related to genetic diseases. The task is to perform
a keyword search on OMIM at the NCBI Web
site, follow the links for each gene record to get
to the detailed description and then extract
PubMed citations from the references. For a
video
demonstration,
please
visit
http://bioinformatics.eas.asu.edu/winagent.htm.
After installing the WinAgent toolbar, the
first step is to visit the NCBI Web site
(www.ncbi.nlm.nih.gov) and press the ‘Record’
button1. Next the user searches OMIM for a
disease condition (say ‘brain tumor’) and clicks
on ‘Go’ to begin the search. A form processing
wizard pops up (Fig. 1) and the user has to set
the type of the form entries as constants or

Figure 2. Region selection
The agent shows all the records broken up into
their attributes (Fig. 3). One has to select and
rename only the desired attributes in each record
and drop the rest. To follow hyperlinks (e.g.,
OMIMid) to the next level, the ‘Follow Link’
option for that attribute has to be selected.

1

WinAgent monitors and records the user’s actions
during the period between ‘Record’ and ‘Stop’ events.

Proceedings of the 2004 IEEE Computational Systems Bioinformatics Conference (CSB 2004)
0-7695-2194-0/04 $20.00 © 2004 IEEE

extraction protocol and can ‘Stop’. It is then
saved in a file with an ‘.agt’ extension.
Agents can be run using the ‘Play’ button. The
user selects the input file containing the
previously saved agent and creates a new XML
file for saving the results. The agent checks if it
has to fill any forms with variable elements
while navigating. If so, it will ask the user for the
values of those variables (e.g., the name of the
disease condition). Automatic browsing begins
on pressing ‘Submit’. When the agent has visited
the appropriate data sources and extracted
information, the collected data is saved in XML
format. The same procedure can be repeated for
different keywords. This can also be done
automatically by specifying an input text file
containing disease names from which WinAgent
can retrieve keywords. The results for all these
keywords are stored in a single output file.

Figure 3. Selecting attributes of interest

2. Conclusions

Figure 4. Confirm results of selection
On pressing ‘Submit’, WinAgent shows
results (Fig. 4) of these selections
confirmation. Clicking on ‘Close’ accepts
results. One can also ‘Cancel’ and repeat
process again if the results are not as desired.

the
for
the
the

WinAgent [1] works like a recording machine of
the data collection protocol. It sits as a small
toolbar in the Internet Explorer having intuitive
buttons like record, play, stop, etc. Once the
scientist has designed the data collection
protocol, selecting the various resources,
identifying relevant information from each
retrieved page and the navigation process (e.g.,
selection of hyperlinks to go from one page to
the other), he launches the WinAgent that
records step by step the overall process. When all
steps are recorded, an agent will be automatically
generated. The agent can be used to collect data
automatically following step by step the data
collection protocol, and store all retrieved data in
an XML document or a database for further data
transformation, integration and annotation.
3.

Acknowledgements

We greatly appreciate the cooperation of
Michael Berens and the Brain Tumor Cancer
Unit at TGen.
Figure 5. Selecting references

4. References
The next step is to navigate a level deeper by
following the links from OMIM identifiers to
retrieve PubMed identifiers from the references
of each OMIM record. This is done by a similar
procedure of ‘Get Region’ on the references
(Fig. 5), ‘Get Item’ on an individual PubMed Id
and then making appropriate attribute selection.
The agent has now been shown the data

[1] N. Julasana, A. Khandelwal, A. Lolage, P. Singh,
P. Vasudevan, H. Davulcu and I. V. Ramakrishnan,
“WinAgent: A System for Creating and Executing
Personal Information Assistants Using a Web
Browser”, 9th International Conference on Intelligent
User Interfaces, Madeira, Portugal, 2004, pp 356-357.

Proceedings of the 2004 IEEE Computational Systems Bioinformatics Conference (CSB 2004)
0-7695-2194-0/04 $20.00 © 2004 IEEE

arXiv:1608.01771v1 [cs.SI] 5 Aug 2016

Community Detection in Political Twitter Networks
using Nonnegative Matrix Factorization Methods
Mert Ozer

Nyunsu Kim

Hasan Davulcu

School of Computing, Informatics,
Decision Systems Engineering
Arizona State University
Tempe, USA 85287
mozer@asu.edu

School of Computing, Informatics,
Decision Systems Engineering
Arizona State University
Tempe, USA 85287
nkim30@asu.edu

School of Computing, Informatics,
Decision Systems Engineering
Arizona State University
Tempe, USA 85287
hdavulcu@asu.edu

Abstract—Community detection is a fundamental task in social
network analysis. In this paper, first we develop an endorsement
filtered user connectivity network by utilizing Heider’s structural
balance theory and certain Twitter triad patterns. Next, we
develop three Nonnegative Matrix Factorization frameworks to
investigate the contributions of different types of user connectivity
and content information in community detection. We show that
user content and endorsement filtered connectivity information
are complementary to each other in clustering politically motivated users into pure political communities. Word usage is
the strongest indicator of users’ political orientation among all
content categories. Incorporating user-word matrix and word
similarity regularizer provides the missing link in connectivityonly methods which suffer from detection of artificially large
number of clusters for Twitter networks.

I. I NTRODUCTION
Twitter has become one of the main stages of political
activity both among politicians and partisan crowds. We have
seen huge political mobilizations over Twitter in recent uprisings such as the Arab Spring and the Gezi protests [1].
Since then, politicians have been engaging in using Twitter to
attract supporters and people have been using it to express their
political views and opinions on various leaders and issues.
Community detection is a fundamental task in social network analysis [2]. A community [3] can be defined as a group
of users that (1) interact with each other more frequently than
with those outside the group and (2) are more similar to each
other than to those outside the group. Utilizing community detection algorithms to detect online political camps has attracted
many researchers [4], [5], [6]. In this work, we propose three
nonnegative matrix factorization frameworks to exploit both
user connectivity and content information in Twitter to find
ideologically pure communities in terms of their members’
political orientations.
Twitter presents three types of connectivity information
between users: follow, retweet and user mention. In this paper,
we do not use follow information since follow relationships
correspond to longer-term structural bonds [10] and it remains
challenging to determine if a follow relationship between a
pair of users indicate political support or opposition. Furthermore, it has been observed that neither user retweets nor user
mentions always indicate endorsement in Twitter [7]. However

in the political sub-domain of Twitter, it has been shown that
retweets tend to happen between like-minded users rather than
between members of opposing camps [8].
Using both connectivity and content information for community detection in social networks has been a popular approach among many researchers’ prior works [4], [5], [6], [3].
In [4], Tang et al. propose a general framework for integrating
multiple heterogenous data sources for community detection.
Tang’s work does not pay attention to identifying the endorsement subgraph of the connectivity graph. In [5] Sachan et al.
propose an LDA-like social interaction model by representing
user connectivity as a document alongside message content.
This approach also does not discriminate between positive
or negative user engagement. In [6], Ruan et al. propose to
use a filtered graph to eliminate ambiguous interactions by
checking content similarity in the user’s neighborhood. In
this formulation, only local content patterns are taken into
consideration whereas in our formulations we incorporate the
global content patterns into our optimization framework.
The contributions of this paper can be summarized as
follows:
•

•

We start with retweets without edits as indicators of
positive endorsements between users and utilize Heider’s
P-O-X triad balance theory [11] to incorporate selected
”structurally balanced” edited retweets and user mentions
into a weighted undirected connectivity graph as additional indicators of positive endorsements.
We develop algorithms which incorporate users’ content
information in our community detection frameworks to
overcome the sparse nature of Twitter connectivity networks. We break down Twitter message content into
three categories; words, hashtags and urls, and design
experiments to measure the performance contributions
of each category. Proposed Nonnegative Matrix Factorization (NMF) algorithms use user-word, user-hashtag
and user-domain frequency matrices to be factorized into
lower rank user vector representations while regularizing
over user connectivity and content similarity to map users
into their respective communities.

Pei et al. in [3] also model the problem as nonnegative

matrix tri-factorization problem which factorizes user-word,
tweet-word and user-user matrices into lower rank representations of users and tweets while regularizing it with user interaction and message similarity matrices. They build user-user
connectivity matrix by utilizing the structural follow relationships which do not capture dynamic political context-sensitive
engagement. They treat all user mentions and retweets identically and without any discrimination for endorsement. Their
framework also lacks word similarity regularization.
We develop and experiment with three nonnegative matrix factorization frameworks: MultiNMF, TriNMF, DualNMF,
which incorporate connectivity alongside different types of
content information as regularizers. After experimenting with
different dimensions of user content and different types of
induced connectivity networks we discovered that incorporating more information does not necessarily yield higher
clustering performance. Highest quality clustering is achieved
through endorsement filtered connectivity based on methods
we develop in Section III alongside user-word matrix based
content regularization. Our DualNMF framework gives purity
scores around 88%, adjusted rand index around 75% and NMI
around 67%. It improves all of the other baseline methods
significantly as presented in Section V and it also improves
over the NMTF framework developed recently by Pei et al.
[3] by 8% in purity, 47% in ARI and by up to 60% in
NMI metrics. Proposed endorsement filtered sub-graph of user
mentions and retweets also improves all baseline methods in
almost all of the experimental setups by up to 109% in NMI,
71% in ARI and 17% in purity.
The rest of the paper is organized as follows. Section
II briefly surveys related work. In Section III, we present
Heider’s theory of P-O-X structural balance of triads and
its application to retweet and mention graphs to identify
endorsement filtered user connectivity networks. In Section IV,
we introduce our three nonnegative matrix factorization frameworks for community detection. In Section V, we present our
experimental design, evaluation metrics and results. Section
VI concludes the paper and discusses future work.
II. R ELATED W ORK
A. Community Detection
Since the introduction of the modularity metric by Newman
in [16], plenty of modularity based community detection
methods have been proposed in the literature [17], [18], [19],
[20]. We employ Blondel et al. [18] and Clauset et al. [19]
works as baseline algorithms to compare with ours due to their
wide popularity among practitioners. A general drawback of
these algorithms, when they are applied to Twitter networks,
is that due to the sparse nature of the connectivity they end
up with an artificially large number of communities.

incorporate Laplacian graph regularization to the standard
NMF algorithm which assumes data points are sampled from
a Euclidian space which is not the case usually for real-world
applications. Gu et al. [31] further incorporate local learning regularization to NMF which assumes that geometrically
neighboring data points are similar to each other, and should be
in the same cluster. For co-clustering purposes Ding et al. [28]
propose nonnegative matrix tri-factorization with orthogonality
constraints. Shang et al. introduce graph dual regularized NMF
algorithm in [27] by claiming that not only observed data but
also features lie on a manifold. Yao et al. [24] apply the same
logic for collaborative filtering domain and propose a dual
regularized one-class collaborative filtering method.
III. T HE S TRUCTURAL BALANCE OF R ETWEET AND
M ENTION G RAPH
Since P-O-X triad balance theory proposed by Heider in
[11], structural balance of signed networks has been studied
extensively. Heider proposed that in a signed triad, only two
combinations of eight possible sign configurations are possible
for a triad to be structurally balanced. Those are the following
cases;
1) three positive edges,
2) one positive and a pair of negative edges.
In other words, there cannot be any structurally balanced triad
having only one negative edge. We adopt this social theory
for the Twitter user connectivity networks, by assuming that
”retweets without edits” imply political endorsement or an
unambiguous positive edge [12]. However, when a retweet is
edited, it has already been shown that [13], it does not necessarily mean endorsement anymore. Moreover, user mentions
do not imply endorsements either. For these reasons, we only
consider retweets without edits as positive edges. For the rest
of the user actions, corresponding to retweets with edits and
users mentions, it is hard to detect positivity or negativity of
the edges.
In certain triad configurations, retweets with edits and user
mentions can be identified as positive edges with the help
of Heider’s triad structural balance (TSB) rules. Since we
do not have unambiguous negative edges, the second case
is not applicable. However, since we have some positive
edges to begin with, we can employ Heider’s first case (i.e.
three positive edges), to infer that in the presence of a triad
with a pair of positive edges, the third edge can also be
labeled as positive. An example configuration with a pair of
positive edges is shown in Figure 1. In this case, TSB rule is
applicable and would allow us to infer that any user mention
or retweet with an edit edge connecting the lower pair of
users in the triad is indeed a positive edge. By employing
this inference mechanism we identify the endorsement filtered
user connectivity network.

B. Nonnegative Matrix Factorization
Nonnegative Matrix Factorization(NMF) algorithms by Lee
et al. [22] and Lin et al. [23] have been extensively used
and extended for different variations of community detection
problems. Cai et al. [29] introduced GNMF algorithm to

IV. P ROPOSED M ETHODS
We propose three methods for clustering politically motivated users in Twitter namely; MultiNMF, TriNMF and

•

•

•

Fig. 1.

An example application of TSB Rule

DualNMF. For MultiNMF method we use document term
representation of user-word, user-hashtag and user-domain matrices to be factorized and regularize the factorization problem
with the user connectivity graph, cosine similarity matrices of
words, domains and hashtag co-occurence matrix. For TriNMF
method we use only user-word and one of user-hashtag or
user-domain matrices and regularize over user connectivity
and cosine domain similarity or hashtag co-occurence matrix.
For DualNMF method we factorize user-word matrix into two
nonnegative lower rank matrices while regularizing it with user
connectivity and cosine word similarity. Before going into the
details of the three algorithms we present notation in Table I. In
TABLE I
N OTATION
Xuw
Xuh

user x word
user x hashtag

Xud

user x domain

R

user x user

M

user x user

∆M

user x user

∆Mw

user x user

C

user x user

Hsim
Dsim
Wsim
α
γ
θ
β
U
H
D
W

hashtag x hashtag
domain x domain
word x word
number
number
number
number
user x cluster
hashtag x cluster
domain x cluster
word x cluster

frequencies of words used by users
frequencies of hashtags used by users
frequencies of distinct domains used
by users
adjacency matrix of
retweet without edit graph
adjacency matrix of
mention and retweet with edit graph
adjacency matrix of mentions
and retweet with edits completing
retweet without edit triads
adjacency matrix of mentions
and retweet with edits completing
retweet without edit triads
weighted by retweet without edit edges
any combination of user connectivity
graphs
hashtag co-occurence matrix
domain similarity matrix
word similarity matrix
user connectivity regularizer parameter
hashtag similarity regularizer parameter
domain similarity regularizer parameter
word similarity regularizer parameter
cluster assignment matrix of users
cluster assignment matrix of hashtags
cluster assignment matrix of domains
cluster assignment matrix of words

this work, instead of using only full user retweet and mention
network we offer three types of user connectivity regularizers
as follows;

R + M: It is the adjacency matrix of the full retweet and
mention graph. If there exists both retweet and mention
edges between two users, weights are summed up.
R + ∆M: It is the adjacency matrix of the union of
retweet and mention graphs in which mention edges and
retweet with edits either complete a missing link in a
triad of retweet without edit or already correspond to a
retweet without edit edge. ∆M can be formally defined
as;
PN
∆M = {(i, j, Mij ) | Rij > 0 ∨ k=1 Rik Rkj > 0}
R + ∆Mw : It is the adjacency matrix of the union of
retweet and mention graphs in which mention edges and
retweet with edits either complete a missing link in a triad
of retweet without edit or already correspond to a retweet
without edit edge. The ones that complete a missing link
in a triad of retweet without edit are weighted by the
multiplication of the weights of two retweet without edit
edges in the triad. ∆Mw can be defined formally as;
∆Mw = {(i, j, Mij (Rij +

PN

k=1

Rik Rkj ))}

For word similarity and domain similarity regularizers
we make use of cosine similarity. It can be formally defined as;
cos(θ) =

vi · vj
k v i k ∗ k vj k

where vi is the user usage frequency vector of ith word or
domain. For hashtag similarity we build similarity matrix by
making use of co-occurences of hashtags in tweets. If two
hashtags occur in the same tweet, we assume that those two
hashtags are similar.
A. MultiNMF with multi regularizers
To incorporate usage of both hashtags and domains of
shared url links by users, we propose an NMF framework
which has the following objective function;
JU,H,D,W =k Xuw − UWT k2F + k Xuh − UHT k2F
+ k Xud − UDT k2F +αT r(UT LC U)
+ γT r(HT LHsim H) + θT r(DT LDsim D)

(1)

T

+ βT r(W LWsim W)
s.t. U ≥ 0, H ≥ 0, D ≥ 0, W ≥ 0
where LC is the Laplacian matrix of adjacency matrix of user
connectivity graph defined as DC − C and DC is the matrix
which contains the degree of each user node in its diagonals.
LHsim , LDsim and LWsim follow the same definition for
hashtags and words. Due to the very fuzzy multi-class nature
of words, hashtags and domain names, we do not include
orthogonality constraints for matrices U, H, D, W, which
usually result in more precise clusters for co-clustering tasks.
It is easy to see that the proposed objective function is not
convex for U, H, D and W, hence we develop an iterative
algorithm which tries to find a local minima by updating each
matrix iteratively as follows;
s
Xuw W + Xuh H + Xud D + αL−
CU
U←U
(2)
UWT W + UHT H + UDT D + αL+
CU

v
u T
u Xuh H + γL−
Hsim H
H←Ht
T
HU U + γL+
Hsim H
v
u T
u Xud D + θL−
Dsim D
D←Dt
T
DU U + θL+
Dsim D
v
u T
u Xuw U + βL−
Wsim W
W ←Wt
T
WU U + βL+
Wsim W
L+
ij

(4)

(5)

L−
ij

B. TriNMF with three regularizers
To incorporate usage of hashtags or domains of shared url
links solely, we propose a new NMF framework which has
the following objective function.
JU,H,W =k Xuw − UWT k2F + k Xuh − UHT k2F
+ αT r(UT LC U) + γT r(HT LHsim H)
+ βT r(WT LWsim W)

To use only user word matrix as user content and regularize
factorization with user connectivity and keyword similarity,
inspired by [24], we present DualNMF objective function as;
JU,W =k Xuw − UWT k2F +αT r(UT LC U)
+ βT r(WT LWsim W)

= (|Lij | − Lij )/2.
[·]
 represents element-wise multiplication and
represents
[·]
element-wise division. Derivation of update rules can be seen
in Appendix A. Complexity of the method can be inferred
as O(i(uwk + uhk + udk + u2 k + h2 k + d2 k + w2 k))
when complexity of multiplying any X matrix with any of
U, H, D, W is considered to be O(uwk), O(uhk), O(udk)
and multiplying any of Laplacian matrices L with any of
U, H, D, W is taken as O(u2 k), O(h2 k), O(d2 k) or O(w2 k)
where i is the number of iterations, u is number of users, h is
the number of hashtags, d is the number of domains, w is the
number of words and k is the number of clusters. The general
algorithmic framework is given at the end of methodology in
Algorithm 1.
where

= (|Lij | + Lij )/2 and

C. DualNMF
(3)

(6)

(10)

s.t. U ≥ 0, W ≥ 0
After following the same procedure introduced in Section
IV-A, we can get the update rules for U and W as;
s
Xuw W + αL−
CU
(11)
U←U
T
UW W + αL+
CU
v
u T
u Xuw U + βL−
Wsim W
(12)
W ←Wt
T
WU U + βL+
Wsim W
Complexity of the method can be inferred as O(i(uwk+u2 k+
w2 k)) after omitting the extra operations done over matrices
Xuh , H and DHsim in the previous method. The general
Algorithm 1 NMF Algorithms
Input:{Xuw , Xuh , Xud , C, Hsim , Dsim , Wsim , α, β, θ, γ}
Output: U
1: Initialize U, H, D, W > 0
2: while ∆residual > threshold do
3:
Update U by using one of Equations 2, 7, 11
4:
Update H by using one of Equations 3, 8
5:
Update D by using Equation 4
6:
Update W by using one of Equations 5, 9, 12
7: end while
8: Assign user i to community j where j = argmaxj Uij .

s.t. U ≥ 0, H ≥ 0, W ≥ 0
where LC is the Laplacian matrix of user connectivity defined
as DC − C and DC is a diagonal matrix which contains the
degree of each user in its diagonals. LHsim and LWsim follows
the same definition for hashtags and words. After applying the
same procedure followed in Section IV-A, we get updating
rules as follows.
s
Xuw W + Xuh H + αL−
CU
(7)
U←U
T
T
UW W + UH H + αL+
CU
v
u T
u Xuh U + γL−
Hsim H
H←Ht
(8)
T
HU U + γL+
Hsim H
v
u T
u Xuw U + βL−
Wsim W
(9)
W ←Wt
T
WU U + βL+
Wsim W
Note that this update rules can be obtained by setting D, Dsim
and θ equal to 0 in Equations 2, 3, 5. Complexity of the method
can be calculated by omitting the costs of operations done over
matrices Xud , D and LDsim . The complexity of the method
is O(i(uwk + uhk + u2 k + h2 k + w2 k)).

algorithm can be summarized as the application of the related
update rules to the matrices U, H, D, W. For MultiNMF with
multi regularizers method, equations 2, 3, 4, 5 are applied.
For TriNMF with three regularizers method, equations 7, 8,
9 are applied and D matrix is not included in calculations.
For DualNMF method, equations 11 and 12 are applied and
H and D matrices are not included in calculations.
V. E XPERIMENTS AND R ESULTS
A. Data Description
We make use of a pair of publicly available1 political Twitter
datasets to evaluate our methods. These datasets are user lists
of 419 British political figures from four major political parties
in the UK, namely; Conservative and Unionist Party, Labour
Party, Scottish National Party, Liberal Democrats and others,
and 349 major Irish political figures from seven political
parties; Fianna Fil, Fine Gael, Green Party, Sinn Fin, United
Left Alliance, Independents. Several statistics for the datasets
are shown in Table II.
1 Users’
Twitter
id
lists
http://mlg.ucd.ie/aggregation/index.html

can

be

obtained

from

TABLE II
DATA ATTRIBUTES

#
#
#
#
#
#
#
#

of
of
of
of
of
of
of
of

Tweets
Retweets
Mentions
Words
Hashtags
URL Domains
Users
Baseline Communities

UK

Ireland

19,947
1,566
4,956
10,766
945
946
233
5

14,656
7,088
22,072
7,973
986
634
258
7

For the UK and Ireland data, we crawl all of the tweets sent
from the accounts of given user id lists. In order not to be
heavily influenced by the extremely polarized election season,
we only used tweets dated after May, 7 2015, which was the
election day in the UK. To balance the share of number of
tweets from each user we limit the number of tweets to 200
per user.
For each dataset, same preprocessing method is followed.
First, words occurring less than 20 times and stop words
are eliminated. After eliminating word features, users and
tweets that lack content are also eliminated. Hashtags and
domains that appear only once are not taken into consideration
either. Statistics shown in Table II show the numbers after
preprocessing.
B. Evaluation Metrics
To evaluate the methods, we make use of three well known
clustering quality metrics, namely; purity, adjusted rand index
and normalized mutual information.
Purity can be formally defined as;
k

P urity =

1X
maxj |Ci ∩ lj |
n i=1

where k is the number of communities found, n is the number
of instances, lj is the set of instances which belong to the class
j, and Ci is the set of instances that are members of community
i.
Adjusted Rand Index [14] can be formally defined as;
ARI =

RI − E[RI]
max(RI) − E[RI]

where
RI =

s + s0

n
2

s is the number of pairs which belong to both same groundtruth class and identified community. s0 is the number of
pairs which belong to both different ground-truth classes and
identified communities. It evaluates the similarity of groundtruth class labels and clustering result.
Normalized Mutual Information can be formally defined
as;
 P (j, i) 
P|l| P|C|
P
(j,
i)log
j=1
i=1
P (i)P (j)
p
NMI =
H(l)H(C)

where, H(l) and H(C) are the entropy of class and community
assignments of l and C. P (j, i) is the probability that randomly
picked user has class label j and belongs to the community i
while P (j) gives the probability of randomly picked user to
be in class j and similarly P (i) to be in community i.
C. Baseline Algorithms
As a baseline to evaluate the performance of using both
connectivity and content information, we design experiments
with connectivity-only and content-only clustering methods.
For connectivity-only method, we use Louvain [18] and
CNM [19] algorithms utilizing modularity optimization over
user adjacency matrix. Modularity is defined as:
ki kj
1 X
(Aij −
)δ(ci , cj )
(13)
Q=
2m ij
2m
where δ(ci , cj ) is the Kronecker delta symbol, ci is the label
of the community to which node i is assigned, and ki is the
degree of node i.
For content-only approach, we experiment with kmeans[21] and conventional non-negative matrix factorization
algorithm [22].
For approaches employing both connectivity and content
information of users, we test GNMF [29] and NMTF [3]
algorithms besides proposed methods. GNMF algorithm is
introduced by Cai et al. to incorporate intrinsic geometric
similarity of users. We feed previously defined three types
of user connectivity graphs’ adjacency matrices as graph
regularization terms to the GNMF algorithm.
Pei et al. work in [3] applies nonnegative matrix trifactorization with regularization to Twitter data. It makes use
of user similarity, [tweet x word] and [user x word] matrices
and regularize the objective function with tweet similarity and
user connectivity matrices. Complexity of the algorithm is
O(rk(mn + mw + nw + m3 + n2 )) where r is the iteration
times. m, n, k, and w denote the number of users, messages,
features and communities.
D. Experimental Design
First set of experiments test the performance of using
connectivity-only information for community detection, labeled as the Experiment Set 1. We test Louvain and CNM
algorithms on three different types of connectivity graphs.
Second set of experiments test the performance of contentonly methods, labeled as Experiment Set 2. We test k-means
and NMF methods. Third set of experiments test the performance of methods utilizing both connectivity and content
information, labeled as Experiment Set 3. We test GNMF and
NMTF frameworks proposed by [3] as baseline algorithms,
alongside our proposed MultiNMF, TriNMF and DualNMF
methods. In user content dimension, we use DualNMF method
to test the experiment design that only uses user-word content.
We use TriNMF method to test the experiment design that
uses user-hashtag or user-domain information in combination
with the user-word information. We use MultiNMF method

TABLE V
I RELAND E XPERIMENT S ET 1 R ESULTS

to test the experiment design that uses all of user-word, userhashtag and user-domain contents. We label these experiments
as Experiment Set 3.1, 3.2 and 3.3 respectively.
E. Experimental Results
First, we present statistics of retweets without edits and user
mentions on the full and endorsement filtered user connectivity
graphs. Table III shows that retweeting without edits indeed
occurs mostly inside like-minded political camps, rather than
cross-camps. Roughly 97% of retweets in the UK data, and
88% of retweets in the Ireland data occur inside like-minded
groups, while these percentages are much lower for users mentions. Our endorsement filtered connectivity network boosts
the percentage of inner group user mentions from 83% to
97% in the UK data and from 59% to 87% in the Ireland
data evidencing that TSB rule in fact identifies positive user
mentions and retweets with edits with high accuracy.

•

Algorithm

User Graph

k

Purity

ARI

NMI

Louvain

R+M
R + ∆M
R + ∆Mw

13
31
31

.8720
.9186
.9224

.7277
.7453
.7536

.6849
.7393
.7518

CNM

R+M
R + ∆M
R + ∆Mw

10
29
29

.7016
.8333
.8333

.4509
.6426
.6426

.4720
.6381
.6381

Using endorsement filtered user connectivity graph usually gives better clustering performance compared to
using full user connectivity graph. There is a pattern of
weighted graph approach outperforming the others.
TABLE VI
UK E XPERIMENT S ET 2 R ESULTS

TABLE III
DATA ATTRIBUTES
UK

Ireland

962
28

1,652
216

Inner Group Retweet + Mention Links
Inter Group Retweet + Mention Links

1,986
398

3,056
2,092

Inner Group Retweet + ∆Mention Links
Inter Group Retweet + ∆Mention Links

1,456
40

2,820
432

Inner Group Retweet Links
Inter Group Retweet Links

We run each experiment 20 times for every method and pick
the maximum score achieved for reporting. Each regularizer
parameter (α, γ, θ, β) are experimented with values 1, 10,
100 and 1000. Best accuracies are usually reached with
experiments in which α and β equal to 10 or 100 while
γ and θ equal to 1. This shows the contribution of user
connectivity and word similarity regularizers, and considerably
lower contributions of hashtag and domain name regularizers
towards overall performance of the algorithms.

Algorithm

User Content

Purity

ARI

NMI

k-Means
NMF

user x word
user x word

.6738
.6395

.2378
.1541

.2018
.1709

TABLE VII
I RELAND E XPERIMENT S ET 2 R ESULTS
Algorithm

User Content

Purity

ARI

NMI

k-Means
NMF

user x word
user x word

.4651
.4186

.0488
.0434

.1672
.1139

Experiment Set 2 indicates that word usage-only based
clustering yields considerably lower accuracies compared to
user connectivity-only based clustering.
TABLE VIII
UK E XPERIMENT S ET 3 R ESULTS
Algorithm

User Graph

User Content

Purity

ARI

NMI

R+M
R + ∆M
R + ∆Mw

user x word

GNMF[29]

.7854
.8069
.8326

.4955
.6099
.6469

.4120
.4922
.5461

NMTF[3]

R+M
R + ∆M
R + ∆Mw

user x word,
tweet x word

.8197
.8112
.8412

.6448
.5657
.5331

.2593
.2471
.3751

TABLE IV
UK E XPERIMENT S ET 1 R ESULTS
Algorithm

User Graph

k

Purity

ARI

NMI

Louvain

R+M
R + ∆M
R + ∆Mw

20
42
42

.9313
.9613
.9484

.4661
.3691
.4291

.5854
.5916
.5916

TriNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x domain

.7597
.7940
.8283

.3707
.5566
.6375

.3158
.4595
.5006

CNM

R+M
R + ∆M
R + ∆Mw

17
41
41

.8498
.9700
.9700

.5656
.6150
.6150

.5257
.6496
.6496

TriNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x hashtag

.7897
.8112
.7768

.5232
.4640
.5001

.4320
.3780
.3837

MultiNMF

R+M
R + ∆M
R + ∆Mw

user x word
user x domain,
user x hashtag

.7554
.8112
.8112

.4025
.5726
.6108

.3343
.4404
.4978

R+M
R + ∆M
R + ∆Mw

user x word

DualNMF

.8326
.8927
.8970

.5674
.7291
.7616

.5146
.6086
.6380

Major findings for Experiment Set 1 can be summarized as
follows:
• Relatively larger clustering scores occur due to artificially
large number of clusters that are found. Considering the
number of users in both datasets, the number of clusters
identified in Experiment Set 1 are not practical for use
(e.g. 29 clusters in Ireland data for 7 political parties).

Major findings from Experiment Set 3 can be summarized
as follows;

TABLE IX
I RELAND E XPERIMENT S ET 3 R ESULTS

VI. C ONCLUSION

Algorithm

User Graph

Content

Purity

ARI

NMI

R+M
R + ∆M
R + ∆Mw

user x word

GNMF[29]

.5543
.6279
.8178

.2447
.4557
.6978

.2881
.4652
.6399

NMTF[3]

R+M
R + ∆M
R + ∆Mw

user x word,
tweet x word

.5969
.6860
.7597

.3119
.3986
.5198

.2144
.2384
.4469

TriNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x domain

.7209
.7946
.8101

.5051
.6045
.6807

.5237
.5313
.6372

TriNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x hashtag

.6938
.7016
.8062

.4202
.5300
.6784

.4431
.4224
.6885

MultiNMF

R+M
R + ∆M
R + ∆Mw

user x word,
user x domain,
user x hashtag

.7481
.6744
.8178

.4777
.4597
.6953

.4938
.4219
.6411

R+M
R + ∆M
R + ∆Mw

user x word

DualNMF

.7364
.7597
.8721

.5561
.6292
.7536

.5397
.6029
.7096

TABLE X
C OMPARISON OF M ETHODS FOR E XPERIMENT S ET 3

Content

•

•

•

•

•

3word
word,
hashtag or domain
word,
hashtag and domain

Connectivity
R+M
3R + ∆Mw
DualNMF
33DualNMF
TriNMF

3TriNMF

MultiNMF

3MultiNMF

Regardless of the experiment set and algorithms used,
endorsement filtered user connectivity graph yields higher
accuracy clustering performance compared to using the
full connectivity graph. Usually weighted graph approach
outperforms the others.
DualNMF method which factorizes user-word matrix
alongside user connectivity and word similarity regularizers yields the highest accuracy clustering performance.
We get much higher scores of clustering accuracy in
Experiment Set 3 compared to Experiment Set 2. Regularizing content-only methods with user connectivity
graphs(GNMF [29]), dramatically increases the quality
of the clustering. DualNMF which incorporates keyword
similarity regularization to GNMF further boosts the
quality of clustering.
Compared to DualNMF method, including tweet messages for NMTF method proposed in [3] does not help to
further improve the clustering quality, while it increases
complexity dramatically. DualNMF provides 9% additional purity, 46% additional ARI score while doubling
the NMI score compared to the baseline NMTF method
of Pei et al. in [3].
Compared to DualNMF method, utilizing hashtag and/or
domain usage information (i.e. TriNMF and MultiNMF)
do not contribute to the overall clustering quality.

In Twitter, content and endorsement filtered connectivity
are complementary to each other in clustering politically
motivated users into pure political communities. Word usage is
the strongest indicator of user’s political orientation among all
content categories. Incorporating user-word matrix and word
similarity regularizer provides the missing link in connectivityonly methods which suffers from detection of artificially large
number of clusters in sparse Twitter networks. Our future work
includes parallel distributed evolutionary community detection
and identification of emerging coalitions and conflicts among
communities.
ACKNOWLEDGMENT
This research was supported by ONR Grants N00014-16-12386 and N00014-15-1-2722.
R EFERENCES
[1] Howard, Philip N., and Aiden Duffy, Deen Freelon, Muzammil Hussain,
Will Mari, and Marwa Mazaid. Opening Closed Regimes: What Was the
Role of Social Media During the Arab Spring? Project on Information
Technology and Political Islam Data Memo 2011.1. Seattle: University
of Washington, 2011.
[2] Girvan,M., Newman M.E.J. (2002). Community structure in social and
biological networks, Proceedings of the National Academy of Sciences,
99(12), pp.7821-7826.
[3] Yulong Pei, Nilanjan Chakraborty, and Katia Sycara. 2015. Nonnegative
matrix tri-factorization with graph regularization for community detection
in social networks. In Proceedings of the 24th International Conference on
Artificial Intelligence (IJCAI’15), Qiang Yang and Michael Wooldridge
(Eds.). AAAI Press 2083-2089.
[4] J. Tang, X. Wang, and H. Liu. Integrating Social Media Data for
Community Detection. In Modeling and Mining Ubiquitous Social Media,
2012.
[5] Mrinmaya Sachan, Danish Contractor, Tanveer A. Faruquie, and L.
Venkata Subramaniam. 2012. Using content and interactions for discovering communities in social networks. In Proceedings of the 21st
international conference on World Wide Web (WWW ’12). ACM, New
York, NY, USA, 331-340.
[6] Y. Ruan, D. Fuhry, and S. Parthasarathy. Efficient community detection
in large networks using content and links. In WWW 13, 2013.
[7] Tufekci, Z. (2014). Big Questions for Social Media Big Data: Representativeness, Validity and Other Methodological Pitfalls. In: International
AAAI Conference on Weblogs and Social Media.
[8] M. D. Conover, B. Goncalves, J. Ratkiewicz, M. Francisco, A. Flammini,
and F. Menczer, Political polarization on Twitter, in Proceedings of the
5th InternationalConference on Weblogs and Social Media, 2011
[9] Sandra Gonzlez-Bailn, Ning Wang, Alejandro Rivero, Javier BorgeHolthoefer, Yamir Moreno, Assessing the bias in samples of large online
networks, Social Networks, Volume 38, July 2014, Pages 16-27, ISSN
0378-8733, http://dx.doi.org/10.1016/j.socnet.2014.01.004.
[10] S. A. Myers, A. Sharma, P. Gupta, and J. Lin. Information network
or social network? The structure of the Twitter follow graph. WWW
Companion, 2014
[11] Heider F. The psychology of interpersonal relations. New York: Wiley,
1958. 322 p.
[12] Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, and Mung Chiang.
2013. Quantifying political leaning from tweets and retweets. In Proceedings of the International AAAI Conference on Weblogs and Social Media
(ICWSM).
[13] D. Boyd, S. Golder and G. Lotan, ”Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter,” System Sciences (HICSS),
2010 43rd Hawaii International Conference on, Honolulu, HI, 2010, pp.
1-10. doi: 10.1109/HICSS.2010.412
[14] Hubert, L., Arabie, P. (1985). Comparing partitions. Journal of Classification, 2, 193218.
[15] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, Cambridge, 2004.

[16] M. Newman. 2006. Modularity and community structure in networks.
Proceedings of the National Academy of Sciences, vol. 103, no. 23, pp.
85778582.

A PPENDIX
D ERIVATION OF E QUATIONS 2, 3, 4, 5

[17] S. Fortunato. 2010. Community detection in graphs. Physics Reports,
vol. 486, pp. 75174

To follow the conventional theory of constrained optimization we rewrite objective function 1 as;

[18] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne
Lefebvre. Fast unfolding of communities in large networks. Journal of
Statistical Mechanics: Theory and Experiment 2008 (10), P10008 (12pp)
doi: 10.1088/1742-5468/2008/10/P10008.
[19] A. Clauset, M. Newman, and C. Moore, Finding community structure in very large networks, Physical Review E, vol. 70, p. 066111, 2004. [Online]. Available:
http://www.citebase.org/cgibin/citations?id=oai:arXiv.org:condmat/0408187
[20] Waltman, L., Van Eck, N. J.. 2013. A smart local moving algorithm
for large-scale modularity-based community detection. European Physical
Journal B, 86, 471.
[21] Lloyd., S. P. (1982). ”Least squares quantization in PCM”
(PDF). IEEE Transactions on Information Theory 28 (2): 129137.
doi:10.1109/TIT.1982.1056489
[22] Daniel D. Lee, H. Sebastian Seung. 2000. Algorithms for Non-negative
Matrix Factorization. In Neural Information Processing Systems (NIPS),
Vol. 13 , pp. 556-562.

JU,H,D,W = T r((Xuw − UWT )(Xuw − UWT )T )
+ T r((Xuh − UHT )(Xuh − UHT )T )
+ T r((Xud − UDT )(Xud − UDT )T )
+ αT r(UT LC U) + γT r(HT LHsim H)
+ θT r(DT LDsim D) + βT r(WT LWsim W)
JU,H,D,W = T r(Xuw XTuw ) − 2T r(Xuw WUT )
+ T r(UWT WUT ) + T r(Xuh XTuh )
− 2T r(Xuh HUT ) + T r(UHT HUT )
+ T r(Xud XTud ) − 2T r(Xud DUT ) + T r(UDT DUT )
+ αT r(UT LC U) + γT r(HT LHsim H)
+ θT r(DT LDsim D) + βT r(WT LWsim W)

[23] C. J. Lin. On the Convergence of Multiplicative Update Algorithms for Nonnegative Matrix Factorization. In IEEE Transactions on
Neural Networks, vol. 18, no. 6, pp. 1589-1596, Nov. 2007. doi:
10.1109/TNN.2007.895831

Let Φ, η, Ω and Ψ be the Lagrangian multipliers for constraints
U, H, D, W > 0 respectively. So the Lagrangian function L
becomes;

[24] Yuan Yao, Hanghang Tong, Guo Yan, Feng Xu, Xiang Zhang,
Boleslaw K. Szymanski, and Jian Lu. 2014. Dual-Regularized OneClass Collaborative Filtering. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge
Management (CIKM ’14). ACM, New York, NY, USA, 759-768.
DOI=http://dx.doi.org/10.1145/2661829.2662042

L = T r(Xuw XTuw ) − 2T r(Xuw WUT ) + T r(UWT WUT )

[25] Linhong Zhu, Aram Galstyan, James Cheng, and Kristina Lerman.
2014. Tripartite graph clustering for dynamic sentiment analysis on
social media. In Proceedings of the 2014 ACM SIGMOD International
Conference on Management of Data (SIGMOD ’14). ACM, New York,
NY, USA, 1531-1542. DOI=http://dx.doi.org/10.1145/2588555.2593682
[26] Quanquan Gu and Jie Zhou. 2009. Co-clustering on manifolds. In
Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining (KDD ’09). ACM, New York,
NY, USA, 359-368. DOI=http://dx.doi.org/10.1145/1557019.1557063
[27] Fanhua Shang, L. C. Jiao, and Fei Wang. 2012. Graph
dual regularization non-negative matrix factorization for coclustering. Pattern Recogn. 45, 6 (June 2012), 2237-2250.
DOI=http://dx.doi.org/10.1016/j.patcog.2011.12.015
[28] Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006. Orthogonal
nonnegative matrix t-factorizations for clustering. In Proceedings of the
12th ACM SIGKDD international conference on Knowledge discovery
and data mining (KDD ’06). ACM, New York, NY, USA, 126-135.
DOI=http://dx.doi.org/10.1145/1150402.1150420
[29] D. Cai, X. He, J. Han and T. S. Huang, Graph Regularized Nonnegative
Matrix Factorization for Data Representation, in IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 33, no. 8, pp. 1548-1560,
Aug. 2011. doi: 10.1109/TPAMI.2010.231
[30] Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based
on non-negative matrix factorization. In Proceedings of the 26th annual
international ACM SIGIR conference on Research and development in
informaion retrieval (SIGIR ’03). ACM, New York, NY, USA, 267-273.
DOI=http://dx.doi.org/10.1145/860435.860485
[31] Zhou, Quanquan Gu Jie. Local learning regularized nonnegative matrix
factorization. IJCAI 2009, Proceedings of the 21st International Joint
Conference on Artificial Intelligence, Pasadena, California, USA, July
11-17, 2009
[32] Linhong Zhu, Aram Galstyan, James Cheng, and Kristina Lerman. Tripartite graph clustering for dynamic sentiment analysis on social media.
In Proceedings of the 2014 ACM SIGMOD International Conference on
Management of Data, pages 1531-1542. ACM, 2014.

+ T r(Xuh XTuh ) − 2T r(Xuh HUT ) + T r(UHT HUT )
+ T r(Xud XTud ) − 2T r(Xud DUT ) + T r(UDT DUT )
+ αT r(UT LC U) + γT r(HT LHsim H) + θT r(DT LDsim D)
+ βT r(WT LWsim W) + T r(ΦUT ) + T r(ηHT )
+ T r(ΩDT ) + T r(ΨWT )
The partial derivatives of Lagrangian function L with respect
to U, H, D, W are as follows;
∂L
= −2Xuw W + 2UWT W − 2Xuh H + 2UHT H−
∂U
2Xud D + 2UDT D + 2αLC U + Φ
∂L
= −2XTuh H + 2UHT H + 2γLHsim H + η
∂H
∂L
= −2XTud H + 2UDT D + 2θLDsim D + Ω
∂D
∂L
= −2XTuw U + 2WUT U + 2βLWsim W + Ψ
∂W
Setting derivatives equal to zero and using KKT complementarity conditions [15] of nonnegativity of matrices
U, H, D, W, ΦU = 0, ηH = 0, ΩD = 0 and ΨW = 0,
we get the update rules given in Equations 2, 3, 4, 5.

Extraction Techniques for Mining Services from Web Sources
Hasan Davulcu, Saikat Mukherjee, LV. Ramakrishnan
Dept. of Computer Science
SUNY Stony Brook
Stony Brook, NY 11794, USA
{davulcu,saikat,ram}@cs.sunysb.edu

tology consisting of a taxonomy of service concepts, their associated attributes (such as names
and addresses) and type descriptions for the attributes. In addition the ontology also associates
an attribute identifier function with each attribute.
Applying the function to a web page will locate
all the occurrences of the attribute in that page.
Each web page is parsed into a DOM (Document Object Model) tree and the identifier functions, specified in the ontology for locating occurrences of the attributes in the page, are applied.
The problem is to group all the attributes corresponding to each service provider. In this paper
we describe a novel ontology-directed propagation technique for identifying and accumulating
all of the attributes related to each service entity
in a web page. By using a concept of scoring and
conjict resolution to prevent erroneous associations, our algorithm groups the attributes related
to each service provider in a web document.

Abstract
The Web has established itself as the dominant
medium for doing electronic commerce. Consequently the number of service providers, both
large and small, advertising their services on the
web continues to proliferate. In this paper we describe new extraction algorithms for mining service directories from web pages. We develop a
novel propagation technique for identifying and
accumulating all of the attributes related to a service entity in a web page. We provide experimental results of the effectiveness of our extraction
techniques by mining a database of veterinarian
service providers from web sources.

1. Introduction
A number of service providers operate their
own web sites promoting their services at length
while others are merely listed in a referral site.
Aggregating all of the providers into a queriable
service directory makes it easy for customers to
locate the best suited servicies for their needs.
An attractive solution is to create the service directory by mining the web for service providers.
Such a solution has several merits. Firstly, it
does not need any explicit participation by the
service provider and hence is scalable. Secondly,
since the process is automated and repeatable the
content can always be kept current. Finally, the
same process can be readily adapted to different domains. We characterize services by an on-

0-7695-1754402 $17.00 0 2002 IEEE

2. Ontology Directed Mining
A service ontology is characterized by a set of
service concepts C, the IS-A relationship between
concepts, a set of single-valued attributes A,, a set
of multi-valued attributes A,, a function A that
associates a set of attributes with a concept, and
the extraction function A t t r i d associoated with
an attribute. Each entity is uniquely identified by
a set of single-valued attributes. We call any such
set as a key. e.g. for service providers two possi-

601

sistent. Observe that whenever r is not conflictfree then any maximally marked node represents
a single entity. All we need to do is simply pick
the attributes in it and create the tuple for that entity. If this is not the case then attributes of an entity may be distributed across neighboring nodes.
In that case we will have to detect the boundaries
separating each entity. In addition even if is
conflict-free the leaf nodes in it will have conflicts
and we will have to detect boundaries separating
the attributes of entities in the text string at the
leaf node.

ble keys are {street, city} and {street, zip}. Let
T be the DOM tree of a page. Parent(n)denotes
the parent of node n and children(n) denotes all
its children. We are interested in identifying subtrees in T in which no single-valued attribute occurs more than once. We use the notion of a mark
for doing so. Whenever rnark(n) is q5 it means
that there exists more than one occurrence of a
single valued attribute in its subtree. Specifically,
the subtrees rooted at a node can be merged as
long as no single-valued attribute occurs in more
than one subtree. A maximally marked node n is
an internal node such that mark(Parent(n))is
4. Let U(.) denote the concatenation of the text
strings associated with the leaf nodes of the subtree rooted at n; Attr be the set of attributes of the
concept c; {kl, ..., k,} be the attributes that constitute the key of c; R(al,...,a,) denote the tuple
of attributes associated with an entity. We will extract one tuple from a home page and several such
tuples from a referral page. We use score(n) to
denote Imark(n)I.Algorithm Extract takes as input the tree of the page and the set of attributes
names of the concept c. It outputs either a single
tuple containing the values of the attributes if it
is a home page or a set of tuples if it is a referral
page. Extract-HomePage takes as input the set of
attribute names whose values are to be extracted
and the set of maximally marked nodes in the document tree. The intuition behind this algorithm is
that the maximally marked node contains the key
attributes associated with the service entity and
any node in the document tree might contain occurrences of the multi-valued attributes. For referral pages we have to extract the attributes of
several entities. The main problem here is associating the extracted attributes with their corresponding entities. We use the notion of a conjicting set that will be used in making such an
association. Let r be as defined in Algorithm Extract. Observe that r is an ordered set of nodes.
Let < m l , mz,. . . , m, > denote the nodes in this
ordered sequence. We say that r is conjict-free
whenever 3,
mi,
mi+lE r such that mark(mi) U
mark(mi+l) is consistent. r is not conflict-free if
all pairs of consecutive nodes are mutually incon-

r

Algorithm Extract (T,
Attr)
begin
1. forall nodes n E T do
2.
mark(n)
3. end
4. Let r = { maximally marked nodes } U ( leaf nodes marked }
5. if3m,,mj E r A (Attrid(ki)(o(mi)),
..., Attrid(k,)(a(mi))}
# { A t t r i d ( k l ) ( o ( m j )_..,
) , Attrid(k,)(o(mj))}then
6.
T i s a referral page
7. else
8. . T i s a h o m e p a g e
9. endif
IO. if T is a home page then
11.
R = ExuactHome-Page(Attr, r)
12. elseif T is a referral page then
{RI,
.._,R,} = Extract_ReferralPage(Attr,r)
13.
14. end
end
Algorithm Extract.Home.Page (Attr,r)
begin
1. pick the node n in r with the maximum score
2. forall a, E Attr A a, E A, do
3.
RI.;] = Attrid(ai)(o(n))
4. end
5. farall ai E Attr A a, E A, do
6.
R[a,]= U
,, Er Attr.id(a,)(o(m,))
7. end
8. return R
end

3. Service Directory Mining System Implementation
The system consists of three main components:
an acquisition component, a classijication component and an exfraction component. The acquisition component retrieves HTML pages from the
web that are likely to be relevant for the intended

602

domain of services. This is done by doing a keyword search for the service with a web search engine. The search engine returns a number of urls
pointing to pages that match the keywords. All
of these web pages are fetched by the acquisition
component.
Algorithm ExVactReferralPage (Attr,r)
begin
I. if r is not conflict-free then
forall m, E r do
2.
3.
if m, is a leaf Amnrk(m,) = Q then
4.
{RI, ..., R"}= Boundary.Detection(Attr, mi)
5.
else
6.
forall a, E Attr do
7.
Q [ n > ]= Attr.id(a,)(m(m;))
8.
end
9.
end
10.
end
1 1 . else
12.
{RI,
..., R,} = Boundary.Detection(Attr, r)
13. end
14. return {RI,
..., %}
end

The classification component filters the retrieved pages into a set of web pages that the classifier has judged to be actually relevant for the intended service. For training the classifer one hand
picks examples of web pages that are relevant to
the intended domain of service. These serve as the
positive examples. One must also choose pages
unrelated to the service as the negative examples. The extraction component, driven by the service ontology, does unsupervised extraction of attribute values from classified pages and builds the
services direcxtory. The mining system described
above was used to create a service directory of
veterinarians. For veterinarian service providers,
we built an ontology consisting of two concepts:
the Service Provider concept at the root, and the
concept Veterinarian. The Service Provider concept consists of the attributes service provider
name, street, city, state, zip, phone, email, url.
The concept Veterinarian consists of the attribute
vet's name. In addition, Veterinarian inherits all
the attributes of Service Provider. The attributes
phone, email and vet's name are multi-valued
while the other attributes are single-valued. Regular expressions were used to identify phone num-

ber, email, state and zip in a page. Rules were
used to identify street, vet's name and service
provider name. We trained the classifier by picking 371 veterinarian home/referral and 303 nonveterinarian web pages. The web search yielded
13,691 distinct pages. The trained classifier was
used to select the relevant pages from them. Classification identified 3400 pages as positive or relevant to the veterinarian domain. The extraction
algorithm was run on all of the 3400 pages. We
provide experimental results of this case study below. From these 3400 positively classified pages,
950 were identified as home pages of veterinarian service providers while 1900 were identified
as referral pages by the extraction algorithm. The
< city, state, tip > triple was used as the key.
There were about 550 pages with missing zips
that were discarded. Table 1 shows the statistics of different attribute values collected for these
2850 pages.

Attribute

Number of R
Home Pages

City
State

950

Zip
Street

Phone
Email
Doctor Name
Hospital Name
URL

950
950
950
806

3
Referral Pages
12300

12300
12300

10930

938
71 1
856

12300

950

Table 1. Number of records extracted for each
attribute for home and referral pages

For comparison we retrieved a total of 650
email addresses and 990 urls of veterinarian
service providers listed in http://vetquest.com,
http://vetworld.com and the yellow pages in
http://www.superpages.com. In contrast our mining system yielded 1718 emails and 950 urls of
home pages.

603

ogy can result in improving the precision of extraction considerably. This is an area worthy of
investigation. Our requirement for the existence
of a key to distinguish between home and referral pages resulted in misclassifying some referral
pages. Relaxing this requirement is a topic of future work.

4. Related Work
Extracrion from semi-structured sources by
wrapper generation methods is an extensively researched topic [IO, 2, 8, I, 11, 9, 121. Wrappers have several disadvantages: (a) a significant amount of work is required to generate the
tules, and (b) they are document specific as they
rely on the syntactic relationship between HTML
tags and the attribute value for proper extraction.
Wrappers are therefore brittle to changes in the
document structure. In contrast our extraction algorithms are independent of any page specific relationships between HTML tags and attribute values. All that is needed is an ontology for the intended service domain. With such an ontology
extraction from any document relevant to that domain can be carried out. Information extraction
techniques as embodied in [5,4,3]use supervised
machine learning methods. Observe that the creation of the ontology is the only supervised step
in our approach. Regardless of the ontology, our
algorithm for associating attribute values to their
corresponding service entity in a web document is
unsupervised. Supervised machine learning techniques that will handle multiple entities in a document are as yet not known. Query languages for
semi-structured documents constitutes an important class of extraction techniques. They all assume that the document schema is known a priori.
In our approach we make no such assumptions.
The work that comes closest to ours is [7, 61. In
this work the extraction problem is formulated as
one of detecting boundaries between records and
hence is applicable to only referral pages. The
boundary detection is based on several different
heuristics which can result in incorrectly identifying the entities.

References
[I] N. Ashish and C. Knoblock. Wrapper generation for semistructured internet sources. A C M S I C M O D Record, 26(4):8IS, 1997.
121 P. Ateeni and G. Mecca. Cut & paste. In A C M Synposiim
on Principles ofDatabase Systems. pages I 17-1 21,Arizona,
June 1997.ACM.
[31 M. E.Califf and R.J. Mooney. Relational learningofpatternmatch rules for information extraction. In Working Notes of
AAA1 Spring Symposium on Applying Machine Learning to
Discourse Processing, pages 6 - 1 I . Menlo Park, CA, 1998.
AAA1 Press.
[41 M. Craven, D. DiPasquo, D. Freitag. A. McCallum, T. M.
Mitchell, K. Nigam, and S. Slattery. Learning to construct
knowledge bases from the world wide web. Artificial Intelligence, ll8(l-2):69-ll3.2000.
[51 M. Craven. D. Dipasquo, D. Freitag, A. K. McCallum, T. M.
Mitchell, K. Nigam, and S. Slattery. Learning to extract symbolic knowledge from the World Wide Web. In Proceedings of AAAI-98, 15th Conference ofthe American Association for A ~ i f i c i a l l n r e l l i g e ~ pages
c ~ , 509-5 16,Madison, US.

1998.AAA1 Press, Menlo Park, US.

[61 D.Embley, Y. Jiang, and Y
:K.

Ng. Record-boundary dis-

covery in Web documents. In ACM SICMOD Conference on
Management of Dora, pages 467478. ACM, 1999.
[71 D. W. Embley, D. M. Campbell, R. D. Smith, and S . W. Liddle. Ontology-based extraction and structuring of information from data-rich unstructured dccuments. I n Proceedings
of the lnrernarional Conference on Knowledge Management.
ACM, 1998.
[81 J. Hammer, H. Garcia-Molina, S. Nestorov, R. Yemeni,
M. M. Breunig, and V. Vassalos. Template-based wrappers
in the tsimmis system. In A C M S I C M O D Conference on
Mnnngement ofData. pages 532-535. ACM. 1997.
[9] N. Kushmerick, D. S. Weld, and R. B. Doorenbos. Wrapper induction for information extraction. I n Intl. Joint Conference on Artificial Intelligence, volume 1, pages 729-737,
Nagoya, Japan, 1997.
[IO] 1. Muslea. Extraction patterns for information extraction
tasks: A survey. In AAAI-99 Workshop on Machine Learning

5. Conclusion

for Information Ertraction.

[ I l l M. Perkowitz, R. Doorenbos, 0. Etzioni, and D. Weld.
Learning to understand information on the internet: An
example-based approach. Journal of lnrelligenr Informarion
Systems, 8(2):133-153, March 1997.
[I21 S. Soderland. Learning information extraction rules for
semi-structured and free text. Machine Learning, 34(13):233-272, 1999.

Engineering an ontology is largely dictated by
the the complexity of the identifier functions. In
the case study reported in this paper we used regular expressions as the identifier extraction functions. Incorporating complex rules in the ontol-

604

