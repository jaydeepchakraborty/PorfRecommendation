Temporal Rules Discovery for Web Data Cleaning
Ziawasch Abedjan§ Cuneyt G. Akcora] Mourad Ouzzani]
Paolo Papotti] Michael Stonebraker§
]

Qatar Computing Research Institute, HBKU

{cakcora,mouzzani,ppapotti@qf.org.qa}

ABSTRACT
Declarative rules, such as functional dependencies, are
widely used for cleaning data. Several systems take them as
input for detecting errors and computing a “clean” version
of the data. To support domain experts,in specifying these
rules, several tools have been proposed to profile the data
and mine rules. However, existing discovery techniques have
traditionally ignored the time dimension. Recurrent events,
such as persons reported in locations, have a duration in
which they are valid, and this duration should be part of
the rules or the cleaning process would simply fail.
In this work, we study the rule discovery problem for temporal web data. Such a discovery process is challenging because of the nature of web data; extracted facts are (i) sparse
over time, (ii) reported with delays, and (iii) often reported
with errors over the values because of inaccurate sources or
non robust extractors. We handle these challenges with a
new discovery approach that is more robust to noise. Our
solution uses machine learning methods, such as association
measures and outlier detection, for the discovery of the rules,
together with an aggressive repair of the data in the mining step itself. Our experimental evaluation over real-world
data from Recorded Future, an intelligence company that
monitors over 700K Web sources, shows that temporal rules
improve the quality of the data with an increase of the average precision in the cleaning process from 0.37 to 0.84, and
a 40% relative increase in the average F-measure.

1.

§

MIT CSAIL

{abedjan,stonebraker@csail.mit.edu}

person	
   des)na)on	
   )me	
  

product	
   company	
   weight	
  

releaseDate	
  

Obama	
   Italy	
  

8pm	
  12	
  Nov	
  

iPhone	
  4	
   Apple	
  

137g	
  

24	
  June	
  2013	
  

Obama	
   France	
  

11am	
  13	
  Nov	
  

iPhone	
  6	
   Apple	
  

129g	
  

19	
  Sept	
  2014	
  

Obama	
  will	
  
arrive	
  in	
  Italy	
  
12	
  Nov	
  8pm	
  

Obama	
  in	
  S.	
  
Africa	
  8.30pm	
  
12	
  Nov	
  

CNN

Twitter

Apple	
  
released	
  new	
  
iPhone	
  on	
  19	
  
Sept…	
  

MacFan

tomorrow	
  
new	
  iPhone	
  
3G/4G/LTE	
  
(09.18)	
   Times

ExtracLon	
  Layer	
  
person	
   des)n.	
  

)me	
  

product	
   company	
   weight	
   releaseDate	
  

Obama	
   Italy	
  

8.00pm	
  11/12/2014	
  

iPhone	
  

Google	
  

137g	
  

6/24/2013	
  

Obama	
   S.	
  Africa	
   8.30pm	
  11/12/2014	
  

iPhone	
  

Apple	
  

3g	
  

9/18/2014	
  

Obama	
   France	
  

iPhone	
  

Apple	
  

129g	
  

9/19/2014	
  

11am	
  11/13/2014	
  

Figure 1: From top to bottom: real facts, their representation on the Web, and the extracted data.
Consider the example depicted on the left hand side of
Figure 1. Obama attended a dinner in Italy, on Nov 12th
2014 at 8 pm; this is a real event and is represented as a tuple
in the relation at the top of the Figure. The information
is correctly reported on a web page from CNN and a web
data extractor identifies that a person (“Obama”) was in
a location (“Italy”) at a certain time; this is an extracted
event (relation at the bottom). However real events can be
reported by multiple sources that may or may not agree on
the details. In fact, another source reports Obama in South
Africa on the same day. As it is unlikely that a person is
reported in two different countries within 30 minutes, such a
contradiction highlights a problem in the data. In this case,
the event was extracted from a social media outlet that did
not have a faithful knowledge about the real event. This
happens in practice and is studied as the problems of truth
discovery [33, 15, 28] or fact-checking [29]. By enforcing a
rule over information from multiple sources, it is possible
to gain understanding about the trustability of the sources
and, ultimately, about the correct value of interest.
Consider another example for releases of products in the
right hand side of Figure 1. The information reported by the
source Times is a real event, but an error in the extractor led
to an incorrect weight, namely 3g, which in fact is a type of
network supported by the phone. Detecting such problems
can help identify faulty extractors [7, 13].
The two above examples highlight that identifying quality problems enables analysis over the sources, the data val-

INTRODUCTION

With the increasing availability of web data, we are witnessing the proliferation of businesses engaged in automatic
data extraction from thousands of web sources with the goal
of gleaning useful information and intelligence about people,
companies, countries, products, and organizations [30]. It is
well recognized that the data cannot be used as-is because
of errors that are in the sources themselves [15, 28, 29, 33]
or that arise with automatic extractors [7, 13].

This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivatives 4.0 International License. To view a copy
of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For
any use beyond those covered by this license, obtain permission by emailing
info@vldb.org.
Proceedings of the VLDB Endowment, Vol. 9, No. 4
Copyright 2015 VLDB Endowment 2150-8097/15/12.

336

ues, and the extractors. These analytics tasks usually rely
on declarative rules (such as key constraints) for detecting
problems in the data. For example, the fact that a product
is always released by a company can be expressed with a
functional dependency (Fd), i.e., product → company. In
the above example, the company releasing the phone cannot be both Google and Apple. Heuristics exploiting the
redundancy are usually used to determine the correct value
(the truth) [28]. However, there are other errors that can
be identified only through temporal functional dependencies,
which are Fds that restrict the rule on the temporal dimension [21]. For example, a domain expert may come up with
a rule stating that a person cannot be reported arriving in
two countries at the same time (person → destination in
a 1-hour window), or that the same product cannot have
different weights reported at the release date (product →
weight in a 6-month window).
Coming up with these rules with the correct duration
value for “same time” is not trivial. A conservative choice
for this duration in a rule, such as “within a minute”, leads
to undetected errors in the Obama example. On the contrary, a high value for duration, such as “two days”, does
capture the problem, but would mark as errors all the tuples in the example, including the correct ones with Italy
and France. Similar challenges arise for product release, a
time window of one day for the weight would not capture
the problem with 3g. Moreover, durations depend on the entity at hand. For instance, Obama travels more frequently
and faster than most people, so he should have a different
temporal rule with a smaller time window.
Discovering constraints has been well studied in the literature [32, 1, 23, 8, 19]. However, a recurring assumption
in these existing techniques is that data is either clean or,
at worst, has a small amount of noise. Obviously, such assumptions do not hold for data extracted from the web due
to the compounded effects of noise coming from the sources
and errors made by the extractors. Moreover, even when it
is possible to mine approximate dependencies over such dirty
data, there is no algorithm to discover useful time-windows,
or durations, to identify errors for the different events, e.g.,
a person is not reported traveling to two countries in a 1hour window. Without such a time dimension, rules are not
usable, as discussed above.
In this paper, we present Aetas1 , our solution to overcome the above challenges by relying on two basic concepts:
(i) the notion of approximation for the discovery of functional dependencies that hold for most of the data, and
(ii) outlier detection techniques for the discovery of the durations. In a nutshell, we first create a set of approximate
Fds that are valid in the smallest meaningful time interval. The dependencies are then ranked with an association
measure, and validated by human experts. For each validated rule, we create a distribution of durations for all the
objects in the data, e.g., how much time is observed within
two consecutive destinations for every person, and mine it
to compute the duration that identifies the lowest extreme
values. This duration is then used as the time window for
the rule to identify temporal outliers.
Our contributions are as follows:
1) We formulate the problem of discovering temporal functional dependencies for data cleaning (Section 2), and
1

present techniques to discover approximate Fds based on
statistical properties of the data (Section 3).
2) Given a rule, we mine the duration that lead to identifying
temporal outliers. We tackle the problem of the sparseness
of the data with value imputation, and reduce the noise by
enforcing the rule in the smallest meaningful time bucket
(Section 4). We also mine rules with constants (akin to
conditional functional dependencies) such that specific durations can be used for specific entities.
3) We show over real and synthetic datasets that our techniques for approximate dependencies and duration discovery
outperform alternative approaches in terms of quality. In
particular, our durations lead to improvement in the data
cleaning process compared to Fds, with an increase of the
average precision in the repair of the temporal data from
0.37 to 0.84, and a 40% relative increase in the average
F-measure (Section 5). Moreover, our technique discovers
durations that lead to higher F-measure than the baselines,
including the durations collected from a group of users.
We discuss related work in Section 6, and in Section 7 we
draw some conclusions and list directions for future work.

2.

RULE DISCOVERY FOR WEB DATA
CLEANING

We first describe the kind of web data we are dealing
with. We then define the syntax and semantics of temporal
dependencies, give a definition of data cleaning, and define
the problem of the rule discovery for web data cleaning.
From web pages to structured data. We are interested
in event data collected from the Web by monitoring news
media. Examples of such data include GDELT (gdeltproject.org) and Recorded Future (www.recordedfuture.com).
Given a web page, the organization in charge of the event
database runs extractors to produce structured data for different events. Examples of events include people traveling to destinations, company acquisitions, and occurrences
of armed attacks. Figure 1 exemplifies data extracted
from text in six web pages: three occurrences for event
PersonTravel with person Obama as the only entity, and
three occurrences for ProductRelease with product being the
entity iPhone. In general, an entity may be an instance of
a person, a location, a company, and so on. In the following, we assume that entity recognition from the text has
been already performed. In addition to the event type specific attributes, e.g., company, destination, all events have
a timestamp attribute, such as time and releaseDate. We
assume that all of these attributes may contain errors.
Temporal Functional Dependencies. We focus on a
specific form of temporal functional dependencies similar to
those described in [21]. We assume a total ordering on the
time attribute t, and that there is a mapping f () that linearizes the different time values into integers. For example,
the value r[t] = (h,m,s) could be mapped to seconds via
f (h,m,s) = 3600h + 60m + s. A time interval ∆ is a pair
with a minimum and a maximum value (for examples in
hours), m and M , respectively, with m ≤ M .
Given the pair < U, t > with a fixed set U = {A1 , . . . , An }
of event type attributes and the time attribute t, a tuple over
< U, t > is a set of < r = {A1 : c1 , . . . , An : cn }, t : ct >,
where ci is a constant. A relation I is a finite set of tuples
over < U, t >.

From “Omnia fert aetas”, Time cancels everything.

337

Definition 1. Let X, Y be two subsets of attributes from
U , ∆ a time interval, and π the permutation of rows of I
increasing on the time value. A temporal functional dependency ( Tfd) over U is an expression X ∧ ∆ → Y
that is satisfied if for all pairs of tuples rπ , rπ+1 ∈ I, s.t.
rπ+1 [t] − rπ [t] ∈ ∆, when rπ [X] = rπ+1 [X], it is the case
that rπ [Y ] = rπ+1 [Y ].
The subsets of attributes X and Y are referred to as lefthand side (LHS) and right-hand side attributes (RHS), respectively. When referring to values of X and Y attributes,
we shall use the terms reference value and attribute value,
respectively.

Aetas
Approximate
FDs discovery

Since the number of possible repairs is usually large and
possibly infinite, a minimality principle is oftentimes used to
identify desirable repairs for the data cleaning problem [22]:
given a database I and a set of dependencies Σ, compute
a repair Ir of I such that Ir |= Σ (consistency) and their
distance cost(Ir , I) is minimal (accuracy). Depending on
the distance function, the desired repair is the one with the
minimal number of cell updates, or the one with minimal
number of tuple deletions. Computing minimal repairs is
NP-hard to be solved exactly for Fds [4, 24] which led to
several heuristics-based methods [10, 12, 16, 24].
Discovering temporal dependencies. Given a relational
schema R and an instance I, the discovery problem for Tfds
is to find all valid Tfds that hold on I. Since web data is
noisy in nature, we are interested in the approximate version
of the problem, i.e., find all valid Tfds, where a rule r is
valid if its support has a value higher than a given threshold
δ. To solve this problem, we developed Aetas, a system to
discover Tfds from web data.
Figure 2 shows the architecture of the system and the
main steps in our solution. Given a noisy dataset, we first
discover approximate functional dependencies, i.e., traditional Fds that hold on most of the relation within a given
atomic duration tα . The use of the atomic duration removes
the temporal aspect of the relation so that dependencies can
be discovered purely in terms of record attributes.
Given a set of approximate Fds, we rank them according
to their support to assist the user in their validation. A user
can either reject a suggested approximate Fd, or validate
it as being a simple Fd or a Tfd. For a validated Tfd,
we then discover its corresponding time interval, including
values that only hold for specific entities as we discussed
previously. Since the data is dirty, we cannot just examine
consecutive occurrences for each entity and collect the minimum duration. Therefore, we compute the distribution of
the durations and mine it to identify the minimum duration
that would eventually cut-off the outlying values, i.e., data
that is invalid. This minimum duration is then assigned
to M and, together with default m = 0, define ∆ for the
approximate Fd at hand.
Finally, Fds and Tfds are fed to a constraint based data
cleaning system, which takes the rules and the noisy data as
input and outputs a consistent updated dataset.

Example 2:
Consider a different instance D for
ProductRelease and the Fd d1 : product → company. Value
errors are reported in bold.
weight
137g
129g
129g

releaseDate
10am 6/24/2014
3pm 9/18/2014
4pm 9/19/2014

If we check the dependency over the data, we get the
following pairs of violating tuples: (t1 ,t2 ),(t2 ,t3 ).
Two possible, alternative repairs are R1 − R2 , as follows:
R1
t1 :
t2 :
t3 :

product
iPhone
iPhone
iPhone

company
Apple
Apple
Apple

weight
137g
129g
129g

releaseDate
10am 6/24/2014
3pm 9/18/2014
4pm 9/19/2014

R2
t1 :
t2 :
t3 :

product
iPhone
iPhone
iPhone

company
Google
Google
Google

weight
137g
129g
129g

releaseDate
10am 6/24/2014
3pm 9/18/2014
4pm 9/19/2014

Data Cleaning
System

valid for d1 . An alternative repair strategy deletes tuple t2
in R1 or tuple t1 , t3 in R2 .
Consider also a Tfd d2 :
product[“iP hone”] ∧
(0, 8months) → weight. A possible repair for violating
tuples (t1 ,t2 ), (t1 ,t3 ) is by updating the value of weight for
t1 to 129g, or to delete t1 .
2

Data Repairing. While Tfds can be used in multiple
applications, such as database design, our focus is on data
quality scenarios. Data repairing is the application we will
use in the following to evaluate the quality of the discovered
dependencies. Given a database instance I of schema R and
a dependency ϕ, if I satisfies ϕ, we write I |= ϕ. If we have
a set of dependencies Σ, I |= Σ if and only if ∀ϕ ∈ Σ, I |= ϕ.
A repair I 0 of an inconsistent instance I is an instance that
satisfies Σ. A repair solution is not unique, as discussed in
the following example.

company
Apple
Google
Apple

Approximate
temporal FD

Figure 2: Architecture of the Aetas system.

While most of the entities for a given event abide by the
same duration in a rule, some entities may require specific
duration values. For example, in the case of ProductRelease
events, new iPhone models are sometimes released with an
interval of time shorter than a year, while for cars the interval is much longer (e.g., BMW X5 car model is renewed
every 6 years). Thus, in the same spirit of conditional function dependencies (CFDs) [5], we are also interested in Tfds
that apply on subsets of tuples. We therefore extend the language to consider constant selections in the left-hand side,
such as product[“iP hone”] ∧ (0, 8 months) → weight. This is
equivalent to having views for specific entities and applying
the Tfd on the view induced by the selection.

product
iPhone
iPhone
iPhone

Minimum duration
discovery

Clean data
Dirty data

Example 1: The rule “a product cannot be released with
two different weights in a time window of a year” defined
over event ProductRelease can be stated as follows: product∧
(0, 1 year) → weight, where ∆ is the pair m = 0 and M =
1 year. In Figure 1, ProductRelease events show conflicting
weight values 3g and 129g on release dates 09 /18 /2014 and
09 /19 /2014 for product iPhone.
2

D
t1 :
t2 :
t3 :

Candidate
approx FDs

Updates (in italic) in R1 and R2 make the new instance

338

3.

FD DISCOVERY OVER DIRTY DATA

example, in Figure 3, source NYT reports two Y values for
reference value B.Obama in time buckets w2 and w3 . For a
reference value x ∈ X, the union of stripes from all sources
constitutes a plate. For a time bucket wi of a given dependency over R, we define the time slice Ri as the list of Y
values for all X values reported within the time bucket wi .
A potential dependency holds for the cube if, for each
plate and for each stripe, it is true that X → Y. If the
dependency holds only for a specific plate for reference value
xi (i.e., a specific entity), then it is a constant dependency
X[xi ] → Y.

Two main characteristics of web data prevent us from
using traditional dependency discovery algorithms. First,
most of these algorithms assume that the data is clean.
As we work with dirty web data from multiple independent sources, this assumption does not hold. There have
been some work to tolerate some dirtiness up to a certain
threshold on the percentage of not conforming tuples [8,
19]. However, dirtiness in real (web) data is so high that
the corresponding threshold leads to the discovery of very
general rules that are not valid in practice. For example,
our test dataset has noise up to 26% wrt the number of tuples (Table 1 in Section 5). A threshold of 26% leads to the
discovery of several key constraints and multiple functional
dependencies that do not hold semantically.
Second, temporal data contains reference values that
change over time, such as Obama with correct values “Italy”
and “France” at two different timestamps. Because of this
temporal nature, traditional Fds do not apply over the relations with extracted data for many events.
We introduce next how we model the data and then how
we tackle the above problems with our approach for discovering approximate Fds.
tie
nti

E

s

C. Ronaldo
G. Clooney
B. Obama

CNN

Sources

Implication discovery. Considering the aforementioned
noise and temporal problems, we devise an algorithm that
works with dirty, temporal data for the discovery of Tfds.
We discover implications by first fixing an atomic time duration tα such that we can mine the dependencies that hold
within a time bucket. We observe that if a Tfd holds for a
certain duration ∆, it also holds for durations ∆0 ≤ ∆. The
best bucket size is the smallest one that contains enough evidence to do the mining. Moreover, the value of the atomic
duration cannot be more fine grained than the time granularity of the timestamps in the data. In our datasets,
the granularity is up to milliseconds, but the data is too
sparse to mine in such a small granularity, we therefore
use tα = 1 hour. The atomic duration tα is applicationdependent and is an input parameter for the algorithm.
Given a tα value, we partition the data and create time
slices R = {R1 , R2 , ..., RN }. Given a time slice Ri , we employ an association rule based method to detect 1-to-1 and
many-to-1 implications. More specifically, we use normalized pointwise mutual information (NPMI) [6], a standard
association measure in collocation extraction, for implication discovery. In a time slice, NPMI of a pair of referenceattribute values x ∈ X and y ∈ Y is defined as:

Twitter

Italy

Italy France

Italy S.Africa Italy

France France

NYT

w0

w1 w2
Time

w3

Figure 3: A dependency cube: w0 is a 1-day time
bucket, s0 is source CNN, a0 is entity B. Obama,
{Italy, S. Africa, France} are attribute values.

i(x, y) = ln(

P (x, y)
)/ − lnP (x, y)
P (x) × P (y)

where P (x, y) is the joint probability of reference value x
and attribute value y, and P (x) is the marginal probability
of reference value x. Intuitively, given a pair of outcomes
x and y that belong to discrete random variables X and Y
(assumed independent), the PMI quantifies the discrepancy
between the probability of their coincidence given their joint
distribution and their individual distributions. Its normalized version, NPMI, can have the following values: if x and
y only occur together i(x, y) = 1.0, if x and y occur independently i(x, y) = 0, and -1 if they never co-occur. We use this
score as an indicator of their correlation in the following.
We learn the implication x → y for a pair of values in the
given time slice. In order to find the X → Y implication,
we need to generalize the NPMI value over all value pairs of
the two attributes. If the implication holds, we expect the
NPMI value of each pair to be positive (i.e., the sign of the
1-to-1 implication) and, overall, i(X, Y ) close to 1.0. In fact,
three factors can decrease NPMI values even in presence of
real implications. First, multiple reference values can have
the same attribute value in a given time slice (i.e., manyto-1 implication). For example, two persons can be in the
same city at the same time. Second, because of a small
bucket size, time slices may contain few instances about the
same reference values. For example, in a bucket all events
might be about Obama traveling to France. We employ a
decision rule to overcome single reference value by assigning

Dependency cube. We start by considering all the possible dependencies with one attribute in the LHS and one
attribute in the RHS. For each potential dependency X → Y,
we make the time dimension (attribute t) discrete by creating time buckets with the size of an atomic time duration.
Given these time buckets, we define a dependency cube over
four data dimensions: data sources S, time buckets W , reference values X, and attribute values Y. Figure 3 shows a
dependency cube for the Obama example:
- The x axis is divided into homogeneous time buckets wi ∈
W (e.g., 1 hour).
- The z axis reports different reference values x ∈ X (e.g.,
B. Obama, iPhone).
- The y axis reports different sources si ∈ S (e.g., CNN).
- The reported cell values for a certain time bucket, source,
and entity are attributes values y ∈ Y (e.g., Italy, Apple).
The size of each dimension in this cube can be large. For
example, Recorded Future continuously collects data from
more than 0.7 Million web sources. However, due to how
events are reported on the web, the data is very sparse and
the size of the dependency cube is manageable in practice.
For a reference value x ∈ X, the sequence of values of Y
reported by one source over time constitutes a stripe. For

339

i(X, Y ) = 1.0 when |X| = 1. Third, dirty data can introduce
different attribute values for the same reference value (i.e.,
1-to-many occurrences), as in the example with Italy and
S. Africa for Obama. As we assume dirtiness in the data,
we need to tolerate this noise. Given these three possible
causes, some value pairs have low NPMI values, thereby
reducing the NPMI value of attributes, i.e., i(X, Y ) < 1.0.
This is a strong signal for the discovery of correlations and
we exploit it in our algorithm.
From implications to dependencies. Given a list of
NPMI values of multiple time slices, our next task is to decide whether the implication X → Y holds on enough time
slices to be considered a dependency. Hence, we aggregate
the expected value of NPMI values over time slices and compute a score. The score is then used to rank the output for
user validation. Aggregation of NPMI values by expected
value is weighted wrt the number of event instances (i.e.,
tuples) in time slices, such that an implication is penalized
if it does not hold over time slices with large numbers of
event instances.
To prune the number of results in the output, we also allow
as input an optional user-defined significance threshold δ. In
this case, we declare an implication to be a dependency if
its aggregated score is higher than the threshold.
Example 3: Consider the case where an event R has instances distributed over a 36 hour period. Given tα =
12 hours, we create three time slices R1 , R2 and R3 , and
compute their NPMI values to be 0.95, 0.3 and 0.5. Probabilities of event instances belonging to these time slices are
P (R1 ) = 0.8, P (R2 ) = 0.15, and P (R3 ) = 0.05. The expected NPMI value E = 0.95 × 0.8 + 0.3 × 0.15 + 0.5 × 0.05 =
0.83. For δ = 0.7, we assert that the implication X → Y
holds. A value of 0.7 is usually used in practice [17].
2

1
2
3
4
5
6
7
8
9
10
11
12

13

14
15
16
17
18

Algorithm 1: Implication detection with NPMI.

Algorithm. We now give a description of our approximate
dependency discovery algorithm. Given two attributes A
and B from an event R, we use Algorithm 1 to compute their
NPMI score, or to find whether a dependency holds over the
two attributes, if a threshold is given. The algorithm takes
as input an atomic duration tα , two attributes A, B from
a relation R, and an optional significance threshold δ. The
algorithm is called twice for each direction, namely A → B
and B → A. If an implication is found for the attributes,
only one, or both of these dependencies may hold.
The algorithm starts by time bucketing the relation into
smaller relations (Line 2). From Line 3, we find the strength
of the implication within the slice. If the sub-relation contains a single attribute value, the decision rule in Line 2
assigns a NPMI value of 1.0 to the slice. Otherwise, we
compute NPMI values of each (a,b) pair in Line 11. Line 12
adds the NPMI value to the expected value of the slice.
Once NPMI values of all pairs have been computed, the
NPMI value of the slice is added to the expected value of the
whole dependency in Line 13. If δ is defined, we check the
termination conditions in Lines 14 and 16. In Line 14, we
compute the NPMI value for the ideal case where all the remaining slices will be 1.0. Similarly, Line 16 checks whether
the expected value is already above the threshold. In both
cases, we stop the computations early if the condition holds
and return 0 or 1 accordingly. If δ is not defined, we return
the NPMI value for the dependency to be used for ranking and user’s consumption. Once the user has selected the
dependencies that are Tfds, we process then for duration
discovery, as described in the next Section.

Early Termination. If a threshold δ is defined, an implication can be declared to hold or to be pruned by considering
a smaller number of slices. We thus stop NPMI computations if the remaining slices will not carry the expected
score below or above the significance threshold δ. Consider
the case when NPMI values of x out of n slices have been
computed. In the best and worst cases, all the remaining
slices can have NPMI values 1 or 0, respectively. If an implication does not hold even in the best case, or holds even in
the worst case, we do not need to compute the NPMI values
of the remaining slices. Otherwise we continue our computation. Formally, given a total of n slices for the implication
A → B, we terminate computations at the xth slice with

A 6→ B











 A→B

Data: A relation R of attributes A, B; atomic time
length tα ; (threshold δ)
Result: A score for the dependency
npmi = 0, current := 0;
Create time buckets R = {R1 , ..., Rn } of R with tα ;
// Iterate on each slice;
foreach R0 ∈ R do
current ← current + |R0 |;
v = 0;
// Decision rule;
if |R0 .B| = 1 then
// Add an NPMI value of 1.0;
v = 1.0;
else
foreach a ∈ R0 .A do
foreach b ∈ R0 .B do
(a)∗P (b)))
i(a, b) = ln(P (a,b)/(P
;
−ln(P (a,b))
v = v + P (a, b) × i(a, b);
// Add expected value of the slice
|R 0 |
npmi = npmi + v × |R| ;
// Termination;
// 1. Dependency will not hold;
then
if δ 6= null ∧ δ > npmi + |R|−current
|R|
return 0;
// 2. Dependency will hold;
if δ 6= null ∧ δ ≤ npmi then
return 1;
return npmi;

if δ >
P
ij (A, B) × P (j)+
j=1:x
P
ik (A, B) × P (k)
k=x+1:n
P
if δ ≤
ij (A, B) × P (j)
j=1:x

where δ is the significance threshold, P (k) is the probability
of an event instance being in the time slice k, and ik (A, B)
is the NPMI value of the kth slice.
Example 4: Consider the scenario of three time slices in
Example 3. After computing the NPMI value of R1 as 0.95,
we can terminate computations for a significance threshold
of δ = 0.7 because the expected value E = 0.95×0.80 = 0.76
is already above δ.
2

340

4.

TIME DURATION DISCOVERY

using truth discovery algorithms [15, 28, 29, 33] or by involving domain experts.

Given an approximate FD X ∧ ∆ → Y with ∆t =(0,tα )
for an event, the goal of duration discovery is to expand the
atomic duration tα to the correct minimum duration M in
∆. In the ideal case, there exists one and only one ∆ such
that no reference value x ∈ X can change its attribute value
y ∈ Y within a time interval (ty , ty + m), where ty is the
reported time of value y.

Time durations. Given the integrated plate, we compute
a distribution of time durations between consecutive, distinct attribute values for every reference value. Figure 4(b)
shows time durations on I, the stripe with the outcome of
the repair step. Even with repaired values in time buckets,
reference values have varying durations for the same temporal dependency. Two factors impact the observed durations:
- Dirty data. Sources can report conflicting values in two
consecutive buckets that cannot be detected by local repairs.
This problem raises many short durations. For example, the
Twitter stripe in Figure 4(a).
- Reporting frequency. Although a reference value
changes its value in the real world, sources may not report
it. In our dataset, only a small set of entities, such as political leaders, have their changes reported frequently. This
leads to some durations that are longer than the real time
windows between two occurrences of an event.
As a result of these factors, time durations constitute a
non-uniform distribution D(x, y), with a range of [tα , |W |].
Our goal is to mine a duration that would remove the outlying values from this distribution.

B. Obama

CNN
Twitter

Italy

Italy France

Integrated

Italy S.Africa Italy

B. Obama
France France

NYT

w0

w1 w2
Time

w3

(a) Baseline durations

Italy

w0

France

w1 w2
Time

w3

(b) Projected durations

Figure 4: Stripes integration for duration discovery.
A naive approach for duration discovery is to take each
stripe of a dependency cube, and find the time it takes for
an attribute value to change from y1 ∈ Y to y2 ∈ Y , i.e.,
ty2 − ty1 . This results in a list of time difference values.
Then, the minimum value among all the time differences
can be chosen as the M in ∆. This approach is shown in
Figure 4(a) for a single plate, where value changes in stripes
are highlighted.
A major assumption in the naive approach is that web
sources correctly report attribute values. When the data
comes from non-authoritative web sources, this assumption
can easily be broken. A more robust approach is to exploit
the evidence coming from multiple sources such that the
accuracy of an attribute value can be “verified”. The idea
is to first repair the data within a time bucket with the
evidence coming from multiple sources. We then compute
durations on a “clean” integrated stripe, as in Figure 4(b).
Below, we first describe how to repair data in the buckets
and then give the full discovery algorithm.
Repair step. Given an approximate FD A → B, we create
a plate p for each reference value a ∈ A, and the plate is
partitioned into time buckets of size tα . Each bucket wn ∈ p
has a time slice of attribute values Rn reported by sources
where |Rn | ≥ |distinct(Rn )| ≥ 1. A new stripe I is created,
where the results of the integration will be reported. In a
bucket, if there exists a b0 such that mode(Rn ) = b0 , then
the corresponding wn bucket for I is updated with b0 . If
there is no majority, the value in bucket wn−1 is assigned to
wn for I.
Figure 5 shows a window
repair for three sources. In
the figure, the value Italy
CNN France
from source Twitter is less
France
frequent than France, so it
Twitter Italy
is not in the result. AlResolved w2
though our repair approach
NYT France
uses a simple majority votDirty w2
ing scheme, any repair algorithm can be plugged into
Figure 5: Repair step.
the system, for example by

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

Data: A dependency cube C for A → B, a cut-off
value c with 1 ≤ c ≤ 100
Result: A time duration M
Define D(A,B) to be an empty duration list;
foreach plate p ∈ C do
Define I to be an empty stripe with |I| equals to
the # of buckets in p;
foreach non-empty bucket wi ∈ p with time slice Ri
do
b0 ← M ode(Ri );
if b0 is not null then
update bucket wi ∈ I with b0 ;
else
update bucket wi ∈ I with value in wi−1 ;
l = 0;
for i=0:length(I) do
if value(wi ) 6= value(wl ) then
add i − l to D(A, B);
l = i;
return percentile(D(A, B), c);
Algorithm 2: Duration discovery for a temporal rule.

Algorithm. Taking into account the above factors, we propose an approach for time duration discovery in Algorithm 2.
A dependency cube C for an approximate functional dependency A → B is given as input as well as a cut-point
1 ≤ c ≤ 100 for the identification of the duration that removes outliers. We use as default value of 10 for the cutpoint, as this is a common value used for trimming of outliers
(e.g., interdecile range). We also show in the experimental
study how this parameter affects the results.
In a nutshell, the algorithm first corrects the erroneous
attribute values reported by the sources for each plate in
an integrated stripe I (Lines 3-10), and then adds the durations over I to a duration list (Lines 11-15). The output is
the minimum time duration value M that removes outlying
durations for the time dependency A → B.

341

The algorithm iterates over each plate (entity) in the cube
(Line 2). For each plate, we create a new, empty integrated
stripe (Line 3). In the time slice for each bucket in the plate,
depending on the source quality, sources can agree or disagree on the attribute value. To alleviate the problem of
sources with poor quality, we employ a repair step (Line 5).
In a simple analysis, if there is a single most frequent value,
this is assigned to the integrated stripe (Line 7). If a majority cannot be determined, the values are ignored and the
value imputation is done with the previous values in the
strip (from the Occam’s razor principle) (Line 9).
After the repair process, the algorithm works on the integrated stripe I and extracts time durations between different consecutive attribute values (Lines 11-16). Parameter
l in Line 11 records the first point in time when the stripe
reports an attribute value. In the following windows, the
source may report the same value, or change it. If the value
changes, the l parameter is used to compute the time difference between the two different attribute values.

Conditional Durations. As we mentioned earlier, some
Tfds may only apply to a subset of entities because
some, usually popular, entities have more frequent attribute
changes at smaller time frames. To discover the corresponding durations, we track the duration sequences of a specific
entity and compute its duration by mining M only with
values from their plate. As the minimum duration is computed based on only the sequences that refer to a specific
entity, this entity has to be popular, i.e., there must be at
least some observations to compute a distribution. As it
is common in statistics, we require 30 observations for the
computation of the percentile. Therefore, we compute constant rules only for entities with at least 30 durations in
their plate. For instance, while 24 hours is the minimum
duration that removes outliers for the majority of persons
in our person travel dataset, persons such as Vladimir Putin
or Ban Ki Moon should have smaller minimum durations,
and this is reflected with their constant rules.

Percentile Plot

5.

1.0

.8

Percentile
Percentile

EXPERIMENTS

In the following, we first study the performance of our
solutions and compare them to baseline alternatives using a
real dataset provided by Recorded Future. We then study
our algorithms in depth with synthetic data.2
We measure the effectiveness of both implication and duration discoveries. We also measure the execution time
needed by the algorithms. Experiments were conducted on
a Linux machine with 24 1.5GHz Intel CPUs and 48GB of
RAM. All algorithms have been implemented in Java with
Heap size set to 12GB.

.6

.4

.2

0
1

10

100

1000

Algorithms. For the implication discovery, we compare
our proposal (Section 3) to CORDS [20], a state of the art
algorithm for the discovery of approximate dependencies.
We test both methods for time slices, therefore they do not
have to deal with the time dimension. For the duration
discovery, we test the following algorithms:

1.E+04

X
Durations
Durations

Figure 6: Duration discovery with percentile plot.
With multiple time durations from multiple integrated
stripes, we use a trimming (truncation) function, namely the
cth percentile, to compute the duration M . The intuition is
that trimming identifies outlying values (trimmed minima),
and we are after the duration that identifies such outliers.
For example, in the probability distribution of time durations, the 10th percentile specifies the time duration value
at which the probability of the time durations is less than or
equal to 0.1. We report an example of a minimum duration
of six hours (x axis) discovered with the 10th percentile (y
axis) in Figure 6.

- Repair-Outliers (RO), our method reported in Section 4
where the durations collection is performed over a unified
view of every plate. These “clean” durations are then used
for mining the minimum duration M that isolates outliers.
- No Repair-Outliers (NR), a variant of our algorithm
where we do not perform repair; we collect all durations
over the stripes to mine M . This method shows the role of
the repair.
- Alignment-Outliers (AL), a variant of sequence alignment in genomics [26] (Section 4). This is an alternative
method that trusts values more than time, as the former
are used for alignment.

Timestamps or Values? It is worth observing that the
algorithm above aligns the timestamps and then compares
values to perform the analysis. An alternative approach is
to align the values, after they have been ordered, and then
perform the counting of the durations. We will show in the
experimental section that an algorithm that relies on values
for alignment performs worse than the one we propose based
on time alignment. In particular, we implemented a variant
of sequence alignment from the bioinformatics domain [26].
Taking all stripes from a plate, we align attribute values
of each pair of stripes. The alignment process creates two
temporary stripes that are the aligned versions of the input
pair; the temporary stripes both report the same value at a
given time, or one of them reports nothing (i.e., reports a
null value) whereas the other reports an attribute value. The
alignment approach mines durations between value changes
only when the change is reported by both stripes.

- No Repair-Probability (NP), an adaptation of the
disagreement decay from the duration discovery algorithm
in [27]. Disagreement decay is the probability that an entity
changes its value within time ∆t. For an increasing ∆t, the
probability of decay 0 ≤ p ≤ 1 also increases. The authors
use a probability distribution D for various ∆t values [27].
We use a probability cut-point δc , such that we select the
smallest ∆t0 that satisfies the condition D(t0 ) ≥ δc as the
duration for our temporal dependency.
2

The annotated real-world data and the program to generate
synthetic data can be downloaded at https://github.com/
Qatar-Computing-Research-Institute/AETAS_Dataset

342

Event
Acquisition
Company Employees #
Company Meeting
Company Ticker
Credit Rating
Employment Change
Insider Transaction
Natural Disaster
Person Travel
Political Endorsement
Product Recall
Voting Result

#
Atts
3
2
5
3
4
7
22
2
6
2
5
2

# Ground
Rules
4
2
6
2
6
11
210
1
2
1
12
2

Rules
Coverage
1.00
0.50
0.80
0.67
1.00
0.86
0.95
0.50
0.67
0.50
0.67
1.00

Rule Annotated
Over Data
acquired company → acquirer company
company → employees number
company → meeting type
ticker → company
company → new rank
person → company
insider → company
location → natural disaster
person → destination
endorser → endorsee
product → company
location → winner

# Annotated
Tuples
217
198
179
1,906
150
186
150
250
372
199
216
215

%
Errors
26
26
17
4
8
14
0
10
21
11
5
10

Table 1: Events, correct rules, and annotated rule used in the real data evaluation.

5.1

Real Data

can be better measured in a controlled environment with
synthetic data, as we discuss in Section 5.2. We therefore
evaluate the quality of the duration discovery on real data in
a different way. Instead of measuring the distance between
durations, we evaluate the quality of a duration by measuring its effect in a target application. In particular, for a Tfd,
we run it multiple times in the same data cleaning tool with
different durations, and measure the quality of the obtained
repairs. We use BigDansing [22] , a data cleaning system
that can handle Tfds with the repair semantics discussed
in Section 2.
For this validation, we manually created ground truth for
a large sample of the data. We randomly picked 12 event
types and discovered rules over their corresponding instance
datasets. For each selected rule, we sampled 1% of the tuples, making sure that (i) at least 150 tuples comprising 5
different entities were selected, (ii) both popular and rare
entities were selected, and (iii) at most 150 tuples were annotated for one single entity. Each tuple has been manually
validated with sources such as Twitter accounts for persons,
LinkedIn official web pages for companies, and stock brokers. Table 1 gives the details about the selected events,
representative rules, and size of the samples.
Given the ground data and the results of the cleaning,
we follow common practices from the data cleaning literature [3, 10] to evaluate the quality of the obtained repair.
We count as correct detections the updates in the repair that
correctly identify dirty values. This corresponds to measuring the effectiveness of repairs based on delete-semantics,
where tuples with errors are removed. We count as correct
changes the updates in the repair that are equal to the original values in the ground truth. Based on these two met,
rics, we can then measure precision P = |changes∩errors|
|changes|
which corresponds to correct changes in the repair, recall
R = |changes∩errors|
, which corresponds to coverage of the
|errors|

Dataset. We obtained a 3-month snapshot of data extracted by Recorded Future, a leading web data analytics
company. The dataset has about 188M JSON documents
with a total size of about 3.9 TB. Each JSON document
contains extracted events defined over entities and their attributes. An entity can be an instance of a person, a location, a company, and so on. Events have also attributes.
In total, there are 150M unique event instances excluding
meta-events such as co-occurrence.

Frequency

1e+05

1e+03

1e+01
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●●
●
●
●●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●●● ●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●● ● ●●
●
●●
●
●
● ●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
● ●
●
● ●
●●
●
●
●●●
●
●
●● ●
●
●
● ●●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
● ●● ● ●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
● ●
●
●
●●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ● ● ●●
●● ●● ●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●● ●
●●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●●
● ● ●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●● ●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●●●●
●●●
●●
●●
●
●
●● ●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●●
●
●
●
●
●
● ●
●
●●
●
●
●●
●●
●●
●●●
●
●
●●●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●●● ●
●●
● ●
●
●
●● ●●
●
●●●
●
●●
● ●●
●
●● ●
●●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●●
●
●
●●
●
●
●●
●●
●
●●
●
●
●
●●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●

1500

1700

1900

2100

Year

Figure 7: Time distribution for events’ timestamp.
The data contains events from circa 15 th to 21 st century.
Figure 7 shows events across years, where each point corresponds to the number of events in a day. Most of the events
occur around Fall 2014, because the data is mostly around
the 3 months covered by the snapshot. Starting from January 2015 onward, the reported events are future forecasts.
Metrics. We crafted ground rules for all events to evaluate
the discovery algorithms. These rules are dependencies that
are semantically correct. We report their number for every
event in Table 1, where we also report as “Coverage” the
ratio of the distinct number of attributes in the set of rules
to the total number of attributes for that event. We then
use this ground truth to evaluate the precision and recall
over the top-k dependencies returned by the algorithms.
In the case of Tfds, crafting also their ground durations is
more challenging since there are rarely clear duration values
that can be set. We argue that it is not correct to compare
the discovered duration value versus an arbitrary manually
set ground truth; different persons may come up with different durations. We also show that a combination of these
arbitrary values does not lead to the best duration value.
Distance between the discovered duration and the real one

×R)
errors, and F-measure F = 2×(P
.
(P +R)
Results. We start with evaluating the discovery of approximate Fds. Table 2 shows the obtained precision and recall
values. We rank the approximate Fds based on their scores
and compute the precision and recall @k= {1,3,5}, where k
is the number of dependencies evaluated after they are ordered with decreasing scores. On average, the precision and
recall @k of the NPMI-sorted results is significantly higher
than the CORDS-sorted results. The significance is most
obvious @k = 1. On average, the NPMI scoring approach
clearly yields better results than the baseline. This is not
surprising, since CORDS was designed to discover approximate Fds on relational databases and it is reported to re-

343

Event
Acquisition
Company Employees #
Company Meeting
Company Ticker
Credit Rating
Employment Change
Insider Transaction
Natural Disaster
Person Travel
Political Endorsement
Product Recall
Voting Result
Avg

k=1
Prec Rec
1
0.25
1
1
1
0.14
1
0.5
1
0.16
0
0
1
0
1
1
0
0
1
1
0
0
1
0.5
0.75 0.38

NMPI
k=3
Prec Rec
0.66
0.5
0.5
1
1
0.42
0.33
0.5
1
0.5
0.66 0.18
1.0
0.01
0.5
1
0.33
0.5
0.5
1
0.66 0.17
1
1
0.68 0.57

k=5
Prec Rec
0.8
1
0.5
1
1
0.7
0.4
1
1
0.83
0.6
0.27
1.0
0.02
0.5
1
0.4
1
0.5
1
0.8
0.33
1
1
0.71 0.76

k=1
Prec Rec
1
0.25
0
0
1
0.14
0
0
1
0.16
0
0
0
0
1
1
0
0
0
0
0
0
1
0.5
0.42 0.17

CORDS
k=3
Prec Rec
0.66
0.5
0.5
1
0.66 0.28
0
0
1
0.5
0.33 0.09
0
0.66
0.5
1
0.33
0.5
0.5
1
0.66 0.17
1
1
0.57 0.50

k=5
Prec Rec
0.6
0.75
0.5
1
0.6
0.42
0.2
0.5
1
0.83
0.4
0.18
0.01 0.02
0.5
1
0.4
1
0.5
1
0.8
0.33
1
1
0.62 0.67

Table 2: Precision/Recall of approximate FD discovery for sample events over 3 months of data.
Event

quire a sample of size between 1k and 2k pairs [20]. Such
amounts of data are not always available when the data is
chunked into time buckets.
We analyze next how different duration values for the
same rule impact the quality of the repairs. In Figure 8(a),
the event is Acquisition. Since a company is usually acquired
only once, the considered rule is a Fd as it is demonstrated
by the improvement in the quality of the repair for both precision and recall when the duration exceeds 3600 days (10
years). The explanation is that for smaller values, the rules
cannot detect errors. As expected, the results in terms of
detection (delete semantics) are much better than the ones
that consider the modification of problematic values (update
semantics). In particular, there is not enough redundancy
in the data to find the correct update for the repair. In Figure 8(b), the event reports the number of employees for a
company. As this information changes over time, different
durations lead to different quality results in the repair. Intuitively, if the time is small, precision is favored over recall,
and the other way around with large values. This is the
behavior we observed for all events with temporal characterization. Also in this case, the detection has much better
performance than the metric considering also the values of
the updates. Finally, Figure 8(c) reports a case where there
is a clear point in which precision falls to low values when
the duration increases. In this case, the duration is too large
and covers several changes of employment for a person, thus
several correct values are detected as problematic.
In Table 3, we report the discovered minimum duration M
and the cleaning quality results with the Repair-Outliers
(RO) and No Repair-Probability (NP) approaches. We
compare them against (i) the results obtained using a Fd,
(ii) the average of the durations suggested by three domain
experts, (iii) the best duration value for the rule, selected
with the previous study (as in Figure 8). The first three
rules do not depend on time since they have only one correct reference value in our dataset. Hence, the Fds perform
best for this case. The duration discovery algorithm was not
able to find a duration that is large enough to make the Tfd
perform better. This is because our dataset mainly contains
events that happened within three months and the discovery approach subsequently suffers from the limited timespan
when identifying these larger durations. Interestingly, values
for the remaining events change over time. In these cases,
the durations discovered with our RO approach always lead

Company Emp #
Company Emp #
Company Meet.
Company Meet.
Credit Rating
Credit Rating
Emp. Change
Emp. Change
Natural Disaster
Natural Disaster
Person Travel
Person Travel
Pol. Endorsement
Pol. Endorsement
Product Recall
Product Recall
Voting Result
Voting Result

Entity

Conditional
M
F
Wal-Mart
24 0.82
Tesco
27
1.0
Val. Pharm. Int.
217 0.68
Wal-Mart
45 0.57
Tysons Foods
53
1.0
NY Method. Hosp.
72 0.66
Sean Moriarty
3168 1.0
Rodney Reid
12
1.0
Argentina
45 0.57
England
7
0.67
C. Ronaldo
24 0.71
Lady Gaga
26 0.73
Ron Paul
96
0.52
Sarah Palin
20.5 0.75
vehicle
108 0.76
cars
32
1.0
Afghanistan
24 0.76
United States
6
0.33

Global
M
F
24 0.82
24 1.0
336 0.68
336 0.57
48 1.0
48 0.0
24 1.0
24 1.0
24 0.57
24 0.6
48 0.69
48 0.69
48 0.54
48 0.75
177 0.76
177 0.89
24 0.76
24 0.86

Table 4: Comparison of F-measure results for conditional and global TFDs.
to better a F-measure value than the Fds and the alternative approach NP. Moreover, in several cases we are able to
achieve the same precision and recall of the best duration
from the previous study. The other Tfd approaches NR
and AL performed similarly to RO on these datasets with
failures in some cases. For example AL discovers a duration of 0 for Voting Result, while NR fails with the events
with higher noise rate, such as Company Employees #. We
shall elaborate on the differences of the Tfd approaches in
more detail in the next subsection. Finally, the average of
the durations collected from the three domain experts show
poor results in terms of F-measure, with the exception of
the Person Travel case. This confirms that manually crafting the correct durations for data cleaning is a hard problem
to be tackled top-down; a bottom-up approach that mines
the data leads to more useful results.
While the minimum durations from Table 3 can be applied
for all the entities, we report in Table 4 minimum duration
values for popular entities over all events. In our approach,
the specific minimum duration for each entity can be computed before the aggregation of the stripes. For popular
entities, these values can lead to better cleaning results. For
example, while the discovered minimum duration for Person Travel is 48 hours (Table 3), conditional rules for pop-

344

P	
  repair	
  

R	
  repair	
  

P	
  detect	
  

R	
  detect	
  

1	
  

1	
  

P	
  repair	
  

R	
  repair	
  

P	
  detect	
  

R	
  detect	
  

P	
  repair	
  

0.8	
  

0.8	
  

0.6	
  

0.6	
  

0.6	
  

0.4	
  

0.4	
  

0.4	
  

0.2	
  

0.2	
  

0.2	
  

0	
  

0	
  

1	
  

10	
  

100	
   1000	
   10000	
  100000	
  
Days	
  

(a) Acquisition (FD)

R	
  repair	
  

P	
  detect	
  

R	
  detect	
  

1	
  
0.8	
  

1	
  

10	
  

100	
  

1000	
   10000	
   100000	
  

Hours	
  

(b) Company Employee # (TFD)

0	
  
1	
  

100	
  
Hours	
  

10000	
  

(c) Employment Change (TFD)

Figure 8: Cleaning results with different durations.
Event

TFD Semantic
M
P
R
Acquisition
48 1.0 0.08
Company ticker
24 0.43 0.25
Insider transaction
24 1.0 1.0
Company Employees # 24 0.74 0.17
Company Meet.
336 0.94 0.5
Credit Rating
48 0.6 0.75
Employment Change
24 1.0 0.88
Natural Disaster
24 0.8 0.5
Person Travel
48 0.61 0.82
Political Endorsement 48 1.0 0.59
Product Recall
177 0.9 0.9
Voting Result
24 1.0 0.6

(RO)
TFD Semantic (NP)
F
M
P
R
F
0.16
840 0.92 0.21 0.34
0.31
1
0.69 0.14 0.23
1.0
264 1.0 1.0 1.0
0.27 1344 0.37 0.17 0.23
0.65
5k
0.4 0.54 0.46
0.67
72 0.56 0.75 0.64
0.94 14k 0.39 0.73 0.51
0.62
29
0.8 0.5 0.62
0.7
72 0.59 0.84 0.69
0.74 216 0.85 0.65 0.73
0.9 7,033 0.41 0.9 0.56
0.75 816 0.79 0.71 0.75

FD
P
0.96
0.96
1.0
0.24
0.38
0.18
0.37
0.51
0.42
0.52
0.38
0.31

semantic
R
F
0.46 0.62
1.0 0.98
1.0 1.0
0.19 0.22
0.53 0.44
0.66 0.29
0.8 0.51
0.91 0.65
0.93 0.58
0.88 0.65
0.9 0.53
0.9 0.59

Humans
Best possible F
M
F
M
P
R
F
0.96 0.46 0.62
0.96 1.0 0.98
1.0 1.0 1.0
1016 0.23
48
0.73 0.20 0.31
4560 0.46
720
0.88 0.53 0.67
4680 0.55
24
0.69 0.75 0.72
12k 0.5
≤720
1 0.88 0.94
255 0.86 [168:500] 0.93 0.78 0.86
36 0.73
24
0.92 0.85 0.88
1200 0.68
[24:70]
1.0 0.59 0.74
352 0.9 [100:400] 0.9 0.9 0.9
4440 0.57
720
0.83 0.75 0.79

Table 3: Precision and recall of the error detection based on duration discovery approaches (M in hours).
ular entities yield higher F-measure than the unconditional
Tfds. Interestingly, there is a case where the conditional
Tfd performs worse that the non-conditional one. Since in
the US there can be multiple elections in different states in
the same day, the algorithm mines a very low duration of 6
hours. This suggests that Voting Result extractors can be
revised to consider American states, instead of one country.

1"

6"

Execution times. Aetas’s runtime is dominated by the
time needed for reading the data from a database. For the
largest dataset, CompanyTicker with more than one million
tuples, and without early termination, Aetas took a total
time of 53 seconds from which 52 seconds were spent to identify the implications and 1 second to discover the minimum
duration for a chosen implication. With early termination,
the process takes less than 2 seconds. The dependency cube
is also easy to maintain in memory as we handle one cube
per Fd at a time and its size is bound by the number of
tuples. Also, when imputing missing timestamps, we do
not materialize their values in the stripe, and we implicitly
maintain the time sequence for a non changing value.

1	
  

0.8	
  

0.8	
  

0.6	
  

0.6	
  

5"

0.6"

4"

0.4"

3"

F.measure"

Total"9me"

0"
0.1" 0.2" 0.3" 0.4" 0.5" 0.6" 0.7" 0.8" 0.9" 1"

delta%

(a) Early termination δ

0.4	
  

2"
1"

0"

TIme%(s)%

F*measure%

0.8"

0.2"

1	
  

7"

Figure 9(b) shows that the discovery algorithm behaves
as expected with respect to the cut-off parameter c: low
cut-off points lead to high precision and higher values lead
to higher recall. This property allows the user to tune the
discovery for their target application requirements. Interestingly, increasing values for c show similar behavior for both
RO and NP, and the default value of c = 10 is close to the
max F-measure value for both methods.

0.2	
  

0.4	
  

Recall	
  

F-­‐measure	
  

Precision	
  

0	
  

0.2	
  
0	
  

5	
   10	
   15	
   20	
   25	
   30	
   35	
   40	
   45	
   50	
  
c	
  

(b) Cut off point c with RO

Figure 9: Study of the input parameters averaged
over 9 temporal events: (a) Execution time and Fmeasure for the top-5 rules, (b) Precision, Recall,
and F-measure of the repair for RO.

5.2

Synthetic Data

The goal of the experiments with synthetic data is to analyze how the discovery algorithms perform wrt different
properties of the data.
Dataset. In each scenario, we generate S sources with information over events for O objects for T timestamps. The
generation of the values follows a Tfd with a given Mg .
Each tuple for a source has an attribute for the entity (reference value), an attribute value for the Tfd, and a timestamp. For example, for Tfd name ∧ ∆ → position with
∆=[0, 2] would generate tuples such as (Jay, worker, 1),
(Jay, worker, 2), (Jay, manager, 3), (Jay, manager, 4),

Parameters. In the above experiments the early termination threshold δ and the cut-off point c for trimming and
duration discovery were set to 0.7 and 10%, respectively.
We report in Figure 9(a) how different values for δ affect
the F-measure of the top-5 dependencies discovered with
our method. We observe that aggressive pruning leads to
faster execution, but to a loss in the quality of the results.
However, an early termination with δ=0.7 reduces the execution from 52 to 6 seconds and preserves the quality.

345

(a) Reporting error Pr

(b) Value error Pe

(c) Time error Pt

(d) Multiple sources

Figure 10: Ratio of the mined duration M wrt the golden duration Mg with different kinds of errors.
(Jay, manager, 5), (Jay, manager, 7), and (Jay, clerk, 8).
For each timestamp and entity, a source has a probability
Pr of not reporting the current value in a tuple, a probability Ph of changing the value (with the current duration up
to a given maximum value), a probability Pe of reporting
a wrong value, and a probability Pt of reporting a wrong
timestamp. We run the different discovery algorithms on
the union of the data from all sources. All experiments have
been carried out for 1000 reference values and the probability of changing the value Ph was set to 0.2.
Metrics. In this controlled environment, we know the properties of our generative model, such as the golden values for
Mg . We can thus measure the quality of the mining as the
ratio of the discovered duration M to the input duration Mg .
A ratio of 1 shows that the method has correctly mined M .
Results. To evaluate the influence of errors and sparseness
in the data, we created different scenarios by varying different parameters. We first tested each type of error, namely
missing values, wrong values, and wrong timestamps in isolation, i.e., we varied the probability of the error at hand
from 0 to 0.6 and fixed the others probabilities to zero. We
had 12 experiments for each error type by varying (i) the
number of reporting sources from 2 to 5, and (ii) Mg to 7,
12, 17. The maximum duration (the time an event holds)
was fixed at Mg + 5. From these experiments, we collected
12 M/Mg ratios, and took their median. We then tested the
role of sources and skewed error rates for our method. In
this experiment, we considered 1, 4, 7, and 10 sources, all of
them with Pr =0.1 and Pt =0.2, and three cases. In case 1,
the first source has Pe =0.1, and at each step we add three
new sources with Pe values 0.1, 0.3, and 0.5. Similarly, in
case 2 (resp. 3), the first source has Pe =0.2 (resp. 0.3), and
at each step we add three sources with Pe values 0.2, 0.4,
and 0.6 (0.3, 0.5, and 0.7 resp.).
Figure 10 shows the overall results. We see that RO performs better than the baselines with all error types. In Figure 10(a), it is easy to see that both RO and NP are robust
to missing values, AL performs poorly because it cannot
align stripes when values do not match, and the absence of
integration leads to several missing values for NR. With errors in the values (Figure 10(b)) RO is the only one able
to perform well with high percentages of noise, while the
other methods experience a big drop in performance. In Figure 10(c)), we see that RO is robust to errors in the timestamps and computes better durations that the others (NP
drops in performance at 0.3). Finally, Figure 10(d) shows
that increasing the number of sources leads to improvement

in the mining. The combination of missing values, errors in
the timestamps, and very erroneous sources make the problem more challenging. In particular, a useful duration is
discovered starting with seven sources in all cases.

6.

RELATED WORK

Our work is related to two main areas, namely, dependency discovery and temporal data management.
In the context of constraints discovery, Fds attracted the
most attention. TANE is a representative for the schemadriven approach to discovery [19], while FASTFD is an
instance-driven approach [32]. Recently, DFD has also been
proposed with improvements in performance [1]. While all
these methods are proven to be valid over clean data, few
solutions have been proposed for discovery over noisy data.
An extension in this direction is the discovery of approximate Fds that hold for a subset of a relation with respect
to a given threshold [23]. A similar extension has been proposed to mine approximate Tfds [11]. The major drawback
of approximate Fds on noisy data is that from a certain
threshold of noise on, such as 26% in our real world data
scenario, the results of the discovery approach will mix up
useful approximate Fds with actual non-dependent columns.
Another aspect of discovering constraints is to measure their importance according to a scoring function.
CORDS [20], which we use as baseline for approximate Fds
discovery, uses statistical correlations for each column pair
to score possible Fds. In conditional functional dependencies (Cfds) discovery, other measures have been proposed,
including support, which is defined as the percentage of the
tuples in the data that match the pattern tableaux (the constants) and χ2 test [8, 14]. Song et al. introduced the concept of differential dependencies [31] by extending Fds with
differential functions, which are dependencies that change
over time. They also mine dependencies, but they have focused on identifying dependencies on clean data only.
Integration and cleaning with temporal data [2, 9, 25, 27]
is also of interest. The related approaches can benefit from
our algorithms. The Prawn integration system [2] can use
our Tfds to detect errors, while the record-linkage systems
for temporal data can exploit our repair-based duration discovery for their mining of temporal behavior. In fact, their
goal is to identify records that describe the same entity over
time and understanding how long a value should hold is
critical for their algorithms. In particular, we adapted the
disagreement decay discovery algorithm from [27] to our setting and indeed it can be applied for minimum duration dis-

346

covery. From the experimental study, it is clear that our
algorithm does better because of the improved robustness
wrt the noise in the data. Notice that, differently from [27],
noisy data cannot be clustered with good results. We therefore decided to go directly to the tuple-pair comparisons in
the cleaning step, and this aggressive cleaning is supported
by the experimental results with low execution times and
good results in terms of quality. Another application for
our rules is truth discovery [15, 28, 29, 33].

7.

[11] C. Combi, P. Parise, P. Sala, and G. Pozzi. Mining
approximate temporal functional dependencies based on
pure temporal grouping. In ICDMW, pages 258–265, 2013.
[12] M. Dallachiesa, A. Ebaid, A. Eldawy, A. Elmagarmid, I. F.
Ilyas, M. Ouzzani, and N. Tang. NADEEF: A Commodity
Data Cleaning System. In SIGMOD, pages 541–552, 2013.
[13] X. L. Dong, E. Gabrilovich, G. Heitz, W. Horn, K. Murphy,
S. Sun, and W. Zhang. From data fusion to knowledge
fusion. PVLDB, 7(10):881–892, 2014.
[14] W. Fan, F. Geerts, J. Li, and M. Xiong. Discovering
conditional functional dependencies. IEEE TKDE,
23(5):683–698, 2011.
[15] A. Galland, S. Abiteboul, A. Marian, and P. Senellart.
Corroborating information from disagreeing views. In
WSDM, pages 131–140, 2010.
[16] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. The
LLUNATIC data-cleaning framework. PVLDB,
6(9):625–636, 2013.
[17] A. Gelman and J. Hill. Data analysis using regression and
multilevel/hierarchical models. Cambridge U. Press, 2006.
[18] M. Gupta, J. Gao, C. C. Aggarwal, and J. Han. Outlier
Detection for Temporal Data. Synthesis Lectures on Data
Mining and Knowledge Discovery. Morgan & Claypool
Publishers, 2014.
[19] Y. Huhtala, J. Kärkkäinen, P. Porkka, and H. Toivonen.
TANE: An efficient algorithm for discovering functional
and approximate dependencies. Comput. J., 42(2):100–111,
1999.
[20] I. F. Ilyas, V. Markl, P. J. Haas, P. Brown, and
A. Aboulnaga. CORDS: Automatic discovery of
correlations and soft functional dependencies. In SIGMOD,
pages 647–658, 2004.
[21] C. S. Jensen, R. T. Snodgrass, and M. D. Soo. Extending
existing dependency theory to temporal databases. IEEE
Trans. Knowl. Data Eng., 8(4):563–582, 1996.
[22] Z. Khayyat, I. F. Ilyas, A. Jindal, S. Madden, M. Ouzzani,
P. Papotti, J.-A. Quiané-Ruiz, N. Tang, and S. Yin.
Bigdansing: A system for big data cleansing. In SIGMOD,
pages 1215–1230, 2015.
[23] J. Kivinen and H. Mannila. Approximate inference of
functional dependencies from relations. In ICDT, pages
129–149, 1995.
[24] S. Kolahi and L. V. S. Lakshmanan. On approximating
optimum repairs for functional dependency violations. In
ICDT, pages 53–62, 2009.
[25] F. Li, M. Lee, W. Hsu, and W. Tan. Linking temporal
records for profiling entities. In SIGMOD, pages 593–605,
2015.
[26] H. Li and N. Homer. A survey of sequence alignment
algorithms for next-generation sequencing. Briefings in
bioinformatics, 11(5):473–483, 2010.
[27] P. Li, X. L. Dong, A. Maurino, and D. Srivastava. Linking
temporal records. PVLDB, 4(11):956–967, 2011.
[28] X. Li, X. L. Dong, K. Lyons, W. Meng, and D. Srivastava.
Truth finding on the deep web: Is the problem solved?
PVLDB, 6(2):97–108, 2012.
[29] J. Pasternack and D. Roth. Making better informed trust
decisions with generalized fact-finding. In IJCAI, pages
2324–2329, 2011.
[30] S. Truvé. A white paper on temporal analytics.
www.recordedfuture.com/assets/RF-White-Paper.pdf.
[31] S. Song, L. Chen, and H. Cheng. Efficient determination of
distance thresholds for differential dependencies. IEEE
Trans. Knowl. Data Eng., 26(9):2179–2192, 2014.
[32] C. M. Wyss, C. Giannella, and E. L. Robertson. FastFDs:
A heuristic-driven, depth-first algorithm for mining
functional dependencies from relation instances. In
DaWaK, pages 101–110, 2001.
[33] B. Zhao, B. I. Rubinstein, J. Gemmell, and J. Han. A
bayesian approach to discovering truth from conflicting
sources for data integration. PVLDB, 5(6):550–561, 2012.

CONCLUSION

We presented Aetas, a system for the discovery of approximate temporal functional dependencies. At the core of
the system are two modules that exploit machine learning
techniques to identify approximate dependencies and their
durations from noisy web data. As we have shown in the experimental study, traditional Fds lead to poor results when
used on a temporal dataset in a data cleaning system. On
the contrary, temporal dependencies can improve the quality of the data; our system is able to discover Tfds with
minimal interactions with the users and with better results
than alternative methods.
As a future direction, we plan to mine Tfds that identify large extreme values over the duration distributions,
i.e., outlying durations that are too long for a certain event.
For example, in many countries politicians have a maximum
number of mandates for a certain position. We also plan to
extend our duration discovery algorithm with more sophisticated methods for temporal outlier detection [18].

8.

ACKNOWLEDGMENTS

This research was supported by Qatar Computing Research Institute (QCRI). The research in this paper used
data kindly provided by Recorded Future.

9.

REFERENCES

[1] Z. Abedjan, P. Schulze, and F. Naumann. DFD: efficient
functional dependency discovery. In CIKM, pages 949–958,
2014.
[2] B. Alexe, M. Roth, and W.-C. Tan. Preference-aware
integration of temporal data. PVLDB, 8(4):365–376, 2014.
[3] G. Beskales, I. F. Ilyas, and L. Golab. Sampling the repairs
of functional dependency violations under hard constraints.
PVLDB, 3(1):197–207, 2010.
[4] P. Bohannon, W. Fan, M. Flaster, and R. Rastogi. A
cost-based model and effective heuristic for repairing
constraints by value modification. In SIGMOD, pages
143–154, 2005.
[5] P. Bohannon, W. Fan, F. Geerts, X. Jia, and
A. Kementsietsidis. Conditional functional dependencies for
data cleaning. In ICDE, pages 746–755, 2007.
[6] G. Bouma. Normalized (pointwise) mutual information in
collocation extraction. In GSCL, pages 31–40, 2009.
[7] M. Bronzi, V. Crescenzi, P. Merialdo, and P. Papotti.
Extraction and integration of partially overlapping web
sources. PVLDB, 6(10):805–816, 2013.
[8] F. Chiang and R. J. Miller. Discovering data quality rules.
PVLDB, 1(1):1166–1177, 2008.
[9] Y.-H. Chiang, A. Doan, and J. F. Naughton. Modeling
entity evolution for temporal record matching. In
SIGMOD, pages 1175–1186, 2014.
[10] X. Chu, I. F. Ilyas, and P. Papotti. Holistic data cleaning:
Putting violations into context. In ICDE, pages 458–469,
2013.

347

What is the IQ of your Data Transformation System?
Giansalvatore Mecca1 Paolo Papotti2 Salvatore Raunich3 Donatello Santoro1,4
1

2

Università della Basilicata – Potenza, Italy
Qatar Computing Research Institute (QCRI) – Doha, Qatar
3
University of Leipzig – Leipzig, Germany
4
Università Roma Tre – Roma, Italy

ABSTRACT

and integration applications [16, 6]. In fact, it would be very useful, given a task that requires to translate some input instance of
a source schema into an output instance of the target schema, to
have a common model to answer the following fundamental question: “what is the right tool for my translation task?”
Answering this question entails being able to compare and classify systems coming from different inspirations and different application domains. For this purpose, several benchmarks have been
proposed [3, 29]. In this paper, we concentrate on an ambitious
task that has not been addressed so far, i.e., we aim at measuring
the level of intelligence of a data transformation system, in order to
base the comparison upon this measure.
In our vision, the level of intelligence of the internal algorithms
of a tool can be roughly defined as the ratio between the quality of
the outputs generated by the system, and the amount of user effort
required to generate them. In other terms, we want to measure,
for each system, how much effort it takes to obtain results of the
highest possible quality.
To make this rather general intuition more precise, we need several tools: (i) a notion of data-transformation system that is sufficiently general to capture a wide variety of the tools under exam,
and at the same time tight enough for the purpose of our evaluation;
(ii) a definition of the quality of a data translation tool on a mapping scenario; (iii) a definition of the user-effort needed to achieve
such quality.

Mapping and translating data across different representations is a
crucial problem in information systems. Many formalisms and
tools are currently used for this purpose, to the point that developers typically face a difficult question: “what is the right tool for
my translation task?” In this paper, we introduce several techniques
that contribute to answer this question. Among these, a fairly general definition of a data transformation system, a new and very efficient similarity measure to evaluate the outputs produced by such a
system, and a metric to estimate user efforts. Based on these techniques, we are able to compare a wide range of systems on many
translation tasks, to gain interesting insights about their effectiveness, and, ultimately, about their “intelligence”.
Categories and Subject Descriptors: H.2 [Database Management]:
Heterogeneous Databases
General Terms: Algorithms, Experimentation, Measurement.
Keywords: Data Transformation, Schema Mappings, ETL, Benchmarks.

1.

INTRODUCTION

The problem of translating data among heterogeneous representations is a long-standing issue in the IT industry and in database
research. The first data translation systems date back to the seventies. In these years, many different proposals have emerged to
alleviate the burden of manually expressing complex transformations among different repositories.
However, these proposals differ under many perspectives. There
are very procedural and very expressive systems, like those used in
ETL [18]. There are more declarative, but less expressive schemamapping systems. Some of the commercial systems are essentially
graphical user-interfaces for defining XSLT queries. Others, like
data-exchange systems, incorporate sophisticated algorithms to enforce constraints and generate solutions of optimal quality. Some
systems are inherently relational. Others use nested data-models to
handle XML data, and in some cases even ontologies.
In light of this heterogeneity, database researchers have expressed a strong need to define a unifying framework for data translation

1.1

Contributions

We develop several techniques that contribute to give an answer
to the question above.
(i) We introduce a very general definition of a data-transformation system, in terms of its input-output behavior; differently from
earlier approaches that have focused their attention on the actual
specification of the transformations, we see a system as a black box
receiving as input some specification of the mapping task and an
instance of the source schema, and producing as output an instance
of the target schema; then, we analyze the system in terms of this
input-output function.
(ii) We define the notion of quality of a data transformation tool on
a given scenario as the similarity of the output instance wrt the expected instance, i.e., the “right” solution that the human developer
expects for a given input. Notice that we allow nested data models and XML data, and therefore measuring the quality of an output
imposes to compare two different trees, notoriously a difficult problem, for which high-complexity techniques are usually needed. We
show, however, that for the purpose of this evaluation it is possible
to define an elegant and very efficient similarity measure that provides accurate evaluations. This comparison technique is a much
needed contribution in this field, since it is orders of magnitude

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CIKM’12, October 29–November 2, 2012, Maui, HI, USA.
Copyright 2012 ACM 978-1-4503-1156-4/12/10 ...$15.00.

872

Figure 1: Sample Scenario in the GUI of an Open-Source Mapping System
faster than typical edit-distance measures, and scales up to large
instances. In addition, it concretely supports the mapping improvement process, since it returns detailed feedback about mismatches
between the two trees. By doing this, it helps users in understanding why their mapping is faulty, and proves much more effective
than simple yes/no measures used in previous benchmarks.

for a number of operations like pivoting, aggregates, rollups [18]
that are not natively supported by other systems in our evaluation.
As a consequence, in any application scenario in which these primitives are needed, the developer has very little choice. However,
there are a number of transformation tasks for which tools of different kinds are indeed applicable, and therefore it makes sense to
compare their level of effectiveness in carrying out these specific
tasks. In fact, this work represents a concrete investigation of the
trade-offs between declarative and procedural approaches, a foundational problem in computer science.
Using the framework proposed in this paper, we conduct a systematic evaluation of data transformation tools. We strongly believe that this evaluation provides precious insights in the vast and
heterogeneous world of transformation systems, and may lead to a
better understanding of its different facets.

(iii) Transformation systems typically require users to provide an
abstract specification of the mapping, usually through some graphical user interface; to see an example, consider Figure 1, which
shows the graphical specification of a mapping scenario. The figure
shows the source and target schema with a number of correspondences used to provide a high-level specification of the mappings,
as it is common in this framework. Based on the mapping specification, given a source instance the system generates an output, i.e.,
a target instance such as the ones shown on the right. Despite the
fact that different tools have usually different primitives to specify
the transformations, it is still possible to abstract the mapping specification as a labeled input graph; our estimate of the user effort is
a measure of the size of an encoding of this graph inspired by the
minimum description length principle in information theory [19].
We believe that this measure approximates the level of user effort
better than previous measures that were based on point-and-click
counts [3].

• On one side, it may be the basis for a new and improved generation of benchmarks that extend the ones developed so far
[3, 29]. Besides providing a better guide to data architects,
this would also help to identify strong and weak points in
current systems, and therefore to elaborate on their improvements.
• In this respect, our framework represents an advancement towards the goal of bringing together the most effective features of different approaches to the problem of data translation. As an example, it may lead to the integration of more
sophisticated mapping components into ETL workflows [9].

(iv) We develop a working prototype of our techniques, and use
it to conduct a comprehensive evaluation of several data transformation systems. In this evaluation, we are able to gain a deeper insight about data transformation tools that exist on the market and in
the research community, based on a novel graphical representation,
called quality-effort graphs. More specifically, we fix a number of
representative scenarios, with different levels of complexity, and
different challenges in terms of expressive power. Then, we run
the various tools on each scenario with specifications of increasing
efforts, and we measure the quality of the outputs. The introduction of quality-effort graphs is a major contribution of this paper: it
allows us to derive several evidences about how much intelligence
the internal algorithms of a tool put into the solution, i.e., how fast
the quality of solutions increases with respect to the increasing effort.

• Finally, as it will be discussed in the following sections, it
provides a platform for defining test scenarios for data exchange systems, another missing component in the mapping
ecosphere.

1.2

Outline

The paper is organized as follows. Section 2 introduces the notion of a transformation system. The quality measure is introduced
in Section 3, its complexity in Section 4. User efforts are discussed
in Section 5. We introduce ways to define a scenario and to select
gold standards in Section 6. Experiments are reported in Section 7.
Related works are in Section 8, conclusions in Section 9.

We want to make it clear that our primary goal is not comparing
transformation systems in terms of expressiveness, as it has been
done in previous benchmarks [3]. In fact, our approach has been
conceived to be applicable to a wide variety of tools, which, as we
discussed above, have rather different inspiration and goals. For
example, in our evaluation, we consider both schema-mapping systems and ETL tools. It is well known that ETL tools are by far
more expressive than schema-mapping systems, since they allow

2.

TRANSFORMATION SYSTEMS

In our view, a data-transformation system is any tool capable of
executing transformation scenarios (also called mapping scenarios
or translation tasks). Regardless of the way in which transformations are expressed, in our setting these scenarios require to translate instances of a source schema into instances of a target schema.

873

• we use the primitives offered by TS to express the desired
transformation; this gives us a specification M - possibly in a
different formalism or query language wrt the expected one,
Me - that is supposed to have the same input-output behavior;

The transformation system is seen as a black box, of which we are
only interested in the input-output behavior, as shown in Figure 2.
For the purpose of our evaluations, we fix a set of translation
tasks. A translation task is defined in terms of a quadruple {S, T,
IS , Ie }, where: (i) S is the source schema; (ii) T is the target
schema; (iii) IS is an input instance, i.e., a valid instance of S; (iv)
Ie , the expected output, is an instance of T generated by applying
the desired transformation to IS .
Notice that the source and target schema may be either explicit,
or implicit, as it happens in many ETL tasks. To better express
the intended semantics of the transformation, it is possible that
also some specification of the mapping, Me , is given, in a chosen language. This, however, is not to be intended as a constraint
on the way in which the transformation should be implemented,
but rather as a means to clarify to developers the relationship between the expected output, Ie , and the input, IS , in such a way that
Ie = Me (IS ).

• we run M on the input instance, IS , to generate the output, a
target instance Ig ;
• then, we measure the quality achieved by the system by comparing Ig to our expected output, Ie . If Ig = Ie , then TS
achieves 100% quality on that specific translation task. Otherwise, the quality achieved by TS is the measure of the similarity between Ig and Ie ;
• once the quality has been measured, we use the techniques in
Section 5 to measure the user-effort, and generate the qualityeffort graph.

3.

THE QUALITY MEASURE

As discussed in the previous Section, our idea is to evaluate the
quality of a tool by measuring the similarity of its outputs wrt a
fixed, expected instance that has been selected in advance using
one of the methods that will be introduced in Section 6.
Since we adopt a nested relational data model, our instances are
trees, as shown in Figure 1. While there are many existing similarity measures for trees, it is important to emphasize that none of
these can be used in this framework, for the following reasons:
(i) We want to perform frequent and repeated evaluations of each
tool, for each selected scenario, and for mapping specifications of
different complexity. In addition, we want to be able to work with
possibly large instances, to measure how efficient is the transformation generated by a system. As a consequence, we cannot rely on
known tree-similarity measures, like, for example, edit distances
[7], which are of high complexity and therefore would prove too
expensive.

Figure 2: Architecture of the Evaluation Framework
We want to emphasize the high level of generality with which
the various components of the architecture have been chosen.
First, as it was discussed in the previous section, we encode the
behavior of a system in terms of its input-output function. By doing
this, we allow ample space to the different formalisms that are commonly used to express the semantics of the transformation and the
mapping specification, Me ; it is in fact perfectly fine to specify the
mapping as a query Q in a concrete query language – say XQuery
– as STBenchmark does [3]. As an alternative, the transformation
can be expressed as a set of embedded dependencies (tgds and egds)
[10], as it happens in schema-mapping and data-exchange works.
To be even more general, it is also possible to assume that a procedural specification of the mappings is provided, as it happens in
some ETL tools.
Second, we are not constraining the primitives offered by the
systems under evaluation, nor the way in which users express the
transformation. This can be done through a GUI, or – going to the
opposite extreme – even by manually writing a query in an executable query language.
Finally, the data model according to which S and T are constructed is a generic one. For the purpose of this paper, we adopt
a nested-relational data model, as it will be detailed in the following Section, that provides a uniform representation for most of the
structures typically found in concrete data models – primarily relational and, to some extent, XML. However, as it will be clear in the
following Section, any data model based on collections, tuples of
attributes and relationships is compatible with our framework.
A distinctive feature of our approach is that we assume that a
“gold standard”, Ie , has been fixed for each scenario. We discuss
in detail how this can be done in Section 6. For the time being,
we want to emphasize that this is the basis of our evaluation. More
specifically, as shown in Figure 2, assume a translation task {S, T,
IS , Ie } is given, and we need to evaluate a transformation system
TS. We proceed as follows:

(ii) The problem above is even more serious, if we think that our
instances may be seen as graphs, rather than trees, as it will be
discussed in the following paragraphs; we need in fact to check
key/foreign-key references that can be seen as additional edges
among leaves, thus making each instance a fully-fledged graph.
Graph edit distance [13] is notoriously more complex than tree edit
distance.
(iii) Even if we were able to circumvent the complexity issues,
typical tree and graph-comparison techniques still would not work
in this setting. To see this, consider that it is rather frequent in mapping applications to generate synthetic values in the output – these
values are called surrogate keys in ETL and labeled nulls in dataexchange. In Figure 1, values D1, D2, I1, I2, I3 are of this kind.
These values are essentially placeholders used to join tuples, and
their actual values do not have any business meaning. We therefore
need to check if two instances are identical up to the renaming of
their synthetic values. We may say that we are rather looking for
a technique to check tree or graph isomorphisms [12], rather than
actual similarities.
It can be seen that we face a very challenging task: we need to
devise a new similarity measure for trees that is efficient, and at the
same time precise enough for the purpose of our evaluation.
In order to do this, we introduce the following key-idea: since the
instances that we want to compare are not arbitrary trees, but rather
the result of a transformation, we expect them to exhibit a number
of regularities; as an example, they are supposedly instances of a

874

• if n is an instance of a set node set(A : τ ), then enc(n) =
enc(father(n)).A;

fixed nested schema that we know in advance. This means that we
know: (a) how tuples in the instances must be structured; (b) how
they should be nested into one another; (c) in which ways they join
via key-foreign key relationships.
We design our similarity metric by abstracting these features of
the two trees in a set-oriented fashion, and then compare these features using precision, recall and ultimately F-measures to derive
the overall similarity. In the following paragraphs, we make this
intuition more precise.

3.1

• if n is an instance of a tuple node [A0 : v0 , A1 : v1 , . . . , An :
vn ], then enc(n) =
enc(father(n)).[Ai0 : enc(vi0 ), . . . , Aik : enc(vik )]
where vi0 . . . vik are the atomic values in the tuple, and Ai0 ,
. . . Aik appear in lexicographic order;
• if n is an instance of a base type τ , then enc(n) = value(n),
where value(n) equals n if n is a constant in dom(τ ), or the
string null if n is a placeholder in NULLS.

Data Model

We fix a number of base data types, τi – e.g., string, integer,
date etc. – each with its domain of values, dom(τi ), and a set
of attribute labels, A0 , A1 . . .. A type is either a base type or a
set or tuple complex type. A set type has the form set(A : τ ),
where A is a label and τ is a tuple type. A tuple type has the form
tuple(A0 : τ0 , A1 : τ1 , . . . , An : τn ), where each Ai is a label and
each τi is either a base type or a set type. A schema is either a set or
a tuple type. Notice that schemas can be seen as (undirected) trees
of type nodes. In the following, we will often blur the distinction
between a schema and the corresponding tree.
Constraints may be imposed over a schema. A constraint is either a functional dependency or an inclusion constraint – i.e., a foreign key – defined in the usual way [1]. Both schemas in Figure 1
are constructed according to this data model. It can be seen that the
source schema is relational, the target schema is nested.
Let us first formalize the notion of an instance of a schema in
our data model, as a tree that may contain constants and invented
values. More specifically, for each base type, τi , we consider the
constants in the corresponding domain dom(τi ). We also consider a countable set of special values, NULLS, typically denoted
by N0 , N1 , N2 . . . that are called placeholders – but in other terminologies have been called labeled nulls and surrogates – which
we shall use to invent new values into the target when required
by the mappings. An instance of the base type, τi , is a value in
dom(τi ) ∪ NULLS. Instances of base types are also called atomic
values.
Instances of a tuple type tuple(A0 : τ0 , A1 : τ1 , . . . , An : τn )
are (unordered) tuples of the form [A0 : v0 , A1 : v1 , . . . , An : vn ],
where, for each i = 0, 1 . . . , n, vi is an instance of τi . Instances
of a set type set(A : τ ) are finite sets of the form {v0 , v1 , . . . , vn }
such that each vi is an instance of τ . An instance of a schema
is an instance of the root type. Sample instances can be found in
Figure 1 (D1, D2, I1, I2, I3 are placeholders); from those examples it should be apparent that, like schemas, also instances can be
seen as undirected trees. In the following, we shall often refer to
tuple nodes in an instance simply as “tuples”.
As it is common in nested data model, we assume a partitioned
normal form (PNF) [26], i.e., at each level of nesting we forbid two
tuples with the same set of atomic values. In light of this, we also
forbid tuples whose atomic values are all placeholders.

3.2

Join identifiers are strings encoding the fact that the same placeholder appears multiple times in an instance. More specifically,
given two tuples t1 , t2 in an instance with atomic attributes A, B,
respectively, they are said to be joinable over attributes A, B if the
same placeholder N appears as the value of attribute A in t1 and
of attribute B in t2 . If t1 = t2 , i.e., we consider the same tuple
twice, then we require that A 6= B, i.e., A and B must be different
attributes.
We are now ready to define the identifiers associated with an
instance. Given an instance I of the target schema T , we define
two different sets of strings associated with I. The tuple ids:
tids(I) = {enc(t) | t is a tuple node in I}
and the join ids:
jids(I) = {enc(t1 ).A = enc(t2 ).B |
t1 , t2 are tuples in I joinable over A, B and
enc(t1 ).A, enc(t2 ).B appear in lexicographic order}
Tuple and join identifiers for the instances in Figure 1 are reported in Figure 3. It is worth noting that the computation of tuple
identifiers requires special care. As it can be seen in the figure, we
keep the actual values of placeholders out of our identifiers, in such
a way that two instances are considered to be identical provided
that they have the same tuples and the same join pairs, regardless
of the actual synthetic values generated by the system.

3.3

Instance Quality

Based on these ideas, whenever we need to compute the quality
of a solution generated by a system, Ig , we compare Ig to the expected output, Ie by comparing their identifiers. More specifically:
we first compute the tuple and join ids in Ie , tids(Ie ), jids(Ie ).
Then, we compute the actual ids in Ig , tids(Ig ), jids(Ig ), and measure their precision and recall wrt to tids(Ie ), jids(Ie ), respectively,
as follows:
|tids(Ig ) ∩ tids(Ie )|
|tids(Ig ) ∩ tids(Ie )|
rtids =
ptids =
|tids(Ig )|
|tids(Ie )|
|jids(Ig ) ∩ jids(Ie )|
|jids(Ig ) ∩ jids(Ie )|
pjids =
rjids =
|jids(Ig )|
|jids(Ie )|
As it is common, to obtain the distance between Ig and Ie , we
combine precisions and recalls into a single F-measure [31], by
computing the harmonic means of the four values as follows:
4
distance(Ig , Ie ) = 1 − 1
1
1
1
+
+
+ rjids
ptids
rtids
pjids

Identifiers

Given a mapping scenario as defined in Section 2, we consider
the target schema, T , and the expected solution, Ie . The features
we associate with a target instance are tuple and join identifiers.
Tuple identifiers are string encodings of paths going from the
root to tuple nodes. To introduce them, we shall first introduce a
function enc() that we recursively apply to nodes. Given a node n
in an instance tree, I, we denote by father(n) the father of n, if it
exists; for the root node, nroot , father(nroot ) is a special, dummy
node, ⊥ such that enc(⊥) equals the empty string. Then, the enc()
function is defined as follows:

Figure 3 reports the values of precision and recall and the overall
F-measure for our example.
We want to emphasize the fact that our technique nicely handles placeholders. Consider for example instance Ie = {R(a, N1 ),
S(N1 , b)}, where a, b are constants, and N1 is a placeholder. Any
instance that is identical to Ie up to the renaming of placeholders –

875

Figure 3: Comparing instances. Instance A is the expected output, B is the generated output
like, for example, Ie = {R(a, N2 ), S(N2 , b)} – has distance 0 wrt
Ie . On the contrary, instances with different constants, and/or additional/missing tuples, are considered different and have distance
greater than 0.
Notice also that our approach also allows us to easily detect
the actual differences with respect to the expected output, i.e., tuples/surrogates that were expected and were not generated, and unexpected tuples/surrogates. Consider for example tuple ids; we define the set of missing tuples and the set of extra tuples as follows
(here − is the set-difference operator):
missingTuples(Ig , Ie ) = tids(Ie ) − tids(Ig )
extraTuples(Ig , Ie ) = tids(Ig ) − tids(Ie )
Similarly for missing joins and extra joins:
missingJoins(Ig , Ie ) = jids(Ie ) − jids(Ig )
extraJoins(Ig , Ie ) = jids(Ig ) − jids(Ie )
When reported to users, these sets represent a precious feedback,
since they clearly point out what are the tuples and surrogates that
cause mismatches between the expected solution and the one generated by a system. In other words, our similarity measure provides
two different comparison indicators: the first one is a number measuring the overall similarity of the two instances; the second one is
a detailed list of “edits” (tuple additions, tuple deletions, surrogate
replacements) that should be applied to the instances to make them
equal. In this respect, it is very similar to a traditional edit distance
measure, as it will be discussed in Section 4.

4.

occurrences ofa placeholder in one of the instances, we generate
nmax = omax
identifiers at most for each placeholder. Note that
2
omax is usually quite low, in the order of 2 or 3; moreover, it typically depends on the mapping and it is independent from n. As a
consequence, we shall approximate the number of join identifiers
by O(p1 + p2 ).
(iii) Finally, in order to compare the two instances and compute the
quality measure, we need to intersect the two identifier sets. To do
this, we can use a sort-merge algorithm, with a cost of O(t1 log(t1 )+
t2 log(t2 )) to compare tuple identifiers, and O(p1 log(p1 ) + p2 log(
p2 )) to compare join identifiers. Since, as discussed above, pi 
ni and we can approximate ti by ni , we have a total cost of O(n1
log(n1 ) + n2 log(n2 )). The overall time cost is therefore lower
than O(nlog(n)).
We are interested in comparing this bound with those of other
comparison techniques. In order to do this, we shall discuss two
different cases.
Case 1: General Case Let us first discuss the case in which the
two instances may contain placeholders. In this case, we find it useful to formalize the relationship between our instances, which we
have defined in Section 3 as undirected trees, and their graph counterpart. Given a schema T , and an instance I of T , the instance
graph associated with I contains all nodes and edges in I. In addition it contains an additional edge between each pair of distinct
leaf nodes labeled by the same placeholder N .
Based on this, we are not looking for identical instance graphs,
but rather isomorphic instance graphs, i.e., instance graphs that are
identical up to the renaming of placeholders. We say that two instance graphs G1 , G2 are isomorphic if there is a bijective mapping,
h, between the nodes of G1 and G2 such that: (i) for each pair of
nodes n1 , n2 in G1 there exists an edge between n1 and n2 if and
only if there exists and edge between h(n1 ) and h(n2 ) in G2 ; (ii)
in addition, the mapping h preserves the labels of non-placeholder
nodes, i.e., if n is labeled by a constant v, then also h(n) is labeled by v. We can state the following soundness property for our
algorithm:

COMPLEXITY AND EXPRESSIBILITY

Since our quality measure is based on the idea of comparing
instances, we believe it is important to explore its relationship to
other known tree-comparison techniques. As we have already noticed, the problem we deal with is at the crossroads of two different
problems: the one of computing tree similarity (like, for example,
tree edit distances), and the one of detecting graph isomorphisms
(due to the presence of placeholders). In this section, we establish
a number of results that relate the complexity and expressibility of
our framework to those of other known techniques. In particular,
we show that our technique is orders of magnitude faster than some
of the known alternatives.
Assume that we need to compare two instances, of n1 , n2 nodes,
t1 , t2 tuple nodes, and p1 , p2 placeholders, respectively. Notice
that ti < ni , pi < ni . Usually pi  ni . On the contrary, while
the number of tuples is strictly lower than the number of nodes, for
large instances ti is of the same order of magnitude as ni , since
tuples in sets are the primary factor of multiplicity for the instance
tree. Therefore, in the following, we shall approximate ti by ni .
Let us call n the maximum value of n1 , n2 . To establish a complexity bound, let us analyze the various steps of the algorithm.
(i) As a first step, our algorithm computes tuple identifiers. This
can be done by visiting the instance and keeping track of the labels
and identifiers of the visited nodes. This step has therefore a cost
of O(n1 + n2 ) and generates t1 + t2 identifiers.
(ii) Then, we generate join pairs. For each placeholder, during
the visit we also keep track of the identifiers of the tuples it appears in. To generate join pairs, we need to combine these identifiers in all possible ways. If we call omax the maximum number of

T HEOREM 4.1. Given two instances I1 , I2 , then distance(I1 ,
I2 ) = 0 if and only if the corresponding instance graphs G1 , G2
are isomorphic.
Notice that the general problem of computing graph isomorphisms is known to be in NP, and only high complexity algorithms
are currently known for its solution [12]. This makes these techniques hardly applicable in practice. Our technique, on the contrary, runs with an O(nlog(n)) time bound, and therefore easily
allows the comparison of large instances. The improvement in the
complexity bound is not surprising, since the problem we concentrate on is a simplified variant of the general graph-isomorphism
problem. It is, however, remarkable that for this specific instance
of the problem such a significant speed-up is achievable.
Case 2: Absence of Placeholders It is interesting to evaluate the
performance of our algorithm also in the case in which instances do
not contain placeholders. In this case, the instance graph coincides
with the instance tree, and the notion of isomorphism degrades into

876

the usual one of equality. It would therefore be possible to apply
one of the known tree-similarity algorithms, like, for example, treeedit distances, to compute the quality of solutions.
The tree-edit distance measures the similarity of a tree T1 with
respect to a tree T2 by counting the number of operations that are
needed to transform T1 into T2 . In our framework, we concentrate
on two operations: node insertions and node deletions. We call
these insert(T1 , T2 ), delete(T1 , T2 ).
In order to state our result, we need to introduce an additional
data structure, called the tuple-reduced tree associated with an instance I. The latter is obtained by taking the induced subtree of I
corresponding to set and tuple nodes, and then by relabeling tuple
nodes by the corresponding tuple identifiers in I. In essence, such
subtree is obtained by discarding nodes corresponding to atomic
attributes, and by embedding their values into tuple labels (Figure
1 shows the tuple-reduced trees of the two instances).
We can state the following result, which shows that our algorithm
correctly computes the tree edit distance of tuple-reduced trees.

source and target schemas; arrows are drawn among schema nodes
to specify logical correspondences. The white boxes are graphical elements on the GUI that specify functions used to manipulate
source values; there are 5 functions in this scenario that generate
additional nodes in the graph.

Figure 4: Sample input graph.
We measure the size of such graphs by encoding their elements
according to a minimum description length technique [19], and then
by measuring the size in bits of such description, with the following
algorithm:

T HEOREM 4.2. Given two instances I1 , I2 without placeholders, then distance(I1 , I2 ) = 0 if and only if the corresponding
tuple-reduced trees T1 , T2 are identical. Moreover, the set of extra tuples (missing tuples) detected by the algorithm is equal to the
set of node insertions (node deletions, respectively) computed by
the tree-edit distance over T1 , T2 , i.e., missingTuples(T1 , T2 ) =
insert(T1 , T2 ), extraTuples(T1 , T2 ) = delete(T1 , T2 ).

• as a first step, we assign a unique id to each node in the graph,
and compute the minimum number of bits, bn , needed to encode node ids. Our example uses a total of 36 nodes, so that
we need 6 bits for their encoding. Therefore bn = 6 bits;

Notice that the best algorithms [7] to compute edit distances have
an upper bound of O(n1 n2 ). Better bounds can be achieved for
ordered trees, but this is clearly not the case we consider, since we
do not assume any order of appearance of tuples inside sets. Our
bound improves this for the particular instance of the problem we
concentrate on. Other works [4] have reported bounds similar to
ours, but, unlike ours, results computed by these algorithms cannot
be related to tree edit distances.

5.

• next, we measure the size of the encoding of nodes in the
graph; the source and target schema nodes are considered
as part of the input and therefore are not counted in the encoding. On the contrary, we encode the additional function
nodes. To do this, we build an encoding of the corresponding function, by assigning a binary id to each function in the
library. Based on the size of the library of functions provided
by the tool (approximately 150), every function node in our
example requires an additional 8 bits for its encoding; therefore, encoding additional nodes requires 5 ∗ (6 + 8) bits;

ESTIMATING USER EFFORTS

Now that we have formalized our quality metric, we need to introduce a way to measure user efforts. This is needed in order to
compute our quality-effort graphs.
Previous works have used point-and-click counts to do this [2].
However, click-counts are often unreliable since they are heavily
influenced by GUI layout choices and, even more important, by
the level of expertise of users. On the contrary, to estimate userefforts, we measure the complexity of the mapping specification by
means of an information-theoretic technique. We model the specification complexity as an input graph with labeled nodes and labeled
edges. This model is general enough to cover a very broad range of
approaches to data transformations.
More precisely, the input graph is an undirected graph G =
(N, E), where N is a set of nodes, and E is a set of edges. Nodes
are partitioned in two groups: schema nodes, and additional nodes.
Given a graphical specification of a transformation, we build the
corresponding input-graph as follows: (i) every element in the
source and target schemas is a schema node in the graph; (ii) arrows among elements in the GUI become edges among nodes in
the graph; (iii) a tool may provide a library of graphical elements
– for example to introduce system functions – that are modeled as
additional nodes in the graph; (iv) extra information entered by the
user (e.g., manually typed text) is represented as labels over nodes
and edges.
We report in Figure 4 the input-graph for a sample scenario specified using a commercial mapping system (this scenario will be discussed in Section 6.1). In this example there are 31 nodes for the

• then, we measure the encoding of edges; each edge in the
graph is encoded by the pair of node ids it connects, with a
cost of 2bn bits; in our example, the graph contains 26 edges
(without labels) that we shall encode by 2 ∗ 6 bits;
• finally, node and edge labels are treated as arbitrary strings,
and therefore are encoded in ASCII; in this example, one of
the graph nodes must be labeled by a single char, for which
we need 8 bits.
The specification complexity is therefore given by the following
sum of costs: (5 ∗ (6 + 8)) + (25 ∗ (2 ∗ 6)) + (6 + 8) = 384
bits. With the same technique we are able to measure the size of
the specification needed by different systems, and compare efforts.
We want to stress that this representation is general enough to
accomodate very different visual paradigms. To give another example, consider Figure 5. It shows the input graph for a complex
ETL workflow. In the graph, each oval represents a workflow step.
The schemas and intermediate recordsets used in the various steps
are encoded as nodes, with their correspondences as edges.
Our abstraction can also be used to model purely textual queries.
In this case, the input graph degenerates into a single node, labeled
with the textual specification of the query.

877

Info, joined via a key-foreign key reference. In doing this, it introduces a surrogate key.1 Notice that the input graph in Figure 4
refers exactly to this scenario.
The intended transformation can be formally expressed in data
exchange as follows:
m1 . ∀e, n, d, q, c, o, r : SourceReaction(e, n, d, q, c, o, r) →
∃F: Reaction(e, n, c, o, F ) ∧ ChemicalInfo(d, q, F )
t1 . ∀e, n, c, o, f : Reaction(e, n, c, o, f )
→ ∃D, Q: ChemicalInfo(D, Q, f )
e1 . ∀d, q, f, f 0 : ChemicalInfo(d, q, f ) ∧ ChemicalInfo(d, q, f 0 )
→ (f = f 0 )
Here, m1 is a source-to-target tgd that states how the target tables should be materialized based on the source data. Dependency
t1 is a target tgd stating the referential integrity constraint over the
target tables. Finally, e1 is a target egd expressing the fact that the
first two attributes are a key for the ChemicalInfo table. Notice how,
by using existential variables, the tgds express the requirement that
a placeholder is used to correlate the target tables, without actually
providing any technical details about its generation.
A data exchange problem may have multiple solutions on a given
source instance, with different degrees of quality. The core universal solution [11] is the “optimal” one, since, informally speaking,
it is the smallest among the solutions that preserve the mapping. A
nice property of the core solution is that there exist polynomial algorithms [11, 15, 30], some of which have been proven to be very
scalable [23] to generate the core.
In light of this, a natural way to design scenarios would be to
express the semantics of the transformation as a set of logical dependencies, and to pick the core universal solution as the expected
output for a given source instance. In our experiments, whenever
this was possible, we used this approach. We want, however, to
remark that this is not the only alternative, as discussed above, nor
it is imposed by the method.

Figure 5: Input graph for an ETL workflow.

6.

DESIGNING SCENARIOS

Based on the techniques introduced in Sections 3 and 5, we have
conducted a comprehensive experimental evaluation based on several transformation scenarios. Before getting to the details of these
experimental results, we want to discuss how it is possible to design
scenarios in this framework.
Designing a scenario amounts to choosing a source and target
schema, and an input-output function, i.e., a set of source instances
given as inputs, and a set of target instances considered as expected
outputs, i.e., as the gold standard. It can be seen that the critical
point of this process consists in deciding what the expected output
should be when some input is fed to the system.

6.1

Selecting the Gold Standard

In this respect, our approach is very general, and allows one to
select expected outputs in various ways. It is in principle possible
to craft the expected output by hand.
A more typical way – as it was done, for example, in [2] – would
be to express the input-output function as a query Q written in a
concrete query language, say SQL or XQuery. In this case, for a
given input IS , the expected output would be Ie = Q(IS ). For
more complex workflows, one would need to choose a reference
system, design the transformation using the system, compute the
result, and take this as the expected output.
This approach has the disadvantage of forcing designers to express all of the details of the intended transformation, thus leaving
very limited space to explore the variants supported by alternative
tools. As an example, it requires to devise a precise strategy to
generate surrogates – for example by using Skolem functions [17].
Data exchange theory [10, 11] represents an elegant alternative.
More precisely, it provides a clean theoretical framework to state
two essential elements of the description of a scenario: (a) a semantics of the transformation in terms of logical expressions; (b)
a clear notion of optimal solution for each mapping scenario and
source instance.
A mapping is specified using dependencies, expressed as logical
formulas of two forms: tuple-generating dependencies (tgds) and
equality-generating dependencies (egds). Source-to-target tgds (st tgds) are used to specify which tuples should be present in the
target based on the tuples that appear in the source. Target tgds and
target egds encode foreign-key and key constraints over the target.
As an example, consider the vertical partition scenario of STBenchmark [2]. This scenario takes a single, non-normalized table,
SourceReaction, and splits it into two tables, Reaction, Chemical-

6.2

Test Scenarios

This discussion suggests another promising facet of our evaluation technique. In fact, it shows that it can be used as the basis for a
regression-test tool for schema-mapping systems whose algorithms
are under development. Suppose, in fact, that we need to test the
algorithms of a new system, currently under development. We may
proceed as follows: (a) fix a set of mapping scenarios expressed as
sets of logical dependencies; (b) use an existing system – for example [21, 22] or [30] – to generate the core universal solution; (c) run
the system under evaluation to generate the output, and measure its
distance from the expected output. Then, in case of errors, use the
feedback to correct the translation algorithms.
In essence, this is an alternative evaluation process with respect
to the one that we have discussed so far, in which there is less emphasis on quality-effort trade-offs, and more on correctness. To the
best of our knowledge, this is the first proposal towards the development of test tools for schema-mapping systems.

7.

EXPERIMENTAL RESULTS

The techniques presented in the paper have been implemented
in a working prototype using Java; the prototype has been used to
perform a large experimental evaluation. To start, we show how
the proposed quality measure scales well up to very large instances
and outperforms existing techniques. Then, we use our framework
to compare several data-translation systems on a common set of
scenarios and discuss the results. All the experiments have been
1
We are actually considering a variant of the original scenario in [3] that
has no key constraints.

878

conducted on a Intel Xeon machine with four 2.66Ghz cores and 4
GB of RAM under Linux.

7.1

current version of OpenII is still in a rather preliminary state, and
therefore decided to exclude it from the evaluation.
To conduct the evaluation, we selected twelve transformation
tasks from the literature, with different levels of complexity. The
selected tasks can be roughly classified in three categories:

Scalability of the Quality Measure

To test the scalability of our quality measure on instances of
large size, we used the instance provided for the Unnesting scenario
in STBenchmark [3]. The original instance contains 2.2 millions
nodes and has a size of 65 MB. We generated a modified version
of the same file by randomly introducing errors, in such a way that
the original and the modified instance had a similarity of 90% according to our quality metrics. Then, we extracted smaller subtrees
from this complete instance, in order to obtain a pool of instances
of increasing size, varying from a few hundreds to 2 million nodes.
Since the nesting scenario does not require the generation of
surrogates, we were able to compare experimentally our measure
against an implementation of a tree-edit distance algorithm for ordered trees [5]. We tested ordered instances because of the lack of
implementations of the tree edit-distance algorithm for unordered
ones. We remark, however, that in the general case of unordered
trees the complexity of the tree edit-distance computation is even
higher.
We compared times of execution to compute our quality measure, the tree edit-distance on the original trees, and the tree editdistance on the more compact tuple-reduced trees introduced in
Section 4. We call this hybrid measure TED-TRT.

(i) basic mapping-operations, taken from STBenchmark [3]: Copy
(Cp), Denormalization (Dn), Vertical Partition (VP), Key Assignment (KA), Value Management (VM), and Nesting (Ne);
(ii) advanced mapping operations, taken from [23, 21]: Minimization (Mi), Fusion (Fu), Nested Keys (NK); these scenarios require the management of more complex target constraints wrt those
above;
(iii) typical ETL tasks: Aggregation (Ag) is a simplified version of
the line workflow from an ETL benchmark [29], while AccessLog
(AL) and CreditCards (CC) are taken from the http://cloveretl.com/examples Web page.
Of these twelve scenarios, five required the introduction of surrogates; two use nested models, the others are relational. For each
scenario we have identified the gold standard, that is, the desired
target solution for the given translation task and source instance.
As discussed in Section 6, expected solutions were identified in
such a way that they contained no unsound or redundant information [11]. Then, we tried to generate the expected output with each
of the tools, and measured efforts and quality.

Figure 6: Execution times with files of increasing size.
Figure 7: Effort needed by the systems to obtain 100% quality in the

Figure 6 summarizes the experimental results in terms of execution times and instance sizes. We experimentally confirmed the results reported in Section 4. More specifically, as stated in Theorem
4.2, the distance measured using our quality measure was identical
to the one measured using the TED-TRT distance. In addition, the
computation was orders of magnitude faster. In the graph on the left
hand side, we show the comparison of our measure against the tree
edit-distance on small instances. On the right, we show times only
for our quality measure, since the tree edit-distance implementation
did not scale to large instances. On the contrary, our comparison
algorithm scaled nicely: it computed the distance between two files
of approximately 65 MB in less than 30 seconds, thus confirming
the low complexity bound established in Section 4.

7.2

various scenarios.

Notice that the three tools under exam use fairly different strategies to compute the transformations: the research mapping systems
generate SQL or XQuery code, the commercial mapping system
typically generates XSLT, while the ETL workflow requires the internal engine in order to produce results. Nevertheless, our general
view of the transformation process permits their comparison. We
also want to mention that, while it is not in the scope of this paper
to compare the systems in terms of scalability, all of the systems in
our experiments scaled well to large instances.
Results are shown in Figure 7 and 8. More specifically, Figure
7 reports the effort needed by the various systems to obtain 100%
quality in the various scenarios, while Figure 8 shows the qualityeffort graphs. There are several evidences in the graphs that we
want to highlight. Let us first look at Figure 7. As a first evidence, we note that the research mapping tool required considerably less effort than the commercial counterparts on the basic and
advanced mapping tasks. On these tasks, the ETL tool was the one
requiring the highest effort to compute the requested transformations. However, we also note that the situation is reversed for the
ETL-oriented tasks. Notice how the commercial mapping system
had intermediate performances. This suggests that these tools are
progressively evolving from the schema-mapping ecosphere into
fully-fledged ETL tools.

Comparison of the Systems

In the spirit of evaluating representative systems from different
perspectives, we have included the following tools: (i) an opensource schema-mapping research prototype [21, 22]; (ii) a commercial schema-mapping system; (iii) a commercial ETL tool.
In addition, to discuss the relationship of our technique to STBenchmark, we have also evaluated the performances of a different
schema-mapping tool [25] on some of the scenarios. It is worth
mentioning that we also considered the idea of including in our
evaluation the open-source OpenII data integration tool [28]. We
found out that, while promising, the data translation module of the

879

Figure 8: Quality-effort graphs. For each scenario, the smaller the area, the higher is the IQ of a transformation tool.
To get more insights about this, let us look at the quality-effort
graphs in Figure 8. The Figure shows how much effort is needed to
get a certain level of quality with a given system. Recall that one of
our aims is that of measuring the “level of intelligence” of a tool,
as its quality/effort ratio. From the graphical viewpoint, this notion
of IQ can be associated with the area of the graph delimited by the
effort-quality function: such area can be taken as a measure of the
progressive effort needed by a tool to achieve increasing levels of
quality in one experiment. The smaller the area, the higher is the
IQ of a system.
We can observe that the research mapping tool handles in a more
natural way some complex operations, like nesting values or data
fusion and redundancy removal. But, as soon as the task at hand
becomes more procedural, like in the key-assignment scenario or
when typical ETL-like operations such as aggregations are required, this advantage becomes less visible or it is completely lost.
This is apparent in the Aggregation, AccessLog, and CreditCard
scenarios – three rather typical data warehousing transformations.
For these tasks, while the effort needed to compute the transformation in commercial systems was in line with those of previous
scenarios, in the case of the research mapping tool the cost was
enormously increased by the need of manually changing the generated SQL code in order to introduce the needed aggregates. In
fact, there is no declarative way of expressing aggregates in data
exchange yet.
In fact, our experiments confirm the intuition that the sophisticated declarative algorithms introduced in recent years in schemamappings research may really provide some advantage in terms
of productivity to the data architect. However, this advantage is
somehow confined to the typical scope of applicability of schemamappings. When users want to deal with more complex scenarios,
i.e., transformations requiring a rather fine-grained manipulation
of values, the adoption of more procedural paradigms brings some
advantages.
We strongly believe that these results clearly confirm the need
for a new strategy for developing data-transformation tools, which
brings together the best of both worlds, in the spirit of [9]. While
the expressive power of procedural ETL tools is necessary to properly handle the wide range of transformations that a data architect

typically faces, still there are a variety of mapping tasks – ranging
from conjunctive queries, data-fusion and instance minimization,
to management of functional dependencies and nested constraints
– for which research mapping tools provide building blocks that
may represent a very powerful addition to commercial transformation systems.
As a final remark, we
think it is useful to put in
perspective the results reported in [3] using STBenchmark.
STBenchmark
uses a coarse-grained yes/no style of evaluation to
measure the performance
of a mapping system on a
given scenario. Our techFigure 9:
Comparison to
nique gives a better unSTBenchmark.
derstanding of the performances of a system, especially in those cases in which it is not
capable of fully capturing the semantics of a transformation.
We considered the mapping algorithm used in the STBenchmark
evaluation [25], and evaluated its performance on three advanced
mapping scenarios. The table in Figure 9 shows the maximum
quality that we were able to obtain with such a system by using
the GUI only, i.e., without manually changing the SQL or XQuery
code (operation, which, as discussed has very high costs in our
metrics). It can be seen that in three of the scenarios the system
failed to achieve 100% quality. Differently from the yes/no output
of STBenchmark, our comparison technique provided a detailed account of the results obtained by the algorithm in these cases. This
is a further evidence that our evaluation framework may help to improve the design of benchmarks, and to gain better insights about
the effectiveness and limitation of tools.

8.

RELATED WORKS

Data translation tasks in enterprise settings are very often tackled using Extraction-Transform-Load (or ETL) tools, where transformations are defined by using sequences of building blocks in
a rather procedural fashion. Representative examples of ETL sys-

880

tems can be found on the market (e.g., Oracle Warehouse Builder or
IBM Information Server) (www.oracle.com/technetwork/developertools/warehouse, www-01.ibm.com/software/data/integration) and in
the open-source community (e.g., Clover ETL or Talend Open Studio) (www.cloveretl.com, www.talend.com).
Different generations of research mapping systems have been developed in the last ten years. A first generation of schema-mapping
systems [24, 25] has been followed first by an intermediate generation [30, 23], focused on the quality of the solutions, and then
by a second generation [21, 22], which is able to handle a larger
class of scenarios. In addition, Altova Mapforce and Stylus Studio
(www.altova.com/mapforce, www.stylusstudio.com) are examples of
commercial mapping systems.
Several works have studied the issue of quality in mapping-systems
with a different focus, either to check desirable properties [27], or
to rank alternative mappings [8].
The analysis of the information-integration and business-intelligence market is the subject of a large number of studies by business consulting firms [14]. However, these reports are centered
around features and functionalities of the various systems, and do
not rely on a quality evaluation metric such as the one developed
in this paper. Closer to our approach, there exist some early benchmarks, designed both for ETL tools [29, 32, 20] and schema-mapping systems [3], which provide a basis for evaluating systems in
the respective areas only. They mainly concentrate on expressibility, by introducing representative, small scenarios [3, 29], or on the
efficiency evaluation [20]. With respect to measuring user efforts,
existing solutions (such as [3]) rely on a simple metric based on
the count of user actions (e.g., the number of clicks and the number of typed characters) in defining the transformation. Their main
limitation, however, is that they fail in answering the main question
addressed in this paper.

9.

CONCLUSIONS

In this paper, we introduce a number of novel techniques that
significantly improve the ones used in previous works. Our quality measure, coupled with the information-theoretic effort measure,
enables the introduction of a new tool, called quality-effort graph,
to study the effectiveness of a data transformation system.
This evaluation framework provides a clear perception of the
level of intelligence of a data transformation tool, and ultimately
measures how productive it is for a given scenario. For the specific
problem of data-translation, it represents a concrete measure of the
trade-off between declarative and procedural approaches.
In addition, we have shown that the technique is very scalable,
despite the fact that we deal with a rather difficult problem, i.e.,
comparing possibly isomorphic graph-like structures.
We believe that this technique sheds some light on the right approach to solve data-integration problems: transforming and integrating data is a multi-faceted problem that requires a combination
of state-of-the-art techniques, bringing together the expressibility
of ETL tools and the declarative algorithms of schema-mapping research. Coupling together these approaches is a challenging but
very promising research problem.
Acknowledgments The authors would like to thank Angela Bonifati and
Yannis Velegrakis for the many helpful discussions on the subject of this
paper.

881

10.

REFERENCES

[1] S. Abiteboul, R. Hull, and V. Vianu. Foundations of Databases.
Addison-Wesley, 1995.
[2] B. Alexe, W. Tan, and Y. Velegrakis. Comparing and Evaluating Mapping
Systems with STBenchmark. PVLDB, 1(2):1468–1471, 2008.
[3] B. Alexe, W. Tan, and Y. Velegrakis. STBenchmark: Towards a Benchmark for
Mapping Systems. PVLDB, 1(1):230–244, 2008.
[4] N. Augsten, M. Bohlen, and J. Gamper. Approximate Matching of Hierarchical
Data Using pq-Grams. In VLDB, pages 301–312, 2005.
[5] A. Bernstein, E. Kaufmann, C. Kiefer, and C. Bürki. SimPack: A Generic Java
Library for Similiarity Measures in Ontologies. Technical report, Department of
Informatics, University of Zurich, 2005.
[6] P. A. Bernstein and S. Melnik. Model Management 2.0: Manipulating Richer
Mappings. In SIGMOD, pages 1–12, 2007.
[7] P. Bille. A Survey on Tree Edit Distance and Related Problems. TCS,
337:217–239, 2005.
[8] A. Bonifati, G. Mecca, A. Pappalardo, S. Raunich, and G. Summa. Schema
Mapping Verification: The Spicy Way. In EDBT, pages 85 – 96, 2008.
[9] S. Dessloch, M. A. Hernandez, R. Wisnesky, A. Radwan, and J. Zhou. Orchid:
Integrating Schema Mapping and ETL. In ICDE, pages 1307–1316, 2008.
[10] R. Fagin, P. Kolaitis, R. Miller, and L. Popa. Data Exchange: Semantics and
Query Answering. TCS, 336(1):89–124, 2005.
[11] R. Fagin, P. Kolaitis, and L. Popa. Data Exchange: Getting to the Core. ACM
TODS, 30(1):174–210, 2005.
[12] F. Fortin. The Graph Isomorphism Problem. Technical report, Department of
Computer Science, University of Alberta, 1996.
[13] X. Gao, B. Xiao, D. Tao, and X. Li. A Survey of Graph Edit Distance. Pattern
Analysis & Application, 13:113–129, 2010.
[14] Gartner. Magic Quadrant for Data Integration Tools.
http://www.gartner.com/technology/, 2011.
[15] G. Gottlob and A. Nash. Efficient Core Computation in Data Exchange. J. of
the ACM, 55(2):1–49, 2008.
[16] L. M. Haas. Beauty and the Beast: The Theory and Practice of Information
Integration. In ICDT, pages 28–43, 2007.
[17] R. Hull and M. Yoshikawa. ILOG: Declarative Creation and Manipulation of
Object Identifiers. In VLDB, pages 455–468, 1990.
[18] R. Kimball and J. Caserta. The Data Warehouse ETL Toolkit. Wiley and Sons,
2004.
[19] D. MacKay. Information Theory, Inference, and Learning Algorithms.
Cambridge University Press, 2003.
[20] T. A. Majchrzak, T. Jansen, and H. Kuchen. Efficiency evaluation of open
source etl tools. In SAC, pages 287–294, 2011.
[21] B. Marnette, G. Mecca, and P. Papotti. Scalable data exchange with functional
dependencies. PVLDB, 3(1):105–116, 2010.
[22] B. Marnette, G. Mecca, P. Papotti, S. Raunich, and D. Santoro. ++S PICY: an
opensource tool for second-generation schema mapping and data exchange.
PVLDB, 4(11):1438–1441, 2011.
[23] G. Mecca, P. Papotti, and S. Raunich. Core Schema Mappings. In SIGMOD,
pages 655–668, 2009.
[24] R. J. Miller, L. M. Haas, and M. A. Hernandez. Schema Mapping as Query
Discovery. In VLDB, pages 77–99, 2000.
[25] L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernandez, and R. Fagin. Translating
Web Data. In VLDB, pages 598–609, 2002.
[26] M. A. Roth, H. F. Korth, and A. Silberschatz. Extended Algebra and Calculus
for Nested Relational Databases. ACM TODS, 13:389–417, October 1988.
[27] G. Rull Fort, F. C., E. Teniente, and T. Urpí. Validation of Mappings between
Schemas. Data and Know. Eng., 66(3):414–437, 2008.
[28] L. Seligman, P. Mork, A. Halevy, K. Smith, M. J. Carey, K. Chen, C. Wolf,
J. Madhavan, A. Kannan, and D. Burdick. OpenII: an Open Source Information
Integration Toolkit. In SIGMOD, pages 1057–1060, 2010.
[29] A. Simitsis, P. Vassiliadis, U. Dayal, A. Karagiannis, and V. Tziovara.
Benchmarking etl workflows. In TPCTC, pages 199–220, 2009.
[30] B. ten Cate, L. Chiticariu, P. Kolaitis, and W. C. Tan. Laconic Schema
Mappings: Computing Core Universal Solutions by Means of SQL Queries.
PVLDB, 2(1):1006–1017, 2009.
[31] C. J. Van Rijsbergen. Information Retrieval. Butterworths (London, Boston),
1979.
[32] L. Wyatt, B. Caufield, and D. Pol. Principles for an etl benchmark. In TPCTC,
pages 183–198, 2009.

KATARA: A Data Cleaning System Powered by Knowledge
Bases and Crowdsourcing
Xu Chu1∗ John Morcos1∗ Ihab F. Ilyas1∗
Mourad Ouzzani2 Paolo Papotti2 Nan Tang2
1

University of Waterloo

2

{x4chu,jmorcos,ilyas}@uwaterloo.ca
ABSTRACT
Classical approaches to clean data have relied on using integrity constraints, statistics, or machine learning. These
approaches are known to be limited in the cleaning accuracy, which can usually be improved by consulting master
data and involving experts to resolve ambiguity. The advent
of knowledge bases (kbs), both general-purpose and within
enterprises, and crowdsourcing marketplaces are providing
yet more opportunities to achieve higher accuracy at a larger
scale. We propose Katara, a knowledge base and crowd
powered data cleaning system that, given a table, a kb, and
a crowd, interprets table semantics to align it with the kb,
identifies correct and incorrect data, and generates top-k
possible repairs for incorrect data. Experiments show that
Katara can be applied to various datasets and kbs, and
can efficiently annotate data and suggest possible repairs.

1.

Qatar Computing Research Institute

{mouzzani,ppapotti,ntang,yye}@qf.org.qa
Al
B
C
tb
Italy
Rome
1 Rossi
tb
2 Klate S. Africa Pretoria
tb
Italy
Madrid
3 Pirlo

D
E
Verona
Italian
Pirates Afrikaans
Juve
Italian

F
G
Proto 1.78
P. Eliz. 1.69
Flero 1.77

Figure 1: A table T for soccer players
Yago [21], DBpedia [31], and Freebase, as well as specialpurpose kbs such as RxNorm1 . There is also a sustained
effort in the industry to build kbs [14]. These kbs are usually well curated and cover a large portion of the data at
hand. In addition, while access to an expert may be limited
and expensive, crowdsourcing has been proven to be a viable
and cost-effective alternative solution.
Challenges. Effectively exploring kbs and crowd in data
cleaning raises several new challenges.
(i) Matching (dirty) tables to kbs is a hard problem. Tables
may lack reliable, comprehensible labels, thus requiring the
matching to be executed on the data values. This may lead
to ambiguity; more than one mapping may be possible. For
example, Rome could be either city, capital, or club in the
kb. Moreover, tables usually contain errors. This would
trigger problems such as erroneous matching, which will add
uncertainty to or even mislead the matching process.
(ii) kbs are usually incomplete in terms of the coverage of
values in the table, making it hard to find correct table patterns and associate kb values. Since we consider data that
could be dirty, it is often unclear, in the case of failing to
find a match, whether the database values are erroneous or
the kb does not cover these values.
(iii) Human involvement is needed to validate matchings and
to verify data when the kbs do not have enough coverage.
Effectively involving the crowd requires dealing with traditional crowdsourcing issues such as forming easy-to-answer
questions for the new data cleaning tasks and optimizing the
order of issuing questions to reduce monetary cost.
Despite several approaches for understanding tables with
kbs [13,28,39], to the best of our knowledge, they do not explicitly assume the presence of dirty data. Moreover, previous work exploiting reference information for repair has only
considered full matches between the tables and the master
data [19]. On the contrary, with kbs, partial matches are
common due to the incompleteness of the reference.
To this end, we present Katara, the first data cleaning
system that leverages prevalent trustworthy kbs and crowdsourcing for data cleaning. Given a dirty table and a kb,

INTRODUCTION

A plethora of data cleaning approaches that are based on
integrity constraints [2,7,9,20,36], statistics [30], or machine
learning [43], have been proposed in the past. Unfortunately,
despite their applicability and generality, they are best-effort
approaches that cannot ensure the accuracy of the repaired
data. Due to their very nature, these methods do not have
enough evidence to precisely identify and update errors. For
example, consider the table of soccer players in Fig. 1 and a
functional dependency B → C, which states that B (country) uniquely determines C (capital). This would identify a
problem for the four values in tuple t1 and t3 over the attributes B and C. A repair algorithm would have to guess
which value to change so as to “clean” the data.
To increase the accuracy of such methods, a natural approach is to use external information in tabular master
data [19] and domain experts [19, 35, 40, 44]. However, these
resources may be scarce and are usually expensive to employ. Fortunately, we are witnessing an increased availability of both general purpose knowledge bases (kbs) such as
∗

Yin Ye2

Work partially done while interning/working at QCRI.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGMOD’15, May 31–June 4, 2015, Melbourne, Victoria, Australia.
c 2015 ACM 978-1-4503-2758-9/15/05 ...$15.00.
Copyright 
http://dx.doi.org/10.1145/2723372.2749431.

1

1247

https://www.nlm.nih.gov/research/umls/rxnorm/

hasCapital
B (country)

hasOfficalLanguage

nationality
A (person)

E (language)
locatedIn

hasClub
D (football
club)

B (Italy)

C (Rome)
hasOfficalLanguage

nationality
A (Rossi)

B (S. Africa)

E (Italian)

A (Klate)

(a) A table pattern ϕs

D (Verona)

B (Italy)

C (Madrid)
hasOfficalLanguage

nationality
A (Pirlo)

E (Italian)
locatedIn

hasClub
bornIn

bornIn
D (Pirates)

F (Proto)

F (P. Eliz.)

(b) t1 : kb validated
(c) t2 : kb & crowd validated
Figure 2: Sample solution overview

Katara first discovers table patterns to map the table to
the kb. For instance, consider the table of soccer players in
Fig. 1 and the kb Yago. Our table patterns state that the
types for columns A, B, and C in the kb are person, country,
and capital, respectively, and that two relationships between
these columns hold, i.e., A is related to B via nationality
and B is related to C via hasCapital. With table patterns,
Katara annotates tuples as either correct or incorrect by
interleaving kbs and crowdsourcing. For incorrect tuples,
Katara will extract top-k mappings from the kb as possible repairs. In addition, a by-product of Katara is that
data annotated by the crowd as being valid, and which is
not found in the kb, provides new facts to enrich the kb.
Katara actively and efficiently involve crowd workers, who
are assumed to be experts in the kbs, when automatic approaches cannot capture or face ambiguity, for example, to
involve humans to validate patterns discovered, and to involve humans to select from the top-k possible repairs.

D (Juve)

F (Flero)

(d) t3 : erroneous tuple

module uses crowdsourcing to select one table pattern. Using the selected table pattern, the data annotation module
interacts with the kb and the crowd to annotate data. It
also generates possible repairs for erroneous tuples. Moreover, new facts verified by crowd will be used to enrich kbs.
Example 1: Consider a table T for soccer players (Fig. 1).
T has opaque values for the attributes’ labels, thus its semantics is completely unknown. We assume that we have
access to a kb K (e.g., Yago) containing information related
to T . Katara works in the following steps.
(1) Pattern discovery. Katara first discovers table patterns
that contain the types of the columns and the relationships
between them. A table pattern is represented as a labelled
graph (Fig. 2(a)) where a node represents an attribute and
its associated type, e.g., “C (capital)” means that the type of
attribute C in kb K is capital. A directed edge between two
nodes represents the relationship between two attributes,
e.g., “B hasCapital C” means that the relationship from B
to C in K is hasCapital. A column could have multiple candidate types, e.g., C could also be of type city. However,
knowing that the relationship from B to C is hasCapital
indicates that capital is a better choice. Since kbs are often incomplete, the discovered patterns may not cover all
attributes of a table, e.g., attribute G of table T is not captured by the pattern in Fig. 2(a).

Contributions. We built Katara for annotating and repairing data using kbs and crowd, with the following contributions.
1. Table pattern definition and discovery. We propose a
new class of table patterns to explain table semantics
using kbs (Section 3). Each table pattern is a directed
graph, where a node represents a type of a column
and a directed edge represents a binary relationship
between two columns. We present a new rank-join
based algorithm to efficiently discover table patterns
with high scores. (Section 4).
2. Table pattern validation via crowdsourcing. We devise
an efficient algorithm to validate the best table pattern via crowdsourcing (Section 5). To minimize the
number of questions, we use an entropy-based scheduling algorithm to maximize the uncertainty reduction
of candidate table patterns.
3. Data annotation. Given a table pattern, we annotate
data with different categories (Section 6): (i) correct
data validated by the kb; (ii) correct data jointly validated by the kb and the crowd; and (iii) erroneous
data jointly identified by the kb and the crowd. We
also devise an efficient algorithm to generate top-k possible repairs for those erroneous data identified in (iii).

(2) Pattern validation. Consider a case where pattern discovery finds two similar patterns: the one in Fig. 2(a),
and its variant with type location for column C. To select the best table pattern, we send the crowd the question
“Which type (capital or location) is more accurate for values (Rome, Pretoria and Madrid)?”. Crowd answers will help
choose the right pattern.
(3) Data annotation. Given the pattern in Fig. 2(a),
Katara annotates a tuple with the following three labels:
(i) Validated by the kb. By mapping tuple t1 in table T
to K, Katara finds a full match, shown in Fig. 2(b)
indicating that Rossi (resp. Italy) is in K as a person
(resp. country), and the relationship from Rossi to Italy
is nationality. Similarly, all other values in t1 w.r.t.
attributes A-F are found in K. We consider t1 to be
correct w.r.t. the pattern in Fig. 2(a) and only to
attributes A-F .
(ii) Jointly validated by the kb and the crowd. Consider t2
about Klate, whose explanation is depicted in Fig. 2(c).
In K, Katara finds that S. Africa is a country, and
Pretoria is a capital. However, the relationship from
S. Africa to Pretoria is missing. A positive answer from
the crowd to the question “Does S. Africa hasCapital
Pretoria?” completes the missing mapping. We consider t2 correct and generate a new fact “S. Africa
hasCapital Pretoria”.

4. We conducted extensive experiments to demonstrate
the effectiveness and efficiency of Katara using realworld datasets and kbs (Section 7).

2.

E (Afrikaans)
locatedIn

hasClub

bornIn
F (city)

C (Pretoria)
hasOfficalLanguage

nationality

locatedIn

hasClub

bornIn

hasCapital

hasCapital

hasCapital
C (Capital)

AN OVERVIEW OF KATARA

Katara consists of three modules (see Fig. 9 in Appendix): pattern discovery, pattern validation, and data
annotation. The pattern discovery module discovers table
patterns between a table and a kb. The pattern validation

1248

(iii) Erroneous tuple. For tuple t3 , there is also no link from
Italy to Madrid in K (Fig. 2(d)). A negative answer
from the crowd to the question “Does Italy hasCapital
Madrid?” confirms that there is an error in t3 , At this
point, however, we cannot decide which value in t3
is wrong, Italy or Madrid. Katara will then extract
related evidences from K, such as Italy hasCapital Rome
and Spain hasCapital Madrid, and use these evidences
to generate a set of possible repairs for this tuple. 2

Table pattern. A table pattern (pattern for short) ϕ of a
table T is a labelled directed graph G(V, E) with nodes V
and edges E. Each node u ∈ V corresponds to an attribute
in T , possibly typed, and each edge (u, v) ∈ E from u to
v has a label P , denoting the relationship between two attributes that u and v represent. For a pattern ϕ, we denote
by ϕu a node u in ϕ, ϕ(u,v) an edge in ϕ, ϕV all nodes in ϕ,
and ϕE all edges in ϕ.
We assume that a table pattern is a connected graph.
When there exist multiple disconnected patterns, i.e., two
table patterns that do not share any common node, we treat
them independently. Hence, in the following, we focus on
discussing the case of a single table pattern.

The pattern discovery module can be used to select the
more relevant kb for a given dataset. If the module cannot
find patterns for a table and a kb, Katara will terminate.

3.

PRELIMINARIES

3.1

Knowledge Bases

Semantics. A tuple t of T matches a table pattern ϕ containing m nodes {v1 , . . . , vm } w.r.t. a kb K, denoted by
t |= ϕ, if there exist m distinct attributes {A1 , . . . , Am } in
T and m resources {x1 , . . . , xm } in K such that:

We consider knowledge bases (kbs) as RDF-based data
consisting of resources, whose schema is defined using the
Resource Description Framework Schema (RDFS). A resource is a unique identifier for a real-word entity. For
instance, Rossi, the soccer player, and Rossi, the motorcycle racer, are two different resources. Resources are represented using URIs (Uniform Resource Identifiers) in Yago
and DBPedia, and mids (machine-generated ids) in Freebase.
A literal is a string, date, or number, e.g., 1.78. A property (a.k.a. relationship) is a binary predicate that represents a relationship between two resources or between a resource and a literal. We denote the property between resource x and resource (or literal) y by P (x, y). For instance,
locatedIn(Milan, Italy) indicates that Milan is in Italy.
An RDFS ontology distinguishes between classes and instances. A class is a resource that represents a set of objects,
e.g., the class of countries. A resource that is a member of a
class is called an instance of that class. The type relationship
associates an instance to a class e.g., type(Italy) = country.
A more specific class c can be specified as a subclass of a
more general class d by using the statement subclassOf(c, d).
This means that all instances of c are also instances of d,
e.g., subclassOf(capital, location). Similarly, a property P1
can be a sub-property of a property P2 by the statement
subpropertyOf(P1 , P2 ). Moreover, we assume that the property between an entity and its readable name is labeled with
“label”, according to the RDFS schema.
Note that an RDF ontology naturally covers the case of a
kb without a class hierarchy such as IMDB. Also, a more expressive languages, such as OWL (Web Ontology Language),
can offer more reasoning opportunities at a higher computational cost. However, kbs in industry [14] as well as popular
ones, such as Yago, Freebase, and DBpedia, use RDFS.

3.2

1. there is a one-to-one mapping from Ai (and xi ) to vi
for i ∈ [1, m];
2. t[Ai ] ≈ xi and either type(xi ) = type(vi ) or
subclassOf(type(xi ), type(vi ));
3. for each edge (vi , vj ) in ϕE with property P , there
exists a property P 0 for the corresponding resources xi
and xj in K such that P 0 = P or subpropertyOf(P 0 , P ).
Intuitively, if t matches ϕ, each corresponding attribute
value of t maps to a resource r in K under a domain-specific
similarity function (≈), and r is a (sub-)type of the type
given in ϕ (conditions 1 and 2). Moreover, for each property
P in a pattern, the property between the two corresponding
resources must be P or its sub-properties (condition 3).
Example 2: Consider tuple t1 in Fig. 1 and pattern ϕs in
Fig. 2(a). Tuple t1 matches ϕs , as in Fig. 2(b), since for each
attribute value (e.g., t1 [A] = Rossi and t1 [B] = Italy) there is
a resource in K that has a similar value with corresponding
type (person for Rossi and country for Italy) for conditions 1
and 2, and the property nationality holds from Rossi to Italy
in K (condition 3). Similarly, conditions 1–3 hold for other
attribute values in t1 . Hence, t1 |= ϕs .
2
We say that a tuple t of T partially matches a table pattern
ϕ w.r.t. K, if at least one of condition 2 and condition 3
holds.
Example 3: Consider t2 in Fig. 1 and ϕs in Fig. 2(a).
We say that t2 partially matches ϕs , since the property
hasCapital from t2 [B] = S. Africa to t2 [C] = Pretoria does
not exist in K, i.e., condition 3 does not hold.
2
Given a table T , a kb K, and a pattern ϕ, Fig. 3 shows
how Katara works on T .
(1) Attributes covered by K. Attributes A–F in Fig. 1 are
covered by the pattern in Fig. 2(a). We consider two cases
for the tuples.

Table Patterns

Consider a table T with attributes denoted by Ai . There
are two basic semantic annotations on a relational table.
(1) Type of an attribute Ai . The type of an attribute is an
annotation that represents the class of attribute values in Ai .
For example, the type of attribute B in Fig. 1 is country.
(2) Relationship from attribute Ai to attribute Aj . The
relationship between two attributes is an annotation that represents how Ai and Aj are related through a directed binary
relationship. Ai is called the subject of the relationship, and
Aj is called the object of the relationship. For example, the
relationship from attribute B to C in Fig. 1 is hasCapital.

(a) Fully covered by K. We annotate such tuples as semantically correct relative to ϕ and K (Fig. 2(b)).
(b) Partially covered by K. We use crowdsourcing to verify whether the non-covered data is caused by the
incompleteness of K (Fig. 2(c)) or by actual errors
(Fig. 2(d)).
(2) Attributes not covered by K. Attribute G in Fig. 1 is not

1249

A table pattern P
(1) Attributes covered by KB
... ... ... ...

Column 1

Column i

the candidate types of a column Ai . We simply consider each
cell t[Ai ], ∀t ∈ T , as a query term, and each candidate type
Ti as a document whose terms are the entities of Ti in K.
The tf-idf score of assigning Ti as the type for Ai is the sum
of all tf-idf scores of all cells X
in Ai :
tf-idf(Ti , Ai ) =
tf-idf(Ti , t[Ai ])

(2) Attributes not
covered by KB
Column i+1

...

Column j

KB

(a) Fully covered by KB
correct
(b) Partially covered by KB
errors

A Table

t∈T

Figure 3: Coverage of a table pattern

where tf-idf(Ti , t[Ai ]) = tf(Ti , t[Ai ]) · idf(Ti , t[Ai ]).
The term frequency tf(Ti , t[Ai ]) measures how frequently
t[Ai ] appears in document Ti . Since every type has a different number of entities, the term frequency is normalized by
the total number of entities of a type.
(
0
if t[Ai ] is not of Type Ti
tf(Ti , t[Ai ]) =
1
otherwise
log (Number of Entities of Type Ti )

covered by the pattern in Fig. 2(a). In this case, Katara
cannot annotate G due to the missing information in K.
For non-covered attributes, we could ask the crowd openended questions, such as “What are the possible relationships
between Rossi and 1.78?”. While approaches have been proposed for open-ended questions to the crowd [38], we leave
the problem of extending the structure of the kbs to future
work, as discussed in Section 9.

4.

For example, consider a column with a single cell Italy
that has both type Country and type Place. Since there is
a smaller number of entities of type Country than that of
Place, Country is more likely to be the type of that column.
The inverse document frequency idf(Ti , t[Ai ]) measures
how important t[Ai ] is. Under local completeness assumption of kbs [15], if the kb knows about one possible type of
t[Ai ], the kb should have all possible types of t[Ai ]. Thus,
we define idf(Ti , t[Ai ]) as follows:
(
0
if t[Ai ] has no type
idf(Ti , t[Ai ]) =
Number of Types in K
otherwise
log Number
of Types of t[Ai ]

TABLE PATTERN DISCOVERY

We first describe candidate types and candidate relationships generation (Section 4.1). We then discuss the scoring
to rank table patterns (Section 4.2). We also present a rankjoin algorithm to efficiently compute top-k table patterns
(Section 4.3) from the candidate types and relationships.

4.1

Candidate Type/Relationship Discovery

We focus on cleaning tabular data for which the schema
is either unavailable or unusable. This is especially true
for most Web tables and in many enterprise settings where
cryptic naming conventions are used. Thus, for table-kb
mapping, we use a more general instance based approach
that does not require the availability of meaningful column
labels. For each column Ai of table T and for each value
t[Ai ] of a tuple t, we map this value to several resources in
the kb K whose type can then be extracted. To this end, we
issue the following SPARQL query which returns the types
and supertypes of entities whose label (i.e., value) is t[Ai ].
Qtypes

Intuitively, the less the number of types t[Ai ] has, the more
contribution t[Ai ] makes. For example, consider a column
that has two cells “Apple” and “Microsoft”. Both have Type
Company, however, “Apple” has also Type Fruit. Therefore,
“Microsoft” being of Type Company says more about the
column being of Type Company than “Apple” says about
the column being of Type Company.
The tf-idf scores of all candidate types for Ai are normalized to [0, 1] by dividing them by the largest tf-idf score of the
candidate type for Ai . The tf-idf score tf-idf(Pij , Ai , Aj ) of
candidate relationship Pij assigned to column pairs Ai and
Aj are defined similarly.

select ?ci
where {?xi rdfs:label t[Ai ],
?xi rdfs:type/rdfs:subClassOf∗ ?ci }

Similarly, the relationship between two values t[Ai ] and
t[Aj ] from a kb K can be retrieved via the two following
SPARQL queries.
Q1rels
Q2rels

4.2

Scoring Model for Table Patterns

A table pattern contains types of attributes and properties
between attributes. The space of all candidate patterns is
very large (up to the Cartesian product of all possible types
and relationships), making it expensive for human verification. Since not all candidate patterns make sense in practice, we need a meaningful scoring function to rank them
and consider only the top-k ones for human validation.
A naive scoring model for a candidate table pattern ϕ,
consisting of type Ti for column Ai and relationship Pij for
column pair Ai and Aj , is to simply add up all tf-idf scores
of the candidate types and relationships in ϕ:
naiveScore(ϕ) = Σm
i=0 tf-idf(Ti , Ai ) + Σij tf-idf(Pij , Ai , Aj )

select ?Pij
where {?xi rdfs:label t[Ai ], ?xj rdfs:label t[Aj ],
?xi ?Pij /rdfs:subPropertyOf∗ ?xj }
select ?Pij
where {?xi rdfs:label t[Ai ],
?xi ?Pij /rdfs:subPropertyOf∗ t[Aj ]}

Query Q1rels retrieves relationships where the second attribute is a resource in kbs and Q2rels retrieves relationships
where the second attribute is a literal value, i.e., untyped.
Example 4: In Fig. 1, both Italy and Rome are stored as
resources in K, thus their relationship hasCapital would be
discovered by Q1rels ; while numerical values such as 1.78 are
stored as literals in the kbs, thus the relationship between
Rossi and 1.78 would be discovered by query Q2rels .
2

However, columns are not independent of each other. The
choice of the type for a column Ai affects the choice of the
relationship for column pair Ai and Aj , and vice versa.

In addition, for two values t[Ai ] and t[Aj ], we consider
them as an ordered pair, thus in total four queries are issued.

Example 5: Consider the two columns B and C in Fig. 1.
B has candidate types economy, country, and state, C has
candidate types city and capital, and B and C have a candidate relationship hasCapital. Intuitively, country as a candi-

Ranking Candidates. We use a normalized version of tfidf (term frequency-inverse document frequency) [29] to rank

1250

Algorithm 1 PDiscovery

Algorithm 2 TypePruning

Input: a table T , a KB K, and a number k.
Output: top-k table patterns based on their scores
1: types(Ai ) ← get a ranked list of candidate types for Ai
2: properties(Ai , Aj ) ← get a ranked list of candidate relationships for Ai and Aj
3: Let P be the top-k table patterns, initialized empty
4: for all Ti ∈ types(Ai ), and Pij ∈ properties(Ai , Aj ) in descending order of tf-idf scores do
5: if |P| > k and TypePruning(Ti ) then
6:
continue
7: generate all table patterns P 0 involving Ti or Pij
8: compute the score for each table pattern P in P 0
9: update P using P 0
10: compute the upper bound score B of all unseen patterns,
and let ϕk ∈ P be the table pattern with lowest score
11: halt when score(ϕk ) > B
12: return P

Input: current top-k table patterns P, candidate type Ti .
Output: a boolean value, true/false means Ti can/cannot be
pruned
1: curMinCohSum(Ai ) ← minimum sum of all coherence scores
involving column Ai in current top-k P
2: maxCohSum(Ai , Ti ) ← maximum sum of all coherence scores
if the type of column Ai is Ti
3: if maxCohSum(Ai , Ti ) < curMinCohSum(Ai ) then
4: return true
5: else
6: return false
subSC(economy, hasCapital) = 0.84
subSC(country, hasCapital) = 0.86
objSC(city, hasCapital) = 0.69
objSC(capital, hasCapital) = 0.83

These scores reflect our intuition in Example 5: country is
more suitable than economy to act as a type for the subject
resources of hasCapital; and capital is more suitable than city
to act as a type for the object resources of hasCapital.
2

date type for column B is more compatible with hasCapital
than economy since capitals are associated with countries,
not economies. In addition, capital is also more compatible
with hasCapital than city since not all cities are capitals. 2

We now define the score of a pattern ϕ as follows:
score(ϕ) = Σm
i=0 tf-idf(Ti , Ai ) + Σij tf-idf(Pij , Ai , Aj )
+Σij (subSC(Ti , Pij ) + objSC(Tj , Pij ))

Based on the above observation, to quantify the “compatibility” between a type T and relationship P , where T serves
as the type for the resources appearing as subjects of the
relationship P , we introduce a coherence score subSC(T, P ).
Similarly, to quantify the “compatibility” between a type
T and relationship P , where T serves as the type for the
entities appearing as objects of the relationship P , we introduce a coherence scores objSC(T, P ). subSC(T, P ) (resp.
objSC(T, P )) measures how likely an entity of Type T appears as a subject (resp. object) of the relationship P .
We use pointwise mutual information (PMI) [10] as a
proxy for computing subSC(T, P ) and objSC(T, P ). We use
the following notations: ENT(T ) - the set of entities in K
of type T , subENT(P ) - the set of entities in K that appear in the subject of P , objENT(P ) - the set of entities
in K that appear in the object of P , and N - the total
number of entities in K. We then consider the following
)|
, the probability of an entity
probabilities: Pr(T ) = |ENT(T
N
|subENT(P )|
, the probability of an
belonging to T , Prsub (P ) =
N
)|
entity appearing in the subject of P , Probj (P ) = |objENT(P
,
N
the probability of an entity appearing in the object of P ,
)|
Prsub (P ∩ T ) = |ENT(T )∩subENT(P
, the probability of an enN
tity belonging to type T and appearing in the subject of P ,
)|
and Probj (P ∩ T ) = |ENT(T )∩objENT(P
, the probability of an
N
entity belonging to type T and appearing in the object of
P . Finally, we can define PMIsub (T, P ):
PMIsub (T, P ) = log

4.3

Given the scoring model of table patterns, we describe how
to retrieve the top-k table patterns with the highest scores
without having to enumerate all candidates. We formulate
this as a rank-join problem [22]: given a set of sorted lists
and join conditions of those lists, the rank-join algorithm
produces the top-k join results based on some score function
for early termination without consuming all the inputs.
Algorithm. The algorithm, referred as PDiscovery, is
given in Algorithm 1. Given a table T , a kb K, and a
number k, it produces top-k table patterns. To start, each
input list, i.e., candidate types for a column, and candidate relationships for a column pair, is ordered according
to the respective tf-idf scores (lines 1-2). When two candidate types (resp. relationships) have the same tf-idf scores,
the more discriminative type (resp. relationship) is ranked
higher, i.e., the one with less number of instances in K.
Two lists are joined if they agree on one column, e.g.,
the list of candidate types for Ai is joined with the list of
candidate relationships for Ai and Aj . A join result is a candidate pattern ϕ, and the scoring function is score(ϕ). The
rank-join algorithm scans the ranked input lists in descending order of their tf-idf scores (lines 3-4), table patterns are
generated incrementally as we move down the input lists.
Table patterns that cannot be used to produce top-k patterns will be pruned (lines 5-6). For each join result, i.e.,
each table pattern ϕ, the score score(ϕ) is computed (lines 78). We also maintain an upper bound B of the scores of all
unseen join results, i.e., table patterns (line 10). Since each
list is ranked, B can be computed by adding up the support scores of the current positions in the ranked lists, plus
the maximum coherence scores a candidate relationship can
have with any types. We terminate the join process if either
we have exhaustively scanned every input list, or we have
obtained top-k table patterns and the score of the kth table
pattern is greater than or equal to B (line 11).
Lines 5-6 in Algorithm 1 check whether a candidate type
Ti for column Ai can be pruned without generating ta-

Prsub (P ∩ T )
Prsub (P )Pr(T )

The PMI can be normalized into [−1, 1] as follows [3]:
NPMIsub (T, P ) =

PMIsub (T, P )
−Prsub (P ∩ T )

To ensure that the coherence score is in [0, 1], we define
the subject semantic coherence of T for P as
subSC(T, P ) =

Top-k Table Pattern Generation

NPMIsub (T, P ) + 1
2

The object semantic coherence of T for P can be defined
similarly.
Example 6: Below are sample coherence scores computed
from Yago.

1251

'1: B (country), C (capital), hasCapital (B, C)
'2: B (economy), C (city), hasCapital (B, C)

ble patterns involving Ti by calling Algorithm 2. The intuition behind type pruning (Algorithm 2) is that a candidate type Ti is useful if it is more coherent with any
relationship Pix than previously examined types for Ai .
We first calculate the current minimum sum of coherence
scores involving column Ai in the current top-k patterns,
i.e., curMinCohSum(Ai ) (line 1). We then calculate the
maximum possible sum of coherence scores involving type
Ti , i.e., maxCohSum(Ai , Ti ) (line 2). Ti can be pruned if
maxCohSum(Ai , Ti ) < curMinCohSum(Ai ) since any table
pattern having Ti as the type for Ai will have a lower score
than the scores of the current top-k patterns (lines 3-6).

type (B)
economy(1.0)
country(1.0)
location(1.0)
state(0.7)
…

type (C)

locatedIn(1.0)
hasCapital(0.9)

City(1.0)
Capital(0.9)
whole(0.5)
artifact(0.1)
Person(0.1)
…

Figure 4: Encoding top-k as a rank-join

Example 7: Consider the rank-join graph in Fig. 4
(k = 2) for a table with just two columns B and
C as in Fig. 1.
The tf-idf scores for each candidate type and relationship are shown in the parentheses. The top-2 table patterns ϕ1 , ϕ2 are shown on the
top.
score(ϕ1 ) = sup(country, B) + sup(capital, C) +
sup(hasCapital, B, C) + 5 × (subSC(country, hasCapital) +
objSC(capital, hasCapital)) = 1.0 + 0.9 + 0.9 + 0.86 + 0.83 =
4.49. Similarly, we have score(ϕ2 ) = 4.47.
Suppose we are currently examining type state for column
B. We do not need to generate table patterns involving state
since the maximum coherence between state and hasCapital
or isLocatedIn is less than the the current minimum coherence score between type of column B and relationship between B and C in the current top-2 patterns.
Suppose we are examining type whole for column C, and
we have reached type state for B and hasCapital for relationship B, C. The bound score for all unseen patterns is
B = 0.7 + 0.5 + 0.9 + 0.86 + 0.83 = 3.78, where 0.7, 0.9 and
0.5 are the tf-idf scores for state, whole and hasCapital respectively, and 0.86 (resp. 0.83) is the maximum coherence
score between any type in types(B) (resp. types(C)) and
any relationship in properties(B, C). Since B is smaller than
score(ϕ2 ) = 4.47, we terminate the rank join process.
2

5.1

Creating Questions for the Crowd

A naive approach to generate crowdsourcing questions is
to express each candidate table pattern as a whole in a single
question to the crowd who would then select the best one.
However, table pattern graphs can be hard for crowd users to
understand (e.g., Fig. 2(a)). Also, crowd workers are known
to be good at answering simple questions [41]. A practical
solution is to decompose table patterns into simple tasks:
(1) type validation, i.e., to validate the type of a column
in the table pattern; and (2) binary relationship validation,
i.e., to validate the relationship between two columns.
Column type validation. Given a set of candidate types
candT(Ai ) for column Ai , one type Ti ∈ candT(Ai ) needs
to be selected. We formulate the following question to the
crowd about the type of a column: What is the most accurate
type of the highlighted column?; along with kt randomly chosen tuples from T and all candidate types from candT(Ai ).
A sample question is given as follows.
Q1 :What is the most accurate type of the highlighted column?
(A, B , C, D, E, F , ...)
(Rossi, Italy , Rome, Verona, Italian, Proto, ...)
(Pirlo, Italy , Madrid, Juve, Italian, Flero ...)
”

 country

 economy

 state

 none of the above

Correctness. Algorithm 1 is guaranteed to produce the
top-k table patterns since we keep the current top-k patterns
in P, and we terminate when we are sure that it will not
produce any new table pattern with a higher score. In the
worst case, we still have to exhaustively go through all the
ranked lists to produce the top-k table patterns. However,
in most cases the top ranked table patterns involve only
candidate types/relationships with high tf-idf scores, which
are at the top of the lists.
Computing coherence scores for a type and a relationship is an expensive operation that requires set intersection.
Therefore, for a given K, we compute offline the coherence
score for every type and every relationship. For each relationship, we also keep the maximum coherence score it can
achieve with any type, to efficiently compute the bound B.

5.

relationship (B, C)

After q questions are answered by the crowd workers, the
type with the highest support from the workers is chosen.
Crowd workers, even if experts in the reference kb, are
prone to mistakes when t[Ai ] in tuple t is ambiguous, i.e.,
t[Ai ] belongs to multiple types in candT(Ai ). However, this
is mitigated by two observations: (i) it is unlikely that all
values are ambiguous and (ii) the probability of providing
only ambiguous values diminishes quickly with respect to
the number of values. Consider two types T1 and T2 in
candT(Ai ), the probability that randomly selected entities
T (T1 )∩EN T (T2 )|
. After q
belong to both types is p = |EN
|EN T (T1 )∪EN T (T2 )|
questions are answered, the probability that all q · kt values
are ambiguous is pq·kt . Suppose p = 0.8, a very high for two
types in K, and five questions are asked with each question
containing five tuples, i.e., q = 5, kt = 5, the probability
pq·kt becomes as low as 0.0038.
For each question, we also expose some contextual attribute values that help workers better understand the question. For example, we expose the values for A, C, D, E in
question Q1 when validating the type of B. If the the number of attributes is small, we show them all; otherwise, we
use off-the-shelf technology to identify attributes that are
related to the ones in the question [23]. To mitigate the risk
of workers making mistakes, each question is asked three
times, and the majority answer is taken. Indeed, our empir-

PATTERN VALIDATION VIA CROWD

We now study how to use the crowd to validate the discovered table patterns. Specifically, given a set P of candidate
patterns, a table T , a kb K, and a crowdsourcing framework, we need to identify the most appropriate pattern for
T w.r.t. K, with the objective of minimizing the number of
crowdsourcing questions. We assume that the crowd workers are experts in the semantics of the reference kbs, i.e.,
they can verify if values in the tables fit into the kbs.

1252

Algorithm 3 PatternValidation

ical study in Section 7.2 shows that five questions are enough
to pick the correct type in all the datasets we experimented.

Input: a set of table patterns P
Output: one table pattern ϕ ∈ P
1: Pre be the remaining table patterns, initialized P
2: initialize all variables V , representing column or column pairs,
and calculate their probability distributions.
3: while |Pre | > 1 do
4: Ebest ← 0
5: vbest ← null
6: for all v ∈ V do
7:
compute the entropy H(v).
8:
if H(v) > Ebest then
9:
vbest ← v
10:
Ebest ← H(v)
11: validate the variable v, suppose the result is a, let Pv=a
to be the set of table patterns with v = a
12: Pre = Pv=a
13: normalize the probability distribution of patterns in Pre .
14: return the only table pattern ϕ in Pre

Relationship validation. We validate the relationship for
column pairs in a similar fashion, with an example below.
Q2 :What is the most accurate relationship for
highlighted columns (A, B, C , D, E, F , ...)
(Rossi, Italy, Rome , Verona, Italian, Proto, ...)
(Pirlo, Italy, Madrid , Juve, Italian, Flero, ...)

 B hasCapital C 
 C locatedIn B 
 none of the above

Candidate types and candidate relationships are stored
as URIs in kbs; thus not directly consumable by the
crowd workers. For example, the type capital is stored as
http://yago-knowledge.org/resource/wordnet capital 10851850,
and the relationship hasCapital is stored as http://yagoknowledge.org/resource/hasCapital. We look up type and
relationship descriptions, e.g., capital and hasCapital, by
querying the kb for the labels of the corresponding URIs.
If no label exists, we process the URI itself by removing the
text before the last slash and punctuation symbols.

5.2

After validating a variable v to have value a, we remove
from P those patterns that have different assignment for
v. The remaining patterns are denoted as Pv=a . Suppose column B is validated to be of type country, then
PvB =country = {ϕ1 , ϕ3 , ϕ4 }. Since we do not know what value
a variable can take, we measure the expected reduction of
uncertainty of variable ϕ after validating variable v, formally
defined as:
E(∆H(ϕ))(v) = Σa P r(v = a)HPv=a (ϕ) − HP (ϕ)

Question Scheduling

We now turn our attention to how to minimize the total
number of questions to obtain the correct table pattern by
scheduling which column and relationship to validate first.
Note that once a type (resp. relationship) is validated, we
can prune from P all table patterns that have a different type
(resp. relationship) for that column (resp. column pair).
Therefore, a natural choice is to choose those columns (resp.
column pairs) with the maximum uncertainty reduction [45].
Consider ϕ as a variable, which takes values from P =
{ϕ1 , ϕ2 , . . . , ϕk }. We translate the score associated with
each table pattern to a probability by normalizing the
i)
. Our translascores, i.e., P r(ϕ = ϕi ) = Σϕ score(ϕ
∈P score(ϕj )

In each iteration, we choose the variable v (column
or column pair) with the maximum uncertainty reduction, i.e., E(∆H(ϕ))(v). Each iteration has a complexity of O(|V ||P|2 ) because we need to examine all |V | variables, each variable could take |P| values, and calculating
HPv=a (ϕ) for each value also takes O(|P|) time. The following theorem simplifies the calculation for E(∆H(v)) with a
complexity of O(|V ||P|).
Theorem 1. The expected uncertainty reduction after
validating a column (column pair) v is the same as the
entropy of the variable. E(∆H(ϕ))(v) = H(v), where
H(v) = −Σa P r(v = a) log2 P r(v = a).
The proof of Theorem 1 can be found in Appendix A.
Algorithm 3 describes the overall procedure for pattern validation. At each iteration: (1) we choose the best variable
vbest to validate next based on the expected reduction of
uncertainty of ϕ (lines 4-10); (2) we remove from Pre those
table patterns that have a different assignment for variable
v than the validated value a (lines 11-12); and (3) we renormalize the probability distribution of the remaining table
patterns in Pre (line 13). We terminate when we are left
with only one table pattern (line 3).

j

tion from scores to probabilities follows the general framework of interpreting scores in [25]. Specifically, our translation is rank-stable, i.e., for two patterns ϕ1 and ϕ2 , if
score(ϕ1 ) > score(ϕ2 ), then P r(ϕ = ϕ1 ) > P r(ϕ = ϕ2 ).
We define the uncertainty of ϕ w.r.t. P as the entropy.
HP (ϕ) = −Σϕi ∈P P r(ϕ = ϕi ) log2 P r(ϕ = ϕi )
Example 8: Consider an input list of five table patterns
P = {ϕ1 , . . . , ϕ5 } as follows with the normalized probability
of each table pattern shown in the last column.
ϕ1
ϕ2
ϕ3
ϕ4
ϕ5

type (B)
country
economy
country
country
state

type (C)
capital
capital
city
capital
capital

P (B, C)
hasCapital
hasCapital
locatedIn
locatedIn
hasCapital

score
2.8
2
2
0.8
0.4

prob
0.35
0.25
0.25
0.1
0.05

Example 9: To validate the five patterns in Example 8,
we first calculate the entropy of every variable. H(vB ) =
−0.7 log2 0.7 − 0.25log2 0.25 − 0.05log2 0.05 = 1.07, H(vC ) =
0.81, and H(vBC ) = 0.93. Thus column B is validated first,
say the answer is country. The remaining set of table patterns, and their normalized probabilities are:

2

We use variables vAi and vAi Aj to denote the type of the
column Ai and the relationship between Ai and Aj respectively. The set of all variables is denoted as V . In Example 8, V = {vB , vC , vBC }, vB ∈ {country, economy, state},
vC ∈ {capital, city} and vBC ∈ {hasCapital, isLocatedIn}.
The probability of an assignment of a variable v to a is obtained by aggregating the probability of those table patterns
that have that assignment for v. For example, P r(vB =
country) = P r(ϕ1 ) + P r(ϕ3 ) + P r(ϕ4 ) = 0.35 + 0.25 + 0.1 =
0.7, P r(vB = economy) = 0.25, and P r(vB = state) = 0.05.

ϕ1
ϕ3
ϕ4

type (B)
country
country
country

type (C)
capital
city
capital

P (B, C)
hasCapital
locatedIn
locatedIn

prob
0.5
0.35
0.15

Now Pre = {ϕ1 , ϕ3 , ϕ4 }. The new entropies are: H(vB ) =
0, H(vC ) = 0.93 and H(vBC ) = 1. Therefore, column pair

1253

hasCapital
B (Italy)

hasCapital
C (Rome)

hasOfficalLanguage

nationality
A (Pirlo)

B (Spain)
nationality

E (Italian)
locatedIn

hasClub
bornIn
D (Juve)

F (Flero)

(a) Possible repair G1

Algorithm 4 Top-k repairs

C (Madrid)

Input: a tuple t, a table pattern ϕ, and inverted lists L
Output: top-k repairs for t
1: Gt = ∅
2: for each attribute A in ϕ do
3: Gt = Gt ∪ L(A, t[A])
4: for each G in Gt do
5: compute cost(t, ϕ, G)
6: return top-k repairs in Gt with least cost values

hasOfficalLanguage

A (Xabi
Alonso)

E (Spanish)
locatedIn

hasClub
D (Real
Madrid)

bornIn
F (Tolosa)

(b) Possible repair G2

Figure 5: Sample instance graphs
edges EI , such that (i) each node vi ∈ VI is a resource in K;
(ii) each edge ei ∈ EI is a property in K; (iii) there is a oneto-one correspondence f from each node v ∈ V to a node
vi ∈ VI , i.e., f (v) = vi ; and (iv) for each edge (u, v) ∈ E,
there is an edge (f (u), f (v)) ∈ EI with the same property.
Intuitively, an instance graph is an instantiation of a pattern
in a given kb.

B, C is chosen, say the answer is hasCapital. We are now left
with only one pattern ϕ1 , thus we return it.
2
In Example 9, we do not need to validate vC following our
scheduling strategy. Furthermore, after validating certain
variables, other variables may become less uncertain, thus
requiring a smaller number of questions to validate.

6.

Example 11: Figures 5(a) and 5(b) are two instance graphs
of the table pattern of Fig. 2(a) in Yago for two players. 2

DATA ANNOTATION

In this section, we describe how Katara annotates data
(Section 6.1). We also discuss how to generate possible repairs for identified errors (Section 6.2).

6.1

Repair cost. Given an instance graph G, a tuple t, and a
table pattern ϕ, the repair cost of aligning t to G w.r.t. ϕ,
n
P
denoted by cost(t, ϕ, G) =
ci , is the cost of changing

Annotating Data

i=1

Katara annotates tuples as correct data validated by kbs,
correct data jointly validated by kbs and the crowd, or data
errors detected by the crowd, using the following two steps.

values in t to align it with G, where ci is the cost of the i-th
change and n the number of changes in t. Intuitively, the less
a repair cost is, the closer the updated tuple is to the original
tuple, hence more likely to be correct. By default, we set
ci = 1. The cost can also be weighted with confidences on
data values [18]. In such case, the higher the confidence
value is, the more costly the change is.

Step 1: Validation by kbs. For each tuple t and pattern ϕ,
Katara issues a SPARQL query to check whether t is fully
covered by a kb K. If it is fully covered, Katara annotates
it as a correct tuple validated by kb (case (i)). Otherwise,
it goes to step 2.

Example 12: Consider tuple t3 in Fig. 1, the table pattern
ϕs in Fig. 2(a), and two instance graphs G1 and G2 in Fig. 5.
The repair cost to update t3 to G1 is 1, i.e., cost(t3 , ϕs , G1 )
= 1, by updating t3 [C] from Madrid to Rome. Similarly, the
repair cost from t3 to G2 is 5, i.e., cost(t3 , ϕs , G2 ) = 5. 2

Step 2: Validation by kbs and Crowd. For each node (i.e.,
type) and edge (i.e., relationship) that is missing from K,
Katara asks the crowd whether the relationship holds between the given two values. If the crowd says yes, Katara
annotates it as a correct tuple, jointly validated by kb and
crowd (case (ii)). Otherwise, it is certain that there exist
errors in this tuple (case (iii)).

Note that the possible repairs are ranked based on repair
cost in ascending order. We provide top-k possible repairs
and we leave it to the users (or crowd) to pick the most
appropriate repair. In the following, we describe algorithms
to generate top-k repairs for each identified erroneous tuple.
Given a kb K and a pattern ϕ, we compute all instance
graphs G in K w.r.t. ϕ. For each tuple t, a naive solution is
to compute the distance between t and each graph G in G.
The k graphs with smallest repair cost are returned as top-k
possible repairs. Unfortunately, this is too slow in practice.
A natural way to improve the naive solution for top-k
possible repair generation is to retrieve only instance graphs
that can possibly be repairs, i.e., the instance graphs whose
values have an overlap with a given erroneous tuple. We
leverage inverted lists to achieve this goal.

Example 10: Consider tuple t2 (resp. t3 ) in Fig. 1 and the
table pattern in Fig. 2(a). The information about whether
Pretoria (resp. Madrid) is a capital of S. Africa (resp. Italy)
is not in kb. To verify this information, we issue a boolean
question Qt2 (resp. Qt3 ) to the crowd as:
Qt2 :Does S. Africa hasCapital Pretoria?

 Yes

 No
Qt3 :Does Italy hasCapital Madrid?

 Yes

 No

In such case, the crowd will answer yes (resp. no) to
question Qt2 (resp. Qt3 ).
2

Inverted lists. Each inverted list is a mapping from a key to
a posting list. A key is a pair (A, a) where A is an attribute
and a is a constant value. A posting list is a set G of graph
instances, where each G ∈ G has value a on attribute A.
For example, an inverted list w.r.t. G1 in Fig. 5(a) is as:

Knowledge base enrichment. Note that, in step 2, for
each affirmative answer from the crowd (e.g., Qt2 above), a
new fact that is not in the current kb is created. Katara
collects such facts and uses them to enrich the kb.

6.2

Generating Top-k Possible Repairs

country, Italy → G1

We start by introducing two notions that are necessary to
explain our approach for generating possible repairs.

Algorithm. The optimized algorithm for a tuple t is given
in Algorithm 4. All possible repairs are initialized (line 1)
and instantiated by using inverted lists (lines 2-3). For each

Instance graphs. Given a kb K and a pattern G(V, E),
an instance graph GI (VI , EI ) is a graph with nodes VI and

1254

Yago
DBPedia
#-type #-relationship #-type #-relationship
WikiTables
54
15
57
18
WebTables
71
33
73
35
RelationalTables
14
7
14
16

possible repair, its repair cost w.r.t. t is computed (lines 45), and top-k repairs are returned (line 6).
Example 13: Consider t3 in Fig. 1 and pattern ϕs in
Fig. 2(a). The inverted lists retrieved are given below.
A, Pirlo

→ G1

B, Italy

→ G1

C, Madrid → G2

X
X
X

D, Juve

→ G1

E, Italian

→ G1

F, Flero

→ G1

X
X
X

Table 1: Datasets and kbs characteristics

7.1

Algorithms. We compared four discovery algorithms.
(i) RankJoin - our proposed approach (Section 4).
(ii) Support - a baseline approach that ranks the candidate
types and relationships solely on their support scores, i.e.,
the number of tuples that are of the candidate’s types and
relationships.
(iii) MaxLike [39] - infers the type of a column and the relationship between a column pair separately using maximum
likelihood estimation.
(iv) PGM [28] - infers the type of a column, the relationship
between column pairs, and the entities of cells by building a
probabilistic graphic model to make holistic decisions.

It is easy to see that the occurrences of instance graphs
G1 and G2 are 5 and 1, respectively. In other words, the
cost of repairing t3 w.r.t. G1 is 6 − 5 = 1 and w.r.t. G2 is
6 − 1 = 5. Hence, the top-1 possible repair for t3 is G1 . 2
The practicability of possible repairs of Katara depends
on the coverage of kbs, while existing automatic data repairing techniques usually require certain redundancy in the
data to perform well. Katara and existing techniques complement each other, as demonstrated in Section 7.4.

7.

Pattern Discovery

EXPERIMENTAL STUDY

We evaluated Katara using real-life data along four dimensions: (i) the effectiveness and efficiency of table pattern
discovery (Section 7.1); (ii) the efficiency of pattern validation via the expert crowd (Section 7.2); (iii) the effectiveness
and efficiency of data annotation (Section 7.3); and (iv) the
effectiveness of possible repairs (Section 7.4).

Evaluation Metrics. A type (relationship) gets a score of
1
1 if it matches the ground truth, and a partial score s+1
if it is the super type (relationship) of the ground truth,
where s is the number of steps in the hierarchy to reach the
ground truth. For example, a label Film for a column, whose
actual type is IndianFilm, will get a score of 0.5, since Film
is the super type of IndianFilm with s = 1. The precision P
of a pattern ϕ is defined as the sum of scores for all types
and relationships in ϕ over the total number of types and
relationships in ϕ. The recall R of ϕ is defined as the sum
of scores for all types and relationships in ϕ over the total
number of types and relationships in the ground truth.

Knowledge bases. We used Yago [21] and DBpedia [27] as
the underlying kbs. Both were transformed to Jena format
(jena.apache.org/) with LARQ (a combination of ARQ
and Lucene) support for string similarity. We set the threshold to 0.7 in Lucene to check whether two strings match.
Datasets.
We used three datasets: WikiTables and
WebTables contains tables from the Web2 with relatively
small numbers of tuples and columns, and RelationalTables
contains tables with larger numbers of tuples and columns.
• WikiTables contains 28 tables from Wikipedia pages. The
average number of tuples is 32.
• WebTables contains 30 tables from Web pages. The average number of tuples is 67.
• RelationalTables has three tables: Person has personal information joined on the attribute country from two sources:
a biographic table extracted from wikipedia [32], and a country table obtained from a wikipedia page3 resulting in 316K
tuples. Soccer has 1625 tuples about soccer players and their
clubs scraped from the Web4 . University has 1357 tuples
about US universities with their addresses5 .
All the tables were manually annotated using types and relationships in Yago as well as DBPedia, which we considered
as the ground truth. Table 1 shows the number of columns
that have types, and the number of column pairs that have
relationships, using Yago and DBPedia, respectively.
All experiments were conducted on Win 7 with an Intel i7
CPU@3.4Ghz, 20GB of memory, and an SSD 500GB hard
disk. All algorithms were implemented in JAVA.

Effectiveness. Table 2 shows the precision and recall of
the top pattern chosen by four pattern discovery algorithms
for three datasets using Yago and DBPedia. We first discuss Yago. (1) Support has the lowest precision and recall in all scenarios, since it selects the types/relationships
that cover the most number of tuples, which are usually
the general types, such as Thing or Object. (2) MaxLike
uses maximum likelihood estimation to select the best
type/relationship that maximizes the probability of values given the type/relationship. It performs better than
Support, but still chooses types and relationships independently. (3) PGM is a supervised learning approach
that requires training and tuning of a number of weights.
PGM shows mixed effectiveness results: it performs better
than MaxLike on WebTables, but worse on WikiTables and
RelationalTables. (4) RankJoin achieves the highest precision and recall due to its tf-idf style ranking, as well as for
considering the coherence between types and relationships.
For example, consider a table with two columns actors and
films that have a relationship actedIn. If most of the values in the films column also happen to be books, MaxLike
will use books as the type, since there are fewer instances of
books than films in Yago. However, RankJoin would correctly identify films as the type, since it is more coherent
with actedIn than books.
The result from DBPedia, also shown in Table 2, confirms
that RankJoin performs best among the four methods. Notice that the precision and recall of all methods are consistently better using DBPedia than Yago. This is because the

2

http://www.it.iitb.ac.in/~sunita/wwt/
http://tinyurl.com/qhhty3p
4
www.premierleague.com/en-gb.html, www.legaseriea.it/en/,
www.premierleague.com/en-gb.html
5
ope.ed.gov/accreditation/GetDownLoadFile.aspx
3

1255

WikiTables
WebTables
RelationalTables

Support
P
R
.54 .59
.65 .64
.51 .51

MaxLike
P
R
.62 .68
.63 .62
.71 .71
Yago
R
P
R
.70 .71 .89
.69 .80 .84
.67 .81 .86
DBPedia

PGM
P
R
.60 .67
.77 .77
.53 .53

RankJoin
P
R
.78 .86
.86 .84
.77 .77

WikiTables
WebTables
RelationalTables

P
.56
.65
.64

P
.61
.76
.74

P
.71
.82
.81

R
.77
.80
.77

Support MaxLike
153
155
160
177
130
140
252
258
Yago
WikiTables
50
54
WebTables
103
104
RelationalTables/Person
400
574
Person
368
431
DBPedia

R
.89
.87
.86

1

0.99

1

0.95

0.98

0.999

0.9

0.97

0.998

0.96

0.997

0.75

Support
MaxLike
PGM
RankJoin

0.7
0.65
0.6
0

2

4

6

8 10 12 14 16 18 20

k

0.85

P/R

0.8

0.8
0.75

Support
MaxLike
PGM
RankJoin

0.7
0.65
0.6
0

2

4

6

P/R

1

0.85

90
189
11047
N.A.

51
107
409
410

Table 3: Pattern discovery efficiency (seconds)

0.95

F-Measure at k

F-Measure at k

Table 2: Pattern discovery precision and recall

0.9

PGM RankJoin
286
153
1105
162
13842
127
N.A.
257

WikiTables
WebTables
RelationalTables/Person
Person

0.95
0.94

0.995

0.93

0.994

Precision
Recall

0.92

8 10 12 14 16 18 20

0.996

1

2

3

4

5

6

8

9

q

k

Precision
Recall

0.993
7

1

2

3

4

5

6

7

8

(a) Yago
(b) DBPedia
Figure 6: Top-k F-measure (WebTables)

(a) Yago
(b) DBPedia
Figure 7: Pattern validation P/R (WebTables)

number of types in DBPedia (865) is much smaller than that
of Yago (374K), hence, the number of candidate types for a
column using DBPedia is much smaller, causing less stress
for all algorithms to rank them.
To further verify the effectiveness of our ranking function,
we report the F-measure F of the top-k patterns chosen by
every algorithm. The F value of the top-k patterns is defined as the best value of F from one of the top-k patterns.
Figure 6 shows F values of the top-k patterns varying k on
WebTables. RankJoin converges faster than other methods
on Yago, while all methods converge quickly on DBPedia due
to its small number of types. Top-k F-measure results for the
other two datasets show similar behavior, and are reported
in Appendix B.

Yago
MUVF AVI
64
79
81
105
24
28

WikiTables
WebTables
RelationalTables

DBPedia
MUVF AVI
88
102
90
118
28
36

Table 4: #-variables to validate
sure the precision and recall of the final chosen validation
w.r.t. the ground truth in the same way as in Section 7.1.
Figure 7 shows the average precision and recall of the validated pattern of WebTables while varying the number of
questions q per variable. It can be seen that, even with
q = 1, the precision and recall of the validated pattern is
already high. In addition, the precision and recall converge
quickly, with q = 5 on Yago, and q = 3 on DBPedia. Pattern
validation results on WikiTables and RelationalTables show a
similar behavior, and are reported in Appendix C.
To evaluate the savings in crowd pattern validation that
are achieved by our scheduling algorithm, we compared
our method (denoted MUVF, short for most-uncertainvariable-first) with a baseline algorithm (denoted AVI for
all-variables-independent) that validates every variable independently. For each dataset, we compared the number of
variables needed to be validated until there is only one table
pattern left. Table 4 shows that MUVF performs consistently
better than AVI in terms of the number of variables to validate, because MUVF may spare validating certain variables
due to scheduling, i.e., some variables become certain after
validating some other variables.
The validated table patterns of RelationalTables for both
Yago and DBPedia are depicted in Fig. 10 in the Appendix.
All validated patterns are also used in the following experimental study.

Efficiency. Table 3 shows the running time in seconds
for all datasets. We ran each test 5 times and report
the average time. We separate the discussion of Person
from RelationalTables due to its large number of tuples.
For Person, we implemented a distributed version of candidate types/relationships generation by distributing the
316K tuples over 30 machines, and all candidates are collected into one machine to complete the pattern discovery.
Support, MaxLike, and RankJoin have similar performance
in all datasets, because their most expensive operation is
the disk I/Os for kbs lookups in generating candidate types
and relationships, which is linear w.r.t. the number of tuples. PGM is the most expensive due to the message passing
algorithms used for the inference of probabilistic graphical
model. PGM takes hours on tables with around 1K tuples,
and cannot finish within one day for Person.

7.2

9

q

Pattern Validation

Given the top-k patterns from the pattern discovery, we
need to identify the most appropriate one. We validated
the patterns of all datasets using an expert crowd with 10
students. Each question contains five tuples, i.e., kt = 5.
We first evaluated the effect of the number of questions used to validate each variable, which is a type or a
relationship, on the quality of the chosen pattern. We mea-

7.3

Data Annotation

Given the table patterns obtained from Section 7.2, data
values are annotated w.r.t. types and relationships in the
validated table patterns, using kbs and the crowd. The
result of data annotation is shown in Table 5. Note that

1256

WikiTables
WebTables
RelationalTables

kb
0.60
0.69
0.83

WikiTables
WebTables
RelationalTables

kb
0.73
0.74
0.90

type
crowd error
0.39
0.01
0.28
0.03
0.17
0
Yago
crowd error
0.25
0.02
0.24
0.02
0.10
0
DBPedia

kb
0.56
0.56
0.89

relationship
crowd error
0.42
0.02
0.39
0.05
0.11
0

kb
0.60
0.56
0.91

crowd
0.36
0.39
0.09

Katara (Yago)
P
R
Person
1.0
0.80
Soccer
N.A.
University 0.95
0.74

Table 6:
Data repairing precision and recall
(RelationalTables)

error
0.04
0.05
0

Since the average number of tuples in WikiTables and
WebTables is 32 and 67, respectively, both datasets are
not suitable since both EQ and SCARE require reasonable data redundancy to compute repairs. Hence, we use
RelationalTables for comparison. We learn from Table 5 that
tables in RelationalTables are clean, and thus are treated as
ground truth. Thus, for each table in RelationalTables, we
injected 10% random errors into columns that are covered by
the patterns to obtain a corresponding dirty instance, that
is, each tuple has a 10% chance of being modified to contain
errors. Moreover, in order to set up a fair comparison, we
used FDs for EQ that cover the same columns as the crowd
validated table patterns (see Appendix D). SCARE requires
that some columns to be correct. To enable SCARE to run,
we only injected errors to the right hand side attributes of
the FDs, and treated the left hand side attributes as correct
attributes (a.k.a. reliable attributes in [43]).

Table 5: Data annotation by kbs and crowd
1

F-Measure at k

F-Measure at k

1
0.8
0.6
0.4
0.2

person
university

0
1

2

3

4

5

6

7

0.8
0.6
0.4
person
soccer
university

0.2
0

8

9

k

(a) Yago

1

2

3

4

5

6

7

8

9

k

(b) DBPedia

Figure 8: Top-k repair F-measure (RelationalTables)
Katara annotates data in three categories (cf. Section 6.1):
when kb has coverage for a value, the value is said to be validated by the kb (kb column in Table 5), when the kb has no
coverage, the value is either validated by the crowd (crowd
column in Table 5), or the value is erroneous (error column in
Table 5). Table 5 shows the breakdown of the percentage of
values in each category. Data values validated by the crowd
can be used to enrich the kbs. For example, a column in one
of the table in WebTables is discovered to be the type state
capitals in the United States. Surprisingly, there are
only five instances of that type in Yago6 , we can add the
rest of 45 state capitals using values from the table to enrich Yago. Note that the percentage of kb validated data is
much higher for RelationalTables than it is for WikiTables and
WebTables. This is because data in RelationalTables is more
redundant (e.g., Italy appears in many tuples in Person table), when a value is validated by the crowd, it will be added
to the kb, thus future occurrences of the same value will be
automatically validated by the kb.

7.4

Effectiveness of k. We examined the effect of using top-k
repairs in terms of F-measure. The results for both Yago
and DBPedia are shown in Fig. 8. The result for soccer using
Yago is missing since the discovered table pattern does not
contain any relationship (cf. Fig. 10 in Appendix). Thus,
Katara cannot be used to compute possible repairs w.r.t.
Yago. We can see the F-measure stabilizes at k = 1 using
Yago, and stabilizes at k = 3 using DBPedia. The result tells
us that in general the correct repairs fall into the top ones,
which justifies our ranking of possible repairs. Next, we
report the quality of possible repairs generated by Katara,
fixing k = 3.
Results of RelationalTables. The precision/recall of Katara,
EQ and SCARE on RelationalTables, are reported in Table 6.
The result shows that Katara always has a high precision
in cases where kbs have enough coverage of the input data.
It also indicates that if Katara can provide top-k repairs,
it has a good chance that the ground truth will fall in them.
The recall of Katara depends on the coverage of the kbs
of the input dataset. For example, DBPedia has a lot of
information for Person, but relatively less for Soccer and
University. Yago cannot be used to repair Soccer because it
does not have relationships for Soccer.
Both EQ and SCARE have precision that is generally lower
than Katara, because EQ targets at computing a consistent
database with the minimum number of changes, which are
not necessarily the correct changes, and the result of SCARE
depends on many factors, such as the quality of the training
data in terms of its redundancy, and a threshold ML parameter that is hard to set precisely. The recall for both EQ and
SCARE is highly dependent on data redundancy, because
they both require repetition of data to either detect errors.

Effectiveness of Possible Repairs

In these experiments, we evaluate the effectiveness of our
possible repairs generation by (1) varying the number k of
possible repairs; and (2) comparing with other state of the
art automatic data cleaning techniques.
Metrics. We use standard precision, recall, and F-measure
for the evaluation, which are defined as follows.
precision = (#-corrected changed values)/(#-all changes)
recall
= (#-corrected changed values)/(#-all errors)
F-measure= 2 × (precision × recall)/(precision + recall)

For comparison with automatic data cleaning approaches,
we used an equivalence-class [2] (i.e., EQ) based approach provided by an open-source data cleaning tool
NADEEF [12], and a ML-based approach SCARE [43].
When Katara provides nonempty top-k possible repairs for
a tuple, we count it as correct if the ground truth falls in
the possible repairs, otherwise incorrect.
6

Katara (DBPedia)
EQ
SCARE
P
R
P
R
P
R
1.0
0.94
1.0 0.96 0.78 0.48
0.97
0.29
0.66 0.29 0.66 0.37
1.0
0.18
0.63 0.04 0.85 0.21

Results of WikiTables and WebTables. Table 7 shows the result of data repairing for WikiTables and WebTables. Both
EQ and SCARE are not applicable on WikiTables and
WebTables, because there is almost no redundancy in them.

http://tinyurl.com/q65yrba

1257

Katara (Yago)
P
R
WikiTables 1.0
0.11
WebTables 1.0
0.40

Katara (DBPedia) EQ SCARE
P
R
P/R P/R
1.0
0.30
N.A.
1.0
0.46
N.A.

tions [13, 28, 39]. Our pattern discovery module shares the
same goal. Compared with the state of the art [28, 39], our
rank join algorithm shows superiority in both effectiveness
and efficiency, as demonstrated in the experiments.
Several attempts have been made to do repairing based
on integrity constraints (ICs) [1, 9, 11, 17, 20]; they try to
find a consistent database that satisfies given ICs in a minimum cost. It is known that the above heuristic solutions do
not ensure the accuracy of data repairing [19]. To improve
the accuracy of data repairing, experts have been involved
as first-class citizen of data cleaning systems [19, 35, 44],
high quality reference data has been leveraged [19, 24, 42],
and confidence values have been placed by the users [18].
Katara differs from them in that Katara leverages kbs as
reference data. As remarked earlier, Katara and IC based
approaches complement each other.
Numerous studies have attempted to discover data quality rules, e.g., for CFDs [6] and for DCs [8]. Automatically
discovered rules are error-prone, thus cannot be directly fed
into data cleaning systems without verification by domain
experts. However, and as noted earlier, they can exploit the
output of Katara, as rules are easier to discover from clean
samples of the data [8].
Another line of work studies the problem of combining
ontological reasoning with databases [5, 33]. Although their
operation could also be used to enforce data validation, our
work differs in that we do not assume knowledge over the
constraints defined on the ontology. Moreover, constraints
are usually expressed with FO logic fragments that restrict
the expressive power to enable polynomial complexity in the
query answering. Since we limit our queries to instancechecking over RDFS, we do not face these complexity issues.
One concern with regards to the applicability of Katara
is the accuracy and coverage of the kbs and the quality
of crowdsourcing: neither the kbs nor the crowdsourcing
is ensured to be completely accurate. There are several efforts that aim at improving the quality and coverage of both
kbs [14–16] and crowdsourcing [4, 26]. With more accurate
and big kbs, Katara can discover the semantics of more
long tail tables, and further alleviate the involvement of experts. A full discussion of the above topics lies beyond the
scope of this work. Nevertheless, kbs and experts are usually more reliable than the data at hand, thus can be treated
as relatively trusted resources to pivot on.

Table 7:
Data repairing precision and recall
(WikiTables and WebTables)
Since there is no ground truth available for WikiTables and
WebTables, we manually examine the top-3 possible repairs
returned by Katara. As we can see, Katara achieves high
precision on WikiTables and WebTables as well. In total,
Katara fixed 60 errors out of 204 errors, which is 29%. In
fact, most of remaining errors in these tables are null values
whose ground truth values are not covered by given kbs.
Summary. It can be seen that Katara complements existing
automatic repairing techniques: (1) EQ and SCARE cannot
be applied to WebTables and WikiTables since there is not
enough redundancy, while Katara can, given kbs and the
crowd; (2) Katara cannot be applied when there is no coverage in the kbs, such as the case of Soccer with Yago; and
(3) when both Katara and automatic techniques can be
applied, Katara usually achieves higher precision due to
its use of kbs and experts, while automatic techniques usually make heuristic changes. The recall of Katara depends
on the coverage of the kbs, while the recall of automatic
techniques depends on the level of redundancy in the data.

8.

RELATED WORK

The traditional problems of matching relational tables and
aligning ontologies have been largely studied in the database
community. A matching approach where the user is also
aware of the target schema has been recently proposed [34].
Given a source and a target single relation, the user populates the empty target relation with samples of the desired
output until a unique mapping is identified by the system. A
recent approach that looks for isomorphisms between ontologies is PARIS [37], which exploits the rich information in the
ontologies in a holistic approach to the alignment. Unfortunately, our source is a relational table and our target is a
non-empty labeled graph, which make these proposals hard
to apply directly. On one hand, the first approach requires
to project all the entities and relationships in the target kb
as binary relations, which leads to a number of target relations to test that is quadratic w.r.t. the number of entities,
and only few instances in the target would match with the
source data. On the other hand, the second approach requires to test 2n combinations of source attributes, given a
relation with n attributes. The reason is that PARIS relies
on structural information, thus all possible attributes should
be tested together to get optimal results. If we tested only
binary relations, structural information would not be used
and inconsistent matches may arise. For example, attributes
A, B can be matched with X, Y in the KB, while at the same
time, attributes B, C may match Z, W , resulting in inconsistency (Attribute B matches two different classes X and
Z). Katara actually solves this problem by first retrieving
top-k types and relationships, and then using a rank-join
based approach to obtain the most coherent pattern.
Another line of related work is known as Web tables semantics understanding, which identifies the type of a column
and the relationship between two columns w.r.t. a given
kb, for the purpose of serving Web tables to search applica-

9.

CONCLUSION AND FUTURE WORK

We proposed Katara, the first end-to-end system that
bridges knowledge bases and crowdsourcing for high quality
data cleaning. Katara first establishes the correspondence
between the possibly dirty database and the available kbs
by discovering and validating the table patterns. Then each
tuple in the database is verified using a table pattern against
a kb with possible crowd involvement when the kb lacks
coverage. Experimental results have demonstrated both the
effectiveness and efficiency of Katara.
One important future work is to cold-start Katara when
there is no available kbs to cover the data, i.e., bootstrapping and extending the kbs at the intensional level by soliciting structural knowledge from the crowd. It would be
also interesting to assess the effects of using multiple kbs
together to repair one dataset. Another line of work is to
extend our current definition of tables patterns, such as a
person column A1 is related to a country column A2 via two
relationships: A1 wasBornIn city, and city isLocatedIn A2 .

1258

10.

REFERENCES

[27] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch,
D. Kontokostas, P. N. Mendes, S. Hellmann, M. Morsey,
P. van Kleef, S. Auer, and C. Bizer. DBpedia - a
large-scale, multilingual knowledge base extracted from
wikipedia. Semantic Web Journal, 6(2):167–195, 2015.
[28] G. Limaye, S. Sarawagi, and S. Chakrabarti. Annotating
and searching web tables using entities, types and
relationships. PVLDB, 3(1), 2010.
[29] C. D. Manning, P. Raghavan, and H. Schütze. Scoring,
term weighting and the vector space model. Introduction to
Information Retrieval, 100, 2008.
[30] C. Mayfield, J. Neville, and S. Prabhakar. ERACER: a
database approach for statistical inference and data
cleaning. In SIGMOD, 2010.
[31] M. Morsey, J. Lehmann, S. Auer, and A. N. Ngomo.
Dbpedia SPARQL benchmark - performance assessment
with real queries on real data. In ISWC, 2011.
[32] J. Pasternack and D. Roth. Knowing what to believe (when
you already know something). In COLING, 2010.
[33] A. Poggi, D. Lembo, D. Calvanese, G. D. Giacomo,
M. Lenzerini, and R. Rosati. Linking data to ontologies. J.
Data Semantics, 10, 2008.
[34] L. Qian, M. J. Cafarella, and H. V. Jagadish.
Sample-driven schema mapping. In SIGMOD, 2012.
[35] V. Raman and J. M. Hellerstein. Potter’s Wheel: An
interactive data cleaning system. In VLDB, 2001.
[36] S. Song, H. Cheng, J. X. Yu, and L. Chen. Repairing vertex
labels under neighborhood constraints. PVLDB, 7(11),
2014.
[37] F. M. Suchanek, S. Abiteboul, and P. Senellart. Paris:
Probabilistic alignment of relations, instances, and schema.
PVLDB, 2011.
[38] B. Trushkowsky, T. Kraska, M. J. Franklin, and P. Sarkar.
Crowdsourced enumeration queries. In ICDE, 2013.
[39] P. Venetis, A. Y. Halevy, J. Madhavan, M. Pasca, W. Shen,
F. Wu, G. Miao, and C. Wu. Recovering semantics of tables
on the web. PVLDB, 2011.
[40] M. Volkovs, F. Chiang, J. Szlichta, and R. J. Miller.
Continuous data cleaning. In ICDE, 2014.
[41] J. Wang, T. Kraska, M. J. Franklin, and J. Feng. Crowder:
Crowdsourcing entity resolution. PVLDB, 2012.
[42] J. Wang and N. Tang. Towards dependable data repairing
with fixing rules. In SIGMOD, 2014.
[43] M. Yakout, L. Berti-Equille, and A. K. Elmagarmid. Don’t
be SCAREd: use SCalable Automatic REpairing with
maximal likelihood and bounded changes. In SIGMOD,
2013.
[44] M. Yakout, A. K. Elmagarmid, J. Neville, M. Ouzzani, and
I. F. Ilyas. Guided data repair. PVLDB, 2011.
[45] C. J. Zhang, L. Chen, H. V. Jagadish, and C. C. Cao.
Reducing uncertainty of schema matching via
crowdsourcing. PVLDB, 6, 2013.

[1] S. Abiteboul, R. Hull, and V. Vianu. Foundations of
Databases. Addison-Wesley, 1995.
[2] P. Bohannon, W. Fan, M. Flaster, and R. Rastogi. A
cost-based model and effective heuristic for repairing
constraints by value modification. In SIGMOD, 2005.
[3] G. Bouma. Normalized (pointwise) mutual information in
collocation extraction. Proceedings of GSCL, pages 31–40,
2009.
[4] S. Buchholz and J. Latorre. Crowdsourcing preference tests,
and how to detect cheating. 2011.
[5] A. Calı̀, G. Gottlob, and A. Pieris. Advanced processing for
ontological queries. PVLDB, 2010.
[6] F. Chiang and R. J. Miller. Discovering data quality rules.
PVLDB, 2008.
[7] F. Chiang and R. J. Miller. A unified model for data and
constraint repair. In ICDE, 2011.
[8] X. Chu, I. F. Ilyas, and P. Papotti. Discovering denial
constraints. PVLDB, 2013.
[9] X. Chu, I. F. Ilyas, and P. Papotti. Holistic data cleaning:
Putting violations into context. In ICDE, 2013.
[10] K. W. Church and P. Hanks. Word association norms,
mutual information, and lexicography. Comput. Linguist.,
16(1):22–29, Mar. 1990.
[11] G. Cong, W. Fan, F. Geerts, X. Jia, and S. Ma. Improving
data quality: Consistency and accuracy. In VLDB, 2007.
[12] M. Dallachiesa, A. Ebaid, A. Eldawy, A. K. Elmagarmid,
I. F. Ilyas, M. Ouzzani, and N. Tang. NADEEF: a
commodity data cleaning system. In SIGMOD, 2013.
[13] D. Deng, Y. Jiang, G. Li, J. Li, and C. Yu. Scalable column
concept determination for web tables using large knowledge
bases. PVLDB, 2013.
[14] O. Deshpande, D. S. Lamba, M. Tourn, S. Das,
S. Subramaniam, A. Rajaraman, V. Harinarayan, and
A. Doan. Building, maintaining, and using knowledge bases:
a report from the trenches. In SIGMOD Conference, 2013.
[15] X. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao,
K. Murphy, T. Strohmann, S. Sun, and W. Zhang.
Knowledge vault: A web-scale approach to probabilistic
knowledge fusion. In SIGKDD, 2014.
[16] X. L. Dong, E. Gabrilovich, G. Heitz, W. Horn, K. Murphy,
S. Sun, and W. Zhang. From data fusion to knowledge
fusion. PVLDB, 2014.
[17] W. Fan. Dependencies revisited for improving data quality.
In PODS, 2008.
[18] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Interaction
between record matching and data repairing. In SIGMOD,
2011.
[19] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Towards certain
fixes with editing rules and master data. VLDB J., 21(2),
2012.
[20] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. The
LLUNATIC Data-Cleaning Framework. PVLDB, 2013.
[21] J. Hoffart, F. M. Suchanek, K. Berberich, and G. Weikum.
YAGO2: A spatially and temporally enhanced knowledge
base from wikipedia. Artif. Intell., 194, 2013.
[22] I. F. Ilyas, W. G. Aref, and A. K. Elmagarmid. Supporting
top-k join queries in relational databases. VLDB J., 13(3),
2004.
[23] I. F. Ilyas, V. Markl, P. J. Haas, P. Brown, and
A. Aboulnaga. Cords: Automatic discovery of correlations
and soft functional dependencies. In SIGMOD, 2004.
[24] M. Interlandi and N. Tang. Proof positive and negative
data cleaning. In ICDE, 2015.
[25] H.-P. Kriegel, P. Kröger, E. Schubert, and A. Zimek.
Interpreting and unifying outlier scores. In SDM, pages
13–24, 2011.
[26] R. Lange and X. Lange. Quality control in crowdsourcing:
An objective measurement approach to identifying and
correcting rater effects in the social evaluation of products
and services. In AAAI, 2012.

APPENDIX
A.

PROOF OF THEOREM 1

The expected uncertainty reduction is computed as the
difference between the current entropy and the expected one.
E(∆H(ϕ))(v) = −HP (ϕ) +

P

a

P r(v = a)Hϕi ∈Pv=a (ϕi )

The uncertainty of the conditional distribution of patterns
given v = a, Hϕi ∈Pv=a (ϕi ) can be computed as follows:
Hϕi ∈Pv=a (ϕi )
=

1259

P

ϕi ∈Pv=a

P

P r(ϕi )
P r(ϕi )

ϕi ∈Pv=a

log2

P

P r(ϕi )
P r(ϕi )

ϕi ∈Pv=a

KATARA

INPUT
Pattern Discovery

Pattern Validation

Algorithm: rank-join
Return: candidate
table patterns

Table T

OUTPUT
Data Annotation

KB validated

Algorithm: entropy
based scheduling

Algorithm: Inverted list
based approach

Crowd validated

Return: a table pattern

Return: annotated data,
new facts, top-k repairs

Possible repairs

Table T'

Trusted KB K

Enriched KB K'

Figure 9: Workflow of Katara
B(capital)
hasCurrency

A(country)

A(club)

5

hasCapital

D(director)

5

isLocatedIn

/ C(currency)

B(positions)

E(stadium)

A(university)

hasOfficalLanguage

isLocatedIn

)
(b) Soccer(Yago)
A(SoccerClub) o

B(city)

O

A(country)

/ C(currency)

D(SoccerManager)

*

C(SoccerPlayer)

officalLanguage

(



E(Stadium)

A(University)

city

7

D
(e) Soccer(DBPedia)

(d) Person(DBPedia)

w

subdivisionName


/ C(city)

position

B

D(language)

B(AdministrativeRegion)

managerclubs
ground

C(city)

(c) University(Yago)

state

clubs
currency

)

C(player)

(a) Person(Yago)

6

O

isLocatedIn

D(language)

capital

B(states)

areaCode



postalCode

E

(f) University(DBPedia)

Figure 10: Validated table patterns
P
However, ϕi ∈Pv=a P r(ϕi ) is exactly P r(v = a). Thus,
we can replace for Hϕi ∈Pv=a (ϕi ).

=−

E(∆H(ϕ))(v)

= H(v)

= −HP (ϕ) +
= −HP (ϕ)+

P P
a

a

P r(v = a)

P P

= −HP (ϕ) +
−

P

a

ϕi ∈Pv=aP r(ϕi )(log2

P P

Pv=a

a

P r(ϕi )
ϕi ∈Pv=a P r(v=a)

P

ϕi ∈Pv=a

log2

P r(ϕi )
P r(v=a)

P r(ϕi )−log2 P r(v = a))

The first double summation is exactly the summation over
all the current patterns, ordering them by the value of v.
Thus, we have the following:

B.

TOP-K PATTERNS ANALYSIS

PATTERN VALIDATION

Figure 12 shows the quality of the validated pattern, varying the number of questions per variable q, on WikiTables
and RelationalTables. Notice that RelationalTables only require one question per variable to achieve 1.0 precision and
recall. This is because RelationalTables are less ambiguous
compared with WikiTables and WebTables. Experts can correctly validate every variable with only one question.

P
= −HP (ϕ) +
P r(ϕ) log2 P r(ϕ)
P
P
− a log2 P r(v = a) ϕi ∈Pv=a P r(ϕi )
a

P r(v = a) log2 P r(v = a)

The above result proves Theorem 1.

C.

E(∆H(ϕ))(v)

P

a

Figure 11 shows the F-measure of the top-k patterns varying k on WikiTables and RelationalTables. It tells us that
RankJoin converges much quicker than other methods on
Yago, while all methods converge quickly on DBPedia due to
its small number of types.

P r(ϕi ) log2 P r(ϕi )

P r(ϕi ) log2 P r(v = a)

= −HP (ϕ) + HP (ϕ) −

P

log2 P r(v = a) × P r(v = a)

1260

1
0.95

0.8
0.75
0.7
Support
MaxLike
PGM
RankJoin

0.65
0.6
0.55
0

2

4

6

0.8
0.75
0.7

Support
MaxLike
PGM
RankJoin

0.65
0.6

8 10 12 14 16 18 20

0

2

4

6

0.8
0.7
0.6

Support
MaxLike
PGM
RankJoin

0.5
0.4

8 10 12 14 16 18 20

k

0

2

4

6

0.9
0.85
0.8
Support
MaxLike
PGM
RankJoin

0.75
0.7
0.65

8 10 12 14 16 18 20

k

(a) Yago (WikiTables)

F-Measure at k

1
0.9

F-Measure at k

0.9
0.85

F-Measure at k

F-Measure at k

0.9
0.85

0

2

4

6

8 10 12 14 16 18 20

k

(b) DBPedia (WikiTables)

k

(c) Yago (RelationalTables)

(d) DBPedia (RelationalTables)

0.98
0.97
0.96
0.95
0.94
0.93
0.92
0.91
0.9
0.89
0.88
0.87

1

1.01

1.01

1.005

1.005

0.999
0.998
0.996

P/R

P/R

0.997

P/R

P/R

Figure 11: Top-k F-measure

1

0.994
Precision
Recall
1

2

3

4

5

6

0.995
Precision
Recall

0.993
0.992
7

8

9

1

2

3

q

(a) Yago (WikiTables)

4

5

6

0.995
Precision
Recall

0.99
7

8

9

1

q

(b) DBPedia (WikiTables)

2

3

4

5

6

7

8

9

(c) Yago (RelationalTables)

DATA REPAIRING

We use the following FDs for algorithm EQ, referring to
Fig. 10.
(1) Person, we used A → B, C, D.
(2) Soccer, we used C → A, B, A → E, and D → A.
(3) University, we used A → B, C and C → B.

1261

Precision
Recall

0.99

q

Figure 12: Pattern validation P/R

D.

1

0.995

1

2

3

4

5

6

7

8

9

q

(d) DBPedia (RelationalTables)

Data Exchange with Data-Metadata Translations
Mauricio A. Hernández

∗

Paolo Papotti

Wang-Chiew Tan

IBM Almaden Research Center

Università Roma Tre

mauricio@almaden.ibm.com

papotti@dia.uniroma3.it

ABSTRACT

‡

UC Santa Cruz

wctan@cs.ucsc.edu

of source-to-target tuple generating dependencies (s-t tgds) [6] or
(Global-and-Local-As-View) GLAV mappings [7, 12]. They have
also been extended to specify the relation of pairs of instances of
nested schemas in [8, 18].
Data exchange systems [9, 13, 14, 21, 22] have been developed
to (semi-) automatically generate the mappings and the transformation code in the desired language by mapping schema elements
in a visual interface. These frameworks alleviate the need to fully
understand the underlying transformation language (e.g. XQuery)
and language-specific visual editor (e.g. XQuery editor). Furthermore, some of these systems allow the same visual specification of
mapping schema elements to be used to generate a skeleton of the
transformation code in diverse languages (e.g., Java, XSLT).
Past research on data exchange, as well as commercial data exchange systems , have largely dealt with the case where the schemas
are given a priori and transformations can only migrate data from
the first instance to an instance of the second schema. In particular, data-metadata translations are not supported by these systems. Data-metadata translations are transformations that convert
data/metadata in the source instance or schema to data/metadata in
the target instance or schema. Such capabilities are needed in many
genuine data exchange scenarios that we have encountered, as well
as in data visualization tools, where data are reorganized in different ways in order to expose patterns or trends that would be easier
for subsequent data analysis.
A simple example that is commonly used in the relational setting
to motivate and illustrate data-metadata translations is to “flip” the
StockTicker(Time, Company, Price) table so that company names
appear as column names of the resulting table [15]. This is akin to
the pivot operation [23] used in spreadsheets such as Excel. After
a pivot on the company name and a sort on the time column, it
becomes easier to see the variation of a company’s stock prices and
also compare against other companies’ performance throughout the
day (see the table on the right below).

Data exchange is the process of converting an instance of one schema
into an instance of a different schema according to a given specification. Recent data exchange systems have largely dealt with the
case where the schemas are given a priori and transformations can
only migrate data from the first schema to an instance of the second schema. In particular, the ability to perform data-metadata
translations, transformation in which data is converted into metadata or metadata is converted into data, is largely ignored. This
paper provides a systematic study of the data exchange problem
with data-metadata translation capabilities. We describe the problem, our solution, implementation and experiments. Our solution is
a principled and systematic extension of the existing data exchange
framework; all the way from the constructs required in the visual
interface to specify data-metadata correspondences, which naturally extend the traditional value correspondences, to constructs required for the mapping language to specify data-metadata translations, and algorithms required for generating mappings and queries
that perform the exchange.

1.

†‡

INTRODUCTION

Data exchange is the process of converting an instance of one
schema, called the source schema, into an instance of a different
schema, called the target schema, according to a given specification. This is an old problem that has renewed interests in recent
years. Many data exchange related research problems were investigated in the context where the relation between source and target
instances is described in a high-level declarative formalism called
schema mappings (or mappings) [10, 16]. A language for mappings of relational schemas that is widely used in data exchange, as
well as data integration and peer data management systems, is that
∗Work partly funded by U.S. Air Force Office for Scientific Research under contract FA9550-07-1-0223.
†Work done while visiting UC Santa Cruz, partly funded by an
IBM Faculty Award.
‡Work partly funded by NSF CAREER Award IIS-0347065 and
NSF grant IIS-0430994.

Time
0900
0900
0905

Symbol
MSFT
IBM
MSFT

Price
27.20
120.00
27.30

Time
0900
0900
0905

MSFT
27.20
27.30

IBM
120.00
-

Observe that the second schema cannot be determined a priori
since it depends on the first instance and the defined transformation.
Such schemas are called dynamic output schemas in [11]. Conceivably, one might also wish to unpivot the right table to obtain
the left one. Although operations for data-metadata translations
have been investigated extensively in the relational setting (see, for
instance, [24] for a comprehensive overview of related work), this
subject is relatively unexplored for data exchange systems in which
source or target instances are no longer “flat” relations and the relationship between the schemas is specified with mappings.

Permission to make digital or hard copies of portions of this work for
personal or classroom use is granted without fee provided that copies
are not made
or distributed
profit
commercial
and
Permission
to copy
without feefor
all or
part or
of this
material isadvantage
granted provided
thatthe
copies
thismade
notice
the fullforcitation
on the firstadvantage,
page.
that
copiesbear
are not
or and
distributed
direct commercial
Copyright
for components
of the
thistitle
work
owned
by others
VLDB
the
VLDB copyright
notice and
of the
publication
andthan
its date
appear,
Endowment
honored.
and
notice is must
givenbethat
copying is by permission of the Very Large Data
Abstracting
with credit
is permitted.
otherwise,totopost
republish,
Base
Endowment.
To copy
otherwise,Toorcopy
to republish,
on servers
totopost
on servers
or torequires
redistribute
to listsspecial
requires
prior specific
or
redistribute
to lists,
a fee and/or
permission
from the
permission
and/or a fee. Request permission to republish from:
publisher,
ACM.
VLDB
‘08, August
24-30,
2008, Inc.
Auckland,
Publications
Dept.,
ACM,
Fax New
+1 Zealand
(212) 869-0481 or
Copyright
2008 VLDB Endowment, ACM 000-0-00000-000-0/00/00.
permissions@acm.org.
PVLDB '08, August 23-28, 2008, Auckland, New Zealand
Copyright 2008 VLDB Endowment, ACM 978-1-60558-305-1/08/08

260

Target: Rcd
Source: Rcd
CountrySales: SetOf Rcd
Sales: SetOf Rcd
country
country
Sales: SetOf Rcd
region
style
style
shipdate
shipdate
units
units
id
price

(a)

for
$s in Source.Sales
exists $c in Target.CountrySales, $cs in $c.Sales
where $c.country = $s.country and $cs.style = $s.style and
$cs.shipdate = $s.shipdate and $cs.units = $s.units and
$c.Sales = SK[$s.country]

(b)

Extending the data exchange framework with data-metadata translation capabilities for hierarchical or XML instances is a highly
non-trivial task. To understand why, we first need to explain the
data exchange framework of [18], which essentially consists of
three components:
• A visual interface where value correspondences, i.e., the relation between elements of the source and target schema can
be manually specified or (semi-)automatically derived with a
schema matcher. Value correspondences are depicted as lines
between schema element in the visual interface and it provides
an intuitive description of the underlying mappings.

“For every Sales tuple, map it to a CountrySales tuple where Sales are
grouped by country in that tuple.”

• A mapping generation algorithm that interprets the schemas
and values correspondences into mappings.

CountrySales
country region style shipdate units price country Sales
USA style shipdate
USA East
Tee 12/07 11 1200
Tee 12/07
USA East
Elec. 12/07 12 3600
Elec. 12/07
USA West Tee 01/08 10 1600
Tee 01/08
UK
West Tee 02/08 12 2000
country Sales
UK style shipdate
Tee 02/08
Sales

• A query generation algorithm that generates a query in some
language (e.g., XQuery) from the mappings that are generated
in the previous step. The generated query implements the specification according to the mappings and is used to derive the
target instance from a given source instance. (Note that the
framework of [14] is similar and essentially consists of only the
second and third components.)
Adding data-metadata translation capabilities to the existing data
exchange framework requires a careful and systematic extension to
all three components described above. The extension must capture
traditional data exchange as a special case. It is worth pointing out
that the visual interface component described above is not peculiar
to [18] alone. Relationship-based mapping systems [20] consist of
a visual interface in which value correspondences are used to intuitively describe the transformation between a source and target
schema and in addition to [18], commercial mapping systems such
as [13, 21, 22] all fall under this category. Hence, our proposed extensions to the visual interface could also be applied to these mapping systems as well. The difference between mapping systems
such as [13, 21, 22] and [18] is, essentially, the second component
of the data exchange framework described above; these commercial systems do not generate mappings, they generate queries (or
executable code) directly.
Our solution is a principled extension to all the three components, from constructs required in the visual interface to depict
data-metadata relationships, new constructs for mappings to describe data-metadata translations, to a redesign of mapping and
query generation algorithms. A novelty of our work is the ability
to handle data-to-metadata translations with nested dynamic output schemas (ndos). This is a major extension of dynamic output
schemas where, intuitively, multiple parts of the output (nested)
schema may be fully-defined only at runtime and is dependent on
the source instance. Ndos schemas capture relational dynamic output schemas as a special case where there is only one level of nesting and only the number of output columns are dynamic. It also
captures relational output schemas as a special case where there is
only one level of nesting and none of the output columns are dynamic.
In what follows, we describe a series of data-metadata translation examples to exemplify our contributions, and introduce background and related work. We detail our mapping and query generation algorithms in Sections 4 and 5, respectively, and describe our
experimental results in Section 6.

2.

(c)

units id

11 ID1
12 ID2
10 ID3

units id

12

ID4

Figure 1: Data-to-Data Exchange

2.1

Data-to-Data Translation (Data Exchange)

Data-to-data translation corresponds to the traditional data exchange where the goal is to materialize a target instance according
to the specified mappings when given a source instance. In data-todata translation, the source and target schemas are given a priori.
Figure 1 shows a typical data-to-data translation scenario. Here,
users have mapped the source-side schema entities into some target side entities, which are depicted as lines in the visual interface.
The lines are called value correspondences. The schemas are represented using the Nested Relational (NR) Model of [18], where
a relation is modeled as a set of records and relations may be arbitrarily nested. In the source schema, Sales is a set of records where
each record has six atomic elements: country, region, style, shipdate, units, and price. The target is a slight reorganization of the
source. CountrySales is a set of records, where each record has two
labels, country and Sales. Country is associated to an atomic type
(atomic types are not shown in the figure), whereas Sales is a set of
records. The intention is to group Sales records as nested sets by
country, regardless of region.
Formally, a NR schema is a set of labels {R1 ,...,Rk }, called
roots, where each root is associated with a type τ , defined by the
following grammar: τ ::= String | Int | SetOf τ | Rcd[l1 : τ1 , ..., ln :
τn ] | Choice[l1 : τ1 , ..., ln : τn ]. The types String and Int are
atomic types (not shown in Figure 1)1 . Rcd and Choice are complex types. A value of type Rcd[l1 : τ1 , ..., ln : τn ] is a set of
label-value pairs [l1 : a1 , ..., ln : an ], where a1 , ..., an are of types
τ1 , ..., τn , respectively. A value of type Choice[l1 : τ1 , ..., ln : τn ]
is a single label-value pair [lk : ak ], where ak is of type τk and
1 ≤ k ≤ n. The labels l1 , ..., ln are pairwise distinct. The set type
SetOf τ (where τ is a complex type) is used to model repeatable
elements modulo order.
In [18], mappings are generated from the visual specification
with a mapping generation algorithm. For example, the visual specification of Figure 1(a) will be interpreted into the mapping expression that is written in a query-like notation shown on Figure 1(b).

DATA-METADATA TRANSLATIONS

In this section, we give examples of data/metadata to data/metadata
translations to exemplify our contributions. We start by describing
some background through an example of data-to-data translation.

261

1
We use only String and Int as explicit examples of atomic types.
Our implementation supports more than String and Int.

Source: Rcd
SalesByCountries: SetOf Rcd
month
USA
<<countries>>
UK
label
Italy
value

Target: Rcd
Sales: SetOf Rcd
month
country
units

(a)

the number of units sold. Hence, the mapping has to specify that
the element names, “USA”, “UK” and “Italy”, in the source schema
are to be translated into data in the target instance.
Placeholders in the source schema Our visual interface allows the
specification of metadata-to-data transformations by first selecting
the set of element names (i.e., metadata) of interest in the source.
In Figure 2, “USA”, “UK” and “Italy” are selected and grouped together under the placeholder hhcountriesii, shown in the middle of
the visual specification in Figure 2. The placeholder hhcountriesii
exposes two attributes, label and value, which are shown underneath hhcountriesii. Intuitively, the contents of label correspond to
an element of the set {“USA”, “UK” and “Italy”}, while the contents of value correspond to value of the corresponding label (e.g.,
the value of “USA”, “UK”, or “Italy” in a record of the set SalesByCountries). To specify metadata-to-data transformation, a value
correspondence is used to associate a label in the source schema
to an element in the target schema. In this case, the label under
hhcountriesii in the source schema is associated with country in the
target schema. Intuitively, this specifies that the element names
“USA”, “UK” and “Italy” will become values of the country element in the target instance. It is worth remarking that label under
hhcountriesii essentially turns metadata into data, thus allowing traditional value correspondences to be used to specify metadata-todata translations. Another value correspondence, which associates
value to units, will migrate the sales of the corresponding countries to units in the target. A placeholder is an elegant extension to
the visual interface. Without placeholders, different types of lines
will need to be introduced on the visual interface to denote different types of intended translations. We believe placeholders provide
an intuitive descriptions of the intended translation with minimal
extensions to the visual interface without cluttering the visual interface with different types of lines. As we shall see in Section 2.3,
a similar idea is used to represent data-to-metadata translations.
The precise mapping that describes this transformation is shown
on Figure 2(b). The mapping states that for every combination of a
tuple, denoted by $s, in SalesByCountries and an element $c in the
set {“USA”, “UK”, “Italy”}, generate a tuple in Sales in the target
with the values as specified in the where clause of the mapping.
Observe that the record projection operation $s.($c) depends of the
value that $c is currently bound to. For example, if $c is bound
to the value “USA”, then $s.($c) has the same effect as writing
$s.USA. Such a construct for projecting records “dynamically” is
actually not needed for metadata-to-data translations. Indeed, the
same transformation could be achieved by writing the following
mapping:

(b)

$s in Source.SalesByCountries, $c in {“USA”, “UK”, “Italy”}
for
exists $t in Target.Sales
where $t.month = $s.month and $t.country = $c and $t.units = $s.($c)
“For every SalesByCountries tuple, map it to a Sales tuple where
Sales are listed by month and country names.”
SalesByCountries
month USA

Jan
Feb

UK Italy

120 223 89
83 168 56

Sales

month country units

Jan
Jan
Jan
Feb
Feb
Feb

USA
UK
Italy
USA
UK
Italy

120
223
89
83
168
56

(c)

Figure 2: Metadata-to-Data Exchange
The mapping language is essentially a for . . . where . . . exists
. . . where . . . clause. Intuitively, the for clause binds variables to
tuples in the source, and the first where clause describes the source
constraints that are to be satisfied by the source tuples declared in
the for clause (e.g., filter or join conditions)2 . The exists clause
describes the tuples that are expected in the target, and the second
where clause describes the target constraints that are to be satisfied
by the target tuples as well as the content of these target tuples.
The mapping in Figure 1 states that for every Sales record in
the source instance that binds to $s, a record is created in the
CountrySales relation together with a nested set Sales. For example, when $s binds to the first tuple in Sales, a record in CountrySales must exists whose country value is “USA” and Sales value
is a record (style=“Tee”, shipdate=“12/07”, units=“11”, id=ID1 ”).
Since the value of id label is not specified, a null “ID1 ” is created as
its value. Records in Target.Sales are grouped by country, which is
specified by the grouping expression “$c.Sales = SK[$s.country]”.
The term SK[$s.country] is essentially the identifier of the nested
set $c.Sales in the target. For the current record that is bound
to $s, the identifier of $c.Sales in the target is SK[USA]. When
$s binds to the second tuple in Source.Sales, an additional record
(style=“Elec.”, shipdate=“12/07”, units=“11”, id=“ID2 ”) with the
same Sales set identifier, SK[USA], must exist in the target. Given
a source instance shown on the bottom-left of Figure 1, a target
instance that satisfies the mapping is shown on the bottom-right.
Such mappings are called as basic mappings.
Although mappings describe what is expected of a target instance, they are not used to materialize a target instance in the data
exchange framework of [18]. Instead, a query is generated from
the mappings, and the generated query is used to perform the data
exchange.

2.2

for $s in Source.SalesByCountries
exists $t1 in Target.Sales, $t2 in Target.Sales, $t3 in Target.Sales
where $t1 .month = $s.month and $t2 .month = $s.month and
$t3 .month = $s.month and
$t1 .country = “USA” and $t2 .country = “UK” and
$t3 .country = “Italy” and
$t1 .units = $s.USA and $t2 .units = $s.UK and
$t3 .units = $s.Italy

The above mapping states that for every tuple $s in SalesByCountries, there exists three tuples $t1 , $t2 and $t3 in Sales, one
for each country “USA”, “UK” and “Italy”, with the appropriate
values for month, country and units.
Since our placeholders are used strictly to pivot metadata into
data values, we can only use them in the source schema during
metadata-to-data translations. Our current implementation allows
placeholders to be created for element names at the same level of
nesting and of the same type. For example, a placeholder could
be created for “USA”, “UK” and “Italy” because they belong to

Metadata-to-Data Translation

An example of metadata-to-data translation is shown in Figure 2.
This example is similar to that of unpivoting the second relation
into the first in the StockTicker example described in Section 1.
Like data-to-data translations, both the source and target schemas
are given a priori in metadata-to-data translations. The goal of the
exchange in Figure 2 is to tabulate, for every month and country,
2

The example in Figure 1(b) does not use the first where clause.

262

the same record and have the same atomic type, say Int. If “USA”
occurs in some other record or, if “USA” is a complex type while
“UK” and “Italy” are atomic types, then it is not possible to create a placeholder for these elements. Although it is conceivable to
allow placeholders for the latter case by generating different mappings according to types of elements in the placeholder, we do not
elaborate on that option here.
Just as in the relational metadata-to-data translations where SQL
does not need to be extended to handle metadata-to-data translations [25], the existing mapping language does not need to be extended to handle metadata-to-data translations as well. As a matter of fact, in Section 4 we will discuss how mapping expressions
containing our placeholders are re-written into the notation used in
Figure 2. In contrast, the situation is rather different with data-tometadata translations, which we shall describe in the next section.

whose value is $s.Price. It is worth noting that the term $t.($s.Symbol)
projects on the record $t dynamically. The attribute on which to
project the record $t is $s.Symbol which can only be determined
during the exchange. This is similar to the dynamic projection of
records that was described in Section 2.1. However, unlike the
example in Section 2.1, the ability to dynamically project records
is crucial for data-to-metadata translations. Since the attribute on
which to project the record $t is determined by the source instance,
the mapping cannot be rewritten into one that does not use such
dynamic constructs. The assertions described by the mapping produce, conceptually, the following data:

2.3

Since Stockquotes is a set of records and all records in the same
set must be homogeneous, we obtain the result that is shown in
the Section 1, where each record has three fields, time, MSFT
and IBM. Indeed, the above homogeneity constraint is captured
by the mapping c. This mapping states that all records in Target.Stockquotes must have the same set of labels.
Our mapping and query generation algorithms can also account
for key constraints. For example, if time is the key of Stockquotes,
then there will be an additional mapping that essentially enforce
that every pair of tuples with the same time value must have the
same MSFT and IBM values. The key constraint is enforced as a
post-processing step on the instance obtained in Section 1. Hence,
there will only be two tuples in the output, corresponding to (0900,
27.20, 120.00) and (0905, 27.30, -) after the post-processing. Note
that inclusion dependencies, such as key/foreign key constraints,
are automatically enforced prior to the post-processing step.
If the desired output is to have three records of possibly heterogeneous types as shown above, then one solution is to specify the
dynamic element in Stockquotes as a Choice type. We shall describe in Sections 4 and 5 how types in an ndos schema are used in
the mapping and query generation process to generate the correct
mapping specification and transformation.
A novelty of our work is that ndos schemas can contain many dynamic elements, which may be arbitrarily nested. This is a major
extension to dynamic output schemas of [11] for the relational case.
We illustrate this with the example in Figure 3. The source schema
is identical to that of Figure 1 and the target is a ndos schema.
It contains two dynamic elements (denoted as label1 , value1 and
label2 , value2 , respectively, in the figure), where one is nested
under the other. Target.ByShipdateCountry is a SetOf Choice
types. This means that every tuple in Target.ByShipdateCountry
is a choice between many different label-value pairs. The set of
label-value pairs is determined at runtime, where the labels in the
set are all the shipdates (e.g., 12/07, 01/08, and 02/08 according
to the source instance shown on the bottom-left of the same figure) and the value associated with each label is a record with a
dynamic element. The set of labels in each record is determined by
the countries in the source instance (e.g., USA, UK) and the value
associated with each label is a set of records of a fixed type (style,
units, price).
The visual specification is interpreted into the mapping shown
on Figure 3(b). The mapping is a constraint that states that for
every Source.Sales tuple $s, there must exists a tuple $t in the
set Target.ByShipdateCountry where a case (or choice) of this tuple has label $s.shipdate and the value is bound to the variable
$u. From the ndos schema, we know that $u is a record. The
term $u.($s.country) states that $u has an attribute $s.country and
from the ndos schema, we know that $u.($s.country) is a set of

Rcd[Time:0900, MSFT:27.20]
Rcd[Time:0900, IBM:120]
Rcd[Time:0905, MSFT:27.30]

Data-to-Metadata Translation

To illustrate data exchange with data-to-metadata translation, we
first revisit the example that was described in Section 1. The mapping below illustrates the exchange where the source schema is a
set of records with three attributes (time, symbol and price) and
the target is a set of records with a time attribute and a dynamic
element, shown as a label and value pair. Schemas with dynamic
elements are called nested dynamic output schemas (ndos).
Source: Rcd
Target: Rcd
StockTicker: SetOf Rcd
Stockquotes: SetOf Rcd
time
time
symbol
label
price
value

Nested Dynamic Output Schema (ndos) A ndos schema is similar to an NR schema except that it can contain dynamic elements.
Like NR schemas, a ndos schema is a set of labels {R1 ,...,Rk },
called roots, where each root is associated with a type τ , defined
by the following grammar: τ ::= String | Int | SetOf τ | Rcd[l1 :
τ1 , ..., lm : τm , $d : τ ] | Choice[l1 : τ1 , ..., lm : τm , $d : τ ].
Observe that the grammar is very similar to that defined for a NR
schema except that Rcd and Choice types can each contain a dynamic element, denoted by $d. A dynamic element has type τ
which may contain dynamic elements within. Intuitively, a dynamic element may be instantiated to one or more element names at
runtime (i.e., during the exchange process). If $d is instantiated to
values p1 , ..., pn at runtime, then all values of p1 , ..., pn must have
the same type τ . Ndos schemas can only be defined in the target.
Note that they are different from source schemas with placeholders.
Dynamic elements are not placeholders since they do not represent
a set of element names that exists in the schema but rather, they are
intended to represent element names that are only determined at
runtime. Our implementation supports the specification of multiple
dynamic elements within a record or choice type although we do
not elaborate on this possibility here.
The visual specification of the figure above is interpreted into the
following mappings by our mapping generation algorithm:
m : for $s in Source.StockTicker
exists $t in Target.Stockquotes
where $t.time = $s.time and $t.($s.Symbol) = $s.Price
c : for $t1 in Target.Stockquotes, $t2 in Target.Stockquotes, l ∈ dom($t1 )
exists $l0 in dom($t2 )
where l = l0

Mapping m asserts that for every record $s in StockTicker, there
must be a record $t in Stockquotes whose time value is the same as
the time value of $s and there is an attribute in $t named $s.Symbol

263

Source: Rcd
Sales: SetOf Rcd
country
region
style
shipdate
units
price

Target: Rcd
ByShipdateCountry: SetOf Choice
label1 <<dates>>
value1: Rcd
label2 <<countries>>
value2: SetOf Rcd
style
units
price

ByShipDateCountry
12/07
USA

(a)

style units price

Tee 11
Elec. 12
USA

style units price

01/08
USA

(b)
$s in Source.Sales
for
exists $t in Target.ByShipdateCountry, $u in case $t of $s.shipdate,
$v in $u.($s.country)
where $v.style = $s.style and $v.units = $s.units and $v.price = $s.price and
$u.($s.country) = SK[$s.shipdate,$s.country]

style units price

Tee
02/08
UK

USA
USA
USA
UK

East
East
West
West

Tee
Elec.
Tee
Tee

ByShipDateCountry
12/07
USA
style units price

Tee 11
Elec. 12
01/08
USA

1200
3600

Tee
02/08
UK

1600

style units price

10

12/07
12/07
01/08
02/08

11
12
10
12

1200
3600
1600
2000

Tee

(c)

12 2000

12/07: Rcd[UK: SetOf{ Rcd[style:Elec., units:15, price:3390] } ]

Target: Rcd
ByShipDateCountry: SetOf Choice
(12/07: Rcd
USA: SetOf Rcd
style, units, price) |
(01/08: Rcd
USA: SetOf Rcd
style, units, price) |
(02/08: Rcd
UK: SetOf Rcd
style, units, price)

The value of “12/07” has type Rcd[UK: SetOf Rcd[style, units,
price]], which is different from the type of the existing label “12/07”
(whose attribute in the record is “USA” instead of “UK”). Since
there can only be one choice with label “12/07”, we obtain the resulting schema and target instance of Figure 4 (right and left, respectively). The resulting target schema has three choices under
ByShipDateCountry (“12/07”, “01/08”, “02/08”), each with a different record type.

2.3.1

12 2000

Remarks

Several remarks are in order now on the semantics of data-tometadata translations.
Data-to-Metadata Translation. As discussed earlier, the target
schema is a ndos schema which may not be fully-defined at compiletime. This is a major departure from the data-to-data and metadatato-data exchange framework where the source and target schemas
are given a priori as part of the input to the exchange problem. Instead, in data-to-metadata translation, a source schema and a ndos
(target) schema are part of the input to the exchange system. Just
like the source schema, the ndos schema is provided by the user.
Apart from materializing a target instance, the data-to-metadata exchange process also produces a target schema in the NR model that
the target instance conforms to. Formally, given a source schema
S, a ndos target schema Γ, a mapping Σ between S and Γ, and
a source instance I of S, the data-to-metadata exchange problem
is to materialize a pair (J, T) so that T conforms to Γ, J is an
instance of T, and (I, J) satisfies Σ. Intuitively, a NR schema T
conforms to a ndos schema Γ if T is a possible “expansion” of Γ
as a result of replacing the dynamic elements during the exchange.
We give the full definition in Appendix A.
Solutions. Observe that the target schema T and the target instance J that consists of the three tuples as shown in the Section 1,
together with the tuple (1111, 35.99, 10.88), also form a solution to
the StockTicker-Stockquotes data-to-metadata exchange problem.
As a matter of fact, the pair (J 0 , T0 ), where T0 is identical to T
except that there is an additional attribute, say CISCO, and J 0 is
identical to J except that it has an additional column for all four
tuples in J with the value “100”, is also a solution. In the presence
of choice types with a dynamic element, solutions can also vary in
the number of choices. For example, one could add an additional
choice with label “03/08” and appropriate type to the target output
schema of Figure 3(c). This new target schema together with the
target instance shown in Figure 3 is also a solution to the exchange
problem shown in the same figure. The semantics behind our construction of a solution to the data-to-metadata exchange problem is

Figure 3: Data-to-Metadata Exchange

(style, units, price) records. The mapping also asserts that there exists a tuple $v in the set of records determined by $u.($s.country)
such that the style, units and price of $v correspond, respectively,
to the style, units and price of $s. The case . . . of construct for
mappings was introduced in [26] to select one choice label among
those available in a choice type. In our example, the term after of is $s.shipdate, whose value can only be determined at runtime. In contrast, only label constants are allowed on the righthand-side of an of clause in [26]. Finally, the term $u.($s.country)
= SK[$s.shipdate,$s.country] states that every set of (style, units,
price) records is grouped by shipdate and country.
Given these semantics, the tuples in Source.Sales will, conceptually, generate the following tuples (we show the types explicitly):
12/07:
12/07:
01/08:
02/08:

1600

Figure 4: Target Instance and Schema for Data-to-Metadata
example

style units price

Tee

10

style units price

“For every Sales tuple, map it to a tuple whose only label is shipdate and value
is a record that tabulates the set of sales by country.”
Sales
country region style shipdate units price

1200
3600

Target: Rcd
ByShipDateCountry: SetOf Choice
UK
(12/07: Rcd
style units price
USA: SetOf Rcd
style, units, price
UK: SetOf Rcd
UK
style, units, price) |
(01/08: Rcd
style units price
Elec. 15 3390
USA: SetOf Rcd
style, units, price) |
(02/08: Rcd
UK: SetOf Rcd
style, units, price)

Rcd[USA: SetOf{ Rcd[style:Tee, units:11, price:1200] } ]
Rcd[USA: SetOf{ Rcd[style:Elec., units:12, price:3600] } ]
Rcd[USA: SetOf{ Rcd[style:Tee, units:10, price:1600] } ]
Rcd[ UK: SetOf{ Rcd[style:Tee, units:12, price:2000] } ]

Since the sets of (style, units, price) records are grouped by shipdate and country, the set of records underneath 12/07 and USA are
identical and contains both records (Tee, 11, 1200) and (Elec., 12,
3600). The resulting instance and schema is shown in Figure 3(c).
As illustrated by the StockTicker example, the arity of record
types with dynamic elements is determined by the source instance.
As shown with this example, the number of choices in a choice type
with a dynamic element is also determined by the source instance.
To see a combination of record and choice “dynamism” at work,
suppose there is an additional Sale tuple (UK, West, Elec., 12/07,
15, 3390) in the source instance. Then, the following conceptual
tuple is asserted by the mapping:

264

based on an analysis of the assertions given by the mappings and input schemas, much like the chase procedure used in [8]. We believe
that query generates the most natural solution amongst all possible
solutions. A formal justification of this semantics is an interesting
problem on its own and part of our future work.

value correspondences that connect elements of the two schemas.
This problem is first explored in Clio [18] for the case when Γ is an
NR schema T and no placeholders are allowed in the source or target schema. Here, we extend the mapping generation algorithm of
Clio to generate MAD mappings that support data-metadata translations.
The method by which data-metadata translations are specified
in our visual interface is similar to Clio’s. A source and a target
schema are loaded into the visual interface and are rendered as two
trees of elements and attributes, and are shown side-by-side on the
screen. Users enter value correspondences by drawing lines between schema elements. After this, the value correspondences can
be refined with transformation functions that define how source values are to be converted to target values. As value correspondences
are entered, the mapping generation algorithm of Clio incrementally creates the mapping expressions that capture the transformation semantics implied by the visual specification.
There are, however, significant differences between Clio’s visual
interface and our visual interface. First, users can create placeholders in the source and target schema. (e.g., see Figure 2(a)). Second,
users can load ndos schemas on the target side of our visual interface and further edit it. Third, users can add value correspondences
that connect placeholders or schema elements in the source schema
with placeholders, dynamic elements, or schema elements in the
target schema.

We detail an interesting metadata-to-metadata translation example in Appendix B.

3.

MAD MAPPINGS

In Section 2, we have informally described the constructs that
are needed for mappings that specify data-metadata translations.
We call such mappings, MAD mappings (short for MetadatA-Data
mappings). The precise syntax of MAD mappings (MM) is described next.
for
where
exists
where

$x1 in g1 , . . . , $xn in gn
ρ($x1 , . . . , $xn )
$y1 in h1 , . . . , $ym in hm
υ($x1 , . . . , $xn , $y1 , . . . $ym ) and MM1 and . . . and MMk

Each gi in the for clause is an expression that either has a SetOf
τ type or a τ type under the label l from the Choice [..., l : τ , ...].
In the former case, the variable $xi will bind to an element in the
set while in the latter case, $xi will bind to the value of the choice
under label l. More precisely, gi is an expression of the form:

4.1

E ::= S | $x | E.L | case E of L | hhdii | {V1 , . . . , Vz } | dom($x)
L ::= l | (E)

where $x is a variable, S is a schema root (e.g., Source in the source
schema of Figure 1(a)) and E.L represents a projection of record
E on label L. The case E of L expression represents the selection of label L under the choice type E. The label L is either a
simple label or an expression. The latter case allows one to model
dynamic projections or dynamic elements under Choice types (e.g.,
see Figure 3(b)). The expression hhdii is a placeholder as described
in Section 2.2. As we shall discuss in the next section, placeholders can always be rewritten as a set of literal values {V1 , . . . , Vz }.
However, we have introduced placeholders in MAD mappings in
order to directly model the visual specification of grouping multiple schema elements (e.g., see hhcountriesii in Figure 2(a)) in our
mapping generation algorithm. The expression dom($x) denotes
the set of labels in the domain of a record $x. Naturally, a variable
$x that is used in an expression gi needs to be declared prior to
its use, i.e., among x1 , ..., xi−1 or in the for clause of some outer
mapping, in order for the mapping to be well-formed.
The expressions ρ($x1 , . . ., $xn ) and υ($x1 , . . ., $xn , $y1 , . . .
$ym ) are boolean expressions over the variables $x1 , ..., $xn and
$x1 , ..., $xn , $y1 , ..., $ym respectively. As illustrated in Section
2, the expression in υ can also includes grouping conditions. The
hi expressions in the exists clause are similar to gi s except that a
hhdii expression in hi represents a dynamic element, and not placeholders. Finally, MAD mappings can be nested. Just like nested
mappings in [8], nested MAD mappings are not arbitrarily nested.
The for clause of a MAD mapping can only extend expressions
bound to variables defined in the for clause of its parent mapping.
Similarly, the exists clause can only extend expressions bound to
variables defined in the exists clause of an ancestor mapping. Note
that MAD mappings captures nested mappings of [8] as a special
case.

4.

Generation of Basic Mappings

A general mapping generation algorithm that produces basic mappings was first described in [18] and subsequently refined in [8] to
produce mappings that can be nested within another. In what follows, we describe briefly the basic mapping generation algorithm
of [18]. Step 1. Tableaux: The generation of basic mappings
starts by compiling the source and target schemas into a set of
source and target tableaux. Let X = hx1 , . . . , xn i be a sequence
of variables over expressions g1 , . . . , gn of set or choice type. A
tableaux is an expression of the form
T ::= {$x1 ∈ g1 , . . . , $xn ∈ gn ; E}
where E is a (possibly empty) conjunction of equalities over the values bounded to the variables in X. Informally, a a tableau capture a
relationship or “concept” represented in the schema. Obvious relationship such as all atomic attributes under a SetOf Rcd or SetOf
Choice type, form “basic” tableaux. Basic tableaux are enhanced
by chasing either the constraints (e.g., referential constraints) that
exist in the schema or the structural constraints in the schema (e.g.,
parent-child relationship).
For example, we can derive two basic tableaux from the target
schema of Figure 1(a): {$x1 ∈ CountrySales} and {$x1 ∈ CountrySales.Sales}. Since CountrySales.Sales is nested under CountrySales, we obtain two tableaux after chasing: {$x1 ∈ CountrySales} and {$x1 ∈ CountrySales, $x2 ∈ $x1 .Sales}. As another
example, suppose we have a relational schema that contains two
tables, Department and Employee, and a referential constraint from
Employee into Department. In this case, there are two trivial tableaux,
{$x1 ∈ Department} and {$x1 ∈ Employee}. After chasing over
the constraint, the resulting tableaux are {$x1 ∈ Department} and
{$x1 ∈ Department, $x2 ∈ Employee; $x1 .did=$x2 .did}. Observe that the Employee tableau is not in the final list because there
cannot be Employee tuples without a related Department tuple, according to the referential constraint.
The output of the tableaux generation step is thus a set of source
tableaux {s1 , ..., sn } and a set of target tableaux {t1 , ..., tm }.
Step 2. Skeletons: Next, a n × m matrix of skeletons is con-

MAD MAPPING GENERATION

In this section, we describe how MAD mappings are generated
when given a source schema S, a target ndos schema Γ, and a set of

265

values are a choice type, we add “$x0 ∈ case $x of ($l)”.) Otherwise, if the values under labels in hhDii is a non-repeating type (e.g.,
record or atomic), we add an assignment: “$x0 := $x.($l)”. In
other words, x0 is assigned the value (record or atomic value) under
the current metadata label $l. As an example, the source schema
of Figure 2(a) will be compiled into a single source tableau {$x0 ∈
Source.SalesByCountries, $x1 ∈ hhcountriesii; $x2 := $x0 .($x1 ) }
Step 2. Skeletons: The generation of skeletons proceeds in the
same manner as described in the previous section. A skeleton of a
potential mapping is created for every possible pair of source and
target tableau.
Step 3. Creating MAD Mappings: At this stage, the value correspondences need to be matched against the tableaux in order to factor them into the appropriate skeletons. To explain how we match,
consider the first two value correspondences in Figure 1(a), which
are represented internally by a pair of sequence of labels.

structed for the set of source tableaux {s1 , ..., sn } and the set of
target tableaux {t1 , ..., tm }. Conceptually, each entry (i, j) in the
matrix is a skeleton of a potential mapping. This means that every
entry provides some information towards the creation of a basic
mapping. Specifically, the skeleton at (i, j) represents the mapping
between source tuples of the form of the tableau si and target tuples
of the form of the tableau tj . Once the skeletons are created, the
mapping system is ready to accept value correspondences.
Observe that both the creation of tableaux and skeletons occurs
during the loading of schemas. As long as the schemas do not
change after being loaded, there is not need to recompute its tableaux
or update the skeleton matrix.
Step 3. Creating Basic Mappings: For each value correspondence that is given by a user (or discovered using a schema matching method [19]), the source side of the correspondence is matched
against one or more source tableaux while the target side is matched
to one or more target tableaux. For every pair of matched source
and target tableaux, we add the value correspondence to the skeleton and mark the skeleton as “active”.
The next step involves an analysis of possible relationships (subsumed or implied by) among all “active” skeletons. Through this
relationship, we avoid the generation of redundant mappings. We
omit the details of how and when skeletons are considered subsumed or implied by another, which are explained in [8, 18].
Any active skeleton that is not implied or subsumed by another,
is reported as a mapping. The construction of a mapping from an
active skeleton is relatively straightforward: essentially, the source
tableau expression becomes the for clause and the first where clauses
of the mapping. The target tableau becomes the exists and second
where clause. Finally, the value correspondences that are associated with the skeleton are added to the second where clause.

4.2

Source.Sales.country → Target.CountrySales.country
Source.Sales.style → Target.CountrySales.Sales.style

In order to compare the above path expressions with expressions
in the tableaux, each variable binding in a tableau expression is
first expanded into an absolute path. For example, recall that a
target tableau for Figure 1(a) is {$y0 ∈ Target.CountrySales, y1 ∈
$y0 .Sales}. The absolute path of y1 is Target.CountrySales.Sales.
For each value correspondence, the path on the left (resp. right)
(called correspondence path) is matched against absolute paths of
source (resp. target) tableaux. A correspondence path p1 is said
to match an absolute path p2 if p2 is a prefix of p1 . Observe that
a match of the left and right correspondence paths of a value correspondence into a source and target tableau corresponds to a selection of a skeleton in the matrix. After a match has been found,
we then replace the longest possible suffix of the correspondence
path with a variable in the tableau. For example, the right correspondence path of the second value correspondence above matches
against the absolute path of the tableau {$y0 ∈ Target.CountrySales,
y1 ∈ $y0 .Sales}. The expression “$y1 .style” is generated as a result. The left correspondence of the same value correspondence
is matched against the only source tableau { $x ∈ Source.Sales }
and the expression “$x.style” is generated. The result of matching
a value correspondence to a source and target tableau is an equality expression (e.g., “$x.style = $y1 .style”) which is added to the
corresponding skeleton in the matrix.
Matching correspondences paths in the presence of dynamic elements or placeholders to tableaux proceeds in a similar manner.
Our translation of value correspondences, that starts or ends at placeholders or dynamic elements, into path expressions is slightly different in order to faciliate the subsequent matching process. When
a value correspondence starts or ends with the label part of a placeholder, the element name corresponding to this label is the name
of the placeholder (i.e., hhDii). If a value correspondence starts or
ends with the value part of a placeholder, the element name corresponding to this value is “&hhDii”, where hhDii is the name of the
placeholder and &hhDii represents the value part of hhDii.
We explain the complete MAD mapping generation process through
two examples next. More details about the mapping generation algorithm are presented in Appendix C.

Generation of MAD mappings

We are now ready to explain how MAD mappings are generated
from our visual specification that consists of a source schema, a
target ndos schema, and value correspondences between the two.
Step 1. Tableaux: We start by compiling the given schemas into
source and target tableaux. This step is similar to Step 1 of the basic
mapping generation algorithm, except that our representation of a
tableau is more elaborate and takes into account placeholders and
dynamic elements:
T 0 ::= {$x1 ∈ g1 , . . . , $xt ∈ gt ; $xt+1 := gt+1 , . . . ; E}
The “assignments” at the end of our tableau representation are only
generated when placeholders or dynamic elements appear in the
schema. In our subsequent discussions, we uniformly denote placeholders and dynamic elements with hhDii.
For every hhDii, we find the set P(D) of all tableaux that include
the context element of hhDii. The context element of hhDii is the
record or choice in the schema in which hhDii occurs. For example,
SalesByCountry is the context element of hhcountriesii in the source
schema of Figure 2(a). Next, we extend each tableaux p ∈ P(D)
by adding two path expressions corresponding to: (a) the metadata
label of hhDii, and (b) the value label of hhDii. Specifically, let $x be
the variable that ranges over the context elements of hhDii. We first
add to p an expression “$l ∈ hhDii” to represent an iteration over
all the metadata values in hhDii3 . After this, we examine the type of
the values under the labels in hhDii. If the values are a set type, we
add to p an expression “$x0 ∈ $x.($l)”. The new variable $x0 will
range over the elements in the set represented by $x.($l). (If the

4.2.1

Examples

Consider the example in Figure 2. When the schemas are loaded,
the system creates one source tableau, {$x1 ∈ Source.SalesByCountry}, and one target tableau {$y1 ∈ Target.Sales}. This results in only one mapping skeleton.
Now the user creates the source placeholder hhcountriesii. Internally, our system replaces the three selected source labels with a

3
Recall that hhDii denotes a set of (label, value) pairs. The expression “$l ∈ hhDii” ranges $l over the labels of hhDii.

266

new element, named hhcountriesii and whose type is SetOf Record[
label: String, value: String]. This change in the schema triggers a
recompilation of the source tableau into: {$x1 ∈ Source.SalesByCountry, $x2 ∈ hhcountriesii; $x3 := $x1 .($x2 )} A new skeleton
using this new source tableau is thus created.
The user then enters the three value correspondences of Figure 2.
Of particular interests to this discussion are the value correspondences that map the placeholder hhcountriesii:

T1

Phase 1:

Phase 2:

Q1 shreds the source
instance I over
relational views of
the target ndos

Q2 assembles the
target instance J from
the relational views
Q3 computes the target
schema T
Q4 is the optional post processing

S1

T2
T3
ndos

conforms-to

T2

T1

T3

T4

T

S
conforms-to

conforms-to

Source.SalesByCountries.hhcountriesii → Target.Sales.country
Source.SalesByCountries.&hhcountriesii → Target.Sales.units

I

These two value correspondences match the new source tableau
and the only target tableau. Hence, the expressions $x2 = $y1 .country
and $x3 = $y1 .units are compiled into the skeleton. Since $x3 is
an assignment in the source tableau, we rewrite the second correspondence as $x1 .($x2 ) = $y1 .units and can redact the $x3 assignment from the mapping.
The following MAD mapping is constructed from that skeleton,
using its source and target tableaux and the matched value correspondences:
(a) for $x1 in Source.SalesByCountry, $x2 ∈ hhcountriesii
exists $y1 in Target.Sales
where $y1 .month = $x1 .month and
$y1 .country = $x2 and $y1 .units = $x1 .($x2 )

As a final rewrite, we replace the hhcountriesii placeholder in the for
clause with the set of labels wrapped by the placeholder, to capture
the actual label values in the mapping expression. The resulting
mapping is exactly the one illustrated in Figure 2(b).
Next, consider the more complex example of Figure 3. Here
there is only one source tableau and one dynamic target tableau.
After the value correspondences are entered, the system emits the
following MAD mapping:
(b) for $x1 in Source.Sales
exists $y1 in Target.ByShipdateCountry,
$y2 in hhdatesii, $y3 in case $y1 of $y2 ,
$y4 in hhcountriesii, $y5 in $y3.($y4 )
where $y2 = $x1 .shipdate and $y4 = $x1 .country and
$y5 .style = $x1 .style and $y5 .units = $x1 .units and
$y5 .price = $x1 .price

We rewrite this expression by first replacing all usages of $y2 and
$y4 in the exists clause with their assignment from the where
clause. Since these assignments are redundant after the replacements, they are redacted from the where clause. Further, since all
uses of $y2 and $y4 were removed from the where clause, their
declarations in the exists clause are also redundant and, therefore,
removed. The resulting MAD mapping is reduced to the mapping
expression presented below:

Q1

r

Q3
r

r

J

r

Q2

Q4

Figure 5: The architecture of the two-phase query generation.

5.

QUERY GENERATION

Mappings have an executable semantics that can be expressed
in many different data manipulation languages. In this section, we
describe how queries are generated from MAD mappings (and the
associated source and target schemas) to translate a source instance
into a target instance according to the semantics of the MAD mappings. Previous works [8, 18] have described algorithms for generating queries from (data-to-data) mappings. Here, we generalize
those algorithms to MAD mappings, which include constructs for
data-metadata transformations. If the visual specification involves
a target ndos schema, the MAD mappings that are generated from
our mapping generation algorithm (described in Section 4) include
constructs for specifying data-to-metadata translations. In this case,
our algorithm is also able to generate a query that outputs a target
schema which conforms to the ndos schema, when executed against
a source instance.
In order to distinguish between the two types of queries generated by our algorithm, we call the first type of queries which
generates a target instance, instance queries, and the second type
of queries which generates a target schema, schema queries. Figure 5 shows where the different kind of queries are used in MAD.
Queries Q1 , Q2 , and Q4 represent our instance queries, and Q3
represent the schema queries. As will discuss shortly, Q2 and Q3
work form the data produced by the first query Q1 .

5.1

Instance Query Generation

Our instance query generation algorithm produces a query script
that, conceptually, constructs the target instance with a two-phase
process. In the first phase, source data is “shredded” into views
that form a relational representation of the target schema (Q1 in
Figure 5). The second phase restructures the data in the relational
views to conform to the actual target schema (Q2 ). We now describe these two stages in details.
The instance query generation algorithm takes as input the compiled source and target schemas and the mapping skeletons that
were used to produce the MAD mappings. Recall that a mapping
skeleton contains a pair of a source and a target tableau, and a set
of compiled value correspondences.
Our first step is to find a suitable ”shredding” of the target schema.
All the information needed to construct these views is in the mapping expression and the target schema. In particular, the exists
clause of the mapping dictates the part of the target schema being
generated. We start by breaking each mapping into one or more
“single-headed” mappings; we create one single-headed mapping

(c) for $x1 in Source.Sales
exists $y1 in Target.ByShipdateCountry,
$y3 in case $y1 of $x1 .shipdate,
$y5 in $y3.($x1 .country)
where $y5 .style = $x1 .style and $y5 .units = $x1 .units and
$y5 .price = $x1 .price

Observe that this mapping differs from the one in Figure 3(b)
in that the grouping condition “$u.($s.country)=SK[$s.shipdate,
$s.country]” is missing here. We assume that the user has explicitly
added the grouping function in Figure 3(b), after the value correspondences are entered (i.e., after the mapping above is obtained).

267

Target = for b in ByShipdateCountry
return [
for s in «dates»
where s.setID = b.datesID
return [
s.label = for c in «countries»
where c.setID = s.countriesID
return [
c.label = for r in SetOfRecord_1
where r.setID = c.SetOfRecord_1
return [ style = r.style,
units = r.units
price = r.price ] ] ] ]

let ByShipdateCountry :=
for s in Sales
return [ datesID = SK1[s.shipdate, s.country, s.style, s.units, s.price] ],
«dates» :=
for s in Sales
return [ setID = SK1[s.shipdate, s.country, s.style, s.units, s.price],
label = s.shipdate,
value = SK2[s.shipdate, s.country, s.style, s.units, s.price],
countriesID = SK2[s.shipdate, s.country, s.style, s.units, s.price] ],
«countries» :=
for s in Sales
return [ setID = SK2[s.shipdate, s.country, s.style, s.units, s.price],
label = s.country, value = SK3[s.shipdate, s.country],
SetOfRecords1_ID = SK3[s.shipdate, s.country] ],
SetOfRecords1 :=
for s in Sales
return [ setID = SK3[s.shipdate, s.country],
style = s.style, units = s.units, price = s.price ]

Figure 7: Second-phase query (Q2 )
is created and the appropriate values are copied into each produced
target tuple. To reconstruct the structure of the target schema, views
are joined on the appropriate fields. For example, SetOfRecord 1
is joined with hhcountriesii on setID and SetOfRecords ID fields in
order to nest all (style, units, price) records under the appropriate
hhcountriesii element. The query that produces the nested data is in
Figure 7. Notice how the label values of the dynamic elements (s
and c) become the set names in the target instance.
While there are simpler and more efficient query strategies that
work for a large class of examples, it is not possible to apply them
in general settings. This two-phase generation strategy allows us to
support user-defined grouping in target schemas with nested sets.
Also, it allows us to implement grouping over target languages that
do not natively support group-by operations (e.g., XQuery 1.0).

Figure 6: First-phase query (Q1 )
for each variable in the exists clause of the mapping bounded to
a set type or a dynamic expression. Each single-headed mapping
defines how to populate a region of the target instance and is used
to define a target view. To maintain the parent-child relationship
between these views, we compute a parent “setID” for each tuple
in a view. This setID tells us under which tuple (or tuples) on the
parent view each tuple on a child view belongs to.
The setID are actually computed by “Skolemizing” each variable
in the exists clause of the mapping4 . The Skolemization replaces
each variable in the exists clause with a Skolem function that depends on all the source columns appear in the where clause.
For example, consider the mapping labeled (b) in Section 2 (the
mapping we compute internally for the example in Figure 3). The
exists clause of the mapping defines four set type or dynamic expressions. Thus, we construct the following views of the target
instance:
ByShipdateCountry
hh dates ii
hh countries ii
SetOfRecord 1

5.2

We can also generate a schema query (Q3 ) when the target schema
is a ndos. Continuing with our example, we produce the following
query:
Schema = Target: Rcd
ByShipdateCountry: SetOf Choice
let dates := distinct-values («dates».label)
for d in dates
where valid(d)
return [
d: Rcd
let cIDs := distinct-values («dates»[.label=d].CountriesID)
for ci in cIDs
return [
let countries := distinct-values («countries»[.setID=ci].label)
for c in countries
where valid(c)
return [
c: SetOf Rcd
style, units, price ] ] ]

( DatesID )
( setID, label, value, CountriesID )
( setID, label, value, SetOfRecords 1ID )
( setID, style, units, price )

Every view contains the atomic elements that are directly nested
under the set type it represents. A view that represents a set type
that is not top-level has a generated setID column that contains
the defined Skolem function. Observe that Skolem functions are
essentially set identifiers which can be used to reconstruct data in
the views according to the structure of the target schema by joining on the appropriate ID fields. For example, the set identifier
for hhcountriesii is SK[$s.shipdate, $s.country, $s.style, $s.units,
$s.price] and the set identifier for SetOfRecords 1 is SK[$s.shipdate,
$s.country]. The latter is obtained directly from the mapping since
it is defined by the user in Figure 3(b).
Figure 6 depicts the generated queries that define each view.
These queries are constructed using the source tableau and the value
correspondences from the skeleton that produced the mapping. Notice that in more complex mappings, multiple mappings can contribute data to the same target element. The query generation algorithm can detect such cases and create a union of queries that are
generated from all mappings contributing data to the same target
element.
The next step is to create a query that constructs the actual target instance using the views. The generation algorithm visits the
target schema. At each set element, it figures which view produces
data that belongs in this set. A query that iterates over the view
4

Schema Query Generation

The schema query follows the structure of the ndos schema closely.
Explicit schema elements, those that are statically defined in the
ndos, appear in the query as-is (e.g., Target and ByShipdateCountry). Dynamic elements, on the other hand, are replaced with a subquery that retrieves the label data from the appropriate relational
view computed by Q1 . Notice that we use distinct-value() when
creating the dynamic target labels. This avoids the invalid generation of duplicate labels under the same Record or Choice type.
Also, we call a user-defined function valid() to make sure we only
use valid strings as labels in the resulting schema. Many schema
models do not support numbers, certain special characters, or may
have length restrictions on their metadata labels. used in the target
as metadata.

5.3

Post-processing

We have an additional module that generates post-processing
scripts that execute over the data produced by the instance query.

We can also use only the key columns, if available.

268

100000

Post-processing is needed when there is a need to enforce the homogeneity (or relational) constraint, or a key constraint.
An example where both the homogeneity and key constraints are
used is the StockTicker-Stockquotes example (described in Section 2.3 with the homogeneity constraint labeled c). The transformation that implements the homogeneity constraint is a relatively straightforward rewriting where an important function is introduced to implement the dom operator.
In our implementation, it is possible to generate a post-processing
script that enforces both homogeneity and key constraints simultaneously. The script that does this for the StockTicker-Stockquotes
example is shown below:

Traditional mappings
MAD mappings
Optimized from MAD mapping

Query execution time [s]

10000

1
0

200

300

400

500

600

Figure 8: Impact of MAD mappings for Metadata-to-Data exchange.
trials is reported. Datasets were generated using ToXgene6 .

6.1

Metadata-to-Data

We use the simple mapping in Figure 2 to test the performance
of the generated instance queries. This simple mapping, with one
placeholder, allows to clearly study the effect of varying the number of labels assigned to the placeholder. We compare the performance of three XQuery scripts. The first one was generated using
a traditional schema mappings and the query generation algorithm
in [18]. For each label value in the source, a separate query over the
source data is generated by [18] and the resulting target instance is
the union of the result of all those queries. The second query script
is our two-phase instance query. The third query script is an optimized version of our instance query. If we only use source placeholders, we know at compile-time the values of the source labels
that will appear as data in the target. When this happens, we can
remove the iteration over the labels in the source and directly write
as many return clauses as needed to handle each value.
We ran the queries and increased the input file sizes, from 69
KB to 110 MB, and a number of distinct labels from 3 to 600.
The generated Input data varied from 600 to 10,000 distinct tuples.
Figure 8 shows the query execution time for the three queries when
the input had 10,000 tuples (the results using smaller input sized
showed the same behavior).
The chart shows that classical mapping queries are outperformed
by MAD mapping queries by an order of magnitude, while the optimized queries are faster by two orders of magnitude. Again, the
example in Figure 2 presents minimal manipulation of data, but effectively shows that one dynamic element in the mapping is enough
to generate better queries than existing solutions for mapping and
query generation. The graph also shows the effect of increasing the
number of distinct labels encoded in the placeholder. Even with
only 3 distinct labels in the placeholder, the optimized query is
faster than the union of traditional mapping queries: translating
10,000 tuples it took 1 second vs 2.5 seconds. Notice that when the
number of distinct labels mapped from the source are more than
a dozen, a scenario not unusual when using data exported from
spreadsheets, the traditional mapping queries take minutes to hours
to complete. Our optimized queries can take less than two minutes
to complete the same task.

Implementation Remarks

In our current prototype implementation, we produce instance
and schema queries in XQuery. It is worth pointing out that XQuery,
as well as other XML query languages such as XSLT, support querying of XML data and metadata, and the construction of XML data
and metadata.
In contrast, relational query languages such as SQL do not allow us to uniformly query data and metadata. Even though many
RDBMS store catalog information as relations and allow users to
access it using SQL, the catalog schema varies from vendor-tovendor. Furthermore, to update or create catalog information, we
would need to use DDL scripts (not SQL). Languages that allow
one to uniformly manipulate relational data-metadata do exists (e.g.,
SchemaSQL [11] and FISQL [25]). It is possible, for e.g., to implement our relational data-metadata translations as FISQL queries.

EXPERIMENTS

We conducted a number of experiments to understand the performance of the queries generated from MAD mappings and compared them with those produced by existing schema mappings tools.
Our prototype is implemented entirely in Java and all the experiments were performed on a PC-compatible machine, with a single
1.4GHz P4 CPU and 760MB RAM, running Windows XP (SP2)
and JRE 1.5.0. The prototype generates XQuery scripts from the
mappings and we used the Saxon-B 9.05 engine to run them. Each
experiment was repeated three times, and the average of the three
5

100

Number of distinct labels

The query above first defines two sets. The first set times is the
set of all values under the key attribute time. The second set attributes is the set of all the attributes names in the target instance
Stockquotes. For each key value in times, all tuples in the target
instance with this key value are collected under a third set called
elements. At this point, the query iterates over all attribute names
in attributes. For each attribute a in attributes, it checks whether
there is a tuple t in elements with attribute a. If yes, the output tuple
will contain attribute a with value as determined by t.a. Otherwise,
the value is null. It is possible that there is more than one tuple in
elements with different a values. In this case, a conflict occurs and
no target instance can be constructed.

6.

100

10

Target’ = let times := Target.Stockquotes.time,
attributes := dom (Target.Stockquotes)
for t in times
return [
Stockquotes= let elements := Target.Stockquotes[time=t]
for a in attributes
return [
if is-not-in (a, elements)
then a = null
else a = elements.a ] ]

5.4

1000

6.2

Data-to-Metadata

To understand the performance of instance queries for data-to6

http://saxon.sourceforge.net/

269

http://www.cs.toronto.edu/tox/toxgene

100000

10000

1000

Query Execution Time [s]

Query execution time [s]

100000

Data exchange (1)
Make hom. (1)
Make hom.+merge (1)
Data exchange w/Merge (2)
Make Hom (2)

100

10000

1000

100

Data exchange + merge + hom. (1)
Data exchange w/merge + hom. (2)

10

1
10

0

20000

40000

60000

80000

100000

120000

Number of tuples
1
1000

3000

6000

12000

24000

44000

60000

120000

Figure 10: Performance of MAD mappings for Data-toMetadata exchange.

Number of tuples

Figure 9: Data exchange and post processing performance.
Figure 10 compares the total execution times of sets (1) and (2).
The time includes the query to generate the target instance and the
needed time to make the record homogeneous and merge the result. The results show that optimized queries are faster than default
queries by an order of magnitude. Notice that queries in set (1)
took more than two minutes to process 12,000 tuples, while the
optimized queries in set (2) needed less than 25 seconds.

metadata mappings, we used the simple StockTicker - Stockquotes
example from Section 2.3. In this example, the target schema used
a single ndos construct. To compare our instance queries with those
produced by a data-data mapping tools, we manually constructed
a concrete version of the target schema (with increasing numbers
of distinct symbol labels). Given n such labels, we created n
simple mappings, one per target label, and produced n data-to-data
mappings. The result is the union of the result of these queries.
The manually created data-to-data mappings performed as well as
our data-to-metadata instance query. Notice, however, that using
MAD we were able to express the same result with only three value
correspondences.
We now discuss the performance of the queries we use to do
post-processing into homogeneous records and merging data using
special Skolem values. In this set of experiments, we used source
instances with increasing number of StockTicker elements (from
720 to 120,000 tuples), and a domain of 12 distinct values for the
symbol attribute. We generated from 60 to 10,000 different time
values, and each time value is repeated in the source in combination with each possible symbol. We left a fraction (10%) of time
values with less than 12 tuples to test the performance of the postprocessing queries that make the records homogeneous. The generated input files range in sizes from 58 KB to 100 MB.
Figures 9 and 10 show the results for two sets of queries, identified as (1) and (2). The set (1) includes the instance queries (Q1
and Q2 in Figure 5 and labeled “Data exchange” in Figure 9), and
two post-processing queries (Q4 ): one that makes the target record
homogeneous (labeled “Make hom”), and another that merges the
homogeneous tuples when they share the same key value (labeled
“Make hom.+merge”). Set (2) contains the same set of queries as
(1) except that the instance queries use a user-defined Skolem function that groups the dynamic content by the value of time. The
instance queries in (1) use a default Skolem function that depends
on the values of time and symbol.
Figure 9 shows the execution time of the queries in the two test
sets as the number of input tuples increase. We first note that
the instance query using the default Skolem function (“Data exchange”) takes more time to complete than the instance query that
uses the more refined, user entered, Skolem function (“Data exchange w/Merge”). This is expected, since, in the second phase,
the data exchange with merge compute only 1 join for each key
value, while the simple data exchange computes a join for each pair
of (time, symbol) values. It is also interesting to point out that the
scripts to make the record homogeneous are extremely fast for both
sets, while the merging of the tuple produced by the data exchange
is expensive since a self join over the output data is required.

7.

RELATED WORK

To the best of our knowledge, there are no mapping or exchange
systems that support data-metadata translations between hierarchical schemas, except for HePToX [3]. Research prototypes, such as
[9, 14], and commercial mapping systems, such as [5, 13, 21, 22],
fully support only data-to-data translations. Some tools [13, 21] do
have partial support for metadata-to-data transformations, exposing
XQuery or XSLT functions to query node names. However, these
tools do not offer constructs equivalent to our placeholder and users
need to create a separate mapping for each metadata value that is
transformed into data (as illustrated in Section 2.2).
In contrast to the XML setting, data exchange between relational schemas that also support data-metadata translations have
been studied extensively. (See [24] for a comprehensive overview
of related work.) Perhaps the most notable work on data-metadata
translations in the relational setting are SchemaSQL [11] and, more
recently, FIRA / FISQL [24, 25]. [15] demonstrated the practical importance of extending traditional query languages with datametadata by illustrating some real data integration scenarios involving legacy systems and publishing.
Our MAD mapping language is similar to SchemaSQL. SchemaSQL allows terms of the form “relation → x” in the FROM clause
of an SQL query. This means x ranges over the attributes of relation. This is similar to our concept of placeholders, where one can
group all attributes of relation under a placeholder, say hhallAttrsii,
and range a variable $x over elements in hhallAttrsii by stating
“$x in hhallAttrsii” in the for clause of a MAD mapping. Alternatively, one can also write “$x in dom(relation)” to range $x
over all attributes of relation or, write “$x in {A1 , ..., An }”, where
the attributes A1 , ..., An of relation are stated explicitly. One can
also define dynamic output schemas through a view definition in
SchemaSQL. For example, the following SchemaSQL view definition translates StockTicker to Stockquotes (as described in
Section 1). The variable D is essentially a dynamic element that
binds to the value symbol in tuple s and this value is “lifted” to
become an attribute of the output schema.
create view DB::Stockquotes(time, D) as
select s.time, s.price
from Source::StockTicker s, s.symbol D

270

This view definition is similar to the MAD mappings m and c
that was described in Section 2.3. (Recall that the mapping m describes the data-to-metadata translation and c is used to enforce the
homogeneity/relational model.) A major difference, however, is
that the SchemaSQL query above produces only two tuples in the
output where tuples are merged based on time. On the other hand,
mappings m and c will generate queries that produce three tuples
as shown in Section 1. It is only in the presence of an additional
key constraint on time that two tuples will be produced.
FISQL is a successor of SchemaSQL that allows more general
relational data-metadata translations. In particular, while SchemaSQL allows only one column of data to be translated into metadata in one query, FISQL has no such restriction. In contrast to
SchemaSQL which is may be non-deterministic in the set of output
tuples it produces due to the implicit merge semantics, FISQL does
not merge output tuples implicitly. MAD mappings are similar to
FISQL in this aspect. However, unlike FISQL which has an equivalent algebra called FIRA, we do not have an equivalent algebra
for MAD mappings. Recall, however, that the purpose of MAD is
to automatically generate mapping expressions that encode these
data-metadata transformation starting from simple lines. In Sections 5 we described how the generated mappings are translated
into a query. If our source and target schemas are relational, we
could translate those queries into SchemaSQL or FISQL.
HePToX [3, 4] is a P2P XML database system that uses a framework that has components that are similar to the first two components of data exchange, as described in Section 1. A peer joins
a network by drawing lines between the peer Document Type Descriptor (DTD) and some existing DTDs in the network. The visual
specification is then compiled into mappings that are expressed as
Datalog-like rules. Hence, these two components of HePToX are
similar to the visual interface and, respectively, mapping generation
components of the data exchange framework. Although HePToX’s
Datalog-like language, TreeLog, can describe data-to-metadata and
metadata-to-data translations, HePToX does not allow target dynamic elements. I.e., neither HePToX’s GUI nor its mapping generation algorithm support nested dynamic output schemas.
Lastly, the problem of generating a target schema using mappings from a source schema (i.e., metadata-to-metadata translations) is also known as schema translation [17]. The ModelGen
operator of Model Management [2] (see Section 3.2 of [1] for a
survey of ModelGen work) relies on a library of pre-defined transformations (or rules) that convert the source schema into a target
schema. Alternatively, [17] uses mappings between meta-metamodels to transform metadata. None of these approaches, however,
allow for data-to-metadata transformations.

8.

erate the mappings and the corresponding queries that perform the
exchange.
Acknowledgments We thank Lucian Popa and Cathy M. Wyss
for insightful comments on the subject of the paper.

9.

CONCLUSION

We have presented the problem of data exchange with data-metadata
translation capabilities and presented our solution, implementation
and experiments. We have also introduced the novel concept of
nested dynamic output schemas, which are nested schemas that
may only be partially defined at compile time. Data exchange
with nested dynamic output schemas involves the materialization
of a target instance and additionally, the materialization of a target schema that conforms to the structure dictated by the nested
dynamic output schema. Our general framework captures relational data-metadata translations and data-to-data exchange as special cases. Our solution is a complete package that covers the entire
mapping design process: we introduce the new (minimal) graphical constructs to the visual interface for specifying data-metadata
translations, extend the existing schema mapping language to handle data-metadata specifications, and extend the algorithms to gen-

271

REFERENCES

[1] P. Bernstein and S. Melnik. Model Management 2.0: Manipulating
Richer Mappings. In SIGMOD, pages 1–12, 2007.
[2] P. A. Bernstein. Applying Model Management to Classical Meta
Data Problems. In CIDR, pages 209–220, 2003.
[3] A. Bonifati, E. Q. Chang, T. Ho, and L. V. S. Lakshmanan. HepToX:
Heterogeneous Peer to Peer XML Databases. Technical Report
CoRR cs.DB/0506002, arXiv.org, 2005.
[4] A. Bonifati, E. Q. Chang, T. Ho, L. V. S. Lakshmanan, and
R. Pottinger. HePToX: Marrying XML and Heterogeneity in Your
P2P Databases. In VLDB(demo), pages 1267–1270, 2005.
[5] M. J. Carey. Data delivery in a service-oriented world: the BEA
aquaLogic data services platform. In SIGMOD, pages 695–705,
2006.
[6] R. Fagin, P. G. Kolaitis, R. J. Miller, and L. Popa. Data Exchange:
Semantics and Query Answering. TCS, 336(1):89–124, 2005.
[7] M. Friedman, A. Y. Levy, and T. D. Millstein. Navigational Plans For
Data Integration. In AAAI/IAAI, pages 67–73, 1999.
[8] A. Fuxman, M. A. Hernández, H. Ho, R. J. Miller, P. Papotti, and
L. Popa. Nested Mappings: Schema Mapping Reloaded. In VLDB,
pages 67–78, 2006.
[9] L. M. Haas, M. A. Hernández, H. Ho, L. Popa, and M. Roth. Clio
Grows Up: From Research Prototype to Industrial Tool. In SIGMOD,
pages 805–810, 2005.
[10] P. G. Kolaitis. Schema mappings, data exchange, and metadata
management. In PODS, pages 61–75, 2005.
[11] L. V. S. Lakshmanan, F. Sadri, and I. N. Subramanian. SchemaSQL A Language for Interoperability in Relational Multi-Database
Systems. In VLDB, pages 239–250, 1996.
[12] M. Lenzerini. Data Integration: A Theoretical Perspective. In PODS,
pages 233–246, 2002.
[13] Altova MapForce Professional Edition, Version 2008.
http://www.altova.com.
[14] S. Melnik, P. A. Bernstein, A. Halevy, and E. Rahm. Supporting
Executable Mappings in Model Management. In SIGMOD, pages
167–178, 2005.
[15] R. J. Miller. Using Schematically Heterogeneous Structures. In
SIGMOD, pages 189–200, 1998.
[16] R. J. Miller, L. M. Haas, and M. A. Hernández. Schema Mapping as
Query Discovery. In VLDB, pages 77–88, 2000.
[17] P. Papotti and R. Torlone. Schema exchange: A template-based
approach to data and metadata translation. In ER, pages 323–337,
2007.
[18] L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernández, and R. Fagin.
Translating Web Data. In VLDB, pages 598–609, 2002.
[19] E. Rahm and P. A. Bernstein. A survey of approaches to automatic
schema matching. VLDB J., 10(4):334–350, 2001.
[20] M. Roth, M. A. Hernández, P. Coulthard, L. Yan, L. Popa, H. C.-T.
Ho, and C. C. Salter. XML mapping technology:Making connections
in an XML-centric world. IBM Sys. Journal, 45(2):389–410, 2006.
[21] Stylus Studio 2008, XML Enterprise Suite, Release 2.
http://www.stylusstudio.com.
[22] Microsoft BizTalk Server 2006 R2.
http://www.microsoft.com/biztalk/.
[23] C. M. Wyss and E. L. Robertson. A Formal Characterization of
PIVOT/UNPIVOT. In CIKM, pages 602–608, 2005.
[24] C. M. Wyss and E. L. Robertson. Relational Languages for Metadata
Integration. ACM TODS, 30(2):624–660, 2005.
[25] C. M. Wyss and F. I. Wyss. Extending Relational Query Optimization
to Dynamic Schemas for Information Integration in Multidatabases.
In SIGMOD, pages 473–484, 2007.
[26] C. Yu and L. Popa. Semantic adaptation of schema mappings when
schemas evolve. In VLDB, pages 1006–1017, 2005.

APPENDIX

role is to copy the attributes as elements on the target. MAD produces the following mapping for this specification:

A. DEFINITION OF “CONFORMS TO”

for $x1 in Source.properties, $x2 in hhattrsii
let $x3 := $x1 .property.($x2 )
exists $y1 in Target.properties, $y2 in hhnamesii, $y3 in hhelemsii
$y4 := $y1 .($y2 ), $y5 := $y1 .($y3 )
let
where $y2 = $x1 .property.@name and
$y4 .@amount = $x1 .property.pval and
$y3 = $x2 and
$y5 = $x3

A NR schema T with roots T10 , ..., Tk0 conforms to a ndos schema
Γ with roots R1 , ..., Rk if there is a permutation of T10 , ..., Tk0 into
T1 , ..., Tk such that Ti conforms to Ri , 1 ≤ i ≤ k.
1. If R is of String type, then T conforms to R if T is of type
String.
2. If R is of Int type, then T conforms to R if T is of type Int.
3. If R is of Rcd[l1 : τ1 , ..., lm : τm , $d : τ ] type, then T
0
conforms to R if T is of type Rcd[l1 : τ10 , ..., lm : τm
, lm+1 :
0
0
0
τm+1 , ..., ln : τn ] and τi conforms to τi , 1 ≤ i ≤ m, and
0
τm+j
conforms to τ , 1 ≤ j ≤ n.

Replacing x2 with a set of literal values, and rewriting to remove
$x3 , and $y2 , . . . , $y5 , we obtain a simplified mapping expression:
for $x1 in Source.properties, $x2 in {‘@lang’,‘@data’,...,‘@format’}
exists $y1 in Target.properties
where $y1 .($x1 .property.@name).@amount = $x1 .property.pval and
$y1 .($x1 .property.@name).($x2 ) = $x1 .property.($x2 )

4. If R is of Choice[l1 : τ1 , ..., lm : τm , $d : τ ] type, then
T conforms to R if T is of type Choice[l1 : τ10 , ..., lm :
0
0
, ..., ln : τn0 ] and τi0 conforms to τi , 1 ≤ i ≤
τm
, lm+1 : τm+1
0
m, and τm+j conforms to τ , 1 ≤ j ≤ n.

B.

C.

METADATA-TO-METADATA TRANSLATION EXAMPLE

Unlike data-to-data, metadata-to-data, or data-to-metadata translations, metadata-to-metadata translations are not as interesting as
they can always be implemented with the traditional data-to-data
translation framework. To see this, consider the example shown
below on the left which, essentially, generates a copy of the source
schema and instance. This is no different from specifying the exchange as shown on the bottom right.
Source: Rcd
Target: Rcd
Sales: SetOf Rcd
Sales: SetOf Rcd
month
month
label
USA
<<countries>> value
UK
label
Italy

value

Source: Rcd
Sales: SetOf Rcd
month
USA
UK
Italy

Target: Rcd
Sales: SetOf Rcd
month
USA
UK
Italy

However, when combined with other data-metadata constructs,
we can accomplish complicated mappings with just a few lines.
Consider the example in Figure 11. This kind of transformations
are not uncommon in many data exchange situations.
Source: Rcd
properties: SetOf Rcd Target: Rcd
properties: SetOf Rcd
property: Rcd
label1 <<names>>
@name
<<@attrs>>
value1: Rcd
@lang
label
@amount
@date
value
label2 <<@elems>>
…
value2
@format
pval
...
<properties>
<price
...
<property name=“price”
lang=“en-us”
date=“01-01-2008”
...
... >
<pval>48.15</pval>
...
</properties>

MAD MAPPING GENERATION

This section uses pseudo-code to summarize the MAD mapping
generation algorithm discussed in Section 4.
Algorithm 1 shows how we prepare the tableaux and skeletons
when dynamic placeholders are present in the source and target
schemas. The main difference between this algorithm and the mapping generation algorithm of Clio [18] are steps 3–6 and 15–22.
Algorithm 2 shows how we process value correspondences and
create MAD mappings using the tableaux and skeletons prepared
by Algorithm 1. This procedure is similar to the one used by Clio.
The main differences are that the MAD mapping generation algorithm 1) needs to take into account the let clauses in the tableaux,
and 2) needs to take into account the value correspondences mapping labels and values to and from dynamic placeholders. This is
done in steps 20–22.
The optional step 23 in Algorithm 2 uses the rewrite method
described in Algorithm 3 to obtain the simplified MAD mapping
expressions we presented in this paper. The simplification removes
all let clauses by in-lining the assignments in the where clauses.
Any assignment in the where clause to the label part of a target dynamic placeholder can be moved to a let clause and, thus,
in-lined too. Finally, the set of labels represented by source-side
placeholder replaces the placeholder name in the for clause.

(a)

(b)

amount=“48.15”
lang=“en-us”
date=“01-01-2008”
... />

Figure 11: A more complex example
This mapping is expressed with a data-to-meta data combined
with a metadata-to-metadata mapping, whose only (but crucial)

272

Algorithm 2 MAD mapping generation
Require: a set of skeletons K and a set of correspondences V
Ensure: a set of MAD mappings M
1: M = ∅;
{For each value correspondence in V }
2: for v ∈ V do
3:
{Find all matching skeletons.}
4:
for k = (ti , tj ) ∈ K do
5:
if source(v) matches ti and target(v) matches tj then
6:
Add v to the set of correspondences matched to k.
7:
Mark k as “active”.
8:
end if
9:
end for
10: end for
{Remove implied and subsumed mappings}
11: for k = (ti , tj ) ∈ K do
12:
{The definition of “subsumed” and “implied” is in [8].}
13:
if k is “active” and is subsumed or implied by another active
skeleton then
14:
Mark k as “inactive”.
15:
end if
16: end for
{Emit the mappings}
17: for k = (ti , tj ) ∈ K do
18:
if k is “active” then
19:
m ← a new mapping for skeleton (ti , tj ).
20:
Add the expressions in ti to the for, let, and where clauses
of m.
21:
Add the expressions in tj to the exists, let, and where
clauses of m.
22:
Use the value correspondences matched to k to create the
s-t conditions in the last where clause of m.
{An optional simplification of the mappings}
23:
m ← rewrite(m)
24:
M ←M ∪m
25:
end if
26: end for

Algorithm 1 Prepare Mapping Skeletons
Require: a source schema S and a target ndos T .
Ensure: a set of mapping skeletons.
{Compute a set of source tableaux τS .}
1: Visit the schema tree S starting from the root
2: for each set type, choice type, or dynamic placeholder do
3:
if visiting a dynamic placeholder hhDii then
4:
Let xi−1 be the variable used in the context tableau expression.
5:
Create a {xi ∈ hhDii} tableau expression.
6:
Add xi+1 := xi−1 .(xi ) to the let clause of the tableau.
7:
else
8:
if visiting a set or choice type expression gi then
9:
Create a {xi ∈ gi } or a {xi ∈ choice gi of L} tableau
expression as in Clio [18].
10:
end if
11:
end if
12: end for
{Compute a set of target tableaux τT .}
13: Visit the schema tree T starting from the root
14: for each set type, choice type, or dynamic placeholder do
15:
if visiting a dynamic placeholder hhDii then
16:
Let yi−1 be the variable used in the context tableau expression.
17:
Create a {yi ∈ hhDii} tableau expression.
18:
if the type of hhDii.value is a set type then
19:
Add xi+1 ∈ xi−1 .(xi ) to the tableau expression.
20:
else
21:
Add xi+1 := xi−1 .(xi ) to the let clause of the tableau.
22:
end if
23:
else
24:
if visiting a set or choice type expression gi then
25:
Create a {xi ∈ gi } or a {xi ∈ choice gi of L} tableau
expression as in Clio [18].
26:
end if
27:
end if
28: end for
29: Enhance the source and target tableau by chasing over the
parent-child relationships and foreign key constraints (details
of this step are in [18]).
{Prepare the skeletons.}
30: Create a set of skeletons K = {(ti , tj ) | ti ∈ τS , tj ∈ τT }.
31: return τS , τT , K

Algorithm 3 rewrite
Require: a mapping m.
Ensure: a simplified version of m.
1: {Remove the let clauses.}
2: for each let clause of the form x := E in m do
3:
Replace occurrences of x with E in the where clause.
4:
Remove x := E from the let clause.
5: end for
{Replace the target-side dynamic placeholders.}
6: for each “x in hhDii” in the exists clause of m do
7:
Find an expression x = E in the where clause.
8:
Remove that expression from the where clause.
9:
Remove “x in hhDii” from the exists clause.
10:
Replace x with E in the where clause.
11: end for
{Replace the source-side placeholders.}
12: for each “x in hhDii” in the for clause of m do
13:
Replace hhDii with a set of literal values {d1 , . . . , dm }.
14: end for
15: return m.

273

Concise and Expressive Mappings with +Spicy
Giansalvatore Mecca1 Paolo Papotti2 Salvatore Raunich1 Marcello Buoncristiano1
1

Dipartimento di Matematica e Informatica – Università della Basilicata – Potenza, Italy
2
Dipartimento di Informatica e Automazione – Università Roma Tre – Roma, Italy

ABSTRACT
We introduce the +Spicy mapping system. The system is
based on a number of novel algorithms that contribute to increase the quality and expressiveness of mappings. +Spicy
integrates the computation of core solutions in the mapping
generation process in a highly efficient way, based on a natural rewriting of the given mappings. This allows for an
efficient implementation of core computations using common runtime languages like SQL or XQuery and guarantees
very good performances, orders of magnitude better than
those of previous algorithms. The rewriting algorithm can
be applied both to mappings generated by the system, or
to pre-defined mappings provided as part of the input. To
do this, the system was enriched with a set of expressive
primitives, so that +Spicy is the first mapping system that
brings together a sophisticate and expressive mapping generation algorithm with an efficient strategy to compute core
solutions.

1. INTRODUCTION
The ability of modern information systems to exchange,
transform and integrate data is nowadays considered a crucial requirement. A fundamental requirement for such data
integration applications is that of manipulating mappings
among data sources. Mappings, also called schema mappings, are executable transformations – say, SQL queries for
relational data or XQuery scripts for XML – that specify
how an instance of the source repository should be translated into an instance of the target repository.
Inspired by the seminal papers about the Clio system [8],
in the last years a rich body of research has studied algorithms and tools for schema mapping generation. These
works have focused on the development of mapping systems
that, given a visual specification of the correspondences between the source and target schemas, generate the mappings
and then the executable scripts needed to perform the translation. However, despite several years of both system and
theory studies, the adoption of mapping systems in real-life

Permission to copy without fee all or part of this material is granted provided
that the copies are not made or distributed for direct commercial advantage,
the VLDB copyright notice and the title of the publication and its date appear,
and notice is given that copying is by permission of the Very Large Data
Base Endowment. To copy otherwise, or to republish, to post on servers
or to redistribute to lists, requires a fee and/or special permission from the
publisher, ACM.
VLDB ‘09, August 24-28, 2009, Lyon, France
Copyright 2009 VLDB Endowment, ACM 000-0-00000-000-0/00/00.

integration applications, such as ETL workflows or EII (Enterprise Information Integration), is quite slow. This is due
to several factors.
One key factor is the quality of solutions produced by
mapping systems. It is well known that a mapping scenario may have many different solutions. These solutions
may differ significantly in size, i.e., they may contain a variable amount of redundant tuples. As shown in [5], for large
source instances the amount of redundancy in the target may
be very large, thus impairing the efficiency of the exchange
and the query answering process. A key contribution of
data exchange research was the formalization of the notion
of core [4], which was identified as an “optimal” solution.
Informally speaking, the core is irredundant, since it is the
smallest among the solutions that preserve the semantics
of the exchange, and provides a “good” semantics for answering queries over the target database. Therefore, it can
be considered a crucial requirement for a schema mapping
system to generate executable scripts that materialize core
solutions for a mapping scenario.
Another factor is the expressibility of the mapping system.
A benchmark for mapping systems called STBenchmark [1]
has been recently proposed to evaluate research mapping
systems and commercial ones. None of the systems was able
to express all the mappings in the benchmark. It is also
known that previous mapping generation algorithms [8] cannot express several natural mappings, like the ones discussed
in [2].
The +Spicy system [7] is an attempt at overcoming these
limitations. It is the first mapping system that brings together a set of expressive mapping generation primitives and
a mapping generation algorithm that generates core solutions. In light of this, we believe +Spicy may contribute
towards the goal of integrating schema mapping concepts
into practical data integration tasks.

2.

OVERVIEW

It is well known that translating data from a given source
database may bring to a certain amount of redundancy into
the target database. To see this, consider the mapping scenario in Figure 1. A source instance is shown in Figure 2.
In this example, the source database contains tables about
books coming from three different data sources, namely the
Internet Book Database (IBD), the Library of Congress database (LOC), and the Internet Book List (IBL). Based on
the correspondences, a constraint-driven mapping system as
Clio would generate for this scenario several mappings, under the form of tuple-generating dependencies (tgds), like the

ones below.

Figure 1: Mapping Bibliographic References
m1 .
m2 .
m3 .
m4 .

∀t : IBDBook(t) → ∃N: Book(t, N )
∀t, p : LOC(t, p) → ∃I: Book(t, I) ∧ Publisher(I, p)
∀t, id : IBLBook(t, id) → Book(t, id)
∀id, p : IBLPublisher(id, p) → Publisher(id, p)

It can be seen how each source has a slightly different organization wrt the others. In particular, the IDB source contains
data about book titles only; mapping m1 copies titles to the
Book table in the target. The LOC source contains book
titles and publisher names in a single table; these are copied
to the target tables by mapping m2 , which also “invents” a
value to correlate the key and the foreign key. Finally, the
IBL source contains data about books and their publishers
in separate tables; these data are copied to the target by
mappings m3 , m4 ; note that in this case we don’t need to
invent any values. These expressions materialize the target

Figure 2: Instances for the References Scenario
instance in Figure 2, called a canonical universal instance.
While this instance satisfies the tgds, still it contains many
redundant tuples, those with a gray background. Consider
for example the tuple t1 = (The Hobbit, null); it can be
seen that the tuple is redundant since the target contains
another tuple t2 = (The Hobbit, 245) for the same book,
which in addition to the title also gives information about
the publisher. The fact that t1 is redundant with respect to
t2 can be formalized by saying that there is an homomorphism from t1 to t2 . A homomorphism, in this context, is a
mapping of values that transforms t1 into t2 . A similar argument holds for the tuple (The Lord of the Rings, null), and
for tuples (The Catcher in the Rye, I2) and (I2, LB Books),
where I2 is the value invented by executing tgd m2 . The
presence of such homomorphisms means that the solution in
Figure 2 has an endomorphism, i.e., a homomorphism into a

sub-instance – the one obtained by removing all redundant
tuples.
The fact that tgds produced by a schema mapping algorithm may generate redundancy in the target is well known
and has motivated several practical proposals (e.g. [5]) towards the goal of removing such redundant data. Unfortunately, these proposals are applicable only in some cases and
do not represent a general solution to the problem. In [4]
the notion of core solutions has been introduced as a “more
desirable” solution than the one in Figure 2. The core is
the smallest among the solutions for a given source instance
that has homomorphisms into all other solutions. The core
of the solution in Figure 2 is in fact the portion of the target
tables with a white background.
A possible approach to the generation of the core for a relational data exchange problem is to generate the canonical
solution, and then to apply a post-processing algorithm for
core identification. Several polynomial algorithms have been
identified to this end [4, 6]. These algorithms provide a very
general solution to the problem of computing core solutions
for a data exchange setting. Also, an implementation of the
core-computation algorithm in [6] has been developed [9],
thus making a significant step towards the goal of integrating core computations in schema mapping systems. However, experience with these algorithms shows that, although
polynomial, they require very high computing times since
they look for all possible endomorphisms among tuples in
the canonical solution. As a consequence, they hardly scale
to large mapping scenarios. Our goal is to introduce a core
computation algorithm that lends itself to a more efficient
implementation as an executable script and that scales well
to large databases. To this end, in the following sections
we introduce two key ideas: the notion of homomorphism
among formulas and the use of negation to rewrite tgds.
Formula Homomorphisms and Rewriting. The first intuition is that it is possible to analyze the set of formulas in
order to recognize when two tgds may generate redundant
tuples in the target. This happens when it is possible to
find a homomorphism between the right-hand sides of the
two tgds. Consider tgds m1 and m3 above; it can be seen
that the conclusion Book(t, N ) of m1 can be mapped into
the conclusion Book(t, id) of m3 by the following mapping
of variables: t → t, N → id; in this case, we say that m3
subsumes m1 . This gives us a nice necessary condition to
intercept possible redundancy (i.e., possible endomorphisms
among tuples in the canonical solution). Note that the condition is merely a necessary one, since the actual generation
of endomorphisms among facts depends on values coming
from the source. Note also that we are checking for the presence of homomorphisms among formulas, i.e., conclusions of
tgds, and not among instance tuples; since the number of
tgds is typically much smaller than the size of an instance,
this task can be carried out very quickly.
Based on these ideas, in our example we find all possible
homomorphisms among tgd conclusions; more specifically,
we look for variable mappings that transform atoms in the
conclusion of one tgd into atoms belonging to the conclusions of other tgds, with the constraint that universal variables are mapped to universal variables. There are three
homomorphisms of this form: (i) from the right hand side
of m1 to the rhs of m3 , as discussed above; (ii) from the rhs
of m1 to the rhs of m2 by the following mapping: t → t,

N → I, i.e., also m2 subsumes m1 ; (iii) from the rhs of m2
to the union of the conclusions of m3 , m4 , by the following
mapping: t → t, I → id, p → p; in this case we say that
m3 , m4 cover m2 .
A second important intuition is that, whenever we identify
two tgds m, m′ such that m subsumes m′ , we may prevent
the generation of redundant tuples in the target instance by
executing them according to the following strategy: (a) generate target tuples for m, the “more informative” mapping;
(b) for m′ , generate only those tuples that actually add some
new content to the target. In our example, we rewrite the
original tgds as follows (universally quantified variables are
omitted):
m3 . IBLBook(t, id) → Book(t, id)
m4 . IBLPublisher(id, p) → Publisher(id, p)
m′2 . LOC(t, p) ∧ ¬(IBLBook(t, id) ∧ IBLPublisher(id, p))
→ ∃I: Book(t, I) ∧ Publisher(I, p)
m′1 . IBDBook(t) ∧ ¬(IBLBook(t, id)) ∧ ¬(LOC(t, p))
→ ∃N: Book(t, N )
Once we have rewritten the original tgds in this form, we can
easily generate an executable transformation under the form
of relational algebra expressions. Here, negations become
difference operators. The algebraic expressions can be easily
implemented in an executable script, say in SQL or XQuery,
to be run in any database engine. As a consequence, there is
a noticeable gain in efficiency with respect to the algorithms
for core computation proposed in [4, 6, 9].1
Expressive Power. It can be seen that the rewriting al-

gorithm can be applied to any set of tgds, not necessarily
generated by the mapping system. To do this, one of our
goals was to extend the expressive power of the mapping
system with respect to previous ones.
Suppose we are given the following set of pre-defined tgds
that refer to a variant of the self-join example in STBenchmark [1]. The target schema contains a single relation Gene
with attributes name, type and protein, which holds together primary genes and secondary genes, called “synonyms”. A primary gene and its synonyms share the same
protein. In the source, we have genes organized in separate tables PrimaryGene and Synonym, connected through
a key-foreign key constraint. In addition, we have a Protein table, from which we want to copy only tuples about
genes coming from the EMBL database. A key feature of
this example is the self-join of table Gene in the target on
the protein attribute.

Figure 3: Inverse of Self Joins
R corresponds to adding to the data source a new set named
R k , for some k, that is an exact copy of R; (ii) we give users
full control over joins in the two data sources, in addition
to those corresponding to foreign key constraints; using this
feature, users can specify arbitrary join paths, like the selfjoin on the protein attribute in m3 ; (iii) finally, we allow
users to express selection conditions on sets, like source =
’EMBL’ on the protein table in m1 .
This richer set of primitives poses some challenges with respect to the rewriting algorithm. In fact, duplications in the
target correspond to different ways of contributing tuples to
the same set. This makes the search for homomorphisms
more delicate, since there exist tgds that write more than
one tuple at a time in the same target table, and therefore
redundancy can be generated not only across different tgds,
but also by firing a single tgd. Our solution to this problem is to adopt a two-step process. First, we rewrite tgds
that populate the target with duplications. Then, we construct a second exchange, in order to merge the content of
all duplications. We apply the rewriting to this exchange
as well in order to remove redundant tuples. The process is
sketched in Figure 4. Complex scenarios with self-joins will

Figure 4: The Double Exchange
be discussed during the demonstration.

3.

SYSTEM DESCRIPTION

The +Spicy system is an evolution of the original Spicy
system [3]. It has been developed in Java using the NetBeans
Platform as a basis for the graphical user interface. A snapshot is shown in Figure 5. The system architecture is shown
in Figure 6. The system supports various usage scenarios,

m1 . Protein(p, g, ‘EBML’ ) → Gene(g, p, ‘primary’ )
m2 . PrimaryGene(i, n, p) → Gene(n, p, ‘primary’ )
m3 . Synonym(n, i) ∧ PrimaryGene(i, n′ , p)
→ Gene(n, p, ‘synonym’), Gene(n′ , p, ‘primary’ )
Our goal is to generate a mapping scenario for these tgds,
and then rewrite them in order to generate core solutions.
In this case, +Spicy proposes to the user the scenario in
Figure 3. To handle arbitrary tgds of this form, we had to
enrich the set of primitives that can be used to specify a
mapping scenario. We extend these inputs in several ways:
(i) we introduce the possibility of duplicating sets in the
source and in the target; to handle tgd m3 above, we duplicate the Gene table in the target; each duplication of a set
1
We have recently learned that a similar approach has been independently undertaken in [10].

Figure 5: A snapshot of the system

that will be shown during the demonstration. The typical
one is that in which a user provides to the system a mapping
specification using the GUI; in doing this, besides specifying
the source and target schema, users can rely on the primitives offered by the system, namely: (i) a rich set of correspondences that include traditional 1:1 correspondences
but also n:1 value correspondences with complex transformation functions, constant correspondences, and correspondences with filters; (ii) the possibility of duplicating sets
in the two schemas; (iii) the possibility to define arbitrary
join-conditions in the sources; (iv) the possibility of specifying selection conditions on sets in the source. The mapping
specification is handled by the mapping generation module,
which generates the tgds. As an alternative, a simple parser
is available to load a set of pre-defined tgds. The parser
will generate a scenario from the tgds, and show it to the
user so that s/he can visually inspect and possibly modify
it. At this point, the user has a set of tgds, either generated

scale to large instances.
With respect to the quality of solutions, we will discuss
how tgds are rewritten in order to generate the core. To
better evaluate the quality of the core solutions generated
by the tgds after the rewriting, we will compare them to the
canonical universal instance generated by the original tgds.
Note that all algorithms discussed in the previous sections
are applicable to both flat and nested data. However, data
exchange research has so far concentrated on relational data
and there is still no formal definition of a data exchange
setting for nested data. Still, we compare the solutions produced by the system for nested scenarios with the ones generated by the basic [8] and the nested [5] mapping generation
algorithms, that we have reimplemented in our prototype.
We show that the rewriting algorithm invariably produces
smaller solutions, without losing informative content.
One final crucial issue is related to performance. In fact,
computing cores may be a challenging task. The polynomialtime algorithm defined in [6] and implemented in [9] usually
requires several hours, even for instances of a few thousand
tuples. On the contrary, our scripts scale well, as shown
in [7]. To show this, we have prepared source databases of
varying sizes for the selected scenarios, from 100K to 1M
tuples. We will show how in practical cases the computation of the core solution is very efficient and scales well to
such large databases. Finally, synthetic scenarios with a
large number of tables and tgds will be used to show that
the rewriting algorithm performs well when the size of the
scenario increases.

4.
Figure 6: Architecture of Spicy
internally or pre-defined and loaded by the parser. Before
moving to the actual query generation phase, the tgds are
rewritten by the rewriting engine in order to ensure that
core solutions are generated.
Based on these rewritten tgds, an executable query either
in SQL or in XQuery can be generated. The system integrates interfaces to various popular SQL and XQuery engines (like PostgreSQL and Saxon), so that the final query
can be executed against one or more source instances and
results can be inspected using the GUI. To simplify the debugging of the mapping scenario and to reduce dependencies
wrt external systems, +Spicy also incorporates an internal
chase engine to execute the tgds and generate solutions internally. In our experience, this is more immediate than
sending a query to an external engine, and greatly helps
users during their work sessions.
The demonstration will be centered around the discussion
of various mapping scenarios, with the goal of showing the
expressiveness and the quality of solutions produced by the
system. We will demonstrate practical scenarios, scenarios
from the literature, and synthetic scenarios of large size.
In terms of expressiveness, we will show how +Spicy can
handle all of the mapping scenarios proposed in [1]. We will
also discuss how the system handles scenarios of the kind
discussed in [2] by allowing users to explicitly manipulate
join conditions. All scenarios will be run both using the
internal engine and by generating SQL or XQuery scripts for
external engines, to show how the internal engine guarantees
more immediacy but external engines are needed in order to

REFERENCES

[1] B. Alexe, W. Tan, and Y. Velegrakis. Comparing and
Evaluating Mapping Systems with STBenchmark.
Proc. of the VLDB Endowment, 1(2):1468–1471, 2008.
[2] Y. An, A. Borgida, R. Miller, and J. Mylopoulos. A
Semantic Approach to Discovering Schema Mapping
Expressions. In Proc. of ICDE, pages 206–215, 2007.
[3] A. Bonifati, G. Mecca, A. Pappalardo, S. Raunich,
and G. Summa. Schema Mapping Verification: The
Spicy Way. In Proc. of EDBT, pages 85 – 96, 2008.
[4] R. Fagin, P. Kolaitis, and L. Popa. Data Exchange:
Getting to the Core. ACM TODS, 30(1):174–210,
2005.
[5] A. Fuxman, M. A. Hernández, C. T. Howard, R. J.
Miller, P. Papotti, and L. Popa. Nested Mappings:
Schema Mapping Reloaded. In Proc. of VLDB, pages
67–78, 2006.
[6] G. Gottlob and A. Nash. Efficient Core Computation
in Data Exchange. J. of the ACM, 55(2):1–49, 2008.
[7] G. Mecca, P. Papotti, and S. Raunich. Core Schema
Mappings. In Proc. of ACM SIGMOD, 2009.
[8] L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernandez,
and R. Fagin. Translating Web Data. In Proc. of
VLDB, pages 598–609, 2002.
[9] V. Savenkov and R. Pichler. Towards practical
feasibility of core computation in data exchange. In
Proc. of LPAR, pages 62–78, 2008.
[10] B. ten Cate, L. Chiticariu, P. Kolaitis, and W. C. Tan.
Laconic Schema Mappings: Computing Core
Universal Solutions by Means of SQL Queries. In
Proc. of VLDB, 2009.

Data Quality between Promises and Results
Paolo Papotti
Arizona State University
Tempe, AZ, USA
ppapotti@asu.edu
I.

I NTRODUCTION

repairing methods have been proposed for several classes of
constraints, such as (Conditional) Functional Dependencies.
However, these methods rely on ad hoc decisions and tend
to hard-code the strategy to repair conflicting values. As a
consequence, there was no general algorithm to solve database
repairing problems that involve different kinds of constraints
and different strategies to select preferred values. In this work
we developed a uniform framework to solve this problem.
We proposed a new semantics for repairs, and a chase-based
algorithm to compute minimal solutions. We implemented
the framework in a DBMS based prototype and reported
experimental results to show its good scalability and superior
quality in computing repairs.

Improving the quality of data is a crucial task for business,
health, and scientific data. Several data cleaning algorithms
have been translated into tools to identify and repair data errors
such as outlying values, duplicate records, typos, missing
values, and violations of rules in general [1], [2], [3], [4].
Given a dataset, oftentimes different types of errors coexist
in it. This leads to the necessity of executing multiple kinds of
tools in order to achieve the desired quality. Given that every
tool involves a human to define the input, to verify the results,
and to guide the cleaning process, it is clear why obtaining
very high quality data is an expensive process even with the
most advanced systems.

In the second project [3], we focused on the problem of
assisting the users in discovering the rules for data-repairing
systems. In fact, to support domain experts in specifying these
rules, several tools have been proposed to profile the data and
mine rules. However, discovery techniques have traditionally
ignored the time dimension. Recurrent events, such as persons
reported in locations, have a duration in which they are valid,
and this duration should be part of the rules or the cleaning
process would simply fail. In this work, we studied the rule
discovery problem for temporal web data. Such a discovery
process is challenging because of the nature of web data;
extracted facts are (i) sparse over time, (ii) reported with
delays, and (iii) often reported with errors over the values
because of inaccurate sources or non robust extractors. We
handled these challenges with a new discovery approach that
is more robust to noise. Our solution uses machine learning
methods, such as association measures and outlier detection,
for the discovery of the rules, together with an aggressive
repair of the data in the mining step itself. Our experimental
evaluation over real-world data from Recorded Future, an
intelligence company that monitors over 700K Web sources,
shows that temporal rules improve the quality of the data with
an increase of the average precision in the cleaning process
from 0.37 to 0.84, and a 40% relative increase in the average
F-measure.

In a time with an increasing number of research groups and
companies focusing on these problems, it is worth asking some
pragmatic questions over the promises done by data quality
tools, and their results over real-world datasets. The goal is
not to identify which tool is the best in general, as there is no
system that can outperform all the others in every application.
Instead, one goal of the talk is to assess to which extend
existing tools identify data errors that exist in real-world data
sets. Even more importantly, we will see in which situations
the cleaning tools are not expressive enough, and in which
cases the benefit of using a data cleaning system in negligible
w.r.t. ad-hoc solutions with procedural code or scripts. Finally,
open problems and promising directions will be presented.
II.

O UTLINE OF THE TALK

Getting closer to graduation, all Ph.D. students start getting
the same question about the upcoming decision for their future:
“Industry or Academia?”. A professor from a university and
a researcher may work on the same research topic, publish
in the same venues, and even live in the same country, but
their everyday jobs differ in many ways. In this talk, I will use
stories on data quality projects to stress the differences I found
while working in different research environments. While every
place is different and keep changing over time, I will present a
personal perspective to Ph.D. students on what they can expect
when following one path or the other, with the ultimate goal
to help them find the way to make their research successful.

R EFERENCES
[1]

I. F. Ilyas and X. Chu, “Trends in cleaning relational data: Consistency
and deduplication,” Foundations and Trends in Databases, vol. 5, no. 4,
pp. 281–393, 2015.
[2] F. Geerts, G. Mecca, P. Papotti, and D. Santoro, “The LLUNATIC datacleaning framework,” PVLDB, vol. 6, no. 9, 2013.
[3] Z. Abedjan, C. G. Akcora, M. Ouzzani, P. Papotti, and M. Stonebraker,
“Temporal rules discovery for web data cleaning,” PVLDB, vol. 9, no. 4,
pp. 336–347, 2015.
[4] W. Fan and F. Geerts, Foundations of Data Quality Management,
ser. Synthesis Lectures on Data Management. Morgan & Claypool
Publishers, 2012.

In particular, I will present two research projects on data
quality that I have conducted in the last eight years, while
working both in academia (in Europe and North America) and
in two research labs (in North America and Asia).
In the first project [2], we proposed a generalization of the
several methods and approaches used for data repair. Datarepairing consists in making a relational database consistent
with respect to a set of given constraints. In recent years,

978-1-5090-2109-3/16/$31.00 © 2016 IEEE

200

ICDE 2016 Workshops

WWW 2010 • Poster

April 26-30 • Raleigh • NC • USA

Exploiting Information Redundancy to Wring Out
Structured Data from the Web
Lorenzo Blanco, Mirko Bronzi, Valter Crescenzi, Paolo Merialdo, Paolo Papotti
Università degli Studi Roma Tre
Dipartimento di Informatica e Automazione
Via della Vasca Navale, 79 — Rome, Italy

blanco, bronzi, crescenz, merialdo, papotti@dia.uniroma3.it

ABSTRACT

HTML tables, while open information extraction systems [4]
exploit lexical-syntactic patterns. As noticed in [2], even if
a small fraction of the Web is organized according to these
patterns, because of the web scale the amount of the involved
data is impressive.
We introduce an automatic, domain independent technique that exploits an unexplored publishing pattern to extract and integrate data from the Web. We concentrate
on web sources that provide multiple pages about the same
conceptual domain (e.g. financial data, product information, etc.) and expose data with some regularity (pages
are generated from a template). Consider for example the
pages reporting attributes for stock quotes (e.g., volume,
last trade, etc.) in Figure 1. Each page is taken from a different site, and each site contains many other pages about
stock quotes. We can abstract this representation and say
that a web page displays a tuple, and that the whole collection of stock quote pages from that site corresponds to a
“StockQuote” relation. Each site in Figure 1 exposes its own
“StockQuote” relation as well. It is easy to experience that
for many disparate real world domains the number of sites
that follow this publishing strategy is huge.
Our technique to extract and integrate data from these
collections of pages leverages off-the-shelf unsupervised wrapper induction algorithms (e.g. [3]), and an original instancebased data matcher to infer mappings among the data produced by the wrappers. An interesting and original feature
of our approach is the exploitation of the mutual dependency
between the wrapper induction and the data matching tasks:
the results of the latter are used as feedback to validate and
improve the extraction rules generated by the former.
Experiments on three different domains, including about
300 sites for more than 175,000 pages, demonstrate that our
techniques are effective and outperform existing approaches
in the quality of the final solutions.

A large number of web sites publish pages containing structured information about recognizable concepts, but these
data are only partially used by current applications. Although such information is spread across a myriad of sources,
the web scale implies a relevant redundancy. We present a
domain independent system that exploits the redundancy
of information to automatically extract and integrate data
from the Web. Our solution concentrates on sources that
provide structured data about multiple instances from the
same conceptual domain, e.g. financial data, product information. Our proposal is based on an original approach that
exploits the mutual dependency between the data extraction
and the data integration tasks. Experiments confirmed the
quality and the feasibility of the approach.

Categories and Subject Descriptors
H.3.m [Information Storage and Retrieval]: Miscellaneous

General Terms
Algorithms, Experimentation.

Keywords
Data extraction, data integration, wrapper generation.

1.

INTRODUCTION

An increasing number of web sites deliver pages containing structured information about recognizable concepts, relevant to specific application domains, such as stock quotes,
athletes, movies. Consider for example the pages shown
in Figure 1, which contain information about stock quotes.
As current search engines are limited in exploiting the data
offered by these sources, the development of scalable techniques to extract and integrate data from fairly structured
large corpora available on the Web is a challenging issue.
Because of the web scale, these activities should be accomplished automatically by domain independent techniques.
To cope with the complexity and the heterogeneity of web
data, state of the art approaches focus on information organized according to specific patterns that frequently occur
on the Web. For example, [2] focuses on data published in

2.

OVERVIEW OF THE SOLUTION

In our framework, a source is a collection of pages generated by a common template, such that each page publishes
information about one instance of a real world domain of
interest. Pages in Figure 1 belong to three different sources
(Yahoo!, Reuters, Google) for the stock quote domain.
A wrapper is a set of extraction rules that apply over
the pages of a source: each rule extracts a string from the
HTML of the page. The application of a wrapper over a
page returns a tuple, and the application of a wrapper over
a source returns a relation, whose schema has as many attributes as the number of extraction rules of the wrapper.

Copyright is held by the author/owner(s).
WWW 2010, April 26–30, 2010, Raleigh, North Carolina, USA.
ACM 978-1-60558-799-8/10/04.

1063

WWW 2010 • Poster

April 26-30 • Raleigh • NC • USA

Figure 1: Three web pages containing data about stock quotes from Yahoo!, Reuters, Google.
Given a set of sources, our goal is (i) to generate one wrapper for each source, and (ii) to correlate in mappings rules
extracting data about the same conceptual attribute from
different sources.
A natural solution to the problem is a two steps waterfall approach, where a schema matching algorithm is applied
over the relations returned by automatically generated wrappers. However, important issues arise when a large number
of sources is involved, and a high level of automation is required.
Wrapper Inference Problem: as wrappers are automatically generated by an unsupervised process, they can produce imprecise extraction rules (e.g., by extracting irrelevant
information mixed with data of the domain).
Integration Problem: since wrappers are generated automatically, the extracted relations are “opaque”, i.e., their
attributes are not associated with any (reliable) semantic
label. Therefore the matching algorithm must rely on an
instance-based approach, which considers only attribute values to match schemas. However, in this context instancebased matching is challenging because sources provide conflicting values (due to publishing errors and heterogeneous
data representation formats) and imprecise extraction rules
return wrong, and thus inconsistent, data.
Our solution exploits the redundancy of data among the
sources to support both the extraction and the matching
steps. In a bootstrapping phase, an unsupervised wrapper
inference algorithm generates a set of extraction rules for
each source. A domain independent instance-based matching algorithm compares data returned by the generated extraction rules among different sources and infers mappings.
The abundance of redundancy among web sources allows
the system to acquire knowledge about the actual domain
and triggers an evaluation of the mapping. Based on the
quality of the inferred mappings, the matching process provides a feedback to the wrapper generation process, which
is thus driven to refine the bootstrapping wrappers in order
to correct imprecise extraction rules. Better extraction rules
generate better mappings thus improving the quality of the
solution.

3.

eral sources. The overlap is almost total for the stock quotes
because most of the sources publish all the NYSE and NASDAQ stock quotes (each stock quote appears on average in
92.8% sources), while it is more articulated for the soccer
players (1.6%) and videogames (24.5%), since only popular
soccer players and popular videogames are present in a large
number of sources. It is worth observing that often sources
provide complementary information about the overlapping
instances. For example, for soccer players some sources provide weight and height, while others nationality and club.
To give a quantitative evaluation of the results, in Table 1
we report, for the 8 largest output mappings, the recall R of
each mapping, i.e the number of correct extraction rules in
every inferred mapping over the number of sources containing the actual attribute.
Soccer players
45,714 pages
(28,064 players)
Attribute
R
Name
90/100
BirthDate
61/90
Height
54/70
Nationality
48/65
Club
43/79
Position
43/59
Weight
34/58
League
14/44

Videogames
68,900 pages
(25,608 videogames)
Attribute
R
Title
86/100
Publisher
59/91
Developer
45/55
Genre
28/46
EsrbRate
20/40
Rel.Date
9/31
Platform
9/24
#Players
6/14

Stock quotes
59,904 pages
(576 stock quotes)
Attribute
R
Symbol
84/99
$Change
73/89
%Change
73/87
Volume
52/83
DayLow
43/54
DayHigh
41/54
LastPrice
29/50
OpenPrice
24/49

Table 1: Top-8 results for three domains.
For the mappings in Table 1, the system correctly assigned extraction rules to mappings with an average precision equals to 0.99, i.e. on average 99% of the rules were
assigned to the correct mapping. It is interesting to report
that a waterfall execution of a wrapping generation algorithm followed by an instance-based matching process produces mappings with an average precision of 0.65, 0.5 and
0.3 for the three domains of interest, respectively; while the
average recall was from 6% to 11% lower than our solution.

4.

REFERENCES

[1] L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti.
Supporting the automatic construction of entity aware search
engines. In ACM WIDM 2008.
[2] M. J. Cafarella, A. Y. Halevy, D. Z. Wang, E. Wu, and
Y. Zhang. Webtables: exploring the power of tables on the
web. PVLDB, 1(1):538–549, 2008.
[3] V. Crescenzi, G. Mecca, and P. Merialdo. roadRunner:
Towards automatic data extraction from large Web sites. In
VLDB 2001.
[4] O. Etzioni, M. Banko, S. Soderland, and D. S. Weld. Open
information extraction from the web. Commun. ACM,
51(12):68–74, 2008.

EXPERIMENTS

To experiment our system on real world scenarios, we collected data sources from the Web over three application domains: Soccer Players, Videogames and Stock Quotes. For
each domain, 100 sources were gathered automatically by
a crawler specifically tailored to this end [1]. Each source
consists of tens to thousands of pages, and each page contains data about one instance of the corresponding domain.
Within the same domain, many instances are shared by sev-

1064

Nested Mappings: Schema Mapping Reloaded
∗

Ariel Fuxman

University of Toronto
afuxman@cs.toronto.edu

Mauricio A. Hernandez

Howard Ho

IBM Almaden Research Center
mauricio@almaden.ibm.com

IBM Almaden Research Center
ho@almaden.ibm.com

Renee J. Miller

Paolo Papotti*

Lucian Popa

University of Toronto
miller@cs.toronto.edu

Università Roma Tre
papotti@dia.uniroma3.it

IBM Almaden Research Center
lucian@almaden.ibm.com

ABSTRACT

tegration tasks. Declarative schema mapping formalisms have been
used to provide formal semantics for data exchange [9, 1], data integration [14], peer data management [12, 5], and model management
operators [18] such as composition [15, 8, 21] and inversion [7].
We start by examining the most widely used formalisms for schema
mappings. For relational schemas, these are based on source-to-target
tuple-generating dependencies (source-to-target tgds) [9] or, equivalently, GLAV (global-and-local-as-view) assertions [10, 14]. For
schemas containing nested data (including XML schemas), direct extensions have been proposed [24, 27]. We consider the expressiveness
of these mappings to understand what semantics they can, and more
importantly cannot, capture. In addition, we study to what extent
these formalisms meet the goal of providing a natural programming
paradigm for mappings. In particular, we identify several issues that
can lead to inaccurate or underspecified mappings. Furthermore, we
show how existing mapping specifications may be fragmented into
many small, overlapping formulas where the overlap may lead to redundant computation, may hinder human understanding of the mappings and, ultimately, may limit the effectiveness of mapping tools.
To alleviate these issues, we propose a new mapping formalism,
nested mappings, that allows for nesting and correlation of mappings.
Nested mappings permit the expression of powerful grouping and
data merging semantics declaratively within the mapping. We show
that nested mappings yield more accurate specifications, and when
used in data exchange can improve the quality of the exchanged data.

Many problems in information integration rely on specifications, called
schema mappings, that model the relationships between schemas. Schema mappings for both relational and nested data are well-known. In
this work, we present a new formalism for schema mapping that extends these existing formalisms in two significant ways. First, our
nested mappings allow for nesting and correlation of mappings. This
results in a natural programming paradigm that often yields more accurate specifications. In particular, we show that nested mappings
can naturally preserve correlations among data that existing mapping
formalisms cannot. We also show that using nested mappings for purposes of exchanging data from a source to a target will result in less
redundancy in the target data. The second extension to the mapping
formalism is the ability to express, in a declarative way, grouping and
data merging semantics. This semantics can be easily changed and
customized to the integration task at hand. We present a new algorithm for the automatic generation of nested mappings from schema
matchings (that is, simple element-to-element correspondences between schemas). We have implemented this algorithm, along with algorithms for the generation of transformation queries (e.g., XQuery)
based on the nested mapping specification. We show that the generation algorithms scale well to large, highly nested schemas. We also
show that using nested mappings in data exchange can drastically reduce the execution cost of producing a target instance, particularly
over large data sources, and can also dramatically improve the quality of the generated data.

1.1 Current Schema Mapping Formalisms
Source-to-target tgds and GLAV assertions are constraints between
relational schemas. They are expressive enough to represent, in a
declarative way, many of the relational schema mappings of interest.
In this work, we start by examining an extension of source-to-target
tgds designed for schemas with nested data that is based on pathconjunctive constraints [23], and that have been used in systems for
data exchange [24], data integration [27], and schema evolution [26,
28]. We refer to such mappings as basic mappings. They form the
basic building blocks for our subsequent nested mappings.1
To illustrate the use of basic mappings, consider the mapping example shown in Figure 1. The source schema, illustrated on the left,
is a nested schema describing departments with their employees and
projects. There is a top-level set of department records, and each department record has a (nested) set of employee records. There is additional nesting in that each employee has a set of dependents and a
set of projects. Each set can be empty, in general. The target schema,
shown on the right, is a slight variation of the source schema.

1. INTRODUCTION
Many problems in information integration rely on specifications
that model the relationships between schemas. These specifications,
called schema mappings, play a central role in both data integration and in data exchange. We consider schema mappings over pairs
of schemas that express a relation on the sets of instances of two
schemas. The benefits of using declarative formalisms for schema
mappings are well-known. Such formalisms have the promise of providing a high-level, natural programming paradigm for mappings,
and can facilitate customization, evolution, and use in different in∗Work done while at IBM Almaden Research Center

Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its
date appear, and notice is given that copying is by permission of the Very
Large Data Base Endowment. To copy otherwise, or to republish, to post
on servers or to redistribute to lists, requires a fee and/or special permission
from the publisher, ACM.
VLDB ‘06, September 12-15, 2006, Seoul, Korea.
Copyright 2006 VLDB Endowment, ACM 1-59593-385-9/06/09.

1
In the literature these basic mappings have sometimes been referred
to as nested constraints or dependencies since they are constraints on
nested data. However, the mappings themselves have no structure or
nesting. Hence, we will use the term basic to distinguish them from
the more structured nested mappings that we are proposing.

67

Tdept:
Tdept Set of [
dname
location
dept:
dept Set of [ m1
budget
dname
emps:
emps Set of [
location
ename
emps:
emps Set of [
salary
ename
dependents:
dependents Set of [
salary
name
dependents:
dependents Set of [
age
name
]
age
projects:
projects Set of [
]
pid
projects:
projects Set of [
]
pname
]
]
m4
projects:
projects Set of [
]
pid
…
pname
]
…
m1: “for every department element, map department info”:
m2

m3

for d in dept ⇒
exists d’ in Tdept [ … (mapping conditions from m )
∧ for e in d.emps ⇒
exists e’ in d’.emps [ … (rest of m -- not covered by m )
∧ for c in e.dependents ⇒
Correlation with
exists c’ in e’.dependents
parent mapping
[ … (rest of m) ]
∧ for p in e.projects ⇒
exists p’ in e’.projects, p’’ in d’.projects
[ … (rest of m) ]
]
]

m4

Figure 2: Nested mapping.

time inefficiency, this puts additional burden on methods for duplicate
elimination or data merging. For the above example, an employee
may be generated three times in the target: once for m2 (with an
empty set of dependents and an empty set of projects), once for m3
(with a non-empty set of dependents) and once for m4 (with a nonempty set of projects). Merging of the three employee records into
one is more than just duplicate elimination: it requires merging of
two nested sets as well. Furthermore, this raises the question of when
to merge in general (since this is not expressed in any way by the
mapping formulas of Figure 1). This brings us to the next point.
Underspecified grouping semantics The formula m2 requires that
for every department and for every employee record in the source
there must exist, in the target, a “copy” of the department record with
a “copy” of the employee record nested underneath. However, it is left
unspecified whether to group multiple employees that are common
for a given department name (dname), or whether to group by other
fields, or whether not to group at all. Again, one of the reasons for
this lack of expressive power is the simplicity of these basic mapping
formulas. In an early version of Clio [24], a default grouping behavior
is used based on partitioned normal form (PNF) which always groups
nested sets of elements by all the atomic elements at the upper levels.
For example, under the PNF semantics, employees will be grouped
by dname and location (assuming that budget is not mapped
and its value is null). In effect, the semantics of the transformation
is specified in two parts: first the mapping formulas, and then the
implicit (PNF-based) grouping semantics. An important limitation of
this approach is that the default grouping semantics is not specified
declaratively, and it cannot be easily changed or customized when it
is not the desired semantics.

∧

m2: “for every department element with employees, map department and
employee info”:
m3: “for every department element with employees with dependents, …”
m4: “for every department element with employees with projects, …”

Figure 1: Multiple “small” mappings.

The formulas that are sketched below the schemas are examples of
basic mappings. They are constraints that describe, in a declarative
way, the mapping requirements. These formulas may be generated
by a tool such as Clio [24] from the lines (or, correspondences) between schema elements, or may be written by a human expert and
interpreted by a model management tool such as Moda [18] or other
integration tools such as Piazza [12]. (We will give a precise semantics for the schema and basic mapping notation in Section 2. The
exact details are not essential for this introductory discussion.)
Each formula (that is, each mi ) deals with one possible “case” in
the source data (where each case is expressed by a conjunction of
navigation paths joined in certain ways). In order to cover all possible
cases of interest, we need many such formulas. However, many of the
cases overlap (i.e., have common navigation paths). Hence, common
mapping behavior must be repeated in many formulas.
For example, the formula m2 must repeat the mapping behavior
that m1 already specifies for department data (although m2 does it in
a more specialized context). Otherwise, if we specify in m2 only the
mapping behavior for employees, we lose in the target the association
that exists in the source between employees and their departments
(since there is no correlation between m1 and m2 ). At the same time,
m1 cannot be eliminated from the specification, since it deals with departments in general (that are not required to have employees). Also,
in the example, m3 and m4 contain a common mapping behavior for
employees and departments (but they differ in that they map different
components of employees: dependents and projects).
Such formulas are (relatively) easy to generate and reason about.
This is, partly, why they have been widely used in research. However,
the number of formulas quickly increases with large schemas, leading
to an explosion in the size of the specification. This explosion as well
as the overlap in behavior causes significant usability problems for
human experts and for tools using these specifications in practice.
Inefficiency in execution In a naive use of basic mappings, each
mapping formula may be interpreted separately. Optimization of these
mappings requires sophisticated techniques that deduce the correlations and common subexpressions within the mappings.
Redundancy in the specification When using basic mappings in data
exchange, the same piece of data may be generated multiple times in
the target due to the multiple formulas. In addition to possible run-

1.2 Nested Mappings
In order to address the above issues, we propose an extension to
basic mappings that is based on arbitrary nesting of mapping formulas within other mapping formulas. We shall call this formalism the
language of nested mappings. As a first observation, it can be argued
that nested mappings offer a more natural programming paradigm for
mapping tasks, since human users tend to design a mapping from
top to bottom, component-wise: define first how the top components
of a schema relate, then define, recursively, via nested submappings,
how the subcomponents relate, and so on. For our earlier example,
the corresponding nested mapping is illustrated in Figure 2. The
nested mapping relates, at the top-level, source departments with target departments; it then continues, in this context of a department-todepartment mapping, with a submapping relating the corresponding
employees, which then continues with submappings for dependents
and projects. At each level, there are correlations between the current
submapping and the upper-level mappings. In particular, nothing is
repeated from the upper level, but instead reused.
Advantages of nested mappings Nested mappings overcome (to a
large extent) the previous shortcomings of basic mappings. First, we
need fewer formulas and overall produce a more natural and accurate
specification. For our example, one nested mapping replaces four ba-

68

sic mappings. In general, we may still need multiple nested mappings
(one common situation is when we have multiple data sources). Second, by using nested mappings, we are able to produce more efficient
data exchange queries. This is because nested mappings factor out
common subexpressions, so we can more easily optimize the number of passes over the same input data. For our example, department
records can be scanned only once, and the entire work involving the
subelements can be done in the same pass (by the submappings). The
execution will also generate much less redundancy in the target data.
An employee is generated once, and all dependents and projects are
added together (by the two corresponding submappings).
Nested mappings also have a natural, built-in, grouping behavior,
that follows the grouping of data in the source. For example, the
above nested mapping requires that all the employees in the target are
grouped in the same way as they are in the source. This grouping
behavior is ideal for mappings between two similar schemas (which
is common in the important case of schema evolution) where much
of the data should be mapped using the identity (or mostly-identity)
mapping. For more complex restructuring tasks, additional grouping behavior may need to be specified. We use a simple, but powerful, mechanism for adding such grouping behavior by using explicit
grouping functions (a restricted form of Skolem functions).
Summary of Contributions
• We propose a nested mapping formalism for representing the relationship between schemas for relational or nested data (Section 2).
• We propose an algorithm for generating nested mappings from matchings, or correspondences, between schema elements. The nested nature of the mappings makes this generation task more challenging
than in the case of basic mappings (Section 3).
• We give an algorithm for the generation of data transformation
queries that implement data exchange based on nested mapping specifications. Notably our algorithm can handle all nested mappings, including those generated by our mapping algorithm as well as arbitrary
customizations of these mappings, which may be made, for example,
by a user to capture specialized grouping semantics (Section 4).
• We show experimentally that the use of nested mappings in data exchange can drastically reduce the execution cost of producing a target
instance, and can also dramatically improve the quality of the generated data. We show examples of important grouping semantics that
cannot be captured by basic mappings, and we empirically show that
underspecified basic mappings may lead to significant redundancy in
data exchange (Section 5).
Related Work Schema mappings are so important in information integration that many mapping formalisms have been proposed for different tasks. Here we mention only a few. The important role of
Skolem functions for merging data has been recognized in a number of approaches [13, 22] and Skolem functions appear explicitly
as part of the XML-QL query language [6]. Work on model management has used embedded dependencies (similar to our basic mappings) which may be augmented with Skolem functions [3, 18]. HePToX [5] uses a datalog-like language that supports nested data and allows Skolem functions, but mappings cannot be nested or correlated.
Grammars have been used to specify mappings for (recursive) DTDs
[2]. While most of these formalisms support nested data, to the best
of our knowledge, none of the existing declarative formalisms support
the expression of nesting between mappings.
Our nested mappings are strictly more expressive than basic mappings. At the same time, they are less expressive than languages used
for composition [8, 21]. In particular, if we restrict ourselves to the
relational model (for comparison purposes), nested mappings are a
strict sublanguage of the second-order tgds (SO tgds) introduced in
[8]: every nested mapping can be rewritten, via Skolemization, into
an equivalent SO tgd (but not vice-versa). However, this rewriting
would erase the nesting structure of the mapping, and it is not clear

d

proj:
proj Set of [
dname
pname
emps:
emps Set of [
ename
salary
]
]

m

dept:
dept Set of [
dname
budget
emps:
emps Set of [
ename
salary
e
projects:
projects Set of [
s
pid
]
p
]
projects:
projects Set of [
pid
pname
]
]

for p in proj ⇒
exists d’ in dept, p’ in d’.projects
where d’.dname=p.dname ∧ p’.pname=p.pname

m  for p in proj, e in p.emps ⇒
exists d’ in dept, p’ in d’.projects, e’ in d’.emps, p’’ in e’.projects
where p’’.pid=p’.pid ∧
d’.dname=p.dname ∧ p’.pname=p.pname ∧
e’.ename=e.ename ∧ e’.salary=e.salary

Figure 3: A mapping scenario with two basic mappings.

to what extent such nesting could be extracted from an arbitrary SO
tgd. The language of SO tgds does not allow nesting of formulas,
but instead allows correlation of formulas via arbitrary Skolem functions, a more powerful but arguably less user-friendly programming
concept. We also note that there is no known algorithm for generating SO tgds; the algorithm for generating nested mappings that we
propose here can, however, be seen as a step in that direction, since
nested mappings correspond to a form of SO tgds.
Many industry tools such as BizTalk Mapper, IBM WebSphere
Data Stage TX, and Stylus Studio’s XML Mapper support the development (by a programmer) of mappings. Some support nested mappings, though in more procedural languages. However, most, if not
all, of the work done to express such mappings is manual. Generation
of mappings (with no nesting) has been considered in the TranSem
system [20], Clio [19, 24] and HePToX [5]. Also, Bohannon et al. [4]
consider the generation of information preserving mappings (based
on path mappings). Our work extends the Clio mapping generation
algorithm to produce nested mappings.
As part of our generation algorithm, we identify common expressions within mappings. Our goal is to identify possible correlations
between mappings that can be exploited to produce more accurate
mapping specifications. Our techniques are in the same spirit of work
on identifying common expressions within complex queries for use in
query optimization [25]. However, unlike query optimization which
must necessarily preserve query equivalence, our techniques lead to
mappings with better semantics, and so do not preserve equivalence.
Notably the generation of efficient queries for data exchange is not
considered in work like Piazza [12] and HePToX [5] which instead
focus on query generation for data integration. In model management [18, 3], query or code generation for data exchange has been
considered for embedded dependencies. Clio [24] generates XQuery,
XSLT, SQL/XML, and SQL queries for basic mappings.

2. MAPPINGS WITHIN MAPPINGS
In this section, we fix the notation and terminology for schemas and
mappings based on our previous work [24, 28]. Furthermore, we take
a closer look at the qualitative differences between basic mappings
and nested mappings.

2.1 Basic Mappings
Consider the mapping scenario illustrated in Figure 3. The two
schemas in the figure (source and target) are shown in a nested relational representation that can be used as a common abstraction for

69

relational and XML schemas (and other hierarchical set-oriented data
formats). This representation is based on sets and records that can
be arbitrarily nested. In the source schema, proj is a set of records
with two atomic components, dname (department name) and pname
(project name), and a set-valued component, emps, that represents a
(nested) set of employee records. The target schema is a reorganization of the source: at the top-level we have a set of department
records, with two nested sets of employee and project records. Moreover, each employee can have its own set of project ids (pids), which
must appear at the department level (this is required by the foreign
key shown in the figure with an arrow).
Formally, a schema is a set of labels (also called roots), each with
an associated type τ , defined by: τ ::= Str | Int | SetOf τ | [ l1 : τ1 ,. . .,
ln : τn ], where l1 , . . . , ln are labels.2 We point out that this is only a
simplified abstraction: in the system that we implemented, we also
deal with choice types, optional elements, nullable elements, etc.
However, the presence of these additional features does not essentially change the formalism.
In Figure 3, we also show two basic mappings that can be used to
describe the relationship between the source and the target schemas.
The first one, m1 , is a constraint that maps department and project
names in the source (independently of whether there exist any employees in emps) to corresponding elements in the target. The second one, m2 , is a constraint that maps department and project names
and their employees (whenever such employees exist).
In the figure, we use a “query-like” notation, with variables bound
to set-type elements. Each variable can be a record and hence contain multiple components. Correspondences between schema elements (e.g., dname to dname) are captured by equalities between
such components (e.g., d0 .dname = p.dname). These equalities are
grouped in the where clause that follows the exists clause of a mapping. Moreover, equalities can also be used to express join conditions
(or other predicates) in the source or in the target. For example, see
the requirement on pid in m2 that appears in the same where clause.
Logic-based notation Alternatively, we will use a “logic-based” notation for mappings that quantifies each individual component in a
record as a variable. In particular, nested sets are explicitly identified
by variables. Each mapping is an implication between a set of atomic
formulas over the source schema and a set of atomic formulas over
the target schema. Each atomic formula is of the form e(x1 , . . . , xn )
where e denotes a set, and x1 , . . . , xn are variables.3 The main difference from the traditional relational atomic formulas is that e may be
a top-level set (e.g., proj), or it may be a variable (in order to denote
sets that are nested inside other sets). We will write the atomic variables in lower-case and the set variables in upper-case. The formulas
corresponding to the mappings m1 and m2 of Figure 3 are:

Target data:

Source data:
proj:
proj

CS uSearch E

E:

Alice 120K
John 90K

dept:
dept
{m ,m }

CS B E P 

dept:
dept

CS B EP 
CS B E P

E:

Alice 120K P ’

E:

John 90K P’

P :
X uSearch

P:
X uSearch

P:
X uSearch

P’:
X

P ’:
X

Figure 4: Source and target instances satisfying {m1 , m2 }.

tuple (d, p, Es ) in proj, and for every tuple (e, s) in the set Es , there
must exist four tuples in the target as follows. First, we must have a
tuple (d, b, E, P ) in dept, where b is some “unknown” budget, E
identifies a set of employee records, and P identifies a set of project
records. Then, there must exist a tuple (e, s, P 0 ) in E, where P 0 identifies a set of project ids. Furthermore, there must exist a tuple (x) in
P 0 , where x is an “unknown” project id. Finally, there must exist a
tuple (x, p) in the previously mentioned set P , where x is the same
project id used in P 0 . Notice that all data required to be in the target
by the mapping satisfies the foreign key for the projects.

2.2 Correlating Mappings via Nesting
We now take a look at actual data in order to understand the semantics of basic mappings, and to see why such specification language is
not entirely satisfactory. In Figure 4, we show source and target instances that satisfy the constraints m1 and m2 . In the source, E0
is a “name”, or set id, for the nested set of employee records corresponding to the tuple given in proj. We assume that every nested set
has such an id. Similarly, E1 , P1 , E2 , . . . , P30 are set ids in the target
instance. The top two target tuples, for dept and P1 , respectively,
ensure that m1 is satisfied; the rest are used to satisfy m2 .
In general, for a given source instance, there may be several target
instances satisfying the constraints imposed by the mapping specification. Given the specification {m1 , m2 }, the target instance shown
in Figure 4 can be considered to be the most general that can be produced (a universal solution [9]), because it is the one that makes the
least assumptions. For example, it does not assume that E1 and E2
are equal (since this is not required by the specification). However,
this target instance may not be satisfactory for a number of reasons.
First, there is redundancy in the output: there are three dept tuples
generated for “CS”, for different instantiations of the left-hand sides
of m1 and m2 . Also, there are three project tuples for “uSearch” (although in different sets). Second, there is no grouping of data in the
target: E2 and E3 are different singleton sets, generated for different
instantiations of the left-hand side of m2 (same for P2 and P3 ). This
does not violate the constraints, however, since the mapping specification does not require E2 and E3 to be equal.
In Figure 5, we show a target instance that is more “desirable”.
This instance has no redundant departments or projects, and it maintains the grouping of employees that exists in the source. While this
instance satisfies the constraints m1 and m2 , for the given source
instance, it is not required by these mappings. In particular, the specification given by {m1 , m2 } does not rule out the undesired target
instance of Figure 4.
We would like to have a specification that “enforces” correlations
such as the ones that appear in the more “desirable” target instance
(e.g., that the two source employees appear in the same set in the
target). In particular, we would like to correlate the mapping m2
with m1 so that it reuses the set id E for employees that is already
asserted by m1 (along with other existentially-quantified elements in

m1 : proj(d, p, Es ) → dept(d, ?b, ?E, ?P ) ∧ P (?x, p)
m2 : proj(d, p, Es ) ∧ Es (e, s)
→ dept(d, ?b, ?E, ?P ) ∧ E(e, s, ?P 0 ) ∧ P 0 (?x) ∧ P (x, p)
For each formula, the variables on the left of the implication are assumed to be universally quantified. The variables on the right that do
not appear on the left are assumed to be existentially quantified. For
clarity, we omit the quantifiers and use a question mark in front of the
first occurrence of an existentially-quantified variable.
To illustrate, in m2 , the variable Es denotes the nested set of employee records (inside a tuple in the top-level set proj). The variables E, P , and P 0 are also set variables, but existentially quantified.
The variables b (for budget) and x (project id) are existentially quantified as well (but atomic). The meaning of m2 is: for every source
2

In Figure 3, we do not show any of the atomic types.
For simplicity of presentation, we assume strict alternation of set
and record types in a schema.
3

70

dept:
dept

CS B E P

P :

X uSearch

E :

Alice 120K P ’
John 90K P ’

P ’:

data is flat and, consequently, the target data is flat (as far as the relationship between departments and projects goes). Furthermore, the
above nested mapping does not merge sets of employees that appear
in different source tuples with the same department name, in contrast
with the target instance shown in Figure 6.
Suppose now that we do want to group into one set all the projects
of a department, and also all the projects for each employee in a department. Also, we want to merge all the employees for a given department. To generate such new groupings of data, we need to add
something else to the specification, since nesting of mappings alone is
not flexible enough to describe such groupings. The mechanism that
we add is that of Skolem functions for set elements. Intuitively, such
functions can express that certain sets in the target must be functions
of certain values from the source. For our example, to express the
desired grouping, we enrich the nested mapping with three Skolem
functions for the three nested set types in the target, as follows:
n0 : for p in proj ⇒
exists d0 in dept, p0 in d0 .projects
where d0 .dname=p.dname ∧ p0 .pname=p.pname ∧
d0 .emps=E[p.dname] ∧ d0 .projects=P[p.dname] ∧
( for e in p.emps ⇒
exists e0 in d0 .emps, p00 in e0 .projects
where p00 .pid=p0 .pid ∧
e0 .ename=e.ename ∧ e0 .salary=e.salary ∧
e0 .projects=P’[p.dname,e.ename] )
The new mapping constrains the target set of projects to be a function of only department name: P[p.dname]. Also, there must be only
one set of employees per department name, E[p.dname], meaning that
multiple sets of employees (for different source tuples with the same
department name) must be merged into one. Similarly, all projects of
an employee in a department must be merged into one set.
More concretely, for the source tuple proj(CS, uSearch, E0 ) of
Figure 6, the outer mapping of n0 requires that the target contains
dept(CS, B1 , E1 , P ). In addition, E[“CS”] (the result of applying
the Skolem function E to the value “CS”) corresponds to E1 . Due
to the inner mapping, the two employees of E0 (“Alice” and “John”)
must be in E1 . Now consider the source tuple (CS, iM ap, E00 ). The
mapping n0 requires the employees working on the “iMap” project
(Bob and Alice) to also be within the set E1 . The reason for this
is that, according to n0 , the employees of “iMap” must also be in
E[“CS”], which is E1 .
Due to lack of space, we omit the precise definition of nested mappings, which is straightforward and follows the examples and intuition given above. We do point out the following natural restriction.
The for clause of a submapping can use a correlation variable (i.e.,
bound in an upper-level mapping) only if that variable is bound in a
for clause of the upper-level mapping. (A similar restriction holds for
the usage of correlation variables in exists clauses.)
Every nested mapping (with no explicit Skolem functions) is equivalent to one in which default Skolem functions are assigned to all the
existentially-quantified set variables (using here the logic-based notation). The default arguments to such Skolem functions are all the
universally quantified variables that appear before the set variable.
As an example, the previous nested mapping n is equivalent to
one in which the target set of projects nested under each dept tuple
is determined by a Skolem function of all three components of the
input proj tuple (i.e, dname, pname, and emps). In other words,
there must be a set of target projects for each input proj tuple. Of
course, this does not require any grouping of projects by departments.
However, once we expose them to a user, the Skolem functions can
be customized, in order to achieve different grouping behavior (such
as the one seen with the earlier mapping n0 ). This is the approach that
we follow in our system: we first generate nested mappings (with no
Skolem functions), then apply default Skolemization, which can then
be altered in a GUI by a user.

X

P’:
X

Figure 5: Target data required by the nested mapping n.
m1 ), without repeating the common part, which is m1 itself. This can
be done using the following nested mapping:
n : proj(d, p, Es ) →
[ dept(d, ?b, ?E, ?P ) ∧ P (?x, p)
∧ [ Es (e, s) → E(e, s, ?P 0 ) ∧ P 0 (x) ] ]
The inner implication in n (the third line) is a submapping. We
refer to the rest of n as the outer mapping. The submapping is correlated to the outer mapping because it reuses the existential variables
E and x. In particular, the submapping requires that for every employee tuple in the set Es (where Es is bound by the outer mapping),
there must exist an employee tuple in the set E, which is also bound
by the outer mapping. Also, there must exist a project tuple in the set
P 0 associated to this employee, and the project id must be precisely
the one (x) already required by the outer mapping. Note that P 0 is
now existentially quantified and bound in the inner mapping.
A fundamental observation about the nested mapping n is that
the “undesirable” target instance of Figure 4 does not satisfy its requirements. For example, when we apply the outer mapping of n to
proj(CS, uSearch, E0 ), we require dept(CS, B1 , E1 , P1 ) to be in
the target. Now, when we apply the submapping to E0 (Alice, 120K)
and E0 (John, 90K), we must have tuples for Alice and John within
the same set E1 . The nested mapping explicitly rules out the target instance of Figure 4, and is a tighter specification for the desired schema
mapping.
Another important observation is that there is no set of basic mappings that is equivalent to the above nested mapping. (It is not hard to
show this and we leave the details for a larger version of this paper.)
Thus, the language of nested mappings is strictly more expressive
than that of basic mappings.
Finally, we show below the nested mapping in query-like notation.
Notice that the variables p, d0 and p0 from the outer level are being
reused in the inner level.
n: for p in proj ⇒
exists d0 in dept, p0 in d0 .projects
where d0 .dname=p.dname ∧ p0 .pname=p.pname ∧
( for e in p.emps ⇒
exists e0 in d0 .emps, p00 in e0 .projects
where p00 .pid=p0 .pid ∧
e0 .ename=e.ename ∧ e0 .salary=e.salary )

2.3 Grouping and Skolem Functions
As seen in the above example, nested mappings can take advantage
of the grouping that exists in the source, and require the target data
to have a similar grouping. In the example, all the employees that
are nested inside one source tuple are required to be nested inside the
corresponding target tuple. In this section, we show how a restricted
form of Skolem functions can be used to model groupings of data that
may not be present in the source.
To illustrate, consider again the source schema in Figure 3. In Figure 6, we show source and target data for this schema. On the left, we
show a source instance that extends the one of Figure 4. In particular,
the “CS” department is associated with two different projects instead
of one. On the right, we show a desired target instance, where projects
are grouped by department name. This target instance is not required
by the nested mapping n, which allows target instances where we may
have multiple department tuples (with the same dname value), each
with a singleton set containing one project. In other words, the source

71

P (=P[“
(=P[“CS”
CS”]):

E =E[“
=E[“CS”
CS”]):



Alice 120K P ’
John 90K P ’
Bob 75K P’

X uSearch
X iMap

B 
 = { d in dept ; }
B  = { d in dept, e in d.emps ; }
B  = { d in dept, e in d.emps, p in e.projects,
p’ in d.projects ; p.pid=p’.pid }
B
 = { d in dept, p in d.projects ; }

A 
 = { p in proj ; }
A  = { p in proj, e in p.emps ; }

P ’:

X

(a)

P	’:

A

X

Figure 6: Example data for the nested mapping n0 .

B

A
(b)

Skolem functions and data merging Our example illustrates how
one occurrence of a Skolem function permits data to be accumulated
into the same set. Furthermore, the same Skolem function may be
used in multiple places of a mapping or even across multiple mappings. Thus, different mappings (correlated via Skolem functions)
may contribute to the same target sets, effectively achieving data merging. This is a typical requirement in data integration. Hence, Skolem
functions are a declarative representation of a powerful array of data
merging semantics.
As an interesting example of a set being shared from multiple places
consider the case when “Alice” has different salaries (120K and 130K)
in the two tuples in the source of Figure 6. Then our mapping n0 requires that there be two different “Alice” tuples in the target (both
in the set E1 = E[“CS”]). Moreover, the same set of projects will
be constructed for the two Alice tuples since the (projects) set id is
a Skolem function (P0 ) of “CS” and “Alice” (and does not take into
account salary). This showcases an interesting feature of the mapping language, which is the ability to merge several components of a
piece of data while still keeping other components separated (perhaps
until further resolution).

B

⇒

Bob 75K
Alice 120K

CS B E P 

X
X

⇒

Alice 120K
John 90K

E’:

{n’}

dept:
dept

⇒
⇒

CS uSearch E
CS iMap
E’

E:

P1’(=P’
(=P’ [“CS”
CS”,”Alice”
Alice”]):

Target data:

proj:
proj

⇒

Source data:

B

B

Figure 7: (a) Source and target tableaux (b) Tableaux hierarchies

earlier schemas in Figure 3. In Figure 7(a), A1 and A2 are primary
paths corresponding to the two set types associated with proj and
emps in the source schema. Note that in A2 , the parent set proj
is also included, since it is needed in order to refer to an instance of
emps. Similarly, B1 , B2 , and B4 are primary paths in the target.
In addition to the structural constraints (parent-child) that are part
of the primary paths, the computation of tableaux also takes into account the integrity constraints that may exist in schemas. For our
example, the target schema includes the following constraint (similar to a keyref in an XML Schema): every project id of an employee
within a department must appear as the id of a project listed under the
department. This constraint is explicitly enforced in the tableau B3
in Figure 7(a). The tableau is constructed by enhancing, via the chase
[16, 23] with constraints, the primary path B30 that corresponds to the
set type projects under emps:
B30 = { d in dept, e in d.emps, p in e.projects; }

3. GENERATION OF NESTED MAPPINGS

The tableau B3 encodes, intuitively, that the concept of a projectof-an-employee-of-a-department requires the following concepts to
exist: the concept of an employee-of-a-department, the concept of a
department, and the concept of a project-of-a-department.
For each schema, the set of its tableaux is obtained by replacing
each primary path with the result of its chase (with all the applicable integrity constraints). For our example, only one primary path is
changed by the chase (into B3 ). The rest remain unchanged (since no
constraints are applicable). For each tableau, for mapping purposes,
we will consider all the atomic type elements that can be referred to
from the variables in the tableau. For example, B3 includes dname,
budget, ename, salary, pid,4 pname. We say that such elements are covered by the tableau. Let us call generators the variable
bindings that appear in a tableau. Thus, a tableau consists of a sequence of generators and a conjunction of conditions.
Step 2. Generation of basic mappings In the second step of the algorithm, basic mappings are generated by pairing in all possible ways
the source and the target tableaux that were generated in the first step.
For each pair (A, B) of tableaux, let V be the set of all correspondences for which the source element is covered by A and for which
the target element is covered by B. For our example, if we consider
the pair (A1 , B1 ) then V consists of one correspondence: dname to
dname, identified by d in the earlier Figure 3. If we consider the pair
(A1 , B4 ) then there is one more correspondence covered: pname to
pname (or p).
Every triple (A, B, V ) encodes a possible basic mapping: the for
and the associated where clause are given by the generators and the
conditions in A, the exists clause is given by the generators in B,
and the subsequent where clause includes all the conditions in B
along with conditions that encode the correspondences (i.e., for every
v in V , there is an equality between the source element of v and

In this section, we describe our algorithm for the generation of
nested mappings. Given two schemas, a source and a target, and a
set of correspondences between atomic elements in the schemas, the
algorithm generates a set of nested mappings that “best” reflect the
given schemas and correspondences. The first two steps in the algorithm (Section 3.1) follow the generation of basic mappings that we
introduced in our previous work [24]. We then describe (Section 3.2)
an additional step in which unlikely basic mappings are pruned. This
significantly reduces the number of basic mappings. We define when
a basic mapping can be nested under another basic mapping in Section 3.3. The pruned basic mappings are then input to the final step in
the algorithm to generate nested mappings (Section 3.4).

3.1 Basic Mapping Generation
We now review the generation algorithm for basic mappings [24].
The main concept is that of a tableau. Intuitively, tableaux are a way
of describing all the “basic” concepts and relationships that exist in
a schema. There is a set of tableaux for the source schema and a set
of tableaux for the target schema. Each tableau is primarily an encoding of one concept of a schema (here, concept is synonymous to a
set type). In addition, each tableau includes all related concepts, that
is, concepts that must exist together according to the referential constraints of the schema or the parent-child relationships in the schema.
This will allow the subsequent generation of mappings that preserve
the basic relationships between concepts. Such preservation is one of
the main properties of our previous algorithm [24], and will continue
to apply for the new algorithm as well.
Step 1. Computation of tableaux Given the two schemas, the sets of
tableaux are generated as follows. For each set type T in a schema, we
first create a primary path that spells out the navigation path from the
root to elements of T . For each intermediate set, there is a variable
to denote elements of the intermediate set. To illustrate, recall the

4

72

We include only one pid, since p.pid is equal to p0 .pid.

ing all the subsumed and implied mappings.5 For our example, we are
left with only the two basic mappings, m1 and m2 , from Figure 3.

the target element of v). We may write the basic mapping represented by (A, B, V ) as ∀A → ∃B.V , with the meaning described
above. For our example, the basic mapping ∀A1 → ∃B4 .{d, p}
is precisely the mapping m1 of Figure 3. Also, the basic mapping
∀A2 → ∃B3 .{d, p, e, s} is the mapping m2 of the same figure.
Among all the possible triples (A, B, V ), not all of them generate
actual mappings. We generate a basic mapping only if it is not subsumed and not implied by other basic mappings. This optimization
procedure is described in the next subsection.

3.3 When Can We Nest?
We now give a formal definition of the notion of a basic mapping
being nestable under another basic mapping. This definition follows
the intuition given in Section 2.2: we nest m2 inside m1 if m1 is
“part” of m2 ; morever the nesting is done by factoring out the common part (m1 ) and adding the “remainder” of m2 as a submapping.
Based on this definition, we will construct a graph (hierarchy) of basic
mappings that will be used by the actual generation algorithm, which
is described in Section 3.4.

3.2 Subtableaux and Optimization
The following concept of subtableau plays an important role in reasoning about basic mappings, and in particular in pruning out unlikely
mappings during generation (see the following Step 3). The same
concept also turns out to be very useful in the subsequent generation
of nested mappings.
D EFINITION 3.1. A tableau A is a subtableau of a tableau A0 (notation A ≤ A0 ) if the generators in A form a superset of the generators
in A0 (possibly after some renaming of variables) and also the conditions in A are a superset of those in A0 or they imply them (modulo
the renaming of variables). We say that A is a strict subtableau of A0
(A < A0 ) if A ≤ A0 and the generators in A form a strict superset of
those in A0 .
For each schema, the subtableau relationship induces a directed acyclic
graph of tableaux, with an edge from A to A0 whenever A ≤ A0 .
Such a graph can be seen as a hierarchy where the tableaux that are
smaller in size are at the top. Intuitively, the tableaux at the top correspond to the more general concepts in the schema, while those at
the bottom correspond to the more specific ones. Although the subtableau relationship is reflexive and transitive, most of the time we are
concerned with the “direct” subtableau edges. For our example, the
two hierarchies (with no transitive edges) are shown in Figure 7(b).
Step 3. Pruning of basic mappings We now complete the algorithm
for generation of basic mappings with an additional step that prunes
unlikely mappings. This step is especially important because it reduces the number of candidate mappings that the nesting algorithm
will have to explore.
A basic mapping ∀A → ∃B.V is subsumed [11] by a basic mapping ∀A0 → ∃B 0 .V 0 if A and B are respective subtableaux of A0 and
B 0 , with at least one being strict, and V = V 0 . Note that if A and B
are respective subtableaux of A0 and B 0 , then necessarily V includes
V 0 (since A and B cover all the atomic elements that are covered
by A0 and B 0 , and possibly more). The subsumption condition says
that we should not consider (A, B, V ) since it covers the same set of
correspondences that are covered by the smaller (and more general)
tableaux A0 and B 0 . For our example, ∀A1 → ∃B2 .{d} is subsumed
by ∀A1 → ∃B1 .{d}.
A basic mapping may be logically implied by another basic mapping. Testing logical implication of basic mappings can be done using
the chase [16, 23], since basic mappings are tuple-generating dependencies (albeit extended over a hierarchical model). Although in our
implementation we use the chase (for completeness), often a simpler
test suffices: a basic mapping m is implied by a basic mapping m0
whenever m is of the form ∀A → ∃B.V and m0 is of the form
∀A → ∃B 0 .V 0 and B 0 is a subtableau of B. Intuitively, all the
target components (with their equalities) that are asserted by m are
asserted by m0 as well (with the same equalities). As an example,
∀A1 → ∃B1 .{d} is implied by ∀A1 → ∃B4 .{d, p}.
We note that subsumption also eliminates some of the implied mappings. In the earlier definition of subsumption, in the particular case
when B and B 0 are the same tableaux then the subsumed mapping is
also implied (by the other one). For example, ∀A2 → ∃B1 .{d} is
subsumed and implied by ∀A1 → ∃B1 .{d}.
The generation algorithm for basic mappings stops after eliminat-

D EFINITION 3.2. A basic mapping ∀A2 → ∃B2 .V2 is nestable
inside a basic mapping ∀A1 → ∃B1 .V1 if the following hold:
(1) A2 and B2 are strict subtableaux of A1 and B1 , respectively,
(2) V2 is a strict superset of V1 , and
(3) there is no correspondence v in V2 − V1 whose target element is
covered by B1 .
For our example, the basic mapping m2 = ∀A2 → ∃B3 .{d, p, e, s}
is nestable inside m1 = ∀A1 → ∃B4 .{d, p}. In particular, A2 and
B3 are strict subtableaux of A1 and B4 ; also there are two correspondences in m2 but not in m1 (e and s) and their target elements are not
covered by B4 .
D EFINITION 3.3. Let m2 = ∀A2 → ∃B2 .V2 be nestable inside
m1 = ∀A1 → ∃B1 .V1 . Without loss of generality assume that all
variable renamings have been applied so that the generators in A1
(B1 ) are literally a subset of those in A2 (B2 ). The result of nesting
m2 inside m1 is a nested mapping of the form:
∀A1 → ∃B1 . [ V1 ∧
∀(A2 − A1 ) → ∃(B2 − B1 ).(V2 − V1 ) ]
where ∀(A2 − A1 ) → ∃(B2 − B1 ).(V2 − V1 ) is a shorthand for
a submapping constructed as follows. The for clause contains the
generators in A2 that are not in A1 . The subsequent where clause (if
needed) contains all the conditions in A2 that are not among (and not
implied by) the conditions of A1 . The exists clause and subsequent
where clause satisfy similar properties with respect to B2 and B1 .
Finally, the last where clause also includes the equalities encoding
the correspondences in V2 − V1 .
It can easily be verified that, for our example, the result of nesting
m2 inside m1 is precisely the nested mapping n. We next explain
conditions (1) and (3) in Definition 3.2 (condition (2) is the more
obvious one). Assume that m2 and m1 are as in Definition 3.2. The
condition that A2 is a strict subtableau of A1 ensures that the for
clause in the submapping that appears in the result of nesting m2
inside m1 is non-empty.
Assume now that B2 is not a strict subtableau of B1 and it is equal
to B1 (the case when there are additional conditions in B2 does not
affect the discussion). Then, the submapping that appears in the result
of nesting of m2 inside m1 is a formula of the form: ∀(A2 − A1 ) →
(V2 − V1 ) (i.e., the equalities on the right are implied by the left-hand
side). There is at least one correspondence v in V2 −V1 , and its source
element is not covered by A1 (otherwise it would be in V1 ). Hence,
in the right-hand side of the above implication, there is at least one
equality asserting that a target element covered by B1 is equal to a
source element covered by A2 − A1 . The problem with this is that
there are many instances of such a source element for one instance
of the target element (since B1 is outside the scope of ∀(A2 − A1 )).
This constraint would effectively require that all such instances of the
source element be equal (and equal to the one instance of the target
5
Although our original algorithm did not include the elimination of
implied mappings, this step can remove many unnecessary formulas.

73

{
{
{
{

d}
d, p }
d, e, s }
d, p, e, s }

(a)

m

(2) tells us that a naive algorithm for creating G might add too many
edges and hence form unnecessary nestings. Indeed, suppose that
mi ⇒ mj and mj ⇒ mk , which also implies that mi ⇒ mk .
Then mi can be nested under mj which can be nested under mk . At
the same time, mi can be nested directly under mk . However, we
prefer the former, deeper, nesting strategy because that interpretation
preserves all source data together with its structure.
To illustrate this point, consider the mapping in Figure 1. There,
we have that m3 ⇒ m2 ⇒ m1 , and also m3 ⇒ m1 . We prefer the
deepest nesting which results in a nested mapping with the following pattern: first map dept tuples, then map the emps tuples under
the current dept tuple, and then map the dependents tuples of
the current emps tuple. The other interpretation, obtained by nesting
m3 directly inside m1 , is not semantically equivalent to the first one.
Indeed, this second interpretation maps all dept tuples but then, for
each dept tuple, it maps the join of emps and dependents tuples
(thus, emps tuples with no dependents are not mapped). In order not
to lose data, we can “fix” this second interpretation by nesting both
m2 and m3 directly inside m1 (using the fact that m2 ⇒ m1 and
m3 ⇒ m1 ). This would have the effect of mapping all tuples of
emps. However, this choice still does not model any correlation between the two submappings m2 and m3 . Hence, there is no merging
of employee tuples and no grouping of dependents within employees.
The first interpretation solves the issue by utilizing, intuitively, all the
available nesting.
To implement the above nesting strategy, which performs the “deepest” nesting possible, our algorithm for constructing G makes sure not
to include any transitively implied edges. More formally, the DAG
G = (M, E) of mappings is constructed so that its set of edges satisfies the following:

⇒

∃A  .
∃A  .
∃A .
∃A .

⇒

∀B  →
∀B →
∀B  →
∀B →

m

m

⇒
⇒

m =
m =
m =
m =

m	
(b)

Figure 8: (a) Reverse basic mappings; (b) Nestable relation
element). Such a constraint is unlikely to be desired (even when it
is satisfiable). Although condition (3) is a bit more subtle, a careful
analysis yields a similar justification.
We illustrate this discussion by considering the reverse of the mapping scenario shown in Figure 3. The schema on the right of that figure is now the source schema, while the schema on the left is the target
schema. The correspondences are the same. Also, the tableaux remain the same as in Figure 7, with the difference that B1 , B2 , B3 , B4
are now source tableaux, and A1 and A2 are target tableaux.
There are four basic mappings (not implied and not subsumed) that
are generated by the algorithm described in Section 3.1. These mappings are shown in Figure 8(a). We have that m5 is nestable inside
m3 and m6 is nestable inside m4 . However, m4 is not nestable inside m3 (because the target tableaux is the same). Similarly, m6 is
not nestable inside m5 . If we try to nest m4 inside m3 , we would
obtain the following nested mapping:
n34 : for d in dept ⇒
exists p0 in proj
where p0 .dname=d.dname ∧
( for p in d.projects ⇒ p0 .pname=p.pname )

E = {(mi → mj ) | mi ⇒ mj ∧ (6 ∃mk )(mi ; mk ∧mk ⇒ mj )}

This constraint says that if there are multiple projects in one dept
tuple (which is possible according to the source schema) then all these
projects are required to have the same pname value (which must also
equal the pname value in the corresponding target proj tuple). This
puts a constraint on the source data that is unlikely to be satisfied.
Our algorithm does not generate such mappings.

The creation of G proceeds in two steps. First, for all pairs (mi , mj )
of mappings in M , we add an edge to G if mi ⇒ mj . Then, for every
edge mi → mj in E, we try finding a longer path mi ; mj . If such
a path exists, we remove mi → mj from E. This is implemented
using a variation of the all-pairs shortest-path algorithm (except that
we are looking for the longest path) and its complexity is O(|M |3 ).
The next step is to extract trees of mappings from G. Each such
tree becomes a nested mapping expression. These trees are computed
in two simple steps. First, all root mappings R in G are identified:
R = {mr | mr ∈ M ∧ (6 ∃m0 )(m0 ∈ M ∧ (mr → m0 ) ∈ E)}.
Then, for each mapping root mr ∈ R, we do a depth-first traversal of
G (following the reverse direction of the edges). Mappings collected
during this visit become part of the tree rooted at mr .
Constructing nested mappings from a tree of mappings raises several issues. First, in Definition 3.3 we explained the meaning of nesting two basic mappings, one under the other. But, in a tree, one mapping can have multiple children that each can be nested inside the
parent. And also, we must apply the definition recursively. We omit
the extensions to Definition 3.3 that are needed to define the result of
nesting a tree of mappings as they are straightforward.
The second, more important issue is that, since these trees are extracted from a DAG, it is possible that they share mappings. In other
words, a mapping can be nested under more than one mapping.
Consider, for example, a mapping scenario that involves three sets:
employees, worksOn, and projects. The worksOn set contains references to employees and projects tuples, capturing an
N:M relationship. Assume that me is a basic mapping for employees, mp is a basic mapping for projects, and mw is a basic
mapping that maps employees and projects by joining them via
worksOn. The resulting graph G of mappings contains two mapping
trees (i.e., two nested mappings), which both yield valid interpretations: T1 = {me ⇐ mw } and T2 = {mp ⇐ mw }. Both trees share

3.4 Nesting Algorithm
In the next step of the algorithm, we use the nestable relation of
Definitions 3.2 and 3.3 to create a set of nested mappings. The input
to this step is the set of basic mappings that result after Step 3.
Step 4. Generation of nested mappings In this step, the algorithm
first constructs a DAG G = (M, E) that represents all possible ways
in which basic mappings can be nested under other basic mappings.
Here, M is the set of basic mappings generated in Step 3, while E
contains edges mi → mj with the property that mi is nestable under
mj according to Definition 3.2. To create nested mappings out of
G, the root mappings of G are identified and a tree of mappings is
extracted from G for each root. Each such tree of mappings becomes
a separate nested mapping.
To understand the shape of G and the issues involved in its construction, we examine the properties of the nestable relation of Definition 3.2. Given two basic mappings mi and mj , let us write
mi ⇒ mj if mi is nestable inside mj . We note that:
(1) The nestable relation is not reflexive and not symmetric. In fact,
stronger statements hold: (a) for all mi , mi 6⇒ mi , and (b) if
mi ⇒ mj , then mj 6⇒ mi . This follows from the strict subtableaux requirement in condition (1) of Definition 3.2.
(2) The nestable relation is transitive: if mi ⇒ mj and mj ⇒ mk
then mi ⇒ mk . This again follows from condition (1) of Definition 3.2 and, further, from conditions (2) and (3).
Because of (1) and (2) above, G is necessarily acyclic. If there is a
path mi ; mj in G, then no path mj ; mi exists in G. Condition

74

mw as a leaf. If we arbitrarily use only one tree and ignore the other,
then source data can be lost: the nested mapping based on T1 maps
all the employees; however, it only maps projects that are associated
with an employee via worksOn (the situation is reversed for T2 ).
However, the inclusion of the shared subtrees in all their “parent”
trees will create nested mappings that lead to redundancy in execution as well as in the generated data. To avoid this, we adopt a simple
strategy to keep a shared subtree only in one of the parent trees and
prune it from all the other. For our example, we can keep T1 intact
and cut the common subtree from T2 , yielding T20 = {mp }. In general, however, the algorithm should not make a choice of which trees
to prune and which to keep intact. This is a semantic and applicationdependent decision. The various choices lead to inequivalent mappings that do not lose data but give preference to certain correlations
in the data (e.g., group projects by employees as opposed to grouping
employees by projects). Furthermore, there can be differences in the
performance of the subsequent execution of the data transformation.
Ideally, a human user could suggest which mapping to generate, if
exposed to all the possible choices of mappings with shared submappings. We have implemented a strategy that selects one of the pruning
choices whenever there is such choice, but in future versions of our
prototype we will allow users to explore the space of such choices.

As it can be seen, the view for each set type includes the atomic type
elements that are directly under the set type. Additionally, it also includes setID columns for each of the set types that are directly nested
under the given set type. Finally, for each set type that is not toplevel (dept is the only top-level set type in this example), there is
an additional column setID. The explanation for this column is the
following (we use emps to illustrate). While in the target schema
there is only one set type emps, in an actual instance there may be
many sets of employee tuples, nested under the various dept tuples.
However, the tuples of these nested sets will all be mapped into one
single table (emps). In order to remember the association between
employee tuples and the sets they belong to, we use the setID column to record the identity of the set for each employee tuple. This
column will then later be used to join with the empsID column under the “parent” table dept, to construct the correct nesting.
We next describe the queries defining the views and how they are
generated. The algorithm starts by Skolemizing each nested mapping
and decoupling it into a set of single-headed constraints, each consisting of one implication and one atom in the right-hand side of the
implication. For our example, the nested mapping n generates the
following four constraints (one for each target atom in n):
r1
r2
r3
r4

4. QUERY GENERATION
One of the main reasons for creating mappings is to be able to automatically create a query or program that transforms an instance of
the source schema into an instance of the target schema. In [24, 11]
we described how to generate queries from basic mapping specifications. Here we extend that work to cover nested mappings. Because
they start from the more expressive nested mapping specification, the
queries that we now generate often perform better, have more functionality in terms of grouping and restructuring, and at the same time
are closer to the mapping specification (thus, easier to understand).
We first present in Section 4.1 a general query generation algorithm
that works for nested mappings with arbitrary Skolem functions for
the set elements (and hence for arbitrary regrouping and restructuring
of the source data). In Section 4.2 we present an optimization that
simplifies the query and can significantly improve performance in the
case of nested mappings with default Skolemization, which are the
mappings that we produce with our mapping generation algorithm.
In particular, this optimization greatly impacts the scenarios where
no complex restructuring of the source is needed (many schema evolution scenarios follow this pattern).

:
:
:
:

proj(d, p, E0 ) → dept(d, null, E[d, p, E0 ], P [d, p, E0 ])
proj(d, p, E0 ) → P [d, p, E0 ] (X[d, p, E0 ], p)
proj(d, p, E0 ) ∧ E0 (e, s) → E[d, p, E0 ] (e, s, P 0 [d, p, E0 , e, s])
proj(d, p, E0 ) ∧ E0 (e, s) → P 0 [d, p, E0 , e, s] (X[d, p, E0 ])

Skolemization replaces every existentially-quantified variable by a
Skolem function that depends on all the universally-quantified variables that appear before the existential variable (in the original mapping). For example, the atomic variable ?x (along with all of its
occurences) is replaced by X[d, p, E0 ], where X is a new Skolem
function name. 6 Atomic variables that do not play an important role
(e.g., not a key or a foreign key) can be replaced by null (see ?b
above). Finally, all existential set variables are replaced by Skolem
terms (if they are not already given by the mapping). Each of the
above constraints can be seen as an assertion of “facts” that relate tuples and set ids. For example, r3 above asserts a fact relating the tuple
(e, s, P 0 [d, p, E0 , e, s]) and the set id E[d, p, E0 ].
Next, the queries defining the contents of the flat views have the
role of “storing” the facts asserted by the above constraints into the
corresponding flat views. For example, all the facts asserted by r3
will be stored into emps, where the setID column is used to store
the set ID (as explained earlier). The following is the set of query
definitions for our four views:
let dept := for p in proj
return [ dname = p.dname,
budget = null,
empsID = E[p.dname, p.pname, p.emps],
projectsID = P[p.dname, p.pname, p.emps]]
emps := for p in proj, e in p.emps
return [ setID = E[p.dname.p.pname,p.emps],
ename = e.ename,
salary = e.salary,
projects1ID = P [p.dname, p.pname, p.emps,
e.ename, e.salary]]
projects1 := for p in proj, e in p.emps
return [ setID = P [p.dname, p.pname, p.emps,
e.ename, e.salary]],
pid = X[p.dname, p.pname, p.emps]],
projects := for p in proj
return [ setID = P[p.dname,p.pname,p.emps],
pid = X[p.dname, p.pname, p.emps]],
pname = p.pname]

4.1 Two-Phase Query
The general algorithm for query generation produces queries that
process source data in two phases. The first-phase query “shreds”
source data into flat (or relational) views of the target schema. The
definition of this query is based on the target schema and on the information encoded in the mappings. The second-phase query is a
wrapping query that is independent of the actual mappings and uses
the shape of the target schema to nest the data from the flat views in
the actual target format.
First-phase query We now describe the construction of the flat views
and of the first-phase query. For each target set type for which there is
some mapping that asserts some tuple for it, there will be a view, with
an associated schema and a query defining it. To illustrate, we will
use the earlier schemas of Figure 3 and the earlier nested mapping n.
The view schema for our example includes the following definitions:

We note that if multiple mappings contribute tuples to a target set
type, then each such mapping will contribute with a query expression

dept(dname, budget, empsID, projectsID)
emps(setID, ename, salary, projects1ID)
projects1(setID, pid)
projects(setID, pid, pname)

6
We really mean here that E0 is the set id and not the contents. Thus,
the Skolem function does not depend on the actual values under E0 .

75

and the corresponding view is defined by the union of all these query
expressions. In the case when the same Skolem function is used from
multiple mappings to define the same set instance (as discussed in
Section 2.3), then the union of queries defining the view will effectively accumulate all the tuples of this set instance within the view
(moreover, all these tuples will have the same set id).
Second-phase query Finally, the previously defined views are used
within a query that combines and nests the data according to the shape
of the target schema. Notice that the nesting of data on the target
is controlled by the Skolem function values computed for the set id
columns in the views.

can also replace P0 [p0 , e] = P0 [p00 , e0 ] with the conjunction of p0 =
p00 and e = e0 , and also P[p] = P[p0 ] with p = p0 . Hence, we obtain
a rewriting where some of the inner loops are unnecessary. The boxes
in q 0 above highlight the “redundant” parts. We can then rewrite q 0
by removing the declaration of p0 and the self-join condition p = p0 .
If we do this at all levels where setID equalities are used, then all
the highlighted parts of the query can be redacted. (In some cases,
the loops are completely replaced by singleton set expressions; this
happens for both projects sets in our example.) The final query is
shown below. It tightly follows the expressions (and optimizations)
encoded in the nested mapping n.

(q) dept = for d in dept
return [
dname = d.dname,
budget = d.budget,
emps = for e in emps
where e.setID = d.empsID
return [
ename = e.ename,
salary = e.salary,
projects = for p in projects1
where p.setID = e.projects1ID
return [ pid = p.pid ]],
projects = for p in projects
where p.setID = d.projectsID
return [ pid = p.pid,
pname = p.pname ] ]

(q’’) dept = for p in proj
return [
dname = p.dname, budget = null,
emps = for e in p.emps
return [
ename = e.ename, salary = e.salary,
projects = { [ pid = X[p.dname, p.pname, p.emps] ] } ],
projects = { [
pid = X[p.dname, p.pname, p.emps],
pname = p.pname ] } ]

5. EXPERIMENTS
We conducted a number of experiments to understand the performance of (a) the nested mapping queries described in Section 4 and
(b) the nested mapping creation algorithm of Section 3. Our nested
mapping prototype is implemented in Java, on top of Clio [11]. The
experiments were performed on a PC-compatible machine, with a single 2.0GHz P4 CPU and 1GB RAM, running Windows XP (SP1) and
JRE 1.4.2. Each experiment was repeated three times, and the average
of the three trials is reported.

4.2 Query Inlining for Default Skolemization
The two-phase algorithm is general in the sense that it can work
for arbitrary restructuring of the data. However, it does require the
data to be flattened before being re-nested in the target format. In
cases where the source and target schemas have similar nesting shape
and the grouping behavior given by the default Skolem functions is
sufficient, the two-phase strategy can be inefficient.
For example, the nested mapping n used in Section 4.1 falls in
this category of nested mappings with default Skolemization. Under
default Skolemization, all the set ids that are created (by the firstphase query) depend on entire source tuples rather than individual
pieces of these tuples. To illustrate, the default Skolem function E for
emps depends on p.dname, p.pname and p.emps, which is equivalent to say that E is a function of the source tuple p. Similarly,
the Skolem function P for projects under departments depends on p.
Also, the Skolem function P0 for projects under employees depends
on p.dname, p.pname, p.emps and e.ename and e.salary, which
means that it is a function of the source tuples p and e. Under such
scenario, we inline the views defined by the first-phase query into the
places where they occur in the second-phase query. For our example
(while taking care of renaming conflicting variable names), we obtain
the following rewrite of q:

5.1 Query Evaluation
We first compare the performance of queries generated using nested
mappings with queries generated from basic mappings. We focus
on a schema evolution scenario where nested mappings with default
Skolemization suffice to express the desired transformation and inlining is applied to optimize the nested mapping query (as described in
Section 4.2). We created a nested schema authorDB, based on the
DBLP7 structure, but with four levels of nesting. The first level contais an author set. Each author tuple has an attribute name and a
nested set of confJournal tuples. Each confJournal tuple has an attribute name and a set of year tuples. Each year tuple contains a yr
attribute and a set of pub elements, each with five attributes: pubId,
title, pages, cdrom, url.
We ran the basic and nested mapping algorithms on four different
settings to create four pairs of mappings (one basic and one nested).
We used authorDB as the source and target schema and added different sets of correspondences to create the four different settings. In
the first, m1 , we only mapped the top-level author set (this means
we used only one correspondence between the name attributes of author). In the second mapping, we mapped the first and the second
level of authorDB (i.e, author and confJournal). Since we mapped
levels 1 and 2, we will refer to this mapping as m12 . We proceeded
in the same fashion adding correspondences for the third and fourth
levels authorDB, creating mappings m123 and m1234 , respectively.
For each mapping, we created two XQuery scripts: one generated
using the basic mappings (using the original Clio query generation
algorithm [24, 11]), and another generated from the nested mappings
(as described in Sections 4.1 and 4.2). Figure 9 compares the generated queries for m12 . To simplify the experiment, we considered
input instances where each author has at least one confJournal element under it, and similarly, each confJournal contains at least one

(q’) dept = for p in proj
return [
dname = p.dname, budget = null,
emps = for p’ in proj, e in p’.emps
where E[p] = E[p’ ]
return [
ename = e.ename, salary = e.salary,
projects = for p’’ in proj, e’ in p’’.emps
where P [p’,e] = P [p’’,e’ ]
return [
pid = X[p’’.dname, p’’.pname, p’’.emps] ] ],
projects = for p’ in proj
where P[p] = P[p’ ]
return [ pid = X[p’.dname, p’.pname, p’.emps],
pname = p’.pname ] ]

Since the Skolem functions are one-to-one id generators, we can
now replace the equalities of the function terms with the equalities of
the arguments. Thus we can replace E[p] = E[p0 ] with p = p0 . We

7

76

http://www.informatik.uni-trier.de/ ley/db/

B2
F2

…

…

Basic query exec. time /
Nested query exec. time

B1
F1

R2: set of

R2: set of
B2
F2

Rn: set of
Bn
Fn

Cm
R1: set of
B1
R2: set of
B2

…

Mappings

Basic query output file size /
Nested query output file size

B1
F1

R1: set of
K1
B1
F1

…

R1: set of

K0
C1

…

R1: set of

Target
R0: set of

Rn: set of
Bn

…

m1234

…

m123

K2
B2
F2

Kn
Cm

…

m1234

R2: set of

Kn
C1

…

R1: set of

K3
B3
F3

R3: set of
K3
B3
R2: set of
K2
B2
R1: set of
K1
B1

Source -- m
R0: set of
K0
K1
K2

…

m123

…

1

100
Input file size
2111 kb
512 kb
312 kb

1
m12

K2
B2
F2

…

10

m1

…

2111 kb
1020 kb
512 kb
312 kb

10

R2: set of

R3: set of

Source -- 1
R0: set of
K0
K1
K2

Rn: set of
Bn
Fn

terates over every source author and confJournal once to create target
author elements (variables x0 and x1 in the query). A second loop
is used to compute the nested confJournal elements (variables x0L1
and x1L1). Further, since we only want to nest the confJournal elements for the current author tuple, the second loop is correlated to
the outer one (the where clause in the query). That is, this query
requires two passes over the input data plus a correlated nested subquery to correctly nest data. In contrast, the nested mapping query
only does one pass over the source author and confJournal data and
does not need any correlation condition since it takes advantage of
existing nesting of the source data.
The basic mapping query strategy can also create a large number
of duplicates in the output instance. To illustrate this problem, we
create a mapping m14 that maps the author and pub levels of the
schema. We ran the queries for m14 and m1234 using an input instance that contains 4173 author elements and a total of 6468 pub
elements nested within those authors. The count of resulting author
and pub elements in the output instance is shown in this table:

Input file size

m 12

K3
B3
F3

Target
Rn: set of

Source-- m
Rn: set of

Figure 11: The chain (left) and authority (right) scenarios.

1000

m1

R3: set of

K1
B1
F1

Figure 9: Basic (left) and nested (right) query for m12 .

100

Source-- 1
Rn: set of

…

let $doc0 := fn:doc("instance.xml") return
<authorDB>
{ for $x0 in $doc0/authorDB/author
return
<author>
<name> { $x0/name/text() } </name>
{ for $x1 in $x0/confJournal
return
<confJournal>
<name>{ $x1/name/text() }</name>
</confJournal> }
</author> }
</authorDB>

…

let $doc0 := fn:doc("instance.xml") return
<authorDB>
{ for $x0 in $doc0/authorDB/author,
$x1 in $x0/confJournal
return
<author>
<name> { $x0/name/text() } </name>
{ for $x0L1 in $doc0/authorDB/author,
$x1L1 in $x0L1/confJournal
where $x0/name/text()=$x0L1/name/text()
return
<confJournal>
<name> { $x1L1/name/text() } </name>
</confJournal> }
</author> }
</authorDB>

Mappings

Mapping
m14
m1234

Figure 10: Execution time (upper) and output file size factors.

B author
6468
6468

B pub
18826
157254

NM author
4173
4173

NM pub
6468
6468

The nested mapping queries do not create duplicates for any of the
two mappings and produce a copy of the input instance (which is the
expected output instance in all these mappings). The basic mapping
queries, on the other hand, create 2295 duplicate author elements. Intuitively, a duplicate is created whenever an author has more than one
publication. Each author duplicate then carries the same set of duplicate publications causing an explosion of duplicate pub elements.
The nested mapping query we automatically generate does not suffer
from this common problem.

year subelement and each year contains at least one pub subelement.
As a consequence, only one basic mapping is enough to map all the
source data. Otherwise we would have to consider additional basic
mappings (e.g., map author elements independently of the existence
of confJournal subelements). This would only make the basic mapping query become more complex and have worse performance. On
the other hand, even in the favorable case where one basic mapping is
enough, we show that the nested mapping query is still much better.
We ran the queries using the Saxon XQuery processor8 with increasingly larger input files. Figure 10 shows that the nested mapping
queries consistently outperformed the basic mapping queries, both in
time and in the size of the output instance generated.9 The upper
part of Figure 10 plots the execution speed-up for the nested mapping
queries (i.e., the ratio of the execution time for the basic mapping
query over the execution time for the query generated with the nested
mapping). The lower chart shows the ratio of the output file size for
the basic mapping over the output file size for the nested mapping.
Both charts use a logarithmic scale in the y-axis.
A cursory inspection of the queries in Figure 9 reveals the reason
for the better execution time of the nested mapping queries. Our basic
mapping query generation strategy repeats the source tableau expression for each target set type. In the case of m12 , the basic query in-

5.2 Algorithm Evaluation
We now study the performance and scalability of the nested mapping creation algorithm. We designed two synthetic scenarios (depicted in Figure 11), chain and authority [27]. The chain scenario
simulates mappings between multiple inter-linked relational tables
and an XML target with large number of nesting levels. The authority
scenario simulates mappings between multiple relational tables referencing a central table and a shallow XML target with a large branching factor (large number of child tables). For each scenario, we used
a schema generator to create schema definitions with variable degrees
of complexity (e.g., number of elements, referential constraints, number of nesting levels). In addition, we also replicated each generated
source schema a number of times in order to simulate the cases of
multiple data sources mapping into one target.
For the chain scenario we increased the number of different sources
(m) and the number of inter-linked relational tables (depth) ( 1 ≤

8

saxon.sourceforge.net
Larger output files for the same mapping indicate more duplicate
tuples in the result.
9

77

120.0

[5] A. Bonifati, E. Q. Chang, T. Ho, V. S. Lakshmanan, and
R. Pottinger. HePToX: Marrying XML and Heterogeneity in
Your P2P Databases. In VLDB, pages 1267–1270, 2005.
[6] A. Deutsch, M. Fernandez, D. Florescu, A. Levy, and D. Suciu.
A Query Language for XML. In WWW8, pages 77–91, 1999.
[7] R. Fagin. Inverting Schema Mappings. In PODS, 2006.
[8] R. Fagin, P. Kolaitis, L. Popa, and W.-C. Tan. Composing
Schema Mappings: Second-Order Dependencies to the Rescue.
In PODS, pages 83–94, 2004.
[9] R. Fagin, P. G. Kolaitis, R. J. Miller, and L. Popa. Data
Exchange: Semantics and Query Answering. TCS,
336(1):89–124, 2005.
[10] M. Friedman, A. Y. Levy, and T. D. Millstein. Navigational
Plans For Data Integration. In AAAI/IAAI, pages 67–73, 1999.
[11] L. M. Haas, M. A. Hernández, H. Ho, L. Popa, and M. Roth.
Clio Grows Up: From Research Prototype to Industrial Tool. In
SIGMOD, pages 805–810, 2005.
[12] A. Y. Halevy, Z. G. Ives, J. Madhavan, P. Mork, D. Suciu, and
I. Tatarinov. The Piazza Peer Data Management System. IEEE
Trans. Knowl. Data Eng., 16(7):787–798, 2004.
[13] R. Hull and M. Yoshikawa. Ilog: Declarative creation and
manipulation of object identifiers. In VLDB, pages 455–468,
1990.
[14] M. Lenzerini. Data Integration: A Theoretical Perspective. In
PODS, pages 233–246, 2002.
[15] J. Madhavan and A. Y. Halevy. Composing Mappings Among
Data Sources. In VLDB, pages 572–583, 2003.
[16] D. Maier, A. O. Mendelzon, and Y. Sagiv. Testing Implications
of Data Dependencies. ACM TODS, 4(4):455–469, 1979.
[17] W. May. Information Extraction and Integration with F LORID:
The M ONDIAL Case Study. Technical Report 131, Universität
Freiburg, Institut für Informatik, 1999.
[18] S. Melnik, P. A. Bernstein, A. Halevy, and E. Rahm. Applying
Model Management to Executable Mappings. In SIGMOD,
pages 167–178, 2005.
[19] R. J. Miller, L. M. Haas, and M. A. Hernández. Schema
Mapping as Query Discovery. In VLDB, pages 77–88, 2000.
[20] T. Milo and S. Zohar. Using Schema Matching to Simplify
Heterogeneous Data Translation. In VLDB, pages 122–133,
1998.
[21] A. Nash, P. A. Bernstein, and S. Melnik. Composition of
Mappings Given by Embedded Dependencies. In PODS, pages
172–183, 2005.
[22] Y. Papakonstantinou, S. Abiteboul, and H. Garcia-Molina.
Object Fusion in Mediator Systems. In VLDB, pages 413–424,
1996.
[23] L. Popa and V. Tannen. An Equational Chase for
Path-Conjunctive Queries, Constraints, and Views. In ICDT,
pages 39–57, 1999.
[24] L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernández, and
R. Fagin. Translating Web Data. In VLDB, pages 598–609,
2002.
[25] P. Roy, S. Seshadri, S. Sudarshan, and S. Bhobe. Efficient and
Extensible Algorithms for Multi Query Optimization. In
SIGMOD, pages 249–260, 2000.
[26] Y. Velegrakis, R. J. Miller, and L. Popa. Preserving Mapping
Consistency under Schema Changes. VLDB Journal,
13(3):274–293, 2004.
[27] C. Yu and L. Popa. Constraint-Based XML Query Rewriting
for Data Integration. In SIGMOD, pages 371–382, 2004.
[28] C. Yu and L. Popa. Semantic Adaptation of Schema Mappings
when Schemas Evolve. In VLDB, pages 1006–1017, 2005.

nesting algorithm (Step 4)

100.0
Time (s)

basic mapping generation
80.0
60.0
40.0
20.0
0.0
2-2

4-4

6-6

8-8

10-10

12-12

14-14

16-16

Mappings (m - n)

Figure 12: Algorithm’s performance for the authority scenario.
m ≤ 20 and 1 ≤ depth ≤ 3). In the worst case, our prototype
took 0.2 seconds to compute the nested mapping. For the authority
scenario, we simultaneously increased the number of sources (m) and
the branching factor (n) (the number of child tables) such that m = n
for each trial. Figure 12 shows the results for authority. For schemas
of small to medium size (when m and n are less than 12), the nesting
algorithm (Step 4 described in Section 3.4) finishes in a few seconds
after the computation of the basic mappings (Steps 1, 2 and 3). But
the time degrades exponentially as the mapping complexity increases.
Note, however, that in the largest case we tried (m = n = 20), the
nesting algorithm (Step 4) took only about 20 seconds.
Finally, we evaluated the algorithm performance with a mapping
that uses the Mondial schema [17], a database of geographical data.
Mondial has a relational representation with 28 relations and a maximum branching factor of 9. Its XML Schema counterpart has a maximum depth of 5 and a maximum branching factor of 9. We mapped
from the relational into the XML representation and created 26 basic
mappings in 1.2 seconds. The nesting algorithm then extracted 10
nested mappings in 2.8 seconds.

6. CONCLUSION
We have introduced a new, structured mapping formalism called
nested mappings that provides a natural way to express correlations
between schema mappings. We demonstrated benefits of this formalism including increased specification accuracy, and the ability to
specify (and customize) grouping semantics declaratively. We provided an algorithm to generate nested mappings from standard schema
matchings. We showed how to compile these mappings into transformation queries that can be much more efficient than their counterparts
obtained from the earlier basic mappings. The new transformation
queries also generate much cleaner data. Certainly nested mappings
have important applications in schema evolution where the mapping
must be able to ensure that the grouping of much of the data is not
changed. Indeed our work here was largely inspired by the inability
of existing mapping formalisms to faithfully represent the “identity
mapping” for many schemas. We are currently evaluating the use
of nested mappings in other tasks including (virtual) data integration
over large schemas and large collections of schemas.

7. REFERENCES
[1] M. Arenas and L. Libkin. XML Data Exchange: Consistency
and Query Answering. In PODS, pages 13–24, 2005.
[2] M. Benedikt, C. Y. Chan, W. Fan, J. Freire, and R. Rastogi.
Capturing both types and constraints in data integration. In
SIGMOD, pages 277–288, 2003.
[3] P. Bernstein, S. Melnik, and P. Mork. Interactive Schema
Translation with Instance-Level Mappings. In VLDB, pages
1283–1286, 2005.
[4] P. Bohannon, W. Fan, M. Flaster, and P. P. S. Narayan.
Information preserving xml schema embedding. In VLDB,
pages 85–96, 2005.

78

Core Schema Mappings
Giansalvatore Mecca1 Paolo Papotti2 Salvatore Raunich1
1

Dipartimento di Matematica e Informatica – Università della Basilicata – Potenza, Italy
2
Dipartimento di Informatica e Automazione – Università Roma Tre – Roma, Italy

ABSTRACT

between sources. Mappings are executable transformations
– say, SQL or XQuery scripts – that specify how an instance
of the source repository should be translated into an instance
of the target repository. There are several ways to express
such mappings. A popular one consists in using tuple generating dependencies (tgds) [3]. We may identify two broad
research lines in the literature.
On one side, we have studies on practical tools and algorithms for schema mapping generation. In this case, the
focus is on the development of systems that take as input
an abstract specification of the mapping, usually made of
a bunch of correspondences between the two schemas, and
generate the mappings and the executable scripts needed to
perform the translation. This research topic was largely inspired by the seminal papers about the Clio system [17, 18].
The original algorithm has been subsequently extended in
several ways [12, 4, 2, 19, 7] and various tools have been
proposed to support users in the mapping generation process. More recently, a benchmark has been developed [1] to
compare research mapping systems and commercial ones.
On the other side, we have theoretical studies about data
exchange. Several years after the development of the initial
Clio algorithm, researchers have realized that a more solid
theoretical foundation was needed in order to consolidate
practical results obtained on schema mapping systems. This
consideration has motivated a rich body of research in which
the notion of a data exchange problem [9] was formalized,
and a number of theoretical results were established. In this
context, a data exchange setting is a collection of mappings –
usually specified as tgds – that are given as part of the input;
therefore, the focus is not on the generation of the mappings,
but rather on the characterization of their properties. This
has brought to an elegant formalization of the notion of a
solution for a data exchange problem, and of operators that
manipulate mappings in order, for example, to compose or
invert them.
However, these two research lines have progressed in a
rather independent way. To give a clear example of this,
consider the fact that there are many possible solutions for
a data exchange problem. A natural question is the following: “which solution should be materialized by a mapping system?” A key contribution of data exchange research
was the formalization of the notion of core [11] of a data
exchange solution, which was identified as an “optimal” solution. Informally speaking, the core has a number of nice
properties: it is “irredundant”, since it is the smallest among
the solutions that preserve the semantics of the exchange,
and represents a “good” instance for answering queries over

Research has investigated mappings among data sources under two perspectives. On one side, there are studies of practical tools for schema mapping generation; these focus on algorithms to generate mappings based on visual specifications
provided by users. On the other side, we have theoretical researches about data exchange. These study how to generate
a solution – i.e., a target instance – given a set of mappings
usually specified as tuple generating dependencies. However,
despite the fact that the notion of a core of a data exchange
solution has been formally identified as an optimal solution,
there are yet no mapping systems that support core computations. In this paper we introduce several new algorithms
that contribute to bridge the gap between the practice of
mapping generation and the theory of data exchange. We
show how, given a mapping scenario, it is possible to generate an executable script that computes core solutions for the
corresponding data exchange problem. The algorithms have
been implemented and tested using common runtime engines
to show that they guarantee very good performances, orders
of magnitudes better than those of known algorithms that
compute the core as a post-processing step.

Categories and Subject Descriptors
H.2 [Database Management]: Heterogeneous Databases

General Terms
Algorithms, Design

Keywords
Schema Mappings, Data Exchange, Core Computation

1.

INTRODUCTION

Integrating data coming from disparate sources is a crucial task in many applications. An essential requirement of
any data integration task is that of manipulating mappings

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGMOD’09, June 29–July 2, 2009, Providence, Rhode Island, USA.
Copyright 2009 ACM 978-1-60558-551-2/09/06 ...$5.00.

655

the target database. It can therefore be considered a natural requirement for a schema mapping system to generate
executable scripts that materialize core solutions.
Unfortunately, there is yet no schema mapping generation algorithm that natively produces executable scripts that
compute the core. On the contrary, the solution produced
by known schema mapping systems – called a canonical solution – typically contains quite a lot of redundancy. This is
partly due to the fact that computing cores is a challenging
task. Several polynomial-time algorithms [11, 13, 20] have
been developed to compute the core of a data exchange solution. These algorithms represent a relevant step forward,
but still suffer from a number of serious drawbacks from
a schema-mapping perspective. First, they are intended as
post-processing steps to be applied to the canonical solution,
and require a custom engine to be executed; as such, they
are not integrated into the mapping system, and are hardly
expressible as an executable (SQL) script. Second and more
important, as it will be shown in our experiments, they do
not scale to large exchange tasks: even for databases of a
few thousand tuples computing the core typically requires
many hours.
In this paper we introduce the +Spicy1 mapping system.
The system is based on a number of novel algorithms that
contribute to bridge the gap between the practice of mapping generation and the theory of data exchange. In particular:
(i) +Spicy integrates the computation of core solutions
in the mapping generation process in a highly efficient way;
after a set of tgds has been generated based on the input provided by the user, cores are computed by a natural rewriting
of the tgds in terms of algebraic operators; this allows for
an efficient implementation of the rewritten mappings using common runtime languages like SQL or XQuery and
guarantees very good performances, orders of magnitude
better than those of previous core-computation algorithms;
we show in the paper that our strategy scales up to large
databases in practical scenarios;
(ii) we classify data exchange settings in several categories, based on the structure of the mappings and on the
complexity of computing the core; correspondingly, we identify several approximations of the core of increasing quality;
the rewriting algorithm is designed in a modular way, so
that, in those cases in which computing the core requires
heavy computations, it is possible to fine tune the trade off
between quality and computing times;
(iii) finally, the rewriting algorithm can be applied both
to mappings generated by the mapping system, or to preexisting tgds that are provided as part of the input. Moreover, all of the algorithms introduced in the paper can be
applied both to relational and to nested – i.e., XML – scenarios; +Spicy is the first mapping system that brings together
a sophisticate and expressive mapping generation algorithm
with an efficient strategy to compute irredundant solutions.
In light of these contributions, we believe this paper makes
a significant advancement towards the goal of integrating
data exchange concepts and core computations into existing
database technology.
The paper is organized as follows. In the following section,
we give an overview of the main ideas. Section 3 provides
some background. Section 4 provides a quick overview of
1

the tgd generation algorithm. The rewriting algorithms are
in Sections 5, 6. A discussion on complexity is in Section 7.
Experimental results are in Section 8. A discussion of related
work is in Section 9.

2.

OVERVIEW

In this section we shall introduce the various algorithms
that are developed in the paper.
It is well known that translating data from a given source
database may bring to a certain amount of redundancy into
the target database. To see this, consider the mapping scenario in Figure 1. A source instance is shown in Figure 2.
A constraint-driven mapping system as Clio would gener-

Figure 1: Mapping Bibliographic References
ate for this scenario several mappings, like the ones below.2
Mappings are tgds that state how tuples should be produced
in the target based on tuples in the source. Mappings can be
expressed using different syntax flavors. In schema mapping
research [12], an XQuery-like syntax is typically used. Data
exchange papers use a more classical logic-based syntax that
we also adopt in this paper.
m1 .
m2 .
m3 .
m4 .

∀t, y, p, i : Refs(t, y, p, i) → ∃N: TRefs(t, y, p, N )
∀i, n : Auths(i, n) → ∃T, Y, P: TRefs(T, Y, P, n)
∀t, y, p, i, n : Refs(t, y, p, i) ∧ Auths(i, n) → TRefs(t, y, p, n)
∀t, p, n : WebRefs(t, p, n) → ∃Y : TRefs(t, Y, p, n)

Mapping m3 above states that for every tuple in Refs that

Figure 2: Instances for the References Scenario
has a join with a tuple in Authors, a tuple in TRefs must
be produced. Mapping m1 is needed to copy into the target
2
Note that the generation of mapping m1 requires an extension
of the algorithms described in [18, 12].

Pronounced “more spicy”.

656

references that do not have authors, like “The SQL92 Standard ”. Similarly, mapping m2 is needed in order to copy
names of authors for which there are no references (none in
our example). Finally, mapping m4 copies tuples in WebRefs.
Given a source instance, executing the tgds amounts to
running the standard chase algorithm on the source instance
to obtain an instance of the target called a canonical universal solution [9]; note that a natural way to chase the
dependencies is to execute them as SQL statements in the
DBMS.
These expressions materialize the target instance in Figure 2. While this instance satisfies the tgds, still it contains
many redundant tuples, those with a gray background. As
shown in [12], for large source instances the amount of redundancy in the target may be very large, thus impairing
the efficiency of the exchange and the query answering process. This has motivated several practical proposals [8, 12,
7] towards the goal of removing such redundant data. Unfortunately, these proposals are applicable only in some cases
and do not represent a general solution to the problem.
Data exchange research [11] has introduced the notion of
core solutions as “optimal” solutions for a data exchange
problem. Consider for example tuples t1 = (null, null, null,
E.F.Codd) and t2 = (A Relational Model..., 1970, CACM,
E.F.Codd) in Figure 2. The fact that t1 is redundant with
respect to t2 can be formalized by saying that there is an
homomorphism from t1 to t2 . A homomorphism, in this context, is a mapping of values that transforms the nulls of t1
into the constants of t2 , and therefore t1 itself into t2 . This
means that the solution in Figure 2 has an endomorphism,
i.e., a homomorphism into a sub-instance – the one obtained
by removing t1 . The core [11] is the smallest among the solutions for a given source instance that has homomorphisms
into all other solutions. The core of the solution in Figure 2
is in fact the portion of the TRefs table with a white background.
A possible approach to the generation of the core for a
relational data exchange problem is to generate a canonical solution by chasing the tgds, and then to apply a postprocessing algorithm for core identification. Several polynomial algorithms have been identified to this end [11, 13].
These algorithms provide a very general solution to the problem of computing core solutions for a data exchange setting.
Also, an implementation of the core-computation algorithm
in [13] has been developed [20], thus making a significant
step towards the goal of integrating core computations in
schema mapping systems.
However, experience with these algorithms shows that, although polynomial, they require very high computing times
since they look for all possible endomorphisms among tuples
in the canonical solution. As a consequence, they hardly
scale to large mapping scenarios. Our goal is to introduce a
core computation algorithm that lends itself to a more efficient implementation as an executable script and that scales
well to large databases. To this end, in the following sections
we introduce two key ideas: the notion of homomorphism
among formulas and the use of negation to rewrite tgds.

between the right-hand sides of the two tgds. Consider tgds
m2 and m3 above; with an abuse of notation, we consider the
two formulas as sets of tuples, with existentially quantified
variables that correspond to nulls; it can be seen that the
conclusion TRefs(T, Y, P, n) of m2 can be mapped into the
conclusion TRefs(t, y, p, n) of m3 by the following mapping
of variables: T → t, Y → y, P → p; in this case, we say
that m3 subsumes m2 ; similarly, m3 also subsumes m1 and
m4 . This gives us a nice necessary condition to intercept
possible redundancy (i.e., possible endomorphisms among
tuples in the canonical solution). Note that the condition
is merely a necessary one, since the actual generation of
endomorphisms among facts depends on values coming from
the source. Note also that we are checking for the presence
of homomorphisms among formulas, i.e., conclusions of tgds,
and not among instance tuples; since the number of tgds is
typically much smaller than the size of an instance, this task
can be carried out quickly.
A second important intuition is that, whenever we identify
two tgds m, m′ such that m subsumes m′ , we may prevent
the generation of redundant tuples in the target instance by
executing them according to the following strategy: (i) generate target tuples for m, the “more informative” mapping;
(ii) for m′ , generate only those tuples that actually add
some new content to the target. To make these ideas more
explicit, we may rewrite the original tgds as follows (universally quantified variables have been omitted since they
should be clear from the context):
m′3 . Refs(t, y, p, i) ∧ Auths(i, n) → TRefs(t, y, p, n)
m′1 . Refs(t, y, p, i) ∧ ¬(Refs(t, y, p, i) ∧ Auths(i, n))
→ ∃N: TRefs(t, y, p, N )
m′2 . Auths(i, n) ∧ ¬(Refs(t, y, p, i) ∧ Auths(i, n))∧
¬(WebRefs(t, p, n)) → ∃X, Y, Z: TRefs(X, Y, Z, n)
m′4 . WebRefs(t, p, n) ∧ ¬(Refs(t, y, p, i) ∧ Auths(i, n))
→ ∃Y : TRefs(t, Y, p, n)
Once we have rewritten the original tgds in this form, we
can easily generate an executable transformation under the
form of relational algebra expressions. Here, negations become difference operators; in this simple case, nulls can be
generated by outer-union operators, ∪∗ , that have the semantics of the insert into SQL statement:3
m′3
m′1
m′2
m′4

: TRefs = πt,y,p,n (Refs 1 Auths)
:
∪∗ (πt,y,p (Refs) − πt,y,p (Refs 1 Auths))
:
∪∗ (πn (Auths) − πn (Refs 1 Auths) − πa (WebRefs))
:
∪∗ (πt,p,n (WebRefs) − πt,p,n (Refs 1 Auths))

The algebraic expressions above can be easily implemented
in an executable script, say in SQL or XQuery, to be run in
any database engine. As a consequence, there is a noticeable
gain in efficiency with respect to the algorithms for core
computation proposed in [11, 13, 20].
Despite the fact that this example looks pretty simple,
it captures a quite common scenario. However, removing
redundancy from the target may be a much more involved
process, as discussed in the following.
Coverages. Consider now the mapping scenario in Figure 3.

The target has two tables, in which genes reference their protein via a foreign key. In the source we have data coming

Subsumption and Rewriting. The first intuition is that it

is possible to analyze the set of formulas in order to recognize
when two tgds may generate redundant tuples in the target.
This happens when it is possible to find a homomorphism

3

We omit the actual SQL code since it tends to be quite long.
Note also that in the more general case Skolem functions are
needed to properly generate nulls.

657

from two different biology databases. Data in the PDB tables comes from the Protein Database, which is organized
in a way that is similar to the target. On the contrary, the
EMBL table contains data from the popular EMBL repository; there, tuples need to be partitioned into a gene and a
protein tuple. In this process, we need to “invent” a value to
be used as a key-foreign key pair for the target. This is usually done using a Skolem function [18]. This transformation

subsumptions is that there can be a much larger number of
possible rewritings for a tgd like m3 , and therefore a larger
number of additional joins and differences to compute. This
is due to the fact that, in order to discover coverages, we
need to look for homomorphisms of every single atom into
other atoms appearing in right-hand sides of the tgds, and
then combine them in all possible ways to obtain the rewritings. To give an example, suppose the source also contains
tables XProtein, XGene that write tuples to Protein and
Gene; then, we might have to rewrite m3 by adding the
negation of four different joins: (i) PDBProtein and PDBGene; (ii) XProtein, XGene; (iii) PDBProtein and XGene;
(iv) XProtein and PDBGene. This obviously increases the
time needed to execute the exchange.
We emphasize that this form of complex subsumption
could be reduced to a simple subsumption if the source
database contained a foreign-key constraint from PDBGene
to PDBProtein; in this case, only two tgds would be necessary. In our experiments, simple subsumptions were much
more frequent than complex coverages. Moreover, even in
those cases in which coverage rewritings were necessary, the
database engine performed very well.

Figure 3: Genes
can be expressed using the following tgds:
m1 . PDBProtein(i, p) → Protein(i, p)
m2 . PDBGene(g, i) → Gene(g, i)
m3 . EMBLGene(p, g) → ∃N: Gene(g, N ) ∧ Protein(N, p)

Handling Self-Joins. Special care must be devoted to tgds

Sample instances are in Figure 4. It can be seen that the
canonical solution contains a smaller endomorphic image
– the core – since the tuples (14-A, N2 ) and (N2, 14-Aantigen), where N2 was invented during the chase, can be
mapped to the tuples (14-A, p1 ) and (p1, 14-A-antigen). In
fact, if we look at the right-hand sides of tgds, we see that
there is a homomorphism from the right-hand side of m3 ,
{Gene(g, N ), Protein(N, p)}, into the right-hand sides of m1
and m2 , {Gene(g, i), Protein(i, p)}: it suffices to map N into
i. However, this homomorphism is a more complex one with
respect to those in the previous example. There, we were
mapping the conclusion of one tgd into the conclusion of another. We call this form of homomorphism a coverage of m3
by m1 and m2 . We may rewrite the original tgds as follows

containing self-joins in the conclusion, i.e., tgds in which the
same relation symbols occurs more than once in the righthand side. One example of this kind is the “self-join” scenario
in STMark [1], or the “RS” scenario in [11]; in this section
we shall refer to a simplified version of the latter, in which
the source schema contains a single relation R, the target
schema a single relation S , and a single tgd is given:

Figure 4: Instances for the genes example
to obtain the core:
m′1 . PDBProtein(i, p) → Protein(i, p)
m′2 . PDBGene(g, i) → Gene(g, i)
m′3 . EMBLGene(p, g) ∧ ¬(PDBGene(g, i) ∧ PDBProtein(i, p))
→ ∃N Gene(g, N ) ∧ Protein(N, p)
From the algebraic viewpoint, mapping m′3 above requires to
generate in Gene and Protein tuples based on the following
expression:
EMBLGene − πp,g (PDBGene 1 PDBProtein)
In the process, we also need to generate the appropriate
Skolem functions to correlate tuples in Gene with the corresponding tuples in Protein. A key difference with respect to

m1 . R(a, b) → ∃x1 , x2 : S(a, b, x1 ) ∧ S(b, x2 , x1 )
Assume table R contains a single tuple: R(1, 1); by chasing m1 , we generate two tuples in the target: S(1, 1, N 1),
S(1, N 2, N 1). It is easy to see that this set has a proper endomorphism, and therefore its core corresponds to the single
tuple S(1, 1, N 1).
Even though the example is quite simple, eliminating this
kind of redundancy in more complex scenarios can be rather
tricky, and therefore requires a more subtle treatment. Intuitively, the techniques discussed above are of little help,
since, regardless of how we rewrite the premise of the tgd,
on a tuple R(1, 1) the chase will either generate two tuples
or none of them. As a consequence, we introduce a more
sophisticate treatment of these cases.
Let us first note that in order to handle tgds like the one
above, the mapping generation system had to be extended
with several new primitives with respect to those offered
by [18, 12], which cannot express scenarios with self-joins.
We extend the primitives offered by the mapping system as
follows: (i) we introduce the possibility of duplicating sets
in the source and in the target; to handle the tgd above, we
duplicate the S table in the target to obtain two different
copies, S 1 , S 2 ; (ii) we give users full control over joins in the
sources, in addition to those corresponding to foreign key
constraints; using this feature, users can specify arbitrary
join paths, like the join on the third attribute of S 1 and S 2 .
Based on this, we notice that the core computation can
be carried-on in a clean way by adopting a two-step process.
As a first step, we rewrite the original tgd using duplications
as follows:
m1 . R(a, b) → ∃x1 , x2 : S 1 (a, b, x1 ) ∧ S 2 (b, x2 , x1 )

658

By doing this, we “isolate” the tuples in S 1 from those in
S 2 . Then, we construct a second exchange to copy tuples
of S 1 and S 2 into S , respectively. However, we can more
easily rewrite the tgds in the second exchange in order to
remove redundant tuples. In our example, on the source
tuple R(1, 1) the first exchange generates tuples S 1 (1, 1, N 1)
and S 2 (1, N 2, N 1); the second exchange discards the second
tuple and generates the core. The process is sketched in
Figure 5. These ideas are made more precise in the following
sections.

it is the case that h(t) = R(A1 : h(v1 ), . . . , Ak : h(vk )) belongs to J’. h is called an endomorphism if J’ ⊆ J; if J’ ⊂ J it
is called a proper endomorphism. We say that two instances
J , J’ are homomorphically equivalent if there are homomorphisms h : J → J’ and h′ : J’ → J. Note that a conjunction
of atoms may be seen as a special instance containing only
variables. The notion of homomorphism extends to formulas
as well.
Dependencies are executed using the classical chase procedure. Given an instance hI, Ji, during the chase a tgd
φ(x) → ∃y(ψ(x, y)) is fired by a value assignment a, that
is, an homomorphism from φ(x) into I, such that there is
no extension of a that maps φ(x) ∪ ψ(x, y) into hI, Ji. To
fire the tgd a is extended to ψ(x, y) by assigning to each
variable in y a fresh null, and then adding the new facts to
J.
Data Exchange Setting A data exchange setting is a
quadruple (S, T, Σst , Σt ), where S is a source schema, T is
a target schema, Σst is a set of source-to-target tgds, and
Σt is a set of target dependencies that may contain tgds
and egds. Associated with such a setting is the following
data exchange problem: given an instance I of the source
schema S, find a finite target instance J such that I and J
satisfy Σst and J satisfies Σt . In the case in which the set
of target dependencies Σt is empty, we will use the notation
(S, T, Σst ).
Given a data exchange setting (S, T, Σst , Σt ) and a source
instance I , a universal solution [9] for I is a solution J such
that, for every other solution J’ there is a homomorphism
h : J → J’. The core [11] of a universal solution J , C, is a
subinstance of J such that there is a homomorphism from
J to C, but there is no homomorphism from J to a proper
subinstance of C.

Figure 5: The Double Exchange

3.

PRELIMINARIES

In the following sections we will mainly make reference
to relational settings, since most of the results in the literature refer to the relational model. However, our algorithms
extend to the nested case, as it will be discussed in Section 8.
Data Model We fix two disjoint sets: a set of constants,
const, a set of labeled nulls, var. We also fix a set of labels A0 , A1 . . ., and a set of relation symbols {R0 , R1 , . . .}.
With each relation symbol R we associate a relation schema
R(A1 , . . . , Ak ). A schema S = {R1 , . . . , Rn } is a collection of relation schemas. An instance of a relation schema
R(A1 , . . . , Ak ) is a finite set of tuples of the form R(A1 :
v1 , . . . , Ak : vk ), where, for each i, vi is either a constant
or a labeled null. An instance of a schema S is a collection
of instances, one for each relation schema in S. We allow
to express key constraints and foreign key constraints over
a schema, defined as usual. In the following, we will interchangeably use the positional and non positional notation
for tuples and facts; also, with an abuse of notation, we will
often blur the distinction between a relation symbol and the
corresponding instance.
Given an instance I , we shall denote by const(I) the set
of constants occurring in I , and by var(I) the set of labeled
nulls in I . dom(I), its active domain, will be const(I)∪var(I).
Given two disjoint schemas, S and T, we shall denote by
hS, Ti the schema {S1 . . . Sn , T1 . . . Tm }. If I is an instance
of S and J is an instance of T, then the pair hI, Ji is an
instance of hS, Ti.
Dependencies Given two schemas, S and T, an embedded
dependency [3] is a first-order formula of the form ∀x(φ(x) →
∃y(ψ(x, y)), where x and y are vectors of variables, φ(x) is
a conjunction of atomic formulas such that all variables in x
appear in it, and ψ(x, y) is a conjunction of atomic formulas.
φ(x) and ψ(x, y) may contain equations of the form vi = vj ,
where vi and vj are variables.
An embedded dependency is a tuple generating dependency if φ(x) and ψ(x, y) only contain relational atoms. It is
an equality generating dependency (egd) if ψ(x, y) contains
only equations. A tgd is called a source-to-target tgd if φ(x)
is a formula over S and ψ(x, y) over T. It is a target tgd if
both φ(x) and ψ(x, y) are formulas over T.
Homomorphisms and Chase Given two instances J , J’
over a schema T, a homomorphism h : J → J’ is a mapping
from dom(J) to dom(J’) such that for each c ∈ const(J),
h(c) = c, and for each tuple t = R(A1 : v1 , . . . , Ak : vk ) in J

4.

TGD GENERATION

Before getting into the details of the tgd rewriting algorithm, let us give a quick overview of how the input tgds are
generated by the system. Note that, as an alternative, the
user may decide to load a set of pre-defined tgds provided
as logical formulas encoded in a fixed textual format.
The tgd generation algorithm we describe here is a generalization of the basic mapping generation algorithm introduced in [18]. The input to the algorithm is a mapping scenario, i.e., an abstract specification of the mapping between
source and target. In order to achieve a greater expressive power, we enrich the primitives for specifying scenarios.
More specifically, given a source schema S and a target T,
a mapping scenario is specified as follows:
(i) two (possibly empty) sets of duplications of the sets in S
and in T; each duplication of a set R corresponds to adding
to the data source a new set named R i , for some i, that is
an exact copy of R;
(ii) two (possibly empty) sets of join constraints over S and
over T; each join constraint specifies that the system needs
to chase a join between two sets; foreign key constraints also
generate join constraints;
(iii) a set of value correspondences, or lines; for the sake of
simplicity in this paper we concentrate on 1 : 1 correspondences of the form AS → AT .4
4

In its general form, a correspondence maps n source attributes
into a target attribute via a transformation function; moreover,
it can have an attached filter that states under which conditions

659

The tgd generation algorithm is made of several steps. As
a first step, duplications are processed; for each duplication
of a set R in the source (target, respectively), a new set R i
is added to the source (target, respectively). Then, the algorithm finds all sets in the source and in the target schema;
this corresponds, in the terminology of [18], to finding primary paths.
The next step is concerned with generating views over the
source and the target. Views are a generalization of logical
relations in [18] and are the building blocks for tgds. Each
view is an algebraic expression over sets in the data source.
Let us now restrict our attention to the source (views in the
target are generated in a similar way).
The set of views, Vinit , is initialized as follows: for each
set R a view R is generated. This initial set of views is then
processed in order to chase join constraints and assemble
complex views; intuitively, chasing a join constraint from set
R to set R’ means to build a view that corresponds to the
join of R and R’ . As such, each join constraint can be seen as
an operator that takes a set of existing views and transforms
them into a new set, possibly adding new views or changing
the input ones. Join constraints can be mandatory or non
mandatory; intuitively, a mandatory join constraint states
that two sets must either appear together in a view, or not
appear at all.
Once views have been generated for the source and the
target schema, it is possible to produce a number of candidate tgds. We say that a source view v covers a value
correspondence AS → AT if AS is an attribute of a set that
appears in v; similarly for target views. We generate a candidate tgd for each pair made of a source view and a target
view that covers at least one correspondence. The source
view generates the left-hand side of the tgd, the target view
the right-hand side; lines are used to generate universally
quantified variables in the tgd; for each attribute in the target view that is not covered by a line, we add an existentially
quantified variable.

5.

A key contribution of this paper is the definition of a
rewriting algorithm that takes as input a set of source-totarget tgds Σ and rewrites them into a new set of constraints
Σ′ with the nice property that, given a source instance I ,
the canonical solution for Σ′ on I coincides with the core of
Σ on I .
We make the assumption that the set Σ is source-based.
A tgd φ(x) → ∃y(ψ(x, y)) is source-based if: (i) the lefthand side φ(x) is not empty; (ii) the vector of universally
quantified variables x is not empty; (iii) at least one of the
variables in x appears in the right hand side ψ(x, y).
This definition, while restricting the variety of tgds handled by the algorithm, captures the notion of a “useful” tgd in
a schema mapping scenario. In fact, note that tgds in which
the left-hand side is empty or it contains no universally quantified variables – like, for example → ∃X, Y : T (X, Y ), or
∀a : S(a) → ∃X, Y : R(X, Y )∧S(Y, X) – would generate target tuples made exclusively of nulls, which are hardly useful
in practical cases.
Besides requiring that tgds are source-based, without loss
of generality we also require that the input tgds are in in normal form, i.e., each tgd uses distinct variables, and no tgd
can be decomposed in two different tgds having the same
left-hand side. To formalize this second notion, let us introduce the Gaifman graph of a formula as the undirected
graph in which each variable in the formula is a node, and
there is an edge between v1 and v2 if v1 and v2 occur in
the same atom. The dual Gaifman graph of a formula is
an undirected graph in which nodes are atoms, and there is
an edge between atoms Ri (xi , y i ) and Rj (xj , y j ) if there is
some existential variable yk occurring in both atoms.
Definition: A set of tgds Σ is in normal form if: (i) for each
mi , mj ∈ Σ, (xi ∪y i )∩(xj ∪y j ) = ∅, i.e, the tgds use disjoint
sets of variables; (ii) for each tgd, the dual Gaifman graph
of atoms is connected.
If the input set of tgds is not in normal form, it is always
possible to preliminarily rewrite them to obtain an input in
normal form.6

TGD REWRITING

5.1

We are now ready to introduce the rewriting algorithm.
We concentrate on data exchange settings expressed as a
set of source-to-target tgds, i.e., we do not consider target
tgds and egds. Target constraints are used to express key
and foreign key constraints on the target. With respect to
target tgds, we assume that the source-to-target tgds have
been rewritten in order to incorporate any target tgds corresponding to foreign key constraints. In [10] it is proven that
it is always possible to rewrite a data exchange setting with
a set of weakly acyclic [9] target tgds into a setting with no
target tgds such that the cores of the two settings coincide,
provided that the target tgds satisfy a boundedness property. With respect to key constraints, they can be enforced
in the final SQL script after the core for the source-to-target
tgds has been generated.5

Formula Homomorphisms

An important intuition behind the algorithm is that by
looking at homomorphisms between tgd conclusions, we may
identify when firing one tgd may lead to the generation of
“redundant” tuples in the target. To formalize this idea,
we introduce the notion of formula homomorphism, which
is reminiscent of the notion of containment mapping used
in [16]. We find it useful to define homomorphisms among
variable occurrences, and not among variables.
Definition: Given an atom R(A1 : v1 , . . . , Ak : vk ) in a
formula ψ(x, y), a variable occurrence is a pair R.Ai : vi .
We denote by occ(ψ(x, y)) the set of variable occurrences in
ψ(x, y). A variable occurrence R.Ai : vi ∈ occ(ψ(x, y)) is a
universal occurrence if vi is a universally quantified variable;
it is a Skolem occurrence if vi is an existentially quantified
variable that occurs more than once in ψ(x, y); it is a pure
null occurrence if vi is an existentially quantified variable
that occurs only once in ψ(x, y).
Intuitively, the term “pure null” is used to denote those
variables that generate labeled nulls that can be safely re-

the correspondence must be applied; our system handles the most
general form of correspondences; it also handles constant lines.
It is possible to extend the algorithms presented in this paper to
handle the most general form of correspondence; this would be
important in order to incorporate conditional tgds [6]; while the
extension is rather straightforward for constants appearing in tgd
premises, it is more elaborate for constants in tgd conclusions,
and is therefore left to future work.
5
The description of the algorithm is out of the scope of this paper.

6

In case the dual Gaifman graph of a tgd is not connected, we
generate a set of tgds with the same premise, one for each connected component in the dual Gaifman graph.

660

placed with ordinary null values in the final instance. There
is a precise hierarchy in terms of information content associated with each variable occurrence. More specifically, we
say that a variable occurrence o2 is more informative than
variable occurrence o1 if one of the following holds: (i) o2 is
universal, and o1 is not; (ii) o2 is a Skolem occurrence and
o1 is a pure null.
Definition: Given two formulas, ψ1 (x1 , y 1 ), ψ2 (x2 , y 2 ), a
variable substitution, h, is an injective mapping from the set
occ(ψ1 (x1 , y 1 )) to occ(ψ2 (x2 , y 2 )) that maps universal occurrences into universal occurrences. In the following we shall
refer to the variable occurrence h(R.Ai : xi ) by the syntax
Ai : hR.Ai (xi ).

appears more than once in the conclusion. In this case we
say that m contains self-joins in tgd conclusions.
(i) a subsumption scenario is a data exchange scenario in
which ΣST may only contain simple endomorphisms, and no
tgd contains self-joins in tgd conclusions.
(ii) a coverage scenario is a scenario in which ΣST may
contain arbitrary endomorphisms, but no tgd contains selfjoins in tgd conclusions.
(iii) a general scenario is a scenario in which ΣST may
contain tgds with arbitrary self-joins.
In the following sections, we introduce the rewriting for
each of these categories.

Definition: Given two sets of atoms R1 , R2 , a formula homomorphism is a variable substitution h such that, for each
atom R(A1 : v1 , . . . , Ak : vk ) ∈ R1 , it is the case that: (i)
R(A1 : hR.A1 (v1 ), . . . , Ak : hR.Ak (vk )) ∈ R2 ; (ii) for each
pair of existential occurrences Ri .Aj : v, Ri′ .A′j : v in R1
it is the case that either hRi .Aj (v) and hRi′ .A′j (v) are both
universal or hRi .Aj (v) = hRi′ .A′j (v).

5.2

Subsumption Scenarios

Definition: Given two tgds m1 , m2 , whenever there is a
simple homomorphism h from ψ1 (x1 , y 1 ) to ψ2 (x2 , y 2 ), we
say that m2 subsumes m1 , in symbols m1  m2 . If h is
proper, we say that m2 properly subsumes m1 , in symbols
m1 ≺ m2 .
Subsumptions are very frequent and can be handled efficiently. One example is the references scenario in Section 2.
There, as discussed, the only endomorphisms in the righthand sides of tgds are simple endomorphisms that map an
entire tgd conclusion into another conclusion. Then, it may
be the case that the two tgds are instantiated with value
assignments a, a′ and produce two sets of facts ψ(a, b) and
ψ′ (a′ , b′ ) such there is an endomorphism that maps ψ(a, b)
into ψ′ (a′ , b′ ). In these cases, whenever m2 subsumes m1 ,
we rewrite m1 by adding to the its left-hand side the negation of the left-hand side of m2 ; this prevents the generation
of redundant tuples.
Note that a set of tgds may contain both proper and
non-proper subsumptions. However, only proper ones introduce actual redundancy in the final instance; non-proper
subsumptions generate tuples that are identical up to the
renaming of nulls and therefore are filtered-out by the semantics of the chase. As a consequence, for performance
purposes it is convenient to concentrate on proper subsumptions.
We can now introduce the rewriting of the original set of
source-to-target tgds Σ into a new set of tgds, Σ′ , as follows.
Definition: For each m = φ(x) → ∃y(ψ(x, y)) in Σ, add to
Σ′ a new tgd msubs = φ′ (x′ ) → ∃y ′ (ψ′ (x′ , y ′ )), obtained by
rewriting m as follows:
(i) initialize msubs = m;
(ii) for each tgd ms = φs (xs ) → ∃y s (ψs (xs , y s )) in Σ such
that m ≺ ms , call h the homomorphism of m into ms ; add
to φ′ (x′ ) a negated sub-formula ∧¬(γs ), where γs is obtained
as follows:
(ii.a) initialize γs = φs (xs );
(ii.b) for each pair of existential occurrences Ri .Aj : v,
Ri′ .A′j : v in ψ(x, y) such that hRi .Aj (v) and hRi′ .A′j (v) are
both universal, add to γs an equation of the form hRi .Aj (v)
= hRi′ .A′j (v);
(ii.c) for each universal position Ai : xi in ψ(x, y), add to
γs an equation of the form xi = hR.Ai (xi ). Intuitively, the
latter equations correspond to computing differences among
instances of the two formulas.
Consider again the W example in the previous paragraph.
The tgds in normal form are reported below. Based on the

Given a set of tgds ΣST = {φi (xi ) → ∃y i (ψi (xi , y i )), i =
1, . . . , n}, a simple formula endomorphism is a formula homomorphism from ψi (xi , y i ) to ψj (xj , y j ), for some i, j ∈
{1, . . . , n}. ASformula endomorphism
is a formula homomorSn
phism from n
i=1 ψi (xi , y i ) − {ψj (xj , y j )}
i=1 ψi (xi , y i ) to
for some j ∈ {1, . . . , n}.
Definition: A formula homomorphism is said to be proper if
either the size of R2 is greater than the size of R1 or there
exists at least one occurrence R.Ai : vi in R1 such that
hR.Ai (vi ) is more informative than R.Ai : vi .
To give an example, consider the following tgds. Suppose
relation W has three attributes, A, B, C:
m1 . A(x1 ) → ∃Y0 , Y1 : W(x1 , Y0 , Y1 )
m2 . B(x2 , x3 ) → ∃Y2 : W(x2 , x3 , Y2 )
m3 . C(x4 ) → ∃Y3 , Y4 : W(x4 , Y3 , Y4 ), V(Y4 )
There are two different formula homomorphisms: (i) the
first maps the right-hand side of m1 into the rhs of m2 :
W.A : x1 → W.A : x2 , W.B : Y0 → W.B : x3 , W.C : Y1 →
W.C : Y2 ; (ii) the second maps the rhs of m1 into the rhs
of m3 : W.A : x1 → W.A : x4 , W.B : Y0 → W.B : Y3 , W.C :
Y1 → W.C : Y4 . Both homomorphisms are proper.
Note that every standard homomorphism h on the variables of a formula induces a formula homomorphism h that
associates with each occurrence of a variable v the same
value h(v). The study of formula endomorphisms provides
nice necessary conditions for the presence of endomorphisms
in the solutions of an exchange problem.
Theorem 5.1 (Necessary Condition). Given a data
exchange setting (S, T, ΣST ), suppose ΣST is a set of sourcebased tgds in normal form. Given an instance I of S, call
J a universal solution
for I. If J contains a proper endoS
morphism, then i ψi (xi , y i ) contains a proper formula endomorphism.
Typically, the canonical solution contains a proper endomorphism into its core. It is useful, for application purposes, to classify data exchange scenarios in various categories, based on the complexity of core identification. To
do this, as discussed in Section 2, special care needs to be
devoted to those tgds m in which the same relation symbol

661

{R.1 : a5 → R.1 : a2 , R.2 : N50 → R.2 : b2 , S.1 : N50 →
S.1 : a3 , S.2 : N51 → S.2 : c3 , T.1 : N51 → T.1 : a4 ,
T.2 : b5 → T.2 : b4 T.3 : N52 → T.3 : N4 }. Based on this,
we rewrite tgd m5 as follows:

proper subsumptions, we can rewrite mapping m1 as follows:
m′1 . A(x1 ) ∧ ¬(B(x2 , x3 ) ∧ x1 = x2 )
∧¬(C(x4 ) ∧ x1 = x4 ) → ∃Y0 , Y1 W(x1 , Y0 , Y1 )
By looking at the logical expressions for the rewritten tgds it
can be seen how we have introduced negation. Results that
have been proven for data exchange with positive tgds extend to tgds with safe negation [14]. To make negation safe,
we assume that during the chase universally quantified variables range over the active domain of the source database.
This is reasonable since – as it was discussed in Section 2 –
the rewritten tgds will be translated into a relational algebra
expression.

5.3

m′5 . E(a5 , b5 ) ∧ ¬(A(a1 , b1 , c1 ) ∧ a5 = a1 ∧ b5 = b1 )
∧¬(B(a2 , b2 ) ∧ F 1 (a3 , b3 ) ∧ F 2 (b3 , c3 ) ∧ D(a4 , b4 )
∧ b2 = a3 ∧ c3 = a4 ∧ a5 = a2 ∧ b5 = b4 )
→ R(a5 , N50 ) ∧ S(N50 , N51 ) ∧ T(N51 , b5 , N52 )
It is possible to prove the following result:
Theorem 5.2 (Core Computation). Given a data exchange setting (S, T, ΣST ), suppose ΣST is a set of sourcebased tgds in normal form that do not contain self-joins in
tgd conclusions. Call Σ′ST the set of coverage rewritings of
ΣST . Given an instance I of S, call J, J’ the canonical solutions of ΣST and Σ′ST for I. Then J’ is the core of J.

Coverage Scenarios

Consider now the case in which the tgds contain endomorphisms that are not simple subsumptions; recall that we are
still assuming the tgds contain no self-joins in their conclusions. Consider the genes example in Section 2. Tgd m3 in
that example states that the target must contain two tuples,
one in the Gene table and one in the Protein table that join
on the protein attribute. However, this constraint do not
necessarily must be satisfied by inventing a new value. In
fact, there might be tuples generated by m1 and m2 that
satisfy the constraint imposed by m3 . Informally speaking,
a coverage for the conclusion of a tgd is a set of atoms from
other tgds that might represent alternative ways of satisfying the same constraint.
Definition: Assume that, for S
tgd m = φ(x) →
S ∃y(ψ(x, y)),
there is an endomorphism
h : i ψi (xi , y i ) → i ψi (xi , y i ) −
S
{ψ(x, y)}. Call i ψi (xi , y i ) a minimal set of formulas such
that h maps
S each atom Ri (. . .) in ψ(x, y) into some atom
Ri (. . .) of i ψi (xi , y i ) a coverage of m; note that if i equals
1 the coverage becomes a subsumption.
The rewriting algorithm for coverages is made slightly
more complicated by the fact that proper join conditions
must in general be added among coverage premises.
Definition: For each m = φ(x) → ∃y(ψ(x, y)) in Σ, add to
Σ′ a new tgd mcov = φ′ (x′ ) → ∃y ′ (ψ′ (x′ , y ′ )), obtained as
follows:
(i) initialize mcov = m
Ssubs , as defined above;
(ii) for each coverage Si ψi (xi , y i ) of m, call h the homomorphism of ψ(x, y) into i ψi (xi , y i ); add to φ′ (x′ ) a negated
sub-formula ∧¬(γc ),Vwhere γc is obtained as follows:
(iia) initialize γc = i φi (xi );
(iib) for each universal position Ai : xi in ψ(x, y), add to γc
an equation of the form xi = hR.Ai (xi )
(iic) for each existentially quantified variable y in ψ(x, y),
and any pair of positions Ai : y, Aj : y such that hR.Ai (y)
and hR.Aj (y) are universal variables, add to γc an equation
of the form hR.Ai (y) = hR.Aj (y).
To see how the rewriting works, consider the following
example (existentially quantified variables are omitted since
they should be clear from the context):
m1 .
m2 .
m3 .
m4 .
m5 .

The proof is based on the fact that, whenever two tgds
m1 , m2 in ΣST are fired to generate an endomorphism, several homomorphisms must be in place. Call a1 , a2 the variable assignments used to fire m1 , m2 ; suppose there is an
homomorphism h from ψ1 (a1 , b1 ) to ψ2 (a2 , b2 ). Then, by
Theorem 5.1, we know that there must be a formula homomorphism h′ from ψ1 (x1 , y 1 ) to ψ2 (x2 , y 2 ), and therefore a
rewriting of m1 in which the premise of m2 is negated. By
composing the various homomorphism it is possible to show
that the rewriting of m1 will not be fired on assignment a1 .
Therefore, the endomorphism will not be present in J’.

6.

REWRITING TGDS WITH SELF-JOINS

The most general scenario is the one in which one relation symbol may appear more than once in the right-hand
side of a tgd. This introduces a significant difference in the
way redundant tuples may be generated in the target, and
therefore increases the complexity of core identification.
There are two reasons for which the rewriting algorithm
introduced above does not generate the core. Note that the
algorithm removes redundant tuples by preventing a tgd to
be fired for some value assignment. Therefore, it prevents
redundancy that comes from instantiations of different tgds,
but it does not control redundant tuples generated within
an instantiation of a single tgd. In fact, if a tgd writes two
or more tuples at a time into a relation R, solutions may still
contain unnecessary tuples. As a consequence, we need to
rework the algorithm in a way that, for a given instantiation
of a tgd, we can intercept every single tuple added to the
target by firing the tgd, and remove the unnecessary ones.
In light of this, our solution to this problem is to adopt a
two-step process, i.e., to perform a double exchange.

6.1

A(a1 , b1 , c1 ) → R(a1 , N10 ) ∧ S(N10 , N11 ) ∧ T(N11 , b1 , c1 )
B(a2 , b2 ) → R(a2 , b2 )
F 1 (a3 , b3 ) ∧ F 2 (b3 , c3 ) → S(a3 , c3 )
D(a4 , b4 ) → T(a4 , b4 , N4 )
E(a5 , b5 ) → R(a5 , N50 ) ∧ S(N50 , N51 ) ∧ T(N51 , b5 , N52 )

Consider tgd m5 . It is subsumed by m1 . It is also covered
by {R(a2 , b2 ), S(a3 , c3 ), T(a4 , b4 , N4 )}, by homomorphism:

662

The Double Exchange

Given a set of source-to-target tgds, ΣST over S and T, as
a first step we normalize the input tgds; we also introduce
suitable duplications of the target sets in order to remove
self-joins. A duplicate of a set R is an exact copy named
Ri of R. By doing this, we introduce a new, intermediate
schema, T’, obtained from T. Then, we produce a new set
of tgds ΣST ′ over S and T’ that do not contain self-joins.
Definition: Given a mapping scenario (S, T, ΣST ) where
ΣST contains self-joins in tgd conclusions, the intermediate
scenario (S, T’, ΣST ′ ) is obtained as follows: for each tgd
m in ΣST add a tgd m′ to ΣST ′ such that m′ has the same

premise as m and for each target atom R(x, y) in m, m′ contains a target atom Ri (x, y), where Ri is a fresh duplicate
of R.
To give an example, consider the RS example in [11]. The
original tgds are reported below:

obviously be satisfied by copying to the target one atom in
S 1 , one in S 2 and one in S 3 . This corresponds to the base
expansion of the view, i.e., the expansion that corresponds
with the base view itself:
e11 .S 1 (x5 , b, x1 , x2 , a) ∧ S 2 (x5 , c, x3 , x4 , a) ∧ S 3 (d, c, x3 , x4 , b)

m1 . R(a, b, c, d) → ∃x1 , x2 , x3 , x4 , x5 : S(x5 , b, x1 , x2 , a)∧
S(x5 , c, x3 , x4 , a) ∧ S(d, c, x3 , x4 , b)
m2 . R(a, b, c, d) → ∃x1 , x2 , x3 , x4 , x5 : S(d, a, a, x1 , b)∧
S(x5 , a, a, x1 , a) ∧ S(x5 , c, x2 , x3 , x4 )

However, there are also other ways to satisfy the constraint.
One way is to use only one tuple from S 2 and one from S 3 ,
the first one in join with itself on the first attribute – i.e., S 2
is used to “cover” the S 1 atom; this may work as long as it
does not conflict with the constants generated in the target
by the base view; in our example, the values generated by
the S 2 atom must be consistent with those that would be
generated by the S 1 atom we are eliminating. We write this
second expansion as follows:

In that case, ΣST ′ will be as follows (variables have been
renamed to normalize the tgds):
m′1 . R(a, b, c, d) → ∃x1 , x2 , x3 , x4 , x5 : S 1 (x5 , b, x1 , x2 , a)∧
S 2 (x5 , c, x3 , x4 , a) ∧ S 3 (d, c, x3 , x4 , b)
m′2 . R(e, f, g, h) → ∃y1 , y2 , y3 , y4 , y5 : S 4 (h, e, e, y1 , f )∧
S 5 (y5 , e, e, y1 , e) ∧ S 6 (y5 , g, y2 , y3 , y4 )

e12 . S 2 (x5 , c, x3 , x4 , a) ∧ S 3 (d, c, x3 , x4 , b)
∧ (S 1 (x5 , b, x1 , x2 , a) ∧ b = c)

We execute this ST ′ exchange by applying the rewritings
discussed in the previous sections. This yields an instance
of T’ that needs to be further processed in order to generate the final target instance. To do this, we need to execute
a second exchange from T’ to T. This second exchange is
constructed in such a way to generate the core. The overall
process is shown in Figure 6. Note that, while we describe

It is possible to see that – from the algebraic viewpoint – the
formula requires to compute a join between S 2 and S 3 , and
then an intersection with the content of S 1 . This is even
more apparent if we look at another possible extension, the
one that replaces the three atoms with a single covering atom
from S 4 in join with itself:
e13 . S 4 (h, e, e, y1 , f ) ∧ S 4 (h′ , e′ , e′ , y1 , f ′ ) ∧ h = h′ ∧
(S 1 (x5 , b, x1 , x2 , a) ∧ S 2 (x5 , c, x3 , x4 , a) ∧ S 3 (d, c, x3 , x4 , b)∧
e = b ∧ f = a ∧ e′ = c ∧ f ′ = a ∧ h′ = d ∧ e′ = c ∧ f ′ = b)

Figure 6: Double Exchange

In algebraic terms, expansion e13 corresponds to computing
the join S 4 1 S 4 and then taking the intersection on the
appropriate attributes with the base view, i.e., S 1 1 S 2 1
S 3.
A similar approach can be used for tgd m′2 above. In this
case, besides the base expansion, it is possible to see that
also the following expansion is derived – S 4 covers S 5 and
S 3 covers S 6 , the join is on the universal variables d and h:

our algorithm as a double exchange, in our SQL scripts we
do not actually implement two exchanges, but only one exchange with a number of additional intermediate views to
simplify the rewriting.
Remark The problem of core generation via executable
scripts has been independently addressed in [21]. There the
authors show that it is possible to handle tgds with self-joins
using one exchange only.

6.2

e21 . S 4 (h, e, e, y1 , f ) ∧ S 3 (d, c, x3 , x4 , b) ∧ h = d ∧
(S 5 (y5 , e, e, y1 , e) ∧ S 6 (y5 , g, y2 , y3 , y4 ) ∧ f = e ∧ g = c)

Expansions

As a first step of the rewriting, for each ST’ tgd, we take the
conclusion, and compute all possible expansions, including
the base expansion. The algorithm to generate expansions
is very similar to the one to compute coverages described
in the first section, with several important differences. In
particular, we need to extend the notion of homomorphism
in such a way that atoms corresponding to duplicates of the
same set can be matched.
Definition: We say that two sets R and R′ are equal up to
duplications if they are equal, or one is a duplicate of the
other, or both are duplicates of the same set. Given two sets
of atoms R1 , R2 , an extended formula homomorphism, h, is
defined as a formula homomorphism, with the variant that
h is required to map each atom R(A1 : v1 , . . . , Ak : vk ) ∈ R1
into an atom R′ (A1 : hR.A1 (v1 ), . . . , Ak : hR.Ak (vk )) ∈ R2
such that R and R′ are not necessarily the same symbol but
are equal up to duplications.
Note that, in terms of complexity, another important difference is that in order to generate expansions we do not
need to exclusively use atoms in other tgds, but may reuse
atoms from the tgd itself. Also, the same
S atom may be used
multiple times in an expansion. Call i ψi (xi , y i ) the union

Although inspired by the same intuitions, the algorithm
used to generate the second exchange is considerably more
complex than the ones discussed before. The common intuition is that each of the original source-to-target tgds represents a constraint that must be satisfied by the final instance.
However, due to the presence of duplicate symbols, there
are in general many different ways of satisfying these constraints. To give an example, consider mapping m′1 above:
it states that the target must contain a number of tuples in
S that satisfy the two joins in the tgd conclusion. It is important to note, however, that: (i) it is not necessarily true
that these tuples must belong to the extent of S 1 , S 2 , S 3 –
since these are pure artifacts introduced for the purpose of
our algorithm – but they may also come from S 4 or S 5 or
S 6 ; (ii) moreover, these tuples are not necessarily distinct,
since there may be tuples that perform a self-join.
In light of these ideas, as a first step of our rewriting
algorithm, we compute all expansions of the conclusions of
the ST’ tgds. Each expansion represents one of the possible
ways to satisfy the constraint stated by a tgd. For each tgd
mi ∈ ΣST ′ , we call ψi (xi , y i ) a base view. Consider again
tgd m′1 above; the constraint stated by its base view may

663

chasing e13 generates one single tuple that subsumes all of
the tuples above: S(k, n, n, N1 , n). We can easily identify
this fact by finding an homomorphism from e11 to e12 and
e13 , and an homomorphism from e12 into e13 . We rewrite
expansions accordingly by adding negations as in the first
exchange.
Definition: Given expansions e = c ∧ i and e′ = c′ ∧ i′ of
the same base view, we say that e′ is more compact than e
if there is a formula homomorphism h from the set of atoms
Rc in c to the set of atoms Rc′ in c′ and either the size of
Rc′ is smaller than the size of Rc or there exists at least
one occurrence R.Ai : vi in Rc such that hR.Ai (vi ) is more
informative than R.Ai : vi .
This definition is a generalization of the definition of a
subsumption among tgds. Given expansion e, we generate a
first rewriting of e, called erew , by adding to e the negation
¬(e′ ) of each expansion e′ of the same base view that is
more compact than e, with the appropriate equalities, as
for any other subsumption. This means, for example, that
expansion e12 above is rewritten into a new formula erew
12 as
follows:

of all atoms in the conclusions of ΣST ′ . To compute its expansions, if the base view has
S size k, we consider all multisets
of size k or less of atoms in i ψi (xi , y i ). If one atom occurs
more than once in a multiset, we assume that variables are
properly renamed to distinguish the various occurrences.
Definition: Given
S a base view ψ(x, y) of size k, a multiset
R of atoms in i ψi (xi , y i ) of size k or less, and an extended
formula homomorphism h from ψ(x, y) to R, an expansion
eR,h is a logical formula of the form c ∧ i, where:
(i) c – the coverage formula – is constructed as follows:
(ia) initialize c = R;
(ib) for each existentially quantified variable y in ψ(x, y),
and any pair of positions Ai : y, Aj : y such that hR.Ai (y)
and hR.Aj (y) are universal variables, add to c an equation
of the form hR.Ai (y) = hR.Aj (y).
(ii) i – the intersection formula – is constructed as follows:
(iia) initialize i = ψ(x, y);
(iib) for each universal position Ai : xi in ψ(x, y), add to i
an equation of the form xi = hR.Ai (xi ).
Note that for base expansions the intersection part can
be removed. It can be seen that the number of coverages
may significantly increase when the number of self-joins increase.7 In the RS example our algorithm finds 10 expansions of the two base views, 6 for the conclusion of tgd m′1
and 4 for the conclusion of tgd m′2 .

6.3

2
3
erew
12 . S (x5 , c, x3 , x4 , a) ∧ S (d, c, x3 , x4 , b)
1
∧(S (x5 , b, x1 , x2 , a) ∧ b = c)
∧¬(S 4 (h, e, e, y1 , f ) ∧ h = h′ ∧ S 4 (h′ , e′ , e′ , y1′ , f ′ )∧
(S 1 (x′5 , b′ , x′1 , x′2 , a′ ) ∧ S 2 (x′5 , c′ , x′3 , x′4 , a′ )
∧ S 3 (d′ , c′ , x′3 , x′4 , b′ )
∧ e = b′ ∧ f = a′ ∧ e′ = c′ ∧ f ′ = a′ ∧ h′ = d′ ∧ f ′ = b′ )
∧ c = e ∧ a = f ∧ d = h′ ∧ c = e′ ∧ b = f ′ )

T’T Tgds

Expansions represent all possible ways in which the original constraints may be satisfied. Our idea is to use expansions as premises for the T’T tgds that actually write to the
target. The intuition is pretty simple: for each expansion e
we generate a tgd. The tgd premise is the expansion itself,
e. The tgd conclusion is the formula eT , obtained from e
by replacing all duplicate symbols by the original one. To
give an example, consider expansion e12 above. It generates
a tgd like the following:

After we have rewritten the original expansion in order to
remove unnecessary tuples, we look among other expansions
to favor those that generate ‘more informative’ tuples in the
target. To see an example, consider expansion e12 above: it
is easy to see that – once we have removed tuples for which
there are more compact expansions – we have to ensure that
expansion e21 of the other tgd does not generate more informative tuples in the target.
Definition: Given expansions e = c ∧ i and e′ = c′ ∧ i′ , we
say that e′ is more informative than e if there is a proper
homomorphism from the set of atoms Rc in c to the set of
atoms Rc′ in c′ .
To summarize, to generate the final rewriting, we consider
the premise, e, of each T’T tgd; then: (i) we first rewrite e
into a new formula erew by adding the negation of all expansions ei of the same base view such that ei is more compact
′
than e; (ii) we further rewrite erew into a new formula erew
rew
by adding the negation of ej , for all expansions ej such
that ej is more informative than e. In the RS example our
algorithm finds 21 subsumptions due to more compact expansions of the same base view, and 16 further subsumptions
due to more informative expansions.
As a final step, we have to look for proper subsumptions
among the normalized tgds to avoid that useless tuples are
copied more than once to the target. For example, tuple
S(N1 , h, k, l, m) – where N1 is not in join with other tuples,
and therefore is a “pure” null – is redundant in presence of
a tuple S(N2 , h, k, l, m) or in the presence of S(i, h, k, l, m).
This yields our set of rewritten T’T tgds.
Also in this case it is possible to prove that chasing these
rewritten tgds generates core solutions for the original ST
tgds.

S 2 (x5 , c, x3 , x4 , a) ∧ S 3 (d, c, x3 , x4 , b)
∧(S 1 (x5 , b, x1 , x2 , a) ∧ b = c) → ∃N3 , N4 , N5 :
→ S(N5 , c, N3 , N4 , a) ∧ S(d, c, N3 , N4 , b)
Before actually executing these tgds, two preliminary steps
are needed. As a first step, we need to normalize the tgds,
since conclusions are not necessarily normalized. Second,
as we already did in the first exchange, we need to suitably rewrite the tgds in order to prevent the generation of
redundant tuples.

6.4

T’T Rewriting

To generate the core, we now need to identify which expansions may generate redundancy in the target. In essence,
we look for subsumptions among expansions, in two possible
ways.
First, among all expansions of the same base view, we
try to favor the ‘most compact’ ones, i.e., those that generate less tuples in the target. To see an example, consider the source tuple R(n, n, n, k); chasing the tuple using
the base expansion e11 generates in the target three tuples:
S(N5 , n, N1 , N2 , n), S(N5 , n, N3 , N4 , n), S(k, n, N3 , N4 , n); if,
however, we chase expansion e12 , we generate in the target only two tuples: S(N5 , n, N3 , N4 , n), S(k, n, N3 , N4 , n);
7
Note that, as an optimization step, many expansions can be
pruned out by reasoning on existential variables.

664

6.5

Skolem Functions

7. COMPLEXITY AND APPROXIMATIONS

Our final goal is to implement the computation of cores
via an executable script, for example in SQL. In this respect,
great care is needed in order to properly invent labeled nulls.
A common technique to do this is to use Skolem functions.
A Skolem function is usually an uninterpreted term of the
form fsk (v1 , v2 , . . . , vk ), where each vi is either a constant
or a term itself.
An appropriate choice of Skolem functions is crucial in
order to correctly reproduce in the final script the semantics
of the chase. Recall that, given a tgd φ(x) → ∃y(ψ(x, y))
and a value assignment a, that is, an homomorphism from
φ(x) into I, before firing the tgd the chase procedure checks
that there is no extension of a that maps φ(x) ∪ ψ(x, y)
into the current solution. In essence, the chase prevents the
generation of different instantiations of a tgd conclusion that
are identical up to the renaming of nulls.
We treat Skolem functions as interpreted functions that
encode their arguments as strings. We call a string generated by a Skolem function a Skolem string. Whenever a
tgd is fired, existential variables in tgd conclusion are associated with a Skolem string; the Skolem string is then used
to generate a unique (integer) value for the variable.
We may see the block of facts obtained by firing a tgd
as a hypergraph in which facts are nodes and null values
are labeled edges that connect the facts. Each null value
that corresponds to an edge of this hypergraph requires an
appropriate Skolem function. To correctly reproduce the
desired semantics, the Skolem functions for a tgd m should
be built is such a way that, if the same tgd or another tgd is
fired and generates a block of facts that is identical to that
generated by m up to nulls, the Skolem strings are identical.
To implement this behavior in our scripts, we embed in the
function a full description of the tgd instantiation, i.e., of
the corresponding hypergraph. Consider for example the
following tgd:

A few comments are worth making here on the complexity of core computations. In fact, the three categories of
scenarios discussed in the previous sections have considerably different complexity bounds. Recall that our goal is to
execute the rewritten tgds under the form of SQL scrips; in
the scripts, negated atoms give rise to difference operators.
Generally speaking, differences are executed very efficiently
by the DBMS under the form of sort-scans. However, the
number of differences needed to filter out redundant tuples
depends on the nature of the scenario.
As a first remark, let us note that subsumptions are nothing but particular forms of coverages; nevertheless, they
deserve special attention since they are handled more efficiently than coverages. In a subsumption scenario the number of differences corresponds to the number of subsumptions. Consider the graph of the subsumption relation obtained by removing transitive edges. In the worst case – the
graph is a path – there are O(n2 ) subsumptions. However,
this is rather unlikely in real scenarios. Typically, the graph
is broken into several smaller connected components, and
the number of differences is linear in the number of tgds.
The worst-case complexity of the rewriting is higher for
coverage scenarios, for two reasons. First, coverages always
require to perform additional joins before computing the actual difference. Second, and more important, if we call k
the number of atoms in a tgd, assume each atom can be
mapped into n other atoms via homomorphisms; then we
need to generate nk different coverages, and therefore nk
differences.
This exponential bound on the number of coverages is
not surprising. In fact, Gottlob and Nash have shown that
the problem of computing core solutions is fixed-parameter
intractable[13] wrt the size of the tgds (in fact, wrt the size of
blocks), and therefore it is very unlikely that the exponential
bound can be removed. We want to emphasize however that
we are talking about expression complexity and not data
complexity (the data complexity remains polynomial).
Despite this important difference in complexity between
subsumptions and coverages, coverages can usually be handled quite efficiently. In brief, the exponential bound is
reached only under rather unlikely conditions; to see why,
recall that coverages tend to follow this pattern:

R(a, b, c) → ∃N0 , N1 : S(a, N0 ), T(b, N0 , N1 ), W(N1 )
The Skolem functions for N0 and N1 will have three arguments: (a) the sequence of facts generated by firing the tgd
(existential variables omitted), i.e., an encoding of the graph
nodes; (ii) the sequence of joins imposed by existential variables, i.e., an encoding of the graph edges; (iii) a reference
to the specific variable for which the function is used. The
actual functions would be as follows:

m1 : A(a, b) → R(a, b)
m2 : B(a, b) → S(a, b)
m3 : C(a, b) → ∃N : R(a, N ), S(b, N )

fsk ({S(A:a),T(A:b),W()},{S.B=T.B, T.C=W.A}, S.B=T.B)
fsk ({S(A:a),T(A:b),W()},{S.B=T.B, T.C=W.A}, T.C=W.A)

Note that m1 and m2 write into the key–foreign key pair,
while m3 invents a value. Complexity may become an issue, here, only if the set of tgds contains a significant number of other tgds like m1 and m2 which write into R and
S separately. This may happen only in those scenarios in
which a very large number of different data sources with a
poor design of foreign key relationships must be merged into
the same target, which can hardly be considered a frequent
case. In fact, in our experiments with both real-life scenarios and large randomly generated schemas, coverages have
never been an issue.
Computing times are usually higher for scenarios with selfjoins in tgd conclusions. In fact, the exponential bound is
more severe in these cases. If we call n the number of atoms
in tgd conclusions, since the construction of expansions requires to analyze all possible subsets of atoms in tgd con-

An important point here is that set elements must be encoded in lexicographic order, so that the functions generate
appropriate values regardless of the order in which atoms
appear in the tgd. This last requirement introduces further subtleties in the way exchanges with self-joins are handled. In fact, note that in tgds like the one above – in
which all relation symbols in the conclusion are distinct
– the order of set elements can be established at script
generation time (they depend on relation names). If, on
the contrary, the same atom may appear more than once
in the conclusion, then functions of this form are allowed:
fsk ({S(A:a),S(A:b)},{S.B=S.B}). It can be seen how facts
must be reordered at execution time, based on the actual
assignment of values to variables.

665

clusions,8 a bound of 2n is easily reached. Therefore, the
number of joins, intersections and differences in the final
SQL script may be very high. In fact, it is not difficult to
design synthetic scenarios like the RS one discussed above
that actually trigger the exponential explosion of rewritings.
However, in more realistic scenarios containing self-joins,
the overhead is usually much lower. To understand why,
let us note that expansions tend to increase when tgds are
designed in such a way that it is possible for a tuple to
perform a join with itself. In practice, this happens very
seldom. Consider for example a Person(name, father) relation, in which children reference their father. It can be
seen that no tuple in the Person table actually joins with
itself. Similarly, in a Gene(name, type, protein) table, in
which “synonym” genes refer to their “primary” gene via the
protein attribute, since no gene is at the same time a synonym and a primary gene. In light of these ideas, we may

We selected a set of seven experiments to compare execution times of the two approaches. The seven experiments include two scenarios with subsumptions, two with coverages,
and three with self-joins in the target schema. The scenarios have been taken from the literature (two from [11], one
from [22]), and from the STMark benchmark. Each test has
been run with 10k, 100k, 250k, 500k, and 1M tuples in the
source instance. On average we had 7 tables, with a minimum of 2 (for the RS example discussed in Section 6) and
a maximum of 10.
A first evidence is that the post processing approach does
not scale. We have been able to run experiments with 1k and
5k tuples, but starting at around 10k tuples the experiments
took on average several hours. This result is not surprising,
since these algorithms exhaustively look for endomorphisms
in the canonical solution in order to remove variables (i.e,
invented nulls). For instance, our first subsumption scenario
with 5k tuples in the source generated 13500 variables in the
target; the post-processing algorithm took on our machine
running PostgreSQL around 7 hours to compute the final solution. It is interesting to note that in some cases the post
processing algorithm finds the core after only one iteration
(in the previous case, it took 3 hours), but the algorithm
is not able to recognize this fact and stop the search. For
all experiments, we fixed a timeout of 1 hour. If the experiment was not completed by that time, it was stopped.
Since none of the scenarios we selected was executed in less
than 1 hour we do not report computing times for the postprocessing algorithm in our graphs. Execution times for the

Figure 7: Containment of Solutions
say that, while it is true that the rewriting algorithm may
generate expensive queries, this happens only in rather specific cases that hardly reflect practical scenarios. In practice,
scalability is very good. In fact, we may say that the 90%
of the complexity of the algorithm is needed to address a
small minority of the cases. Our experiments confirm this
intuition.
It is also worth noting that, when the complexity of the
rewriting becomes high, our algorithms allows to produce
several acceptable approximations of the core. In fact, the
algorithm is modular in nature; when the core computation
requires very high computing times and does not scale to
large databases, the mapping designer may decide to discard
the “full” rewriting, and select a “reduced” rewriting (i.e., a
rewriting wrt to a subset of homomorphisms) to generate an
approximation of the core more efficiently. This can be done
by rewriting tgds with respect to subsumptions only or to
subsumptions and coverages, as shown in Figure 7.

8.

EXPERIMENTAL RESULTS

The algorithms introduced in the paper have been implemented in a working prototype written in Java. In this
section we study the performance of our rewriting algorithm
on mapping scenarios of various kinds and sizes. We show
that the rewriting algorithm efficiently computes the core
even for large databases and complex scenarios. All experiments have been executed on a Intel Core 2 Duo machine
with 2.4Ghz processor and 4 GB of RAM under Linux. The
DBMS was PostgreSQL 8.3.

Figure 8: SQL Experiments

Computing Times. We start by comparing our algorithm

SQL scripts generated by our rewriting algorithms are reported in Figure 8. Figure 8.a shows executing times for the
four scenarios that do not contain self-joins in the target; as
it can be seen, execution times for all scenarios were below
2 minutes.
Figure 8.b reports times for the three self-join scenarios.

with an implementation [20] of the core computation algorithm developed in [13], made available to us by the authors.
In the following we will refer to this implementation as the
“post-processing approach”.
8

In fact, all multisets.

666

It can be seen that the RS example did not scale up to 1M
tuples (computing the core for 500K tuples required 1 hour
and 9 minutes). This is not surprising, given the exponential
behavior discussed in the previous Section. However, the
other two experiments with self-join – one from STMark
and another from [22] – did scale nicely to 1M tuples.

outperformed basic mappings in all the examples. Nested
mappings had mixed performance. In the first scenario they
were able to compute a non-redundant solution. In the second scenario, they brought no benefits wrt basic mappings.

Scalability on Large Scenarios. To test the scalability of
our algorithm on schemas of large size we generated a set of
synthetic scenarios using the scenario generator developed
for the STMark benchmark. We generated four relational
scenarios containing 20/50/75/100 tables, with an average
join path length of 3, variance 1. Note that, to simulate realapplication scenarios, we did not include self-joins. To generate complex schemas we used a composition of basic cases
with an increasing number between 1 and 15, in particular
we used: Vertical Partitioning (3/6/11/15 repetitions), Denormalization (3/6/12/15), and Copy (1 repetition). With
such settings we got schemas varying between 11 relations
with 3 joins and 52 relations with 29 joins.
Figure 8.c summarizes the results. In the graph, we report
several values. One is the number of tgds processed by the
algorithm, with the number of subsumptions and coverages.
Then, since we wanted to study how the tgd rewriting phase
scales on large schemas, we measured the time needed to
generate the SQL script. In all cases the algorithm was able
to generate the SQL script in a few seconds. Finally, we
report execution times in seconds for source databases of
100K tuples.

Figure 9: XML Experiments
Figure 9.b shows how the percent reduction changes with
respect to the level of redundancy in the source data. We
considered the statDB experiment, and generated several
source instances of 1k tuples based on a pool of values of
decreasing size. This generates different levels of redundancy
(0/20/40/60%) in the source database. The reduction in the
output size produced by the rewriting algorithm with respect
to nested mappings increases almost linearly.

Nested Scenarios. All algorithms discussed in the previous

sections are applicable to both flat and nested data. As it is
common [18], the system adopts a nested relational model
that can handle both relational and nested data sources (i.e,
XML).
Note that data exchange research has so far concentrated
on relational data. There is still no formal definition of a
data exchange setting for nested data. Still, we compare the
solutions produced by the system for nested scenarios with
the ones generated by the basic [18] and the nested [12] mapping generation algorithms, which we have reimplemented in
our prototype. We show that the rewriting algorithm invariably produces smaller solutions, without losing informative
content.
For the first set of experiments we used two real data sets
and a synthetic one. The first scenario maps a fragment of
DBLP9 to one of the Amalgam publication schemas10 . The
second scenario maps the Mondial database11 to the CIA
Factbook schema12 . As a final scenario we used the StatDB
scenario from [18] with synthetic random data. For each
experiment we used three different input files with increasing
size (n, 2n, 4n).
Figure 9.a shows the percent reduction in the output size
for our mappings compared to basic mappings (dashed line)
and nested mappings. As output size, we measured the
number of tuples, i.e., the number of sequence elements in
the XML. Larger output files for the same scenario indicate
more redundancy in the result. As expected, our approach

9.

RELATED WORK

In this section we review some related works in the fields
of schema mappings and data exchange.
The original schema mapping algorithm was introduced
in [18] in the framework of the Clio project. The algorithm relies on a nested relational model to handle relational
and XML data. The primary inputs are value correspondences and foreign key constraints on the two sources that
are chased to build tableaux called logical relations; a tgd
is produced for each source and target logical relations that
cover at least one correspondence. Our tgd generation algorithm is a generalization of the basic mapping algorithm
that captures a larger class of mappings, like self-joins [1] or
those in [2]. Note that the need for explicit joins was first
advocated in [19]; the duplication of symbols in the schemas
has been first introduced in the MapForce commercial system (www.altova.com/MapForce).
The amount of redundancy generated by basic mappings
has motivated a revision of the algorithm known as nested
mappings [12]. Intuitively, whenever a tgd m1 writes into an
external target set R and a tgd m2 writes into a set nested
into R, it is possible to “merge” the two mappings by nesting
m2 into m1 . This reduces the amount of redundant tuples
in the target. Unfortunately, nested mappings are applicable only in specific scenarios – essentially schema evolution
problems in which the source and the target database have
similar structures – and are not applicable in many of the
examples discussed in this paper.

9

http://dblp.uni-trier.de/xml
http://www.cs.toronto.edu/˜miller/amalgam
11
http://www.dbis.informatik.uni-goettingen.de/Mondial
12
https://www.cia.gov/library/publications/the-world-factbook
10

667

The notion of a core solution was first introduced in [11];
it represents a nice formalization of the notion of a “minimal” solution, since cores of finite structures arise in many
areas of computer science (see, for example, [15]). Note that
computing the core of an arbitrary instance is an intractable
problem [11, 13]. However, we are not interested in computing cores for arbitrary instances, but rather for solutions of a
data exchange problem; these show a number of regularities,
so that polynomial-time algorithms exist.
In [11] the authors first introduce a polynomial greedy
algorithm for core computation, and then a blocks algorithm.
A block is a connected component in the Gaifman graph
of nulls. The block algorithm looks at the nulls in J and
computes the core of J by successively finding and applying a
sequence of small useful endomorphisms; here, useful means
that at least one null disappears. Only egds are allowed as
target constraints.
The bounds are improved in [13]. The authors introduce
various polynomial algorithms to compute cores in the presence of weakly-acyclic target tgds and arbitrary egds, that
is, a more general framework than the one discussed in this
paper. The authors prove two complexity bounds. Using an
exhaustive enumeration algorithm they get an upper bound
of O(vm|dom(J)|b ), where v is the number of variables in J,
m is the size of J, and b is the block size of J. There exist
cases where a better bound can be achieved by relying on
hypertree decomposition techniques. In such cases, the upper bound is O(vm[b/2]+2 ), with special benefits if the target
constraints of the data exchange scenario are LAV tgds. One
of the algorithms introduced [13] has been revised and implemented in a working prototype [20]. The prototype uses
a relational DBMS to chase tgds and egds, and a specialized
engine to find endomorphisms and minimize the solution.
Unfortunately, as discussed in Section 8, the technique does
not scale to real size databases.
+Spicy is an evolution of the original Spicy mapping
system [5], which was conceived as a platform to integrate
schema matching and schema mappings, and represented
one of the first attempt at the definition of a notion of quality for schema mappings.

10.

algorithm, which proved very useful during the tests of the
system. Finally, we are very grateful to Paolo Atzeni for all
his comments and his advice.

11.

REFERENCES

[1] B. Alexe, W. Tan, and Y. Velegrakis. Comparing and
Evaluating Mapping Systems with STBenchmark. Proc. of
the VLDB Endowment, 1(2):1468–1471, 2008.
[2] Y. An, A. Borgida, R. Miller, and J. Mylopoulos. A
Semantic Approach to Discovering Schema Mapping
Expressions. In Proc. of ICDE, pages 206–215, 2007.
[3] C. Beeri and M. Vardi. A Proof Procedure for Data
Dependencies. J. of the ACM, 31(4):718–741, 1984.
[4] P. Bohannon, E. Elnahrawy, W. Fan, and M. Flaster.
Putting Context into Schema Matching. In Proc. of VLDB,
pages 307–318. VLDB Endowment, 2006.
[5] A. Bonifati, G. Mecca, A. Pappalardo, S. Raunich, and
G. Summa. Schema Mapping Verification: The Spicy Way.
In Proc. of EDBT, pages 85 – 96, 2008.
[6] L. Bravo, W. Fan, and S. Ma. Extending Dependencies
with Conditions. In Proc. of VLDB, pages 243–254, 2007.
[7] L. Cabibbo. On Keys, Foreign Keys and Nullable
Attributes in Relational Mapping Systems. In Proc. of
EDBT, pages 263–274, 2009.
[8] L. Chiticariu. Computing the Core in Data Exchange:
Algorithmic Issues. MS Project Report, 2005. Unpublished
manuscript.
[9] R. Fagin, P. Kolaitis, R. Miller, and L. Popa. Data
exchange: Semantics and query answering. Theor. Comput.
Sci., 336(1):89–124, 2005.
[10] R. Fagin, P. Kolaitis, A. Nash, and L. Popa. Towards a
Theory of Schema-Mapping Optimization. In Proc. of ACM
PODS, pages 33–42, 2008.
[11] R. Fagin, P. Kolaitis, and L. Popa. Data Exchange: Getting
to the Core. ACM TODS, 30(1):174–210, 2005.
[12] A. Fuxman, M. A. Hernández, C. T. Howard, R. J. Miller,
P. Papotti, and L. Popa. Nested Mappings: Schema
Mapping Reloaded. In Proc. of VLDB, pages 67–78, 2006.
[13] G. Gottlob and A. Nash. Efficient Core Computation in
Data Exchange. J. of the ACM, 55(2):1–49, 2008.
[14] T. J. Green, G. Karvounarakis, Z. G. Ives, and V. Tannen.
Update Exchange with Mappings and Provenance. In Proc.
of VLDB, pages 675–686, 2007.
[15] P. Hell and J. Nešetřil. The Core of a Graph. Discrete
Mathematics, 109(1-3):117–126, 1992.
[16] A. Y. Levy, A. O. Mendelzon, Y. Sagiv, and D. Srivastava.
Answering queries using views. In PODS, pages 95–104,
1995.
[17] R. J. Miller, L. M. Haas, and M. A. Hernandez. Schema
Mapping as Query Discovery. In Proc. of VLDB, pages
77–99, 2000.
[18] L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernandez, and
R. Fagin. Translating Web Data. In Proc. of VLDB, pages
598–609, 2002.
[19] A. Raffio, D. Braga, S. Ceri, P. Papotti, and M. A.
Hernández. Clip: a Visual Language for Explicit Schema
Mappings. In Proc. of ICDE, pages 30–39, 2008.
[20] V. Savenkov and R. Pichler. Towards practical feasibility of
core computation in data exchange. In Proc. of LPAR,
pages 62–78, 2008.
[21] B. ten Cate, L. Chiticariu, P. Kolaitis, and W. C. Tan.
Laconic Schema Mappings: Computing Core Universal
Solutions by Means of SQL Queries. Unpublished
manuscript – http://arxiv.org/abs/0903.1953, March 2009.
[22] L. L. Yan, R. J. Miller, L. M. Haas, and R. Fagin. Data
Driven Understanding and Refinement of Schema
Mappings. In Proc. of ACM SIGMOD, pages 485–496,
2001.

CONCLUSIONS

We have introduced new algorithms for schema mappings
that rely on the theoretical foundations of data exchange to
generate optimal solutions.
From the theoretical viewpoint, it represents a step forward towards answering the following question: “is it possible to compute core solutions by using the chase ?” However,
we believe that the main contribution of the paper is to show
that, despite their intrinsic complexity, core solutions can be
computed very efficiently in practical, real-life scenarios by
using relational database engines.
+Spicy is the first mapping generation system that integrates a feasible implementation of a core computation algorithm into the mapping generation process. We believe that
this represents a concrete advancement towards an explicit
notion of quality for schema mapping systems.
Acknowledgments We would like to thank the anonymous reviewers for their comments that helped us to improve the presentation. Our gratitude goes also to Vadim
Savenkov and Reinhard Pichler who made available to us an
implementation of their post-processing core-computation

668

That’s All Folks! L LUNATIC Goes Open Source
Floris Geerts1 Giansalvatore Mecca2 Paolo Papotti3 Donatello Santoro2,4
1

3

University of Antwerp – Antwerp, Belgium 2 Università della Basilicata – Potenza, Italy
Qatar Computing Research Institute (QCRI) – Doha, Qatar 4 Università Roma Tre – Roma, Italy

ABSTRACT
It is widely recognized that whenever different data sources need
to be integrated into a single target database errors and inconsistencies may arise, so that there is a strong need to apply data-cleaning
techniques to repair the data. Despite this need, database research
has so far investigated mappings and data repairing essentially in
isolation. Unfortunately, schema-mappings and data quality rules
interact with each other, so that applying existing algorithms in a
pipelined way – i.e., first exchange then data, then repair the result
– does not lead to solutions even in simple settings. We present the
L LUNATIC mapping and cleaning system, the first comprehensive
proposal to handle schema mappings and data repairing in a uniform way. L LUNATIC is based on the intuition that transforming
and cleaning data are different facets of the same problem, unified
by their declarative nature. This holistic approach allows us to incorporate unique features into the system, such as configurable user
interaction and a tunable trade-off between efficiency and quality of
the solutions.

1.

INTRODUCTION

Data transformation and data cleaning represent two important
technologies in data management. Data transformation, or data exchange [2], is the process of translating data coming from one or
more relational sources into a single target database. Data repairing [3] uses declarative constraints, like functional and inclusion
dependencies, to detect and repair inconsistencies in the data.
It is natural to think of data exchange and data repairing as two
strongly interrelated activities. In fact, the source databases are
often structured according to different conventions and rules, and
may be dirty. As a consequence, inconsistencies and errors often
arise when the sources are brought together into a target schema
that comes with its own integrity constraints.
On the contrary, the database literature has so far studied these
two problems in isolation, with the consequence that there is currently neither a clear semantics, nor adequate techniques to handle
data translation and data repairing in an integrated fashion. One
might expect that pipelining data exchange algorithms [2] and data
repairing algorithms like those in [3] is sufficient. Unfortunately,

This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivs 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/3.0/. Obtain permission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st - 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 13
Copyright 2014 VLDB Endowment 2150-8097/14/08.

this is not the case. In fact, we have shown [5] that schema mappings and data quality constraints interact in such a way that simply
pipelining the two semantics often does not return solutions.
L LUNATIC [4, 5] is the first tool that can solve mapping and
cleaning scenarios in a unified fashion. It provides a novel semantics and new algorithms to generate solutions in the presence of
dirty data. It offers a number of unique features, as follows:
(i) Users have a common language to express both schema mappings for data exchange purposes, and data quality rules for data
repairing, within an integrated tool capable of executing them.
(ii) Users can declaratively express preference rules to select the
best strategy to repair data in case of conflicts; for example, they
can easily specify that values with higher confidence or currency
should be preferred.
(iii) The semantics accommodates user inputs in the process in a
principled way, for example to discard unwanted solutions or to
interactively resolve conflicts among values for which there is no
clear preference rule.
(iv) Finally, the system allows users to fine-tune the trade-off between the quality of the solutions and the efficiency of computing
them. This is crucial due to the exponential nature of data repairing algorithms. In fact, different applications typically require different levels of quality. For example, a medical data-integration
scenario requires very high accuracy, while a web aggregator of
sports-related data may tolerate a minor quality to reduce users’
efforts and execution time.
In the following sections we describe the organization of the
demonstration. First, we outline the kind of mapping and cleaning
scenarios that will be demonstrated. We concentrate on a synthetic
example that serves the purpose of showing all of the main features
of the system. During the demo, however, we will also discuss
scenarios from real-life applications that have been conducted with
the system, including medical data, events and sports data from the
Web, and bibliographic data about publications. Given the focus of
this proposal, we deliberately choose to omit many of the technical
details that are in published papers [4, 5]. We rather concentrate on
a description of the system from the user perspective, and illustrate
what an attendee may learn by playing with it.
The system is available under an open-source license at the following URL: http://db.unibas.it/projects/llunatic/.

2.

MAPPING AND CLEANING

Consider the data scenario shown in Figure 1. Here we have several different hospital-related data sources that must be correlated
to one another. The first repository has information about Patients
and Surgeries. The second one about MedTreatments. Our goal is

1565

Source #1 – Confidence 0.5

Initial Target

PATIENTS

Name

Phone

City

123

W. Smith

324-0000

LA

SURGERIES
t2

123

Med

Date

111

Name

0.8

NY 112321

t5 222 L. Lennon 122-1876

0.9

null 781658

t6 222 L. Lennon 000-0000

0.0

SF

Eye surg. 12/01/2013


 SSN
 Name
 City
 Insur.
 Treat.
t3 124 W. Smith

LA

Med

Lapar.

SSN
 Salary
 Insur.
 Treat.
Abx

Dental

Date

t7

111

10K

07/01/2012

t8

111

25K

Abx

Cholest. 08/12/2012

t9 222

30K

Med

Eye surg. 06/10/2012

Master Data
HOSPITALS MD
SSN
tm

222

Name

Phone

L. Lennon 122-1876

City
SF

784659

TREATMENTS

Source #2 – Confidence 0.7
MEDTREATMENTS

Phone
 Conf
 City
 CC#

M. White 408-3334

Patients
SSN
Name
Phone
City
Surgeries
SSN
Insurance
Treatment
Date

MedTreatments
SSN
Name
City
Insurance
Treatment

a) Instances

Customers
SSN
Name
Phone
City
CC#
Treatments
SSN
Salary
Insurance
Treatment
Date

Target

SSN
 Insur.
 Treat.

SSN
t4

Source #1

SSN

Source #2

t1

CUSTOMERS

b) Value Correspondences

Figure 1: A Hospital Mapping and Cleaning Scenario.

to move data from the source database into a target database that organizes data in terms of Customers with their addresses and creditcard numbers, and medical Treatments paid by insurance plans.
Schema mappings To move data from the sources to the target,
users may specify declarative schema mappings. As it is common,
the mappings for our example are specified under the form of value
correspondences in Figure 1.b. Intuitively, the lines from attributes
of Source #2 to attributes of the target state that, for each tuple in the
MedTreatments source table, there must be corresponding tuples in
the Customers and Treatments target tables. Correspondences are
translated into a set of mappings under the form of tuple generating
dependencies (tgds) [10, 8] (which we omit here for the sake of
space), and executed as SQL scripts.
Target Constraints Notice that, besides deciding how to populate
the target to satisfy the mappings above, users must also deal with
the problem of generating instances that comply with target constraints, as follows.
(i) Functional and Inclusion Dependencies: Traditionally,
database architects have specified constraints of two forms: inclusion constraints and functional dependencies. In our example, we
have a foreign-key constraint stating that the SSN attribute in the
Treatments table references the SSN of a customer in Customers.
The target database also comes with a number of functional dependencies: d1 = (SSN, Name → Phone), d2 = (SSN, Name
→ CC#) and d3 = (Name, City → SSN) on table Customers.
Here, d1 requires that a customer’s social-security number (SSN)
and name uniquely determine his or her phone number (Phone).
Similarly for d2 and d3 . Notice that we do not assume that the
target database is empty. In fact, in Figure 1.a we have reported
an instance of the target. There, the pair of tuples {t5 , t6 } violates
both d1 and d2 ; the database is thus dirty.
(ii) Conditional Dependencies: Besides standard functional and
inclusion dependencies, the recent literature has shown that more
advanced forms of constraints are often necessary [3]. Therefore,
an expressive data-cleaning tool needs to support a larger class of
data-quality rules. Here we mention conditional functional dependencies and conditional inclusion dependencies [3], among others.
We designed our example in such a way to incorporate some of
these as well. We assume two conditional functional dependencies
(CFDs): (i) a CFD d4 = (Insur[Abx] → Treat[Dental]) on table
Treatments, expressing that insurance company ‘Abx’ only offers
dental treatments (‘Dental’). Tuple t8 violates d4 , adding more

dirtiness to the target database. (ii) In addition, we also have an
inter-table CFD d5 between Treatments and Customers, stating that
the insurance company ‘Abx’ only accepts customers who reside in
San Francisco (‘SF’). Tuple pairs {t4 , t7 } and tuples {t4 , t8 } violate this constraint.
(iii) Master Data and Editing Rules: Finally, as it is common in
corporate information systems [7], an additional master-data table is available; this table contains highly-curated records whose
values have high accuracy and are assumed to be clean. We also
assume an additional editing rule, d6 , that states that whenever
a tuple t in Customers agrees on the SSN and Phone attributes
with some master-data tuple tm in Hospitals MD, then the tuple t must take its Name and City attribute values from tm , i.e.,
t[Name, City] = tm [Name, City]. Tuple t5 does not adhere to this
rule as it has a missing city value (NULL) instead of ‘SF’ as provided by the master-data tuple tm .
Mapping and Cleaning In summary, our example is such that:
(a) it requires to map different source databases into a given target
database; (b) it assumes that the target database may be non-empty,
and that both the sources and the target instances may be dirty and
generate inconsistencies when the mappings are executed; (c) it
comes with a rich variety of data-quality constraints over the target.
Given the source instances and the target, our goal is to generate
a target instance that satisfies the mappings and that it is clean wrt
target constraints. We call this a mapping & cleaning scenario.
L LUNATIC is the first system to provide a unified semantics and
a set of executable algorithms to solve mapping and cleaning scenarios. The semantics is a conservative extension of the one of
data exchange and incorporates many of the features found in datarepairing algorithms (see [6] for a comparison to other semantics).

3.

QUICK OVERVIEW OF THE SYSTEM

The GUI of the L LUNATIC system is reported in Figure 2. Any
experience with the system starts by specifying a scenario (item 1
in Figure 2), i.e., a set of source databases and a target database,
that does not need to be empty. Among the source databases, users
may indicate some that are considered as authoritative – like master
data. Both the source and the target database can be browsed to
inspect the data (item 2).
The next step is concerned with specifying the mappings, i.e., the
tgds, and the data quality constraints over the target. The system
provides a graphical user interface for this task (items 3 and 4).

1566

Figure 2: L LUNATIC in action

In addition, users may specify configuration options (item 5), as
explained in the following section.
Given a set of source instances, a target instance, a set of tgds and
a set of data quality rules, L LUNATIC computes a set of minimal
solutions, i.e., target instances that satisfy the constraints (tgds and
egds), and “minimally change” the original target instance. To do
this, it runs a parallel-chase procedure, that generates a chase tree
(item 6 in Figure 2). Leaves in the chase tree are solutions that can
be inspected by users (item 7) to analyze the modifications to the
original database.
Traditionally [3], solutions to data-repairing problems have been
considered simply as sets of updates to the cells of the original
database. Here, by cell we denote a tuple-attribute pair t.A, i.e.,
the value of attribute A in tuple t. In contrast, L LUNATIC models
updates to the target database in terms of a novel data structure
called cell groups. Cell groups are essentially “repair instructions
with lineage”, since they carry full provenance information for each
change to the database, i.e.: (i) which conflicting cells in the target
were modified, and which were their values; (ii) which value was
chosen to repair the conflict; (iii) whether this value comes from
one or more of the cells in the source databases, and if these cells
are authoritative. Cell groups are the core element of the semantics,
and represent the cornerstone of user interactions in L LUNATIC, as
it will be discussed in the following section. In fact, proper care has
been devoted to provide users with a flexible interface to inspect
solutions and their cell groups (item 8).

4.

EXPERIENCES WITH THE SYSTEM

The demonstration will be centered around a number of experiences that should illustrate what are the main challenges in mapping and cleaning scenarios and how the system solves them. Attendees will be able to interact directly with the system, in such
a way that the process will resemble a hands-on tutorial. In the
following paragraphs we illustrate these experiences.
a. Need and Benefits of an Integrated Semantics To start, we
shall demonstrate why a new, integrated semantics for mapping
and cleaning is necessary, and we cannot simply reuse existing algorithms for schema-mappings and for data-repairing. We have

formally proven that such an approach does not work in general [5,
6]. To show this in a practical way, L LUNATIC can be configured to
run with several semantics. One of them is the result of pipelining
a standard chase algorithm for tgds [9, 8], with some of the popular
algorithms to repair functional dependencies [1]. By running the
system with this semantics on the scenario in Figure 1, and others,
we will show that even in simple cases constraints interact in such
a way that alternating the execution of mappings and the repairing
of data quality constraints does not terminate. In addition, when
the pipelining process terminates, the quality of the solutions computed by this procedure are quite poor, and definitely worse than
those generated by L LUNATIC.
b. Solutions as Upgrades Data-repairing algorithms in the literature [3] try to repair a dirty database by making the smallest number of changes to its cells. There are various minimality conditions for repairs, and repairing algorithms are centered around these notions. Consider our example in Figure 1. Tuples t5 = h222, L. Lennon, 122-1876 . . .i, t6 =
h222, L. Lennon, 000-000 . . .i in the target violate the functional
dependency d1 : SSN, Name → Phone over table Customers, i.e.,
the two phone cells generate a conflict. There are many possible
ways to repair the database. For example, we may change cell
t6 .Phone to value 122-1876; as an alternative, we may change cell
t5 .Phone to value 000-0000.
However, these are not the only possible repairs. For example,
we might decide to change both cells to a new number, say 5556789. This repair, however, it is not minimal, since it requires to
change two different cells, while a single cell-change is sufficient
the ones above. There are many other minimal repairs, even for
this very simple example. One of these changes t5 .SSN to a new
SSN, say 333 (similarly for t6 .SSN). These are called “backward
repairs”, since they falsify the premise of the dependency instead
of enforcing its conclusion.
Traditionally, as long as repairs involve a minimal number of
changes, they are considered as equally acceptable. However, our
example above shows that the minimality criterion is rather weak,
and algorithms often choose a repair arbitrarily. L LUNATIC is
based on a different philosophy. Its semantics is centered around

1567

the notion of an upgrade: a repair is a solution as long as it improves
the quality of the original database. Improvements cannot be made
arbitrarily. On the contrary, a change to the database is considered
an upgrade only when it unequivocally improves the data.
To specify when changes are actually upgrades, users can declaratively specify preference rules. Consider our example above; notice that confidence values are associated with phone numbers (see
Figure 1.a). Then, it is natural to state a preference rule saying that
a value of a Phone-cell should be preferred to another as soon as
its confidence is higher. In L LUNATIC this is easily done by a few
clicks that identify Conf as the ordering attribute for Phone. Given
this rule, not all repairs above are equally acceptable. More specifically, changing t6 .Phone to value 122-1876 is an upgrade of the
target, while the opposite is not.
Ultimately, by specifying preference rules, L LUNATIC users may
specify a partial order over the repairs of a dirty database. In turn,
this yields an elegant notion of a solution: it is a minimal upgrade
of the original database that satisfies the constraints.
c. Effective User-Interaction When no preference rule is available, L LUNATIC does not make arbitrary choices, and rather marks
conflicts so that users may resolve them later on. Conflicts are
marked using special values called lluns. A llun is a distinguished
symbol, Li , distinct from nulls and constants, that is introduced
whenever there is no clear way to upgrade the dirty database by
changing a cell to a new constant.
Users will be asked to consider again tuples t5 , t6 above, and
focus on the conflict on the credit-card numbers, 781658 for t5 ,
781659 for t6 . In this case we have no clear preference strategy.
Therefore, the only acceptable upgrade to the database according to
the semantics of L LUNATIC is to change both t5 .CC# and t6 .CC#
to a llun L. Here, L represents a value that may be either 781658
or 781659, or even a different constant as long as it ensures that
it resolves the conflict and upgrades the quality of the database.
Unfortunately, this value is currently unknown, and only an expert
user may identify it, possibly at a later time.
Lluns are different from the ordinary variables used in previous
approaches, due to their relationship to cell-groups. Recall that
cell-groups are the building blocks used by L LUNATIC to specify repairs. They not only specify how to change the cells of
the database, but also carry full provenance information for the
changes. Based on this combination, L LUNATIC offers powerful
features to collect user-inputs. In fact, we will show how users may
easily stop the chase to provide inputs. They may pick up a node in
the chase tree, consult its history in terms of changes to the original
database, inspect the lluns that have been introduced, and analyze
the associated cell groups. Based on this, informed decisions are
taken in order to remove lluns and replace them with the appropriate constants, or discard unwanted repairs.
During the demonstration, we plan to convey to attendees two
main notions. The first one is the fact that lluns and cell groups, together, provide a natural and effective basis to support users in their
choices. The second one is that by appropriately providing inputs
it is possible to drastically prune the size of the chase tree. Figure
3 shows one of our experiments: we run the chase for the same
scenario several times, each with an increasing number of inputs
provided by the user. With no user inputs, the chase tree counts
over 130 leafs, i.e., possible solutions. With as little as 10 inputs,
the tree collapses to a single solution. We will reproduce this example during the demonstration, to show how small quantities of
inputs from the user may significantly prune the size of the chase
tree, and therefore speed-up the computation of solutions.

150

d. Pay-As-You-Go Data
# of nodes
Cleaning At the core of 125
the system stands a pow- 100
75
erful disk-based chase en50
gine, capable of generating parallel-chase trees and
25
multiple solutions.
0
UI
2 UI
4 UI
6 UI
8 UI 10 UI
As it is well known, the
chase is a principled algoFigure 3: Impact of user-inputs on
rithm that plays a key role in
the size of the chase
many important databaserelated problems: data exchange, computing certain answers, query minimization, query
rewriting using views, and constraint implication. Given its generality, L LUNATIC can be effectively used in all of these settings.
On real-life examples, it is crucial to guarantee good computation times. L LUNATIC strives to provide the best compromise
between an exploration of the space of repairs that is more systematic and thorough that previous algorithms, and a good scalability
on large mapping and cleaning problems. A key feature, in this
respect, are cost managers. Cost managers are predicates over the
chase tree that a user may specify in order to accept or refuse nodes.
They can be used to implement different heuristics to prune the
chase tree. For example, a forward-only cost manager accepts only
forward changes and discards those that contain backward ones; a
maximum-size cost manager accepts new branches in the chase tree
as long as the number of leaves is less than N .
During the demonstration we plan to show how to obtain a good
compromise between the quality of the solutions and the execution
times by working with cost managers and their parameters, userspecified preference rules, and user-inputs, which, together, give a
fine-grained control over the solution-generation process.

5.

REFERENCES

[1] M. Dallachiesa, A. Ebaid, A. Eldawy, A. K. Elmagarmid,
I. Ilyas, M. Ouzzani, and N. Tang. Nadeef: a commodity data
cleaning system. In SIGMOD, pages 541–552, 2013.
[2] R. Fagin, P. Kolaitis, R. Miller, and L. Popa. Data Exchange:
Semantics and Query Answering. TCS, 336(1):89–124, 2005.
[3] W. Fan and F. Geerts. Foundations of Data Quality
Management. Morgan & Claypool, 2012.
[4] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. The
L LUNATIC Data-Cleaning Framework. PVLDB,
6(9):625–636, 2013.
[5] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. Mapping and
Cleaning. In ICDE, pages 232–243, 2014.
[6] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. Mapping and
Cleaning: the Llunatic Way. Technical Report TR 1-2014 –
University of Basilicata
http://www.db.unibas.it/projects/llunatic/, 2014.
[7] D. Loshin. Master Data Management. Knowl. Integrity, Inc.,
2009.
[8] B. Marnette, G. Mecca, P. Papotti, S. Raunich, and
D. Santoro. ++Spicy: an OpenSource Tool for
Second-Generation Schema Mapping and Data Exchange.
PVLDB, 4(12):1438–1441, 2011.
[9] G. Mecca, P. Papotti, and S. Raunich. Core Schema
Mappings: Scalable Core Computations in Data Exchange.
Inf. Systems, 37(7):677–711, 2012.
[10] L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernandez, and
R. Fagin. Translating Web Data. In VLDB, pages 598–609,
2002.

1568

KATARA: Reliable Data Cleaning
with Knowledge Bases and Crowdsourcing
Xu Chu1∗
Mourad Ouzzani2
1

University of Waterloo

John Morcos1∗
Paolo Papotti2
2

Qatar Computing Research Institute

{x4chu, jmorcos, ilyas}@uwaterloo.ca

{mouzzani, ppapotti, ntang}@qf.org.qa

ABSTRACT
Data cleaning with guaranteed reliability is hard to achieve
without accessing external sources, since the truth is not
necessarily discoverable from the data at hand. Furthermore, even in the presence of external sources, mainly knowledge bases and humans, effectively leveraging them still
faces many challenges, such as aligning heterogeneous data
sources and decomposing a complex task into simpler units
that can be consumed by humans. We present Katara, a
novel end-to-end data cleaning system powered by knowledge bases and crowdsourcing. Given a table, a kb, and a
crowd, Katara (i) interprets the table semantics w.r.t. the
given kb; (ii) identifies correct and wrong data; and (iii) generates top-k possible repairs for the wrong data. Users will
have the opportunity to experience the following features of
Katara: (1) Easy specification: Users can define a Katara
job with a browser-based specification; (2) Pattern validation: Users can help the system to resolve the ambiguity
of different table patterns (i.e., table semantics) discovered
by Katara; (3) Data annotation: Users can play the role
of internal crowd workers, helping Katara annotate data.
Moreover, Katara will visualize the annotated data as correct data validated by the kb, correct data jointly validated
by the kb and the crowd, or erroneous tuples along with
their possible repairs.

1.

INTRODUCTION

Many attempts have been made to improve data quality using, for example, integrity constraints [1, 6], statistics [8], or machine learning [9]. However, it is usually
hard, if not impossible, to guarantee the accuracy of the
data cleaning process without verifying it via experts or external sources [5].
The advent of knowledge bases (kbs), both generalpurpose and within enterprises, and crowdsourcing marketplaces are providing new opportunities to achieve higher
∗

Ihab F. Ilyas1∗
Nan Tang2
Yin Ye3∗

Work partially done while at QCRI.

This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivs 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/3.0/. Obtain permission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 41st International Conference on
Very Large Data Bases, August 31st - September 4th 2015, Kohala Coast,
Hawaii.
Proceedings of the VLDB Endowment, Vol. 8, No. 12
Copyright 2015 VLDB Endowment 2150-8097/15/08.

3

Google

yye@google.com

accuracy at a larger scale. However, there is no end-toend data cleaning system that would effectively bridge kbs
and crowdsourcing, thus enabling reliable data cleaning for
a wide range of applications.
We recently proposed Katara [2], a novel end-to-end data
cleaning system that effectively leverages prevalent trustworthy kbs and crowdsourcing for data cleaning. Its main
functionalities are to interpret table semantics, identify correct and wrong data, and generate top-k possible repairs for
wrong data. This demo will demonstrate the following three
key features of this system.
(1) Easy specification. Katara provides a succinct GUI
that allows users to declare the target table and the reference kb. Moreover, Katara offers to use the crowd either
through a local crowd platform or through Amazon MTurk.
(2) Pattern validation. Given the selected table and the
kb, Katara will compute and identify top-k table patterns,
which will then be visualized to the users via a simple graphical representation. Since it is hard for a system to automatically pick the best table pattern, the users, by studying the
given table, may select the most appropriate table pattern
that explains the semantics of the table.
(3) Data annotation. Once the table pattern is selected,
Katara scans all the tuples and for each tuple, it marks the
tuple as correct if the information in the kb covers all values
in the tuple, i.e., the kb is complete vis-à-vis the tuple.
Otherwise, since the kb cannot cover all values in the given
tuple, an ambiguity is raised about whether this is caused by
the incompleteness of the kb or the data is simply wrong.
To resolve such ambiguity, Katara will post questions to
the crowd. After getting answers from the crowd, Katara
can more accurately annotate the data.
Related Work. Table understanding, including identifying column types and the relationship between columns, has
been addressed by several techniques [7]. Katara differs
from them in two main aspects. First, we focus on finding coherent table patterns for the purpose of data cleaning.
In other words, instead of explaining table semantics at the
schema level, we need to find table patterns that can align information at the instance level. Second, existing techniques
do not explicitly assume the presence of dirty data.
The widely studied integrity constraint (IC) based data
cleaning aims to find a consistent database that satisfies
the given ICs with a minimum cost. The corresponding
heuristic solutions do not usually ensure full accuracy [5]. To
address such a shortcoming, several approaches have been
proposed such as involving experts as first-class citizen [10]

1952

KATARA

INPUT
Pattern Discovery
Algorithm: rank-join

Table T

Return: candidate
table patterns

OUTPUT

Pattern Validation

Data Annotation

KB validated

Algorithm: entropy
based scheduling

Algorithm: Inverted list
based approach

Crowd validated

Return: a table pattern

Return: annotated data,
new facts, top-k repairs

Trusted KB K

Possible repairs

Table T'

Enriched KB K'

Figure 1: Workflow of KATARA
leveraging high quality reference data [5], and using userprovided confidence values [4]. Katara stands from these
approaches along two aspects: (i) It does not require experts
to give high quality data quality rules as input and (ii) it
does not require experts to guide the repair process. Instead,
Katara explores a crowd of experts using a general pay-asyou-go approach, leading to lower costs, and leverages kbs,
either those readily available in an enterprise setting [3] or
general kbs such as Yago and DBpedia.
In this work, we assume that both kbs and expert sourcing
are more trusted than the data we have at hand. Improving
the accuracy of kbs or the crowd is orthogonal to Katara,
and much effort has been spent on this aspect [3]. Furthermore, some kbs, especially in enterprise settings, are usually
carefully curated with considerable manual work. Though
not error-free, these are much more reliable than the data we
have at hand and thus can be treated as relatively trusted
resources. In addition, we do not assume that the kbs to be
complete since this is not the case in most practical cases.

then be extracted. To this end, we issue SPARQL queries
that return the types and supertypes of entities whose label
(i.e., value) is t[Ai ]. The relationships between two values
are retrieved in a similar fashion. To rank candidate types
of a column Ai , we use a normalized version of tf-idf. Also,
to avoid enumerating all candidates, we rely on early termination with a rank-join formulation of the problem.

2.

(3) Data annotation. Given a table pattern, Katara annotates each tuple with one of the following three labels:

KATARA ARCHITECTURE

Katara has three modules (see Fig. 1), namely, pattern
discovery, pattern validation, and data annotation. The pattern discovery module discovers table patterns between a
table and a kb. The pattern validation module allows users
to select the best table pattern. Using the selected table
pattern, the data annotation module interacts with the kb
and the crowd to annotate the data. It also generates possible repairs for the erroneous tuples. Moreover, new facts
verified by the crowd are used to enrich kbs.
(1) Pattern discovery. Katara first discovers table patterns
that contain the types of the columns and the relationships
between a table and a kb. A table pattern is represented
as a labeled graph where a node represents an attribute
and its associated type. A directed edge between two nodes
represents the relationship between two attributes.
Note that a relational schema may not be easily aligned
with an ontology for reasons such as cryptic naming conventions. We use an instance based approach to discover
table-kb mappings. Such an approach does not require the
availability of meaningful column labels. For each column
Ai of table T and for each value t[Ai ] of a tuple t, we map
that value to several resources in the kb K whose type can

(2) Pattern validation. Depending on the data at hand and
the complexity of the reference kb, the number of candidate table patterns can vary from a handful of patterns to
dozens. If the number of the candidate table patterns is
small, as will be demonstrated in this demo, we simply visualize them for the users to pick the right table pattern for
the table at hand. We assume that the users can easily understand the tuples of the table relative to the reference kb.
On the contrary, if the size of the candidate table patterns
is large, we have proposed in [2] methods to decompose a
table pattern into smaller patterns. We then use such small
patterns to formulate simpler (sometimes binary) questions,
which crowd workers are known to be good at answering.

• Validated by the kb K. If we match a tuple to K over all
the attributes in the pattern, the tuple is semantically
correct w.r.t. the table pattern and the kb.
• Jointly validated by the kb and the crowd. If there is
only a partial match from a tuple to K, either K is
incomplete or the tuple is simply erroneous. To find
out we ask the crowd to verify the non-covered data.
• Erroneous tuple. For the erroneous tuple confirmed by
the crowd, Katara extracts information from K and
join them to generate a set of possible repairs for this
tuple.
In general, the number of possible repairs for an error can
be large. Most automatic repair algorithms use minimality
as a guiding principle to pick among multiple repairs that
make the tuple conforming to the patterns. The intuition is
that a repair with a smaller number of changes is preferable
to others with more changes, as less changes preserve more
values from the original instance. We thus rank possible
repairs based on the number of changes in ascending order.

1953

Figure 2: KATARA specification

Figure 3: Validating top-k patterns
We expose the top-k possible repairs to the users (or crowd)
for selection.

3.

DEMONSTRATION OVERVIEW

In this demonstration, we will offer users the opportunity to experience the following features of Katara: (1)
Job specification: Users specify both the input data and
reference kb, and the method and parameters supported by
Katara. (2) Pattern validation: Katara computes and visualizes the top-k table patterns such that the users can pick
the one that they consider as the most appropriate. (3) Data
annotation: Users will experience how Katara seamlessly
bridges kbs and crowdsourcing to effectively annotate data
with high fidelity. We mainly use Web tables, as given in [2].
We will show the following results by running Katara.
(1) Job specification. Figure 2 displays the Katara GUI
for specifying a job. Users can specify their input tabular
table file and see the data, select the reference kb, tune some
parameters such as the number of sampling tuples needed
and the number k for the top-k table patterns, and select a
pattern discovery algorithm.
One of the datasets we shall use in the demonstration is
similar to the table below. In this proposal, we illustrate
Katara using this table.
Angola
Bahrain
Cambodia

Luanda
Manama
Phnom Penh

Angolar
Bahraini dinar
Riel

Portuguese
Arabic
Khmer

(2) Pattern validation. After the users provide a job specification as described in (1), Katara will invoke the selected
algorithm to compute the top-k table patterns. The space
of all candidate patterns is very large (up to the Cartesian
product of all possible types and relationships in the kb),
thus making it expensive for human verification. Hence,

we rank these candidate patterns and only return the topk most meaningful ones for final human validation. Since
columns are not independent of each other, our scoring function captures the coherence of column types and their relationships. Please see [2] for more details.
Given the above table and the specification shown in
Fig. 2, Katara will find the top-5 patterns. We only show
two of them in Fig. 3 due to space constraints. For example, the first pattern (i.e., Pattern 0) states that the four
columns are economy, capital, currency and language in SimpleYago, and that the relationships between the columns are
hasCapital, hasCurrency and hasOfficalLanguage. The semantics of Pattern 1 can be explained similarly. Let us assume
for the rest of the explanation that the users will pick Pattern 0 as the correct semantics of the given table.
(3) Data annotation. After the pattern is selected by the
users as discussed in (2) above, Katara works as follows.
(i) kb annotation. For each tuple, if it is fully covered by
the given kb such that each value has the given type in the
kb and each pair of values has the selected relationship in
the kb, the tuple is considered as correct. Otherwise, if the
tuple is not fully covered, the cause might be that either the
kb is incomplete or the tuple contains some errors.
Consider the pattern selected in (2) above, the annotated tuples are shown in Fig. 4. Here, the tuples with
no background color are considered to be fully covered by
the kb. For example, the following information exists in
SimpleYago: Armenia is an economy, Yerevan is a capital,
Dram is a currency, Armenian is a language, and the relationships from Armenia to Yerevan is hasCapital, from Armenia
to Dram is hasCurrency and from Armenia to Armenian is
hasOfficialLanguage.
The tuples that are not fully covered by the kb are marked

1954

Figure 4: KB annotation

Figure 5: Crowd questions

with a pink background, meaning that either some attribute
values do not have the type specified in the table pattern,
e.g., Angolar is not a currency in SimpleYago, or two attribute values of the same tuple are not linked by the relationship in the table pattern, e.g., there is no hasCurrency
from Angola to Angolar in SimpleYago.
(ii) Crowd data validation. In the case that a tuple is not
fully covered by the kb, there are two possible reasons. The
kb is incomplete but the tuple is correct, or the tuple itself
contains errors. To resolve such ambiguity, we generate binary questions for the crowd workers. There are two classes
of questions: “Is A an instance of type” for type validation,
and “Do A and B have the relationship of relation” for relationship validation? Please refer to Fig. 5 for sample questions. These questions can be posted to either an internal
crowd, or MTurk. In this demo, we will post it to internal
crowd so that attendee can easily play with Katara.
(iii) kb and crowd annotation. After collecting the answers,
Katara will further show the annotated result (see Fig. 6).
In this figure, the tuples that are fully covered by the kb
will remain with no background color, e.g., Armenia and
Bahrain. The tuples that are identified to be correct by the
kb and the crowd are marked in green, e.g., the users confirm that Angolar is a currency, which is missing from SimpleYago. The tuples that are considered to contain errors
are marked in red, e.g., the currency of Cambodia should
be standardized as Cambodian Riel. The tuples that still
contain ambiguity remain with the pink background, which
means that more questions should be answered by the crowd
to further resolve the ambiguity.
Moreover, for each tuple that is identified to contain errors, we will generate a set of possible repairs from the kb
(more details can be found in [2]) It is then up to the user
to select a repair from the candidate list.
Summary. This demonstration exhibits how Katara can
easily help bridge kbs and crowdsourcing to achieve reliable
data cleaning. Our proposal focuses on (a) A GUI that
provides a set of user-tunable parameters for specifying a
Katara job; (b) Visualizing the most meaningful patterns
such that users can easily pick the most appropriate one; and
(c) Katara’s ability to annotate data and visualize them to

Figure 6: KB & crowd annotation

users. In addition, a by-product of Katara is that data
annotated by the crowd as being valid, and which is not
found in the kb, provides new facts to enrich the kb. Last
but not least, these well annotated data are an important
asset for several data analytic jobs such as data mining,
machine learning, and data profiling.

4.

REFERENCES

[1] X. Chu, I. F. Ilyas, and P. Papotti. Holistic data
cleaning: Putting violations into context. In ICDE,
2013.
[2] X. Chu, J. Morcos, I. F. Ilyas, M. Ouzzani, P. Papotti,
N. Tang, and Y. Ye. KATARA: a data cleaning
system powered by knowledge bases and
crowdsourcing. In SIGMOD, 2015.
[3] O. Deshpande, D. S. Lamba, M. Tourn, S. Das,
S. Subramaniam, A. Rajaraman, V. Harinarayan, and
A. Doan. Building, maintaining, and using knowledge
bases: a report from the trenches. In SIGMOD
Conference, 2013.
[4] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Interaction
between record matching and data repairing. In
SIGMOD, 2011.
[5] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Towards
certain fixes with editing rules and master data.
VLDB J., 21(2), 2012.
[6] Z. Khayyat, I. F. Ilyas, A. Jindal, S. Madden,
M. Ouzzani, P. Papotti, J.-A. Quiané-Ruiz, N. Tang,
and S. Yin. Bigdansing: A system for big data
cleansing. In SIGMOD, 2015.
[7] G. Limaye, S. Sarawagi, and S. Chakrabarti.
Annotating and searching web tables using entities,
types and relationships. PVLDB, 2010.
[8] C. Mayfield, J. Neville, and S. Prabhakar. ERACER:
a database approach for statistical inference and data
cleaning. In SIGMOD, 2010.
[9] M. Yakout, L. Berti-Equille, and A. K. Elmagarmid.
Don’t be SCAREd: use SCalable Automatic
REpairing with maximal likelihood and bounded
changes. In SIGMOD, 2013.
[10] M. Yakout, A. K. Elmagarmid, J. Neville, M. Ouzzani,
and I. F. Ilyas. Guided data repair. PVLDB, 2011.

1955

Repeatability and Workability Evaluation of SIGMOD
2011
Philippe Bonnet1 , Stefan Manegold2 , Matias Bjørling1 , Wei Cao3 , Javier Gonzalez1 , Joel Granados1 ,
Nancy Hall4 , Stratos Idreos2 , Milena Ivanova2 , Ryan Johnson5 , David Koop6 , Tim Kraska7 , René Müller8 ,
Dan Olteanu9 , Paolo Papotti10 , Christine Reilly11 , Dimitris Tsirogiannis12 , Cong Yu13 , Juliana Freire6 ,
and Dennis Shasha14
1
2
3
4
5

9

CWI, Netherlands
10

Remnin University, China
11

University of Wisconsin, USA

12

SIGMOD has offered, since 2008, to verify the experiments published in the papers accepted at the conference. This year, we have been in charge of reproducing
the experiments provided by the authors (repeatability),
and exploring changes to experiment parameters (workability). In this paper, we assess the SIGMOD repeatability process in terms of participation, review process
and results. While the participation is stable in terms of
number of submissions, we find this year a sharp contrast between the high participation from Asian authors
and the low participation from American authors. We
also find that most experiments are distributed as Linux
packages accompanied by instructions on how to setup
and run the experiments. We are still far from the vision
of executable papers.

INTRODUCTION

The assessments of the repeatability process conducted
in 2008 and 2009 pointed out several problems linked
with reviewing experimental work [2, 3]. There are obvious barriers to sharing the data and software needed to
repeat experiments (e.g., private data sets, IP/licensing
issues, specific hardware). Setting up and running experiments requires a lot of time and work. Last but
not least, repeating an experiment does not guarantee
its correctness or relevance.
So, why bother? We think that the repeatability process is important because it is good scientific practise.
SIGMOD Record, June 2011 (Vol. 40, No. 2)

Microsoft, USA

13
14

UC Berkeley,USA

ABSTRACT

Oxford University, UK

University of Texas Pan Am, USA

University of Utah, USA
7

IBM Almaden, USA

Università Roma Tre, Italy

University of Toronto, Canada
6

1.

8

ITU, Denmark

Google, USA

New York University, USA

To quote the guidelines for research integrity and good
scientific practice adopted by ETH Zurich1 : All steps
in the treatment of primary data must be documented in
a form appropriate to the discipline in question in
such a way as to ensure that the results obtained from
the primary data can be reproduced completely.
The repeatability process is based on the idea that in
our discipline, the most appropriate way to document
the treatment of primary data is to ensure that either
(a) the computational processes that lead to the generation of primary data can be reproduced and/or (b) the
computational processes that execute on primary data
can be repeated and possibly extended. Obviously, the
primary data obtained from a long measurement campaign cannot be reproduced. But our take is that the best
way to document the treatment of these primary data is
to publish the computational processes that have been
used to derive relevant graphs. On the other hand, the
primary data obtained when analyzing the performance
of a self-contained software component should be reproducible. Ultimately, a reviewer or a reader should
be able to re-execute and possibly modify the computational processes that led to a given graph. This vision of
executable papers has been articulated in [1].
This year, as a first step towards executable papers,
we encouraged SIGMOD authors to adhere to the fol1

http://www.vpf.ethz.ch/services/
researchethics/Broschure

45

lowing guidelines:2
(a) Use a virtual machine (VM) as the environment
for experiments.
(b) Explicitly represent pre- and post-conditions for
setup and execution tasks.
(c) Rely on a provenance-based workflow infrastructure to automate experimental setup and execution
tasks.
Ideally, a common infrastructure guarantees the uniformity of representation across experiments so reviewers need not re-learn the experimental setup for each
submission. The structure of workflows should help
reviewers understand the design of the experiments as
well as determine which portions of the code are accessible. While virtual machines ensure the portability
of the experiments so reviewers need not worry about
system inconsistencies, explicit pre- and post-conditions
make it possible for reviewers to check the correctness
of the experiment under the given conditions.
In the rest of the paper, we look back on the repeatability process conducted for SIGMOD 2011.

2.

ASSESSMENT

2.1 Participation
Renée Miller, PC-chair for SIGMOD 2011, agreed to
add a couple of questions to the submission site. 73%
of the authors said that they would participate in the repeatability process. As we will see in Section 2.1.2, the
percentage of accepted papers actually submitted to the
repeatability and workability committee was limited to
35%. The reasons cited for not participating were:
1. intellectual property rights on software
2. sensitive data
3. specific hardware requirements
None of these reasons, however, explain the geographic
distribution of authors participating to the repeatability
process shown in Figure 1. This graph compares the
number of papers accepted at SIGMOD and the number of papers participating in the repeatability process
grouped by the region of origin of the first author (Asia,
America, Europe, Oceania). While this grouping is largely
arbitrary (some authors might not be associated to the
same region as the first author), the trends that appears in
Figure 1 is significant. To put it bluntly, almost all Asian
authors participate in the repeatability process, while
2
See the Repeatability section of the ACM SIGMOD
2011 home page: http://www.sigmod2011.org/calls_
papers_sigmod_research_repeatability.shtml

46

Figure 1: Distribution of participants to the
repeatability process per region of first author.
few American authors do. Some American authors have
complained that the process requires too much work for
the benefit derived [2], but we believe that several observations can improve this cost/benefit calculation
1. [more benefit] repeatable and workable experiments bring several benefits to a research group
besides an objective seal of quality: a) higher quality software resulting from the discipline of building repeatable code b) an improved ability to train
newcomers to a project by having them "play with
the system"
2. [less cost] using the right tools, a research group
can make a research experiment repeatable easily
(we are working on an upcoming companion article which contains a tutorial on how to make this
happen).

2.1.1 Process
As in 2009, our goal was to complete the repeatability reviews before the conference started, so that authors
could advertise their result during their presentation (a
first straightforward way to guarantee some benefit for
authors). We placed the submission to the repeatability committee at the same time as the deadline for the
camera ready copy of the paper: leaving one month to
the author of accepted papers to prepare their submission and leaving two months for reviewers to work on
an average of three submissions each.
The availability of the Elastic Cloud Computing infrastructure via a grant from Amazon allowed us to experiment with a great variety of hardware and software
platforms. Experiments were run on servers equipped
with 26 CPUs or 40 GB of RAM, running OS ranging from Windows to CentOS. The availability of the
Condor-based Batlab infrastructure from Miron Livny’s
group at U.Wisconsin allowed a reviewer to repeat a
SIGMOD Record, June 2011 (Vol. 40, No. 2)

cluster-based experiment with 40 machines - as opposed
to 3 on the original paper. Note also that a few authors made their own cluster infrastructure available via
a gateway which made it possible for reviewers to repeat
the data acquisition phase of the authors’ papers.
The most frequently asked question by authors at submission time was where can I upload a package with
the system and data needed to reproduce my experiments?. Authors were asked to make their experiment
available for download. This was a major problem for
a Chinese group whose experiment package could not
be downloaded properly despite numerous attempts. On
the other hand, a group from ETH Zurich fully complied
to ETH Guidelines for research integrity and made
their software and data publicly available online3 .
A problem mentioned in the previous editions of the
repeatability process was the high burden on reviewers
when setting up experiments. To mitigate this problem,
as explained in the introduction, we advocated this year
that authors should consider submitting a virtual machine containing system and data. This effort was far
from successful as illustrated in Figure 2. The vast majority of submissions were Linux or Windows packages
with instructions on how to set them up and run the experiments. For most papers, the set up phase (specially
on Linux) was well designed and required low overhead
for the reviewer. However, many papers which did not
get the repeatability label failed in the set-up phase, often because some dependencies had not been made explcit; such problems would have been avoided with a well
tested virtual machine.

load on the reviewers was quite uneven. Figure 3 shows
the number of experiments per paper - which is a good
indicator of the time needed to run the experiments. We
still miss a good indicator for the time needed to setup
the experiments. This year, we simplified the grades
given to each paper: not repeatable, repeatable or repeatable&workable.

Figure 3: Distribution of number of experiments per submission.
This year, we set up a web site running on an EC2
server www.sigmod11repeatability.org with instructions
for authors, a couple of examples showing how to use
the Vistrails workflow engine to setup experiments and
the submission site. We relied on an instance of HotCRP4
to support submissions of instructions as well as anonymous interactions between authors and reviewers during
the reviewing period. While HotCRP was fully satisfactory in terms of stability, functionality and ease of use;
the setting of automatic emails from a GMail account
created for the sigmod11repeatability.org domain
turned out to be a problem - spam filters prevented mails
and notifications sent by hotCRP to reach their destination.

2.1.2

Figure 2: Operating system used for the submissions to the repeatability process in 2011.
Each paper was assigned a primary reviewer. A secondary reviewer was introduced in case the primary reviewers had problems downloading a submission, or setting it up because of OS or hardware mismatch. The
3

http://people.inf.ethz.ch/jteubner/
publications/soccer-players/

SIGMOD Record, June 2011 (Vol. 40, No. 2)

Results

Figure 4 shows the results from the repeatability process since 20085 . In terms of percentages, the participation increased slighlty in 2011 compared to 2009 and
2010–those years where only accepted papers were considered for the repeatability process–while the percentage of repeatable papers remained stable.
The results were announced to the authors prior to the
conference (at the exception of two papers). Results will
be associated as labels on the existing article repositories
4

http://www.cs.ucla.edu/~kohler/hotcrp/
The results from 2008 and 2009 are presented in the
SIGMOD Record articles [2, 3]; the results from 2010
are available at http://event.cwi.nl/SIGMOD-RWE/
2010/
5

47

3.

CONCLUSION

The SIGMOD 2011 repeatability initiative attempted
to increase participation and enhance the quality of submissions by offering tools to authors and reviewers. This
has succeeded only partly: virtual machines and workflows simplify the process for reviewers but are harder
to implement for authors than sending a shell script.
Unfortunately, the shell scripts have many system dependencies that may make them difficult to repeat or to
build upon by future researchers. An ongoing research
challenge is to develop tools to help authors create high
quality repeatable computational experiments with reasonable effort.
Figure 4: Repeatability results since 2008
(either ACM or PubZone). More importantly, the experiments themselves should be archived in a repository of
repeatable experiment. Setting up such a repository for
the SIGMOD community is the next obvious challenge.

Acknowledgements
We would like to thank Amazon for their EC2 grant and
Miron Livny and his team (especially Brooklin Gore)
for their help with the Batlab infrastructure.

4.

REFERENCES

[1] David Koop, Emanuele Santos, Phillip Mates,
Huy T. Vo, Philippe Bonnet, Bela Bauer, Brigitte
Surer, Matthias Troyer, Dean N. Williams, Joel E.
Tohline, Juliana Freire, and Cláudio T. Silva. A
provenance-based infrastructure to support the life
cycle of executable papers. Procedia CS,
4:648–657, 2011.
[2] S. Manegold, I. Manolescu, L. Afanasiev, J. Feng,
G. Gou, M. Hadjieleftheriou, S. Harizopoulos,
P. Kalnis, K. Karanasos, D. Laurent, M. Lupu,
N. Onose, C. Ré, V. Sans, P. Senellart, T. Wu, and
D. Shasha. Repeatability & workability evaluation
of SIGMOD 2009. SIGMOD Rec., 38:40–43,
December 2010.
[3] I. Manolescu, L. Afanasiev, A. Arion, J. Dittrich,
S. Manegold, N. Polyzotis, K. Schnaitter,
P. Senellart, S. Zoupanos, and D. Shasha. The
repeatability experiment of SIGMOD 2008.
SIGMOD Rec., 37:39–45, March 2008.

48

SIGMOD Record, June 2011 (Vol. 40, No. 2)

++Spicy: an Open-Source Tool for Second-Generation
Schema Mapping and Data Exchange
Bruno Marnette1∗ Giansalvatore Mecca2 Paolo Papotti3
Salvatore Raunich4 Donatello Santoro2,3
1

INRIA Saclay & ENS Cachan, France – 2 Università della Basilicata, Potenza, Italy
3
Università Roma Tre, Roma, Italy – 4 University of Leipzig, Leipzig, Germany

ABSTRACT
Recent results in schema-mapping and data-exchange research may be considered the starting point for a new generation of systems, capable of dealing with a significantly
larger class of applications. In this paper we demonstrate
the first of these second-generation systems, called ++Spicy.
We introduce a number of scenarios from a variety of data
management tasks, such as data fusion, data cleaning, and
ETL, and show how, based on the system, schema mappings and data exchange techniques can be very effectively
applied to these contexts. We compare ++Spicy to the previous generations of tools, to show that this is much-needed
advancement in the field.

1.

INTRODUCTION

There are many different classes of applications that need
to exchange, correlate, or integrate data. An essential requirement of these applications is that of manipulating mappings between sources. Mappings are executable transformations that specify how an instance of the source repository should be translated into an instance of the target
repository. We may identify two broad research lines in the
recent literature.
On one side, we have studies on practical tools and algorithms for schema-mapping generation. In this case, the
focus is on the development of systems that take as input an abstract specification of the mapping, usually made
of a bunch of correspondences between the two schemas,
and generate the mappings – typically under the form of
tuple-generating dependencies(tgds) [3] – and the executable
scripts needed to perform the translation. This research
topic was largely inspired by the seminal papers about the
Clio system [19, 21].
On the other side, we have theoretical studies about data
exchange. Several years after the initial schema-mapping
algorithms had been proposed, researchers developed a rich
∗

Work funded by the ERC grant Webdam

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Articles from this volume were invited to present
their results at The 37th International Conference on Very Large Data Bases,
August 29th - September 3rd 2011, Seattle, Washington.
Proceedings of the VLDB Endowment, Vol. 4, No. 12
Copyright 2011 VLDB Endowment 2150-8097/11/08... $ 10.00.

body of research in which the notion of a data exchange problem [9] was formalized, and a number of theoretical results
were established. In this context, the focus is not on the generation of the mappings, but rather on the characterization
of their properties and of their solutions.
We can sketch a line of evolution in schema-mappings and
data exchange systems, through three main generations.
First-Generation Systems The first generation of schema-mapping systems – primarily Clio [21], but also HePToX
[5], and the early version of Spicy [6]1 – were focused on
the process of generating complex logical dependencies (i.e.,
tgds) based on a nice and user-friendly abstraction of the
mapping provided by users under the form of value correspondences. It is interesting to note that they proposed a
rather general data model, based on nested relations, that
allowed for the treatment of both relational and XML-based
mapping tasks.
These systems also had a limited data-exchange support,
in that they were able to generate scripts (for example in
SQL or XQuery) to execute the mappings and materialize
a target solution. In this process, several key notions were
introduced, like the one of Skolem functions to handle existentially quantified variables [21].
However, at that stage, the systems suffered from a major drawback: these systems failed to generate solutions of
good quality. They were mainly restricted to canonical solutions [9], which tend to include significant redundancy, as
shown in [16, 17].
The Intermediate Generation Once the theory of dataexchange had become more mature, it was clear that producing solutions of quality was a critical requirement. The
notion of a core universal solution [11] was formalized as the
“optimal” solution, since it is universal, i.e., it does not contain any arbitrary information that does not follow from the
source instance and the mappings, and among the universal
solutions is the one of the smallest size.
Sophisticated algorithms were developed [11, 13, 14] to
post-process a canonical solution generated by a schemamapping tool, and minimize it to find its core [20]. These
tools have the merit of being very general, but fail to be
scalable: even though the algorithms are polynomial, their
implementation requires to couple complex recursive computations with SQL to access the database, and therefore
hardly scale to large databases. In fact, empirical results
1

Notice that also the recent OpenII [23] integration suite – that
supports a broader class of integration tasks – incorporates a
Clio-like first-generation mapping module.

1438

show that they are hardly usable in practice due to unacceptable execution times for medium size databases [16, 17].
A different approach to the generation of core solutions
was undertaken in [24, 16, 17]. In these proposals, scalability
is a primary concern. Given a mapping scenario composed
of source-to-target tgds (s-t tgds), the goal is to rewrite the
tgds in order to generate a runtime script, for example in
SQL, that, on input instances, materializes core solutions.
This is a key requirement in order to embed the execution
of the mappings in more complex application scenarios, that
is, in order to make data-exchange techniques a real “plug
and play” feature of integration applications. +Spicy2 [16,
18] is an example of mapping tool of this generation.
However, these tools still have some serious limitations,
that prevent their adoption in real-life scenarios. We may
summarize these limitations as follows.
(a) They have limited support for target constraints. Handling target constraints – i.e., keys and foreign keys, represented by egds and target tgds [9], respectively – is a
crucial requirement in many mapping applications. Notice
that foreign-key constraints were at the core of the original schema-mapping algorithms, and, under appropriate hypothesis, can always be rewritten as part of the source-totarget tgds [10]. Therefore, the main problem is represented
by key constraints. This intermediate generation of tools
cannot handle key constraints in a scalable way. Either
they employ the post-processing approach to enforce keys
– in which case the computation of the core fails to scale to
large databases, as shown in [15] – or were limited to generate scalable SQL script for scenarios with source-to-target
tgds only, and no egds [16, 18].
(b) They are limited to relational scenarios, and cannot handle XML or nested datasets. This is a consequence of the
fact that data-exchange research has primarily concentrated
on the relational setting, and for a long time no notion of
data exchange for more complex models was available. In
a way, this is a setback with respect to the early systems,
which had supported nested relations since the beginning. It
is interesting to note that a benchmark for mapping systems
has been recently proposed [1]. However, none of the tools
of the intermediate generation can be evaluated using the
benchmark – for example in order to compare the quality of
their solutions – since most of the scenarios in the benchmark refer to nested structures, and these systems are not
capable to generate core solutions for a nested data model.
++Spicy: a Second-Generation Tool Two recent results have paved the way towards the emergence of a fullyfledged second-generation schema-mapping and data-exchange tool.
(a) On the one side [15], the core-oriented rewriting algorithms developed in [16, 18] have been extended to handle a
very large class of mapping scenarios with target functional
dependencies, i.e., target egds. This is a significant advancement, as keys are very important in all cases in which data
coming from different sources needs to be integrated and
correlated.

for the fragment of XML data exchange in which the data
model is restricted to correspond to nested relations [22].
A very important result was reported in [7]: the authors
show that the generation of universal solutions for a nested
scenario can be reduced to the generation of solutions for a
traditional, relational scenario, even in the presence of target constraints. The authors also provide an algorithm to
perform the reduction.
3
++Spicy is the first example of a second-generation mapping tool based on these very recent algorithms. In this
demo:
(i) we show how ++Spicy can deal with different data management tasks, including data fusion, data cleaning and ETL
scenarios, which, in our opinion, represent very promising
areas of application of the latest schema-mappings and dataexchange techniques;
(ii) we compare ++Spicy with previous-generation and commercial mapping systems, and show how it is capable to
generate optimized SQL and XQuery code in a fully automated way based on a very simple and intuitive graphical
specification of the mapping, even in the presence of nested
sources and target constraints; with respect to previous systems, which were restricted to rather simplistic and unrealistic examples, or failed to generate compact solutions, we
show that the algorithms embedded in the engine enable the
management of more realistic and complex tasks;
(iii) we show that ++Spicy can efficiently compute core solutions even for large databases and large scenarios; moreover, its capability of producing executable scripts in SQL
or XQuery facilitates the process of executing the actual
exchange outside of the system, by running the scripts in
conventional engines.
Notice that, by the time of the conference, the system will
be made freely available under an open-source license.

2.

DEMONSTRATION

(b) On the other side, a theory of XML data exchange [2,
7] was developed. While the XML setting studied in these
papers is very general, and, for its generality, leads to several negative results, important properties were established

Figure 1: Mapping GUI.
In the demonstration we show how the user can specify
mapping tasks with ++Spicy in a natural and friendly way
by using the GUI in Figure 1. As it is common, the user
loads and manipulates schemas and instances – both relational and XML – with their constraints, possibly adding
target keys by dragging and dropping attributes. Then, s/he
draws arrows among elements of the schemas in order to define the desired transformation. Such arrows express the

2

3

Pronounced “more spicy”.

1439

Pronounced “much more Spicy”.

equivalence relationship among elements, independently of
the underlying data model or of logical design choices; as an
alternative, a module is available to load a set of pre-defined
tgds in logical form from a text file.
Inputs to the system are a source and a target schema,
along with their key and foreign-key constraints, an abstract specification of the mapping in terms of correspondences, and one or more source instances. The output is an
executable transformation, that can be run on the source
instance to generate a core solution for the target. The system hides the technical details of the tgd generation and
rewriting phase, and automatically generates an SQL or
XQuery script that can be fed to an external engine (e.g.,
PostgreSQL, Saxon, etc) to generate the target instance.
In the demonstration we let the audience interact with
++Spicy using a set of schemas including relational ones
with keys, nested XML schemas with keys, and scenarios
taken from the schema mapping benchmark [1]. In particular, we focus on four classes of scenarios, as discussed in the
following paragraphs, to highlight the practical impact of a
second generation mapping system. For each scenario, we
compare the output of ++Spicy to that of alternative systems (previous generation ones, commercial ones 4 ) in terms
of quality results and of the execution times.
Data-fusion Consider the data-fusion scenario in Figure 2.

Figure 2: Mapping person data.
It requires to merge together financial data from three different source tables (Figure 2.a.): (i) a table about subscribers
of pension funds; (ii) a table with the email addresses of
the people receiving the company mailing list; (iii) a table
about clients and their check accounts. The target schema
contains two tables, one about persons, the second about
accounts. On these tables, we have two keys: name is a key
for Person, while number is a key for Account.
Based on the correspondences among elements, as represented in Figure 1, a first-generation mapping system generates for this scenario several s-t tgds, which specify how
data should be moved from the source to the target. Target
egds can be used to encode the required key constraints on
the target, as follows. Note how the third tgd, m3 , invents
a value to perform a vertical partition of the Client table.
4
Such as Altova MapForce (http://www.altova.com/mapforce)
or StylusStudio (http://www.stylusstudio.com).

m1 . ∀n, pf : Subscriber(n, pf ) → ∃Y1 , Y2 : Person(n, pf, Y1 , Y2 )
m2 . ∀n, e : MailingList(n, e) → ∃Y1 , Y2 : Person(n, Y1 , e, Y2 )
m3 . ∀n, acc : Client(n, acc) →
∃Y1 , Y2 , Z : (Person(n, Y1 , Y2 , Z) ∧ Account(Z, acc))
e1 . ∀n, p, e, a, p0 , e0 , a0 : Person(n, p, e, a) ∧ Person(n, p0 , e0 , a0 )
→ (p = p0 ) ∧ (e = e0 ) ∧ (a = a0 )
0
e2 . ∀n, i, i : Account(i, n) ∧ Account(i0 , n) → (i = i0 )
However, as first-generation systems ignore egds, by using
such s-t tgds the best we can achieve is to generate efficiently
a pre-solution, i.e., a solution for the s-t tgds only, as shown
in Figure 2.c. It is easy to see that it is unsatisfactory as it
violates the required key constraints and suffers from an unwanted entity fragmentation effect: information about the
same entities (e.g., Abi, Perry or the account 001/25 ) is
spread across several tuples, each of which gives a partial
representation of the entity. If we take into account the
usual dimensions of data quality [4], such an instance must
be considered of very low quality in terms of compactness
(or minimality). In fact, on large source instances, the level
of redundancy due to entity fragmentation can seriously impair both the efficiency of the translation and the quality of
answering queries over the target database.
Based on these requirements, it is natural to desire the
generation of a solution as the one shown in Figure 2.b.
During the demo, we show how previous generation systems are unable to generate the desired solution. We also
demonstrate how the core solution can be materialized by
chasing the dependencies above with a post-processing step
to minimize the pre-solution. Unfortunately, existing chase
engines that are capable of performing this task hardly scale
to large databases [16]. Using the algorithms in [15], the
engine in ++Spicy generates an SQL script that is order
of magnitude faster (e.g., seconds vs hours for the same
database).
STBenchmark STBenchmark [1] proposes various mapping scenarios with nested sources. For each scenario, it
proposes one or more sample source instances, and the expected solutions. Interestingly, there are no key constraints
in the benchmark. A key observation is that, for all mapping
scenarios in the benchmark, the expected solution always
corresponds to the core.
During the demonstration, we will compare the various
mapping systems, including commercial ones, based on their
performance on the mapping tasks in STBenchmark. Some
scenarios will be enriched by adding key constraints. In all
cases in which sources are nested, relational mapping systems are not applicable. The remaining ones show significant differences in terms of quality of the generated solutions. It is also important to note that there is a significant
trade-off between the quality of the output and the amount
of effort required to specify the mapping, especially for commercial systems. ++Spicy provides a very good compromise, since it generates core solutions in a scalable way with
minimal user interaction.
Data-cleaning Commercial data cleaning systems are based on approaches in which cleaning actions have to be explicitly specified by users using transformation operations.
They usually focus on data profiling, to identify data quality
issues, and record matching, to remove duplicate entities,
by using ad-hoc techniques and rules with special attention for specific types of data, such as addresses or phone

1440

numbers. A more principled approach to cleaning is based
on constraints [12]. Consider for instance the database E
in Figure 3.a as given. In ++Spicy we let the user de-

Figure 3: Data cleaning example.
fine a key constraint for the attribute name. To enforce the
new constraint, the system rewrites the corresponding egd
e. Employees(n, a, s) ∧ Employees(n, a0 , s0 ) → (a = a0 ) ∧ (s =
s0 ) into a data exchange from the given database E to a
new empty one with the same structure plus the egd e. In
this setting, the algorithm described in the previous example
produces a schema mapping which outputs the database in
Figure 3.b. Then, thanks to the key constraint on the Employees table, the system detects the inconsistency on Paul’s
age, and reports it to the user, which must decide how to
handle it by properly curating the data. We will show how
the resulting scripts scale extremely well even with large
databases with hundreds of thousands of tuples.
ETL ETL tools are widely used in data warehousing environments to express data transformations as a composition
of operators in a procedural fashion. Operators vary from
simple data mappings between tables to more complex manipulations, such as joins, splits of data and merging of data
from different sources. Usually, these tools are used by developers that want to achieve an efficient implementation of
a data exchange task.
Compared to mapping systems, the superior popularity of
ETL systems is due to their richer semantics, which allow
them to express more operations [8], and to the declarative
nature of schema mapping tools that can become a limit
with complex transformations where intermediate steps are
needed. For this reason, it is important to support scenarios
where flows of mappings, defined using intermediate results,
are preferable to a single, monolithic mapping with a large
number of complex s-t tgds. ++Spicy allows the design of
chains of mappings and introduces functional dependencies
in the target, thus enabling operations that were not possible with first-generation mapping tools. We will show how
the expression of data exchange scenarios by mapping tools
is preferable to ETL systems in terms of easiness of use,
without losing efficiency in the execution, by comparing the
same scenario implemented with the two paradigms. To give

Figure 4: ETL graph.
an intuition of the minimal input required by ++Spicy, consider that for the ETL scenario in Figure 4 only two lines and
two labels are required to express the same data exchange
scenario, as exemplified by the following s-t tgd:
m. Students(n1 , b1 , c1 , p1 ) ∧ Emps(n1 , d1 , p1 , e1 )∧
(p1 = ‘Msc’) → Master(N1 , b1 , d1 , ‘M 0 )
To support complex flows of mappings, ++Spicy introduces two main operators. The first is used for chaining
mapping scenarios. The second can be used to merge the
output of different scenarios.

3.

REFERENCES

[1] B. Alexe, W. Tan, and Y. Velegrakis. Comparing and
Evaluating Mapping Systems with STBenchmark. PVLDB,
1(2):1468–1471, 2008.
[2] M. Arenas and L. Libkin. XML Data Exchange:
Consistency and Query Answering. J. of the ACM,
55(2):1–72, 2008.
[3] C. Beeri and M. Vardi. A Proof Procedure for Data
Dependencies. J. of the ACM, 31(4):718–741, 1984.
[4] J. Bleiholder and F. Naumann. Data fusion. ACM Comp.
Surv., 41(1):1–41, 2008.
[5] A. Bonifati, E. Q. Chang, T. Ho, L. Lakshmanan,
R. Pottinger, and Y. Chung. Schema Mapping and Query
Translation in Heterogeneous P2P XML Databases. VLDB
J., 41(1):231–256, 2010.
[6] A. Bonifati, G. Mecca, A. Pappalardo, S. Raunich, and
G. Summa. Schema Mapping Verification: The Spicy Way.
In EDBT, pages 85 – 96, 2008.
[7] R. Chirkova, L. Libkin, and J. Reutter. Tractable XML
Data Exchange via Relations. Technical report, North
Carolina State University, 2010.
[8] S. Dessloch, M. A. Hernandez, R. Wisnesky, A. Radwan,
and J. Zhou. Orchid: Integrating Schema Mapping and
ETL. In ICDE, pages 1307–1316, 2008.
[9] R. Fagin, P. Kolaitis, R. Miller, and L. Popa. Data
Exchange: Semantics and Query Answering. TCS,
336(1):89–124, 2005.
[10] R. Fagin, P. Kolaitis, A. Nash, and L. Popa. Towards a
Theory of Schema-Mapping Optimization. In ACM PODS,
pages 33–42, 2008.
[11] R. Fagin, P. Kolaitis, and L. Popa. Data Exchange: Getting
to the Core. ACM TODS, 30(1):174–210, 2005.
[12] H. Galhardas, D. Florescu, D. Shasha, E. Simon, and C.-A.
Saita. Declarative data cleaning: Language, model, and
algorithms. In VLDB, pages 371–380, 2001.
[13] G. Gottlob and A. Nash. Efficient Core Computation in
Data Exchange. J. of the ACM, 55(2):1–49, 2008.
[14] B. Marnette. Generalized Schema Mappings: From
Termination to Tractability. In ACM PODS, pages 13–22,
2009.
[15] B. Marnette, G. Mecca, and P. Papotti. Scalable data
exchange with functional dependencies. PVLDB,
3(1):105–116, 2010.
[16] G. Mecca, P. Papotti, and S. Raunich. Core Schema
Mappings. In SIGMOD, pages 655–668, 2009.
[17] G. Mecca, P. Papotti, and S. Raunich. Core Schema
Mappings: Scalable Core Computations in Data Exchange.
Technical Report Spicy WR-01-2011, Dipartimento di
Matematica e Informatica - Università della Basilicata,
2010.
[18] G. Mecca, P. Papotti, S. Raunich, and M. Buoncristiano.
Concise and Expressive Mappings with +Spicy. PVLDB,
2(2):1582–1585, 2009.
[19] R. J. Miller, L. M. Haas, and M. A. Hernandez. Schema
Mapping as Query Discovery. In VLDB, pages 77–99, 2000.
[20] R. Pichler and V. Savenkov. DEMo: Data Exchange
Modeling Tool. PVLDB, 2(2):1606–1609, 2009.
[21] L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernandez, and
R. Fagin. Translating Web Data. In VLDB, pages 598–609,
2002.
[22] A. Roth, M. F. Korth, H. and A. Silberschatz. Extended
Algebra and Calculus for Nested Relational Databases.
ACM TODS, 13:389–417, October 1988.
[23] L. Seligman, P. Mork, A. Halevy, K. Smith, M. J. Carey,
K. Chen, C. Wolf, J. Madhavan, A. Kannan, and
D. Burdick. OpenII: an Open Source Information
Integration Toolkit. In SIGMOD, pages 1057–1060, 2010.
[24] B. ten Cate, L. Chiticariu, P. Kolaitis, and W. C. Tan.
Laconic Schema Mappings: Computing Core Universal
Solutions by Means of SQL Queries. PVLDB,
2(1):1006–1017, 2009.

1441

Bart in Action: Error Generation and Empirical
Evaluations of Data-Cleaning Systems

Donatello Santoro

Patricia C. Arocena

Boris Glavic

University of Basilicata, Italy

University of Toronto, Canada

Giansalvatore Mecca

Renée J. Miller ∗

Paolo Papotti

University of Basilicata, Italy

University of Toronto, Canada

Arizona State University, US

ABSTRACT

Illinois Inst. of Technology, US

BART

Repairing erroneous or conflicting data that violate a set
of constraints is an important problem in data management. Many automatic or semi-automatic data-repairing algorithms have been proposed in the last few years, each with
its own strengths and weaknesses. Bart is an open-source
error-generation system conceived to support thorough experimental evaluations of these data-repairing systems. The
demo is centered around three main lessons. To start, we
discuss how generating errors in data is a complex problem,
with several facets. We introduce the important notions of
detectability and repairability of an error, that stand at the
core of Bart. Then, we show how, by changing the features
of errors, it is possible to influence quite significantly the
performance of the tools. Finally, we concretely put to work
five data-repairing algorithms on dirty data of various kinds
generated using Bart, and discuss their performance.

1.

∗

BART GUI

Command Line

Error-generation Engine
DBMS
Clean
DB

Dirty
Dirty
Dirty
Version
Version
Version

Error
Generation
Task

Figure 1: Bart System Overview

have worked to develop consolidated tools and benchmarks
for empirically evaluating algorithms. Thorough evaluation
of data-cleaning systems requires systematic control over the
amount of errors in a dataset, and over how hard these errors
are to repair. Dirty datasets must be paired with a groundtruth clean dataset to enable evaluation of the quality of a
repair produced by a cleaning algorithm. To support rigorous empirical evaluations, an error-generation system must
be able to generate multiple dirty versions of a dataset with
low user effort, and scale to large datasets.
Bart. Bart1 [1] is the first error-generation tool explicitly
conceived to support empirical evaluations of data-repairing
algorithms as per the requirements outlined above. It takes
as input a clean database and a set of data-quality rules, and
injects errors into the database. Rules are expressed using
the powerful language of denial constraints [11] and errors
can be of several kinds, such as typos, duplicated values,
nulls, and outliers. We show the major components of the
system in Figure 1. A user interacts with Bart by creating
error-generation tasks, using a GUI or CL interface. These
tasks are then interpreted by Bart’s error-generation engine
to create dirty versions of a clean database.
The system provides the highest possible level of control
over the error-generation process. Among other parameters,
it allows users to choose the percentage of errors, whether
they want a guarantee that errors are detectable using the
given constraints, and even provides an estimate of how hard
it will be to restore the database to its original clean state.
Bart is open-source: its codebase is available on GitHub2

INTRODUCTION

Data quality is a very important concern in data management. To date, many (disparate) automatic and semiautomatic data-cleaning algorithms have been proposed in
the database community [10]. These algorithms come from
different inspirations. Most of them are constraint-based :
they assume that the target database comes with a set of
data-quality rules – for example, functional dependencies
(FDs) or conditional functional dependencies (CFDs) – and
data is repaired to remove violations to these constraints.
Others rely on statistical measures of data quality or statistics combined with constraints [13].
Due to the richness of research proposals, it is important
to conduct thorough and fair experimental evaluations to
assess each tool’s potential. In fact, other fields in the data
quality landscape, like entity resolution and record linkage,
∗Supported in part by NSERC.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

1
Bart: Benchmarking Algorithms for data Repairing and
Translation

SIGMOD’16, June 26-July 01, 2016, San Francisco, CA, USA
c 2016 ACM. ISBN 978-1-4503-3531-7/16/06. . . $15.00

DOI: http://dx.doi.org/10.1145/2882903.2899397

2

2161

https://github.com/dbunibas/BART

Player
t1 :
t2 :
t3 :
t4 :
t5 :
t6 :

Name
Giovinco
Giovinco
Pirlo
Pirlo
Vidal
Vidal

Season
2013-14
2014-15
2014-15
2015-16
2014-15
2015-16

Team
Juventus
Toronto
Juventus
N.Y.City
Juventus
Bayern

Stadium
Juv.Stadium
BMO Field
Juv.Stadium
Yankee St.
Juv.Stadium
Allianz Ar.

Goals
3
23
5
0
8
3

(iii) We run a data-repairing algorithm over Id to obtain
a repaired instance Irep . We measure the quality of the
algorithm using precision and recall. Intuitively, we count
how many changes in Ch have been restored to their original
values in Irep . Further details are in the full paper [1].
We want now to emphasize how different ways to change
the cells of the clean instance may lead to errors that show
completely different features when considered from the perspective of a data-repairing tool.

Figure 2: Example Clean Database
and can be further extended by the community to develop
new features and functionalities.

2.1

Demonstration Overview. The demonstration will convey three primary insights about the system.

When evaluating a constraint-based repair algorithm, we
want to make sure the errors we inject are detectable by
the system in question. After all, an error that cannot be
detected, cannot be repaired. To reason about detectability,
we need a notion for determining whether a cell change is
involved in a constraint violation. Consider the following
cell change: ch1 = ht1 .Season := 2012-13i that updates tuple
t1 as follows:

(i) First, we will discuss how the fine control over the features of errors distinguishes Bart from previous error-generating techniques used in evaluating data-repairing systems.
(ii) Then, we will discuss how the characteristics of errors
may significantly influence the quality of repairs generated
by a system.

Player
t1 :

(iii) Finally, we will demonstrate five different algorithms
in action on dirty data generated using Bart, to reveal new
insights on their (relative) performance as the characteristics
of the errors are varied by Bart.

Name
Giovinco

Season
2012-13

Team
Juventus

Stadium
Juv.Stadium

Goals
3

This change does not introduce a violation to any of the
constraints in Σ, i.e., after the cell change the modified
database instances does fulfill all the constraints. Therefore, any data-repairing tool that relies on the constraints
to detect dirtiness in the database will not be able to detect
the change. We call this an undetectable change.
More formally, a cell change ch = hti .A := vi in Ch introduces a detectable error in I for constraint dc if: (i) cell
ti .A is involved in a violation with dc in instance Id , and
(ii) cell ti .A was not involved in a violation with dc in instance I . Here “involved in a violation” is defined based on
the fact that a constraint can be associated with a query
that returns sets of cells that cause a violation. We call
this type of queries violation-detection queries. A cell is
involved in a violation with a constraint dc if it is in the result of the violation-detection query for dc [1]. For example,
the violation-detection query for d2 is Qd2 (i, i0 , t, a, a0 ) =
Player(i, n, s, t, a, g), Player(i0 , n0 , s0 , t, a0 , g 0 ), a 6= a0 , i 6= i0 .
This query returns the Team and Stadium attributes of pairs
of Player tuples with the same team, but different stadiums.3
Using the tuple id’s, sets of cells involved in violations can
be determined based on the result of this query.
In addition to generating changes to the clean database
that are detectable using the constraints Σ, Bart may also
be configured to generated random changes. Generating
detectable changes requires Bart to reason efficiently and
holistically about a set of changes to ensure that they are
detectable using a given set of constraints.
Interestingly, this latter requirement significantly increases
the complexity of the error-generation process. In fact, generating a given number of errors in a clean database that are
detectable using a set of constraints Σ is an NP-complete
problem [1]. To deal with this complexity Bart implements
several novel optimizations [1] that balance the need for control over the nature of errors and scalability.

Overall, the attendees will learn how the availability of a
tool like Bart may help to level the field and raise the bar
for evaluation standards in data cleaning.
The paper is organized as follows. Section 2 introduces
the main motivation for the system, and the notions of detectability and repairability of errors. Section 3 provides an
overview of the system and of its main use cases. Finally,
Section 4 discusses the organization of the demo, and the
main lessons that can be learned from it.

2.

Detectability

CONCEPT AND MOTIVATION

Assume we are given a database about soccer players,
shown in Figure 2, and we want to assess the performance
of repair algorithms according to a few data-quality rules.
(i) A first FD stating that Name and Season are a key for the
table: d1 : Name, Season → Team, Stadium, Goals.
(ii) And, a second FD stating that Team implies Stadium:
d2 : Team → Stadium.
We specify these rules in Bart using the language of denial constraints. Denial constraints are a very expressive
language, capable of capturing most data-quality rules used
for data-repairing, including FDs, CFDs, cleaning equalitygenerating dependencies, editing rules, fixing rules, and ordering constraints [10]. For the sake of simplicity, here we
omit the technical details about the syntax and semantics
of denial constraints, and show example data-quality rules
in the more familiar syntax of FDs.
To evaluate data-repair systems, we proceed as follows.
(i) We start with a clean instance I , like the one in Figure 2,
and the set of constraints Σ = {d1 , d2 } discussed above.

2.2

Repairability

An alternative change that indeed introduces a detectable
error is the following: ch2 = ht1 .Season := 2014-15i. After

(ii) We inject errors by applying a set of cell changes; each
cell change ch = hti .A := vi updates the value of attribute
A in tuple ti to a new value v, e.g, ht1 .Season := 2011-12i.
By applying a set of cell changes Ch to I , we obtain a new
instance Id = Ch(I ), named the dirty instance.

3

We assume every tuple has a unique identifier that per
convention is the first attribute. Queries are expressed in a
notation similar to Datalog.

2162

this update, tuples t1 and t2 violate FD d1 , which states
that Name and Season are a key for the table:
Player
t1 :
t2 :

Name
Giovinco
Giovinco

Season
2014-15
2014-15

Team
Juventus
Toronto

Stadium
Juv.Stadium
BMO Field

Goals
3
23

This change is easily detected using the constraints. Still,
it is quite difficult for an automatic data-repairing algorithm
to restore the database to its clean state. Notice, in fact,
that after this change, the original value 2013-14 has been
removed from the active domain of the dirty database. A
correct repair cannot be found by any repair algorithm that
uses the values in the database as the candidates for repair.
Bart uses the notion of repairability of an error to characterize this aspect. In the case above, it would assign repairability 0 to change ch2 . Different detectable changes
may have quite different repairability values.
As an example, consider now change ch3 = ht1 .Stadium :=
Delle Alpii. The change is detectable using FD d2 . In addition, the redundancy in the dirty database may be used to
repair the database:
Player
t1 :
t3 :
t5 :

Name
Giovinco
Pirlo
Vidal

Season
2013-14
2014-15
2014-15

Team
Juventus
Juventus
Juventus

Stadium
Delle Alpi
Juv.Stadium
Juv.Stadium

Goals
3
5
8

The new, dirty tuple t1 is involved in two violations to d2 ,
one with t3 , another with t5 . In both cases, the change is
in violation with Juv.Stadium. By a straightforward probabilistic argument, Bart would calculate a 2/3 repairability
for this error, and rank it as a medium-repairability error.
Errors may have higher repairability, even 1 in some cases.
Consider, for example, an additional rule d3 : Team[Juventus],
Season[2013 − 14] → Stadium[Juv.Stadium]. This CFD rule
states unequivocably that Juventus has played its home games
for season 2013–14 in the Juventus Stadium. Since this
knowledge is part of the constraint, the dirty cell can easily
be restored to its original, clean state.

2.3

Other Kinds of Errors

To conclude this discussion about the features of errors,
we notice that the notions of detectability and repairability,
that are centered around detecting violations to constraints,
are not the only ones supported by Bart.
Consider, for example, change ch4 = ht1 .Goals := 123i.
This change is not detectable using the constraints. However, it might be detected by a statistics-based data-repairing
algorithm, because it introduces an outlier into the distribution of values of the Goals attribute. Bart can be configured
in order to generate changes of this kind as well.

3.

OVERVIEW OF THE SYSTEM

Bart provides users with the graphical user interface shown
in Figure 3 to handle error-generation tasks. An errorgeneration task, E is composed of four key elements: (i) a
database schema S; (ii) a set Σ of denial constraints (DCs)
encoding data quality rules over S; (iii) an instance I of S
that is clean with respect to Σ; (iv) a set of configuration
parameters Conf (shown in Figure 3.1) to control the errorgeneration process. These parameters specify, among other
things, which relations can be changed, how many errors
should be introduced, and how many of these errors should
be detectable. They also let the user control the degree of
repairability of the errors.
Based on this, Bart supports several uses cases. The
main one consists of generating a desired degree of detectable

2163

errors for each constraint. In addition, users may also specify a range of repairability values for each constraint; Bart
will estimate the repairability of changes, and only generate
errors with estimated repairability within that range.
In addition to detectable errors, Bart may also generate
random errors of several kinds: typos (e.g., ‘databse’), duplicated values, bogus or null values (e.g., ‘999’, ‘***’). Random
errors may be freely mixed with constraint-induced ones. Finally, Bart can introduce outliers in numerical attributes.
Bart provides sophisticated features to analyze the characteristics of errors that are introduced in the data. It generates charts to analyze the number of errors detected by each
constraint, and their estimated repairability (shown in Figure 3.3). It also offers a versioning system, that allows users
to generate different dirty databases for the given scenario,
and compare the characteristics of their errors.
Finally, Bart offers a flexible set of metrics to measure the
quality of the repairs generated by a data-repairing tool. In
fact, different algorithms can repair data in different ways.
For example, some algorithms can produce repairs that mark
dirty cells using a variable, while others always restore the
dirty instance with a constant value. Different metrics have
been proposed and are implemented in Bart to uniformly
evaluate these heterogenous changes in the data [2, 9].

4.

DEMONSTRATION

The demo will be centered around the empirical evaluation of several data-repairing algorithms over dirty data
generated using Bart. We will use two publicly available
tools, Llunatic [9] and Nadeef [6], to run four rule-based
data-repairing algorithms: (i) Greedy [3, 5]; (ii) Holistic [4];
(iii) Llunatic [8]; and (iv) Sampling [2]. In addition, we will
evaluate (v) SCARE [14], a statistics-based tool.
The tools will be tested with several repair tasks, based
on synthetic and real datasets, some of them constraintbased and some statistics-based. We briefly list them here:
(i) Employees is a synthetic scenario in the full paper [1];
(ii) Customers is a synthetic scenario from Geerts et al. [8];
(iii) Tax is a synthetic scenario from Fan et al. [7]; (iv)
Bus is a real-world scenario from Dallachiesa et al. [6]; and
(v) Hospital is a real-world scenario used in several datarepairing papers (e.g., [6, 8]).
Datasets and constraints have been chosen to exhibit different characteristics. Some have high redundancy in their
data. Others contain numerical attributes, and constraints
containing ordering (<, >) comparisons. Some datasets
have master-data [12] and CFDs, while others have only
FDs. All these differences help to validate our techniques
and the tools under exam.
Notice the purpose of this evaluation is not to assess the
quality of the repair algorithms, rather to show how Bart
can be used to uncover new insights into the data-repairing
process. Some important insights are the following.
Lesson 1: Data-repairing is not yet Mature. We expect that a wide degree of variability in quality among all
algorithms will emerge from our evaluations. This variability does not clearly emerge from evaluations reported in the
literature, an observation that suggests there is no definitive
data-repairing algorithm yet.
Lesson 2: Repairability Matters. We will observe different trends with respect to repairability. Typically, repair
algorithms return very good repairs when sufficient infor-

Figure 3: User Interface
mation is available (i.e., high repairability); however, their
quality tends to degrade quickly as repairability decreases.
A key observation is that repairability has a strong correlation with the quality of the repairs. In this respect, we believe it nicely captures the “hardness” of the data-repairing
problem and it helps in getting a concrete intuition of the
power and the limits of existing solutions.
Lesson 3: We Need to Document Our Dirty Data.
We may conclude that tools exhibit quite different performance on data-repairing problems of different nature, and
the repairability is a natural proxy to characterize how “difficult” a data-repairing problem is.
In light of this and to level the field, we believe it is crucial to have at our disposal systematic error-generation tools
and to properly document the characteristics of the dirty
data used in empirical evaluations of data-repairing solutions. Bart is a concrete step in this direction.
Lesson 4: Generating Errors is Hard. The problem
of systematically generating errors, however, is not an easy
one. We will show how different configurations of the errorgeneration task affect the overall scalability of the system,
and discuss the main optimizations that Bart relies on in
order to tame the complexity of the process.

5.

[4] X. Chu, I. F. Ilyas, and P. Papotti. Holistic Data
Cleaning: Putting Violations into Context. In ICDE,
pages 458–469, 2013.
[5] G. Cong, W. Fan, F. Geerts, X. Jia, and S. Ma.
Improving Data Quality: Consistency and Accuracy.
In VLDB, pages 315–326, 2007.
[6] M. Dallachiesa, A. Ebaid, A. Eldawy, A. K.
Elmagarmid, I. F. Ilyas, M. Ouzzani, and N. Tang.
NADEEF: a Commodity Data Cleaning System. In
SIGMOD, pages 541–552, 2013.
[7] W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis.
Conditional Functional Dependencies for Capturing
Data Inconsistencies. ACM TODS, 33, 2008.
[8] F. Geerts, G. Mecca, P. Papotti, and D. Santoro.
Mapping and Cleaning. In ICDE, pages 232–243, 2014.
[9] F. Geerts, G. Mecca, P. Papotti, and D. Santoro.
That’s All Folks! LLUNATIC Goes Open Source.
PVLDB, 7(13):1565–1568, 2014.
[10] I. F. Ilyas and X. Chu. Trends in cleaning relational
data: Consistency and deduplication. Foundations and
Trends in Databases, 5(4):281–393, 2015.
[11] A. Lopatenko and L. Bravo. Efficient Approximation
Algorithms for Repairing Inconsistent Databases. In
ICDE, pages 216–225, 2007.
[12] D. Loshin. Master Data Management. Knowl.
Integrity, Inc., 2009.
[13] N. Prokoshyna, J. Szlichta, F. Chiang, R. J. Miller,
and D. Srivastava. Combining quantitative and logical
data cleaning. PVLDB, 9(4):300–311, 2015.
[14] M. Yakout, L. Berti-Équille, and A. K. Elmagarmid.
Don’t be SCAREd: Use SCalable Automatic
REpairing with Maximal Likelihood and Bounded
Changes. In SIGMOD, pages 553–564, 2013.

REFERENCES

[1] P. C. Arocena, B. Glavic, G. Mecca, R. J. Miller,
P. Papotti, and D. Santoro. Messing-Up with BART:
Error Generation for Evaluating Data Cleaning
Algorithms. PVLDB, 9(2):36–47, 2015.
[2] G. Beskales, I. F. Ilyas, and L. Golab. Sampling the
Repairs of Functional Dependency Violations under
Hard Constraints. PVLDB, 3(1):197–207, 2010.
[3] P. Bohannon, M. Flaster, W. Fan, and R. Rastogi. A
Cost-Based Model and Effective Heuristic for
Repairing Constraints by Value Modification. In
SIGMOD, pages 143–154, 2005.

2164

Lightning Fast and Space Efficient Inequality Joins
Zuhair Khayyat3∗ William Lucia§ Meghna Singh§ Mourad Ouzzani§
Paolo Papotti§
Jorge-Arnulfo Quiané-Ruiz§
Nan Tang§
Panos Kalnis3
3

§
Qatar Computing Research Institute
King Abdullah University of Science and Technology (KAUST)

{zuhair.khayyat,panos.kalnis}@kaust.edu.sa,
williamlucia.wl@gmail.com
{mesingh,mouzzani,ppapotti,jquianeruiz,ntang}@qf.org.qa

ABSTRACT
Inequality joins, which join relational tables on inequality
conditions, are used in various applications. While there
have been a wide range of optimization methods for joins in
database systems, from algorithms such as sort-merge join
and band join, to various indices such as B + -tree, R∗ -tree
and Bitmap, inequality joins have received little attention
and queries containing such joins are usually very slow. In
this paper, we introduce fast inequality join algorithms. We
put columns to be joined in sorted arrays and we use permutation arrays to encode positions of tuples in one sorted
array w.r.t. the other sorted array. In contrast to sort-merge
join, we use space efficient bit-arrays that enable optimizations, such as Bloom filter indices, for fast computation of
the join results. We have implemented a centralized version
of these algorithms on top of PostgreSQL, and a distributed
version on top of Spark SQL. We have compared against
well known optimization techniques for inequality joins and
show that our solution is more scalable and several orders
of magnitude faster.

1.

ONCE UPON A TIME . . .

Bob1 , a data analyst working for an international provider
of cloud services, wanted to analyze revenue and utilization
trends from different regions. In particular, he wanted to
find out all those transactions from the West-Coast that last
longer and produce smaller revenues than any transaction
in the East-Coast. In other words, he was looking for any
customer from the West-Coast who rented a virtual machine
for more hours than any customer from the East-Coast, but
who paid less. Figure 1 illustrates a data instance for both
tables. He wrote the following join query for such a task:
Qt : SELECT east.id, west.t id
FROM east, west
WHERE east.dur < west.time AND east.rev > west.cost;
∗

Work partially done while doing an internship at QCRI.
We motivate the problem with a real-life story. Names and
queries have been changed to protect confidentiality.
1

This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivs 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/3.0/. Obtain permission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 42nd International Conference on
Very Large Data Bases, September 5th - September 9th 2016, New Delhi,
India.
Proceedings of the VLDB Endowment, Vol. 8, No. 13
Copyright 2015 VLDB Endowment 2150-8097/15/09.

east
r1
r2
r3

id
100
101
102

dur
140
100
90

rev cores
12
2
12
8
5
4

west
s1
s2
s3
s4

t id time cost cores
404 100
6
4
498 140 11
2
676
80
10
1
742
90
5
4

Figure 1: East-Coast and West-Coast transactions
Bob first ran Qt over 200K transactions on the distributed
system storing the data (System-X). Given that the input
dataset is ∼1GB, he expected to have his answer in a minute
or so. However, he waited for more than three hours without
seeing any result. He immediately thought that this problem
comes from System-X and killed the query. He then used
an open-source DBMS-X to run his query. Although join is
by far the most important and most studied operator in the
relational algebra [1], Bob had to wait for over two hours
until DBMS-X returned the results. He found that Qt is
processed by DBMS-X as a Cartesian product followed by
a selection predicate, which is problematic due to the huge
number of unnecessary intermediate results.
In the meantime, Bob heard that a big DBMS vendor
was in town to highlight the power of their recently released
distributed DBMS to process big data (DBMS-Y). So he
visited them with a small (few KBs) dataset sample of the
tables to run Qt . Surprisingly, DBMS-Y could not run Qt
for even that small sample! He spent 45 minutes waiting
while one of the DBMS-Y experts was trying to solve the
issue. Bob left the query running and the vendor never
contacted him again. In fact, DBMS-Y is using underneath
the same open-source DBMS-X that Bob tried before. He
thus understood that a simple distribution of the process
does not solve his problem. Afterwards, Bob decided to call
one of his friends working for a very famous DBMS vendor.
His friend kindly accepted to try Qt on their DBMS-Z, which
is well reputed to deal with terabytes of data. A couple of
days later, his friend came back to him with several possible
ways (physical plans) to run Qt on DBMS-Z. Nonetheless,
all these query plans still had the quadratic complexity of a
Cartesian product with its inherent inefficiency.
Despite the prevalence of this kind of queries in applications, such as temporal and spatial databases, and data
cleaning, no off-the-shelf efficient solutions exist. There
have been countless techniques to optimize the different flavors of joins in various settings [13]. In the general case
of a theta join, one may assume that one of the relations
is small enough to fit in memory; a nested loop join with
the small relation stored in memory would deliver acceptable performance. However, such assumption may not hold
when joining two big relations. Queries could also contain
selection predicates or an equi-join with a high selectivity,

2074

which would reduce the size of the relations to be fed to the
inequality join. However, this is not necessarily true with
low-selectivity predicates, such as gender or region, where
the obtained relations are still very large. Furthermore, similar to Qt , there is a large spectrum of applications where
the above two assumptions do not necessarily hold. Such
applications require to join large relations using inequalities
only, such as in temporal and spatial databases, and data
cleaning applications. For example, in data analysis in a
temporal database, one may want to find all employees and
managers that overlapped while working in a certain company [12]. In data cleaning, when detecting violations based
on denial constraints, one may want to find all pairs of tuples such that one individual (represented in the tuple) pays
more taxes but earns less than another individual [7].
Bob then started looking at alternatives. Common ways
of optimizing such queries include sort-merge joins [9] and
interval-based indexing [6, 11, 16]. Sort merge join reduces
the search space by sorting the data based on the joining attributes and merging them. However, it still has a quadratic
complexity for queries with inequality join conditions only.
Interval-based indexing reduces the search space of such
queries even further by using bitmap interval indexing [6].
However, such indices require large memory space [22] and
long index building time. Moreover, Bob would have to create multiple indices to cover all those attributes referenced
in his query workload. One may build such indices at query
time, but their long construction time renders such an approach impractical.
With no hope in the horizon, Bob decided to talk with his
friends who happen to do research in data analytics. They
happily started working on this interesting problem. After
several months of hard work, they came out with IEJoin,
a new algorithm that utilizes bit-arrays and positional permutation arrays to achieve fast inequality joins. Given the
inherent quadratic complexity of inequality joins, IEJoin
follows the RAM locality is King principle coined by Jim
Gray. The use of memory-contiguous data structures with
small footprint results in orders of magnitude performance
improvement over the prior art. The basic idea of our proposal is to create a sorted array for each inequality comparison and compute their intersection, which would output the
join results. The prohibitive cost of the intersection operation is alleviated through the use of a permutation array
to encode positions of tuples in one sorted array w.r.t. the
other sorted array (assuming that there are only two conditions). A bit-array is then used to emit the join results.
Contributions. We claim the following contributions:
(1) We present novel, fast and space efficient inequality join
algorithms (Sections 2 and 3).
(2) We discuss two optimization techniques to significantly
speed up the computation (Section 4). Specifically, we exploit Bloom filters to reduce the search space, and reorganize
data to improve data locality.
(3) We describe how to implement the proposed algorithms in distributed data processing systems, such as
Spark SQL [2] to handle very large datasets (Section 5).
In particular, we use attribute metadata (e.g., min and max
values) to greatly reduce data shuffling.
(4) We implemented our algorithms on both PostgreSQL
and Spark SQL (Section 6). We conducted an extensive

experimental study by comparing against well known optimization techniques. The results show that our proposed
solution is more general, scalable, and orders of magnitude
faster than known prior art (Section 7).
We discuss the related work in Section 8 and conclude the
paper in Section 9.

2.

OVERVIEW

In this section we restrict our discussions to queries with
inequality predicates only. Each predicate is of the form:
Ai op Bi . Here, Ai (resp., Bi ) is an attribute in relation R
(resp., S), and op is an inequality operator in {<, >, ≤, ≥}.
In the following, we motivate our work and give the intuition
of how our algorithms work by using self-join queries.
Example 1: (Single predicate) Consider the west table in
Figure 1 and an inequality self-join query Qs as follows:
Qs : SELECT s1 .t id, s2 .t id
FROM west s1 , west s2
WHERE s1 .time > s2 .time;

Query Qs returns a set of pairs {(si , sj )}
where si takes more time than sj ; the result is
{(s2 , s1 ), (s2 , s3 ), (s2 , s4 ), (s1 , s3 ), (s1 , s4 ), (s4 , s3 )}.
2
A natural idea to handle an inequality join on one attribute is to leverage a sorted array. For instance, we
sort west’s tuples on time in ascending order into an array L1 :hs3 , s4 , s1 , s2 i. We denote by L[i] the i-th element in
array L, and L[i, j] its sub-array from position i to position
j. Given a tuple s, any tuple at L1 [k] (k ∈ [1, i − 1]) has
a time value that is less than L1 [i], the position of s in L1 .
Consider Example 1, tuple s1 in position L1 [3] joins with
tuples in positions L1 [1, 2], namely s3 and s4 .
Example 2: (Two predicates) Let us now consider a more
challenging case of a self-join with two inequality conditions:
Qp : SELECT s1 .t id, s2 .t id
FROM west s1 , west s2
WHERE s1 .time > s2 .time AND s1 .cost < s2 .cost;

Qp returns pairs (si , sj ) where si takes more time but pays
less than sj ; the result is {(s1 , s3 ), (s4 , s3 )}.
2
Similar to attribute time in Example 1, one can additionally sort attribute cost in ascending order into an array L2 :hs4 , s1 , s3 , s2 i. Thus, given a tuple s, any tuple L2 [l]
(l ∈ [j + 1, n]), where n is the size of the input relation, has
higher cost than the one in s, where j is the position of s in
L2 . Our observation here is as follows. For any tuple s0 , to
form a join result (s, s0 ) with tuple s, the following two conditions must be satisfied: (i) s0 is on the left of s in L1 , i.e., s
has a larger value for time than s0 , and (ii) s0 is on the right
of s in L2 , i.e., s has a smaller value for cost than s0 . Thus,
all tuples in the intersection of L1 [1, i − 1] and L2 [j + 1, n]
satisfy these two conditions and belong to the join result.
For example, s4 ’s position in L1 (resp. L2 ) is 2 (resp. 1).
Hence, L1 [1, 2 − 1] = hs3 i and L2 [1 + 1, 4] = hs1 , s3 , s2 i, and
their intersection is {s3 }, producing (s4 , s3 ). To get the final
result, we simply need to repeat the above process for each
tuple.
The challenge is how to perform the aforementioned intersection operation in an efficient manner. There already
exist several indices, such as R-tree and B + -tree, that can
possibly help. R-tree is ideal for supporting two or higher
dimensional range queries. However, the main shortcoming

2075

(1) Initialization
L1 s3 (80) s4 (90) s1 (100) s2 (140) (sort ↑ on time)

H
H

L2 s4 (5)

P

s1 (6)

s3 (10)

s2 (11)

2 3 1 4 (permutation array)

(sort ↑ on cost)
1 2 3 4
B 0 0 0 0 (bit-array)
s3 s4 s1 s2

(2) Visit tuples respect to L2
(a)
B
(b)
B
(c)
B
(d)
B

H·
H·
H
·
H ·
−−→

⇒

0 1 0 0

Output:

⇒

0 1 1 0

Output:

0 1 1 0

⇒

1 1 1 0

Output: (s4 , s3 ), (s1 , s3 )

1 1 1 0

⇒

1 1 1 1

Output:

0 0 0 0

−→

0 1 0 0
−−−−→

(d) Visit P [4]. This visit corresponds to tuple s2 . The process is similar to the above steps with an empty join result.
The final result of Qp is the union of all the intermediate
results from the above steps, i.e., {(s4 , s3 ), (s1 , s3 )}.
There are few observations that make our solution appealing. First, there are many efficient techniques for sorting large arrays, e.g., GPUTeraSort [14]. In addition, after
getting the permutation array, we only need to sequentially
scan it once. Hence, we can store the permutation array on
disk, instead of memory. Only the bit-array is required to
stay in memory, to avoid random disk I/Os. Thus, to execute queries Qs and Qp on 1 billion tuples, we only need 1
billion bits (i.e., 125 MB) of memory space.

3.

Figure 2: IEJoin process for query Qp
of using R-trees for inequality joins is that it is unclustered;
we cannot avoid random I/O access when retrieving join
results. B + -tree is a clustered index. The bright side is
that for each tuple, only sequential disk scan is required
to retrieve relevant tuples. However, the dark side is that
we need to repeat this n times, where n is the number of
tuples, which is prohibitively expensive. When confronted
with such problems, one common practice is to use spaceefficient and CPU-friendly indices; in this paper, we employ
a bit-array.
In a nutshell, our method, namely IEJoin, sorts relation
west on time and cost, creates a permutation array for cost
w.r.t. time, and leverages a bit-array to emit join results. We
will briefly present the algorithm below, and defer a detailed
discussion to Section 3. Figure 2 depicts the process.
(1) Initialization. Sort both time and cost values in ascending order, as depicted by L1 and L2 , respectively. While
sorting, compute a permutation (reordering) array of elements of L2 in L1 , as shown by P . For example, the first
element of L2 (i.e., s4 ) corresponds to position 2 in L1 .
Hence, P [1] = 2. Initialize a bit-array B with length n and
set all bits to 0, as shown by B with array indices above the
rectangles and corresponding tuples below the rectangles.
(2) Visit tuples in the order of L2 . Scan the permutation
array P and operate on the bit-array as shown below.
(a) Visit P [1]. First visit tuple s4 (1st element in L2 ) and
check in P what is the position of s4 in L1 (i.e., position 2).
Then go to B[2] and scan all bits in higher positions than 2.
As all B[i] = 0 for i > 2, there is no tuple that satisfies the
join condition of Qp w.r.t. s4 . Finish this visit by setting
B[2] = 1, which indicates that tuple s4 has been visited.
(b) Visit P [2]. This corresponds to tuple s1 . It processes s1
in a similar manner as s4 , without outputting any result.
(c) Visit P [3]. This visit corresponds to tuple s3 . Each
non-zero bit on the right of s3 (highlighted by grey cells)
corresponds to a join result, because each marked cell corresponds to a tuple that pays less cost (i.e., being visited first)
but takes more time (i.e., on the right side of its position).
It thus outputs (s4 , s3 ) and (s1 , s3 ).

CENTRALIZED ALGORITHMS

We now detail our novel inequality join algorithms based
on sorting, permutation arrays, and bit-arrays. We will first
discuss the case with two relations and only with operators
in {<, >, ≤, ≥}, and then describe its extension to support
multiple join conditions in Section 3.1. The special case of
self-joins is presented in Section 3.2.

3.1

IEJoin

Algorithm. IEJoin, is shown in Algorithm 1. It takes
a query Q with two inequality join conditions as input and
returns a set of result pairs. It first sorts the attribute values
to be joined (lines 3-6), computes the permutation array
(lines 7-8) and two offset arrays (lines 9-10). Each element
of an offset records the relative position from L1 (resp. L2 )
in L01 (resp. L02 ). The algorithm also sets up the bit-array
(line 11) as well as the result set (line 12). In addition, it
sets an offset variable to distinguish between the inequality
operators with or without equality conditions (lines 13-14).
It then visits the values in L2 in the desired order, which is
to sequentially scan the permutation array from left to right
(lines 15-22). For each tuple visited in L2 , it first sets all
bits for those t in T 0 whose Y 0 values are smaller than the
Y value of the current tuple in T (lines 16-18), i.e., those
tuples in T 0 that satisfy the second join condition. It then
uses the other offset array to find those tuples in T 0 that also
satisfy the first join condition (lines 19-22). It finally returns
all join results (line 23). Let us illustrate this algorithm with
the following example.
Example 3: Figure 3 shows how Algorithm 1 works for Qt
(from Section 1). It first does the initialization (step (1) in
the figure). For example, when visiting the first item in L2
(r3 ) in step (2)(a), it first finds its relative position in L02 at
step (2)(a)(i). Then it visits all tuples in L02 whose cost values are no larger than r3 [rev] at step (2)(a)(ii). Afterwards,
it uses the relative position of r3 [dur] at L01 (step (2)(a)(iii))
to populate all join results (step (2)(a)(iv)). The same process sequentially applies to r1 (step (2)(b)) and r2 (step
(2)(c)), and the only result is returned at step (2)(c)(v). 2
Correctness. It is easy to check that the algorithm will
terminate and that each result in join result satisfies the join
condition. For completeness, observe the following. For any
tuple pair (ri , sj ) that should be a result, sj will be visited first and its corresponding bit is set to 1 (lines 17-18).
Afterwards, ri will be visited and the result (ri , sj ) will be
identified (lines 20-22) by the algorithm.

2076

Algorithm 1: IEJoin

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

input : query Q with 2 join predicates t1 .X op1 t2 .X 0 and
t1 .Y op2 t2 .Y 0 , tables T, T 0 of sizes m and n resp.
output: a list of tuple pairs (ti , tj )
let L1 (resp. L2 ) be the array of X (resp. Y ) in T
let L01 (resp. L02 ) be the array of X 0 (resp. Y 0 ) in T 0
if (op1 ∈ {>, ≤}) sort L1 , L01 in descending order
else if (op1 ∈ {<, ≥}) sort L1 , L01 in ascending order
if (op2 ∈ {>, ≤}) sort L2 , L02 in ascending order
else if (op2 ∈ {<, ≥}) sort L2 , L02 in descending order
compute the permutation array P of L2 w.r.t. L1
compute the permutation array P 0 of L02 w.r.t. L01
compute the offset array O1 of L1 w.r.t. L01
compute the offset array O2 of L2 w.r.t. L02
initialize bit-array B 0 (|B 0 | = n), and set all bits to 0
initialize join result as an empty list for tuple pairs
if (op1 ∈ {≤, ≥} and op2 ∈ {≤, ≥}) eqOff = 0
else eqOff = 1
for (i ← 1 to m) do
off2 ← O2 [i]
for j ← O2 [i − 1] to O2 [i] do
B 0 [P 0 [j]] ← 1
off1 ← O1 [P [i]]
for (k ← off1 + eqOff to n) do
if B 0 [j] = 1 then
add tuples w.r.t. (L2 [i], L02 [k]) to join result

19
20
21
22

23

return join result

Complexity. Sorting arrays and computing their permutation array is in O(m · log m + n · log n) time, where m
and n are the sizes of the two input relations (lines 3-8).
Computing the offset arrays will take linear time using sortmerge (lines 9-10). The outer loop will take O(m · n) time
(lines 15-22). Hence, the total time complexity of the algorithm is O(m · log m + n · log n + m · n). It is straightforward
to see that the total space complexity is O(m + n).
Multiple join conditions. For more than two join predicates on a single inequality join, we simply pick two inequality predicates and apply IEJoin. We then filter the materialized results and evaluate the remaining predicates. This
approach has very low memory footprint and it is a standard
solution in relational databases. A query optimizer will have
to decide which predicate to process first based on the selectivity of different predicates. In Section 6, we explain how
we integrate IEJoin into existing query optimizers.

3.2

IESelfJoin

In this section, we present the algorithm for self-join
queries with two inequality operators. While IEJoin can
be used, IESelfJoin is more efficient for self-joins since it
uses two sorted arrays instead of four.
Algorithm. IESelfJoin (Algorithm 2) takes a self-join
inequality query Q as input, and returns a set of result pairs.
The algorithm first sorts the two lists of attributes to be
joined (lines 2-5), computes the permutation array (line 6),
and sets up the bit-array (line 7) as well as the result set
(line 8). It also sets an offset variable to distinguish inequality operators with or without equality (lines 9-10). It
then visits the values in L2 in the desired order, which is
to sequentially scan the permutation array from left to right
(lines 11-16). For each tuple visited in L2 , it needs to find all
tuples whose X values satisfy the join condition. This is performed by first locating its corresponding position in L1 via

Algorithm 2: IESelfJoin

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

input : query Q with 2 join predicates t1 .X op1 t2 .X and
t1 .Y op2 t2 .Y , table T of size n
output: a list of tuple pairs (ti , tj )
let L1 (resp. L2 ) be the array of column X (resp. Y )
if (op1 ∈ {>, ≤}) sort L1 in descending order
else if (op1 ∈ {<, ≥}) sort L1 in ascending order
if (op2 ∈ {>, ≤}) sort L2 in ascending order
else if (op2 ∈ {<, ≥}) sort L2 in descending order
compute the permutation array P of L2 w.r.t. L1
initialize bit-array B (|B| = n), and set all bits to 0
initialize join result as an empty list for tuple pairs
if (op1 ∈ {≤, ≥} and op2 ∈ {≤, ≥}) eqOff = 0
else eqOff = 1
for (i ← 1 to n) do
pos ← P [i]
for (j ← pos + eqOff to n) do
if B[j] = 1 then
add tuples w.r.t. (L1 [j], L1 [i]) to join result
B[pos] ← 1

16
17

return join result

looking up the permutation array (line 12). Since the bitarray and L1 have a one-to-one positional correspondence,
the tuples on the right of pos will satisfy the join condition
on X (lines 13-15), and these tuples will also satisfy the join
condition on Y if they have been visited before (line 14).
Such tuples will be joined with currently visited tuple as
results (line 15). Afterwards, the visited tuple will also be
marked (line 16). It finally returns all join results (line 17).
Note that the different sorting orders, i.e., ascending or
descending for attribute X and Y in lines 2-5, are chosen to
satisfy various inequality operators. One may observe that
if the database contains duplicated values, when sorting one
attribute X, its corresponding value in attribute Y should
be considered, and vice versa, in order to preserve both orders for correct join result. Hence, in IESelfJoin, when
sorting X, we use an algorithm that also takes Y as the
secondary key. Specifically, when some X values are equal,
their sorting orders are decided by their Y values (lines 2-3),
similarly for the other way around (lines 4-5). Please refer
to the example in Section 2 for query Qp using IESelfJoin.
Correctness. It is easy to check that the algorithm will
terminate and that each result in join result satisfies the join
condition. For completeness, observe the following. For
any tuple pair (t1 , t2 ) that should be in the result, t2 is
visited first and its corresponding bit is set to 1 (line 16).
Afterwards, t1 is visited and the result (t1 , t2 ) is identified
(lines 14-15) by IESelfJoin.
Complexity. Sorting two arrays and computing their permutation array is in O(n · log n) time (lines 2-8). Scanning
the permutation array and scanning the bit-array for each
visited tuple run in O(n2 ) time (lines 11-16). Hence, in total, the time complexity of IESelfJoin is O(n2 ). It is easy
to see that the space complexity of IESelfJoin is O(n).

4.

OPTIMIZATION

We discuss two optimization techniques for our inequality
join algorithms. The first one is to use indices to improve
the lookup performance for the bit-array (Section 4.1). The
second one is to union arrays, so as to improve data locality
and reduce the data to be loaded into the cache (Section 4.2).

2077

east

west

(1) Initialization
L01 s3 (80) s4 (90) s1 (100) s2 (140) (sort ↑ on time)

L1 r3 (90) r2 (100) r1 (140) (sort ↑ on dur)

H
H
H

H

L2 r3 (5) r1 (12) r2 (12)

(sort ↑ on rev)

L02 s4 (5)

P

(permutation array)

P0 2 3 1 4

(permutation array)

O1 2 3 4

(offset of L1 w.r.t. L01 )

(bit-array)

O2 1 3 5

(offset of L2 w.r.t. L02 )

1 2 3 4
B0 0 0 0 0
s3 s4 s1 s2

1 3 2

(2) Visit tuples with regards to L2
(a) visit L2 [1] for r3 : (i) O2 [1] = 1; (ii) P 0 [1] = 2

s1 (6)

s3 (10)

(iii) O1 [P [1]] = 2;

B0 0 1 0 0
(b) visit L2 [2] for r1 : (i) O2 [2] = 3;

(ii) P 0 [2] = 3, P 0 [3] = 1

−−−→

(v) Output:

(iii) O1 [P [2]] = 4;

(iv)

·

(v) Output:

−
→

(v) Output: (r2 , s2 )

B0 : 1 1 1 0

(ii) P 0 [4] = 4
B0

·

(iv)

(sort ↑ on cost)

B0 : 0 1 0 0

B0 1 1 1 0
(c) visit L2 [3] for r2 : (i) O2 [3] = 5;

s2 (11)

(iii) O1 [P [3]] = 3;

(iv)
B0

1 1 1 1

·

: 1 1 1 1

Figure 3: IEJoin process for query Qt
Bloom filter
C1
B

0

0

0

1

0

0

1

0

C2
0

0

0

1

0

C4

C3

(i) pos 6

0

0

0

0

0

0

0

(ii) pos 9

Figure 4: Example of using a Bloom filter

4.1

Bloom Filter to Improve Bit-array Scan

An analysis on both IEJoin and IESelfJoin shows that,
for each value being visited (i.e., lines 20-22 in Algorithm 1
and lines 13-15 in Algorithm 2), we need to scan all the bits
on the right of the current position. When the query selectivity is high, this is unavoidable for producing the correct
results. However, when the query selectivity is low, iteratively scanning a long sequence of 0’s will be a performance
bottleneck. We thus adopt a Bloom filter to guide which
part of the bit-array should be visited.
Given a bit-array B with size n and a predefined chunk
size c, our Bloom filter is a bit-array with size dn/ce where
each bit corresponds to a chunk in B, with 1 indicating that
the chunk contains at least a 1 and 0 otherwise.
Example 4: Consider the bit-array B in Figure 4. Assume
that the chunk size c = 4. The bit-array B will be partitioned into four chunks C1 –C4 . Its Bloom filter is shown
above B in the figure and consists of 4 bits. We consider
two cases. Case (i): visit B[6], in which case we need to find
all the 1’s in B[i] for i > 6. The Bloom filter tells that only
chunk 2 needs to be checked, and it is safe to ignore chunks
3 and 4. Case (ii): visit B[9], the Bloom filter can tell that
there is no need to scan B, since there cannot be any B[j]
where B[j] = 1 and j > 9.
2

4.2

Union Arrays on Join Attributes

In testing Algorithm 1, we found that there are many
cache loads and stores. A deeper analysis of the algorithm
shows that the extra cache loads and stores may be caused

by cache misses when sequentially visiting different arrays.
Take Figure 3 for example. In Step (2)(a), we visited arrays
L2 , O2 , P 0 , P and O1 in sequence, with each causing at least
one cache miss. Step (2)(b) and Step (2)(c) show a similar
behavior. An intuitive solution is to merge the arrays on join
attributes and sort them together. Again, consider Figure 3.
We can merge L1 and L01 into one array and sort them. Similarly, we can merge L2 and L02 , and P and P 0 . Also, O1 and
O2 are not needed in this case, and B 0 needs to be extended
to be aligned with the merged arrays. This solution is quite
similar to IESelfJoin discussed in Section 3.2. However,
we need to prune join results for tuples that come from the
same table. This can be easily done using a Boolean flag
for each position, where 0 (resp., 1) denotes that the corresponding value is from the first (resp. the second) table. Our
experiments (Section 7.2) show that the simple union operation can significantly reduce the number of cache misses,
and thus improve the total execution time.

5.

DISTRIBUTED INEQUALITY JOINS

We present a distributed version of the IEJoin along the
same lines of state-of-the-art general purpose distributed
data processing systems, such as Hadoop’s MapReduce [8]
and Spark [23]. Our goal is twofold: (i) scale our algorithm to very large input relations that do not fit into the
main memory of a single machine and (ii) improve efficiency
even further. We assume that work scheduling and data
block assignment are handled by any general purpose resource manager, such as YARN (http://hadoop.apache.org/
docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) and
Mesos (http://mesos.apache.org).
The simplest approach for running IEJoin in a distributed
setting is to: (i) construct k data blocks of each input relation; (ii) apply Cartesian product (or self-Cartesian product
for a single relation input) on the data blocks; and (iii) run
IEJoin (either on a single table or two tables input) on
the k2 data block pairs. However, this would generate large
amount of network traffic and some data block pairs may not

2078

Algorithm 3: Distributed IEJoin

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

input : Query q, Table tin1 ,Table tin2
output: Table tout
//Pre-processing
DistT1 ← read tin1 in distributed blocks
DistT2 ← read tin2 in distributed blocks
foreach row r ∈ DistT1 and DistT2 do
r ← global unique ID
DistT1 ← sort and equally partition DistT1 rows
DistT2 ← sort and equally partition DistT2 rows
forall the block bi ∈ DistT1 do
D1i ← pivot and ref erence values in bi
M T 1i ← min and max values in pivot and
ref erence lists of D1i
forall the block bj ∈ DistT2 do
D2j ← pivot and ref erence values in bj
M T 2j ← min and max values in pivot and
ref erence lists of D2j
Virt ← all block combinations of MT1 and MT2
forall the (M
T T 1i , M T 2j ) pairs ∈ V irtij do
if M T 1i
M T 2j then
M Dataij ← blocks from D1i and D2j
//IEJoin function
forall the block pairs (D1i , D2j ) ∈ M Dataij do
RowIDResultij ← IEJoin(q,D1i ,D2j )
//Post-processing
forall the rowID pairs (i, j) ∈ RowIDResultij do
rowi ← row id i in DistT1
rowj ← row id j in DistT2
tout ← merge(rowi ,rowj )

necessarily generate results. A naive work-around would be
to reduce the number of blocks for each relation to maximize
the usability of each data block. However, very large data
blocks introduce work imbalance and require larger memory
space for each worker.
We solve the above distribution challenges by introducing
efficient pre-processing and post-processing phases. These
two phases allow us to reduce communication overhead and
memory footprint for each node, without modifying the
data block size. The pre-processing phase generates spaceefficient data blocks for the input relation(s); predicts which
pair of data blocks may report query results; and copies
and transfers through the network only useful pairs of data
blocks. IEJoin, in the distributed version, returns the join
results as a pair of rowIDs instead of returning the actual
rows. It is the responsibility of the post-processing phase to
materialize the final results by resolving the rowIDs into actual relation rows. We use the internal rowIDs of Spark SQL
to uniquely identify different rows since the input relations
may not have a unique row identifier. We summarize in
Algorithm 3 the implementation of the distributed algorithm when processing two input tables. We omit the discussion about single relation input since it is straightforward to follow. The distributed join process is composed
of three main phases (the pre-processing, IEJoin, and postprocessing phases), which are described below.
Distributed pre-processing. After assigning unique
rowIDs to each input row (lines 2-5), the pre-processing step
globally sorts each relation and partitions each sorted relation to k equally-sized partitions, where the number of partitions depends on the relation size and default block size

(lines 6-7). For example, if the default block size is b and
e.
the relation input size is M , the number of partitions is d M
b
Note that the global sorting maximizes data locality within
the partitions, which in turn increases the overall runtime efficiency. This is because global sorting partially answers one
of the inequality join conditions, where it physically moves
tuples closer toward their candidate pairs. In other words,
global sorting increases the efficiency of block pairs that generate results, while block pairs that do not produce results
can be filtered out before actually processing them. After
that, for each sorted partition, we generate a single data
block that stores only the attribute values referenced in the
join conditions in a list. These data blocks D1 and D2 do
not store the actual relation rows in order to reduce the
network overhead and reduce its memory footprint. This
follows the semi-join principle. We also extract metadata
that contain the block ID, and the min/max values of each
referenced attribute value from each data block (lines 8-15).
Then, we create kM T 1 × kM T 2 virtual block combinations
and filter out block combinations that do not generate results (lines 16-19). Notice that blocks with non-intersecting
min-max values do not produce results.
Distributed IEJoin. After pre-processing, we obtain a list
of overlapping block pairs. We simply run IEJoin (either
for a single or two relations) for each of these pair blocks
in parallel. Specifically, we merge-sort the attribute values in D1 and D2 and run IEJoin over the merged block.
The permutation and bit arrays generation are similar to
the centralized version. However, the distributed IEJoin
does not have access to the actual relation rows. Therefore,
each parallel IEJoin instance outputs a pair of rowIDs that
represents the joined rows (lines 21-22).
Distributed post-processing. In the final step, we materialize the result pairs by matching each rowID-pair, from
the output of the distributed IEJoin, with the rowIDs
of DistT 1 and DistT 2 (lines 24-27). We run this postprocessing phase in parallel, as a distributed hash join based
on the rowIDs, to speed up the materialization of the final
join results.

6.

IMPLEMENTATION DETAILS

We now describe the integration of our algorithms into
PostgreSQL (Section 6.1) and Spark SQL (Section 6.2).

6.1

PostgreSQL

PostgreSQL processes a query in three stages: parsing,
planning, and execution. Parsing extracts relations and
predicates and creates query parse trees. Planning creates
query plans and invokes the query optimizer to select a plan
with the smallest estimated cost. Execution runs the selected plan and emits the output.
Parsing and Planning. PostgreSQL uses merge and hash
join operators for equijoins and naive nested loop for inequality joins. PostgreSQL looks for the most suitable join
operator for each join predicate. We extend this check to
verify if it is IEJoin-able by checking if a predicate contains
a scalar inequality operator. If so, we save the operator’s
oid in the data structure associated with the predicate. For
each operator and ordered pair of relations, the list of predicates that the operator can handle is created. For example,
two equality predicates over the same pair of relations are
associated to one hash join operator.

2079

Sort1
s1
s2
s3
s4

idx time cost pos
3
80
10
1
4
90
5
2
1
100
6
3
2
140 11
4

Sort2
s1
s2
s3
s4

idx time cost pos
4
90
5
2
1
100
6
3
3
80
10
1
2
140 11
4

Dataset
Employees
Employees2
Events
Events2
MDC
Cloud

Figure 5: Permutation array creation for self-join Qp
Next, the Planner estimates the execution cost for possible
join plans. Every node in the plan has a base cost, which
is the cost of executing the previous nodes, plus the cost
for the actual node. Using existing PostgreSQL methods,
we added a cost function for our operator; it is evaluated
as the sum of the cost for sorting inner and outer relations,
CPU cost for evaluating all output tuples (approximated
based on the IEJoin-predicates), and the cost of evaluating
additional predicates for each tuple (i.e., the ones that are
not involved in the actual join). Next, PostgreSQL selects
the plan with the lowest cost.
Execution. At the executor, incoming tuples from outer
and inner relations are stored into TupleTableSlot arrays.
These copies of the tuples are required as PostgreSQL may
not have the content of the tuple at the same pointer location
when the tuple is sent for the final projection. This step
is a platform-specific overhead that is required to produce
an output. The outer relation (of size N ) is parsed first,
followed by the inner relation (of size M ). If the inner join
data is identical to the corresponding outer join data (selfjoin), we drop the inner join data and the data structure has
size N instead of 2N .
We illustrate in Figure 5 the data structure and the permutation array computation with an example for the selfjoin Qp . The data structure is initialized with an index (idx)
and a copy of the attributes of interest (time and cost for
Qp ). Next, the data is sorted for the first predicate (time)
using the system function qsort with special comparators (as
defined in Algorithm 1) to handle cases where two values for
a predicate are equal. The result of the first sort is reported
at the left-hand side of Figure 5. The last column (pos) is
now filled with the ordering of the tuples according to this
sorting. As a result, we create a new array to store the index
values for the first predicate. We use this array to select the
tuple IDs at the time of projecting tuples. The tuples are
then ordered again according to the second predicate (cost),
as reported in the right-hand side of Figure 5. After the
second sorting, the new values in pos are the values for the
permutation array, denoted by perm.
Finally, we create and traverse a bit-array B of size
(N + M ) (N in case of self-join) along with a Bloom filter
bit-array, as discussed in Section 4.1. If the traversal finds
a set bit, the corresponding tuples are sent for projection.
Additional predicates (if any) are evaluated at this stage,
and, if the conditions are satisfied, tuples are projected.

6.2

Spark SQL

Spark SQL [2] allows users to query structured data on top
of Spark [23]. It stores the input data as a set of in-memory
Resilient Distributed Datasets (RDD). Each RDD is partitioned into smaller cacheable blocks, where each block fits
in the memory of a single machine. Spark SQL takes as input the datasets location(s) in HDFS and an SQL query, and
outputs an RDD that contains the query result. The default
join operation in Spark SQL is inner join. When passing a
join query to Spark SQL, the optimizer searches for equality

Number of rows
10K – 500M
1B – 6B
10K – 500M
1B – 6B
24M
470M

Size
300KB – 17GB
34GB – 215GB
322KB – 14GB
32GB – 202GB
2.4GB
28.8GB

Table 1: Size of the datasets
join predicates that can be used to evaluate the inner join
operator as a hash-based physical join operator. If there
are no equality join predicates, the optimizer translates the
inner join physically to a Cartesian product followed by a
selection predicate.
We implemented the distributed version of IEJoin as a
new Spark SQL physical join operator. To make the optimizer aware of the new operator, we added a new rule to
recognize inequality conditions. The rule uses the first two
inequality conditions for the IEJoin operator. In case of
additional inequality join conditions, it evaluates them as a
post selection operation on the output of the first two join
conditions. The distributed operator utilizes Spark RDD operators to process the algorithm in distributed fashion. As a
result, the distributed IEJoin operator depends on Spark’s
memory management to store the user’s input relation. If
the result does not fit in the memory of a single machine,
we temporarily store the result into HDFS. After all IEJoin
instances finish writing into HDFS, the distributed operator
passes the HDFS file pointer to Spark, which constructs a
new RDD of the result and passes it to Spark SQL.

7.

EXPERIMENTAL STUDY

In this section, we evaluate IEJoin with several datasets
on a set of inequality queries (Section 7.1). We study the
effect of sorting and caching (Section 7.2). We then compare IEJoin with existing systems on both a centralized
(Section 7.3) and a distributed environment (Section 7.4).

7.1

Datasets, Queries, and Algorithms

Datasets. We used both synthetic and real-world data
(summarized in Table 1) to evaluate our algorithms.
(1) Employees. A dataset that contains employees’ salary
and tax information [3] with eight attributes: state, married,
dependents, salary, tax, and three others for notes. The relation has been populated with real-life data: tax rates, income brackets, and exemptions for each state in the USA
have been manually collected to generate synthetic tax
records. We used the following self-join query to identify
anomalies [7]:
Q1 : SELECT r.id, s.id
FROM Employees r, Employees s
WHERE r.salary < s.salary AND r.tax > s.tax;

The above query returns a set of employee pairs, where
one employee earns higher salary than the other but pays
less tax. To make sure that we generate output for Q1 , we
selected 10% random rows and increased their tax values.
Employees2 is a group of larger input datasets with up to 6
Billion records, but with only 0.001% random changes to tax
values. The higher selectivity is used to test the distributed
algorithm on large input files.
(2) Events. A synthetic dataset that contains start and end
time information for a set of independent events. Each event

2080

contains the name of the event, event ID, number of attending people, and the sponsor ID. We used this dataset with
a self-join query that collects pairs of overlapping events:
Q2 : SELECT r.id, s.id
FROM Events r, Events s
WHERE r.start ≤ s.end AND r.end ≥ s.start
AND r.id 6= s.id;

Again, to make sure we generate output for Q2 , we selected 10% random events and extended their end values.
We also generate Events2 as larger datasets with up to 6
Billion records, but with 0.001% extended random events.
(3) Mobile Data Challenge (MDC). This is a 50GB real
dataset [18] that contains behavioral data of nearly 200 individuals collected by Nokia Research (https://www.idiap.
ch/dataset/mdc). The dataset contains physical locations,
social interactions, and phone logs of the participating individuals. We used two relations, Shops and Persons, from
the dataset with the following join query that, for all shops,
looks for all persons that are close to a shop up to a distance
c along the x-axis (xloc) and the y-axis (yloc):
Q3 : SELECT s.name, p.name
FROM Shops s, Persons p
WHERE s.xloc − c < p.xloc AND s.xloc + c > p.xloc
AND s.yloc − c < p.yloc AND s.yloc + c > p.yloc;

(4) Cloud [20]. A real dataset that contains cloud reports from 1951 to 2009, through land and ship stations
(ftp://cdiac.ornl.gov/pub3/ndp026c/). We used a selfjoin query Q4 , similar to Q3 , to compute for every station
all stations within a distance c = 10. Since the runtime for
Q3 and Q4 is dominated by the output size, we mostly used
them for scalability analysis in the distributed case.
Centralized Systems. We evaluated the following centralized systems in our experiments:
(1) PG-IEJoin. We implemented IEJoin inside PostgreSQL v9.4, as discussed in Section 6.1. We compare it
against the baseline systems below.
(2) PG-Original. We use PostgreSQL v9.4 as a baseline
since it is the most widely used open source DBMS. We ran
automatic configuration tuning with pgtune [21] to maximize
the benefit from large main memory.
(3) PG-BTree & PG-GiST. For optimization purposes, we
use indices for Q1 and Q2 with two alternative approaches:
a B-tree index and GiST. For PG-BTree, we define a Btree index for each attribute in a query. For PG-GiST, we
use the GiST access method built inside PostgreSQL, which
considers arbitrary indexing schemes and automatically selects the best technique for the input relation. Although Q1
and Q2 appear similar, they require different data representation to be able to index them using GiST. The inequality
attributes in Q1 are independent, each condition forms a
single open interval. However, the inequality attributes in
Q2 are dependent, together they form a single closed interval. To use GiST in Q1 , we had to convert salary and tax
attributes into a single geometric point data type SalTax, as
shown in Q1i . Similarly for Q2 , we converted start and end
attributes into a single range data type StartEnd, as shown
in Q2i .
Q1i : SELECT r.id, s.id
FROM Employees r, Employees s
WHERE r.SalTax >∧ s.SalTax
AND r.SalTax  s.SalTax;

Q2i : SELECT r.id, s.id
FROM Events r, Events s
WHERE r.StartEnd && s.StartEnd AND r.id 6= s.id;

In the rewriting of the above queries in PG-GiST, operator “>∧ ” corresponds to “is above?”, operator “” means
“is strictly right of ?”, and operator “&&” indicates “overlap?”. For geometric and range type, GiST uses a Bitmap
index to optimize its data access with large datasets.
(4) MonetDB. We used MonetDB Database Server Toolkit
v1.1 (Oct2014-SP2), which is an open-source columnoriented database, in a disk partition of size 669GB.
(5) DBMS-X. We used a leading commercial centralized
relational database.
Single node experimental setup. For the centralized
evaluation, we used a Dell Precision T7500 equipped with
two 64-bit quad-core Intel Xeon X5550 (8 physical cores and
16 CPU threads) and 58GB RAM.
Distributed systems. For these experiments, we used the
following systems:
(1) Spark SQL-IEJoin. We implemented IEJoin inside
Spark SQL v1.0.2 (https://spark.apache.org/sql/), as
detailed in Section 6.2. We evaluated the performance of
our techniques against the baseline systems below.
(2) Spark SQL & Spark SQL-SM. Spark SQL is the default implementation in Spark SQL. Spark SQL-SM is an optimized version based on distributed sort-merge join in [17].
It contains three phases: partitioning, sorting, and joining.
Partitioning selects a join attribute to distribute the data
based on some statistics, e.g., cardinality estimation. Sorting sorts each partition into many sorted lists, each list corresponds to an inequality condition. Finally, we apply a
distributed sort merge join over the sorted lists to produce
results. We also improve the above method by pruning the
non-overlapping partitions to be joined.
(3) DPG-BTree & DPG-GiST. We use a commercial version of PostgreSQL with distributed query processing. This
allows us to compare Spark SQL-IEJoin to a distributed
version of PG-BTree and PG-GiST.
Multi-node experimental setup. We use a compute
cluster of 17 Shuttle SH55J2 machines (1 master with 16
workers) equipped with Intel i5 processors with 16GB RAM,
and connected to a high-end Gigabit switch.

7.2

Parameters Setting

We show the effect of the two optimizations (Section 4),
as well as the effect of global sorting (Section 5).
Bloom filter. We run query Q2 on 10M tuples to show the
performance gain of using a Bloom filter. Results are shown
in Table 2. Note that the chunk size is an optimization parameter that is machine specific. For this experiment, L1
cache was 256KB. Intuitively, the larger the chunk size the
better. However, a very large chunk size defeats the purpose of using Bloom filters to reduce the bit-array scanning
overhead. The experiment shows that the performance gain
is 3X between 256 bits and 1,024 bits and 1.5X between
1,024 bits and 4,096 bits. Larger chunk sizes show worse
performance, as shown with chunk size of 16,384 bits.
Union arrays. To show the performance gain due to the
union optimization, we run IEJoin, with and without the
union array, using 10M tuples from the Events dataset. We

2081

Chunk (bit)
1
64
Time (sec) >1 day 1623

256
896

1024
296

4096
158

16384
232

3500

Parameter (M/sec)
IEJoin (union)
cache-references
6.5
cache-references-misses
3.9
L1-dcache-loads
459.9
L1-dcache-load-misses
8.7
L1-dcache-stores
186.8
L1-dcache-store-misses
1.9
L1-dcache-prefetches
4.9
L1-dcache-prefetches-misses
2.2
LLC-loads
5.1
LLC-load-misses
2.9
LLC-stores
3.8
LLC-store-misses
1.1
LLC-prefetches
3.1
LLC-prefetch-misses
2.2
dTLB-loads
544.4
dTLB-load-misses
0.9
dTLB-stores
212.7
dTLB-store-misses
0.1
Total time (sec)
125

IEJoin
8.4
4.8
1,240.6
10.9
567.5
1.9
7.0
2.7
6.6
3.7
3.7
1.2
4.1
2.9
1,527.2
1.6
592.6
0.1
325

Table 3: Cache statistics on 10m rows (Events data)
collect the following statistics, shown in Table 3: (i) L1 data
caches (dcache), (ii) last level cache (LLC), and (iii) data
translation lookaside buffer (dTLB). Note that the optimized algorithm with union arrays is 2.6 times faster than
the original one. The performance gain in the optimized
version is due to the lower number of cache loads and stores
(L1-dcache-loads, L1-dcache-stores, dTLB-loads and TLBstores), which is 2.7 to 3 times lower than the original algorithm. This behavior is expected since the optimized IEJoin
has fewer arrays compared with the original version.
Global sorting on distributed IEJoin. As presented in
Algorithm 3, the distributed version of our algorithm applies
global sorting at the pre-processing phase (lines 6-7). In this
experiment, we compare the performance of Q1 and Q2 with
and without global sorting. Figure 6 shows the results of
this experiment. At a first glance, one may think that the
global sorting affects the performance of distributed IEJoin
as it requires shuffling data through the network. However,
global sorting improves the performance of the distributed
algorithm by 2.4 to 2.9 times. This is because global sorting
allows us to filter out block-pair combinations that do not
generate results. We also observe that the time required by
the IEJoin process itself is one order of magnitude faster
when using global sorting.
We further breakdown the runtime of Q1 and Q2 in Table 4 to measure the impact of global sorting. Here, the
pre-processing time includes the data loading from HDFS,
global sorting, partitioning, and block-pairs materialization.
Even though global sorting increases the overhead of the preprocessing phase, we observe that the runtime for this phase
is at least 30% less compared with the case without global
sorting due to the reduced network overhead from eliminating unnecessary block pairs. The results confirm the above
observation: IEJoin is one order of magnitude faster when
pre-processing includes global sorting. This greatly reduces
the network overhead and increases the memory locality in
the block combinations that are passed to our algorithm.

Runtime (Seconds)

Table 2: Bloom filters on 10m rows (Events data)

Pre-processing

3000

IEJoin

2500

Post-processing

2000
1500
1000
500
0
Q1 Q2
With global sort

Q1 Q2
Without global sort

Figure 6: IEJoin using 100m rows on 6 workers
Query Pre-process IEJoin Post-process
With global sorting
Q1
632
162
519
Q2
901
84
391
Without global sorting
Q1
1,025
1,714
426
Q2
1,182
1,864
349

Total
1,313
1,376
3,165
3,395

Table 4: Time breakdown (secs) of Figure 6
Based on the above experiments, in the following tests we
used 1,024 bits as the default chunk size, union arrays, and
global sorting for distributed IEJoin.

7.3

Single-node Experiments

In this set of experiments, we study the efficiency of
IEJoin on datasets that fit the main memory of a single
compute node and compare its performance with alternative centralized systems.
IEJoin vs. baseline systems. Figure 7 shows the results
for queries Q1 and Q2 in a centralized environment, where
the x-axis represents the input size in terms of the number of
tuples, and the y-axis represents the corresponding running
time in seconds. The figure reports that PG-IEJoin outperforms all baseline systems by more than one order of magnitude for both queries and for every reported dataset input
size. In particular, PG-IEJoin is up to more than three
(resp., two) orders of magnitude faster than PG-Original
and MonetDB (resp., DBMS-X). We can clearly see that
the baseline systems cannot compete with PG-IEJoin since
they all use the classic Cartesian product followed by a selection predicate to perform queries with only inequality join
conditions. In fact, this is the main reason why they cannot
run for bigger datasets.
IEJoin vs. indexing. We now consider two different variants of PostgreSQL, each using a different indexing technique (GiST and BTree), to better evaluate the efficiency of
our algorithm with bigger datasets in a centralized environment. We run Q1 and Q2 on datasets with 10M and 50M
records. Figure 8 presents the results. In both experiments,
IEJoin is more than one order of magnitude faster than
PG-GiST. In fact, IEJoin is more than three times faster
than the GiST indexing time alone. We stopped PG-BTree
after 24 hours of runtime. Our algorithm performs better
than these two baseline indices because it better utilizes the
memory locality.
We observe that the memory consumption for MonetDB
increases exponentially with the input size. For example,
MonetDB uses 419GB for an input dataset with only 200K

2082

MonetDB
DBMS-X

PG-IEJoin
PG-Original

Runtime (Seconds)

Runtime (Seconds)

10000
1000
100
10
1
0.1
0.01

10K

50K

1000
100
10
1
0.1
0.01

100K

Query
Q1
Q1
Q1
Q1
Q2
Q2
Q2
Q2
Q3
Q3
Q3
Q3

MonetDB
DBMS-X

PG-IEJoin
PG-Original

10000

10K

Input size

50K

100K

Input size

(a) Q1
(b) Q2
Figure 7: IEJoin (centralized)
Indexing
X

10000

Querying
X

Runtime (Seconds)

Runtime (Seconds)

2000
1500
1000
500
0 P
G

P
P
PG PG PG
-IE G- G G-B
-IE - G -B
Joi iST Tre
Joi iST Tre
n
n
e
e

Q1
L3 cache

L3Q2cache

Indexing
X

8000

Querying
X

6000

Query

2000
0 P
G

P
P
PG PG PG
-IE G- G G-B
-IE - G -B
Joi iST Tre
Joi iST Tre
n
n
e
e
Q2

Q1
Q2

L3Q1cache

100
0

Cache-eﬃcient
Naive CP IEJoin
CPCache-eﬃcient
Naive CP
4500
3000
1500
0

Time(secs)
0.30
0.28
1.12
67.5
0.14
0.28
11.51
92.53
63.37
453.79
822.63
3,128.03

Mem(GB)
0.1
0.2
0.3
7.6
0.4
0.8
1.3
9.3
0.4
1.1
1.3
3.0

Table 5: Runtime and memory usage (PG-IEJoin)
Data
reading
158
319

Data
sorting
240
332

Bitarray
scanning
165
215

Total
time (secs)
563
866

Table 6: Time breakdown on 50M rows
Single-node summary. IEJoin outperforms existing
baselines by at least an order of magnitude for two main
reasons: it avoids the use of the expensive Cartesian product and it nicely exploits memory locality by using memorycontiguous data structures with a small footprint. In other
words, our algorithm avoids as much as possible going to
memory to fully exploit the CPU speed.

Cache-eﬃcient CP

450
Time (s)

200

Naive
IEJoin
CP
Time (ms)

Time (ms)

300

Output
9K
23K
68K
3M
0.2K
0.8K
42K
2M
154M
1B
2B
7B

4000

(a) 10M rows
(b) 50M rows
Figure 8: IEJoin vs. BTree and GiST (centralized)
IEJoin

Input
100K
200K
350K
10M
100K
200K
1M
10M
500k
1.5M
2M
4M

300
150
0

(a) L1 cache
(b) L2 cache
(c) L3 cache
Figure 9: Q1 runtime for data that fits caches
records. In contrast to MonetDB, IEJoin makes better
use of the memory. Table 5 shows that IEJoin uses around
150MB for Q1 (or less for Q2 ) for an input dataset of 200K
records (MonetDB requires two orders of magnitude more
memory). Note that in Table 5, we report the overall memory used by sorted attribute arrays, permutation arrays,
and the bit-array. Moreover, although IEJoin requires only
9.3GB of memory for an input dataset of 10M records, it
runs to completion in less than one hour (3,128 seconds) for
a dataset producing more than 7 billion output records.
We further analyze the breakdown time of IEJoin on the
50M rows datasets, as shown in Table 6. The table shows
that, by excluding the time required to load the dataset
into memory, scanning the bit-array takes only 40% of the
overall execution time where the rest is mainly for sorting.
This shows the high efficiency of our algorithm.
IEJoin vs. cache-efficient Cartesian product. We now
push further our evaluation to better highlight the memory
locality efficiency of IEJoin. We compare its performance
with both naive and cache-efficient Cartesian product joins
for Q1 on datasets that fit the L1 cache (256 KB), L2 cache
(1 MB), and L3 cache (8 MB) of the Intel Xeon processor.
We used 10K rows for L1 cache, 40K rows for L2 cache, and
350K rows for L3 cache. Figure 9 reports the results of this
experiment. When the dataset fits in the L1 cache, IEJoin
is 1.8 times faster than the cache-efficient Cartesian product
and 2.4 times faster than the naive Cartesian product. Furthermore, as we increase the dataset size of Q1 to be stored
at the L2 and L3 caches, we see that IEJoin becomes one
and two orders of magnitude faster than the Cartesian product, respectively. This is because of the delays of L2 and L3
caches and the complexity of the Cartesian product.

7.4

Multi-node Experiments

We now evaluate our proposal in a distributed environment and using larger datasets.
Distributed IEJoin vs. baseline systems. It is worth
noting that we had to run these experiments on a cluster
of 6 compute nodes only due to the limit imposed by the
free version of the distributed PostgreSQL system. Additionally, in these experiments, we stopped the execution of
any system that exceeds 24 hours. Figure 10 shows the results of all distributed systems we consider for queries Q1
and Q2 . This figure again shows that our algorithm significantly outperforms all baseline systems. It is at least one
order of magnitude faster that all other systems. In particular, we observe that only DPG-GiST could terminate before
24 hours for Q2 . In such a case, IEJoin is twice faster than
the time required to run GiST indexing alone. These results
show the high superiority of our algorithm over all baseline
systems also in a distributed environment.
Scaling input size. We further push the evaluation of
the efficiency in a distributed environment with bigger input datasets: from 100M to 500M records with large results size (Employees & Events), and from 1B to 6B records
with smaller results size (Employees2 & Events2). As we
now consider IEJoin only, we run this experiment on our
entire 16 compute nodes cluster. Figure 11 shows the runtime results as well as the output sizes. We observe that
IEJoin gracefully scales along with input dataset size in
both scenarios. We also observe in Figure 11(a) that, when
the output size is large, the runtime increases accordingly
as it is dominated by the materialization of the results. In
Figure 11(a), Q1 is slower than Q2 as its output is three
orders of magnitude larger. When the output size is relatively small, both Q1 and Q2 scale well with increasing input

2083

8000
4000
0

Indexing
X

16000

Querying
X
X

12000
8000
4000
0

Sp

D
D
S
S
ar PG- PG- par par
kS
k
k
B
QL GiST Tre SQL SQL
e
-IE
-SM
Joi
n

With materialization
Without materialization

600

Sp

D
D
S
S
ar PG- PG- par par
kS
k
k
B
QL GiST Tre SQL SQL
e
-IE
-SM
Joi
n

500
400
300
200
100
0

4.3M

10
1000

1
0.1

100
100M

200M 300M
Q1 time
Q2 time

0.01
400M 500M
Q1 result
Q2 result

(a) Employees & Events

10000

100
10

1000

1
0.1

100
1B

8000

Q3

7000
Runtime (Seconds)

100

12000
9000
6000
3000
0

430M

20.8M

208M

2050M

IEJoin output size limit

(b) Cloud - Q4

Figure 12: Runtime of IEJoin (c = 10)

10000
1000

With materialization
Without materialization

15000

(a) MDC - Q3

Result size (Millions)

10000

100000
Runtime (Seconds)

1000

Result size (Millions)

Runtime (Seconds)

10000

18000

IEJoin output size limit

(a) Q1
(b) Q2
Figure 10: Distributed IEJoin (100M rows, 6 nodes)
100000

43M

Runtime (Seconds)

12000

20000

X

Q3a

6000
5000
4000
3000
2000
1000

0.01
2B
3B
4B
5B
6B
Q1 time
Q1 result
Q2 time
Q2 result

0

c=5

c=10

(a) Runtime

(b) Employees2 & Events2

6000

Q3b
X
Result size (Billions)

Querying
X
X

Runtime (Seconds)

Indexing
X
X

16000

Runtime (Seconds)

Runtime (Seconds)

20000

Q3

5000

Q3a

Q3b

X

X

4000
3000
2000
1000
0

c=5

c=10

(b) Output size

Figure 11: Distributed IEJoin, 6B rows, 16 nodes

Figure 13: Without result materialization (c = 5, 10)

size (see Figure 11(b)). Below, we study in more details the
impact of the output size on performance.

A band join [9] of two relations R and S has a join predicate that requires the join attribute of S to be within some
range of the join attribute of R. The join condition is expressed as R.A − c1 ≤ S.B & S.B ≤ R.A + c2 , where c1
and c2 are constants. The band-join algorithm [9] partitions
the data from relations R and S into partitions Ri and Si
respectively, such that for every tuple r ∈ R, all tuples of
S that join with r appear in Si . It assumes that Ri fits
into memory. Contrary to IEJoin, the band join is limited
to a single inequality condition type, involving one single
attribute from each column. IEJoin works for any inequality conditions and attributes from the two relations. While
band join queries can be processed using our algorithm, not
all IEJoin queries can run with a band join algorithm.
Interval joins are frequently used in temporal and spatial data. The work in [11] proposes the use of the relational Interval Tree to optimize joining interval data. Each
interval intersection is represented by two inequality conditions, where the lower and upper times of any two tuples are
compared to check for overlaps. This work optimizes nonequijoins on interval intersections, where they represent each
interval as a multi-value attribute. Compared to our work,
they only focus on improving interval intersection queries
and cannot process general purpose inequality joins.
Spatial indexing is widely used in several applications with
multidimensional datasets, such as Bitmap indices [5,19], Rtrees [15] and space filling curves [4]. In PostgreSQL, support for spatial indexing algorithms is provided through a
single interface known as Generalized index Search Tree [16]
(GiST). From this collection of indices, Bitmap index is
the most suitable technique to optimize multiple attribute
queries that can be represented as 2-dimensional data. Examples of 2-dimensional datasets are intervals (e.g., start
and end time in Q2 ), GPS coordinates (e.g., Q3 ), and any
two numerical attributes that represent a point in an XY
plot (e.g., salary and tax in Q1 ). The main disadvantage of
the Bitmap index is that it requires large memory footprint
to store all unique values of the composite attributes [6, 22].
Bitmap index is a natural baseline for our algorithm, but,
unlike IEJoin, it does not perform well with high cardinality attributes, as demonstrated in Figure 6. R-trees, on the

Scaling dataset output size. We test our system’s scalability in terms of the output size using two real datasets
(MDC and Cloud) as shown in Figure 12. To have a full
control on this experiment, we explicitly limit the output
size from 4.3M to 430M for MDC, and 20.8M to 2050M for
Cloud. The figures clearly show that the output size affects
the runtime; the larger the output size, the longer it will take
to produce them. They also show that materializing a large
number of results is costly. Take Figure 12(a) for example, when the output size is small (i.e., 4.3M), materializing
them or not will have similar performance. However, when
the output size is big (i.e., 430M), materializing the results
takes almost 2/3 of the entire running time, as expected.
In order to run another set of experiments with much bigger output size, we created two variants of Q3 for MDC data
by keeping only two predicates over four (less selectivity).
Figure 13 shows the scalability results of these experiments
with no materialization of results. For Q3a , IEJoin produced more than 1, 000B records in less than 3, 000 seconds.
For Q3b , we stopped the execution after 2 hours with more
than 5, 000B tuples in the temporary result. This demonstrates the good scalability of our solution.
Multi-node summary. Similarly to the centralized environment, IEJoin outperforms existing baselines by at least
one order of magnitude. In particular, we observe that it
gracefully scales in terms of input (up to 6B input tuples).
This is because our algorithm first performs a join at the
metadata level, which is orders of magnitude smaller than
the actual data. As a result, it shuffles only those data partitions that can potentially produce join results. Typically,
IEJoin processes a small number of data partitions.

8.

RELATED WORK

Several cases of inequality joins have been studied; these
include band joins, interval joins and, more generally, spatial
joins. IEJoin is specially optimized for joins with at least
two predicates in {“<”, “>”, “≤”, “≥”}.

2084

other hand, are not suitable because an inequality join corresponds to window queries that are unbounded from two
sides, and consequently intersect with a large number of
internal nodes of the R-tree, generating unnecessary disk
accesses.
Several proposals have been made to speed-up join executions in MapReduce (e.g., [10]). However, they focus on
joins with equalities and hence are forced to perform massive data shuffling to be able to compare each tuple with
each other. There have been few attempts to devise efficient
implementation of theta-join in MapReduce [20, 24]. [20]
focuses on pair-wise theta-join queries. It partitions the
Cartesian product output space with rectangular regions of
bounded sizes. Each partition is mapped to one reducer.
The proposed partitioning guarantees correctness and workload balance among the reducers while minimizing the overall response time. [24] further extends [20] to solve multiway theta-joins. It proposes an I/O and network cost-aware
model for MapReduce jobs to estimate the minimum time
execution costs for all possible decomposition plans for a
given query, and selects the best plan given a limited number of computing units and a pool of possible jobs. We propose a new algorithm to do the actual inequality join based
on sorting, permutation arrays, and bit arrays. The focus
in these previous proposals is on efficiently partitioning the
output space and on providing a cost model for selecting the
best combination of MapReduce jobs to minimize response
time. In both proposals, the join is performed with existing algorithms, which in the case of inequality conditions
corresponds to Cartesian product followed by a selection.

9.

...THE END

To help Bob with his inequality join, we proposed two
algorithms for the efficient evaluation of joins defined with
inequality conditions. Our approach relies on auxiliary data
structures that enable efficient computations and require
small memory footprint. We presented a novel algorithm
that exploits data locality in the data structures to achieve
orders of magnitude speedup in the computation, and an
optimized version of the same for self-joins. For both algorithms, we discussed extensions and optimizations. Finally,
we presented both centralized and distributed versions of the
algorithms, which are implemented on top of PostgreSQL
and Spark SQL, respectively. Through extensive experiments over both synthetic and real data, we demonstrated
that our solution is superior to baseline systems: it is 1.5 to 3
orders of magnitude faster than commercial and open-source
centralized databases; and is at least 2 orders of magnitude
faster than the original Spark SQL. More interestingly, we
experimentally showed that, although theoretically the algorithm does not break the quadratic time bound, its performance is proportional to the size of the output. Future
directions include the selectivity estimation for inequality
join conditions to achieve better query optimization.

10.

ACKNOWLEDGMENTS

Portions of the research in this paper used the MDC
Database made available by Idiap Research Institute,
Switzerland and owned by Nokia.

11.

REFERENCES

[1] S. Abiteboul, R. Hull, and V. Vianu. Foundations of
Databases. Addison-Wesley, 1995.

[2] M. Armbrust, R. S. Xin, C. Lian, Y. Huai, D. Liu, J. K.
Bradley, X. Meng, T. Kaftan, M. J. Franklin, A. Ghodsi,
and M. Zaharia. Spark SQL: Relational Data Processing in
Spark. In SIGMOD, pages 1383–1394, 2015.
[3] P. Bohannon, W. Fan, F. Geerts, X. Jia, and
A. Kementsietsidis. Conditional functional dependencies for
data cleaning. In ICDE, pages 746–755, 2007.
[4] C. Böhm, G. Klump, and H.-P. Kriegel. XZ-Ordering: A
Space-Filling Curve for Objects with Spatial Extension. In
SSD, pages 75–90, 1999.
[5] C.-Y. Chan and Y. E. Ioannidis. Bitmap Index Design and
Evaluation. In SIGMOD, pages 355–366, 1998.
[6] C.-Y. Chan and Y. E. Ioannidis. An Efficient Bitmap
Encoding Scheme for Selection Queries. In SIGMOD, pages
215–226, 1999.
[7] X. Chu, I. F. Ilyas, and P. Papotti. Holistic data cleaning:
Putting violations into context. In ICDE, pages 458–469,
2013.
[8] J. Dean and S. Ghemawat. MapReduce: Simplified Data
Processing on Large Clusters. Communications of the
ACM, 51(1):107–113, 2008.
[9] D. J. DeWitt, J. F. Naughton, and D. A. Schneider. An
Evaluation of Non-Equijoin Algorithms. In VLDB, pages
443–452, 1991.
[10] J. Dittrich, J. Quiané-Ruiz, A. Jindal, Y. Kargin, V. Setty,
and J. Schad. Hadoop++: Making a yellow elephant run
like a cheetah (without it even noticing). PVLDB,
3(1):515–529, 2010.
[11] J. Enderle, M. Hampel, and T. Seidl. Joining Interval Data
in Relational Databases. In SIGMOD, pages 683–694, 2004.
[12] D. Gao, C. S. Jensen, R. T. Snodgrass, and M. D. Soo. Join
Operations in Temporal Databases. VLDB J., 14(1):2–29,
2005.
[13] H. Garcia-Molina, J. D. Ullman, and J. Widom. Database
systems. Pearson Education, 2009.
[14] N. K. Govindaraju, J. Gray, R. Kumar, and D. Manocha.
Gputerasort: high performance graphics co-processor
sorting for large database management. In SIGMOD, pages
325–336, 2006.
[15] A. Guttman. R-trees: A Dynamic Index Structure for
Spatial Searching. In SIGMOD, pages 47–57, 1984.
[16] J. M. Hellerstein, J. F. Naughton, and A. Pfeffer.
Generalized Search Trees for Database Systems. In VLDB,
pages 562–573, 1995.
[17] Z. Khayyat, I. F. Ilyas, A. Jindal, S. Madden, M. Ouzzani,
P. Papotti, J.-A. Quiané-Ruiz, N. Tang, and S. Yin.
BigDansing: A System for Big Data Cleansing. In
SIGMOD, pages 1215–1230, 2015.
[18] J. K. Laurila, D. Gatica-Perez, I. Aad, B. J., O. Bornet,
T.-M.-T. Do, O. Dousse, J. Eberle, and M. Miettinen. The
Mobile Data Challenge: Big Data for Mobile Computing
Research. In Pervasive Computing, 2012.
[19] T. L. Lopes Siqueira, R. R. Ciferri, V. C. Times, and C. D.
de Aguiar Ciferri. A Spatial Bitmap-based Index for
Geographical Data Warehouses. In SAC, pages 1336–1342,
2009.
[20] A. Okcan and M. Riedewald. Processing Theta-Joins using
MapReduce. In SIGMOD, pages 949–960, 2011.
[21] G. Smith. PostgreSQL 9.0 High Performance: Accelerate
your PostgreSQL System and Avoid the Common Pitfalls
that Can Slow it Down. Packt Publishing, 2010.
[22] K. Stockinger and K. Wu. Bitmap indices for data
warehouses. Data Warehouses and OLAP: Concepts,
Architectures and Solutions, 5:157–178, 2007.
[23] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and
I. Stoica. Spark: Cluster computing with wobitmap
indicesrking sets. In HotCloud, pages 10–10, 2010.
[24] X. Zhang, L. Chen, and M. Wang. Efficient Multi-way
Theta-Join Processing Using MapReduce. PVLDB,
5(11):1184–1195, 2012.

2085

Rheem: Enabling Multi-Platform Task Execution
Divy Agrawal2∗ Lamine Ba1 Laure Berti-Equille1 Sanjay Chawla1 Ahmed Elmagarmid1
Hossam Hammady1 Yasser Idris1 Zoi Kaoudi1 Zuhair Khayyat4∗ Sebastian Kruse3∗
Mourad Ouzzani1 Paolo Papotti5∗ Jorge-Arnulfo Quiané-Ruiz1 Nan Tang1 Mohammed J. Zaki6∗
1

4

Qatar Computing Research Institute, HBKU 2 University of California, Santa Barbara 3 Hasso Platner Institute
King Abdullah University of Science and Technology 5 Arizona State University 6 Rensselaer Polytechnic Institute

ABSTRACT

the different processing platforms and to deal with their interoperability. There is a clear need for a system that eases
the integration among different processing platforms by automatically dividing a task into subtasks and determining
the underlying platform for each subtask.
Motivating Example. Let us illustrate these two problems by using an oil & gas use case [7]. A single oil company
can produce more than 1.5TB of diverse data per day [3],
which may be structured or unstructured. During the exploration phase, data has to be acquired, cleaned, integrated,
and analyzed in order to predict if a reservoir would be profitable. Thousands of downhole sensors in exploratory wells
produce real-time seismic data for monitoring resources and
environmental conditions. Users integrate these data with
the physical properties of the rocks to visualize volume and
surface renderings. From these visualizations, geologists and
geophysicists formulate hypotheses and verify them with
machine learning methods. Thus, an application supporting such a complex analytic pipeline has to access several
sources for historical data (relational, but also text and semistructured), remove the noise from the streaming data coming from the sensors, possibly integrate a subset of these
data sources, and run both traditional (such as SQL) and
statistical analytics (such as machine learning algorithms)
over different processing platforms.
Rheem. We recently presented a vision of Rheem1 , our
solution to tackle these two problems and thus provide data
processing freedom [2]. We propose a three-level data processing abstraction that allows a large variety of applications to achieve processing platform independence as well as
multi-platform task execution. The latter makes our proposal distinct from [5], which does not include an optimizer
for automatic multi-platform task execution. Furthermore,
in contrast to [6], Rheem also considers the cost of data
movement across underlying platforms. In this demonstration, we will showcase the benefits of Rheem in terms of
flexibility and efficiency. To prove its applicability, we will
present three applications, namely, machine learning, data
cleaning, and truth discovery.

Many emerging applications, from domains such as healthcare and oil & gas, require several data processing systems
for complex analytics. This demo paper showcases Rheem, a
framework that provides multi-platform task execution for
such applications. It features a three-layer data processing abstraction and a new query optimization approach for
multi-platform settings. We will demonstrate the strengths
of Rheem by using real-world scenarios from three different
applications, namely, machine learning, data cleaning, and
data fusion.

1.

NEED FOR FREEDOM

Following the philosophy “one size does not fit all”, we
have embarked on an endless race of developing data processing platforms for supporting different tasks, e.g., DBMSs
and MapReduce-like systems. While these systems allow us
to achieve high performance and scalability, users still face
two major problems.
Platform Independence. Users are faced with a large
number of choices on where to process their data. Each
choice comes with possibly orders of magnitude differences
in terms of performance. Moreover, whenever a new platform that achieves better performance than the existing ones
becomes available, users are enticed to move to the new
platform, e.g., Spark taking over Hadoop. Typically, such
a move does not come without pain. Therefore, there is a
clear need for a system that frees us from the burden and
cost of re-implementing and migrating applications from one
platform to another.
Multi-Platform Task Execution. Several complex data
analytic pipelines are emerging in many different domains.
These complex pipelines require combining multiple processing platforms to perform each task of the process and then
integrating the results. Performing this combination and integration requires users to be intimate with the intricacies of
∗

Work done while at QCRI.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

2.

RHEEM OVERVIEW

The current prototype of Rheem provides two main concepts of the vision presented in [2]: the data processing
abstraction and the multi-platform task optimizer. In this
section, we start by describing the general architecture of
Rheem. We then briefly discuss its two main features.

SIGMOD’16, June 26-July 01, 2016, San Francisco, CA, USA
c 2016 ACM. ISBN 978-1-4503-3531-7/16/06. . . $15.00


1

DOI: http://dx.doi.org/10.1145/2882903.2899414

2069

http://da.qcri.org/rheem/

Figure 2: Rheem development interface.
Such an abstraction enables both ease-of-use, by hiding underlying implementation details from the users (e.g., task
parallelization), and high performance, by allowing several
optimizations, e.g., seamless distributed execution. A logical operator works on data quanta; these are the smallest
units of data from an input datasets. For example, a data
quantum might represent a tuple in the input dataset or a
row in a matrix. This fine-grained data model allows us to
apply operators in a highly parallel fashion and thus achieve
better performance.
To provide more insights about this layer, let us illustrate
its use via a machine learning (ML) example. Consider a
developer who wants to offer end users logical operators to
implement various ML algorithms. He can define three basic operators: (i) Initialize, to initialize algorithm-specific parameters, e.g., initializing cluster centroids, (ii) Process, for
the computations required by the ML algorithm, e.g., finding the nearest centroid of a point, and (iii) Loop, for specifying the stopping condition. Users can implement algorithms, e.g., SVM, K-means, and linear/logistic regression,
using these operators.
Core Layer. This layer, which is at the heart of Rheem,
exposes a pool of Rheem operators. Each represents an algorithmic decision for executing an analytic task. A Rheem
operator is a platform-independent implementation of a logical operator whereby a developer can deploy a new application on top of Rheem. For example, in the above ML
example, the application optimizer maps Initialize to a Map
Rheem operator and Process to a GroupBy Rheem operator.
The system also allows developers to define new operators as
needed. Once an application has produced a Rheem plan,
the system translates this plan into an execution plan by
optimizing it according to the underlying processing platforms. Therefore, in contrast to DBMSs, Rheem produces
execution plans that can run on multiple platforms.
Platform Layer. An execution operator (in an execution
plan) defines how a task is executed on the underlying processing platform. In other words, an execution operator is
the platform-dependent implementation of a Rheem operator. For instance, consider again the above ML example,
the MapPartitions and ReduceByKey execution operators for
Spark are one way to perform Initialize and Process.
Defining mappings between Rheem and execution operators is the developers’ responsibility whenever a new execution platform is plugged in. Rheem relies on an internal mapping structure that models the correspondences be-

Figure 1: Rheem architecture.

2.1

Overview

Overall, our system provides a three-layer data processing
abstraction that sits between user applications and data processing platforms such as Hadoop and Spark (see Figure 1):
(i) an application layer that models all application-specific
logic; (ii) a core layer that provides the intermediate representation between applications and processing platforms;
and (iii) a platform layer that embraces the underlying processing platforms. The communication among these three
layers is enabled by operators defined as user defined functions (UDFs). Rheem provides a set of operators at each
layer, namely, logical operators, Rheem operators, and execution operators. Using user-provided implementations of
the logical operators specific to the application, the application layer produces a set of possible optimized Rheem plans.
An application passes these Rheem plans to the core layer
together with the cost functions to help the optimizer in
choosing the best plan. These cost functions are obtained
by applications and passed to the core layer as UDFs. At
the core layer, Rheem performs several multi-platform optimizations and outputs an execution plan. Then, the underlying processing platforms might further optimize a subplan
for better performance. Notice that, in contrast to a DBMS,
Rheem decouples the core (physical) level from the execution one. This separation allows applications to express a
Rheem plan in terms of algorithmic needs, without being
tied to any platform.
These three layers allow Rheem to provide applications
with platform independence, which is exploited by the
Rheem optimizer to perform multi-platform task execution.
Such execution is crucial to achieve the best performance at
all times. In the following, we discuss in more details the
data processing abstraction as well as the Rheem optimizer.

2.2

Data Processing Abstraction

We now discuss how developers can define operators at
the three levels of the Rheem data processing abstraction.
Application Layer. A logical operator is a UDF that acts
as an application-specific unit of data processing. Basically,
it is a template where users provide the logic of their tasks.

2070

tween operators together with context information such as
cost functions. The context is needed for the effective and
efficient execution of each operator.

2.3

dressed. Finally, distributing an MLA is not always the best
choice to proceed, especially if the data is small or the MLA
is sequential. In this demo, we will show how Rheem tackles the above problems through platform independence and
multi-platform execution.
Scalable Clustering. We will consider the real case from a
large airline based in the middle east to carry out large scale
clustering for personalizing promotion offers to customers.
For this, we will use the K-means algorithm and demonstrate the power of Rheem to distribute the execution of a
classical clustering algorithm depending on the dataset size.
The audience will also be able to choose among a selection of
input datasets are taken from UCI, a publicly available ML
respository, and visually see the Rheem plans produced by
the application as well as the execution plans produced. The
goal of this use case is to show how platform independence
not only frees the users from platform-specific implementation details but can also lead to huge performance benefits.
Multi-Platform Gradient Descent. We consider three
variations of gradient descent (GD), namely batch GD
(BGD), stochastic GD and mini-batch GD (MGD). The audience will be able to choose among these three methods to
perform classification on a set of input datasets from UCI
as well as tune some parameters such as the batch size. Depending on the chosen method and the size of the batch, the
users will be able to see how a specific plan can run partly
in Spark and partly in a JVM platform. The overall goal of
this use case is to demonstrate how multi-platform execution
achieves better performance.
Datasets.
We will use a dataset from a large airline
based in the middle east. The data is spread across intraorganizational boundaries making it impossible to create a
unified dataset to apply a standard clustering algorithm. We
will show how Rheem operators can be used to decouple the
algorithm design from the underlying processing platform
which in turn can be decoupled from the precise data storage layout. The second set of datasets we will use is taken
from UCI, a publicly available ML repository.

Multi-Platform Optimization

The optimizer is responsible for translating a given abstract RheemPlan (or a set of alternatives) into the most
efficient ExecutionPlan. The resulting ExecutionPlan consists
of a set of platform-specific subplans. Notice that this optimization problem is quite different from traditional database
query optimization and thus poses several challenges. First,
Rheem itself is highly extensible and hence neither the
RheemPlan operators nor the platforms are fixed. Second, the optimizer should be extensible on how to translate
RheemPlans to platform-specific plans. Finally, Rheem’s
data processing abstraction is based on UDFs, so operators
appear to the optimizer as black-boxes; making cost and
cardinality estimations harder.
Rheem tackles these challenges as follows. First, it applies
an extensible set of graph transformations to the RheemPlan
to find alternative ExecutionPlans. Then, it compares those
alternatives by using the ExecutionOperator’s cost functions.
These can either be given or learned, and are parameterized
w.r.t. the underlying hardware (e.g., number of computing nodes for distributed operators). Because Rheem data
processing abstraction is based on UDFs, which are blackboxes for the optimizer, domain-specific optimizations have
to be done in collaboration with applications. Rheem lets
applications expose semantic properties about their functions as in [10], and furthermore provides optimization hints
(e.g., numbers of iterations), constraints (e.g., physical collocation of operators), and alternative plans. The optimizer
uses those artifacts where available in a best-effort approach.
When the optimizer has decided upon an ExecutionPlan,
the Engine executes that plan by (i) scheduling the different
subplans, (ii) orchestrating the data flow across platforms,
and (iii) collecting statistics of the execution to further improve its optimizations.

3.

DEMONSTRATION

3.2

Data Cleaning

Our main goal in this demo is to show the benefits of
Rheem with respect to three main aspects: (i) diversity
of applications, (ii) platform independence, and (iii) multiplatform execution. The audience will be able to interact
with Rheem directly through its GUI, as well as with applications, namely machine learning, data cleaning, and truth
discovery, built on top of the system. The GUI enables users
to drag-and-drop Rheem operators to create a physical plan
and see how the Rheem optimizer transforms it into an execution plan (see Figure 2).

Data cleaning, which is detecting and repairing data errors, is critical in data management and data analytics. This
is because high-quality business decisions must be made
based on high-quality data. In response to the need of supporting heterogeneous and ad-hoc quality rules for various
applications, we have built a commodity data cleaning system Nadeef [4]. We have further extended Nadeef using Rheem [8]. In this demo, users will have the opportunity to experience how Rheem can boost the performance
of Nadeef through platform independence.

3.1

Rule Specification in Logical Level. Figure 3 displays
the Nadeef GUI for specifying data quality rules. The users
can either (a) load rules using rule classes e.g., CFDs, MDs
or DCs; or (b) implement a customized rule by writing functions based on programming interface in a few lines of code.

Machine Learning

Many current applications, such as our oil & gas example,
require highly efficient machine learning algorithms (MLAs)
to perform scalable statistical analytics. However, ensuring
high efficiency for MLAs is challenging because of (i) the
amount of data involved and (ii) the number of times a
MLA has to process the data. Simply distributing MLAs
is not obvious due to their iterative nature. Hence, current
distributed solutions suffer from performance bottlenecks.
In addition, users have to deal with many physical details
for implementing a MLA. Achieving high performance and
ease-of-use are major lacunae that need to be urgently ad-

Scalable Nadeef.
We will first show how logical
Nadeef operators can be mapped to Rheem operators.
Through these different operators, Rheem frees Nadeef
from platform-specific implementation details. That is,
Rheem will decide, based on the logical Nadeef operators, the best platform to execute this task for best performance. Moreover, to show extensibility, we extended the

2071

ing their scalability [11]. In fact, most truth discovery techniques are platform-dependent implementations with singlenode execution. In this demo, we will use a typical data
fusion scenario to demonstrate how Rheem enables truth
discovery algorithms to scale and be platform independent,
with no additional effort.
Scalable Truth Discovery. We will show how a truth
discovery application can be built on top of Rheem, thanks
to the flexibility of its operators. In particular, we will show
how such an application can leverage the platform independence of Rheem to overcome the common single-node execution of such algorithms. We will show how Rheem leverages Spark by seamlessly distributing operations like value
similarity.
Datasets. We showcase our scalable Rheem truth discovery application using a large-size real-world biographical
dataset. We show the improvement obtained from distributing RHEEM operations on SPARK against single-node execution on Java platform. Our dataset, whose entries (claims
and sources) are extracted from several Wikipedia articles,
contains over 10 millions values from one million sources.

4.
Figure 3: Rule specification in Nadeef.
set of physical Rheem operators with a new join operator
(called IEJoin [9]) to boost performance. This new operator
provides a fast algorithm for joins containing only inequality
conditions.
Datasets. We use three real-world large datasets. The first
one is from an intelligence company that monitors over 700K
Web sources. Errors are captured by temporal FD rules [1].
The second is a traffic data of Doha. Duplicates and erroneous readings are reported due to anomalies both in the
detectors and in the Bluetooth devices. The third dataset
comes from sensors reading from wells in an oil field. Erroneous readings need to be detected from different sensors in
the Well during its normal operation, shutting, and opening . They are detected using different statistics-based data
cleaning rules. We will demonstrate the significant performance gain (in orders of magnitude), by leveraging the easy
mapping from Nadeef logical operators to Rheem operators, and the platform independence of Rheem to find the
fastest execution plan.

3.3

REFERENCES

[1] Z. Abedjan, C. G. Akcora, M. Ouzzani, P. Papotti,
and M. Stonebraker. Temporal rules discovery for web
data cleaning. PVLDB, 9(4):336–347, 2015.
[2] D. Agarwal, S. Crawla, A. Elmagarmind, Z. Kaoudi,
M. Ouzzani, P. Papati, J.-A. Quiané-Ruiz, N. Tang,
and M. J. Zaki. Road to Freedom in Big Data
Analytics. In EDBT, 2016.
[3] A. Baaziz and L. Quoniam. How to use big data
technologies to optimize operations in upstream
petroleum industry. In 21st World Petroleum
Congress, 2014.
[4] M. Dallachiesa et al. NADEEF: A Commodity Data
Cleaning System. In SIGMOD, 2013.
[5] A. Elmore et al. A Demonstration of the BigDAWG
Polystore System. In VLDB 2015 (demo), 2015.
[6] I. Gog, M. Schwarzkopf, N. Crooks, M. P. Grosvenor,
A. Clement, and S. Hand. Musketeer: All for One, One
for All in Data Processing Systems. In EuroSys, 2015.
[7] A. Hems, A. Soofi, and E. Perez. How innovative oil
and gas companies are using big data to outmaneuver
the competition. Microsoft White Paper,
http://goo.gl/2Bn0xq, 2014.
[8] Z. Khayyat, I. F. Ilyas, A. Jindal, S. Madden,
M. Ouzanni, P. Papotti, J.-A. Quiané-Ruiz, N. Tang,
and S. Yin. BigDansing: A System for Big Data
Cleansing. In SIGMOD, 2015.
[9] Z. Khayyat, W. Lucia, M. Singh, M. Ouzzani,
P. Papotti, J.-A. Quiané-Ruiz, N. Tang, and P. Kalnis.
Lightning Fast and Space Efficient Inequality Joins.
PVLDB, 8(13), 2015.
[10] A. Rheinländer, A. Heise, F. Hueske, U. Leser, and
F. Naumann. SOFA: An extensible logical optimizer
for UDF-heavy data flows. Inf. Syst., 52:96–125, 2015.
[11] D. A. Waguih and L. Berti-Equille. Truth Discovery
Algorithms: An Experimental Evaluation. CoRR,
1409.6428, 2014.

Truth Discovery

Many real-world applications, e.g., our oil & gas use case,
might face the problem of discovering the truth when merging conflicting information from a collection of heterogeneous sources. Such settings can use truth discovery algorithms that aim at resolving conflicts from different data
sources in an automatic manner. Most of current truth discovery algorithms conduct an iterative process, which converges when a certain accuracy threshold or a given userspecified number of iterations is reached. As a result, some
operations, such as the pairwise value similarity, highly impact the performance of truth discovery algorithms, limit-

2072

Clip:

a Visual Language for Explicit Schema
Mappings

Alessandro Raffio*, Daniele Braga*, Stefano Ceri*, Paolo Papottit, Mauricio A. Hemaindezt
*Dipartimento di Elettronica e Informazione, Politecnico di Milano
Piazza Leonardo da Vinci 32, Milano, Italy
raffio,braga,ceri at elet.polimi.it

tDipartimento di Informatica e Automazione, Universita di Roma Tre
Via della Vasca Navale 79, Roma, Italy
papotti at dia.uniroma3.it

+IBM Almaden Research Center
San Jose, CA, US

mauricio at almaden.ibm.com

prototype developed jointly between IBM Almaden Research
Center and the University of Toronto; it automatically creates
mappings given two schemas and a set of value mappings
between their atomic elements. Clio builds an internal representation capable of representing relational and XML schemas;
depending on the kind of source and target schemas, Clio can
render queries that convert source data into target data in a
number of languages (XQuery, XSLT, SQL/XML, SQL).
This paper introduces Clip, a new schema mapping language developed at Politecnico di Milano. Users of Clip enter
mappings by drawing lines across schema elements and by
annotating some of the lines. Clip then produces the queries
that implement the mapping. The main difference introduced
by Clip relative to its predecessors, and most specifically to
Clio that greatly influenced Clip's design, is the introduction of
structural mappings in addition to value mappings; this gives
users greater control over the produced transformations. Clip
mappings are less dependent on hidden automatisms, whose
assumptions may fail in capturing the intended semantics.
Clip was designed to work with XML Schemas. But, just
like Clio, Clip also works with relational schemas, as long as
they are converted in a canonical way into XML Schemas. In
general, Clip should work with any schema model that can be
visually represented as a nested containment of relations.

Abstract- Many data integration solutions in the market today
include tools for schema mapping, to help users visually relate
elements of different schemas. Schema elements are connected
with lines, which are interpreted as mappings, i.e. high-level
logical expressions capturing the relationship between source and
target data-sets; these are compiled into queries and programs
that convert source-side data instances into target-side instances.
This paper describes Clip, an XML Schema mapping tool distinguished from existing tools in that mappings explicitly specify
structural transformations in addition to value couplings. Since
Clip maps hierarchical XML schemas, lines appear naturally
nested. We describe the transformation semantics associated with
our "lines" and how they combine to form mappings that are
more expressive than those generated by Clio, a well-known
mapping tool. Further, we extend Clio's mapping generation
algorithms to generate Clip's mappings.
I. INTRODUCTION

Schema mappings are created and maintained for two purposes. In the most popular usage scenario, given a schema
mapping between a source and a target schema, mapping
tools automatically create a transformation script (a query
or program) which converts a source side instance into a
target side instance. In other words, mapping tools help data
integration engineers write long and complex programs to
transform data. Another usage of schema mappings is to
maintain relationships between schema elements, for later use
in impact analysis (change management) and data lineage.
However, we do not further consider this problem in the paper.
Schema mapping tools are characterized by GUIs that place
a source structure on one side of the screen and a target
structure on the other side. Users specify the correspondences
by drawing lines across the source and target structures. Lines
are typically annotated with features that carry some of the
transformation semantics (e.g., filtering predicates, functions,
etc.). Many such tools are available in the market today (e.g.,
Altova MapForce, Microsoft BizTalk, Stylus Studio, IBM
Rational Data Architect)'. Clio [1][2] is a schema mapping

1On

the Web:

A. Motivating Example and Approach
XML Schemas are represented as trees; attributes2 and
text are represented by black and white circles respectively,
elements by squares, cardinalities by both icons and labels:3
value a @name: type

nodeso type

elements[tagname

multiple [ZI1tagname [1..*]

elements[Jtagname [0..*]

2The attribute name, next to the black circle, is preceded by an "@" sign
to resemble the XPath symbol for accessing attributes.
3Notice that the question marks and the zeroes in the minimum cardinality
indicate optionality, while shadowed icons and stars in the maximum cardi-

www.altova.com/MapForce, www.microsoft.com/biztalk,

www.stylusstudio.com, www.ibm.com/software/data/integration/rda.

978-1-4244-1837-4/08/$25.00 (© 2008 IEEE

single W3tagname [0..1]

nality indicate multiplicity.

30

ICDE 2008

D source

Referential integrity between attributes is represented by a
dashed line (as for @pid attributes of regEmps, which refer to
those of Projs).
Consider the following XML document instance describing
two departments (compacted due to space limits):
source ---dept ---dname

=

ICT
Did =
niame =
Did =
niame =

Ijdept [1..*]

D dname
0 value:String
Wj Proj [O..*]
i- * 0 @pid:int
C pname
D
W

iar

obo

-@pid

o value:String
regEmp [0..*]
*@pid: int

C target

tI.. department [1..*]
t project [O..*]
@ name: String
W employee [0..*]
@name: String

D ename

o value:String

-ename

;Z sal
o value:int

pid

drew Clai

rname

Fig. 1. A simple yet problematic mapping
name

=

Mark Tan

name

=

Jim

,Iar}

-dept ---dnamE
--- Proj-

Did
I'--- pname

---Proj---@pid
'

---

---

desired default behavior. This paper addresses how to express
this and more complex mappings, by controlling structural
transformations.

pname

=
=

Brand promotion
0032

regEmp ---@~pid

B. Related Work
The schema mapping generation problem received lots of
attention by the research community in the last years [1] [3] [4].
Schema mapping generation studies how to automatically
create schema mappings given as input two schemas and
a set of value mappings between their atomic elements. In
essence, mapping generation algorithms ascribe transformation
semantics to value mappings generating first order logical
formulas that are independent of the actual transformation
language and easier to study than executable scripts.
With respect to existing approaches, we allow users to
express more complex mappings by introducing a new GUI,
extended with set correspondences and aggregate functions.
Our contribution tries to bring an initial answer to the growing
demand for more complex schema mappings [5]. To generate
such mappings we introduce new generation algorithms that
generate second-order logical formulas, which allow to express grouping and aggregate by means of functions. Without
functions, our mapping language reduces to the language of
GLAV mappings [6], or source-to-target tgds (tgd stands for
tuple generating dependency) [7]; in the object-relational case,
in fact, it is a nested-relational extension of second order
tgds [8] and the two languages coincide in the relational
case. Nested second order tgds have been already presented
in schema evolution scenarios [9] and in the nested mapping
paradigm [2]. Strictly referring to the mapping language
expressiveness, the main difference with these approaches is
that they only consider Skolem functions, while we extend the
set of supported functions.

Appliances

=

0001

=

---ename

=

Richard Dawson

30000
0032
regEmp ---@~pid
=
Mark Tane
l--- ename
--- sal = 10000
|
'--- regEmp ---@~pid = 0001
|--- ename = Steven Aiking
'--- sal = 20000
'--- sal

=

=

---

Each department has a name, a list of projects (with an
identifier and a name), and a list of regular employees (with
a name, a salary, and a @pid attribute referring to the project
they work on). This instance is valid w.r.t the source XML
Schema on the left of Figure 1. We want to transform the
source instance into one of the target schema on the right of
Figure 1. The desired output is:
-arget ---departm
--- eml

I--- eml

I

-departm

---

emf

emi
emi
emi

ject ---@~name
ject ---@~name
,loyee ---@ nam,
,loyee ---@ nam,
,loyee ---@ nam,
,loyee ---@ nam,
ject ---@~name
ject ---@~name
,loyee ---@ nam,
,loyee ---@ nam,
,loyee ---@ nam,

Dpliances

Dbotics
John Smith
%ndrew Clare
dark Tane
Jim Bell1i sh
ad promoti

Dpliances

Richard Daws
dark Tane

None of the generation tools we are aware of can obtain
such (simple) result. Figure 1 is actually an attempt to express
our mapping in Clio. It only gets close to the target, as
it compiles to a transformation that outputs projects and
employees, but encloses each node in a different department
element, not preserving containment and sibling relationships:
-arget --- departmE
--- departmE
'

---departmE

ject ---@~namr
ject ---@~namr

Dbc

II. THE CLIP LANGUAGE
Clip builds on the visual representation of XML Schemas
introduced in Section I-A and uses two different kinds of lines
to connect source and target nodes:
Value mappings connect value nodes to establish correspondences between atomic values (as in Clio). In general, value
mappings convert one or more source values into a target

ian

nam

This happens because structural transformations are inferred
from value mappings instead of being under the user's control.
The inference rule states, in this case, that a department be
generated for each mapped value; however, this is not the

31

Value mappings are thin arrows with open
ends connecting value nodes; optional
labels may specify an aggregate function.
Builders are thick arrows with close ends
connecting elements and possibly build
nodes.
Build nodes have at least 1 incoming and
at most 1 outgoing builderand a label
expresses filtering conditions in terms of
the variables on fhe builders.
A special build node is used for grouping.
The grouping attributes are reported along
with the 'group-by" keyword.
CPTs are trees of build nodes and
context arcs.

«agg regale» N.

$x.att1=$y.att2

$x

[

group-by

attributes
5

'

Ildept [1..*]
dname
o

value:

department [1..*]
project [0..*]

String

Proj [0..*]

r*0

...

|

name:

pid:int

$r.sal. value

pname
o

value: String

regEmp [O..*]

>

11000

String

employee [0..*]
-

@name:

String

-

I *@pid:int

D-] ename
-111

Context

value: String
sal
o value: int

o

A mapping with context propagation

Fig. 4.

- ~~ Propagation
-_
----~Tree
r

target

source

A. Clip Mapping examples
Simple mapping. Figure 3 shows a simple mapping, where an
employee is created for each regEmp whose salary ($r.sal.value)
LZsource
greater than 11000. For each such employee, the name
is
dept [1 .*]
target
is copied to the name attribute (a value mapping
(ename)
| dname
department [1..*]
value: String
$rsal
connects ename.value to @name). Note the condition expressed
Proj [0.*]
using variable $r taken from the incoming builder. This
value: int
r +-0@ pid:int
mapping is indeed expressible in standard Clio with the same
]jpname
~ employee [0.*]
value mapping and a suitable filtering condition.
-@ name: String
o value: String
[ regEmp[O.*]
/ W1 works-in [0.....
This mapping is underspecified with respect to the target
1]
@ pid:int
0 value: int
schema. For example, the target area element plays no role
! oename
in the mapping and is not generated. However, this is not a
value: String
$rename.value
LZsal
problem because the area element is optional and, thus, does
o value: int
not need to be generated. The mapping also does not specify
how
many department elements should be generated. A notion
Fig. 3. A simple Clip mapping
of universal solutions for data exchange was introduced by [7].
Universal solutions satisfy all data dependencies without any
extra assumptions. In the case of our mapping example, if only
value. Simple one-to-one value mappings represent the identity the value mapping is given, then at least two valid solutions
function and copy a single source value as a target value. More exists: one that has one target department with one nested
complicated transformations require the user to add a scalar employee element for each source regEmp, and another that
function that transform the one or more source values into has only one target department and as many nested employee
a target value. For example, value mappings can concatenate
elements as source regEmp elements. For our purposes, though,
multiple source values or perform an arithmetic operation on we adopt a minimum-cardinality principle and build as few
source values. Object mappings (or builders) connect elements
elements as possible, compatible with the schema constraints.
and rule structural transformations. Intuitively, builders repre- When no builders are given, Clip generates the minimum
sent iterators on the source nodes they are drawn from: in number of elements necessary for the result to comply with the
each iteration, a new element is constructed, of the kind of target schema. For our example, Clip produces the following
the target node reached by the builder.
solution:
We introduce some further notations. Build nodes are an- target---departm
nam
.ndrew Clare
em]
nam
.ichard Daws
notated nodes placed between the schemas and connected to
nam
em]
schema nodes by builders; build nodes can also be connected
one to another by context arcs into tree structures, named conWhen builders are used, the proliferation of generated
text propagation trees (CPTs). Build nodes have l..n incoming elements is strictly controlled by the builders and the target
builders, 0.. 1 incoming context arcs, 0.. 1 outgoing builders and schema constraints.
O..n outgoing context arcs. Incoming builders may be tagged Context propagation. The mapping in Figure 4 creates a
with variables, representative of the nodes from which they department for each dept and collects all the regEmps (with
originate, if they need to be referenced in node labels. Last, salary greater than 11000) of the dept as employees of the
group nodes are build nodes marked by a "group-by" label and
corresponding department. This is performed by drawing a first
a list of grouping attributes. Figure 2 summarizes the visual
builder from dept to department (through a build node) and
a second builder from regEmp to employee (through another
syntax of Clip.
Fig. 2.

The Clip syntax in

a

nutshell

value

>

11000

I

32

---

z7va.\1r

D source
udept[l -*]
D dname
0 value: String
X Proj [0..*]

build node). The build nodes are connected by a context arc to
enforce a hierarchy of builders: regEmps are mapped according
to the inner builder within the context of a specific dept, fixed
by the outer builder (intuitively, by an "outer iteration"). The
result is as follows:
arget---departmE
'---departmE

nam
nam
nam

emi
emi

r

F ] pname

|

A, 1r,

'---departm

nam
nam
nam
nam
nam
nam

emi
emi
emi
emi
emi
emi

* @name: String

o value: String

regEmp[O.*]
@pid: int
ename

Omitting the context arc causes all employees (with sal >
11000) to appear, repeated, within all departments, disregarding the original containment relationships:
arget ---departm

*i project [0..*]
*0 @name: String
F employee [O*]

r#0@pid:int

%ndrew Clare
Richard Daws
:P4-

D target
21 department[1 ..*]

o value: String
Z sal
o value: int

%ndrew Clare
Richard Daws
Steven Aikin
%ndrew Clare
Richard Daws

Fig. 5. A more complex Clip mapping
D

source

D target

- dept [l..]
'LZdname
0 value: String
j Proj [O*]
r * 0-¢ pid: int
g pname
o value: String
W regEmp [0.*]
* @ pid: int

1

project-emp [l..*]

*@.pname: String
Context propagation tree. The mapping in Figure 5 is the first
(0 @ename: String
$p.@pid =
example of what cannot be obtained by state-of-the-art tools.
$r.@pid
This mapping solves the problem we discussed in Section I by
specifying a builder and propagating its context twice. Namely,
department elements are generated by the topmost builder,
while project and employee elements are generated considering
ename
the current topmost mapping into department.
o value: String
Cartesian product and join. When a build node is reached
JI sal
o value: int
from two or more source schema nodes, Clip computes the
Cartesian product of the source data selected by each builder.
Fig. 6. A join constrained by a CPT
Users can add a filtering condition on the label of the build
node. If this condition involves two different variables, Clip
computes a Join between the source data selected by the build
the overall Cartesian product of all regEmps and Projs in the
node.
The mapping in Figure 6 combines Projs and regEmps into whole document.
a flattened list of elements that represents the association of Grouping. Group nodes allow users to group source data
employees to the project they work on (joined on @pid). The based on a set of grouping attributes. While regular build nodes
topmost build node has no output builder (nothing has to produce simple sequences of elements, a group node creates
be built at dept granularity); however, a context arc restricts a set of sequences of elements, grouped along the grouping
the context of the Cartesian product of Projs and regEmps to attributes. The cardinality of the result (number of sequences)
nodes within the same dept. The second build node filters is given by the number of distinct values of the grouping
the Cartesian product pairs ($p.@pid = $r.@pid), so that the attributes. The outgoing builder, if any, constructs one target
computed operation is a join. This join condition can be element for each distinct values of the grouping attributes.
The mapping in Figure 7 groups Projs by name. A project is
entered by the user or can be automatically suggested using
created for each distinct project name, i.e. for each group of
the existing referential integrity constraint. The result is:
homonymous Projs. Each such group is passed as context to
target ---pro ject-emp ---pname
Appliances
=

'---Sename

=

John

'---@ename

=

Andrrew Clai

'---@ename

=

Djaect-empr---pname=Applliances

D source
dept [1 *]
$
D..dname
0 value: String

Dj ect-emp---@pname =Rob

otics
Jim Bellisl

Dj ect-emp---@pname= Rob

---Sename = Mark Tane

Dj ect-emp---@pname
'---@ename

=Brand promot
= Richard DawE

'---@ename

=

'---@ename

=

project [1.*]

$p.pname.va

0 @name: String

Proj [0.*] lTh Pro [O *] * / t W ~~~~~em ployee [O.......*]

r* 0 @pidx,nt

r
=Appliances
ject-emp---@pname

Dj ect-emp---@pname

target
Dtre

Group-by

"2

I]pname

Mark Tane

o value: String
j regEmp [0 *]
| @pid:int

=Brand promot
Steven Aikir

To better illustrate the roles of these constructs, let us briefly
consider two variants of the mapping in Figure 6. If we omit
the join condition, then a full Cartesian product is computed
and each Proj is associated with all regEmps within their dept.
If we also omit the top-level build node, then Clip computes

$p2

0

$p2@pid

$r.@pid

@name:String

/

ename

o value: String
-Li sal
o value: int

Fig. 7. A mapping with grouping and join

33

D source

the inner builder, so that regEmps are chosen by comparing
their @pids attributes with those of the Projs in one group
only. Thus, the generated target will contain as many project
elements as there are distinct values of project names in the
source instance. Further, the list of employee elements under
project will contain the employees that work in that project
(independently of the department these employees report to).
This construction is necessary because projects are repeated
within departments. The result is as follows:
= Appl
L~mployee ---E
L~mployee ---E
L~mployee ---E

Xname

tar

* dept [1..*]

D dname
0 value:String
W Proj [0.*]
.
@pid: in
D-] pname
0 value:String
W regEmp [O..*]

Fig. 8. Inverting the nesting hierarchy

ian

name
name
name

mdrew Clai
lark Tane

D

Robc
L~mployee ---E name = Mark Tai
L~mployee ---E name = JimBelr
Xname = Brand Prom
zard Daw
mployee---@name =
Xname

=

mployee---@name

-departm
-departm

L

nam

=

2

=

'---@avg-sa1

=

3
20000

@ pid: int
ename
o value: String
sal
o value: int

b)

In a), a single element is safely connected to a repeating
element. This mapping produces a target singleton set, a valid
target instance that "fits" into the target element cardinality.
In b), the result of a Cartesian product is connected to a nonrepeating element. Since the one of the input is a repeating
element, unless we explicitly aggregate the Cartesian product
results into a single value, no valid target instance can be
generated from this (unsafe) mapping. Safe builders guarantee
that every instance bound by the builder can be accommodated
in the target.
A CPT is valid if it is a composition of safe builders,
forming a tree which is topologically aligned with the structure
of the target schema, i.e. the hierarchy of the build nodes
reflects that of the nodes in the target schema reached by
the outgoing builders. The examples below illustrate legal and
illegal configurations.

=

numProj

|--- @numEmps

III.

a)

=r
name

@name

regEmp [0..*]

A. Valid Object Mappings
We say that a builder is safe if it goes from more constraining to less constraining schema elements, in terms of
the cardinality of those elements. For example, consider the
following two builders:

@numProj
@numEmps
@avg-sal
---

<<count»
@pid: int
pname
O value: String
<<a vg>>

* @ num Proj: int
* @ num Emps: int
*.@avg-sal: int

that valid mappings require safe builders; i.e., builders that
allow all source data to be somehow copied to the target side.
However, in certain cases, users might need to enter unsafe
builders into a mapping. Clip marks these mappings as invalid,
but does not restrict the user from entering them.

Last, the mapping in Figure 9 demonstrates aggregate
functions by computing for each dept the number of projects
and employees and the average salary. Notice that the value
mappings tagged ((count)) can start from multiple elements,
too, making an exception to the syntactic rule. The result is
as follows:

'---departm

<coun_t

[ target
j department [1..*]
* @ name: String

Fig. 9. A mapping with aggregates

-@rname = Robotics
-deepartment---@name
name = Brand prom

arget---departm

.
I

nam
nam

-departm

- dept [1..*]
D dname
L
0 value: String
[ Proj [0..*]

W

ia

-name

source

r- .

=

Note that values of pnames are legally mapped to the attribute @name of a single project because pname is a grouping
attribute and is therefore univocally determined. Non-grouping
values have multiple and a-priori different values, and cannot
be mapped to the output elements, unless condensed into one
value by aggregate functions (as exemplified in the following).
Other examples. The mapping in Figure 8 still maps all
Proj elements with the same name into a single project. The
departments involved in each project appear nested under each
target project element. Note that the "innermost" build node
takes depts from a higher nesting level in the source with
respect to Projs, so as to invert the hierarchy. The result is
as follows:
_aY

F target
poet[
Group-by
1$p.pname.valueh~- project [1..*]
0 @name: String
W department [0..*]
name: String
*

VALIDITY OF MAPPINGS

Not all combinations of value mappings and builders produce valid target instances (i.e. instances that conform to the
target schema). We say that a mapping is valid if, given any
instance of the source schema, the mapping produces a valid
instance of the target schema. In this section we discuss several
syntactic rules that Clip uses to detect valid mappings. We note

34

A

a) Linear Valid mapping: the
CPT is aligned with both the
source and target schemas

G

&.w

..

B att3

A

Iq

D

att4
E

b) Valid: the atomic attribute att3 is a
direct descendant of B. Since att5
does not directly descend from D, an
E element will be produced, too.
att5

CUc

c) Valid: The builder uses B as its
input node. A is in the path to the
root from B, andA is also in the path

I

attl

qB2att2

-4Batt3

INVALID
Inverted
c)
mapping: the CPT is not
aligned with the target
FU schema.

&Z-1

c

|

F

Bv

b) Inverted Valid mapping:
the CPT is aligned with the
target, but not with the source
schema.

L-

attl
att2

FD

att4
X]E

from atti.

att5

d) INVALID: the nearest ancestor
of att3 is B, which is not mapped
by any builder.
'att5

The consistency rules above do not apply for value mappings with aggregate functions. Aggregate functions produce
a single result out of a set of values with any cardinality. So,
the driver of an aggregate value mapping is always valid. If a
builder is defined above, it fixes the context of the mapping,
restricting the number of values considered by the aggregate
node at each iteration; if no builder is specified, the whole
document is the scope of aggregation.

B. Valid Value Mappings

Source data instances are converted into target data instances using the value mappings connecting leaf nodes (attributes or text values) of the schemas. This data conversion
occurs in the evaluation context prescribed by the CPT; i.e.,
each value mapping is driven by a set of build nodes. For
each value mapping, we can identify its driver by finding the
builder node that encompasses the value mappings source and
target elements.
More formally, let vi be a value mapping between a set of
source schema nodes source(vi) and a target schema node
target(vi). Given a schema node e, we define path(e) as the
unique set of schema nodes visited if we walk up the schema
hierarchy, starting at e and ending at the root node. Starting
from target(vi), we search upward in path(target(vi)) and
stop at the first target node ti that is the target side of a
builder. We call this builder Bi = (Si, ti), where Si is the
set of source schema nodes used by Bi, the driver of vi. In
other words, we find a builder (Si, ti) such that ,/]tk with
a builder (Sk, tk) in the CPT and path(ti) c path(tk). The
following diagram illustrates these relations using an example.
Here, Bi = {A}, C) is the driver of vi.
Sj.---

*..
0

Bj

attl
att2

IV. LANGUAGE SEMANTICS

This section introduces extensions for handling the new
Clip's features (builders and aggregates) to the internal languages that we developed in previous works on schema
mapping generation [1][2].
A. Mapping language
We define the semantics of mappings by means of a querylike notation; in the notation, expressions and terms are defined
by e ::= S x e.l and t ::= e F[e]; where S is a schema
root, x is a variable, I is a label, e.l is a record projection, and
F is a function symbol. An explicit mapping is represented by
a (nested) tgd (tuple generated dependency) in the following
form:
M ::= Vx1 c gl,.. :Sn, Egn |C1
(C2 A Ml A
A Mn)
Yi C g, ,YnCg
where Ml, . . ., Mn are submappings of M. Each submapping
is itself a mapping; M is an ancestor of Ml... Mn and
(recursively) of the submappings of Ml, . . ., Mn. Each xi C gi
(yi e git') is a source (target) generator. In a source (target)
generator, the head of an expression gi must either be a source
(target) schema root, or a variable defined in a source (target)
generator of M (in which case it must be some xj with j < i),
or in a source (target) generator of an ancestor of M. A source
(target) expression is an expression over a variable defined
in a source (target) generator of M or one of its ancestors.
The expression C1 consists of a conjunction of comparisons
of type a, oper a2, where a, is a source expression, a2
is either a source expression or a constant and oper is an
operator for equality, inequality or membership (in this case,
a2 cannot be a constant). The expression C2 has three kinds of
formulas. First, it has target conditions: comparisons between
target expressions of atomic type and constants. Second, it

....t

'~~ . ,4k. C

= > ..........

t...arget(vi)
att4

> att5

att3

A value mapping vi is valid if (i) we can find a builder
Bi as described above, and (ii) for all source nodes sv e
source(vi), we can find a source node Sb C Si such that
path(sv) \ path(sb) does not contain any repeating source

elements. In other words the invalid value mapping uses at
least one source-side node that is inside a repeating element
that is not bounded by a builder and, thus, Clip does not know
how to iterate over that set. The following examples illustrate
these concepts:
C

AA

Oattl
att2

Batt3

a) Valid: the atomic attributes
involved are direct descendants of

D

att4
X]E

the nodes involved in the builder.

att5

35

of values for that target set for every different combination
of input parameters to the Skolem function. We represent
our Skolem functions as taking two parameters: the grouping
context and the grouping attributes. The grouping context
is a list of nodes that restrict the scope of the grouping
attributes (i.e. a list of target variables already bound in some
outer levels). The grouping attributes are the dimensions along
which groups are formed. The tgd for Figure 7 is:

has source-to-target conditions: equalities between source and
target expressions of atomic type. Finally, it has equalities with
functions: equalities of the form e =F[e,... em] where
e is a target expression of set type, e-... em are source
expressions, and F is a function.

B. Semantics of builders and build nodes
We now discuss the tgds representing the mappings of
Section II.A. Section V will address the algorithmic generation
of these tgds.
Simple mapping. The mapping of Figure 3 translates to the
following simple tgd:
V d C source.dept, r C d.regEmp r.sal.value > 11000
3 d' C target.department, e' C d'.employee
e'..name = rename.value

Note that the above tgd does not capture the minimumcardinality semantics, which requires the construction of only
one target department for each source department. As already
mentioned, the universal solution would construct a department for each employee; we, nevertheless, enforce minimum
cardinality in the generated XQuery, not in the tgd expressions.
We discuss our generation of XQuery scripts in Section VI.
Context propagation. The mapping depicted in Figure 4
contains a context arc that constrains the scope of the inner
mapping within the context of the outer one. This is expressed
in our nested tgds with a sub-mapping (in square brackets) as
follows:
V d C source.dept - 3 d' C target.department,
[V r C d.regEmp r.sal.value> 11000 3 e' C d'.employee e'.@name = rename.value]
The submapping is correlated to the outer mapping by references to the variables d and d'.
Context propagation tree. The tgd corresponding to the
mapping in Figure 5 has two nested mappings, one for projects
and one for employees. Both submappings refer to the same
variables d and d' which are bound by the outer mapping:
V d C source.dept -) 3 d' C target.department,
[Vp C d.Proj -) 3p' C d'.project
p'.@pid p.@Qpid,p'.@Qname = p.pname.value],
[V r C d.regEmp r.sal.value>l I1000 e' C d'.employee
e'.@name r.ename.value]
Cartesian product and join. The tgd for Figure 6 is:

3group-by
(Vd C source.dept,p C d.Proj
3 p'C target .project
p' group-by(I, [p.pname.value]),
p'.@name= p.pname.value,
[Vp2 C p, d2 C source.dept, r C d2.regEmp
p2.@Qpid = r.@pid
3e' C p'.employee e'.@name = rename.value])

The first argument of group-by is I , as Proj elements are
unrestrictedly chosen from the whole data set.
Inverting the hierarchy. The tgd for Figure 8 is:
3group-by

(Vd C source.dept,p C d.Proj

3 p'C target .project
p' group-by(I, p.pname.value),
p'.@name= p.pname.value,
[Vd2 C source.dept,p C d2.Proj 3 d' C p'departmentt d'.@name d2.ename.value])

Again, the first parameter of group-by is I; indeed, in the
examples the grouping node is the root of the CPT. Note that
we need a second variable d2 and a condition (p C d2.Proj)
in order to compute the inversion.
Aggregates. We introduce one function for each type of
aggregate and specify the attribute to aggregate as an expression rooted in the variable which restricts the context of
aggregation. The tgd for Figure 9 is:
3 count, avg
(V d C source.dept - 3d' C target.department

d'.@name = d.dname.value,
d'.@numProj = count(d.Proj),
d'.@QnumEmps = count(d.regEmp),
d'. Qavg-sal = avg(d.regEmp.sal.value))

This example shows how we formalize aggregates independently of grouping, so as to emphasize that they are independent extensions. More specifically, a context of aggregation
V d C source.dept -has to be always specified because hierarchical data structures
[Vp C d.Proj, r C d.regEmp p.@Qpid = r.@Qpid
naturally provide several "structural" aggregation levels, which
3p'/C target.project-emp
p'.@Qpname= p.pname.value,
are "wired" within their nested topology. In Clip, the level of
p'/.@ename = rename.value]
aggregation can be fixed by grouping, but also by ordinary
Notice that the outer mapping only restricts the context of builders. Referring to the example, not all the projects are
the inner one (no element is generated at the outer level); counted, but only those within a given department, as count
also, two variables in the inner mapping span over the full set is attached to a value mapping whose driver originates from
of Proj and regEmp (under the current dept) to find pairs with dept.
corresponding identifiers.
V. MAPPING GENERATION
Grouping. To support grouping, we introduce a special
We now describe how to semi-automatically generate Clip
Skolem function into the mappings. This function is assigned
a target-side set variable, implying we will generate a group mappings by extending techniques used in Clio. Clio generates

36

tgds using only the schemas and value mappings (mappings
between atomic elements) as input. In effects, Clio deduces the
builders and the context propagation trees that encompasses
the given value mappings. However, as we have mentioned
before, Clio cannot automatically create some of the mappings
we manually enter in Clip. In this section we discuss how we
extended Clio's mapping generation algorithm to cover Clip's
mappings.

Z ROOT

fA[O..]

0 value: String
B [O..]

O value: String
' C [O."]
O value: String
D[O ]
o vaiue: String

EZROOT
. . ].S [O..*]
@attl: String
L. . I G [0. *]

* @aft2: String
*.. @aat3: String

2E [O.."]

O value: String

A. Mapping generation in Clio
Clio's mapping generation algorithm was first described
in [1]. Given a source and a target schema, Clio identifies
source and target tableaux. A tableau is a set of schema
elements (or attributes) that are semantically related; elements
are related, for instance, when they are siblings under the
same repeating element (the columns of a table in a relational
schema), or when the containing repeating elements are related
via a parent-child relationship. Intra-schema constraints (e.g.,
key/foreign keys) are also used to define tableaux by chasing
existing tableaux over the constraints.
Consider for example the source schema in Figure 4. Clio
detects three tableaux in that schema: one for the dept set,
another for the dept with Proj set, and a larger one that involves
all sets in the schema and is computed by chasing over the
@pid foreign key. We represent each one of these tableaux
using this shorthand notation: {dept}, {dept-Proj}, and {dept-

Fig. 10. A generic mapping with its tableaux and dependency graph

dependency graph. Assume that only the value mappings from
B and D are given as input (i.e., the user did not enter the
value mapping from A). Clio activates two mapping skeletons:
AB -> FG and AD -> FG. But since the more general
skeleton A -> F is not active, Clio's current algorithm cannot
nest the two active mappings. Further, notice that what we
want is to compute a Cartesian product between B and D
using A as a context. To get Clio to produce something close
to this, we first need to add an ABD source tableau (shown as
A(B x D) in Figure 10). With this tableau in place, Clio emits
another mapping: ABD -> FG. But the query generated by
this mapping pairs all B values with all D values regardless
of A.
Our extension to Clio's nesting algorithm resolves these
limitations. Our extension works as follows. First, the nested
mappings are computed as usual. Then, we identify all "root"
nested mappings (active mappings that are not nested under
other mappings). In the case of our example, all mappings
are root nested mappings. We then walk up the hierarchy of
mapping skeletons, starting from the nested mapping roots,
looking for more general mappings that intersect our paths. In
the case of our example, this common mapping is A -> F. We
mark this mapping as active (regardless of whether it contains
value mappings or not) and recompute the nested mappings
using the new root mappings.
Consider our first example in which the only active mappings are AB -> FG and AD -> FG. Our extended algorithm
detects A -> F as a new root nested mapping and nests AB ->
FG and AD -> FG inside this new root mapping. Nesting
mappings has the effect of removing redundant source and
target variables, resulting in the following nested expression:

Proj-regEmp, @pid=@pid}.

After the source and target tableaux are computed, Clio
creates a matrix source vs. target tableaux. Each entry in this
matrix relates a source with a target tableaux and is called
a mapping skeleton. For each value mapping entered by the
user, Clio matches the source and target end-points of the
value mappings against all the mapping skeletons and mark
as active those skeletons encompassing some value mappings.
Each active skeleton that is not implied or subsumed by others,
emits a logical mapping. For example, for the simple mapping
in Figure 4, there are 3 source tableaux (mentioned above) and
2 target tableaux ({department}, {department-project}). This creates 6 mapping skeletons. The entered value correspondence
will only match the {dept-Proj-regEmp, @pid=@pid} source
tableau and the {department} target tableau. From this skeleton,
Clio emits the following tgd expression:
V d C source.dept, p C d.Proj, r C d.regEmp p.pid = r.@Qpid
3 d' C target. department , e' d'.employee
e'.@name = rename
Nested Mappings: A further refinement to this mapping
generation algorithm was presented in [2]. Logical mappings
generated by Clio are possibly related in that they share part
of the source and target expressions. In those cases, mappings
can be "nested" inside others, reducing the overall number of

Va C A -3f C F
[Vb C a.B ) 3g C f.G g.@Qatt2 b.value],
[Vd Ca.D - 3g f.G g.@Qatt3 d.value]
In the case of our second example, after the ABD source
skeleton is added, Clio outputs ABD -> FG as an active
mapping. It turns out that ABD -> FG is a sub-mapping of
A -> F (it is not a sub-mapping of AB -> FG or AD -> FG
because the target side of the mappings is the same.) Our
extension will again detect that the more general A -> F

mapping expressions.

B. Mapping generation in Clip
To illustrate the problem with Clio's mapping generation
consider the mapping in Figure 10. Here Clio would compute
the 5 source tableaux and the 2 target tableaux shown in the

37

can nest ABD -> FG inside. The resulting nested mapping
captures the Cartesian product with respect to the A values
that we wanted:
Va C A -3f c F
[Vb C a.B,d C a.D - 3g c f.G g.@att2 = b.value,
g.@catt3= d.value]

thing not explicitly captured in the tgd expressions. We do this
by translating all those elements of the target schema which
are not existentially quantified into constant tags and placing
such tags in a specific position within the nested FLWOR
expressions. More specifically, such constant tags are placed to
wrap the FLWOR expression F generated in correspondence
with the specific submapping, instead of being placed inside
the return clause of F; thus, only one element for the whole
clause is generated, instead of one for each iteration. In other
words, our translation principle is that all the for clauses in
the generated FLWOR expressions are pushed as down as
possible in the query structure, whenever their nesting level
is not enforced by explicit quantification.

There are two important observations regarding the relation
of Clip mappings and Clio mappings. First, Clip's build nodes
correspond to Clio's mapping skeletons. For each build node,
we look at all its source side builders and match them against
the computed source tableaux. If a build node appears in a
context propagation tree, we collect all source-side builder arcs
and match all of them to a source tableau. If no source tableau
is found, we create a new tableau that will cover our source
builders and add it to Clio's list. We do the same for the target
side of each builder. At the end of this process, we will have
identified a source and a target tableau that form the context
of our build node. By definition, this tableaux pair is the Clio
mapping skeleton that matches the build node.
The second observation is that the context propagation tree
tells us how to nest the mappings at each build node. In
other words, a CPT is a nested mapping. Clip users can
rely on our extended nested mapping generation algorithm to
automatically compute CPTs for the input value mappings or,
users can explicitly enter build nodes and context lines and
our algorithm will make sure that the mappings are nested
according to the given CPT.
VI. TRANSLATION OF MAPPINGS INTO XQUERY
The transformation of data instances from the source to the
target schema is done by a program generated from the tgds.
Since Clip is designed for XML Schema mappings, XQuery
is a natural candidate as our transformation language. The
algorithm that translates tgds into XQuery is an extension of
the code generation algorithms developed for Clio's nested
mappings [2]. Here we sketch the general ideas behind the
algorithm and concentrate only on the novel and more interesting generation of queries involving grouping and aggregate
functions.
The query generation algorithm takes as input a nested
mapping M as defined in Section IV-A and produces an
XQuery FLWOR expression F as output. Each sub-mapping
of M translates into one (nested) FLWOR expression of F. F
has the following structure: a for clause captures the iteration
implied by every universally quantified variable of M; a where
clause captures the join and filtering predicates in C1; a
return clause constructs the XML items for the target schema
elements mentioned in the existentially quantified part of the
mapping; elements bound to some of the variables defined in
the for clauses are copied in the proper position according to
the value mappings expressed by the conditions in C2. The
sub-mappings of M recursively replicate this structure, so that
the scope of quantified variables in the tgds directly translates
to that of variables in the nested FLWOR expressions.
It is worth noting that the generated XQuery expressions
take into account our minimum-cardinality assumption, some-

21

k

m

x

m

_a
III

Group-by

{grouping-attl,

...

xgrouping-attn}

{ filtering conditions over 1,. x

XQuery Grouping expressions. Consider the above generic
grouping build node. The corresponding tgd is the following:
3 group-by
(VlI ..L,. ..
...X I{filtering conditions} 3 t C ... T t = group-by([a, . I k][grouping-attrs]),....

Generating an XQuery expression from this kind of tgd
expression is a hard task, because the current XQuery standard does not include a grouping statement or clause. We
implement the grouping semantics using the existing clauses
and functions as follows. First, the for and where clauses are
generated in the same manner as with regular build nodes.
Then, for each grouping attribute, we create a let clause that
computes the distinct values of that attribute in the input data.
We then add a for clause that loops over every such distinct
value of the grouping attributes. Each iteration over these loops
defines the current unique value that is the key value of the
group. We then output the desired target elements using the
contents of the current group. Any submapping receives the
current group as its context. The above generic tgd translates
to the following XQuery template:
let $context :=
(for $m in ... /M, ..., $x in ... X
where (: filtering conditions :)
return <element> {$m} ... {$x} </element>),
$dim_1
distinct-values($context/ ... @attrl),

$dim_n
distinct-values($context/ ... @attrn)
return
for $dl_val in $dim_1, ..., $dn_val in $dim_n
let $group := (for $x in $context
where $x/ ... @attrl = $dl_val
and ...
and $x/ ... @attrm = $dm_val
return $x
return
(: target element, value mappings, submappings :)

Consider, for example, the mapping in Figure 7, where
38

projects

are

grouped together by their

name.

TABLE I
FLEXIBILITY OF CLIP

The tgd is:

3 groupby
(Vd C source.dept,p C d.Proj -) 3p'C target.project
p'
groupby(I, p.pname.value),
p'.@name= p.pname.value)

Example
Value mappings
(Source)
Figure 1 in [2]
7
Figure 3 in [2]
4
Figure I in [1]
3
Figure 1 (this paper) _ _2_ _

This translates to the following XQuery that builds a
<project> element for each different project name:
<target> {
let $context
(for $p in source/dept/Proj
return <element> {$p} </element>),
$pname_vals := distinct-values(

mappings. Instead, Table I shows a lower-bound of how many
different meaningful mappings we could draw using Clip
starting from the same value mappings. This exercise was done
using 3 published examples of Clio mappings plus one of the
mappings we used in this paper. The first column shows the
source of the mapping example, the second column shows how
many value mappings are involved in this example, and the
third column shows how many more different nested mappings
we could create when compared to nested mappings generated
by Clio.
In our future work, the GUI will be augmented by including
schema matching tools, i.e. tools suggesting related elements
and structures within two complex source and target XML
schemes, and by adding filters highlighting some of the
lines and of the source and target structures, providing a
clear rendering of the lines in the middle [11]; these view
mechanisms allow users to concentrate on a portion of the
schemas at a time. These additions will help users work with
large schemas, as otherwise they could be overwhelmed by
schema complexity and by the number of lines from source
more

$context/Proj/pname/text())

for $pname_val in $pname_vals
let $group := (for $p in $context/Proj
where $p/pname/text()=$pname_val
return $p )
return <project name = {$pname_val} />

} </target>

As for aggregates, the availability of native XQuery aggregate functions facilitates the translation with respect to the case
of grouping. The tgd for the mapping in Figure 9 (limited to
the first two value mappings) is:
3 count
(V dC source.dept 3d' C target.department
d'.@name = d.dname.value,
d'.@rnumProj = count(d.Proj))

This translates to the following XQuery statement:
<target> {
for $d in source/dept
return <department

to target.

{$d/dname/text()}
numProj
{count($d/Proj)} />
name

} </target>

Extra meaningful
mappings with Clip
4
1
1
4

ACKNOWLEDGEMENTS
Paolo Papotti was partially supported by an IBM Faculty Award
grant and Alessandro Raffio was partially supported within a joint
study agreement between IBM and Politecnico di Milano.s

Notice that the information about the aggregation context
carried by the count function in the tgd is used to determine the
starting point of the path expression argument of the XQuery
count function (namely, variable $d).

REFERENCES
[1] L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernandez, and R. Fagin,
"Translating Web Data," in VLDB, 2002, pp. 598-609.
[2] A. Fuxman, M. A. Hernandez, H. Ho, R. J. Miller, P. Papotti, and
L. Popa, "Nested Mappings: Schema Mapping Reloaded," in VLDB,
2006, pp. 67-78.
[3] s. Melnik, P. A. Bernstein, A. Halevy, and E. Rahm, "Applying Model
Management to Executable Mappings," in SIGMOD, 2005, pp. 167-178.
[4] A. Bonifati, E. Q. Chang, T. Ho, V. S. Lakshmanan, and R. Pottinger,
"HePToX: Marrying XML and Heterogeneity in Your P2P Databases,"
in VLDB, 2005, pp. 1267-1270.
[5] P. A. Bernstein and S. Melnik, "Model management 2.0: manipulating
richer mappings," in SIGMOD, 2007, pp. 1-12.
[6] M. Lenzerini, "Data Integration: A Theoretical Perspective," in PODS,
2002, pp. 233-246.
[7] R. Fagin, P. G. Kolaitis, R. J. Miller, and L. Popa, "Data Exchange:
Semantics and Query Answering," in ICDT, 2003, pp. 207-224.
[8] R. Fagin, P. Kolaitis, L. Popa, and W.-C. Tan, "Composing Schema
Mappings: Second-Order Dependencies to the Rescue," in PODS, 2004,

VII. CONCLUSION
Clip's current implementation includes two components: a
GUI for mapping expression and a translator, which produces
tgds corresponding to the mapping. The translation of tgds
into XQuery is currently ongoing, but this task was already
performed by some of the authors in the context of Clio and of
its nested version, and it will not introduce significant technical
challenges. The GUI interface has been designed by reusing
our experience gained in XQBE [10] and Clio [1], by aiming
at the best balance between ease of use, expressive power, and
effectiveness.
The main "performance" metric for Clip is the number of
legal Clip mappings that can be generated for a given set of
value mappings. In particular, we can compare the "flexibility"
of the mapping interface of Clio and Clip. By flexibility we
mean how many different (meaningful) mappings the tool
allows us to visually construct. Again, the "meaningfulness"
of mappings depends a lot on the particular data integration
scenario and it is difficult to enumerate all such possible

pp.

83-94.

[9] C. Yu and L. Popa, "Semantic Adaptation of Schema Mappings when
Schemas Evolve," in VLDB, 2005, pp. 1006-1017.
[10] D. Braga, A. Campi, S. Ceri, and A. Raffio, "XQBE: a visual environment for learning XML query languages," in SIGMOD, 2005, pp.
903-905.

[11] G. G. Robertson, M. P. Czerwinski, and J. E. Churchill, "Visualization
of Mappings Between Schemas," in SIGCHI, 2005.

39

Flint: Google-Basing the Web
Lorenzo Blanco, Valter Crescenzi, Paolo Merialdo, Paolo Papotti
Roma Tre University - Italy
[blanco,crescenz,merialdo,papotti]@dia.uniroma3.it

ABSTRACT

by effective techniques for discovering, crawling and indexing the
collections of pages containing information of interest.
This paper presents F LINT, a system to support users in the tasks
of discovering, annotating and indexing data-rich pages publishing
information of interest. The system is domain independent, and the
only required input is a small set of sample pages, each one containing data about one instance of an entity of interest. The system automatically infers an intensional summary of the target entity from
the input pages, and searches the Web for pages publishing data
representing instances of such an entity. The retrieved pages are
then indexed and semantically annotated. Also, based on manual
annotations that can be performed with a minimal manual effort by
the user, the values of relevant attributes of the underlying conceptual entity are extracted and stored in a suitable database. The information contained in the indexed pages can thus be searched with
the traditional IR approach, or by database style queries against the
extracted data.
To give an example consider the three Web pages in Figure 1: the
data published in each of them describe one instance of the H OCK EY P LAYER conceptual entity. F LINT automatically searches the
Web for pages publishing data that represent instances of the same
H OCKEY P LAYER entity. The retrieved pages are then indexed and
annotated as instances of the H OCKEY P LAYER conceptual entity.
Also, the user can mark (by means of a suitable GUI) on the
sample pages values of attributes of the target entity. Based on this
information, the system infers a set of rules for extracting the values
corresponding to the same attributes from all the retrieved pages.
Again from our example, suppose the user labels on the sample
pages the values of attributes such as Position, Birth date,
Height and Weight. The system infers rules for extracting the
values of these attribute from the retrieved pages (clearly from the
pages that contain them).
As a proof of concept, we have implemented the system leveraging on some facilities that Google have recently launched: Google
Co-op1 and Google Base.2

Several Web sites deliver a large number of pages, each publishing
data about one instance of some real world entity, such as an athlete,
a stock quote, a book. Even though it is easy for a human reader
to recognize these instances, current search engines are unaware
of them. Technologies for the Semantic Web aim at achieving this
goal; however, so far they have been of little help in this respect, as
semantic publishing is very limited.
We have developed a system, called F LINT, for automatically
searching, collecting and indexing Web pages that publish data representing an instance of a certain conceptual entity. F LINT takes as
input a small set of labeled sample pages: it automatically infers a
description of the underlying conceptual entity and then searches
the Web for other pages containing data representing the same entity. F LINT automatically extracts data from the collected pages and
stores them into a semi-structured self-describing database, such as
Google Base. Also, the collected pages can be used to populate a
custom search engine; to this end we rely on the facilities provided
by Google Co-op.

1.

INTRODUCTION

The proliferation of tools and facilities for publishing data on the
Web is rapidly transforming a large portion of the Web in a huge
collection of data-rich pages. Usually, the data delivered by these
pages are exposed according to an implicit schema, and represent
instances of some real world entity. While the richness of data contained in this increasing portion of the Web represents a promising
opportunity, there is a lack of tools and techniques that support the
consumption of the information it offers.
Wrapping techniques have been recently developed for extracting and annotating data from semi-structured Web pages (e.g. [2,
13, 17, 22], see [10] for a recent survey on the topic). These techniques take as input a set of pages that share a common template,
infer the implicit schema according to which data are organized in
the pages, and generate a wrapper that can be used to extract the
data from any page that share the same template of the input set.
However, the adoption of these techniques at a large scale is not feasible mainly because so far the wrapping approach is not supported

• Google Co-op allows users to build a custom search engine
over a domain of interest. With Google Co-op a user first
specifies a set of labels (Facet in the Google terminology)
each one representing a concept from the domain of interest,
and then associates each label with a set of pages that fit the
corresponding concept. Labels can then be explicitly used
in the search process to restrict the results over a specific
concept. In our context, we populate a Google Co-op search
engine by associating the set of pages retrieved by F LINT
with a suitable label (typically the name of the conceptual
entity).

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
EDBT’08, March 25–30, 2008, Nantes, France.
Copyright 2008 ACM 978-1-59593-926-5/08/0003 ...$5.00.

1
2

720

http://www.google.com/coop
http://base.google.com

Figure 1: Three Web pages representing instances of the H OCKEY P LAYER entity
• Google Base can be seen as a self-describing, semi-structured
database where users can upload their structured data. Google
Base adopts a very simple data model: users describe their
data using an item type and attribute/value pairs, without
any restriction on names and values. In our context, Google
Base is used to store the values extracted from the retrieved
pages by F LINT. Each page gives rise to an item, of the same
type of the corresponding entity, and the set of attribute/value
pairs associated with each item corresponds to the data extracted from that page.

interaction between I NDESIT and O UTDESIT is recursively
run, as long as new pages are found.
• Finally, the DATA E XTRACTOR module infers wrappers to
extract annotated values from the collected pages.
As discussed in the previous Section, the extracted data and the
retrieved pages are loaded into Google Base and Google Co-op,
whose querying facilities can be used for searching the results computed by F LINT.
In the remainder of this section we provide some more details
about the main components of the F LINT architecture.

The search and querying facilities offered by Google Co-op and
Google Base can thus be used to search and query the results of
F LINT.
The rest of the paper is organized as follows. Section 2 provides a brief overview of our method for searching entities by sample; Section 3 discusses related work and Section 4 illustrates the
demonstration scenario.

2.

2.1

I NDESIT
I NDESIT is responsible of seeking the Web site of a given seed
page with the goal of collecting pages offering the same intensional
information as the sample.
I NDESIT relies on the observation that, within a large Web site,
pages offering the same intensional information usually share a
common template and common access paths. For example, consider the Web sites of the three sample pages shown in Figure 1: it
is likely that in each of these Web sites, pages describing a hockey
player have the same template as the corresponding sample page,
and that the access paths to these pages are organized according to
a common pattern.
Based on these ideas I NDESIT implements a crawling algorithm
that scans a given Web site toward pages sharing the same structure of an input seed page [6]. The crawler navigates the Web site
searching for pages that contain lists of links leading to pages which
are structurally similar to the seed page. These lists of links work
like indexes to the searched pages. Therefore, I NDESIT follows the
links offered by these lists to collect the target set.
With respect to our running example, the output of I NDESIT is
the set of hockey player pages published in the Web sites of each
sample page. In our perspective, each of these pages embeds data
representing an instance of the H OCKEY P LAYER entity.

SYSTEM OVERVIEW

Figure 2 shows the main architectural components of F LINT. We
now briefly comment the role of each component in the task of
retrieving and annotating pages representing instances of the same
conceptual entity as the input sample page. F LINT targets its effort
against that vast portion of the Web which is composed by large,
fairly structured Web sites, exploiting the regularities they exhibit.
F LINT works in four stages, as follows.
• First, it scans the Web sites of the sample pages in order to
discover other pages representing instances of the same entity; this task is achieved by I NDESIT [6].
• From the pages obtained in this initial stage, the E NTITYA N ALYZER module automatically extracts a description of the
entity exemplified by the sample pages.
• Then, based on the entity description and on the pages collected in the first stage, the O UTDESIT module launches searches
on the Web to discover new pages containing data that represent instances of the target entity. The pages found in this
step are iteratively passed as input seeds to I NDESIT, and this

721

2.2

E NTITYA NALYZER
Once I NDESIT has collected a number of pages, the E NTITYA N ALYZER module computes a description of the conceptual entity
for which the sample pages represent instances.

DataExtractor

FLINT

Indesit

Labeled
sample
pages

Outdesit

EntityAnalyzer

Google
API

Search
Engine

Figure 2: F LINT architecture
The description of a conceptual entity is composed by (i) an
intensional description, and (ii) a keyword. The intensional description is expressed by means of a set of terms, each representing
the name of an elementary feature, an attribute, of the entity (e.g.,
position, age, best score). The keyword is a term, extracted from
the Web sites of the sample pages, that characterizes the overall
conceptual domain of the entity (e.g., hockey).
Our method for extracting the entity description is based on the
assumptions that different instances of the same conceptual entity are likely to share a common set of attributes, and that—as
Web pages are produced for human consumption—several attribute
names are explicitly published in the templates of the sample pages.
It is worth saying that this phenomenon has been observed also by
Madhavan et al. in their studies on Web scale data integration [19].
For each sample page, the E NTITYA NALYZER computes the set
of terms that belong to the corresponding template. This task is
performed by analyzing the set of terms that occur in a bunch of
structurally similar pages returned by I NDESIT, and removing those
elements that belong also to the “site template”, i.e. to that portion
of the template that is shared by every page in the site. In this way,
from each sample page a set of terms is extracted: their intersection
is used as intensional description of the entity.
The entity description is completed by a keyword which is generated by analyzing, with standard term weighting techniques, the
words that appear in a number of pages belonging to the Web sites
of the input samples.

until new pages are found.
To correctly expand the search on the Web, F LINT needs to address several issues. First, it has to feed the search engine with keywords that are likely to produce new pages representing instances
of the input entity. Second, as these pages will be used to run a new
instance of I NDESIT, it has to filter them in order to choose those
that really correspond to instances of the input entity.
The keywords to be submitted to the search engine are generated
by means of a simple yet effective technique. As we are searching
for instances of a given entity, we need values that work as identifiers for the instances of the entity. We observe that, since pages
are designed for human consumption, the anchors associated with
the links to our instance pages usually satisfy this properties: they
are expressive, and they univocally identify the instance described
in the target page. In our running example, the anchor to a player
page usually corresponds to the name of the athlete.
Therefore, F LINT issues a number of queries against a search engine, where each query is composed by the anchor of a link to one
of the pages retrieved by the previous I NDESIT execution. To focus
the search engine toward the right domain, each query is completed
with the keyword associated to the entity description: by narrowing
the search we avoid false results due to homonyms.
Each search produces a number of results; however, only a fraction of the returned pages are suitable for our purposes. A crucial
issue is how to drop out pages that do not represent instances of
the target entity. The inclusion of false positives in this step would
compromise the whole process, as any error would be propagated
in the successive steps. This problem can be avoided by checking
its template: only pages whose template contains terms that match
with the intensional description of the entity are taken into account.
Each selected page is then given as input to I NDESIT, which collects again structurally similar pages from each site. The new anchors found by I NDESIT are then used by O UTDESIT to perform
new searches on the Web.
The pages retrieved by O UTDESIT are used to populate a custom

2.3

O UTDESIT
The results produced by the initial I NDESIT execution, together
with the inferred entity description, are used to propagate the search
on the Web. This step is performed by O UTDESIT, which issues a
set of queries against a search engine and elaborates the results in
order to select only those pages that can be considered as instances
of the target entity. Finally, the selected pages are used as seeds to
trigger again an I NDESIT scan, and the whole process is repeated

722

search engine. As discussed above, our prototype relies on Google
Co-op: the user creates a facet with the name of the conceptual
entity and associates to it all the pages retrieved by O UTDESIT.
According to the search facilities offered by Google Co-op the label
can be used to restrict or to refine searches over the indexed pages.

2.4

DATA E XTRACTOR
The DATA E XTRACTOR module performs the task of extracting
data from the pages retrieved by O UTDESIT. To achieve this goal
the system requires that the user labels the values of single-valued
attributes of interest on a sample page. Based on a light version
of ROAD RUNNER [13, 3, 12], a system to automatically infer Web
wrappers for data rich pages, the DATA E XTRACTOR generates a
wrapper program to extract the labeled values. The inferred wrapper can extract data from every page sharing the same template
as the labeled sample page. Since I NDESIT retrieves pages from
one site according to their structural similarities with a seed page,
the inferred wrapper is executed over all the pages obtained by the
same I NDESIT scan that collected the sample page.
The DATA E XTRACTOR exploits the redundancy of data published in the retrieved pages to generate wrappers for all the pages
retrieved by O UTDESIT.
By means of standard record linkage techniques [16], the DATA E X TRACTOR groups the retrieved pages by instance. Then, the values
extracted in the initial phase are used to progressively (and automatically) label the remaining pages. The labeled pages are used
by the wrapper generation system to create new wrappers. The process is repeated until all the retrieved pages have been processed.
This approach is inspired by a machine learning technique introduced by Lerman et al. [18] for the issue of wrapper maintenance.
The extracted data are then stored into a semi-structured, self describing data base. Currently we rely on Google Base, but it would
be easy to upload the extracted data in other on line services, such
as, for example, Freebase3 or Swivel4 . In Google Base, the data
extracted from each page give rise to a new item, with a suitable
title and its set of labeled values.

3.

RELATED WORK

Our method is inspired to the pioneering DIPRE technique developed by Brin [7]. With respect to DIPRE, which infers patterns
that occur locally within single web pages to encode tuples, we
infer global access patterns offered by large Web sites containing
pages of interest.
Several Web information extraction (IE) techniques have been
derived from DIPRE [1, 15, 5]. Compared to our approach these
approaches are not able to exploit the information offered by data
rich pages. In fact, they concentrate on the extraction of facts: large
collections of named-entities (such as, for example, names of scientists, politicians, cities), or simple binary predicates, e.g. bornin(politician, city). Moreover, they are mostly effective with facts
that appear in well-phrased sentences, whereas they fail to elaborate data that are implied by Web page layout or mark-up practices,
such as those typically published in Web sites containing data rich
pages.
Our work is also related to researches on focused web crawling [9, 21], which face the issue of fetching web pages relevant to
a given topic. However our goal is different as we attempt to retrieve pages that publish data representing an instance of the entity
exemplified by means of an input set of sample pages.
The problem of retrieving documents that are relevant to a user’s
3
4

information need is the main objective of the information retrieval
field [4, 20]. Although our problem is different in nature, in our
method we also exploit state-of-the-art keyword extraction and term
weighting results from IR. We observe that the task performed by
O UTDESIT might resemble the “similar pages” facility offered by
several search engines. However, the semantics of the O UTDESIT
searches is radically different, as our method aims at searching for
pages similar in the intensional description, not in the extensional
one.
There are several recent research projects that address issues related to ours. The goal of C IMPLE is to develop a platform to
support the information needs of the members of a virtual community [14]. Compared to our method, Cimple requires an expert
to provide a set of relevant sources and to design an entity relationship model describing the domain of interest. The MetaQuerier
developed by Chang et al. has similar objectives to our proposal,
as it aims at supporting exploration and integration of databases on
the Web [11]. However it concentrates on the deep-web, while we
search for pages on the surface-web. A new data integration architecture for Web data is the subject of the PayGo project [19]; the
project focuses on the heterogeneity of structured data on the Web:
it concentrates on explicit structured sources, such as Google Base
and the schema annotations of Google Co-op, while our approach
aims at finding data rich pages containing information of interest.
Somehow, our approach can be seen as a service for populating
the data sources over which PayGo works. Cafarella et al. are developing a system to populate a probabilistic database with data
extracted from the Web [8]. Data extraction is performed by TextRunner [5], an information extraction system that suffers the same
problems discussed above for IE systems, and therefore is not suitable for working on data rich Web pages, which are the target of
our searches.

4.

STATUS OF THE DEMONSTRATION

We have focused our experiments5 on the sport domain. The motivation of our choice is that it is easy to interpret the published information, and then to evaluate the precision of the results produced
by our method. The goal of our experiments was to search for a set
of pages, each one containing data about one athlete (player) of a
given sportive discipline. We have concentrated on several disciplines, such as basketball, hockey, golf, and so on.
For each experiment we report:
• the input pages;
• the entity description inferred by the system;
• the set of pages retrieved by the system (together with some
detail to illustrate how the entity description is used by the
system to filter pages returned by a search engine during the
discovery process).
The data extracted by the system have been uploaded into a
Google Base database, whose url is available from the demonstration Web site, together with the url of a Google Co-op custom,
entity aware search engine populated with the retrieved pages.

5.

ACKNOWLEDGMENTS

This work was supported by the PRIN-2006 Program of the Italian Ministry of Scientific Research. Paolo Papotti was partially
supported by an IBM Faculty Award grant.
5

http://www.freebase.com
http://www.swivel.com

The experimental results are available at http://flint.dia.-

uniroma3.it.

723

6.

REFERENCES

Management of Data (SIGMOD’2004), Paris, France, 2004.
[18] K. Lerman, S. Minton, and C. A. Knoblock. Wrapper
maintenance: A machine learning approach. J. Artif. Intell.
Res. (JAIR), 18:149–181, 2003.
[19] J. Madhavan, S. Cohen, X. L. Dong, A. Y. Halevy, S. R.
Jeffery, D. Ko, and C. Yu. Web-scale data integration: You
can afford to pay as you go. In CIDR 2007, Third Biennial
Conference on Innovative Data Systems Research, Asilomar,
CA, USA, January 7-10, 2007, pages 342–350, 2007.
[20] C. D. Manning, P. Raghavan, and H. Schütze. Introduction to
Information Retrieval. Cambridge University Press, 2008.
http://www.informationretrieval.org.
[21] S. Sergej, B. Michael, G. Jens, S. Stefan, T. Martin,
W. Gerhard, and ZimmerPatrick. The bingo! system for
information portal generation and expert web search. In
CIDR 2003, First Biennial Conference on Innovative Data
Systems Research, Asilomar, CA, USA, 2003, 2003.
[22] J. Wang and F. Lochovsky. Data-rich section extraction from
html pages. In Proceedings of the 3rd International
Conference on Web Information Systems Engineering (WISE
2002), 12-14 December, Singapore, pages 313–322. IEEE
Computer Society, 2002.

[1] E. Agichtein and L. Gravano. Snowball: extracting relations
from large plain-text collections. In DL ’00: Proceedings of
the fifth ACM conference on Digital libraries, pages 85–94,
New York, NY, USA, 2000. ACM Press.
[2] A. Arasu and H. Garcia-Molina. Extracting structured data
from web pages. In ACM SIGMOD International Conf. on
Management of Data (SIGMOD’2003), San Diego,
California, pages 337–348, 2003.
[3] L. Arlotta, V. Crescenzi, G. Mecca, and P. Merialdo.
Automatic annotation of data extracted from large web sites.
In WebDB, pages 7–12, 2003.
[4] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern
Information Retrieval. ACM Press / Addison-Wesley, 1999.
[5] M. Banko, M. Cafarella, S. Soderland, M. Broadhead, and
O. Etzioni. Open information extraction from the web. In
IJCAI, 2007.
[6] L. Blanco, V. Crescenzi, and P. Merialdo. Efficiently locating
collections of web pages to wrap. In WEBIST, 2005.
[7] S. Brin. Extracting patterns and relations from the World
Wide Web. In Proceedings of the First Workshop on the Web
and Databases (WebDB’98) (in conjunction with EDBT’98),
pages 102–108, 1998.
[8] M. J. Cafarella, O. Etzioni, and D. Suciu. Structured queries
over web text. IEEE Data Eng. Bull., 29(4):45–51, 2006.
[9] S. Chakrabarti, M. van den Berg, and B. Dom. Focused
crawling: a new approach to topic-specific Web resource
discovery. Computer Networks (Amsterdam, Netherlands),
31(11–16):1623–1640, 1999.
[10] C. Chang, M. Kayed, M. Girgis, and K. Shaalan. A survey of
web information extraction systems. IEEE Transactions on
Knowledge and Data Engineering, 18(10):1411–1428,
October 2006.
[11] K. C.-C. Chang, H. Bin, and Z. Zhen. Toward large scale
integration: Building a metaquerier over databases on the
web. In CIDR 2005, Second Biennial Conference on
Innovative Data Systems Research, Asilomar, CA, USA,
2007, pages 44–66, 2005.
[12] V. Crescenzi and G. Mecca. Automatic information
extraction from large web sites. Journal of the ACM, 51(5),
September 2004.
[13] V. Crescenzi, G. Mecca, and P. Merialdo. ROAD RUNNER:
Towards automatic data extraction from large Web sites. In
International Conf. on Very Large Data Bases (VLDB 2001),
Roma, Italy, September 11-14, pages 109–118, 2001.
[14] A. Doan, R. Ramakrishnan, F. Chen, P. DeRose, Y. Lee,
R. McCann, M. Sayyadian, and W. Shen. Community
information management. IEEE Data Eng. Bull.,
29(1):64–72, 2006.
[15] O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu,
T. Shaked, S. Soderland, D. S.Weld, and A. Yates.
Unsupervised named-entity extraction from the web: An
experimental study. Artificial Intelligence, 165:91–134,
2005.
[16] N. Koudas, S. Sarawagi, and D. Srivastava. Record linkage:
similarity measures and algorithms. In SIGMOD ’06:
Proceedings of the 2006 ACM SIGMOD international
conference on Management of data, pages 802–803, New
York, NY, USA, 2006. ACM Press.
[17] K. Lerman, L. Getoor, S. Minton, and C. Knoblock. Using
the structure of web sites for automatic segmentation of
tables. In ACM SIGMOD International Conf. on

724

Mapping and Cleaning
Floris Geerts 1 ,

Giansalvatore Mecca 2 ,

Paolo Papotti 3 ,

Donatello Santoro 2,4

1

University of Antwerp – Antwerp, Belgium

2

Università della Basilicata – Potenza, Italy

floris.geerts@ua.ac.be
giansalvatore.mecca@gmail.com
3

Qatar Computing Research Institute (QCRI) – Doha, Qatar
ppapotti@qf.org.qa
4

Universita` Roma Tre – Roma, Italy
donatello.santoro@gmail.com

(i) we assume that the source databases may contain inconsistencies, and – while this is not mandatory in our approach –
possibly come with an associated confidence. In our example,
we assume that a confidence of 0.5 has been estimated for the
first data source, and 0.7 for the second;

Abstract—We address the challenging and open problem of
bringing together two crucial activities in data integration and
data quality, i.e., transforming data using schema mappings,
and fixing conflicts and inconsistencies using data repairing.
This problem is made complex by several factors. First, schema
mappings and data repairing have traditionally been considered
as separate activities, and research has progressed in a largely
independent way in the two fields. Second, the elegant formalizations and the algorithms that have been proposed for both
tasks have had mixed fortune in scaling to large databases. In
the paper, we introduce a very general notion of a mapping and
cleaning scenario that incorporates a wide variety of features,
like, for example, user interventions. We develop a new semantics
for these scenarios that represents a conservative extension of
previous semantics for schema mappings and data repairing.
Based on the semantics, we introduce a chase-based algorithm
to compute solutions. Appropriate care is devoted to developing
a scalable implementation of the chase algorithm. To the best of
our knowledge, this is the first general and scalable proposal in
this direction.

(ii) besides possibly unreliable sources, we allow for the presence of authoritative sources; in our example, as it is common
in large organizations, a master-data [4] table, Hospital MD , is
available, containing a small set of curated tuples that represent
a source of highly reliable and synchronized information for
the target. Differently from sources 1 and 2, which will be
primarily used to move data into the target and can generate
conflicts, in our approach authoritative sources are used to
repair the target and remove inconsistencies;
(iii) the target can be modified and we do not assume the
target to be empty, as it is typical in data translation.
The target also comes with a number of constraints. More
specifically: (a) there is an inclusion constraint of the form
Prescriptions.npi ⊆ Doctors.npi; (b) attribute id is a key for
table Prescriptions ; (c) table Doctors has two keys, namely
attributes npi (the National Provider Identifier for doctors) and
name . Notice that the source and target tables may contain
inconsistencies. For example, Dr. R. Chase is assigned different
npi s by the source database (1112 and 1222).

I. I NTRODUCTION
Schema mappings have been proposed in the database
literature as an enabling technology for modern data-driven
applications. Mappings are executable transformations that
specify how an instance of a source repository should be
translated into an instance of a target repository. A rich body
of research has investigated mappings, both with the goal
of developing practical algorithms [1], and nice and elegant
theoretical foundations [2].

A data architect facing this scenario must therefore deal
with two different tasks. On the one side, s/he has to develop
the mappings to exchange data from the source databases to
the target. On the other side, s/he has to devise appropriate
techniques to repair inconsistencies that may arise during the
process. We next discuss this in more detail.

However, it is also well known that data often contain
inconsistencies, and that dirty data incurs economic loss and
erroneous decisions [3]. The data-cleaning (or data-repairing)
process consists in removing inconsistencies with respect to
some set of constraints over the target database.

Step 1: Data Translation The desired transformation can be
expressed as a set of tuple generating dependencies (tgds) [2]:
i.e., two source-to-target tgds and one target tgd to express
the given inclusion constraint, as follows (as usual, universal
quantifiers in front of the tgds are omitted):
mst1 . Treat(id, pat, hos, npi), Phys(npi, doc, sp)
→ Presc(id, pat, npi, 0.5), Doc(npi, doc, sp, hos, 0.5)
mst2 . MedPresc(id, pat, npi, doc, sp)
→ ∃Y : Presc(id, pat, npi, 0.7), Doc(npi, doc, sp, Y, 0.7)
mt1 . Presc(id, pat, npi, cf) → ∃Y1 ,Y2 ,Y3 : Doc(npi,Y1 ,Y2 ,Y3 , cf)
Each tgd states a constraint over the target database. For
example, tgd mst2 says that for each tuple in the MedPrescriptions source table, there must be corresponding tuples in
the Prescriptions and Doctors target tables; Y is an existential
variable representing values that are not present in the source
database but must be present in the target.

We may say that both schema-mappings and data cleaning
are long-standing research issues in the database community.
However, so far they have been essentially studied in isolation.
On the contrary, we notice that whenever several possibly
dirty databases are put together by schema mappings, there is
a very high probability that inconsistencies arise, and therefore
there is even more need for cleaning. Solving this problem is
a challenging task, as discussed in the following example.
Example 1: Consider the mapping scenario shown in Figure
1 in which several different hospital-related data sources must
be correlated to one another. The first repository has information about Treatments and Physicians . The second one about
MedPrescriptions . In turn, the target database organizes data in
terms of Prescriptions and Doctors . Notice how we are leaving
ample freedom in the choice of sources. In fact:

978-1-4799-2555-1/14/$31.00 © 2014 IEEE

232

ICDE Conference 2014

Source #1 – Confidence 0.5
TREATMENTS

SOURCE

ID Patient
t1 P1

W. Smith

PHYSICIANS

Hospital

NPI

NPI

PPTH

1112

t2 1112

Source #2 – Confidence 0.7

R. Chase

urol

HOSPITALS MD

ID Patient NPI Doctor Spec
t3 P1 I. Smith 1222 R. Chase diag

TARGET

Spec

Source #3 – Master Data

MEDPRESCRIPTIONS

tm G.House diag

PPTH

DOCTORS

ID Patient NPI Conf

NPI Name

Spec Hospital Conf

t5 5556 G. House surg

PPTH

0.5

t6 5555 G. House urol

null

0.1

(a) Initial instances

PRESCRIPTIONS

PRESCRIPTIONS
ID Patient

t11 P1 I. Smith

ID Patient NPI

NPI Conf

t10 P1 W. Smith 1112

t11 P1 I. Smith

0.5

1222 0.7

NPI Name

1222

DOCTORS
NPI Name

Spec Hospital

t5 5556 G. House diag

PPTH

t14 1222 R. Chase

PPTH

diag

Minimal Solution a

DOCTORS

Doctor Spec Hospital

PRESCRIPTIONS
(empty)

Name

Spec Hospital Conf

t5 5556 G. House surg

PPTH

0.5

t6 5555 G. House urol

null

0.1

t13 1112 R. Chase

urol

PPTH

0.5

t14 1222 R. Chase

diag

null

0.7

(b) PreSolution for the tgds

PRESCRIPTIONS
ID Patient NPI

DOCTORS
NPI Name

Spec Hospital

t10 La W. Smith 1222 t5 5556 G. House diag
t11 P1 I. Smith 1222 t14 1222 R. Chase diag

PPTH
PPTH

Minimal Solution b
(c) Minimal, clean solutions

Fig. 1: A Hospital Mapping Scenario.

Step 2: Conflict Resolution Once the mappings have been
executed – for example by translating them into an SQL script
– a pre-solution like that in Figure 1.(b) is generated. This
instance satisfies the s-t tgds and the target inclusion constraint.
However, it contains inconsistencies wrt the key constraints
(highlighted in bold) due to conflicts in the sources.

among the sources with respect to a very wide class of target
constraints;

As it was recently noted [5], this data repairing problem
can be seen as a cleaning scenario composed of cleaning
equality generating dependencies (egds). These generalize
many common data quality constraint formalisms, as follows
(confidence attributes are omitted):
e1 . Presc(id, p, npi), Presc(id, p′ , npi′ ) → p = p′ , npi = npi′
e2 . Doc(npi, n, sp, h),Doc(npi, n′, sp′, h′) → n = n′,sp = sp′,h = h′
e3 . Doc(npi, n, sp, h), Doc(npi′ , n, sp′ , h′ ) → npi = npi′,sp = sp′,h = h′
e4 . HospMD(n, sp, h), Doc(npi, n, sp’, h) → sp = sp′
Egd e4 is a cleaning egd [5], an extended form of egd in which
both source (the master data table) and target symbols may
appear in the premise; it corresponds to an editing rule [6], and
states how values in the Doctors table can be corrected based
on the master-data table Hospital MD . Notice that repairing the
pre-solution may cause a violation of the tgds and hence the
mappings in Step 1 need to be applied again.

In fact, we will show that simply pipelining existing data
exchange and data repairing algorithms often does not provide
solutions. This is due to the fact that mappings and quality
constraints may interact in a complex way, and therefore
require the development of new methods in order to be handled
together. Such development is far from trivial, for a number
of reasons. On the one side, the standard semantics of schema
mappings [2] has been conceived with no conflicts in mind. On
the other side, most of the recent data repairing [3] and conflict
resolution [7] algorithms concentrate on a single inconsistent
table that is dirty and needs to be repaired, and can hardly be
extended to handle a complex data transformation task

(ii) the framework is a conservative extension of the wellknown framework of data exchange, and of most of the existing
algorithms for data repairing; at the same time, it considerably
extends its reach in both activities; in fact, on the one side it
brings a powerful addition to schema mappings, by allowing
for sophisticated conflict resolution strategies, authoritative
sources, and non-empty target databases; on the other side,
it extends data repairing to a larger classes of constraints,
especially inclusion dependencies and conditional inclusion
dependencies [3], that are very important in the management
of referential integrity constraints, and for which very little
work exists;
(iii) we implement the new semantics for mapping and
cleaning scenarios under the form of a chase algorithm. This
has the advantage of building on a popular and principled
algorithmic approach, but it has a number of subtleties. In
fact, our chase procedure is more sophisticated than the
standard one, in various respects. To give an example, we
realize that in the presence of inconsistencies user inputs may
be crucial. To this aim, we introduce a nice abstraction of
user inputs and show how it can be seamlessly integrated into
the chase. This may pave the way to the development of new
tools for data repairing, in the spirit of [8];
(iv) in addition, scalability is a primary concern of this work.
Given the complexity of our chase procedure, addressing this
concern is quite challenging. Therefore, we introduce a number
of new optimizations to alleviate computing times, making the
chase a viable option to exchange and repair large databases.
In fact, as a major result, we show in our experiments that
the chase engine is orders of magnitude faster than existing
engines for data exchanges, and show superior scalability wrt
previous algorithms for data repairing [9], [10] that were
designed to run in main memory.

Contributions We build on our recent work [5] and make
two important and nontrivial contributions. The first one is the
formalization of a new framework for mapping and cleaning
that tackles the problems discussed in Example 1. The second
consists in the development of a general-purpose chase engine
that scales nicely to large databases. More specifically:

To the best of our knowledge, this is the first proposal
that achieves the level of generality needed to handle three
different kinds of problems: traditional mapping problems,
traditional data repairing problems, and the new and more
articulated category of data translation problems with conflict
resolution, as exemplified in Example 1. In fact, we believe
that this proposal may bring new maturity to both schema
mappings and data repairing.

(i) we develop a general framework for mapping and cleaning
that can be used to generate solutions to complex data transformation scenarios, and to repair conflicts and inconsistencies

233

II.

repair contains cell group g, that has been generated at previous
chase steps. Forward chasing tuples t5 , t6 with master data
tuple tm and egd e4 further corrects the spec cells to value
diag. Since the value comes from an authoritative source,
we repair the violation by a new cell group g ′ , in which
cell tm .spec is explicitly stored as a justification, in symbols:
g ′ = hdiag → {t5 .spec, t6 .spec} by {tm .spec}i. Conditional
functional dependencies [3] are handled in a similar way.
Again, the new repair Rep′ = {g ′ } is an upgrade of Rep. In
fact, its cell groups carry more justifications, and are therefore
better supported by authoritative sources.

OVERVIEW

A Quick Data Repairing Tutorial We start by providing a
quick summary of the L LUNATIC data repairing framework
[5]. Let us first ignore tgds and concentrate on tuples that are
already present in the target. Assume that we are given the
constraints expressed by egds e1 –e4 , and need to repair the
target. Consider for example egd e3 that states that name is
a key for Doctors . Tuples t5 : (5556, G. House, surg, PPTH)
and t6 : (5555, G. House, urol, null) contain various violations.
In data repairing terminology, these correspond to cells – i.e.,
tuple attributes – with conflicting values.

Lluns So far we have used various rules to solve conflicts,
in part standard (constants are better than nulls, authoritative
values are better than target values), in part scenario-dependent
and specified through p . There are, however, cases in which
these rules do not suggest any clear strategy to remove a violation. Consider the npi attribute. Suppose that no specification
has been provided as to which of the values of npi should be
preferred. In this case, to forward chase dependency e3 , we
need to change cells t11 .npi and t12 .npi into some unknown
value that may be either 5555 or 5556, or some other preferred
value that we currently do not know.

Conflicts and Partial Orders Our approach is to chase the
database using the dependencies to remove the conflicts. There
are various ways to chase tuples t5 and t6 with egd e3 . One
obvious way is to equate the values of conflicting cells. This
corresponds to chasing the egd in a forward way, and requires
selecting a preferred value among the conflicting values.
Discarding the null value is an obvious choice for the
hospital attribute. In other cases, the value to prefer is less

obvious, and may differ from case to case. In fact, a flexible
data repairing algorithm should allow users to easily specify
their preference strategies. Consider for example attribute Spec .
Since tuples come with a confidence value, it is reasonable in
this example to choose the value for specialty with the higher
confidence, surg in this case. An elegant way to model such
preferences is to specify a partial order p , i.e., an order of
preference among values of cells in the database. Assume the
partial order specified by the user gives preference to values
with higher confidence, i.e., urol (conf. 0.1) p surg (conf.
0.5). Then, we may forward chase tuples t5 , t6 for attribute
spec by changing cell t6 .spec to surg.

To mark this fact, we introduce a new class of symbols,
called lluns. In essence, lluns are placeholders used to mark
conflicts for which a solution will need to be provided later
on, for example by asking for user inputs. In this respect, lluns
can thus be regarded as the opposite of nulls since lluns carry
“more information” than constants.
We may summarize by saying that forward chasing t5 and
t6 with egds e3 , e4 generates the following repair to upgrade
the original target database (we omit cell group justifications
when they are empty; L0 is a llun):
Rep = { g1 = hPPTH → {t5 .hospital, t6 .hospital}i
g2 = hdiag → {t5 .spec, t6 .spec} by {tm .spec}i
g3 = hL0 → {t5 .npi, t6 .npi}i . . .}

Cell Groups and Upgrades We model this partial repair by
a cell group. Its primary function is to express a relationship
among cells that need to be repaired together. In essence, since
t5 .spec and t6 .spec have been equated to satisfy egd e3 , we
want to keep track of this during the chase, so that we do not
disrupt this equality in the following. We write this new cell
group g that has been generated by the chase by the syntax:
g = hsurg → {t5 .spec, t6 .spec}i
and call {t5 .spec, t6 .spec} the occurrences of g.
Cell groups are at the core of the chase algorithm. In fact,
we see databases as collections of cell groups, with the starting
database corresponding to the trivial set of cell groups in which
each cell is assigned its own value. Each chase step removes
one or more conflicting cell groups, and generates a new cell
group to remove the violation. Therefore, after our first chase
step, a repair Rep = {g} of J has been generated.

Backward Chasing and Chase Trees There are, however,
different ways to enforce an egd. Besides equating values to
satisfy the conclusion, one may think of falsifying the premise.
For example, consider egd e3 that states that name is a key, i.e.,
two tuples that agree on name cannot have different values. As
an alternative to forward chasing tuples t5 , t6 , we may also
chase them backward. To do this, it suffices to change the
name attribute of either tuple to a new llun, Li . This means
that there are three different ways to chase t5 , t6 with e3 : (i)
the forward repair discussed above; (ii) two backward repairs,
the first one made of cell group gb1 = hLb1 → {t5 .name}i, the
second of gb2 = hLb2 → {t6 .name}i.

A crucial feature of our approach is that Rep(J) is an
improvement over J , since it was obtained by changing the
value of a conflicting cell to a “better” value, according to
the partial order specified by the user. We therefore say that
Rep(J) is an upgrade of the original dirty database J .

To be more precise, in both cases we shall change into a
llun the entire cell group of t5 .name and t6 .name. This is in fact
a key feature of the chase algorithm: it preserves cell groups
in order to guarantee that each chase step actually upgrades
the target wrt the previous one.

Cell groups are even more powerful than this, since they
can also carry lineage information for changes to the database,
in terms of values from authoritative sources. Consider for
example egd e4 , that specifies how to correct the target
database based on master data tuples. Recall that our current

It should be clear that the chase is in fact a parallel chase,
and generates a tree of repairs. This is due not only to the
presence of forward and backward repairs, but also to the fact
that dependencies may be repaired in different orders, and this
may yield different results.

234

by dom(I ) the set of constants and nulls in I . We assume the
presence of unique tuple identifiers in an instance; by ttid we
denote the tuple with id “tid ” in I. A cell is a location in I
specified by a tuple id/attribute pair ttid .Ai .

Cleaning Scenarios The ideas discussed above have been
formalized under the notion of a cleaning scenario [5]. Intuitively, in a cleaning scenario, clean source instances of a
source schema S (e.g., master data) are linked to dirty target
instances of a target schema T by means of cleaning egds Σe ;
solutions to cleaning scenarios can be regarded as repairs of
the target instances, and are generated by means of the chase
procedure discussed above, that, in contrast to its counterpart
in data exchange, never fails.

Dependencies A relational atom over R is a formula of the
form R(x) with R ∈ R and x is a tuple of (not necessarily distinct) variables. A tuple-generating dependency(tgd) over R
is a formula of the form ∀ x φ(x) → ∃ yψ(x, y) , where φ(x)
and ψ(x, y) are conjunctions of relational atoms over R. Given
two disjoint schemas, S and T , a tgd over hS, T i is called a
source-to-target tgd (s-t tgd) if φ(x) only contains atoms over
S, and ψ(x, y) over T . Furthermore, a target tdg is a tgd in
which both φ(x) and ψ(x, y) only contain atoms over T . An
equality generating dependency (egd) over T is a formula of
the form ∀ x(φ(x) → xi = xj ) where φ(x) is a conjunction
of relational atoms over T and xi and xj occur in x.

Mapping and Cleaning A significant limitation of cleaning
scenarios is that they do not support tgds. We next provide
the main intuitions behind our extension of cleaning scenarios
with tgds, referred to as mapping and cleaning scenarios.
The first crucial intuition is that, while the chase of tgds
has traditionally been seen as the insertion of tuples into the
target, with some effort it is possible to model it in terms
of cell groups and repairs. Consider tgd mt1 in our example,
stating the inclusion dependency Prescr.npi ⊆ Doc.npi (similar
arguments hold for the s-t tgds).

Notice that, besides these standard definitions, in this paper
we will make use of extended tgds and cleaning egds over
schemas S, T , where φ(x) is a conjunction of relational atoms
over S ∪T , i.e., we mix source and target atoms in the premise.

Assume that a new tuple tx with npi 123 has been added to
the prescriptions by chasing the s-t tgds, and this violates the
constraint. Then, we chase tuple tx with mt1 as follows: (i)
we add a new tuple ty : (123, N1 , N2 , N3 ) to table Doctors ;
(ii) in addition, we update the current repair by adding a new
cell group, to keep track that the foreign key tx .npi and the
primary key ty .npi are related to each other: gx = h123 →
{tx .npi, ty .npi}i. This is crucial in our semantics: in this way,
we guarantee that any following step upgrades the database by
maintaining this cell group, and no unnecessary regressions are
introduced in the repair.

We assume the standard definition [2] of a mapping scenario, solution, universal solution, and core solution as the
smallest of the universal solutions for a mapping scenario M
over instances hI , J i of hS, T i. We also assume the standard
definition of the chase of a mapping scenario M over hI , J i.
IV. M APPING & C LEANING S CENARIOS
Building on the L LUNATIC framework, we next provide an
extension that supports both tgds and egds.
Definition 1 [M APPING &C LEANING S CENARIO] Given a domain D = CONSTS ∪ NULLS ∪ LLUNS, a mapping & cleaning
scenario over D is a tuple MC = {S, Sa , T , Σt , Σe , p
, User}, where: (i) S ∪ Sa is the source schema and T is
the target schema; Sa denotes the set of authoritative source
tables; (ii) Σt is a set of extended tgds, as defined in Section
III; (iii) Σe is a set of cleaning egds, as defined in Section
III; (iv) p is a specification of a partial-order for the values
of cells in the target database; (v) User is a partial function
over repairs used to model user inputs.

Notice also that we do not backward-chase tgds, since the
only way to backward-chase a tgd like mt1 consists in deleting
tuple tx , and there is general consensus that deleting tuples
causes unnecessary loss of information.
Chasing with User Inputs User inputs are modeled in mapping & cleaning scenarios by means of an oracle function,
User, that works on repairs, or, better, on the cell groups they
are made of. Function User is a partial function, to model the
fact that users are usually only requested to provide fragments
of inputs. It may be invoked on any node of the chase tree,
and may either (i) change the value of a cell group, (ii) refuse
a cell group – and therefore the entire repair of a chase node
– because it is considered as a wrong way to clean the target.
In the chase, we therefore have a third kind of step, aside
from those that chase egds and tgds, and these correspond to
invoking function User over a node of the tree.

LLUNS ,

p and User are described in more detail next.

LLUNS and partial orders Let hI , J i be an instance of
hS, Sa , T i. Apart from taking values from CONSTS and
NULLS , target instances J may take values from a third countably infinite set of values, LLUNS = {L1 , L2 , . . .}, distinct
from CONSTS and NULLS.

These extensions come with a cost. In Section V, we show
that a complete revision of the notions of satisfaction of a
dependency, upgrade, and ultimately of the notion of a solution
is required.

A preference relation ⊳ among values formalizes the
relationship between NULLS, CONSTS and LLUNS. Given two
values v1 , v2 ∈ NULLS ∪ CONSTS ∪ LLUNS, we say that v2 is
more informative than v1 , in symbols v1 ⊳ v2 , if v1 and v2 are
of different types, and either (i) v1 ∈ NULLS, i.e., v1 is a null
value; or (ii) v2 ∈ LLUNS, i.e., v2 is a llun.

III. S CHEMA -M APPINGS BACKGROUND
A schema R is a finite set {R1 , . . . , Rk } of relation
symbols, with each Ri having a fixed arity ni ≥ 0. Let
CONSTS be a countably infinite domain of constant values and
NULLS be a countably infinite set of labeled nulls, distinct from
CONSTS . An instance I = (I1 , . . . , Ik ) of R consists of finite
relations Ii ⊂ (CONSTS ∪ NULLS)ni , for i ∈ [1, k]. We denote

We also assume that values of cells from authoritative
tables in I are always preferred over those coming from J.
In addition, users can declaratively specify preferred values
in CONSTS by means of a partial order  among the values
of cells in hI , J i. One easy and effective way to do this was

235

In this section, we formalize the semantics of solutions
of a mapping & cleaning scenario. Intuitively, a solution of a
mapping and cleaning scenario is a set of repair instructions of
the target (represented by cell groups) that upgrades the initial
dirty target instance and that satisfies the dependencies in Σt
and Σe with a revised notion of satisfaction. We first give the
formal definition of a solution. Then, we introduce the main
ingredients of the definition.

introduced in [5]. In essence, users may specify a preference
rule for each attribute of the target schema under the form of
a partially-ordered set. Consider Example 1; as we discussed,
it would be natural to state that, whenever two different
specialties are present for the same doctor, the one with
higher confidence should be preferred. To specify this, users
may associate with attribute spec the poset (DC ONF , ≤), where
DC ONF is the domain of attribute conf, and ≤ is the standard
order relation over real values. In essence, we are saying that,
whenever specialties v1 and v2 of spec-cells need to be ordered:
(i) we look up the corresponding conf-cells, i.e., the values
of the conf attribute of the respective tuples; (ii) we order
them according to ≤; (iii) we induce an order for the original
specialty values from that.

Definition 2 [S OLUTION] Given a mapping&cleaning scenario
M = {S, Sa , T , Σt , Σe , p , User}, and an input instance
hI , J i, a solution for M over hI , J i is a repair Rep s.t.:
(i) Rep upgrades the initial target instance J, in symbols
J p,User Rep;

Associated with a mapping & cleaning scenario we therefore have a partial order p , that subsumes both ⊳, and ,
i.e., ⊳ ∪  ⊆ p . We use lluns to ensure the completeness of
p . In fact, given a set of cells from a database, C, we define
the upper bound vallub (C), as the least upper bound of their
values according to p , if this exists, or a new llun value.

(ii) hI, Rep(J)i satisfies after repairs Σt ∪ Σe under p,User .
We first revise the definition of a cell group to allow for
tgds. Let hI , J i be an instance of hS, Sa , T i. We denote by
auth-cells(I) and target-cells(J) the set of cells in the authoritative tables in I and in the target instance J, respectively. Let
new-cells(J) denote the (infinite) set of cells corresponding
to tuples over T that are not in J. Intuitively, new-cells(J)
represent possible insertions in J.

User Inputs We abstract user inputs by seeing the user as an
oracle. More formally, we call a user-input function a partial
function User that takes as input a pair of instances, hI , J i, and
a set of cells C over hI , J i and returns one of the following values, denoted by User(C): (i) v, to denote that the target cells in
C should be repaired to value v; (ii) ⊥, to denote that repairing
the cells in C would represent an incorrect modification to the
database, and therefore the repair should not be be performed.

Definition 3 [C ELL G ROUP] A cell group g over hI , J i is a
triple g = hv, occ(g), just(g)i, where:
(i) v = val(g) is a value in

∪ NULLS ∪ LLUNS;

(ii) occ(g) is a finite set of cells in target-cells(J) ∪
new-cells(J), called the occurrences of g;

It is readily verified that Example 1 can be regarded as an
instance of a mapping & cleaning scenario.
V.

CONSTS

(iii) just(g) is a finite set of cells in auth-cells(I), called the
justifications of g. We denote by cells(g) the set occ(g) ∪
just(g).

S EMANTICS

One may wonder why a new semantics is needed after all.
Indeed, why can’t we simply rely on the standard semantics
for tgds [2], and on known data repairing algorithms, like those
in [5], [9] or [10]? As an example, let Σt be a set of tgds and
Σe be a set of egds, and I and J instances of S ∪ Sa and
T , respectively. Assume that we simply pipeline the chase of
tgds, chasede
Σt , [2], and a repair algorithm for egds, repairΣe ,
as reported in Figure 2.

A cell group g can be read as “change (or insert) the cells
in occ(g) to value val(g), justified by the values in just(g)”.
We therefore shall often write a cell group as g = hv →
occ(g), by just(g)i for the sake of readability.
In a mapping & cleaning scenario, we are only interested
in cell-groups that are consistent with the partial order p
and user oracle User. We say that a cell group g = hv →
occ(g), by just(g)i has a valid value if one of the following
conditions holds.

pipelineΣt ∪Σe (hI , J i)
hI, Jtmp i := hI , J i;
while (true)
hI, Jtmp i := chasede
Σt (hI, Jtmp i);
hI, Jtmp i := repairΣe (hI, Jtmp i);
if (hI, Jtmp i |= Σt ∪ Σe ) return Sol := Jtmp ;
end while

(a) val(g) = User(cells(g)), for some constant v ∈
i.e., the value of g comes from some user input;

CONSTS ,

(b) User(cells(g)) is undefined, and val(g) = vallub (cells(g)),
i.e., the cell group takes the value of the least upper bound as
defined in Section IV; in this case we say that g is strict.

Fig. 2: The pipeline algorithm.

Unfortunately, interactions between tgds and egds often
prevent that pipelining the two semantics returns a solution,
as illustrated by the following proposition.

(c) User(cells(g)) is undefined, and val(g) ∈
value of g is a llun value.

LLUNS ,

i.e., the

In the following, we assume that all cell groups are valid.

Proposition 1: There exist sets Σt of non-recursive tgds, Σe
of cleaning egds, and instances hI , J i such that procedure
pipelineΣt ∪Σe (hI , J i) does not return solutions.

Example 2: Consider relation R(A, B), with three dependencies: (i) an FD A → B, and two CFDs [3]: (ii) A[a] →
B[v1 ], A[a] → B[v2 ]; the first one states that whenever R.A
is equal to “a”, R.B should be equal to “v1 ” (similarly
for the second); in our approach, these are encoded by egds

In addition, as we will show in our experiments, even
in those cases in which pipelineΣt ∪Σe (hI , J i) does return a
solution, its quality is usually rather poor.

236

e1 : R(x, y), c1(x, z) → y = z, and e2 : R(x, y), c2(x, z) →
y = z; in fact, we see constants as values coming from the
source database; to this end, we introduce two source tables,
c1, c2, with a single tuple tc1 , tc2 each, encoding the patterns
in the CFDs: tc1 : (A : a, B : v1 ), tc2 : (A : a, B : v2 ) [5]. Notice
that the two CFDs clearly contradict each other. Nevertheless,
differently from previous approaches, later on we will provide
a semantics for this well known example [3] by leveraging
lluns first, and then user inputs. Assume R contains two tuples:
t1 : R(a, 1), t2 : R(a, 2), and that p states that greater values
of the A attribute are to be preferred over smaller ones (i.e.,
the poset associated with B is its own domain with the standard
≤ on integers). Following is a set of cell groups:
g1 = h1 → {t1 .B}, by ∅i
g1′ = hL1 → {t1 .B}, by ∅i
g2 = h2 → {t1 .B, t2 .B}, by ∅i
g2′ = h3 → {t1 .B, t2 .B}, by ∅i
g3 = hv1 → {t1 .B, t2 .B}, by {tc1 .B}i
g4 = hL → {t1 .B, t2 .B}, by {tc1 .B, tc2 .B}i
g5 = hk → {t1 .B, t2 .B}, by {tc1 .B, tc2 .B}i

user input. Furthermore, since the presence of tgds possibly
results in the creation of tuples that are not in the initial target
instance, we then show how to compare cell groups and repairs
with different tuple ids.
We denote by p,User the partial order over cell groups.
Intuitively, when comparing cell groups with p,User , we
should give priority to values specified by user-inputs, then
to values from authoritative sources, and then again to values
taken according to the preference strategy specified by p .
More formally, given cell groups g and g ′ with valid values,
we say that g p,User g ′ iff:
(i) occ(g) ⊆ occ(g ′ ) and just(g) ⊆ just(g ′ ), i.e., we say that
a cell group g ′ can only be preferred over a cell group g
according to p if a containment property is satisfied; if the
containment property is not satisfied then these cell groups
represent incomparable ways to modify a target instance. In
addition, we require that one of the following holds:

valid, strict
valid, non strict
valid, strict
non valid
valid, strict
valid, strict
valid, strict if
User(cells(g4 )) = k

(ii.a) val(g ′ ) ∈ CONSTS and val(g ′ ) = User(cells(g ′ )), i.e.,
the value of g ′ comes from a user input, or:
(ii.b) User(cells(g)) and User(cells(g ′ )) are undefined, and
either g and g ′ are strict, or val(g) ⊳ val(g ′ ). In fact, if g
and g ′ are strict, the containment property above guarantees
that g ′ carries more occurrences and/or more justifications, and
therefore it is to be preferred. As an alternative, it might be
that g ′ is not strict, i.e., it is “overgeneralizing” the value of
its occurrences.

Cell groups are used to specify repairs, i.e., modifications
to the target database. In our approach, this can be done in two
ways: (i) by changing cell values, as specified in cell groups;
(ii) by adding new tuples as an effect of tgds.
Definition 4 [R EPAIR] A repair Rep of J is a set of cell groups
over hI , J i such that there exists a set of tuples ∆J, distinct
from J, for which:

Consider Example 2. Cell groups are ordered as follows:
g1 p,User g2 p,User g3 p,User g4 p,User g5

(i) each cell occurring in Rep corresponds to a cell in J ∪ ∆J;

It is also true that g1 p,User g1′ , since g1′ is not strict, and
L1 is more informative than 1 (item ii.b).

(ii) each cell in J ∪ ∆J occurs at most once in Rep;
(iii) each cell in ∆J takes the value specified by Rep.

Id Mappings and Upgrades We next lift p,User from cell
groups to repairs. To accommodate for the introduction of new
tuples, as possibly required by the tgds, we need to be able
to compare cell groups and repairs with different tuple ids.
Consider Example 3: to discover that Rep1 is minimal, we
need to map tuple t1 ∈ ∆J1 into tuple t2 ∈ ∆J2 . We do this
using id mappings.

We denote by Rep(J) the target instance obtained by changing
the values in J as specified by Rep and by inserting ∆J.
Example 3: Consider an authoritative source table R(A, B),
and a target table S(A, B), with a tgd R(x, y) → ∃z : S(x, z).
Suppose I = {t : R(1, 2)}, following is a set of repairs:
∆J1 = {t1 : S(1, N1 )}
∆J2 = {t2 : S(1, 3)}

Let Rep and Rep′ be two repairs over hI, Ji. An id mapping
hid from Rep to Rep′ maps tuple ids appearing in Rep, denoted
by tids(Rep), into those appearing in Rep′ , tids(Rep′ ). We
denote by hid (ti ) the image of ti . An id mapping hid can be
extended to cells and then to cell groups. Given cell ti .Aj , its
image according to hid is the cell hid (ti ).Aj . Given g and hid ,
the image of g according to hid is the cell group: hid (g) =
hv ′ → hid (occ(g)) by hid (just(g))i with v ′ defined as follows:

Rep1 = {g11 = h1 → {t1 .B}, by {t.A}i
g12 = hN1 → {t1 .B}, by ∅i}
Rep2 = {g21 = h1 → {t2 .B}, by {t.A}i
g22 = h3 → {t2 .B}, by ∅i}

A repair is simply one possible way of changing the target
database. Different ∆J yield completely different repairs. On
the contrary, in a mapping & cleaning scenario, we are only
interested in repairs that are solutions for the given scenario,
i.e., that represent actual upgrades of the target and satisfy
the dependencies. For example, {g1 } and {g2 } are repairs for
Example 2, but are not solutions. Among these, of particular
interest are minimal solutions. In Example 3 Rep1 is minimal,
while Rep2 is not. We introduce these notions next.

(i) if g is strict, then v ′ = vallub (hid (cells(g))), and we say
that hid (g) is strict;
(ii) if val(g) = L ∈

LLUNS ,

then v ′ = L;

(iii) if val(g) = User(cells(g)), then v ′ = User(hid (cells(g))),
if it is defined; otherwise, we say that hid (g) is undefined.

Partial Order over Cell Groups In order to define when a
repair Rep can be regarded as an upgrade to the initial target
instance, we lift the partial order p from values to repairs. To
achieve this, we revise the partial order on cell groups (as used
in cleaning scenarios) such that it properly takes into account

We rely on id mappings to revise our notion of an upgrade.
Definition 5 [U PGRADE] Given two repairs Rep and Rep′ over
hI, Ji, a partial order specification p and an oracle User
of cell groups over hI, Ji, we say that Rep′ upgrades Rep,

237

denoted by Rep p,User Rep′ , if there exist an id mapping
hid : tids(Rep) → tids(Rep′ ) such that for each cell group
g ∈ Rep there exists a cell group g ′ ∈ Rep′ such that hid (g)
is defined, and hid (g) p,User g ′ .

by h with z is g = hv1 , {}, {tc1 .B}i (it has no occurrences
since z is mapped to a single source cell), and the cell group
of y in Rep, g5 upgrades g.
Next, consider a tgd m : ∀x, z(φ(x, z) → ∃y (ψ(x, y)))
that is not satisfied by hI, Rep(J)i. Let h be a homomorphism
of φ(x, z) into hI, Rep(J)i that cannot be extended to a
homomorphism h′ of ψ(x, y) into hI, Rep(J)i. We now want
to regard m is being satisfied after repairs whenever Rep(J)
is an upgrade of the canonical repair for m and h.

Satisfaction after Repairs It is crucial that our semantics
properly handle the interaction of tgds and egds. We have
had several hints that enforcing egds may disrupt the logical
satisfaction of tgds, or even of other egds. To see this, consider
first Example 2. Notice that, after we upgrade the database with
cell group g5 we write a user input, k into cells t1 .B, t2 .B, to
obtain two identical tuples t1 : R(a, k), t2 : R(a, k) . However,
the two (contradicting) conditional functional dependencies
in this example state that, whenever R.A equals a, R.B
must be equal to v1 , v2 , respectively. Therefore, after g5 , the
corresponding egds are not satisfied in the standard sense.

represents the
Intuitively, the canonical repair Repcan
h
“standard way” to repair the tgds, defined as follows. Let hcan
be the canonical homomorphism that extends h by injectively
assigning a fresh labeled null with each existential variable.
Consider the new instance Jcan = J ∪ hcan (ψ(x, y)), obtained
by adding to J the set of tuples in hcan (ψ(x, y)), each with a
fresh tuple id. Then, Repcan
is such that:
h

To give another example with tgds, consider our motivating
Example 1. The s-t tgd mst1 uses source tuples t1 , t2 from
source #1 to generate tuple t13 : (1112, R. Chase, urol, PPTH)
into the target; we call t13 the canonical repair for mst1 and
t1 , t2 . After egds have been enforced, along the lines discussed
in Section II, however, the tuple is upgraded in several ways,
and becomes (1222, R. Chase, diag, PPTH). Again, after the
changes the target instance does not satisfy tgd mst1 in the
standard sense.

(i) Jcan = Repcan
h (J);
(ii) Repcan
coincides with Rep when restricted to cells(J);
h
(iii) it contains a cell group gh (z) over hI, Jcan i for each
variable z ∈ x̄ ∪ ȳ.
Definition 7 [S ATISFACTION A FTER R EPAIRS (T GDS )] We
say that hI, Rep(J)i satisfies after repairs m under partial
order p,User if, whenever there is an homomorphism h of
φ(x, z) into hI, Rep(J)i, then (i) either m is satisfied by
hI, Rep(J)i in the standard sense, or (ii) Repcan
p,User Rep.
h

In both these cases, however, we still want to consider
these repairs as solutions, since they are the result of an “improvement” of values that originally satisfied the dependencies,
but were dirty. Let Rep be a repair over hI , J i. Clearly, if
hI, Rep(J)i satisfies an edg or tgd in the standard semantics,
nothing needs to be done. Otherwise, we revise the semantics
for edgs and tgds. In order to do this, given a dependency
(egd or tgd), variables x, x′ , and an homomorphism h of the
premise into hI, Rep(J)i, we want to be able to compare the
cell groups associated by h with x, x′ , to check whether one
value, say h(x), is an upgrade for h(x′ ), or vice versa.

Consider Example 1 and solution (a) in Figure 1. While tgd
mst1 is not satisfied in the standard sense – solution (a) does
not contain its canonical repair – it is satisfied after repairs
following the previous definition.
This concludes the formalization of the notion of a solution
as given in Definition 2. We are interested in solutions that
are minimal, i.e., they do not contain unneeded tuples into the
target and upgrade the initial target instance as little as possible. To quantify minimality we leverage p to decide when
one repair Rep′ strictly upgrades another repair Rep, denoted
by Rep ≺p,User Rep′ . More specifically, Rep ≺p,User Rep′ if:

Notice that a variable x may have several occurrences in a
formula. Homomorphism h maps each occurrence into a cell
of the database. We denote by cellsh (x) the set of cells in
hI, Rep(J)i associated by h with occurrences of x. Then, we
define the notion of a cell group associated by h with x, gh (x),
as the result of merging all cell groups of cells in cellsh (x).

(i) Rep p,User Rep′ , but not the other way around; or
(ii) Rep p,User Rep′ , according to id mapping hid ,
Rep′ p,User Rep, according to id mapping h′id , and h′id is
surjective while hid is not surjective.

More formally: gh (x) = hv → occ, by justi, where: (i) v =
h(x); (ii) occ (resp. just) is the union of all occurrences (resp.
justifications) of the cell groups in Rep for cells in cellsh (x);
(iii) in addition, just contains all cells in cellsh (x) that belong
to the authoritative tables in I.

Definition 8 [M INIMAL S OLUTIONS] A minimal solution for a
mapping and cleaning scenario is any solution that is minimal
wrt the partial order ≺p,User .

We can now define the notion of satisfaction after repairs.
Definition 6 [S ATISFACTION A FTER R EPAIRS (E GDS )] We
say that hI, Rep(J)i satisfies after repairs e wrt the partial
order p,User if, whenever there is an homomorphism h of
φ(x) into hI, Rep(J)i, then (i) either the value of h(xi ) and
h(xj ) are equal (standard semantics), or (ii) it is the case that
gh (xi ) p,User gh (xj ) or gh (xj ) p,User gh (xi ).

Two minimal solutions for our motivating example are
shown in Figure 1. These can be made non-minimal by
adding unneeded tuples, or unnecessary changes (like, for
example, changing the name of Dr. House to a llun Lb ).
Our semantics is a conservative extension of both the one
of mapping scenarios and of cleaning scenarios. In fact, a
cleaning scenario C corresponds to a mapping & cleaning
scenario in which S, Σt and User are absent. A mapping
scenario M is a mapping & cleaning scenario in which Σt
and Σe are standard dependencies, LLUNS, Sa and User are

Consider Example 2. Our solution is Rep = {g5 } (or g4 ,
if no user inputs are provided); egd e1 : R(x, y), c1(x, z) →
y = z is indeed satisfied after repairs by Rep; consider, in fact,
homomorphism h mapping z to v1 ; the cell group associated

238

absent and p simply states that nulls are less informative than
constants. In addition:

′
to another cell group gij
that is an immediate successor of gij
′
in the partial order: Repbij = Rep − {gij } ∪ {gij
}

Theorem 2: Every (core) solution of a mapping scenario
corresponds to a (minimal) solution of its associated mapping
& cleaning scenario, and vice versa. Similarly for (minimal)
solutions of cleaning scenarios.

Chase Step for User Inputs We say that User applies to
a group g ∈ Rep if User(cells(g)) is defined, and returns a
value that is different from val(g). We say that User refuses
Rep if User(cells(g)) = ⊥. If User refuses Rep, we mark
Rep as invalid. Otherwise, we denote by User(Rep) the repair
obtained from Rep by changing any cell group g ∈ Rep such
that User applies to g, to a new cell group gUser obtained from
g by changing val(g) to User(cells(g)).

VI.

T HE C HASE

We compute solutions for mapping & cleaning scenarios
by means of a generalized chase procedure. In essence, our
chase generates a tree of repairs by three main kind of steps:
(i) chasing egds (forward and backward) with cell groups; (ii)
chasing tgds (forward only) with cell groups; (iii) correcting
cell groups or refuting repairs by means of User.

Chase steps generate a chase tree whose root is hI , J i, and
for each valid node Rep, the children of Rep are the repairs
Rep0 , Rep1 , . . . , Repn generated by steps above. Any valid leaf
in the tree is called a result.
If the chase procedure terminates, it generates solutions,
although not necessarily minimal ones.

We fix a mapping & cleaning scenario M =
{S, Sa , T , Σt , Σe , p , User} and instances I of S ∪ Sa and J
of T . Given a (possibly empty) repair Rep of J, a dependency
d (tgd or egd) is said to be applicable to hI, Rep(J)i with
homomorphism h if h is an homomorphism of the premise of
d into hI, Rep(J)i that violates the conditions for hI, Rep(J)i
to satisfy after repairs d. Recall that, given an homomorphism
h of a formula φ(x̄) into hI, Rep(J)i, we denote by gh (x) the
cell group associated by h with variable x.

Theorem 3: Given a mapping & cleaning M =
{S, Sa , T , Σt , Σe , p , User}, instances I of S ∪ Sa and J of
T , and oracle User, the chase of hI , J i with Σt , Σe , User may
not terminate after a finite number of steps. If it terminates,
it generates a finite set of results, each of which is a solution
for M over hI , J i.
We can prove that, as soon as the tgds are non recursive,
than the chase terminates. This result is far from trivial, since,
as we discussed, egds interact quite heavily with tgds by
updating values in the database. We conjecture that this result
can be extended to more sophisticated termination conditions
for tgds [11].

Chase Step for Tgds Given an extended tgd m : ∀x, z
(φ(x̄, z̄) → ∃ȳ : (ψ(x̄, ȳ))) in Σt applicable to hI, Rep(J)i
with homomorphism h, by chasing m on hI, Rep(J)i with h
we generate a new repair Rep′ obtained from Rep, in symbols
Rep →m,h Rep′ , by:
(i) removing all cell groups gh (x), for all x ∈ x̄;

Theorem 4: Given a mapping & cleaning M =
{S, Sa , T , Σt , Σe , p , User}, instances I of S ∪ Sa and J of
T , and oracle User, if Σt is a set of weakly-acyclic tgds [2]
then the chase of hI , J i with Σt , Σe , User terminates.

(ii) adding the new cell groups in the canonical repair Repcan
h ,
i.e.: Rep′ = Rep − {gh (x)|x ∈ x̄} ∪ Repcan
.
h
Chase Step for Egds We first introduce the notions of witness
and witness variable for an egd. Let e : ∀x (φ(x) → x = x′ )
be a cleaning egd. A witness variable for e is a variable x ∈ x̄
that has multiple occurrences in φ(x̄). For an homomorphism
h of φ(x̄) into hI, Rep(J)i, we call a witness, wh for e and
h, the vector of values h(x̄w ) for the witness variables x̄w of
e. Given the tuples in Example 2, {R(1, a), R(1, b)}, and egd
R(x, y), R(x, y ′ ) → y = y ′ , x is the witness variable, and
h(x) = 1 is the witness.

Cost Managers and User Managers In terms of complexity,
it is well-known [3] that a database can have an exponential
number of repairs, even for a cleaning scenario with a single
FD and when no backward chase steps are allowed. Given
such a high level of complexity, as it is common in data
repairing, we need to devise ways to prune the chase tree
and discard some of the solutions in favor of others. In [5],
we incorporate these pruning methods into the chase process
in a principled and user-customizable way by introducing a
component, called the cost manager. Intuitively, given a chase
tree t, a cost manager is a predicate CM over the nodes for t.
For each node n in the chase tree, i.e., for each repair, it may
either select it (CM(n) = true), or discard it (CM(n) = false).

Given a cleaning egd e : ∀x (φ(x) → x = x′ ) in Σe
applicable to hI, Rep(J)i with homomorphism h, by forward
chasing e on hI, Rep(J)i with h we generate a new repair
Repf obtained from Rep by:
(i) removing gh (x) and gh (x′ );

During the chase, we shall systematically make use of the
cost manager. Whenever a chase step is discarded by the cost
manager, the corresponding branch is no longer pursued. The
trivial cost manager is the one that keeps all solutions, and may
be used for very small scenarios. Our implementation offers a
rich library of cost managers. Among these, we mention the
following (further details are available in [5]): (i) a forwardonly cost manager (FO): it accepts only forward repairs and
discards all nodes that contain backward ones; (ii) a maximum
size cost manager (S N): it accepts new branches in the chase
tree as long as the number of leaves is less than N ; (iii) a

′

(ii) adding the new cell group lubp,User (gh (x), gh (x )), i.e.:
Repf = Rep − {gh (x), gh (x′ )} ∪ lubp,User (gh (x), gh (x′ )).
By backward chasing e on hI, Rep(J)i with h we try to
falsify the premise in all possible ways. To do this, we generate
a number of new repairs as follows: for each witness variable
xi ∈ x̄w of e, and each cell cj ∈ cellsh (xi ), consider the
corresponding cell group according to Rep, gij = gRep (cj ).
If val(gij ) ∈ CONSTS and just(gij ) = ∅, generate a new
repair Repbij obtained from Rep by changing all cells in gij

239

R1 , R2 to find a tuple t that satisfies the premise; (ii) query
R3 to check that t contains a value of x that should actually
be copied to R3 ; (iii) add the new tuple to R3 ; in addition,
we also have to properly update cell groups; to do this: (iv)
for each cell associated in t with variable x, we need to query
tables occurrences and justifications to extract the cell group of
the cell, and build the a new cell group as the union of these;
(v) store the new cell group for x in tables occurrences and
justifications ; (vi) do the same for the existentially quantified
variable, y. Then, move to the next violation and iterate.

frequency cost manager (FR) that intuitively uses the frequency
of values for an attribute to decide whether to forward or
backward chase: values that are similar to the most frequent
one are forward chased, the others are bacwkard chased. Notice
that combinations of these strategies are possible, to obtain,
e.g., a FR- S 5 cost manager.
In a mapping & cleaning setting, we also plug into the
chase a second component, called the user manager. This is
a second predicate over nodes in the tree, that it is used to
decide when the chase should be stopped to request for user
inputs. There are several simple strategies to do this: (i) a
step-by-step user manager stops the chase and asks for inputs
after each new node is added to the tree; (ii) an after-llun
user manager only stops for nodes that contain lluns; (iii) a
level n cost manager stops the chase after n new levels have
been added to the tree, and so on.
VII.

It is easy to see that this amounts to perform several
thousands of queries, even for a very small database. More
importantly, we are forced to mix queries, operations in main
memory, and updates to the database, and send many single
statements to the dbms using different connections, with a
perverse effect on performance. In the next paragraphs, we
develop a number of optimizations that alleviate this problem.

S CALABILITY AND O PTIMIZATIONS
Caching Cell Groups A key optimization in order to speed
up the chase consists in caching cell groups in main memory.
This, however, has a number of subtleties. We tested several
caching strategies for cell groups. The first, straightforward
one, is a typical cache-aside, lazy loading strategy, in which a
cell group is first searched in the cache; in case it is missing,
it is loaded from the database and stored in the cache. As it
will be shown in our tests, this strategy is too slow.

A key goal of this paper is to develop a scalable implementation of the chase algorithm. To do this, we build on
optimizations for egds introduced in [5], and in particular the
notion of delta databases, a representation system conceived
to efficiently store and process chase trees. Our goal in
this paper is to introduce new optimizations that guarantee
good performance when chasing tgds, and at the same time
considerably improve performance on egds.

Greedy strategies perform better. We tried a cache-as-sor,
greedy strategy in which the first time a cell group for a step s
is requested, we load into the cache all cell groups for s, with
two queries (one for occurrences, one for justifications). This
strategy works very well for the first few steps. Then, as soon
as the chase goes on, for large databases it tends to become
slower since the main memory limit is easily reached (no cell
group is ever evicted from the cache), and some of the cell
groups need to be swapped out to the disk. Since accessing
the file system on disk is slower than querying the database,
performances degrade.

When Does the Chase Scale? Of the many variants of
the chase, the ones that scale nicely are those that can be
implemented as queries in a first-order language, and therefore
as SQL scripts. To give an example, consider the s-t tgd
R1 (x, z), R2 (x, v) → ∃y : R3 (x, y). Assume R3 is empty.
Then, as it was detailed in [12], chasing this tgd amounts to
run the following SQL statement, where sk(x) is a Skolem
term used to generate the needed labeled null:
insert into R3 select x, sk(x) from R1 ,R2 where R1 .x = R2 .x
We call this a batch chase execution. In fact, chasing s-t
tgds, or even the more general FO-rules [13] is extremely fast.
On the contrary, the chase becomes slow whenever it needs to
be executed in a violation-by-violation fashion. Unfortunately,
our chase procedure does not allow for easy batch-mode
executions, because of a crucial factor: during the chase, we
need to keep track of cell groups, and properly maintain them.
Repairing a violation for either a tgd or an egd changes the set
of cell groups, and therefore may influence other violations.

To find the best compromise between storage-efficiency
and performance, we noticed that our chase algorithm has a
high degree of locality. In fact, when chasing node s in the
tree to generate its children, only cell groups valid at step s
are needed. Then, after we move from s to its first child, s′ ,
cell groups of s will not be needed for a while. We therefore
designed a single-step, greedy caching strategy, that caches cell
groups for a single step at a time. In essence, we keep track
of the step s currently in the cache; whenever a cell group for
a different step s′ is requested, we clean the cache and load
all cell groups for s′ . Our experiments show that this brings
excellent improvements in terms of running times.

In our approach, cell-groups are stored during the chase
using two additional database tables, one for occurrences, one
for justifications. More specifically, we assign a unique id
to each node in the chase tree, i.e., to each chase step. We
represent each cell in the database as a triple: (table, tupleId,
attributeName). In addition, each cell group has a unique id.
Then table occurrences has schema (stepId, cellGroupId, cellGroupValue, table, tupleId, attributeName); table justifications
has schema (stepId, cellGroupId, table, tupleId, attributeName,
sourceValue).

Chasing Tgds in Batch Mode A second, major optimization,
consists in chasing tgds in batch mode. In essence, we want
to clearly separate updates to the dbms (that are much more
efficient when are run in batch mode), from the analysis
and update of cell groups. To do this, we use a multi-step
strategy that we shall explain using our sample tgd above. As
a first step, we update the dbms in batch mode. To avoid the
introduction of unnecessary tuples, we insert into table R3 only
those tuples that contain values that are not already in R3 , by
the following statement:

Consider now the tgd above, and assume also R1 , R2 are
target tables. Suppose our chase is at step s. In our approach,
to chase the tgd by literally following the definition of a chase
step, we need to do the following: (i) query the target to join

240

insert into R3 select x, sk(x) from R1 ,R2 where
R1 .x = R2 .x and x not in ( select x from R3 ).
Once all of the needed tuples have been inserted into the
database, we maintain cell groups. To do this, we store all
values of x that have been copied to R3 into a violations
temporary table. Then, we run the following query, that gives
us the cells for which we need to update cell groups:
select R1 .x, R2 .x, R3 .x, R3 .y from R1 , R2 , R3
where R1 .x = R2 .x and R2 .x = R3 .x
and x in (select x from violations).
We scan the result, and properly merge the cell groups. Notice
that this step is usually very fast, since we use the cache.
Finally, we update the occurrence and justifications table.

Scenarios. Based on these datasets, we generated 4 different
scenarios. For each scenario we also fixed an expected solution,
called DBexp , as follows:
(i) a mapping & cleaning scenario Hospital-Norm-MC based
on the Hospital-Norm dataset, with 3 tables, 2 tgds and 12
egds, and the standard partial order specification; the expected
instance, in this case, corresponds to the original tables;
(ii) a mapping & cleaning scenario Doctors-MC based on the
Doctors dataset, with the dependencies in Section I (a total of
4 source tables, 2 target tables, 3 tgds and 11 egds), and the
partial order specification discussed in the paper;
(iii) a cleaning scenario Hospital-Den-CL based on the
Hospital-Den dataset, with 1 table, 9 functional dependencies,

Chasing Egds in Batch Mode We also use an aggressive
strategy to chase egds. Generally speaking, violations for egds
should be solved one at a time, since they interact with each
other. Consider for example this common egd, encoding a
conditional functional dependency: R(x), S(x, y) → x = y,
where S is source table. Assume the following tuples are
present R(1), S(1, a), S(1, b). We first query the database to
find out violations using the following query:
select x, y from R, S where R.x = S.x and x <> y.
This will return two violations, the first arising from
R(1), S(1, a), the second from R(1), S(1, b). However, as
soon as we repair the first one and change R.x to a, the second
violation disappears. To see this, it is necessary to repeat the
query and realize that the result is empty.

and the standard partial order specification; the expected instance, in this case, is the original table;
(iv) a data exchange scenario Doctors-DE based on the Doctors
dataset, with the same dependencies as the mapping and
cleaning one, but no conflicts among the sources; we generated
a clean and consistent version of the source tables, and based
on those the expected instance as the core universal solution
for the given set of tgds and egds.
It remains to discuss how we fixed the expected instance for the
Doctors-MC scenario. We considered the clean and consistent
versions of the source tables used for scenario Doctors-DE,
and the core universal solution, C, of the mapping scenario.
Then, we introduced random noise and inconsistencies in the
sources, and fed them to the mapping and cleaning scenario.
We adopt as an expected solution the core universal solution
C discussed above.

Despite this, we do not want to process violations one at a
time, but rather in batch mode. During the chase, we keep track
in main memory of the cell groups that need to be maintained
to solve violations. Before writing updates to the database,
we check if the resulting set of cell groups is consistent with
each other, i.e., each cell of the database is changed only once.
As soon as we realize that a cell group is not consistent, we
discard the update and iterate the query.
VIII.

Algorithms. We run L LUNATIC with several cost managers
and several caching strategies, as discussed in Sections VI,
VII. In addition, we compared our system to several other
algorithms in the literature, as follows:
(a) an implementation of the Mimimum Cost algorithm proposed in [9] (M IN .C OST) to repair FDs and IDs, in which IDs
are repaired only by tuple insertions, and not by deletions or
modifications;

E XPERIMENTS

Experiments have been executed running the Java prototype
of the L LUNATIC system on a Intel i7 machine with 2.6Ghz
processor and 8GB of RAM under MacOS. The DBMS was
PostgreSQL 9.2.

(b) an implementation of the P IPELINE algorithm in Section
V, obtained by coupling a standard chase engine for tgds, and
the repair algorithm for FDs in [10]; for the latter, for each
experiment, we took 100 samples.

Datasets. We selected three datasets. The first two are based on
real data from the US Department of Health & Human Services
(http://www.medicare.gov/hospitalcompare/), and the third one is
synthetic.

(c) the

DEMO

system [14] chase engine for mappings.

Quality Metrics. A general and efficient algorithm to measure
the similarity of two complex databases by taking into account
foreign keys, different cell ids, and placeholders, like labeled
nulls or lluns has been recently developed in [15]. Based on
this algorithm, we report two different quality measures. The
first one is the actual similarity, sim(Rep, DBexp ), measured by
the algorithm in [15]. In the comparison, lluns are considered
as partial matches, and counted as 0.5 each.

(a) Hospital-Norm is the normalized version of the hospital
data, of which we considered 3 tables with 2 foreign keys, a
total of 20 attributes, and approximately 150K tuples.
(b) Hospital-Den is a highly denormalized version of the same
data, with 100K tuples and 19 attributes, traditionally used in
data quality experiments.
(c) Doctors, corresponds to our running example in Figure 1.

In the Hospital-Norm-MC this measure can be misleading.
There we start with a clean target database, DBclean , and
introduce random noise to generate a dirty database, DBdirty .
On average, the dirty copy is approximately 90% similar
to the clean one, and therefore all repairs will also have

Errors. In order to test our algorithms with different levels
of noise, we introduced errors in the datasets using a random
noise generator. For each datasets, we generated copies with a
number of noisy cells ranging from 5% to 10% of the total.

241

LLUNATIC-FR-S1

LLUNATIC-FR-S5

LLUNATIC-FR-S10

LLUNATIC-FR-S50

PIPELINE

MIN.COST

DEMO

15000

5000

5000

2500
sec.

2500
sec.

0

400 K

700 K

2500

5000
sec.

0

100 K

5000

10000

sec.

0

0

1000 K 100 K

400 K

700 K

1000 K

10 K

a.1: Scalability, Hospital-CL Single-Step C. a.2: Scalability, Hospital-CL Greedy Cache
1,00

40 K

70 K

K

100 K

a.3: Scalability, Hospital-CL, Lazy Cache
400

1,00

250 K

500 K

750 K

1000 K

b.1: Scalability. Doctors-DE Scenario
# of nodes

avg. sim(Rep, DBexp)
5000
0,95

0,95

2500
sec.

200

max. sim(Rep, DBexp)

0,90

min. sim(Rep, DBexp)

0,90

0

0

100 K

400 K

700 K

1000 K

5k, 6%-10%

c.1: Scalability Doctors-MC

10k, 6%-10%

25k, 6%-10%

5k, 6%-10%

c.2: Max Quality, Doctors-MC
max. rep-rate(Rep, DBexp)

5000
30%
2500
sec.

d.1: Scalability, Hospital-Norm-MC

0,05

0,04

0,03

0,02

10k, 6%-10%

0,01

0,05

0,04

0,03

0,02

5k, 6%-10%

0,01

0,05

1000 K

0,04

700 K

0,03

400 K

0,02

-30%

0

0,01

0%

100 K

25k, 6%-10%

5k, 6%-10% 0,006
10k, 6%-10%0,011
25k, 6%-10%
0,001

c.3: Min&Avg. Quality, Doctors-MC
150

60%

10k, 6%-10%

25k, 6%-10%

# of nodes

c.4: Chase Tree Size, Doctors-MC
150

100

100

50

50

0

0

5k, 6%-10% 0,006
10k, 6%-10% 0,011
25k, 6%-10%
0,001

d.2: Quality, Hospital-Norm-MC

d.3: Chase Tree Size, Hospital-Norm-MC

# of nodes

UI

2 UI

4 UI

6 UI

8 UI

10 UI

d.4: User Inputs. Hospital-Norm-MC

Fig. 3: Experimental results.

results that are significantly better than those reported for the
denormalized scenario in Experiment a, even though in this
case we are chasing tgds and egds together.

high similarity to the clean instance. In this case we report
a repair rate defined as: rep-rate(Rep, DBexp ) = 1 − (1 −
sim(Rep, DBexp ))/(1−sim(DBdirty , DBexp )). In essence, we
measure how much of the original noise a repairing algorithm
actually removed. Whenever an algorithm returned more than
one repair for a database, we calculated the maximum, minimum, and average quality.

The execution times achieved by the algorithm can be considered as a remarkable result for problems of this complexity.
They are even more surprising if we consider the size of the
chase trees that our algorithm computes, which may reach
several hundreds of nodes as reported in Figure 3.c4 . Consider
also that each node in the tree is a copy of the entire database.

Experiment a: Hospital-Den-CL We report in Figures 3.a1 –
a3 scalability results for some of our cost managers and the
different caching strategies discussed in Section VII (lazy,
greedy, and single step). The charts confirm that, due to the
locality of the chase algorithm, the single-step cache represents
the best choice in terms of performance. Further experiments
were performed with a single-step cache manager.

Figures 3.c2 –c3 report the quality achieved by the various
cost managers, in terms of the similarity to the core instance,
sim(Rep, DBexp ). L LUNATIC is the only system capable of
handling scenarios of this complexity, and therefore no baseline is available. Notice that achieving 100% quality is in some
cases impossible, since the sources have been made dirty in a
random way, and some conflicts are not even detected by the
dependencies. However, quality of the solutions is very high.
This is a consequence of the rich preference rules that come
with this scenario.

The optimizations introduced in this paper bring a dramatic
improvement in terms of performance: the chase engine is up
to four times faster than the original version reported in [5].
This is a significant achievement, since Hospital-Den is a sort
of a worst-case scenario, with a large denormalized table with
many FDs on it, and this aggravates query and update times.

Experiment d: Hospital-Norm-MC Figure 3.d1 confirms the
excellent scalability of chasing tgds and egds on normalized
databases, even with cost managers that produce multiple solutions and generate chase trees with hundreds of nodes (Figure
3.d3 ). We do not report computation times for the PIPELINE
and M IN .C OST algorithms since they were designed to run in
main memory and do not scale to large databases. Notice that
experiment Hospital-Norm-MC is faster than Doctors-MC, even
though the overall number of dependencies is similar. This is
not surprising, since scenario Doctors-MC comprises the full

Experiment b: Doctors-DE The scalability of our chase engine is confirmed in Figure 3.b1 . We compare the performance
of L LUNATIC to the data exchange chase engine DEMO on
scenario Doctors-DE. It can be seen that our implementation
is orders of magnitude faster than DEMO.
Experiment c: Doctors-MC The overall scalability of the
chase is confirmed on scenario Doctors-MC in Figure 3.c1 . In
fact, the normalized nature of the data guarantees performance

242

range of dependencies that can be handled in our framework,
including s-t tgds, while Hospital-Norm-MC relies on tgds only
to express target referential integrity constraints.

and are based on tuple deletions, not on cell changes; preferences are among tuples, not cell values. Also, they do not
consider tgds, the main challenge dealt with in our framework.

In terms of quality, we notice that finding the right repairs
for Hospital-Norm-MC is quite hard, since here we have no
preference relations, and there is very little redundancy in the
tables. In Figure 3.d2 we report metric rep-rate(Rep, DBexp )
for the three algorithms that we ran on this scenario. Two
things are apparent: L LUNATIC was able to partially repair
the dirty database, but the overall quality was lower than the
maximum one achieved in scenario Doctors-MC.

Recently, a chase procedure to infer accuracy information
represented by partial orders was devised in [25]. An integration of these ideas into our framework is left as future work.
R EFERENCES
[1]
[2]
[3]

On the contrary, both the M IN .C OST, and the PIPELINE
somehow lowered the quality. In fact, on the one side,
the M IN .C OST algorithm cannot backward repair cells. The
PIPELINE algorithm samples repairs in a random fashion and
cannot properly handle interactions among tgds and egds. As a
consequence, both algorithms manage to generate a consistent
repair, but at the cost of adding many unnecessary tuples to
the target to repair foreign keys, and this lowers their score.

[4]
[5]
[6]

[7]

We finish by mentioning Figure 3.d4 , in which we study
the impact of user inputs on the chase process. We run the
experiment for 25K tuples interactively, and provided random
user inputs by alternating the change of a llun value with the
rejection of a leaf. It can be seen that small quantities of inputs
from the user may significantly prune the size of the chase tree,
and therefore speed-up the computation of solutions.
IX.

[8]
[9]

[10]

[11]

R ELATED W ORK

There has been a host of work on both data exchange and
data quality management (see [16] and [3] for recent surveys,
respectively). Although unifying approaches have been
proposed, e.g., [9], [3], [17], these deal with specific classes of
constraints only: [9] considers inclusion and functional dependencies; [3] extends this to their conditional counterparts; and
[17] treats entity resolution together with data integration. Our
mapping & cleaning approach is applicable to general classes
of constraints and provides an elegant notion of solution.

[12]

[13]

[14]
[15]

This work is an extension of our earlier work on cleaning
scenarios [5] by accommodating for mappings, and work on
data exchange [2]. As discussed in [5], cleaning scenarios
incorporate many data cleaning approaches including [9],
[18], [6], [19], [10] and [20]. The same holds for mapping
& cleaning scenarios. Furthermore, some of the ingredients
of our scenarios are inspired by, but different from, features
of other repairing approaches (e.g., repairing based on both
premise and conclusion of constraints [18], [10], cells [10],
[9], groups of cells [9], partial orders and its incorporation
in the chase [21]). As previously observed, these approaches
support limited classes of constraints. A flexible data quality
system was recently proposed [22] which allows user-defined
constraints but does not allow tgds.

[16]
[17]

[18]
[19]
[20]
[21]

[22]

Uncertainty in schema mappings has been investigated
in [23], with reference to a different, data-integration setting.

[23]

We are not aware of any prior studies on optimizations for
the chase. The only available chase engine for data exchange
is DEMO [14], which hardly scales to large databases.

[24]

Algorithms for data repairing with preference relations
were introduced in [24]. They only consider denial constraints,

[25]

243

L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernandez, and R. Fagin,
“Translating Web Data,” in VLDB, 2002, pp. 598–609.
R. Fagin, P. Kolaitis, R. Miller, and L. Popa, “Data Exchange: Semantics
and Query Answering,” TCS, vol. 336, no. 1, pp. 89–124, 2005.
W. Fan and F. Geerts, Foundations of Data Quality Management.
Morgan & Claypool, 2012.
D. Loshin, Master Data Management. Knowl. Integrity, Inc., 2009.
F. Geerts, G. Mecca, P. Papotti, and D. Santoro, “The L LUNATIC DataCleaning Framework,” PVLDB, vol. 6, no. 9, pp. 625–636, 2013.
W. Fan, J. Li, S. Ma, N. Tang, and W. Yu, “Towards certain fixes with
editing rules and master data,” PVLDB, vol. 3, no. 1, pp. 173–184,
2010.
J. Bleiholder and F. Naumann, “Data fusion,” ACM Comp. Surv., vol. 41,
no. 1, pp. 1–41, 2008.
L. Chiticariu and W. C. Tan, “Debugging Schema Mappings with
Routes,” in VLDB, 2006, pp. 79–90.
P. Bohannon, M. Flaster, W. Fan, and R. Rastogi, “A cost-based model
and effective heuristic for repairing constraints by value modification,”
in SIGMOD, 2005, pp. 143–154.
G. Beskales, I. F. Ilyas, and L. Golab, “Sampling the repairs of functional dependency violations under hard constraints,” PVLDB, vol. 3,
pp. 197–207, 2010.
S. Greco, F. Spezzano, and I. Trubitsyna, “Stratification criteria and
rewriting techniques for checking chase termination,” PVLDB, vol. 4,
no. 11, pp. 1158–1168, 2011.
B. ten Cate, L. Chiticariu, P. Kolaitis, and W. C. Tan, “Laconic Schema
Mappings: Computing Core Universal Solutions by Means of SQL
Queries,” PVLDB, vol. 2, no. 1, pp. 1006–1017, 2009.
G. Mecca, P. Papotti, and S. Raunich, “Core Schema Mappings:
Scalable Core Computations in Data Exchange,” Inf. Systems, vol. 37,
no. 7, pp. 677–711, 2012.
R. Pichler and V. Savenkov, “DEMo: Data Exchange Modeling Tool,”
PVLDB, vol. 2, no. 2, pp. 1606–1609, 2009.
G. Mecca, P. Papotti, S. Raunich, and D. Santoro, “What is the IQ of
your Data Transformation System?” in CIKM, 2012, pp. 872–881.
M. Arenas, P. Barceló, L. Libkin, and F. Murlak, Relational and XML
Data Exchange. Morgan & Claypool, 2010.
M. Hernández, G. Koutrika, R. Krishnamurthy, L. Popa, and R. Wisnesky, “Hil: a high-level scripting language for entity integration,” in
EDBT, 2013, pp. 549–560.
G. Cong, W. Fan, F. Geerts, X. Jia, and S. Ma, “Improving data quality:
Consistency and accuracy,” in VLDB, 2007, pp. 315–326.
W. Fan, J. Li, S. Ma, N. Tang, and W. Yu, “Interaction Between Record
Matching and Data Repairing,” in SIGMOD, 2011, pp. 469–480.
M. Yakout, A. K. Elmagarmid, J. Neville, M. Ouzzani, and I. F. Ilyas,
“Guided data repair,” PVLDB, vol. 4, no. 5, pp. 279–289, 2011.
L. Bertossi, S. Kolahi, and L. Lakshmanan, “Data Cleaning and Query
Answering with Matching Dependencies and Matching Functions,” in
ICDT, 2011, pp. 268–279.
M. Dallachiesa, A. Ebaid, A. Eldawy, A. K. Elmagarmid, I. Ilyas,
M. Ouzzani, and N. Tang, “Nadeef: a commodity data cleaning system,”
in SIGMOD, 2013, pp. 541–552.
X. L. Dong, A. Y. Halevy, and C. Yu, “Data integration with uncertainty,” VLDB J., vol. 18, no. 2, pp. 469–500, 2007.
S. Staworko, J. Chomicki, and J. Marcinkowski, “Prioritized repairing
and consistent query answering in relational databases,” Ann. Math.
Artif. Intell., vol. 64, no. 2-3, pp. 209–246, 2012.
Y. Cao, W. Fan, and W. Yu, “Determining the relative accuracy of
attributes,” in SIGMOD, 2013, pp. 565–576.

Scalable Data Exchange with Functional Dependencies
Bruno Marnette1 Giansalvatore Mecca2 Paolo Papotti3
1

2

Oxford University Computing Laboratory, UK and INRIA Saclay, France
Dipartimento di Matematica e Informatica – Università della Basilicata – Potenza, Italy
3
Dipartimento di Informatica e Automazione – Università Roma Tre – Roma, Italy

ABSTRACT
The recent literature has provided a solid theoretical foundation for the use of schema mappings in data-exchange applications. Following this formalization, new algorithms have
been developed to generate optimal solutions for mapping
scenarios in a highly scalable way, by relying on SQL. However, these algorithms suffer from a serious drawback: they
are not able to handle key constraints and functional dependencies on the target, i.e., equality generating dependencies
(egds). While egds play a crucial role in the generation of
optimal solutions, handling them with first-order languages
is a difficult problem. In fact, we start from a negative
result: it is not always possible to compute solutions for
scenarios with egds using an SQL script. Then, we identify
many practical cases in which this is possible, and develop a
best-effort algorithm to do this. Experimental results show
that our algorithm produces solutions of better quality with
respect to those produced by previous algorithms, and scales
nicely to large databases.

1.

INTRODUCTION

Schema mappings are expressions that specify how an instance of a source database can be transformed into an instance of a target database. In recent years, they have received an increasing attention both from the research community and the tool market.
A schema-mapping system is used to support the process
of generating and executing mappings in practical scenarios. It typically allows users to provide an abstract specification of the mapping as a set of correspondences among
schema elements, specified through a friendly user-interface.
Based on such specification, the mapping system will first
generate a number of mappings – usually under the form of
tuple-generating dependencies (tgds) [4] that correlate source
tables with target tables; then, based on these mappings, an
executable transformation, i.e., a runtime script in SQL or
XQuery that can be practically used to run the mappings
and generate solutions.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Articles from this volume were presented at The
36th International Conference on Very Large Data Bases, September 13-17,
2010, Singapore.
Proceedings of the VLDB Endowment, Vol. 3, No. 1
Copyright 2010 VLDB Endowment 2150-8097/10/09... $ 10.00.

After the seminal Clio papers [20, 21] introduced the key
algorithmic techniques needed to generate mappings from
correspondences, and executable scripts from mappings, a
solid theoretical foundation for the data-exchange problem
was laid in the framework of data exchange research [10, 12].
More recently, sophisticated algorithms have been proposed [17, 24] to take advantage of the theoretical background
of data exchange and adopt a more principled approach to
the generation of solutions. In fact, a data-exchange problem may have many different solutions. Universal solutions [10] were first identified as preferred solutions, since
they contain only information that follows from the source
instance and the mapping. Among these, the notion of core
solution [12] was identified as the “optimal one”, since it is
the smallest among universal solutions. Second-generation
mapping systems [17, 24] made a significant step forward
by introducing algorithms that materialize core solutions by
using runtime SQL scripts.
Despite their increasing maturity, these techniques still
suffer from a significant limitation: they provide very limited
support for target dependencies. While target constraints
are recognized as an important feature of data exchange,
they introduce a number of subtleties in the computation of
solutions. Notice that target constraints typically come in
two forms: target tgds, and target equality-generating dependencies (egds) [4]. These two kinds of constraints have received an unequal share of attention in schema-mappings research. Target tgds corresponding to foreign key constraints
– by far the most common form – are handled quite nicely
by mapping systems [21, 18]: in fact, the intuition of chasing
foreign keys to generate source-to-target tgds is at the core
of the original Clio mapping-generation algorithm.
On the contrary, state-of-the-art schema-mapping systems
cannot handle target egds, i.e., there is currently no system
to efficiently generate optimal solutions for mapping scenarios with key constraints and functional dependencies, as
discussed in the next example.
Motivation Suppose we want to solve the mapping problem shown in Figure 1. This is a typical data-fusion example [5] which requires to merge together data from three
different tables – possibly from three different original data
sources – as shown in Figure 1.a: (i) a table about students
and their birthdates; (ii) a table about employees and their
salaries; (iii) a table about drivers and the cars they drive.
The target schema contains two tables, one about persons,
the second about cars. On these tables, we have two keys:
name is a key for Person, while plate is a key for Car. Based
on these requirements, it is natural to expect the generation

105

of a solution as the one shown in Figure 1.b.

Figure 1: Mapping Person Data
However, there is currently no schema-mapping tool capable of generating an SQL script that materializes this expected instance.
Notice that the required mapping can be easily expressed
as a data-exchange scenario, i.e., as a set of source-to-target
tgds that specify how data should be moved from the source
to the target, and a set of target egds that encode the required key constraints on the target, as follows. Note how
the third tgd, m3 , “invents” a value to perform a vertical
partition of the Driver source table:
m1 . ∀n, bd : Student(n, bd) → ∃Y1 , Y2 : Person(n, bd, Y1 , Y2 )
m2 . ∀n, s : Employee(n, s) → ∃Y1 , Y2 : Person(n, Y1 , s, Y2 )
m3 . ∀n, plate : Driver(n, plate) →
∃Y1 , Y2 , Z : (Person(n, Y1 , Y2 , Z) ∧ Car(Z, plate))
e1 . ∀n, b, s, c, b′ , s′ , c′ : Person(n, b, s, c) ∧ Person(n, b′ , s′ , c′ )
→ (b = b′ ) ∧ (s = s′ ) ∧ (c = c′ )
e2 . ∀p, i, i′ : Car(i, p) ∧ Car(i′ , p) → (i = i′ )
e3 . ∀i, p, p′ : Car(i, p) ∧ Car(i, p′ ) → (p = p′ )
In fact, the tgds above are exactly those that a schema
mapping tool as Clio [21] or +Spicy [18] would generate.
Formally speaking, since the desired solution is a universal
solution for the mappings, it can be materialized by chasing
the dependencies above, possibly with a post-processing step
to minimize the solution. Even though there exist chase
engines [23] that are capable of performing this task, as it
will be shown in our experimental evaluation, they hardly
scale to large databases. This is one of the reasons why
schema-mapping systems generate SQL or XQuery scripts
to perform the translation.
However, by using the script generation algorithms described in [17, 24], the best we can achieve is to generate
a pre-solution, i.e., a solution for the tgds only, as shown
in Figure 1.c. It is easy to see how the pre-solution is unsatisfactory from several points of view. In fact, it violates
the required key constraints, and therefore it is not even a
legal instance for the target. Moreover, it suffers from an
unwanted entity fragmentation effect, in the sense that information about the same entities (e.g., Jim, Mike or the
car abc123 ) is spread across several tuples, each of which
gives a partial representation of the entity. If we take into
account the usual dimensions of data quality [5], it should

be clear that such an instance must be considered of very
low quality in terms of compactness (or minimality). In fact,
on large source instances, the level of redundancy due to entity fragmentation can seriously impair both the efficiency
of the translation and the quality of answering queries over
the target database.
The reason why state-of-the-art mapping systems perform
so poorly on this example is that handling egds is a complicated task. In fact, we start from an expected negative
result: it is not possible, in general, to enforce a set of egds
using a first-order language such as SQL. This was conjectured first in [24] and is hardly surprising, since, as it can
be seen by looking at Figure 1, chasing egds has the effect
of equating values and merging tuples, effect that in general
requires the power of recursion to be implemented.
Despite this, in this paper we argue that it is actually possible to generate solutions for egds in many practical cases,
and make several important contributions towards this goal.
Contributions The main technical problem addressed in
this paper is the following: given a mapping scenario containing a set of source-to-target tgds and a set of target egds,
our goal is to generate an executable SQL script that can
be run to generate good solutions for the given scenario. To
do this:
(i) we introduce a best-effort rewriting algorithm that takes
as input a scenario with s-t tgds and egds and, whenever
this is possible, rewrites it into a new scenario without egds
that can be efficiently implemented using an SQL script;
in the paper, we show that our algorithm succeeds in many
practical cases, including the example above; a key intuition
behind the algorithm is that source constraints can be of
high value in order to generate solutions that satisfy the
required target constraints;
(ii) the rewriting takes advantage of a number of novel techniques; among these, a notion of overlap tgds, based on the
idea of chasing egds at the formula level to avoid the introduction of unneeded null values, and a sophisticated skolemization strategy; in this way, we significantly push forward
the expressibility of our SQL scripts;
(iii) then, we investigate the issue of generating optimal
solutions, i.e., core universal solutions; we show that the
rewriting algorithm to handle egds is modular in nature,
since it can be coupled with the core-computation algorithms developed in [17, 24]; this process is definitely non
trivial, due to the complex rewriting that we use for egds; in
this way, we provide a much needed extension to the corecomputation techniques in [17, 24];
(iv) finally, the techniques developed in the paper have been
implemented in the +Spicy mapping system [18]; using the
system, we provide a comprehensive evaluation of the algorithms presented in the paper, to show that they scale very
well to large databases, and that they actually generates solutions that are much more compact than those generated
by current mapping algorithms.
Applications This is the first algorithm that enables the
generation of solutions for mapping scenarios with egds in a
scalable way. To see the relevance of this achievement, consider that schema-mappings have been identified as a key
component of several classes of applications that exchange or
integrate data, for instance ETL [8], object-relational mapping [19], data fusion [5], and schema-integration [22]. It
can also be seen that key constraints – and therefore egds

106

– play a very important role in all of these applications.
Therefore, we believe that this paper represents a significant
contribution towards the goal of reaching the full maturity
of schema-mappings systems.
Outline The paper is organized as follows. Preliminary
definitions are in Sections 2 and 3. The main algorithms of
the paper are in Sections 4, 5, and 6. Experimental results
are in Section 7. Finally, a discussion of related work is in
Section 8.

2.

BACKGROUND

We fix two disjoint sets: a set of constants, const, a set
of labeled nulls, nulls. Labeled nulls are used to “invent”
values according to existential variables in tgd conclusions.
We assume the standard definition of a relational schema,
relational instance over const ∪ nulls [10], functional dependency, and homomorphism [10] between instances, as detailed in Appendix A.
Tgds and Egds Given two schemas, S and T, in the following by ∀x : φ(x) we shall denote a conjunction of atomic formulas that may contain relational atoms in S or T and equations of the form xi = xj . A source-to-target tgd (s-t tgd) [4]
is a first-order formula of the form ∀x(φ(x) → ∃y(ψ(x, y))
where φ(x) ranges over S and ψ(x, y) is a conjunction of
relational atoms over T. An equality generating dependency
(egd) [4] is a formula of the form ∀x(φ(x) → (xi = xj )).
Examples of s-t tgds and egds were provided in Section 1.
In the following, universal quantifiers will be omitted.
The Chase Given a vector of variables v, an assignment
for v is a mapping a : v → const ∪ nulls that associates
with each universal variable a constant in const, and with
each existential variable either a constant or a labeled null.
Given a formula φ(x) with free variables x, and an instance
I, we say that I satisfies φ(a(x)) if I |= φ(a(x)).
Given instances I, J, during the naive chase a tgd φ(x) →
∃y(ψ(x, y)) is fired for all assignments a such that I |=
φ(a(x)); to fire the tgd, a is extended to y by injectively
assigning to each yi ∈ y a fresh null, and then adding the
facts in ψ(a(x), a(y)) to J.
To chase an egd φ(x) → (xi = xj ) over an instance J, for
each assignment a such that J |= φ(a(x)), if a(xi ) 6= a(xj ),
the chase tries to equate the two values. We distinguish
two cases: (i) both a(xi ) a(xj ) are constants; in this case,
the chase procedure fails, since it attempts to identify two
different constants; (ii) at least one of a(xi ), a(xj ) is a null
– say a(xi ) – in this case chasing the egd generates a new
instance J’ obtained from J by replacing all occurrences of
a(xi ) by a(xj ). To give an example, consider egd e1 :
e1 . Person(n, b, s, c) ∧ Person(n, b′ , s′ , c′ )
→ (b = b′ ) ∧ (s = s′ ) ∧ (c = c′ )
On the two tuples generated by chasing the tgds, Person
(Jim, 1980, N3 , N4 ), Person (Jim, N5 , 25, 000, N6 ), chasing
the egd has two different effects:
(i) it replaces nulls by constants; in our example, it equates
N3 to the constant 25, 000, and N5 to the constant 1980,
based on the same value for the key attribute, Jim;
(ii) on the other side, the chase may equate nulls; in our
example, it equates N4 to N6 , to generate a single tuple
Person(Jim, 1980, 25, 000, N4 ).
Mapping Scenario Given a source schema S and a target
schema T, in this paper we concentrate on mapping scenarios consisting of a set of s-t tgds, Σst , and a set of target

egds Σt that correspond to functional dependencies over T.
We find it useful to consider as part of the input scenario
also a set of source egds, Σs , corresponding to source functional dependencies. In fact, as it will become apparent in
the following sections, reasoning about source dependencies
is a key component of our algorithms. In light of this, we
will often write our scenarios as M = (S, T, Σs , Σst , Σt ),
and we will assume that any source instance I is valid with
respect to Σs .
A target instance J is a solution of M and a source instance I , denoted J ∈ Sol(M, I), iff (I, J) |= Σst ∪ Σt . A
solution J is universal [10], denoted J ∈ USol(M, I), iff for
every solution K there is an homomorphism from J to K .
We call a canonical solution a solution obtained by chasing
the dependencies in Σst ∪ Σt over I . Canonical solutions are
universal solutions [10].
Given a scenario M, and an instance I, a core [12] of a
universal solution J ∈ USol(M, I), denoted C ∈ Core(M, I),
is a subinstance C ⊆ J such that there is a homomorphism
from J to C, but there is no homomorphism from J to a
proper subinstance of C. Cores of universal solutions are
themselves universal solutions [12], and they are all isomorphic to each other. It is therefore possible to speak of the
core solution as the “optimal” solution, in the sense that it
is the solution of minimal size [12].

3.

COMPUTING SOLUTIONS WITH SQL

We concentrate on mapping scenarios consisting of s-t
tgds and egds. We do not consider target tgds, since in
schema-mapping systems target tgds corresponding to foreign key constraints are typically rewritten into the s-t tgds
using the techniques in [20, 21]. To generate solutions by
means of SQL, we rewrite the original scenario as a set of
first-order rules, i.e., essentially s-t tgds with negation in the
premise and Skolem terms in the conclusion. Then, from
these dependencies, we generate the needed SQL script.
First-Order Rules Given a set of variables x, a Skolem
term over x is a term of the form t(x) = f (t1 (x), . . . , tk (x))
where f is a function symbol of arity k and {t1 (x), . . . , tk (x)}
are either universal variables in x or in turn Skolem terms
over x. Skolem terms are used to create fresh labeled nulls
on the target. Given an assignment of values a for x, with
the Skolem term above we (injectively) associate a labeled
null Nf (t1 (a(x)),...,tk (a(x))) .
Given a source schema S and a target schema T, an FOrule is a dependency of the form ϕ(x) → ψ(x) where ϕ(x)
is a first-order query over S with output tuple x and ψ is a
conjunction of atoms of the form R(t1 , . . . , tn ) over T such
that each term ti is either a variable ti ∈ {x} or a Skolem
term over x. In our setting, FO-rules will often have the
standard form φ(x) ∧ ¬φ′ (x′ ) → ψ(x), where φ(x) is a conjunctive query, and φ′ (x′ ) is a disjunction of conjunctive
queries (possibly with equalities and inequalities). We shall
call this a CQ∧¬UCQ rule. Consider the Person-Car example in the previous Section; following is an FO-rule from
its rewriting:
r2 : Student(n1 , b1 )∧Employee(n2 , s2 )∧n1 = n2 ∧¬∃n3 , p3 :
(Driver(n3 , p3 ) ∧ n1 = n3 ) → Person(n1 , b1 , s2 , f (n1 ))
O
Given a source instance I over S, a set of FO-rules ΣF
st can
be chased to generate a canonical target instance. The naive
chase straightforwardly generalizes to FO-rules (the pseudocode is detailed in the Appendix, Algorithm 1). Given a

107

source instance I, we call chase ΣF O (I) the instance obtained
st
by chasing the FO-rules. Notice that the chase of FO-rules
can be naturally implemented as an SQL script. Consider,
for example, rule r2 above. It can be implemented by the
following SQL statement:
INSERT into Person
SELECT s.n, s.b, e.s, append(‘f(’,s.n,‘)’)
FROM Student s, Employee e
WHERE s.n = e.n AND
s.n NOT IN (SELECT d.n FROM Driver d)
First-Order Implementations Given a mapping scenario
O
M, our goal is to derive a finite set of FO-rules, ΣF
st , that
represents an FO-implementation of M. From the rules, we
shall then derive an SQL script to generate solutions. However, the process of computing solutions has some subtleties.
We want to emphasize an important difference between the
original scenario and its FO-implementation.
The original scenario M includes a set of target egds Σt ;
since the chase of egds may fail, as discussed above, for
some source instances there may be no solutions. On the
contrary, an FO-implementation is simply a set of FO-rules,
O
i.e., it corresponds to a new scenario MF O = (S, T, ΣF
st )
in which target egds have been dropped, and therefore it
does not have a similar failure condition. To make things
more symmetric, we say that a set of FO-rules fails on I
with respect to a set of target constraints Σt iff it generates
a target instance J that does not satisfy the target egds, i.e.,
O
J 6|= Σt . Otherwise we say that ΣF
st succeeds on I wrt Σt .
Based on this, in order to compute solutions for the original scenario M, we do as follows: (i) we generate a set of
FO-rules that represents an implementation of M; (ii) we
translate the rules into an SQL script; (iii) we run the script
on the source instance to materialize the resulting instance,
J = chase ΣF O (I), into a temporary database; (iv) we also
st
generate a number of additional boolean queries to check if
the target egds are actually satisfied by J; if not, the script
fails and rollbacks the overall transaction (v) otherwise, we
return the output as a solution.
Completeness Ideally, we would like to be able to generate
a complete implementation for every given scenario M. We
O
call a set of FO-rules, ΣF
st , a complete FO-implementation of
M if, for every valid source instance I: (i) if M has solutions
O
on I, then ΣF
O (I) is a universal
st succeeds on I and chase ΣF
st
solution for M over I, i.e., chase ΣF O (I) ∈ USolM (I); (ii) if
st
O
M has no solutions on I, i.e., Sol(M, I) = ∅, then ΣF
st fails
on I wrt Σt .
Complete FO-implementations are desirable since they allow computing a universal solution whenever there exists
one and to identify the source instances that, together with
Σst , contradict the target egds in Σt . With this in mind,
a natural question is whether complete FO-implementations
always exist. Unfortunately, this is not the case for scenarios
with target egds. In fact, we can state the following result,
which was first conjectured in [24].1 A sketch of the proof
is reported in Appendix C.
Theorem 3.1. There is a scenario M = (S, T, Σst , Σt )
where Σt is a set of functional dependencies over T such
that no complete FO-implementation exists for M.
1
A similar result was reported in [9] for LAV mappings in data
integration systems.

To give an intuition about the intrinsic complexity of handling egds with FO-languages, consider that by progressively
equating nulls during the chase, egds may generate large
blocks of facts that are connected with each other via labeled
nulls. Intuitively, such connected component of unbounded
width in the graph of facts are impossible to capture using
a first-order language such as SQL. Notice also that this behavior does not happen without egds, since s-t tgds always
generate fact-blocks of bounded size wrt the size of tgd conclusions.
A Best-Effort Approach Theorem 3.1 above leaves us no
hope of defining an algorithm that always returns a complete
implementation for a mapping scenario with target egds. In
spite of this, we have two crucial observations: (i) first, in
practical cases many scenarios do have FO-implementations;
(ii) second, by reasoning on the source constraints we can
find good implementations for these cases. Consider again
the example above. As soon as additional constraints are
imposed on the source – as it typically happens in real-life
mapping scenarios – the emergence of critical cases becomes
less likely.
In our example, since the tgd actually performs a vertical
partition of the Driver table, we might want to consider the
source functional dependency Driver.name → Driver.plate,
stating that each driver drives at most one car, without
which the vertical partition does not make sense (a driver
that drives more than one car would cause a violation of
the key constraint on the car id). If the source functional
dependency holds, it is actually possible to generate a complete implementation, as follows. Notice how the rule makes
use of non-standard Skolem terms in order to capture the
semantics of the target egds (as discussed in Section 5):
r1 . Driver(n, p) → Person(n, f (p)) ∧ Car(f (p), p)
In the following sections, we develop a best-effort algorithm
that, given a mapping scenario M = (S, T, Σs , Σst , Σt ), generates a sound FO-implementation for M. A set of FO-rules,
O
ΣF
st , is a sound FO-implementation of M if, for every valid
O
source instance I, ΣF
succeeds on I, then chase ΣF O (I)
st
st
is a universal solution for M over I, i.e., chase ΣF O (I) ∈
st
USolM (I).
Despite the fact that our algorithm does not always return
complete implementations, it has two nice properties that
make it quite effective in order to handle scenarios with egds.
First, our solutions turn out to be complete in most practical cases. We prove that, given a sound implementation
O
ΣF
for M, it is decidable if it represents also a complete
st
implementation for M, and provide an algorithm for this
check.
O
Second, and more important, whenever ΣF
st turns out to
be complete, we can derive from it an alternative implemenO∗
tation, ΣF
, that generates core solutions for M. Notice,
st
in fact, that complete implementations return universal solutions that are not guaranteed to be minimal. Computing
core solutions adds further complexity to the problem and is
considered here as a separate step (as discussed in Section 6).
The next sections are devoted to the development of the
rewriting algorithm. Recall that chasing egds has the effect
of reducing the number of nulls in the target instance, in two
ways. The first one is by replacing one null by a constant;
the second one is by equating one null to another null. We
simulate these two effects in our FO-rules by two different
techniques:

108

(i) the first technique is concerned with discovering overlaps
among tgds in order to properly equate nulls to constants;
(ii) the second technique tries to discover an appropriate
skolemization strategy for the tgds obtained at the previous
step in order to properly equate null values.

4.

DISCOVERING OVERLAPS

Two atoms overlap when they may generate facts for a
relation having the same key and different tuple values.
More formally, given a set of atoms ϕ(x, y), we say that
two atoms R(t1 , . . . , tn ), R(t′1 , . . . , t′n ) in ϕ(x, y) generate
an overlap for a functional dependency R.hi1 . . . ik i → j if,
for each l = 1, . . . , k, either til and t′il are both universal variables, or they are the same existential variable, i.e.,
til = t′il ∈ y. Given two tgds m1 : φ1 (x1 ) → ∃y 1 (ψ1 (x1 , y 1 )),
m2 : φ2 (x2 ) → ∃y 2 (ψ2 (x2 , y 2 )), we say that they overlap
whenever there are overlaps in ψ1 (x1 , y 1 ) ∪ ψ2 (x2 , y 2 ).
Consider the Person-Car example in Section 1: it can
be seen that mappings m1 , m2 generate an overlap on the
Person atoms (in both cases the key, n, is universal). Our
intuition is that it is possible to rewrite the mappings into
a new tgd that directly generates the target atoms that are
produced by chasing the original tgds first and then the egd,
as follows:
o1 . Student(n1 , bdate) ∧ Employee(n2 , salary) ∧ n1 = n2
→ Person(n1 , bdate, salary, Y0 )
Mapping o1 above is called an overlap tgd. It is interesting
to note that the conclusion of o1 may be constructed using
the following intuitive algorithm: (i) take the conjunction
of the conclusions of the two original tgds; (ii) “chase” the
atoms according to the egd to equate existential and universal variables. In essence, we are chasing at the formula level
to incorporate the semantics of the egds into a set of new
s-t tgds that are easier to chase at the instance level.
Algorithm 3 in the Appendix generates the actual overlap
tgds. It takes as input a mapping scenario M = (S, T, Σst ,
Σt ), and generates a new set tgds, Σovl
M , by recursively chasing overlaps. In our example, besides o1 above, the algorithm will also generate the following tgds:
o2 . Student(n1 , b1 ) ∧ Employee(n2 , s2 ) ∧ Driver(n3 , p3 )∧
n1 = n2 ∧ n2 = n3 → Person(n1 , b1 , s2 , C3 ) ∧ Car(C3 , p3 )
o3 . Student(n1 , b1 ) ∧ Driver(n3 , p3 ) ∧ n1 = n3 →
Person(n1 , b1 , S3 , C3 ) ∧ Car(C3 , p3 )
o4 . Employee(n2 , s2 ) ∧ Driver(n3 , p3 ) ∧ n2 = n3 →
Person(n2 , B3 , s2 , C3 ) ∧ Car(C3 , p3 )
However, the union of the original tgds and these new tgds
is not logically equivalent to the original scenario. To see
why, consider that, by replacing variables, egds do not simply add new tuples, but typically also remove some existing ones: consider for example the following two tuples:
Person(Jim, 1980, N0 , N1 ), Person(Jim, N4 , 25, 000, N5 );
when the chase enforces the key constraint over Person, a
new tuple is generated, Person(Jim, 1980, 25, 000, N1 ), but
at the same time the original tuples are removed from the solution. Therefore, to correctly simulate the effect of egds by
means of overlap tgds, a further rewriting step is necessary:
we need to rewrite tgds so that they fire only when no overlap can be generated. To do this, we use negation in the
premise, as shown in Algorithm 4 in the Appendix. This
generates a set of CQ∧¬UCQ rules, addneg(Σst ∪ Σovl
M ).
To give an example, the procedure above rewrites tgd m1 as

follows:
m′1 . Student(n1 , b1 ) ∧ ¬(Employee(n2 , s2 ) ∧ n1 = n2 )∧
¬(Employee(n2 , s2 ) ∧ Driver(n3 , p3 ) ∧ n1 = n2 ∧ n2 = n3 )
∧ ¬(Driver(n3 , p3 ) ∧ n1 = n3 ) → Person(n1 , b1 , S1 , C1 )
By doing this, we have obtained a new scenario, with a new
set of s-t tgds, addneg(Σst ∪ Σovl
M ). This new scenario is
logically equivalent [11] to the original one, i.e., the two
scenarios have the same solutions for any source instance,
as stated by the following theorem.
Theorem 4.1. Given a scenario M = (S, T, Σst , Σt ), the
scenario Movl = (S, T, addneg(Σst ∪ Σovl
M ), Σt ) is logically
equivalent to M.
In light of this, in our rewriting we assume that, as a
first step, M is always rewritten as Movl , and that further
rewritings are performed on Movl . Note that, in the worst
case, the number of new dependencies generated by the algorithm is exponential in the size of the original tgds. In
fact, in the case of n s-t tgds that overlap on the same key,
the algorithm would generate O(2n ) overlap tgds.

5.

SKOLEMIZATION STRATEGY

Once the original s-t tgds have been rewritten wrt to overlaps, we need to find an appropriate way to skolemize existential variables in order to generate the FO-rules that will
be used as a basis for the final SQL script. Recall that the
skolemization strategy must be chosen in such a way to appropriately equate nulls. We know that this is not always
doable. In fact, the algorithm introduced in this Section
may fail.
Given a tgd, the standard skolemization strategy would
generate a Skolem term for each existential variable whose
arguments are all universal variables occurring in the tgd
conclusion. Our goal is to develop a novel skolemization
scheme capable of capturing the semantics of egds.
Determinations In order to find the right skolemization
for an existential variable, our algorithm looks for determinations for that variable. Given a tgd m : φ(x) →
∃y(ψ(x, y)), and an existential variable yi ∈ y, a determination for yi is a pair [dj , xk ], where (i) dj is a functional dependency R.hi1 . . . ik i → j in Σt ; (ii) for some atom
R(t1 , . . . , tn ) in ψ(x̄, ȳ) it is the case that {ti1 , . . . , tik } =
xk ⊆ x, and tj = y. Intuitively, a determination tells us
that, according to the egds, the value of the existential variable yi functionally depends on the values of the universal
variables in xk ; in fact, with each determination [dj , xk ] we
associate a Skolem term of the form fdj (xk ). In order to
take care of variable occurrences for which there is no relevant functional dependency, we also include the standard
determination, [(m, yi ), x], to which we associate the standard Skolem term f(m,yi ) (x).
Our algorithm is based on two main intuitions. The first
one is that determinations can be in many cases minimized
by looking at the source functional dependencies, in order
to make them more compact. The second one is that, albeit
an existential variable has multiple determinations, in many
cases it is possible to find a most general determination,
i.e., a determination such that by satisfying it, all other
determinations – i.e., all other functional dependencies –
are satisfied.
We illustrate these ideas by means of the following example, where the source table PaperAuthors lists names of

109

authors of papers along with the order in which they appear:
m. PaperAuthors(title, name, order) → Author(A, name)
∧AuthorPaper(A, P, order) ∧ Paper(P, title)
d1 : Author.2 → Author.1 d2 : Paper.2 → Paper.1
d3 : PaperAuthors.2 → PaperAuthors.3
The standard Skolem term for the existential variables is
f(m,yi ) (title, name, order ), which corresponds to the standard determination [(m, yi ), {title, name, order }]. However,
if we consider functional dependency d3 on the source, we
can minimize this as [(m, yi ), {title, name}], since the value
of variable order is functionally dependent on name.
Consider now variable A. It has two occurrences and two
determination. One occurrence is in relation AuthorPaper,
for which there are in fact no functional dependencies, and
we take the standard (minimized) determination, [(m, A),
{title, name}]. However, the second occurrence is in relation
Author, for which we have an egd, d1 . The egd suggests that
this occurrence should depend on variable name only, which
gives us a determination [d1 , {name}]. We need therefore to
reconcile the two determinations in order to uniquely identify a skolemization strategy for A. However, in this case, we
may easily see that the set of variables {name} functionally
depends on {name, title}. The intuition behind this is the
following: consider two different instantiations of the tgd m;
if the variable A has equal values whenever variable name
has equal values, then it certainly has equal values whenever
both name and title have equal values. We therefore select
[d1 , {name}] as the most general determination for A, since
enforcing it guarantees that all constraints on the variable
will be satisfied. Thus, the chosen Skolem term for A will
be fd1 (name).
Minimizing Determinations We make extensive use of
the standard implication algorithms for functional dependencies in order to reason about determinations. Recall [1]
that, given a set of functional dependencies over a relation
R, there exists a linear-time algorithm to compute the closure, Ā+ , of set of attributes Ā; Ā+ represents the set of all
attributes that functionally depend on Ā, directly or transitively.
We extend this algorithm in order to work on the universal variables of a tgd. More specifically, given a tgd
m : φ(x) → ∃y(ψ(x, y)), consider a subset xi of x. We
compute the closure, xi + of xi by the following fixpoint algorithm: (i) initialize xi + = xi ; (ii) for each source functional dependency R.hi1 . . . ik i → j such that, for some atom
R(t1 , . . . , tn ) in m, it is the case that {ti1 , . . . , tik } ⊆ xi + ,
and tj ∈ x, add tj to xi + , until a fixpoint is reached.
We say that a set of variables xj in a tgd m functionally
depends on a set of variables xi if xj ⊆ xi + . Given a set
of universal variables, xi we may also introduce a notion
of minimization, min(xi ), as any minimal subset such that
min(xi )+ = xi + .
Skolemization Given an existential variable in a tgd m,
we find all of its determinations, one for each occurrence
in m; in order to find the most general determination, if it
exists, we need a way to compare two (minimized) determinations. To do this, we introduce a partial order among
determinations, as follows. We say that a determination
[d1 , x1 ] is more general than a determination [d2 , x2 ], in symbols [d1 , x1 ] ≤ [d2 , x2 ], if there exist minimizations min(x1 ),
min(x2 ) such that min(x1 ) functionally depends on min(x2 ).
Whenever a variable has multiple determinations, our al-

gorithm looks for a greatest lower-bound according to the
partial order defined above. The pseudo-code is reported
in Algorithm 5 in the Appendix. Notice that the algorithm
may fail and return ⊥ whenever it is unable to find a most
general determination for a variable.
Consider again the Driver example in Section 3:
m3 . Driver(n, p) → Person(n, Y ) ∧ Car(Y, p)
d1 . Person.1 → Person.2
d2 . Car.2 → Car.1
In this case, variable Y has two determinations: [d1 , {n}],
and [d2 , {p}]. But, by the source functional dependency, we
know that the values of the car plate depend functionally on
those of the driver name; thus, the most general determination for Y is [d2 , {p}]. This is what the Skolem function in
the rewriting does.
If the algorithm succeeds, we have identified a sound skolemization strategy, skolemize, that takes a tgd, and associates with each existential variable the Skolem term fdj (xk )
corresponding to its most general determination, [dj , xk ].

6.

GETTING TO THE CORE

We are now ready to introduce our main results. Given a
mapping scenario, M = (S, T, Σs , Σst , Σt ), our algorithms
will either fail, or generate a set of FO-rules
O
ovl
ΣF
st = skolemize(addneg(Σst ∪ ΣM ))
FO
Our first result is that Σst is a sound implementation of M,
as stated below (sketches of the proofs are in Appendix C).
O
Theorem 6.1. ΣF
st is a sound implementation of M.
O
A natural question is whether ΣF
is also complete. It
st
is possible to prove that the property of being complete
O
is decidable, i.e., once ΣF
has been computed, we can
st
O
also establish if it is complete. Recall that ΣF
is a set
st
of CQ∧¬UCQ rules. In light of this, we have the following
important decidability result.

Theorem 6.2. The following problem is decidable: given
O
a sound FO-implementation ΣF
st of a scenario M where the
O
body of each FO-rule is in CQ∧¬UCQ, is ΣF
st complete ?
It is worth noting that sound implementations in most
cases turn out to be complete. In fact, for all examples discussed in the paper, including those used in the experimental
evaluation, we were able to generate complete implementations. Even more important, complete implementations can
be used as a basis for the generation of core solutions. Recall
that FO-implementations are guaranteed to return universal
solutions for a mapping scenario with egds, but not necessarily core ones.
We say that a set of FO-rules is a core implementation for
M if it is a complete implementation that always returns
core solutions for M. We show that the core-computation
algorithms developed for scenarios without egds [17, 24] can
be used as building-blocks to turn a complete implementaO
F O∗
tion ΣF
.
st into a core implementation Σst
Theorem 6.3. Given a complete implementation for M
it is always possible to derive a core implementation for M
The latter result is absolutely non trivial and it is based
on a sophisticated encoding of the egds in the core scenario. The main intuition is that, by relying on a comO
plete implementation ΣF
of M, for each source instance
st
it is possible to “materialize” all functional dependencies
in a universal solution. To do this, we introduce an additional relation symbol Fd for each functional dependency

110

d : Rhi1 , . . . , ik i → j in Σt ; then, for each rule φ(x̄) → ψ(x̄)
O
in ΣF
st that contains an atom R(t1 , . . . , tn ) in its conclusion,
we introduce a tgd of the form φ(x̄) → Fα (ti1 , . . . , tik , tj ).
This allows for the treatment of egds using core-oriented
rewritings that were conceived for s-t tgds only. Further
details are reported in Appendix C.

7.

EXPERIMENTAL RESULTS

The algorithms introduced in the paper have been implemented in the working prototype of the +Spicy system. In
this section we study the performance of our rewriting algorithm on mapping scenarios of various kinds and sizes.
We show that the rewriting algorithm efficiently computes
solutions of quality for scenarios with egds, even for large
databases and high numbers of tgds.
For our experiments we selected 8 scenarios, called sa , sb ,
sc , sd , s25 , s50 , s75 , s100 , as detailed in Appendix D.

Figure 2: Size Reduction in Solutions
Quality of Solutions (Figure 2) One of the main claims
of this paper is that egds significantly improve the quality
of solutions with respect to scenarios that do not take them
into consideration. To support this claim, we concentrate on
the compactness dimension of data quality. For experiments
sa –sd , we generated two different SQL scripts. One script
was generated by our rewriting algorithm to generate core
solutions for the egds. The second script was generated to
materialize a core pre-solution, i.e., a core solution for the
s-t tgds only. Differently from the first script, this solution
does not satisfies the egds. Then, we measured the sizes of
the two solutions in terms of tuples and reported the ratio.
Since egds are triggered when equal values are generated
in different target tuples, we are interested in testing how
the size ratio changes in presence of different levels of redundancy in the source instance; here, redundancy means
the probability that the same atomic value appears in more
than one tuple. For each scenario we generated two synthetic
source instances based on a pool of values of decreasing size.
This generates different levels of redundancy (20% and 40%)
in the source database.
Results are in Figure 2. It can be seen that in all experiments the egds brought a significant reduction in the size
of solutions. As it was to be expected, such reduction increases as the redundancy in the source instance increases,
since there is a higher probability that different tuples have
equal values on key attributes and can be merged together.
An obvious question is at what cost this improvement of
quality comes. In the following, we discuss the efficiency
of the rewriting algorithm along two different dimensions.
First, we study the cost of computing solutions for source
instances of increasing size. Then, we study the cost for
scenarios of increasing size.
Scalability wrt Large Source Instances (Figure 3)
To study how the algorithm performs on databases of large
sizes, we considered scenarios sa –sd and generated source

instances of increasing size (100k, 500k, and 1M tuples).
Then, we computed solutions and reported execution times
in Figure 3.
Notice that, while in this paper we restrict ourselves to
SQL, and therefore FO-logic, a number of recent works about
mappings (see, for example [15]) have undertaken the approach of coupling SQL for database access with a controlled
form of recursive control-logic implemented in a procedural
programming language, typically Java. These works show
that it is possible in some cases to achieve good scalability
even in the execution of recursive queries. Unfortunately,
this is not true in the setting we consider in this paper,
i.e., chasing egds and/or generating core solutions. To show
this, we have compared computing times of the rewriting algorithm to those of a custom chase engine for egds [23] that
uses a combination of SQL and Java to generate solutions.
We fixed a timeout of one hour. If one experiment was not
completed by that time, we stopped it.
Figure 3.a shows computing times for egd-compliant core
solutions generated by our rewriting. Figure 3.b shows computing times for core pre-solutions generated without considering egds. Finally, Figure 3.c shows computing times for
the chase engine.
It is easy to see that the chase engine hardly scales to
large databases. While execution times for the SQL scripts
scaled nicely to 1M tuples, in order to meet the timeout
with the chase engine we had to reduce the size of instances
to 1k, 5k, 10k. Even so, computing times were significantly
higher. In essence, our experiments show that, using the
custom chase engine, even simply chasing a set of egds –
let alone computing a core solution – may take quite a long
time. To see why, note that during the chase, whenever
a null is mapped to some other value, it must be replaced
everywhere in the database; this may require a high number
of queries. In essence, once the nulls have been generated,
removing them in a scalable way becomes very hard. This
confirms the impression that, whenever this is possible, the
SQL-based approach should be preferred to the actual chase
of the egds.
It is also interesting to note that execution times for the
rewriting on all scenarios were comparable to those needed
to generate the pre-solution, so that we can conclude that
the increased quality discussed in previous paragraph comes
at a very acceptable cost.

Figure 4: Execution Times for Large Scenarios
Scalability wrt Large Scenarios (Figure 4) In the
table in Figure 4, we report several values for scenarios
s25 , s50 , s75 , s100 : (a) the number of input tgds; (b) the number of input egds; (c) the number of final tgds, after overlaps
have been processed. Then, we report two different times.
Since we wanted to study how the rewriting phase scales
with the size of the scenario, we measured the time needed
to generate the SQL script. Finally, we report execution
times in seconds for source databases of 100K tuples.
As it was to be expected, the cost of the script genera-

111

Figure 3: Execution Times for Source Instances of Increasing Size
tion phase increased exponentially with the number of tgds.
This is due to the fact that the algorithm has in general to
inspect an exponential number of possible overlaps. On the
contrary, the actual script execution times remained pretty
low, even for scenario s100 , where more than 120 tgds were
processed.

8.

RELATED WORK

As discussed in the previous sections, the notion of a data
exchange problem was originally introduced in [10] and the
properties of core solutions were first studied in [12]. Sophisticated polynomial algorithms for core computation have
been given in [12] first, and then in [13, 23, 16]. These
algorithms assume that a specialized engine is used to postprocess a canonical solution, find endomorphisms and generate the core. Rewriting algorithms to generate core solutions
by means of SQL scripts have been given in [17, 24]. As it
was already discussed, these approaches are not applicable
to scenarios with target dependencies.
More recently, a rewriting algorithm for mappings that
also considers target egds [14] has been proposed. However,
in this case the purpose of the rewriting is quite different,
since it aims at optimizing and normalizing the input constraints; intuitively, the goal is to minimize the constraints
to make them easier to handle and to improve the quality
of solutions. The rewriting of [14] is therefore independent
from the one proposed in this paper and the two can be
easily combined.
The complexity of dealing with functional dependencies
has also been studied in the context of data integration,
both for LAV [9, 2] and GAV [7] mappings. In that context,
query rewriting techniques were developed to compute query
rewritings in presence of functional dependencies.
The presence of key constraints plays a key role also in
the data fusion literature [5]. However, these works adopt
a different approach: they merge data as a separate step
from the data translation and do not consider the presence
of labeled nulls (i.e., generated values).
An early attempt to partially incorporate key constraints
in mapping systems has been proposed by [6]. There, users
are supposed to provide specialized inputs so that the mapping algorithm can handle keys.
Acknowledgments Many thanks go to Vadim Savenkov and
Reinhard Pichler for making available their chase engine. The
second author would like to thank Assunta Lacerra, for her precious support during the writing of the paper. This work has been
partially funded by the Engineering and Physical Sciences Research Council (EPSRC) and the Advanced European Research
Council grant Webdam.

9.[1] S.REFERENCES
Abiteboul, R. Hull, and V. Vianu. Foundations of

[2] F. N. Afrati and N. Kiourtis. Computing Certain Answers
in the Presence of Dependencies. Inf. Systems,
35(2):149–169, 2010.
[3] B. Alexe, W. Tan, and Y. Velegrakis. Comparing and
Evaluating Mapping Systems with STBenchmark. PVLDB,
1(2):1468–1471, 2008.
[4] C. Beeri and M. Vardi. A Proof Procedure for Data
Dependencies. J. of the ACM, 31(4):718–741, 1984.
[5] J. Bleiholder and F. Naumann. Data fusion. ACM Comp.
Surv., 41(1):1–41, 2008.
[6] L. Cabibbo. On Keys, Foreign Keys and Nullable
Attributes in Relational Mapping Systems. In EDBT, 2009.
[7] A. Calı̀, D. Calvanese, G. De Giacomo, and M. Lenzerini.
Data integration under integrity constraints. Inf. Systems,
29(2):147–163, 2004.
[8] S. Dessloch, M. A. Hernandez, R. Wisnesky, A. Radwan,
and J. Zhou. Orchid: Integrating Schema Mapping and
ETL. In ICDE, 2008.
[9] O. M. Duschka and A. Y. Levy. Recursive Plans for
Information Gathering. In IJCAI, pages 778–784, 1997.
[10] R. Fagin, P. Kolaitis, R. Miller, and L. Popa. Data
Exchange: Semantics and Query Answering. TCS,
336(1):89–124, 2005.
[11] R. Fagin, P. Kolaitis, A. Nash, and L. Popa. Towards a
Theory of Schema-Mapping Optimization. In ACM PODS,
2008.
[12] R. Fagin, P. Kolaitis, and L. Popa. Data Exchange: Getting
to the Core. ACM TODS, 30(1):174–210, 2005.
[13] G. Gottlob and A. Nash. Efficient Core Computation in
Data Exchange. JACM, 55(2):1–49, 2008.
[14] G. Gottlob, R. Pichler, and V. Savenkov. Normalization
and Optimization of Schema Mappings. PVLDB,
2(1):1102–1113, 2009.
[15] T. J. Green, G. Karvounarakis, Z. G. Ives, and V. Tannen.
Update Exchange with Mappings and Provenance. In
VLDB, 2007.
[16] B. Marnette. Generalized Schema Mappings: From
Termination to Tractability. In ACM PODS, 2009.
[17] G. Mecca, P. Papotti, and S. Raunich. Core Schema
Mappings. In SIGMOD, 2009.
[18] G. Mecca, P. Papotti, S. Raunich, and M. Buoncristiano.
Concise and Expressive Mappings with +Spicy. PVLDB,
2(2):1582–1585, 2009.
[19] S. Melnik, A. Adya, and P. Bernstein. Compiling mappings
to bridge applications and databases. In SIGMOD, 2007.
[20] R. J. Miller, L. M. Haas, and M. A. Hernandez. Schema
Mapping as Query Discovery. In VLDB, 2000.
[21] L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernandez, and
R. Fagin. Translating Web Data. In VLDB, 2002.
[22] A. D. Sarma, X. Dong, and A. Y. Halevy. Bootstrapping
Pay-As-You-Go Data Integration Systems. In SIGMOD,
pages 861–874, 2008.
[23] V. Savenkov and R. Pichler. Towards Practical Feasibility
of Core Computation in Data Exchange. In LPAR, 2008.
[24] B. ten Cate, L. Chiticariu, P. Kolaitis, and W. C. Tan.
Laconic Schema Mappings: Computing Core Universal
Solutions by Means of SQL Queries. PVLDB,
2(1):1006–1017, 2009.

Databases. Addison-Wesley, 1995.

112

APPENDIX
A. DATA MODEL
In this section we recall some well-known definition about
the relational data model, taken from data-exchange literature [10].
We fix two disjoint sets: a set of constants, const, a set of
labeled nulls, nulls. We also fix a set of labels {A0 , A1 , . . .},
and a set of relation symbols {R0 , R1 , . . .}. With each relation symbol R we associate a relation schema R(A1 , . . . , Ak ).
A schema S = {R1 , . . . , Rn } is a collection of relation schemas. An instance of a relation schema R(A1 , . . . , Ak ) is a
finite set of tuples of the form R(A1 : v1 , . . . , Ak : vk ), where,
for each i, vi is either a constant or a labeled null.
An instance of a schema S is a collection of instances,
one for each relation schema in S. In the paper, we interchangeably use the positional and non positional notation
for tuples and facts; also, with an abuse of notation, we will
often blur the distinction between a relation symbol and the
corresponding instance. A ground instance is an instance I
without labeled nulls.
Given two disjoint schemas, S and T, we shall denote by
hS, Ti the schema {S1 . . . Sn , T1 . . . Tm }. If I is an instance
of S and J is an instance of T, then the pair hI, Ji is an
instance of hS, Ti.
Given a relation schema R(A1 , . . . , Ak ), a functional dependency is an expression of the form R.Ā → B̄, where Ā
and B̄ are sets of attributes in {A1 , A2 , . . . Ak }. Key constraints are functional dependencies such that B̄ = {A1 , A2 ,
. . . Ak }. As an alternative, we also write functional dependencies using the positional notation R.[i0 , . . . , in ] → j.
Given two instances J , J’ over a schema T, a homomorphism h : J → J’ is a mapping from dom(J) to dom(J’)
such that for each c ∈ const(J), h(c) = c, and for each
tuple t = R(A1 : v1 , . . . , Ak : vk ) in J it is the case that
h(t) = R(A1 : h(v1 ), . . . , Ak : h(vk )) belongs to J’. h is
called an endomorphism if J’ ⊆ J; if J’ ⊂ J it is called a
proper endomorphism.
We say that two instances J , J’ are homomorphically
equivalent if there are homomorphisms h : J → J’ and
h′ : J’ → J. Note that a conjunction of atoms may be
seen as a special instance containing only variables. The
notion of homomorphism extends to formulas as well.

B.

PSEUDO-CODE

Chasing FO-Rules Given a source instance I over S, FOrules are executed by running the naive-chase procedure to
generate a canonical target instance. The chase essentially
fires rules to generate atoms in the target whenever a rule
premise is satisfied by I, as detailed in Algorithm 1.
Algorithm 1 Chasing FO-Rules
O
a set of FO-rules, ΣF
st over hS, Ti,
an instance I of S
Output: an instance chase ΣF O (I)

Input:

st

Let chase ΣF O (I) = ∅
st
FO
For each ϕ(x) → ψ(x) ∈ Σst
Let Qϕ (I) = {a(x) | a assignment s.t.I |= ϕ(a(x))}
For each a(x) ∈ Qϕ (I)
chase ΣF O (I) = chase ΣF O (I) ∪ {ψ(a(x))}
st

st

Computing Overlap Tgds Overlap tgds are computed
by means of a chase procedure that works on formulas, as

detailed in Algorithm 2. The procedure assumes, without
loss of generality, that the s-t tgds are in normal form. A
set of tgds Σ is in normal form if for each mi , mj ∈ Σ,
(xi ∪ y i ) ∩ (xj ∪ y j ) = ∅, i.e, the tgds use disjoint sets of
variables. Given an attribute Ai and an atom R(v), we use
the notation vR(v),Ai to denote the variable associated with
attribute Ai in atom R(v).
There are a few observations in order here. First, note
that the definition of overlap above does not require that
mi and mj be distinct tgds. In fact, overlaps may occur
also among atoms in the same tgd, like, for example, in:
A(x, y, z) → R(x, N1 , z, N2 )∧R(y, N1 , N3 , z). The algorithm
needs to take care of these cases as well. Second, when an
overlap is processed, the chase algorithm also generates a
number of pre-conditions and a number of consistency conditions. Pre-conditions will be used shortly to construct the
actual overlap tgd. On the contrary, consistency conditions
we will used later on to infer a number of further constraints
that the source instance must comply with.
Algorithm 2 Chasing Formulas with Egds
a collection of atoms, R, over T,
an overlap O for atoms R(v1 ), R(v2 ) ∈ R,
and functional dependency Ā → B̄ ∈ Σt
Output: a new set of atoms chaseO (R),
a set of preconditions preconO ,
a set of consistency conditions consO

Input:

Let chaseO (R) = R
Let preconO = ∅
Let consO = ∅
For i = 0, . . . , |Ā|
Replace all occ. of xR(v1 ),Āi in chaseO (R) by xR(v2 ),Āi
preconO = preconO ∪ {xR(v1 ),Āi = xR(v2 ),Āi }
Repeat until fixpoint
For each functional dependency Ān → B̄ n ∈ Σt
For each pair of atoms R(vh ), R(vk ) in chaseO (R)
If vR(vh ),Āni = vR(vk ),Āni , for each i = 0, . . . |Ān |
For j = 0, . . . |B̄ n |
If vR(vh ),B̄jn is universal
Replace all occurrences of vR(vk ),B̄jn
in chaseO (R) by vR(vh ),B̄jn
Else
Replace all occurrences of vR(vh ),B̄jn
in chaseO (R) by vR(vk ),B̄jn
If both vR(vh ),B̄jn and vR(vk ),B̄jn are univ.
consO = consO ∪ {vR(vh ),B̄jn = vR(vk ),B̄jn }
Based on Algorithm 2, Algorithm 3 generates the actual overlap tgds. It takes as input a mapping scenario
M = (S, T, Σst , Σt ), and generates a new set tgds, Σovl
M,
by recursively chasing overlaps for tgds; to speed-up the
process while keeping it sound, overlaps are processed only
among tgds that do not have common ancestors. In order
to do this, we keep track of the provenance of overlap tgds.
We say that an overlap tgd mOi,j is derived from mi , mj .
mOh,k is transitively derived from mi if it is derived from
a tgd mh such that either mh = mi or mh is transitively
derived from mi ; in this case, mi is called an ancestor for
mOh,k .
Algorithm 4 takes care of adding the necessary negations
to generate the final set of CQ∧¬UCQ rules.
As noted above, chasing formulas to generate overlap tgds

113

Algorithm 3 Overlap Tgds

Algorithm 5 Determinations

Input: a mapping scenario M = (S, T, Σst , Σt )
Output: a set of overlap tgds Σovl
M
ovl
Let ΣM
=∅
Repeat until fixpoint
For each overlap Oi,j between mi , mj in Σst ∪ Σovl
M
Let mi : φi (xi ) → ∃y i (ψi (xi , y i ))
Let mj : φj (xj ) → ∃y j (ψj (xj , y j ))
If mi , mj do not have common ancestors
mOi,j : (φi (xi ) ∪ φj (xj )) ∧ preconOi,j →
chaseOi,j (ψi (xi , y i ) ∪ ψj (xj , y j ))
ovl
Σovl
M := ΣM ∪ {mOi,j }

a tgd m : φ(x) → ∃y(ψ(z, y)),
a set of functional dependencies Σt
Output: a mapping mgdm : y → {[di , xj ]} or ⊥

Input:

/* Step 1: Find all determinations */
For each existential variable yi ∈ y
Let detsm (yi ) = ∅
For each occurrence R.Bj : yi in some atom of m
Let fds(R, Bj ) the set of f.d. for R s.t. Bj ∈ B̄
If fds(R, Bj ) is empty
detsm (yi ) := detsm (yi ) ∪ [(m, yi ), min(z)]
Else
For each di : Āi → B̄i ∈ fds(R, Bj )
Let vars(Ā) be the variables associated with Ā
If vars(Ā) are all universal
detsm (yi ) := detsm (yi ) ∪ [di , min(vars(Ā))]
/* Step 2: Find most-general determinations */
For each existential variable yi ∈ y
If not exists glb(detsm (yi ))
return ⊥
mgdm (yi ) := glb(detsm (yi ))

Algorithm 4 Adding Negated Atoms
Input: a set of tgds Σ = Σst ∪ Σovl
M
Output: a new set of tgds addneg(Σ)
Let addneg(Σ) = ∅
For each tgd mi : φi (xi ) → ∃y i (ψi (xi , y i )) in Σ
If there is no tgd mj transitively derived from mi
addneg(Σ) := addneg(Σ) ∪ {mi }
Else
Let m′i := φi (xi ) → ∃y i (ψi (xi , y i ))
For each mj : φj (xj ) → ∃y j (ψj (xj , y j )) ∈ Σovl
M
transitively derived from mi
Add ¬(φj (xj )) to the premise of m′i
addneg(Σ) := addneg(Σ) ∪ {m′i }

may also generates a number of consistency conditions, i.e.,
additional variable equations. In fact, during the chase, it
may be the case that occurrences of a universal variable
are replaced by occurrences of another universal variable; of
course, this makes sense only in those cases in which the two
variables have the same value.
To check these potential failures as early as possible, our
algorithm also infers a number of egds that a source instance
should comply with in order to have solutions for the given
scenario. This is done quite easily: for each overlap tgd
φ(x) → ∃y(ψ(x, y)), we consider all consistency conditions
of the form vi = vj produced during the chase for that tgd,
and generate a source egd of the form φ(x) → (vi = vj ).
Most General Determinations We use the notation m :
φ(x) → ∃y(ψ(z , y)) to denote a tgd in which z is the set of
universal variables occurring in the body, with z ⊆ x. The
pseudo-code that looks for most-general determinations is
detailed in Algorithm 5.
Please note that we might also decide to reconsider the
skolemization strategy in such a way that it never fails. The
algorithm may fail during the skolemization phase whenever
it is not possible to find a most general determination for
an existential variable. As an alternative, in order to push
further our best-effort approach, we may as well decide to
pick one of the candidate determinations for that variable
and output the rewriting anyhow. The resulting implementation would still be sound, since we return a solution only
when it actually satisfies the given target egds. As an advantage, however, for some scenarios there might be specific
input instances for which a solution can be computed anyway.

C.

SKETCH OF THE PROOFS

Theorem 3.1 There is a scenario M = (S, T, Σst , Σt )

where Σt is a set of functional dependencies over T such
that no complete FO-implementation exists for M.
Proof. (Sketch) – Consider the following scenario M,
with a single tgd and a single egd:
m1 . A(x, y) → ∃N: R(x, N ) ∧ R(y, N )
d1 . R.1 → R.2
The source instance I can be interpreted as the encoding of
a directed graph G = {(n1 , n2 )|A(n1 , n2 ) ∈ I}. The final
effect of the egd on a solution J associated with I is that
of assigning the same null values to all target tuples that
originate from arcs belonging to a connected component in
G. To see this, consider the source instance I = {A(a, b),
A(b, c), A(d, e)}.
By chasing the s-t tgd we obtain a pre-solution J’ =
{R(a, N0 ), R(b, N0 ), R(b, N1 ), R(c, N1 ), R(d, N2 ), R(e, N2 )}.
Chasing the egds produces the effect of equating N0 , N1 ,
that originate from arcs in the same connected component:
J = {R(a, N0 ), R(b, N0 ), R(c, N0 ), R(d, N2 ), R(e, N2 )}.
Consider the target query Qt = {(x, y)|∃z : B(x, z)∧
B(y, z)} and let Qs be a source query capturing the certain answers of Qt , T
that is, such that for all source instance
I we have Qs (I) = {Qt (J)|J ∈ Sol(M, I)}.
Assimilating every instance I with an undirected graph,
we can check that Qs (I) consists of the pair of constants
(c1 , c2 ) such that I contain a path from c1 to c2 and therefore
Qs cannot expressed by a first-order query.
We can finally observe that every scenario M that has
O
a complete FO-implementation ΣF
enjoys the following
st
property: for every target conjunctive-query Qt there is a
first-order formula Qs capturing the certain answers of Qt
O
in M. Qs can in fact be obtained from Qt and ΣF
by
st
applying known query rewriting techniques.
It follows that M cannot have complete FO-implementations.
FO
Theorem 6.1 Σst
is a sound implementation of M.

Proof. (Sketch) – Given an instance I that satisfies Σs ,
the solution chase ΣF O (I) satisfies the original s-t tgds. To
st
see this, consider that for every tgd φ(x̄) → ∃ȳ(ψ(x̄, ȳ)) in
O
Σst , ΣF
contains a rule φ(x̄) → ψ ′ (x̄). We can observe
st

114

that for all tuple of constant c̄ we have ψ ′ (c̄) |= ∃ȳ, ψ(c̄, ȳ),
and therefore (I, chase ΣF O (I)) |= Σst .
st
O
O
If ΣF
succeeds on I, then ΣF
also satisfies Σt , and
st
st
therefore is a solution for M. To show that chase ΣF O (I) is
st
O
universal, consider that, by construction of the rules in ΣF
st ,
it only contains sound information. In fact, let F be the set
FO
of function symbols occurring
V in Σst ; consider the secondorder formula ΓM = ∃F, r∈ΣF O ∀x̄ φr (x̄) → ψr (x̄). The
st
property of soundness follows from the fact that this formula
ΓM is (by construction) logically implied by Σs ∧ Σst ∧ Σt .
Therefore, chase ΣF O (I) is a universal solution.
st

Theorem 6.2 The following problem is decidable: given a
O
sound FO-implementation ΣF
of a scenario M where the
st
O
body of each FO-rule is in CQ∧¬UCQ, is ΣF
st complete ?
Proof. (Sketch) – Given a source instance I such that
I |= Σs we say that I is solvable iff there is a solution for M
and I (in which case there also exists a universal solution for
M and I). Notice that we can easily decide whether a given
instance I is solvable (for instance: by using the standard
chase procedure).
O
A sound FO-implementation ΣF
is complete iff for all
st
solvable I we have chase ΣF O (I) |= Σt . The key idea of the
st
proof is then to show that we have the following small-model
property:
O
Lemma C.1. If a sound FO-implementation ΣF
is not
st
complete, then there exists a small solvable I0 of size ||I0 || ≤
2 · ||M|| such that chase ΣF O (I0 ) 6|= Σt .
st
O
To prove this, let ΣF
st be a sound FO-implementation of
FO
M and assume that Σst is not complete. Let I be a (possibly large) solvable instance such that J = chase ΣF O (I) 6|=
st
Σt . Let (A1 , A2 ) be a pair of atoms in J violating a functional dependency, that is, {A1 , A2 } 6|= Σt .
For each atom Ai ∈ {A1 , A2 } we can find a rule ri : φi (x̄)∧
¬φ′i (x̄) → ψi (x̄) and a tuple āi such that: Ai ∈ ψi (āi ),
I |= φi (āi ) and I 6|= φ′ (āi ).
Since φ1 and φ2 are two conjunctive queries we can easily
construct a subinstance I0 ⊆ I such that I0 |= φ1 (ā1 )∧φ2 (ā2 )
while ||I0 || ≤ ||φ1 || + ||φ2 ||. Since φ′1 and φ′2 are two union
of conjunctive queries, and I0 ⊆ I, we have I0 6|= φ′1 (ā1 )
and I0 6|= φ′2 (ā2 ). Therefore, {A1 , A2 } ⊆ chase ΣF O (I0 ) and
st
chase ΣF O (I0 ) 6|= Σt .
st
In only remains to observe that solvability is monotonic:
since I is solvable and I0 ⊆ I, the instance I0 is also solvable,
and the Lemma is proven.
Based on the small-model property above, we can finally
O
decide whether ΣF
st is complete by (i) enumerating all the
possible source instances I of size ||I|| ≤ 2 · ||M|| (there is
only an exponential number of them up to isomorphism), (ii)
selecting the ones that are solvable, and finally (iii) testing
whether for each remaining solvable instance I it is the case
that chase ΣF O (I) |= Σt .
st

Before getting to the proof of Theorem 6.3, we need to
introduce some preliminary notions. In the following, for
the sake of space, we will simplify the notation as follows: a
O
set of FO-rules, ΣF
st , will be denoted simply by R; given a
source instance I, the canonical instance chase ΣF O (I), will
st
be denoted simply by R(I).
Given a scenario M = (S, T, Σs , Σst , Σt ) and a complete
FO-implementation R for M, Algorithm 6 takes care of generating a core implementation R∗ for M.
Our algorithm is the composition of two sets of FO-rules.
As a first step, we introduce an additional set of relation

symbols, F, containing one relation Fdi for each functional
dependency di ∈ Σt . Intuitively, based on the complete implementation of M provided by R, these relations are used
to “materialize” the extent of each functional dependency
in an instance R(I). This is done by a first set of rules, RF .
The second set of rules is a core-rewriting, RC . In order
to compute such core rewritings we shall use as a building
block one of the algorithms introduced in [17, 24]. These
algorithms, however, assume a linear order on the active
domain of the source. For example, the algorithm in [24]
assumes that dependencies have premises in FO< and may
contain inequalities < between variables. Since the coreenforcing script will be composed with another set of FOrules, we need to properly define the behavior of a set of
rules R over a non-ground instance I. To do this, we first
extend the partial order on const to Skolem terms in the
natural way by assuming a fixed order <F on the function
symbols. Then, we assume that the canonical solution R(I)
is obtained by chasing the rules in the usual way.
We obtain our core implementation, R∗ , by composing
RF and RC . In fact, we can state the following composition lemma: for every pair of FO-rules, Ra and Rb , we can
compute a set of FO-rules Rab such that, for all ground instances I we have Rab (I) = Rb (Ra (I)). Notice, however,
that this is not even necessary in practice. In fact, in order
to generate the final SQL script, it is sufficient to execute
the two scripts in sequence: first the script derived from RF ,
and then and the one derived from RC on the intermediate
result generated by the first.
It is worth noting that, as a preliminary step, Algorithm 6
rewrites the s-t tgds in M by adding overlap tgds, i.e., it generates a new, logically equivalent scenario M′ = (S, T, Σs ,
Σst ∪ Σovl
st , Σt ). This step has no cost, since it has already
been performed to generate the complete implementation
R, as discussed in Section 4, with a main difference: to
make the resulting dependencies more manageable for corecomputation purposes, here we do not add negated atoms
to tgd premises. This choice is justified by two observations:
(i) in this way, the core rewriting is drastically simplified;
(ii) the needed negations will be added anyway by the corerewriting algorithm.
Theorem 6.3 Given a complete implementation M it is
always possible to derive a core implementation for M
Proof. (Sketch) Consider the set of rules R∗ defined in
Algorithm 6. We now show that, given a solvable source
instance I for M, J = R∗ (I) is a core solution in Core(M, I).
Part 1: R∗ (I) is a universal solution – R∗ (I) ∈ USol(M, I)
It is clear that J = R∗ (I) is a pre-solution, i.e., (I, R∗ (I)) |=
Σst and we can also check that R∗ (I) is sound, meaning
that for all K ∈ Sol(I, M) there is an homomorphism from
R∗ (I) to K. To show that R∗ (I) is a universal solution it
only remains to prove that R∗ (I) |= Σt .
We define the set N of affected nulls as the smallest set
of nulls such that, for every d : Rhi1 , . . . , ik i → j in Σt and
every atom R(t1 , . . . , tn ) in J, if {ti1 , . . . , tik } ⊆ (N ∪const)
then tj ∈ (N ∪ const). We say that an atom R(t1 , . . . , tn )
of J is affected when there exists some d : Rhi1 , . . . , ik i → j
in Σ such that {ti1 , . . . , tik } ⊆ (N ∪ const).
Intuitively, the steps 2 and 4 of the algorithm ensure that
Σt is satisfied by every pair of affected atoms in J while the
steps 3 and 5 ensure that Σt is satisfied by the remaining
atoms.
Consider Σ′′st = addneg(Σ′st ) = addneg(Σst ∪ Σovl
st ),
and the set of FO-rules R′′ obtained from Σ′′st by replacing every existential variable in the head of a tgd by its

115

Algorithm 6 Core Computation
a scenario M = (S, T, Σs , Σst , Σt ),
a complete FO-implementation R for M
Output: a core implementation R∗ for M

Input:

/* Step 1: Generate the additional schema F */
Let F be the schema {Fd | d : Rhi1 , . . . , ik i → j ∈ Σt },
where Fd is a fresh predicate symbol of arity k + 1
/* Step 2: Generate RF from S to F */
Let RF := ∅
For every φ(x̄) → ψ(x̄) in R, every d : Rhi1 , . . . , ik i → j
in Σt and every atom R(t1 , . . . , tn ) in ψ
RF := RF ∪ {φ(x̄) → Fd (ti1 , . . . , tik , tj )}.
/* Step 3: Generate Σ′st by adding overlap tgds to Σst */
Let Σ′st := Σst ∪ Σovl
st
/* Step 4: Generate ΣF
st from (S ∪ F) to T */
′
Let ΣF
:=
Σ
st
st
Repeat until fixpoint
For each r : ∀x̄ φ(x̄) → ∃y, z̄, ψ(x̄, y, z̄) in ΣF
st , each d :
Rhi1 , . . . , ik i → j in Σt , and each atom R(t1 , . . . , tn ) in
ψ such that tj = y and {ti1 , . . . , tik } ⊆ {x̄}
Replace r in ΣF
st by the rule
r′ : ∀x̄, y Fd (ti1 , . . . , tik , y) ∧ φ(x̄) → ∃z̄ ψ(x̄, y, z̄).
/* Step 5: Generate a core rewriting RC for ΣF
st */
Considering the scenario MF =((S∪A), T, ∅, ΣF
st , ∅), use
the algorithm in [24] (or that in [17]) to generate a core
implementation RC for MF .
/* Step 6: Generate a core implem. R∗ for M */
Generate the set of rules R∗ as the composition of RF and
RC such that, for every ground instance I of S we have
R∗ (I) = RC (I ∪ RF (I)).

standard skolemization. We can observe that the instance
J = R∗ (I) is contained (up to isomorphism) in the instance
J ′′ = R′′ (I ∪ RF (I)). Consider now d : Rhi1 , . . . , ik i → j in
Σt and two atoms R(t1 , . . . , tn ) and R(t′1 , . . . , t′n ) in J ′′ such
that (ti1 , . . . , tik ) = (ti′1 , . . . , ti′k ). If these atoms are (both)
affected in J ′′ , we can check that RF (I) contains exactly
one atom of the form Fd (ti1 , . . . , tik , t′′ ). Then, the term tb
and t′b are both equal to t′′ and d is satisfied.
Otherwise, none of these two atoms is affected and there
is some l ∈ {1, . . . , k} such that tl is of the form fm,y h. . .i
for some rule m of R′′ and some existential variable y in the
head of R′′ . The two atoms have therefore been produced
by the same rule of R′′ and it follows from the definition of
overlaps (and in particular, the use of the chase) that d is
satisfied.
A crucial observation here is that, even though we refer
to addneg in this proof, it is not needed to (and preferable
not to) use addneg in the algorithm because the step of
core computation already takes care of preventing the introduction of unnecessary atoms in J that may violate Σt .
As an example, consider Σst = {m : S(x, y) → ∃U, V, W,
R(x, U, V )∧R(y, U, W )∧T (V, W )} and Σt = {d : Rh1, 2i→3}.
The overlap algorithm produces a new tgd m′ : S(x′ , x′ ) →
∃U ′ , V ′ , R(x′ , U ′ , V ′ ) ∧ T (V ′ , V ′ ) and addneg adds the inequality constraint x 6= y to the body of m. After skolemizing the existential variables in a standard way we obtain
R′′ such that, for all instance I0 of {S}, R′′ (I0 ) |= d. Then
for every core solution J0 for ({A}, {R, T }, {m, m′ }) and I0 ,
since J0 is contained in R′′ (I0 ) up to isomorphism, we have
also J0 |= d.

When the instance I ′ = I ∪ RF (I) is ground, the step 4 of
the algorithm ensures that the universal solution J = R∗ (I)
is a core and therefore J ∈ Core(M, I). The difficulty comes
from the fact that I ′ generally contains some nulls (the
Skolem terms introduced by RF ). We can show however
that the nulls of I ′ that are used by RC can safely be treated
as constants with respect to core computation. More precisely, if we let N be the set of affected nulls (as defined in
Part 1) we can observe that N coincides precisely with the
set of nulls occurring both in I ′ and J. We can then check
that for every homomorphism h : J → J and every ni ∈ N
we necessarily have h(n1 ) = ni . This property can finally
be proven by induction after observing that, for every atom
a = R(t1 , . . . , tn ) in J and every d : Rhi1 , . . . , ik i → j in Σt ,
if h is the identity on {ti1 , . . . , tik } then h is also the identity
on {tj } (because {a, h(a)} ⊆ J and J |= d).

D.

DESCRIPTION OF SCENARIOS

For our experiments we selected 8 scenarios. Of these, 3
(denoted as sa , sb , sc ) are variants of scenarios taken from
the literature (one from [12], one from [13], one from [3]).
The fourth one (sd ) was explicitly constructed in order to
test the behavior of the rewriting in case of an exponential
number of overlaps. For these experiments, the number of
tgds varies between 4 and 10, and the number of egds varies
between 5 and 13.
Four additional synthetic scenarios were used to test the
scalability of the algorithm with respect to larger number
of relations and dependencies. Using the scenario generator
developed for STBenchmark [3], we generated four relational
scenarios (s25 , s50 , s75 , s100 ) containing 20/50/75/100 tables,
with an average join path length of 3, variance 1. To generate complex schemas we used a composition of basic cases
with an increasing number between 1 and 15, in particular
we used: Vertical Partitioning (3/6/11/15 repetitions), Denormalization (3/6/12/15), and Copy (1 repetition). With
such settings we got schemas varying between 11 relations
with 3 joins and 52 relations with 29 joins. The number
of tgds varies between 22 and 93, and the number of egds
between 25 and 100 (corresponding to one key for each relation).
All experiments have been executed on a Intel Core 2 Duo
machine with 2.4Ghz processor and 2 GB of RAM under
Linux. The DBMS was PostgreSQL 8.3.
By looking at Figures 3.a and 3.b it is possible to see that
times for scenario sc were strongly in favor of the rewriting. The reason for this is related to the Skolem minimization procedure. Scenario sc includes existential variables for
which the standard Skolem terms depends on 10 different
universal variables; this means that, for source instances on
which such variables may assume rather long values, the corresponding Skolem string may become rather large. It turns
out that generating many large Skolem strings is often a
bottleneck in the execution of the SQL script. This is probably due to the fact that appending strings is not always a
very optimized operation in a DBMS. As a consequence, the
generation of the pre-solution is rather slow. The rewriting
does not incur this cost, since the skolemization algorithm
minimizes Skolem terms, so that they depend on a single
variable (representing the key value for the corresponding
tuple); this generates much shorter strings that are manipulated more efficiently by the engine. It remains an open
problem to find alternative encodings for Skolem terms that
alleviate these problems.

Part 2: R∗ (I) is a core solution – R∗ (I) ∈ Core(M, I)

116

Information Systems 37 (2012) 677–711

Contents lists available at SciVerse ScienceDirect

Information Systems
journal homepage: www.elsevier.com/locate/infosys

Core schema mappings: Scalable core computations
in data exchange$
Giansalvatore Mecca a,n, Paolo Papotti b, Salvatore Raunich c
a

Dipartimento di Matematica e Informatica, Universita della Basilicata, viale dell’Ateneo Lucano 10, I-85100 Potenza, Italy
Universita Roma Tre, Roma, Italy
c
University of Leipzig, Leipzig, Germany
b

a r t i c l e i n f o

abstract

Article history:
Received 7 March 2011
Received in revised form
22 March 2012
Accepted 26 March 2012
Recommended by: L. Wong
Available online 3 April 2012

Research has investigated mappings among data sources under two perspectives. On
the one side, there are studies of practical tools for schema mapping generation; these
focus on algorithms to generate mappings based on visual speciﬁcations provided by
users. On the other side, we have theoretical researches about data exchange. These
study how to generate a solution – i.e., a target instance – given a set of mappings
usually speciﬁed as tuple generating dependencies. Since the notion of a core solution
has been formally identiﬁed as an optimal solution, it is very important to efﬁciently
support core computations in mapping systems. In this paper, we introduce several
new algorithms that contribute to bridge the gap between the practice of mapping
generation and the theory of data exchange. We show how, given a mapping scenario, it is
possible to generate an executable script that computes core solutions for the corresponding data exchange problem. The algorithms have been implemented and tested using
common runtime engines to show that they guarantee very good performances, orders of
magnitudes better than those of known algorithms that compute the core as a postprocessing step.
& 2012 Elsevier Ltd. All rights reserved.

Keywords:
Schema mappings
Data exchange
Core computation

1. Introduction
Integrating data coming from disparate sources is a crucial
task in many applications. An essential requirement of any
data integration task is that of manipulating mappings
between sources. Mappings are executable transformations – say, SQL or XQuery scripts – that specify how an
instance of the source repository should be translated into
an instance of the target repository. We may identify two
broad research lines in the literature.

$
Portions of this paper have appeared under the title Core Schema
Mappings in the Proceedings of the ACM SIGMOD 2009 Conference.
n
Corresponding author. Tel. þ 39 0971 205855;
fax. þ 39 0971 205897.
E-mail addresses: giansalvatore.mecca@gmail.com,
giansalvatore.mecca@unibas.it (G. Mecca).

0306-4379/$ - see front matter & 2012 Elsevier Ltd. All rights reserved.
http://dx.doi.org/10.1016/j.is.2012.03.004

On the one side, we have studies on practical tools and
algorithms for schema mapping generation. In this case, the
focus is on the development of systems that take as input
an abstract speciﬁcation of the mapping, usually made of
a bunch of correspondences between the two schemas,
and generate the mappings and the executable scripts
needed to perform the translation. This research topic was
largely inspired by the seminal papers about the Clio
system [26,27]. The original algorithm has been subsequently extended in several ways [17,5,2,29,7] and various tools have been proposed to support users in the
mapping generation process. More recently, a benchmark
has been developed [1] to compare research mapping
systems and commercial ones.
On the other side, we have theoretical studies about
data exchange. Several years after the development of
the initial Clio algorithm, researchers have realized that

678

G. Mecca et al. / Information Systems 37 (2012) 677–711

a more solid theoretical foundation was needed in order
to consolidate the practical results obtained on schema
mapping systems. This consideration has motivated a rich
body of research in which the notion of a data exchange
problem [12] was formalized, and a number of theoretical
results were established. In this context, a data exchange
setting is a collection of mappings – usually speciﬁed
as tuple generating dependencies (tgds) [4] – that are given
as part of the input; therefore, the focus is not on the
generation of the mappings, but rather on the characterization of their properties. This has brought to an elegant
formalization of the notion of a solution for a data
exchange problem, and of operators that manipulate
mappings in order, for example, to compose [14] or invert
[11,3] them.
However, for a long time, these two research lines have
progressed in a rather independent way. To give a clear
example of this, consider the fact that there are many
possible solutions for a data exchange problem. A natural
question is the following: ‘‘which solution should be
materialized by a mapping system?’’ A key contribution
of data exchange research was the formalization of the
notion of core [13] universal solution, which was identiﬁed as the ‘‘optimal’’ solution for a data exchange scenario. Informally speaking, the core universal solution has
a number of nice properties: it is ‘‘irredundant’’, since
it is the smallest among the solutions that preserve the
semantics of the exchange, and it represents a ‘‘good’’
instance for answering conjunctive queries over the target
database. It can therefore be considered a natural requirement for a schema mapping system to generate executable scripts that materialize core solutions.
Unfortunately, there is yet no schema mapping generation algorithm that natively produces executable scripts that
compute the core. On the contrary, the solution produced
by known schema mapping systems – called a canonical
solution – typically contains quite a lot of redundancy. This
is partly due to the fact that computing cores is a challenging task.
A possible approach to the generation of core solutions
for a relational data exchange problem is the following:
(i) ﬁrst, to generate a canonical solution by chasing the
source-to-target tgds; to do this, a mapping system
typically generates an SQL or XQuery script that performs
this step very efﬁciently, even on large source instances;
(ii) then, to apply a post-processing algorithm for core
identiﬁcation.
Several polynomial algorithms have been identiﬁed to
this end [13,18]. These algorithms provide a very general
answer to the problem of computing core solutions for a
data exchange setting. Also, an implementation of the
core-computation algorithm in [18] has been developed
[30] by using a combination of SQL for database access
and a controlled form of recursive control-logic implemented in Java.
Although polynomial, experience with these algorithms shows that they hardly scale to large mapping
scenarios. In fact, they exhaustively look for endomorphisms inside the canonical universal solution in order to
identify which null values and which tuples can be
removed. This kind of computation can take very high

computing times, even on databases of a few thousand
tuples, as shown in our experiments.
This paper makes several important contributions
towards the goal of making the computation of core solutions a scalable functionality of mapping systems. More
speciﬁcally:

 given a mapping scenario consisting of source-to-





target tgds, we introduce a rewriting algorithm that
generates a new set of dependencies that can be used
to generate core solutions for the original tgds; these
dependencies can be translated into an SQL script
and ran inside any conventional database engine,
thus achieving a very high degree of ﬂexibility and
performance;
the algorithm has been implemented into the þSPICY
mapping system; in the paper, we conduct an experimental evaluation on large mapping scenario that
conﬁrms the scalability of our solution;
the rewriting algorithm is based on a new characterization of the core, in terms of witness blocks and
expansions; we introduce these notions and show
how they represent a natural tool for the rewriting of
the given tgds.

The algorithms developed in this paper concentrate on
mapping scenarios made of source-to-target tgds only.
However, as it will be discussed in Section 2.1, they represent
an essential building block for more general algorithms that
handle larger classes of constraints.
2. Overview
Consider the mapping scenario informally described in
Fig. 1, where also a source instance is shown. The source
database contains tables about books coming from three
different data sources, namely the Internet Book Database
(IBD), the Library of Congress database (LOC), and the
Internet Book List (IBL).
The desired mapping can be expressed using the
following set of tuple-generating dependencies (tgds):
m1 :8t,p : LOCðt,pÞ-(I : Bookðt,IÞ4PublisherðI,pÞ
m2 :8t,id : IBLBookðt,idÞ-Bookðt,idÞ
m3 :8id,p : IBLPublisherðid,pÞ-Publisherðid,pÞ
m4 :8t : IBDBookðtÞ-(N : Bookðt,NÞ
It can be seen how each source has a slightly different
organization wrt the others. In particular, the IBD source
contains data about book titles only; mapping m4 copies
titles to the Book table in the target. The LOC source
contains book titles and publisher names in a single table;
these are copied to the target tables by mapping m1,
which also ‘‘invents’’ a value to correlate the key and the
foreign key. Finally, the IBL source contains data about
books and their publishers in separate tables; these data
are copied to the target by mappings m2 ,m3 ; note that in
this case we do not need to invent any values.

G. Mecca et al. / Information Systems 37 (2012) 677–711

679

Fig. 1. Mapping bibliographic references.

These expressions materialize the target instance in
Fig. 1, called a canonical universal solution. While this
instance satisﬁes the tgds, still it contains many redundant tuples, those with a gray background. In fact, after
removing these tuples from the target we still have a
solution, called the core universal solution, since it is the
smallest among the solutions that preserve the semantics
of the mappings. The core of the solution in Fig. 1 is in fact
the portion of the target tables with a white background.
The main intuition behind our approach is that it is
much more efﬁcient to prevent the generation of redundant tuples in a solution, than trying to remove them after
they have been generated by the chase. Following this
intuition, given a set of s-t tgds Sst , our goal is to rewrite
them as a new set of dependencies S0st , such that, for any
source instance I, chasing S0st yields the core universal
solution for Sst and I.
Witness blocks and expansions: In order to achieve our
goal, we need to analyze the given tgds in order to
recognize when one of them may generate redundant
tuples in the target. We introduce two key concepts in
order to do this, namely the notions of witness blocks and
expansions. Intuitively, a witness block is a set of facts that
guarantee that a tgd is satisﬁed for some assignment of
constants to the universal variables.
Let us restrict our attention to tgd m4. The premise of
m4 is satisﬁed by the source atom IBDBook(The Hobbit)
in I, and generates a target tuple t 1 ¼ BookðTheHobbit,N1 Þ.
We call this tuple a witness block for tgd m4 and source
atom IBDBook(The Hobbit), since it guarantees that I and J
satisfy m4. However, by looking at the canonical solution J,
we may see that there are alternative witness blocks for
the same tgd and source atom. Consider for example tuple
t 2 ¼ BookðTheHobbit,245Þ: it also guarantees that tgd m4 is
satisﬁed wrt the source atom IBDBook(The Hobbit). For
core computation purposes, this second tuple t2 is ‘‘preferable’’ with respect to the t1 since it contains less nulls,
and therefore it makes t1 redundant.
This notion can be formalized by saying that there is
an homomorphism from t1 to t2. A homomorphism, in this
context, is a mapping of values that transforms t1 into t2.
The presence of such homomorphisms means that the
solution in Fig. 1 has an endomorphism, i.e., a homomorphism into a sub-instance—the one obtained by removing all
redundant tuples. The core is the smallest endomorphic
image of a universal solution.

Witness blocks are at the foundations of our algorithm.
In fact, an important property of witness blocks is that
they can be captured by a set of ﬁrst-order queries on the
target database called expansions. Given a tgd m, expansions are formulas that express all possible ways of
satisfying the tgd. The base expansion of each tgd is the
tgd conclusion. Other expansions can be constructed by
combining atoms in other tgds. In our example, we may
say that, besides the base expansion Eb4 ¼ Bookðt,NÞ, tgd
m4 also has an alternative expansion, E0 ¼ Bookðt,idÞ based
on the conclusion of m2.
More interestingly, by looking at these two formulas,
we can easily recognize that facts generated by E0 make
redundant those generated by Eb4 . With an abuse of
notation, we consider the two formulas as (sets of) tuples,
with existentially quantiﬁed variables that correspond to
nulls; it can be seen that Eb4 can be mapped into E0 by the
following mapping of variables: t-t, N-id, which we call
a formula homomorphism. This gives us a nice necessary
condition to intercept possible redundancy. Note that we
are checking for the presence of homomorphisms among
formulas, i.e., conclusions of tgds, and not among instance
tuples; since the number of tgds is typically much smaller
than the size of an instance, this task can be carried out
quickly.
In this simple case, which we call a subsumption, the
conclusion of a tgd, m4, is mapped into the conclusion of
another one, m2. In general, it is possible to build expansions for a tgd by combining atoms from different tgds.
Consider for example tgd m1. Besides its base expansion,
Eb1 ¼ (I : Bookðt,IÞ4PublisherðI,pÞ, there is a second expansion E00 ¼ Bookðt,idÞ,Publisherðid,pÞ, obtained by joining
atoms from m2 and m3. Also in this case we may ﬁnd a
homomorphism of the base expansion into E00 . We call this
a coverage of m1 by m2 and m3. In both cases, the second
expansion is preferable to the base one in terms of the
witness blocks it generates, because it uses less existential
variables.
Source rewritings and negations: A second important
intuition is that whenever we identify for a given tgd an
expansion preferable to the base expansion, we may
prevent the generation of redundant tuples in the target
by executing the tgds according to the following strategy:
(i) generate witness blocks for the preferred expansions;
(ii) use the base expansion to generate only those witness
blocks that actually add some new content to the target.

680

G. Mecca et al. / Information Systems 37 (2012) 677–711

In order to turn this intuition into an actual rewriting
strategy, we notice that, while expansions are formulas
over the target database, they can be easily rewritten as
formulas over the source. Intuitively, the source rewriting
of an expansion E states a condition on the source
database to realize the witness blocks associated with E
in the target. In our example, expansions are rewritten as
follows:
sourceRew ðBookðt,idÞÞ ¼ IBLBookðt,idÞ
sourceRew ðBookðt,idÞ,Publisherðid,pÞÞ
¼ IBLBookðt,idÞ,IBLPublisherðid,pÞ
Once expansions have been rewritten over the source, we
may rewrite the original tgds by adding negations to their
premises. More speciﬁcally, whenever an expansion E is
preferable to the base expansion of tgd m, we rewrite its
premise by adding the negation of the source rewriting of
E. In our example, this gives us the following rules that
represent a core schema-mapping for the original scenario,
since they can be executed on a source instance to
generate the core universal solution:
r 1 : 8x1 ,x2 : LOCðx1 ,x2 Þ4:ð(x4 ,x5 : IBLBookðx1 ,x4 Þ4IBLPublisher
ðx5 ,x2 Þ4x4 ¼ x5 Þ-Bookðx1 ,f ðx1 ,x2 ÞÞ4Publisherðf ðx1 ,x2 Þ,x2 Þ:
r 2 : 8x3 ,x4 : IBLBookðx3 ,x4 Þ-Bookðx3 ,x4 Þ:
r 3 : 8x5 ,x6 : IBLPublisherðx5 ,x6 Þ-Publisherðx5 ,x6 Þ:
r 4 : 8x7 : IBDBookðx7 Þ4:ð(x2 : LOCðx7 ,x2 ÞÞ4:ð(x4 : IBLBookðx7 ,x4 ÞÞ
-Bookðx7 ,f ðx7 ÞÞ:

Notice that this example does not contain what we call selfjoins in tgd conclusions, i.e., each relation may appear at
most once in the right-hand side of a tgd. As it will be made
clear in the following sections, expansions and their rewritings become more complex when self-joins are present.
The rewriting algorithm: Based on these ideas, in the
paper we undertake the following approach. After some
preliminary notions introduced in Sections 3 and 4:
(i) we formalize the notion of a witness block, and
introduce a partial order among witness blocks;
based on this, in Section 5 we show that it is possible
to provide a new characterization of the core of the
universal solutions for a mapping scenario in terms
of a set of ‘‘maximal’’ witness blocks;
(ii) then, in Section 6 we formalize the notion of an
expansion, and provide algorithms for generating all
expansions for a tgd;
(iii) in order to generate core solutions, we need to select,
among all witness blocks generated by an expansion,
only the maximal ones; to do this, we introduce a
notion of formula homomorphism among expansions;
in essence, in order to prevent the generation of
homomorphisms among facts, we study homomorphisms among the formulas that generate them;
(iv) based on formula homomorphisms, in Section 7 we
rewrite expansions by introducing negations. In
this way, we are able to formalize an alternative
characterization of the core in terms of expansions,
which represents the basis for the actual rewriting
algorithm;

(v) given a set of s-t tgds, Sst , in Sections 8 and 10 we
introduce the algorithms to rewrite the original tgds
as a new set of dependencies; we call these dependencies First-Order rules (FO-rules), since they are
strictly more expressive than ordinary tgds; more
speciﬁcally, they allow for negations in the premise
and Skolem functions [21,27] in the conclusion, as
discussed in Section 9. The rewritten set of rules is
called a core schema mapping.
The overall algorithm is summarized in Section 11.
Once the original scenario M has been rewritten as a set of
FO-rules, given a source instance I, in order to generate a
core solution for M and I it is sufﬁcient to chase the rules
over I; this can be done very efﬁciently using common
runtime languages like SQL (or XQuery) and guarantees very
good performances, orders of magnitude better than those
of previous core-computation algorithms.
In fact, we have implemented the algorithms developed in the paper as part of the þSPICY [25,6] working
prototype. In Section 12 we discuss the complexity of the
core-computation procedure and in Section 13 we report
experimental results based on the use of the system that
show how our strategy scales up to large databases in
practical scenarios. In light of this, as discussed in Section 14,
we believe that this paper makes a signiﬁcant advancement towards the goal of bridging the gap between the
practice of schema mapping systems and the theory of
data exchange.
2.1. Target constraints
Note that in this paper we restrict our attention to data
exchange settings expressed as a set of source-to-target
tgds only. We do not consider target tgds and target egds
[4]. For this class of mappings we show that it is always
possible to generate a core schema mapping.
This result is somehow optimal. In fact, with respect to
target tgds, it was shown in [31] that it is in general not
possible to rewrite a scenario with s-t tgds and target tgds
into a laconic mapping [31]. Their result extends to core
schema-mappings as well. The authors conjecture that
the same also holds for target egds. Their conjecture was
recently proven in [23], where the authors show that it is
not possible in general to rewrite a set of s-t tgds and
target egds as an equivalent set of s-t dependencies.
Nevertheless, the techniques developed in this paper
represent an important building block towards the goal of
developing scalable core-computation techniques for
large classes of scenarios that include target constraints,
as follows.
Let us ﬁrst consider target tgds. These are typically
used in mapping applications to encode foreign-key constraints on the target schema. We may say that target tgds
corresponding to foreign-key constraints have received
quite a lot of attention and are handled nicely by schemamapping systems [26,27]. The main idea is that, whenever
the set of target tgds has an appropriate boundedness
property, they can be rewritten into the source-to-target
tgds [16]. In fact, the intuition of chasing foreign keys to
generate source-to-target tgds is at the core of the original

G. Mecca et al. / Information Systems 37 (2012) 677–711

Clio mapping-generation algorithm. Once the target tgds
corresponding to foreign keys have been rewritten under
the form of s-t tgds, then the techniques developed in this
paper can be used to generate core universal solutions.
Let us now consider target egds. Egds are typically
used to encode key constraints and functional dependencies over the target. Handling key constraints is a delicate
task, due to the particular form of processing that they
require on the target instances—essentially equating
values. However, in [23] it was shown that, for a very
large fraction of cases, it is possible to rewrite a mapping
scenario containing s-t tgds and target egds as a set of FOrules, and then use these rules to compute core universal
solutions for the original scenario. The results in [23]
heavily rely on the algorithms developed in this paper in
order to perform the rewriting, thus conﬁrming the
relevance of our contributions.
3. Background
Data model: We ﬁx two disjoint sets: a set of constants,
and a set of labeled nulls, VARS. We also ﬁx a set of
labels fA0 ,A1 . . .g, and a set of relation symbols fR0 ,R1 , . . .g.
With each relation symbol R we associate a relation
schema RðA1 , . . . ,Ak Þ. A schema S ¼ fR1 , . . . ,Rn g is a collection of relation schemas. An instance of a relation schema
RðA1 , . . . ,Ak Þ is a ﬁnite set of tuples of the form
RðA1 : v1 , . . . ,Ak : vk Þ, where, for each i, vi is either a
constant or a labeled null. An instance of a schema S is a
collection of instances, one for each relation schema in S.
In the following, we will interchangeably use the positional and non-positional notation for tuples and facts;
also, with an abuse of notation, we will often blur the
distinction between a relation symbol and the corresponding instance.
Given an instance I, we shall denote by constsðIÞ the set
of constants occurring in I, and by varsðIÞ the set of labeled
nulls in I. Its active domain, denoted by domðIÞ, is the set
constsðIÞ [ varsðIÞ. A ground instance is an instance I without labeled nulls (where domðIÞ ¼ constsðIÞ). Given two
disjoint schemas, S and T, we shall denote by /S,TS the
schema fS1 . . . Sn ,T 1 . . . T m g. If I is an instance of S and
J is an instance of T, then the pair /I,JS is an instance
of /S,TS.
Dependencies: Given two schemas, S and T, an
embedded dependency [4] is a ﬁrst-order formula of the
form 8xðfðxÞ-(yðcðx,yÞÞ, where x and y are vectors of
variables, fðxÞ is a conjunction of atomic formulas such
that all variables in x appear in it, and cðx,yÞ is a
conjunction of atomic formulas. Formulas fðxÞ and
cðx,yÞ may contain equations of the form vi ¼vj, where
vi and vj are variables.
An embedded dependency is a tuple generating dependency if fðxÞ and cðx,yÞ only contain relational atoms. A
tgd is called a source-to-target tgd if fðxÞ is a formula over
S and cðx,yÞ over T. It is a target tgd if both fðxÞ and cðx,yÞ
are formulas over T.
Mapping scenario: A mapping scenario (also called a
data-exchange scenario or a schema mapping) is a quadruple M ¼ ðS,T, Sst , St Þ, where S is a source schema, T is a
target schema, Sst is a set of source-to-target tgds, and St
CONSTS,

681

is a set of target dependencies that may contain tgds and
egds. In the case of interest for this paper, i.e., the case in
which the set of target dependencies St is empty, we will
use the notation ðS,T, Sst Þ.
Solutions: A source instance for M is a ground instance
I of the source schema S. A target instance for M is an
instance J of the target schema T. A target instance J is
a solution of M and a source instance I (denoted
J 2 SolðM,IÞ) iff /I,JSFSst [ St .
Given two instances J, J0 over a schema T, a homomorphism h : J-J0 is a mapping from domðJÞ to domðJ 0 Þ
such that for each c 2 constsðJÞ, hðcÞ ¼ c, and for each tuple
t ¼ RðA1 : v1 , . . . ,Ak : vk Þ in J it is the case that hðtÞ ¼ RðA1 :
hðv1 Þ, . . . ,Ak : hðvk ÞÞ belongs to J 0 . Homomorphism h is
called an endomorphism if J 0 DJ; if J 0  J it is called a
proper endomorphism. We say that two instances J, J0 are
homomorphically equivalent if there are homomorphisms
0
h : J-J0 and h : J 0 -J.
A solution J is universal [12] iff for every solution K
there is a homomorphism from J to K. The set of universal
solutions for M and I is denoted by USolðM,IÞ. Associated
with scenario M is the following data exchange problem:
given a source instance I, return none iff no solution exists,
or return a universal solution J 2 USolðM,IÞ.
The chase: Tgds are executed using the classical chase
procedure. In order to deﬁne these, we need to introduce
the notion of an assignment. Given a formula jðx,yÞ,
where x is a vector of universally quantiﬁed variables,
and y is a vector of existentially quantiﬁed variables, an
assignment for jðx,yÞ is a mapping a : x [ y-consts [
vars that associates with each variable xi 2 x a constant
aðxi Þ 2 consts, and with each variable yi 2 y a value aðyi Þ
that can be either a constant or a labeled null.
We say that an assignment a for jðx,yÞ is canonical if it
injectively associates a labeled null with each existential
variable yi 2 y. The set of facts aðjðx,yÞÞ is called a
canonical block if a is canonical. Given a formula fðxÞ with
free variables x, and an instance I, we say that I satisﬁes
fðxÞ with assignment a if IFfðaðxÞÞ.
Given an instance /I,JS, during the naive chase, a tgd
8x : f ðx Þ-(y ðc ðx ,y ÞÞ is ﬁred for all value assignments a
such that IFfðaðxÞÞ by extending a to a canonical assignment a0 by injectively assigning to each variable yi 2 y a
fresh null, and then adding the facts in cða0 ðxÞ,a0 ðyÞÞ to J.
Given a scenario M ¼ ðS,T, Sst Þ, we call a canonical
solution any solution obtained by chasing the dependencies in Sst . Any canonical solution is a universal solution
[12]. Since all solutions obtained by using the naive chase
are equal up to the renaming of nulls, we shall often speak
of the canonical universal solution.
4. Core schema mappings
We ﬁnd it useful to introduce a labeling system to
identify the provenance [8] of tuples in the canonical
solution. More speciﬁcally, for each tgd m in Sst and each
atom Rð. . .Þ in the conclusion of m, we associate with Rð. . .Þ
a unique integer label, i. Then, given a source instance I,
for each tuple t in the canonical universal solution J, we
keep track of its provenance, provenanceðtÞ, as a set of
labeled relation symbols. More formally, whenever t 0 is

682

G. Mecca et al. / Information Systems 37 (2012) 677–711

generated during the chase by ﬁring tgd m and instantiating atom Rð. . .Þ in the conclusion of m, we add to the set
provenanceðtÞ the symbol Ri , where i is the label of R.
Consider the following scenario, in which tgds have
been labeled:
m1 :Aðx1 ,x2 ,x3 Þ-(Y 1 : S1 ðx1 ,Y 1 Þ4S2 ðY 1 ,x2 Þ4T 3 ðY 1 ,x3 Þ
m2 :Bðx4 ,x5 Þ-S4 ðx4 ,x5 Þ
m3 :Cðx6 ,x7 Þ4Dðx7 ,x8 Þ-S5 ðx6 ,x7 Þ
and the source instance: I ¼ fAð1; 2,3Þ,Bð1; 2Þ,Cð1; 2Þ,
Dð2; 3Þg. Following we report the canonical universal
solution and the provenance of tuples, as follows:
J ¼ fSð1,N 0 Þ½fS1 g,SðN0 ,2Þ½fS2 g,TðN 0 ,3Þ½fT 3 g,Sð1; 2Þ½fS4 ,S5 gg
Based on the labeling system that we have introduced,
in the following we shall use labeled formulas of the form
Ri ð. . .Þ as queries that retrieve from an instance all tuples t
in relation R such that Ri 2 provenanceðtÞ. More speciﬁcally, given an atom Ri ðx,yÞ, where x is a set of universally
quantiﬁed variables, and y a set of existentially quantiﬁed
variables, an assignment a for x,y, and a canonical
instance J, we say that JFaðRi ðx,yÞÞ if the following hold:
(i) J contains a tuple t ¼ RðaðxÞ,aðyÞÞ; (ii) Ri 2 provenanceðtÞ.
Similarly for a conjunction of labeled atoms of the
form jl ðx,yÞ.
The canonical solution has the nice property of being a
universal solution [12]. Also, the naive chase of a set of s-t
tgds can be implemented very efﬁciently using ﬁrst-order
languages as SQL as a set of queries on the source and
insert statements into the target.1 However, the canonical
solution is not, in general, a core solution, and it is known
that it may contain quite a lot of redundancy.
In the next sections, we concentrate on the following
problem: given a data-exchange scenario M ¼ ðS,T, Sst Þ,
generate an executable script in a ﬁrst-order language like
SQL that, when run on a source instance I computes the
core universal solution for M on I.
A central idea behind our approach is that the computation of core solutions can be implemented by properly
rewriting the original scenario into a new set of source-totarget dependencies. However, in order to properly perform the rewriting, we resort to dependencies that are
strictly more expressive than ordinary tgds. In particular,
we will make extensive use of negation in the premise,
and of Skolem terms in the conclusion. We call these more
expressive dependencies FO-rules.
4.1. First-order rules
Before introducing the deﬁnition of what a FO-rule is,
we need to formalize the notion of a Skolem term. Given a
set of variables x, a Skolem term over x is a term of the
form f ðx1 , . . . ,xk Þ where f is a function symbol of arity k
and x1 , . . . ,xk are universal variables in x.
Skolem terms are used to create fresh labeled nulls on
the target. Traditionally, Skolem functions are considered as
1

Notice that this is not the case if we also allow target constraints,
i.e., target tgds or target egds.

uninterpreted functions. Given an assignment of values c for
x, with an uninterpreted Skolem term f ðxÞ we (injectively)
associate a labeled null N f ðcðxÞÞ . As an alternative, we may
consider Skolem functions as being interpreted. In this
second case, we associate with each function symbol f of
i
arity k a function f : constsk -vars, and, for each value
assignment c for x, we compute the labeled null as the
i
result of fi over cðxÞ, f ðcðxÞÞ.
If we assume a linear order, o, over the underlying set
of constants, consts, then, a special class of interpreted
Skolem functions are those that rely on the linear order.
We shall refer to these functions as linear-order dependent.
In this paper, we concentrate on a subset of these
functions, obtained by composing two simple functions:
(i) appendðv1 ,v2 Þ, where v1 and v2 may be either a
constant string or a universally quantiﬁed variable, whose
result is the string obtained by concatenating the two
values; (ii) sortðx1 ,x2 Þ, where x1 and x2 are universally
quantiﬁed variables; this function returns a string in
which the values of the variables appear in increasing
order according to the given linear order; for example, if
x1 ¼ d,x2 ¼ c and c o d, sortðx1 ,x2 Þ is the string ‘‘½c,d’’.
To give an example, consider the function: f ðx1 ,x2 Þ ¼
appendð‘‘f ’’,sortðx1 ,x2 ÞÞ; on inputs x1 ¼ 1,x2 ¼ 2; 1o 2, the
Skolem function generates ‘‘f ½1; 2’’, while on inputs x1 ¼ 4,
x2 ¼ 3; 3 o4, generates ‘‘f ½3; 4’’.
Deﬁnition 1 (FO-rule). Given a source schema S and a
target schema T, an FO-rule is a dependency of the form
8x : fðxÞ-cðxÞ where f is a ﬁrst-order formula over S and
c is a conjunction of atoms of the form Rðt1 , . . . ,tn Þ, with
R 2 T and each term ti is either a variable t i 2 fxg or a
Skolem term over x.
Some FO-rules were introduced in the ﬁnal rewriting
of the sample scenario in Section 2.
Whenever a set of FO-rules fr 1 , . . . ,r n g uses linearorder dependent Skolem functions, we shall say that
fr 1 , . . . ,r n g is also linear-order dependent, and denote it by
R o ¼ fr 1 , . . . ,r n g.
To execute a set of FO-rules, we now introduce an
extension of the naive chase procedure.
Deﬁnition 2 (Chasing FO-rules). Given an FO-rule 8x :
fðxÞ- cðxÞ, we call Q f ðxÞ the ﬁrst-order query over S
obtained from fðxÞ considering x as free variables. We
denote by Q f ðIÞ the set of tuples c 2 domðIÞ9x9 such that c
is an answer of Q f over I. Given c 2 Q f ðIÞ, we then denote
by cðcÞ the set of atoms obtained from c by replacing
each variable xi 2 x by the corresponding ci 2 c and
replacing each Skolem term by the corresponding labeled
null.
Given a set R ¼ fr 1 , . . . ,r n g of FO-rules of the form above
r i :8x : fi ðxÞ-ci ðxÞ and a source instance I, we deﬁne the
result of the chase of R over I as follows:
0
1
[
[
@
RðIÞ ¼
ðci ðcÞÞA
i2½1,n

c 2Q f ðIÞ
i

Based on this, it should be apparent how FO-rules lend
themselves to a natural implementation as an SQL script.
Consider for example rule r1 in Section 2. Based on the

G. Mecca et al. / Information Systems 37 (2012) 677–711

rule, we can materialize tuples in the Book table by the
following SQL statement (similarly for Publisher). Notice
how string manipulation functions are used to generate
the needed Skolem terms:
INSERT INTO Book
SELECT LOC.title, append(‘f(’, LOC.title, ‘,’, LOC.publisher,‘)’)
FROM SELECT LOC.* FROM LOC
EXCEPT
SELECT b.title, p.name FROM IBLBook b, IBLPublisher p WHERE
pubId ¼id

683

Deﬁnition 4 (Normal form for Tgds). A set of tgds Sst is
in normal form if: (i) for each mi , mj 2 Sst , ðx i [ y i Þ
\ðx j [ y j Þ ¼ |, i.e, the tgds use disjoint sets of variables;
(ii) for each tgd mi , the tgd conclusion of mi is normalized.
If the input set of tgds is not in normal form, it is
always possible to preliminarily rewrite them to obtain
an input in normal form. In particular, we introduce
a transformation, called normalize, that takes a formula,
jðx,yÞ, and generates a new set of formulas, normalize
ðjðx,yÞÞ, one for each connected component in the dual
Gaifman graph of the original formula.

4.2. Computing core solutions
5. A characterization of the core
Given a scenario M ¼ ðS,T, Sst Þ, and an instance I, the
core [13] of a universal solution J 2 USolM ðIÞ, C, is a
subinstance of J such that there is a homomorphism from
J to C, but there is no homomorphism from J to a proper
subinstance of C. It is known [13] that cores of the
universal solutions for a scenario M and source instance
I are all isomorphic to each other, and therefore it is
possible to speak of the core universal solution.
We are now ready to introduce the notion of a core
schema-mapping:
Deﬁnition 3 (Core schema mapping). Given a scenario
M ¼ ðS,T, Sst Þ, a set of FO-rules R is called a core schema
mapping for M if, for any source instance I, the canonical
target instance RðIÞ is the core universal solution for M
over I.
The main contribution of the paper is a set of algorithms
that, given a mapping scenario, rewrite the given tgds as a
set of FO-rules, R o , that represent a core schema mapping.
Notice that – as it is common in practical applications – we
shall assume a linear order on the underlying set of
constants, consts, and exploit the linear order in our
Skolemization, as will be detailed in Section 9.
A very similar notion of laconic schema mapping was
introduced in [31]. Notice, however, that a laconic schema
mapping is required to be logically equivalent to the
original scenario, i.e., to have the same set of solutions.
We do not require the same.
As it was discussed earlier, we concentrate on mapping
scenarios composed of a set of s-t tgds Sst and do not
consider target constraints. Without loss of generality we
require that the input tgds are in normal form, i.e., each
tgd uses distinct variables, and no tgd can be decomposed
in two different tgds having the same left-hand side.2 To
formalize this notion, let us introduce the Gaifman graph
of a formula as the undirected graph in which each
variable in the formula is a node, and there is an edge
between v1 and v2 if v1 and v2 occur in the same atom.
The dual Gaifman graph of a formula is an undirected
graph in which nodes are atoms, and there is an edge
between atoms Ri ðx i ,y i Þ and Rj ðx j ,y j Þ if there is some
existential variable yk occurring in both atoms. We say
that a formula jðx,yÞ is normalized if its dual Gaifman
graph is connected.
2

This requirement is pretty common [13,31,19].

This section provides an important result upon which we
shall build the rewriting algorithms reported in the remainder of the paper. It introduces the key concept of a witness
block, and shows how it is possible to characterize the core
of the universal solutions for a mapping scenario by means
of witness blocks. In doing this, it outlines a core computation strategy that will be exploited in the next sections.
Consider a scenario M with a set of s-t tgds Sst ; given
a source instance, I, each tgd in Sst represents a constraint
that must be satisﬁed by any solution J for M over I.
Informally speaking, a witness block is a set of facts in J
that guarantees that a tgd in Sst is satisﬁed for some
vector of constants c. More formally:
Deﬁnition 5 (Witness block). Given a scenario M ¼
ðS,T, Sst Þ, a source instance I, and a universal solution
J 2 USolM ðIÞ, for each tgd m: 8x : f ðx Þ-(y ðc ðx ,y ÞÞ 2 Sst
and any assignment c for x such that IFfðcðxÞÞ, a witness
block for /I,JS, m, and c ¼ cðxÞ is a set of facts wD J such
that, for some assignment d for y, it is the case that
w ¼ cðcðxÞ,dðyÞÞ.
In the following we shall use the following notation:
/I,J S

(a) W m,c
blocks
/I,J S
(b) W m
(c) W /I,J S

will be used to denote the set of all witness
for /I,JS, m, and c;
the set of all witness blocks for /I,JS and m;
the set of all witness blocks of /I,JS.

A key intuition is that there are usually multiple ways
to satisfy a tgd m for some vector of constants a, i.e., a
solution usually contains multiple witness blocks for m
and a. The following examples show how the core can be
characterized in terms of witness blocks.
Example 5.1. Consider the following scenario, that is
essentially equivalent to the one in Section 2:
m1 : 8x1 ,x2 : Aðx1 ,x2 Þ-(Y 1 : Sðx1 ,Y 1 Þ4TðY 1 ,x2 Þ
m2 : 8x3 ,x4 : Bðx3 ,x4 Þ-Sðx3 ,x4 Þ
m3 : 8x5 ,x6 : Cðx5 ,x6 Þ-Tðx5 ,x6 Þ
m4 : 8x7 : Dðx7 Þ-(Y 0 : Sðx7 ,Y 0 Þ
and the solution: J ¼ fSð1,N0 Þ,TðN0 ,2Þ,Sð1; 3Þ,Tð3; 2Þ, Sð1,N1 Þg
for source instance: I ¼ fAð1; 2Þ,Bð1; 3Þ,Cð3; 2Þ, Dð1Þg.

684

G. Mecca et al. / Information Systems 37 (2012) 677–711

Following are the witness blocks for J:
/I,J S

W m1 ,/1;2S ¼ ffSð1,N 0 Þ,TðN 0 ,2Þg,fSð1; 3Þ,Tð3; 2Þgg
/I,J S

W m2 ,/1;3S ¼ ffSð1; 3Þgg

instances of tgd conclusions. In some cases the witness blocks
of a tgd are also fact blocks. In other cases, they are unions of
fact blocks. However, the following example shows that there
may be witness blocks in an instance that are neither fact
blocks nor unions of fact blocks.

/I,J S

W m4 ,/1S ¼ ffSð1,N1 Þg,fSð1,N 0 Þg,fSð1; 3Þgg

Example 5.3. Consider now the following scenario M:

/I,J S

W m3 ,/3;2S ¼ ffTð3; 2Þgg

m1 :Aðx0 ,x1 ,x2 ,x3 Þ4Bðx3 ,x4 Þ-(Y 0 ,Y 1 ,Y 2 ,Y 3 : Sðx3 ,x0 ,Y 0 ,x1 Þ

The core of J is as follows: J 0 ¼ fSð1; 3Þ,Tð3; 2Þg. The witness
blocks for J 0 are as follows:
/I,J S

0
¼ ffSð1; 3Þ,Tð3; 2Þgg
W m1 ,/1;2S

/I,J S

0
W m4 ,/1S
¼ ffSð1; 3Þgg

/I,J S

0
W m2 ,/1;3S
¼ ffSð1; 3Þgg

/I,J S

0
W m3 ,/3;2S
¼ ffTð3; 2Þgg

In Example 5.1 witness blocks are quite simple due to
the absence of duplicate symbols in tgd conclusions.
Whenever the conclusion f ðx Þ of a tgd m is such that
the same relation symbol occurs more than once, we say
that m contains self-joins. The following example shows
the witness blocks of a scenario with self-joins in tgd
conclusions.
Example 5.2. Consider now the following scenario M
(tgds have been labeled):
m1 :Aðx1 ,x2 ,x3 Þ-(Y 1 ,Y 2 : S1 ðx1 ,Y 1 ,Y 2 Þ4S2 ðx2 ,x3 ,Y 2 Þ
m2 :Bðx4 ,x5 Þ-(Y 3 ,Y 4 : S3 ðx4 ,x5 ,Y 3 Þ4S4 ðY 4 ,x5 ,Y 3 Þ
and the source instance I ¼ fAð3; 1,2Þ,Að1; 1,2Þ,Bð1; 2Þg. The
canonical solution J is the following:
J ¼ fSð3,N 0 ,N 1 Þ½S1 ,Sð1; 2,N 1 Þ½S2 , Sð1,N2 ,N3 Þ½S1 ,
Sð1; 2,N 3 Þ½S2 ,

Sð1; 2,N 4 Þ½S3 ,

SðN5 ,2,N4 Þ½S4 g

The witness blocks in J are as follows:
/I,J S

W m1 ,/3;1,2S ¼ ffSð3,N0 ,N 1 Þ,Sð1; 2,N 1 Þgg
/I,J S

W m1 ,/1;1,2S ¼ ffSð1,N2 ,N 3 Þ,Sð1; 2,N 3 Þg,
fSð1; 2,N1 Þg,fSð1; 2,N 3 Þg,fSð1; 2,N4 Þgg
/I,J S

W m2 ,/1;2S ¼ ffSð1; 2,N 4 Þ,SðN 5 ,2,N 4 Þg,fSð1; 2,N1 Þg,
fSð1; 2,N3 Þg,fSð1; 2,N 4 Þgg
The core of J is as follows: J 0 ¼ fSð3,N0 ,N1 Þ,Sð1; 2,N1 Þg,
with the following witness blocks:

4SðY 1 ,x0 ,Y 0 ,x0 Þ4SðY 1 ,x2 ,Y 2 ,Y 3 Þ
and the source instance: I ¼ fAð1; 1,2; 1Þ,Að2; 1,2; 1Þ, Bð1; 4Þg.
The core universal solution for M over I is J 0 ¼
fSð1; 1,N0 ,1Þ,Sð1; 2,N5 ,1Þ,SðN6 ,2,N5 ,2Þg.
The witness blocks for J 0 are as follows:
/I,J S

0
W m1 ,/1;1,2;1S
¼ ffSð1; 1,N0 ,1Þ,Sð1; 2,N5 ,1Þgg

/I,J S

0
W m1 ,/2;1,2;1S
¼ ffSð1; 2,N5 ,1Þ,SðN6 ,2,N5 ,2Þgg

/I,J S

0
is not a fact
Notice how the witness block in W m1 ,/1;1,2;1S
block, nor the union of two fact blocks (it is rather the union
of a fact block and a fragment of another fact block). &

Our goal is to ﬁnd a characterization of the core in
terms of its witness blocks. However, as it can be seen
from the examples above, some of the witness blocks in
the canonical solution are redundant, and therefore the
corresponding tuples need to be removed to generate the
core. As it is natural, we use the notion of a homomorphism to deﬁne what a ‘‘redundant’’ witness block is.
Recall that, given two instances J, J0 , a homomorphism
h : J-J 0 is a mapping from domðJÞ to domðJ 0 Þ that maps
constants to themselves (i.e. for each c 2 constsðJÞ, hðcÞ ¼ c)
such that for each tuple t ¼ RðA1 : v1 , . . . ,Ak : vk Þ in J it is
the case that hðtÞ ¼ RðA1 : hðv1 Þ, . . . ,Ak : hðvk ÞÞ belongs to J 0 .
We say that h is injective if it maps distinct atoms in J into
distinct atoms of J 0 . We say that h is surjective, or that it is
a surjection, if every atom of J 0 is the image of some atom
of J according to h.
There are several ways in which tuples can be made
redundant in a solution. Generally speaking, tuples are
redundant whenever they introduce unnecessary nulls. To
formalize this notion, we introduce a classiﬁcation of
homomorphisms, as follows:
Deﬁnition 6 (Classiﬁcation of homomorphisms). Given
two instances J, J0 , and a homomorphism h : J-J 0 :

/I,J S

0
W m1 ,/3;1,2S
¼ ffSð3,N0 ,N 1 Þ,Sð1; 2,N 1 Þgg

/I,J S

0
W m1 ,/1;1,2S
¼ ffSð1; 2,N1 Þgg

/I,J S

0
W m2 ,/1;2S
¼ ffSð1; 2,N1 Þgg

An important observation is that other algorithms for
core computation [13,18,31] have so far concentrated on a
different notion of ‘‘blocks’’, namely fact blocks. Informally
speaking, a fact block in an instance is a set of facts that
are joined via labeled nulls. More formally, it is a connected component in the dual Gaifman graph of an
instance, in which facts are the nodes, and there exists
an edge between any two facts in which the same labeled
null appears.
We want to emphasize that witness blocks are a different
concept with respect to fact blocks, since they are essentially

(i) h is compacting if it is surjective, and 9varsðJ 0 Þ9 o
9varsðJÞ9; we write J!J 0 if there is a compacting
homomorphism of J into J0 ;
(ii) h is proper if it is injective and not surjective, i.e., J 0
contains at least one atom that is not the image of an
atom of J; in symbols, we write that J o J 0 ;
(iii) h is an isomorphism if it is surjective and injective and
its inverse is also a homomorphism; in this case, we
say that J and J 0 are isomorphic, in symbols J ﬃ J 0 .
In terms of witness blocks, we can identify two main
reasons according to which a witness block w can be
made redundant by another witness block w0 for the same

G. Mecca et al. / Information Systems 37 (2012) 677–711

685

tgd and assignment. The ﬁrst one is if w0 is more compact
than w, i.e., if there exists a compacting homomorphism
of w into w0 .
To give an example, consider Example 5.2, and tgd m2:

that multiple isomorphic copies of a witness block survive
in the result. To see this, consider the following example:

m2 :Bðx4 ,x5 Þ-(Y 3 ,Y 4 : Sðx4 ,x5 ,Y 3 Þ4SðY 4 ,x5 ,Y 3 Þ

m1 : Aðx1 ,x2 ,x3 Þ-(Y 1 ,Y 2 : Sðx1 ,Y 1 ,Y 2 Þ4Sðx2 ,x3 ,Y 2 Þ

The tgd has the following witness blocks:

m2 : Bðx4 ,x5 Þ-(Y 3 ,Y 4 : Sðx4 ,x5 ,Y 3 Þ4SðY 4 ,x5 ,Y 3 Þ

/I,J S

W m2 ,/1;2S ¼ ffSð1; 2,N4 Þ,SðN5 ,2,N 4 Þg,fSð1; 2,N4 Þg,fSð1; 2,N 1 Þgg
Notice, however, that the witness block w1 ¼ fSð1; 2,N 4 Þ,
SðN 5 ,2,N 4 Þg has a compacting homomorphism into w2 ¼
fSð1; 2,N4 Þg; in fact, w2 contains a lower number of
nulls than w1 . This means that w1 is redundant for core
computation properties.
It can be seen that the ! relation associated with
compacting homomorphisms is antisymmetric and transitive, and therefore induces a partial order on witness blocks.
A ﬁrst intuition of our algorithm is that of selecting, among
all possible witness blocks, only those that represent maximal elements with respect to this partial order, in order to
minimize the null values in the ﬁnal result.
However, even such maximal elements may still be
redundant. In fact, other tgds and assignments may generate
witness blocks that are ‘‘more informative’’. A witness block
w0 is said to be more informative than a witness block w if
there exists a proper homomorphism of w into w0 .
Consider again Example 5.2. We have learned that w2 ¼
fSð1; 2,N4 Þg is a maximal element with respect to compacting homomorphisms for m2 and /1; 2S. However, it is still
redundant in terms of core computation. In fact, among the
witness blocks of m1 we ﬁnd w3 ¼ fSð3,N 0 ,N1 Þ,Sð1; 2, N 1 Þg. It
can be seen that w2 has a proper homomorphism into w3 . In
essence, the null N1 carries ‘‘more information’’ than N4. We
need therefore to discard w2 and consider w3 only to
generate the core.
Again, proper homomorphisms induce a partial order
on the set of witness blocks. In light of this, our core
computation procedure will proceed as follows:
(i) we shall ﬁrst select the most compact witness blocks
/I,J S

in any set W m,a , i.e., all maximal elements with
respect to the ! partial order;
(ii) then, we will exclude all elements such that there are
more informative witness blocks, i.e., we will select
the maximal elements with respect to the o partial
order.

Example 5.4. Consider again the mapping scenario in
Example 5.2:

and a different source instance: I ¼ fAð1; 1,2Þ,Bð1; 2Þg, for
which the canonical universal solution is J ¼ fSð1,N0 ,N 1 Þ,
Sð1; 2,N1 Þ,Sð1; 2,N4 Þ,SðN5 ,2,N 4 Þg.
Following are some sets of witness blocks for J:
/I,J S

W m1 ,/1;1,2S ¼ ffSð1,N 0 ,N 1 Þ,Sð1; 2,N 1 Þg,fSð1; 2,N 4 Þg,fSð1; 2,N 1 Þgg
/I,J S

W m2 ,/1;2S ¼ ffSð1; 2,N4 Þ,SðN5 ,2,N 4 Þg,fSð1; 2,N4 Þg,fSð1; 2,N 1 Þgg
By taking the union of the set of maximal witness
blocks, we obtain the following solution: J n ¼ fSð1; 2,N 4 Þ,
Sð1; 2,N1 Þg that is obviously not the core. In fact, the two
maximal witness blocks are isomorphic to each other, and
we need to consider only one of them. We may say that,
by selecting maximal witness blocks, we are able to
identify two alternative subsets of J that correspond to
the core, so that we need to pick one of them. &
After we have selected a set of maximal witness
blocks, to generate an isomorphism-free solution we
introduce a minimization algorithm, called reduce, that
works as follows:
(a) given a set of witness blocks W, it identiﬁes all
equivalence classes E 0 , . . . ,E k of isomorphic witness
blocks in W;
(b) for each equivalence class, it (nondeterministically)
selects exactly one representative, wE i ;
(c) then, it returns the subset of W obtained by taking
the representative of each equivalence class, i.e.,
W ¼ fwE i 9i ¼ 0, . . . ,kg.
Based on this intuition, we are ready to formalize our
characterization of the core.
Theorem 1. Given a scenario M ¼ ðS,T, Sst Þ, and a source
instance I, suppose J is a universal solution for M over I.
Consider the subset J 0 of J deﬁned as follows:
[
reduceðmostInformativeðmostCompactðW /I,J S ÞÞÞ
J0 ¼
ð1Þ
Then, J 0 is the core of J.
The proof is in Appendix B.

More formally, given a set of witness blocks W, we
deﬁne:
0

0

mostCompactðWÞ ¼ fw9w 2 W4:(w 2 W : w!w g

mostInformativeðWÞ ¼ fw9w 2 W4:(w0 2 W : w o w0 g
By doing this, we are able to remove most of the
redundancy in the original solution. Unfortunately, not
enough to generate the core. In fact, it may be the case

6. Expansions
Given a mapping scenario, M ¼ ðS,T, Sst Þ, our goal is to
rewrite the given tgds under a set of FO-rules that
represents a core schema mapping for M, and then to
generate an SQL script from them. In order to perform the
rewriting, we shall rely on the characterization of the core
introduced in Section 5. Our intuition is that it is possible
to select the needed witness blocks by using a set of
ﬁrst-order rules. In this Section we introduce the central

686

G. Mecca et al. / Information Systems 37 (2012) 677–711

notion of an expansion of a tgd conclusion, that we shall
use in the next sections to perform the rewriting. More
speciﬁcally:
(i) in this section we formalize the deﬁnition of an
expansion for a tgd, and discuss the relationship
among expansions and witness blocks; in our algorithm, we use expansions as a means to study
possible overlaps between tgds’ effects;
(ii) in order to do this, we introduce the notion of
a formula homomorphism; we use formula homomorphisms to rewrite expansions in such a way to
select only maximal witness blocks;
(iii) in the next section, we provide an alternative characterization of the core that mirrors the one in
Section 5, but it is based on expansions and their
homomorphisms;
(iv) ﬁnally, in Section 8 we show that expansions, which
are formulas over the target, can be fairly easily
rewritten as formulas over the source database; in this
way, we provide a solid foundation for the rewriting
algorithm developed in the following sections.
In the following, we assume that the input scenario,
M, is ﬁxed. To simplify the notation, from now on we
shall omit explicit references to M whenever this is clear
from the context. Recall that in our labeling system, atom
Si corresponds to all tuples in relation S whose provenance contains label i. In fact, in this section we will
systematically make use of labeled formulas. We use the
notation jl ðx,yÞ to denote a labeled conjunctive formula
with universally quantiﬁed variables x, and existentially
quantiﬁed variables y.
6.1. Introducing expansions
Once the canonical universal solution J for I has been
generated by chasing the original tgds, our next step is to
select the witness blocks that belong to the core. Notice
that, since J is a ﬁnite instance, W /I,J S is a ﬁnite set, and
therefore the set of witness blocks for each tgd is ﬁnite.
Our intuition is to generate a set of queries, called
expansions, as discussed in Section 2, that capture the
witness blocks in W /I,J S .
With respect to the simple examples discussed in
Section 2, in the general case the process of constructing
expansions is made more complicated in case of tgds with
self-joins in their conclusions. To give an example, consider mapping m1 in Example 5.2. Tgd m1 states that the
target must contain a number of tuples in S that satisfy
the tgd conclusion. The formula
1

2

E11 ¼ S ðx1 ,Y 1 ,Y 2 Þ4S ðx2 ,x3 ,Y 2 Þ
is called the base expansion of m1, and by running the
corresponding query over J we ﬁnd a number of witness
blocks for m1. In our example, it selects the witness blocks
w0 ¼ fSð3,N 0 ,N 1 Þ,Sð1; 2,N 1 Þg and w1 ¼ fSð1,N 2 ,N 3 Þ,Sð1; 2,N 3 Þg.
However, it does not capture all witness blocks for tgd
m1. In fact, tuples in J that satisfy the conclusion of m1 (i)
do not necessarily belong to the extent of S1 , S2 , since they
may also come from S3 or S4 ; (ii) these tuples are not

necessarily distinct, since there may be tuples that perform a self-join.
One alternative way to generate valid witness blocks
for m1 is to use only one tuple from S2 in join with itself
on the last attribute – i.e., S2 is used to ‘‘cover’’ the S1
atom. However, this may work as long as the two atoms
generate tuples that do not conﬂict with the constants in
the base expansion of m1; in our example, the values
generated by the S2 atom must be consistent with those
that would be generated by the S1 atom in the base
expansion, i.e., x2 ¼ x1 . Generally speaking, any expansion
must be consistent with the base expansion, i.e., with the
tgd conclusion. We write this second expansion as follows, ﬁrst in its extended and more verbose form, to
emphasize that an intersection with the base expansion is
required, and then in its simpliﬁed form

E12 ¼ S2 ðx2 ,x3 ,Y 2 Þ4(x1 ,Y 1 : ðS1 ðx1 ,Y 1 ,Y 2 Þ
4S2 ðx2 ,x3 ,Y 2 Þ4x1 ¼ x2 Þ
¼ S2 ðx2 ,x3 ,Y 2 Þ4(Y 1 : ðS1 ðx2 ,Y 1 ,Y 2 ÞÞ
Expansion E12 captures witness blocks w2 ¼ fSð1; 2,N 1 Þg
and w3 ¼ fSð1; 2,N 3 Þg. To identify the last witness block for
m1, w4 ¼ fSð1; 2,N4 Þg, we need one ﬁnal expansion, that
uses a single atom for S3 (notice, in fact, that this witness
block contains an atom whose provenance is S3 in mapping m2)

E13 ¼ S3 ðx4 ,x5 ,Y 3 Þ4(x1 ,x2 ,x3 ,Y 1 ,Y 2 : ðS1 ðx1 ,Y 1 ,Y 2 Þ
4S2 ðx2 ,x3 ,Y 2 Þ4x4 ¼ x1 4x4 ¼ x2 4x5 ¼ x3 Þ
¼ S3 ðx4 ,x5 ,Y 3 Þ4(Y 1 ,Y 2 : ðS1 ðx4 ,Y 1 ,Y 2 Þ4S2 ðx4 ,x5 ,Y 2 ÞÞ
A similar approach can be used for tgd m2 above.
In this case, the algorithm ﬁrst generates the base
expansion:

E21 ¼ S3 ðx4 ,x5 ,Y 3 Þ4S4 ðY 4 ,x5 ,Y 3 Þ
As it was noted [31,19], the base expansion is hardly
useful for core computation purposes. Consider the facts
obtained from E21 by considering each xi as a constant and
each Yi as a labeled null. It is easy to see that this set is not
a core. In fact, the second atom is useless with respect
to the ﬁrst one. More interesting expansions are the
following:

E22 ¼ S3 ðx4 ,x5 ,Y 3 Þ
E23 ¼ S2 ðx2 ,x3 ,Y 2 Þ4(x4 ,x5 ,Y 3 ,Y 4 : ðS3 ðx4 ,x5 ,Y 3 Þ
4S4 ðY 4 ,x5 ,Y 3 Þ4x2 ¼ x4 4x3 ¼ x5 Þ
¼ S2 ðx2 ,x3 ,Y 2 Þ4(Y 3 ,Y 4 : ðS3 ðx2 ,x3 ,Y 3 Þ4S4 ðY 4 ,x3 ,Y 3 ÞÞ
Notice that no intersection is present in E22 , since we
know that atom S3 always covers S4.
These ideas are made more precise in the following
paragraphs.
6.2. Formula homomorphisms
In order to develop an algorithm that ﬁnds all expansions of a tgd conclusion, we introduce a notion of formula
homomorphism, which is reminiscent of the notion of
containment mapping used in [22]. We ﬁnd it useful to

G. Mecca et al. / Information Systems 37 (2012) 677–711

deﬁne homomorphisms among variable occurrences, and
not among variables.
Deﬁnition 7 (Variable occurrence). Given an atom Rl ðA1 :
v1 , . . . ,Ak : vk Þ in a formula jl ðx,yÞ, a variable occurrence is
a pair Rl :Aj : vi . A variable occurrence Rl :Aj : vi in jl ðx,yÞ is
a universal occurrence if vi 2 x; it is an existential occurrence if vi 2 y.
In the following, we denote by occðjl ðx,yÞÞ the set of all
variable occurrences in jl ðx,yÞ; u-occðjl ðx,yÞÞ,e-occðjl ðx,yÞÞ
will denote the set of universal and existential occurrences,
respectively. Similarly, occðvÞ,u-occðvÞ,e-occðvÞ will denote
the set of all (universal, existential) occurrences of a given
variable v.
Deﬁnition 8 (Formula homomorphism). Given two conjunctive formulas, jl ðx,yÞ and j0l ðx 0 ,y 0 Þ, a formula homof
morphism is an injective mapping h from the set
l
0l 0 0
occðj ðx,yÞÞ to occðj ðx ,y ÞÞ such that:
f

(i) h maps universal occurrences into universal
occurrences;
(ii) for each atom Rl ðA1 : v1 , . . . ,Ak : vk Þ 2 jl ðx,yÞ, it is the
f
f
case that Rðh ðRl :A1 : v1 Þ, . . . ,h ðRl :Ak : vk ÞÞ 2 j0l ðx 0 ,y 0 Þ;
(iii) for each pair of occurrences of an existential variable
y 2 y, Rli :Aj : y, Rln :Am : y it is the case that either
f
f
h ðRli :Aj : yÞ and h ðRln :Am : yÞ are both universal, or
they are occurrences of the same existential variable
y0 2 y 0 .
f

We say that a formula homomorphism h is injective if
it maps distinct atoms of jl ðx,yÞ into distinct atoms of
j0l ðx 0 ,y 0 Þ. It is surjective if every atom in j0l ðx 0 ,y 0 Þ is the
f
image of some atom in jl ðx,yÞ according to h .
Consider for example the two formulas over relations
RðA,B,CÞ and TðA,B,CÞ: jl ¼ R1 ðx1 ,x2 ,Y 1 Þ4T 2 ðx3 ,x1 ,Y 1 Þ and
j0l ¼ R3 ðx04 ,x05 ,x06 Þ4T 4 ðx09 ,x07 ,x08 Þ. There exists a formula
f
homomorphism h of jl into j0l , based on the following
mapping of variable occurrences:
f

h ðR1 :A : x1 Þ-R3 :A : x04

f

h ðR1 :B : x2 Þ-R3 :B : x05

f

h ðT 2 :A : x3 Þ-T 4 :A : x09

f

h ðT 2 :C : Y 1 Þ-T 4 :C : x08

h ðR1 :C : Y 1 Þ-R3 :C : x06
h ðT 2 :B : x1 Þ-T 4 :B : x07

f

f

Consider now the two formulas: jl ¼ R1 ðx1 ,x2 ,Y 2 Þ and
j0l ¼ R2 ðx04 ,x05 ,Y 03 Þ4R0 3 ðY 04 ,x05 ,Y 03 Þ. There exists a formula
f
homomorphism h of jl into j0l , based on the following
mapping of variable occurrences:
f0

h ðR1 :A : x1 Þ-R2 :A : x04

f0

h ðR1 :B : x2 Þ-R2 :B : x05

f0

h ðR1 :C : Y 2 Þ-R2 :C : Y 03
It can be seen that, since formula homomorphisms map
variable occurrences into variable occurrences, they may
relate occurrences of the same variable on the lefthand side to occurrences of different variables on the
right-hand side. In the following, we shall refer to the
f
f
variable occurrence h ðRl :Aj : vi Þ by the syntax Aj : h l ðvi Þ,
R :Aj
f
so that hRl :A ðvi Þ will be the variable whose occurrence is
j

687

associated with occurrence Rl :Aj of vi. Consider for
f
example h above. The two occurrences of variable x1 in
jl are mapped to occurrences of different universal
f
f
variables in j0l ; in fact, hR1 :A ðx1 Þ ¼ x04 , while hT 2 :B ðx1 Þ ¼ x07 .
Parallel to the classiﬁcation of homomorphisms among
facts introduced in Section 5, it is useful to classify
formula homomorphisms in several categories, as follows.
Deﬁnition 9 (Classiﬁcation of formula homomorphisms).
Given two formulas jl ðx,yÞ and j0l ðx 0 ,y 0 Þ, and a formula
f
homomorphism h from occðjl ðx,yÞÞ to occðj0l ðx 0 ,y 0 ÞÞ:
f

(i) h is compacting if it is surjective, and either
9j0l ðx 0 ,y 0 Þ9 o9jl ðx,yÞ9 or 9y 0 9 o9y9, i.e., either j0l ðx 0 ,y 0 Þ
is smaller than jl ðx,yÞ or it contains less existential
variables;
f
(ii) h is said to be proper if it is injective but not
surjective, i.e., there is at least one atom in j0l ðx 0 ,y 0 Þ
which is not the image of an atom of jl ðx,yÞ.
f

In our examples above, formula homomorphism h is
f0
compacting, while h is proper.
It is very important to discuss the relationship between
formula homomorphisms and the corresponding homomorphisms among facts. In essence, we study formula
homomorphisms in order to detect possible homomorphisms among facts that are instances of the formulas. However, the presence of a formula homomorphism does not
guarantee that actual homomorphisms arise among facts.
Notice, in fact, that homomorphisms among formulas map
occurrences of a universal variable into occurrences of other
universal variables. However, these variables do not necessarily receive the same values when the formulas are
instantiated. Therefore, the homomorphism may be ‘‘realized’’ or not among formula instances depending on values
assumed by the universal variables.
f
Consider for example the formula homomorphism h
1
2
3 0
l
0l
0
0
of j ¼ R ðx1 ,x2 ,Y 1 Þ4T ðx3 ,x1 ,Y 1 Þ and j ¼ R ðx4 ,x5 ,x6 Þ4
T 4 ðx09 ,x07 ,x08 Þ, above. Consider now two instances of the
formulas: w ¼ fRð1; 2,N0 Þ,Tð3; 1,N0 Þg and w0 ¼ fRð1; 2,4Þ,
Tð3; 1,4Þg, respectively. Given this assignments of values
to the variables, it can be seen that the set of facts w has in
fact a (compacting) homomorphism into w0 . This is in
general not true if we change the assignments.
Consider now instances: w00 ¼ fRð1; 2,N0 Þ,Tð3; 1,N0 Þg and
w000 ¼ fRð1; 4,6Þ, Tð3; 5,7Þg, respectively. There is no homomorphism of w00 into w000 . The reason for this is twofold. (i)
First, the formula homomorphism maps the occurrence
of x2 into the occurrence of x05 . By doing this, the formula
homomorphism is imposing a restriction on the values of
variables x2 ,x05 : in order to realize the homomorphism
among formula instances, it must be the case that the two
f
variables receive the same value. (ii) Second, h maps the
0
0
two occurrences of N0 into occurrences of x6 ,x8 ; this means
that, in order for the homomorphism to be realized, it is
necessary that the value of x06 equals that of x08 .
f
More formally, given a formula homomorphism h , we
may introduce several sets of equalities among universal
f
variables that are associated with h :
(a) the set intersecthf states the set of equalities among
universal variables of jl ðx,yÞ and universal variables of

688

G. Mecca et al. / Information Systems 37 (2012) 677–711

j0l ðx 0 ,y 0 Þ that must hold to realize the homomorphism
among instances of the two formulas:

E11 ¼ S1 ðx1 ,Y 1 ,Y 2 Þ4S2 ðx2 ,x3 ,Y 2 Þ

f

intersecthf ðx,x 0 Þ ¼ fxi ¼ x0j 9h ðR:A : xi Þ

E12 ¼ S2 ðx2 ,x3 ,Y 2 Þ4(Y 1 : ðS1 ðx2 ,Y 1 ,Y 2 ÞÞ

¼ R:A : x0j ,xi 2 x, x0j 2 x 0 g
(b) the set joinshf states the set of equalities among
universal variables of j0l ðx 0 ,y 0 Þ whose occurrences are
images of occurrences of the same existential variable in
jl ðx,yÞ
joinshf ðx

0

f
Þ ¼ fx0h ¼ x0l 9x0h ¼ hRi :Aj ðyk Þ,

x0l ¼

The set expansionsM ðm1 Þ contains the following three
expansions (in simpliﬁed form):

f
hRn :Am ðyk Þ,yk

2 yg

The set equalhf will be the union of the two, as
follows:
equalhf ðx,x 0 Þ ¼ intersecthf ðx,x 0 Þ [ joinshf ðx 0 Þ
Intuitively, a formula homomorphism can be realized
only by assignments that satisfy the equalities in
equalhf ðx,x 0 Þ. In our example, we have that
equalhf ðx,x 0 Þ ¼ fx1 ¼ x04 ,x2 ¼ x05 ,x3 ¼ x09 ,x1 ¼ x07 g [ fx06 ¼ x08 g
This notion is made more precise in Appendix C, where
we provide several results on the existence of formula and
fact homomorphisms.

E13 ¼ S3 ðx4 ,x5 ,Y 3 Þ4(Y 1 ,Y 2 : ðS1 ðx4 ,Y 1 ,Y 2 Þ4S2 ðx4 ,x5 ,Y 2 ÞÞ
Similarly for expansionsM ðm2 Þ

E21 ¼ S3 ðx4 ,x5 ,Y 3 Þ4S4 ðY 4 ,x5 ,Y 3 Þ
E22 ¼ S3 ðx4 ,x5 ,Y 3 Þ
E23 ¼ S2 ðx2 ,x3 ,Y 2 Þ4(Y 3 ,Y 4 : ðS3 ðx2 ,x3 ,Y 3 Þ4S4 ðY 4 ,x3 ,Y 3 ÞÞ
Notice that an expansion E can be also considered as a
query Eðx 1 ,y 1 Þ with free variables x 1 ,y 1 . In fact, we will
shortly prove that the result of evaluating such queries on
a solution J 2 USolM ðIÞ returns exactly a set of witness
blocks in W /I,J S .
More formally, given an instance J, and an assignment
a1 for x 1 ,y 1 , we say that JFa1 ðEðx 1 ,y 1 ÞÞ if the following
holds: (i) JFa1 ðwl ðx 1 ,y 1 ÞÞ; (ii) there exists an assignment
l
a2 such that JFa2 ðc ðx 2 ,y 2 ÞÞ; (iii) a1 ,a2 are such that
equalhf ða1 ðx 1 Þ,a2 ðx 2 ÞÞ evaluates to true.
E
Algorithm 1 describes how to generate the set
expansionsM ðmÞ.
Algorithm 1.

6.3. Deﬁning and ﬁnding expansions
We are now ready to introduce the notion of an
expansion. Given a scenario M ¼ ðS,T, Sst Þ, we shall denote
S
by R ¼ i ci ðx i ,y i Þ the union of all tgd conclusions in Sst
and by Rpow
the set of all multisets of atoms in R of size k
k
or less; whenever multiple copies of the same atom
appear in a multiset, we assume that they have been
properly renamed to avoid variable collisions.

FINDING EXPANSIONS
Input:

a scenario M ¼ ðS,T, Sst Þ,
a tgd m: 8x : f ðx Þ-(y ðc ðx ,y ÞÞ 2 Sst
Output: the set of expansions expansionsM ðmÞ
Let expansionsM ðmÞ ¼ |
S l
Let R ¼ fci ðx i ,y i Þ98x i : fi ðx i Þ-(y i ðci ðx i ,y i ÞÞ 2 Sst g
Let k ¼ 9c ðx ,y Þ9
l

Deﬁnition 10 (Expansions of a Tgd). Given a scenario M
l
and a tgd m : fðx 2 Þ-(y 2 ðc ðx 2 ,y 2 ÞÞ in Sst , the set of
expansions of m wrt M, denoted by expansionsM ðmÞ, is
the set of logical formulas of the form


^
E ¼ wl ðx 1 ,y 1 Þ4(x 2 ,y 2 : cl ðx 2 ,y 2 Þ equalhf ðx 1 ,x 2 Þ
E

l

Rpow
k

(k is
where w ðx 1 ,y 1 Þ is a multiset of labeled atoms in
l
the size of c ðx 2 ,y 2 Þ) and there exists a surjection
f
hE : cðx 2 ,y 2 Þ-wðx 1 ,y 1 Þ.
Without loss of generality, in the following we shall
assume that expansions are such that x 1 \ x 2 ¼ |,
y 1 \ y 2 ¼ |, i.e., x 1 ,x 2 (y 1 ,y 2 , respectively) are disjoint.
We report here the tgds of Example 5.2, and the set of
their expansions
m1 : Aðx1 ,x2 ,x3 Þ-(Y 1 ,Y 2 : S1 ðx1 ,Y 1 ,Y 2 Þ4S2 ðx2 ,x3 ,Y 2 Þ
m2 : Bðx4 ,x5 Þ-(Y 3 ,Y 4 : S3 ðx4 ,x5 ,Y 3 Þ4S4 ðY 4 ,x5 ,Y 3 Þ

l

Rename variables in c ðx ,y Þ as c ðx 2 ,y 2 Þ
be the set of all (renamed) multisets of atoms in R of size k
Let Rpow
k
or less
For each wl ðx 1 ,y 1 Þ 2 Rpow
k
f

l

If there exists a surjection h : c ðx 2 ,y 2 Þ-wðx 1 ,y 1 Þ
Let intersecthf ðx 1 ,x 2 Þ ¼ |
l

For each Rl :Ai : x2j 2 u-occðc ðx 2 ,y 2 ÞÞ
intersecthf ðx 1 ,x 2 Þ ¼ intersecthf ðx 1 ,x 2 Þ [ fx2j ¼ h

f
ðx Þg
Rl :Ai 2j

Let joinshf ðx 1 Þ ¼ |
l

For each pair Rli :Aj : y2k ,Rln :Am : y2k in e-occðc ðx 2 ,y 2 ÞÞ
f

ðRli :Aj

f

: y2k Þ and h ðRln :Am : y2k Þ are in u-occð l ðx 1 ,y 1 ÞÞ
such that h
f
f
joinshf ðx 1 Þ ¼ joinshf ðx 1 Þ [ fhRl :A ðy2k Þ ¼ hRl :A ðy2k Þg

w

i

j

n

m

Let equalhf ðx 1 ,x 2 Þ ¼ intersecthf ðx 1 ,x 2 Þ [ joinshf ðx 1 Þ
expansionsM ðmÞ ¼ expansionsM ðmÞ[
V
l
fwl ðx 1 ,y 1 Þ4(x 2 ,y 2 : ðc ðx 2 ,y 2 Þ equalhf ðx 1 ,x 2 ÞÞg

It can be seen that the number of expansions of a tgd
conclusion increases with the number of self-joins, and it is
in general exponential in the size of the input tgds. We call
expansionsðMÞ the set of all expansions of the tgds in Sst .

G. Mecca et al. / Information Systems 37 (2012) 677–711

689



6.4. Normalizing expansions

E0 ¼ w0l ðx 01 ,y 01 Þ4(x 02 ,y 02 : c0l ðx 02 ,y 02 Þ

As we discussed in Section 5, witness blocks do not need
to coincide with fact blocks. More speciﬁcally, a witness block
may be in some cases a fact block of the core solution (i.e., a
connected component of the dual Gaifman graph of the core),
but it may also be the union of portions of fact blocks, whose
facts are joined over constants instead of nulls.
This feature of witness blocks has a direct counterpart
in their expansions. In fact, it is easy to see that expansions may be non-normalized. Recall that we say that a
formula is normalized if its dual Gaifman graph is connected. Since, as discussed in Section 4, we require that
rules are in normal form, in this section we introduce a
normalization procedure for expansions.
More speciﬁcally, given an expansion E 2 expansionsðMÞ,
we generate a new set of normalized expansion components,
normalizeðEÞ ¼ normalizeðwðx 1 ,y 1 ÞÞ. Recall from Section 4
that, given a formula jðx,yÞ, normalizeðjðx,yÞÞ generates
the connected components in the dual Gaifman graph of
jðx,yÞ.
Consider the scenario in Example 5.3. Here are two
expansions in expansionsðMÞ (we report them partially to
simplify the example)

Ea ¼ S1 ðx3 ,x0 ,x0 ,Y 0 ,x1 Þ4S2 ðY 1 ,x0 ,x0 ,Y 0 ,x0 Þ4( . . .
Eb ¼ S1 ðx3 ,x0 ,x0 ,Y 0 ,x1 Þ4S1 ðx3 ,x00 ,x00 ,Y 00 ,x01 Þ4( . . .
Of these expansions, Ea is normalized; on the contrary, Eb is
not normalized (the two atoms do not join on an existential
variable, but rather on x3). Therefore, we generate the
following set of normalized expansion components:
normalizeðEb Þ ¼ ffSðx3 ,x0 ,x0 ,Y 0 ,x1 Þg,fSðx3 ,x00 ,x00 ,Y 00 ,x01 Þgg

^


equalhf ðx 01 ,x 02 Þ
E0

(i) E0 is more compact than E if there exists a compacting
f
homomorphism hc : wl ðx 1 ,y 1 Þ-w0l ðx 01 ,y 01 Þ; in symbols:
0
E !E ;
(ii) E0 is more informative than E if there exists a proper
f
homomorphism hp : wl ðx 1 ,y 1 Þ-w0l ðx 01 ,y 01 Þ; in symbols:
0
EoE .

Consider our running example. As we have discussed
above, among the expansions of tgd m1 we ﬁnd

E11 ¼ S1 ðx1 ,Y 1 ,Y 2 Þ4S2 ðx2 ,x3 ,Y 2 Þ
E13 ¼ S3 ðx4 ,x5 ,Y 3 Þ4(x1 ,x2 ,x3 ,Y 1 ,Y 2 : ðS1 ðx1 ,Y 1 ,Y 2 Þ
4S2 ðx2 ,x3 ,Y 2 Þ4x4 ¼ x1 4x4 ¼ x2 4x5 ¼ x3 Þ
It can be seen that there is a compacting formula homof
morphism hc of wl11 ¼ S1 ðx1 ,Y 1 ,Y 2 Þ4S2 ðx2 ,x3 ,Y 2 Þ into wl13 ¼
3
S ðx4 ,x5 ,Y 3 Þ. This is a signal that E13 may generate witness
blocks that are more compact than those generated
by E11 . And in fact, this is the case in our example: w11 ¼
fSð1,N2 ,N3 Þ,Sð1; 2,N3 Þg is made redundant by w13 ¼
fSð1; 2,N4 Þg.
In order to remove witness block w11 from the extent
of E11 , we may think of rewriting E11 by adding the
negation of E13 , with the appropriate equalities suggested
f
by their compacting homomorphism, hc , as follows:

E11 ðx 11 ,y 11 Þ4:(x 13 ,y 13 ðE13 ðx 13 ,y 13 Þ4equalhf ðx 11 ,x 13 ÞÞ
c

The actual formula is the following:
S1 ðx1 ,Y 1 ,Y 2 Þ4S2 ðx2 ,x3 ,Y 2 Þ4:(x04 ,x05 ,Y 03 : ðS3 ðx04 ,x05 ,Y 03 Þ
4(x001 ,x002 ,x003 ,Y 001 ,Y 002 : ðS1 ðx001 ,Y 001 ,Y 002 Þ4S2 ðx002 ,x003 ,Y 002 Þ
4x04 ¼ x001 4x04 ¼ x002 4x05 ¼ x003 Þ4x1 ¼ x04 4x2 ¼ x04 4x3 ¼ x05 Þ

7. Expansions and the core
Our goal in this section is to introduce an alternative
characterization of the core that mimics the one given in
Section 5 but relies on expansions. In order to do this, we
build on the intuition that expansions of a tgd allow us to
identify all witness blocks for that tgd. However, as shown
in Theorem 1, in order to generate core solutions, we need
to select only maximal witness blocks, ﬁrst the most
compact ones, and then the most informative among
these. In the next subsections we discuss these aspects.
7.1. More compact and more informative expansions
Formula homomorphisms may help us to identify
when an expansion generates witness blocks that are
more compact or more informative than another. In fact,
we introduce a parallel deﬁnition of a more compact and
more informative expansion.
Deﬁnition 11 (More compact and more informative expansions). Given expansions


^
E ¼ wl ðx 1 ,y 1 Þ4(x 2 ,y 2 : cl ðx 2 ,y 2 Þ equalhf ðx 1 ,x 2 Þ
E

The important observation is that this formula is a new
query over the canonical universal solution J; differently
from the original expansion, E11 , only witness blocks that
belong to the core satisfy this new query.
Notice that in some cases, given expansions E and E0 ,
there can be different formula homomorphisms of wl ðx 1 ,y 1 Þ
into w0l ðx 01 ,y 01 Þ. Consider as an example wl ¼ R1 ðx1 ,Y 1 Þ, w0l ¼
R2 ðx2 ,Y 2 Þ4R3 ðx3 ,Y 2 Þ. It can be seen that a ﬁrst proper
formula homomorphism maps R1 ðx1 ,Y 1 Þ to R2 ðx2 ,Y 2 Þ, while
a second one maps R1 ðx1 ,Y 1 Þ to R3 ðx3 ,Y 2 Þ. We write E!hf E0
(E o hf E0 ) when we need to make explicit that E0 is more
compact (more informative, respectively) than E according
f
to formula homomorphism h .
Following this approach, given an expansion E, we
generate a new query based on E, called mostComp ðEÞ,
by adding to E the negation of each expansion E0 that is
more compact than E, with the appropriate equalities, as
follows.
Deﬁnition 12 (Selecting most-compact witness blocks:
mostCompðMÞ). Given a scenario M, its set of expansions
expansionsðMÞ, and an expansion


^
E ¼ wl ðx 1 ,y 1 Þ4(x 2 ,y 2 : cl ðx 2 ,y 2 Þ equalhf ðx 1 ,x 2 Þ
E

690

G. Mecca et al. / Information Systems 37 (2012) 677–711

in expansionsðMÞ the formula mostComp ðEÞ is obtained as
follows:
(i) initialize mostComp ðEÞ ¼ E;
(ii) for any E0 ¼ w0l ðx 0 ,y 0 Þ4(x 0 ,y 0 : ðc0l ðx 0 ,y 0 Þ V equal f
1 1
2 2
2 2
h

E0

ðx 01 ,x 02 ÞÞ

in

expansionsðMÞ

and

any

compacting

f

formula homomorphism hc such that E!hf E0 , i.e., E0
c

E, and the associated query Eðx 1 ,y 1 Þ, both formulas
mostComp ðEÞ and mostInf ðEÞ can be seen as queries with
free variables x 1 ,y 1 . We write these queries as follows:
mostComp ðEÞðx 1 ,y 1 Þ, and mostInf ðEÞðx 1 ,y 1 Þ. However, to
simplify the notation, whenever it is possible we will omit
to explicitly reference variables.
8. Rewriting expansions over the source

f

is more compact than E according to hc , add to
mostComp ðEÞ a formula
 ^

equalhf ðx 1 ,x 01 Þ
4:(x 01 ,y 01 : E0
c

We shall denote by mostCompðMÞ the set of all rewritings of the form mostComp ðEÞ, E 2 expansionsðMÞ.
After this ﬁrst rewriting, coherently with the strategy
outlined in Theorem 1, we look among other expansions
to favor those that generate more informative witness
blocks in the target, and we further rewrite mostComp ðEÞ
accordingly. In doing this, we generate a further formula,
called mostInf ðEÞ, as follows:
Deﬁnition 13 (Selecting most-informative witness blocks:
mostInfðMÞ). Given a scenario M, its set of expansions
expansionsðMÞ, and an expansion


^
E ¼ wl ðx 1 ,y 1 Þ4(x 2 ,y 2 : cl ðx 2 ,y 2 Þ equalhf ðx 1 ,x 2 Þ
E

in expansionsðMÞ the formula mostInf ðEÞ is obtained as
follows:
(i) initialize mostInf ðEÞ ¼ mostComp ðEÞ;
(ii) for any E0 ¼ w0l ðx 0 ,y 0 Þ4(x 0 ,y 0 : ðc0l ðx 0 ,y 0 Þ V equal f
1 1
2 2
2 2
h

E0

ðx 01 ,x 02 ÞÞ in expansionsðMÞ and any proper formula
f

homomorphism hi such that E o hf E0 , i.e., E0 is more
i

f

informative than E according to hi , add to mostInf ðEÞ a
formula


^
4:(x 01 ,y 01 : mostComp ðE0 Þ equalhf ðx 1 ,x 01 Þ

An expansion is a formula over the target schema.
However, in this section we show that it is possible to
rewrite it in terms of source relations. We call this the
source rewriting of the expansion. While an expansion is a
query to select atoms in the target instance, its source
rewriting states a ‘‘precondition’’ over the source for the
existence of these atoms.
8.1. Source rewritings
The strategy to rewrite expansions over the source is
quite straightforward, thanks to our labeling technique. In
fact, an expansion is a conjunction of labeled atoms taken
from the tgd conclusions. To each of these atoms we may
associate a premise, i.e., the left-hand side of the respective tgd. By joining the premises of all of its atoms we
obtain the source rewriting of an expansion. Notice how
our provenance system plays a central role during this
step: in fact, by looking at the label of each atom of an
expansion, we immediately know the tgd it comes from,
and therefore its premise.
Deﬁnition 14 (Premise of an atom and of a formula).
l
Given a tgd m: 8x : f ðx Þ-(y ðc ðx ,y ÞÞ, and an atom
l
l
R ðx i ,y i Þ 2 c ðx ,y Þ, its premise, premiseðRl ðx i ,y i ÞÞ, is the formula fðxÞ.
Given a conjunctive formula, wl ðx 1 ,y 1 Þ, its premise is the
formula:
^
fpremiseðRli ðx i ,y i ÞÞ9Rli ðx i ,y i Þ
premiseðwl ðx 1 ,y 1 ÞÞ ¼
2 wl ðx 1 ,y 1 Þg

i

We shall denote by mostInfðMÞ the set of all rewritings of the form mostInf ðEÞ, for every E 2
expansionsðMÞ.
To summarize, in order to select maximal witness
blocks, we consider each expansion E of a tgd m; then:
(a) we ﬁrst rewrite E into a new formula mostComp ðEÞ by
adding the negation of all expansions Ei such that Ei is
more compact than E; we expect these new formulas
to select the most-compact witness blocks among
those associated with E;
(b) then, we further rewrite mostComp ðEÞ into a new
formula mostInf ðEÞ by adding the negation of
mostCompðEj Þ, for all expansions Ej such that Ej is
more informative than E.

Deﬁnition 15 (Source rewriting). Given a tgd m: fðx 2 Þl
(y 2 ðc ðx 2 ,y 2 ÞÞ and an expansion E ¼ wl ðx 1 ,y 1 Þ4 (x 2 ,y 2 :
V
l
ðc ðx 2 ,y 2 Þ equalhf ðx 1 ,x 2 ÞÞ in expansionsM ðmÞ, its source
E
rewriting, sourceRew ðEÞ, is the following formula:
sourceRew ðEÞ ¼ premiseðwl ðx 1 ,y 1 ÞÞ
^
4(x 2 : ðfðx 2 Þ equalhf ðx 1 ,x 2 ÞÞ
E

Notice that, while an expansion E of a tgd m can be
seen as a query Eðx 1 ,y 1 Þ whose clude both universally
and existentially quantiﬁed variables of m, its source
rewriting, sourceRew ðEÞ, is on the contrary a query
sourceRew ðEÞðx 1 Þ in which all the free variables are
universally quantiﬁed variables of m.
Consider the scenario in Example 5.1
m1 : 8x1 ,x2 : Aðx1 ,x2 Þ-(Y 1 : S1 ðx1 ,Y 1 Þ4T 2 ðY 1 ,x2 Þ
m2 : 8x3 ,x4 : Bðx3 ,x4 Þ-S3 ðx3 ,x4 Þ

Similarly to expansions, also their rewritings can be
considered as queries. More speciﬁcally, given an expansion

m3 : 8x5 ,x6 : Cðx5 ,x6 Þ-T 4 ðx5 ,x6 Þ

G. Mecca et al. / Information Systems 37 (2012) 677–711

m4 : 8x7 : Dðx7 Þ-(Y 0 : S5 ðx7 ,Y 0 Þ
Tgd m1 has the following expansions:

E11 ¼ S1 ðx1 ,Y 1 Þ4T 2 ðY 1 ,x2 Þ ðthe base expansionÞ
E12 ¼ S3 ðx3 ,x4 Þ4T 4 ðx5 ,x6 Þ4x4
¼ x5 4(Y 1 : ðS1 ðx3 ,Y 1 Þ4T 2 ðY 1 ,x6 ÞÞ
Their source rewritings are as follows:
sourceRew ðE11 Þ ¼ Aðx1 ,x2 Þ
sourceRew ðE12 Þ ¼ Bðx3 ,x4 Þ4Cðx5 ,x6 Þ4x4
¼ x5 4(x1 ,x2 :ðAðx1 ,x2 Þ4x3 ¼ x1 4x6 ¼ x2 Þ
¼ Bðx3 ,x4 Þ4Cðx5 ,x6 Þ4x4 ¼ x5 4ðAðx3 ,x6 ÞÞ

691

Deﬁnition 16 (Rewriting mostComp ðÞ over the source).
Given a scenario M, its set of expansions expansions
ðMÞ, their rewritings mostCompðMÞ, for each E in
expansionsðMÞ, the formula sourceRew ðmostComp ðEÞÞ is
obtained as follows:
(i) initialize sourceRew ðmostComp ðEÞÞ ¼ sourceRew ðEÞ;
(ii) for any expansion E0 in expansionsðMÞ such that E0
f
is more compact than E, call hc the compacting
l
homomorphism of w ðx 1 ,y 1 Þ into w0l ðx 01 ,y 01 Þ; add to
sourceRew ðmostComp ðEÞÞ a formula


^
4:(x 01 : sourceRew ðE0 Þ equalhf ðx 1 ,x 01 Þ
c

Deﬁnition 17 (Rewriting mostInf ðÞ over the source). Given
a scenario M, its set of expansions expansionsðMÞ,
their rewritings mostInfðMÞ, for each expansion E in
expansionsðMÞ, the formula sourceRew ðmostInf ðEÞÞ is
obtained as follows:

Similarly for tgd m4

E41 ¼ S5 ðx7 ,Y 0 Þ ðthe base expansionÞ
sourceRew ðE41 Þ ¼ Dðx7 Þ
E42 ¼ S1 ðx1 ,Y 1 Þ4(Y 0 : ðS5 ðx1 ,Y 0 ÞÞ
sourceRew ðE42 Þ ¼ Aðx1 ,x2 Þ4ðDðx1 ÞÞ
E43 ¼ S3 ðx3 ,x4 Þ4(Y 0 : ðS5 ðx3 ,Y 0 ÞÞ
sourceRew ðE43 Þ ¼ Bðx3 ,x4 Þ4ðDðx3 ÞÞ
Consider the scenario in Example 5.2
m1 : Aðx1 ,x2 ,x3 Þ-(Y 1 ,Y 2 : S1 ðx1 ,Y 1 ,Y 2 Þ4S2 ðx2 ,x3 ,Y 2 Þ

(i) initialize
sourceRew ðmostInf ðEÞÞ ¼ sourceRew ðmostComp ðEÞÞ;
(ii) for any expansion E0 in expansionsðMÞ such that E0
f
is more informative than E, call hi the proper
homomorphism of wl ðx 1 ,y 1 Þ into w0l ðx 01 ,y 01 Þ; add to
sourceRew ðmostInf ðEÞÞ a formula:


^
4:(x 01 : sourceRew ðmostComp ðEÞ0 Þ equalhf ðx 1 ,x 01 Þ
i

m2 : Bðx4 ,x5 Þ-(Y 3 ,Y 4 : S3 ðx4 ,x5 ,Y 3 Þ4S4 ðY 4 ,x5 ,Y 3 Þ

Consider, again, Example 5.1 above. Since we know
that E12 is more compact than E11 , the formula
sourceRew ðmostComp ðE11 ÞÞ will be as follows:

Here are some expansions and their source rewritings

sourceRew ðmostComp ðE11 ÞÞ ¼ Aðx1 ,x2 Þ

E11 ¼ S1 ðx1 ,Y 1 ,Y 2 Þ4S2 ðx2 ,x3 ,Y 2 Þ

4:(x03 ,x04 ,x05 ,x06 : ðBðx03 ,x04 Þ4Cðx05 ,x06 Þ4x04 ¼ x05
4ðAðx03 ,x06 Þ4x1 ¼ x03 4x2 ¼ x06 ÞÞ

E12 ¼ S2 ðx2 ,x3 ,Y 2 Þ4(Y 1 : ðS1 ðx2 ,Y 1 ,Y 2 ÞÞ
3

1

¼ Aðx1 ,x2 Þ4:(x04 ,x05 : ðBðx1 ,x04 Þ4Cðx05 ,x2 Þ4x04 ¼ x05 Þ
2

E13 ¼ S ðx4 ,x5 ,Y 3 Þ4(Y 1 ,Y 2 : ðS ðx4 ,Y 1 ,Y 2 Þ4S ðx4 ,x5 ,Y 2 ÞÞ
sourceRew ðE11 Þ ¼ Aðx1 ,x2 ,x3 Þ

In this speciﬁc case, sourceRew ðmostInf ðE11 ÞÞ ¼
sourceRew ðmostComp ðE11 ÞÞ, since there is no expansion
that is more informative than E11 .

sourceRew ðE12 Þ ¼ Aðx1 ,x2 ,x3 Þ4x1 ¼ x2
sourceRew ðE13 Þ ¼ Bðx4 ,x5 Þ4ðAðx4 ,x4 ,x5 ÞÞ

8.2. Adding negations to source rewritings
Given an expansion E, its source rewriting, sourceRew ðEÞ
tells us a precondition for the generation of all of its witness
blocks. However, we want to select only the maximal ones
with respect to the partial orders ! and o. We therefore
need to generate new expressions that give us preconditions
for the set of most compact and most informative witness
blocks associated with an expansion, respectively.
In order to do this, given E, we now introduce
the formulas sourceRew ðmostComp ðEÞÞ and sourceRew
ðmostInf ðEÞÞ, that are analogous to mostComp ðEÞ and
mostInf ðEÞ, but are rewritten on the source. To generate
sourceRew ðmostComp ðEÞÞ, the intuition is again that
whenever expansion E0 is more compact than E, we add
to the source rewriting of E the negation of sourceRew ðE0 Þ.

9. Skolemization
We are almost ready to discuss how to build the ﬁnal
rules that will give us the needed core schema-mapping.
Our main intuition is to generate a rule for each expansion, E, in which sourceRew ðmostInf ðEÞÞ represents the
left-hand side, and wðx 1 ,y 1 Þ the right-hand side. In fact,
sourceRew ðmostInf ðEÞÞ tells us under which conditions
the set of maximal witness blocks associated with E
belong to the core, while wðx 1 ,y 1 Þ actually generates the
witness blocks into the target.
Before formalizing this idea in the next section, we
need to discuss our skolemization strategy. This is in fact
crucial in order to properly handle the presence of multiple isomorphic witness blocks.
Deﬁnition 18 (Skolemization strategy). Given a formula
jðx,yÞ with universal variables x, and existential variables
y, a skolemization strategy is a mapping skol that associates
with each existential variable yi 2 y a Skolem term f j,yi ðxÞ.

692

G. Mecca et al. / Information Systems 37 (2012) 677–711

There are several possible skolemization strategies to
pick from. The standard strategy, skolstd , would be that of
associating a different, uninterpreted Skolem function
f m,Y i with each existential variable Y i of a tgd m, and
taking as arguments all universal variables occurring in
the conclusion. However, this scheme would not be able
to properly handle isomorphisms among witness blocks.
Handling isomorphic witness blocks by means of
expansions is a tricky issue. In fact, there are two possible
sources of isomorphisms among witness blocks. The ﬁrst
one corresponds to isomorphic copies of a witness block
that are generated by different expansions. This is the
easiest one to capture. However, there is also the possibility that isomorphic witness blocks are generated by the
same expansion. As it was noted in [31], this may happen
if an expansion has non-trivial automorphisms.
Consider for example a tgd that has an expansion as
the following:

E ¼ R1 ðx1 ,Y 1 Þ4R2 ðx2 ,Y 1 Þ
it can be seen that wðx,yÞ has a non-trivial automorphism
that maps x1 into x2 and vice-versa. Therefore, the
expansion might select pairs of isomorphic witness blocks
in J of the form fRð1,N 0 Þ,Rð2,N0 Þg and fRð2,N1 Þ,Rð1,N 1 Þg.
These are difﬁcult to detect and remove.
The standard skolemization strategy can be used to
handle isomorphisms of the ﬁrst kind, but it is hopeless in
front of automorphisms of a rule conclusion. Our solution to
the problem is to use interpreted Skolem functions rather
than uninterpreted ones, and design them in such a way that
all isomorphic copies of a witness block collapse into one.
More speciﬁcally, we introduce a notion of isomorphism–
invariant skolemization strategy. In order to do this, we
compare the result of a given skolemization with that of the
standard strategy. More speciﬁcally, given a formula jðx,yÞ,
and an assignment a to x, we call aðskolðjðx,yÞÞÞ the
instance of jðx,yÞ generated as follows: (a) ﬁrst replace
each existential variable yi 2 y by the corresponding Skolem
term; this gives a new formula j0 ðxÞ that depends on x
only; (b) then, generate the set of facts aðj0 ðxÞÞ. We call
aðskolstd ðjðx,yÞÞÞ the standard instance of the formula.
Deﬁnition 19 (Isomorphism–invariant skolemization strategy). A skolemization strategy, skol, is isomorphism–
invariant if:
(i) given a formula, jðx,yÞ,and an assignment a, then
aðskolðjðx,yÞÞÞ is isomorphic to aðskolstd ðjðx,yÞÞÞ;
(ii) given two formulas, jðx,yÞ, j0 ðx 0 ,y 0 Þ, and two assignments a,a0 , if the standard instances aðskolstd ðjðx,yÞÞÞ,
a0 ðskolstd ðj0 ðx 0 ,y 0 ÞÞÞ are isomorphic, then aðskolðjðx,yÞÞÞ
¼ a0 ðskolðj0 ðx 0 , y 0 ÞÞÞ.
The deﬁnition above essentially states two requirements: (i) skol should be such that it generates formula
instances, i.e., sets of facts, that are compatible with those
that would be generated by the standard skolemization;
(ii) at the same time, whenever the standard skolemization would generate distinct, isomorphic formula
instances, skol on the contrary generates identical sets
of facts.

There are several ways in which it is possible to build
an isomorphism–invariant strategy for a tgd or an
FO-rule. Our solution is to adopt a skolemization strategy,
that we call skolG , based on an encoding of the dual
Gaifman graph of the tgd conclusion. Recall that the dual
Gaifman graph of a formula is an undirected graph in
which nodes are atoms, and there is an edge between
atoms Ri ðx i ,y i Þ and Rj ðx j ,y j Þ if there is some existential
variable yk occurring in both atoms. We may design an
isomorphism–invariant Skolem function by returning
strings that encode the structure of the graph. More
speciﬁcally, we embed in the Skolem string a full description of the Gaifman graph, in terms of constants, in such a
way that whenever two formulas generate isomorphic
blocks the strings coincide.
Consider for example the following formula: Sðx0 ,Y 0 Þ,
Tðx1 ,Y 0 ,Y 1 Þ,WðY 1 ,Y 2 Þ. The Skolem functions for Y 0 and Y 1
will have three arguments: (i) a description of the graph
nodes – i.e., of the tuples generated by the formula – in
terms of constants; (ii) an encoding of the graph edges,
i.e., of the joins associated with common existential
variables; (iii) a reference to the speciﬁc existential
variable for which the function is used. Notice that each
existential variable that appears more than once in the
formula – like Y 0 or Y 1 – corresponds to one of the joins;
on the contrary, existential variables that occur only once
are not related to joins.
In the following we report the actual Skolem strings
that are needed in order to properly replace the existential variables. We assume that these strings are generated
by an interpreted Skolem function that appends constant
strings and variable values. Notice how we use symbols t i ,
ji , v to refer to tuples, i.e., nodes in the graph, joins, i.e.,
arcs in the graph, and variables respectively
Y 0 : f sk ðt1 : S½A : x0 ,t2 : T½A : x1 ,t3 : W½ ,
j1 : ½t1:B ¼ t2:B,j2 : ½t2:C ¼ t3:A,v : j1Þ
Y 1 : f sk ðt1 : S½A : x0 ,t2 : T½A : x1 ,t3 : W½ ,
j1 : ½t1:B ¼ t2:B,j2 : ½t2:C ¼ t3:A,v : j2Þ
Y 2 : f sk ðt1 : S½A : x0 ,t2 : T½A : x1 ,t3 : W½ ,
j1 : ½t1:B ¼ t2:B,j2 : ½t2:C ¼ t3:A,v : t3:BÞ
Notice that, differently from standard Skolem terms, we
are using a global function symbol, f sk , rather than
different symbols for each tgd. The main intuition behind
this strategy is that tgd conclusions that may generate
isomorphic blocks are essentially identical up to the
renaming of nulls. Therefore, they have isomorphic dual
Gaifman graphs, and the strings generated by such encodings are identical.
At the same time, the skolemization strategy is compatible with the standard one. In fact, for each tgd m and
existential variable Y, it generates a Skolem term that is
associated with m, since it encodes the structure of the
relative graph, and Y, and that depends on the universal
variables that appear in the conclusion of m.
An important point here is that set elements in the
Skolem string must be encoded in lexicographic order, so
that the functions generate appropriate values regardless

G. Mecca et al. / Information Systems 37 (2012) 677–711

of the order in which atoms appear in the rule conclusion.
This last requirement introduces further subtleties in the
way exchanges with self-joins are handled. In fact, note
that in formulas like the one above – in which all relation
symbols in the conclusion are distinct – the order of set
elements can be established at script generation time
(they depend on relation names). If, on the contrary, the
same atom may appear more than once in the conclusion,
then things change.
Consider for example the following formula, with a
non-trivial automorphism: Sðx0 ,Y 0 Þ, Sðx1 ,Y 0 Þ. In this case,
for Y 0 a function of this form would be generated:
f sk ðt1 : S½A : x0 ,t2 : S½A : x1 ,j1 : ½t1:B ¼ t2:B,v : j1Þ
It can be seen that by assignments x0 ¼ 0,x1 ¼ 1 and
x0 ¼ 1,x1 ¼ 0 the function would generate two different
strings, and therefore the two blocks would not be
collapsed. To avoid this, we sort the tuples at execution
time based on the actual assignment of values to the
variables
skolðY 0 Þ ¼ f sk ðsortðS½A : x0 ,S½A : x1 Þ,j1 : ½S:B ¼ S:B,v : j1Þ
in this way, on both assignments the same Skolem string
is generated: f sk ðt1 : ½SðA : 0Þ,t2 : ½SðA : 1Þ,j1 : ½S:B ¼ S:B,
v : j1Þ, and the two blocks are collapsed. To generate these
strings,
we therefore need a composition of append and sort
functions.
To do this, we assume that there is a linear order on
the constants. This is consistent with the linear-order
requirement that was introduced in [31].
10. Expansion rules
We are now ready to introduce our ﬁnal rewriting. In the
following, we introduce the notion of expansion rules for a
scenario M. Recall that we want our rules to be normalized.
Therefore, for each normalized expansion E we build one
rule, that uses the formula sourceRew ðmostInf ðEÞÞ – in
which only universally quantiﬁed variables appear – as a
premise, and skolG ðwðx 1 ,y 1 ÞÞ as a conclusion, where skolG is
the skolemization strategy introduced in the previous section. In case E is not normalized, we generate several
normalized rules, one for each normalized component in
normalizeðEÞ:
Deﬁnition 20 (Expansion rules). For each expansion E in
expansionsðMÞ we generate a set of expansion rules,
expansionRule ðEÞ, of the form:
8x 1 : sourceRew ðmostInf ðEÞÞðx 1 Þ-skolG ðwi ðx 1 ,y 1 ÞÞ
where wi ðx 1 ,y 1 Þ is a normalized component in normalizeðEÞ.
The set of expansion rules of a scenario M is the set

SeM ¼ fexpansionRule ðEÞ9E 2 expansionsðMÞg
As discussed in the previous sections, a straightforward optimization consists of generating expansion rules
only for expansions whose set of atoms is a core.
Consider Example 5.1. Given expansion

E11 ¼ S1 ðx1 ,Y 1 Þ4T 2 ðY 1 ,x2 Þ

693

i.e., the base expansion of tgd m1 , we have shown in
Section 8 that
sourceRew ðmostInf ðE11 ÞÞ ¼ Aðx1 ,x2 Þ4:(x04 ,x05 : ðBðx1 ,x04 Þ
4Cðx05 ,x2 Þ4x04 ¼ x05 Þ
Then, we generate the rule
r 1 : 8x1 ,x2 : Aðx1 ,x2 Þ4:ð(x04 ,x05 : Bðx1 ,x04 Þ4Cðx05 ,x2 Þ4x04 ¼ x05 Þ
-Sðx1 ,skolG ðY 1 ÞÞ4TðskolG ðY 1 Þ,x2 Þ:
Consider now the scenario in Example 5.3. Here are two
expansions in expansionsðMÞ (we report them partially to
simplify the example)

Ea ¼ S1 ðx3 ,x0 ,x0 ,Y 0 ,x1 Þ4S2 ðY 1 ,x0 ,x0 ,Y 0 ,x0 Þ4( . . .
Eb ¼ S1 ðx3 ,x0 ,x0 ,Y 0 ,x1 Þ4S1 ðx3 ,x00 ,x00 ,Y 00 ,x01 Þ4( . . .
As we have already noted, Eb is not normalized. Therefore,
our algorithm generates the following set of rules:
r a : sourceRew ðmostInf ðEa ÞÞ-skolG ðSðx3 ,x0 ,x0 ,Y 0 ,x1 Þ
4SðY 1 ,x0 ,x0 ,Y 0 ,x0 ÞÞ
r b1 : sourceRew ðmostInf ðEb ÞÞ-skolG ðSðx3 ,x0 ,x0 ,Y 0 ,x1 ÞÞ
r b2 : sourceRew ðmostInf ðEb ÞÞ-skolG ðSðx3 ,x00 ,x00 ,Y 00 ,x01 ÞÞ
We call rules like r b1 ,r b2 decomposed rules. Notice that, by
doing this, we may end up with different rules that generate
the same portions of a witness block. As a consequence, some
further processing is needed. In our example, rule r a generates a witness block that is also a fact-block. On the
contrary, r b1 and r b2 generate fragments of the same witness
block. As a consequence, we need to ﬁre r b1 ,r b2 only as long
as a more-informative atom is not generated by r a .
To handle non-normalized blocks, we look for proper
homomorphisms among rule conclusions: we extend the
notion of formula homomorphism to skolemized formulas,
by considering Skolem terms as existentially quantiﬁed
variable; then, we introduce a further rewriting, as follows:
Deﬁnition 21 (Final rewriting: finalRew ðÞ). For each rule r
2 SeM , the rule finalRew ðrÞ is obtained as follows: (a) if r is
not a decomposed rule, then finalRew ðrÞ ¼ r; (b) otherwise, if r is a decomposed rule of the form r:fðxÞ-cðxÞ:
(i) initialize finalRew ðrÞ ¼ r;
0
0
0
(ii) for any rule r 0 :f ðx 0 Þ-c ðx 0 Þ in SeM such that c ðx 0 Þ is
more informative than cðxÞ according to homomorphf
ism hi , add to the premise of finalRew ðrÞ a formula
^
0
4:(x 0 : ðf ðx 0 Þ equalhf ðx,x 0 ÞÞ
i

This gives us a new set of FO-rules, obtained as
follows:

ScM ¼ ffinalRew ðrÞ9r 2 SeM g
11. The rewriting algorithm
We are now ready to summarize the actual rewriting
algorithm. The algorithm takes as input a set of s-t tgds,
Sst , and generates a set of FO-rules that is a core schemamapping for Sst .

694

G. Mecca et al. / Information Systems 37 (2012) 677–711

A comprehensive example will be discussed in
Appendix A.
(Step 1) Generating expansions: given a mapping scenario
M ¼ fS,T, Sst g, as a ﬁrst preliminary step for each
tgd m 2 Sst , we generate the set of expansions
expansionsM ðmÞ using Algorithm 1 in Section 6;
the union of these sets of expansions gives us
the set of all expansions of M, expansionsðMÞ;
(Step 2) Partial orders among expansions: as a second
preliminary step, we analyze expansions in
expansionsðMÞ and ﬁnd their formula homomorphisms, and generate the more-compact
partial order, !, and the more-informative
partial order, o, among expansions, as discussed in Section 7;
(Step 3) Source rewritings of expansions: we rewrite each
expansion E in expansionsðMÞ as a formula over
the source database by computing its source rewriting, sourceRew ðEÞ, as discussed in Section 8.1;
(Step 4) Adding negations to source rewritings: since we are
interested in ﬁnding preconditions for the maximal
witness blocks associated with an expansion,
we use the partial orders among expansions
to add negations and generate the formulas sourceRew ðmostComp ðEÞÞ and sourceRew
ðmostInf ðEÞÞ, as discussed in Section 8.2;
(Step 5) Generating expansion rules: based on the results
of the previous steps, this step generates a
preliminary set of FO-rules, called expansion
rules, one for each expansion; this step uses
the skolemization strategy skolG discussed in
Section 9;
(Step 6) Adding ﬁnal negations: as a ﬁnal step, the set of
expansion rules generated at Step 5 is further
rewritten in order to guarantee that no portion
of a witness block is generated twice.
Based on this algorithm, we are ready to introduce our
main result, whose full proof is reported in Appendix C:
Theorem 2. Given a scenario M ¼ ðS,T, Sst Þ, ScM is a core
schema mapping for M.
Based on Theorem 2, our approach to the problem of
generating core universal solutions in a scalable way is
the following. Given a scenario M ¼ ðS,T, Sst Þ, we apply
the 6-step rewriting algorithm introduced in the previous
sections to generate the associated core schema-mapping,
ScM . Then, from this set of FO-rules, we generates a
runtime SQL script that can be executed on source
instances to efﬁciently generate core solutions.
12. Complexity
A few comments are worth making here on the
complexity of the rewriting algorithm. Recall that our
goal is to execute the rewritten rules under the form of
SQL scripts; in the scripts, source rewritings of expansions
give rise to joins, and negated atoms give rise to difference operators. Generally speaking, joins and differences

are executed very efﬁciently by the DBMS. However,
the number of joins and differences needed to ﬁlter out
redundant tuples largely depends on the nature of the
scenario.

12.1. Worst-case complexity
The worst-case complexity of the rewriting algorithm
in the case of tgds with self-joins is clearly exponential.
Recall that we say that a scenario M ¼ ðS,T, Sst Þ has selfjoins in tgd conclusions (or simply self-joins) if the same
relation symbol appears more than once in the conclusion
of a tgd in Sst . The scenarios in Examples (5.2)–(5.4) have
self-joins in tgd conclusions. The scenario in Example 5.1
does not have self-joins.
In fact, given a scenario M ¼ fS,T, Sst g, in the general
case, in order to generate expansions for a tgd m, we need
. This is the set of all multisets of
to generate the set Rpow
k
size k or less of atoms in the conclusions of tgds in Sst . If
we call n the total number of atoms in tgd conclusions, we
k1
have that 9Rpow
9 ¼ Ski ¼ 1 ðn þ
n1 Þ.
k
It can be seen that, since the number of candidate
expansions is exponentially large, the rewriting algorithm
may be forced to check an equally large number of
formula homomorphisms among them. Therefore, the
time complexity of the rewriting algorithm is exponential
in the size of the given scenario.
The target language of the rewriting algorithm has
polynomial data complexity, like the input language of
source-to-target tgds. However, for some classes of mappings, rewriting may result in the exponential increase in
the number of rules. To see this, let’s call mk a tgd with k
atoms in its conclusion. Consider the worst case in which
each element in Rpow
is a valid expansion for mk , and
k
therefore generates a negated subformula in the rewriting. It can be easily seen that the number of joins,
intersections and differences in the ﬁnal SQL script would
be exponentially high with respect to k. In fact, it is
possible to design synthetic scenarios that actually trigger
such exponential explosion.

12.2. Scenarios with no self-joins
The fact that rewriting may lead to an exponential
number of rules is not surprising. In fact, Gottlob and
Nash have shown that the problem of computing core
solutions is ﬁxed-parameter intractable [18] wrt the size
of the tgds (in fact, wrt the size of blocks), and therefore
it is very unlikely that the exponential bound can be
removed.
However, it is important to make a distinction
between scenarios with self-joins in tgd conclusions and
scenarios that do not have self-joins. If we restrict our
attention to scenarios that do not have self-joins, things
improve signiﬁcantly. In fact, we have the following
result.
Theorem 3. Given a scenario M ¼ ðS,T, Sst Þ, suppose Sst
does not contain self-joins in tgd conclusions. Given a source

G. Mecca et al. / Information Systems 37 (2012) 677–711

instance I, call J a canonical universal solution for M over I,
and J 0 the core universal solution for M over I. Then:
(i) for any fact block bf in J, either all tuples in bf belong
also to J 0 , or none of them does;
(ii) for each tgd m 2 Sst whose conclusion has size k, all
/I,J S
witness blocks in W m
have size exactly k.

The proof is in Appendix D. Based on Theorem 3, we
can infer several conclusions. First, if a tgd whose conclusion has size k does not have self-joins, then it may only
have ﬁxed-size expansions, i.e., sets of atoms of size exactly
k. In fact, it is easy to see that it is not necessary to
S
consider multisets of atoms in fci ðxi ,yi Þg (no atom may
appear more than once, since there are no self-joins), and
that exactly one atom in the expansion is needed to
‘‘cover’’ each of the k distinct atoms in the tgd conclusion.
Second, since, according to Theorem 3, the core is the
union of a subset of fact blocks in J, it is possible to see
that we only need to discover which fact blocks to keep
and which to discard. Since the tgds in Sst are normalized
by hypothesis, fact blocks correspond to instances of the
base expansions. Therefore, we adopt the following
strategy:
(a) for each tgd, we generate only ﬁxed-size expansions;
(b) then, we concentrate on the base expansion alone;
(c) we ﬁnd compacting and proper homomorphisms
of the base expansion into all other expansions, in
order to remove unnecessary witness blocks from the
result;
(d) ﬁnally, we generate one FO-rule for each base expansion in order to produce the result, and disregard rules
for other expansions.
Based on these ideas, we ﬁnd it useful to classify the
relevant homomorphisms among expansions in a scenario
without self-join in two categories. We call a subsumption
any compacting or proper homomorphism of a base
expansion into a base expansion. We call a coverage any
compacting or proper homomorphism of a base expansion into a ﬁxed-size expansion that uses atoms of
different tgd conclusions. Consider the tgds in Example
5.1. There is a subsumption of the base expansion of m4 ,
Sðx7 ,Y 0 Þ by the base expansion of m2 , Sðx3 ,x4 Þ. There is a
coverage of the base expansion of m1 , Sðx1 ,Y 1 Þ, TðY 1 ,x2 Þ, by
the union of atoms Sðx3 ,x4 Þ, Tðx5 ,x6 Þ. It should be apparent
from the discussion above that, if a scenario does not have
self-joins, then subsumptions and coverages are the only
relevant forms of homomorphisms that must be taken
into account in order to generate the core.
Scenarios that only contain subsumptions, and scenarios that only contain subsumptions and coverages represent two cases of non-trivial rewritings for which it is
interesting to discuss the complexity. In the ﬁrst case –
subsumptions only – the rewritten scenario contains a
number of rules that is linear wrt the original scenario,
and contains at most a quadratic number of negations.
To see this, consider a given scenario with n tgds, and
call ci ðx i ,y i Þ the conclusion of the ith tgd. We write

695

ci ðx i ,y i Þ!s cj ðx j ,y j Þ to denote the fact that the conclusion
of mi is subsumed by the conclusion of mj . Consider
now the graph of the subsumption relation obtained by
removing transitive edges. We know that !s is a partial
order. In the worst case, it is also a total order, and the
graph is a path, i.e. (for some reordering of the tgds)
c1 ð. . .Þ!s c2 ð. . .Þ!s . . . !s cn ð. . .Þ. Therefore, there are
ðnðn þ 1ÞÞ=2 subsumptions relations, i.e., Oðn2 Þ differences
in the ﬁnal SQL script.
Coverages may require to introduce an exponential
number of differences in the ﬁnal script. If we call k the
number of atoms in a tgd, and assume that each atom can
be mapped into n other atoms via homomorphisms, then
we might be forced to generate nk different coverages,
and therefore nk differences. However, such exponential
bound is reached only under rather unlikely conditions; to
see why, recall that coverages tend to follow the pattern
shown below:
m1 : Aðx1 ,x2 Þ-(Y : Rðx1 ,YÞ4SðY,x2 Þ
m2 : Bðx3 ,x4 Þ-Rðx3 ,x4 Þ
m3 : Cðx5 ,x6 Þ-Sðx5 ,x6 Þ
Note that m2 and m3 write into the key–foreign key pair,
while m1 invents a labeled null. Complexity may become
an issue, here, only if the set of tgds contains a signiﬁcant
number of other tgds like m2 and m3 which write into S
and T separately. This may happen only in those scenarios
in which a very large number of different data sources
with a poor design of foreign key relationships must be
merged into the same target. In fact, in Section 13, we
report experiments with large, randomly generated schemas; in all cases, the number of differences introduced in
the ﬁnal script by coverages remained linear wrt the
original number of tgds (see, for example, Fig. 7: at the
most, with 100 tables and 82 randomly generated tgds,
we had 12 coverages to handle).
13. Experimental results
The algorithms introduced in the paper have been
implemented in the working prototype of the þ SPICY
[25] system, written in Java. In this section, we ﬁrst study
the performance of the SQL scripts generated by our
rewriting algorithm on mapping scenarios of various
kinds and sizes. We show that the rewriting algorithm
efﬁciently computes the core, even for large databases
and complex scenarios. Then, we study the scalability of
the rewriting algorithm with respect to synthetic scenarios of increasing complexity. We show that the algorithm
scales with respect to a large number of relations and
joins. All experiments have been executed on a Intel Core
2 Duo machine with 2.4 GHz processor and 4 GB of RAM
under Linux. The DBMS was PostgreSQL 8.3.
13.1. Execution times for core computation
We selected a set of nine experiments to evaluate the
execution times of the SQL scripts generated using our
algorithms. The nine scenarios are divided in two groups.

696

G. Mecca et al. / Information Systems 37 (2012) 677–711

The ﬁrst group includes two scenarios with subsumptions
only ðs1 ,s2 Þ and two with subsumptions and coverages
ðc1 ,c2 Þ. The second group is composed of ﬁve scenarios
with self-joins ðsj1 sj5 Þ. The scenarios were taken from
the literature (s2 and sj3 from [13], sj2 from [33]), and
from variants of the basic scenarios in STBenchmark [1].
Among the scenarios with self-joins, sj4 and sj5 are the
ones with automorphisms in rule conclusions: sj4 is a
variant of Example 5.2, while sj5 has been designed
to artiﬁcially trigger the exponential complexity of the
algorithm.
The complexity of the scenarios can be described in
terms of their structure (number of tgds, atoms, joins), but
also in terms of the number of subsumptions, coverages,
and expansions in their rewritings. We report such
statistics in the tables in Figs. 2 and 3, for the ﬁrst and
the second group, respectively.
Computing times for large source instances: To study
how the algorithm performs on databases of large sizes,
we ran every scenario with ﬁve different source instances
of 10k, 100k, 250k, 500k, and 1M tuples, respectively. We
start by comparing our algorithm with an implementation
[30] of the core computation algorithm developed in [18],
made available to us by the authors. In the following we
will refer to this implementation as the ‘‘post-processing
approach’’.
A ﬁrst evidence is that the post processing approach
does not scale. We have been able to run experiments
with 1k and 5k tuples, but starting at around 10k tuples
the experiments took on average several hours. This result
is not surprising, since these algorithms exhaustively look
for endomorphisms in the canonical solution in order to
remove variables (i.e, invented nulls). For instance, our
ﬁrst subsumption scenario with 5k tuples in the source
generated 13 500 variables in the target; the post-processing algorithm took on our machine running PostgreSQL
around 7 h to compute the ﬁnal solution. It is interesting
to note that in some cases the post-processing algorithm
ﬁnds the core after only one iteration (in the previous

case, it took 3 h), but the algorithm is not able to
recognize this fact and stop the search. For all experiments, we ﬁxed a timeout of 1 h. If the experiment was
not completed by that time, it was stopped. Since none of
the scenarios we selected was executed in less than 1 h
we do not report computing times for the post-processing
algorithm in our graphs.
To put these results in perspective we need to mention
that the algorithms implemented by the core-computation engine we are considering are signiﬁcantly more
general than the ones in this paper. In fact, they allow
to handle scenarios with arbitrary target dependencies,
i.e., target tgds and egds. The poor performance
registered in our tests is strictly related to the generality
of the approach, which requires complex recursive
computations.
Still, we believe there is some interesting evidence in
this comparison. In fact, coherently with the approach
followed in this paper, in our tests we have only considered mapping scenarios without target constraints. Our
experimental results show that in this speciﬁc case our
SQL-based approach performs much better. Since we were
interested in comparing execution times of the scripts for
core solutions to those of scripts for canonical solutions,
we ran the two sets of scripts over the same scenarios
and reported the results in Figs. 4 and 5. Fig. 4 shows
execution times for the four scenarios that do not contain
self-joins in the target. As it can be seen, execution times
for all scenarios were extremely fast for both conﬁgurations. The overhead introduced by the rewriting of the
FO-rules using negations is always acceptable, with a
maximum of around 10 s for scenarios of one million
tuples.
Fig. 5 reports the results for the ﬁve scenarios with
self-joins. It can be seen that the ﬁrst three self-joins
scenarios, sj1 2sj3 , show times increasing linearly and did
scale up to 1M tuples both in the core and in the canonical
scripts executions. The difference is instead notable with
sj4 and sj5 , but is not surprising for two reasons. First,

Fig. 2. Statistics for the scenarios in the ﬁrst group.

Fig. 3. Statistics for the scenarios in the second group.

G. Mecca et al. / Information Systems 37 (2012) 677–711

697

Fig. 4. SQL scripts: execution times for the ﬁrst group.

Fig. 5. SQL scripts: execution times for the second group.

considering that many self-joins can trigger the exponential behavior discussed in the previous Section. Second,
the running time to interpret the Skolem functions ﬁlls
some of the overhead time. For these reasons, the core
computation script for sj4 took up to four times the
canonical script execution time (21 min for the 1 million
tuples source instance), while we stopped the execution
for sj5 on the biggest input (the core script took 41 min for
the 500k tuples source instance).
Quality of solutions: We now want to study to which
extent core universal solutions are more compact
than canonical solutions. To do this, we consider source
databases with different degrees of ‘‘redundancy’’. We
dropped sj5 from this comparison. For each of the remaining eight scenarios, we generated ﬁve synthetic source
instances of ﬁxed size (10K tuples) based on a pool of
values of decreasing size. This process generated different
levels of redundancy (from 0% to 40%) in the source
databases and enabled a comparison of the quality of
the two solutions. Fig. 6 shows the percent reduction in
the output size for core solutions compared to canonical
solutions. As output size, we measured the number of
tuples in the solutions. Fig. 6a shows results for the four
scenarios that do not contain self-joins in the target. As
expected, core solutions are more compact than canonical
ones in all the scenarios and this behavior becomes more
apparent with the increasing redundancy. The two subsumptions scenarios – s1 and s2 – follow the trend, but
less signiﬁcantly than the two coverage scenarios c1 and
c2 . This is not surprising, since the design of the tgds in s1
and s2 tend to generate many duplicate tuples in the
solutions, and those are removed both by the core script

and the canonical one. Fig. 6b reports the percent reductions for the four self-join scenarios. Again, core solutions
are more compact than canonical ones in all the scenarios
except sj1 . This is also expected, since sj1 is a full mapping
and no Skolem nor null values are generated in the
solution, i.e. canonical and core solution coincide.

13.2. Algorithm scalability on large scenarios
To test the scalability of our algorithm on schemas of
large size we generated a set of synthetic scenarios using
the scenario generator developed for the STBenchmark.
We generated four relational scenarios containing 20/50/
75/100 tables, with an average join path length of 3,
variance 1. Note that, to simulate real-application scenarios, we did not include self-joins. To generate complex
schemas we used a composition of basic cases with an
increasing number between 1 and 15, in particular we
used: Vertical Partitioning (3/6/11/15 repetitions), Denormalization (3/6/12/15), and Copy (1 repetition). With
such settings we got schemas varying between 11 relations with three joins and 52 relations with 29 joins. Fig. 7
summarizes the results. In the graph, we report several
values. One is the number of tgds processed by the
algorithm, with the number of subsumptions and coverages. Then, since we wanted to study how the tgd
rewriting phase scales on large schemas, we measured
the time needed to generate the SQL script. In all cases the
algorithm was able to generate the SQL script in a few
seconds. Finally, we report execution times in seconds for
source databases of 100K tuples.

698

G. Mecca et al. / Information Systems 37 (2012) 677–711

Fig. 6. Core vs canonical: size reduction in solutions.

Fig. 7. Algorithm scalability with large synthetic scenarios.

14. Related work
In this section we review some related works in the
ﬁelds of schema mappings and data exchange.
The original schema mapping generation algorithm
was introduced in [26,27] in the framework of the Clio
project. The algorithm relies on a nested relational model
to handle relational and XML data. The primary inputs are
value correspondences and foreign key constraints on the
two sources that are chased to build tableaux called
logical relations; a tgd is produced for each source and
target logical relations that cover at least one correspondence. The tgd generation algorithm we use in our system
is a generalization of the basic mapping algorithm that
captures a larger class of mappings, like self-joins [1] or
those in [2]. Note that the need for explicit joins was ﬁrst
advocated in [29].
The amount of redundancy generated by basic mappings has motivated a revision of the algorithm known as
nested mappings [17]. Intuitively, whenever a tgd m1
writes into an external target set R and a tgd m2 writes
into a set nested into R, it is possible to ‘‘merge’’ the two
mappings by nesting m2 into m1 . This reduces the amount
of redundant tuples in the target. Another attempt to
reduce the redundancy generated by basic mappings has
been proposed by [7]. Unfortunately, these approaches
are applicable only in some speciﬁc cases and do not
represent a general solution to the problem of generating
core universal solutions.
The notion of a core solution was ﬁrst introduced in
[13]; it represents a nice formalization of the notion of a
‘‘minimal’’ solution, since cores of ﬁnite structures arise in

many areas of computer science (see, for example, [20]).
Note that computing the core of an arbitrary instance is
an intractable problem [13,18]. However, we are not
interested in computing cores for arbitrary instances,
but rather for solutions of a data exchange problem; these
show a number of regularities, so that polynomial-time
algorithms exist.
In [13] the authors ﬁrst introduce a polynomial greedy
algorithm for computing the core of universal solutions,
and then a blocks algorithm. A block is a connected
component in the Gaifman graph of nulls. The block
algorithm looks at the nulls in J and computes the core
of J by successively ﬁnding and applying a sequence of
small useful endomorphisms; here, useful means that at
least one null disappears. Only egds are allowed as target
constraints.
The bounds are improved in [18]. The authors introduce various polynomial algorithms to compute the core
of universal solutions in the presence of weakly-acyclic
target tgds and arbitrary egds, that is, a more general
framework than the one discussed in this paper. The
authors prove two complexity bounds. Using an exhaustive enumeration algorithm they get an upper bound of
b
Oðvm9domðJÞ9 Þ, where v is the number of variables in J, m
is the size of J, and b is the block size of J. There exist cases
where a better bound can be achieved by relying on
hypertree decomposition techniques. In such cases, the
upper bound is Oðvm½b=2 þ 2 Þ, with special beneﬁts if the
target constraints of the data exchange scenario are LAV
tgds. One of the algorithms introduced [18] has been
revised and implemented in a working prototype [30].
The prototype uses a relational DBMS to chase tgds and

G. Mecca et al. / Information Systems 37 (2012) 677–711

egds, and a specialized engine to ﬁnd endomorphisms
and minimize the universal solution. Unfortunately, as
discussed in Section 13, the technique does not scale to
large size databases.
The problem of computing the core for a set of s-t tgds
using SQL queries has been recently studied in [9,24,31].
Ref. [9] represents an early approach at computing core
solutions for schema mappings speciﬁed by the limited class
of s-t tgds with single atomic formulas (without repetition
of existential quantiﬁed variables) in the conclusions.
The ﬁrst proposal of an algorithm for rewriting a set of
s-t tgds in order to generate core solutions was introduced
in [24]. Differences between that paper and this one are
discussed in detail in Appendix C. Here we want to note that
the algorithm presented in [24] is a two-step algorithm. In
this paper, we show that it is possible to reduce the twostep process to a single exchange. Notice how this produces
a speed-up in computing times: in fact, our rewriting
algorithm produces SQL scripts that are signiﬁcantly faster
than those reported in the experiments of [24].
After the appearance of the initial algorithm in [24],
another algorithm was independently proposed in [31].
The authors develop an algorithm to rewrite a set of s-t
tgds as a laconic mapping, that is, a new set of dependencies from which to generate an SQL script that computes
core solutions for the original scenario. There are several
differences with the approach we propose in this paper.
As a ﬁrst difference, both approaches rely on a linear
order over the underlying domain in order to perform the
rewriting. The linear order is necessary in order to break
automorphisms in tgd conclusions. However, there is
difference in the way these cases are handled by the
two algorithms. The algorithm in [31] uses side-conditions,
i.e., inequalities among universally quantiﬁed variable,
while in our approach we sort values inside Skolem terms.
Notice that the algorithm proposed in [31] is more
general than the one proposed in this paper, since it can
be applied to dependencies that make use of arbitrary
ﬁrst-order formulas in the premises, and not only conjunctive formulas. This is done by relying on a procedure
called certain(), to rewrite the certain answers of a query
on the target as a query on the source. In the paper,
the authors introduce a practical version of the algorithm
in which certain() is computed by adopting a variant of
the MiniCon [28] algorithm, which works for conjunctive formulas. They also announce [32] a more general
algorithm that, given a scenario M and an FO target query
q, computes a source query q0 deﬁning certainM,q ðÞ.3
Second, in terms of dependencies generated by the
rewriting, a laconic mapping tends to contain a lower
number of dependencies with more complex premises
with respect to a core schema mapping, which typically
contains more rules. In fact, as discussed in Section 5,
laconic mappings reason on fact-blocks and fact-block
types at a ‘‘global’’ level, while core schema mappings
reason on witness blocks at a ‘‘local’’ level, i.e., at the
tgd level.

3
Provided that the active domain of the source database contains at
least two elements.

699

Finally, with respect to the complexity of the rewriting
algorithm, we notice that laconic mappings require to
compute certain() many times – actually, a combinatorial
number of times with respect to the size of the existential
variables – for each fact-block type. This may be expensive, since computing certain() requires to run a highcomplexity algorithm (e.g. MiniCon). Our algorithm, albeit
exponential, looks for formula homomorphisms, whose
number is typically lower than those that must be
computed to generate laconic mappings. Also, in the case
in which a scenario does not contain self-joins, based on
Theorem 3, our algorithm does a minimal rewriting that is
typically faster to compute.
At the moment it is unclear how these differences
affect performances. In fact, no implementation of the
laconic mappings algorithm is currently available, so that
it was not possible to compare the performances of the
two algorithms.
Both algorithms can be used as building blocks for more
complex rewritings, such as the one introduced in [23].
We note that the notion of provenance in a schema
mappings framework has been studied in [10] under the
notion of routes. In this paper we make a more restricted
use of the notion of provenance, in order to identify atoms
in a canonical solution.
Finally, tools that somewhat resemble our expansions
have been independently developed in papers about
inverting schema-mappings. In [15], a notion of a minimal
generator is introduced as a conjunct that identiﬁes a
possible contribution to a target symbol. The purpose is
to identify how different sets of fact-blocks should be
treated when inverting the mapping. In [3], a notion of an
existential replacement is introduced as a tool to rewrite
queries posed over the target of a mapping scenario as
tools over the source. A novel exponential-time algorithm
is developed to this end. We notice that, in our approach,
the corresponding step of rewriting expansions as queries
over the source is greatly simpliﬁed by our provenancebased labeling system.

15. Conclusions
We have introduced new algorithms to compute solutions for data exchange scenarios that show how, despite
their intrinsic complexity, core solutions can be computed
very efﬁciently in practical, real-life scenarios by using
relational database engines.
We believe that this represents a concrete advancement towards the goal of adopting data exchange techniques in real-life mapping scenarios.
In this respect, there are several interesting research
problems that are worth studying in order to further
bridge the gap between the practice of schema mappings
and the theory of data exchange. A relevant one is the
development of similar algorithms to handle target constraints. Another one is the extension of the notion of data
exchange setting to nested data. A ﬁnal one is the revision
of existing schema mapping benchmarks in order to
explicitly incorporate the notion of a core solution as
the ‘‘optimal’’ solution.

700

G. Mecca et al. / Information Systems 37 (2012) 677–711

In this case, we discover that there are several compacting formula homomorphisms among expansions. In brief:

Acknowledgments
The authors would like to thank the anonymous
reviewers for their comments that helped us to improve
the presentation. Thanks also go to Vadim Savenkov and
Reinhard Pichler who made available to us an implementation of their post-processing core-computation algorithm. Finally, we are very grateful to Paolo Atzeni,
Bruno Marnette, and Donatello Santoro for their precious
comments on early drafts of the paper.
Appendix A. A rewriting example
This section is devoted to presenting a complete
example of application of the rewriting algorithm. We
will make reference to the scenario in Example 5.3

more compact : E1 !E2

E1 !E3 E2 !E3

more informative : E2 o E1
To be more speciﬁc, there are 2 different compacting homomorphisms of E2 into E3 . To see this, notice that we can map
the atoms of wl2 into those of wl3 as follows (for the sake of
brevity we omit the mappings of variable occurrences): S1
into the ﬁrst occurrence of S1 in wl2 , and S2 into the second
occurrence of S1 . But we also have an alternative compacting
formula homomorphism by the opposite mapping: S1 into
the second occurrence of S1 , S2 into the ﬁrst occurrence. We
need to consider both homomorphisms in our rewritings.
(Step 3) source rewritings of expansions

1

m1 : Aðx0 ,x1 ,x2 ,x3 Þ4Bðx3 ,x4 Þ-(Y 0 ,Y 1 ,Y 2 ,Y 3 : S ðx3 ,x0 ,Y 0 ,x1 Þ

4S2 ðY 1 ,x0 ,Y 0 ,x0 Þ4S3 ðY 1 ,x2 ,Y 2 ,Y 3 Þ

We now need to rewrite expansions as formulas over
the source database. The source rewritings are as follows:
sourceRew ðE1 Þ ¼ Aðx0 ,x1 ,x2 ,x3 Þ4Bðx3 ,x4 Þ

While this scenario contains a single tgd, still it requires
the application of all steps of the rewriting algorithm.

sourceRew ðE2 Þ ¼ Aðx0 ,x1 ,x0 ,x3 Þ4Bðx3 ,x4 Þ
4(x04 : ðAðx0 ,x1 ,x0 ,x3 Þ4Bðx3 ,x04 ÞÞ
¼ Aðx0 ,x1 ,x0 ,x3 Þ4Bðx3 ,x4 Þ

A.1. (Step 1) generating expansions
As a ﬁrst step, we generate all expansions of tgd m1 . In
this case, our algorithm generates four expansions, as
follows:

sourceRew ðE3 Þ ¼ Aðx0 ,x0 ,x2 ,x3 Þ4Bðx3 ,x4 Þ
4Aðx00 ,x01 ,x00 ,x3 Þ4Bðx3 ,x04 Þ4
(x004 : ðAðx0 ,x0 ,x00 ,x3 Þ4Bðx3 ,x004 ÞÞ

E1 ¼ S1 ðx3 ,x0 ,Y 0 ,x1 Þ4S2 ðY 1 ,x0 ,Y 0 ,x0 Þ4S3 ðY 1 ,x2 ,Y 2 ,Y 3 Þ
E2 ¼ S1 ðx3 ,x0 ,Y 0 ,x1 Þ4S2 ðY 1 ,x0 ,Y 0 ,x0 Þ4
(Y 000 . . . ðS1 ðx3 ,x0 ,Y 000 ,x1 Þ4S2 ðY 001 ,x0 ,Y 000 ,x0 Þ4S3 ðY 001 ,x0 ,Y 002 ,Y 003 ÞÞ

E3 ¼ S1 ðx3 ,x0 ,Y 0 ,x0 Þ4S1 ðx3 ,x00 ,Y 00 ,x01 Þ4
(Y 000 . . . ðS1 ðx3 ,x0 ,Y 000 ,x0 Þ4S2 ðY 001 ,x0 ,Y 000 ,x0 Þ4S3 ðY 001 ,x00 ,Y 002 ,Y 003 ÞÞ

(Step 4) adding negations to source rewritings
For each expansion Ei , based on sourceRew ðEi Þ, we
need to generate the two expressions, sourceRew
ðmostCompðEi ÞÞ, and sourceRew ðmostInf ðEi ÞÞ.
Let us ﬁrst concentrate on sourceRew ðmostComp ðEi ÞÞ.
We consider the compacting homomorphisms found at
Step 2, and introduce negations accordingly. We ﬁrst notice
that
sourceRew ðmostComp ðE3 ÞÞ ¼ sourceRew ðE3 Þ

E4 ¼ S1 ðx3 ,x0 ,Y 0 ,x0 Þ4
(Y 000 . . . ðS1 ðx3 ,x0 ,Y 000 ,x0 Þ4S2 ðY 001 ,x0 ,Y 000 ,x0 Þ4S3 ðY 001 ,x0 ,Y 002 ,Y 003 ÞÞ

Notice the difference between E3 and E4 . While E4
generates witness blocks made of a single S1 tuple that
self-joins twice in order to cover the tuples that would be
generated by S2 and S3 , E3 , on the contrary uses two
different S1 tuples, that join on the ﬁrst attribute. It is also
possible to see, however, that E4 can be considered as a
special case of E3 , when x0 ¼ x00 and x0 ¼ x01 , i.e., when the
two tuples coincide. Therefore, all witness blocks generated by E4 are also generated by E3 . We therefore discard
E4 and in the following steps only consider E1 , E2 , E3 .
(Step 2) partial orders among expansions

since there are no expansions that are more compact than E3 .
Consider now E1 ; there exists one formula homomorphism
f
h12 such that E2 is more compact than E1 ; as a consequence,
sourceRew ðmostComp ðE1 ÞÞ has the following form:
sourceRew ðmostComp ðE1 ÞÞ ¼ Aðx0 ,x1 ,x2 ,x3 Þ4Bðx3 ,x4 Þ4
:(x04 : Aðx0 ,x1 ,x0 ,x3 Þ4Bðx3 ,x04 Þ4
:( . . . negations of sourceRew ðE3 Þ
Similarly for E2 . The complete formulas are too complex to be
reported in full. In fact, from now on we shall discuss only
fragments of the rewritten formulas.
Let us now consider sourceRew ðmostInf ðEi ÞÞ. There is
only one proper formula homomorphism to be taken into
account. We therefore have:
sourceRew ðmostInf ðE1 ÞÞ ¼ sourceRew ðmostComp ðE1 ÞÞ
sourceRew ðmostInf ðE2 ÞÞ ¼ sourceRew ðmostComp ðE2 ÞÞ4

Once expansions have been derived, we analyze their
formula homomorphisms, in order to build the morecompact and more-informative partial orders.

:(x 0 : ðsourceRew ðmostComp ðE1 ÞÞ
4equalhf ðx,x 0 ÞÞ
21

G. Mecca et al. / Information Systems 37 (2012) 677–711

sourceRew ðmostInf ðE3 ÞÞ ¼ sourceRew ðmostComp ðE3 ÞÞ
(Step 5) generating expansion rules
We are ready to generate our expansion rules.
We generate one expansion rule for each expansion
Ei . The premise is represented by the formula
sourceRew ðmostInf ðEi ÞÞ; the conclusion by wi , with the
appropriate Skolem terms to replace existential variables.
However, we notice that E3 is not normalized. As a
consequence, we normalize it into two expansion rules
E3a and E3b , as follows:

701

-Sðx3 ,x0 ,skolG ðr 2 ,Y 0 Þ,x1 Þ4
SðskolG ðr 2 ,Y 1 Þ,x0 ,skolG ðr 2 ,Y 0 Þ,x0 Þ
finalRew ðr 3a Þ : sourceRew ðmostInf ðE3a ÞÞ4
:ð(x 0 : sourceRew ðmostInf ðE1 ÞÞ4equalh1 ðx,x 0 ÞÞ4
:ð(x 00 : sourceRew ðmostInf ðE2 ÞÞ4equalh2 ðx,x 00 ÞÞ
-Sðx3 ,x0 ,skolG ðr 3a ,Y 0 Þ,x0 Þ
finalRew ðr 3b Þ : sourceRew ðmostInf ðE3b ÞÞ4
:ð(x 0 : sourceRew ðmostInf ðE1 ÞÞ4equalh1 ðx,x 0 ÞÞ4
:ð(x 00 : sourceRew ðmostInf ðE2 ÞÞ4equalh2 ðx,x 00 ÞÞ
-Sðx3 ,x00 ,skolG ðr 3b ,Y 0 0 Þ,x01 Þ

r 1 : sourceRew ðmostInf ðE1 ÞÞ-skolG ðSðx3 ,x0 ,Y 0 ,x1 Þ
4SðY 1 ,x0 ,Y 0 ,x0 Þ4SðY 1 ,x2 ,Y 2 ,Y 3 ÞÞ

This set of rules is the core schema mapping for the given
scenario.

r 2 : sourceRew ðmostInf ðE2 ÞÞ-skolG ðSðx3 ,x0 ,Y 0 ,x1 Þ
4SðY 1 ,x0 ,Y 0 ,x0 ÞÞ

Appendix B. Proof of Theorem 1

r 3a : sourceRew ðmostInf ðE3 ÞÞ-skolG ðSðx3 ,x0 ,Y 0 ,x0 ÞÞ
r 3b : sourceRew ðmostInf ðE

0
0
0
3 ÞÞ-skolG ðSðx3 ,x0 ,Y 0 ,x1 ÞÞ

Here we report some of the actual Skolem terms generated by skolG .
Let us consider the Skolem terms for r 2
skolG ðr 2 ,Y 0 Þ ¼ f ðt1 : S½A : x3 ,B : x0 ,D : x1 t2 : S½B : x0 ,D : x0 ,
j1 : ½t1:C ¼ t2:C,v ¼ j1Þ
skolG ðr 2 ,Y 1 Þ ¼ f ðt1 : S½A : x3 ,B : x0 ,D : x1 t2 : S½B : x0 ,D : x0 ,
j1 : ½t1:C ¼ t2:C,v ¼ noj-t2:AÞ
The Skolem terms for r 1 are as follows:
skolG ðr 1 ,Y 0 Þ ¼ f ðt1 : S½A : x3 ,B : x0 ,D : x1 t2 : S½B : x0 ,D : x0 
t3 : S½B : x2 ,j1 : ½t1:C ¼ t2:C,j2 : ½t2:A ¼ t3:A,v ¼ j1Þ
skolG ðr 1 ,Y 1 Þ ¼ f ðt1 : S½A : x3 ,B : x0 ,D : x1 t2 : S½B : x0 ,D : x0 
t3 : S½B : x2 ,j1 : ½t1:C ¼ t2:C,j2 : ½t2:A ¼ t3:A,v ¼ j2Þ
skolG ðr 1 ,Y 2 Þ ¼ f ðt1 : S½A : x3 ,B : x0 ,D : x1 t2 : S½B : x0 ,D : x0 
t3 : S½B : x2 ,j1 : ½t1:C ¼ t2:C,j2 : ½t2:A ¼ t3:A,v ¼ noj-t3:CÞ

skolG ðr 1 ,Y 3 Þ ¼ f ðt1 : S½A : x3 ,B : x0 ,D : x1 t2 : S½B : x0 ,D : x0 
t3 : S½B : x2 ,j1 : ½t1:C ¼ t2:C,j2 : ½t2:A ¼ t3:A,v ¼ noj-t3:DÞ

Notice how, in this case, it is not necessary to resort to the
sortðÞ function, since the rule conclusions do not contain
nontrivial automorphisms, and therefore it is possible to
distinguish the tuple terms at script compilation time.
(Step 6) adding ﬁnal negations
We now look for proper homomorphisms among the
four rule conclusions. We notice that r 1 and r 2 subsume
r 3a with homomorphisms h1 and h2 , we therefore rewrite
r 3a as finalRew ðr 3a Þ. A similar rewriting applies for r 2b .
finalRew ðr 1 Þ : sourceRew ðmostInf ðE1 ÞÞ
-Sðx3 ,x0 ,skolG ðr 1 ,Y 0 Þ,x1 Þ4
SðskolG ðr 1 ,Y 1 Þ,x0 ,skolG ðr 1 ,Y 0 Þ,x0 Þ4
SðskolG ðr 1 ,Y 1 Þ,x2 ,skolG ðr 1 ,Y 2 Þ,skolG ðr 1 ,Y 3 ÞÞ
finalRew ðr 2 Þ : sourceRew ðmostInf ðE2 ÞÞ

Theorem 1. Given a scenario M ¼ ðS,T, Sst Þ, and a source
instance I, suppose J is a universal solution for M over I.
Consider the subset J 0 of J deﬁned as follows:
[
reduceðmostInformativeðmostCompactðW /I,J S ÞÞÞ
J0 ¼
ðB:1Þ
Then, J 0 is the core of J.
Proof. Before getting to the actual proof, let us introduce
two preliminary results about witness blocks.
Proposition 4. Given a solution J 2 USolM ðIÞ, for any tgd m
/I,J S
and vector of constants a, the set of witness blocks W m,a is
closed under isomorphisms.
/I,J S

Proof. Consider a witness block w 2 W m,a , and suppose
there exists w0 2 W /I,J S such that w ﬃw0 , i.e., there exists
an isomorphism h : w0 -w. We need to show that
/I,J S
/I,J S
w0 2 W m,a . Since w 2 W m,a , we know that w ¼ cða,bÞ,
for some vector b of values in domðJÞ. To prove the claim it
0
0
is sufﬁcient to show that also w0 ¼ cða,b Þ, for some b . But
1
1
1
we know that w0 ¼ h ðwÞ ¼ h ðcða,bÞÞ ¼ cða,h ðbÞÞ.
This proves the claim. &
Consider instance J 0 deﬁned according to Eq. (B.1). It
can be seen that the witness blocks of J0 fall in two
categories: beside ‘‘principal’’ witness blocks, there may
be induced witness blocks. A witness block w in W /I,J0 S is
said to be induced if it is a proper subset of another
witness block w0 in W /I,J0 S . We shall call principal any
witness block that is not induced.
Proposition 5. Consider the set of witness blocks W /I,J0 S .
There cannot be two witness blocks w,w0 2 W /I,J0 S such that
w is a principal witness block, w0 is an induced witness block
and there exists an injective homomorphism h : w-w0 .
Proof. We shall prove the claim by contradiction. Suppose there exists h : w-w0 . Since w0 is induced, there
exists w00 in W /I,J0 S such that w0  w00 and w00 is a principal
witness block. But this means that h is also a homomorphism of w into w00 ; moreover, h is proper, since it is
injective by hypothesis and there are atoms in w00 w0 that
do not belong to hðwÞ. However, this is not possible by
construction of J0 , since w is a maximal witness block

702

G. Mecca et al. / Information Systems 37 (2012) 677–711

with respect to o, and therefore cannot have proper
homomorphisms into other witness blocks of J0 . &
We are now ready to prove the main claim. We need to
prove the following:
Part 1. J 0 is a universal solution for M over I, i.e.,
J 0 2 USolM ðIÞ;
Part 2. J 0 does not contain any smaller endomorphic
image that is also a solution.
Part 1: J0 is a universal solution for M over I. To prove
that J0 2 USolM ðIÞ, we shall ﬁrst prove that J 0 is a solution,
and then that it is universal.
To prove that J 0 is a solution, i.e., J0 2 SolM ðIÞ, it is
sufﬁcient to show that, for any tgd m: 8x : f ðx Þ-(y
ðc ðx ,y ÞÞ in Sst , and any vector of constants a such that
IFfðaÞ, the set of witness blocks corresponding to m and
/I,J S
a in J 0 , W m,a0 , is not empty.
Consider now a tgd m: 8x : f ðx Þ-(y ðc ðx ,y ÞÞ in Sst ,
and a vector of constants a such that IFfðaÞ. We know
/I,J S
that the set of witness blocks W m,a is not empty, since J
is a solution; also, it is a ﬁnite set, since J is ﬁnite. Consider
/I,J S
a maximal element w in W m,a0 with respect to the !
relation. We need to distinguish two cases.
(a) There is no other w0 2 W /I,J S such that wo w0 , i.e.,
w is also maximal with respect to o . In this case, since
witness blocks are closed under isomorphisms by
Proposition 4, the equivalence class of witness blocks
/I,J S
isomorphic to w, E w , is included in W m,a . Call wE w the
representative selected for E w by reduce; by construction
/I,J S
of J 0 , wE w belongs to W m,a0 , which cannot be empty.
(b) There exists some w0 2 W /I,J S such that w ow0 ; in
this case, consider the set of witness blocks W ¼ fwi 9wi 2
W /I,J S andwo wi g, and a maximal element wn in W. Consider the equivalence class of witness blocks isomorphic to
wn , E wn , and call wE wn the representative selected for E wn by
reduce; by construction of J0 , wE wn D J 0 . We know that the
following homomorphisms exist:
h : w-wn

0

h : wn -wE wn
0

It can be seen that the set of tuples h ðhðwÞÞ is a subset of
wE wn and therefore is contained in J0 . We now show that
/I,J S
0
h ðhðwÞÞ 2 W m,a0 . In fact, we know that w ¼ cða,bÞ; there0
0
fore, hðwÞ ¼ cða,hðbÞÞ, and h ðhðwÞÞ ¼ cða,h ðhðbÞÞÞ. As a
/I,J 0 S
/I,J 0 S
0
consequence, h ðhðwÞÞ 2 W m,a , and W m,a is not empty.
This proves that J 0 is a solution. We now need to prove
that J 0 is a universal solution for M over I, i.e., that for any
other solution J0 2 SolM ðIÞ there exists a homomorphism
0
h : J0 -J0 . Since we know that J is universal, we also know
that there exists a homomorphism h : J-J 0 . As a conse0
quence, in order to show that h of J 0 into J0 exists, it is
sufﬁcient to show that there exists a homomorphism of
h0 : J0 -J. But we know that h0 exists and it is the identity
mapping, since J 0 is a subset of J. Therefore, we have
h0 : J0 -J

h : J-J 0

By composing the two homomorphisms, we obtain a
0
homomorphism h of J 0 into J 0 . As a consequence,
J 0 2 USolM ðIÞ.

Part 2: J 0 does not contain any smaller endomorphic
image that is also a universal solution – We shall prove
the claim by contradiction. Suppose there exists a smaller
universal solution than J0 , i.e., a solution J#  J0 . Since J # is
properly contained in J 0 , there is at least one tuple t in
J 0 J # ; by removing t from J 0 , we are also removing any
witness block w 2 W /I,J0 S that contains t.
Since J # is a universal solution, it must be homomorphically equivalent to J 0 . Therefore, we know there exists
a homomorphism h# : J 0 -J# . This is obviously true also
for any subset of J 0 . Let us consider one of the principal
witness blocks w that belong to J 0 but not to J# , i.e.,
w 2 W /I,J0 S W /I,J# S . Let us call hw the restriction of h# to
w, i.e., hw : w-J# . We shall now prove that such a
homomorphism cannot exist.
Let us consider the image of w in J# , hw ðwÞ. Note that
wahw ðwÞ, since w is not contained in J # . On the contrary,
since hw ðwÞ is contained in J # , it is also contained in J0 . Call
/I,J S

W m,a0

one of the witness-block sets to which w belongs.

It is easy to see that, since w is of the form cða,bÞ, hw ðwÞ is
of the form cða,hw ðbÞÞ. Therefore, also hw ðwÞ is a witness
/I,J S

/I,J S

block in W m,a0 . As a consequence, we know that W m,a0

contains two distinct witness blocks, w and hw ðwÞ.
We notice that hw : w-hw ðwÞ is a surjective mapping.
Let us consider the number of nulls in w and hw ðwÞ. There
are three possible cases.
(a) 9varsðhw ðwÞÞ9 ¼ 9varsðwÞ9—in this case hw ðwÞ is
simply a renaming of the labeled nulls of w and hw must
be one to one; this means that w and hw ðwÞ are isomorphic; therefore, hw ðwÞ cannot be a principal witness
block, by construction of J0 ; however, it cannot be an
induced witness block, either, because of Proposition 5;
(b) 9varsðhw ðwÞÞ9 49varsðwÞ9—in this case, since hw is
surjective, any labeled null in hw ðwÞ is the image of some
value in w; but since there are more nulls hw ðwÞ than in w, it
just be the case that some of the nulls in hw ðwÞ are image of
a constant in w, and this contradicts the deﬁnition of
homomorphism; therefore, this cannot be the case;
(c) 9varsðhw ðwÞÞ9 o9varsðwÞ9—in this case, since hw is
surjective, hw would be a compacting homomorhpism;
this is clearly not possible, since by hypothesis w is
maximal with respect to !.
Therefore, we have shown that hw cannot exist. This
proves Part 2 of the claim and concludes the proof.

Appendix C. Proof of Theorem 2
Before getting to the actual proof of our main result,
we need to introduce some preliminary notions and
theorems. More speciﬁcally, we shall ﬁrst introduce an
important technical result relating expansions and witness blocks, taken from [24]. After we have proven this
result, we shall move to the actual proof of Theorem 2.
Isomorphisms and the two-step rewriting: It can be seen
from our characterization of the core in Section 5 that a
crucial, ﬁnal step is to remove isomorphic witness blocks
from the canonical solutions. To discuss how we handle
this step in our approach, let us ﬁrst introduce the notion
of isomorphism-free solution.

G. Mecca et al. / Information Systems 37 (2012) 677–711

Deﬁnition 22 (Isomorphism-free solution). Given a scenario M, a source instance I, and a solution J 2 SolM ðIÞ,
we say that J is isomorphism–free if there exist no witness
blocks w,w0 2 W /I,J S such that wﬃ w0 .
Let us assume for now that, given a scenario M and a
source instance I, we have generated an isomorphism-free
solution. We notice that one simple way to do this is to
generate J by running the standard chase procedure
instead of the naive one. The semantics of the standard
chase, in fact, prevents the generation in the canonical
solution of any form of isomorphisms.
Based on this assumption, we are ready to introduce a
ﬁrst approach to the generation of a core schema mapping
for a scenario M. This approach is based on the idea of
performing two exchanges:
(i) given I, generate an isomorphism-free canonical solution J by running the standard chase of the tgds in Sst ;
(ii) for each expansion E 2 expansionsðMÞ, generate the
formula mostInf ðEÞ, and run the corresponding queries
on J to select the maximal witness blocks; then copy
these witness blocks to a new instance, J0 , to generate
the core universal solution for M and I.
Notice that the second step can be performed by
chasing the following set of full tgds, one for each
expansion E 2 expansionsðMÞ, over J
8x 1 ,y 1 : mostInf ðEÞðx 1 ,y 1 Þ-wðx 1 ,y 1 Þ

ðC:1Þ

No new null value needs to be invented in this process,
since we are only copying to J 0 witness blocks that are
already in J. Therefore, the core schema-mapping generated by this approach would be the composition of the
original set of s-t tgds, and this new set of full tgds. This is,
in fact, the strategy originally proposed in [24].
A disadvantage of this strategy is that it requires a
two-step process, i.e., it assumes that two different
exchanges are executed: the ﬁrst is needed to generate
J, and the second to select inside J the witness blocks that
belong to the core. Our ﬁnal goal is to generate a core
schema-mapping through which it is possible to compute
core universal solutions by means of a single exchange.
Nevertheless, behind the two-step strategy there is an
important result, which represents the main technical
result of [24], that we state here:
Theorem 6. Given a scenario M ¼ ðS,T, Sst Þ, a source
instance I, call J a canonical universal solution of Sst over I.
If J is isomorphism–free, consider the set of expansions
expansionsðMÞ and, for each expansion


^
E ¼ wl ðx 1 ,y 1 Þ4(x 2 ,y 2 : cl ðx 2 ,y 2 Þ equalhf ðx 1 ,x 2 Þ
E

its rewriting, mostInf ðEÞ. The following set:
[
J
E mostInf ¼
faðwl ðx 1 ,y 1 ÞÞ9as:t: JFaðmostInf ðEÞðx 1 ,y 1 ÞÞg
E2expansionsðMÞ

is such that
E mostInf ¼ reduceðmostInformativeðmostCompactðW /I,J S ÞÞÞ
J

703
J

From Theorem 1 it follows that E mostInf is exactly the
core of J. We ﬁnd it useful to prove Theorem 6 ﬁrst and
then use it to simplify the proof of Theorem 2. However,
before getting to the actual proof of Theorem 6, we shall
introduce several preliminary deﬁnitions and lemmas.
Images of a variable: Since formula homomorphisms
map variable occurrences to variable occurrences, they
are not mappings among variables. In fact, they typically
relate occurrences of a variable with occurrences of
several different variables. To formalize this notion, we
need to introduce the notion of an image of a variable
according to a formula homomorphism.
Deﬁnition 23 (Image of a variable). Given a formula
f
homomorphism h : jðx,yÞ-j0 ðx 0 ,y 0 Þ; for every variable
f
vi in jðx,yÞ, the image of vi according to h is the set of
variables V hf ðvi Þ whose occurrences are images of occurf
rences of vi via h , deﬁned as follows:
f

V hf ðvi Þ ¼ fv0k 9R:A : vi 2 occðjðx,yÞÞ and h ðR:A : vi Þ ¼ R:A : v0k g

Let us ﬁrst establish an important property of variable
images.
Proposition 7. Given an instance J and two formulas
jðx,yÞ, j0 ðx 0 ,y 0 Þ such that there exists a formula homof
morphism h : jðx,yÞ-j0 ðx 0 ,y 0 Þ, suppose there are assignments a,a0 such that: JFaðjðx,yÞÞ, JFa0 ðj0 ðx 0 ,y 0 ÞÞ and a,a0
are such that equalhf ðaðxÞ,a0 ðx 0 ÞÞ evaluates to true. Then, for
every variable v 2 x [ y, it is the case that:

 a0 has the same value on all variables v0 2 V h ðvÞ;
 if v is universal, then it is the case that aðvÞ ¼ a0 ðv0 Þ, for
f

every variable v0 2 V hf ðvÞ.
Proof. We shall ﬁrst prove the claim in the case in which
the variable is universal, and then existential.
Consider a universal variable x 2 x. Consider
intersecthf ðx,x 0 Þ. We know that it contains an equality
of the form x ¼ x0 for every variable occurrence in j0 ðx 0 ,y 0 Þ
f
such that R:A : x0 ¼ h ðR:A : xÞ. This means that, for any two
0 0
variables xi ,xj in V hf ðxÞ, intersecthf ðx,x 0 Þ contains equalities of the form x ¼ x0i , x ¼ x0j . Since we know that a,a0 are
such that equalhf ðaðxÞ,a0 ðx 0 ÞÞ evaluates to true, it must be
the case that aðxÞ ¼ a0 ðx0i Þ ¼ a0 ðx0j Þ, which proves the claim.
Consider now an existentially quantiﬁed variable y 2 y.
We need to prove that all variables in v0 2 V hf ðvÞ receive
by a0 the same value. In this case, for every pair of
occurrences Ri :Aj : y,Rn :Am : y, by deﬁnition of a formula
homomorphism, we have two possible cases: (i) either
both occurrences are mapped to occurrences of the same
existential variable y0 ; in this case, V hf ðyÞ is a singleton set
containing y0 , and the claim is obviously true; (ii) or they
are mapped to universal occurrences of variables x0h ,x0k ;
but in this case, joinshf ðx,x 0 Þ contains an equality of the
form x0h ¼ x0k , and it must be the case that a0 ðx0h Þ ¼ a0 ðx0k Þ;
therefore all variables in V hf ðyÞ receive the same value by
a0 . This proves the claim. &
Homomorphisms and formula homomorphisms: We now
want to establish two important lemmas that show the

704

G. Mecca et al. / Information Systems 37 (2012) 677–711

dual nature of homomorphisms among facts and homomorphisms among formulas. More speciﬁcally, under
appropriate conditions, whenever one exists there exists
also the other. In order to do this, we need to introduce a
notion of compatibility among these two kinds of homomorphisms, as follows.

This proves that h is a valid homomorphism, and also that
f
h is indeed compatible with h,a,a0 . To complete the proof,
we need to show that:

Deﬁnition 24 (Compatible formula homomorphisms).
Given a formula homomorphism between conjunctive
f
formulas, h : jðx,yÞ-j0 ðx 0 ,y 0 Þ, and assignments a,a0 such
that there is a homomorphism h : aðjðx,yÞÞ-a0 ðj0 ðx 0 ,y 0 ÞÞ,
f
we say that h is compatible with h,a,a0 if, for every
variable v 2 x [ y, and for every variable v0 2 V hf ðvÞ, it is
the case that hðaðvÞÞ ¼ a0 ðv0 Þ.

Suppose h is a surjection. Then, every atom Ri ðx i 0 ,y i 0 Þ 2
j0 ðx 0 ,y 0 Þ is the image of some atom Ri ðx i ,y i Þ 2 jðx,yÞ. By
construction of h, it is the case that a0 ðRi ðx i 0 ,y i 0 ÞÞ is the
image of aðRi ðx i ,y i ÞÞ according to h, and therefore h is a
surjection.
f
Similarly, assume h is proper. We want to prove that h
is proper. We shall ﬁrst prove that (a) h is injective, and
f
then that (b) h is not surjective.
(a) We shall prove that h is injective by contradiction.
Assume h is not injective. This means that there are two
distinct facts Rðt 0 Þ,Rðt 1 Þ in aðjðx,yÞÞ such that hðRðt 0 ÞÞ ¼
hðRðt 1 ÞÞ. Call Rl ðx,yÞ,R0l ðx,yÞ the atoms in jðx,yÞ such that
Rðt 0 Þ ¼ aðRl ðx,yÞÞ,Rðt 1 Þ ¼ aðR0l ðx,yÞÞ. We know that Rl ðx,yÞ,
R0l ðx,yÞ are distinct atoms, since Rðt 0 ÞaRðt 1 Þ. We have
that:

In essence, we are requiring that h maps the value aðvÞ of
a variable v to the value a0 ðv0 Þ of every variable that is in the
f
image of v according to h . We also need to introduce the
notion of an invertible assignment. Recall that an assignment
a for jðx,yÞ is canonical if it injectively associates a labeled
null with each existential variable yi 2 y. The set of facts
aðjðx,yÞÞ is called a canonical block if a is canonical.

 if hf is a surjection, then h is a surjection;
 if hf is proper, and a0 isinvertible, then h is proper.
f

Deﬁnition 25 (Invertible assignment). A canonical assignment is called invertible if 9aðjðx,yÞÞ9 ¼ 9jðx,yÞ9, i.e., each
atom in jðx,yÞ generates a different fact.

hðRðt 0 ÞÞ ¼ hðaðRl ð. . . Aj : vk . . .ÞÞÞ ¼ Rð. . . Aj : hðaðvk ÞÞ . . .Þ

We are now ready to state our result about the dual
nature of fact and formula homomorphisms.

hðRðt 1 ÞÞ ¼ hðaðR0l ð. . . Aj : vk . . .ÞÞÞ ¼ Rð. . . Aj : hðaðvk ÞÞ . . .Þ

f
ðvk ÞÞ . . .Þ ¼
Rl :Aj

¼ Rð. . . Aj : a0 ðh

f

f

a0 ðh ðRl ð. . . Aj : vk . . .ÞÞÞ

f

¼ Rð. . . Aj : a0 ðhR0l :A ðvk ÞÞ . . .Þ ¼ a0 ðh ðR0l ð. . . Aj : vk Þ . . .ÞÞÞ
j

Lemma 8. Given an instance J, and two conjunctive formulas jðx,yÞ and j0 ðx 0 , y 0 Þ such that there exists a formula
f
homomorphism h : jðx,yÞ-j0 ðx 0 ,y 0 Þ, suppose there exist
canonical assignments a,a0 such that JFaðjðx,yÞÞ, JFa0 ðj0
ðx 0 ,y 0 ÞÞ, and a,a0 are such that equalhf ðaðxÞ,a0 ðx 0 ÞÞ evaluates
to true. Then, there exists a homomorphism h : aðjðx,yÞÞf
a0 ðj0 ðx 0 ,y 0 ÞÞ, and h is compatible with h,a,a0 . Moreover:

We therefore have that
f

f

Notice, however, that h is injective by hypothesis, and
therefore
f

 if hf is a surjection, then h is a surjection;
 if hf is proper, and a0 is invertible, then h is proper.
Proof. We shall construct h, and then show that it is a
valid homomorphism. For any variable v 2 x [ y, consider
the value aðvÞ, and let us deﬁne hðaðvÞÞ in such a way that
hðaðvÞÞ ¼ a0 ðv0 Þ, where v0 is any variable in V hf ðvÞ. By
Proposition 7, we know that this is a well deﬁned
mapping. Note also that, by construction, if h is indeed a
f
homomorphism, h is compatible with h,a,a0 .
We shall now prove that h is a homomorphism of
aðjðx,yÞÞ into a0 ðj0 ðx 0 ,y 0 ÞÞ. To see this, consider any atom
f
Ri ð. . . Aj : vk . . .Þ in jðx,yÞ. Recall that, since h is a valid
f
formula homomorphism, we know that Ri ð. . . Aj : hRi :Aj
ðvk Þ . . .Þ 2 j0 ðx 0 ,y 0 Þ. By construction of h, we also know
that, for any occurrence Ri :Aj : vk , we have that hðaðvk ÞÞ ¼
f
a0 ðhRi :Aj ðvk ÞÞ. It follows that
hðaðRi ð. . . Aj : vk . . .ÞÞÞ ¼ Ri ð. . . Aj : hðaðvk ÞÞ . . .Þ ¼
f

f

Ri ð. . . Aj : a0 ðhRi :Aj ðvk ÞÞ . . .Þ ¼ a0 ðRi ð. . . Aj : hRi :Aj ðvk Þ . . .ÞÞ
2 a0 ðj0 ðx 0 ,y 0 ÞÞ

f

a0 ðh ðRl ð. . . Aj : vk . . .ÞÞÞ ¼ a0 ðh ðR0l ð. . . Aj : vk Þ . . .ÞÞÞ

f

h ðRl ð. . . Aj : vk . . .ÞÞah ðR0l ð. . . Aj : vk Þ . . .ÞÞ
But this contradicts the hypothesis that a0 is invertible, since
we are concluding that a0 maps two different atoms in
j0 ðx 0 ,y 0 Þ to the same fact. Therefore, hf must be injective.
(b) We now want to prove that h is not surjective. Since
f
h is proper, there exists an atom Ri ðx i 0 ,y i 0 Þ 2 j0 ðx 0 ,y 0 Þ that
is not the image of an atom in jðx,yÞ. Since a0 is invertible,
the fact a0 ðRi ðx i 0 ,y i 0 ÞÞ is such that it can only be generated
by Ri ðx i 0 ,y i 0 Þ. Since there is no atom in jðx,yÞ that maps
into Ri ðx i 0 ,y i 0 Þ,by construction of h, a0 ðRi ðx i 0 ,y i 0 ÞÞ cannot
belong to the image of h. This proves that h is proper. &
The relationship among fact homomorphisms and formula homomorphisms stated in Lemma 8 has a dual aspect,
as stated in Lemma 9. Before stating the lemma, we need to
introduce a tool that plays an important role in the proof.
Given a formula jðx,yÞ, an instance of jðx,yÞ is a set of
facts of the form jðaðxÞ,aðyÞÞ, for some assignment a,
obtained by replacing each variable vi by aðvi Þ. Consider for example jð/x0 ,x1 ,x2 S,/y0 ,y1 SÞ ¼ Sðx0 ,x1 ,y0 Þ4
Sðx2 ,y1 ,y0 Þ. Following are two of its canonical blocks:
Sða,b,N 0 Þ4Sðc,N1 ,N0 Þ and Sða,b,N2 Þ4Sða,N3 ,N2 Þ. Here are
two other instances of the formula that are not canonical
blocks: Sða,b,N 4 Þ (in this case að/x0 ,x1 ,x2 SÞ ¼ /a,b,aS,

G. Mecca et al. / Information Systems 37 (2012) 677–711

að/y0 ,y1 SÞ ¼ /N4 ,bS), and Sða,b,N5 Þ4Sðc,d,N 5 Þ. Consider
now the formula jð/x0 ,x1 S,/y0 SÞ ¼ Sðx0 ,y0 Þ4Sðx1 ,y0 Þ;
the following blocks are canonical: Sða,N 0 Þ4Sðb,N 0 Þ,
Sða,N 1 Þ (in this case að/x0 ,x1 SÞ ¼ /a,aS).
Given a formula and one of its instances, we introduce
a way to map each tuple in the formula instance to an
atom in the formula, as follows:
Deﬁnition 26 (Mapping facts to atoms). Given a formula
jðx,yÞ and a canonical assignment a for it, we introduce a
mapping, called atoma , of each fact in aðjðx,yÞÞ to an atom
in jðx,yÞ. More speciﬁcally, given a fact t 2 aðjðx,yÞÞ, we
deﬁne atoma ðtÞ as one atom Ri ðx i ,y i Þ of jðx,yÞ such that
t ¼ aðRi ðx i ,y i ÞÞ. Notice that, if a is invertible, there is exactly
one atom of this kind for each fact. This is not true in general
if a is not invertible; in this case, we nondeterministically
pick one of the atoms associated with each fact.
Lemma 9. Given two conjunctive formulas jðx,yÞ, j0 ðx 0 ,y 0 Þ,
suppose there are assignments a,a0 such that there exists a
homomorphism h : aðjðx,yÞÞ-a0 ðj0 ðx 0 ,y 0 ÞÞ and a0 is a canonical assignment for j0 ðx 0 ,y 0 Þ. Then, there exists a formula
f
f
homomorphism: h : jðx,yÞ-j0 ðx 0 ,y 0 Þand h is compatible
with h,a,a0 . Moreover:

 if h is a surjection, and a0 is invertible, then hf is a
surjection;

 if h is proper, and a,a0 are invertible, then hf is proper.
Proof. Let us call w ¼ aðjðx,yÞÞ, w0 ¼ a0 ðj0 ðx 0 ,y 0 ÞÞ. We want
f
to build a formula homomorphism h of jðx,yÞ into
0 0 0
j ðx ,y Þ. As a ﬁrst step, we introduce a mapping of each
atom Ri ðx i ,y i Þ 2 jðx,yÞ to an atom in j0 ðx 0 ,y 0 Þ, called
matchh,a,a0 ðRi ðx i ,y i ÞÞ, according to the following strategy:

705
f

in j0 ðx 0 ,y 0 Þ. In fact, by construction h maps each occurrence
Ri :A : vi in Ri ðx i ,y i Þ to the corresponding occurrence in
matchh,a,a0 ðRi ðx i ,y i ÞÞ.
f
We note that h maps universal occurrences into universal occurrences. In fact, each universal occurrence
Ri :Aj : vk in jðx,yÞ is ﬁrst mapped to a constant in
aðRi ð. . . Aj : vk . . .ÞÞ 2 w, and then to a constant in
hðaðRi ð. . . Aj : vk . . .ÞÞÞ 2 w0 ; then, since we know that w0 is
a canonical block for j0 ðx 0 ,y 0 Þ, only universal variables are
mapped by a0 to constants; therefore, occAj ðmatchh,a,a0
ðRi ðx i ,y i ÞÞÞ must be a universal occurrence.
Finally, consider two occurrences Ri :Aj : yk , Rn :Am : yk of
the same existential variable yk in jðx,yÞ. There are two
possible cases:
(i) aðyk Þ is a constant; in this case, since w0 is a canonical
block, we know that both Ri :Aj : yk , Rn :Am : yk will be
mapped to universal variable occurrences in j0 ðx 0 ,y 0 Þ;
(ii) aðyk Þ is a labeled null; in this case, both occurrences in
jðx,yÞ will be mapped to the same labeled null aðyk Þ in w;
this, in turn, can be either mapped to a constant or a labeled
null by h; if hðaðyk ÞÞ is a constant, then, by the same
reasoning as (i) above, we know that both occurrences of
yk will be mapped to universal occurrences; if, on the
contrary, hðaðyk ÞÞ is a labeled null, since we know that w0
is a canonical block, i.e., a0 maps existential variables
injectively to labeled nulls, both occurrences will be mapped
to occurrences of the same existential variable in j0 ðx 0 ,y 0 Þ.
f
This proves that h is a valid formula homomorphism.
f
To show that h is compatible with h,a,a0 , we need to
prove that, for every variable v 2 x [ y, and for every
variable v0 2 V hf ðvÞ, it is the case that hðaðvÞÞ ¼ a0 ðv0 Þ. Call
f
Ri :Aj : v the occurrence of v such that h ðRi :Aj : vÞ ¼
f
Ri :Aj : v0 . By construction of h , we know that
f

Ri ð. . . Aj : v0 . . .Þ ¼ h ðRi ð. . . Aj : v . . .ÞÞ

 we ﬁrst map Ri ðx i ,y i Þ to aðRi ðx i ,y i ÞÞ 2 w;
 then, we map aðRi ðx i ,y i ÞÞ to hðaðRi ðx i ,y i ÞÞÞ 2 w0 ;
 ﬁnally, we map hðaðRi ðx i ,y i ÞÞÞ to atoma ðhðaðRi ðx i ,

¼ matchh,a,a0 ðRi ð. . . Aj : v . . .ÞÞÞ
¼ atoma0 ðhðaðRi ð. . . Aj : v . . .ÞÞÞÞ

0

y i ÞÞÞÞ 2 j0 ðx 0 ,y 0 Þ;

¼ atoma0 ðRi ð. . . Aj : hðaðvÞÞ . . .ÞÞ

i.e., we have that: matchh,a,a0 ðRi ðx i ,y i ÞÞ ¼ atoma0 ðhðaðRi
ðx i ,y i ÞÞÞÞ.
f
To build h , we consider each variable occurrence Ri :Aj :
f
vk in jðx,yÞ, and choose h ðRi :Aj : vk Þ to be the corresponding variable occurrence in matchh,a,a0 ðRi ðx i ,y i ÞÞ, called
occAj ðmatchh,a,a0 ðRi ðx i ,y i ÞÞÞ, i.e.
f

h ðRi :Aj : vk Þ ¼ occAj ðmatchh,a,a0 ðRi ðx i ,y i ÞÞÞ

ðC:2Þ

f

To prove that h is a formula homomorphism of jðx,yÞ into
j0 ðx 0 ,y 0 Þ, according to the deﬁnition, we need to show that:
f

0

0

 h maps each atom in jðx,yÞ to an atom in j ðx ,y Þ;
 hf maps universal occurrences in jðx,yÞ to universal


0

occurrences in j0 ðx 0 ,y 0 Þ;
f
h is such that two different occurrences of the same
existential variable in jðx,yÞ are either mapped to
universal occurrences, or to occurrences of the same
existential variable in j0 ðx 0 ,y 0 Þ.
f

It can be seen immediately that, by construction, h maps
each atom Ri ðx i ,y i Þ in jðx,yÞ to an atom matchh,a,a0 ðRi ðx i ,y i ÞÞ

Recall now that, by the deﬁnition of atoma0 , a0 ðatoma0 ðtÞÞ ¼
t. If we apply a0 to both the ﬁrst and the last atom in the
equation above, we therefore have
a0 ðRi ð. . . Aj : v0 . . .ÞÞ ¼ Ri ð. . . Aj : a0 ðv0 Þ . . .ÞÞ
¼ a0 ðatoma0 ðRi ð. . . Aj : hðaðvÞÞ . . .ÞÞÞ ¼ Ri ð. . . Aj : hðaðvÞÞ . . .Þ
Based on this, we can conclude that a0 ðv0 Þ ¼ hðaðvÞÞ. This
f
proves that h is compatible with h,a,a0 . To complete the
proof, we need to prove that:

 if h is a surjection, and a0 is invertible, then hf is a
surjection;

 if h is proper, and a,a0 are invertible, then hf is proper.
These follows immediately from the deﬁnition of atoma0 .
In fact, assume h is a surjection and a0 is invertible. In this
case, by deﬁnition of invertible assignment, atoma0 is both
injective and surjective. Therefore matchh,a,a0 is obtained
by the composition of three surjective mappings – a, h,
and atoma0 , and therefore it is itself surjective. As a
f
consequence, h is surjective.

706

G. Mecca et al. / Information Systems 37 (2012) 677–711

Similarly, assume h is proper, and a,a0 are invertible. We
f
need to prove that (a) h is injective, and (b) there is an
atom in j0 ðx 0 ,y 0 Þ that is not the image of an atom in jðx,yÞ
f
according to h .
f
To prove part (a), i.e, that h is injective, we notice that
matchh,a,a0 is obtained by the composition of three injecf
tive mappings – a, h, and atoma0 , and therefore h is
injective. To prove part (b), we notice that, since h is
proper, hðaðjðx,yÞÞ does not coincide with a0 ðj0 ðx 0 ,y 0 ÞÞ. Call
a0 ðRi ðx 0i ,y 0i ÞÞ the atom in a0 ðj0 ðx 0 ,y 0 ÞÞ that is not image of an
atom in aðjðx,yÞÞ. Since each atom in a formula generates
a single fact, it must be the case that Ri ðx 0i ,y 0i Þ does not
belong to the image of jðx,yÞ according to matchh,a,a0 .
f
Therefore, h is a proper formula homomorphism.
This proves the claim. &
Lemma 9 has a direct impact on the way in which our
rewritings are evaluated, as stated by the following
Lemma.
Lemma 10. Given two conjunctive formulas jðx,yÞ,
j0 ðx 0 ,y 0 Þ, suppose there are canonical assignments a,a0 such
that JFaðjðx,yÞÞ and JFa0 ðj0 ðx 0 ,y 0 ÞÞ and there exists a
f
homomorphism: h : aðjðx,yÞÞ-a0 ðj0 ðx 0 ,y 0 ÞÞ. Call h the for0 0 0
mula homomorphism of jðx,yÞinto j ðx ,y Þ compatible with
h,a,a0 . Then, equalhf ðaðxÞ,a0 ðx 0 ÞÞ evaluates to true.
Proof. Consider equalhf ðx,x 0 Þ; it contains equalities of
two forms
f

intersecthf ðx,x 0 Þ ¼ fxk ¼ x0k 9 xk 2 x, Ri :Aj : x0k ¼ hRi :Aj ðxk Þg
f

f

joinshf ðx 0 Þ ¼ fx0h ¼ x0l 9 yk 2 y, x0h ¼ hRi :Aj ðyk Þ,x0l ¼ hRn :Am ðyk Þg
equalhf ðx,x 0 Þ ¼ intersecthf ðx,x 0 Þ [ joinshf ðx 0 Þ

Let us ﬁrst consider intersecthf ðx,x 0 Þ. To prove the claim
we need to show that aðxk Þ ¼ a0 ðx0k Þ, whenever Ri :Aj : x0k ¼
f
f
hRi :Aj ðxk Þ. But it is easily seen that, since x0k 2 V hf ðxk Þ, and h
0
is compatible with h,a,a , by deﬁnition of compatible
homomorphism, it is the case that hðaðxk ÞÞ ¼ a0 ðx0k Þ.
Since both xk and x0k are universal, aðxk Þ is a constant,
and therefore hðaðxk ÞÞ ¼ aðxk Þ ¼ a0 ðx0k Þ. This proves that
intersecthf ðaðxÞ,a0 ðx 0 ÞÞ evaluates to true.
Let us now consider joinshf ðx 0 Þ. To prove the claim, we
need to show that, a0 ðx0h Þ ¼ a0 ðx0l Þ, for every pair of universal
variables in the image of some yk 2 y. But since both x0h
and x0l belong to V hf ðyk Þ, by deﬁnition of compatible
homomorphism it must be the case that hðaðyk ÞÞ ¼
a0 ðx0h Þ ¼ a0 ðx0l Þ. Therefore also joinshf ða0 ðx 0 ÞÞ evaluates to
true. This proves the claim. &
We are now ready to move to the proof of Theorem 6
introduced above:
Theorem 6. Given a scenario M ¼ ðS,T, Sst Þ, a source
instance I, call J a canonical universal solution of Sst over I.
If J is isomorphism–free, consider the set of expansions
expansionsðMÞ and, for each expansion


^
E ¼ wl ðx 1 ,y 1 Þ4(x 2 ,y 2 : cl ðx 2 ,y 2 Þ equalhf ðx 1 ,x 2 Þ
E

its rewriting, mostInf ðEÞ. The following set
[
J
E mostInf ¼
faðwl ðx 1 ,y 1 ÞÞ9a s:t: JFaðmostInf ðEÞÞg
E2expansionsðMÞ

is such that
E mostInf ¼ reduceðmostInformativeðmostCompactðW /I,J S ÞÞÞ
J

Proof. In order to prove the claim, we introduce the
following additional sets:
[
EJ ¼
faðwl ðx 1 ,y 1 ÞÞ9a s:t JFaðEÞg
E2expansionsðMÞ
J

E mostComp ¼

[

faðwl ðx 1 ,y 1 ÞÞ9a s:t JFaðmostComp ðEÞÞg

E2expansionsðMÞ

The proof is organized in three parts. We shall prove the
following claims:
Part 1. E J ¼ W /I,J S
J

/I,J S
Þ
Part 2. E mostComp ¼ mostCompactðW
/I,J S
ÞÞ
Part 3. E mostInf ¼ mostInformativeðmostCompactðW
J

Notice, in fact, that, once we have proven Part 3, the thesis
follows immediately from the hypothesis that J is isomorphism-free. This means that any equivalence class E i of isoJ
morphic witness blocks in E mostInf is a singleton. Therefore,
reduceðÞ is the identity mapping on mostInformativeð
mostCompactðW /I,J S ÞÞ, and therefore the thesis is proven.
Part 1 – E J ¼ W /I,J S
J
/I,J S
We shall ﬁrst prove that E m D W m , and then that
/I,J S
J
W m D E m , for each m 2 Sst .
Part 1 [ﬁrst half]: E J DW /I,J S .
To show that E J DW /I,J S , consider a set of facts wE 2 E J .
We need to show that wE is a witness block for some tgd
m: 8x : f ðx Þ-(y ðc ðx ,y ÞÞ, i.e., there exists a vector of
/I,J S
constants a w such that wE 2 W m,a . This amount to prove
w
that there exists an assignment aw that satisﬁes the
following two conditions:

 IFfðaw ðxÞÞ, i.e., a w ¼ aw ðxÞ;
 wE has the form cðaw ðxÞ,aw ðyÞÞ.
In the following, we construct such an assignment aw .
J
We know that wE belongs to some E m , for some tgd
l
fðx 2 Þ-(y 2 ðc ðx 2 ,y 2 ÞÞ and some expansion


^
E ¼ wl ðx 1 ,y 1 Þ4(x 2 ,y 2 : cl ðx 2 ,y 2 Þ equalhf ðx 1 ,x 2 Þ
E

Recall that we know that JFa1 ðEðx 1 ,y 1 ÞÞ, for some assignment a1 , i.e., JFa1 ðwl ðx 1 ,y 1 ÞÞ ¼ wE ; call a2 the assignment
l
such that JFa2 ðc ðx 2 ,y 2 ÞÞ. We also know that
equalhf ða1 ðx 1 Þ, a2 ðx 2 ÞÞ evaluates to true.
E
Since, by deﬁnition of expansion, there exists a surjecf
tive formula homomorphism h of cðx 2 ,y 2 Þ into wðx 1 ,y 1 Þ,
we know by Lemma 8 that there exists a surjective
l
homomorphism h : a2 ðc ðx 2 ,y 2 ÞÞ-a1 ðwl ðx 1 ,y 1 ÞÞ. Since h is
surjective, we have that
l

l

wE ¼ a1 ðwl ðx 1 ,y 1 ÞÞ ¼ hða2 ðc ðx 2 ,y 2 ÞÞÞ ¼ c ðhða2 ðx 2 ÞÞ,hða2 ðy 2 ÞÞÞ
l

If we take aw ¼ hJa2 , then wE has the form aw ðc ðx 2 ,y 2 ÞÞ.

G. Mecca et al. / Information Systems 37 (2012) 677–711

In order to complete the proof that wE is a witness block for
m, we also need to prove that aw is such that IFfðaw ðx 2 ÞÞ.
l
Recall that we know that JFa2 ðcðx 2 ,y 2 ÞÞ. Since a2 ðc
ðx 2 ,y 2 ÞÞ is a witness block for m in J, and J is a canonical
universal solution, we know that it must be the case that
IFfða2 ðx 2 ÞÞ. We now show that aw ðx 2 Þ ¼ a2 ðx 2 Þ, and therefore IFfðaw ðx 2 ÞÞ. In fact, for any variable x2i 2 x 2 , by
deﬁnition we have that aw ðx2i Þ ¼ hða2 ðx2i ÞÞ. But x2i is a
universal variable, and therefore a2 ðx2i Þ is a constant. As a
consequence, h is the identity on it. It follows that
aw ðx2i Þ ¼ a2 ðx2i Þ.
This proves that wE is a witness block for tgd m, and
concludes the proof of the ﬁrst half of Part 1.
Part 1 [second half]: W /I,J S D E J .
Consider a witness block w 2 W /I,J S . Call m: fðx 2 Þ-(y 2
/I,J S
l
ðc ðx 2 ,y 2 ÞÞ a tgd such that w 2 W m . We need to prove
J
that w 2 E m .
/I,J S
Since w is a witness block in W m , we know that there
exists an assignment aw such that w ¼ cðaw ðx 2 Þ, aw ðy 2 ÞÞ.
We need to prove that w is an instance of some
expansion E of m. In order to show that E exists, we now
construct wl ðx 1 ,y 1 Þ as follows: since w is a set of facts in a
canonical universal solution J 2 USolM ðIÞ, for each fact t i
in w, we consider its provenance, provenanceðt i Þ, and we
pick exactly one of the labeled atoms in it. We notice that
w is a canonical block for wl ðx 1 ,y 1 Þ, i.e., there exists a
canonical assignment aw such that w ¼ aw ðwl ðx 1 ,y 1 ÞÞ.
Also, aw is an invertible assignment. In fact, by construction,
9wðx 1 ,y 1 Þ9 ¼ 9w9 ¼ 9aw ðwðx 1 ,y 1 ÞÞ9. We shall now prove that


^
E ¼ wl ðx 1 ,y 1 Þ4(x 2 ,y 2 : cl ðx 2 ,y 2 Þ equalhf ðx 1 ,x 2 Þ
E

is actually a valid expansion for m. In order to do this, it is
f
necessary to prove that there is a surjection h : cðx 2 ,y 2 Þwðx 1 ,y 1 Þ.
By Lemma 9, we know that there exists a formula homof
l
morphism h of c ðx 2 ,y 2 Þ into wðx 1 ,y 1 Þ. In fact, we know that
w ¼ aw ðcðx 2 ,y 2 ÞÞ ¼ aw ðwðx 1 ,y 1 ÞÞ. Therefore, there is a trivial
automorphism hi of aw ðcðx 2 ,y 2 ÞÞ into aw ðwðx 1 ,y 1 ÞÞ; since aw
is a canonical assignment for wðx 1 ,y 1 Þ, Lemma 9 holds.
f
Moreover, since hi is surjective, and aw is invertible, also h
f
is surjective. This proves that h is a valid formula homomorphism and a surjection, and that E is a valid expansion.
To complete the proof, we need to show that
JFaw ðEðx 1 ,y 1 ÞÞ. Since we already know that JFaw
ðwl ðx 1 ,y 1 ÞÞ ¼ w, this amounts to show that there exists an
assignment ab such that JFab ðcðx 2 ,y 2 ÞÞ, and equalhf
E
ðaw ðx 1 Þ,ab ðx 2 ÞÞ evaluates to true.
Let us take ab ¼ aw , and show that equalhf ðaw ðx 1 Þ,aw ðx 2 ÞÞ
E
evaluates to true. But this immediately follows from Lemma
10. In fact, we have already noted that there is a trivial
automorphism hi of aw ðcðx 2 ,y 2 ÞÞ into aw ðwl ðx 1 ,y 1 ÞÞ.
f
Since h is by construction compatible with hi , aw , aw , then
by Lemma 10 it follows that equalhf ðaw ðx 1 Þ,aw ðx 2 ÞÞ evaluates
E
to true.
This proves that JFaw ðEðx 1 ,y 1 ÞÞ, and concludes the proof
of Part 1.
J
Part 2 –E mostComp ¼ mostCompactðW /I,J S Þ.
J
Part 2 (ﬁrst half): – E mostComp D mostCompactðW /I,J S Þ.
Before proving the claim, let us introduce a preliminary
lemma.

707

Lemma 11. Given a universal solution J and an expansion E,
any assignment a such that JFaðmostComp ðEÞÞ is an invertible assignment.
Proof. We shall prove the claim by contradiction. Suppose,
in fact, that a is not invertible. This means that there exist
two different atoms Rl ð. . .Þ,R0l ð. . .Þ in wl ðx 1 ,y 1 Þ that generate
the same fact. Consider now the formula w0l ðx 01 ,y 01 Þ obtained
from wl ðx 1 ,y 1 Þ by removing any atom like R0l ð. . .Þ. Call a0 the
restriction of a to x 01 ,y 01 . Notice that aðwl ðx 1 ,y 1 ÞÞ ¼ a0
ðw0l ðx 01 ,y 01 ÞÞ ¼ w0 , and that a0 is invertible by construction.
Based on Lemma 9, we know that, since there is a
surjective homomorphism (the identity) from aðwl ðx 1 ,y 1 ÞÞ
to a0 ðw0l ðx 01 ,y 01 ÞÞ, and a0 is invertible, there must be a
f0
surjective formula homomorphism h of wl ðx 1 ,y 1 Þ into
0l 0
0
w ðx 1 ,y 1 Þ. Therefore, we have that
^
E0 ¼ w0l ðx 01 ,y 01 Þ4(x 2 ,y 2 : ðcl ðx 2 ,y 2 Þ equalhf ðx 01 ,x 2 ÞÞ
E0

is a valid expansion of m. In fact, since E is an expansion,
f
l
we know there exists a surjection hE : c ðx 2 ,y 2 Þ-wl ðx 1 ,y 1 Þ,
f
l
and therefore there exists a surjection hE0 : c ðx 2 ,y 2 Þ0l 0
0
w ðx 1 ,y 1 Þ, obtained by the composition of the two surjecf
f
tive formula homomorphisms hE and hE0 .
Consider now the two expansions E, E0 . We know there
f0
exists a surjection h of wl ðx 1 ,y 1 Þ into w0l ðx 01 ,y 01 Þ. But notice
f0
that h is also compacting, since 9w0l ðx 01 ,y 01 Þ9 o9wl ðx 1 ,y 1 Þ9.
Therefore, mostComp ðEÞ has the following form:
mostComp ðEÞ ¼ E4:(x 01 ,y 01 : ðE0 4equalhf 0 ðx 1 ,x 01 ÞÞ4 . . .
But then, by Lemma 10, it is not possible that
JFaðmostComp ðEÞÞ, since the negated subformula evaluates to true. This contradicts our hypothesis. Therefore, a
must be invertible. &
In order to prove the claim, we need to prove that
J
any block of facts wE 2 E mostComp belongs also to
mostCompactðW /I,J S Þ. This amounts to show that wE is
maximal with respect to !, i.e., there is no other witness
block w0 such that w0 !wE . We shall prove this by
contradiction.
Call E the expansion such that, for assignment a,
JFaðmostComp ðEÞÞ ¼ wE . By Lemma 11, we know that a
is invertible. Suppose there exists a witness block w0 such
that w0 !wE . By Part 1 of the proof, we know there exists
an expansion E0 in expansionsðMÞ and an invertible
assignment a0 such that JFa0 ðE0 ðx 1 0 ,y 1 0 ÞÞ ¼ w0 .
Since we assume that w0 !wE , i.e., there is a compacting
0
homomorphism h : wE -w0 , we know by Lemma 9 that
f0
there must be a formula homomorphism h of wðx 1 ,y 1 Þ
into w0 ðx 01 ,y 01 Þ, deﬁned as follows:
f0

0

h ðRi :Aj : vk Þ ¼ occAj ðatoma0 ðh ðaðRi ð. . . Aj : vk . . .ÞÞÞÞÞ
f0

0

We also know that h is a surjection, since h is a
surjection, and a0 is invertible. We want to prove that
f0
h is compacting, i.e., either 9w0 ðx 01 ,y 01 Þ9 o 9wðx 1 ,y 1 Þ9 or
9y 01 9 o 9y 1 9.
Since h

f0

is a surjection, we know that 9w0 ðx 01 ,y 01 Þ9 r
f0

9wðx 1 ,y 1 Þ9. If 9w0 ðx 01 ,y 01 Þ9 o 9wðx 1 ,y 1 Þ9 then h is compacting.
Suppose, on the contrary, that 9w0 ðx 01 ,y 01 Þ9 ¼ 9wðx 1 ,y 1 Þ9.
0
Since we know that h is compacting, it is the case that it

708

G. Mecca et al. / Information Systems 37 (2012) 677–711

is a surjection and 9varsðw0 Þ9 o 9varsðwE Þ9. It is possible to
see that this may only happen if at least one of the
following cases occurs:
0
(a) h maps some null N 2 varsðwE Þ to a constant;
0
(b) h is not an injective mapping of varsðwE Þ into
varsðw0 Þ, i.e., it maps two different nulls Ni ,Nj 2 varsðwE Þ
to the same null N k 2 varsðw0 Þ.
Let us consider the two cases separately. In case (a), call
0
y the existential variable such that aðyÞ ¼ N. If h ðaðyÞÞ is a
constant, all occurrences of the form occAj ðatoma0 ðRk ð. . . Aj :
0
h ðaðyÞÞ . . .ÞÞÞ are universal occurrences, since a0 is a canonical assignment, and therefore a constant can be generated by a0 only from a universal variable; this means that
all occurrences of y are mapped to universal occurrences,
and it is the case that V hf 0 ðyÞ does not contain any
existential variables. Consider the mapping of variables
I : y 1 -y 01 that associates with each existential variable yi
in wðx 1 ,y 1 Þ the existential variable I ðyi Þ in w0 ðx 01 ,y 01 Þ that is
f0
image of yi via h , i.e., such that I ðyi Þ 2 V hf 0 ðyi Þ, if this
exists. This is in fact a mapping since, by deﬁnition, a
formula homomorphism either maps all occurrences of an
existential variable to occurrences of the same existential
variable, or to universal occurrences. It can be seen that I
f0
is surjective, since h is surjective, but it is not total, since
we know that V hf 0 ðyÞ only contains universal variables. As
f0
a consequence, it must be the case that 9y 1 9 4 9y 01 9, i.e., h
is compacting.
Consider now case (b) above. By a similar argument, it
can be seen that also in this case I is surjective but it is
not injective; in fact, two different existential variables
are mapped to the same image. Also in this case, this can
f0
happen only if 9y 1 9 4 9y 01 9, i.e., if h is compacting.
We have shown that there exists an expansion E0 such
f0
that there is a compacting homomorphism h from
0 0
0
wðx 1 ,y 1 Þ into w ðx 1 ,y 1 Þ. This means that mostComp ðEÞ is
of the following form:
mostComp ðEÞ ¼ E4:(x 1 0 ,y 1 0 : ðE0 4equalhf 0 ðx 1 ,x 1 0 ÞÞ4 . . .
but this in turn implies that it is not possible that
J
wE 2 E mostComp . In fact, it must be the case that
JjaðmostComp ðEÞÞ. To see this, notice that there are
assignments a,a0 such that wE ¼ aðwðx 1 ,y 1 ÞÞ D J, w0 ¼
0
a0 ðw0 ðx 01 ,y 01 ÞÞ DJ, and a homomorphism h : wE -w0 . There0
0
fore, by Lemma 10, equalhf 0 ðaðx 1 Þ,a ðx 1 ÞÞ evaluates to true,
and the existentially quantiﬁed subformula evaluates to
true. This means that we have reached a contradiction,
J
since wE cannot belong to E mostComp , and the claim is
proven.
This proves the ﬁrst half of Part 2.
J
Part 2 [second half]: mostCompactðW /I,J S Þ DE mostComp .
In order to prove the claim, we need to show that, for
any witness block w that belongs to mostCompactðW /I,J S Þ,
J
it is the case that w 2 E mostComp . We know, by Part 1 of
the proof, that there exists an expansion E such that,
for someinvertible assignment a, it is the case that JF
aðEðx 1 ,y 1 ÞÞ ¼ w. We need to prove that JFaðmostComp ðEÞÞ.
We shall prove the claim by way of contradiction. More
speciﬁcally, suppose that Jj aðmostComp ðEÞÞ. Since
JFaðEðx 1 ,y 1 ÞÞ ¼ w, by deﬁnition of mostComp ðEÞ, there
must be some expansion E0 such that there is a compactf0
ing homomorphism h of wðx 1 ,y 1 Þ into w0 ðx 01 ,y 01 Þ, and, for

some assignment a0 , JFa0 ðE0 ðx 1 0 ,y 1 0 ÞÞ, and a,a0 are such
that the formula equalhf 0 ðaðx 1 Þ,a0 ðx 01 ÞÞ evaluates to true. In
this case, in fact, by construction of mostComp ðEÞ, E0
appears in it in a negated subformula which evaluates
to true.
Consider now w0 ¼ a0 ðw0 ðx 01 ,y 01 ÞÞ. Based on Lemma 8, we
0
know that there exists a homomorphism h : w-w0 . We
0
f0
also know that h is a surjection, since h is a surjection.
0
We now want to prove that h is compacting, i.e.,
f0
0
9varsðw Þ9 o9varsðwÞ9. Since we know that h is compacting, we also know that either 9w0 ðx 01 ,y 01 Þ9 o 9wðx 1 ,y 1 Þ9 or
9y 01 9 o9y 1 9.
Let us ﬁrst consider the case in which 9w0 ðx 01 ,y 01 Þ9 o
9wðx 1 ,y 1 Þ9. Since we know that a isinvertible, it must be
the case that
9w0 9 ¼ 9a0 ðw0 ðx 01 ,y 01 ÞÞ9 r9w0 ðx 01 ,y 01 Þ9 o 9wðx 1 ,y 1 Þ9
¼ 9aðwðx 1 ,y 1 ÞÞ9 ¼ 9w9
i.e., 9w0 9 o9w9. This may only happen if w contains at least
0
0
two distinct atoms RðtÞ,Rðt 0 Þ such that h ðRðtÞÞ ¼ h ðRðt 0 ÞÞ. It
can be seen that these atoms must contain some labeled
0
nulls. In fact, any ground atom can be mapped by h only
to itself, and therefore must belong to both w and w0 .
0
0
Moreover, since h ðRðtÞÞ ¼ h ðRðt 0 ÞÞ, it must be the case that:
(a) at least one labeled null in RðtÞ or in Rðt 0 Þ is mapped by
0
h to a constant;
(b) some null N0 in Rðt 0 Þ is mapped to the same null to
0
0
which a null N in RðtÞ is mapped, i.e., h ðNÞ ¼ h ðN 0 Þ.
In case (a), since there is at least one null that is mapped
0
0
by h to a constant, and h is a surjection, it must be the
0
0
case that 9varsðw Þ9 o 9varsðwÞ9, i.e., h is compacting.
In case (b), we know that the size of the image of
0
0
varsðwÞ according to h , h ðvarsðwÞÞ, is smaller than the size
of varsðwÞ, since two different nulls in w are mapped to
0
the same null in w0 ; but we know that h is a surjection,
0
and therefore it must be the case that 9h ðvarsðwÞÞ9 ¼
9varsðw0 Þ9. Therefore, we have that
0

9varsðw0 Þ9 ¼ 9h ðvarsðwÞÞ9o 9varsðwÞ9
0

and also in this case h is compacting.
Let us now consider the second case, the one in which
f0
f0
h is such that 9y 01 9 o 9y 1 9. Since h is a surjection, this
may happen in the following cases:
0

 there exists y 2 y 1 such that its image according to hf ,
V hf 0 ðyÞ contains only universal variables;

 there exist yi ,yj 2 y 1 such that V h ðyi Þ ¼ V h ðyj Þ ¼ y0 2
f0

f0

y 1 0 , i.e., two different variables are mapped to the same
existential variable.
0

We know that h is a surjection by construction. But then, it
must be the case that 9varsðw0 Þ9 o9varsðwÞ9. Suppose, in
fact, that for every other existential variable yk 2 y,yk ay, yk
is mapped to a different variable y0k 2 y 01 . Then, since a,a0 are
canonical, varsðwÞ contains a distinct element aðyk Þ, and
0
varsðw0 Þ a distinct element h ðaðyk ÞÞ, for any of such variables. But in turn, in both cases, varsðwÞ contains a distinct
element aðyÞ for which there is no counterpart in varsðw0 Þ.

G. Mecca et al. / Information Systems 37 (2012) 677–711
0

Therefore, h is compacting, and w!w0 . But this is
obviously a contradiction, since w 2 mostCompactðW /I,J S Þ,
and therefore w is maximal with respect to !. Therefore,
we have proven the claim.
This concludes the proof of Part 2.
J
Part 3 – E mostInf ¼ mostInformativeðmostCompactðW /I,J S ÞÞ.
J
Part 3 [ﬁrst half]: E mostInf D mostInformative ðmostCompact
/I,J S
ðW
ÞÞ.
In order to prove the claim, we need to prove that any
J
block of facts wE 2 E mostInf belongs also to mostInformative
ðmostCompactðW /I,J S ÞÞ. This amounts to prove that wE is
maximal with respect to o, i.e., there exists no witness
block w0 such that there is a proper homomorphism
0
h : wE -w0 .
The proof is very similar to that of the ﬁrst half of Part 2.
Also in this case we proceed by way of contradiction. We
call E the expansion such that JFaðmostInf ðEÞÞ ¼ wE , for
some assignment a. Notice that, by construction of
mostInf ðEÞ, any assignment such that JFaðmostInf ðEÞÞ is
also such that JFaðmostComp ðEÞÞ; as a consequence, by
Lemma 11, we know that a is invertible.
Assume that there exists a witness block w0 in
mostCompactðW /I,J S Þ such that there exists a proper
0
homomorphism h : wE -w0 . In this case, we know by Part
2 of the proof that there exists an expansion E0 and an
invertible assignment a0 such that JFa0 ðmostComp ðE0 ÞÞ.
0
Also, since h is proper and a,a0 are invertible, by Lemma 9
we know that there is a proper formula homomorphism
f0
h of wðx 1 ,y 1 Þ into w0 ðx 01 ,y 01 Þ. This means that mostInf ðEÞ
has the following form:
mostInf ðEÞ ¼ mostComp ðEÞ4:(x 1 0 ,y 1 0
: ðmostComp ðE0 Þ4equalhf 0 ðx 1 ,x 1 0 ÞÞ4 . . .
It follows that JjaðmostInf ðEÞÞ. In fact, equalhf 0 ðaðx 1 Þ,a0 ðx 1 0 ÞÞ
evaluates to true by Lemma 10. This means that it is not
J
possible that wE 2 E mostInf , i.e., we have reached a contradiction. This proves the claim.
Part 3 [second half]: mostInformativeðmostCompact
J
ðW /I,J S ÞÞ DE mostInf .
In order to prove the claim, we need to show that,
for any witness block w that belongs to mostInformative
J
ðmostCompactðW /I,J S ÞÞ, it is the case that w 2 E mostInf . The
proof is very similar to that of the second half of Part 2.
We know from Part 1 of the proof that there exists an
expansion E and invertible assignment a such that
JFaðEðx 1 ,y 1 ÞÞ ¼ w. We need to prove that JFaðmostInf ðEÞÞ.
Again, this is done by way of contradiction. Assume
JjaðmostInf ðEÞÞ; there must be some expansion E0 such
f0
that there is a proper homomorphism h of wðx 1 ,y 1 Þ into
0 0
0
0
w ðx 1 ,y 1 Þ, for some assignment a , JFa0 ðmostComp ðE0 ÞÞ,
and a,a0 are such that equalhf 0 ðaðx 1 Þ,a0 ðx 1 0 ÞÞ evaluates
to true.
Consider now w0 ¼ a0 ðw0 ðx 01 ,y 01 ÞÞ. Based on Lemma 8,
0
we know there must be a homomorphism h : w-w0 .
0
We want to show that h is proper. By Lemma 11,
we know that a0 isinvertible. Since a0 is invertible, by
Lemma 8, we know that h is proper, and therefore w o w0 .
But this is not possible, since w is maximal with
respect to o .
This proves the claim and concludes the proof. &

709

Based on Theorem 6, we are now ready to prove our
main result:
Theorem 2. Given a scenario M ¼ ðS,T, Sst Þ, ScM is a core
schema mapping for M.
Proof. In order to prove the claim, we need to show that,
given a source instance I, the result of the chase of ScM
over I is the core universal solution for M over I, J 0 , i.e.
J chase ¼ ScM ðIÞ ﬃ J 0
We shall make use of the strong connection between the
two possible strategies suggested in the paper to generate
the core: the two-step one, and the single-step one that
uses source rewritings. More speciﬁcally, recall that, as an
alternative to chasing ScM , we might generate the core
following a two-step process. Based on Theorem 6, we
ﬁrst generate an isomorphism-free solution, J, by standard
chasing Sst on I, and then chase the following set of full
rules, SfM , one for each expansion E 2 expansionsðMÞ:

SfM ¼ fr full
E : 8x 1 ,y 1 : mostInf ðEÞ-wðx 1 ,y 1 Þ9E
2 expansionsðMÞg
Notice how this double-exchange approach uses a composition of s-t tgds plus full FO-rules. Our strategy in the
proof is to show that chasing the skolemized FO-rules in
ScM generates the same result using a single exchange.
However, there is a signiﬁcant difference in structure
among these two sets of rules: the rules in ScM are the
product of a normalization step and of a further rewriting
step, according to finalRew ðÞ.
We shall therefore apply the same normalization also to
conclusion of rules in SfM ; in doing this, despite the fact
that these rules only contain universally quantiﬁed variables, in each rule we shall treat the variables in y 1 as
existentially quantiﬁed. This will generate a new set of
full FO-rules SnM .
As a ﬁrst intermediate result, we now want to prove
that this normalization and this ﬁnal rewriting do not
have impact on the generation of the core.
Lemma 12. Given an isomorphism-free solution, J, the
result of chasing the two sets of rules SfM and SnM over J is
the same.
Proof. Let us call J0 the result of chasing SfM , and J n the
result of chasing SnM over J.
Being SfM a set of full dependencies, the normalization
procedure generates a set of logically equivalent new
dependencies. Since finalRew ðÞ only adds negated atoms
to the premises of these equivalent dependencies, we
know that J n DJ 0 . We now want to prove that it is also the
case that J0 D Jn .
Consider a witness block w in J0 . Assume w is generated
by a rule of the form
r full
E : 8x 1 ,y 1 : mostInf ðEÞ-wðx 1 ,y 1 Þ
and assignment a.
Consider aðmostInf ðEÞÞ. If wðx 1 ,y 1 Þ is normalized, w also
belongs to J n . Assume wðx 1 ,y 1 Þ is not normalized. Then, let
us consider each of its connected components. Each

710

G. Mecca et al. / Information Systems 37 (2012) 677–711

component ji ðx i ,y i Þ generates a rule of the form

if and only if there is a block of facts

r full
E,i :

premiseðr cE,i ÞðaðxÞÞ 2 Q premiseðrc Þ ðIÞ

mostInf ðEÞ-ji ðx i ,y i Þ

We want to prove that all sets of facts corresponding to
S
instances of the connected components, i faðji ðx i ,y i ÞÞg
belong to J n . There are two possible cases:
(a) JFaðfinalRew ðr full
E,i Þðx 1 ,y 1 ÞÞ, and therefore aðji ðx i ,y i ÞÞ
belongs to Jn as well;
(b) JjaðfinalRew ðr full
E,i Þðx 1 ,y 1 ÞÞ; this means that there
0 0 0
must be a different rule r full
E0 ,j with a conclusion j ðx ,y Þ
f
such that there is a proper formula homomorphism h
of jðx i ,y i Þ into j0 ðx 0 ,y 0 Þ, and some assignment a0 such
that: (i) JFa0 ðmostInf ðE0 ÞÞ and (ii) the formula equalhf
ðaðx i Þ,a0 ðx 0 ÞÞ evaluates to true. Suppose, without loss of
generality, that r full
E0 ,j is maximal with respect to proper
homomorphisms, i.e., its conclusion does not have homomorphisms into other rule conclusions. Notice that, since
JFa0 ðmostInf ðE0 ÞÞ,it must be the case that a0 ðj0 ðx 0 ,y 0 ÞÞ also
belongs to J0 ; moreover, since we assume that r full
E0 ,j is
maximal with respect to o , it also belongs to Jn .
In this case, by Lemma 8, we know there exists a
homomorphism h of aðjðx i ,y i ÞÞ into a0 ðj0 ðx 0 ,y 0 ÞÞ, and that
both belong to J 0 . Let us consider the image of aðjðx i ,y i ÞÞ
according to h: hðaðjðx i ,y i ÞÞÞ. It is possible to see that
aðjðx i ,y i ÞÞ ¼ hðaðjðx i ,y i ÞÞÞ, i.e., h must be the identity
mapping.
In fact, assume aðjðx i ,y i ÞÞahðaðjðx i ,y i ÞÞÞ. In this case,
consider the original witness block w 2 J, of which
aðjðx i ,y i ÞÞ is a connected component. By taking the other
connected components, and adding to them hða0 ðj0 ðx 0 ,y 0 ÞÞÞ
it would be possible to construct a new witness block w0
that also belongs to J0 , such that wJw0 , i.e., w is not an
induced witness block of w0 , and there exists a homomorphism of w into w0 . Notice that this homomorphism is
either proper, or compacting, or an isomorphism. But this
obviously contradicts the hypothesis, since by deﬁnition
of J0 w is not an induced block, it cannot have isomorphic
witness blocks, and is maximal with respect to ! and o .
Since h is the identity mapping, then aðji ðx i ,y i ÞÞ belongs
to J n .
This proves that J n contains all connected components
of w, and therefore w itself. &
We are now ready to correlate the results of the two
sets of rules, ScM , and SnM . We call premiseðrÞ the premise
of rule r. For each expansion E, we know that, for each rule
r cE,i 2 ScM of the form
r cE,i : premiseðr cE,i Þ-jskol ðx 1 Þ
there is a corresponding rule r nE,i 2 SnM of the form
r nE,i : premiseðr nE,i Þ-jðx 1 ,y 1 Þ
We concentrate on the two queries
Q premiseðrcE,i Þ ðIÞ

and

Q premiseðrnE,i Þ ðJÞ

We now want to prove the following Lemma.
Lemma 13. Consider an expansion E 2 expansionsðMÞ, a
source instance I and a canonical universal solution
J 2 USolM ðIÞ. There is a block of facts
premiseðr nE,i ÞðaðxÞ,bðyÞÞ 2 Q premiseðrnE,i Þ ðJÞ

E,i

Proof. Consider expansion E. It is possible to see that, by
construction, a block of facts of the form Eðaðx 1 Þ,bðy 1 ÞÞ
may exist in J if and only if a block of facts generated by
sourceRew ðEÞ – i.e., a block of the form sourceRew
ðEÞðaðx 1 ÞÞ – exists in I. In fact, recall that


^
E ¼ wl ðx 1 ,y 1 Þ4(x 2 ,y 2 : cl ðx 2 ,y 2 Þ equalhf ðx 1 ,x 2 Þ
E

l

sourceRew ðEÞ ¼ premiseðw ðx 1 ,y 1 ÞÞ4(x 2 : ðfðx 2 Þ
^
equalhf ðx 1 ,x 2 ÞÞ
E

Let us consider the three parts of each formula. Recall that
any assignment c such that JFwl ðcðx 1 Þ,cðy 1 ÞÞ must be a
canonical assignment. For wl ðaðx 1 Þ,bðy 1 ÞÞ to be contained in
J, each fact Rl ðaðx i Þ,bðy i ÞÞ in it must be contained in J. But, by
deﬁnition of canonical universal solution, this may happen
only if each premise of the corresponding tgds is satisﬁed by
a, i.e., if premiseðwl ðx 1 ,y 1 ÞÞðaðx 1 ÞÞ is contained in I.
Consider now the existentially quantiﬁed subformula.
Obviously there exist assignments a2 ,b2 such that
l
JFc ða2 ðx 2 Þ,b2 ðy 2 ÞÞ if and only if IFfða2 ðx 2 ÞÞ. Note also
that the two sets of equalities are exactly the same.
Therefore we may conclude that a block of facts of the
form Eðaðx 1 Þ,bðy 1 ÞÞ may exist in J if and only if a block of
facts of the form sourceRew ðEÞðaðx 1 ÞÞ exists in I.
A very similar argument holds for mostComp ðEÞ and
sourceRew ðmostComp ðEÞÞ, and also for mostInf ðEÞ and
sourceRew ðmostInf ðEÞÞ. Since the two sets of rules are
normalized in the same way, the claim also holds for the
ﬁnal rewritings generated by finalRew ðÞ. &
Based on Lemma 13, we have established a very close
connection between expansions and their source rewriting. More speciﬁcally, given an isomorphism-free solution
J, consider the two sets
J chase ¼ ScM ðIÞ

J 0 ¼ SnM ðJÞ

We can show that the two instances are equal up to
isomorphisms. In fact, consider a rule r cE,i 2 ScM , and the
corresponding rule r nE,i 2 SnM , as deﬁned above.
According to Lemma 13, the premise of r cE,i is satisﬁed by I
for an assignment a if and only if the premise of r nE,i is also
satisﬁed by J for assignment a on x 1 . Let us consider the
facts generated by ﬁring the two rules. We know that
jðaðx 1 Þ,bðy 1 ÞÞ is a fact block in J0 , while jskol ðaðx 1 ÞÞ is a
block of facts generated by properly assigning values to
Skolem terms.
However, the two blocks are isomorphic. In fact, we know
that jðaðx 1 Þ,bðy 1 ÞÞ is a canonical block, and therefore it has
been generated by the standard skolemization strategy over
jðaðx 1 Þ,bðy 1 ÞÞ. But, by hypothesis, the chosen skolemization
strategy, skol, is isomorphism-invariant, and therefore it
produces blocks of facts isomorphic to those produced by
the standard skolemization strategy.
Moreover, we know that, since J is isomorphism-free, J0
contains exactly one isomorphic copy of each witness
block. But this is true also for Jchase , since skol is isomorphism-invariant, and therefore by deﬁnition isomorphic

G. Mecca et al. / Information Systems 37 (2012) 677–711

instances of rule conclusions collapse into a single
representative.
Since we have proven that Jchase ﬃJ 0 , this concludes the
proof. &
Appendix D. Proof of Theorem 8

Theorem 3. Given a M ¼ ðS,T, Sst Þ such that Sst does not
contain self-joins in tgd conclusions. Given a source instance
I, call J a canonical universal solution for M over I, and J 0 the
core universal solution for M over I. Then:
(1) for any fact block bf in J, either all tuples in bf belong
also to J 0 or none of them does;
(2) for each tgd m 2 Sst whose conclusion has size k, all
/I,J S
witness blocks in W m
have size exactly k.
Proof. Let us ﬁrst prove item 1 of the claim. Assume there
is a fact block bf D J such that bf JJ 0 but at least one tuple in
bf belongs to J 0 . Let us ﬁrst note that, since bf must contain
more than one tuple, it therefore contains at least one null
value. Call bf 0 the proper subset of bf that belongs to J 0 . Call
N any null value in bf bf 0 , and Ri ðt N Þ,Rj ðt N0 Þ two tuples that
contain N, such that Rj ðt N0 Þ 2 bf 0 , Ri ðt N Þ 2 bf bf 0 . Notice
that, since M does not contain self-joins in tgd conclusions,
it must be the case that Ri aRj .
Since J 0 is the core of J, there must be an endomorphism
h : J-J0 . Consider the image of bf according to h, hðbf Þ. It
is possible to see that Ri ðt N Þ=
2hðbf Þ. In fact, since Ri ðt N Þ 2 bf
but Ri ðt N Þ=
2bf 0 , it must be the case that hðRi ðt N ÞÞaRi ðt N Þ.
This, in turn, means that hðNÞaN. In fact, Ri ðt N Þ cannot be
mapped to any other tuple Rj ðt N0 Þ 2 bf 0 that contains N,
since we know that Ri aRj . Therefore, Ri ðt N Þ must be
mapped to a tuple that not contains N, and hðNÞaN.
Consider now a tuple Rj ðt N0 Þ 2 bf 0 . Since hðNÞaN, it must
be the case that hðRj ðt N0 ÞÞaRj ðt N0 Þ. But this means that J 0
contains two different tuples, Rj ðt N0 Þ and hðRj ðt N0 ÞÞ, and
therefore it has an endomorphism into its proper subset
J 0 fRj ðt N0 Þg. As a consequence, J0 is not the core universal
solution. This contradicts the assumption and proves
the claim.
Let us now prove item 2. Assume there is a tgd m with
conclusion cðx,yÞ of size k such that there exists a witness
/I,J S
0
block w 2 W m
of size k ak. By deﬁnition of witness
0
block, we know that it must be k r k. Therefore, it must
0
be the case that k ok. But this is clearly impossible, since
any witness block for m must be an instance of cðx,yÞ
according to some assignment c. Since cðx,yÞ does not
contain self-joins, for any assignment c, cðcðx,yÞÞ is a
collection of facts each belonging to a different relation
and therefore has size exactly k, which contradicts the
assumption. This concludes the proof. &

References
[1] B. Alexe, W. Tan, Y. Velegrakis, Comparing and evaluating mapping
systems with STBenchmark, Proceedings of the VLDB Endowment 1
(2) (2008) 1468–1471.

711

[2] Y. An, A. Borgida, R.J. Miller, J. Mylopoulos. A semantic approach to
discovering schema mapping expressions, in: Proceedings of the
ICDE, 2007, pp. 206–215.
[3] M. Arenas, J. Pérez, C. Riveros, The recovery of a schema mapping:
bringing exchanged data back, ACM TODS 34 (4) (2009) 1–48.
[4] C. Beeri, M.Y. Vardi, A proof procedure for data dependencies,
Journal of the ACM 31 (4) (1984) 718–741.
[5] P. Bohannon, E. Elnahrawy, W. Fan, M. Flaster, Putting context into
schema matching, in: Proceedings of the VLDB, VLDB Endowment,
2006, pp. 307–318.
[6] A. Bonifati, G. Mecca, A. Pappalardo, S. Raunich, G. Summa, Schema
mapping veriﬁcation: the spicy way, in: Proceedings of the EDBT, 2008,
pp. 85–96.
[7] L. Cabibbo, On keys, foreign keys and nullable attributes in relational
mapping systems, in: Proceedings of the EDBT, 2009, pp. 263–274.
[8] J. Cheney, L. Chiticariu, W.C. Tan, Provenance in databases: why, how,
and where, Foundations and Trends in Databases 1 (4) (2009)
379–474.
[9] L. Chiticariu, Computing the core in data exchange: algorithmic
issues, MS Project Report, 2005, unpublished manuscript.
[10] L. Chiticariu, W.C. Tan, Debugging schema mappings with routes,
in: Proceedings of the VLDB, 2006, pp. 79–90.
[11] R. Fagin, Inverting schema mappings, ACM TODS 32 (4) (2007).
[12] R. Fagin, P.G. Kolaitis, R.J. Miller, L. Popa, Data exchange: semantics
and query answering, Theoretical Computer Science 336 (1) (2005)
89–124.
[13] R. Fagin, P.G. Kolaitis, L. Popa, Data exchange: getting to the core,
ACM TODS 30 (1) (2005) 174–210.
[14] R. Fagin, P.G. Kolaitis, L. Popa, W. Tan, Composing schema mappings: second-order dependencies to the rescue, ACM TODS 30 (4)
(2005) 994–1055.
[15] R. Fagin, P.G. Kolaitis, L. Popa, W. Tan, Quasi-inverses of schema
mappings, ACM TODS 33 (2) (2008) 1–52.
[16] R. Fagin, P.G. Kolaitis, A. Nash, L. Popa, Towards a theory of schemamapping optimization, in: Proceedings of the ACM PODS, 2008,
pp. 33–42.
[17] A. Fuxman, M.A. Hernández, C.T. Howard, R.J. Miller, P. Papotti,
L. Popa, Nested mappings: schema mapping reloaded, in: Proceedings of the VLDB, 2006, pp. 67–78.
[18] G. Gottlob, A. Nash, Efﬁcient core computation in data exchange,
Journal of the ACM 55 (2) (2008) 1–49.
[19] G. Gottlob, R. Pichler, V. Savenkov, Normalization and optimization
of schema mappings, Proceedings of the VLDB Endowment 2 (1)
(2009) 1102–1113.
[20] P. Hell, J. Nešetřil, The core of a graph, Discrete Mathematics 109
(1–3) (1992) 117–126.
[21] R. Hull, M. Yoshikawa, ILOG: declarative creation and manipulation
of object identiﬁers, in Proceedings of the VLDB, 1990, pp. 455–468.
[22] A.Y. Levy, A.O. Mendelzon, Y. Sagiv, D. Srivastava, Answering
queries using views, in: PODS 1995, pp. 95–104.
[23] B. Marnette, G. Mecca, P. Papotti, Scalable data-exchange with
functional dependencies, Proceedings of the VLDB Endowment 3
(1) (2010) 105–116.
[24] G. Mecca, P. Papotti, S. Raunich, Core schema mappings, in
Proceedings of the ACM SIGMOD, 2009, pp. 655–668.
[25] G. Mecca, P. Papotti, S. Raunich, M. Buoncristiano, Concise and
expressive mappings with þSPICY, Proceedings of the VLDB Endowment 2 (2) (2009) 1582–1585.
[26] R.J. Miller, L.M. Haas, M.A. Hernandez, Schema mapping as query
discovery, in: Proceedings of the VLDB, 2000, pp. 77–99.
[27] L. Popa, Y. Velegrakis, R.J. Miller, M.A. Hernandez, R. Fagin, Translating web data, in: Proceedings of the VLDB, 2002, pp. 598–609.
[28] R. Pottinger, A. Halevy, MiniCon: a scalable algorithm for answering
queries using views, VLDB Journal 10 (2–3) (2001) 182–198.
[29] A. Rafﬁo, D. Braga, S. Ceri, P. Papotti, M.A. Hernández, clip: a visual
language for explicit schema mappings, in: Proceedings of the ICDE,
2008, pp. 30–39.
[30] V. Savenkov, R. Pichler, Towards practical feasibility of core
computation in data exchange, in: Proceedings of the LPAR, 2008,
pp. 62–78.
[31] B. ten Cate, L. Chiticariu, P. Kolaitis, W.C. Tan, Laconic schema mappings: computing core universal solutions by means of SQL queries,
Proceedings of the VLDB Endowment 2 (1) (2009) 1006–1017.
[32] B. ten Cate, P. Kolaitis, Structural characterizations of schemamapping languages, in: Proceedings of the ICDT, 2009, pp. 63–72.
[33] L.L. Yan, R.J. Miller, L.M. Haas, R. Fagin, Data driven understanding
and reﬁnement of schema mappings, in: Proceedings of the ACM
SIGMOD, 2001, pp. 485–496.

Clip: a Tool for Mapping Hierarchical Schemas
Alessandro Raffio

Daniele Braga

Stefano Ceri

Politecnico di Milano
Milano, Italy

Politecnico di Milano
Milano, Italy

Politecnico di Milano
Milano, Italy

raffio@elet.polimi.it

braga@elet.polimi.it

ceri@elet.polimi.it

Paolo Papotti

Mauricio A. Hernández

Università Roma Tre
Roma, Italy

Almaden Research Center
IBM San Jose, California, US

mauricio@almaden.ibm.com

papotti@dia.uniroma3.it

source
dept [1..*]
dname
value:String
? Proj [0..*]
@pid:int
pname
value:String
? regEmp [0..*]
@pid: int
ename
value:String
sal
value:int

ABSTRACT
Many data integration solutions in the market today include visual
tools for schema mapping. Users connect schema elements with
lines that are interpreted as high-level logical expressions capturing
the relationship between source and target data-sets. These expressions are compiled into queries or programs that convert sourceside data instances into target-side instances. In this demo we
showcase Clip, an XML Schema mapping tool. Clip is distinguished from existing tools in that mappings explicitly specify structural transformations in addition to value correspondences. We
show how Clip’s users enter mappings by drawing lines and how
these lines are translated into XQuery.
Categories and Subject Descriptors: H.2.3 [Languages]: Query
languages
General Terms: Languages, Design, Experimentation.

1.

target
department [1..*]
? project [0..*]
@name: String
?

employee [0..*]
@name: String

Figure 1: A simple yet problematic mapping

INTRODUCTION

Schema mappings are declarative constructs that relate source
and target schemas. Mappings are typically created since they 1)
can be automatically translated into a query or program that converts source instances into target instances, and 2) are easier to enter
and specify than the equivalent transformation program. Mapping
tools help data integration engineers write long and complex data
transformation programs with GUI gestures that, in many cases,
involve drawing lines connecting source and target object.
Schema mapping tools are typically characterized by a GUI that
places a source structure on one side of the screen and a target structure on the other side. Users specify correspondences by drawing
lines across these structures and annotating them with features that
carry some of the transformation semantics (e.g., filtering predicates, functions, etc.). Many examples of mapping tools are available in the market today (e.g., Altova MapForce1 , Microsoft BizTalk2 ,
Stylus Studio3 , IBM Rational Data Architect4 ), all of which are capable of generating a query or program (of varying complexity)
that transforms source data into target data. Clio is a schema mapping prototype which automatically creates mappings given two
schemas and a set of value mappings between their atomic elements. Depending on the kind of source and target schemas, Clio
1

www.altova.com/MapForce
www.microsoft.com/biztalk
3
www.stylusstudio.com
4
www.ibm.com/software/data/integration/rda
2

Copyright is held by the author/owner(s).
SIGMOD’08, June 9–12, 2008, Vancouver, BC, Canada.
ACM 978-1-60558-102-6/08/06.

1271

can render queries that convert source data into target data in a number of languages (XQuery, XSLT, SQL/XML, SQL) [2].
This demo presents Clip [5], a tool that uses a new schema mapping language specifically designed for hierarchical schemas. Similarly to other mappings tools, users of Clip enter mappings by
drawing lines across schema elements and by annotating some of
these lines. Clip then produces the queries that implement the mapping. The main difference of Clip relative to its predecessors is the
introduction of structural mappings in addition to value mappings.
Structural mappings relate schema elements with complex structure, determining correspondences at an higher level of abstraction.
This gives users greater control over the resulting transformations,
which are much less dependent on hidden automatisms and therefore less frequently fail in capturing the intended semantics. Clip
was designed to work with XML Schemas, but can also work with
relational schemas, as long as they are converted in a canonical
XML representation. In general, Clip should work with any schema
model that can be visually represented as a nested containment of
relations.
The schema mapping generation problem a roused deep interest among the research community in the last years [4, 3, 1].
Schema mapping generation studies how to automatically create
schema mappings given as input two schemas and a set of value
mappings between their atomic elements. With respect to existing approaches, Clip allows users to express more complex mappings by introducing a new GUI, extended with structural correspondences and aggregate functions. Our contribution tries to bring
an initial answer to the growing demand for more complex schema
mappings.

2.

A MOTIVATING EXAMPLE

Value mappings are thin arrows with
open ends connecting value nodes;
optional labels may specify an
aggregate function. Builders are thick
arrows with close ends connecting
$x.att1=$y.att2 elements and possibly build nodes.

<<aggregate>>

This demo is about the Clip GUI, which has been designed and
implemented as an extension of Clio. Consider the following XML
document describing two departments (compacted due to space
limits):

$x
$y

source
|--dept--dname = ICT
|
|---Proj--@pid = 0001
|
|
‘---pname = Appliances
|
|---Proj--@pid = 0002
|
|
‘---pname = Robotics
|
|---regEmp--@pid = 0001
|
|
|---ename = J.Smith
|
|
‘---sal = 10000
|
|---regEmp--@pid = 0001
|
|
|---ename = A.Clarence
|
|
‘---sal = 12000
|
|---regEmp--@pid = 0002
|
|
|---ename = M.Tane
|
|
‘---sal = 10500
|
‘---regEmp--@pid = 0002
|
|---ename = J.Bellish
|
‘---sal = 11000
‘--dept--dname = Marketing
|---Proj--@pid = 0001
|
‘---pname = Brand promotion
|---Proj--@pid = 0032
|
‘---pname = Appliances
|---regEmp--@pid = 0001
|
|---ename = R.Dawson
|
‘---sal = 30000
|---regEmp--@pid = 0032
|
|---ename = M.Tane
|
‘---sal = 10000
‘---regEmp--@pid = 0001
|---ename = S.Aiking
‘---sal = 20000

group-by
{ attributes }

Build nodes have at least 1 incoming
and at most 1 outgoing builder and a
label expresses filtering conditions in
terms of the variables on the builders.

A special build node is used for grouping. The grouping
attributes are reported along with the “group-by” keyword.
CPTs are trees of build nodes
and context arcs.

11234536
27189
93126
37446
Figure 2: The Clip syntax in a nutshell
all needed structural-level mappings in all obvious manners. These
mappings are known as “basic mappings” and can be expressed as
tuple-generating dependencies (TGD) [4]. In the case of this example, Clio computes two basic mappings, each of which will results
on a separate query. These mappings can be expressed follows:
(1) ∀ d ∈ /source/dept, p ∈ $d/Proj →
∃ d ∈ /target/department, p ∈ $d /project
where $p/pname = $p /@name
(2) ∀ d ∈ /source/dept, p ∈ $d/Proj, r ∈ $d/regEmp
where $p/@pid = $r/@pid →
∃ d ∈ /target/department, e ∈ $d /employee
where $p/pname = $e/@name

Each department has a name, a list of projects (with an identifier and a name), and a list of regular employees (with a name, a
salary, and a @pid attribute referring to the project they work on).
This instance is valid w.r.t the source XML Schema on the left of
Figure 1.
We want to transform the source instances to make them compliant with the target schema on the right of Figure 1. The desired
output is:

A second strategy is to combine the basic mappings into “nested
mappings” [2]. But in the case of this example, Clio cannot simplify the basic mappings into a nested mappings because Clio does
not know how to extract the common dept to department part of
the two basic mappings as the top-level of a nested mapping.
In the next section, we introduce the mapping language of Clip.
We return to this mapping example in Section 4 and explain how
this mapping is expressed with Clip.

target---department---project---@name = Appliances
|
|---project---@name = Robotics
|
|---employee---@name = J.Smith
|
|---employee---@name = A.Clarence
|
|---employee---@name = M.Tane
|
‘---employee---@name = J.Bellish
‘---department---project---@name = Brand promotion
|---project---@name = Appliances
|---employee---@name = R.Dawson
|---employee---@name = M.Tane
‘---employee---@name = S.Aiking

3. OVERVIEW OF THE CLIP LANGUAGE
Clip’s GUI represents XML Schemas as trees; attributes and text
are represented by black and white circles respectively, elements
by squares, cardinalities by icons and labels:
value
nodes

Figure 1 shows how we can attempt entering this transformation
using Clio. Clio only gets close to the desired target and produces a
query that outputs projects and employees, but encloses each node
in different department element, not preserving containment and
sibling relationships:

@name:
type
type

single ? tagname
elements [0..1]
tagname

multiple
tagname
elements ? [1..*]
tagname
[0..*]

Figure 1 depicts a source and a target XML schema rendered
using our tree representation. The source schema, for example,
has a root-level source element containing a repeatable and nonoptional dept element. This dept element contains, in turn, a leaflevel element named dname, and repeatable and optional structural
elements Proj and regEmp. The line between the @pid attributes
depicts a key/foreign key constraint specified in the schema.
Figure 2 summarizes the visual syntax of Clip. Clip uses two
different kinds of lines to connect source and target nodes:

target---department---project---@name = Appliances
|---department---project---@name = Robotics
...
‘---department---employee---@name = S.Aiking

To understand why this happens, we first need to understand how
Clio generates mappings from value mapping lines. Clio uses two
(related) strategies to generate mappings. In the first strategy, Clio
creates mapping expressions that surrounds the mapping lines with

Value mappings connect value nodes to establish correspondences
between atomic values (as in Clio). Value mappings are entered manually in Clip but future versions could use schema
matching techniques to create some of them automatically [6].

1272

source
dept [1..*]
dname
value: String
? Proj [0..*]
@pid: int
pname
value: String
? regEmp [0..*]
@pid: int
ename
value: String
sal
value: int

The second context arc reaching the regEmp → employee object mapping also operates in the context of the top-level dept →
department mapping. For illustration purposes, we added a filter
to the build node of this object mapping, so as to only transform
those regEmp elements whose sal value is greater that 11000.
In effect, this Clip mapping can be written as the following “nested
mapping” expression:

target
department [1..*]
?

project [0..*]
@name: String

?

employee [0..*]
@name: String

(3) ∀ d ∈/source/dept →
∃ d ∈/target/department
where (∀ p ∈ $d/Proj →
∃ p ∈ /target/project
where $p/pname = $p /@name )∧
(∀ r ∈ $d/regEmp
∃ e ∈ $d /employee
where $r/pname = $e/@name ∧ $r/sal > 11000 )

$r

$r.sal.value
> 11000

Figure 3: A Clip mapping

After Clip converts the mapping into a query that captures the
semantics above, we obtain the following target instance:

Object mappings (or builders) connect elements and rule structural
transformations. Intuitively, object mappings represent iterators on their source schema nodes. For each source iteration,
a new element for the target schema node is constructed.

target---department---project---@name = Appliances
|
|---project---@name = Robotics
|
‘---employee---@name = A.Clarence
‘---department---project---@name = Brand Promotion
...

Value mappings can be annotated with a transformation function
that expresses how source data values are converted into a target
value. For example, users can express that a target name value
is created by concatenating a first-name and a last-name values
from the source. These annotations are entered on a node placed in
the middle of the value mapping lines.
Similarly, object mapping lines can be annotated with expressions that provide some extra transformation semantics. These annotations are also entered on nodes placed in the middle of object
mapping lines. These nodes are called Build nodes. As an example, we can add filtering conditions on the mapping going from the
source depts to the target departments, or add join conditions on
mappings involving two or more nodes of the source schema. Also,
we can annotate object mappings with grouping conditions, so that
the target elements are not built anymore for every source element
connected to the builder, but for every group of elements with the
same values for the grouping dimensions.
Build nodes can also be connected to other build nodes using
context arcs, so as to form trees of build nodes, named context
propagation trees (CPTs). Complex hierarchical transformations
are expressed by means of CPTs that connect top-level object mappings to lower-level object mappings.
Not all combinations of value mappings and object mappings
produce valid target instances (i.e., instances conforming to the target schema). A mapping is valid if, given any instance of the source
schema, it produces a valid instance of the target schema. We cannot discuss here the rather sophisticated syntactic rules that Clip
uses to enforce the expression of valid mappings. The syntax and
semantics of Clip are fully described in [5].

In order to stress the role of the context propagation tree, we observe that omitting the context arcs between the outer and the two
inner drivers in the mapping of Figure 3 would cause all projects
and all (rich) employees to be repeated within all departments, disregarding the original containment relationships:

4.

target---department---project---@name = Appliances
|
|---project---@name = Robotics
|
|---project---@name = Brand Promotion
|
|---employee---@name = A.Clarence
...
...
‘---department---project---@name = Appliances
...

5. THE DEMO
The current implementation of Clip includes two components: a
GUI for the expression and editing of mappings, and a translator
that produces XQuery transformations corresponding to the mappings. The GUI is designed as an extension of Clio, reusing our
experience on the definition of visual languages gained with Clio
and other query systems, and by aiming at the best balance between ease of use, expressive power, and effectiveness. The query
generation component takes the builders and value mappings entered in the GUI, together with information on the source and target schemas, to compute a set of nested FLWOR expressions. As
an example, the mapping of Figure 3 translates to the following
query:
for $d in /source/dept
return <department>
{ for $p in $d/Proj
return <project>
{ attribute name {$p/pname/text()} }
</project> }
{ for $r in $d/regEmp
where $r/sal/text() > 11000
return <employee>
{ attribute name {$r/ename/text()} }
</employee> }
</department>

A CLIP MAPPING

We now discuss how the simple mapping described in Section 2
is expressed in Clip. Figure 3 shows the CPT and the value mappings required to capture the desired transformation semantics. The
top-level build node of the CPT creates a department for each
source dept. For each such dept, the context arc reaching the
Proj → project object mapping expresses that for each source Proj
within the current dept, a target project is created as a sub-element
of the current target department element. The pname → @name
value mapping executes in the context of this inner build node.

Notice that in this query, as well as in the nested TGD expression (3) in Section 4, we omitted the join between regEmp and
Proj elements. This join condition is not needed in this transformation, since we do not need to maintain the association between the

1273

Figure 4: A screenshot of the Clip GUI in action.
regEmp and Proj elements on the target side. Clio, on the other
hand, does produce this join condition (see mapping expression (2)
in Section 2) because it might become useful if a value under Proj
needs to be copied into the target under employee. This demonstrates that Clip users can fine tune some of the mapping expressions produced by Clio into more efficient transformations.
Figure 4 shows a screenshot of Clip’s GUI with a more complex mapping, involving a two builders and grouping conditions to
invert the hierarchical structure of the source schema and collect
departments under projects grouped by pname. The mapping
resolves to the following XQuery expression:

The demo will stress the expressivity gain of Clip compared to
its predecessor. We will demonstrate Clip using schemas of varying degree of complexity and show how different transformations,
from simple mappings, to joins with aggregates, are drawn with our
GUI. We will then show how these mappings are translated into relatively complex XQuery fragments. To appreciate the expressiveness of Clip, we can also invite our audience to try expressing some
common XQuery transformations using only Clip mapping lines.

6. REFERENCES
[1] A. Bonifati, E. Q. Chang, T. Ho, V. S. Lakshmanan, and
R. Pottinger. HePToX: Marrying XML and Heterogeneity in
Your P2P Databases. In VLDB, pages 1267–1270, 2005.
[2] A. Fuxman, M. A. Hernández, H. Ho, R. J. Miller, P. Papotti,
and L. Popa. Nested Mappings: Schema Mapping Reloaded.
In VLDB, pages 67–78, 2006.
[3] S. Melnik, P. A. Bernstein, A. Halevy, and E. Rahm. Applying
Model Management to Executable Mappings. In SIGMOD,
pages 167–178, 2005.
[4] L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernández, and
R. Fagin. Translating Web Data. In VLDB, pages 598–609,
2002.
[5] A. Raffio, D. Braga, S. Ceri, P. Papotti, and M. A. Hernández.
Clip: a visual language for explicit schema mappings. In
ICDE, 2008.
[6] E. Rahm and P. A. Bernstein. On Matching Schemas
Automatically. VLDB Journal, 2001.

let $pnames := distinct-values(/source/dept/Proj/pname)
for $pname in $pnames
let $group := ( for $p in /source/dept/Proj
where $p/pname = $pname return $p )
return <project>
{ attribute name { $pname } }
{ attribute numDept
{ count(for $d in /source/dept
where some $p1 in $d/Proj, $p2 in $group
satisfies $p1 is $p2
return $d) } }
{ for $d in /source/dept
where some $p1 in $d/Proj, $p2 in $group
satisfies $p1 is $p2
return <department>
{ attribute name {$d/dname} }
</department> }
</project>

The query above results in the following target instance:
target---project---@name = Appliances
|
|---@numDept = 2
|
|---department---@name
|
‘---department---@name
|---project---@name = Robotics
|
|---@numDept = 1
|
‘---department---@name
‘---project---@name = Robotics
|---@numDept = 1
‘---department---@name

= ICT
= Marketing

= ICT

= ICT

1274

Extraction and Integration of
Partially Overlapping Web Sources
Mirko Bronzi1 , Valter Crescenzi1 , Paolo Merialdo1 , Paolo Papotti2
1

2

Università degli Studi Roma Tre, Rome, Italy
Qatar Computing Research Institute, Doha, Qatar

{bronzi,

crescenz, merialdo, papotti}@dia.uniroma3.it

ABSTRACT
We present an unsupervised approach for harvesting the data exposed by a set of structured and partially overlapping data-intensive
web sources. Our proposal comes within a formal framework tackling two problems: the data extraction problem, to generate extraction rules based on the input websites, and the data integration
problem, to integrate the extracted data in a unified schema. We
introduce an original algorithm, WEIR, to solve the stated problems
and formally prove its correctness. WEIR leverages the overlapping data among sources to make better decisions both in the data
extraction (by pruning rules that do not lead to redundant information) and in the data integration (by reflecting local properties of a
source over the mediated schema). Along the way, we characterize
the amount of redundancy needed by our algorithm to produce a
solution, and present experimental results to show the benefits of
our approach with respect to existing solutions.

1.

Figure 1: Detail pages of stock quotes from the Reuters and
Google finance websites.

INTRODUCTION

It is well recognized that the Web is a valuable source of information and that making use of its data is an incredible opportunity
to create knowledge with both scientific and commercial implications. Although the impressive number of sources and domains
on the Web has given rise to several research proposals for automatically extracting and integrating web data, so far extraction and
integration problems have been mainly tackled separately: many
researchers have proposed techniques for extracting data from the
Web [12], while others have concentrated their solutions on integrating the extracted data [5].
Information extraction systems, such as ReVerb [21], tackle the
two issues synergically and aim at extracting and integrating huge
amounts of data from the Web. However, these systems concentrate
on textual corpora: they exploit lexical-syntactic patterns, which
are not suitable for extracting data embedded in HTML pages, and
they cannot handle generic tuples of more than two attributes as
they only extract binary relations.
This paper proposes a novel technique for the automatic extraction and integration of data from large websites that publish detail pages about objects of vertical domains. As an example con-

sider Figure 1, which presents pages containing stock quote details.
Given a domain of interest, large sets of detail pages collected from
multiple websites can be considered as data-intensive information
sources, with striking characteristics. As observed in [19] and [27],
these sources partially overlap, i.e., they provide redundant information both at the schema level (some attributes are published by
several sources), and at the instance level (some objects are published by several sources). In addition, pages from the same collection share a common structure, as they are generated by scripts that
encode data into a local HTML template. Also, we observe that it
is rather easy to collect detail pages from these websites by means
of a crawler based on set-expansion techniques (e.g., [6]) for the
surface Web, or by using form-filling techniques (e.g., [28]) for the
hidden Web.
The process of extracting and integrating data-intensive sources
can be articulated as follows: (i) transform the set of web pages
from each source into a relation by creating web wrappers, i.e.,
data extraction programs; (ii) integrate these relations by defining semantic mappings between the data exposed by the wrappers;
(iii) create a mediated schema starting from the mappings and assign a global label (a meaningful name) to each mapping.
Although each task is addressed by several proposals, considerable human effort is still needed in each step of the process. As related to data extraction, in order to craft production-level wrappers,
supervised approaches [18, 22] require annotated pages, whereas
unsupervised ones [3, 16] cannot provide accurate results without
external feedback. Despite recent results [5], the schema matching problem in our context is challenging because web data are

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Articles from this volume were invited to present
their results at The 39th International Conference on Very Large Data Bases,
August 26th - 30th 2013, Riva del Garda, Trento, Italy.
Proceedings of the VLDB Endowment, Vol. 6, No. 10
Copyright 2013 VLDB Endowment 2150-8097/13/10... $ 10.00.

805

2.

: Yahoo!

: Reuters

inherently imprecise, and sources may provide conflicting information for the same objects [27]. Similar issues apply to schema
integration approaches based on clustering, where even a manually tuned algorithm cannot be applied to every scenario. These
technical challenges are reflected in systems designed for web data
extraction and integration [9, 23, 24, 30], where ad-hoc user input
(such as annotated pages) is required to achieve acceptable results.
Our solution aims at overcoming limits of existing techniques
by leveraging unique characteristics of data-intensive information
sources. Our approach builds on a generative model that assumes
the existence of a hidden abstract relation. According to our model,
each source publishes the detail pages by embedding into a local
HTML template a partial view over the same abstract relation, possibly introducing imprecise values. From this perspective, the data
extraction and integration issues can be seen as the problem of inverting the generative process, i.e., discovering the abstract relation
given the detail pages.
Given an input set sources, a set of candidate wrappers is automatically generated for each source. Rather than choosing the
output wrappers based only on the local regularities of each source,
as done by traditional unsupervised data-extraction approaches, our
solution relies on the redundancy of data extracted among different
overlapping sources. The main intuition is that a correct wrapper
will most likely extract data that match with those extracted from
at least one other correct wrapper from a different source.
Data redundancy is also exploited by an instance-based clustering algorithm to define the mappings among the data extracted from
different sources, as done via traditional integration approaches.
However, we propose an original algorithm that relies on natural
constraints caught by our generative model in order to automatically create mappings, without any dependency on external information, such as thresholds or domain knowledge. Also, our algorithm is able to identify meaningful labels for the mediated schema.
Contributions. The paper makes the following contributions:
(i) we formulate an abstract generative model that characterizes
partially overlapping data-intensive web sources; (ii) based on the
model, we introduce a formal setting to state the data extraction
and integration problem, exploiting the redundancy of information among the sources; (iii) we propose an unsupervised algorithm, WEIR (Web-Extraction and Integration of Redundant data),
to solve the stated problem, and formally study its correctness;
(iv) we show robustness and performance of our approach against
alternative solutions in an experimental evaluation with real-world
websites.
Outline. The paper is organized as follows: Section 2 introduces our abstract generative model for partially overlapping web
sources; Section 3 formally states the problem of extracting and integrating data from these sources. Then, we present our algorithm,
WEIR , to solve the problem: Section 4 faces the integration issue
assuming that the wrappers are correct, and Section 5 discusses
its extension to real wrappers. Section 6 presents an experimental
evaluation: we compare our approach with other proposals, using
pages from real websites and a public dataset. Section 7 discusses
related work and Section 8 concludes the paper.

: Google

TICKER

HIGH

LOW

…

TICKER PRICE VOLUME

CAP

…

TICKER VOLUME CAP

AAPL

259.4

253.4

…

AAPL

256.9

29,129,000 506B

…

AAPL

29,1M

506B …

CSCO

21.09

20.73

…

CSCO

20.91

34,770,000 226B

…

CAT

7,7M

56B

…

…

…

…

…

…

…

…

…

…

TICKER

PRICE

HIGH

LOW

VOLUME

CAP

…

AAPL

256.88

259.40

253.35

29,129,032

506B

…

CSCO

20.91

21.09

20.72

34,777,673

111.5B …

CAT

60.76

62.42

60.05

7,709,405

56B

…

…

…

…

…

…

…
…

…

Figure 2: The abstract generative model: partially overlapping
web sources as the result of a pipeline of operators that embed
views over an abstract relation into HTML templates.

by publishing data taken from H. We call abstract instances the
tuples of the relation H. Each tuple represents a real-world object
of the entity of interest. For example, in the case of the S TOCK Q UOTE entity, the abstract instances of H model the data about the
Cisco stock quote, the IBM stock quote, and so on. H has a set of
attributes, called abstract attributes, and we write A ∈ H to denote
that A is an abstract attribute of H. In our example, they represent
the attributes associated with a stock quote, such as Ticker, Price,
Volume, etc.
Given a set of sources S = {S1 , . . . , Sm }, each source can be
seen as the result of a generative process applied over the abstract
relation H. The attributes published by a source S are called physical attributes of S (or simply attributes), as opposed to the abstract
attributes of H. We write a ∈ A to indicate that a is a physical
attribute associated with the abstract attribute A ∈ H, and ai to
denote that a is a physical attribute published by the source Si . We
call domain, denoted D = (S, H), a pair of elements such that S
is a set of sources publishing attributes of an abstract relation H.
Every source publishes a subset of the abstract attributes, for
a subset of the abstract instances. Sources can change formats,
and they can introduce mistaken values. To model inconsistencies
among redundant sources, we assume that sources are noisy: they
may introduce errors, imprecise or null values, over the data picked
from the abstract relation. Sources can also publish attributes that
do not come from the abstract relation, such as advertisements,
page publication or modification date. However, we treat these attributes as coming from the abstract relation H and published by
exactly one source.
To summarize, the set of pages published by a source Si can be
thought of as a view over the abstract relation, obtained by the following expression: Si = λi (ei (πi (σi (H)))), where σi (selection):
returns a relation containing a subset of the abstract instances; πi
(projection): returns a relation containing a subset of the abstract
attributes; ei (error): returns a relation, such that each value is kept
or replaced with either a null value, or a wrong value; λi (encode):
produces a web page by encoding a tupleinto an HTML template.
From this perspective, as we formalize in Section 3, the extraction of data from the sources corresponds to invert the λi operators, for obtaining the relation ei (πi (σi (H))) associated with each

THE GENERATIVE MODEL

We are interested in extracting and integrating information about
an entity of interest, starting from a set of websites publishing detail
pages containing the attribute values of its instances.
In order to formalize our problem, we introduce the abstract generative model for data-intensive websites depicted in Figure 2. We
can imagine that an abstract relation H provides data about all the
instances of the entity, and that sources generate their detail pages

806

3.

source Si . The overall integration problem corresponds to reconstruct H from the data published by the set of sources S.
We conclude the presentation of the generative model with two
important properties that model natural constraints on the data published by data-intensive sources. As we shall discuss in the next
sections, we leverage these properties to automatically extract and
integrate data from the sources.

In this section, we introduce the notions of wrapper and mapping; then, we state the problem of recovering the abstract relation
from a set of web sources that publish its attributes.

3.1

the original values of the abstract attributes. However, we expect
that a source is locally consistent: if it publishes an abstract attribute more than once, the corresponding physical attributes are
identical. For example, we expect that if a source presents the stock
price for a company in different portions of its pages, all the reported values are identical. To formalize this property, we say that
a domain (S, H) is locally consistent if and only if:
∀a ∈ A, b ∈ B, LC(a, b) : A = B ⇒ a = b
where LC(a, b) denotes that a and b are physical attributes published by the same source. This property implies that, whenever
the same source delivers different attributes, we can conclude that
they correspond to distinct abstract attributes.

Separable semantics. We expect that errors introduced by the
sources do not distort data to the extent that attributes with different
semantics have more similar values than attributes with the same
semantics.
To formalize this property, we need to introduce a tool to compare the values of the attributes. We rely on a normalized distance
function d(·, ·) that compares the values of two physical attributes,
and returns a real number between 0 and 1: the more similar the
values, the lower the distance. In Section 4.3 we introduce a concrete definition for the distance function.
Based on the distance function over the attributes of a domain,
we bound the errors introduced in the publishing process as follows: let dA denote the maximal distance among attributes related
to a redundant abstract attribute A ∈ H:
max

ai ,aj ∈A: ai 6=aj

Example 2 Figure 4(a) depicts the DOM trees for the pages of a
hypothetical source S publishing attributes of an abstract relation
H = {Ticker, High, CEO, Volume}, and Figure 4(b) reports some
extraction rules expressed as XPath expressions: r1 , r2 , r3 , and
r5 are correct rules for the attributes Ticker, High, CEO and Volume, respectively. Note that r 4 is a weak rule, since it extracts
the Volume only for the right page of Figure 4(a) (for the left page,
it extracts the CEO). The wrapper ws ={r1 , r5 } is sound whereas
wns ={r4 , r5 } is not; the wrapper wc ={r1 , r2 , r3 , r5 , r6 } is complete but not sound, whereas wsc ={r1 , r2 , r3 , r5 } is sound and
complete.

d(ai , aj );

and let DA denote the minimal distance among an attribute of A ∈
H and any other physical attribute related to a different abstract
attribute B ∈ H:
DA =

min

a∈A,b∈B:A6=B

Wrappers and Mappings

In our framework, a data source S is an ordered set of pages
S = {p1 , . . . , pn } from the same website, such that each page
publishes information about one object of the real-world entity of
interest. A wrapper w is a set of extraction rules (or simply rules),
w = {r1 , . . . , rk } over a web page. The value extracted from a
rule r over a page p, denoted by r(p), can be either a string from
the HTML source code of p, or a special null value.
The application of a rule r over a source S returns the ordered set
of values r(p1 ), . . . , r(pn ) denoted by r(S); a wrapper over a page
p returns a tuple t = hr1 (p), . . . , rk (p)i; a wrapper over the set of
pages of a source S returns a relation having as many attributes as
the rules of the wrapper, and as many tuples as the pages in S.
Given a domain D = (S, H), a extraction rule r of the source
S ∈ S is correct if there exists an abstract attribute A ∈ H and
a corresponding physical attribute a ∈ A such that r(S) = a. A
correct rule extracts all and only the values of the same abstract
attribute (i.e., values with the same semantics) for all the pages of
its associated source. Therefore, a correct rule extracts an attribute,
and in the following the two concepts are used interchangeably: we
denote a correct rule also with the physical attribute a it extracts.
An extraction rule is weak if it is correct only for a non empty
proper subset of the pages it is applied to.
A wrapper is sound if it includes only correct rules, and complete
if it includes all the correct rules.

Local consistency. Sources may introduce errors that modify

dA =

PROBLEM DEFINITION

A mapping, denoted by m, is a set of rules associated with different sources (that is, a mapping cannot contain rules from the same
wrapper). Extraction rules are grouped into mappings to express
the semantic equivalence of two or more physical attributes.
A mapping is sound with respect to an abstract attribute A, if
it groups only correct rules that extract attributes related to A. A
mapping is complete with respect to an abstract attribute A if it
contains all the correct rules that extract all the attributes published
in S and related to A.

d(a, b).

We say that a domain D = (S, H) has separable semantics if and
only if: ∀A ∈ H : dA < DA .
Example 1 Figure 3 introduces an example that will be used in the
paper. In Figure 3(a), the physical attributes of two instances, the
fictional stock quotes with ticker symbols X and Y, are represented
as points on a Cartesian plane. The coordinates of each point correspond to the values of an attribute for the two objects. Ideally
the physical attributes associated with the same abstract attribute
should coincide; however, because of the errors introduced by the
sources, they do not.
Figure 3(a) also reports some distances to illustrate the separable semantics assumption. E.g., even in the presence of publishing
errors, the minimum distance between any pair of attributes formed
by one Low attribute and one High attribute (DHigh = DLow =
d(l1 , h2 ) = 0.133) is greater than the max distance within a pair
of High attributes (dHigh = d(h1 , h3 ) = 0.121).

3.2

Abstract Relation Discovery Problem

Given a set of input sources S, our problem can be stated as
that of finding a sound and complete mapping for every abstract
attribute of its underlying abstract relation H:
Problem (Abstract Relation Discovery) Given the sources S of
a domain D = (S, H), find a set of mappings M such that:
M = {mA : mA = {a, a ∈ A}, A ∈ H}.
In Sections 4 and 5 we introduce a solution to this problem based
on our generative model. Note that, by definition, behind the problem of building sound and complete mappings, there is the related

807

Stock Y

h2 2
l1 1

dLow:
1 Low

h1 1

High
Cap

v3

d Vol

Vol

v1 1

v1

c4

DLow=DHigh: l1 1
h1 1

D Cap
=D Low

D High
l1

dLow

l2

dHigh: h1 1

v2

h3

h2
h1

D Vo

l

dVol: v1 1
DVol: v2 2

dHigh

(a)

l1 1

DCap: h2 2

Stock X

0.1
0.11
0.12
0.121
0.13
0.133
0.135
0.14
… … ...
0.16
0.2
… … ...
0.33

Step

Mappings M

0: { {l1}, {l2}, {h1}, {h2}, {h3},{v1}, {v2}, {v3}, {c4} }

3 h
3

1: { {l1}, {l2 },{h1}, {h2, h3 }, {v1}, {v2},{v3}, {c4}

2 l
2

}

2: { {l1, l2}, {h1}, {h2, h3 }, {v1}, {v2}, {v3}, {c4} }

2 h
2
3 h
3
2

v2

2

h2

2

l2

1 h1
3 v
3
3 h
3
4

c4

(b)

3: { {l1, l2 }, {h1, h2, h3 },

{v1}, {v2}, {v3}, {c4} }

4: { {l1, l2 }, {h1, h2, h3 },

{v1}, {v2}, {v3}, {c4} }

5: { {l1, l2 }, {h1, h2, h3 },

{v1, v2}, {v3}, {c4} }

6: { {l1, l2 }, {h1, h2, h3 },

{v1, v2},

{v3}, {c4} }

7: { {l1, l2 }, {h1, h2, h3 },

{v1, v2},

{v3}, {c4} }

8: { {l1, l2 }, {h1, h2, h3 },
…

{v1, v2},

{v3}, {c4} }

i: { {l1, l2 }, {h1, h2, h3 },

{v1, v2 , v3},

{c4} }

i+1: { {l1, l2 }, {h1, h2, h3 },
{v1, v2, v3 },
…
j: { {l1, l2 }, {h1, h2, h3 },
{v1, v2, v3 },

{c4} }
{c4 } }

(c)

Figure 3: Running Example over a domain with abstract relation: H = {Low, High, Vol, Cap} and sources S = {S1 (l1 , h1 ,v1 ), S2 (l2 , h2 , v2 ), S3 (h3 , v3 ), S4 (c4 )}. (a) input physical attributes represented on the Cartesian plane; (b) a subset of the pairs
of attributes ordered by distance as processed by WEIR; (c) tracing of the WEIR algorithm: updated mappings are in boldface;
complete mappings are in italic.
Listing 1 WEIR

problem of inferring sound and complete wrappers. In Section 5.3,
we also present a complementary technique, which relies on a side
effect of our solution, that faces the practical problem of associating meaningful labels with the mappings.

4.

Input: a set of sources S = {S} and related wrappers {wS , S ∈ S};
Output: a set M of complete and sound mappings;

1: let R ← W EAK -R EMOVAL({wS , S ∈ S});
2: let M ={m, m= {r}, r ∈ R}; // starts with singleton mappings
R
3: let P =
; // the set of all unordered pairs of elements of R
2

ABSTRACT RELATION DISCOVERY

In this section, we present WEIR (Web-Extraction and Integration
of Redundant data), our algorithm for solving the Abstract Relation
Discovery problem. For the sake of presentation, we first discuss
our solution in a simplified setting in which the extraction issues
are ignored to make apparent the underlying integration problem;
we assume that sound and complete wrappers are available for all
sources, and we prove the correctness of our solution. Then, in
Section 5 we consider a realistic scenario, where wrappers are complete, but not sound, i.e., they include also incorrect rules. We show
how the redundancy of information can be exploited to select the
correct rules, and we prove that the overall solution is correct with
respect to the subset of redundant attributes in the abstract relation.

4.1

4: for {r, s} ∈ P, r 6= s, ordered by d(r, s) do
5: if (∃r0 ∈ m(r), s0 ∈ m(s) : LC(r0 , s0 ) or
m(r) is complete or m(s) is complete) then

6:
mark m(r) as complete, mark m(s) as complete;
7: else
8:
M ← (M \ {m(r), m(s)}) ∪ {m(r) ∪ m(s)}; // merge
9: end if
10: end for
11: return M;

computes one local stop condition per mapping by exploiting the
generative model properties.
The algorithm processes all possible pairs of attributes; the distance between a pair of attributes from the same source represents
an upper bound for the distance of their mappings: the local consistency entails that they have different semantics (a source cannot publish the same attribute twice, with different values), and the
separable semantics implies that all other attributes at a greater distance cannot be merged with them, otherwise the local consistency
assumption would be violated.
Listing 1 reports the pseudo-code of our algorithm: it takes as
input the set of sources S and the corresponding wrappers, and
maintains a set of mappings M (line 2), initialized as a set of singleton mappings, each composed of one extraction rule. The rules
from the input wrappers are first filtered by the W EAK -R EMOVAL
invocation (line 1), which exploits the properties of the generative model to remove the incorrect rules, as we describe later in
Section 5. In the main loop (lines 4-10), the algorithm iteratively
processes all the unordered pairs {r, s} of distinct rules at nondecreasing distances, and evaluates whether the associated mappings (denoted m(r) and m(s), respectively) refer to the same abstract attribute or not, i.e., whether they should be merged or kept

The Underlying Integration Problem

We consider a simplified setting in which a sound and complete
wrapper is available for each input source. The Abstract Relation
Discovery Problem reduces to that of integrating a set of relations
(one per source) that directly expose the attributes of each website.
To generate mappings among the attributes we resort to an instancebased approach [5] that aggregates physical attributes with similar
values into the same mapping. If sources published only correct
data, a naive algorithm that merges only identical attributes could
easily solve the problem. However, the task of matching attributes
is not trivial since different attributes can assume similar values and
web sources might introduce errors.
Our algorithm initializes each physical attribute as its own singleton mapping; then, it greedily processes pairs of attributes with
non-decreasing distances, deciding whether the corresponding mappings must be grouped together based on a merging condition. It
can be seen as a hierarchical agglomerative clustering [29] that processes all the attributes from the sources. The main difference is
that our solution does not rely on a global stop condition (e.g.,
based on the number of the clusters, or on their distances), but it

808

HTML
DIV[@id='tkr']
TR

X

TD
High

TD
16.13

HTML
TABLE

DIV[@id='tkr']

TR

TD
CEO

TR

TD
Dan

TD

Volume

38M

TR

TR

Y

TD

TABLE

TD
High

TD
15.06

TD

TD

Volume

46M

(a)

extraction rules
r1 : //div[@id=’tkr’]/text()
r2 : //td[contains(text(),’High’)] /../td[2]/text()
r3 : //td[contains(text(),’CEO’)] /../td[2]/text()
r4 : /html[1]/table[1]/tr[2]/td[2]/text()
r5 : //td[contains(text(),’Volume’)] /../td[2]/text()
r6 : /html[1]/table[1]/tr[3]/td[2]/text()

values
{X, Y}
{16.13, 15.06}
{Dan, null}
{Dan, 46M}
{38M, 46M}
{38M, null}

(b)

Figure 4: DOM trees of two pages; some extraction rules working on them and the extracted values.
separated. The properties of the generative model are exploited by
the conditional instruction at lines 5-9: it distinguishes the mappings that have to be kept separated from those that should be
merged. Its condition holds whenever the mappings associated with
r and s contain attributes coming from the same source (LC(r0 , s0 )
holds), or because at least one of them belongs to a mapping that
has been already completed.
In the former case, the local consistency imposes that r and s
belong to different abstract attributes. Since pairs are processed
at non-decreasing distance, in the following iterations any other
addition to m(r) or to m(s) would violate the separable semantics assumption. Therefore, m(r) and m(s) are marked as complete (line 6) to indicate that they cannot accept other attributes
afterwards. Coherently, if the condition at line 5 holds because at
least one of the mappings (m(r) or m(s)) is already completed, the
other mapping has to be considered complete as well.
If the condition at line 5 is false, their mappings are considered
associated with the same abstract attribute and merged (line 8).

versely, if the redundancy is completely absent, and each abstract
attribute is published by exactly one source that publishes just its
values, then WEIR would output one large mapping containing all
the physical attributes.
Nevertheless, the assumption that every possible pair of attributes
is published at least by one source is unrealistic, even for a large set
of redundant sources. In particular, it is unlikely for pairs of rare
attributes, i.e., those published just by a few sources. However, as
the following example shows, even a small number of pairs suffices
to produce transitive effects on a large number of sources.
Example 4 Consider the running example in Figure 3(c) right before the step j in which the pair {h2 , c4 } is processed. There exist
sources that publish both attributes Low and High (S1 and S2 ).
Also note that Cap is a rare attribute, published only by S4 , and
that h2 is, among the other attributes, the closest to it. Although
a source that publishes both Cap and High is not available to directly enforce their separation, since d(l2 , h2 ) < d(h2 , c4 ), we can
conclude that c4 and h2 are different attributes, otherwise also l2 ,
which is closer to h2 than to c4 , should be merged with l2 . But this
is not allowed by the local consistency of their source (S2 ).

Example 3 Figure 3(b) shows a subset of 11 out of the 36 pairs
of attributes, ordered by distance, for the four hypothetical sources
of our running example: S1 (l1 , h1 , v1 ), S2 (l2 , h2 , v2 ), S3 (h3 , v3 ),
and S4 (c4 ). Figure 3(c) reports the trace of a sample execution of
WEIR over these sources. After the initialization phase that creates
the singleton mappings (step 0), the algorithm processes the pair
{h2 , h3 } with the lowest distance among all the pairs of rules. The
mappings containing h2 and h3 are merged yielding the configuration of mappings shown at step 1. Similarly, the mappings shown
at step 2 are obtained by merging m(l1 ) and m(l2 ), as the pair
{l1 , l2 } is processed right after step 1.
Then, the algorithm processes pairs of rules at increasing distances, and merges the associated mappings (steps 3-5) only if not
already marked as complete. At step 6, the elements of the pair
{l1 , h2 } do not belong to the same source, but the associated mappings already contain attributes from the same source (e.g., l1 , h1
from S1 and l2 , h2 from S2 ). Therefore, their mappings are kept
separated and marked as complete. Notice that a correspondent
pair of attributes from the same source – e.g., {l1 , h1 } – is processed only later, before step 8. The pairs processed in the meanwhile (steps 6-7), do not produce any change since the involved
mappings are already complete.
After step i, a pair containing the rule h3 of a complete mapping
is processed: this is an hint that also the other mapping (that containing v2 ) has to be marked complete and that the two attributes
have different semantics.

4.2

The reasoning can be repeated transitively and two attributes can
be kept separated by the local consistency of sources publishing
other attributes by means of an arbitrary number of interposed attributes.
Example 5 Continuing the previous example, if another source
S5 published an attribute about the dividends, d5 ∈ Div, with
d(d5 , c4 ) > d(h2 , c4 ), even if no source publishes both Cap and
Div, we can deduce that d5 6∈ Cap. Otherwise, to merge d5 with
c4 , we would also have to add c4 into the mapping containing every h ∈ High. Transitively, we would end up by merging, again,
the mapping of the High with that of Low and to violate the local
consistency of the sources S1 and S2 that publish both.
The above concepts are formalized in the following definition.
Definition 1 (Separable Domain) Given a domain D = (S, H),
a pair of abstract attributes A, B ∈ H are separable, denoted
Sep(A, B), iff ∀a ∈ A, b ∈ B, A 6= B:
LC(a, b) ∨ [∃ c ∈ C, C ∈ H : Sep(A, C) ∧ d(a, c) < d(a, b)].
D is a separable domain iff all its pairs of abstract attributes are
separable: ∀A, B ∈ H : A 6= B ⇒ Sep(A, B).
We can now present the following theorem, which characterizes
the amount of redundancy needed to solve the Abstract Relation
Discovery Problem for a domain.

Integration Correctness and Complexity

We now discuss the amount of redundancy that WEIR needs to
produce a correct solution.
Consider two extreme cases. If every possible pair of attributes
is published at least by one source, WEIR is trivially correct. Con-

Theorem 1 (WEIR Integration Correctness) In case of correct
wrappers, WEIR is a solution for the Abstract Relation Discovery
Problem if the domain is separable.

809

dT () is type-aware pairwise comparison between two non-null values belonging to the type T .
In the case of String and URL, dT (·, ·) is a standard distance,
namely the Jensen-Shannon distance.2 For the Date, ISBN, Phone
types, the distance function simply returns 0 if the two elements are
equal, 1 if otherwise. For numeric types, the computation is more
involved, as fT (·, ·) measures the ratio of objects that differ more
than a predetermined relative threshold ρ. We compute the threshold ρ with respect to the average size of the compared numbers,
so the P
greater the values, the larger the tolerated differences. Let

The proof, which is rather straightforward, can be found in [8].
For the time-complexity analysis of our algorithm we measure
the size of the input with the total number n of extraction rules,
which can be assumed at most linear with the number of input
sources |S|. We also assume constant the cost of computing a
distance between any two rules. Computing the distances among
all the possible pairs of rules is O(n2 ). With a disjoint-set datastructure [15, Chapter 21], the operation of finding a mapping,
given a rule, is O(1), and that task of merging two mappings is
O(n). Therefore:

|v id |

r
vr = id∈I
be the average of the absolute values extracted
|I|
by the rule r, and let v = min(vr , vs ). We define ρ = v · θ (we set
θ = 0.02 in our experiments). Finally, we define:

1 , iff |vr − vs | > ρ;
dT (vr , vs ) =
0 , otherwise.

Proposition 1 (WEIR worst case time complexity) The worst case
time complexity of WEIR is O(n3 ), where n is the total number of
extraction rules.

4.3

A Type-Aware Distance Function

We conclude this section by discussing the distance function on
which the whole integration process is based. On the Web, redundant sources publish data of many different types and with different
unit of measures. Our distance function has been defined considering these factors that could prevent the redundancy from being
recognized and exploited.

This covers all the rules r extracting numeric values, i.e., Number,
Length, Weight, and Currency.

5.

THE EXTRACTION PROBLEM

The formalisms used by state-of-the-art unsupervised wrapper
generator systems, such as ROAD RUNNER [16] and E X A LG [3],
are expressive enough to define a complete wrapper for a vast majority of web sources. However, the wrappers produced by these
systems usually are neither complete nor sound. In fact, the induction engines of these systems have to evaluate several candidate solutions that rank the extraction rules according to their effectiveness
in describing the regularities in the template of the input pages. For
example, E X A LG analyzes the co-occurrence of tokens in a large
number of pages sharing a common template, and ROAD RUNNER
tries to incrementally align a set of sample pages to separate their
underlying template from the embedded data. Unfortunately, the
sole knowledge associated with the template is not always sufficient
to converge towards the best rules, and the wrappers generated by
these systems have limited accuracy and coverage.
In our approach, the wrappers are created exploiting both the
regularities that locally arise in each source, and the redundancy of
data that globally occurs among the sources. With the objective of
obtaining complete wrappers, we generate several alternative extraction rules, possibly including also weak rules, by analyzing the
HTML regularities of the sources. Then, to achieve the wrapper
soundness, we remove weak rules by leveraging the redundancy of
information. With respect to traditional unsupervised approaches,
the selection of the correct rules is not performed during the inference phase based on local criteria, but it is based on the matching
of each rule with the data extracted from at least another source.
On the other hand, our technique works only for redundant data.

Normalization and Types. The extracted values are associated with a type taken from a simple hierarchy of common web
data types, such as String, Date, Length, as shown in Figure 5(a).
The most specific data type is preferred, with String used whenever
no other type applies. Type inference is performed by matching the
extracted data with a set of predefined patterns. Figure 5(b) shows
a sample of the patterns used by our system.
For some of these types it is also possible to detect the units of
measure (e.g., for Length: kilometers, centimeters, miles, feet, etc.)
and to convert the extracted values to a reference unit measure. For
example, Length values are converted in centimeters and Weight
values are converted in kilograms.

Distance Functions. To compute the distance between a pair
of rules we adopt an instance-based distance function. In our context it is rather easy to associate every page with a natural identifier
for the object described in the page. In many cases these identifiers
are directly used to harvest the pages (for example, when pages are
obtained by querying a form) or derived during the page collection
phase. Otherwise it is possible to obtain them starting from a small
seed set, as we shall describe in Section 5.4. Therefore, to exploit
this opportunity we rely on instance-based distance functions that
compare pairwise values as follows.1
Given two rules r and s, their distance d(r, s) is computed by
averaging the pairwise distance between the values extracted from
pages publishing data of the same instance. Let I be a set of natural
identifiers of the objects published in the pages; and let (vrid , vsid )
denote the pair of values extracted by two rules r, s from the pages
associated with the instance of identifier id ∈ I:
P
id
id
id∈I fTr ∩Ts (vr , vs )
d(r, s) =
|I|

5.1

Extraction Rules Generation

where Tr ∩ Ts is the most specific type containing both values
extracted by r and those extracted by s, and fT (·, ·) is defined as:

 0 , iff vr = vs ;
1 , iff vr 6= vs and (vr = null or vs = null);
fT (vr , vs ) =

dT (vr , vs ), otherwise;

In the generation of the initial set of extraction rules we leverage
the local regularities that occur among pages of the same source.
For each source, (i) we analyze the DOM tree representations of
the pages to locate nodes that are part of the template; then, (ii) for
every textual leaf node that does not belong to the template, we
generate a set of XPath expressions; finally, (iii) we filter those
that are unlikely to be effective extraction rules.
To discover the template nodes, for each source we compute the
occurrences of the (textual) leaf nodes in the pages. Following the
intuition developed in [3], we classify as template nodes the text

1
An alternative, not requiring the presence of soft-id, is to adopt
distance functions that compare opaque columns, e.g., [25, 26].

secondstring.UnsmoothedJS,

2

810

We use the variant implemented by the Java class
described in [14].

com.wcohen.-

String
Number

Date ISBN URL Phone

Currency Length Weight

Date
Length
Currency
Number
ISBN

\d+ (-|/) \d+ (-|/) \d+
(m|cm|km|ft|’|yd|in|’’) \d+ | \d+ (m|cm|km|ft|’|yd|’’)
($|e|EUR|USD) \d+ | \d+ ($|e|EUR|USD)
\d+((,|.)\d+)?
(\d{3}(-)?)?\d{10}

(a)

(b)

Figure 5: A type hierarchy, and a sample of the syntactic patterns (expressed as Java Regex expressions) used to infer the types.
Listing 2 W EAK -R EMOVAL

leaf nodes that occur exactly once with same value and same rootto-leaf path (without indices) in a significant percentage (40%) of
pages. The rationale is that it is unlikely that these nodes have the
same path and the same number of occurrences by chance; rather,
it is likely that they comes from a piece of the underlying HTML
template used to create the pages.
The remaining textual nodes are considered as candidate data to
be extracted; for them, we generate extraction rules. We distinguish
two types of XPath expressions: absolute extraction rules, and relative extraction rules. Absolute extraction rules specify a linear
path (i.e., a sequence of nodes having a parent-child relationship)
including the indices, that start either from the document root or
from a node having an ‘id’ attribute. Relative extraction rules specify a path starting from a template node, which we call the pivot
of the rule. A relative extraction rule for the textual node l pivoted
into the template node t is computed by appending three expressions: (i) an expression that matches the pivot node t, (ii) the path
from t to the first ancestor node, nlt , shared by t and l, (iii) the path
from nlt to l (which descends from the shared ancestor node to the
target textual node). To avoid an excessive proliferation of rules,
we bound the length of the relative extraction rules. The length of
a relative extraction rule r, denoted δ(r), is the number of XPath
steps following the first ../ after the pivot.3

Input: a set of wrappers {wS , S ∈ S};
Output: all and only the correct rules from the input set of wrappers;

1: let R = 
{r, r ∈wS , S ∈ S}; // set of all the rules

R
; // the set of all unordered pairs of elements of R
2
for {r, s} ∈ P : r, s ∈ wS , r(S) ∩ s(S) = ∅, ordered by d(r, s) do
if (@ r∗ ∈ R, r∗ ∈ wS : (r∗ is correct)∧
(r(S)∩r∗ (S) 6= ∅)∧(s(S)∩r∗ (S) 6= ∅)) then
mark r as correct, mark s as correct;
end if
end for
return the subset of rules marked as correct in R;

2: let P =
3:
4:
5:
6:
7:
8:

observe that it is unlikely that the values extracted by a weak rule,
which works correctly only for a subset of the pages, have the best
match with the values extracted by a correct extraction rule from
a different source. Conversely, correct rules related to the same
abstract attribute extract matching values from different sources.

5.2

Weak Rules Removal

The presence of weak rules can be detected by observing that
there is always a non empty intersection between the nodes identified by a weak rule and those identified by a correct rule of the
same source.

Example 6 Consider a set of pages such as those shown in Figure 4(a). Assuming that nodes ‘High’, ‘Volume’ and ‘CEO’ appear
once with the same root-to-leaf path in many other pages of the
source, they would be considered as template nodes. Figure 4(b)
reports an example of the rules generated by the second step: r1 ,
r4 , and r6 are absolute rules (r4 and r6 start from the document
root, r1 is rooted at the node with an ‘id’ attribute), whereas the
r2 , r3 , and r5 are relative rules that specify a path starting from
a pivot node. These relative rules have length δ(r2 ) = δ(r3 ) =
δ(r5 ) = 2.

Example 7 Consider again the rules in Figure 4(b) and the pages
in Figure 4(a): extraction rule r4 is weak, since it mixes the CEO
from the page on the left with the Volume from the page on the right.
The set of nodes it extracts has a non empty intersection with those
extracted by the (correct) rules r3 and r5 .
These intuitions are applied in the W EAK -R EMOVAL procedure
invocation at line 1 of WEIR; its pseudo-code is shown as Listing 2.
It takes as input a set of wrappers, one per each source, and returns
a subset of all their rules, freed from the weak rules (line 8).
W EAK -R EMOVAL processes (line 3) all the pairs of non overlapping rules (r(S) ∩ s(S) = ∅) from the same site (r, s ∈ wS )
at non-decreasing distance. The procedure assumes that a pair of
correct rules that refer to the same abstract attributes are closer, and
then processed earlier, than a pair of rules that includes (at least)
a weak rule. Therefore, W EAK -R EMOVAL marks as correct a pair
of rules when they are processed for the first time (line 5). When
a pair of rules from the same source is processed, if one (possibly
both) of them has a non empty intersection with the values of some
rule already marked as correct (denoted r∗ in the listing, at line 4),
it is considered weak, and the pair is skipped.
The W EAK -R EMOVAL procedure eliminates weak rules based
on the assumption that correct rules from different sources are closer
to each other than weak rules incidentally extracting similar values.
This assumption can be formalized by considering the distance between weak and correct rules. With an abuse of notation, we write
that r ∈ A to state that an extraction rule r extracts at least one

The above step produces several extraction rules, some of which
are useless. We use straightforward heuristics to filter out rules
that are unlikely to extract valuable data. We discard rules whose
extracted values include template nodes, or a large majority (more
than 80%) of null values. Finally, among a group of rules extracting
identical data, we prefer the shortest relative one, i.e., the relative
rule whose pivot is closer to the extracted values.4
The generated wrappers contain several rules, including rules extracting useless data, as well as weak rules, such as r4 and r6 in
Figure 4(b). As in our setting domain data appear in more than
one source, to select the correct rules we exploit the redundancy of
data across several sources: useless rules can be discarded because
they extract data not matching with any other source. Pruning weak
rules is trickier, as they partially extract correct data. However we
3
We observed that producing rules with length greater than 6 does
not produce any benefit.
4
On average, for each source the system selects 97 rules out of
1380 generated.

811

mapping m. The second factor, δ l (m), is the average length of the
P
rules r ∈ Rl (m), i.e., δ l (m) = r∈Rl (m) |Rδ(r)
l (m)| .
Essentially, a label gets a good score if it is both redundant among
the sources and close to the extracted values. Observe that the ranking results of the scoring function becomes more and more effective
as the redundancy of the attributes increases.

correct value of the abstract attribute A. The error introduced by
weak rules can then be defined as follows.
Definition 2 (Minimum Extraction Error) Given an abstract attribute A ∈ H from a domain D = (S, H), and a set of wrappers
{wS , S ∈ S} over its sources, we call minimum extraction error
for A, denoted eA , the minimal distance between a correct rule r∗
extracting A values and all other non overlapping rules from the
same site:
eA =

argmin

5.4

d(r∗ , r).

S∈S,r,r ∗ ∈wS ,r ∗ ∈A:r(S)∩r ∗ (S)=∅

A redundant abstract attribute A satisfies the minimum extraction
error assumption iff dA < eA . The correctness of W EAK -R EMO VAL is then characterized by the following Lemma:
Lemma 1 (W EAK -R EMOVAL Correctness) W EAK -R EMOVAL
erases all and only the incorrect rules of the redundant attributes
satisfying the minimum extraction error assumption.
P ROOF. It follows immediately by the minimum extraction error assumption and by the ordered processing of all the pairs of
rules: if a redundant abstract attribute A is such that dA < eA , then
the elements of any pair of its correct rules are closer than a pair of
non overlapping rules including one weak rule of A.
It is worth noting that the two compared quantities, eA and dA ,
are related to different aspects: dA is a measure of the maximum
publication error introduced by the sources; eA is a measure of the
minimum extraction error introduced by an incorrect rule.
In the presence of complete wrappers with the minimum extraction error assumption holding, WEIR receives from the invocation
of W EAK -R EMOVAL at line 1 all and only the correct rules. Therefore, as it immediately follows from Lemma 1 and Theorem 1:
Theorem 2 (WEIR correctness) In case of complete wrappers,
WEIR is a solution for the Abstract Relation Discovery Problem restricted its redundant abstract attributes if the domain is separable
and the minimum extraction error assumption holds.

6.

Note that W EAK -R EMOVAL, and namely the computation of the
intersection of the values extracted by two rules at line 4, is computed within the O(n3 ) complexity of WEIR, i.e., WEIR’s worstcase time complexity does not increase in presence of weak rules.

5.3

Labeling

6.1

WEIR Evaluation

Dataset. We collected 40 data sources from the Web over four
domain entities: soccer players, stock quotes, video games, and
books.5 Detail pages for the video games and soccer players domains were gathered by means of a crawler based on a set expansion technique [6]. For stock quotes and books we queried the
forms of 10 finance sites with ticker symbols, and the forms of
10 bookstore sites with ISBN codes.6 For all the sources, pages
are associated with a natural identifier: for the crawled pages, the
identifiers correspond to the terms used by the crawler in the set
expansion phase; for the pages returned from the forms, the identifiers are the terms used to query the forms. Each detail page of
our dataset contains data about one instance of the corresponding

any mapping m and label l such that Rl (m) is not empty. The first
factor, [1 −

EXPERIMENTAL RESULTS

We conducted experiments over real world websites to evaluate
the performance of our approach. We first present WEIR extraction
and integration quality results, followed by an analysis of its behavior w.r.t. the amount of redundancy among sources. Then, we compare WEIR against alternative unsupervised solutions, and against a
system requiring human annotations as part of the input [24].

We now present a complementary technique to associate each
mapping with a semantic label. The candidate labels for a mapping
are obtained as a side-effect of the rule generation procedure: they
are the texts playing the role of pivots in the relative rules of the
mapping. This approach is not reliable if applied on a single source,
but we leverage the redundancy among the textual nodes that occur
in the HTML templates of different sources.
We have crafted a simple yet effective heuristic method to rank
these texts as candidate semantic labels of the mappings computed
by WEIR: a template text is considered a good candidate label for
a mapping if (i) it is frequently present in the templates of the involved sources, and (ii) it occurs close to the extracted values.
Given a mapping m, let Rl (m) ⊆ m be the subset of its relative rules based on the same textual pivot l; we define a score (the
lower the better) for any candidate label l of a mapping m, as foll
(m)|
] · δ l (m). It is computed for
lows: score(m, l) = [1 − |R|m|
|Rl (m)|
],
|m|

Associating Pages with Soft-ids

We conclude by describing a simple technique to derive the softids for the objects described in the detail pages of the sources. We
remark that detail pages gathered by means of an ad-hoc crawler or
by filling forms are inherently associated with soft-ids. However,
we applied a simple set-expansion technique to derive soft-id from
the extracted values in the case they are not already available, as it
happens for a dataset used in our experimental activity. The only
requirement is that a small seed set of soft-ids is already available.
Let I be a seed set of strings that identify a few objects in the
sources, and let Q be a priority queue storing all the sources in S.
For example, dealing with sources containing detail pages about
movies, I could contain a few movie titles (e.g., “The Godfather”,
“Full Metal Jacket”, “Hannah and Her Sisters”).
The priority associated to a source S in Q corresponds to the cardinality of the largest intersection between a rule r in the wrapper
wS inferred by our wrapper generator for that source, and the seed
set, i.e., maxr∈wS |r(S) ∩ I|. Then, the source S with the highest
priority in Q is dequeued; the values extracted by r are elected as
soft-id for the pages in S, and the seed set is expanded by adding all
the new soft-ids. The process iterates until the queue Q is empty,
or neither the seed set nor the queue size change anymore.
It is worth observing that the above method can generate imprecise ids because weak rules could have been erroneously selected, as well. Anyway, it is unlikely that the wrong ids are propagated (since they do not match with the values extracted by other
sources). Also, our main goal is to obtain a set of top-k soft-id.
As we shall discuss in the next Section, even with a small number
of overlapping instances and with a non negligible error rate in the
pairwise page alignment, WEIR produces accurate results.

5
6

considers the frequency of a label l in the

812

The dataset is available at: http://www.dia.uniroma3.it/db/weir.
Stock quotes pages were collected at the market close.

Domain
soccer players
stock quotes
video games
books

domain entity. Table 1 reports the number of pages (#p) and the
number of distinct instances (#o) for each domain.
At the extensional level, several instances are shared by multiple sources. The overlap is almost total for stock quotes and books
(there are 3 and 2 sources that cover all the instances, respectively),
while it is more articulated for soccer players and video games, that
include both large popular sites as well as small ones: there is no
source that covers more than 33% and 47% of all the instances, respectively. To characterize the extensional redundancy, for each domain we consider the overlap graph whose nodes are the sources,
with an edge between two nodes if the corresponding sources share
at least q instances. For all the domains, the corresponding graphs
have one connected component with q = 5. However, while for
the stock quotes and the book domains every source is connected
with all the others (any pair of sources has at least q instances in
common), the same property does not hold for the soccer player
and the video game domains. To describe how the sources overlap,
we report the diameter of their overlap graphs. The diameter d of a
graph is defined as the length of the longest path among the shortest
paths between all pairs of nodes in the graph. Table 1 reports the
diameter (d) of the overlap graphs, with q = 5. For stock quotes
and books, d equals 1 (every source is indeed connected with all
the others); for video games and soccer players d equals 2 and 3,
respectively (many source pairs do not have shared instances: they
are connected indirectly through one or two intermediate sources).
At the schema level, no source publishes all the attributes of a
domain, some attributes are published by all the sources, and others occur less frequently. The attributes of the four domains have
interesting and distinctive features. For stock quotes, most of the
attributes are numeric, and several of them have very similar values
(high and low, open and close prices). The soccer players domain
includes attributes of different types with heterogeneous formats in
the various sources. For example, height and weight of players are
expressed in several different units of measure (e.g., meters vs. feet
and inches) and are published according to different formats (e.g.,
m 1.82 vs. 182cm). We counted five different representations for
values of type Date, four for Height, three for Weight. In the video
games and books domains most of the attributes are strings and the
page templates are more irregular than those of the other domains.

#p
5,850
4,656
12,339
1,318

#o
4,178
573
5,364
196

d
3
1
2
1

P
0.90
0.90
0.93
0.94

R
0.93
0.81
0.90
0.78

F
0.91
0.85
0.91
0.84

Time
80 s
67 s
204 s
15 s

Table 1: Dataset features and WEIR performance.

used). The precision is around 0.9 for every domain. Also the recall
is high, ranging from 0.78 (books) to 0.93 (soccer players).
In the stock quotes domain, which is challenging since it includes
several numeric attributes with very similar values, the precision
and recall results testify that our algorithm can integrate data from
different sources by self-tuning its tolerance to the actual errors in
the sources, even if the amount of error changes among attributes
of the same domain.
Most of WEIR errors are caused by mappings that have been considered complete too early. The most frequently violated hypothesis by real sources is the minimum extraction error. Some pairs of
weak rules resulted closer than pairs of the corresponding correct
rules. The involved weak and correct rules often differ only for a
marginal percentage of the extracted values. This problem occurs
mainly for attributes of type String: by inspecting their values, we
realized that a publication error involving strings can result higher
than the corresponding extraction error, since the string distance
function is less effective in measuring small differences.

Labels Detection Results.

WEIR is able to find the right label for many mappings: 77% of soccer players mappings, 45%
for stock quotes, 100% for video games, and 57% for books. Figure 6 reports the top-2 candidate labels produced for the attributes
in the golden datasets. There are three main reasons for the errors
in the labels returned by our system. First, for some attributes a
textual label in the template of the pages does not exist: e.g., the
title of a book. Second, some labels are mixed in the same template
node, e.g., ‘high/low’ for a stock quote, or they are split across
multiple leaf DOM nodes, e.g., <span>52 week</span><span
color="red">low</span>. Third, as discussed earlier, more redundancy is needed in order to mitigate the noise, for instance due
to tabular templates (in which many attributes are visually close
to each other). The needed redundancy is not available for “rare”
attributes, such as Edition for books and Number for soccer players.

Metrics. We compare WEIR output against a golden solution, obtained by manually composing extraction rules and mappings. Figure 6 details the attributes of our golden dataset and the corresponding occurrences among the sources (we do not report the results for
the attributes used as soft identifiers). We use the standard metrics
of precision (P ), recall (R), and F-measure (F ), which are computed for each abstract attribute, at the level of the values extracted
by the rules of the mapping. Each attribute Ai is associated with
the computed mapping mj with the best coverage of its values.
The precision and the recall of the output mapping mj are defined with respect to their associated abstract attribute Ai , as foln
n
lows: P (mj ) = nijj , and R(mj ) = niji , where nj is the number
of values extracted by the rules of mj , ni is the number of values extracted in the golden mapping Ai , and nij is the number
of values extracted by the rules in mj found in the golden mapping Ai . As usual, the F-measure of a mapping mj is defined as:
P (m )·R(m )
F (mj ) = 2 · P (mjj)+R(mjj ) .

Sensitivity to Extensional Redundancy. We now consider
the extensional redundancy of information among the sources, an
important factor that can affect our approach. We have seen that
WEIR computes the distances between two physical attributes by
comparing a number of aligned instances. Two main aspects may
influence the performances: (i) the number of instances that are
involved to compute the distance between a pair of attributes, and
(ii) the precision of the alignment between them.
Figure 7(a) plots the F-measure over the four domains versus
the number of overlapping instances used to compute the distance
function between two sources. Precision and recall are not shown
for the sake of readability; however, precision is not much affected,
and therefore the F-measure mainly reflects the recall results. With
a small number of overlapping instances, the distances function
is less reliable, leading to a recall loss. When the number of required overlapping instances is very large, some sources do not
share enough instances to compare their attributes, preventing their
merging. Note how the latter observation does not affect the stock
quotes domain, where all sources share a large set of instances.

Extraction and Integration Results. Table 1 reports average
precision (P), recall (R) and F-measure (F) for the four domains.
Experiments run with the distance functions requiring at least 5
overlapping instances (when more overlaps were present they were

813

Position (10/10) Birthplace (6/7)
1/1/1
1 / 0.86 / 0.92
position 2.0 place of birth 3.0
dob 3.0
weight 3.0
Last value (10/10)
1/1/1
high 2.0
3.0

Height (6/6)
1/1/1
height 2.33
born 4.0

S OCCER P LAYERS
Nat. Team (6/6)
Club (4/5)
0.98 / 0.98 / 0.98 0.86 / 0.69 / 0.77
born 1.5 current club 1.0
nationality 1.5 ...nederlands 3.0

S TOCK Q UOTES
Day high (8/10) Day low (7/9) 52 wk high (9/9) 52 wk low (7/9)
1 / 0.8 / 0.89
0.61 / 0.48 / 0.54
0.8 / 0.8 / 0.85
0.99 / 0.77 / 0.87
ask size 3.0
eps 3.0 52 week high 2.0
low 1.0
date 3.0
earnings 3.0
260 days 2.0
year low 2.0

Author (8/10)
0.97 / 0.78 / 0.86
by author 3.0
author 3.0

Title (9/10) Publisher (7/6)
0.94 / 0.85 / 0.89 0.99 / 0.85 / 0.91
link 3.0
english 2.0
by author 3.0
publisher 2.5

Publisher (9/10)
0.93 / 0.93 / 0.93
publisher 2.25
release date 3.5

V IDEOGAMES
Developer (8/8)
ESRB (7/8)
0.86 / 0.86 / 0.86
1 / 0.87 / 0.93
developer 2.6 esrb rating 2.0
see tech... 4.0
see tech... 4.0

Weight (4/4)
Birthdate (7/8)
1/1/1
1 / 0.88 / 0.93
nationality 1.0
date of birth 3.0
weight 2.0 place of birth 5.0

Nationality (3/2)
0.67 / 1 / 0.8
nationality 2.5
... name 4.0

Number (2/2)
0.67 / 1 / 0.8
... club 5.0

Change % (5/6)
Open (8/8)
Volume (7/8) Change $ (5/5)
0.83 / 0.69 / 0.76
0.89 / 1 / 0.94
1 / 0.88 / 0.93
0.58 / 0.81 / 0.68
date 3.0 previous close 2.0
volume 4.0
today 1.0
market cap 4.0
open 2.0 share volume 4.0 last trade 3.0

B OOKS
ISBN13 (5/7)
Binding (4/6) Publication date (6/5)
Edition (2/3)
1 / 0.71 / 0.83
0.91 / 0.61 / 0.73
0.83 / 1 / 0.91
0.96 / 0.64 / 0.77
isbn-13 1.0
binding 1.0
published 2.0 amazon price 3.0
isbn 1.0
...details 5.0 publication date 3.0
Genre (8/8)
0.94 / 0.94 / 0.94
genre 2.3
n. of player 3.0

Legenda:

D OMAIN
Attribute (#w/#g)
P/R/F
label1 score1
label2 score2

Figure 6: The abstract attributes of the golden dataset, and the corresponding WEIR results: occurrences of the physical attributes
in the mapping computed by WEIR, and in the golden dataset (#w/#g); Precision, Recall, F-measure (P / R / F); the top-2 labels and
their associated scores (lower scores are better, correct labels are in boldface).
tion phases are performed with the traditional approaches. We call
this solution the “waterfall” approach (this is indicated as the WF
configuration): the extraction is completed before the integration
starts, and the two phases are completely separated. To evaluate
the specific impact of our techniques, we also use the standard approach for one of the two phases, and our approach for the other
one. More specifically, we set the following configurations: (i) we
rely on ROAD RUNNER to infer the wrappers, and on our algorithm
to compute the mappings over the relations produced by the wrappers (this is the RR configuration); (ii) we infer rules with our approach (running also the W EAK -R EMOVAL procedure), and compute the mappings with the HAC algorithm (HAC configuration).
Figure 8 summarizes the obtained results: WEIR always outperforms the alternative approaches, in every domain. The better precision obtained by the waterfall approach with the video games has
to be considered together with the low recall. WEIR is more effective than ROAD RUNNER in choosing the correct rules, thus being
able to achieve better precision and recall. Observe that in the stock
quotes domain our integration algorithm (which is used also in the
RR configuration) has a strong impact on the precision: there are
many attributes with similar values, and an algorithm with a fixed
threshold (even if manually tuned) is not flexible enough to distinguish all of them correctly.

Figure 7: WEIR F-measure sensitivity to: (a) number of overlapping instances; (b) page alignment error-rate.

As described in Section 4.3, our instance-based distance function relies on the presence of soft identifiers to align the pages. To
test the robustness of the approach with respect to alignment errors, we run the system after introducing a certain amount of errors
in the alignment of the instances. Figure 7(b) shows that, the performances decrease with an increasing error rate. However, it is
worth observing that even with a significant error rate (40%) the Fmeasure is still higher than 0.7 for all domains. With higher error
rates, the loss of alignment among pages is partially compensated
by aggregate characteristics of data: value ranges and types. Note
that the stock quotes domain exhibits the worst degradation, since
most of attributes are numeric with very similar values. Conversely,
for soccer players, the type richness of the attributes and their different ranges of values attenuate the loss.

6.2

6.3

Comparison with a Human Bootstrapped
Approach

Finally, we compared WEIR against a state-of-the-art approach
that takes as input human annotations to bootstrap the process [24].
The approach aims at extracting the data for a set of attributes from
all the sources of a vertical domain. The user specifies the set of
attributes to be extracted by manually annotating one input source.
Since the code is not public, we used their dataset (SWDE, available at http://swde.codeplex.com) in order to conduct the comparison: it includes about 124K detail pages from 80 websites related
to 8 different domains (10 sites per domain, 200–2,000 pages per
website). For each domain there are 3–5 target attributes (32 in total), for which SWDE provides a golden set of values extracted (by
means of handcrafted rules) from all the pages. We used the dataset
“as-is” and, to be comparable with the results reported in [24], we
used their performance metrics that are based on precision and recall over the expected values in the golden dataset.

Comparison with Other Approaches

To compare WEIR against other fully automatic approaches, we
conducted experiments by using a traditional unsupervised wrapper inference system for the extraction phase, and a hierarchical
agglomerative clustering algorithm for the integration phase.
As wrapper generator system, we used the most recent implementation of ROAD RUNNER [16]. For the integration phase we
implemented a standard hierarchical agglomerative clustering algorithm (HAC, in the following), with a distance-r stopping criterion, i.e., it merges only pairs with distance at most r. We manually
tuned the threshold r to the value 0.7 to achieve the best results.
Using ROAD RUNNER and HAC we assembled three alternative
systems. First, a system where both the extraction and the integra-

814

P

0.33

0.5

0.83

0.86

0.81

0.97

0.94

0.95

0.99

0.94

0.98

0.97

0.97

0.99

0.82

0.81

0.89

0.83

0.98

0.83

0.79

0.78

0.93

0.80

0.89

0.97

0.85

0.87

0.84

0.49

0.93

0.96

0.90

0.96

0.86

0.95

0.79

0.87

0.86

0.91

0.87

0.94

0.88

0.70

0.75

0.69

0.90
0.72

0.94

1

0.87

Figure 8: Comparison of different extraction and integration approaches over several domains.

R
F

0.25
WEIR

SWDE
AUTOS
(Model)

WEIR

SWDE
BOOKS
(ISBN)

WEIR

SWDE

CAMERAS
(Manufacturer Part #)

WEIR

SWDE

WEIR

JOBS
(Title)

SWDE
MOVIES
(Title)

WEIR

SWDE

NBA PLAYERS
(Name)

WEIR

SWDE

RESTAURANTS
(Phone)

WEIR

SWDE

UNIVERSITIES
(Name)

Figure 9: Comparison of WEIR vs [24] over the SWDE dataset: AUTOS (Model, Price, Engine, Fuel), B OOKS (Title, Author, ISBN, Publisher,
Date), C AMERAS (Model, Price, Manufacturer), J OBS (Title, Company, Location, Date), M OVIES (Title, Director, Genre, MPAA), NBA P LAYERS
(Name, Team, Height, Weight), R ESTAURANTS (Name, Address, Phone, Cuisine), U NIVERSITIES (Name, Phone, Website, Type).
We have derived the soft-ids by applying the set-expansion technique discussed in Section 5.4 on the attribute reported below the
domain name in Figure 9. Note that all these domains have a natural identifier except jobs. We then computed the overlap of the
sources: for q = 1, all the domains with a soft-id have an overlap graph composed of one connected component. By manually
inspecting the pages, we realized that job sources do not overlap.
We used the job Title as a soft-id just to complete the experiments.
Results in Figure 9 show that WEIR has better performance in all
domains except the jobs and the cameras domains. In the former
case we correctly extract only the job Title. As related to the cameras domain, the loss of recall is due to one attribute (out of three),
i.e., Model. This attribute actually represents a description of the
item rather than the model name (which is actually present in the
pages): its values are heterogeneous and long textual descriptions
(even more than 20 terms and 100 characters), for which our string
distance function is not effective.
As a conclusive note, WEIR found a meaningful label for 57%
of the mappings corresponding to the above attributes. In addition, it was able to extract and integrate more attributes than those
specified in the SWDE dataset, such as (we report one example per
domain): Transmissions for Autos, ListPrice for Books, MegaPixel
for Cameras, RunningTime for Movies, BirthDate for NBA Players,
Website for Restaurants, and Address for Universities.

7.

Snowball [1]) take a seed set of facts as input, then interleave pattern inference and relation extraction steps to expand the seed set.
Recently, so called “open” information extraction approaches [4,
21] have been developed: they automatically discover phrases to
extract new facts from sentences, and they are not constrained to
learn an extractor only for the target relations specified by means
of training samples. We share with them the ability of discovering new attributes. However, as argued in [4], these systems are
designed for textual corpora, and they cannot extract data from in
HTML templates, which conversely represent our target.
A large body of works has tackled the challenge of extracting
and integrating structured data from the Web [9, 23, 30]. One distinguishable feature of our work is the ability to gather and leverage domain knowledge at runtime to automatically tune the integration process. The massive exploitation of the structured Web
has been studied for data published in HTML tables and lists [10,
20]. However, these works focus on the extraction of rich relational schemas, without addressing the issue of integrating the extracted data. O CTOPUS [9] and C IMPLE [30] support users in the
creation of datasets from web data by means of a set of operators
to perform search, extraction, data cleaning and integration. Such
systems have a more general application scope than ours, but they
heavily involve users in the process. Similarly, [22, 23, 24, 31] require labeled examples to bootstrap the extraction process and, with
the notable exception of [31], they can extract only data from the attributes annotated in the input pages (i.e., no new attributes are discovered in the integration process). In [31] the extraction process
is propagated to several sources by adapting a previously learned
wrapper to new unseen sites. During this process, new training examples are produced to learn wrappers tailored to the new sites.
This system discovers new attributes by assuming that they share
some formatting feature with an attribute of the same site learned
by means of the training examples. As clarified by their experimental evaluation, this technique fails on pages containing spurious text
fragments similar to those related to domain attributes.
D EIMOS [2] is an end-to-end system that, given a seed website
and some background knowledge of the domain, discovers related
websites and builds semantic web services from them. The system relies on different techniques due to the goal of dealing with

RELATED WORK

Web data extraction involve several tasks: source discovery, wrapper generation, data integration, and data cleaning. In this work we
focus on extraction and integration, but we developed an end-toend system that covers crawling [6] and data cleaning [7] as well.
Web data extraction has been addressed in multiple works in the
last decade (see [12] for a survey). An approach related to ours
is TurboWrapper [13], which introduces a composite architecture
including several wrapper generator systems aiming at improving
the results of the single participating systems taken separately.
Information extraction systems aim at extracting collections of
facts (binary relations, such as born-inhscientist, cityi) from massive corpora of plain text collections. Early approaches (such as

815

services instead of sources as in our case. The different setting
leads to different solutions: instead of learning simple matchings
(element correspondences), it automatically learns Datalog mappings between websites [11]. Moreover, it assumes that the current
knowledge suffices for describing new sources, while we can always easily extend the abstract relation by adding new attributes.
Our integration technique may be seen as a specialized agglomerative, hierarchical clustering algorithm [29]. The domain-separability allows us to define, locally to each abstract attribute, a stop
condition that guarantees the correctness for our setting.
The problem of finding labels for web data has been studied
in [17], which exploits the redundancy of information on Web. But
their approach mostly applies for categorical values.

8.

[10] M. J. Cafarella, A. Y. Halevy, D. Z. Wang, E. Wu, and
Y. Zhang. Webtables: exploring the power of tables on the
web. PVLDB, 1(1), 2008.
[11] M. J. Carman and C. A. Knoblock. Learning semantic
definitions of online information sources. J. Artif. Int. Res.,
30(1):1–50, 2007.
[12] C.-H. Chang, M. Kayed, M. R. Girgis, and K. F. Shaalan. A
survey of web information extraction systems. IEEE Trans.
Knowl. Data Eng., 18(10):1411–1428, 2006.
[13] S.-L. Chuang, K. C. Chang, and C. X. Zhai. Context aware
wrapping: Synchronized data extraction. In VLDB, 2007.
[14] W. W. Cohen, P. Ravikumar, and S. E. Fienberg. A
comparison of string distance metrics for name-matching
tasks. In IIWeb, 2003.
[15] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.
Introduction to Algorithms (3. ed.). MIT Press, 2009.
[16] V. Crescenzi and P. Merialdo. Wrapper inference for
ambiguous web pages. Applied Artificial Intelligence,
22(1&2):21–52, 2008.
[17] A. da Silva, D. Barbosa, J. M. Cavalcanti and M A. Sevalho.
Labeling Data Extracted from the Web. In OTM, 2007.
[18] N. N. Dalvi, R. Kumar, and M. A. Soliman. Automatic
wrappers for large scale web extraction. PVLDB, 4(4), 2011.
[19] N. N. Dalvi, A. Machanavajjhala, and B. Pang. An analysis
of structured data on the web. PVLDB, 5(7), 2012.
[20] H. Elmeleegy, J. Madhavan, and A. Y. Halevy. Harvesting
relational tables from lists on the web. PVLDB, 2(1), 2009.
[21] O. Etzioni, A. Fader, J. Christensen, S. Soderland, and
Mausam. Open information extraction: The second
generation. In IJCAI, 2011.
[22] P. Gulhane, A. Madaan, R. R. Mehta, J. Ramamirtham,
R. Rastogi, S. Satpal, S. Sengamedu, A. Tengli, and
C. Tiwari. Web-scale information extraction with vertex. In
ICDE, 2011.
[23] P. Gulhane, R. Rastogi, S. H. Sengamedu, and A. Tengli.
Exploiting content redundancy for web information
extraction. PVLDB, 3(1), 2010.
[24] Q. Hao, R. Cai, Y. Pang, and L. Zhang. From one tree to a
forest: a unified solution for structured web data extraction.
In SIGIR, 2011.
[25] A. Jaiswal, D. Miller, and P. Mitra. Uninterpreted schema
matching with embedded value mapping under opaque
column names and data values. IEEE Trans. Knowl. Data
Eng., 22(2):291–304, 2010.
[26] J. Kang and J. F. Naughton. On schema matching with
opaque column names and data values. In SIGMOD, 2003.
[27] X. Li, X. L. Dong, K. Lyons, W. Meng, and D. Srivastava.
Truth finding on the deep web: Is the problem solved?
PVLDB, 6(2), 2013.
[28] J. Madhavan, L. Afanasiev, L. Antova, and A. Y. Halevy.
Harnessing the deep web: Present and future. In CIDR, 2009.
[29] C. D. Manning, P. Raghavan, and H. Schütze. Introduction to
Information Retrieval, Cambridge University Press, 2008.
[30] W. Shen, P. DeRose, R. McCann, A. Doan, and
R. Ramakrishnan. Toward best-effort information extraction.
In SIGMOD, 2008.
[31] T.-L. Wong and W. Lam. Learning to adapt web information
extraction knowledge and discovering new attributes via a
bayesian approach. IEEE Trans. Knowl. Data Eng.,
22(4):523–536, 2010.

CONCLUSIONS AND FUTURE WORK

Web data extraction and integration is still an expensive process,
which needs human supervision in many steps to achieve high quality results. In this paper we introduced WEIR, a new algorithm that,
given a collection of detail pages from several web sources, is able
to automatically extract and match their data even in presence of
partial overlap and errors. Ours is a principled approach based on
reasonable assumptions on the input websites. Experiments over
real web sources show that it is more effective than traditional extraction and integration approaches with comparable running time.
We assume that entities described in the detail pages can be associated with an identifier, which is used to align the pages. On
the Web, for many domains this is a natural assumption, and the
identifiers can be derived during the page harvesting phase, or automatically extracted from the page collections starting from a small
seed set. As we mentioned, for other domains where such an assumption does not hold, we will need to generalize WEIR by relying
on distance functions for opaque columns, such as those developed
in [25, 26], which do not require to align the columns’ elements.
Our approach focuses on detail pages with a single entity. However, there are many sites where data about many instances appear
in the same page, e.g., in tables. How to extend our algorithms to
handle these pages is left to future work.

9.

REFERENCES

[1] E. Agichtein and L. Gravano. Snowball: extracting relations
from large plain-text collections. In DL ’00, 2000.
[2] J. Ambite, S. Darbha, A. Goel, C. Knoblock, K. Lerman,
R. Parundekar, and T. Russ. Automatically constructing
semantic web services from online sources. In ISWC, 2009.
[3] A. Arasu and H. Garcia-Molina. Extracting structured data
from web pages. In SIGMOD, 2003.
[4] M. Banko, M. Cafarella, S. Soderland, M. Broadhead, and
O. Etzioni. Open information extraction from the web. In
IJCAI, 2007.
[5] P. A. Bernstein, J. Madhavan, and E. Rahm. Generic schema
matching, ten years later. PVLDB, 4(11), 2011.
[6] L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti.
Supporting the automatic construction of entity aware search
engines. In WIDM, 2008.
[7] L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti.
Probabilistic models to reconcile complex data from
inaccurate data sources. In CAiSE, 2010.
[8] M. Bronzi, V. Crescenzi, P. Merialdo, and P. Papotti.
Extraction and integration of partially overlapping web
sources. Tech. rep., DIA - Roma Tre - TR201, Dec. 2012.
[9] M. J. Cafarella, A. Y. Halevy, and N. Khoussainova. Data
integration for the relational web. PVLDB, 2(1), 2009.

816

2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology

Wrapper Generation for Overlapping Web Sources
Mirko Bronzi, Valter Crescenzi, Paolo Merialdo, Paolo Papotti
Dipartimento di Informatica ed Automazione
Università degli Studi Roma Tre
Via della Vasca Navale, 79 – Rome, Italy
bronzi,crescenz,merialdo,papotti@dia.uniroma3.it
Abstract—Exploiting the huge amount of data available on
the Web involves the generation of wrappers to extract data
from webpages. We argue that existing approaches for web data
extraction from data-intensive websites miss the opportunities
related to the presence of redundant information on the Web.
We propose an innovative approach that aims at pushing
further the level of automation of existing wrapper generation
systems by leveraging the redundancy of data on the Web.
An experimental evaluation of the proposed solution shows a
relevant improvement for the precision of the extracted data,
without a signiﬁcant loss in the recall.

different sources at wrappers generation time.
Exploiting redundancy to improve wrapping: Consider
Figure 1, it shows some pages containing data about soccer players from two websites. Each page reports several
attributes for a player (e.g. name, height, etc.). Abstracting
from the physical organization of data into the HTML code,
each page can be seen as a tuple (with data about one
player), and the collections of player pages from a single
site can be seen as one relation. Notice that the pages
from different sources provide redundant information, both
at the intensional and at the extensional levels: they offer a
common set of attributes for the soccer player conceptual
entity (e.g., weight, position), and many players occur in
both sites (such as Lionel Messi).
Suppose now that we want to bootstrap a system which
needs information about soccer players and we want to use
as a starting source ESPN. In other words, our goal is to
extract the data from the ﬁrst website in Figure 1. If we
know at least another website containing pages for the same
domain (such as the one on the right), we can give both sites
as input to the wrapper generator, thus providing much more
evidence to the algorithms about the values of interest for
the domain. In fact, the relevant values are on both sources,
while data that are not related to the domain (like the
menus, navigational information, or the advertising) do not
overlap. Following these intuitions, we developed a system
that integrates data coming from redundant web sources [8].
In this paper we introduce techniques that exploit the redundancy of information to support the automatic generation
of wrappers. Our solution leverages local regularities to
accomplish the extraction activity, and exploits the global
redundancy of information to discern the relevant data from
useless information without user intervention.
Paper Outline: Section II introduces a deﬁnition of
the wrapper inference problem and technical insight about
the extraction process. Section III reports the results of
the experimental activity that we conducted to evaluate our
approach. Section IV discusses related works and concludes
the paper.

I. I NTRODUCTION
There is a very large and constantly increasing number of
web sources that provide data about any domain of interest
(e.g., commercial and ﬁnancial products, public persons). A
fundamental issue to exploit such data is the development
of scalable techniques to automatically extract data from
fairly structured webpages. An effective large scale and automated extraction of structured web data is a much desired
commodity for many useful information systems: search
engines, tools for competitor analysis, ﬁnancial applications,
and so on. To achieve this goal, several techniques have been
proposed for the automatic generation of web wrappers, i.e.
data extraction programs for data intensive websites (see [3]
for a survey). These techniques leverage the regularities
that occur among pages created from the same template
(e.g. [5], [1]). However, they suffer some limitations due
to the automatic nature of the approach. As the wrappers
are generated by inferring the underlying template, the
extracted data include all the contents of the input pages,
without actually distinguishing between relevant data and
noisy information, such as advertisements, contextual links
or menus. In addition, since there could be the need to
extract complex schemes, it is possible to have errors in
the rules inferred by the wrapper generator, thus impairing
the quality of the data. These problems are usually solved
by introducing a human intervention to post-process the
generated wrappers. In this paper we argue that an alternative
and effective solution to automatically generate wrappers can
be achieved by exploiting the redundancy of information
spread over the Web. Current wrapper generation solutions
work in fact by considering only one website at a time,
thus ignoring the knowledge that can be inferred by relating
978-0-7695-4513-4/11 $26.00 © 2011 IEEE
DOI 10.1109/WI-IAT.2011.160

II. T HE W RAPPER I NFERENCE P ROBLEM
Given a web source, the goal of a wrapper inference
system is to generate a precise and complete wrapper.
32

Figure 1.

Left: pages containing data about two players from the same website. Right: another page from a different website with a common player.

Existing automatic wrapper generation solutions, such as
ExAlg [1] and RoadRunner [5], rely on regularities in the
structure of the pages of a given website in order to infer
a wrapper. Unfortunately, such wrappers are rarely optimal
for two reasons.
First, in order to generate wrappers efﬁciently, algorithms
behind existing tools constrain the expressive power of the
internal language that formalizes the extraction rules. These
restrictions limit the effectiveness of the systems: in some
cases they are not able to infer rules to extract all the data
of interest, thus reducing the completeness of the output
relation. For example, in ExAlg and RoadRunner, because of
these limitations data of interest may be extracted into large
chunks of text,1 mixing several attribute values and noise.
Second, automatic systems are not able to discern values
of interest for the domain from noise information. For this
reason, human intervention is always needed to improve the
precision of the output relation.
In order to overcome such limitations, we take as input
two or more web sources, each consisting of a collection of
pages regarding the same set of instances of a given entity.
Given a target web source, we infer its wrapper by exploiting
not only the local regularities of the pages, but also the
redundancy of relevant data published by different sources.
Such small sets of pages can be either manually given by
the user, or automatically collected by specialized surface
crawlers, such as [2], or deep web crawlers. Intuitively, the
precision of the wrapping process can be improved since the

overlapping of data over distinct sources is a useful feedback
for validation: it is very unlikely that noise information
overlaps.
A. Wrappers Generation
We start by enumerating all the possible candidate extraction rules for any website given as input according
to a formal language for describing them. For inferring
wrappers we use a non-supervised technique based on the
analysis of the HTML template shared by a sample set of
regularly structured pages. In our framework, each page can
be considered as an HTML encoding of a ﬂat tuple, and
a template can be seen as an HTML page containing as
many placeholders as the tuple attributes. Then, the page
generation task can be modeled as a process that replaces
those placeholders with attribute values. The next step is
responsible of the wrappers validation. The main intuition
behind our algorithm is that only correct rules will extract
the same values on several sources.
Rule Generation We use XPath expressions to specify
the extraction rules, and we consider as null the values
extracted by expressions that either do not match or extract an empty string. We consider a class of candidate
extraction rules composed of two types of rules. Absolute
extraction rules are XPath expressions that specify the full
path, including node positions, from the root to the leaf
to extract (e.g., /html[1]/table[1]/tr[2]/td[2]/text()). Labelled extraction rules are XPath expressions obtained by juxtaposing
an expression that locates a label node (e.g., Age) close
to the values to extract, and an expression that reaches

1 In ExAlg [1] these are called “partially extracted attributes”, while in
RoadRunner they are called “subtrees”.

33

the value starting from the template node’s position (e.g.,
//td[contains(text(),’Age’)] /../td[2]/text()).
Many generated rules are useless and can be discarded
before the matching process. We use three ﬁlters: null-ﬁlter
discards all the rules that extract too many null values;
long-pcdata-ﬁlter discards rules extracting very long texts;
costant-ﬁlters discards rules extracting a constant value from
all input pages. The rational behind these ﬁlters is simple: it
is unlikely that rules extracting either too many null values
or too long strings will ever match with values extracted
from another source.
Wrapper Validation At rules generation time, for each
generated wrapper there is not enough information to automatically conclude which are the useful rules, and many
useless and incorrect rules are generated. However, redundant information extracted from different sources allow us
to discern the signiﬁcant rules.
We consider extraction rules from two wrappers equivalent if the values that they extract coincide. In practice,
a strict deﬁnition of equivalence between values from of
distinct sources is not feasible since data from different websites may differ for a number of reasons, including: errors,
inconsistencies, use of different unit of measure and even
imperfect extraction rules. Therefore, we relax the notion of
equivalent values to a more practical notion of equivalence
heuristically determined by means of appropriate similarity
metrics.2

Rules Generation and Filtering For each site, we observed the number of extraction rules generated using as
wrapper generator quick, full, and RR. There are a few
interesting remarks that emerge from this study. The number
of rules generated by full originally is from 2.5 to 5 times
those generated by quick, and two orders of magnitude the
size of the rules generated by RR. After the ﬁlters, the
difference is much smaller, but still full is the setting that
generates the largest number of rules for each website in
every domain. We show next that the larger number of
rules leads to greater execution time, and better recall. On
the other hand, RR generates a much smaller number of
extraction rules without employing any speciﬁc step to ﬁlter
the useless rules.

SOCCER

WINE

FINANCE

Figure 2.

RR
30%
81%
43%
9%
32%
14%
15%
57%
23%

quick
83%
84%
83%
77%
79%
78%
77%
74%
75%

1 VS 4
full
84%
89%
86%
78%
86%
81%
77%
97%
85%

RR
90%
74%
81%
77%
60%
67%
82%
25%
38%

Experimental results for standard wrappers vs our techniques.

Effectiveness Evaluation In order to verify the impact of
our main intuition, we exploited the ability of the system in
improving precision and recall for the generated wrappers.
For each website, we have manually crafted the correct
extraction rules for the attributes of interest for all websites.
We then evaluated the generated wrappers by comparing
their extracted values with those extracted applying the
correct rules on the same pages.3 In the following we report
a measure of how many values of the values automatically
extracted by the system are correct w.r.t. to the correct ones
(P ), how many of the correct values have been found (R),
and their harmonic average (F ). The table in Figure 2 shows
a comparison of the results obtained by running the wrapper
generators over the 15 websites considered. For each domain
and each generation technique, we also report the average
over ﬁve executions (one for each website).
We start the discussion by looking at the results for the
standard wrapper generation (1 VS 0), that is, the case in
which our approach is not applied. Standard techniques
exhibit similar results: the recall is usually very high (most
of the information of interest is extracted), but the precision
is low (many useless information is extracted as well). The
lower recall for RR is expected given the much smaller

III. E XPERIMENTS
In order to evaluate the effectiveness of our approach,
we implemented a prototype solution and run a set of
experiments on 15 websites over three domains (S OCCER,
F INANCE, W INE). We use the standard metrics precision
(P ), recall (R), and F-measure (F ) for the values A generated by our algorithm with respect to the values B extracted
|A∩B|
by manually deﬁned wrappers as: P = |A∩B|
|A| ; R = |B| ; F
∗R
= 2∗P
P +R .
Experimental setup To highlight the improvements of
our approach over existing techniques, we examined our
wrappers (generated with the techniques described in previous Section) and the wrappers generated with the stateof-the-art system ROAD RUNNER [5]. We then evaluated the
results of these different wrapper generation algorithms over
5 different sites for each domain: the quick setting considers
only absolute rules, the full setting uses absolute rules union
a set of relative rules, the RR setting uses rules derived from
the regular expressions generated by ROAD RUNNER.
For each domain we ﬁxed a website and, to form a group
of ﬁve sources, we took the top-4 websites retrieved by our
crawler [2]. As a result of this phase, we got for each site
5 to 20 pages referring to the same set of objects.
2 In

P
R
F
P
R
F
P
R
F

1 VS 0
quick
full
39% 39%
90% 95%
54% 55%
21% 14%
76% 97%
32% 24%
15% 11%
76% 80%
25% 19%

3 Notice that for each correct extraction rule we compare only the best
matching values in a generated wrapper. This metric captures an important
feature in the evaluation: a wrapper may be extracting all the information
of interest spread over multiple items, thus obtaining a lower recall

case of strings, we uses the Jensen-Shannon distance

34

effectiveness of our approach: in the 1 VS 1 case the F value
increased on average of the 48% over the standard wrappers
(from 0.54 to 0.80). Considering additional sources improves
the recall, with F values over 0.83 for the remaining cases.
IV. R ELATED W ORK
The automatic generation of wrappers has been quite studied
by several authors, refer to [3] for a survey on the subject.
In [6] the authors propose an approach that exploits data
redundancy among a web source and a large set of example
seed records to ﬁnd out extraction rules. The effectiveness
of the approach is limited whenever it processes pages
containing data about entities whose attributes have values
from the same domain but with different semantics, such as
the minimum and maximum values for a stock quote. Also
the approach needs a large set of seed records and a large
number of pages. Another project that faces extraction issues
for web data by exploiting information redundancy among
different sources is TurboWrapper [4], which introduces a
complex architecture to exploit several wrapper inference
systems at the same time. The design of TurboWrapper
is motivated by the observation that different web sources
publishing data of the same domain exhibit a strong redundancy at the schema level. However, TurboWrapper does not
consider the redundancy of information that occurs at the
instance-level, but it rather relies on syntactic structure of
attributes (e.g. the ISBN number for books). Our approach
overcomes this limit as we match extraction rules based on
the actual extracted data for corresponding instances.
The redundancy of information has been also exploited in
the context of wrappers maintenance [7], but the considered
redundancy concerns data extracted from the same source at
different times.

Figure 3. P , R, F metrics vs number of auxiliary sources for the S OCCER
domain and the quick setting.

number of rules for the W INE and F INANCE domains.
Notice that the standard procedures to reﬁne such wrappers
rely on manual intervention to identify and reﬁne the correct
rules. This process can become very expensive when the
number of rules found by the system increases. Even with
the most conservative wrapper generator in our experiments
(RR), we can observe an average of 28 to 54 rules, depending
on the domain, which require manual effort to improve the
results in terms of precision.
We now focus on the results of our approach examining a
set of ﬁve websites for each domain (1 VS 4). In general, our
technique greatly improves the precision with a very small
loss in recall. In some cases, for example the full rules in the
F INANCE domain, the quick and the RR rules for the W INE
domain, the recall is also improved by our technique. For
every scenario, and every wrapper generation technique, the
F-measure is greatly improved, with a minimum improvement of 54% and a maximum of 379% (average: 175%).
The full setting always improves the average F value with
respect to the quick and RR settings: the larger number of
rules evaluated leads to a signiﬁcantly better recall R in
every scenario, with longer execution times. In the S OCCER
domain, the full setting requires a total of 14 minutes to
execute, more than 3 times the time needed for the quick
setting and 4 minutes more than RR.
As a ﬁnal evaluation, we exploit one scenario to see
how the number of sources impacts the quality of the
ﬁnal wrapper. Figure 3 shows how the number of auxiliary
sources affects the results using the websites in the S OCCER
domain and quick rules. We considered all the possible
combinations of matching sources for every website and
computed the average of their results. For instance, to
get the 1 VS 0 results, we computed P , R, and F by
considering each website at a time. We then averaged their
values over the ﬁve sources and reported the value in the
graph. To compute the 1 VS 1 results, we considered the
ten possible combinations of two websites, and reported the
average value. Similarly for the other points in the charts.
As discussed above, wrappers alone are able to gather many
correct values (high recall) but with a very low precision.
It is sufﬁcient to add just 1 auxiliary website to see the

R EFERENCES
[1] A. Arasu and H. Garcia-Molina. Extracting structured data
from web pages. In ACM SIGMOD 2003.
[2] L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti. Supporting
the automatic construction of entity aware search engines. In
ACM WIDM 2008.
[3] C. Chang, M. Kayed, M. Girgis, and K. Shaalan. A survey of web information extraction systems. IEEE TKDE
18(10):1411–1428, October 2006.
[4] S.-L. Chuang, K. C.-C. Chang, and C. X. Zhai. Context-aware
wrapping: Synchronized data extraction. In VLDB 2007.
[5] V. Crescenzi, G. Mecca, and P. Merialdo. ROAD RUNNER:
Towards automatic data extraction from large Web sites. In
VLDB 2001.
[6] P. Gulhane, R. Rastogi, S. H. Sengamedu, and A. Tengli.
Exploiting content redundancy for web information extraction.
PVLDB, 3(1):578–587, 2010.
[7] K. Lerman, S. Minton, and C. A. Knoblock. Wrapper maintenance: A machine learning approach. J. Artif. Intell. Res.
(JAIR), 18:149–181, 2003.
[8] P. Papotti, V. Crescenzi, P. Merialdo, M. Bronzi, and L. Blanco.
Redundancy-driven web data extraction and integration. In
WebDB, 2010.

35

Benchmarking the Chase
Michael Benedikt 1
Boris Motik 1
1

University of Oxford
Oxford, UK

George Konstantinidis 1
Paolo Papotti 3
Efthymia Tsamoura 1
2

University of Basilicata
Potenza, Italy

ABSTRACT

3

Arizona State University
Phoenix, AZ, USA

databases. But while the theorem proving and the database communities have a long history of creating benchmarks and detailed
evaluation methodologies (e.g., SMTLib [37] and TPTP [40] in the
former, the TPC family [39] in the latter), there is little corresponding infrastructure to support experimental validation of techniques
such as the chase that combine reasoning and data management.
This paper aims to take a major step in changing this situation.
We present a new benchmark for chase systems covering a wide
range of scenarios. Since the systems in the literature support different kinds of dependencies, we have developed dependency sets
with different structural properties, and datasets of varying sizes.
We also define example tasks for two main applications of the
chase: (i) data exchange, which involves materializing an instance
of a target schema satisfying a given set of dependencies with respect to an instance of a source schema; and (ii) computing certain
answers to conjunctive queries over databases with dependencies.
We then analyze a variety of publicly available systems on our
benchmark in order to answer the following questions:
• How do existing chase-related systems fare in absolute terms
on these tasks? That is, to what extent can they be considered as proof that the chase-based approaches to solving
these tasks are practically feasible?
• What algorithmic and architectural choices are most critical
for the performance of chase-related systems?
• Are there other approaches or other kinds of systems that can
perform these same tasks and, if so, how do they compare to
tools that use the chase?
In an attempt to answer these questions, we considered a number of
systems that implement the chase as a component, including systems motivated from data exchange, data cleaning, query reformulation, and query answering.
We mentioned above that many communities have looked at techniques similar to the chase, and at problems similar to data exchange and query answering. To better understand the connection
with the related communities, we also applied our benchmark to
systems that are not specifically “branded” as chase systems, but
that can nonetheless perform some of the tasks that the chase addresses. In particular, we looked at Datalog engines that support
function symbols as they can solve both data exchange and query
answering problems, as well as a leading theorem prover that can
solve various query answering problems.
Organization. In the rest of this paper, we first present some
background about the chase (Sections 2 and 3). Next, we describe
our test systems (Section 4), and discuss our testing infrastructure
and test scenarios (Section 5). Then, we present the system comparison results (Section 6), followed by a discussion of the insights
gained (Section 7) and the future challenges that emerged from our
study (Section 8). Finally, we close with a discussion of the re-

The chase is a family of algorithms used in a number of data management tasks, such as data exchange, answering queries under dependencies, query reformulation with constraints, and data cleaning. It is well established as a theoretical tool for understanding
these tasks, and in addition a number of prototype systems have
been developed. While individual chase-based systems and particular optimizations of the chase have been experimentally evaluated
in the past, we provide the first comprehensive and publicly available benchmark—test infrastructure and a set of test scenarios—for
evaluating chase implementations across a wide range of assumptions about the dependencies and the data. We used our benchmark
to compare chase-based systems on data exchange and query answering tasks with one another, as well as with systems that can
solve similar tasks developed in closely related communities. Our
evaluation provided us with a number of new insights concerning
the factors that impact the performance of chase implementations.

1.

Giansalvatore Mecca 2
Donatello Santoro 2

INTRODUCTION

The chase [25] is a long-standing technique developed by the
database community for reasoning with constraints, also known
as dependencies, expressed as universal implications possibly containing existential quantification in the conclusion. When applied
to a set of dependencies and a set of facts, the chase extends the
facts in a forward-chaining manner to satisfy the dependencies.
The chase has been intensively studied as a theoretical tool, but
over the last decade practical aspects, such as developing optimizations of the chase algorithms and building chase-based systems for
various tasks, have also been considered. Although algorithms for
chasing source-to-target dependencies have shown promise in practice, with target dependencies scalability has been achieved only in
quite restricted cases. The performance of chase implementations
on large sets of complex dependencies and large datasets remains
unknown. This suggests that it is time to evaluate the extent to
which computing the chase is practically feasible.
The chase is closely related to and can be seen as a special case
of theorem proving calculi such as tableaux and resolution, and it
can also be seen as a generalization of standard query evaluation in
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

PODS’17, May 14-19, 2017, Chicago, Illinois, USA
© 2017 ACM. ISBN 978-1-4503-4198-1/17/05. . . $15.00
DOI: http://dx.doi.org/10.1145/3034786.3034796

37

lated work and conclusions (Sections 9 and 10). We emphasize
that full details regarding the systems under test, our test infrastructure, and our scenarios are available on the benchmark Web
page (http://dbunibas.github.io/chasebench).

2.

extension of h to a homomorphism of ρ(~x,~y) into I exists, and if τ
is an EGD, then h(xi ) 6= h(x j ). Finally, dependency τ is satisfied in
I if there does not exist an active trigger for τ in I.
Let Σ be a set of dependencies and let I be a finite, null-free
instance. Instance J is a solution for Σ and I if I ⊆ J and J |= τ for
each τ ∈ Σ. A solution J for Σ and I is universal if, for each solution
J 0 for Σ and I, a term mapping µ from the active domain of J to the
active domain of J 0 exists such that µ(J) ⊆ J 0 and µ(c) = c holds
for each constant value c ∈ Const. Solutions for Σ and I are not
unique, but universal solutions are unique up to homomorphism.
Queries.
A conjunctive query (CQ) is a formula of the form
V
∃~y i Ai , where Ai are relational, null-free atoms. A substitution σ
is an answer to Q on instance I if the domain of σ is precisely the
free
variables of Q, and if σ can be extended to a homomorphism of
V
i Ai in I. By choosing a canonical ordering for the free variables
~x of Q, we often identify σ with an n-tuple σ (x1 ), . . . , σ (xn ). The
output of Q on I is the set Q(I) of all answers to Q on I.
Answering queries under dependencies. Let Σ be a set of dependencies, let I be a finite, null-free instance, and let Q be a CQ.
A substitution σ is a certain answer to Q on Σ and I if σ is an answer to Q on each solution J for Σ and I. The task of finding all
certain answers to Q on Σ and I is called query answering under
dependencies, and we often abbreviate it to just query answering.
The following fundamental result connects universal solutions and
query answering: for each substitution σ and each universal solution J for Σ and I, substitution σ is a certain answer to Q on Σ and
I if and only if σ is a null-free answer to Q on J [15].
The chase. The chase modifies an instance by a sequence of
chase steps until all dependencies are satisfied. Let I be an instance,
let τ be a TGD or an EGD of the form (1) or (2), and let h be a
trigger for τ in I. If τ is a TGD, applying the chase step for τ and h
to I extends I with facts of the conjunction h0 (ρ(~x,~y)), where h0 is
a substitution such that h0 (xi ) = h(xi ) for each variable xi ∈~x, and
h0 (y j ), for each y j ∈~y, is a fresh labeled null that does not occur in
I. Moreover, if τ is an EGD, applying the chase step for τ and h to
I fails if h(xi ) 6= h(x j ) and {h(xi ), h(x j )} ⊆ Const, and otherwise
it computes µ(I) where µ = {h(x j ) 7→ h(xi )} if h(xi ) ∈ Const, and
µ = {h(xi ) 7→ h(x j )} if h(xi ) 6∈ Const.
For Σ a set of TGDs and EGDs and I a finite, null-free instance, a
chase sequence for Σ and I is a (possibly infinite) sequence I0 , I1 , . . .
such that I = I0 and, for each i > 0, instance Ii (if it exists) is obtained from Ii−1 by applying a successful chase step to a dependency τ ∈ Σ and an active trigger h for τ in Ii−1 . The sequence
must be fair: for each τ ∈ Σ, each i ≥ 0, and each active trigger h
for τ in Ii , some j > i must exist such that h is not an active trigger
for τ in I j (i.e., no chase step should be postponed indefinitely).
The result
of a chase sequence is the (possibly infinite) instance
S
T
I∞ = i≥0 j≥i I j . Since EGD chase steps can fail, a chase sequence for a given Σ and I may not exist. Moreover, chase steps
can be applied in an arbitrary order so a chase sequence for Σ and I
is not unique. Finally, EGD steps are not monotonic (i.e., Ii−1 6⊆ Ii
holds when Ii is obtained by applying an EGD step to Ii−1 ), and so
I∞ is not uniquely determined by Σ and I. Still, each result I∞ of a
chase sequence for Σ and I is a universal solution for Σ and I [15].
A finite chase sequence is terminating. A set of dependencies Σ
has terminating chase if, for each finite, null-free instance I, each
chase sequence for Σ and I is terminating. For such Σ, the chase
provides an effective approach to computing certain answers to a
CQ Q on Σ and I: we compute (any) chase I∞ of Σ and I, we compute the output Q(I∞ ), and finally remove all substitutions that are
not null-free. Checking if a set of dependencies Σ has terminating
chase is undecidable [13]. Weak acyclicity [15] was the first suffi-

BACKGROUND

Database basics. Let Const, Nulls, and Vars be mutually disjoint, infinite sets of constant values, labeled nulls, and variables,
respectively. Intuitively, constant values are unique; labeled nulls
represent unknown values; and variables are used in dependencies
and queries. A value is a constant value or a labeled null, and a
term is a value or a variable. We often abbreviate an n-tuple of
terms t1 , . . . ,tn as ~t, and we often treat it as a set and write ti ∈~t.
A schema is a set of relation names (or just relations), each associated with a nonnegative integer called arity. An instance I of a
schema assigns to each n-ary relation R in the schema a (possibly
infinite) set I(R) of n-tuples of values. The active domain of I is
the set of all values occurring in a tuple of some I(R). A relational
atom has the form R(~t) where R is an n-ary relation and ~t is an ntuple of terms. An equality atom has the form t1 = t2 where t1 and
t2 are terms. A fact is an atom that does not contain variables. An
instance can equivalently be seen as a set of relational facts, so we
use notation R(~t) ∈ I and ~t ∈ I(R) interchangeably. An atom (resp.
an instance) is null-free if it does not contain null values.
Term mappings. A term mapping σ is a partial mapping of
terms to terms; we write σ = {t1 7→ s1 , . . . ,tn 7→ sn } to denote that
σ (ti ) = si for 1 ≤ i ≤ n. For α a term, an atom, a conjunction of
atoms, or a set of atoms, σ (α) is obtained by replacing each occurrence of a term t in α that also occurs in the domain of σ with
σ (t) (i.e., terms outside the domain of σ remain unchanged). The
composition of σ with a term mapping µ is the term mapping σ ◦ µ
whose domain is the union of the domains of σ and µ, and it is defined by (σ ◦ µ)(t) = σ (µ(t)). A substitution σ is a term mapping
whose domain contains only variables and whose range contains
only values; moreover, σ is null-free if its range contains only constants;
finally, σ is a homomorphism of a conjunction of atoms
V
ρ = i Ai into an instance I if the domain of σ is the set of all
variables occurring in ρ and σ (ρ) ⊆ I.
Dependencies and solutions. Semantic relationships between
relations can be described using dependencies, which come in two
forms. A Tuple Generating Dependency (TGD) is a logical sentence of the form (1), where λ (~x) and ρ(~x,~y) are conjunctions of
relational, null-free atoms whose free variables are contained in ~x,
and ~x ∪~y correspondingly.
∀~x λ (~x) → ∃~y ρ(~x,~y)

(1)

An Equality Generating Dependency (EGD) is a logical sentence
of the form (2), where λ (~x) is a conjunction of relational, null-free
atoms over variables ~x, and {xi , x j } ⊆~x.
∀~x λ (~x) → xi = x j

(2)

The left-hand side of a TGD or an EGD (i.e., the conjunction λ (~x))
is the body of the dependency, and the right-hand side is the head.
A dependency is linear if it has exactly one atom in the body. By a
slight abuse of notation, we often treat heads and bodies as sets of
atoms, and we commonly omit the leading universal quantifiers.
Let I be an instance and let τ be a TGD of the form (1) or an
EGD of the form (2). The notion of dependency τ holding in I (or
I satisfying τ, written I |= τ) is given by first-order logic, and it can
be restated using homomorphisms as follows. A trigger for τ in I
is a homomorphism h of λ (~x) into I. Moreover, an active trigger
for τ in I is a trigger h for τ in I such that, if τ is a TGD, then no

38

Algorithm 1 R ESTRICTED -C HASE(Σ, I)
1: ∆I := I
2: while ∆I 6= 0/ do
3: N := 0,
/
µ := 0/
4: for each τ ∈ Σ with body λ (~x) do
5:
for each trigger h for τ in I such that h(λ (~x)) ∩ ∆I 6= 0/ do
6:
if h is an active trigger for τ in µ(N ∪ I) then
7:
if τ = ∀~x λ (~x) → ∃~y ρ(~x,~y) is a TGD then
8:
h0 := h ∪ {~y 7→ ~v} where ~v ⊆ Nulls is fresh
9:
N := N ∪ h0 (ρ(~x,~y))
10:
else if τ = ∀~x λ (~x) → xi = x j is an EGD then
11:
if {h(xi ) 6= h(x j )} ⊆ Const then fail
12:
ω := {max(h(xi ), h(x j )) 7→ min(h(xi ), h(x j ))}
13:
µ := µ ◦ (ω ◦ µ)
14: ∆I := µ(N ∪ I) \ I
15: I := µ(I) ∪ ∆I

can translate λ (~x) into SQL in a straightforward manner. However,
evaluating λ (~x) in each iteration “from scratch” would be very inefficient since it would repeatedly consider triggers from all previous
iterations. Algorithm 1 thus uses seminaïve evaluation [1]. It maintains an auxiliary set ∆I of “newly derived facts” from the most
recent iteration, and it requires at least one atom of h(λ ) in line 5
to be contained in ∆I. As a consequence, each combination of τ
and h is considered at most once during algorithm’s execution.
Applying chase steps. An important consideration is how query
λ (x) is evaluated over I in line 5: we must ensure that we do not
miss any applicable triggers, and that we retrieve each such trigger once. One possibility is to compute and store the result in a
temporary relation, but this can impose a significant overhead. It is
therefore often preferable to evaluate λ (x) over I in a “streaming”
mode, where a trigger h is returned as soon as it is computed; but
then, set I should not change in lines 4–13 or the modifications to
I could affect the enumeration of triggers in line 5. To ensure this,
the chase steps in Algorithm 1 do not modify I directly. Instead, all
changes are accumulated in an auxiliary set N and a term mapping
µ; after each iteration, ∆I contains the “newly derived subset” of N
(line 14), which is propagated to I (line 15). The algorithm terminates when ∆I becomes empty in line 2 since another iteration then
cannot derive new facts.
Handling EGDs. Applying a chase step to an EGD τ of the
form (2) and trigger h poses several problems. A minor issue is
that, when h(xi ) and h(x j ) are both labeled nulls, we can replace
either value with the other. To make this choice deterministic, one
can totally order all values so that all constant values are smaller
than all labeled nulls, and then always replace the larger value with
the smaller one (line 12). Constant values are thus never replaced
with labeled nulls, and this also ensures uniqueness of the chase for
a specific chase variant that we discuss shortly.
A more complex issue is to ensure that I does not change in
lines 5–13. To achieve this, Algorithm 1 uses a term mapping µ
that accumulates all the required changes to I (line 13): at the end
of each iteration, µ(v) is defined for each labeled null v that is to
be replaced with µ(v). The expression in line 13 ensures that µ
is correctly updated when several EGD steps are applied in single iteration. For example, consider µ = {v1 7→ v2 , v3 7→ v4 } and
ω = {v2 7→ v3 }; then, ω ◦ µ = {v1 7→ v3 , v2 7→ v3 , v3 7→ v4 } “normalizes” µ with ω, and µ ◦ (ω ◦ µ) = {v1 7→ v4 , v2 7→ v4 , v3 7→ v4 }
reflects the cumulative effects of all EGDs.
At the end of each iteration (lines 14–15), the facts in both I and
N may require updating, and all facts newly derived in the iteration
most be added to ∆I to correctly further trigger dependencies.
Checking active triggers. Assume that h is a trigger for a dependency τ in an instance I, and consider checking whether h is
an active trigger for τ in µ(N ∪ I) in line 6. It should be clear
that h is a trigger for τ in µ(N ∪ I) if and only if µ(h(x)) = h(x)
for each variable x from the domain of h. In other words, we
simply check whether “h up-to-date with µ.” Thus, to complete
the active trigger check, if τ is of the form (2), we check whether
h(xi ) 6= h(x j ) holds; and if τ is of the form (1), we check γ(I) = 0/
where γ = ∃~y ρ(h(~x),~y) is a boolean CQ. The latter can be implemented using standard query evaluation techniques, and RDBMSbased systems can simply extend the CQ from line 5 with a NOT
EXISTS (SELECT * WHERE ...) condition. In the rest of this section and in Section 7 we discuss several theoretical and practical
drawbacks of checking for active triggers and present alternatives.
Chase variants. The chase variant given in Algorithm 1 is called
the restricted chase to stipulate that triggers are restricted only to
the active ones (cf. line 6). A drawback of the restricted chase is

cient polynomial-time condition for checking if Σ has terminating
chase. Stronger sufficient (not necessarily polynomial-time) conditions have been proposed subsequently [20, 32].
Data exchange. In relational-to-relational data exchange [15], a
transformation of an arbitrary instance of a source schema into an
instance of a target schema is described using
• a set Σst of s-t (source-to-target) TGDs where all body atoms
use relations of the source schema and all head atoms use
relations of the target schema, and
• a set Σt of target dependencies (i.e., TGDs or EGDs) whose
atoms use relations of the target schema.
Given a finite, null-free instance I of the source schema, the objective of data exchange is to compute a universal solution to the set of
dependencies Σ = Σst ∪ Σt and I. If Σ has terminating chase, then a
universal solution can be computed using the chase, and it can be
used to answer queries [15].

3.

IMPLEMENTING THE CHASE

In this section we discuss the main challenges that chase implementations must address. Our discussion is largely independent
of the specific technology, but we discuss briefly how these issues
impact RDBMS-based implementations.
General structure. Computing the chase is naturally expressed
as a fixpoint computation. Algorithm 1 achieves this by following
the definition of a chase sequence from Section 2: in each iteration
(lines 2–15), the algorithm examines each dependency τ (line 4)
and trigger h (line 5), checks whether h is active (line 6), and, if
so, applies the chase step (lines 8–9 for TGDs and lines 11–13 for
EGDs). Although chase implementations may and do depart from
the specifics of Algorithm 1, the algorithm is useful because it allows us to discuss the following issues, which each implementation
must address in one form or another.
• To avoid an important source of inefficiency, we must ensure
that each relevant trigger for τ and I is considered in line 5 at
most once.
• Applying chase steps should not interfere with the enumeration of triggers.
• The EGD chase steps are complex and require care.
• Checking whether a trigger is active can be expensive.
Identifying triggers. Let τ be a TGD or an EGD of the form
(1) or (2), respectively, and let I be a finite instance. Clearly, substitution h is a trigger for τ and I if and only if h is an answer to
λ (~x) on I. Hence, the triggers for τ can be determined in line 5
by evaluating λ (~x) as a CQ over I. The latter can be solved using
any known join algorithm, and an RDBMS-based implementation

39

after which the chase terminates: functional term f (x1 ) in (7) captures the fact that the fresh null depends only on x1 , and so applying (7) to R(a, f (a)) and R(b, f (b)) does not introduce more nulls
as in Example 3. Normalization is very important since it eliminates variables within Skolem terms; for example, skolemizing (3)
directly produces (9), and applying (9) to I does not terminate.

that the chase solution is not unique (even up to isomorphism of
labeled nulls), as Example 1 shows.
E XAMPLE 1. Let Σ and I be as in (3) and (4).
R(x1 , x2 ) → ∃y R(x1 , y) ∧ A(y) ∧ A(x2 )

(3)

I = { R(a, b), R(b, b) }

(4)

R(x1 , x2 ) → R(x1 , f (x1 , x2 )) ∧ A( f (x1 , x2 )) ∧ A(x2 )

Dependencies in Σ are weakly acyclic [15] so the restricted chase
terminates on all finite instances, but the chase solution depends on
the ordering of chase steps. Triggers h1 = {x1 7→ a, x2 7→ b} and
h2 = {x1 7→ b, x2 7→ b} for (3) are both active in I. Applying the
TGD step to h1 makes h2 inactive by deriving R(a, v1 ), A(v1 ), and
A(b). In contrast, applying the TGD step to h2 makes h1 inactive
by deriving R(b, v1 ), A(v1 ), and A(b).

Since functional terms provide canonical “names” for labeled
nulls, a global counter of labeled nulls is not required, which may
simplify implementation. For example, deriving the first atom of
(7) can be implemented using the SQL query (10), which does not
interact with other TGDs.
INSERT INTO R(a,b)
SELECT DISTINCT R.a, append('_Sk_f(',R.a,')') FROM R

The chase can be optimized by normalizing TGDs prior to applying the chase: for each TGD τ ∈ Σ of the form (1) such that ρ(~x,~y)
can be rewritten as ρ1 (~x1 ,~y1 ) ∧ ρ2 (~x2 ,~y2 ) so that y~1 ∩ y~2 = 0,
/ we
replace τ in Σ with ∀~x λ (~x) → ∃~yi ρi (~xi ,~yi ) for i ∈ {1, 2}. Example 2 shows how normalization can lead to smaller instances.

(5)
(6)

E XAMPLE 5. Let Σ, I, h1 , and h2 be as in Example 1. Both h1
and h2 are active for I, so the parallel chase applies the TGD step
to both triggers independently and derives R(a, v1 ), A(v1 ), A(b),
R(a, w1 ), and A(w1 ). No active triggers exist after this step so the
parallel chase terminates.

Checking whether trigger h is active (line 6) can be difficult in
practice, particularly for a test against µ(N ∪ I); we discuss these
issues in detail in Section 7. Furthermore, the dependence on the
ordering of chase steps can make the chase more difficult to analyze
from a theoretical point of view. This motivates several variants in
which the check in line 6 is either eliminated or weakened.
The unrestricted (or oblivious) chase simply eliminates line 6.
Such a simple solution removes the overhead of checking active
triggers and the dependence on the ordering of chase steps. But,
as Example 3 shows, the unrestricted chase does not necessarily
terminate even for weakly acyclic TGDs.

The single-TGD-parallel (or 1-parallel) chase checks active triggers w.r.t. all facts derived thus far apart from the ones derived by
the TGD τ currently considered in line 4. As with the parallel
chase, implementing this variant can be much easier than for the
restricted chase (see Section 7).
While the check in line 6 is not needed with skolemized TGDs, it
can still be used, in which case we obtain the restricted (or parallel
or 1-parallel) Skolem chase: each of these two chase variants never
produces more facts than the original variant, and in certain cases
it can produce fewer facts.
The frugal chase [22] further considers triggers that are only partially active. Let τ be a TGD τ of the form (1) such that ρ(~x,~y) can
be rewritten as ρ1 (~x1 ,~y1 ) ∧ ρ2 (~x2 ,~y2 ) where ~x1 and ~x2 , and ~y1 and
~y2 are not necessarily disjoint, and let h be a trigger for τ in an instance I. Then, h is partially active for τ in I if h can be extended to
a homomorphism h0 such that (i) ρ1 (h0 (~x1 ), h0 (~y1 )) ⊆ I, (ii) h0 (~y1 )
contains only labeled nulls, and (iii) for each fact P(~w) ∈ I such that
~w ∩ h0 (~y1 ) 6= 0,
/ we have P(~w) ∈ ρ1 (h0 (~x1 ), h0 (~y1 )) (i.e., each atom
in I that joins “existentially” with the image of ρ1 is contained in
the image of ρ1 ). Since ρ1 is already satisfied, the frugal chase
satisfies the TGD τ by extending I with ρ2 (h00 (~x2 ), h00 (~y2 )), where
h00 is a homomorphism that extends (one such) h0 by mapping the
variables in ~y2 \~y1 to fresh labeled nulls.

E XAMPLE 3. For Σ and I defined as in Example 1, the unrestricted chase derives the following infinite set of facts.
R(a, v1 ), A(v1 ), R(a, v2 ), A(v2 ), . . .
A(b), R(b, w1 ), A(w1 ), R(b, w2 ), A(w2 ), . . .
The unrestricted Skolem chase [26, 38, 28] also eliminates line 6,
but it also skolemizes TGDs: in each TGD τ of the form (1), each
existentially quantified variable y ∈~y is replaced with a function
term f (~z) where f is a fresh function symbol and ~z contains all
variables occurring in both the head and the body of τ. The chase
then proceeds as in Algorithm 1, but without line 6 and by using the
preprocessed τ in line 7. The result of the Skolem chase is unique
(if the EGD steps are determinized as in line 8). Although cases
exist where the restricted chase terminates but the Skolem chase
does not, the known acyclicity conditions [20] ensure termination
of both. Example 4 illustrates the Skolem chase.

E XAMPLE 6. Let Σ contain the TGD (5) from Example 2 and
TGD (11), and let I be as in (12).

E XAMPLE 4. Normalizing and then skolemizing TGD (3) produces (7) and (8).
R(x1 , x2 ) → R(x1 , f (x1 )) ∧ A( f (x1 ))

(7)

R(x1 , x2 ) → A(x2 )

(8)

(10)

The parallel chase [13] weakens line 6 so that it checks whether
h is active in I, rather than in µ(N ∪ I); since I is fixed in an iteration, this can make checking active triggers much easier to implement. Known acyclicity conditions [20] ensure termination of the
parallel chase, and the solution is deterministic, although it may be
larger than the one produced by the restricted chase. Example 5
illustrates the parallel chase.

E XAMPLE 2. Normalizing TGD (3) produces (5) and (6). Now
by applying (6) to I from Example 1 we derive A(b), which makes
triggers h1 and h2 for (5) both inactive.
R(x1 , x2 ) → ∃y R(x1 , y) ∧ A(y)
R(x1 , x2 ) → A(x2 )

(9)

B(x) → ∃y R(x, y)

(11)

I = { B(a) }

(12)

Assume now that the frugal chase first applies (11) and thus extends I with fact R(a, v1 ). Then, h = {x1 7→ a, x2 7→ v1 } is a partially active trigger for TGD (5): we can decompose the head of
(5) such that ρ1 = R(x1 , y) and ρ2 = A(y), and we can extend h to

Applying TGDs (7) and (8) to the set of facts I from Example 1
produces facts R(a, f (a)), A( f (a)), A(b), R(b, f (b)), and A( f (b)),

40

h0 = {x1 7→ a, x2 7→ v1 , y 7→ v1 } such that h0 (ρ1 ) is contained in the
current instance, and h0 (y) = v1 does not occur in the instance outside of h0 (ρ1 ). Hence, the frugal chase satisfies (5) by adding A(v1 )
to the instance, instead of introducing a fresh labelled null.

other communities that either (i) implement algorithms related to
the chase, or (ii) can answer queries over dependencies using very
different approaches. Many systems satisfy these requirements, so
we decided to restrict our attention to several prominent representatives. In particular, we considered Datalog engines in the former,
and a resolution-based theorem prover in the latter category.
DLV [24] is a mature disjunctive Datalog system supporting
a range of features such nonmonotonic negation, aggregates, and
user-defined functions. The system comes in several flavors: a
RAM-based version that supports function symbols in the rules, another RAM-based version with native support for TGDs [23], and
an RDBMS-based version that supports neither function symbols
nor TGDs. The latter is not applicable to our setting, and we used
the version with function symbols since it proved to be more stable. We implemented a preprocessing skolemization step, allowing
DLV to support the unrestricted Skolem chase for TGDs; however,
the system does not support EGDs [33]. The system can be used
without a query, in which case it computes and outputs the chase
solution. If a query is provided, the system evaluates the query over
the chase and outputs the result. To obtain certain answers, we externally postprocess the query output to remove functional terms.
RDF OX [30] is a high-performance RAM-based Datalog engine.
It was originally designed for Datalog with EGDs over the RDF
data model and without the unique name assumption. To support
the chase, RDF OX was extended as follows. First, a builtin function was added that produces a labeled null unique for the function’s arguments, which emulates Skolem terms. Second, a builtin
function was added that checks whether a CQ is not satisfied in the
data, thus enabling both the restricted and the unrestricted chase
variants. Third, a mode was implemented that handles EGDs under unique name assumption. Fourth, to support relations of arbitrary arity, a preprocessor was implemented that shreds n-tuples
into RDF triples and rewrites all dependencies accordingly.
E [36] is a state of the art first-order theorem prover that has
won numerous competitions. It takes as input a set ofVaxioms F
and a conjecture H, and it decides the unsatisfiability of F ∧ ¬H.
E implements the paramodulation with selection [31] calculus, of
which the unrestricted Skolem chase is an instance: each inference
of the Skolem chase is an inference of paramodulation (but not vice
versa). Paramodulation requires F to be represented as a set of
clauses—that is, first-order implications without existential quantifiers but possibly containing function symbols and the equality
predicate. Thus, F can capture EGDs and the result of preprocessing TGDs as described in Section 3. Moreover, E can also be used
in a mode where F contains arbitrary first-order formulas, thus
capturing TGDs directly without any preprocessing; however, this
approach proved less efficient, so we did not use it in our experiments. Finally, conjecture H can contain free variables,
in which
V
case E outputs each substitution σ that makes F ∧ ¬σ (H) unsatisfiable; thus, E is interesting as it can answer queries without
computing the chase in full. Ordered paramodulation targets firstorder logic, which is undecidable; hence, E is not guaranteed to
terminate on all inputs, not even if F encodes dependencies on
which the unrestricted Skolem chase terminates. The system’s behavior can be configured using many parameters, and we used the
values suggested by the system’s author.

The core chase [13] omits the active trigger check completely;
however, after each iteration it replaces the current instance with its
core [16]—the smallest subset of I that is homomorphically equivalent to I. The core chase produces a smallest finite solution whenever one exists, but efficient computation of the core on instances
of nontrivial sizes is an open problem.

4.

THE SYSTEMS TESTED

As we explained in Section 1, our objectives are to determine
whether existing chase implementations can support data exchange
and query answering on nontrivial inputs, and to identify the implementation decisions most relevant to performance. To answer
these questions, we used nine publicly available systems shown in
Table 1. We group the systems based on their primary motivation.
Systems motivated by data exchange. DEM O [34] was one of
the first data exchange engines. It implements the restricted chase
for s-t TGDs and target TGDs and EGDs; moreover, upon termination, it computes the core of the solution using the algorithm by
Gottlob and Nash [19]. The system runs on top of PostgreSQL or
HSQLDB, and we used the former in our experiments.
C HASE F UN [12] is a more recent data exchange system. It supports only s-t TGDs and functional dependencies, and it implements a variant of the unrestricted Skolem chase that orders TGD
and EGD chase steps to reduce the size of the intermediate chase results. We used an implementation that runs on top of PostgreSQL.
Systems motivated by data cleaning. L LUNATIC [18] was initially developed for data cleaning, but has since been redesigned as
an open-source data exchange system. It handles s-t TGDs and target TGDs and EGDs, and it can also compute certain query answers
over the target instance. It implements the 1-parallel Skolem chase
(the default), the unrestricted and restricted Skolem chase, and the
restricted chase with fresh nulls (i.e., without Skolem terms). The
system runs on top of PostgreSQL.
Systems motivated by query reformulation. Three of our test
systems use the chase for query reformulation.
P EGASUS [29] is a system for finding minimal queries equivalent
to a given query with respect to a set of TGDs and EGDs. It uses
the Chase & Backchase method of Deutsch, Popa, and Tannen [14],
which internally uses the restricted chase implemented in RAM.
PDQ [10, 11] takes a query, a set of integrity constraints, and
a set of interface descriptions (e.g., views or access methods), and
it produces an equivalent query that refers only to the interfaces
and whose cost is minimal according to a preselected cost function.
By extending the Chase & Backchase method, PDQ reduces query
reformulation to checking query containment under TGDs and/or
EGDs, and the latter problem is solved using an implementation of
the restricted chase on top of PostgreSQL.
Systems motivated by query answering. G RAAL [7] is an
open-source toolkit developed for computing certain answers to
queries under dependencies. Although the system was not originally designed for chase computation, it uses a “saturation algorithm” that can be seen as variant of the standard chase. G RAAL
can be used both in an RAM and on secondary storage, such an
RDBMS, a triple store, or a graph database. We used the RAMbased version as we found it to be faster.
Chase-related systems. A prominent goal of our work was to
investigate how chase implementations fare against systems from

5.

THE BENCHMARK

Our C HASE B ENCH benchmark consists of two distinct parts.
The first part is described in Section 5.1 and it comprises several
tools that allow the generation and processing of test data in a common format. The second part is described in Section 5.2 and it comprises a number of test scenarios, each consisting of a (i) schema

41

System

s-t TGDs

t TGDs

EGDs

Cert. Ans.

Engine

Strategy

Sources

unrestricted skolem chase
restricted chase + core computation
restricted chase
restr./unrestr./1-parallel skolem/fresh-nulls chase
restricted chase
restricted chase

X
X
X
X

Explicit chase implementations

C HASE F UN
DEM O
G RAAL
L LUNATIC
PDQ
P EGASUS

X
X
X
X
X
X

X
X
X
X
X

DLV
E
RDF OX

X
X
X

X
X
X

FDs only
X
X
X
X

X
X
X

RDBMS
RDBMS
RAM
RDBMS
RDBMS
RAM

Chase-related systems

X
X

X
X
X

RAM
RAM
RAM

unrestricted skolem chase
paramodulation
restricted/unrestricted skolem chase

X
X

Table 1: Summary of the tested systems
description, (ii) a source instance, (iii) sets of s-t TGDs, target
TGDs, and/or target EGDs, and (iv) possibly a set of queries. We
used existing resources whenever possible; for example, we repurposed scenarios produced by I B ENCH [4], as well as instances of
varying sizes produced by the instance-generation tool TOX GENE
[8]. We divide our scenarios into the following two groups.
Correctness scenarios were designed to verify that the systems
correctly produce universal solutions. Checking homomorphisms
between solutions is computationally challenging so these scenarios generate very small solutions. Moreover, these scenarios do not
contain queries, as correctness of query answering can be verified
by simply comparing the certain answers on larger scenarios.
Data exchange and query answering scenarios aim to test the
performance of computing the target instance and of answering
queries over the target instance in data exchange. These scenarios vary the data size and the complexity of the dependencies to
simulate different workloads.

5.1

that are consistent with a collection of TGDs and EGDs, and in fact
the chase failed on all instances initially produced by TOX GENE
so this problem is not merely hypothetical. Thus, we developed a
simple instance repair tool. Given a set of dependencies Σ and an
instance I on which the chase of Σ fails, the tool proceeds as follows. First, it computes the chase of Σ and I without the unique
name assumption: when equating two distinct constant values, one
value is selected as representative and the other one as conflicting,
the latter is replaced with the former, and the chase continues. Second, the tool removes from I all facts containing a conflicting value
encountered in the previous step. The chase of Σ and the subset of I
produced in this way is guaranteed to succeed. This strategy proved
very effective in practice: on average it removed slightly more than
1% of the facts from I, and so the size and the distribution of the
source instance remain largely unaffected.
3. The target TGD generator. In addition to generating large
source instances, generating scenarios with a significant number
of s-t TGDs, target TGDs, and target EGDs was critical for adequately evaluating the performance of the tested systems. One of
our goals was to push the systems to their limit by testing them on
deep chase scenarios that generate very large instances using long
chase sequences. To this end, we developed a custom target TGD
generator that can generate weakly acyclic TGDs while controlling
their depth and complexity. In our experiments we used the generator to develop scenarios from scratch, but the generator can also
be used to increase the difficulty of existing scenarios.
Our generator is based on our earlier approach [22] for generating linear dependencies, which we extended to support an arbitrary
number of body atoms. The generator creates a predicate name
space of a certain required size, and it uniformly chooses a (parameterized) number of predicates to populate a conjunction of atoms
(which can become a query or a dependency). A parameter is used
to control the maximum number of repeated relations in this formula, and another parameter is used to determine the arity of the
atoms. The generator can create “chain” conjunctions (the last variable of an atom is joined with the first one of the next atom), “star”
conjunctions (the first variable of all atoms also occurs in a designated “center” atom that has a different arity than the other atoms),
and “random” ones in which variables are chosen out of a variable
name space and are distributed in a conjunction (the larger the variable name space, the fewer joins are introduced). Each dependency
is generated by first creating a conjunction and then selecting the
subset of atoms that make up the head. The size of the dependencies’ bodies can be fixed; for example, this parameter was 1 in our
D EEP scenarios, thus producing linear TGDs. The generator maintains a weakly-acyclic dependency graph and tests each generated
dependency against the ones there were created previous; if the
newly created dependency violates weak acyclicity, it is discarded
and a new dependency is produced.

Test Infrastructure

1. The common format. In their “native” versions, the systems
from Section 4 take a wide range of input formats. For example,
RDBMS-based systems expect the source instance to be preloaded
into a relational database, whereas RAM-based systems typically
read their input from files. Moreover, the structure of the inputs
varies considerably; for example, RDF OX expects data to be represented as triples, and E expects data to be encoded as first-order
sentences in the TPTP format. The translation between various input formats is often straightforward, but in certain cases (e.g., if
schema shredding or the transformation of TGDs is required) it involves choices that can significantly affect a system’s performance.
To allow for an automated and fair comparison of all systems regardless of their implementation details, we standardized the input
structure for all systems. Theorem-proving formats such as TPTP
can express first-order sentences and can thus represent dependencies and queries, but such formats are difficult to read for humans
and so we found them inappropriate for C HASE B ENCH. We thus
developed our own “common format” for describing all parts of a
scenario (i.e., the schema description, the source instance, the dependencies, and the queries). We also wrote a parser for the common format, which was used to develop wrappers for the systems.
The wrapper was provided by the system designers whenever possible, but for E, DLV, and P EGASUS we developed the wrappers
ourselves. Our tests required each system to read the scenarios in
the common format, so our test results cover all times necessary to
parse and transform the inputs.
2. The instance repair tool. Generating scenarios with EGDs
and large source instances is complex since, due to the size of the
source instance, it is difficult to ensure that no EGD chase step fails.
For example, TOX GENE does not necessarily generate instances

42

4. The query generator. To evaluate the performance of computing certain answers, nontrivial queries over the target schema
are required. Existing benchmarks such as LUBM [21] come with
several manually curated queries, which we incorporated into our
scenarios. To obtain queries for scenarios where the target schema
is automatically generated, we developed a new query generator.
The tool is given a desired number of joins in the output query
NJ , the desired number of selections conditions in the output query
NS , the percentage of attributes that will be projected P, and a set
of tables of the target schema. It first identifies a set of joinable
attribute pairs from the tables of the source and target schema by
analyzing the dependencies. Two attributes are considered joinable
if they join either in the head or the body of the rule; moreover, if a
variable is passed from the body to the head of the rule, then the attributes that map to these variables are considered joinable as well;
and finally, the joinable attributes are transitively closed. Next, the
algorithm iteratively creates NJ join conditions by choosing at random a table T and attribute a from the schema and then choosing a
random table and attribute that is joinable with T.a. The algorithm
then proceeds analogously to create NS selections by first choosing an attribute that does not appear in a join condition and then
a introducing randomly selected value from the attribute’s domain.
Finally, the algorithm randomly projects P attributes. By focusing on joinable attributes, the algorithm is more likely to produce a
query with a nonempty output, but does not guarantee it; therefore,
we discarded and regenerated each query that produced an empty
output on the target instance as long as needed.
5. The homomorphism checker. Checking correctness of computing certain query answers is easy: certain answers are unique for
the query so the systems’ outputs can be compared syntactically.
Checking correctness of the chase is more involved since the result
of the restricted chase is not unique and, even with the unrestricted
Skolem chase, it is unique only up to the renaming of Skolem functions. Hence, to verify the correctness of the systems, we developed a tool that can check the existence of homomorphisms, mutual homomorphisms, and isomorphisms between instances. The
tool enumerates all candidate homomorphisms using brute force,
so it can be used only on relatively small instances (i.e., few thousands facts). Consequently, we designed our correctness scenarios
so that they produce small solutions.

5.2

tions such as joins over the source schema, vertical partitioning,
and self-joins both in the source and in the target schemas. The
scenarios cover standard examples from some of the prior papers
and surveys on the chase (e.g., [32, 27]), including cases where
TGDs and EGDs interact, where the chase fails, and where various
acyclicity conditions are used to ensure chase termination.
b. Manually curated scenarios. Our scenarios from this family
are based on the D OCTORS data integration task from the schema
mapping literature [17]. These scenarios are relatively small in
terms of the number of relations, attributes, and dependencies (cf.
Table 2), but we believe that they represent a useful addition to
the benchmark for two reasons. First, these scenarios are based
on schemas inspired by real databases about medical data. Second, they simulate a common use case for data exchange: take
two databases from the same domain but with different schemas
and bring them to a unified target representation. We used TOXGENE to generate instances of the source schema of 10 k, 100 k,
500 k, and 1 M facts. D OCTORS contains EGDs that refer to more
than one relation in the body, which cannot be handled by all systems in our evaluation. Hence, we also generated a simplified version, called D OCTORS FD, that contains only EGDs corresponding
to functional dependencies. Consequently, the manually curated
family contains eight scenarios. We also used the query generator
described in Section 5.1 to generate nine queries covering most of
the possible joins among the three target relations.
c. LUBM scenarios. LUBM [21] is a popular benchmark in the
Semantic Web community. It does not simulate a data exchange
task, but it is useful as it comes with nontrivial target TGDs and
queries designed to test various aspects of query answering. Using
the LUBM data generator we produced instances with 90 k, 1 M,
12 M, and 120 M facts and transformed them as follows.
• The LUBM generator produces data as RDF triples, which
we converted into a source instance using vertical partitioning: a triple hs, p, oi is transformed into a unary fact osrc (s)
if p = rdf :type, and into a binary fact psrc (s, o) otherwise.
• For each unary relation osrc from the previous step, we added
the s-t TGD ∀x osrc (x) → o(x), and similarly for each binary
relation. Thus, the s-t TGDs simply copy the source instance
into the target instance.
• The dependencies in LUBM are encoded using an ontology,
which we converted into target TGDs using vertical partitioning and the known correspondences between description
logics and first-order logic [5].
• We manually converted all SPARQL queries into CQs; this
was possible for all queries.
As a consequence of these transformations, the source and the target schemas of LUBM contain only unary and binary relations.
Also, the source instance of LUBM-120M is much larger than any
other source instance in our scenarios.
d. I B ENCH scenarios. I B ENCH [4] is a tool for generating dependencies whose properties can be finely controlled using a wide
range of parameters. For our purpose, we selected two existing sets
of dependencies [4, Section 5] that consist of second-order TGDs,
primary keys, and foreign keys. To obtain dependencies compatible with most of our systems, we modified a parameter in the
I B ENCH scripts to generate ordinary s-t TGDs instead of secondorder TGDs, thus obtaining two scenarios of the I B ENCH family.
• STB-128, derived from an earlier ST-benchmark [3], is the
smaller scenario of the family.
• O NT-256, a scenario motivated by ontologies, has several
times larger source instance.

Test Scenarios

Our benchmark consists of a total of 23 scenarios, each comprising a source and target schema, a source instance, a set of dependencies, and possibly a set of queries; all dependencies in all
scenarios are weakly acyclic. We classify the scenarios into five
families, as shown in Table 2. The first family contains six small
scenarios for testing correctness of data exchange, whereas all other
families are aimed at testing the performance of computing the target instance and the certain answers. The I B ENCH and the LUBM
families were derived from the well established benchmarks in the
database and the Semantic Web communities, respectively. Finally,
we developed the M ANUALLY C URATED and the D EEP families
ourselves to test specific aspects of the chase. We identify each
scenario using a unique identifier; for D OCTORS and LUBM scenarios, the identifier is obtained by combining the scenario name
with the source instance size, such as LUBM-90k. We discuss
next the main features of our scenario families.
a. Correctness tests. As we explained in Section 5.1, our homomorphism checker can handle only small instances. We thus
prepared six scenarios that produce small chase results, while aiming to cover exhaustively the different combinations of various features. All scenarios contain s-t TGDs and test different combina-

43

Family

C ORR .
C ORR .
C ORR .
C ORR .
C ORR .
C ORR .
M AN .C.
M AN .C.
LUBM
I B ENCH
I B ENCH
D EEP
D EEP
D EEP

Scenario
Name

E MP D EPT
T GDS - A
T GDS - B
E GDS
T GDS E GDS - A
T GDS E GDS - B
D OCTORS FD
D OCTORS
LUBM
STB-128
O NT-256
D EEP 100
D EEP 200
D EEP 300

Source Schema
Rel
Attr

Target Schema
Rel
Attr

s-t
TGDs

Tot

1
1
2
1
1
1
3
3
30
111
218
1000
1000
1000

2
5
3
1
5
5
5
5
74
176
444
299
299
299

1
2
5
1
3
6
5
5
30
128
256
1000
1000
1000

2
5
2
0
5
5
0
0
106
39
273
100
200
300

3
3
8
2
3
3
24
24
76
535
1210
5000
5000
5000

5
12
9
2
12
12
17
17
179
832
1952
1495
1495
1495

t TGDs
Inc.Dep

2
5
1
0
5
5
0
0
91
39
273
50
100
150

EGDs
Tot
FDs

Qrs

Source Instance Facts

0
0
0
1
4
4
8
10
0
193
921
0
0
0

0
0
0
0
0
0
9
9
14
20
20
20
20
20

1
1
8
3
4
80
10 k, 100 k, 500 k, 1 M
10 k, 100 k, 500 k, 1 M
90 k, 1 M, 12 M, 120 M
150 k
1M
1k
1k
1k

0
0
0
1
4
4
8
8
0
193
921
0
0
0

Table 2: Summary of the test scenarios
Test Setup. All systems apart from E and DLV (as we discuss
shortly) were required to perform the following steps:
1. load the source instance from .csv files on disk;
2. load the dependencies in the common format;
3. run the chase of the s-t TGDs;
4. run the chase of the target dependencies;
5. save the target instance to .csv files on disk; and
6. run each query and save its results to .csv files on disk.
For each scenario, each system was allowed three hours to complete
all of these tasks; if the system ran out of time or memory, we count
the scenario in question as failure and report no results (not even
the loading times). In order to analyze the relative contribution
of all the steps, we captured the times for all steps independently
whenever the systems supported such time measurements. We also
repeated the experiments with s-t TGDs only.
Figure 1 shows (I) the chase execution times (steps 3+4), (II) the
source import and target export times (steps 1 + 5), (III) the s-t
TGDs chase times (step 3), (IV) the total chase times (steps 1 + 2 +
3 + 4), and (V) the query execution times (step 6) for the scenarios where the results vary significantly between different system
configurations. Figure 1 shows the results only for the 1-parallel
Skolem chase for L LUNATIC and the unrestricted Skolem chase for
RDF OX. All results for all scenarios and all chase variants supported by the systems are given in the appendix. The data sizes
in the STB-128 and O NT-256 scenarios do not vary so we report
our results as a single bar chart; for all other scenarios we use line
charts that show scaling with the data size. Test coverage of different systems is reported separately.
E and DLV do not report any intermediate times so we treated
them differently. The reasoning strategy of DLV is closely related
to the Skolem chase so we report its “total chase time”; however, we
found no reasonable analogous time for E. Moreover, to compare E
and DLV with the other systems, in Figure 1.VI we show the query
evaluation times measured in a different way: (i) for DLV and E,
we report the total time needed to answer all queries; and (ii) for all
other systems we report the import and chase times multiplied by
the number of queries (thus compensating for the fact that, for each
query, DLV and E load the dataset and perform reasoning from
scratch), plus the sum of all query times.
A “db-to-db” test protocol, where the source and the target instances are stored in an RDBMS, might have been more natural for
RDBMS-based systems. Our “file-to-file” test protocol, however,
has two advantages. First, even in RDBMS-based systems importing the data may require indexing or building supporting data structures (e.g., we discuss the dictionary encoding in Section 7), which

For both of these scenarios, we used the integration of I B ENCH and
TOX GENE to generate 1 k facts per source relation. Next, we used
our instance repair tool from Section 5.1 to ensure that the chase
does not fail. Finally, we generated 20 queries for each scenario
using our query generator.
e. The D EEP scenarios. The final family of scenarios was developed as a “pure stress” test. We used our target TGD generator
to generate three scenarios with 1000 source relations, 299 target
relations, 1000 linear s-t TGDs, and increasing numbers (100, 200,
and 300) of linear target TGDs. Moreover, to generate the source
instance, we globally fixed a substitution σ that maps each variable x ∈ Vars into a distinct constant value σ (x) ∈ Const; then, for
each linear s-t TGD with R(~x) in the body, we added σ (R(~v)) to
the source instance. Thus, all source instances contain just one fact
per relation; however, the TGDs are very complex so they produce
over 500 M facts on the largest D EEP 300 scenario.
The TGDs were taken from previous work [22] and they have
the following structure. All TGD heads have three relations joined
in a chain. Each atom has arity four and each dependency can have
up to three repeated relations. The three head predicates and the
body predicate have been chosen randomly out of a space of 300
predicates. We generated 10% of all TGDs from a smaller subset
of predicates of size 60. Also, around 10% of the s-t TGD heads
were constructed by getting the body and two (out of three) head
atoms of a target TGD. This causes some target TGDs to almost
map entirely to these particular s-t TGDs. After generating each
dependency, a weak acyclicity test was used to discard the dependency if acyclicity was violated.

6.

SYSTEM COMPARISON

We ran a total of 40 tests per system: 6 correctness tests, 17 chase
computation tests, and 17 query answering tests. Our correctness
tests revealed a number of errors in the systems, most of which
were corrected by the system authors during the evaluation period.
All systems eventually passed all correctness tests, except for P E GASUS which failed two correctness tests but could not be updated.
Complete results of the performance tests, including a breakdown
of all times, are given in the appendix and on the benchmark Web
site. Figure 1 summarizes some results that we discuss next.
Hardware configuration. We ran all tests on a server with six
physical 1.9 GHz Xeon v3 cores, 16 GB of RAM, and a 512 GB
SSD, running Ubuntu v16. Our configuration is thus not very far
from that of a high-end laptop.

44

0

Time	(sec)

100

10

10

0,1
90000
1000000

500000
#Source	Tuples

100

100

1

1
0,1

Ont-256

1000

1
900000 9000000 90000000
#Source	Tuples

e.	Deep	- Import+Export	Time

1000

Time	(sec)

Time	(sec)

10

d.	iBench	- Chase	Time
1000

STB-128

100

100

c.	Deep	- Chase	Time

10000

1000

1000

Time	(sec)

b.	LUBM	- Chase	Time

10000

Time	(sec)

a.	DoctorsFD	- Chase	Time

10000

10

10

1

100

200
#T-TGDs

300

1

Llunatic

0,1

RDFox

100

I.	Chase	execution	times

STB-128

100

1

0,1

0,1
0

500000
#Source	Tuples

1000000

0,01
90000

1000

1000

10

10

1
90000

1

900000 9000000 90000000
#Source	Tuples

100

100

100

1

10000

Time	(sec)

10

10

Time	(sec)

Time	(sec)

Time	(sec)

1

1

0

500000
#Source	Tuples

1000000

0,01
90000

10

10000

1000000

200
#T-TGDs

V.	Query execution	times

300

200

100

10

100

100

1000

100

1

0,01
900000 9000000 90000000
#Source	Tuples

90000

1000

0,1

0,1
0,1

10000

100

100
10

300

o	.Query	Answering	Time	- Deep

n.	Query	Answering	Time	- LUBM

m.	Deep	- Query	 Time

1000

200
#T-TGDs

(Import	+	Chase	+	Export)

Time	(sec)

l.	LUBM	- Query	Time

1000

100

IV.	Total	times

Time	(sec)

k.	DoctorsFD- Query	Time

10
1

900000 9000000 90000000
#Source	Tuples

III. Source-to-target chase	execution	times

100

j.	Deep	- Total	Time

10000

Ont-256

1000

10

Time	(sec)

Time	(sec)

Time	(sec)

100

i.	LUBM	- Total	Time	

h.	iBench	- ST-TGDs	Time
10000

1000

Time	(sec)

g.	LUBM	- ST-TGDs	Time

f.	Doctors	- ST-TGDs	Time

300

II.	Import and	export	times

(including	 source-to-target	and	target	dependencies)

1000

200
#T-TGDs

10
1

1
DLV

Graal

Llunatic

RDFox

VI.	Query	answering	times

(total	time	needed	to	answer	all	queries	after	the	chase)

Figure 1: Experimental results for scenarios where times vary significantly between system configurations
can incur overheads; thus, reporting the import and the chase times
separately allows us to investigate these issues. Second, reporting
the cumulative times allows us to take DLV and E into account.
Test coverage. On some tests, certain systems did not support
all required features, they terminated with an exception, or did not
finish in three hours; such tests are not shown in Figures 1. Table 3
summarizes test coverage for all tested systems. For each system
and each of the three test categories, we report the number of tests
that the system was applicable to and the number of successfully
completed tests; furthermore, we report the causes of failures (if
any). Note that for most systems a failure in data exchange on a
test scenario implies failure in the corresponding query answering
tests, and we count such failures only once in the table.
As one can see, L LUNATIC was the only system that completed
all tests, closely followed by RDF OX which ran out of memory
on LUBM-120M and D EEP 300. DEM O and the systems that use
chase for query reformulation exhibited low coverage; for example, PDQ completed only five out of the 17 data exchange tests.
C HASE F UN completed all tests that it was applicable to.

7.

that it covers a range of scenarios, which allowed us to answer the
questions we posed in Section 1. Specifically, we could observe the
behavior of the chase on a diverse set of systems and inputs, which
allowed us to derive many general conclusions about the tradeoffs
in implementing different chase variants (see Section 3). Moreover,
we could compare the effectiveness of the systems specifically designed for the chase with that of the systems that tackle related
problems. We summarize our findings in the rest of this section.
Restricted vs. unrestricted vs. parallel chase. A major decision facing a chase system designer is whether to implement the
active trigger check (line 6 of Algorithm 1). By analyzing the performance of different chase variants implemented in L LUNATIC
and RDF OX on our scenarios (Table 4 shows some, and Table 7
in the appendix shows all results), we obtained several important
insights. Interestingly, the tradeoffs associated with this question
are quite different for RDBMS- and RAM-based systems.
Implementing the restricted chase in RDBMS-based systems is
quite challenging. Triggers are retrieved in such systems using SQL
queries, so it is natural to embed the active trigger check into these
queries. For example, query (13) attempts to retrieve the active
triggers for TGD (3) from Example 1.

LESSONS LEARNED

Our tests should not be misunderstood as aiming primarily for a
performance competition: the tested systems were designed under
completely different assumptions, often targeting tasks currently
not covered by our benchmark. A key feature of our benchmark is

SELECT DISTINCT R.a FROM R WHERE NOT EXISTS
(SELECT * FROM R AS R1, A AS A1, A AS A2
WHERE R.a=R1.a AND R1.b=A1.a AND R.b=A2.a)

45

(13)

System

Tests Run

Corr.

Chase

Tests Completed

Query

Total

Corr.

Chase

Failures

Query

Total

Timeouts

Memory

0
0
4
17
5
0

5
9
11
40
16
4

0
11
0
0
12
17

0
3
3
0
0
0

5
3
15

13
9
36

2
14
0

0
0
2

Explicit chase implementations

C HASE F UN
DEM O
G RAAL
L LUNATIC
PDQ
P EGASUS

1
6
3
6
6
6

4
17
7
17
17
17

0
0
7
17
17
0

5
23
17
40
40
23

DLV
E
RDF OX

3
6
6

7
0
17

7
17
17

17
23
40

1
6
3
6
6
4

4
3
4
17
5
0

Chase-related systems

3
6
6

5
0
15

Table 3: Test coverage

L LUNATIC 1-Parallel
L LUNATIC Unrest
L LUNATIC Rest
RDF OX Unrest
RDF OX Rest

Ch.Time
2.67
6.37
2196.00
0.19
0.21

LUBM-90k
# Facts Query T.
141,213
0.29
177,738
0.31
141,213
0.23
177,738
0.06
141,213
0.06

Ch.Time
19.25
33.35
7521.00
15.23
24.02

D EEP 200
# Facts Query T.
902,636
0.41
926,324
0.44
893,990
0.36
926,324
0.03
892,516
0.03

Table 4: Results for variants of the chase
RDBMSs, however, evaluate queries fully before any updates, so
query (13) cannot detect that applying a chase step to one trigger
makes another trigger inactive (as in Example 1). To properly implement the restricted chase, we must check triggers one-by-one
using independent queries; for example, we can add LIMIT 1 to
(13) and run one query per trigger, but this is very slow on large
instances in most RDBMSs. Table 7 confirms this: with both the
1-parallel and the unrestricted Skolem chase, L LUNATIC runs orders of magnitude faster than with the restricted Skolem chase, and
some of the latter tests timed out.
Query (13), however, can be used to implement the 1-parallel
chase, which can even eliminate a common source of overhead: by
combining DISTINCT and the active triggers check, the query never
produces duplicate answers, so separate duplicate elimination is not
needed. Indeed, as Table 7 shows, L LUNATIC is faster with the 1parallel Skolem chase than with the unrestricted Skolem chase.
In contrast, RAM-based systems can more efficiently interleave
queries with updates, which can make the active triggers check easier. For example, RDF OX identifies active triggers using a subquery similar to (13), but its optimized RAM-based indexes [30]
can efficiently answer the NOT EXISTS subqueries while taking into
account the result of all concurrent updates. Thus, as Table 7 shows,
the performance of the restricted and the unrestricted Skolem chase
in RDF OX differs by only a couple of seconds.
Solution sizes and getting to the core. Another question to consider is the impact of the chase variant on the size of the universal
solution. Our benchmark again allowed us to investigate this issue:
Table 4 shows the solution sizes obtained by the chase variants in
L LUNATIC and RDF OX (Table 7 in the appendix shows all results).
As one can see, solutions produced by the restricted chase can be
between 4% and 21% smaller than those produced by the unrestricted chase; however, we did not observe any significant impact
on the performance of query answering. Thus, we conclude that
the choice of the chase variant can be mainly driven by the ease of
implementation, rather than the solution size.
An interesting question is whether computing the core can further reduce solution sizes. To this end, we ran DEM O to compute
the core of the universal solutions for scenarios in our benchmark.
DEM O computed the core for D OCTORS-10k and LUBM-90k; the

computation failed on all other scenarios, and the core was in both
cases of the same size as the result of the restricted chase. We were
not able to test the impact of computing the core on most of our
scenarios: as we discuss in Section 8, computing the core of large
instances is an important open problem. We could, however, answer this question partially: a set Σ of s-t TGDs can be rewritten
into a set Σ0 such that the restricted chase of Σ0 returns the core of
the universal solution for Σ [28]. We ran this experiment on the
D OCTORS scenario and only s-t TGDs; full results are shown in
Table 8 in the appendix. On the 10k and 100k scenarios, the core
target instances were 18% smaller then the ones produced using unrestricted Skolem chase, suggesting that scalable methods for core
computation in a more general setting could be practically relevant.
Implementing EGDs. In order to apply the EGDs, a system
typically first retrieves and deletes all affected facts, applies µ, and
inserts the result back into the instance. These operations require
mass updates that are much more expensive in an RDBMS-based
than a RAM-based system, which makes supporting EGDs in an
RDBMS very challenging. Our benchmark results offer evidence
for this observation: the chase times of L LUNATIC were considerably higher than of RDF OX on the O NT-256 scenario, which had
the largest and most complex set of EGDs (see Figure 1.I.d).
Representing labeled nulls. Chase systems must consistently
distinguish labeled nulls from constant values, which turned out to
be a source of complexity in RDBMS-based systems. We noticed
two common solutions to this problem.
RDBMS-based systems represent labeled nulls using a suitable
encoding, which is often string-based; for example, string values
are encoded in query (10) using the '_Sk_' prefix. Each attribute is
thus represented using just one relation column, so join conditions
are expressed naturally. Nevertheless, value decoding requires pattern matching; for example, to compute certain answers, L LU NATIC filters string-typed labeled nulls using NOT LIKE '_Sk_' in
the WHERE clause. This is a considerable source of overhead, for
two reasons. First, pattern matching is costly. Second, this kind of
conditions are not handled nicely by the query planner: as usual,
the optimizer pushes selections down towards the scans, but this
forces the database engine to apply pattern matching to most facts
in the target relations. In most cases, filtering out labeled nulls as a

46

last step of the query would be faster, since the number of facts to
analyze is much smaller. This effect can be observed in the query
times of L LUNATIC on D EEP 300, one of our very large scenarios:
the system answers Query 16 in 147 s, but of these, 118 s (80%) are
used to filter out labeled nulls; similarly, Query 20 takes 44 s, of
which 30 s (68%) are used to filter out labeled nulls.
There are no obvious solutions to this issue. One could implement an alternative query execution strategy that forces the query
planner to materialize the intermediate result of the query and then
filter labeled nulls at the very end, but this incurs a materialization overhead. An alternative is to adopt a multi-column representation of labeled nulls. For example, DEM O represents labeled
nulls using three columns per attribute of the target relation: one
boolean column determines whether the attribute contains a constant value or a labeled null, one column stores constant values,
and one column stores labels of labeled nulls. This cleanly separates labeled nulls from constant values; however, it triples the size
of the database, and it greatly complicates join conditions, which
also often confuses the query optimizer.
In summary, our benchmark allowed us to examine the drawbacks of both approaches, to the point that we consider the development of new, performance-oriented representations of labeled
nulls an interesting open research question.
Query execution and query characteristics. All systems that
successfully computed the chase also successfully evaluated all relevant queries. Most queries can be answered very quickly (typically in under 1 s). Query evaluation was slowest on D EEP 300
and LUBM-120M because the target instances of that scenario are
much larger than in the case of other scenarios.
We analyzed the queries in our benchmark to detect a possible
correlation between execution times and various complexity parameters, such as the number of variables in the head and the body,
the number of joins and so on; the parameters for all queries are
shown on the benchmark Web site. We observed no clear correlation between the query parameters and the query answering times.
Moreover, we found removing duplicates and filtering out labeled
nulls to be significant sources of overhead for query answering, and
these have no clear connection to the shape and size of the query.
Dictionary encoding. Columnar databases often compress data
using a dictionary encoding, which can be applied in the chase setting using the following steps:
• one fixes an invertible mapping e of values to integers;
• the set of dependencies Σ and the input instance I are encoded as Σe = e(Σ) and Ie = e(I), respectively;
• the encoded chase Je of Σe and Ie is computed; and
• Je is decoded as J = e−1 (Je ) by inverting the mapping.
Clearly, J is the chase of Σ and I. This process improves performance in a number of ways. First, the encoded instance is usually much smaller than the original. Second, comparing integers is
faster than comparing strings. Third, data access structures such as
indexes are smaller so joins are faster. Fourth, dictionary encoding removes a problem specific to the Skolem chase: target TGDs
may produce very “deep” Skolem terms that can be expensive to
manage, but dictionary encoding reduces the depth of such terms.
DLV, G RAAL, and RDF OX all use a variant of this idea as they
load the input, and E achieves similar benefits using term indexing [35]. Dictionary encoding is less common in RDBMS-based
systems, but it is even more useful as it reduces the amount of
data to be transferred from secondary storage, and we were again
able to evaluate the impact of this optimization using the benchmark. To this end, we ran L LUNATIC with and without the encoding on LUBM-120M and D EEP 300. To isolate the impact of the

encoding, the source instance was available (but not encoded) in
the RDBMS. First, we measured the chase time on the unencoded
instance. Second, we measured the total time needed to encode the
source instance, run the chase, and decode the solution. Despite
the overhead of the encoding process, the execution with dictionary encoding was 46% faster on LUBM-120M and 58% faster on
D EEP 300. In our performance comparison, L LUNATIC was configured to turn on the encoding on all scenarios with target TGDs.
Chase vs. first-order theorem proving. E answered all queries
in less than a minute on the D OCTORS-10k and D OCTORS FD-10k
scenarios, but it was not as efficient on the remaining scenarios (all
times are given in the appendix): it took more than an hour for
D OCTORS-100k and D OCTORS FD-100k, and it ran out of memory
on LUBM-1M, LUBM-12M, and LUBM-120M. Nevertheless,
although E was not specifically designed for answering queries
over dependencies, it could still process nontrivial scenarios. Note
that the Skolem chase is actually an instance of the theorem proving calculus used in E, so the performance gap between E and the
other systems is most likely due to the generality of the former.
This generality can actually be beneficial in some cases. As Figure 1.VI.o shows, E performed very well on D EEP 100 by answering each query in several seconds, so we analyzed the saturated
set of clauses produced by E. We noticed that, by a combination
of forward and backward reasoning, the system derived many intermediate clauses (i.e., “lemmas”). Some lemmas were obtained
by composing s-t TGDs with target TGDs to obtain new s-t TGDs,
which introduced “shortcuts” in proofs and thus sped up query answering. In fact, queries over weakly-acyclic linear dependencies
(i.e., dependencies with just one atom in the body, which covers
all D EEP scenarios) can always be answered fully using such an
approach [2]. Thus, E “discovered” this method using a general
first-order theorem proving technique.
Query reformulation vs. query answering. Systems that use
chase to support query reformulation (i.e., P EGASUS and PDQ)
fared badly on all tests. The chase implementation in these systems
is optimized for small instances obtained from the queries being
reformulated, rather than for sizes found in data exchange.
Maturity of the chase. Despite the increasing complexity of the
test scenarios, some consisting of over 1000 dependencies and generating up to 500 M facts, several systems successfully completed
most tests on mainstream hardware (a CPU with 6 cores, 16 GB of
RAM, and a 512 GB SSD). In addition, some systems were able to
complete the chase and answer the queries in the majority of the
tests within a few minutes. Thus, our results suggest that applying
the chase to tasks of nontrivial sizes is practically feasible.

8.

FUTURE CHALLENGES

Our experiments also gave us some insight regarding directions
for future work in the community.
Modular implementations. While a good benchmark should
provide a range of workloads for testing the chase, systems with
a more modular architecture are needed in order to test hypotheses about the performance of chase. A prominent example of this
kind is comparing RDBMS- and RAM-based systems: one would
ideally use a system that can work either in RAM or on top of
an RDBMS, and it would allow one to measure the impact of this
choice independently of the myriad of other implementation factors; however, no such system exists at present. A more modular chase implementation would also be beneficial in practice since
many design choices (including the one above) are appropriate for
some scenarios and not for others.

47

Understanding the chase with EGDs. The handling of EGDs
in the chase is an area in need of much further study, both in terms
of theory (e.g., termination guarantees and static analysis) and implementation. Equality reasoning has been studied extensively in
the theorem-proving community (e.g., [6]), and thus a first step
would be to better understand the applicability of these techniques
to the chase with EGDs.
Computing the core. An open question is to what extent can
computing the core reduce the size of the target instance. We investigated this for s-t TGDs, but to answer this question more generally scalable techniques for computing the core in the presence of
target dependencies are needed.
Alternative approaches to query answering. The chase is often used for computing the certain answers to queries—a task that,
in some cases, can also be tackled using radically different reasoning techniques such as theorem provers. In this paper we took a
first look at comparing the chase to the other approaches, but more
work is needed to further compare and possibly even combine the
chase with the related approaches. The classical chase procedure is
particularly well-suited for use cases where the certain answers for
many queries must be computed for the same instance. For comparison with other approaches, it might be appropriate to consider
a more dynamic setting, as well as possibly develop a set of complexity measures appropriate to such a setting.
Answering queries without materializing the chase. As our
experiments show, the chase can be very large so computing and
maintaining it can be impractical. It can sometimes thus be desirable to answer queries without first materializing the chase. A lot of
work has already been invested in query rewriting, where a query is
transformed to take into account the effect of dependencies so that
the transformed query can be evaluated directly over the source instance. This, however, is feasible only with simple dependencies
such as linear TGDs, and answering queries in cases where query
rewriting is not applicable remains an open problem. Goal-oriented
query answering techniques from Datalog such as magic sets [9]
provide a promising starting point for this investigation.
Ordering of steps. We found considerable evidence in our experiments that ordering chase steps is important in practice; for
example, the good performance of C HASE F UN is due to its careful
ordering of EGD and TGD steps. More research is needed to understand the impact of the ordering: even with TGDs only, a particular
order can make the active triggers check more effective.

9.

consists of a number of mapping tasks, where each task comprises
two schemas, possibly in different data models, and a description
of a transformation between them. In addition, ST-Benchmark provides a generator for mapping specifications themselves. It also
modifies TOX GENE—a generator of schema instances with certain
properties. The tests reported in [3] focus on common transformations on a nested relational model, as opposed to relational data
exchange. The ST-Benchmark suite of tools is no longer available,
so we could not reuse it. However, several pieces of the infrastructure used in the ST-benchmark, such as TOX GENE and I B ENCH (a
successor of the specification generator from the ST-benchmark),
play a prominent role in our work.
I B ENCH [4] is a tool for generating metadata for benchmarking
mapping scenarios. It can generate source and target schemas, s-t
TGDs, and target EGDs. It is publicly available, it provides some
support for data generation via TOX GENE, and it has already been
used for testing schema mapping tools. We complemented I B ENCH
with a number of additional tools, as discussed in Section 5.

10.

CONCLUSIONS

Our work provides the first broad look at the performance of
chase systems. Our contributions include a new benchmark that
comprises test infrastructure and many test scenarios, experimental results for nine prominent systems, and insights about aspects
of the systems’ implementation. We intend to maintain and extend
our infrastructure for benchmarking the chase presented in this paper as new systems appear and as existing systems are improved.
We feel that our work can be easily extended to support new classes
of dependencies and other chase-related tasks, such as query reformulation with respect to dependencies.
We hope that our infrastructure will have an impact beyond the
specific techniques examined here. As mentioned in the introduction, many automated reasoning communities, from SMT solving
to description logics, have invested enormous effort in evaluation
in the past years. In contrast, while the research literature on extending database systems with reasoning capabilities is extensive,
evaluation methodologies are much less developed. A major contribution of this work, beyond its results on the chase, is as a preliminary step in addressing this gap. Evaluation infrastructure is
not just a matter of providing synthetic data and dependency generators: it includes common formats, common test harnesses, and
much more (see Section 5). We hope that some of our infrastructure and methods will spur activity in other evaluation tasks around
reasoning in database systems.
Finally, in this work we took an initial step in comparing reasoning systems produced by the database community with the systems
developed by other related communities.

RELATED WORK

Experimental evaluations of many chase based systems have already been conducted [34, 28, 27, 17, 22, 12]. In many cases we
reused and extended systems from these earlier studies, but our
work differs in several important ways. First, previous studies have
involved a limited number of systems, typically one or two. To the
best of our knowledge, this is the first attempt to compare a large
number of a very different, yet related systems. Second, earlier
studies have considered only a narrow range of scenarios closely
related to the function of the tested systems. We evaluate the systems on over 20 scenarios covering a wide range of assumptions,
testing both correctness and scalability, covering a broad spectrum
of steps related to the chase from data loading to query execution.
Finally, the systems, datasets, and scenarios from the earlier studies
have not been made available to the community—an important step
towards advancing the experimental culture of the community.
Previous efforts that are more similar in spirit to our work include ST-Benchmark [3] and iBench [4]. ST-Benchmark is concerned with evaluating tools for generating schema mappings. It

11.

ACKNOWLEDGMENTS

The work by Benedikt, Motik, and Konstantinidis was funded by
the EPSRC grants PDQ (EP/M005852/1), ED3 (EP/N014359/1),
DBOnto (EP/L012138/1), and MaSI3 (EP/K00607X/1).

12.

REFERENCES

[1] S. Abiteboul, R. Hull, and V. Vianu. Foundations of
Databases. Addison-Wesley, 1995.
[2] F. N. Afrati and N. Kiourtis. Computing certain answers in
the presence of dependencies. Inf. Syst., 35(2):149–169,
2010.
[3] B. Alexe, W.-C. Tan, and Y. Velegrakis. STBenchmark:
towards a benchmark for mapping systems. In VLDB, 2008.

48

[4] P. C. Arocena, B. Glavic, R. Ciucanu, and R. J. Miller. The
iBench integration metadata generator. In VLDB, 2015.
[5] F. Baader, D. Calvanese, D. McGuinness, D. Nardi, and P. F.
Patel-Schneider, editors. The Description Logic Handbook:
Theory, Implementation and Applications. Cambridge
University Press, 2007.
[6] Franz Baader and Tobias Nipkow. Term Rewriting and All
That. Cambridge University Press, 1998.
[7] J.-F. Baget, M. Leclère, M.-L. Mugnier, S. Rocher, and
C. Sipieter. Graal: A toolkit for query answering with
existential rules. In RuleML, 2015.
[8] D. Barbosa, A. Mendelzon, and K. Keenleyside, J.and Lyons.
ToXgene: A template-based data generator for XML. In
SIGMOD, 2002.
[9] C. Beeri and R. Ramakrishnan. On the power of magic. In
PODS, pages 269–283, 1987.
[10] M. Benedikt, J. Leblay, and E. Tsamoura. PDQ: Proof-driven
query answering over web-based data. In VLDB, 2014.
[11] M. Benedikt, J. Leblay, and E. Tsamoura. Querying with
access patterns and integrity constraints. In VLDB, 2015.
[12] A. Bonifati, I. Ileana, and M. Linardi. Functional
Dependencies Unleashed for Scalable Data Exchange. In
SSDBM, 2016.
[13] A. Deutsch, A. Nash, and J. Remmel. The chase revisited. In
PODS, 2008.
[14] A. Deutsch, L. Popa, and V. Tannen. Query reformulation
with constraints. SIGMOD Record, 35(1):65–73, 2006.
[15] R. Fagin, P.G. Kolaitis, R.J. Miller, and L. Popa. Data
Exchange: Semantics and Query Answering. TCS,
336(1):89–124, 2005.
[16] R. Fagin, P.G. Kolaitis, and L. Popa. Data Exchange: Getting
to the Core. TODS, 30(1):174–210, 2005.
[17] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. Mapping and
Cleaning. In ICDE, 2014.
[18] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. That’s All
Folks! LLUNATIC Goes Open Source. In VLDB, 2014.
[19] G. Gottlob and A. Nash. Efficient Core Computation in Data
Exchange. J. of the ACM, 55(2):1–49, 2008.
[20] B. Cuenca Grau, I. Horrocks, M. Krötzsch, C. Kupke,
D. Magka, B. Motik, and Z. Wang. Acyclicity notions for
existential rules and their application to query answering in
ontologies. JAIR, 47:741–808, 2013.
[21] Y. Guo, Z. Pan, and J. Heflin. LUBM: A benchmark for
OWL knowledge base systems. Web Semantics: Science,
Services and Agents on the World Wide Web, 3(2-3), 2011.
[22] G. Konstantinidis and J. L. Ambite. Optimizing the chase:
Scalable data integration under constraints. In VLDB, 2015.
[23] N. Leone, M. Manna, G. Terracina, and P. Veltri. Efficiently
computable Datalog∃ programs. In KR, 2012.
[24] N. Leone, G. Pfeifer, W. Faber, T. Eiter, G. Gottlob, S. Perri,
and F. Scarcello. The DLV system for knowledge
representation and reasoning. TOCL, 7(3):499–562, 2006.
[25] D. Maier, A. O. Mendelzon, and Y. Sagiv. Testing
implications of data dependencies. TODS, 4(4):455–469,
1979.
[26] B. Marnette. Generalized Schema Mappings: From
Termination to Tractability. In PODS, 2009.
[27] B. Marnette, G. Mecca, and P. Papotti. Scalable Data
Exchange with Functional Dependencies. In VLDB, 2010.

[28] G. Mecca, P. Papotti, and S. Raunich. Core Schema
Mappings: Scalable Core Computations in Data Exchange.
Inf. Systems, 37(7):677–711, 2012.
[29] M. Meier. The backchase revisited. VLDB J., 23(3):495–516,
2014.
[30] B. Motik, Y. Nenov, R. Piro, I. Horrocks, and D. Olteanu.
Parallel Materialisation of Datalog Programs in Centralised,
Main-Memory RDF Systems. In AAAI, 2014.
[31] R. Nieuwenhuis and A. Rubio. Paramodulation-Based
Theorem Proving. In Handbook of Automated Reasoning,
volume I. Elsevier, 2001.
[32] A. Onet. The chase procedure and its applications in data
exchange. In DEIS, pages 1–37, 2013.
[33] S. Perri, F. Scarcello, G. Catalano, and N. Leone. Enhancing
DLV instantiator by backjumping techniques. Ann. Math.
Art. Int., 51(2-4):195–228, 2007.
[34] R. Pichler and V. Savenkov. DEMo: Data Exchange
Modeling Tool. In VLDB, 2009.
[35] I. V. Ramakrishnan, R. Sekar, and A. Voronkov. Term
Indexing. In Handbook of Automated Reasoning. Elsevier,
2001.
[36] Stephan Schulz. System Description: E 1.8. In LPAR, 2013.
[37] SMT-LIB. http://smtlib.cs.uiowa.edu/.
[38] B. ten Cate, L. Chiticariu, P. Kolaitis, and W. C. Tan. Laconic
Schema Mappings: Computing Core Universal Solutions by
Means of SQL Queries. In VLDB, 2009.
[39] TPC. http://www.tpc.org/.
[40] TPTP. http://www.cs.miami.edu/~tptp/.

49

APPENDIX
FULL EXPERIMENTAL RESULTS

1

500000
#Source	Tuples

1000000

0,1
0

500000
#Source	Tuples

0

g.	DoctorsFD	- ST-TGDs	Time

1

0,1
0

500000
#Source	Tuples

10000

1000

0,1

0,1
90000

900000 9000000 90000000
#Source	Tuples

q.	Deep	- ST-TGDs	Time

10

100

200
#T-TGDs

300

100

u.	iBench	- Chase	Time
1000

200
#T-TGDs

Time	(sec)

100

10

1000

100

100

10

1

Llunatic
STB-128

1

1

RDFox
Ont-256

10

STB-128

RDFox

STB-128

Ont-256

Ont-256

Time	(sec)

t.	Deep	- Query	Time

1000

10
1

0,1
0,01
200
#T-TGDs

300

100

x.	iBench	- Total	Time	
(Imp+Chase+Exp)

200
#T-TGDs

300

y.	iBench	- Query	Time
10

10
1

Llunatic

900000 9000000 90000000
#Source	Tuples

100

100

1000

Time	(sec)

1000

100

300

w.	iBench	- Import+Export	Time

v.	iBench	- ST-TGDs	Time
10000

s.	Deep	- Total	Time	
(Imp+Chase+Exp)

1
200
#T-TGDs

1

0,01
90000

10

100

300

900000 9000000 90000000
#Source	Tuples

100

0,1

0,1

10

0,1

1
90000

1

1

1000000

o.	LUBM	- Query	Time

1000

1000

10

10

500000
#Source	Tuples

100

10000

Time	(sec)

Time	(sec)

Time	(sec)

100

n.	LUBM	- Total	Time	
(Imp+Chase+Exp)

r.	Deep	- Import+Export	Time

100

1

0

10

900000 9000000 90000000
#Source	Tuples

j.	DoctorsFD- Query	Time

1000000

100

1000

100

500000
#Source	Tuples

1000

0,1
90000

900000 9000000 90000000
#Source	Tuples

1000

1000

0

10000

10

1000000

0,1

0,1

1

0,01
90000

p.	Deep	- Chase	Time

10000

Time	(sec)

Titolo

1

1

m.	LUBM	- Import+Export	Time

100

10

10

1000000

1000

100

100

500000
#Source	Tuples

500000
#Source	Tuples

10

Time	(sec)

1000

0

1000000

l.	LUBM	- ST-TGDs	Time

k.	LUBM	- Chase	Time

10000

0

100

Time	(sec)

1000000

i.	DoctorsFD	- Total	Time	
(Imp+Chase+Exp)

1

Time	(sec)

500000
#Source	Tuples

1000000

10

1

0,01
0

500000
#Source	Tuples

100

0,1

0,1

0,1
0

1000

Time	(sec)

Time	(sec)

1

1000000

10

10

10

500000
#Source	Tuples

h.	DoctorsFD	- Import+Export	
Time

100

100

100

1

1

0,1

1000000

1000

1000

Time	(sec)

0,01

f.	DoctorsFD	- Chase	Time

10000

Time	(sec)

1

Time	(sec)

0

Time	(sec)

1

10

10

0,1

0,1

1

Time	(sec)

Time	(sec)

Time	(sec)

10

100

10

10

e.	Doctors	- Query	Time

100

Time	(sec)

100

100

d.	Doctors	Total	Time	
(Imp+Chase+Exp)

1000

Time	(sec)

1000

c.	Doctors	- Import+Export	Time

100

Time	(sec)

b.	Doctors	- ST-TGDs	Time
1000

Time	(sec)

a.	Doctors	- Chase	Time

10000

Time	(sec)

A.

1

0,1

Llunatic

RDFox

STB-128

Llunatic

Ont-256

STB-128

RDFox
Ont-256

Figure 2: Full charts of the chase experiments
b.	Query	Answering	Time	- Doctors	10k
1000

100
Time	(sec)

Time	(sec)

100

10

c.	Query	Answering	Time	- LUBM
10000

1000

1000

100

10

100

10
1

1
Llunatic

RDFox

E

Llunatic

RDFox

Graal
Llunatic
90000 1000000 12000000

RDFox

Figure 3: Full charts of the query answering experiments

50

10
1

DLV

1
E

d	.Query	Answering	Time	- Deep

10000

Time	(sec)

a.	Query	 Answering	Time	DoctorsFD	10k

Time	(sec)

1000

DLV

E

Graal Llunatic PDQ
100
200

RDFox

Chase Times
Doctors

2
10k
Chase
10000
ChaseFUN
DEMo
DLV
Graal
Llunatic
PDQ
RDFox

686,00

12,63
13,55
0,18

Chase
ST-TGDs
10000
0,35
31,00
0,54
13,71
0,08
12,00
0,11

100k

Import/Exp Total time
10000
10000

Query
10000

Chase
100000

19,56
1220,00
0,81

4,14

691,00

2,37
2,06
0,33

15,78

0,22

0,90

0,10

Chase
ST-TGDs
100000
0,62
1777,00
2,68
1936,96
0,35
388,00
0,46

500k

Import/Exp Total time
100000
100000

Query
100000

Chase
500000

23,70

1,30

34,76

2,03

0,97

3,73

10k

10000
0,46
658,00

1,60
14,50
0,21

Chase
ST-TGDs

Chase
1000000

Chase
ST-TGDs
1000000
2,58

Import/Exp Total time
1000000
1000000

Query
1000000

26,98
3,86

39,82

5,18

55,96

6,57

10,12

5,88

7,53

3,32
1622,00
4,57

Query

Chase

Chase
ST-TGDs

5,11

62,00

10,00

13,01

21,59

13,41

10000
31,00
0,56
13,71
0,07
12,00
0,13

Query

Chase
100000
0,83

10000
0,66
4,09

10000
1,00
665,00

10000

2,73
2,66
0,33

4,74

0,26

0,90

0,10

Chase
ST-TGDs
100000

2,02
1258,00
0,81

1777,00
2,69
1936,96
0,37
388,00
0,46

500k

Import/Exp Total time

Query

Chase

Chase
ST-TGDs

100000
1,14

100000
2,00

100000

500000
1,97

500000

3,04
4,14
1,60

5,59

1,41

3,99

2,03

0,97

3,73

1,77
977,00
2,27

Query

Chase

Chase
ST-TGDs

1m

Import/Exp Total time
500000
3,17

500000
5,00

500000

1000000
3,55

4,32

8,46

5,54

6,48

6,39

10,15

5,85

7,49

3,52
1622,00
4,56

Query

Chase

Chase
ST-TGDs

13,03

1000000

Import/Exp Total time

Query

1000000
5,75

1000000
9,00

1000000

6,64

13,78

11,22

12,67

20,77

13,23

27,05

LUBM
001
Chase
90000
503,00
9,73
2,67
0,19

Chase
ST-TGDs
90000
0,54
127,00
1,78
4,44
0,13
410,00
0,07

010

Import/Exp Total time

Query

Chase

Chase
ST-TGDs

90000

90000

90000

1000000

3,44
3,37
4,03

512,00
3,00
14,90
8,29

0,90
0,29

162,00
10,79

0,68

1,06

0,06

1,12

5

1000000
2,29
19,04
59,02
0,95
1276,00
0,39

100

Import/Exp Total time

12000000
22,17

01k

Import/Exp Total time

1000000

1000000

1000000

12000000

12000000

27,00
13,13

28,00
188,56
24,31

7,44
2,69

107,35

9,47

129,21

237,44

19,56

4,59

6,63

0,37

10,34

3,65

57,03

77,04

3,96

Query

Chase

Chase
ST-TGDs

197,10

12000000

12000000

Import/Exp Total time

Chase
STB-128

10,10
10,73

Chase
ST-TGDs
STB-128
5,46
48,59
160,38
2,16
1088,00
6,65

304,00
1415,53

117,82

1543,17

2739,29

Ont-256

Import/Exp Total time
STB-128
STB-128

Query
STB-128

Chase
Ont-256

39,31

50,08

1,92

257,12

14,22

27,19

0,81

35,27

Chase
ST-TGDs
Ont-256
10,23
118,01
566,46
5,34
1045,00
18,73

6

100

2,98
12,13
11,82
9,49

Chase
ST-TGDs
100
4,73
0,67
0,53
1,90
8,93

Query
Ont-256

108,55

366,18

0,82

40,87

82,71

0,29

Deep
200,00

100,00
Chase

Import/Exp Total time
Ont-256
Ont-256

Import/Exp Total time
100

100

2,00
18,96
106,51
0,47

1,00
4,98
31,83
118,33
10,98

Query

Chase

100

200

0,02
0,17
0,05
0,02

Chase
ST-TGDs

300,00

Import/Exp Total time
200

200

200

300

192,00
19,25

200
4,50
0,65
0,52
1,86

7,61
21,51

22,00
199,61
41,59

0,39
0,41

2805,48

15,23

9,03

4,41

20,96

0,04

300
4,31
0,67
0,52
2,79

Import/Exp Total time
300

300

770,15

3576,85

253,16

8,86

Query Answering Times
Doctors DoctorsFD
10k
10k
399,00
383,00
138,05
38,63
6,96
7,02
LUBM
010
356,00
2479,28
267,31
66,47

DLV
Graal
Llunatic
RDFox

001
31,00
176,32
95,42
9,26

DLV
E
Graal
Llunatic
PDQ
RDFox

Deep
100
200
16,00
184,00
79,00
62,82
3843,99
609,39
783,85
1566,79
212,64
333,78

100
3960,00
2393,79
653,21

Table 6: Full results of the query answering experiments

51

Query

300

Table 5: Full results of the chase experiments

E
Llunatic
RDFox

Query

120000000 120000000 120000000 120000000 120000000
906,51

iBench
STB-128

ChaseFUN
DLV
Graal
Llunatic
PDQ
RDFox

1,64
977,00
2,26

100k

Import/Exp Total time

4

ChaseFUN
DLV
Graal
Llunatic
PDQ
RDFox

Query
500000

DoctorsFD

Chase

ChaseFUN
DEMo
DLV
Graal
Llunatic
PDQ
RDFox

1m

Import/Exp Total time
500000
500000

13,06
2,75
4,06
1,53

3

ChaseFUN
DEMo
DLV
Graal
Llunatic
PDQ
RDFox

Chase
ST-TGDs
500000
1,49

Unsupported
Timeout

214,71

Llunatic	-	1-Parallel
Llunatic	-	Unrestricted
Llunatic	-	Restricted
RDFox	-	Unrestricted
RDFox	-	Restricted

Llunatic	-	1-Parallel
Llunatic	-	Unrestricted
Llunatic	-	Restricted
RDFox	-	Unrestricted
RDFox	-	Restricted

Llunatic	-	1-Parallel
Llunatic	-	Unrestricted
Llunatic	-	Restricted
RDFox	-	Unrestricted
RDFox	-	Restricted

Llunatic	-	1-Parallel
Llunatic	-	Unrestricted
Llunatic	-	Restricted
RDFox	-	Unrestricted
RDFox	-	Restricted

Doctors
100k
#Target	
Query	
Tuples
Time
81.000
1,30
81.000
1,38
81.000
2,83
81.000
0,97
81.000
0,98

Chase	
Time
12,63
15,40
69,12
0,18
0,14

10k
#Target	
Query	
Chase	
Tuples
Time
Time
9.734
0,22
19,56
9.734
0,21
20,69
9.734
0,21 3948,14
9.734
0,10
0,81
9.734
0,11
0,67

Chase	
Time
2,67
6,37
2196,80
0,19
0,21

LUBM
001
#Target	
Query	
Chase	
Tuples
Time
Time
141.213
0,29
10,79
177.738
0,31
32,99
141.213
0,23
177.738
0,06
1,12
141.213
0,06
0,94

Chase	
Time
10,10
34,04

iBench
STB-128
#Target	
Query	
Chase	
Tuples
Time
Time
1.918.178
1,92 257,12
1.918.217
1,40 307,13

Ont-256
#Target	
Query	
Tuples
Time
5.673.830
0,82
5.674.103
0,73

1.918.217
1.918.178

5.674.103
5.673.830

10,73
10,87

Chase	
Time
12,13
15,89
22,23
9,49
18,86

0,81
0,78

35,27
36,83

Deep
100
#Target	
Query	
Chase	
Tuples
Time
Time
18.386
0,17
19,25
19.537
0,22
33,35
18.347
0,17 7521,92
19.537
0,02
15,23
18.385
0,02
24,02

Chase	
Time
34,76
44,91
3,73
2,92

500k
#Target	
Query	
Tuples
Time
397.000
5,18
397.000
5,60
397.000
397.000

5,88
5,75

010
#Target	
Query	
Tuples
Time
1.779.697
2,69
2.246.699
1,69
2.246.699
1.779.697

0,45
0,42

0,29
0,30

200
#Target	
Query	
Tuples
Time
902.636
0,41
926.324
0,44
893.990
0,36
926.324
0,03
892.516
0,03

Table 7: Impact of the chase variants

Unrestricted
Chase	Time

10k
100k

0,17
1,14

#	Tuples
11.808
97.500

Restricted	Less	Favorable
Chase	Time
79,40
12404,00

#	Tuples
10.208
95.634

Doctors	(S-T	tgds	only)
Restricted	Random
Chase	Time
89,33
11340,00

#	Tuples
9.867
85.168

Table 8: Impact of the core

52

Restricted	Favorable
Chase	Time
87,96
103720,00

#	Tuples
9.734
81.000

Core
Chase	Time
0,25
0,73

#	Tuples
9.734
81.000

Interactive and Deterministic Data Cleaning
A Tossed Stone Raises a Thousand Ripples
Jian He1˚
Enzo Veltri2
Giansalvatore Mecca2
1

Tsinghua University, China

Donatello Santoro2
Guoliang Li1
3˚
Paolo Papotti
Nan Tang4

3
Università della Basilicata, Potenza, Italy
Arizona State University, USA
Qatar Computing Research Institute, HBKU, Qatar
2

4

{hej13, liguoliang}@tsinghua.edu.cn, ppapotti@asu.edu, ntang@qf.org.qa
{enzo.veltri, donatello.santoro, giansalvatore.mecca}@gmail.com
ABSTRACT

t1
t2
t3
t4
t5
t6

We present Falcon, an interactive, deterministic, and
declarative data cleaning system, which uses SQL update
queries as the language to repair data. Falcon does not
rely on the existence of a set of pre-defined data quality
rules. On the contrary, it encourages users to explore the
data, identify possible problems, and make updates to fix
them. Bootstrapped by one user update, Falcon guesses a
set of possible sql update queries that can be used to repair
the data. The main technical challenge addressed in this
paper consists in finding a set of sql update queries that is
minimal in size and at the same time fixes the largest number of errors in the data. We formalize this problem as a
search in a lattice-shaped space. To guarantee that the chosen updates are semantically correct, Falcon navigates the
lattice by interacting with users to gradually validate the
set of sql update queries. Besides using traditional one-hop
based traverse algorithms (e.g., BFS or DFS), we describe
novel multi-hop search algorithms such that Falcon can
dive over the lattice and conduct the search efficiently. Our
novel search strategy is coupled with a number of optimization techniques to further prune the search space and efficiently maintain the lattice. We have conducted extensive
experiments using both real-world and synthetic datasets to
show that Falcon can e↵ectively communicate with users
in data repairing.

Table 1: Dataset Tdrug with drug tests.

1.

INTRODUCTION

High quality data is important to all businesses, and data
cleaning is an important but tedious step. In fact, removing
errors in order to get high quality data takes most of data
analysts’ time [31], and some studies predict a shortage of
people with the skills and the know-how for these tasks [33].
Consequently, the number and variety of users who are
getting close to the data for data quality tasks are destined
to increase, and we cannot assume that only IT sta↵ and
data scientists are in charge of the data cleaning process.
The above requirement poses new and interesting research challenges. Indeed, a large body of the research
has been conducted on rule-based data repairing, which
consists of using integrity constraints to identify data errors [11, 12, 17, 25, 40], and automated algorithms to enforce
these constraints over the data [7, 22, 23, 32, 43]. However,
in the evolving scenario of data cleaning, these approaches
show a serious limitation. Specifically, they assume that
data quality rules are declared upfront by domain experts
who understand the data and write logical formulas or procedural code. Despite many promising results, these systems
have failed short in terms of adoption in industrial tools.
We address the problem of improving the data cleaning
process by involving non-expert users as first-class citizens,
and present Falcon, a novel system for interactive data repairing. Falcon departs from other interactive data cleaning systems [20, 27, 37, 41, 46], since it brings together a simple, user-oriented interaction paradigm with the benefits of
a declarative, deterministic, and expressive data quality language – sql update (sqlu) queries. In fact, the system is
bootstrapped by an update to the data made by the user to
rectify an error; based on that, it infers a set of sqlu queries
that can be used as data quality rules to correct more errors.
We illustrate by example how it works.

CCS Concepts
•Information systems Ñ Extraction, transformation
and loading; Data cleaning;

Keywords
Data Cleaning; Interactive; Deterministic; Declarative
˚

Date
Molecule
Laboratory
Quantity
11 Nov
C16 H16 Cl
Austin
200
12 Nov statinÑC22 H28 F
Austin
200
12 Nov
C24 H75 S6
N.Y.Ñ New York 1000Ñ100
12 Nov
statin
Boston
200
13 Nov
statin
Austin
200
15 Nov
C17 H20 N
Dubai
150

Work partially done while interning/working at QCRI.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

Example 1: Table 1 reports a sample real-world dataset
Tdrug for experiments collected from di↵erent labs. Each
record represents the quantity and date of a test done in

SIGMOD ’16, June 26–July 1, 2016, San Francisco, CA, USA.
c 2016 ACM. ISBN 978-1-4503-3531-7/16/06. . . $15.00
DOI: http://dx.doi.org/10.1145/2882903.2915242

893

a lab over a certain molecule. Errors are highlighted. Consider the following three user updates.
t3 rLaboratorys – “New York”
t3 rQuantitys – 100
3 : t2 rMolecules – “C22 H28 F”
1:
2:

1

T

(from “N.Y.”)
(from 1000)
(from “statin”)

Falcon Table UI

Interactive

There exist multiple interpretations for each update. For
instance, two possible semantics behind 1 could be either
reformatting all “N.Y.” to “New York” as shown in Q1 , or
changing all Laboratory values to “New York” as shown in
Q11 , regardless of their original values.

Rule Engine
3

True/False

Figure 1: Falcon workflow.

Q1 : UPDATE Tdrug SET Laboratory = “New York”
WHERE Laboratory = “N.Y.”;
Q11 :

Query

2

Obviously, Falcon can prune the search space based on the
validation on Q. The loop for steps ∑ and ∏ terminates
when either all usable queries have been identified, or the
user has no more capacity for the current . Afterwards,
the user may go back to step ∂ to inspect another repair.

UPDATE Tdrug SET Laboratory = “New York”;

Similarly, one possible interpretation of 2 , as given in
Q2 , is that it is specific for Molecule and Date. Hence, it is
hard to generalize this update to apply it to other tuples.

Contributions. We present Falcon, a novel interactive
data cleaning system, with the following contributions.
(1) To design data quality rules, we adopt the standard and
deterministic language of sql update statements (Section 2).
We discuss how to organize the search space of candidate
rules as a lattice, and its pruning principles, by leveraging
the properties of the lattice (Section 3).
(2) We devise efficient algorithms for selecting candidate
queries to e↵ectively interact with the user (Section 4).
In particular, in contrast to traditional traversal (one-hop)
based approaches (Section 4.1), we present novel multiplehop search algorithms such that Falcon can accurately discover useful queries in a small number of steps (Section 4.2).
(3) We describe optimization techniques to improve the efficiency of lattice maintenance (Section 5.1). We also propose
closed query sets to compress the lattice so as to improve the
search efficiency (Section 5.2).
(4) Implemented on top of an open-source data wrangling
tool OpenRefine (http://openrefine.org), we have conducted
experiments with real-world and synthetic data to show the
e↵ectiveness and efficiency of Falcon (Section 6).
Section 7 presents related work. Section 8 closes this paper, followed by our agenda for future work.

Q2 : UPDATE Tdrug SET Quantity = 100
WHERE Molecule = “C24 H75 S6 ” AND Date = “12 Nov”;

Update 3 is more interesting. Consider the following
three interpretations with di↵erent e↵ects. Q3 repairs errors in both t2 and t5 . Q13 also repairs both t2 and t5 , but
additionally, it modifies t4 rMolecules to “C22 H28 F”, which is
an erroneous update, since in Boston they test a di↵erent
statin molecule. On the other hand, the tuple-specific query
Q23 only corrects t2 but misses the chance to repair t5 .
Q3 : UPDATE Tdrug SET Molecule = “C22 H28 F”
WHERE Molecule = “statin” AND Laboratory = “Austin”;
Q13 : UPDATE Tdrug SET Molecule = “C22 H28 F”
WHERE Molecule = “statin”;
Q23 : UPDATE Tdrug SET Molecule = “C22 H28 F”
WHERE Molecule = “statin” AND Laboratory = “Austin”
AND Date = “12 Nov” AND Quantity = 200;

From Example 1, one may observe that there might exist
a large number of sqlu queries. Indeed, this large number
is not surprising, as up to thousands of precise and reliable
update queries can be needed in real-world settings, such as
Walmart catalog [14]. However, while an update is a perfect
starting point for the process of inferring the general scripts,
it comes with new challenges in terms of user interactions.
First, the search space for a new update is exponential to
the number of the attributes, and domain experts cannot
manually validate each of these sqlu queries. We have to
assume that a budget (e.g., #-user interactions) is given for
a specific update. Second, the discovery algorithm must be
fast (e.g., able to react in seconds) to enable user interactions. However, each interaction may trigger the update of
data, which makes the search space a dynamic environment.
This dynamic behavior, together with the large search space
and a budget of user capacity, prevents the use of traditional tools for interactive response, such as precomputing
and caching. In order to efficiently manage all potential updates, and e↵ectively interact with users, we propose Falcon, which works as follows.

2.

PROBLEM STATEMENT

We first introduce the rules used to repair data (Section 2.1). We then describe the search space of rules given
one user update (Section 2.2) and formally define the problem studied in this paper (Section 2.3). Finally, we discuss
its associated fundamental problems (Section 2.4).

2.1

SQL Update Queries: Mother Tongue

We adopt a simple and standard language to repair the
database, the language of update statements in sql (sqlu).
An sqlu statement updates records in a table T on attributes A, B . . ., when some conditions hold. In this work,
we restrict the language to the case where updates are done
on one attribute A of table T with only boolean conjunctions:

Workflow. The workflow of Falcon is depicted in Figure 1.
∂ The user examines the data and provides a repair over
table T . ∑ Given , Falcon generates a set of sqlu queries
as rules. It then selects a query Q whose validity is yet
unknown, and asks the user to verify it. ∏ Based on the
user verification on Q to be either True (i.e., valid) or False
(i.e., invalid), if Q is True, it utilizes Q to repair more data.

UPDATE

T

SET

A“a

WHERE

boolean conjunctions

More specifically, each boolean conjunction is of the form
B “ vB , where B is an attribute of table T and vB is
a constant value from the domain of B, e.g., Molecule “
“statin”. Attribute B could also be the attribute to be updated (i.e., B “ A), such as Laboratory in Q1 of Example 1.

894

We shall use the terms sqlu queries and data quality rules
(or simply rules) interchangeably in the following. We will
also treat updates and repairs equally.

to either validate the query as semantically correct, or invalidate it otherwise. Naturally, we want to find all valid sqlu
queries and use them to repair the database. A straightforward strategy is to ask the user to check every possible
query. Of course, this method is rather expensive as there
could be a large number of possible queries, for which we will
use containment relationships among queries to improve the
search of queries (Section 3).
Furthermore, the user normally has limited capacity for
the number of queries he/she can verify. To this end, we
want to find the cost-e↵ective queries to maximize the number of repaired tuples based on the queries validated by the
user, which is formally defined below.

Remark. sqlu queries used in this work are quite di↵erent
from the integrity constraints (ICs) that are widely adopted
by other data cleaning systems, such as functional dependencies [1], conditional functional dependencies [16], conditional inclusion dependencies [7], and denial constraints [12].
ICs are used to capture errors as violations, where one violation is a set of values that is not semantically coherent
when putting together. In other words, ICs do not explicitly
specify how to change data values to resolve violations. In
contrast, sqlu statements explicitly specify how to change
data values, which are thus considered to be deterministic.
The proposed sqlu is powerful enough to support existing
deterministic cleaning languages such as fixing rules [43],
constant CFDs [16], and widely used ETL rules.
Note that in this work we restrict our discussion to conjunctive sqlu queries for three reasons. (1) It is easy for
users to understand, which is important for interacting with
users; (2) It is efficient to reason about the relationship between di↵erent queries; and (3) It is known that queries
with other formulae such as disjunctions or negations can
be rewritten into an equivalent conjunctive formula [1].

2.2

Budget repair problem.
Given a set Q of sqlu
queries, a table T , and a budget B for the number of
interactions the user can a↵ord, the budget repair prob1
lem
î is to select B queries Q from Q, so as to maximize
| QPQ1 ^validpQq“T QpT q|.
Here, validpQq is a boolean function that is T (resp. F) if
Q is a valid query (resp. not), and QpT q represents the set
of repairs of applying query Q over table T .
Observe that in the above problem, given a query Q, the
validity of Q (i.e., validpQq) is unknown, to be verified by
the user. Such a problem is typically categorized under the
framework of online algorithms [3], where one can process
input piece-by-piece in a serial fashion (i.e., the verification
validpQq of some Q), without having the entire input (i.e.,
the value validpQq for each Q in Q) available from the start.

Search Space for One Repair

Consider a repair
: trAs – a1 that changes the value
of trAs from error a to its correct value a1 with a ‰ a1 . We
want to generalize this action so as to repair more errors.
Naturally, there exist multiple queries to interpret this repair . Implicitly, for each query, the SET clause is A – a1 .
Hence we focus on the WHERE clause. Consider a boolean
condition as B “ vB , where B could be any attribute in
relation R. In an open-world assumption, the constant vB
can be assigned from an infinite set of values, which is neither reasonable nor feasible in practice. Instead, we adopt a
closed-world assumption by only using the evidence from tuple t, the tuple that is being repaired. In other words, for a
query Q w.r.t. the above update , if an attribute B appears
in the WHERE condition of Q, then the boolean conjunction
is B “ trBs, which is to bind the constant vB to the value
trBs. As a special query, we consider H as no condition being enforced in the WHERE clause. Stating in another way,
it is to update all A values in T to a1 .
In summary, given a repair trAs – a1 for tuple t in table
T of relation R, the set Q of all rules for such a repair is:
UPDATE

T

SET

A “ a1

WHERE

O✏ine problem. Its corresponding o✏ine variant is the
following. Given as input that whether each query Q in Q
is valid or not is known, how to select B queries from Q to
maximize the number of repaired tuples. The objective of
designing an online algorithm is to get answers as accurate as
the o✏ine problem. It is easy to see that the o✏ine problem
of its online version (i.e., the budget repair problem) is NPhard, which can be readily proved by a reduction from the
maximum-coverage problem [34].
On analogy of what is proved in [5], when the o✏ine variant is NP-hard, there is no efficient algorithm for computing
an optimal solution for its online algorithm. In other words,
when the o✏ine variant is intractable, there is no hope to
find an optimal solution with the cost in a constant factor
of the online variant (a.k.a. a competitive analysis [39]).
However, not all is lost. As will be shown later, we can
organize all queries in a graphical structure, such that when
the user verifies a query Q as valid or invalid, we can even
generate more inputs by computing the validity of queries
Q1 that are related to Q (Section 3). Even better, we devise efficient algorithms to search over the above graphical
structure (Section 4) and empirically show the e↵ectiveness
of the presented strategies (Section 6).

X “ trXs

where X is an arbitrary subset of R, which can range from
the empty set H to all attributes in R (i.e., X “ R). Hence,
there are 2|R| possibilities of X, where |R| is the arity of
relation R. In other words, we can infer 2|R| queries for
each update. Consider update 3 in Example 1, we can
infer 24 “ 16 queries, where three of them are shown as
Q3 , Q13 and Q23 .

2.3

2.4

Fundamental Problems

Let Q` be a set of valid queries w.r.t. one user update.

Termination problem. The termination problem determines whether a rule-based process will stop, given Q` and
an instance T . We can readily verify that no matter in what
order the queries in Q` are executed, the whole process will
terminate, since the execution of each query is deterministic.

Problem Statement

Given a repair, one wants to find the queries that are
semantically correct so as to repair the database.
Valid sqlu query. Given a repair, an sqlu query is valid
if the query is semantically correct. Since we do not know
which queries are valid in advance, we need to ask the user

Conflicting queries. Two queries Q1 and Q2 are conflicting queries if there exists a tuple t1 such that the following

895

DMLQ(1)
˝

two sequences of sql updates will obtain di↵erent results:
(1) Q1 pQ2 pt1 qq, i.e., applying Q2 first to t followed by Q1 ,
and (2) Q2 pQ1 pt1 qq.
Note that, the search space w.r.t. one repair : trAs – a1
is a set Q of queries (Section 2.2), where each query Q P Q is
a way to generalize the action of changing trAs to a specific
value a1 , by considering di↵erent attribute combinations. In
other words, no query Q will change a tuple to a value a2
that is di↵erent from a1 . Hence, conflicting queries will not
be generated in one lattice.

v

DM(2)
‚

w

✏

DL(1)
˝

✏ 

D(3)
`

Determinism problem. The determinism problem asks
whether all repairing processes (with di↵erent repairing orders of the sqlu queries) end up with the same repair, given
Q` and an instance T .
It is easy to verify that, given Q` and T , regardless of the
orders
of the queries in Q` are applied, all data repairs are
î
QPQ` QpT q, where T is the original instance. Hence, any
set of rules is trivially deterministic.

3.

}

'

DML(1) DMQ(2)
˝
‚

✏

t

v

DQ(2)
‚

'

M(3)
Ÿ

DLQ(1) MLQ (2)
˝
˛

*

tv

~

ML(2)
˛

'

*

✏

L(3)
ô

w

*

✏

MQ(3)
Ÿ

*

'

LQ(3)
ô

✏

Q(4)
‹

~ w

H (6)
:

Figure 2: A sample lattice graph.
A set with a partial order is a partially ordered set, or
poset. Hence, Q is a poset on the partial order ® of rule
containment. Moreover, consider any two rules Q and Q1 .
They have a greatest lower bound: the most specific query
that is contained by both Q and Q1 . This query, denoted
by Q^Q1 , is the one w.r.t. attrpQqYattrpQ1 q. Also, they have
a least upper bound: the most general query that contains
both Q and Q1 . This query, denoted by Q _ Q1 , is the one
w.r.t. attrpQq X attrpQ1 q. Therefore, we can organize the
queries in our search space as a lattice.

A LATTICE: FALCON SEARCH SPACE

In this section, we shall present our organization of the
search space, so as to enable both efficient and e↵ective
search over the candidate rules. We start by discussing the
relationship between two data quality rules.
Rule containment. For two rules Q and Q1 , we say that
Q is contained by Q1 (or Q1 contains Q), denoted by Q ®
Q1 , if for all possible database instances T over the input
schema R, the result of QpT q is a subset of the result of
Q1 pT q (i.e., QpT q Ñ Q1 pT q).
Intuitively, the rule containment captures the semantic
relationship among rules. In other words, no matter which
database T is used, Q will update a subset of T tuples that
Q1 will update if Q ® Q1 , since Q is more specific than Q1 .

Query lattice. Given a repair
and a database instance
T , we denote by pQ, ®, T q the corresponding lattice, or simply pQ, ®q when T is clear from the context. Each node in
the lattice corresponds to a query Q P Q. Each directed
edge from node Q to Q1 indicates that Q ® Q1 (Q is contained in Q1 ) and |attrpQq| “ |attrpQ1 q|`1 (with one di↵erent
attribute). Moreover, the a↵ected number associated with
each query is maintained in the lattice (we will discuss how
to compute the number in Section 5.1.2).

Example 2: Consider queries Q3 , Q13 and Q23 in Example 1. It is straightforward to see that both Q3 and Q23 are
contained by Q13 (i.e., Q3 ® Q13 and Q23 ® Q13 ), and Q23 is
contained by Q3 (i.e., Q23 ® Q3 ).
It is readily to verify that the query containment “®” is
a partial order over the set Q of all possible rules, which is
reflexive, antisymmetric, and transitive. More specifically:

Example 3: Figure 2 depicts the lattice for dataset Tdrug
and update 3 given in Example 1. Each capital letter is an
abbreviation of an attribute, e.g., D for Date. The node ML
is for the query Q3 on attributes Molecule and Laboratory.
The edge from ML to M indicates that the query Q3 (for
ML) is contained in Q13 (for M). The number 2 in node ML
is the a↵ected number of |Q3 pTdrug q|. Moreover, the greatest
lower bound (resp. lowest upper bound) of ML and DL is
MDL (resp. L). We postpone the discussion of the shapes
in the figure, e.g., “ô”, “‹” and “˝”, to Section 5.2.

[Reflexivity] Q ® Q, for anyFQ P Q.
F[Antisymmetry] If Q ® Q1 and Q1 ® Q, then Q “ Q1 .
[Transitivity] If Q ® Q1 and Q1 ® Q2 , then Q ® Q2 . F

Valid and maximal valid nodes. Given a lattice pQ, ®q,
the node relative to a rule Q is valid if it is semantically
correct, thus should be executed to repair data. In our work,
if the validity of a rule is unknown, we rely on the user to
verify (see more details in Section 2.3). Fortunately, if a rule
Q is known to be valid, we can infer that Q1 is also valid if
Q1 ® Q. Moreover, the node relative to a valid rule Q is
maximal valid, if no Q2 is valid and Q ® Q2 .

For a query Q, we denote by attrpQq the set of distinct
attributes in its WHERE condition.
Note that for each user update, the sqlu queries have
the same value constraint on the same attribute, and thus
the rule containment verification is equivalent to a simpler condition: Q ® Q1 if attrpQ1 q is a subset attrpQq.
For instance, Q3 ® Q13 since attrpQ13 q “ tMoleculeu Ñ
tMolecule, Laboratoryu “ attrpQ3 q.

Example 4: Consider the lattice in Figure 2. Assume that
there are two valid queries to be applied: ML (Q3 in Example 1); and the other query DL that represents on a certain
date a certain lab works on only one molecule. All red nodes
are invalid queries, i.e., the queries that users will semantically invalidate. The other nodes are valid nodes. Moreover,
the blue nodes DL and ML are maximal valid nodes.
One nice property of using a lattice is that it provides
opportunities to prune nodes to be visited during traversal.

A↵ected tuples. For each query Q and instance T , we
call the tuples in QpT q a↵ected tuples, i.e., the tuples that
Q will repair. We also call |QpT q| the a↵ected number of
Q, relative to T . Consider Q3 and Tdrug in Example 1 for
instance. The a↵ected tuples are Q3 pTdrug q “ tt2 , t5 u, and
its corresponding a↵ected number is |Q3 pTdrug q| “ 2.
We discuss next how to organize these queries to facilitate
search strategies.

896

Lattice pruning. If a node Q is valid, by inference, all
nodes Q1 where Q1 ® Q are valid. On the other hand, if a
node Q is invalid, by inference, all nodes Q2 where Q ® Q2
are invalid. The rationality behind the above inferences is
that: if one query is valid, then any query that is more
specific is also valid; conversely, if it is invalid, then any
query that is more general is also invalid.
We denote by Q/ (i.e., above Q in the lattice) the queries
that Q contains, and Q' (i.e., below Q in the lattice) the
queries that contain Q. These notations naturally
î extend to
a set of queries, Q/ and Q' , such that Q/ “ QPQ Q/ and
î
Q' “ QPQ Q' .

‚

|v

✏

˝F 1

! ✏ }

v

˝

v

✏ s

|

˝

✏ u

(*

(‚

‚

* ‚✏

( ‚✏

"( ✏ zu
‚

"

‚

✏ su

˝

)˝

✏ |

" ✏

D2

‚

D3

D1 A1

$

‚

‚

˝F 3

A2 A3

‚

*# ✏

‚F 2

‚

w

‚

✏

‚

$ ✏ |
‚

"( ✏ z
‚

Figure 3: Lattice search algorithms. (Nodes ˝/‚/‚
represent valid nodes/maximal valid nodes/invalid
nodes. Red/green/blues edges are used to explain
di↵erent search strategies: BFS/DFS/Ducc.)
bottom) are more likely to be valid (resp. invalid). Hence,
if we traverse the lattice top-down, we have more chances
to visit a valid node Q. However, since it is close to the
top, the number of inferred valid nodes Q/ is small. On
the other hand, if we traverse the lattice bottom-up, we have
more chances to visit an invalid node Q1 . However, since it
is close to the bottom, the number of inferred invalid nodes
Q1 ' is small.
As shown in Example 6 and the above discussion, traversal based algorithms are locality based – they follow edge
connections from visited nodes. In such a way, Falcon can
only glide over the lattice. This is obviously not ideal when
the lattice is big but the budget B is small, which is exactly
the case we face. Hence, we propose new algorithms next.

ALGORITHMS: FALCON IN ACTION

In this section, we first describe some traversal based algorithms to solve our budget repair problem (Section 4.1).
We then present advanced algorithms to efficiently navigate
the search space (Section 4.2).
When discussing the algorithms, we assume that the lattice has been built given the user provided repair. The algorithms are designed for traversing the lattice and interacting
with the user. Details of constructing and maintaining the
lattice will be provided in Section 5.1.

4.1

t

˝

Example 5: Consider again the lattice in Figure 2. During
interactions with the user, if DL is validated, we can then
derive that DL/ “ {DML, DLQ, DMLQ} is valid. Consider now DQ, if DQ is invalidated, we can then derive that
DQ' “ {D, Q, H} is invalid.
The notation used in this paper is summarized in Table 3
in Appendix A.

4.

˝

B1 B2 B3

One-Hop Search: Falcon Glide

4.2

In traditional traversal algorithms of a lattice L the search
is based on some seeds, and then neighbours of the seeds
(i.e., one-hop) are visited by following edge connections. For
example, Breadth-first search (BFS) traverses L, by starting
at the bottom and explores the neighbor nodes first, before
moving to the next level neighbors. Depth-first search (DFS)
di↵erentiates in that after visiting a node, it explores as far
as possible along each branch before backtracking. A recent
traversal proposal, Ducc [28], bootstraps the search with a
DFS-style exploration until a node of interest is found. Then
it traverses the lattice alternating visits over valid and invalid nodes, in order to identify the border between them.
While the algorithm was defined to find minimal unique column combination, it can be used for any lattice traversal.
To better understand how di↵erent algorithms work, we
illustrate by an example below.

Multi-Hop Search: Falcon Dive

Now that we know that traversal based algorithms are
not suitable for our studied problem, we need to devise new
algorithms so that Falcon can dive on the lattice.

4.2.1

Binary Jump

Given a budget B, our objective is to define a divideand-conquer strategy that efficiently identifies nodes that are
both valid and not very close to the top, so as to maximize
the number of tuples to be repaired. To this purpose, we
present an strategy, namely binary jump, inspired by classical binary search. Roughly speaking, we treat the search
space as a linear space (i.e., an array) by sacrificing some
structural connections, and sort the nodes based on their associated a↵ected numbers. We can then do multi-hop search
to locate a candidate node to be verified with the user.
Note that conventionally, a binary search finds the position of a target value within a sorted array. Di↵erent from
it, binary jump does not have a target value to be searched.
In other words, binary jump is just inspired by binary search
by doing half-interval style lattice traversal.

Example 6: Figure 3 shows how various search algorithms
work, where red nodes indicate invalid nodes, blue nodes
represent maximal valid nodes, and the other nodes (i.e.,
small circles) are valid nodes. Let B “ 3, the number of
questions the user can verify. BFS search will visit the nodes
in a breadth-first fashion, e.g., in the order B1, B2, B3. DFS
search will visit the nodes in a depth-first fashion, e.g., in the
order D1, D2, D3. Di↵erent from BFS and DFS, Ducc [28]
explores the graph in a zigzag fashion, which tries to pivot on
valid nodes and explores their neighbors, e.g., in the order
A1, A2, A3. Since the above methods are edge based, the
search paths are indicated on edges.
Now let us give some insight why traversal based algorithms fail for our problem. Nodes close to the top (resp.

Binary jump over a path. We first discuss binary jump
over a path. Consider DMLQ Ñ DLQ Ñ LQ Ñ Q Ñ H
in Figure 2. The ground truth of the validity of them is
(T, T, F, F, F), where T means valid and F means invalid.
The search algorithm does not know the ground truth, so
initially we have p?, ?, ?, ?, ?q. To find the truth with traversal based approaches, we need OpN q questions in average,
where N is the length of the path. However, using binary

897

/

D6. [New search space.] Let Q? “ QzpQX Y QX ' q, i.e.,
search on the nodes that are not linked to any verified node.
It then goes to step D2.

jump will reduce it to Oplog N q questions, which is optimal,
by applying inferences of finding all valid/invalid nodes.
Next we discuss the meaning of “binary”. Straightforwardly, binary may refer to the o↵set as standard binary
search. However, we need to incorporate the information of
a↵ected number. Hence, the binary search could refer to the
median number. For instance, in Figure 2, the path DMLQ
Ñ DLQ Ñ LQ Ñ Q Ñ H corresponds to the a↵ected numbers p1, 2, 3, 4, 6q and the binary jump is to find the value
that is closest to rp1 ` 6q{2s “ 4.
For binary jump, we introduce a parameter d to bound
the search depth, which is the number of iterations one
can do binary jump before termination. Given a path
Q1 , Q2 , ¨ ¨ ¨ , Qx , we first ask the middle node Qx{2 . If the
node is valid, we ask the next middle node between Qx{2 and
Qx ; otherwise, we ask the next middle node between Q1 and
Qx{2 . After d wrong searches, the process terminates. We
refer to this search strategy as BinaryJump(). The rationale behind using the parameter d is that if we are following
the wrong direction, we should be aware and go back to the
right track, as a fault confessed is half redressed. We will
discuss how d is set in practice in Section 6.
Note that the number of the most general query (i.e., the
empty set at the bottom of the lattice) will change the whole
column, which makes the median number an optimistic estimation. To make it more realistic, instead, we set the
binary jump using log scale to find the value that is closest
p1`6q
s “ 3.
to e.g., rlog2

Complexity. It is easy to see that there are up to B iterations, and the sort (D2) dominates the cost. Hence, the
total time complexity is OpB ¨|Q|¨log|Q|q. Here, budget B is
typically small. Although the size of Q could be large for a
big relation, we will discuss an optimization in Section 5.1.1
about how to ensure that the size of Q is easily manageable.

4.2.2

Attribute correlations. The attribute correlation between
two attributes A and B, denoted by corpA, Bq, is to measure
how close they are to each other.
The intuition of using attribute correlations is that, if node
Q is correlated to attribute A that is being updated, then
it is more likely to be semantically relevant and useful for
the repair process. In general, we may get such information
from data profiling tools that measure attributes correlation.
We adopt the techniques proposed in CORDS [29] to profile a database T of relation R. Specifically, CORDS computes for each attribute pair a score in r0, 1s. Note that
pA, Bq and pB, Aq are di↵erent pairs. The score of an attribute pair pA, Bq equals to 1 means that it is a soft FD, indicating that A approximately uniquely determines B. Otherwise, it is a score computed using 2 statistics by examining the attribute values in attributes A and B.
In our lattice, oftentimes, we want to estimate the correlation between the attributes in a query Q (i.e., attrpQq)
and the attribute A being updated. In other words, we need
to compute the correlation between a set of attributes to a
single attribute.

From a path to a lattice. In order to take the advantage
of binary jump for lattice traversal, the broad intuition is to
do dimension reduction from a lattice to a one-dimensional
structure. That is, if we treat all nodes in the lattice uniformly, by sorting them in ascending order on their associated a↵ected numbers, we get a sorted array similar to the
one discussed above for the path.

Let Q? denote a set of unvalidated nodes, Q` represent
a set of valid nodes, and Q´ indicate a set of invalid nodes.
Next we present the algorithm.
Algorithm. Given a lattice pQ, ®q w.r.t. a repair
over
table T , a budget B for the number of questions the user
can answer, and a depth d to bound the search depth, the
algorithm for binary jump is given below.
´

Attribute Correlation: A Good Bait

Intuitively, we want to greedily select at each step the
node Q that is more likely to repair a large number of tuples.
However, since we do not know what are the correct nodes
until we verify them with the user, we need to estimate this
information. To define the score of a node, we augment the
existing information on the a↵ected number of each query
Q, i.e., the number of A values Q can repair (Section 3),
with the likelihood of a certain node to be related to the
current attribute A.

Using attribute correlations. We modified the algorithm
presented in CORDS to compute the correlation between a
set X of attributes and an attribute A, denoted by corpX, Aq.
In CORDS, an attribute pair pA, Bq is a soft FD if the support value suppA, Bq is above a given threshold ⌧ (see [29]
for more details). Similarly, we output pX, Bq as a soft FD
if the support value suppX, Bq ° ⌧ . Otherwise, we compute
the correlation score in r0, 1s for pX, Bq as follows.

`

D1. [Initialization.] Let Q “ H, Q “ ttopu (the top
node of the lattice), and Q? “ QzpQ´ Y Q` q. Also, let QX
be the set of nodes verified by users, initially empty.
D2. [Sort.] Sort unvalidated nodes Q? based on their affected numbers in ascending order.
D3. [Binary jump] Do the binary jump over Q? and select
one node Q, which is referred to as BinaryJump() . If the
user still has capacity (the total number of interactions is
below B), it interacts with user to verify Q, and updates
QX “ QX Y tQu. Otherwise, the whole process terminates.
If Q is valid, it goes to step D4; otherwise, it goes to D5
below, if Q is invalid.
D4. [Q is valid.] Apply Q over table T and update the
a↵ected numbers of nodes in Q. Set Q` “ Q` Y Q/ (infer
and enlarge valid nodes). Let Q? “ Q' and go to step D2.
D5. [Q is invalid.] Set Q´ “ Q´ Y Q' (infer and enlarge
invalid nodes). If the current depth is d, it goes to step D6.
Otherwise, Q? “ Q/ and goes to step D2.

2

corpX, Bq “
nq
m
m
m
1
2
k
ÿ
ÿ
ÿ
pnv1 ,v2 ,...,vk ´ ev1 ,v2 ,...,vk q2
2
“
...
ev1 ,v2 ,...,vk
v1 “1 v2 “1
vk “1
k
k
π
π
njvj
nji njv2 ...njvk
P rpvj q “ n
ev1 ,v2 ,...,vk “ n
“ 1 k´1
n
n
j“1
j“1
k
k
π
ÿ
mi ´
mi ` k ´ 1
q“
i“1

(1)
(2)
(3)
(4)

i“1

Here, k is the number of attributes in X and mi is the
number of distinct values in the i-th attribute. Moreover,
pv1 , v2 , ..., vk q is a tuple where the value of the j-th attribute is vj . Also, nv1 ,v2 ,...,vk is the frequency of tuple

898

C16 H16 Cl
statin
C24 H75 S6
C17 H20 N

Austin
1
2
0
0
3

N.Y.
0
0
1
0
1

Boston
0
1
0
0
1

Dubai
0
0
0
1
1

be used. The intuition is that, given an update trAs – a1 ,
not all attributes are relevant. Consequently, constructing
a lattice by incorporating irrelevant attributes will decrease
both efficiency and e↵ectiveness. Hence, we propose to pick
top-k attributes that are related to the attribute A being
updated, based on the attribute correlation score discussed
in Section 4.2.2. We refer to such a strategy as partial lattice
materalization, which performs much faster than a full materialization of the entire lattice, without losing accuracy. This
reduces the time complexity from Op2|R| ¨ |T |q to Op2k ¨ |T |q
where k could be much smaller than |R| in practice.
Practically, attribute correlation plays an important role
in devising e↵ective search strategies. We combine functional dependencies (FDs) and highly related attribute sets
(rules) to improve the search strategy. Please see the experiment in Appendix D.1 for more details on this point.

1
3
1
1

Table 2: A 2-way contingency table.
pv1 , v2 , ..., vk q, and ev1 ,v2 ,...,vk is the estimated frequency
based on the probability of vj appearing in the j-th attribute, i.e., njvj {n, where njvj is the frequency of vj in the
j-th attribute and n is the number of tuples.
Example 7:
Consider Table 1, and a given soft
FD in the traditional form: tMolecule, Laboratoryu Ñ
Quantity. Naturally, we have that the correlation value for
corptMolecule, Laboratoryu, Quantityq “ 1, since they can be
verified from the soft FD given above.
Consider now X “ tMoleculeu and B “ Laboratory.
Since there is no corresponding soft FD as tMoleculeu Ñ
Laboratory, we compute its correlation value by normalizing
2
statistics.
To do so, we first compute contingency table (see Table 2). We then compute expected count of each symbol
tuple. Consider tuple {statin,Austin}. The expected count
¨ nLaboratory
q{n “ 0.5, and the real
estatin,Boston “ pnMolecule
stain
Boston
count nstatin,Austin “ 1. Thus the di↵erence is pnstatin,Austin ´
estatin,Boston q2 {estatin,Boston “ 0.5. By summing up all di↵erences we have 2 “ 12.67, the degrees of freedom q “
4 ¨ 4 ´ p4 ` 4q ` 2 ´ 1 “ 9, thus corptMoleculeu, Laboratoryq “
12.67{p6 ˚ 9q “ 0.235.
We now give our greedy algorithm for multi-hop search
driven by correlation and a↵ected number.

5.1.2

Initialization. Given an update trAs – a1 , we need to
compute the a↵ected number of each query Q in the lattice.
The straightforward way of executing an sqlu query for each
node is very costly.
We approach the problem of initializing a↵ected numbers
by leveraging the containment relationships between nodes.
Consider two queries Q and Q1 , if Q ® Q1 , then given any
database T , we have QpT q Ñ Q1 pT q. Clearly, we can compute the result of QpT q from Q1 pT q. This is exactly the
problem of answering queries using materialized views [26].
Given the simplicity of the sqlu queries adopted in this
work, the query rewriting is simply to apply a selection using a constant value.
Example 8: Consider two queries Q3 , Q13 , and the dataset
Tdrug in Example 1. If we compute Q13 over Tdrug first as
Q13 pTdrug q, the result of Q3 pTdrug q is simply to select all tuples
from Q13 pTdrug q whose Laboratory values are Austin.
The above example suggests a simple way of computing
a↵ected numbers of lattice nodes in a bottom-up fashion.
Indeed, only one sqlu query is needed for the bottom node
of the lattice. Afterwards, in the bottom-up procedure, for
each query Q, it applies the aforementioned query rewriting technique on Q1 pT q to compute QpT q, where Q ® Q1
indicates that Q1 is one level below Q.

Correlation aware binary jump (CoDive). We revise
binary jump by using the correlation information, a↵ecting
D3 in Section 4.2.1. Note that the function BinaryJump()
will locate a node Q in the sorted list Q? . Instead of asking
the user to verify Q, we revise it with the following methodology. (1) We pick more nodes around Q in the sorted list,
with w on its left and the other w on its right. (2) For the
above 2w ` 1 nodes, we compute their scores (a↵ected number multiplies correlation score) and select the one with the
largest score, which will then be verified by the user. We
will discuss how w is set in practice in Section 6.

5.

Maintenance. Given the lattice pQ, ®q for table T and
update , when some rule Q is validated by the user, the
tuples a↵ected by Q will be repaired, i.e., QpT q will result in
a repaired database T 1 where T 1 “ T ‘ QpT q, i.e., applying
Q to T . For each yet unvalidated rule Q1 , the above changes
should be reflected, i.e., the number of a↵ected tuples should
be changed correspondingly, from |Q1 pT q| to |Q1 pT 1 q|.
The straightforward way is to execute Q1 pT 1 q to refresh
|Q1 pT 1 q|, or an optimized way of using the query rewriting
technique discussed above. However, in such incremental
scenarios, incremental algorithms have been developed for
various applications (see [36] for a survey). For incremental
algorithms, the updates are typically computed from a↵ected
areas, not the entire dataset. In our case, the a↵ect area is
exactly the a↵ected tuples QpT q. Next, we discuss how to
compute, for each unvalidated rule Q1 , the new |Q1 pT 1 q|.
Case 1 [Q1 ® Q]: |Q1 pT 1 q| “ 0.
Case 2 [Q ® Q2 ]: |Q2 pT 1 q| “ |Q2 pT q| ´ |QpT q|.
Case 3 [Q and Q3 are disjoint]: Neither Q ® Q3 nor
Q3 ® Q holds. We have |Q3 pT 1 q| “ |Q3 pT q| ´ |Q3 pQpT qq|.

OPTIMIZATIONS

In this section, we first discuss optimizations for maintaining the lattice (Section 5.1). We then describe a technique
to compress the search space, which can be applied to all algorithms (Section 5.2). We also discuss an extension when
external sources are available (Appendix B).

5.1

Lattice Maintenance

There are two main challenges when maintaining the lattice: its potential large size, and the updates of a↵ected
numbers of lattice nodes during each interaction. We address these two issues below.

5.1.1

Initialize and Maintain Affected Numbers

Partial Lattice Materialization

For some dataset, the number of attributes in R can be
large, such that a full materialization of the lattice is prohibitively expensive with 2|R| nodes.
Fortunately, in our framework, the update provided by the
user is a strong indicator to guide which attributes should

899

The above case 1 says that, if a valid rule Q is executed,
then the tuples that can be a↵ected by the queries Q1 it
contains have already been repaired. It is safe to set their
a↵ected numbers to 0 directly. The above case 2 tells that,
for all the queries Q2 that contains Q, the set of tuples
QpT q that Q2 can a↵ect has been repaired. Hence, it is
simple to reduce their a↵ected numbers by |QpT q|. In case
3, since neither Q3 ® Q nor Q ® Q3 holds, it first checks
the number of tuples that Q3 can a↵ect w.r.t. Q by executing Q3 pQpT qq, and then deducts its cardinality |Q3 pQpT qq|
from its maintained value |Q3 pT q|.

ition behind our choice is that, the more specific the query
is, the easier it is for the user to verify. Hence, we define the
most representative rule in a closed rule set to be the query
Q with the largest number of predicates w.r.t. |attrpQq|.
For instance, in Example 10, the representative rule for
the rule set of “˝”, {DMLQ, DML, DLQ, DL}, is DMLQ.

Example 9: Consider Fig. 2. Assume that during one
interaction, the users validate ML (i.e., query Q3 in Example 1). The a↵ected tuples are Q3 pTdrug q “ tt2 , t5 u and
|Q3 pTdrug q| “ 2. One can directly set the numbers associated
with DML, DLQ, and DMLQ to 0 (case 1). Moreover, it is
safe to change the number with node M as 3 ´ 2 “ 1. Similarly, we change the number with L (resp. H) to 1 (resp. 4)
(case 2). Consider DL and tuples Q3 pTdrug q “ tt2 , t5 u, it is
easy to verify that DL can update t2 but not t5 , hence the
number with DL will be changed as 1 ´ 1 “ 0 (case 3).

Example 11: Consider Figure 3 and the case that an algorithm has to test node F1. By computing the closed rule
set (nodes marked with ˝), the rule at the top is tested. If
the rule is valid, and therefore being executed, all the nodes
marked with ˝ will have empty updates now, so we can avoid
their computation. But if the rule is invalid, we can prune
all the nodes in the set, which is a big benefit compared with
the failed test of F1. In the latter case, we would still have
to validate the remaining nodes marked with ˝, even if we
can already derive that they are not valid.
The major di↵erence of our lattice, in contrast to traditional closed item set lattice used for data mining [42], is
that our lattice is dynamically changed. More specifically,
for each node Q, its associated information |QpT q| might
change during each interaction, such that the closed rule
sets will change correspondingly.

Benefits of the closed rules set. Any search algorithm
over the lattice can benefit from the closed rules set. Given
a node in a set, there are consequences that favor the search
both if the rule is judged valid or invalid. Remember that
we expose and test the representative rule. If it is true, we
do not need to compute the updates for any query in the
same closed rule set any more. If the answer is no, we also
have a benefit in terms of pruning of the nodes, since all the
nodes in the set can be safely discharged.

Time complexity. Cases 1 and 2 are clearly in constant
time. For case 3, the cost is reduced from computing Q3 pT 1 q
(i.e., the entire table) to Q3 pQpT qq (the tuples a↵ected by
Q) where |QpT q| is typically much smaller than |T |.

5.2

Closed Rule Sets

A natural question, when searching a lattice, is whether
there is any redundancy in the behavior of the rules, so we
turn our attention now on how to identify such redundancy.
Closure operator f . Given a lattice pQ, ®q for update
and table T , we define a closure operator f . For any
Q P Q, let f pQq “ tQ1 u and the following properties hold:
(1) Q ® Q1 ; (2) |QpT q| “ |Q1 pT q|; and (3) EQ2 P Q where
Q1 ‰ Q2 , Q1 ® Q2 , and |Q1 pT q| “ |Q2 pT q|.
Intuitively, the closure operator f is to locate the maximal
ancestor of a query Q that has the same e↵ect on the number
tuples they can change. Consider Fig. 2 for example, we have
f (DMLQ) = {DL}, and f (DMQ) = {DM, DQ}.

6.

EXPERIMENTAL STUDY

We implemented Falcon in Java and used PostgreSQL
9.3 as the underlying DBMS. All experiments were conducted on a MacBook Pro with an Intel i7 CPU@2.3Ghz
and 16GB of memory. Our frontend extends OpenRefine.
Datasets. We used four real-world datasets and one synthetic dataset, described as follows.
∂ Soccer is a real dataset with 7 attributes and 1625 tuples about soccer players and their clubs scraped from the
Web (www.premierleague.com/en-gb.html, www.legaseriea.
it/en/, www.bundesliga.com/en/).

Closed rule sets. Given a lattice pQ, ®q, two rules Q and
Q1 belong to the same closed rule set, i↵ f pQq “ f pQ1 q.
The smallest (minimal) closed rule set contains one rule Q,
i.e., f pQq “ tQu and no other rule Q1 where f pQ1 q “ tQu.

∑

Hospital is based on a dataset from US Department
of Health & Human Services (http://www.medicare.gov/
hospitalcompare/). It has 12 attributes and 100k tuples.

Example 10: Consider Fig. 2. The shapes identify distinct
closed rule sets. For example, the closed rule set for “˝” is
{DMLQ, DML, DLQ, DL}, since they are connected and
have the same a↵ected numbers. Also, the closed rule set
for “‚” is {DMQ, DM, DQ}, similar for other shapes.
It deserves to note that the concept of closed rule sets is in
the instance level, i.e., queries in the same closed rule set will
change the same set of tuples for the given dataset. However,
they are not the same in the semantic level, i.e., some of
them might be valid while the others might be invalid. In
order to better understand the above discussion, consider
an extreme case that each lattice node can change only one
tuple, which makes all candidate queries in one closed rule
set. Apparently, they contain both valid and invalid rules.
In other words, the closed rule set ignores the factor that
whether a rule is valid or not.

∏ BUS is one of the UK government public datasets available at http://data.gov.uk/data and deals with bus schedules and routes. It contains 15 attributes and 250K tuples.
π DBLP is based on the popular collection of authors, publications and venues from http://dblp.uni-trier.de/xml/. We
downloaded the whole xml dataset, and translated it into
a single relational table with 15 attributes. We considered
instances of 1M and 5M tuples, for quality and scalability
tests, respectively.
∫ Synth is a dataset we designed starting from the original Soccer dataset in order to study the scalability over
the number of tuples and a larger number of attributes.
The dataset has 10 attributes and we used a generator
from http://www.cs.toronto.edu/tox/toxgene/ to create instances of di↵erent sizes.

Representative rule. One natural question, given a closed
rule set, is which query to be verified by the user. The intu-

900

Algorithms. We implemented several algorithms for the
exploration of the lattice. First, we study our own proposals
for multi-hop search. Dive is the binary jump algorithm
presented in Section 4.2.1. CoDive is its extention to make
use of the attributes correlation information, when this is
available, as described in Section 4.2.2.
These are compared with one-hop search strategies (Section 4.1). Beside BFS and DFS, we have also implemented
Ducc [28], which was designed to reduce the number of tests
during the discovery of all the minimal unique column combinations in a given dataset. As we will show in the results,
Ducc is better than BFS and DFS for extensive searches of
maximal rules in the lattice, but it was not designed to deal
with small values of budget B for user interactions.
In addition to these, we also compared our results with the
greedy search algorithm for the o↵-line version of our problem. This algorithm, O↵Line, is aware of the valid nodes
in the lattice. Given this information, it greedly picks the
node that maximizes the error coverage at each step, with
the number of steps equals to the budget B.

ing in order to improve the quality of repairs. Given a set
of rules, it will incrementally ask users to solicit the right
repairs suggested by the rules. We tested an incremental
variant of algorithm ∑ above, by using GDR to suggest repairs (i.e., cell updates) to users. In this case, additional
budget is used to answer GDR user queries.
π Active Learning in Lattice Traversal: Finally, we compared our methods to an active learning variant of our
lattice-based approach that was designed ad-hoc for this purpose. In the active learning algorithm, we first generated
some features for each node, including attribute indicator,
attribute values, original value, and updated value. We then
trained a support vector machine (SVM) model with labeled
nodes. Finally we used active learning to select the best node
to ask users in each iteration.
Errors and Metrics. Since the considered datasets are
clean, we introduce noise to verify the algorithms behaviour
in the cleaning process. To start, we manually defined a set
of CFDs [16] and fixing rules [43] for each scenario. We used
8 rules for Soccer, 124 rules for Hospital, 8 rules for BUS, 69
rules for DBLP, and 12 rules for Synth.
Afterwards, we used an error-generation tool to inject errors into the clean instances. To make our error-generation
more systematic, we relied on an open-source error generation tool by Arocena and others [6]. The tool allows users to
inject various kinds of errors within a clean database, both
rule-based, random and statistics-based. Being based on an
open-source tool, our error generation configuration can be
easily shared and reused.
We keep running an algorithm until all the introduced
errors are fixed either by a rule or by the user updates.
Then, we focus our attention on the interaction cost. We
adopt natural metrics: the number of user-provided updates
U , the number of users’ answers for nodes validation A, and
we simply add them up to get the total interaction cost
TC . Notice that the latest metric is treating both kinds of
interaction with the same weight, i.e., they are considered
equally difficult for the user. Despite more sophisticated
combinations are possible, we found that the simple sum
gives a global overview of the algorithms behaviour that is
close to the real overall experience of the users.
In order to have an indicator of the advantage of using
interactive cleaning, we also measure the benefit of an algorithm in comparison to the manual update of all the errors. We first define the cost ratio as the number of actions
divided by the number of errors. Manually updating 100
errors requires 100 user actions (updates) for a cost ratio
of 1. However, by using our tool, it may be the case that
25 actions can fix 100 errors, therefore the cost ratio would
be 0.25. Given an algorithm ↵, a dataset D, and the interaction cost TC to obtain a set of queries Q covering all
introduced errors, we define the benefit of the algorithm as
BNF↵ “ 1 ´ TC {|QpDq|.
Finally, we measure the execution times for the algorithms
in the generation of the lattice and in its maintenance.
Notice that we do not assume that users always provide
correct inputs. On the contrary, the impact of user mistakes
is studied in one of our experiments.

Baselines. We compared Falcon with four baselines.
Refine: Our proposal generalizes the transformation
language of existing tools such as OpenRefine (http:
//openrefine.org/) and Trifacta Wrangler (https://www.
trifacta.com/trifacta-wrangler/) [27]. These tools enable
users to define transformations by examples exactly as in
our setting. Users modify values in a cell for attribute A
and the systems suggests possible transformations over the
remaining tuples for A. While we do not focus on string
manipulation as some of these tools, our language supports
rules (i.e., transformations) with look-up over any combination of columns in the relation. In fact, given an update,
these tools enable the inference of only two transformations
that are comparable to our language: either the single cell
is updated (the top of the lattice) or the erroneous value e
is replaced with the new value v for all the occurrences in
the attribute. The latter corresponds to one of the nodes
in our lattice. More precisely, the standardization rule is:
UPDATE T SET A = v WHERE A = e.
Given this context, a natural baseline algorithm models
these transformation tools. This algorithm, namely Refine,
checks for every user update the node that generalizes it to
a standardization rule, or picks the rule at the top of the
lattice if the validation fails.
∑ Rule-Learning Approaches: Many previous approaches
have concentrated on learning data-quality rules (e.g., [12,
17]). Therefore, we compared our algorithm with one of
these methods. More specifically:
piq starting from a dirty database, we asked users to clean a
sample of tuples (part of the budget was used to do this);
piiq based on the sample tuples, we used a CFD-miner to
learn a number of SQL-updates; since it is known that rulemining algorithms may discover semantically invalid rules
(due to “overfitting”) we asked users to select a subset of
semantically valid rules (the second part of the budget was
used for this purpose); and
piiiq we used the set of SQL-updates to repair the dirty
instance, and measured the benefit score (see below).
∏ Guided Data Repairs: To explore the impact of active
learning, we used GDR [46]. GDR (“Guided Data Repairs”)
is a recently proposed algorithm that relies on active learn-

∂

Experiments. We conducted five experiments. piq Exp-1
compares benefits of the various lattice-traversal algorithms
with di↵erent budget values, and show that CoDive maximizes the benefit. piiq Exp-2 studies the impact of di↵erent

901

1,0

1,0

0,5

0,5

0,5

-0,5

DFS

BFS

Ducc

Dive

CoDive

-0,5

DFS

BFS

Ducc

Dive

CoDive

Soccer
Synth	1M

Hospital
DBLP

Synth	10k
BUS

-1,5

DFS

-0,5

BFS

Ducc

Dive

CoDive

-1,0

-1,0

-1,0
-1,5

0,0

0,0

Benefit

Benefit

0,0

Benefit

1,0

Soccer
Synth	1M

Hospital
DBLP

-1,5

Synth	10k
BUS

Soccer
Synth	1M

Hospital
DBLP

Synth	10k
BUS

(a) Budget=2
(b) Budget=3
(c) Budget=5
Figure 4: Benefit for the various algorithms for the five datasets.
10000

parameters of the models. In particular, we show that closed
rule sets, an optimization technique discussed in Section 5.2,
always reduces the cost. piiiq Exp-3 compares CoDive with
closed rule sets to the four baselines. Interestingly, our algorithm outperforms all of the baselines. pivq Exp-4 studies
scalability. pvq Finally, Exp-5 investigates the robustness of
Falcon w.r.t. user mistakes.

Soccer

Hospital

Synth	10k

Soccer

Hospital

Synth	10k

1000

#	Answers

#	Updates

1000

10000

100
10
1

100
10
1

DFS

BFS

Dive

CoDive

DFS

BFS

Dive

CoDive

#User	Interactions

AVG	U	- Synth	10k
AVG	U	- Hospital

AVG	A	- Synth	10k
AVG	A	-Hospital

1000

100

10

w=2

w=3

w=5

w=7

w=10

#	User	Interactions

(a) Number of User Updates (b) Number of User Answers
Figure 5: Impact of closed rule sets for B “ 2.

Exp-1: Lattice search algorithms. We now turn our attention to the comparison of the di↵erent search algorithms.
Figure 4 reports the benefit of each algorithm for the six
datasets over increasing budget B (i.e., maximum number
of questions after an update).
We start with the setting where the user is willing to answer only two questions (B=2) in Figure 4(a). The proposed
algorithms, Dive and CoDive, consistently report a positive
gain, which, for CoDive, can be interpreted as a reduction
of the total user interaction cost between 22% (Soccer) and
97% (BUS). The plot also reveals that one-hop algorithms
fail for the budget exploration of the lattice, with the notable exception of the Hospital dataset. This results is not
surprising if we look more closely at this scenario. Hospital
schema has a large number of FDs with always one or two
attributes in the left hand side (LHS) of the rules. This is
reflected in the CFDs that we used to introduce the errors.
Rules with one or two LHS attributes are at the bottom of
the lattice, and this is the most favourable setting for onehop based algorithms, since they all start from the bottom.
On the other hand, when rules start to have more attributes
in the LHS, more nodes must be checked to take a decision,
these algorithms fail and Dive and CoDive greatly outperform them. Similar results can be observed with B “ 3 in
Figure 4(b). More details are provided in Appendix D.2.
By increasing the budget to five questions, as reported in
Figure 4(c), all algorithms can explore the lattice further
at each update and the performance improve accordingly.
This improvement is bigger for one-hop based algorithms as
they are now able to get closer to the maximal rules in the
traversal with a smaller number of updates.
Finally, since algorithm O↵Line does not need to perform
the search, for each update it is able to identify immediately
the maximal rule. Therefore as expected O↵Line is always
able to completely fix the data with a number of steps that
is equal to the number of rules used to introduce noise.

Dive	- No	B
Dive	- B=5

CoDive	- No	B
CoDive	- B=5

200
150
100
50

d=1

d=2

d=3

d=4

d=5

d=6

(a) Avg costs over B “ 2, 3, 5 (b) Costs for Synth 1k wrt d
for algorithm CoDive wrt w for B=5 and without budget
Figure 6: E↵ects over the interaction cost. U and A
are the number of user updates and user answers.
rithms with and without this optimization, and we measure
the di↵erence in the number of required user updates and
user answers to cover all errors on three scenarios (Soccer,
Hospital and Synth 10k) (see Figure 5). All methods benefit
from the optimization, with the exception of Ducc, which
does not show any di↵erence, and thus is not reported.
The method that gains most benefit from this optimization is DFS. The explanation is that with low budgets, such
as B “ 2, DFS always reaches the level in the lattice with
two attributes. While the rule corresponding to the node
may be too general and therefore invalidated by the user,
it may be part of a closed rule set. Therefore, the user is
o↵ered the representative rule, which is more specific and,
in some cases, true. This happens also for rules with only
one attribute in the LHS for Hospital, as discussed above. In
fact, even BFS, which never goes beyond nodes with only
one attribute in the LHS with low B values, benefits from
the closed rules set for this dataset.
As shown in Exp-1, on average CoDive has higher benefit
values than Dive. However, the quality of CoDive depends
on the value for parameter w (Section 4.2.2). We report in
Figure 6(a) the experimental results with di↵erent w values.
Each reported value is the number of user updates U (user
answers A) averaged over the results for B equals to 2, 3,
and 5. Both for Hospital and Synth 10k the best results are
observed with w “ 3. The parameters does not impact the
results for Soccer.

Exp-2: Closed rule sets and parameters. The results
for the previous experiments have been conducted with the
closed rule sets computed in the lattice. In fact, this optimization enables a reduction both in the number of updates
and in the number of questions. To illustrate the impact
of the closed rule sets, we executed the lattice search algo-

902

5000

1,0

Benefit

0,5
0,0
-0,5

CoDive

Refine

Rule	
Learn.

GDR

Active	
Learn.

Interactions

5000

Repairs

4000

4000

3000

3000

2000

2000

1000

1000

Interactions

Repairs

0

0

No	Errors

1%

2%

5%

No	Errors

1%

2%

5%

(a) Hospital
(b) BUS
Figure 9: Impact of user mistakes.

-1,0
-1,5

Soccer

Hospital

Synth	10k

Synth	1M

DBLP

BUS

We start by analyzing the impact of the techniques discussed in Section 5.1 for lattice maintenance. Figure 8(a)
shows the total execution time for an update, defined as the
time to create the lattice plus the time to update it with
rules validated by the user in the interaction. We find it
interesting to show that di↵erent updates can lead to very
di↵erent execution times, because of the size of the queries
involved in the lattice. Therefore, for the same scenario,
we report both the execution times for the first user update,
and corresponding interaction, and the times for the 4th user
update. For all combinations of updates and scenarios, the
incremental maintenance is 3–5 times faster than the naive
solution that rebuilds the lattice for every rule validated by
the user (4 times faster on average for the first five updates).
Creating the lattice requires to run queries to collect the
data, and intersection over the sets of tuples to find the corresponding number of a↵ected tuples for each node. When
the dataset is large, the creation of the lattice can require
a couple of seconds, as reported in Figure 8(b-c) for the
average of the first ten updates. However, the creation is required only when a new user update is given, and the maintenance of the lattice in the rule validation always requires
less than 20ms with our technique.
Finally, we study how the number of attributes in the
dataset influences the performance. For this experiment we
selected subsets of attributes of Hospital and also extended
it with two more attributes by joining another table. Figure 8(d) shows the average times over the first five updates
for the creation of the lattice and its maintenance with our
technique. While the response time is always below 10ms,
the creation of the lattice takes on average about 10 seconds,
with a maximum of 30 seconds for the first user update. As
discussed in Section 5.1.1, it is important to be able to identify the attributes of interest for the mining to limit the
exponential explosion of the number of nodes in the lattice.

Figure 7: Benefit compared with the baselines.
We also report in Figure 6(b) the experimental results for
the synthetic datasets with di↵erent values of the parameter d, as introduced for the binary jump algorithm in Section 4.2.1. Experiments over di↵erent B values and datasets
also confirm that d “ 3 leads to the best results in terms of
optimization of the interaction cost.
Exp-3: Comparison to the baselines. Figure 7 reports
a comparison of our CoDive algorithm to the four baselines
discussed in Section 6. We fixed a timeout of two hours for
all tests. Notice that not all algorithm terminated within the
timeout. This accounts for the missing bars in the chart.
Our approach significantly outperforms all baselines.
First, CoDive results are significantly better than those
based on rule discovery. This suggests that our novel
paradigm for data repairing is an improvement w.r.t. previous approaches in which quality rules are established upfront. Interestingly, this is confirmed also in the case in
which rule discovery is coupled with an interactive algorithm, like GDR. In fact, the additional number of user interactions needed to run GDR brings to even lower benefit.
Results confirm our intuition that using user updates to
lead the discovery of rules in an incremental way yields more
complete and e↵ective repairs than state-of-the-art rulelearning algorithms, which can return incomplete or redundant sets of constraints. In fact, in our experiments neither
RuleLearning, nor GDR was able to repair all of errors in
the data. Detailed comparison is reported in Appendix D.2.
CoDive algorithm also outperforms its active learning
variant. Since ActiveLearning shares the same infrastructure as CoDive, here results are better w.r.t. RuleLearning
and GDR. In fact, as for CoDive, whenever it terminated
also ActiveLearning was able to repair all errors. ActiveLearning worked well in datasets with few rules, such as
BUS and Synth 10k, while performed poorly in datasets with
many rules like Hospital. Appendix C reports further details
on active learning algorithm. Overall, however, benefit levels are lower. Hence, the active learning variant pays the
price in terms of user-interactions of the additional training
phase, which does not bring benefits w.r.t. CoDive.
Finally, CoDive outperforms Refine because of the less
expressive language in the latter. While we discover rules
using any combination of columns, Refine either generates
rules for the entire column, which is unlikely to hold for
data errors, or rules that update a single tuple. Single tuples
updates are always correct and promptly validated, but their
very small coverage leads to no benefit in using this tool.

Exp-5: User Mistakes. We also tested the robustness of
our approach w.r.t. to user-errors. That is, we do not assume that users always provide correct answers. On the contrary, assume users may sporadically make mistakes. These
may be of two kinds, as follows. We notice that in both
cases, our algorithm is essentially self-healing:
piq The user performs a wrong update. This is the easier
case, since we can expect that from a wrong update, only
invalid rules are generated; these will be rejected by the user,
and the error is fixed.
piiq Following a valid update, the user wrongfully validates
an invalid rule. This case in more delicate, since, following the wrong rule, the algorithm will indeed perform some
incorrect updates. Overall, however, this will simply generate more dirtiness in the database, and the user still has a
chance to correct this new dirtiness that s/he has introduced
in the database in subsequent iterations.

Exp-4: Scalability. We report the performance of the
lattice construction and maintenance in Figure 8. Times
are reported in ms and the y axis is in log scale.

903

Hospital	- 1st	update

Hospital	- 4th	update

Synth	10k	- 1st	update

Synth	10k	- 4th	update

100000

100000

Avg	lattice	creation

Avg	lattice	maintenance

100000

Avg	lattice	creation

Avg	lattice	maintenance

Avg	lattice	creation

1000

1000

1000

100
10

100
10
1

1
naïve

incremental

1k

10k

100k

1M

Time	(ms)

1000

Time	(ms)

10000

Time	(ms)

10000

Time	(ms)

10000

10000

100
10
1

Avg	lattice	maintenance

100
10
1

100k

500k

1M

2M

5M

4	atts

8	atts

10	atts

14	atts

(a) Total execution time (ms) (b) Avg execution time (ms) (c) Avg execution time (ms) (d) Avg exec. time (ms) with
for ith update: lattice creation with increasing # of tuples for with increasing # of tuples for increasing # of attributes for
Synth (first 10 updates)
DBLP (first 10 updates)
Hospital (first 10 updates)
+ maintenance
Figure 8: Efficiency for the lattice creation and maintenance: Dive algorithm, unbounded B.
A key property is that the rules we discover at each step
are applied only once – i.e., during the step they were generated in – and therefore they can be fixed by further interaction. This requires that the user is requested to reconsider
some previously updated cells. As a consequence, repair
ratios decrease in case of errors. In addition, we need to
prevent cyclic behaviors. To do this, the system checks updates and notifies users whenever it is updating a cell that
has been repaired in previous iterations. This helps users to
identify previous mistakes, and prevents cycles.
Figure 9 shows the impact of user mistakes. Assume that
users made mistakes with a given probability – ranging from
1% to 5% – and compare results to the case without mistake.
Experiments confirm that the system is able to recover from
these errors, at the price of more user interactions.

7.

From an algorithmic perspective, most of these approaches
exploit active learning to validate with users informative examples; we show in Section 6 Exp-3 how other signals, such
as correlation, can better guide the search in our setting.
Rule-based data cleaning. Rule-based approaches for
data cleaning are divided between methods to discover the
rules from clean data [11, 12, 17, 25, 40], and algorithms and
systems to apply the rules over dirty data to automatically
fix the detected errors [7,13,15,18,19,21,23,24,30,32,43,46].
Our proposal overcomes some of the shortcomings in these
methods. In terms of rules discovery, mining on dirty data
leads to a lot of useless rules, therefore most of the methods
report e↵ective results assuming a clean sample. On the
contrary, we naturally start from dirty data. In terms of
cleaning, we restrict our language to deterministic updates,
which do no need variables or placeholders that the users
ultimately have to manually verify. In terms of learning
from user repairs, the closest approach to our solution is the
use of previous repairs to model “repair preferences” [41].
However, this approach needs a set of rules to be given as
input and it only refines them, without discovering new ones.
Closed frequent itemset. The concept of closed frequent
itemset is widely used in data mining (see [48] for a survey),
where it refers to a set of itemsets that are both frequent
(i.e., the support value is above a given threshold) and closed
(i.e., there is no superset that is closed). In fact, our closed
rule set is inspired from closed frequent itemset, with the
major di↵erence that our data structure (i.e., the lattice)
keeps changing during interactions. Traditionally, the search
space for closed frequent itemset in data ming is static.

RELATED WORK

Data transformation. Interactive systems for data transformation [27,37,44] also reason about the updated attribute
to learn transformation rules. They mainly focus on string
manipulation and reformatting at the text level. In contrast,
we use more expressive SQL scripts. Consequently, we discover not only rules that contain one attribute that is being
updated syntactically, but also rules that combine multiple
attributes to semantically determine new repairs. Our language and algorithms can lead to smaller interaction cost,
as discussed in Section 6 Exp-3.
Machine learning for cleaning. Given a set of user updates, they can be used as training data to train machine
learning models, which in turn can be used to predict other
repairs [41, 45]. However, ML models are typically blackboxes that identify updates without explanations, which are
hard to be trusted by users, especially for critical applications that need repairs with guaranteed correctness. Instead,
sqlu queries are declarative and are preferred for human validation. Moreover, to train a machine learning model with
updates, they must be semantically consistent, i.e., they
refer to the same type of errors. In practice, however, this
assumption does not always hold since multiple updates may
refer to di↵erent types of errors. This heterogeneity may hinder the usability of the trained machine learning model for
prediction. Di↵erent from them, Falcon is bootstrapped
by a single update, and ensures the following interactions
are related to the queries with consistent semantics.
Query by examples. Several proposals have exploited the
opportunity of using examples to discover queries [2, 8, 9, 38,
49,50], schema matchings [35,47], and schema mappings [4].
They mainly focus on finding how to join multiple tables. In
contrast to them, we study how to discover sqlu queries on
one table, with the main challenge of understanding the update semantics that is not considered by other approaches.

8.

CONCLUSION AND FUTURE WORK

We have presented Falcon, an interactive, declarative
and deterministic data cleaning system. We have demonstrated that Falcon can e↵ectively interact with users to
generalize user-solicited updates, and clean-up data with a
significant benefit w.r.t. the number of required interactions.
A number of possible future studies using Falcon are
apparent. First of all, we plan to extend it by using external
sources, as remarked in Appendix B. Moreover, we will
leverage the information obtained from previous interactions
with the user w.r.t. multiple data updates.
Acknowledgement. This work was partly supported
by the 973 Program of China (2015CB358700), NSF
of China (61422205, 61472198), Huawei, Shenzhou,
Tencent, FDCT/116/2013/A3, MYRG105(Y1-L3)-FST13GZ, National High-Tech R&D (863) Program of China
(2012AA012600), and the Chinese Special Project of Science and Technology (2013zx01039-002-002).

904

9.

REFERENCES

[27] J. Heer, J. M. Hellerstein, and S. Kandel. Predictive
interaction for data transformation. In CIDR, 2015.
[28] A. Heise, J. Quiané-Ruiz, Z. Abedjan, A. Jentzsch, and
F. Naumann. Scalable discovery of unique column
combinations. PVLDB, 7(4), 2013.
[29] I. F. Ilyas, V. Markl, P. J. Haas, P. Brown, and
A. Aboulnaga. CORDS: automatic discovery of correlations
and soft functional dependencies. In SIGMOD, pages
647–658, 2004.
[30] M. Interlandi and N. Tang. Proof positive and negative in
data cleaning. In ICDE, 2015.
[31] S. Kandel, A. Paepcke, J. M. Hellerstein, and J. Heer.
Enterprise data analysis and visualization: An interview
study. IEEE Trans. Vis. Comput. Graph., 18(12), 2012.
[32] Z. Khayyat, I. F. Ilyas, A. Jindal, S. Madden, M. Ouzzani,
J.-A. Quiane-Ruiz, P. Papotti, N. Tang, and S. Yin.
BigDansing: a system for big data cleansing. In SIGMOD,
2015.
[33] J. Manyika. Big data: The next frontier for innovation,
competition, and productivity. Technical report, McKinsey
Global Institute, 2011.
[34] C. H. Papadimitriou. Computational Complexity. Addison
Wesley, 1994.
[35] L. Qian, M. J. Cafarella, and H. V. Jagadish. Sample-driven
schema mapping. In SIGMOD, pages 73–84, 2012.
[36] G. Ramalingam and T. W. Reps. A categorized
bibliography on incremental computation. In POPL, 1993.
[37] V. Raman and J. M. Hellerstein. Potter’s wheel: An
interactive data cleaning system. In VLDB, pages 381–390,
2001.
[38] Y. Shen, K. Chakrabarti, S. Chaudhuri, B. Ding, and
L. Novik. Discovering queries based on example tuples. In
SIGMOD, pages 493–504, 2014.
[39] D. D. Sleator and R. E. Tarjan. Amortized efficiency of list
update and paging rules. Commun. ACM, 28(2), 1985.
[40] S. Song and L. Chen. Efficient discovery of similarity
constraints for matching dependencies. Data Knowl. Eng.,
87, 2013.
[41] M. Volkovs, F. Chiang, J. Szlichta, and R. J. Miller.
Continuous data cleaning. In ICDE, 2014.
[42] J. Wang, J. Han, and J. Pei. CLOSET+: searching for the
best strategies for mining frequent closed itemsets. In
SIGKDD, 2003.
[43] J. Wang and N. Tang. Towards dependable data repairing
with fixing rules. In SIGMOD, 2014.
[44] B. Wu and C. A. Knoblock. An iterative approach to
synthesize data transformation programs. In IJCAI, pages
1726–1732, 2015.
[45] M. Yakout, L. Berti-Equille, and A. K. Elmagarmid. Don’t
be scared: use scalable automatic repairing with maximal
likelihood and bounded changes. In SIGMOD, pages
553–564, 2013.
[46] M. Yakout, A. K. Elmagarmid, J. Neville, M. Ouzzani, and
I. F. Ilyas. Guided data repair. PVLDB, 2011.
[47] Z. Yan, N. Zheng, Z. G. Ives, P. P. Talukdar, and C. Yu.
Actively soliciting feedback for query answers in keyword
search-based data integration. PVLDB, 6(3):205–216, 2013.
[48] M. J. Zaki and W. Meira. Data Mining and Analysis:
Fundamental Concepts and Algorithms. Cambridge
University Press, 2014.
[49] M. Zhang, H. Elmeleegy, C. M. Procopiuc, and
D. Srivastava. Reverse engineering complex join queries. In
SIGMOD, 2013.
[50] M. M. Zloof. Query by example. In AFIPS. ACM, 1975.

[1] S. Abiteboul, R. Hull, and V. Vianu. Foundations of
Databases. Addison-Wesley, 1995.
[2] A. Abouzied, J. M. Hellerstein, and A. Silberschatz. Playful
query specification with dataplay. PVLDB,
5(12):1938–1941, 2012.
[3] S. Albers. Online algorithms: a survey. Math. Program.,
97(1-2), 2003.
[4] B. Alexe, L. Chiticariu, R. J. Miller, and W. C. Tan. Muse:
Mapping understanding and design by example. In ICDE,
pages 10–19, 2008.
[5] C. Ambühl. O✏ine list update is np-hard. In Algorithms ESA 2000, 8th Annual European Symposium, 2000.
[6] P. C. Arocena, B. Glavic, G. Mecca, R. J. Miller,
P. Papotti, and D. Santoro. Messing up with BART: error
generation for evaluating data-cleaning algorithms.
PVLDB, 9(2), 2015.
[7] P. Bohannon, M. Flaster, W. Fan, and R. Rastogi. A
cost-based model and e↵ective heuristic for repairing
constraints by value modification. In SIGMOD, 2005.
[8] A. Bonifati, R. Ciucanu, and S. Staworko. Interactive
inference of join queries. In EDBT, 2014.
[9] A. Bonifati, R. Ciucanu, and S. Staworko. Interactive join
query inference with JIM. PVLDB, 7(13), 2014.
[10] C. Chang and C. Lin. LIBSVM: A library for support
vector machines. ACMTIST, 2(3):27, 2011.
[11] F. Chiang and R. J. Miller. Discovering data quality rules.
PVLDB, 1(1), 2008.
[12] X. Chu, I. F. Ilyas, and P. Papotti. Discovering denial
constraints. PVLDB, 6(13), 2013.
[13] M. Dallachiesa, A. Ebaid, A. Eldawy, A. K. Elmagarmid,
I. F. Ilyas, M. Ouzzani, and N. Tang. NADEEF: a
commodity data cleaning system. In SIGMOD, 2013.
[14] O. Deshpande, D. S. Lamba, M. Tourn, S. Das,
S. Subramaniam, A. Rajaraman, V. Harinarayan, and
A. Doan. Building, maintaining, and using knowledge
bases: A report from the trenches. In SIGMOD, 2013.
[15] A. Ebaid, A. K. Elmagarmid, I. F. Ilyas, M. Ouzzani,
J. Quiané-Ruiz, N. Tang, and S. Yin. NADEEF: A
generalized data cleaning system. PVLDB,
6(12):1218–1221, 2013.
[16] W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis.
Conditional functional dependencies for capturing data
inconsistencies. ACM Trans. Database Syst., 33(2), 2008.
[17] W. Fan, F. Geerts, J. Li, and M. Xiong. Discovering
conditional functional dependencies. IEEE Trans. Knowl.
Data Eng., 23(5), 2011.
[18] W. Fan, F. Geerts, N. Tang, and W. Yu. Inferring data
currency and consistency for conflict resolution. In ICDE,
2013.
[19] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Interaction
between record matching and data repairing. In SIGMOD,
2011.
[20] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Towards certain
fixes with editing rules and master data. VLDB J., 21(2),
2012.
[21] H. Galhardas, D. Florescu, D. Shasha, E. Simon, and C.-A.
Saita. Declarative data cleaning: Language, model, and
algorithms. In VLDB, 2001.
[22] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. The
LLUNATIC data-cleaning framework. PVLDB, 6(9), 2013.
[23] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. Mapping
and Cleaning. In ICDE, pages 232–243, 2014.
[24] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. That’s all
folks! LLUNATIC goes open source. PVLDB,
7(13):1565–1568, 2014.
[25] L. Golab, H. J. Karlo↵, F. Korn, B. Saha, and
D. Srivastava. Discovering conservation rules. In ICDE,
2012.
[26] A. Y. Halevy. Answering queries using views: A survey.
VLDB J., 10(4), 2001.

APPENDIX
A. SUMMARY OF NOTATION
We summarize the notations used in the paper in Table 3.

905

Symbol
Q
QpT q
Q1 ® Q 2
attrpQq
: trAs – a1
pQ, ®q w.r.t.
Q/ (Q/ )
Q' (Q' )

Rank
1
2
3
4
5
...
99
100

Description
a sqlu query, or a data quality rule
a↵ected tuples of Q over table T
Q1 is contained by Q2
attributes in the WHERE condition of Q
an update of trAs to a1
a lattice of queries Q on partial order ®
the set of queries that Q (queries Q) contains
the set of queries that contains Q (queries Q)

Table 3: Notations used in the paper.

D
1

Indicators
M
L
Q
2
1
0

D
11 Nov

AttributeValues
M
L
statin
Austin

Q
null

Original
M
statin

Updated
M
C22 H28 F

(2) We need to use these features to select the node with the
maximum benefit to be labelled, which is then interact with
users to verify the selected node.
We explain in more detail about the implementation of
the above two steps below.

USING EXTERNAL SOURCES

Feature Selection. We generate a number of features for
each node Q and train a Support Vector Machine (SVM)
model with LIBSVM [10]. For a node Q, the features include attribute indicator, attribute value, the original value
before the update, and the updated value. Attribute indicator indicates whether the attribute is included in the node
(rule): if included, the indicator is 1; 0 otherwise. While if
the attribute is being updated, the indicator value is 2.

In this section, we discuss an extension of our system when
external sources are present. Often times, external sources
(e.g., master data) are available and contain high quality
data. Next, we shall discuss how to leverage such information by Falcon.
Consider a dirty table T with schema R and master data
M with schema Rm . Assume without loss of generality that
|R| “ |Rm |, and the alignment of attribute from each attribute A P R to A P R1 is given. For all other attributes in
either relation that are not aligned will be ignored. Moreover, given the update : trAs – a1 , we assume that A P R.
T.A “ M.A1

UPDATE T

SET

FROM

WHERE T rXs “ M rX 1 s

T, M

Question Generation. There are two phases in question
generation. First, in the initial 20 user updates, we use Ducc
to explore the lattice to label the nodes, taking the nodes
(and the corresponding features) from the user labelling as
the training data to train a SVM model that bootstraps the
active learning. Second, in each iteration, we apply the SVM
model to predict the label and corresponding probability of
each node, and select the node with the highest probability of being valid (reported by SVM) to ask users. After
obtaining a label from user, we use lattice pruning technique (discussed in Section 3) to label other nodes in the
lattice and add them to existing training data to re-train
the SVM model.
We illustrate by an example for the active learning
method.

P

Note that, di↵erently from the sqlu queries defined in
Section 2.1, we enforce the condition that A R X. The
reason is that we assume that master data contains only
correct values, but not errors. In such case, the number
of potential queries is 2|R|´1 , which is the number of all
combinations of attributes in RztAu.
From the extension, we can repair errors from instance
level to schema level by using the same lattice and the same
algorithms.

C.

Correlation
1
1
0.822
0.789
0.654
...
0.303
0.006

Table 5: Correlation of attributes in Soccer dataset
when Stadium is updated.

Table 4: Features of node DML.

B.

Attributes Set
{Stadium, Club Country}
{Soccer Manager, Soccer Club }
{Stadium, Soccer Club}
{Stadium, Soccer Manager}
{Stadium, Soccer Club, Soccer Manager}
...
{Stadium, Playercountry, Soccer Club}
{Stadium, Position}

Example 12: Consider node DML in Figure 2. Firstly,
we generate features as illustrated in Table 4. Attribute
indicator of D is 1 because node DML includes attribute
D, indicator of M is 2 since it is the updated attribute.
Attribute values are the corresponding values taken from the
update 3 in Example 1. The original value and updated
value for the update are shown in the table.
Secondly, in each iteration, we apply SVM model to predict the probability of being valid of each node in the lattice.
Suppose node ML has the highest probability to be valid
that is 0.78. We then ask the user to label node ML and,
since in our example the node is valid, we label all nodes
above ML, i.e., {ML, DML, MLQ, DMLQ} to be valid and
we add them to re-train the SVM model.

ACTIVE LEARNING APPROACH

Active learning is a special case of semi-supervised machine learning with the goal of substantially reducing the
number of labelling when training a model. All the labels
for learning are obtained without reference to the learning
algorithm, while in active learning the learner interactively
chooses which data points to label. The hope of active learning is that interaction can substantially reduce the number of
labels required. The method relies on interactively querying
the user for labelling the data trying to maximize the benefit
for the actual learning algorithm.
We adopt a similar idea in our setting, as described below.
In order to use active learning in our problem, i.e., to
predict that which node (or query) in the lattice is valid or
invalid, we face two main issues to be addressed.

D.

(1) We need to generate features for nodes in the lattice,
which are used to capture their characteristics.

Correlation guides the search to nodes that are likely to
have a semantic connection. How to compute correlation

D.1

906

ADDITIONAL EXPERIMENTS
Correlation Score Results

DFS
BFS
Ducc
Dive
CoDive
|QpT q|

Soccer
U
A
11
33
82 246
25
75
15
41
8
19
82

Hospital
U
A
129
387
423
1269
129
387
219
657
206
412
2000

Synth 10k
U
A
177
531
729 2187
70
210
29
87
24
72
1640

Synth 1M
U
A
5094
15282
14035 42105
3083
9249
74
222
74
222
15000

DBLP
U
A
1462 4386
1338 4014
1122 3036
462
1386
140
420
6086

BUS
U
A
3646 10938
4172 12516
3646 10938
312
936
48
144
4172

Table 6: Comparison of the lattice search algorithms with B “ 3: U is the number of user updates, A is the
number of user answers, and |QpT q| is the total number of errors.

CoDive B=5
Refine
Rule Learning
GDR
Active Learning
|QpT q|

Soccer
TC
Rep
49
82
132
82
194
27
225
30
217
82
82

Hospital
TC
Rep
567
2000
4000 2000
315
500
1025
500
2157 2000
2000

Synth 10k
TC
Rep
70
1640
2470 1640
474
1212
1578
943
214
1640
1640

Synth 1M
TC
Rep
394
15000
13326 15000
4800
15000
15000

DBLP
TC
Rep
560
6086
12172 6086
502
0
6086

BUS
TC
Rep
96
4172
7258 4172
1191
757
1220 4172
4172

Table 7: Comparison of the baselines. Here TC is the total interaction cost for the user, Rep is the number of
repaired cells, and |QpT q| is the number of errors.
is discussed in Section 4.2.2 to improve the binary jump
strategy. This is crucial in order to discover set of attributes
that form rules that are worth validating with the user. Note
that if many null values are present, we only count non-null
values, and the attributes with many null values will have
low correlation score. To clarify the role of the correlation
score, consider the following example.
1:

Ducc perform better because of the simple rules that have
been used to model the injection of the errors in the data.
All rules for this dataset have only one or two attributes
in the left hand side of the rules, such as Zip Ñ State or
Address, City Ñ State. In these cases, the correct rules are
at the bottom of the lattice, which is the level that DFS
and Ducc explore first. On the contrary, if our algorithms
miss the correct node at the bottom, they would start exploring the rest of the lattice and converge to the bottom
again slowly, thus with a larger number of questions. Notice
that, as discussed in Exp-2, the closed rule sets optimization
shows a significant improvement on the Hospital scenario for
the DFS algorithm.
We also remark that the Hospital dataset has a number of
rather specific features. This dataset was created by joining
several tables, originally in normal form, in order to obtain a
large number of functional dependencies and redundancy in
the data for testing rule-based data repair algorithms [20,22].
Given the highly denormalized table resulting from these
joins, the dataset should not be considered representative of
a standard data cleaning task.
Table 7 reports the results for CoDive compared with the
baselines. Missing numbers denote cases for which the tool
was stopped after the fixed timeout (two hours) for all tests.
We observe that Rule Learning and GDR were not able to
cover all errors because of the limited scope of the discovered
rules. This is due to the limited size of the sample used
in mining. A larger sample would lead to better results in
terms of recall, but with a higher cost for the collection of the
clean tuples. Refine is always able to detect all errors, but
with a much larger number of interaction because of its less
expressive language, compared to CoDive. Finally, active
learning has worse performance w.r.t. CoDive because of
required training data, and in two cases with large dataset
it was not able to terminate before the timeout.

t1 rStadiums – “Volkswagen Arena” (from “Weserstadion”)

The user updates attribute Stadium. Table 5 shows a summary of the correlation scores, relative to attribute Stadium,
for attributes in the Soccer dataset. Every set of attributes
in the table can be seen as the left hand side attributes to
form a FD. In fact, we know the conclusion of the rule for the
given update (the Stadium attribute), but we do not know
which attributes to use in the premise of the rule. From
the correlation scores, we can deduce that each Stadium usually belongs to one Soccer Club and has one Soccer Manager,
while each Stadium could have many Positions. Thus the correlation score of Stadium, SoccerClub, SoccerManager (row
with rank 5) is much larger than that of Stadium and Position
(row at rank 100). Our algorithm uses this intuition to guide
the search strategy, and the experiment results also verify
that the correlations are e↵ective in avoiding rules that are
unlikely to be validated by the user, such as the one deriving
from row at rank 100.

D.2

More Details on Exp-1 and Exp-3

We now discuss in more details the di↵erent search algorithms and baselines for all datasets.
As reported in Table 6, all search algorithms, with the
exception of BFS, lead to a clean dataset with a number of
user updates U that is smaller than the number of errors in
the data (|QpT q|, reported at the bottom). When considering the user answers (A), their number is from 4 to 68 times
smaller than the cost of manually fixing the errors (without
any rule nor tool), when considering the best performing
algorithm (numbers in bold). In particular, both for number of required updates and for number of required answers,
CoDive is always the method with the lowest e↵ort, with
the exception of the Hospital dataset. In this case, DFS and

907

Data & Knowledge Engineering 68 (2009) 665–682

Contents lists available at ScienceDirect

Data & Knowledge Engineering
journal homepage: www.elsevier.com/locate/datak

Schema exchange: Generic mappings for transforming data and metadata q
Paolo Papotti 1, Riccardo Torlone

*

Università Roma Tre, Dipartimento di Informatica e Automazione, Via della Vasca Navale, 79 – 00146 Roma, Italy

a r t i c l e

i n f o

Article history:
Available online 27 February 2009

Keywords:
Data exchange
Schema exchange
Schema templates
Schema mappings
Dependencies
Universal solutions

a b s t r a c t
In this paper we present and study the problem of schema exchange, a natural extension of
the data exchange problem in which mappings are deﬁned over classes of similar schemas.
To this end, we ﬁrst introduce the notion of schema template, a tool for the representation
of a set of schemas sharing the same structure. We then deﬁne the schema exchange notion
as the problem of: (i) taking a schema that matches a source template, and (ii) generating a
new schema for a target template, on the basis of a mapping between the two templates
deﬁned by means of FO dependencies. This framework allows the deﬁnition, once for all,
of generic transformations that can be applied to different schemas. A method for the generation of a ‘‘correct” solution of the schema exchange problem is proposed and a number
of general results are given. We also show how it is possible to generate automatically,
from a schema exchange solution, a data exchange setting that reﬂects the semantics of
the mappings between the original templates. This allows the deﬁnition of queries to
migrate data from a source database into the one obtained as a result of a schema
exchange.
Ó 2009 Elsevier B.V. All rights reserved.

1. Introduction
In the last years, we have witnessed an increasing complexity of database applications, especially when several, possibly
autonomous data sources need to be accessed, transformed and combined. There is a consequent growing need for advanced
tools and ﬂexible techniques supporting the management, the exchange, and the integration of different and heterogeneous
sources of information.
In this trend, the data exchange problem has received recently great attention, both from a theoretical [12,14] and a practical point of view [27]. In a data exchange scenario, given a mapping between a schema, called the source schema, and another schema, called the target schema, the goal is to take data structured under the source schema and transform it into a
format conforming to the target schema.
In this paper, we address the novel problem of schema exchange, which naturally extends the data exchange process to
sets of similar schemas: while the data exchange process operates over speciﬁc source and target schemas, the goal of schema exchange is rather the deﬁnition of generic transformations of data under structurally similar schemas. To this aim, we
introduce the notion of schema template, which is used to represent a class of different database schemas sharing the same
structure. Then, given a mapping between the components of a source and a target template, the goal is the translation of any
database whose schema conforms to the source template into a format conforming to the target template.

q
A preliminary version this paper appeared, under the title ‘‘Schema Exchange: A Template-Based Approach to Data and Metadata Translation”, in the
Proceedings of the 26th International Conference on Conceptual Modeling (ER), 2007.
* Corresponding author. Tel.: +39 06 5733 3377; fax: +39 06 5733 3612.
E-mail addresses: papotti@dia.uniroma3.it (P. Papotti), torlone@dia.uniroma3.it (R. Torlone).
1
Partially supported by an IBM Faculty Award.

0169-023X/$ - see front matter Ó 2009 Elsevier B.V. All rights reserved.
doi:10.1016/j.datak.2009.02.005

666

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

This framework can be used to support several activities involved in the management of heterogeneous data sources.
First, it allows the deﬁnition, once for all, of ‘‘generic” transformations that work for different but similar schemas, such
as the denormalization of a pair of relation tables based on a foreign key between them. Then, it can support the reuse of
a data exchange setting, since a mapping between templates can be derived from a mapping between schemas for later
use in similar scenarios. Finally, it can be used to implement model translations, that is, translations of schemas and data from
one data model to another (e.g., from relational to XML), a problem largely studied in recent years [1,4,24].
As has been done for data exchange [14], we introduce a formal notion of solution for the schema exchange problem and
present a technique for the automatic generation of solutions. This is done by representing constraints over templates and
mappings between them with a special class of ﬁrst order formulas. These dependencies are used to generate the solution by
applying the chase procedure [2] to an ‘‘encoding” of the source schema. Then, we propose an algorithm for deriving automatically a data exchange setting from a schema exchange solution and we show that this setting reﬂects the semantics of
the mappings deﬁned over templates at a higher level of abstraction. In this way, it is possible to migrate data from a source
database into the database obtained as a result of the schema exchange.
The ﬁnal goal of our research is the development of a design tool in which the user can: (i) describe data collections presenting structural similarities, by means of a source template T 1 , (ii) deﬁne the structure of a possible transformation of the
source by means of a target template T 2 and a set of correspondences over T 1 and T 2 , graphically represented by lines between T 1 and T 2 , (iii) translate any data source over a schema matching with T 1 into a format described by a schema matching with T 2 , (iv) make use of a set of predeﬁned schema exchange settings as design patterns for the development of new data
exchange settings, and (v) convert a data exchange setting into a schema exchange one for its reuse.
The structure of the paper is as follows. In Section 2 we illustrate and motivate the schema exchange problem by means of
a number of practical examples. In Section 3 we set the basic deﬁnitions and recall some useful results on the data exchange
problem. In Section 4 we introduce formally the notion of template and show how templates and schemas are related. In
Section 5, we deﬁne the problem of schema exchange and show how ‘‘correct” solutions can be generated and, in Section
6, we present a method for converting a schema exchange into a data exchange setting. Finally, in Section 7 we compare
our work to existing literature and in Section 8 we draw some conclusions and sketch future directions of research.
2. Motivating examples
A graphical example of a data exchange scenario is reported in Fig. 1. On the left-hand side there is a source schema
involving the relations Emp and Dept linked by a foreign key and, on the right-hand side, a target schema that involves
the single relation Employee. The correspondences between the source and the target, expressed by means of arrows, suggest that the target can be obtained from the source by joining the relations Emp and Dept according to the foreign key, projecting the result on the attributes involved by the arrows, and storing the result into the relation Employee. This
transformation can be speciﬁed, in a precise way, by means of the following ﬁrst order rule, called source-to-target
dependency:

Empðe; n; s; dÞ; Deptðd; dn; bÞ ! Employeeðe; n; s; dn; bÞ
The goal of the given scenario is indeed to derive in an automatic way the queries needed to translate data structured under
the source schema into a format conforming to the target schema, according to the given source-to-target dependencies.
Let us now consider the data exchange setting reported graphically in Fig. 2. The example is different (we now have students and courses) but the underlying transformation to obtain the target is indeed similar: again, we need to ‘‘denormalize”
the source by joining two relations using a referential constraint between them.
The basic idea of our approach is that this transformation can be conveniently represented, in general terms, by means of
a framework, which we have called schema exchange, reported graphically in Fig. 3. In this scenario we have templates of
schema, that is, generic structures representing a set of database schemas with a similar conﬁguration, and a mapping
between them, again informally represented by means of arrows. The aim is to represent a generic transformation that
can be applied to different, but structurally similar, data sources.

Dept (
DId
DeptName
Building
)
Emp (
EmpNo
Name
Salary
Dept
)
Fig. 1. An example of data exchange.

Employee (
EmpNo
Name
Salary
DeptName
Building
)

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

Student(
SId
SName
Enrollment
)
Course(
CId
CName
)

667

Registration(
SId
SName
CName
)

Fig. 2. Another example of data exchange.

Relation(
Name
Key(
Name )
Attribute(
Name )
FKey(
Name )
)
Relation(
Name
Key(
Name )
Attribute(
Name )
)

Relation (
Name
Key(
Name )
Attribute(
Name )
Attribute(
Name )
)

Fig. 3. An example of schema exchange that generalizes the data exchanges in Figs. 1 and 2.

Intuitively, the semantics of this scenario, in the example at hand, is the following. Given a relational schema involving at
least two relations related by a foreign key, for each pair of relations R and R0 such that: (i) R has a key, a non-empty set of
non-key, non-foreign-key attributes and at least a foreign key attribute that refers to (the key of) R0 , and (ii) R0 has a key and a
non-empty set of non-key, non-foreign-key attributes, generate a target relation with the same attributes and the same key
attributes of R, and with the attributes of R0 not involved in the key.
Also in this case, the semantics can be precisely speciﬁed by a source-to-target dependency deﬁned over templates, as
follows:

RelationðnR Þ; KeyðnK ; nR Þ; AttributeðnA ; nR Þ; FKeyðnF ; nR ; n0R Þ; Relationðn0R Þ; Keyðn0K ; n0R Þ; Attributeðn0A ; n0R Þ
! Relationðn00R Þ; KeyðnK ; n00R Þ; AttributeðnA ; n00R Þ; Attributeðn0A ; n00R Þ
We will make clear syntax and semantics of this rule later. Intuitively, it speciﬁes that given a pair of (generic) relations in
the source named nR and n0R , linked by a foreign key named nF , there exists a relation in the target named n00R with the same
key as nR and the attributes of both nR and n0R . Actually, we will show that this rule: (i) captures both the data exchange settings reported in Figs. 1 and 2, and (ii) can be used to generate the source-to-target dependencies reﬂecting the semantics of
the mappings at the schema exchange level, described graphically in Fig. 3.
The notion of schema exchange can be used to support several practical problems related to the exchange of information
between heterogeneous repositories of data:
 First, it can be used to support the design of a data exchange setting, since a schema exchange models naturally a design
pattern for data exchange, that is, a general repeatable solution to a commonly occurring data transformation from a format into another;
 Moreover, it can support the reuse of a data exchange setting, since a schema exchange that generalizes a given data
exchange mapping can be derived and later used to implement a transformation similar to the original one;
 Finally, a schema exchange framework can be used to represent and tackle the problem of model translation [1,4,24], in
which the goal is the translation of schemas and data between heterogeneous data models. In fact, even if we mainly refer
to the relational model, our notion of schema template can represent indeed schemas of a large variety of data models.
The rest of this paper is devoted to the precise deﬁnition of this notion of schema exchange, and the investigation of its
general properties.
We point out that, as we have said in the introduction, the ﬁnal goal of our research is the design of a practical tool, based
on schema exchange, supporting the user in the design, reuse and maintenance of data exchange settings. The aim of the
present study is providing a solid, theoretical basis for this tool.

668

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

3. Background
In this section we illustrate the notions that are at the basis of our work: schemas of the relational model, data dependencies and the problem of data exchange. We note that, in this paper, we make use of ‘‘database” terminology in which
a schema is the description of the database structure and a (data) model provides a set of constructs for the speciﬁcation
of schemas (e.g., schemas are deﬁned as relations in the relational model). In other contexts (for instance in OMG [22]) schemas are called models, and models are called metamodels.
3.1. Schemas and dependencies
A schema S is composed by a set of relations RðA1 ; . . . ; An Þ, where R is the name of the relation and A1 ; . . . ; An are its attributes. Each attribute is associated with a set of values called the domain of the attribute. An instance of a relation RðA1 ; . . . ; An Þ
is a ﬁnite set of tuples, each of which associates with each Ai a value taken from its domain. An instance I of a schema S contains an instance of each relation in S.
A dependency over a schema S is a ﬁrst order formula of the form: 8xð/ðxÞ ! vðxÞÞ where /ðxÞ and vðxÞ are formulas over S,
and x are the free variables of the formula, ranging over the domains of the attributes occurring in S.
We will focus on two special kinds of dependencies: the tuple generating dependencies (tgd) and the equality generating
dependencies (egd), as it is widely accepted that they include all of the naturally-occurring constraints on relational databases. A tgd has the form: 8xð/ðxÞÞ ! 9yðwðx; yÞÞ where /ðxÞ and wðx; yÞ are conjunctions of atomic formulas. If y is empty
(no existentially quantiﬁed variables), then we speak about a full tgd. An egd has the form: 8xð/ðxÞ ! ðx1 ¼ x2 ÞÞ where /ðxÞ is
a conjunction of atomic formulas and x1 , x2 are variables in x. For simplicity, hereinafter we will omit all the quantiﬁers in
formulas assuming that variables occurring in the left-hand size of a formula are universally quantiﬁed and those occurring
only in the right-hand size are existentially quantiﬁed.
Example 1. Given a schema composed by the relations:

DepartmentðDid; DeptNameÞ; EmployeeðEmpno; Name; DeptÞ
a dependency of the form: Departmentðx1 ; x2 Þ; Departmentðx1 ; x02 Þ ! ðx2 ¼ x02 Þ is an egd stating that Did is a key for Department,
whereas a dependency of the form: Employeeðx1 ; x2 ; x3 Þ ! Departmentðx3 ; x4 Þ is a tgd stating that there is a foreign key from
the attribute Dept of Employee to the attribute Did of Department.
3.2. Data exchange
In the relational-to-relational data exchange framework [12], a data exchange setting is described by a four-tuple
M ¼ ðS; T; Rst ; Rt Þ, where:
 S is a source schema,
 T is a target schema,
 Rst is a ﬁnite set of s–t (source-to-target) tgds /ðxÞ ! vðx; yÞ where /ðxÞ is a conjunction of atomic formulas over S and
vðx; yÞ is a conjunction of atomic formulas over T, and
 Rt is a ﬁnite set of tgds or egds over T.
Given an instance I of S, a solution for I under M is an instance J of T such that ðI; JÞ satisﬁes Rst and J satisﬁes Rt . A solution
may have distinct labelled nulls denoting unknown values.
Example 2. The transformation described graphically in Fig. 1 is precisely captured by the following data exchange setting:





S ¼ fEmpðEmpNo; Name; Salary; DeptÞ; DeptðDId; DeptName; BuildingÞg.
T ¼ fEmployeeðEmpNo; Name; Salary; DeptName; BuildingÞg.
Rst ¼ fEmpðx1 ; x2 ; x3 ; x4 Þ; Deptðx4 ; x5 ; x6 Þ ! Employeeðx1 ; x2 ; x3 ; x5 ; x6 Þg.
Rt ¼ fEmployeeðx1 ; x2 ; x3 ; x5 ; x6 Þ; Employeeðx1 ; x02 ; x03 ; x05 ; x06 Þ ! ðx2 ¼ x02 Þ; ðx3 ¼ x03 Þ; ðx5 ¼ x05 Þ; ðx6 ¼ x06 Þg.

Given the following instance I of the source schema S:

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

669

A solution for I under the above data exchange setting is the following instance J of T:

Note that, according to the deﬁnition, other tuples can occur in the target, possibly involving values that do not occur in
the source.
According to the ﬁnal observation in the example above, there are in general many possible solutions for I under M. A
solution J is universal if there is a homomorphism from J to every other solution for I under M. A homomorphism from an
instance J to an instance J 0 is a function h from constant values and nulls occurring in J to constant values and nulls occurring
in J 0 such that: (i) it is the identity on constants (that is, on the values occurring in the instance), and (ii) for each tuple
t ¼ ða1 ; . . . ; ak Þ of J we have that hðtÞ ¼ ðhða1 Þ; . . . ; hðak ÞÞ is a tuple of J 0 .
In [14] it was shown that, when a solution for an instance I under M exists and the dependencies in Rt are either egds or
weakly acyclic tgds (a class of tgds which admits limited forms of cycles among different relation arguments) a universal
solution for I under M can be computed in polynomial time by applying the chase procedure to I using Rst [ Rt . This procedure
requires a preliminary notion: a substitution is a function s from constant values and variables to variables, constant values,
and nulls that is the identity on constants. The chase takes as input an instance I and generates another instance by applying
chase steps based on the given dependencies. There are two kinds of chase steps:
 A tgd /ðxÞ ! wðx; yÞ can be applied to I if there is a substitution s such that the application of s to variables and constants
occurring in each atom of /ðxÞ produces a tuple in I; in this case, the result of its application is I [ s0 ðAÞ, for each atom A
occurring in wðx; yÞ, where s0 is the extension of s to y obtained by assigning fresh labelled nulls to the variables in y;
 An egd /ðxÞ ! ðx1 ¼ x2 Þ can be applied to I if there is a substitution s such that: (i) the application of s to variables and
constants occurring in each atom of /ðxÞ produces a tuple in I and (ii) sðx1 Þ–sðx2 Þ; in this case, the result of its application
is the following: if one of sðx1 Þ and sðx2 Þ is a constant and the other is a null, then the null is changed to the constant, otherwise the nulls are equated unless they are both constants, since in this case the process fails.
The chase of I is obtained by applying all applicable chase steps exhaustively to I.
Example 3. It is easy to see that the tgd in Rst of Example 2 can be applied to the instance I since there is a substitution that
maps x1 to 234, x2 to John, x3 to 50K, x4 to D2, x5 to Management, and x6 to Harrison. The result of the application is the
tuple in the target instance J. This is indeed the only applicable chase step.
Universal solutions are particularly important also for query answering, since Fagin et al. [14] have shown that the (certain) answers to a query q over the target schema can be obtained by evaluating q over any universal solution.
The problem is that, in general, there may exist multiple, different universal solutions for a data exchange problem. However, in [12] it has been shown that there is one particular universal solution, called the core, which is the most compact one.
More speciﬁcally, the core of a universal solution J is (up to isomorphism) the smallest subset J  of J such that J  is homomorphically equivalent to J. Gottlob and Nash [16] have shown that the core of a universal solution of a data exchange problem, whose source-to-target constraints are tgds and whose target constraints consist of egds and weakly acyclic tgds, can be
computed in polynomial time.

4. Schema templates and encoding of schemas
In this section, we introduce the notion of schema template, which is used to encode database schemas sharing the same
structure. We also show how instances of templates, called e-schemas, can be converted to relational schemas and vice
versa.
4.1. Schema templates
We ﬁx a ﬁnite set C of construct names. A construct Cðp1 ; . . . ; pk Þ has a name C in C and a ﬁnite set p1 ; . . . ; pk of distinct
properties, each of which is associated with a set of values called the domain of the property. In principle, the set C can contain constructs from several data models so that we can include in our framework schemas of different models. In this paper
however, for sake of simplicity, we focus on the relational model; the approach can be extended to richer data models, but
challenging issues already arise in this setting.

670

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

Therefore, given a set R of relation names, we ﬁx the following relational constructs and properties. For each construct, the
properties that allow the identiﬁcation of the construct are underlined.
Construct names

Properties (domain)

Relation
Attribute
Key
FKey

name
name
name
name

ðRÞ
(string), nullable (boolean), in ðRÞ
(string), in ðRÞ
(string), in ðRÞ, refer ðRÞ

The Relation construct has only the name property, whose domain is R. The constructs Key and FKey denote attributes that
belong to a primary key and to a foreign key, respectively. The property in of the constructs Attribute, Key and FKey has R as
domain and is used to specify the relation that includes the construct. Finally, the property refer of the construct FKey has also
R as domain and is used to specify the relation it refers to. Clearly, other properties can be considered for every construct and
have not been included to keep the notation simple. For instance, we could associate the properties basic_type and default_value with the construct Attribute.
Basically, a template is a set of constructs with a set of dependencies associated with them, which are used to specify
constraints over single constructs and semantic associations between different constructs.
Deﬁnition 1. Template A (schema) template is a pair ðC; RC Þ, where C is a ﬁnite collection of constructs and RC is a set of
dependencies over C.
Example 4. An example of a template T ¼ ðC; RC Þ contains the following set of constructs:

C ¼ fRelationðnameÞ; Keyðname; inÞ; Attributeðname; nullable; inÞ; FKeyðname; in; referÞg
and the dependencies:

RC ¼ fd1 ¼ KeyðnK ; nR Þ ! RelationðnR Þ; d2 ¼ AttributeðnA ; u; nR Þ ! RelationðnR Þ; d3 ¼ FKeyðnF ; nR ; n0R Þ
! RelationðnR Þ; Relationðn0R Þ; d4 ¼ AttributeðnA ; u; nR Þ ! ðu ¼ trueÞg
The tgds d1 and d2 state the membership of keys and attributes to relations, respectively. The dependency d3 states the membership of a foreign key to a relation and its reference to another relation. Finally, the egd d4 states that we are considering
only relations with attributes that allow null values.
Note that the dependencies d1 , d2 and d3 in Example 4 should always hold in a relational schema. Actually, they can be
considered as axioms of the relational model. For this reason, in the following we will call them relational dependencies and
assume that they always belong to the dependencies associated with a relational template.
Let us now introduce the notion of e-schemas. Basically, an e-schema corresponds to the encoding of a (relational) schema
and is obtained by instantiating a template.
Deﬁnition 2. e-Schemas An e-schema component S over a construct C is a function that associates with each property of C a
value taken from its domain. A e-schema S over a template ðC; RC Þ is a ﬁnite set of e-schema components over constructs in C
that satisfy RC .
Example 5. A valid e-schema for the template of Example 4 is the following:

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

671

It is easy to see that this e-schema represents a relational table EMP with EmpName as key, Salary as attribute and Dept
as foreign key, and a relational table DEPT with DeptNo as key and Building as attribute.
Note that e-schemas in Example 5 is similar to the way in which commercial databases store metadata in catalogs. We
can therefore easily verify whether a relational schema stored in a DBMS matches a given template deﬁnition: this can be
done by querying the catalog of the system and checking the satisfaction of the dependencies.
In the following, an e-schema component over a construct Cðp1 ; . . . ; pk Þ will be called a relation component if C ¼ Relation,
an attribute component if C ¼ Attribute, a key component if C ¼ Key, a foreign key component if C ¼ FKey. Moreover, we will
denote an e-schema component over a construct Cðp1 ; . . . ; pk Þ by Cðp1 : a1 ; . . . ; pk : ak Þ. Alternatively, we will use, for each
construct, a tabular notation with a column for each property.
In the next sections, we describe how the notion of e-schema can be converted into a ‘‘standard” relational schema, and
vice versa.
4.2. Relational decoding
Basically, in order to convert an e-schema into a relational schema we need to deﬁne the formulas describing the semantics of the various e-schema components, according to the intended meaning of the corresponding constructs.
Let S be an e-schema over a template T ¼ ðC; RC Þ. The relational decoding of S is a pair ðS; RS Þ where:
 S contains a relation RðK 1 ; . . . ; K m ; A1 ; . . . ; An ; F 1 ; . . . ; F p Þ for each relation component r 2 S such that:
– R ¼ rðnameÞ,
– K i ¼ ki ðnameÞ ð1 6 i 6 mÞ for each key component ki 2 S such that ki ðinÞ ¼ R,
– Aj ¼ aj ðnameÞ ð0 6 j 6 nÞ for each attribute component aj 2 S such that aj ðinÞ ¼ R,
– F k ¼ fk ðnameÞ ð0 6 k 6 pÞ for each foreign key component fk 2 S such that fk ðinÞ ¼ R.
 RS contains an egd over the relation RðK 1 ; . . . ; K m ; A1 ; . . . ; An Þ 2 S of the form:

Rðx1 ; . . . ; xm ; y1 ; . . . ; yn Þ; Rðx1 ; . . . ; xm ; y01 ; . . . ; y0n Þ ! ðy1 ¼ y01 ; . . . ; yn ¼ y0n Þ
for each relation component r 2 S such that:
– R ¼ rðnameÞ;
– K i ¼ ki ðnameÞ ð1 6 i 6 mÞ for each key component ki 2 S such that ki ðinÞ ¼ R.
 RR contains a tgd over a pair of relation schemas RðA1 ; . . . ; Am ; F 1 ; . . . ; F n Þ and R0 ðK 1 ; . . . ; K n ; A01 ; . . . ; A0p Þ in S of the form:

Rðx1 ; . . . ; xm ; y1 ; . . . ; yn Þ ! R0 ðy1 ; . . . ; yn ; z1 ; . . . ; zp Þ
for each pair of relation components r and r 0 in S such that:
– R ¼ rðnameÞ and R0 ¼ r 0 ðnameÞ;
– F i ¼ fi ðnameÞ ð1 6 i 6 nÞ for each foreign key component fi 2 S such that fi ðinÞ ¼ R and fi ðreferÞ ¼ R0 .

Example 6. Let us consider the e-schema S of Example 5, which is repeated here for convenience:

672

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

The relational representation of S is the pair ðS; RS Þ where:

S ¼ fEMPðEmpName; Salary; DeptÞ; DEPTðDeptNo; BuildingÞg

RS ¼ fEMPðx1 ; x2 ; x3 Þ; EMPðx1 ; x02 ; x03 Þ ! ðx2 ¼ x02 ; x3 ¼ x03 Þ; DEPTðx1 ; x2 Þ; DEPTðx1 ; x02 Þ
! ðx2 ¼ x02 Þ; EMPðx1 ; x2 ; x3 Þ ! DEPTðx3 ; x02 Þg
In the same line, a procedure for the encoding of a relational schema, that is for the transformation of a relational schema
ðS; RS Þ into an e-schema S, can also be deﬁned. This procedure will be illustrated in the following section.
4.3. Relational encoding
Let S be a relational schema with a set of dependencies RS including, as usual, egds and tgds. The encoding of S is an eschema S such that:
 S contains a relation component r for each relation RðA1 ; . . . ; An Þ 2 S such that rðnameÞ ¼ R;
 S contains an attribute component a for each attribute A of a relation R 2 S not involved in an egd or in a tgd RS over R such
that: aðnameÞ ¼ Ai and aðinÞ ¼ R;
 S contains a key component ki ð1 6 i 6 mÞ for each egd in RS over a relation schema RðK 1 ; . . . ; K m ; A1 ; . . . ; An Þ 2 S of the
form:

Rðx1 ; . . . ; xm ; y1 ; . . . ; yn Þ; Rðx1 ; . . . ; xm ; y01 ; . . . ; y0n Þ ! ðy1 ¼ y01 ; . . . ; yn ¼ y0n Þ
such that: ki ðnameÞ ¼ K i and ki ðinÞ ¼ R;
 S contains a foreign key component fi ð1 6 i 6 nÞ for each tgd over a pair of relation schemas RðA1 ; . . . ; Am ; F 1 ; . . . ; F n Þ and
R0 ðK 1 ; . . . ; K n ; A01 ; . . . ; A0p Þ in S of the form:

Rðx1 ; . . . ; xm ; y1 ; . . . ; yn Þ ! R0 ðy1 ; . . . ; yn ; z1 ; . . . ; zp Þ
such that: fi ðnameÞ ¼ F i , f ðinÞ ¼ R, and f ðreferÞ ¼ R0 .
Example 7. Let us consider the relational schema ðS; RS Þ where:

S ¼ fORDERðOrderNo; Item; QuantityÞ; PRODUCTðProdNo; PriceÞg

RS ¼ fORDERðx1 ; x2 ; x3 Þ; ORDERðx1 ; x02 ; x03 Þ ! ðx2 ¼ x02 ; x3 ¼ x03 Þ; PRODUCTðx1 ; x2 Þ; PRODUCTðx1 ; x02 Þ
! ðx2 ¼ x02 Þ; ORDERðx1 ; x2 ; x3 Þ ! PRODUCTðx2 ; x02 Þg
The encoding of ðS; RS Þ is the following e-schema:

5. Schema exchange
In this section we deﬁne the schema exchange problem as the application of the data exchange problem to templates of
schemas.

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

673

5.1. Source-to-target template dependency
In order to investigate the problem of data and metadata translation between templates we introduce the following
notion.
Deﬁnition 3. s–t template dependency Given a source template T1 and a target template T2 , a source-to-target (s–t)
template dependency is a tuple generating dependency of the form: /ðxÞ ! wðx; yÞ, where /ðxÞ is a conjunction of atomic
formulas over the components of C1 and wðx; yÞ is a conjunction of atomic formulas over the components of C2 .
There are two different but equivalent semantics that can be associated with s–t template dependencies. In a declarative
semantics, they represent constraints required to hold on pairs of instances over the source and the target template. Under
this semantics, multiple pairs of e-schemas may satisfy the relationship. Alternatively, under the transformational semantics,
the dependencies are interpreted as transformations from the source to the target: given a source e-schema S over the source
template, there is a procedural way of computing a target e-schema S0 over the target template. This transformational
semantics follows the data exchange line of work and it is the one we adopt in the schema exchange deﬁnition we introduce
next.
5.2. The problem of schema exchange
Given a source template T1 ¼ ðC1 ; RC1 Þ, a target template T2 ¼ ðC2 ; RC2 Þ, and a set RC1 C2 of s–t template dependencies, we
denote a schema exchange setting by the triple ðT1 ; T2 ; RC1 C2 Þ.
Deﬁnition 4. Schema exchange Let ðT1 ; T2 ; RC1 C2 Þ be a schema exchange setting and S1 a source e-schema over ðC1 ; RC1 Þ.
The schema exchange problem consists in ﬁnding a ﬁnite target e-schema S2 over ðC2 ; RC2 Þ such that S1 [ S2 satisﬁes RC1 C2 . In
this case S2 is called a solution for S1 or, simply, a solution.
Example 8. Consider a schema exchange problem in which the source template T1 ¼ ðC1 ; RC1 Þ and the target template
T2 ¼ ðC2 ; RC2 Þ are the following:

C1 ¼ fRelationðnameÞ; Keyðname; inÞ; Attributeðname; inÞg
C2 ¼ fRelationðnameÞ; Keyðname; inÞ; Attributeðname; inÞ; FKeyðname; in; referÞg
with the relational dependencies in RC1 and in RC2 shown in Example 4.
Assume now that we would like to split each relation over T1 into a pair of relations over T2 related by a foreign key and
assign to the ﬁrst relation in the target the name of the source relation. This scenario is graphically shown (informally) in
Fig. 4 and is precisely captured by the following set of tgds RC1 ;C2 :

RC1 ;C2 ¼ fRelationðnR Þ; KeyðnK ; nR Þ; AttributeðnA ; nR Þ
! RelationðnR Þ; KeyðnK ; nR Þ; FKeyðnF ; nR ; n0R Þ; Relationðn0R Þ; KeyðnF ; n0R Þ; AttributeðnA ; n0R Þg
We stress the fact that Fig. 4 just describes, in an easy-to-understand but informal way, a transformation that is instead precisely represented by the above dependency. Other representations could be used in a practical tool with a user friendly
interface. This subject is however outside the goal of this paper which is rather devoted to the study of the fundamental
properties of the schema exchange framework. We also point out that the construct names in the preconditions and in
the conclusions have different meanings, since they refer to the source and the target respectively.

Relation (
Name
Key(
Name )
Attribute(
Name )
)

Relation(
Name
Key(
Name )
FKey(
Name )
)
Relation(
Name
Key(
Name )
Attribute(
Name )
)

Fig. 4. Schema exchange setting for Example 8.

674

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

Consider now the following e-schema S for the template T1 :

According to the decoding procedure described in Section 4.2, this e-schema encodes the relational schema:
S ¼ fEMPðEmpName; DeptName; FloorÞg in which EmpName is the key. A possible solution S01 for this setting is:

where R1 ; R2 ; K1 ; K2 are labelled nulls. The decoding of this solution contains three relations: EMPðEmpName; K1 ; K2 Þ,
R1 ðK1 ; DeptNameÞ, and R2 ðK2 ; FloorÞ, in which the attributes K1 ; K2 of relation EMP are foreign keys for R1 and R2 , respectively. There are several null values because the dependencies in RC1 ;C2 do not allow the complete deﬁnition of the target
e-schema.
Actually many other solutions for the same schema exchange problem exist. For instance the following e-schema S02 :

where R1 and K1 are labelled nulls. By decoding this solution we obtain two relations: EMPðEmpName; K1 Þ and
R1 ðK1 ; DeptName; FloorÞ, where K1 of relation EMP is a foreign key for R1 .

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

675

Two issues arise from Example 8: which solution to choose and how to generate it. Solution S02 in the example seems to be
less general than S01 . This is captured precisely by the notion of homomorphism. In fact, it is easy to see that, while there is a
homomorphism from S01 to S02 ðR2 #R1 ; K 2 #K 1 Þ, there is no homomorphism from S02 to S01 . It follows that S02 contains ‘‘extra”
information whereas S01 is a more general solution. As in data exchange [12,14], we argue that the ‘‘correct” solution is the
most general one, in the sense above. This solution is called universal, as illustrated in the next section.
5.3. Universal solutions and core
The discussion on the possible solutions for a schema exchange problem leads to the following deﬁnition.
Deﬁnition 5. Universal solution A solution S of the schema exchange problem is universal if there exists a homomorphism
from S to all the other solutions.
The next result provides a method for the generation of a universal solution of a given schema exchange problem. It follows from an analogous result for the data exchange problem.
Theorem 1. Let ðT1 ; T2 ; RC1 C2 Þ be a data exchange setting and S1 be an e-schema over T1 and assume that RC2 includes only
weakly acyclic tgds. The chase procedure over S1 using RC1 C2 [ RC2 terminates and generates a universal solution if a solution exists
and fails otherwise.
Proof. A schema exchange setting can be viewed as a data exchange setting over a source schema S1 and a target schema S2
that involve a relation for each construct of T1 and T2 , respectively. Under this view, S1 is an instance of S1 . It follows that
the results on data exchange shown in [14] can be applied to our scenario. In particular, the fact that, if a solution exists, the
application of the chase procedure to an instance of S1 using the given dependencies terminates and generates a universal
solution. In this case, by construction, the output is an instance of T2 and, by deﬁnition, is also a universal solution of the
given schema exchange problem. h
Theorem 1 provides a procedural way to generate a universal solution of a schema exchange problem. However, what we
obtain is only one of the possible universal solutions and it turns out that, in general, it is not necessarily the best in terms of
size. This is clariﬁed in the following example.
Example 9. Consider a schema exchange problem in which the source template T1 ¼ ðC1 ; RC1 Þ and the target template
T2 ¼ ðC2 ; RC2 Þ are the following:

C1 ¼ fRelationðnameÞ; Keyðname; inÞ; Attributeðname; inÞg
C2 ¼ fRelationðnameÞ; Keyðname; inÞg
with the usual relational constraints in RC1 and in RC2 shown in Example 4. Note that the second template models schemas
with only key attributes.
Assume now that we would like just to copy each relation over T1 into a relation over T2 having a new key. This
transformation can be implemented by the following tgd:

RC1 ;C2 ¼ fRelationðnR Þ; KeyðnK ; nR Þ; AttributeðnA ; nR Þ ! RelationðnR Þ; Keyðn0K ; nR Þg
Note that the rule is applicable only if the relation in the source contains at least one key and one attribute, even if the latter
is not used in the target. Let us consider the following e-schema S over T1 :

If we apply the chase over S using over RC1 ;C2 we obtain the following universal solution S0 :

676

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

where K1 and K2 are labelled nulls that represent any key attribute names: they form together a composite key for the relation ORDER.
However, it is easy to see that the following e-schema S00 is also a universal solution:

Note that S00 is homomorphically equivalent to S0 (that is, there is a homomorphism from S0 to S00 and vice versa) but it is
more compact than S0 .
Solutions S0 and S00 of Example 9 are both universal but S00 is clearly preferable to S0 since it is smaller in size. This solution
is actually the core of all the universal solutions for S. It has been shown that the core is unique, as all universal solutions for a
schema exchange problem have the same core up to isomorphism [12]. The core for a schema exchange problem with only
egd as target constraints can be computed in naive way by successively removing tuples from a universal solution, as long as
the solution resulting in each step satisfy the dependencies. Recently, Gottlob and Nash [16] have proposed a polynomial
time algorithm that computes the core of a universal solution of a data exchange problem whose source-to-target constraints are tgds and whose target constraints consist of egds and weakly acyclic tgds. Hereinafter, we will only refer to
the core of universal solutions.

6. From schema to data exchange
In this section we propose a transformation process that generates a data exchange setting as an instance of a schema
exchange setting for a given data source.
6.1. Metalinks and S–D transformation process
Before discussing the transformation process, a preliminary notion is needed. In order to convert the schema exchange
setting into a data exchange setting, we need to keep track of the correspondences between the source schema and the solution of the schema exchange problem. This can be seen as an application of the data provenance problem to schema exchange. To this end, by extending to our context a notion introduced in [11], we introduce the notion of metalink to
describe the relationships between source and target metadata.
Deﬁnition 6. Metalink Let S be an e-schema and R be a set of s–t template dependencies. A metalink for S is an expression of
the form I!r;s I0 where I # S and I0 is the result of the application of a chase step on I based on the dependency r 2 R and the
substitution s.
Note that, since a reduced number of elements are involved in schema exchange, we can store all the metalinks and we do
not need to compute them partially and incrementally as in [11].
Given a relational database over a schema S1 and a schema exchange setting ðT1 ; T2 ; RC1 C2 Þ such that the encoding S1 of
S1 is an instance of T1 , we aim at generating a target database over a schema S2 such that the encoding S2 of S2 is a universal
solution for S1 . We call such process S–D transformation and it can be summarized as follows.
Input:
Output:
(1)
(2)

A schema S1 with constraint RS1 and a schema exchange setting ðT1 ; T2 ; RC1 C2 Þ;
A data exchange setting ðS1 ; S2 ; RS1 S2 ; RS2 Þ;
Encode ðS1 ; RS1 Þ into an e-schema S1 ;
Apply the chase procedure to S1 using RC1 C2 and save the metalinks in a set M during the execution: each chase step based on the dependency
r 2 R with a substitution s adds to M a metalink I!r;s I0 ;
(3) Decode the result S2 of the chase procedure into a schema S2 with constraints RS2 ;
(4) For each metalink I!r;s I0 in M:
(a) let L be the set of relations in S1 mentioned in I: annotate all the attributes A in L with s1 ðAÞ;
(b) let R be the set of relations in S2 mentioned in I0 : annotate all the attributes A0 in R with s1 ðA0 Þ;
(c) replace all the attributes in L and R with fresh variables by associating the same variable to the attributes with the same annotation according to the constraints in RS1 and RS2 ;
(d) add the tgd L ! R to a set RS1 S2 ;
(5) Return the data exchange setting ðS1 ; S2 ; RS1 S2 ; RS2 Þ.

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

Relation(
Name
Key(
Name )
FKey(
Name )
Attribute(
Name )
)
Relation(
Name
Key(
Name )
Attribute(
Name )
)

677

Relation (
Name
Key(
Name )
Attribute(
Name )
Attribute(
Name )
Attribute(
Name )
)

Fig. 5. Schema exchange scenario for Example 10.

Example 10. Let us consider the schema exchange setting described graphically in Fig. 5 and represented by the following
set of tgds RC1 ;C2 :

fv1 ¼ RelationðnR Þ; KeyðnK ; nR Þ; FKeyðnF ; nR ; n0R Þ; AttributeðnA ; nR Þ; Relationðn0R Þ; Keyðn0K ; n0R Þ; Attributeðn0A ; n0R Þ
! RelationðnR Þ; KeyðnK ; nR Þ; AttributeðnF ; nR Þ; AttributeðnA ; nR Þ; Attributeðn0A ; nR Þg
Intuitively, the dependency occurring in RC1 ;C2 speciﬁes that the target is obtained by joining two source relations according to a foreign key deﬁned between them. Now consider the following source schema:

S ¼ fEMPðeid; name; didÞ; DEPTðdid; dnameÞg;

RS ¼ fEMPðx1 ; x2 ; x3 Þ; EMPðx1 ; x02 ; x03 Þ ! ðx2 ¼ x02 ; x3 ¼ x03 Þ; DEPTðx1 ; x2 Þ; DEPTðx1 ; x02 Þ ! ðx2 ¼ x02 Þ; EMPðx1 ; x2 ; x3 Þ ! DEPTðx3 ; x01 Þg
The encoding of S is the e-schema S that follows:

Let fs1 ; . . . ; s7 g be the e-components of S. The application of the chase based on the given tgd produces the set of e-schema
components ft1 ; . . . ; t 5 g:

678

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

The metalink generated by this chase step is: fs1 ; . . . ; s7 g!v1 ;s1 ft1 ; . . . ; t5 g, where s1 is the substitution:

fnR #EMP; nK #eid; nF #did; nA #name; n0R #DEPT; n0K #did; n0A #dnameg
The chase ends successfully and produces an e-schema S0 whose decoding is the schema ðS0 ; RS0 Þ where:

S0 ¼ fEMPðeid; name; did; dnameÞg

RS0 ¼ fEMPðx1 ; x2 ; x3 ; x4 Þ; EMPðx1 ; x02 ; x03 ; x04 Þ ! ðx2 ¼ x02 ; x3 ¼ x03 ; x4 ¼ x04 Þg
Now, on the basis of the above metalink, we derive the following sets of annotated relations:

L ¼ fEMPðeid½nK ; name½nA ; did½nF Þ; DEPTðdid½n0K ; dname½n0A Þg
R ¼ fEMPðeid½nK ; name½nA ; did½nF ; dname½n0A Þg
By replacing all the attributes in L and R with variables in such a way that the attributes with the same annotation are replaced with the same variable, we obtain the following s–t tgd:

d1 ¼ S:EMPðx1 ; x2 ; x3 Þ; S:DEPTðx3 ; x4 Þ ! S0 :EMPðx1 ; x2 ; x3 ; x4 Þ
The ﬁnal data mapping scenario is reported graphically in Fig. 6.
As a ﬁnal comment, we note that the example can be naturally extended to a more complex source schema. If, for
instance, the relation EMP had two attributes, the tgd v1 would ﬁre twice, producing in the second chase step a further
attribute schema component for EMP in the target e-schema, and therefore another attribute in the target schema.
6.2. Properties of the S–D transformation process
A number of general results on the S–D transformation process can be shown. First, the fact that the output of the process
is a ‘‘correct” result, that is, the solution of the data exchange problem reﬂects the semantics of the schema exchange problem given as input. In order to introduce the concept of correctness in this context, a preliminary notion is needed.
Given an s–t tgd d over relational schemas, the encoding of d is a tgd over schema templates obtained by applying a procedure similar to the one deﬁned in Section 4.3 for schemas.
More precisely, let d be an s–t tgd over a source schema S and a target schema T, and let RS and RT be two set of dependencies over S and T, respectively.
^ such that:
The encoding of d is an s–t template tgd d
^ contains an atom of the form RelationðRÞ for each relation R occurring in the
 the left-hand side (right-hand side) of the tgd d
lhs (rhs) of d;
^ contains a set of atoms of the form KeyðK ; RÞ ð1 6 i 6 mÞ for each relation RðK 1 ; . . . ; K m ; A1 ; . . . ; An Þ occur the lhs (rhs) of d
i
ring in d such that there is an egd:

Rðx1 ; . . . ; xm ; y1 ; . . . ; yn Þ; Rðx1 ; . . . ; xm ; y01 ; . . . ; y0n Þ ! ðy1 ¼ y01 ; . . . ; yn ¼ y0n Þ
in RS ðRT Þ;
^ contains a set of atoms of the form FKeyðF ; R; R0 Þ ð1 6 i 6 nÞ for each relation RðA1 ; . . . ; Am ; F 1 ; . . . ; F n Þ
 the lhs (rhs) of d
i
occurring in d such that there is a tgd:

Rðx1 ; . . . ; xm ; y1 ; . . . ; yn Þ ! R0 ðy1 ; . . . ; yn ; z1 ; . . . ; zp Þ
in RS ðRT Þ;
^ contains a set of atoms of the form AttributeðA ; RÞ ð1 6 i 6 p 6 nÞ for each relation RðK 1 ; . . . ; K m ; A1 ; . . . ; An Þ
 the lhs (rhs) of d
i
occurring in the lhs (rhs) of d such that Ai in not involved in the rhs of an egd in RS ðRT Þ or in both the lhs and rhs of a tgd in
RS ðRT Þ.

EMP(
eid
name
did
)
DEPT(
did
dname
)

EMP(
eid
name
did
dname
)

Fig. 6. Data exchange scenario for Example 10.

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

679

Example 11. Let us consider the tgd d1 of Example 10, which is reported here for convenience:

d1 ¼ S:EMPðx1 ; x2 ; x3 Þ; S:DEPTðx3 ; x4 Þ ! S0 :EMPðx1 ; x2 ; x3 ; x4 Þ
The encoding of d1 is the following s–t template tgd.

v2 ¼ RelationðEMPÞ; Keyðeid; EMPÞ; Attributeðname; EMPÞ; FKeyðdid; EMP; DEPTÞ;
RelationðDEPTÞ; Keyðdid; DEPTÞ; Attributeðdname; DEPTÞ ! RelationðEMPÞ; Keyðeid; EMPÞ;
Attributeðname; EMPÞ; Attributeðdid; EMPÞ; Attributeðdname; EMPÞ
The tgd v2 in the example above is less general than the original tgd v1 for the schema exchange scenario described in Example 10. However, it generates the same output S0 on the given input S. This exactly captures the fact that the data exchange
problem obtained as output captures the semantics of the schema exchange problem given as input.
This intuition is captured by the following correctness result.
Theorem 2. Let ðS; S0 ; RSS0 Þ be the output of the S–D transformation process when ðT1 ; T2 ; RC1 C2 Þ and S are given as input and let
b be the set of s–t tgds obtained by encoding the s–t tgds in R 0 . The encoding S0 of S0 is a universal solution for the encoding S of S
R
SS
b Þ.
under the schema exchange setting ðT1 ; T2 ; R
Proof. The e-schema S0 is obtained in step 2 of the S–D transformation process by chasing the encoding S of S using the
dependencies in RC1 C2 . The proof proceeds by induction on the number n of chase steps needed to generate S0 . Speciﬁcally,
we show that, for every 0 6 i 6 n, indicating with Si the e-schema produced after the ith step in chasing S using dependenb
b such that Si is
cies in RC1 C2 , there is an e-schema Si produced as an intermediate result in chasing S using dependencies in R
bi
n
c
b Þ, if there is a
homomorphic to S . Since, by Theorem 1, if a solution exists, S is a universal solution for S under ðT1 ; T2 ; R
b Þ and the claim follows.
Sn then the former is also a universal solution for S under ðT1 ; T2 ; R
homomorphism from Sn ¼ S0 to c
b are empty by construcThe basis is immediate since, for n ¼ 0, the e-schema S0 as well as the sets of dependencies RSS0 and R
b Þ. With respect to the induction, assume that there is a homotion, and so S0 is also a trivial universal solution for ðT1 ; T2 ; R
i
d
morphism h from Si1 to Si1 and let Si1 !r;s Si be the metalink corresponding to the ith chase step over S based on the tgd
r 2 RC1 C2 and the substitution s. The result of the application of this chase step is, by deﬁnition of chase step, Si ¼ Si1 [ sðRÞ,
where R is the right-hand side of r. By construction (step 4 of the S–D transformation process), there is a tgd r0 2 RSS0 which
is built from the above metalink by associating the same variable to the attributes of S and S0 that are mapped by r. Since the
^ . Moreover, by
encoding of a tgd preserves the bindings of the variables, it follows that there is a substitution s from r to r
^ to the encoding S of
deﬁnition of encoding of a tgd it easily follows that there is a substitution ^s from the left-hand side b
L of r
i1
^ to Sd
.
S. Since S is not modiﬁed by the chase procedure, we have that ^s is also a substitution from the left-hand side ofr
d
i1
^
^
Hence, we can apply a chase step based on r and s to S . The result of the application of this chase step is, by deﬁnition,
b
d
i1
b where R
b is the right-hand side of r
b ¼ s ðRÞ, we have that Sbi ¼ Sd
^ , and since R
Si ¼ Si1 [ ^sð RÞ,
[ ^sðs ðRÞÞ. Now let L be the
i
i
d
left-hand side of r: since, by the inductive hypothesis, there is a homomorphism h from Si1 and Si1 , we have that h  s is a
d
d
L, and ^s, from b
L to Si1 , is
homomorphism from L to Si1 . At the same time, the composition of the substitutions s , from L to b
i
d
i1

also a substitution from L to S . It follows that, on the variables occurring in L and R, h  s ¼ ^s  s . Hence, we have that
i
i
i
i
d
b
b
h ðSi Þ ¼ h ðSi1 Þ [ h ðsðRÞÞ ¼ Si1 [ ^sðs ðRÞÞ ¼ Si , and so h is a homomorphism from Si to Si . h
The following completeness result can also be shown. We say that a data exchange setting is constant-free if no constants
are used in formulas.
Theorem 3. Any constant-free data exchange setting can be obtained from the S–D transformation process over some schema
exchange setting.
Proof. Given a data exchange setting M ¼ ðS; S0 ; RSS0 Þ we can derive a schema exchange setting ðT1 ; T2 ; RC1 C2 Þ from which M
can be obtained with the S–D transformation process by: (i) deﬁning two templates T1 and T2 such that S is an instance of
T1 and S0 is an instance of T2 (this can be easily done on the basis of the structure of the encodings of S and S0 ), and (ii)
encoding the tgds in RSS0 . The schema exchange setting we obtain is already suitable for our purposes, but it can be made
even more general by replacing each constant with a unique variable. It is also easy to see that this cannot be done if constants appear in RSS0 since the encoding procedure does not consider them. h
Intuitively, the above theorem states that a schema exchange can be generalized by substituting distinct variables for
constants and since the S–D transformation does not handle constants the data exchange setting must be constant-free.
7. Related work
To our knowledge, the notion of schema exchange studied in this paper is new. In general, we can say that our contribution can be set in the framework of metadata management. Metadata can generally be thought as information that describes,
or supplements, actual data. Several studies have addressed metadata related problems, such as, interoperability [18,23,29],

680

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

annotations and comments on data [7,10,15], data provenance [9], and a large list of more speciﬁc problems, like data quality
[20]. While the list is not exhaustive, it witnesses the large interest in this important area and the different facets of the
problem.
Most of the proposed approaches focus on a speciﬁc kind of metadata and are not directly applicable to other cases without major modiﬁcations. Bernstein set the various problems within a very general framework called model management [3–
5]. In [6] the authors show the value of this framework to approach several metadata related problems, with a signiﬁcant
reduction of programming effort. Our contribution goes in this direction: as in model management, schemas and mappings
are treated as ﬁrst class citizens.
In particular, the schema exchange problem offers some novel research opportunity in the context of the ModelGen operator. The ModelGen operator realizes a schema translation from a source data model M s to a target data model M t and returns
an executable mapping (or a transformational script) for the translation of the instances of the source schema. For instance,
the ModelGen operator could be used to translate a relational database into a schema for an XML document (e.g., a DTD).
ModelGen has been implemented in commercial products as a non-generic way to translate schemas between speciﬁc pairs
of data models. In particular, the most supported scenario in available tools is the translation of ER diagrams into relational
schemas.
Several proposals for the general problem have also been proposed in the last years [1,21,25]. All the frameworks
share the same approach: (i) the system rewrites a source schema S into a representation S0 for a universal metamodel;
(ii) a sequence of rule-based transformations modiﬁes S0 to translate or eliminate the constructs that are not allowed in
the target data model; (iii) after n transformations, S0 can be rewritten into the representation for the target data model.
It is evident that in this translation process a crucial role is done by the rule-based transformations. Surprisingly all the
frameworks lack a tool for the design of such transformations at the data model level. In this paper, we provide a novel
contribution to this problem by studying a framework for schema translation with a clear and precise semantics, that can
be at the basis of an innovative tool supporting the design and the automatic generation of transformations over
schemas.
It is also important to locate schema exchange in the context of data exchange. Our work is largely inspired by the theoretical foundations explored by Fagin et al. in the last years [12,14]. We remark that schema exchange is the ﬁrst proposal to
extend their results to metadata and to introduce the novel notion of encoding of schemas and tgds. One of the most important practical contributions in data exchange are the algorithms for the generation of the tgds representing the schema mappings [13,27]. Those algorithms take as input the schemas (with their constraints) and the arrows between the elements of
the schemas (named correspondences in literature). We point out that in many practical settings the target schema does not
exist, and it must be designed from scratch with a manual process before designing the mapping. This means that the user
must deal with at least two manual steps: (i) the deﬁnition of the target schema and (ii) the deﬁnition of the correspondences between the elements. Many attempts have been done to automate the generation of the target schema
[1,8,19,21,24,25] and the identiﬁcation of the correspondences between different schemas (the schema matching problem,
see [30] for a survey). Proposed solutions gave important but partial contributions: in general settings, with existing tools
it is not possible to avoid the manual time-consuming work we have highlighted. Our approach effectively tackles this problem. As we have shown, in many cases it is possible to deﬁne generic transformations over templates of schemas and the use
of these transformations can support the activity described above. Finally, we mention that the notion of template has been
used in many contexts to support the design and reuse of software components [28].
Some of the results of this paper have been presented, in a preliminary form, in [26]. In this paper we have extended our
earlier work in several ways: we have added a lot of practical examples to make more clear the various notions and results;
we have added a new section (Section 2) that illustrates a motivating scenario and provides an overview of the approach; we
have reﬁned the notions of schema exchange, encoding and decoding of e-schemas, s–t dependencies and metaroute (now
called metalink); we have introduced a new and more general S–D transformation process (now the new algorithm is selfcontained as it does not refer to notions and techniques deﬁned elsewhere); we have included the proofs for all the results;
we have added a new section (Section 7) discussing related works.

8. Conclusion and future work
In this paper, we have introduced and studied the schema exchange problem, a generalization of data exchange to sets of
schemas with similar structure. This problem consists of taking a schema that matches a source template and generating a
new schema for a target template, on the basis of a set of dependencies deﬁned over the two templates. To tackle this problem, we have presented a method for the generation of a ‘‘correct” solution of the problem and a process aimed at automatically generating a data exchange setting from a schema exchange solution.
As a follow-up to this theoretical study, we have developed a prototype, called GAIA, that relies on the theoretical framework illustrated in this paper and supports several activities involved in the management of heterogeneous data sources; in
particular, the design and reuse of schema mappings. Speciﬁcally, GAIA provides a graphical interface that allows the user to:
(i) describe data collections presenting structural similarities, by means of a source template T 1 , (ii) deﬁne the structure of a
possible transformation of the source by means of a target template T 2 and a set correspondences between T 1 and T 2 ,
graphically represented by lines, (iii) translate any data source over a schema matching with T 1 into a format described

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

681

by a schema matching with T 2 , (iv) make use of a set of predeﬁned template mappings as design patterns for the development
of new schema mappings, and (v) convert a schema mapping into a template mapping to be reused in a different application.
GAIA asks user intervention when, for instance, there is the need to choose appropriate names for nulls occurring in schemas.
We have successfully tested this tool in a number of cases from industrial settings. Further information on this tool can be
found in: http://gaia.dia.uniroma3.it/.
We believe that other interesting directions of research can be pursued within the schema exchange settings. We sketch
some of them:
 Combining data and metadata. The framework we have presented can be extended to support mappings and constraints
involving data and metadata at the same time [17]. This scenario allows the user to specify the transformation of metadata
into data and vice versa. For instance, we could move the name of a relational attribute into a tuple of a relation, or we
could generate a target schema with a set of attributes identiﬁed by some data in the source. It is evident that, adding
support to data and metadata in the same dependency, is a required step to extend the class of data exchange settings
that can be generated in the S–D transformation process.
 Reuse of existing data exchange. We have developed a preliminary technique for generalizing data exchange mappings
given by the user into a generic mapping over templates. The idea is to extract the design pattern for each data exchange
scenario, and possibly ﬁnd common patterns when the given scenarios are two or more. Once a transformation is
expressed in a schema exchange setting, it can be later used to derive a data exchange transformation similar to the original one for a different pair of schemas.
 Metaquerying. A template is actually a schema and it can therefore be queried. A query over a template is indeed a metaquery since it operates over metadata. There are a number of metaqueries that are meaningful. For instance, we can
retrieve with a query over a template the pairs of relations that can be joined, being related by a foreign key. Also, we
can verify whether there is a join path between two relations.
 Special class of solutions. Given a schema exchange problem, can we verify whether all the solutions of the problem satisfy
some relevant property? For instance, we would like to obtain only relations that are acyclic or satisfy some normal form.
We are also investigating under which conditions a schema exchange problem generates a data exchange setting with certain properties, e.g., the fact that the dependencies belong to some relevant classes.

References
[1] P. Atzeni, P. Cappellari, P.A. Bernstein, Model-independent schema and data translation, in: International Conference on Extending Database
Technology (EDBT), 2006, pp. 368–385.
[2] C. Beeri, M.Y. Vardi, A proof procedure for data dependencies, Journal of ACM 31 (4) (1984) 718–741.
[3] P.A. Bernstein, Applying model management to classical meta data problems, in: Conference on Innovative Data Systems Research (CIDR), 2003, pp.
209–220.
[4] P.A. Bernstein, S. Melnik, Model Management 2.0: Manipulating Richer Mappings, in: SIGMOD Conference, 2007, pp. 1–12.
[5] P.A. Bernstein, A.Y. Levy, R.A. Pottinger, A vision for management of complex models, SIGMOD Record 29 (4) (2000) 55–63.
[6] P.A. Bernstein, E. Rahm, Data warehouse scenarios for model management, in: International Conference on Conceptual Modeling (ER), 2000, pp. 1–15.
[7] D. Bhagwat, L. Chiticariu, W.C. Tan, G. Vijayvargiya, An annotation management system for relational databases, in: International Conference on Very
Large Data Bases (VLDB), 2004, pp. 900–911.
[8] S. Bowers, L.M.L. Delcambre, The uni-level description: a uniform framework for representing information in multiple data models, in: International
Conference on Conceptual Modeling (ER), 2003, pp. 45–58.
[9] P. Buneman, S. Khanna, W.C. Tan, Why and where: a characterization of data provenance, in: International Conference on Database Theory (ICDT),
2001, pp. 316–330.
[10] P. Buneman, S. Khanna, W.C. Tan, On propagation of deletion and annotations through views, in: Symposium on Principles of Database Systems (PODS),
2002, pp. 150–158.
[11] L. Chiticariu, W.C. Tan, Debugging schema mappings with routes, in: International Conference on Very Large Data Bases (VLDB), 2006, pp. 79–90.
[12] R. Fagin, P.G. Kolaitis, L. Popa, Data exchange: getting to the core, ACM Transaction on Database Systems 30 (1) (2005) 174–210.
[13] A. Fuxman, M.A. Hernández, H. Ho, R.J. Miller, P. Papotti, L. Popa, Nested mappings: schema mapping reloaded, in: International Conference on Very
Large Data Bases (VLDB), 2006, pp. 67–78.
[14] R. Fagin, P.G. Kolaitis, R.J. Miller, L. Popa, Data exchange: semantics and query answering, Theoretical Computer Science 336 (1) (2005) 89–124.
[15] F. Geerts, A. Kementsietsidis, D. Milano, MONDRIAN: annotating and querying databases through colors and blocks, in: International Conference on
Data Engineering (ICDE), 2006, pp. 82–93.
[16] G.Gottlob, A. Nash, Data exchange: computing cores in polynomial time, in: Symposium on Principles of Database Systems (PODS), 2006, pp. 40–49.
[17] M.A. Hernández, P. Papotti, W.C. Tan, Data exchange with data–metadata translations, Proceedings of the VLDB Endowment 1 (1) (2008) 260–273.
[18] L.V.S. Lakshmanan, F. Sadri, S.N. Subramanian, SchemaSQL: an extension to SQL for multidatabase interoperability, ACM Transaction on Database
Systems 26 (4) (2001) 476–519.
[19] P. McBrien, A. Poulovassilis, Data integration by bi-directional schema transformation rules, in: International Conference on Data Engineering (ICDE),
2003, pp. 227–238.
[20] G. Mihaila, L. Raschid, M.-E. Vidal, Querying ‘‘quality of data” metadata, in: IEEE Meta-Data Conference, 1999.
[21] P. Mork, P.A. Bernstein, S. Melnik, Teaching a schema translator to produce O/R views, in: International Conference on Conceptual Modeling (ER), 2007,
pp. 102–119.
[22] OMG Model Driven Architecture, Internet document, 2001. <http://www.omg.org/mda/>.
[23] OASIS XRI Data Interchange (XDI), Internet document, 2008. <http://www.oasis-open.org/>.
[24] P. Papotti, R. Torlone, Heterogeneous data translation through XML conversion, Journal of Web Engineering 4 (3) (2005) 189–204.
[25] P. Papotti, R. Torlone, Automatic generation of model translations, in: International Conference on Advanced Information Systems Engineering (CAiSE),
2007, pp. 36–50.
[26] P. Papotti, R. Torlone, Schema exchange: a template-based approach to data and metadata translation, in: International Conference on Conceptual
Modeling (ER), 2007, pp. 323–337.

682

P. Papotti, R. Torlone / Data & Knowledge Engineering 68 (2009) 665–682

[27] L. Popa, Y. Velegrakis, R.J. Miller, M.A. Hernández, R. Fagin, Translating web data, in: International Conference on Very Large Data Bases (VLDB), 2002,
pp. 598–609.
[28] S. Purao, V. Storey, A. Sengupta, M. Moore, Reconciling and cleansing: an approach to inducing domain models, in: International Workshop on
Information Systems and Technologies (WITS), 2000, pp. 61–66.
[29] C.M. Wyss, E. Robertson, Relational languages for metadata integration, ACM Transaction on Database Systems 30 (2) (2005) 624–660.
[30] E. Rahm, P.A. Bernstein, A survey of approaches to automatic schema matching, VLDB Journal 10 (4) (2001) 334–350.

Paolo Papotti is currently a Post Doctoral Researcher at the Department of Computer Engineering and Automation at Università
Roma Tre. He has been a visiting scientist at the IBM Almaden research center (CA, USA) and a Post Doc. Scholar at the University
of California Santa Cruz (USA).
His research interests are in data integration and data exchange. In particular, he is interested in techniques that improve and
make automatic the process of searching and organizing data. His work has been presented in workshops, journals, and all the
major international conferences of the ﬁeld, including ACM-Sigmod, VLDB and IEEE-ICDE. He has submitted two invention
disclosures at the US Patent of Technology and he has served as reviewer for international conferences, workshops and journals.

Riccardo Torlone is a full professor in the area of Information Systems at Università Roma Tre. He received his Dr. Ing. degree in
Electrical Engineering from Università di Roma ‘‘La Sapienza”. Before joining Università Roma Tre, he was member of the
research staff at IASI-CNR in Rome, where he has still a research appointment. He also had a visiting research appointment at the
University of California Los Angeles.
His research has considered various topics in the database ﬁeld, including: relational database theory, active and deductive
databases, tools for database design, models and languages for object-oriented databases, data warehouses and OLAP systems,
Web based information systems, integration and transformation of heterogeneous data sources, adaptive systems and personalization. He has published his research results in the major journals of the ﬁeld and in the refereed proceedings of all the
major conferences.
He has authored the most spread book on databases in Italy, published also in an international edition and in several versions,
and has authored two other books. He has organized many national and international scientiﬁc events (conferences and schools)
and is currently serving as a member of the EDBT Association. He is member of the editorial board of an international journal on
computer science and has been program committee member for many international conferences. He is currently the chair of the graduate and undergraduate programs in Computer Engineering at Università Roma Tre.

Detecting Data Errors:
Where are we and what needs to be done?
∗

Ziawasch Abedjan§ Xu Chu` Dong Deng‡
Raul Castro Fernandez§ Ihab F. Ilyas`
]
∞
Mourad Ouzzani Paolo Papotti Michael Stonebraker§ Nan Tang]
§
‡

{abedjan,

MIT CSAIL ` University of Waterloo ∞ Arizona State University
Tsinghua University ] Qatar Computing Research Institute, HBKU

ddong, raulcf, stonebraker}@csail.mit.edu {x4chu, ilyas}@uwaterloo.ca
{mouzzani, ntang}@qf.org.qa ppapotti@asu.edu

ABSTRACT

19, 29]. With the increasing prevalence of data-centric approaches to business and scientific problems with data as a
crucial asset, data cleaning has become even more important. Hence, it is important to study the effectiveness of
the available error detection solutions (both as industrystrength tools and academic prototypes) in solving realworld cleaning problems.

Data cleaning has played a critical role in ensuring data
quality for enterprise applications. Naturally, there has been
extensive research in this area, and many data cleaning algorithms have been translated into tools to detect and to possibly repair certain classes of errors such as outliers, duplicates, missing values, and violations of integrity constraints.
Since different types of errors may coexist in the same data
set, we often need to run more than one kind of tool. In this
paper, we investigate two pragmatic questions: (1) are these
tools robust enough to capture most errors in real-world data
sets? and (2) what is the best strategy to holistically run multiple tools to optimize the detection effort? To answer these
two questions, we obtained multiple data cleaning tools that
utilize a variety of error detection techniques. We also collected five real-world data sets, for which we could obtain
both the raw data and the ground truth on existing errors.
In this paper, we report our experimental findings on the
errors detected by the tools we tested. First, we show that
the coverage of each tool is well below 100%. Second, we
show that the order in which multiple tools are run makes
a big difference. Hence, we propose a holistic multi-tool
strategy that orders the invocations of the available tools
to maximize their benefit, while minimizing human effort
in verifying results. Third, since this holistic approach still
does not lead to acceptable error coverage, we discuss two
simple strategies that have the potential to improve the situation, namely domain specific tools and data enrichment.
We close this paper by reasoning about the errors that are
not detectable by any of the tools we tested.

1.

1.1

The Current State

Available data cleaning solutions and tools belong to one
or more of the following four categories:
• Rule-based detection algorithms [1, 6, 15, 16, 25, 33] that
can be embedded into frameworks, such as Nadeef [8,
23], where a rule can vary from a simple “not null”
constraint to multi-attribute functional dependencies
(FDs) to user-defined functions. Using this class of
tools, a user can specify a collection of rules that clean
data will obey, and the tool will find any violations.
• Pattern enforcement and transformation tools such as
OpenRefine, Data Wrangler [21] and its commercial descendant Trifacta, Katara [7], and DataXFormer [3]. These tools discover patterns in the data,
either syntactic (e.g., OpenRefine and Trifacta) or
semantic (e.g., Katara), and use these to detect errors
(cells that do not conform with the patterns). Transformation tools can also be used to change data representation and expose additional patterns.
• Quantitative error detection algorithms that expose
outliers, and glitches in the data [2, 9, 28, 32, 34].
• Record linkage and de-duplication algorithms for detecting duplicate data records [13, 26], such as the
Data Tamer system [30] and its commercial descendant Tamr. These tools perform entity consolidation
when multiple records have data for the same entity.
As a side effect of this process, conflicting values for
the same attribute can be found, indicating possible
errors.

INTRODUCTION

In the last two decades there has been intensive research
in developing data cleaning algorithms and tools [10, 11, 14,
∗Work done while doing an internship at QCRI.

When considering these tools, several challenges have
emerged. Most notably:
This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivatives 4.0 International License. To view a copy
of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For
any use beyond those covered by this license, obtain permission by emailing
info@vldb.org.
Proceedings of the VLDB Endowment, Vol. 9, No. 12
Copyright 2016 VLDB Endowment 2150-8097/16/08.

1. Synthetic data and errors. Most cleaning algorithms
have been evaluated on synthetic or real-world data
with synthetically injected errors. While these might
be appropriate for testing the soundness of the algorithm [4], the effectiveness of any given tool to detect

993

errors “in the wild” is unclear. The lack of real data
sets (along with appropriate ground truth) or a widely
accepted cleaning benchmark makes it hard to judge
the effectiveness of existing cleaning tools. It is equally
difficult to reason about the use cases where a given
tool should be employed without real-world data.

to estimate the upper-bound recall as follows. We first classify the remaining errors that are not detected by the tools
based on their type. Then, any error whose type matches
the type that is detectable by a given tool is counted towards the recall of that tool. For example, if the ground
truth revealed an error that can be captured by defining a
new functional dependency, we count this error towards the
upper-bound recall of the rule-based tool.
With the above considerations in mind, we aim to answer
the following questions:

2. Combination of error types and tools. Real-world data
usually contains multiple types of errors. Moreover,
the same error might be findable by more than one
type of tool. For example, an error might be part of
conflicting duplicate records and violate an integrity
constraint at the same time. Considering only one
type of algorithm in any study misses an opportunity
to accumulate evidence from multiple tools.

1. What is the precision and recall of each tool? In other
words, how prevalent are errors of the type that are
covered by each tool?
2. How many errors in the data sets are detectable by
applying all the tools combined?
3. Since human-in-the-loop is a well accepted paradigm,
how many false positives are there? These may well
drain a human effort budget and cause a cleaning effort
to fail. Is there a strategy to minimize human effort
by leveraging the interactions among the tools?

3. Human involvement. Almost all practical tools involve
humans, for example, to verify detected errors, to specify cleaning rules, or to provide feedback that can be
part of a machine learning algorithm. However, humans are not free and enterprises often impose a human budget to limit the cost of a cleaning project.
Since errors detected by different tools overlap, ordering the application of these tools to minimize total
human involvement is an important task that requires
assessing the interaction among tools.

1.2

1.3

Main Findings

Based on our experiments, we draw three conclusions:
• Conclusion 1: There is no single dominant tool. In
essence, various tools worked well on different data
sets. Obviously, a holistic “composite” strategy must
be used in any practical environment. This is not surprising since each tool has been designed to detect errors of a certain type. Moreover, empirical evaluation
with multiple real data sets show that the distributions
of errors types vary significantly from one data set to
another.
• Conclusion 2: By assessing the overlap of errors detected by the various tools, it is possible to order the
application of these tools to minimize false positives
(and hence user engagement). Due to the large variance in quality from data set to data set, this ordering
strategy must be data set specific. We present a composite tool with these characteristics in Section 3.5.2
and its experimental evaluation in Section 4.5.2.
• Conclusion 3: The percentage of errors that can be
found by the ordered application of all tools (the combined overall recall) is well less than 100%. In Section 4, we report our experimental results to find additional errors. These focus on type-specific cleaning
tools, such as an address cleaning service and the process of enrichment, which is to search for additional
attributes and values that can assist in the cleaning
operation. Even with the addition of a type-specific
address tool and enrichment, we show that there is
still a need to develop new ways of finding data errors
that can be spotted by humans, but not by the current
cleaning algorithms.

Our Evaluation Methodology

To shed light on the current state of data cleaning tools,
we embarked on an experimental endeavor to address the
above challenges. Specifically, we assembled a collection of
real-world data sets (Section 2) that represent the multiple kinds of dirty data one finds in practice. Several of these
data sets are proprietary and were obtained under NDA (non
disclosure agreement) restrictions. While this prevents us
from publishing them as a benchmark, it was important to
our study to work with as many real-world data sets as possible. We also obtained full or partial ground truth for each
data set, so we can judge the performance and capabilities
of available error detection tools.
We have also selected a collection of data cleaning tools
(Section 3). These tools were chosen because, in aggregate,
they are capable of detecting the error types (Section 2.1)
found in practice, namely pattern violations, constraint violations, outliers, and conflicting duplicates. Note that in
this study, we are interested in the automatic error discovery and not in automatic repair, since automatic repairing
is rarely allowed in practice.
We have systematically run all the tools on all data sets
and in this paper we report results (precision and recall)
based on the ground truth (Section 4). Each tool has been
configured in a best-effort fashion (e.g., by iteratively choosing good parameters or by defining a reasonable set of quality rules). In all cases we used the capabilities that the
tool provided, and did not resort to heroics, such as writing
data-set specific Java code.
While we did our best in using a tool, we can still miss errors that can be otherwise captured if the right configuration
(e.g., more rules, more patterns, or better tuned parameter
values) were used. To capture this phenomena, we introduce the notion of upper-bound recall, which is an estimate
for the maximum recall of a tool if it has been configured
by an oracle. Since we do not have an oracle and cannot
optimally configure a tool, we use the known ground truth

Section 5 closes the paper with conclusions and suggestions for future work.

2.

DATA ERRORS AND DATA SETS

In this section, we first discuss the type of errors that are
reported in this paper. We then describe the real-world data
sets that we used.

994

Table 1: Experimental data sets
data set
MIT VPF
Merck
Animal
Rayyan Bib
BlackOak

Figure 1: Error type taxonomy

2.1

# columns
42
61
14
11
12

# rows
24K
2262
60k
1M
94k

ground truth
13k (partial)
2262
60k
1k (partial)
94k

Errors
6.7%
19.7%
0.1%
35%
34%

to procurement. This procurement database contains information about vendors and individuals that supply MIT
with products and services. Whenever MIT executes a purchase order, a new entry with details about the contracting
party is added to the vendor master data set. This record
contains identification information, such as name, address,
and phone numbers, semi-automatically generated business
codes depending on the contract type, and meta-data, such
as creation date and creator. The ongoing process of adding
new entries introduces duplicates and other data errors. A
particular case of errors relates to inconsistent formatting.
For example, addresses with formats that deviate from the
USPS address standard are considered erroneous values that
need to be fixed, e.g., addresses that contain “Street” or
“Str” instead of “ST”. Similar rules apply for phone numbers and company names. Furthermore, contact information for suppliers changes over time, as suppliers move, get
acquired, or introduce new branches. This introduces additional errors, which must be removed.
To obtain ground truth for this data set, employees of
VPF manually curated a random sample of 13,603 records
(more than half of the data set) and marked erroneous fields.
Most of the errors affected address and company names and
included missing street numbers, wrong capitalization, and
attribute values that appear in the wrong column.

Types of Data Errors

There have been several surveys on classifying data errors [6, 17, 19, 24, 29]. Rahm et al. [29] and Kim et al. [24]
look at data errors from the perspective of how the errors
were introduced into the data. Hellerstein et al. [17] follow on the same line when considering numerical data and
surveying quantitative data cleaning approaches [17]. Ilyas
et al. [19] consider errors as violations of qualitative rules
and patterns, such as denial constraints [6]. We adopt a
taxonomy that covers all the error types mentioned in these
surveys.
We define an error to be a deviation from its ground truth
value. More formally, given a data set, a data error is an
atomic value (or a cell) that is different from its given ground
truth. Figure 1 illustrates the four types of errors that are
used in this study, which are classified as either quantitative
or qualitative ones.
1. Outliers include data values that deviate from the
distribution of values in a column of a table.
2. Duplicates are distinct records that refer to the same
real-world entity. If attribute values do not match, this
could signify an error.
3. Rule violations refer to values that violate any kind
of integrity constraints, such as Not Null constraints
and Uniqueness constraints.
4. Pattern violations refer to values that violate syntactic and semantic constraints, such as alignment, formatting, misspelling, and semantic data types.

(2) Merck

We conducted our experiments on several real-world data
sets that were provided by various organizations. Table 1
lists these data sets along with the number of rows, number
of columns, and whether we have full or partial ground truth
for each data set. Additionally, the corresponding ratio of
erroneous cells range from 0.1% to 34% across the data sets.
The types of errors found in each data set are given in Table 2. We see that the four error types are prevalent across
all data sets with the exception of the Animal data set not
having outliers and only MIT VPF and BlackOak having
duplicates. In the rest of this section, we give details about
each data set including the data quality issues that we found
and how we obtained ground truth.

Merck provided us with a data set that describes IT services
and software systems within the company that are partly
managed by third parties. Each system is characterized by
several attributes, such as location, number of end users,
and level of technical support that has been agreed on, e.g.,
seven days a week or 8x5 hours. The data set is used for
optimization purposes. In particular, Merck is interested
in identifying opportunities to decommission a service or to
downsize the level of support. The data set has 68 different
attributes but is very sparse.
Most errors in this data set consist of columns that are not
consistently formatted – due to different parties introducing
the information in the database. Merck provided the custom
cleaning script that they used to produce a cleaned version
of the data set. We used this version as ground truth. The
script applies various data transformations that normalize
columns and allow for uniform value representation. We
used this script to help us in formulating rules and transformations for our cleaning tools. Some of the transformations
happen invisibly as side effects of other operations. For example, one of the functions that normalizes the retirement
date of a software system also changes the encoding of missing values to “NA”. Hence, we can only capture a subset of
the rules and transformations by examining the script.

(1) MIT VPF

(3) Animal

The MIT Office of the Vice President for Finance (VPF)
maintains several finance databases, one of which is related

The animal data set was provided by scientists at UC Berkeley who are studying the effects of firewoood cutting on

Note that our categorization does not perfectly partition
errors since some errors may fit into more than one category.
Also, our categories are not necessarily exhaustive. Instead,
they mainly serve the purpose of categorizing errors that are
detectable from available real-world data sets.

2.2

Data Sets

995

Table 3: Coverage of error types by the tools

M

IT
V
P
M
F
er
ck
A
ni
m
al
R
ay
ya
n
B
B
la
ck ib
O
ak

dB
oo
st
D
C
-C
O lean
pe
nR
ef
T
ri
fa ine
c
t
P
en a
ta
h
K
ni o
m
e
K
at
ar
Ta a
m
r

Table 2: Error types found in each data set

Pattern violations
Constraint violations
Outliers
Duplicates

3
3
3
3

3
3
3

3
3

3
3
3

3
3
3
3

Pattern violations
Constraint violations
Outliers
Duplicates

small terrestrial vertebrates. Each record contains information about the random capture of an animal, including
the time and location of the capture, properties such as tag
number, sex, weight, species, and age group, and a field indicating whether the capture is the first capture of the animal.
The data set was collected from 1993 to 2012.
Each capture was recorded on paper first, and then manually entered into spreadsheets. Thus, errors such as shifted
fields and wrong numeric values were introduced to the data
set. The scientists identified and corrected several hundreds
erroneous cells in the data set. We use the manually cleaned
data set as ground truth.

Rayyan1 is a system built at QCRI to assist scientists in
the production of systematic reviews [12]. These are literature reviews focused on a research question, e.g., is vitamin
C good for a cold [18], and identify and synthesize all research evidence relevant to that question. For each review,
Rayyan’s users start by searching multiple databases, e.g.,
MEDLINE and EMBASE, using multiple queries. They
then consolidate their search results into long lists (from
100’s to 1000’s) of references to studies that they feed to
Rayyan. Rayyan is used to select the relevant studies and
perform other analysis for their reviews.
Since these references are coming from multiple sources
and some users may manually manipulate these citations,
the data is prone to errors. To obtain ground truth, we manually checked a sample of 1,000 references from Rayyan’s
database and marked erroneous cells. These references
contain tens of columns, such as article title, journal title,
journal abbrevation, language, and journal issn. There are
many missing values and inconsistencies in the data, such
as journal title and journal abbrevation being switched and
author names found within the journal title.

3.1

3

3

3

3
3

Outlier Detection

Outlier detection tools detect data values that do not follow the statistical distribution of the bulk of the data. We
interpret these values as data errors.

Tool 1: dBoost
dBoost [27] is a novel framework that integrates several
of the most widely applied outlier detection algorithms:
histograms, Gaussian and multivariate Gaussian mixtures
(GMM) [5]. Histograms create a de facto distribution of
the data without making any assumption a priori by counting occurrences of unique data values. Gaussian and GMM
assume that each data value was drawn from a normal distribution with a given mean and standard deviation or with
a multivariate Gaussian distribution, respectively.
One key feature of dBoost is that it decomposes run-on
data types into their constituent pieces. For example, date
is expanded into month, day and year, so that each can be
analyzed separately for outliers. To achieve good precision
and recall, it is necessary to configure the parameters of the
different outlier detection methods: number of bins and their
width for histograms, and mean and standard deviation for
Gaussian and GMM.

(5) BlackOak
BlackOak Analytics is a company that provides entity resolution solutions. They provided us with an anonymized
address data set and its dirtied version that they use for
evaluation. The errors are randomly distributed and affect
the spelling of values, their formatting, completeness, and
field separation. We purposely included this data set to
study the difference in error detection performance between
real-world address data sets and a synthetic data set.

3.2

Rule-based Error Detection

Rule-based data cleaning systems rely on data quality
rules to detect errors. Data quality rules are often expressed
using integrity constraints, such as functional dependencies
or denial constraints [6]. A violation is a collection of cells
that do not conform to a given integrity constraint. A data
error is a cell from a constraint column that is involved in a
violation. Intuitively, in order to resolve a violation, at least
one cell involved in the violation must be changed.

DATA CLEANING TOOLS

In selecting the data cleaning tools, we made sure that
they cover all the error types described in Section 2.1. These
tools include publicly available tools and commercial products to which we had access to or their community versions.
1

3

Our choices are listed in Table 3 together with the types of
errors they were designed to find. For some error-types we
picked multiple tools that focus on different subtypes of a
given error type. For instance, Katara focuses on the discovery of semantic pattern violations while wrangling tools
focus on syntactic pattern violations. In describing the tools
below, we explain in details the choice of each specific tool.
In order to mitigate the risk of a suboptimal tool configuration and insufficient rules in the case of rule-based tools,
we used an iterative fine-tuning process for each tool. Specifically, we compared the initially detected errors with the
ground truth and adjusted the tool configuration or the rules
accordingly to improve performance. We also examined the
undetected errors and counted those that could have been
captured by a tool, under the optimal configuration, as “detectable”. These detectable errors are then counted towards
the recall upper bound of that tool.
Next, we will present to apply the tools individually, followed by combination approaches to improve performance.

(4) Rayyan Bib

3.

3
3

rayyan.qcri.org

996

Tool 2: DC-Clean

least frequent values. Additionally, Trifacta shows attribute values that deviate strongly from the value lengths
distribution in the specific attribute. Furthermore, Trifacta maps each column to the most prominent data type
and identifies values that do not fit the data type.

We use the tool DC-Clean that focuses on denial constraints
(DCs) [6] and DCs subsume the majority of the commonly
used constraint languages. For a given data set, we design a
collection of DCs to capture the semantics of the data. For
example, for the animal data set, one of the DCs states that
“if there are two captures of the same animal indicated by
the same tag number, then the first capture must be marked
as original”. We report any cell that participates in at least
one violation as erroneous.

3.3

Tool 5: Katara
Katara relies on external knowledge bases, such as
Yago [31], to detect and correct errors that violate a semantic pattern [7]. Katara first identifies the type of a column
and the relationship between two columns in the data set
using a knowledge base. For example, the type of column
A in a table might correspond to type Country in Yago and
the relationship between columns A and B in a table might
correspond to the predicate HasCapital in Yago. Based on
the discovered types and relationships, Katara validates
the values in the table using the knowledge base and human
experts. For example, a cell “New York” in column A is
marked to be an error, since it is not a country in Yago.

Pattern-based Detection

In our evaluation, we include five pattern-based detection tools: (1) OpenRefine, an open source data wrangling
tool; (2) Trifacta, the community version of a commercial
data wrangling tool; and (3) Katara [7], a semantic pattern
discovery and detection tool. OpenRefine and Trifacta
provide different exploration techniques that we can use to
discover data inconsistencies. When applied on our data
sets, both tools subsume errors that can be captured by
most existing ETL tools. In fact, we were able to confirm
this statement by running two popular ETL tools, namely
(4) Knime and (5) Pentaho. Finally, while OpenRefine
and Trifacta focus on syntactic patterns, Katara focuses
on semantic patterns matched against a knowledge base.

Tool 6: Pentaho
Pentaho3 provides a graphical interface where the data
wrangling process can be orchestrated through creating a
directed graph of ETL operations. Any data manipulation
or rule validation operation can be added as a node into the
ETL pipeline. It provides routines for string transformation
and single column constraint validation.

Tool 3: OpenRefine
OpenRefine is an open source wrangling tool that can digest data in multiple formats. Data exploration is performed
through faceting and filtering operations. Faceting resembles
a grouping operation that lets look at different kinds of aggregated data. The user specifies one column for faceting
and OpenRefine generates a widget that shows all the distinct values in this column and their number of occurrences.
The user can also specify an expression on multiple columns
for faceting and OpenRefine generates the widget based on
the values of the expression. The user can then select one or
more values in the widget and OpenRefine filters the rows
that do not contain the selected values.
Data cleaning uses an editing operation. The user edits
one cell at a time, and can also edit a text facet and all the
cells consistent with this facet will be updated accordingly.
For example, in the Rayyan Bib data set, by faceting on the
language column, we have two facets ‘eng’ and ‘English’ occurring 110 and 3 times, respectively, exposing a consistency
problem. The users can also cluster the cells in a column
and then edit the cells in each cluster to a consistent value.
Finally, the users can highlight the cells in a column using
the Google refine expression language (GREL) 2 .

Tool 7: Knime
Knime4 focuses on workflow authoring and encapsulating data processing tasks, including curation and machine
learning-based functionality in compassable nodes. Similar
to Pentaho, the curator orchestrates multiple ETL workflows to clean and curate the data, but the rules and the
procedure has to be specified. The significant difference of
Pentaho w.r.t. OpenRefine and Trifacta is that the user
has to know exactly what kind of rules and patterns need to
be verified, since both Pentaho and Knime do not provide
ways to automatically display outliers and type mismatches.

3.4

Duplicate Detection

If two records refer to the same real-world entity but contain different values for an attribute, it is a strong signal
that at least one of the values is an error.

Tool 8: Tamr
Tamr is a tool with industrial strength data integration algorithms for record linkage and schema mapping. Tamr is
based on machine learning models that learn duplicate features through expert sourcing and similarity metrics. It is
the commercial descendant of the Data Tamer system [30].

Tool 4: Trifacta
Trifacta is the commercial descendant of DataWrangler [22]. It can predict and apply various syntactic data
transformations for data preparation and data cleaning.
For this study, we use the available community version of
Trifacta. Using Trifacta, one can apply business and
standardization rules through the available transformation
scripts. For exploratory purposes, Trifacta applies a frequency analysis to each column to identify the most and

3.5

Combination of Multiple Tools

The first problem in trying to use multiple independent
tools is how to properly combine them. A simple option is
to run all tools and then apply a union or min-k strategy. A
more sophisticated solution is to have users manually check
a sample of the detected errors, which can then be used to
guide the sequence of multiple tool invocation.
3

2

https://github.com/OpenRefine/OpenRefine/wiki/
GREL-String-Functions

4

997

http://www.pentaho.com
http://www.knime.com/

3.5.1

Union All and Min-k

its detected errors with a human expert, which can be used
to estimate the precision of each tool.
S3. [Pick a tool to maximize entropy.] To maximize the
entropy, among all tools not yet considered, it picks the one
with the highest estimated precision (> σ) on the sample,
and verifies its detected errors on the complete data set that
have not been verified before.
S4. [Update and iterate.] Since errors validated from step
S3 may have been detected by the other tools, we update
the precision of the other tools and go to S3 to pick the next
tool, if there are still tools with estimated precision > σ.
In Section 4.5.2, we show that regardless of each tool’s
individual performance, the proposed order reduces the cost
of manual verification with marginal reduction of recall.

We consider two simple strategies: (i) Union all takes
the union of the errors emitted by all tools, and (ii) Min-k
considers as errors those errors detected by at least k-tools
while excluding those detected by less than k tools.

3.5.2

Ordering Based on Precision

The simplest way to involve users, when combining different tools, is to ask them to exhaustively validate the union
of the outputs. This is prohibitively expensive given the human effort of validating the large number of candidate errors.
For example, for the BlackOak data set, a user would have
to verify the 982,000 cells identified as possibly erroneous
to discover 382,928 actual errors. Obviously, results from
tools with poor performance in error detection should not
be evaluated. We thus present a sampling-based method to
select the order in which available tools should be evaluated.

4.

EXPERIMENTS

We performed extensive experiments for each combination of tool and data set. For the MIT VPF and Rayyan
Bib data set, we applied each tool only on the sample with
ground truth. This showed the behavior of each tool and
the types of errors each is able to detect on each data set.
We report the metrics we used in Section 4.1, the tuning
we performed for every tool in Section 4.2, and the degree
of user-involvement in Section 4.3. This process was timeconsuming, and different configurations might lead to different performance results. We show in Section 4.4 the results
we obtained and put the numbers in context w.r.t. the four
different types of errors presented in Section 2.1.
After the analysis in isolation, we then report in Section 4.5 the insights we obtained from the experiments on
different tool combinations. Finally, we analyze the remaining errors and compute the recall upper-bound in Section 4.6.
This analysis motivates two possible extensions, namely,
domain-specific data cleaning tools and enrichment, which
we discuss in Sections 4.7 and 4.8, respectively.

Cost model. The performance of a tool can be measured by
precision and recall in detecting errors. Unfortunately recall
can be computed only if all errors in the data are known,
which is impossible when we execute the detection tools on a
new data set. However, precision is easy to estimate, and is
a proxy for the importance of a tool in the detection process.
Suppose the cost of having a human check a detected error is
C and the value of identifying a real error is V. Obviously, the
value should be higher than the cost. Hence, P ∗ V > (P +
N ) ∗ C, where P is the number of correctly detected errors
and N is the number of erroneously detected errors (false
positives). We can rewrite the inequality as: P/(P + N ) >
C/V. If we set a threshold σ = C/V, we conclude that any
tool with a precision below σ should not be run, as the cost
of checking is greater than the value obtained. While this
ratio is domain dependent and unknown in most cases, it
is natural to have large V values for highly valuable data.
With the corresponding small σ all tools will be considered,
thus boosting the recall. On the other hand, if the C is high
and dominates the ratio, we save cost by focusing only on
the validation of tools that are very precise, compromising
recall. For each tool, we estimate its precision on a given
data set by checking a random sample of the detected errors.
Intuitively, we can run all the tools with a precision larger
than the threshold and evaluate the union of their detected
error sets. However, the tools are not independent since
the sets of detected errors may overlap. We observed that
some tools are not worth evaluating even if their precision is
higher than the threshold, since the errors they detect may
be covered by other tools with higher estimated precision
(which would have been run earlier).

4.1

Evaluation Metrics

To show the effectiveness of the different tools, we measure
their accuracy in finding potential errors using precision and
recall, defined as follows. Given a data set D and its cleaned
version G, we define the function diff as diff(G, D) = E, as
the set of all erroneous cells in D. Furthermore, for each
tool T , let T (D) be the set of cells in D marked as errors
by T . We define precision P of a tool as the fraction of cells
that are correctly marked as errors and the recall R as the
fraction of the actual errors that are discovered by tool T :
P =

Maximum entropy-based order selection. Based on
the above observations, we discuss a composite strategy that
takes into account the ratio between the total cost and the
total obtained value. Following the Maximum Entropy principle [20], we design an algorithm that assesses the estimated precision for each tool. Furthermore, our algorithm
estimates the overlap between the tool results. As high entropy refers to uncertainty, picking the tool with the highest
precision achieves best entropy reduction. It works in the
following four steps.
S1. [Run individual tool.] Run all the tools on the entire
data set and get their detected errors.
S2. [Estimate precision by checking samples.] For each
tool, estimate its precision by verifying a random sample of

|T (D) ∩ E|
|T (D)|

R=

|T (D) ∩ E|
|E|

Note that for the rule-based and duplicate detection systems, we identify inconsistencies that typically involve multiple cells while only one of the cells might actually be a true
error. In our study, we mark all attribute values involved in
an inconsistency as errors, thus favoring recall at the cost of
precision. To aggregate P and R we use the harmonic mean
or F-measure as F = 2(R × P )/(R + P ).

4.2

Usage of Tools

As noted earlier, for each tool, we use all of its features on
a best effort basis using expert knowledge of the data sets.
However, we do not write user-defined programs for any of
the tools.

998

Table 4: Data quality rules defined on each data set
Data set
MIT VPF
Merck
Animal

Rule Type
FD
DC
Check
FD
DC

Rayyan Bib Check
FD
DC
BlackOak
FD

Number
3
5
14
2
2
14
9
1
5

Examples
Zip code → State; EIN → Company name
Phone number not empty if vendor has an EIN; Either street field or PO field must be filled
Support level should not be null; Employer status should be either Y,N, or N/A
Tag → Species; FD:Tag→A/S-2
For any two captures of the same animal with same Tag and ID), the second
capture must be re-capture
journal ISSN should not be null; author should not be null
journal abbreviation →journal title; journal abbreviation → journal ISSN
No two articles are allowed to have the same title
Zip code → State; Zip code→ City

Table 5: Error detection performance of each tool on each data set

DC-Clean
Trifacta
OpenRefine
Pentaho
KNIME
Gaussian
Histogram
GMM
Katara
Tamr
Union

MIT VPF
P
R
F
0.25 0.14 0.18
0.94 0.86 0.90
0.95 0.86 0.90
0.95 0.59 0.73
0.95 0.59 0.73
0.07 0.07 0.07
0.13 0.11 0.12
0.14 0.29 0.19
0.40 0.01 0.02
0.16 0.02 0.04
0.24 0.93 0.38

P
0.99
0.99
0.99
0.99
0.99
0.19
0.13
0.17
0.33

Merck
R
0.78
0.78
0.78
0.78
0.78
0.00
0.02
0.32
0.85

F
0.87
0.87
0.87
0.87
0.87
0.01
0.04
0.22
0.48

P
0.12
1.0
0.33
0.33
0.33
0
0
0
0.55
0.128

Animal
R
0.53
0.03
0.001
0.001
0.001
0
0
0
0.04
0.575

F
0.20
0.06
0.002
0.002
0.002
0
0
0
0.073
0.209

Rayyan Bib
P
R
F
0.740 0.549 0.630
0.714 0.591 0.647
0.947 0.603 0.737
0.717 0.584 0.644
0.717 0.584 0.644
0.412 0.131 0.199
0.395 0.164 0.232
0.534 0.391 0.451
0.598 0.393 0.474
0.473 0.850 0.608

P
0.46
0.96
0.99
1.0
1.0
0.91
0.52
0.38
0.88
0.41
0.39

BlackOak
R
F
0.43 0.44
0.93 0.94
0.95 0.97
0.66 0.79
0.66 0.79
0.73 0.81
0.51 0.52
0.37 0.38
0.06 0.11
0.63 0.50
0.99 0.56

viate from formatting or single column rules, such as phone
number should not be null or zipcodes consist of zip+4, i.e.,
the basic five-digit code plus four additional digits.

dBoost
We applied three algorithms from dBoost: Gaussian, histogram, and GMM. All of them require input parameters,
which we chose using the following trial-and-error methodology. The Gaussian and GMM approach required the parameters mean and standard deviation. The histogram approach
required a value for its bin width. For each data set, we tried
various parameter values and compared the discovered outliers with the ground truth for each one. Our final choice
was the parameter values with the highest F-measure score.
For efficiency reasons, we first tuned the parameters on samples, and then applied the top three discovered parameter
values on the complete data set. The ultimate choice was
the one with the highest F-measure on the complete data
set.

Trifacta
We used Trifacta in a similar manner to OpenRefine for
enforcing formatting and single-column rules. Additionally,
we considered the outlier detection and type-verification engines in Trifacta as additional ways of detecting errors.

Katara
In our Katara implementation [7], cells are verified against
a knowledge base. If a cell is not in the knowledge base, we
declare it to be an error.
Since knowledge bases are inevitably domain-specific, we
manually constructed one for each data set. For example,
for Animal data, the domain experts provided us with a list
of animal species, which were entered into Katara. For address data, we created a knowledge base of geographical locations that contains information about cities, regions, and
countries with different formats and abbreviations. Similarly, we used an ISSN knowledge base for Rayyan Bib data
that contains journal ISSNs, journal names, journal abbreviations, journal languages, etc.

DC-Clean
For the MIT VPF data set, we obtained business rules from
the data owner. We inferred rules for the Merck data set
by examining the cleaning script provided by the data set
owner. The rules were single-column constraints, such as
denial of specific values in a column, functional dependencies, and conditional functional dependencies. In addition
and for each data set, we manually constructed FD rules
based on obvious n-to-1 relationships, such as Zip code →
State. Table 4 illustrates the number of each type of rule
for each data set. As mentioned earlier, we report any cell
that participates in at least one violation to be erroneous.

Pentaho and Knime
We can use ETL tools, such as Pentaho and Knime, in
a similar manner as OpenRefine and Trifacta. However,
we have to model each transformation and validation routine
as a workflow node in the ETL process instead of applying
the rules directly to the data. Both tools provide nodes for
enforcing single column rules and string manipulations. The
final result of the workflows designed in this tool are data
sets where the potential errors have been marked through
transformations.

OpenRefine
Instead of manually checking each attribute value, we applied OpenRefine’s facet mechanism. The numerical facet
shows the fraction of data cells in a column that are numerical and their distribution, while text facets show sorted distinct column values and their frequencies. Furthermore, we
used the transformation engine for detecting values that de-

999

Table 6: Error detection performance of at least k detection tools on each data set
k
1
2
3
4
5
6
7
8

MIT VPF
P
R
F
0.24 0.93 0.38
0.48 0.90 0.63
0.58 0.41 0.48
0.79 0.09 0.16
0.76 0.03 0.06
0.90 0.00 0.01
0.80 0.00 0.00
0
0
0

P
0.33
0.889
0.996
0.997
0.993
1.0
0
0

Merck
R
0.84
0.789
0.787
0.280
0.015
0.000
0
0

F
0.47
0.834
0.879
0.438
0.029
0.000
0
0

P
0.128
0.241
1.0
0
0
0
0
0

Animal
R
0.575
0.030
0.001
0
0
0
0
0

Tamr
We used Tamr off-the-shelf and applied several rounds of
training for each data set. Each round labels 100 candidate
duplicate pairs as matching or not-matching. Since we have
ground truth, this labeling can be performed automatically.
After each round we retrain the machine learning algorithms
and compute precision and recall. If they stay stable for
more than two iterations, we stop training and run Tamr
on the whole data set to discover clusters of duplicates. For
each cluster, we identify conflicting fields and mark them as
errors.

4.3

User involvement

In general, the user is involved in four activities:
1. The user configures the tools by declaring and specifying rules and patterns that she is aware of and can
be validated by the given tool.
2. The user performs data exploration to find errors using
Trifacta and OpenRefine.
3. The user validates the result of the error detection
tools. In our experiments, this is needed to compute
the precision and recall.
4. The user has to manually go through the remaining
errors and try to categorize them based on the given
error types. This reasoning allows us to compute the
recall upper bound.

4.4

Individual Tools Effectiveness

In this section we report the performance (precision, recall, and F-measure) of each data cleaning tool for all data
sets, and Table 5 summarizes the results. There are several observations that we can make. The two data transformation tools, Trifacta and OpenRefine, have either the
highest or the second highest results with regard to recall
on the MIT VPF, Merck, and BlackOak data sets where
formatting issues mattered most. However, both tools have
poor results on the Animal data set where most errors are
of semantic nature, where the value is syntactically valid
and from the correct domain but wrong with regard to the
ground truth. On all data sets, the results of Trifacta and
OpenRefine subsumed the results of Pentaho and Knime. This is not surprising as all of these tools provide basic
wrangling capabilities. However, Trifacta and OpenRefine also automatically hint at data type mismatches and
outliers.
The rule based system DC-Clean had comparably good
performance on the Merck, Animal, and Rayyan Bib
data sets in terms of F-measure because the rules in
these data sets covered particular error-prone columns.
DC-Clean,Pentaho,Knime, Trifacta, and OpenRefine
achieved exactly the same precision and recall on the Merck

F
0.209
0.053
0.002
0
0
0
0
0

Rayyan Bib
P
R
F
0.473 0.850 0.608
0.650 0.738 0.691
0.831 0.599 0.696
0.928 0.432 0.590
0.969 0.164 0.281
0.962 0.032 0.062
0.897 0.007 0.014
0
0
0

P
0.391
0.553
0.733
0.904
0.972
0.993
0.999
1.0

BlackOak
R
F
0.999 0.56
0.999 0.712
0.979 0.838
0.915 0.909
0.739 0.839
0.437 0.607
0.135 0.237
0.013 0.025

data set, as all patterns could be translated into DC check
rules. The recall of OpenRefine and Trifacta is higher
on the BlackOak data set since their outlier detection routines show syntactical problems that could not be anticipated with DC rules.
Outlier detection methods do nothing on the Animal data
set, since there are no outliers. In general, outlier methods
performed very poorly on the remaining data sets because
dirty data was typically frequent patterns not anomalies.
Outlier detection would be more effective on high-quality
data sets where errors are rather rare. Katara had fair
precision on the BlackOak data set where location values
could be easily verified via the knowledge base but performed poorly on the real-world MIT VPF data set where
errors were mostly found in address values rather than in
geographic locations.
We ran the duplicate detection tool only on data sets having duplicates, i.e., MIT VPF and BlackOak. For MIT VPF,
the tool discovered all existing duplicates. The low precision
and recall reported in Table 5 reflects the fact that most errors are not of type duplicate. In the case of BlackOak, we
found most (but not all) of the duplicates. If all duplicates
would have been found, the recall of exposing errors would
have increased from 63% to 98%, which means that 98% of
the errors in the data set were involved in duplicate records.
The precision would have slightly decreased by 2%.
By comparing the results on MIT VPF and BlackOak, we
clearly see that most of the tools are much more effective
on the synthetic BlackOak data. As a result, future work in
this area should not use synthetic faults.
Also note that no single tool is dominant on all data sets.
Outlier detection algorithms covered shifted values, rules
generally captured inconsistencies among re-occurring values, while transformation tools covered formatting errors.
Duplicate detection systems were very effective, but only in
data sets where there were a lot of duplicates. Hence, in the
next section we turn to aggregating evidence from multiple
tools.

4.5

Tool Combination Effectiveness

In this set of experiments, we will report the result of
different strategies of combining multiple tools.

4.5.1

Union All and Min-k

The last row in Table 5 lists performance that can be
obtained by the union of all tool outputs. Of course, the
combination of tools achieves better recall than any single
tool, typically above 60% and often much higher.
While the union of all tools results in higher recall than
any single tool, the precision suffers significantly, as the
union of all results increases the set of false positives. In
the particular case of MIT VPF, we can see that the 97%

1000

(a)

Merck: 23,049 out of 27,208 errors

(b)

Animal: 802 out of 1,394 errors

(c)

Rayyan Bib: 3275 out of 3853 errors

(d) MIT VPF: 36,410 out of 39,158 errors (e) BlackOak: 382,928 out of 383,003 errors
Figure 2: Overlaps of error types: T1: Duplicates, T2: Constraint Violations, T3: Outliers, T4: Pattern Violations
recall holds for 24% precision. To maximize precision, a
natural approach is to use a min-k voting system. Table 6
presents the precision and recall when we require at least
k-tools agree on a cell being erroneous. As expected, by
increasing k, we increase precision but lose recall. We can
further observe that there is no k where the F-measure is
the highest on all data sets. Note that for this and the following experiments, we excluded Pentaho and Knime since
their results were totally subsumed by the results of OpenRefine and Trifacta, and hence would not contribute to
additional insights.
In order to study how different types of errors appear together in a data set, we analyzed the overlap of discovered
errors. Figure 2 shows for each data set a Venn diagram
where the number in each area represents correctly detected
errors that belong to the overlapping error types. Here T1
corresponds to duplicates, T2 corresponds to constraint violations, T3 corresponds to outliers (unioning of results of
Gaussian, GMM, and histogram), and T4 corresponds to
pattern violations (unioning results of Trifacta, OpenRefine,Katara). In each data set note there are errors that
belong to multiple error types. Therefore they can be discovered by multiple tools. In MIT VPF for example, outliers
and pattern violations overlap the most; for Merck, Animal,
and Rayyan Bib, constraint violations and pattern violations overlap the most; while for BlackOak, duplicates and
pattern violations overlap the most.
Comparing the min-k approach results to the Venn diagrams note that in data sets where errors infrequently overlap, the min-k approach suffers significant loss of recall with
increasing k. This is the case for Animal data where 94%
of the detected errors are detected by a single tool, specifically 87% (701) are constraint violations and 7% (59) are

pattern violations. On the other hand in BlackOak, we see
a strong overlap among tools. For example, 26% (99,959) of
the detected errors belong to all four error types, while only
1.7% (6,632) belong to exactly one error type. Hence, recall
stays above 90% in min-k up to k = 4 and then gradually
decreases for higher k.
The main problem of the min-k approach is that it depends on a manually picked k, which can depend on the
given data set and the set of tools. Therefore, we proposed
(as described in Section 3.5) a different approach that optimizes the accuracy of the tool ensemble through a benefitbased ordering of tool application.

4.5.2

Ordering based on Benefit and User Validation

For each data set, we randomly sampled 5% of the detected errors for each tool and compared them with ground
truth to estimate the precision of the tool. As noted in Section 3.5.2, we should then run the tools with sufficiently high
precision in order. As a baseline, we took the simple union
of all the algorithms, for which all the detected errors need
to be validated. In addition, we varied the threshold σ (see
Section 3.5.2) from 0.1 to 0.5, thereby generating five runs
for each data set. We also depict the result of the simple
union of all the algorithms.
To show the improvement in performance of our algorithm
relative to the baseline, we computed the percentages of detected errors that need to be validated as well as the actual
errors. Figures 3 and 4 show the results. Using our strategy,
a human has to validate significantly fewer detected errors
while losing only a few true errors compared to the union
strategy. In other words, it sacrificed a bit of recall to save
on human engagement. For example, on Merck, when the
threshold for σ is 0.1, a human would need to validate 35%

1001

0.4
0.2
0

0.8
0.6
0.4
0.2
0

Union 0.1

0.2

0.3

0.4

0.5

true error

validate
1
0.8
0.6
0.4
0.2
0

Union 0.1

Threshold

0.2

0.3

0.4

0.5

true error

validate

Ratio to Simple Union

0.6

1

Ratio to Simple Union

0.8

true error

validate

Ratio to Simple Union

Ratio to Simple Union

Ratio to Simple Union

true error

validate
1

1
0.8
0.6
0.4
0.2
0

Union 0.1

Threshold

0.2

0.3

0.4

0.5

true error

validate
1
0.8
0.6
0.4
0.2
0

Union 0.1

Threshold

0.2

0.3

0.4

0.5

Union 0.1

Threshold

0.2

0.3

0.4

0.5

Threshold

0.6
0.4
0.2
0
Union

0.1

0.2

0.3

0.4

0.5

0.8
0.6
0.4
0.2
0
Union

0.1

0.2

0.3

0.4

0.5

precision

recall
1
0.8
0.6
0.4
0.2
0
Union

0.1

0.2

0.3

0.4

0.5

precision

recall

Precision and Recall

0.8

precision

recall
1

Precision and Recall

1

Precision and Recall

precision

recall

Precision and Recall

Precision and Recall

(a) MIT VPF
(b) Merck
(c) Animal
(d) Rayyan Bib
(e) BlackOak
Figure 3: The relative ratio of validated errors and true errors of the ordering strategy to those of the simple union strategy
1
0.8
0.6
0.4
0.2
0
Union

0.1

0.2

0.3

0.4

0.5

precision

recall
1
0.8
0.6
0.4
0.2
0
Union

0.1

0.2

0.3

0.4

Threshold

Threshold

Threshold

Threshold

Threshold

(a) MIT VPF

(b) Merck

(c) Animal

(d) Rayyan Bib

(e) BlackOak

0.5

Figure 4: The absolute precision and recall of the union and the ordering strategy with different thresholds
of the detected errors compared to the union strategy, while
capturing 92% of its true errors. Similar results can be observed for the other thresholds. For MIT VPF, less than
20% of the errors need to be validated for a relative ratio of
true errors above 80%. This is because the ordering strategy
dynamically updates the estimation of the tools’ precision
and drops those tools that have an estimated proportion of
true errors in their non-validated errors below the threshold.
Looking more carefully at each data set, we make the
following observations. For Merck and BlackOak, there are
tools with precision and recall larger than 95% and 78%,
respectively (see Table 5). Once the detected errors for one
of these tools have been validated, the relative benefit for
the other tools becomes so low that they are never used.
For MIT VPF, the scenario is different. One tool performs
very poorly on this data set, reporting lots of detected errors
with very low precision. This tool is immediately filtered by
the lowest threshold (0.1), but the other tools are able to
identify most of its true errors, therefore the results are very
good with a smaller effort.
For Animal, all the tools have an extremely low recall except the rule-based tool that has a much larger recall but
with a precision of 0.12 which is the smallest among all the
applicable tools. We thus see a sharp fall in recall when this
tools is discarded starting with threshold 0.2. For Rayyan
Bib, there are neither dominant tools nor extremely low precision tools. We observe that by increasing the threshold,
the percentage of validated values decreases faster than the
percentage of identified true errors.
In general, the experiments suggest that it is better to
start with a conservative threshold if recall is a priority.
However, if the budget is constrained, the ordering can reduce the cost while preserving high coverage in most cases.
This insight is supported by the analysis of how the absolute
precision and the recall change depending on the threshold
values, as reported in Figure 4.

4.6

Discussion and Improvements

In the previous experiments, we reported the recall for
each tool as the number of errors identified by the tool.

Table 7: Remaining errors not detected by the tools
Data set
MIT VPF
Merck
Animal
Rayyan Bib
BlackOak

Best effort
Recall
0.92
0.85
0.57
0.85
0.99

Upper-bound
Recall
0.98 (+1,950)
0.99 (+4,101)
0.57
0.91 (+231)
0.99

Remaining
Errors
798
58
592
347
75

As discussed in Section 4, we used the tools to the best of
their capabilities based on our expertise on those tools and
configured each tool for each data set individually. Now,
we turn to the recall upper-bound of the tool combinations.
For this purpose, we count the errors of the type that the
tool is built for towards the recall of the tool. For example,
we identified that one additional pattern enforcement rule
can be added and can cover 98% of the remaining errors for
Merck. Similarly, in the Rayyan Bib data set, there are a
series of errors that affect the formatting of page numbers,
e.g., 122-6 is used instead of 122-126. Again those could
be identified through a regular expression in OpenRefine.
The result is a recall upper bound for each data set, noted
in the second column of Table 7.
Table 7 also reports the remaining errors that cannot be
detected by any of the given tools. In MIT VPF, for example, the remaining errors refer to wrong addresses or incomplete company names that cannot be identified by any
of the tools. For BlackOak, the bigger portion of the remaining errors relates to wrong or misspelled last names.
The best hope to discover these kinds of errors is to use
domain-specific tools or dictionaries that contain those attribute values. Similarly, the remaining errors in the Animal
data set can only be exposed through an expert or the use of
domain-specific dictionaries. Another set of errors is caused
by the confusion of cities that could not be detected because
of missing context, such as zip codes. In the Merck data set,
the remaining errors refer to random non-repetitive values in
the status column that need manual treatment. In the following, we discuss two possible data cleaning improvements,
namely, domain-specific data cleaning tools and enrichment.

1002

Table 9: Precision and Recall after enrichment

Table 8: Cleaning addresses with AddressCleaner
Data set
MIT VPF
BlackOak

4.7

Precision
0.71
0.72

Recall
0.68
0.61

Union Recall
0.95
0.999

Domain Specific Tools

For the MIT VPF and BlackOak data sets, we can use an
address verification tool, AddressCleaner5 , that matches
any given address to the best address in its database and
formats it uniformly. The API reads an address string and
returns a JSON object with the matched address and additional components, such as longitude/latitude coordinates
and zip+4 values. Since the free service has access limitations, we could only apply it on a 1000 row sample of MIT
VPF and BlackOak.
Table 8 lists the precision and recall of AddressCleaner
on each data set and its contribution to the combined recall
of all tools. The recall of this tool alone can be at most
67% on MIT VPF and 61% on BlackOak since the service
applies only on fields referring to address, city, state, and
zip code. Limiting the dataset to only those columns the recall on both datasets exceeds 97%. Interestingly, AddressCleaner does not have 100% precision, which is due to
wrong zip+4 determinations, which occur if the address is
too ambiguous or if different representations of neighborhood areas are present. For example, Jamaica Plain and
Boston are both valid city representations for the same address. The inclusion of the AddressCleaner to the tool
union slightly increases the combined recall on both data
sets, with 2 and 13 new errors detected for MIT VPF and
BlackOak, respectively. However, we can see that generalpurpose tools cover a significant portion of the errors in these
data sets. Therefore, the added utility of a domain specific
tool is questionable on our study data sets.

4.8

Enrichment

Some strategies for error identification fail because of a
lack of evidence. In particular, rule-based systems and duplicate detection strategies would benefit from additional
attributes that can be utilized for new rules or for further
disambiguation of entities. Naturally, a promising direction
is to first enrich a given data set by adding more attributes
to each record that are obtained from other data sources.
Additional data sources can be relational web tables from
the public internet or corporate knowledge or databases. We
show the benefit of data enrichment using a straightforward
approach.
For the MIT VPF and BlackOak data sets, we obtained
additional tables from the data owners that could be joined
to the dirty tables we had. Using this technique, we manually enriched the original data sets with additional columns.
We only used columns that did not introduce additional duplicate rows. As a result, the results below are a best case
scenario. In the case of BlackOak, we appended three more
columns that included the person name, address information
and the date of birth in a different representation. Using
these 3 columns, we defined four more functional dependencies. In the case of MIT VPF, we appended seven more
columns by joining the vendor master table with another
5

Anonymized by authors.

Data set
MIT VPF
BlackOak

Rule-based
P
R
(+6%) 0.31 (+6%)0.20
0.46
0.43

Duplicates
P
R
(+2%) 0.18 (+1%) 0.03
0.41 (+5%) 0.68

table called Remit-To-Address. These seven attributes contained additional address data that we were able to compare
with the address of the same company in our data set using
seven new DCs. Moreover, the added attributes can be used
to assist with duplicate detection.
Table 9 shows the impact of the new rules and the improvement in duplicate detection in the two data sets. Duplicate detection leads to 5% better recall on BlackOak
because Tamr found more duplicate pairs that contained
conflicting values. In the case of MIT VPF, however, no
additional improvement was seen, because Tamr already
found all the duplicates prior to the enrichment process.
Rule-based error detection did not improve the results on
BlackOak, since all new rules overlapped with previously
defined ones. On the other hand, MIT VPF clearly benefited from the new rules, which exposed 32 more errors and
generated higher precision and recall. In summary, targeted
enrichment has the potential to improve the performance of
duplicate detection and rule-based systems.

5.

CONCLUSION AND FUTURE WORK

In this paper, we assembled a collection of real-world data
sets that exhibit most of the errors found in practice as well
a collection of data cleaning tools that have the potential
to detect the various types of errors. We performed an extensive empirical study by running each tool on each data
set and reported on its best possible performance. We also
examined how combinations of these tools can improve performance. Our experimental results led us to the following
conclusions: (1) There is no single dominant tool for the
various data sets and diversified types of errors. Single tools
achieved on average 47% precison and 36% recall (Table 5),
showing that a combination of tools is needed to cover all
the errors. (2) Picking the right order in applying the tools
can improve the precision and help reduce the cost of validation by humans. As shown in Figure 4, compared to the
simple union of all tools, a benefit-based ordering algorithm
achieved 28% average precision gain with only 3.5% average
recall loss on all the data sets when the threshold was 0.1.
(3) Domain specific tools can achieve high precision and recall compared to general-purpose tools, achieving on average
71% precision and 64% recall, but are limited to certain domains (Table 8). (4) Rule-based systems and duplicate detection benefited from data enrichment. In our experiments,
we achieved an improvement of up to 10% more precision
and 7% more recall (Table 9).
There are several promising future directions:
(1) A holistic combination of tools. We showed that
there is no single dominant data cleaning tool for all data
sets and blindly combining tools will likely decrease precision. Our ordering algorithm is a start in this direction.
However, new holistic approaches to combining tools may
be able to perform even better.
(2) A data enrichment system. The more knowledge
and context available for a data set, the easier it is to expose

1003

erroneous values. While we showed some improvements using enrichment, it was applied in an ad-hoc way. More work
is needed on how to enrich the data set with data from an
added data source that would be most useful for data cleaning. In particular, we need to find automatic approaches
to enrichment via corporate knowledge bases. Furthermore,
data sets with public information, such as the Rayyan Bib
data set, could be enriched through web tables and other
open data. For example the attributes journal title and
journal abbreviation, which suffer from missing values, can
be enriched through tools, such as DataXFormer [3], by looking for semantic transformations of journal title to its abbreviation.
(3) A novel interactive dashboard. Throughout this paper, it is clear that user engagement is central to any data
cleaning activities. At the same time, such engagement is
costly and must be comensurate with the potential benefit
from cleaning. It is therefore crucial to devise novel interactive dashboards to help users better understand the data
and be efficiently involved in the data cleaning process.
(4) Reasoning on real-world data. Analysis of synthetic
errors is important for testing the functionality of a single
tool but it does not help to identify the usefulness of a tool
in the real-world. We believe that it is vital to shift the focus
from optimizing single error-type tools towards creating endto-end data quality solution for real-world data sets.

6.

ACKNOWLEDGMENTS

We thank the Office of the VPF at MIT, the IT department of Merck, and BlackOak Analytics for kindly providing
their data sets. We also thank William D. Tietje from UC
Berkeley for providing the Animal data set.

7.

REFERENCES

[1] Z. Abedjan, C. Akcora, M. Ouzzani, P. Papotti, and
M. Stonebraker. Temporal rules discovery for web data
cleaning. PVLDB, 9(4):336 –347, 2015.
[2] Z. Abedjan, L. Golab, and F. Naumann. Profiling relational
data: a survey. VLDB Journal, 24(4):557–581, 2015.
[3] Z. Abedjan, J. Morcos, I. F. Ilyas, P. Papotti, M. Ouzzani,
and M. Stonebraker. DataXFormer: A robust data
transformation system. In ICDE, 2016.
[4] P. C. Arocena, B. Glavic, G. Mecca, R. J. Miller,
P. Papotti, and D. Santoro. Messing-Up with BART: Error
Generation for Evaluating Data Cleaning Algorithms.
PVLDB, 9(2):36–47, 2015.
[5] V. Chandola, A. Banerjee, and V. Kumar. Anomaly
detection: A survey. ACM Comput. Surv.,
41(3):15:1–15:58, July 2009.
[6] X. Chu, I. F. Ilyas, and P. Papotti. Holistic data cleaning:
Putting violations into context. In ICDE, 2013.
[7] X. Chu, J. Morcos, I. F. Ilyas, M. Ouzzani, P. Papotti,
N. Tang, and Y. Ye. Katara: A data cleaning system
powered by knowledge bases and crowdsourcing. In
SIGMOD, 2015.
[8] M. Dallachiesa, A. Ebaid, A. Eldawy, A. Elmagarmid, I. F.
Ilyas, M. Ouzzani, and N. Tang. Nadeef: A commodity
data cleaning system. In SIGMOD, 2013.
[9] T. Dasu and J. M. Loh. Statistical distortion: Consequences
of data cleaning. PVLDB, 5(11):1674–1683, 2012.
[10] A. Doan, A. Y. Halevy, and Z. G. Ives. Principles of Data
Integration. Morgan Kaufmann, 2012.
[11] X. L. Dong and D. Srivastava. Big Data Integration.
Synthesis Lectures on Data Management. Morgan &
Claypool Publishers, 2015.

[12] A. Elmagarmid, Z. Fedorowicz, H. Hammady, I. Ilyas,
M. Khabsa, and O. Mourad. Rayyan: a systematic reviews
web app for exploring and filtering searches for eligible
studies for cochrane reviews. In Abstracts of the 22nd
Cochrane Colloquium, page 9. John Wiley & Sons, Sept.
2014.
[13] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios.
Duplicate record detection: A survey. IEEE Transactions
on Knowledge and Data Engineering (TKDE), 19(1):1–16,
2007.
[14] W. Fan and F. Geerts. Foundations of Data Quality
Management. Morgan & Claypool, 2012.
[15] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Towards certain
fixes with editing rules and master data. VLDB Journal,
21(2):213–238, 2012.
[16] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. Mapping
and Cleaning. In ICDE, 2014.
[17] J. M. Hellerstein. Quantitative data cleaning for large
databases, 2008.
[18] H. Hemila and E. Chalker. Vitamin c for preventing and
treating the common cold. Cochrane Database Syst Rev, 1,
2013.
[19] I. F. Ilyas and X. Chu. Trends in cleaning relational data:
Consistency and deduplication. Foundations and Trends in
Databases, 5(4):281–393, 2015.
[20] E. T. Jaynes. On the rationale of maximum-entropy
methods. Proceedings of the IEEE, 70(9):939–952, 1982.
[21] S. Kandel, A. Paepcke, J. Hellerstein, and J. Heer.
Wrangler: Interactive visual specification of data
transformation scripts. New York, NY, USA, 2011.
[22] S. Kandel, A. Paepcke, J. M. Hellerstein, and J. Heer.
Enterprise data analysis and visualization: An interview
study. IEEE Trans. Vis. Comput. Graph.,
18(12):2917–2926, 2012.
[23] Z. Khayyat, I. F. Ilyas, A. Jindal, S. Madden, M. Ouzzani,
P. Papotti, J.-A. Quiané-Ruiz, N. Tang, and S. Yin.
Bigdansing: A system for big data cleansing. In SIGMOD,
pages 1215–1230, 2015.
[24] W. Kim, B.-J. Choi, E.-K. Hong, S.-K. Kim, and D. Lee. A
taxonomy of dirty data. Data Min. Knowl. Discov.,
7(1):81–99, Jan. 2003.
[25] S. Kolahi and L. V. S. Lakshmanan. On Approximating
Optimum Repairs for Functional Dependency Violations. In
ICDT, 2009.
[26] F. Naumann and M. Herschel. An Introduction to
Duplicate Detection. Synthesis Lectures on Data
Management. Morgan & Claypool Publishers, 2010.
[27] C. Pit-Claudel, Z. Mariet, R. Harding, and S. Madden.
Outlier detection in heterogeneous datasets using
automatic tuple expansion. Technical Report
MIT-CSAIL-TR-2016-002, CSAIL, MIT, 32 Vassar Street,
Cambridge MA 02139, February 2016.
[28] N. Prokoshyna, J. Szlichta, F. Chiang, R. J. Miller, and
D. Srivastava. Combining quantitative and logical data
cleaning. PVLDB, 9(4):300–311, 2015.
[29] E. Rahm and H.-H. Do. Data cleaning: Problems and
current approaches. IEEE Data Engineering Bulletin,
23(4):3–13, 2000.
[30] M. Stonebraker, D. Bruckner, I. F. Ilyas, G. Beskales,
M. Cherniack, S. Zdonik, A. Pagan, and S. Xu. Data
curation at scale: The Data Tamer system. In CIDR, 2013.
[31] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a core
of semantic knowledge. In WWW, pages 697–706, 2007.
[32] M. Vartak, S. Rahman, S. Madden, A. Parameswaran, and
N. Polyzotis. Seedb: Efficient data-driven visualization
recommendations to support visual analytics. PVLDB,
8(13):2182–2193, Sept. 2015.
[33] J. Wang and N. Tang. Towards dependable data repairing
with fixing rules. In SIGMOD, pages 457–468, 2014.
[34] E. Wu and S. Madden. Scorpion: Explaining away outliers
in aggregate queries. PVLDB, 6(8):553–564, June 2013.

1004

The Data Analytics Group
at the Qatar Computing Research Institute
George Beskales
Gautam Das
Ahmed K. Elmagarmid
Ihab F. Ilyas
Felix Naumann
Mourad Ouzzani
Paolo Papotti
Jorge Quiane-Ruiz
Nan Tang
Qatar Computing Research Institute (QCRI), Qatar Foundation, Doha, Qatar
Web Site: http://www.da.qcri.qa/

{gbeskales,gdas,aelmagarmid,ikaldas,fnaumann,mouzzani,ppapotti,jquianeruiz,ntang}@qf.org.qa

1.

DATA ANALYTICS AT QCRI

The Qatar Computing Research Institute
(QCRI), a member of Qatar Foundation for Education, Science and Community Development, started
its activities in early 2011. QCRI is focusing on
tackling large-scale computing challenges that
address national priorities for growth and development and that have global impact in computing
research. QCRI has currently ﬁve research groups
working on diﬀerent aspects of computing, these
are: Arabic Language Technologies, Social Computing, Scientiﬁc Computing, Cloud Computing,
and Data Analytics.
The data analytics group at QCRI, DA@QCRI
for short, has embarked in an ambitious endeavour
to become a premiere world-class research group by
tackling diverse research topics related to data quality, data integration, information extraction, scientiﬁc data management, and data mining. In the
short time since its birth, DA@QCRI has grown to
now have eight permanent scientists, two software
engineers and around ten interns and postdocs at
any given time. The group contributions are starting to appear in top venues.

2.

RESEARCH FOCUS

DA@QCRI has built expertise focusing on three
core data management challenges: extracting data
from its natural digital habitat, integrating a large
and evolving number of sources, and robust cleaning
to assure data quality and validation.
We are focusing on the interaction among these
three core data management challenges, which we
call the “Data Trio”. We are investigating multiple new directions, including: handling unstructured data; interleaving extraction, integration, and
cleaning tasks in a more dynamic and interactive
process that responds to evolving datasets and realtime decision-making constraints; and leveraging
the power of human cycles to solve hard problems
SIGMOD Record, December 2012 (Vol. 41, No. 4)

such as data cleaning and information integration.
In this report, we describe sample research
projects related to the data trio as well some initial results. In the ﬁrst couple of years, we have
been mainly focusing on data quality management.

3.

DATA QUALITY

It is not surprising that the quality of data is
becoming one of the diﬀerentiating factors among
businesses and the ﬁrst line of defence in producing
value from raw input data. As data is born digitally
and is directly fed into stacks of information extraction, data integration, and transformation tasks, insuring the quality of the data with respect to business and integrity constraints has become more important than ever. Due to these complex processing
and transformation layers, data errors proliferate
rapidly and sometimes in an uncontrolled manner,
thus compromising the value and high-order information or reports derived from data.
Capitalizing on our combined expertise in data
quality [3, 4, 10, 11, 13, 15–18, 21, 22, 27, 29], we have
launched several projects to overcome diﬀerent challenges encountered in this area.

3.1

NADEEF - A Commodity Data Cleaning System

While data quality problems can have crippling
eﬀects, there is no end-to-end oﬀ-the-shelf solution
to (semi-)automate error detection and correction
w.r.t. a set of heterogeneous and ad-hoc quality
rules. In particular, there is no commodity platform
similar to general purpose DBMSs that can be easily customized and deployed to solve applicationspeciﬁc data quality problems. To address this critical requirement, we are building Nadeef, a prototype for an extensible and easy-to-deploy data
cleaning system that leverages the separability of
two main tasks: (1) specifying integrity constraints
33

and how to repair their violations in isolation; and
(2) developing a core platform that holistically applies these routines in a consistent, and a userguided way. More speciﬁcally, we are tackling the
following challenges for emerging applications:
Heterogeneity. Business and integrity constraintbased data quality rules are expressed in a large variety of formats and languages(e.g., [2,5,7,12,14,26])
from rigorous expressions (as in the case of functional dependencies), to plain natural language
rules enforced by code embedded in the application
logic itself (as in most practical scenarios). This diversity hinders the creation of one uniform system
to accept heterogenous data quality rules and enforces them on the data within the same framework.
For example, data collected by organizations, such
as Qatar Statistics Authority, is checked against
several constraint types, such as range constraints,
not-null constraints, inclusion dependencies, as well
as other sophisticated constraints (e.g., the age difference between a person and his/her father should
be greater than 15). Additionally, data may come
from diﬀerent sources and with diﬀerent formats.
Thus, we need to revisit how heterogenous quality
rules can be speciﬁed on and applied to heterogeneous data.
Interdependency. Even when we consider a single
type of integrity constraints, such as functional dependencies, computing a consistent database while
making a minimum number of changes is an NPhard problem [5]. Due to the complexity and interdependency of various data quality rules, solutions have usually been proposed for a single type
of rule. Considering multiple types of rules at the
same time is considered an almost impossible task.
While some attempts have recently tried to consider
multiple homogenous sets of rules that can be expressed in a uniﬁed language [6, 16], the problem is
still far from being solved.
Deployment. A large number of algorithms and
techniques have been proposed (e.g., [5, 16, 20, 29]),
each requiring its own setting and staging of the
data to be cleaned. Hence, it is almost impossible to
download one of them from a software archive and
run it on the data without a tedious customization
task, which in some cases is harder than the cleaning
process itself.
Data custodians. Data is not born an orphan.
Real customers have little trust in the machines
to mess with the data without human consultation. For example, at Qatar Statistics Authority,
all data changes must be justiﬁed and reviewed by
domain experts before being committed. Due to
the limited processing power of humans, scalabil34

ity of (semi-)manual techniques is very limited and
does not speak to the requirements of today’s largescale applications. Several attempts have tackled the problem of including humans in the loop
(e.g., [13,17,23,29]). Unfortunately, these attempts
still suﬀer from the aforementioned problems, but
provide good insights on including humans in eﬀective and scalable ways.
Metadata management. Cleaning data requires
collecting and maintaining a massive amount of
metadata, such as data violations, lineage of data
changes, and possible data repairs. In addition,
users need to understand better the current health
of the data and the data cleaning process through
summarization or samples of data errors before
they can eﬀectively guide any data cleaning process.
Providing a scalable data cleaning solution requires
eﬃcient methods to generate, maintain, and access
such metadata. For example, we need specialized
indices that facilitate fast retrieval of similar tuples
or limit pairwise comparisons to speciﬁc data partitions.
Incremental cleaning. Data is evolving all the
time. The simplistic view of stopping all transactions, and then cleaning and massaging the data
is limited to historical and static datasets. Unfortunately, theses settings are becoming increasingly
rare in practice. Data evolution suggests a highly
incremental cleaning approach. The system has to
respond to new evidences as they become available
and dynamically adjusts its belief on the quality and
repairing mechanisms of the data.
To achieve the separability between quality rule
speciﬁcation that uniformly deﬁnes what is wrong
and (possibly) why; and the core platform that
holistically applies these routines to handle how to
identify and clean data errors, we introduce an interface class Rule for deﬁning the semantics of data
errors and possible ways to ﬁx them. This class
deﬁnes three functions: vio(s) takes a single tuple
s as input, and returns a set of problematic cells.
vio(s1 , s2 ) takes two tuples s1 , s2 as input, and returns a set of problematic cells. ﬁx(setcell) takes
a nonempty set of problematic cells as input, and
returns a set of suggested expressions to ﬁx these
data errors.
The overall functioning of Nadeef is as follows.
Nadeef ﬁrst collects data and rules deﬁned by
the users. The rule compiler module then compiles these heterogeneous rules into homogeneous
constructs. Next, the violation detection module
ﬁnds what data is erroneous and why they are as
such, based on user provided rules. After identifying errors, the data repairing module handles
SIGMOD Record, December 2012 (Vol. 41, No. 4)

the interdependency of these rules by treating them
holistically. Nadeef also manages metadata related to its diﬀerent modules. These metadata can
be used to allow domain experts and users to actively interact with the system. As we progress
in this project, we will post new developments at
http://da.qcri.org/NADEEF.

3.2

Holistic Data Cleaning

The heterogeneity and the interdependency challenges mentioned above motivated the study of
novel repair algorithms aiming at automatically
producing repairs of high quality and for a large
class of constraints. Our holistic data cleaning algorithm [6] tackles the two problems by exploiting a
more general language for constraint deﬁnition and
by introducing a holistic approach to their repair.
As a ﬁrst step toward generality, we deﬁne quality
rules by means of denial constraints (DCs) with adhoc predicates. DCs subsume existing formalisms
and can express rules involving numerical values,
with predicates such as “greater than” and “less
than”.
To handle interdependency, violations induced by
the DCs are compiled into a conﬂict hypergraph in
order to capture the interaction among constraints
as overlaps of the violations on the data. The proposed mechanism generalizes previous deﬁnitions of
hypergraphs for FD repairing. It is also the ﬁrst
proposal to treat quality rules with diﬀerent semantics and numerical operators in a uniﬁed artifact.
Such holistic view of the conﬂicts is the starting
point for a novel deﬁnition of repair context, which
allows automatic repairs with high quality and scalable execution time w.r.t. the size of the data. The
repair algorithm is independent of the actual cost
model. Experiments on heuristics aiming at cardinality and distance minimality show that our algorithm outperforms previous solutions in terms of
the quality of the repair.

3.3

Guided Data Repair

GDR, a Guided Data Repair framework [28, 29]
incorporates user feedback in the cleaning process
with the goal of enhancing and accelerating existing automatic repair techniques while minimizing
user involvement. GDR consults the user on the
updates that are most likely to be beneﬁcial in improving data quality. GDR also uses machine learning methods to identify and to apply the correct updates directly to the database without the actual involvement of the user on these speciﬁc updates. To
rank potential updates for consultation by the user,
GDR ﬁrst groups these repairs and quantiﬁes the
utility of each group using the decision-theory conSIGMOD Record, December 2012 (Vol. 41, No. 4)

cept of value of information (VOI). An active learning module orders updates within a group based on
their ability to improve the learned model. The user
is solicited for feedback, which is used to repair the
database and to adaptively reﬁne the training set
for the model.

3.4

Crowd-Cleaning

A main limitation of GDR is interacting with a
single user to clean the data. While this is suﬃcient for a small number of violations, it is deﬁnitely a bottleneck when the number of violations
rises to the order of thousands or millions. We
propose a data cleaning approach that is based on
crowd-sourcing. That is, we use thousands of users
to resolve the violations found in data. Crowdsourcing has been successfully used in other data
management contexts to process large amounts of
data (e.g., [19]).
Consulting a large number of users raises various
challenges such as the need for partitioning the data
in an eﬃcient and balanced way, assigning individual partitions to the best-matching human-cleaners,
and resolving conﬂicts among their feedbacks. In
order to partition the data in an eﬀective way, we
ﬁrst detect existing violations as well as the potential violations that might appear during the course
of data cleaning. Additionally, we keep track of
the previously solved violations. The data is partitioned based on the obtained violations through
standard graph clustering algorithms. Each partition is assigned to a human-cleaner such that a
global objective function is maximized. This function reﬂects a load balancing criterion as well as the
quality of matching between each partition and the
expertise of the assigned human-cleaner.

3.5

Large-Scale Deduplication in Data
Tamer

Recently, we introduced SCADD, a SCAlable
DeDuplication system, to enable scalable data deduplication to a large number of nodes. SCADD is
part of a large data integration system named Data
Tamer, which we are currently developing in collaboration with MIT [24]. One of the goals of SCADD
is to learn a deduplication classiﬁer that (i) carefully selects which attributes to consider, (ii) successfully handles missing values, and (iii) aggregates
the similarities between diﬀerent attributes. To devise a new system to perform data deduplication at
a large scale, we have to deal with several research
challenges, including:
Large data volume. Existing deduplication techniques are not suitable for processing the sheer
35

amount of data that is processed by modern applications. Scalable deduplication is challenging
not only because of the large amount of records,
but also because of the signiﬁcant amount of data
sources. For example, in web site aggregators, such
as Goby.com, tens of thousands of web sites need
to be integrated with several sites that are continuously added every day. On average, each source
contains tens of attributes and thousands of records.
Data heterogeneity and errors in data. Largescale data management systems usually consist of
heterogenous datasets that have diﬀerent characteristics. For example, Goby.com collects data
about hundreds of diﬀerent entity types, such as
golf courses, restaurants, and live music concerts.
Some entities might have unique attributes (e.g.,
the number of holes in a golf course) and diﬀerent
distributions of common attributes (e.g., the range
of vertical drops in downhill skiing sites vs. the
range of vertical drops in water parks). In such scenarios, datasets experience a large amount of noise
in attributes, such as non-standard attribute names,
non-standard formats of attribute values, syntactical errors, and missing attribute values.
Continuously adding new data sources and
new user feedback. Most of the modern applications, such as Goby.com, continuously collect data
over time and integrate the newly arrived data into
a central database. Data usually arrives at relatively high rates (e.g., a few sources need to be
integrated daily at Goby.com). The deduplication
decisions depend on a number of factors, such as
training data, previous deduplication results from
legacy systems, and explicit deduplication rules set
by expert users. Such an evidence is expected to
evolve over time to (i) reﬂect better understanding of the underlying data or (ii) accommodate new
data sources that might be substantially diﬀerent
from the existing sources. Clearly, it is infeasible
to rerun the deduplication process from scratch in
these cases. We need to provide eﬃcient methods
to update the deduplication results when new data
or new deduplication rules arrive.
Distributed environment.
For the large
amounts of data gathered by current applications,
it is inevitable to use multiple computing nodes to
bring down the computational complexity. However, parallelising the data deduplication process
causes another set of challenges: (i) it increases the
overhead of shipping data between nodes and (ii) it
requires coordinating the data deduplication task
across multiple computing nodes.
As a result, SCADD has a signiﬁcant emphasis
on the incremental aspect of the problem by al36

lowing every task in the deduplication process to
be eﬃciently reevaluated when new data arrives
or deduplication rules are changed. Furthermore,
one of the main insights for improving the scalability of deduplication is partitioning the input data
into categories (e.g., possible categories in data collected by Goby.com are golf clubs, museums, and
skiing sites). Such categorization allows for obtaining high-quality deduplication rules that are suitable for each speciﬁc category. Categorization also
allows for reducing the number of records that need
to be considered when running the data deduplication process (i.e., records that belong to diﬀerent
categories cannot be duplicates and hence can be
safely ignored).

4.

DATA PROFILING

An important step before any kind of datamanagement, -cleaning, or -integration that can be
well performed is data proﬁling, i.e., determining
various metadata for a given (relational) dataset.
These metadata include information about individual columns, such as uniqueness, data type, and
value patterns, and information about combinations of columns, such as inclusion dependencies or
functional dependencies. With more and more and
larger and larger datasets, especially from external
sources and non-database sources, (big) data proﬁling is becoming ever more important. Research
faces two principle challenges: eﬃciency and new
functionality.
The eﬃciency challenge of data proﬁling arises
from both the volume of data and the additional
complexity in the size of the schema, for instance
when verifying the uniqueness of all column combinations [1]. Especially for the sizes that “big data”
promises/threatens, distributed computation is unavoidable. We are currently investigating how to
scale conventional data proﬁling tasks to a large
number of nodes. In this quest, we hope to exploit the fact that many intermediate calculations
for various metadata are the same, so that the overall cost of calculating a “proﬁle” is lower than the
sum of the costs for the individual methods.
New kinds of data demand new proﬁling functionality: many tools and research methods concentrate
on relational data, but much data now comes in
other forms, such as XML, RDF, or text. While
some proﬁling tasks and methods can be carried
over, others either do not apply or must be newly
developed. For example, when examining a linked
data source, it is often useful to precede any further work by a topical analysis to ﬁnd out what
the source is about and by a graph analysis to ﬁnd
SIGMOD Record, December 2012 (Vol. 41, No. 4)

out how well the source is interlinked internally and
externally to other sources.

5.

DATA ANALYTICS
WORLD BANK

WITH

THE

We are building an analytics stack on unstructured data to leverage the vast amount of information available in news, social media, emails and
other digital sources. We give one example of a recent collaboration with the World Bank. The World
Bank implements several projects worldwide to provide countries with ﬁnancial help. The details of
those projects are documented and published along
with ﬁnancial details about the aid. The success of a
project is usually reﬂected in the local social growth
indicators where this project was implemented. A
ﬁrst step in analyzing the success of projects is to
create a map that displays project locations to determine where aid ﬂows are directed within countries; a process called geo-tagging. Currently, the
task of extracting project locations and geo-tagging
is executed by a group of volunteers who study
project documents and manually locate precise activity locations on a map, a project called Mapping
for Results.
We have developed a tool to retrieve the documents and reports relevant information to the
World Bank projects. We also run various classiﬁers and natural language processing tools on those
text documents to extract the mentioned locations.
Those locations are then geo-coded and displayed
on a map. The developed tool combines these
locations with other ﬁnancial data (e.g., procurement notices and contract awards) of projects into
a map, thus providing a holistic view about the
whereabouts of projects expenses. The technologies
used in this project include the UIMA extraction
framework and a just-in-time information extraction stack [9].

6.

ANALYTICS AND MINING OF WEB
AND SOCIAL MEDIA DATA

Online data content is increasing at an explosive
rate, as seen in the proliferation of websites, collaborative and social media, and hidden web repositories
(the so-called deep web). To cope with this information overload, designing eﬃcient ways of analyzing, exploring and mining such data is of paramount
importance. In collaboration with the social computing group at QCRI, we are developing mining
algorithms for crawling, sampling, and analytics of
such online repositories. Our methods address the
challenges of scale and heterogeneity (e.g., structured and unstructured) of the underlying data, as
SIGMOD Record, December 2012 (Vol. 41, No. 4)

well as access restrictions such as proprietary query
views oﬀered by hidden databases and social networks.
As a speciﬁc example, we are developing a system
for addressing the problem of data analytics over
collaborative rating/tagging sites (such as IMDB
and Yelp). Such collaborative sites have become
rich resources that users frequently access to (a)
provide their opinions (in the forms of ratings, tags,
comments) of listed items (e.g., movies, cameras,
restaurants, etc.), and (b) also consult to form judgments about and choose from among competing
items. Most of these sites either provide a confusing overload of information for users to interpret
all by themselves, or a simple overall aggregate of
user feedback. Such simple aggregates (e.g., average rating over all users who have rated an item, aggregates along pre-deﬁned dimensions, a tag cloud
associated with the item) is often too coarse and
cannot help a user quickly decide the desirability
of an item. In contrast, our system allows a user
to explore multiple carefully chosen aggregate analytic details over a set of user demographics that
meaningfully explain user opinions associated with
item(s) of interest. Our system allows a user to systematically explore, visualize and understand user
feedback patterns of input item(s) so as to make an
informed decision quickly. Preliminary work in this
project has been published in [8, 25].

7.

REFERENCES

[1] Z. Abedjan and F. Naumann. Advancing the
discovery of unique column combinations. In
Proceedings of the International Conference
on Information and Knowledge Management
(CIKM), 2011.
[2] M. Arenas, L. E. Bertossi, and J. Chomicki.
Consistent query answers in inconsistent
databases. Theory and Practice of Logic
Programming (TPLP), 3(4-5), 2003.
[3] G. Beskales, I. Ilyas, L. Golab, and
A. Galiullin. On the relative trust between
inconsistent data and inaccurate constraints.
In Proceedings of the International Conference
on Data Engineering (ICDE), 2013.
[4] G. Beskales, M. A. Soliman, I. F. Ilyas, and
S. Ben-David. Modeling and querying possible
repairs in duplicate detection. Proceedings of
the VLDB Endowment, 2009.
[5] P. Bohannon, W. Fan, M. Flaster, and
R. Rastogi. A cost-based model and eﬀective
heuristic for repairing constraints by value
modiﬁcation. In Proceedings of the ACM
International Conference on Management of
Data (SIGMOD), 2005.
37

[6] X. Chu, I. Ilyas, and P. Papotti. Holistic data
cleaning: Putting violations into context. In
Proceedings of the International Conference
on Data Engineering (ICDE), 2013.
[7] G. Cong, W. Fan, F. Geerts, X. Jia, and
S. Ma. Improving data quality: Consistency
and accuracy. In Proceedings of the
International Conference on Very Large
Databases (VLDB), 2007.
[8] M. Das, S. Thirumuruganathan,
S. Amer-Yahia, G. Das, and C. Yu. Who tags
what? an analysis framework. Proceedings of
the VLDB Endowment, 5(11):1567–1578,
2012.
[9] A. El-Helw, M. H. Farid, and I. F. Ilyas.
Just-in-time information extraction using
extraction views. In Proceedings of the ACM
International Conference on Management of
Data (SIGMOD), pages 613–616, 2012.
[10] M. G. Elfeky, A. K. Elmagarmid, and V. S.
Verykios. TAILOR: A record linkage tool box.
In Proceedings of the International Conference
on Data Engineering (ICDE), 2002.
[11] A. K. Elmagarmid, P. G. Ipeirotis, and V. S.
Verykios. Duplicate record detection: A
survey. IEEE Transactions on Knowledge and
Data Engineering (TKDE), 19(1), 2007.
[12] W. Fan, F. Geerts, X. Jia, and
A. Kementsietsidis. Conditional functional
dependencies for capturing data
inconsistencies. ACM Transactions on
Database Systems (TODS), 33(2), 2008.
[13] W. Fan, F. Geerts, N. Tang, and W. Yu.
Inferring data currency and consistency for
conﬂict resolution. In Proceedings of the
International Conference on Data Engineering
(ICDE), 2013.
[14] W. Fan, X. Jia, J. Li, and S. Ma. Reasoning
about record matching rules. Proceedings of
the VLDB Endowment, 2(1), 2009.
[15] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu.
Cerﬁx: A system for cleaning data with
certain ﬁxes. Proceedings of the VLDB
Endowment, 4(12):1375–1378, 2011.
[16] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu.
Interaction between record matching and data
repairing. In Proceedings of the ACM
International Conference on Management of
Data (SIGMOD), 2011.
[17] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu.
Towards certain ﬁxes with editing rules and
master data. VLDB Journal, 21(2), 2012.
[18] W. Fan, J. Li, N. Tang, and W. Yu.
Incremental detection of inconsistencies in

38

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]

distributed data. In Proceedings of the
International Conference on Data Engineering
(ICDE), pages 318–329, 2012.
M. J. Franklin, D. Kossmann, T. Kraska,
S. Ramesh, and R. Xin. CrowdDB: answering
queries with crowdsourcing. In Proceedings of
the ACM International Conference on
Management of Data (SIGMOD), 2011.
H. Galhardas, D. Florescu, D. Shasha,
E. Simon, and C.-A. Saita. Declarative data
cleaning: Language, model, and algorithms.
In Proceedings of the International Conference
on Very Large Databases (VLDB), 2001.
B. Marnette, G. Mecca, and P. Papotti.
Scalable data exchange with functional
dependencies. Proceedings of the VLDB
Endowment, 3(1), 2010.
F. Naumann. Quality-Driven Query
Answering for Integrated Information
Systems, volume 2261 of LNCS. Springer,
2002.
V. Raman and J. M. Hellerstein. Potter’s
Wheel: An interactive data cleaning system.
In Proceedings of the International Conference
on Very Large Databases (VLDB), 2001.
M. Stonebraker, D. Bruckner, I. F. Ilyas,
G. Beskales, M. Cherniack, S. Zdonik,
A. Pagan, and S. Xu. Data curation at scale:
The Data Tamer system. In Proceedings of the
Conference on Innovative Data Systems
Research (CIDR), 2013.
S. Thirumuruganathan, M. Das, S. Desai,
S. Amer-Yahia, G. Das, and C. Yu. Maprat:
Meaningful explanation, interactive
exploration and geo-visualization of
collaborative ratings. Proceedings of the
VLDB Endowment, 5(12):1986–1989, 2012.
J. Wijsen. Database repairing using updates.
ACM Transactions on Database Systems
(TODS), 30(3), 2005.
M. Yakout, A. K. Elmagarmid, H. Elmeleegy,
M. Ouzzani, and A. Qi. Behavior based record
linkage. Proceedings of the VLDB
Endowment, 3(1):439–448, 2010.
M. Yakout, A. K. Elmagarmid, J. Neville, and
M. Ouzzani. GDR: A system for guided data
repair. In Proceedings of the ACM
International Conference on Management of
Data (SIGMOD), 2010.
M. Yakout, A. K. Elmagarmid, J. Neville,
M. Ouzzani, and I. F. Ilyas. Guided data
repair. Proceedings of the VLDB Endowment,
4(5), 2011.

SIGMOD Record, December 2012 (Vol. 41, No. 4)

Visionary Paper

Road to Freedom in Big Data Analytics
Divy Agrawal∗ Sanjay Chawla Ahmed Elmagarmid Zoi Kaoudi Mourad Ouzzani
Paolo Papotti Jorge-Arnulfo Quiané-Ruiz Nan Tang Mohammed J. Zaki∗
Data Analytics Center, Qatar Computing Research Institute, HBKU

ABSTRACT

these become available. For example, Spark SQL [3] and MLlib [2]
are the Spark counterparts of Hive [24] and Mahout [1].
The second roadblock is that datasets are often produced by
different sources and hence they natively reside on different storage
platforms. As a result, users often perform tedious, time-intensive,
and costly data migration and integration tasks for further analysis.
Let us illustrate these roadblocks with an Oil & Gas industry example [13]. A single oil company can produce more than 1.5TB of
diverse data per day [6]. Such data may be structured or unstructured and come from heterogeneous sources, such as sensors, GPS
devices, and other measuring instruments. For instance, during the
exploration phase, data has to be acquired, integrated, and analyzed
in order to predict if a reservoir would be profitable. Thousands of
downhole sensors in exploratory wells produce real-time seismic
data for monitoring resources and environmental conditions. Users
integrate these data with the physical properties of the rocks to visualize volume and surface renderings. From these visualizations,
geologists and geophysicists formulate hypotheses and verify them
with ML methods, such as regression and classification. Training
of the models is performed with historical drilling and production
data, but oftentimes users have to go over unstructured data, such
as notes exchanged by emails or text from drilling reports filed in
a cabinet. Thus, an application supporting such a complex analytic pipeline has to access several sources for historical data (relational, but also text and semi-structured), remove the noise from the
streaming data coming from the sensors, and run both traditional
(such as SQL) and statistical analytics (such as ML algorithms)
over different processing platforms.
Similar examples can be drawn from many other domains such
as healthcare: e.g., IBM reported that North York hospital needs
to process 50 diverse datasets, which are on a dozen different internal systems [15]. These emerging applications clearly show the
need for complex analytics coupled with a diversity of processing
platforms, which raises two major research challenges.
Data Processing Challenge. Users are faced with various choices
on where to process their data, each choice with possibly orders
of magnitude differences in terms of performance. However, users
have to be intimate with the intricacies of the processing platform to
achieve high efficiency and scalability. Moreover, once a decision
is taken, users may end up being tied up to a particular platform.
As a result, migrating the data analytics stack to a more efficient
processing platform often becomes a nightmare. Thus, there is a
need to build a system that offers data processing platform independence. Furthermore, complex analytic applications require executing tasks over different processing platforms to achieve high performance. For example, one may aggregate large datasets with traditional queries on top of a relational database such as PostgreSQL,
but ML tasks might be much faster if executed on Spark [28]. How-

The world is fast moving towards a data-driven society where data
is the most valuable asset. Organizations need to perform very diverse analytic tasks using various data processing platforms. In
doing so, they face many challenges; chiefly, platform dependence,
poor interoperability, and poor performance when using multiple
platforms. We present R HEEM, our vision for big data analytics
over diverse data processing platforms. R HEEM provides a threelayer data processing and storage abstraction to achieve both platform independence and interoperability across multiple platforms.
In this paper, we discuss our vision as well as present multiple research challenges that we need to address to achieve it. As a case in
point, we present a data cleaning application built using some of the
ideas of R HEEM. We show how it achieves platform independence
and the performance benefits of following such an approach.

1.

WHY TIED TO ONE SINGLE SYSTEM?

Data analytic tasks may range from very simple to extremely
complex pipelines, such as data extraction, transformation, and
loading (ETL), online analytical processing (OLAP), graph processing, and machine learning (ML). Following the dictum “one
size does not fit all” [23], academia and industry have embarked on
an endless race to develop data processing platforms for supporting
these different tasks, e.g., DBMSs and MapReduce-like systems.
Semantic completeness, high performance, and scalability are key
objectives of such platforms. While there have been major achievements in these objectives, users still face two main roadblocks.
The first roadblock is that applications are tied to a single processing platform, making the migration of an application to new
and more efficient platforms a difficult and costly task. Furthermore, complex analytic tasks usually require the combined use of
different processing platforms. As a result, the common practice is
to develop several specialized analytic applications on top of different platforms. This requires users to manually combine the results
to draw a conclusion. In addition, users may need to re-implement
existing applications on top of faster processing platforms when
∗ Work

done while at QCRI.

c 2016, Copyright is with the authors. Published in Proc. 19th Inter
national Conference on Extending Database Technology (EDBT), March
15-18, 2016 - Bordeaux, France: ISBN 978-3-89318-070-7, on OpenProceedings.org. Distribution of this paper is permitted under the terms of the
Creative Commons license CC-by-nc-nd 4.0

Series ISSN: 2367-2005

479

10.5441/002/edbt.2016.45

ever, this requires a considerable amount of manual work in selecting the best processing platforms, optimizing tasks for the chosen
platforms, and coordinating task execution. Thus, this also calls for
multi-platform task execution.
Data Storage Challenge. Data processing platforms are typically
tightly coupled with a specific storage solution. Moving data from
a certain storage (e.g., a relational DB) to a more suitable processing platform for the actual task (e.g., Spark on HDFS) requires
shuffling data between different systems. Such shuffling may end
up dominating the execution time. Moreover, different departments
in the same organization may go for different storage engines due
to legacy as well as performance reasons. Dealing with such heterogeneity calls for data storage independence.
To tackle these two challenges, we envision a system, called
R HEEM1 , that provides both platform independence and interoperability (Section 2). In the following, we first discuss our vision
for the data processing abstraction (Section 3), which is fully based
on user-defined functions (UDFs) to provide adaptability as well as
extensibility. This processing abstraction allows both users to focus only on the logic of their data analytic tasks and applications to
be independent from the data processing platforms. We then discuss how to divide a complex analytic task into smaller subtasks
to exploit the availability of different processing platforms (Section 4). As a result, R HEEM can run simultaneously a single data
analytic task over multiple processing platforms to boost performance. Next, we present our first attempt to build an instance application based on some of the ideas of R HEEM and the resulting
benefits (Section 5). We then show how we push down the processing abstraction idea to the storage layer (Section 6). This storage
abstraction allows both users to focus on their storage needs and the
processing platforms to be independent from the storage engines.
Some initial efforts are also going into the direction of providing data processing platform independence [11, 12, 21] (Section 7).
However, our vision goes beyond the data processing. We not only
envision a data processing abstraction but also a data storage abstraction, allowing us to consider data movement costs during task
optimization. We give a research agenda highlighting the challenges that need to be tackled to build R HEEM in Section 8.

2.

Figure 1: R HEEM data processing abstraction.
mains [4, 6, 13, 15]. These pipelines require first combining multiple processing platforms to perform each task of the process and
then integrating the results. For instance, many companies are already adopting a lambda architecture, which combines both batch
and stream processing. Our vision goes beyond batch or stream
processing to any kind of data analytics paradigm. We envision a
system that eases the integration among different processing platforms by automatically dividing a task into subtasks and determining the underlying platform for each subtask.
R HEEM. The foundation of our vision is a three-layer data processing abstraction that sits between user applications and data processing platforms (e.g., Hadoop or Spark). Figure 1 depicts these
three layers: the application layer models all application-specific
logic; the core layer provides the intermediate representation between applications and processing platforms; and the platform
layer embraces the underlying processing platforms. In contrast
to DBMSs, R HEEM decouples physical and execution levels. This
separation allows applications to express physical plans in terms of
algorithmic needs only, without being tied to a particular processing platform. The communication among these levels is enabled
by operators defined as UDFs. These three layers allow R HEEM to
provide applications with platform independence. Providing platform independence is the first step towards realizing multi-platform
task execution, which is crucial to achieve the best performance at
all times. For example, Figure 2 shows the benefits of running
the SVM algorithm on different datasets from LIBSVM2 with only
one hundred iterations, as a Spark job and as a plain Java program.
We observe that, for small datasets, executing SVM as a plain Java
program is up to one order of magnitude faster than executing it on
Spark. Indeed, this performance gap gets bigger with the number of
iterations. Using Spark pays off for big datasets only. Such results
show a great potential for platform independence and ultimately
multi-platform execution. R HEEM will be able to receive a complex analytic task, seamlessly divide it into subtasks, schedule each
task on the best processing platform, monitor task execution, and
aggregate results for users or applications. Achieving our vision requires tackling several challenges that we will discuss throughout

OUR VISION

We envision a system that frees applications and users from
being tied to a single data processing platform (platform independence) and provides interoperability across different platforms
(multi-platform task execution). We discuss these two aspects in
the following. We discuss data storage independence in Section 6.
Processing Platform Independence. Whenever a new platform
that achieves better performance than existing ones becomes available, one is enticed to move to the new platform. However, such
move does not usually come without pain. There is a clear need for
a system that frees us from the burden and cost of re-implementing
applications from one platform to another. Mahout [1] and MLlib [2] clearly illustrate this need, as all ML algorithms initially
implemented in Hadoop had to be re-implemented in Spark. In
addition, there are cases where, for the same task but with a different input, one platform is better than another. Thus, the system
we envision should not only provide platform independence, but
also should be able to select the best available platform to execute
a given task in order to deliver better performance.
Multi-Platform Task Execution. We are witnessing the emergence of complex data analytic pipelines in many different do1 Rheem

2 http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/

is a native gazelle species in Qatar.

480

are available to the developer to deploy a new application on top of
R HEEM. Developers can still define new operators as needed.
Example 2: In the above ML example, the application optimizer
maps Initialize to a Map physical operator and Process to a
GroupBy physical operator. R HEEM provides two different implementations for GroupBy: the SortGroupBy (sort-based) and
HashGroupBy (hash-based) operators from which the optimizer of
the core level will have to choose.
2

Figure 2: SVM on Spark and Java.
the paper and then summarize in Section 8.
To show the benefits of our R HEEM vision, we have fully developed one data cleaning application based on it [19]. While this
initial instance provides only platform independence, its performances are encouraging and already demonstrate the advantages
of our vision (see Section 5).
Similar to the data processing abstraction, we envision a threelevel data storage abstraction to uphold data processing tasks. The
data storage abstraction is also composed of an application, a core,
and an execution level. The R HEEM data storage abstraction functions symmetrically as the data processing abstraction. We shall
further discuss this storage abstraction in Section 6.

3.

Once an application has produced a physical plan for a given
input task, R HEEM divides this physical plan into task atoms,
i.e., sub-tasks, which are the units of execution. A task atom (a part
of the execution plan) is a sub-task to be executed on a single data
processing platform. It will then translate the task atoms into an
execution plan by optimizing each task atom according to a target
platform. Finally, it schedules each task atom to be executed on its
corresponding platform. Therefore, in contrast to DBMSs, R HEEM
produces execution plans that can run on multiple platforms.
Platform Layer. At this layer, execution operators (in an execution plan) define how a task is executed on the underlying processing platform. In other words, an execution operator is the platformdependent implementation of a physical operator. R HEEM relies on
existing data processing platforms to actually run input tasks.

DATA PROCESSING ABSTRACTION

In this section, we detail the abstraction layers of R HEEM and
show how users can interact with the system at each layer.

3.1

Example 3: Again in the above ML example, the MapPartitions
and ReduceByKey execution operators for Spark are one way to
perform Initialize and Process.
2

Abstraction Layers

In contrast to a logical operator, an execution operator works on
multiple data quanta rather than a single one, which enables the
processing of multiple data quanta with a single function call.
Flexible operator mappings. Defining mappings between execution and physical operators is the developers’ responsibility whenever a new platform is plugged to the core. Our goal is to rely on a
mapping structure to model the correspondences between operators
together with context information. Such context is needed for the
effective and efficient execution of each operator. For instance, the
Process logical operator maps to two different physical operators
(SortGroupBy and HashGroupBy). In this case, a developer could
use the context to provide hints to the optimizer for choosing the
right physical operator at run time. Developers will provide only
a declarative specification of such mappings; the system will use
them to translate physical operators to execution operators.

R HEEM provides a set of operators at each layer, namely, logical operators, physical operators, and execution operators. The
input of the application layer is the logical operators provided by
users (or generated by a declarative query parser) and the output is
a physical plan. The physical plan is then passed to the core layer
where multi-platform optimizations take place to produce an execution plan. In contrast to a DBMS, R HEEM decouples the physical
level from the execution one. This separation allows applications to
express a physical plan in terms of algorithmic needs only, without
being tied to a particular processing platform.
Application Layer. A logical operator is an abstract UDF that
acts as an application-specific unit of data processing. One can see
a logical operator as a template where users provide the logic of
their tasks. Such abstraction enables both ease-of-use, by hiding
implementation details from users, and high performance, by allowing several optimizations, e.g., seamless distributed execution.
A logical operator works on data quanta, which are the smallest
units of data elements from the input datasets. For instance, a data
quantum represents a tuple in the input dataset or a row in a matrix.
This fine-grained data model allows R HEEM to apply operators in
a highly parallel fashion and thus achieve better performance.

3.2

User Interaction

We distinguish between two types of users: end-users, who interact with the applications, and developers, who interact with the
system at all the three layers. We discuss below how developers
define operators (UDFs) at every layer of the abstraction.
Application layer. At this layer, developers model a data processing application by specifying a set of abstract logical operators. End-users implement these operators to express their analytic
tasks. R HEEM provides an abstract LogicalOperator that defines
the method applyOp. Logical operators of any application extend LogicalOperator and provide an implementation of applyOp.
R HEEM invokes this method at runtime to apply a logical operator. In addition to logical operators, an application developer could
also expose a declarative language for users to define their tasks
(e.g., queries). The application is then responsible for translating
a declarative query into a logical plan. Then, the application optimizer translates the logical plan into a physical plan.
Core layer. R HEEM provides a pool of physical operators for applications to produce physical plans. To enable extensibility, the

Example 1: Consider a developer who wants to offer end users
logical operators to implement various ML algorithms. The developer can define three basic operators: (i) Initialize, for initializing
algorithm-specific parameters, e.g., initializing cluster centroids,
(ii) Process, for the computations required by the ML algorithm,
e.g., finding the nearest centroid of a point, (iii) Loop, for specifying the stopping condition. Users implement algorithms such as
SVM, K-means, and linear/logistic regression with them.
2
The application optimizer translates logical operators into physical operators that will form the physical plan at the core layer.
Core Layer. This layer is the heart of R HEEM. It exposes a pool
of physical operators, each representing an algorithmic decision
for executing an analytic task. A physical operator is a platformindependent implementation of a logical operator. These operators

481

system also provides an abstract PhysicalOperator, with the abstract method applyOp, for developers to define their own physical
operators. Developers define a new physical operator to fill two different needs. First, developers define a wrapper operator to execute
the logical operator together with some physical details, such as algorithmic decisions and schema details. The wrapper operator follows the signature of the logical operator. Second, since the output
of a specific operator might not fit as input of a subsequent operator,
developers define enhancer operators to fill possible gaps between
wrapper operators. For example, an application for K-means clustering might only expose the GetCentroid (for getting the closest
centroid of a data point) and SetCentroids (for computing the new
centroids) logical operators. GetCentroid outputs a data point and
its closest centroid, while SetCentroids requires a centroid and all
its closest data points. Here, the developer provides a GroupBy
enhancer operator between GetCentroid and SetCentroid.
Platform layer. To model a data processing platform, developers
extend the abstract ExecutionOperator and implement its applyOp
method. There are two main scenarios. If a new physical operator has been defined, e.g., because the developer is adding a new
application, then it must be supported with a corresponding execution operator in the actual execution platform. In a different
scenario, the developer is adding a new platform to the execution
layer. In this case, every physical operator must be supported with
a corresponding execution operator in the new execution platform.
R HEEM uses these execution operators to produce an execution
plan and pushes “down” execution details to the underlying platform, such as data distribution, parallel execution, and data storage. At the end, the target processing platform simply performs an
execution plan in its own data and processing model.

4.

a physical plan from an application and passes it to the multiplatform task optimizer to generate a plan for execution on the
underlying processing platforms. We envision the multi-platform
task optimizer to deal with five aspects. First, the optimizer should
consider operators as first-class citizens. That is, its optimization
process should be fully based on UDFs optimization techniques.
We will base our solutions on different existing optimization techniques, such as Manimal [16], PACTs [25], and SOFA [22], but we
also need to devise new optimization techniques to support different processing platforms. Second, the optimizer should consider
rules and cost models for its optimizations as plugins and not hardcoded as in traditional database optimizers. In other words, these
two aspects should be decoupled from the optimizer in order to allow for extensibility when new processing platforms are added by
developers. Third, it has to consider inter-platform cost models to
effectively take into account the cost of moving data and computation across underlying processing platforms. The main difficulty
here comes from the fact that underlying frameworks are typically
highly heterogeneous in terms of both data representations and processing paradigms. Fourth, it should divide a physical plan into
task atoms according to the supported underlying data processing
platforms. Recall that it is the underlying processing platform that
ensures the execution of task atoms. The main challenge in this
aspect is to find a way to divide a task into atoms seamlessly from
users. Last, but not least, it should also apply traditional physical
optimizations, whenever possible. Examples are shared scans and
optimized data access paths, such as index access. Achieving this
is difficult as such optimizations should be general in order to be
efficient on any processing platform.
Once an execution plan is built, the multi-platform task optimizer
passes it to the Executor (see Figure 1) for: (i) scheduling the resulting execution plan on the selected data processing frameworks,
(ii) monitoring the progress of plan execution, (iii) coping with failures, and (iv) aggregating and returning results to users.

MULTI-LAYER OPTIMIZATIONS

In contrast to traditional data management systems, which are
tied to a specific data processing platform, R HEEM’s goal is not
only platform independence but also multi-platform execution. To
efficiently deal with both aspects, we envision optimizations at each
layer: (i) at the application layer, we validate an input task, translate
it into a logical plan, and then produce an optimized physical plan,
(ii) at the core layer, we translate a physical plan into an execution
plan by dividing the query into task atoms, and (iii) at the platform
layer, we further refine a task atom based on the actual platform.

4.1

4.3

Application-Layer Optimizations

5.

In our envisioned system, users will be able to express their tasks
either procedurally (via logical operators) or declaratively (using a
query language). Given an input task, the application optimizer
builds a logical plan and performs some pre-defined optimizations,
such as operator push-down. Once the logical plan is built, the application optimizer produces an optimized physical plan by translating each logical operator into a wrapper physical operator. Recall
that a wrapper receives a logical operator as input. Additionally,
the application optimizer might use enhancer physical operators to
boost performance; for example, it may plug-in a GroupBy physical operator followed by a CrossProduct operator to perform a
cross product inside a group only. This avoids a costly cross product over the entire input dataset. As an example of this kind of
optimization we refer to [19]. Then, the application sends the optimized physical plan to the core layer.

4.2

Platform-Layer optimizations

Once at a target processing platform, we envision a third optimization phase that uses plugged-in platform-specific optimization tools. For instance, if the selected platform for a task atom
is Hadoop, we could further optimize an execution plan by using
Starfish [14]. Notice that the data processing platform itself can
also perform some additional optimizations, e.g., if an execution
plan is given as input to Spark in the form of a Shark query [26].

APPLICATIONS

Clearly, a large number of applications benefit from our vision.
As a proof of concept, we present here a data cleaning application
we developed using part of R HEEM’s vision, mainly platform independence. We are currently developing two other applications: a
machine learning application and a graph processing application.

5.1

Data Cleaning in RHEEM

Demand. Ensuring high quality data is challenging because of
the variety of data dirtiness, such as typos, duplicates, and missing
values. However, detecting errors is a combinatorial problem that
quickly becomes expensive with the size of the data, thus limiting
the applicability of cleaning systems.
Our solution. We built B IG DANSING, a Big Data Cleansing on
top of R HEEM [19]. The two distinct features of B IG DANSING are
its ease-of-use and high scalability; both natural consequences of
the R HEEM abstraction vision. B IG DANSING models data quality
rules with five operators, namely Scope, Block, Iterate, Detect,
and GenFix. These operators allow users to capture the semantics

Core-Layer Optimizations

The optimizations at the core layer are the responsibility of the
multi-platform task optimizer (see Figure 1). R HEEM receives

482

Figure 3: R HEEM execution times for violations detection.
of error detection and possible repairs generation at the application
layer (see [19] for details).
Ease-of-use. The developer of the application has to come up with
the physical plan of the cleaning process (preferably via an automatic optimization process). The detection part of B IG DANSING
is composed of a sequence of five physical operators, which are
fed with the logics coming from the corresponding logical operators. Similarly, the logical operators require only a few lines of
code [19], allowing for ease-of-use.
High scalability. Figure 3 shows a comparison of the performance
for a single Detect UDF versus our operators for the same task.
The left-side subfigure clearly shows the benefits of the abstraction
with operators that enables finer granularity for the distributed execution. The right-side subfigure shows a comparison of B IG DANS ING against state-of-the-art approaches on Spark. We observe that
R HEEM enables orders of magnitude better performance than baselines, which we had to stop after 22 hours. Here, as an example
of extensibility, we extended the set of physical R HEEM operators
with a new join operator (called IEJoin [20]) to boost performance.

5.2

Figure 4: R HEEM data storage abstraction.
minimum unit of data quanta transformation (e.g., projection).
The benefit of such a data storage abstraction is twofold. First, it
provides interoperability across storage platforms to simplify users
specification about how to store or transform their datasets from
one platform to another. Second, it offers opportunities to fully optimize a data flow in order to further improve the performance of
data processing tasks. Still, two main challenges make this problem
hard: (i) how to unify the abstraction for data storage and access
over multiple platforms, and (ii) how to seamlessly decide where
and how to store data. Our current efforts into this direction include Cartilage [18], which is a unified data storage representation.
In summary, Cartilage introduces the notion of data transformation
plans, analogous to logical query plans, that specify a sequence of
data transformations that should be applied to raw data as it is uploaded into a storage system. This allows for intermediate storage
optimizations based on users and applications needs. For example,
WWHow! [17] is a first effort for a unified data storage optimizer.
Embracing hot data. Accessing data through a unified data storage
might degrade the performance of processing platforms because the
data might not be in the required format. Thus, we envision processing platforms or storage applications with specialized buffers
for embracing frequently accessed data in their native format.

When to Use RHEEM?

It should be clear at this point how the proposed three layers
enable better performance and more freedom in developing applications with respect to existing solutions. However, there is a tradeoff. A developer who decides to use R HEEM instead of one of the
alternative systems may need to implement new operators as required by the target application. This is because our pool of default
physical operators is not as exhaustive as the operators provided by
the specific underlying platforms. For example, we had to implement the IEJoin operator in R HEEM to boost the performance of
our data cleaning application. While this may require extra effort
from a developer, we believe that this a reasonable price to pay for
platform independence when performance is crucial.

6.

7.

RELATED WORK

The closest work to us is Musketeer [12], which provides an intermediate representation between applications and data processing platforms. While Musketeer has the merit of proposing an optimizer for the supported applications and platforms, it considers
neither the costs of data movement across processing platforms nor
the fact that multiple platforms may be able to perform the same
job. Furthermore, it lacks the extensibility that we advocate with
our proposal. In fact, only Musketeer developers can integrate new
processing platforms or applications. This is in fact similar to integrating a new storage system on an existing processing platform,
such as Spark or Hadoop MapReduce. In contrast, in R HEEM, users
can achieve these tasks with mappings and new physical operators.
DBMS+ [21] is another work that aims at embracing several processing and storage platforms for declarative processing. However,
DBMS+ is not adaptive and extensible as it is limited by the expressiveness of its declarative language. Furthermore, it is unclear how
it abstracts underlying platforms seamlessly. BigDAWG [11] has
recently been proposed as a federated system that enables users to
run their queries over multiple vertically-integrated systems such
as column stores, NewSQL engines, and array stores. As a result, users can leverage the advantages of each of them. However,
users explicitly specify the underlying platforms (called islands)
on which their queries must run on. This implies that users need to
know how to divide their queries into subqueries and which underlying platform is best suited for each of them.

DATA STORAGE ABSTRACTION

So far we have focused our attention on the data processing side
of our vision. Symmetrically to the data processing, we envision a
data storage abstraction to provide interoperability among different
data storage platforms.
Figure 4 illustrates the R HEEM data storage abstraction. Each
layer of abstraction has a set of operators (i.e., UDFs): logical
operators (l-store) at the application layer, physical operators (pstore) at the core layer, and execution operators (x-store) at the
platform layer. At the application layer different storage applications, e.g., Dropbox, or data processing platforms, e.g., Hadoop or
PostgreSQL, output a physical storage plan in a homogeneous format defined by R HEEM. Then, at the core layer, R HEEM takes the
physical storage plan as input and produces an optimized execution
storage plan. Finally, at the platform layer, a data storage platform
stores or accesses a dataset according to the execution storage plan.
Note that an execution storage plan is composed of storage atoms,
i.e., the counterpart of task atoms, which are processed by a different data storage platform. It is worth noting that while a data
quantum is the data unit itself (e.g., a tuple), a storage atom is the

483

9.

Other groups have been working on a general platform for big
data analytics [5, 7–9, 27, 29]. For example, AsterixDB [5] offers
an open data model, native data storage and indexing, declarative
querying over multiple datasets, and a rule-based optimizer. SimSQL [10] compiles SQL queries into Java code that can run on top
of Hadoop. Moreover, users can use UDFs to materialize views
with simulated data, which enables a range of applications requiring stochastic analytics. PACTs [7] extends the MapReduce programming model with second-order functions on top of Nephele, a
processing platform that R HEEM can also use as underlying platform. However, none of the above systems provides the multiplatform data processing and storage we propose with R HEEM.

8.

[1]
[2]
[3]
[4]
[5]
[6]

[7]

ROAD TO FREEDOM

[8]

“I have walked that long road to freedom. I have tried not
to falter; I have made missteps along the way. But I have
discovered the secret that after climbing a great hill, one only
finds that there are many more hills to climb... and I dare not
linger, for my long walk is not yet ended.”
– Nelson Mandela –

[9]
[10]

[11]

Oftentimes, users are confronted with the hard decision to choose
the right processing platform given the requirements of their analytic application. In addition, their data, born out of various processes, ends up in different storage platforms. To make things
worse, the same application may have tasks requiring different platforms to be performed efficiently. Thus, there is a real urgency
to free both users and data from (i) being tied to a specific platform, either for processing or storage, and (ii) going through the
pain of moving from one platform to another, depending on the
requirements of their applications and the characteristics of their
data. While the road to freedom is full of challenges, R HEEM data
processing and storage abstractions hold promise to achieve this
freedom. As a case in point, a data cleaning application [19] is our
first success towards this goal. IEJoin [20] also showcases the extensibility of R HEEM. While we have laid down the basic ideas on
how to build R HEEM, many challenges remain to be addressed.
(1) Extensibility. How to adapt to extensions and improvements in
a data processing platform without requiring the developers to go
into the source code? What is the right language to provide hints
to the optimizer? We envision an optimization process based on a
flexible data model, such as RDF. Developers will specify mappings between operators as well as encode rule- and cost-based
models in RDF triples. The optimizer will use this RDF representation as a first-class citizen in its optimization process.
(2) Multi-platform optimization. How to divide a task into atoms,
assign the best platform to each atom, and combine results? We
envision a solution based on data processing profiles and interplatform cost models. A data processing profile denotes the type
of data processing a platform can support, e.g., batch-processing
profile for Hadoop. An inter-platform cost model will capture different multi-platform aspects, such as the cost of transferring and
transforming data from one processing platform to another.
(3) Unified storage abstraction. How to provide a unified abstraction for data storage and access for multiple storage platforms?
How to decide where and how to store data? We envision a threelayer abstraction as discussed in Section 6. This abstraction will
enable storage platforms with specialized data buffers to embrace
frequently accessed data in their native format.
In summary, the above challenges can be categorized into five
main research themes: (i) processing and storage abstractions,
(ii) platform-independent task specification, (iii) multi-platform
optimization, (iv) multi-platform execution, and (v) data storage
and data movement optimizations.

[12]

[13]

[14]
[15]
[16]
[17]
[18]
[19]

[20]

[21]
[22]

[23]
[24]

[25]

[26]

[27]

[28]
[29]

484

REFERENCES

Apache Mahout. http://mahout.apache.org/.
Spark MLlib: http://spark.apache.org/mllib/.
Spark SQL. http://spark.apache.org/sql/.
Powering Big Data at Pinterest. Interview with Krishna Gade.
http://goo.gl/UMGSvy, April 2015.
S. Alsubaiee et al. AsterixDB: A scalable, open source BDMS.
PVLDB, 7(14), 2014.
A. Baaziz and L. Quoniam. How to use big data technologies to
optimize operations in upstream petroleum industry. In 21st World
Petroleum Congress, 2014.
D. Battré, S. Ewen, F. Hueske, O. Kao, V. Markl, and D. Warneke.
Nephele/PACTs: A Programming Model and Execution Framework
for Web-Scale Analytical Processing. In SoCC, 2010.
V. Borkar, M. Carey, R. Grover, N. Onose, and R. Vernica. Hyracks:
a flexible and extensible foundation for data-intensive computing. In
ICDE, 2011.
F. Bugiotti, D. Bursztyn, A. Deutsch, I. Ileana, and I. Manolescu.
Invisible Glue: Scalable Self-Tuning Multi-Stores. In CIDR, 2015.
Z. Cai, Z. Vagena, L. L. Perez, S. Arumugam, P. J. Haas, and C. M.
Jermaine. Simulation of database-valued markov chains using
simsql. In SIGMOD, 2013.
A. Elmore et al. A Demonstration of the BigDAWG Polystore
System. In VLDB 2015 (demo), 2015.
I. Gog, M. Schwarzkopf, N. Crooks, M. P. Grosvenor, A. Clement,
and S. Hand. Musketeer: All for One, One for All in Data Processing
Systems. In EuroSys, 2015.
A. Hems, A. Soofi, and E. Perez. How innovative oil and gas
companies are using big data to outmaneuver the competition.
Microsoft White Paper, http://goo.gl/2Bn0xq, 2014.
H. Herodotou and S. Babu. Profiling, what-if analysis, and cost-based
optimization of mapreduce programs. PVLDB, 4(11), 2011.
IBM. Data-driven healthcare organizations use big data analytics for
big gains. White paper, http://goo.gl/AFIHpk.
E. Jahani, M. J. Cafarella, and C. Ré. Automatic Optimization for
MapReduce Programs. PVLDB, 4(6):385–396, 2011.
A. Jindal, J. Quiané-Ruiz, and J. Dittrich. WWHow! Freeing Data
Storage from Cages. In CIDR, 2013.
A. Jindal, J. Quiané-Ruiz, and S. Madden. CARTILAGE: adding
flexibility to the hadoop skeleton. In SIGMOD, 2013.
Z. Khayyat, I. F. Ilyas, A. Jindal, S. Madden, M. Ouzanni, P. Papotti,
J.-A. Quiané-Ruiz, N. Tang, and S. Yin. BigDansing: A System for
Big Data Cleansing. In SIGMOD, 2015.
Z. Khayyat, W. Lucia, M. Singh, M. Ouzzani, P. Papotti, J.-A.
Quiané-Ruiz, N. Tang, and P. Kalnis. Lightning Fast and Space
Efficient Inequality Joins. PVLDB, 8(13), 2015.
H. Lim, Y. Han, and S. Babu. How to Fit when No One Size Fits. In
CIDR, 2013.
A. Rheinländer, A. Heise, F. Hueske, U. Leser, and F. Naumann.
SOFA: An extensible logical optimizer for UDF-heavy data flows.
Inf. Syst., 52:96–125, 2015.
M. Stonebraker and U. Çetintemel. “One Size Fits All": An Idea
Whose Time Has Come and Gone (Abstract). In ICDE, 2005.
A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, S. Anthony,
H. Liu, P. Wyckoff, and R. Murthy. Hive: A Warehousing Solution
over a Map-reduce Framework. PVLDB, 2(2), 2009.
K. Tzoumas, J.-C. Freytag, V. Markl, F. Hueske, M. Peters,
M. Ringwald, and A. Krettek. Peeking into the Optimization of Data
Flow Programs with MapReduce-style UDFs. In ICDE, 2013.
R. S. Xin, J. Rosen, M. Zaharia, M. J. Franklin, S. Shenker, and
I. Stoica. Shark: SQL and Rich Analytics at Scale. In SIGMOD,
2013.
Y. Yu, M. Isard, D. Fetterly, M. Budiu, U. Erlingsson, P. K. Gunda,
and J. Currey. DryadLINQ: A System for General-purpose
Distributed Data-parallel Computing Using a High-level Language.
In OSDI, 2008.
M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica.
Spark: Cluster computing with working sets. In HotCloud’10, 2010.
J. Zhou et al. SCOPE: Parallel Databases Meet MapReduce. VLDB
J., 21(5), 2012.

DataXFormer: An Interactive Data Transformation Tool
John Morcos1 Ziawasch Abedjan2 Ihab F. Ilyas1
Mourad Ouzzani3 Paolo Papotti3 Michael Stonebraker2
1
2
3
University of Waterloo
MIT CSAIL
Qatar Computing Research Institute
{jmorcos,ilyas}@uwaterloo.ca {abedjan,stonebraker}@csail.mit.edu {mouzzani,ppapotti}@qf.org.qa

ABSTRACT

values in a repository of reference data. We refer to the former type of transformations as “syntactic transformations”
and to the latter as “semantic transformations”.
Syntactic transformations are supported by many tools,
including the popular MS-Excel, Google Spreadsheets and
the more recent Wrangler [7]. However, to the best of our
knowledge, no automatic system or tool is available that
significantly covers the class of semantic transformations,
Semantic transformations are prevalent in many real-world
integration tasks, as witnessed in workloads of the data curation tool Tamr [9] and other data vendor companies.
When companies need to explore or aggregate their data,
they create dynamic views that join multiple databases.
Here, it is desirable to dynamically discover the desired
transformations that allows them to join multiple sources
on a unified attribute. Furthermore, useful mappings, such
as US Dollars to EUR, genome to coordinates, location to
temperature, change over time and requires users to lookup in a continuously updated repository, making previously
acquired reference datasets obsolete.
The main reason of why semantic transformations have
been neglected so far is that they cannot be computed by
solely considering the input values and applying a formula or
a string operation on them. Rather, the required transformations can often be found in a mapping table that is either
explicitly available to the application (e.g., as a dimension
table in a data warehouse) or hidden behind a transformation service or a Web form.
While collecting the adequate reference data for one specific transformation task is doable, it requires tedious crawling and curation exercises. This process cannot easily scale
for a large number of transformation tasks covering the long
tail of domains and topics. Usually, the subject matter expert in a company who is interested in converting its data
does not have the skills to obtain the reference data or to
code the transformation formula. Considering that the Web
provides a large repository of resources, it is useful to have
a service that can leverage these resources for on-demand
transformation tasks.
At CIDR 2015, we presented DataXFormer [1], a system that leverages Web tables and Web forms to perform
syntactic and semantic transformations. DataXFormer expects a transformation task specified as a variable-sized collection of examples, giving the entire input X and some
examples of the desired output Y . The following are
examples for the airport code to city name transformation: {(LHR, London), (ORD, Chicago), (CDG, P aris)}.
In most cases, the user provides the column labels of the

While syntactic transformations require the application of
a formula on the input values, such as unit conversion or
date format conversions, semantic transformations, such as
zip code to city, require a look-up in some reference data.
We recently presented DataXFormer, a system that leverages
Web tables, Web forms, and expert sourcing to cover a wide
range of transformations. In this demonstration, we present
the user-interaction with DataXFormer and show scenarios
on how it can be used to transform data and explore the
effectiveness and efficiency of several approaches for transformation discovery, leveraging about 112 million tables and
online sources.

Categories and Subject Descriptors
H.2.5 [Database
Databases

Management]:

Heterogeneous

Keywords
data integration, data enrichment, data transformation, web
tables, web forms, wrapper, deep web

1.

INTRODUCTION

Several data analytics tasks require integrating various
heterogeneous data sources, where the same or highly related information might be expressed in different forms.
While there has been a considerable amount of research on
schema matching and schema mapping, the related task of
transforming the actual values from one representation to
the target representation has been neglected. Examples of
data transformation tasks include mapping stock symbols
to company names, changing date format from MM-DD to
DD-MM and replacing cities by their countries. While some
transformations such as liters to gallons can be performed
by applying a formula or a program on the input values, others, such as company name to stock symbol and event to date,
require finding the mappings between the input and output
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGMOD’15, May 31–June 4, 2015, Melbourne, Victoria, Australia.
c 2015 ACM 978-1-4503-2758-9/15/05 ...$15.00.
Copyright 
http://dx.doi.org/10.1145/2723372.2735366.

883

Query:
(Input values X, Examples E)

Web Tables
Subsystem

Web Forms
Subsystem
The Web

Form Query
Transformer

Table Query
Transformer

Form Retrieval

Table Retrieval

Forms
Wrappers

Evaluation

Wrapper
Generator

in one of their column projections. We refer to this minimum threshold as τ . Our experiments showed that τ = 2
is a reasonable threshold for filtering irrelevant tables while
maintaining very high recall [1].
A major challenge is to maintain the interactivity of the
system through fast response times while being able to sift
through 112 million tables in the corpus to find matches for
the given examples. Thus, we maintain an inverted index
that points to the columns and tables that contain the X or
Y values. Since Web tables are heterogeneous and differ in
schema and size and some even lack column labels, we store
the tables within a universal main table (relation Cells as
in Figure 2); every cell of a Web table is represented by a
tuple that records the table, column and row IDs, along with
the tokenized, stemmed form of its value (tokenized). Using
this schema, we can simulate an inverted index by having
an index on the tokenized column. The relation is ordered
by tableid, columnid, rowid, simultaneously achieving two
advantages: (i) every column from a web table is stored contiguously, and (ii) the space requirement of this schema is
alleviated by compression, which is provided by most modern column-stores.

Wrapped
Forms

Index

Candidate
Tables

Corpus
Refinement

Augment

Solution Integration

Result: F={(x1,y1), (x2,y2), …(xn,yn)}

Figure 1: DataXFormer architecture

input and output values for the desired transformation,
which we denote by IX and IY , e.g., IX = airportcode and
IY = city.
In this demonstration, we show the various features of
DataXFormer and how it finds transformation answers in
Web tables and Web forms. We also show how DataXFormer interacts with the user to enrich its corpora, provide better answers and unlock previously unusable Web
forms. In particular, we trace the intermediate steps of
DataXFormer to find and wrap Web forms in an interactive manner. An initial version of DataXFormer is already
available at http://dataxformer.org.

2.

DataXFormer OVERVIEW

Figure 2: Schema for storing Web tables in a column-store

DataXFormer (see Figure 1) consists of two complementary
transformation engines: one based on locally-stored static
Web tables and another based on dynamically discovered
Web forms. The user starts by submitting a query, such
as the query that transforms airport codes to city names.
DataXFormer converts the user query into the corresponding
internal forms for each of the retrieval components, which
in turn return candidate Web tables and Web forms.
DataXFormer dispatches the transformation query simultaneously to both transformation engines. While the two
engines differ in their retrieval interface, response time and
coverage, we are currently taking the straightforward approach of engaging both systems simultaneously. In a final
step, the solution integration component presents the best
effort results from both sources for a given query. Here,
the user can intervene to choose among different transformation discovery approaches and results. We present and
rank the results of each subsystem separately because of the
significant difference in response time. Note that the Web
forms subsystem has a higher response time than the local
Web tables subsystem because it has to submit a request to
a remote server to transform each value. Having an automatic mechanism for merging the results of both sources is
a future effort, considering that they are good at different
transformation tasks.

2.1

In the current prototype, we store the Web tables in a
multi-node Vertica instance. Vertica employs projections on
tables in place of an inverted index. A projection is a specialized materialized view that is efficient to maintain and
load. We use a projection on Cells that is sorted on the
tokenized values. Additionally, DataXFormer includes a similarity index to retrieve all existing similar representations of
given X and Y values within a given edit distance threshold.
Using a projection, the table retrieval component retrieves
a set of relevant candidate tables for the given user query
and verifies the coverage of the candidate tables with respect to the query examples. If the coverage is above the
user-defined threshold, DataXFormer extracts the rest of the
required transformation values.
We now proceed to describe the transformation discovery
approaches that use DataXFormer’s Web table repository.
Direct matching The fastest and simplest way to find the
required transformation in the tables is to find tables that
contain the required minimum number of given examples,
as evidence for the two columns that encode the transformation. While the look-up operation is very fast, it is very
sensitive to the choice of the initial examples. In Figure 3,
we refer to the direct matching as iteration 1 of the look-up
operation, which matches Tables 1 and 2, with transformations for the airport codes FRA, LAX, and BOS.

Web tables subsystem

The task of the tables subsystem is to identify all the tables that contain a minimum number of the user examples

884

X	
  

Query
Y	
  

Table 1
code	
   city	
  

BER	
  

Berlin	
  

LAX	
  

Los	
  Angeles	
  

JFK	
  

New	
  York	
  

JFK	
  

New	
  York	
  

ORD	
   Chicago	
  

ORD	
   Chicago	
  

HBE	
  

?	
  

BOS	
  

IST	
  

?	
  

FRA	
  

?	
  

BOS	
  

?	
  

DFW	
   ?	
  

Boston	
  
Table 2
airport	
   city	
  

apc	
  

FRA	
  

Frankfurt	
  

DFW	
   Dallas	
  

DFW	
  

Dallas	
  

HBE	
  

Alexandria	
  

JFK	
  

New	
  York	
  

IST	
  

Istanbul	
  

BER	
  

Berlin	
  

FRA	
  

Frankfurt	
  

Iteration 1

plies an expectation-maximization approach [5] that assigns
scores to transformation results, which in turn affect the perceived authoritativeness of the tables providing them. Initially, the model takes into account the prevalence of the
number of correct examples in a table. Furthermore, the
model takes into account how authoritative a table is, based
on its initial score. The initial score of a table changes based
on user-feedback. Over time, we capture whether users accept or reject transformation results and use their evaluation
to enhance or decrease initial scores of the corresponding tables. Also it is possible to manually add authoritative tables
to the repository. More details on the reconciliation process
of DataXFormer and how it converges can be found in our
previous paper [1].

Table 3
city	
  

Iteration 2

2.2

Figure 3: Iterations for transformation discovery for τ = 2

Web forms subsystem

A Web form allows user-interaction through different
fields and buttons. The input values given by a user
are either processed directly on the client side through
JavaScript or are sent as an HTTP request (GET or POST)
to a back-end server for further processing. Figure 4a illustrates a Web form for discovering location attributes of a
given airport code. Putting ORD into the search field on
Web page (a) retrieves the result Web page (b) that contains the corresponding city name Chicago.
As with Web tables, we assume a Web form is relevant if
it covers at least τ of the example transformations. There
are two main challenges in using Web forms: (1) as there
is no common repository of Web forms, we have to look for
them through crawling billions of Web pages; and (2) a new
Web form appears as a black box. This means that an invocation strategy (i.e., wrapper) has to be developed to use
the form for producing the desired transformations. It has
been shown [4] that both tasks are very hard, even with human intervention. DataXFormer tackles both challenges using search engines and a Web browser simulator that allows
to analyze the HTML code of a Web form. Additionally, the
user is involved whenever any of the tasks fail.
Web form retrieval DataXFormer maintains a local repository of previously used Web forms. The repository is organized as a document store where each document represents
one wrapper. Using the document store is suitable for this
task because we also store and index the textual content
of Websites that contain the specific Web form. If DataXFormer fails to find a relevant form in the repository, it uses a
search engine to find forms online. In our preliminary experimental results [1], we noticed that the keyword query “IX
to IY ” has a high success rate for finding a page containing a transformation form in its top results. By examining
the search engine results, DataXFormer identifies candidate
forms. Then, for each candidate form, we generate a “wrapper” to be able to query the form and to obtain the transformation values. In case DataXFormer fails to generate a
wrapper or to find a relevant Web form, the user is asked
for help. The wrapped forms are invoked using the input
examples and are evaluated based on their coverage in the
evaluation step. Candidate Web forms are then queried using the examples from the user query.
Wrapping Web forms To wrap Web forms, we have to
simulate a Web browser and probe the forms using the given
examples to identify the relevant input field to fill in the X
values, the output field that contains the desired transformation result Y , and the request method for invoking the

Iterative matching The direct matching approach is fast
and simple. However, the transformation entries may sometimes be fragmented over several tables (Figure 3). Most
Web tables were designed for human consumption, and thus,
large tables are usually fragmented over separate tables on
different Web pages in a given Web site. Since the input
examples are very few, only a small fraction of all the tables
that encode the transformation might match the transformation query. In Figure 3, our query examples only match
tables that focus on airports in the USA, and cannot find
the transformation for IST and HBE in those tables.
To increase the recall, the iterative matching looks for
indirect matches by using newly found results as examples
for the next iteration. As illustrated in Figure 3, the new
iteration uses the transformation results for FRA and DFW
in Table 2 as new examples, which enables DataXFormer to
reach Table 3 and find the transformations for HBE and
IST. Iterative matching increases the recall at the cost of a
longer execution time. There might also be a reduction in
precision as non-evaluated transformation results are used
as examples. We solve this issue in the reconciliation phase
where different results are ranked based on their redundancy
and the authoritativeness of their sources.
Fuzzy matching To further increase recall, the user might
decide to relax the matching of values, accounting for typos
and slight changes in representation. Our basic approach
already tolerates some fuzziness by considering tokenization
and stemming of transformation values. However, variations
such as typos or abbreviations of a term cannot be captured
by stemming and tokenization. For example, “Washington
DC” and “Washington D.C.” can only be matched by tolerating a certain degree of edit distance, since most tokenization techniques would assume that “DC” is a token by itself,
while “D.C.” consists of two tokens. We apply the popular Levenshtein similarity measure to capture some of these
fuzzy matches. Fuzzy matching takes a step further towards
increasing recall, at the expense of the execution time.
Reconciling different transformation results Our
transformation results may stem from multiple different tables. Hence, for some X values, multiple Y may be retrieved.
To reconcile these contradicting results, DataXFormer ap-

885

(a)

Website	
  content	
  

GET	
  request	
  URL:	
  

h6p://www.world-­‐airport-­‐
codes.com/search/?s=ORD	
  

Wrapper	
  
components	
  

Tokenized	
  and	
  
indexed	
  

Field	
  

Values	
  

URL	
  

h&p://www.world-­‐airport-­‐codes.com/	
  

Form	
  URL	
  

h&p://www.world-­‐airport-­‐codes.com/search/	
  

Form	
  type	
  

GET	
  

Input	
  Param	
   s	
  
Output	
  ﬁeld	
   /html/body/div/div/arEcle/table/tbody/…	
  

(b)

Input	
  parameter:	
  	
  
?s	
  

OpEons	
  
Input	
  label	
  

Air	
  port	
  code	
  

Output	
  label	
   City	
  

Output	
  ﬁeld:	
  

/html/body/div/div/arJcle/
/html/body/div[1]/div/
table/tbody/tr/	
  
arJcle/table/tbody/tr/td[4]	
  

	
  

(a) Web form components that need to be stored

Website	
  
content	
  

World	
  Airport	
  Codes	
  place	
  ﬁnd	
  47,000	
  airport	
  
codes	
  abbreviaEons	
  runway	
  lengths	
  airport	
  
informaEon……	
  

X	
  values	
  

ORD	
  LAX	
  SFX	
  SHA	
  JFK	
  FRA	
  …	
  

Y	
  values	
  

Chicago	
  Berlin	
  Shanghai	
  Los	
  Angeles	
  Frankfurt	
  
…	
  

(b) Web form wrapper as a document in the
repository

Figure 4: Web form wrapper components

form. Current wrapper generation approaches [4, 8] invoke
the form using all possible combinations of input fields, select fields, and radio buttons, and use the results to identify
the semantics of the various components of the form. In the
current prototype, we apply some heuristics to reduce the
number of combinations; for example, we only consider text
and text area fields as input fields for our X values, and we
choose radio button options that resemble our attribute labels or denote the transformation direction. In Figure 4a, we
highlight the visible relevant components such as the Website content, the GET request URL, the input parameter s,
and the XPath for the output value.
Once the XPath to our given example is discovered on the
output result page, DataXFormer invokes the wrapped form
using the remaining input examples and evaluates its coverage in the evaluation step. If the user is satisfied with the
Web form results, she can trigger the Augment component
to store the Web form results as a new table in the table
repository. Finally, DataXFormer stores all components of a
successfully wrapped Web form as illustrated in Figure 4b.
Some of the stored fields (framed red in Figure 4b) contain
meta-data that is indexed to effectively retrieve the wrapper for a given transformation query. For example, Website
content and example X values are tokenized and indexed.
Other fields that are framed in blue in Figure 4b store the
configuration parameters of the wrapper, such as the Form
URL and Input Param.

2.3

further cleaned the dataset by removing cells that contained
comments or other long text content which does not serve
our purpose. The disk usage of the Vertica storage comprises
about 350 GB on 4 nodes.
The Web forms are stored as documents, where each document contains the URI of a Web form and all its relevant
fields, to allow reconstructing the generated wrapper of the
Web form. Furthermore, we store some meta-data, such as
the textual content of the Website that provides the Web
form, as well as a limited set of previously performed example transformations. Some of the wrappers refer to the same
Web form with a different configuration, e.g., both directions
for km to/from miles.

3.

3.1

Datasets and Implementation Setup

2

Using DataXFormer

A user can submit a query either by uploading a CSV file
containing the two columns X and Y or by filling in the
examples through a simple table form (see Figure 5).
Then, the user can start the transformation engine, which
will initiate the selected transformation discovery algorithm
(direct, indirect, or fuzzy matching) over the Web tables
and the Web form retrieval. DataXFormer dispatches both
subsystems in parallel. Figure 6 presents the retrieved indirect match results for the query. The selected values in the

DataXFormer uses a Vertica column-store to store the Web
tables and a Lucene index that indexes successfully wrapped
Web forms. Initially, the Web tables corpus was filled with
the Dresden Web Table Corpus 1 , a collection of about 112
million data tables extracted from the Common Crawl 2 . We
1

DEMONSTRATION

An initial version of DataXFormer, which is already at
http://dataxformer.org, answers transformation queries
through direct and iterative matching on Web tables and
through indexed Web forms.
The demonstration’s audience can come up with its own
transformation queries for DataXFormer. For example a user
might be interested in discovering transformations of her
favourite athlete to associated club. Additionally, we will
provide and describe some prepared transformation queries
from our previous benchmark [1], such as airport code to
city, company ticker to company name, and EUR to USD.

https://wwwdb.inf.tu-dresden.de/misc/dwtc/
http://commoncrawl.org/

886

Figure 6: Tranformation results of the iterative matching
approach

Figure 5: Our transformation query aims at transforming
airport code to the city location. Here, we provided the
corresponding cities for three of the airport codes.
Figure 7: DataXFormer found a Webform that can find the
desired cities.
city column are the transformation results with the highest
scores, which are displayed in brackets. At the same time,
DataXFormer also presents the results that were produced
by a Web form (Figure 7).
If the user is not satisfied with the results, she might
use the drop down boxes to choose from alternative transformations, if available (see Figure 8). Note that the results are ranked according to their confidence score assigned
by DataXFormer’s expectation-maximization approach [1].
Upon choosing an option, pop-up widgets provide further
background information, such as the number of tables that
agreed on this specific transformation. Furthermore, the
user can display the actual content of a table to understand
the provenance of a transformation result (see Figure 9).
Alternatively, the user can resend the transformation
query with a subset of successfully performed transformations. Depending on the initial matching algorithm, the
user can also choose to enhance the transformation results
by changing the matching algorithm on the Web tables.
If the user is satisfied, she can save the complete result
set or a subset of the results by checking the corresponding
check boxes and add it to the corpus. A table that is stored
by a user has a higher initial score. We will demonstrate
the evolution of the corpus by resubmitting the initial query
and showing that the results and scores have been changed.

3.2

out having any background in Web programming. DataXFormer analyzes a given website, extracts its HTML elements, and simulates its Web forms within DataXFormer’s
own framework. In Figure 10 DataXFormer identifies two
Web forms on http://world-airport-codes.com, and renders an interactive wrapper induction interface. Note, that
DataXFormer identifies human-readable descriptions. Popups will show the corresponding original HTML code. The
user can then continue to configure the selected form, by
choosing the correct input field and form options. DataXFormer captures the user’s interaction with the simulated
form and uses the selected parameters to wrap the original
Web form for the transformation task.

4.

RELATED WORK

The closest tool to DataXFormer is Google Spreadsheets 3 .
The autofill feature uses machine learning to predict the
transformation values of a column given a few examples.
In contrast, DataXFormer does not try to explicitly define
the transformation, but rather finds the necessary look-up
tables in a corpus of Web tables and Web forms. Since
Google Spreasheets tries to interpolate the transformation
solely from the input, it cannot provide semantic transformations, such as “airport code to city”, while DataXFormer
can since it uses external knowledge.
InfoGather [10] uses Web tables for attribute discovery
and entity augmentation. While the two approaches are
quite similar, InfoGather relies on pre-computing the full
semantic-match graph for the whole corpus, while DataX-

Tracing Web form discovery and userbased wrapper induction

Our demonstration also includes a walkthrough of the
Web form subsystem by showing the intermediate steps and
results. We will show how DataXFormer leverages the search
engine, and which aspects of a Website matter for the automatic wrapper induction. Furthermore, we show a scenario
where the user can intervene in the wrapper induction with-

3

887

http://spreadsheets.google.com

Figure 8: The user can choose between different transformation results, which are ranked according to their confidence
score

X
<h1> Search for an airport </h1>
<p> Welcome to World Airport Codes, th
<form role = “search” class = “search s
<input type = “text” value = “” name = “s

Figure 10: DataXFormer identifies the forms on a web page
and presents them as radio options.

[2]

[3]

[4]
Figure 9: DataXFormer shows the provenance of a transformation result. Note that some tables do not have column
headers.

[5]

[6]
Former finds the required tables on the fly, and leverages
Web forms as well.
Wrangler [7] provides an easy interface to specify scripts
of data transformation. It supports the concept of semantic
roles as well, providing additional functions to be included in
the scripts. DataXFormer requires minimal user interaction,
as it locates Web tables and forms automatically. Similarly
to Google Spreadsheets, Wrangler cannot provide transformations that rely on external knowledge.
Many proposed systems focus on answering keyword
queries over relational tables by efficiently generating candidate networks of joined tuples to form answers, with search
terms from the query spanning multiple records across multiple tables [2, 3, 6]. DataXFormer differs from these systems
in that it tries to find tables that provide a transformation
given a few examples for the transformation, rather than
building candidate tuple networks.

5.

[7]

[8]

[9]

[10]

REFERENCES

[1] Z. Abedjan, J. Morcos, M. Gubanov, I. Ilyas,
M. Stonebraker, P. Papotti, and M. Ouzzani.

888

Dataxformer: Leveraging the web for semantic data
transformations. In CIDR, 2015.
B. Aditya, G. Bhalotia, S. Chakrabarti, A. Hulgeri,
C. Nakhe, P. Parag, and S. Sudarshan. Banks:
Browsing and keyword searching in relational
databases. In VLDB, pages 1083–1086, 2002.
S. Agrawal, S. Chaudhuri, and G. Das. Dbxplorer: A
system for keyword-based search over relational
databases. In ICDE, pages 5–16, 2002.
L. Barbosa and J. Freire. An adaptive crawler for
locating hidden-web entry points. In WWW, pages
441–450, New York, NY, USA, 2007.
A. P. Dawid and A. M. Skene. Maximum likelihood
estimation of observer error-rates using the em
algorithm. Applied statistics, pages 20–28, 1979.
V. Hristidis and Y. Papakonstantinou. Discover:
Keyword search in relational databases. In VLDB,
pages 670–681, 2002.
S. Kandel, A. Paepcke, J. Hellerstein, and J. Heer.
Wrangler: Interactive visual specification of data
transformation scripts. In CHI, pages 3363–3372, New
York, NY, USA, 2011.
J. Madhavan, D. Ko, L. Kot, V. Ganapathy,
A. Rasmussen, and A. Halevy. Google’s deep web
crawl. PVLDB, 1(2):1241–1252, Aug. 2008.
M. Stonebraker, D. Bruckner, I. F. Ilyas, G. Beskales,
M. Cherniack, S. B. Zdonik, A. Pagan, and S. Xu.
Data curation at scale: The data tamer system. In
CIDR, 2013.
M. Yakout, K. Ganjam, K. Chakrabarti, and
S. Chaudhuri. Infogather: Entity augmentation and
attribute discovery by holistic matching with web
tables. In SIGMOD, pages 97–108, New York, NY,
USA, 2012.

Supporting the Automatic Construction of Entity Aware
Search Engines
Lorenzo Blanco, Valter Crescenzi, Paolo Merialdo, Paolo Papotti
Dipartimento di Informatica e Automazione
Università degli Studi Roma Tre - Italy

[blanco,crescenz,merialdo,papotti]@dia.uniroma3.it
ABSTRACT

Within each site, pages containing the same intensional information, i.e. instances of the same conceptual entity, offer the same
type of information, which is organized according to a common
template. In addition, the access paths (e.g. from the home page)
to these pages obey to a common pattern. Again from our basketball example: in a given web site, the pages of two distinct players
contains data—such as name, date of birth, and so on—that are organized according to the same page template. Also, these pages
can be reached following similar navigation paths from the home
page.
Although it is easy for a human reader to recognize these instances, as well as the access paths to the corresponding pages,
current search engines are unaware of them. Technologies for the
Semantic web aim at overcoming these limitations; however, so far
they have been of little help in this respect, as semantic publishing
is very limited.
To overcome this issue, search engine companies are providing
facilities to build personal search engines that can be specialized
over specific domains. A prominent example is Google Co-op, a
Google facility that allows users to indicate sets of pages to be included in the personal search engine, and to assign a label (facet in
the Google terminology) to them. Labels aim at providing a semantic meaning to the page contents, and are used to enhance the search
engine querying system. For data rich pages, labels typically represent a name for underlying conceptual entity. For example, a user
interested in building a personal search engine about the basketball
world can provide the system with web pages containing data about
players, such as those in Figure 1, and then she can associate them
with the label BASKETBALL P LAYER to indicate that they contain
data about instances of the basketball player conceptual entity. An
alternative approach with similar goals is based on mass labeling
facilities, such as del.icio.us or reddit.com, which allow
users to collaboratively annotate pages with labels.
We observe that although these approaches support users in the
definition of search engines that are somehow aware about the presence of instances of a given entity, the issue of gathering the relevant pages must be performed manually by the user.
This paper proposes an original and effective domain independent solution to tackle the issue of the page gathering task. We
believe that our method can help the above facilities scaling, as
it automatically discovers pages containing data that represent instances of a given conceptual entity.
Our method takes as input a small set of sample pages from distinct web sites: it only requires that the sample pages contain data
about an instance of the conceptual entity of interest. Then, leveraging redundancies and structural regularities that locally occur on
the web, our method automatically discovers pages containing data
about other instances of the conceptual entity exemplified by the

Several web sites deliver a large number of pages, each publishing
data about one instance of some real world entity, such as an athlete,
a stock quote, a book. Although it is easy for a human reader to
recognize these instances, current search engines are unaware of
them. Technologies for the Semantic Web aim at achieving this
goal; however, so far they have been of little help in this respect, as
semantic publishing is very limited.
We have developed a method to automatically search on the web
for pages that publish data representing an instance of a certain
conceptual entity. Our method takes as input a small set of sample pages: it automatically infers a description of the underlying
conceptual entity and then searches the web for other pages containing data representing the same entity. We have implemented
our method in a system prototype, which has been used to conduct
several experiments that have produced interesting results.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search process

General Terms
Algorithms

1.

INTRODUCTION

There is an increasing number of web sites that deliver “data rich”
pages, where the published information is organized according to
an implicit schema. These pages usually contain high quality data
that represent instances of some conceptual entity. Consider web
sites that publish information about popular sport events, or web
sites that publish financial information: their pages embed data
that describe instances of conceptual entities such as athlete, match,
team, or stock quote, company, and so on. To give a concrete example, observe the web pages in Figure 1. Each of them contains data
describing one instance of the BASKETBALL P LAYER conceptual
entity.
For the sake of scalability of the publishing process, the structure
of pages and navigation paths of these web sites are fairly regular.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
WIDM’08, October 30, 2008, Napa Valley, California, USA.
Copyright 2008 ACM 978-1-60558-260-3/08/10 ...$5.00.

149

Figure 1: Web pages representing instances of the BASKETBALL P LAYER conceptual entity.

Searching Entity Pages within One Site. The first step of

input samples, as follows.

our method is to search the target pages within the web sites of
each sample page. This task is performed by I NDESIT, a crawling
algorithm designed to drive a scan of a given web site toward pages
sharing the same structure of an input seed page [5].
I NDESIT relies on the observation that, within a large web site,
pages offering a description of the same conceptual entity (e.g.,
BASKETBALL P LAYER) usually share a common template and similar access paths.
I NDESIT efficiently navigates the web site to collect pages containing lists of links toward pages which are structurally similar to
the seed page. Following these links it gathers pages with the type
of information of the seed. With respect to our running example,
the output of I NDESIT is the set of basketball player pages published in the web sites of each sample page.

• it crawls the web sites of the input sample pages to collect
pages with data about other instances of the conceptual entity
of interest;
• from these pages, it automatically extracts a description of
the entity exemplified by the sample pages;
• using the information computed in the previous steps, it launches web searches to discover new pages. The results of
these searches are analyzed using the entity description. Pages
representing valid instances of the target entity are stored,
and are used to recursively trigger the process.
It is important to notice that our technique has a different semantics with respect to the “similar pages” facility offered by search
engines. Given as input two web pages from two different web
sites describing the basketball players “Kobe Bryant” and “Bill
Bradley”, our method aims at retrieving many web pages that are
similar at the intensional level, e.g. pages about other basketball
players, not necessarily the same two sample players.
The rest of the paper is organized as follows. Section 2 provides
a brief overview of our method that, after a discussion on related
work in Section 3, is detailed in Sections 4 and 5. Section 6 illustrates the result of the experiments we have conducted to evaluate
the effectiveness of the approach. Section 7 presents our concluding remarks and future work.

2.

Learning a Description of the Conceptual Entity. As
a second step, our method computes a description for the target
conceptual entity. To this end, we rely on the observation that pages
containing data about instances of the same conceptual entity share
a common set of characterizing keywords that appear in the page
template.
In our approach, the description of a conceptual entity is then
composed by a set of keywords that are extracted from the set of
terms that lay on the templates of the input sample pages. Our
experiments show that these keywords effectively characterize the
overall conceptual domain of the entity with very promising results.
Given a set of structurally similar pages returned by I NDESIT, the
entity description is generated by computing the terms that belong
to the corresponding template. This task is performed by analyzing
the set of terms that occur in the pages and by removing those elements that belong also to the “site template”, i.e. to that portion of
the template that is shared by every pages in the site. In this way,
from each sample page a set of terms is extracted. Terms that are
shared in the templates of different web sites are then selected as
keywords for the entity description.

OVERVIEW

The ultimate goal of our method is to automatically discover web
pages that contain data describing instances of a given conceptual
entity. We assume that the user provides as input a few input sample pages. It is not important that the sample pages contain data
about the same instance; we only require they come from different web sites, and they contain data that represent instances of the
same conceptual entity. Pages such as those in Figure 1 could be
used as input to collect pages with data about instances of the BAS KETBALL P LAYER conceptual entity.

150

Triggering new Searches on the Web. The results produced
by the initial I NDESIT executions and the keywords in the entity
description are used to propagate the search on the web. This step
is done by the O UTDESIT algorithm, which issues a set of queries
against a search engine and elaborates the results in order to select
only those pages that can be considered as instances of the target
entity. Then, the selected pages are used as seeds to trigger again an
I NDESIT scan, and the whole process is repeated until new pages
are found.
To correctly expand the search on the web, we need to address
two main issues. First, we have to feed the search engine with keywords that are likely to produce new pages representing instances
of the input entity. Second, as these pages will be used to run a
new instance of I NDESIT, we have to filter them in order to choose
those that really correspond to instances of the conceptual entity of
interest.
To generate the keywords to be submitted to the search engine
we adopt a simple yet effective solution. As we are searching for
instances of a given entity, we need values that work as identifiers
for the instances of the entity. We observe that, since pages are
designed for human consumption, the anchors associated with the
links to our instance pages usually satisfy these properties: they
are expressive, and they univocally identify the instance described
in the target page. In our example, the anchor to a player page
usually corresponds to the name of the athlete. Therefore, we issue
a number of queries against a search engine, where each query is
composed by the anchor of a link to one of the pages retrieved by
the previous I NDESIT execution. Also, to focus the search engine
toward the right domain, each query is completed with keywords
from the entity description.
As search results typically include pages that are not suitable
for our purposes, we filter the off-topic pages by requiring that the
keywords of the entity description are contained in their templates.

information, namely, pages containing data that represent instances
of the conceptual entity exemplified by means of an input set of
sample pages.
Vidal et al. present a system, called G O G ET I T ! that takes as
input a sample page and an entry point to a web site and generates
a sequence of URL patterns for the links a crawler has to follow to
reach pages that are structurally similar to the input sample [22],
therefore their approach is limited to address the issue tackled by
our I NDESIT crawler.
The problem of retrieving documents that are “relevant” to a
user’s information need is the main objective of the information
retrieval field [18]. Although our problem is different in nature, in
our method we exploit state-of-the-art keyword extraction and term
weighting results from IR [18].
There are several recent research projects that address issues related to ours. The goal of C IMPLE is to develop a platform to support the information needs of the members of a virtual community [13]. Compared to our method, C IMPLE requires an expert to
provide a set of relevant sources and to design an entity relationship
model describing the domain of interest. The MetaQuerier developed by Chang et al. has similar objectives to our proposal, as it
aims at supporting exploration and integration of databases on the
web [10]. However it concentrates on the deep-web.
A new data integration architecture for web data is the subject of
the PAY G O project [17]; the project focuses on the heterogeneity
of structured data on the web: it concentrates on explicit structured sources, such as Google Base and the schema annotations of
Google Co-op, while our approach aims at finding data rich pages
containing information of interest. Somehow, our approach can be
seen as a service for populating the data sources over which PAY G O
works.
Cafarella et al. are developing a system to populate a probabilistic database with data extracted from the web [8]. Data extraction
is performed by T EXT RUNNER [3], an information extraction system which is not suitable for working on data rich web pages that
are the target of our searches.
Other related projects are TAP and S EM TAG by Guha et al. [14,
12]. TAP involves knowledge extracted from structured web pages
and encoded as entities, attributes, and relations. S EM TAG provides
a semantic search capability driven by the TAP knowledge base.
Contrarily to our approach, TAP requires hand-crafted rules for
each site that it crawls, and when the formats of those sites change,
the rules need to be updated.

The three steps described above are repeated to collect new relevant pages: the results that are selected from each search are used as
I NDESIT seeds to gather further pages and to trigger new searches.

3.

RELATED WORK

Our method is inspired to the pioneering DIPRE technique developed by Brin [7]. With respect to DIPRE, which infers patterns
that occur locally within single web pages to encode tuples, we
infer global access patterns offered by large web sites containing
pages of interest. DIPRE also inspired several web information
extraction techniques [1, 3]. Compared to our approach these appraches are not able to exploit the information offered by data rich
pages. In fact, they concentrate on the extraction of facts: large
collections of named-entities (such as, for example, names of scientists, politicians, cities), or simple binary predicates, e.g. bornin(politician, city). Moreover, they are effective with facts that appear in well-phrased sentences, whereas they fail to elaborate data
that are implied by web page layout or mark-up practices, such as
those typically published in web sites containing data rich pages.
Our work is also related to researches on focused crawlers (or
topical crawlers) [9, 20, 19], which face the issue of efficiently
fetching web pages that are relevant to a specific topic. Focused
crawlers typically rely on text classifiers to determine the relevance
of the visited pages to the target topic. Page relevance and contextual information—such as, the contents around the link, the lexical
content of ancestor pages—are used to estimate the benefit of following URLs contained in the most of relevant pages. Although
focused crawlers present some analogy with our work, our goal is
different as we aim at retrieving pages that publish the same type of

4.

SEARCHING PAGES BY STRUCTURE:
INDESIT

Given a seed page p0 containing data of interest, the goal of the
I NDESIT algorithm [5] is to pick out from its site the largest number of pages similar in structure to p0 and the anchors pointing to
such pages. The underlying idea of I NDESIT is that while crawling,
it is possible to acquire knowledge about the navigational paths the
site provides and to give higher priority to the most promising and
efficient paths, i.e. those leading to a large number of pages structurally similar to the seed.
I NDESIT relies on a simple model that abstracts the structure of a
web page. The model adopted by I NDESIT to abstract the structure
of a web page is based on the following observations: (i) pages
from large web sites usually contain a large number of links, and
(ii) the set of layout and presentation properties associated with the
links of a page can provide hints about the structure of the page
itself. Therefore, whenever a large majority of the links of two
pages share the same layout and presentation properties, then it is

151

likely that the two pages share the same structure. Based on these
observations, in I NDESIT the structure of a web page is described
by means of the presentation and layout properties of the links that
it offers, and the structural similarity between pages is measured
with respect to these features.
The page model is used by a crawling algorithm that explores
a given Web site toward pages sharing the same structure of an
input seed page. The crawler navigates the Web site searching for
pages that contain lists of links leading to pages that are structurally
similar to the seed page. Since these lists of links work like indexes
to the searched pages, the crawler rely on them to reach the target
set of pages.
The experimental results of our evaluation are reported in Figure 2 and summarize the experiments in [5]. We report the average
recall (R), the average precision (P ), and the average number of
downloaded pages (#dwnl) over 37 I NDESIT executions.
R
95.31%

P
96.56%

Algorithm O UTDESIT
Parameter: N number of iterations
Input: a set of sample pages S = {p0 , . . . , pk }
containing data about instances of the same conceptual entity
Output: a set of pages about the input conceptual entity;
1. begin
Let R be a set of result pages;
Let R = I NDESIT(p);
// apply I NDESIT to all input pages in S and
5.
// insert the resulting pages into R
Let σE = {t1 , . . . , tn } be the entity intensional description
computed from R;
Let kE be the domain entity description computed from R;
for (i=0; i<N, 1++) do begin
10.
Let A be the set of new anchors leading to
the pages returned by the last I NDESIT invocations;
for all terms a ∈ A do begin
Let W be the set of pages returned by a search
engine when looking for a ∧ (t1 ∨ . . . ∨ tn ) ∧ kE ;
15.
for all pages p ∈ W do begin
if the domain of p has been already visited continue
if (isInstance(p, σE )) begin
add I NDESIT(p) to R
end
20.
end
end
end
end

#dwnl
3,389.22

Figure 2: I NDESIT experimental results.

5.

SEARCHING ENTITIES ON THE WEB:
OUTDESIT

I NDESIT searches for entity pages within the same site of the input samples. We now describe how the search of entity pages can
be extended on the web. The overall idea is to use the results obtained by a first run of I NDESIT on the sample pages in order to issue a number of queries against a search engine, such as Google or
Yahoo!, with the objective of finding new sources offering other instances of the same entity. This task is performed by the O UTDESIT
algorithm, which is described in Figure 3.
As we are interested in finding instances of the target entity, we
need to search the web by means of keywords that works as instance identifiers. Our approach is to extract these identifiers from
the results of the previous I NDESIT executions. Namely, we use
the anchors of links pointing to the pages collected by I NDESIT as
keywords (lines 10–11 in Figure 3). The rationale is that as web
pages are produced for human consumption, the anchors of links
pointing to entity pages are likely to be values that univocally identify the target instance. E.g., in our basketball players scenario, the
anchor of the links to each player page is the name of the player.
Observe that, for the sake of usability, this feature has a general validity on the web. For example, the anchor to a book page usually
is the title of the book; the anchor to a stock quote is its name (or a
representative symbol), etc..
We leverage this property to run searches on the web (lines 9–
22). O UTDESIT launches one search for each new anchor found in
the previous I NDESIT execution. To better focus the search engine,
each query is composed by an anchor plus a set of keywords, that
we call the entity description. Observe (line 14) that the query is
composed by a conjunction of three terms: (i) an anchor; (ii) a
domain keyword kE , which characterizes the overall conceptual
domain; (iii) a disjunction of the keywords terms t1 , . . . , tn , which
describe the conceptual entity. All these keywords are extracted
automatically from the sample pages, as described in the following
of this section.
Each search produces a number of result pages,1 which are analyzed with the isInstance function to check whether they repre-

sent instances of the target entity (line 17). For each page that is
classified as an entity page, a new instance of I NDESIT is run (line
18), and the whole process is iterated until new pages are found.
A fundamental issue in each iteration is to check whether a page
returned by the search engine can be considered as an instance of
the target conceptual entity. The search engine can in fact return
pages that, though containing the required keywords, are not suitable for our purposes. Typical examples are pages from forums,
blog, or news where the keywords occurs by chance, or because
they are in a free text description. To control this aspect O UTDESIT
requires that the keywords of the entity description appear in the
template of the retrieved page.
Then, for each page returned by the search engine, an instance
of I NDESIT is run to obtain a set of structurally similar pages,2 and
their template is computed. If the computed template contains the
keywords of the entity description, the page is considered valid;
otherwise it is discarded.
Valid pages are finally used as seeds for new I NDESIT scans,
thus contributing to further discover new pages in the iterative step
performed by O UTDESIT.

1
For each search, we take the first 30 result pages returned by the
search engine.

2
In this step, we run a ”light” version of I NDESIT, which quickly
returns a small set of pages.

Function isInstance
Parameter: t template similarity threshold
Input: a page p,
an intensional description σE of the conceptual entity
Output: true iff p is a page about the searched conceptual entity
begin
Let I = I NDESIT(p);
if |I| = 1
return false
Let T be the set of tokens in the template of I;
Let D be the set of English terms in T ;
|σE ∩D|
return true iff |σ
> t;
E|
end

Figure 3: The O UTDESIT algorithm.

152

5.1

Learning the Entity Description

chance) with some other token appearing in some page; for example with an instantiated value embedded in the template. However,
observe that if the tokens that occur once in all the pages can be
considered template’s elements, it is reasonable that they indicate
delimiters of homogeneous page segments, i.e. segments generated
by the same piece of the underlying template. Then it is possible to
inspect each segment, in order to further discover new template tokens. Occurrences of tokens that are not unique on the original set
of pages could become unique within the more focused context of
a segment. To illustrate this point, let us continue with the previous
example: observe that the token Height, which is likely to belong
to the page template, cannot be included in the computed set, because it occurs twice in the second page (it appears in the profile
of the player described in that page). But consider the segments
of pages delimited by the tokens detected in the previous step: the
token Height occurs once in the second segment of every page,
which delimited by the tokens Weight and <TABLE>.

The description of an entity E, is composed by an intensional description and by a domain keyword. The intensional description,
denoted σE , consists of a set of terms σE = {t1 , t2 , . . . , tn } and is
extracted from the sample pages by analyzing the terms that occur
in their templates. The domain keyword, denoted kE , characterizes general features of the entity and is generated by adapting in
our context standard keyword extraction techniques.

Extraction of the Intensional Description. Our approach
for generating the set of keywords to be associated with the conceptual entity is based on the observation that pages from large
web sites are built over a template that usually contains labels describing the semantics of the data presented in the pages. Consider
again the three basketball player pages in Figure 1 and observe labels such as weight, height, position, college: they are used by the
page designers to provide a meaning to the published data.
Our method for extracting a characterizing description of the entity is based on the assumption that instances of the same conceptual entity have data that refer to a core set of common attributes,
even these from different sources. For example, it is likely that
most of the instances of the BASKETBALL P LAYER conceptual entity present fields to describe height, weight and college data. This
is a strong yet realistic assumption; in their studies on web scale
data integration issues, Madhavan et al. observe that in the huge
repository of Google Base, a recent offering from Google that allows users to upload structured data into Google, “there is a core
set of attributes that appear in a large number of items” [17].3
Also, in web pages, these data are usually accompanied by explicative labels, and then they belong to the page template. For example,
in the three sample pages shown in Figure 1 (it is worth saying that
these pages have been randomly chosen from the web) there are
several labels that are present in all three pages. Our method aims
at catching these labels to characterize the description of the target
entity. To this end, we first compute terms that do belong to the
page templates of the sample pages. Then, we choose, as characterizing keywords, those that appear in all the templates.
To illustrate our solution for extracting terms from the page template it is convenient to consider a web page as a sequence of tokens, where each token is either a HTML tag or a term (typically an
English word). Each token t is associated a path, denoted path(t),
which corresponds to the associated path in the DOM tree. Two
tokens are equal if they have the same path. In the following, for
the sake of readability, we may blur the distinction between token
and path associated with the token, assuming that different tokens
have different paths.
To detect tokens from the template of a given page we have
adapted in our context a technique proposed by Arasu and GarciaMolina [2]. They observe that given a set of pages P generated
by the same template, sets of tokens having the same path and frequency of occurrence in every page in P are likely to belong to the
page template.
Let us introduce an example to show how we use these sets to infer a conceptual entity description. Figure 4 shows the sequence of
tokens corresponding to three pages in Figure 1. The set of tokens
whose paths occur exactly once is given by: Weight, Profile,
<TR>, <TABLE>, <B>. It is reasonable to assume that they belongs to the template that originated the three pages.
The above condition allows us to discover template elements, but
it might not hold if a token belonging to the template coincides (by

Algorithm T EMPLATE T OKENS
Input: a set of token sequences S = {s1 , . . . , sn }
Output: a set of tokens
begin
Let T be an empty set of tokens;
Let E0 = {e1 , . . . , ek } be the list of tokens
that occur exactly once in every element of S;
for each token ei ∈ E0 do begin
Let S i = {si1 , . . . , sin } be a set of sequences such
that sij = subSequence(sj , E, ei ) ∀j = 1, . . . , n;
add T emplateT okens(S i ) to T ;
end
return T ;
end
Function subSequence(s, E, ei )
Input: s a sequence of tokens s = t0 · . . . · tn
E a list of tokens e0 , . . . , ek , ei ∈ s ∀i = 1, . . . , k
ei a token, e ∈ E
Output: a subsequence of s
begin
Let i be the index of ei in s;
if (i==0) begin
start = 0;
end = index − 1;
end
if (i==k) begin
start = i + 1;
end = n;
end
else begin
start = i + 1;
Let end be the index of ei+1 in s;
end = end − 1;
end
return tstart · . . . · tend ;
end

Figure 5: The T EMPLATE T OKENS algorithm to detect tokens
belonging to the template of a set of pages.
Given a set of pages, the set of tokens that are likely to belong to
the template are computed using the T EMPLATE T OKENS algorithm
in Figure 5. The algorithm extracts tokens occurring once and uses
them to segment the input pages. Segments are then recursively
processed to discover other template tokens. The English terms
contained in the set of tokens returned by T EMPLATE T OKENS are

3
In the Google Base terminology, an item corresponds to a set of
attribute-value pairs.

153

Figure 4: Pages as sequences of tokens.
likely to belong to the template of the input page. However some
of them could be originated also by that portion of the template
that is usually shared by every page in a site (comprehending page
portions such as headers, footers, navigational bars, and so on). To
eliminate these terms, we apply the T EMPLATE T OKENS algorithm
over a broader set of pages, which includes the home page of the
sample page site. The terms returned by this execution are then subtracted from the set of terms found in the template of the instance
pages. This procedure is performed for each sample page. Finally,
in order to obtain the core of terms that is shared by instance pages
from different sources, we compute the intersection among the sets
of terms computed from each sample.4 We report in Figure 6 some
examples of the entity description generated using our tool.
DOMAIN
BASKETBALL
GOLF
HOCKEY
SOCCER

We have focused our experiments on the sport domain. The
motivation of our choice is that it is easy to interpret the published information, and then to evaluate the precision of the results produced by our method. The goal of our experiments was to
search for a set of pages, each one containing data about one athlete (player) of a given sportive discipline. We have concentrated
on four disciplines: basketball, soccer, hockey, and golf. Therefore, we may say that our experiments aimed at discovering pages
publishing data about instances of the following conceptual entities: BASKETBALL P LAYER, S OCCER P LAYER, H OCKEY P LAYER,
and G OLF P LAYER.
For each discipline we have taken three sample pages, from three
different web sites, each one publishing data about one player of
that discipline. Then, for each sample set we have run O UTDESIT.
In the following we presents the results of this activity.

attributes
pts, height, weight, min, ast
college, events, height, season, weight
born, height, log, round, shoots, weight
club, height, nationality, weight

6.1

Extracted Intensional Descriptions. The results of the entity descriptions generation are reported in Figure 6. A first observation is that all the terms may actually represent reasonable
attribute names for the corresponding player entity. Also, we notice that there is a core set of terms which is shared by athletes
from different disciplines (namely, height and weight). Since our
experiments involve a taxonomy of the athlete category, it is reasonable that athletes of different sports are described by a core set
of attributes.

Figure 6: Generated descriptions for four conceptual entities.

Domain Keyword Extraction. Our approach for extracting
a keyword characterizing the conceptual domain of the entity represented by the sample pages is rather standard. We compute the
intersection among the terms that appear in all the sample pages
and in the home pages of their sites. The goal is to extract the keywords that most frequently occur in the web sites of the samples.
The resulting set of terms are then weighted with the standard TFIDF scheme [18]. In particular, we consider the term frequency of
each term t as the occurrences of the term in the whole set of pages
including the samples and the home pages of their sites. To compute the IDF factor, we consider the estimated occurrence of the t
on the web, as reported in the web Term Document Frequency and
Rank service of the UC Berkeley Digital Library Project. The term
with the highest weight is then associated to the entity description.
In our example, the term “basketball” is associated to the BASKETBALL P LAYER conceptual entity.

6.

Conceptual Entity Description

Extracted Domain Keywords. Figure 7 presents the keywords
extracted from each set of sample pages.5 Observe that the keywords with the greatest weight correctly characterize the domain
(they actually correspond to the sport discipline). The domain keyword plays a fundamental role in the O UTDESIT iterations. First,
as it is used to generate a more constrained query for the search
engine, it allows the system to elaborate a smaller (and more pertinent) set of pages. Second, in case of homonymous athletes involved in different disciplines, the presence of the domain keyword
in the query can constrain the search towards the right discipline.

Using Entity Descriptions. We have manually analyzed the

EXPERIMENTS

behavior of the isInstance() function, which uses the entity description to check whether a given page is valid for our purposes.
We have run a single iteration of O UTDESIT with a set of anchors

We have developed a prototype that implements O UTDESIT and
we have used it to perform some experiments to validate our techniques.
4
The resulting set is also polished by removing terms that do not
correspond to English nouns.

5
We only show terms for which the TF-IDF weight is at least 30%
of the maximum.

154

DOMAIN
TF
IDF
BASKETBALL
basketball
29.0
5.61
season
27.0
5.08
team
24.0
4.07
players
14.0
5.30
GOLF
golf
64.0
5.29
leaderboard 17.0 10.29
stats
26.0
5.65
players
25.0
5.30
HOCKEY
hockey
22.0
6.30
teams
11.0
5.26
SOCCER
soccer
28.0
5.59
keyword

new instance pages against the number of new web sites discovered
by O UTDESIT. In order to have comparable results, we have run
two iterations for each discipline.
Starting from three sample pages, for each conceptual entity our
method automatically discovered several thousands of pages. By a
manual inspection, conducted on a representative subset of the results, we can conclude that all the retrieved pages can be considered
as instances of the entity exemplified by the input sample pages.
The graphs also plot the number of distinct anchors that are
found in each step. Somehow they can approximate the number
of distinct players. As expected, it is evident that they increase less
than the number of pages.

TF-IDF
162.89
137.39
97.86
74.26
338.63
175.07
147.06
132.62
138.68
57.90

7.

Figure 7: Extracted keywords.
pointing to 500 S OCCER P LAYER pages, selected randomly from
10 soccer web sites. The search engine returned about 15000 pages
distributed over about 4000 distinct web sites. We have then manually evaluated the web sites to measure the precision and the recall
of the isInstance() function over the pages returned by the search
engine. In particular, we studied how precision and recall behave
varying the value for the threshold t in the O UTDESIT algorithm.
As expected, we can see In Figure 8 how raising the threshold the
precision increases and the recall decreases. The system achieves a
100% precision when the number of keywords from the description
required to be in the template of the page under evaluation is at
least 75%. When only 50% of the keywords are required, the pages
marked as valid are 74% of the total valid pages returned by the
search engine, and the precision is still high at 72%.
It is interesting to notice that only 20% of the web pages returned
by the search engine were pages whose data describe instances of
the same conceptual entity exemplified by the sample pages. An
example of non valid pages that frequently occurred in results returned by the search engine are personal pages (blog), news or
forum pages: they are pertinent with the keywords passed to the
search engine, but they are not instances of the conceptual entity
as in our definition. It is worth saying that some of these pages
also contained terms of the intensional description. However, these
terms did not appear in the page template as required by our function, and then these pages were correctly discarded.

8.

REFERENCES

[1] E. Agichtein and L. Gravano. Snowball: extracting relations
from large plain-text collections. In DL,2000.
[2] A. Arasu and H. Garcia-Molina. Extracting structured data
from web pages. In SIGMOD, 2003.
[3] M. Banko, M. Cafarella, S. Soderland, M. Broadhead, and
O. Etzioni. Open information extraction from the web. In
IJCAI, 2007.
[4] L. Barbosa, and J. Freire. An adaptive crawler for locating
hiddenwebentry points. WWW, 2007.

Figure 8: Performance of the isInstance() function varying
the threshold t.

6.2

CONCLUSIONS AND FUTURE WORK

We have presented a method to automatically discover pages publishing data about a certain conceptual entity, given as input only a
small set of sample pages.
The set of pages retrieved by our method can be used to build a
custom, entity aware search engine. As a proof of concept, we are
building entity aware search engines for sport fans.6 To this end
we are populating a Google Co-op search engine with the pages retrieved by O UTDESIT in our experiments. Each page is associated
with an annotation (facet in the Google Co-op terminology) corresponding to the name of the entity exploited by O UTDESIT. Users
can use these annotations to semantically refine the query results by
restricting the search towards pages associated with the annotation.
The results of the experimental activity suggest improvements
that need to be developed, as well as new intriguing research directions. The current method for learning the entity description is a
bit simplistic. In particular, we have observed that computing the
keywords in the bootstrapping phase is too rigid. We are therefore developing a novel method, based on a probabilistic model,
to dynamically compute a weight for the terms of the templates
of the pages that O UTDESIT retrieves. Another issue that need to
be addressed to improve our approach deals with the development
of methods to consider also pages from the hidden web: techniques
such as those proposed for building focused crawlers for the hidden
web could be profitably adapted in our context [15, 4].
An important research direction that we are investigating is the
extension of automatic wrapping techniques (such as those proposed in [11] and [2]) to extract, mine and integrate data from the
sources O UTDESIT retrieves, as we have recently demonstrated [6].
Another interesting study deals with the development of record
linkage techniques for the instances retrieved by our system. We
believe that our method, which progressively discovers new instances from previously achieved results, can provide an interesting
basis for new approaches. Finally, we believe that a challenging issue is to study extensions of our framework in order to take into
account also relationships among different entities.

156.62

Quantitative Evaluation

The number of pages discovered by O UTDESIT for our four target
entities are depicted in Figure 9. Each graph plots the number of

6

155

http://flint.dia.uniroma3.it/

Basketball

Golf

7000

4500
4000

6000

3500
5000
3000
4000

2500
pages

3000

pages

2000

players

players
1500

2000
1000
1000

500

0

0
1

5

9

13

17

21

25

29

33

37

41

45

49

53

57

61

65

69

73

77

81

85

1

3

5

7

9

11

13

15

17

19

sites

21

23

25

27

29

31

33

35

37

39

41

sites

Soccer

Hockey

9000

14000

8000

12000

7000
10000
6000
8000

5000
pages

4000

players

pages

6000

players

3000
4000
2000
2000

1000
0

0
1

6

11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86 91 96 101 106 111 116 121 126 131 136 141 146 151 156 161 166 171

1

5

9

13

17

21

25

sites

29

33

37

41

45

49

53

57

61

65

69

sites

Figure 9: Pages and players found by O UTDESIT.
[5] L. Blanco, V. Crescenzi, and P. Merialdo. Efficiently locating
collections of web pages to wrap. In WEBIST, 2005.
[6] L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti. Flint:
Google-basing the Web. In EDBT (Demo section), 2008.
[7] S. Brin. Extracting patterns and relations from the World
Wide Web. In WebDB, 1998.
[8] M. J. Cafarella, O. Etzioni, and D. Suciu. Structured queries
over web text. IEEE Data Eng. Bull., 29(4):45–51, 2006.
[9] S. Chakrabarti, M. van den Berg, and B. Dom. Focused
crawling: a new approach to topic-specific Web resource
discovery. Computer Networks, 31(11–16):1623–1640, 1999.
[10] K. C.-C. Chang, B. He, and Z. Zhen. Toward large scale
integration: Building a metaquerier over databases on the
web. In CIDR, 2005.
[11] V. Crescenzi, G. Mecca, and P. Merialdo. ROAD RUNNER:
Towards automatic data extraction from large Web sites. In
VLDB, 2001.
[12] S. Dill, N. Eiron, D. Gibson, D. Gruhl, R. Guha, A. Jhingran,
T. Kanungo, S. Rajagopalan, A. Tomkins, J. A. Tomlin, and
J. Y. Zien. Semtag and seeker: bootstrapping the semantic
web via automated semantic annotation. In WWW, 2003.
[13] A. Doan, R. Ramakrishnan, F. Chen, P. DeRose, Y. Lee,
R. McCann, M. Sayyadian, and W. Shen. Community
information management. IEEE Data Eng. Bull.,
29(1):64–72, 2006.

[14] R. Guha and R. McCool. Tap: a semantic web platform.
Computer Networks, 42(5):557–577, August 2003.
[15] B. He, and K. C.-C. Chang. Statistical Schema Matching
across Web Query Interfaces. SIGMOD 2003.
[16] D. Hand, H. Mannila, and S. P. Principles of Data Mining.
MIT Press, 2001.
[17] J. Madhavan, S. Cohen, X. L. Dong, A. Y. Halevy, S. R.
Jeffery, D. Ko, and C. Yu. Web-scale data integration: You
can afford to pay as you go. In CIDR, 2007.
[18] C. D. Manning, P. Raghavan, and H. Schütze. Introduction to
Information Retrieval. Cambridge University Press, 2008.
[19] G. Pant and P. Srinivasan. Learning to crawl: Comparing
classification schemes. ACM Trans. Inf. Syst.,
23(4):430–462, 2005.
[20] S. Sizov, M. Theobald, S. Siersdorfer, G. Weikum,
J. Graupmann, M. Biwer, and P. Zimmer. The bingo! system
for information portal generation and expert web search. In
CIDR, 2003.
[21] C. Van Rijsbergen. Information Retrieval, 2nd edition. Dept.
of Computer Science, University of Glasgow, 1979.
[22] M. L. A. Vidal, A. Soares da Silva, E. Silva de Moura, and
J. M. B. Cavalcanti. Structure-driven crawler generation by
example. SIGIR, 2006.

156

The L LUNATIC Data-Cleaning Framework
Floris Geerts1 Giansalvatore Mecca2 Paolo Papotti3 Donatello Santoro2,4
3

1
University of Antwerp – Antwerp, Belgium 2 Università della Basilicata – Potenza, Italy
Qatar Computing Research Institute (QCRI) – Doha, Qatar 4 Università Roma Tre – Roma, Italy

ABSTRACT

gies is how they use the constraints to modify the dirty data by
changing values into “preferred ” values. Preferred values can be
found from, e.g., master data [24], tuple-certainty and value-accuracy [19], freshness and currency [17], just to name a few.
– Repairing strategies also differ in the kind of repairs that they
compute. Since the computation of all possible repairs is infeasible in practice, conditions are imposed on the computed repairs to
restrict the search space. These conditions include, e.g., various
notions of (cost-based) minimality [7, 8, 10] and certain fixes [18].
Alternatively, sampling techniques are put in place to randomly select repairs [7].
It is thus safe to say that there is already a good arsenal of approaches and techniques for data cleaning at our disposal. In this
paper, we want to capitalize on this wealth of knowledge about the
subject, and investigate the following foundational problem: what
happens to the data administrator facing a complex data-cleaning
problem that requires to bring together several of the techniques
discussed above? This problem is illustrated in the following example.

Data-cleaning (or data-repairing) is considered a crucial problem in
many database-related tasks. It consists in making a database consistent with respect to a set of given constraints. In recent years,
repairing methods have been proposed for several classes of constraints. However, these methods rely on ad hoc decisions and tend
to hard-code the strategy to repair conflicting values. As a consequence, there is currently no general algorithm to solve database
repairing problems that involve different kinds of constraints and
different strategies to select preferred values. In this paper we develop a uniform framework to solve this problem. We propose a
new semantics for repairs, and a chase-based algorithm to compute
minimal solutions. We implemented the framework in a DBMSbased prototype, and we report experimental results that confirm
its good scalability and superior quality in computing repairs.

1.

INTRODUCTION

In the constraint-based approach to data quality, a database is
said to be dirty if it contains inconsistencies with respect to some
set of constraints. The data-cleaning (or data-repairing) process
consists in removing these inconsistencies in order to clean the
database. It represents a crucial activity in many real-life information systems as unclean data often incurs economic loss and erroneous decisions [15].
Data cleaning is a long-standing research issue in the database
community. Focusing on recent years, many interesting proposals
have been put forward, all with the goal of handling the many facets
of the data-cleaning process.
– A plenitude of constraint languages has been devised to capture various aspects of dirty data as inconsistencies of constraints.
These constraint languages range from standard database dependency languages such as functional dependencies and inclusion dependencies [1], to conditional functional dependencies [16] and
conditional inclusion dependencies [15], to matching dependencies
[14] and editing-rules [18], among others. Each of these languages
allows to capture different forms of dirtiness in data.

Example 1: Consider the database shown in Fig. 1 consisting of
customer data (C USTOMERS), with their addresses and credit-card
numbers, and medical treatments paid by insurance plans (T REAT MENTS ). We refer to these two tables as the target database to be
cleaned. As is common in corporate information systems [24], an
additional master-data table is available; this table contains highlycurated records whose values have high accuracy and are assumed
to be clean. In our approach, master data is referred to as the source
database, since it is a source of reliable clean data.
t1
t2
t3

SSN
111
222
222

t4
t5
t6

– Various repairing strategies have been proposed for these constraint languages. One of the distinguishing features of these strate-

tm

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Articles from this volume were invited to present
their results at The 39th International Conference on Very Large Data Bases,
August 26th - 30th 2013, Riva del Garda, Trento, Italy.
Proceedings of the VLDB Endowment, Vol. 6, No. 9
Copyright 2013 VLDB Endowment 2150-8097/13/07... $ 10.00.

NAME
M. White
L. Lennon
L. Lennon
SSN
111
111
222
SSN
222

C USTOMERS
P HONE C ONF
S TR
408-3334 0.8
Red Ave.
122-1876 0.9
NULL
000-0000 0.0
Fry Dr.

T REATMENTS
S ALARY I NSUR .
T REAT
10K
Abx
Dental
25K
Abx
Cholest.
30K
Med
Eye surg.

C ITY
NY
SF
SF

CC#
112321
781658
784659

DATE
10/1/2011
8/12/2012
6/10/2012

M ASTER DATA (Source Table)
NAME
P HONE
S TR
F. Lennon 122-1876 Sky Dr.

C ITY
SF

Figure 1: Customers, Treatments and Master Data.
We first illustrate the problem of specifying a set of constraints
under which the target database is regarded to be clean, as follows:
(a1) Standard functional dependencies (FD): d1 = (SSN, NAME
→ P HONE) and d2 = (SSN, NAME → CC#) on table C US TOMERS . The pair of tuples {t2 , t3 } in the target database violates
both d1 and d2 ; the database is thus dirty.

625

(a2) A conditional FD (CFD): d3 = (I NSUR[Abx] → T REAT
[Dental]) on table T REATMENTS, expressing that insurance company ‘Abx’ only offers dental treatments (‘Dental’). Tuple t5 violates d3 , adding more dirtiness to the target database.

repair as an updated database that satisfies the constraints, it is not
possible to say what represents a “good” repair in this case.
Problem 2: Missing Repair Algorithms Since there is no semantics, we have no algorithms at our disposal to compute repairs.
Notice that combining the repairing algorithms available for each
of the constraints in isolation does not really help, since repairing
a constraint of one type may break one of a different type. Also,
current algorithms tend to hard-code the way in which preferred
values are used for the purpose of repairing the database. As a
consequence, there is no way to incorporate the different strategies
illustrated in (b1) and (b2) into existing repairing algorithms in a
principled way.

(a3) A master-data based editing rule (eR), d4 , stating that whenever a tuple t in C USTOMERS agrees on the SSN and P HONE attributes with some master-data tuple tm , then the tuple t must take
its NAME , S TR , C ITY attribute values from tm . Tuple t2 does not
adhere to this rule.
(a4) An inter-table CFD d5 between T REATMENTS and C USTOME RS , stating that the insurance company ‘Abx’ only accepts customers who reside in San Francisco (SF). Tuple pairs {t1 , t4 } and
tuples {t1 , t5 } violate this constraint.

Problem 3: Main-Memory Implementations and Scalability
Third, even if we were able to devise a reasonable semantics for
this kind of scenarios, we would still face a paramount problem,
i.e., computing solutions in a scalable way despite the high complexity of the problem. Computing repairs requires to explore a
space of solutions of exponential size wrt the size of the database.
In fact, previous proposals have mainly adopted main-memory implementations to speed-up the computation, with a rather limited
scalability (in the order of the tens of thousands of tuples).
Contributions The main contribution of this paper consists in
developing a uniform framework for data-cleaning problems that
solves the issues discussed above. More specifically:

With the dirty target database at hand, we are faced with the
problem of repairing it. The main problem is to identify and select
“preferred values” as modifications to repair the data.
(b1) Consider FD d1 . To repair the target database one may want to
equate t2 [P HONE] and t3 [P HONE]. The FD does not tell, however,
to which phone number these attribute values should be repaired:
‘122-1876’ or ‘000-0000’, or even a completely different value. As
it happens in this kind of problems, we assume that the P HONE
attribute values in the C USTOMERS table come with a confidence
(Conf.) value. If we assume that one prefers values with higher
confidence, we can repair t3 [P HONE] by changing it to ‘122-1876’.

(i) We introduce a language to specify constraints based on equality generating dependencies (egds) [4] that generalizes many of the
constraints used in the literature. This standardizes the way to express dependencies, and extends them to express inter-table constraints, with several benefits in terms of scalability, as discussed in
our experiments.

(b2) Similarly, when working with the T REATMENTS table, we
may use dates of treatments to infer the currency of other attributes.
If the target database is required to store the most recent value for
the salary by FD d6 = (SSN → S ALARY), this may lead us to
repair the obsolete salary value ‘10K’ in t4 with the more recent
(and preferred) value ‘25K’ in t5 .

(ii) The core contribution of the paper consists in the definition
of a novel semantics for the data-cleaning problem. The definition
of such a semantics is far from trivial, since our goal is to formalize the process of cleaning an instance as the process of upgrading
its quality, regardless of the specific notions of value preference
adopted in a given scenario. Our semantics builds on two main
concepts. First, we show that seeing repairs simply as cell updates
is not sufficient. On the contrary, we introduce the new notion of a
cell group, that is essentially a “partial repair with lineage”; then,
we formalize the notion of an upgrade by introducing a very general
notion of a partial order over cell groups; the partial order nicely
abstracts all of the most typical strategies to decide when a value
should be preferred to another, including master data, certainty, accuracy, freshness and currency. In the paper, we show how users
can easily plug-in their preference strategies for a given scenario
into the semantics. Finally, by introducing a new category of values, called lluns, we are able to complete the lattice of instances
induced by the partial order, and to provide a natural hook for incorporating user feedbacks into the process.

(b3) Notice that we don’t always have a clear policy to choose preferred values. For example, when repairing t2 [CC#] and t3 [CC#]
for FD d2 , there is no information available to resolve the conflict.
This means that the best we can do is to “mark” the conflict, and
then, perhaps, ask for user-interaction in order to solve it.
Another crucial aspect that complicates matters is the interaction between dependencies: repairing them in different orders may
generate different repairs.
(c1) Consider dependencies d1 and d4 . As discussed above, we
can use d1 to repair tuples t2 , t3 such that both have phone-number
‘122-1876’; then, since t2 and t3 agree with the master-data tuple
tm , we can use d4 to fix names, streets and cities, to obtain: (222,
F. Lennon, 122-1876, Sky Dr., SF, 781658), for t2 , and (222, F.
Lennon, 122-1876, Sky Dr., SF, 784659), for t3 . However, if, on
the contrary, we apply d4 first, only t2 can be repaired as before;
then, since t2 and t3 do not share the same name anymore, d1 has
no violations. We thus get a different result, of inferior quality.

(iii) We introduce the notion of a minimal solution and develop algorithms to compute minimal solutions, based on a parallel-chase
procedure. The definition of the chase is far from trivial, since
our goal is to guarantee both generality and proper scalability. To
start, we chase violations not at tuple level, but at equivalenceclass level [8]. This allows us to introduce a notion of a cost
manager as a plug-in for the chase algorithm that selects which
repairs should be kept and which ones should be discarded. The
cost manager abstracts and generalizes all of the popular solutionselection strategies, including similarity-based cost, set-minimality,
set-cardinality minimality, certain regions, sampling, among others. In Example 1, our semantics generates minimal solutions as

A first, striking observation about our example is that, despite
many studies on the subject, there is currently no way to handle
this kind of scenarios. This is due to several strong limitations of
the known techniques.
Problem 1: Missing Semantics First, although repairing strategies exist for each of the individual classes of constraints discussed
at items (a1), (a2) and (a3), there is currently no formal semantics for their combination. In fact, the interactions shown in (c1)
require a uniform treatment of the repairing process and a clear
definition of what is a repair. Aside from the generic notion of a

626

System
[8]
[10]
[23]
[18]
[7]
L LUNATIC

D EPENDENCY L ANGUAGE
FDs
CFDs ERs Int.T.CFDs
√
√
√
√
√
√
√
√
√
√
√

R EPAIR S TRATEGY
RHS
LHS
√
√
√
√
√
√
√
√
√
√

VALUE P REFERENCE
Confid.
Currency
Master
√
√
√
√

ext. dependencies

chase proced.

partial order

Cost
√
√
√

√
√

√

√

S OLUTION S ELECTION
Certain Card.Min Sampling

√
√

√

√
√

√
√

cost manager

Table 1: Feature Comparison.

2.

the ones in Figures 2 and 3, where Li values represent lluns (confidence values have been omitted); notice that other minimal solutions exist for this example. Cost managers allow users to differentiate between these two solutions, which have completely different
costs in terms of chase computation, and ultimately to fine-tune the
tradeoff between quality and scalability of the repair process.

t1
t2
t3

SSN
111
222
222

t4
t5
t6

C USTOMERS
P HONE
S TR
C ITY
CC#
408-3334 Red Ave.
SF
112321
122-1876
Sky Dr.
SF
L0
122-1876
Sky Dr.
SF
L0
T REATMENTS
S ALARY I NSUR .
T REAT
DATE
25K
Abx
Dental
10/1/2011
25K
Abx
Dental
8/12/2012
30K
Med
Eye surg. 6/10/2012

NAME
M. White
F. Lennon
F. Lennon
SSN
111
111
222

Figure 2: Repaired Instance #1.

t1
t2
t3

SSN
L1
L2
222

t4
t5
t6

C USTOMERS
P HONE
S TR
C ITY
CC#
408-3334 Red Ave.
NY
112321
122-1876
NULL
SF
781658
000-0000
Fly Dr.
SF
784659
T REATMENTS
S ALARY I NSUR .
T REAT
DATE
25K
Abx
Dental
10/1/2011
25K
L3
Choles.
8/12/2012
30K
Med
Eye surg. 6/10/2012

NAME
M. White
L. Lennon
L. Lennon
SSN
111
111
222

PRELIMINARIES

We start by presenting some background notions and introducing
the constraint language used in the paper.
A schema S is a finite set {R1 , . . . , Rk } of relation symbols,
with each Ri having a fixed arity ni ≥ 0. Let CONSTS be a countably infinite domain of constant values, typically denoted by lowercase letters a, b, c, . . . . Let NULLS be a countably infinite set of
labeled nulls, distinct from CONSTS. An instance I = (I1 , . . . , Ik )
of S consists of finite relations Ii ⊂ (CONSTS ∪ NULLS)ni , for i ∈
[1, k]. Let R be a relation symbol in S with attributes A1 , . . . , An
and I an instance of R. A tuple is an element of I and we denote
by t.Ai the value of tuple t in attribute Ai . Furthermore, we always assume the presence of unique tuple identifiers for tuples in
an instance. That is, ttid denotes the tuple with id “tid ” in I. Given
two disjoint schemas, S and T , if I is an instance of S and J is an
instance of T , then the pair hI , J i is an instance of hS, T i.
A relational atom over T is a formula of the form R(x) with
R ∈ T and x is a tuple of (not necessarily distinct) variables.
Traditionally, an equality generating dependency (egd) over T is
a formula of the form ∀ x(φ(x) → xi = xj ) where φ(x) is a
conjunction of relational atoms over T and xi and xj occur in x.
To express data-cleaning contraints, we rely on a specific form
of egd. More specifically, besides relation atoms, we also consider
equation atoms of the form t1 = t2 , where t1 , t2 are either constants in CONSTS or variables, and allow for both source and target
atoms in the premise. In our approach, a cleaning egd is then a
formula of the form ∀ x(φ(x) → t1 = t2 ) where φ(x) is a conjunction of relational and equation atoms over hS, T i, and t1 = t2
is of the form xi = c or xi = xj , for some variables xi , xj in x
and constant c ∈ CONSTS. Furthermore, at most one variable in the
conclusion of an egd can appear in the premise as part of a relation
atom over S. The latter condition is to ensure that the egd specifies
a constraint on the target database rather than on the fixed source
database. With an abuse of notation, in the following we shall often
refer to these cleaning egds simply as egds.
Egds for our running example are expressed as follows:
e1 . Cust(ssn, n, p, s, c, cc), Cust(ssn, n, p0 , s0 , c0 , cc0 ) → p = p0
e2 . Cust(ssn, n, p, s, c, cc), Cust(ssn, n, p0 , s0 , c0 , cc0 ) → cc = cc0
e3 . Treat(ssn, s, ins, tr, d), ins = ‘Abx’ → tr = ‘Dental’
e4 . Cust(ssn, n, p, s, c, cc), MD(ssn, n0 , p, s0 , c0 ) → n = n0
e5 . Cust(ssn, n, p, s, c, cc), MD(ssn, n0 , p, s0 , c0 ) → s = s0
e6 . Cust(ssn, n, p, s, c, cc), MD(ssn, n0 , p, s0 , c0 ) → c = c0
e7 . Cust(ssn, n, p, str , c, cc), Treat(ssn, sal , ins, tr, d),
ins = ‘Abx’ → c = ‘SF’
e8 . Treat(ssn, s, ins, tr, d), Treat(ssn, s0 , ins0 , tr0 , d0 ) → s = s0
An immediate observation is that constants in egds can be avoided altogether, by encoding them in additional tables in the source
database. Consider dependency e3 in our example in which two
constants appear: ‘Abx’ in attribute I NSUR and ‘Dental’ in attribute T REAT. We extend S with an additional binary source table,
denoted by C STe3 with attributes I NSUR and T REAT, corresponding to the “constant” attributes in e3 . Furthermore, we instantiate
C STe3 with the single tuple te3 : (Abx, Dental). Given this, e3 can

Figure 3: Repaired Instance #2.
(iv) We develop an implementation of the chase engine, called
L LUNATIC. To the best of our knowledge, L LUNATIC is the first
system that runs over the DBMS to compute repairs. We devote
special care in implementing our parallel chase – which may generate large trees of repairs – in a scalable way. A key ingredient
of our solution is the development of an ad-hoc representation systems for solutions, called delta relations. In our experiments, we
show that the chase engine scales to databases with millions of tuples, a considerable advancement in scalability wrt previous mainmemory implementations.
We believe that these contributions make a significant advancement with respect to the state-of-the-art. To start, our proposal
generalizes many previous approaches. Table 1 summarizes the
features of L LUNATIC with respect to some of these approaches.
L LUNATIC is the first proposal to achieve such a level of generality. Even more important, this work sheds some light on the crucial
aspect of data-cleaning problems, namely the trade-offs between
the quality of solutions and the complexity of repairing algorithms.
This allows us to select data-repairing algorithms with good scalability and superior quality with respect to previous proposals, as
our experiments show.
Organization of the Paper The preliminaries are in Section 2.
In Sections 3, 4, and 5 we introduce the key components of the
semantics of a cleaning scenario, which is defined in Section 6. The
chase algorithm is described in Sections 7 and 8. Our experiments
are reported in Section 9. Related work is described in Section 10.

627

be expressed as an egd without constants, as follows:
e03 . Treat(ssn, s, ins, tr, d), Cste3 (ins, tr0 )

into the semantics. Third, cost functions are used to (heuristically)
compare different repairs and choose the “good” ones.
In the following sections, we develop a new semantics for cleaning scenarios that departs from this standard in three significant
ways. Our first intuition is that, in order to generalize the semantics
to larger classes of constraints and different ways to pick-up preferred values, it is not sufficient to reason about single-cell updates.
On the contrary, we need to introduce a notion of “repairs with a
lineage”, called cell groups, in the sense that: (a) we keep track
of cells that need to be repaired together; (b) we keep track of the
provenance of their values, especially if they come from the source
database.
A second, key idea, is that the strategy to select preferred values
and repair conflicts should be factored-out of the actual repairing
algorithm. Our solution to do that is to introduce a notion of a
partial order over cell groups. The partial order plays a central role
in our semantics, since it allows us to identify when a repair is an
actual “upgrade” of the original database.
Finally, we introduce a principled way to check when a repaired
instance satisfies the constraints, and to compare repairs with one
another. This is based on an extension to data cleaning of the notion
of instance homomorphism [12] that is typically used to compare
the relative information content of database instances.
The next sections are devoted to these notions.

0

→ tr = tr

In general, S can be extended with such constants tables, one for
each CFD, and their source tables contain tuples for the constants
used to define the CFD. In other words, these tables coincide with
the pattern tableaux associated with the CFDs [16]. Of course, one
needs to provide a proper semantics of egds such that whenever
such constant tables are present, egds have the same semantics as
CFDs. We give such semantics later in the paper.
Further extensions of egds with, e.g., built-in predicates, matching functions and negated atoms, are needed to encode matching
dependencies and constraints for numerical attributes [20, 9]. We
do not consider them in this paper for simplicity of exposition.

3.

CLEANING SCENARIOS AND LLUNS

Our uniform framework for data repairing is centered around
the concept of a cleaning scenario. A cleaning scenario consists
essentially of a source schema S, a target schema T , and a set
of constraints Σ. Here, S and T represent the two databases involved in the repairing process (see Example 1): (i) S, the source
database, provides clean and reliable information as input for the
repairing process (like, for example, master data). We assume
that source databases cannot be changed and consist of constants
from CONSTS only; (ii) T , the target database, corresponds to the
database that is dirty relative to Σ, and that needs to be repaired.
The target database may contain constants from CONSTS and null
values from NULLS. Such null values indicate missing or unknown
values. However, we also allow the target database to contain a
third class of values, called lluns (pronounced “loons”), which we
introduce next.
Recall from Example 1 that t2 and t3 form a violation for the
dependency e2 (stating that customers with equal ssns and names
should have equal credit-card numbers), and that the target database
could be repaired by equating t2 .CC# = t3 .CC#. However, as discussed before, no information is available as to which value should
be taken in the repair. In such case, we repair the target database
(for e2 ) by changing t2 .CC# and t3 .CC# into the llun L0 , that is
to indicate that we need to introduce a new value for the credit-card
number that may be either 781658 or 784659, or some other preferred value. In this case, such value is currently unknown and we
mark it so that it might be resolved later on into a constant, e.g., by
asking for user input.
We denote by LLUNS = {L1 , L2 , . . .} an infinite set of symbols, called lluns, distinct from CONSTS and NULLS. Lluns can
be regarded as the opposite of nulls since lluns carry “more information” than constants. In our approach, they play two important
roles: (i) they allow us to complete the lattice induced by our partial orders, as it will be discussed in the next section; (ii) they provide a clean way to record inconsistencies in the data that require
the intervention of users to be resolved.
With this in mind, given an instance J of T , along with an instance I of S, the goal is to compute a repair of J, i.e., a set of
updates to J such that the resulting instance satisfies the constraints
in Σ.
Early works about database repairing [3, 21] followed an approach that relied on tuple-insertions and tuple-deletions. Since
tuple-deletions may bring to unnecessary loss of information, the
recent literature has concentrated on tuple updates, instead. Roughly
speaking, we may say that the semantics adopted in these works are
centered around three main ideas. First, a repair is seen as a set of
changes to the cells of the database (each cell being an attribute of
a tuple). Second, the logic to repair conflicting values is hardcoded

4.

CELL GROUPS AND REPAIRS

Given instance hI , J i of hS, T i, we represent the set of changes
made to repair the target database J in terms of cell groups. As
the name suggests, cell groups are groups of cells, i.e., locations
in a database specified by tuple/attribute pairs ttid .Ai . For example, t2 .CC# and t3 .CC# are two cells in the C USTOMERS table.
Observe the following:
(a) As our example shows, to repair inconsistencies, different cells
are often changed together, i.e., they are either changed all at the
same time or not changed at all. For example, t2 .CC# and t3 .CC#
are both modified to the same llun value in Figure 2. Cell groups
thus need to specify a set of target cells, called occurrences of the
group, and a value to update them.
(b) In addition, in some cases the target cells to repair receive their
value from the source database; consider Example 1 and dependency e5 . When repairing t2 , cell t2 .S TREET gets the value ‘Sky
Dr.’ from cell tm .S TREET. Since source cells contain highly reliable information, it is important to keep track of the relationships
among changes to target cells and values in the source. To do this,
a cell group c has a set of associated source cells carrying provenance information about the repair in terms of cells of the source
database. We call these source cells the justifications for c, since
they provide lineage information for the change we make to the target cells in c, i.e., to its occurrences. Occurrences and justifications
need to be kept separate since we can only update target cells, while
source cells are immutable.
(c) Cell groups provide an elegant way of describing repairs. Indeed, in order to specify a repair it suffices to provide the original
target database together with the set of cell groups to modify. In
other words, cell groups can be seen as partial repairs with lineage.
These observations are captured by the following definitions:
Definition 1 [C ELL G ROUP] A cell group g over an instance hI , J i
of hS, T i is a triple hv → C, by Cs i where: (i) v is a value in
CONSTS ∪ NULLS ∪ LLUNS ; (ii) C is a finite set of cells of the
target instance, J, called the occurrences of g, denoted by occ(g);

628

(iii) Cs is a finite set of cells of the source instance, I, called the
justifications of g, denoted by just(g).
A cell group g = hv → C, by Cs i can be read as “change the
target cells in C to value v, justified by the source cells in Cs ”. We
define a repair to an instance hI , J i as a set of cell groups.

to be able to specify different partial orders for different repairing
problems in a simple manner. To do this, the user only has to specify for each attribute in the target schema when two values are preferred over each other. This is done by specifying an assignment Π
of so-called ordering attributes to T . As we will see shortly, such
an assignment automatically induces a partial order on cell groups.

Definition 2 [R EPAIR] A repair Rep = {g0 , . . . , gk } for instances
hI , J i is a (possibly empty) finite set of cell groups over hI , J i,
such that each cell of J occurs in at most one cell group gi .
That is, each cell in J is either unchanged in a repair or it is
modified in a unique way as described by the cell group to which it
belongs. We denote by gRep (c) the cell-group of cell c according to
Rep.

A Hierarchy of Information Content. In order to define our
partial order, let us first introduce a simple hierarchy between the
three kinds of values that appear in a database, namely nulls, constants, and lluns. More specifically, given two values v1 , v2 ∈
NULLS ∪ CONSTS ∪ LLUNS , we say that v2 is more informative
than v1 , in symbols v1  v2 if v1 and v2 are of different types, and
one of the following holds: (i) v1 ∈ NULLS, i.e., the first value is a
null value; or (ii) v2 ∈ LLUN, i.e., the second value is a llun.

Example 2: In our example, consider the repair Rep1 , consisting
of the following cell groups referred to as g1 , . . . , g7 :

User-Specified preferred values. We say that an attribute A of T
has ordered values if its domain DA is a partially ordered set. To
specify which values should be preferred during the repair, users
may associate with each attribute Ai of T a partially ordered set
PAi = hD, ≤i. The poset PAi associated with attribute Ai may
be the empty poset, or its domain DAi if Ai has ordered values, or
the domain of a different attribute DAj that has ordered values. In
the latter case, we call Aj the ordering attribute for Ai . Intuitively,
PAi specifies the order of preference for values in the cells of Ai .
An assignment of ordering attributes to attributes in T is denoted
by Π. For reasons that become clear shortly, Π is referred to as the
partial order specification.
In our example, the DATE attribute in the T REATMENTS table,
and the confidence column, C ONF, in the C USTOMERS table have
ordered values (to simplify the treatment, we consider C ONF as
an attribute of the table). For these attributes, we choose the corresponding domain as the associated poset (i.e., we opt to prefer more recent dates and higher confidences). Other attributes,
like the P HONE attribute in the C USTOMERS table, have unordered
values; we choose C ONF as the ordering attribute for P HONE (a
phone number will be preferred if its corresponding confidence
value is higher). Notice that there may be attributes, like S ALARY
in T REATMENTS, that have ordered values; however, the natural
ordering of values does not reflect our notion of a preferred value.
To model the correct notion of preference, we use DATE as the
ordering attribute for S ALARY (we prefer most recent salaries). Finally, attributes like SSN will have an empty associated poset, i.e.,
all constant values are equally preferred. Below is a summary of
the assignment Π of ordering attributes in our example (attributes
not listed have an empty poset):


PC USTOMERS.C ONF
= DC USTOMERS.C ONF 




= DT REATMENTS.DATE 
 PT REATMENTS.DATE

PC USTOMERS.P HONE
= DC USTOMERS.C ONF
Π=





 PT REATMENTS.S ALARY = DT REATMENTS.DATE 

PC USTOMERS.SSN
= ∅

Rep1 = { g1 : hL0 (781658, 784659) → {t2 .CC#, t3 .CC#}, by ∅i
g2 : hF.Lennon → {t2 .NAME, t3 .NAME}, by {tm .NAME}i
g3 : h122-1876 → {t2 .P HN, t3 .P HN}, by ∅i
g4 : hSky Dr. → {t2 .S TR, t3 .S TR}, by {tm .S TR}i
g5 : hDental → {t5 .T REAT}, by {tc3 .T REAT}i
g6 : hSF → {t1 .C ITY}, by {tc7 .C ITY}i}
g7 : h25K → {t4 .S ALARY, t5 .S ALARY}, by ∅i}
Cell group g1 fixes credit-card numbers for dependency e2 ; it
has empty justifications because no source relation is involved in
e2 . On the contrary, cell groups g2 and g4 repair tuples t2 , t3 for
dependencies e4 , e5 ; justifications for these groups contain the respective cells in the master-data tuple tm . Similarly, for cell groups
g5 , g6 ; here tc3 , tc7 are the tuples in the C STe3 , C STe7 tables, encoding the constants in the original CFDs.
When applied to the original database in Figure 1, repair Rep1
yields the repaired instance shown in Figure 2. Clearly, other repairs are possible. For example, to resolve e3 one may consider
changing the value of the cell t5 .I NSURANCE into a new llun value
L1 , i.e., an unknown value that improves ‘Abx’. The following repair, Rep2 , follows the same approach to satisfy all dependencies,
and yields the repaired instance shown in Figure 3:
Rep2 = {g7 , hL1 → {t1 .SSN}, by ∅i, hL2 → {t2 .SSN}, by ∅i
hL3 → {t5 .I NSURANCE}, by ∅i}
Note that J itself can be seen as the empty repair, Rep∅ .
Given hI, Ji, we say that a repair is complete if each cell of J
occurs in a cell group in Rep, i.e., all cells in J are covered by the
repair. We may assume, without loss of generality, that a repair is
always complete. Indeed, a repair Rep can be easily completed into
a complete repair Rep0 , as follows:
(i) initially, we let Rep0 = Rep;
(ii) for each cell c of J that is not changed by Rep, if val(c) ∈
0
CONSTS , then we add to Rep the cell group hval(c) → {c}, by ∅i;
(iii) for each cell c of J that is not changed by Rep, if val(c) ∈
0
NULLS , then we add to Rep the cell group of c with value val(c),
occurrences consisting of all cells of J in which val(c) occurs and
empty justifications.
From now on, we always assume a repair to be complete, and we
blur the distinction between a repair Rep and the instance Rep(J )
obtained by applying Rep to J .

5.

Partial order on cell values. Given an assignment Π, we can define a corresponding partial order Π
J for the values of cells of the
target instance J as follows. For any pair of values v1 , v2 we say
that v1 Π
J v2 iff one of the following holds:
(i) either v1 = v2 or v1 v2 , i.e., the values are equal or the second
one is more informative than the first;

THE PARTIAL ORDER

(ii) v1 appears in cell t1 .A1 , v2 in cell t2 .A2 in J , and both are
constants in CONSTS; then, assume the ordering attributes for A1
and A2 , called A01 , A02 have the same poset, i.e., PA01 = PA02 ; call
v10 , v20 the values of cells t1 .A01 , t2 .A02 . Then, v1 Π
J v2 iff v1 = v2
or v10 < v20 according to PA01 = PA02 .

We are now ready to introduce another crucial ingredient of our
framework: the partial order. The partial order is the core element
of the semantics of our repairs and, as already mentioned, is used
to indicate preferred upgrades to the target database. We want users

629

of cells in hI , J i induces a partial order Π over the cell groups
and repairs of hI , J i. In fact, Π is semi-join lattice.

We also consider values of the source instance I . In our approach, source values are immutable, and all equally preferable.
So, we assume that the partial order I over values in I is based
on rule (i) only. We call Π
hI ,J i the partial order over values of cells
in hI , J i obtained by the union of Π
J and I , with the additional
rule that values of source cells are always preferable to values of
target cells, i.e., for each target cell t.At and source cell t0 .As , it
0
is always the case that val(t.At ) Π
hI ,J i val(t .As ). In fact, we
always give preference to values from the source, like master-data
or constant values in dependencies.
Given the partial order Π
hI ,J i , in the following we want to be
able to compute upper bounds for cell values. To do this, we use
lluns. Indeed, for any set C of cells we denote by lub Π (C) the

Notice that, besides the standard rules above, users may specify
additional custom rules to plug-in other value-selection strategies
and refine the lattice of cell groups. As an example, a frequency
rule may state that the lub of cell groups g and g 0 with constant
values c1 and c2 and empty justifications should take as value c1
(c2 , resp.) if |occ(g1 )| > |occ(g2 )| (|occ(g2 )| > |occ(g1 )|, resp.).

6.

SEMANTICS

With the partial order specification Π in place, we now define a
cleaning scenario as a quadruple CS = {hS, T i, Σ, Π}. Given a
cleaning scenario and an instance hI , J i, we address the problem
of defining a solution for CS over hI , J i. Intuitively, a solution is
a repair for hI , J i that satisfies the set Σ of egds and is an upgrade
of the original target instance J relative to Π . We next formalize
these notions.
Consider an instance hI, Rep(J)i and a set Σ of constraints.
Usually, Rep(J) is called a solution if hI, Rep(J)i satisfies Σ using the standard semantics of first-order logic. Since we want to
—rather ambitiously— ensure that there is always a solution we
need to revise this semantics. In contrast, previous proposals often
fail to return a repair or are stuck in an endless loop during repairing, as is illustrated next.
Consider dependency e3 from Example 1. Suppose that a contradictory dependency e003 .Treat(ssn, s, ins, tr, d), ins = ‘Abx0
→ tr = ‘Cholest.0 is specified. In addition, assume that only modifications to the T REAT attribute-values are allowed. Clearly, there is
no repair made of constants that can satisfy both dependencies [16].
However, one may consider of changing ‘Dental’ and ‘Cholest.’ to
a llun L that improves both original values. In essence, the llun has
the role of indicating to the user that the constraints are contradictory. In our setting, we want to regard this repair as a solution of a
conflicting cleaning scenario.
Consider an egd e : ∀x φ(x̄) → x = x0 . First, recall that, in the
standard semantics, hI, Rep(J)i satisfies e if for any homomorphism h that maps the variables x̄ into values of hI, Rep(J)i such
that φ(h(x̄)) is true, then also h(x) = h(x0 ) must be true. We want
this to hold in our semantics as well. However, we want more. That
is, we allow h(x) 6= h(x0 ) as long as the cell group corresponding
to h(x) is an upgrade to the cell groups corresponding to h(x0 ), or
vice versa.
To make this precise, we need to extend h to a mapping from
variables to cell groups. Since h associates values to variables,
it also associates with each variable xi ∈ x̄ a set of cells from
hI, Rep(J)i, called cells h (xi ), one for each occurrence of xi and
all with the same value, h(xi ). We use these to define the cell group
of xi according to h, as follows.
Given a formula φ(x), a repair Rep, an homomorphism h of
φ(x̄) into hI, Rep(J)i, and a variable xi ∈ x̄, the cell group of xi
according to h is defined as gh (xi ) = hh(xi ) → C, by Cs i where C
(resp. Cs ) is the union of all occurrences (resp. justifications) of cell
groups gRep (ci ) in Rep, for each cell ci ∈ cells h (xi ). In addition,
Cs contains all cells in cells h (xi ) that belong to the source I.
We are now ready to introduce our extended notion of satisfaction, namely satisfaction after repairs:

hI ,J i

value that is (i) either the least upper bound for values of all cells in
C according to ΠhI ,J i , if it exists; (ii) a new value Ni not in J, if
all cells in C have null values; (iii) a new llun value Lj otherwise.
Partial order on cells groups. The partial order Π
hI ,J i over cell
values induces a partial order on the cell groups of hI , J i. Before
we turn to the definition, we want to exclude from the comparison
cell groups that correspond to unjustified ways of changing the target. In order to do this, we say that a cell group g has a valid value
if one of the following conditions holds. Consider the value vallub
that is the least upper bound of values in occ(g) ∪ just(g) according
to ΠhI ,J i , i.e., vallub = lub Π (just(g) ∪ occ(g)). Then, either
hI ,J i

val(g) = vallub , i.e., the cell group takes the value of the least upper bound, or vallub val(g), i.e., the cell group takes an even more
informative value.
Given cell groups g and g 0 with valid values, we say that g Π g 0
iff (i) occ(g) ⊆ occ(g 0 ) and just(g) ⊆ just(g 0 ), and (ii) either
val(g) and val(g 0 ) are values of the same type (null, constant, or
llun), or val(g)  val(g 0 ). In essence, we say that a cell group g 0
can only be preferred over a cell group g according to the partial
order, if a containment property is satisfied, and the value of g 0 is at
least as informative as the value of g. If the containment property
is not satisfied for g and g 0 then these cell groups are incomparable relative to the partial order. Indeed, cell groups that change
unrelated groups of cells represent incomparable ways to modify a
target instance.
Example 3: Consider a simple relation R(A, B), with three dependencies: (i) an FD A → B, and two CFDs: (ii) A[a] →
B[x], A[a] → B[y]. Notice that the two CFDs clearly contradict each other. Assume R contains two tuples: t1 : R(a, 1), t2 :
R(a, 2), and that PA is A itself. Following is a set of ordered cell
groups:
h1 → {t1 .B}, by ∅i
Π
h2 → {t1 .B, t2 .B}, by ∅i
Π
hx → {t1 .B, t2 .B}, by {tc1 .x}i
Π
hL → {t1 .B, t2 .B}, by {tc1 .x, tc2 .y}i
Partial order on repairs. Given an instance hI , J i, a partial order
Π over cell groups in hI , J i, and two complete repairs, Rep, Rep0 ,
we say that Rep0 upgrades Rep, denoted by Rep Π Rep0 , if for
each group g ∈ Rep there exists a group g 0 ∈ Rep0 such that g Π
g 0 . If Rep Π Rep0 and the converse does not hold, then we write
Rep ≺Π Rep0 . A repair Rep0 is thus preferable to Rep whenever
Rep Π Rep0 . This is where the real strength of the partial order
lies: it provides a uniform way of incorporating information on
preferred repairs.

Definition 3 [S ATISFACTION A FTER R EPAIRS] Given an egd e :
∀x φ(x) → x = x0 , an instance hI, Ji, and a repair Rep, we say
that hI, Rep(J)i satisfies after repairs e wrt the partial order Π if,
whenever there is an homomorphism h of φ(x) into hI, Rep(J)i,
then (i) either the value of h(x) and h(x0 ) are equal, or (ii) it is
the case that gh (x) Π gh (x0 ) or gh (x0 ) Π gh (x).

Proposition 1: Given an assignment Π of ordering attributes to
attributes in T , the corresponding partial order Π
hI ,J i over values

630

We can now find a repair that satisfies the conflicting egds e3
and e003 above. Given a tuple t in the target, consider Rep that
repairs t.T REAT with L, and justifies the change with both cells
in the source corresponding to constants ‘Dental’ and ‘Cholest.’.
Now, despite the fact that L is not equal to any of the constants in
the dependencies, both dependencies are satisfied after repairs by
Rep(J).

repair. In addition, the cell-group provides complete information
about the conflict, both in terms of which target cells – and therefore which original values – where involved, and also in terms of
source values that justify the change.

7.

COMPUTING SOLUTIONS

In order to generate solutions for cleaning scenarios, we resort
to a variant of the traditional chase procedure for egds [12]. However, our chase is a significant departure from the standard one, for
several reasons: (i) during the chase, we shall make extensive use
of the partial order, Π ; (ii) to generate all possible solutions, a
dependency may be chased both forward, to satisfy its conclusion,
or backward, to falsify its premise; this, in turn, means that the
we need to consider a disjunctive chase, which generates a tree of
alternative repairs; (iii) finally, and most important, we shall not
consider violations at the tuple level, as it is common [12], but at
the higher level of equivalence classes.
To explain this latter difference, consider a simple functional dependency A → B over relation R(A, B, C), with tuples t1 =
R(1, 2, x), t2 = R(1, 2, y), t3 = R(1, 4, z), t4 = R(2, 5, w),
t5 = R(2, 5, v). It is highly inefficient to analyze the violations of
this FD at the tuple level; in fact, eventually, the B value of t1 , t2 , t3
will all become equal, and therefore one may prefer grouping and
fixing them together. In the literature [8, 15] this has been formalized by means of equivalence classes. We want to introduce a
similar concept into our chase algorithm. Given the higher generality of our dependency language, we need a number of preliminary
definitions.

Definition 4 [S OLUTION] Given a cleaning scenario CS = {hS, T i,
Σ, Π} and instance hI , J i a solution for CS over hI , J i is a repair Rep such that: (i) J Π Rep, i.e., Rep upgrades J; and (ii)
hI, Rep(J)i satisfies after repairs Σ wrt Π .
An important property of cleaning scenarios is that every input
instance has a solution, albeit a solution that is not necessarily minimal and is rather uninformative.
Theorem 2: Given a scenario CS = {hS, T i, Σ, Π} and an input
instance hI , J i, there always exists a solution for CS and hI , J i.
Proof: Indeed, there is always a solution corresponding to the repair that changes all cells of J to a single llun L, and justifies it by
all cells in I, i.e., Reptrivial = hL → cells(J), by cells(I)i.
2
Among all possible repairs, we are interested in those that minimally upgrade the dirty instance.
Definition 5 [M INIMAL S OLUTION] A minimal solution for a cleaning scenario is any solution Rep that is minimal wrt ≺Π , i.e., such
that there exists no other solution Rep0 such that Rep0 ≺Π Rep.
The repair Rep1 in Example 2 is a minimal solution for the scenario in Example 1: it is an upgrade of J , it satisfies the dependencies, and by undoing any of its changes violations arise. Minimal
solutions are not unique. Indeed, also repair Rep2 in Example 2
is a minimal solution. As an example of a non-minimal solution,
one can add to Rep2 the cell group hL4 → {t2 .NAME}, by ∅i.
The resulting repair Rep3 is still a solution but not a minimal one
(Rep2 ≺Π Rep3 ). Consider now repair Rep4 , obtained by adding
a cell group h111111 → {t2 . CC#}, by ∅i to Rep2 . In this case,
Rep4 is not a solution because the last cell group is totally unjustified wrt the partial order, and therefore it is not true that Rep4 (J)
is an upgrade of the original target instance.
An important property is that two repairs can be efficiently compared wrt to the partial order. We assume here that the partial order
of two values v ΠhI ,J i v 0 can be checked in constant time.

Preliminary Notions Recall that, given an homomorphism h of a
formula φ(x̄) into hI, Rep(J)i, we denote by gh (x) the cell group
associated by h with variable x. We first introduce the notions of
witness and witness variable for a dependency e. Intuitively, the
witness variables are those variables upon which the satisfiability of
the dependency premise depends; these are all variables that have
more than one occurrence in the premise, i.e., they are involved in
a join or in a selection.

Theorem 3: Given two solutions Rep, Rep0 for a scenario CS over
instance hI , J i, one can check Rep Π Rep0 in O(n+kmlog(m))
time, where n is the number of cells in J, k is the maximum number
of cell groups in Rep, Rep0 , and m is the maximum size of a cell
group in Rep, Rep0 .
Given a cleaning scenario CS and an instance hI , J i, the data
repairing problem consists of computing all minimal solutions for
CS over hI , J i. We provide a chase-based algorithm for the data
repairing problem in the next section.

Definition 6 [W ITNESS] Let e : ∀x (φ(x) → x = x0 ) be an egd. A
witness variable for e is a variable x ∈ x̄ that has multiple occurrences in φ(x̄). For an homomorphism h of φ(x̄) into hI, Rep(J)i,
we call a witness, wh for e and h, the vector of values h(x̄w ) for
the witness variables x̄w of e.
Consider, for example, dependency e8 in Example 1 (we omit
some of the variables for the sake of conciseness): e8 . Treat(ssn, s,
. . .), Treat(ssn, s0 , . . .) → s = s0 . Assume that the target instance
T REATMENTS contains tuples t4 = (ssn : 222, salary : 10K, . . .),
t5 = (ssn : 222, salary : 25K, . . .). We have an homomorphism
h that maps the first atom of e8 into t4 , and the second one into
t5 . In this case, the witness variable, i.e., the variable that imposes
the constraint that the two tuples have the same SSN, is ssn, and its
value is 222.

What are Lluns, in the End? The role and the importance of lluns
should now be apparent. While lluns are nothing more than symbols from a distinguished set, like constants and nulls, their use in
conjunction with cell groups makes them a powerful addition to the
semantics. Not only they allow us to complete the lattice of cellgroups and repairs, but, when appearing inside cell-groups, they
also provide important lineage information to support users in the
delicate task of resolving conflicts. Consider again Example 3 in
Section 5. The cell group hL → {t1 .B, t2 .B}, by {tc1 .x, tc2 .y}i is
a clear indication that it was not possible to fully resolve the conflicts, and therefore user interventions are needed to complete the

Definition 7 [E QUIVALENCE C LASS] Given a repair Rep, and an
egd e : ∀x (φ(x) → x = x0 ), let x̄w ⊆ x̄ be the witness variables
of e. An equivalence class for Rep and e, H, is a set of homomorphisms of φ(x̄) into hI, Rep(J)i such that all hi ∈ H have equal
witness values hi (x̄w ).
Notice that equivalence classes induce classes of tuples in a natural way. In our example above, the tuples are partitioned into two
equivalence classes, as follows: ec1 = {t1 , t2 , t3 } (with witness 1)
and ec2 = {t4 , t5 } (with witness 2).
To identify a violation, we look for different values in the conclusion of e. To see an example, consider the equivalence class

631

w-cellrs H (ci )) by the cell group gi0 that is an immediate successor
of gi according to Π as follows:
Reprs H = Reprs H − {gi } ∪ {gi0 }

ec1 (witness 1), composed of the three tuples {t1 , t2 , t3 }: to identify the violation, we notice that they have two different values for
the B attribute, 2 and 4, respectively. To formalize this, we introduce the set of witness groups, w-groupsH , and conclusion groups,
c-groupsH , for H and e, as the set of cell groups associated by
any homomorphism h ∈ H with the witness variables, x̄w , and the
conclusion variables, x, x0 , respectively:
S
w-groupsH = h∈H,xw ∈x̄w gh (xw )
S
S
c-groupsH = h∈H gh (x) ∪ h∈H gh (x0 )

Note that such a successor always exists. Indeed, gi0 = hLi →
occ(gi ), by ∅i, where Li is a new LLUN value, is a successor of gi .
Given Rep, each repair strategy rs iH for H generates a different
step, Reprs i . We simultaneously consider all these chase steps, in
H
parallel, and write Rep →e,H Reprs 0 , Reprs 1 . . . , Reprs n .
H

H

H

Consider again dependency e8 in Example 1, and the equivalence class associated with witness ssn = 222. The cell groups
for the conclusion cells are: g = h10K → {t4 .S ALARY}, by ∅i
and g 0 = h25K → {t5 .S ALARY}, by ∅i. Notice that the two
cell groups are incomparable, and therefore we have a violation.
The chase procedure generates three different repairs for the violation: (a) the forward repair is: Repf,f = h25K → {t4 .S ALARY,
t5 .S ALARY}, by ∅i (25K is more recent than 10K as a salary, and
therefore it is preferred); as you can see, the least upper bound is
constructed in such a way that it contains the union of occurrences
and the union of justifications of the two conflicting groups; (b) the
first backward repair, which changes the first occurrence of the witness variable ssn to a llun L1 : Repb,f = hL1 → {t4 .SSN}, by ∅i;
(c) the second backward repair, changing the second occurrence of
ssn to L2 : Repf,b = hL2 → {t5 .SSN}, by ∅i.

We say that an equivalence class for Rep and e generates a violation if it has at least two conclusion groups with different values and such that there is no ordering among them, i.e, there exist g1 , g2 ∈ c-groupsH such that val(g1 ) 6= val(g2 ) and neither
g1 Π g2 nor g2 Π g1 . In this case, we say that e is applicable to
hI, Rep(J)i with H.
The Chase We are now ready to define the notion of a chase step.
Our goal is to define the chase in such a way that it is as general
as possible, but at the same time it allows to plug-in optimizations
to tame the exponential complexity. In order to do this, we introduce the crucial notion of a repair strategy for an equivalence class,
which provides the hook to introduce the notion of a cost manager
in the next section.
A repair strategy rs H for H is a mapping from the set of conclusion cell-groups, c-groupsH of Rep and H, into the set {f , b}
(where f stands for “forward”, and b for “backward”). We call the
forward groups, forw-grs H , of rs H the set of groups gi such that
rs H (gi ) = f , and the backward groups, back-grs H , those such
that rs H (gi ) = b.
For each backward group g ∈ back-grs H and for each target
cell ci ∈ g, we assume that the repair strategy rs H also identifies
(whenever this exists) one of the witness cells in w-groupsH to be
backward-repaired. This cell, denoted by w-cellrs H (ci ), must be
such that:

Definition 9 [C HASE T REE] Given a cleaning scenario CS = {S,
T , Σ, Π}, a chase of hI , J i with Σ is a tree whose root is hI , J i,
i.e., the empty repair, and for each node Rep, the children of Rep
are the repairs Rep0 , Rep1 , . . . , Repn such that, for some e ∈ Σ
and some H, it is the case that Rep →e,H Rep0 , Rep1 , . . . , Repn .
The leaves are repairs Rep` such that there is no dependency applicable to hI, Rep` (J)i with some equivalence class H. Any leaf in
the chase tree is called a result of the chase of hI , J i with Σ.
Note that, as usual, the chase procedure is sensitive to the order
of application of the dependencies. Different orders of application of the dependencies may lead to different chase sequences and
therefore to different results.
We next show that the chase procedure always generates solutions, i.e., it is sound, and it terminates after a finite number of
steps. Furthermore, all minimal solutions can be obtained in this
way, i.e., the chase is complete for minimal solutions.

(i) it belongs to the same tuple as ci ;
(ii) the corresponding cell group gi according to Rep has a constant
value, i.e., val(gi ) ∈ CONSTS;
(iii) the corresponding cell group gi has empty justifications, i.e.,
just(gi ) = ∅.
Observe that we do not chase backward in two cases: first, when
cells contain nulls or lluns; in fact, nulls and lluns are essentially
placeholders, and there is no need to replace a placeholder by another one, since this is does not represent an upgrade of the repair;
second, when cell values have a justification from the source; since
we use the source to model high-reliability data, we consider it unacceptable to disrupt a value coming from the source in favor of a
llun.
Each chase step is defined based on a specific repair strategy.

Theorem 4: Given a cleaning scenario CS = {S, T , Σ, Π} and
an instance hI , J i, the chase of hI , J i with Σ (i) terminates; (ii)
it generates a finite set of results, each of which is a solution for CS
over hI , J i; and (iii) it generates all minimal solutions.
Complexity It is well-known [5] that a database can have an exponential number of solutions, even for a cleaning scenario with a single FD and when no backward chase steps are allowed. In general,
it is readily verified that a cleaning scenario can have at most an
exponential number of solutions. When considering the disjunctive
chase procedure, as outlined above, one can verify that each solution is computed in a number of steps that is polynomial in the size
of the data. For this, it suffices to observe that one can associate an
integer-valued function f on repairs such that f (Rep) < f (Rep0 )
whenever Rep →e,H Rep0 during the chase. Intuitively, f depends
on the number of llun values and sizes of cell groups in the repairs.
Since both the number of lluns and size of cell groups is bounded by
the input instance, we may infer that f cannot be increased further
after polynomially many steps, i.e., when a solution is obtained.
In contrast, computing all solutions by means of the chase takes
exponential time in the size of instance. Indeed, given the polynomial size of each branch in the chase tree, as argued above, and the

Definition 8 [C HASE S TEP] Given a cleaning scenario CS = {S,
T , Σ, Π}, and a complete repair Rep of J, let e : ∀x (φ(x) → x =
x0 ) be an egd in Σ, applicable to hI, Rep(J)i with H. For each
repair strategy rs H , a chase step generates a new repair Reprs H
defined as follows:
(i) to start, we initialize Reprs H = Rep
(ii) then, we replace all forward groups by their least upper bound:
Reprs H = Reprs H − forw-grs H ∪ lubΠ (forw-grs H )
(iii) finally, we add the backward repairs, i.e, for each backward
group g ∈ back-grs H , and cell ci ∈ occ(g), we replace gi = gRep (

632

fact that the branching factor is polynomially bounded by the input,
the overall chase tree is exponential in size. We discuss techniques
to handle this high complexity in the next section.

8.

pruning strategies, to incorporate the notion of a certain region, and
refute all steps in which changes are made to attributes of the target
that are considered to be “fixed”, i.e., reliable, or perform different
forms of sampling. In the following, we shall always assume that a
cost manager has been selected in order to perform the chase.

A SCALABLE CHASE

The chase procedure defined in the previous section provides an
elegant operational semantics for cleaning scenarios. However, as
argued above, computing all solutions has very high complexity,
which makes the chase often impractical. In this section, we introduce a number of techniques that improve the scalability of the
chase, namely: a central component of our framework, the cost
manager, and a representation systems for chase trees, called delta
databases.

8.1

8.2

Delta Databases

Even with cost managers in place, the parallel nature of our chase
algorithm imposes to store a possibly large tree of repairs. A naive
approach in which new copies of the whole database are created
whenever we need to generate a new node in the tree, is clearly inefficient. To solve this problem, we introduce an ad-hoc representation system for nodes in our chase trees, called delta databases.
Delta databases are a formalism to store a finite set of worlds into a
single relational database. Intuitively, they allow to store “deltas”,
i.e., modifications to the original database, rather than entire instances as is done in the naive approach.
Delta relations rely on an attribute-level storage system, inspired
by U-relations [2], modified to efficiently store cell groups and
chase sequences. More specifically, (i) each column in the original database is stored in a separate delta relation, to be able to
record cell-level changes; (ii) chase steps are identified by a function with a prefix property, such that the id of the father of n is a
prefix of the encoding of n; this allows to quickly reconstruct the
state of the database at any given step, using fast SQL queries; (iii)
additional tables are used to store cell groups, i.e., occurrences and
justifications.
More formally, we introduce a function stepId() that associates a
string id with each chase step, i.e., with each node in the chase tree,
and has the prefix property such that, stepId(father (n)) is a prefix
of stepId(n), for each n. For this, we use the function that assigns
the id r to the root, r.0, r.1, . . ., r.n to its children, and so on.

Introducing the Cost Manager

Chasing at the equivalence-class level is more efficient than chasing at the tuple level, but by itself it does not reduce the total
number of solutions, and ultimately the complexity of the whole
chase process. In fact, previous proposals have chosen many different and often ad-hoc ways to reduce the complexity by discarding
some of the solutions in favor of others. Among these we mention various notions of minimality of the repairs [3, 8, 7], certain
regions [18], and sampling [7]. We propose to incorporate these
pruning methods into the chase process in a more principled and
user-customizable way by introducing a component, called the cost
manager.
Definition 10 [C OST M ANAGER] Given a cleaning scenario, CS
and instance hI , J i, a cost manager for CS and hI , J i is a predicate
CM over repair strategies to be used during the chase. For each
repair strategy rs H for equivalence class H, it may either accept it
(CM(rs H ) = true), or refuse it (CM(rs H ) = false).
During the chase, we shall systematically make use of the cost
manager. Whenever we need to chase an equivalence class, we only
generate repairs corresponding to repair strategies accepted by the
cost manager. The standard cost manager is the one that accepts
all repair strategies, and may be used for very small scenarios. As
an alternative, our implementation offers a rich library of cost managers. Among these, we mention the following, that have been used
in experiments:
– a maximum size cost manager (S N): it accepts repair strategies as
long as the number of leaves in the chase tree (i.e., the repairs produced so far) are less than N ; as soon as the size of the chase tree
exceeds N , it accepts only the first one of them, and rejects the rest;
as a specific case, the S 1 cost manager only considers one order of
application of the dependencies, and ignores other permutations;
– a frequency cost manager (FR): in order to repair equivalence
class H for dependency e, FR adopts the following rules; it relies
on the frequency of values appearing in conclusion cells, and on
a similarity measure for values (based on the Levenshtein distance
for strings); then: (i) it rejects repair strategies that backward-chase
cells with the most frequent conclusion value; (ii) for every other
conclusion cell, if its value is similar (distance below a fixed threshold) to the most frequent one, the cell is forward-chased; otherwise,
it is backward chased; this is typically used with a frequency rule
in the partial order of cell-groups;
– a forward-only cost manager (FO): it accepts forward-only repair
strategies, and rejects those that perform backward repairs.
Notice that combinations of these strategies are possible, to obtain, e.g., a FR- S 5 or a FR- S 1-FO cost manager. The FR- S 5 relies on value frequencies and, in addition, it considers five different
permutations of the dependencies, and for each of them will compute one repair. Alternative cost managers may implement different

Definition 11 [D ELTA DATABASE] Given a target database schema
R = {R1 , . . . , Rk }, a delta database for R contains the following
tables: (i) a delta table Ri Aj with attributes (tid , stepId, value),
for each Ri and each attribute Aj of Ri ; (ii) a table occurrences,
with schema (stepId, value, tid , table, attr); (iii) a table justifications, with schema (stepId, value, tid , table, attr).
During the chase, we store the whole chase tree into the delta
database. We do not perform updates, which are slow, but execute inserts instead. Whenever, at step s, a cell tid .A in table R
is changed to value v, we store a new tuple in the delta table R A
with value (tid , stepId, v). Using this representation, it is possible
to store trees of hundreds of nodes quite efficiently. In addition, it
is relatively easy to find violations using SQL (the actual queries
are omitted for space reasons).
In the next section we show how the combination of our advanced chase procedure and its implementation under the form of
delta databases scale to large repairing problems with millions of
tuples and large chase trees.

9.

EXPERIMENTS

The proposed algorithms have been implemented in a working
prototype of the L LUNATIC system, written in Java. In this section, we consider several cleaning scenarios, of different nature
and sizes, and study both the quality of the repairs computed by
our system, and the scalability of the chase algorithm. We show
that our algorithm produce repairs of better quality with respect to
other systems in the literature, and at the same time scales to large
databases. All experiments have been executed on a Intel i7 machine with 2.6Ghz processor and 8GB of RAM under Linux. The
DBMS was PostgreSQL 9.2.

633

LLUNATIC-FR-S1

LLUNATIC-FR-S10

LLUNATIC-FR-S50

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

METRIC 1

VERTEX COVER

MIN. COST

AVG
MIN

0.5
0.4
0.3

AVG

0.2
MIN

0.1

(a) Max F-Measure (Metric 1) for HOSPITAL

SAMPLING-500

0.6

1% 2% 3% 4% 5% 1% 2% 3% 4% 5% 1% 2% 3% 4% 5%
5k
10k
25k
0.8

LLUNATIC-FR-S1-FO

0.1
1% 2% 3% 4% 5% 1% 2% 3% 4% 5% 1% 2% 3% 4% 5%
5k
10k
25k

1% 2% 3% 4% 5% 1% 2% 3% 4% 5% 1% 2% 3% 4% 5%
5k
10k
25k

(b) Avg, Min F-Measure (Metric 1) for HOSPITAL (c) Max F-Measure (Metric 0.5) for HOSPITAL
2000

CUSTOMERS

7500

METRIC 1

0.7

1500

0.6

1000

HOSPITAL
HOSPITAL

5000
METRIC 0.5

METRIC 0.5
0.5

500

0.4

0

2500

CUSTOMERS
CUSTOMERS

1% 2% 3% 4% 5% 1% 2% 3% 4% 5% 1% 2% 3% 4% 5%
5k
10k
25k

(d) Max F-Measure for the CUSTOMERS Scenario

00

20
20K

40
40K

60
60K

80
80K

(e) HOSPITAL Execution Times (sec)

100
100K

0
100K
100

250K
250

400K
400

550K
550

700K
700

850K 1000
1M
850

(f) Large Datasets Execution Times (sec)

Figure 4: Experimental results for H OSPITAL and C USTOMERS.
Datasets and Scenarios. We selected two scenarios. (i) The first
one, H OSPITAL, is based on a dataset from US Department of
Health & Human Services (http://www.medicare.gov/hospitalcompare/).
The database contains a single table with 100K tuples and 19 attributes, over which we specified 9 functional dependencies. (ii)
The second one, C USTOMERS, corresponds to our running example in Figure 1. The database schema contains 3 tables with 16
attributes, plus 2 additional tables encoding constants in CFDs. Dependencies are the ones in Section 1. We synthetically generated up
to 1M tuples for the 2 target relations, with a proportion of 40% in
the C USTOMERS table, and 60% in T REATMENTS; the master-data
table contains 20% of the tuples present in C USTOMERS. We consider master-data tuples outside the total, as they cannot be modified. For this scenario, we defined the partial order as discussed in
Section 5.
It is worth noting that these scenarios somehow represent opposite extremes of the spectrum of data-repairing problems. In fact,
the H OSPITAL scenario contains functional dependencies only, and
therefore is quite standard in terms of constraints; however, it can
be considered a worst-case in terms of scalability, since all data are
stored as a single, non-normalized table, with many attributes and
lots of redundancy; over this single table, the 9 dependencies interact in various ways, and there is no partial-order information that
can be used to ameliorate the cleaning process.
On the contrary, the C USTOMERS scenario contains a complex
mix of dependencies; this increased complexity of the constraints
is compensated by the fact that data is stored as two normalized tables, with no redundancy, and clear preference strategies are given
for some of the attributes.

from Mechanical Turk (MT) (https://www.mturk.com/mturk/) to perform data entry for a random sample of tuples from the original
database. Workers were shown the original tuple under the form
of a jpeg image, and needed to manually copy values into a form.
We used different groups of workers with different approval rates;
approval rates measure the quality of a worker in terms of the percentage of previous jobs positively evaluated within MT. Approval
rates varied between 50% and 99%; for these, we observed a percentage of wrong values between 5% and 1%. These errors were
then complemented with those generated by the random noise generator.
For both datasets, we generated dirty copies with a number of
noisy cells ranging from 1% to 5% of the total. Changes to the
original values were done only for attributes involved in dependencies, in order to maximize the probability of generating detectable
violations.
Algorithms. We tested L LUNATIC with several cost managers chosen among those presented in Section 8. We chose variants of
the L LUNATIC -FR- S N cost manager – the frequency cost-manager
that generates up to N solutions – with N = 1, 10, 50, and the
L LUNATIC -FR- S 1-FO, the forward-only variant of L LUNATIC -FRS 1. We do not report results obtained by the standard cost manager,
as it only can be used with small instances due to its high computing times.
In order to compare our system to previous approaches, we tested
also the following FD repair algorithms from the literature, implemented as separate systems: (a) Mimimum Cost [8] (M IN . C OST);
(b) Vertex Cover [23] (V ERTEX C OVER); (c) Repair Sampling [7]
(S AMPLING), for which, for each experiment, we took 500 samples,
as done in the original paper.
Notice that these systems support a smaller class of constraints
wrt to the ones expressible with cleaning egds (essentially FDs
and, in some cases, CFDs). Several of the constraints in the C US TOMERS scenario are outside of this class, and therefore cannot

Errors. In order to test our algorithms with different levels of
noise, we introduced errors in the two datasets. Part of these errors were generated by a random-noise generator. However, in order to be as close as possible to real scenarios, in the H OSPITAL
dataset we also used a different source of noise. We asked workers

634

handled by these algorithms. We therefore performed the comparison on the H OSPITAL scenario only.

However, this algorithm picks up repairs in a random way. On the
contrary, L LUNATIC’s chase algorithm explores the space of solutions is a more systematic way, and this explains its improvements
in quality.
Figures 4.(d) reports results for the C USTOMERS scenario. Recall that L LUNATIC is the first system that is able to handle such
kind of scenarios with complex constraints. We notice that quality
results are better than those on H OSPITAL; this is a consequence of
the clear user-specified preference rules.

Quality Metrics. We used precision-recall metrics. More specifically, for each clean database, we generated the set Cp of perturbated cells. Then, we run each algorithm to generate a set of repaired cells, Cr , and computed precision (P ), recall (R), and Fmeasure (F = 2 × (P × R)/(P + R)) of Cr wrt Cp . Since several
of the algorithms may introduce variables as repairs – like our lluns
– we calculated two different metrics.
The first one is the one adopted in [7], which we call Metric 0.5:
(i) for each cell c ∈ Cr repaired to the original value in Cp , the
score was 1; (ii) for each cell c ∈ Cr changed into a value different
from the one in Cp , the score was 0; (iii) for each cell c ∈ Cr
repaired to a variable value, if the cell was also in Cp , the score
was 0.5. In essence, a llun or a variable is counted as a partially
correct change. This gives an estimate of precision and recall when
variables are considered as a partial match.
Since our scenarios may require a consistent number of variables, due to the need for backward repairs, and this metric disfavors variables, we also adopt a different metric, which counts all
correctly identified cells. In this metric, called Metric 1.0, item
(iii) above becomes: for each cell c ∈ Cr repaired to a variable
value, if the cell was also in Cp , the score was 1.
Whenever an algorithm returned more than one repair for a database, we calculated P, R, and F for each repair; in the graphs, we
report the maximum, minimum, and average values.

Scalability The trade-offs between quality and scalability are shown
in Figures 4.(e) and 4.(f). Figure 4.(e) compares execution times
for the various algorithms on the H OSPITAL scenario up to 100K
tuples, with 1% perturbation. Recall that L LUNATIC is the first
DBMS-based implementation of a data repairing algorithm. Therefore, our implementation is somehow disfavored in this comparison. To see this, consider that, when producing repairs, mainmemory algorithms may aggressively use hash-based data structures to speed-up the computation of repairs, at the cost of using
more memory. Using the DBMS, our algorithm is constrained to
use SQL for accessing and repairing data; to see how this changes
the cost of a repair, consider that even updating a single cell (a very
quick operation when performed in main memory) when using the
DBMS requires to perform an UPDATE, and therefore a SELECT
to locate the right tuple.
Nevertheless, the L LUNATIC -FR- S 1 cost manager scales nicely
and had better performances than some of the main memory implementations. We may therefore say that graphs (c) and (e) in
Figure 4 give us a concrete perception of the trade-offs between
complexity and accuracy, and allow us to say that the L LUNATIC FR-S1 is the best compromise for the H OSPITAL scenario. Other
algorithms do not allow to fine tune this trade-off. To see an example, consider the R EP. S AMPLING algorithm: we noticed that
taking 1000 samples instead of 500 doubles execution times, but it
does not produce significant improvements in quality.
Figure 4.(f) clearly shows the benefits that come with a DBMS
implementation wrt main-memory ones, namely the possibility of
scaling up to very large databases. While previous works [8, 7]
have reported results up to a few thousand tuples, we were able to
investigate the performance of the system on databases of millions
of tuples. The figure shows that L LUNATIC scales in both scenarios to large databases. For the H OSPITAL scenario we replicated
the original dataset ten times with 1% errors. In these cases, execution times in the order of the hours for millions of tuples can be
considered as a remarkable result, since no system had been able
to achieve them before on problems of such exponential complexity. It is interesting to note that performances were significantly
better on the C USTOMERS scenario. This is not surprising: as we
discussed above, the C USTOMERS database contains non redundant, normalize tables. In fact, this clearly shows the benefit of a
constraint language that allows to express inter-table cleaning constraints.
It is also worth noting that storing chase trees as delta databases
is crucial in order to achieve such a level of scalability. Without
such a representation system times would be orders of magnitude
higher.

Quality Figure 4 shows quality and scalability results. We start by
showing that L LUNATIC produces repairs of significantly higher
quality with respect to those produced by previous algorithms. We
ran L LUNATIC with the cost managers listed above, and the three
competing algorithms on samples of the H OSPITAL dataset with increasing size (5k to 25k tuples) and increasing percentage of errors
(1% to 5%). We do not report values for the L LUNATIC -FR- S 50
cost manager, since they differ for less than one percentage point
from those of L LUNATIC -FR- S 10.
The maximum F-measure for Metric 1 is in Figure 4.(a); for the
two algorithms that return more than one solution, the minimum
and average F-measures are reported in Figure 4.(b). The maximum F-measure for Metric 0.5 is in Figure 4.(c). Quality results
for algorithms M IN . C OST, V ERTEX C OVER, and R EP. S AMPLING are
consistent with those reported in [7], which also conducted a comparison of these three algorithms on scenarios in which left and
right-hand-side repairs were necessary.
It is not surprising that the F-measure in these cases is quite low.
Consider, in fact, a relation R(A, B) with FD A → B and a tuple
R(a, 1); suppose the first cell is changed to introduce an error, so
that the tuple becomes R(x, 1). There are many cases in which
this error is not fixed by repairing algorithms. This happens, in
fact, whenever the new tuple, R(x, 1), does not get involved in
any conflict, and therefore the error goes undetected. In addition,
even if a violation is raised, an algorithm may choose to repair the
right-hand side of the dependency, thus missing the correct repair.
Finally, even when a left-hand-side repair is correctly identified,
algorithms have no clue about the right value for the A attribute,
and may do little more than introducing a variable – a llun in our
case – to fix the violation. All of these cases contribute to lower
precision and recall.
The superior quality achieved by L LUNATIC variants can be explained by first noticing that algorithms capable of repairing both
right and left-hand sides of dependencies obtained better results
than those that only perform forward repairs. Besides L LUNATIC,
the only other algorithm capable of backward repairs is S AMPLING.

10.

RELATED WORK

Several classes of constraints have been proposed to characterize and improve the quality of data (see [13, 15] for surveys). Most
relevant to this paper are the (semi-)automated repairing algorithms
for these constraints [7, 8, 10, 18, 19, 23]. These methods differ in
the constraints that they admit, e.g., FDs [7, 8], CFDs [10, 23], in-

635

clusion dependencies [8], and editing rules [18], and the underlying
techniques used to improve their effectiveness and efficiency, e.g.,
statistical inference [10], measures of the reliability of the data [8,
18], and user interaction [10, 25].
All of these methods work for a specific class of constraints only,
with the exception of [19, 9]. These works explore the interaction among different kinds of dependencies, but they do not have a
unified formal semantics with a definition of solution, neither the
generality of our partial order to model preferences.
In industrial settings, most data quality related tasks are executed
with ETL tools (e.g, Talend, and Informatica PowerCenter). These
systems are employed for data transformations and have low-level
modules for specific data quality tasks, such as verification of addresses and phone numbers. More complex operations are also partially available, but lack the support for constraints.
We do allow for forward and backward chasing. Similarly, [10,
23, 7] resolve violations by changing values for attributes in both
the premise and conclusion of constraints. They do, however, only
support a limited class of constraints. Previous works [23, 7] have
used variables in order to repair the left-hand side of dependencies.
With respect to variables, our lluns are a more sophisticated tool.
In our approach, the full power of lluns is achieved in conjunction
with cell-groups: for each llun, the corresponding cell group provides complete provenance data for the llun, both in terms of target
and source cells. Therefore, it represents an ideal support for user
intervention, when the value of the llun must be resolved to some
constant. In fact, lluns and cell-groups can be seen as a novel representation system [22] for solutions, that stands in between of the
naive tables of data exchange, and of the more expressive c-tables,
trying to strike a balance between complexity and expressibility.
An approach similar to ours has been proposed in [6], with respect to a different cleaning problem. The authors concentrate
on scenarios with matching dependencies and matching functions,
where the main goal is to merge together values based on attribute
similarities, and develop a chase-based algorithm. They show that,
under proper assumptions, matching functions provide a partial order over database values, and that the partial order can be lifted
to database instances and repairs. A key component of their approach is the availability of matching functions that are essentially
total, i.e., they are able to merge any two comparable values. In
fact, the problem they deal with can be seen as an instance of the
entity-resolution problem. In this paper, we deal with the different
problem of data-repairing under a large class of data-cleaning constraints, and have a more ambitious goal, i.e., to embed different
forms of value preference into a general semantics for the cleaning
process. Our main intuition is that the notion of a partial order is an
effective way to let users specify value preferences, and to incorporate them into the semantics in a principled way. In order to do this,
we have shown that reasoning on the ordering of values – as in [6]
– or on the ordering of single cells is not enough. On the contrary,
it is necessary to devise a more sophisticated notion of a partial
order for cell-groups, i.e., groups of cells that need to be repaired
together and for which lineage information is maintained. Also, we
do not make strong assumptions about the possibility of resolving
all conflicts among values in the database, and therefore introduce
lluns as a third category of values besides nulls and constants.
A comparison of the features supported by existing methods and
our repairing method is given in Table 1. We believe that this work
makes a concrete step forward towards the goal of developing a
uniform formalism for data cleaning, and may stimulate further research on this subject. With a similar spirit, [11] has developed
a unifying view of previous approaches by abstracting different
classes of constraints with respect to a different problem, that of

query answering over inconsistent data.
Our framework can be seen as an extension of the data exchange
setting [12]. With respect to the standard chase algorithms for egds,
our chase always terminates and never fails, by leveraging the partial order. We are not aware of any extension of the data exchange
setting that allows the introduction of special values (like lluns) to
avoid failing chase computations. In fact, we are currently extending our formalism to accommodate for mapping and cleaning scenarios, in such a way to maintain the results from the data exchange
literature and enlarge them to data repairing.

11.

REFERENCES

[1] S. Abiteboul, R. Hull, and V. Vianu. Foundations of Databases.
Addison-Wesley, 1995.
[2] L. Antova, T. Jansen, C. Koch, and D. Olteanu. Fast and Simple
Relational Processing of Uncertain Data. In ICDE, pages 983–992,
2008.
[3] M. Arenas, L. Bertossi, and J. Chomicki. Consistent Query Answers
in Inconsistent Databases. In PODS, pages 68–79, 1999.
[4] C. Beeri and M. Vardi. A Proof Procedure for Data Dependencies. J.
of the ACM, 31(4):718–741, 1984.
[5] L. Bertossi. Database Repairing and Consistent Query Answering.
Morgan & Claypool, 2011.
[6] L. Bertossi, S. Kolahi, and L. Lakshmanan. Data Cleaning and Query
Answering with Matching Dependencies and Matching Functions. In
ICDT, pages 268–279, 2011.
[7] G. Beskales, I. F. Ilyas, and L. Golab. Sampling the repairs of
functional dependency violations under hard constraints. PVLDB,
3:197–207, 2010.
[8] P. Bohannon, M. Flaster, W. Fan, and R. Rastogi. A cost-based model
and effective heuristic for repairing constraints by value
modification. In SIGMOD, pages 143–154, 2005.
[9] X. Chu, I. F. Ilyas, and P. Papotti. Holistic Data Cleaning: Putting
Violations into Context. In ICDE, 2013.
[10] G. Cong, W. Fan, F. Geerts, X. Jia, and S. Ma. Improving data
quality: Consistency and accuracy. In VLDB, pages 315–326, 2007.
[11] T. Eiter, M. Fink, G. Greco, and D. Lembo. Repair Localization for
Query Answering from Inconsistent Databases. ACM TODS,
33(2):1–51, 2008.
[12] R. Fagin, P. Kolaitis, R. Miller, and L. Popa. Data Exchange:
Semantics and Query Answering. TCS, 336(1):89–124, 2005.
[13] W. Fan. Dependencies Revisited for Improving Data Quality. In
PODS, pages 159–170, 2008.
[14] W. Fan, H. Gao, X. Jia, J. Li, and S. Ma. Dynamic constraints for
record matching. VLDB J., 20(4):495–520, 2011.
[15] W. Fan and F. Geerts. Foundations of Data Quality Management.
Morgan & Claypool, 2012.
[16] W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis. Conditional
Functional Dependencies for Capturing Data Inconsistencies. ACM
TODS, 33, 2008.
[17] W. Fan, F. Geerts, and J. Wijsen. Determining the Currency of Data.
In PODS, pages 71–82, 2011.
[18] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Towards certain fixes with
editing rules and master data. PVLDB, 3(1):173–184, 2010.
[19] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Interaction Between
Record Matching and Data Repairing. In SIGMOD, pages 469–480,
2011.
[20] S. Flesca, F. Furfaro, and F. Parisi. Querying and Repairing
Inconsistent Numerical Databases. TODS, pages 1–77, 2010.
[21] G. Greco, S. Greco, and E. Zumpano. A Logical Framework for
Querying and Repairing Inconsistent Databases. TKDE,
15(6):1389–1408, 2003.
[22] T. Imieliński and W. Lipski. Incomplete Information in Relational
Databases. J. of the ACM, 31(4):761–791, 1984.
[23] S. Kolahi and L. V. S. Lakshmanan. On Approximating Optimum
Repairs for Functional Dependency Violations. In ICDT, 2009.
[24] D. Loshin. Master Data Management. Knowl. Integrity, Inc., 2009.
[25] M. Yakout, A. K. Elmagarmid, J. Neville, M. Ouzzani, and I. F. Ilyas.
Guided data repair. PVLDB, 4(5):279–289, 2011.

636

Benchmarking Data Curation Systems
Patricia C. Arocena∗
University of Toronto
Renée J. Miller∗
University of Toronto

Boris Glavic
Illinois Institute of Technology
Paolo Papotti
Arizona State University

Giansalvatore Mecca
University of Basilicata
Donatello Santoro
University of Basilicata

Abstract
Data curation includes the many tasks needed to ensure data maintains its value over time. Given the
maturity of many data curation tasks, including data transformation and data cleaning, it is surprising
that rigorous empirical evaluations of research ideas are so scarce. In this work, we argue that thorough
evaluation of data curation systems imposes several major obstacles that need to be overcome. First, we
consider the outputs generated by a data curation system (for example, an integrated or cleaned database
or a set of constraints produced by a schema discovery system). To compare the results of different
systems, measures of output quality should be agreed upon by the community and, since such measures
can be quite complex, publicly available implementations of these measures should be developed, shared,
and optimized. Second, we consider the inputs to the data curation system. New techniques are needed
to generate and control the metadata and data that are the input to curation systems. For a thorough
evaluation, it must be possible to control (and systematically vary) input characteristics such as the
number of errors in data cleaning or the complexity of a schema mapping in data transformation. Finally,
we consider benchmarks. Data and metadata generators must support the creation of reasonable goldstandard outputs for different curation tasks and must promote productivity by enabling the creation of
a large number of inputs with little manual effort. In this work, we overview some recent advances in
addressing these important obstacles. We argue that evaluation of curation systems is itself a fascinating
and important research area and challenges the curation community to tackle some of the remaining
open research problems.

1

Introduction

A curated database is a valuable asset that has been created and maintained with a great deal of human effort [13].
The term data curation has been used as an umbrella term to encompass the activities required to maintain and
add value to data over its lifetime, and more specifically the tools and algorithms that attempt to reduce human
curation effort by automating some of these important activities. Some data curation tasks that have received
significant attention in the database literature include data cleaning (identifying and repairing errors in data),
entity resolution (identifying and resolving duplicates in data), data transformation (exchanging or translating
Copyright 2016 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for
advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any
copyrighted component of this work in other works must be obtained from the IEEE.
Bulletin of the IEEE Computer Society Technical Committee on Data Engineering
∗

Supported by NSERC

47

data), data integration (including data federation), data provenance (understanding the origin of data), metadata
or schema discovery, data and metadata profiling (including statistical profiling of data and data usage), and data
archiving. Some of these tasks, such as data profiling [1], are also important to operational data management,
for example, statistical profiles can be used to improve query performance. In this paper, we focus on the use of
these tasks to curate data by improving the value or quality of information. In contrast to basic data management
problems like query or transaction processing, data curation has not benefitted from the availability of commonly
accepted benchmarks which can be used to compare systems, resolve discrepancies, and advance the field. As a
result, evaluation and comparison of systems have relied on a few real data and metadata scenarios (for example,
the Illinois Semantic Integration Archive1 , the Sherlock@UCI2 data cleaning and entity resolution data sets or
the more recent annealling standard [30]). Large scale sharing of real scenarios is simply not feasible due to the
inherent value and proprietary nature of many data resources. And importantly, real scenarios do not provide
control over many of the input characteristics that may impact the performance or efficacy of a system. As a
result, researchers rely on ad hoc scenario generators with knobs to control a few selected data or metadata
characteristics.
At first sight it may be surprising that evaluations of data curation tasks are not up to par with evaluations
of query performance in database management systems. However, as we will explain in the following, the
different nature of data curation problems imposes unique challenges for evaluations which are not faced when
evaluating query performance. From a high-level view, any empirical evaluation of an algorithm roughly follows
this pattern: 1) identify relevant input parameters, reasonable values of these parameters, and output measures;
2) select an input parameter to be varied systematically; 3) vary the selected parameter while keeping all other
parameters fixed; 4) run the system on input generated for each parameter value; and 5) evaluate the system
output using a selected set of output measures. This process is necessarily iterative with the results of one
iteration in the evaluation influencing the parameters considered in the next.
The papers in this special issue of the IEEE Data Engineering Bulletin consider the problem of assessing or
improving data quality, often this may be done in the context of a specific curation task. In contrast, in our work,
we are seeking to understand the principles behind evaluating a specific curation system that seeks to improve
data value or quality in a specific, quantifiable way. We consider the problem of assessing whether a system has
achieved a specific goal of improving data quality or whether it has reduced the human effort needed to achieve
a specific curation goal. To illustrate some of the challenges, in this paper we will concentrate on two specific
data curation tasks: data exchange and constraint-based data repair.

1.1

Evaluating Data Exchange Systems

Data exchange systems take as input a pair of schemas (typically named source and target), an instance of the
source schema, and a mapping specifying a relationship between the schemas [15]. They output a transformation
of the source instance into a target instance that must satisfy the mapping. However, different systems may create
target instances that differ, for example, on how much redundancy they contain [16] and the transformation code
they produce may differ in performance [24].
Consider the simple example schemas in Figure 1 that are related by mappings m1 , m2 and m3 . Assume we
want to compare two data exchange systems E1 and E2 . In this example, system E1 creates a target instance
J1 and system E2 creates a target instance J2 . Both instances satisfy the mapping specification and in fact both
are universal solutions [15]. To evaluate and compare these, we need to select an appropriate output measure.
As with traditional data management problems, output measures may be performance-based, for example, the
response time to perform data exchange. Unlike traditional data management, the accuracy of the system is
also an important output measure. For data exchange, an output measure may compare the quality of two
exchanged instances, as suggested by Alexe et al. [2]. This measure considers one output superior to another
1
2

http://pages.cs.wisc.edu/˜anhai/wisc-si-archive/
http://sherlock.ics.uci.edu/data.html

48

S OURCE I NSTANCE
PStat
t1 :
t2 :
t3 :

Name
Giovinco
Giovinco
Pirlo

Season
2012-13
2014-15
2015-16

Team
Juventus
Toronto
N.Y.City

G
3
23
0

Stdm
t1 :
t2 :

Team
Toronto
N.Y.City

Stadium
BMO Field
Yankee St.

JuveDB
t1 :
t2 :

Name
Giovinco
Pirlo

Season
2012-13
2014-15

M APPINGS
m1 . PStat(name, seas, team, goals) → ∃N Player(name, seas, team, goals), Team(team, N)
m2 . Stdm(team, stdm) → Team(team, stdm)
m3 . JuveDB(name, seas) → ∃N Player(name, seas, ‘Juv.′ , N), Team(‘Juv.′ , ‘Juv.Stadium′ )
S OLUTION J1

Player
t1 :
t2 :
t3 :
t4 :

Name
Giovinco
Giovinco
Pirlo
Pirlo

Season
2012-13
2014-15
2015-16
2014-15

Team
Juventus
Toronto
N.Y.City
Juventus

Goals
3
23
0
N1

Team
t1 :
t2 :
t3 :

Name
Toronto
N.Y.City
Juventus

Stadium
BMO Field
Yankee St.
Juventus St.

S OLUTION J2

Player
t1 :
t2 :
t3 :
t4 :
t5 :

Name
Giovinco
Giovinco
Pirlo
Giovinco
Pirlo

Season
2012-13
2014-15
2015-16
2012-13
2014-15

Team
Juventus
Toronto
N.Y.City
Juventus
Juventus

Goals
3
23
0
N1
N2

Team
t1 :
t2 :
t3 :
t4 :
t5 :
t6 :

Name
Juventus
Toronto
N.Y.City
Toronto
N.Y.City
Juventus

Stadium
N3
N4
N5
BMO Field
Yankee St.
Juventus St.

Figure 1: Example Data Exchange Scenario With Two Data Exchange Solutions
if it contains less redundancy (in this example J1 would be preferred over J2 ). Alternatively, if we know what
the expected output is, then we can measure the difference between an output produced by a system and the
expected one, often referred to as a gold standard. For data exchange, the output measure typically involves
comparing (potentially large) database instances and these measures can be complex [2, 25]. Since a systematic
comparison involves many different input scenarios, we must have efficient (and shared) implementations of
these measures.
Of course, a real comparison of systems E1 and E2 must consider not just a few example scenarios, but
should include sets of input scenarios that differ on specific input parameters. For data exchange, these input
parameters might include the size of the schemas, the size of the mapping, or the complexity of the mappings and
schemas. They could also include parameters that measure metadata quality, such as how normalized a schema
is (for example, a fully normalized schema may guarantee minimal redundancy in the data [22]). Agreement on
what are the right input parameters (for example, how mapping complexity is characterized and controlled) and
what are reasonable values for these parameters, is an important part of benchmarking. In addition to the data
generators that are commonly available for benchmarking query processing, we need metadata generators that
can efficiently generate schemas, schema constraints, and mappings in such a way as to provide control over
chosen input parameters.

1.2

Evaluating Constraint-based Data Repairing

In data cleaning, business rules or constraints are often used to express expectations or rules that data should
satisfy [20]. Data that is inconsistent with a set of constraints can be cleaned (or repaired) by using evidence in
the data. A common example is to select a repair that has the minimal number of changes (or the minimal cost
changes) to be consistent with the constraints. Consider the player relation in Figure 2 and assume we are given
the following constraints:

49

(i) A functional dependency (FD) stating that Name and Season are a key for the table:
d1 : Player : Name, Season → Team, Stadium, Goals.
(ii) And, a second FD stating that Team implies Stadium: d2 : Player : Team → Stadium.
Player
t1 :
t2 :
t3 :
t4 :
t5 :
t6 :

Name
Giovinco
Giovinco
Pirlo
Pirlo
Vidal
Vidal

Season
2012-13
2014-15
2014-15
2015-16
2014-15
2015-16

Team
Juventus
Toronto
Juventus
New York City
Juventus
Bayern

Stadium
BMO Field
BMO Field
Juventus Stadium
Yankee Stadium
Juventus Stadium
Allianz Arena

Goals
3
23
5
0
8
3

Figure 2: Example Dirty Database
The database in Figure 2 is dirty with respect to d2 . A possible repair, that may be created by a data
repair system is to change t1 [Stadium] to the value “Juventus Stadium”. There are, however, many possible
alternative repairs, like, for example, changing t1 [T eam] to the value “Toronto”, or changing both t3 [Stadium]
and t5 [Stadium] to the value “BMO Field”.
To evaluate a data repair algorithm, we must know the expected “gold-standard” output. Then we can use
a simple recall and precision measure to evaluate the accuracy of the system (or a weighted recall-precision
measure where credit is given for selecting a repair that is close to the desired value).
In addition, to evaluate repair systems, we must have a dirty data generator, that is a data generator that can
introduce errors into the data in a systematic way. Again, we must have agreement as to what are the important
input parameters for error generation. Certainly we should be able to control the number of errors, but also the
quality of the errors – what makes an error hard or easy to repair? The generator should be able to introduce
errors in data of different sizes and having different constraints. Ideally, the generator would handle a large class
of constraints as a wide variety of constraint languages have been used in cleaning. Error generation could also
be coupled with constraint generation to generate different number or types of constraints. And again, it must be
possible to generate dirty data efficiently, giving the user (the system evaluator) as much control over the chosen
input parameters as possible.

1.3

Evaluating Data Curation

The two examples of data exchange and data repair illustrate the following requirements for large-scale evaluation of data curation tasks.
(a) Quality of the Outputs. Unlike query evaluation where there is one fixed expected output of a system (the
unique query result) and a small number of performance related measures to be considered (e.g., throughput,
mean latency, or memory consumption), in data curation there may be multiple acceptable outputs for a task
that exhibit different quality. This is complicated by the fact that the quality of an output may be a multifaceted combination of potentially conflicting quality measures. For instance, if a database that violates a set
of constraints should be cleaned by removing tuples causing violations, then two important measures of the
quality of a cleaned database are the number of remaining constraint violations (how clean is the solution)
and the number of tuples that got removed (how much information is lost). Obviously, both measures cannot
be fully optimized simultaneously. Many established quality measures are computationally hard or non-trivial
to implement, such as measures for data exchange quality that require comparing potentially large database
instances on criteria that include how well they preserve source information [2, 25]. A rigorous comparison of
solutions for a data curation task has to take multiple quality measures into account to complement performance

50

metrics.3 When comparing two data curation algorithms we are interested in understanding their performance
as well as the quality of the produced results. For example, if we are faced with the task of cleaning a very large
dataset and have to decide which cleaning algorithm to apply to the problem, then to make an informed decision
we need to not only know what is the quality of solutions produced by the algorithms, but also how well the
performance scales over the input size.
Challenge. The community needs to agree on common measures of output quality to use in evaluating data
curation tasks and share implementations of these measures. When exact computation is infeasible on large
or complex output, research is needed on developing approximate measures that are still effective measures of
output quality.
(b) Input Parameters. As our examples illustrate, the input to data curation tasks can be quite complex. To
evaluate query performance the input parameters that are typically considered are hardware and software of the
machine used for the experiments, size of the dataset, data distribution, and complexity of the queries. Input
parameters for data curation tasks are more diverse and each task may have its own set of relevant parameters.
For example, in data cleaning one may evaluate an algorithm varying the number of errors in the input dataset
and the data size for a fixed schema, while for data exchange the schema size is an important input parameter.
For constraint or schema discovery, the amount of redundancy in the data may be an important parameter [4].
For data profiling, the relative independence (or dependence) of attribute distributions may be important [1].
Furthermore, creating an input that conforms to a specific input parameter setting may be non-trivial for some
parameters. For example, an important input parameter for constraint-based data cleaning is the number of
constraint violations in the input dataset. However, introducing a given number of errors into a clean database is
known to be a computationally hard problem [6].
Challenge. The community must agree on the important input parameters (which may include parameters that
vary the quality of the input) for different curation tasks. We must develop and share data and metadata generators that are able to efficiently generate input scenarios for data curation systems. Furthermore, these generators
must be able to systematically vary the value of input parameters independently. Providing this fine-grained
control of input characteristics while still efficiently generating large numbers of input scenarios is an important
research challenge.
(c) Benchmarks. It is important that researchers be able to rely on common benchmarks to test their systems and
compare results. In the context of data curation, benchmarks need to provide inputs to the system and establish
which measures need to be used to assess the quality of outputs. Notice that some quality measures compare the
quality of a solution against a gold standard, that is, a solution that is considered to be the correct or canonical
solution for an input. Producing such a gold standard for some curation tasks can be highly complicated.
Challenge. We should use data and metadata generators to create new, community-accepted benchmarks for
different curation tasks. In addition to generating varied input scenarios, these benchmarks should include when possible - a gold standard output for each input scenario.
Given these challenges, it is more understandable that the standard of experimental evaluations of data
curation systems is still quite low. Most researchers do not have the time and resources to implement complex
quality metrics and solve the sometimes complex problems that arise when creating realistic inputs that conform
with specific input parameter settings. It is often simply not feasible to spend this amount of effort “just” to
evaluate a single system. A reasonable approach to improve the situation is 1) to make implementations of
community-approved quality metrics publicly available and 2) to develop benchmarking solutions which enable
the generation of synthetic, but realistic, metadata and data that conform to specific input parameters. This
3

To avoid confusion, in this work we use the term performance metric to refer to measures of the runtime performance of an
algorithm (e.g., average runtime or memory consumption) and quality metric to refer to a measure of the quality of a solution produced
by a curation algorithm.

51

Domain
Data Exchange

Constraint-based Data Repair

Discovery of Constraints
or Data Quality Rules

Example Input Parameters
Schema and relation size
Degree of schema normalization
Mapping size and complexity
Amount and type of mapping incompleteness
Data size
No. of errors
“Hardness” of errors
No. of constraints
Redundancy in the data
Data size

Example Output Measures
Preservation of source information
Size of target data
Redundancy in target data
Similarity to gold-standard target
No. of repairs
No. of errors remaining in repair
Similarity of repair to gold standard
Precision and Recall compared
to a gold standard

Figure 3: Exemplary Input Parameters and Output Quality Measures for Data Curation
generation should require little user effort and it should be easy to recreate an input scenario produced by a
benchmarking solution to make evaluations repeatable, e.g., by sharing the small configuration file that was
used as an input for a benchmark generator instead of sharing the potentially large scenario itself.
In this paper, we consider the state-of-the-art in addressing these three evaluation challenges for two data
curation tasks: data exchange and constraint-based data repair. Note that the purpose of this work is not to give a
comprehensive overview of all quality metrics and benchmark generators that have been proposed, but rather to
outline what are the major roadblocks for experimental evaluation of data curation solutions as well as discuss
a few exemplary quality measures (as introduced in IQ Meter and others [25]) and systems that generate input
scenarios for data curation tasks (BART [6] and iBench [5]). Figure 3 shows some exemplary relevant input
parameters that we may want to vary for an evaluation and some meaningful output quality measures.
The remainder of this paper is organized as follows. We discuss output quality measures in Section 2, input
parameters in Section 3, and benchmarking systems in Section 4. In Section 5, we briefly describe some new
evaluations of curation systems that use these benchmarking systems. We conclude and discuss future work in
Section 5.

2

Quality of the Outputs

In many fields, there are widely accepted quality measures that allow for the evaluation of solutions. Some,
like precision and recall are pervasive and easy to compute, but require the existence of a gold standard output.
Others, including metrics for clustering quality [26] that may be used to evaluate curation tasks like schema
discovery [4], are computationally expensive. For data curation, the study of quality metrics is in its infancy.
A (non-comprehensive) set of notable exceptions include metrics for evaluating matchings [8] and recent approaches for measuring the quality of data exchange solutions [2, 25]. Given that data curation tools often try
to reduce human curation effort, some of these output measures try to quantify how well a tool succeeds in
automating curation [3, 23, 25]. An important role of benchmarking is to enable the sharing of a standard suite
of performance and quality metrics. For many curation tasks, these metrics may be computationally expensive
and non-trivial to implement. In this section, we delve deeper into output measures for data exchange and data
repair focusing on a few exemplary measures that illustrate the complexity of measuring the quality of a data
curation system’s output.

52

2.1

Data Repair Accuracy Measures

Let us start by discussing accuracy measures for data repairing. Here, there is a natural measure in terms of
(weighted) precision and recall. To see this, consider our example in Figure 2. Assume a repair algorithm is
executed over the dirty database, that we denote by Id . The algorithm will apply a number of changes to attribute
values, which we will call cell changes, of the form t1 [Stadium] := “JuventusStadium′′ , to Id in order to
obtain a repaired instance, denoted by Irep . We call Chrep the set of cell updates applied by the system to repair
the database.
There are multiple – in fact, exponentially many – repairs that can be generated using this strategy. Typically,
a notion of minimality of the repairs [10] has been used to discriminate among them (where minimality applies
to the set of cell changes that create a repair). However, there are usually many repairs that minimally change
the database, not necessarily all of the same quality.
Many researchers have in fact adopted a different approach to their evaluations. We can assume that the
dirty database, Id , has been generated starting from a clean database, I , and by injecting errors for the purpose
of the evaluation. We know, therefore, the gold standard for this experiment, i.e., the original, clean version of
the database. We also know the set of changes Chd that are needed to restore the dirty instance Id to its original
clean version. Finally, since the only modification primitive is that of cell updates, the three instances I , Id and
Irep all have the same set of tuple ids.
In this framework, a natural strategy to measure the performance of the algorithm over this database instance
is to count the differences between the repaired instance and the correct one. In fact, the quality of the repair can
be defined as the F-Measure of the set Chrep , measured with respect to Chd . That is, compute the precision and
recall of the algorithm in restoring the dirty instance to its clean state. The higher the F-measure, the closer Irep
is to the original clean instance I . An F-measure of 1 indicates that the algorithm has changed the instance to
its clean state by fixing all errors within the database, i.e., Irep = I.
Notice that many data repairing algorithms not only use constant values to repair the database, but also
variables [25]. A cell may be assigned a variable when the data-repairing algorithm has detected that the cell is
dirty, but it cannot establish the original constant value. Data-repairing algorithms have used different metrics
to measure the quality of repairs with constants and variables. Precision and recall may be computed using the
following measures:
(i) Value: count the number of cells that have been restored to their original values;
(ii) Cell-Var: in addition to cells restored to their original values, count (with, for example, 0.5 score) the cells
that have been correctly identified as erroneous and marked with a variable (in this case, changing a dirty cell to
a variable counts 0.5);
(iii) Cell: count with a score of 1 all of the cells that have been identified as erroneous, both those that have
been restored to their original value, and those that have been marked with a variable (in this case, changing a
dirty cell to a variable counts 1).

2.2

Quality Measures For Data Exchange

The notion of quality for data exchange systems is more elaborate and there are many proposals in the literature
for comparing the output of two data exchange algorithms [12, 2, 25].
IQ Meter [25] proposed two main building blocks for evaluating different data exchange systems: a measure of the quality of the solution with respect to a gold-standard solution, and a measure of the user-effort in
designing the transformation with a given tool. Both measures were designed for a nested-relational data model
for the source and target databases, capable of handling both relational data and XML trees.
Measuring Output Quality with IQ-Meter. Given a data exchange scenario, this measure assumes that a

53

gold standard has been given in terms of the output instance expected from the data exchange. An algorithm
then determines the similarity of the output instance of a transformation tool with respect to this expected
instance. Given the nested-relational data model, instances can be compared with traditional metrics such as
tree or graph edit distance, but none of these can be used with large datasets because of their high complexity.
Moreover, typical tree and graph-comparison techniques would not work in this setting. It is common in data
transformations to generate synthetic values in the output – called labeled nulls in data-exchange and surrogate
keys in Extract-Transform-Load (ETL) systems. These are placeholders used to join tuples, and their actual
values do not have any business meaning. Therefore the measure needs to check if two instances are identical
up to the renaming of their synthetic values. We may say that we are looking for a technique to check tree or
graph isomorphisms, rather than actual similarities. Unfortunately, techniques for tree and graph isomorphisms
are extremely expensive over the size of the instances.
A more efficient quality measure relies on the following key-idea: the instances to be compared are not
arbitrary trees, but rather the result of data exchange. Since they are instances of a fixed known schema, this
means that we know in advance how tuples are structured, how they should be nested into one another; and in
which ways they join via key-foreign key relationships.
The quality measure abstracts these features in a set-oriented fashion, and then compares them by using
precision, recall, and ultimately F-measures to derive the overall similarity. More specifically, for each instance,
it computes: (i) a set of tuple identifiers, also called local identifiers, one for each tuple in the instance; (ii)
a set of nested tuple identifiers, called global identifiers, which capture the nesting relationships among tuples;
(iii) a set of pairs of tuple identifiers, called join pairs, one for each tuple t1 that joins a tuple t2 via a foreign
key. It then compares the respective sets to compute precision, recall, and the overall F-Measure that gives the
level of similarity. In addition to the measure of similarity, this metric provides feedback about errors in terms
of missing and extra tuples in the generated output.
Other measures of accuracy for the data exchange setting have also been proposed [2, 12]. In particular, two
similarity measures have been used to quantify the preservation of data associations from a source database to a
target database [2]. Notice that in these approaches a gold-standard target instance is not provided for evaluation,
therefore the authors focus their effort in measuring similarity between the original dataset and the result of the
transformation. The first measure materializes data associations (joins of two or more relations) using the given
referential constraints such as foreign keys or constraints logically implied by these constraints. Once all the
associations are computed on the source and on the target, the similarity can be measured. Despite the different
use case, this measure is reminiscent of the features discussed above in the IQ-Meter set-oriented measure.
However, the second type of association in [2] pushes the idea further by considering all the natural joins that
exist among the tuples of two relations. This measure, inspired by the notion of full disjunction, captures more
associations than the one based only on the given constraints, and ultimately leads to a more precise measure of
the similarity of two instances.
The IQ-Meter User Effort Measure. There are several possible ways to estimate user effort [3, 23, 25]. Basic
techniques rely on the time needed to completely specify a correct transformation, or on the number of user
interactions, such as clicks in the GUI [3]. A more sophisticated measure computes the complexity of the
mapping specification provided through a data transformation tool GUI [25]. The IQ-Meter measure models the
specification as an input-graph with labeled nodes and labeled edges. Every element in the schemas is a node
in the graph. Arrows among elements are edges among nodes in the graph. If the tool provides a library of
graphical elements – for example to introduce system functions – these are modeled as additional nodes. Extra
information entered by the user (e.g., manually typed text) is represented as labels over nodes and edges. The
measure evaluates the size of such graphs by encoding their elements according to a minimum description length
technique, and then by counting the size in bits of such description. Experience shows that this model is general
enough to cover every data transformation, spanning from schema mapping transformations to ETL ones, and
provides more accurate results with respect to previous metrics based on point-and-click counts.
54

Figure 4: IQ Comparison of Three Transformation Systems over Four Scenarios
The most prominent feature of the two measures is that they enable us to plot quality-effort graphs to compare
different systems over the same transformation scenario, as shown in Figure 4. Each plot shows how the quality
achieved by a data transformation system varies with different levels of efforts. Intuitively, the size of the area
below the curve in the plot for a given system is proportional to the amount of effort that is required to achieve
high quality outputs with this system. Higher quality with lesser effort means higher effectiveness for the given
task, and ultimately “more intelligence”.

3

Input Parameters

In data curation, often the goal is to ensure the curated output is of higher quality than the input data (and/or
metadata). Hence, in evaluating a system, it is important to understand what are the important characteristics
of this input data or metadata that can influence the performance of the system (as judged by both traditional
performance metrics and also by data or metadata quality metrics). In this section, we review recent work
on identifying important input parameters. In the next section, we consider how modern data and metadata
generators provide flexible control over the values of some of these parameters so each can be used as an
independent variable in system evaluations.

3.1

Input Parameters for Data Exchange

A data exchange scenario is a source schema (optionally with a set of constraints), an instance of the source
schema, a target schema (optionally with constraints), and a mapping from the source to the target. Important
input parameters include the size of the metadata, the size of the source data, and also the size and complexity
of the mapping. We detail below how the complexity or quality of the input metadata has been characterized.
In contrast to data repair, where the characteristics of the data to be cleaned play a major role, in data exchange,
the characteristics of the metadata typically are the deciding factor for a system’s performance.
Metadata Parameters. An important input characteristic for data exchange and mapping operators like mapping composition or mapping adaptation is the relationship between the source and target schema. In evaluating
a mapping composition system, Bernstein et al. [9] use target schemas that were created from a source using
a set of schema evolution primitives (for example, an add-attribute primitive). Yu and Popa [31] used a
similar set of primitives to evaluate a mapping adaptation system. STBenchmark [3] generalized this idea to use
a set of mapping primitives. Each primitive describes a specific relationship between a source and target schema
(for example, vertically partitioning one relation into two fragments).
Two additional mapping quality dimensions identified in the iBench system are (1) the degree to which
mappings reuse or share source and target relations (metadata sharing) and (2) the amount of incompleteness
in a mapping. Metadata sharing directly influences how intertwined the produced mappings are. This in turn
determines the degree to which data from the source must be merged in the target (if target metadata is shared)
or the data from the source has to be copied to different places in the target (if source metadata is shared). As
metadata sharing is increased, some data exchange systems can produce redundant target data or data with too
55

much incompleteness (labelled nulls) thereby decreasing the accuracy of these methods compared to a goldstandard output or decreasing the quality of the output when using an output measure based on the amount of
redundancy and incompleteness in the target instance.
The number and complexity of mappings is another input parameter that can influence the performance of a
data exchange or mapping system (such as a mapping inversion system or mapping composition system). The
complexity of a mapping includes the language of the mapping which may be global-as-view (GAV), localas-view (LAV), source-to-target (ST) tuple-generating-dependencies (TGDS), full ST TGDS, or other mapping
languages [29]. In addition, the number of joins (in the source or target) used in a mapping may influence the
performance of a data exchange system. For mapping languages that permit the modeling of incompleteness
(that is, existentials in the target expression), the amount of incompleteness is also an important parameter that
may influence system performance.
In addition to mapping characteristics, schema characteristics can influence system performance. These
include the number and type of constraints in the schemas (for example, keys only vs. general FDs). By changing
the constraints, a user can control whether a schema is normalized. The number and type of constraints may
influence the performance or quality of the output of a data curation system. For example, the amount (and type)
of target equality-generating-constraints (such as keys) may influence how hard it is for a data exchange system
to create a consistent target instance.

3.2

Input Parameters for Data Repair

In constraint-based cleaning, data dependencies are used to detect data quality problems [20]. In quantitative
data cleaning, distributional models are used and values that deviate from a distribution (outliers) are considered
to be errors [19]. Data cleaning, or repair, is typically done by minimizing the number or cost of changes
needed to create a consistent database [11, 21], or by finding a repair whose distribution is statistically close
to the original data [28]. There are two main aspects that must be taken into consideration when evaluating
constraint-based data repairing systems: the role of constraints and the role of the data.
Constraint Parameters. Different repair algorithms have been developed for different fragments of first-order
logic. While the most popular are FDs and conditional functional dependencies (CFDs), lately there have been
some proposals to handle also denial constraints [14]. Of course, constraint languages differ in expressive power
which leads to different sets of rules and ultimately to different repairs.
Another parameter to consider is the number of constraints, or rules, in a cleaning scenario. A larger number
of constraints usually leads to a better repair, as more external information, expressed in the rules, is enforced
over the dirty data. However, a larger number of constraints also leads to a higher execution time in the detection
and repair process.
Data Parameters. While the role of constraints is quite evident for data cleaning, a more subtle but equally
important role is played by the features of the data, and especially of the errors. Recently, BART4 [6] identified
two important properties of errors: detectability and repairability.
When evaluating a constraint-based repair algorithm, we want to make sure that the dirty input database
used in the evaluation only contains errors that are detectable by the system in question. After all, an error that
cannot be detected, cannot be repaired. To reason about detectability, we need a notion for determining whether
a cell change is involved in a constraint violation. This notion assumes the existence of a clean gold standard
database and a cell change is assumed to describe a difference between the gold standard and the dirty database.
Consider our database in Figure 2. Assume now that the dirty cell (in bold) has been restored to its original,
clean value (“Juventus Stadium”), i.e., we have a clean database and want to introduce errors in it. To start,
4

Bart: Benchmarking Algorithms for data Repairing and Translation

56

consider the following cell change: ch1 = ⟨t1 .Season := 2012-13⟩ that updates tuple t1 as follows:
Player
t1 :

Name
Giovinco

Season
2012-13

Team
Juventus

Stadium
Juventus Stadium

Goals
3

This change does not introduce a violation to any of the constraints in our example. Therefore, any datarepairing tool that relies on the constraints to detect dirtiness in the database will not be able to identify it. We
call this an undetectable change.
When we introduce errors into clean data for the purpose of benchmarking, it is important to control the
number and the behavior of the errors, but it is hard to control the exact number of errors that are guaranteed
to be detectable using a given set of constraints. In fact, this requirement turns the complexity of the errorgeneration process into an NP-complete problem [6].
Once an error has been detected, the second step in the cleaning process is to repair it. Of course, some
errors are easier to repair than other. Back to our example, a change that indeed introduces a detectable error is
the following: ch2 = ⟨t1 .Season := 2014-15⟩. After this update, tuples t1 and t2 violate FD d1 , which states
that Name and Season are a key for the table:
Player
t1 :
t2 :

Name
Giovinco
Giovinco

Season
2014-15
2014-15

Team
Juventus
Toronto

Stadium
Juventus Stadium
BMO Field

Goals
3
23

This change is easily detected using the constraints. Still, it is quite difficult for an automatic data-repairing
algorithm to restore the database to its clean state. Notice, in fact, that after this change, the original value
2013-14 has been removed from the active domain of the dirty database. There is no evidence in the dataset
to guide an algorithm to guess the correct value for a repair. Therefore, a correct repair cannot be found by any
repair algorithm that uses the values in the database as the candidates for repair.
BART uses the notion of repairability of an error to characterize this aspect. In the case above, it would
assign repairability 0 to change ch2 . Different detectable changes may have quite different repairability values.
As an example, consider now change ch3 = ⟨t1 .Stadium := Delle Alpi⟩. The change is detectable using FD d2 .
The redundancy in the example dirty database may be used to repair it:
Player
t1 :
t3 :
t5 :

Name
Giovinco
Pirlo
Vidal

Season
2013-14
2014-15
2014-15

Team
Juventus
Juventus
Juventus

Stadium
Delle Alpi
Juventus Stadium
Juventus Stadium

Goals
3
5
8

The new dirty tuple t1 is involved in two violations of d2 , one with t3 , another with t5 . In both cases, the
new stadium value Delle Alpi is in conflict with value Juventus Stadium. By a straightforward probabilistic
argument, BART would calculate a 2/3 repairability for this error, and rank it as a medium-repairability error.
In other cases, errors may have higher repairability, even 1 in some cases. Consider, for example, the case
in which an additional CFD states that every person with age 40 is named Anne. Since this knowledge is part
of the constraint, any tool would easily restore a dirty age value (̸= 40) for a person named Anne to its gold
standard, clean state.

4

Benchmarking

We consider how modern data and metadata generators provide flexible control over the values of some of the
input parameters we have identified so each can be used as an independent variable in system evaluations. We
focus on the BART error generator that can be used to evaluate data repair systems and the iBench metadata
generator that can be used to evaluate data exchange and mapping operators.

57

4.1

Data Repair: BART

BART is an open-source system that introduces algorithms that guarantee a compromise between control over
the nature of errors and scalability [6]. BART introduces a new computational framework based on violationgeneration queries for finding candidate cells (tuple, attribute pairs) into which detectable errors can be introduced. While these queries can be answered efficiently, determining if detectable errors can be introduced is
computationally hard. To overcome this issue, optimizations for violation-generation queries are introduced.
In particular, extracting tuple samples, along with computing cross-products and joins in main memory, brings
considerable benefits in terms of scalability. Moreover, the authors identify a fragment of denial constraints
(DCs) called symmetric constraints that considerably extend previous fragments for which scalable detection
techniques have been studied. The main benefit of this subclass it that algorithms for detecting and generating
errors with symmetric constraints have significantly better performance than the ones based on joins and allow
the introduction of controllable errors over large datasets.
The Error-Generation Problem. BART permits users to declarative specify how to introduce errors into a
clean dataset for benchmarking purposes. The input of the tool is an error-generation task E, which is composed
of four key elements: (i) a database schema S; (ii) a set Σ of DCs encoding data quality rules over S; (iii) an
instance I of S that is clean with respect to Σ; and (iv) a set of configuration parameters to control the errorgeneration process. These parameters specify, among other things, which relations can be changed, how many
errors should be introduced, and how many of these errors should be detectable. They also let the user control
the degree of repairability of the errors.
Use Cases. BART supports several uses cases. The main one consists of generating a desired degree of detectable errors for each constraint. In addition, users may specify a range of repairability values for each constraint. In this case, BART estimates the repairability of changes, and only generates errors with estimated
repairability within that range. In addition to detectable errors, BART can generate random errors of several
kinds: typos (e.g., ‘databse’), duplicated values, bogus or null values (e.g., ‘999’, ‘***’). Random errors may
be freely mixed with constraint-induced ones. Finally, BART can introduce outliers in numerical attributes.

4.2

Data Transformation: iBench

iBench [5] is an open-source system that supports the creation of metadata for a variety of integration tasks
including but not limited to data exchange, data integration, and mapping operators. As a metadata generator,
iBench can be used to generate independent schemas with an arbitrary or controlled set of mappings between
them. The user has at his disposal, the ability to control over thirty distinct metadata dimensions. We now
overview how iBench advances the state-of-the-art in benchmarking data-transformation systems.
The Metadata-Generation Problem. Intuitively, iBench takes a metadata-generation task Γ and produces an
integration scenario that fulfills the task. Here an integration scenario is a tuple M = (S, T, ΣS , ΣT , Σ, I, J, T ),
where S and T are schemas, ΣS and ΣT are source and target constraints, Σ is a mapping between S and T,
I is an instance of S satisfying ΣS , J is an instance of T satisfying ΣT , and T is a program that implements
the mapping Σ. A user writes a metadata-generation task (or configuration) Γ by specifying minimum and
maximum values for a set Π of input parameters. Note that iBench users do not need to specify every input
parameter, rather only the ones they want to control.
For example, a user may request an integration scenario with independent schemas of up to five attributes
per relation and with only LAV mappings.5 To do this, he creates a simple metadata-generation task, specifying that the input parameters πSourceRelSize (number of attributes per source relation) and πT argetRelSize
5

Recall that LAV mappings have a single relation atom in the source [29].

58

Targetaaaaa
Homeaa
Teamaaaa
Stdmaaaa
Addressaa
Teamaa
Idaa aaaa
Nameaaa
Playera
Nameaaa
Seasona
Idaa aaaa
Goalsa a

Source .
Stdmaaaa a
Teamaaaa
Stadiumaa
aaaa
PStataaaaaa
Nameaaa
Seasonaa
Teamaaaa
Goalsaaa

Figure 5: A Sample Integration Scenario Output by iBench
(number of attributes per target relation) be between two and five, and that the input parameter πSourceM apSize
(number of source atoms per mapping) be exactly one. We show in Figure 5 an integration scenario fulfilling these requirements. Note that both the source and target relations have up to five attributes. The black
solid lines in the figure represent mapping correspondences (variables that are shared between the source
and the target), and the two LAV mappings being output here are as follows: one takes a source relation
Stdm(Team,Stadium) and copies it to a target relation Home(Team,Stdm,Address), and another
mapping takes a source relation PStat(Name,Season,Team,Goals) and vertically partitions it into two
target relations, Team(Id,Name) and Player(Name,Season,Id,Goals).
iBench supports two kinds of input parameters, scenario parameters that help control the characteristics of
integration scenarios to generate arbitrary independent metadata, and primitive parameters that help control the
precise relationship between the source and target schemas. As shown in our example above, typical scenario
parameters include controlling the schema size and complexity, the number of mappings and the complexity of
the mapping language (from using ST TGDS to richer second-order TGDS [17] which are useful in evaluating
integration tasks like mapping composition), and the amount and type of constraints per relation. Notice for
example that in Figure 5, the second mapping creates two target keys for Team and Player, and a foreign
key between them. Primitive parameters, on the contrary, act over individual mapping primitives where each
mapping primitive is a parameterized integration scenario encoding a common transformation pattern (e.g.,
vertical or horizontal partitioning). Using primitive parameters, for instance, a user can constrain these scenarios
to use a particular type of join (i.e., Star or Chain), or use a different number of partitions when constructing the
mapping expressions.
Mapping generation, with the level of control provided by iBench, is computationally hard [5], and moreover
there may be metadata tasks for which no solution exists. iBench employs a holistic approach in examining
the input requirements, and may choose to relax some scenario parameters to produce a solution. Suppose a
user requests an integration scenario with source relations with exactly two attributes (scenario parameter) and
requests the use of a vertical-partitioning primitive that partitions a source relation into three fragments (primitive
parameter). This task has conflicting requirements, as in order to create a target mapping expression with three
fragments we need to have a source relation with at least three attributes. In this case, iBench’s best-effort
algorithm chooses to obey the restriction on number of partitions and violate the restriction on source relation
size, that is, in the presence of conflicts, primitive parameters have precedence over scenario parameters. Still
any output generated by iBench is guaranteed to be a correct solution with respect to the relaxed constraints.
Use Cases. iBench supports several use cases. The first, main use case deals with generating integration
scenarios that have independent schemas with random mappings and constraints. iBench can generate arbitrary
59

constraints such as FDs (including keys) and inclusion dependencies (including foreign keys), and the user
can easily specify the percentage of constraints per relation, as well as the size of keys, for example. In the
second use case, a user can request the generation of primitive-based parameterized scenarios. Notice that these
scenarios can be used as a gold standard. Also, using a combination of scenario and primitive input parameters,
a user can easily ask for a mix of independent and primitive-based scenarios. This allows for the creation of
scenarios with some redundancy. By using metadata sharing, the third use case, we can use iBench to create
even more realistic and complex scenarios where the degree of source and target sharing among mappings can be
also controlled. Notice that in practice, most integration scenarios exhibit mappings that reuse source or target
relations. A fourth use case allows users to import real-world integration scenarios (i.e., schema, mappings, data)
into iBench, and systematically vary and scale them along with any other requested metadata. This feature is
crucial for evaluating systems that exploit very specific transformation patterns [5]. The main innovation here
has been to view the characteristics of metadata as independent variables which can be systematically controlled
(via the input parameters of iBench) to generate flexible, diverse metadata in a fast, scalable way.

5

Success Stories

We now discuss some successful evaluations using the iBench and BART systems and the quality measures
introduced in Section 2, focusing specifically on how these approaches enabled these evaluations.

5.1

Measuring Success Rates of Mapping Translation in Data Exchange

Arocena et al. [7] proposed an approach for rewriting second-order TGDS into equivalent first-order mappings
(ST TGDS or nested mappings). Testing whether a second-order TGD is equivalent to an ST TGD or nested
mapping is undecidable [18], hence this approach is correct, but not complete, meaning it may not be able to
rewrite the second-order input mapping even if an equivalent first-order mapping exists. Given this incompleteness result it was important to evaluate the algorithm’s success rate over a broad range of realistic mappings and
compare this rewriting algorithm to alternative approaches (such as an earlier rewriting technique by Nash et
al. [27]). The evaluation goal here was to answer the question: “How often do these algorithms succeed in practice?”. iBench was used to generate schemas, schema constraints (including keys and FDs), and schema mappings expressed as second-order TGDS. The iBench system enabled the systematic (and efficient) generation
of a large number of diverse mappings (over 12 million), in which the degree and complexity of incompleteness
(i.e., the Skolem functions used for modeling value invention) could be controlled.
This comprehensive evaluation would not have been possible without using an efficient metadata generator
like iBench that provides control over both mapping and schema characteristics. A small collection of realworld scenarios would have not been representative enough to show the differences in algorithms. Relying on
a few synthetic scenarios would have not been realistic enough. These experiments depended on the following
specific features of iBench: efficient schema and mapping generation (both in terms of computational resources
and in terms of user effort where the user only needs to set a few configuration parameters), the ability to
generate second-order TGD mappings, support for varying the type and amount of incompleteness in mappings,
and varying the type and amount of schema constraints.

5.2

Evaluating the Quality per Effort Ratio of Data Transformation Systems

The IQ metric for transformation quality and user effort was used to answer the question “how much user effort
is it required with a particular system to reach a certain data-transformation quality?”. In this evaluation [25], the
IQ metric was essential because it enabled two very important results: 1) a level comparison of user effort among
systems that use very diverse means of user interaction to create a data transformation task (e.g., a mappingbased system may have a GUI that focuses on the task of schema matching and mapping generation while the
60

actual data transformation step is automatic once a mapping has been designed, while an ETL tool focuses
on building a workflow out of basic data transformation steps such as surrogate key generation); and 2) a fair
comparison of output quality by comparing the transformed data to a gold standard. An important property of
the IQ quality measure is that it measures the quality of the final output (target instance) instead of the generated
mappings. This makes the measure applicable for comparing data-transformation systems as diverse as ETL
tools and data exchange systems.

5.3

Evaluating the Effect of Repairability on Data Cleaning Quality

BART was used to evaluate how the repairability of a dirty instance affects the quality of the repairs produced
by data-cleaning systems [6]. Though preliminary, this evaluation demonstrated that different algorithms show
different trends with respect to how repairability affects quality. BART was essential to this evaluation because
it enabled the generation of dirty instances with a guaranteed number of errors that are detectable by the constraints, while at the same time controlling how hard these errors are to repair (repairability). BART helped
greatly reduce the amount of effort needed to generate the multitude of dirty versions of a dataset. Creating
several dirty versions of a clean dataset amounted to just changing a few configuration parameters. Importantly,
BART was designed with performance in mind. The actual error-generation process is highly efficient and, thus,
it is feasible to generate many large dirty instances for an experiment within very reasonable time.

6

Conclusion and Future Work

We have discussed some of the important challenges in evaluating data curation tasks where both the performance of a system and the accuracy (or quality) of the curation result must be considered. We have presented
some of the important input parameters (the independent variables in an evaluation) that have been identified,
along with accuracy measures. We motivated the need for data and metadata generators that can efficiently
produce suites of input data or metadata for curation systems conforming to specific settings of the input parameters. We focused our discussion on two curation tasks, data exchange and data repair, and discussed the
state-of-the-art in evaluating these important tasks. Much remains to be done in understanding the research
challenges inherent in evaluating these and other data curation tasks. We feel this is an area that is ripe for
innovation. As data curation lies at the heart of data science, we need evaluation standards and tools that inspire
confidence in our solutions and drive the field forward.

References
[1] Z. Abedjan, L. Golab, and F. Naumann. Profiling Relational Data: A Survey. The VLDB Journal, 24(4):557–581,
2015.
[2] B. Alexe, M. A. Hernández, L. Popa, and W.-C. Tan. MapMerge: Correlating Independent Schema Mappings. The
VLDB Journal, 21(2):191–211, 2012.
[3] B. Alexe, W. Tan, and Y. Velegrakis. Comparing and Evaluating Mapping Systems with STBenchmark. PVLDB,
1(2):1468–1471, 2008.
[4] P. Andritsos, R. J. Miller, and P. Tsaparas. Information-Theoretic Tools for Mining Database Structure from Large
Data Sets. In SIGMOD, pages 731–742, 2004.
[5] P. C. Arocena, B. Glavic, R. Ciucanu, and R. J. Miller. The iBench Integration Metadata Generator. PVLDB,
9(3):108–119, 2015.
[6] P. C. Arocena, B. Glavic, G. Mecca, R. J. Miller, P. Papotti, and D. Santoro. Messing-Up with BART: Error Generation for Evaluating Data Cleaning Algorithms. PVLDB, 9(2):36–47, 2015.

61

[7] P. C. Arocena, B. Glavic, and R. J. Miller. Value Invention in Data Exchange. In SIGMOD, pages 157–168, 2013.
[8] Z. Bellahsene, A. Bonifati, F. Duchateau, and Y. Velegrakis. On Evaluating Schema Matching and Mapping. In
Schema Matching and Mapping, pages 253–291. Springer, 2011.
[9] P. A. Bernstein, T. J. Green, S. Melnik, and A. Nash. Implementing Mapping Composition. The VLDB Journal,
17(2):333–353, 2008.
[10] L. Bertossi. Database Repairing and Consistent Query Answering. Morgan & Claypool, 2011.
[11] P. Bohannon, M. Flaster, W. Fan, and R. Rastogi. A Cost-Based Model and Effective Heuristic for Repairing Constraints by Value Modification. In SIGMOD, pages 143–154, 2005.
[12] A. Bonifati, G. Mecca, A. Pappalardo, S. Raunich, and G. Summa. The Spicy System: Towards a Notion of Mapping
Quality. In SIGMOD Conference, pages 1289–1294, 2008.
[13] P. Buneman, J. Cheney, W.-C. Tan, and S. Vansummeren. Curated Databases. In PODS, pages 1–12, 2008.
[14] X. Chu, I. F. Ilyas, and P. Papotti. Discovering Denial Constraints. PVLDB, 6(13):1498–1509, 2013.
[15] R. Fagin, P. Kolaitis, R. Miller, and L. Popa. Data Exchange: Semantics and Query Answering. TCS, 336(1):89–124,
2005.
[16] R. Fagin, P. Kolaitis, and L. Popa. Data Exchange: Getting to the Core. ACM Transactions on Database Systems,
30(1):174–210, 2005.
[17] R. Fagin, P. Kolaitis, L. Popa, and W. Tan. Composing Schema Mappings: Second-Order Dependencies to the
Rescue. ACM Transactions on Database Systems, 30(4):994–1055, 2005.
[18] I. Feinerer, R. Pichler, E. Sallinger, and V. Savenkov. On the Undecidability of the Equivalence of Second-Order
Tuple Generating Dependencies. Information Systems, 48:113–129, 2015.
[19] J. Hellerstein. Quantitative Data Cleaning for Large Databases. In Technical report, UC Berkeley, Feb 2008.
[20] I. F. Ilyas and X. Chu. Trends in Cleaning Relational Data: Consistency and Deduplication. Foundations and Trends
in Databases, 5(4):281–393, 2015.
[21] S. Kolahi and L. V. S. Lakshmanan. On Approximating Optimum Repairs for Functional Dependency Violations. In
ICDT, pages 53–62, 2009.
[22] S. Kolahi and L. Libkin. An Information-Theoretic Analysis of Worst-Case Redundancy in Database Design. ACM
Transactions on Database Systems, 35(1), 2010.
[23] S. Kruse, P. Papotti, and F. Naumann. Estimating Data Integration and Cleaning Effort. In EDBT, pages 61–72,
2015.
[24] B. Marnette, G. Mecca, and P. Papotti. Scalable Data Exchange with Functional Dependencies. PVLDB, 3(1):105–
116, 2010.
[25] G. Mecca, P. Papotti, S. Raunich, and D. Santoro. What is the IQ of your Data Transformation System? In CIKM,
pages 872–881, 2012.
[26] M. Meila. Comparing Clusterings: an Axiomatic View. In ICML, pages 577–584, 2005.
[27] A. Nash, P. A. Bernstein, and S. Melnik. Composition of Mappings Given by Embedded Dependencies. TODS,
32(1):4, 2007.
[28] N. Prokoshyna, J. Szlichta, F. Chiang, R. J. Miller, and D. Srivastava. Combining Quantitative and Logical Data
Cleaning. PVLDB, 9(4):300–311, 2015.
[29] B. ten Cate and P. G. Kolaitis. Structural Characterizations of Schema-Mapping Languages. In ICDT, pages 63–72,
2009.
[30] T. Vogel, A. Heise, U. Draisbach, D. Lange, and F. Naumann. Reach for Gold: An Annealing Standard to Evaluate
Duplicate Detection results. J. Data and Information Quality, 5(1-2):5:1–5:25, 2014.
[31] C. Yu and L. Popa. Semantic Adaptation of Schema Mappings when Schemas Evolve. In VLDB Conference, pages
1006–1017, 2005.

62

The VLDB Journal (2017) 26:125–150
DOI 10.1007/s00778-016-0441-6

SPECIAL ISSUE PAPER

Fast and scalable inequality joins
Zuhair Khayyat1 · William Lucia2 · Meghna Singh2 · Mourad Ouzzani2 ·
Paolo Papotti3 · Jorge-Arnulfo Quiané-Ruiz2 · Nan Tang2 · Panos Kalnis1

Received: 21 December 2015 / Revised: 3 August 2016 / Accepted: 19 August 2016 / Published online: 7 September 2016
© Springer-Verlag Berlin Heidelberg 2016

Abstract Inequality joins, which is to join relations with
inequality conditions, are used in various applications. Optimizing joins has been the subject of intensive research
ranging from efficient join algorithms such as sort-merge
join, to the use of efficient indices such as B + -tree, R ∗ -tree
and Bitmap. However, inequality joins have received little
attention and queries containing such joins are notably very
slow. In this paper, we introduce fast inequality join algorithms based on sorted arrays and space-efficient bit-arrays.
We further introduce a simple method to estimate the selectivity of inequality joins which is then used to optimize multiple
predicate queries and multi-way joins. Moreover, we study
an incremental inequality join algorithm to handle scenarios

B

Zuhair Khayyat
zuhair.khayyat@kaust.edu.sa
William Lucia
williamlucia.wl@gmail.com
Meghna Singh
mesingh@qf.org.qa
Mourad Ouzzani
mouzzani@qf.org.qa
Paolo Papotti
ppapotti@asu.edu
Jorge-Arnulfo Quiané-Ruiz
jquianeruiz@qf.org.qa
Nan Tang
ntang@qf.org.qa
Panos Kalnis
panos.kalnis@kaust.edu.sa

1

King Abdullah University of Science and Technology,
Thuwal, Saudi Arabia

2

Qatar Computing Research Institute, HBKU, Doha, Qatar

3

Arizona State University, Tempe, AZ, USA

where data keeps changing. We have implemented a centralized version of these algorithms on top of PostgreSQL,
a distributed version on top of Spark SQL, and an existing
data cleaning system, Nadeef. By comparing our algorithms
against well-known optimization techniques for inequality
joins, we show our solution is more scalable and several
orders of magnitude faster.
Keywords Inequality join · PostgreSQL · Spark SQL ·
Selectivity estimation · Incremental

1 Once upon a time . . .
Bob,1 a data analyst working for an international provider
of cloud services, wanted to analyze revenue and utilization
trends from different regions. In particular, he wanted to find
out all those transactions from the west coast that last longer
and produce smaller revenues than any transaction in the east
coast. In other words, he was looking for any customer from
the west coast who rented a virtual machine for more hours
than any customer from the east coast, but who paid less.
Figure 1 illustrates a data instance for both tables. He wrote
the following query for such a task:
Q t : SELECT east.id, west.t_id
FROM east, west
WHERE east.dur < west.time AND east.rev > west.cost;

Bob first ran Q t over 200K transactions on the distributed system storing the data (System-X). Given that the input
dataset is ∼1GB, he expected a minute response time or so.
However, he waited for more than three hours without seeing
1

We motivate the problem with a real-life story. Names and queries
have been changed for confidentiality reasons.

123

126

Z. Khayyat et al.
east
r1
r2
r3

id
100
101
102

west
s1
s2
s3
s4

t id time cost cores
404 100
6
4
498 140
11
2
676
80
10
1
742
90
5
4

dur
140
100
90

rev
9
12
5

cores
2
8
4

Fig. 1 East coast and west coast transactions

any result. He immediately thought that this problem comes
from System-X and killed the query. He then used an opensource DBMS-X to run his query. Although join is by far the
most important and most studied operator in relational algebra [1], Bob had to wait for over two hours until DBMS-X
returned the results. He found that Q t is processed by DBMSX as a Cartesian product followed by a selection predicate,
which is problematic due to the huge number of unnecessary
intermediate results.
In the meantime, Bob heard that a big DBMS vendor was
in town to highlight the power of their recently released distributed DBMS to process big data (DBMS-Y). So he visited
them with a small (few KBs) dataset sample of the tables to
run Q t . Surprisingly, DBMS-Y could not run Q t for even
that small sample! He spent 45 minutes waiting while one of
the DBMS-Y experts was trying to solve the issue. Bob left
the query running and the vendor never contacted him again.
In fact, DBMS-Y is using underneath the same open-source
DBMS-X that Bob tried before. He thus understood that a
simple distribution of the process does not solve his problem.
Afterward, Bob decided to call one of his friends working for
a very famous DBMS vendor. His friend kindly accepted to
try Q t on their DBMS-Z, which is well reputed to deal with
terabytes of data. A couple of days later, his friend came back
to him with several possible ways (physical plans) to run Q t
on DBMS-Z. Nonetheless, all these query plans still had the
quadratic complexity of a Cartesian product with its inherent
inefficiency.
Despite the prevalence of this kind of queries in several
applications, such as temporal and spatial databases, and data
cleaning, no off-the-shelf efficient solutions exist. There have
been countless techniques to optimize the different flavors of
joins in various settings [19]. In the general case of a thetajoin, these techniques are mostly based on two assumptions:
(i) one of the relations is small enough to fit in memory and
(ii) queries contain selection predicates or an equijoin with a
high selectivity, which would reduce the size of the relations
to be fed to the inequality join. The first assumption does not
hold when joining two big relations. The second assumption
is not necessarily true with low selectivity predicates, such as
gender or region, where the obtained relations are still very
large. Furthermore, similar to Q t , there is a large spectrum
of applications where the above two assumptions do not necessarily hold. For example, for data analytics in a temporal

123

database, a typical query would be to find all employees and
managers that overlapped while working in a certain company [18]. In data cleaning, when detecting violations based
on denial constraints, an example of a query is to find all pairs
of tuples such that one individual pays more taxes but earns
less than another individual [10].
Bob then started looking at alternatives. Common ways
of optimizing such queries include sort-merge joins [13] and
interval-based indexing [9,17,22]. Sort-merge join reduces
the search space by sorting the data based on the joining
attributes and merging them. However, it still has a quadratic
complexity for queries with inequality joins only. Intervalbased indexing reduces the search space of such queries
even further by using bitmap interval indexing [9]. However,
such indices require large memory space [36] and long index
building time. Moreover, Bob would have to create multiple
indices to cover all those attributes referenced in his query
workload. Such indices can be built at query time, but their
long construction time renders them impractical.
With no hope in the horizon, Bob decided to talk with his
friends who happen to do research in data analytics. They
happily started working on this interesting problem. After
several months of hard work, they came out with IEJoin,
a new algorithm that utilizes bit-arrays and positional permutation arrays to achieve fast inequality joins. Given the
inherent quadratic complexity of inequality joins, IEJoin follows the RAM locality is King principle coined by Jim Gray.
The use of memory-contiguous data structures with small
footprint leads to orders of magnitude performance improvement over the prior art. The basic idea of our proposal is to
create a sorted array of tuples for each inequality comparison and compute their intersection, which would output the
join results. The prohibitive cost of the intersection operation is alleviated through the use of a permutation array to
encode positions of tuples in one sorted array w.r.t. the other
sorted array (assuming that there are only two conditions). A
bit-array is then used to emit the join results.
This work extends [25] along several lines including
dealing with the not equal operator, improving the bitmap,
optimizing multi-predicate queries and query planning for
multi-way joins through selectivity estimation, and an incremental version of the algorithm.
Contributions
(1) We present novel, fast and space-efficient inequality join
algorithms (Sects. 2 and 3). Furthermore, we discuss
two optimization techniques to significantly speedup the
computation (Sect. 3.3). Specifically, we exploit bitmaps
to reduce the search space and reorganize data to improve
data locality.
(2) We discuss selectivity estimation for optimizing multipredicate queries and query planning for multi-way joins
(Sect. 4).

Fast and scalable inequality joins

(3) We devise incremental inequality join algorithms to deal
with dynamic data (Sect. 5). We show that our proposed
algorithms, leveraging packed-memory array [5], can be
easily adapted to handle data updates.
(4) To handle very large datasets, we present a distributed
version of our algorithm that can be deployed on systems
such as Spark SQL [4]. In particular, we use attribute
metadata (e.g., min and max values) to greatly reduce
data shuffling costs (Sect. 6).
(5) We implement our algorithms in three existing systems,
namely PostgreSQL, Spark SQL and Nadeef (Sect. 7).
We conduct an extensive experimental study by comparing against well-known optimization techniques. The
results show that our proposed solution is more general,
scalable and orders of magnitude faster than the stateof-the-art (Sect. 8).
Furthermore, we discuss related work in Sect. 9 and conclude the paper in Sect. 10.

2 Solution overview
In this section, we restrict our discussions to queries with
inequality predicates only. Each predicate is of the form:
Ai op Bi where Ai (resp., Bi ) is an attribute in a relation R (resp., S) and op is an inequality operator in the set
{<, >, ≤, ≥}.
Example 1 [Single predicate] Consider the west table in
Fig. 1 and an inequality self-join query Q s :
Q s : SELECT s1 .t_id, s2 .t_id
FROM west s1 , west s2
WHERE s1 .time > s2 .time;

Query Q s returns a set of pairs {(si , s j )} where si takes
more time than s j ; the result is {(s2 , s1 ), (s2 , s3 ), (s2 , s4 ),
(s1 , s3 ), (s1 , s4 ), (s4 , s3 )}.
A natural idea to handle inequality join on one attribute is
to leverage a sorted array. For instance, we sort west tuples on
time in ascending order in array L 1 :s3 , s4 , s1 , s2 . We denote
by L[i] the i-th element in array L, and L[i, j] its sub-array
from position i to position j. Given a tuple s, any tuple at
L 1 [k] (k ∈ [1, i −1]) has time value less than L 1 [i], the position of s in L 1 . Consider Example 1, tuple s1 in position L 1 [3]
joins with tuples in positions L 1 [1, 2], namely s3 and s4 .
Example 2 [Two predicates] Let us now consider a self-join
with two inequality conditions:
Q p : SELECT s1 .t_id, s2 .t_id
FROM west s1 , west s2
WHERE s1 .time > s2 .time AND s1 .cost < s2 .cost;

127

Q p returns pairs (si , s j ) where si takes more time but pays
less than s j ; the result is {(s1 , s3 ), (s4 , s3 )}.
Similar to attribute time in Example 1, we also sort
attribute cost in ascending order into an array L 2 :s4 , s1 , s3 ,
s2 , as shown below.
L 1 s3 (80) s4 (90) s1 (100) s2 (140) (sort ↑ on time)

H
L s (5)
2

4

s1 (6)

s3 (10)

s2 (11)

(sort ↑ on cost)

Thus, given a tuple s whose position in L 2 is j, any tuple
L 2 [l] (l ∈ [ j + 1, n]) has a higher cost than s, where n is the
size of the input relation. Our observation here is as follows.
For any tuple s 
 , to form a join result (s, s 
 ) with tuple s,
the following two conditions must be satisfied: (i) s 
 is on
the left of s in L 1 , i.e., s has a larger value for time than
s 
 and (ii) s 
 is on the right of s in L 2 , i.e., s has a smaller
value for cost than s 
 . Thus, all tuples in the intersection of
L 1 [1, i − 1] and L 2 [ j + 1, n] satisfy these two conditions
and belong to the join result. For example, s4 ’s position in
L 1 (resp. L 2 ) is 2 (resp. 1). Hence, L 1 [1, 2 − 1] = s3 
and L 2 [1 + 1, 4] = s1 , s3 , s2 , and their intersection is {s3 },
producing (s4 , s3 ). To obtain the final result, we simply repeat
the above process for each tuple.
The challenge is how to perform the aforementioned intersection operation in an efficient manner. There already exist
several indices, such as R-tree and B + -tree, that can possibly
help. R-tree is ideal for supporting two or higher dimensional
range queries. However, the non-clustered nature of R-trees
makes them inadequate for inequality joins; we cannot avoid
random I/O access when retrieving join results. B + -tree is a
clustered index. The bright side is that for each tuple, only
sequential disk scan is required to retrieve relevant tuples.
However, we need to repeat this n times, where n is the
number of tuples, which is prohibitively expensive. When
confronted with such problems, one common practice is to
use space-efficient and CPU-friendly indices; in this paper,
we employ bit-arrays.
As discussed earlier, the idea to handle an inequality join
on one attribute is to leverage a sorted array, as shown in
Example 1. When two different attributes appear in the join,
in order to leverage a similar idea, a natural solution is to use
a permutation array between two sorted arrays L 1 and L 2 .
Given the i-th element in L 1 , a permutation array can tell
its corresponding position in L 2 in constant time. Moreover,
when visiting items in L 1 , we need to keep track of the items
we have seen so far, for which we use a bit-array to make
such a connection.
Generally speaking, our method, namely IEJoin, sorts
relation west on time and cost, creates a permutation array
for cost w.r.t. time and leverages a bit-array to emit join
results. We will briefly present the algorithm below and defer
detailed discussion to Sect. 3. Figure 2 depicts the process.

123

128

Z. Khayyat et al.
(1) Initialization
L1 s3 (80) s4 (90) s1 (100) s2 (140) (sort ↑ on time)

H s (5)
L
H
2

4

s1 (6)

s3 (10)

s2 (11)

(sort ↑ on cost)

1 2 3 4
P 2 3 1 4 (permutation array) B 0 0 0 0 (bit-array)
s 3 s 4 s1 s2
(2) Visit tuples w.r.t. L2

H· −−→
0 0 0 0
(b) H · −→
B 0 1 0 0
(c) H
· −−−−→
B 0 1 1 0
(d) H
·

(a)
B

B

1 1 1 0

⇒

0 1 0 0

Output:

⇒

0 1 1 0

Output:

⇒

1 1 1 0

Output: (s4 , s3 ), (s1 , s3 )

⇒

1 1 1 1

Output:

In this section, we describe our inequality join algorithms
using permutation arrays and bit-arrays. We start by discussing the case with two relations and operators in {<, >,
≤, ≥}, followed by describing the not equal operator (i.e.,
=) (Sect. 3.1). We then discuss the special case of self-joins
(Sect. 3.2). We close this section by some optimization techniques (Sect. 3.3).
3.1 IEJoin

Fig. 2 IEJoin process for query Q p

S1. [Initialization] Sort both time and cost values in ascending order, as depicted by L 1 and L 2 , respectively. While
sorting, compute a permutation (reordering) array of elements of L 2 in L 1 , as shown by P. For example, the first
element of L 2 (i.e., s4 ) corresponds to position 2 in L 1 . Hence,
P[1] = 2. Initialize a bit-array B with length n and set all
bits to 0, as shown by B with array indices reported above
the cells and corresponding tuples reported below them.
S2. [Visit tuples in the order of L 2 ] Scan the permutation
array P and operate on the bit-array.
(a) Visit P[1]. First, visit tuple s4 (1st element in L 2 ) and
check in P what is the position of s4 in L 1 (i.e., position
2). Then, go to B[2] and scan all bits in positions higher
than 2. As all B[i] = 0 for i > 2, there is no tuple that
satisfies the join condition of Q p w.r.t. s4 . Finish this
visit by setting B[2] = 1, which indicates that tuple s4
has been visited.
(b) Visit P[2]. This is for tuple s1 . It processes s1 in a similar
manner as s4 , without emitting any result.
(c) Visit P[3]. This visit corresponds to tuple s3 . Each
nonzero bit on the right of s3 (highlighted by gray cells)
corresponds to a join result, because each marked cell
corresponds to a tuple that pays less cost (i.e., being visited first) but takes more time (i.e., on the right side of
its position). It thus outputs (s4 , s3 ) and (s1 , s3 ).
(d) Visit P[4]. This visit corresponds to tuple s2 and does
not return any result.
The final result of Q p is the union of all the intermediate
results from the above steps, i.e., {(s4 , s3 ), (s1 , s3 )}.
A few observations make our solution appealing. First,
there are many efficient techniques for sorting large arrays,
e.g., GPUTeraSort [20]. In addition, after getting the permutation array, we only need to sequentially scan it once. Hence,
we can store the permutation array on disk, in case there is
not enough memory. Only the bit-array is required to stay in
memory, to avoid random disk I/Os. Thus, to execute queries
Q s and Q p on 1 billion tuples, theoretically, we only need 1
billion bits (i.e., 125 MB) of memory.

123

3 Centralized algorithms

We present our join algorithm with two inequality predicates
involving two relations. We refer the reader to Sect. 4 on how
we deal with more than two predicates and inequality joins
over multiple relations.
Algorithm The algorithm, IEJoin, is shown in Algorithm 1.
It takes a query Q with two inequality join conditions as input
and returns a set of result pairs. It first sorts the attribute
values to be joined (lines 3–6), computes the permutation
array (lines 7–8) and two offset arrays (lines 9–10). We defer
details of computing the permutation arrays to Sect. 7. Each
element of an offset records the relative position from L 1
(resp. L 2 ) in L 
1 (resp. L 
2 ). The offset array is computed by
a linear scan of both sorted arrays (e.g., L 1 and L 
1 ). The
algorithm also sets up the bit-array (line 11) as well as the

Algorithm 1: IEJoin

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

23

input : query Q with 2 join predicates t1 .X op1 t2 .X 
 and
t1 .Y op2 t2 .Y 
 , tables T, T 
 of sizes m and n resp.
output: a list of tuple pairs (ti , t j )
let L 1 (resp. L 2 ) be the array of X (resp. Y ) in T
let L 
1 (resp. L 
2 ) be the array of X 
 (resp. Y 
 ) in T 

if (op1 ∈ {>, ≥}) sort L 1 , L 
1 in descending order
else if (op1 ∈ {<, ≤}) sort L 1 , L 
1 in ascending order
if (op2 ∈ {>, ≥}) sort L 2 , L 
2 in ascending order
else if (op2 ∈ {<, ≤}) sort L 2 , L 
2 in descending order
compute the permutation array P of L 2 w.r.t. L 1
compute the permutation array P 
 of L 
2 w.r.t. L 
1
compute the offset array O1 of L 1 w.r.t. L 
1
compute the offset array O2 of L 2 w.r.t. L 
2
initialize bit-array B 
 (|B 
 | = n), and set all bits to 0
initialize join_result as an empty list for tuple pairs
if (op1 ∈ {≤, ≥} and op2 ∈ {≤, ≥}) eqOff = 0
else eqOff = 1
for (i ← 1 to m) do
off2 ← O2 [i]
for j ← 1 to min(off2 , size(L 2 )) do
B 
 [P 
 [ j]] ← 1
off1 ← O1 [P[i]]
for (k ← off1 + eqOff to n) do
if B 
 [k] = 1 then
add tuples w.r.t. (L 2 [i], L 
2 [k]) to join_result
return join_result

Fast and scalable inequality joins

129

(1) Initialization

east

west

L1 r3 (90) r2 (100) r1 (140) (sort ↑ on dur)

H
L r (5) r (9)
H1 3 2
P
H2 3 4
O
2

3

1

1

O2 1 3 5

r2 (12)

L1 s3 (80) s4 (90) s1 (100) s2 (140) (sort ↑ on time)

(sort ↑ on rev)

L2 s4 (5)

(permutation array)

P

(oﬀset of L1 w.r.t. L1 )

1 2 3 4
B 0 0 0 0
s 3 s4 s1 s2

(oﬀset of L2 w.r.t. L2 )

(2) Visit tuples w.r.t. L2
(a) visit L2 [1] for r3 : (i) O2 [1] = 1;

(ii) P [1] = 2
B

(b) visit L2 [2] for r1 : (i) O2 [2] = 3;

(c) visit L2 [3] for r2 : (i) O2 [3] = 5;

1 1 1 1

s2 (11)

2 3 1 4

(iii) O1 [P [1]] = 2;

H

(sort ↑ on cost)
(permutation array)

(bit-array)

(iv)

·

−−−→

(v) Output:

B : 0 1 0 0
(iii) O1 [P [2]] = 4;

(iv)

·

(v) Output:

−
→

(v) Output: (r2 , s2 )

B : 1 1 1 0

1 1 1 0

(ii) P [4] = 4
B

s3 (10)

0 1 0 0

(ii) P [2] = 3, P [3] = 1
B

s1 (6)

(iii) O1 [P [3]] = 3;

(iv)

·

B : 1 1 1 1

Fig. 3 IEJoin process for query Q t

result set (line 12). In addition, it sets an offset variable to
distinguish between the inequality operators with or without
equality conditions (lines 13–14). It then visits the values
in L 2 in the appropriate order, which is to sequentially scan
the permutation array from left to right (lines 15–22). For
each tuple visited in L 2 , it first sets all bits for those t in
T 
 whose Y 
 values are smaller than the Y value of the current tuple in T (lines 16–18), i.e., those tuples in T 
 that
satisfy the second join condition. It then uses the other offset array to find those tuples in T 
 that also satisfy the first
join condition (lines 19–22). It finally returns all join results
(line 23).
Example 3 Figure 3 shows how Algorithm 1 processes Q t
(from Sect. 1). In initialization (step 1), L 1 , L 2 , L 
1 and L 
2 are
sorted; P (resp. P 
 ) is the permutation array between L 1 and
L 2 (resp. L 
1 and L 
2 ), where the details of computation are
given in implementation details for PostgreSQL in Sect. 7.1
Fig. 7; O1 (resp. O2 ) is the offset array of L 1 relative to L 
1
(resp. L 2 relative to L 
2 ), e.g., O1 [1] = 2 means that the
relative position of value L 1 [1] = 90 in L 
1 is 2. Clearly, O1
(resp. O2 ) can be computed by sequentially scanning L 1 and
L 1 (resp. L 2 and L 
2 ), and B 
 is the bit-array with all bits
initialized to be 0.
After the initialization, the algorithm starts visiting tuples
w.r.t. L 2 (step(2)). For example, when visiting the first item
in L 2 (r3 ) in step (2)(a), it first finds its relative position in
L 
2 at step (2)(a)(i). Then, it visits all tuples in L 
2 whose cost
values are no larger than r3 [rev] at step (2)(a)(ii). Afterward,
it uses the relative position of r3 [dur] at L 
1 (step (2)(a)(iii))
to populate all join results (step (2)(a)(iv)). The same process
applies to r1 (step (2)(b)) and r2 (step (2)(c)), and the only
result is returned at step (2)(c)(v).
Correctness The algorithm terminates and the results satisfy
the join condition. For any tuple pair (ri , s j ) that should be a
result, s j will be visited first and its corresponding bit is set

to 1 (lines 17–18). Afterward, ri will be visited and the result
(ri , s j ) will be identified (lines 20–22) by the algorithm.
Complexity Sorting arrays and computing their permutation
array together are in O(m · log m + n · log n) time, where
m and n are the sizes of the two input relations (lines 3–8).
Computing the offset arrays will take linear time using sortmerge (lines 9–10). The outer loop will take O(m · n) time
(lines 15–22). Hence, the total time complexity of the algorithm is O(m · log m + n · log n + m · n). It is straightforward
to see that the total space complexity is O(m + n).
Not equal operator In the case of the not equal “=” operator,
we simply rewrite the query as two queries, with the (>)
and (<) operators, respectively. We then merge the results of
these two queries through the UNION ALL SQL command.
Example 4 Consider the following query with one = condition:
Q k : SELECT r.id, s.id
FROM Events r, Events s
WHERE r.start ≤ s.end AND r.end = s.start;

We translate Q k into the union of two queries as follows:
Q 
k : SELECT r.id, s.id
FROM Events r, Events s
WHERE r.start ≤ s.end AND r.end < s.start
UNION ALL
SELECT r.id, s.id
FROM Events r, Events s
WHERE r.start ≤ s.end AND r.end > s.start;

The performance of IEJoin for queries with a “=” operator
strongly depends on the attribute domain itself. Attributes
with smaller ranges will typically lead to higher selectivity,
while larger ranges to lower selectivity. Note that as higher
selective attributes produce fewer results, compared to lower
ones, they have a faster IEJoin runtime.

123

130

Outer joins IEJoin can also support left, right and full outer
joins. For left outer joins, any tuple that does not find any
matches while scanning, the bit-array (lines 20–22 in Algorithm 1) is paired with a null value. The right outer join is
processed by flipping the order of input relations, executing
a normal left outer join and then reversing the order of the
join result. The IEJoin output should return (L 
2 [k], L 2 [i])
or (N U L L , L 2 [i]) instead of the original order (line 22 in
Algorithm 1) to generate the correct result for the right outer
join. The full outer join is translated into one left outer join
that includes the normal IEJoin output, and one right outer
join that only emits results with null values.
3.2 IESelfJoin
In this section, we present the algorithm for self-join queries
with two inequality operators.
Algorithm IESelfJoin (Algorithm 2) takes a self-join
inequality query Q and returns a set of result pairs. The
algorithm first sorts the two lists of attributes to be joined
(lines 2–5), computes the permutation array (line 6), and sets
up the bit-array (line 7) as well as the result set (line 8). It
also sets an offset variable to distinguish inequality operators with or without equality (lines 9–10). It then visits the
values in L 2 in the desired order, i.e., sequentially scan the
permutation array from left to right (lines 11–16). For each
tuple visited in L 2 , it needs to find all tuples whose X values
satisfy the join condition. This is performed by first locating
its corresponding position in L 1 via looking up the permutation array (line 12) and marked in the bit-array (line 13).
Since the bit-array and L 1 have a one-to-one positional cor-

Algorithm 2: IESelfJoin

16

input : query Q with 2 join predicates t1 .X op1 t2 .X and
t1 .Y op2 t2 .Y , table T of size n
output: a list of tuple pairs (ti , t j )
let L 1 (resp. L 2 ) be the array of column X (resp. Y )
if (op1 ∈ {>, ≥}) sort L 1 in ascending order
else if (op1 ∈ {<, ≤}) sort L 1 in descending order
if (op2 ∈ {>, ≥}) sort L 2 in descending order
else if (op2 ∈ {<, ≤}) sort L 2 in ascending order
compute the permutation array P of L 2 w.r.t. L 1
initialize bit-array B (|B| = n), and set all bits to 0
initialize join_result as an empty list for tuple pairs
if (op1 ∈ {≤, ≥} and op2 ∈ {≤, ≥}) eqOff = 0
else eqOff = 1
for (i ← 1 to n) do
pos ← P[i]
B[pos] ← 1
for ( j ← pos + eqOff to n) do
if B[ j] = 1 then
add tuples w.r.t. (L 1 [ j], L 1 [P[i]]) to join_result

17

return join_result

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

123

Z. Khayyat et al.
Table 1 Secondary key sorting order for Y/ X in op1 /op2
op2 sort order
<

>

≤

≥

<

Asc/Des

Des/Des

Asc/Asc

Des/Asc

>

Asc/Asc

Des/Asc

Asc/Des

Des/Des

≤

Des/Des

Asc/Des

Des/Asc

Asc/Asc

≥

Des/Asc

Asc/Asc

Des/Des

Asc/Des

op1 sort
order

respondence, the tuples on the right of pos will satisfy the
join condition on X (lines 14–16), and these tuples will also
satisfy the join condition on Y if they have been visited before
(line 15). Such tuples will be joined with the tuple currently
being visited as results (line 16). It finally returns all join
results (line 17).
Note that the different sorting orders, i.e., ascending or
descending for attribute X and Y in lines 2–5, are chosen to
satisfy various inequality operators. One may observe that
if the database contains duplicated values, when sorting one
attribute X , its corresponding value in attribute Y should be
considered, and vice versa, in order to preserve both orders
for correct join result. Hence, in IESelfJoin, when sorting
X , we use an algorithm that also takes Y as the secondary
key. Specifically, when some X values are equal, their sorting
orders are decided by their Y values (lines 2–3), similarly for
the other way around (lines 4–5). Note that if both attributes
are duplicates, we make sure that the sorting of the attributes
is consistent by relying on the internal record (tuple) ids. In
particular, when the values are equal in both X and Y , we
sort them in increasing order according to their internal id.
Moreover, we show in Table 1 the sorting orders for op1 ’s
secondary key (Y ) and op2 ’s secondary key (X ) when the
dataset contains duplicate values. For example, if op1 ’s condition is (≤) and op2 ’s condition is (>), according to Table 1,
equal values in op1 are sorted based on ascending order
of their Y values, while op2 ’s equal values are sorted on
descending order of their X values. Please refer the example
in Sect. 2 for query Q p using IESelfJoin .
Correctness It is easy to check that the algorithm will terminate and each result in join_result satisfies the join condition.
For completeness, observe the following. For any tuple pair
(t1 , t2 ) that should be in the result, t2 is visited first and its corresponding bit is set to 1 (line 13). Afterward, t1 is visited and
the result (t1 , t2 ) is identified (lines 15–16) by IESelfJoin.
Complexity Sorting two arrays and computing their permutation array is in O(n · log n) time (lines 2–8). Scanning the
permutation array and scanning the bit-array for each visited
tuple run in O(n 2 ) time (lines 11–16). Hence, in total, the
time complexity of IESelfJoin is O(n 2 ). It is easy to see
that the space complexity of IESelfJoin is O(n).

Fast and scalable inequality joins

131

3.3 Enhancements
We discuss two techniques to improve performance: (i)
Indices to improve the lookup performance for the bit-array.
(ii) Union arrays to improve data locality and reduce the data
to be loaded into the cache.
Bitmap index to improve bit-array scan An analysis on both
IEJoin and IESelfJoin shows that for each visited value
(i.e., lines 20–22 in Algorithm 1 and lines 14–16 in Algorithm 2), we need to scan all the bits on the right of the
current position. When the query selectivity is low, this
is unavoidable for producing the correct results. However,
when the query selectivity is high, iteratively scanning a long
sequence of 0’s will be a performance bottleneck. We thus
adopt a bitmap to guide which parts of the bit-array should be
visited.
Given a bit-array B of size n and a predefined chunk size
c, our bitmap is a bit-array with size 
n/c where each bit
corresponds to a chunk in B, with 1 indicating that the chunk
contains at least a 1, and 0 otherwise.
Example 5 Consider the bit-array B in Fig. 4. Assume that
the chunk size c = 4. The bit-array B will be partitioned
into four chunks C1 –C4 . Its bitmap is shown above B in the
figure and consists of 4 bits. We consider two cases.
Case 1: visit B[6], in which case we need to find all the 1’s
in B[i] for i > 6. The bitmap tells that only chunk 2 needs
to be checked, and it is safe to ignore chunks 3 and 4.
Case 2: visit B[9], the bitmap tells that there is no need to
scan B, since there cannot be any B[ j] where B[ j] = 1 and
j > 9.
To further improve the bitmap, we avoid unnecessary
scanning by stopping at the maximum modified value in the
bitmap index. For example, in Fig. 4, we maintain a max scan
index variable (MaxIndex) with value 2 to stop the bitmap
from unnecessarily scanning bitmap indices 3 and 4. The
initial value of this variable is 0, which means that both the
bitmap and the bit-array are empty. As we iteratively modify
the bit-array (line 18 in Algorithm 1 and line 13 in Algorithm 2) and edit the bitmap, we make sure to update the
max scan index variable if the current updated filter index is
larger than the last recorded max filter index.
max

C1
B

0

0

0

1

0

0

1

0

C2
0

0

0

1

0

C4

C3

(i) pos 6

0

0

0

0

0

0

0

Union arrays on join attributes In testing Algorithm 1,
we found that there are several cache loads and stores.
A deeper analysis shows that the extra cache loads and
stores may be caused by cache misses when sequentially
visiting different arrays. Take Fig. 3 for example. In step
(2)(a), we visit arrays L 2 , O2 , P 
 , P and O1 in sequence,
with each causing at least one cache miss. Step (2)(b) and
step (2)(c) show a similar behavior. An intuitive solution
is to merge the arrays on join attributes and sort them
together. Again, consider Fig. 3. We can merge L 1 and
L 
1 into one array and sort them, which will result in
s3 (80), r3 (90), s4 (90), r2 (100), s1 (100), r1 (140), s2 (140).
Similarly, we can merge L 2 and L 
2 , and P and P 
 . Also,
O1 and O2 are not needed in this case, and B 
 needs to be
extended to be aligned with the merged arrays. This solution is similar to IESelfJoin (Sect. 3.2. However, we need
to prune join results for tuples that come from the same table.
This can be easily done using a Boolean flag for each position,
where 0 (resp. 1) denotes that the corresponding value is from
the first (resp. the second) table. Our experiments (Sect. 8.5)
show that this simple union can significantly reduce the number of cache misses, and thus improve execution time.

4 Query optimization
We introduce an approach to estimate the selectivity of
inequality join predicates that is then used to optimize
inequality join queries. In particular, we tackle the cases of
selecting the best join predicate when joining two relations
on more than two join predicates and selecting the best join
order for a multi-way IEJoin. Note that our goal is not to
build a full-fledged query optimizer and implement it in an
existing system, but rather to propose a simple, yet efficient,
approach for optimizing joins with inequality conditions.
4.1 Selectivity estimation
To estimate the selectivity of an inequality join predicate,
we simply count the number of overlapping sorted blocks
obtained from a sample of the input tables. We assume that
the join has two inequality predicates as in Algorithm 1. We
first obtain a sample from the two tables to join, using uniform
random sampling.2 We give the samples as input to Algorithm 3. We then sort the two samples based on the attributes
involved in one of the join predicates (lines 2–5), while using
the remaining join attributes to sort duplicate tuples. Next,
we divide each sample into smaller blocks (line 6) and test
all combinations of block pairs for potential overlaps based
on min/max values of all of the join attributes in each block
(lines 7–10). The idea is that overlapping blocks may gener-

(ii) pos 9

2

Fig. 4 Example of using a bitmap

We experimentally show in Sect. 8.7 that our algorithm requires only
1 % of the input data to be accurate.

123

132

Z. Khayyat et al.

Algorithm 3: IEJoin Selectivity Estimation
.X 


1
2
3
4
5

input : join predicates t1 .X op1 t2
and t1 .Y op2 t2
sample tables T s & T s 
 , block size n
output: Number of overlapping blocks
overlappingBlocks ← 0
if (op1 ∈ {<, ≤}) then
sort T s & T s 
 in descending order on X and X 

else if (op1 ∈ {>, ≥}) then
sort T s & T s 
 in ascending order on X and X 


10

Partition T s & T s 
 into B&B 
 blocks of size n
for (i ← 1 to B) do
for ( j ← 
1 to B 
 ) do
if T si T s 
j then
increment overlappingBlocks by 1

11

return overlappingBlocks

6
7
8
9

.Y 
 ,

ate results for a given query, while non-overlapping blocks
cannot generate results at all. Finally, we return the number of overlapping blocks as the selectivity estimation of the
inequality join (line 11). The lower the number of overlapping blocks, the higher the selectivity of the inequality join
condition.
While we cannot determine the exact size of the join output in overlapping blocks before execution, sorting helps
with good estimates. This is because sorting blocks naturally
increases the locality of the input relations as we consider
inequality join conditions. Thus, the number of overlapping blocks is a good approximation of the selectivity of
an inequality join condition.

icates. Clearly, a large N would increase the overhead of
the selectivity estimation. To lower this overhead, we reuse
the sorted input relations in multiple instances of Algorithm 3 that share a similar join predicate. For example,
IEJoin can sort on attribute time (or totalUsers) to reuse
it in join predicates time, totalUsers and time, cost
(resp. time, totalUsers and cost, totalUsers) as they
both share attribute time (resp. totalUsers). Note that sorting based on one attribute or another does not impact the
selectivity estimation of the pair combination.
4.3 Multi-way join optimization
For multi-way inequality joins, we follow a common
approach in optimizing such joins, such as in [31]. Generally
speaking, we execute a multi-way inequality join as a series
of two-way inequality joins formed as a left-deep plan. We
adopt a greedy approach where we choose the order of the
two-way joins based on their estimated selectivity, i.e., the
two-way joins with the higher selectivity (lowest number of
overlapping blocks as computed by Algorithm 3) are pushed
down in the plan.
Algorithm 4 builds the execution plan for a multi-way
join query. We first estimate the selectivity of all possible
combinations of two-way joins using Algorithm 3 (lines 3–
6). We discard Cartesian products. At level one of the leftdeep plan, we pick the two relations with the most selective

Algorithm 4: Multi-way IEJoin planner
4.2 Join optimization with multiple predicates
Given an inequality join query on two relations and with
more than two join predicates, we need to determine which
two predicates should be joined first to minimize the cost.
The remaining predicates will be evaluated on the result of
the join. To this end, we need to estimate the selectivity of
all pair combinations of the join predicates in the query. For
example, we compare the selectivity estimation of three pair
combinations (1 -3 ) for Q w ,

1
2
3
4
5
6

7
8
9
10

Q w : SELECT s1 .t_id, s2 .t_id
FROM west s1 , west s2
WHERE s1 .time > s2 .time and s1 .cost < s2 .cost
and s1 .totalUsers < s2 .totalUsers;

11
12
13
14
15

as follows:
16

1 : s1 .time > s2 .time & s1 .cost < s2 .cost
2 : s1 .time > s2 .time & s1 .totalUsers < s2 .totalUsers
3 : s1 .cost < s2 .cost & s1 .totalUsers < s2 .totalUsers
 
We need to test N2 combinations to choose the best
join predicates for a given query with N input join pred-

123

17
18

19
20

input : input relations R
output: join plan
n ← |R|
Est ← empty set
for (i ← 1 to n) do
for ( j ← i + 1 to n) do
if Ri & R j share IEJoin predicates then
Est ← selectivity estimation of Ri  R j
for ( plan I ndex ← 1 to n) do
if planIndex = 1 then
min ← smallest estimation ∈ Est
plan[1] ← Ri , where i ∈ min
plan[2] ← R j , where j ∈ min
else if planIndex > 2 then
Est 
 ← empty set
foreach Ri  R j estimation ∈ Est do
if (Ri |R j ) ∈ plan[1, plan I ndex − 1] and
(Ri ∧ R j ) ∈
/ plan[1, plan I ndex − 1] then
Est 
 ← selectivity estimation of Ri  R j
min ← smallest estimation ∈ Est 

plan[planIndex] ← Ri (or R j ), where (i ∧ j) ∈ min and
Ri (or R j ) ∈
/ plan[1, plan I ndex − 1]
remove min from Est
return plan

Fast and scalable inequality joins

inequality join (lines 8–11). We then proceed by selecting the
one relation, among the remaining ones, that would deliver
the most selective inequality join if joined with the previous
level in the plan (lines 12–18). This is performed by selecting
the relation that has the smallest number of overlapped blocks
when joined with a relation from the previous level. We only
consider joins that share an inequality predicate with one
relation in the previous levels (lines 14–16). In each level, we
append the most selective two-way join to the final execution
plan (lines 17–18) and remove it from the pool of available
joins (line 19). By repeating this process until there is no more
relations to join, we compute a full IEJoin order. Algorithm 4
returns an array that describes the left-deep plan. The plan is
obtained by joining the relations in the order they appear in
the array, i.e., first join plan[1] with plan[2], then the result
with plan[3] and so on.

5 Incremental inequality joins

133
(1) Compute Qp (T ) (see Figure 2)
L1 s3 (80) | s4 (90) | s1 (100) | s2 (140) |

H | s (5) s (6) | | s (10) | s (11)
L
H 0 3 5 0 0 1 0 7 (permutation array)
P
2

4

1

3

2

(2) Insert δ = {s5 (95, 8)}, maintain L1 , L2 and P
L1 s3 (80) | s4 (90) s5 (95) s1 (100) | s2 (140) |

H | s (5) s (6) s (8)
L
H0 3 5 4 0 1 0 7
P
4

2

B

1

5

| s3 (10) | s2 (11)
(permutation array)

0 0 0 0 0 0 0 0 (bit-array)

new pos = 4

(3) Visit tuples w.r.t. L2
(a) Set bits as visited before new pos
B 0 0 1 0 1 0 0 0 (for s4 and s1 )

·

(b) visit the new insertion s5
−−→
B

0 0 1 0 1 0 0 0

Output: (s5 , s1 )

⇒ 0 0 1 1 1 0 0 0

·

(c) visit s3 (only compare with new insertion s5 )
−−→
B

0 0 1 1 1 0 0 0

Output: (s3 , s5 )

⇒ 1 0 1 1 1 0 0 0

In this section, we present algorithms for incremental
inequality joins when the data keeps changing with insertions and deletions. We compute Q(D ⊕ ΔD), where Q is
an inequality join on D that contains either one or two relations, and ΔD is the data updates that are insertions ΔD +
or deletions ΔD − . Adapting our algorithms for incremental
computation faces two challenges: (i) maintaining the sorted
arrays and (ii) only computing results w.r.t. ΔD.
Maintain a sorted array Given a sorted array L and an
unsorted array ΔL, the problem is to compute a sorted array
for elements in L ⊕ ΔL.
(1) Sort-merge [27]. The straightforward solution is to first
sort ΔL and then merge two sorted arrays L and ΔL
in linear time. This approach is appropriate for batch
updates.
(2) Packed-memory array [5]. A widely adopted approach
for maintaining a dynamic set of N elements in sorted
order in Θ(N )-sized array is to use Packed-memory
array (PMA). The idea is to insert Θ(N ) empty spaces
among the elements of the array such that only a small
number of elements need to be shifted around per update.
Thus, the number of element moves per update is only
O(log2 N ). This approach is ideal for continuous query
answering in a streaming fashion.
Note that, PMA leaves empty spaces in the array. For
instance, the arrays L 1 and L 2 in Fig. 5 are stored using
PMAs, where “|”’s denote the empty spaces. Also, PMAs
handle deletions [5]. Although we use PMAs to maintain
the sorted arrays by leaving gaps (i.e., the “|”’s) for future
updates, our algorithms are unchanged by simply ignoring
these gaps.

·

(d) visit s2 (only compare with new insertion s5 )
−−→
B

1 0 1 1 1 0 0 0

Output:

⇒ 1 0 1 1 1 0 0 1

Fig. 5 Incremental IESelfJoin process for query Q p

Let us start by discussing the incremental inequality join
on one relation with insertions. We will then discuss the case
with deletions, and then on two relations.
Incremental IESelfJoin We illustrate the idea of computing incremental results on one relation with insertions by an
example.
Example 6 Consider query Q p from Example 2 and the data
T used in Fig. 2. Computing Q p (T ) is the same as described
in Fig. 2, shown as step (1) in Fig. 5. Consider a new tuple
insertion ΔD + = {s5 (95, 8)}. As shown in Fig. 5 step (2),
it finds the right positions in both sorted arrays, i.e., 95 in
position 4 of L 
1 , 8 in position 4 of L 
2 and the permutation
array is updated.
Similar to the process described in Sect. 2, it visits tuples
in L 
2 from left to right (Fig. 5 step (3)).
(a) For all tuples whose time values are less than s5 that is
8, set all corresponding bits as 1’s since they satisfy one
join condition on attribute time.
(b) Visit s5 and output all results for (s5 , si ) whose si is on
the right of s5 in B (i.e., satisfying both join conditions),
which is {(s5 , s1 )}.
(c) For other tuples on the right of s5 in L 
2 , output a new
join result if it contains the new tuple s5 . When visiting
s3 , output {(s3 , s5 )}.
(d) When visiting s2 , the process is similar to (c), with an
empty output.

123

134

From the above example, we can see that only new results
are produced, i.e., those coming from ΔD + . The incremental
algorithm using PMA is a simple adaption of Algorithm 2,
which is thus omitted here. Moreover, when the update contains a set of insertions, the procedure for each one is the
same as described in Example 6.
For deletions ΔD − , we use a similar methodology as discussed above to compute updated results. The difference is
that, instead of adding these new results, we remove them
from old results.
Incremental IEJoin We now discuss how to extend Algorithm 1 on two relations R and S to support incremental
processing. We will focus on insertions only (ΔR + and
ΔS + ), since deletions are similar to insertions by only removing results.
Take Fig. 3 for reference, we perform the following three
steps to run IEJoin: (1) maintain the sorted lists w.r.t. the
insertions ΔR + and ΔS + ; (2) maintain the offsets for R ⊕
ΔR + relative to S ⊕ ΔS + ; and (3) compute the new results.
Step (1) is the same as discussed in IESelfJoin. Step (2)
can be performed in a way similar to the sorted merge join by
linearly scanning both sorted arrays in R⊕ΔR + and S⊕ΔS +
to set the corresponding offsets. Step (3) is to run IEJoin by
only outputting results related to ΔR + or ΔS + , i.e., (ΔR + ,
S), (R, ΔS + ) or (ΔR + , ΔS + ). If insertions are only ΔR + ,
IEJoin ends by visiting the last element of ΔR + in the bitarray. Otherwise, it continues until all tuples in R ⊕ ΔR +
are visited. When processing both insertions and deletions,
results from (ΔR + ,ΔS − ) or (ΔR − ,ΔS + ) are ignored.

Z. Khayyat et al.

network overheads as it transfers identical copies of the data
to different workers. A naïve solution would be to reduce
the number of blocks by increasing their size. However, very
large blocks introduce work imbalance and require larger
memory for each worker.
6.1 Scalable IEJoin
We solve the above challenges through efficient preprocessing and post-processing phases that reduce data replication
by minimizing the number of required data block pairs. We
achieve this by pruning unnecessary data block pairs early
before wasting any resources. The preprocessing phase generates space-efficient data blocks for the input relation(s),
predicts which pair of data blocks may report query results,
and only materializes useful pairs of data blocks. IEJoin,
in its scalable version, returns the join results as a pair
of tupleIDs instead of returning the actual tuples. It is the
responsibility of the post-processing phase to materialize
the final results by resolving the tupleIDs into actual tuples.
We use the internal tupleIDs of existing systems to uniquely
identify different tuples. We summarize in Algorithm 5 the
implementation of the scalable algorithm when processing
two input tables.

Algorithm 5: Scalable IEJoin

1

6 Scalable inequality joins

2
3
4

We present a scalable version of IEJoin along the same
lines of state-of-the-art general-purpose distributed data
processing systems, such as Hadoop’s MapReduce [12] and
Spark [37]. Our goal is twofold: (i) scale our algorithm to very
large input relations that do not fit into the main memory of
a single machine and (ii) minimize processing overheads to
improve the efficiency even further.
A simple approach for scaling IEJoin is to (i) construct k
data blocks of each input relation, (ii) apply Cartesian product (or self-Cartesian product for a single relation input) on
the data blocks, and (iii) run IEJoin (either on a single table
or two tables input) on all remaining data block pairs (up to
k 2 ). This approach suffers from a high processing overhead
caused by the excessive block replication. In particular, this
excessive data replication increases the CPU overhead by
scheduling a large number of tasks and redundantly processing replicated blocks (i.e., sorting identical blocks in different
threads). It also causes high memory overheads where different tasks maintain identical copies of the same data. In
distributed settings, this approach additionally causes large

123

5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

input : Query Q with 2 join predicates t1 .X op1 t2 .X 
 and
t1 .Y op2 t2 .Y 
 , Table t1 ,Table t2
output: Table tout
//Preprocessing
foreach tuple r ∈ t1 and t2 do
r ← global unique ID
DistT1 ← sort t1 on t1 .X and partition to n blocks
DistT2 ← sort t2 on t2 .X 
 and partition to m blocks
for (i ← 1 to n) do
D1i ← all X and Y values in Dist T 1i
M T 1i ← min and max of X and Y in D1i
for ( j ← 1 to m) do
D2 j ← all X 
 and Y 
 values in Dist T 2 j
M T 2 j ← min and max of X 
 and Y 
 in D2 j
Virt ← MT1 × MT2
forall the (M
T 1i ,M T 2 j ) pairs ∈ Virt do
if M T 1i
M T 2 j then
(M T 1i , M T 2 j ) ← (D1i , D2 j )
else
Remove (M T 1i , M T 2 j ) from Virt
//IEJoin function
forall the block pairs (D1i , D2 j ) ∈ Virt do
T upleI D Result ← IEJoin(Q,D1i ,D2 j )
//Post-processing
forall the tupleID pairs (v, w) ∈ T upleI D Result do
tuplev ← tuple id v in DistT1
tuplew ← tuple id w in DistT2
tout ← (tuplev ,tuplew )

Fast and scalable inequality joins

135

tation and bit arrays generation are similar to the centralized
version. However, the scalable IEJoin does not have access
to the actual relation tuples. Therefore, each parallel IEJoin
instance outputs a pair of tupleIDs that represents the joined
tuples (lines 19–20).
Post-processing In the final step, we materialize the result
pairs by matching each tupleID-pair from the output of the
distributed IEJoin with the tupleIDs of Dist T 1 and Dist T 2
(lines 22–25). We run this post-processing phase in parallel,
as a scalable hash join based on the tupleIDs, to speedup the
materialization of the final join results.
6.2 Multi-threaded and distributed IEJoin
Fig. 6 An example of the IEJoin preprocessing stage with two relations R and S

Preprocessing After assigning unique tupleIDs to each input
tuple (lines 2–3), the preprocessing step globally sorts each
relation on a common attribute of one of the IEJoin predicates (i.e., salary in Q 1 ). Then, it partitions each sorted
relation to k = 
 M
b  equally sized partitions, where M is the
relation input size and b is the default block size (lines 4–5).
Note that global sorting before partitioning maximizes data
locality within partitions, which in turn decreases the overall
runtime. This is because global sorting partially answers one
of the inequality join conditions, where it physically moves
tuples closer to their candidate pairs. In other words, global
sorting increases the efficiency of block pairs that generate
results, while block pairs that do not produce results can be filtered out before actually processing them. After that, for each
sorted partition, we generate a single data block that stores
only the attribute values referenced in the join conditions in
a list. Following the semi-join principle, these data blocks do
not store the entire tuples thus allowing to reduce the memory,
disk I/O and network overheads. We also extract metadata
that contain the block ID and the min/max values of each
referenced attribute value from each data block (lines 6–11).
Then, we create n M T 1 × m M T 2 virtual block combinations
and filter out block combinations with non-intersecting min–
max values since they do not produce results (lines 12–17).
Figure 6 illustrates the preprocessing of two relations R and
S. It starts by sorting and partitioning R into three blocks and
S into two blocks. It then generates the metadata blocks and
executes Cartesian product on all metadata blocks. Next, it
removes all non-overlapping blocks. Finally, it recovers the
original content for all overlapping metadata blocks, i.e., μR1
recovers its data from block R1 .
IEJoin We now have a list of overlapping block pairs. We simply run an independent IEJoin for each of these pair blocks
in parallel. Specifically, we merge the attribute values in D1
and D2 and run IEJoin over the merged block. The permu-

The scalable solution we just described can be adapted to
a multi-threaded setting as follows. For the preprocessing
phase, we sort and partition inputs in parallel. If the input
does not fit into memory, we apply external sorting. Next,
block metadata are extracted by all threads, while each thread
examines an independent set of block metadata pairs to eliminate non-overlapping blocks in parallel. The remaining block
metadata pairs are then materialized, from either a cached
copy in memory or from disk, into block pairs. Each thread
then applies the IEJoin algorithm on a different block pair
to generate partial join results. Once the complete result is
computed, we materialize the output using multi-threaded
hash-based join. In a distributed setting, we follow the same
process, but we use compute nodes as the main processing
units instead of threads. We discuss in Sect. 7.2 our distributed version on top of Spark SQL.

7 Integration into existing systems
We describe the integration of our algorithms into three existing systems: PostgreSQL, a popular open-source DBMS
(Sect. 7.1); Spark SQL, a popular SQL-like engine on top
of Spark (Sect. 7.2); and Nadeef [24], a distributed data
cleaning system (Sect. 7.3).
7.1 PostgreSQL
PostgreSQL processes queries in three stages: parsing, planning, and execution. Parsing extracts relations and predicates
and creates query parse trees. Planning creates query plans
and invokes the query optimizer to select a plan with the
smallest estimated cost. Execution runs the selected plan and
emits the output.
Parsing and planning PostgreSQL uses merge and hash join
operators for equijoins and naïve nested loop for inequality
joins. PostgreSQL looks for the most suitable join operator
for each join predicate. We extend this check to verify that a
join is IEJoin-able by checking if a predicate contains a scalar

123

136

Z. Khayyat et al.

inequality operator. If it is the case, we save the operator’s
oid in the data structure associated with the predicate. For
each operator and ordered pair of relations, we create a list
of predicates that the operator can handle. For example, two
equality predicates over the same pair of relations are associated to one hash join. In the presence of inequality joins, we
make sure that the PostgreSQL optimizer chooses our IEJoin
algorithm, while optimizations for other operators are done
by PostgreSQL optimizer on top of the optimized IEJoin.
Next, the Planner estimates the execution cost for possible join plans. In the presence of both equality and inequality
joins, the optimizer will delay all inequality joins as they are
usually less selective than equality joins. More specifically,
every node in the plan has a base cost, which is the cost of executing the previous nodes, plus the cost for the actual node.
We added a cost function for our operator; it is evaluated as
the sum of the cost for sorting inner and outer relations, CPU
cost for evaluating all output tuples (approximated based on
PostgreSQL’s default inequality joins estimation), and the
cost of evaluating additional predicates for each tuple (i.e., the
ones that are not involved in the actual join). Next, PostgreSQL selects the plan with the lowest cost.
Execution The executor stores incoming tuples from outer
and inner relations into arrays of type TupleTableSlot, which
is PostgreSQL’s default data structure that stores the relation
tuples. These copies of the tuples are required as PostgreSQL
may not have the content of the tuple at the same pointer location when the tuple is sent for the final projection. This step
is a platform-specific overhead that is required to produce an
output. The outer relation (of size N ) is parsed first, followed
by the inner relation (of size M). If the inner join data is identical to the corresponding outer join data (self-join), we drop
the inner join data and the data structure has size N instead of
2N . If there are more than two IEJoin predicates, then we follow the procedure explained in Sect. 4.2, i.e., we pass to the
algorithm the pair of predicates with the highest selectivity.
We illustrate in Fig. 7 the data structure and the permutation array computation with an example for self-join Q p . We
initialize the data structure with an index (idx) and a copy of
the attributes of interest (time and cost for Q p ). Next, we
sort the data on the first predicate (time) using system function qsort with special comparators (defined in Algorithm 1)
to handle cases where two values for a predicate are equal.
Sort1
s1
s2
s3
s4

idx
3
4
1
2

time cost pos
80
10
1
90
5
2
100
6
3
140
11
4

Sort2
s1
s2
s3
s4

idx
4
1
3
2

time cost pos
90
5
2
100
6
3
80
10
1
140
11
4

Fig. 7 Permutation array creation for self-join Q p

123

The result of the first sort is reported at the left-hand side of
Fig. 7. The last column (pos) is now filled with the ordering
of the tuples according to this sorting. As a result, we create
a new array to store the index values for the first predicate.
We use this array to select tuple IDs at the time of projecting
tuples. The tuples are then ordered again according to the
second predicate (cost), as reported in the right-hand side of
Fig. 7. After the second sorting, the new values in pos are
the values for the permutation array.
Finally, we create and traverse a bit-array B of size (N +
M) (N in case of self-join) along with a bitmap, as discussed
in Sect. 3.3. If the traversal finds a set bit, the corresponding
tuples are sent for projection. Predicates, not selected by the
optimizer in the case of multi-predicate IEJoin, are evaluated
at this stage and, if the conditions are satisfied, tuples are
projected.

7.2 Spark SQL
Spark SQL [4] allows users to query structured data on top
of Spark [37]. It stores the input data as a set of in-memory
Resilient Distributed Datasets (RDD). Each RDD is partitioned into smaller cacheable blocks, where each block fits
in the memory of a single machine. Spark SQL takes as input
the datasets location(s) in HDFS and an SQL query, and outputs an RDD that contains the query result. The default join
operation in Spark SQL is inner join. When passing a join
query to Spark SQL, the optimizer searches for equality join
predicates that can be used to evaluate the inner join operator
as a hash-based physical join operator. If there are no equality
join predicates, the optimizer translates the inner join physically to a Cartesian product followed by a selection predicate.
We implemented the distributed version of IEJoin as a
new Spark SQL physical join operator. To make the optimizer
aware of the new operator, we added a new rule to recognize
inequality conditions. The rule passes all inequality conditions to the IEJoin operator. If the operator receives more
than two inequality join conditions, it deploys the IEJoin
optimizer to find the two highest selective inequality conditions. It executes the join using such conditions and evaluates
the rest of the join conditions as a post selection operation on
the output. Similar to the PostgreSQL case, in the presence of
both equality and inequality joins, it orders inequality joins
after all equality joins. The distributed operator utilizes Spark
RDD operators to run both the IEJoin and its optimizer. As a
result, distributed IEJoin depends on Spark’s default memory
management to partition and store the user’s input relation.
If the result does not fit in the memory of a single machine,
we temporarily store the result into HDFS. After all IEJoin
instances finish writing into HDFS, the distributed operator
passes the HDFS file to Spark, which constructs a new RDD
of the result and passes it to Spark SQL.

Fast and scalable inequality joins

137

(a)

(b)

ing blocks’ metadata with the original data blocks to recover
their content (Fig. 8e) through two RDD join() operators;
one for each input relation. We then apply an independent
instance of IEJoin on every block pair in parallel by using
RDD flatMapToPair() operator (Fig. 9a). Finally, we
join the IEJoin result with the original relations R and
S, using two RDD join() operators, to recover the full
attribute information of the result (Fig. 9b).
7.3 NADEEF

(c)
(d)

(e)

Fig. 8 Spark’s DAG for the preprocessing phase

Fig. 9 Spark’s DAG for IEJoin the post-processing phase

Figures 8 and 9 show how the distributed IEJoin is
processed in Spark. First, we globally sort the two relations
using Spark RDD sort (Fig. 8a). Next, we generate a set of
distributed data blocks for each relation through the RDD
mapPartitionsWithIndex() function (Fig. 8b). As
described in Sect. 6, the block transformation does not store
the actual tuples; it only stores the attributes in the join
predicates. We then transform the data blocks into metadata
blocks using their statistics (Fig. 8c). Afterward, we apply
a Cartesiana product on the block metadata of R and S and
remove non-overlapping block metadata through the RDD
filter() operator (Fig. 8d). Next, we join the remain-

Nadeef [11,15,16,24] is a generalized data cleaning system,3 implemented on top of PostgreSQL and Rheem [3].
Rheem is a data processing framework that provides independence from and interoperability among existing data processing platforms. Given a dirty dataset where errors are detected
as violations of some data quality rules (e.g., integrity constraints), Nadeef outputs a data instance free of violations.
Nadeef operates in two phases: violation detection and data
repair. In the detection phase, it finds all data errors w.r.t. the
data quality rules; in the repair phase, it corrects data errors
by using a repair algorithm, i.e., an algorithm that updates
the data instance to make it consistent w.r.t. the rules.
In the data violation detection phase, Nadeef has to generate all possible violation candidates for verification. We
can see this candidate generation such as a self-join operation as it has to check each input tuple for possible violations
with other tuples. For example, a rule might state that given
a dataset for employees in the same State, for every two distinct individuals, the one earning a lower salary should have
a lower tax rate. In this case, Nadeef avoids performing
a cross product by using a distributed sort-merge join-like
approach [24]. In a nutshell, it first range partitions the input
dataset and sorts each of the resulting range partitions in order
to sort-merge join overlapping data partitions in a distributed
fashion. It is in this last joining step that we integrate the
IEJoin algorithm. That is, instead of performing a simple
sort-merge join, we perform our algorithm as explained in
Sect. 3. More specifically, we implemented the distributed
version of IEJoin in Nadeef by (i) extending the Rheem
framework in order to expose the new join as a physical
operator and by (ii) implementing the distributed version of
IEJoin as an execution operator for Spark. Notice that the
IEJoin execution operator is similar to its implementation in
Spark SQL discussed in Sect. 7.2. In addition, we extended
the Nadeef optimizer to take this new join operator into
consideration.

8 Experimental study
We evaluate IEJoin along several dimensions: (i) Effect of
sorting and caching (Sect. 8.5); (ii) IEJoin in a centralized
3

http://github.com/daqcri/NADEEF.

123

138

Z. Khayyat et al.

greater than (>). The dataset size is fixed (100K tuples),
but we vary the ratio of female students between 0.01
and 50 % to diversify the selectivity of the query.

Table 2 Size of the datasets
Dataset

Number of tuples

Size

Employees

10K–500M

300KB–17GB

Employees2

1B–8B

34GB–287GB

Events

10K–500M

322KB–14GB

Events2

1B–6B

32GB–202GB

MDC

24M

2.4GB

Cloud

470M

28.8GB

Grades

100K

1.7MB

environment (Sect. 8.6); (iii) Query optimization techniques
(Sect. 8.7); (iv) Incremental algorithms (Sect. 8.8); and (v)
IEJoin in a distributed environment (Sect. 8.9).
8.1 Datasets
We used both synthetic and real-world data (summarized in
Table 2) to evaluate our algorithms.
(1) Employees contains employees’ salary and tax information [6] with eight attributes: state, married,
dependents, salary, tax, age, and three others for
notes. The relation has been populated with real-life
data for tax rates, income brackets, and exemptions
which were then used to generate synthetic tax records.
Employees2 in Table 2 is a group of larger input datasets
with up to 6 Billion records, but with only 0.001 % random changes to tax values. We lower the percentage of
changes to test the distributed algorithm on large input
files while avoiding extremely large output files.
(2) Events is a synthetic dataset that contains start and
end time information for a set of independent events.
Each event contains the name of the event, event ID, the
number of attending people, and the sponsor ID. To make
sure we generate output for a given query, we extended
end values for 10 % random events. Events2 contains
larger datasets with up to 6 Billion records and 0.001 %
extended random events.
(3) Mobile Data Challenge (MDC) is a 50GB real
dataset [26,28] that contains behavioral data of nearly
200 individuals collected by Nokia Research (https://
www.idiap.ch/dataset/mdc). The dataset contains physical locations, social interactions, and phone logs of the
participating individuals.
(4) Cloud [33] is a real dataset that contains cloud reports
from 1951 to 2009, through land and ship stations (ftp://
cdiac.ornl.gov/pub3/ndp026c/).
(5) Grades is a synthetic dataset for testing the not equal
(=) join predicate. It is composed of a list of students
with attributes id, name, gender, grade, and age. We
encode gender values by using numeric values to be
able to compare them using conditions less than (<) and

123

8.2 Algorithms
We compare our algorithms with several centralized as well
as distributed algorithms
Centralized systems For our centralized setting, we use the
following systems:
(1) C++ IEJoin. This is a standalone C++ implementation
of IEJoin to run experiments that cannot be executed
within a DBMS. For example, we use this implementation to evaluate the incremental experiments of
IEJoin (Sect. 8.8). The implementation uses optimized
data structures from the Boost library4 to maximize
the performance gain of IEJoin through a faster array
scanning.
(2) PG- IEJoin. We implemented IEJoin inside PostgreSQL
v9.4, as discussed in Sect. 7.1. We compare it against the
baseline systems below.
(3) PG- Original. We used PostgreSQL v9.4 as a baseline.
We tuned it with pgtune to maximize the benefit from
large main memory.
(4) PG- BTree & PG-GiST. We also used two variants
of PostgreSQL using indices. PG- BTree uses a B-tree
index for each attribute in a query. PG-GiST uses the
GiST access method in PostgreSQL, which considers
arbitrary indexing schemes and automatically selects the
best technique for the input relation.
(5) MonetDB. We used MonetDB Database Server Toolkit
v1.1 (Oct2014-SP2), an open-source column-oriented
database, in a disk partition of size 669GB.
(6) DBMS- X. We used a leading commercial centralized
relational database.
Distributed systems We used the following systems:
(1) Spark SQL- IEJoin. We implemented IEJoin inside
Spark SQL v1.0.2 (https://spark.apache.org/sql/) as
detailed in Sect. 7.2. We evaluated the performance of
IEJoin against the baseline systems below.
(2) Spark SQL & Spark SQL- SM. Spark SQL is
the default implementation in Spark SQL. Spark SQLSM [24] is an optimized version based on distributed
sort-merge join (Sect. 7.3). We also improve the above
method by pruning the non-overlapping partitions to be
joined.
(3) DPG- BTree & DPG-GiST. We used a commercial
version of PostgreSQL with distributed query process4

http://www.boost.org/

Fast and scalable inequality joins

ing. This allows us to compare Spark SQL- IEJoin to
a distributed version of PG- BTree and PG-GiST.
8.3 Queries
We evaluate our algorithms from different perspectives and
using several queries with inequality join conditions. It is
worth noting that our main goal in the experiments is to show
the value of optimizing inequality queries using our approach
irrespective of the presence of other operators.
For our first experiment, we used the following self-join
query over Employees to find violations of a data quality
rule [10]:
Q 1 : SELECT r.id, s.id
FROM Employees r, Employees s
WHERE r.salary < s.salary AND r.tax > s.tax;

The query returns a set of employee pairs, where one
employee earns higher salary than the other but pays less
tax. To make sure that we generate output for Q 1 , we selected
10 % random tuples and increased their tax values. We also
used a self-join query that collects pairs of overlapping events
(in the Events dataset):
Q 2 : SELECT r.id, s.id
FROM Events r, Events s
WHERE r.start ≤ s.end AND r.end ≥ s.start
AND r.id = s.id;

We extended end values for 10 % random events to make
sure we generate output for Q 2 . Throughout all of our experiments, we either use these two queries or slightly modified
versions thereof. For example, we added a third join predicate to Q 1 over age to get Q 
1 and extended Q 1 and Q 2 to get
Q mw to study how our algorithms deal with multi-predicate
conditions and with multi-way joins, respectively.
Q 
1 : SELECT r.id, s.id
FROM Employees r, Employees s
WHERE r.salary < s.salary AND r.tax > s.tax
AND r.age > s.age;
Q mw : SELECT count(*)
FROM R r, S s, T t, V v, W w
WHERE r.salary > s.salary AND r.tax < s.tax
AND s.start ≤ t.end AND s.end ≥ t.start
AND t.salary > v.salary AND t.tax < v.tax
AND v.start ≤ w.end AND v.end ≥ w.start;

We also used slightly modified versions of Q 1 and Q 2 for
our comparison with baselines using indexes. This is because
although Q 1 and Q 2 appear to be similar, they require different data representation to be indexed with GiST. The
inequality attributes in Q 1 are independent, each condition
forms a single open interval, while in Q 2 they are dependent,
together they form a single closed interval. Thus, we convert
salary and tax attributes into a single geometric point data

139

type SalTax to get Q 1i . Similarly for Q 2 , we convert start
and end attributes into a single range data type StartEnd to
get Q 2i .
Q 1i : SELECT r.id, s.id
FROM Employees r, Employees s
WHERE r.SalTax >∧ s.SalTax
AND r.SalTax  s.SalTax;
Q 2i : SELECT r.id, s.id
FROM Events r, Events s
WHERE r.StartEnd && s.StartEnd AND r.id = s.id;

In the rewriting of these queries for PG-GiST, operator
“>∧ ” corresponds to “is above?”, operator “” means “is
strictly right of?”, and operator “&&” indicates “overlap?”.
For geometric and range types, GiST uses a Bitmap index to
optimize its data access with large datasets.
In addition to the above two main queries, we used Q 3
to evaluate our algorithms when producing different output
sizes. This query looks for all persons that are close to a shop
up to a distance c along the x-axis (xloc) and the y-axis (yloc):
Q 3 : SELECT s.name, p.name
FROM Shops s, Persons p
WHERE s.xloc − c < p.xloc AND s.xloc + c > p.xloc
AND s.yloc − c < p.yloc AND s.yloc + c > p.yloc;

We also used a self-join query Q 4 , similar to Q 3 , to compute all stations within distance c = 10 for every station.
Since the runtime in Q 3 and Q 4 is dominated by the output size, we mostly used them for scalability analysis in the
distributed case.
Furthermore, we used query Q 5 for our not equal join
predicates experiment.
Q 5 : SELECT r.name, s.name FROM Grades r, Grades s
WHERE r.gender = s.gender AND r.grade > s.grade;

Notice that PostgreSQL executes Q 5 with nested loops.
In our solution, the query is rewritten to Q 
5 .
Q 
5 : SELECT r.name, s.name FROM Grades r, Grades s
WHERE r.gender < s.gender AND r.grade > s.grade
UNION ALL
SELECT r.name, s.name FROM Grades r, Grades s
WHERE r.gender > s.gender AND r.grade > s.grade;

As attribute gender in Q 5 has only two distinct values, it
might not provide the complete picture of the performance
of (=) IEJoins. Thus, we additionally used Q 6 and Q 7 to
analyze the effect of non-binary attributes on IEJoin with
(=) as the join predicates.
Q 6 : SELECT r.name, s.name FROM Grades r, Grades s
WHERE r.age = s.age AND r.grade > s.grade;
Q 7 : SELECT r.name, s.name FROM Grades r, Grades s
WHERE r.gender = s.gender AND r.grade = s.grade;

123

140

Z. Khayyat et al.

8.4 Setup

Table 5 Cache statistics on 10M tuples (events data)

For the centralized evaluation, we used a Dell Precision
T7500 with two 64-bit quad-core Intel Xeon X5550 and
58GB RAM. For our distributed experiments, we used a cluster of 17 Shuttle SH55J2 machines (1 master with 16 workers)
with Intel i5 processors with 16GB RAM, and connected to
a high-end 1 Gigabit switch. For both settings, all arrays are
stored in memory.
8.5 Parameter setting
We start our experimental evaluation by showing the effect
of the two optimizations (Sect. 3.3), as well as the effect of
global sorting (Sect. 6).
Bitmap Bitmaps are used when big array scanning is expensive. The chunk size is an optimization parameter that is
machine-dependent. We run query Q 2 on 10M tuples with
size 322MB to show the performance gain of using a bitmap
based on three different implementations of IEJoin: the centralized C++, the C implementation of PostgreSQL, and the
Java implementation of Spark SQL. For Spark SQL, we only
ran a single instance of IEJoin in a centralized setting.
Results are shown in Table 3. Intuitively, the larger the
chunk size, the better. However, a very large chunk size
defeats the purpose of using bitmaps to reduce the bit-array
scanning overhead. The experiment shows that the performance gain is 3X between 256 bits and 1,024 bits and around
1.8X between 1,024 bits and 4,096 bits. Larger chunk sizes
show worse performance, as shown with chunk size of 16,384
bits. The experiment in Table 4 shows the performance of the
three implementations of IEJoin when adding the max scan
index optimization to the bitmap. This index optimization

Table 3 Bitmaps on 10M tuples (events data)
Chunk size (bits)

C++ (s)

PostgreSQL (s)

Spark SQL (s)

1

>1 day

>1 day

>1 day

64

2333

3139

1623

256

558

817

896

1024

158

242

296

4096

81

117

158

16384

139

142

232

Table 4 Bitmaps on 10M tuples with max scan index optimization
(Events data)
Max index?

C++ (s)

PostgreSQL (s)

Spark SQL (s)

No

81

117

158

Yes

23

47

46

123

Parameter (M/s)

IEJoin (union)

IEJoin

Cache-references

6.5

8.4

Cache-references-misses

3.9

4.8

L1-dcache-loads

459.9

1,240.6

L1-dcache-load-misses

8.7

10.9

L1-dcache-stores

186.8

567.5

L1-dcache-store-misses

1.9

1.9

L1-dcache-prefetches

4.9

7.0

L1-dcache-prefetches-misses

2.2

2.7

LLC-loads

5.1

6.6

LLC-load-misses

2.9

3.7

LLC-stores

3.8

3.7

LLC-store-misses

1.1

1.2

LLC-prefetches

3.1

4.1

LLC-prefetch-misses

2.2

2.9

dTLB-loads

544.4

1,527.2

dTLB-load-misses

0.9

1.6

dTLB-stores

212.7

592.6

dTLB-store-misses

0.1

0.1

Total time (s)

125

325

improves the performance of IEJoin around 3.5 times compared to the best results achieved by the bitmap in Table 3.
Union arrays To study the impact of the union optimization,
we run IEJoin with and without the union array using 10M
tuples in the Events dataset. We collect the following statistics with Spark SQL- IEJoin, as shown in Table 5: (i) L1
data caches (dcache), (ii) last level cache (LLC), and (iii) data
translation lookaside buffer (dTLB). The optimized algorithm with union arrays is 2.6 times faster than the original
one. The performance gain in the optimized version is due to
the lower number of cache loads and stores (L1-dcache-loads,
L1-dcache-stores, dTLB-loads and TLB-stores), which is 2.7
to 3 times smaller than the original algorithm. This behavior
is expected since the optimized IEJoin has fewer arrays w.r.t.
the original version.
Global sorting on distributed IEJoin As presented in Algorithm 5, the distributed version of our algorithm applies
global sorting in the preprocessing phase (lines 6–7). We
report in Table 6 a detailed performance comparison for Q 1
and Q 2 with and without global sorting on 100M tuples
from Employees and Events datasets. The preprocessing time
includes data loading from HDFS, global sorting, partitioning, and block-pairs materialization. One may think that the
global sorting impairs the performance of distributed IEJoin
as it shuffles data through the network. However, global sorting improves the performance of the distributed algorithm by
2.4 to 2.9 times. More specifically, the runtime for the pre-

Fast and scalable inequality joins

141

Table 6 IEJoin time analysis on 100M tuples and 6 workers
Query

Preprocess

IEJoin

Post-process

Total

With global sorting (s)
Q1

632

162

519

1313

Q2

901

84

391

1376

Without global sorting (s)
Q1

1025

1714

426

3165

Q2

1182

1864

349

3395

processing phase with global sorting is at least 30 % faster
compared to the case without global sorting. Moreover, we
also note that the time required by IEJoin is one order of
magnitude faster when using global sorting. This is because
global sorting enables to filter out block-pair combinations
that do not generate results. This greatly reduces the network
overhead and increases the memory locality in the block combinations that are passed to our algorithm.
Based on the above experiments, in the following tests we
used 1,024 bits as the default chunk size, the max scan index
optimization, union arrays, and global sorting for distributed
IEJoin.

8.6 Single-node experiments
In this set of experiments, we study the efficiency of IEJoin
on datasets that fit the main memory of a single compute
node and compare its performance with alternative centralized systems.
(a)

IEJoin vs. baseline systems Figure 10 shows the results for
queries Q 1 on Employees dataset and Q 2 on Events dataset
in a centralized environment using 10, 50 and 100 K tuples.
The x-axis represents the input size in terms of the number of tuples, and the y-axis represents the corresponding
running time in seconds. The figure reports that PG- IEJoin
outperforms all baseline systems by more than one order of
magnitude for both queries and for every reported dataset
input size. In particular, PG- IEJoin is up to more than three
(resp., two) orders of magnitude faster than PG- Original
and MonetDB (resp., DBMS-X). Clearly, the baseline systems cannot compete with PG- IEJoin since they all use the
classic Cartesian product followed by a selection predicate.
In fact, this is the main reason why they cannot run for bigger
datasets.
IEJoin versus indexing We now consider the two variants
of PostgreSQL, PG- BTree & PG-GiST, to evaluate the
efficiency of our algorithm on bigger datasets. We run again
Q 1 and Q 2 on 10M and 50M records using both Employees
and Events datasets. Figure 11 shows the results. In both
experiments, IEJoin is more than one order of magnitude
faster than PG- GiST. In fact, IEJoin is more than three times
faster than the GiST indexing time alone. We stopped PGBTree after 24 hours of runtime. Our algorithm performs
better than these two baseline indices because it better utilizes
the memory locality.
Memory consumption The memory consumption for MonetDB increases exponentially with the input size. For example, MonetDB uses 419GB for an input dataset with only
200K records. In contrast to MonetDB, IEJoin makes better
(b)

Fig. 10 IEJoin versus baseline systems (centralized). a Q 1 . b Q 2
(a)

(b)

Fig. 11 IEJoin versus BTree and GiST (centralized). a 10M tuples. b 50M tuples

123

142

Z. Khayyat et al.

Table 7 Runtime and memory usage (PG- IEJoin)
Query

Input

Output

Time (s)

Mem (GB)

Q1

100K

9K

0.30

0.1

Q1

200K

1.1K

0.5

0.2

Q1

1M

29K

2.79

0.9

Q1

10M

3M

27.64

8.8

Q2

100K

0.2K

0.34

0.1

Q2

200K

0.8K

0.65

0.2

Q2

1M

20K

3.38

0.9

Q2

10M

2M

59.6

9.7

Q4

100K

6M

2.8

0.1

Q4

200K

25M

10.6

0.2

Q4

1M

0.4B

186

0.9

Q4

10M

50.5B

28,928

8.2

Table 8 Time breakdown on 50M tuples, all times in seconds
Query
reading

Data
sorting

Data
scanning

Bit-array

Total time

Q1

82

31

4

117

Q2

85

31

3

119

Q1

46

94

4

146

Q2

48

238

24

310

C++ IEJoin

PG- IEJoin

Spark SQL- IEJoin (Single Node)
Q1

158

240

165

563

Q2

319

332

215

866

use of the memory. Table 7 shows that IEJoin uses around
200MB for Q 1 and Q 2 for an input dataset of 200K records,
while MonetDB requires two orders of magnitude more
memory. In Table 7, we also report the overall memory used
by sorted attribute arrays, permutation arrays, and the bitarray. Moreover, although IEJoin requires 8.2GB of memory
for an input dataset of 10M records, it runs to completion in
about 8 hours (28,928 seconds) for a dataset producing more
than 50 billion output records.
Time breakdown We further analyze the breakdown time of
IEJoin on Employees and Events datasets with 50M tuples.
Table 8 shows that, by excluding the time required to load
the dataset into memory, scanning the bit-array takes only
10 % of the overall execution time in C++ IEJoin, 5 % in
PG- IEJoin, and 40 % in Spark SQL- IEJoin, while the rest
is mainly for sorting. This shows the high efficiency of our
algorithm.
IEJoin with the not equal (=) join predicate We tested the
performance of IEJoin with the not equal join predicate with
Q 5 on the Grades dataset. PG- Original runs the query by

123

using nested loops, while PG- IEJoin uses two instances of
IEJoin to process Q 
5 . As shown in Fig. 12a, PG- IEJoin is
from four times to two orders of magnitude faster than PGOriginal. Note that the query output becomes larger while
increasing the Female frequency in the dataset.
We also tested the effect of a non-binary attribute by using
queries Q 5 , Q 6 , and Q 7 . For PG- IEJoin, the queries were
transformed into a union of inequality joins as discussed
before. Figure 12b shows that running PG- IEJoin on Q 5
and Q 7 is an order of magnitude faster than PG- Original,
while on Q 6 , it is four times faster. This is because the join
attribute in Q 5 and one of the join attributes in Q 7 generate
far less results than Q 6 . In fact, the output of Q 6 is five times
higher than the one of Q 5 and three times higher than the one
of Q 7 . This difference on Q 6 is expected since both attributes
of the not equal join predicates are not binary.
IEJoin versus cache-efficient Cartesian product We further
push our evaluation to better highlight the memory locality efficiency on Q 1 using Employees dataset. We compare
the performance of C++ IEJoin with both naïve and cacheefficient Cartesian product joins for Q 1 on datasets that fit
the L1 cache (256 KB), L2 cache (1 MB), and L3 cache (8
MB) of the Intel Xeon processor. We used 10K tuples for
L1 cache, 40K tuples for L2 cache, and 350K tuples for L3
cache. We do not report results for query Q 2 because they
are similar to Q 1 . In Fig. 13, we see that when the dataset
fits in the L1 cache, IEJoin is two orders of magnitude faster
than both the cache-efficient Cartesian product and the naïve
Cartesian product. Furthermore, as we increase the dataset
size for Q 1 to be stored at the L2 and L3 caches, we see
that IEJoin becomes almost three and four orders of magnitude faster than the Cartesian product. This is because of
the delays of L2 and L3 caches and the complexity of the
Cartesian product.
Single-node summary IEJoin outperforms existing baselines
by at least an order of magnitude for two main reasons: It
avoids the use of Cartesian product and it exploits memory
locality by using memory-contiguous data structures with
small footprint. In other words, our algorithm avoids as much
as possible going to memory to fully exploit the CPU speed.
8.7 IEJoin optimizer experiments
In this section, we evaluate the accuracy of the IEJoin
optimizer when dealing with multi-predicate and multi-way
queries. We use Q 
1 for multi-predicate IEJoin and Q mw
for multi-way IEJoin. For the multi-predicate experiment,
we evaluate the performance of IEJoin by using the highest selective predicates, found by Algorithm 3, compared to
using other predicates. For the multi-way IEJoin, we compare the runtime of the plan generated by Algorithm 4 against
other plans.

Fast and scalable inequality joins

143
(a)

(b)

Fig. 12 Q 5 runtime with different Female distributions, and Q 5 , Q 6 and Q 7 runtimes with 10 % Female distribution. a Q 5 . b Q 5 , Q 6 and Q 7

(a)

(b)

(c)

Fig. 13 Q 1 runtime for data that fits caches. a L1 cache. b L2 cache. c L3 cache

Multi-predicate IEJoin We evaluate the accuracy of the selectivity estimation algorithm on query Q 
1 usinmg C++ IEJoin.
We first use Algorithm 3 to calculate the selectivity estimation for all three predicate pairs using the join attributes
salary, tax, and age.
To evaluate the estimation algorithm, we generated two
different distributions of attribute age for query Q 
1 : a low
selectivity distribution that generates a large output with
salary and tax attributes, and a high selectivity distribution that generates a small output with attribute salary. The
low selectivity distribution injects random noise on age
attribute in each tuple, generating a large output for the
IEJoin on (r.salary < s.salary AND r.age > s.age) or on
(r.tax > s.tax AND r.age > s.age). For the high selectivity distribution, however, we carefully assign a value proportional to the salary attribute to generate a small output for
the IEJoin on (r.salary < s.salary AND r.age > s.age).
We show the selectivity estimation for both distributions of
the age attribute in Tables 9 and 10. The tables report selectivity estimations when using input sample of size 1, 5, 10,
and 20 %. We determine the best join predicate pair for Q 
1
by selecting the predicate pair with the minimum overlapping blocks. According to Tables 9 and 10, the (salary, tax)
join predicate pair has the highest selectivity in all input
samples. Although the difference between predicates pairs
(salary, tax) and (salary, age) in Table 10 is relatively
small, we notice that it gets larger as we increase the size
of the sample input. To validate this observation, we execute
IEJoin on each join predicate pair and compare the output
size and runtime values.
We show in Fig. 14a that choosing (salary, tax) is the
right decision. In fact, the output size of (salary, tax) with
the low selectivity age attribute is at least two orders of mag-

Table 9 Q 
1 ’s selectivity estimation on 50M tuples Employees dataset
with low selectivity age attribute
Join predicates

Estimation on a % sample
(number of overlapping blocks)
1%

5%

10 %

20 %

Salary & tax

501

2535

5130

10K

Salary & age

125,250

3.1M

12M

50M

Tax & age

125,250

3.1M

12M

50M

Table 10 Q 
1 ’s selectivity estimation on 50M tuples Employees dataset
with high selectivity age attribute
Join predicates

Estimation on a % sample
(number of overlapping blocks)
1%

5%

10 %

20 %

Salary & tax

500

2545

5144

10K

Salary & age

503

2559

5237

11K

Tax & age

124750

3M

12M

50M

nitude lower than the results with other predicates, and it is
50 % lower than (salary, age) with the high selectivity age
attribute. In Fig. 14b, we show the runtime difference among
predicate pairs with the high selectivity age attribute. Join
predicate pair (salary, tax) is orders of magnitude faster than
predicate pair (tax, age), but only a couple of seconds faster
than predicate pair (salary, age). Although the performance
difference between (salary, tax) and (salary, age) pairs are
small in the dataset with high selectivity age, the IEJoin
query optimizer was able to detect that using (salary, tax)
is faster than using (salary, age). We also notice that the
runtime for the low selectivity age dataset is orders of mag-

123

144

Z. Khayyat et al.
(a)

(b)

Fig. 14 Result size and runtime for Q 
1 . a Result size. b Runtime

Table 11 Relation sizes, selectivity estimations and actual output for
individual joins in Q mw
Number of tuples

Table 12 Runtime of different multi-way join plans for Q mw
1st Join

Join

Estimation

Result size

R

2M

RS

388

576961

S

38M

ST

6.7M

31.5M

T

28M

TV

1999

876513

V

10M

VW

0.5M

12.5M

W

22M

–

–

–

nitude faster with predicate pair (salary, tax) compared with
other combinations.
Multi-way IEJoin We test the multi-way IEJoin optimization
in Algorithm 4 with the five relations in Q mw . Each relation in Q mw contains a random number of tuples from both
Employees and Events datasets. We summarize in Table 11
the size of each relation, the selectivity estimation (computed
with Algorithm 3 on 1 % sample size), and the join result size
based on the inequality conditions in Q mw . Table 11 shows
that the selectivity estimation is consistent with the actual
join result size; the lower the estimation the smaller the output
size and the higher the estimation the larger the output size.
Based on Algorithm 4, the optimal plan for Q mw is ((((R 
S)  T )  V )  W ) according to the selectivity estimations in Table 11. To evaluate the quality of the optimal plan,
we evaluate the performance of all multi-way IEJoin plans
for Q mw with PG-IEJoin, and report the results in Table 12.
Plan (((R  S)  T )  V )  W that was generated by
Algorithm 4 is indeed the fastest one. Although (R  S)
has higher selectivity than (T  V ), evaluating (T  V ) is
slightly faster than (R  S). Indeed (R  S) produces less
results compared to (T  V ) (Table 11), but since sorting
dominates the performance for IEJoin, as shown in Table 8,
(R  S) becomes slightly slower with a larger number of
tuples to sort (40M tuples) compared to (T  V ) (38M
tuples). Nevertheless, ((((R  S)  T )  V )  W )
remains the fastest plan because it eliminates more tuples
earliercompared to the rest.

3rd Join

4th Join

Total

RS

106s

T

77s

V

21s

W

53s

257s

ST

390s

R

41s

V

21s

W

51s

503s

V

67s

TV

VW

123

2nd Join

Join selectivity

103s

319s

S

108s

R

4s

W

51s

512s

W

55s

R

4s

516s

R

4s

W

53s

268s

W

58s

R

4s

273s

W

57s

S

106s

R

3s

269s

T

91s

S

108s

R

4s

522s

Optimizer experiments summary With only 1 % of the data,
selectivity estimation in Algorithm 3 is able to accurately
distinguish between high and low selective IEJoins. Note
that the 1 % sample size may not be applicable for other
datasets since the minimum sample size depends on the distribution of the join attributes. The performance degradation
of not selecting the optimal query execution plan for multipredicate and multi-way IEJoins varies between 5 % and
orders of magnitude performance drop. With negligible overhead, our selectivity estimation allows a DBMS to tune its
query optimizer to select the optimal plan for multi-predicate
and multi-way IEJoins.

8.8 Incremental IEJoin experiments
We developed the incremental algorithm in Sect. 5 using C++
IEJoin and tested it on Q 1 . Again, we do not report results
for Q 2 since they were similar to those of Q 1 . We used
five different implementations for the experiments in this
section: (1) Non-incremental (our original implementation),
(2) incremental non-batched sort-merge (ΔI nc), (3) incremental batched sort-merge (B-ΔI nc), (4) incremental with
the packed-memory array (PMA) data structure to dynamically maintain the cached input (P-ΔI nc), and (5) σ I nc
which is similar to B-ΔI nc but returns the full output. The
three variations of ΔI nc return results that correspond only

Fast and scalable inequality joins

145

Fig. 15 Runtime of ΔI nc, B-ΔI nc, non-incremental, and P-ΔI nc
on small update sizes

to the updates, while σ I nc generates results from both the
updates and the cached input.
Incremental IEJoin with small updates We first study the
advantage of our P-ΔI nc algorithm on small updates. In
Fig. 15, we compare the runtime of ΔI nc, B-ΔI nc, nonincremental, and P-ΔI nc by using 80M tuples as cached
input. We consider ΔI nc as the baseline for incremental
IEJoin. In Fig. 15, ΔI nc is twice faster than the nonincremental IEJoin for update of size 1. As the size of the
update increases, ΔI nc slows down because it processes each
update individually. To avoid this overhead, we use P-ΔI nc
to increase the efficiency of individual updates and B-ΔI nc
to process updates in a batched fashion. Since the update sizes
are relatively small, the runtime of both B-ΔI nc and the nonincremental algorithm across different update sizes remain
constant. Figure 15 shows that B-ΔI nc is always twice
faster than the non-incremental algorithm, while P-ΔI nc
is 30 % better than B-ΔI nc and three times faster than the
non-incremental with update of size 1. The processing overhead of P-ΔI nc is directly proportional to the update size,
and its performance declines as the update size increases.
Although P-ΔI nc is up to two orders of magnitude faster
than ΔI nc on higher update sizes, B-ΔI nc is better than PΔI nc on updates larger than 50. The limitation of P-ΔI nc
on updates larger than 50 is inherited from the design of
PMA which works better with individual updates. Note that
P-ΔI nc has an extra 60 % memory overhead, compared to
the non-incremental IEJoin, due to the extra empty spaces
maintained by the PMA.
(a)

(b)

Incremental IEJoin with large updates In these experiments,
we focus on B-ΔI nc and σ I nc since both ΔI nc and P-ΔI nc
do not work well with large updates. We show in Fig. 16a–c
a comparison between the runtime (without data loading) of
B-ΔI nc and non-incremental IEJoin with different update
and cached input sizes. In Fig. 16a, we start with 10M tuples
as cached input. B-ΔI nc is 40 % faster on the update of
size 1M tuples and has 5 times less output compared to
the non-incremental algorithm. For updates larger than 1M,
however, we notice that the performance of B-ΔI nc significantly drops as the difference in the output sizes between
the B-ΔI nc and the non-incremental becomes insignificant.
Similar behavior can be observed in Fig. 16b, c with cached
input of 20 and 30M tuples, respectively. ΔI nc is 50 and
30 % faster than the non-incremental algorithm on updates of
sizes 1M and 5M tuples, respectively, while its performance
drops on updates larger than 5M tuples in both cases. We
also notice that B-ΔI nc on the 1M tuples update has 10
times smaller output in Fig. 16b and 15 times smaller output in Fig. 16c compared to the non-incremental algorithm.
From the above three figures, B-ΔI nc clearly shows significant improvement over the non-incremental algorithm (up
to 50 %) with update sizes that generate significantly smaller
output. In this experimental setup, an efficient update size for
B-ΔI nc does not exceed 25 % of the cached input size.
We further tested the effect of update sizes on disk reading and memory consumption on B-ΔI nc when using 30M
tuples as cached input. Based on Fig. 17a, B-ΔI nc gains
up to 90 % performance increase in disk reading time compared to the non-incremental algorithm with small updates
sizes. The downside of the incremental algorithm is that it
has 60 % higher memory overhead compared to the nonincremental one, caused by data structures required to enable
fast IEJoin updates. We also compare the performance difference between B-ΔI nc and σ I nc in Fig. 17b using 30M
tuples as cached input, where the only difference between
them is the size of the output. σ I nc is at most 20 % slower
than B-ΔI nc when the output difference between them is
large on the 1M tuples update. However, the performance
gap between B-ΔI nc and σ I nc becomes negligible as the
output difference gets smaller on larger update sizes.

(c)

Fig. 16 Runtime and output size of non-incremental and B-ΔI nc IEJoin on big update sizes. a 10M cached input. b 20M cached input. c 30M
cached input

123

146

Z. Khayyat et al.
(a)

(b)

Fig. 17 Memory and I/O overhead of B-ΔI nc, and runtime and output size of B-ΔI nc and σ I nc (30M cached input). a Memory & I/O overhead.
b B-ΔI nc versus σ I nc

Incremental experiments summary When the update size
is smaller than 50, the PMA-based incremental algorithm
performs 30 % better than the sort-merge incremental algorithm and three times better than the original IEJoin. As we
increase the size of the updates, the batched sort-merge-based
incremental algorithm becomes more efficient than the PMAbased one. For update sizes that do not exceed 25 % of the
cached input size, the batched sort-merge-based incremental
algorithm is twice faster than the original algorithm.
8.9 Multi-node experiments
We now evaluate our proposal in a distributed environment
and by using larger datasets.
Scalable IEJoin versus baseline systems We should note that
we had to run these experiments on a cluster of 6 compute
nodes only due to the limit imposed by the free version of
the distributed PostgreSQL system. Additionally, in these
experiments, we stopped the execution of any system that
exceeded 24 hours. We test the scalable IEJoin using the
parallel IEJoin (with 6 threads in a single machine while
enabling disk caching) and the distributed IEJoin (on 6 compute nodes). Figure 18 shows the results of all distributed
systems we consider for queries Q 1 and Q 2 . This figure
shows again that both versions of our algorithm significantly
outperform all baselines. It is on average more than one
order of magnitude faster. In particular, we observe that only
DPG- GiST could terminate before 24 hours for Q 2 . The
distributed IEJoin is twice faster than the time required to
run GiST indexing alone. Moreover, distributed IEJoin is, as
(a)

Fig. 18 Distributed IEJoin (100M tuples, 6 nodes/threads). a Q 1 . b Q 2

123

expected, faster than the parallel multi-threaded version. This
is because the multi-threaded version has a higher processing
overhead due to resource contention. These results show the
high superiority of our algorithm over all baseline systems.
Scaling input size We further push the evaluation of the
efficiency in a distributed environment with bigger input
datasets: from 100M to 500M records with large results size
(Employees and Events), and from 1B to 6B records with
smaller results size (Employees2 and Events2). As we now
consider IEJoin only, we run this experiment on our entire 16
compute nodes cluster. Figure 19 shows the runtime results
as well as the output sizes. We observe that IEJoin gracefully scales along with input dataset size in both scenarios.
We also observe in Fig. 19a that, when the output size is large,
the runtime increases accordingly as it is dominated by the
materialization of the results. In Fig. 19a, Q 1 is slower than
Q 2 as its output is three orders of magnitude larger. When
the output size is relatively small, both Q 1 and Q 2 scale well
with increasing input size (see Fig. 19b). Below, we study in
more details the impact of the output size on performance.
Scaling dataset output size We test our system’s scalability
in terms of the output size using two real datasets (MDC
and Cloud) as shown in Fig. 20. To have full control on this
experiment, we explicitly limit the output size from 4.3M to
430M for MDC, and 20.8M to 2050M for Cloud. The figures clearly show that the output size affects runtime; the
larger the output size, the longer it will take to produce them.
They also show that materializing a large number of results
is costly. Take Fig. 20a for example, when the output size is
small (i.e., 4.3M), materializing them or not will have simi(b)

Fast and scalable inequality joins

147
(a)

(b)

Fig. 19 Distributed IEJoin, 6B tuples, 16 nodes. a Employees & Events. b Employees2 & Events2
(a)

(b)

Fig. 20 Runtime of IEJoin (c = 10). a MDC - Q 3 . b Cloud—Q 4
(a)

(b)

Fig. 21 Without result materialization (c = 5, 10). a Runtime. b Output size

lar performance. However, when the output size is big (i.e.,
430M), materializing the results takes almost 2/3 of the entire
running time.
In order to run another set of experiments with a much
bigger output size, we created two variants of Q 3 for MDC
dataset by keeping only two predicates over four (less selectivity). Figure 21 shows the scalability results of these
experiments with no materialization of results. For Q 3a ,
IEJoin produced more than 1, 000B records in less than
3, 000 seconds. For Q 3b , we stopped the execution after 2
(a)

hours with more than 5, 000B tuples in the temporary result.
This demonstrates the good scalability of our solution.
Speedup and scaleup We also test speedup and scaleup efficiency of the distributed IEJoin by using Employees2 dataset
and query Q 1 . Figure 22a shows that our algorithm has
outstanding speedup thanks to the scalability optimizations.
IEJoin was only 4, 3 and 16 % slower than the ideal speedup
when processing 8B rows on 4, 8 and 16 workers respectively. Figure 22b shows the scaleup efficiency of IEJoin as
we proportionally increase the cluster size and input size.
(b)

Fig. 22 Speedup (8B rows) and Scaleup on Q 1 . a Speedup. b Scaleup

123

148

We observe that distributed IEJoin also has good scaleup: on
4 workers (2B rows) and 8 workers (4B rows) it was only
5 and 20 % slower than the ideal scaleup. However, due to
the increase in dataset size, the sorting overhead in IEJoin
becomes larger. This explains why scalable IEJoin, on 16
workers with 8B rows input, is 46 % slower than the ideal
scaleup.
Multi-node summary Similarly to the centralized case,
IEJoin outperforms existing baselines by at least one order of
magnitude. In particular, we observe that it gracefully scales
in terms of input (up to 6B tuples). This is because our algorithm first join the metadata, which are orders of magnitude
smaller than the actual data. As a result, it shuffles only those
data partitions that can potentially produce join results. Typically, IEJoin processes a small number of data partitions.

9 Related work
Several cases of inequality joins have been studied in the literature; these include band joins, interval joins and, more
generally, spatial joins. IEJoin is specially optimized for
joins with at least two predicates in {“<”, “>”, “≤”, “≥”}.
A band join [13] of two relations R and S has a join
predicate that requires the join attribute of S to be within
some range of the join attribute of R. The join condition is
expressed as R.A − c1 ≤ S.B & S.B ≤ R.A + c2 , where
c1 and c2 are constants. The band-join algorithm [13] partitions the data from relations R and S into partitions Ri and
Si respectively, such that for every tuple r ∈ R, all tuples of
S that join with r appear in Si . It assumes that Ri fits into
memory. Contrary to IEJoin, band join is limited to a single
inequality condition type, involving one single attribute from
each column. IEJoin works for any inequality conditions and
attributes from the two relations. While band join queries can
be processed using our algorithm, not all IEJoin queries can
be reduced to band join.
Interval joins are frequently used in temporal and spatial
data. The work in [17] proposes the use of the relational
Interval Tree to optimize joining interval data. Each interval intersection is represented by two inequality conditions,
where the lower and upper times of any two tuples are
compared to check for overlaps. This work optimizes nonequijoins on interval intersections, where they represent each
interval as a multi-value attribute. Compared to our work,
they only focus on improving interval intersection queries
and cannot process general-purpose inequality joins.
Spatial indexing is widely used in several applications
with multidimensional datasets, such as bitmap indices [8,
30], R-trees [21] and space filling curves [7]. In PostgreSQL,
support for spatial indexing algorithms is provided through a
single interface known as Generalized index Search Tree [22]

123

Z. Khayyat et al.

(GiST). From this collection of indices, bitmap index is the
most suitable technique to optimize multiple attribute queries
that can be represented as 2-dimensional data. Examples of
2-dimensional datasets are intervals (e.g., start and end time
in Q 2 ), GPS coordinates (e.g., Q 3 ), and any two numerical
attributes that represent a point in an XY plot (e.g., salary and
tax in Q 1 ). The main disadvantage of the bitmap index is that
it requires large memory footprint to store all unique values
of the composite attributes [9,36]. Bitmap index is a natural
baseline for our algorithm, but, unlike IEJoin, it does not perform well with high cardinality attributes, as demonstrated
in Fig. 8. R-trees, on the other hand, are not suitable because
an inequality join corresponds to window queries that are
unbounded from two sides, and consequently intersect with
a large number of internal nodes of the R-tree, generating
unnecessary disk accesses.
The patent in [32] also presents an algorithm to optimize
the Cartesian product when joining two tables based on a single inequality condition. The algorithm partitions the input
relations into smaller blocks based on the value distribution
and min/max values of the join predicate. It then applies
Cartesian product on a subset of the input partitions, where
it eliminates unnecessary partitions depending on the join
condition. Compared with our approach, this algorithm optimizes the Cartesian product through partitioning based on
only one single inequality join predicate.
Several other proposals have been made to speedup join
executions in MapReduce (e.g., [14]). However, they focus
on joins with equalities thus requiring massive data shuffling to be able to compare each tuple with each other. There
have been few attempts to devise efficient implementation
of theta-join in MapReduce [33,38]. [33] focuses on pairwise theta-join queries. It partitions the Cartesian product
output space with rectangular regions of bounded sizes. Each
partition is mapped to one reducer. The proposed partitioning guarantees correctness and workload balance among the
reducers while minimizing the overall response time. [38]
further extends [33] to solve multi-way theta-joins. It proposes an I/O and network cost-aware model for MapReduce
jobs to estimate the minimum time execution costs for all
possible decomposition plans for a given query, and selects
the best plan given a limited number of computing units and
a pool of possible jobs. We propose a new algorithm to do the
actual inequality join based on sorting, permutation arrays,
and bit-arrays. The focus in these previous proposals is on
efficiently partitioning the output space and on providing a
cost model for selecting the best combination of MapReduce
jobs to minimize response time. In both proposals, join is
performed with existing algorithms, where inequality conditions correspond to Cartesian product followed by selection.
A large number of approaches focused on selectivity
estimation. However, most existing work on selectivity estimation has focused on equijoins [2,23,29,34,35]. There are

Fast and scalable inequality joins

few proposals for the general case of theta-join [38] and spatial join [31]. Nevertheless, most of these proposals estimate
the selectivity of the inequality join to be O(n 2 ) because it is
evaluated as Cartesian product. IEJoin significantly differs
from these works as it does not consider Cartesian product.
It uses an efficient selectivity estimation technique that computes the number of overlapping sorted blocks obtained from
a sample of the input relations.

10 . . . The end
To help Bob with his inequality join queries, we proposed
novel algorithms for efficiently evaluating these queries. We
rely on auxiliary data structures that enable efficient computations and require a small memory footprint. Our algorithms
exploit data locality to achieve orders of magnitude computation speedup. We introduced selectivity estimation to
support multi-predicate and multi-way join queries. We
devised incremental versions to deal with continuous queries
on changing data. We implemented these algorithms on both
a centralized and a distributed system, namely PostgreSQL
and Spark SQL, respectively. We additionally implemented
IEJoin in Nadeef, an open-source data cleaning system. Our
experiments demonstrate that IEJoin is superior to baseline
systems: it is 1.5 to 3 orders of magnitude faster than commercial and open-source centralized databases and at least
2 orders of magnitude faster than the original Spark SQL.
While the algorithm does not break the theoretical quadratic
time bound, our experiments show performance results that
are proportional to the size of the output.
Acknowledgements Portions of the research in this paper used the
MDC Database made available by Idiap Research Institute, Switzerland
and owned by Nokia.

References
1. Abiteboul, S., Hull, R., Vianu, V.: Foundations of Databases.
Addison-Wesley, Reading (1995)
2. Afrati, F.N., Ullman, J.D.: Optimizing joins in a map-reduce environment. In: EDBT, pp. 99–110 (2010)
3. Agrawal, D., Chawla, S., Elmagarmid, A.K., Ouzzani, Z.K.M.,
Papotti, P., Quiané-Ruiz, J., Tang, N., Zaki, M.J.: Road to freedom
in big data analytics. In: EDBT, pp. 479–484 (2016)
4. Armbrust, M., Xin, R.S., Lian, C., Huai, Y., Liu, D., Bradley, J.K.,
Meng, X., Kaftan, T., Franklin, M.J., Ghodsi, A., Zaharia, M.:
Spark SQL: relational data processing in spark. In: SIGMOD, pp.
1383–1394 (2015)
5. Bender, M.A., Hu, H.: An adaptive packed-memory array. TODS
32(4) 26:1–26:43 (2007)
6. Bohannon, P., Fan, W., Geerts, F., Jia, X., Kementsietsidis, A.:
Conditional functional dependencies for data cleaning. In: ICDE,
pp. 746–755 (2007)
7. Böhm, C., Klump, G., Kriegel, H.-P.: XZ-Ordering: A space-filling
curve for objects with spatial extension. In: SSD, pp. 75–90 (1999)

149
8. Chan, C.-Y., Ioannidis, Y. E.: Bitmap index design and evaluation.
In: SIGMOD, pp. 355–366 (1998)
9. Chan, C.-Y., Ioannidis, Y.E.: An efficient bitmap encoding scheme
for selection queries. In: SIGMOD, pp. 215–226 (1999)
10. Chu, X., Ilyas, I.F., Papotti, P.: Holistic data cleaning: putting violations into context. In: ICDE, pp. 458–469 (2013)
11. Dallachiesa, M., Ebaid, A., Eldawy, A., Elmagarmid, A., Ilyas,
I.F. Ouzzani, M., Tang, N.: NADEEF: a commodity data cleaning
system. In: SIGMOD (2013)
12. Dean, J., Ghemawat, S.: MapReduce: Simplified data processing
on large clusters. Commun. ACM 51(1), 107–113 (2008)
13. DeWitt, D.J., Naughton, J.F., Schneider, D.A.: An evaluation of
non-equijoin algorithms. In: VLDB, pp. 443–452 (1991)
14. Dittrich, J., Quiané-Ruiz, J., Jindal, A., Kargin, Y., Setty, V., Schad,
J.: Hadoop++: making a yellow elephant run like a cheetah (without
it even noticing). PVLDB 3(1), 515–529 (2010)
15. Ebaid, A., Elmagarmid, A.K., Ilyas, I.F., Ouzzani, M., QuianéRuiz, J., Tang, N., Yin, S.: NADEEF: a generalized data cleaning
system. PVLDB 6(12), 1218–1221 (2013)
16. Elmagarmid, A.K., Ilyas, I.F., Ouzzani, M., Quiané-Ruiz, J., Tang,
N., Yin, S.: NADEEF/ER: generic and interactive entity resolution.
In: SIGMOD, pp. 1071–1074 (2014)
17. Enderle, J., Hampel, M., Seidl, T.: Joining interval data in relational
databases. In: SIGMOD, pp. 683–694 (2004)
18. Gao, D., Jensen, C.S., Snodgrass, R.T., Soo, M.D.: Join operations
in temporal databases. VLDB J. 14(1), 2–29 (2005)
19. Garcia-Molina, H., Ullman, J.D., Widom, J.: Database Systems.
Pearson Education (2009)
20. Govindaraju, N.K., Gray, J., Kumar, R., Manocha, D.: GPUTeraSort: high performance graphics co-processor sorting for large
database management. In: SIGMOD, pp. 325–336 (2006)
21. Guttman, A.: R-trees: a dynamic index structure for spatial searching. In: SIGMOD, pp. 47–57 (1984)
22. Hellerstein, J.M., Naughton, J.F., Pfeffer, A.: Generalized search
trees for database systems. In: VLDB, pp. 562–573 (1995)
23. Kemper, A., Kossmann, D., Wiesner, C.: Generalised hash teams
for join and group-by. In: VLDB, pp. 30–41 (1999)
24. Khayyat, Z., Ilyas, I.F., Jindal, A., Madden, S., Ouzzani, M.,
Papotti, P., Quiané-Ruiz, J.-A., Tang, N., Yin, S.: BigDansing: a
system for big data cleansing. In: SIGMOD, pp. 1215–1230 (2015)
25. Khayyat, Z., Lucia, W., Singh, M., Ouzzani, M., Papotti, P., QuianéRuiz, J.-A., Tang, N., Kalnis, P.: Lightning fast and space efficient
inequality joins. PVLDB 8(13), 2074–2085 (2015)
26. Kiukkonen, N., Blom, J., Dousse, O., Gatica-Perez, D., Laurila, J.:
Towards rich mobile phone datasets: lausanne data collection campaign. In: ICPS (2010)
27. Knuth, D. E.: The Art of Computer Programming, Volume III:
Sorting and Searching. Addison-Wesley, Reading (1973)
28. Laurila, J.K., Gatica-Perez, D., Aad, I., Bornet, O., Do, T.-M.-T.,
Dousse, O., Eberle, J., Miettinen, M.: The mobile data challenge:
big data for mobile computing research. In: Pervasive Computing
(2012)
29. Lohman, G., Mohan, C., Haas, L., Daniels, D., Lindsay, B.,
Selinger, P., Wilms, P.: Query processing in R*. In: Query Processing in Database Systems, pp. 31–47 (1985)
30. Lopes Siqueira, T.L., Ciferri, R.R., Times, V.C., de Aguiar Ciferri,
C.D.: A spatial bitmap-based index for geographical data warehouses. In: SAC, pp. 1336–1342 (2009)
31. Mamoulis, N., Papadias, D.: Multiway spatial joins. TODS 26(4),
424–475 (2001)
32. Morris, J., Ramesh, B.: Dynamic Partition Enhanced Inequality
Joining Using a Value-count Index, 1 2011. US Patent 7,873,629
B1
33. Okcan, A., Riedewald, M.: Processing theta-joins using MapReduce. In: SIGMOD, pp. 949–960 (2011)

123

150
34. Schneider, D.A., DeWitt, D.J.: A performance evaluation of four
parallel join algorithms in a shared-nothing multiprocessor environment. In: SIGMOD (1989)
35. Selinger, P.G., Astrahan, M.M., Chamberlin, D.D., Lorie, R.A.,
Price, T.G.: Access path selection in a relational database management system. In: SIGMOD, pp. 23–34 (1979)
36. Stockinger, K., Wu, K.: Bitmap indices for data warehouses. Data
Wareh OLAP Concepts Archit Solut 5, 157–178 (2007)

123

Z. Khayyat et al.
37. Zaharia, M., Chowdhury, M., Franklin, M.J., Shenker, S., Stoica,
I.: Spark: cluster computing with working sets. In: HotCloud, pp.
10–10 (2010)
38. Zhang, X., Chen, L., Wang, M.: Efficient multi-way theta-join
processing using MapReduce. PVLDB 5(11), 1184–1195 (2012)

8

Editorial

The term provenance refers, broadly, to information about the origin, context, derivation, lineage, ownership, or history of some artifact. The provenance of data is more
specifically a form of structured metadata that records the activities involved in data
production. Data provenance applies to a broad variety of data types, from database
records, to scientific datasets, to business transaction logs, web pages, social media
messages, and more. At the same time, different definitions and measures of quality
apply to each of these data types in different domains. While the relationship between
quality and quality assessment methods have been widely explored in this journal over
the past few years, we felt that the relationship between provenance, the quality of
data, and quality assessment methods deserved further study.
This special issue of JDIQ is an attempt to formally investigate the multiple facets
of this relationship. The call for articles for this issue was an attempt to identify a set
of key questions and broad areas around the concepts of provenance and data quality.
Quality through provenance: Is it possible to formalize the intuition that knowledge of
the history of a dataset can be instrumental to assessing the quality of the underlying
data? How general can such a formal model be? What would a formal definition of
provenance-based information quality metrics look like?
Quality of provenance: Next, and complementary to the previous point, are there useful
metrics and methods for describing and assessing the quality or information content
of data provenance itself ?
Quality of query results: Third, how can the theory of data provenance be used to better
understand the results of a (relational) database query?
Quality improvement: Finally, can provenance be used to inform interventions to improve upon the quality of a dataset? Also, are there interesting application use cases
associated with each of these questions?
In response to our call, we received nine submissions and selected four articles, following two rounds of highly qualified peer reviews. Three of the articles directly answer
some of the previous questions. The first article presents a model for using provenance
in data quality assessment, specifically of Linked Data. The second presents a framework for assessing the quality of provenance itself. The third includes a formal model
for explaining missing answers from relational queries using data provenance. The
fourth addresses a common infrastructure problem, namely, how to embed references
to provenance within the data.
In the first article in this collection, QUAL: A Provenance-Aware Quality Model,
Baillie, Edwards, and Pignotti address one of the core themes of this special issue,
namely, how knowledge of the provenance of a dataset can help assess the quality of
that dataset. Their contribution is a data quality assessment model, Qual-DM, and associated software framework, which relies on a variety of contextual metadata around
the data—including provenance. Furthermore, the software generates the provenance
of the quality assessment activity itself.
Qual-DM is aligned with and extends PROV, the W3C recommendation for a conceptual model of provenance. Qual-DM concepts are formally defined as classes in the
c 2015 ACM 1936-1955/2015/02-ART8 $15.00

DOI: http://dx.doi.org/10.1145/2692312

ACM Journal of Data and Information Quality, Vol. 5, No. 3, Article 8, Publication date: February 2015.

8:2

Editorial

OWL2 Description Logic, and inference rules are introduced to describe how certain
concepts can be derived from others. The Qual-O operational model, an extension of
the W3C PROV-O ontology, realizes the formal model and provides a concrete foundation for the Qual quality assessment service. The service functionality is evaluated on
datasets from two case studies, namely, for a public transport passenger information
service and for an “invasive species monitoring” scenario. The authors also show that
the functionality of the Qual model compares favorably with those of other quality
assessment models from the literature and present performance data on the OWL
reasoning tasks associated with the assessment.
In the next article, Provenance Quality Assessment Methodology and Framework, Cheah and Plale address a complementary problem. The authors note that
assessing the quality of provenance itself should be a primary concern, if provenance is
to be used reliably in data quality assessment. Following this observation, the authors
focus on provenance traces that describe the execution of complex, long-running, and
possibly distributed processes. They observe that collecting provenance traces for these
processes is error-prone, due to the distributed and possibly heterogeneous nature of
the underlying computing infrastructure. Errors can be introduced in the provenance
trace that describes complex, long-running processes at various stages during its lifetime, namely, during capture, storage, query, and “stitching.” The latter is the process
of collecting multiple provenance fragments from independent systems and joining
them together. When the traces are collected through unreliable message passing after
process execution, provenance traces may also be incomplete. Finally, independently
collected trace fragments may be temporally inconsistent, that is, relative to the constraints defined in the PROV-CONSTRAINT specification.
With this motivation, the article describes a framework for the analysis of the quality of the provenance itself, along three of the typical “dimensions” cited in information quality management: correctness (the extent to which provenance is error free),
completeness (the extent to which the trace completely describes, as a set level of abstraction, the underlying process execution), and relevance. The latter is more of a
subjective dimension having to do with the usefulness of the provenance trace given a
target analytical task.
The article employs different techniques, mostly heuristic in nature, to detect likely
anomalies in a collection of provenance fragments, to detect temporal constraint violations, and to estimate the completeness of a provenance graph. A score model that
combines these estimates is then proposed as an overall measure of quality for a complex provenance graph.
The framework is demonstrated on one synthetic dataset, previously developed by
the authors, as well as two real-world provenance datasets, a data-ingesting workflow
from NASA (AMSR-E), and a previously published dataset for data integration in the
tourism domain.
Our third question was how to use provenance to improve the quality of query results.
This challenge is tackled by the third article, A Hybrid Approach to Answering
Why-Not Questions on Relational Query Results. In this work, Melanie Herschel
tackles the problem of explaining missing answers in results of queries on relational
databases. While traditional lineage techniques provide an explanation for data that is
in the query output (e.g., a set of justifying source tuples), missing-answer explanations
provide explanations for data that is not in the output. This information is useful to
improve the quality of the source data, as it can drive the users’ attention on the part
of the data where the errors are most likely to be.
The author reviews related techniques by dividing them into two categories, those
that are based on highlighting missing data (i.e., source data that is needed to make
the missing answer appear) and those that propose query changes to make the missing
ACM Journal of Data and Information Quality, Vol. 5, No. 3, Article 8, Publication date: February 2015.

Editorial

8:3

answer appear in the output. Since both have limitations with regard to completeness or
conciseness, the proposed hybrid approach subsumes previous techniques by allowing
highlighting both missing data and relevant parts of queries. This intuition is captured
in a framework and the existing techniques are modeled as instances of the framework.
The formalism makes use of conditional tuples (c-tuples) to keep track of conditions
relevant to the execution of a query on a tuple, as it has proven to be effective for this line
of research. The main algorithm computes hybrid explanations in four steps: computing
a generic witness, annotating the witness to identify parts of the query that may block
tuples, computing derivations, and computing explanations. An experimental study
shows that the proposed cost metric helps in identifying the explanations that are
valuable for human interpretation, and that the algorithms have competitive runtime
compared to the state-of-the-art systems.
Self-Identifying Data for Fair Use explores the association between data and
its provenance. Chong, Skalka, and Vaughan start from the observation that research
data repositories routinely state some kind of fair use policy on the datasets they host.
However, they report on anecdotal evidence of routine violations of fair use policy,
despite research scientists being generally well intentioned. The authors argue that
ensuring a persistent coupling between datasets and their provenance information,
during normal scientific usage of the data, may help alleviate the problem.
Their approach to establishing such a permanent and reliable association involves a
new watermarking technique, whereby a short metadata payload is embedded within
the dataset.
Watermarking has been widely used to facilitate copyright protection, typically on
images, using steganography techniques: an invisible signature is embedded in the
main image, in such a way that (1) it can be retrieved, provided one has access to
details of the algorithm used for the embedding, and (2) it cannot otherwise be easily
detected or erased.
It is well known that watermarking inevitably alters the data. In typical imaging applications, this is normally acceptable. With scientific data, however, any change to the
data may render it unusable or invalid for experimental purposes. Thus, the main technical challenge faced in this research involves finding a watermarking scheme that does
not interfere with normal scientific use. The main insight is to use the least significant
bits of the numerical data in the file in order to minimize the effect of data corruption
when the data is used in calculations. The approach is tested and validated on a real
environmental research dataset.
The embedding of provenance metadata is only one of the many techniques made
possible by this general approach. One limitation is the size of the metadata payload
that can be embedded, however. Thus, an ideal application scenario for this technique
involves storing the provenance trace itself in a separate repository and embedding a
reference to the trace inside the data.
The systematic collection, analysis, and usage of data provenance are only now entering the mainstream, bringing both opportunities and new challenges. We highlight
two such challenges. The first relates to an understanding of the cost and benefit
framework around the collection of provenance data. The cost of collecting provenance
data depends both on the characteristics of the data (its structure, size, heterogeneity) and on the data management environment that is responsible for its production
and consumption (e.g., automated vs. manual data processing, structured queries vs.
search interfaces, etc.). A comprehensive framework must be developed to investigate
the tradeoffs between the cost of collecting and managing provenance and its benefits
vis-à-vis quality assessment and improvement.
The second challenge revolves around novel applications of data provenance. While
the primary motivation to obtain provenance data was to improve the quality of query
ACM Journal of Data and Information Quality, Vol. 5, No. 3, Article 8, Publication date: February 2015.

8:4

Editorial

results, we are witnessing novel practices, such as providing clear and concise explanations for problems. An example is using provenance in the case where the result of
a query includes values that appear to be outliers; the explanation may involve an unusually high price for specific items. The data quality tools that are available to detect
errors in the data are not adequate to provide an explanation for these occurrences,
but provenance-enabled systems today can compute their lineage across data sources.
Can these systems be extended to provide more useful explanations of those errors? An
example may be the ability to identify processes or employees that are more likely to
produce erroneous data values. This kind of concise description would provide better
insight to the users but cannot be easily determined without access to provenance data.
The opportunities for exploiting data provenance in this context have not been fully
explored.
We wish to thank Editor in Chief Louiqa Raschid and Senior Associate Editor Felix Naumann for their support and guidance throughout this process, and Ms. Janet
Cavanagh, administrative assistant for the journal, for her help in coordinating this
special issue.
Finally, we thank the following reviewers who donated their time and offered their
expertise to make this issue possible. Their contribution is greatly appreciated.
Khalid Belhajjame, University Paris - Dauphine
Domenico Beneventano, Universita degli Studi di Modena e Reggio Emilia
Laure Berti-Equille, Qatar Computing Research Institute
James Cheney, University of Edinburgh
Victor Cuevas, Universidad de Guadalajara, Mexico
Tom De Nies, Ghent University
Helena Galhardas, University of Lisbon
Ashish Gehani, SRI International
Boris Glavic, Illinois Institute of Technology
Paul Groth, Elsevier
Olaf Hartig, University of Waterloo
Scott Jensen, San Jose State University
Andrea Maurino, Universita degli Studi di Milano-Bicocca
Gianni Mecca, Universita della Basilicata
Renee Miller, University of Toronto
Luc Moreau, University of Southampton
Mourad Ouzzani, Qatar Computing Research Institute
Jaehong Park, The University of Texas at San Antonio
Dr. Paolo Missier
Newcastle University
Dr. Paolo Papotti
Qatar Computing Research Institute
Guest Editors

ACM Journal of Data and Information Quality, Vol. 5, No. 3, Article 8, Publication date: February 2015.

IQ-M ETER – An Evaluation Tool for
Data-Transformation Systems
Giansalvatore Mecca 1 ,
1

Paolo Papotti 2 ,

Donatello Santoro 1,3

Università della Basilicata – Potenza, Italy
giansalvatore.mecca@gmail.com
2
QCRI – Doha, Qatar
ppapotti@qf.org.qa
3
Università Roma Tre – Roma, Italy
donatello.santoro@gmail.com

Abstract—We call a data-transformation system any system
that maps, translates and exchanges data across different representations. Nowadays, data architects are faced with a large
variety of transformation tasks, and there is huge number of
different approaches and systems that were conceived to solve
them. As a consequence, it is very important to be able to
evaluate such alternative solutions, in order to pick up the right
ones for the problem at hand. To do this, we introduce IQM ETER, the first comprehensive tool for the evaluation of datatransformation systems. IQ-M ETER can be used to benchmark,
test, and even learn the best usage of data-transformation tools.
It builds on a number of novel algorithms to measure the quality
of outputs and the human effort required by a given system, and
ultimately measures “how much intelligence” the system brings
to the solution of a data-translation task.

I. I NTRODUCTION
The problem of translating data among heterogeneous
representations is a long standing issue in the IT industry and
in database research. The first data translation systems date
back to the seventies. In these years, many different proposals
have emerged to alleviate the burden of manually expressing
complex transformations among different repositories, so that
today we have a very broad class of systems conceived for
a variety of purposes, including schema-matching, schemamappings and data-exchange, data-integration, ETL (ExtractTransform-Load), object-relational mapping, data-fusion, and
data-cleaning, to name a few.

(i) a definition of a data-transformation system that is sufficiently general to capture a wide class of tools – including
research schema-mapping systems, commercial mapping systems and transformation-editors, and ETL tools – and at the
same time tight enough for the purpose of our evaluation;
(ii) a new fast algorithm to measure of the quality of the
outputs produced by a data translation tool on a given mapping
scenario;
(iii) a natural definition and measure of the user-effort needed
to achieve such quality.
Ultimately, these techniques allow us to measure the level
of intelligence of a data-transformation tool, defined as the
ratio between the quality of the outputs generated by the
system on a given task, and the amount of user effort required
to generate them. The lower the effort required to obtain results
of the best quality with a system, the higher its IQ. IQM ETER builds on these techniques, and provides sophisticated
functionalities for the evaluation of data transformation tools.
IQ-M ETER in Action IQ-M ETER is used to run evaluation
sessions on data-transformation systems (DTSs). In our view,
a DTS is any tool capable of executing transformation scenarios (also called mapping scenarios or translation tasks). An
evaluation session may consider a single DTS, or several DTS
that need to be compared to each other.

The number of approaches is justified by the ample variety
of scenarios that data architects need to deal with. For some of
them the declarative techniques proposed in the literature are
rather effective; others are such that only procedural and more
expressive systems, like those used in ETL, are of real help.
Therefore, a crucial design decision that may strongly impact
the success of a project is to pick up the right tool for a given
translation task.

Regardless of the way in which transformations are expressed, in our setting transformation scenarios require to
translate instances of one or more source schemas into instances of a target schema. The transformation system is seen
as a black box, of which we are only interested in the inputoutput behavior. For each transformation task, inputs to the
system are: (a) the source schemas; (b) the target schema; (c)
the source instances that need to be translated.

To answer this question, we must be able to compare
and classify systems coming from different inspirations and
different application domains. Surprisingly enough, very little
work has been conducted in this direction. In fact, only a few
early benchmarks exist [1], [2], [3], and they are rather limited
in scope, both in terms of the class of systems they consider,
and of the aspects they evaluate.

We do not constrain the way in which the translation is
expressed by the DTS, as long as it is input by the user through
a graphical user-interface and then stored on the disk as a
scenario file using some readable format (typically, systems
store the transformation as an XML file). To handle a DTS,
it needs a driver, i.e., a module capable of parsing scenario
files generated by a DTS. A snapshot of the system in action
on three different systems – ++S PICY, Altova MapForce, and
CloverETL, is shown in Figure 1.

IQ-M ETER is the first comprehensive tool for the evaluation of data-transformation systems conceived to address this
concern. It can facilitate data architects’ work on complex
scenarios with both commercial and research systems. It is
based on three core ideas introduced in [4]:

978-1-4799-2555-1/14/$31.00 © 2014 IEEE

1218

Given a scenario, an IQ-M ETER evaluation session must
also specify an expected output, i.e., a target instance that is
considered as the right output for the given transformation, as

ICDE Conference 2014

Fig. 1: IQ-M ETER in action

it will be discussed in more detail in the next section. The
session consists of a sequence of executions of the various
DTSs, in which the user progressively refines a specification
of the intended transformation using the DTS GUI, and then
runs the DTS engine to actually translate the data. IQ-M ETER
measures the quality of the outputs and the amount of effort
for the different executions using the algorithms introduced in
[4], and dynamically shows a detailed evaluation report about
the session. The most prominent feature of this report is a
quality-effort graph, that shows how the quality achieved by
a DTS varies with different specifications of the mapping,
and therefore with different levels of efforts, as shown in
Figure 1. Intuitively, the smaller is the area below the plot
for a given system, the lesser is the effort required to achieve
high quality outputs. Higher quality with lesser effort means
higher effectiveness for the given task, and ultimately “more
intelligence”.
We introduce the main techniques behind the system in the
next sections. In the meanwhile, we want to emphasize that
IQ-M ETER is a unique tool for evaluating data transformation
systems. In its most basic form, it can be used as a test tool
for new DTSs, in order to measure the correctness of the
transformations expressible with the system. However, it has
a number of other interesting applications, as follows:
(a) when used on existing benchmarks, like [2], [3], it brings
two significant advantages; on the one side, it is general
and extensible, since it allows to mix and compare systems
with different inspirations, like, for example, schema-mapping
tools and ETL tools; in addition, previous benchmarks only
considered the following typical question: “is this tool capable
of expressing this scenario?” on the contrary, IQ-M ETER
provides deeper insights about the effectiveness of a DTS
through its quality-effort graphs, to the extent that it gives
a clear indication about which system is the most appropriate
for the given scenario;
(b) but, even more important in our vision, it represents
the first example of a learning tool for data-transformation

1219

systems. In fact, it supports learning in two ways. On the one
side, it may help users to approach the adoption of a new
system, by presenting them with simple scenarios, asking them
to provide some initial specification of the transformation,
and dynamically showing how this increases of decreases the
quality of the input;
(c) on the other side, it can be used to challenge expert users to
find different and more compact ways – i.e., ways that require
less effort – to express a transformation without lowering the
quality of the efforts.
We believe that this kind of evaluation provides precious
insights in the vast and heterogeneous world of transformation
systems, and may lead to a better understanding of its different
facets. In a more general sense, IQ-M ETER represents a concrete way to investigate the trade-offs between declarative and
procedural approaches, a foundational problem in computer
science.
II. OVERVIEW
In this section we introduce the main building blocks of
our approach, namely: the quality measure, and the user-effort
measure. For further details we refer the reader to [4].
The Quality Measure To achieve a good level of generality,
we assume a nested-relational data model for the source and
target databases, capable of handling both relational data and
XML trees. Given a transformation task, we assume that a
“gold standard”, has been fixed for each scenario in terms of
the output instance expected from the translation. To be able
to dynamically update reports during an evaluation session, a
key requirement of our approach is the availability of a fast
algorithm to measure the similarity of the outputs of a DTS wrt
this expected instance. Given the nested relational data model,
our instances are trees. Figure 2 shows an example. Indeed,
while there are many existing similarity measures for trees, it
is important to emphasize that none of these can be used in
this framework, for the following reasons:

Fig. 2: Comparing Instances

(i) we want to perform frequent and repeated evaluations of
each tool, for each selected scenario and mapping specifications of different complexity. Also, we want to be able to
work with possibly large instances, to measure how efficient is
the transformation generated by a system. As a consequence,
we cannot rely on known tree-similarity measures, like, for
example, edit distances, that are typically quite expensive;
(ii) the problem above is even more serious, if we think that
our instances may be seen as graphs, rather than trees; we need
in fact to check key-foreign key references, that can be seen
as additional edges, thus making each instance a fully-fledged
graph; graph edit distance is knowingly much more complex
than tree edit distance;
(iii) even though we were able to circumvent the complexity
issues, typical tree and graph-comparison techniques would not
work in this setting. To see this, consider that it is typical in
mapping applications to generate synthetic values in the output
– these values are called labeled nulls in data-exchange and
surrogate keys in ETL. In Figure 2, values D1, D2, I1, I2, I3
are of this kind. These values are essentially placeholders used
to join tuples, and their actual values do not have any business
meaning. We therefore need to check if two instances are
identical up to the renaming of their synthetic values.
It can be seen that we face a very challenging task: devising
a new similarity measure for trees that is efficient, and at the
same time precise enough for the purpose of our evaluation.
In order to do this, we introduce the following key-idea: since
the instances that we want to compare are not arbitrary trees,
but rather the result of a transformation, they are supposed
to exhibit a number of regularities; as an example, they are
supposedly instances of a fixed nested schema, that we know
in advance. This means that we know: (a) how tuples in the
instances must be structured; (b) how they should be nested
into one another; (c) in which ways they join via key-foreign
key relationships.
We design our similarity metric by abstracting these features of the two trees in a set-oriented fashion, and then
compare these features using precision, recall and ultimately Fmeasures to derive the overall similarity. More specifically, for
each instance, we compute: (i) first a set of tuple identifiers,
also called local identifiers, one for each tuple in the instance;
(ii) a set of nested tuple identifiers, called global identifiers,
that capture the nesting relationships among tuples; (iii) a set
of pairs of tuple identifiers, called join pairs, one for each tuple
t1 that joins a tuple t2 via a foreign key. Tuple identifiers and
join pairs for our example are reported in Figure 2.
It is worth noting that the computation of tuple identifiers
requires special care. As it can be seen in the Figure, we
keep these values out of our identifiers, in such a way that

1220

two instances are considered to be identical provided that they
generate the same tuples and the same join pairs, regardless
of the actual synthetic values generated by the system.
Based on these ideas, to simplify the treatment we may say
that, both for the generated instance and for the expected one,
we generate two sets: the set of global identifiers, and the set
of join pairs. Then, we compare the respective sets to compute
precision and recall, and compute an overall F-Measure that
gives us the level of similarity. Figure 2 reports the values of
precision and recall and the overall F-measure for our example.
Notice that, in addition to the measure of quality, our
algorithm is also capable of providing detailed feedbacks about
errors, in terms of missing and extra tuples in the generated
output.
Estimating User Efforts To estimate user efforts, we measure
the complexity of the mapping specification provided through
the DTS GUI. To do this, we model the specification as an
input-graph with labeled nodes and labeled edges. Experience
shows that this model is general enough to cover every data
transformation, spanning from schema mapping transformations to ETL ones, and provides more accurate results wrt to
previous metrics based on point-and-click counts.
Every element in the source and target schemas is a node in
the graph. Arrows among elements in the GUI become edges
among nodes in the graph. The tool may provide a library of
graphical elements – for example to introduce system functions
– that are modeled as additional nodes in the graph. Extra
information entered by the user (e.g., manually typed text) is
represented as labels over nodes and edges.

Fig. 3: Input graph for a vertical partition scenario in a commercial
mapping system.

We measure the size of such graphs by encoding their
elements according to a minimum description length technique
[5], and then by measuring the size in bits of such description.
We report in Figure 3 the input-graph for a sample vertical
partition scenario expressed using a commercial mapping
system. The graph consists of the following elements: (i)
29 nodes for the source and target schemas; notice that the
schemas are considered as part of the input and are not counted
in the encoding; (ii) there are 5 functions in this scenario that

generate additional nodes; this makes a total of 34 nodes, so
that we shall need 6 bits for their encoding; function nodes
are labeled by the associated function; based on the size of
the library of functions provided by the tool, every function
node requires an additional 8 bits for its encoding; (iii) 25
edges (without labels), that we shall encode as pairs of node
identifiers (2*6 bits); (iv) finally, to specify the mapping, one
of the schema nodes must be labeled by a char, which we need
to encode.
The specification complexity is therefore given by the
following sum of costs: (5*(6+8)) + (25*(2*6)) + (6+8) = 384
bits.
Designing Scenarios Designing a scenario amounts to choosing a source and target schema, and an input-output function,
that is, a set of source instances given as inputs, and a set
of target instances considered as expected outputs (the gold
standard). It can be seen that the critical point of this process
consists in deciding what the expected output should be when
some input is fed to the system.
In this respect, our approach is very general, and allows
one to select expected outputs in various ways.
(i) it is in principle possible to craft the expected output by
hand. However, this can be done only for small scenarios;
(ii) a more typical way – as it was done, for example, in [2]
– would be to express the input-output function as a query Q
written in a concrete query language, say SQL or XQuery. In
this case, for a given input IS , the expected output would be
Ie = Q(IS );
(iii) as an alternative, one can choose a reference system,
design the transformation using the system, compute the result,
and take this as the expected output. In many cases can be
done, for example, by considering a tool that employs the
algorithms in [6], [7], which, given a data-exchange scenario
expressed as a set of embedded dependencies, are able to
compute the “optimal” solution, i.e., the core universal solution
[8], in a scalable way.
We want to remark that the system does not impose any
of these methods, and all of them are acceptable.
III.

of different facets of data-transformation domain, we will
introduce some new scenarios obtained by enriching existing
STBenchmark scenarios with functional dependencies and key
constraints [10], in order to make them more challenging and
emphasize the relationships between mappings and data-fusion
techniques.
In each evaluation session, we will provide a description
of the transformation to participants, let them pick-up one or
more of the tools under evaluation, and then challenge them
to give answers to the following questions:
(a) what is the most effective tool to solve this scenario ?
(b) are you able to correctly express this transformation given
a tool you don’t know ?
(c) given a correct transformation expressed using tool X, are
you able to express the same transformation with the same
quality and less effort ?
Finally, users will be asked to design their own scenarios
by selecting a source and target schema, input instances and
expected output, and run evaluation sessions on them.
During the sessions, users will be presented with instances
of various sizes. Thanks to its fast algorithms, IQ-M ETER will
dynamically compute quality and efforts, and update its report
in real time. In addition, it will provide users with detailed
explanations for both. As it can be glimpsed in Figure 1, in
fact, it provides explanations for the quality measure, in terms
of missing and extra tuples in the target, and for the effort
measure by showing the computed effort graph. This will show
clearly how problems in mapping design can be difficult to
debug if proper tools are not adopted.
R EFERENCES
[1]

[2]

[3]
[4]
[5]

D EMONSTRATION

In order to get participants as much involved as possible,
the demonstration will be centered around a gamification of
IQ-M ETER. In the spirit of evaluating representative systems
from different perspectives, we will use the prototype to compare: (i) a schema-mapping research prototypes implementing
algorithms of the first-generation [9] and second-generation
[10]; (ii) a commercial schema-mapping system; (iii) an
open-source ETL tool.
We will select a number of test scenarios of different
levels of complexity. Some scenarios will come from previous benchmarks for schema-mappings, like STBenchmark
[2]. Other scenarios will come from benchmarks for the
ETL world [3]. Finally, we will also consider scenarios from
other application domains, like data fusion [11]. To further
pursue the investigation of problems that are at the boundaries

1221

[6]

[7]

[8]
[9]
[10]
[11]

C. Thomsen and T. Bach Pedersen, “ETLDiff: A Semi-automatic
Framework for Regression Test of ETL Software,” in DaWaK, 2006,
pp. 1–12.
B. Alexe, W. Tan, and Y. Velegrakis, “STBenchmark: Towards a
Benchmark for Mapping Systems,” PVLDB, vol. 1, no. 1, pp. 230–244,
2008.
A. Simitsis, P. Vassiliadis, U. Dayal, A. Karagiannis, and V. Tziovara,
“Benchmarking etl workflows,” in TPCTC, 2009, pp. 199–220.
G. Mecca, P. Papotti, S. Raunich, and D. Santoro, “What is the IQ of
your Data Transformation System?” in CIKM, 2012, pp. 872–881.
D. MacKay, Information Theory, Inference, and Learning Algorithms.
Cambridge University Press, 2003.
G. Mecca, P. Papotti, and S. Raunich, “Core Schema Mappings:
Scalable Core Computations in Data Exchange,” Information Systems,
vol. 37, no. 7, pp. 677 – 711, 2012.
B. ten Cate, L. Chiticariu, P. Kolaitis, and W. C. Tan, “Laconic Schema
Mappings: Computing Core Universal Solutions by Means of SQL
Queries,” PVLDB, vol. 2, no. 1, pp. 1006–1017, 2009.
R. Fagin, P. Kolaitis, and L. Popa, “Data Exchange: Getting to the
Core,” ACM TODS, vol. 30, no. 1, pp. 174–210, 2005.
L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernandez, and R. Fagin,
“Translating Web Data,” in VLDB, 2002, pp. 598–609.
B. Marnette, G. Mecca, and P. Papotti, “Scalable data exchange with
functional dependencies.” PVLDB, vol. 3, no. 1, pp. 105–116, 2010.
J. Bleiholder and F. Naumann, “Data fusion,” ACM Comp. Surv., vol. 41,
no. 1, pp. 1–41, 2008.

Estimating Data Integration and Cleaning Effort
Sebastian Kruse

Hasso Plattner Institute (HPI),
Germany

sebastian.kruse@hpi.de

Paolo Papotti

Qatar Computing Research
Institute (QCRI), Qatar

ppapotti@qf.org.qa

ABSTRACT

felix.naumann@hpi.de

for data integration and data quality1,2 , and it is well recognized that
most of those expenses are for human labor. Thus, when embarking
on a data integration and cleaning project, it is useful and important to estimate in advance the effort and cost of the project and to
find out which particular difficulties cause these. Such estimations
help deciding whether to pursue the project in the first place, planning and scheduling the project using estimates about the duration
of integration steps, budgeting in terms of cost or manpower, and
finally monitoring the progress of the project. Cost estimates can
also help integration service providers, IT consultants, and IT tool
vendors to generate better price quotes for integration customers.
Further, automatically generated knowledge of how well and how
easy a data source fits into a given data ecosystem improves source
selection.
However, “project estimation for [. . . ] data integration projects
is especially difficult, given the number of stakeholders involved
across the organization as well as the unknowns of data complexity
and quality.” [14]. Any integration project has several steps and
tasks, including requirements analysis, selection of data sources,
determining the appropriate target database, data transformation
specifications, testing, deployment, and maintenance. In this paper, we focus on exploring the database-related steps of integration
and cleaning and automatically estimate their effort.

Data cleaning and data integration have been the topic of intensive
research for at least the past thirty years, resulting in a multitude of
specialized methods and integrated tool suites. All of them require
at least some and in most cases significant human input in their
configuration, during processing, and for evaluation. For managers
(and for developers and scientists) it would be therefore of great
value to be able to estimate the effort of cleaning and integrating
some given data sets and to know the pitfalls of such an integration
project in advance. This helps deciding about an integration project
using cost/benefit analysis, budgeting a team with funds and manpower, and monitoring its progress. Further, knowledge of how
well a data source fits into a given data ecosystem improves source
selection.
We present an extensible framework for the automatic effort estimation for mapping and cleaning activities in data integration
projects with multiple sources. It comprises a set of measures and
methods for estimating integration complexity and ultimately effort, taking into account heterogeneities of both schemas and instances and regarding both integration and cleaning operations. Experiments on two real-world scenarios show that our proposal is
two to four times more accurate than a current approach in estimating the time duration of an integration process, and provides a
meaningful breakdown of the integration problems as well as the
required integration activities.

1.

Felix Naumann

Hasso Plattner Institute (HPI),
Germany

1.1

Challenges

There are simple approaches to estimate in isolation the complexity of individual mapping and cleaning tasks. For the mapping,
evaluating its complexity can be done by counting the matchings,
i.e., correspondences, among elements. For the cleaning problem, a
natural solution is to measure its complexity by counting the number of constraints on the target schema. However, as several integration approaches have shown, the interactive nature of these two
problems is particularly complex [5, 11, 13]. For example, a data
exchange problem takes as input two relational schemas, a transformation between them (a mapping), a set of target constraints,
and answers two questions: whether it is possible to compute a
valid solution for a given setting and how. Interestingly, to have a
solution, certain conditions must hold on the target constraints, and
extending the setting to more complex languages or data models
bring tighter restrictions on the class of tractable cases [6, 12].
In our work, the main challenge is to estimate complexity and
effort in a setting that goes beyond these ad-hoc studies while satisfying four main requirements:
Generality: We require independence from the language used to
express the data transformation. Furthermore, real cases often fail
the existence of solution tests considered in formal frameworks,

COMPLEXITY OF INTEGRATION AND
CLEANING

Data integration and data cleaning remain among the most
human-work-intensive tasks in data management. Both require a
clear understanding of the semantics of schema and data – a notoriously difficult task for machines. Despite much research and
development of supporting tools and algorithms, state-of-the-art integration projects involve significant human resource cost. In fact,
Gartner reports that 10% of all IT cost goes into enterprise software

c 2015, Copyright is with the authors. Published in Proc. 18th International Conference on Extending Database Technology (EDBT), March
23-27, 2015, Brussels, Belgium: ISBN 978-3-89318-067-7, on OpenProceedings.org. Distribution of this paper is permitted under the terms of the
Creative Commons license CC-by-nc-nd 4.0.

1
2

61

http:// www.gartner.com/ technology/ research/ it-spending-forecast/
http:// www.gartner.com/ newsroom/ id/ 2292815

10.5441/002/edbt.2015.07

e.g., weak acyclicity condition [11], but an automatic estimation is
still desirable for them in practice.
Completeness: Only a subset of the constraints that hold on the
data are specified over the schema. In fact, business rules are commonly enforced at the application level and are not reflected in the
metadata of the schemas, but should nevertheless be considered.
Granularity: Details about the integration issues are crucial for
consumption of the estimation. For a real understanding and proper
planning, it is important to know which source and/or target attributes are cause of problems and how, e.g., phone attributes in
source and target schema have different formats. Existing estimators do not reason over actual data structures and thus make no
statements about the causes of integration effort.
Configurability and extensibility: The actual effort depends on
subjective factors, such as the capabilities of available tools and
the desired quality of the output. Therefore, intuitive, yet rich configuration settings for the estimation process are crucial for its applicability. Moreover, users must be able to extend the range of
problems covered by the framework.
These challenges cannot be tackled with existing syntactical
methods to test the existence of solutions, as they work only in
specific settings (Generality), are restricted to declarative specifications over the schemas (Completeness), and do not provide details
about the actual problems (Granularity). On the other hand, as systems that compute solutions require human interaction to finalize
the process [8, 13], they cannot be used for estimation purpose and
their availability is orthogonal to our problem (Configurability).

1.2

Data
Integration
Scenario

Schema
Matching

Complexity
Assessment

Integration
Tools

Data
Profiling

Integration
Practitioner

Estimated
Complexity
Effort
Estimation

influence

Estimated
Effort

Measured
Effort

Integration
Result

estimates
Estimation Side

Production Side

Figure 1: Overview of effort estimation and execution of data integration scenarios.
The second phase, effort estimation, builds upon the complexity
assessment to estimate the actual effort for overcoming the previously revealed integration challenges in some useful unit, such as
workdays or monetary cost. Thereby, this phase addresses configurability by taking external parameters into account, such as the
experience of the integration practitioner and the features of the
integration tools to be used.

Approaching Effort Estimation

1.3

Figure 1 presents our view on the problem of estimating the effort of data integration. The starting point is an integration scenario with a target database and one or more source databases. The
right-hand side of the figure shows the actual integration process
performed by an integration specialist, where the goal is to move
all instances of the source databases into the target database. Typically, a set of integration tools are used by the specialist. These
tools have access to the source and target and support her in the
tasks. The process takes a certain effort, which can be measured,
for instance as amount of work in hours or days or in a monetary
unit.
Our goal is to find that effort without actually performing the integration. Moreover, we want to find and present the problems that
cause this effort. To this end, we developed a two-phase process as
shown on the left-hand side of Figure 1.
The first phase, the complexity assessment, reveals concrete
integration challenges for the scenario. To address generality,
these problems are exclusively determined by the source and target schemas and instances; if and how an integration practitioner
deals with them is not addressed at this point. Thus, this first phase
is independent of external parameters, such as the level of expertise
of the specialist or the available integration tools. However, it is
aided by the results of schema matching and data profiling tools,
which analyze the participating databases and produce metadata
about them (to achieve completeness). The output of the complexity assessment is a set of clearly defined problems, such as number
of violations for a constraint or number of different value representations. This detailed breakdown of the problems achieves granularity and is useful for several tasks, even if not interpreted as an
input to calculate actual effort. Examples of application are source
selection [9], i.e., given a set of integration candidates, find the
source with the best ‘fit’; and support for data visualization [7],
i.e., highlight parts of the schemas that are hard to integrate.

Contributions and structure

Section 2 presents related work and shows that we are the first to
systematically address a dimension of data integration and cleaning
that has been passed over by the database community but is relevant
to practitioners. In particular, we make the following contributions:
• Section 3 introduces the extensible Effort Estimation framework (E FES), which defines a two-dimensional modularization of the estimation problem.
• Section 4 describes an estimation module for structural conflicts between source and target data. This module incorporates a new formalism to compare schemas in terms of mappings and constraints.
• Section 5 reports an estimation module for value heterogeneities that captures formatting problems and anomalies
in data that may be missed by structural conflicts.
These building blocks have been evaluated together in an experimental study on two real-world datasets and Section 6 reports on
the results. Finally, we conclude our insights in Section 7.

2.

RELATED WORK

When surveying computer science literature, a pattern becomes
apparent: much technology claims (and experimentally shows) to
reduce human effort. The veracity of this claim is evident – after all, any kind of automation of tedious tasks is usually helpful.
While for scientific papers this reduction is enough of a claim, the
absolute measures of effort and its reduction are rarely explained
and measured.
General effort estimation. There are several approaches for effort estimation in different fields, however, none of them considers
information coming from the datasets.

62

In the software domain, an established model to estimate the cost
of developing applications is COCOMO [3, 4], which is based on
parameters provided by the users such as the number of lines of
existing code. Another approach decomposes an overall work task
into a smaller set of tasks in a “work breakdown structure” [16].
The authors manually label business requirements with an effort
class of simple, medium, or complex, and multiply each of them
by the number of times the task must be executed.
In the ETL context, Harden [14] breaks down a project into various subtasks, including requirements, design, testing, data stewardship, production deployment, but also the actual development
of the data transformations. For the latter he uses the number of
source attributes and assigns for each attribute a weighted set of
tasks (Table 1). In sum, he calculates slightly more than 8 hours of
work for each source attribute.
Task
Requirements and Mapping
High Level Design
Technical Design
Data Modeling
Development and Unit Testing
System Test
User Acceptance Testing
Production Support
Tech Lead Support
Project Management Support
Product Owner Support
Subject Matter Expert
Data Steward Support

modify the proposed match result into the intended result” in terms
of additions and deletions of matching attribute pairs [19].
Data-oriented effort estimation. In [25], the authors offer a
“back of the envelope” calculation on the number of comparisons
needed to be performed by human workers to detect duplicates in
a dataset. According to them, the estimate depends on the way the
potential duplicates are presented, in particular their order and their
grouping. Their ideas fit well into our effort model and show that
specific tooling indeed changes the needed effort, independently
of the complexity of the problem itself. Complementary work on
source selection has focused on the benefit of integrating a new
source based on its marginal gain [9, 23].
Data-cleaning and data-transformation.
Many systems
(e.g., [8, 13]) address the problem of detecting violations over the
data given a set of constraints, as we also do in one of our modules for complexity estimation. The challenge for these systems
is mostly the automatic repair step, i.e., how to update the data to
make it consistent wrt. the given constraints with a minimal number of changes. None of these systems provide tools to estimate the
complexity of the repair nor the user effort before actually executing the methods to solve the integration problem. In fact, the challenge is that solving the problem involves the users, and estimating
this effort (even in presence of these tools) is our main goal. Similar
problems apply to data exchange and data transformation [1, 15].
In the field of model management, the use of metamodels has
been investigated to represent in a more general language several
alternative data models [2,21]. Our cardinality-constrained schema
graphs (Section 4) can be seen as a proposal for a metamodel with
a novel static analysis of cardinalities to identify problems in the
underlying schemas and the mapping between them.

Hours per attribute
2.0
0.1
0.5
1.0
1.0
0.5
0.25
0.2
0.5
0.5
0.5
0.5
0.5

3.

Table 1: Tasks and effort per attribute from [14].

THE EFFORT ESTIMATION FRAMEWORK

One can find other lists of criteria to be taken into account when
estimating the effort of an integration project3 . These include factors we include in our complexity model, such as number of different sources and types, duplicates, schema constraints, and others we exclude for sake of space from our discussion, such as
project management, deployment needs, and auditing. There are
also mentions of factors that influence our effort model, such as
familiarity with the source database, skill levels, and tool availability. However, merely providing a list of factors is only a first
step, whereas we provide novel measures for the database-specific
causes for complexity and effort. In fact, existing methods: (i) lack
a direct numerical analysis of the schemas and datasets involved in
the transformation and cleaning; (ii) do not regard the properties of
the datasets at a fine grain and cannot capture the nature of the possible problems in the scenario, (iii) do not consider the interaction
of the mapping and the cleaning problems.
Schema-matching for effort estimation. In our work, we exploit schema matching to bootstrap the process. This is along
the lines of what authors of matchers suggested. For example,
in [24] the authors have pointed out the multiple usages of schema
matching tools beyond the concrete generation of correspondences
for schema mappings. In particular, they mention “project planning” and “determining the level of effort (and corresponding cost)
needed for an integration project”. In a similar fashion, in the evaluation of the similarity flooding algorithm, Melnik et al. propose
a novel measure “to estimate how much effort it costs the user to

Real-world data integration scenarios host a large number of different challenges that must be overcome. Problems arise in common activities, such as the definition of a mapping between different schemas, the restructuring of the data, and the reconciliation
of their value format. We first describe these problems and then
introduce our solution.

3
Such as http:// www.datamigrationpro.com/ data-migration-articles/
2012/ 2/ 9/ data-migration-effort-estimation-practical-techniques-from-r.
html and http:// www.information-management.com/ news/ 1093630-1.html

Example 3.1. Figure 2 shows an integration scenario with music records data. Both source and target relational schemas (Figure 2a) define a set of constraints, such as primary keys (e.g., id in

3.1

Data Integration Scenario

A data integration scenario comprises: (i) a set of source
databases; (ii) a target database, into which the source databases
shall be integrated; and (iii) correspondences to describe how these
sources relate to the target. Each source database consists of a relational schema, an instance of this schema, and a set of constraints,
which must be satisfied by that instance. Likewise, the target database can carry constraints and possibly already contains data as
well that satisfies these constraints. Furthermore, each correspondence connects a source schema element with the target schema
element, into which its contents should be integrated.
Oftentimes constraints are not enforced at the schema level but
rather at the application level or simply in the mind of the integration expert. Even worse, for some sources (e.g., data dumps),
a schema definition may be completely missing. To achieve completeness, techniques for schema reverse engineering and data profiling [20] can reconstruct missing schema descriptions and constraints from the data.

63

records
PK
NN
NN

id
title
artist
genre

Not all kinds of integration issues can be detected by analyzing
the schemas, though. The data itself is equally important. While
we assume that every instance is valid wrt. its schema, when data is
integrated new problems can arise. For example, all sources might
be free of duplicates, but there still might be target duplicates when
they are combined [22]. These conflicts can also arise between
source data and pre-existing target data.

tracks
FK,NN
NN

record
title
duration

Target
albums
PK
NN
FK,NN

id
name
artist_list

songs
FK
NN
FK

Example 3.3. Tables 2b and 2c report sample instances of the
tracks table and the songs table, respectively. The duration of
tracks in the target database is encoded as a string with the format m:ss, while the length of songs is measured in milliseconds in
the source. The two formats are locally consistent, but the source
values need a proper transformation when integrated into the target
column, thereby demanding a certain amount of effort.
⇧

album
name
artist_list
length

artist_credits
PK,FK
PK
NN

artist_list
position
artist

artist_lists
PK

id

3.2

Source

(a) Schemas, constraints, and correspondences.

record
1
1
1

title
“Sweet Home Alabama"
“I Need You"
“Don’t Ask Me No Questions"
...

duration
“4:43"
“6:55"
“3:26"

(b) Example instances from the target table tracks.

album
s3
s3
s3

name
artist_list
“Hands Up"
a1
“Labor Day"
a1
“Anxiety"
a2
...

A General Framework

Facing different kinds of data integration and cleaning actions,
there is the need of different specialized models to decode their
complexity and estimate their effort properly. We tackle this problem with our general effort estimation framework E FES. It handles
different kinds of integration challenges by accepting a dedicated
estimation module to cope with each of them independently. Such
modularity makes it easier to revise and refine individual modules
and establishes the desired extensibility by plugging new ones. In
this work, we present modules for the three general and, in our experience, most common classes of integration activities: writing an
executable mapping, resolving structural conflicts, and eliminating
value heterogeneities. While the latter two are explained in subsequent sections, we present the mapping module in this section to
explain our framework design. For generality, the modules do not
depend on a fixed language to express the transformations.
Figure 3 depicts the general architecture of E FES. The architecture implements our goal of delivering a set of integration problems
and an effort estimate by explicitly dividing the estimation process
into an objective complexity assessment, which is based on properties of schemas and data, followed by the context-dependent effort
estimation. We now describe these two phases in more detail.

length
215900
238100
218200

(c) Example instances from the source table songs.

Figure 2: An example data integration scenario.

Complexity Assement

records), foreign keys (record in tracks, represented with dashed
arrows), and not nullable values (title in tracks).
Solid arrows between attributes and relations represent correspondences, i.e., two attributes that store the same atomic information or two relations that store the same kind of instances. The
source relation albums corresponds to the target relation records
and its source attribute name corresponds to the title attribute in
the target. That means, that the albums from the source shall be
integrated as records into the target, while the source album names
serve as titles for the integrated records.
⇧

Effort Estimation

We assume correspondences between the source and target
schemas to be given, as they can be automatically discovered with
schema matching tools [10]. Notice that correspondences are not
an executable representation of a transformation, thus they do not
induce a precise mapping between sources and the target. However,
they contain enough information to reason over the complexity of
data integration scenarios and detect their integration challenges, as
in the following example.
Example 3.2. The target schema requires exactly one artist
value per record, whereas the source schema can associate an arbitrary number of artist credits to each album. This situation implies
that integrating any source album with zero artist credits violates
the not-null constraint on the target attribute records.artist. Moreover, two or more artist credits for a single source album cannot
be naturally stored by the single target attribute. Integration practitioners have to solve these conflicts. Hence, this schema heterogeneity increases the necessary effort to achieve the integration. ⇧

Data Integration Scenario
Estimation Module
Data
Complexity
Detectors
Data Complexity Reports

Task
Planners

Expected
Quality
Tasks

Effort
Calculation
Functions

Execution
Settings

Effort Estimate

Figure 3: Architecture of E FES.

3.3

Complexity assessment

The goal of this first phase is to compute data complexity reports
for the integration scenario. These reports serve as the basis for the

64

Example 3.6. Consider again the problem with the cardinality
of record artists. There are schema mappings tools [18] that are
able to automatically create a synthetic value in the target, if a
source album has zero artist credits, and to automatically create
multiple occurrences of the same album with different artists, if
multiple artists are credited for a single source album. Such tools
would reduce the mapping effort.
⇧

subsequent effort estimation but also are used to inform the user
about integration problems within the scenario. This is particularly
useful for source selection [9] and data visualization [7].
Each estimation module provides a data complexity detector that
extracts complexity indicators from the given scenario and writes
them into its report. There is no formal definition for such a report;
rather, it can be tailored to the specific, needed complexity indicators. For example, the mapping module builds on the following
idea: For each table in the target schema and each source database
that provides data for that table, some connection has to be established to fetch the source data and write it into the target table. The
overall complexity of the mapping creation is composed of the individual complexities for establishing each of these connections.
Furthermore, every connection can be described in terms of certain metrics, such as the number of source tables to be queried, the
number of attributes that must be copied, and whether new IDs for
a primary key need to be generated.

For the effort estimation, each estimation module has to provide
a task planner that consumes its data complexity report and outputs
tasks to overcome the reported issues. Each of these tasks is of a
certain type, is expected to deliver a certain result quality, and comprises an arbitrary set of parameters, such as on how many tuples it
has to be executed. We defined two instances of expected quality,
namely low effort (removal of tuples) and high quality (updates).
This criterion is extensible to other repair actions, but it already
allows to choose between alternative cleaning tasks as shown in
Example 3.5.

Example 3.4. The data complexity report for the scenario in
Figure 2 can be found in Table 2. To fetch the data for the
records table, the three source tables albums, artist_lists, and
artist_credits have to be combined, two attributes must be copied,
and unique id values for the integrated tuples must be generated. ⇧
Target table

Source tables

Attributes

3
3

2
2

records
tracks

Example 3.7. A complexity report for the scenario from Figure 2 states that there are albums without any artist in the source
data that lead to a violation of a not-null constraint in the target.
The corresponding task model proposes the alternative actions Reject violating tuple (low effort) or Add missing value (high quality)
to solve the problem.
⇧

Primary key
yes
no

Once the list of tasks has been determined, the effort for their
execution is computed. For this purpose, the user specifies in advance for each task type an effort-calculation function that can incorporate task parameters. As an example, we report the effortcalculation functions for the execution settings of our experiments
in Table 9. The framework uses these functions to estimate the effort for each of the tasks. Finally, the total of all these task estimates
forms the overall effort estimate.

Table 2: Mapping complexity report of the scenario in Figure 2.

3.4

Effort estimation

Based on the data complexity, the effort estimation shall produce
a single estimate of the human work to address the different complexities. However, going from an objective complexity measure to
a subjective estimate of human work requires external information
about the context. We distinguish one aspect that is specific to the
data integration problem, (i) the expected quality of the integration
result, and, as a more common aspect, (ii) the execution settings for
the scenario.

Example 3.8. We exemplify the effort-calculation functions for
the tasks derived from the report in Table 2. The Create mapping
task might be done manually with SQL queries. Then an adequate
function would be
effort = 3mins · tables + 1min · attributes + 3mins · PKs

leading to an overall effort of 25 (18 + 4 + 3) minutes. However, if a tool can generate this mapping automatically based on
the correspondences (e.g., [18]), then a constant value, such as
effort = 2mins, can reflect this circumstance, leading to an overall effort of four minutes.
⇧

(i) Expected quality: Data cleaning is the operation of updating an instance such that it becomes consistent wrt. any constraint
defined on its schema [8, 13]. However, such results can be obtained by automatically removing problematic tuples, or by manually solving inconsistencies involving a domain expert. Each
choice implies different effort.

The above described task-based approach offers several advantages over an immediate complexity-effort mapping [14], where a
formula directly converts statistics over the schemas into an effort
estimation value. Our model enables configurability, as it treats execution settings as a first-class component in the effort-calculation
functions and these can be arbitrarily complex as needed. Furthermore, instead of just delivering a final effort value, our effort estimate is broken down according to its underlying tasks. This granularity helps users understand the required work and explains how
the estimate has been created, thus giving the users the opportunity
to properly plan the integration process.

Example 3.5. Consider again Example 3.3 with the duration
format mismatch. As duration is nullable, one simple way to solve
the problem is to drop the values coming from the new source. A
better, higher quality solution is to transform the values to a common format, but this operation requires a script and a validation by
the user, i.e., more effort [15].
⇧
(ii) Execution settings: The execution settings represent the circumstances under which the data integration shall be conducted.
Examples of common context information are the expertise of the
integration practitioners and their familarity with the data [4]. In
our setting, we also model the level of automation of the available
integration tools, and how critical the errors are, e.g., integrating
medical prescriptions requires more attention (and therefore effort)
than integrating music tracks.

4.

STRUCTURAL CONFLICTS

Structural heterogeneities between source and target data structures are a common problem in integration scenarios. This section
describes a module to detect these problems and estimate the effort

65

arising out of them. It can be plugged into the framework architecture in Figure 3 with the following workflow: Its data complexity detector (structure conflict detector) analyzes how source and
target data relate to each other, and counts the number of emerging structural conflicts. Based on those conflicts, the task planner
(structure repair planner) then (i) derives a set of cleaning tasks
to make the conflicting source data fit into the target schema, and
(ii) estimates how often each such task has to be performed. These
tasks can finally be fed into the effort calculation functions.

4.1

records

tracks

1..*

1..*

1..*

1

1..*

1..*

1

1

0..1

1

1

1

0..1

title'

duration

artist

title

genre

1

id

0..1

Target

record

albums

songs

1..*

1..*

1

1..*

1..*

1..*

1..*

1

1

1

1

1

1

0..1

artist_list

Structure Conflict Detector

name

1

id

0..1

0..1

In the first step of structural conflict handling, all source and
target schemas of the given scenario are converted into cardinalityconstrained schema graphs (short CSG), a novel modeling formalism that we specifically devised for our task. It offers a single,
yet expressive constraint formalism with a set of inference operators that allow elegant comparisons of schemas. Additionally, it
is more general than the relational model and can describe (integrated) database instances that do not conform to the relational
model. For instance, an integrated tuple might provide multiple
values for a single attribute, like in Example 3.2. The higher expressiveness of CSGs allows to reason about necessary cleaning
tasks to make the integrated databases conform to the relational
model. In the following, we formally define CSGs and explain how
to convert relational databases into CSGs.

1..*

artist

position

album

artist_list''

1

1

1

1..*

1..*

1..*

name' artist_list'

length

0..1
1

1

0..1
1

id'
1
1

artist_credits

artist_lists

Source

Figure 4: The integration scenario translated into cardinalityconstrained schema graphs.
To express schema constraints in CSGs, all relationships are annotated with prescribed cardinalities, that restrict the number of
elements and/or values of connected nodes that must relate to each
other via the annotated relationship. For example, tracks.record
is not nullable, which means, that each tracks tuple must provide
exactly one record value. Translated to CSGs, this means that for
each tuple ti , the relationship ⇢tracks!record must contain exactly
one link:

D EFINITION 1. A CSG is a tuple = (N, P, ), where N is a
set of nodes and P ⇢ N 2 is a set of relationships. Furthermore,
 : P ! 2N expresses schema constraints by prescribing cardinalities for relationships.

8ti : |{v 2 IN (record) | (idti , v) 2 IP (⇢tracks!record )}| = 1

D EFINITION 2. A CSG instance is a tuple I( ) = (IN , IP ),
where IN assigns a set of elements to each node in N and IP
assigns to each relationship links between those elements.

.

Formally, this is expressed by (⇢tracks!record ) = {1}, which is
also graphically annotated in Figure 4. However, tracks.record
is not subject to a unique-constraint. In consequence, every
record value can be found in one or more tuples. Therefore,
(⇢record!tracks ) = 1..⇤ = {1, 2, 3, . . .}. By means of prescribed
cardinalities, unique, not-null, and foreign key constraints can be
expressed, as well as two conformity rules for relational schemas:
each tuple can have at most one value per attribute, and each attribute value must be contained in a tuple.
As stated above, another important feature of CSGs is the ability
to combine relationships into complex relationships and to analyze
their properties. As one effect, prescribing cardinalities not only to
atomic but also to complex relationships further allows to express
n-ary versions of the above constraints and functional dependencies. We devised the following relationship construction operators:

To convert a relational schema, for each of its relations, a corresponding table node (rectangle) is created to represent the existence of tuples in that relation. Furthermore, for each attribute, an
attribute node (round shape) is created and connected to its respective table node via a relationship. While these attribute nodes hold
the set of distinct values of the original relational attribute, the relationships link tuples and their respective attribute values. With
this proceeding, any relational database can be turned into a CSG
without loss of information.
Example 4.1. Figure 4 depicts two CSGs for the example scenario schemas in Figure 2a, one for the source and one for the
target schema4 . For instance, the example tracks tuple t = (1,
“Sweet Home Alabama”, “4:43”) from Figure 2b is represented in
the CSG instance as follows: The table node tracks 2 N holds an
abstract element idt , i.e., idt 2 IN (tracks), representing the tuple’s identity. Likewise, record 2 N holds exactly once the value 1,
i.e., 1 2 IN (record), and the relationship ⇢tracks!record contains
a link for these elements, i.e., (idt , 1) 2 IP (⇢tracks!record ), thus
stating that t[record] = 1. The other values for the title and
duration attributes are represented accordingly. Furthermore, foreign key relationships are represented by special equality relationships (dashed line) that link all equal elements of two nodes, e.g.,
all common values of the id and record nodes in the target CSG of
Figure 4.
⇧

‘ ’: The composition concatenates two adjacent relationships.
Formally, IP (⇢1

def

⇢2 ) = IP (⇢1 ) IP (⇢2 ).

‘[’: The union of two relationships ⇢1 [⇢2 contains all links of the
def

two relationships, i.e., IP (⇢1 [ ⇢2 ) = IP (⇢1 ) [ IP (⇢2 ).
This is particularly useful, when multiple source relationships need to be combined.

‘1’: The join operator connects links from relationships ⇢A!C , ⇢B!C with equal codomain values,
thereby inducing a relationship between A ⇥ B
def

and C.
Formally, IP (⇢A!C
1 ⇢B!C )
=
{((a, b), c) : (a, c) 2 IP (⇢A!C ) ^ (b, c) 2 IP (⇢A!B )}.
The join can be combined with other operators to express
n-ary uniqueness constraints.

4
Some correspondences between the schemas are omitted for
clarity, but are not generally discarded.

66

‘k’: The collateral of two relationships ⇢A!B k⇢C!D
induces a relationship between A ⇥ C and
def

B ⇥ D: IP (⇢A!B k⇢C!D ) = {((a, c), (b, d)) :
(a, b) 2 IP (⇢A!B ) ^ (c, d) 2 IP (⇢C!D )}. The collateral
can be applied to express n-ary foreign keys.

Based on these definitions, efficient algorithms can be devised to
infer the constraints of complex relationships.
L EMMA 1. Let ⇢1 , ⇢2 2 P be two relationships in a graph
and ⇢1 ’s end node is ⇢2 ’s start node. Then the cardinality  of
⇢1 ⇢2 can be inferred as
def

(⇢1
=

⇢2 ) = (⇢1 ) (⇢2 )

a1 ..b1

def

a2 ..b2 = (sgn a1 · a2 )..(b1 · b2 )

where sgn(0) = 0 and sgn(n) = 1 for n > 0.
L EMMA 2. Let ⇢1 , ⇢2 2 P be two relationships in a graph .
Then the cardinality of ⇢1 [ ⇢2 can be inferred as
8
(⇢1 ) [ (⇢2 ) if IP (⇢1 ) and IP (⇢2 ) have
>
>
>
disjoint domains
>
>
>
>
< (⇢1 ) + (⇢2 ) if IP (⇢1 ) and IP (⇢2 ) have equal
def
domains but disjoint codomains
(⇢1 [⇢2 ) =
>
>
(⇢
)
+̂(⇢
)
if IP (⇢1 ) and IP (⇢2 ) have equal
>
1
2
>
>
>
domains and overlapping
>
:
codomains
def

def

where 1 + 2 = {a + b : a 2 1 ^ b 2 2 } and 1 +̂2 = {c :
a 2 1 ^ b 2 2 ^ max{a, b}  c  (a + b)}.

Note, that Lemma 2 can also be applied to relationships with partially overlapping domains by splitting those into the overlapping
and the disjoint parts.
L EMMA 3. Let ⇢1 , ⇢2 2 P be two relationships in
a graph
with a common end node and let m =
min{max (⇢1 ), max (⇢2 )}. Then the cardinality of ⇢1 1 ⇢2
can be inferred as
⇢
def
;
if m = 0 _ m = ?
(⇢1 1 ⇢2 ) =
1..m otherwise

Constraint in target schema

1

Violation count in source data

(⇢records!artist ) = 1
(⇢artist!records ) = 1..⇤

and its inverse cardinality as
((⇢1 1 ⇢2 )

First, the relationship’s start and end node are matched to nodes
in the source schema via the correspondences, in this case to
albums and artist. Then, a path is sought between those nodes.
In the example, there are two possible paths, namely albums !
artist_list ! id0 ! artist_list00 ! artist_credits ! artist, and
albums ! id ! album ! songs ! artist_list0 ! id0 !
artist_list00 ! artist_credits ! artist. To resolve this ambiguity, it is assumed that the most concise detected source relationship
is the best match for the atomic target relationship. A relationship
is more concise than another relationship, if its (inferred) cardinality 1 is more specific than the other relationship’s cardinality 2 ,
i.e., 1 ⇢ 2 . In the case of equal cardinalities, the shorter relationship is preferred, according to Occam’s razor principle5 . Here,
both detected relationships have the same inferred cardinality 0..⇤
according to Lemma 1, but the former is shorter and therefore selected as match.
Having matched a target relationship to a source relationship,
comparing these two can finally reveal structural conflicts. The
example target relationship records ! artist has the annotated
cardinality 1, but its corresponding source relationship is less concise, having an inferred cardinality of 0..⇤. This lower conciseness
causes a structural conflict: The target schema accepts only one
artist value per record, while the source potentially offers an arbitrary amount of artists per album. To refine the statement about this
violation, we can count the number of albums in the source data,
that are associated to no or more than one artist, hence, determining the number of actually conflicting data elements. This violation
count is applicable to any database constraint that can be expressed
in CSG as listed above. Supporting more advanced constraints in
CSGs, such as conditional functional dependencies [8], is left for
future work.
The above described matching and checking process is performed for each target relationship. In the example scenario, there
is only one more structural violation: artist ! records has 0..⇤
as inferred cardinality, so there may be artists with no albums. Afterwards, all collected structure violations, depicted in Table 3, are
forwarded to the structure repair planner.

503
102

Table 3: Complexity report of the structure conflict detector.

)

def

= (min (⇢1 ) · min (⇢2 ))..(max (⇢1 ) · max (⇢2 ))

4.2

L EMMA 4. Let ⇢1 , ⇢2 2 P be two relationships in a graph .
Then the cardinality of ⇢1 k⇢2 can be inferred as

Structure Repair Planner

The structure repair planner proposes necessary cleaning tasks to
cope with the structural violations in an integration scenario, that
form the base for the following effort calculation. It ships with
ten such cleaning tasks listed in Table 4; one per type of violation,
e.g., of a not-null constraint, and expected result quality (low or
high). The structure conflict detector can automatically select exactly those tasks that the integration practitioner has to perform in
the data integration scenario to fix structural violations.
However, simply designating a task for each given violation is
not sufficient, as data cleaning operations usually have side effects
that can cause new violations. For instance, the structure conflict
detector reveals that there are 102 artists in the source data that have
no albums and can thus not be represented in the target schema.

def

(⇢1 k⇢2 ) = 0..(max (⇢1 ) · max (⇢2 ))
Given the means to combine relationships and infer their cardinality, it is now possible to compare the structure of source and
target schemas. As data integration seeks to populate the target
relationships with data from the sources, the structure conflict detector must determine how the atomic target relationships are represented in the source schemas. In general, target relationships can
correspond to arbitrarily complex source relationships, in particular to compositions. The composition operator particularly allows
to treat the matching of target relationships to source relationships
as a graph search problem, as is exemplified with the atomic target
relationship records ! artist from Figure 4.

5
Among competing hypotheses, the one with the fewest assumptions should be selected.

67

consistent repair strategies. Additionally, the knowledge of the necessary cleaning tasks in a data integration scenario, including their
order, are a valuable aid that can positively impact the integration
effort spent on coping with structural conflicts. Therefore, the ordered task list is provided to the user. Finally, the determined cleaning tasks are fed into the user-defined effort calculation functions,
which automatically determine the effort for dealing with structural
violations in the given scenario. Table 5 presents this effort for the
example scenario.

Result quality
High quality

Constraint

Low effort

Not null violated
Unique violated
Multiple attribute
values
Value w/o
enclosing tuple
FK violated

Reject tuple
Set values to null
Keep any value

Add missing value
Aggregate tuples
Merge values

Drop value

Create enclosing
tuple
Add referenced value

Delete dangling
value

Task

Repetitions

Table 4: Structural conflicts and their corresponding cleaning tasks.

Add tuples (records)
Add missing values (title)
Merge values (title)

102
102
503

The high-quality solution is to apply the task Create tuples for detached values, that creates record tuples to store these artists, so
that they do not have to be discarded. These new tuples would
violate the not-null constraint on the title attribute, though, so subsequent cleaning tasks are necessary. To account for such impacts,
we simulate applied cleaning tasks on virtual CSG instances as exemplified in Figure 5. In addition to the prescribed cardinalities, the
target CSG is annotated with actual cardinalities. In contrast to the
prescribed cardinalities, those do not prescribe schema constraints
but describe the state of the (conceptually) integrated source data –
in terms of its relationships’ cardinalities. Hence, the actual cardinalities are initialized with the inferred cardinalities from the source
database. Figure 5a depicts this initial state. As long as there are
actual cardinalities (on the left-hand side) that are not subsets of
the prescribed ones, the CSG instance is invalid wrt. its constraints.
Now, if the structure repair planner has chosen a cleaning task, e.g.,
adding new records tuples for artists without albums, its (side) effects are simulated by modifying the actual cardinalities, as shown
in Figure 5b with bold print. So, amongst others the actual cardinality of artist ! records is changed from 0..⇤ to 1..⇤, reflecting
that all artists appear in a record after the task, and the cardinality of
records ! title is altered from 1 to 0..1, stating that some records
would then have no title. The latter forms a new constraint violation. Now, a successive repair task can be applied on this altered
CSG instance, e.g., the task Add missing values, which leads to the
state of Figure 5c.

Total

records

records

Effort
5 mins
204 mins
15 mins
224 mins

Table 5: High-quality structure repair tasks and their estimated effort using the effort calculation functions from Table 9.

5.

VALUE HETEROGENEITIES

Value heterogeneities are a frequent class of data integration
problems with a common factor: corresponding attributes in the
source and target schema use different representations for their values. For instance, in Example 3.3 the target table tracks stores song
durations as strings, whereas the source table songs stores these
durations in milliseconds as integers. An integration practitioner
might therefore want to convert or discard the source values to
avoid having different value representations in the tracks.duration
attribute. Thus, value heterogeneities can increase the integration
effort.
This section presents a module in E FES to estimate the effort
caused by value heterogeneities. The data complexity is computed
by the value fit detector, which analyzes the source and target data
to detect different types of value heterogeneities between them.
These heterogeneities are then reported to the value transformation planner, the task model that proposes data cleaning tasks in
response to the heterogeneity issues. Finally, the effort for the proposed tasks can be calculated.

5.1

records

Value Fit Detector

The basic approach of the value fit detector is to aggregate source
0..*⊈1..* 1..*⊆1..*
1..*⊆1..* 1..*⊆1..*
1..*⊆1..* 1..*⊆1..*
and target data into statistics and compare these statistics to detect
1..*⊈1
1⊆1
1..*⊈1
1..*⊈1
0..1⊈1
1⊆1
heterogeneities. Statistics are eligible for this evaluation, because
artist
title
gen!
artist
title
gen!
artist
title
gen
they allow efficient comparison for large amounts of data, while
enabling extensibility (as new functions can be added) and com(a) Initial state.
(b) State after Add new
(c) State after Add misspleteness (as issues that are not captured by available metadata can
tuples for records.
ing values for title.
be discovered). Furthermore, statistics help to detect the especially
meaningful, general data properties that characterize the data as a
Figure 5: Extract of a virtual CSG instance as cleaning tasks are
whole. In particular, if the source data does not match the observed
performed on it.
or specified characteristics of the target dataset, plainly integrating
this source data would impair the overall quality of the integration
This procedure of picking a task and simulating its effects is reresult: integration practitioners might want to spend effort to make
peated until the virtual CSG instance contains no more violations.
source data consistent with the target data characteristics.
Furthermore, the structure repair planner orders the repair tasks, so
The value fit detector implements this idea as follows: Given
that tasks that cause new structural violations (or might break an alan integration scenario, it processes all pairs of source and target
ready fixed violation) precede the task that fixes this violation. This
attributes that are connected by a correspondence. For each such
is not computationally expensive, because we need to order only
pair, statistic values of both attributes are calculated, with the target
tasks that affect a common relationship, but doing so allows for the
attribute’s datatype designating which exact statistic types to use.
detection of “infinite cleaning loops”, where the execution order
In particular, we consider the following statistics:
of cleaning tasks forms a cycle. In most cases, these cycles are
a consequence of contradicting repair tasks. E FES proposes only
• The fill status counts the null values in an attribute and the

68

values that cannot be cast to the target attribute’s datatype.

are specific to the actual statistics. Intentionally, the importance
score describes how important the statistic type at hand is for the
target attribute. For example, in the duration attribute, all values
have the same text pattern [number ":" number], so the string format statistic is presumably an important characteristic and should
therefore have a high importance score. If it had many different text
patterns in contrast, its importance would be close to 0. In addition,
the fit value measures to what extent the source attribute statistics fit
into the target attribute statistics. For instance, the length attribute
provides only values with the differing pattern [number] leading to
a low fit value. Eventually, the fit values for all applied statistic
types are averaged using the importance scores as weights:
⌘
X⇣
def
f =
i St (⌧ ) · f Ss (⌧ ), St (⌧ )

• The constancy is the inverse of Shannon’s information entropy and is useful to classify whether the values of an attribute come from a discrete domain [17].
• The text pattern statistic collects frequent patterns in a string
attribute.
• Character histogram captures the relative occurrences of
characters in a string attribute.
• The string length statistic determines the average string
length and its standard deviation for a string attribute.
• Similarly, the mean statistic collects the mean value and standard deviation of a numeric attribute.

⌧

This overall fit value tells to what extent the source attribute fulfills
the most important characteristics of the target attribute. If it falls
below a certain threshold, we assume domain-specific differences
in between the compared attributes and Algorithm 1 issues an according value heterogeneity, e.g., Different value representations
between the attributes length and duration. In experiments with
importance scores and fit values between 0 and 1, we found 0.9
to be a good threshold to separate seamlessly integrating attribute
pairs from those that had notably different characteristics.
The set of all value heterogeneities for all attribute pairs forms
the complexity report of the value fit detector that can in the following be processed by the value transformation planner. Table 6
shows the complexity report for the example scenario. Note that
the value heterogeneities can carry additional information that are
derived from the attribute statistics as well and that can be useful
to produce accurate estimates. These parameters are not further
described in this paper.

• The histogram statistic describes numeric attributes as histograms.
• Value ranges are used to determine the minimum and maximum value of a numeric attribute.
• For attributes with values from a discrete domain, the top-k
values statistic identifies the most frequent values.
For Example 3.3, the string-typed duration target attribute designates the fill status, the text pattern statistic, the character histogram, the string length statistic, and the top-k values as interesting statistics to be collected.
In the next step, a decision model identifies, based on the gathered statistics values, the different types of value heterogeneities
within the inspected attribute pair. Algorithm 1 outlines this decision model, which consists of a sequence of rules. The evaluation of each rule has its own, mostly simple, logic. The first rule
(substantiallyFewerSourceValues), for instance, is evaluated by
comparing the fill status statistics of the source and target attribute.
Algorithm 1: Detect value heterogeneities.

1
2
3
4
5
6
7
8
9
10

Data: source attribute statistics Ss , target attribute statistics St
Result: value heterogeneities V
if substantiallyFewerSourceValues(Ss , St ) then
add Too few source elements to V ;

Value heterogeneity

Additional parameters

Different value representation
(length ! duration)

274.523 source values,
260.923 distinct source values

Table 6: Complexity report of the value fit detector.

5.2

if hasIncompatibleValues(Ss ) then
add Different value representations (critical) to V ;

Value Transformation Planner

The value transformation planner proposes tasks to solve value
heterogeneities as specified in Table 7. In contrast to the structure repair tasks from Section 4.2, those tasks do not have interdependencies. Therefore, the value transformation planner can simply propose an appropriate task for each given value heterogeneity
based on the expected result quality of the data integration. For the
four different types of value heterogeneities, there are only five different tasks, because for a low-effort integration result, value heterogeneities can in most cases be simply ignored. So, the Different
value representations between the duration and length attributes
might either be neglected (leading to no additional effort) or, for a
high-quality integration result, the value fit detector issues the task
Convert values. This task is then again fed into the effort calculation functions that compute the effort that is necessary for the task
completion. Table 8 illustrates the resulting effort estimate.

if domainRestricted(Ss ) ^ ¬domainRestricted(St ) then
add Too coarse-grained source values to V ;
else if ¬domainRestricted(Ss ) ^ domainRestricted(St ) then
add Too fine-grained source values to V ;
else if domainSpecificDifferences(Ss , St ) then
add Different value representations to V ;

For the above example attribute pair, the fill-statuses are for both
attributes near 100 %, there are no incompatible source values (integers can always be cast to strings), and neither of the attributes
is domain-restricted. Still, possible domain-specific differences between them might be present. The evaluation of this last rule is
more complex. For this purpose, a set of statistics, that are specific to the target attribute’s datatype, are computed, e.g., the string
format and string length statistic for the string-typed, not domainrestricted duration attribute. To compare these statistics among attributes, for each of them of type ⌧ , an importance score i St (⌧ )
and a fit value f Ss (⌧ ), St (⌧ ) are calculated. These calculations

6.

EXPERIMENTS

To show the viability of the general effort estimation architecture and its different models, we conducted experiments with realworld data from two different domains. In the following, we first
introduce the system and its configuration. We then describe the

69

Value heterogeneity
Too few elements
Different representations
critical
uncritical
Too specific
Too general

Result quality
low effort
high quality
-

Add values

Drop values
-

Convert values
Convert values
Generalize values
Refine values

Parameters

Convert values
(length ! duration)

274.523 values,
260.923 distinct values

Total

Effort
15 mins
15 mins

Table 8: Value transformation tasks and their estimated effort.

3 · #repetitions
(if #dist-vals < 120) 30,
(else) 0.25 · #dist-vals
0.5 · #dist-vals
0.5 · #values
10
2 · #values
10
0
5
5
5
5
5
5
5
5
3 · #FKs + 3 · #PKs + #atts +
3 · #tables

scenarios with different schemas.
Effort Estimates. In order to obtain effort estimations, we applied the following procedure to each data scenario twice, once
striving for a low-effort integration result, once for a high-quality
result. At first, we executed E FES on the scenario to obtain the
data complexity reports and a set of initially proposed mapping
and cleaning tasks. If a data complexity aspect was properly recognized but we preferred a different integration task, we have adapted
the proposed tasks. For instance, in one scenario, our prototype
proposed to provide missing FreeDB IDs for music CDs to obtain
a high-quality result; this ID is calculated from the CD structure
with a special algorithm. Since there was no way for us to obtain
this value, we exchanged this proposal with Reject tuples to delete
source CDs without such a disc ID instead.
Ground Truth Effort. Finally, we gathered the ground truth
of necessary integration tasks manually and conducted them with
SQL scripts and pgAdmin, thereby measuring the execution time of
each task. We believe these two manually integrated scenarios with
time annotations are a contribution per se, as they can be used also
for benchmarking of mapping and cleaning systems for other data
integration projects.

Setup

The E FES prototype is a Java-based implementation of the effort
estimation architecture presented in the paper, along with the three
modules discussed. It offers multiple configuration options via an
XML file and a command-line interface. As input, the prototype
takes correspondences between a source and a target dataset, all
stored in PostgreSQL databases. The prototype and the datasets
are available for download.6
Configuration and External Factors. In our experiments, the
only available software for conducting the integration are (i) manually written SQL queries, and (ii) a basic admin tool like pgAdmin.
We also assume the user has not seen the datasets before and that
she is familiar with SQL. Based on these external factors, corresponding effort conversion functions are reported in Table 9. For
example, the intuition behind the formula for adding values is that
it takes a practitioner two minutes to investigate and provide a single missing value. In contrast, deleting tuples with missing values
requires five minutes, because independently from the number of
violating tuples, one can write an appropriate SQL query to perform this task. Furthermore, we fine-tuned these settings for our
experiments as we explain in Section 6.2.
Integration Scenarios. We considered two real-world case studies. The first is the well-known Amalgam dataset from the bibliographic domain, which comprises four schemas with between 5 and
27 relations, each with 3 to 16 attributes. The second is a new case
study we created with a set of three datasets with discographic data.
In those datasets, there are three schemas with between 2 and 56 relations and between 2 and 19 attributes each. Links to the original
datasets can be found on the web page mentioned above.
For each case study, we created four integration scenarios, each
consisting of a source and target database and manually created
correspondences, because we do not want the evaluation results to
depend on the quality of a schema matcher at this point. Within
each domain, we included a data integration scenario with identical source and target schema and three other, randomly selected
6

Aggregate values
Convert values

Table 9: Effort calculation functions used for the experiments.

integration scenarios and how we created the ground truth effort by
manually integrating them. Finally, we compare our system to a
baseline approach from the literature and to the measured effort in
creating the ground truth.

6.1

Effort function (mins)

Generalize values
Refine values
Drop values
Add values
Create enclosing tuples
Delete detached values
Reject tuples
Keep any value
Add tuples
Aggregate tuples
Delete dangling values
Add referenced values
Delete dangling tuples
Unlink all but one tuple
Write mapping

Table 7: Value heterogeneities and corresponding cleaning tasks.
Task

Task

6.2

Experimental Results

The correspondences that were created for the case study
datasets have been fed into E FES to compare its effort estimates
to the actual effort. As a baseline, we used the standard approach
based on attribute counting [14], as discussed in Section 2. To obtain fair calibrations of E FES and this baseline model, we employed
cross validation: We used the effort measurements from the bibliographic domain to calibrate the parameters of E FES and the attribute counting approach for the estimation of the music domain
scenarios, and vice versa. Thus, we have for both scenarios different training and test data and both models can be regarded as
equally well-trained. To compare the two models against the measured effort, we applied the root-mean-square error (rmse):
v
⇣
⌘2
uP
u
measured(s) estimated(s)
t s2S
measured(s)
rmse =
#scenarios

http:// hpi.de/ naumann/ projects/ repeatability/

70

350
Mapping

Cleaning

Cleaning (Values)

Cleaning (Structure)

300

Effort [min]

250
200
150
100
50

s1-s2 (low eff.)

s1-s2 (high qual.)

s1-s3 (low eff.)

s1-s3 (high qual.)

s3-s4 (low eff.)

s3-s4 (high qual.)

s4-s4 (low eff.)

Counting

Measured

Efes

Counting

Measured

Efes

Counting

Measured

Efes

Counting

Measured

Efes

Counting

Efes

Measured

Counting

Measured

Efes

Counting

Measured

Efes

Counting

Measured

Efes

0

s4-s4 (high
qual.)

Figure 6: Effort estimates (E FES), actual effort (Measured), and baseline estimates (Counting) of the Bibliographic scenario.
where S is a set of integration scenarios.
We start our analysis with the Amalgam dataset with the results
reported in Figure 6. E FES consistently outperforms the counting
approach in all scenarios. This is explained by the fact that the baseline has no concept of heterogeneity between values in the datasets,
but it is one of the main complexity drivers in these integration scenarios. In terms of the root-mean-square error, E FES achieves 0.47,
while the baseline obtains 1.90 (lower values indicates better estimations), thus there is an improvement in the effort estimation by a
factor of four. Moreover, E FES not only provides the total number
of minutes, but also a detailed break down of where the effort is to
be expected. This turned out to be particular useful to revise the
effort estimate as described above, thus enriching the estimation
process with further input. In fact, it makes a significant difference
if an integration practitioner has to add hundreds of missing values
or if tuples with missing values are dropped. The baseline approach
also distinguishes between mapping and cleaning efforts, but it relates them neither to integration problems nor actual tasks. The
s4-s4 scenario demonstrates this: source and target database have
the same schema and similar data, so there are no heterogeneities
to deal with. While we can detect this, the counting approach estimates considerable cleaning effort.
When we move to the music datasets, the results in Figure 7
show a smaller difference between the two estimation approaches.
In fact, E FES outperforms the baseline four times, in three cases
baseline does a better job, and in one case the estimate is basically
the same. The explanation is that in this domain, there are fewer
problems at the data level and the effort is dominated by the mapping, which strongly depends on the schema. However, when we
look at the root-mean-square error, E FES achieves 1.05, while the
baseline obtains 1.64. Therefore, even in cases where E FES cannot
exploit all of its modules, and when counting should perform at its
best, our systematic estimation is better.
It is important to consider the generality of the presented comparison. The two case studies are based on real-world data sets with
different complexity and quality. When putting the results over the
eight scenarios together, E FES achieves a root-mean-square error of
0.84, while the baseline obtains 1.70. In terms of execution time,
E FES relies on simple SQL queries only for the analysis of the data
and completes within seconds for databases with thousands of tuples. This overhead can be neglected in the context of the dominat-

ing integration cost.

7.

CONCLUSIONS

We have tackled the problem of estimating the complexity and
the effort for data integration scenarios. As data integration is
composed of many different activities, we proposed a novel system, E FES, that integrates different ad-hoc estimation modules in
a unified fashion. We have introduced three modules that take
care of estimating the complexity and effort of (i) mapping activities, (ii) cleaning of structural conflicts arising because of different
structures and integrity constraints, and (iii) resolving heterogeneity in integrated data, such as different formats. Experimental results show that our system outperforms the standard baseline up to
a factor of four in terms of precision of the estimated effort time in
minutes. When compared to the effective time required by a human
to achieve integration, E FES provides a close estimate for most of
the cases.
We believe that our work is only a first step in this challenging
problem. One possible general direction is to integrate E FES with
approaches that measure the benefit of the integration, such as the
marginal gain [9]. This integration would allow to plot cost-benefit
graphs for the integration: the more effort, the better the quality of
the result.
A rather technical challenge in our system is to drop the assumption that correspondences among schemas are given. In practice, the effort for creating quality correspondences cannot be completely neglected – although, in our experience it takes considerably
less time than other integration activities – and automatically generated correspondences introduce uncertainty wrt. the produced estimates. The accuracy measure as proposed Melnik et al. [19] seems
to be a good starting point to tackle this issue.
Acknowledgments. We would like to thank ElKindi Rezig for
valuable discussions in the initial phase of this project.

8.

REFERENCES

[1] B. Alexe, W.-C. Tan, and Y. Velegrakis. STBenchmark:
towards a benchmark for mapping systems. Proceedings of
the VLDB Endowment, 1(1):230–244, Aug. 2008.
[2] P. Atzeni, G. Gianforme, and P. Cappellari. A universal
metamodel and its dictionary. In Transactions on

71

200
Cleaning (Structure)

Cleaning (Values)

Cleaning

Mapping

Effort [min]

160

120

80

40

f1-m2 (low eff.)

f1-m2 (high qual.)

m1-d2 (low eff.)

m1-f2 (low eff.)

m1-f2 (high qual.)

d1-d2 (low eff.)

Counting

Measured

Efes

Counting

Efes

Measured

Counting

Efes

Measured

Counting

Efes

m1-d2 (high qual.)

Measured

Counting

Efes

Measured

Counting

Measured

Efes

Counting

Measured

Efes

Counting

Measured

Efes

0

d1-d2 (high
qual.)

Figure 7: Effort estimates (Efes), actual effort (Measured), and baseline estimates (Counting) of the Music scenario.

[3]
[4]

[5]
[6]

[7]
[8]

[9]
[10]
[11]

[12]

[13]

Large-Scale Data-and Knowledge-Centered Systems I, pages
38–62. Springer, 2009.
B. Boehm. Software Engineering Economics. Prentice-Hall,
Englewood Cliffs, NJ, 1981.
B. Boehm, C. Abts, A. W. Brown, S. Chulani, B. K. Clark,
E. Horowitz, R. Madachy, D. J. Reifer, and B. Steece.
Software Cost Estimation with COCOMO II. Prentice-Hall,
Englewood Cliffs, NJ, 2000.
A. Calì, D. Calvanese, G. De Giacomo, and M. Lenzerini.
Data integration under integrity constraints. Information
Systems, 29(2):147–163, 2004.
A. Calì, G. Gottlob, and T. Lukasiewicz. A general
datalog-based framework for tractable query answering over
ontologies. In Proceedings of the Symposium on Principles
of Database Systems (PODS), pages 77–86, 2009.
M. P. Consens, I. F. Cruz, and A. O. Mendelzon. Visualizing
queries and querying visualizations. SIGMOD Record,
21(1):39–46, 1992.
M. Dallachiesa, A. Ebaid, A. Eldawy, A. Elmagarmid,
I. Ilyas, M. Ouzzani, and N. Tang. Towards a commodity
data cleaning system. In Proceedings of the International
Conference on Management of Data (SIGMOD), pages
541–552, 2013.
X. L. Dong, B. Saha, and D. Srivastava. Less is more:
Selecting sources wisely for integration. Proceedings of the
VLDB Endowment, 6(2):37–48, 2012.
J. Euzenat and P. Shvaiko. Ontology matching.
Springer-Verlag, Heidelberg (DE), 2nd edition, 2013.
R. Fagin, P. G. Kolaitis, R. J. Miller, and L. Popa. Data
exchange: Semantics and query answering. In Proceedings
of the International Conference on Database Theory (ICDT),
pages 207–224, Siena, Italy, 2003.
R. Fagin, P. G. Kolaitis, L. Popa, and W. C. Tan:. Composing
schema mappings: Second-order dependencies to the rescue.
In Proceedings of the Symposium on Principles of Database
Systems (PODS), pages 83–94, Paris, France, 2004.
F. Geerts, G. Mecca, P. Papotti, and D. Santoro. Mapping and
Cleaning. In Proceedings of the International Conference on
Data Engineering (ICDE), pages 232–243, 2014.

[14] B. Harden. Estimating extract, transform, and load (ETL)
projects. Technical report, Project Management Institute,
2010.
[15] S. Kandel, A. Paepcke, J. Hellerstein, and J. Heer. Wrangler:
interactive visual specification of data transformation scripts.
In CHI, pages 3363–3372, 2011.
[16] A. Kumar P, S. Narayanan, and V. M. Siddaiah. COTS
integrations: Effort estimation best practices. In Computer
Software and Applications Conference Workshops, 2010.
[17] D. MacKay. Information Theory, Inference, and Learning
Algorithms. Cambridge University Press, 2003.
[18] B. Marnette, G. Mecca, P. Papotti, S. Raunich, and
D. Santoro. ++Spicy: an opensource tool for
second-generation schema mapping and data exchange.
Proceedings of the VLDB Endowment, 4(12):1438–1441,
2011.
[19] S. Melnik, H. Garcia-Molina, and E. Rahm. Similarity
flooding: A versatile graph matching algorithm. In
Proceedings of the International Conference on Data
Engineering (ICDE), pages 117–128, 2002.
[20] F. Naumann. Data profiling revisited. SIGMOD Record,
42(4):40–49, 2013.
[21] P. Papotti and R. Torlone. Schema exchange: Generic
mappings for transforming data and metadata. Data &
Knowledge Engineering (DKE), 68(7):665–682, 2009.
[22] E. Rahm and H. H. Do. Data cleaning: Problems and current
approaches. IEEE Data Engineering Bulletin, 23(4):3–13,
2000.
[23] T. Rekatsinas, X. L. Dong, L. Getoor, and D. Srivastava.
Finding quality in quantity: The challenge of discovering
valuable sources for integration. In Proceedings of the
Conference on Innovative Data Systems Research (CIDR),
2015.
[24] K. P. Smith, M. Morse, P. Mork, M. H. Li, A. Rosenthal,
M. D. Allen, and L. Seligman. The role of schema matching
in large enterprises. In Proceedings of the Conference on
Innovative Data Systems Research (CIDR), 2009.
[25] J. Wang, T. Kraska, M. J. Franklin, and J. Feng. CrowdER:
Crowdsourcing entity resolution. Proceedings of the VLDB
Endowment, 5(11):1483–1494, 2012.

72

A Short History of Schema Mapping Systems
(Extended
Abstract)
A Short History
of Schema
Mapping Systems?
Giansalvatore Mecca1 , Paolo Papotti2 , and Donatello Santoro1
Giansalvatore Mecca1 , Paolo Papotti2 , and Donatello Santoro1
2
2

1

1
1

Università della Basilicata – Potenza, Italy
UniversitàResearch
della Basilicata
– Potenza,
Qatar Computing
Institute
(QCRI) –Italy
Doha, Qatar
Qatar Computing Research Institute (QCRI) – Doha, Qatar

Introduction

There are many applications that need to exchange, correlate, and integrate
heterogenous data sources. These information integration tasks have long been
identified as important problems and unifying theoretical frameworks have been
advocated by database researchers [5].
To solve these problems, a fundamental requirement is that of manipulating
mappings among data sources. The application developer is typically given two
schemas – one called the source schema, the other called the target schema –
that can be based on different models, technologies, and rules. Mappings, also
called schema mappings, are expressions that specify how an instance of the
source repository should be translated into an instance of the target repository.
In order to be useful in practical applications, they should have an executable
implementation – for example, by means of SQL queries or XQuery scripts.
This latter feature is a key requirement in order to embed the execution of
the mappings in more complex application scenarios, that is, in order to make
mappings a plug and play component of integration systems.
Traditionally, data transformation has been approached as a manual task
requiring experts to understand the design of the schemas and write scripts
to translate data. As this work is time-consuming and prone to human errors,
mapping generation tools have been created to make the process more abstract
and user-friendly, thus easier to handle for a larger class of people.
In this paper, we outline a history of the different phases that have characterized the research about automatic tools and techniques for schema mappings
and data exchange. We identify three different ages, as follows.
The Heroic Age The heroic age of schema-mappings research started ten years
ago with the seminal papers about the Clio system [17, 19]: a first generation of
tools was proposed to support the process of generating complex logical dependencies – typically tuple-generating dependencies [4] – based on a user-friendly
abstraction of the mapping provided by the users. Once the dependencies are
computed, these tools transform them into executable scripts to generate a target
solution in a scalable and portable way.
Early schema-mapping tools proved to be very effective in easing the burden
of manually specifying complex transformations, and were successfully transferred, to some extent, into commercial products (e.g., [13]). However, several
?

Extended Abstract

Nicola Ferro and Letizia Tanca (Eds.): SEBD 2012, Edizioni Libreria Progetto, Padova, Italia
ISBN: 978-88-96477-23-6, Copyright (c) 2012 - Edizioni Libreria Progetto and the authors

100

G. Mecca, P. Papotti, and D. Santoro

years after the development of the initial Clio algorithm, researchers realized that
a more solid theoretical foundation was needed in order to consolidate practical
results obtained on schema mappings systems. This consideration has motivated
a rich body of research about data exchange that characterizes the next age.
The Silver Age Data exchange [5, 8, 19] formally studies the semantics of generating an instance of a target database given a source database and a set of
mappings. It has formalized the notion of a data exchange problem [8], and has
established a number of results about its properties.
After the first data exchange studies, it was clear that a key problem in
schema-mappings tools was that of the quality of the solutions. In fact, there
are many possible solutions to a data-exchange problem, and these may largely
differ in terms of size and contents. The notion of the core of the universal
solutions [10] was identified as the “optimal” solution, since it is the smallest
among the solutions that preserve the semantics of the mapping.
In the last three years an intermediate generation of tools [16, 21] have
emerged to address the problem of generating solutions of optimal quality, while
guaranteeing at the same time the portability and scalability of the executable
scripts. Nevertheless, despite the solid results both in system and theory fields,
the adoption of mapping systems in real-life integration applications, such as
ETL workflows or Enterprise Information Integration, has been quite slow. This
seems to be due to three main factors: (a) these systems were not able, at first,
to handle functional dependencies over the target, which is a key requirement in
order to obtain solutions of quality; (b) the results were obtained primarily for
relational databases, and did not extend to nested models and XML; (c) finally,
there was no open-source schema-mapping tool available to the community.
The Golden Age A number of recent results [14, 7], along with the public availability of the first open-source mapping tools – like ++Spicy [15] and OpenII
[20] – seem to be a promising starting point towards the solution of these problems and the beginning of a new, golden age for mapping tools. These works,
along with others [1, 11], are giving new vitality to schema-mappings research
and suggest new applications, beyond traditional data exchange and data integration tasks.
In the following, we first describe the three ages in Section 2 and we then
draw some conclusions in Section 3.

2
2.1

A History of Schema Mappings In Three Ages
The Heroic Age

The first mapping generation tools were created to make the process of defining
transformations among schemas easier and more effective with respect to manually developed scripts. This first generation of tools includes primarily Clio [12,
13, 17, 19] and systems [6, 20] which incorporate a Clio-like first-generation mapping module. We summarize the features of these early mapping tools as follows.

Nicola Ferro and Letizia Tanca (Eds.): SEBD 2012, Edizioni Libreria Progetto, Padova, Italia
ISBN: 978-88-96477-23-6, Copyright (c) 2012 - Edizioni Libreria Progetto and the authors

A Short History of Schema Mapping Systems

101

Value Correspondences The goal of simplifying the mapping specification was
pursued by introducing a GUI that allows users to draw arrows, or correspondences, between schemas in order to define the desired transformation.

Fig. 1. Schema mapping scenario.

Consider the example shown in Figure 1, where data from multiple sources
should be transformed into data for a target schema with a foreign key constraints between two relations. A correspondence maps atomic elements of the
source schema to elements of the target schema, independently of the underlying
data model or of logical design choices, and can be derived automatically with
schema matching components. Notice that, while correspondences are easy to
create and understand, they are a “poor” language to express the full semantics
of data transformations. For this reason, a schema mapping tool should be able to
interpret the semantics the user wants to express with a set of correspondences.
Mapping Generation Based on value correspondences, mapping systems generate
logical dependencies to specify the mapping. These dependencies are logical formulas of two forms: tuple-generating dependencies (tgds) or equality-generating
dependencies (egds). There are two classes of constraints. Source-to-target tgds
(s-t tgds), i.e., tgds that use source relations in the premise and target relations
in the conclusion, are used to specify which tuples should be present in the target
based on the tuples that appear in the source. In an operational interpretation,
they state how to “translate” data from the source to the target. Target schemas
are also modeled with constraints: target tgds, i.e., tgds that only use target symbols, are used to specify foreign-key constraints on the target; while target egds
are used to encode functional dependencies, such as keys, on the target database.
The mapping scenario in Figure 1 has three different source tables: (i) a
table about subscribers of a service; (ii) a table with the email addresses of the
people receiving the company mailing list; (iii) a table about clients and their
check accounts. The target schema contains two tables, one about persons, the
second about accounts. On these tables, we have two keys: name is a key for the
persons, while number is a key for the accounts. Based on the correspondences
drawn in Figure 1, a Clio-like system would generate the following set of tgds:
Source-to-Target Tgds
m1 . ∀n : Subscriber(n) → ∃Y1 , Y2 : Person(n, Y1 , Y2 )
m2 . ∀n, e : MailingList(n, e) → ∃Y1 : Person(n, e, Y1 )
m3 . ∀n, acc : Client(n, acc) → ∃Y1 , Z : (Person(n, Y1 , Z) ∧ Account(Z, acc))

Nicola Ferro and Letizia Tanca (Eds.): SEBD 2012, Edizioni Libreria Progetto, Padova, Italia
ISBN: 978-88-96477-23-6, Copyright (c) 2012 - Edizioni Libreria Progetto and the authors

102

G. Mecca, P. Papotti, and D. Santoro

In addition, the following egds translate the key constraints on the target schema:
Target Egds
e1 . ∀n, e, a, e0 , a0 : Person(n, e, a) ∧ Person(n, e0 , a0 ) → (e = e0 ) ∧ (a = a0 )
e2 . ∀n, i, i0 : Account(i, n) ∧ Account(i0 , n) → (i = i0 )
Mapping Execution via Scripts To execute the mappings, schema-mapping systems rely on the traditional chase procedure [8]. The chase is a fixpoint algorithm
which tests and enforces implication of data dependencies, such as tgds, in a
database. To be more specific, a first-generation system, after the mappings had
been generated, would discard the target dependencies, and translate the sourceto-target ones under the form of an SQL or XQuery script that implements the
chase and can be applied to a source instance to return a solution.
Notice, in fact, that the chase of a set of s-t tgds on I can be naturally
implemented using SQL. Given a tgd φ(x) → ∃y(ψ(x, y)), in order to chase it
over I we may see φ(x) as a first-order query Qφ with free variables x over the
source database. We execute Qφ (I) using SQL in order to find all vectors of
constants that satisfy the premise and we then insert the appropriate tuple into
the target instance to satisfy ψ(x, y). Skolem functions [19] are typically used to
automatically “generate” some fresh nulls for y.
However, these systems suffer from a major drawback: they did not have a
clear theoretical foundation, and therefore it was not possible to reason about
the quality of the solutions.
2.2

The Silver Age

Data exchange was conceived as an attempt to formalize the semantics of schema
mappings. It formalized many aspects of the mapping execution process, as follows.
Data Exchange Fundamentals. In a data-exchange setting, the source
and target databases are modeled by having two disjoint and infinite sets of
values that populate instances: a set of constants, const, and a set of labeled
nulls, nulls [8]. Labeled nulls are used to “invent” values according to existential
variables in tgd conclusions. The reference data model is the relational one.
A mapping scenario (also called a data exchange scenario or a schema mapping) is a quadruple M = (S, T, Σst , Σt ), where S is a source schema, T is a
target schema, Σst is a set of source-to-target tgds, and Σt is a set of target
dependencies that may contain tgds and egds [8].
Given two disjoint schemas, S and T, we denote by the pair hS, Ti the schema
{S1 . . . Sn , T1 . . . Tm }. If I is an instance of S and J is an instance of T, then
the pair hI, Ji is an instance of hS, Ti.A target instance J is a solution [8] of M
and a source instance I iff hI, Ji |= Σst ∪ Σt , i.e., I and J together satisfy the
dependencies. Given a mapping scenario M = (S, T, Σst , Σt ), a pre-solution for
M and a source instance I is a solution over I for scenario Mst = (S, T, Σst ),
obtained from M by removing target constraints. In essence, a pre-solution is
a solution for the s-t tgds only, and it does not necessarily enforce the target

Nicola Ferro and Letizia Tanca (Eds.): SEBD 2012, Edizioni Libreria Progetto, Padova, Italia
ISBN: 978-88-96477-23-6, Copyright (c) 2012 - Edizioni Libreria Progetto and the authors

A Short History of Schema Mapping Systems

103

Fig. 2. Source instance (a) and three possible solutions (b-d).

constraints. Given the source data in Figure 2.a, the canonical pre-solution is reported in Figure 2.b. A mapping scenario may have multiple solutions on a given
source instance: each tgd only states an inclusion constraint and does not fully
determine the content of the target. Among the possible solutions we restrict
our attention to universal solutions, which only contain information from I and
Σst ∪ Σt . Universal solutions have a crucial property: they have a homomorphism (i.e., a constant-preserving mapping of values) into all the solutions for a
data exchange problem. Intuitively, this guarantees that the solution does not
contain any arbitrary information that does not follow from the source instance
and the mappings. Under a condition of weak acyclicity of the target tgds, an
universal solution for a mapping scenario and a source instance can be computed
in polynomial time by resorting to the classical chase procedure [8]. A solution
generated by the chase is called a canonical solution. In light of this, we may say
that early mapping systems were restricted to generate canonical pre-solutions,
since they chased s-t tgds only.
Tools of the Intermediate Generation. Once the theory of data-exchange
had become mature, it was clear that producing solutions of quality was a critical
requirement. The notion of a core solution [10] was formalized as the “optimal”
solution, since it is universal, and among the universal solutions is the one of the
smallest size. In our example, the core solution is reported in Figure 2.d.
Sophisticated algorithms were developed to post-process a canonical solution
generated by a schema-mapping tool, and minimize it to find its core [10, 18].
These tools have the merit of being very general, but fail to be scalable: even
though the algorithms are polynomial, their implementation requires to couple
complex recursive computations with SQL to access the database, and therefore do not scale nicely scale to large databases. In fact, empirical results show
that they are hardly usable in practice due to unacceptable execution times for
medium size databases [16].
It was therefore clear that, in order to preserve the effectiveness and generality
of mapping tools, reasoning about the mapping was necessary. First, a number
of approaches were proposed to optimize schema mappings in order to improve
the efficiency of their execution and manipulation. In fact, schema mappings
may present redundancy in their expressions, due for example to the presence
of unnecessary atoms or unrelated variables, thus negatively affecting the data
management process [9, 18].

Nicola Ferro and Letizia Tanca (Eds.): SEBD 2012, Edizioni Libreria Progetto, Padova, Italia
ISBN: 978-88-96477-23-6, Copyright (c) 2012 - Edizioni Libreria Progetto and the authors

104

G. Mecca, P. Papotti, and D. Santoro

A different approach to the generation of core solutions was undertaken in [16,
21]. In these proposals, scalability is a primary concern. Given a mapping scenario composed of source-to-target tgds (s-t tgds), the goal is to rewrite the
tgds in order to generate a runtime script, for example in SQL, that, on input
instances, materializes core solutions. This is a key requirement in order to embed the execution of the mappings in more complex application scenarios, that
is, in order to make data-exchange techniques a real “plug and play” feature
of integration applications. +Spicy [16] is an example of mapping tool of this
generation. These works exploit the use of negation in the premise of the s-t tgds
to rewrite them intercepting possible redundancy. Consider again our running
example; algorithms for SQL core-generation would rewrite m1 to make sure
that no redundant data are copied to the target from the relation Subscriber :
m01 . Subscriber(n) ∧ ¬(MailingList(n, E)) ∧ ¬(Client(n, A)) → Person(n, Y1 , Y2 )
Experiments [16] show that, in the computation of the core solution, with executable scripts there is a gain in efficiency of orders of magnitude with respect to
the post-processing algorithms. This is not surprising, as these mapping rewriting approaches preserve the possibility to execute transformations in standard
SQL, with the guarantee of scalability to large databases and of portability to
existing applications.
However, these tools still have some serious limitations, that prevent their
adoption in real-life scenarios. We may summarize these limitations as follows.
(a) They have limited support for target constraints. Handling target constraints
– i.e., keys and foreign keys, represented by egds and target tgds [8], respectively
– is a crucial requirement in many mapping applications. Notice that foreignkey constraints were at the core of the original schema-mapping algorithms, and,
under appropriate hypothesis, can always be rewritten as part of the source-totarget tgds [9]. Unfortunately this is not the case for target edgs.
Consider again the running example; the best a tool from this generation can
obtain with executable scripts is the core pre-solution reported in Figure 2.c,
where the redundancy coming from the source-to-target tgds has been removed,
but the solution lacks the enforcement of the target key constraints.
(b) They are limited to relational scenarios, and cannot handle XML or nested
datasets. This is a consequence of the fact that data-exchange research has primarily concentrated on the relational setting, and for a long time no notion of
data exchange for more complex models was available. In a way, this is a setback
with respect to the early systems, which had supported nested relations since
the beginning with a pragmatical approach. In fact, they were able to produce
results for XML setting, but without the precise definition of quality that core
solutions provide. It is interesting to note that a benchmark for mapping systems
has been recently proposed [2]. However, none of the tools of the intermediate
generation can be evaluated using the benchmark – for example in order to compare the quality of their solutions – since most of the scenarios in the benchmark
refer to nested structures, and these systems are not capable to generate core
solutions for a nested data model.

Nicola Ferro and Letizia Tanca (Eds.): SEBD 2012, Edizioni Libreria Progetto, Padova, Italia
ISBN: 978-88-96477-23-6, Copyright (c) 2012 - Edizioni Libreria Progetto and the authors

A Short History of Schema Mapping Systems

2.3

105

Time for a Golden Age

Recent results have faced these problems and paved the way towards the emergence of a new-generation of schema-mapping and data-exchange tools.
A first important advancement is related to the management of functional
dependencies over the target. Although it is not always possible, in general, to
enforce a set of egds using a first-order language as SQL, it has been proposed
a best-effort algorithm that rewrites the above mapping into a new set of s-t
tgds that directly generate the target tuples that are produced by chasing the
original tgds first and then the egds [14]. As egds merge and remove tuples from
the pre-solution, to correctly simulate their effect the algorithm puts together
different s-t tgds and uses negation to avoid the generation of unneeded tuples
in the result. Other approaches and semantics for the rewriting of s-t tgds have
also been recently introduced [1].
Another important aspect is the extension to nested relations and XML.
The theoretical properties of data exchange in a general XML setting have been
recently studied [3, 7], and, due to the generality, have been shown to exhibit
several negative properties. However, important results were established for the
fragment of XML data exchange in which the data model is restricted to correspond to nested relations. A very important result was reported in [7]: the
authors show that the generation of universal solutions for a nested scenario can
be reduced to the generation of solutions for a traditional, relational scenario,
even in the presence of target constraints. The authors also provide an algorithm
to perform the reduction.
We believe that these recent results, together with important theoretical
studies, such as the ones on mapping invertion [11], open new possibilities for
research on schema mappings. A notable example of a new generation of tool has
been recently presented [15]. The ++Spicy tool can deal with different data management tasks, including data fusion, data cleaning and ETL scenarios, which, in
our opinion, represent very promising areas of application of the latest schemamappings and data-exchange techniques.

3

Conclusions

Schema mapping management is an important research area in data transformation, exchange and integration systems. From the early prototypes developed
ten years ago, important results have been consolidated, but, despite the good
results, the adoption of mapping systems in real-life integration applications has
been slow. We have shown how emerging trends are overcoming the limits of
the initial proposal and are going to encourage the developing of more systems
based on schema mappings. On one side, novel theoretical results are paving the
way to the creation of innovative applications for real world problems. On the
other side, a new generation of tools for the creation and optimization of schema
mappings are widening the opportunities offered by such technology.

Nicola Ferro and Letizia Tanca (Eds.): SEBD 2012, Edizioni Libreria Progetto, Padova, Italia
ISBN: 978-88-96477-23-6, Copyright (c) 2012 - Edizioni Libreria Progetto and the authors

106

G. Mecca, P. Papotti, and D. Santoro

References
1. B. Alexe, M. A. Hernández, L. Popa, and W. C. Tan. Mapmerge: Correlating
independent schema mappings. PVLDB, 3(1):81–92, 2010.
2. B. Alexe, W. Tan, and Y. Velegrakis. Comparing and Evaluating Mapping Systems
with STBenchmark. PVLDB, 1(2):1468–1471, 2008.
3. M. Arenas and L. Libkin. XML Data Exchange: Consistency and Query Answering.
J. of the ACM, 55(2):1–72, 2008.
4. C. Beeri and M. Vardi. A Proof Procedure for Data Dependencies. J. of the ACM,
31(4):718–741, 1984.
5. P. A. Bernstein and S. Melnik. Model Management 2.0: Manipulating Richer
Mappings. In SIGMOD, pages 1–12, 2007.
6. A. Bonifati, G. Mecca, A. Pappalardo, S. Raunich, and G. Summa. Schema Mapping Verification: The Spicy Way. In EDBT, pages 85 – 96, 2008.
7. R. Chirkova, L. Libkin, and J. Reutter. Tractable XML Data Exchange via Relations. In CIKM, 2011.
8. R. Fagin, P. Kolaitis, R. Miller, and L. Popa. Data Exchange: Semantics and Query
Answering. TCS, 336(1):89–124, 2005.
9. R. Fagin, P. Kolaitis, A. Nash, and L. Popa. Towards a Theory of Schema-Mapping
Optimization. In ACM PODS, pages 33–42, 2008.
10. R. Fagin, P. Kolaitis, and L. Popa. Data Exchange: Getting to the Core. ACM
TODS, 30(1):174–210, 2005.
11. R. Fagin, P. G. Kolaitis, L. Popa, and W. C. Tan. Schema Matching and Mapping,
chapter Schema Mapping Evolution Through Composition and Inversion. 2011.
12. A. Fuxman, M. A. Hernández, C. T. Howard, R. J. Miller, P. Papotti, and L. Popa.
Nested Mappings: Schema Mapping Reloaded. In VLDB, pages 67–78, 2006.
13. L. M. Haas, M. A. Hernández, H. Ho, L. Popa, and M. Roth. Clio Grows Up: from
Research Prototype to Industrial Tool. In SIGMOD, pages 805–810, 2005.
14. B. Marnette, G. Mecca, and P. Papotti. Scalable data exchange with functional
dependencies. PVLDB, 3(1):105–116, 2010.
15. B. Marnette, G. Mecca, P. Papotti, S. Raunich, and D. Santoro. ++Spicy:
an opensource tool for second-generation schema mapping and data exchange.
PVLDB, 4(11):1438–1441, 2011.
16. G. Mecca, P. Papotti, and S. Raunich. Core Schema Mappings. In SIGMOD, 2009.
17. R. J. Miller, L. M. Haas, and M. A. Hernandez. Schema Mapping as Query Discovery. In VLDB, pages 77–99, 2000.
18. R. Pichler and V. Savenkov. DEMo: Data Exchange Modeling Tool. PVLDB,
2(2):1606–1609, 2009.
19. L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernandez, and R. Fagin. Translating
Web Data. In VLDB, pages 598–609, 2002.
20. L. Seligman, P. Mork, A. Halevy, K. Smith, M. J. Carey, K. Chen, C. Wolf, J. Madhavan, A. Kannan, and D. Burdick. OpenII: an Open Source Information Integration Toolkit. In SIGMOD, pages 1057–1060, 2010.
21. B. ten Cate, L. Chiticariu, P. Kolaitis, and W. C. Tan. Laconic Schema Mappings: Computing Core Universal Solutions by Means of SQL Queries. PVLDB,
2(1):1006–1017, 2009.

Nicola Ferro and Letizia Tanca (Eds.): SEBD 2012, Edizioni Libreria Progetto, Padova, Italia
ISBN: 978-88-96477-23-6, Copyright (c) 2012 - Edizioni Libreria Progetto and the authors

DataXFormer: A Robust Transformation Discovery
System
Ziawasch Abedjan∗ , John Morcos† , Ihab F. Ilyas† , Mourad Ouzzani‡ , Paolo Papotti‡ , Michael Stonebraker∗
∗ MIT

CSAIL
of Waterloo
‡ Qatar Computing Research Institute, HBKU
{abedjan,stonebraker}@csail.mit.edu, {jmorcos,ilyas}@uwaterloo.ca, {mouzzani,ppapotti}@qf.org.qa
† University

Abstract—In data integration, data curation, and other data
analysis tasks, users spend a considerable amount of time
converting data from one representation to another. For example
US dates to European dates or airport codes to city names.
In a previous vision paper, we presented the initial design of
DataXFormer, a system that uses web resources to assist in
transformation discovery. Specifically, DataXFormer discovers
possible transformations from web tables and web forms and
involves human feedback where appropriate. In this paper, we
present the full fledged system along with several extensions. In
particular, we present algorithms to find (i) transformations that
entail multiple columns of input data, (ii) indirect transformations
that are compositions of other transformations, (iii) transformations that are not functions but rather relationships, and
(iv) transformations from a knowledge base of public data. We
report on experiments with a collection of 120 transformation
tasks, and show our enhanced system automatically covers 101
of them by using openly available resources.

I.

I NTRODUCTION

When integrating data from multiple sources there is often
a need to perform transformations. These transformations
entail converting a data element from one representation to
another, e.g., unit, currency, and date format conversions, or
generating a semantically different but related value, e.g.,
airport code to city name, or ISBN to book title. Consider
the simple scenario of Figure 1 where we are integrating two
tables containing information about soccer players. We can
quickly notice differences in how data is represented in each
table. First, one table records player height in meters while
the other uses feet and inches. Second, one table stores league
and team symbol while the other records team name. Both
require transformations. While the first transformation can be
computed via a formula, the second requires looking up a
dictionary or other data sources.
The Web contains huge amounts of data that can be used
as a reference for these transformations. However, it is clearly
very tedious for a human to construct such transformations
manually, which is one of the reasons why data analysts
spend the overwhelming majority of their time “massaging”
their data into usable form. Discovering transformations on
demand requires a concentrated effort of engineers and domain
experts to identify the relevant web sources, understand the
relationships among their attributes, and write programs that
convey this information in the target table [1]. We aim to
support the user by automatically discovering transformations
given some input and output examples.

978-1-5090-2020-1/16/$31.00 © 2016 IEEE

Name	
  

Team	
  

Height	
   League	
  

Lastname	
   Teamname	
  

Messi	
  

FCB	
  

1.70	
  

La	
  Liga	
  

Neuer	
  

FC	
  Bayern	
  Munich	
   6’4’’	
  

HT	
  

Neuer	
  

FCB	
  

1.93	
  

Bundesliga	
  

Ronaldo	
  

Real	
  Madrid	
  

Pirlo	
  

Juve	
  

Serie	
  A	
  

Pirlo	
  

6’1’’	
  
5’10’’	
  

Source 1

Source 2

Lastname	
   Team	
   League	
  

Teamname	
  

HT	
  

Height	
  

Neuer	
  

FCB	
  

Bundesliga	
   FC	
  Bayern	
  Munich	
   6’1’’	
  

1.93	
  

Messi	
  

FCB	
  

La	
  Liga	
  

?	
  

1.70	
  

Pirlo	
  

Juve	
  

Serie	
  A	
  

?	
  

5’10’’	
  	
   ?	
  

?	
  

Real	
  Madrid	
  

6’4’’	
  	
  	
   ?	
  

Ronaldo	
  

Fig. 1: Values transformation is a critical activity when integrating different
sources of data.

In our initial paper, we showed how to explore web
tables and web forms to discover transformations [2]. We
used a large corpus of web tables to find the desired user
transformations. For static cases, such as converting airport
codes to city names, web tables are a useful information
source since there are several such conversion tables on the
Web. For finding time-varying transformations, such as Euros
to Dollars, there are several web forms that will perform
them. More recently, we have made our system available on
http://www.dataxformer.org and presented a demo at SIGMOD
2015 [3]. Consequently, we have expanded our collection of
transformation tasks from 50 to 120. Our initial prototype covered only 52% of the transformation tasks. This is especially
due to the limited types of transformations that were supported,
focusing only on functional 1-to-1 relationships, while the
extended workload contains non-functional and multi-column
transformations. Also, our previous approach was limited to
the discovery of transformations that appeared in the same
table. Thus, we substantially improved DataXFormer and
were able to increase its coverage to 84%, which is a significant
improvement. In this paper, we present the full-fledged system
with a focus on algorithms for web tables and the study of
the capabilities and the limits of these tables for data transformation discovery. We should note that in practice, our system
can be deployed either following the SaaS model on the cloud,
where DataXFormer runs as a service on the cloud with
a RESTful interface, or on-premise, where DataXFormer
runs on the enterprise computing infrastructure. We begin in
Section II with a detailed specification of the transformation
problem and the overall architecture of DataXFormer with a

1134

ICDE 2016 Conference

A

brief description of previously introduced components. In the
following sections, we introduce our new algorithms that are
the main contributions of this paper:
•

•

E:
Q:

We introduce a general algorithm for discovering
tables that contain the desired transformations (Section III). The new algorithm uses an inductive approach to expand the set of web tables that might
match the user’s request and allows multiple attributes
as input values.
We present an efficient approach to discover suitable
join attributes in web tables and hence to identify
multi-hop transformations (Section III-C).

•

We show how the above algorithms can be adapted
to find non-functional mappings, for example the
transformation from a soccer team to all of its players,
with minimal user verification efforts (Section IV).

•

We show how to extend DataXFormer to search
knowledge bases (KBs) [4] for transformations that are
not covered by web tables or web forms (Section V).

Finally, we present a comprehensive evaluation in Section VI where we report on experiments with a collection
of 120 transformation tasks, and show that our enhanced
system finds 101 of them, with high accuracy. A discussion
of related work (Section VII) and future directions of research
(Section VIII) conclude the paper.
II.

P ROBLEM STATEMENT AND OVERVIEW

In this Section, we give the problem statement for transformation discovery, describe the targeted types of transformations, and present the overall architecture of DataXFormer.

xi	
  

B	
  

yi	
  

6’1’’	
  

1.93	
   E:

xq	
  

yq	
  

xi	
  

yi	
  

xq	
  

yq	
  

C

Bundesliga	
   FC	
  Bayern	
  Munich	
   E:

FCB	
  

Q:

Q:

xi	
  

yi	
  

FC	
  Bayern	
  Munich	
  

xq	
  

Neuer	
  

yq	
  

FC	
  Bayern	
  Munich	
  

?	
  

5’10’’	
  

?	
  

FCB	
  

La	
  Liga	
  

?	
  

6’4’’	
  

?	
  

Juve	
  

Serie	
  A	
  

?	
  

?	
  

…	
  

?	
  

…	
  

…	
  

?	
  

…	
  
Real	
  Madrid	
  
	
  

Single attribute

Multi-attribute

Functional Transformations
(one to one, many to one)

?	
  
?	
  
…	
  

...	
  

…	
  

Non-Functional Transformations
(one to many, many to many)

Fig. 2: A: Functional single-attribute transformation: inches to meters. B:
Functional multi-attribute transformation: team abbreviation, league to team
name. C: Non-functional transformation: team name to team players.

to query the available corpus to produce the instances of yq .
Unfortunately, the first step suffers from serious drawbacks:
Modeling the explicit relationship R, either as a query [8]–
[10], [12] or as a mapping [4], [7], [11], is a hard problem
since it requires to discover exact matchings and mappings
among the sources [13], [14]. Moreover, these techniques often
assume the availability of well curated resources, which is
almost never true. In contrast, most useful transformations are
usually scattered among multiple resources, such as web tables
and automatically generated knowledge bases that contain
errors or do not carry any schema information. Therefore,
we propose an example-driven technique for searching and
pruning available resources to find the desired transformation
results and by doing so, we avoid the task of explicitly
modeling the relationship between the available examples.
B. Challenges

A. Problem Statement
The example-driven transformation discovery problem can
be defined as follows: Given a set of n example pairs E =
{(x1 , y1 ), . . . , (xn , yn )} that satisfy some hidden relationship
R, along with a set of query values Q, we want to find
all yq such that (xq ∈ Q, yq ) is a pair drawn from the
same relationship R. A pair (xi , yi ) ∈ E represents the
transformation of xi into yi .

Discovering transformations by example on large scale web
data poses several unique challenges:

Some transformations, such as in feet to meters (see Figure 1), are syntactic and can be computed by directly applying
a program or a formula to the input values [5], [6]. Other
transformations, that we classify to belong to the larger class
of semantic transformations, cannot be guessed by the input
values. They require lookups in some reference data expressing
a relationship among the attributes of interest.
Transformations can be many-to-one (functional), e.g., zip
code to city, or many-to-many (non-functional), e.g., books to
authors. Figure 2 illustrates different types of transformations
based on our running example. Input values can also be
composed of multiple attributes as in our example in Figure 2
with x1 =“FCB, Bundesliga”.
Most existing techniques [4], [7]–[12] that have been
proposed to produce a transformation leading to yq values can
be described as a two-stage approach: (1) explicitly model the
relationship R between the given xi ’s and yi ’s, and (2) use R

1135

1) Crawling, indexing, and querying the web resources:
Every resource type poses specific technical challenges
in terms of data collection, storage, and indexing because
of the web scale.
2) Complex latent search: Using the examples to locate relevant resources that encode the same hidden relationship.
On the one hand, requiring a resource to contain all of
the examples at once, e.g., as a conjunctive query, leads
to very limited recall, since it is rare that one resource has
all the examples. On the other hand, using small subsets
leads to resources that encode different relationships due
to the ambiguous values that refer to different real-world
entities, e.g., “Rome” and “Paris” match both cities and
restaurants, or that express more than one relationship,
e.g., “(Messi,La Liga)” matches relationships debutIn and
topScorerIn, but these two relationships return different
league values for input value “Ronaldo”.
3) Indirect transformation: transformations may not be found
directly in any given web resources but may still be
uncovered through joins.
4) Ambiguity in non-functional transformations: The number of transformation results might vary for each input
value. We have to incorporate an efficient way of supervision to identify false transformations results.

5) Integration of other systems: Plugging external tools as
additional providers require to adapt our problem specification to existing query interfaces.
In the next Section, we give an overview of
DataXFormer and discuss how it tackles the above
challenges by using new efficient algorithms with best
practice ideas and involving the user or expert into the
discovery process.
Transformation task: Examples (xi,yi) ∈E, values xq∈Q, [headers hx,hy]

Form Query
Adapter

Table Query
Adapter

Candidate
Tables

The Web
Refinement

Form Retrieval

Experts

Wrapped
Forms

Evaluation

Wrapper Generator

Combiner

Table
Filter

Direct
Transformer
Indirect
Transformer

Index

Relationship
Finder

Web
Tables

Complementary subsystems

HITs Query
Adapter

Tables

Forms

Crowd

Augment
Solution Presenter
Ranked transformations with scores

Fig. 3: DataXFormer architecture

C. System Overview
Figure 3 illustrates the architecture of DataXFormer.
Subsystems are deployed for each type of resources with
the goal of discovering transformations. Web tables cover the
largest set of semantic transformations among the different
resources as they explicitly contain the desired transformations
and allow to discover multi-attribute and non-functional mappings. Web forms can only be used through the accessible input
fields but proved to be very effective at covering single attribute
transformations that (i) are based on a specific formula, such
as fraction to decimal, (ii) are time-dependent, such as USD to
EUR, or (iii) have an infinite domain size, such as long/lat to
location. As we will see in Section V, we also exploit KBs to
extend the coverage of our system.
DataXFormer consumes a transformation task in the
form of n example pairs (xi , yi ) ∈ E along with a set
of m query values xq ∈ Q, and relevant column headers
hX1 , . . . , hXk and hY , when available. It simultaneously submits this task to each subsystem. In turn, each subsystem
first applies a filter step that leverages the given examples
to identify the most relevant resources, e.g., tables and web
sites, and then applies a refine step to resolve ambiguities and
contradicting results. The user can interact with the system
to validate intermediate results or wrap web forms [3]. Once
transformation results have been retrieved and scored, successful transformation results, if not already present as a full table,
are stored as new tables in the web tables subsystem.
DataXFormer also uses expert sourcing for validation
and creation of functional transformations [2]. We use the same
approach for validating composite and many-to-many relationships. Experts can also be involved in finding transformations
for cases where the other resources fail.
Web Tables The web tables subsystem discovers transformations by searching for tables that contain the given examples.

The intuition is that these candidate tables containing example
transformations from E might also contain possible results
for the remaining input values xq ∈ Q. The table retrieval
algorithm retrieves candidate tables by using an inverted index
that maps each cell value to its table and column identifiers.
Candidate results are then analyzed by the refinement component, which verifies the mappings in each candidate table
with respect to the given examples. The web tables subsystem
discovers functional multi-column, and non-functional transformations, as discussed in Sections III and IV, respectively.
Web Forms We use the same web form subsystem presented
in our demonstration [3]. Therefore, we only give a brief
overview of the subsystem in this section. A web form is a
user interface on a website that allows user-interaction through
different fields and buttons. As with web tables, we assume
a web form is relevant if it covers some of the n example
transformations. There are two main challenges in using web
forms: (1) as there is no common repository of web forms,
we have to look for them through billions of web pages;
and (2) any new web form appears to us as a black box.
This means that an invocation strategy (i.e., wrapper) has
to be developed to use the form for producing the desired
transformations. It has been shown [15] that both tasks are very
hard, even with human intervention. We tackle these challenges
by using search engines and a web browser simulator that
allows to analyze the HTML code of a form. The user
is involved whenever any of the tasks fail. DataXFormer
maintains a local repository of previously used web forms.
The repository is organized as a document store where each
document represents one wrapper. If the system fails to find
a relevant form in the repository, it uses a search engine to
find forms online. By examining the search engine results,
DataXFormer identifies candidate forms. Then, for each
candidate form, we generate a “wrapper” to be able to query
the form and to obtain the transformation values. In case the
system fails to generate a wrapper or to find the relevant form,
the user can be involved [3]. The wrapped forms are tested
using the input examples and are evaluated based on their
coverage in the evaluation step. Candidate web forms are then
queried using the input values from Q.
Complementary sources DataXFormer can be extended by
additional resources, such as knowledge bases, text documents,
or code repositories. We have presented ideas on how to
involve the crowd in our vision paper [2], and we show in
Section V how to use knowledge bases as a complementary
resource for transformation discovery.
In this paper, we shed light on the coverage and transformation quality achieved by the subsystems for web forms,
web tables, and knowledge bases, and explain their strength
and weaknesses for the transformation discovery use case.
III.

S INGLE -C OLUMN TO M ULTI -C OLUMN
T RANSFORMATIONS

The general workflow of the table subsystem for finding
functional transformations is to discover tables that cover a
subset of input values in Q. As mentioned in Section II-B, one
of the key challenges is to decide when a table is relevant to the
transformation task based on the given example pairs in E. The

1136

Tables

Columns

tableid	
   url	
  

>tle	
  

ini>al	
  
weight	
  

tableid	
  

colid	
  

header	
  

1	
  

World	
  
airports	
  

0.8	
  

1	
  

1	
  

Code	
  

1	
  

2	
  

Loca>on	
  

…	
  

…	
  

…	
  

4	
  

1	
  

apc	
  

4	
  

2	
  

Loca>on	
  

…	
  

….	
  

…	
  

www..	
  

2	
  

hHp…	
  

-­‐	
  

0.5	
  

3	
  

hHp…	
  

airports	
  

0.5	
  

…	
  

…	
  

…	
  

0.5	
  

indexes. While the table itself is a projection that is stored as
depicted in Figure 4, we use another projection on relation
Cells sorted on the tokenized values. The two projections
allow us to swiftly retrieve relevant table IDs for tokenized
input examples, and to easily load the content of a table based
on its table and column IDs. An additional trie index allows
fuzzy matching when the given transformation examples and
the values in the database differ with a small edit-distance.

Projection on Cells:
Sort order from left to the right

Cells
tableid	
   colid	
   rowid	
   term	
  

term	
  
tokenized	
  

term	
  
tableid	
   colid	
   rowid	
  
tokenized	
  

term	
  

1	
  

1	
  

1	
  

FRA	
  

fra	
  

ber	
  

1	
  

1	
  

5	
  

BER	
  

1	
  

1	
  

2	
  

JFK	
  

j-	
  

ber	
  

2	
  

1	
  

4	
  

BER	
  

…	
  

…	
  

…	
  

…	
  

….	
  

ber	
  

4	
  

1	
  

2	
  

BER	
  

3	
  

2	
  

1	
  

Dallas	
  

dallas	
  

berlin	
  

1	
  

2	
  

5	
  

Berlin	
  

….	
  

…	
  

…	
  

…	
  

….	
  

…	
  

…	
  

…	
  

…	
  

…	
  

4	
  

2	
  

4	
  

Hessen	
   hessen	
  

ord	
  

1	
  

1	
  

3	
  

ORD	
  

B. Querying Web Tables

Fig. 4: Schema for storing web tables in a column-store showing an airport
code to country example

While the index on input values enables the system to
retrieve all tables that contain a specific value, it is important
to filter irrelevant tables as early as possible. This is crucial
as we are working on millions of tables, which means that
we want to retrieve only those tables that contain a relevant
subset of examples in E. Our original prototype was optimized
to identify tables with two columns that contain at least τ
of the given example pairs (xi , yi ) ∈ E. In our previous
experimental study on 50 transformation tasks, τ = 2 proved to
be a reasonable filter for avoiding irrelevant tables and meeting
a sweet spot in terms of precision/recall [2].

next challenge is that most tables in the corpus have few rows
(on average 12). In a scenario with a small number of examples
(xi , yi ) and a much larger set Q, the system may fail to find the
latter by looking only at the relevant tables. Finally, different
resources might support different output values for the same
xq ∈ Q. Thus, we need a scoring system that incorporates
confidence scores for resources and transformation results.

SELECT colX1.tableid, colX2.colid, [colX2.colid,…], colYid
FROM
(SELECT tableid, colid
FROM Cells
WHERE term_tokenized IN (X1)
GROUP BY tableid, colid
HAVING COUNT(DISTINCT tokenized) >= tau) AS colX1,
[(SELECT tableid, colid
FROM Cells
WHERE term_tokenized IN (X2)
GROUP BY tableid, colid
HAVING COUNT(DISTINCT tokenized) >= tau) AS colX2, ...]
(SELECT tableid, colid
FROM Cells
WHERE term_tokenized IN (Y)
GROUP BY tableid, colid
HAVING COUNT(DISTINCT tokenized) >= tau) AS colY

Given these challenges, our solution is based on an inductive filter and refine approach that leverages intermediate
results in multiple iterations. This way DataXFormer finds
more tables on the fly by extending the set of the given
transformation examples with promising intermediate results.
In the following, we first discuss the index structure and the
filtering mechanism and how they can be easily generalized
for multi-column input values. Then we discuss the inductive
approach and the accompanying refine process to consolidate
transformation results.

WHERE colX1.tableid = colY.tableid
[AND colX1.tableid = colX2.tableid AND ...]
AND colX1.colid <> colY.colid
[AND colX1.colid <> colX2.colid AND colX2.colid <> colY.colid
AND ...]

A. Indexing Web tables
Traditional keyword search applications rely on predefined
foreign key relationships and specialized indexes that require
domain knowledge and context information. Since web tables
are heterogeneous, differ in schema and size, and oftentimes
lack column labels, we store them in the most general way.
Ideally, we require to have an index on all possible input values
to find relevant tables. In our previous paper, we compared the
storage of web tables as documents to a universal main table in
a column store [2]. The later storage scheme, described below,
turned out to be the most efficient option.

Fig. 5: Black: code for finding single column input values. Blue: Additional
code needed for composite input values.

Relation Cells in Figure 4 represents every cell of a web
table by a tuple with the IDs for table, column and row,
along with the tokenized, stemmed form of its value. We also
maintain additional metadata for each table, such as column
headers, source URL, title, and an initial weight expressing its
authoritativeness. The initial weight influences the confidence
score we compute to rate transformation results.

The universal main table structure enables us to submit
only one query for the retrieval of all tables that contain at least
τ of the input/output pairs. The query in Figure 5 joins two
subqueries, one to find the columns that contain the X values
and another to identify the columns that contain the Y values,
where a column is uniquely identified using the table and
the column IDs. By comparing the table IDs, DataXFormer
retrieves only column pairs that appear in the same table. The
result of the query is a set of triples, each comprising a table
ID and two column IDs. To maximize coverage, we tokenize
and stem every value x and y from the input.

We store the web tables in a multi-node Vertica instance.
Vertica employs projections (sorted views) on tables in place of

Using the above query skeleton, adapting the table retrieval component for multi-attribute input values xi =

1137

X1, X2, . . . , Xk is straightforward. We simply extend the
query with more subqueries; one for each additional input
column. The extensions are outlined by means of square
brackets and blue color. For every additional input column,
the query has to generate a subquery and additional checks in
the WHERE clause. The checks in the where clause enforce
that the discovered columns in the web tables are different
from each other. While identifying candidate tables this way
is very fast, we still need to make sure that the identified tables
contain the example values in the appropriate rows.
The cost of processing the above query increases with
the number of examples. This issue becomes pertinent as we
follow an inductive approach. After a filter-refine iteration,
we use the retrieved intermediate results to find additional
tables that eventually cover the remaining xq ∈ Q. Since not
every intermediate result is correct, they do not necessarily
contribute to the recall of our system. Additionally, querying
for a large number of examples hurts the performance without
a significant gain. We therefore limit the set of intermediate
results to the top k based on confidence scores that we compute
and update after each filter-refine iteration.
C. Joining tables
In the previous subsection, we only addressed the discovery
of tables that maintain both the input and the output values
of a transformation example. We refer to the strategy based
on this assumption as direct transformations. However, the
actual transformation input and output columns could reside
in different tables and be linked through other columns not
provided in the transformation task. This can happen if the
tables have a foreign key relationship or an inclusion dependency through two columns that share some values Z. We
deem transformations that were discovered based on joined
tables as indirect transformations. Unfortunately, we do not
have complete knowledge of the underlying schema, therefore
we cannot rely on existing algorithms, such as [16]–[18], that
rely on specialized indices, predefined foreign key-primary key
relationships, or meta-data information and dependencies.
To find the transformation result in this scenario, it is
necessary to join at least two tables and align the input and
output values accordingly. However, a brute-force search of
all possible inclusion dependencies among all web tables that
contain the actual input values X would be prohibitive. This
problem is even more striking when more than one table has to
be joined to link an example input value x to its output value
y. Accordingly multiple intermediate inclusion dependencies
lists Z1 , Z2 , . . . , Zk have to be discovered to serve as join
attributes. Our intuition to reduce the search space, without
missing useful joins for the transformation at hand, is to require
that the values zq (i) are functionally dependent on the input
values xq , and (ii) functionally determine the transformation
results yq , simultaneously.
Algorithm 1 searches for joinable tables in a breadth-first
manner. The algorithm consumes as input the initial examples
E and the values xq ∈ Q. With each iteration, the indirection
path increases until the maximum length of an indirection
path has been reached, or all xq values have been covered.
The algorithm starts (path = 1) by identifying all tables that
contain the x values of the provided examples (line 4). After

Algorithm 1 tableJoiner
Input: Initial examples E, a set of input values Q, initial tables TE
Output: tables
1: path ← 1
2: repeat
3:
if path = 1 then
4:
TX ← QueryForTables(E.X)
5:
TX ← TX \ TE
6:
for all T ∈ TX do
7:
[Z0 , Z2 , . . . , Zn ] ← findJoinColumns(T, E.X)
8:
for all Zi ∈ [Z0 , Z2 , . . . , Zn ] do
9:
TJ ← findJoinableTables(Zi , E, T )
10:
if TJ 6= ∅ then
11:
if Tj covers τ (xi , yi ) ∈ E then
12:
tables.Append(Tj )
13:
currentT ableP aths.Append(path < T, Zi >)
14:
else
15:
newT ableP aths ← ∅
16:
for all < Tp , Z >∈ currentT ableP aths do
17:
TZ ← QueryForTables(Z) \ previously seen tables
18:
for all Tz ∈ TZ do
19:
Tz ← Tz 1Z Tp
20:
[Z0 , Z2 , . . . , Zn ] ← findJoinColumns(Tz , Z)
21:
for all Zi ∈ [Z0 , Z2 , . . . , Zn ] do
22:
Tj ← findJoinableTables(Zi , E, Tz )
23:
if Tj 6= ∅ then
24:
if Tj covers τ (xi , yi ) ∈ E then
25:
tables.Append(Tj )
26:
newT ableP aths.Append(path < Tj , Zi >)
27:
currentT ableP aths ← newT ableP aths
28:
ClearFrequentedTables(currentT ableP aths)
29:
path = path + 1
30:
alltableP aths.Append(currentT ableP aths)
31: until |path| > maxLength ∨ X = scoredAnswers.X

excluding the tables that provide a direct transformation of
(xi , yi ) ∈ E, the algorithm checks each remaining table in
TX for potential join columns Zi (line 7). A column can serve
as a join column if a functional dependency X → Zi holds
with regard to the input values E.X that are covered by the
table at hand. This check is performed in findJoinColumns,
which requires a pairwise FD validation test with regard to
the values in other columns. If an FD between the column
that contains our input values X and another column Z in
the current table T holds, we extract the subset of values that
appear in the same rows as our given input values X and call
it Zi . Furthermore, we check whether the cardinality of the
discovered Zi is greater or equal than the corresponding E.Y .
If this is not the case, we know apriori that the functional path
to the transformation result cannot be maintained.
Next the algorithm searches for tables that can be joined
to the current table T based on each of the discovered IND
attributes Zi . If the table also contains correct mappings
to Y , we store the table as a new joined resource table.
f indJoinableT ables creates and filters joined tables that
maintain the approximate mapping E.X → Z → E.Y for τ
of the examples in E . Next, all joined tables in Tj are checked
for indirect transformations. We store the table that contains
the mapping X → Z if longer paths need to be processed.
In later iterations (path > 0), the procedure is repeated
by starting the search from the most recent set of Z values in
joined tables Tj that were discovered in the previous path.

1138

We treat each set Z as our current set of input values.
This maintains the semantics of the transformation as each
consecutive join maintains the functional dependency. As we
are following a breadth-first traversal of the joinable tables
graph, the set of Z values increases exponentially after each
iteration. Our strategy to reduce the search space is to require
the functional dependency constraint on at least τ of the initial
examples throughout the indirection and no contradiction with
regard to the given examples, as shown in Algorithm 1 (line 6).
The breadth-first approach ensures that DataXFormer finds
transformations from shorter indirections first, which is desirable considering that shorter indirection paths might constitute
stronger relations than longer indirection paths.
D. Scoring and Ranking Transformations
In the refine phase, we load the content of each candidate
column pair and check whether the values from a pair (xi , yi )
in the input query appear in the same row in the candidate
table. If τ examples still match after considering the row correspondence of the discovered (xi , yi ) pairs, DataXFormer
collects all transformations yq for xq ’s that reside in the same
table.
The retrieved tables may provide conflicting answers by
returning different yq values for the same xq input. A naive
reconciliation solution would be to deliver the most frequent
yq for each given xq (majority voting). However, this approach
suffers from its inability to properly score results and tables
while taking into account the iterative nature of the process.
Instead, we propose a solution with the following desiderata:
First, it is necessary to compute confidences scores for the
tables to estimate their authoritativeness and rate their success
in covering examples. Second, as we incorporate results of
previous iterations as additional examples, we also need a
confidence score for those examples. Third, in each iteration
we need to recompute the scores of previously found transformations. Finally, it should be possible to incorporate additional
score components based on the given dataset, such as schema
similarity, provenance-based scores, or user feedback.
We adopt an iterative expectation-maximization (EM) approach [19] that incorporates confidence scores. The confidence score of each table, i.e., the probability it provides
a correct answer, is estimated based on the current belief
about the probability of the answers. Initially each table is
assigned with a score based on the number of user examples
it covers and the Jaccard similarity of labels, if available. The
score of the table is weighted with an initial weight, which is
stored in relation T ables, and either has a default value or a
value assigned by an expert. The answer scores are updated
based on the newly computed table scores, and the process
is repeated until convergence is reached, i.e., the sum of all
score changes is below a very small value . In the end, for
each value xq ∈ Q, the scores of possible answers form a
probability distribution. The described process works for both
types of functional transformations, i.e., multi-attribute and
single attribute. All components of a multi-attribute input value
have to appear simultaneously to be considered.
Algorithm 2 describes the reconciliation process based on
EM and embedded within the overall workflow of the system’s
filter-refine iterations. The maximization and expectation steps
are described in Algorithms 3 and 4, respectively.

Algorithm 2 Expectation-Maximization
Input: A set of initial examples E = {(x, y), . . .}, input values Q
Output: Scored answers
1: answers ← E
2: tables ← E
3: f inishedQuerying ← f alse
4: oldA ← null
5: repeat
6:
if not finishedQuerying then
7:
tables ← QueryForTables(answers)
8:
tables ← tables ∪ tableJoiner(answers, Q, tables)
9:
for all t ∈ tables do
10:
for all answer(xq , yq ) ∈ t do
11:
if xq ∈ Q then
12:
UpdateLineage(t, answer)
13:
answers.Add(xq , yq )
14:
if not new xq was covered by tables then
15:
f inishedQuerying ← true
16:
UpdateTableScores(answers, tables) //maximization step
17:
UpdateAnswerScores(answers,
tables) //Expectation step
P
18:
∆scores =
|answers.score(x, y) − oldA.score(x, y)|
19:
oldA = answers;
20: until f inishedQuerying ∧ ∆scores < 

In each iteration of Algorithm 2, we query for tables using
the new weighted examples (line 7), until no more values in
Q can be covered (lines 6 and 15). We also incorporate in
line 8, tables that we joined on the fly. We treat each joined
table as a table on its own. In line 16, we implement the
maximization step by updating table scores (estimated errorrates) based on the current belief in the answers (answer
scores). At the bootstrap stage, initial scores are assigned based
on the percentage of user-given examples that were covered
by a table. In each iteration, if new unseen tables are found
in the query, the lineage of the newly found tables and the
answers (transformations) they provide are recorded for later
EM calculations. When updating table or answer scores, it is
necessary to be able to identify which tables contained which
answers. In the expectation step of every iteration, the scores
(i.e., probabilities) of the discovered answers are updated. The
system converges when the total (absolute) change in answer
scores is below a small value  (line 20).
The score of a table is determined by the ratio of the sum
of the scores of correct examples to the sum of all examples
that are believed to be correct. For examples in the table, we
retrieve the answer score (line 8 in Algorithm 3) computed in
the previous maximization step. Original examples maintain
a score of 1.0 since they are provided by the user. The test
IsM ax checks whether the score of the current pair (x, y) is
the pair that has the highest score among all pairs with input
value x. If this is the case, we assume that according to the
current belief (x, y) is a correct mapping and we increment the
counter good with its relative score. Otherwise, we increment
bad by 1. We assign a default score for the example input
values that do not occur in the table, unseenX to estimate
the relevance of the table on these examples. All scores are
weighed with the table prior priort , that is either set by a user
or set to a default value of 0.5.
Finally, the score of each table is multiplied by a smoothening factor α slightly less than 1.0 (line 14) to prevent producing zeroes when calculating answers scores. This factor also

1139

Algorithm 3 UpdateTableScores

IV.

Input: answers, tables
Output: estimated table scores
1: for all t ∈ tables do
2:
good ← 0
3:
bad ← 0
4:
total ← 0
5:
coveredXs ← {} //holds example x’s appearing in t
6:
for all answer(x, y) ∈ t do
7:
coveredXs ← coveredXs ∪ {x}
8:
score ←GetScore(x, y)
9:
if IsMax(score, x) then
10:
good ← good + score
11:
else
12:
bad ← bad + 1 P

13:
unseenX←
max score(x, y)
14:

F ROM N :1 T RANSFORMATIONS TO N : M
R ELATIONSHIPS

In functional transformations, the fact that there is only
one correct answer was used to prune many resources based
on the given set of examples E and to keep only the most
likely transformation value yq for an xq . However, in manyto-many relationships, a newly found yq0 can be another correct
mapping. In order to cover many-to-many (i.e., non-functional)
transformations, we have to adapt our scoring system to allow
multiple solutions for one input value, e.g., countries to k
cities. This is particularly hard for two main reasons: We
do not know how many values are correct for a given xq .
Restricting the output to a fixed number of values limits the
recall of the transformation. On the other hand, accepting
all possible mappings will result in extracting relationships
that do not adhere to the actual transformation task. This
easily leads to mixing heterogeneous values in the result.
This of course impacts the precision of the transformation.
We should note that while it is possible to discover more
functional transformations by joining tables on the FD constraint (Section III-C), applying indirect transformations for
non-functional transformations would require a completely
different strategy as the pruning strategy via FDs cannot be
adopted. We leave this for future work.

x∈coveredXs∧(x,y)∈answers
/
priort ·good
)
priort ·good+(1−priort )·(bad+unseenX)

SetScore(t,α ·

Algorithm 4 UpdateAnswerScores
Input: answers, tables
Output: estimated answer probabilities
1: for all x ∈ X do
2:
A ← answers.getAnswers(x)
3:
scoreOf N one ← 1
4:
for all table ∈ answers.getT ables(x) do
5:
scoreOfNone ← scoreOfNone · (1 − table.score)
6:
for all (x, y) ∈ A do
7:
score(x, y) := 1
8:
if table supports (x, y) then
9:
score(x, y) ← score(x, y) · table.score
10:
else
11:
score(x, y) ← score(x, y) · (1 − table.score)
12:
for all (x, y) ∈ A do
P
13:
score(x, y) ← scoreOfNone+score(x,y)
score(x,y)

In the following, we discuss two approaches for the general n:m mapping problem. The first automatically clusters
mappings into correct or false ones based on their score
distribution. The second, implemented in DataXFormer,
asks the user for feedback on effectively chosen sample results.
A. Automated Clustering

(x,y)∈A

represents the uncertainty about the rest of the table, resulting
from the ambiguity in transformations and table dirtiness.
Algorithm 4 shows the expectation step. We make the
simplifying assumption that the error rates of the tables are
independent. Such assumption allows us to calculate the estimated probability that a value y is the transformation of a
value x by a simple multiplication. For every value x ∈ Q,
the probability that y is the transformation of x is computed
as the product of the probability of correctness of each table
t that supports (x, y), estimated by the score of the table
score(t), and the probability of each table t0 listing another
value being wrong, estimated as 1 − score(t0), where score(t)
is the estimated error rate of the table t (lines 4 to 11).
We also consider the possibility that none of the provided
answers for this x is correct by aggregating scoreOfNone.
The estimated probabilities are then normalized by dividing
them over the sum of the scores of the possible answers as
well as the score of the event that none of the answers are
correct (lines 12 to 13). The scores are normalized to form a
probability distribution, with the highest score being used as
an example for the next iteration, with its probability as the
weight. Recall that table scores are multiplied by α to avoid
zeroes when multiplying probabilities.

Once the candidate yq values for a xq are retrieved and
scored, one way to identify the set of interest is to use a
threshold. However, we would have to chose an arbitrary
threshold that only holds for some use cases. Another way
is to use clustering to separate yq values with high scores and
yq values with low scores. We start by defining the distance
between two scores as their absolute difference.
Definition 1: Let y1 and y2 be two candidate values for an
input value xq and c a function to get their score, the distance
between x1 and x2 is: D(y1 , y2 ) = |c(y1 ) − c(y2 )|
Two values with high scores are expected to have a smaller
distance between them than the distance between a highscoring value and a low-scoring one. Our goal is to obtain
an optimal separation between high- and low-scoring values.
Let H be the set of high-scoring values and L the set
of low-scoring ones. Intuitively, a separation is preferable to
another one if by adding a value x ∈ L to H, the difference
between the sum of pair-wise distances among all values of
H ∪ {x} and the sum of their scores becomes smaller. The
intuition is clarified in the following gain function.
Definition 2: Let S consist of the candidate values for yi ∈
Yq and L be a subset of S. We define the gain of L as the
sum of all scores of elements in L subtracted by the sum of
all score distances among elements in L:
X
X
X
G(L) =
c(s) −
D(yj , yk )
s∈L

1140

1≤j<|L| j<k≤|L|

We define an optimal separation as the one that maximizes
this function for H. As an optimal separation requires an exponential exploration over the possible subsets, we use a nearest
neighbor chain algorithm for agglomerative clustering [20].
We first order tuples in S in descending order wrt their scores,
and create a cluster for each value. We pick the highest scoring
cluster, and keep adding to it the next value in the order, while
computing the separation gain at each step. We terminate after
reaching a separation where the gain attains a local maximum.
The algorithm requires a linear space and quadratic time (pairwise distances) in the number of tuples.
B. Generating Validation Samples for n:m Relationships
The major flaw in the above approach is that it does
not have the opportunity of taking into account any negative
examples that would penalize irrelevant resources and their
candidates. In the absence of any known negative information,
we need to ask the user for feedback. Thus, we extend our
web table system for discovering functional transformations
with a feedback round to the loop. After every iteration, some
mappings are selected for validation, and the user’s feedback
is propagated to prune tables with incorrect mappings. For
this purpose, we slightly modify the formula in line 14 of
Algorithm 3 to:
α·

priort · good − 2 · (1 − priort ) · bad
priort · good + (1 − priort ) · (bad + unseenX)

The variable bad refers to the number of false mappings
contained in a table. To be conservative, this number is
doubled. Whenever the score is negative, we drop the table
completely as a supporting source. The user feedback after
each filter and refine iteration also prevents the system from
using wrong transformation results as examples for the next
iteration. In our experiments, the algorithms usually converged
before the 5th iteration, when asking the user for 10 validations
per iteration.
The challenge is then how to identify the most useful set of
examples for user validation. Similar to any other classification
task, validating a transformation sample can improve the
precision of our approach only if the sample represents the
whole set of discovered transformations. For our approach,
whenever false mappings are retrieved in the filter phase, it is
desirable that some of them appear in the validation sample.
This way, it is possible to identify tables and sources that lead
to false mappings, which are more critical for the quality of
the approach than the confirmation of correct mappings. We
feed these gained insights into the EM model and recompute
the scores of tables and answers. In the following, we compare
different policies for selecting the mappings to validate, with
the goal of maximizing the number of false mappings.
Frequency-based Selection A mapping between two values
can appear in multiple tables. If a mapping has a high
frequency, it co-occurs in several tables given the user-given
examples. The frequency could be considered as a score for the
relevance of identified mappings. Hence, a natural intuition to
identify false mappings for the validation is to expose lowfrequent ones to the user. Of course, this approach might
overlook prominent but irrelevant mappings. Penalizing a rare
mappings does not have a strong effect on the learning process
because it affects only few tables.

Score-based Selection Following the intuition of relevance in
the frequency-based selection method, one can also incorporate
the actual answer scores that have been computed by the
expectation maximization model in the functional scenario
from Section III-D. Notice that the expectation maximization
model scores tables and their mappings based on the ratio
of existing true and false mappings. Although, in the n:m
scenario, false mappings are not known before the first user
validation, we can still rank the answers by scores because the
coverage of true values affects the score.
Diversified Selection A further intuition to generate a validation set is to represent as many tables as possible. Thus,
example selection can be modeled as a maximum coverage
problem, where each example is associated with the set of tables that contain it: given a number of examples (sets) we want
to maximize the number of covered tables (the elements). This
problem is NP-hard, but the simple greedy algorithm has linear
complexity and guaranteed approximation ratio. However, the
maximum coverage strategy is likely to lead to a validation
sample of very frequent mappings that also co-occur in many
tables. This way it is not possible to distinguish tables from
each other. Thus, we prioritize mappings that have a minimum
overlap w.r.t. their containing tables. We extend the problem to
the weighted version, where we want to maximize the number
of covered tables and the number of non-overlapping tables
(the weight). For this purpose, we start with the least frequent
mapping and greedily add mappings that have the least overlap
in tables with the already chosen mappings. The weighted
version of the algorithm maintains the same approximation
ratio of the standard version. In Section VI-C2, we compare
the above proposals and show that the diversified selection
based on minimal overlap yields the best results.
V.

C OMPLEMENTING A PPROACHES

DataXFormer can also be complemented with other
querying systems. A compelling example are alternative systems that try to discover relationships by mapping relational
tables to knowledge bases. In the following, we outline how
we adapted a data curation system that relies on knowledge
bases for our transformation discovery problem.
Knowledge Bases for Transformations. Human-curated
knowledge bases (KBs) contain high-quality information,
which, intuitively, can lead to high precision in the discovered
yq values even in the presence of one source only. In fact,
differently from the web tables, it is possible to apply discovery
algorithm to explicitly identify the underlying relationship in
terms of the target ontology [4]. An obvious drawback of
knowledge bases is that they usually only cover prominent
head-topic transformations, thus limiting the general coverage.
Moreover, a knowledge base usually suffers from inconsistencies that affect the accuracy of discovered patterns [21].
We leverage recent developments in the discovery of mappings between a relation and a KB [4]. This system generates
candidate mappings to express the underlying relationship in
terms of the knowledge base, based on support and internal
coherence. A mapping is a graph whose edges are properties
(relationships) in the knowledge base and its internal nodes
are types or placeholders for entities in the knowledge base.
Its end nodes are literals that corresponds to attributes in the

1141

discuss the coverage of the Web resources, and finally report
the quality of the transformations at the instance level.
A. Experimental Setup
Our evaluation refers to 120 transformation tasks that
comprise 79 functional single column, 10 multi-column, and
31 non-functional transformation tasks. We manually collected
ground truth for each of the tasks with a finite domain, such
as city and zipcode to country, and generated random input
values for formula based transformations, such as fraction to
decimal.

Fig. 6: Mapping an input query to a knowledge base

example given as input. A mapping applies to a given example
if (i) there are entities in the KB that align with the connected
types and relationships, and (ii) the corresponding literals at
the end nodes match the values in the example. For example,
consider the relation in Figure 6 reporting a list of countries
with their capitals. On the right hand side of the figure, the
reported mapping holds for the first tuple if there is a triple
(“USA”,hasCapital,“Washington DC”) in the KB.
Two-stage Discovery. Given a trusted KB, in the first step,
we use the given examples to discover the explicit mapping
between the dataset and the KB. As for the matching of the
tables, we are interested in mappings that hold for a minimum
number of examples. It is possible that multiple patterns
apply to the example mappings. In case they have the same
support, we favour more restrictive patterns. Consider again
Figure 6. Assume we have a second relationship hasCity that
matches our examples. hasCity clearly subsumes hasCapital.
If they have the same support (i.e., they are all capitals), we
would prefer the first, which is more restrictive (i.e., has a
lower frequency). In our study we take the top-1 mapping for
functional transformations and use the automated clustering
approach in Section IV for non-functional mappings.

We use the Dresden Web Table Corpus [24], which contains
about 120 millions web tables from about 4 billions web pages.
Additionally, we cleared the tables from all text and comment
columns with more than 200 characters to reduce storage and
runtime. The tables are stored in a four node Vertica DBMS
consuming 700 GB of disk space including all projections. The
knowledge base subsystem, adapted from Katara [4], is tested
on top of the most recent versions of the YAGO and DBpedia
KBs.
We consider two metrics: (i) coverage and (ii) quality. The
coverage refers to the fraction of transformations for which a
system was able to find relevant web tables, web forms, and
mappings with a KB. The quality assesses the correctness and
completeness of a transformation task in terms of precision
and recall.

In the second step, the discovered relationship is used for
each xq , e.g., “France”, to find all the yq values related to it
through this mapping in the KB, e.g., “Athens”.
Implementation. For fast navigation and querying of the
KB, triples in DataXFormer are indexed by all components:
subject, object and predicate. Denoting the subject by ‘S’, the
predicate by ‘P’ and the object by ‘O’, the resulting six indices
are SPO, SOP, PSO, POS, OSP, and OPS. Literals always fall
into objects of triples. The main disadvantage of this setting is
that storing a KB takes 6 times its original size, but it allows
fast querying by any part of a triple. Our implementation works
on top of YAGO [22] and DBpedia [23], two general purpose
KBs.
Mapping to curated resources, such as a KB, can return
high quality transformations if the mapping is encoded in the
resource. The problem, however, is coverage, as a general
purpose KB usually only covers head topics, as we show next.
VI.

C ASE S TUDY

To evaluate our system, we collected 120 transformation
tasks from real world users and experts from a data curation company, more than doubling the size of our previous
study [2]. In the following, we describe our experimental setup,

Fig. 7: The Venn diagram illustrates the number of transformations covered
by each resource

B. Coverage of Web Resources
Figure 7 depicts a diagram on how many of the 120
transformation tasks could be covered by the given resources.
The number of covered transformation tasks are put between
parentheses in front of each resource while the other numbers
show the overlaps. A transformation is deemed to be covered
if our system finds a resource, such as a web table, that can at
least partially solve the task, i.e., there is a table that contains
at least three pairs of input/output values from the ground truth.
As we can see, most of the transformation tasks (76) can
be covered by web tables, followed by KBs and web forms.
Interestingly, the web tables nearly completely subsume the
coverage of the KB subsystem. Only 11 transformations could
be exclusively covered with KBs, including transformations,

1142

such as soccer player to birth place, soccer player to birthdate,
or country to head of state. This is expected since discovering
exact, complex relationships is hard without human supervision, and the KBs have strong evidence only for head topic
transformations. The web form subsystem covers 28 of the
transformation tasks, all of which are functional single column
transformations, including for example celsius to fahrenheit,
airport code to city, or ip address to domain. Among those
transformation tasks, 14 could only be covered by web forms,
mostly relating to formula based transformations.
Figure 7 also illustrates the difference in coverage between
DataXFormer and its previous version. The new system
covers 35 more transformations. In particular, 28 transformations are covered by means of the newly proposed approaches,
namely, indirect discovery, multi-column, and non-functional
transformations, and 11 with the addition of a KB system.
Transformations that could not been covered by any of our web
resources, included English to German, sentence to language,
text to encoding, Gregorian to Hijri, car plate to details,
company to Bloomberg id, bank to country, bank+city to swift,
bank to city,bank to swift code, for which we could find a web
form but could not wrap the web form with our system.
TABLE I: C OVERAGE BY TRANSFORMATION
Approach
Single Column
Multi Column
Non-functional
Total

TYPE

#

tables

forms

KB

79
10
31
120

50 (63%)
7 (70%)
19 (61%)
76 (63%)

28 (23%)
0
0
28 (23%)

17 (22%)
0
17 (55%)
34 (28%)

Joined
coverage
68 (86%)
7 (70%)
25 (81%)
101 (84%)

Table I shows the coverage with regard to the transformation task type. While multi-column tasks could only be
covered by the new web tables subsystem, non-functional
transformations could be covered by both the tables and
the KB subsystems. Compared to the its original version,
DataXFormer now also covers one more functional transformation by means of its indirect transformation algorithm.
The transformation Unesco site to country did not appear in
any web table explicitly, but could be covered by joining
two tables, each containing one side of the transformation,
on columns with country ISO codes. We report the indirection
result with max path length of 1. Our studies showed that
allowing a path length higher than 1 did not improve the
results. At the same time, the runtime was strikingly high.
While the algorithm runs a couple of minutes for path length
1, it takes several hours for path length 2.
C. Transformation Quality
We measure the quality of each transformation result
w.r.t. precision and recall. In our previous study, we reported
experiments on the effects of the threshold τ and of the number
of examples n [2]. The main insight is that τ = 2 leads
to the highest harmonic mean (F-Measure) of precision and
recall on our workload. Threshold τ = 1 leads to more false
positives, resulting in low precision, while higher values for
τ penalizes the recall. We found that the average precision is
above 90% for any number of examples n, while the recall
increases steadily with higher values, e.g., 32% for n = 2,
70% for n = 5, and 80% for 20 to 40 examples.

In this paper, we discuss the performance of each subsystem on functional transformations. We then show the impact of
different example selection strategies for semi-automatic discovery of non-functional transformations. For all the following
experiments, we set τ = 2 and n = 5.
TABLE II: AVERAGE PRECISION AND RECALL OF EACH SUBSYSTEM
Subsystem
Web tables
Web forms
KB
best

Precision
0.80
0.96
0.32
0.87

Recall
0.76
0.83
0.19
0.76

#
57
28
17

1) Functional Transformations: Table II illustrates the average precision and recall values for all functional transformations that were partially covered by the respective subsystem.
For all the experiments, we used 5 randomly chosen transformation examples from the available ground truth. Surprisingly,
the KB subsystem shows worse performance than the web
tables and web forms subsystems. There are several reasons
for this result. First of all, KBs can suffer from incompleteness
and inconsistencies in the use of the relationships, e.g., the
properties location and locationCity are used interchangeably
for the same semantics [21]. In addition, the same property
might have values with completely different semantics, e.g.,
location might point to a city, a region, or even a country. This
ambiguity may lower the recall, as the property path would not
always lead to the desired transformation.
Aggregating the precision and recall of the best component for a specific transformation results in 87% precision
and 76% recall. A web form usually yields high precision
results as the corresponding transformation tasks implement a
formula, which always returns the correct transformation upon
a successful wrapping. Web tables yield very high results on
tasks where the complete domain of the transformation fits in
few overlapping tables. For example, there are single tables
that contain transformations, such as university to state (0.93
Pr/0.93 Re), or airport code to city (1.0 Pr/0.80 Re).
TABLE III: M AJORITY VOTING VS . EM
Method
Majority voting
EM model

Precision
0.56
0.80

PROPAGATION SYSTEM

Recall
0.68
0.76

F-Measure
0.66
0.78

Table III compares the EM scoring model against a majority voting approach that scores answers by their frequency. The
table denotes the average precision and recall over all functional transformations and shows that EM model significantly
improves the performance over a naive voting approach.
TABLE IV: AVERAGE

PRECISION AND RECALL OF WEB TABLES ALGO RITHMS FOR FUNCTIONAL TRANSFORMATIONS

Algorithm
Direct single column
Direct multi column
Indirect single column

Precision
0.83
0.95
0.79

Recall
0.76
0.67
0.56

#
49
7
14

Table IV additionally breaks down the results based on the
approach that is applied to solve the task. For this experiment,

1143

TABLE V: AVERAGE PRECISION AND RECALL OF DIFFERENT APPROACHES
FOR THE NON - FUNCTIONAL TRANSFORMATIONS
Approach

Precision
supervised
Frequency based (least)
0.82
Frequency based (most)
0.69
Score based
0.72
Diversified (coverage)
0.84
Diversified (least overlap)
0.86
unsupervised
Automated clustering
0.45
Bayesian network
0.35
KB
0.3

Correct	
  

Diversiﬁed	
  (max.	
  
set	
  cover)	
  

Recall

FMeasure

Score	
  based	
  

0.57
0.62
0.62
0.56
0.58

0.67
0.66
0.67
0.67
0.69

Frequency	
  based	
  
(least)	
  

0.25
0.63
0.25

0.3
0.45
0.27

Wrong	
  

Frequency	
  based	
  
(most)	
  
Diversiﬁed	
  (min.	
  
overlap)	
  
0	
  

2	
  

4	
  

6	
  

8	
  

10	
  

Fig. 8: The average ratio of correct and wrong transformations selected by the
strategies for user-verification.

we run our system twice for each covered single column
transformation task: first considering only direct transformations and then considering only joined tables. As expected,
the indirect transformation approach yields poorer precision
and recall results if performed in isolation. Therefore, the best
way for finding useful transformations is to try to find direct
mappings first, and then to turn to indirect transformations for
covering values from Q that have not been covered.
2) Non-Functional Mappings: We study the impact of 3
supervised and 3 unsupervised methods for the non-functional
transformations. In particular, we use the unsupervised clustering and the supervised selection strategies over the web
corpus from Section IV-B, the unsupervised KB subsystem,
and a state of the art approach based on Bayesian networks
for discovering set of values from web resources [25]. In
particular, the Bayesian approach learns true positive and false
negative rates of sources by sampling from skewed Beta and
Bernoulli distributions, but does not assume any ground truth
as input. Therefore, we adapted it to incorporate the given
examples as true facts without sampling their probability.
Table V reports averaged results for every method on the
subset of 31 non-functional transformations that were covered
by that subsystem. Each transformation was executed with 3
supervised iterations. In each iteration, the user validated 10
example results. The strategy for diversified example selection
based on minimal overlap yields the best results. The bad
recall performance of the clustering approach is caused by the
exclusion of supposedly wrong results (false negatives) as it
does not allow rare correct transformations to be captured in
the results. Despite the examples, the inherent assumption of
global truth of prior distributions in the Bayesian approach
leads to arbitrary samples of truth probabilities that overestimate the rating of a table leading to low precision but
relatively high recall. The approaches on top of web tables
outperform the KB approach in terms of precision, because
ambiguities and inconsistencies in KBs are present also in the
non-functional scenario.
Figure 8 shows why the approach based on minimal overlap
yields the best results. It selects on average more incorrect
examples, which can be flagged as wrong transformations by
the user and ultimately lead the algorithm to assign correct
scores to the sources. At the same time, it affects more tables
with its maximization effort than the approach that selects the
least frequent or least scored transformation results.
Figure 9 shows the effect of the number of iterations on
the transformation quality with the diversification strategy. At

1	
  
0.9	
  
0.8	
  
0.7	
  
0.6	
  
0.5	
  
0.4	
  
0.3	
  
0.2	
  
0.1	
  
0	
  

Pr	
  (before)	
  

1	
  

2	
  

Pr	
  (a6er)	
  

3	
  

	
  R	
  (before)	
  

R	
  (a6er)	
  

4	
  

5	
  

supervised	
  Itera-ons	
  

Fig. 9: Average precision and recall of DataXFormer before and after
validating 10 examples per iteration.

each iteration the user validated 10 examples. The plot shows
that the precision increases with the validation after each step.
However, we notice a diminishing return in this gain, because
we quickly reach enough information to make a decision. At
the same time, the recall of the system slightly increases at
each iteration. We achieved similarly results when running the
same experiment with 20 validation examples per iteration.
VII.

R ELATED W ORK

There have been several attempts to tame the difficult task
of transformation discovery [5], [6], [26]. Singh et al. [26]
propose an approach for semantic string transformations,
which is then implemented in MS Excel. While their approach
combines syntactic and semantic transformations, it is limited
to tasks where the tables containing the desired transformations
are known a priori and are only very few, mostly 1 or 2 according to their benchmark. Similarly, Kandel et al. [6] support
a language for the transformations involving manipulation of
the data in the current relation. Our approach instead identifies transformations from a huge corpus of tables, requiring
handling noise and ambiguity from these tables. Arasu et
al. [5] address the problem of resolving abbreviations by string
matching techniques. We discover this kind of transformations
by explicitly looking them up in web tables.
Research on web tables has mostly focused on issues
related to search, extraction, and integration [27], [28]. Web
tables have been also regarded as a large repository for
knowledge discovery. For example, InfoGather [29] addresses
entity augmentation by searching for related attributes of given
entities. It precomputes and indexes all inclusion dependencies

1144

to directly find overlapping tables. In our scenario with more
than 100 million tables and usually missing schema, it is
not feasible to discover and store all the possible inclusion
dependencies. Instead, we discover overlapping columns at
runtime. Furthermore, our system provides significant extensions through the various types of transformations.

[4]

Another line fo related research relates to providing information retrieval capabilities over structured databases. Most of
the proposals [8]–[12] focus on efficiently generating candidate
networks of joined tuples to form answers to a keyword query.
In some of these systems, such as [16], [18], specialized
indices or predefined foreign key-primary key relationships are
used to prune the space of candidate results. Others assume
the availability of context information [30] or meta-data and
dependencies among keywords [17]. However, we are not
interested in tuple networks, but rather in tables with columns
covering most of the example transformations and the input
values to be transformed. Since we do not have a complete
knowledge of the underlying schema, our approach depends on
instance matching (for web tables and KBs) and on discovering
needed meta-data (for web forms).

[7]

Finally, we are the first to address the problem of discovering transformations for sets of entities; a task that is
more challenging than simply discovering a unique target
value. Identifying the entities in a set can be seen as an
enumeration query in an open-world context. This problem has
been tackled in the context of crowdsourcing [31] and truth
discovery [25]. However, the crowdsourcing solution cannot
be directly applied to our setting as they assume clean data.
As we have shown in the experiments, the truth discovery
algorithm makes strong assumptions on the prior distributions
of the latent variables, while we do not make such assumption
and we are still able to achieve better results.
VIII.

[3]

[8]
[9]

[10]
[11]
[12]

[13]
[14]
[15]
[16]
[17]

[18]

[19]

[21]
[22]
[23]

[24]
[25]

[26]
[27]
[28]

R EFERENCES

[2]

[6]

[20]

C ONCLUSION

This paper presents a full fledged system for transformation
discovery. In particular, we focus on how our system fully
exploits web tables to discover multi-column, non-functional,
and indirect transformations. A comprehensive study based
on 120 transformation tasks demonstrates the usefulness of
our system and shows the strength and weaknesses of each
resource type for a transformation task. Future work requires
reasoning on filtering transformations within a resource. Currently, we do not reject subsets of a table but only a table as
a whole. An example-based approach combined with outlier
detection and String pattern analysis could make the resource
discovery more fine-granular. Furthermore, the discovery of
transformation results through text analysis is an interesting
and promising challenge.

[1]

[5]

S. Kandel, A. Paepcke, J. M. Hellerstein, and J. Heer, “Enterprise
data analysis and visualization: An interview study,” IEEE Trans. Vis.
Comput. Graph., vol. 18, no. 12, pp. 2917–2926, 2012.
Z. Abedjan, J. Morcos, M. Gubanov, I. Ilyas, M. Stonebraker, P. Papotti,
and M. Ouzzani, “DataXFormer: Leveraging the web for semantic
transformations,” in CIDR, 2015.
J. Morcos, Z. Abedjan, I. Ilyas, M. Stonebraker, P. Papotti, and
M. Ouzzani, “DataXFormer: An interactive data transformation tool,”
in SIGMOD, 2015.

[29]

[30]
[31]

1145

X. Chu, J. Morcos, I. F. Ilyas, M. Ouzzani, P. Papotti, N. Tang, and
Y. Ye, “Katara: A data cleaning system powered by knowledge bases
and crowdsourcing,” in SIGMOD, 2015, pp. 1247–1261.
A. Arasu, S. Chaudhuri, and R. Kaushik, “Learning string transformations from examples,” PVLDB, vol. 2, no. 1, pp. 514–525, 2009.
S. Kandel, A. Paepcke, J. Hellerstein, and J. Heer, “Wrangler: Interactive visual specification of data transformation scripts,” in CHI, 2011.
B. Alexe, B. ten Cate, P. G. Kolaitis, and W. C. Tan, “Designing and
refining schema mappings via data examples,” in SIGMOD, 2011.
S. Agrawal, S. Chaudhuri, and G. Das, “Dbxplorer: A system for
keyword-based search over relational databases,” in ICDE, 2002.
B. Aditya, G. Bhalotia, S. Chakrabarti, A. Hulgeri, C. Nakhe, P. Parag,
and S. Sudarshan, “Banks: Browsing and keyword searching in relational databases,” in VLDB, 2002, pp. 1083–1086.
V. Hristidis and Y. Papakonstantinou, “Discover: Keyword search in
relational databases,” in VLDB, 2002, pp. 670–681.
L. Qian, M. J. Cafarella, and H. V. Jagadish, “Sample-driven schema
mapping,” in SIGMOD, 2012.
Y. Shen, K. Chakrabarti, S. Chaudhuri, B. Ding, and L. Novik, “Discovering queries based on example tuples,” in SIGMOD, 2014, pp. 493–
504.
P. A. Bernstein, J. Madhavan, and E. Rahm, “Generic schema matching,
ten years later,” PVLDB, vol. 4, no. 11, pp. 695–701, 2011.
A. Das Sarma, X. Dong, and A. Halevy, “Bootstrapping pay-as-you-go
data integration systems,” in SIGMOD, 2008, pp. 861–874.
L. Barbosa and J. Freire, “An adaptive crawler for locating hidden-web
entry points,” in WWW, 2007, pp. 441–450.
A. Balmin, V. Hristidis, and Y. Papakonstantinou, “Objectrank:
Authority-based keyword search in databases,” in VLDB, 2004.
S. Bergamaschi, E. Domnori, F. Guerra, R. Trillo Lado, and Y. Velegrakis, “Keyword search over relational databases: a metadata approach,” in SIGMOD, 2011, pp. 565–576.
J. Feng, G. Li, and J. Wang, “Finding top-k answers in keyword search
over relational databases using tuple units,” TKDE, vol. 23, no. 12, pp.
1781–1794, 2011.
A. P. Dawid and A. M. Skene, “Maximum likelihood estimation of
observer error-rates using the em algorithm,” Applied statistics, pp. 20–
28, 1979.
F. Murtagh, “Clustering in massive data sets,” in Handbook of massive
data sets. Springer, 2002, pp. 501–543.
Z. Abedjan, J. Lorey, and F. Naumann, “Reconciling ontologies and the
web of data,” in CIKM, 2012, pp. 1532–1536.
F. M. Suchanek, G. Kasneci, and G. Weikum, “Yago: A core of semantic
knowledge,” in WWW, 2007, pp. 697–706.
C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak,
and S. Hellmann, “Dbpedia - a crystallization point for the web of data,”
Web Semantics, vol. 7, no. 3, pp. 154–165, Sep. 2009.
J. Eberius, M. Thiele, K. Braunschweig, and W. Lehner, “Top-k entity
augmentation using consistent set covering,” in SSDBM, 2015.
B. Zhao, B. I. P. Rubinstein, J. Gemmell, and J. Han, “A bayesian
approach to discovering truth from conflicting sources for data integration,” PVLDB, vol. 5, no. 6, pp. 550–561, 2012.
R. Singh and S. Gulwani, “Learning semantic string transformations
from examples,” PVLDB, vol. 5, no. 8, pp. 740–751, 2012.
M. J. Cafarella, A. Halevy, and N. Khoussainova, “Data integration for
the relational web,” PVLDB, vol. 2, no. 1, pp. 1090–1101, Aug. 2009.
A. Das Sarma, L. Fang, N. Gupta, A. Halevy, H. Lee, F. Wu, R. Xin,
and C. Yu, “Finding related tables,” in SIGMOD, 2012, pp. 817–828.
M. Yakout, K. Ganjam, K. Chakrabarti, and S. Chaudhuri, “Infogather:
Entity augmentation and attribute discovery by holistic matching with
web tables,” in SIGMOD, 2012, pp. 97–108.
R. Pimplikar and S. Sarawagi, “Answering table queries on the web
using column keywords,” PVLDB, vol. 5, no. 10, pp. 908–919, 2012.
B. Trushkowsky, T. Kraska, M. J. Franklin, and P. Sarkar, “Crowdsourced enumeration queries,” in ICDE, 2013, pp. 673–684.

Generating Concise Entity Matching Rules
Rohit Singh:
Vamsi Meduri;
Ahmed Elmagarmid‹
Samuel Madden:
;
‹
Paolo Papotti Jorge-Arnulfo Quiané-Ruiz Armando Solar-Lezama: Nan Tang‹
‹

:
CSAIL, MIT, USA ; Arizona State University, USA
Qatar Computing Research Institute, HBKU, Doha, Qatar

{rohitsingh, madden, asolar}@csail.mit.edu, {vmeduri, ppapotti}@asu.edu
{aelmagarmid, jquianeruiz, ntang}@hbku.edu.qa
ABSTRACT

tems that use probabilistic models – such as machine learning methods based on SVMs [2], or fuzzy matching [4] – are
much harder to interpret and hence are often not preferred
in applications that handle critical data such as healthcare.
In contrast, systems that are rule-based (“deterministic”) [3]
o↵er better interpretability, particularly when the rules can
be constrained to be simple (i.e., consist of relatively few
clauses). A key question, however, is whether such simple
rules can match the e↵ectiveness of probabilistic approaches
while preserving interpretability.
Although hand-writing EM rules may be practical in some
limited domains, doing so is extremely time consuming and
error-prone. Hence, a promising direction is to use automatic methods to generate deterministic EM rules, e.g., by
learning rules from training examples. This learning should
be done with as few examples as possible, because generating the training examples is itself laborious too.
We have developed a system that can e↵ectively learn
EM rules that (i) match the performance of probabilistic
methods, (ii) produce concise and interpretable rules, and
(iii) learn rules from limited training examples. Our approach is to use program synthesis [5] (PS), in which a program (set of rules) is generated by using positive and negative examples as constraints that guide the synthesizer towards rules that match the examples. The demo will demonstrate the following three key features: (1) Interpretability,
(2) Easy customization, and (3) High performance.

Entity matching (EM) is a critical part of data integration
and cleaning. In many applications, the users need to understand why two entities are considered a match, which
reveals the need for interpretable and concise EM rules. We
model EM rules in the form of General Boolean Formulas
(GBFs) that allows
ö arbitrary attribute
ô matching combined
by conjunctions ( ), disjunctions ( ), and negations ( ).
GBFs can generate more concise rules than traditional EM
rules represented in disjunctive normal forms (DNFs). We
use program synthesis, a powerful tool to automatically generate rules (or programs) that provably satisfy a high-level
specification, to automatically synthesize EM rules in GBF
format, given only positive and negative matching examples.
In this demo, attendees will experience the following features: (1) Interpretability – they can see and measure the
conciseness of EM rules defined using GBFs; (2) Easy customization – they can provide custom experiment parameters for various datasets, and, easily modify a rich predefined
(default) synthesis grammar, using a Web interface; and (3)
High performance – they will be able to compare the generated concise rules, in terms of accuracy, with probabilistic
models (e.g., machine learning methods), and hand-written
EM rules provided by experts. Moreover, this system will
serve as a general platform for evaluating di↵erent methods
that discover EM rules, which will be released as an opensource tool on GitHub.

1.

2.

INTRODUCTION

RULES AND ALGORITHMS

Let RrA1 , A2 , . . . , An s and SrA11 , A12 , . . . , A1n s be two relations with corresponding sets of n aligned attributes Ai and
A1i (i P r1, ns). As in previous work [6], we assume that the
attributes between two relations have been aligned and provided as an input; this can either be manually specified by
the users, or done automatically using o↵-the-shelf schema
matching tools [1]. Let r, s be records in R, S and rrAi s, srA1i s
be the values of attributes Ai , A1i in records r, s, respectively.
A similarity function f prrAi s, srA1i sq computes a similarity
score in the real interval r0, 1s. A bigger score means that
rrAi s and srA1i s have a higher similarity. Examples of similarity functions are cosine similarity, edit distance, and Jaccard
similarity. A library of similarity functions F is a set of such
general purpose similarity functions, such as the Simmetrics
(https://github.com/Simmetrics/simmetrics) Java package.

Entity matching (EM), where a system or user finds
records that refer to the same real-world object, is a fundamental part of data integration and data cleaning.
There is a key tension in EM algorithms: On one hand,
algorithms that properly match records, i.e., high accuracy,
are clearly preferred. On the other hand, algorithms need
to be interpretable – that is, the user of the system needs to
understand why two entities are considered a match. SysPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

Attribute-Matching Rules. An attribute-matching rule
is a Boolean predicate representing f prrAi s, srA1i sq• ✓,
where i P r1, ns is an index, f is a similarity function

SIGMOD ’17, May 14–19, 2017, Chicago, IL, USA.
c 2017 ACM. ISBN 978-1-4503-4197-4/17/05. . . $15.00
DOI: http://dx.doi.org/10.1145/3035918.3058739

1635

DC
DAG
DLF

#-Postive #-Negative #-Attributes
Examples
Examples
14, 280
170, 379
9
1, 300
95, 707
4
6, 048
335, 196
10
DC = Cora, DLF = Locu-FourSquare
DAG = Amazon-GoogleProducts

Table 1: Dataset Satistics
and ✓ P r0, 1s is a threshold value. Attribute-matching
rule f prrAi s, srA1i sq• ✓ evaluating to true means that rrAi s
matches srA1i s relative to f and ✓.

Notation. Let frAi s • ✓ be an attribute-matching rule
f prrAi s, srA1i sq• ✓ since Ai and A1i are known to have been
aligned. For example, Soundexrtitles • 0.937 is an attributematching rule that applies Soundex similarity function on the
title attribute from R and the name attribute from S (omitted in the notation), and, compares it with the threshold
0.937. Note that for brevity we refer to attribute-matching
rules as atoms of a larger Matching Rule.

Figure 1: Schema Alignment
Moreover, the specification should also include a set of positive (and negative) matching examples as a file. Table 2
shows one positive matching example for the Cora dataset.
The attendees can select one of the pre-specified datasets
(Table 1) and generate a concise EM rule for that dataset:

Boolean Formula Matching Rule. A Boolean formula matching rule is an arbitrary Boolean formula with
attribute-matching
rules as ö
its variables (or atoms) and conô
junction ( ), disjunction ( ) and negation ( ) as allowed
operations. Some Boolean Formula Matching Rules are:

'synth :

ô
'1 : ô
pLevensteinrtitles • 0.8
Equalrvolumes • 1.0
Equalrpagess • 1.0) ô
'2 : ( Levensteinrtitles
• 0.5 Equalrauthors • 1.0)
ö
'2 )
'3 : ( ' 1
'4 : if (noNullsrauthors • 1.0) then '2 else '1

ChapmanMatchingSoundexrauthors • 0.937
ô
if
noNullsrdates • 1
then CosineGram2 rdates • 0.681
˘ö
else
NeedlemanWunchrtitles • 0.733
`
EditDistancertitles
• 0.73
˘
ô
OverlapTokenrvenues • 0.268

This EM rule specifies under which conditions two records
are considered as a match. Our tool can automatically figure
out (1) what similarity function and associated threshold
to use for each selected attribute; and more interestingly,
(2) under what
ö logic they should
ô be composed, e.g., with
conjunctions ( ), disjunctions ( ), negations ( ), and ifthen-else operations.
Below, we focus on the default parameters use for the
demo, some evaluation comparison with the prior state-ofthe-art, and our Web interface for users to tune parameters.

Note that “if puq then pvq else pwq” can be
with
ô expressed
ö
ô
the help of the negation operator ( ) as pu vq p u wq
but the “if then else” representation is more interpretable
and concise. This is where we use the power of program
synthesis to find a General Boolean Formula (GBF) that
represents a Boolean Formula matching rule.
Synthesis Algorithm. Our novel synthesis algorithm
searches for GBFs from a rich interpretable grammar (Figure 4) using a small set of matching and non-matching examples as constraints while smartly expanding the set to
include important corner cases. This process is repeated
multiple times to account for noise in the provided data and
the best GBF is chosen across all generated GBFs by maximizing an optimization metric (e.g. F-measure).

3.

`

3.1

Default Configuration

Experiment Setup. The system optimizes F-measure on
the training data while performing 5-fold cross-validation
and compares the results with other interpretable (Decision
Trees of depth 3 or 4, SIFI [6]) and non-interpretable (Deep
decision trees, SVM) techniques.
Algorithm Parameters. The system uses a generic set
of 30 string similarity functions including equality, checking
for null values, and 28 others from the Simmetrics package.
The internal parameters (cuto↵s and heuristics) for synthesis
have been empirically selected to balance running time with
accuracy of the produced rules.

DEMONSTRATION OVERVIEW

In this demonstration, we will show how easy it is for the
users to obtain concise EM rules with our system without
tuning parameters (Default Configuration). We will show
the performance of our system compared with other state-ofthe-art techniques (Evaluation). In addition, we allow users
to customize the system to suit their needs (Customization).
We will focus on the EM application and show the underlying PS problems o↵-line if there is interest.

Synthesis Grammar. The system uses
ôa rich interpretable
ö
grammar for GBFs with conjunction ( ), disjunction ( ),
if-then-elses, and negation ( ) as allowed operations. The
grammar is predefined – the user needs neither to provide
the grammar, nor to be knowledgeable in program synthesis.

Datasets. Table 1 shows three real-world datasets to be
used in this demonstration. We prune the Cartesian product of records (comprising of up to 400 million pairs) from
positive examples to construct negative examples.

3.2

Evaluation

Interpretability. Besides our results, the attendees can see
the performance comparison among the state-of-the-art solutions. For every experiment, the results will be displayed
in a web-based interface as shown in Figure 2. Generated

Dataset Specification. The schemas of two relations in
the datasets are aligned either by o↵-the-shelf tools, or by
the user using our Web interface, as shown in Figure 1.

1636

rPR
sPS

author
brodley, c.e., and
utgo↵, p.e.
carla brodley and
paul utgo↵.

title
multivariate
decision trees
multivariate
trees.

venue

address

publisher

editor

date

volume

pages

machine learning

null

null

null

1995

19(1)

45-77

machine learning

null

null

null

1995

19

null

Table 2: A Positive Matching Example from Cora Dataset

Figure 2: Results Interface
As shown above, the rule 'tree based on decision tree
is much more complicated than the rule 'synth from our
technique, as shown earlier in this section.
High Performance. Figure 2 also shows the performance
of our algorithm and its comparison with other methods
when optimizing F-measure. We will show that our tool
provides comparable F-measures as other methods. The
results obtained for our datasets can be seen in Figure 3.
RuleSynth, RS-BestTh, and RS-SynthComp are three
variants of our algorithm - RS-SynthComp will be used in
the default configuration.

Figure 3: F-measure comparison

Debugging. Users can download the experiment logs and
the records that were misclassified (by clicking on False positives / False negatives in Figure 2) by our algorithm and
re-run the experiment based on the insights gained from
them with more customizations (as discussed next).

rules will be compared against those generated by other
techniques to demonstrate the conciseness of our rules. For
example, we show the rule generated with a decision tree
('tree ) for the Cora dataset (Tables 1 & 2) below:
'tree :

`

OverlapGram
3 rtitles • 0.484
ô
ô MongeElkanrvolumes •˘ 0.429
ö
Soundexrtitles • 0.939
`
OverlapGram
2 rpagess • 0.626
ô
• 0.429
`
˘˘ ö
ô MongeElkanrvolumes
Soundexrtitles • 0.939
`
ChapmanMeanLengthrtitles
• 0.978 ˘
`
ô
`OverlapGram3 rauthors • 0.411
˘˘ ö
ô
MongeElkanrvolumes • 0.429
`
CosineGram
2 rtitles • 0.730
ô
3 rauthors • 0.411 ˘˘
`
ô OverlapGram
MongeElkanrvolumes • 0.429

3.3

Customization

The attendees have the opportunities to customize the
demo scenarios by modifying the dataset to be used, the
grammar feeding the PS engine, and other parameters.
Datasets. The interface in Figure 5 can be used to manipulate existing datasets by (1) introducing errors; (2) reducing the number of positive and/or negative examples; and
(3) adding null values in the dataset. These changes make

1637

Figure 4: Modifying EM Rule Grammars
Algorithm Parameters. The set of similarity functions
can be augmented by adding or replacing custom functions
provided as native Python code. The algorithm cuto↵s and
heuristics can be chosen from a specified range of options.
Synthesis Grammar. A rich set of predefined grammar
rules and bounds/constraints on top of them can be modified, e.g., one can limit any operator (^, _, , if) to occur
only at the topmost level, or limit their number of occurrences as they deem fit (Figure 4). This allows experts to
see the e↵ects of changing the grammar. For example, richer
grammars can express more rules but may result in slower
convergence in the synthesis algorithm.

4.

REFERENCES

[1] P. A. Bernstein, J. Madhavan, and E. Rahm. Generic
schema matching, ten years later. PVLDB,
4(11):695–701, 2011.
[2] M. Bilenko and R. J. Mooney. Adaptive duplicate
detection using learnable string similarity measures. In
SIGKDD, pages 39–48, 2003.
[3] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios.
Duplicate record detection: A survey. IEEE Trans.
Knowl. Data Eng., 19(1):1–16, 2007.
[4] I. Fellegi and A. Sunter. A theory for record linkage.
Journal of the American Statistical Association, 64
(328), 1969.
[5] A. Solar-Lezama. The sketching approach to program
synthesis. In APLAS, pages 4–13, 2009.
[6] J. Wang, G. Li, J. X. Yu, and J. Feng. Entity matching:
How similar is similar. PVLDB, 4(10), 2011.

Figure 5: Dataset Customization
the EM scenario much harder and enable the attendees to
see strengths and limitations of the di↵erent approaches.
Optimization Metric. The optimization metric can be
set to one of F-measure, Precision, Recall, Accuracy, or can
be provided by a user of the tool as native Python code,
such as a weighting function that increases the importance
of subsets of data to match correctly that are important
for the final application, e.g., certain subsets pertaining to
specific authors in the Cora dataset may need to be matched
at a higher accuracy than the others.

1638

Messing Up with Bart:
Error Generation for Evaluating Data-Cleaning Algorithms
∗

Patricia C. Arocena

University of Toronto, Canada

Boris Glavic

Giansalvatore Mecca

Illinois Inst. of Technology, US

University of Basilicata, Italy

Renée J. Miller ∗

Paolo Papotti

Donatello Santoro

University of Toronto, Canada

QCRI Doha, Qatar

University of Basilicata, Italy

ABSTRACT

language of denial constraints [21]. Denial constraints (DC)
are a rule-language that can express many data quality rules
including functional dependencies (FD), conditional functional dependencies (CFD) [7], cleaning equality-generating
dependencies [16], and fixing rules [24]. In addition, Bart
permits a user to declare parts of a database as immutable,
and hence we can express editing rules [14] that use master
data to determine a repair.
Bart provides the highest possible level of control over
the error-generation process, allowing users to choose, for
example, the percentage of errors, whether they want a guarantee that errors are detectable using the given constraints,
and even provides an estimate of how hard it will be to restore the database to its original clean state (a property we
call repairability). This control is the main innovation of the
generator and distinguishes it from previous error generators
that control mainly for data size and amount of error. Bart
permits innovative evaluations of cleaning algorithms that
reveal new insights on their (relative) performance when executed over errors with differing degrees of repairability.
Solving this problem proves surprisingly challenging, for
two main reasons. First, we introduce two different variants of the error-generation problem, and show that they
are both NP-complete. To provide a scalable solution, we
concentrate on a polynomial-time algorithm that is correct,
but not complete. Even so, achieving the desired level of
scalability remains challenging. In fact, we show that there
is a duality between the problem of injecting detectable errors in clean data, and the problem of detecting violations
to database constraints in dirty data, a task that is notoriously expensive from the computational viewpoint. Finding
the right trade-off between control over errors and scalability
is the main technical problem tackled in this paper.

We study the problem of introducing errors into clean databases for the purpose of benchmarking data-cleaning algorithms. Our goal is to provide users with the highest possible
level of control over the error-generation process, and at the
same time develop solutions that scale to large databases.
We show in the paper that the error-generation problem is
surprisingly challenging, and in fact, NP-complete. To provide a scalable solution, we develop a correct and efficient
greedy algorithm that sacrifices completeness, but succeeds
under very reasonable assumptions. To scale to millions of
tuples, the algorithm relies on several non-trivial optimizations, including a new symmetry property of data quality
constraints. The trade-off between control and scalability is
the main technical contribution of the paper.

1.

INTRODUCTION

We consider the problem of empirically evaluating datacleaning algorithms. Currently, there are no openly-available
tools for systematically generating data exhibiting different
types and degrees of quality errors. This is in contrast to related fields, for example, entity resolution, where well-known
datasets and generators of duplicated data exist and can be
used to assess the performance of algorithms.
We allow the user to control of the data distribution by
providing a clean database (DB) into which our error generator, Bart1 , introduces errors. Bart supports different
kinds of random errors (including typos, duplicated values,
outliers and missing/bogus values). Our main technical innovation, however, is related to the problem of generating
errors in the presence of data quality rules. Bart permits
a user to specify a set of data quality rules in the powerful

Example 1: [Motivating Example] Consider a database
schema composed of two relations Emp and MD and the data
shown in Figure 1. Suppose we are given a single data quality rule (a functional dependency) d1 : Emp : Name → Dept.
We can introduce errors into this database in many ways.
An example error is to change the Salary value of tuple t4
to “3000”. This change (ch0 ) creates an error that is not
detectable using the given data quality rule d1 . On the
contrary, a second change (ch1 ) that changes the Name value
of tuple t2 to “John” introduces a detectable error since this
change causes tuples t1 and t2 to agree on employee names,
but not on department values. Of course, if other errors
are introduced into the DB (for example, t1 .Dept is changed

∗Supported in part by NSERC BIN.
1
Bart: Benchmarking Algorithms for data Repairing and
Translation

This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivatives 4.0 International License. To view a copy
of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For
any use beyond those covered by this license, obtain permission by emailing
info@vldb.org.
Proceedings of the VLDB Endowment, Vol. 9, No. 2
Copyright 2015 VLDB Endowment 2150-8097/15/10.

36

Emp
:
:
:
:

Name
John
Paul
Jim
Frank

Dept
Staff
Sales
Staff
Mktg

Salary
1000
1300
1000
1500

tm1 :
tm2 :

Name
John
Frank

Dept
Staff
Mktg

Mng
Mark
Jack

t1
t2
t3
t4
MD

Mng
Mark
Frank
Carl
Jack

have significantly better performance than the ones based on
joins. Finally, we discuss the benefits of these optimizations
when reasoning about error detectability and repairability.
(iv) We present a comprehensive empirical evaluation of our
error generator to test its scalability. Our experiments show
that the error-generation engine takes less than a few minutes to complete tasks that require the execution of dozens
of queries, even on DBs of millions of tuples. In addition, we
discuss the relative influence of the various activities related
to error generation, namely identifying changes, applying
changes to the DB, checking detectability and repairability.
(v) The generator provides an important service for datacleaning researchers and practitioners, enabling them to more
easily do robust evaluations of algorithms, and compare solutions on a level playing field. To demonstrate this, we
present an empirical comparison of several data-repairing
algorithms over Bart data. We show novel insights into
these algorithms, their features and relative performance
that could not have been shown with existing generators.
Our error generator is open source and publicly available
(db.unibas.it/projects/bart). We believe the system will
raise the bar for evaluation standards in data cleaning.
The problem of error generation has points of contact with
other topics in database research, such as the view-update
problem [2], the problems of missing answers [19] and whynot provenance [18], and database testing [4]. However, our
new algorithms and optimizations - specifically designed for
the error-generation problem - are what allows us to scale to
large error-generation tasks. These techniques have not previously been identified and will certainly have applicability
to any of the related areas.
Organization of the Paper. We introduce notation and
definitions in Sections 2 and 3. Section 4 formalizes the
error-generation problem. Section 5 presents the violationgeneration algorithm, followed by a number of important
optimizations in Section 6. Use cases of the system are presented in Section 7. We empirically evaluate our techniques
in Sections 8 and 9, and discuss related work in Section 10.
We conclude in Section 11.

Figure 1: Example Clean Database
to “Sales”), then ch1 may no longer be detectable. Bart
permits the user to control not only the number of errors,
but also how many errors are detectable and whether they
are detectable by a single rule or many rules.
We not only strive to introduce detectable errors into I ,
we also want to estimate how hard it would be to restore
I to its original, clean state. Consider the detectable error
introduced by ch1 . This change removed the value Paul
from the DB. Most repair algorithms for categorical values
will use evidence in the DB to suggest repairs. If Paul is not
in the active domain, then changing t2 .N ame to Paul may
not be considered as a repair. Bart lets the user control
the repairability of errors by estimating how hard it would
be to repair an error to its original value.
2
Error-Generation Tasks. We formalize the problem of
error generation using the notion of an error-generation task,
E, that is composed of four key elements: (i) a database
schema S, (ii) a set Σ of denial constraints (DCs) encoding
data quality rules over S, (iii) a DB I of S that is clean with
respect to Σ, and (iv) a set of configuration parameters Conf
to control the error-generation process. These parameters
specify, among other things, which relations are immutable,
how many errors should be introduced, and how many of
these errors should be detectable. They also let the user
control the degree of repairability of the errors.
We concentrate on a specific update model, one in which
the database is only changed by updating attribute values,
rather than through insertions or deletions. This update
model covers the vast majority of algorithms that have been
proposed in the recent literature [3, 5, 6, 8, 14, 16, 20, 24,
25]. And importantly, it suggests a simple, flexible and scalable measure to assess the quality of repairs, a fundamental
goal of our work, as we shall discuss in Section 7.
Contributions. Our main contributions are the following.
(i) We present the first framework for generating random
and data-cleaning errors with fine-grained control over error
characteristics. We allow users to inject a fixed number of
detectable errors into a clean DB and to control repairability.
(ii) We introduce a new computational framework based
on violation-generation queries for finding candidate cells
(tuple, attribute pairs) into which detectable errors can be
introduced. We study when these queries can be answered
efficiently, and show that determining if detectable errors
can be introduced is computationally hard.
(iii) We introduce several novel optimizations for violationgeneration queries. We show that extracting tuple samples, along with computing cross-products and joins in main
memory, brings considerable benefits in terms of scalability. We also identify a fragment of DCs called symmetric
constraints that considerably extend previous fragments for
which scalable detection techniques have been studied. We
develop new algorithms for detecting and generating errors
with symmetric constraints, and show that these algorithms

2.

CONSTRAINTS AND VIOLATIONS

We assume the standard definition of a relational database
schema S, instance I , and tuple t. In addition, we assume
the presence of unique tuple identifiers in an instance. That
is, tid denotes the tuple with identifier (id) “id ” in I. A cell
in I is a tuple element specified by a tuple id and attribute
pair htid , Ai i. The value of a cell htid , Ai i in I is the value of
attribute Ai in tuple tid . As an alternative notation we use
tid .Ai for a cell htid , Ai i. A relational atom over a schema S
is a formula of the form R(x̄) where R ∈ S and x̄ is a tuple
of (not necessarily distinct) variables. A comparison atom
is a formula of the form v1 op v2 , where v1 is a variable and
v2 is either a variable or a constant, and op is one of the
following comparison operators: =, 6=, >, <, ≤, ≥.
We use the language of denial constraints (DCs) [21] to
specify cleaning rules. We introduce a normal form for DCs:
Definition 1: Denial Constraint – A denial constraint (DC)
in normal form over schema S is a formulamof the form
^
∀ x¯1 , . . . , x¯n : ¬(R1 (x¯1 ), . . . , Rn (x̄n ), (v1i op v2i ))
i=1

such that (i) R1 (x̄1 ), . . . , Rn (x̄n ) are relational atoms, where
variables are not reused within or between atoms, and (ii)

37

Vm

3.

i
i=1 (v1

op v2i ) is a conjunction of comparison atoms, each
of the form xk op xl or xk op c, where c ∈ Consts.
2

PROPERTIES OF ERRORS

Our primary concern is to provide users with fine-grained
control over the error-generation process. To do so, we concentrate on two important properties of the changes we make
to the clean instance, namely detectability and repairability.

Example 2: Continuing with Example 1, we introduce the
following sample declarative constraints.
(i) We already discussed FD d1 : Emp : Name → Dept.
(ii) The FD Name → Mng holds over table Emp, but only for
those employees in “Sales”. We specify this as a conditional
FD (CFD) d2 : Emp : Dept[“Sales”], Name → Mng.
(iii) A standardization rule over table Emp states that all
employees working in department “Staff” must have a salary
of one-thousand dollars. We model this using a (singletuple) CFD d3 : Emp : Dept[“Staff”] → Sal[“1000”].
(iv) An editing rule [14] d4 prescribes how to repair inconsistencies over Emp based on master-data from MD: “when
Emp and MD agree on Name and Dept, change Emp.Mng to
the value taken from MD.Mng”. This rule requires that MD
can not be changed (the MD table is immutable).
(v) Finally, we consider an ordering constraint d5 stating
that any manager should have a salary that is not lower
than the salaries of his or her employees.
Next we encode our sample constraints d1 –d5 as a set of
DCs (universal quantifiers are omitted).
dc1 : ¬(Emp(n, d, s, m), Emp(n’, d’, s’, m’), n = n’, d 6= d’)
dc2 : ¬(Emp(n, d, s, m), Emp(n’, d’, s’, m’), n = n’, d = d’,
d = “Sales”, m 6= m’)
dc3 : ¬(Emp(n, d, s, m), d = “Staff”, s 6= “1000”)
dc4 : ¬(Emp(n, d, s, m), MD(n’, d’, m’), n = n’, d = d’, m 6= m’)
2
dc5 : ¬(Emp(n, d, s, m), Emp(n’, d’, s’, m’), m = n’, s’ < s)

Detectability. We define precisely when a cell change ch
introduces an error that is detectable using Σ. We say that
cell ti .A is involved in a violation with dc in instance I if
there exists an assignment m such that I violates dc according to m, and ti .A ∈ vio-contextdc (m).
Definition 4: Detectable Errors – Consider a set of cell
changes Ch to instance I that yield a new instance Id =
Ch(I ). We say that a cell change ch = hti .A := vi in Ch
introduces a detectable error for constraint dc if: (i) cell
ti .A is involved in a violation with dc in instance Id and (ii)
cell ti .A was not involved in a violation with dc in instance
I 0 = Ch0 (I), where Ch0 = Ch − {ch}.
2
The cell-change ch1 = ht2 .Name := “John”i from Example 1 introduces a detectable error for dc1 . Notice that
it does not introduce any violation to dc2 -dc5 . Hence, we
say that ch1 is detectable by exactly-one constraint in Σ =
{dc1 , ...dc5 }. This is not always the case. For example, the
change ch2 = ht4 .Mgr = “John”i is detectable using both dc5
(managers must have a greater salaries than their employees), and dc4 (employees present in the master-data table,
must have the manager indicated by the master data). We
say that ch2 is at-least-one detectable, or simply detectable.
Reasoning about detectability is important. There are
repair algorithms [8, 20] that exploit simultaneous violations
to multiple constraints to select among repairs. Given a
set of constraints Σ, we want our error-generation method
to control the detectability of errors, and whether they are
exactly-one or at-least-one detectable.

To formalize the semantics of a DC, we introduce the notion of an assignment to the variables of a logical formula
ϕ(x̄). DCs in normal form have the nice property that each
variable has a unique occurrence within a relational atom.
We can therefore define an assignment as a mapping m of
the variables x̄ to the cells of I . We formalize the notion of
a violation for a constraint using assignments.

Repairability. The notion of repairability provides an estimate of how hard it is to restore a DB with errors to its
original, clean state. It is clear that such an estimate depends on the actual algorithm used to repair the data. To
propose a practical notion, we concentrate on a specific class
of repair algorithms. First, we restrict our attention to repair algorithms that use value modification, i.e., they fix
violations by modifying one or more values in the dirty DB.
Second, we assume that these algorithms rely on a minimality principle, according to which repairs that minimally
modify the given DB are to be preferred. We notice that
the vast majority of algorithms in the recent literature [3, 5,
6, 8, 14, 16, 20, 21, 24] fall in this category.
Consider again Example 1, and assume instance Id only
has the following tuples t10 -t14 :

Definition 2: Violation – Given a constraint dc : ∀ x̄ :
¬(φ(x̄)) over schema S, and an instance I of S. We say that
dc is violated in I if there exists an assignment m such that
I |= φ(m(x̄)). For a set Σ of DCs, I |= Σ if none of the
constraints in Σ is violated in I .
2
Consider constraint dc1 in Example 2. Assume two tuples are present in the instance of Emp: t1 = Emp(John,
Staff, 1000, Mark), t6 = Emp(John, Sales, 1000, Mark). These tuples are a violation of dc1 according to assignment m that
maps variables to cells and atoms to tuples as follows (we
omit s, s’, m, m’):
m(n) = t1 .Name m(d) = t1 .Dept m(Emp(n, d, s, m)) = t1
m(n’) = t6 .Name m(d’) = t6 .Dept m(Emp(n’, d’, s’, m’)) = t6
To determine if a constraint is violated, we must examine
values assigned to variables used in comparison atoms. To
emphasize this, we call the context variables of a constraint
dc in normal form those variables that appear in comparison atoms (in our example, {n, n’, d, d’}). A context for a
violation is the set of cells associated with context variables
(in our example, {t1 .Name, t1 .Dept, t6 .Name, t6 .Dept}).

t10 : Emp(John, Staff, . . .) t11 : Emp(John, Sales, . . .)
t12 : Emp(John, Sales, . . .) t13 : Emp(John, Mktg, . . .)
t14 : Emp(John, Mktg, . . .)
there are eight violation contexts for constraint dc1 , each
one using two tuples with equal name and different department. However, an algorithm that is inspired by the principle of minimal repairs would be primarily interested in
knowing that these 8 contexts reduce to a group of 10 cells
(the name and dept cells of tuples 10-14). Variants of this
notion, initially called equivalence class [6], have been formalized under the names of homomorphism class [16] and
repair context [8]. We call this a context equivalence class.

Definition 3: Violation Context – Given a constraint dc :
∀ x̄ : ¬(φ(x̄)) over schema S in normal form, and an instance
I of S, assume I contains a violation of dc according to
assignment m. The violation context for dc and m, denoted
by vio-contextdc (m), is the set of cells that are the image
according to m of the context variables of dc.
2

38

Definition 5: Context Equivalence Class – Given a constraint dc over schema S in normal form, and an instance I
of S, we say that two cells of I share a context if they share
a violation context for dc and I . A context equivalence class
E for dc and I is an equivalence class of cells induced by the
transitive closure of the relation “share a context”.
2

denote the context variables (i.e., variables that appear in a
comparison atom) of dc by x̄c , and the rest by x̄nc .
Definition 7: Vio-Detection Query – The violation-detection
(vio-detection) query for dc is a conjunctive query with free
¯ x̄c of the form: DQdc (id,
¯ x̄c ) = φ(id,
¯ x̄c , x̄nc ). 2
variables id,
Consider our example dc1 , its vio-detection query is the
following (notice how we add the predicate id < id’ to avoid
returning every pair of tuples twice):
DQdc1 (id, id’, n, n’, d, d’) = Emp(id, n, d, s, m),
Emp(id’, n’, d’, s’, m’), n = n’, d 6= d’, id < id’

It is natural to reason about the repairability of a violation based on its context equivalence class. To formalize the notion of repairability, we start from a cell change
ch = ht.A := vd i introducing a detectable error to dc. We
assume we have identified the context equivalence class E
for cell t.A and dc. From this, we derive a bag of candidate values, denoted by candidate-values(t.A, E, dc). Then,
we define the repairability as the probability of restoring t.A
to its original value by uniformly at random picking a value
from candidate-values(t.A, E, dc):

Injecting detectable errors into a clean DB requires the
identification of cells that can be changed in order to trigger
violations. To find cells with this property, we notice that
each comparison in the normal form of a DC suggests a
different strategy to generate errors.
Consider dc1 : ¬(Emp(n, d, s, m), Emp(n’, d’, s’, m’), n = n’, d 6=
d’) stating that there cannot exist two employees with equal
names and different departments. Given a clean DB, there
are two ways to change it to violate this constraint.
(i) Enforce the inequality: we may find tuples with equal
names, and equal departments (n = n’, d = d’), and change
one of the departments in such a way that they become
different (d becomes different from d’).
(ii) Enforce the equality: we may find tuples with different
names, and different departments (n 6= n’, d 6= d’) and change
one of the names in such a way that n becomes equal to n’.
Based on this intuition, we introduce the notion of a
violation-generation query (vio-gen query in the following).
These queries are obtained from a violation-detection query
for a DC by negating one of the comparisons.

Definition 6: Repairable Error and Repairability – Given a
cell change ch = ht.A := vd i that changes the cell t.A in I
from a value vc to vd and assume t.A belongs to a violation
context for constraint dc, call E the context equivalence class
of cell t.A and dc. Let V = candidate-values(t.A, E, dc) be
the bag of candidate repair values from E for t.A and dc.
We say ch is a repairable error if vc ∈ V. The repairability
of ch is computed by dividing the number of occurrences of
vc in V by the size of V.
2
The definition of function candidate-values(t.A, E, dc) is
elaborate but quite standard in data repairing. For the sake
of readability, we introduce it by means of examples.
(i) Consider our sample equivalence class for tuples t10 -t14 .
For FD dc1 in Example 2 and change t11 .Dept from “Sales”
to “Staff”, we select all values from cells of the Dept attribute
in the equivalence class: {Staff, Staff, Sales, Mktg, Mktg}. The
error is repairable, and the repairability is 1/5 = 0.2.
(ii) For a CFD, like dc3 in Example 2 (employees of the
staff department need to have salaries of $1000), and change
t1 .Salary from 1000 to 2000, the only candidate value is dictated by the constant, and is exactly 1000. Therefore, the
repairability is 1. This is similar to fixing rules. Editing
rules like dc4 in Example 2 use master-data tuples, i.e., immutable cells, and are treated accordingly: candidate values
are taken from the master-data cells only.
(iii) Finally, consider ordering constraints like dc5 in Example 2 (managers have higher salaries than their employees).
Assume we change Paul’s salary to 2000 in t2 to make it
higher than his manager’s salary. It is easy to see that there
are infinite real values that can be used to repair the violation. In this case, the repairability is 0.
Notice that a change may be detectable by several constraints, and thus have different repairability values. In this
case, we consider the maximum of these values.

4.

Definition 8: Vio-Gen Queries – Given a constraint dc in
normal form, consider its vio-detection query, DQdc . A viogen query GQdc,i for dc is obtained from DQdc by changing
a comparison atom of the form v1i op v2i into its negation,
¬(v1i op v2i ). The latter is called a target comparison.
2
In our example, we have two vio-gen queries, as follows
(target comparisons are enclosed in brackets):
GQdc1 ,1 (id, id’, n, n’, d, d’) = Emp(id, n, d, s, m),
Emp(id’, n’, d’, s’, m’), n = n’, (d = d’), id < id’
GQdc1 ,2 (id, id’, n, n’, d, d’) = Emp(id, n, d, s, m),
Emp(id’, n’, d’, s’, m’), (n 6= n’), d 6= d’, id < id’
Definition 9: Vio-Gen Cell – Given an instance I , a viogen cell for vio-gen query GQdc,i is a cell in the result of
GQdc,i over I that is associated with a variable in the target
comparison of GQdc,i .
2
Consider our DB in Example 1. The two vio-gen queries
for dc1 identify cells t4 .Dept and t1 .Name that can be used
to inject errors, as follows:

FORMALIZATION OF THE PROBLEM

Recall that, given an instance I of S and a set Σ of DCs, to
detect violations we find assignments that generate violation
contexts. This can be done by running a violation-detection
query for dc and I . Recall that id is an attribute storing
the tuple id of a relation. For a constraint dc with multiple
¯ in queries
relation atoms, we abuse notation by using id
to denote a vector of variables bound to all tuple ids from
¯ x̄)), we
these atoms. Given a DC of the form dc : ¬(φ(id,

GQdc1 ,1 result
t4 : Emp(Frank, Mktg, . . .)
t5 : Emp(Frank, Mktg, . . .)

change
t4 .Dept
:= “xxx”

after change
t4 : Emp(Frank, xxx, . . .)
t5 : Emp(Frank, Mktg, . . .)

GQdc1 ,2 result
t1 : Emp(John, Staff, . . .)
t2 : Emp(Paul, Sales, . . .)

change
t1 .Name
:= “Paul”

after change
t1 : Emp(Paul, Staff, . . .)
t2 : Emp(Paul, Sales, . . .)

Query GQdc1 ,1 identifies cells t4 .Dept = t5 .Dept = “Mktg”.
By making these cells different, we are guaranteed to introduce a violation (the tuples have equal names). Similarly,
query GQdc1 ,2 captures the fact that we can introduce a

39

detectable violation by equating cells t1 .Name = “John”,
t3 .Name = “Paul” (since they have different departments).
A few remarks are in order. Vio-gen queries also identify
a context for each vio-gen cell, i.e., a set of cells that are the
image of variables in comparison atoms. For example, the
context of cell t4 .Dept is composed of cells {t4 .Dept, t4 .Name,
t5 .Dept, t5 .Name}. After we have identified a vio-gen cell,
then we need to identify an appropriate value to generate
the actual cell change and update the DB. Together, a viogen cell and a context determine which values can be used to
update the cell. We need to keep track of contexts to avoid
new changes that are accidentally repairing other violations.
These aspects will be discussed in the next section.
We now define the error-generation problem for an errorgeneration task E = hS, Σ, I , Confi. For each constraint
dc ∈ Σ, a parameter (dc) in Conf determines the number
of detectable errors that should be introduced in I .

execute each vio-gen query over I to identify a set of vio-gen
cells (possibly all), and the associated violation contexts.
Task 2: Value Whitelists and Blacklists. For a vio-gen
cell and one of its contexts, we need to find a set of values
that would inject errors into the cell. That is, our algorithm
first determines the context and then determines the actual
changes that are admissible in this context. Here we make
use of a value whitelist (candidate values) and value blacklist
(values that cannot be used). Admissible values for a cell
are obtained by the set difference between the cell’s whitelist
and blacklist. These depend on the constraint, the context,
and the target comparison. In our previous examples:
(i) The target comparison of query GQdc1 ,1 is an equality
(d = d0 ), and we want to generate changes that falsify the
equality; therefore, once we have identified cell t4 .Dept and
its value, “Mktg”, the whitelist contains any string – denoted
by “*” – and the blacklist is the single value {“Mktg”}.
(ii) The target comparison of query GQdc1 ,2 is a not-equal
(n 6= n0 ), and our changes should falsify it; therefore, once
we have identified cell t1 .Name with value “John”, and the
value “Paul” in another cell t2 .Name which shares a violation
context with t1 .Name, the whitelist for cell t1 .Name contains
{“Paul”}, the blacklist contains the original value {“John”}.
The process of identifying values to change vio-gen cells
has some additional subtleties. Consider query GQdc2 ,3 for
constraint dc2 in our running example:
GQdc2 ,3 (id, id’, . . .) = Emp(id, n, d, s, m), Emp(id’, n’, d’, s’, m’),
n = n’, (d 6= d’), d = “Sales”, m 6= m’
Here, the value of variable d in the target comparison is constrained to be “Sales”. As a consequence, we cannot change
its value. This general rule is necessary for detectability:
any change we make to the DB for a vio-gen query must
break the target comparison but preserve all other comparisons within the query. Therefore, we can only change the
value of d0 and make it equal to “Sales”, that is, the whitelist
of vio-gen cells for d0 will only contain that value.
Overall, we determine valid values for vio-gen cells by
computing equivalence classes based on equality comparisons and use an interval algebra for ordering constraints.
Task 3: Handling Interactions. The purpose of this
step is to avoid possible interference among different changes
to the DB. To see the problem, following our discussion of
dc1 , suppose vio-gen query GQdc1 ,1 suggests a change to cell
t4 .Dept to make it equal to “xxx”. Assume after that query
GQdc1 ,2 suggests a change to the Name cell of the same tuple,
to make it equal to “Paul”. If we apply both changes to the
DB, we get the following instance:

Definition 10: Error-Generation Problem – Given an errorgeneration task E = hS, Σ, I , Confi, find (dc) at-least-one
(resp. exactly-one) detectable errors in I for each dc ∈ Σ. 2
It turns out that both variants of the error-generation
problem are NP-complete.
Theorem 1: The exactly-one and at-least-one error-generation problems for a task E are NP-complete.
Before we present algorithms and optimizations to solve
the error-generation problem, we observe that the vio-gen
queries for a constraint dc are nothing else than vio-detection
queries for DCs that can be considered as being “dual” to dc.
Consider constraint dc1 . The two vio-gen queries correspond
to the detection queries of constraints dc11 , dc21 :
dc1 : ¬(Emp(n, d, s, m), Emp(n’, d’, s’, m’), n = n’, d 6= d’)
dc11 : ¬(Emp(n, d, s, m), Emp(n’, d’, s’, m’), n = n’, d = d’)
dc21 : ¬(Emp(n, d, s, m), Emp(n’, d’, s’, m’), n 6= n’, d 6= d’)
Since DCs are closed with respect to the negation of comparison atoms, this is a general property: any vio-gen query
for constraint dc coincides with the vio-detection query of a
constraint dc0 . This highlights the duality between queries
for injecting new errors and those for detecting errors.

5.

THE VIO-GEN ALGORITHM

We now develop a vio-gen algorithm to solve the errorgeneration problem. To efficiently generate large erroneous
datasets, we aim for a solution that is in PTIME and scales
to large databases. Our algorithm is a correct, but not complete, solution for the at-least-one error-generation problem.
When it succeeds, it returns a solution to the input task.
We use vio-gen queries to identify cells to change; also, we
avoid interactions between cell changes by enforcing that two
cell changes cannot share a context. Thus, our algorithm
may fail to find a solution if there is no solution without
shared contexts. However, as long as there are sufficient
vio-gen cells and non-overlapping contexts available, our algorithm will succeed. Intuitively, whether this is the case
depends mainly on the requested error ratios (and to a lesser
extent on the constraints); in practice, ratios are typically
∼1%-10% of the DB size, and therefore the probability of
success is very high. The main tasks are as follows.
Task 1: Finding Vio-Gen Cells and Contexts. Given
dc and I , we generate the vio-gen queries for dc. Then, we

clean DB
t4 : Emp(Frank, Mktg, . . .)
t5 : Emp(Frank, Mktg, . . .)

change
t4 .Dept
:= “xxx”
t4 .Name
:= “Paul”

after changes
t4 : Emp(Paul, xxx, . . .)
t5 : Emp(Frank, Mktg, . . .)

It is easy to see that the instance obtained after these two
changes is not dirty for dc1 , since the second change is removing the detectable error introduced by the first one.
Our algorithm, while generating changes also stores their
violation contexts in main memory. We discard a candidate
change that may remove violations introduced by previous
changes. More precisely, a vio-gen cell is changed only if:
(i) the cell itself does not belong to the context of any other
previous change; (ii) none of the cells in its context have
been changed by previous changes.

40

multiple constraints. If requested by a user, the system will
compute how many constraints are violated by a change.
This can be used to output exactly-one detectable changes.
In addition, it may also compute repairability and filter solutions to guarantee a level of repairability if specified in the
task configuration (Definition 6). This is done as follows.
(i) After changes have been generated and applied to yield
the dirty DB Id , we run the detection query on Id for each
constraint dc in Σ to find all violation contexts for dc and Id ;
recall that this task is essentially the same as Task 1 (vio-gen
queries and vio-detection queries are structurally identical);
we keep counters for each cell change to know how many
violation contexts the cell change occurs in, and how many
constraints it violates; notice that the size of the final set
of exactly-one changes is typically lower than the original
number of at-least-one detectable changes. Therefore, in
some cases the system may need to generate a larger number
of changes in order to output a desired number of exactlyone detectable changes, as discussed in Section 8.
(ii) We compute repairability based on the context equivalence classes (optimizations are discussed in Section 6.2).

Task 4: Generating Random Changes. Bart generates the desired percentage of detectable errors for a constraint and an attribute in the task configuration. Recall
that error-percentages are expressed wrt the size of the DB
table, e.g., 1% of errors in the cells of attribute A in a table
R of 100K tuples corresponds to 1000 errors.
A crucial requirement is that, during this process, vio-gen
cells that may be needed later on are not discarded. In fact,
we do not know if the clean DB contains a sufficient number
of non-interacting vio-gen cells. Suppose we ask for 1000
errors for an attribute, but there are fewer vio-gen cells. In
this case, Bart will generate as many changes as possible,
and log a warning about the impossibility of meeting the
configuration parameter. To handle this, we resorted to a
main-memory sampling algorithm. The algorithm works as
follows. Assume we want to generate n changes for vio-gen
query GQ and attribute R.A:
(i) Before executing GQ, we select a random probability pGQ
within a range that can be configured by the user (usually
10% to 50%), and a random offset oGQ , again randomly
generated in a range from 1% to 10% of the size of R; these
parameters will be used to sample cells at query execution.
(ii) Then, we execute the GQ, and scan its result set; for
each tuple t, we extract the vio-gen cell and vio-gen context,
and check if it overlaps with the ones generated so far; in
this case, we discard the tuple; otherwise, we consider the
tuple as a candidate to generate a detectable change.
(iii) A candidate tuple can be either processed or skipped;
notice however that tuples that are skipped are not lost, but
rather stored in a queue of candidate tuples, in case we need
them later on; first, we skip the first oGQ tuples that would
generate valid vio-gen contexts; after this, for each candidate
tuple we draw a random number random(t): we process the
tuple and generate a new change whenever random(t) <
pGQ ; otherwise, we store the tuple in the candidate queue.
(iv) The process stops as soon as we have generated n changes;
if, however, no more tuples are available in the query result
before this happens, then we go back to the queue of candidate tuples, and iterate the process.
We now state a correctness result. The proof and pseudocode for the algorithm are in our technical report [1].

6.

OPTIMIZATIONS

We now discuss the scalability of the vio-gen algorithm.
The algorithm is designed to solve all subtasks of the errorgeneration problem, i.e., find n changes with at-least-one detectability, and compute detectability and repairability measures. The main cost factor is related to executing vio-gen
queries (in turn, vio-detection queries when measuring detectability and repairability). These techniques do not scale
to large DBs, for two main reasons.
Problem 1: Computing Cross-Products. Some constraints may result in vio-generation queries with no equalities. Such queries would end up being executed as crossproducts, and therefore have inherent quadratic cost. As an
example, consider query GQdc5 ,1 for constraint dc5 :
GQdc5 ,1 (id, id’, . . .) = Emp(id, n, d, s, m), Emp(id’, n’, d’, s’, m’),
(m 6= n’), s’ < s
This query has a very common pattern, one we have already
observed in the vio-gen queries of constraint dc1 .
Problem 2: Computing Expensive Joins. Even when
equalities are present, and joins are performed, this may be
slow. In fact, the distribution of values in the DB might
lead to join results of very large cardinality. Consider an
FD d : Emp : Dept → Mngr, stating that department names
imply manager names. One of the vio-gen queries for d
would be:
GQd,1 (id, id’, . . .) = Emp(id, n, d, s, m), Emp(id’, n’, d’, s’, m’),
(d = d’), (m = m’), id < id’
As we execute this query on a clean instance I , where equal
departments correspond to equal managers, the result of this
join may be much larger than the size of table Emp (e.g.,
when there are few departments).
These problems illustrate that the evaluation of vio-gen
queries to generate errors may not scale well to large DBs.
We introduce two important optimizations to greatly improve performance. We first exploit the fact that we only
have to produce a certain percentage of errors by avoiding the full execution of vio-gen queries involving crossproducts. Second, we identify a class of DCs, called symmetric constraints, for which the vio-gen queries can be significantly simplified.

Theorem 2: Given an error-generation task E = hS, Σ, I ,
Confi, call Ch the output of the vio-gen algorithm over E.
If the algorithm succeeds, then Ch is an at-least-one solution
for task E.
The cost of our vio-gen algorithm is dominated by query
execution. It is in PTIME in the instance size and the number of constraints and is NP-hard in the size of constraints.
Theorem 3: Let E = hS, Σ, I , Confi be an error-generation
task. The vio-gen algorithm runs in PTIME in kIk and kΣk
and is NP-hard in the maximal size (number of atoms) of
constraints in Σ.
Note that this is the worst-case complexity of the algorithm. As we show in Section 8, the algorithm performs very
well in practice, because using sampling we seldom have to
fully evaluate a vio-gen query and the size of constraints is
typically limited to a few relational atoms.
Task 5: Checking Detectability and Repairability.
Theorem 2 guarantees that the vio-gen algorithm generates
detectable changes. Our algorithm only guarantees at-leastone detectability, but may generate changes that violate

41

Sampling Cross-Products

(i) It has a node for every relational atom, comparison atom,
variable and constant in φ(x̄).

For testing data-cleaning algorithms, often we want to
generate a set of errors that is (much) smaller than all errors that could potentially be introduced. Thus, we consider sampling tables, and then computing cross-products
over these samples in main memory as an alternative to
computing the cross-product using a declarative query. To
understand our intuition, consider the typical vio-gen query
with inequalities for an FD, for example dc1 :
GQdc1 ,2 (id, id’, . . .) = Emp(id, n, d, s, m), Emp(id’, n’, d’, s’, m’),
(n 6= n’), d 6= d’, id < id’
We search for tuples with different names and departments.
In any DB with a sufficiently large number of tuples, we
should have a good probability of finding tuples that differ
from each other. Whenever we need to generate n changes:
(i) We scan the tables in the query to extract a sample of c·n
tuples (c being a configuration parameter), and materialize
them in memory.
(ii) We compute the cross-product in memory, filter results
according to the comparisons ((n 6= n’), (d 6= d’)), and use
the results for identifying vio-gen cells and their contexts;
we stop as soon as n contexts have been found.
(iii) If we are not able to find n non-overlapping contexts,
then we repeat the process, i.e., we choose a random offset,
re-sample the tables and iterate; an iteration limit is used
to stop the process before it becomes too expensive.
This strategy has a worst-case cost that is comparable (or
even worse) to that of computing the whole cross-product.
To avoid this cost, we limit the number of iterations. This
may only happen when the cross-product is empty, or has
very few results, both very unlikely cases. Our experiments
confirm that typically, our optimized version runs orders of
magnitude faster than computing the cross-product using
the DBMS, even when using the LIMIT clause to reduce
the number of results.

(iii) There is an edge between the node for atom Ri (x̄i ) and
each node for a variable x ∈ x̄i ; if x appears within attribute
A, the edge is labeled by Ri .A.
(iv) There is an edge between the node for atom v op v 0 (v, v 0
either variable or constant) and the nodes for v, v 0 .
2
We are interested in subformulas that are isomorphic to
each other according to some mapping h of the variables.
Thus, given h, we use it to break up the graph, and look for
isomorphic connected components. Given a mapping of the
variables h : x̄ → x̄, we define the reduced formula graph for
h as the graph obtained from G(φ(x̄)) by removing all nodes
corresponding to comparisons of the form x op h(x).
Definition 12: Symmetric Formula – A formula in normal form ¬(φ(x̄)) with no ordering comparisons and at least
one equality is symmetric with respect to mapping h if its
reduced connection graph contains exactly two connected
components, and these are isomorphic to each other according to h, i.e., (i) a variable label v can be mapped to h(v),
(ii) every other label is preserved.
2
In our example, both dc1 and dc2 are symmetric constraints (with mapping n → n’, d → d’, s → s’, m → m’).
The formula graph for dc2 is depicted in Figure 2. Notice
how, for the purpose of testing symmetry, we add the implied comparison atom d0 = “Sales”.

="

s$ m$
="

ry"

Dep

“Sales”(

≠"

t"

ry"

n$ d$

Symmetric Constraints

It is possible to find tuples that violate CFDs by running
join-free queries using the group-by operator [5, 12]. A similar technique was used earlier for scalable consistent query
answering over schemas containing keys [15]. In this section,
we build on these results, and extend them in several ways:
(i) We generalize Bohannon et al.’s [5] treatment of CFDs
by formalizing the notion of symmetric denial constraints;
this significantly enlarges the class of constraints for which
this optimization can be adopted.
(ii) We develop a general algorithm to optimize the execution of symmetric queries with at least one equality and
arbitrary inequalities, and show how we can efficiently compute both violation contexts and context equivalence classes.
(iii) We conduct an extensive experimental evaluation to
show the benefits of this optimization.
We now blur the distinction between DCs, vio-gen queries,
and detection queries, and speak simply about their logical
formulas. Consider a DC d : ¬(φ(x̄)) in normal form; assume that φ(x̄) has no ordering comparisons (>, <, ≤, ≥),
and contains at least one equality. Intuitively, d is symmetric if it can be “broken up” in two isomorphic subformulas.
To formalize this idea, we use the following definition.

="

Emp$
Dep

t"

Emp$
Sala

6.2

(ii) A node for relational atom Ri (x̄i ) has label Ri ; a node
for comparison v op v 0 has label op ; a node for variable x
(constant c) has label x (c, respectively).

Sala

6.1

m’$ s’$ d’$ n’$
="
“Sales”(

Figure 2: Formula Graph for dc2
It is easy to see that DCs encoding FDs are all symmetric.
This is also true for CFDs, with the exception of singletuple constraints which do not require joins, like dc3 in our
example. Editing rules, like dc4 , and ordering constraints,
like dc5 , on the contrary, are not symmetric.
The vio-detection query for a symmetric constraint is always symmetric. However, not all vio-gen queries for a symmetric constraint need to be symmetric as some may not
have equalities. For example, we know dc1 is symmetric
(n = n’, d 6= d’), and among its vio-gen queries, one is symmetric (n = n’, d = d’), the second one is not (n 6= n’, d 6= d’).
Symmetric constraints significantly reduce the number of
joins required to execute the corresponding queries. The
intuition is that we can only consider one of the isomorphic
subqueries, and avoid redundant work. We represent these
subcomponents of the original queries by means of relational
atoms with adornments. An adornment for a variable x ∈ x̄i
is a superscript of the form = or 6=, denoted by x= or x6= .
We use adornments to track the constraints that are imposed
over variables within the original symmetric query.

Definition 11: Formula Graph – Given ¬(φ(x̄)) in normal
form, its formula graph, G(φ(x̄)), is defined as follows:

42

Given a symmetric formula φ(x̄) in normal form, to optimize its execution we generate a reduced formula with adornments, denoted by reduce(φ(x̄)). Following are the reduced
formulas for the symmetric vio-gen queries of dc1 , dc2 (we
use boldface to mark the variables that were involved in the
target comparison of the original query):
reduce(GQdc1 ,1 ) = Emp(id, n= , d= , s, m)
reduce(GQdc2 ,1 ) = Emp(id, n= , d= , s, m= ), d = “Sales”
reduce(GQdc2 ,2 ) = Emp(id, n6= , d= , s, m6= ), d = “Sales”
reduce(GQdc2 ,3 ) = Emp(id, n= , d= , s, m6= ), d 6= “Sales”
Intuitively, we use reduce(GQdc1 ,1 ) to derive a (join-free)
SQL query that will give us all tuples t in Emp such that
there exists a tuple t0 with a different tuple id, equal name
and equal department. Similarly for reduce(GQdc2 ,1 ) (equal
names, equal departments both “Sales”, equal managers).
Reduced formulas are constructed using the following algorithm (ids are treated separately).
(i) Start with φ(x̄) and variable mapping h; consider the
formula φ0 (x̄0 ) corresponding to one of the connected components of the reduced formula graph for φ(x̄) and h.
(ii) For each comparison x op h(x) in φ(x̄), add to variable
x in φ0 (x̄0 ) adornment op .
Following is a more complex constraint. It states that an
FD dc6 : Emp : Name → Manager holds for tuples of Emp that
correspond to master-data tuples on Name and Dept:
dc6 : ¬(Emp(n, d, s, m), MD(n’, d’, m’), n = n’, d = d’,
Emp(n”, d”, s”, m”), MD(n”’, d”’, m”’), n” = n”’, d” = d”’,
n = n”, m 6= m”)
The constraint is symmetric and this is the reduced formula
of one of its symmetric vio-gen queries (as usual, we explicitly mention id attributes in the formula) :
reduce(GQdc6 ,1 ) = Emp(id, n6= , d, s, m6= ), MD(id’, n’, d’, s’, m’),
n = n’, d = d’

6.3

to the same context equivalence class for dc and I if and
only if: (i) they belong to contexts vc1 , vc2 for dc and I ; (ii)
vc1 , vc2 have equal values for the equality variables of dc. 2
To construct contexts and equivalence classes, we generate
an SQL query with aggregates, denoted by sym-sql(reduce(Q)),
from the reduced formula. In our example:
SELECT id, name, dept, mngr FROM emp WHERE name, dept IN
(SELECT name, dept FROM emp WHERE dept = ’Sales’
GROUP BY name, dept HAVING COUNT(DISTINCT mngr) > 1)
ORDER BY name, dept

Contexts and equivalence classes are built as follows:
(i) We run query sym-sql(reduce(Q)).
(ii) We group tuples in the result with equal name and dept
attribute values, yielding all context equivalence classes.
(iii) To construct the actual contexts, tuples within an equivalence class are combined in all possible ways in memory.
The General Case. Let us now consider a generic formula
with adornments, with at least one equality and multiple
inequalities, like the following:
reduce(GQdc2 ,2 ) = Emp(id, n6= , d= , s, m6= ), d = “Sales”
Here, we are looking for pairs of tuples that have equal departments (with value “Sales”), and both different names,
and different managers. Handling both inequalities makes
the construction of contexts more complex. The intuition
behind the algorithm is to execute multiple aggregates within
our SQL query, one for each inequality, to identify cells that
are candidates to generate violation contexts. In our example, sym-sql(reduce(GQdc2 ,2 )) would be the following query:
SELECT id, name, dept, mngr FROM emp
WHERE dept, name IN (SELECT dept, name FROM emp
GROUP BY dept, name HAVING COUNT(DISTINCT name) > 1)
AND dept, mngr IN (SELECT dept, mngr FROM emp
GROUP BY dept, mngr HAVING COUNT(DISTINCT mngr) > 1)
ORDER BY dept

The Benefits of Symmetry

Reduced formulas suggest an alternative query execution
strategy that is based on a limited use of joins and favors
group-by clauses. This strategy was previously used to find
tuples involved in violations of (multi-tuple) CFDs [5]. We
extend those algorithms to a larger class of constraints. Our
work can handle multiple inequality adornments, while the
original technique only considered one inequality at a time.
We first summarize the main ideas behind the use of
group-by clauses for formulas with at most one inequality,
then discuss the general case.
Simple Case: At Most One Inequality Adornment.
Context equivalence classes for symmetric queries with at
most one inequality can be computed by grouping tuples.
Consider constraint dc2 and the reduced formula for its symmetric vio-gen query:
reduce(GQdc2 ,1 ) = Emp(id, n= , d= , s, m6= ), d = “Sales”
In this case, a vio-gen context consists of two tuples with
equal names and departments (equal to “Sales”), and different managers. A context equivalence class is composed
of all tuples that have equal values of names and departments (“Sales”), such that there exist at least two different managers associated with them. Notice that this is a
general property. We call the set of variables with equality adornments in a reduced formula its equality variables
(corresponding to variables equated in original formula).

Notice, however, that Property 1 does not hold in this
case. Indeed, belonging to the result of this query is a necessary but not sufficient condition for a cell to be in a violation context for GQdc2 ,2 . In fact, we have no guarantee
that two tuples from the result of the SQL query above satisfy all inequalities. To select the cells that actually belong
to contexts, we need to build the actual contexts, and keep
only those in which all inequalities are satisfied.
A crucial optimization, here, is to select a relatively small
set of candidate tuples for each context equivalence class.
To do this, we group the tuples in the result of the query
on the values of the equality variable (dept in our example).
Then, we combine the candidate tuples within each set to
generate the actual violation contexts.
Once the contexts have been generated, building the context equivalence classes is straightforward: it suffices to hash
contexts based on the values of the equality variables, and
compute equivalence classes from the cells in each bucket.
We show experimentally in Section 8 that this strategy performs very well even for large DBs.
Since symmetry guarantees good performance and helps
us avoid extensive sampling, whenever this is compatible
with the configuration parameters, our algorithm tries to
favor symmetric vio-gen queries over non-symmetric ones.

7.

Property 1: Given a symmetric constraint dc with at most
one inequality, and instance I , two cells c1 , c2 of I belong

USE CASES OF THE TOOL

Use-Case 1: Generate Detectable Errors. The main

43

purpose of Bart is to generate constraint-induced errors.
We control an error-generation task E by a set of configuration parameters Conf. The main parameters are:
(i) Authoritative sources: names of DB relations that are to
be considered immutable (empty by default).
(ii) Error percentages: desired degree of detectable errors
for each vio-gen query. We specify percentages with respect
to the number of tuples in a table (e.g., 1% errors in a table
of 100K tuples means errors are introduced in 1000 cells).
(iii) Repairability range: users may also specify a range of repairability values for each vio-gen query; Bart will estimate
the repairability of changes, and only generate errors with
estimated repairability within that range. This simplifies
the generation of configurations with controlled repairability (e.g., low), as shown in our experiments.
Use-Case 2: Generate Random Errors. In addition
to detectable errors, Bart may also generate random errors
of several kinds: typos (e.g., ‘databse’), duplicated values,
bogus or null values (e.g., ‘999’, ‘***’), and outliers. Random
errors may be freely mixed with constraint-induced ones.
Use-Case 3: Computing Repairability. Given a set
of constraints Σ and a dirty DB Id , Bart can be used to
compute the repairability of the violations in Id with respect
to Σ, as per Definition 6. This can also be done in the case
in which Id was not generated by the tool itself.
Use-Case 4: Measuring Repair Quality. Our ultimate goal is to benchmark data-repairing algorithms over
Bart data. Suppose we run some algorithm A over a dirty
instance Id as generated by Bart, and we obtain a repaired instance Irep,A by a set ChA of cell changes, i.e.,
Irep,A = ChA (Id ). The tool adopts a natural strategy to
measure the performance of A over a task E.
We call Ch−1 the set of cell changes that are needed to
bring Id back to its original state, I . Since we assume that
I , Id and Irep,A all have the same set of tuple ids, we define
the quality of A over E as the F-Measure of the set ChA ,
measured with respect to Ch−1 . That is, we compute the
precision and recall of A in fixing the errors introduced in
the original clean instance I . The higher the F-measure, the
closer Irep,A is to the original clean instance I .
Since data-repairing algorithms have used different metrics to measure the quality of repairs, Bart has been designed to be flexible in this respect. Precision and recall
may be computed using the following measures:
(i) Value: we count the number of cells that have been restored to their original values.
(ii) Cell-Var: in addition to cells restored to their original
values, we count (with 0.5 score) the cells that have been
correctly identified as erroneous, and changed to a variable.
(iii) Cell: we count the cells that have been identified as
erroneous, regardless of the value assigned by the algorithm.

8.

Fan et al. [13]; (iv) Bus is a real-world scenario from Dallachiesa et al. [11]; and (v) Hospital is a real-world scenario
used in several data repairing papers (e.g., [11, 14, 16]).
Note that all datasets have different characteristics. Hospital and Bus have higher redundancy in their data. Tax
and Employees are the only datasets with constraints containing ordering (<, >) comparisons. Some datasets have
master-data and CFDs, while others have only FDs. All
these differences help to validate our techniques.
Settings. To measure the scalability of error generation, we
focus on execution times. Each task has been run five times
and we report the average execution time. Because our focus
is on error generation for testing data-cleaning algorithms,
we report our study on different % of errors. We first study
the range (1% - 10%), in which most cleaning algorithms can
perform reasonably well, and then (25% - 75%). To show the
effectiveness of our optimizations for symmetric constraints,
we compare them against the standard execution.
Scalability. Figures 3.a-c report Bart’s execution times
on synthetic data sets, over an increasing number of tuples
for 1%, 5%, and 10% injected errors, respectively. As expected, execution times increase both with the size of the
input (number of tuples) and the number of changes (% of
required errors). Our system is very fast, taking at most 6.6
minutes for one million tuples in the worst scenario (10%
errors). The same observations apply for real-world data, as
we show in the first three pairs of bars in Figure 3.d. Notice
that, without our optimizations, the execution of all scenarios exceeds the time threshold we set (30 minutes). For this
reason we do not report the data.
Symmetric Queries. For each scenario, we extracted all
symmetric vio-gen queries and executed them with and without our optimization (we report the number of symmetric
queries per scenario in Figure 4). Figure 3.e and f report the
execution times over an increasing number of tuples and 5%
injected errors for the synthetic datasets, with and without
the symmetric optimization, respectively. Figure 3.d reports
results for real datasets in the last two pairs of bars, using a
10 minutes timeout for each symmetric vio-gen query. Note
how the symmetric strategy scales linearly with the size of
the data, while the same observation does not hold for the
standard join-based strategy (the DBMS adopts a nestedloop execution plan). Our symmetric optimization runs up
to two orders of magnitude faster on these datasets.
Success Rate. Our algorithm trades completeness for scalability. It may fail to return the required number of changes,
even if these exists, because it non-optimally uses violation
contexts. To gain more insight on this aspect, we studied
the behavior of the algorithm when 25%, 50%, and 75% of
the size of the DB is required to be made dirty. In Figure 3.g
we report the fraction of changes that have been generated.
Figure 3.h reports the breakdown of the success rate for every constraint of Customers. To limit the incidence of the
distribution of data on the experiment, for each constraint
we first computed an estimate of the maximal number of
detectable changes that can be generated. Then, when running the actual experiments, we made sure to never ask for
a number of changes per constraint higher than this estimate. It is interesting to note that in all scenarios, the algorithm generates 100% of the required changes for scenarios
with 25% error rate. This can be considered satisfactory for
most error-generation applications. The percentage reduces
progressively for 50% and 75% errors.

EXPERIMENTAL RESULTS

We describe a detailed evaluation of the Bart Java prototype over synthetic and real-world datasets. We ran experiments on a machine with 8GB RAM, 2.6 GHz Intel Core i7,
MacOS 10.10, and PostgreSQL 9.3.
Tasks. We tested five tasks, based on synthetic and real
datasets (full details are reported in our technical report [1]).
We briefly list them here: (i) Employees is the running example used in the paper; (ii) Customers is a synthetic scenario
from Geerts et al. [16]; (iii) Tax is a synthetic scenario from

44

Total Error-Generation time

300
200

200
100

10000
sec

Employees
Customers
Tax

300
Time (sec)

300

400

400
sec

Employees
Customers
Tax

Time (sec, log)

Employees
Customers
Tax

Time (sec)

Time (sec)

600
sec
500

400
sec

200
100

Hospital 100K
Bus 250K

1000

100

100
0

200K 400K 600K 800K
Size (# tuples)

0

1M

a) Tasks Employees, Customers
and Tax (1% errors)

200K 400K 600K 800K
Size (# tuples)

0

1M

b) Tasks Employees, Customers
and Tax (5% errors)

200K 400K 600K 800K
Size (# tuples)

1000
100
10
1

200K 400K 600K 800K
Size (# tuples)

Tax

100
10
1

e) Symmetric Optimization On, 5%

200K 400K 600K 800K
Size (# tuples)

25p

100

1000

1M

10%

Sym.Opt. Sym.Opt
On 5% Off 5%

Success Rates

Customers

10000
sec
Time (sec, log)

Time (sec, log)

10000
sec

Employees

5%

d) Tasks Hospital 100K and
Bus 250K

1M

50p

75p

80
60
40
20
0

25p

100

Success Rate (percentage)

Tax

Success Rate (percentage)

Customers

1%

c) Tasks Employees, Customers
and Tax (10% errors)

Total time to execute symmetric vio-gen queries
Employees

10

1M

50p

75p

80

60

40

20

0

Emp
1 Cust
2

f) Symmetric Optimization Off, 5%

cfd1
1 cfd2
2 md1
3 md2
4 md3
5 fd1
6

Tax
Bus
3 Hosp
4
5

g) Total Success Rate

fd2
7

fd3
8

h) Success Rate for Dep.(Cust200K)

Times to generate errors, apply changes to the clean instance and check detectability and repairability

300
Time (sec)

Time (sec)

300

1200

Check Detectability and Repairability
Apply Changes
Generate Errors

200

200

100

100

0

0

1000
sec
800

500
sec

Check Detectability and Repairability
Apply Changes
Generate Errors

400
Time (sec)

400
sec

Check Detectability and Repairability
Apply Changes
Generate Errors

Time (sec)

400
sec

600
400

100k 200k 400k 800k

1M

i) Task Employees, 5%

100k 200k 400k 800k

1M

j) Task Customers, 5%

300
200
100

200
0

Check Detectability and Repairability
Apply Changes
Generate Errors

100k 200k 400k 800k

0

1M

k) Task Tax, 5%

Hospital

Bus

l) Tasks Hospital and Bus, 5%

0K

Emp Cust Tax Hosp Bus
200k 200k 200k 100k 250k

1.0

2500 changes for each dep.

0.8
0.6
0.4
0.2
0

0.4
0.2
0

Emp

Cust

0.8
0.6
0.4
0.2
0

Tax

n) Exactly-One avg. perc. wrt Total
Changes for Emp, Cust and Tax Tasks
Avg Repairability for Constraint

Avg Repairability for Constraint

m) At-Least-One vs Exactly-One
Detectability for All Scenarios, 5%

0.6

cfd1
fd1
fd2
fd3
Overall Avg Repairability: 0.25

q) Low Repairability Configuration
Task Customers, 200k, 5%

1.0

800 changes for each dep.

0.8
0.6
0.4
0.2
0

Avg Repairability for Constraint

3K

0.8

2500 changes for each dep.

cfd1
fd2
md1
md2
Overall Avg Repairability: 0.81

o) High Repairability Configuration
Task Customers, 200k, 5%
1.0

0.6
0.4
0.2
0

e0 e1 e2 e3 e4 e8
Overall Avg Repairability: 0.46

r) Medium Repairability Configuration
Task Hospital, 100k, 5%

2500 changes for each dep.

0.8

1.0

1600 changes for each dep.

0.8
0.6
0.4
0.2
0

cfd1 fd1 fd2 md1 fd3 md2
Overall Avg Repairability: 0.55

p) Medium Repairability Configuration
Task Customers, 200k, 5%
Avg Repairability for Constraint

6K

1.0

Avg Repairability for Constraint

9K

1.0

Avg Repairability for Constraint

# Cell Changes

12K

Exactly One Changes (avg %)

Detectability and Repairability
At-Least-One Detectable Changes
Exactly One Detectable Changes

e0
e1
e2
e3
Overall Avg Repairability: 0.50

s) Medium Repairability Configuration
Task Tax, 200k, 5%

1.0

2000 changes for each dep.

0.8
0.6
0.4
0.2
0

e1
e2
e3
e4
e5
Overall Avg Repairability: 0.43

t) Medium Repairability Configuration
Task Emp, 200k, 5%

Evaluation of the Tools

0.2

0.2

0

0

High52
Rep Med
53Rep Low
54Rep

u) Tool Evaluation: Hospital 20k, 5%,
High, Med, Low Repairability
(Cell-Var metric)

0.2
0

High55
Rep Med
56Rep Low
57Rep

High58
Rep Med
59Rep Low
60Rep
w) Tool Evaluation: Tax 20k, 5%,
High, Med, Low Repairability
(Cell-Var metric)

v) Tool Evaluation: Bus 20k, 5%,
High, Med, Low Repairability
(Cell-Var metric)

Figure 3: Experimental Results

45

0.8

F-Measure

avg. 0.20

avg. 0.25

0.4

avg. 0.31

0.6

1.0

Llunatic
Holistic
Greedy
Sampling

0.8

F-Measure

avg. 0.20

0.4

avg. 0.29

0.6

1.0

Llunatic
Holistic
Greedy
Sampling

0.8

F-Measure

avg. 0.19

avg. 0.27

0.4

avg. 0.33

F-Measure

0.8
0.6

1.0

Llunatic
Holistic
Greedy
Sampling

avg. 0.35

1.0

SCARE - Cell
SCARE - Value

0.6
0.4
0.2
0

Missing
Typos
Outliers
61
62
63
x) Tool Evaluation: Tax-Rnd 20k, 5%,
Random errors and Outliers
(Cell and Value metric)

Datasets

Type and Size

#Tables #Attributes #Constraints

#Vio-Gen #Symmetric #Non Symmetric #Queries with
Queries
Queries
Queries
No Equalities

Employees Synthetic, 100K to 1M

2

7

5

12

4

5

3

Customers Synthetic, 100K to 1M
Tax Synthetic, 100K to 1M
Bus
Real 250K
Hospital
Real, 100K

3
1
1
1

16
13
5
19

8
5
12
7

22
18
24
16

7
12
10
11

12
2
2
0

3
4
12
5

Figure 4: Datasets and Tasks
and made it dirty with 5% errors and different repairability
levels: High (approximately 0.8 rep.), Med (0.5 rep), and
Low (0.25 rep.). For testing SCARE [25], we used task (iv)
Tax-Rnd. We injected errors of different kinds in the Tax
dataset: (a) 5% missing values, (b) 5% typos, and (c) 5%
outliers over numerical attributes.
We only report results for the constraint-based algorithms
over detectable errors (tasks (i)–(iii)), and for SCARE over
random errors (task (iv)). As expected, the performance of
the algorithms is quite poor when they are applied to errors
that are outside of their scope.
Results. The purpose of this evaluation is not to assess
the quality of repair algorithms, rather to show how Bart
can be used to uncover new insights into the data-repairing
process. We measured the quality of repairs in terms of precision/recall using the Value, Cell-Var, and Cell metrics as explained in Section 7. When multiple repairs were returned,
we chose the best one. We show the results in Figure 3.u–x.
(a) We notice a wide degree of variability in quality among
all algorithms, from excellent repairs to low-quality ones
(a trend observed in Figures 3.u–w). This variability does
not clearly emerge from evaluations reported in the literature, an observation that suggests there is no definitive
data-repairing algorithm yet.
(b) We observe different trends with respect to repairability: (b.1) some of the algorithms return very good repairs
when sufficient information is available (i.e., high repairability); however, their quality tends to degrade quickly as repairability decreases; (b.2) on the contrary, the Sampling algorithm [3] – for which we return the best and average quality among a random sample of 500 repairs – is less affected
by different levels of repairability.
(c) For Task (iv) (as shown in Figure 3.x), (c.1) different
kinds of random errors pose challenges of different complexity, with missing values being the easiest ones to detect;
(c.2) interestingly, these errors are more easily detected than
fixed, as shown by the differences between the Cell and Value
metrics (the first one only counts cells that have been correctly identified as erroneous, while the second one requires
that they have been restored to their original value).
A key observation that emerges from this study is that
repairability has a strong correlation with the quality of the
repairs, thus capturing the “hardness” of the problem. Our
study also shows the importance of having systematic errorgeneration tools for evaluating data-repairing solutions.

Total Execution Time. Figures 3.i-k report total execution times over an increasing number of tuples and 5%
injected errors for datasets Employees, Customers, and Tax,
respectively. Not surprisingly, most of the time is consumed
by the updates to the DB (there are thousands of updates,
which are known to be slow). Error generation does not
dominate the total time and is stable over the different
scenarios, while the computation of detectability and repairability vary significantly depending on the characteristics of the constraints. Again, Tax is the scenario with the
most complex constraints, and this is reflected in the execution time for the detection (note the different scale in the
y axis). Real-data in Figure 3.l is also consistent with synthetic datasets of the corresponding size in the distribution
of the time for the different tasks. Hospital requires less time
because of the size and the smaller set of constraints.
Detectability and Repairability. Figure 3.m shows how
the ratio between at-least-one and exactly-one detectable
changes depends on the constraints and the data. For example, Hospital has redundant data and overlapping rules,
thus it is less likely to find exactly-one detectable changes.
To further study this relationship, we used 10 experiments
with different DB sizes and error percentages over the three
synthetic datasets. We report in Figure 3.n the average percentage of exactly-one detectable changes, with 95% confidence intervals. We observe that, given an error-generation
task, it is fairly easy to estimate the number of at-leastone detectable changes to request in order to obtain a given
number of exactly-one detectable changes.
Figures 3.o-q report the average repairability of errors in
Customers per constraint, using three configurations (i.e.,
high, medium, and low repairability). Every constraint has
the same required amount of errors (5%), and all configurations used the same clean instance of 200k tuples. Intuitively, a high repairability configuration involves mostly
rules with master data and CFDs, while a low repairability
one involves FDs. Results for medium-repairability configurations on the remaining scenarios are in Figures 3.r-t.

9.

DATA REPAIRING TOOL EVALUATION

In this section, we present an empirical comparison of several data-repairing algorithms over Bart data. We show a
number of novel insights into these algorithms that could
not have been shown with existing error generators.
Tools and Algorithms. We used two publicly available
tools, namely Llunatic [17] and Nadeef [11], to run four
data-repairing algorithms: (i) Greedy [6, 9]; (ii) Holistic [8];
(iii) Llunatic, the chase-based algorithm [16]; and (iv) Sampling [3]. In addition, we obtained a copy of the (v) SCARE
[25] statistics-based tool from the authors.
Tasks. We tested four tasks, three of these constraint-based
and one statistics-based (i.e., random errors). For testing
constraint-based algorithms, we used (i) Hospital, (ii) Bus,
and (iii) Tax. We restricted the set of DCs to FDs and
CFDs as only these can be handled by the algorithms under
consideration. We selected a clean instance of 20K tuples,

10.

RELATED WORK

Many researchers have had to consider the problem of injecting errors into clean data in order to evaluate a cleaning
method. Typically, these approaches inject random errors
by scanning DB attributes involved in data-quality rules and
changing attribute values with some fixed probability [3, 6,
8, 11, 16]. While often not described in detail, none of this
work claims to control the properties of errors like their detectability or repairability. In some approaches, each (individual) injected error is (at-least-one) detectable [9], but no

46

guaranteed is made that a set of X errors introduces X detectable errors. No claim is made that their error injection
process would work for constraints beyond those considered
in their experiments or that it is scalable.
Missing Answers and Why-Not Provenance. Given
a query Q and a DB I , a solution to the missing answer
problem is an explanation for why Q(I ) does not include
one or more tuples. Instance-based approaches [18, 19] determine how the input instance can be changed to make the
missing answer appear in the result. We view the problem
of introducing errors into a clean instance as an instancebased missing answer problem, i.e., how to modify the DB
to make vio-detection query results non-empty. As we strive
to perform only updates of attributes values, approaches
that insert or delete tuples to explain missing answers are
inapplicable to our problem. While Huang et al.’s technique [19] supports updates, it is known to produce some
incorrect answers (see [18]). How-To queries as supported
by Tiresias [22] could also be used to encode error generation. Since these approaches rely on constraint solvers and
on possible world semantics, they need to consider multiple
solutions and minimize the presence of non-expected tuples
in the queries (i.e., side-effects). In contrast, by applying
at-least-one detectability we do not have to compute multiple solutions and minimize side-effects. Furthermore, we
introduce a novel optimization for symmetric queries which
has not been considered by missing answer approaches.
View Update. The missing answer problem and also ours,
are essentially a new take on the view update problem [2].
The problem of introducing errors is equivalent to inserting
tuples into views corresponding to the violation-detection
queries. Here we only consider approaches that translate
view insertions into updates to the base data. While an
overview of this problem is beyond the scope of the present
work, two examples are the work of Shu [23], who proposes
to encode the problem as constraint satisfaction, and Cong
et al. [10], who study the complexity of the problem and its
relationship to annotation propagation. One major cost factor in view update is computing a translation that minimizes
the side-effects on the view and/or the instance. We avoid
that cost with our at-least-one detectability semantics.

11.

[3] G. Beskales, I. F. Ilyas, and L. Golab. Sampling the
Repairs of Functional Dependency Violations under
Hard Constraints. PVLDB, 3:197–207, 2010.
[4] C. Binnig, D. Kossmann, and E. Lo. Reverse Query
Processing. In ICDE, pages 506–515, 2007.
[5] P. Bohannon, W. Fan, F. Geerts, X. Jia, and
A. Kementsietsidis. Conditional Functional
Dependencies for Data Cleaning. In ICDE, pages
746–755, 2007.
[6] P. Bohannon, M. Flaster, W. Fan, and R. Rastogi. A
Cost-Based Model and Effective Heuristic for
Repairing Constraints by Value Modification. In
SIGMOD, pages 143–154, 2005.
[7] L. Bravo, W. Fan, and S. Ma. Extending Dependencies
with Conditions. In VLDB, pages 243–254, 2007.
[8] X. Chu, I. F. Ilyas, and P. Papotti. Holistic Data
Cleaning: Putting Violations into Context. In ICDE,
pages 458–469, 2013.
[9] G. Cong, W. Fan, F. Geerts, X. Jia, and S. Ma.
Improving Data Quality: Consistency and Accuracy.
In VLDB, pages 315–326, 2007.
[10] G. Cong, W. Fan, F. Geerts, J. Li, and J. Luo. On the
Complexity of Annotation Propagation and View
Update Analyses. IEEE TKDE, 24(3):506–519, 2012.
[11] M. Dallachiesa, A. Ebaid, A. Eldawy, A. K.
Elmagarmid, I. F. Ilyas, M. Ouzzani, and N. Tang.
NADEEF: a Commodity Data Cleaning System. In
SIGMOD, pages 541–552, 2013.
[12] W. Fan and F. Geerts. Foundations of Data Quality
Management. Morgan & Claypool, 2012.
[13] W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis.
Conditional Functional Dependencies for Capturing
Data Inconsistencies. ACM TODS, 33, 2008.
[14] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Towards
Certain Fixes with Editing Rules and Master Data.
PVLDB, 3(1):173–184, 2010.
[15] A. Fuxman, E. Fazli, and R. J. Miller. ConQuer:
Efficient Management of Inconsistent Databases. In
SIGMOD, pages 155–166, 2005.
[16] F. Geerts, G. Mecca, P. Papotti, and D. Santoro.
Mapping and Cleaning. In ICDE, pages 232–243, 2014.
[17] F. Geerts, G. Mecca, P. Papotti, and D. Santoro.
That’s All Folks! LLUNATIC Goes Open Source.
PVLDB, 7(13):1565–1568, 2014.
[18] M. Herschel and M. A. Hernández. Explaining Missing
Answers to SPJUA Queries. PVLDB, 3(1):185–196,
2010.
[19] J. Huang, T. Chen, A. Doan, and J. F. Naughton. On
the Provenance of Non-Answers to Queries over
Extracted Data. PVLDB, 1(1):736–747, 2008.
[20] S. Kolahi and L. V. S. Lakshmanan. On
Approximating Optimum Repairs for Functional
Dependency Violations. In ICDT, 2009.
[21] A. Lopatenko and L. Bravo. Efficient Approximation
Algorithms for Repairing Inconsistent Databases. In
ICDE, pages 216–225, 2007.
[22] A. Meliou and D. Suciu. Tiresias: the Database
Oracle for How-To Queries. In SIGMOD, pages
337–348, 2012.
[23] H. Shu. Using Constraint Satisfaction for View
Update. J. Intelligent Inf. Sys., 15(2):147–173, 2000.
[24] J. Wang and N. Tang. Towards Dependable Data
Repairing with Fixing Rules. In SIGMOD, pages
457–468, 2014.
[25] M. Yakout, L. Berti-Équille, and A. K. Elmagarmid.
Don’t be SCAREd: Use SCalable Automatic
REpairing with Maximal Likelihood and Bounded
Changes. In SIGMOD, pages 553–564, 2013.

CONCLUSIONS AND FUTURE WORK

We have presented the first scalable error-generation tool
that provides a high degree of control over the generation
process. Bart generates errors by value modification and
we are currently exploring the introduction of insertions and
deletions. The main technical challenge to be solved is that
this completely changes the nature of the quality metric and
in its full generality, would require identifying tuple homomorphisms among two databases, a problem for which there
are currently no scalable algorithms.

12.

REFERENCES

[1] P. C. Arocena, B. Glavic, G. Mecca, R. J. Miller,
P. Papotti, and D. Santoro. Error Generation for
Evaluating Data Cleaning Algorithms. Technical
Report TR-01-2015 –
http://db.unibas.it/projects/bart/files/TR-01-2015.pdf,
Università della Basilicata, 2015.
[2] F. Bancilhon and N. Spyratos. Update Semantics of
Relational Views. ACM TODS, 6(4):557–575, 1981.

47

Discovering Denial Constraints
Xu Chu

⇤

University of Waterloo

x4chu@uwaterloo.ca

Ihab F. Ilyas

Paolo Papotti

ikaldas@qf.org.qa

ppapotti@qf.org.qa

QCRI

ABSTRACT
Integrity constraints (ICs) provide a valuable tool for enforcing correct application semantics. However, designing ICs requires experts and time. Proposals for automatic discovery have been made
for some formalisms, such as functional dependencies and their extension conditional functional dependencies. Unfortunately, these
dependencies cannot express many common business rules. For
example, an American citizen cannot have lower salary and higher
tax rate than another citizen in the same state. In this paper, we
tackle the challenges of discovering dependencies in a more expressive integrity constraint language, namely Denial Constraints
(DCs). DCs are expressive enough to overcome the limits of previous languages and, at the same time, have enough structure to
allow efficient discovery and application in several scenarios. We
lay out theoretical and practical foundations for DCs, including a
set of sound inference rules and a linear algorithm for implication
testing. We then develop an efficient instance-driven DC discovery algorithm and propose a novel scoring function to rank DCs for
user validation. Using real-world and synthetic datasets, we experimentally evaluate scalability and effectiveness of our solution.

1.

INTRODUCTION

As businesses generate and consume data more than ever, enforcing and maintaining the quality of their data assets become critical
tasks. One in three business leaders does not trust the information
used to make decisions [12], since establishing trust in data becomes a challenge as the variety and the number of sources grow.
Therefore, data cleaning is an urgent task towards improving data
quality. Integrity constraints (ICs), originally designed to improve
the quality of a database schema, have been recently repurposed
towards improving the quality of data, either through checking the
validity of the data at points of entry, or by cleaning the dirty data
at various points during the processing pipeline [10, 13].Traditional
types of ICs, such as key constraints, check constraints, functional
⇤Work done while interning at QCRI.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Articles from this volume were invited to present
their results at The 39th International Conference on Very Large Data Bases,
August 26th - 30th 2013, Riva del Garda, Trento, Italy.
Proceedings of the VLDB Endowment, Vol. 6, No. 13
Copyright 2013 VLDB Endowment 2150-8097/13/13... $ 10.00.

QCRI

dependencies (FDs), and their extension conditional functional dependencies (CFDs) have been proposed for data quality management [7]. However, there is still a big space of ICs that cannot be
captured by the aforementioned types.
E XAMPLE 1. Consider the US tax records in Table 1. Each
record describes an individual address and tax information with 15
attributes: first and last name (FN, LN), gender (GD), area code
(AC), mobile phone number (PH), city (CT), state (ST), zip code
(ZIP), marital status (MS), has children (CH), salary (SAL), tax
rate (TR), tax exemption amount if single (STX), married (MTX),
and having children (CTX).
Suppose that the following constraints hold: (1) area code and
phone identify a person; (2) two persons with the same zip code
live in the same state; (3) a person who lives in Denver lives in
Colorado; (4) if two persons live in the same state, the one earning
a lower salary has a lower tax rate; and (5) it is not possible to
have single tax exemption greater than salary.
Constraints (1), (2), and (3) can be expressed as a key constraint,
an FD, and a CFD, respectively.
(1) : Key{AC, P H}
(2) : ZIP ! ST
(3) : [CT = ‘Denver’] ! [ST = ‘CO’]
Since Constraints (4) and (5) involve order predicates (>, <),
and (5) compares different attributes in the same predicate, they
cannot be expressed by FDs and CFDs. However, they can be expressed in first-order logic.
c4 : 8t↵ , t 2 R, q(t↵ .ST = t .ST ^ t↵ .SAL < t .SAL
^t↵ .T R > t .T R)
c5 : 8t↵ 2 R, q(t↵ .SAL < t↵ .ST X)
Since first-order logic is more expressive, Constraints (1)-(3) can
also be expressed as follows:
c1 : 8t↵ , t 2 R, q(t↵ .AC = t .AC ^ t↵ .P H = t .P H)
c2 : 8t↵ , t 2 R, q(t↵ .ZIP = t .ZIP ^ t↵ .ST 6= t .ST )
c3 : 8t↵ 2 R, q(t↵ .CT = ‘Denver’ ^ t↵ .ST 6= ‘CO’)
The more expressive power an IC language has, the harder it is
to exploit it, for example, in automated data cleaning algorithms,
or in writing SQL queries for consistency checking. There is an
infinite space of business rules up to ad-hoc programs for enforcing
correct application semantics. It is easy to see that a balance should
be achieved between the expressive power of ICs in order to deal
with a broader space of business rules, and at the same time, the
restrictions required to ensure adequate static analysis of ICs and
the development of effective cleaning and discovery algorithms.
Denial Constraints (DCs) [5, 13], a universally quantified first
order logic formalism, can express all constraints in Example 1 as
they are more expressive than FDs and CFDs. To clarify the connection between DCs and the different classes of ICs we show in

1498

TID
t1
t2
t3
t4
t5
t6
t7
t8

FN
Mark
Chunho
Annja
Annie
Anthony
Mark
Ruby
Marcelino

LN
Ballin
Black
Rebizant
Puerta
Landram
Murro
Billinghurst
Nuth

GD
M
M
F
F
M
M
F
F

AC
304
719
636
501
319
970
501
304

PH
232-7667
154-4816
604-2692
378-7304
150-3642
190-3324
154-4816
540-4707

CT
Anthony
Denver
Cyrene
West Crossett
Gifford
Denver
Kremlin
Kyle

ST
WV
CO
MO
AR
IA
CO
AR
WV

ZIP
25813
80290
64739
72045
52404
80251
72045
25813

MS
S
M
M
M
S
S
M
M

CH
Y
N
N
N
Y
Y
Y
N

SAL
5000
60000
40000
85000
15000
60000
70000
10000

TR
3
4.63
6
7.22
2.48
4.63
7
4

STX
2000
0
0
0
40
0
0
0

MTX
0
0
4200
40
0
0
35
0

CTX
2000
0
0
0
40
0
1000
0

Table 1: Tax data records.
Figure 1 a classification based on two criteria: (i) single tuple level
vs table level, and (ii) with constants involved in the constraint vs
with only column variables. DCs are expressive enough to cover
interesting ICs in each quadrant. DCs serve as a great compromise
between expressiveness and complexity for the following reasons:
(1) they are defined on predicates that can be easily expressed in
SQL queries for consistency checking; (2) they have been proven
to be a useful language for data cleaning in many aspects, such as
data repairing [10], consistent query answering [5], and expressing
data currency rules [13]; and (3) while their static analysis turns out
to be undecidable [3], we show that it is possible to develop a set of
sound inference rules and a linear implication testing algorithm for
DCs that enable an efficient adoption of DCs as an IC language, as
we show in this paper.

different attributes and one operator. Given two tuples, we have 2m
distinct cells; and we allow six operators (=, 6=, >, , <, ). Thus
the size of the predicate space P is: |P| = 6 ⇤ 2m ⇤ (2m 1). Any
subset of the predicate space could constitute a DC. Therefore, the
search space for DCs discovery is of size 2|P| .
DCs discovery has a much larger space to explore, further justifying the need for a reasoning mechanism to enable efficient pruning, as well as the need for an efficient discovery algorithm. The
problem is further complicated by allowing constants in the DCs.
(3) Verification. Since the quality of ICs is crucial for data quality, discovered ICs are usually verified by domain experts for their
validity. Model discovery algorithms suffer from the problem of
overfitting [6]; ICs found on the input instance I of schema R may
not hold on future data of R. This happens also for DCs discovery.
E XAMPLE 3. Consider DC c7 on Table 1, which states that
first name determines gender.
c7 : 8t↵ , t 2 R, q(t↵ .F N = t .F N ^ t↵ .GD 6= t .GD)
Even if c7 is true on current data, common knowledge suggests
that it does not hold in general.

Figure 1: The ICs quadrant.
While DCs can be obtained through consultation with domain
experts, it is an expensive process and requires expertise in the constraint language at hand as shown in the experiments. We identified
three challenges that hinder the adoption of DCs as an efficient IC
language and in discovering DCs from an input data instance:
(1) Theoretical Foundation. The necessary theoretical foundations
for DCs as a constraint language are missing [13]. Armstrong Axioms and their extensions are at the core of state-of-the-art algorithms for inferring FDs and CFDs [15, 17], but there is no similar
foundation for the design of tractable DCs discovery algorithms.
E XAMPLE 2. Consider the following constraint, c6 , which
states that there cannot exist two persons who live in the same zip
code and one person has a lower salary and higher tax rate.
c6 : 8t↵ , t 2 R, q(t↵ .ZIP = t .ZIP ^ t↵ .SAL < t .SAL
^t↵ .T R > t .T R)
c6 is implied by c2 and c4 : if two persons live in the same zip
code, by c2 they would live in the same state and by c4 one person
cannot earn less and have higher tax rate in the same state.

Statistical measures have been proposed to rank the constraints
and assist the verification step for specific cases. For CFDs it is
possible to count the number of tuples that match their tableaux [8].
Similar support measures are used for association rules [2].
Unfortunately, discovered DCs are more difficult to verify and
rank than previous formalisms for three reasons: (1) similarly to
FDs, in general it is not possible to just count constants to measure
support; (2) given the explosion of the space, the number of discovered DCs is much larger than the size of discovered FDs; (3)
the semantics of FDs/CFDs is much easier to understand compared
to DCs. A novel and general measure of interestingness for DCs is
therefore needed to rank discovered constraints.
Contributions. Given the DCs discovery problem and the above
challenges, we make the following three contributions:
1. We give the formal problem definition of discovering DCs
(Section 3). We introduce static analysis for DCs with three
sound axioms that serve as the cornerstone for our implication testing algorithm as well as for our DCs discovery algorithm (Section 4).

In order to systematically identify implied DCs (such as c6 ), for
example, to prune redundant DCs, a reasoning system is needed.
(2) Space Explosion. Consider FDs discovery on schema R, let
|R| = m. Taking an attribute as the right hand side of an FD, any
subset of remaining m 1 attributes could serve as the left hand
side. Thus, the space to be explored for FDs discovery is m⇤2m 1 .
Consider discovering DCs involving at most two tuples without
constants; a predicate space needs to be defined, upon which the
space of DCs is defined. The structure of a predicate consists of two

2. We present FASTDC, a DCs discovery algorithm (Section 5).
FASTDC starts by building a predicate space and calculates
evidence sets for it. We establish the connection between
discovering minimal DCs and finding minimal set covers for
evidence sets. We employ depth-first search strategy for finding minimal set covers and use DC axioms for branch pruning. To handle datasets that may have data errors, we extend
FASTDC to discover approximate constraints. Finally, we
further extend it to discover DCs involving constant values.

1499

3. We propose a novel scoring function, the interestingness of a
DC, which combines succinctness and coverage measures of
discovered DCs in order to enable their ranking and pruning
based on thresholds, thus reducing the cognitive burden for
human verification (Section 6).
We experimentally verify our techniques on real-life and synthetic data (Section 7). We show that FASTDC is bound by the
number of tuples |I| and by the number of DCs |⌃|, and that the
polynomial part w.r.t. |I| can be parallelized. We show that the implication test substantially reduces the number of DCs in the output,
thus reducing users’ effort in verifying DCs. We also verify how effective our scoring function is at identifying interesting constraints.

2.

RELATED WORK

Our work finds similarities with several bodies of work: static
analysis of ICs, dependency discovery, and scoring of ICs.
Whenever a dependency language is proposed, the static analysis should be investigated.Static analysis for FDs has been laid out
long ago [1], in which it is shown that static analysis for FDs can
be done in linear time w.r.t. the number of FDs and three inference
rules are proven to be sound and complete. Conditional functional
dependencies were first proposed by Bohannon et al. [7], where implication and consistency problems were shown to be intractable.
In addition, a set of sound and complete inference rules were also
provided, which were later simplified by Fan [14]. Though denial
constraints have been used for data cleaning as well as consistent
query answering [5, 10], static analysis has been done only for special fragments, such as currency rules [13].
In the context of constraints discovery, FDs attracted the most
attention and whose methodologies can be divided into schemadriven and instance-driven approaches. TANE is a representative
for the schema-driven approach [17]. It adopts a level-wise candidate generation and pruning strategy and relies on a linear algorithm for checking the validity of FDs. TANE is sensitive to the
size of the schema. FASTFD is a an instance-driven approach [19],
which first computes agree-sets from data, then adopts a heuristicdriven depth-first search algorithm to search for covers of agreesets. FASTFD is sensitive to the size of the instance. Both algorithms were extended in [15] for discovering CFDs. CFDs discovery is also studied in [8], which not only is able to discover
exact CFDs but also outputs approximate CFDs and dirty values
for approximate CFDs, and in [16], which focuses on generating
a near-optimal tableaux assuming an embedded FD is provided.
The lack of an efficient DCs validity checking algorithm makes the
schema-driven approach for DCs discovery infeasible. Therefore,
we extend FASTFD for DCs discovery.
Another aspect of discovering ICs is to measure the importance
of ICs according to a scoring function. In FDs discovery, Ilyas et
al. examined the statistical correlations for each column pair to discover soft FDs [18]. In CFDs discovery some measures have been
proposed, including support, which is defined as the percentage of
the tuples in the data that match the pattern tableaux, conviction,
and 2 test [8, 15]. Our scoring function identifies two principles
that are widely used in data mining, and combines them into a unified function, which is fundamentally different from previous scoring functions for discovered ICs.

3.

DENIAL CONSTRAINTS AND DISCOVERY PROBLEM

In this section, we first review the syntax and semantics of DCs.
Then, we define minimal DCs and state their discovery problem.

3.1 Denial Constraints (DCs)
Syntax. Consider a database schema of the form S = (U, R, B),
where U is a set of database domains, R is a set of database predicates or relations, and B is a set of finite built-in operators. In this
paper, B = {=, <, >, 6=, , }. B must be negation closed, such
that we could define the inverse of operator as .
We support the subset of integrity constraints identified by denial
constraints (DCs) over relational databases. We introduce a notation for DCs of the form ' : 8t↵ , t , t , . . . 2 R, q(P1 ^. . .^Pm ),
where Pi is of the form v1 v2 or v1 c with v1 , v2 2 tx .A, x 2
{↵, , , . . .}, A 2 R, and c is a constant. For simplicity, we assume there is only one relation R in R.
For a DC ', if 8Pi , i 2 [1, m] is of the form v1 v2 , then we
call such DC variable denial constraint (VDC), otherwise, ' is a
constant denial constraint (CDC).
The inverse of predicate P : v1 1 v2 is P : v1 2 v2 ,with 2 = 1 .
If P is true, then P is false. The set of implied predicates of P is
Imp(P ) = {Q|Q : v1 2 v2 }, where 2 2 Imp( 1 ). If P is true,
then 8Q 2 Imp(P ), Q is true. The inverse and implication of the
six operators in B is summarized in Table 2.

Imp( )

=,

=
6
=

,

6=
=
6
=

>,

>


<
<
, 6=

<, , 6=


>


Table 2: Operator Inverse and Implication.
Semantics. A DC states that all the predicates cannot be true at
the same time, otherwise, we have a violation. Single-tuple constraints (such as check constraints), FDs, and CFDs are special
cases of unary and binary denial constraints with equality and inequality predicates. Given a database instance I of schema S and a
DC ', if I satisfies ', we write I |= ', and we say that ' is a valid
DC. If we have a set of DC ⌃, I |= ⌃ if and only if 8' 2 ⌃, I |= '.
A set of DCs ⌃ implies ', i.e., ⌃ |= ', if for every instance I of
S, if I |= ⌃, then I |= '.
In the context of this paper, we are only interested in DCs with
at most two tuples. DCs involving more tuples are less likely in
real life, and incur bigger predicate space to search as shown in
Section 5. The universal quantifier for DCs with at most two tuples
are 8t↵ , t . We will omit universal quantifiers hereafter.

3.2 Problem Definition

Trivial, Symmetric, and Minimal DC. A DC q(P1 ^ . . . ^ Pn )
is said to be trivial if it is satisfied by any instance. In the sequel,
we only consider nontrivial DCs unless otherwise specified. The
symmetric DC of a DC '1 is a DC '2 by substituting t↵ with t ,
and t with t↵ . If '1 and '2 are symmetric, then '1 |= '2 and
'2 |= '1 . A DC '1 is set-minimal, or minimal, if there does not
exist '2 , s.t. I |= '1 , I |= '2 , and '2 .P res ⇢ '1 .P res. We use
'.P res to denote the set of predicates in DC '.
E XAMPLE 4. Consider three additional DCs for Table 1.
c8 :q(t↵ .SAL = t .SAL ^ t↵ .SAL > t .SAL)
c9 :q(t↵ .P H = t .P H)
c10 :q(t↵ .ST = t .ST ^ t↵ .SAL > t .SAL ^ t↵ .T R < t .T R)
c8 is a trivial DC, since there cannot exist two persons that have
the same salary, and one’s salary is greater than the other. If we
remove tuple t7 in Table 1, c9 becomes a valid DC, making c1 no
longer minimal. c10 and c4 are symmetric DCs.
Problem Statement. Given a relational schema R and an instance I, the discovery problem for DCs is to find all valid minimal
DCs that hold on I. Since the number of DCs that hold on a dataset

1500

is usually very big, we also study the problem of ranking DCs with
an objective function described in Section 6.

4.

STATIC ANALYSIS OF DCS

Since DCs subsume FDs and CFDs, it is natural to ask whether
we can perform reasoning the same way. An inference system for
DCs enables pruning in a discovery algorithm. Similarly, an implication test is required to reduce the number of DCs in the output.

4.1

Inference System

Armstrong Axioms are the fundamental building blocks for implication analysis for FDs [1]. We present three symbolic inference
rules for DCs, denoted as I, analogous to such Axioms.
Triviality: 8Pi , Pj , if Pi 2 Imp(Pj ), then q(Pi ^ Pj ) is a trivial
DC.
Augmentation: If q(P1 ^ . . . ^ Pn ) is a valid DC, then q(P1 ^
. . . ^ Pn ^ Q) is also a valid DC.
Transitivity: If q(P1 ^ . . . ^ Pn ^ Q1 ) and q(R1 ^ . . . ^ Rm ^ Q2 )
are valid DCs, and Q2 2 Imp(Q1 ), then q(P1 ^ . . . ^ Pn ^ R1 ^
. . . ^ Rm ) is also a valid DC.
Triviality states that, if a DC has two predicates that cannot be
true at the same time (Pi 2 Imp(Pj )), then the DC is trivially
satisfied. Augmentation states that, if a DC is valid, adding more
predicates will always result in a valid DC. Transitivity states, that
if there are two DCs and two predicates (one in each DC) that cannot be false at the same time (Q2 2 Imp(Q1 )), then merging two
DCs plus removing those two predicates will result in a valid DC.
Inference system I is a syntactic way of checking whether a set
of DCs ⌃ implies a DC '. It is sound in that if by using I a DC '
can be derived from ⌃, i.e., ⌃ `I ', then ⌃ implies ', i.e., ⌃ |=
'. The completeness of I dictates that if ⌃ |= ', then ⌃ `I '.
We identify a specific form of DCs, for which I is complete. The
specific form requires that each predicate of a DC is defined on two
tuples and on the same attribute, and that all predicates must have
the same operator ✓ except one that must have the reverse of ✓.
T HEOREM 1. The inference system I is sound. It is also complete for VDCs of the form 8t↵ , t 2 R, q(P1 ^ . . . ^ Pm ^ Q),
where Pi = t↵ .Ai ✓t .Ai , 8i 2 [1, m] and Q = t↵ .B✓t .B with
Ai , B 2 U.
Formal proof for Theorem 1 is reported in the extended version
of this paper [9]. The completeness result of I for that form of
DCs generalizes the completeness result of Armstrong Axioms for
FDs. In particular, FDs adhere to the form with ✓ being =. The
partial completeness result for the inference system has no implication on the completeness of the discovery algorithms described
in Section 5. We will discuss in the experiments how, although not
complete, the inference system I has a huge impact on the pruning
power of the implication test and on the FASTDC algorithm.

4.2

Implication Problem

Implication testing refers to the problem of determining whether
a set of DCs ⌃ implies another DC '. It has been established that
the complexity of the implication testing problem for DCs is coNPComplete [3]. Given the intractability result, we have devised a
linear, sound, but not complete, algorithm for implication testing to
reduce the number of DCs in the discovery algorithm output.
In order to devise an efficient implication testing algorithm, we
define the concept of closure in Definition 1 for a set of predicates
W under a set of DCs ⌃. A predicate P is in the closure if adding
P to W would constitute a DC implied by ⌃. It is in spirit similar
to the closure of a set of attributes under a set of FDs.

D EFINITION 1. The closure of a set of predicates W, w.r.t. a
set of DCs ⌃, is a set of predicates, denoted as Clo⌃ (W), such that
8P 2 Clo⌃ (W), ⌃ |=q(W ^ P ).
Algorithm 1 G ET PARTIAL C LOSURE :
Input: Set of DCs ⌃, Set of Predicates W
Output: Set of predicates called closure of W under ⌃ : Clo⌃ (W)
1: for all P 2 W do
2: Clo⌃ (W) Clo⌃ (W) + Imp(P )
3: Clo⌃ (W) Clo⌃ (W) + Imp(Clo⌃ (W))
4: for each P , create a list LP of DCs containing P
5: for each ', create a list L' of predicates not yet in the closure
6: for all ' 2 ⌃ do
7: for all P 2 '.P res do
8:
LP
LP + '
9: for all P 2
/ Clo⌃ (W) do
10: for all ' 2 LP do
11:
L'
L' + P
12: create a queue J of DC with all but one predicate in the closure
13: for all ' 2 ⌃ do
14: if |L' | = 1 then
15:
J
J +'
16: while |J| > 0 do
17: ' J.pop()
18: P
L' .pop()
19: for all Q 2 Imp(P ) do
20:
for all ' 2 LQ do
21:
L'
L' Q
22:
if |L' | = 1 then
23:
J
J +'
24: Clo⌃ (W) Clo⌃ (W) + Imp(P )
25: Clo⌃ (W) Clo⌃ (W) + Imp(Clo⌃ (W))
26: return Clo⌃ (W)

Algorithm 1 calculates the partial closure of W under ⌃, whose
proof of correctness is provided in [9]. We initialize Clo⌃ (W) by
adding every predicate in W and their implied predicates due to
Axiom Triviality (Line 1-2). We add additional predicates that are
implied by Clo⌃ (W) through basic algebraic transitivity (Line 3).
The closure is enlarged if there exists a DC ' in ⌃ such that all but
one predicates in ' are in the closure (Line 15-23). We use two lists
to keep track of exactly when such condition is met (Line 3-11).
E XAMPLE 5. Consider ⌃={c1 , . . . , c5 } and W = {t↵ .ZIP =
t .ZIP, t↵ .SAL < t .SAL}.
The initialization step in Line(1-3) results in Clo⌃ (W) =
{t↵ .ZIP = t .ZIP, t↵ .SAL < t .SAL, t↵ .SAL  t .SAL}.
As all predicates but t↵ .ST 6= t .ST of c2 are in the closure, we add the implied predicates of the reverse of t↵ .ST 6=
t .ST to it and Clo⌃ (W) = {t↵ .ZIP = t .ZIP, t↵ .SAL <
t .SAL, t↵ .SAL  t .SAL, t↵ .ST = t .ST }. As all predicates but t↵ .T R > t .T R of c4 are in the closure (Line 22), we
add the implied predicates of its reverse, Clo⌃ (W) = {t↵ .ZIP =
t .ZIP, t↵ .SAL < t .SAL, t↵ .SAL  t .SAL, t↵ .T R 
t .T R}. No more DCs are in the queue (Line 16).
Since t↵ .T R  t .T R 2 Clo⌃ (W), we have ⌃ |=q(W ^
t↵ .T R > t .T R), i.e., ⌃ |= c6 .

Algorithm 2 tests whether a DC ' is implied by a set of DCs
⌃, by computing the closure of '.P res in ' under , which is ⌃
enlarged with symmetric DCs. If there exists a DC in , whose
predicates are a subset of the closure, ' is implied by ⌃. The proof
of soundness of Algorithm 2 is in [9], which also shows a counterexample where ' is implied by ⌃, but Algorithm 2 fails.
E XAMPLE 6. Consider a database with two numerical
columns, High (H) and Low (L). Consider two DCs c11 , c12 .

1501

Algorithm 2 I MPLICATION T ESTING

building EviI . Section 5.3 describes a search procedure for finding minimal covers for EviI . In order to reduce the execution time,
the search is optimized with a dynamic ordering of predicates and
branch pruning based on the axioms we developed in Section 4. In
order to enable further pruning, Section 5.4 introduces an optimization technique that divides the space of DCs and performs DFS on
each subspace. We extend FASTDC in Section 5.5 to discover approximate DCs and in Section 5.6 to discover DCs with constants.

Input: Set of DCs ⌃, one DC '
Output: A boolean value, indicating whether ⌃ |= '
1: if ' is a trivial DC then
2: return true
3:
⌃
4: for 2 ⌃ do
5:
+ symmetric DC of
6: Clo ('.P res) = getClosure('.P res, )
7: if 9 2 , s.t. .P res ✓ Clo ('.P res) then
8: return true

5.1 Building the Predicate Space

c11 : 8t↵ , (t↵ .H < t↵ .L)
c12 : 8t↵ , t , (t↵ .H > t .H ^ t .L > t↵ .H)
Algorithm 2 identifies that c11 implies c12 . Let ⌃ = {c11 } and
W = c12 .P res. = {c11 , c13 }, where c13 : 8t , (t .H < t .L).
Clo (W) = {t↵ .H > t .H, t .L > t↵ .H, t .H < t .L}, because t .H < t .L is implied by {t↵ .H > t .H, t .L > t↵ .H}
through basic algebraic transitivity (Line 3).
Since c13 .P res ⇢ Clo (W), the implication holds.

5.

DCS DISCOVERY ALGORITHM

Algorithm 3 describes our procedure for discovering minimal
DCs. Since a DC is composed of a set of predicates, we build a
predicate space P based on schema R (Line 1). Any subset of P
could be a set of predicates for a DC.

Given a database schema R and an instance I, we build a predicate space P from which DCs can be formed. For each attribute
in the schema, we add two equality predicates (=, 6=) between two
tuples on it. In the same way, for each numerical attribute, we add
order predicates (>, , <, ). For every pair of attributes in R,
they are joinable (comparable) if equality (order) predicates hold
across them, and add cross column predicates accordingly.
Profiling algorithms [11] can be used to detect joinable and comparable columns. We consider two columns joinable if they are of
same type and have common values2 . Two columns are comparable if they are both of numerical types and the arithmetic means of
two columns are within the same order of magnitude.
E XAMPLE 7. Consider the following Employee table with
three attributes: Employee ID (I), Manager ID (M), and Salary(S).
TID
t9
t10
t11

Algorithm 3 FASTDC
Input: One relational instance I, schema R
Output: All minimal DCs ⌃
1: P B UILD PREDICATE SPACE(I, R)
2: EviI
B UILD EVIDENCE SET(I, P)
3: MC S EARCH MINIMAL COVERS(EviI , EviI , ;, >init , ;)
4: for all X 2 MC do
5: ⌃ ⌃+q(X)
6: for all ' 2 ⌃ do
7: if ⌃ ' |= ' then
8:
remove ' from ⌃

Given P, the space of candidate DCs is of size 2 . It is not
feasible to validate each candidate DC directly over I, due to the
quadratic complexity of checking all tuple pairs. For this reason,
we extract evidence from I in a way that enables the reduction of
DCs discovery to a search problem that computes valid minimal
DCs without checking each candidate DC individually.
The evidence is composed of sets of satisfied predicates in P,
one set for every pair of tuples (Line 2). For example, assume
two satisfied predicates for one tuple pair: t↵ .A = t .A and
t↵ .B = t .B. We use the set of satisfied predicates to derive the
valid DCs that do not violate this tuple pair. In the example, two
sample DCs that hold on that tuple pair are q(t↵ .A 6= t .A) and
q(t↵ .A = t .A ^ t↵ .B 6= t .B). Let EviI be the sets of satisfied
predicates for all pairs of tuples, deriving valid minimal DCs for
I corresponds to finding the minimal sets of predicates that cover
EviI (Line 3)1 . For each minimal cover X, we derive a valid minimal DC by inverting each predicate in it (Lines 4-5). We remove
implied DCs from ⌃ with Algorithm 2 (Lines 6-8).
Section 5.1 describes the procedure for building the predicate
space P. Section 5.2 formally defines EviI , gives a theorem that
reduces the problem of discovering all minimal DCs to the problem
of finding all minimal covers for EviI , and presents a procedure for
|P|

1

For sake of presentation, parameters are described in Section 5.3

I(String)
A1
A2
A3

M(String)
A1
A1
A1

S(Double)
50
40
40

We build the following predicate space P for it.
P1 : t↵ .I = t .I
P5 : t↵ .S = t .S
P9 : t↵ .S < t .S
P2 : t↵ .I 6= t .I
P6 : t↵ .S 6= t .S
P10 : t↵ .S t .S
P3 : t↵ .M = t .M P7 : t↵ .S > t .S
P11 : t↵ .I = t↵ .M
P4 : t↵ .M 6= t .M P8 : t↵ .S  t .S
P12 : t↵ .I 6= t↵ .M
P13 : t↵ .I = t .M P14 : t↵ .I 6= t .M

5.2 Evidence Set
Before giving formal definitions of EviI , we show an example
of the satisfied predicates for the Employee table Emp above:
EviEmp = {{P2 , P3 , P5 , P8 , P10 , P12 , P14 },
{P2 , P3 , P6 , P8 , P9 , P12 , P14 }, {P2 , P3 , P6 , P7 , P10 , P11 , P13 }}.
Every element in EviEmp has at least one pair of tuples in I such
that every predicate in it is satisfied by that pair of tuples.
D EFINITION 2. Given a pair of tuple htx , ty i 2 I, the satisfied predicate set for htx , ty i is SAT (htx , ty i) = {P |P 2
P, htx , ty i |= P }, where P is the predicate space, and htx , ty i |= P
means htx , ty i satisfies P .
The evidence set of I is EviI = {SAT (htx , ty i)|8htx , ty i 2 I}.
A set of predicates X ✓ P is a minimal set cover for EviI if
8E 2 EviI , X \ E 6= ;, and @Y ⇢ X, s.t. 8E 2 EviI , Y \ E 6= ;.
The minimal set cover for EviI is a set of predicates that intersect with every element in EviI . Theorem 2 transforms the problem of minimal DCs discovery into the problem of searching for
minimal set covers for EviI .
T HEOREM 2. q(X1 ^ . . . ^ Xn ) is a valid minimal DC if and
only if X = {X1 , . . . , Xn } is a minimal set cover for EviI .
2
We show in the experiments that requiring at least 30% common
values allows to identify joinable columns without introducing a
large number of unuseful predicates. Joinable columns can also be
discovered from query logs, if available.

1502

Proof. Step 1: we prove if X ✓ P is a cover for EviI , q(X1 ^ . . . ^
Xn ) is a valid DC. According to the definition, EviI represents all
the pieces of evidence that might violate DCs. For any E 2 EviI ,
there exists X 2 X, s.t. X 2 E; thus X 2
/ E. I.e., the presence of
X in q(X1 ^ . . . ^ Xn ) disqualifies E as a possible violation.
Step 2: we prove if q(X1 ^. . .^Xn ) is a valid DC, then X ✓ P is
a cover. According to the definition of valid DC, there does not exist tuple pair htx , ty i, s.t. htx , ty i satisfies X1 , . . . , Xn simultaneously. In other words, 8htx , ty i, 9Xi , s.t. htx , ty i does not satisfy
Xi . Therefore, 8htx , ty i, 9Xi , s.t. htx , ty i |= Xi , which means
any tuple pair’s satisfied predicate set is covered by {X1 , . . . , Xn }.
Step 3: if X ✓ P is a minimal cover, then the DC is also minimal.
Assume the DC is not minimal, there exists another DC ' whose
predicates are a subset of q(X1 ^ . . . ^ Xn ). According to Step
2, '.P res is a cover, which is a subset of X = {X1 , . . . , Xn }. It
contradicts with the assumption that X ✓ P is a minimal cover.
Step 4: if the DC is minimal, then the corresponding cover is
also minimal. The proof is similar to Step 3.
⇧
E XAMPLE 8. Consider EviEmp for the table in Example 7.
X1 = {P2 } is a minimal cover, thus q(P2 ), i.e., q(t↵ .I = t .I)
is a valid DC, which states I is a key.
X2 = {P10 , P14 } is another minimal cover, thus q(P10 ^ P14 ),
i.e., q(t↵ .S < t .S ^ t↵ .I = t .M ) is another valid DC, which
states that a manager’s salary cannot be less than her employee’s.
The procedure to compute EviI follows directly from the definition: for every tuple pair in I, we compute the set of predicates
that tuple pair satisfies, and we add that set into EviI . This operation is sensitive to the size of the database, with a complexity of
O(|P| ⇥ |I|2 ). However, for every tuple pair, computing the satisfied set of predicates is independent of each other. In our implementation we use the Grid Scheme strategy, a standard approach to
scale in entity resolution [4]. We partition the data into B blocks,
and define each task as a comparison of tuples from two blocks.
2
The total number of tasks is B2 . Suppose we have M machines,
we need to distribute the tasks evenly to M machines so as to fully
2
utilize every machine, i.e., we need to ensure B2 = w ⇥ M with
w the numberpof tasks for each machine. Therefore, the number of
blocks B = 2wM . In addition, as we need at least two blocks
in memory at any given time, we need to make sure that (2 ⇥ |I|
⇥
B
Size of a Tuple) < Memory Limit.

5.3

DFS for Minimal Covers

Algorithm 4 presents the depth-first search (DFS) procedure for
minimal covers for EviI . Ignore Lines (9-10) and Lines (11-12)
for now, as they are described in Section 5.4 and in Section 6.3,
respectively. We denote by Evicurr the set of elements in EviI not
covered so far. Initially Evicurr = EviI . Whenever a predicate P
is added to the cover, we remove from Evicurr the elements that
contain P , i.e., Evinext = {E|E 2 Ecurr ^ P 2
/ E} (Line 23).
There are two base cases to terminate the search:
(i) there are no more candidate predicates to include in the cover,
but Evicurr 6= ; (Lines 14-15); and
(ii) Evicurr = ; and the current path is a cover (Line 16). If the
cover is minimal, we add it to the result MC (Lines 17-19).
We speed up the search procedure by two optimizations: dynamic ordering of predicates as we descend down the search tree
and branching pruning based on the axioms in Section 4.
Opt1: Dynamic Ordering. Instead of fixing the order of
predicates when descending down the tree, we dynamically order the remaining candidate predicates, denoted as >next , based
on the number of remaining evidence set they cover (Lines 23

Algorithm 4 S EARCH M INIMAL C OVERS
Input: 1. Input Evidence set, EviI
2. Evidence set not covered so far, Evicurr
3. The current path in the search tree, X ✓ P
4. The current partial ordering of the predicates, >curr
5. The DCs discovered so far, ⌃
Output: A set of minimal covers for Evi, denoted as MC
1: Branch Pruning
2: P
X.last // Last Predicate added into the path
3: if 9Q 2 X P , s.t. P 2 Imp(Q) then
4: return //Triviality pruning
5: if 9Y 2 MC, s.t. X ◆ Y then
6: return //Subset pruning based on MC
7: if 9Y = {Y1 , . . . , Yn } 2 MC, and 9i 2 [1, n],
and 9Q 2 Imp(Yi ), s.t. Z = Y i [ Q and X ◆ Z then
8: return //Transitive pruning based on MC
9: if 9' 2 ⌃, s.t. X ◆ '.P res then
10: return //Subset pruning based on previous discovered DCs
11: if Inter(') < t, 8' of the form q(X ^ W) then
12: return //Pruning based on Inter score
13: Base cases
14: if >curr = ; and Evicurr 6= ; then
15: return //No DCs in this branch
16: if Evicurr = ; then
17: if no subset of size |X| 1 covers Evicurr then
18:
MC
MC + X
19: return //Got a cover
20: Recursive cases
21: for all Predicate P 2>curr do
22: X X + P
23: Evinext evidence sets in Evicurr not yet covered by P
24: >next total ordering of {P 0 |P >curr P 0 } wrt Evinext
25: S EARCH M INIMAL C OVERS(EviI , Evinext , X, >next , ⌃)
26: X X P

-24). Formally, we define the cover of P w.r.t. Evinext as
Cov(P, Evinext ) = |{P 2 E|E 2 Evinext }|. And we say
that P >next Q if Cov(P, Evinext ) > Cov(Q, Evinext ), or
Cov(P, Evinext ) = Cov(Q, Evinext ) and P appears before Q
in the preassigned order in the predicate space. The initial evidence
set EviI is computed as discussed in Section 5.2. To computer
Evinext (Line 21), we scan every element in Evicurr , and we add
in Evinext those elements that do not contain P .
E XAMPLE 9. Consider EviEmp for the table in Example 7.
We compute the cover for each predicate,
such as Cov(P2 , EviEmp ) = 3, Cov(P8 , EviEmp ) = 2,
Cov(P9 , EviEmp ) = 1, etc. The initial ordering for the predicates
according to EviEmp is >init = P2 > P3 > P6 > P8 > P10 >
P12 > P14 > P5 > P7 > P9 > P11 > P13 .
Opt2: Branch Pruning. The purpose of performing dynamic
ordering of candidate predicates is to get covers as early as possible
so that those covers can be used to prune unnecessary branches of
the search tree. We list three pruning strategies.
(i) Lines(2-4) describe the first pruning strategy. This branch
would eventually result in a DC of the form ' :q(X P ^ P ^ W),
where P is the most recent predicate added to this branch and W
other predicates if we traverse this branch. If 9Q 2 X P , s.t.
P 2 Imp(Q), then ' is trivial according to Axiom Triviality.
(ii) Lines(5-6) describe the second branch pruning strategy,
which is based on MC. If Y is in the cover, then q(Y) is a valid
DC. Any branch containing X would result in a DC of the form
q(X ^ W), which is implied by q(Y) based on Axiom Augmentation, since Y ✓ X.
(iii) Lines(7-8) describe the third branching pruning strategy,
which is also based on MC. If Y is in the cover, then q(Y i ^ Yi ) is

1503

a valid DC. Any branch containing X ◆ Y i [ Q would result in a
DC of the form q(Y i ^Q^W). Since Q 2 Imp(Yi ), by applying
Axiom Transitive on these two DCs, we would get that q(Y i ^W)
is also a valid DC, which would imply q(Y i ^ Q ^ W) based on
Axiom Augmentation. Thus this branch can be pruned.

5.4

Dividing the Space of DCs

Instead of searching for all minimal DCs at once, we divide the
space into subspaces, based on whether a DC contains a specific
predicate P1 , which can be further divided according to whether a
DC contains another specific predicate P2 . We start by defining evidence set modulo a predicate P , i.e., EviP
I , and we give a theorem
that reduces the problem of discovering all minimal DCs to the one
of finding all minimal set covers of EviP
I for each P 2 P.
D EFINITION 3. Given a P 2 P, the evidence set of I modulo
P is, EviP
{P }|E 2 EviI , P 2 E}.
I = {E
T HEOREM 3. q(X1 ^ . . . ^ Xn ^ P ) is a valid minimal DC,
that contains predicate P , if and only if X = {X1 , . . . , Xn } is a
minimal set cover for EviP
I .
E XAMPLE 10. Consider EviEmp for the table in Example 7,
P13
1
EviP
Emp = ;, EviEmp = {{P2 , P3 , P6 , P7 , P10 , P11 }}. Thus
q(P1 ) is a valid DC because there is nothing in the cover for
1
EviP
Emp , and q(P13 ^ P10 ) is a valid DC as {P10 } is a cover for
P13
EviEmp . It is evident that EviP
Emp is much smaller than EviEmp .
However, care must be taken before we start to search for minimal covers for EviP
I due to the following two problems.
First, a minimal DC containing a certain predicate P is not necessarily a global minimal DC. For instance, assume that q(P, Q) is
a minimal DC containing P because {Q} is a minimal cover for
EviP
I . However, it might not be a minimal DC because it is possible that q(Q), which is actually smaller than q(P, Q), is also a valid
DC. We call such q(P, Q) a local minimal DC w.r.t. P , and q(Q)
a global minimal DC, or a minimal DC. It is obvious that a global
minimal DC is always a local minimal DC w.r.t. each predicate in
the DC. Our goal is to generate all globally minimal DCs.
Second, assume that q(P, Q) is a global minimal DC. It is an
local minimal DC w.r.t. P and Q, thus would appear in subspaces
Q
EviP
I and EviI . In fact, a minimal DC ' would then appear in
|'.P res| subspaces, causing a large amount of repeated work.
DCs
+R1

R1
+R2

E XAMPLE 11. Consider EviEmp for the table in Example 7,
P4
1
since EviP
Emp = ; and EviEmp = ;, then Q = {P1 , P2 , P3 , P4 }.
Thus we perform |P| |Q| = 10 searches instead of |P| = 14.
2. Additional Branch Pruning. Since we perform DFS according to the taxonomy tree in a bottom-up fashion, DCs discovered
from previous searches are used to prune branches in current DFS
described by Lines(9-10) of Algorithm 4.
Since Algorithm 4 is an exhaustive search for all minimal covers
for EviI , Algorithm 3 produces all minimal DCs.
T HEOREM 4. Algorithm 3 produces all non-trivial minimal
DCs holding on input database I.
Complexity Analysis of FASTDC. The initialization of evidence sets takes O(|P| ⇤ n2 ). The time for each DFS search to
find all minimal covers for EviP
I is O((1 + wP ) ⇤ KP ), with
wP being the extra effort due to imperfect search of EviP
I , and
KP being the number of minimal DCs containing predicate P .
Altogether, our FASTDC algorithm has worst time complexity of
O(|P| ⇤ n2 + |P| ⇤ (1 + wP ) ⇤ KP ).

5.5 Approximate DCs: A-FASTDC

R2
+R3

DCs not containing R1 , which can be divided based on containing
R2 or not, i.e., +R2 and R2 . We will divide R2 as in Figure 2.
We can enforce searching for DCs not containing Ri by disallowing Ri in the initial ordering of candidate predicates for minimal
cover. Since this is a taxonomy of all DCs, no minimal DCs can be
generated more than once.
We solve the first problem by performing DFS according to the
taxonomy tree in a bottom-up fashion. We start by search for DCs
containing R3 , not containing R1 , R2 . Then we search for DCs,
containing R2 , not containing R1 , and we verify the resulting DC
is global minimal by checking if the reverse of the minimal cover
3
is a super set of DCs discovered from EviR
I . The process goes
on until we reach the root of the taxonomy, thus ensuring that the
results are both globally minimal and complete.
Dividing the space enables more optimization opportunities:
1. Reduction of Number of Searches. If 9P 2 P, such that
Q
EviP
I = ;, we identify two scenarios for Q, where DFS for EviI
can be eliminated.
(i) 8Q 2 Imp(P ), if EviP
I = ;, then q(P ) is a valid DC. The
search for EviQ
I would result in a DC of the form q(Q^W), where
W represents any other set of predicates. Since Q 2 Imp(P ),
applying Axiom Transitivity, we would have that q(W) is a valid
DC, which implies q(Q ^ W) based on Axiom Augmentation.
(ii) 8Q 2 Imp(P ), since Q 2 Imp(P ), then Q |= P . It
follows that Q ^ W |= P and therefore q(P ) |=q(Q ^ W) holds.

R3

Figure 2: Taxonomy Tree.
We solve the second problem first, then the solution for the first
problem comes naturally. We divide the DCs space and order all
searches in a way, such that we ensure the output of a locally minimal DC is indeed global minimal, and a previously generated minimal DC will never appear again in latter searches. Consider a predicate space P that has only 3 predicates R1 to R3 as in Figure 2,
which presents a taxonomy of all DCs. In the first level, all DCs can
be divided into DCs containing R1 , denoted as +R1 , and DCs not
containing R1 , denoted as R1 . Since we know how to search for
local minimal DCs containing R1 , we only need to further process

Algorithm FASTDC consumes the whole input data set and requires no violations for a DC to be declared valid. In real scenarios,
there are multiple reasons why this request may need to be relaxed:
(1) overfitting: data is dynamic and as more data becomes available, overfitting constraints on current data set can be problematic;
(2) data errors: while in general learning from unclean data is a
challenge, the common belief is that errors constitute small percentage of data, thus discovering constraints that hold for most of
the dataset is a common workaround [8, 15, 17].
We therefore modify the discovery statement as follows: given a
relational schema R and instance I, the approximate DCs discovery
problem for DCs is to find all valid DCs, where a DC ' is valid if
the percentage of violations of ' on I, i.e., number of violations of
' on I divided by total number of tuple pairs |I|(|I| 1), is within
threshold ✏. For this new problem, we introduce A-FASTDC.

1504

Different tuple pairs might have the same satisfied predicate set.
For every element E in EviI , we denote by count(E) the number of tuple pairs htx , ty i such that E = SAT (htx , ty i). For
example, count({P2 , P3 , P6 , P8 , P9 , P12 , P14 }) = 2 for the table in Example 7 since SAT (ht10 , t9 i) = SAT (ht11 , t9 i) =
{P2 , P3 , P6 , P8 , P9 , P12 , P14 }.
D EFINITION 4. A set of predicates X ✓ P is an ✏-minimal
cover for EviI if Sum(count(E))  ✏|I|(|I| 1), where E 2
EviI , X \ E = ;, and no subset of X has such property.

Theorem 5 transforms approximate DCs discovery problem into
the problem of searching for ✏-minimal covers for EviI .
T HEOREM 5. q(X1 ^. . .^Xn ) is a valid approximate minimal
DC if and only if X={X1 , . . . , Xn } is a ✏-minimal cover for EviI .

There are two modifications for Algorithm 4 to search for ✏minimal covers for EviI : (1)
Pthe dynamic ordering of predicates is
based on Cov(P, Evi) = E2{E2Evi,P 2E} count(E); and (2)
the base cases (Lines 12-17) are either when the number of violations of the corresponding DC drops below ✏|I|(|I| 1), or the
number of violation is still above ✏|I|(|I| 1) but there are no
more candidate predicates to include. Due to space limitations, we
present in [9] the detailed modifications for Algorithm 4 to search
for ✏-minimal covers for EviI .

5.6

Constant DCs: C-FASTDC

FASTDC discovers DCs without constant predicates. However,
just like FDs may not hold on the entire dataset, thus CFDs are
more useful, we are also interested in discovering constant DCs
(CDCs). Algorithm 5 describes the procedure for CDCs discovery.
The first step is to build a constant predicate space Q (Lines 1-6)3 .
After that, one direct way to discover CDCs is to include Q in the
predicate space P, and follow the same procedure in Section 5.3.
However, the number of constant predicates is linear w.r.t. the number of constants in the active domain, which is usually very large.
Therefore, we follow the approach of [15] and focus on discovering ⌧ -frequent CDCs. The support for a set of constant predicates
X on I, denoted by sup(X, I), is defined to be the set of tuples
that satisfy all constant predicates in X. A set of predicates is said
to be ⌧ -frequent if |sup(X,I)|
⌧ . A CDC ' consisting of only
|I|
constant predicates is said to be ⌧ -frequent if all strict subsets of
'.P res are ⌧ -frequent. A CDC ' consisting of constant and variable predicates is said to be k-frequent if all subsets of '’s constant
predicates are ⌧ -frequent.
E XAMPLE 12. Consider c3 in Example 1, sup({t↵ .CT =
‘Denver’}, I) = {t2 , t6 }, sup({t↵ .ST 6= ‘CO’}, I) =
{t1 , t3 , t4 , t5 , t7 , t8 }, and sup({c3.P res}, I) = ;. Therefore, c3
is a ⌧ -frequent CDC, with 28
⌧.
We follow an “Apriori” approach to discover ⌧ -frequent constant
predicate sets. We first identify frequent constant predicate sets of
length L1 from Q (Lines 7-15). We then generate candidate frequent constant predicate sets of length m from length m 1 (Lines
22-28), and we scan the database I to get their support (Line 24). If
the support of the candidate c is 0, we have a valid CDC with only
constant predicates (Lines 12-13 and 25-26); if the support of the
candidate c is greater than ⌧ , we call FASTDC to get the variable
DCs (VDCs) that hold on sup(c, I), and we construct CDCs by
combining the ⌧ -frequent constant predicate sets and the variable
predicates of VDCs (Lines 18-21).

Algorithm 5 C-FASTDC
Input: Instance I, schema R, minimal frequency requirement ⌧
Output: Constant DCs
1: Let Q ; be the constant predicate space
2: for all A 2 R do
3: for all c 2 ADom(A) do
4:
Q
Q + t↵ .A✓c, where ✓ 2 {=, 6=}
5:
if A is numerical type then
6:
Q
Q + t↵ .A✓c, where ✓ 2 {>, , <, }
7: for all t 2 I do
8: if t satisfies Q then
9:
sup(Q, I)
sup(Q, I) + t
10: Let L1 be the set of frequent predicates
11: for all Q 2 Q do
12: if |sup(Q, I)| = 0 then
13:
+q(Q)
14: else if |sup(Q,I)|
⌧ then
|I|
15:
L1
L1 + {Q}
16: m 2
17: while Lm 1 6= ; do
18: for all c 2 Lm 1 do
19:
⌃
FASTDC(sup(c, I),R)
20:
for all ' 2 ⌃ do
21:
+ , ’s predicates comes from
S c and '
22: Cm = {c|c = a [ b ^ a 2 Lm 1 ^ b 2 Lk 1 ^ b 2
/ a}
23: for all c 2 Cm do
24:
scan the database to get the support of c, sup(c, I)
25:
if |sup(c, I)| = 0 then
26:
+ , ’s predicates consist of predicates in c
|sup(c,I)|
27:
else if
⌧ then
|I|
28:
Lm
Lm + c
29: m m + 1

6.

RANKING DCS

Though our FASTDC (C-FASTDC) is able to prune trivial, nonminimal, and implied DCs, the number of DCs returned can still be
too large. To tackle this problem, we propose a scoring function to
rank DCs based on their size and their support from the data. Given
a DC ', we denote by Inter(') its interestingness score.
We recognize two different dimensions that influence Inter('):
succinctness and coverage of ', which are both defined on a scale
between 0 and 1. Each of the two scores represents a different yet
important intuitive dimension to rank discovered DCs.
Succinctness is motivated by the Occam’s razor principle. This
principle suggests that among competing hypotheses, the one that
makes fewer assumptions is preferred. It is also recognized that
overfitting occurs when a model is excessively complex [6].
Coverage is also a general principle in data mining to rank results [2]. They design scoring functions that measure the statistical
significance of the mining targets in the input data.
Given a DC ', we define the interestingness score as a linear
weighted combination of the two dimensions: Inter(') = a ⇥
Coverage(') + (1 a) ⇥ Succ(').

6.1 Succinctness
Minimum description length (MDL), which measures the code
length needed to compress the data [6], is a formalism to realize
the Occam’s razor principle. Inspired by MDL, we measure the
length of a DC Len('), and we define the succinctness of a DC ',
i.e., Succ('), as the minimal possible length of a DC divided by
Len(') thus ensuring the scale of Succ(') is between 0 and 1.

3
We focus on two tuple CDCs with the same constant predicates
on each tuple, i.e., if t↵ .A✓c is present in a two tuple CDC, t .A✓c
is enforced by the algorithm. Therefore, we only add t↵ .A✓c to Q.

Succ(') =

1505

Min({Len( )|8 })
Len(')

One simple heuristic for Len(') is to use the number of predicates in ', i.e., |'.P res|. Our proposed function computes the
length of a DC with a finer granularity than a simple counting of
the predicates. To compute it, we identify the alphabet from which
DCs are formed as A = {t↵ , t , U, B, Cons}, where U is the set of
all attributes, B is the set of all operators, and Cons are constants.
The length of ' is the number of symbols in A that appear in ':
Len(') = |{a|a 2 A, a 2 '}|. The shortest possible DC is of
length 4, such as c5 , c9 , and q(t↵ .SAL  5000).
E XAMPLE 13. Consider a database schema R with two
columns A, B, with 3 DCs as follows:
c14 :q(t↵ .A = t .A), c15 :q(t↵ .A = t .B),
c16 :q(t↵ .A = t .A ^ t↵ .B 6= t .B)
Len(c14 ) = 4 < Len(c15 ) = 5 < Len(c16 ) = 6. Succ(c14 ) = 1,
Succ(c15 ) = 0.8, and Succ(c16 )=0.67. However, if we use |'.P res|
as Len('), Len(c14 ) = 1 < Len(c15 ) = 1 < Len(c16 ) = 2, and
Succ(c14 )=1, Succ(c15 )=1, and Succ(c16 )=0.5.

6.2

Coverage

Frequent itemset mining recognizes the importance of measuring
statistical significance of the mining targets [2]. In this case, the
support of an itemset is defined as the proportion of transactions in
the data that contain the itemset. Only if the support of an itemset is
above a threshold, it is considered to be frequent. CFDs discovery
also adopts such principle. A CFD is considered to be interesting
only if their support in the data is above a certain threshold, where
support is in general defined as the percentage of single tuples that
match the constants in the patten tableaux of the CFDs [8, 15].
However, the above statistical significance measures requires the
presence of constants in the mining targets. For example, the frequent itemsets are a set of items, which are constants. In CFDs discovery, a tuple is considered to support a CFD if that tuple matches
the constants in the CFD. Our target DCs may lack constants, and
so do FDs. Therefore, we need a novel measure for statistical significance of discovered DCs on I that extends previous approaches.
E XAMPLE 14. Consider c2 , which is a FD, in Example 1. If
we look at single tuples, just as the statistical measure for CFDs,
every tuple matches c2 since it does not have constants. However,
it is obvious that the tuple pair ht4 , t7 i gives more support than the
tuple pair ht2 , t6 i because ht4 , t7 i matches the left hand side of c2 .
Being a more general form than CFDs, DCs have more kinds of
evidence that we exploit in order to give an accurate measure of the
statistical significance of a DC on I. An evidence of a DC ' is a pair
of tuples that does not violate ': there exists at least one predicate
in ' that is not satisfied by the tuple pair. Depending on the number
of satisfied predicates, different evidences give different support to
the statistical significance score of a DC. The larger the number of
satisfied predicates is in a piece of evidence, the more support it
gives to the interestingness score of '. A pair of tuples satisfying
k predicates is a k-evidence (kE). As we want to give higher score
to high values of k, we need a weight to reflect this intuition in the
scoring function. We introduce w(k) for kE, which is from 0 to 1,
and increases with k. In the best case, the maximum k for a DC '
is equal to |'.P res| 1, otherwise the tuple pair violates '.
D EFINITION 5. Given a DC ':
A k-evidence (kE) for ' w.r.t. a relational instance I is a tuple
pair htx , ty i, where k is the number of predicates in ' that are
satisfied by htx , ty i and k  |'.P res| 1.
(k+1)
The weight for a kE (w(k)) for ' is w(k) = |'.P
.
res|

E XAMPLE 15. Consider c7 in Example 3, which has 2 predicates. There are two types of evidences, i.e., 0E and 1E.
ht1 , t2 i is a 0E since t1 .F N 6= t2 .F N and t1 .GD = t2 .GD.
ht1 , t3 i is a 1E since t1 .F N 6= t3 .F N and t1 .GD 6= t3 .GD.
ht1 , t6 i is a 1E since t1 .F N = t6 .F N and t1 .GD = t6 .GD.
Clearly, ht1 , t3 i and ht1 , t6 i have higher weight than ht1 , t2 i.
Given such evidence, we define Coverage(') as follows:
Coverage(') =

P|'.P res|

1
|kE| ⇤ w(k)
k=0
P|'.P res| 1
|kE|
k=0

The enumerator of Coverage(') counts the number of different
evidences weighted by their respective weights, which is divided by
the total number of evidences. Coverage(') gives a score between
0 and 1, with higher score indicating higher statistical significance.
E XAMPLE 16. Given 8 tuples in Table 1, we have 8*7=56 evidences. Coverage(c7 ) = 0.80357, Coverage(c2 ) = 0.9821. It
can be seen that coverage score is more confident about c2 , thus
reflecting our intuitive comparison between c2 and c7 in Section 1.
Coverage for CDC is calculated using the same formula, such as
Coverage(c3 ) = 1.0.

6.3 Rank-aware Pruning in DFS Tree
Having defined Inter, we can use it to prune branches in the
DFS tree when searching for minimal covers in Algorithm 4. We
can prune any branch in the DFS tree, if we can upper bound the
Inter score of any possible DC resulting from that branch, and the
upper bound is either (i) less than a minimal Inter threshold, or
(ii) less than the minimal Inter score of the Top-k DCs we have
already discovered. We use this pruning in Algorithm 4 (Lines 1112), a branch with the current path X will result in a DC ': q(X ^
W), with X known and W unknown.
Succ score is an anti-monotonic function: adding more predicates increases the length of a DC, thus decreases the Succ of a
DC. Therefore we bound Succ(') by Succ(')  Succ(q(X)).
However, as Coverage(') is not anti-monotonic, we cannot use
q(X) to get an upper bound for it. A direct upper bound, but not
useful bound is 1.0, so we improve it as follows. Each evidence
(k+1)
E or tuple pair is contributing w(k) = |'.P
to Coverage(')
res|
with k being the number of predicates in ' that E satisfies. w(k)
can be rewritten as w(k) = 1 |'.Plres| with l being the number
of predicates in ' that E does not satisfy. In addition, we know l is
greater than or equal to the number of predicates in X that E does
not satisfy; and we know that |'.P res| must be less than the |P|
.
2
Therefore, we get an upper bound for w(k) for each evidence. The
average of the upper bounds for all evidences is a valid upper bound
for Coverage('). However, to calculate this bound, we need to iterate over all the evidences, which can be expensive because we
need to do that for every branch in the DFS tree. Therefore, to get
a tighter bound than 1.0, we only upper bound the w(k) for a small
number of evidences4 , and for the rest we set w(k)  1. We show
in the experiments how different combinations of the upper bounds
of Succ(') and of Coverage(') affect the results.

7.

EXPERIMENTAL STUDY

We experimentally evaluate FASTDC, Inter function, AFASTDC, and C-FASTDC. Experiments are performed on a Win7
machine with QuadCore 3.4GHz cpu and 4GB RAM. The scalability experiment runs on a cluster consisting of machines with the
same configuration. We use one synthetic and two real datasets.
4

We experimentally identified that 1000 samples improve the upper
bound without affecting execution times.

1506

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

10

80
70
60
50
40
30

0

0.2

0.4

0.6

0.8

100
50
0

1

(e) Threshold for Joinable Columns

G-F-Measure

1

0.5

0.55

0.6

1

0.6
0.4

0.2

0.2
0.2

0.4

0.6

0.8

0
1

6

8

10

Weight a

0.8

0.8

0.6
0.4
Tax
SPStock
Hospital
1

1.5

2

2.5

3

3.5

0

4

4.5

5

Noise Level ( * 0.001)

14

16

18

0

0.2

(m) Varying Noise Level

0.6

0.8

0

1

0

0.2

200

0.8
Tax
SPStock
Hospital

50
0
1

2

3

4

5

6

7

8

0.8
0.6
0.4
0.2
0
12

14

Tax
SPStock
Hospital
10 20 30 40 50 60 70 80 90 100

Noise in Percentage of Rows

(n) Skewed Noise in Columns

1

Tax
SPStock
Hospital

0.6
0.4

0
9

1

0
10

0.8

0.2

(k) A-FASTDC Running Time

Noise in # Cols

0.6

(h) G-Recall - Tax
1

100

0.4

Weight a

250

0.2
8

0.4

Appro. Level (*0.000001)

0.4

6

Top-5
Top-10
Top-15
Top-20

0.2

150

20

0.6

4

0.4

(g) G-Precision - Tax

Tax
SPStock
Hospital

2

0.6

Weight a

G-Recall

1

1

0.2

0.8

(j) Interestingness - Hospital

G-Recall

G-Recall

(i) G-F-Measure - Tax

0

12

5 10 15 20 25 30 35 40 45 50

0.8

0.4

Top-k

1

0.2

0.75

G-Precision
G-Recall
G-F-Measure

0.8

0.4

0

0.7

1

# Predicates

0.6

(f) Ranking Function in Pruning

0.6

0

0.65

100

(d) Scalability in |P| - Tax

Top-5
Top-10
Top-15
Top-20

0.8

Thre

Top-5
Top-10
Top-15
Top-20

0.8

1

150

Percentage of Common Values

1000

5 10 15 20 25 30 35 40 45 50

(c) Scalability in |P| - Tax

a=0.6
a=0.5
a=0.4

200

10000

# Predicates

G-Precision

90

DFS Time(secs)

# Predicates

250

100000

10
1

(b) Scalability in |P| - Tax

Tax
SPStock
Hospital

100

10

# Predicates

(a) Scalability in |I| - Tax

FASTDC
FASTDC-DS
FASTDC-DO

1e+06

100

5 10 15 20 25 30 35 40 45 50

# Tuples (*1M)

110

1000

Wasted Work

100

Running Time (secs)

10

1000

1e+07

FASTDC
FASTDC-DS
FASTDC-DO

G-Recall

100

10000

FASTDC
FASTDC-DS
FASTDC-DO

G-Recall

1000

10000

(o) Skewed Noise in Rows

1

2

3

4

5

6

7

8

9

Appro. Level (*0.000001)

(l) Varying Approximation Level
Running Time (secs)

FASTDC
FASTDC+7
FASTDC+20

# Minimal DCs

Running Time(secs)

Running Time(mins)

10000

1000
900
800
700
600
500
400
300
200
100
0

Tax
SPStock
Hospital

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

frequency thre

(p) C-FASTDC Running Time

Figure 3: FASTDC scalability (a-f), ranking functions w.r.t. ⌃g (g-j), A-FASTDC scalability (k) and quality (l-o), C-FASTDC scalability (p).
Synthetic. We use the Tax data generator from [7]. Each record
represents an individual’s address and tax information, as in Table 1. The address information is populated using real semantic
relationship. Furthermore, salary is synthetic, while tax rates and
tax exemptions (based on salary, state, marital status and number
of children) correspond to real life scenarios.
Real-world. We use two datasets from different Web sources5 .
Hospital data is from the US government. There are 17 string
attributes, including Provider # (PN), measure code (MC) and name
(MN), phone (PHO), emergency service (ES) and has 115k tuples.
SP Stock data is extracted from historical S&P 500 Stocks. Each
record is arranged into fields representing Date, Ticker, Open Price,
High, Low, Close, and Volume of the day. There are 123k tuples.

7.1

Scalability Evaluation

We mainly use the Tax dataset to evaluate the running time of
FASTDC by varying the number of tuples |I|, and the number of
predicates |P|. We also report running time for the Hospital and the
SP Stock datasets. We show that our implication testing algorithm,
though incomplete, is able to prune a huge number of implied DCs.
5

http://data.medicare.gov, http://pages.swcp.com/stocks

Algorithms. We implemented FASTDC in Java, and we test
various optimizations techniques. We use FASTDC+M to represent running FASTDC on a cluster consisting of M machines. We
use FASTDC-DS to denote running FASTDC without dividing the
space of DCs as in Section 5.4. We use FASTDC-DO to denote
running FASTDC without dynamic ordering of predicates in the
search tree as in Section 5.3.
Exp-1: Scalability in |I|. We measure the running time in minutes on all 13 attributes, by varying the number of tuples (up to 1
million tuples), as reported in Figure 3a. The size of the predicate
space |P| is 50. The Y axis of Figure 3a is in log scale. We compare the running time of FASTDC and FASTDC+M with number
of blocks B=2M to achieve load balancing. Figure 3a shows a
quadratic trend as the computation is dominated by the tuple pairwise comparison for building the evidence set. In addition, Figure 3a shows that we achieve almost linear improvement w.r.t the
number of machines on a cluster; for example, for 1M tuples, it
took 3257 minutes on 7 machines, but 1228 minutes on 20 machines. Running FASTDC on a cluster is a viable approach if the
number of tuples is too large to run on a single machine.
Exp-2: Scalability in |P|. We measure the running time in seconds using 10k tuples, by varying the number of predicates through

1507

including different number of attributes in the Tax dataset, as in
Figure 3b. We compare the running time of FASTDC, FASTDCDS, and FASTDC-DO. The ordering of adding more attributes is
randomly chosen, and we report the average running time over 20
executions. The Y axes of Figures 3b, 3c and 3d are in log scale.
Figure 3b shows that the running time increases exponentially w.r.t.
the number of predicates. This is not surprising because the number of minimal DCs, as well as the amount of wasted work, increases exponentially w.r.t. the number of predicates, as shown in
Figures 3c and 3d. The amount of wasted work is measured by the
number of times Line 15 of Algorithm 4 is hit. We estimate the
wasted DFS time as a percentage of the running time by wasted
work / (wasted work + number of minimal DCs), and it is less than
50% for all points of FASTDC in Figure3d. The number of minimal DCs discovered is the same for FASTDC, FASTDC-DS, and
FASTDC-DO as optimizations do not alter the discovered DCs.
Hospital has 34 predicates and it took 118 minutes to run on a
single machine using all tuples. Stock has 82 predicates and it took
593 minutes to run on a single machine using all tuples.
Exp-3: Joinable Column Analysis. Figure 3e shows the number of predicates by varying the % of common values required
to declare joinable two columns. Smaller values lead to a larger
predicate space and higher execution times. Larger values lead to
faster execution but some DCs involving joinable columns may be
missed. The number of predicates gets stable with low percentage
of common values, and with our datasets the quality of the output
is not affected when at least 30% common values are required.
Exp-4: Ranking Function in Pruning. Figure 3f shows the
DFS time taken for the Tax dataset varying the minimum Inter
score required for a DC to be in the output. The threshold has
to exceed 0.6 to have pruning power. The higher the threshold,
the more aggressive the pruning. In addition, a bigger weight for
Succ score (indicated by smaller a in Figure 3f) has more pruning
power. Although in our experiment golden DCs are not dropped by
this pruning, in general it is possible that the upper bound of Inter
for interesting DCs falls under the threshold, thus this pruning may
lead to losing interesting DCs. The other use of ranking function
for pruning is omitted since it has little gain.
Dataset
Tax
Hospital
SP Stock

# DCs Before
1964
157
829

# DCs After
741
42
621

% Reduction
61%
73%
25%

Table 3: # DCs before and after reduction through implication.
Exp-5: Implication Reduction. The number of DCs returned
by FASTDC can be large, and many of them are implied by others.
Table 3 reports the number of DCs we have before and after implication testing for datasets with 10k tuples. To prevent interesting
DCs from being discarded, we rank them according to their Inter
function. A DC is discarded if it is implied by DCs with higher
Inter scores. It can be seen that our implication testing algorithm,
though incomplete, is able to prune a large amount of implied DCs.

7.2

Qualitative Analysis

Table 4 reports some discovered DCs, with their semantics explained in English6 . We denote by ⌃g the golden VDCs that have
been designed by domain experts on the datasets. Specifically, ⌃g
for Tax dataset has 8 DCs; ⌃g for Hospital is retrieved from [10]
and has 7 DCs; and ⌃g for SP Stock has 6 DCs. DCs that are
implied by ⌃g are also golden DCs. We denote by ⌃s the DCs
6

All datasets, as well as their golden and discovered DCs are available at “http://da.qcri.org/dc/”.

returned by FASTDC. We define G-Precision as the percentage
of DCs in ⌃s that are implied by ⌃g , G-Recall as the number of
DCs in ⌃s that are implied by ⌃g over the total number of golden
DCs, and G-F-Measure as the harmonic mean of G-Precision and
G-Recall. In order to show the effectiveness of our ranking function, we use the golden VDCs to evaluate the two dimensions of
Inter function in Exp-6, the performance of A-FASTDC in Exp7. We evaluate C-FASTDC in Exp-8. However, domain experts
might not be exhaustive in designing all interesting DCs. In particular, humans have difficulties designing DCs involving constants.
We show with U -P recision(⌃s ) the percentage of DCs in ⌃s that
are verified by experts to be interesting, and we report the result in
Exp-9. All experiments in this section are done on 10k tuples.
Exp-6: Evaluation of Inter score. We report in Figures 3g– 3i
G-Precision, G-Recall, and G-F-Measure for Tax, with ⌃s being
the Top-k DCs according to Inter by varying the weight a from 0
to 1. Every line is at its peak value when a is between 0.5 and 0.8.
Moreover, Figure 3h shows that Inter score with a = 0.6 for Top20 DCs has perfect recall; while it is not the case for using Succ
alone (a = 0), or using Coverage alone (a = 1). This is due to
two reasons. First, Succ might promote shorter DCs that are not
true in general, such as c7 in Example 3. Second, Coverage might
promote longer DCs that have higher coverage than shorter ones,
however, those shorter DCs might be in ⌃g ; for example, the first
entry in Table 4 has higher coverage than q(t↵ .AC = t .AC ^
t↵ .P H = t .P H), which is actually in ⌃g . For Hospital, Inter
and Coverage give the same results as in Figures 3j, which are
better than Succ because golden DCs for Hospital are all FDs with
two predicates, therefore Succ has no effect on the interestingness.
For Stock, all scoring functions give the same results because its
golden DCs are simple DCs, such as q(t↵ .Low > t↵ .High).
This experiment shows that both succinctness and coverage are
useful in identifying interesting DCs. We combine both dimensions into Inter with a = 0.5 in our experiments. Interesting DCs
usually have Coverage and Succ greater than 0.5.
Exp-7: A-FASTDC. In this experiment, we test A-FASTDC on
noisy datasets. A noise level of ↵ means that each cell has ↵ probability of being changed, with 50% chance of being changed to
a new value from the active domain and the other 50% of being
changed to a typo. For a fixed noise level ↵ = 0.001, which will introduce hundreds of violating tuple pairs for golden DCs, Figure 3l
plots the G-Recall for Top-60 DCs varying the approximation level
✏. A-FASTDC discovers an increasing number of correct DCs as
we increase ✏, but, as it further increases, G-Recall drops because
when ✏ is too high, a DC whose predicates are a subset of a correct DC might get discovered, thus the correct DC will not appear.
For example, the fifth entry in Table 4 is a correct DC; however,
if ✏ is set too high, q(t↵ .P N = t .P N ) would be in the output.
G-Recall for SPStock data is stable and higher than the other two
datasets because most golden DCs for SPStock data are one tuple
DCs, which are easier to discover. Finally, we examine Top-60 DCs
to discover golden DCs, which is larger than Top-20 DCs in clean
datasets. However, since there are thousands of DCs in the output,
our ranking function is still saving a lot of manual verification.
Figure 3m shows that for a fixed approximate level ✏= 4 ⇥ 10 6 ,
as we increase the amount of noise in the data, the G-Recall for
Top-60 DCs shows a small drop. This is expected because the
nosier gets the data, the harder it is to get correct DCs. However,
A-FASTDC is still able to discover golden DCs.
Figure 3n and 3o show how A-FASTDC performs when the noise
is skewed. We fix 0.002 noise level, and instead of randomly distributing them over the entire dataset, we distribute them over a certain region. Figure 3n shows that, as we distribute the noise over

1508

1

Dataset
Tax

2

Tax

3

Tax

4
5
6
7
8
9
10
11

Hospital
Hospital
SP Stock
SP Stock
Tax
Tax
Tax
Hospital

DC Discovered
q(t↵ .ST = t .ST ^ t↵ .SAL < t .SAL
^t↵ .T R > t .T R)
q(t↵ .CH 6= t .CH ^ t↵ .ST X < t↵ .CT X
^t .ST X < t .CT X)
q(t↵ .M S 6= t .M S ^ t↵ .ST X = t .ST X)
^t↵ .ST X > t↵ .CT X)
q(t↵ .M C = t .M C ^ t↵ .M N 6= t .M N )
q(t↵ .P N = t .P N ^ t↵ .P HO 6= t .P HO)
q(t↵ .Open > t↵ .High)
q(t↵ .Date = t .Date ^ t↵ .T icker = t .T icker)
q(t↵ .ST = ‘FL’ ^ t↵ .ZIP < 30397)
q(t↵ .ST = ‘FL’ ^ t↵ .ZIP
35363)
q(t↵ .M S 6= ‘S’ ^ t↵ .ST X 6= 0)
q(t↵ .ES 6= ‘Yes’ ^ t↵ .ES 6= ‘No’)

Semantics
There cannot exist two persons who live in the same state,
but one person earns less salary and has higher tax rate at the same time.
There cannot exist two persons with both having CTX higher than STX,
but different CH. If a person has CTX, she must have children.
There cannot exist two persons with same STX, one person has higher STX than
CTX and they have different MS. If a person has STX, she must be single.
Measure code determines Measure name.
Provider number determines Phone number.
The open price of any stock should not be greater than its high of the day.
Ticker and Date is a composite key.
State Florida’s ZIP code cannot be lower than 30397.
State Florida’s ZIP code cannot be higher than 35363.
One has to be single to have any single tax exemption.
The domain value of emergency service is yes or no.

Table 4: Sample DCs discovered in the datasets.
a larger number of columns, the G-Recall drops because noise in
more columns affect the discovery of more golden DCs. Figure 3o
shows G-Recall as we distribute the noise over a certain percentage
of rows; G-Recall is quite stable in this case.
Exp-8: C-FASTDC. Figure 3p reports the running time of CFASTDC varying minimal frequent threshold ⌧ from 0.02 to 1.0.
When ⌧ = 1.0, C-FASTDC falls back to FASTDC. The smaller
the ⌧ , the more the frequent constant predicate sets, the bigger the
running time. For the SP Stock dataset, there is no constant predicate set, so it is a straight line. For the Tax data, ⌧ = 0.02 results in
many frequent constant predicate sets. Since it is not reasonable for
experts to design a set of golden CDCs, we only report U-Precision.
Dataset
Tax
Hospital
SP Stock
Tax-Noise
Hosp.-Noise
Stock-Noise

k=10
1.0
1.0
1.0
0.5
0.9
0.9

FASTDC
k=15
0.93
0.93
1.0
0.53
0.8
0.93

k=20
0.75
0.7
1.0
0.5
0.7
0.95

k=50
1.0
1.0
0
1.0
1.0
0

C-FASTDC
k=100 k=150
1.0
1.0
1.0
1.0
0
0
1.0
1.0
1.0
1.0
0
0

Table 5: U-Precision.
Exp-9: U-Precision. We report in Table 5 the U-Precision for
all datasets using 10k tuples, and the Top-k DCs as ⌃s . We run
FASTDC and C-FASTDC on clean data, as well as noisy data. For
noisy data, we insert 0.001 noise level, and we report the best result
of A-FASTDC using different approximate levels. For FASTDC on
clean data, Top-10 DCs have U-precision 1.0. In fact in Figure 3g,
Top-10 DCs never achieve perfect G-precision because FASTDC
discovers VDCs that are correct, but not easily designed by humans, such as the second and third entry in Table 4. For FASTDC
on noisy data, though the results degrade w.r.t. clean data, at least
half of the DCs in Top-20 are correct. For C-FASTDC on either
clean or noisy data, we achieve perfect U-Precision for the Tax and
the Hospital datasets up to hundreds of DCs. SP Stock data has no
CDCs. This is because C-FASTDC is able to discover many business rules such as entries 8-10 in Table 4, domain constraints such
as entry 11 in Table 4, and CFDs such as c3 in Example 1.

8.

CONCLUSION AND FUTURE WORK

Denial Constraints are a useful language to detect violations and
enforce the correct application semantics. We have presented static
analysis for DCs, including three sound axioms, and a linear implication testing algorithm. We also developed a DCs discovery
algorithm (FASTDC), as well as A-FASTDC and C-FASTDC. In
addition, experiments shown that our interestingness score is effective in identifying meaningful DCs. In the future, we want to

investigate sampling techniques to alleviate the quadratic complexity of computing the evidence set.

9.

ACKNOWLEDGMENTS

The authors thank the reviewers for their useful comments.

10.

REFERENCES

[1] S. Abiteboul, R. Hull, and V. Vianu. Foundations of Databases.
Addison-Wesley, 1995.
[2] R. Agrawal, T. Imielinski, and A. N. Swami. Mining association
rules between sets of items in large databases. In SIGMOD, 1993.
[3] M. Baudinet, J. Chomicki, and P. Wolper. Constraint-generating
dependencies. J. Comput. Syst. Sci., 59(1):94–115, 1999.
[4] O. Benjelloun, H. Garcia-Molina, H. Gong, H. Kawai, T. E. Larson,
D. Menestrina, and S. Thavisomboon. D-swoosh: A family of
algorithms for generic, distributed entity resolution. In ICDCS, 2007.
[5] L. E. Bertossi. Database Repairing and Consistent Query Answering.
Morgan & Claypool Publishers, 2011.
[6] C. M. Bishop. Pattern Recognition and Machine Learning
(Information Science and Statistics). Springer-Verlag, 2006.
[7] P. Bohannon, W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis.
Conditional functional dependencies for data cleaning. ICDE, 2007.
[8] F. Chiang and R. J. Miller. Discovering data quality rules. PVLDB,
1(1):1166–1177, 2008.
[9] X. Chu, I. F. Ilyas, and P. Papotti. Discovering denial constraints.
Tech. Report QCRI2013-1 at http://da.qcri.org/dc/.
[10] X. Chu, I. F. Ilyas, and P. Papotti. Holistic data cleaning: Putting
violations into context. In ICDE, 2013.
[11] T. Dasu, T. Johnson, S. Muthukrishnan, and V. Shkapenyuk. Mining
database structure; or, how to build a data quality browser. In
SIGMOD, pages 240–251, 2002.
[12] D. Deroos, C. Eaton, G. Lapis, P. Zikopoulos, and T. Deutsch.
Understanding Big Data: Analytics for Enterprise Class Hadoop and
Streaming Data. McGraw-Hill, 2011.
[13] W. Fan and F. Geerts. Foundations of Data Quality Management.
Morgan & Claypool Publishers, 2012.
[14] W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis. Conditional
functional dependencies for capturing data inconsistencies. ACM
Trans. Database Syst., 33(2), 2008.
[15] W. Fan, F. Geerts, J. Li, and M. Xiong. Discovering conditional
functional dependencies. IEEE TKDE, 23(5):683–698, 2011.
[16] L. Golab, H. J. Karloff, F. Korn, D. Srivastava, and B. Yu. On
generating near-optimal tableaux for conditional functional
dependencies. PVLDB, 1(1):376–390, 2008.
[17] Y. Huhtala, J. Kärkkäinen, P. Porkka, and H. Toivonen. TANE: An
efficient algorithm for discovering functional and approximate
dependencies. Comput. J., 42(2):100–111, 1999.
[18] I. F. Ilyas, V. Markl, P. J. Haas, P. Brown, and A. Aboulnaga.
CORDS: Automatic discovery of correlations and soft functional
dependencies. In SIGMOD, pages 647–658, 2004.
[19] C. M. Wyss, C. Giannella, and E. L. Robertson. FastFDs: A
heuristic-driven, depth-first algorithm for mining functional
dependencies from relation instances. In DaWaK, 2001.

1509

Descriptive and Prescriptive Data Cleaning
Anup Chalamalla1∗, Ihab F. Ilyas1∗, Mourad Ouzzani2 , Paolo Papotti2
1

University of Waterloo,

2

Qatar Computing Research Institute (QCRI)

akchalam@cs.uwaterloo.ca, ilyas@uwaterloo.ca, {mouzzani,ppapotti@qf.org.qa}

ABSTRACT

not on the original data but rather on reports or views, and
at a later stage in the data processing life cycle.
Example 1. Consider the report T (see Figure 1) about
shops for an international franchise. The HR department
enforces a set of policies in the franchise workforce and identifies two problems in T . The first violation (ta and tb , in
bold) comes from a rule stating that, in the same shop, the
average salary of the managers (Grd=2) should be higher
than that of the staff (Grd=1). The second violation (tb
and td , in italic) comes from a rule stating that a bigger
shop cannot have a smaller staff.

Data cleaning techniques usually rely on some quality rules
to identify violating tuples, and then fix these violations using some repair algorithms. Oftentimes, the rules, which are
related to the business logic, can only be defined on some target report generated by transformations over multiple data
sources. This creates a situation where the violations detected in the report are decoupled in space and time from
the actual source of errors. In addition, applying the repair
on the report would need to be repeated whenever the data
sources change. Finally, even if repairing the report is possible and affordable, this would be of little help towards identifying and analyzing the actual sources of errors for future
prevention of violations at the target. In this paper, we propose a system to address this decoupling. The system takes
quality rules defined over the output of a transformation
and computes explanations of the errors seen on the output.
This is performed both at the target level to describe these
errors and at the source level to prescribe actions to solve
them. We present scalable techniques to detect, propagate,
and explain errors. We also study the effectiveness and efficiency of our techniques using the TPC-H Benchmark for
different scenarios and classes of quality rules.

1.

T
ta
tb
tc
td
te
tf
Emps
t1
t2
t3
t4
t5
t6
t7
t8
t9
t10

INTRODUCTION

A common approach to address the problem of dirty
data [8] is to apply a set of data quality rules or constraints
over a target database, to “detect” and to eventually “repair”
erroneous data [1, 7, 10, 3, 15]. Tuples or cells (attributevalue of a tuple) in a database D that are inconsistent w.r.t.
a set of rules Σ are considered to be in violation, thus possibly “dirty”. A repairing step tries to “clean” these violations by producing a set of updates over D leading to a new
database D0 that satisfies Σ. Unfortunately, in many real life
scenarios [13], the picture is different, and data and rules are
decoupled in space and time; constraints are often declared
∗

Shop
NY1
NY1
NY2
NY2
LA1
LND

Shops
t11
t12
t13

Size
46 ft2
46 ft2
62 ft2
62 ft2
35 ft2
38 ft2

EId
e4
e5
e7
e8
e11
e13
e14
e18
e14
e18

Name
John
Anne
Mark
Claire
Ian
Laure
Mary
Bill
Mike
Claire

SId
NY1
NY2
LA1

Grd
2
1
2
1
2
1

AvgSal
99 $
100 $
96 $
90 $
105 $
65 £

Dept
S
D
S
S
R
R
E
D
R
E

City
New York
New York
Los Angeles

Sal
91
99
93
116
89
94
91
98
94
116

#Emps
1
3
2
2
2
2
Grd
1
2
1
1
1
2
1
2
2
2

State
NY
NY
CA

Region
US
US
US
US
US
EU

SId
NY1
NY1
NY1
NY1
NY2
NY2
NY2
NY2
LA1
LA1

Size
46
62
35

JoinYr
2012
2012
2012
2012
2012
2012
2012
2012
2011
2011

Started
2011
2012
2011

Figure 1: A report T on data sources Emps & Shops.
To explain these errors, we adopt an approach that summarizes the violations in terms of predicates on the database.
In the example, since the problematic tuples have Attribute
Region set to U S, we describe (explain) the violations in
the example as [T.Region = U S]. Note that this explanation summarizes all tuples that are involved in a violation,
and not necessarily the erroneous tuples; in many cases, updating only one tuple in a violation (set of tuple) is enough
to bring the database into a consistent state. For example, a
repairing algorithm would identify tb .Grd as a possible error
in the report. Hence, by updating tb .Grd, the two violations
would be removed. Limiting the erroneous tuples can guide
us to a more precise explanation. In the example, the explanation [T.Region = U S ∧T.Shop = N Y 1] is more specific, if
we believe that tb .Grd is the erroneous cell. The process of
explaining data errors is two-fold: identifying a set of potential erroneous tuples (cells); and finding concise descriptions

Work partially done while at QCRI.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGMOD’14, June 22–27, 2014, Snowbird, UT, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2376-5/14/06 ...$15.00.
http://dx.doi.org/10.1145/2588555.2610520.

445

Figure 2: System Architecture.
4 such as [Emps.JoinY r = 2012]
prescriptive explanation 
and [Shops.State = N Y ] on the source tables. When applicable, a repair is computed over the target, thus allow2 such as
ing the possibility of a more precise description 
[T.Region = U S ∧ T.Shop = N Y 1], and a more precise pre3 based on propagating errors to the
scriptive explanation 
sources such as [Emps.Dept = S] and [Emps.EId = e8].
Building DBRx raises several challenges: First, propagating the evidence about violating tuples from the target to
the sources can lead to a lineage that covers a large number
of source tuples. For example, an aggregate query would
clump together several source tuples, with only few containing actual errors. Simply partitioning the source tuples as
dirty and clean is insufficient; tuples do not contribute to violations in equal measure. Second, we need a mechanism to
accumulate evidences on tuples across multiple constraints
and violations and hence identify the most likely tuples with
errors. For the target side, there are several repairing algorithms that we can use. But for the source side, a new
algorithm, which relaxes the requirements of repair semantics, is needed. Third, after identifying the likely errors,
mining the explanations involves two issues that we need
to deal with: (1) what are the explanations that accurately
cover all and only the identified erroneous tuples?; and (2)
how to generate explanations concise enough in order to be
consumable by humans?
We summarize our contributions in this paper as follows:
a. We introduce the problem of descriptive and prescriptive data cleaning (Section 2). We define the notion of
explanation, and formulate the problem of discovering
explanations over the annotated evidence of errors at
the sources (Section 3).
b. We develop a novel weight-based approach to annotate the lineage of target violations in source tuples
(Section 4).
c. We present an algorithm to compute the most likely
errors in presence of violations that involve large number of tuples with multiple errors (Section 5).
d. We combine multi-dimensional mining techniques with
approximation algorithms to efficiently solve the explanation mining problem (Section 6).
We perform an extensive experimental analysis using the
TPC-H Benchmark and real-world datasets (Section 7). We
conclude the paper with a discussion of related work (Section 8) and of future direction of research (Section 9).

that summarize these errors and that can be consumed by
users or by other analytics layers.
We highlight the problem of explaining errors when errors are identified in a different space and at a later stage
than when errors were digitally born. Consider the following
query that generated Table T in Example 1. Since violations
detected in the report are actually caused by errors that
crept in at an earlier stage, i.e., from the sources, propagating these errors from a higher level in the transformation to
the underlying sources can help in identifying the source of
the errors and in prescribing actions to correct them.
Example 2. Let us further assume that the previous report T is the result of a union of queries over multiple shops
of the same franchise. We focus on the query over source
relations Emps and Shops for the US region (Figure 1).
Q: SELECT SId as Shop, Size, Grd, AVG(Sal) as
AvgSal, COUNT(EId) as #Emps,‘US’ as Region
FROM US.Emps JOIN US.Shops ON SId
GROUP BY SId, Size, Grd
We want to trace back the tuples that contributed to the
problems in the target. Tuples ta − td are in violation in
T and their lineage is {t1 − t8 } and {t11 − t12 } over Tables
Emps and Shops. By removing these tuples from any of the
sources, the violation is removed. Two possible explanations
of the problems are therefore [Emps.JoinY r = 2012] On
Table Emps, and [Shops.State = N Y ] on Table Shops.
As we mentioned earlier, tb is the erroneous tuple that was
identified by the repairing algorithm. Its lineage is {t1 , t3 , t4 }
and {t11 } over Tables Emps and Shops, respectively. By
focusing on this tuple, we can compute more precise explanations on the sources, such as [Emps.Dept = S]. Drilling
even further, an analysis on the lineage of tb may identify
t4 as the most likely source of error since by removing t4 ,
the average salary goes down enough to clear the violation.
Therefore, the most precise explanation is [Emps.EId = e8].
The example shows that computing likely errors enables the
discovery of better explanations. At the source level, this
leads to the identification of actions to solve the problem.
In the example, the employee with id e8 seems to be the
cause of the problem.
We propose Database Prescription (DBRx for short)
(Figure 2), a system to support descriptive and prescriptive data cleaning. DBRx takes quality rules defined over
the output of a transformation and computes explanations
of the errors. Given a transformation scenario (sources Si ,
1 < i < n, and query Q) and a set of quality rules Σ, DBRx
computes a violation table V T of tuples not complying with
1 in
Σ. V T is mined to discover a descriptive explanation 
Figure 2 such as [T.Region = U S]. The lineage of the violation table over the sources enables the computation of a

2.

PROBLEM STATEMENT

Let S = {S1 , S2 , . . . , Sn } be the set of schemas of n source
relations, where each source schema Si has di attributes
Si
Si
Si
i
AS
1 , . . . , Adi with domains dom(A1 ), . . . , dom(Adi ). Let R

446

be the schema of a target view generated from S. Without
loss of generality, we assume that every schema has a special
attribute representing the tuple id. A transformation is a
union of SPJA queries on an instance I of S that produces
a unique instance T of R with t attributes AT1 , . . . , ATt .
Any instance T of a target view is required to comply with
a set of data quality rules Σ. We clarify the rules supported
in our system in the next Section. For now, we characterize
them with the two following functions:
• Detect(T ) identifies cells in T that do not satisfy a
rule r ∈ Σ, and store them in a violation table V (T ).
• Error (V (T )) returns the most likely erroneous cells
for the violations in V (T ) and store them in an error
table E(T ).
While Detect has a clear semantics, Error needs some
clarifications. At the target side, we consider the most likely
erroneous cells as simply those cells that a given repair algorithm decides to update in order to produce a clean data
instance, i.e., an instance that is consistent w.r.t. the input
rules. Our approach can use any of the available alternative
repair algorithms, e.g., [15], (Section 3.3). At the source, we
need to deal with the lineage of problematic cells instead of
the problematic cells themselves, to produce the most likely
erroneous cells. Existing repair algorithms were not meant
to handle such a scenario; we show in Section 5 our own
approach to produce these cells.
Our goal is to describe problematic data with concise explanations. Explanations are composed of queries over the
relations in the database as follows.

Unfortunately, since all errors must be covered and no
clean tuples are allowed in the cover, the exact solution in
the worst case does not exist. In other cases, it may be a
set of queries s.t. each query covers exactly one tuple. The
number of queries in the explanation equals the number of
errors (as for exp8 in Example 3), making the explanation
hard to consume.
To allow more flexibility, we drop the strict requirement
over the precision of the solution and allow it to cover some
clean tuples. We argue that explanations such as exp7 can
better highlight problems over the sources and are easier to
consume. More specifically, we introduce a weight function
for a query q, namely w(q), that depends on the number of
clean and erroneous tuples that it covers:
w(q) = |E(R) \ cover(q)| + λ ∗ |cover(q) ∩ C|
where C is the set of clean tuples, w(E) is the sum w(q1 ) +
. . . + w(qn ), qi ∈ E that we want to minimize, and the constant λ has a value in [0,1]. The weight has two roles. First,
it favors queries that cover many errors (first part of the
weight function) to minimize the number of queries to obtain full coverage in E. Second, it favors queries that cover
few clean tuples (second part). Constant λ weighs the relative importance of clean tuples w.r.t. errors. In fact, if clean
and erroneous tuples are weighted equally, selective queries
with |cover(q) ∩ C| = ∅ are favored, since they are more
precise, but they lead to larger size for E. On the contrary,
obtaining a smaller explanation justifies the compromise of
covering some clean tuples. We set parameter λ to the error
rate for the scenario, we shall describe in Section 6 how it is
computed. We now state the relaxed version of the problem.
Definition 3 (Relaxed DPDC). Given a relation R,
a corresponding violation table V (R), an Error function
for V (R), and a weight function w(E), a solution for the
relaxed DPDC problem is an explanation Eopt s.t.

Definition 1. An explanation is a set E of conjunctive
queries where q ∈ E is a query of k selection predicates
Si
i
(AS
l1 = vl1 ) ∧ · · · ∧ (Alk = vlk ) over a table Si with di
attributes, 1 ≤ k ≤ di , and vlj (1 ≤ j ≤ k) are constant
values from the domain of the corresponding attributes. We
denote with size(E) the number of queries in E.

Eopt = argmin(cover(E) ⊇ E(R))

There are three requirements for an explanation: (i) coverage - covers error tuples, (ii) conciseness - has small number
of queries, and (iii) accuracy - covers mostly error tuples.

w(E)

When the DPDC problem is solved over the target (resp.
sources), it computes descriptive (resp. prescriptive) explanations. We can map this problem to the well-known
weighted set cover problem, which is proven to be an NPComplete problem [4], where the universe are the errors in
E(R) and the sets are all the possible queries over R.

Example 3. Consider again relation Emps. Let us assume that t1 , t3 , t4 , and t7 are error tuples. There are
alternative explanations that cover them. The most concise
is exp7 :(Emps.Grd=1), but one clean tuple is also covered
(t5 ). Explanation exp8 :(Emps.eid=e4 ) ∨ (Emps.eid=e7 )
∨(Emps.eid=e8 ) ∨ (Emps.eid=e14 ) has a larger size, but it
is more accurate since no clean tuples are covered.

3.

VIOLATIONS AND ERRORS

While many solutions are available for the standard data
cleaning setting, i.e., a database with a set of constraints,
we show in this section how the two levels in our framework,
namely target and source, make the problem much harder.

We define cover of a query q as the set of tuples retrieved by q. The cover of an explanation E is the union
of cover(q1 ), . . . , cover(qn ), qi ∈ E. For a relation R with
a violation table V (R) computed with Detect, we denote
with C the clean tuples R\ Error (V (R)). We now state the
exact Descriptive and Prescriptive Data Cleaning (DPDC)
problem:

3.1

Data Quality Rules

Quality rules can be usually expressed either using known
formalisms or more generally through arbitrary code. We
thus distinguish between two classes of quality rules over
relational databases.
Examples for the first class are conditional functional dependencies (CFDs) and check constraints (CCs). Since rules
in these formalisms can be expressed with the more general
class of denial constraints (DCs), we will refer to this language in the following and denote such rules with ΣD .1

Definition 2 (Exact DPDC). Given a relation R, a
corresponding violation table V (R), and an Error function
for V (R), a solution for the exact DPDC problem is an
explanation Eopt s.t.
Eopt = argmin(E|(cover(E) = E(R)))
size(E)

1
If function Error over the target is not available (
),
the problem is defined on V(R) instead of E(R).

1
Our repair model focuses on detecting problems on the existing data with the big portion of business rules supported

447

Consider
a
set
of
finite
built-in
operators
B = {=, <, >, 6=, ≤, ≥}. A DC has the general form

rule, this function outputs an update to the database to satisfy the violations identified by the corresponding detect.
For rules in ΣP , the repair function must be provided [7].
If such a function is not available (as in many cases), our
explanations will be limited to violations and their lineage,
1 and 
,
4 respectively. For a DC in ΣD , computing
cases 
its repair function is straightforward: the repair function is
the union of the inverse for each predicate in it.

ϕ : ∀tα , tβ , tγ , . . . ∈ R, q(P1 ∧ . . . ∧ Pm )
where Pi is of the form v1 φv2 or v1 φ const with v1 , v2 of the
form tx .A, x ∈ {α, β, γ, . . .}, A ∈ R, and const is a constant.
For simplicity, we use DCs with only one relation S in S.
Example 4. The rules in the running example correspond to the following DCs (for simplicity, we omit the universal quantifiers):
c1 :q(tα .Shop = tβ .Shop∧tα .Avgsal > tβ .Avgsal∧tα .Grd <
tβ .Grd)
c2 :q(tα .Size > tβ .Size ∧ tα .#Emps < tβ .#Emps)

Example 6. Given the rules in the running example, a
repair function would compute the following updates:
repair(c1 ): (tα .Shop 6= tβ .Shop)∨(tα .Avgsal ≤ tβ .Avgsal)∨
(tα .Grd ≥ tβ .Grd)
repair(c2 ) : (tα .Size ≤ tβ .Size) ∨ (tα .#Emps ≥ tβ .#Emps)
The repair problem (even with FDs only) is known to have
NP complexity [15]. Heuristic algorithms to compute repairs
identify the minimal number of cells to change to obtain an
instance that conforms to the rules. More precisely, for a
violation table V (T ) and the repair functions F = f1 , . . . , fn
for n rules in Σ, Repair(V (T ),F ) computes a set of cell
updates on the database s.t. it satisfies Σ. While we are not
interested in the actual updates to get a repair, we consider
the cells to be updated by the repair algorithm to be the
likely errors, therefore Error coincides with repair.

The second class includes rules expressed with arbitrary
declarative languages (such as SQL) and procedural code
(such as Java programs) [7]. These are alternatives to the
traditional rules in ΣD . We denote these more general rules
with ΣP . Thus, Σ = ΣD ∪ ΣP .
Example 5. A rule expressed in Java could use an external web service to validate if the ratio of the size of the
staff and the size of the shop comply with a local legal policy.

3.2

Target Violation Detection

3.4

Given a set of rules Σ, we require that any rule r ∈ Σ has
a function detect that identifies groups of cells (or tuples)
that together do not satisfy r. We call such set of cells a
violation. We collect all such violations over T w.r.t. Σ in
a violation table with the schema (vid, r, tid, att, val), where
vid represents the violation id, r is the rule, tid is the tuple
id, att is the attribute name of the cell, and val is the value
tid.att of that cell. We denote the violation table of a target
1
view T as V (T ). We mine V (T ) for explanations in case 
.
For DCs in ΣD , detect can be easily obtained. A DC
states that all the predicates cannot be true at the same
time, otherwise, we have a violation. Given a database instance I of schema S and a DC ϕ, if I satisfies ϕ, we write
I |= ϕ, and we say that ϕ is a valid DC. If we have a set of
DC Σ, I |= Σ if and only if ∀ϕ ∈ Σ, I |= ϕ.
For rules in ΣP , the output emitted by the code when
applied on the data can be used to extract the output required by detect. In Example 5, in case of non compliance
with the policy, the cells Size, #Emps and Region will be
considered as one violation.

3.3

From Target to Sources

We have introduced how violations and errors can be detected over the target. Unfortunately, a target rule can be
rewritten at the sources only in limited cases. This is not
possible for the rules expressed as Java code in ΣP as we
treat them as black-boxes. For rules in ΣD , the rewriting
depends on the SQL script in the transformation. Rules may
involve target attributes whose lineage is spread across multiple relations (as in Example 1), thus the transformation
is needed in order to apply them. An alternative approach
is to propagate the violations from the target to source at
the instance level. However, going from the target to the
sources introduces new challenges.

T
ta
tb

Shop
NY1
NY2

avgHrs
23
25

Shif ts
t1
t2
t3
t4
t5
t6
t7
t8
t9

Sid
NY1
NY1
NY1
NY1
NY1
NY1
NY1
NY2
NY2

Hours
20
20
30
30
22
22
17
20
30

Week
11
11
12
12
13
13
14
11
11

Clerk
John
Anne
Anne
John
John
John
John
Laure
Bill

Figure 3: Average Hours by Shop.

Target Errors Detection

As we mentioned in the introduction (Example 2), the
ability to identify actual errors can improve the performance
2 We can rely on the literature on data
of the system (case 
).
repairing as a tool to identify the errors in a database. If
a cell needs to be changed to make the instance consistent,
then that cell is considered as an error.
Repair refers to the process of correcting detected violations. Several algorithms have been proposed for repairing
inconsistent data, mainly based on declarative data quality
rules (such as in ΣD ) [1, 10, 3]. These rules naturally have a
static semantics for violation detection (as described above)
and a dynamic semantics to remove them. This can be modeled with a repair function. Given a violation for a certain

Example 7. Consider a source relation Shifts and a target relation T (Figure 3) obtained with the following query:
SELECT SId as Shop, AVG(Hours) as avgHrs
FROM Shifts where SID like ‘NY%’
GROUP BY SId
Given the check constraint ¬(avgHrs < 25) over T , tuple ta
is a violation. By removing its lineage (t1 −t7 ), the violation
is removed. However, we are interested in identifying most
likely errors and considering the entire lineage may not be
necessary. In fact, it is possible to remove the violation by
just removing a subgroup of the lineage. In particular, all
the subsets of size 1 to 4 involving t1 , t2 , t5 , t6 , t7 are possible
alternatives, whose removal removes the violation on ta .

by DCs. However, more complex repair models for missing
tuples, such as [11], can be supported with extensions.

It is easy to see that the lineage of the violation leads to
the problematic tuples over the source. Computing a repair

448

on the source requires a new repair algorithm such that by
updating some source tuples, the results of the query change
and satisfy the constraints. This is always possible, for example by removing the entire lineage. However, similarly to
the target level, the traditional concept of minimality can
still guide the process of identifying the source tuples that
need to change. There are two motivations for this choice.
First, treating the entire lineage as errors is far from the reality for a query involving a large number of tuples. Second,
considering the entire lineage for explanation discovery will
not help in finding meaningful explanations. Unfortunately,
it is known that computing all the possible subsets of such
lineage is a NP problem even in simpler settings with one
SPJU query [5]. We can easily see from the example how
the number of subsets can explode.
The above problem shows the impossibility of computing
a minimal repair for the target violations over the sources.
However, we are interested in identifying the source error
tuples in order to discover explanations, not in computing
a target repair. Thus, the source Error module will use
the minimality principle, without the need to compute a
target repair. In Section 4, we introduce scoring functions
to quantify the importance of source tuples w.r.t. target
violations. We then use these scores in two algorithms that
return the most likely error source tuples (Section 5).

4.

removes the violation for the second rule in Example 1 (the
two stores would have the same number of employees), even
though NY1 as a value is not involved in the violation.
We derive from sensitivity analysis [14] our definitions of
contribution and removal scores. The intuition is that we
want to compute the sensitivity of a model to its input. In
general, given a function, the influence is defined by how
much the output changes given a change in one of the input
variables. In our context, the models are the operators in
the SQL query, which take a set of source tuples as input
and output the problematic tuples in the view.
Definition 4. A contribution score csv (c) of a problematic source cell c w.r.t. a target violation v is defined as the
difference between the original result and the updated output
after removing c divided by the number of cells that satisfy
the SQL operator.
A removal score rsv (t) of a problematic source tuple t
w.r.t. a target violation v is 1 if by removing c, v is removed, 0 otherwise.
A score vector CSV of a cell for contribution scores (RSV
of a tuple for removal scores) is a vector [cs1 , . . . , csm ]
([rs1 , . . . , rsm ]), where m is the number of violations and
cs1 , . . . , csm ∈ R (rs1 , . . . , rsm ∈ B). If a problematic cell
or tuple does not contribute to a certain violation, we put
an empty field in the vector. We will omit the subscript if
there is no confusion.
We assume that the transformation is a SPJAU query.
We compute CSVs and RSVs using the query tree. For an
SPJAU query, every node in the tree is one of the following
five operators: (1) selection (S), (2) projection (P), (3) join
(J), (4) aggregation (A), and (5) union (U).

EVIDENCE PROPAGATION

The evidence propagation module involves two tasks. The
first task is to trace the lineage of tuples in violations at the
target to source tuples. To this end, we implemented inverse
query transformation techniques proposed by Cui et al. [6].
The second task is to determine how to propagate violations
as evidence over the source.
For each tuple t in a violation v ∈ V (T ), we denote the
cells in t that are involved in v as problematic cells. These
cells are in turn computed from some source cells, also labeled as problematic. To solve a violation, we consider the
delete operation over the sources. However, as discussed
above, we do not want to identify the minimal groups of
problematic source tuples that need to be removed. On the
contrary, we take a practical approach. We look at tuples
individually by using two scores that quantify the effect of
source tuples and source cells in the lineage of each violation.
Cells Contribution. Given a violation v, we want to
measure how much the value in each problematic source cell
contributes to v. In fact, not all problematic source cells
contribute equally to v.

Example 10. Figure 5 shows the query tree for our running example. It has three operators: (1) the ./ operator,
(2) the aggregation operator with the group by, and (3) the
projection on columns Sid, Size, Grd, Region, Sal, and
Eid (not shown in the figure for the sake of space).

4.1

Computing CSVs

We compute CSVs for cells in a top-down fashion over
the operator tree. Each leaf of the tree is a source tuple,
with its problematic cells annotated
P with a CSV. Let v be a
violation in V (T ) on a rule r ∈ . We initialize, cs of each
problematic cell in target T to 1. Let I l be an intermediate
result relation computed by an operator Ol ∈ {S, P, J, A, U }
at level l of the tree, whose input is a non-empty set of
intermediate source relations Inp(Ol ) = I1l−1 , I2l−1 , . . . . In
our rewriting, we compute the scores for problematic cells
of Inp(Ol ) from the cell scores of I l .
Let cl be a problematic cell in I l , cs(cl ) its contribution
score, val(cl ) its value, and Lin(cl , I l−1 ) its lineage. Procedure 1 computes cs for intermediate cells.
Procedure 1. (Intermediate Cell CS): Let Ikl−1 be an intermediate relation contributing to cell cl . We have two
cases for computing cs(cl−1 ), cl−1 ∈ Lin(cl , Ikl−1 ):
(a) If Ol = A (cl is an aggregate cell) and r ∈ ΣD ,
then cs(cl−1 ) depends on the aggregate operator op
and on the constraint predicate P ∈ r being violated,
P : val(cil )φval(cl0 ) with φ ∈ {<, >, ≤, ≥}:
• if op ∈
{avg, sum}, then cs(cl−1 ) is

Example 8. The first violation in Example 1 covers
problematic tuples ta and tb and the problematic cells over
attributes Shop, Grd, AvgSal. The cells are in turn computed
from t11 .Sid, t1 -t4 .Grd, t1 -t4 .SId, and t1 -t4 .Sal. One of the
predicates that trigger the violation is tb .AvgSal>ta .AvgSal.
Tuple tb .AvgSal is computed from t1 .Sal, t3 .Sal and t4 .Sal.
Among them, the high value of t4 .Sal is a more likely cause
for the violation than t1 .Sal or t3 .Sal.
Tuples Removal. Wrongly joined tuples can trigger an
extra tuple in the result of a query, thus causing a violation
in the target. We want to measure how much removing a
problematic source tuple removes v.

val(cl−1 )
if φ ∈
val(gi ),gi ∈Lin(cl ,I l−1 )
val(cl−1 )
l
P
cs(c ) · (1 − val(g ),g ∈Lin(cl ,I l−1 ) )
i
i
P

Example 9. Let us assume that the correct value for
t1 .SId is a shop different from NY1, say NY2. Erasing t1

449

{<, ≤}, and
if φ ∈ {>, ≥};

T
I11
i11
i12
i13
i14
i15
i16
i17
i18
i19
i110
Emps
t1
t2
t3
t4
t5
t6
t7
t8
t9
t10

Sid [CSV]
NY1 [ 13 ,‘’]
NY1 [1,‘’]
NY1 [ 13 , ]
NY1 [ 31 , ‘’]
NY2
NY2
NY2
NY2
LA1
LA1

Size [CSV]
46 ft2 [‘’, 13 ]
46 ft2
46 ft2 [‘’, 13 ]
46 ft2 [‘’, 13 ]
62 ft2 [‘’, 12 ]
62 ft2
62 ft2 [‘’, 12 ]
62 ft2
35 ft2
35 ft2

Grd [CSV]
1 [ 13 , 31 ]
2 [1,‘’]
1 [ 13 , 31 ]
1 [ 13 , 31 ]
1 [‘’, 12 ]
2
1 [‘’, 12 ]
2
2
2

Sal [CSV]
91
, ‘’]
91 [ 300
99 [0, ‘’]
93
93 [ 300
, ‘’]
116 [ 116
,‘’]
300
89
94
91
98
94 $
116 $

Eid [CSV]
e4 [‘’, 31 ]
e5
e7 [‘’, 13 ]
e8 [ ‘’, 31 ]
e11 [‘’, 12 ]
e13
e14 [‘’, 12 ]
e18
e19
e20

I12 = T
Group By(Sid, Size, Grd, Region)
Compute Avg(Sal), Count(eid)
I11
./
Emps.Sid = Shops.Sid
I10 = Emps
I20 = Shops

Figure 4: Procedure 1 Applied on Intermediate Source I11 .
EId [CSV]
e4 [‘’, 13 ]
e5
e7 [‘’, 13 ]
e8 [‘’, 13 ]
e11 [‘’, 12 ]
e13
e14 [‘’, 12 ]
e18
e14
e18

Sal [CSV]
91
,‘’]
91 [ 300
99 [0,‘’]
93
,‘’]
93 [ 300
116 [ 116
,‘’]
300
89
94
91
98
94
116

Grd [CSV]
1 [ 13 , 31 ]
2 [1,‘’]
1 [ 13 , 31 ]
1 [ 13 , 31 ]
1 [‘’, 12 ]
2
1 [‘’, 12 ]
2
2
2

SId[CSV]
NY1 [ 13 ,‘’]
NY1 [1,‘’]
NY1 [ 13 ,‘’]
NY1 [ 13 ,‘’]
NY2
NY2
NY2
NY2
LA1
LA1

Emps

Shops

Figure 5: Query Tree.

[RSV]
[0,1]
[1,‘’]
[0,1]
[1,1]
[‘’,0]
[]
[‘’,0]
[]
[]
[]

Shops
t12
t13
t14

SId [CSV]
NY1 [2,‘’]
NY2
LA1

Size [CSV]
46 [‘’,1]
62 [‘’,1]
35

[RSV]
[1,1]
[‘’,1]
[]

Figure 7: Procedures 1 and 2 Applied on Shops.

Figure 6: Procedures 1 and 2 Applied on Emps.
• if op ∈ {max, min}, let Lin¬P (cl , Ikl−1 ) ⊆
Lin(cl , Ikl−1 ) be the subset of cells that vio1
late P , cs(cl−1 ) is
for cl−1 ∈
¬P
l l−1
|Lin

(c ,Ik

Algorithm 1: ComputeCSV(T , V (T ), S)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

)|

Lin¬P (cl , Ikl−1 ), and 0 for all other cells.
1
(b) else, cs(cl−1 ) is cs(cl ) ·
l l−1
|Lin(c ,Ik

)|

Example 11. Figure 4 reports the CSVs of problematic
cells in the intermediate relation I11 . These are computed by
rewriting I12 , which is T , as shown in Figure 5. For example,
tb .Grd is computed from cells i11 .Grd, i13 .Grd, and i14 .Grd.
By case (b) these cells get a score of 13 .
Similarly, tb .AvgSal is aggregated from i11 .Sal, i13 .Sal, and
1
i4 .Sal, and ta .AvgSal from i12 .Sal. By case (a) the scores
of i11 .Sal, i12 .Sal, i13 .Sal, and i14 .Sal are based on the values
of the cells, as shown in Fig. 4. Score of i12 .Sal is computed
as 0 using the first part of case (a).

14:
15:
16:

Procedure 1 has two cases depending on the query operators and Σ. In case (a), where an aggregate is involved in
a violation because of the operator of a rule, we have additional information with regards to the role of source cells in
a violation. In case (b), which involves only SPJU operators where the source values are not changed in the target,
we uniformly distribute the scores of the problematic cells
across the contributing cells. Notice that case (a) applies
PD
PP
for
only, since the actual test in
is not known.
However, case (b) applies for both types of rules.
An intermediate source cell may be in the lineage of several intermediate problematic cells. In this case, their cell
scores are accumulated by summation following Procedure 2.

OT ← Operator that generated T
h ← Highest level of query tree
Inp(OT ) ← I1h−1 , . . . , Irh−1
h
rstate ← (T, OT , Inp(OT ))
stateStack ←new Stack()
stateStack.push(rstate)
for each violation v ∈ V (T ) do
while !stateStack.empty() do
nextState ← stateStack.pop()
if nextState[1] is T then
pcells ← v c (T ) {Problematic cells at T}
else
pcells ← Lin(v c (T ), nextState(1)) {Problematic
cells at an intermediate relation}
for each cell c ∈ pcells do
computeScores(c, v, l, nextState)
for each intermediate relation I l−1 ∈ nextState[3]
do
Apply Procedure 2 on problematic cells of I l−1
Ol−1 ← operator that generated I l−1
newState ← (I l−1 , Ol−1 , Inp(Ol−1 ))
stateStack.push(newState)

17:
18:
19:
20:
21:
22: function computeScores(c, v, l, nstate)
23: for each intermediate relation I l−1 ∈ nstate[3] do
24:
Apply Procedure 1 on c, nstate[2], I l−1

Example 12. In Fig. 7, CSVs of t12 .SId for the violation between ta and tb are computed from 4 cells in the intermediate relation I11 in Figure 4. Cells i11 .SId, i13 .SId, i14 .Sid
have a score of 13 and i12 .Sid has a score 1. Procedure 2
computes cs(t12 .Sid) =2 w.r.t. this violation.

Procedure 2. (Intermediate Cell Accumulation): Let
O = O(cl−1 , I l ) denote the set of all cells computed from
cell cl−1 ∈ Ikl−1 in the intermediate result relation I l by opP
erator O, cs(cl−1 ) = cl ∈Ol cs(cl−1 , cl ).
l

450

Given a target relation T , its violation table V (T )
and source relations S, Algorithm 1 computes CSVs of
the problematic cells. The algorithm defines a state as
a triple (I l , Ol , Inp(Ol )).
It initializes the root state
(T, OT , Inp(OT )) (line 4), where OT is the top operator in
the tree that computed T . For each violation v and for each
problematic cell c, we compute the scores of problematic
cells (lineage of c) in all relations in Inp(OT ) (Lines 10-13)
using Procedure 1 (Line 24). For each intermediate relation
in Inp(OT ), we use Procedure 2 to accumulate the cs scores
of each problematic cell and compute its final cs score w.r.t.
the violation v (Lines 16-17). We then add new states to the
stack for each relation in Inp(OT ). The algorithm computes
scores all the way up to source relations until the stack is
empty, terminating when all the generated states have been
visited. Examples of CSVs are shown in Figures 6 and 7.
Once CSVs are computed for cells, we compute them for
tuples by summing up the cell scores along the same violation while ignoring non contributing cells.

4.2

Definition 5. Given two source tuples s1 and s2 in v, we
define their distance as:
D(s1 , s2 ) = |(csv (s1 ) + rsv (s1 )) − (csv (s2 ) + rsv (s2 ))|
Two tuples with high scores are expected to have a smaller
distance between them than the distance between a highscoring tuple and a low-scoring one. Our goal is to obtain
an optimal separation between high- and low-scoring tuples.
Let Hv be the set of high-scoring tuples and Lv the set
of low-scoring ones. Intuitively, a separation is preferable to
another one if by adding a tuple s ∈ Lv to Hv , the difference
between the sum of pair-wise distances among all tuples of
Hv ∪ {s} and the sum of their scores becomes smaller. The
intuition is clarified in the following gain function.
Definition 6. Let the score of a tuple si for violation v
be cv (si ) = (csv (si ) + rsv (si )). Let Lin(v, S) consists of the
lineage tuples of v in S and Lv be a subset of Lin(v, S). We
define the separation gain of Lv as:
X
X
X
SG(Lv ) =
(cv (s)) −
D(sj , sk )

Computing RSVs

s∈Lv

In contrast to contribution scores, removal scores are directly computed on tuples and are Boolean. If a violation
can be eliminated by removing a source tuple, independently
of the other tuples, then such a tuple is important. This
heuristics allow us to identify minimal subsets of tuples in
the lineage of a violation that can solve it through removal.
Instead of computing all subsets, checking for each source
tuple allows fast computation.
We use a simple bottom-up algorithm to compute RSVs.
It starts with the source tuples in the lineage of a violation.
For each source relation S and for each problematic tuple
s ∈ S, it removes both s and the tuples computed from it
in the intermediate relations in the path from S to T in the
query tree. If the violation is removed, we assign a score
1 to si , 0 otherwise. RSVs for the source relations in the
running example are shown in Figures 6 and 7.

5.

We define an optimal separation as the one that maximizes
this function for Hv .
Example 13. Consider six source tuples for a violation v having scores {s1 :0.67, s2 :0.54, s3 :0.47, s4 :0.08,
s5 :0.06, s6 :0.05}. The sum of pair-wise distances for Hv =
{s1 , s2 , s3 } is 0.24, while the sum of scores is 1.68, thus
SG(Hv )=1.44. If we add s4 to Hv , the pair-wise distances
of Hv0 : {s1 , s2 , s3 , s4 } raises to 1.67 and the sum of scores
to 1.76. Clearly, this is not a good separation, and this is
reflected by the low gain SG(Hv0 )=0.08. Similarly, if we remove s3 from Hv the new SG also decreases to 1.14.
As it is exponential in the number of subsets to obtain an
optimal separation, we provide a greedy heuristic to compute its approximation using ideas from the nearest neighbor chain algorithm for agglomerative clustering [18]. We
first order all the tuples in Lin(v, S) in the descending order
of their scores, and designate each tuple as its own cluster.
We start with the highest scoring tuple’s cluster, and keep
adding to it the next tuple in the order, while computing the
separation gain at each step. We terminate after reaching
a separation where the gain attains a local maximum. We
generate two clusters, the cluster that is being extended and
the subset of tuples that are not in this cluster. In Example 13, the gain after adding s1 , s2 , and s3 is 0.6, 1.14, and
1.44, respectively. After adding s4 , the gain becomes 0.08
and therefore we stop at s3 . From each violation v, its Hv is
added to the set of most likely source error tuples. The algorithm requires a linear space and quadratic time (pair-wise
distances) in the number of tuples.

LIKELY ERRORS DISCOVERY

Given the target violations, we use our scoring methods
3
to identify the most likely errors at the source (scenarios 
4 Since the goal is to correctly separate the potential
and 
).
error tuples from non-error tuples, we use the intuition that
most likely errors are expected to have higher scores. A topk analysis of the tuples’ scores for each violation can identify
potential errors. However, there is no k that works for all
scenarios.
We present two approaches to solve this problem. In the
first approach, we design an outlier function to separate high
and low scoring tuples for each violation. In the second
approach, we show a reduction from the facility location
problem and apply a polynomial time log n-approximation
algorithm to compute the likely source errors [12].

5.1

1≤j<|Lv | j<k≤|Lv |

5.2

Global Error Separation

Since we have multiple violations, instead of looking at
scores locally per violation, we introduce an alternative approach that looks for the most likely error tuples globally.
We accumulate evidences coming from multiple violations
as in the following example.

Distance Based Local Error Separation

In several cases (such as queries with aggregates), source
tuples in the lineage of a violation consist of a subset of
tuples that have high scores based on our scoring model,
while the remaining have low scores. To precisely measure
the distance between tuples, we define it as follows.

Example 14. Consider two violations v1 and v2 , and
four source tuples s1 –s4 . Let the scores of the tuples be v1 :
(s1 [0.8], s2 [0.1], s3 [0.1]), v2 : (s3 [0.5], s4 [0.5]). Here, s1 is

451

the most likely error tuple for v1 and s3 is the one for v2 as
it is the one that contributes most over the two violations.

Favor precision w.r.t. to recall is desirable, as it is easier to
discover explanations from fewer errors than discover them
from a mix of error and clean tuples. This will become
evident in the experiments.

The goal is to select a subset of tuples that globally contribute the most to the violations. We can formulate this
problem using the known NP-Hard uncapacitated facility
location problem (FLP) [17]. The uncapacitated facility location problem is described as follows.
a. a set Q = {1, . . . , n} of potential sites for locating facilities,
b. a set D = {1, . . . , m} of clients whose demands need
to be served by the facilities,
c. a profit cqd for each q ∈ Q and d ∈ D made by serving
the demand of client d from the facility at q,
d. a non-negative cost fq for each q ∈ Q associated with
opening the facility at site q.
The objective is to select a subset Q ⊆ Q of sites to open
facilities and to assign each client to exactly one facility s.t.
the difference of the sum of maximum profit for serving each
client and the sum of facility costs is maximized, i.e.,
X
X
argmax(
max(cqd ) −
fq )
Q⊆Q

d∈D

q∈Q

6.

q∈Q

We obtain a reduction from the FLP to the problem of
computing most likely errors in PTIME in the number of
violations and in the number of source tuples. For each client
d, we associate a violation vj ∈ V (T ). Let Lin(V (T ), S) =
∪vj ∈V (T ) Lin(vj , S), n = |Lin(V (T ), S)|, and m = |V (T )|.
For each site q, we associate a source tuple in Lin(V (T ), S).
For each tuple s in Lin(vj , S), we associate the cost cqd
between site q (s) and client d (vj ) with the score (csj (s) +
rsj (s)). We assume the fixed cost fq of covering a source
tuple to be 1. A solution to our problem is optimal if and
only if a solution to the facility location problem is optimal.
We present a greedy heuristic [17] for this problem as follows.
We start with an empty set Q of tuples, and at each step
we add to Q a tuple s ∈ Lin(V (T ), S) \ Q that yields the
maximum improvement in the objective function:
X
X
f (Q) =
max(cqd ) −
fq
d∈D

q∈Q

w(q) = |E(R) \ cover(q)| + λ ∗ |cover(q) ∩ C|
Our goal is to cover in E the tuples in E(R), while minimizing the sum of weights of the queries in E. An explanation is optimal if and only if a solution to the weighted set
cover is optimal. By using the greedy algorithm for weighted
set cover [4], we can compute a log(|E(R)|)-approximation
to the optimal solution. The explanation is constructed
incrementally by selecting one query at a time. Let the
marginal cover of a new query q w.r.t. E be defined as the
number of tuples from (R) that are in q and that are not
already present in E:

q∈Q

For a tuple s ∈ Lin(V (T ), S)) \ Q, let ∆s (Q) = f (Q ∪
{s}) − f (Q) denote the change in the function value. For a
violation vj , let uj (Q) be max(csj (s) + rsj (s)), and uj (∅) =

mcover(q) = (q ∩ E(R)) \ (E ∩ E(R))

s∈Q

0. Let δjs (Q) = csj (s) + rsj (s) − uj (Q). Then, we write
∆s (Q) as follows:
∆s (Q) = f (Q ∪ {s}) − f (Q)
X  δ (Q) if δ (Q) > 0 
js
−1
=
( 0js
otherwise

EXPLANATION DISCOVERY

The problem of explanation discovery pertains to selecting an optimal explanation of the problematic tuples from a
large set of candidate queries. An optimal explanation covers the most likely error tuples, while minimizing the number
of clean tuples being covered and the size of the explanation.
We compute optimal explanations in two steps. We first
determine candidate queries. We then use a greedy algorithm for the weighted set cover, with weights based on the
function over query q defined in Section 2.
Candidate Queries Generation The goal is to generate
the candidate queries for a source S with d dimensions. The
algorithm first generates all queries with a single predicate
for each attribute Al of R, s.t. the queries cover at least one
tuple in E(R). A data structure P [1..d] is used to store the
queries of the respective attributes. The algorithm then has
a recursive step in which queries of each attribute (Al0 ) are
expanded in a depth-first manner by doing a conjunction
with queries of attributes Al . . . Ad where l = l0 + 1. The
results of the queries are added to a temporary storage P 0
and are expanded in the next recursive step.
Computing Optimal Explanations In the second
stage, we compute the optimal explanation from the generated candidate queries. In Section 2, we defined the weight
associated with each query as follows.

Algorithm 2: GreedyPDC(candidate queries P over R)
1: Eopt ← {}
2: bcover(Eopt ) ← {}
3: while bcover(Eopt ) < |E(R)| do
4:
minCost ← ∞
5:
min q ← null
6:
for each query q ∈ P do
w(q)
7:
cost(q) ← mcover(q)
8:
if cost(q)≤ minCost then
9:
if cost(q)= minCost and
bcover(q) < bcover(min q) then
10:
continue to next query
11:
min q ← q
12:
minCost = cost(q)
13:
Add min q to Eopt
14:
bcover(Eopt ) ← bcover(Eopt ) ∪ bcover(min q)

(1)

vj ∈V (T )

The −1 corresponds to the cost of covering a tuple. In
each iteration, of the heuristic, ∆s (Q) is computed for each
s ∈ Lin(V (T ), S)) \ Q. We add a tuple s whose marginal
cost ∆s (Q) is maximum. The algorithm terminates if either
there are no more tuples to add or if there is no such s with
∆s (Q) > 0.
The algorithm identifies tuples whose global (cumulative)
contributions (to all violations) is significantly higher than
others. This global information leads to higher precision
compared to the distance based error separation, but to a
lower recall if more than one tuple is involved in a violation.

452

At each step, Algorithm 2 adds to E the query that minimizes the weight and maximizes the marginal cover. Let
bcover(q) = E(R) ∩ cover(q), similarly for bcover(Eopt ).
Parameter λ weighs the relative importance of the clean
tuples w.r.t. errors. In practice, the number of errors in a
database is a small percentage of the data. If clean and erroneous tuples are weighted equally in the weight function,
selective queries that do not cover clean tuples are favored.
This can lead to a large explanation size. We set the parameter λ to be the error rate, as it reflects the proportion
between errors and clean tuples. If the error rate is very low,
it is harder to get explanation with few clean tuples, thus we
give them a lower weight in the function. If there are many
errors, clean tuples should be considered more important in
3 and
taking a decision. For mining at the source level (
4 in Figure 2), we estimate the error rate by dividing the

number of likely errors by the number of tuples in the lineage of the transformation (either violations or errors from
the target).

7.

We consider two parameters for inducing errors w.r.t.
ground explanation Eg : source error rate e (ratio of number of error tuples to |Lin(V (T ))|), and explanation rate n
(a fixed percentage of e · |Lin(V (T ))|). Error rate e corresponds to the total number of errors to induce, while n
corresponds to the number of such error tuples that should
satisfy the ground explanation. The remaining errors, i.e.,
(1 − n) · e · |Lin(V (T ))|, are induced on random tuples which
do not match the ground explanation (Lin(V (T )) \ Eg ).
For each r ∈ ΣD and for each predicate P ∈ r, we identify
the corresponding attributes AP and tuples in cover(Eg ),
and modify their values up to the budget of errors. We make
sure that errors are detectable over the target schema. In
scenarios S1 and S2, one error tuple at the source is sufficient
to detect a target violation, while for S3 we need to introduce
two or three errors to induce a target violation.
Metrics. We introduce two metrics to test DBRx. For each
metric, we show how to compute precision(P) and recall (R).
Error Discovery Quality – evaluates the quality of the
likely errors discovery and the scoring. We compare the errors computed by Error over the lineage versus the changes
introduced in the errors induction step (B).

EXPERIMENTS

An end-to-end evaluation of our system requires a setup
with one or more source relations and a set of target schemas
on which business rules can be defined. In Sec. 7.1, we test
the quality of error and explanation discovery modules with
datasets from the TPC-H benchmark. In Sec. 7.2, we compare the explanations computed by DBRx against two alternative systems on five real-world datasets.

Explanation Quality – evaluates the quality of the discovered explanations. We measure the quality of an explanation computed by DBRx by testing the tuples overlap with
the ground explanations.

7.1

PE =

PErr =

Synthetic Dataset

The TPC-H Benchmark data generator defines a general
schema typical of many businesses. We picked two representative queries as target schemas, namely Q3 and Q102 , and
defined three scenarios with the following rules in ΣD :
Scenario S1. cQ10 :q(tα .revenue > δ1 ).
Scenario S2. c0Q10 :q(tα .name = tβ .name∧
tα .c phone[1, 2] 6= tβ .c phone[1, 2]).
Scenario S3. cQ3 :q(tα .revenue > δ2 ),
c0Q3 :q(tα .o orderdate = tβ .o orderdate∧
tα .o shippriority 6= tβ .o shippriority).
Rules cQ10 and cQ3 are check constraints over one tuple,
while c0Q10 and c0Q3 are FDs over pairs of tuples. In these
scenarios, we focus on ΣD to show the impact of (i) the
repair computation over the target and of (ii) the role of
Error in the source. We use ΣP in the real-data study.
Error Induction on TPC-H. We generate instances and
queries using dbgen and qgen tools, respectively. We assign
values to parameters in the rules s.t. the reports have no
violations. We then add errors in the source relations s.t.
the reports have violations when recomputed.
Each experiment has a source instance D, a transformation Q, and target rules Σ. We identify candidate source
attributes A based on the lineage of the attributes in the
rule. Since our goal is to explain errors, we induce errors
s.t. they happen on tuples covered by some pre-set explanations, or ground explanations. This allows us to test how
good we are at recovering these explanations. We induce
errors for a given ground explanation over the source, such
as Eg = {q1 : (lineitem.l ship = R), q2 : (lineitem.l ship =
S)}, while enforcing that attributes in Eg are not in A.
2

E(T ) ∩ B
E(T )

RErr =

E(T ) ∩ B
B

cover(Eopt ) ∩ cover(Eg )
cover(Eopt ) ∩ cover(Eg )
RE =
cover(Eopt )
cover(Eg )

Algorithms. We implemented the algorithms introduced
in the paper and baseline techniques to compare the results.
For scoring, we use the technique based on outliers detection (Local Outliers) and the one based on the facility
location problem (Global-FLP). As baselines, we consider
all the tuples in the lineage with the same base score 1 (NoLet), and the tuple(s) with the highest score for each violation (Top-1). For explanation discovery, we implemented
Algorithm 2.
Results. We discuss three experiments designed to measure the effectiveness and efficiency of the modules in DBRx.
Since we can compute target repairs for these scenarios, we
3 and mining
discuss mining on propagated target errors (
)
4 All measures refer to
on propagated target violations (
).
the relations where errors have been introduced.
Experiment A: Quality of Error Discovery. We start
testing the quality of the alternative Error implementations. For space reason we report the error F-measure.
In ExpA-1, we fix the queries in the ground explanation and increase the error rate without any random errors
(n = 1). We start by discussing the results for the case
4
of the rewriting of the target violations (
).
Figure 8a
shows that all the methods perform well for S1, with the
exception of No-LET. This shows that computing errors is
easy when there is only a simple check constraint over an
aggregate value. Figure 8b shows that only Global-FLP
obtains high F-measure with S2. This is because it uses the
global information given by the many pair-wise violations.
Figure 8c shows that for S3, which has multiple constraints
and multiple errors, the methods obtain comparable results.
However, a close analysis shows that, despite that having

The TPC-H documentation [20] contains the SQL code.

453

4
(a) Errors F, S1 

4
(b) Errors F, S2 

4
(c) Errors F, S3 

3
(d) Errors F, S2 

4
(e) Expl. F, S1 

4
(f) Expl. F, S2 

4
(g) Expl. F, S3 

3
(h) Expl. F, S2 

4
4
4
4
(i) Err. F, Random, S3 
(j) Expl. F, Random, S1 
(k) Expl. F, Random, S2 
(l) Expl. F, Random, S3 
Figure 8: Experimental results for the prescriptive data cleaning problem.
multiple errors violate the hypothesis of Global-FLP, it
still achieves the best precision, while the best recall is obtained by No-LET. Figure 8d shows again S2, but com3 Compared
puted on the rewriting of the target repair (
).
to Figure 8b, Global-FLP does slightly worse, while all the
others improve. This reflect the effect of the target repair
at the source level: it gives less, but more precise, information to the error discovery. This provides less context to
Global-FLP, which takes advantage of the larger amount
4 Similar behavior is observed in S3.
of evidence in 
.
In ExpA-2, we study how having 50% random errors affects the quality of error discovery. Figure 8i reports the
error F-measure for S3. Despite the random errors, the results do not differ much from the simpler scenario in Figure
8c. We observed similar results for S1 and S2.
Experiment B: Quality of Explanations. We test the
quality of the explanations with different Error modules.
In ExpB-1, we study how increasing the error rate affects
the results. Figure 8e shows that all the errors detection
methods have similar results for S1, with the exception of
No-LET. This is not surprising, as in this scenario errors are
easy to identify and the size of the aggregate is large. Figure
8f reflects the quality of the error detection of Global-FLP
(as seen in Figure 8b) on the quality of the explanation for
S2 8g shows that the precision in error detection has higher
impact than the recall for the discovery of explanation. As
discussed for Figure 8c, Global-FLP has the highest precision for S3, but the lowest recall. This shows that is better
to identify fewer errors with higher confidence. Figure 8h

shows the explanation F-measure for S2 on the rewriting
3
of the target repair (
).
Compared to Figure 8f, most of
the methods improved their quality, while Global-FLP’s
quality decreased. This is a direct consequence of the detect
error quality shown in Figure 8d. Examples of ground and
discovered explanations are reported in Figure 9.
In ExpB-2, we study how having 50% random errors affects the quality of the discovered explanations. Figures 8j
and 8k show the explanation’s F-measure for scenarios S1
and S2, respectively. Despite the error discovery did not
change much with the random noise, the discovery of explanation is affected in these two scenarios, as it is clear
from the comparison with Figures 8e and 8f. This behaviour
shows that the quality of the explanations is only partially
related to the error detection and that a large amount of errors that cannot be explained can make hard the discovery
of existing explanations. Fortunately, results on real data
show that useful explanations can still be discovered. Moreover, Figure 8l shows consistent result for S3 w.r.t. the case
without random errors (Figure 8g). This shows that the accumulation of errors from two different quality rules has a
strong effect even in noisy scenarios.
Experiment C: Running Time. We measured the average running time for TPC-H data of size 10 and 100 MB.
For the 100 MB dataset and S1, the average running time
across different error rates is 100.29 seconds for rewriting the
violations and computing their score. The average running
time for the Error function is less than 2 seconds. The
pattern mining including the candidate pattern generation

454

Exp.

Ground Explanation
shipmode = RAIL ∧
shipinstruct = TBR
shipmode=SHIP ∧
shipinstruct=DIP

No-LET

Top-1

• l returnf lag = R

• l
l
• l
l

S1
4


• l
l
• l
l

S2
3


• c mktsegment =
• c nationkey = 3
HOU SE ∧ c author = a1
• c nationkey = 20
• c mktsegment =
• c nationkey = 16
AU T O ∧ c author = a2

Local Outliers

Global FLP

shipmode = RAIL ∧ • l shipmode = RAIL ∧ • l shipmode = RAIL ∧
l shipinstruct =TBR
l shipinstruct =TBR
shipinstruct =TBR
shipmode = SHIP ∧ • l shipmode = SHIP ∧ • l shipmode = SHIP ∧
shipinstruct =DIP
l shipinstruct =DIP
l shipinstruct =DIP

• c nationkey = 3
• c nationkey = 20
• c nationkey = 16

• c nationkey = 3
• c nationkey = 20
• c nationkey = 16

• c mktsegment =
HOU SE ∧ c author = a1
• c mktsegment =
AU T O ∧ c author = a2

Figure 9: Explanation output for scenarios S1 and S2.
took 52 seconds. The results for S2 and 100 MB vary only in
the rewriting module, as it took 430 seconds because of the
large number of pair-wise violations. The execution times
for 10 MB are at least 10 times smaller with all modules.

7.2

with an attribute source that keeps track of the source relation. The target rule is name → birthdate.

Real Data

We run DBRx using the Global-FLP technique on five
real-world scenarios with different types of data quality
rules. In some of the scenarios, we also compare DBRx with
Scorpion [21] and the technique on tracing data errors [16].
Since ground explanations are not available, we measure the
output quality only in terms of precision of the explanations.
We manually mark an explanation as correct based on our
own knowledge of the data. The precision is the number
of correct queries in the optimal explanation over the total
number of queries in the explanation.
Datasets and Data Quality Rules on Target. The five
scenarios we evaluate are described as follows:
t sensors [21] The source consists of sensor data with 2.3M
tuples over 7 attributes. The target is a transformation
that averages temperatures grouped by hour over a selection of dates. The rule over the target is a check constraint
(avg(temperature) < 23).
elections [21] The source contains 18 months campaign expenses from the 2012 US Presidential Election in a 14 attributes and 116K tuples table.The target reports total expenses of Barack Obama on each date, and the rule constrains this amount to be less than $10M on any date.
p sensors [16] In this scenario, measurements of nine mobile
phone sensors are recorded and classifiers are defined to determine five Boolean variables. The classifiers act as transformations and the measurements as source data. Classifiers
that do not determine the variables correctly due to input
errors are identified by comparing their output against a
ground truth with a rule expressed as a Java code in ΣP .
stocks We constructed two tables about stocks. The first,
namely Q, contains daily stock quotes of S&P 500 companies for a two-month period from a trusted source with 30K
tuples. The second table is built by extracting mentions of
companies and their stock quotes from 45k Bloomberg articles during the same time period. On every page, we ran two
regular expression based extractors (E1 and E2) and an induction based one (E3). We mapped the two sources to the
target schema [company, date, stock price, src], where the
attribute src was either extracted or master. We cleaned
the output of E2 to ensure that its tuples are correct. A
target rule in Java code (ΣP ) states that two tuples are in
violation if, given the same company and the same date, the
difference among the price values is higher than 10% of the
one coming from Q.
players In this scenario, we obtained data with information
about soccer players from 6 web sites. The data has 7 attributes. The transformation is the union of the 6 sources

Figure 10: Precision of explanations with real datasets.
Algorithms and Results. Figure 10 shows the results we
obtained by running DBRx on the five scenarios and for different cases. For t sensors and elections, we compare the
3 with Scoroutput of our mining at the source (case 
)
pion [21]; we both achieve the same results. For t sensors,
the explanation responsible for high temperatures is sensorid=15. For elections, the explanation for high expenses
is recipient st=‘DC’ ∧ recipient nm=‘GMMB INC.’. For
p sensors, we could only obtain a fragment of the data used
in [16]. This data induces errors in one of the observations
from the sensor “gps”. We were able to retrieve this pattern
by applying our techniques with the same performance of
the system in [16].
In scenarios stocks and players, we mine both the target
and the source and present their results in Fig 10, thus showing all of cases targeted by DBRx. Alternative techniques [22,
21, 16] do not apply here, since we have arbitrary SQL in the
transformation, and declarative constraints as target rules,
thus the ground truth is not available.
For stocks, the system is able to compute explanations
with perfect precision when mining on errors E(T ), both
2 (src=extracted) and 
3 (extractor=E1 ∧ extracfor cases 
tor=E3), while some mistakes are made when mining the
1 and 
.
4 This reflects the
violation (V (T )), in both cases 
impact of a repair of very high quality at the target, which
is possible because a reliable source is available.
Things change slightly for the more complicated case of
players. Here there are six sources that often disagree and
may fail to form clear majorities over correct values for the
birthdate of a player. This is reflected in a low precision in
2 and 
3 show the positive impact
all cases, but again cases 
of the error computation at the target. Examples of correct
explanations at the source level are src=espn ∧ birthdate=0
and src=footymania ∧ birthdate=0/-1/2000.

8.

RELATED WORK

Provenance. Several proposals tackled the problem of
verifying the semantic correctness of data transformations
by pointing at anomalies in the results. These have been
mainly termed as “Why questions” [2] and “Why-Not ques-

455

9.

tions” [11, 19]. In the first case, the system finds the origin
of some tuples or cells in the results. Provenance can be
useful to extend DBRx to transformations expressed as black
boxes. In particular, a transformation system (e.g., a Java
program) supporting the eager approach (aka bookkeeping)
carries extra annotations that can be used to produce evidence tables. Our rewriting technique is a lazy approach
to provenance that can be readily deployed on an existing
system supporting SPJUA queries.
In the second case, we look for explanations about tuples
that were expected but are missing from the results. Two
models have been proposed for this case. One adjusts the
query to provide the desired output [19]. Such model does
not apply to DBRx because we trust the transformations. The
other model explains a missing tuple t in terms of insertions
to the database s.t. t appears in the result. We can extend
DBRx with this model by allowing inclusion dependencies in
ΣD and implementing existing algorithms [11] in the evidence propagation module.
Causality. There have been proposals to discover explanations to problematic values in the results of an aggregate
query [22, 21] or of a transformation process [16]. These
share the same goals as our proposal. However, they have
limitations that limit their applicability. Scorpion [22, 21]
works on aggregate queries only, and target constraints are
limited to one tuple check constraint with a variable and a
constant. It lacks the support for arbitrary Σ and arbitrary
SQL, which are contributions of our work. CARE [16] requires the availability of the ground truth in order to detect
errors. This is not realistic in a data cleaning setting. Moreover, it requires lineage information and it does not tackle
the problem of propagating the evidence to the sources.
Similar attempts over probabilistic databases (e.g., [14])
also rank “sensitive” individual tuples by interest. However,
they do not construct explanations based on predicates.
Dependency Propagation. The problem of propagating dependencies is to determine, given a target view over
the sources and their dependencies, if another dependency
holds on the view. We address the inverse process with SPJAU views; this is not supported by existing approaches and
leads to undecidability [9]. Moreover, our instance-driven
rewriting allows extension for scenarios where the transformation is not a query, but a black box with provenance.
View Updates. In the view update problem, the goal is
to minimize the number of tuples to delete in the sources,
such that the desired change in the target is obtained and no
other target tuples are modified. The side-effect free variant
of this problem is related to our rewriting from target to
source. Unfortunately, the problem is intractable even for
views defined in terms of simple SPJU queries [5]. This
intractability motivated our scoring scheme; we drop the
requirement to solve the view update in an exact fashion
and opt for scores that can be computed efficiently.
Data Cleaning. Data cleaning focuses on detecting and
repairing errors on a database by using declarative constraints [3, 15, 7, 10, 1]. In our target Error module, we
can use any of these algorithms. However, they rely on properties of the violations that do not apply when the violations
are rewritten over the sources. Thus, they cannot be used
at the source Error module. Extending them to compute
a target repair through updates on the sources requires to
solve the view update problem and is thus not tractable.

CONCLUSIONS

Given a view over sources and a set of quality rules over
it to identify violations, we introduced explanations both at
the target and at the source levels for the problematic data.
To make these explanations easy to consume for users, we
formulated a problem that minimise their size while guaranteeing coverage of the violations. The main intuitions behind
this work are that (i) violations at the target level can be
expressed as evidence of problems over the sources and (ii)
summarising such evidence leads to meaningful explanations
of the problems. We plan to extend this work by considering
multi-level transformations, such as ETL processes, where at
each step rules for data cleaning can be enforced.

10.

REFERENCES

[1] G. Beskales, I. F. Ilyas, and L. Golab. Sampling the repairs
of functional dependency violations under hard constraints.
PVLDB, 3(1):197–207, 2010.
[2] J. Cheney, L. Chiticariu, and W. C. Tan. Provenance in
databases: Why, how, and where. Foundations and Trends
in Databases, 1(4):379–474, 2009.
[3] X. Chu, I. F. Ilyas, and P. Papotti. Holistic data cleaning:
Putting violations into context. In ICDE, 2013.
[4] V. Chvatal. A greedy heuristic for the set-covering problem.
Mathematics of operations research, 4(3):233–235, 1979.
[5] G. Cong, W. Fan, F. Geerts, J. Li, and J. Luo. On the
complexity of view update analysis and its application to
annotation propagation. IEEE TKDE, 24(3):506–519, 2012.
[6] Y. Cui and J. Widom. Practical lineage tracing in data
warehouses. In ICDE, pages 367–378, 2000.
[7] M. Dallachiesa, A. Ebaid, A. Eldawy, A. Elmagarmid,
I. Ilyas, M. Ouzzani, and N. Tang. Towards a commodity
data cleaning system. In SIGMOD, 2013.
[8] W. Fan and F. Geerts. Foundations of Data Quality
Management. Morgan & Claypool Publishers, 2012.
[9] W. Fan, S. Ma, Y. Hu, J. Liu, and Y. Wu. Propagating
functional dependencies with conditions. PVLDB,
1(1):391–407, 2008.
[10] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. The
Llunatic Data-Cleaning Framework. PVLDB,
6(9):625–636, 2013.
[11] M. Herschel and M. A. Hernández. Explaining missing
answers to spjua queries. PVLDB, 3(1):185–196, 2010.
[12] D. S. Hochbaum. Heuristics for the fixed cost median
problem. Mathematical programming, 22(1):148–162, 1982.
[13] W. H. Inmon. Building the Data Warehouse. John Wiley
Publishers, 2005.
[14] B. Kanagal, J. Li, and A. Deshpande. Sensitivity analysis
and explanations for robust query evaluation in
probabilistic databases. In SIGMOD, pages 841–852, 2011.
[15] S. Kolahi and L. V. S. Lakshmanan. On approximating
optimum repairs for functional dependency violations. In
ICDT, 2009.
[16] A. Meliou, W. Gatterbauer, S. Nath, and D. Suciu. Tracing
data errors with view-conditioned causality. In SIGMOD,
pages 505–516, 2011.
[17] P. B. Mirchandani and R. L. Francis. Discrete location
theory. 1990.
[18] F. Murtagh. Clustering in massive data sets. In Handbook
of massive data sets, pages 501–543. Springer, 2002.
[19] Q. T. Tran and C.-Y. Chan. How to conquer why-not
questions. In SIGMOD, pages 15–26, 2010.
[20] Transaction Processing Performance Council. The TPC
Benchmark H 2.16.0. http://www.tpc.org/tpch, 2013.
[21] E. Wu and S. Madden. Scorpion: Explaining away outliers
in aggregate queries. PVLDB, 6(8):553–564, 2013.
[22] E. Wu, S. Madden, and M. Stonebraker. A demonstration
of dbwipes: Clean as you query. PVLDB, 5(12):1894–1897,
2012.

456

Big Data Quality – Whose problem is it?

I.

Shazia Sadiq

Paolo Papotti

The University of Queensland
Brisbane, Queensland, Australia
shazia@itee.uq.edu.au

Arizona State University
Tempe, AZ, USA
ppapotti@asu.edu

I NTRODUCTION

The increased reliance on data driven enterprise has seen an
unprecedented investment in big data initiatives. Organizations
averaged US$8M in investments in big data-related initiatives
and programs in 2014, with 70% of large enterprises and
56% of small and medium enterprises (SMEs) having already
deployed, or planning to deploy, big-data projects [1]. As
companies intensify their efforts to get value from big data,
the growth in the amount of data being managed continues
at an exponential rate, leaving organizations with a massive
footprint of unexplored, unfamiliar datasets. On February 8th ,
2015, a group of global thought leaders from the database
research community outlined the grand challenges in getting
value from big data [2]. The key message was the need to
develop the capacity to ‘understand how the quality of data
affects the quality of the insight we derive from it’.
Poor data quality is being termed as the dark side of big
data, inhibiting the discovery of new insight and value [3]. IBM
reports that only one in three business leaders trust the information they use to make decisions [4]. Yet, there is a plethora
of reports on the value of initiatives that stem from processing
of large and diverse datasets [5], [6]. Value is advocated as
the 5th V of big data (together with volume, velocity, variety,
and veracity), and is arguably the promise of the big data era.
Organizational big data investment strategies regarding what
data to collect, clean, integrate, and analyze are typically driven
by some notion of perceived value. We note that the value of
the dataset is inescapably tied to the underlying quality of
the data [7]. The last 20 years of data quality research [8],
[9] have been based on this fundamental principle of fitness
for use [10]. However for big data, value and quality may be
correlated, but they are conceptually different [2]. For example,
a complete and accurate list of names of all countries in Asia
may not have much value. Whereas incomplete and noisy GPS
data from public transport vehicles may have a high perceived
value for transport engineers and urban planners.
In the current data landscape, users are confronted with
new, often externally sourced, potentially very large datasets,
with little or no knowledge of its schemata that arguably have
relevance and perceived value for them. Knowledge of data
quality discovery and repair methods is imperative to develop
an organizational and technological capacity to assess and
predict, to a desired level of confidence, the perceived value
of a dataset. Data quality discovery and repair techniques are
highly contextual and their success depends on their fitness
against both the data quality dimension (e.g., completeness,
consistency, timeliness, accuracy, reliability) as well as the
type of data (e.g., structured/relational, text, spatial, time series,

978-1-5090-2020-1/16/$31.00 © 2016 IEEE

social/graph, RDF/web). The techniques include those driven
by logic such as data dependency constraints and integrity
rules; those driven by numerical and statistical approaches;
and several others such as probabilistic, learning and empirical
methods. This diversity in techniques and settings leads to
different positions on fitness and makes the topic well suited to
an open discussion in order to enhance broader understanding
and raises awareness.
II.

PANEL OVERVIEW

Given that the ICDE community has a long history of
contributions on data integrity and control, the topic of big
data quality is of high interest. In the last 3 years, there have
been 22 papers at ICDE on the topic of data quality indicating
a high level of expertise and interest within the community. We
aim to present a panel discussion at ICDE2016 where panelists
will debate and attempt to reach consensus on the following
questions:
•

Under what settings a method is better than another?
Can we identify the right method beforehand or there
is no hope beyond trial and error?

•

How to evaluate data cleaning approaches? Injecting
synthetic errors helps in evaluating algorithms, but is it
leading the research towards the real-world problems?

•

When do you stop the cleaning process? Is there a
general stopping condition in practice? And a principled one?

•

How to fill the gap between the domain experts, who
know the semantics and what is an error, and the IT
staff/data scientists, who are able to write the rules
and train the models?
III.

PANELISTS

•

Tamraparni Dasu (AT&T Labs-Research)

•

Juliana Freire (New York University)

•

Ihab Ilyas (University of Waterloo)

•

Felix Naumann (Hasso Plattner Institute)

•

Eric Simon (SAP)

Tamraparni Dasu: Tamraparni Dasu is a research scientist
at the AT&T Labs-Research in New York. Her research interests include applications in computational statistics, statistical
algorithms for stream mining, information visualization, data
quality and quantitative data cleaning. She has a PhD in

1446

ICDE 2016 Conference

Mathematical Statistics from University of Rochester and a
Masters in Mathematics from the Indian Institute of Technology, Dehli. Tamraparni has written a comprehensive book
on Exploratory Data Mining and Data Cleaning, as well as
several technical papers on glitch detection and distortion and
effect of data repair and data cleaning. She currently holds 2
industry patents. Tamraparni’s personal interests include fiction
writing and literary translation. The proceeds from her two
published novels (written under the pseudonym T. Dasu) have
been donated to various charities.
Juliana Freire: Juliana Freire is a Professor at the Department of Computer Science and Engineering at New York
University. She also holds an appointment in the Courant
Institute for Mathematical Science and is a faculty member at
the NYU Center of Data Science. Her research interests are in
large-scale data analysis, visualization, and provenance management. Professor Freire is an active member of the database
and Web research communities, having co-authored over 130
technical papers and holding 9 U.S. patents. She has received
several awards, including an NSF CAREER, an IBM Faculty
award, and a Google Faculty Research award. Her research has
been funded by grants from the National Science Foundation,
Department of Energy, National Institutes of Health, University
of Utah, NYU, Sloan Foundation, Betty Moore Foundation,
Google, Amazon, Microsoft Research, Yahoo! and IBM.
Ihab Ilyas: Ihab Ilyas is a professor in the Cheriton
School of Computer Science at the University of Waterloo. He
received his PhD in computer science from Purdue University,
West Lafayette. His main research is in the area of database
systems, with special interest in data quality, managing uncertain data, rank-aware query processing, and information extraction. Ihab is a recipient of the Ontario Early Researcher Award
(2009), a Cheriton Faculty Fellowship (2013), an NSERC
Discovery Accelerator Award (2014), and a Google Faculty
Award (2014), and he is an ACM Distinguished Scientist. Ihab
is a co-founder of Tamr, a startup focusing on large-scale data
integration and cleaning. He serves on the VLDB Board of
Trustees, and he is an associate editor of the ACM Transactions
of Database Systems (TODS).

INRIA in France. There, he created the research project that
produced the innovation transferred to Medience. Eric received
a PhD and Habilitation in Computer Science from University
of Paris VI in 1986 and 1992 respectively. His area of
expertise is in advanced database systems, query optimization,
data integration methods and algorithms, data cleaning, and
business intelligence.
Shazia Sadiq: Shazia Sadiq is currently working in the
School of Information Technology and Electrical Engineering
at The University of Queensland, Brisbane, Australia. She is
part of the Data and Knowledge Engineering (DKE) research
group and is involved in teaching and research in databases and
information systems. Shazia holds a PhD from The University
of Queensland in Information Systems and a Masters degree
in Computer Science from the Asian Institute of Technology,
Bangkok, Thailand. Her main research interests are innovative
solutions for Business Information Systems that span several
areas including business process management, governance, risk
and compliance, and information quality and use. Shazia is a
board member of the International Association for Information
and Data Quality Asia Pacific chapter, convener of the Queensland Data Quality Roundtable, Deputy chair of the National
Committee on Information and Communication Sciences at
the Australian Academy of Science, and a University of
Queensland Teaching Excellence Award Winner.
Paolo Papotti: Paolo Papotti is an Assistant Professor of
Computer Science in the School of Computing, Informatics,
and Decision Systems Engineering (CIDSE) at Arizona State
University. He got his Ph.D. in Computer Science at Università
degli Studi Roma Tre (2007, Italy) and before joining ASU
he had been a senior scientist at Qatar Computing Research
Institute. His research is focused on systems that assist users
in complex, necessary tasks and that scale to large datasets
with efficient algorithms and distributed platforms. The two
main themes of his research are data integration and data
quality. He is associate editor for the ACM Journal of Data
and Information Quality (JDIQ).
R EFERENCES
[1]
[2]

Felix Naumann: Felix Naumann studied mathematics at
the Technical University of Berlin and received his diploma
in 1997. As a member of the graduate school “Distributed
Information Systems” at Humboldt-University of Berlin, he
finished his PhD thesis in 2000. In the following two years
Felix Naumann worked at the IBM Almaden Research Center.
From 2003-2006 he was an assistant professor at HumboldtUniversity. Since 2006 he is a full professor at the Hasso
Plattner Institute heading the information systems group. His
research interest are in data quality, data cleansing, data
profiling, and text mining.
Eric Simon: Eric Simon is currently Chief Architect of
SAP’s Enterprise Information Management product division.
He is also the engineering lead for SAP HANA product
“Enterprise Semantic Services” that was first released in 2015.
He was before a Senior Director of engineering at Business
Objects for Data Access and Data Federator, two core products
of the Business Objects Enterprise Suite. In 2001, Eric cofounded and led Medience, a French start-up specialized in
data federation technology, later acquired by Business Objects
in 2005. Previously, Eric was a tenure research scientist at

[3]
[4]
[5]
[6]

[7]

[8]
[9]

[10]

1447

IDG Enterprise, “IDG enterprise research reports 2014.”
S. Abiteboul, X. L. Dong, O. Etzioni, D. Srivastava, G. Weikum,
J. Stoyanovich, and F. M. Suchanek, “The elephant in the room: getting
value from big data,” in 18th International Workshop on Web and
Databases (WebDB), 2015, pp. 1–5.
B. Saha and D. Srivastava, “Data quality: The other face of big data,”
in ICDE, 2014.
J. Taylor, “IBM big data and information management,” May 2011,
available: www-01.ibm.com/software/data/bigdata/.
D. Darlin, “Airfares made easy (or easier),” 2006, available:
http://www.nytimes.com/2006/07/01/business/01money.html?pagewanted=all.
M. Scherer, “How obama’s data crunchers helped him win,”
2015, available: http://edition.cnn.com/2012/11/07/tech/web/obamacampaign-tech-team/.
C. A. O’Reilly, “Variations in decision makers’ use of information
sources: The impact of quality and accessibility of information,”
Academy of Management Journal, vol. 25, pp. 756–771, 1982.
M. Ouzzani, P. Papotti, and E. Rahm, “Introduction to the special issue
on data quality,” Inf. Syst., vol. 38, no. 6, pp. 885–886, 2013.
S. Sadiq, N. K. Yeganeh, and M. Indulska, “20 years of data quality
research: themes, trends and synergies,” in 22th Australasian Database
Conference (ADC), 2011, pp. 153–162.
J. Juran, Quality Control Handbook. New York: McGraw-Hill Publishing, 1962.

Creating Nested Mappings with Clio∗
Mauricio A. Hernández

Howard Ho

Lucian Popa

IBM Almaden Research Center

Ariel Fuxman

IBM Yamato Software Laboratory

Renée J. Miller

Paolo Papotti

University of Toronto

1

Takeshi Fukuda

Università Roma Tre

Introduction

Schema mappings play a central role in many data integration and data exchange scenarios. In those applications,
users need to quickly and correctly specify how data represented in one format is converted into a different format.
Clio [8] is a joint research project between IBM and the
University of Toronto studying the creation, maintenance,
and use of schema mappings. There have always been two
goals in our work in Clio: 1) the automatic creation of
logical assertions that capture the way one or more source
schemas are mapped into a target schema, and 2) the generation of transformation queries or programs that transform
a source data instance into a target data instance.
In the past, we have demonstrated how Clio can meet
these goals [7] using a mapping generation engine that produced simple logical assertions we now call “basic mappings”. Basic mappings are constructed by matching users’
correspondences between schema objects with “ﬂattened”
representations of source and target types. The details of
the algorithms for creating basic mappings and generating queries can be found in [8]. Basic mappings are simple source-to-target tuple generation dependencies (s-t tgds,
also known as GLAV mappings) and have received considerable attention in the last few years [1, 2, 3, 5, 6].
We have since updated the Clio engine to produce a
new kind of mapping declaration that we call “nested mappings” [4]. Nested mappings take advantage of the existing
nesting in the schemas and, by analyzing the basic mappings produced by Clio, determine the best way to nest the
assertions within each other. As discussed in [4], given a
set of basic mappings, ﬁguring out when one basic mapping can be nested inside another nested mapping is nontrivial and certain heuristics are needed to prune the space
of possible nesting combinations. By nesting two or more
basic mappings into a nested mapping, we reduce the number of mapping formulas produced by Clio. Further, in the
cases where the schemas are nested (e.g., a typical XML
∗ This work was funded in part by the U.S. Air Force Ofﬁce for Scientiﬁc Research under contract FA9550-06-1-0226.

1-4244-0803-2/07/$20.00 ©2007 IEEE

Figure 1. A mapping example
Schema), we consider the nested mappings speciﬁcation to
be a more natural way of capturing the mapping speciﬁcation. We also demonstrate that the queries we produce
are better than the ones we produced using basic mappings
(both in execution performance and in the more subjective
“readability” of the query script).

2

Clio Mappings

“Basic” Mappings. Figure 1 shows a simple schema mapping in Clio. Here, the user has loaded a source and a target
schema and has already entered the value correspondences
(the solid lines across). The source schema on the left
represents information about projects and their contributing regular and temporary employees. There is a top-level
set of project records (Proj), and each project record has
a department name (dname), a project name (pname), and
nested sets of regular (regEmps) and temporary employees (tempEmps). The target schema on the right side allows the grouping of project and employee records by department (Dept). Furthermore, each employee can have a
list of his or her projects (pid). The line marked r in the
ﬁgure speciﬁes a foreign key constraint that exists in the

1487

Figure 2. Nested mappings
target schema. According to the constraint, each projs element is associated with a projects element (assuming
non-null foreign keys). The mapping task here is to break
the source department and project name information into
the corresponding ﬁelds in the target. Further, we need to
union the two lists of source employees (regular and temporary) into one list in the target schema.
Notice that the user will not map anything to the pid
ﬁelds (the ones with the key/foreign key constraint) or to
the budget ﬁeld. It is up to our mapping engine and query
generator to ﬁgure out what to do about those ﬁelds.
This set of value correspondences is compiled into the
following three basic mappings1 . The ﬁrst basic mapping,
m1 , maps information from the top-level source projects
into the target department and project sets. The second one,
m2 , maps projects with regular employees into the entire
target schema. The mapping m3 is similar to m2 but deals
with source projects and temporary employees.
m1 :
m2 :
m3 :

proj(d, p, Er , Et ) → dept(d, ?b, ?E, ?P ) ∧ P (?x, p)
proj(d, p, Er , Et ) ∧ Er (e, s)
→ dept(d, ?b, ?E, ?P ) ∧ E(e, s, ?P  ) ∧ P  (?x) ∧ P (x, p)
proj(d, p, Er , Et ) ∧ Et (e, s)
→ dept(d, ?b, ?E, ?P ) ∧ E(e, s, ?P  ) ∧ P  (?x) ∧ P (x, p)

Nested Mappings. Given those basic mappings, our nested
mapping engine tries to nest one inside the other. The obvious solution in this case is to nest both m2 and m3 inside
m1 . We write the resulting nesting mapping as follows2 :
n:

proj(d, p, Er , Et ) →
[ dept(d, ?b, ?E, ?P ) ∧ P (?x, p)
∧ [ Er (e, s) → E(e, s, ?P  ) ∧ P  (x) ]
∧ [ Et (e, s) → E(e, s, ?P  ) ∧ P  (x) ] ]

Figure 2 shows Clio’s rendering of n. The dotted lines
and boxes represent the nesting of m2 within m1 . Each box
1 In this logic-based notation, nested sets are replaced by variables. For
example, in m1 , Er represent the set of source regular employees. An
existentially quantiﬁed variable has a ‘?’ the ﬁrst time it occurs.
2 The details of the algorithm used to derive this mapping are in [4].

1-4244-0803-2/07/$20.00 ©2007 IEEE

corresponds to a basic mapping and lines between them indicate nesting. The lines from the source set elements into
target set elements represent the universally and existentially quantiﬁed set variables of each mapping. For instance,
we interpret the lines for m1 as follows: for all Proj tuples,
create a Dept tuple with a nested projects tuple inside.
For m2 , we retrieve all regEmps tuples nested within the
current Proj tuple and produce the necessary emps and
projs target structures nested within the current Dept tuple.
Clio computes a default group-by condition for each target nested level. A novel feature of our nested mapping
implementation is that users can modify these conditions
allowing arbitrary grouping of data at each level.

3

The Demo

In the demo, we showcase Clio’s nested mapping creation engine on several data exchange scenarios. Given a
pair of schemas, users can enter simple lines between the
schema elements and see how Clio automatically generates
the nested mappings. We highlight a query generation algorithm that deals, in a general way, with the arbitrary restructuring and grouping of the data on the target. Clio
generates these queries using two XML query languages:
XQuery and XSLT. We discuss our experience using these
two languages to express complex transformations that require grouping and duplicate elimination. We also discuss
the effect of changing grouping conditions at any level. Although we plan to conduct most of our demo using Clio,
we also show how the nested mapping technology impacts
other IBM mapping tools.

References
[1] L. Chiticariu and W.-C. Tan. Debugging schema mappings
with routes. In VLDB, pages 79–90, 2006.
[2] R. Fagin. Inverting Schema Mappings. In PODS, pages 50–
59, 2006.
[3] R. Fagin, P. G. Kolaitis, R. J. Miller, and L. Popa. Data Exchange: Semantics and Query Answering. TCS, 336(1):89–
124, 2005.
[4] A. Fuxman, M. A. Hernández, H. Ho, R. J. Miller, P. Papotti,
and L. Popa. Nested mappings: Schema mapping reloaded.
In VLDB, pages 67–78, 2006.
[5] J. Madhavan and A. Y. Halevy. Composing Mappings Among
Data Sources. In VLDB, pages 572–583, 2003.
[6] A. Nash, P. A. Bernstein, and S. Melnik. Composition of Mappings Given by Embedded Dependencies. In PODS, pages
172–183, 2005.
[7] L. Popa, M. A. Hernández, Y. Velegrakis, R. J. Miller, F. Naumann, and H. Ho. Mapping XML and Relational Schemas
with CLIO, System Demonstration. In ICDE, page 498, 2002.
[8] L. Popa, Y. Velegrakis, R. J. Miller, M. A. Hernández, and
R. Fagin. Translating Web Data. In VLDB, pages 598–609,
2002.

1488

BigDansing: A System for Big Data Cleansing
Zuhair Khayyat†∗ Ihab F. Ilyas‡∗
Alekh Jindal]
Samuel Madden]
Mourad Ouzzani§
Paolo Papotti§
Jorge-Arnulfo Quiané-Ruiz§
Nan Tang§
†

Si Yin§

§
]
Qatar Computing Research Institute
CSAIL, MIT
‡
King Abdullah University of Science and Technology (KAUST)
University of Waterloo

zuhair.khayyat@kaust.edu.sa
ilyas@uwaterloo.ca
{alekh,madden}@csail.mit.edu
{mouzzani,ppapotti,jquianeruiz,ntang,siyin}@qf.org.qa
ABSTRACT

t1
t2
t3
t4
t5
t6

Data cleansing approaches have usually focused on detecting and fixing errors with little attention to scaling to big
datasets. This presents a serious impediment since data
cleansing often involves costly computations such as enumerating pairs of tuples, handling inequality joins, and dealing with user-defined functions. In this paper, we present
BigDansing, a Big Data Cleansing system to tackle efficiency, scalability, and ease-of-use issues in data cleansing. The system can run on top of most common general
purpose data processing platforms, ranging from DBMSs
to MapReduce-like frameworks. A user-friendly programming interface allows users to express data quality rules both
declaratively and procedurally, with no requirement of being
aware of the underlying distributed platform. BigDansing
takes these rules into a series of transformations that enable
distributed computations and several optimizations, such as
shared scans and specialized joins operators. Experimental
results on both synthetic and real datasets show that BigDansing outperforms existing baseline systems up to more
than two orders of magnitude without sacrificing the quality
provided by the repair algorithms.

1.

zipcode
10001
90210
60601
90210
60827
90210

city
NY
LA
CH
SF
CH
LA

state
NY
CA
IL
CA
IL
CA

salary
24000
25000
40000
88000
15000
81000

rate
15
10
25
28
15
28

Table 1: Dataset D with tax data records
have a lower tax rate; and (r3) two tuples refer to the same
individual if they have similar names, and their cities are
inside the same county. We define these rules as follows:
(r1) φF : D(zipcode → city)
(r2) φD : ∀t1 , t2 ∈ D, ¬(t1 .rate > t2 .rate ∧ t1 .salary < t2 .salary)
(r3) φU : ∀t1 , t2 ∈ D, ¬(simF(t1 .name, t2 .name)∧
getCounty(t1 .city) = getCounty(t2 .city))

φF , φD , and φU can respectively be expressed as a functional dependency (FD), a denial constraint (DC), and a
user-defined function (UDF) by using a procedural language.
φU requires an ad-hoc similarity function and access to a
mapping table to get the county information. Tuples t2 and
t4 form an error w.r.t. φF , so do t4 and t6 , because they have
the same zipcode but different city values. Tuples t1 and t2
violate φD , because t1 has a lower salary than t2 but pays
a higher tax; so do t2 and t5 . Rule φU does not have data
errors, i.e., no duplicates in D w.r.t. φU .
2

INTRODUCTION

Cleansing the above dataset typically consists of three
steps: (1) specifying quality rules; (2) detecting data errors w.r.t. the specified rules; and (3) repairing detected errors. Generally speaking, after step (1), the data cleansing
process iteratively runs steps (2) and (3) until obtaining an
instance of the data (a.k.a. a repair) that satisfies the specified rules. These three steps have been extensively studied
in single-node settings [6, 7, 11, 16–18, 23, 34]. However, the
main focus has been on how to more effectively detect and
repair errors, without addressing the issue of scalability.
Dealing with large datasets faces two main issues. First,
none of the existing systems can scale to large datasets for
the rules in the example. One main reason is that detecting
errors, e.g., with φF and φD , or duplicates, e.g., with φU , is
a combinatorial problem that quickly becomes very expensive with large sizes. More specifically, if |D| is the number
of tuples in a dataset D and n is the number of tuples a given
rule is defined on (e.g., n = 2 for the above rules), the time
complexity of detecting errors is O(|D|n ). This high complexity leads to intractable computations over large datasets,
limiting the applicability of data cleansing systems. For instance, memory-based approaches [6, 7] report performance
numbers up to 100K tuples while a disk-based approach [17]
reports numbers up to 1M records only. Apart from the scale

Data quality is a major concern in terms of its scale as
more than 25% of critical data in the world’s top companies is flawed [31]. In fact, cleansing dirty data is a critical
challenge that has been studied for decades [10]. However,
data cleansing is hard, since data errors arise in different
forms, such as typos, duplicates, non compliance with business rules, outdated data, and missing values.
Example 1: Table 1 shows a sample tax data D in which
each record represents an individual’s information. Suppose
that the following three data quality rules need to hold on
D: (r1) a zipcode uniquely determines a city; (r2) given two
distinct individuals, the one earning a lower salary should
∗

name
Annie
Laure
John
Mark
Robert
Mary

Partially done while at QCRI.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGMOD’15, May 31–June 4, 2015, Melbourne, Victoria, Australia.
c 2015 ACM 978-1-4503-2758-9/15/05 ...$15.00.
Copyright 
http://dx.doi.org/10.1145/2723372.2747646.

1215

problem, implementing such rules in a distributed processing platform requires expertise both in the understanding of
the quality rules and in the tuning of their implementation
over this platform. Some research efforts have targeted the
usability of a data cleansing system (e.g., NADEEF [7]), but
at the expense of performance and scalability.
Therefore, designing a distributed data cleansing system
to deal with big data faces two main challenges:

than baseline systems without sacrificing the quality provided by the repair algorithms (Section 6).

2.

FUNDAMENTALS AND AN OVERVIEW

We discuss the data cleansing semantics expressed by BigDansing and then give an overview of the system.

2.1

Data Cleansing Semantics

In BigDansing, the input data is defined as a set of data
units, where each data unit is the smallest unit of input
datasets. Each unit can have multiple associated elements
that are identified by model-specific functions. For example, tuples are the data units for the relational model and
attributes identify their elements, while triples are the data
units for RDF data (see Appendix C). BigDansing provides
a set of parsers for producing such data units and elements
from input datasets.
BigDansing adopts UDFs as the basis to define quality
rules. Each rule has two fundamental abstract functions,
namely Detect and GenFix. Detect takes one or multiple
data units as input and outputs a violation, i.e., elements
in the input units that together are considered as erroneous
w.r.t. the rule:
Detect(data units) → violation

(1) Scalability. Quality rules are varied and complex: they
might compare multiple tuples, use similarity comparisons,
or contain inequality comparisons. Detecting errors in a
distributed setting may lead to shuffling large amounts of
data through the network, with negative effects on the performance. Moreover, existing repair algorithms were not
designed for distributed settings. For example, they often
represent detected errors as a graph, where each node represents a data value and an edge indicates a data error. Devising algorithms on such a graph in distributed settings is
not well addressed in the literature.
(2) Abstraction. In addition to supporting traditional quality rules (e.g., φF and φD ), users also need the flexibility
to define rules using procedural programs (UDFs), such as
φU . However, effective parallelization is hard to achieve with
UDFs, since they are handled as black boxes. To enable scalability, finer granular constructs for specifying the rules are
needed. An abstraction for this specification is challenging
as it would normally come at the expense of generality of
rule specification.

GenFix takes a violation as input and computes alternative,
possible updates to resolve this violation:
GenFix(violation) → possible fixes
The language of the possible fixes is determined by the
capabilities of the repair algorithm. With the supported
algorithms, a possible fix in BigDansing is an expression of
the form x op y, where x is an element, op is in {=, 6=, <, >, ≥
, ≤}, and y is either an element or a constant. In addition,
BigDansing has new functions to enable distributed and
scalable execution of the entire cleansing process. We defer
the details to Section 3.
Consider Example 1, the Detect function of φF takes two
tuples (i.e., two data units) as input and identifies a violation whenever the same zipcode value appears in the two
tuples but with a different city. Thus, t2 (90210, LA) and
t4 (90210, SF ) are a violation. The GenFix function could
enforce either t2 [city] and t4 [city] to be the same, or at least
one element between t2 [zipcode] and t4 [zipcode] to be different from 90210. Rule φU is more general as it requires special
processing. Detect takes two tuples as input and outputs a
violation whenever it finds similar name values and obtains
the same county values from a mapping table. GenFix could
propose to assign the same values to both tuples so that one
of them is removed in set semantics.
By using a UDF-based approach, we can support a large
variety of traditional quality rules with a parser that automatically implements the abstract functions, e.g., CFDs [11]
and DCs [6], but also more procedural rules that are provided by the user. These latter rules can implement any detection and repair method expressible with procedural code,
such as Java, as long as they implement the signatures of
the two above functions, as demonstrated in systems such
as NADEEF [7]. Note that one known limitation of UDFbased systems is that, when treating UDFs as black-boxes,
it is hard to do static analysis, such as consistency and implication, for the given rules.
BigDansing targets the following data cleansing problem:
given a dirty data D and a set of rules Σ, compute a repair

To address these challenges, we present BigDansing, a
Big Data Cleansing system that is easy-to-use and highly
scalable. It provides a rule specification abstraction that
consists of five operators that allow users to specify data
flows for error detection, which was not possible before.
The internal rewriting of the rules enables effective optimizations. In summary, we make the following contributions:
(1) BigDansing abstracts the rule specification process
into a logical plan composed of five operators, which it optimizes for efficient execution. Users focus on the logic of their
rules, rather than on the details of efficiently implementing
them on top of a parallel data processing platform. More
importantly, this abstraction allows to detect errors and find
possible fixes w.r.t. a large variety of rules that cannot be
expressed in declarative languages (Section 3).
(2) BigDansing abstraction enables a number of optimizations. We present techniques to translate a logical plan into
an optimized physical plan, including: (i) removing data
redundancy in input rules and reducing the number of operator calls; (ii) specialized operators to speed up the cleansing
process; and (iii) a new join algorithm, based on partitioning
and sorting data, to efficiently perform distributed inequality self joins (which are common in quality rules);
(3) We present two approaches to implement existing repair algorithms in distributed settings. First, we show how
to run a centralized data repair algorithm in parallel, without changing the algorithm (Section 5.1). Second, we design
a distributed version of the seminal equivalence class algorithm [5] (Section 5.2).
(4) We use both real-world and synthetic datasets to extensively validate our system. BigDansing outperforms baseline systems by up to more than two orders of magnitude.
The results also show that BigDansing is more scalable

1216

of general purpose data processing frameworks ranging from
MapReduce-like systems to databases.
(1) Logical layer. A major goal of BigDansing is to allow
users to express a variety of data quality rules in a simple
way. This means that users should only care about the logic
of their quality rules, without worrying about how to make
the code distributed. To this end, BigDansing provides five
logical operators, namely Scope, Block, Iterate, Detect, and
GenFix, to express a data quality rule: Scope defines the relevant data for the rule; Block defines the group of data units
among which a violation may occur; Iterate enumerates the
candidate violations; Detect determines whether a candidate
violation is indeed a violation; and GenFix generates a set
of possible fixes for each violation. Users define these logical operators, as well as the sequence in which BigDansing
has to run them, in their jobs. Alternatively, users provide
a declarative rule and BigDansing translates it into a job
having these five logical operators. Notice that a job represents the logical plan of a given input quality rule.
(2) Physical layer. In this layer, BigDansing receives a
logical plan and transforms it into an optimized physical
plan of physical operators. Like in DBMSs, a physical plan
specifies how a logical plan is implemented. For example,
Block could be implemented by either hash-based or rangebased methods. A physical operator in BigDansing also
contains extra information, such as the input dataset and the
degree of parallelism. Overall, BigDansing processes a logical plan through two main optimization steps, namely the
plan consolidation and the data access optimization, through
the use of specialized join and data access operators.

Figure 1: BigDansing architecture
which is an updated instance D0 such that there are no violations, or there are only violations with no possible fixes.
Among the many possible solutions to the cleansing problem, it is common to define a notion of minimality based on the cost of a repair.
A popular
cost
is the following:
P function [5] for relational data
0
0
t∈D,t0 ∈Dr ,A∈AR disA (D(t[A]), D(t [A])), where t is the fix
0
for a specific tuple t and disA (D(t[A]), D(t [A])) is a distance
between their values for attribute A (an exact match returns
0). This models the intuition that the higher is the sum of
the distances between the original values and their fixes, the
more expensive is the repair. Computing such minimum repairs is NP-hard, even with FDs only [5, 23]. Thus, data
repair algorithms are mostly heuristics (see Section 5).

2.2

(3) Execution layer. In this layer, BigDansing determines
how a physical plan will be actually executed on the underlying parallel data processing framework. It transforms a
physical plan into an execution plan which consists of a set
of system-dependent operations, e.g., a Spark or MapReduce
job. BigDansing runs the generated execution plan on the
underlying system. Then, it collects all the violations and
possible fixes produced by this execution. As a result, users
get the benefits of parallel data processing frameworks by
just providing few lines of code for the logical operators.

Architecture

We architected BigDansing as illustrated in Figure 1.
BigDansing receives a data quality rule together with
a dirty dataset from users (1) and outputs a clean
dataset (7). BigDansing consists of two main components:
the RuleEngine and the RepairAlgorithm.
The RuleEngine receives a quality rule either in a UDFbased form (a BigDansing job) or in a declarative form (a
declarative rule). A job (i.e., a script) defines users operations as well as the sequence in which users want to run their
operations (see Appendix A). A declarative rule is written
using traditional integrity constraints such as FDs and DCs.
In the latter case, the RuleEngine automatically translates
the declarative rule into a job to be executed on a parallel
data processing framework. This job outputs the set of violations and possible fixes for each violation. The RuleEngine
has three layers: the logical, physical, and execution layers.
This architecture allows BigDansing to (i) support a large
variety of data quality rules by abstracting the rule specification process, (ii) achieve high efficiency when cleansing
datasets by performing a number of physical optimizations,
and (iii) scale to big datasets by fully leveraging the scalability of existing parallel data processing frameworks. Notice that, unlike a DBMS, the RuleEngine also has an execution abstraction, which allows BigDansing to run on top

Once BigDansing has collected the set of violations and
possible fixes, it proceeds to repair (cleanse) the input dirty
dataset. At this point, the repair process is independent
from the number of rules and their semantics, as the repair
algorithm considers only the set of violations and their possible fixes. The way the final fixes are chosen among all the
possible ones strongly depends on the repair algorithm itself.
Instead of proposing a new repair algorithm, we present in
Section 5 two different approaches to implement existing
algorithms in a distributed setting. Correctness and termination properties of the original algorithms are preserved in
our extensions. Alternatively, expert users can plug in their
own repair algorithms. In the algorithms that we extend,
each repair step greedily eliminates violations with possible fixes while minimizing the cost function in Section 2.1.
An iterative process, i.e., detection and repair, terminates
if there are no more violations or there are only violations
with no corresponding possible fixes. The repair step may
introduce new violations on previously fixed data units and
a new step may decide to again update these units. To ensure termination, the algorithm put a special variable on
such units after a fixed number of iterations (which is a user

1217

(1) Scope (zipcode, city)

defined parameter), thus eliminating the possibility of future
violations on the same data.

3.

RULE SPECIFICATION

BigDansing abstraction consists of five logical operators: Scope, Block, Iterate, Detect, and GenFix, which are
powerful enough to express a large spectrum of cleansing
tasks. While Detect and GenFix are the two general operators that model the data cleansing process, Scope, Block,
and Iterate enable the efficient and scalable execution of that
process. Generally speaking, Scope reduces the amount of
data that has to be treated, Block reduces the search space
for the candidate violation generation, and Iterate efficiently
traverses the reduced search space to generate all candidate
violations. It is worth noting that these five operators do not
model the repair process itself (i.e., a repair algorithm). The
system translates these operators along with a, generated or
user-provided, BigDansing job into a logical plan.

3.1

zipcode

city

t1

10001

NY

t2

90210

LA

t3

60601

CH

t4

90210

SF

t5

60601

CH

t6

90210

LA

(2) Block (zipcode)

B1
B2

B3

zipcode

city

t3

60601

CH

t5

60601

CH

t1

10001

NY

t2

90210

LA

t4

90210

SF

t6

90210

LA

(3) Iterate
(t3, t5)
(t2, t4)
(t2, t6)
(t4, t6)
(4) Detect
(t2, t4)
(t4, t6)
(5) GenFix
t2[city] = t4[city];
t6[city] = t4[city]

Figure 2: Logical operators execution for rule FD φF
Iterate passes each unique combination of two tuples inside
each block, producing four pairs only (instead of 13 pairs):
(t3 , t5 ) from B1 , (t2 , t4 ), (t2 , t6 ), and (t4 , t6 ) from B3 . Listing 6 (Appendix B) shows the code required by this Iterate
operator for rule φF .
(4) Detect takes a single U , a pair-U , or a list o U s, as
input and outputs a list of violations, possibly empty.
Detect(U | hUi , Uj i | listhU 0 i) → {listhviolationi}

Logical Operators

As mentioned earlier, BigDansing defines the input data
through data units U s on which the logical operators operate. While such a fine-granular model might seem to incur a
high cost as BigDansing calls an operator for each U , it in
fact allows to apply an operator in a highly parallel fashion.
In the following, we define the five operators provided
by BigDansing and illustrate them in Figure 2 using the
dataset and rule FD φF (zipcode → city) of Example 1. Notice that the following listings are automatically generated
by the system for declarative rules. Users can either modify
this code or provide their own for the UDFs case.

Considering three types of inputs for Detect allows us to
achieve better parallelization by distinguishing between different granularities of the input. For example, having 1K
U s as input, rather than a single list of U s, would allow
us to run 1K parallel instances of Detect (instead of a single
Detect instance). In Figure 2, Detect outputs two violations,
v1 = (t2 , t4 ) and v2 = (t6 , t4 ), as they have different values
for city; and it requires the lines of code in Listing 1.
public ArrayList <Violation> detect(TuplePair in ) {
1
ArrayList <Violation> lst = new ArrayList<Violation>();
2
if (! in . getLeft () . getCellValue (1) . equals(
in . getRight() . getCellValue (1))) {
3
Violation v = new Violation(”zipcode => City”);
4
v.addTuple(in. getLeft ()) ;
5
v.addTuple(in.getRight()) ;
6
lst .add(v); }
7
return lst ;
}

(1) Scope removes irrelevant data units from a dataset.
For each data unit U , Scope outputs a set of filtered data
units, which can be an empty set.
Scope(U ) → listhU 0 i
Notice that Scope outputs a list of U s as it allows data units
to be replicated. This operator is important as it allows
BigDansing to focus only on data units that are relevant
to a given rule. For instance, in Figure 2, Scope projects on
attributes zipcode and city. Listing 4 (Appendix B) shows
the lines of code for Scope in rule φF .

Listing 1: Code example for the Detect operator.
(5) GenFix computes a set of possible fixes for a given
violation.
GenFix(violation) → {listhPossibleFixesi}

(2) Block groups data units sharing the same blocking key.
For each data unit U , Block outputs a blocking key.
Block(U ) → key

For instance, assuming that only right-hand side values can
be modified, GenFix produces one possible repair for each
detected violation (Figure 2): t2 [city] = t4 [city] and t6 [city] =
t4 [city]. Listing 2 shows the code for this GenFix.

The Block operator is crucial for BigDansing’s scalability
as it narrows down the number of data units on which a violation might occur. For example, in Figure 2, Block groups
tuples on attribute zipcode, resulting in three blocks, with
each block having a distinct zipcode. Violations might occur
inside these blocks only and not across blocks. See Listing 5
(Appendix B) for the single line of code required by this
operator fro rule φF .

public ArrayList <Fix> GenFix(Violation v) {
1
ArrayList <Fix> result = new ArrayList<Fix>();
2
Tuple t1 = v.getLeft () ;
3
Tuple t2 = v.getRight() ;
4
Cell c1 = new Cell(t1.getID() , ”City ”, t1 . getCellValue (1)) ;
5
Cell c2 = new Cell(t2.getID() , ”City ”, t2 . getCellValue (1)) ;
6
result .add(new Fix(c1,”=”,c2)) ;
7
return result ;
}

(3) Iterate defines how to combine data units U s to generate candidate violations. This operator takes as input a list
of lists of data units U s (because it might take the output
of several previous operators) and outputs a single U , a pair
of U s, or a list of U s.
Iterate(listhlisthU ii) → U 0 | hUi , Uj i | listhU 00 i

Listing 2: Code example for the GenFix operator.
Additionally, to better specify the data flow among the
different operators, we introduce a label to stamp a data
item and track how it is being processed.
In contrast to DBMS operators, BigDansing’s operators
are UDFs, which allow users to plug in any logic. As a result,
we are not restricted to a specific data model. However, for

This operator allows to avoid the quadratic complexity for
generating candidate violations. For instance, in Figure 2,

1218

Found
Found
Rule

Found
GenFix

Detect

Iterate
Not found

Not found

S
Block
Scope

D1

Logical
plan

S
Scope

Block

T

Not found

D2

Iterate

Block

Not found

Figure 3: Planner execution flow

S
T

M
Iterate

V

Detect

V

GenFix

W

Figure 4: Example of a logical plan
ease of explanations all of our examples assume relational
data. We report an RDF data cleansing example in Appendix C. We also report in Appendix D the code required
to write the same rule in a distributed environment, such
as Spark. The proposed templates for the operators allow
users to get distributed detection without any expertise on
Spark, only by providing from 3 to 16 lines of Java code. The
benefit of the abstraction should be apparent at this point:
(i) ease-of-use for non-expert users and (ii) better scalability
thanks to its abstraction.

3.2

translations for each logical operator. Since quality rules
may involve joins over ordering comparisons, we also introduce an efficient algorithm for these cases. We discuss these
aspects in this section. In addition, we discuss data storage
and access issues in Appendix F.

4.1

The Planning Process

The logical layer of BigDansing takes as input a set of
labeled logical operators together with a job and outputs a
logical plan. The system starts by validating whether the
provided job is correct by checking that all referenced logical operators are defined and at least one Detect is specified.
Recall that for declarative rules, such as CFDs and DCs,
users do not need to provide any job, as BigDansing automatically generates a job along with the logical operators.
After validating the provided job, BigDansing generates
a logical plan (Figure 3) that: (i) must have at least one
input dataset Di ; (ii) may have one or more Scope operators; (iii) may have one Block operator or more linked to an
Iterate operator; (iv) must have at least one Detect operator
to possibly produce a set of violations; and (v) may have
one GenFix operator for each Detect operator to generate
possible fixes for each violation.
BigDansing looks for a corresponding Iterate operator for
each Detect. If Iterate is not specified, BigDansing generates one according to the input required by the Detect operator. Then, it looks for Block and Scope operators that
match the input label of the Detect operator. If an Iterate
operator is specified, BigDansing identifies all Block operators whose input label match the input label of the Iterate.
Then, it uses the input labels of the Iterate operator to find
other possible Iterate operators in reverse order. Each new
detected Iterate operator is added to the plan with the Block
operators related to its input labels. Once it has processed
all Iterate operators, it finally looks for Scope operators.
In case the Scope or Block operators are missing, BigDansing pushes the input dataset to the next operator in
the logical plan. If no GenFix operator is provided, the output of the Detect operator is written to disk. For example,
assume the logical plan in Figure 4, which is generated by
BigDansing when a user provides the job in Appendix A.
We observe that dataset D1 is sent directly to a Scope and a
Block operator. Similarly, dataset D2 is sent directly to an
Iterate operator. We also observe that one can also iterate
over the output of previous Iterate operators (e.g., over output DM ). This flexibility allows users to express complex
data quality rules, such as in the form of “bushy” plans as
shown in Appendix E.

4.

Physical Operators

There are two kinds of physical operators: wrappers and
enhancers. A wrapper simply invokes a logical operator.
Enhancers replace wrappers to take advantage of different
optimization opportunities.
A wrapper invokes a logical operator together with the corresponding physical details, e.g., input dataset and schema
details (if available). For clarity, we discard such physical
details in all the definitions below. In contrast to the logical
layer, BigDansing invokes a physical operator for a set D
of data units, rather than for a single unit. This enables the
processing of multiple units in a single function call. For
each logical operator, we define a corresponding wrapper.
(1) PScope applies a user-defined selection and projection
over a set of data units D and outputs a dataset D0 ⊂ D.
PScope(D) → {D0 }
(2) PBlock takes a dataset D as input and outputs a list
of key-value pairs defined by users.
PBlock(D) → maphkey, listhU ii
(3) PIterate takes a list of lists of U s as input and outputs
their cross product or a user-defined combination.
PIterate(listhlisthU ii) → listhU i | listhPairhU ii
(4) PDetect receives either a list of data units U s or a list
of data unit pairs U -pairs and produces a list of violations.
PDetect(listhU i | listhPairhU ii) → listhV iolationi
(5) PGenFix receives a list of violations as input and outputs a list of a set of possible fixes, where each set of fixes
belongs to a different input violation.
PGenFix(listhV iolationi) → listh{P ossibleF ixes}i
With declarative rules, the operations over a dataset are
known, which enables algorithmic opportunities to improve
performance. Notice that one could also discover such operations with code analysis over the UDFs [20]. However, we
leave this extension to future work. BigDansing exploits
such optimization opportunities via three new enhancers operators: CoBlock, UCrossProduct, and OCJoin. CoBlock is
a physical operator that allows to group multiple datasets
by a given key. UCrossProduct and OCJoin are basically two
additional different implementations for the PIterate operator. We further explain these three enhancers operators in
the next section.

BUILDING PHYSICAL PLANS

When translating a logical plan, BigDansing exploits two
main opportunities to derive an optimized physical plan:
(i) static analysis of the logical plan and (ii) alternative

1219

T1

Algorithm 1: Logical plan consolidation
input : LogicalPlan lp
output: LogicalPlan clp
1 PlanBuilder lpb = new PlanBuilder();
2 for logical operator lopi ∈ lp do
3
lopj ← findMatchingLO(lopi , lp);
4
DS1 ← getSourceDS(lopi );
5
DS2 ← getSourceDS(lopj );
6
if DS1 == DS2 then
7
lopc ← getLabelsFuncs(lopi , lopj );
8
lopc .setInput(DS1 , DS2 );
9
lpb.add(lopc );
10
lp.remove(lopi , lopj );
11
12
13
14
15

T1

Block

T1

D1

Iterate

T2

Scope

D1

Scope

T1,T2

T12

Detect

T12

GenFix

T2

Block

T2
T1,T2

(a) Logical plan
Block

T1,T2

Iterate

T12

Detect

T12

GenFix

(b) Consolidated logical plan
T1,T2
D1

T1,T2
PScope

T1,T2
PBlock

T12
PIterate

T12
PDetect

PGenFix

(c) Physical plan

Figure 5: Plans for DC in rule 1
cal plan into a physical plan. It maps each logical operator
to its corresponding wrapper, which in turn maps to one or
more physical operators. For example, it produces the physical plan in Figure 5(c) for the consolidated logical plan in
Figure 5(b). For enhancers, BigDansing exploits some particular information from the data cleansing process. Below,
we detail these three enhancers operators as well as in which
cases they are used by our system.

if lpb.hasConsolidatedOps then
lpb.add(lp.getOperators());
return lpb.generateConsolidatedLP();
else
return lp;

4.2

Scope

• CoBlock takes multiple input datasets D and applies a
group-by on a given key. This would limit the comparisons
required by the rule to only blocks with the same key from
the different datasets. An example is shown in Figure 16
(Appendix E). Similar to the CoGroup defined in [28], in the
output of CoBlock, all keys from both inputs are collected
into bags. The output of this operator is a map from a key
value to the list of data units sharing that key value. We
formally define this operator as:

From Logical to Physical Plan

As we mentioned earlier, optimizing a logical plan is performed by static analysis (plan consolidation) and by plugging enhancers (operators translation) whenever possible.
Plan Consolidation. Whenever logical operators use a
different label for the same dataset, BigDansing translates
them into distinct physical operators. BigDansing has to
create multiple copies of the same dataset, which it might
broadcast to multiple nodes. Thus, both the memory footprint at compute nodes and the network traffic are increased.
To address this problem, BigDansing consolidates redundant logical operators into a single logical operator. Hence,
by applying the same logical operator several times on the
same set of data units using shared scans, BigDansing is
able to increase data locality and reduce I/O overhead.
Algorithm 1 details the consolidation process for an input
logical plan lp. For each operator lopi , the algorithm looks
for a matching operator lopj (Lines 2-3). If lopj has the
same input dataset as lopi , BigDansing consolidates them
into lopc (Lines 4-6). The newly consolidated operator lopc
takes the labels, functions, and datasets from lopi and lopj
(Lines 7-8). Next, BigDansing adds lopc into a logical plan
builder lpb and removes lopi and lopj from lp (Lines 9-10).
At last, if any operator was consolidated, it adds the nonconsolidated operators to lpb and returns the consolidated
logical plan (Lines 11-13). Otherwise, it returns lp (Line 15).
Let us now illustrate the consolidation process with an
example. Consider a DC on the TPC-H database stating
that if a customer and a supplier have the same name and
phone they must be in the same city. Formally,
DC : ∀t1 , t2 ∈ D1, ¬(t1 .c name = t2 .s name∧
(1)
t1 .c phone = t2 .s phone ∧ t1 .c city 6= t2 .s city)

CoBlock (D) → maphkey, listhlisthU iii
If two non-consolidated Block operators’ outputs go to the
same Iterate and then to a single PDetect, BigDansing
translates them into a single CoBlock. Using CoBlock allows us to reduce the number of candidate violations for the
Detect. This is because Iterate generates candidates only
inside and not across CoBlocks (see Figure 6).
• UCrossProduct receives a single input dataset D and applies a self cross product over it. This operation is usually
performed in cleansing processes that would output the same
violations irrespective of the order of the input of Detect.
UCrossProduct avoids redundant comparisons, reducing the
, with n being
number of comparisons from n2 to n×(n−1)
2
the number of units U s in the input. For example, the output of the logical operator Iterate in Figure 2 is the result
of UCrossProduct within each block; there are four pairs instead of thirteen, since the operator avoids three comparisons for the elements in block B1 and six for the ones in B3
of Figure 2. Formally:
UCrossProduct(D) → listhPairhU ii

For this DC, BigDansing generates the logical plan in Figure 5(a) with operators Scope and Block applied twice over
the same input dataset. It then consolidates redundant logical operators into a single one (Figure 5(b)) thereby reducing the overhead of reading an input dataset multiple times.
The logical plan consolidation is only applied when it does
not affect the original labeling of the operators.

If, for a single dataset, the declarative rules contain only
symmetric comparisons, e.g., = and 6=, then the order in
which the tuples are passed to PDetect (or to the next logical
operator if any) does not matter. In this case, BigDansing
uses UCrossProduct to avoid materializing many unnecessary
pairs of data units, such as for the Iterate operator in Figure 2. It also uses UCrossProduct when: (i) users do not
provide a matching Block operator for the Iterate operator;
or (ii) users do not provide any Iterate or Block operator.

Operators Translation. Once a logical plan have been
consolidated, BigDansing translates the consolidated logi-

• OCJoins performs a self join on one or more ordering comparisons (i.e., <, >, ≥, ≤). This is a very common operation

1220

T1

T1
Scope1

D1

Block1

T1
Iterate

D2

T12

Detect

T12

Algorithm 2: OCJoin
input : Dataset D, Condition conds[ ], Integer nbParts
output: List TupleshTuplei
// Partitioning Phase
1 PartAtt ← getPrimaryAtt(conds[].getAttribute());
2 K ← RangePartition(D, PartAtt, nbParts);
3 for each ki ∈ K do
4
for each cj ∈ conds[ ] do // Sorting
5
Sorts[j] ← sort(ki , cj .getAttribute());

GenFix

Block2
T2
T2
(a) Logical plan with non-consolidated Block operators

Scope2

T2
T1
D1

PScope1

D2

PScope2

T1
CoBlock

T2

T2

T12

PIterate

T12

PDetect

T12

PGenFix

(b) Physical plan

Figure 6: Example plans with CoBlock
in rules such as DCs. Thus, BigDansing provides OCJoin,
which is an efficient operator to deal with join ordering comparisons. It takes input dataset D and applies a number of
join conditions, returning a list of joining U -pairs. We formally define this operator as follows:

6
7
8
9
10
11
12

for each kl ∈ {ki+1 ...k|K| } do
if overlap(ki , kl , PartAtt) then // Pruning
tuples = ∅;
for each cj ∈ conds[ ] do // Joining
tuples ← join(ki , kl , Sorts[j], tuples);
if tuples == ∅ then
break;

OCJoin(D) → listhPairhU ii
13

Every time BigDansing recognizes joins conditions defined
with ordering comparisons in PDetect, e.g., φD , it translates Iterate into a OCJoin implementation. Then, it passes
OCJoin output to a PDetect operator (or to the next logical operator if any). We discuss this operator in detail in
Section 4.3.

14

partition may exist in multiple computing nodes, we apply
sorting before pruning and joining phases to ensure that each
partition is sorted at most once.

We report details for the transformation of the physical
operators to two distributed platforms in Appendix G.

4.3

if tuples != ∅ then
Tuples.add(tuples);

Pruning. Once all partitions ki ∈ K are internally sorted,
OCJoin can start joining each of these partitions based on
the inequality join conditions. However, this would require
transferring large amounts of data from one node to another. To circumvent such an overhead, OCJoin inspects
the min and max values of each partition to avoid joining
partitions that do not overlap in their min and max range
(the pruning phase, line 7). Non-overlapping partitions do
not produce any join result. If the selectivity values for the
different inequality conditions are known, OCJoin can order
the different joins accordingly.

Fast Joins with Ordering Comparisons

Existing systems handle joins over ordering comparisons
using a cross product and a post-selection predicate, leading
to poor performance. BigDansing provides an efficient adhoc join operator, referred to as OCJoin, to handle these
cases. The main goal of OCJoin is to increase the ability
to process joins over ordering comparisons in parallel and
to reduce its complexity by reducing the algorithm’s search
space. In a nutshell, OCJoin first range partitions a set of
data units and sorts each of the resulting partitions in order
to validate the inequality join conditions in a distributed
fashion. OCJoin works in four main phases: partitioning,
sorting, pruning, and joining (see Algorithm 2).

Joining. OCJoin finally proceeds to join the overlapping
partitions and outputs the join results (lines 9-14). For this,
it applies a distributed sort merge join over the sorted lists,
where some partitions are broadcast to other machines while
keeping the rest locally. Through pruning, OCJoin tells the
underlying distributed processing platform which partitions
to join. It is up to that platform to select the best approach
to minimize the number of broadcast partitions.

Partitioning. OCJoin first selects the attribute, P artAtt,
on which to partition the input D (line 1). We assume
that all join conditions have the same output cardinality.
This can be improved using cardinality estimation techniques [9,27], but it is beyond the scope of the paper. OCJoin
chooses the first attribute involved in the first condition.
For instance, consider again φD (Example 1), OCJoin sets
P artAtt to rate attribute. Then, OCJoin partitions the input
dataset D into nbP arts range partitions based on P artAtt
(line 2). As part of this partitioning, OCJoin distributes
the resulting partitions across all available computing nodes.
Notice that OCJoin runs the range partitioning in parallel.
Next, OCJoin forks a parallel process for each range partition
ki to run the three remaining phases (lines 3-14).

5.

DISTRIBUTED REPAIR ALGORITHMS

Most of the existing repair techniques [5–7, 11, 17, 23, 34]
are centralized. We present two approaches to implement a
repair algorithm in BigDansing. First, we show how our
system can run a centralized data repair algorithm in parallel, without changing the algorithm. In other words, BigDansing treats that algorithm as a black box (Section 5.1).
Second, we design a distributed version of the widely used
equivalence class algorithm [5, 7, 11, 17] (Section 5.2).

Sorting. For each partition, OCJoin creates as many sorting
lists (Sorts) as inequality conditions are in a rule (lines 45). For example, OCJoin creates two sorted lists for φD :
one sorted on rate and the other sorted on salary. Each
list contains the attribute values on which the sort order is
and the tuple identifiers. Note that OCJoin only performs
a local sorting in this phase and hence it does not require
any data transfer across nodes. Since multiple copies of a

5.1

Scaling Data Repair as a Black Box

Overall, we divide a repair task into independent smaller
repair tasks. For this, we represent the violation graph as
a hypergraph containing the violations and their possible
fixes. The nodes represent the elements and each hyperedge
covers a set of elements that together violate a rule, along
with possible repairs. We then divide the hypergraph into
smaller independent subgraphs, i.e., connected components,

1221

CC1
c1
c1

v1

v1

c2

v2

v2

“b1”, not only there are two changes, but the final instance
is also inconsistent.
2

c2
Repair

Fixes

We tackle the above problem by assigning the role of master to one machine and the role of slave to the rest. Every
machine applies a repair in isolation, but we introduce an
extra test in the union of the results. For the violations that
are solved by the master, we mark its changes as immutable,
which prevents us to change an element more than once. If a
change proposed by a slave contradicts a possible repair that
involve a master’s change, the slave repair is undone and a
new iteration is triggered. As a result, the algorithm always
reaches a fix point to produce a clean dataset, because an
updated value cannot change in the following iterations.
BigDansing currently provides two repair algorithms using this approach: the equivalence class algorithm and a
general hypergraph-based algorithm [6, 23]. Users can also
implement their own repair algorithm if it is compatible with
BigDansing’s repair interface.

c4

c4
CC2
c3

v3

c5

Hypergraph

c3

v3

c5

Repair

Fixes

Connected components
of the hypergraph

Figure 7: Data repair as a black box.
and we pass each connected component to an independent
data repair instance.
Connected components. Given an input set of violations,
form at least one rule, BigDansing first creates their hypergraph representation of such possible fixes in a distributed
manner. It then uses GraphX [37] to find all connected components in the hypergraph. GraphX, in turn, uses the Bulk
Synchronous Parallel (BSP) graph processing model [25] to
process the hypergraph in parallel. As a result, BigDansing gets a connected component ID for each hyperedge. It
then groups hyperedges by the connected component ID.
Figure 7 shows an example of connected components in a
hypergraph containing three violations v1, v2, and v3. Note
that violations v1 and v2 can be caused by different rules.
Violations v1 and v2 are grouped in a single connected component CC1, because they share element c2. In contrast, v3
is assigned to a different connected component CC2, because
it does not share any element with v1 or v2.
Independent data repair instance. Once all connected
components are computed, BigDansing assigns each of
them to an independent data repair instance and runs such
repair instances in a distributed manner (right-side of Figure 7). When all data repair instances generate the required
fixes, BigDansing updates the input dataset and pass it
again to the RuleEngine to detect potential violations introduced by the RepairAlgorithm. The number of iterations
required to fix all violations depends on the input rules, the
dataset, and the repair algorithm.
Dealing with big connected components. If a connected component does not fit in memory, BigDansing uses
a k-way multilevel hypergraph partitioning algorithm [22] to
divide it into k equal parts and run them on distinct machines. Unfortunately, naively performing this process can
lead to inconsistencies and contradictory choices in the repair. Moreover, it can fix the same violation independently
in two machines, thus introducing unnecessary changes to
the repair. We illustrate this problem with an example.

5.2

Scalable Equivalence Class Algorithm

The idea of the equivalence class based algorithm [5] is to
first group all elements that should be equivalent together,
and to then decide how to assign values to each group. An
equivalence class consists of pairs of the form (t, A), where
t is a data unit and A is an element. In a dataset D, each
unit t and each element A in t have an associated equivalence
class, denoted by eq(t, A). In a repair, a unique target value
is assigned to each equivalence class E, denoted by targ(E).
That is, for all (t, A) ∈ E, t[A] has the same value targ(E).
The algorithm selects the target value for each equivalence
class to obtain a repair with the minimum overall cost.
We extend the equivalence class algorithm to a distributed
setting by modeling it as a distributed word counting algorithm based on map and reduce functions. However, in
contrast to a standard word count algorithm, we use two
map-reduce sequences. The first map function maps the violations’ possible fixes for each connected component into
key-value pairs of the form hhccID,valuei,counti. The key
hccID,valuei is a composite key that contains the connected
component ID and the element value for each possible fix.
The value count represents the frequency of the element
value, which we initialize to 1. The first reduce function
counts the occurrences of the key-value pairs that share the
same connected component ID and element value. It outputs key-value pairs of the form hhccID,valuei,counti. Note
that if an element exists in multiple fixes, we only count its
value once. After this first map-reduce sequence, another
map function takes the output of the reduce function to create new key-value pairs of the form hccID,hvalue,countii.
The key is the connected component ID and the value is the
frequency of each element value. The last reduce selects the
element value with the highest frequency to be assigned to
all the elements in the connected component ccID.

Example 2: Consider a relational schema D with 3 attributes
A, B, C and 2 FDs A → B and C → B. Given the instance
[t1 ](a1, b1, c1), [t2 ](a1, b2, c1), all data values are in violation: t1 .A, t1 .B, t2 .A, t2 .B for the first FD and t1 .B, t1 .C,
t2 .B, t2 .C for the second one. Assuming the compute nodes
have enough memory only for five values, we need to solve
the violations by executing two instances of the algorithm
on two different nodes. Regardless of the selected tuple, suppose the first compute node repairs a value on attribute A,
and the second one a value on attribute C. When we put
the two repairs together and check their consistency, the
updated instance is a valid solution, but the repair is not
minimal because a single update on attribute B would have
solved both violations. However, if the first node fixes t1 .B
by assigning value “b2” and the second one fixes t2 .B with

6.

EXPERIMENTAL STUDY

We evaluate BigDansing using both real and synthetic
datasets with various rules (Section 6.1). We consider a
variety of scenarios to evaluate the system and answer the
following questions: (i) how well does it perform compared
with baseline systems in a single node setting (Section 6.2)?
(ii) how well does it scale to different dataset sizes compared
to the state-of-the-art distributed systems (Section 6.3)?
(iii) how well does it scale in terms of the number of nodes

1222

ϕ3
ϕ4
ϕ5
ϕ6
ϕ7
ϕ8

(FD):
(UDF):
(UDF):
(FD):
(FD):
(FD):

0

1M

0K
Rule φ1

10

20

0K 0K
Rule φ2
Number of rows

10 1M
0K
Rule φ3

Time (Seconds)

951

10

19
121
31

500

160

2: Statistics of the datasets
Rule
Zipcode → City
∀t1 , t2 ∈ T axB, ¬(t1 .Salary > t2.Salary
∧t1 .Rate < t2 .Rate)
o custkey → c address
Two rows in Customer are duplicates
Two rows in NCVoter are duplicates
Zipcode → State
PhoneNumber → Zipcode
ProviderID → City, PhoneNumber

1500
1000

Violation detection

NADEEF

BigDansing

80

Data repair

60
40
20
0

1%

5%

10%

50%

Error percentage

(a) Rules ϕ1 , ϕ2 , and ϕ3 .
(b) Rule ϕ1 .
Figure 8: Data cleansing times.
The two declarative constraints, DC ϕ2 and FD ϕ3 in Table 3, are translated to SQL as shown below.
ϕ2 :
ϕ3 :

Table 3: Integrity constraints used for testing
(Section 6.4)? (iv) how well does its abstraction support a
variety of data cleansing tasks, e.g., for deduplication (Section 6.5)? and (v) how do its different techniques improve
performance and what is its repair accuracy (Section 6.6)?

6.1

100

2000

Rows
32M
9M
166k

36

Dataset
customer2
NCVoter
HAI

14
30
165

Table
Identifier
ϕ1 (FD):
ϕ2 (DC):

Rows
100K – 40M
100K – 3M
100K – 1907M
19M

Runtime (Seconds)

Dataset
T axA1 –T axA5
T axB1 –T axB3
T P CH1 –T P CH10
customer1

SELECT a.Salary, b.Salary, a.Rate, b.Rate
FROM TaxB a JOIN TaxB b
WHERE a.Salary > b.Salary AND a.Rate < b.Rate;
SELECT a.o custkey, a.c address, b.c address FROM
TPCH a JOIN TPCH b ON a.custkey = b.custkey
WHERE a.c address 6= b.c address;

We also consider two parallel data processing frameworks:
(3) Shark 0.8.0 [38]: This is a scalable data processing engine for fast data analysis. We selected Shark due to its
scalability advantage over existing distributed SQL engines.
(4) Spark SQL v1.0.2:It is an experimental extension of
Spark v1.0 that allows users run relational queries natively
on Spark using SQL or HiveQL. We selected this system as,
like BigDansing, it runs natively on top of Spark.
We ran our experiments in two different settings: (i) a
single-node setting using a Dell Precision T7500 with two
64-bit quad-core Intel Xeon X5550 (8 physical cores and 16
CPU threads) and 58GB RAM and (ii) a multi-node setting
using a compute cluster of 17 Shuttle SH55J2 machines (1
master with 16 workers) equipped with Intel i5 processors
with 16GB RAM constructed as a star network.

Setup

Table 2 summarizes the datasets and Table 3 shows the
rules we use for our experiments.
(1) TaxA. Represents personal tax information in the
US [11]. Each row contains a person’s name, contact information, and tax information. For this dataset, we use
the FD rule ϕ1 in Table 3. We introduced errors by adding
random text to attributes City and State at a 10% rate.
(2) TaxB. We generate TaxB by adding 10% numerical random errors on the Rate attribute of TaxA. Our goal is to
validate the efficiency with rules that have inequality conditions only, such as DC ϕ2 in Table 3.
(3) TPCH. From the TPC-H benchmark data [2], we joined
the lineitem and customer tables and applied 10% random
errors on the address. We use this dataset to test FD rule ϕ3 .
(4) Customer. In our deduplication experiment, we use
TPC-H customer with 4.5 million rows to generate tables
customer1 with 3x exact duplicates and customer2 with 5x
exact duplicates. Then, we randomly select 20% of the total
number of tuples, in both relations, and duplicate them with
random edits on attributes name and phone.
(5) NCVoter. This is a real dataset that contains North Carolina voter information. We added 20% random duplicate
rows with random edits in name and phone attributes.
(6)
Healthcare
Associated
Infections
(HAI)
(http://www.hospitalcompare.hhs.gov). This real dataset
contains hospital information and statistics measurements
for infections developed during treatment. We added 10%
random errors on the attributes covered by the FDs and
tested four combinations of rules (ϕ6 – ϕ8 from Table 3).
Each rule combination has its own dirty dataset.
To our knowledge, there exists only one full-fledged data
cleansing system that can support all the rules in Table 3:
(1) NADEEF [7]: An open-source single-node platform supporting both declarative and user defined quality rules.
(2) PostgreSQL v9.3: Since declarative quality rules can be
represented as SQL queries, we also compare to PostgreSQL
for violation detection. To maximize benefits from large
main memory, we configured it using pgtune [30].

6.2

Single-Node Experiments

For these experiments, we start comparing BigDansing
with NADEEF in the execution times of the whole cleansing process (i.e., detection and repair). Figure 8(a) shows
the performance of both systems using the TaxA and TPCH
datasets with 100K and 1M (200K for ϕ2 ) rows. We observe
that BigDansing is more than three orders of magnitude
faster than NADEEF in rules ϕ1 (1M) and ϕ2 (200K), and
ϕ3 (1M). In fact, NADEEF is only “competitive” in rule ϕ1
(100K), where BigDansing is only twice faster. The high
superiority of BigDansing comes from two main reasons:
(i) In contrast to NADEEF, it provides a finer granular abstraction allowing users to specify rules more efficiently; and
(ii) It performs rules with inequality conditions in an efficient
way (using OCJoin). In addition, NADEEF issues thousands of SQL queries to the underlying DBMS. for detecting violations. We do not report results for larger datasets
because NADEEF was not able to run the repair process for
more than 1M rows (300K for ϕ2 ).
We also observed that violation detection was dominating
the entire data cleansing process. Thus, we ran an experiment for rule ϕ1 in TaxA with 1M rows by varying the
error rate. Violation detection takes more than 90% of the
time, regardless of the error rate (Figure 8(b)). In particular, we observe that even for a very high error rate (50%),
the violation detection phase still dominates the cleansing
process. Notice that for more complex rules, such as rule
ϕ2 , the dominance of the violation detection is even ampli-

1223

2000
BigDansing

Spark SQL

BigDansing

Spark SQL

NADEEF

Shark

14000

NADEEF

Shark

NADEEF

Shark

200,000

Dataset size (rows)

100,000

Dataset size (rows)

562

377

1,000,000

44

140

500

0

300,000

423

9336

8780
4529

100,000

1000

29
8
47

0

10,000,000

PostgreSQL

10

4000

1500

6
34
2
5
7

1,000,000

6000

62

100,000

8000

2000

80

0

86

1000

18
368
37
8
47

2000

10000

30

3000

12000

7982

4000

PostgreSQL

10
833
2133
3731

3183

PostgreSQL

Runtime (Seconds)

16000
Runtime (Seconds)

Spark SQL
4153

5000

BigDansing

5
55
0.264
4
2

Runtime (Seconds)

6000

10,000,000

Dataset size (rows)

10M

Dataset size (rows)

0

1M

2M

3M

Dataset size (rows)

0

196133
65115

138932
46907

11880

40000

8670

80000

92236
30195

20000

Spark SQL

120000

5113

40000

BigDansing-Hadoop

2307
52886
17872

Time (Seconds)

60000
7730

40M

Shark

80000

5319

20M

100000

BigDansing-Spark
160000

712
24803
9263

0

337
2302
662

5000

150
865
313

10000

Spark SQL

1240

Shark

14113

Spark SQL

15000

121
503
159
3739

Time (Seconds)

BigDansing-Hadoop

200000
BigDansing-Spark

120000
Time (Seconds)

BigDansing-Spark

20000

126822

(a) TaxA data with ϕ1
(b) TaxB data with ϕ2
(c) TPCH data with ϕ3
Figure 9: Single-node experiments with ϕ1 , ϕ2 , and ϕ1

647M 959M 1271M1583M1907M
Dataset size (rows)

(a) TaxA data with ϕ1
(b) TaxB data with ϕ2
(c) Large TPCH on ϕ3
Figure 10: Multi-nodes experiments with ϕ1 , ϕ2 and ϕ3

6.3

fied. Therefore, to be able to extensively evaluate BigDansing, we continue our experiments focusing on the violation
detection phase only, except if stated otherwise.
Figures 9(a), Figure 9(b), and 9(c) show the violation
detection performance of all systems for TaxA, TaxB, and
TPCH datasets. TaxA and TPCH datasets have 100K, 1M,
and 10M rows and TaxB has 100K, 200K, and 300K rows.
With equality-based rules ϕ1 and ϕ3 , we observe that PostgreSQL is always faster than all other systems on the small
datasets (100K rows). However, once we increase the size by
one order of magnitude (1M rows), we notice the advantage
of BigDansing: it is at least twice faster than PostgreSQL
and more than one order of magnitude faster than NADEEF.
For the largest dataset (10M rows), BigDansing is almost
two orders of magnitude faster than PostgreSQL for the FD
in TaxA and one order of magnitude faster than PostgreSQL
for the DC in TaxA. For the FD in TPCH, BigDansing is
twice faster than PostgreSQL. It is more than three orders
of magnitude faster than NADEEF for all rules. Overall, we
observe that BigDansing performs similarly to Spark SQL.
The small difference is that Spark SQL uses multithreading
better than BigDansing. We can explain the superiority
of BigDansing compared to the other systems along two
reasons: (i) BigDansing reads the input dataset only once,
while PostgreSQL and Shark read it twice because of the
self joins; and (ii) BigDansing does not generate duplicate
violations, while SQL engines do when comparing tuples using self-joins, such as for TaxA and TPCH’s FD. Concerning
the inequality-based rule ϕ2 , we limited the runtime to four
hours for all systems. BigDansing is one order of magnitude faster than all other systems for 100K rows. For 200K
and 300K rows, it is at least two orders of magnitude faster
than all baseline systems. Such performance superiority is
achieved by leveraging the inequality join optimization that
is not supported by the baseline systems.

Multi-Node Experiments

We now compare BigDansing with Spark SQL and Shark
in the multi-node setting. We also implemented a lighter
version of BigDansing on top of Hadoop MapReduce to
show BigDansing independence w.r.t. the underlying
framework. We set the size of TaxA to 10M, 20M, and 40M
rows. Moreover, we tested the inequality DC ϕ2 on TaxB
dataset with sizes of 1M, 2M, and 3M rows. We limited the
runtime to 40 hours for all systems.
BigDansing-Spark is slightly faster than Spark SQL for
the equality rules in Figure 10(a). Even though BigDansing-Spark and Shark are both implemented on top of Spark,
BigDansing-Spark is up to three orders of magnitude faster
than Shark. Even BigDansing-Hadoop is doing better than
Shark (Figure 10(a)). This is because Shark does not process joins efficiently. The performance of BigDansing over
baseline systems is magnified when dealing with inequalities.
We observe that BigDansing-Spark is at least two orders
of magnitude faster than both Spark SQL and Shark (Figure 10(b)). We had to stop Spark SQL and Shark executions
after 40 hours of runtime; both Spark SQL and Shark are
unable to process the inequality DC efficiently.
We also included a testing for large TPCH datasets of
sizes 150GB, 200GB, 250GB, and 300GB (959M, 1271M,
1583M, and 1907M rows resp.) producing between 6.9B and
13B violations. We excluded Shark as it could not run on
these larger datasets. BigDansing-Spark is 16 to 22 times
faster than BigDansing-Hadoop and 6 to 8 times faster
than Spark SQL (Figure 10(c)). BigDansing-Spark significantly outperforms Spark SQL since it has a lower I/O complexity and an optimized data cleansing physical execution
plan. Moreover, the performance difference between BigDansing-Spark and BigDansing-Hadoop stems from Spark
being generally faster than Hadoop; Spark is an in-memory
data processing system while Hadoop is disk-based.

1224

0

ncvoter

#-workers

cust1
Dataset

cust2

40000
20000
0

61772
82524

22912
27078

Runtime (Seconds)

Cross product

60000

126

16

UCrossProduct

80000

103

8

2000

OCJoin

100000

97
4279
4953

12 4

4000
125

0

6000

525

25000

8000

109

50000

Shark

164

75000

BigDansing

103

Spark SQL

100000

8435

10000

BigDansing
Time (Seconds)

Runtime (Seconds)

125000

100,000

200,000

300,000

Data size (rows)

(a) 500M rows TPCH on ϕ3
(b) Deduplication in BigDansing
(c) OCJoin and UCrossProdut
Figure 11: Results for the experiments on (a) scale-out, (b) deduplication, and (c) physical optimizations.

6.4

Scaling BigDansing Out

Dansing using only the Detect operator. We see in Figure 12(a) that running a UDF using the full BigDansing
API makes BigDansing three orders of magnitudes faster
compared to using Detect only. This clearly shows the benefits of the five logical operators, even for single-node settings.

We compared the speedup of BigDansing-Spark to
Spark SQL when increasing the number of workers on a
dataset size of 500M rows. We observe that BigDansingSpark is at least 3 times faster than Spark SQL starting
from one single worker and up to 16 (Figure 11(a)). We
also notice that BigDansing-Spark is about 1.5 times faster
than Spark SQL while using only half the number of workers used by Spark SQL. Although both BigDansing-Spark
and Spark SQL generally have a good scalability, BigDansing-Spark performs better than Spark SQL on large input
datasets since it does not copy the input data twice.

6.5

Scalable data repair. We study the runtime efficiency
of the repair algorithms used by BigDansing. We ran an
experiment for rule ϕ1 in TaxA with 1M rows by varying
the error rate and considering two versions of BigDansing:
the one with the parallel data repair and a baseline with a
centralized data repair, such as in NADEEF. Figure 12(b)
shows the results. The parallel version outperforms the centralized one, except when the error rate is very small (1%).
For higher rates, BigDansing is clearly faster, since the
number of connected components to deal with in the repair
process increases with the number of violations and thus
the parallelization provides a stronger boost. Naturally, our
system scales much better with the number of violations.
Repair accuracy. We evaluate the accuracy of BigDansing using the traditional precision and recall measure:
precision is the ratio of correctly updated attributes (exact
matches) to the total number of updates; recall is the ratio
of correctly updated attributes to the total number of errors.
We test BigDansing with the equivalence class algorithm
using HAI on the following rule combinations: (a) FD ϕ6 ;
(b) FD ϕ6 and FD ϕ7 ; (c) FD ϕ6 , FD ϕ7 , and FD ϕ8 . Notice
that BigDansing runs (a)-combination alone while it runs
(b)-combination and (c)-combination concurrently.
Table 4 shows the results for the equivalence class algorithm in BigDansing and NADEEF. We observe that BigDansing achieves similar accuracy and recall as the one obtained in a centralized system, i.e., NADEEF. In particular, BigDansing requires the same number of iterations as
NADEEF even to repair all data violations when running
multiple rules concurrently. Notice that both BigDansing
and NADEEF might require more than a single iterations
when running multiple rules because repairing some violations might cause new violations. We also tested the equivalence class in BigDansing using both the black box and
scalable repair implementation, and both implementations
achieved similar results.
We also test BigDansing with the hypergraph algorithm
using DC rule φD on a TaxB dataset. As the search space
of the possible solutions in φD is huge, the hypergraph algorithm uses quadratic programming to approximates the
repairs in φD [6]. Thus, we use the euclidean distance to
measure the repairs accuracy on the repaired data attributes
compared to the attributes of the ground truth.

Deduplication in BigDansing

We show that one can run a deduplication task with BigDansing. We use cust1, cust2, and NCVoters datasets on
our compute cluster. We implemented a Java version of
Levenshtein distance and use it as a UDF in both BigDansing and Shark. Note that we do not consider Spark SQL
in this experiment since UDFs cannot be implemented directly within Spark SQL. That is, to implement a UDF in
Spark SQL the user has to either use a Hive interface or apply a post processing step on the query result. Figure 11(b)
shows the results. We observe that BigDansing outperforms Shark for both small datasets as well as large datasets.
In particular, we see that for cust2 BigDansing outperforms
Shark up to an improvement factor of 67. These results not
only show the generality of BigDansing supporting a deduplication task, but also the high efficiency of our system.

6.6

BigDansing In-Depth

Physical optimizations. We first focus on showing the
benefits in performance of the UCrossProduct and OCJoin
operators. We use the second inequality DC in Table 3 with
TaxB dataset on our multi-node cluster. Figure 11(c) reports the results of this experiment. We notice that the
UCrossProduct operator has a slight performance advantage
compared to the CrossProduct operator. This performance
difference increases with the dataset size. However, by using
the OCJoin operator, BigDansing becomes more than two
orders of magnitudes faster compared to both cross product
operators (up to an improvement factor of 655).
Abstraction advantage. We now study the benefits of
BigDansing’s abstraction. We consider the deduplication
scenario in previous section with the smallest TaxA dataset
on our single-node machine. We compare the performance
difference between BigDansing using its full API and Big-

1225

φD

100000

precision

recall

precision

recall

Iterations

0.713
0.861
0.923
|R,G|/e
17.1

0.776
0.875
0.928
|R,G|
8183

0.714
0.861
0.924
|R,G|/e
17.1

0.777
0.875
0.929
|R,G|
8221

1
2
3
Iter.
5

1000
100
10
1

(a)

Table 4: Repair quality using HAI and TaxB datasets.

140

BigDansing

120

BigDansing (with serial repair)

100
80
60
40
20

Full API Detect only
Abstraction

0

(b)

1%

5%

10%

50%

Error percentage

Figure 12: (a) Abstraction and (b) scalable repair.

Table 4 shows the results for the hypergraph algorithm in
BigDansing and NADEEF. Overall, we observe that BigDansing achieves the same average distance (|R, G|/e) and
similar total distance (|R, G|) between the repaired data R
and the ground truth G. Again, BigDansing requires the
same number of iterations as NADEEF to completely repair
the input dataset. These results confirm that BigDansing
achieves the same data repair quality as in a single node setting. This by providing better data cleansing runtimes and
scalability than baselines systems.

7.

160

BigDansing
10000

Time (Seconds)

Rule(s)
ϕ6
ϕ6 &ϕ7
ϕ6 –ϕ8

BigDansing

Time (Seconds)

NADEEF

gate queries by performing cleansing over small samples of
the source data. SampleClean focuses on obtaining unbiased
query results with confidence intervals, while BigDansing
focuses on providing a scalable framework for data cleansing.
In fact, one cannot use SampleClean in traditional query
processing where the entire input dataset is required.
As shown in the experiment section, scalable data processing platform, such as MapReduce [8] or Spark [41], can
implement the violation detection process. However, coding the violation detection process on top of these platforms is a tedious task and requires technical expertise.
We also showed that one could use a declarative system
(e.g., Hive [32], Pig [28], or Shark [1]) on top of one of these
platforms and re-implement the data quality rules using its
query language. However, many common rules, such as rule
φU , go beyond declarative formalisms. Finally, these frameworks do not natively support inequality joins.
Implementing efficient theta-joins in general has been
largely studied in the database community [9, 27]. Studies vary from low-level techniques, such as minimizing disk
accesses for band-joins by choosing partitioning elements using sampling [9], to how to map arbitrary join conditions to
Map and Reduce functions [27]. These proposals are orthogonal to our OCJoin algorithm. In fact, in our system, they
rely at the executor level: if they are available in the underlying data processing platform, they can be exploited when
translating the OCJoin operator at the physical level.
Dedoop [24] detects duplicates in relational data using
Hadoop. It exploits data blocking and parallel execution
to improve performance. Unlike BigDansing, this servicebased system maintains its own data partitioning and distribution across workers. Dedoop does not provide support
for scalable validation of UDFs, nor repair algorithms.

RELATED WORK

Data cleansing, also called data cleaning, has been a
topic of research in both academia and industry for decades,
e.g., [6, 7, 11–15, 17–19, 23, 29, 34, 36]. Given some “dirty”
dataset, the goal is to find the most likely errors and a possible way to repair these errors to obtain a “clean” dataset.
In this paper, we are interested in settings where the detection of likely errors and the generation of possible fixes is
expressed through UDFs. As mentioned earlier, traditional
constraints, e.g., FDs, CFDs, and DCs, are easily expressed
in BigDansing and hence any errors detected by these constraints will be detected in our framework. However, our
goal is not to propose a new data cleansing algorithm but
rather to provide a framework where data cleansing, including detection and repair, can be performed at scale using a
flexible UDF-based abstraction.
Work in industry, e.g., IBM QualityStage, SAP BusinessObjects, Oracle Enterprise Data Quality, and Google Refine has mainly focused on the use of low-level ETL rules [3].
These systems do not support UDF-based quality rules in a
scalable fashion as in BigDansing.
Examples of recent work in data repair include cleaning
algorithms for DCs [6, 17] and other fragments, such as FDs
and CFDs [4, 5, 7, 11, 17]. These proposals focus on specific
logical constraints in a centralized setting, without much regards to scalability and flexibility as in BigDansing. As
shown in Section 5, we adapted two of these repair algorithms to our distributed platform.
Another class of repair algorithms use machine learning
tools to clean the data. Examples include SCARE [39] and
ERACER [26]. There are also several efforts to include users
(experts or crowd) in the cleaning process [33,40]. Both lines
of research are orthogonal to BigDansing.
Closer to our work is NADEEF [7], which is a generalized data cleansing system that detects violations of various
data quality rules in a unified programming interface. In
contrast to NADEEF, BigDansing: (i) provides a richer
programming interface to enable efficient and scalable violation detection, i.e., block(), scope(), and iterate(), (ii)
enables several optimizations through its plans, (iii) introduces the first distributed approaches to data repairing.
SampleClean [35] aims at improving the accuracy of aggre-

8.

CONCLUSION

BigDansing, a system for fast and scalable big data
cleansing, enables ease-of-use through a user-friendly programming interface. Users use logical operators to define rules which are transformed into a physical execution
plan while performing several optimizations. Experiments
demonstrated the superiority of BigDansing over baseline
systems for different rules on both real and synthetic data
with up to two order of magnitudes improvement in execution time without sacrificing the repair quality. Moreover, BigDansing is scalable, i.e., it can detect violation on
300GB data (1907M rows) and produce 1.2TB (13 billion)
violations in a few hours.
There are several future directions. One relates to abstracting the repairing process through logical operators,
similar to violation detection. This is challenging because
most of the existing repair algorithms use different heuristics
to find an “optimal” repair. Another direction is to exploit
opportunities for multiple data quality rule optimization.

1226

9.

REFERENCES

[28] C. Olston, B. Reed, U. Srivastava, R. Kumar, and
A. Tomkins. Pig Latin: A Not-so-foreign Language for
Data Processing. In SIGMOD, 2008.
[29] E. Rahm and H. H. Do. Data cleaning: Problems and
current approaches. IEEE Data Eng. Bull., 23(4):3–13,
2000.
[30] G. Smith. PostgreSQL 9.0 High Performance: Accelerate
your PostgreSQL System and Avoid the Common Pitfalls
that Can Slow it Down. Packt Publishing, 2010.
[31] N. Swartz. Gartner warns firms of ‘dirty data’. Information
Management Journal, 41(3), 2007.
[32] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka,
S. Anthony, H. Liu, P. Wyckoff, and R. Murthy. Hive: A
Warehousing Solution over a Map-reduce Framework.
PVLDB, 2(2):1626–1629, 2009.
[33] Y. Tong, C. C. Cao, C. J. Zhang, Y. Li, and L. Chen.
CrowdCleaner: Data cleaning for multi-version data on the
web via crowdsourcing. In ICDE, 2014.
[34] M. Volkovs, F. Chiang, J. Szlichta, and R. J. Miller.
Continuous data cleaning. In ICDE, 2014.
[35] J. Wang, S. Krishnan, M. J. Franklin, K. Goldberg,
T. Kraska, and T. Milo. A Sample-and-Clean Framework
for Fast and Accurate Query Processing on Dirty Data. In
SIGMOD, 2014.
[36] J. Wang and N. Tang. Towards dependable data repairing
with fixing rules. In SIGMOD, 2014.
[37] R. S. Xin, J. E. Gonzalez, M. J. Franklin, and I. Stoica.
GraphX: A Resilient Distributed Graph System on Spark.
In First International Workshop on Graph Data
Management Experiences and Systems, GRADES. ACM,
2013.
[38] R. S. Xin, J. Rosen, M. Zaharia, M. J. Franklin, S. Shenker,
and I. Stoica. Shark: SQL and Rich Analytics at Scale. In
SIGMOD, 2013.
[39] M. Yakout, L. Berti-Equille, and A. K. Elmagarmid. Don’t
be SCAREd: use SCalable Automatic REpairing with
maximal likelihood and bounded changes. In SIGMOD,
2013.
[40] M. Yakout, A. K. Elmagarmid, J. Neville, M. Ouzzani, and
I. F. Ilyas. Guided data repair. PVLDB, 2011.
[41] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and
I. Stoica. Spark: Cluster computing with working sets. In
HotCloud, 2010.

[1] Shark (Hive on Spark). https://github.com/amplab/shark.
[2] TPC-H benchmark version 2.14.4.
http://www.tpc.org/tpch/.
[3] C. Batini and M. Scannapieco. Data Quality: Concepts,
Methodologies and Techniques. Springer, 2006.
[4] G. Beskales, I. F. Ilyas, and L. Golab. Sampling the repairs
of functional dependency violations under hard constraints.
PVLDB, 3:197–207, 2010.
[5] P. Bohannon, M. Flaster, W. Fan, and R. Rastogi. A
cost-based model and effective heuristic for repairing
constraints by value modification. In SIGMOD, 2005.
[6] X. Chu, I. F. Ilyas, and P. Papotti. Holistic Data Cleaning:
Putting Violations into Context. In ICDE, 2013.
[7] M. Dallachiesa, A. Ebaid, A. Eldawy, A. Elmagarmid, I. F.
Ilyas, M. Ouzzani, and N. Tang. NADEEF: A Commodity
Data Cleaning System. In SIGMOD, 2013.
[8] J. Dean and S. Ghemawat. MapReduce: Simplified Data
Processing on Large Clusters. Communications of the
ACM, 51(1):107–113, 2008.
[9] D. J. DeWitt, J. F. Naughton, and D. A. Schneider. An
evaluation of non-equijoin algorithms. In VLDB, 1991.
[10] W. Fan and F. Geerts. Foundations of Data Quality
Management. Morgan & Claypool, 2012.
[11] W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis.
Conditional Functional Dependencies for Capturing Data
Inconsistencies. ACM Transactions on Database Systems
(TODS), 33(2):6:1–6:48, 2008.
[12] W. Fan, F. Geerts, N. Tang, and W. Yu. Conflict resolution
with data currency and consistency. J. Data and
Information Quality, 5(1-2):6, 2014.
[13] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Interaction
between record matching and data repairing. In SIGMOD,
2011.
[14] W. Fan, J. Li, N. Tang, and W. Yu. Incremental Detection
of Inconsistencies in Distributed Data. In ICDE, 2012.
[15] I. Fellegi and D. Holt. A systematic approach to automatic
edit and imputation. J. American Statistical Association,
71(353), 1976.
[16] T. Friedman. Magic quadrant for data quality tools.
http://www.gartner.com/, 2013.
[17] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. The
LLUNATIC Data-Cleaning Framework. PVLDB,
6(9):625–636, 2013.
[18] F. Geerts, G. Mecca, P. Papotti, and D. Santoro. Mapping
and Cleaning. In ICDE, 2014.
[19] M. Interlandi and N. Tang. Proof positive and negative in
data cleaning. In ICDE, 2015.
[20] E. Jahani, M. J. Cafarella, and C. Ré. Automatic
optimization for mapreduce programs. PVLDB,
4(6):385–396, 2011.
[21] A. Jindal, J.-A. Quiané-Ruiz, and S. Madden. Cartilage:
Adding Flexibility to the Hadoop Skeleton. In SIGMOD,
2013.
[22] G. Karypis and V. Kumar. Multilevel K-way Hypergraph
Partitioning. In Proceedings of the 36th Annual
ACM/IEEE Design Automation Conference, DAC. ACM,
1999.
[23] S. Kolahi and L. V. S. Lakshmanan. On Approximating
Optimum Repairs for Functional Dependency Violations. In
ICDT, 2009.
[24] L. Kolb, A. Thor, and E. Rahm. Dedoop: Efficient
Deduplication with Hadoop. PVLDB, 2012.
[25] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert,
I. Horn, N. Leiser, and G. Czajkowski. Pregel: A System
for Large-scale Graph Processing. In SIGMOD, 2010.
[26] C. Mayfield, J. Neville, and S. Prabhakar. ERACER: a
database approach for statistical inference and data
cleaning. In SIGMOD, 2010.
[27] A. Okcan and M. Riedewald. Processing theta-joins using
mapreduce. In SIGMOD, 2011.

APPENDIX
A. JOB EXAMPLE
1
2
3
4
5
6
7
8
9
10
11
12
}

BigDansing job = new BigDansing(”Example Job” );
String schema = ”name,zipcode,city, state , salary , rate ”;
job .addInputPath(schema,”D1”, ”S”, ”T”);
job .addInputPath(schema,”D2”, ”W”);
job .addScope(Scope, ”S”);
job .addBlock(Block1, ”S”) ;
job .addBlock(Block2, ”T”);
job . addIterate ( ”M”, Iterate1 , ”S”, ”T”);
job . addIterate ( ”V”, Iterate2 , ”W”, ”M”);
job .addDetect(Detect, ”V”);
job .addGenFix(GenFix,”V”);
job .run() ;

Listing 3: Example of a user’s BigDansing job.
Let us explain with a BigDansing job example how users
can specify to BigDansing in which sequence to run their
logical operators. Users can fully control the execution flow
of their logical operators via data labels, which in other words
represent the data flow of a specific input dataset. For example, users would write the BigDansing job in Listing 3
to generate the logical plan in Section 3.2. First of all, users
create a job instance to specify BigDansing their requirements (Line 1). Users specify the input datasets they want

1227

to process by optionally providing their schema (Line 2) and
their path (D1 and D2 in Lines 3 & 4). Additionally, users
label input datasets to defines the number data flows they
desire to have (S and T for D1 and W for D2). Notice that, in
this example, the job creates a copy of D1. Then, users specify the sequence of logical operations they want to perform
to each data flow (Lines 5-11). BigDansing respect the order in which users specify their logical operators, e.g., BigDansing will first perform Scope and then Block1. Users
can also specify arbitrary number of inputs and outputs.
For instance, in Iterate1 get data flows S and T as input and
outputs a signal data flow M. As last step, users run their
job as in Line 12.

RDF input

Subject

Predicate

Paul

student_in

Yale

Subject

John

student_in

UCLA

Paul

Yale

Sally

student_in

UCLA

John

UCLA

WIlliam

professor_in

UCLA

Sally

UCLA

Paul

advised_by

WIlliam

Paul

WIlliam

John

advised_by

WIlliam

John

WIlliam

Sally

advised_by

WIlliam

Sally

WIlliam

(2) Block (Subject)

B1

B2

LOGICAL OPERATORS LISTINGS
B3

We show in Listing 4 the code for Scope, Listing 5 the code
for Block and in Listing 6 the code for Iterate in rule φF .

(3) Iterate

Object

Subject

Object1

Paul

Yale

Paul

William

Yale

Paul

WIlliam

John

WIlliam

UCLA

Sally

William

UCLA

John

UCLA

John

WIlliam

Sally

WIlliam

Sally

UCLA

(5) Iterate
(Paul, John)
(Paul, Sally)
(John, Sally)

Object1

Subject

William

Paul

Object2
Yale

WIlliam

John

UCLA

William

Sally

UCLA

(6) Detect

(7) GenFix

(Paul, John)
(Paul, Sally)

Paul.Yale = John.UCLA
Paul.Yale = Sally..UCLA

Figure 13: Logical operators execution for the RDF
rule example.

Listing 4: Code example for the Scope operator.

John

Sally

se
vi

in

William

student_in

nt_
de

advised_by

stu

y

_b

ed

vis

Yale

ad

Listing 5: Code example for the Block operator.

d_

by

Paul

}

student_in

public String block(Tuple t) {
1
return t . getCellValue (0) ;// zipcode

Object2

(4) Block (Object1)

B1

public Tuple scope(Tuple in ) {
1
Tuple t = new Tuple(in.getID());
2
t . addCell( in . getCellValue (1)) ;// zipcode
3
t . addCell( in . getCellValue (2)) ;// city
4
return t ;
}

Object

Subject

ad

B.

(1) Scope (student_in,
advised_by)

Object

UCLA

professor_in

public Iterable <TuplePair> iterator( ListTupleList <String> in) {
1
ArrayList <TuplePair> tp = new ArrayList<TuplePair>();
2
List <Tuple> inList = in. getIterator () . next() .getValue() ;
3
for ( int i = 0; i < inList . size () ; i ++) {
4
for ( int j = i + 1; j < inList . size () ; j++) {
5
tp .add(new TuplePair( inList . get( i ) , inList . get( j ))) ;
6
}
7
}
8
return tp ;
}

Figure 14: Example of an RDF graph.
RDF

Scope

Block

Iterate

Block

Iterate

Detect

GenFix

Figure 15: An RDF logical plan in BigDansing.
Listing 6: Code example for the Iterate operator.

C.

in each group. The output of this Iterate is passed to the
second Block and Iterate operators to group incoming triples
based on advisor name (i.e., William). The Detect operator
generates violations based on incoming triples with different university names. Finally, the GenFix operator suggests
fixing the university names in incoming violations.

RDF DATA REPAIR

Let us explain how BigDansing works for RDF data
cleansing with an example. Consider an RDF dataset containing students, professors, and universities, where each
student is enrolled in one university and has one professor
as advisor. The top-left side of Figure 13 shows this RDF
dataset as set of triples (RDF input) and Figure 14 shows
the graph representation of such an RDF dataset. Let’s assume that there cannot exist two graduate students in two
different universities and have the same professor as advisor.
According to this rule, there are two violations in this RDF
dataset: (Paul, John) and (Paul, Sally).
Figure 15 illustrates the logical plan generated by BigDansing to clean this RDF dataset while the rest of Figure 14 explains the role of each logical operator on the RDF
data. The plan starts with the Scope operator where it removes unwanted RDF projects on attributes Subject and
Object and passes only those triples with advised by and
study in predicates to the next operator. After that, we apply the first Block to group triples based on student name
(i.e., Paul) followed by Iterate operator to join the triples

D.

SPARK CODE FOR φF IN EXAMPLE 1

We show in Listing 7 an example of the physical translation of the logical plan for rule φF in Spark. Lines 3 to 7
in Listing 7 handles the processing of the UDF operators in
Listings 4, 5, 6, 1 and 2. Lines 8 to 16 implement the repair algorithm in Section 5.2. The repair algorithm invokes
Listing 8 in line 9 to build the graph of possible fixes and
finds the graph’s connected components through GraphX.
Finally, lines 17 to 19 in Listing 7 applies the selected candidate fixes to the input dataset. Note that the Spark plan
translation in Listing 7 is specific to rule φF , where lines 3
to 16 changes according the the input rule or UDF. The repair algorithm in Listing 7 seems far complex than the rule
engine. However, as the number of violations in a dataset
does not usually exceeds 10% of the input dataset size, the
rule engine dominates the execution runtime because it has

1228

//Input
1 JavaPairRDD<LongWritable,Text> inputData =
sc.hadoopFile(InputPath,
org .apache.hadoop.mapred.TextInputFormat.class,
LongWritable. class , Text. class , minPartitions ) ;
2 JavaRDD<Tuple> tupleData = tmpLogData.map(new
StringToTuples());
//−−−−Rule Engine−−−−
3 JavaRDD<Tuple> scopeData = tupleData.map(new fdScope());
4 JavaPairRDD<Key,Iterable<Tupke>> blockData =
scopedData.groupByKey(new
fdBlock()) ;
5 JavaRDD<TuplePair> iterateData = blockingData.map(new
fdIterate());
6 JavaRDD<Violation> detectData = iterateData.map(new fdDetect());
7 JavaRDD<Fix> genFixData = detectData.map(new fdGenFix());
//−−−−Repair Algorithm−−−−
8 JavaRDD<Edge<Fix>> edges = genFixData.map(new extractEdges());
9 JavaRDD<EdgeTriplet<Object, Fix>> ccRDD = new
RDDGraphBuilder(edges.rdd()).buildGraphRDD();
10 JavaPairRDD<Integer, Fix> groupedFix = ccRDD.mapToPair(new
extractCC());
11 JavaPairRDD<Integer, Tuple4<Integer, Integer, String , String>>
stringFixUniqueKeys = groupedFix .flatMapToPair(new
extractStringFixUniqueKey()) ;
12 JavaPairRDD<Integer, Tuple4<Integer, Integer, String , String>>
countStringFixes = stringFixUniqueKeys .reduceByKey(new
countFixes());
13 JavaPairRDD<Integer, Tuple4<Integer, Integer, String , String>>
newUniqueKeysStringFix = countStringFixes .mapToPair(new
extractReducedCellValuesKey()) ;
14 JavaPairRDD<Integer, Tuple4<Integer, Integer, String , String>>
reducedStringFixes = newUniqueKeysStringFix .reduceByKey(new
reduceStringFixes ()) ;
15 JavaPairRDD<Integer,Fix> uniqueKeysFix =
groupedFix.flatMapToPair(new
extractFixUniqueKey()) ;
16 JavaRDD<Fix> candidateFixes candidateFixes =
uniqueKeysFix. join ( reducedStringFixes ) . values () .flatMap(new
getFixValues()) ;
//−−−−Apply results to input−−−−
17 JavaPairRDD<Long, Iterable<Fix>> fixRDD = candidateFixes.keyBy(
new getFixTupleID()).groupByKey();
18 JavaPairRDD<Long, Tuple> dataRDD = tupleData.keyBy(new
getTupleID());
19 JavaRDD<Tuple> newtupleData =
dataRDD.leftOuterJoin(fixRDD).map(
new ApplyFixes());

Scope

Scope

Block

Block

Block

Iterate

Iterate

Iterate

Detect

Detect

Block

GenFix

GenFix

Iterate

C2

C1

C3

Block

GenFix

Figure 16: Example of bushy plan
The plan starts with applying the Scope operator. Instead
of calling Scope for each rule, we only invoke Scope for each
relation. Next we apply the Block operator as follows: block
on “City” for c1, on “Role” for c2, and on “LID” and “MID”
for c3. Thereafter, for c1 and c2, we proceed to iterate
candidate tuples with violations (Iterate) and feed them to
Detect and GenFix operators respectively. For c3, we iterate
over all employees who are managers combine them with
data units from the global table G and then finally feed
them to the Detect and GenFix operators.
The key thing to note in the above bushy data cleaning
plan is that while each rule has its own Detect/GenFix operator, the plan shares many of the other operators in order
to reduce: (1) the number of times data is read from the
base relations, and (2) the number of duplicate data units
generated and processed in the dataflow.

F.

DATA STORAGE MANAGER

BigDansing applies three different data storage optimizations: (i) data partitioning to avoid shuffling large amounts
of data; (ii) data replication to efficiently support a large variety of data quality rules; and (iii) data layouts to improve
I/O operations. We describe them below.

more data to process compared to the repair algorithm.

(1) Partitioning. Typically, distributed data storage systems split data files into smaller chunks based on size. In
contrast, BigDansing partitions a dataset based on its content, i.e., based on attribute values. Such a logical partitioning allows to co-locate data based on a given blocking key.
As a result, BigDansing can push down the Block operator
to the storage manager. This allows avoiding to co-locate
datasets while detecting violations and hence to significantly
reduce the network costs.

class RDDGraphBuilder(var edgeRDD: RDD[Edge[Fix]]) {
def buildGraphRDD:JavaRDD[EdgeTriplet[VertexId, Fix]] = {
var g: Graph[Integer , Fix ] = Graph.fromEdges(edgeRDD, 0)
new JavaRDD[EdgeTriplet[VertexId,Fix ]](
g.connectedComponents().triplets )
}
}

Listing 8: Scala code for GraphX to find connected
components of fixes

E.

L

Detect

Listing 7: Spark code for rule φF

1
2
3
4

G

(2) Replication. A single data partitioning, however,
might not be useful for mutliple data cleansing tasks. In
practice, we may need to run several data cleansing jobs as
data cleansing tasks do not share the same blocking key. To
handle such a case, we replicate a dataset in a heterogenous
manner. In other words, BigDansing logically partitions
(i.e., based on values) each replica on a different attribute.
As a result, we can again push down the Block operator for
multiple data cleansing tasks.

BUSHY PLAN EXAMPLE

We show a bushy plan example in Figure 16 based on the
following two tables and DC rules from [6]:
Table Global (G): GID, FN, LN, Role, City, AC, ST, SAL
Table Local (L): LID, FN, LN, RNK, DO, Y, City, MID, SAL
(c1) : ∀t1 , t2 ∈ G, ¬(t1 .City = t2 .City ∧ t1 .ST 6= t2 .ST)
(c2) : ∀t1 , t2 ∈ G, ¬(t1 .Role = t2 .Role ∧ t1 .City = ”N Y C” ∧
t2 .City 6= ”N Y C” ∧ t2 .SAL > t1 .SAL)
(c3) : ∀t1 , t2 ∈ L, t3 ∈ G, ¬(t1 .LID 6= t2 .LID ∧ t1 .LID = t2 .MID ∧
t1 .FN ≈ t3 .FN∧t1 .LN ≈ t3 .LN∧t1 .City = t3 .City∧t3 .Role 6= ”M ”)

(3) Layout. BigDansing converts a dataset to binary format when storing it in the underlying data storage framework. This helps avoid expensive string parsing operations.

1229

Also, in most cases, binary format ends up reducing the file
size and hence I/Os. Additionally, we store a dataset in
a column-oriented fashion. This enables pushing down the
Scope operator to the storage manager and hence reduces
I/O costs significantly.
As underlying data storage layer, we use Cartilage [21] to
store data to and access data from HDFS. Cartilage works
both with Hadoop and Spark and uses HDFS as the underlying distributed file system. Using Cartilage, the storage
manager essentially translates BigDansing data access operations (including the operator pushdowns) to three basic
HDFS data access operations: (i) Path Filter, to filter the
input file paths; (ii) Input Format, to assign input files to
workers; and (iii) Record Reader, to parse the files into tuples. In Spark, this means that we specify these three UDFs
when creating RDDs. As a result, we can manipulate the
data access right from HDFS so that this data access is invisible and completely non-invasive to Spark. To leverage
all the data storage optimizations done by BigDansing, we
indeed need to know how the data was uploaded in the first
place, e.g., in which layout and sort order the data is stored.
To allow BigDansing to know so, in addition to datasets,
we store the upload plan of each uploaded dataset, which is
essentially the upload metadata. At query time, BigDansing uses this metadata to decide how to access an input
dataset, e.g., if it performs a full scan or an index scan, using the right UDF (path filter, input format, record reader)
implementation.

G.
G.1

Spark-CrossProduct & -UCrossProduct.
The
Executor receives two input RDDs and outputs an RDDPair
of the resulting cross product. Notice that we extended
Spark’s Scala code with a new function self Cartesian() in
order to efficiently support the UCrossProduct operator. Basically, self Cartesian() computes all the possible combinations of pair-tuples in the input RDDs.
Spark-OCJoin. The Executor receives two RDDs and a set
of inequality join conditions as input. The Executor applies
the OCJoin operator on top of Spark as follows. First, it
extracts PartAtt (the attribute on which it has to partition
the two input RDDs) from both RDDs by using the keyBy()
Spark function. Then, the Executor uses the sortByKey()
Spark function to perform a range partitioning of both
RDDs. As a result, the Executor produces a single RDD
containing several data blocks using the mapP artitions()
Spark function. Each data block provides as many lists
as inequality join conditions; each containing all U s sorted
on a different attribute involved in the join conditions. Finally, the Executor uses the self Cartesian() Spark function
to generate unique sets of paired data blocks.
Spark-PDetect. This operator receives a PIterate operator, a PDetect operator, and a single RDD as input. The
Executor first applies the PIterate operator and then the
PDetect operator on the output. The Executor implements
this operator using the map() Spark function.
Spark-PGenFix The Executor applies a PGenFix on each
input RDD using spark’s map() function. When processing
multiple rules on the same input dataset, the Executor generates an independent RDD of fixes for each rule. After that
it combines all RDDs of possible repairs into a single RDD
and pass it to BigDansing’s repair algorithm. This operator is also implemented by the Executor inside the Detect
operator for performance optimization purposes.

EXECUTION LAYER
Translation to Spark Execution Plans

Spark represents datasets as a set of Resilient Distributed
Datasets (RDDs), where each RDD stores all U s of an input dataset in sequence. Thus, the Executor represents each
physical operator as a RDD data transformation.

G.2

Spark-PScope. The Executor receives a set of RDDs as
well as a set of PScope operators. It links each RDD with one
or more PScope operators, according to their labels. Then,
it simply translates each PScope to a map() Spark operation
over its RDD. Spark, in turn, takes care of automatically
parallelizing the map() operation over all input U s. As a
PScope might output a null or empty U , the Executor applies
a f ilter() Spark operation to remove null and empty U s
before passing them to the next operator.

Translation to MR Execution Plans

We now briefly describe how the Executor runs the four
wrapper physical operators on MapReduce.
MR-PScope. The Executor translates the PScope operator
into a Map task whose map function applies the received
PScope. Null and empty U are discarded within the same
Map task before passing them to the next operator.
MR-PBlock. The Executor translates the PBlock operator
into a Map task whose partitioner function applies the received PBlock to set the intermediate key. The MapReduce
framework automatically groups all U s that share the same
key. The Executor does the same for the CoBlock operator,
but it also labels each intermediate key-value pair with the
input dataset label for identification at Reduce tasks.

Spark-PBlock. The Executor applies one Spark groupBy()
operation for each PBlock operator over a single RDD. BigDansing automatically extracts the key from each U in parallel and passes it to Spark, which in turn uses the extracted
key for its groupBy operation. As a result, Spark generates a
RDDPair (a key-value pair data structure) containing each a
grouping key (the key in the RDDPair) together with the list
of all U s sharing the same key (the value in the RDDPair).

MR-PIterate. The Executor translates PIterate into a
Reduce task whose reduce function applies the received
PIterate.

Spark-CoBlock. The Executor receives a set of RDDs and
a set of PBlock operators with matching labels. Similar to
the Spark-PBlock, the Spark-CoBlock groups each input RDD
(with groupBy()) using its corresponding PBlock. In addition, it performs a join() Spark operation on the keys of the
output produced by groupBy(). Spark-CoBlock also outputs
an RDDPair, but in contrast to Spark-PBlock, the produced
value is a set of lists of U s from all input RDDs sharing the
same extracted key.

MR-PDetect. The Executor translates the PDetect operator into a Reduce task whose reduce function applies the received PDetect. The Executor might also apply the received
PDetect in the reduce function of a Combine task.
MR-PGenFix. The Executor translates the PGenFix operator into a Map task whose map function applies the received PRepair. The Executor might also apply the received
PGenFix at the reduce function of PDetect.

1230

RULE M INER: Data Quality Rules Discovery
Xu Chu1?

Ihab F. Ilyas1?

Paolo Papotti2

Yin Ye2

1

2

University of Waterloo, Canada
{x4chu,ilyas}@uwaterloo.ca
Qatar Computing Research Institute (QCRI), Qatar
{ppapotti,yye}@qf.org.qa

Abstract—Integrity constraints (ICs) are valuables tools for
enforcing correct application semantics. However, manually designing ICs require experts and time, hence the need for
automatic discovery. Previous automatic ICs discovery suffer
from (1) limited ICs language expressiveness; and (2) timeconsuming manual verification of discovered ICs. We introduce
RULE M INER, a system for discovering data quality rules that
addresses the limitations of existing solutions.

I.

I NTRODUCTION

As businesses generate and consume data more than ever,
enforcing and maintaining the quality of their data assets
become critical tasks. One in three managers does not trust
the information used to make decisions [5], since establishing
trust in data becomes a challenge as the variety and the number
of sources grow. Therefore, data cleaning is an urgent task
towards improving data quality. Integrity constraints (ICs),
originally designed to improve the quality of a database
schema, have been recently repurposed towards improving the
quality of data, either through checking the validity of the
data at points of entry, or by cleaning the dirty data at various
points during the processing pipeline [4], [6], [7]. ICs are often
referred to as data quality rules when used for data cleaning.
Manually designing data quality rules often requires domain experts and engineers to express those rules in some
formal language for automatic enforcement. As data owners
are often not data quality rules experts, automatic rules discovery is necessary. Unfortunately, existing literature on automatic
data quality rules discovery suffers from two main drawbacks.
First, existing discovery algorithms are usually designed for a
single rule language, thus not able to uncover many useful
quality rules. Second, there are large amount of generated
rules, and not all of them are useful due to overfitting and
errors in the data. Manual evaluation of the output is a tedius
process and requires expertise of the language at hand.
We introduce RULE M INER, a system for discovering data
quality rules that addresses the limitations of existing solutions.
The system discovers rules expressed with Denial constraints
(DCs), which subsume many rule languages, including functional dependencies (FDs), conditional functional dependencies (CFDs), and check constraints (Section II). RULE M INER
efficiently mines data (Section III) and employs a novel interestingness function to rank the discovered rules (Section IV).
In the demonstration we show how rules are discovered
by our system from multiple real-world scenarios such as
Freebase and enterprise data (Section V). Rules are exposed
by means of compact positive and negative data examples for
? Work done while at QCRI.

978-1-4799-2555-1/14/$31.00 © 2014 IEEE

1222

effective user validation and can be explored with adjustable
ranking based on different properties of the rules. We also show
how to use the discovered DCs by automatically generating
violation detection and repairing java procedural code, which
can be further refined by expert users.
II.

DATA Q UALITY RULES

In the next three sections we introduce the language used to
express quality rules in our approach, the algorithm to discover
them, and the functions to rank discovered rules, respectively.
For further details we refer the reader to [3]. We start by
introducing the language with an example.
Example 1: Consider the US tax records table in Table I.
Each record describes an individual address and tax information with 14 attributes: first and last name (FN, LN), gender
(GD), area code (AC), mobile phone number (PH), city (CT),
state (ST), zip code (ZIP), marital status (MS), has children
(CH), salary (SAL), tax rate (TR), tax exemption amount if
single (STX) or married (MTX).
Suppose that the following constraints hold: (1) area code
and phone identify a person; (2) two persons with the same zip
code live in the same state; (3) a person who lives in Denver
lives in Colorado; (4) within a state, a person earning a lower
salary cannot have a higher tax rate; and (5) a salary is always
higher than the corresponding single tax exemption.
Constraints (1), (2), and (3) can be expressed as a key
constraint, an FD, and a CFD, respectively.
(1) : Key{AC, P H}
(2) : ZIP → ST
(3) : [CT = ‘Denver’] → [ST = ‘CO’]
Since Constraints (4) and (5) involve order predicates
(>, <), and (5) compares different attributes in the same predicate, they cannot be expressed by FDs and CFDs. However,
they can be expressed in first-order logic.
c4 : ∀tα , tβ ∈ R, ¬(tα .ST = tβ .ST ∧ tα .SAL < tβ .SAL
∧tα .T R > tβ .T R)
c5 : ∀tα ∈ R, ¬(tα .SAL < tα .ST X)
Since first-order logic is more expressive, Constraints (1)(3) can also be expressed as follows:
c1 : ∀tα , tβ ∈ R, ¬(tα .AC = tβ .AC ∧ tα .P H = tβ .P H)
c2 : ∀tα , tβ ∈ R, ¬(tα .ZIP = tβ .ZIP ∧ tα .ST 6= tβ .ST )
c3 : ∀tα ∈ R, ¬(tα .CT = ‘Denver’ ∧ tα .ST 6= ‘CO’)
Denial Constraints (DCs) [7], a universally quantified
first order logic formalism, can express all constraints in

ICDE Conference 2014

TID
t1
t2
...
t8

FN
Mark
Chunho
...
Marcelino

LN
Ballin
Black
...
Nuth

GD
M
M
...
F

AC
304
719
...
304

PH
232-7667
154-4816
...
540-4707

CT
Anthony
Denver
...
Kyle

TABLE I.

ST
WV
CO
...
WV

ZIP
25813
80290
...
25813

MS
S
M
...
M

CH
Y
N
...
N

SAL
5000
N
...
10000

TR
3
60000
...
4

STX
2000
4.63
...
0

MTX
0
0
...
0

TAX DATA RECORDS .

Example 1 as they are more expressive than FDs, CFDs,
and check/domain constraints. Consider a database schema
of the form S = (U, R, B), where U is a set of database
domains, R is a set of database predicates or relations, and
B = {=, <, >, 6=, ≤, ≥}. We use a notation for DCs of the
form ϕ : ∀tα , tβ , tγ , . . . ∈ R, ¬(P1 ∧ . . . ∧ Pm ), where
Pi is of the form v1 φv2 or v1 φc with v1 , v2 ∈ tx .A, x ∈
{α, β, γ, . . .}, A ∈ R, and c is a constant. For simplicity, we
assume there is only one relation R in R. A DC states that
all the predicates cannot be true at the same time, otherwise,
there is a violation.
The more expressive power an IC language has, the harder
it is to exploit it, for example, in automated data cleaning
algorithms, or in its static analysis. There is an infinite space
of business rules up to ad-hoc programs for enforcing correct
application semantics. In RULE M INER we rely on DCs as
they achieve a balance between expressiveness and complexity:
(1) they can be expressed as SQL queries for consistency
checking; (2) they have been proven to be a useful language
for data cleaning in many aspects [4]; and (3) there are sound
inference rules and a linear implication testing algorithm for
them that enable efficient use, as we show next.
III.

D ISCOVERY A LGORITHM

Given a relational schema R and an instance I, our goal
is to find all valid minimal DCs that hold on I.
Consider traditional FDs discovery on R with |R| = m.
Taking an attribute as the right hand side of an FD, any subset
of remaining m − 1 attributes could serve as the left hand side.
Thus, the space to be explored for FDs discovery is m ∗ 2m−1 .
Consider now discovering DCs involving at most two tuples
without constants. A predicate space needs to be defined, upon
which the space of DCs is built. The structure of a predicate
consists of two different attributes and one operator. Given two
tuples, we have 2m distinct cells; and we allow six operators
(=, 6=, >, ≤, <, ≥). Thus the size of the predicate space P is:
|P| = 6 ∗ 2m ∗ (2m − 1). Any subset of the predicate space
could constitute a DC. Therefore, the search space for DCs
discovery is of size 2|P| . Checking if each candidate DC is a
valid minimal DC is not feasible because of the exponential
number of candidate DCs. Therefore, we turn to an instancedriven approach that strongly relies on DCs static analysis.
Figure 1 shows the flow of RULE M INER and the main steps
for the mining algorithm. In the Evidence Generation,
we build a data structure EviI to collect evidence from I.
EviI is built by comparing each pair of tuples in I to find the
set of satisfied predicates in P. The discovery problem is then
reduced to finding all minimal set covers on EviI with a depth
first search (DFS) visit in the Discovery Algorithm.
To speed up the search for minimal set covers, we rely
on properties of DCs. Specifically, the Inference System

1223

Fig. 1.

System Architecture for RULE M INER.

I is composed of three inference rules: triviality provides a
syntactic way of checking if a DC is trivial or not, while
augmentation and transitivity state how a DC is implied by
other DCs. The discovery algorithm provides candidate DCs
to I, which checks if they are implied or not. Implied DCs
can be pruned to early terminate branches of the search tree.
As per discussed in [3], the complexity of our algorithm is
O(|P| ∗ n2 + |P| ∗ (1 + wP ) ∗ KP ), which is quadratic in the
number of tuples n, thus successfully avoiding the exponential
checking of all DCs, which has a complexity of O(2|P| ∗ n2 ).
RULE M INER provides two parameters that users can set
depending on the scenario. The approximation ratio represents
the maximum number of violations allowed in the instance.
This allows the discovery of DCs when there are errors in the
input dataset. In addition, to discover rules involving constants
such as c3 , the constant frequency parameter sets the minimum
number of tuple pairs that a predicate with constants must
satisfy in order to be considered evidence.
IV.

R ANKING F UNCTIONS

Though our discovery algorithm is able to prune trivial,
non-minimal, and implied DCs, the number of DCs returned
can still be large. To tackle this problem, RULE M INER employs a scoring function to rank rules based on their size and
their support from the data. Given a DC ϕ, we denote by
Inter(ϕ) its interestingness score.
We recognize two dimensions that influence Inter(ϕ):
succinctness and coverage of ϕ, which are both defined on
a scale between 0 and 1. Each score represents a different yet
important intuitive dimension to rank discovered DCs.
Succinctness is motivated by the Occam’s razor principle,
which suggests that, among competing hypotheses, the one that
makes fewer assumptions is preferred. It is also recognized that
overfitting occurs when a model is excessively complex [2].
Minimum description length (MDL), which measures the code
length needed to compress the data, is a formalism to realize

Fig. 2.

RULE M INER front end interface with Negative-Positive (NE-PE) pair for c2 generated from tuple pair ht1 , t8 i.

the Occams razor principle. We first define the alphabet of
DCs as A = {tα , tβ , U, B, Cons}, where U is the set of all
attributes, B is the set of all operators, and Cons are constants.
We define the length of a DC to be the number of symbols in
A that appear in ϕ. We define succinctness of a DC ϕ Succ(ϕ)
to be the minimal possible length of any DC divided by the
length of ϕ. The minimal length is 4, as in c5 , which has
Succ(c5 ) = 44 = 1. Other examples are Succ(c1 ) = 45 = 0.8
and Succ(c4 ) = 48 = 0.5.
Coverage is also a general principle in data mining to
rank results [1]. These scoring functions measure the statistical
significance of the mining targets in the input data. In frequent
itemset mining, the support of an itemset is defined as the
proportion of transactions in the data that contain the itemset.
Only if the support of an itemset is above a threshold, it is
considered to be frequent. The coverage of ϕ Coverage(ϕ)
depends on the support from I. Each tuple pair gives a support
to ϕ depending on the number of satisfied predicates. For
example, in Table I, tuple pair ht1 , t8 i satisfies one predicate
tα .ZIP = tβ .ZIP in c2 and gives more support than ht1 , t2 i,
which satisfies no predicate in c2 . The Coverage(ϕ) is defined
by aggregating all different supports from all tuple pairs
divided by the total number of tuple pairs.

Given a DC ϕ, we define the interestingness score as a linear weighted combination of the two dimensions Inter(ϕ) =
a × Coverage(ϕ) + (1 − a) × Succ(ϕ).

1224

V.

D EMONSTRATION O UTLINE

We demonstrate various aspects of RULE M INER system:
(1) the front end of RULE M INER, including how the users
set the parameters and how discovered rules are presented
and validated; (2) a back end view of how RULE M INER
generates DCs internally; and (3) how expert users can apply
a discovered rule to see analytics on cleaning metadata and
further fine-tune the rule with procedural java code.
1. Front End Interface. The front end interface for
RULE M INER has the parameter specification, rules ranking,
and their presentation and validation.
The upper panel in Figure 2 shows the input to RULE M(1) Data source selection; (2) The approximation ratio
specifies the maximum amount of errors for a valid rule; (3)
The constant frequency defines how often a constant must
appear in order to be considered for a rule; (4) A filtering
option lets the user select a subset of the rules depending on
the formalism of interest; and (5) A filtering option lets the
user select a subset of the rules depending on the interested
columns of the input table.
INER :

Discovered rules are presented ranked according to their
Inter score as shown in the left panel in Figure 2.
In order to verify if a rule indeed applies, users need to
validate. English translation and data examples for each rule
assist the users in understanding the discovered rules.
Each DC is translated into a meaningful English sentence.
For example, c1 is translated into “there cannot exist two

tuples tα , tβ in Tax data records, such that they have same
area code, and they have same mobile phone number”. While
c4 is translated into “there cannot exist two tuples tα , tβ in
Tax data records, such that they have same state, the salary of
tuple tα is less than that of tuple tβ , and the tax rate of tuple
tα is greater than that of tβ ”.
Data examples further assist the users in understanding
the rules. Given a DC ϕ, with ϕ.P res that denotes its set
of predicates:
•

A Negative Example (NE) of a DC ϕ w.r.t. instance I is a pair of tuples htx , ty i, s.t. ∀P ∈
ϕ.P res, htx , ty i |= P .

•

A Positive Example (PE) of a DC ϕ w.r.t. instance I is
a pair of tuples htx , ty i, s.t. ∃P ∈ ϕ.P res, htx , ty i 2
P.

•

A NE-PE pair of a DC ϕ w.r.t. instance I is two pairs
of tuples, such that the first pair is a NE, the second
pair is a PE, and NE and PE differ in exactly one cell.

four predicates we have built for a input table with two
columns A and B. We also visualize the evidence as a table
with P1 , P2 , P3 , P4 , and count; each row of the evidence
table represents a satisfied predicate set for some tuple pairs;
count is the number of tuple pairs that have the same satisfied
predicate set. The DFS tree in Figure 3 is a snapshot of the
search procedure for minimal set covers of the evidence set.
The user can follow the search procedure by dragging the slider
at the bottom.

A NE of a DC ϕ is a pair of tuples that satisfies all
predicates in ϕ, i.e., a violation of I for ϕ. For a discovered
rule ϕ, we present to the users one NE and x corresponding
PEs, where x equals to the number of cells involved in ϕ.
If there is a violation in I, we use it as NE and change it to
generate the PEs [4]. If there is no violation in I, we pick a PE
that has as many predicates in ϕ as possible, and modifies it to
have a NE. For example, the right panel in Figure 2 presents
one NE and four corresponding PEs for c2 .
A NE-PE pair helps the user understand what is wrong in
the NE by looking at the difference between the NE and the
PE. If the user agrees NE is wrong data and PE is correct
data, then the NE-PE pair is correct and the rule is valid.
Otherwise, if the user disagrees because either NE is correct
or PE is incorrect, then the NE-PE pair is incorrect and the
rule is dropped.

Fig. 4.

Procedural java code for violation detection and repairing.

3. Rules for Violation Detection and Repairing. Finally,
we demonstrate how to automatically generate violation detection and repairing java procedural code for discovered DCs by
feeding discovered DCs to Nadeef data cleaning system [6].
The generated java procedural code can be used by any thirdparty data cleaning system. Furthermore, expert users can even
refine the generated code to go beyond the limitations of DCs
language, as shown in Figure 4.
R EFERENCES
[1]
[2]
[3]
[4]
[5]

[6]

[7]
Fig. 3. RULE M INER back end internals with predicate space, evidence set,
and a snapshot of the DFS tree in the discovery algorithm.

2. Back End Internals. The back end shows how RULE Mdiscovers DCs. Figure 3 shows the predicate space with

INER

1225

R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules
between sets of items in large databases. In SIGMOD, 1993.
C. M. Bishop. Pattern Recognition and Machine Learning (Information
Science and Statistics). Springer-Verlag, 2006.
X. Chu, I. F. Ilyas, and P. Papotti. Discovering denial constraints. PVLDB,
6(13), 2013.
X. Chu, I. F. Ilyas, and P. Papotti. Holistic data cleaning: Putting
violations into context. In ICDE, 2013.
D. Deroos, C. Eaton, G. Lapis, P. Zikopoulos, and T. Deutsch. Understanding Big Data: Analytics for Enterprise Class Hadoop and Streaming
Data. McGraw-Hill, 2011.
A. Ebaid, A. Elmagarmid, I. F. Ilyas, M. Ouzzani, J.-A. Quiane-Ruiz,
N. Tang, and S. Yin. Nadeef: A generalized data cleaning system.
PVLDB, 6(12), 2013.
W. Fan and F. Geerts. Foundations of Data Quality Management. Morgan
& Claypool Publishers, 2012.

Contextual Data Extraction and Instance-Based Integration
Lorenzo Blanco Valter Crescenzi Paolo Merialdo Paolo Papotti
Dipartimento di Informatica ed Automazione
Università degli Studi Roma Tre
blanco,crescenz,merialdo,papotti@dia.uniroma3.it

ABSTRACT
We propose a formal framework for an unsupervised approach tacking at the same time two problems: the data extraction problem, for
generating the extraction rules needed to gain data from web pages,
and the data integration problem, to integrate the data coming from
several sources. We motivate the approach by discussing its advantages with regard to the traditional “waterfall approach”, in which
data are wholly extracted before the integration starts without any
mutual dependency between the two tasks.
In this paper, we focus on data that are exposed by structured and
redundant web sources. We introduce novel polynomial algorithms
to solve the stated problems and present theoretical results on the
properties of the solution generated by our approach. Finally, a
preliminary experimental evaluation show the applicability of our
model with real-world websites.

1.

INTRODUCTION

The development of scalable techniques to extract and integrate
data from fairly structured large corpora available on the web is a
challenging issue, because the web scale imposes the use of unsupervised and domain independent techniques. To cope with the
complexity and the heterogeneity of web data, state-of-the-art approaches focus on information organized according to specific patterns that frequently occur on the web. Meaningful examples are
presented in [6], which focuses on data published in HTML tables,
and information extraction systems, such as TextRunner [1], which
exploits lexical-syntactic patterns. As noticed in [6], even if a small
fraction of the web is organized according to these patterns, due to
the web scale the amount of data involved is impressive: in their
case, more than 154 millions tables were extracted from 1.1% of
the considered pages.
In large data-intensive websites, we observe two important characteristics that suggest new opportunities for the automatic extraction and integration of web data:
• local regularities: in these sites, large amounts of data are
usually offered by thousands of pages, each encoding one
tuple in a local HTML template. For example, each page

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. This article was presented at the workshop Very
Large Data Search (VLDS) 2011.
Copyright 2011.

shown in Figure 1 comes from a different source and publishes information about a single company stock.
• global information redundancy: at the web scale many sources
provide similar information. The redundancy occurs both a
the schema level (the same attributes are published by more
than one source) and at the instance level (some objects are
published by more than one source). In our example, many
attributes are present in all the sources (e.g., the company
name, last trade price, volume); while others are published
by a subset of the sources (e.g., the “Beta” indicator). At the
extensional level, there is a set of stock quotes that are published by more sources. As web information is inherently
imprecise, redundancy also implies inconsistencies; that is,
sources can provide conflicting information for the same object (e.g., a different value for the volume of a given stock).
These observations lead us to focus on pages that are published
following the one-tuple-per-page pattern: in each structured page
you can find information about a single tuple. If we abstract this
representation, we may say that a collection of structurally similar
pages provided by the same site corresponds to a relation. According to this abstraction, the websites for pages in Figure 1 expose
their own version of the “StockQuote” relation.1
Starting from the crawled web pages (for instance by using the
specialized crawler introduced in [3]), our goal is: (i) transform the
web pages coming from each source into a relation, and (ii) integrate these relations creating a database containing the information
provided by all the sources. A state-of-the-art solution to this problem is a two-steps waterfall approach, where a schema matching
algorithm is applied over the relations returned by automatically
generated wrappers. However, when a large number of sources is
involved and a high level of automation is required, important issues may arise:
• Wrapper Inference Issues: since wrappers are automatically
generated by an unsupervised process, they can produce imprecise extraction rules (e.g., rules that extract irrelevant information mixed with data of the domain). To obtain correct
rules, the wrappers should be evaluated and refined manually.
• Integration Issues: the relations extracted by automatically
generated wrappers are “opaque”, i.e., their attributes are
not associated with any (reliable) semantic label. Therefore
the matching algorithm must rely on an instance-based approach, which considers attribute values to match schemas.
1
For the sake of simplicity, we consider only the one-tuple-perpage pattern. Other variations of this pattern can be easily developed for example by preprocessing the pages with tools that fragment the HTML tables into rows [7].

over the hidden relation T .
The attributes published by a source are called called physical
attributes, as opposed to the conceptual attributes of T , and we
write S(a) to denote that a source S publishes a physical attribute
a, and a ∈ A to state that a physical attribute a contains data from
a conceptual attribute A.
A source publishes information about a subset of the conceptual
instances, and different sources may publish different subsets of its
conceptual attributes.
To model the presence of conflicting data that usually occur among
redundant sources, we assume that sources are noisy: they may introduce errors, imprecise or null values, over the data picked from
the hidden relation. As depicted in Figure 2, for each source Si we
can abstract the page generation process as the application of the
following operators over the hidden relation:
Figure 1: Two web pages containing data about stock quotes
from Reuters and Google finance websites.
However, due to errors introduced by the publishing process,
instance-based matching is challenging because the sources
may provide conflicting values. Also, imprecise extraction
rules return wrong, and thus inconsistent, data.
In [2] we presented a best-effort solution to solve these issues
by taking advantage of the mutual coupling between the wrapper
inference and the data integration tasks. In the present paper, we
investigate the foundations of the problem and propose a principled solution based on the following contributions: (i) we propose
a formal setting to state the data extraction and the data integration problems for redundant and structured web sources; (ii) we
formulate a set of hypothesis that capture a few natural constraints
that characterize this kind of web sources; (iii) we propose novel
unsupervised polynomial algorithms to solve the stated problems
whenever these hypotheses hold; (iv) we present an experimental
evaluation of our model with real-world websites.
In the next section we describe a generative model of the web
pages to introduce our formal setting of structured and redundant
sources. Section 3 contains the algorithms to solve the integration and extraction problems in the case of perfectly overlapping
sources. In Section 4 we show how removing this hypothesis affects our solution, while in Section 5 we discuss preliminary experiments with a set of sources gathered from the Web. Section 6
discusses related works and concludes the paper.

2.

THE GENERATIVE MODEL

We are interested in extracting and integrating all the available information about a target entity, such as the S TOCK Q UOTE entity
of our running example. As on the Web several sources publish
information about the same entity, we can imagine that there exists
a hidden relation T , which contains all the true information about
the objects that belong to the entity, and that sources generate their
pages by taking data from T .
We call conceptual instances the set of tuples I of the relation T .
Each tuple I ∈ T represents a real-world object of the target entity
of interest. For example, in the case of the S TOCK Q UOTE entity,
the conceptual instances of T model the data about the Apple stock
quote, the Yahoo! stock quote, and so on. T has a set of attributes
A called conceptual attributes. In our example they represent the
attributes associated with a stock quote, such as the company name,
the current trade price, the volume, and so on.
Given a set of sources S = {S1 , . . . , Sm }, each source Si , i =
1 . . . m can be seen as the result of a generative process applied

• Selection σi : returns a relation containing a subset of the
conceptual instances, σ(I) ⊆ I.
• Projection πi : returns a relation containing a subset of the
conceptual attributes, π(A) ⊆ A.
• Error ei : is a function that returns a relation, such that each
correct value is kept or replaced with a null value, a synthetic
value, or a value similar to the correct one.
• Encode λi : is an encoding function that produces a web page
for each tuple by embedding its values into a HTML template.
The set of pages published by a source Si can be thought as a
view over the hidden relation, obtained by applying the above operators as follows: Si = λi (ei (πi (σi (T )))). From this perspective,
the extraction and the integration problems can be thought in terms
of these operators. The extraction becomes the inversion of the λ
operator. That is, obtaining for each source Si the associated relation Vi = ei (πi (σi (T ))). The integration becomes the problem of
reconstructing T from the views associated with the sources.
Notice that both problems are far from being trivial as the stateof-the-art automatic wrapper inference systems are not able to create perfect wrappers, and the integration task is further complicated
by the presence of errors and the absence of reliable semantic labels.
In the following we discuss under which assumptions on the intensional and extensional redundancy exhibited by the sources, our
approach is able to deal with a bounded amount of error.

2.1

Intensional Redundancy

We now discuss two properties of the generative model. The first
property expresses that the data published by each source are locally consistent. That is, within the physical attributes published by
the same source, there cannot be distinct attributes with the same
semantics. For example, if a website states that the current value
of the stock quote “YHOO” is 17.01 there cannot be another place
in the same site where you can find a different value with the same
semantics. Therefore, we can write:
P ROPERTY 1. Local consistency:
∀ai , aj ∈ A : S(ai ) = S(aj ) ⇒ ai = aj
(in a conceptual attribute A there cannot exist two physical attributes coming from the same source).
The second property formalizes the presence of redundancy at
the intensional level. Namely, we assume that every possible pair
of conceptual attributes is published at least by one source. Let

Figure 2: The publishing process: the web sources are views over the hidden relation generated by four operators.
S(Ai ) denotes a predicate that returns true if the source S publishes
the conceptual attribute Ai . Therefore, a set of sources S is called
intensionally redundant if the following property holds:
P ROPERTY 2. Intensional redundancy:
∀Ai , Aj : i 6= j ∃ S : S(Ai ) ∧ S(Aj )
(every possible pair of conceptual attributes is published at least
by one source)
Notice that given any set of websites, this property may hold
only for appropriately chosen subsets. However, the web scale represents an opportunity to gain enough redundancy: (i) in each domain, usually there is a subset of attributes that is published by
most of the sources;2 (ii) as the number of considered websites
increases, the probability of meeting a new conceptual attribute decreases, and the probability of intensionally redundancy increases.
In the following we call core-attributes the attributes for which
the intensional redundancy holds, and we call rare-attributes all the
others.

Given a set of sources S, each Si publishes a view of the hidden relation T such that Vi = ei (πi (σi (T ))). The integration
problem can be thought as the creation of sets of physical attributes
m1 , . . . , mn , called mappings, such that each attribute a belongs to
a mapping m and each mapping contains all and only the attributes
with the same semantics. The problem can be defined as follows:
P ROBLEM 1. Integration Problem : given a set of source views
V = V1 , . . . , Vn , where Vi = ei (πi (σi (T ))), find a set of mappings M such that M = {m : a1 , a2 ∈ m ⇔ a1 , a2 ∈ A}.

We first discuss the integration problem by ignoring the extraction
issues, then we discuss the main properties of the extraction rules,
and, finally, how the integration and the extraction problems can
be tackled together. Therefore, for the time being we assume we
have a wrapper generator that is capable of perfectly inverting the
encode operator λ. In other words, we do not work on web pages,
but directly on the views of T published by the sources.

Intuitively, we solve the problem by evaluating the physical attributes of each source and by building aggregations of attributes
with the same semantics from the sources. If at the end of the process each mapping contains all and only the physical attributes with
the same semantics, we have a solution for the problem. For example, given a1 , a3 ∈ V1 and a2 ∈ V2 with a1 , a2 having the same
semantics, a solution is m1 = {a1 , a2 } and m2 = {a3 }.
If the sources publish correct data only, then a naive greedy algorithm easily solves the problem above. However, real sources
introduce noise in the values (modeled by the error function e) that
can make the integration difficult or even not possible.
To identify physical attributes with the same semantics, we rely
on a distance function d(ai , aj ) among the values of a set of instances corresponding to the same real-world object.3 This function compares aligned values and returns a score between 0 and 1.
The more similar are the values, the lower is the distance. As the
distance function works by comparing values of aligned instances,
it can be easily extended to work also on conceptual attributes. We
denote d(a, A) the distance between the values of the physical attribute a and the values of the conceptual attribute A.
In the following, we study a class of error functions for which
our algorithm can compute a solution by bounding the amount of
errors that sources are allowed to introduce.
For every conceptual attribute A, let tA denote a minimal threshold such that any physical attribute ai belongs to A if for each
aj ∈ A, d(ai , aj ) < tA . For example, in the finance domain,
a low threshold should be associated with the conceptual attribute
Max value of a stock quote. This is required as there are other
conceptual attributes, like the current Price and the Min value, that
have similar values. On the other hand, the mapping for the trading
Volume conceptual attribute can have an higher threshold since it

2
In [11] the authors write “there is a core set of attributes that appear in a large number of items”.

3
We assume that instances can be aligned by applying a standard
record-linkage technique [9].

2.2

Extensional Redundancy

At the extensional level, we assume that sources publish a common set of instances. For the sake of presentation, in the next section we rely on the hypothesis that all the source publish the same
set of instances I. Later, in Section 4 we discuss how to remove
this hypothesis.

3.

EXTRACTION AND INTEGRATION ALGORITHMS

Our solutions to the extraction and integration problems are discussed in this section: we propose the problem statements, the definition of solutions and the algorithms that solve them in polynomial
time.

3.1

Integration Problem

does not usually assume values close to those of other attributes.
Note that tA is an ideal unknown threshold: it is not given as input
of the integration problem and it is not necessary to know it a priori
to compute the solution.
In order to solve the integration problem, it is required that the
publishing errors cannot introduce enough noise to confuse the semantics of a physical attribute. We call this property as separable
semantics:
P ROPERTY 3. Separable semantics:
∀A1 , A2 , ai ∈ A1 , aj ∈ A2 : ai 6= aj ∧ A1 6= A2 ⇒
d(ai , aj ) > max(tA1 , tA2 )
(it is possible to distinguish the semantics of physical attributes).
In order to solve the integration problem with noisy sources, we
define the greedy clustering Algorithm 1.
Algorithm 1 A BSTRACT I NTEGRATION
Input: A set of locally consistent, intensionally redundant sources
with separable physical attributes.
Output: The correct set of mapping M.
Let G = (N, E) be a graph where every attribute ai for every
source Si ∈ S is a node n ∈ N . For every pair of distinct nodes
ai , aj ∈ N such that S(ai ) 6= S(aj ) add an edge e between them
to E and let d(ai , aj ) be the weight of e.
Let m(ai ) be the mapping containing the attribute ai .
1. Add to M a mapping m = {ai } for each node ni ∈ N ,
2. insert in a list L the edges E,
3. sort L by the weight of the edges in ascending order,
4. for each edge (a1 , a2 ) ∈ L:
(a) let m be the union of the attributes in m(a1 ) and m(a2 )
(b) if in m there is no pair of ai , aj such that S(ai ) =
S(aj )
(c)

then add m and remove m(a1 ), m(a2 ) from M

(d)

else break.

We are now ready to prove the correctness of the integration algorithm.
L EMMA 3.1. A BSTRACT I NTEGRATION is correct.
P ROOF. Moved to Appendix A.
A BSTRACT I NTEGRATION is O(n2 ) over the total number of
physical attributes, in fact most of the time is required to create
the edges of the graph G.
In the following we introduce the extraction problem, that is,
how to get the physical attributes we considered as input of the
integration problem.

3.2

Extraction Rules

In our framework, a data source S is a collection of pages S =
p1 , . . . , pn from the same website, such that each page publishes
information about one object of a real-world entity of interest.
We distinguish between two different types of values that can
appear in a page: target values, that is, values that are derived from
the hidden relation T , and noise values, that is, values that are not
of interest for our purpose (e.g., advertising, template, layout, etc).

We consider as given an unsupervised wrapper generator system W. A wrapper w is an ordered set of extraction rules, w =
{er1 , . . . , erk }, that apply over a web page: each rule extracts a
string from the HTML of the page. We denote er(p) the string returned by the application of the rule er over the page p. The application of a wrapper w over a page p, denoted w(p), returns a tuple
t = her1 (p), . . . , erk (p)i; therefore, the application of a wrapper
over the set of pages of a source S returns a relation w(S), which
has as many attributes as the number of extraction rules of the wrapper. A column of the relation is a vector of values denoted V (eri ):
it is the result of the application of an extraction rule eri over the
pages of a source.
We say that an extraction rule er∗ is correct if for every given
page it extracts a value of the same conceptual attribute (i.e., target
values with the same semantics) or a null value if the value for the
attribute is missing in that page. If a correct extraction rule only
extracts noise values, it is considered noisy. We also say that an
extraction rule erw is weak if it mixes either target values with
different semantics or target values with noise values.
Unsupervised wrapper generators are powerful enough to infer
the correct extraction rules needed to cover the data exposed by
what we call regularly structured websites.
P ROPERTY 4. Regularly structured sources: The sources S =
{Si } are regularly structured w.r.t. a given unsupervised wrapper
generator system W, if W generates for each source Si ∈ S a set
of rules wi containing all the correct rules.
However, wrapper generators cannot automatically identify, among
the generated rules, which are the correct ones. They also produce
weak rules, since, at wrapper generation time there is not enough
information to automatically establish if a rule is either correct or
weak. The integration algorithm has been presented considering
only the correct rules (i.e., physical attributes). However, noisy
rules, if considered along with the correct ones, are harmless as
they can be identified and deleted during the integration step. They
will eventually generate singleton mappings of size one since the
distance between a noisy rule and a correct rule prevent them from
grouping. Similar arguments apply for distances amongst noisy
rules.
Weak rules require a more detailed discussion, and unfortunately
they cannot be identified and disregarded at wrapper generation
step. In the following we show that, if we keep the same assumptions introduced for the integration problem, we can always identify
weak rules during the integration step.

3.3

Extraction Problem

The extraction problem is defined as follows:
P ROBLEM 2. Extraction Problem: given a set of sources S =
{Si }, produce a set of wrappers W ∗ = {wi }, such that wi contains all and only the correct rules for Si .
We now describe how we leverage the redundant information
among different sources to identify and filter out the weak rules.
Let eri and erj be two extraction rules. We say that two extraction
rules “overlap” if they extract from a page the same occurrence of
the same string. In this case, one of them must be a weak rule. In
other terms, if many rules are extracting the same value occurrence
from at least a page, only one of them is a correct rule and all the
others are weak ones. With an abuse of notation, we will say that
er ∈ A to state when an extraction rule extracts at least a correct
value of the conceptual attribute A. Notice that, as a weak rule
erw can extract values from n conceptual attributes, we can say
erw ∈ A1 , . . . , An .

4.

Figure 3: The Extraction algorithm in action.

These intuitions are applied in the following greedy algorithm
which solves the extraction problem:
Algorithm 2 A BSTRACT E XTRACTION
Input: A set of locally consistent, intensionally redundant sources
with separable physical attributes; the set of wrappers W produced
by a wrapper generator system W w.r.t. which the sources are regularly structured.
Output: A set of wrappers W ∗ that do not contain weak rules.
1. while there is a er ∈ W which is not marked as correct:
(a) let d(V (eri ), V (erj )) be the minimal distance between
the values of two extraction rules in W and at least one
of them is not marked as correct
(b) mark eri and erj as correct, (they are correct rules)
(c) remove from W all the rules that overlaps with eri
(they are weak rules)
(d) remove from W all the rules that overlaps with erj
(they are weak rules)
2. now W is W ∗ .

The algorithm A BSTRACT E XTRACTION takes as input a set of
wrappers W and computes W ∗ which does not contain weak rules.
To explain the algorithm, we rely on the example in Figure 3. Consider two websites and two objects (say, two stock quotes) that are
published by both sites. In the figure, a circle represents the values
extracted by an extraction rule and its number represents the website is has been executed on. For example, in the diagram on the
left, the dark circle marked with 1 extracts from the first website the
values 10 for the first object and 11 for the second object. Notice
that the input of the algorithm are the circles annotated with the extracted values and the website of provenience. Let say now that in
step (a) the algorithm identifies the dark circles as the closest ones,
and mark them as correct in step (b). At this point both the circles
with horizontal stripes overlap with correct rules and can therefore
be removed in steps (c) and (d). In the diagram on the right we
show the resulting scenario: the dark circles are now the closest
rules and are marked as correct. The remaining dashed circles do
not match at all (i.e., they are noisy rules) and raise the creation
of two singleton mappings. To prove that the above algorithm is
correct we introduce the following lemma:
L EMMA 3.2. A BSTRACT E XTRACTION is correct.
P ROOF. Moved to Appendix A.
A BSTRACT E XTRACTION is O(n2 ) over the total number of extraction rules generated by the automatic wrapper generation system. Like in the case of A BSTRACT I NTEGRATION, most of the
time is spent computing distances between the extracted values.

NON-OVERLAPPING SOURCES

So far we have simplified the discussion by hypothesizing that
every source publishes data about every object in I. In this section
we remove this simplification and use IiA to denote the subset of I
for which Si provides values of the conceptual attribute A.
The distances amongst physically attributes from several sources
have been computed using an instance-based metric that relies on
the availability of a set of shared instances between the involved
sources. Therefore, if we want to compute the direct distance between two attributes d(ai , aj ), with S(ai ) = Si and S(aj ) = Sj ,
we need a non-empty overlap of objects between Si and Sj (Si 6=
Sj ), otherwise we consider d(ai , aj ) = ∞.
To formalize this aspect, and given a positive integer parameter
q, let OVq,A (Si , Sj ) be a predicate true iff |IiA ∩ IjA | ≥ q, i.e.
both Si and Sj publish a value of the attribute A for a shared set of
at least q instances.
Intuitively, OVq,A (Si , Sj ) is true if we consider Si and Sj to
share enough instances (at least q) to be directly comparable on A’s
values. The value of q has to be chosen according to a trade-off:
the higher the value, the more reliably the instance-based distance
would perform, as it can be computed over a larger set of shared
instances; however, it is possible that a lower number of sources
will be directly comparable.
We are now ready to tackle the main issue: how to the compute
the distance when the direct distance is not defined? i.e., the overlap
is not sufficient or not available at all and OVq,A (Si , Sj ) does not
hold.
We introduce the indirect distance d by leveraging the intermediate sources sharing instances with two sources not directly
having enough overlap. Given a third source Sw , such that both
OVq,A (Si , Sw ) and OVq,A (Sw , Sj ) hold, as for the shortest path
among two points is a straight line, we can easily write: d(ai , aj ) ≤
d(ai , aw ) + d(aw , aj ). In this case, we have an upper bound for
d(ai , aj ) that we call indirect distance, based on the availability of
two direct distances between (Si , Sw ) and between (Sw , Sj ).
In the previous example we used just one intermediate source
(Sw ); the same principle can be trivially extended to a generic
number of intermediate sources. However, the more intermediate sources are involved, the less precise is the bound imposed by
d(ai , aj ). In the case that we have multiple possible indirect distances, the bound chosen is the smallest one.
∗
the transitive closure of OVq,A : OVq,A (Si , Sj )
We call OVq,A
is true iff it is possible to compute a distance (direct or indirect)
between Si , Sj for the attribute A. If two attributes ai and aj
are not comparable over the same conceptual attribute A, that is,
∗
OVq,A
(Si , Sj ) is false, we set d(ai , aj ) = ∞.
Let S(A) denote the set of websites in S publishing values of
the conceptual attribute A ∈ A : a set of websites S are called
extensionally redundant if the following property holds:
P ROPERTY 5. Extensional redundancy:
∗
∀A ∈ A OVq,A
= S(A) × S(A)
( the overlap of websites’ objects allows the computation of indirect
distances )
Essentially, it is required that the indirect-distance d can be computed between any pair of physical attributes.
Observe that analogously to rare-attributes, a concept of rareinstances could be introduced. Anyway, the crawler [3] used during
the experiments gathers up extensionally redundant websites.
All the results previously obtained continue to hold with the
new definition of distance, and can be applied even in presence of
sources that do not contain all the objects I of the hidden relation
T provided that the input sources are extensional redundant. How-

ever, in order to obtain a solution with indirect distances, we increase the complexity of the algorithms from quadratic to cubic, as
we reduce the computation of the distance function to the problem
of finding shortest paths in a graph by modeling physical attributes
as nodes and the distances among them as weighted edges. This
can be solved with the Floyd–Warshall algorithm [14].

5.

EXPERIMENTAL EVALUATION

In this section we present a preliminary experimental evaluation
of our model, conducted by using a special-purpose crawler [3] to
collect 100 websites over three application domains: Soccer, Videogames, and Finance. Each source consists of tens to thousands
of pages, and each page contains detailed data about one object
of the corresponding entity. For each domain, we then selected
the 20 largest sources and manually verified the hypothesis of our
generative process.
Intensional Redundancy We start evaluating the redundancy at
the schema level. We observe that in the soccer and video-game domains the majority of the conceptual attributes are not rare. In fact,
in the soccer domain, only 25% of the attributes are rare and they
mostly come from websites that are not only about soccer players.
For example, a website containing info about olympic athletes exposes the attribute medals, while a club website exposes the debut
date only for the players coming from its own youth academy. For
the video-games the percentage of rare attributes is slightly over
30% of the total, in this case rare attributes come from distinct information with very similar semantics, such as difficulty and learning curve. Finally, in the stock quote scenario the percentage of
rare attributes is over 40% of the total. This is not surprising, as
in financial domain there is a large number of attributes (89 for 20
sources) due to the presence of many indicators used for technical
analysis. For this domain, 20 sites are sufficient only to get a very
rough estimation of the set of attributes published by the sites of
the domain. We expect that the percentage of rare attributes would
significantly drop as the number of web sources increases.
Extensional Redundancy Within the same domain, several objects are shared by several sources. The overlap is almost total for
the stock quotes, while it is more articulated for soccer players and
video-games as these domains include both large popular sites and
small ones. Over the 100 sources, we computed that each soccer
player object appears on average in 1.6 sources, each video-game
in 24.5 sources, and each stock quote in 92.8 sources. In particular,
OVq,A (Si , Sj ) is true with q = 5 for all the non rare attributes for
all the websites.
Errors and Thresholds We manually verified the thresholds for
the conceptual attributes of the three domains. As expected, those
have very different values, depending on the domain and the attribute considered. As an example, in the finance domain a threshold of 0.023 is needed for the Max value of a stock quote (0.029 for
the Min), while for the Volume a threshold of 0.5 is sufficient. In
the soccer and video-games cases, the thresholds are higher, such
as 0.44 for PlayerName (or video-game Title) and 0.36 for his
BirthCountry. More importantly, we were able to verify that the
separable semantics property is always verified.

6.

RELATED WORK

Our techniques are related to projects on the integration of web
data, such as PAYA S YOU G O [12]. However, the proposed integration techniques are based on the availability of attribute labels,
while our approach aims at integrating unlabeled data from web
sites. TurboWrapper [8] has similar limits: it relies on syntactic
structure of attributes (e.g. the ISBN number for books) with-

out considering the redundancy of information that occurs at the
instance-level.
The exploitation of structured web data is the primary goal of
WebTables [6] and ListExtract [10], which concentrate on data published in HTML tables and lists, respectively. Compared to information extraction approaches, WebTables and ListExtract extract
relations with involved relational schemas but it does not address
the issue of integrating the extracted data.
Cafarella et al. have described a system to populate a probabilistic database with data extracted from the web [4]. However, the
data are retrieved by TextRunner [1], an information extraction system that is not targeted to data rich web pages as ours. Octopus [5]
and Cimple [13] support users in the creation of data sets from web
data by means of a set of operators to perform search, extraction,
data cleaning and integration. Although such systems have a more
general application scope than ours, they involve users in the process, while our approach is completely automatic.

7.

REFERENCES

[1] M. Banko, M. Cafarella, S. Soderland, M. Broadhead, and
O. Etzioni. Open information extraction from the web. In
IJCAI, 2007.
[2] L. Blanco, M. Bronzi, V. Crescenzi, P. Merialdo, and
P. Papotti. Redundancy-driven web data extraction and
integration. In WebDB, 2010.
[3] L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti.
Supporting the automatic construction of entity aware search
engines. In WIDM, pages 149–156, 2008.
[4] M. J. Cafarella, O. Etzioni, and D. Suciu. Structured queries
over web text. IEEE Data Eng. Bull., 29(4):45–51, 2006.
[5] M. J. Cafarella, A. Y. Halevy, and N. Khoussainova. Data
integration for the relational web. PVLDB, 2(1):1090–1101,
2009.
[6] M. J. Cafarella, A. Y. Halevy, D. Z. Wang, E. Wu, and
Y. Zhang. Webtables: exploring the power of tables on the
web. PVLDB, 1(1):538–549, 2008.
[7] L. Chen, S. Ye, and X. Li. Template detection for large scale
search engines. In Proceedings of the 2006 ACM symposium
on Applied computing, SAC ’06, pages 1094–1098, New
York, NY, USA, 2006. ACM.
[8] S.-L. Chuang, K. C.-C. Chang, and C. X. Zhai.
Context-aware wrapping: Synchronized data extraction. In
VLDB, pages 699–710, 2007.
[9] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios.
Duplicate record detection: A survey. IEEE Trans. Knowl.
Data Eng., 19(1):1–16, 2007.
[10] H. Elmeleegy, J. Madhavan, and A. Y. Halevy. Harvesting
relational tables from lists on the web. PVLDB,
2(1):1078–1089, 2009.
[11] J. Madhavan, S. Cohen, X. L. Dong, A. Y. Halevy, S. R.
Jeffery, D. Ko, and C. Yu. Web-scale data integration: You
can afford to pay as you go. In CIDR 2007, pages 342–350,
2007.
[12] A. D. Sarma, X. Dong, and A. Y. Halevy. Bootstrapping
pay-as-you-go data integration systems. In SIGMOD
Conference, pages 861–874, 2008.
[13] W. Shen, P. DeRose, R. McCann, A. Doan, and
R. Ramakrishnan. Toward best-effort information extraction.
In SIGMOD Conference, pages 1031–1042, 2008.
[14] S. Skiena. The Algorithm Design Manual (2. ed.). Springer,
2008.

APPENDIX
A.

< d(Vi [0, . . . , n + 1], Vk [0, . . . , n + 1]).

PROOFS

We start with a preliminary lemma needed to prove the correctness of the presented algorithms.
L EMMA A.1. I NTRAC LOSERT HAN I NTER.
∀eri∗ , erj∗ ∈ A1 , erk ∈ A2 d(V (eri∗ ), V (erj∗ )) < d(V (eri∗ ), V (erk ))
P ROOF. The extraction rule erk can be correct or weak. We
prove the lemma for the two cases:
1. erk is correct (erk∗ ): consider eri∗ , erj∗ ∈ A1 and erk∗ ∈ A2
when the property separable semantics holds.
By definition: ∀eri∗ , erj∗ ∈ A1 ∃ tA1 : d(V (eri∗ ), V (erj∗ )) <
tA1
Separable semantics: ∀A1 , A2 , eri∗ ∈ A1 , erk∗ ∈ A2 : i 6=
k ∧ A1 6= A2 ⇒ d(V (eri∗ ), V (erk∗ )) > max(tA1 , tA2 )
We can derive:
d(V (eri∗ ), V (erj∗ )) < tA1 ≤ max(tA1 , tA2 ) <
< d(V (eri∗ ), V (erk∗ )).
Therefore:
d(V (eri∗ ), V (erj∗ )) < d(V (eri∗ ), V (erk∗ )).
2. erk is weak (erkw ): in the following we treat single values
as singleton vectors that we denote with V [i, . . . , j] the subvector of values for V from index i (included) to index j (excluded). We first introduce a monotonicity property of the
distance function. Given two vectors V1 and V2 with n values
and a distance d(V1 , V2 ) between them, let V20 be a copy of
V2 . If we replace the i-th element V2 [i] with a new element
V2 [i]0 such that d(V1 [i], V2 [i]) < d(V1 [i], V2 [i]0 ) it follows
that d(V1 , V2 ) < d(V1 , V20 ).4
In this second case erkw is a weak rule, that is, it can potentially contains values taken from A1 , A2 , or any other
A. We consider the instance-aligned vectors of values Vk0 =
V (erkw ), Vi0 = V (eri∗ ) and Vj0 = V (erj∗ ) and we remove
from the analysis the instances where erkw , eri∗ , and erj∗ extract the same value: let Vk , Vi and Vj be the vectors with the
remaining values. As errw cannot contain only values coming
from A1 (otherwise it would not be a weak rule, but a correct extraction rule of A1 ) the length of these vectors must be
greater than zero, and notice also that Vk0 now does not contain any value coming from A1 (they have been all removed).
We show now by induction on the length of the vectors that
d(V (eri∗ ), V (erj∗ )) < d(V (eri∗ ), V (erkw )).
Base case: let Vk [0] be the first value for Vk . We know that it
is a correct value for a conceptual attribute different from A1 .
Therefore, for the property we just showed in the previous
case:
d(Vi [0], Vj [0]) < d(Vi [0], Vk [0]).
Inductive step: the inductive hypothesis is
d(Vi [0, . . . , n], Vj [0, . . . , n]) < d(Vi [0, . . . , n], Vk [0, . . . , n]).
We show that it is true for n + 1 elements of the vectors.
Again, for the property we just showed d(Vi [n + 1], Vj [n +
1]) < d(Vi [n + 1], Vk [n + 1]) holds. For the monotonicity
property of the distance function, it is true that
d(Vi [0, . . . , n + 1], Vj [0, . . . , n + 1]) <
4

This is a natural property of the Euclidean distance.

L EMMA A.2. A BSTRACT I NTEGRATION is correct.
P ROOF. When the property separable semantics holds, the weights
of the edges among attributes with different semantics are always
higher than the weights of the edges among attributes with the same
semantics. This implies that the edges in L are divided in two sublists. In the first sublist (lower weights) we have pairs of attributes
that have the same semantics. We can therefore add to the solution all the pairs in the first sublist. In the second sublist (higher
weights) we have pairs of attributes with different semantics and
we need to avoid to add an edge from this sublist to the solution.
The problem here is that it is not know a-priori where the second
sublist starts. But we know that when the algorithm gets to the first
edge of the second sublist, all and only the attributes with same
semantics have been grouped in mappings.
Therefore the partial solution is correct.
We now need to show that the algorithm stops at the first edge of
the second sublist. The first edge in the second sublist is an edge
between two mappings m1 , m2 with different semantics. When
the property intensional redundancy holds, there must be a source
which publishes two attributes ai , aj such that they are contained in
m1 and m2 , respectively. By the local consistency of sources there
cannot be a mapping that contains ai , aj , and therefore the first
edge of the second sublist is detected and the algorithm ends.
L EMMA A.3. A BSTRACT E XTRACTION is correct.
P ROOF. In any iteration of step (a) we select two correct extraction rules er1∗ , er2∗ ∈ A1 . This is equivalent to show that if
we list the pairs of extraction rules in ascending order, the first
pair is certainly one with correct extraction rules. Suppose, by absurd, that the first pair contains a weak rule. This contradicts the
I NTRAC LOSERT HAN I NTER Lemma.
Every time two correct extraction rules er1∗ or er2∗ are chosen, all
the weak rules containing at least a value in common with er1∗ or
er2∗ are removed (steps (c) and (d)). Therefore, after the algorithm
has chosen all the correct rules, there cannot be a weak rule in W as
weak rules mix values shared with correct rules and they have been
discarded as soon as the correct rules have been identified.

WWW 2011 – Demo

March 28–April 1, 2011, Hyderabad, India

Automatically Building Probabilistic Databases
from the Web
Lorenzo Blanco, Mirko Bronzi, Valter Crescenzi, Paolo Merialdo, Paolo Papotti
Università degli Studi Roma Tre
Dipartimento di Informatica e Automazione - Rome, Italy

{blanco,bronzi,crescenz,merialdo,papotti}@dia.uniroma3.it
ABSTRACT

few sample pages describing instances of an entity of interest (e.g., video games, stock quotes etc.), the system is
able to: (i) locate a set of web sources publishing pages
with data about the entity instances; (ii) extract data from
these pages; (iii) integrate the extracted data in a mediated schema; (iv) analyze the integrated data according to
a probabilistic model and characterize the accuracy of the
involved sources.
Our system leverages the regularities that occur in large
data intensive web sites and the redundancy of data on the
Web. Consider for example the pages shown in Figure 1.
They come from different sources and contain structured
information about video games. Sources exhibit intra-site
regularities: they publish many pages, each containing information about one video game. Pages from the same source
are generated according to a site-specific HTML template. If
we abstract this representation, we may say that each web
page displays a tuple, and that a collection of pages provided by the same site corresponds to a relation. According
to this abstraction, each site in Figure 1 exposes its own
“VideoGame” relation.
Also, observe the inter-site information redundancy: as
the Web scales, many sources provide similar information.
The redundancy occurs both a the schema level (same attributes published by several sources) and at the extensional
level (several objects are published by multiple sources). In
our example, at the schema level many attributes are present
in almost all the sources (e.g., developer name, release date,
title); while others are published by a subset of the sources
(e.g., the suggested “resolution” ). At the extensional level,
there is a set of video games whose attribute values are published by multiple sources. As web information is inherently
imprecise, redundancy also implies inconsistencies: sources
can provide conflicting information for the same object (e.g.,
different release dates of a given video game).
Based on these observations, given an entity of interest, we
may abstract the presence of a hidden conceptual relation,
with schema R(A1 , . . . , An ), from which pages of different
sources are generated: each source Si can be seen as a view
on the hidden relation.
For each source Si , we can abstract the page generation
process as the application of the following steps over the
hidden relation: (i) selection of a subset of the tuples from
the hidden relation (σi ); (ii) projection on a subset of the
attributes (πi ); (iii) introduction of errors, approximations
and null values (ei ); (iv) encoding into web pages of the
tuples of the resulting relation according to a HTML template (λi ).

A relevant number of web sites publish structured data about
recognizable concepts (such as stock quotes, movies, restaurants, etc.). There is a great chance to create applications
that rely on a huge amount of data taken from the Web. We
present an automatic and domain independent system that
performs all the steps required to benefit from these data:
it discovers data intensive web sites containing information
about an entity of interest, extracts and integrate the published data, and finally performs a probabilistic analysis to
characterize the impreciseness of the data and the accuracy
of the sources. The results of the processing can be used to
populate a probabilistic database.

Categories and Subject Descriptors
H.4.3 [Information Systems Applications]: Information
browsers

General Terms
Documentation, Experimentation

Keywords
Web Data Extraction, Data Integration, Probabilistic Data

1.

INTRODUCTION

Existing search engines have recently started to offer services
and facilities that aim at exploiting the impressive amounts
of data available on the Web. For example, the results of a
Google query involving a popular stock quote symbol (e.g.
IBM or AAPL) are enhanced by a tuple containing the values of the major attributes (open value, day’s range, volume,
etc.) for that stock quote. However, enhanced answers are
available only for very popular domains (finance, forecast)
and for a limited number of entities (for example, only the
most popular stock quote symbols are recognized). Also,
since there are many sources that report attribute values for
the same objects, data conflicts frequently arise. Therefore,
there is the need to characterize the accuracy of the sources
and the quality of the proposed data.
We have recently developed a system to exploit the huge
amount of structured data available on the Web. Given a
Copyright is held by the International World Wide Web Conference Committee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2011, March 28–April 1, 2011, Hyderabad, India.
ACM 978-1-4503-0637-9/11/03.

185

WWW 2011 – Demo

March 28–April 1, 2011, Hyderabad, India

Figure 1: Given a hypothetic hidden relation for the entity VideoGame containing all the true values for all
the attributes, we abstract the generation of three web sites (ign.com, boardgamegeek.com, teamxbox.com,
respectively) as a sequence of projection, selection, approximation, and encoding over it.

Related Work. To cope with the complexity and the het-

From this perspective, our system addresses the following
problem: given a conceptual entity, described by means of a
few input sample pages, build the underlying hidden relation.
Our system decomposes such a challenging goal in three
main steps: (i) source localization: finding all sources publishing data about the target conceptual entity and getting
the pages describing its instances; (ii) data extraction and
integration: inferring wrappers to extract data from each
source and then integrating the redundant data; (iii) probabilistic analysis: to address the intrinsic imprecision of web
data, the hidden relation is exposed in the form of a probabilistic relation: each attribute value is associated with a
probability distribution function resulting from a Bayesian
analysis of the conflicting values.
Our system begins with a bootstrap step to build an initial
hidden relation R0 from the input sample pages. Relation
R0 feeds an iterative process involving two steps: during
the i-th iteration, the sources localization step uses Ri to
locate other meaningful sources; then the data extraction
and integration step computes Ri+1 by including the new
sources. As the iterations progress, it becomes harder and
harder to discover new sources, and the process terminates
when no more sources cannot be effectively discovered.1
After the last iteration, the probabilistic analysis step is
activated by taking as input the last Ri produced. It returns
a probabilistic relation in which a probability distribution
function is assigned to each attribute of each tuple.

erogeneity of web data, state-of-the-art approaches focus on
information organized according to specific patterns that frequently occur on the Web. Meaningful examples are presented in [5], which focuses on data published in HTML
tables, and information extraction systems, such as TextRunner in [1], which exploits lexical-syntactic patterns. As
noticed in [5], even if a small fraction of the Web is organized according to these patterns, due to the web scale
the amount of data involved is impressive. Recently, the
problem of truth discovery among redundant and overlapping web sources has also been faced by the Solomon system at AT&T [7]. Similarly to our probabilistic analyzer,
the core of Solomon is a technique that detects copying between sources. Recently, applications that need to manage
large and imprecise data sets are emerging in many different domains (e.g., sensors, RFID, information extraction).
In order to store these large volumes of probabilistic data
and support complex queries, probabilistic database management systems are becoming the standard solution (see [6]
for a recent survey of different approaches). One of the features of our system is to automatically provide probabilistic
data from the Web that are suitable for such databases.

2.

SYSTEM DESCRIPTION

Figure 2 depicts the main process executed by the system.
It takes as input a small set of web pages for an entity of
interest, searches and processes web sources to build the hidden relation, and computes accuracy measures of the sources
to assign a probability distribution to the attributes of each
object. In the following we introduce the main ideas behind
the techniques used to attack several issues: sources localiza-

1

In this context, our measure of effectiveness is simply the
number of new pages that in each iteration are crawled and
discarded without contributing to the hidden relation.

186

WWW 2011 – Demo

March 28–April 1, 2011, Hyderabad, India

Figure 2: Overview of the system.
tion, data extraction and integration, probabilistic analysis,
searching and browsing of the probabilistic repository.
Source Localization [3]. The data of the hidden relation
are spread across different sources that need to be found. To
achieve this goal, we developed a structure-driven crawling
algorithm that locate collections of pages containing data
of interest. The input of the module is the hidden relation computed by a previous iteration of the system. The
crawler uses such input for two basic tasks: (i) querying
web search engines (such as Google) to obtain new candidate sources of interest and (ii) analyze the search results
in order to filter out the sources that are not relevant for
our purposes: the system considers relevant the data intensive sources that publish information about the domain of
interest. The crawler uses the data (and the metadata) in
the current hidden relation to formulate the search engine
queries, and infers a description of the underlying conceptual
entity to filter the query results.
Extraction and Integration [2]. These challenging issues are attacked by exploiting the redundancy of data among
the sources. In a bootstrapping phase, an unsupervised
wrapper inference algorithm generates a set of extraction
rules for each source. A instance-based matching algorithm
compares data returned by the generated extraction rules
among different sources and infers mappings among them.
In the integration phase, the abundance of redundancy among
web sources allows the system to acquire knowledge about
the domain and triggers an evaluation of the mappings.
Based on the quality of the inferred mappings, the matching
process provides a feedback to the wrapper generation process, which is thus driven to refine the bootstrapping wrappers in order to correct imprecise extraction rules. Better
extraction rules generate better mappings thus improving
the quality of the solutions. Also, the system relies on the
analysis of the templates performed during the process in order to extract from the page suitable semantic labels for the
mappings. These labels are also used during the source localization to generate a description of the target conceptual
entity.

Probabilistic Analysis [4]. As many sources use different formats, approximations, or even introduce errors, it is
very common to find conflicting values for the same attribute
of the same object. The computation of the correct value is
not always possible (it may be the case that no source is reporting it on the Web), but a probabilistic analysis allows us
to identify the most probable values based on the evidence
accumulated in the hidden relation. The simplest way of reconciliate conflicting values is using a voting approach, which
considers true the most frequent value. However, in the web
context, this is a simplistic solution since (i) some sources
are more reliable than others (thus they should weight more)
and (ii) some sources may copy data from other sources and
should not be considered at all.
The system performs an iterative Bayesian analysis that
is able to evaluate three factors: the consensus among the
sources, the sources’ accuracy and the presence of copiers,
that is, sources copying their data from other sources. As
a result of this analysis, each source is characterized by an
accuracy measure, and the hidden relation is summarized
into a probabilistic relation in which attribute values are
described by means of a probability distribution function.
Visualization. The generated probabilistic data can be
directly used in one of the many recent probabilistic database
management systems [6]. However, an important goal of the
system is to assist the user with an interactive browsing of
the repository. In order to provide such service, the repository can be queried with a keyword based interface and
from the output probabilistic relation is navigable to verify
its provenance (i.e., which sources present that value, what
are the accuracy of such sources) and its probability distribution (i.e., which alternative values are published with
lower probabilities) as shown in Figure 3.

3.

DEMOSTRANTION

We show how end users can benefit from the system through
three main demonstrative uses-cases: (i) discovering sources
given a sample of web pages, (ii) extracting and integrating

187

WWW 2011 – Demo

March 28–April 1, 2011, Hyderabad, India

structured data from the domain sources, and (iii) probabilistically modeling the information.
Our demonstration uses a local mirror of a fraction of the
Web. In particular we will use hundreds of randomly chosen
web sites containing sources for several domains.
At the beginning of the demonstration we will let the audience to pick a small number of sources publishing information about the same domain. This step corresponds to the
input represented in Figure 2. As a first result, the system
will produce a populated probabilistic database containing
the integrated information for all the pages of the chosen
sources. The database will be used to show different possible interactions with the audience at an increasing level of
detail:
• in the simplest scenario, the system allows the users
to perform keyword based queries on the repository.
For instance, given the query “The Sims 3” in the
videogames domain, the most probable values for the
attributes of the instance will be shown. E.g.: publisher=“Electronic Arts”, release date=“June 2, 2009”,
etc.
• in another scenario, the system allows the users to analyze the detail of the output of the probabilistic framework: for each object and each attribute returned by
a query, it is possible to visualize the probability distributions with all the values published by the sources,
the accuracy of the sources, the provenance of the values (snippet of the Web page). An example is shown
in Figure 3.
Later we demonstrate how our system finds new sources of
the target domain. The audience decides how many new web
sites should be found in the local mirror of the Web. The
system identifies and shows new valid sources to process. We
show statistics about the discovered information (in terms
of number of new tuples and new attributes in the hidden
relation, etc.) and we will run queries to show the impact
of the information just discovered.
On demand we will show technical insights of the three
modules:
• source localization module: we show the description of the entity of interest, which is automatically
inferred to find new sources, and how it is updated
during the process;

Figure 3: Screenshot of the system GUI.
[3] L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti.
Supporting the automatic construction of entity aware
search engines. In WIDM, pages 149–156, 2008.
[4] L. Blanco, V. Crescenzi, P. Merialdo, and P. Papotti.
Probabilistic models to reconcile complex data from
inaccurate data sources. In CAiSE, pages 83–97, 2010.
[5] M. J. Cafarella, A. Y. Halevy, D. Z. Wang, E. Wu, and
Y. Zhang. Webtables: exploring the power of tables on
the web. PVLDB, 1(1):538–549, 2008.
[6] N. N. Dalvi, C. Ré, and D. Suciu. Probabilistic
databases: diamonds in the dirt. Commun. ACM,
52(7):86–94, 2009.
[7] X. Dong, L. Berti-Equille, Y. Hu, and D. Srivastava.
Global detection of complex copying relationships
between sources. PVLDB, 3(1):1358–1369, 2010.

• extraction and integration module: we show the
extraction rules and the mappings automatically generated, and how the wrapper inference and the integration processes affect each other;
• probabilistic analyzer module: using both real and
synthetic scenarios, we show how the probability distribution functions, the accuracy of the sources, and
the presence of copiers are mutually dependent.

4.

REFERENCES

[1] M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. Open information extraction from the
web. In IJCAI, 2007.
[2] L. Blanco, M. Bronzi, V. Crescenzi, P. Merialdo, and
P. Papotti. Redundancy-driven web data extraction
and integration. In WebDB, 2010.

188

Holistic Data Cleaning: Putting Violations Into
Context
Xu Chu1?

Ihab F. Ilyas2

Paolo Papotti2

1

2

University of Waterloo, Canada
x4chu@uwaterloo.ca
Qatar Computing Research Institute (QCRI), Qatar
{ikaldas,ppapotti}@qf.org.qa

Abstract—Data cleaning is an important problem and data
quality rules are the most promising way to face it with a
declarative approach. Previous work has focused on specific
formalisms, such as functional dependencies (FDs), conditional
functional dependencies (CFDs), and matching dependencies
(MDs), and those have always been studied in isolation. Moreover,
such techniques are usually applied in a pipeline or interleaved.
In this work we tackle the problem in a novel, unified
framework. First, we let users specify quality rules using denial
constraints with ad-hoc predicates. This language subsumes
existing formalisms and can express rules involving numerical
values, with predicates such as “greater than” and “less than”.
More importantly, we exploit the interaction of the heterogeneous constraints by encoding them in a conflict hypergraph.
Such holistic view of the conflicts is the starting point for a
novel definition of repair context which allows us to compute
automatically repairs of better quality w.r.t. previous approaches
in the literature. Experimental results on real datasets show that
the holistic approach outperforms previous algorithms in terms
of quality and efficiency of the repair.

I. I NTRODUCTION
It is well recognized that business and scientific data are
growing exponentially and that they have become a first-class
asset for any institution. However, the quality of such data is
compromised by sources of noise that are hard to remove in the
data life-cycle: imprecision of extractors in computer-assisted
data acquisition may lead to missing values, heterogeneity
in formats in data integration from multiple sources may
introduce duplicate records, and human errors in data entry can
violate declared integrity constraints. These issues compromise
querying and analysis tasks, with possible damage in billions
of dollars [9]. Given the value of clean data for any operation,
the ability to improve their quality is a key requirement for
effective data management.
Data cleaning refers to the process of detecting and correcting errors in data. Various types of data quality rules
have been proposed for this goal and great efforts have been
made to improve the effectiveness and efficiency of their
cleaning algorithms (e.g., [4], [8], [21], [18], [13]). Currently
existing techniques are used in isolation. One naive way to
enforce all would be to cascade them in a pipeline where
different algorithms are used as black boxes to be executed
sequentially or in an interleaved way. This approach minimizes
the complexity of the problem as it does not consider the
? Work done while interning at QCRI.

978-1-4673-4910-9/13/$31.00 © 2013 IEEE

interaction between different types of rules. However, this
simplification can compromise the quality in the final repair
due to the lack of end-to-end quality enforcement mechanism
as we show in this paper.
Example 1.1: Consider the GlobalEmployees table (G for
short) in Figure 1. Every tuple specifies an employee in a
company with her id (GID), name (FN, LN), role, city, area
code (AC), state (ST), and salary (SAL). We consider only two
rules for now. The first is a functional dependency (FD) stating
that the city values determine the values for the state attribute.
We can see that cells in t4 and t6 present a violation for this
FD: they have the same value for the city, but different states.
We highlight the set S1 of four cells involved in the violation
in the figure. The second rule states that among employees
having the same role, salaries in NYC should be higher. In this
case cells in t5 and t6 are violating the rule, since employee
Lee (in NYC) is earning less than White (in SJ). The set S2
of six cells involved in the violation between Lee and White
is also highlighted.
The two rules detect that at least one value in each set of
cells is wrong, but taken individually they offer no knowledge
of which cells are the erroneous ones.


Fig. 1: Local (L) and Global (G) relations for employees data.

Previously proposed data repairing algorithms focus on repairing violations that belong to each class of constraints in
isolation, e.g., FD violation repairs [6]. These techniques miss
the opportunity of considering the interaction among different
classes of constraints violations. For the example above, a

458

ICDE Conference 2013

desirable repair would update the city attribute for t6 with
a new value, thus only one change in the database would fix
the two violations. On the contrary, existing methods would
repair the FD by changing one cell in S1 , with an equal chance
to pick any of the four by being oblivious to violations in
other rules. In particular, most algorithms would change the
state value for t6 to NY or the state for t4 to AZ. Similarly,
rule based approaches, when dealing with application-specific
constraints such as the salary constraint above, would change
the salaries of t5 or t6 in order to satisfy the constraints. None
of these choices would fix the mistake for city in t6 , on the
contrary, they would add noise to the existing correct data.
This problem motivates the study of novel methods to
correct violations for different types of constrains with desirable repairs, where desirability depends on a cost model
such as minimizing the number of changes, the number of
invented values, or the distance between the value in the
original instance and the repair. To this end, we need quality
rules that are able to cover existing heterogeneous formalisms
and techniques to holistically solve them, while keeping the
process automatic and efficient.
Since the focus of this paper is the holistic repair of a set of
integrity constraints more general than existing proposals, we
introduce a model that accepts as input Denial Constraints
(DCs), a declarative specification of the quality rules which
generalizes and enlarge the current class of constraints for
cleaning data. Cleaning algorithms for DCs have been proposed before [4], [20], but they are limited in scope, as they
repair numeric values only, generality, only a subclass of DCs
is supported, and in the cost model, as they aim at minimizing
the distance between original database and repair only. On the
contrary, we can repair any value involved in the constraints,
we do not have limits on the allowed DCs, and we support
multiple quality metrics (including cardinality minimality).
Example 1.2: The two rules described above can be expressed with the following DCs:
c1 : ¬(G(g, f, n, r, c, a, s), G(g 0 , f 0 , n0 , r0 , c0 , a0 , s0 ),
(c = c0 ), (s 6= s0 ))
0
0
0 0 0 0 0
c2 : ¬(G(g, f, n, r, c, a, s), G(g , f , n , r , c , a , s ),
(r = r0 ), (c = “N Y C”), (c0 6= “N Y C”), (s0 > s))
The DC in c1 corresponds to the FD: G.CIT Y → G.ST and
has the usual semantics: if two tuples have the same value for
city, they must have the same value for state, otherwise there
is a violation. The DC in c2 states that every time there are
two employees with the same rank, one in NYC and one in a
different city, there is a violation if the salary of the second
is greater than the salary of the first.

Given a set of DCs and a database to be cleaned, our
approach starts by compiling the rules into data violations
over the instance, so that, by analyzing their interaction, it is
possible to identify the cells that are more likely to be wrong.
In the example, t6 [CIT Y ] is involved in both violations,
so it is the candidate cell for the repair. Once we have
identified what are the cells that are most likely to change,
we process their violations to get information about how

to repair them. In the last step, heterogeneous requirements
from different constraints are holistically combined in order to
fix the violations. In the case of t6 [CIT Y ], both constraints
are satisfied by changing its value to a string different from
“NYC”, so we update the cell with a new value.
A. Contributions
We propose a method for the automatic repair of dirty data,
by exploiting the evidence collected with the holistic view of
the violations:
•

•

•

We introduce a compilation mechanism to project denial
constraints on the current instance and capture the interaction among constraints as overlaps of the violations on
the data instance. We compile violations into a Conflict
Hypergraph (CH) which generalizes the one previously
used in FD repairing [18] and is the first proposal to
treat quality rules with different semantics and numerical
operators in a unified artifact.
We present a novel holistic repairing algorithm that
repair all violations together w.r.t. one unified objective
function. The algorithm is independent of the actual cost
model and we present heuristics aiming at cardinality and
distance minimality.
We handle different repair semantics by using a novel
concept of Repair Context (RC): a set of expressions
abstracting the relationship among attribute values and
the heterogeneous requirements to repair them. The RC
minimizes the number of cells to be looked at, while
guaranteeing soundness.

We verify experimentally the effectiveness and scalability of
the algorithm. In order to compare with previous approaches,
we use both real-life and synthetic datasets. We show that the
proposed solution outperforms state of the art algorithms in all
scenarios. We also verify that the algorithms scale well with
the size of the dataset and the number of quality rules.
B. Outline
We discuss related work in Section II, introduce preliminary
definitions in Section III, and give an overview of the solution
in Section IV. Technical details of the repair algorithms are
discussed in Section V. System optimizations are discussed
in Section VI, while experiments are reported in Section VII.
Finally, conclusions and future work are discussed in Section
VIII.
II. R ELATED W ORK
In industry, major database vendors have their own products
for data quality management, e.g., IBM InfoSphere QualityStage, SAP BusinessObjects, Oracle Enterprise Data Quality, and Google Refine. These systems typically use simple,
low-level ETL procedural steps [3]. On the other hand, in
academia, researchers are investigating declarative, constraintbased rules [4], [5], [13], [8], [11], [12], [21], [18], which
allow users to detect and repair complicated patterns in the

459

data. However, a unified approach to data cleaning that combines evidence from heterogeneous rules is still missing and
it is the subject of this work.
Interleaved application of FDs and MDs has been studied
before [13] and some works (e.g., [6], [5], [8]) compute sets of
cells that are connected by violations from different FDs. This
connected component is usually called “equivalence class”
and it is a special case of the notion of repair context that
we introduce next. Another work [18] exploits the interaction
among FDs by using a hypergraph. In our proposal we extend
the use of hypergraphs to denial constraints, thus significantly
generalizing the original proposal. Moreover, we simplify it,
by considering only current violations, thus avoiding a large
number of hyperedges that they compute in order to execute
the repair process with a single iteration. In fact, by using
multiple iterations we can be more general and compute
interactions of rules happening in more than two steps.
In this work we compute repairs with a large class of
operators in the quality rules: =, 6=, <, >, ≤, ≥, ≈ (similarity).
Most of the previous approaches [6], [8] were dealing only
with equality, and can be seen as special cases of our work.
Exceptions are [4], [20], where the authors propose algorithms
to repair numerical attributes for denial constraints. In our
work we extend their results in three important aspects: (i)
we treat both strings and numeric values together, thus not
restricting updates to numeric values only; (ii) we do not
limit the input constraints to local denial constrains; and
(iii) we allow multiple quality metrics (including cardinality
minimality), while still minimizing the distance between the
numeric values in the original and the repaired instances. We
show in the experimental study that our algorithms provide
repairs of better quality, even for the quality metric in [4].
In general, denial constraints can be extracted from existing business rules with human intervention. Moreover, a
source of constraints with numeric values from enterprise
databases is data mining [2]. Inferred rules always have a
confidence, which clearly points to data quality problems in
the instances. For example, a confidence of 98.5% for a rule
“discountedPrice<unitPrice” implies that 1.5% of the records
require some cleaning.
III. P RELIMINARIES

and θ ∈ B. Similarity predicate ≈ is positive when the edit
distance between two strings is above a user-defined threshold
δ.
Single-tuple constraints (such as SQL CHECK constraints),
Functional Dependencies, Matching Dependencies, and Conditional Functional Dependencies are special cases of unary
and binary denial constraints with equality and similarity
predicates.
Given a database instance I of schema S and a DC ϕ, if I
satisfies ϕ, we write I |= ϕ. If we have a set of DC Σ, I |= Σ
if and only if ∀ϕ ∈ Σ, I |= ϕ. A repair I 0 of an inconsistent
instance I is an instance that satisfies Σ and has the same
set of tuple identifiers in I. Attribute values of tuples in I
and I 0 can be different and, for infinite domains of attributes
in R, there is an infinite number of possible repairs. Similar
to [5], [18], we represent the infinite space of repairs as a finite
set of instances with fresh attribute values. In a repair, each
fresh value F V for attribute A can be replaced with a value
from Dom(A) \ Doma (A), where Doma (A) is the domain
of the values for A which satisfy at least a predicate for each
denial constraints involving F V . In other words, fresh values
are values of the domain for the actual attributes which do not
satisfy any of the predicates defined over them.
Notice that our setting does not rely on restrictions such
as local constraints [20] or certain regions [14]: it is possible
that a repair for a denial constraint triggers a new violation
for another constraint. In order to enforce termination of the
cleaning algorithm fresh values are introduced in the repair.
More details are discussed in the following sections.
B. Problem Definition
Since the number of possible repairs is usually very large
and possibly infinite, it is important to define a criterion to
identify desirable ones. In fact, we aim at solving the following
data cleaning problem: given as input a database I and a
set of denial constraints Σ, we compute a repair Ir of I
such that Ir |= Σ (consistency) and their distance cost(Ir ,
I) is minimum (accuracy). A popular cost function from the
literature [6], [8] is the following:
X
disA (I(t[A]), I(t0 [A]))
t∈I,t0 ∈Ir ,A∈AR

A. Background
Consider database schema of the form S = (U, R, B), where
U is a set of database domains, R is a set of database predicates
or relations, and B is a set of finite built-in predicates. In this
paper, B = {=, <, >, 6=, ≤, ≈}. For an instance I of S, and
an attribute A ∈ U, and a tuple t, we denote by Dom(A) the
domain of attribute A. We denote by t[A] or I(t[A]) the value
of the cell of tuple t under attribute A.
In this work we support the subset of integrity constraints identified by denial constraints (DCs) over relational
databases. Denial constraints are first-order formulae of the
form ϕ : ∀x¬(R1 (x1 ) ∧ . . . ∧ Rn (xn ) ∧ P1 ∧ . . . ∧ Pm ), where
Ri ∈ R is a relation atom, and x = ∪xi , and each Pi of
the form v1 θc, or v1 θv2 , where v1 , v2 ∈ x, c is a constant,

where t0 is the repair for tuple t and disA (I(t[A]), I(t0 [A]))
is a distance between their values for attribute A (an exact
match returns 0)1 . There exist many similarity measurements
for structured values (such as strings) and our setting does
not depend on a particular approach, while for numeric values
we rely on the squared Euclidian distance (i.e., the sum of
the square of differences). We call this measure of the quality
the Distance Cost. It has been shown that finding a repair of
minimal cost is NP-complete even for FDs only [6]. Moreover,
1 We omit the confidence in the accuracy of attribute A for tuple t because it
is not available in many practical settings. While our algorithms can support
confidence, for simplicity we will consider the cells with confidence value
equals to one in the rest of the paper, as confidence does not add specific
value to our solution.

460

minimizing the above function for DCs and numerical values
only it is known to be a MaxSNP-hard problem [4].
Interestingly, if we rely on a binary distance between
values (0 if they are equal, 1 otherwise), the above cost
function corresponds to aiming at computing the repair with
the minimal number of changes. The problem of computing
such cardinality-minimal repairs is known to be NP-hard to
be solved exactly, even in the case with FDs only [18]. We
call this quality measure Cardinality-Minimality Cost.
Given the intractability of the problems, our goal is to
compute nearly-optimal repairs. We rely on two directions to
achieve it: approximation holistic algorithms to identify cells
that need to be changed, and local exact algorithms within the
cells identified by our notion of Repair Context. We detail our
solutions in the following sections.
IV. S OLUTION OVERVIEW
In this Section, we first present our system architecture, and
we explain two data structures: the conflict hypergraph (CH)
to encode constraint violations and the repair context (RC) to
encode violation repairs.

Fig. 2: Architecture of the system.

The DCs Parser provides rules for detecting violations
(through the Detect module) and rules for fixing the violations
to be executed by the LookUp module as we explain in the
following example.
Example 4.2: Given the database in Figure 1, the DCs
Parser processes constraint c3 and provides the Detect module
the rule to identify a violation spanning ten cells over tuples
t1 , t2 , and t3 as highlighted. Since every cell of this group
is a possible error, DCs Parser dictates the LookUp module
how to fix the violation if any of the ten cells is considered
to be incorrect. For instance, the violation is repaired if “Paul
Smith” is not the manager of “Mark White” in L (represented
by the repair expression (l 6= m0 )), if the employee in L does
not match the one in G because of a different city (c 6= c∗ ),
or if the role for the employee in G is updated to manager
(r∗ = M ).

We described how each DC is parsed so that violations and
fixes for that DC can be obtained. However, our goal is to
consider violations from all DCs together and generate fixes
holistically. For this goal we introduce two data structures: the
Conflict Hypergraph (CH), which encodes all violations into a
common graph structure, and the Repair Context (RC), which
encodes all necessary information of how to fix violations
holistically. The Detect module is responsible for building the
CH that is then fed into the LookUp module, which in turn
is responsible for building the RC. The RC is finally passed
to a Determination procedure to generate updates. Depending
on the content of the RC, we have two Determination cores,
i.e., Value Frequency Map (VFP) and Quadratic Programming
(QP). The updates to the database are applied, and the process
is restarted until the database is clean (i.e., empty CH), or a
termination condition is met.
B. Violations Representation: Conflict Hypergraph

A. System Architecture
The overall system architecture is depicted in Figure 2. Our
system takes as input a relational database (Data) and a set of
denial constraints (DCs), which express the data quality rules
that have to be enforced over the input database.
Example 4.1: Consider the LocalEmployee table (L for
short) in Figure 1. Every tuple represents information stored
for an employee of the company in one specific location:
employee local id (LID), name (FN, LN), rank (RNK), number
of days off (DO), number of years in the company (Y), city
(CT), manager id (MID), and salary (SAL). LocalEmployee
table and GlobalEmployee table constitute the input database.
We introduce a third DC:
c3 : ¬(L(l, f, n, r, d, y, c, m, s), L(l0 , f 0 , n0 , r0 , d0 , y 0 , c0 , m0 , s0 ),
G(g ∗ , f ∗ , n∗ , r∗ , c∗ , a∗ , s∗ ), (l 6= l0 ), (l = m0 ),
(f ≈ f ∗ ), (n ≈ n∗ ), (c = c∗ ), (r∗ 6= “M ”))
The constraint states that a manager in the local database L
cannot be listed with a status different from “M” in the global
database G. The rule shows how different relations, similarity
predicate, and self-joins can be used together.


We represent the violations detected by the Detect module in
a graph, where the nodes are the violating cells and the edges
link cells involved in the same violation. As an edge can cover
more than two nodes, we use a Conflict Hypergraph (CH)
[18]. This is an undirected hypergraph with a set of nodes
P representing the cells and a set of annotated hyperedges
E representing the relationships among cells violating a constraint. More precisely, a hyperedge (c; p1 , . . . , pn ) is a set of
violating cells such that one of them must change to repair the
constraint, and contains: (a) the constraint c, which induced the
conflict on the cells; (b) the list of nodes p1 , . . . , pn involved
in the conflict.
Example 4.3: Consider Relation R in Figure 3a and the
following constraints (expressed as FDs and CFDs for readability): ϕ1 : A → C, ϕ2 : B → C, and ϕ3 : R[D = 5] →
R[C = 5]. CH is built as in Figure 3b: ϕ1 has 1 violation e1 ;
ϕ2 has 2 violations e2 , e3 ; ϕ3 has 1 violation e4 .

The CH represents the current state of the data w.r.t.
the constraints. We rely on this representation to analyze
the interactions among violations on the actual database. A
hyperedge contains only violating cells: in order to repair it,

461

order to compute a repair.2 For instance, a possible MVC for
the CH in Figure 3b identifies t2 [C] and t4 [C].
After detecting all violations in the current database and
building the CH, the next step is to generate fixes taking into
account the interaction among violations. In order to facilitate
a holistic repair, we rely on another data structure, which is
discussed next.
(a) Data

(b) CH

Fig. 3: CH Example.

at least one of its cells must get a new value. Interestingly, we
can derive a repair expression for each of the cell involved in
a violation, that is, for each variable involved in a predicate
of the DC. Given a DC d : ∀x¬(P1 ∧ . . . ∧ Pm ) and a set of
violating cells (hyperedge) for it V = {v1 , . . . , vn }, for each
vi ∈ V there is at least one alternative repair expression of
the form vi ψt, where t is a constant or a connected cell in V .
This leads to the following property of hyperedges.
Lemma 4.4: All the repairs for hyperedge e have at most
cardinality n with n ≤ m, where m is the size of the biggest
chain of connected variables among its repair expressions. 
Proof Sketch. We start with the case with one predicate only in
the DC d. If it involves a constant, then the repair expression
contains only one cell and its size coincides with the size of
the hyperedge. If it is a predicate involving another cell, then at
least one of them is going to change in the repair. We consider
now the case with more than a predicate in d. In this case,
as the predicates allowed are binary, there may be a chain of
connected variables of size m in the repair expressions: when
a value is changed for a cell, it may triggers changes in the
connected ones. Therefore, in the worst case, to repair them
m changes are needed.

The Lemma states an upper bound for the number of
changes that are needed to fix a hyperedge. More importantly,
it highlights that in most cases one change suffices as we show
in the following example.
Example 4.5: In c3 the biggest chain of variables in the
repair expressions comes from l = l0 (from the first predicate)
and l 6= m0 (from the second predicate). This means that,
to repair violations for c3 , at most three changes are needed.
Notice that only one change is needed for most of the cells
involved in a violation.

A naı̈ve approach to the problem is to compute the repair
by fixing the hyperedges one after the other in isolation.
This would lead to a valid repair, but, if there are interacting
violations, it would certainly change more cells than the repair
with minimal cost. As our goal is to minimize changes in the
repair, we can rely on hyperedges for identifying cells that
are very likely to be changed. The intuition here is that, in
the spirit of [18], by using algorithms such as the Minimum
Vertex Cover (MVC), we can identify at the global level what
are the minimum number of violating cells to be changed in

C. Fixing Violation Holistically: Repair Context
We start from cells that MVC identifies as likely to be
changed, and incrementally identify other cells that are involved in the current repair. We call the starting cells and the
newly identified ones frontier. We call repair expressions the
list of constant assignments and constraints among the frontier.
The frontier and the repair expressions form a Repair Context
(RC). We elaborate RC using the following example.
Example 4.6: Consider the database and CH in Example
4.3. Suppose we have t2 [C] and t4 [C] from the MVC as
starting points. We start from t2 [C], which is involved in 3
hyperedges. Consider e1 : given t2 [C] to change, the expression
t2 [C] = t1 [C] must be satisfied to solve it, thus bringing t1 [C]
into frontier. Cell t1 [C] is not involved in other hyperedges,
so we stop. Similarly, t2 [C] = t3 [C] must be satisfied to
resolve e2 and t3 [C] is brought into the frontier. For e3 ,
t2 [C] = t4 [C] is the expression to satisfy, however, t4 [C]
is involved also in e4 . We examine e4 given t4 [C] and we
get another expression t4 [C] = 5. The resulting RC consists
of frontier: t1 [C], t2 [C], t3 [C], t4 [C], and repair expressions:
t2 [C] = t1 [C], t2 [C] = t3 [C], t2 [C] = t4 [C], t4 [C] = 5.
Notice that by starting from t4 [C] the same repair is
obtained and the frontier contains only four cells instead of
ten in the connected component of the hypergraph.

An RC is built from a starting cell c with violations from
DCs D with a recursive algorithm (detailed in the next section)
and has two properties: (i) there is no cell in its frontier that
is not (possibly transitively) connected to c by a predicate in
the repair expression of at least a d ∈ D, (ii) every cell that
is (possibly transitively) connected to c by a predicate in the
repair expression of at least a d ∈ D is in its frontier. In
other terms, RC contains exactly the information required by
a repair algorithm to make an informed, holistic decision.
Lemma 4.7: The Repair Context contains the sufficient and
necessary information to repair all the cells in its frontier. 
Proof Sketch. We start with the necessity. By definition, the RC
contains the union of the repair expressions over the cells in
the frontier. If it is possible to find an assignment that satisfies
the repair expressions, all the violations are solved. It is evident
that, if we remove one expression, then it is not guaranteed
that all violations can be satisfied. The repair expressions are
sufficient because of the repair semantics of the DCs. As the
frontier contains all the connected cells, any other cell from
V would add an expression that is not needed to repair the
violation for d and would require to change a cell that is not
needed for the repair.

2 In order to keep the execution time acceptable an approximate algorithm
is used to compute the MVC.

462

We can now state the following result regarding the RC:
Proposition 4.8: An RC always has a repair of cardinality
n with n ≤ u, where u is the size of its frontier.

Proof Sketch. From Lemmas 4.7 and 4.4 it can be derived that
(i) it is always possible to find a repair for RC, and (ii) in the
worst case the repair has the size of the union of the chains
of the connected variables in its repair expressions.

In practice, the number of cells in a DC is much smaller
than the number of cells in the respective hyperedges. For
t6 [CIT Y ], the size of the RC is one, while there are nine
cells in the two hyperedges for c1 and c2 .
Given the discussion above, for each cell in the MVC we
exploit its violations with the LookUp module to get the RC.
Once all the expressions are collected, a Determination step
takes as input the RC and computes the valid assignments for
the cells involved in it. In this step, we rely on a function
to minimize the cost of changing strings (VFM) and on
an external Quadratic Programming (QP) tool in order to
efficiently solve the system of inequalities that may arise when
numeric values are involved. The assignments computed in this
step become the updates to the original database in order to fix
the violations. The following example illustrates the use of QP,
while LookUp and Determination processes will be detailed
in the next section.
Example 4.9: Consider again the L relation in Figure 1.
Two DCs are defined to check the number of extra days off
assigned to each employee:
c4 : ¬(L(l, f, n, r, d, y, c, m, s), (r = “A”), (d < 3)
c5 : ¬(L(l, f, n, r, d, y, c, m, s), (y > 4), (d < 4))
In order to minimize the change, the QP formulation of the
problem for t1 [DO] is (x − 2)2 with constraints x ≥ 3 and
x ≥ 4. Value 4 is returned by QP and assigned to t1 [DO]. 
The holistic reconciliation provided by the RC has several
advantages: the cells connected in the RC form a subset of
the connected components of the graph and this leads to better
efficiency in the computation and better memory management.
Moreover, the holistic choice done in the RC minimizes the
number of changes for the same cell; instead of trying different
possible repairs, an informed choice is made by considering
all the constraints on the connected cells. We will see how
this leads to better repairs w.r.t. previous approaches.
V. C OMPUTING T HE R EPAIRS
In this Section we give the details of our algorithms. We
start by presenting the iterative algorithm that coordinates the
detect and repair processes. We then detail the technical solutions we built for D ETECT, L OOK U P, and D ETERMINATION.
A. Iterative Algorithm
Given a database and a set of DCs, we rely on Algorithm 1.
It starts by computing violations, the CH, and the MVC over
it. These steps bootstrap the outer loop (lines 5–26), which is
repeated until the current database is clean (lines 19–22) or a
termination condition is met (lines 23–26). Cells in the MVC
are ranked in order to favor those involved in more violations

Algorithm 1 Holistic Repair
Input: Database data, Denial Constraints dcs
Output: Repair data
1: Compute violations, conflict hypergraph, MVC.
2: Let processedCells be a set of cells in the database that
have already been processed.
3: sizeBefore ← 0
4: sizeAfter ← 0
5: repeat
6:
sizeBefore ← processedCell.size()
7:
mvc ← Re-order the vertices in MVC in a priority
queue according to the number of hyperedges
8:
while mvc is not empty do
9:
cell ← Get one cell from mvc
10:
rc ← Initialize a new repair context for that cell
11:
edges ← Get all hyperedges for that cell
12:
while edges is not empty do
13:
edge ← Get an edge from edges
14:
L OOK U P(cell, edge, rc)
15:
end while
16:
assignments ← D ETERMINATION(cell, exps)
17:
data.update(assignments)
18:
end while
19:
reset the graph: re-build hyperedges, get new MVC
20:
if graph has no edges then
21:
return data
22:
end if
23:
tempCells ← graph.getAllCellsInAllEdges()
24:
processedCells ← processedCells ∪ tempCells
25:
sizeAfter ← processedCell.size()
26: until sizeBefore ≤ sizeAfter
27: return data.P ostP rocess(tempCells, M V C)

and are repaired in the inner loop (lines 8–18). In this loop,
the RC for the cell is created with the L OOK U P procedure.
When the RC is completed, the D ETERMINATION step assigns
the values to the cells that have a constant assignments in the
repair expressions (e.g., t1 [A] = 5). Cells that do not have
assignments with constants (e.g., t1 [A] 6= 1), keep their value
and their repair is delayed to the next outer loop iteration. If
the updates lead to a new database without violations, then it
can be returned as a repair, otherwise the outer loop is executed
again. If no new cells have been involved w.r.t. the previous
loop, then the termination condition is triggered and the cells
without assignments are updated with new fresh values in the
post processing final step.
The outer loop has a key role in the repair. In fact, it is
possible that an assignment computed in the determination
step solves a violation, but raises a new one with values that
were not involved in the original CH. This new violation is
identified at the end of the inner loop and a new version of the
CH is created. This CH has new cells involved in violations
and therefore the termination condition is not met.
Before returning the repair, a post-processing step updates

463

all the cells in the last MVC (computed at line 19) to fresh
values. This guarantees the consistency of the repair and no
new violations can be triggered. Pushing to the very last the
assignment of a fresh value forces the outer loop to try to find
a repair with constants until the termination condition is met,
as we illustrate in the following example.
Example 5.1: Consider again only rules c1 and c2 in the
running example. After the first inner loop iteration, the RC
contains an assignment t6 [CITY] 6= “N Y C”, which is not
enforced by the determination step and therefore the database
does not change. The HC is created again (line 19) and it still
has violations for c1 and c2 . The cells involved in the two
violations go into tempCells and sizeAfter is set to 9. A new
outer loop iteration sets sizeBefore to 9, the inner loop does
not change the data, and it gets again the same graph at line
19. As sizeBefore = sizeAfter, it exits the outer loop and the
post processing assigns t6 [CITY] = “F V ”.

Proposition 5.2: For every set of DCs, if the determination
step is polynomial, then Holistic Repair is a polynomial
time algorithm for the data cleaning problem.

Proof sketch. It is easy to see that the output of the algorithm
is a repair for an input database D with DCs dcs. In the outer
loop we change cells to constants that satisfy the violations and
in the post process we resolve violations that were not fixable
with a constant by introducing fresh values. As fresh values
do not match any predicate, the process eventually terminates
and returns a repair which does not violate the DCs anymore.
The vertex cover problem is an NP-complete problem and
there are standard approaches to find approximate solutions.
We use a greedy algorithm with factor k approximation, where
k is the maximum number of cells in a hyperedge of the HC.
Our experimental studies show that a k approximation of the
MVC lead to better results w.r.t. alternative ways to identify
the seed cells for the algorithm. The complexity of the greedy
algorithm is linear in the number of edges. In the worst case,
the number of iterations of the outer loop is bounded by the
number of constraints in dc plus one: it is possible to design
a set of DCs that trigger a new violation at each repair, plus
one extra iteration to verify the termination condition. The
complexity of the algorithm is bounded by the polynomial
time for the detection step: three atoms in the DC need a
cubic number of comparisons in order to check all the possible
triplets of tuples in the database. In practice, the number of
tuples is orders of magnitude bigger than the number of DCs
and therefore the size of the data dominates the complexity
O(|data|c |dcs|), where c is the largest number of atoms in a
rule of dcs. The complexity of the inner loop depends on the
number of edges in the CH and on the complexity of L OOK U P
and D ETERMINATION that we discuss next.

Though Algorithm 1 is sound, it is not optimal, as it is
illustrated in the following example.
Example 5.3: Consider again Example 4.3. We showed a
repair with four changes obtained with our algorithm, but there
exists a cardinality minimal repair with only three changes:
t1 [C] = 3, t2 [C] = 3, t4 [D] = N V .

We now describe the functions to generate and manipulate the

predicate in dcs
predicate in repair exps

=
6
=

6=
=

>
<=

>=
<

<
>=

<=
>

≈t
6=t

TABLE I: Table of conversion of the predicates in a DC for
their repair. Predicate 6=t states that the distance between two
strings must be greater than t.

building blocks of our approach.
B. D ETECT: Identifying Violations
Identifying violations is straightforward: every valid assignment for the denial constraint is tested, if all the atoms for an
assignment are satisfied, then there is a violation.
However, the detection step is the most expensive operation
in the approach as the complexity is polynomial with the
number of atoms in the DC as the exponent. For example,
in the case of simple pairwise comparisons (such as in FDs),
the complexity is quadratic in the number of tuples, and it
is cubic for constraints such as c3 in Example 4.1. This is
also exacerbated by the case of similarity comparisons, when,
instead of equality check, there is the need to compute edit
distances between strings, which is an expensive operation.
In order to improve the execution time on large relations,
optimization techniques for matching records [10] are used.
In particular, the blocking method partitions the relations into
blocks based on discriminating attributes (or blocking keys),
such that only tuples in the same block are compared.
C. L OOK U P: Building the Repair Context
Given a hyperedge e = {c; p1 , . . . , pn } and a cell p =
ti [Aj ] ∈ P , the repair expression r for p may involve other
cells that need to be taken into account when assigning a value
to p. In particular, given e and p, we can define a rule for the
generation of repair expressions.
As p ∈ Aφ (c), then it is required that r : pφc c, where φc is
the predicate converted as described in Table I. Variable c can
be a constant or another cell. For denial constraints, we defined
a function DC.Repair(e,c), based on the above rule, which
automatically generates a repair expression for a hyperedge e
and a cell c. We first show an example of its output when
constants are involved in the predicate and then we discuss
the case with variables.
Example 5.4: Consider the constraint c2 from the example
in Figure 1. We show below two examples of repair expressions for it.
DC.Repair((c2 ; t5 [ROLE], t5 [CITY], t5 [SAL], . . . , t6 [SAL]),
t5 [ROLE]) = {t5 [ROLE] 6= “E”}
DC.Repair((c2 ; t5 [ROLE], t5 [CITY], t5 [SAL], . . . , t6 [SAL]),
t6 [SAL]) = {t6 [SAL] ≥ 80}
In the first repair expression the new value for t5 [ROLE]
must be different from “E” to solve the violation. The repair expression does not state that its new value should be
different from the active domain of ROLE (i.e., t5 [ROLE] 6=
{“E”,“S”,“M”}), because in the next iteration of the outer
loop it is possible that another repair expression imposes

464

t5 [ROLE] to be equal to a constant already in the active domain
(e.g., a MD used for entity resolution). If there is no other
expression suggesting values for t5 [ROLE], in a following step
the termination condition will be reached and the post-process
will assign a fresh value to t5 [ROLE].

Given a cell to be repaired, every time another variable is
involved in a predicate, at least another cell is involved in the
determination of its new value. As these cells must be taken
into account, we also collect their expressions, thus possibly
triggering the inclusion of new cells. We call L OOK U P the
recursive exploration of the cells involved in a decision.

Algorithm 3 D ETERMINATION
Input: Cell cell, Repair Context rc
Output: Assignments assigns
1: exps ← rc.getExps()
2: if exps contain >, <, >=, <= then
3:
QP ← computeSatisfiable(exps)
4:
assigns ← QP.getAssigments()
5: else
6:
V F M ← computeSatisfiable(exps)
7:
assigns ← V F M.getAssigments()
8: end if
9: return assigns

Algorithm 2 L OOK U P
Input: Cell cell, Hyperedge edge, Repair Context rc
Output: updated rc
1: exps ← Denial.repair(edge, cell)
2: frontier ← exps.getF rontier()
3: for all cell ∈ frontier do
4:
edges ← cell.getEdges()
5:
for all edge ∈ edges do
6:
exps ← exps ∪ L OOK U P(cell,edge,rc).getExps()
7:
end for
8: end for
9: rc.update(exps)
Algorithm 2 describes how, given a cell c and a hyperedge
e, L OOK U P processes recursively in order to move from a
single repair expression for c to a Repair Context.
Proposition 5.5: For every set of DCs and cell c, L OOK U P
always terminates in linear time and returns a Repair Context
for c.

Proof sketch. The correctness of the RC follows from the
traversal of the entire graph. Cycles are avoided as in the
expressions the algorithm keeps track of previously visited
nodes. As it is a Depth-first search, its complexity is linear in
the size of graph and is O(2V − 1), where V is the largest
number of connected cells in an RC.

Example 5.6: Consider the constraint c3 from the example
in Figure 1 and the DC:
c6 : ¬(G(g, f, n, r, c, a, s), (r = “V ”), (s < 200))
That is, a vice-president cannot earn less than 200. Given
t3 [ROLE] as input, L OOK U P processes the two edges over it
and collects the repair expressions t3 [ROLE] 6= “V ” from c6
and t3 [ROLE] = “M ” from c3 .

D. D ETERMINATION: Finding Valid Assignments
Given the set of repair expressions collected in the RC,
the D ETERMINATION function returns an assignment for the
frontier in the RC. The process for the determination is
depicted in Algorithm 3: Given an RC and a starting cell,
we first choose a (maximal) subset of the repair expressions
that is satisfiable, then we compute the value for the cells in
the frontier aiming at minimizing the cost function, and update
the database accordingly later.

In Algorithm 3, we have two determination procedures. One
is Value Frequency Map (VFM), which deals with string typed
expressions. The other is quadratic programming (QP), which
deals with numerical typed expressions3 .
1) Function computeSatisfiable: Given the current set of
expressions in the context, this function identifies the subset
of expressions that are solvable.
Some edges may be needed to be removed from the RC to
make it solvable. First, a satisfiability test verifies if the repair
expressions are in contradiction. If the set is not satisfiable,
the repair expressions coming from the hyperedge with the
smallest number of cells are removed. If the set of expressions
is now satisfiable, the removed hyperedge is pushed to the
outer loop in the main algorithm for repair. Otherwise, the
original set minus the next hyperedge is tested. The process
of excluding hyperedges is then repeated for pairs, triples, and
so on, until a satisfiable set of expressions is identified. In the
worst case, the function is exponential in the number of edges
in the current repair context. The following example illustrates
how the function works.
Example 5.7: Consider the example in Figure 1 and two
new DCs:
c7 : ¬(L(l, f, n, r, d, y, c, m, s), (r = “B”), (d > 4))
c8 : ¬(L(l, f, n, r, d, y, c, m, s), (y > 7), (d < 6))
That is, an employee after 8 years should have at least 6
extra days off, and an employee of rank “B” cannot have
more than 4 days. Given t2 [DO] as input by the MVC,
L OOK U P processes the two edges over it and collects the
repair expressions t2 [DO] ≤ 4 from c7 and t2 [DO] ≥ 6 from
c8 . The satisfiability test fails (x ≤ 4 ∧ x ≥ 6) and the
computeSatisfiable function starts removing expressions from
the RC, in order to maximize the set of satisfiable constraints.
In this case, it removes c7 from the RC and sets t2 [DO] = 6
to satisfy c8 . Violation for c7 is pushed to the outer loop, and,
as in the new MVC there are no new cells involved, the post
processing step updates t2 [RNK] to a fresh value.

2) Function getAssignments: After getting the maximum
number of solvable expressions, the following step aims at
computing an optimal repair according to the cost model at

465

3 We

assume all numerical values to be integer for simplicity

hand. We therefore distinguish between string typed expressions and numerical typed expressions for both cost models:
cardinality minimality and distance minimality
String Cardinality Minimality. In this case we want to
minimize the number of cells to change. For string type,
expressions consist only of = and 6=, thus we create a mapping
from each candidate value to the occurrence frequency (VFM).
The value with biggest frequency count will be chosen.
Example 5.8: Consider a schema R(A, B) with 5 tuples t1 = R(a, b), t2 = R(a, b), t3 = R(a, cde), t4 =
R(a, cdf ), t5 = R(a, cdg). R has an F D : A → B.
Suppose now we have an RC with set of expressions t1 [B] =
t2 [B] = t3 [B] = t4 [B] = t5 [B]. VFM is created with
b → 2, cde → 1, cdf → 1, cdg → 1. So value b is chosen.

String Distance Minimality. In this case we want to
minimize the string edit distance. Thus we need a different
VFM, which maps from each candidate value to the edit
distance if this value were to be chosen.
Example 5.9: Consider the same database as Example 5.8.
String cardinality minimality is not necessarily string distance
minimality. Now VFM is created as follows: b → 12, cde →
10, cdf → 10, cdg → 10. So any of cde, cdf, cdg can be
chosen.

Numerical Distance Minimality. In this case we want to
minimize the squared distance. QP is our determination core.
In particular, we need to solve the following objective function:
for each cell with value v involved in a predicate of the DC,
a variable x is added to the function with (x − v)2 . The
expressions in the RC are transformed into constraints for the
problem by using the same variable of the function. As the
objective function given as a quadratic has a positive definite
matrix, the quadratic program is efficiently solvable [19].
Example 5.10: Consider a schema R(A, B, C) with a tuple
t1 = R(0, 3, 2) and the two repair expressions: r1 : R[A] <
R[B] and r2 : R[B] < R[C]. To find valid assignments, we
want to minimize the quadratic objective function (x − 0)2 +
(y−3)2 +(z−2)2 with two linear constraints x < y and y < z,
where x, y, z will be new values for t1 [A], t1 [B], t1 [C]. We
get solution x = 1, y = 2, z = 3 with the value of objective
function being 3.

Numerical Cardinality Minimality. In this case we want
(i) to minimize the number of changed cells, and (ii) to
minimize the distance for those changing cells. In order
to achieve cardinality minimality for numerical values, we
gradually increase the number of cells that can be changed
until QP becomes solvable. For those variables we decide
not to change, we add constraint to enforce it to be equal to
original values. It can be seen that this process is exponential
in the number of cells in the RC.
Example 5.11: Consider the same database as in Example
5.10.Numerical distance minimality is not necessary numerical
cardinality minimum. It can be easily spotted that x = 0, y =
1, z = 2 whose squared distance is 4 only has one change,
while x = 1, y = 2, z = 3 whose squared is 3 has three
changes.


VI. O PTIMIZATIONS AND E XTENSIONS
In this section, we briefly discuss two optimization techniques adopted in our system, followed by two possible extensions that may be of interest to certain application scenarios.
Detection Optimization. Violation detection for DCs
checks every possible grounding of predicates in denial constraints. Thus improving the execution times for violation
detection implies reducing the number of groundings to be
checked. We face the issue by verifying predicates in a order
based on their selectivity. Before enumerating all grounding
combinations, predicates with constants are applied first to rule
out impossible groundings. Then, if there is an equality predicate without constants, the database is partitioned according
to two attributes in the equality predicate, so that grounding
from two different partitions need not to be checked. Consider
for example c3 . The predicate (r∗ 6= ‘M 0 ) is applied first to
rule out grounding with attribute r∗ equals M . Then predicate
(l = m0 ) is chosen to partition the database, so groundings
with values of attributes l and m0 not being in the same
partition will not be checked.
Hypergraph Compression. The conflict hypergraph provides a violation representation mechanism, such that all
information necessary for repairing can be collected by the
L OOK U P module. Thus, the size of the hypergraph has an
impact on the execution time of the algorithm. We therefore
reduce the number of hyperedges without compromising the
repair context by removing redundant edges. Consider for
example a table T (A, B) with 3 tuples t1 = (a1, b1), t2 =
(a1, b2), t3 = (a1, b3) and an FD: A → B; it has three
hyperedges and three expressions in the repair context, i.e.,
t1 [B] = t2 [B], t1 [B] = t3 [B], t2 [B] = t3 [B]. However, only
two of them are necessary, because the expression for the third
hyperedge can be deduced from the first two.
Custom Repair Strategy. The default repair strategy can
easily be personalized with a user interface for the L OOK U P
module. For example, if a user wants to enforce the increase
of the salary for the NYC employee in rule c2 , she just
needs to select the s variable in the rule. An alternative
representation of the rule can be provided by sampling
the rule with an instance on the actual data, for example
¬(G(386, M ark, Lee, E, N Y C, 552, AZ, 75), G(M ark,
W hite, E, SJ, 639, CA, 80), (80 > 75)), and the user highlights the value to be changed in order to repair the violation.
We have shown how repair expressions can be obtained
automatically for DCs. In general, the Repair function can be
provided for any new kind of constraints that is plugged to
the system. In case the function is not provided, the system
would only detect violating cells with the Detect module. The
iterative algorithm will try to fix the violation with repair
expressions from other interacting constraints or, if it is not
possible, it will delay its repair until the post-processing step.
Manual Determination. In certain applications, users may
want to manually assign values to dirty cells. In general, if a
user wants to verify the value proposed by the system for a
repair, and eventually change it, she needs to analyze what are

466

the cells involved in a violation. In this scenario, the RC can
expose exactly the cells that need to be evaluated by the user
in the manual determination. Even more importantly, the RC
contains all the information (such as constants assignments
and expressions over variables) that lead to the repair. In the
same fashion, fresh values added in the post processing step
can be exposed to the user with their RC for examination and
manual determination.
VII. E XPERIMENTAL S TUDY
The techniques have been implemented as part of the
NADEEF data cleaning project at QCRI4 and we now present
experiments to show their performance. We used real-world
and synthetic data to evaluate our solution compared to
state-of-the-art approaches in terms of both effectiveness and
scalability.
A. Experimental Settings
Datasets. In order to compare our solution to other approaches we selected three datasets.
The first one, HOSP, is from US Department of Health &
Human Services [1]. HOSP has 100K tuples with 19 attributes
and we designed 9 FDs for it. The second one, CLIENT [4],
has 100K tuples, 6 attributes over 2 relations, and 2 DCs
involving numerical values. The third one, EMP, contains
synthetic data and follows the structure of the running example
depicted in Figure 1. We generated up to 100K tuples for
the 17 attributes over 2 relations. The DCs are c1 , . . . , c6 as
presented in the paper.
Errors in the datasets have been produced by introducing
noise with a certain rate, that is, the ratio of the number of
dirty cells to the total number of cells in the dataset. An error
rate e% indicates that for each cell, there is a e% probability
we are going to change that cell. In particular, we update the
cells containing strings by randomly picking a character in the
string, and change it to “X”, while cells with numerical values
are updated with randomly changing a value from an interval.5
Algorithms. The techniques presented in the paper have
been implemented in Java. As our holistic Algorithm 1 is
modular with respect to the cost function that the user wants
to minimize, we implemented the two semantics discussed
in Section V-D. In particular we tested the getAssigment
function both for cardinality minimality (RC-C) and for the
minimization of the distance (RC-D).
We implemented also the following algorithms in Java: the
FD repair algorithms from [5] (Sample), [6] (Greedy), [18]
(VC) for HOSP; and the DC repair algorithm from [4] (MWSC)
for CLIENT. As there is no available algorithm able to repair
all the DCs in EMP, we compare our approach against a
sequence of applications of other algorithms (Sequence). In
particular, we ran a combination of three algorithms: Greedy
for DCs c1 , MWSC for c2 , c4 , c5 , c6 , and a simple, ad-hoc
algorithm to repair c3 as it is not supported by any of the
existing algorithms. In particular, for c3 we implemented a
4 http://da.qcri.org/NADEEF/
5 Datasets

can be downloaded at http://da.qcri.org/hc/data.zip

simplified version of our Algorithm 1, without MVC and
with violations fixed one after the other without looking at
their interactions. As there are six alternative orderings, we
executed all of them for each test and picked the results from
the combination with the best performance. For ≈t we used
string edit distance with t = 3: two strings were considered
similar if the minimum number of single-character insertions,
deletions and substitutions needed to convert a string into the
other was smaller than 4.
Metrics. We measure performance with different metrics,
depending on the constraints involved in the scenario and on
the cost model at hand. The number of changes in the repair is
the most natural measure for cardinality minimality, while we
use the cost function in Section III-B to measure the distance
between the original instance and its repair. Moreover, as the
ground truth for these datasets is available, to get a better
insight about repair quality we measured also precision (P ,
corrected changes in the repair), recall (R, coverage of the
errors introduced with e%), and F-measure (F = 2 × (P ×
R) (P + R)). Finally, we measure the execution times needed
to obtain a repair.
As in [5], we count as correct changes the values in the
repair that coincide with the values in the ground truth, but
we count as a fraction (0.5) the number of partially correct
changes: changes in the repair which fix dirty values, but their
updates do not reflect the values in the ground truth. It is
evident that fresh values will always be part of the partially
correct changes.
All experiments were conducted on a Win7 machine with a
3.4GHz Intel CPU and 4GB of RAM. Gurobi Optimizer 5.0
has been used as the external QP tool [16] and all computations
were executed in memory. Each experiment was run 6 times,
and the results for the best execution are reported. We decided
to pick the best results instead of the average in order to favor
Sample, which is based on a sampling of the possible repairs
and has no guarantee that the best repair is computed first.
B. Experimental Results
We start by discussing repair quality and scalability for each
dataset. Depending on the constraints in the dataset, we were
able to use at least two alternative approaches. We then show
how the algorithms can handle a large number of constraints
holistically. Finally, we show the impact of the MVC on our
repairs.
Exp-1: FDs only. In the first set of experiments we show
that the holistic approach has benefits even when the constraints are all of the same kind, in this case FDs. As in this
example all the alternative approaches consider some kind of
cardinality minimality as a goal, we ran our algorithm with the
getAssigment function set for cardinality minimality (RC-C).
Figures 4(a-c) report results on the quality of the repairs
generated for the HOSP data with four systems. Our system
clearly outperforms all alternatives in every quality measure.
This verifies that holistic repairs are more accurate than alternative fixes. The low values for the F-measure are expected:
even if the precision is very high (about 0.9 for our approach

467

(a) HOSP # of changes

(b) HOSP F-measure

(c) HOSP % errors

(d) HOSP Exec. time

(e) C LIENT # of changes

(f) C LIENT Distance

(g) C LIENT % errors

(h) C LIENT Exec. time

(i) E MP # of changes

(j) E MP Distance

(k) E MP % errors

(l) E MP Exec. time

Fig. 4: Experimental results for the data cleaning problem.

on 5% error rate), recall is always low because many randomly
introduced error cannot be detected. Consider for example
R(A,B), with an FD: A → B, and two tuples R(1,2), R(1,3).
An error introduced for a value in A does not trigger a
violation, as there is not match in the left hand side of the
FD, thus the erroneous value cannot be repaired.
Figure 4(c) shows the number of cells changed to repair
input instances (of size 10K tuples) with increasing amounts of
errors. The number of errors increases when e% increases for
all approaches; however, RC-C benefits of the holism among
the violations and is less sensitive to this parameter.
Execution times are reported in Figure 4(d), we set a timeout
of 10 minutes and do not report executions over this limit. We
can notice that our solution competes with the fastest algorithm
and scales nicely up to large databases. We can also notice that
VC does not scale to large instances due to the large size of
their hypergraph, while our optimizations effectively reduces
the number of hyperedges in RC-C.
Exp-2: DCs with numerical values. In the experiment for
C LIENT data, we compare our solution against the state-of-theart for the repair of DCs with numerical values (MWSC) [4].
As MWSC aims at minimizing the distance in the repair, we
ran the two versions of our algorithm (RC-C and RC-D).
Figures 4(e-f) show that RC-C and RC-D provide more
precise repairs, both in terms of number of changes and

distance, respectively. As in Exp-1, the holistic approach
shows significant improvements over the state-of-the-art even
with constraints of the same kind only, especially in terms
of cardinality minimality. This can be observed also with
data with increasing amount of errors in Figures 4(g). Notice
that RC-C and RC-D have very similar performances for this
example. This is due to the fact that the dataset was designed
for MWSC, which supports only local DCs. For this special
class the cardinality minimization heuristic is not needed in
order to obtain minimality. However, Figure 4(g) shows that
the overhead in execution time for RC-C is really small and the
execution times for our algorithms is comparable to MWSC.
Exp-3: Heterogeneous DCs. In the experiments for the
EMP dataset, we compare RC-C and RC-D against Sequence.

In this dataset we have more complex DCs and, as expected,
Figures 4(i) and 4(k) show that RC-C performs best in terms
of cardinality minimality. Figure 4(j) reports that both RCC and RC-D perform significantly better than Sequence in
terms of Distance cost. We observe that all approaches had low
precision in this experiment: this is expected when numerical
values are involved, as it is very difficult for an algorithm
to repair a violation with exactly the correct value. Imagine
an example with value x violating x > 200 and an original,
correct value equals to 250; in order to minimize the distance
from the input, value x is assigned 201 and there is a

468

significant distance w.r.t. the true value.
Execution times in Figure 4(l) show that the three algorithms
have the same time performances. This is not surprising, as
they share the detection of the violations which is by far the
most expensive operation due to the presence of a constraint
with three atoms (c3 ). The cubic complexity for the detection
of the violations clearly dominates the computation. Techniques to improve the performances for the detection problem
are out of the scope of this work and are currently under
study in the context of parallel computation on distributed
infrastructures [17].

Datasets used in the experimental evaluation fit in the main
memory, but, in case of larger databases, it may be needed
to put the hypergraph in secondary memory and revise the
algorithms to make scale in the new setting. This is a technical
extension of our work that will be subject of future studies.
Another subject worth of future study is how to automatically
derive denial constraints from data, similarly to what has been
done for other quality rules [7], [15], since experts designed
constraints are not always readily available.
Finally, Repair Context can encode any constraint defined
over constants and variables, thus opening a prospective beyond binary predicates. We believe that by enabling mathematical expressions and aggregates in the constraints we can
make a step forward the goal of bridging the gap between the
procedural business rules, used in the enterprise settings, and
the declarative constraints studied in the research community.
R EFERENCES

(a) HOSP # of DCs

(b) MVC vs Order (log. scale)

Fig. 5: Results varying the number of constraints and the
ordering criteria in Algorithm 1.
Exp-4: Number of Rules. In order to test the scalability
of our approach w.r.t. the number of constraints, we generated
DCs for the HOSP dataset and tested the performance of the
system. New rules have been generated as follows: randomly
take one FD c from the original constraints for HOSP, one
of its tuples t from the ground truth, and create a CFD c0 ,
such that all the attributes in c must coincide with the values
in t (e.g., c0 : Hosp[Provider#=10018] → Hosp[Hospital=“C.
E. FOUNDATION”]). We then generated an instance of 5k
tuples with 5% error rate and computed a repair for every
new set of DCs. For each execution, we increased the number
of constraints as input. The results in Figure 5a verifies that
the execution times increase linearly with the number of
constraints.
Exp-5: MVC contribution. In order to show the benefits of
MVC on the quality of repair, we compared the use of MVC
to identify conflicting cells versus a simple ordering based
on the number of violations a cell is involved (Order). For
the experiment we used datasets with 10k tuples, 5% error
rate and RC-C. Results are reported in Figure 5b. For the
hospital dataset the number of changes is almost the double
with the simple ordering (3382 vs 1833), while the difference
is smaller for the other two experiments because they show
fewer interactions between violations.
VIII. C ONCLUSIONS AND F UTURE W ORK
Existing systems for data quality handle several formalisms
for quality rules, but do not combine heterogeneous rules
neither in the detection nor in their repair process. In this work
we have shown that our approach to holistic repair improves
the quality of the cleaned database w.r.t. the same database
treated with a combination of existing techniques.

[1] http://www.hospitalcompare.hhs.gov/, 2012.
[2] R. Agrawal, T. Imieliński, and A. Swami. Mining association rules
between sets of items in large databases. SIGMOD Rec., 22(2), 1993.
[3] C. Batini and M. Scannapieco. Data Quality: Concepts, Methodologies
and Techniques. Springer, 2006.
[4] L. Bertossi, L. Bravo, E. Franconi, and A. Lopatenko. Complexity and
approximation of fixing numerical attributes in databases under integrity
constraints. In DBPL, 2005.
[5] G. Beskales, I. F. Ilyas, and L. Golab. Sampling the repairs of functional
dependency violations under hard constraints. PVLDB, 3(1), 2010.
[6] P. Bohannon, W. Fan, M. Flaster, and R. Rastogi. A cost-based model
and effective heuristic for repairing constraints by value modification.
In SIGMOD, 2005.
[7] F. Chiang and R. J. Miller. Discovering data quality rules. PVLDB,
1(1), 2008.
[8] G. Cong, W. Fan, F. Geerts, X. Jia, and S. Ma. Improving data quality:
Consistency and accuracy. In VLDB, 2007.
[9] W. W. Eckerson. Data quality and the bottom line: Achieving business
success through a commitment to high quality data. The Data Warehousing Institute, 2002.
[10] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios. Duplicate record
detection: A survey. TKDE, 19(1), 2007.
[11] W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis. Conditional functional
dependencies for capturing data inconsistencies. TODS, 33(2), 2008.
[12] W. Fan, X. Jia, J. Li, and S. Ma. Reasoning about record matching
rules. PVLDB, 2(1), 2009.
[13] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Interaction between record
matching and data repairing. In SIGMOD Conference, 2011.
[14] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Towards certain fixes with
editing rules and master data. VLDB J., 21(2), 2012.
[15] L. Golab, H. J. Karloff, F. Korn, D. Srivastava, and B. Yu. On generating
near-optimal tableaux for conditional functional dependencies. PVLDB,
1(1), 2008.
[16] Gurobi. Gurobi optimizer reference manual, 2012.
[17] T. Kirsten, L. Kolb, M. Hartung, A. Gross, H. Köpcke, and E. Rahm.
Data partitioning for parallel entity matching. In QDB, 2010.
[18] S. Kolahi and L. V. S. Lakshmanan. On approximating optimum repairs
for functional dependency violations. In ICDT, 2009.
[19] M. Kozlov, S. Tarasov, and L. Khachiyan. Polynomial solvability of
convex quadratic programming. USSR Computational Mathematics and
Mathematical Physics, 20(5), 1980.
[20] A. Lopatenko and L. Bravo. Efficient approximation algorithms for
repairing inconsistent databases. In ICDE, pages 216–225, 2007.
[21] M. Yakout, A. K. Elmagarmid, J. Neville, M. Ouzzani, and I. F. Ilyas.
Guided data repair. PVLDB, 4(5), 2011.

469

