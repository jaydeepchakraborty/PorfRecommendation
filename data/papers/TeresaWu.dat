IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 35, NO. 4, APRIL 2016

1127

Efﬁcient Small Blob Detection Based on Local
Convexity, Intensity and Shape Information
Min Zhang, Member, IEEE, Teresa Wu*, Scott C. Beeman, Luise Cullen-McEwen, John F. Bertram, Jennifer R.
Charlton, Edwin Baldelomar, and Kevin M. Bennett

Abstract—The identiﬁcation of small structures (blobs) from
medical images to quantify clinically relevant features, such as
size and shape, is important in many medical applications. One
particular application explored here is the automated detection of
kidney glomeruli after targeted contrast enhancement and magnetic resonance imaging. We propose a computationally efﬁcient
algorithm, termed the Hessian-based Difference of Gaussians
(HDoG), to segment small blobs (e.g. glomeruli from kidney) from
3D medical images based on local convexity, intensity and shape
information. The image is ﬁrst smoothed and pre-segmented into
small blob candidate regions based on local convexity. Two novel
3D regional features (regional blobness and regional ﬂatness)
are then extracted from the candidate regions. Together with
regional intensity, the three features are used in an unsupervised
learning algorithm for auto post-pruning. HDoG is ﬁrst validated
in a 2D form and compared with other three blob detectors
from literature, which are generally for 2D images only. To test
the detectability of blobs from 3D images, 240 sets of simulated
images are rendered for scenarios mimicking the renal nephron
distribution observed in contrast-enhanced, 3D MRI. The results
show a satisfactory performance of HDoG in detecting large
numbers of small blobs. Two sets of real kidney 3D MR images
(6 rats, 3 human) are then used to validate the applicability of
HDoG for glomeruli detection. By comparing MRI to stereological
measurements, we verify that HDoG is a robust and efﬁcient
unsupervised technique for 3D blobs segmentation.
Index Terms—Kidney, machine learning, quantiﬁcation and estimation, segmentation, shape analysis.

I. INTRODUCTION
EGMENTING many small structures is often important
in medical imaging analysis as it labels regions of interest
from which quantitative measures (e.g., size, shape) can be

S

Manuscript received November 11, 2015; revised December 10, 2015; accepted December 10, 2015. Date of publication December 17, 2015; date of
current version March 31, 2016. This work was supported by NIH DK091722
and a grant from the NIH Diabetic Complications Consortium. Asterisk indicates corresponding author.
M. Zhang is with Department of Radiology, Mayo Clinic, Scottsdale, AZ
85259 USA (e-mail: zhang.min@mayo.edu).
*T. Wu is with School of Computing, Informatics, and Decision Systems
Engineering, Arizona State University, Tempe, AZ 85287 USA (e-mail: teresa.
wu@asu.edu).
S. C. Beeman is with School of Medicine, Washington University in St. Louis,
St. Louis, MO 63130 USA (e-mail: beemans@mir.wustl.edu).
L. Cullen-McEwen and J. F. Bertram are with Department of Anatomy and
Developmental Biology, School of Biomedical Sciences, Monash University,
Melbourne, Victoria, 3800 Australia (e-mail: luise.cullen-mcewen@monash.
edu; john.bertram@monash.edu).
J. R. Charlton is with Department of Pediatrics, University of Virginia Medical Center, Charlottesville, VA 22903 USA (e-mail: jrc6n@hscmail.mcc.virginia.edu).
E. Baldelomar and K. M. Bennett are with Department of Biology, University
of Hawaii at Manoa, Manoa, HI 96822 USA (e-mail: ebaldelo@hawaii.edu;
kevinben@hawaii.edu).
Color versions of one or more of the ﬁgures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TMI.2015.2509463

drawn for disease diagnosis, prognosis and staging. Examples of small structures may include cells or cell nuclei in
histopathology/ﬂuoroscopic images [1] or MR images [2], exudative lesions in retinal images [3], breast lesions in ultrasound
images [4], and glomeruli in contrast-enhanced MR images
of the kidney [5]–[7]. One common observation is that small
structures preserve local homogenous imaging properties such
as convexity of the intensity function and shape. In the ﬁeld
of computer vision, the problem of detecting such structures is
known as blob detection.
Detecting blobs from medical images is challenging due to
issues including but not limited to 1) large image intensity variation around the blobs, 2) clustering of the blobs and ambiguous
boundaries, and 3) imaging artifacts (e.g. signal noise). Extensive work has been done to overcome these issues. One classic
approach is the Laplacian of Gaussian (LoG) [8]. The LoG is
based on scale-space theory. In scale-space theory, a 2D image
or a slice of a 3D image is treated as part of a stack of images controlled by a scale parameter . A multi-scale Gaussian
scale-space representation of the image is derived as the convolution of the raw image over the Gaussian kernel with respect
to , preserving the key spatial properties of the imaged structures [9]. When increases, the number of local minima in a
dark blob does not increase and the number of local maxima in
a bright blob does not decrease, so a diffusion process can be
employed to identify the blobs. For similarly sized blobs, one
“optimal” scale exists. If the image has blobs with a large range
of sizes, multiple values of may apply. Detectors based on LoG
kernels have had some success in detecting symmetric blobs
[8] but are known to have limitations in identifying rotationally
asymmetric blobs. The generalized Laplacian of the Gaussian
(gLoG) [10] was developed to detect rotationally asymmetric
structures by employing multiple asymmetric Gaussian kernels.
Both detectors pinpoint the centroid of the blob, and a regular ellipse with an estimated radius over the centroid is superimposed on the images to obtain the required measurements.
Since the convolution kernels (e.g. LoG, gLoG) are set globally for the whole image, it is very likely that the local noise is
not fully attenuated by a global ﬁlter, which would create unnecessary local extrema. For images with considerable noise,
(e.g., MRI), and relatively small (or even tiny) blobs, the performance of these types of detectors may deteriorate due to local
extrema in the convolution response map, which was empirically shown in [1]. Therefore, post-pruning is usually required
for improved performance of this type of detector. Most recently, the Hessian-based Laplacian of Gaussian (HLoG) [1]
was proposed to increase the detectability of small blobs from
noisy medical images, since it used regional features that are tol-

0278-0062 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

1128

erant to local noise for post-pruning. However, all the detectors
reviewed above are restricted to 2D images due to the computational burden of the multi-scale LoG transformation.
Here we focus on the speciﬁc problem of detecting contrast-labeled kidney glomeruli from 3D MR images. The
kidney is a life-sustaining organ with an enormous ability to
adapt structurally and functionally to meet the needs of each
individual across a multitude of environments. The nephron is
the individual ﬁltering unit of the kidney and is comprised of
a glomerulus, a tuft of fenestrated capillaries responsible for
ﬁltering the blood that enters from the afferent arteriole, and
an attached tubule, which reabsorb certain constituents of the
blood prior to urinary excretion. A decline in renal function, a
late surrogate of reduced nephron mass, is strongly associated
with signiﬁcant cardiovascular morbidity and mortality, even
in the earliest stages of chronic kidney disease [11], [12].
Although clinicians can measure glomerular ﬁltration rate
and this will decline with progressive chronic kidney disease,
this assessment is a measure of the total surface area of all
glomeruli and cannot account for glomerular hypertrophy that
occurs as an adaptive mechanism to low nephron number. To
date, the standard methods for nephron enumeration are post
mortem, destructive techniques that only estimate glomerulus
number [13]–[15]. It is highly desirable to have a clinically
translatable method to count a living individuals nephron mass
to evaluate kidney morphology and function. Recent advances
in MRI and contrast agents makes the detection of glomeruli in
vivo feasible, which could transform kidney disease diagnosis
using cationic-ferritin enhanced (CFE) MRI [5], [7], [16]–[18].
In CFE-MRI, a cationic, magnetic ferritin-based nanoparticle
is intravenously injected. The agent binds temporarily to the
basement membrane of the renal glomerulus, allowing in vivo
and ex vivo detection and counting of individual glomeruli
and nephrons. This approach requires 3D detection and measurement of each glomerulus in the MR image, in some ways
similar to computational problem in pattern recognition in other
systems. However, there is especially challenging for glomeruli
because they are very small and have a high spatial frequency
close to that of the image noise. Furthermore, glomerulus
detection requires considerable computational effort due to
the MRI resolution; only highly efﬁcient detectors are suitable
for high-throughput in vivo studies leading to the potential for
clinical applications.
In this research we propose the Hessian-based Difference of
Gaussians (HDoG) that incorporates local convexity, intensity
and shape information to efﬁciently detect blobs in 3D images.
In HDoG, we ﬁrst apply the Difference of Gaussians (DoG)
transformation to the raw image to quickly smooth out local
noise and enhance blob structures. Hessian-based convexity
analysis is then conducted to pre-segment and delineate the
blob candidate regions with the same local convexity. Since the
Hessian analysis may also identify false blobs (e.g. artifacts),
post-pruning is necessary to remove the false identiﬁcation.
To achieve this, two novel 3D regional local shape features
with fast computation are introduced: regional blobness
and regional ﬂatness
. The average image intensity [5],
[7], is also introduced. These three features are derived from
each blob candidate region and fed into a tuning-free, unsupervised clustering algorithm—the variational Bayesian Gaussian

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 35, NO. 4, APRIL 2016

mixture model (VBGMM) [19] as a post-pruning process.
Since most blob detectors from existing literature are for 2D
images, we modiﬁed HDoG to identify the 2D version of the
regional features and compared HDoG-2D with LoG, gLoG
and HLoG to detect cells in 15 pathological images and 200
ﬂuorescence microscopy images. We observe that, while comparable to HLoG-2D and gLoG (most cases, see Section IV-A),
HDoG-2D outperformed LoG and had the least computing
time, as expected. Next, we comprehensively evaluated the use
of HDoG to detect 3D blobs. Due to a lack of 3D datasets in
the literature for comparison, we generated simulated images
with known ground truth as a ﬁrst validation. 240 sets of
simulated images (each set has 10 images, total 2400 images)
were created with 12 different blob sizes and 20 different blob
counts mimicking the kidney glomerulus distributions. After
observing satisfactory performance, we studied two sets of real
kidney MR and compared the results from HDoG against the
stereological estimates. The two sets of kidney MR images
were: six rat kidney MR images of matrix size
(each glomerulus is sized 1.5 2.5 voxels in radius) and three
(each
human kidney MR images of matrix size
glomerulus is sized 1.5 2.5 voxels in radius). Stereological
estimates of glomerulus number, (considered the gold standard
histological approach), were available for both datasets for
comparison. Based on the results from both 2D (HDoG-2D)
and 3D (HDoG) experiments, we conclude that the HDoG
detector is able to automatically and accurately segment small
blobs.
In summary, the contributions of this research were threefold: (1) We develop a validated efﬁcient 3D blob detector
(HDoG) using local convexity, intensity and shape information;
(2) In HDoG, we employ the DoG to approximate LoG convolution to successfully address the computational challenges
of blob detection from 3D images; and (3) We develop the
two novel 3D shape features of regional blobness and regional
ﬂatness to assist 3D blob detection. Fast calculation of the
features is provided. The remainder of the paper is organized
as follows: Section II discusses the acquisition of the image
data. Section III describes our proposed HDoG in detail, and
Section IV demonstrates comparison experiments on 2D medical images and validation experiments on 3D synthetic data.
Two 3D real kidney MR image datasets of different image
sizes are evaluated in Section V, followed by the discussion of
the computational performance and novel regional features of
HDoG. The conclusions are drawn in Section VI.
II. IMAGE DATA ACQUISITION
Five image datasets, including two sets of 2D images, one set
of 3D synthetic images, and two sets of 3D MR kidney images,
were used in this paper to test the performance of the proposed
algorithm.
A. 2D Pathological and Fluorescent Microscopic Images
15 pathological images with image size 600 800 were obtained from [10] and 200 synthetic ﬂuorescent microscopic images with image size 256 256 were obtained from [20].

ZHANG et al.: EFFICIENT SMALL BLOB DETECTION BASED ON LOCAL CONVEXITY, INTENSITY AND SHAPE INFORMATION

Fig. 1. Slice 100 (of 256) from Simulated 3D Blob Images with Different Parameter Settings ranging from least dense to the densest scenario: (A) 3D Blob
and
. (B) 3D Blob Image with
and
Image with
. (C) 3D Blob Image with
and
. (D) 3D Blob
and
.
Image with

B. 3D Synthetic Image Data
Based on the kidney glomerulus distribution from MRI, we
rendered 240 simulated scenarios with 12 different sizes and 20
different quantities of Gaussian blobs. For each scenario, ten
3D images were randomly generated, yielding a total of 2400
3D images. Speciﬁcally, a 3D Gaussian function was used to
generate blobs with size controlled by Gaussian parameter .
The radius of the blob was estimated by
voxels by
observation. Then
identical blobs at size were randomly
spread and stacked in a 3D image with matrix size
. The 240 scenarios were simulated based on the blob size
ranging from 0.5 to 6 with step size 0.5, and the number of blobs
ranging from 1,000 to 96,000 with step size 5,000. As shown
in Fig. 1, when the size and the number of blobs were small,
the blobs were sparse and well separated from each other (e.g.,
Fig. 1(a)). When the size and the number of blobs was large, the
blobs were dense and were highly likely to be clumped together
(e.g., Fig. 1(d)).
C. 3D Rat and Human Image Data
1) Rat Kidney Data: All animal experiments were approved
by the Institutional Animal Care and Use Committee. Six rat
kidney glomeruli were magnetically labeled using the cationic
ferritin (CF) nanoparticle, derived from horse spleen ferritin
(Sigma Aldrich, St. Louis, MO). CF is a natural nanoparticle
proposed as a nontoxic contrast agent for renal imaging [5], [7].
CF was synthesized by the method in [21] with some modiﬁcation described in [7]. Male Sprague Dawley Rats (215–245 g)
were anesthetized using inhaled isoﬂurane and given three intravenous bolus injections of CF, (5.75 mg/100 g total in phosphate

1129

buffered saline), spaced 1.5 h apart. The rats were then euthanized under anesthesia by transcardial perfusion of saline followed by 10% formalin. Kidneys were removed and stored and
imaged in glutaraldehyde. MRI was performed using a Varian
(Agilent, Palo Alto, CA) 800 MHz NMR with an 89 mm bore
using a DOTY three-axis imaging gradient set. A 3D gradientecho image was acquired with
and a resolution of
. 3D images were reconstructed with
image size
.
2) Human Kidney Data: Three human donor kidneys, not
suitable for transplant, were obtained from the International Institute for the Advancement of Medicine (IIAM, Edison, NJ,
USA). All kidneys were procured by the organization with informed consent and with the approval of the Internal Review
Board. Kidneys were perfused at cross-clamp with saline and
University of Wisconsin preservative solution, removed, and
stored in the solution on ice for transfer. They were received on
site within 24 hours. Kidneys were ﬁrst perfused with 120 ml
phosphate buffered saline (PBS) and glomeruli were labeled by
intravenous injection of 300 mg of CF. The kidneys were again
ﬂushed with PBS via the renal artery to clear intravascular CF,
ﬁxed with 10% neutral buffered formalin, and stored in formalin
for at least 24 hours. The kidneys were removed and washed
three times in 500 ml PBS over 24 hours prior to MRI. They
were imaged in a sealed plastic container in PBS using a Bruker
7T/35 MRI scanner with a 72 mm quadrature transmit/receive
radiofrequency coil (Bruker, Billerica, MA). Kidneys were imaged with a 3D gradient-recalled echo (GRE) pulse sequence
with
and a 117
isotropic resolution
[5]. The 3 human kidney 3D MR images were reconstructed
with image size
.
III. HESSIAN-BASED DIFFERENCE OF GAUSSIAN DETECTOR
The proposed HDoG detector has four-phase: (1) DoG transformation, (2) Hessian pre-segmentation, (3) local features extraction, and (4) post-pruning for ﬁnal identiﬁcation. Each phase
is discussed below.
A. Phase I: DoG Approximation of LoG Transformation
A blob is a region in an image that is darker (or brighter) than
its surroundings. The convexity of the intensity function within
a blob is consistent. However, image noise may lead to discontinuities in the convexity of the intensity function. The noise must
be smoothed out so the asymptotic convex (or concave) shape
of the blobs is highlighted. The DoG is chosen for this purpose
because (1) it smooths the image noise by enhancing objects at
the selected scale [9], (2) it is a fast approximation of the LoG
ﬁlter highlighting blob structure [8], and (3) compared to LoG,
DoG is computationally efﬁcient and preserves detection accuracy [22].
Let a 3D image be
. The scale-space representation
at point
, with scale parameter , is
the convolution of image
with the Gaussian kernel
:
(1)

1130

Here

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 35, NO. 4, APRIL 2016

is the convolution operator Gaussian kernel:
. The Laplacian of

is:
(2)
Since

, we have:

(3)
To locate an optimal scale for the blobs, similar to [8], we add
-normalization to the LoG detector as the normalized LoG detector
. Thus, the approximation of normalized
LoG is:

(4)
Here is introduced to automatically determine the optimum
scale for the blobs. It is usually set to two. For details on tuning
, refer to [1] .
The major disadvantage of the LoG kernel in computational
efﬁciency is that the LoG kernel is not separable (cannot be decomposed to multiple one dimensional kernels). For a 3D image
with matrix
, the kernel size is
(e.g.,
), the convolution of the LoG kernel has complexity
. In HDoG, since the Gaussian kernel is separable, that is, the 3D image can be convolved by three 1D
Gaussian kernels, the complexity of DoG is
. DoG reduces the complexity of LoG by the power degree of the kernel size.
During the normalized DoG transformation, a dark blob is
converted to a bright blob and vice versa. To avoid confusion,
we subsequently refer to the normalized DoG blob as the “transformed blob”. The following discussion focuses on identifying
the dark blobs that are transformed into bright blobs, (called
“transformed bright blobs”), but the same process applies to
identifying bright blobs (transformed into dark blobs) for other
applications. This is demonstrated by experimental validation
in Section IV-A-2). This normalized DoG transformation underlies the Hessian analysis for the pre-segmentation.
B. Phase II: Hessian Pre-Segmentation
If the image is smoothed by the normalized DoG, for a voxel
in the normalized DoG image
at
scale , the Hessian matrix is:

Since the transformed bright blob is shaped as a concave ellipse, where brightness fades isotropically, every voxel within
the blob is concave elliptical. Therefore, we identify the transformed bright blobs using the following proposition.
Proposition 1: In a normalized DoG-transformed 3D image,
every voxel of a transformed bright blob has a negative deﬁnite
Hessian matrix.
Proof: [23] provides a detailed discussion of the relationship between the eigenvalues and geometric shape. Speciﬁcally,
if voxel
is concave elliptical, all the eigenvalues of
are negative, that is,
.
Since each voxel in the transformed bright blob is concave elliptical, its eigenvalues are all negative and the Hessian matrix
of the voxel is negative deﬁnite.
Proposition 1 provides a necessary but not sufﬁcient property
that a voxel in a transformed bright blob must satisfy. That is,
if a voxel resides in a transformed bright blob, the Hessian matrix of the voxel is negative deﬁnite. However, not every voxel
having a negative deﬁnite Hessian matrix must be within a transformed bright blob. This proposition ensures the construction of
the blob candidate set, (see Deﬁnition 1), which is a superset of
the true blobs with some falsely identiﬁed blobs.
Definition 1: A blob candidate in normalized DoG space
is a 6-connected component of set
, where
is the
binary indicator such that if the voxel
has a negative
deﬁnite Hessian matrix, then
; otherwise,
.
Fast Hessian analysis: As a more computationally efﬁcient alternative to computing the eigenvalues
of
, the deﬁniteness of the Hessian matrix
can be assessed by the three leading principal minors
,
and
. The Hessian matrix is negative deﬁnite if and only if
,
and
. As a result, from Proposition
1 and the Definition 1, we can highlight the voxels belonging
to transformed bright blobs using the three leading principal
minors.
Theoretically, the candidate set will contain all the true blobs
and some false ones. Next, we derive regional features (Phase
III) from the superset and conduct post pruning (Phase IV) to
remove the false blobs.
C. Phase III: Extracting 3D Regional Features
The Hessian describes the second order ellipsoid of the blob
structure and the absolute eigenvalues
of the Hessian
denote the semi-axis lengths of the ellipsoid. Reference [23]
introduced two classic geometric features in blob detection:
,
the likelihood of blobness, and
, ﬂatness (the second-order
structureness). Based on the assumption that
, these are deﬁned as:
(6)
(7)
, for an idealized blob, that is,
,
.
. The higher
is the more
salient the blob is against the local background.
Where

(5)

ZHANG et al.: EFFICIENT SMALL BLOB DETECTION BASED ON LOCAL CONVEXITY, INTENSITY AND SHAPE INFORMATION

Fast Regional Blobness Extraction: To calculate
, the eigenvalues
of the Hessian matrix
are computed for each voxel, requiring
intensive computations. To address this concern, we propose
regional blobness feature motivated by the Hessian-afﬁne
detector in [24].
For the Hessian matrix of each voxel, deﬁned in (5), the
regional Hessian matrix of the DoG-transformed (smoothed)
image is deﬁned as:

1131

every voxel within a candidate blob, the regional Hessian
matrix is negative deﬁnite and
. Thus,

(12)
where
be computed by three 2

, using [25].
2 principal minors of
:

can

(13)
1) Fast Regional Flatness Extraction: We deﬁne regional
ﬂatness as:
(8)
Eq. (8) is the summed Hessian matrices of voxels within the
candidate region . This regional Hessian matrix describes the
second-order distribution of derivatives in the region of the candidate blob. We let
be the eigenvalues of the regional
Hessian matrix, and (6) can be rewritten as:

By substituting the denominator (maximum) with
(average),
(10)
The value of

is calculated by
(11)

Proposition 2:
.
Proof: Since:

retains the property of

that

Based on Arithmetic-Geometric Mean Inequality, we have:

Therefore,
, we conclude that
retains the property
.
The use of
improves the efﬁciency of computation
because we can compute the principal minors rather than the
eigenvalues. Since the Hessian matrix is negative deﬁnite in

(14)
or

(15)
and
reduce the computational cost in (6) and (7)
signiﬁcantly. This is because
and
only calculate trace
, determinant
and 2
2 principal minors
. Additional computation solve the characteristic
polynomial:
(expanded from
) to retain the eigenvalues
of the Hessian
. Secondly,
and
are calculated from
the regional Hessian matrix for each blob candidate region
instead of every voxel. In this research, blob candidates v.s.
voxels is 90 K v.s.16777 K in rat MR images and 1 M v.s. 235
M in human MR images.
D. Phase IV: Auto Post-Pruning
Other than
and
, the third feature
, the average
intensity of region , (commonly used in the literature as
post pruning thresholding feature) [5], [7], is computed. We
then input these three features into an unsupervised variational
Bayesian Gaussian Mixture Model (VBGMM) [19] to remove
false positive identiﬁcations from the blob candidate set. The
VBGMM is more robust than maximum likelihood Gaussian
Mixture Models because it treats parameters like mean vector
and variance-covariance matrix in Gaussian Mixtures Models
as distributions instead of deterministic values and uses hyper
parameters to control them. This helps avoid the singularity
issues faced by the maximum likelihood Gaussian mixture
models. In addition, unlike other pruning algorithms which are
thresholding based, the VBGMM requires no parameter tuning.
The blob candidate regions form a multivariate Gaussian mixture and therefore are clustered into blob regions and non-blob
regions using Bayesian inference.
IV. VALIDATION EXPERIMENTS
In the literature, ground truth data are usually provided in the
form of the coordinates of the blob centers. Following [1], a blob

1132

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 35, NO. 4, APRIL 2016

candidate is true positive if and only if its center is in a detection pair
for which the corresponding (nearest) ground
truth center has not been paired, and their Euclidean distance
is within the scope of certain diameter . Therefore the
number of true positives
is calculated by (16). Precision,
recall, and F-score are calculated by (17), (18), and (19), respectively:

TABLE I
VALIDATION RESULTS (F-SCORE) ON 15 2D PATHOLOGIC IMAGES

(16)
(17)
(18)
(19)
is the number of ground truth points and is the
where
number of blob candidates. As in [10], [26], is a diameter
threshold and can be set to a positive value
. If is
small, fewer blob candidates will be counted because the distance between the blob candidate centroid and the ground truth
point must be small. If is large, more blob candidates will be
counted because the threshold distance is relaxed. Ideally, the
upper bound of should be set to no more than the diameter of
a blob, since if is too large a neighborhood blob will be miscounted. Since the blob structures may have irregular shapes,
the upper bound of can be set marginally higher than the estimated diameter to take this into account.
A. Validation Tests on 2D Images
To assess the performance of HDoG, we performed initial
tests on 2D images using three evaluation metrics: precision,
recall, and F-score. Three state-of-art detectors, LoG, gLoG and
HLoG, were studied for comparison.
For 2D images, the 2D versions of the regional Hessian matrix
, the regional blobness
, and the regional ﬂatness
are used in [1]:
(20)

(21)
(22)
are eigenvalues of
.
where
1) Dataset 1: 2D Pathological Images: 15 pathological images were used to compare HDoG with the LoG, gLoG and
HLoG methods. Since the average blob candidate diameter is
13.46 pixels, diameter threshold was varied from 0 to 16 to
reﬂect the distance of two adjacent blobs. F-scores are reported
in Table I.
As shown in Table I, the average F-score for HDoG was
higher than for LoG and gLoG, but marginally lower than for
HLoG. The statistical difference between HDoG and LoG was

The normalizing factor
based on prior experiments. Detailed of
the parameter setting please refer to [1]). Paired T-tests were performed
at a signiﬁcance level of 0.05. Symbols in cell means: + statistically
outperformed; = Statistically Comparable;—Statistically Underperformed

signiﬁcant (
,
, two-sample t-test). We thus conclude that HDoG statistically outperforms LoG. The F-score difference between HDoG and gLoG was not signiﬁcant. Although
the F-score of HDoG was lower than HLoG, the difference was
not signiﬁcant. We conclude that HDoG is statistically comparable HLoG. We also note that, even for the 2D images, gLoG
needed 30 seconds per image and HDoG was 5 times faster than
that of gLoG (6 seconds per image) on the same computational
environment (Windows PC with Intel Xeon 2.0 GHz CPU and
32 GB of memory). The computation time of HLoG was 10 seconds per image.
2) Dataset 2: 2D Fluoroscopic Images: The second 2D validation experiment used 200 2D ﬂuorescent microscopy images.
Additional noise was added to the images to test the robustness
of the detectors [20]. Unlike Dataset 1, these images contained
bright blobs rather than dark blobs. Therefore, the data were
converted into images containing small dark structures using
. (We assume
varied from 0 to 1, or we would
have to standardize
to the [0,1] range). The parameter
settings for LoG and gLoG were as suggested in [10]. Similarly
to Dataset 1, we varied the parameter from 0 to 12 (the estimated blob candidate diameter is 9.1 pixels). The F-score
results are summarized in Table II. (Precision and recall are in
Supplementary).
For smaller blobs in this dataset, HDoG statistically outperformed LoG and gLoG on the F-score measure
, and was
statistically comparable to HLoG. We note that HDoG does not
perform as well as LoG and gLoG when is extremely tight
. The reason is that HDoG naturally generates blob regions but not the centroid of the blobs. It is expected the extrema
(from HDoG) is not perfectly spot on the true centroid. When
is small, this difference is more apparent. As for the computing
time, the average processing time for HDoG was 0.9 second/per
image, 1.2 second/per image for HLoG, 10 second/per image for
gLoG, using a Windows PC with Intel Xeon 2.0 GHz CPU and
32 GB of memory.
In summary, initial tests with both 2D datasets showed that
HDoG-2D has the potential for accurate blob detection in 3D

ZHANG et al.: EFFICIENT SMALL BLOB DETECTION BASED ON LOCAL CONVEXITY, INTENSITY AND SHAPE INFORMATION

1133

TABLE II
VALIDATION RESULTS (F-SCORE) ON 200 2D FLUORESCENT IMAGES

The normalizing factor
based on prior experiments. Details of the parameter settings are described in [1]). Paired T-tests were performed at a
signiﬁcance level of 0.05. + Statistically Outperformed; = Statistically Comparable;—Statistically Underperformed

Fig. 2. Contour Plot of F-Scores on Simulated 3D Images with Different Parameter Settings. In terms of blob size and quantity, the yellow shadow area A shows
the scenarios of glomerulus detection on rat kidney images, and yellow shadow area B shows scenarios for glomerulus detection in human kidney images in our
).
cases (based on the estimation of image size

images. It had statistically similar or better performance compared to the 2D detectors from the literature, but was more computationally efﬁcient. This advantage will be more obvious with
the larger sized 3D images.
B. Validation Tests on 3D Synthetic Imaging Data
We further explored how well the HDoG segments blobs in
3D images. 240 scenarios were tested to evaluate the performance of HDoG. For each scenario, the average F-score of the
10 images was calculated. In this set of experiment, since the
blobs were generated by regular Gaussian kernels, to calculate
the F-score, the evaluation parameter is set to be the diameter
of the blobs i.e.
. Results are shown in the contour
plot of Fig. 2. As the blob sizes
increased, the F-score decreased. As the number of blobs
increased, the F-score also
decreased. This may reﬂect that the likelihood of blobs touching
(overlapping) increases, leading to a deteriorated performance
in segmentation. One special case was observed when
,
with a blob of diameter 3, since radius is
and
or 6000. This may be explained by the blob intensity relative to

imaging noise. Detailed numbers for each scenario are listed in
Supplementary.
As seen in Fig. 2, when the blob size varies from 0.5 to 3,
(with diameter ranges from 3 to 13), and the number of blobs
ranges from 1000 to about 40,000, the F-score can be 0.95 or
higher. When the blobs are small and sparse, imaging properties
such as size, convexity, and intensity distribution may be well
preserved, leading to more accurate detection. When the blob
size was above 5.5 (diameter 23 and higher) and the number
of blobs was 81,000 and higher, the F-score was lower than
0.1. This is because a large number of blobs were stacked, as
shown in Fig. 1(d). We conclude that HDoG performs well in
detecting small, and even tiny blobs. When the blob size was
small, even with a large number ( 95000) of blobs, an F-score
of 0.85 can still be achieved. In the present application of segmenting glomeruli from 3D kidney MR image, where the size of
glomeruli is small and the count is high, HDoG should be useful
to identify the glomeruli for clinical applications. In the next
section, we explore the application of HDoG on one rat kidney
dataset and one human kidney dataset. The scenarios of the rat

1134

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 35, NO. 4, APRIL 2016

TABLE III
GLOMERULUS COUNTS FOR SIX RAT KIDNEYS USING THE HDOG,
ACID MACERATION, AND STEREOLOGY METHODS

, Intel Xeon 2.0 GHz CPU and 32 GB of memory. Only 3 CF rats
were counted by stereology

kidney data can be mapped to area “A” highlighted in Fig. 2,
and the scenarios of the human kidney data can be mapped to
area “B” highlighted in Fig. 2.
V. GLOMERULUS SEGMENTATION USING HDOG

we performed stereology in three rats as published in [7].
Acid maceration is more commonly used, but is less accurate.
Therefore we use them both for comparison. From Table III,
we observed that HDoG consistently identiﬁed glomeruli in
all six kidneys with reasonable computation times ( 5 min).
Although there were some differences between the glomerular
counts, the three methods generally agreed well. Fig. 3 shows
the segmentation results on representative axial view slices
of the six rats, where slice 100 (of 256) is shown for rats
CF1, CF2, and CF3, and slice 150 (of 256) is shown for rats
CF4, CF5, and CF6. From the ﬁgure, we see the glomerular
size is very small (radius is less than 2 voxel) and the most
glomeruli were identiﬁed and contoured in green. Ideally, with
all the blobs being Gaussian blobs, the detection capability of
HDoG can reach up to about 0.95 in F-score as the region A
in Fig. 2 (blob number ranging 26,000 to 36,000, and size
). Fig. 3(m) and (n) are a sample of visually checking
on the segmentation results. After comparing count numbers,
visually checking the accuracy and referring to the scenarios in
Fig. 2 A, we concluded that HDoG can automatically identify
glomeruli in rat MR images.

In this section, the applicability of HDoG to segmenting
kidney glomeruli from MRI is studied. We conducted experiments on rat and human kidney 3D MR images separately.
Clinically, there are three major differences between the healthy
rat and human kidney that affect glomerular morphology:
1) Human kidneys contain many more glomeruli than rat
kidney (200,000–2,000,000 compared to 20,000–40,000, in
our simulation, we simulated the cases of 700,000–1,500,000
glomeruli for human comparing to 26,000–36,000 for rat). 2)
Glomeruli in humans are typically larger than those of the rat,
and 3) The human kidney comprises many lobes, each of which
contains a distinct set of glomeruli, nephrons, and medullary
region, while the rat kidney contains only one. Despite these
structural differences and the difference in size, rat models have
been essential to understanding human disease. Two notable
models are the spontaneously hypertensive rat (SHR) model
of human hypertension and the puromycin-induced model of
focal and segmental glomerulosclerosis [27], [28]. Rat models
provide a critical window into the direct correlation between
the microstructural bases of the pathophysiology of disease,
which is typically impossible to observe in humans. Thus, both
rat and human studies are critical to translation of these MRI
techniques to the clinic.
Two sets of real data were studied: six CF-labeled 3D MR
images of rat kidneys and three CF-labeled 3D MR images of
human kidneys. Since there are no labeled blobs (glomeruli) for
those empirical MR images, the blob count obtained by HDoG
is compared with that obtained using a manual acid maceration
method [28] and the disector-fractionator stereological method
[13]. Both methods are established histological techniques for
estimating glomerular number.

We validated the MRI-based glomerular counts using the
physical dissector/fractionator stereological method described
by Cullen-McEwen et al. [14], [27].We applied the HDoG algorithm to each human 3D MR image using the same platform
as for the rats. The results are shown in Table IV.
From Table IV, HDoG consistently counted glomeruli in all
three kidneys with reasonable computing times ( 1 h for CF2
and CF3, 4h for CF1). We again observed discrepancies between the glomerular counts, but our histology experts conﬁrmed that the two methods generally agreed well. Fig. 4 shows
the segmentation results on selected axial-view slices of the
human kidneys. As seen on the ﬁgure, the glomeruli are very
small (radius is less than 2 voxels), and almost all glomeruli
were identiﬁed by HDoG, (contoured in green), and the image
intensity distribution was inhomogeneous across slices. To illustrate the ability of HDoG to segment glomeruli in different
regions, a top slice (slice 100/896) and a middle slice (slice
500/896) are shown. To map the scenarios in Fig. 3, we converted the images from the matrix size
to
, as a result, the relative glomerular quantities
are reduced to the range of 41,000 96,000 to keep the blob density, of which the performance of HDoG could be around 0.85.
Fig. 4(m) and (n) show a sample of visually checking on the
segmentation results from human kidney data. After comparing
count numbers, visually checking the accuracy and referring to
scenarios in Fig. 2 B, we concluded that the HDoG algorithm
can automatically identify glomeruli in human MR images.

A. Dataset 1: Six 3D Rat Kidney MR Images

C. Discussion of Computation Time

Table III shows the glomerular counts obtained with each
method (HDoG, acid maceration, and disector-fractionator
stereology) for the six rat kidneys.
For this experiment we have three stereological counts
missing. Although stereology is most accurate, it is extremely
expensive and time-consuming to perform. For this reason,

Since the computing time on the rat MRI is satisfactory (less
than 5 minutes/image), we focus the discussion on the human
MRI in this section. We observed that the glomerular segmentation of Human CF1 kidney took a much longer time than the
other two. The time is mainly determined by the VBGMM clustering process. We contend the low contrast of CF1 vs. CF2 and

B. Dataset 2: Three 3D Human Kidney MR Images

ZHANG et al.: EFFICIENT SMALL BLOB DETECTION BASED ON LOCAL CONVEXITY, INTENSITY AND SHAPE INFORMATION

1135

Fig. 3. Glomerular segmentation results from 3D MR images of rat kidneys (selected slices presented). (A-C) Slice 100 for rats CF1, CF2, and CF3. (D-F) Slice
150 for rats CF4, CF5, and CF6. (G-I) segmentation results for (A-C), respectively. Identiﬁed glomeruli are contoured in green. (J-L) segmentation results for
(D-F), respectively, where identiﬁed glomeruli are contoured in green. (M) is the zoomed-in region from (A) while (N) is the segmentation result of (M).

Fig. 4. Glomerular segmentation results for 3D MR images of human kidneys (selected slices): (A-C) Original slice 100 for human CF1, CF2, and CF3 kidneys.
(D-F) Slice 500 for human CF1, CF2, CF3 kidneys. (G-I) Identiﬁcation results for (A-C), respectively, where identiﬁed glomeruli are contoured in green. (J-L)
Identiﬁcation results for (D-F), respectively, where identiﬁed glomeruli are contoured in green. (M) is the zoomed-in region from (D) while (N) is the segmentation
result of (M).
TABLE IV
GLOMERULUS COUNTS FOR THREE HUMAN KIDNEYS USING
THE HDOG AND STEREOLOGY TECHNIQUES

, Intel Xeon 2.0 GHz CPU and 32 GB of memory.

CF3 may be the main factor leading to the longer time. To explore the variations of contrast among the three human kidneys,
the intensity distribution of both glomeruli vs. the whole image
is generated and shown in Fig. 5. The -axis of the ﬁgure shows
the bins of absolute value of the features, while the -axis shows
the frequency of the value bins. As seen, the difference of the
distribution modes between the true glomeruli and the image (an
indicator of contrast), highlighted in the ﬁgure, shows that CF1
had the lowest contrast while CF3 had the highest contrast. As

a result, CF1 required a longer time for VBGMM for converged
solution (12,375 seconds). While CF2 has medium contrast, it
has comparable computing time (2366 seconds) to that of CF3
(2088 seconds). This can be explained by the fact that CF2 is
from the hypertensive patient, having fewer perfused glomeruli
and regions of vascular and glomerular sclerosis, coupled with
glomerular hypertrophy in the perfused portions of the kidney
[5]. We suspect that variation in image contrast, either through
normal physiological variation or image acquisition, will impact
computation time.
D. Discussion on Novel Regional Features
A signiﬁcant contributor to HDoG performance is the regional features used in the post-pruning process. Unlike the simulated data, the distributions of the three features in real data are
shown in Fig. 6. Fig. 6(a), (b) and (c) are the distributions for the
true glomeruli, while the (D), (E) and (F) are the distributions
for the non-glomeruli from CF1, CF2 and CF3 respectively. It
is evident that the true glomerular cluster had distinct pattern on
the distributions of average intensity (in blue), regional blobness

1136

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 35, NO. 4, APRIL 2016

Fig. 5. Intensity frequency histograms of glomeruli against whole kidney image from: (A) Human CF1 (B) Human CF2 kidney (C) Human CF3. Frequency range
was [0, 0.6] and the intensity range was [0, 1] in the ﬁgure. Vertical lines indicate the modes of the intensity distribution.

Fig. 6. Frequency histograms of average intensity, regional blobness and regional ﬂatness for human CF1, CF2 and CF3 kidney 3D MR images. (A)-(C) True
glomerular cluster frequency histograms for human CF1, CF2 and CF3 respectively. (D)- (F) Non-glomerular cluster frequency histograms for human CF1, CF2
and CF3 respectively. Frequency range was [0, 0.5] and the -axis range was [0, 1] in the Figure.

(in red) and regional ﬂatness (in green). We note the regional
ﬂatness and regional blobness of true glomeruli are higher in
average value than those of non-glomeruli since the values fall
more frequently at high value bins as shown in Fig. 6. The
true-glomerular cluster also has a lower average intensity compared to the non-glomerular cluster (e.g., background) as expected. This can be explained that in the nanoparticle-enhanced
MR imaging, a glomeruli is shown as a dark ellipse, which has
higher blobness, higher ﬂatness and lower intensity measures.
E. The Limitations of HDoG
While promising, we do observe some limitations of HDoG.
One of these is missed detection and a false positive detection problem (Fig. 7). For example, some glomeruli in the images may have shown discontinuity due to the imaging acquisition artifact. As in our Deﬁnition 1, a blob is a connected region; this discontinuity will lead to the missed detection even
after applying the DoG transformation (smoothing process). A
second possible reason for missed detection may arise from
the post-pruning process, where some true glomeruli may be
clustered into the non-glomerular group (False negatives, also
known as a Type II error). A similar issue from the post-pruning
unsupervised algorithm is the false positive (also known as Type
I error).

To minimize those error, some studies have attempted to segment blobs from 3D images using supervised learning algorithms, for example to detect breast lesions in ultrasound images
[4], to identify coronary calciﬁcations in CT scans [3], etc. However, the supervised learning algorithms require prior knowledge of labeled training blobs. Without prior knowledge being
available, as the case in the glomerular segmentation, supervised classiﬁers do not apply. Though it is possible to have some
glomeruli manually labeled as prior to support the use of supervised classiﬁer, unfortunately, given the large number of blobs
( 30k in the rat and 200,000–2,000,000 in the human kidney),
and the small sized blobs (some even overlapped), manual labeling is labor intensive, and may not be accurate. Therefore,
in this work we focused on unsupervised blob detectors. Future work may incorporate a semi-supervised algorithm [29] requiring partial labeling as training. Additional prior knowledge
inputs, (for example, glomeruli mostly lie on the area of kidney
cortex), may help improve detection accuracy.
VI. CONCLUSION
In this study, we propose a computationally efﬁcient blob
detector, called the Hessian-based difference of Gaussians
(HDoG) detector, to detect 3D glomeruli, which is an instance
of 3D blob from 3D kidney MR images. HDoG has two

ZHANG et al.: EFFICIENT SMALL BLOB DETECTION BASED ON LOCAL CONVEXITY, INTENSITY AND SHAPE INFORMATION

1137

Fig. 7. Glomerular segmentation results for 3D MR images of Rat CF1 kidney and Human CF1 Kidney (part of the slice on Fig.5): (A) Part of Slice for Rat
CF1. (B) Identiﬁcation results for (A), where identiﬁed glomeruli centers are marked in red-cross. (C) Part of Slice for human CF1. (D) Identiﬁcation results for
(C), where identiﬁed glomeruli centers are marked in red-cross. Circles show the error of missed detection while the rectangles show the error of false positive
detection.

main improvements on computational efﬁciency. First, DoG
is employed in the imaging smoothing process to address
the computing challenge. Second, two novel 3D blob shape
features with fast extractions are proposed. We have thoroughly
validated the HDoG with blob detectors from literature in
2D image datasets. In addition, we test the performance of
HDoG on 3D blob detection using 240 synthetic simulated 3D
images. Last, two sets of kidney 3D MR images are studied.
Experimental results indicate that HDoG can automatically
and accurately count and label glomeruli in a reasonable time
frame ( 5 minutes per rat MRI; 4 hours per human MRI). To
our knowledge, this is the ﬁrst report of a robust, unsupervised
technique to detect massive small magnetically labeled structures in both 2D and 3D images. We conclude HDoG may be
a powerful preclinical or clinical tool to noninvasively detect
molecular structures in medical images. It is noted that MR is
of particular interest here as this research is motivated by the
disease mechanism (kidney glomeruli). We want to highlight
that our proposed HDoG is generalized algorithm that uses
regional geometric information to detect small blobs. From
this aspect, it has the potential to be applied to a wide range
of imaging modalities such as CT, Ultrasound, Ophthalmic
Photography, for small blob detection.
REFERENCES
[1] M. Zhang, T. Wu, and K. Bennett, “Small blob identiﬁcation in medical images using regional features from optimum scale,” IEEE Trans.
Biomed. Eng., vol. 62, no. 4, pp. 1051–1062, Apr. 2015.
[2] P. H. Mills et al., “Automated detection and characterization of SPIOlabeled cells and capsules using magnetic ﬁeld perturbations,” Magn.
Reson. Med., vol. 67, pp. 278–289, Jan. 2012.
[3] C. I. Sanchez et al., “Contextual computer-aided detection: Improving
bright lesion detection in retinal images and coronary calciﬁcation
identiﬁcation in CT scans,” Med. Image Anal., vol. 16, pp. 50–62, Jan.
2012.
[4] W. K. Moon et al., “Computer-aided tumor detection based on multiscale blob detection algorithm in automated breast ultrasound images,”
IEEE Trans. Med. Imag., vol. 32, no. 7, pp. 1191–1200, Jul. 2013.
[5] S. C. Beeman et al., “MRI-based glomerular morphology and
pathology in whole human kidneys,” Am. J. Physiol.-Renal Physiol.,
vol. 306, pp. F1381–F1390, 2014.
[6] M. Zhang, T. Wu, and K. M. Bennett, “A novel Hessian based algorithm for kidney glomeruli detection in 3D MRI,” in Proc. SPIE 9413
Med. Imag., Orlando, FL, 2015, pp. 94132N–94132N-9.
[7] S. C. Beeman et al., “Measuring glomerular number and size in perfused kidneys using MRI,” Am. J. Physiol. Renal Physiol., vol. 300,
pp. F1454–F1457, Jun. 1, 2011.
[8] T. Lindeberg, “Feature detection with automatic scale selection,” Int.
J. Comput. Vis., vol. 30, pp. 79–116, 1998.
[9] T. Lindeberg, Scale-Space Theory in Computer Vision. New York:
Springer, 1993.

[10] H. Kong, H. C. Akakin, and S. E. Sarma, “A generalized laplacian of
Gaussian ﬁlter for blob detection and its applications,” IEEE Trans.
Cybern., vol. PP, pp. 1–15, 2013.
[11] W. E. Hoy et al., “Nephron number, glomerular volume, renal disease and hypertension,” Curr. Opin. Nephrol. Hypertension, vol. 17,
pp. 258–265, 2008.
[12] J. R. Charlton, C. H. Springsteen, and J. B. Carmody, “Nephron number
and its determinants in early life: A primer,” Pediatr. Nephrol., vol. 29,
pp. 2299–2308, Dec. 2014.
[13] J. F. Bertram, M. C. Soosaipillai, S. D. Ricardo, and G. B. Ryan,
“Total numbers of glomeruli and individual glomerular cell types in
the normal rat kidney,” Cell Tissue Res., vol. 270, pp. 37–45, 1992.
[14] L. A. Cullen-McEwen, R. N. Douglas-Denton, and J. F. Bertram, “Estimating total nephron number in the adult kidney using the physical
disector/fractionator combination,” Methods Molecular Biol., vol. 886,
pp. 333–350, 2012.
[15] J. F. Bertram, “Analyzing renal glomeruli with the new stereology,”
in International Review of Cytology, W. J. Kwang and J. Jonathan,
Eds. New York: Academic, 1995, vol. 161, pp. 111–172.
[16] K. M. Bennett et al., “MRI of the basement membrane using charged
nanoparticles as contrast agents,” Magn. Reson. Med., vol. 60, pp.
564–574, 2008.
[17] K. M. Bennett, J. F. Bertram, S. C. Beeman, and N. Gretz, “The
emerging role of MRI in quantitative renal glomerular morphology,”
Am. J. Physiol. Renal Physiol., vol. 304, pp. F1252–F1257, May 15,
2013.
[18] E. J. Baldelomar et al., “Phenotyping by magnetic resonance imaging
nondestructively measures glomerular number and volume distribution
in mice with and without nephron reduction,” Kidney Int., 2015.
[19] C. M. Bishop, Pattern Recognition and Machine Learning. New
York: Springer, 2006, vol. 1.
[20] V. Lempitsky and A. Zisserman, “Learning to count objects in images,”
in Adv. Neural Inf. Process. Syst., 2010, pp. 1324–1332.
[21] D. Danon, L. Goldstein, Y. Marikovsky, and E. Skutelsky, “Use of
cationized ferritin as a label of negative charges on cell surfaces,” J.
Ultrastructure Res., vol. 38, pp. 500–510, 1972.
[22] D. Lowe, “Distinctive image features from scale-invariant keypoints,”
Int. J. Comput. Vis., vol. 60, pp. 91–110, 2004, 2004/11/01.
[23] A. Frangi, W. Niessen, K. Vincken, and M. Viergever, “Multiscale
vessel enhancement ﬁltering,” in Medical Image Computing and Computer-Assisted Interventation — MICCAI'98, W. Wells, A. Colchester,
and S. Delp, Eds. Berlin, Germany: Springer , 1998, vol. 1496, pp.
130–137.
[24] K. Mikolajczyk et al., “A comparison of afﬁne region detectors,” Int.
J. Comput. Vis., vol. 65, pp. 43–72, 2005, 2005/11/01.
[25] C. D. Meyer, Matrix Analysis and Applied Linear Algebra. Philadelphia, PA: SIAM, 2000.
[26] E. Bernardis and S. X. Yu, “Pop out many small structures from a very
large microscopic image,” Med. Image Anal., vol. 15, pp. 690–707,
2011.
[27] L. A. Cullen-McEwen, M. M. Kett, J. Dowling, W. P. Anderson, and J.
F. Bertram, “Nephron number, renal function, arterial pressure in aged
GDNF heterozygous mice,” Hypertension, vol. 41, pp. 335–340, Feb.
2003.
[28] J.-P. Bonvalet, M. Champion, F. Wanstok, and G. Berjal, “Compensatory renal hypertrophy in young rats: Increase in the number of
nephrons,” Kidney Int., vol. 1, pp. 391–396, 1972.
[29] O. Chapelle, B. Schlkopf, and A. Zien, Semi-Supervised Learning.
Cambridge, MA: MIT Press, 2010.

SCIENCE CHINA
Information Sciences

. RESEARCH PAPER .

December 2012 Vol. 55 No. 12: 2849–2864
doi: 10.1007/s11432-012-4748-7

Clustering mechanism for electric
tomography imaging
YUE ShiHong1 ∗ , WU Teresa2 , CUI LiJun1 & WANG HuaXiang1
2School

1School of Electrical Engineering and Automation, Tianjin University, Tianjin 300072, China;
of Computing, Informatics, Decision Systems Engineering, Arizona State University, Tempe 85287-0112, USA

Received August 25, 2012; accepted October 30, 2012

Abstract Electrical tomography (ET) imaging, developed in the 1980s, has attracted much industrial and
research attentions owing to its low cost, quick response, lack of radiation exposure, and non-intrusiveness
compared to other tomography modalities. However, to date applications thereof have been limited owing to its
low imaging resolution. The issue with space resolution in existing ET imaging reconstruction methods is that
they employ a mathematical approach based on an ill-posed equation with inconsistent solutions. In this paper,
we propose a novel ET imaging method based on a data-driven approach. By recovering the cluster structures
hidden in the ET imaging process followed by the application of a fuzzy clustering algorithm to identify the
cluster structures, there is no need to study the ill-posed mathematical formulation. The proposed method has
been tested by means of three experiments, including image reconstructions of a human lung image and plastic
rode shape, as well as two simulations executed on the Comsol platform. The results show that the proposed
method can reconstruct ET images with much higher space resolution more quickly than the existing algorithms.
Keywords

electrical tomography, ill-posed problem, fuzzy clustering, space resolution

Citation
Yue S H, Wu T, Cui L J, et al. Clustering mechanism for electric tomography imaging. Sci China Inf
Sci, 2012, 55: 2849–2864, doi: 10.1007/s11432-012-4748-7

1

Introduction

Electrical tomography (ET) imaging technology [1] is a nondestructive visualization measurement technique. Owing to its quick response, non-invasiveness, low-cost in obtaining 2D/3D distribution parameter
information, it has been widely used in many ﬁelds including medicine and industrial measurement. During the past twenty years, the ET technique has become a major research theme in process testing
technology [2–4] .
The ET technique mainly includes electrical resistance tomography (ERT) [5], electrical capacitance
tomography (ECT) [6] and electro magnetic tomography (EMT) [7]. By exciting a group of electromagnetic sources in turn in the ﬁeld objects under investigation, the distributions of permittivity, electric
conductivity or magnetic permeability by the boundary measures can be visually reconstructed. According to the electrostatic and magnetic analogy principles [8,9], all three ET techniques have the same
∗ Corresponding

author (email: shyue1999@tju.edu.cn)

c Science China Press and Springer-Verlag Berlin Heidelberg 2012


info.scichina.com

www.springerlink.com

2850

Yue S H, et al.

Sci China Inf Sci

December 2012 Vol. 55 No. 12

mathematical mechanisms and expressions, and follow the same physical laws, even though there are
diﬀerences in their physical and the hardware used to measure the data. In general, the methods for ET
imaging reconstruction can be categorized into two groups: linearized algorithms and nonlinear iterative
algorithms [10]. The linearized approach includes both non-iterative and iterative methods. Both are
based on linear approximation theory. Non-iterative methods usually have a short imaging time but low
resolution. Some examples of non-iterative methods are back projection [11], Noser [12], the single-step
Tikhonov method [13], and the hybrid method [14]. Typical linearized iterative methods are the Landweber iteration [15] and the conjugate gradient (CG) algorithms [16]. In nonlinear iterative algorithms,
the optimal solution is identiﬁed according to linear approximation theory using the distributions of
the dielectric constant, conductivity, or magnetic permeability. Examples of these methods include the
regularized Gauss-Newton algorithm [17] and nonlinear conjugate gradient methods [18].
While the existing algorithms show promise in a number of applications, some issues remain unsolved.
These include:
1) The ill-posed equation. Most of the existing ET imaging methods depend on an optimization process
associated with an ill-posed mathematical equation. The process suﬀers from handling an ill-conditioned,
highly nonlinear and inconsistent problem [1,7,8,12]. As a result, the reconstructed images usually have
low resolution.
2) Soft-ﬁeld eﬀect. Owing to the use of weak current excitation to produce the measured data in ET,
which must be a weak signal, any small measured errors or noise may lead to a large variance in the
investigated object image. Essentially, any measure data is associated with the material distributions of
the whole ﬁeld under investigation. This is called the “soft-ﬁeld” eﬀect, which is undesirable in practice
[7,19]. This eﬀect leads to instability in the ET imaging process and often unacceptable results under
noisy conditions [20,21].
3) Dependability of the reconstructed image. This is a challenging problem in quantitatively measuring
the quality of the reconstructed images. Most reconstructed images by the existing algorithms have
limited interpretability or understandability [19]. Various researchers have proposed the use of simulation,
visualization, and enhancing the hardware to obtain comparable statistics for imaging quality assessment
[21,22]. However, these methods are less applicable in practice.
To overcome the above problems, we propose a new data-driven ET imaging reconstruction method.
First, as is known in ET, materials with the same conductivity, permittivity, or permeability must
have similar distributions of the measured data, and vice versa. In an imaging ﬁeld, the objects under
investigation consist of materials with diﬀerent conductivity, permittivity, or permeability.
When applying clustering techniques, the same distribution of measured data should respond to the
same clusters, and diﬀerent distributions should respond to diﬀerent clusters. Thus, the task of ET
imaging is the application of a clustering method to identify all the clusters. Clustering validity indices,
which are metrics for measuring clustering performance, can be employed to guide the clustering process
for good quality imaging reconstruction. One signiﬁcant advantage is that the data-driven approach does
not require an analysis dependent on the ill-posed mathematical problem, which has long been recognized
as a critical issue in ET imaging. In this paper we explore the use of a fuzzy clustering algorithm for
the following reasons. First, fuzzy clustering is known to handle problems with uncertain, inaccurate,
and incomplete information. Second, fuzzy clustering results can be evaluated directly by a single fuzzy
clustering validity index or a group of indices [23,24]. The quality of the ET images can be quantitatively
assessed. In addition, fuzzy clustering has been widely adopted in industry [25–27].
The remainder of this paper is organized as follows. We present mathematical descriptions of ET
image reconstruction in Section 2, while in Section 3, we present the fuzzy clustering-based ET image
reconstruction algorithm and analyze its properties compared with those of classical algorithms in the
ﬁeld. In Section 4, we investigate the eﬀectiveness of the proposed algorithm through three sets of experiments. The experimental results show that the new algorithms can greatly enhance ET imaging
resolution. Furthermore, the proposed algorithm is simple, has low computational complexity, and provides a new method for ET image reconstruction. Our conclusions and future research directions are
discussed in Section 5.

Yue S H, et al.

2

Sci China Inf Sci

2851

December 2012 Vol. 55 No. 12

Related work

This section includes a mathematical description of ET image reconstruction, and three commonly used
ET image reconstruction algorithms: linear back projection (LBP) [17], conjugate gradient [18], and
Landweber iteration [15] methods.
The task of ET image reconstruction is to determine the permittivity distributions by ECT, the
magnetic permeability distributions by EMT, and/or the conductivity distributions by ERT; in other
words, the distribution of the material over the cross-section based on the electrical measurements.
According to the electrostatic analogy principle [28], ERT, EMT, and ECT have the same mathematical
representation but a diﬀerent physical meaning. Therefore, we focus our explanation on ERT only in the
following.
Basically, in the two-dimensional case, the relationship between the space distribution of conductivity
and the electric potential distribution can be derived from Maxwell’s equation [19], which given as Poisson
equations is deﬁned as
∇(σ∇ϕ) = 0,
(1)
where σ and ϕ refer to the electrical conductivity and electrical potential in the investigated domain,
respectively. It is possible to collect information on the space distributions of the conductivity using a
group of measured boundary voltages. In the mathematical sense, this process is called an ERT inverse
problem [20], while the problem of ﬁnding the electrical potential and potential diﬀerences U between
electrode pairs for a given current injection is called the forward problem with the distribution of σ [20].
Based on ﬁnite element method (FEM) [29,30], the linearized and discrete form of Eq. (1) can be
expressed as
(2)
M×1 U = JM×N N ×1 σ,
where J is a Jacobian matrix, i.e., a sensitivity distribution matrix, giving the sensitivity map for each
electrode pair, and M and N are the numbers of measurements and elements (pixels), respectively. The
normalized form of Eq. (2) is given by Eq. (3),
U = Sσ,

(3)

where σ is the normalized voltage vector or capacitance vector, S is the Jacobian matrix of normalized
voltage with respect to normalized permittivity, i.e., the transducer sensitivity matrix, and σ is the
normalized permittivity vector corresponding to the gray level of the pixels for visualization. In the
discrete form, the aim of image reconstruction for ET is to ﬁnd the unknown gray value of g for any pixel
from the known σ using Eq. (3), that is
(4)
σ = S −1 U.
However, a direct analytical solution of Eq. (4) does not exist since the inverse problem is both
nonlinear and ill-posed; thus, a small amount of noise in the measured data could lead to large errors
in the estimated conductivity. Consequently, it is necessary to use numerical techniques to approximate
S −1 as accurately as possible after applying some residual criterion. One example criterion is minimal
least error, deﬁned as
(5)
f = U − Sσ22 → min.
Eq. (5) is used to identify the optimal values of σ .
Many variants of Eq. (5) [21] have been proposed to solve the ill-posed problem, among which is one
notable technique, the conjugate gradient (CG) method, which is known for its eﬃciency. In the CG
method, an iterative function is deﬁned as:
g k+1 = g k + ak P k ,

s.t. P k = −∇f (g k ) +

∇f (g k )2 k−1
P
,
∇f (g k−1 )2

k = 2, 3, . . . ,

(6)

where g k is the gray value of the kth iterations, the original values of P k constitute a negative gradient
vector, and the optimal values of ak are often solved by:
a∗k = argminak f (g k + ak P k ),

k = 1, 2, . . . .

(7)

2852

Yue S H, et al.

Sci China Inf Sci

December 2012 Vol. 55 No. 12

Once the convergence condition or a prior estimate of the number of iterations had been satisﬁed, a ﬁnal
estimate of g for all pixels is obtained.
Other commonly used ET imaging algorithms are the linear back projection (LBP) [17] and Landweber [18] methods. In the LBP algorithm the conductivity distributions are assumed to consist of a
number of discrete regions within the measurement space such that the conductivity within each region
is constant. From Eq. (4), we have
σ = S T U/S T Uλ ,

s.t. Uλ = [1, 1, . . . , 1].

(8)

Eq. (8) shows that the gray values of any pixel are calculated using a weighted form.
The Landweber iteration method, originally designed to solve the ill-posed problem using a strategy
similar to the gradient descent algorithm, is deﬁned as:
Ĝk+1 = Ĝk − αS T (S Ĝk − U ),

(9)

where constant α is known as the gain factor and is used to control the convergence rate. As the iterative
process described by Eq. (9) proceeds, the norm of the capacitance residual is minimized. Since the norm
tends to be a value greater than zero, the original algorithm is often modiﬁed as


Ĝk+1 = P Ĝk − αS T (S Ĝk − U ) .

(10)

The value of P is adopted by including a nonlinear function to constrain the estimated image so that
Ĝk+1 ∈ [0, 1]; that is, when the normalized gray level is less than zero it is constrained to be zero and
when it is greater than “1” it is constrained to be “1”.
In this paper, the LBP, CG and Landweber algorithms are used for ET imaging reconstruction and
are compare with our proposed method in terms of both space and time resolution.

3

Fuzzy clustering-based ET image reconstruction

In this section we ﬁrst explain the basic steps of the proposed fuzzy clustering-based ET imaging algorithm. Then, we justify the proposed algorithm and analyze its performance compared with existing ET
imaging algorithms. In this section we ﬁrst illustrate the basic steps of the proposed fuzzy clusteringbased ET imaging algorithm. Next, we justify the proposed algorithm and analyze its performances
compared with existing ET imaging algorithms.
3.1

Vectorization of measured ET data

In this study, we used an ERT system consisting of 16 electrodes, evenly distributed around a pipe with
a 16 cm radius, to illustrate the clustering structure of an ET image in Figure 1.
Figure 1 shows the distribution of electrodes in our experiments and a view of the cross-section, while
the adjacent exciting strategy is used for data collection in the ERT system. Without loss of generalization, we applied 16 electrodes evenly distributed around a pipe with a 16 m radius to illustrate the
adjacent driven pattern when applying the ERT modality. Figure 1 shows the distribution of electrodes in
our experiments and the adjacent exciting strategy is used for data collection. First, exciting current I is
added to electrode pair (1, 2), and the 15 voltage values of (2, 3), (3, 4), . . . , (16, 1) are measured. Next,
the exciting current is added to (2, 3), and the voltages of (3, 4), (4, 5), . . . , (1, 2) are measured from
successive pairs of neighboring electrodes and so on. Finally, the exciting current is added to (16, 1), and
the voltages of (1, 2), (2, 3), . . . , (15, 16) are measured from successive pairs of neighboring electrodes.
After the excitation electrode pair is switched 16 times, 16 groups of measurements are obtained of which
the two measurements including the excitation electrode pairs are discarded owing to considerable errors.
Thus, 13 groups of measurements in each excitation pair except the excitation electrode pair, are used
for image construction. Considering the reciprocity of electrode distributions, a total of 104 independent

Yue S H, et al.

E10

VM

E9
E8

E11
A

E12

C
E14
B

E15

Projection field

IS

E7

E12

E13

E5

E14

E5

E14

E2

Figure 1

E4
E16
E1

E2

E8
E7

E12

E6

E15

E9

E11

E8

E13

E3
E1

E10

E9

E6

E4

E16

E10

2853

December 2012 Vol. 55 No. 12

E11
E7

E13

Sci China Inf Sci

E6
E5

A

E15

E3

E4
E3

E16
E1

E2

Vector generation process in the proposed algorithm.

values are used to extract the internal conductivity information [11,15]. The total number of independent
measurements [11] M is
M = K(K − 3)/2,

(11)

where K is the number of measuring and exciting electrodes used.
To reconstruct a frame of the image, the cross-section is discretized by rectangular or triangular units
related to the pixels in the ERT image (see Figure 1). The cross-section boundary and any pair of
equipotential lines connected to two adjacent electrodes construct a projection ﬁeld (see Figure 2). Each
extraction electrode corresponds to 16 projection ﬁelds over the entire cross-section, and thus any pixel
in the cross-section is covered by 16 projection ﬁelds from the 16 diﬀerent extraction electrodes (see
Figure 3).
To illustrate the process of constructing a vector for any pixel, we randomly take three pixels A, B,
and C in the area under investigation (see Figure 1). Each pixel is covered by 16 projection ﬁelds from
the 16 excitation electrodes. Let VA , VB and VC be the corresponding vectors of pixels A, B, and C.
Since each pixel corresponds to 16 measured voltages, the measured values of the three pixels consist of
the following three 16-dimensional vectors:
⎧
T
⎪
⎪
⎨ VA = (vE10 −E11 ,E12 −E13 , vE10 −E11 ,E13 −E14 , . . . , vE10 −E11 ,E9 −E10 ) ,
VB = (vE15 −E16 ,E16 −E1 , vE15 −E16 ,E1 −E2 , . . . , vE15 −E16 ,E14 −E15 )T ,
⎪
⎪
⎩ V = (v
T
C
E4 −E5 ,E5 −E6 , vE4 −E5 ,E6 −E7 , . . . , vE4 −E5 ,E3 −E4 ) ,

(12)

where the ﬁrst suﬃx of any vector refers to the closest excitation electrode pair, the other are the measured
electrode pair when applying the excitation electrode pair. For example, the ﬁrst component of the vector
associated with pixels in circle A corresponds to electrode pair E10 − E11 , those in circle B to electrode
pair E15 − E16 , and those in circle C to electrode pair E4 − E5 . In this form, we can ensure that any two
pixels correspond to the same vector if the two pixels are symmetrical on the cross-section center and
correspond to materials with the same permittivity values.
After vectorizing all pixels in the investigated ﬁeld, a vector set is obtained, which can be input to the
fuzzy clustering algorithm described in the next section.
3.2

Fuzzy clustering algorithm and optimal number of clusters

Let X = {xi } be a vector set with n members. The aim of the clustering is to partition all vectors
into a given number of clusters, c, in terms of the similarity between the members. The fuzzy c-means
(FCM) [23] algorithm is an eﬃcient technique for dealing with uncertain and vague information, and

2854

Yue S H, et al.

Sci China Inf Sci

December 2012 Vol. 55 No. 12

applies the following objective function:
minJ(u, v) =

n
c 


2
um
ij dij ,

c


s.t.

i=1 j=1

uij = 1,

0<

i=1

n


uij  n,

(13)

j=1

where dij = xj − vi , vi is the prototype of the ith cluster, uij is the membership degree of the jth
point with respect to the ith cluster, and m is a fuzziness exponent in the range 1.5 ∼ 2.5. The optimal
membership degree and prototype functions for Eq. (13) are
uij =

	 c

r=1

and
vi =




2/(m−1)
dij

n


uij xj

−1
2/(m−1)
drj


n

j=1

uij .

,

(14)

(15)

j=1

All fuzzy membership degrees consist of a n × c fuzzy partition matrix U = [uij ].
When applying the fuzzy clustering algorithm, the number of clusters c is given as a prior. There
are two possible approaches for determining the number of clusters c. For instance, in most cases the
actual number of clusters may be known since the multiphase ﬂows in a measured ﬁeld consist of three
determined components including gas, water, and oil in a crude oil transmission pipe. In such situations,
we take the number of clusters to be greater than three, since the ET imaging inevitably contains trail
traces, which have to be represented by at least one additional cluster. In the cases where no known
priors are available, clustering validity indices can be applied. Given a dataset, the number of clusters
can be determined by a proper function referred to as the clustering validity index [31–33]. The two most
frequently used indices in fuzzy clustering are partitioning entropy (PE) [34] and the Xie-Beni index [27].
Partitioning entropy is deﬁned in [34] as
c

VPE

n

1 
=−
uij loga uij ,
n i=1 j=1

(16)

where a is the base of the logarithm. The PE index is a scalar measure of the amount of fuzziness in a
given dataset. This index is computed for a given value of c greater than 1 and its value ranges within [0,
loga c]. In general, an optimal c∗ can be identiﬁed by solving min1<c<n VPE to produce the best clustering
performance.
Xie and Beni [27] deﬁned a validity index as

c 
n
m
2
Jm (U, V )
i=1
j=1 uij xj − vi 
VXB =
.
(17)
=
Sep(V )
nmini=j vi − vj 2
The optimal number of clusters can be obtained by minimizing (17) for all possible numbers of clusters.
When a validity index suggests the optimal number of clusters, it is, in fact, identifying the optimal
clustering partitions. Thus, the use of a validity index in clustering diﬀerentiates this unsupervised
approach from most existing algorithms that are supervised [17,26]. Meanwhile, the use of validity
indices assists in the quantitative evaluation of ET imaging quality.
3.3

Algorithm description

In this research we studied a system with 16 electrodes and constructed a 16-dimensional vector for ET
imaging using fuzzy clustering. We refer to the proposed algorithm as FCET-16 (fuzzy clustering-based
electrical tomography). Assume that there are a total of K electrodes and that all pairs of electrodes
are excited counterclockwise along with a circle-shape tomography in the pipe. The proposed FCET-16
algorithm includes the following six modules:

Yue S H, et al.

Sci China Inf Sci

December 2012 Vol. 55 No. 12

2855

Module 1. Partition the cross-section into a set of pixels (e.g., square or triangular units). The investigated area is discretized by rectangular or triangular units related to the pixels of the ERT image. Let
the number of units (pixels) be N .
Module 2. Compute all comparative boundary measured values (e.g., voltages). If the cross-section
has no material, which is known as an empty ﬁeld, the measured values consist of K groups of regular
distributions. If there are materials in the cross-section, the measured voltages are irregular. The changes
(increment) in the two groups of measured values construct comparable voltage values for generating the
vectors in the proposed algorithm.
Module 3. Map K voltages of any pixel into a K-dimensional vector in which the ﬁrst component is
the voltage of the closest excitation electrode pair to the pixel. The cross-section is partitioned into K
projection ﬁelds for any excitation electrode and any pixel is covered by K projection ﬁelds from diﬀerent
excitation electrodes. For the jth partitioned pixel there are K measured data related to the K projection
ﬁelds from K excitation electrode pairs, which are denoted as vj,1 , vj,2 , . . . , vj,K , j = 1, 2, . . . , N . Thus,
the jth pixel can be represented by a K-dimensional vector as
(vj,j1 , vj,j2 , . . . , vj,jK )T ,

for all j = 1, 2, . . . , N,

(18)

where vj,j1 , vj,j2 , . . . , vj,jK is the counterclockwise reordering of vj,1 , vj,2 , . . . , vj,K ensuring that the ﬁrst
component of any vector is the measured value from the closest of the K excitation electrodes.
Module 4. Partition all vectors in x into c clusters C1 , C2 , . . . , Cc using a fuzzy clustering algorithm.
Let uij be the membership degree of the jth vector (pixel) to the ith cluster, and vi be the clustering
prototype of the ith cluster, i = 1, 2, . . . , c, j = 1, 2, . . . , n.
Module 5. Determine the gray value of any pixel from its fuzzy membership degrees in fuzzy clustering.
Let G(vi ) be the gray value of the pixel of the ith clustering prototype, i = 1, 2, . . . , c. Thus, the gray
value of the jth pixel g(j) is determined in a weighted average form by,
g(j) =

c


uij G(vi ),

j = 1, 2, . . . , N.

(19)

i=1

All pixels are allocated diﬀerent gray values based on Eq. (19).
Module 6. Eliminate trial trace. Let G(v1∗ ), G(v2∗ ), . . . , G(vc∗ ) be the gray values of all cluster prototypes
∗
∗
) = G(vm+2
) = · · · = G(vc∗ ) and recalculate
satisfying G(v1∗ )  G(v2∗ )  · · ·  G(vc∗ ). Further, let G(vm+1
the gray values of all pixels using Eq. (19), where m is the number of clusters that are associated with
the m objects under investigation. These gray values can be used to reconstruct a frame of the image to
show the various conductivities in the cross-section.
In Module 3, if V̄j , σj , σ̄j denote the mean, variance, and two-order variance of the 16 components of
the jth vector, respectively, the following vector is constructed
(vj,j1 , vj,j2 , . . . , vj,jk )T → (V̄j , σj , σ̄j )T , j = 1, 2, . . . , N.

(20)

All vectors in Eq. (20) consist of one additional set of vectors X̄. After applying Modules 3–6 to all
vectors in X̄, another form is derived from our proposed imaging algorithm. We call this method FCET-3,
owing to the use of only three features of each vector.
Hereafter, we generally refer to FCET-16 and FCET-3 as the FCET algorithm.
3.4

Algorithm analysis

The FCET algorithm is explained by means of its basic characteristics, performance analysis, and interrelation with the LBP algorithm and also compared to other existing algorithms.

Yue S H, et al.

2856

1.0
0.8
0.6
0.4
0.2
0.0
−0.2
−0.4
−0.6
−0.8
−1.0
−1.0

Inside:
1.3510,0.0058,1.3827

Outside:
0.9966,0.0968,1.3798
−0.5

0.0
(a)

0.5

1.0

Sci China Inf Sci

December 2012 Vol. 55 No. 12

1.5

1.0
0.8
0.6
0.4
0.2
0.0
−0.2
−0.4
−0.6
−0.8
−1.0

1.0

0.5

−1.0

−0.5

0.0
(b)

0.5

1.0

0.0
0

Circle inside:
Circle outside:
2

4

6

8
(c)

10

12

14

16

Figure 2 Vector distributed diﬀerences between the investigated objects and surrounding vectors. (a) Original image;
(b) reconstructed image from the LBP algorithm; (c) vector distributions of these inside and outside vectors.

3.4.1 Comparison of the characteristics of the FCET and LBP algorithms
Figure 2 (a) and (b) shows the original image and a reconstructed image by LBP, respectively. The
investigated objects are contained in a circle and we use 20 individual pixels from within and outside the
circle. Figure 2(c) shows the vector distributions of these pixels. In contrast to the original image, it can
be observed that there are marginal trail traces around the real objects in the reconstructed image. The
ten outside pixels comprise only a part of the trail traces. This is the main reason that the ET image
has low resolution. A good reconstructed ET image is required to minimize the within-cluster diﬀerence
while maximizing between-cluster diﬀerences. The 20 pixels within and outside the circle belong to two
clusters and thus they should be distinguished by any ET imaging algorithm.
Figure 2(a) shows that the mean values of the inside and outside pixels are 1.3510 and 0.9966, respectively, while their ﬁrst order diﬀerences are 0.0058 and 0.0968, respectively. Consequently, the FCET-3
algorithm can distinguish the two groups of vectors better than the LBP algorithm. If the two components of the ﬁrst and second order diﬀerences are assumed to be zero, the FCET-3 algorithm is reduced
to the LBP algorithm.
We also analyzed the interrelation of the FCET-16 and LBP algorithms based on the diﬀerent measures
used. Let XA = (xA1 , xA1 , . . . , xA16 )T and XB = (xB1 , xB1 , . . . , xB16 )T be any two vectors of pixels A
and B, respectively, created by the FCET-16 algorithm. The similarity of the two pixels is measured by
a special d∞ norm, with the diﬀerence between XA and XB given as
d(A, B) =

16


|xAk − xBk |.

(21)

k=1


16

16
The LBP algorithm applies the averaging gray values of k=1 xAk /16 or k=1 xBk /16 to identify the
real conductivity values of the two pixels, with their diﬀerence given by
	 16

16

1 
(22)
xAk −
xBk .
d(A, B) =
16
k=1

k=1

As a result, the space resolution of the reconstructed image is determined by the relative diﬀerence of any
two pixels. The larger the diﬀerence is between any two pixels, the larger is the resolution that the two
pixels can represent. After reducing the measured values in Eq. (21) to 1/16 thereof, Eqs. (21) and (22)
satisfy
	 16

16
16

1 
1 
(23)
|xAk − xBk | 
xAk −
xBk .
16
16
k=1

k=1

k=1

Eq. (23) shows that Eq. (21) gives a larger relative diﬀerence between any two pixels than Eq. (22).
Speciﬁcally, if the gray values of all components of A are uniformly larger or smaller than those of B,
Eqs. (21) and (22) are perfectly identiﬁable. This shows that the LBP algorithm is only a special case

Yue S H, et al.

Table 1

Sci China Inf Sci

2857

December 2012 Vol. 55 No. 12

Comparable properties of these ET imaging algorithms

Algorithm

Solution
space

Solution
convergence

Convergence
speed

Solution
consistency

Solution
interpretability

Image
evaluation

Landweber

Large

Uncertain

Small

Inconsistent

Weak

No

CG

Large

Uncertain

Small

Consistent

Fair

No

FCET

Small

Converge

Converge

Consistent

Strong

Yes

when the FCET-16 algorithm uses d∞ norm. Consequently, when diﬀerent similarity measures are used,
the FCET algorithm can provide richer imaging results.
One advantage of the FCET-3 algorithm is that it can eﬃciently eliminate trial trace according to
Module 6. Trial trace vectors have a larger variance than the investigated object vectors. It is expected
that the two classes of vectors should be partitioned into diﬀerent clusters. The trial traces are, in fact,
those pixels that originally comprised the background in the investigated ﬁeld, but were assigned gray
degrees incorrectly. In Module 6, these vectors are adjusted to their correct gray degrees. Similarly,
two classes of vectors must be very diﬀerent in the FCET-16 algorithm if they have very large variance
diﬀerences. It should be noted that the trial trace vector can also be adjusted by Module 6.
3.4.2 General comparison of the FCET and other ET algorithms
The FCET and existing ET imaging algorithms optimize diﬀerent objective functions in the reconstruction
of ET images. Table 1 gives a general comparison of the FCET algorithm and two representative iterative
ET imaging algorithms, Landweber and CG.
The contents of the table are explained below.
(1) Solution space. Let a two-dimensional investigated ﬁeld be partitioned into N × N pixels. Assume
that there are L gray levels. The solution space of Eqs. (5) and (6) consists of all combinations of
N × N pixels mapped onto the L gray levels. For example, if N = 30 and L = 256, there are a total
of 256 × 256 × · · · × 256 (total 30 × 30) feasible solutions. As N increases, the number of combinations
increases dramatically. It is thus highly unlikely (if not impossible) that a global optimal solution will be
lacated.
By optimizing the objective function of Eq. (13) instead, only c clustering prototypes are required to
be derived from a set of N × N vectors according to the L gray levels. Using the same example (N = 30,
L = 256), the number of feasible solutions is c × 30 × 30 × 256. Thus, the task of ﬁnding the optimal
solution of Eq. (13) is much less computationally intensive compared to existing ET algorithms.
(2) Solution convergence. Convergence of the optimization process in FCET depends on a typical
optimization theorem, that is, a monotonic and bounded data sequence will converge to a local solution
[23,27]. We can therefore, conclude that the FCET algorithm converges. Note that Landweber and CG
may diverge if their parameter settings are inaccurate [23,26].
(3) Convergence speed. The computational complexity of FCET depends on the FCM implementation
which is O(ctn)[23], where c is the number of clusters, t is the number of iteration, and n is the number
of members to be clustered. Since the solution space of FCET is much smaller than that of Landweber
and CG as explained earlier, the convergence processes vary. In Figure 4 we show the convergence speed
of FCET compared to that of CG and Landweber using the same dataset as in Figure 2. It is shown that
the convergence error of FCET is of the order 10−7 , while those of CG and Landweber are of the order
10−5 .
(4) Solution consistency. This refers to the repeated consistency of the quality of reconstructed images.
Though FCM is highly dependent on its initialization, optimal results can easily be identiﬁed if Eq. (13)
obtains minimal values under numerous initial conditions [2,23,26]. In contrast, Landweber and CG often
suﬀer from inconsistency through repetition since Eqs. (5) and (6) typically suﬀer from the problem of
containing a large number of local minima [7,20].
(5) Solution interpretability. One advantage of the FCET algorithm is that the partitioned results can
be examined at any step during the optimization process. Unlike Landweber and CG, which employ a

Yue S H, et al.

2858

Sci China Inf Sci

December 2012 Vol. 55 No. 12

black-box approach, FCM aims to maximize the between-cluster distance while minimizing the withincluster distance, and the results are relatively easier to interpret. In addition, Landweber and CG are
diﬃcult to implement in practice according to diﬀerent parameter settings, owing to the need for prior
knowledge.
(6) Imaging evaluation. To the best of our knowledge, there are currently no quantitative methods for
evaluating ET imaging quality. In the case that no prior information is available, the clustering validity
index may be applied to determine the appropriate number of clusters in FCET images. This index
cannot be adopted in existing ET imaging algorithms owing to the problem formulation. In our proposed
data-drive method, we take full advantage of the functionality of a validity index.

4

Experiments

In this section we discuss the three diﬀerent sets of experiments carried out to assess the eﬀectiveness of
the FCET algorithm. For comparison, we also applied the LBP, CG, and Landweber iteration algorithms
to reconstruct these images from the same measured data. The experiments were performed in the ERT
or ECT sensitivity ﬁelds and analyzed by two metrics: time and space resolution. Here, time resolution
is deﬁned as the time required to reconstruct an ET image, while space resolution refers to the relative
error of all K pixels in the reconstructed image given by
ζ=

K
1  (gj − gj∗ )
,
K j=1
gj

(24)

where gj is the reference gray degree of the jth pixel, that is, the real gray value; gj∗ is the gray value
of the jth pixel in the reconstructed image, j = 1, 2, . . . , K, and ζ is the average of all errors of the K
pixels.
For cases where the number of clusters in the investigated ﬁeld is known as a prior, we set the number
of clusters c in the FCET algorithm to
c = 2 × q + 1,
(25)
where q is the number of diﬀerent ET identiﬁable clusters of investigated objects; the coeﬃcient 2 refers
to any cluster of investigated objects and its trail trace; and 1 refers to the even background. For cases
where the number of clusters is not known, validity indices are employed to suggest an appropriate
number of clusters. Owing to the use of the FCM algorithm in FCET, diﬀerent initializations impact
the reconstructed image greatly. This problem can be addressed by applying multiple initializations and
using the best result of the objective function with the minimal value in practice [23,26]. In the following
experiments, we selected four initializations of the FCET algorithm to overcome this problem.
4.1

Test on continuously distributed materials

This set of experiments was performed in Comsol 2.3.1) The original images tested comprise two or three
circles with continuously distributed materials (see Figure 3 (a) and (d)).
These circles have the same conductivity and should be shown in the same degree of gray. On the other
hand, the background should correspond to two diﬀerent degrees of gray, while the trail trace contains
at least one cluster. The correct number of clusters is three according to Eq. (25). Figure 3 (b) and (e)
shows the images obtained by the Landweber algorithm while (c) and (f) obtained by the LBP algorithms,
respectively. The trail traces in a reconstructed image can be used to evaluate the space resolution of
the image visually. Figure 3 shows that the trail traces are widely distributed, and that some circles are
incorrectly connected to the same area. We also applied the FCET-3 algorithm with diﬀerent parameter
settings for image reconstruction (see Figure 5). This ﬁgure shows that the FCET-3 algorithm is able
to distinguish these circles, and can reconstruct images with smaller trail traces under a wide range of
parameter settings.
1) http://www.comsol.cn.

Yue S H, et al.

Sci China Inf Sci

2859

December 2012 Vol. 55 No. 12

0.8
0.4

8

1.0

7

0.5

0
−10
−20

6
0.0

0.0

−30

5

−0.4

4 −0.5

−0.8

3
−1.0
−1.0 −0.5

(a)

0
0.0
(b)

0.5

−50
−1.0

1.0
1.0

−40

−1.0

−0.5

0.0
(c)

0.5

1.0

6.5 1.0

0.8

0.0

70
60
50
40

2.5 −0.5

30
20

5.5
0.4

0.5

4.5
0.0
3.5
−0.4

10

−0.8
−1.0 −0.5
(d)

0.0
(e)

0.5

1.0

1.5 −1.0

−1.0

−0.5

0.0
(f)

0.5

1.0

Figure 3 Reconstructed images from Landweber and LBP algorithms. (a) and (d) show the original images; (b) and (e)
show the reconstructed images from the Landweber algorithm; and (c) and (f) are the reconstructed image from the LBP
algorithm.

Original image

Figure 4

CG

LBP

Desire image

Original and reconstructed images by the Landweber and LBP algorithms.

These results demonstrate further that the FCET-3 algorithm outperforms the other two algorithms
with the given datasets. However, when applying FCET-16 in this experiment, the space resolution is
reduced. The reconstructed results of the three circles are illustrated in Figure 5 (g)–(i), which indicates
that the investigated objects are not clearly reconstructed and that the real circles are shown as a group
of irregular shapes. We believe that the FCM algorithm in the FCET algorithm is responsible for this
failure since in a high-dimensional space, it is always diﬃcult to obtain the real clustering centers in

Yue S H, et al.

2860

(a)

0.8

60

Sci China Inf Sci

December 2012 Vol. 55 No. 12

(b)

0.8

60

0.4

40 0.4

0.0

0.0

0.0

−0.4

20
−0.4

0.4

40

−0.8

20

−1.0 −0.5

0.0

0.5

1.0
(d)

0.8
0.4

−1.0 −0.5

60

40

−0.4

20−0.4
0
0.0

0.5

0.4

(e)

60

−1.0 −0.5

20

0
60

40
20

0 −0.8
0.0

0.5

(h)

60 0.8
0.4

0
−1.0 −0.5

1.0

0.0

0.5

40

1.0
(i)

60 0.8
0.4

20
−0.4

20

−0.8

0 −0.8

0 −0.8

1.0

(f)

−0.4

−0.4

0.5

1.0

0.4

0.0

0.0

0.5

0.8

0.0

−1.0 −0.5

0.0

0.0

−1.0 −0.5

40

20

−0.4

0

40

−0.8

1.0
(g)

0.8

1.0

0.4
0.0

−1.0 −0.5

0.5

0.8

0.0

−0.8

0.0

60

40

−0.8

−0.8
0

(c)

0.8

60
40

0.0
20
−0.4

−1.0 −0.5

0.0

0.5

1.0

0
−1.0 −0.5

0.0

0.5

1.0

Figure 5 Reconstructed images: (a)–(f) by FCET-3 under diﬀerent parameter settings (fuzzy exponent m and number
of clusters c); (g)–(i) by FCET-16.

the clustering prototypes from the FCM algorithm [32]. This problem could be addressed by applying
diﬀerent clustering algorithms; this will be investigated in the near future.
4.2

Test in an ECT field

In this simulation, the ﬁrst two models represent iron rods with diﬀerent diameters inserted into plastic
particles (representative of an oil bubble), the third model represents ﬁlling the half pipe plastic particles
(representative of a half pipe of oil), with air comprising the background materials, while the fourth
experiment simulates the entire pipeline with plastic particles (representing the whole oil tube). The
ﬁrst and fourth columns in Figure 4 depict the original image and the most desirable image. This is a
typical ECT ﬁeld, and the FCET-16, FCET-3, Landweber, and LBP algorithms were used for imaging
the objects under investigation. In the set of experiments, plastic particles, iron rods, and air correspond
to the three gray degrees in the reconstructed images, together with trail traces, and thus the number of
clusters in the FCET algorithm is set to 7 in Eq. (25). Figure 4 shows the reconstructed images by the
Landweber and LBP algorithms, while Figure 6 shows the images by the FCET algorithm.
Conclusions derived from these results are given below.
1) Time resolution. The average imaging speeds of the Landweber, LBP, FCET-16, and FCET-3
algorithms are about 4, 14, 4, and 7 frames/s, respectively. The FCET-16 algorithm is the slowest. Since
the LBP algorithm does not require iteration, its imaging speed is the fastest of the imaging algorithms
tested.
2) Space resolution. Figure 4 shows that if the investigated objects consist of multiple components, the
LBP algorithm cannot identify all the objects. The reconstructed images by the Landweber algorithm
contain a great many trail traces and thus, suﬀer from low space resolution. The images in Figure 6
show that the FCET algorithm can distinguish most of the investigated objects, especially in the above

Yue S H, et al.

Sci China Inf Sci

2861

December 2012 Vol. 55 No. 12

FCET-16
m=1.5
c=4

FCET-3
m=1.5
c=4

Figure 6

Reconstructed images from the FCET-16 and FECT-3 algorithms.

(a)

(b)

(c)

(d)

Figure 7 Experiments on a human lung using an ERT system. (a) Data acquisition electrodes; (b) reconstructed XCT
image; (c) reconstructed image by LBP algorithm; (d) reconstructed image by CG algorithm.
Table 2

Conductivity of a variety of tissues

Tissues

Right lung

Left lung

Adipose

Bone

Others

Permittivity (1/Ωm)

0.25–1

0.042–0.138

0.006

0.037

0.323

second and fourth models, and that the reconstructed images are closer to the desired images than those
obtained by existing algorithms. Essentially, the FCET-3 algorithm performs better than the FCET-16
algorithm for imaging materials consisting of only a few components and little noise.
4.3

Image reconstruction of a human lung

Test results from the actual diagnostic process of a human lung and experimental data were obtained
from 32 electrodes placed around the chest of a man (see Figure 7(a)). Accordingly, the cross-section
was partitioned into 1600 units (pixels) for image reconstruction such that it is possible to represent
more details of the soft tissues of the human lung in the reconstructed ERT image. We reconstructed
the image of the human lung using the FCET-16, FCET-3, LBP, and CG algorithms based on these
measured voltages. At the same time, the scanned XCT image of the man using X-ray inversion recovery
pulse sequences was available to serve as a reference image (see Figure 7(b)). Usually, the XCT image
reﬂects the bone skeleton of a human thorax well, whereas the ERT to some extent, is better at reﬂecting
the structure of soft tissues. Figure 7 (c) and (d) shows the reconstructed images by the LBP and
CG algorithms, respectively. Compared with the X-ray image, the clusters in the two images are badly
expressed since the clusters contain too many trail traces.
The human lung contains approximately ﬁve kinds of tissues according to rough conductivity diﬀerences, as shown in Table 2. Figure 8 summarizes the reconstructed images by the FCET-3 and FCET-16
algorithms for a wide range of m. In this ﬁgure, the tissue details of the human lung can almost be
identiﬁed and are much clearer than those by the LBP and CG algorithms when compared with the
X-ray reference images.
The index of clustering validity is used to estimate the correct number of clusters. Figure 9 shows the

Yue S H, et al.

2862

Sci China Inf Sci

m=1.5

December 2012 Vol. 55 No. 12

m=2.0

m=2.6

m=3.0

FCET-3

FCET-16

Figure 8

Experimental results by the FCET-16 algorithm with a variety of parameter settings.

0.35
VPE
VXB

0.30

0.25

0.20

0.15

0.10

Figure 9

2

3

4

5

6

7

8

9

10 11 12 13 14 15

Estimating the number of clusters using VPE and VXB indices.

graphs of VPE and VXB as c increases, where VXB clearly achieves its minimum at c = 4, whereas VPE
does not achieve a clear and invariant minimum. This case conﬁrms that the diﬀerent validity indices
may have diﬀerent applicable ranges.
After stacking 100 continuous frames of images by the FCET-3 algorithm at 7 frame/s, the human
lung breathing process starts (observed in the video), and the validity index graphs are dynamically
reconstructed accordingly. But, the graphs are not constructed in real-time since the process has to take
all possible values of the number of clusters into consideration and recommend an optimal number to
be used for imaging reconstruction. Compared with the x-ray reference images, the FCET-3 algorithm
robustly reconstructs clearer images than the CG method. Please refer to the video in the attachment.
The execution time for the LBP algorithm is the fastest of the three algorithms; it can reconstruct
about 12 images per second, since image reconstruction is done without any iteration. The CG and
FCET-16 algorithms both depend on necessary iteration processes and thus are much shower than the
LBP algorithm. The CG algorithm can reconstruct, on average, six image frames per second, while
FCET-16 reconstruct three frames of ERT images per second. Thus, since FCET-16 algorithm is the
slowest of the three algorithms, image speed is not one of the advantages of the FCET-16 algorithm.
3) Time resolution. The reconstruction speeds of the CG, LBP, and FCET-3 algorithms are 6 frames/s,
12 frames/s, and 8 frames/s, respectively. The LBP algorithm is the fastest owing to the lack of an
iteration process. In practice, the reconstruction speed for ET imaging should generally be higher than
5 frames/s. Thus, the FCET-3 algorithm easily satisﬁes this requirement.
Finally, we generalize the characteristics of the FCET algorithm as follows. With respect to the time
resolution of reconstructing an image frame, the tested ET imaging algorithms are ordered in increasing

Yue S H, et al.

Sci China Inf Sci

December 2012 Vol. 55 No. 12

2863

time resolution as
LBP ≺ PECT ≺ CG ≺ PECT-16 ≺ Landweber.

(26)

The main advantage of the FECT algorithm over the other imaging algorithms is its high space resolution.
FECT-16 and FCET-3 have their individual applicable ranges. The FECT-16 algorithm can recover
more detail of the investigated objects, and performs better with multiple components and under noisy
conditions. In contrast, the FCET-3 algorithm performs better with materials consisting of only a few
components and little noise. Moreover, the FCET-3 algorithm is faster than the FCET-16 algorithm.

5

Conclusions

In this research, we noted that the challenging issues in existing ET imaging reconstruction mainly lie
in the ill-posed problem formulation. We proposed a novel idea for ET imaging by exploring the images
alone instead of the analytical models in image reconstruction.
We ﬁrst recovered the natural clustering structures hidden in the ET measured data and then applied
fuzzy clustering to the ET imaging. Consequently, we proposed a novel image reconstruction algorithm
known as FCET. Reconstructed images by the FCET algorithm have much higher space resolution over
a wide range of parameter settings. Our simulations and actual experiments demonstrated the feasibility
and eﬀectiveness of the FCET algorithm. It should be noted that some experimental results obtained by
the FCET algorithm cannot be computed by other existing algorithms. This further demonstrates that
the FCET algorithm could lead to new directions in the ET imaging research ﬁeld.
Owing to the promising results in the current study, our future studies will include the following:
1) Using sensitive coeﬃcients in the investigated ﬁeld to generate vectors to cluster. The use of sensitive
coeﬃcients to realize better space resolution has been demonstrated in the existing ET image algorithms.
2) Determining diﬀerent weighting values for diﬀerent components of all vectors. The current research has
shown that the measured data for diﬀerent electrode pairs contain diﬀerent noise-to-signal ratios, and thus
have diﬀerent eﬀects on the ﬁnal ET imaging. Consequently, the use of diﬀerent weights to present these
diﬀerent eﬀects are desirable for enhancing the imaging space resolution. 3) Using diﬀerent clustering
algorithms for ET imaging. Clustering techniques have been developed over several decades, and several
research methods and achievements are rich and eﬃcient. In fact, although the FCM algorithm applied
in this study is the simplest, it is not optimal. In most applications variants of the FCM algorithm have
been applied to obtain better clustering results. We intend further improving the proposed algorithm in
future research.

Acknowledgements
This work is supported by National Science Foundation of China (Grant Nos. 61174014, 60572065, 60772080) and
National Science Foundation of Tianjin (Grant No. 08JCYBJC13800). We would like to thank all the reviewers
for their insightful comments and valuable suggestions.

References
1 Inez F. Electrical impedance tomography (EIT) in applications related to lung and ventilation: a review of experimental
and clinical activities. Physiol Meas, 2000, 21: 1–12
2 Trevor Y. Status of electrical tomography in industrial applications. J Electron Imaging, 1990, 10: 608–619
3 Clay M, Ferree T. Weighted regularization in electrical impedance tomography with applications to acute cerebral
stroke. IEEE Trans Med Imaging, 2002, 21: 629–637
4 William R. EIT reconstruction algorithms: pitfalls, challenges and recent developments. Physiol Meas, 2004, 25:
125–142
5 Marashdeh Q, Fan L, Du B, et al. Electrical capacitance tomography—a perspective. Ind Eng Chem Res, 2008, 47:
3708–3719
6 Du B, Warsito W, Fan L. Imaging the choking transition in gas-solid risers using electrical capacitance tomography.
Ind Eng Chem Res, 2006, 45: 5384–5395

2864

Yue S H, et al.

Sci China Inf Sci

December 2012 Vol. 55 No. 12

7 Yin W, Peyton A J. A planar EMT system for the detection of faults on thin metallic plates. Meas Sci Technol, 2006,
17: 2130–2135
8 Hayt W H, Buck J A. Engineering Electromagnetic, 7th ed. New York: McGraw-Hill, 2006
9 Yang W, Liu S. Electrical capacitance tomography with square sensor. Electron Lett, 1999, 35: 295–296
10 Cao Z, Wang H, Xu L. Electrical impedance tomography with an optimized calculable square sensor. Rev Sci Instrum,
2008, 79: 103710–103719
11 Santosa F, Vogelius M. A back projection algorithm for electrical impedance imaging. SIAM J Appl Math, 1990, 50:
216–243
12 Cheney M, Isaacson D, Newell J C, et al. Noser: an algorithm for solving the inverse conductivity problem. Int J
Imaging Syst Technol, 1990, 6: 266–275
13 Vauhkonen M, Vadasz D, Karjalainen P A, et al. Tikhonov regularization and prior information in electrical impedance
tomography. IEEE Trans Med Imaging, 1998, 17: 285–293
14 Hu L, Wang H X, Zhao B et al. A hybrid reconstruction algorithm for electrical impedance tomography. Meas Sci
Technol, 2007, 18: 813–818
15 Yang W Q, Spink D M, York T A, et al. An image-reconstruction algorithm based on Landweber’s iteration method
for electrical-capacitance tomography. Meas Sci Technol, 1999, 10: 1065–1069
16 Player M A, van Weereld J, Allen A R, et al. Truncated-Newton algorithm for three dimensional electrical impedance
tomography. Electron Lett, 1999, 35: 2189–2191
17 Bikowski J, Mueller J. 2D EIT reconstructions using Calderon’s method. Inverse Probl Imaging, 2008, 2: 43–61
18 Vauhkonen M. Electrical impedance tomography and prior information. Dissertation for the Doctoral Degree. University of Kuopio, 1997
19 Wang M. Inverse solutions for electrical impedance tomography based on conjugate gradients methods. Meas Sci
Technol, 2002, 13: 101–117
20 Polydorides N. Image reconstruction algorithm for soft-ﬁeld tomography. Dissertation for the Doctoral Degree. University of Manchester Institute of Science and Technology, 2002
21 Vogel C R. Computational Methods for Inverse Problems. Philadelphia: SIAM, 2002
22 Xu R, Wunsch D. Survey of clustering algorithm. IEEE Trans Neural Netw, 2005, 16: 645–678
23 Yue S, Wang J, Wu T. A new separation measure to improve the eﬀectiveness of the clustering validation evaluation.
Inf Sci, 2010, 80: 748–764
24 Bezdek J C. Pattern Recognition with Fuzzy Objective Function Algorithms. New York: Plenum Press, 1981
25 Yue S, Wang J, Wu T. A new unsupervised approach to clustering. Sci China Inf Sci, 2010, 189: 1345–1357
26 Wu K, Yang M. Alternative c-means clustering algorithms. Pattern Recognit, 2002, 35: 2267–2278
27 Xie X L, Beni G. A validity measure for fuzzy clustering. IEEE Trans Pattern Anal Mach Intell, 1991, 13: 841–847
28 Taﬂove A, Hagness S C. Computational Electromagnetic: The Finite Diﬀerence Time-Domain Method, 3rd ed. Boston:
Artech House, 2005
29 Ni G Z, Yang S Y, Qian X Y, et al. Numerical Calculation of Engineering Electromagnetic Field (in Chinese). Beijing:
Machinery Industry Press, 2004
30 Murai T, Kagawa Y. Electrical impedance computed tomography based on a ﬁnite element model. IEEE Trans Biomed
Eng, 1985, 32: 177–184
31 Wu K, Yang M. Alternative c-means clustering algorithms. Pattern Recognit, 2002, 35: 2267–2278
32 Zhang D, Chen S. Clustering incomplete data using kernel-based fuzzy c-means algorithm. Neural Process Lett, 2003,
18: 155–162
33 Yang W Q, Spink D M, York T A , et al. An image-reconstruction algorithm based on Landweber’s iteration method
for electrical-capacitance tomography. Meas Sci Technol, 1999, 10: 1065–1069
34 Soleimani M, Lionheart W. Nonlinear image reconstruction for electrical capacitance tomography using experimental
data. Meas Sci Technol, 2005, 16: 1987–1996

Expert Systems with Applications 37 (2010) 4955–4965

Contents lists available at ScienceDirect

Expert Systems with Applications
journal homepage: www.elsevier.com/locate/eswa

Mix-ratio sampling: Classifying multiclass imbalanced mouse brain images
using support vector machine
Min Hyeok Bae, Teresa Wu *, Rong Pan
Department of Industrial, Systems and Operations Engineering, Arizona State University, Tempe, Arizona 85287-5906, USA

a r t i c l e

i n f o

Keywords:
Sampling procedure
Imbalanced dataset
Multiclass classiﬁcation
Support vector machine
Data mining
Brain image segmentation

a b s t r a c t
Support Vector Machine (SVM) is a classiﬁer designed to achieve optimized classiﬁcation accuracy. It has
been applied to numerous applications associated with images. Yet challenges remain when applying
SVM on segmenting mouse brain images. This is due to the fact that each high-resolution mouse brain
image is a very large data set and it is a multiclass classiﬁcation problem with extremely imbalanced data
size for different classes. To address these issues, a mix-ratio sampling approach for SVM is proposed
which determines various over-sampling ratios for different minority classes. In addition, to improve
the imaging classiﬁcation accuracy, spatial information is incorporated into the classiﬁcation problem.
Five mouse Magnetic Resonance Microscopy (MRM) images are collected to test the accuracy of classifying 21 brain structures. The ﬁrst comparison experiment demonstrates the SVM with mix-ratio sampling
method relieves the imbalance problem for multiclass more effectively and efﬁciently than the SVM with
simple over-sampling method. In the second comparison experiment, another classiﬁer, Artiﬁcial Neural
Network (ANN) is used to compare against SVM based on the same mix-ratio sampled data and the
results indicate that SVM shows better classiﬁcation performance than ANN. Thirdly, the cross validation
is conducted to demonstrate SVM with mix-ration sampling can classify multiclass imbalanced data with
high accuracy.
Published by Elsevier Ltd.

1. Introduction
Progresses in the ﬁeld of medical imaging technologies such as
Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Computed Tomography (CT), etc. have enabled one to
non-invasively delineate anatomical structures of a subject (e.g.,
human brain, mouse brain). One major advantage of accurate and
precise delineation of neuroanatomical structures in the case of
human brain is that it gives strong foundation to help in the early
diagnosis of a variety of neurodegenerative disorders, as it is found
that neurodegenerative disorders are frequently related with
structural changes in the human brain (Fischl et al., 2002). For
example, the cause and progression of Alzheimer’s Disease (AD)
is associated with the abnormal accumulation of proteins forming
neuroﬁbrillary tangles and amyloid plaques in the brain, and this
results in volumetric changes in medial temporal lobe structures,
including the hippocampus and the parahippocampal gyrus (Busatto et al., 2003). To accurately capture the volumetric changes,
there is a need to segment brain structures precisely. Currently,
most segmentation practices on brain images are done by trained
anatomists or technician manually. Because of the large amount
* Corresponding author. Tel.: +1 480 965 4259; fax: +1 480 965 8692.
E-mail addresses: teresa.wu@asu.edu (T. Wu), rong.pan@asu.edu (R. Pan).
0957-4174/$ - see front matter Published by Elsevier Ltd.
doi:10.1016/j.eswa.2009.12.018

of data from high-resolution images, the execution time of the
manual segmentation could take several days, which makes it
impractical for high-throughput brain image analysis and delays
the research of brain disease and ﬁnding the cures. Therefore, there
have been increasing demands for automated segmentation methods which can delineate the neuroanatomical structures of human
brains more efﬁciently and reliably.
Several different automated segmentation methods have been
developed: atlas based segmentation, probabilistic information
based segmentation and machine learning-based segmentation.
The atlas based segmentation method is to non-linearly register
an image to a manually labeled atlas image (Hartmann, Parks, Martin, & Dawant, 1999; Heckemann, Hajnal, Aljabar, Rueckert, &
Hammers, 2006; Iosifescu et al., 1997). The nonrigid transformation maps the labels of each voxel in the atlas image to the image
being segmented. The probabilistic information based approach
uses probabilistic information extracted from training datasets of
MR images for the segmentation (Ali, Dale, Badea, & Johnson,
2005; Fischl et al., 2002). Probabilistic information of MR intensity,
prior probability of a label of a voxel at a location in a 3D images
and pairwise probability of a labeling given labels of neighboring
voxel are incorporated to predict a label of the voxel. The machine
learning based segmentation use various classiﬁers to classify each
voxel in a brain image into a number of classes using various MR

4956

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

intensity information as input features. ANN was used to segment
brain images into the three tissues of white matter, gray matter
and cerebrospinal ﬂuid (Reddick, Glass, Cook, Elkin, & Deaton,
1997). Powell et al. (2008) demonstrate that machine learning
methods, such as Support Vector Machine (SVM) and ANN, outperform the template or probability-based methods in segmentation
of human brain MR images. They segmented MR images of human
brains into eight brain structures.
Even though the machine learning based segmentation method
has been successfully used for automated segmentation, it still has
some limitations to be applied to brain image segmentation problems which have high-resolution images as data sets and a large
number of neuroanatomical structures to be segmented, such as
mouse brain images. The multiclass classiﬁcation is a challenging
problem, especially when the data size is extremely large and sizes
of classes are imbalanced. In fact, most classiﬁers perform well for
classifying balanced data, but lose their classiﬁcation power when
dealing with imbalanced data (Li, 2007). The sizes of neuroanatomical structures of human or animal brains vary dramatically.
For example, each mouse brain MRM image used for this study
has over 4 million (128  128  256) voxels. The number of voxels
of Cerebral Cortex structure is over 500 K, while that the number of
voxels of Interpeduncular Nucleus structure is only 1600. In addition, 21 neuroanatomical structures to be segmented in this study
are considerably large number of classes comparing to other multiclass data mining applications. Another limitation for applying
the machine learning based segmentation method is class overlapping. The MR signals from each anatomical structure often overlap
with each other which make it even difﬁcult to get distinct intensity information to classify the neuroanatomical structures. As stated by Fischl et al. (2002), it appears that a global segmentation
method which can effectively segment the structures using only
the MR intensity information does not exist. These limitations have
made the machine learning based segmentation difﬁcult to be applied to automated brain image segmentation problems. Therefore,
it is necessary to develop an effective classiﬁcation approach which
is capable to handle multiclass, imbalanced data with overlapped
MR signals. In this research, we introduce a mix-ratio sampling
method to handle the imbalanced multiclass problem. This method
is a supervised sampling method, which ﬁrst identiﬁes minority
classes followed by determining different sampling ratios for the
minority classes. Secondly, the location information (spatial information) of each voxel in the MR brain image is added as features to
improve the classiﬁcation performance.
In this study, we introduce an automated segmentation method
for mouse brain using a data mining technique, SVM, which in general is linear classiﬁer aiming to locate the hyberplane that maximally separates the classes. Recently SVM has received a lot of
attention from the machine learning community due to the following reasons. First, SVM has the good generalization ability. Unlike
the artiﬁcial neural network (ANN) method, which is to minimize
the sum-of-square error between the output and the target label
so its performance depends on a large size of the training data
set, SVM attempts to maximize the margin between the classes,
so the generalization performance does not drop signiﬁcantly
when the training data are scarce (Abe, 2005). Secondly, by using
kernel functions, SVM can map the input space into a high-dimensional feature space. Therefore, SVM can classify objects which are
not linearly separable. In addition, since SVM is solved with quadratic programming, it can achieve a global optimal solution. As
summarized by Shin and Cho (2006), SVM has accomplished great
success in a variety of applications including handwritten character recognition, object detection and recognition, text classiﬁcation, fault detection, fraud detection, etc. In the ﬁeld of medical
image classiﬁcation, Nattkempera et al. (2005) use SVM in the
breast tumor classiﬁcation and ﬁnd that SVM performs better than

K Nearest Neighbor (KNN) and Decision Tree (DT) classiﬁer. Brain
tumor recognition using SVM is studied by Luts, Heerschap, Suykens, and Huffel (2007) and they conclude SVM provides better
classiﬁcation accuracy comparing to the Linear Discriminant Analysis (LDA). SVM also outperforms the Fisher Linear Discriminant
(FLD) classiﬁer in classiﬁcation of brain states from whole functional magnetic resonance imaging (fMRI) volumes (Mourao-Miranda, Bokde, Born, Hampel, & Stetter, 2005), and presents better
results than neural network Self-Organizing Maps (SOMs) in classifying normal and abnormal human brain MR images (Chaplot,
Patnaik, & Jagannathan, 2006).
Even though SVM had better classiﬁcation ability than any
other classiﬁers, it has some weak points (Abe, 2005). First, it is
hard to extend to multiclass problem as it was initially designed
to identify the decision function for binary class. Secondly, existing
concerns for SVM are the scalability and computational efforts required in training and testing. Since training of SVM is associated
with solving dual problem which has the same number of variables
with the number of training data, it is intractable to apply to large
scale problems. The third weak point is the choice of kernels and
their parameters. The generalization ability of SVM depends on
selection of kernels and their parameters. Since the selection is
done by monitoring generalization abilities of various different
SVM models, it is a hard task to choose good kernel and their
parameters for the large scale problem. Finally, SVM does not perform well for imbalanced data like most of classiﬁers. Therefore, it
is not straightforward to apply SVM to the brain image segmentation to be addressed in this paper since it is an imbalanced multiclass classiﬁcation problem with large scale datasets. In this
research, mix-ratio sampling is proposed to enables SVM to be
applicable to the imbalanced multiclass problem. We compare
the mix-ratio sampling method with the simple over-sampling
method to demonstrate the effectiveness of SVM on mix-ratio sampling in solving multiclass imbalanced problems. We then compare
SVM with another classiﬁer, ANN, using the same data set which is
sampled by the mix-ratio sampling method to see which classiﬁer
is appropriate to the imbalanced multiclass problem. Cross validation is further conducted to illustrate the general accuracy can be
achieved by the proposed method.
This paper is structured as following: Section 2 reviews SVM basics and several SVM classiﬁers for multiclass classiﬁcation. Section
3 describes the methods for handling the class imbalance problem
and class overlapping problem, including the mix-ratio sampling
approach. The mouse brain image dataset and the automated segmentation procedure that we implemented are described in Section 4 following by three sets of experiments. In Section 5, we
compare the two sampling methods, simple over-sampling and
mix-ratio sampling, to show the superiority of the proposed sampling method, report the performance comparison of SVM and
ANN and provide the 5-fold cross validation testing result of mouse
brain segmentation. Finally, conclusion and future work are provided in Section 6.

2. SVMs for classifying multiple imbalanced classes
2.1. Support vector machines basics
The basic idea of SVM is to construct an optimal hyperplane
which gives the maximum separation margin between two classes.
In a binary classiﬁcation problem, let us consider a training set
xi 2 Rn with its label set yi 2 fþ1; 1g, where yi ¼ þ1 for positive
class and yi ¼ 1 for negative class, for all the training data i = 1,
2, . . ., m, where m is the number of the data and n is the dimension
of the data. The hyperplane f(x), that separates the given data, is
deﬁned as:

4957

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

f ðxÞ ¼

m
X

wi xi þ b ¼ wT x þ b;

ð1Þ

i¼1

where w is the n dimensional weight vector, b is a bias term. The w
and b describe the shape and position of hyperplane. The distance
between a hyperplane and the datum nearest to the hyperplane is
the margin which is calculated by 2=kwk. SVM training involves
ﬁnding the optimal hyperplane which has the maximum margin,
subject to the following constraint:

yi ðwT xi þ bÞ P 1 for i ¼ 1; . . . ; m:

s:t:

m
X
1 T
ni
w wþC
2
i¼1

ð3Þ

yi ðw  Uðxi Þ þ bÞ P 1  ni ; i ¼ 1; . . . ; m;
ni P 0;

i ¼ 1; . . . ; m:

where C is the penalty parameter,n ¼ fn1 ; . . . ; nm g is a slack variable.
The penalty parameter (C) controls the trade-off between the model
complexity and classiﬁcation error. A larger C usually leads to higher training accuracy, but may over-ﬁt the training data. The nonnegative slack variable ðni Þ allows Eq. (3) to always yield feasible
solutions even when the optimal hyperplane does not have the
maximum margin. Uðxi Þ is a non-linear transformation which projects the samples into a higher-dimensional dot-product space
called the feature space.
By introducing the non-negative Lagrangian multiplier ai and bi ,
we convert the constrained problem given by Eq. (3) into an
unconstrained problem as shown in Eq. (4):

Min Lðw; b; n; a; bÞ ¼

1 T
w wþC
2

m
X

Kðxi ; xj Þ ¼ expðckxTi  xj k2 Þ; c > 0;

ni 

m
X

i¼1

ai ðyi ðwT  Uðxi Þ

i¼1

þ bÞ  1 þ ni Þ 

where c in Eq. (8) is a parameter related to the span of an RBF kernel. The smaller the value is, the wider the kernel spans.
2.2. SVM for multiclass classiﬁcation

m
X

ð4Þ

bi ni ;

To extend the application of SVM for multiclass classiﬁcation, a
number of methods have been developed and they mainly fall in
three categories: One-Against-All (OAA), One-Against-One (OAO)
and All-At-Once (AAO). In OAA method, one SVM is trained with
the positive class representing one class and the negative class representing the others. Therefore, it builds n different SVM models
where n is the number of the classes. Given yi 2 f1; . . . ; ng, the
kth SVM solves the following problem:
k

Min Q P ðwk ; b ; nk Þ ¼
s:t:

m
X
1 k T k
nki
ðw Þ ðw Þ þ C
2
i¼1
k

ðwk ÞT  Uðxi Þ þ b P 1  nki ; if yi ¼ k;
k

k T

ðw Þ  Uðxi Þ þ b 6 1 þ
nki

where a ¼ fa1 ; . . . ; am g and b ¼ fb1 ; . . . ; bm g. For the optimal solution of Eq. (4), the Karush–Kuhn–Tucker (KKT) condition is satisﬁed
as follows:

@Q ðw; b; n; a; bÞ
@Qðw; b; n; a; bÞ
@Q ðw; b; n; a; bÞ
¼ 0;
¼ 0 and
¼ 0;
@w
@b
@n
ai ðyi ðwT  Uðxi Þ þ bÞ  1 þ ni Þ ¼ 0; bi ni ¼ 0; ai ¼ 0;
bi ¼ 0 and ni ¼ 0 for i ¼ 1; . . . ; m:
ð5Þ
By substituting the conditions of Eq. (5) into Eq. (4), we can convert
Eq. (4) to the following Lagrangian dual problem:

Max LðaÞ ¼

i¼1

s:t:

m
1X
ai 
ai aj yi yj Uðxi ÞT Uðxj Þ
2 i;j¼1

ð6Þ

Solving Eq. (9) yields a decision function for the kth SVM model:
k
ðwk ÞT  Uðxi Þ þ b . Totally, n decision functions will be derived. The
class of a sample, x, is predicted by the following equation:
k

f ðxÞ ¼ arg max ððwk ÞT  UðxÞ þ b Þ:

The optimal solution ai  for the dual problem determines the

parameters w and b of the following optimal hyperplane

¼ sign

m
X

yi ai Kðx; xi Þ þ b



m
X

!

T

i UðxÞ Uðxi Þ

yi a

þb



i¼1

!
;

ð10Þ

k¼1;...;n

In OAO method, a SVM is trained to classify the kth class and the
lth class. Therefore, it constructs nðn  1Þ=2 SVM models. To build
the SVM model for the kth class and the lth class, the following
problem is solved:
kl

Min Q P ðwkl ; b ; nkl Þ ¼
s:t:

m
X
1 kl T kl
nkl
ðw Þ ðw Þ þ C
i
2
i¼1
kl

ðwkl ÞT  Uðxi Þ þ b P 1  nkl
i ; if yi ¼ k;
kl

kl T

ðw Þ  Uðxi Þ þ b 6 1 þ

nkl
i ;

ð11Þ

if yi ¼ l;

nki P 0; i ¼ 1; . . . ; m and k; l ¼ 1; . . . ; n:
The idea of AAO is similar to that of OAA, but it determines n
decision functions at once where the kth decision function separates the kth class from the other classes. We deﬁne the decision
function for class k by:
k

j

ðwk ÞT  UðxÞ þ b P ðwj ÞT  UðxÞ þ b for k – j; k ¼ 1; . . . ; n:

Min Q ðw; b; nÞ ¼
s:t:

f ðxÞ ¼ signðw  Uðxi Þ þ b Þ ¼ sign

if yi – k;

n
m X
X
1X
ðwk ÞT ðwk Þ þ C
nki
2 k¼1
i¼1 k–y
i

i¼1



ð9Þ

ð12Þ

All the optimal decision functions are solved by the one problem given by:

0 6 ai 6 C; i ¼ 1; . . . ; m:
m
X
ai yi ¼ 0; i ¼ 1; . . . ; m:



nki ;

P 0; i ¼ 1; . . . ; m; and k ¼ 1; . . . ; n:

i¼1

m
X

ð8Þ

ð2Þ

The optimal hyperplane is obtained by solving the following optimization problem:

Min

that we can calculate the inner product, UðxÞT Uðxi Þ, easily. In this
research, Radial Basis Function (RBF) is used which is deﬁned as
follows:

ð7Þ

i¼1

where Kðxi ; xj Þ is a kernel function deﬁned as Kðxi ; xj Þ ¼ Uðxi ÞT Uðxj Þ.
The kernel function performs the non-linear mapping implicitly so

y

k

ðwyi ÞT  Uðxi Þ þ b i P ðwk ÞT  Uðxi Þ þ b þ 2  nki ;

ð13Þ

nki P 0; i ¼ 1; . . . ; m; k – yi ; k ¼ 1; . . . ; n:
Clearly OAA needs to build n classiﬁers and OAO will develop
nðn  1Þ=2 models while AAO requires only one model. Yet, it is relatively difﬁcult to separate the training data for OAA and AAO comparing to OAO (Abe, 2005). Hsu and Lin (2002) compared the
training and testing times and classiﬁcation accuracies of the three
multiclass SVM methods based on various datasets. They reported
that the training time of OAO method is faster than the OAA method and the AAO method. The experimental results on large scale

4958

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

datasets showed that OAO is more efﬁcient for practical use than
OAA and AAO method. It is much more challenging to solve a multiclass problem which has imbalanced and overlapped class without an optimal SVM classiﬁer for multiclass problem.

3. Mix-ratio sampling for SVM
3.1. Class-balancing
To improve the performance of machine learning methods for
the imbalanced class classiﬁcation problem researchers have used
two different approaches: sampling approach and variant classiﬁers approach.
Sampling approach aims to sample training sets with different
ratios for the majority class and the minority class so as to boost
(suppress) the size of the training data from the minority (majority) class. There are different ways to draw samples from the original dataset: random sampling refers to randomly extract the
samples from the original dataset and informative sampling takes
the samples based on different rules speciﬁed for a speciﬁc problem. Other than sampling strategies, sampling approaches can also
be classiﬁed based on the number of samples taken from the
majority and minority classes. Three sampling approaches are
studied in the literature including under-sampling, over-sampling
and combination of over-sampling and under-sampling. Undersampling aims to reduce the size of a majority class. Over-sampling
is an approach used for minority classes by randomly duplicating
the existing data (Japkowicz, 2000) or creating synthetic data (Chawla, Bowyer, Hall, & Kegelmeyer, 2002). It is interesting to note
that Japkowicz (2000) reports that the over-sampling method in
general outperforms the under-sampling from the experimental
results. Still, research has also looked into the combination of under-sampling and over-sampling. Ling and Li (1998) obtain the
best lift index, which is a performance measure, when the sampling points of the majority class and minority class are the same.
Shin and Cho (2006) propose an informative sampling, neighborhood property based pattern selection algorithm (NPPS), to select
only data located around decision boundary because the data have
the most information on determining the decision boundary in
SVM. They can substantially reduce the number of training data
and the training time while maintaining the classiﬁcation
performance.
Other than sampling approach, researchers have studied how to
increase the sensitivity of the SVM classiﬁer to the minority classes. For example, Veropoulos, Campbell, and Cristianini (1999) employ different error costs in the objective function for majority
classes and minority classes. Wu and Chang (2005) propose a kernel-boundary-alignment algorithm (KBA) which adjusts the class
boundary by adaptively modifying the kernel matrix based on
the imbalanced training data distribution.
The approaches explained above have shown their efﬁciency
and effectiveness in dealing with the imbalanced dataset. However, these approaches are designed for binary classes where there
is only one majority class and one minority class. In multiclass
classiﬁcation problem, especially when the number of classes is
large, applying similar sampling procedures designed for the binary class problem is questionable. This is probably due to the fact
that it is difﬁcult to differentiate majority versus minority classes.
Assume there are n classes, the largest class and the smallest class
can be regarded as the majority class and the minority class,
respectively, then the rest n  2 classes can be either majority or
minority. Therefore, there are 2n2 possible cases of dividing the
classes into two groups. Simply applying the over-sampling or under-sampling techniques reviewed above with different sampling
ratios for the 2n2 possible cases is computational expensive and

intractable. Therefore, there is a need of intelligent sampling which
can provide an efﬁcient and effective sampling procedure in the
multiclass problem.
Akbani, Kwek, and Japkowicz (2004) point out that under-sampling of the majority class can hurt the classiﬁcation performance
because it discards valid data points which contain important
information. Therefore, our approach focuses on the over-sampling
procedure. While maintaining the number of the training data
from majority classes the same, our objective is to increase the
number of the training data from minority classes. Now the questions are which classes are the true minority classes, and what
sampling ratios are appropriate to each minority class, respectively. To determine the minority class and the appropriate sampling ratio, one baseline classiﬁcation on the original dataset is
conducted, followed by a number of classiﬁcations on the dataset
generated from different sampling ratios (e.g., 5 K, 10 K data size).
The classiﬁcation performances of each class with different sampling ratios are then compared with the baseline classiﬁcation.
The classes which have better performance from the sampled
training set than the original training set are identiﬁed as the
minority classes and the best sampling ratio among the experiments for the speciﬁc structure is recorded and used for the classiﬁcation in the following steps. On the other hand, the classes with
no performance improvement with different sampling ratios are
treated as majority classes. The original dataset for such classes
are then wholly used in SVM. Therefore, the selected sampling ratio of each minority class varies and it is termed mix-ratio
sampling.
As stated before, the best performance can be achieved when
the number of data from the majority class and the number of data
from the minority class are same (Ling & Li, 1998). Due to the large
size of the dataset and the large number of classes, we decide to
draw the same number of samples from different structures for
the speciﬁc sampling ratio. Thus, it is true that some classes will
be over sampled while others might be under sampled. However,
this will not lessen the classiﬁcation power as the ones under sampled will use the original dataset in the SVM model. The pseudocode of the mix-ratio sampling algorithm is presented in Fig. 1. It
initializes the training set (S) and the set of the sampling ratios
(N) (lines 2–4). A multiclass SVM is then implemented on the original dataset and its classiﬁcation performance for each class is
kept as the baseline classiﬁcation ðMB Þ (lines 5–8). Next, different
SVM models with different sampling ratios are implemented. The
training set (S) is updated by drawing the same number of samples
from each class according to the different ratios (N(i) = 1 k, 5 k,
10 k, 15 k and 20 k) (lines 10–11). Performance matrix M(i,j) is
used to record the classiﬁcation result for class j using SVM model
with sampling ratio i (lines 12–15). Finally, minority classes and
their over-sampling ratios are determined (lines 17–24). For each
class, the best performance (max(M(i,j)) among the different SVM
models i is compared with the baseline classiﬁcation ðMB ðjÞÞ). If
the best performance (max(M(i,j)) for a class is better than the
baseline classiﬁcation ðMB ðjÞÞ), then the class is determined as a
minority class and its sampling ratio (N(i)) is used as its over-sampling ratio. Otherwise, the class is determined as a majority class.
3.2. Class-overlapping
When applying SVM to MR image segmentation, another critical issue is the overlapping of MR signals. Consider a MR image
protocols, T2-weighted. Fig. 2 illustrates the histograms of the
T2-weighted MRM intensities for seven different neuroanatomical
structures in a mouse brain such as Cerebral cortex (CORT), Cerebellum (CBLM), Midbrain (MIDB), Thalamus (THAL), Olfactory bulb
(OLFB), Hippocampus (HC) and Cerebral peduncle (CPU). There are
serious overlaps among the T2-weighted intensities from the dif-

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

4959

Fig. 1. Mix-ratio sampling algorithm.

Fig. 2. Intensity histograms for seven neuroanatomical structures.

ferent structures. Hence, it appears difﬁcult to effectively segment
the neuroanatomical structures of mouse brains using only the MR
intensity information.
To solve the class overlapping problem in MR image segmentation, the spatial information has been used by constructing a probabilistic atlas (Evans, Kamber, Collins, & MacDonald, 1994;
Thompson & Toga, 1996). The goal of constructing a probabilistic
atlas is to create neuroanatomical templates and investigate quan-

titative information on inter-subject variations in neuroanatomical
structures. The probabilistic atlas describes the statistical properties of neuroanatomical structures on each coordinate in MR
images. Fischl et al. (2002) explain how the spatial information
could help solve the class overlapping problem. They claim that
the number of possible neuroanatomical structures at a given atlas
coordinate is relatively small and the relative location of each
structure in the atlas is characteristic (e.g. the substantia nigra

4960

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

(SNR) is anterior to the cerebral peduncle (CPED)). Therefore, information on global position in the atlas could help MR image
segmentation.
Nevertheless, the atlas based segmentation has some drawbacks. Because of the inherent variability of the brains in a population, some structures can be blurred out in the resulting average
brain (Thompson & Toga, 1996). The registration can be biased to
the individual anatomy of the brains in the population because
only a single average brain is used (Sharief, Badea, Dale, & Johnson,
2008). Since the approach depends on the statistical properties of
neuroanatomical structures on each coordinate, the classiﬁcation
performance heavily depends on the accuracy of registration process. The traditional SVM classiﬁer assumes that all the data are
independent of each other. The spatial information is not facilitated by the SVM framework. In our study, we will incorporate
the spatial information into the SVM classiﬁer. We add the x, y
and z coordinates of each data point in 3D MR images as features.
Even though the intensity features from some classes overlap each
other, the three location features from those classes are surely different; therefore, adding the location features can aid in differentiating overlapped classes. Because the spatial information is used as
features, we can easily implement it in any classiﬁer. As discussed
earlier, the probabilistic atlas based segmentations use the spatial
information of absolute position in MR images. Hence, registration
error can cause failure of the segmentation process. In this research, both the MR signal intensity and the voxel coordinates
are used as features, so it is robust to inaccurate registration.
4. Mix-ratio sampling based SVM for mouse brain segmentation
4.1. Dataset
The dataset used in this study was provided by the Center for In
Vivo Microscopy in Duke University Medical Center. This dataset
was also previously studied in Ali et al. (2005). Five formalin-ﬁxed
C57BL/6J male mice of approximately 9 weeks in age were used.
The MRM image acquisition consisted of isotropic 3D T2-weighted
scans. MRM is MRI at a microscopic level with ultra-high spatial
resolution. Typical spatial resolution is about 100 lm per voxel.
The matrix sizes of the images are 128  128  256. A 9-parameter
afﬁne registration was applied to each mouse brain. Manual labeling of 21 neuroanatomaical structures was done by two experts
using T2-weighted datasets of the ﬁve mouse brains. Table 1 presents the list of the 21 neuroanatomaical structures and abbreviations to be segmented in this work. These labelings were used as
true labeling for each voxel.

Table 1
21 segmented structures and abbreviations.
Structures

Abbreviation

Structures

Abbreviation

Cerebral cortex
Cerebral peduncle
Hippocampus
Caudate putamen
Globus pallidus
Internal capsule

CORT
CPED
HC
CPU
GP
ICAP

AC
CBLM
VEN
PON
SNR
INTP

Periacqueductal
gray
Inferior colliculus
Medulla oblongata
Thalamus
Midbrain

PAG

Anterior commissure
Cerebellum
Ventricular system
Pontine nuclei
Substantia nigra
Interpeduncular
nucleus
Olfactory bulb
Optic tract
Trigeminal tract
Corpus callosum

OPT
TRI
CC

INFC
MED
THAL
MIDB

OLFB

ference in spatial location of the two labelings can cause signiﬁcant
decreases in the numerator of VOP i ðLA ; LM Þ, this performance index
is more sensitive to the spatial difference of the two labelings than
the volumetric difference. The volume difference percentage for
class i is used for quantifying the size difference of the structures
delineated by the two segmentations, and it is deﬁned as:

VDPi ðLA ; LM Þ ¼

jVðLA Þ  VðLM Þj
 100%:
ðVðLA Þ þ VðLM ÞÞ=2

ð15Þ

This performance index is the smaller the better. When all the labelings from the two segmentations are identical, VDPi ðLA ; LM Þ is 0.
Usually in the imbalanced classiﬁcation problem, the accuracies
of the large classes are better than those of the small classes.
Therefore, the overall accuracy can be very high even though the
accuracies of the small classes are poor. For example, considering
an imbalanced multiclass classiﬁcation problem which has three
classes, the ratio of the number of data points in each class is
97:2:1. If a classiﬁer predicts all the test data as the majority class,
its accuracy is 97%. Such performance measure is questionable
since it obviously gives more considerations on larger structures.
To resolve this problem, the overall performance should reﬂect
equally on performances of each class regardless its size. This proposed measurements, average_VOP and average_VDP, consider different structures equally, and they are deﬁned as:

Av erage VOPðLA ; LM Þ ¼
Av erage VDPðLA ; LM Þ ¼

Pn

i¼1 VOP i ðLA ; LM Þ

;
n
i¼1 VDP i ðLA ; LM Þ
;
n

Pn

ð16Þ

where n is the number of the neuroanatomical structures.

4.2. Performance evaluation

4.3. SVM models with different sampling strategies

In order to validate the proposed automated segmentation procedure, the volume overlap percentage (VOP) and volume difference percentage (VDP) (Ali et al., 2005; Fischl et al., 2002) are
calculated to compare the automated delineation with the manual
delineation of each structure. Denote LA and LM as labeling of the
structure i by automated segmentation and manual segmentation,
respectively, and V(L) as a function which gives the volume of the
labeling. The volume overlap percentage for class i is deﬁned as:

The experiment is implemented in a sequence of ﬁve steps: (1)
Find the best parameters for SVM models, (2) train different SVM
models using the original training set and the sampled training
sets, (3) compare each class’s performance between the original
training set and the sampled training sets and determine the
minority classes and their sampling ratios, (4) train a SVM model
with the mix-ratio sampling, and (5) test and validate the
performance.

VOPi ðLA ; LM Þ ¼

VðLA \ LM Þ
 100%:
ðVðLA Þ þ VðLM ÞÞ=2

ð14Þ

So, it is the ratio of the volume where automated labeling agrees
with manual labeling and the average volume of automated labeling and manual labeling. This performance index is the larger the
better. When all the labelings from the automated and manual segmentation are coincided, VOP i ðLA ; LM Þ is 100%. Because a slight dif-

4.3.1. Designing SVM classiﬁer
A training set and a testing set are used for ﬁnding the best
parameters for the SVM models which will be used in this study.
The training set consists of the randomly chosen three mice and
the testing set consists of the rest two mice. Because of the limitation of the PC memory, we discarded the blank voxel from the data
set in this study to reduce the size of the data set. The RBF kernel is

4961

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965
Table 2
Summary of the 11 SVM models.
Model

Overall
sampling
ratio

Number of
under-sampled
classes

Number of
over-sampled
classes

R-1k and I-1k
R-5k and I-5k
R-10k and I-10k
R-15k and I-15k
R-20k and I-20k
O-SVM

4.55%
22.76%
45.52%
68.30%
91.04%
No sampling

19
12
9
9
8
0

2
9
12
12
13
0

chosen based on our preliminary experiment. The penalty parameter (C) and the RBF parameter ðcÞ should be determined for the
RBF kernel based SVM. The best parameters is determined through
a grid search over the range C = {10, 100, 500, 1000, 5000, 10,000}
and c ¼ f1; 10; 50; 100g. The combination of the C = 1000 and
c ¼ 10 is found to be the best parameter setting. In this study, LibSVM for Matlab (Chang & Lin, 2001) is used.

4.3.2. Investigating SVM performance with various sampling ratios
To ﬁnd better sampling procedure for this problem, we compare
a random sampling and an informative sampling. The informative
sampling used in our study is based on the rationale that the data
located around the boundary have the most information on determining the decision boundary in SVM. We pick only the data points
that have at least one different class’s neighbor among the k-nearest neighbors. The data points which have neighbors from different
class must be located around the boundary.
Table 2 summarizes different SVM models implemented including sampling ratios, the numbers of under-sampled and over-sam-

pled class and the numbers of training data for the different SVM
models. Taking R-1k and I-1k as an example, R-1k model represents the SVM model trained with the combination of random
over-sampling and random under-sampling, and its training set
has randomly sampled 1000 data points from each class. I-1k model indicates the SVM model trained with the training set which has
the sampled 1000 data points from each class with the informative
sampling procedure. The number of data used in the training set of
the R-1k or I-1k model corresponds to 4.55% of the original dataset.
Sampling 1000 data points from each class results in the two oversampled classes and the 19 under-sampled classes. Different sampling ratios will draw 22.76%, 45.52%, 68.30% and 91.04% for 5 k,
10 k, 15 k and 20 k respectively. Since the size of the dataset will
be larger than the original when the sampling ratio is greater than
20 k, considering the computation effort, 20 k is the maximum
sampling ratio used in this study. O-SVM model indicates the
SVM model trained with the original dataset. Using the best
parameters (C = 1000 and c ¼ 10) determined in Step 1, 11 different SVM models are studied (shown in Table 2).
Table 3 shows the testing result which is the average value of
the two testing mouse brain images. First, we can conclude that
the O-SVM model gives the best performance among all SVM models. As discussed previously, under-sampling of majority classes
can cause a negative effect on the performances of those classes.
This result conﬁrms that the negative effect of the under-sampling
on the overall performance is larger than the positive effect of the
over-sampling of minority classes. Secondly, we ﬁnd that random
sampling is more effective than informative sampling in this case,
which implies that information obtained from only the boundary
region is not enough to build the multi-hyperplanes in the imbalanced multiclass problem. This is probably due to the large number
of classes to be classiﬁed and a number of classes have very small

Table 3
The test performances of the eleven different SVM models.

Average_VOP
Average_VDP

R-1k

R-5k

R-10k

R-15k

R-20k

I-1k

I-5k

I-10k

I-15k

I-20k

O-SVM

64.12
29.57

66.23
21.59

66.37
21.03

66.85
20.57

66.66
20.17

54.56
33.64

56.36
30.54

58.12
28.36

57.52
32.56

57.88
29.80

68.55
19.21

Table 4
The test performances of each structure of the SVM models and mix-ratio.
Structure

O-SVM

R-5k

R-10k

R-15k

R-20k

Sampling ratio (%)

VOP

VDP

VOP

VDP

VOP

VDP

VOP

VDP

VOP

VDP

CORT.
CPED
HC
CPU
GP
ICAP
PAG
INFC
MED
THAL
MIDB
AC
CBLM
VEN
PON
SNR
INTP.
OLF.
OPT
TRI
CC

93.21
52.34
84.08
84.21
64.95
58.13
69.62
77.74
89.25
90.53
87.41
31.19
95.28
46.00
69.67
39.35
43.56
90.73
46.76
66.10
59.36

2.64
6.72
4.57
3.27
10.32
22.55
36.78
13.40
13.92
3.94
1.51
20.04
1.52
42.91
17.42
73.45
76.54
6.27
18.04
12.42
15.10

88.42
49.02
82.76
79.30
58.02
56.41
72.04
78.14
88.51
88.30
84.90
26.25
92.28
41.61
65.43
46.69
45.91
88.37
42.67
63.41
52.47

11.36
13.00
4.28
16.02
23.01
21.43
13.00
19.26
5.93
1.22
8.13
52.85
3.89
29.54
10.67
33.94
58.16
6.29
34.22
24.34
62.97

90.36
46.24
81.99
80.04
59.00
54.97
74.58
78.82
88.01
88.66
84.61
28.70
93.68
44.04
65.80
40.69
43.19
89.81
42.82
62.77
54.95

6.37
23.41
4.60
10.43
14.73
37.51
3.28
10.53
12.58
0.66
9.33
55.45
2.78
15.93
5.37
48.61
64.87
5.34
35.47
29.37
45.02

91.25
49.11
82.67
81.42
63.10
53.94
70.33
78.98
88.48
89.72
86.06
27.32
94.68
45.46
65.47
39.12
41.48
91.08
42.99
65.30
55.83

5.33
13.19
6.74
5.63
21.55
35.38
11.53
8.81
10.03
1.03
3.48
62.23
3.63
7.64
4.83
46.65
67.84
5.90
43.24
26.20
41.10

91.34
48.36
82.50
81.06
61.16
54.13
68.29
79.29
88.50
89.67
85.35
27.74
93.91
44.30
65.83
48.20
36.72
90.78
40.93
64.84
56.88

4.39
13.72
6.36
3.54
25.31
33.89
6.40
19.38
10.37
0.52
6.06
68.30
2.54
14.17
4.90
23.93
81.28
7.24
33.94
21.35
36.08

Average

68.55

19.21

66.23

21.59

66.37

21.03

66.85

20.57

66.66

20.17

Proposed approach
VOP

VDP

–
–
–
–
–
200
300
300
–
–
–
–
–
200
1300
1200
1300
–
–
–
–

93.17
49.23
84.09
84.21
64.96
57.90
74.68
78.25
89.37
90.31
87.17
32.42
95.06
49.85
65.91
44.54
45.28
90.77
46.60
65.96
59.28

2.16
14.46
3.59
3.38
10.32
23.04
6.83
2.78
12.89
1.46
2.57
11.44
3.00
26.21
4.52
42.19
47.20
6.17
17.75
14.22
15.11

–

69.00

12.92

4962

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

sizes. Informative sampling tends to emphasize the larger classes
which have larger boundary and will therefore, discount the
importance of smaller classes.

sampled data on multiclass classiﬁcation problem, we further compared the two classiﬁers, SVM and ANN. ANN has already been
used for several brain segmentation studies (Magnotta et al.,
1999; Powell et al., 2008). In those studies, only small number of
structures (<9) were segmented. Therefore, we want to compare
the two classiﬁers when they are used for the large number of
structures and the variation of structure sizes is extremely large.
Finally, we present the test result of 5-fold cross validation. All
the test results presented in Section 4 are based on the ﬁxed training set and testing set; while by the cross validation implemented
in this section we can discuss the generalization ability of our proposed segmentation method.

4.3.3. Identifying minority classes and optimal sampling ratios
Once SVM models with different sampling ratios are implemented, the minority classes and their over-sampling ratios can
be determined by comparing each class’s performance between
the O-SVM model and the R-SVM or I-SVM models. If the performance of O-SVM model is better than that of R-SVM and I-SVM
models, the class is determined as a majority class; otherwise, it
is a minority class with the appropriate sampling ratio is set to
the model with the best performance.
Since Table 3 indicates that most R-SVM models (R-5k, R-10k,
R-15k and R-20k) in general outperform the I-SVM models in this
experiment. Therefore, the comparison experiments between the
four R-SVM models and O-SVM are conducted with the results
summarized in Table 4. The seven structures, ICAP, PAG, INFC,
VEN, PON, SNR and INTP, have better performance from the RSVM models. In the PAG structures, the R-10k model gives the best
performance. The R-10k model has 10,000 data points from the
PAG structure and the number of points of PAG in a mouse in the
training set is 3648. Hence, the R-10k model over-sampled from
the PAG structures with the ratio of about 274% (=10,000/3648),
rounded to 300%. Same calculation applies to the other six structures. The sampling ratios for SVM are highlighted in Table 4 and
the test performance of the mix-ratio sampling – based SVM model
is presented in the last column. Clearly, we obtain better class performances from the most of the seven structures by over-sampling
from the structures. That is, for the PAG structure, the class performance (VOP, VDP) is improved from (69.62, 36.78) of O-SVM to
(74.68, 6.83) of mix-ratio sampling – based SVM. For the INFC
structure, the improvement is made from (77.74, 13.40) to
(78.25, 2.78). In addition to the increase of the class performances
of the over-sampled structures, the class performances of the nonsampled classes, the majority classes, remained almost same. For
example, for the CORT structure, the class performance remained
almost same from (93.21, 2.64) to (93.17, 2.16). Thus, the overall
performance (Average_VOP, Average_VDP) is improved from
(68.55, 19.21) to (69.00, 12.92). This result shows that proposed
approach is effective to the imbalanced multiclass problem. In Table 4, the best performed model for each structure is highlighted,
which is then later used for training the SVM model. The result
in Table 4 is based on the experiments with the training set and
the testing set which is described in Section 4.3.1.

To show the better performance of the mix-ratio sampling over
the simple over-sampling we compared the test performance of
the mix-ratio sampling to those of the original data set and the
simple over-sampling as seen in Table 5. ‘No sampling’ in the second column in Table 5 means that the training set used for that
experiment is the original data set without any sampling. The simple over-sampling means that the seven minority classes are oversampled by a same sampling ratio. There are three different simple
over-sampled training sets which have the over-sampled minority
classes by the ratios (300%, 500% and 1000%). The third row of Table 5, ‘+/’, presents the margin of the test performance of the original training set and the different sampling training sets. Even
though there is little improvement in the VOP from the sampling
methods, the mix-ratio sampling shows the best VOP and VDP
defeating all the simple over-sampling. This means that there exist
different sampling ratios for the different classes which can improve the classiﬁcation performance of each class. There is a little
surprising result that the performance of the 500% simple oversampling is better than that of the 1000% simple over-sampling.
This means that increasing the number of data of the minority classes with a same ratio does not guarantee the aid in the classiﬁcation of the minority classes. The last row of Table 5 presents the
number of data of each training set. The number of data of the
mix-ratio sampling is similar to that of the 300% simple over-sampling and less than those of the 500% and 1000% simple over-sampling. That means the mix-ratio sampling gives better performance
with less training data than the simple over-sampling. The results
presented in Table 5 tell us the mix-ratio sampling is more effective and efﬁcient for the multiclass problem than the simple
over-sampling.

5. Results

5.2. Comparison with the artiﬁcial neural network (ANN) classiﬁer

In the previous section, we described how to determine minority classes that need to be over-sampled and their sampling ratios,
and developed a mix-ratio sampling SVM method. In this section,
we present some comparison experiments to show the proposed
method outperform existing methods. First, we compared the proposed sampling method, the mix-ratio sampling method, with the
simple over-sampling method to show the effectiveness and efﬁciency of the SVM on mix-ratio sampled data for imbalanced dataset. Second, to demonstrate the applicability of SVM on mix-ratio

We compared the multiclass classiﬁcation accuracy of SVM for
mouse brain images with an ANN by testing the two classiﬁers
to the same dataset which is sampled by the mix-ratio sampling
method. ANN has been successfully used for the brain image segmentation (Magnotta et al., 1999; Powell et al., 2008). We implemented OAA multiclass classiﬁcation which builds 21 ANN
models. To ﬁnd proper model parameters and architecture which
yield better classiﬁcation performance, several different ANN models were tried. The ANN models used in this research has feedfor-

5.1. Comparison with other sampling procedure

Table 5
The performance comparison of the mix-ratio sampling to the simple over-sampling.

Average VOP and VDP
+/
Number of data

No sampling

300% simple over-sampling

500% simple over-sampling

1000% simple over-sampling

Mix-ratio sampling

68.55
19.21
0.00%
0.00%
1,449,936

68.74
0.29%
1,576,642

68.77
0.33%
1,703,348

68.45
0.14%
2,020,113

69.00
0.66%
1,589,026

14.78
23.02%

13.70
28.65%

15.00
21.91%

12.92
32.73%

4963

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965
Table 6
The performance comparison of SVM and ANN (on mix-ratio sampled data set).
CORT.

CPED

HC

CPU

GP

ICAP

PAG

INFC

MED

THAL

SVM

93.17
2.16

49.23
14.46

84.09
3.59

84.21
3.38

64.96
10.32

57.90
23.04

74.68
6.83

78.25
2.78

89.37
12.89

90.31
1.46

87.17
2.57

ANN

88.95
3.81

23.74
53.12

74.22
18.76

79.88
6.33

64.55
4.89

60.36
11.25

77.83
5.09

81.45
13.79

84.86
27.00

90.72
4.65

84.97
4.05

AC

CBLM

VEN

PON

SNR

INTP.

OLFB.

OPT

TRI

SVM

32.42
11.44

95.06
3.00

49.85
26.21

65.91
4.52

44.54
42.19

45.28
47.20

90.77
6.17

46.60
17.75

65.96
14.22

59.28
15.11

69.00
12.92

ANN

32.17
15.56

93.59
1.75

31.33
9.36

59.04
53.20

65.06
52.30

69.25
51.69

88.36
17.97

52.45
20.59

61.09
42.35

16.03
113.53

65.71
25.29

CC

MIDB

Average

Table 7
Test result of the mix-ratio sampling with 5-fold cross-validation.

Volume
VOP
VDP

Volume
VOP
VDP

CORT.

CPED

HC

CPU

GP

ICAP

PAG

INFC

MED

THAL

37.34%
94.18
1.27

0.55%
65.09
9.15

6.02%
86.47
5.02

5.07%
86.78
4.16

0.47%
75.81
9.20

0.61%
70.62
5.94

0.71%
82.04
5.12

1.13%
84.98
4.47

4.72%
89.88
11.29

6.91%
92.83
3.36

AC

CBLM

VEN

PON

SNR

INTP.

OLFB.

OPT

TRI

CC

Average

0.37%
32.18
58.39

12.24%
96.41
1.40

1.89%
59.44
24.35

0.20%
74.21
20.23

0.33%
65.86
12.20

0.11%
55.82
74.51

6.03%
92.08
8.07

0.52%
62.64
17.01

1.11%
72.54
13.80

3.12%
62.09
21.82

100%
75.84
14.97

ward backpropagation networks with two hidden layers which
have ﬁve and three hidden nodes, respectively. Both hidden layers
used hyperbolic tangent sigmoid function as their transfer functions. The models were trained with a learning rate of 0.05,
momentum constant of 0.95. Table 6 presents the test performance
comparison between SVM and ANN. The upper rows show VOP of
each structure and the lower rows shows VDP. In the last column,
the average VOP and VDP of the two classiﬁers are presented.
Based on the average VOP and VDP, SVM outperform ANN by
5.01% and 48.91%, respectively.
5.3. Testing the mix-ratio sampling with cross-validation
Based on the minority classes and their sampling ratios determined by the mix-ratio sampling procedure, we tested the procedure with 5-fold cross-validation to show the generalization
ability of the procedure. Table 7 presents the test result of the
mix-ratio sampling based SVM using 5-fold cross-validation. All
the results in Table 7 is the average value of testing ﬁve mice with
ﬁve different training sets which have different four mice in each
training set. The upper rows in the table show the percentages of
each structure’s volume within a brain. The middle and bottom
rows show VOP and VDP of the 21 structures. The SVM method
based the mix-ratio sampling showed good segmentation performance for the larger structures, which take more than 5% of the
whole brain, such as CORT, HC, CPU, THAL, MIDB AND OLFB. The
VOP’s for all the larger structures were more than 85 and the VDP’s
for all the larger classes except OLFB were less than 5. That means
that the automated segmentation method works exceedingly well
for larger structures. Since the objective of the mix-ratio sampling
procedure is to cure the imbalance problem of multiclass dataset,
the test performance of the smaller classes is more important.
We consider the structures whose relative volumes are less than
1.5% as smaller structures. CPED, GP, ICAP, PAG, INFC, AC, VEN,
PON, SNR, INTP, OPT and TRI are considered as the smaller structures in this study. The proposed method works well for most of
the smaller structures; the segmentation performance for most of
the smaller structures except AC, VEN and INTP are (VOP > 65,

MIDB
8.40%
90.71
3.59

VDP < 20). The two smaller structures, AC and INTP, could not get
satisfactory results. The INTP structure is the smallest and the AC
structure is the third smallest structure. Because of their extremely
small volume in a brain, the segmentation method could not give
good performance for these structures. However, most of the smaller structures were well segmented by the proposed segmentation
method. Based on the test result of the cross-validation, the mixratio sampling based SVM can be an automated method for brain
image segmentation. Fig. 3 illustrates VOP (top) and VDP (bottom)
of the 21 mouse brain structures. The automated segmented and
manual segmented images of the slices at two speciﬁc coronal levels in a mouse brain are displayed in Figs. 4 and 5.
6. Conclusion and future work
In this study, we proposed a new sampling procedure, mix-ratio
sampling, which can handle the imbalanced problem of multiclass
classiﬁcation. In binary classiﬁcation problem, large number of
studies had proposed different sampling procedures to handle
the imbalance problem. However, applying the existing sampling
techniques to the multiclass class problems is computational
expensive and intractable when they have large number of classes.
The new sampling procedure proposed in this paper introduced a
new way to apply sampling procedures to imbalanced problem
which have large number of classes. The mix-ratio sampling procedure can determine minority classes which are hard to be differentiated by classiﬁers and need to be over-sampled. The proposed
procedure considers classes whose classiﬁcation performance can
be improved by over-sampling as minority classes. It also tells
the sampling ratios of the minority classes. The classiﬁcation performance of the proposed procedure was compared with the simple over-sampling procedure which is widely used for the
imbalanced problem. The comparison result showed that the
mix-ratio sampling procedure can alleviate the multiclass imbalanced problem more effectively and efﬁciently.
To show the applicability of the mix-ratio sampling procedure
to an imbalanced multiclass problem, we applied the procedure
to the mouse brain segmentation problem. The segmentation prob-

4964

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

Fig. 3. The test performance of the mix-ratio sampling with 5-fold cross-validation.

Fig. 4. Images from a slice of 3D MRM. Left: automated segmentation and right: manual segmentation.

Fig. 5. Images from a slice of 3D MRM. Left: automated segmentation and right: manual segmentation.

lem is to classify each voxel in 3D MRM images of mouse brain into
21 neuroanatomical structures. We built the SVM model based on
the mix-ratio sampling. The comparison with ANN showed that

SVM yielded better classiﬁcation performance than ANN for this
dataset. The test result which is based on 5-fold cross-validation
showed that the enhanced SVM method based on the mix-ratio

M.H. Bae et al. / Expert Systems with Applications 37 (2010) 4955–4965

sampling to automate the segmentation of the MRM images of
mouse brains segmented well all the structures. Most of the small
structures were segmented well from the larger structures in spite
of the small sizes.
In an image segmentation problem, contextual information of a
voxel can aid in segmentation. The contextual information is obtained from the dependency between the voxel and its neighborhood. Markov Random Field (MRF) theory is a class of probability
theory for modeling the spatial or contextual dependencies of
physical phenomena. Therefore, MRF has been widely studied for
image segmentation. It has been used for brain image segmentation by modeling probabilistic distribution of the labeling of a voxel jointly with the consideration of the labels of a neighborhood of
the voxel (Ali et al., 2005; Fischl et al., 2002). If the MR intensity
information modeled by the SVM model and the contextual information modeled by MRF can be integrated into a segmentation
method, it can produce a more robust and more accurate segmentation of mouse brain MR images. That kind of approach will be
studied in our future research.
References
Abe, S. (2005). Support vector machines for pattern classiﬁcation (advances in pattern
recognition). Secaucus, NJ: Springer-Verlag, New York, Inc..
Akbani, R., Kwek, S., & Japkowicz, N. (2004). Applying support vector machines to
imbalanced datasets. Lecture Notes in Computer Science, 3201, 39–50.
Ali, A. A., Dale, A. M., Badea, A., & Johnson, G. A. (2005). Automated segmentation of
neuroanatomical structures in multispectral MR microscopy of the mouse
brain. NeuroImage, 27(2), 425–435.
Busatto, G. F., Garridob, G. E. J., Almeidac, O. P., Castrod, C. C., Camargoa, C. H. P.,
Cida, C. G., et al. (2003). A voxel-based morphometry study of temporal lobe
gray matter reductions in Alzheimer’s disease. Neurobiology of Aging, 24(2),
221–231.
Chang, C., & Lin, C. (2001). LIBSVM : A library for support vector machines. Software
available at <http://www.csie.ntu.edu.tw/~cjlin/libsvm>.
Chaplot, S., Patnaik, L. M., & Jagannathan, N. R. (2006). Classiﬁcation of magnetic
resonance brain images using wavelets as input to support vector machine and
neural network. Biomedical Signal Processing and Control, 1, 86–92.
Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE:
Synthetic minority oversampling technique. Journal of Artiﬁcial Intelligence
Research, 16, 321–357.
Evans, A. C., Kamber, M., Collins, D. L., & MacDonald, D. (1994). An MRI-based
probabilistic atlas of neuroanatomy. Magnetic resonance scanning and epilepsy.
New York: Plenum Press. pp. 263–274.
Fischl, B., Salat, D. H., Busa, E., Albert, M., Dieterich, M., Haselgrove, C., et al. (2002).
Whole brain segmentation: Automated labeling of neuroanatomical structures
in the human brain. Neuron, 33, 341–355.
Hartmann, S. L., Parks, M. H., Martin, P. R., & Dawant, B. M. (1999). Automatic 3D
segmentation of internal structures of the head in MR images using a

4965

combination of similarity and free-form transformations: Part I, validation on
severely atrophied brains. IEEE Transactions on Medical Imaging, 18, 917–926.
Heckemann, R. A., Hajnal, J. V., Aljabar, P., Rueckert, D., & Hammers, A. (2006).
Automatic anatomical brain MRI segmentation combining label propagation
and decision fusion. NeuroImage, 33, 115–126.
Hsu, C. W., & Lin, C. J. (2002). A comparison of methods for multi-class support
vector machines. IEEE Transactions on Neural Networks, 13, 415–425.
Iosifescu, D. V., Shenton, M. E., Warﬁeld, S. K., Kikinis, R., Dengler, J., Jolesz, F. A., et al.
(1997). An automated registration algorithm for measuring MRI subcortical
brain structures. NeuroImage, 6, 1, 13–2.
Japkowicz, N. (2000). The class imbalance problem: Signiﬁcance and strategies. In
Proceedings of the 2000 international conference on artiﬁcial intelligence (ICAI’2000): Special track on inductive learning, Las Vegas, Nevada.
Li, C. (2007). Classifying imbalanced data using a bagging ensemble variation (BEV).
In Proceedings of the 45th annual southeast regional conference (pp. 203–208).
Ling, C., & Li, C. (1998). Data mining for direct marketing problems and solutions. In
Proceedings of the fourth international conference on knowledge discovery and data
mining (KDD-98). New York: AAAI Press.
Luts, J., Heerschap, A., Suykens, J. A. K., & Huffel, S. V. (2007). A combined MRI and
MRSI based multiclass system for brain tumour recognition using LS-SVMs with
class probabilities and feature selection. Artiﬁcial Intelligence in Medicine, 40,
87–102.
Magnotta, V. A., Heckel, D., Andreasen, N. C., Cizadlo, T., Corson, P. W., Ehrhardt, J. C.,
et al. (1999). Measurement of brain structures with artiﬁcial neural networks:
Two- and three-dimensional applications. Radiology, 211(3), 781–790.
Mourao-Miranda, J., Bokde, A. L. W., Born, C., Hampel, H., & Stetter, M. (2005).
Classifying brain states and determining the discriminating activation patterns:
Support vector machine on functional MRI data. NeuroImage, 28, 980–995.
Nattkempera, T. W., Arnricha, B., Lichtea, O., Timma, W., Degenhardb, A., Pointonc,
L., et al. (2005). Evaluation of radiological features for breast tumour
classiﬁcation in clinical screening with machine learning methods. Artiﬁcial
Intelligence in Medicine, 34, 129–139.
Powell, S., Magnotta, V. A., Johnson, H., Jammalamadaka, V. K., Pierson, R., &
Andreasen, N. C. (2008). Registration and machine learning-based automated
segmentation of subcortical and cerebellar brain structures. NeuroImage, 39,
238–247.
Reddick, W. E., Glass, J. O., Cook, E. N., Elkin, T. D., & Deaton, R. J. (1997). Automated
segmentation and classiﬁcation of multispectral magnetic resonance images of
brain using artiﬁcial neural networks. IEEE Transactions on Medical Imaging, 16,
6, 911–91.
Sharief, A. A., Badea, A., Dale, A. M., & Johnson, G. A. (2008). Automated
segmentation of the actively stained mouse brain using multi-spectral MR
microscopy. NeuroImage, 39, 136–145.
Shin, H., & Cho, S. (2006). Response modeling with support vector machines. Expert
Systems with Applications, 30, 746–760.
Thompson, P. M., & Toga, A. W. (1996). A surface-based technique for warping 3dimensional images of the brain. IEEE Transactions on Medical Imaging, 15, 1–16.
Veropoulos, K., Campbell, C., & Cristianini, N. (1999). Controlling the sensitivity of
support vector machines. In Proceedings of the international joint conference on AI
(pp. 55–60).
Wu, G., & Chang, E. Y. (2005). KBA: Kernel boundary alignment considering
imbalanced data distribution. IEEE Transactions on Knowledge and Data
Engineering, 17(6), 786–795.

Heterogeneous Data Fusion for Alzheimer’s Disease Study
Jieping Ye 1,2 , Kewei Chen 5 , Teresa Wu 3 , Jing Li 3 , Zheng Zhao 2 , Rinkal Patel 2 ,
Min Bae 3 , Ravi Janardan 4 , Huan Liu 2 , Gene Alexander 5 , and Eric Reiman 5

1

Center for Evolutionary Functional Genomics, The Biodesign Institute, Arizona State University,
Tempe, AZ 85287
2
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287
3
Department of Industrial Engineering, Arizona State University, Tempe, AZ 85287
4
Department of Computer Science & Engineering, University of Minnesota, Minneapolis, MN 55455
5
Banner Alzheimer’s Institute and Banner PET Center, Banner Good Samaritan Medical Center,
Phoenix, AZ 85006

ABSTRACT

Keywords

Eﬀective diagnosis of Alzheimer’s disease (AD) is of primary importance in biomedical research. Recent studies
have demonstrated that neuroimaging parameters are sensitive and consistent measures of AD. In addition, genetic
and demographic information have also been successfully
used for detecting the onset and progression of AD. The
research so far has mainly focused on studying one type
of data source only. It is expected that the integration of
heterogeneous data (neuroimages, demographic, and genetic
measures) will improve the prediction accuracy and enhance
knowledge discovery from the data, such as the detection of
biomarkers. In this paper, we propose to integrate heterogeneous data for AD prediction based on a kernel method.
We further extend the kernel framework for selecting features (biomarkers) from heterogeneous data sources. The
proposed method is applied to a collection of MRI data from
59 normal healthy controls and 59 AD patients. The MRI
data are pre-processed using tensor factorization. In this
study, we treat the complementary voxel-based data and region of interest (ROI) data from MRI as two data sources,
and attempt to integrate the complementary information by
the proposed method. Experimental results show that the
integration of multiple data sources leads to a considerable
improvement in the prediction accuracy. Results also show
that the proposed algorithm identiﬁes biomarkers that play
more signiﬁcant roles than others in AD diagnosis.

Neuroimaging, tensor factorization, heterogeneous data source
fusion, multiple kernel learning, biomarker detection

1. INTRODUCTION
Currently, approximately 5 million people in the US about 10% of the population over 60 are aﬄicted by Alzheimer’s
disease (AD), the most common form of dementia. The direct cost to care the patients by family members or health
care professional is estimated to be over $100 billion per
year. As the population ages over the next several decades,
it is expected that the AD cases and the associated costs
will go up dramatically. Recognizing the urgent need to
slow down or completely prevent from the occurrence of a
health care crisis in US and worldwide, AD researchers have
intensiﬁed their eﬀorts to investigate ways to delay, cure,
or prevent the onset and progression of AD. Objective and
quantitative criteria, so called, biomarkers, are essential to
evaluate the eﬀectiveness of a potential treatment or prevention strategy. Thus, research on exploring biomarkers in
the form of a test of cerebrospinal ﬂuid (CSF) or blood, or
images from brain scans has attracted great attention.
Recent studies have demonstrated that imaging parameters from brain scans are more sensitive and consistent measures of disease progression than cognitive assessment [32].
Some studies have shown that imaging measures correlate
with cognitive test performance in Mild Cognitive Impairment (MCI)1 and AD - an initial step in the validation of
markers that accurately predict the course of the disease.
Evidently, neuroimaging research oﬀers great potential to
identify the sensitive and speciﬁc biomarkers that can identify individuals early in the course of dementing illness. This
opens up opportunities to implement treatments in the early
stages of disease when intervention may be most beneﬁcial.
The volumetric T1 weighted MRI is a high-resolution structural imaging technique that allows for the visualization of
brain anatomy with a high degree of contrast between brain
tissue types. It can be used to measure speciﬁc structures
(e.g., hippocampus, entorhinal cortex, amygdale, etc.), a
region of interest (ROI) approach, and detect the volume
changes of the structures for AD vs. Normal [21]. Recently,

Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications Data Mining; J.3 [Life and Medical Sciences]: Health,
Medical information systems

General Terms
Algorithms

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD’08, August 24–27, 2008, Las Vegas, Nevada, USA.
Copyright 2008 ACM 978-1-60558-193-4/08/08 ...$5.00.

1

Mild Cognitive Impairment (MCI) is a transition stage between the cognitive changes of normal aging and the more
serious problems related to dementia.

1025

structural MRI images have been used to quantify reductions of whole brain volume in sequentially acquired scans
[8]. Promising methodological developments in the analysis
of structural MRI data also include the use of probabilistic
brain maps [3] to compute regional alterations in gray matter, white matter, CSF, and whole brain, and to examine
cross-sectional diﬀerence or longitudinal changes on voxelby-voxel basis, an approach referred to as the voxel-based
morphometry (VBM) [36]. Another neuroimaging technique
is the so called positron emission tomography (PET). With
diﬀerent radioactive tracers, PET provides information on
various physiological, biochemical and/or metabolic processes.
In addition, other types of data, e.g., demographic information, such as age, gender, education, genetic makeup (such
as the possession of the allele of Apolipoprotein e4), etc,
have also been shown to be associated with AD.
While promising, the research so far has mainly focused
on studying only one type of neuroimaging, e.g., MRI, fMRI
(functional MRI), or FDG-PET using either region of interest or voxel-based approach [2, 8, 9, 24, 35, 44]. It
is expected that combining diﬀerent types of neuroimaging will help the prediction. However, even for the same
neuroimaging data, diﬀerent features constructed by diﬀerent approaches (region of interest versus voxel-based approaches) might complement each other. Integrating ROI
and voxel-based information from the same type of neuroimaging data and incorporating demographic information
is expected to improve the prediction.
In this paper, we propose a kernel method to fuse heterogeneous data (diﬀerent types of features from single MRI
data, together with demographic and genetic information)
for accurately classifying subjects and discovering useful knowledge on biomarkers. More speciﬁcally, our main contributions to the AD research are summarized as follows:
• Dimensionality Reduction via Tensor Factorization: Neuroimages such as those from MRI are
represented as a three-dimensional array, which contains a huge number of features (voxels). Due to the
natural tensor representation of such images, dimensionality reduction based on tensor factorization is one
eﬀective approach for reducing data dimensionality [19,
25, 26]. We propose to apply N -mode SVD [41] and
the out-of-core technique [43] for the factorization.

sources is expected to give a more accurate determination of biomarkers as well as to aid its understanding.
Traditional feature selection algorithms work on a single data source only. We propose to integrate multiple kernel learning and traditional feature selection for
biomarker detection from multiple data sources.
To the best of our knowledge, our uses of (1) tensor factorization for extracting features from AD-related neuroimages;
(2) multiple kernel learning for integrating AD-related multiple data sources; and (3) feature selection from multiple
data sources are novel contributions to the AD research.
We evaluated the proposed method using data acquired
under the support of ADNI2 from 59 normal control (NC)
and 59 AD patients. The data includes 118 MRI images represented as a three-dimensional array of size 181 × 217 × 181
(in the standard Talairach space and with 1 cubic mm voxel
size), as well as demographic information such as age, gender, and genetic information based on Apolipoprotein E e4
(APOE4). SPM53 was used together with the optimized
voxel-based-morphometry to generate the modulated gray
matter map in the customized template space4 . Experimental results show that multiple kernel learning achieves
a considerable improvement in the prediction accuracy in
comparison with classiﬁcation based on each data source individually. Results also show that the proposed algorithm
is able to identify a number of brain regions that are known
to be aﬀected by AD.
The rest of the paper is organized as follows. Tensor factorization for feature extraction from MRI data is presented
in Section 2. We introduce multiple kernel learning for heterogeneous data fusion in Section 3. Biomarker detection
from multiple data sources is discussed in Section 4. Experimental results are presented in Section 5. Finally, Section 6
concludes this paper with discussions and future work.

2. IMAGE FEATURE EXTRACTION VIA
TENSOR FACTORIZATION
As high resolution 3D data, volumetric MRI data have a
huge number of voxels at the time they were acquired from
each subject. In our VBM pre-processing, we kept the voxel
size in the template space to be in 1 cubic mm resulting
in the image dimension of 181×217×181. Dimensionality
reduction, which extracts a small number of features by removing the irrelevant, redundant, and noisy information,
is crucial for the analysis of such data. Some neuroimaging studies use sub-sampling or a region of interest (ROI)
based approach such as the Automated Anatomical Labeling (AAL) [40] to reduce the data dimensionality. Although
the within-ROI variation is ignored, AAL ROI summarizes
the information from multiple brain regions with much reduced dimension and these regions are representative over
the whole brain volume. These techniques, however, may
not be able to account for the information variation within
each region of interest. Additionally, a traditional dimensionality reduction technique called Principle Component
Analysis (PCA) [16] has been used widely in many applications [34, 39], including a well-known adaptation in the

• Multiple Data Source Fusion via Multiple Kernel Learning: The integration of diﬀerent types of
features (region of interest and tensor features) from
MRI data and non-imaging data such as demographic
information is expected to improve the prediction accuracy for our AD study. Multiple kernel learning
(MKL) provides a general framework for learning from
multiple data sources. It has been applied for combining various biological data for enhanced biological
inference [22]. We propose to apply a discriminant
analysis-based formulation [47] for the integration.
• Knowledge (Biomarker) Discovery via Multisource Feature Selection: In addition to oﬀering
a more accurate prediction of AD, another important
component of the AD study is knowledge gained on
the linkage between structural and functional abnormalities. Feature selection [13, 29] is an eﬀective vehicle for such a discovery and the use of multiple data

2

http://www.loni.ucla.edu/Research/Databases/
http://www.ﬁl.ion.ucl.ac.uk/spm/
4
Gene Alexander’s group processed the MRI images used
in this study. More details on these data can be found in
Section 5.1.
3

1026

where U (n) ∈ In ×Rn has orthonormal columns for n =
1, · · · , N . When Rn is much smaller than In for all n, the
core tensor C and the basis matrices {U (n) }N
n=1 give a compact representation of the original tensor A, resulting in data
compression.
Given the basis matrices {U (n) }N
n=1 , the core tensor C can
be readily computed as C = A ×1 (U (1) )T · · · ×N (U (N ) )T .
Thus, the optimization problem focuses on the computation
of the basis matrices only. An iterative approach can be
applied for the computation [25, 43]. Each iterative step
optimizes only one of the basis matrices, while keeping the
other N − 1 basis matrices ﬁxed.
The iterative algorithm above may be computationally
expensive and the solution depends on the initialization. In
this paper, we apply an approximation algorithm called N mode SVD [41], which has been applied successfully in computer vision and computer graphics [41, 42]. Deﬁne Dn as
an In × In matrix whose (u, v)-th entry (1 ≤ u, v ≤ In ) is
given by:

neuroimaing ﬁeld, often referred to as the scaled sub-proﬁle
modeling (SSM) [1]. PCA adopts the vector representation
for images by concatenating all voxels within a pre-deﬁned
brain volume into a single vector. One inherent problem
with this approach is that some information on spatial relationships (such as the correlation among diﬀerent slices of
the 3D image) is not explicitly accounted for. One eﬀective
way to overcome these limitations is to treat a collection
of three-dimensional images as a tensor and apply tensor
factorization [19, 25, 26].

2.1 Background on Tensors
A tensor, also known as multidimensional matrix [45], is
a higher order generalization of a vector (ﬁrst order tensor)
and a matrix (second order tensor). An N th-order tensor is
denoted as A ∈ I1 ×I2 ×···×IN . An element of the tensor A is
denoted as ai1 ···in ···iN , where 1 ≤ in ≤ In , for n = 1, · · · , N .
An N th-order tensor A is of rank-one if it can be expressed
as the outer product of N vectors:
A = x1 ⊗ x2 ⊗ · · · ⊗ xN ,

(1)

···
i1

where xn ∈
, for all 1 ≤ n ≤ N .
A generalization of the product of two matrices is the
product of a tensor and a matrix. The mode-n product
of a tensor A ∈ I1 ×I2 ×···×IN and a matrix Q ∈ Jn ×In ,
whose element is denoted as qjn in , where 1 ≤ jn ≤ Jn and
1 ≤ in ≤ In , is a tensor, denoted as
In

A ×n Q ∈

I1 ×···×In−1 ×Jn ×In+1 ···×IN

,

(2)

Ã = C ×1 U (1) ×2 U (2) × · · · ×N U (N ) ,

(A ×n Q)i1 ···in−1 jn in+1 ···iN =

(1) T

(3)
I1 ×I2 ×···×IN

Let B ∈
be another tensor, whose general
element is denoted as bi1 ···in ···iN . The scalar product of two
tensors A and B is deﬁned as:
···
i2

ai1 ···in ···iN bi1 ···in ···iN .



Diﬀerent neuroimaging features (voxel-based tensor and
ROI-based AAL features from the same data source) may
capture diﬀerent but complementary characteristics of the
data. For example, the voxel-based tensor features focus
more on the global information, while AAL features focus
on representative multiple ROI (local) information, though
potential information overlaps exist between these two types
of data (generated from the same MRI data set). A joint
analysis of these data can potentially exploit their complementary information and improve the prediction. Such prediction can be further improved by incorporating additional
non-imaging data sources, such as demographic information.
Multiple Kernel Learning (MKL) provides a general framework for learning from multiple data sources [23]. MKL
works by ﬁrst constructing a kernel from each of the data
sources and then combining these kernels based on a certain criterion for improved classiﬁcation performance. In
addition to the SVM-based formulation in [23], we apply a
discriminant analysis-based formulation.

The Frobenius norm of a tensor A is then deﬁned as
A, A.

(5)

The mode-n vectors of A are the In -dimensional vectors
obtained from A by varying index in while keeping the
other indices ﬁxed. They form the column vectors of matrix
A(n) ∈ In ×(I1 I2 ···In−1 In+1 ···IN ) that results from ﬂattening
the tensor A. The n-th rank of A, denoted as rankn (A), is
deﬁned as the dimension of the vector space spanned by the
mode-n vectors: rankn (A) = rank(A(n) ).

2.2 Tensor Factorization
Given a tensor A ∈ I1 ×I2 ×···×IN , a rank-(R1 , · · · , RN )
factorization of A is formulated as ﬁnding a lower-rank tensor Ã ∈ I1 ×I2 ×···×IN with rankn (Ã) = Rn ≤ rankn (A),
for all n, such that the following least-squares cost function
is minimized:

 

Ã = argminÂ A − Â .

(6)

3.1

(7)

Kernel methods work by embedding the input data into
some high-dimensional feature space and they are generally
formulated as convex optimization problems [37, 38]. The
key fact underlying the success of kernel methods is that the

More speciﬁcally, Ã can be expressed as follows:
Ã = C ×1 U (1) ×2 U (2) × · · · ×N U (N ) ,

(8)

(N ) T

3. MULTIPLE DATA SOURCE FUSION

(4)

iN

||A|| =

(2) T

where C = A ×1 (U ) ×2 (U ) × · · · ×N (U
) .
Since the size of the tensor A considered in this paper
can easily exceed the memory capacity of a single machine,
we develop an out-of-core algorithm by partitioning a tensor
into smaller blocks as in [43].

ai1 ···in−1 in in+1 ···iN qjn in .
in

i1

ai1 ···in−1 uin+1 ···iN · ai1 ···in−1 vin+1 ···iN ,
iN

where ai1 ···in−1 uin+1 ···iN and ai1 ···in−1 vin+1 ···iN are elements
of the N th-order tensor A ∈ I1 ×I2 ×···×IN . It follows that
Dn is a symmetric and positive semi-deﬁnite matrix. Let
Dn = Un Σn UnT be the SVD of Dn . Denote U (n) the basis
matrix, which consists of the ﬁrst Rn columns of Un . The
approximation of the original tensor A is given by

whose entries are given by

A, B =

···
in−1 in+1

1027

Background on Kernel Methods

embedding into feature space can be determined uniquely by
specifying a kernel function that computes the dot product
between data points in the feature space implicitly. In other
words, the kernel function implicitly deﬁnes the nonlinear
mapping to the feature space and expensive computations in
the high-dimensional feature space can be avoided by evaluating the kernel function in the original attribute space.
Thus one of the central issues in kernel methods is the selection of kernels.
We call K : X × X → R a kernel function [37], where X
is the input space, if it satisﬁes the ﬁnitely positive semideﬁnite property: for any x1 , · · · , xm ∈ X , the Gram matrix G ∈ m×m , deﬁned by Gij = K(xi , xj ) is symmetric
and positive semideﬁnite. Any kernel function K implicitly
maps the input set X to a high-dimensional (possibly inﬁnite) Hilbert space HK equipped with the inner product
(·, ·)HK through a mapping φK : X → HK as

can be found by solving the following Semideﬁnite Program
(SDP):
min
θ,t

subject to

G=

G G=

θi ri = 1, θi ≥ 0

θi Gi ,
i=1

,

m+

=
i=1

φK (x+
i )/m+ ,

(9)

4.1

m−

=

	

	 0,
(11)

m

.

Background on Feature Selection

Given a data set with d features {F1 , F2 , ..., Fd }, the task
of a feature selection algorithm is to remove as many irrelevant (and redundant) features as it can and ﬁnd a feature
subset {Fj1 , ..., Fjr } (r < d), such that with dimensionallyreduced data, a learning algorithm can achieve similar or
better performance. Feature selection has been used widely
in many applications including text mining [11, 17], image
processing [12], and bioinformatics [6, 14, 27, 28]. Traditional feature selection algorithms work on a single data
source only. The challenge is how to develop eﬀective feature selection algorithms from multiple data sources, called
“multi-source feature selection”.

4.2

φK (x−
i )/m− .

Feature Selection from Multiple Data
Sources

Assume that among the p data sources {Di }pi=1 of m instances (subjects), Dt (1 ≤ t ≤ p) is the target for feature
(biomarker) selection. In our study, p = 5 data sources are
involved. In feature selection from multiple data sources, we
aim to remove irrelevant (and redundant) features according to the global pattern extracted from all p data sources.
Clearly this is diﬀerent from standard feature selection. To
the best of our knowledge, feature selection from multiple
data sources has not been well-addressed in the literature.
We propose to use multiple kernel learning for feature selection from multiple data sources. Speciﬁcally, multiple
kernel learning is applied for information fusion from multiple data sources for pattern extraction. The combined kernel
matrix extracts the pattern of the data in the form of pairwise similarities, which can then be used as the input for a

i=1

In RKDA, the separation between the two classes is mea− 2
sured by the ratio of the variance (wT (μ+
K − μK )) between
T
the classes to the variance w ΣK w within the classes, where
ΣK = φK (X) P φK (X)T /m is the covariance matrix of the
data in the feature space and φK (X) is the data matrix in
the feature space. Speciﬁcally, RKDA in the binary-class
case maximizes the following objective function:
− 2
T
F (w, K) = (wT (μ+
K − μK )) /w (ΣK + μI)w,

a

a
t

Recall from the introduction that identifying biomarkers
which are sensitive to AD onset or progression [31] is extremely important in AD study. Feature selection is commonly used for selecting a small subset of features for building a comprehensible learning model with good generalization performance [13, 29, 30]. Such a small subset of features
can then be used as ‘biomarkers’. We propose to apply feature selection from multiple data sources, including the AAL
data and the voxel-based tensor data, both from the same
single MRI modality, as well as various types of demographic
information.

i=1

μ−
K

θi Gi

4. BIOMARKER DETECTION FROM
MULTIPLE DATA SOURCES

where ri = trace(Gi ). In the following, we assume that all
kernel matrices have been centered. This is equivalent to
centering the data as the pre-processing step.
+
For binary-class problems, we are given {x+
1 , · · · , xm+ }
−
−
and {x1 , · · · , xm− }, the collections of data points from the
positive and negative (AD and normal) classes, respectively.
The total number of data points is m = m+ + m− . For a
given kernel function K, the basic idea of kernel discriminant analysis with regularization, called RKDA [47] is to
ﬁnd a direction w in the feature space HK onto which the
m+
− m−
projections of the two sets {φK (x+
i )}i=1 and {φK (xi )}i=1
are well separated. Deﬁne the centroids of the two classes
in the feature space as follows:
μ+
K

p
i=1
T

The problem can be further formulated as a quadratically
constraint quadratic programming (QCQP) problem [48],
which is more eﬃcient to solve than SDP.

Assume that we are given p kernel matrices G1 , · · · , Gp .
In MKL, the optimal kernel matrix G∗ lies in the set G
deﬁned as





a = [1/m+ , · · · , 1/m+ , −1/m− , · · · , −1/m− ]T ∈

3.2 Optimal Kernel Combination

p

1
μ

where θ = [θ1 , · · · , θp ]T , r = [trace(G1 ), · · · , trace(Gp )]T ,
and

In binary classiﬁcations, the algorithms learn a classiﬁer f :
X → {−1, +1} whose decision boundary between the two
classes is aﬃne in the feature space: f (x) = sgn(wT φK (x) +
b), where w ∈ K is the vector of feature weights, b ∈
is
the intercept, and sgn(u) = +1, if u > 0, and −1 otherwise.

p

I+

θ ≥ 0, θT r = 1,

K(x, z) = (φK (x), φK (z))HK .

 



t

(10)

where μ > 0 is a regularization parameter.
It can be shown [47] that for a given set of p centered kernel matrices G1 , · · · , Gp , the optimal kernel matrix G∗ =
p
i=1 θi Gi ∈ G that optimizes the criterion in Eq. (10)



1028

5.1

generic feature selection algorithm. We plan to study two
feature selection algorithms, SPEC [50] and ReliefF [20], as
both algorithms use the pairwise similarities (or distances)
and the feature vectors as their input.
SPEC is a framework for both supervised and unsupervised
feature weighting [50]. Given a data set D, the similarities
among instances can be captured by a set of pairwise instance similarities  and its induced graph  . SPEC treats
features as functions deﬁned on D and selects features in
terms of the smoothness on the manifold formed by the observed data instances. The smoothness of a feature fi is
evaluated by comparing the feature with the spectrums of
L, the normalized Laplacian matrix of  :
r(fi ) = ϕ (fi ; γ(λ1 ), . . . , γ(λm ); ξ1 , . . . , ξm ) .

Five feature (data) sources are used in this study, including tensor and AAL features from MRI images, two
types of demographic information related to AD: age and
gender, and genetic information based on Apolipoprotein
E e4 (APOE4)5 . It is well known that Apolipoprotein E
e4 (APOE4) is a risk factor for AD. Compared to APOE4
non-carriers whose onset age for AD of 84 and risk of 20%,
people with one/two copy/copies of APOE4 get the disease at younger age (onset age of 75/68) and increased risk
(47%/91%). We derive linear kernels for tensor and AAL
features. A Gaussian kernel with an appropriate parameter
value is used for the age feature. A simple binary kernel matrix is constructed based on gender feature: if two samples
share a common gender, their corresponding kernel matrix
entry is 1, otherwise it is set to 0. We use ApoE Genotyping Allele 1 and Allele 2 to divide the samples into three
groups: APOE4 non-carriers, heterozygotes, and homozygote groups. A kernel matrix similar to the one for the
gender feature is then constructed.

(12)

In Eq. (12), (λi , ξi )i=1,... ,m denotes the spectrum (or eigen)
decomposition of the normalized Laplacian matrix L. γ(·)
is an increasing function which is used to modify the eigenvalues of L and has an eﬀect of removing noise [49]. ϕ(·) is
a predeﬁned smoothness measure function, which compares
the feature with the spectrums of L. In [15, 50], a robust
smoothness measure function is deﬁned as:




  




   
m

ϕ(fi ) =

j=2

α2j γ(λj )

m

j=2

1
2

fˆi = D fi

=

2;

(13)

1
· D 2 fi , αj = ξjT fˆi

(14)

α2j

−1

5.2

T

1 − f i ξ1

According to spectral clustering theories [33], the eigenvalues of L measure the separability of the components of the
graph  and the eigenvectors are the corresponding soft cluster indicators. We compare the normalized feature vector
fˆi with the eigenvectors of L (measured by αj ’s deﬁned in
Eq. (14)). The intuition behind Eq. (13) is that the better fˆi
aligns to the leading eigenvectors of L, the better the feature
fi can separate the data as a function deﬁned on D. Note
that in Eq. (13), we ignore the ﬁrst eigenvector of L. The
reason is that the trivial eigenvector ξ1 only carries density
information around instances and does not determine separability. In the application, the set  of pairwise instance
similarities can be obtained from the learned kernel matrix
from the last section.
ReliefF is a well-known supervised feature selection algorithm derived as an extension of Relief [18]. It determines
the relevance of a feature according to its contribution to
the hypothesis margin [4] of the observed data. In ReliefF,
the relevance of a feature fi is deﬁned as:
r(fi ) =

1
2

Performance of Tensor Factorization

We apply tensor factorization based on N -mode SVD for
extracting features from the 118 MRI images. The whole
collection of images can be represented as a 4th order tensor
A ∈ I1 ×I2 ×I3 ×I4 with I1 = 181, I2 = 217, I3 = 181, and
I4 = 118. The fourth dimension of the 4th order tensor,
which corresponds to the 118 subjects, is ﬁxed (i.e., R4 =
I4 = 118), while the other three dimensions (I1 , I2 , and
I3 ) which correspond to the size of a single 3D image are
reduced to R1 , R2 , and R3 , respectively. Following Eq. (8),
the approximation tensor Ã is given as

T

f i γ(L) f i

Data Sources and Kernels

Ã = C ×1 U (1) ×2 U (2) ×3 U (3) ,
R1 ×R2 ×R3 ×I4

(15)
(i)

where C ∈
is the core tensor and U
∈
Ii ×Ri
is the basis matrix along the i-th dimension. The
j-th slice of Ã along the fourth dimension is of size R1 ×
R2 × R3 , which is the compressed representation for the jth image.
We use the Tensor Toolbox in [5] as the building block
for our implementation. The factorization performance is
measured in terms of compression ratio and information loss
[10, 46]. The information loss (IL) is given by

 
A − Â

IL =


A


,

(16)

and the compression ratio (CR) is given by

m

(
xt,i − N M (xt )i 
 − 
xt,i − N H(xt )i 
).

CR =

t=1

I1 I2 I3 m
,
R1 R2 R3 m + I1 R1 + I2 R2 + I3 R3

(17)

where m = I4 = 118 is the total number of samples.
For simplicity, we set the reduced dimensionalities, i.e, R1 ,
R2 , and R3 to a common value in our experiment. The result
is summarized in Table 1. We can observe that we achieve a
compression ratio of about 263 when the size of each image
is reduced to 30 × 30 × 30, while the majority (96%) of the
information from the original data is kept. This signiﬁcantly

where xt,i denotes the value of instance xt on feature fi ,
N H(x) and N M (x) denote the nearest points to x in the
data with the same and diﬀerent label respectively, and 
 · 

is a distance measurement. In this application, the neighborhoods of instances can be determined by the learned kernel
matrix from the last section.

5
Human apolipoprotein E (apoE) is a 34-kDa protein containing 299 amino acid residues. There are three major isoforms of human apoE (namely apoE2, apoE3, and apoE4),
which are the products of three alleles (e2, e3, and e4) at a
single gene locus on chromosome 19q13.2.

5. EXPERIMENTS
We evaluate the eﬀectiveness of the proposed methods on
a collection of 118 samples consisting of 59 normal healthy
controls and 59 AD patients.

1029

Table 2: Performance of MKL based on RKDA and
SVM in comparison with RKDA and SVM based
on each of the ﬁve data sources alone. Prediction in
terms of sensitivity and speciﬁcity.

Table 1: Performance of tensor factorization on 118
MRI images.
Reduced Dimension
20 × 20 × 20
30 × 30 × 30
40 × 40 × 40

Compression Ratio
889
263
111

Information Loss
22%
4%
1%

Method

RKDA
reduces the memory and disk space, which is critical when
analyzing and transmitting large volume neuroimaging.
To visually evaluate the compression performance, we randomly pick one image from the data set. We compare the
original and reconstructed images (reduced dimension is 20×
20 × 20) using four slices in three diﬀerent views (sagittal,
coronal, and axial), as shown in Figure 1. We can observe
from the ﬁgure that the reconstructed slices are visually
very similar to the original ones, even with a compression
ratio as high as 889. In the following experiment, we set
R1 = R2 = R3 = 20, resulting in a 8000-dimensional representation for each MRI image, as using a larger dimensionality doesn’t improve the performance much.

SVM

Data Source
Tensor
AAL
APOE4
Gender
Age
Combination
Tensor
AAL
APOE4
Gender
Age
Combination

Sensitivity
0.785
0.605
0.705
0.740
0.505
0.950
0.800
0.780
0.745
0.440
0.665
0.945

Speciﬁcity
0.790
0.760
0.415
0.545
0.595
0.895
0.795
0.845
0.680
0.460
0.515
0.850

the comparison is based on the assumption that we use preexisting AD domain knowledge from our collaborators at
Banner Alzheimer’s Institute at Phoenix as the gold standard. It is clear from Table 3 that both SPEC+G∗ and
ReliefF+G∗ based on MKL perform signiﬁcantly better than
their counterparts SPEC+G2 and ReliefF+G2 based on a
single data source. For example, among the top 20 regions from SPEC+G∗ , 16 of them are conﬁrmed to be ADrelated, while there are only 11 AD-related regions from
SPEC+G2 . Figure 2 highlights the top 12 regions detected
by SPEC+G∗ .
Our multiple kernel learning procedure not only provides
adequate distinction of AD and normal subjects as shown
in Table 2, but also identiﬁes regions that play more signiﬁcant roles than others in such classiﬁcation (as shown in Table 3). These brain regions, interestingly enough, included
left/right parahippocampla, hippocampus, amygdala, L/R
Fusiform, various temporal regions, lingual, and occipital.
It is worth noting that our MKL procedure was blind to the
prior knowledge of brain regions associated with AD. Nevertheless, the regions that are known to be aﬀected by AD are
those that have contributed the most in our MKL analysis.
This further conﬁrms the promise of MKL for data fusion
for the AD study.

5.3 Performance of MKL for AD Prediction
In this experiment, we evaluate the multiple kernel learning algorithm for integrating ﬁve data sources denoted as
tensor, AAL, age, gender, and APOE4. The study is performed by repeated random splitting of the data into training and test sets of ratio of 2 : 1. To reduce the variability,
the splitting is repeated 20 times and the results are averaged.
Table 2 presents the prediction performance of various
algorithms (RKDA and SVM6 using each of the ﬁve data
sources by itself and the combination of them based on
MKL) in terms of sensitivity and speciﬁcity. We can observe from the table that multiple kernel learning based approaches (using the combination of all ﬁve data sources),
outperform all other methods based on a single data source
in terms of both sensitivity and speciﬁcity. For example,
RKDA-based MKL achieves a sensitivity about 0.950, which
is signiﬁcantly higher than RKDA based on any single data
source. We can obtain the same observation in the case of
SVM. This implies that diﬀerent data sources contain complementary information and the integration of them leads
to a signiﬁcant improvement for AD prediction.

5.4 Performance of Biomarker Detection

6. DISCUSSIONS AND CONCLUSION

In this experiment, we evaluate the proposed algorithm
for selecting features based on learning from multiple data
sources. All ﬁve data sources are used to learn the kernel
Gram matrix and AAL is used as the target data source
with 116 brain regions as the feature set. We also report
the feature selection result using AAL data source only for
comparison.
Table 3 presents the top 20 regions (features) obtained
by two feature selection algorithms: SPEC and ReliefF using G2 (the kernel matrix constructed from the AAL data
source only) and G∗ (combined kernel matrix from all ﬁve
data sources based on RKDA). It is important to note that

In this paper, we have proposed a kernel method for integrating heterogeneous data for AD prediction. We further
extend the kernel framework for selecting features (biomarkers) from heterogeneous data sources. Our experiments show
the integration of multiple data sources leads to a considerable improvement in the prediction accuracy. Results also
show that the proposed multi-source feature selection algorithm identiﬁes biomarkers (brain regions) that play more
signiﬁcant roles than others in AD diagnosis.
The tensor factorization used in this paper assumes no
prior knowledge on the importance of entries from a given
tensor. A uniform weight is applied to all entries. In our AD
study, certain collections of entries in the brain are known to
be more important. It is thus desirable to put higher weights
to these voxels. We plan to examine weighted tensor factor-

6

We used the LIBSVM implementation in [7] and the SVMbased MKL in [23] with the regularization parameter estimated through 5-fold cross-validation.

1030

Figure 1: Original and reconstructed images (reduced dimension is 20 × 20 × 20) shown as slices in sagittal
view (rows 1 and 2), in coronal view (rows 3 and 4), and in axial view (rows 5 and 6). The ﬁrst, third, and
ﬁfth rows represent 4 slices from the original image, while the second, fourth, and sixth rows represent the
corresponding 4 slices from the reconstructed image. Observe that, even with a compression ratio as high as
889, the reconstructed slices are visually very similar to the original ones.

1031

Table 3: The top 20 regions (ranked from top to bottom) from SPEC and ReliefF using G2 (kernel matrix
constructed from the AAL data source alone) and G∗ (combined kernel matrix from all ﬁve data sources
based on RKDA). Regions with bold typeface are relevant according to existing AD domain knowledge.
SPEC+G2
TempPlMidL
TempPlMidR
FusiformR
FusiformL
TempPlSupL
TempInfR
LingualL
Cereb6L
LingualR
CerebCr1L
ParaHippR
TempInfL
OccInfR
ParaHippL
TempPlSupR
Cereb6R
CerebCr1R
Vermis3
Vermis45
CerebCr2L

SPEC+G∗
ParaHippR
FusiformR
ParaHippL
FusiformL
TempInfR
AmygdR
TempInfL
TempPlMidR
LingualR
TempPlMidL
OccInfR
TempPlSupL
LingualL
OccInfL
AmygdL
HippR
TempPlSupR
TempMidR
CerebCr1L
Cereb6L

ReliefF+G2
HippR
AmygdL
AmygdR
HippL
Cereb8L
Cereb9L
PutamenR
PutamenL
Vermis9
Cereb8R
InsulaL
PallidumR
PallidumL
Cereb7bR
InsulaR
Cereb7bL
CuneusR
Vermis8
Cereb9R
TempMidL

ization in the future. In this study, we have focused on
MRI data. While MRI provides anatomical/structural information about the disease, the complementary PET technique with the positron-emitting radiotracer FDG allows researchers to examine the glucose hypometabolic pattern in
AD patients in comparison with normals by measuring the
cerebral metabolic rate for glucose (CMRgl). We expect
that the fusion of MRI data (structural neuroimaging data)
with PET data (functional neuroimaging data) as well as
demographic data will further improve the prediction accuracy, and provide a more sensitive measure of longitudinal
changes as well as a more powerful indication of any potential treatment/drug evaluations.

ReliefF+G∗
AmygdL
AmygdR
HippR
HippL
Vermis9
ParaHippL
ParaHippR
TempPlSupR
TempPlSupL
TempInfR
Cereb9L
Cereb8L
Cereb9R
Vermis10
FusiformL
TempPlMidR
TempInfL
TempPlMidL
Vermis8
Vermis7

[8] K. Chen, E. Reiman, G. Alexander, D. Bandy, R. Renaut,
W. Crum, N. Fox, and M. Rossor. An automated algorithm
for the computation of brain volume change from sequential
MRI’s using an iterative principal component analysis and
its evaluation for the assessment of whole brain atrophy
rates in patients with probable Alzheimer’s disease.
Neuroimage, 22(1):134–143, 2004.
[9] M. Davison. Multidimensional Scaling. New York: Wiley,
1983.
[10] C. Ding and J. Ye. 2-Dimensional singular value
decomposition for 2D maps and images. In Proceedings of
the Fifth SIAM International Conference on Data Mining
(SDM), pages 24–34, 2005.
[11] G. Forman. An extensive empirical study of feature
selection metrics for text classiﬁcation. Journal of Machine
Learning Research, 3:1289–1305, 2003.
[12] G. Fung and J. Stoeckel. Svm feature selection for
classiﬁcation of spect images of alzheimers disease using
spatial information. Knowledge and Information Systems,
11:243–258, 2007.
[13] I. Guyon and A. Elisseeﬀ. An introduction to variable and
feature selection. Journal of Machine Learning Research,
3:1157–1182, 2003.
[14] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene
selection for cancer classiﬁcation using support vector
machines. Mach. Learn., 46(1-3):389–422, 2002.
[15] X. He, D. Cai, and P. Niyogi. Laplacian score for feature
selection. In Advances in Neural Information Processing
Systems 18. MIT Press, 2005.
[16] I. T. Jolliﬀe. Principal Component Analysis.
Springer-Verlag, New York, 1986.
[17] S. S. Keerthi. Generalized lars as an eﬀective feature
selection tool for text classiﬁcation with svms. In
International Conference on Machine Learning (ICML),
2005.
[18] K. Kira and L. Rendell. A practical approach to feature
selection. In Sleeman and P. Edwards, editors, ICML ’92:
Proceedings of the Ninth International Conference on
Machine Learning, pages 249–256. Morgan Kaufmann,
1992.
[19] T. Kolda. Orthogonal tensor decompositions. SIAM J.
Matrix Anal. Appl., 23(1):243–255, 2001.
[20] I. Kononenko. Estimating attributes: Analysis and
extensions of relief. In ECML, page 171182, 1994.
[21] J. Krasaski and et al. Relation of medial temporal lobe
volumes to aget and memory function in nondemented

Acknowledgment
This research is supported in part by funds from the Arizona
State University and the National Science Foundation (NSF)
under Grant No. IIS-0612069.

7. REFERENCES
[1] G. Alexander and et al. Regional network of MRI gray
matter volume in healthy aging. Neuroreport, 17:951–956,
2006.
[2] G. Alexander and J. Moeller. Application of the scaled
subproﬁle model to functional imaging in neuropsychiatric
disorder: a principal component approach to modeling
brain function in disease. Human Brain Mapping,
2(1-2):79–94, 2004.
[3] G. Alexander and E. Reiman. Neuroimaging. M.F. Weiner,
A.M. Lipton (eds.). The Dementias: Diagnosis, Treatment
and Research, 3rd edition, 2003.
[4] R. G. Bachrach, A. Navot, and N. Tishby. Margin based
feature selection - theory and algorithms. In International
Conference on Machine Learning (ICML), 2004.
[5] B. Bader and T. Kolda. MATLAB Tensor Toolbox Version
2.2. http://csmr.ca.sandia.gov/∼tgkolda/TensorToolbox/,
January 2007.
[6] G. C. Cawley and N. L. C. Talbot. Gene selection in cancer
classiﬁcation using sparse logistic regression with bayesian
regularization. BIOINFORMATICS, 22:2348–2355, 2006.
[7] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support
vector machines, 2001. Software available at
http://www.csie.ntu.edu.tw/∼cjlin/libsvm.

1032

Figure 2: Top 12 regions (highlighted) detected by SPEC+G∗ (ranked from left to right and top to bottom).
Each region is shown in a diﬀerent view to enhance the visualy quality.

[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

adults with down’s syndrome: implications for the
prodromal phase of Alzheimer’s disease. American Journal
of Psychiatry, 159:74–81, 2002.
G. Lanckriet, T. D. Bie, N. Cristianini, M. Jordan, and
W. Noble. A statistical framework for genomic data fusion.
Bioinformatics, 20(16):2626–2635, 2004.
G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and
M. I. Jordan. Learning the kernel matrix with semideﬁnite
programming. Journal of Machine Learning Research,
5:27–72, 2004.
O. Lange, A. Meyer-Baese, M. Hurdal, and S. Foo. A
comparison between neural and fuzzy cluster analysis
techniques for functional MRI. Biomedical Signal
Processing and Control, 1(3):243–252, 2006.
L. Lathauwer, B. Moor, and J. Vandewalle. On the best
Rank-1 and Rank-(R1,R2,· · · ,RN) approximation of
higher-order tensors. SIAM J. Matrix Anal. Appl.,
21(4):1324–1342, 2000.
D. Leibovici and R. Sabatier. A singular value
decomposition of k-way array for a principal component
analysis of multiway data, PTA-k. Linear Algebra and Its
Applications, 269:307–329, 1998.
T. Li, C. Zhang, and M. Ogihara. A comparative study of
feature selection and multiclass classiﬁcation methods for
tissue classiﬁcation based on gene expression.
BIOINFORMATICS, 20:2429–2437, 2004.
Y. Li, C. Campbell, and M. Tipping. Bayesian automatic
relevance determination algorithms for classifying gene
expression data. BIOINFORMATICS, 18:1332–1339, 2002.
H. Liu and H. Motoda. Feature Selection for Knowledge
Discovery and Data Mining. Boston: Kluwer Academic
Publishers, 1998.
H. Liu and L. Yu. Toward integrating feature selection
algorithms for classiﬁcation and clustering. IEEE
Transactions on Knowledge and Data Engineering,
17:491–502, 2005.
H. Matsuda. Role of neuroimaging in Alzheimer’s disease,
with emphasis on brain perfusion SPECT. Journal of
Nuclear Medicine, 48(8):1289–1300, 2007.
S. Molchan. The Alzheimer’s Disease Neuroimaging
Initiative. Business Briefing: US Neurology Review, pages
30–32, 2005.
A. Ng, M. Jordan, and Y. Weiss. On spectral clustering:
Analysis and an algorithm. In The 14th Advances in Neural
Information Processing Systems (NIPS), 2001.
K. Nishino, Y. Sato, and K. Ikeuchi. Eigen-texture method:
appearance compression based on 3d model. In Proceedings
of IEEE Conference on Computer Vision and Pattern
Recognition, pages 618–624, 1999.
D. Pokrajac, V. Megalooikonomou, A. Lazarevic,
D. Kontos, and Z. Obradovic. Applying spatial distribution

[36]

[37]

[38]
[39]
[40]

[41]

[42]

[43]

[44]
[45]
[46]
[47]

[48]

[49]

[50]

1033

analysis techniques to classiﬁcation of 3d medical images.
Artificial Intelligence in Medicine, 33(3):261–280, 2005.
E. Reiman, R. Caselli, G. Alexander, and K. Chen.
Tracking the decline in cerebral glucose metabolism in
persons and laboratory animals at genetic risk for
Alzheimer’s disease. Clinical Neuroscience Research,
1:194–206, 2001.
B. Schökopf and A. Smola. Learning with Kernels: Support
Vector Machines, Regularization, Optimization and
Beyond. MIT Press, 2002.
J. Shawe-Taylor and N. Cristianini. Kernel Methods for
Pattern Analysis. Cambridge University Press, 2004.
M. Turk and A. Pentland. Eigenfaces for recognition.
Journal of Cognitive Neuroscience, 3(1):71–86, 1991.
N. Tzourio-Mazoyer and et al. Automated anatomical
labelling of activations in SPM using a macroscopic
anatomical parcellation of the MNI MRI single subject
brain. Neuroimage, 15:273–289, 2002.
M. Vasilescu and D. Terzopoulos. Multilinear analysis of
image ensembles: Tensorfaces. In Proceedings of the
European Conference on Computer Vision (ECCV), pages
447–460, 2002.
M. Vasilescu and D. Terzopoulos. Tensortextures:
multilinear image-based rendering. ACM Trans. Graph.,
23(3):336–342, 2004.
H. Wang, Q. Wu, L. Shi, Y. Yu, and N. Ahuja. Out-of-core
tensor approximation of multi-dimensional matrices of
visual data. ACM Trans. Graph., 24(3):527–535, 2005.
K. Worsley and K. Friston. Analysis of fMRI time series
revisited-again. NeuroImage, 2:173–181, 1995.
R. Wrede. Introduction to Vector and Tensor Analysis.
New York: Wiley, 1963.
J. Ye. Generalized low rank approximations of matrices.
Machine Learning, 61:167–191, 2005.
J. Ye, J. Chen, and S. Ji. Discriminant kernel and
regularization parameter learning via semideﬁnite
programming. In Proceedings of the twenty-fourth
International Conference on Machine Learning, pages
1095–1102, 2007.
J. Ye, S. Ji, and J. Chen. Learning the kernel matrix in
discriminant analysis via quadratically constrained
quadratic programming. In Proceedings of the 13th ACM
SIGKDD international conference on Knowledge discovery
and data mining, pages 854–863, 2007.
T. Zhang and R. Ando. Analysis of spectral kernel design
based semi-supervised learning. In Advances in Neural
Information Processing Systems 18, pages 1601–1608, 2006.
Z. Zhao and H. Liu. Spectral feature selection for
supervised and unsupervised learning. In International
Conference on Machine Learning (ICML), 2007.

www.sciedu.ca/air

Artificial Intelligence Research

2015, Vol. 4, No. 2

ORIGINAL RESEARCH

K Nearest Gaussian-A model fusion based
framework for imbalanced classification with noisy
dataset
Miao He1 , Jeffery D. Weir2 , Teresa Wu

∗1, 3

, Alvin Silva4 , Dianna-Yue Zhao3 , Wei Qian3, 5

1

School of Computing, Informatics, Decision Systems Engineering, Arizona State University, Tempe, USA
Department of Operational Science, Graduate School of Engineering & Management, Air Force Institute of Technology,
Write-Patterson AFB, USA
3
Sino-Dutch Biomedical and Information Engineering School, Northeastern University, Shenyang, China
4
Department of Radiology, Mayo Clinic, Scottsdale, USA
5
Department of Electrical and Computer Engineering, University of Texas, El Paso, TX, USA
2

Received: May 10, 2015
DOI: 10.5430/air.v4n2p126

Accepted: July 28, 2015
Online Published: August 12, 2015
URL: http://dx.doi.org/10.5430/air.v4n2p126

Abstract
Data quality issues such as data imbalance and data noise have great impact on the performances of many classifiers. Although
the co-existence of imbalance and noise appears in many real world datasets, the issue of imbalance and noise have mostly
been treated separately due to their different causes and problematic consequences. However, doing so may ignore the mutual
effects thus may not achieve optimal classification performance. In this research, we propose a model fusion based framework,
termed K Nearest Gaussian (KNG) to tackle the imbalance and noise issues jointly. KNG employs generative modeling method
(GMM) to extract the data characteristics from the training data which are less sensitive to data imbalance and noise. The data
characteristics are then used to establish Gaussian confidence regions which are used to achieve final classification in a K nearest
neighbor (KNN) manner. Experiments on seven UCI benchmark datasets and one medical imaging dataset show KNG method
greatly outperforms traditional classification methods in dealing with imbalanced classification problems with noisy dataset.

Key Words: Classification, Discriminative model, Generative model, K nearest neighbor, Gaussian mixture model

1

Introduction

Classification is a supervised learning problem which identifies the labels of new observations given a training dataset.
Classification methods extract knowledge from the training
dataset, and use the learned information to build models to
predict the class of new observations. Therefore, the success
of the classification methods highly depends on the quality
of the training dataset. The real world datasets suffer from
many quality issues.[1–3] Among them, the presences of im-

balance and noise are the key factors which draw great attentions.[3–5] Data imbalance occurs when one class (minority class) is greatly outnumbered by another class (majority
class). Most classification methods generally tend to underestimate the minority class due to the fact that majority
class dominates the whole dataset. As a result, the performance of most classification methods degrades for imbalanced dataset. One special case of imbalanced classification is one-class classification (a.k.a. unary classification)

∗ Correspondence: Teresa Wu; Email: Teresa.Wu@asu.edu; Address: School of Computing, Informatics, Decision Systems Engineering, Arizona
State University, Tempe, AZ, 85287-5906, USA.

126

ISSN 1927-6974

E-ISSN 1927-6982

www.sciedu.ca/air

Artificial Intelligence Research

2015, Vol. 4, No. 2

where learning is conducted on the training data containing only the instances of one class. Of particular interest,
this research is for more general imbalanced classification
problem which focuses on learning from both minority class
and majority class. Data noise occurs when the data has
been corrupted by various reasons such as systematic uncertainty, measurement error, human error, etc.[2, 5] It can
be characterized as (1) attribute noise, which refers to the
corruption in the features, and (2) class noise, which occurs
when the instances are incorrectly labeled. Noise may hinder the knowledge extraction from the data and thus makes
the classifier less effective, particularly if the classifier is
noise-sensitive.

identifying the class boundaries than discriminative models.

proaches ignore the mutual effects among imbalance and
noise may lead to new problems. For example, data cleaning techniques[7] have been widely used in dealing with data
noise which removes the noisy instances. If the removed instances happen to be the minority class, doing so may aggravate the level of imbalance. On the other hand, oversampling method such as synthetic minority oversampling
technique (SMOTE),[8] which has been widely used for imbalanced datasets, may cause the data even noisier if the
oversampled instances happen to be the noisy ones. One
may argue that techniques may be carefully chosen to handle the data imbalance followed by data noise or vice versa,
however, this two-step procedure may not be computational
efficient. A desirable solution is to tackle these two issues
jointly.

Presently, there are a number of studies attempting to overcome the classification problem with imbalance issue. They
can be categorized into two approaches: data-level approach
and algorithm-level approach.

Noticing the complementary nature of the generative and
discriminative classifiers, in this research, we propose a
novel generative-discriminative model fusion based framework, termed K Nearest Gaussian (KNG). A generative
classifier, Gaussian Mixture Model (GMM) is used to model
the training data as Gaussian mixtures and form adjustable
confidence regions of each Gaussian. GMM is chosen
here due to its capability in modeling arbitrary shaped densities.[11] Motivated by the idea of K-nearest neighbors
(KNN), KNG finds nearest Gaussians to classify the testing data instances. To validate the performance of KNG,
we use 7 UCI benchmark datasets. We purposely modify
Data imbalance and data noise often co-exist in the real the datasets to make them imbalanced and noisy. The exworld datasets, that is, the dataset is imbalanced as well perimental study shows that KNG method is more effecas noisy. Taking the CT imaging dataset as an exam- tive and robust than other widely used classification methple, the cancer patient often has a small portion of can- ods, such as Support Vector Machine (SVM),[12] Artificial
cer tissues compared with normal tissues on the CT images Neural Network (ANN),[13] Decision Tree (C4.5)[14] and
which makes the dataset imbalanced. And the reconstruc- KNN.[15] We further conduct a case study on a medical
tion method[6] used to generate the CT images comes with a imaging dataset to test the applicability of KNG in real
systematic uncertainty making the images inherently noisy. world application. The result also shows that KNG outperData imbalance affects the learning by degrading the recog- forms other commonly used classification methods.
nition power of the classifier on the minority class because
the majority class dominates, while data noise affects the
learning by providing inaccurate information to the classi- 2 Literature review
fier and thus misleads the classifier. Because of these differences, data imbalance and data noise issues have been 2.1 Review of techniques on handling imbalanced
data
treated separately in the data mining field. Yet, such ap-

The data-level approach uses different sampling techniques
to increase/decrease the size of the training data in order
to generate a balanced dataset. The representative methods
are: undersampling,[4] oversampling[4] and SMOTE.[8] Undersampling randomly removes the data instances of majority class and thus may lead to information loss. Oversampling increases the size of the data by generating replicates of minority class. One possible way is to add Gaussian
noise from the same distribution to the replicates to properly
present the original dataset.[16] However, it is known oversampling may lead to over fitting.[3] SMOTE oversamples
Most research on addressing the imbalance and noise em- the minority class by generating artificial data which are the
ploys discriminative models[9] which are effective in finding convex combination of the existing ones and thus improves
the class boundaries.[9, 10] However, discriminative models learning. However, SMOTE may not perform well when the
are sensitive to data imbalance and noise though, since they data instances used to generate new instances happen to be
work on the raw training data directly. Alternatively, gen- outliers and noisy examples.[17] Generally, the data-level aperative models[9] study the probability distribution of the proach alters the original training data distributions to make
training data and extract data characteristics from the train- the dataset less imbalanced. However, the change of origiing data which can be used to achieve classification. This nal data may compromise the underlying knowledge of the
is also known as semi-supervised learning which is consid- training data and thus is expected to be avoided.
ered as an extension of classification with added probabilistic information. Generative models may be less effective in The algorithm-level approach augments the existing methods to make them less sensitive to data imbalance. Many of

Published by Sciedu Press

127

www.sciedu.ca/air

Artificial Intelligence Research

the existing studies tackle the imbalance data by developing extensions of SVM. For example, boundary movement
(BM-SVM)[18] method changes the threshold value in SVM
decision function to push the class boundary towards the
majority class, Kernel-boundary alignment (KBA)[19] modifies the kernel matrix used in SVM training, and cost-SVM
(cSVM)[20] applies different penalties to different classes.
There are also a number of studies on extensions of ANN to
tackle the imbalance issue. For example, two-step ANN[21]
optimizes the weights and decision threshold values by using particle swarm optimization (PSO) to recognise the
monirity class, HIPPO method[22] trains the ANN in a novelty detection approach, and cost sensitive ANN[23] integrates the misclassification cost to ANN. In summary, most
of the algorithm-level approaches are extensions of the base
classifiers such as SVM and ANN. Generally, these extensions are algorithm dependent and application dependent.
Thus their effectiveness is limited by certain application
context.

2015, Vol. 4, No. 2

under certain context.
As a summary of both imbalance handling and noise handling techniques, data-level approach alters the original distribution of training dataset which may lead to loss of valuable information and thus is expected to be avoided. The
algorithm-level approach is developed based on existing
classifiers (such as SVM, ANN, C4.5), all of which employ
discriminative models which are sensitive to data imbalance
and noise since they work on the raw training data directly.

3

Proposed approach: K Nearest Gaussian

In this study, we propose a novel method, K Nearest Gaussian (KNG). Specifically, we employ a generative model,
GMM, into the training process to extract the data characteristics from training data. GMM has been shown promising in dealing with data imbalance issue in our previous
study[29] since the extracted data characteristics are expected
to be less sensitive to data imbalance and noise. The idea of
2.2 Review of techniques on handling noisy dataset KNN finding the class boundary is adopted here to differentiate the classes based on the extracted data characteristics.
The existing noise handling techniques can also be cat- In this section, we first review the basics of KNN in section
egorized into two approaches: data-level approach and 3.1 and GMM in section 3.2 followed by the details of our
algorithm-level approach.
proposed KNG in section 3.3.
Data-level approach, also known as noise elimination
techniques, handles the noise issue by removing the
noise instances from the training data. For example,
AJAX method[7] uses four types of data transformations—mapping, matching, clustering, and merging to detect and remove the noise data, Brodley and Friedl[24] compare the single algorithm filter, majority vote filter and consensus filter to identify and eliminate mislabeled training instances, Miranda et al.[25] combine the prediction of four
different machine learning methods to guide the noise detection and removal. These data-level approaches focus on
detecting and removing the noise instances. However, these
methods generally cannot distinguish the noise cases from
the rare cases. The removal of rare cases may lead bias to
the training data. In addition, noise instances which contain
error in some features may still contain useful information
in other features. Thus, the removal of noise under this circumstances may lead to loss of valuable information.

Algorithm-level approach tackles the noisy dataset by improving the learning process of an algorithm to make it
less sensitive to data noise. For example, Pechenizkiy et
al.[26] use feature extraction technique as a preprocessing
step in the training to diminish the effect of class noise,
Mingers[27] compares different search heuristics and stopping criteria in decision tree construction in dealing with
noise data, Quinlan[28] applies a post-pruning decision tree
building procedure to deal with noise data. Although most
of the algorithm-level approaches do not require data preprocessing, they are generally algorithm dependent or application dependent, thus are effective only when applied
128

3.1

K nearest neighbor

KNN is a discriminative model that classifies instances
based on the majority voting of its k nearest neighbors.[30]
Figure 1 is the illustration example of KNN algorithm.

Figure 1: Illustration example of KNN algorithm
In Figure 1, X is a testing instance, circles and triangles
are positive and negative class instances, respectively. KNN
first calculates the distances from X to other training instances, and classify X according to the majority voting of
its k nearest neighbors. K is predefined by the user. In Figure 1, when k = 1, X is classified as negative class since the
nearest neighbor is negative, while when k = 3, X is classified as positive class since the majority of its three nearest
neighbors is positive. Thus, X can be classified based on
the neighboring instances.
ISSN 1927-6974

E-ISSN 1927-6982

www.sciedu.ca/air

3.2

Artificial Intelligence Research

Table 1: Notations used in KNG algorithm

Gaussian mixture model

GMM is a generative model which is widely used to model
the distribution of the training data.[31, 32] GMM is used in
this research due to its well-known property in modeling
arbitrary shaped densities without pre-assumptions on the
distribution.[11] In addition, GMM has less parameters to
tune compared with other generative models such as Hidden Markov model[33] or Restricted Boltzmann machine.[34]
GMM models the probability density function of the feature
vector x by using a mixture of weighted Gaussians as shown
in equation 1:
PGM M (x|yi ) =

M
X

2
cim N (x, µim , σim
)

(1)

m=1

Where:
2
N (x, µim , σim
)

=

1
d

2 )2
(2πσim

e

kx−µim k2
− 12
σ2
im

Symbol

Meaning

Xtrain

Training dataset

Xtest

Testing dataset

y

True label

ypred
NumF

Predicted label
Number of folds in cross validation

n+ , n-

Number of Gaussian centers for +1/-1 class

µ+, σ2+

Centers and variances for GMM (+1 class)

µ-, σ2-

Centers and variances for GMM (-1 class)

β+

Confidence region adjusting coefficient (+1 class)

β-

Confidence region adjusting coefficient (-1 class)

A

Search range of ߚଵ

B

Search range of ߚଶ

k

Number of nearest Gaussians

CM

Confusion matrix

EvalMetric

Evaluation metric

(2)
Input:
; /* training data */
; /* testing data */
K; /* number of nearest Gaussians */
n+; /* number of Gaussian centers for positive class */
n-; /* number of Gaussian centers for negative class */
A; /* search range of
*/
B; /* search range of
*/

2
σim

Cim , µim and
are the weight, mean and covariance
of the mth mixture for class i. M is the number of mixtures which is predefined by the user. GMM method is
an unsupervised method reflecting the intra-class information. Given a training dataset with binary class labels
{(x1 , y1 ), · · · , (xn , yn )}, y ∈ {−1, 1}, the data are first divided into two groups by their class labels. Then the coef2
ficients Cim , µim and σim
are computed using the Expectation Maximization (EM) algorithm[35] to find maximum
likelihood function of the parameters iteratively.

3.3

K Nearest Gaussian (KNG)

Inspired by the KNN algorithm, which classifies an instance
based on neighboring instances, we propose our KNG algorithm to tackle the imbalance and noise data issues. Instead of using the neighboring data instances, KNG uses
the neighboring Gaussian mixtures to achieve classification.
Specifically, KNG first applies GMM method to model the
distributions of each class, and the data characteristics (such
as centroid, variance) of each Gaussian can be then used to
calculate the distances of the testing instance to the confidence region of each Gaussian. The smaller the distance,
the higher probability that the testing instance belongs to
the corresponding Gaussian distribution. Based on the distance to each Gaussian, the testing instance can be classified by majority voting. The data characteristics extracted
by GMM method, comparing with raw training data, are expected to be less sensitive to imbalanced and noisy dataset.
This makes KNG a promising method to deal with imbalanced and noisy data. The notations and pseudo code of
KNG algorithm can be found in Table 1 and Figure 2.
Published by Sciedu Press

2015, Vol. 4, No. 2

Output:
bestEvalMetric; /* the best Evaluation metric found */
Classifier; /* output classifier with EvalMetric*/
Function Calls:
GMMtrain (); /* train GMM classifier */
ComputeDist_PR (); /* compute point to region distance */
Sort (); /* sort the distances in ascending order */
ComputeCM (); /* compute confusion matrix */
ComputeEval (); /* compute evaluation metrics */
Begin
∈A
1) foreach
2)
foreach
∈B
3)
for h= 1: NumF
4)
[ ,
, ,
] ← GMMtrain (
, n+ , n-);
5)
foreach xi ∈
6)
foreach j ∈ n+
7)
Dist_PR (xi, j) ← ComputeDist_PR (xi, ,
,	 );
8)
end foreach
9)
foreach q ∈ n10)
Dist_PR (xi, q + n+) ← ComputeDist_PR (xi, ,
, );
11)
end foreach
12)
[order] ← Sort (Dist_PR(xi,:));
13)
yi pred = sum(y(order(1:K)));
14)
end foreach
15)
end for
16)
CM ← ComputeCM (y, ypred);
17)
EvalMetric ← ComputeEval (CM);
18)
if EvalMetric >= bestEvalMetric
19)
then bestEvalMetric ← EvalMetric
20)
end if
21) end foreach
22) end foreach
23) return [bestEvalMetric, Classifier];
End

Figure 2: Pseudo code for KNG Algorithm
In KNG algorithm, the ComputeDist_PR function is used to
compute point to region distance, which is defined as fol129

www.sciedu.ca/air

Artificial Intelligence Research

lowing:

2015, Vol. 4, No. 2

bustness of the proposed KNG with the increasing number
of Gaussian for both +/- classes.

Dist_P R(xi , µi , σi2 , β) = EuclideanDist(xi , µi )−βσi
(3)
β+ and β− are used to adjust the radius of the confidence
region for positive(minority) and negative (majority) Gaussians, respectively. They can be seen as weights for positive/negative classes. The unequal settings of β+ and β−
afford the KNG algorithm the flexibility to favor one class
more than the other. This property is very useful in dealing with imbalanced data in which the majority class dom- Figure 4: Impact of number of Gaussians to formation of
inates. Thus, by assigning higher β+ , KNG can be more class boundary
inclined to positive class and more positive instances can
be recognized. This can be shown in the following illustration example. In Figure 3, we apply GMM to find the
Gaussian mixtures for positive/negative classes. Circles are
positive instances and triangles are negative instances. The
Gaussian mixtures are represented by the concentric circles
where different circles represent different β values.
Figure 5: Impact of different β+ , β− to formation of class
boundary

Figure 3: Finding Gaussian mixtures for positive/negative
classes
KNG algorithm has five parameters to tune in order to
achieve its best classification performance: number of nearest Gaussians k, number of positive Gaussians n+ , number of negative Gaussians n− , and adjusting factors β+ , β− .
Number of nearest Gaussians k adjusts the number of Gaussians that will be used in finding the class boundary. When
k is small, only the nearby Gaussians are used in finding the
boundary, while when k is large, many far-away Gaussians
are involved in finding the boundary.
Figure 4 shows the impact of the number of Gaussians to
formation of class boundary. We keep k, β+ , β− , n+ as constant (all equal to one) while just change n- to see how the
increase of number of Gaussians for one class would affect
the formation of class boundary. When n− equals n+ , the
two classes are linearly separated by a straight line. When
we increase n− to 2 (see Figure 4b), the class boundary
bends more towards the positive class (dark gray region) and
thus more instances can be classified as negative. In addition, the linear boundary (see Figure 4a) becomes the intersection of two linear borderlines. If we further increase n−
(see Figure 4c), the class boundary can be further refined,
which shows as two intersections of three linear borderlines.
However, one potential issue with the increased number of
Gaussians is overfitting. It is our intention to assess the ro130

Figure 5 shows different settings of β+ and β− can push
the class boundary towards certain classes. To make it simple, we assume that all Gaussian mixtures have same variance. Figure 5a shows the positive (dark gray) and negative (light gray) class regions with the equal setting of β+
and β− (β+ =1, β− =1). The border of the two regions is
the class boundary. In Figure 5b and 5c, we observe that
increasing β+ (β+ = 2, β− = 1) can push the boundary
towards negative class and thus more instances can be classified as positive while increasing β− (β+ = 1, β− = 2)
can push the boundary towards positive class and thus more
instances can be classified as negative. As aforementioned,
β+ and β− are used as class-specific weights to adjust the
radius of the confidence region for positive/ negative Gaussians (circles with dash lines). Thus the tuning of β+ and β−
can push the class boundary towards certain class. For imbalanced datasets, the class boundary always skews towards
the positive class since the negative class dominates. Thus,
by assigning higher β+ , KNG can push the class boundary
to positive class and more positive instances can be recognized.

4

Experiments and results

In this section, we first evaluate the performance of KNG
using seven UCI benchmark datasets. Next, in a case
study we use a medical imaging dataset to test KNG on
real world application. To evaluate the performance of the
classifier, we use Gmean measure which has been widely
used[36–38] on imbalanced classifier for its ability to evaluate
the performance of a classifier on
√ both positive and negative
classes. Gmean is defined as acc+ ∗ acc− , where acc+
ISSN 1927-6974

E-ISSN 1927-6982

www.sciedu.ca/air

Artificial Intelligence Research

2015, Vol. 4, No. 2

(also called sensitivity) and acc− (also called specificity) are Table 3: Search ranges of Parameters
Method
Parameter
positive and negative class prediction accuracy, respectively.
Area Under the Curve (AUC[4] ) measure is also provided.
γ
SVM(rbf_kernel)

UCI benchmark datasets

The seven benchmark datasets we used in the experiments
are collected from UCI Machine Learning Repository.[39]
We call these datasets original datasets. The details of the
original datasets are summarized in Table 2. The multiclass
datasets are preprocessed as binary class problems, and the
number in the name indicates the positive class. For example, in iris2, class 2 is used as positive class and all the other
classes in the original data have been joined to represent the
negative class. Based on the original datasets, we generate
the imbalanced datasets by randomly removing 80% of the
negative class instances. Then, we further add 20% of random noise to make the datasets both imbalanced and noisy.
The noise is introduced using the following rules as literatures[5] did:
• Class noise: 20% of the class labels are randomly replaced by the opposite class labels
• Attribute noise: 20% of each attribute are replaced by
random values from the domain (value range) of that
attribute

Table 2: The UCI dataset used in the experiments
#Features

Imbalance Ratio
of Original dataset

Imbalance Ratio
of Imbalanced
dataset

Dataset

#Instance

breast_
cancer

683

10

1.9

9.3

diabetes

768

8

1.9

9.3

iris2

150

4

2

10.0

mammographic

830

5

1.1

5.3

yeast1

1484

8

2.2

11.0

wine2

178

13

1.5

7.6

glass3

214

9

1.8

9.2

We compare the performance of KNG method with SVM,
ANN, C4.5 and KNN. These methods are chosen because
they are widely used in classification problems. The KNG
method is developed using MATLAB. SVM is performed
using the libsvm MATLAB codes.[40] ANN, C4.5 and KNN
are performed using a machine learning software WEKA
3.6.9.[41] In this study, we use grid search technique[42] in
the parameter tuning process since it’s easy to implement.
The search ranges of the parameters are summarized in Table 3. Each method is performed using 10 fold cross validation. Because of the random nature of GMM method,
the result of KNG algorithm is performed 20 times for each
dataset, and the mean and standard deviation are reported.
Published by Sciedu Press

0-512

C

0-2048

confidence factor

0.1-0.5

KNN

# nearest neighbors k

1-9

ANN

learning rate

0.1-0.8

C4.5

4.1

Range

KNG

# nearest Gaussians k

1-5

#centers(+1 class, -1 class)

1-5

adjusting factors β+, β-

1-3

Table 4 shows the experimental results of Gmean measures
for both original and imbalanced and noisy (I+N) datasets.
For original datasets, KNG achieves best Gmean in three
out of seven datasets, and for iris2, wine2 datasets, KNG
is just marginal worse than the best method. This shows
that KNG is comparable to other classification methods on
original datasets. For I+N datasets, KNG greatly outperforms other methods in all seven datasets: for breast_cancer
dataset, KNG (0.967) outperforms the second best method
SVM (0.787) by 0.180; for diabetes dataset, KNG (0.708)
outperforms the second best method ANN (0.331) by 0.377;
for iris2 dataset, KNG (0.941) outperforms the second best
method ANN (0.763) by 0.178; for mammographic dataset,
KNG (0.789) outperforms the second best method KNN
(0.564) by 0.225; for yeast1 dataset, KNG (0.624) outperforms the second best method KNN (0.418) by 0.206; for
wine2 dataset, KNG (0.967) outperforms the second best
method KNN (0.676) by 0.291; for glass3 dataset, KNG
(0.721) outperforms the second best method SVM (0.509)
by 0.212. In summary, the average outperformance of KNG
to the second best method is 0.24. In all, KNG method
is very effective in dealing with imbalanced classification
problem with noisy datasets.
As shown in Table 5, the AUC measures are similar to
Gmean. KNG does not show outperformance on the original dataset, but for I+N datasets, KNG outperforms all other
methods in all seven datasets. For datasets such as mammographic and yeast1, methods such as ANN show less than
0.5 AUC measures which indicates worse than random performance. However, on these datasets, KNG shows much
better AUC measures (0.676± 0.000). We conclude KNG
is effective in dealing with imbalanced and noisy data.
We further analyze the robustness of each method using the
change of Gmean and the change of AUC which are defined as the measures on I+N datasets minus that of original
datasets. These metrics show that to what extent the coexistence of imbalance and noise can affect the performance
of a classifier. Small change of Gmean (or change of AUC)
would indicate the model is robust since it is less affected
by imbalance and noise. As seen, the performance of SVM,
C4.5, ANN and KNN drop dramatically on I+N datasets
compared with on original datasets. However, KNG main131

www.sciedu.ca/air

Artificial Intelligence Research

tains the minimal change of both Gmean and AUC for all
seven datasets, which is shown in Table 6 and Table 7. The
average change of Gmean for KNG is less than 1.6%, and
change of AUC is 1.3%, both are significantly better than
the other four methods. This is because the traditional classification methods, SVM, C4.5, ANN, KNN work on the
training raw data directly which is sensitive to data imbalance and noise and thus their performances are highly af-

2015, Vol. 4, No. 2

fected by the co-existence of imbalance and noise. KNG,
on the other hand, is designed to work on data characteristics extracted from the training data which are less sensitive
to data imbalance and noise. As a result, KNG is able to preserve the performance when imbalance and noise co-exist in
datasets. We conclude KNG is robust in dealing with both
imbalance and noise issues.

Table 4: Gmean metric
SVM
Dataset

C4.5

ANN

KNN

KNG

Orig

I+N

Orig

I+N

Orig

I+N

Orig

I+N

Orig

I+N

breast_cancer

0.976

0.787

0.959

0.000

0.962

0.517

0.970

0.457

0.976 ± 0.002

0.967 ± 0.000

diabetes

0.712

0.136

0.690

0.000

0.710

0.331

0.683

0.283

0.723 ± 0.002

0.708 ± 0.000

iris2

0.954

0.548

0.910

0.000

0.960

0.763

0.960

0.000

0.957 ± 0.014

0.941 ± 0.011

mammographic

0.836

0.111

0.838

0.435

0.816

0.237

0.800

0.564

0.797 ± 0.000

0.789 ± 0.000

yeast1

0.618

0.179

0.658

0.000

0.643

0.000

0.647

0.418

0.675 ± 0.000

0.624 ± 0.000

wine2

0.986

0.463

0.952

0.000

0.979

0.497

0.964

0.676

0.972 ± 0.000

0.967 ± 0.000

glass3

0.716

0.509

0.710

0.246

0.673

0.392

0.808

0.448

0.728 ± 0.014

0.721 ± 0.053

Table 5: AUC metric
SVM

C4.5

ANN

KNN

KNG

Dataset
Orig

I+N

Orig

I+N

Orig

I+N

Orig

I+N

Orig

I+N

breast_cancer

0.978

0.808

0.969

0.496

0.988

0.630

0.990

0.522

0.979±0.000

0.965±0.001

diabetes

0.753

0.570

0.764

0.500

0.812

0.549

0.785

0.480

0.746±0.000

0.729±0.000

iris2

0.971

0.790

0.945

0.485

0.994

0.785

0.996

0.485

0.958±0.018

0.950±0.004

mammographic

0.853

0.555

0.871

0.578

0.887

0.487

0.851

0.597

0.820±0.000

0.810±0.000

yeast1

0.700

0.562

0.755

0.497

0.775

0.497

0.745

0.467

0.696±0.000

0.676±0.000

wine2

0.988

0.682

0.955

0.500

0.998

0.524

0.982

0.677

0.984±0.000

0.973±0.000

glass3

0.785

0.657

0.707

0.486

0.727

0.484

0.827

0.510

0.763±0.028

0.762±0.051

Table 6: Robustness evaluation (Change of Gmean)

Table 7: Robustness evaluation (Change of AUC)

Dataset

SVM

C4.5

ANN

KNN

KNG

Dataset

breast_cancer

-18.9%

-95.9%

-44.5%

-51.3%

-0.9%

breast_cancer

-17.0%

-47.3%

-35.8%

-46.8%

-1.4%

diabetes

-57.6%

-69.0%

-37.9%

-40.0%

-1.4%

diabetes

-18.3%

-26.4%

-26.3%

-30.5%

-1.7%

iris2

-40.6%

-91.0%

-19.7%

-96.0%

-1.6%

iris2

-18.1%

-46.0%

-20.9%

-51.1%

-0.8%

-0.8%

Mammographic

-29.8%

-29.3%

-40.0%

-25.4%

-2.0%

-13.8%

-25.8%

-27.8%

-27.8%

-2.0%

Mammographic

-72.5%

-40.3%

-57.9%

-23.6%

SVM

C4.5

ANN

KNN

KNG

yeast1

-43.9%

-65.8%

-64.3%

-22.9%

-5.1%

yeast1

wine2

-52.3%

-95.2%

-48.2%

-28.8%

-0.5%

wine2

-30.6%

-45.5%

-47.4%

-30.5%

-1.1%

glass3

-20.7%

-46.4%

-28.1%

-36.0%

-0.7%

glass3

-12.8%

-22.1%

-24.3%

-31.7%

-0.1%

Average

-43.8%

-71.9%

-42.9%

-42.7%

-1.6%

Average

-20.1%

-34.6%

-31.8%

-34.8%

-1.3%

To further evaluate the applicability of KNG, a case study is
conducted on a medical imaging dataset which is collected
from Mayo Clinic, Arizona. The comparison experiment is
discussed in the next section.

132

4.2

Renal stone dataset

The case study is conducted on a renal stone dataset which is
collected from Department of Radiology, Mayo Clinic Arizona. This dataset is a Dual Energy CT dataset with 65 instances and 18 features for each instance, as shown in Table
8. In the 65 instances, 9 of them are cystine stones, and the
rest 56 are non-cystine stones. The objective of this case
ISSN 1927-6974

E-ISSN 1927-6982

www.sciedu.ca/air

Artificial Intelligence Research

study is to test if KNG can be used to effectively distinguish cystine stones from non-cystine stones with the presence of unneglectable level of imbalance and noise in the
data. This renal stone dataset has an imbalance ratio of 6.2,
and the noise comes from the systematic uncertainty of the
reconstruction methods which are used to generate the CT
images, as we mentioned in section 1.
The classification of cystine stones is important in clinical
practice for the following reasons. Firstly, cystine stone is
too dense to be broken up by extracorporeal shock wave
lithotripsy (a commonly used treatment method), which is
effective in breaking other types of stones, such as uric acid
stones, calcium oxalate stones and struvite stones, etc. Cystine stones are usually treated using percutaneous nephron
lithotripsy which is designed for removing dense stones.

2015, Vol. 4, No. 2

Thus, the diagnosis of cystine stone has a significant impact
on following treatment. Secondly, cystine stone is usually
caused by cystinuria, which is a genetic autosomal recessive metabolic disorder.[43] Thus, the diagnosis of cystine
stone indicates that the patient needs to take additional genetic screening tests other than medical treatment.[44]
We compare the performance of KNG with other machine
learning algorithms which has been widely used in renal
stone classification studies,[45, 46] such as SVM, C4.5, ANN,
KNN, Random Forest (RF) and Naive Bayes.
The experiments are performed using Weka 3.6.9 with 5fold cross validation technique applied. In addition to
Gmean, we also report sensitivity, specificity and AUC
which are commonly used evaluation metrics for medical
diagnosis field.

Table 8: The Renal Stone dataset
Dataset

#Classes

#Examples

#Positive

#Negative

IR

#Features

RenalStone

2

65

9

56

6.2

18

Feature Description
11 energy level measures
1 effective atomic number
6 material density measures

Figure 6: Sensitivity, specificity, Gmean and AUC metrics
Published by Sciedu Press

133

www.sciedu.ca/air

Artificial Intelligence Research

From Figure 6, we see that SVM completely fails on this
dataset. The zero sensitivity shows that SVM has no recognition ability of the cystine stones. C4.5, ANN, KNN and
RF show similar performance with equal sensitivity (44%)
and close specificity (93%, 96%, 95%, 93%, respectively),
which lead to very similar Gmean measures (64%, 65%,
65%, 64%, respectively) and similar AUC measures(71%,
79%, 74%, 71%, respectively). NB shows equal specificity (84%) with KNG, but much lower sensitivity (67% vs.
79%), lower Gmean (75% vs. 81%) and lower AUC (67%
vs. 82%). KNG achieves highest sensitivity (79%), highest Gmean (81%) and higest AUC (82%) among all seven
methods while maintains high specificity (84%). In conclusion, KNG outperforms other six methods in classification
of cystine stones.

5

Conclusion and discussion

In this research, we propose a discriminative and generative model fusion approach, KNG, to tackle classification
problems with imbalance and noise issues jointly. Instead
of modeling on the raw data directly, KNG applies GMM
to model the training data as Gaussian mixtures and form
adjustable confidence regions of each Gaussian which are
less sensitive to data imbalance and noise. The classification

References
[1] Seiffert C, Khoshgoftaar TM, Hulse JV, et al. An empirical study of
the classification performance of learners on imbalanced and noisy
software quality data. Information Sciences. 2014; 259: 571-95.
http://dx.doi.org/10.1016/j.ins.2010.12.016
[2] Zhu X, Wu X. Class noise vs. attribute noise: A quantitative study.
Artificial Intelligence Review. 2004; 22(3): 177-210. http://dx.d
oi.org/10.1007/s10462-004-0751-8
[3] He H, Garcia EA. Learning from Imbalanced Data. Knowledge and
Data Engineering. IEEE Transactions on. 2009; 21(9): 1263-84.
[4] Chawla NV. Data Mining for Imbalanced Datasets: An Overview.
In: Data Mining and Knowledge Discovery Handbook, Springer;
2005. p. 853-67.
[5] Sáez JA, Galar M, Luengo J, et al. Tackling the Problem of Classification with Noisy Data using Multiple Classifier Systems: Analysis of the Performance and Robustness. Information Sciences.
2013; 247: 1-20. http://dx.doi.org/10.1016/j.ins.2013.
06.002
[6] Hsieh J, Nett B, Yu Z, et al. Recent advances in CT image reconstruction. Current Radiology Reports. 2013; 1(1): 39-51. http:
//dx.doi.org/10.1007/s40134-012-0003-7
[7] Galhardas H, Florescu D, Shasha D, et al. AJAX: an extensible data
cleaning tool. ACM SIGMOD Record. 2000; 29(2): 590. http:
//dx.doi.org/10.1145/335191.336568
[8] Chawla NV, Bowyer KW, Hall LO, et al. SMOTE: Synthetic Minority Over-sampling Technique. Journal of Artificial Intelligence
Research. 2002; 16: 321-57.
[9] Jordan A. On Discriminative vs. Generative Classifiers: A Comparison of Logistic Regression and Naive Bayes. Advances in Neural
Information Processing Systems. 2002; 14: 841.

134

2015, Vol. 4, No. 2

is achieved by majority voting of the K nearest Gaussians
for the testing instances. The experimental results on seven
UCI datasets and one medical imaging dataset show that
KNG is more effective in dealing with imbalanced dataset
with noisy features than other commonly used classification
methods.
In the experiments, we find the performance of KNG highly
depends on the proper settings of parameters. As we can see
in Table 3, there are five parameters to tune in the KNG algorithm, each of which has a wide search range. The parameters are tuned through grid search method in the experiments
which is criticized to be computationally expensive and thus
inefficient.[47] In addition, the search ranges of these parameters are determined by empirical experience which may not
lead to optimal model performance. Facing all the above
challenges, we plan to improve the KNG algorithm by employing advanced optimizer, such as Particle Swarm Optimization[48] to address the computation concerns as well as
improving the parameter tuning performance as one future
task. Secondly, one notable fact reported in this research is
the experiments are conducted on mildly imbalanced (imbalanced ratio ranges from 5 to 11) dataset. We plan to
further explore the applicability of KNG on heavily imbalanced problems (e.g., 10e-5 in credit card fraud detection
problem[16] ) as another future work.

[10] Lasserre J. Hybrid of generative and discriminative methods for machine learning, University of Cambridge; 2008.
[11] Lindsay BG. Mixture models: Theory, geometry, and applications.
Mathematics. 1995.
[12] Cortes C, Vapnik V. Support-vector Networks. Machine Learning.
1995; 20(3): 273-97.
[13] Kriesel D. A brief introduction to neural networks. Retrieved August
15, 2011.
[14] Quinlan JR. C4. 5: programs for machine learning. Morgan kaufmann. 1993; 1.
[15] Tan PN, Steinbach M, Kumar V. Introduction to data mining. Pearson Addison Wesley; 2006.
[16] Salazar A, Safont G, Vergara L. Surrogate techniques for testing
fraud detection algorithms in credit card operations. In: Security
Technology (ICCST). International Carnahan Conference on. 2014.
p.1-6.
[17] He H, Bai Y, Garcia EA, et al. ADASYN: Adaptive Synthetic
Sampling Approach for Imbalanced Learning. In: Neural Networks, IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on, 2008.
[18] Wu G, Chang EY. Class-boundary Alignment for Imbalanced
Dataset Learning. In: ICML 2003 Workshop on Learning from Imbalanced Data Sets II, Washington, DC; 2003.
[19] Wu G, Chang EY. Aligning Boundary in Kernel Space for Learning Imbalanced Dataset. In: Data Mining, ICDM’04. Fourth IEEE
International Conference on. IEEE, 2004.
[20] Veropoulos K, Campbell C, Cristianini N. Controlling the Sensitivity of Support Vector Machines. In: Proceedings of the International
Joint Conference on Artificial Intelligence; 1999.
[21] Adam A, Ibrahim Z, et al. A Two-Step Supervised Learning Artificial NeuralL Network for Imbalanced Dataset Problems. InterISSN 1927-6974

E-ISSN 1927-6982

www.sciedu.ca/air

[22]
[23]
[24]
[25]

[26]

[27]
[28]

[29]

[30]
[31]

[32]

[33]
[34]

[35]

Artificial Intelligence Research

national Journal of Innovative Computing Information and Control.
2012; 8(5a): 3163-72.
Japkowicz N, Myers C, Gluck M. A novelty detection approach to
classification. In: IJCAI; 1995.
Berardi VL, Zhang GP. The effect of misclassification costs on neural network classifiers. Decision Sciences. 1999; 30(3): 659-82.
Brodley CE, Friedl MA. Identifying mislabeled training data. arXiv
preprint, no. arXiv:1106.0219, 2011.
Miranda AL, Garcia LPF, Carvalho AC, et al. Use of classification
algorithms in noise detection and elimination. In: Hybrid Artificial
Intelligence Systems; 2009. p. 417-24.
Pechenizkiy M, Tsymbal A, Puuronen S, et al. Class noise and supervised learning in medical domains: The effect of feature extraction. In: Computer-Based Medical Systems, 2006. CBMS 2006.
19th IEEE International Symposium on 2006.
Mingers J. An empirical comparison of selection measures for
decision-tree induction. Machine learning. 1989; 3(4): 319-42.
Quinlan JR. The effect of noise on concept learning. In: Machine
learning: An artificial intelligence approach, Morgan Kaufmann;
1986. p. 149-66.
He M, Wu T, Silva A, et al. Augmenting Cost-SVM with Gaussian Mixture Models for Imbalanced Classification. Artificial Intelligence Research. 2015; 4(2): 93.
Cover T, Hart P. Nearest neighbor pattern classification. Information
Theory, IEEE Transactions on. 1967; 13(1): 21-7.
Wang K, Ren Z. Enhanced Gaussian Mixture Models for Object
Recognition using Salient Image Features. In: International Conference on Mechatronics and Automation, ICMA; 2007.
Reynolds DA, Rose RC. Robust text-independent speaker identification using Gaussian mixture speaker models. Speech and Audio
Processing, IEEE Transactions on. 1995; 3(1): 72-83.
Rabiner L, Juang BH. An introduction to hidden Markov models.
ASSP Magazine, IEEE. 1986; 3(1): 4-16.
Fischer A, Igel C. An introduction to restricted Boltzmann machines. In: Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. 2012: 14-36.
Dempster AP, Laird NM, Rubin DB. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal statistical
Society. 1977; 39(1): 1-38.

Published by Sciedu Press

2015, Vol. 4, No. 2

[36] Akbani R, Kwek S, Japkowicz N. Applying Support Vector Machines to Imbalanced Datasets. In: Machine Learning: ECML 2004,
Berlin Heidelberg, 2004.
[37] Wang HY. Combination Approach of SMOTE and Biased-SVM
for Imbalanced Datasets. In: Neural Networks; 2008. IJCNN
2008.(IEEE World Congress on Computational Intelligence). IEEE
International Joint Conference on IEEE; 2008.
[38] Imam T, Ting KM, Kamruzzaman J. z-SVM: An SVM for Improved
Classification of Imbalanced Data. In: AI 2006: Advances in Artificial Intelligence, Berlin Heidelberg, 2006.
[39] Bache K, Lichman M. UCI Machine Learning Repository, Irvine,
CA: University of California, School of Information and Computer
Science.
[40] Chang CC, Lin CJ. LIBSVM : A Library for Support Vector Machines. ACM Transac-tions on Intelligent Systems and Technology.
2011; 2(27): 1-27.
[41] Hall M, Frank E, Holmes G, et al. The WEKA Data Mining Software: An Update. SIGKDD Explorations. 2009; 11(1).
[42] Bergstra J, Bengio Y. Random search for hyper-parameter optimization. The Journal of Machine Learning Research. 2012; 13: 281305.
[43] Wu J. Chapter 58 – Urolithiasis. In: Integrative Medicine, 3rd ed,
WB Saunders Company, 2012.
[44] Breuning MH, Hamdy NA. From gene to disease; SLC3A1,
SLC7A9 and cystinuria. Nederlands tijdschrift voor geneeskunde.
2003; 147(6): 245.
[45] Krishna Apparao R. Statistical and Data Mining Aspects on Kidney Stones: A Systematic Review and Meta-analysis. Open Access
Scientific Reports. 2012.
[46] Kumar K, et al. Artificial Neural Networks for Diagnosis of Kidney
Stones Disease. I.J. Information Technology and Computer Science.
2012; 7: 20-25.
[47] Bergstra J, Bengio Y. Random search for hyper-parameter optimization. The Journal of Machine Learning Research. 2012; 13: 281305.
[48] Kennedy J. Particle swarm optimization. In: Encyclopedia of Machine Learning, Springer US; 2010. p. 760-6.

135

1022

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

Petri Net Translation Patterns for the Analysis of
eBusiness Collaboration Messaging Protocols
Andrew L. Feller, Teresa Wu, Dan L. Shunk, and John Fowler

Abstract—Electronic messaging protocols such as RosettaNet
(RN) automate the asynchronous exchange of business documents between collaborating trading partners over the Internet.
Such protocols commonly employ mechanisms such as retries,
time-outs, and fault handling to overcome uncertainty in the
timing and reliability of message transmission, receipt, and
processing. To determine the reliability and performance of such
a protocol under varying network and message processing conditions, we have developed reusable patterns for Petri net modeling of these common mechanisms and used them to assemble
a timed Petri net simulation that represents the RN standard’s
behavior. The reusable patterns are derived through translation
of the abstract representations provided in the RN standard into
an executable model, paving the way for a multimodeling approach for supply chain communications. The resulting stochastic
Petri net is simulated to generate performance curves guiding
improved protocol reliability.
Index Terms—Communication system fault tolerance, message
systems, modeling, multimodeling, Petri nets, protocols, simulation, supply chain.

I. I NTRODUCTION

P

ETRI net models have a good track record for performance
validation of time-dependent concurrent processes, such as
communication and messaging protocols, as demonstrated in
[1]–[3]. One challenge in using this approach, however, is the
need for the analyst to generate complex Petri nets from scratch,
which accurately represent a protocol’s behavioral rules and
implied event sequencing. To make this modeling process easier
and more reliable, we have developed preset patterns for model
generation. Our purpose in presenting this research is to provide
a reusable set of such patterns that can improve the analysis,
design, and implementation of messaging protocols used to
integrate business processes across company boundaries. This
paper focused on modeling the popular RosettaNet (RN) supply
chain collaboration standard; however, the same patterns will
be useful for modeling other communication and messaging
protocols. This research contributes to the practice of model
generation and presents analysis results characterizing the robustness of an RN Partner Interface Process (PIP) under varying
Manuscript received June 15, 2007; revised July 11, 2008. First published
August 14, 2009; current version published August 21, 2009. This work
was supported in part by a grant from Intel Corporation. This paper was
recommended by Associate Editor M. P. Fanti.
A. L. Feller is with the Arizona State University, Tempe, AZ 85282 USA, and
also with American Express Technologies, Phoenix, AZ 85027 USA (e-mail:
andrew.feller@asu.edu).
T. Wu, D. L. Shunk, and J. Fowler are with the Department of Industrial Engineering, Arizona State University, Tempe, AZ 85287 USA (e-mail:
teresa.wu@asu.edu; dan.shunk@asu.edu; john.fowler@asu.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TSMCA.2009.2025022

message timing and loss rate conditions. In particular, we used a
timed Petri net (TPN) simulation derived from the RN protocol
to determine the rates at which a single-action PIP will fail
when individual messages are lost or delayed by the network,
or when a trading partner’s acknowledgment response is delayed. This question is important because of the number of RN
transactions that are processed by firms on a continuous basis.
Reviewing RN log files at a prominent semiconductor firm
showed over 400 000 PIPs processed in a month with a failure
rate of approximately 3% [4]. It is of considerable importance
to determine whether the protocol’s dynamic behavior could be
part of the problem.
Our research was conducted in the context of developing
methods for translating supply chain models between levels
of abstraction. Supply chain models fall into two paradigms:
conceptual models that represent the supply chain in ways
that are readily accessible for practitioners to comprehend
and analysis or execution models based on mathematical and
computer representations used to simulate and analyze system
performance. Conceptual models are easy to understand and
have face validity but do not allow for detailed model execution
and analysis of system-wide behavior. Formal analysis and
execution models allow system behavior and performance to be
analyzed or executed in a simulation environment but require
knowledge of the methodology being used and significant
expertise to build. As supply chains grow in complexity and
interdependence, more expressive and complete methods are
needed for translating and integrating conceptual and analytical
modeling approaches to enable the analyses of the integrated
flows of information and materials that determine a supply
chain’s performance.
Our approach uses a multimodeling method developed using
conceptual graphs, which is a knowledge representation method
from the field of artificial intelligence (AI) [5]. The approach
builds on systems theory [6] to generate translation patterns
using conceptual graphs that capture all required behaviors
specified in the RN protocol. The patterns are then used to
generate an executable TPN.
There are three contributions made by this research. First, we
define a set of patterns for translating RN PIPs into Petri nets
which accurately represent the messaging behavior and event
sequencing that occurs under the RN implementation framework (RNIF). Next, we apply these patterns to generate a complete Petri net model representing the asynchronous behavior of
a single-action PIP including all possible state/event paths that
the protocol can lead to, including successful completion, failure processing, and recovery. Finally, we use the derived Petri
net to analyze the performance of a PIP to determine robust

1083-4427/$26.00 © 2009 IEEE

FELLER et al.: PETRI NET TRANSLATION PATTERNS

thresholds for message timing and loss rates. In particular, we
answer the question “what are the network thresholds below
which RN’s performance becomes unacceptably degraded?”
The following sections are organized as follows. In
Section II, we provide background on multiparadigm modeling,
RN, and the elements of Petri nets and conceptual graphs relevant to our approach. We then present our translation patterns in
Section III, followed by the generation of a complete Petri net
representing a single-action PIP in Section IV. In Section V, we
present analytical results for the PIP’s performance, followed
by conclusions and further research in supply chain multimodeling in Section VI.
II. BACKGROUND
This section discusses prior works in multiparadigm modeling and introduces RN PIPs and RNIF along with the elements
of Petri nets and conceptual graphs used in our approach to
model translation and analysis.
A. Multimodeling
This paper builds on research and practice in multimodeling
frameworks. Multimodeling entered mainstream system analysis in the mid 1990s with a number of candidate approaches
resolving into the Unified Modeling Language (UML) standard,
which is an object-oriented system-modeling method that has
gained wide practical acceptance and use [7], [8]. UML’s
success is due in large part to the successful balancing of
the conflicting goals of human comprehension versus rigorous
analysis [9]. UML diagrams represent systems using succinct
diagrams that aid human understanding; however, the syntax
and semantics do not directly enable formal analysis and execution. UML behavior diagrams are used to represent RN PIPs;
however, an approach for model translation and simulation to
analyze RN’s capabilities had not been accomplished.
To our knowledge, few works have been done to translate
between abstraction levels in supply chain models; however,
multiparadigm model translation is an emerging field with new
methodologies under development. Zeigler first developed a
formal representation for discrete event simulation and discussed abstraction techniques [10]. Fishwick’s early work in
simulating complex processes over multiple levels of abstraction in 1986 [11] began a stream of research into multimodeling
[12], [13]. Fishwick and Zeigler reported on a multimodeling
methodology used to bridge levels of abstraction in qualitative
models [14], yet the proposed framework did not solve the
translation between multiparadigm models. As Fishwick and
Ziegler [14] stated:
The problem of semiautomating the process of taking
a conceptual nonexecutable object-based model of the
system and converting it into an executable model remains
a very hard problem that has taken form in AI, simulation,
and software engineering.
Subsequent work by Fishwick and others has developed
multimodeling methods for real-time simulation [15] and general multimodeling software frameworks [16]. More recently,
Vangheluwe et al. [17] describe the emerging field of multi-

1023

paradigm modeling, asserting it can assist with the following:
1) transformation between models described in different formalisms; 2) clarifying relationships between models at different levels of abstraction; and 3) describing models in formal
specifications or metamodeling.
A number of multiformalism metamodels have been developed for translating between models described in different
formalisms [18]; however, the focus has been on linking analytical models, such as in the hybrid environments surveyed by
Barton and Lee [19], which link discrete event simulation with
differential and algebraic equations, and not with linking across
levels of abstraction.
Mapping between levels of abstraction in simulation frameworks is beginning to be addressed at simulation conferences
[17], [20]–[22]; however, model translation in the context of
supply chains has not been addressed. Current research works
reported in the IEEE T RANSACTIONS ON S YSTEMS , M AN ,
AND C YBERNETICS —PART A, ACM Transactions on Modeling and Computer Simulation, and IEEE T RANSACTIONS
ON C ONTROL S YSTEM T ECHNOLOGY [18], [23], [24] focus
on modeling imbedded control systems and translating between
analytical formalisms. Kim et al. [25] use hybrid modeling
to represent traffic networks; however, specific techniques for
translating supply chain models are not considered. Still, some
of the conclusions reached for analyzing these systems are
relevant to the supply chain domain. For example, Lee and Hsu
[26] translate between UML diagrams and Petri nets for model
execution and conclude that UML diagrams lack the precise
semantics needed for qualitative and quantitative analyses of
system behavior. Lin and Jeng also transform UML diagrams
into Petri nets to verify the logical correctness and dynamic
behavior of a system framework [27], and Du et al. [28]
extend message sequence charts using Petri nets to analyze the
soundness of cooperative workflows. Similar to our approach,
these researchers all use Petri nets for analysis. In their works,
however, they provide no formal mechanism or patterns for
translation, relying on the modeler’s expertise to perform the
mapping. Researchers have translated UML diagrams into several variants of Petri nets, such as object coordination nets [29],
[30], colored or high-level Petri nets [31]–[33], generalized
stochastic Petri nets [34], [35], and other approaches including
generalized semi-Markov processes [36], queuing networks
based on execution graph/machinery models [37], and firstorder temporal logic [38]. These studies, however, did not focus
on patterns for model translation or on messaging protocols.
Similar to this paper, prior UML diagram translations have
focused on the behavioral diagrams that come closer to defining
a finite state machine for an object’s behavior. The addition of
rigor for execution is required due to problems in the formal
tractability of statecharts for specifying and verifying designs
[39]. The UML diagrams and derived executable models share
properties that allow the modeling primitives to be correlated, leading to partial mappings of one modeling approach
to another. Graph grammars provide a compact method for
translation between graphical system representations but lack
the semantic richness needed to map from abstract conceptual
models to an executable representation. This leaves a gap in
the current research—translation patterns between conceptual

1024

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

Fig. 2.

Fig. 1. UML diagrams representing single-action PIP 4A1: “Notify of Strategic Forecast.”

and executable models needed in the supply chain domain.
Our method for model translation uses system theory and
conceptual graphs to fill this gap. We build on Fishwick’s
framework [13] for classifying models using system theory to
organize our approach and provide a formal basis for model
translation.
B. Conceptual Model: RN
RN PIPs are specialized system-to-system eXtended Markup
Language (XML)-based dialogs used to execute business
processes between trading partners [40]. Each PIP specification
includes a business document with XML vocabulary and a
business process with the choreography of the message dialog.
Several hundred different PIPs are used to automate standard
information flows between supply chain participants, such as
exchanging forecasts and transmitting orders, and thousands
of partner connections have been implemented in industry.
It is not unusual for a trading partner to execute millions
of PIP transactions in a year, yet the conceptual model for
most exchanges is quite simple. Every PIP specification uses
a UML sequence diagram to document a prespecified twoway transaction pattern for the intercompany business process
and a modified UML statechart diagram to further define the
PIP functional behavior (see Fig. 1) [41]. In this example, the
success path has one trading partner sending a business message
containing a forecast notification. Once the message format is
verified, the receiving partner sends an acknowledgment.
In addition to these diagrams, PIP specifications contain
tables and text specifying trading partner roles and performance
controls, such as retry and time-out parameters.
The general behavior for fault management required of all
PIPs is specified in the RNIF. RN uses a two-stage process
for validating and processing business messages, which first
determines if it is readable and well formatted. If it is, an ac-

Two-stage validation and processing of an RN message.

knowledgment signal is returned to the sender. If not readable at
all, the receiver waits for a resend. If readable but not correctly
formatted, an exception signal is sent, and the originator of
the PIP aborts processing. After a correctly formatted message
is acknowledged, the receiver then processes the message’s
business content. This second stage can complete normally or
find a business rule violation in the content, leading to a subsequent error condition. Depending on whether the PIP would
have been completed by this message and acknowledgment
signal or would have required a subsequent responding business
message, the failure to process the business content results
in either an exception signal message or the initiation of a
Notification of Failure (NoF)—PIP 0A1. The invocation of the
NoF PIP begins another pattern of PIP execution with a twostage process for validating and processing the NoF message
and its own sequence of retries and message processing. This
general pattern for the two-stage process of handling a received
RN message is shown in Fig. 2. The RNIF specification provides flowcharts and descriptive text defining these message
validation and error handling processes, but no methods or
guidelines for analysis.
We have chosen RN as our conceptual model for three reasons. First, it provides a structured starting point for developing
model translations for supply chain messaging.
Second is our interest in determining causes for observed
failure rates in RN transactions. Finally, the patterns derived
from RN can be used to analyze other protocols, such as the
European Internet Electronic Data Interchange (EDI) guidelines [42].
C. Translating to an Executable Model: Petri Nets
A Petri net is a bipartite directed graph of alternating place
and transition nodes connected by arrows [43]. States are
represented by places shown as circles on the graph. Events are
represented by transitions that are shown as bars. The places can
hold tokens represented by dots within a circle. Finally, a transition rule specifies how tokens migrate through the network.
The basic rule is that all of the places preceding a transition
must hold a token in order for the transition to fire. Firing the
transition then removes a token from each of the transition’s
input places and puts tokens in each of the transition’s output
places. Formally specified, a Petri net structure “S” is as a
four-tuple: S = (P, T, I, O), where P = {p1 , p2 , p3 , . . . , pn }
is a finite set of places, T = {t1 , t2 , t3 , . . . , tm } is a finite set

FELLER et al.: PETRI NET TRANSLATION PATTERNS

of transitions, the set of places and transitions is disjoint, I is
the input function that maps places to transitions, and O is the
output function that maps transitions to places.
Sometimes the I and O functions are combined into a single
flow function F . The state of a system is represented by tokens
using a marking vector M with an integer valued number of
tokens (≥ 0) at each place in P .
A number of Petri net variants have been applied to supply chain and communication protocol modeling. This paper
focused on timed place transition (P/T) nets. A P/T net adds
integer valued arc weights and places capacities to the net
definition to achieve a more concise representation. TPNs allow
the representation of temporal behavior by adding a duration
parameter to the transitions of a Petri net. This allows the
execution of a modular form of discrete event simulation that
can be used to characterize system behavior using traditional
simulation techniques. The duration or delay timings can also
be represented as random variables. The assignment of exponentially distributed random variables to the duration periods
in a TPN defines a stochastic Petri net. Petri nets have been
used for modeling and analysis of the quality of Web services
by Xiong et al. [44] and combined with network simulation
tools by Ye and MacGregor to enable formal verification of the
correctness of a network protocol [45].
We build upon this background to develop Petri net translation patterns that accurately represent the behavior of the RN
messaging protocol and the RNIF. To document the patterns,
we use conceptual graphs. Conceptual graphs are a synthesis
of Peirce’s existential graphs [5] with dependence graphs and
the semantic networks of AI and can be translated to logically
equivalent expressions in predicate calculus [5], [46]. A conceptual graph is a connected bipartite graph made of concept
nodes and conceptual relation nodes. Every relation node has
one or more arcs, each of which must be connected to some
concept. Concepts are mapped into a set of type labels, and
the type labels are arranged into a hierarchical lattice called a
type hierarchy which imposes a partial ordering based on the
meaning of the concept types. Conceptual relations are also
mapped into a hierarchy with relations of the same type having
the same number of arcs.
Conceptual graphs have been used in the development of
expert systems, semantic database retrieval, inference capabilities, and natural language processing. Feller and Rucker used
conceptual graphs to extend structured analysis modeling in
the analysis of shop floor system requirements, developing a
metamodel for system analysis that generated a TPN [47], [48].
Willumsen [49] developed a method for generating executable
prototypes from conceptual models using rule-based languages,
but this was a general approach for model execution focused
on information system design and not tailored for supply chain
modeling. Baresi and Pezze [31] used graph grammars to
develop customization rules for translating UML models into
Petri nets; however, the graph grammars lack the richness of
conceptual graphs’ ability to ground the mappings semantically
and the extensibility to develop new and varying translations. In
our use of conceptual graphs to generate Petri net patterns for
RN, we limit our type hierarchy of concepts to two fundamental
types: states (Q in system theory notation) and transitions (δ in

1025

Fig. 3. Example of a conceptual graph translation pattern.

system theory notation). The states and transitions are labeled
to provide semantic relevance for the translation patterns.
A simple example for translating a portion of the RNIF
flowchart for sending a message that also enables retries into
a Petri net is shown in Fig. 3.
III. T RANSLATION PATTERNS
Translation patterns for generating Petri nets for RN PIPs
were derived by first analyzing PIP and Petri net representations
using the system theory framework to characterize the required
mapping. This clarified underlying patterns of abstraction,
specification, and representational capability that the modeling
approaches provide. This was followed by conceptual modeling
of the constructs needed to span the gap from the RN model to
an executable Petri net.
A. System Theoretic Analysis
The initial step uses the seven elements of system theory
to analyze RN PIP and RNIF specifications, including text,
tables, and diagrams. Our analysis of the expressive capability
of the RN model from a knowledge representation standpoint
provides a formal basis for correlating modeling elements from
RN to the states and events of a Petri net.
To demonstrate, consider an example PIP: 4A1—Notify of
Strategic Forecast and system theory’s time set variable T . In
the RN conceptual model, time is captured by numbering steps
in a UML sequence diagram in a fashion similar to a flowchart.
The business operational view for a PIP specification provides
the modified statechart diagram shown in Fig. 1 [41]. Start and
end state conditions are given in the PIP specification along
with a 2-h time limit to acknowledge receipt and a maximum
retry count of three. An RN UML sequence diagram for this
interaction set specified in the functional service view of the
PIP specification is also shown in Fig. 1. This diagram shows
the sequencing of the forecast notification and acknowledgment, which is a simple two-step handshake. Based on these
views, the time set for this PIP can be described as an index
variable representing the time at which the system reaches one
of four possible states. The possible states are the initial or
START state, a “Request Sent” intermediate state, and either
a successful “Acknowledge Received” END state or a FAILED
state. The examination of the intermediate request sent state,
however, shows that there are elements of the state set Q that
are not represented. This state may last for up to 8 h (up to three
retries with 2-h maximum duration each for acknowledgment);

1026

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

TABLE I
A NALYSIS OF RN AND P ETRI N ET M ODELS U SING S YSTEM T HEORY E LEMENTS

however, the UML diagram does not represent the number of
requests sent or how long the state might endure—these are implicit and result from other information provided in tables in the
PIP specification and the rules for retries specified in the RNIF.
Comparing the time set for the target executable model, a
TPN formulation for simulating this sequence would require a
place initialized with a token count to explicitly represent the
allowed number of retries, a counter place for the number of
attempted retries, a failure transition that requires three retry
tokens, and a timed retry transition. This indicates the need for
additional modeling primitives in the executable representation
that the RN diagrams do not contain. As demonstrated using
this example, the time set T and state set Q are modeling
elements that must be augmented when translating an RN PIP
into a Petri net.
In similar fashion, our analysis focused on identifying all differences in the seven system modeling elements in the RN specifications and the required elements needed in a Petri net model.
We cataloged the modeling capabilities of the RN specifications
and the required Petri net, and their respective placement in
the system theory framework. The application of this approach
identified five key differences. These were the following.

end may be stateless. The Petri net, on the other hand,
must include explicit places for the intermediate states of
each trading partner and must represent the dependence
structures that determine timing and sequencing for each
of these states.
4) The transition function δ. Petri net transitions and execution rules are required to allow net execution, and a complete transition function between all states is required.
The state changes represented on RN PIP diagrams are
not fully formed since the trading partner roles have
no internally defined states and, therefore, no transition
function.
5) The output function λ. Finally, the RN specification
has no formal output analysis functions such as reachability analysis. This is an obvious difference between
conceptual and executable models: the lack of a formal
capability for output analysis of system behavior. A Petri
net allows analysis for specific properties of the overall
system. Various output functions are needed, depending
on the desired analysis, for example, analysis for possible
deadlock states, or simulation results to characterize RN’s
performance.

1) The time set T . As described, the representation of explicit time sequences is needed for intermediate states
and events. Some information about timing is provided
in text, tables, and flowcharts in the PIP specification but
is not included in the specification in a structured format.
2) The output set Y . RN PIPs do not have a fully defined output function. The sequence diagrams show a
return response, but the RN specifications call for failure
processing to be specified in individual trading partner
agreements, allowing the possibility of an ambiguous end
state. The Petri net will require specific places to hold
tokens representing the end state of both trading partners.
An additional specification of places is needed for representing all messages that retries and error processing may
require.
3) The state set Q. RN does not specify a complete set
of internal trading partner states. The diagrams for the
PIP show that the internal conditions other than start and

These results are summarized in Table I. Our analysis exposed a lack of system theoretic elements in the RN PIP and
RNIF specifications needed to analyze a PIP’s behavior.
In particular, RN specifies a limited time set, state set, output
set, and transition function and has no output function for analytical results. RN does specify a more thorough input set and
a detailed definition of the allowed content for each message.
The gaps we identified pinpoint the model elements that need
to be added to an RN conceptual model to make it executable.
In particular, a Petri net model for a PIP must add places to
represent all possible messages, including exception signals and
notifications of failure, and requires a sequential state/execution
model that reflects the implied time set, including retries,
acknowledgment time-out, and exception processing, to derive
the complete set of possible states, i.e., the output set.
The next step in developing general patterns for RN PIPs was
recognizing that partner roles interact solely by the exchange of
interface messages in specific sequences and time frames and

FELLER et al.: PETRI NET TRANSLATION PATTERNS

1027

Fig. 4. All possible interface messages for a single-action PIP success or
failure.

listing these interface messages as states with transitions for
sending and receiving them. For the single-action PIP patterns,
there are eight possible interface messages:
1) single-action PIP messages
a) initial message: Msg;
b) acknowledgment: Ack;
c) exception: Excp;
2) NoF messages
a) sender NoF: S-NoF;
b) acknowledgment of sender NoF: S-NoF Ack;
c) responder NoF: R-NoF;
d) acknowledgment of responder NoF: R-NoF Ack;
3) out of band messages
a) manual reset.
These messages are organized for generating the Petri net, as
shown in Fig. 4, with the interface messages in the middle
and send/receipt transitions on either side representing the
trading partner’s actions. The success path for this type of
PIP is represented by the top two rows of two message places
and two send/receive transition pairs. All other messages and
send/receive transitions are used for failure processing.
Next, a set of patterns for modeling the internal states of the
trading partners connected to the interface messages is specified
using conceptual graphs.
B. Conceptual Graph Pattern Mapping
RN is a message passing protocol; thus, we begin by focusing
on the business and signal (acknowledgment and exception)
messages that can be sent and the process specifications for
sending and receiving them.
RNIF provides flowcharts to define the process for message retries, receipt, acknowledgment, validation, and failure
processing; however, the flowcharts cannot be directly translated due to ambiguous states and processes in the conceptual
model. To execute properly, two patterns were formulated to
represent the messaging structure. The first, shown in Fig. 5,
handles retries beginning with a start state and the initiating
“send message” event. This is followed by two states: a sent
message and the enablement of retries. Subsequent states and
events capture the retries and the possible resulting states, in-

Fig. 5. (a) Conceptual graph: Send message with retries. (b) Resulting Petri
net pattern for retry mechanism.

cluding a retry failure, an acknowledgment leading to success,
or receipt of an exception signal leading to an exception failure.
Note that there is a general pattern that can be used to model
retries with a transition that initiates retries (A), and output for
retries (B), retry failure (C), and transitions that halt retries due
to success or failure events (D).
The second pattern follows from Fig. 2, represents the receipt, validation, and processing of a business message by the
message recipient, and signals results to the sender. This pattern
begins with the message-sent state from the prior graph and
allows the recipient to handle unreadable or invalid messages.
As specified in the RNIF, this pattern also enables resending
an acknowledgment or exception signal if a subsequent retry
message is received. These patterns are shown in Fig. 6.
The other significant pattern required for the analysis of
an RN PIP behavior is processing a failure notice: RN PIP
0A1—NoF. RN has defined this special PIP for handling failure
notification when a failure arises after the normal processing of
a PIP may be completed by the other trading partner. This requires special handling to ensure that the status of the business
process being conducted remains synchronized between both
parties. When a NoF is received, the recipient must abort the
current process and prevent subsequent processing no matter
what state he/she is in. This requires an interrupt scheme and
the capability to clear any possible state occurring when the
NoF is received. In addition, the NoF receipt must be acknowledged to alert the trading partner when (and whether) the
process state has been synchronized; otherwise, an out of band

1028

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

Fig. 6. Conceptual graph and resulting Petri net pattern for RNIF compliant message receipt, validation, processing, and signaling of success or failure.

communication is required. The conceptual graph pattern and
resulting Petri net structure for processing a failure notice are
shown in Fig. 7. This graph starts with the failure notification
state and includes a general pattern of linkages to the potential
state and event sets that must be cleared or aborted when a NoF
message is received. Note that, for clearing an undetermined
state, the model requires an intermediate state for processing
the NoF and a series of events (labeled δA , δB , δC , etc.) for
clearing any potential state that the trading partner might be in.
The output of this graph is a failure state for the PIP initiator
and a closed incomplete state for the PIP recipient. This pattern
must be combined with the first pattern for sending an action

message, so that retries are generated for the NoF as well. As
specified in the RNIF, a retry failure for the NoF does not
generate another NoF but goes to an out of band communication
(e.g., a phone call) to reset the process.
The translation patterns shown in Figs. 5–7 augment the RN
model for analysis. The time set is specified using timed transitions, the state and output sets now contain all intermediate
states and possible outcomes, and the transition function is
well defined based on the constraint of alternating state/event
nodes and the use of Petri net firing rules. Output functions
are also available using Petri net analysis capabilities, such as
reachability graphs and simulations.

FELLER et al.: PETRI NET TRANSLATION PATTERNS

1029

TABLE II
R EGRESSION T ESTING PATHS FOR A S INGLE -ACTION PIP

Fig. 7. Conceptual graph and resulting Petri net structure for processing a
NoF—PIP 0A1.

The following conventions have been used to simplify the
conceptual graphs. State concepts are labeled with Q and
subscripted with a state label. Event or transition concepts
are labeled with δ and subscripted with an event label, and
timed events have a bold outline. Arrows without a conceptual
relation (circle) imply a functional flow relation linking states
and events. In one instance, an increment action is used. Conceptual graphs use a diamond for actors that can perform an
algorithm, for example, incrementing a counter for the number
of retries.
The only remaining element needed for model execution is
the input set Ω. Tokens are initially required in the set {QStart ,
QRetriesLeft (3), QNo Readable Msg , and QNo NoF Rcvd }.
Adding the initial marking completes the executable model.
IV. A PPLICATION —S INGLE -ACTION PIP
The patterns we developed were combined to compose a
complete Petri net for a single-action PIP. The first step was to
link the patterns to the business and signal interface messages
that can occur. For a single-action PIP, there is one business
message and two potential NoF messages that require retry
mechanisms. The business message can be responded to by

an acknowledgment or an exception, and the NoF can be
acknowledged or not—in which case, an out of band reset
message is required. These messages are represented by sent
message states at the interface between the trading partners.
Transitions are added to send, receive, and possibly lose each
of these interface messages. A composition process was used
to combine the patterns to generate the complete Petri net.
One key feature of the net is a processing gate with an
initialized token indicating that no NoF has been received. This
place is used as an input and an output for all processing steps
other than those used to process a failure notification. This
freezes processing regardless of state when the trading partner
receives a failure notice.
It should be noted that earlier attempts to develop a Petri
net representing RN PIP behavior without these patterns were
considerably more difficult and time consuming, yet resulted
in a network that was overspecified and had process anomalies
that were only discovered later during testing. This motivated
our development of reusable patterns representing common
building blocks of protocol behavior.
Regression testing of the pathways that the process can take
was initially used to determine the correctness of the result. This
regression testing exercised all of the contingencies identified
in Table II. Through structured token game exercises with
the model, we were able to validate correct processing and
address unforeseen contingencies when augmenting the model
for continuous simulation.
The resulting Petri net structure is shown in Fig. 8. Surprisingly, the resulting network for a simple single-action PIP has
238 562 possible states, making the analysis of the reachability tree cumbersome. For this reason, the resulting structure
was additionally tested using linear temporal logic (LTL) with
Büchi Automota using the TPN analyzer (TINA) from the

1030

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

Fig. 8. Complete Petri net for a single-action PIP.

LAA–CNRS [50] to assure that the only output (deadlock)
states correspond to the synchronized outputs specified by the
protocol (END or FAIL) and that one of these final states is
always achieved.
LTL statements for checking this are shown in (1) and (2)
using LTL symbols [ ] (box) for always and  (diamond) for
eventually
[ ] (QStart ⇒   ((QEnd ∧ QComplete )
∨(QFailed ∧ QClosedIncomplete ))) .

(1)

Equation (1) indicates that the start state always eventually
leads to either the End/Complete or the Failed/Closed Incomplete state. To assure that these are the only end states, the LTL
checker is used to test (2). This tests that the End/Complete
and Failed/Closed Incomplete states are the only cases that
deadlock
¬ ((QEnd ∧ QComplete ) ∨ (QFailed ∧ QClosedIncomplete ))
⇒ ¬DEAD.

(2)

Results from these tests were true, indicating that the model
allows only synchronized end states. Additional verification
using reachability and simulation ensured that the model’s
behavior matched the RN specification.

We augmented the Petri net with additional structure to
reset initial conditions and enable continuous processing of the
modeled protocol with variable random choice of process paths,
message loss rates, and variable random delay timings. This
allowed running the model for a 100 000 iterations in a few
minutes, allowing all paths to be exercised and verifying that the
frequency of intermediate states conforms to what is expected
and allowable.
This also allowed controlled experimentation for performance analysis. Results from these experiments are reported
in the following section.
V. P ERFORMANCE A NALYSIS
The Petri net structure was created and analyzed using the
set of publicly available research tools shown in Fig. 9. The
Platform Independent Petri net Editor [51], [52] was used to
construct the net and perform visual token game simulations,
invariant analysis, and model testing. This editor generates a
standard Petri Net Markup Language file that is readily input
into the TINA tool mentioned earlier for LTL model checking
and formatting. A textual output from TINA was then modified
manually to generate an input file for a TPN tool [53] for
running simulations. To simplify the generation of multiple
runs, additional patterns were devised to reset token markings to

FELLER et al.: PETRI NET TRANSLATION PATTERNS

Fig. 9.

1031

Petri net tools used for model construction and analysis.

the original start state and capture protocol success and failure
counts for output analysis.
The augmented Petri net for TPN simulation requires no
transient since each run starts in a known state and can run
many thousands of iterations in less than a minute. The TPN
tools allow probabilities to be defined for the transitions following free-choice places. This allows network message loss
rates to be specified using transitions for lost messages that
were added to each interface message. The tools also allow
for deterministic or stochastic (exponentially distributed) timed
transitions. Deterministic transitions were used for the time-out
of retries and stochastic transitions for message transmission
and acknowledgment processing times. C language functions
for pseudorandom sampling imbedded in the tool generate the
delay. The TPN tools were operated with a script to enable input
parameters to be modified for each set of runs. One hundred
thousand runs were simulated for each setting to generate a ratio
of protocol successes versus failures.
Two research questions were considered: 1) What is the impact of network conditions on RN’s success rate and 2) what is
the impact of a trading partner’s delay to acknowledge a message on the success rate? The network condition question focuses on how robust the protocol is to network-caused message
losses and stochastic message delivery timing. In other words,
what are the network performance thresholds below which
the RN protocol’s performance begins to degrade and become
unacceptable? The second question considers how sensitive the
protocol is to acknowledgment processing and response time. In
other words, how long is “too long” for a trading partner to send
a response. Full factorial experiments were run to answer these
questions, running the Petri net over a wide range of reasonable
parameter values.
A. Sensitivity to Network Conditions
Experiments were run to determine the protocol’s sensitivity
to message transmission speed and loss rate. With variable conditions on the Internet and trading partners that span the globe,
messages take varying amounts of time to get to their destination and may be lost with varying frequency. For our analysis,
we varied random message loss rates in 1% increments from
0.1% to 10% and varied the average message speed using an
exponentially distributed processing time in increments from
6 to 24 min. Acknowledgment time was pegged at twice the
message transmission time to allow for reasonable messageacknowledgment turnaround. These experiments showed that

Fig. 10. Protocol sensitivity to network conditions.

the RN protocol for a single-action PIP achieves highly reliable
communications (> 99%) as long as the message loss rate
is below 6% for very fast (i.e., 6 min) message transmission
and below 3.5% for very slow (i.e., 24 min) transmission. For
message loss rates above 4%–5%, the performance degrades,
losing about 0.4% in yield for each percentage increase in individual message loss. These results are summarized graphically
in Fig. 10, where transmission times were adjusted to provide a
visible spread in the curves. These results are not tractable analytically due to repeated interactions between retried messages
and probabilistic message loss and delay timing.
B. Sensitivity to Acknowledgment Processing Time
Additional experiments were run to determine the protocol’s
sensitivity to delays in acknowledgment response time. Using
an exponential random variable for delay, the average delay
time was varied in regular increments from 1.2 min to 15 h. The
protocol is very robust to acknowledgment delays as long as the
acknowledgment time is less than an hour and then degrades
quickly, losing nearly 10% for each additional hour of average
delay. This is not surprising; the protocol is asynchronous and
should be able to handle reasonable delays in response.
Still, it shows that average downtimes longer than an hour
in a trading partner’s responding system will begin to create
failed PIPs that need to be handled. These results are shown
graphically in Fig. 11.
After completing our simulation analysis, we determined
that the result for exponentially distributed delay times might
be determined analytically by combining three cumulative

1032

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

VI. C ONCLUSION AND F UTURE R ESEARCH

Fig. 11. Protocol sensitivity to acknowledgment delays—simulated and analytical results.

exponential distributions. If the probability of success for a
single retry is governed by the cumulative exponential distribution in
P(success) = 1−e−(Time−out/(ProcessingDelay+AckTransmitDelay))
(3)
then assuming independence with three retries, the protocol
yields a success/delay curve according to




Y ield = 1− 1−P(success) 1−P(success) 1−P(success) . (4)
Graphing the analytical versus simulated results (Fig. 11)
verifies the Petri net simulation; however, the simulation can
be used to determine performance parameters that analytical
solutions cannot such as cycle time. Cycle time analysis performed using our model showed potential business impacts in
scenarios where RN is used to transmit time-sensitive data such
as for just-in-time inventory replenishment.
We found that, when degraded network performance and
response delays cause PIPs to fail between 0.025% and 1.5% of
the time, the successful PIPs complete in less than 2 h between
68% and 82% of the time and less than 4 h over 94% of the time.
The PIPs that do fail under these conditions, however, take
more than 6 h to fail and reset between 55% and 94% of the
time. Even with rapid partner response, network failure rates
that exceed 3% begin to cause failed PIPs to take over 6 h more
than 13% of the time, indicating that using RN for just-in-time
operations that must complete the same day may not be robust
unless network conditions are adequate.
The prediction of failure rates for the protocol based on
trading partners’ ability to respond might be used to assess the
costs of manually resetting failed transactions in a high volume
transaction environment. We have observed RN transaction log
files [54] that indicate a preponderance of failed PIPs come
from relatively few specific trading partners. This analytical
result supports the establishing of an objective benchmark for
acknowledgment response times in the 5-min range to improve
protocol performance.

We conclude with several comments about our work and a
discussion on future work to generalize and automate the use of
our patterns through the use of a composition algorithm.
This research has been validated through testing and analysis
augmented with comparisons to industry logs of RN transactions. We find the model consistent with the RN protocol
specification and our observations of the protocol’s behavior
in practice, but we were unable to characterize the log files
statistically for comparison with our results since network
delays, loss rates, and acknowledgment delays could not be
extracted from the logs and we were limited to viewing the logs
on site due to proprietary restrictions.
We also recognize a number of needed extensions, such as
analyzing the RN two-action PIP and other protocols such
as the GS1 EDI over the Internet Transport Communication
Guidelines [42] which allow for configurable retry schedules.
For the GS1 guidelines, it would be useful to determine how
tuning the retry mechanism can improve robustness under
degraded network conditions and processing delays.
An area of interest considered for future work is the generalization and automation of model translation using the messaging behavior patterns we developed. We have looked into
composition rules for using these patterns and have some useful
results. First, the protocol to model must conform to the same
rules as RN, such as timed retries, a two-stage process for
validation and processing of messages, and a rule that aborts
processing when a failure message is received. An algorithm
for composing a model using our patterns would then begin
by classifying the interface messages that are sent between the
parties into categories that commonly occur in communication
protocols. In RN, we have the following four: 1) messageWithRetry; 2) acknowledgmentSignal; 3) exceptionSignal; and
4) failMessageWithRetry.
Each message is represented by a place with transitions
attached to send, receive, and possibly lose the message. The
patterns attach to the appropriate message send and receive
transitions: retry patterns to send the messages with retry;
receipt pattern with validation, processing, and signaling of
results to the receipt of these messages; and failure processing
to the receipt of exception and fail signals. One key to making
an algorithmic process work is attributing the places and transitions in the patterns to indicate the states that must be cleared
and events that must be prevented when an exception or failure
notice is received. Then, the necessary “clear state” transitions
can be added and linked to a “Process Failure Notice” state
place to pull a token from each possible internal state place, and
internal event transitions can be linked to a “No-NoF Received”
place to prevent internal events when the process aborts.
We performed this algorithm manually when composing the
Petri net for our study; however, when extending the patterns
to handle a greater number of interactions such as in RN’s twoaction PIP, we found that considerable additional complexity
was introduced due to a proliferating number of potential
state combinations that occur across the trading partner boundary and the addition of a time-out clock maintained by the
PIP originator. Time-out is an additional failure mechanism

FELLER et al.: PETRI NET TRANSLATION PATTERNS

implemented in the RNIF for two-action PIP that is still being
considered in our research.
For example, whether an exception occurs during the initial
validation of message format or later during processing, the
message requires two different exception states. This is because, for the latter, the acknowledgment of message format
will deactivate the retry timer. Thus, one type of exception deactivates a timer, but the other does not. Adding the possibility
of a time-out failure for the PIP originator requires the addition
of new message places to handle exception signals that may be
sent after an acknowledgment signal has deactivated the timeout mechanism.
When we consider the challenge of supply chain message
modeling more generally, we note that the creation and application of models that accurately represent the behavior of supply
networks are challenging for at least two reasons.
One is the inherent complexity of the environment being
modeled. For example, we were surprised in the course of our
work that a simple single-action message/handshake protocol
could explode into nearly a quarter of a million possible states!
A second reason for difficulty in applying models of supply
chain performance is that analytical models are often difficult to
comprehend by practitioners in industry, precisely because they
are driven by the high level of complexity in the environment.
Our research is directed at building bridges to span this division
by combining tools and approaches from software engineering,
AI, and simulation.
We believe that this research represents an important step
toward developing a rigorous approach for creating multiparadigm modeling frameworks in the supply chain domain. Future
work will be directed at making the analysis of supply chain
messaging and collaboration more accessible while increasing
the use of analytical methods in practice. To this end, we plan to
develop translations for other conceptual models of the supply
chain into executable simulation models.
ACKNOWLEDGMENT
A. L. Feller would like to thank W. Zuberek for the assistance
with the TPN tool and the RN staff at Intel for their support.
The authors would like to thank the reviewers and editor for
their comments that have helped to improve this paper.
R EFERENCES
[1] K. Garg, “An approach to performance specification of communication
protocols using timed Petri nets,” IEEE Trans. Softw. Eng., vol. SE-11,
no. 10, pp. 1216–1225, Oct. 1985.
[2] B. Berthomieu and M. Diaz, “Modeling and verification of time dependent
systems using time Petri nets,” IEEE Trans. Softw. Eng., vol. 17, no. 3,
pp. 259–273, Mar. 1991.
[3] A. Yakovlev, S. Furber, R. Krenz, and A. Bystrov, “Design and analysis
of a self-timed duplex communication system,” IEEE Trans. Comput.,
vol. 53, no. 7, pp. 798–814, Jul. 2004.
[4] Intel Corporation, Intel RosettaNet Transaction Logs, Chandler, AZ, 2006.
[5] J. Sowa, Conceptual Structures: Information Processing in Mind and
Machine. Reading, MA: Addison-Wesley, 1984.
[6] L. Padulo and M. A. Arbib, Systems Theory: A Unified State Space Approach to Continuous and Discrete Systems. Philadelphia, PA: Saunders,
1974.
[7] Introduction to OMG’s Unified Modeling Language (UML). [Online].
Available: http://www.omg.org/gettingstarted/what_is_uml.htm
[8] OMG,
UML Summary, 1997. ver. 1. [Online]. Available:
http://www.omg.org/cgi-bin/doc?ad/97-08-11

1033

[9] OMG UML Specification, Mar. 2003. ver. 1.5 formal/03-03-01. [Online].
Available: http://www.omg.org/docs/formal/03-03-01.pdf
[10] B. Zeigler, Theory of Modeling and Simulation. New York: Wiley, 1976.
[11] P. A. Fishwick, “Hierarchical reasoning, simulating complex processes
over multiple levels of abstraction,” Ph.D. dissertation, Univ.
Pennsylvania, Pittsburgh, PA, 1986.
[12] P. A. Fishwick, “The role of process abstraction in simulation,” IEEE
Trans. Syst., Man, Cybern., vol. 18, no. 1, pp. 18–39, Jan./Feb. 1988.
[13] P. A. Fishwick, “An integrated approach to system modeling using a
synthesis of artificial intelligence, software engineering and simulation
methodologies,” ACM Trans. Model. Comput. Simul., vol. 2, no. 4,
pp. 307–330, Oct. 1992.
[14] P. A. Fishwick and B. P. Zeigler, “A multimodel methodology for qualitative model engineering,” ACM Trans. Model. Comput. Simul., vol. 2,
no. 1, pp. 52–81, Jan. 1992.
[15] K. Lee and P. A. Fishwick, “OOPM/RT: A multimodeling methodology
for real-time simulation,” ACM Trans. Model. Comput. Simul., vol. 9,
no. 2, pp. 141–170, Apr. 1999.
[16] R. M. Cubert and P. A. Fishwick, “A framework for distributed objectoriented multimodeling and simulation,” in Proc. IEEE Winter Simul.
Conf., 1997, pp. 1315–1322.
[17] H. Vangheluwe, J. de Lara, and P. Mosterman, “An introduction to multiparadigm modeling and simulation,” in Proc. AIS Conf. (AI, Simul. Planning High Autonomy Syst.), 2002, pp. 9–20.
[18] P. J. Mosterman and H. Vangheluwe, “Guest editorial: Special issue
on computer automated multi-paradigm modeling,” ACM Trans. Model.
Comput. Simul., vol. 12, no. 4, pp. 249–255, Oct. 2002.
[19] P. I. Barton and C. K. Lee, “Modeling, simulation, sensitivity analysis,
and optimization of hybrid systems,” ACM Trans. Model. Comput. Simul.,
vol. 12, no. 4, pp. 256–289, Oct. 2002.
[20] H. Vangheluwe and J. de Lara, “Metamodels are models too,” in Proc.
IEEE Winter Simul. Conf., 2002, pp. 597–605.
[21] H. Vangheluwe and J. de Lara, “Computer automated multi-paradigm
modeling: Meta modeling and graph transformation,” in Proc. IEEE Winter Simul. Conf., 2003, pp. 595–603.
[22] M. Traore, “A meta-theoretic approach to modeling and simulation,” in
Proc. IEEE Winter Simul. Conf., 2003, pp. 604–612.
[23] J.-S. Lee, M. C. Zhou, and P.-L. Hsu, “Multiparadigm modeling for
hybrid dynamic systems using a Petri net framework,” IEEE Trans.
Syst., Man, Cybern. A, Syst., Humans, vol. 38, no. 2, pp. 493–498,
Mar. 2008.
[24] P. J. Mosterman, J. Sztipanovits, and S. Engell, “Special issue on computer
automated multi-paradigm modeling in control systems,” IEEE Trans.
Control Syst. Technol., vol. 12, no. 2, pp. 223–234, Mar. 2004.
[25] Y. W. Kim, T. Kato, S. Okuma, and T. Narikiyo, “Traffic network control
based on hybrid dynamical system modeling and mixed integer nonlinear
programming with convexity analysis,” IEEE Trans. Syst., Man, Cybern.
A, Syst., Humans, vol. 38, no. 2, pp. 346–357, Mar. 2008.
[26] J. Lee and P. Hsu, “Design and implementation of the SNMP agents for
remote monitoring and control via UML and Petri nets,” IEEE Trans.
Control Syst. Technol., vol. 12, no. 2, pp. 293–302, Mar. 2004.
[27] C.-P. Lin and M. D. Jeng, “An expanded SEMATECH CIM framework for
heterogeneous applications integration,” IEEE Trans. Syst., Man, Cybern.
A, Syst., Humans, vol. 36, no. 1, pp. 76–90, Jan. 2006.
[28] Y. Y. Du, C. J. Jiang, and M. C. Zhou, “Modeling and analysis of real-time
cooperative systems using Petri nets,” IEEE Trans. Syst., Man, Cybern. A,
Syst., Humans, vol. 37, no. 5, pp. 643–654, Sep. 2007.
[29] H. Giese and G. Wirtz, “Visual modeling,” J. Vis. Lang. Comput., vol. 12,
no. 2, pp. 183–202, Apr. 2001.
[30] H. Giese, J. Graf, and G. Wirtz, “Closing the gap between object-oriented
modeling of structure and behavior,” in Proc. 2nd Int. Conf. UML—
Beyond the Standard, Fort Collins, CO, Oct. 28–30, 1999, pp. 534–549.
[31] L. Baresi and M. Pezze, “On formalizing UML with high-level Petri nets,”
in Concurrent Object Oriented Programming and Petri Nets: Advances in
Petri Nets, vol. 2001, Lecture Notes in Computer Science. New York:
Springer-Verlag, 2001, pp. 276–304.
[32] J. Garrido and M. Gea, “A coloured Petri net formalisation for a UMLbased notation applied to cooperative system modeling,” in Proc. 9th
Int. Workshop Interactive Syst. Des., Specification, Verification, 2002,
pp. 16–28.
[33] J. Xu and J. Kuusela, “Modeling execution architecture of software system
using colored Petri nets,” in Proc. 1st Int. Workshop Softw. Perform.,
Santa Fe, NM, 1998, pp. 70–75.
[34] J. Merseguer, J. Campos, S. Bernardi, and S. Donatelli, “A compositional
semantics for UML state machines aimed at performance evaluation,” in
Proc. 6th IEEE Int. WODES, 2002, pp. 295–302.
[35] S. Bernardi, S. Donatelli, and J. Merseguer, “From UML sequence diagrams and statecharts to analyzable Petri net models,”

1034

[36]

[37]
[38]

[39]
[40]
[41]
[42]
[43]
[44]
[45]
[46]
[47]

[48]
[49]
[50]
[51]
[52]
[53]
[54]

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 39, NO. 5, SEPTEMBER 2009

in Proc. 3rd Int. Workshop Softw. Perform., Rome, Italy, 2002,
pp. 35–45.
C. Lindemann, A. Thümmler, A. Klemm, M. Lohmann, and O. Waldhorst,
“Performance analysis of time-enhanced UML diagrams based on stochastic processes,” in Proc. 3rd Int. Workshop Softw. Perform., Rome,
Italy, 2002, pp. 25–34.
V. Cortellessa and R. Mirandola, “Deriving a queuing network based
performance model from UML diagrams,” in Proc. 2nd Int. Workshop
Softw. Perform., Ottawa, ON, Canada, 2000, pp. 58–70.
L. Lavazza, G. Quaroni, and M. Venturelli, “Combining UML and formal
notations for modeling real-time systems,” in Proc. 8th Eur. Softw. Eng.
Conf. Held Jointly With 9th ACM SIGSOFT Int. Symp. Found. Softw. Eng.,
2001, pp. 196–206.
A. Simons, “On the compositional properties of UML statechart diagrams,” in Proc. 3rd Workshop Rigorous Object Oriented Methods, 2000,
pp. 4.1–4.19.
RosettaNet Implementation Framework: Core Specification: RosettaNet,
Jul. 13, 2001. [Online]. Available: http://rosettanet.org
RosettaNet PIP Specification, Cluster 4: Inventory Management, Segment A: Collaborative Forecasting, PIP4A1: Notify of Strategic Forecast:
RosettaNet2002. [Online]. Available: www.RosettaNet.org
Issue 1 EDIINT AS1 and AS2 Transport Communication Guidelines:
GS1, Feb. 2006. [Online]. Available: http://www.gs1.org/docs/gsmp/
EDIINT_AS1_AS2_Transport_Comm_Guide_i1.pdf
J. L. Peterson, Petri Net Theory and the Modeling of Systems.
Englewood Cliffs, NJ: Prentice-Hall, 1981.
P. C. Xiong, Y. S. Fan, and M. C. Zhou, “QoS-aware web service configuration,” IEEE Trans. Syst., Man, Cybern. A, Syst., Humans, vol. 38, no. 4,
pp. 888–895, Jul. 2008.
Q. Ye and M. H. MacGregor, “Combining Petri nets and ns-2: A hybrid
method for analysis and simulation,” in Proc. 4th Annu. CNSR Conf.,
May 24–25, 2006, pp. 139–148.
J. Sowa, “Relating diagrams to logic,” in Proc. 1st ICCS, Conceptual
Graphs Knowl. Represent., Quebec City, QC, Canada, Aug. 4–7, 1993,
pp. 1–35.
A. Feller and R. Rucker, “Extending structured analysis modeling with
A.I.: An application to MRPII profiles and SFC data communications
requirements specifications,” in Optimization of Manufacturing Systems
Design. Amsterdam, The Netherlands: North Holland, 1990.
A. Feller and R. Rucker, “Meta-modeling systems analysis primitives,” in
Conceptual Structures: Current Research and Practice. New York: Ellis
Horwood, 1992, ch. 10, pp. 201–220.
G. Willumsen, “Executable conceptual models in information systems engineering,” Ph.D. dissertation, Norwegian Inst. Technol. Univ.
Trondheim, Trondheim, Norway, 1993.
B. Berthomieu, P.-O. Ribet, and F. Vernadat, “The tool
TINA—Construction of abstract state spaces for Petri nets and time
Petri nets,” Int. J. Prod. Res., vol. 42, no. 14, pp. 2741–2756, Jul. 2004.
J. D. Bloom, “Pipe—A platform independent Petri net editor,” M.S. thesis,
Imperial College, London, U.K., 2003.
ver. 2.0 Platform Independent Petri Net Editor (PIPE), The Home Page
of PIPE (Platform Independent Petri Net Editor). [Online]. Available:
http://pipe2.sourceforge.net
W. Zuberek, “Timed Petri nets—Definitions, properties and applications,”
Microelectron. Reliab., vol. 31, no. 4, pp. 627–644, 1991.
Intel Corporation, Intel RosettaNet BCRN Indicators—Transaction Summary Report, Chandler, AZ, 2006.

Andrew L. Feller received the B.S. and M.S. degrees in industrial engineering from Arizona State
University (ASU), Tempe, where he is currently
working toward the Ph.D. degree.
He is also working with American Express Technologies, Phoenix, AZ, leading modeling and simulation projects focused on improving information
technology capacity planning and service process
performance. During the past several years, he has
led consulting practices as a Vice President and the
Director at two Arizona firms, improving operational
efficiency and financial performance for clients such as Honeywell and MD
Helicopters. He coauthored two book chapters on advanced system analysis
methods and architectures. His research interests include modeling and simulation in manufacturing, supply chain, and service management.
Mr. Feller is a member of the Society of Manufacturing Engineers and the
Institute of Industrial Engineers. He is the recipient of the alumni of the year
and academic distinction from the ASU Corporate Leaders program.

Teresa Wu received the Ph.D. degree in industrial
engineering from The University of Iowa, Iowa City,
in 2001.
She is currently an Associate Professor of industrial engineering with the Department of Industrial
Engineering, Arizona State University, Tempe. Her
research interests include distributed decision support, distributed information systems, supply chain
modeling, and disruption management. She has over
25 articles published in journals, such as the International Journal of Production Research, Omega, Data
and Knowledge Engineering, and American Society of Mechanical Engineer:
Journal of Computing and Information Science in Engineering, and the IEEE
T RANSACTIONS ON E NGINEERING M ANAGEMENT. She serves on the Editorial Review Board for the International Journal of Production Research, IEEE
T RANSACTIONS ON E NGINEERING M ANAGEMENT, Computer Standards and
Interfaces, and the International Journal of Electronic Business Management.

Dan L. Shunk received the Ph.D. degree in industrial engineering from Purdue University, West
Lafayette, IN, in 1976.
He is currently an Avnet Professor of supply network integration in industrial engineering with the
Department of Industrial Engineering, Arizona State
University, Tempe, where he is pursuing research
into collaborative commerce, global new product
development, model-based enterprises, and global
supply network integration.
Dr. Shunk won a Fulbright Award in 2002–2003,
the 1996 Society of Manufacturing Engineers (SME) International Award for
Education, the 1991 and 1999 I&MSE Faculty of the Year award, the 1989
SME Region VII Educator of the Year award, and the 1982 SME Outstanding
Young Engineer award. He was the Chair of AutoFact in 1985.

John Fowler received the Ph.D. degree in industrial
engineering from Texas A&M University, College
Station, in 1990.
He is currently a Professor of industrial engineering with the Department of Industrial Engineering,
Arizona State University, Tempe, and he was the
Center Director for the Factory Operations Research
Center that was jointly funded by International SEMATECH and the Semiconductor Research Corporation. His research interests include modeling,
analysis, and control of semiconductor manufacturing systems. He is an Area Editor for SIMULATION: Transactions of the
Society for Modeling and Simulation International (SCS).
Dr. Fowler is a member of the Institute of Industrial Engineers (IIE), Institute
for Operations Research and the Management Sciences (INFORMS), and SCS.
He is an IIE Fellow, the Vice President of Chapters/Fora for INFORMS, and
the Treasurer of Omega Rho and is on the Winter Simulation Conference
Board of Directors. He is an Associate Editor of the IEEE T RANSACTIONS
ON S EMICONDUCTOR M ANUFACTURING .

676

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 60, NO. 4, NOVEMBER 2013

Supply Chain Risk Management: An Agent-Based
Simulation to Study the Impact of Retail Stockouts
Teresa Wu, Simin Huang, Jennifer Blackhurst, Xiaoling Zhang, and Shanshan Wang

Abstract—In today’s highly dynamic and competitive environment, companies are under the pressure to improve their supply
chain strategy in order to be more responsive to customer demand.
However, supply chains are susceptible to unanticipated events that
make responsive supply chain management a challenging task. One
such disruption event is a product stockout at the retailer. In this
paper, we investigate retail stockouts through the development of
an agent-based simulation model in order to develop a better understanding of the effect of different stockout lengths for different
products (with different consumer response profiles to stockouts)
on both the retailer and the manufacturer of the product. We consider the change of market share as a measure of resilience for
both the manufacturer and the retailer to examine the impact of
the stockout. Insights are developed for manufacturers and retailers in responding to the stockout disruption.
Index Terms—Agent-based simulation, consumer response,
market share, retail stockout, supply network disruption
management.

I. INTRODUCTION
FFECTIVE supply chain management is a critical component of a firm’s ability to fill consumer demand. Supply
chains are challenging to manage for a variety of factors including their inherent complexity and dynamic nature. Adding
to this complexity is the fact that supply chains are susceptible to unanticipated events that interrupt the flow of materials
and information among suppliers, manufacturers, retailers, and
customers within a supply chain [7]. These disruptions may result from natural disasters, labor disputes, supplier bankruptcy,
or retail stockouts, just to name a few [6]. Recently, supply
chain risk management has become an area of focus for both
researchers and practitioners alike. Research has shown that
supply chain disruptions can have a negative impact onto the

E

Manuscript received March 10, 2010; revised December 10, 2010, June 10,
2011, November 14, 2011, and February 23, 2012; accepted February 29, 2012.
Date of publication May 1, 2012; date of current version October 16, 2013. This
work was supported in part by the National Science Foundation of China under
Grant 71071084. Review of this manuscript was arranged by Department Editor
S. (Sri) Talluri.
T. Wu is with the Industrial Engineering, Arizona State University, Tempe,
AZ 85281 USA (e-mail: teresa.wu@asu.edu).
S. Huang is with the Department of Industrial Engineering, Tsinghua University, Beijing 100084, China (e-mail: huangsimin@mail.tsinghua.edu.cn).
J. Blackhurst is with the Supply Chain and Information Systems Department, College of Business, Iowa State University, Iowa 50011 USA (e-mail:
jvblackh@iastate.edu).
X. Zhang is with the Harbin Institute of Technology, Harbin, Heilongjiang
150001, China (e-mail: xiaoling.zhang1221@gmail.com).
S. Wang is with the GE Global Research Center, Niskayuna, NY 12309 USA
(e-mail: wshanshan@asu.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TEM.2012.2190986

firm affecting supply chain operations and financial performance [4], [6], [14], [15]. For example, Hendricks and Singhal [14] show that publically announced glitches to the supply
chain can decrease shareholder value by over 10%. Loss of
market share has also been discussed as a measure of disruption
impact in work by Sheffi and Rice [38]. Therefore, we adapt using the market share level (both before and after a stockout) as a
measure of resilience or the ability to respond and recover from
a stockout disruption. Our paper builds upon past research, such
as [35], to develop insights into creating a consumer-responsive
supply chain and to call in the literature such as [15] in which
the need for research to improve the responsiveness of supply
chains is discussed (in the context of facing disruptions). While
there have been quantitative studies conducted to investigate the
supply chain risk, much of this study has focused more on the
early stages of disruptions, such as how to respond to disruption/uncertainties caused by suppliers, manufacturing systems,
and/or consumer demand and measuring performance through
inventory levels, lead time, and cost [2], [13]. Research is needed
to understand the factors that affect supply chain performance
after a stockout occurs.
In this paper, we seek to understand how stockout disruptions
impact a supply chain through the creation of an agent-based
simulation of a consumer goods supply chain. We focus not only
on the impact for the customer and the retailer, but also upstream
ramifications at the manufacturer. The upstream impact is of
importance when considering the impact of a stockout on the
supply chain as a whole. Kulp et al. [20] state that retail stockouts
may lead to unexpected orders with the manufacturer, leading
to stockouts at the manufacturer location. Additionally, recent
research has noted that stockouts may have severe impacts for
the entire supply chain by impacting or influencing consumer
purchasing behavior [8], [48], [49]. We view the supply chain
as an interconnected network where disruptions may propagate
throughout the network [3], [6]. In this paper, we seek to better
understand this impact from a supply chain level by not only
examining and quantifying the impact of stockout to the retailer
but to the manufacturer as well.
We develop an agent-based simulation model to investigate
stockouts in terms of consumers, retailers, and manufacturers.
Our study is framed in a supply chain of two manufacturers
each delivering its own brand of product to two retailers where
consumers may choose from two brands of the same product at
two different retailers. When one brand of product encounters
a stockout scenario, the agent-based simulation model analyzes
the impact of the consumer response. Based on the marketing
literature (in particular the work of Gruen and Corsten [11],
Gruen et al. [12], and Corsten and Gruen [10]), we investigate

0018-9391/$ © 2012 IEEE

WU et al.: SUPPLY CHAIN RISK MANAGEMENT: AN AGENT-BASED SIMULATION TO STUDY THE IMPACT OF RETAIL STOCKOUTS

the impact of different stockout lengths in terms of market share
impact for both the retailer and the manufacturer for a number
of different product types, each with unique consumer response
profiles. Motes and Castleberry [28] also note that consumer reaction to a stockout is product specific in nature. These consumer
response profiles map typical responses based on empirical research for consumers when facing a stockout of particular type
of product.
This has implications for how firms manage products through
the supply chain: different products may require different riskmitigation strategies and different locations in the supply chain
may feel a difference in the impact to a chosen mitigation strategy. We contend that understanding the impact of a consumer
response in the face of a stockout disruption at both the manufacturer and the retailer is an important step toward developing
better methods for supply chain risk management and resilience.
This may be used to develop more effective supply chain management strategies at both the manufacturer level and the retailer
level for products facing stockout disruptions.
The remainder of this paper is organized as follows.
Section II reviews literature related to stockout disruptions and
agent-based simulation models. Section III presents the agentbased simulation framework developed for this research. Section
IV presents a series of experiments along with a discussion of
the results and managerial insights. Finally, Section V presents
conclusions and future work.
II. LITERATURE REVIEW
In this section, we review the literature on stockout disruptions and supply chain risk as a foundation for the agent-based
simulation model. Research in the supply chain risk management area has been varied to include a wide array of topics from
risk management strategies in global supply chains [24] to analysis of supply chain disruption on stock price [15]. Tang [42]
provides an excellent overview of supply chain risk research
based on broad categories of supply, demand, product, and
information management as well as risk-mitigation strategies.
Supply chain risk research uses a variety of methods and tools
including empirical studies such as case-based research [7], and
mathematical and simulation models [9], [43], [45]. Research
can be general in nature, such as developing a supply chain risk
management framework [36] or quite specific such as looking
at procurement risk management at Hewlett-Packard [30]. In
this paper, we focus on stockouts which can create “a ripple
effect by distorting demand and leading to inaccurate forecasts.
Retailer costs also include the time employees spend trying to
satisfy shoppers who ask about a specific item. For a typical
U.S. grocery store, such cost amounts to $800 per week. The
corollary for shoppers is the amount of time spent waiting for
resolution that could be spent more productively for the retailer
in shopping—an estimated 20% of the average time for a shopping trip” [11].
Recent research also illustrates the magnitude of stockouts.
Musalam et al. [29] state that the average out of stock rate
in the U.S. and Europe is 8%. Jing and Lewis [17] study the
grocery industry and find that 8% of products in a supermarket

677

are out of stock at any moment in time and the percentage
rises to 15% for promotional items. Clearly, stockouts have
an impact on retailers, manufacturers, and consumers. In other
words, stockouts have an impact on the entire supply chain.
In this paper, we develop an agent-based simulation model
where each consumer is represented by a consumer agent, each
manufacturer is represented by a product agent, and each retailer
is represented by a store agent. Thus, the stockout impact on
both the retailer (store) and the manufacturer (product) can be
explored. We measure impact to the retailer in terms of the
change of store market share and we measure impact to the
manufacturer in terms of the change of product market share.
To our knowledge, this is the first agent-based simulation study
of supply chain risk (stockouts) which explores multiple factors
associated with stockout scenarios as well as looking at the
impact of retail stockouts both up and down the supply chain.
In doing so, we hope to establish a baseline for future research
in this area.
A. Market Share
Research has discussed the concept of market share in the
face of supply chain disruptions. For example, Sheffi and Rice
[38] develop a disruption profile which measures resilience of a
supply chain to a disruption in terms of loss of market share. Min
and Zhou [26] discuss how a supply chain may increase its value
by increasing market share. Pettit [32] and Pettit et al. [33] have
presented the concept of market position and market share in a
supply chain disruption context. The concept of market share
has been used in the marketing literature related to stockouts.
See, for example, [28] and [37]. In addition, we investigate
different market positions (in terms of initial market share) as a
method to measure resilience or the capability to respond and
recover from a stockout disruption where initial market share
is measured before the stockout and resulting market share is
measured after the stockout occurrence. In other words, does
initial market share impact the magnitude of loss in the event of
a stockout?
Companies may also become more opportunistic and develop
ways to exploit vulnerabilities of a competitor in order to gain
market share during a stockout situation for a product and even
for a store. For example, when Johnson & Johnson pulled products (Tylenol, Motrin, and Benadryl) off the shelves due to
quality issues in the manufacturing process, retailers such as
Walgreen’s and CVS used this opportunity to “get their private
label brands front and center with the consumer and grab market
share from Johnson & Johnson” [18]. This has been discussed
in the literature as well. Gruen and Corsten [11] discuss how
stockouts may encourage consumer to not only try new brands
(impacting the manufacturer) but also try new stores (impacting
the retailer). Given the increasing trends of online shopping,
changing stores is just one click of the mouse which is expected
to occur often.
B. Stockout Duration
Researchers have discussed how negative consequences of
disruption amplify the longer it lasts [14]. Tomlin [45] states

678

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 60, NO. 4, NOVEMBER 2013

that “extra capacity is of little use if a disruption is nearly over
by the time capacity is available.” While the temporal nature of
the disruption is discussed in the literature, we do not know the
significance of the duration length and if the significance varies
for different products in terms of market share changes. Motes
and Castleberry [28] studied aspect of stockout durations that
were short in nature but call for more in depth research regarding
stockouts of longer duration. More recently, Gruen et al. [12]
note that data on stockout duration are sparse in the literature.
We utilize the research of Gruen and Corsten [11] and Gruen
et al. [12] to use empirically validated distribution patterns for
each customer response to a stockout for a variety of product
types in our agent-based simulation model.
III. METHOD: MODELS AND SIMULATION FOR
SUPPLY CHAIN DISRUPTIONS
Work in a variety of supply chain disruption topic areas using a range of tools and methods has provided insights into
the emerging area of supply chain risk management. For example, mathematical models may provide optimal solutions to a
tightly defined problem while simulation may provide insight
into behaviors and metrics of supply chains experiencing risk.
In reviewing research to date in the area of supply chain risk
management, there is certainly more research on the supply
side, where risk is stemming from the supply base. Less has
been done on the demand side (consumer side) which presents
an opportunity to contribute to the body of knowledge in supply
chain risk management.
Exemplary research on the demand side includes the following: Weiss and Rosenthal [50] develop an optimal inventory
policy for economic order quantity (EOQ) inventory systems
which may have a disruption in either supply or demand. Qi et
al. [34] investigate a one supplier–one retailer supply chain that
experiences a disruption in demand after the production plan has
been made. Nagurney et al. [31] discuss supply chain network
equilibrium models including electronic commerce and multiple
tiers of decision makers. Xiao et al. [51] discuss coordination of
supply chain after demand disruptions when retailers compete.
Most demand side risk research to date may be classified into
the area of optimization models with supply chains of limited
size. In addition, many models make significant assumptions
and simplifications which can cause the models to “fall short of
current business needs” [19]. Therefore, more comprehensive
and flexible models are needed [3], [19].
As an alternative to mathematical optimization methods, simulation may be an effective tool for modeling complex interdependences, evaluating alternative designs and policies, and
analyzing performance tradeoffs for supply chain systems [39],
[42], [46]. Simulation models have been used in recent research
to better understand aspects of supply chain risk. Examples include analyzing sourcing risk in emerging markets [5] and risk
in supply chains stemming from natural disasters [25]. While
simulation models are of value, basic simulation models may
be enhanced to use autonomous agents allowing the simulation model to offer new and distinct advantages. Agent-based
simulation offers the ability to allow individual agents to make

autonomous decisions and the agents may adapt, change, and
learn over time—much like real systems. Moreover, agents may
have relationships with other agents and may form organizations [23].
A. Agent-Based Simulation
Agent-based simulation is a special case of simulation for
modeling complex systems which use autonomous interacting
agents. It has advantages over other modeling methods (including basic simulation models) in its ability to model very complex
and dynamic systems. Through the modeling of the actions and
interactions of autonomous agents, the emerging behavior of
a complex system can be revealed. Klibi et al. [19] call for
models to strike a balance between realism and tractability—
agent-based simulation moves us in that direction. For example,
Macal and North [22] present an application in the electric power
industry where the system experiences regular breakdowns and
agent-based simulation proves to be a valuable tool to model
the realistic scenarios and conduct meaningful analysis. Agentbased simulation has been applied to a wide variety of areas including manufacturing, vehicle routing, forest wild fires, energy
systems, economics, and biology, just to name a few [22], [40].
More recently, agent-based simulation has been applied in a
supply chain context. One of the first applications of agentbased simulation is the work by Swaminathan et al. [40], where
agent-based simulation was demonstrated to be an effective tool
to model supply chain dynamics as well as incorporating aspects of uncertainty related to supply, demand, and processes,
and the inclusion of decision-making procedures. More recent
examples of agent-based approaches include [52] where different options for planning, scheduling, and configuration are
investigated. Allwood and Lee [1] use agents to select between
competing vendors, manage inventory, and determine price.
Application of agent-based simulation is promising in a
supply chain context because the agents (entities in the supply chain) may interact, negotiate, and coordinate with each
other [44]. Another advantage of using agent-based simulation
is granularity: such as the ability to model the consumer at the
individual level. For a review of agent-based systems in manufacturing and supply chain management, we refer the reader
to [21]. Given the ultimate goal of managing a resilient supply
network is to attract and retain consumers during and after disruption events, there is a need to model consumers’ responses
to stockout and how this impacts supply chain performance
(including both manufacturers and retailers). Companies which
have prepared for disruptions in the supply chain may be opportunistic and use disruptions as an opportunity to strengthen
relationships with consumers [38] as discussed earlier in the
Johnson & Johnson scenario [18].
IV. AGENT-BASED SIMULATION FRAMEWORK
In order to better understand these issues, we develop an
agent-based simulation model to investigate stockouts in terms
of consumers, retailers, and manufacturers. The use of agents
within the simulation allows us to model behavior at the individual level. In doing so, a market consisting of a number

WU et al.: SUPPLY CHAIN RISK MANAGEMENT: AN AGENT-BASED SIMULATION TO STUDY THE IMPACT OF RETAIL STOCKOUTS

679

B. Store Agents

Fig. 1.

Agent-based simulation for a supply chain.

of consumers with different characteristics can be studied. By
using agent-based simulation, we are able to study a wide range
of factors and may develop models to represent large and complex supply chains. In this section, we present the agent-based
framework to the impact of stockouts of a product at a retail
store (see Fig. 1). Three types of autonomous agents are introduced: consumer agents, store agents, and product agents. For
simplification purposes, let us assume that there are two competing manufacturers (A versus B) and two competing retailers
(Store 1 versus Store 2). Manufacturer A produces product A
and ships it to Store 1 and Store 2. A’s competitor, Manufacturer
B produces product B and ships it to Store 1 and Store 2. Manufacturer A and Manufacturer B are competing for market share
for products they produce and Store 1 and Store 2 are competing
for store market share.
A. Consumer Agents
Early stockout research, such as [37], classified customers
into three categories: brand-loyal (customers loyal to a specific
brand), habitual buyers (customers with a weaker bond with the
product simply buying out of habit), and not loyal to any brand
at all. Later, the actions or decisions of these customer types
are discussed in the context of a stockout. For example, Zinn
and Lui [53] coin the term “SDL” which covers three customer
actions or decisions customer makes when facing a stockout:
substitute, delay, or leave the store. More recently, van Woensel
et al. [47], Gruen and Corsten [11], and Gruen et al. [12] expand
the details of the decisions to include more options and details
related to decisions customers make: 1) purchase at another
store; 2) delay the purchase (and come back at a later time to
repurchase); (3) substitute with the same brand; 4) substitute
with a different brand; and 5) do not purchase.
In addition to the five types of customer responses, each consumer has an initial set of preferences for the product and for the
store (where a proportion of customers prefer one brand over
the other and a proportion of customers prefer one store over
another). Consideration of these preferences will more realistically represent customer purchasing behavior. The preference
setting is in conjunction with the store’s and manufacturer’ position in the market (initial market share) which is discussed in
more details in Section V. For example, if Store 1’s market share
is 55%, we assume that 55% of the customers have an initial
preference to shop at Store 1.

Store agents are created to represent the retailers. These
agents are responsible to record the inventory of each type of
product and the number of consumer agents interacting with
the store for the specific purchase (e.g., purchasing product A
versus B). Store agents are also responsible to note the stockout and record the stockout status for a period of time. Since
there will be other stores in the market, especially for the online
shopping scenarios which may be a future research area, here
we assume that the total market for Store 1 and Store 2 is 70%
leaving 30% share to the other types of stores for more realistic
representation. Thus, we develop a number of experiments on
different initial store market share composition with total being
70% (refer to the detailed experiment setting in Section V).
C. Product Agents
Product agents are created to represent the manufacturers
in the supply chain. We assume that each manufacturer will
have a number of products available to the retailers and, thus,
consumers. We also assume for some products that there are
substitutable (e.g., different sizes) products available. An example of this is different size bottle of the same shampoo. Since
the model developed in this study is based on the data from
the recent retail stockout literature by Gruen and Corsten [11]
which focuses on Procter & Gamble (P&G), we assume that the
manufacturer composition is fixed, that is, 55% (such as P&G)
versus 45% (such as P&G competitors). As a result, we use
absolute market share measures as introduced by Szymanski
et al. [41] which is defined later in this paper. Certainly, future
research may investigate different manufacturing compositions
as well as additional manufacturers competing for customers.
D. Decision Heuristics of Agents
The choice of agent-based simulation allows learning and
evolution in the simulation. Each agent has a memory and will
evolve and learn over time as it interacts with the other agents
in the simulation. The basic decision heuristics of consumer
agents, store agents, and product agents applied in this research
are summarized in Table I.
Note that more advanced rules may be incorporated into the
framework in future research. For example, for store agents,
promotion strategies in responding to stockout can be included.
For product agents, demand forecast for production planning
can be explored. For consumer agents, “word of mouth impact”
can be studied. However, since the objective of this research is to
develop an agent-based framework to explore the impacts of different disruption lengths and different consumer responses for
a variety of product types for both retailers and manufacturers,
we plan to explore these extensions in future work.
V. EXPERIMENTATION AND DISCUSSION
The agent-based simulation was developed using the software NetLogo, a multi-agent programmable modeling environment (http://ccl.northwestern.edu/netlogo/, freeware supported
by the Center for Connected Learning and Computer-Based

680

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 60, NO. 4, NOVEMBER 2013

TABLE I
AGENT DECISION RULES

Modeling at Northwestern University). With agent-based simulation framework developed, the research questions we investigate in this paper are as follows.
1) Product type and consumer response: Understanding that
consumers having different responses to different product stockouts, we seek to understand how the consumer
response will impact the market share of retailer and manufacturer. Specifically, will a variety of products with
different consumer response profiles impact the market
share (product and store) differently? We investigate this
question first at an aggregate level and then for specific
products.
2) Initial market share: We seek to develop a richer understanding of how the initial market share might be a significant factor impacting the changes in market share of the
manufacturer and the retailer under a stockout disruption.
3) Stockout duration: Finally, we seek a better understanding
of the significance stockout duration on the market share
of manufacturer and retailer.
We design a series of experiments to study the impact of
three factors: consumer response, initial store market share, and
stockout duration. Based on the Gruen and Corsten [11] and
Gruen et al. [12] studies, stockout duration usually can be categorized into four categories: 1) less than 8 h; 2) 8 h to 1 day; 3)
1 day to 3 days; and 4) more than 3 days.
In the experiments, we set four levels of stockout with the
mean of each stockout duration being 4, 16, 28, and 42 h. For
store composition, seven levels are introduced with one store
share increased from 10%, 20%, 30%, 35%, 40%, 50% to 60%;
and the other store share decreased from 60% to 10%. We run 10
simulation hours before the stockout and an additional 10 simulation hours after the stockout to track the resilience (recovery
of market share). Customer responses for different products are
collected from the Gruen and Corsten [11] and Gruen et al. [12]
studies which report stockout for five consumer goods products:
cosmetics, shampoo, coffee, paper towels, and salted snacks.

A. Simulation Experiment I: Customer Response
In the first set of experiments, we explore how different consumer responses (five in total: purchase at another store c1 , delay
the purchase c2 , substitute with the same brand c3 , substitute
with a different brand c4 , and do not purchase c5 ) impact the
store and product market share under different stockout duration
lengths and initial store market share.
1) Experimental Design and Results: Since the focus of
this experiment set is on customer responses, we exhaustively explore all the combinations of customer responses with
c1 +c2 +c3 +c4 +c5 = 1. We choose two out of four levels for
stockout duration (16 and 28 h), and three out of seven levels for
initial store market share (10% versus 60%, 35% versus 35%,
and 60% versus 10%) for the experiment design. Regarding
the customer response, we follow the standard simplex-lattice
design procedure [27]. A {p, m} simplex-lattice design for p
components consists of points defined by the following coordinate settings: the proportions assumed by each component
take m+1 equally spaced values from 0 to 1, xi = 0, 1/m,
2/m, . . . , 1 for i = 1, 2, . . . , p, (x1 + x2 + · · · + xp = 1),
and all possible combinations of the proportions are used in
the experiment. Since all the possible combinations of the proportions are used, the experimental region is a regular (p−1)
dimensional simplex. Therefore, issues of multicollinearity do
not exist [16], [27]. In this research, we have five consumer
responses (ci , i = 5), so p = 5. Each response (ci ) equally
has 1/5 as increment (decrement) changing from 0 to 1, so m
= 5. Therefore, ci = 0, 1/5, 2/5, 3/5, 4/5, 1 for i = 1, 2, . . .,
5 (c1 + c2 + c3 + c4 + c5 = 1). The simplex-lattice design
will consist of (p + m − 1)!/m!(p − 1)! = 126 experiments.
Thus, we have 756 (2×3×126) scenarios. For each scenario,
ten simulation replications are conducted to analyze the impact
on product market share changes (Diff_Product_Marketshare)
and store market share changes (Diff_Store_Marketshare). In
this paper, we use absolute market share which is the ratio of a
business’ sales to total sales in the served market [41]. In other
words, before the store (e.g., Store 1) experiences a stockout
of a product (e.g., Product A), the market share of the product
and market share of the store are measured. These same items
are measured after the stockout and the differences are defined
as “Diff_Product_Marketshare” and “Diff_Store_Marketshare.”
A larger measure of “Diff_Product_Markethshare” and/or
“Diff_Store_Marketshare” indicates a greater negative impact
from the stockout. The results are summarized in Table II.
2) Results Discussion: Based upon the results of this experiment, we develop a number of insights. First, the significant
factors impacting product market share were found to be stockout duration, initial store market share, and two of five types of
customers (including customers who tend to delay purchase and
customers who tend to substitute with a different brand). That
is, the longer the stockout duration, and/or the larger the initial
store market share, and/or the more the consumers tending to delay purchase or substitute with a different brand, the greater the
market share changes. In other words, market share of the stockout product will decrease after the disruption. Actions may be
taken, however, to counteract this. When there is the potential of

WU et al.: SUPPLY CHAIN RISK MANAGEMENT: AN AGENT-BASED SIMULATION TO STUDY THE IMPACT OF RETAIL STOCKOUTS

681

TABLE II
(a) PARAMETER ESTIMATES FOR DIFF_PRODUCT_MARKETSHARE. (b) PARAMETER ESTIMATES FOR DIFF_STORE_MARKETSHARE

a stockout known in advance (such as a production shortage), the
manufacturer should work closely with retailers for customer retention. It is not just simply the short-term dissatisfaction of the
retailer the manufacturer must face, but rather a longer term and
potentially more detrimental impact—customers moving away
from the brand. This expands on previous research showing that
retailer and manufacturer stockouts are shown to be significantly
and positively associated with each other [19], [20]. In addition,
we recommend that based upon the results of this set of experiments the manufacturer should partner with the retailer to focus
on the specific customer groups when stockout occurs. Interestingly, among the two groups of significant customer responses,
those customers making the substitution decision (to substitute
with different brands) are of a much higher magnitude [0.1077:
see Table II(a)] of impact than the customers who chose to delay
the purchase [0.0122: see Table II(a)]. This is echoed in research
looking specifically at online retailing [16] where it is recommended to prioritize inventory by customer traits. Our research
quantifies which type of customer is most impactful.
Next, we find it interesting that for the store market share we
observe a different set of significant factors impacting the resilience of the retailer: stockout duration, customers who tend to
purchase at another store, and customers who delay purchase,
but not the initial store market share. The decision of a customer to go to another store is found to have a much higher
impact [0.1189—see Table II(b)] than the customers delaying
the purchase [0.0132—see Table II(b)].
Finally, we observe that the type of customers who make the
decision to not purchase has no impact for both product and
store market share. We speculate that the reason for this centers
on how we measure the impact of the stockout in terms of market
share change. Since we use absolute market share measure (the
ratio of a business’ sales to total sales in the served market [41]),
the customers who do not purchase have no impact on both the
business’ sale and the served market.
In summary, not all types of customers will have the same
impact on the market share (both product and store). Customers
who tend to delay purchase will negatively impact both product and store market share though the magnitude is moderately
less than customers tending to substitute the brands (for product
market share) and tending to visit another store (for store market

share). Note that the major difference between customers who
tend to delay purchase and customers who do not purchase is
that the former one will come back to the market with the intention to make the purchase decision again. It is possible that the
customers will choose to substitute or go to another store if a
stockout continues. They also might choose to simply not purchase the item or to delay the purchase again. However, if they
are intent of getting the product, they will likely choose a substitute or go to another store. We recommend that retailers and
manufacturers focus efforts at the supply chain level to tap into
benefits resulting from joint efforts when possible to develop
strong customer relationship programs. One such opportunity is
to specifically target the customers who choose to delay the purchase by focusing on incenting the customer substitute within
the same brand as a win–win strategy for both manufacturer
and retailer. Therefore, both manufacturer and retailer should
work closely to ensure that substitutes are available (for example, the same brand of shampoo but in a different size bottle). In
other words, rather than focusing on a single product, bundles
of products from the same brand should be investigated.
B. Simulation Experiment II: Product Type
While Experiment I focuses on customer response in general,
this is not the entire story. Different product groups will result in
different responses to a stockout from the customer. Gruen and
Corsten [11] and Gruen et al. [12] provide details for five types
of product groups with each having different customer response
composition. These are shown in Table III. Therefore, we develop another set of experiments to investigate if the stockout
duration and initial store market share will significantly impact
the market shares (both product and store) of specific product
types.
1) Experimental Design and Results: This set of experiments is designed to have four levels for stockout duration (4,
16, 28, and 42 h) and seven levels for initial store market share
with store 1 versus store 2 being 10% versus 60%, 20% versus 50%, 30% versus 40%, 35% versus 35%, 40% versus 30%,
50% versus 20%, and 60% versus 10%, respectively. Five product types are studied: cosmetics, shampoo, coffee, paper towels,
and salted snacks (customer responses are adopted from [11]

682

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 60, NO. 4, NOVEMBER 2013

TABLE III
CONSUMER RESPONSES TO DIFFERENT PRODUCT STOCKOUTS (ADOPTED FROM [11] AND [12])

TABLE IV
(a) PARAMETER ESTIMATES FOR PRODUCT-BASED DIFF_PRODUCT_MARKETSHARE. (b) PARAMETER ESTIMATES
FOR PRODUCT-BASED DIFF_STORE_MARKETSHARE

and [12] and summarized in Table III). Thus, for each product,
there are 28 (4×7) scenarios. We again conduct ten simulation
replications for each scenario. Results are shown in Table IV.
2) Results Discussion: As with Experiment I, based upon the
results of Experiment II, we develop a number of insights. We
observe from Table IV(a) that for the five product groups, both
stockout duration and initial store market share are significant
factors impacting resilience or the change of product market
share. However, the magnitude of impact varies. In the five
product groups, only cosmetic’s market share is more affected
by the stockout duration than the initial store market share. This
could be explained by customer’s purchasing behavior as we
speculate that cosmetic customers have strong preferences for
specific brands. If the product the customer is looking for is
out of stock, one can expect that the customer will shop around
and purchase the brand at another store. Therefore, the retailer
is more affected with this type of product stockout. In contrast,
products including shampoo, coffee, paper towels, and salted
snacks are more impacted by the size of initial market share
of the retail store. Paper towels are impacted the most. This
may be explained by customers tending to purchase the paper
towels at a preferred store. In other words, they have a stronger
preference to the store than to the brand. If a stockout occurs to
paper towels, the customer will simply choose another brand.
Therefore, the manufacturer is more affected by these types

of product stockout. The same rationale applies to shampoo,
coffee, and salted snack with a moderately smaller degree of
impact compared to paper towels.
Looking at Table IV(b), it appears that both stockout duration and store market position are significant factors impacting
the store market share across all five products. However, we do
observe differences in terms of impact on the market share for
product and store among the five products. The magnitude of
the stockout duration impacting the store for all five products is
at the same scale. However, the larger the initial store market
share, the larger the impact on the store position ranging in order
from most severe with cosmetics, then shampoo, coffee, paper
towels, and, finally, salted snacks. To expand on this point and
offer insights, we discuss options when initial market share is
high versus low. When initial store market share is high, the
impact on both manufacturer and retailer is high. This suggests
opportunities for the manufacturer to work more closely with
larger retailers. For example, it may be worth the effort for the
manufacturer to invest in collaborative forecasting and inventory replenishment. Conversely, when the initial store market
share is low, the retailer may see an opportunity for growth
in market share (gaining position relative to competitors) and
should first invest in location-specific options to improve shelf
replacement in addition to investigating marketing incentives
before investing in collaborative initiatives.

WU et al.: SUPPLY CHAIN RISK MANAGEMENT: AN AGENT-BASED SIMULATION TO STUDY THE IMPACT OF RETAIL STOCKOUTS

Experiment II also suggests that there may exist a distinction between cosmetics and the other four products which are
not shown in the data collected from [11] and [12]. In fact, for
cosmetics, the impact of the stockout duration is more significant than initial market share. Therefore, we not only suggest
that both large and small retailers alike focus on product with
high levels of brand loyalty (such as cosmetics), but we also
recommend that the manufacturer explores customer segmentation opportunities and develops business strategies for different
group of products. Efforts may be made to understand which
types of products the manufacturer may be able to influence
customers to develop strong bond or preferences either through
incentives. We suggest that these activities span to include product development and innovation so the customers are strongly
tied to the product brand (e.g., iPhone from Apple).
As for retailer, such distinction is not obvious which suggests the equal importance of all five products. This calls for
more in-depth studies when data on more products are available. However, one avenue that might be explored is looking
at focusing on the types of products that would cause the customer to leave the store. For example, as discussed earlier if a
customer is more strongly bonded to certain types of products
(such as cosmetics) over other types of products (such as paper
towels), efforts should focus on those types of products that will
keep a customer in the store versus causing them to leave if the
product is out of stock. Therefore, understanding the products
that customers most strongly bond to may be key in retaining
customers.

683

Fig. 2. (a) Implementing stockout reduction strategies (manufacturer).
(b) Implementing stockout reduction strategies (retailer).

C. Impact of Reducing Stockout Duration
In the previous set of experiments, it was shown that the duration of the stockout is significant for all five products. While
it is beneficial to model the impact to confirm the significance,
the implication that stockout duration is significant is not surprising. In this section, we will conduct a sensitivity analysis to
investigate the impact of reducing the stockout duration to see
if a reduction of stockout duration will impact all products on
the same scale.
Let us assume from the experiments that the retailer and
the manufacturer are capable to learn and adapt to the impact
of stockouts from different type of products and attempt to
reduce the stockout duration from 14 to 2 h using a stockout
disruption mitigation strategy such as higher levels of safety
stock or better inventory tracking systems. Fig. 2(a) and (b)
illustrates the market share reduction from the perspective of
both the manufacturer and the store.
We observe that there is a wide array of market share changes
across the five products when reducing stockout time from 28 to
14 h. In addition, there are differences between the manufacturer
and the retailer. Again, we conclude that this has implications
on where the manufacturer and retailer should invest time and
resources (as discussed in [11] and [12]).
There has been recent research discussing methods to reduce
stockout duration such as data accuracy, forecast improvement,
and RFID implementation just to name a few [11], [12]. However, our research comes with a caution to both manufacturer

Fig. 3.

Implementing stockout reduction strategies (retailer).

and retailers that not all products are the same and there are
different results in terms of impact at different locations of the
supply chain. The overall change in market share responding to
reduction of the stockout duration from 28 h to 14 h is illustrated
in Fig. 3.
Different products have different impacts, meaning that the
stockout of one product will not result in the same losses as a
different product. To compound this, the stockout of a product
may have very different impacts when comparing the manufacturer and the retailer. Fig. 3 illustrates that reducing stockout
duration from 28 to 14 h will have a drastically different effect
on the manufacturer and retailer for salted snacks. For the manufacturer, there is a great difference (almost 3% of market share)

684

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 60, NO. 4, NOVEMBER 2013

while for the retailer the difference is quite small (0.11%). This
suggests a critical stockout duration threshold in between 28 and
14 h for the manufacture but not for the retailer. This may also
have implication on how manufacturers and retailers partner
with each other for promotions and sales. For example, for the
stockout products that the manufacturer and the retailer suffer
different same scale of loss (e.g., salted snack), different partner
strategies should be implemented by the two parties. Conversely,
shampoo shows a very similar result for both the manufacturer
and the retailer. Therefore, stockout mitigation strategies need
to be carefully thought out. Moreover, there seems to be a great
opportunity at the supply chain level to have manufactures and
retailers partner in a more effective manner to reduce stockouts.
If we look at the results on a product-by-product basis at different
ends of the spectrum, we can also yield some additional insights.
1) For salted snacks, the difference between the manufacturer
and the retailer is the most pronounced. The impact is
much more serious to the manufacturer than that to the
retailer. We speculate this is due to the fact that the retailer
offers a large selection of product options (in addition,
the retailer may also have a private label brand). In this
case, the burden must fall on the manufacturer proactively
monitor supply, forecasting, and deliveries.
2) For cosmetics, the opposite is true in that the impact is
more severe to the retailer. As discussed earlier, here the
retailer needs to be more proactive and concentrate efforts to more closely monitor shelves to ensure product
availability. Again, not all products impact retailers and
manufacturers in the same manner.
3) For products like shampoo, the manufacturer and retailer
suffer similar fates. Therefore, with these types of products with similar market share changes, there is a stronger
incentive and justification for developing joint initiatives.
VI. CONCLUSION AND FUTURE WORK
We consider the impact of different stockout durations, different product types (with difference customer response profiles),
and initial market shares of the retailer on the end market share
of the product or brand (manufacturer) and on the end market
share of the store (retailer). We quantitatively demonstrate the
impact of stockouts (of varying lengths of product not being on
the store shelf) of different product types (five types: cosmetics,
shampoo, coffee, paper towels, and salted snacks) in a timeline
of before, during, and after a stockout. In doing so, we establish
a magnitude of resilience of the retailer and manufacturer in the
face of a stockout disruption.
In summary, we find that significant factors for product market share are stockout duration, initial store market share, and
two of five types of customers including customers who tend to
delay purchase and customers who tend to substitute with a different brand. We also find different significant factors for store
market share that are stockout duration and two of five types of
customers including customers who tend to visit another store
and who tend to delay the purchase. We discuss how the manufacturer and retailer may find mutual benefit in partnering to
1) address the targeted types of customers (those that affect

market share in significant manner); and 2) products that have
high levels of loyalty associated with them. In addition, stockout reduction strategies should be carefully considered. Not all
products and customers are impacted in the same manner and
the impact changes at different locations in the supply chain. In
addition, the results highlight critical thresholds. Think of the
example from Johnson & Johnson earlier in this paper where a
retailer may have an opportunity to take advantage of another
brand’s stockout to gain market share for their own brands. This
highlights the possibility or opportunity for both retailers and
manufacturer to take advantage of stockout scenarios to gain
market share.
Additionally, we discuss the need for understanding critical
threshold levels in terms of stockout duration. It is interesting to
note that these are different for the retailer and the manufacturer.
Finally, we wish to note that within this paper, we are not looking at the causes of retailer stockouts but rather the impact to
the retailer and manufacturer of these stockouts quantitatively.
Interestingly, Gruen et al. [12] note that stockouts may be attributed to the following causes: 28% resulting from upstream
causes (manufacturer), 25% stemming from the fact that the
product is in the store but, for some reason, not on the shelf, and
47% may be attributed to store ordering and forecasting. This
is also discussed in [29]. Retail store inventory systems may
not be able to distinguish between inventory on the shelf and
inventory in the back room which could certainly exacerbate
this problem. Additionally, there are issues with the accuracy
level of these systems. Certainly, investigating these causes in
more depth would be a fruitful future research topic.
This research provides a foundation for quantitatively studying supply chain stockouts through the development of an agentbased simulation. There are many opportunities to extend this
research by focusing on causes of stockouts and developing tailored mitigation strategies (such as price reductions and inventory policies) at both the manufacturer and the retailer as well
as joint activities between the two entities. In addition, other
factors should be investigated such as product market share,
day of the week or hour in the day, or location of the manufacturer and the retailer. Internet-based sales might also be an
interesting extension. In addition, this research was based on the
consumer goods products. Certainly, a wider range of product
types could be explored in the future digging deeper into issues
such as perishability and speed of consumption by the consumer.
Moreover, the nature of agent-based simulation allows for more
agents to be introduced giving the user the ability to model more
complex and realistic supply chains. Another interesting extension would be to exploit the nature of agent-based simulation
to explore learning across supply chain members (agents) to
investigate effectiveness of better information sharing, adaptive
mitigation strategies, or knowledge management across agents
to be how these complex systems will adapt over time. Finally,
more advanced rules may be incorporated in the future such as
consumer response the promotions or “word of mouth impact”
or other factors that may influence a shift in store or product
loyalty. Our hope is that this paper will provide the foundation
for a future stream of research for exploring the complex topic
of disruption stockout risk in a supply chain.

WU et al.: SUPPLY CHAIN RISK MANAGEMENT: AN AGENT-BASED SIMULATION TO STUDY THE IMPACT OF RETAIL STOCKOUTS

REFERENCES
[1] J. M. Allwood and J.-H. Lee, “The design of an agent for modeling supply
chain network dynamics,” Int. J. Prod. Res., vol. 43, no. 22, pp. 4875–
4898, 2005.
[2] B. M. Beamon, “Measuring supply chain performance,” Int. J. Oper.
Prod. Manage., vol. 19, no. 3–4, pp. 275–292, 1999.
[3] J. Blackhurst, C. Craighead, D. Elkins, and R. B. Handfield, “An empirically derived agenda for quantitative tools to analyze and reduce supply
chain disruption impacts,” Int. J. Prod. Res., vol. 43, no. 19, pp. 4067–
4081, 2005.
[4] J. Blackhurst, K. Scheibe, and D. Johnson, “Supplier risk assessment and
monitoring for the automotive industry,” Int. J. Phys. Distrib. Logistics
Manage., vol. 38, no. 2, pp. 143–165, 2008.
[5] Y. B. Canbolat, G. Gupta, S. Matera, and K. Chelst, “Analysing risk
in sourcing design and manufacture of components and sub-systems to
emerging markets,” Int. J. Prod. Res., vol. 46, no. 18, pp. 5145–5164,
2008.
[6] S. Chopra and M. Sodhi, “Managing risk to avoid supply-chain breakdown,” MIT Sloan Manage. Rev., vol. 46, no. 1, pp. 53–61, 2004.
[7] C. W. Craighead, J. Blackhurst, M. J. Rungtusanatham, and R. B. Handfield, “The severity of supply chain disruptions: Design characteristics
and mitigation capabilities,” Dec. Sci., vol. 38, no. 1, pp. 131–156,
2007.
[8] C. Eroglu, B. Williams, and M Waller, “Consumer-driven retail operations:
The moderating effects of consumer demand and case pack quantity,” Int.
J. Phys. Distrib. Logistics Manage., vol. 41, no. 5, 2011.
[9] M. Goh, J. Lim, and F. Meng, “A stochastic model for risk management in
global supply chain networks,” Eur. J. Oper. Res., vol. 182, pp. 164–173,
2007.
[10] T. Gruen and D. Corsten, “Stock-outs cause walkouts,” Harvard Business
Rev., vol. 82, no. 5, pp. 26–28, 2004.
[11] T. Gruen and D. Corsten, “A comprehensive guide to retail out-of-stock
reduction in the fast-moving consumer goods industry,” The Grocery Manufacturers Association (GMA), Food and Marketing Institute (FMI), National Association of Chain Drug Stores (NACDS), 2008. ISBN: 978-3905613-04-9.
[12] T. Gruen, D. Corsten, and S. Bharadwaj, “Retail out-of-stocks: A worldwide examination of extent, causes, and consumer responses,” The Grocery Manufacturers of America, The Food Marketing Institute and CIES—
The Food Business Forum, 2002.
[13] P. Gunasekaran and E. Tirtiroglu, “Performance measures and metrics in a
supply chain environment,” Int. J. Oper. Prod. Manage., vol. 21, no. 1–2,
pp. 71–87, 2001.
[14] K. Hendricks and V. Singhal, “The effect of supply chain glitches on
shareholder wealth,” J. Oper. Manage., vol. 21, no. 5, pp. 501–522,
2003.
[15] K. Hendricks and V. Singhal, “An empirical analysis on the effect of
supply chain disruptions on long-run stock price performance and equity risk of the firm,” Prod. Oper. Manage., vol. 14, no. 1, pp. 35–52,
2005.
[16] D. H. Jang and C. M. Anderon-Cook, “Fraction of design space plots for
evaluating ridge estimators in mixture experiments,” Quality Reliab. Eng.
Int., vol. 27, no. 1, pp. 27–34, 2011.
[17] X. Jing and M. Lewis, “Stockouts in online retailing,” J. Marketing Res.,
vol. XLVIII, pp. 342–354, 2011.
[18] P. Kavilanz. (2010, Aug. 6). “Tylenol’s side effect? A brand boycott,”
CNNMoney.com [Online]. Available: http://money.cnn.com/2010/08/
06/news/companies/Tylenol_recalls_opportunity_for_generics/index.htm
[19] W. Klibi, A. Martel, and A. Guitouni, “A design for robust value-creating
supply chain networks: A critical review,” Eur. J. Oper. Res., vol. 203,
pp. 283–293, 2010.
[20] S. Kulp, H. Lee, and E. Ofek, “Manufacturer benefits from information
integration with retail customers,” Manage. Sci., vol. 50, no. 4, pp. 431–
444, 2004.
[21] J.-H. Lee and C.-O. Kim, “Multi-agent systems applications in manufacturing and supply chain management: A review paper,” Int. J. Prod. Res.,
vol. 46, no. 1, pp. 233–265, 2008.
[22] C. Macal and M. North, “Tutorial on agent-based modeling and simulation,” in Proc. Winter Simulation Conf., 2005, pp. 2–15.
[23] C. Macal and M. North, “Tutorial on agent-based modeling and simulation:
Part 2. How to model with agents?” in Proc. Winter Simulation Conf., 2006,
pp. 73–83.
[24] I. Manuj and J. Mentzer, “Global supply chain risk management strategies,” Int. J. Phys. Distrib. Logistics Manage., vol. 38, no. 3, pp. 192–223,
2008.

685

[25] H. Miller and K. Engemann, “A Monte Carlo simulation model of supply
chain risk due to natural disasters,” Int. J. Technol., Policy Manage., vol. 8,
no. 4, pp. 460–480, 2008.
[26] H. Min and G. Zhou, “Supply chain modeling: Past, present and future,”
Comput. Ind. Eng., vol. 43, no. 1–2, pp. 231–249, 2002.
[27] D. C. Montgomery, Design and Analysis of Experiments, 6th ed. Hoboken, NJ: Wiley, 2007.
[28] W. Motes and S. Castleberry, “A longitudinal field test of stockout effects
on multi-brand inventories,” Acad. Marketing Sci. J., vol. 13, no. 4,
pp. 54–68, 1985.
[29] A. Musalem, M. Olivares, E. Bradlow, C. Terwiesch, and D. Corsten,
“Structural estimation of the effect of out-of-stocks,” Manage. Sci.,
vol. 56, no. 7, pp. 1180–1197, 2010.
[30] V. Nagali, J. Hwang, D. Sanghera, M. Gaskins, M. Pridgen, T. Thurston,
P. Mackenroth, D. Branvold, P. Scholler, and G. Shoemaker, “Procurement risk management (PRM) at Hewlett-Packard company,” Interfaces,
vol. 38, no. 1, pp. 51–60, 2008.
[31] A. Nagurney, J. Cruz, J. Dong, and D. Zhang, “Supply chain networks,
electronic commerce and supply side and demand side risk,” Eur. J. Oper.
Res., vol. 164, pp. 120–142, 2005.
[32] T. Pettit, “Supply chain resilience: development of a conceptual framework, an assessment tool and an implementation process,” Ph.D. dissertation, Ohio State Univ., 2009.
[33] T. Pettit, J. Fiskel, and K. Croxton, “Ensuring supply chain resilience:
Development of a conceptual framework,” J. Business Logistics, vol. 31,
pp. 1–21, 2010.
[34] X. Qi, J. Bard, and G. Yu, “Supply chain coordination with demand
disruptions,” Omega, vol. 32, pp. 301–312, 2004.
[35] A. Reichart and M. Holweg, “Creating the customer-responsive supply
chain: A reconciliation of concepts,” Int. J. Oper. Prod. Manage., vol. 27,
no. 11, pp. 1144–1172, 2007.
[36] B. Ritchie and C. Brindley, “Supply chain risk management and performance: A guiding framework for future development,” Int. J. Oper. Prod.
Manage., vol. 27, no. 3, pp. 303–322, 2007.
[37] P. Schary and B. W. Becker, “The impact of stock-out on market share:
Temporal effects,” J. Bus. Logistics, vol. 1, no. 1, pp. 31–46, 1978.
[38] Y. Sheffi and J. Rice, “A supply chain view of the resilient enterprise,”
MIT Sloan Manage. Rev., vol. 47, no. 1, pp. 41–48, 2005.
[39] J. D. Sterman, Business Dynamics: Systems Thinking and Modeling for a
Complex World. Boston, MA: Irwin/McGraw-Hill, 2000.
[40] J. Swaminathan, S. Smith, and N. Sadeh, “Modeling supply chain dynamics: a multiagent approach,” Dec. Sci., vol. 29, no. 3, pp. 607–632,
1998.
[41] D. M. Szymanski, S. G. Bharadwaj, and P. R. Varadarajann, “An analysis
of the market share-profitability relationship,” J. Marketing, vol. 57, no. 3,
pp. 1–18, 1993.
[42] C. Tang, “Perspectives in supply chain risk management,” Int. J. Prod.
Econ., vol. 103, pp. 451–488, 2006.
[43] S. Terzi and S. Cavalieri, “Simulation in the supply chain context: a
survey,” Comput. Ind., vol. 53, no. 1, pp. 3–16, 2004.
[44] J. Tian and H. Tianfield, “Literature review upon multi-agent supply chain
management,” in Proc. 5th Int. Conf. Mach. Learning Cybern., 2006,
pp. 89–94.
[45] B. Tomlin, “On the value of mitigation and contingency strategies for
managing supply chain disruption risks,” Manage. Sci., vol. 52, no. 5,
pp. 639–657, 2006.
[46] D. J. van der Zee and J. G. A. J. van der Vorst, “A modeling framework for
supply chain simulation: opportunities for improved decision making,”
Dec. Sci., vol. 36, no. 1, pp. 65–95, 2005.
[47] T. van Woensel, K. van Donselaar, R. Broekmeulen, and J. Fransoo, “Consumer responses to shelf out-of-stocks of perishable products,” Int. J.
Phys. Distrib. Logistics Manage., vol. 37, no. 9, pp. 704–718, 2007.
[48] M. Waller, A. Tangari, and B. Williams, “Case pack quantity’s effect on
retail market share: an examination of the backroom logistics effect and
the store-level fill rate effect,” Int. J. Phys. Distrib. Logistics Manage.,
vol. 38, no. 6, pp. 436–451, 2008.
[49] M. Waller, B. Williams, A. Tangari, and S. Burton, “Marketing at the
retail shelf: an examination of logistics in SKU market share,” J. Acad.
Marketing Sci., vol. 38, pp. 105–117, 2010.
[50] H. Weiss and E. Rosenthal, “Optimal ordering policies when anticipating a
disruption in supply or demand,” Eur. J. Oper. Res., vol. 59, pp. 370–382,
1992.
[51] T. Xiao, G. Yu, Z. Sheng, and Y. Xia, “Coordination of a supply chain
with one-manufacturer and two-retailers under demand promotion and
disruption management decisions,” Ann. Oper. Res., vol. 135, no. 1,
pp. 87–109, 2005.

686

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 60, NO. 4, NOVEMBER 2013

[52] D. Zhang, A. Anosike, M. Lim, and O. Akanle, “An agent-based approach
for e-manufacturing and supply chain integration,” Comput. Ind. Eng.,
vol. 51, pp. 343–360, 2006.
[53] W. Zinn and P. Liu, “Consumer response to retail stockouts,” J. Bus.
Logistics, vol. 22, no. 1, pp. 49–71, 2001.

Teresa Wu received the Ph.D. degree in industrial
engineering from the University of Iowa, Iowa City,
in 2001.
She is an Associate Professor of Industrial Engineering Program at School of Computing, Informatics, and Decision Systems Engineering, Arizona
State University, Tempe. Her current research interests include distributed decision support, distributed
information system, supply chain modeling, and disruption management. She has published articles in
journals such as IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, Information Science, and Journal of Operations
Management.
Dr. Wu serves on the Editorial Review Board for the IEEE TRANSACTIONS ON
ENGINEERING MANAGEMENT, the International Journal of Production Research,
the Journal of Medical and Health informatics, the Computer and Standard Interface, and the International Journal of Electronic Business Management.

Simin Huang received the Ph.D. degree in industrial
engineering from SUNY, Buffalo, NY, in 2004.
He is a Professor in the Department of Industrial
Engineering, Tsinghua University, Beijing, China.
His current research interests include supply chain
risk management, production scheduling, and network design. He has published papers in peer-review
journals including Annals of Operations Research,
IIE Transactions, International Journal of Production Research, Naval Research Logistics, Transportation Research Part B: Methodological, etc.
Dr. Huang has been serving as an Associate Editor of the IIE TRANSACTIONS
since 2005.

Jennifer Blackhurst received the Doctorate degree
in industrial engineering from the University of Iowa,
Iowa City, in 2002.
She is the Walker Professor in Logistics and Supply Chain Management and an Associate Professor
of Supply Chain Management at the College of Business, Iowa State University. Her research interests include supply chain risk and disruption, supply chain
coordination, and supplier assessment and selection.
Her publications have appeared in journals such as
Journal of Operations Management, Journal of Business Logistics, International Journal of Production Research, Decision Sciences, and IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT.
Dr. Blackhurst is an Associate Editor for Decision Sciences Journal and Journal of Purchasing and Supply Management and serves on the editorial review
boards for IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, Journal of
Operations Management, Journal of Business Logistics, and Journal of Supply
Chain Management.

Xiaoling Zhang received the Master degree in finance from the Harbin Institute of Technology,
Harbin, Heilongjiang, China, in 2008. She is currently
working toward the Ph.D. degree in Management Science and Engineering Program at Harbin Institute of
Technology, Shenzhen Graduate School.
Her current research interests include supply chain
dynamics, supply chain modeling, and disruption
management. She has published articles in International Journal of Production Research.

Shanshan Wang received the Ph.D. degree in industrial engineering from Arizona State University,
Tempe, in 2010.
She is an Operations Researcher in Management
Science Laboratory, GE Global Research Center,
Niskayuna, NY. Her current research interests include
market simulation, supply chain modeling, and realtime simulation and scheduling.

Data & Knowledge Engineering 63 (2007) 879–893
www.elsevier.com/locate/datak

MMR: An algorithm for clustering categorical data using
Rough Set Theory
Darshit Parmar, Teresa Wu *, Jennifer Blackhurst
Department of Industrial Engineering, PO Box 875906, Arizona State University, Tempe, AZ 85287-5906, USA
Received 5 August 2006; received in revised form 18 April 2007; accepted 29 May 2007
Available online 13 June 2007

Abstract
A variety of cluster analysis techniques exist to group objects having similar characteristics. However, the implementation of many of these techniques is challenging due to the fact that much of the data contained in today’s databases
is categorical in nature. While there have been recent advances in algorithms for clustering categorical data, some are
unable to handle uncertainty in the clustering process while others have stability issues. This research proposes a new algorithm for clustering categorical data, termed Min–Min-Roughness (MMR), based on Rough Set Theory (RST), which has
the ability to handle the uncertainty in the clustering process.
Published by Elsevier B.V.
Keywords: Cluster analysis; Categorical data; Rough Set Theory; Data mining

1. Introduction
Cluster analysis is a data analysis tool used to group data with similar characteristics. It has been used in
data mining tasks such as unsupervised classiﬁcation and data summation, as well as segmentation of large
heterogeneous data sets into smaller homogeneous subsets that can be easily managed, separately modeled
and analyzed [12]. The basic objective in cluster analysis is to discover natural groupings of objects [14]. Cluster analysis techniques have been used in many areas such as manufacturing, medicine, nuclear science, radar
scanning and research and development planning. For example, Jiang et al. [13] analyze a variety of cluster
techniques for complex gene expression data. Wu et al. [40] develop a clustering algorithm speciﬁcally designed
to handle the complexities of gene data that can estimate the correct number of clusters and ﬁnd them. Wong
et al. [39] present an approach used to segment tissues in a nuclear medical imaging method known as positron
emission tomography (PET). Mathieu and Gibson [26] use cluster analysis as a part of a decision support tool
for large-scale research and development planning to identify programs to participate in and to determine
resource allocation. Finally, Haimov et al. [8] use cluster analysis to segment radar signals in scanning land
and marine objects.
*

Corresponding author.
E-mail address: teresa.wu@asu.edu (T. Wu).

0169-023X/$ - see front matter Published by Elsevier B.V.
doi:10.1016/j.datak.2007.05.005

880

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

A problem with many of the clustering methods and applications mentioned above is that they are applicable for clustering data having numerical values for attributes. Most of the work in clustering is focused on
attributes with numerical value due to the fact that it is relatively easy to deﬁne similarities from the geometric
position of the numerical data. Unlike numerical data, categorical data have multi-valued attributes. Thus,
similarity can be deﬁned as common objects, common values for the attributes, and the association between
the two. In such cases, the horizontal co-occurrences (common attributes for the objects) as well as the vertical
co-occurrences (common values for the attributes) can be examined [40].
A number of algorithms for clustering categorical data have been proposed including work by Huang [12],
Gibson et al. [5], Guha et al. [6], Ganti et al. [4], and Dempster et al. [2]. While these methods make important
contributions to the issue of clustering categorical data, they are not designed to handle uncertainty in the
clustering process. This is an important issue in many real world applications where there is often no sharp
boundary between clusters. Recently, there has been work in the area of applying fuzzy sets in clustering categorical data including work by Huang [12] and Kim et al. [16]. However, these algorithms require multiple
runs to establish the stability needed to obtain a satisfactory value for one parameter used to control the membership fuzziness.
Therefore, there is a need for a robust clustering algorithm that can handle uncertainty in the process of
clustering categorical data. This research proposes a clustering algorithm based on Rough Set Theory
(RST). The proposed algorithm, named Min–Min-Roughness (MMR), is designed to deal with uncertainty
in the process of clustering categorical data. In addition, the algorithm is implemented and tested with three
real world data sets. To compare the algorithm performance in handling uncertainty, Soybean and Zoo data
sets are used and the results are compared with fuzzy set theory based algorithms (including K-modes, fuzzy
K-modes and fuzzy centroids). To test the applicability to large scale date sets, the Mushroom data set is used
and the results are compared with Squeezer, K-modes and LCBCDC, as well as ROCK and a traditional hierarchical algorithm. The contributions of our proposed approach include:
(1) Unlike previous methods, MMR gives the user the ability to handle uncertainty in the clustering process.
(2) Using MMR, the user is able to obtain stable results given only one input: the number of clusters.
(3) MMR has the capability of handling large data sets.
This paper is structured as follows: Section 2 presents an overview of standard clustering methods existing
in the literature. In Section 3, the basics of the rough set theory are introduced followed by the proposed
MMR algorithm. A synthetic data set is used to illustrate the MMR algorithm. Section 4 discusses the implementation of the algorithm and the results from the application of the algorithm on Soybean, Zoo and Mushroom data sets (from the UCI Machine Learning Repository1). In addition, the comparison results are
analyzed. Section 5 presents conclusions and identiﬁes future research directions.
2. Literature review
In this section, an overview of methods available in the literature to cluster categorical data is presented.
Ralambondrainy [33] proposes a method to convert multiple category attributes into binary attributes using
0 and 1 to represent either a category absence or presence, and to treat the binary attributes as numeric in the
K-means algorithm. Dempster et al. [2] presents a partitional clustering method, called the Expectation-Maximization (EM) algorithm. EM ﬁrst randomly assigns diﬀerent probabilities to each class or category, for each
cluster. These probabilities are then successively adjusted to maximize the likelihood of the data given the
speciﬁed number of clusters. Since the EM algorithm computes the classiﬁcation probabilities, each observation belongs to each cluster with a certain probability. The actual assignment of observations to a cluster is
determined based on the largest classiﬁcation probability. After a large number of iterations, EM terminates
at a locally optimal solution. Han et al. [9] propose a clustering algorithm to cluster related items in a market
database based on an association rule hypergraph. A hypergraph is used as a model for relatedness. The
1

Available at http://www.ics.uci.edu/~mlearn/MLRepository.html.

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

881

approach targets binary transactional data. It assumes item sets that deﬁne clusters are disjoint and there is no
overlap amongst them. However, this assumption may not hold in practice as transactions in diﬀerent clusters
may have a few common items. K-modes [12] extends K-means and introduces a new dissimilarity measure for
categorical data. The dissimilarity measure between two objects is calculated as the number of attributes
whose values do not match. The K-modes algorithm then replaces the means of clusters with modes, using
a frequency based method to update the modes in the clustering process to minimize the clustering cost function. One advantage of K-modes is it is useful in interpreting the results [12]. However, K-modes generates
local optimal solutions based on the initial modes and the order of objects in the data set. K-modes must
be run multiple times with diﬀerent starting values of modes to test the stability of the clustering solution.
Huang [12] also proposes the K-prototypes algorithm, which allows clustering of objects described by a combination of numeric and categorical data. CACTUS (Clustering Categorical Data Using Summaries) [4] is a
summarization based algorithm. In CACTUS, the authors cluster for categorical data by generalizing the definition of a cluster for numerical attributes. Summary information constructed from the data set is assumed to
be suﬃcient for discovering well-deﬁned clusters. CACTUS ﬁnds clusters in subsets of all attributes and thus
performs a subspace clustering of the data. Guha et al. [6] propose a hierarchical clustering method termed
ROCK (Robust Clustering using Links), which can measure the similarity or proximity between a pair of
objects. Using ROCK, the number of ‘‘links’’ are computed as the number of common neighbors between
two objects. An agglomerative hierarchical clustering algorithm is then applied: ﬁrst, the algorithm assigns
each object to a separate cluster, clusters are then merged repeatedly according to the closeness between clusters, where the closeness is deﬁned as the sum of the number of ‘‘links’’ between all pairs of objects. Gibson
et al. [5] propose an algorithm called STIRR (Sieving Through Iterated Relational Reinforcement), a generalized spectral graph partitioning method for categorical data. STIRR is an iterative approach, which maps categorical data to non-linear dynamic systems. If the dynamic system converges, the categorical data can be
clustered. Clustering naturally lends itself to combinatorial formulation. However, STIRR requires a non-trivial post-processing step to identify sets of closely related attribute values [4]. Additionally, certain classes of
clusters are not discovered by STIRR [4]. Moreover, Zhang et al. [41] argue that STIRR cannot guarantee
convergence and therefore propose a revised dynamic system algorithm that assures convergence. He et al.
[11] propose an algorithm called Squeezer, which is a one-pass algorithm. Squeezer puts the ﬁrst-tuple in a
cluster and then the subsequent-tuples are either put into an existing cluster or rejected to form a new cluster
based on a given similarity function. He et al. [10] explore categorical data clustering (CDC) and link clustering (LC) problems and propose a LCBCDC (Link Clustering Based Categorical Data Clustering), and compare the results with Squeezer and K-mode. In reviewing these algorithms, some of the methods such as
STIRR and EM algorithms cannot guarantee the convergence while others have scalability issues. In addition,
all of the algorithms have one common assumption: each object can be classiﬁed into only one cluster and all
objects have the same degree of conﬁdence when grouped into a cluster [7]. However, in real world applications, it is diﬃcult to draw clear boundaries between the clusters. Therefore, the uncertainty of the objects
belonging to the cluster needs to be considered.
One of the ﬁrst attempts to handle uncertainty is fuzzy K-means [34]. In this algorithm, each pattern or
object is allowed to have membership functions to all clusters rather than having a distinct membership to
exactly one cluster. Krishnapuram and Keller [18] propose a probabilistic approach to clustering in which
the membership of a feature vector in a class has nothing to do with its membership in other classes and
modiﬁed clustering methods are used to generate membership distributions. Krishnapuram et al. [17] present several fuzzy and probabilistic algorithms to detect linear and quadratic shell clusters. Note the initial
work in handling uncertainty was based on numerical data. Huang [12] proposes a fuzzy K-modes algorithm
with a new procedure to generate the fuzzy partition matrix from categorical data within the framework of
the fuzzy K-means algorithm. The method ﬁnds fuzzy cluster modes when a simple matching dissimilarity
measure is used for categorical objects. By assigning conﬁdence to objects in diﬀerent clusters, the core and
boundary objects of the clusters can be decided. This helps in providing more useful information for dealing
with boundary objects. More recently, Kim et al. [16] have extended the fuzzy K-modes algorithm by using
fuzzy centroids to represent the clusters of categorical data instead of the hard-type centroids used in the
fuzzy K-modes algorithm. The use of fuzzy centroids makes it possible to fully exploit the power of fuzzy
sets in representing the uncertainty in the classiﬁcation of categorical data. However, fuzzy K-modes and

882

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

fuzzy centroids algorithms suﬀer from the same problem as K-modes, that is they require multiple runs with
diﬀerent starting values of modes to test the stability of the clustering solution. In addition, these algorithms
have to adjust one control parameter for membership fuzziness to obtain better solutions. This necessitates
the eﬀort for multiple runs of these algorithms to determine an acceptable value of this parameter. Therefore, there is a need for a categorical data clustering method, having the ability to handle uncertainty in the
clustering process while providing stable results. One methodology with potential for handling uncertainty is
Rough Set Theory (RST) which has received considerable attention in the computational intelligence literature since its development by Pawlak in the 1980s. Unlike fuzzy set based approaches, rough sets have no
requirement on domain expertise to assign the fuzzy membership. Still, it may provide satisfactory results
for rough clustering. The objective of this research is to develop a rough set based approach for categorical
data clustering. The approach, termed Min–Min-Roughness (MMR), is presented and its performance is
evaluated on large scale data sets.
3. Min–Min-Roughness (MMR) algorithm
3.1. Nomenclature
U
universe or the set of all objects (x1, x2, . . .)
X
subset of the set of all objects, (X  U)
xi
object belonging to the subset of the set of all objects, xi 2 X
A
the set of all attributes (features or variables)
ai
attribute belonging to the set of all attributes, ai 2 A
V(ai)
set of values of attribute ai (or called domain of ai)
B
non-empty subset of A(B  A)
XB
lower approximation of X with respect to B
XB
upper approximation of X with respect to B
Rai ðX Þ roughness with respect to {ai}
Roughaj ðai Þ mean roughness on attribute ai with respect to {aj}
MR (ai) minimum roughness of attribute ai
MMR minimum of MR of all attributes
Ind(B) indiscernibility relation
[xi]Ind(B) equivalence class of xi in relation Ind(B), also known as elementary set in B.

3.2. Rough Set Theory (RST)
RST is an approach to aid decision making in the presence of uncertainty [30,31]. It classiﬁes imprecise,
uncertain or incomplete information expressed in terms of data acquired from experience. In RST, a set of
all similar objects is called an elementary set, which makes a fundamental atom of knowledge [29]. Any union
of elementary sets is called a crisp set and other sets are referred to as rough set [29]. As a result of this definition, each rough set has boundary-line elements. For example, some elements cannot be deﬁnitively classiﬁed as members of the set or its complement. In other words, when the available knowledge is employed,
boundary-line cases cannot be properly classiﬁed. Therefore, rough sets can be considered as uncertain or
imprecise. Upper and lower approximations are used to identify and utilize the context of each speciﬁc object
and reveal relationships between objects. The upper approximation includes all objects that possibly belong to
the concept while the lower approximation contains all objects that surely belong to the concept.
Let U be the set of all objects, A be the set of all attributes, B be a non-empty subset of A, V be the set of all
attribute values and U · A ! V be an information function.
Deﬁnition 1 (Indiscernibility relation (Ind(B))). Ind(B)is a relation on U. Given two objects, xi, xj 2 U, they are
indiscernible by the set of attributes B in A, if and only if a(xi) = a(xj) for every a 2 B. That is, (xi, xj 2 Ind(B)
if and only if "a 2 B where B  A, a(xi) = a(xj).

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

883

Deﬁnition 2 (Equivalence class ð½xi IndðBÞ Þ). Given Ind(B), the set of objects xi having the same values for the set
of attributes in B consists of an equivalence classes, [xi]Ind(B). It is also known as elementary set with respect to
B.
Deﬁnition 3 (Lower approximation). Given the set of attributes B in A, set of objects X in U, the lower approximation of X is deﬁned as the union of all the elementary sets which are contained in X. That is,
X B ¼ [fxi j½xi IndðBÞ  X g

ð1Þ

Deﬁnition 4 (Upper approximation). Given the set of attributes B in A, set of objects X in U, the upper approximation of X is deﬁned as the union of the elementary sets which have a non-empty intersection with X. That
is,
X B ¼ [fxi j½xi IndðBÞ \ X 6¼ ;g

ð2Þ

Deﬁnition 5 (Roughness). The ratio of the cardinality of the lower approximation and the cardinality of the
upper approximation is deﬁned as the accuracy of estimation, which is a measure of roughness. It is presented
as
RB ðX Þ ¼ 1 

jX B j
jX B j

ð3Þ

If RB(X) = 0, X is crisp with respect to B, in other words, X is precise with respect to B. If RB(X) < 1, X is
rough with respect to B, that is, B is vague with respect to X. It is this measure, roughness, which allows an
object to belong to a cluster with diﬀerent degrees of belonging using the MMR algorithm. In other words,
MMR has the ability to deal with uncertainty with the calculation of lower bound and upper bound that gives
a degree of belonging rather than having the same degree of belonging to all objects.
Recently, RST has found many diﬀerent applications. Examples include semiconductor manufacturing
[20,35], the automobile industry [21], business failure predictions [3], customer retention [19], intelligent image
ﬁltering [42], clinical databases [36], classiﬁcation of highway sections [22], and web mining [15] just to name a
few. RST has been used extensively for supervised learning with a focus on classiﬁcation problems, where
prior group membership is known. Results generated usually are rules for group membership [32]. Clustering
within the context of RST is attracting increasing interest. Lingras [22] explores how to use a rough set genome
to represent a rough set theoretic classiﬁcation scheme. Later, the modiﬁed K-means algorithm [24] and Kohonen Neural Network [23] are proposed to create intervals of clusters based on RST. Furthermore, considering
the possibility that one object may belong to more than one cluster, Lingras et al. [25] investigate three methodologies (genetic algorithms, K-means and Kohonen Self-Organizing Maps) for clustering based on the properties of rough sets for developing the lower-upper bound representation of the clusters. Voges et al. [37]
propose a technique called rough clustering, which is a simple extension of RST, and apply it to the problem
of market segmentation. However, the majority of research in exploring RST for clustering aims to handle
numerical data sets where distance can be easily derived from the data set. Instead of deﬁning distance
between objects, MMR is developed for categorical data based on the concept of roughness.

3.3. Min–Min-Roughness (MMR)
Mazlack et al. [27] attempt to use RST for choosing partitioning attributes for clustering. They use a measure called total roughness to determine the crispness of the partition. However, for partitioning, the method
starts with binary valued attributes and uses the total roughness criterion only for multi-valued attributes.
This creates from a handicap due to the fact that the partitioning is done on a binary attribute even though
the total roughness for a multi-valued attribute is lower. MMR overcomes this drawback by clustering the
objects on all attributes. In addition, MMR proposes a new way to measure data similarities based on the
roughness concept. MMR utilizes a measure termed mean roughness comparable to that proposed by Mazlack et al. [27] based on RST. This is reproduced below:

884

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

Deﬁnition 6 (Mean roughness [27]). Given ai 2 A, V(ai) refers to the set of values of attribute ai, X is a subset
of objects having one speciﬁc value, a, of attribute ai, that is, X(ai = a), X aj ðai ¼ aÞ refers to the lower
approximation, and X aj ðai ¼ aÞ refers to the upper approximation with respect to {aj}, then Raj ðX Þ is deﬁned
as the roughness of X with respect to {aj}, that is
Raj ðX jai ¼ aÞ ¼ 1 

jX aj ðai ¼ aÞj
jX aj ðai ¼ aÞj

;

where ai ; aj 2 A

and

ai 6¼ aj

ð4Þ

Let jV(ai)j be the number of values of attributes ai, the mean roughness on attribute ai with respect to {aj} is
deﬁned as
Roughaj ðai Þ ¼

Raj ðX jai ¼ a1 Þ þ    Raj ðX jai ¼ ajV ðai Þj Þ
;
jV ðai Þj

ð5Þ

where ai, aj 2 A and ai 5 aj.
Next, Min-Roughness (MR) and Min–Min-Roughness (MMR) developed in this research are introduced.
Deﬁnition 7 (Min-Roughness (MR)). Given n attributes, MR, min-roughness of attribute ai (ai 2 A) refers to
the minimum of the mean roughness, that is,
MRðai Þ ¼ Min ðRougha1 ðai Þ; . . . Roughaj ðai Þ . . . ;

where ai ; aj 2 A; ai 6¼ aj ; 1 6 i; j 6 n

ð6Þ

Deﬁnition 8 (Min–Min-Roughness (MMR)). Given n attributes, the MMR is deﬁned as the minimum of the
Min-Roughness of the n attributes. That is,
MMR ¼ MinðMRða1 Þ; . . . MRðai Þ; . . .

where ai 2 A; i goes from 1 to cardinality ðAÞ

ð7Þ

The lower the mean roughness is, the higher the crispness of the clustering. Min-Roughness (MR) (Deﬁnition 7) determines the best crispness each attribute can achieve. MMR (Deﬁnition 8) determines the best split
on the attributes. The MMR algorithm (as shown in Table 1) iteratively divides the group of objects with the
goal of achieving better clustering crispness. The algorithm takes the number of clusters, k, as one input and
will terminate when this pre-deﬁned number k, is reached.
Next, we present an illustrative example of the MMR algorithm.
[Illustrative Example]: Table 2 introduces a data set (I) used to illustrate the application of the MMR
algorithm. There are ten objects (m = 10) and six attributes (n = 6). The maximum number of values is 4
(l = 4). Our interest is to create clusters of similar objects. As seen from the data set in Table 2, variables can be multi-valued. That is, the domain of an attribute can contain more than two distinct
values.
First, the mean roughness on each attributes ai (i = 1, . . ., 6) is calculated. Let us take attribute a1 as an
example. The mean roughness on a1 with respect to {a2} is calculated as following. There are three elementary
sets for a1: X (a1 = Small) = {3, 5, 7, 8}, X (a1 = Medium) = {2, 4, 10} and X (a1 = Big) = {1, 6, 9}. There are
four elementary sets for a2: X(a2 = Blue) = {1, 4}, X(a2 = Red) = {2}, X(a2 = Yellow) = {3, 5, 7, 8} and
X(a2 = Green) = {6, 9, 10}. According to Deﬁnition 3 and 4, the lower approximation of X (a1 = Small) is
{3, 5, 7, 8} and the upper approximation is the same, the lower approximation of X (a1 = Medium) is {2}
and the upper approximation is {1, 2, 4, 6, 9, 10}, the lower approximation of X (a1 = Big) is empty thus there
is no need of calculating the upper approximation. According to Deﬁnition 6, the mean roughness on a1 with
respect to {a2} is 0.6111. Following the same procedure, the mean roughness on a1 with respect to {a3}, {a4},
{a5}, {a6} is computed. These calculations are summarized in Table 3. Similar calculations are performed for
all the attributes.
Second, the partitioning attribute with the MMR is found. Table 4 shows the calculations and illustrates
that attribute a1 and a3 have the same MMR. In using the algorithm, it is recommended to look at the next
lowest MMR inside the attributes that are tied and so on until the tie is broken. In the case where all the numbers are tied, selecting any attribute randomly can break the tie. In the example, the second MMR corresponding to attribute a1 is lower than that of a3. Therefore, attribute a1 is selected as the partitioning attribute and
binary splitting is conducted.

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

885

Table 1
MMR Algorithm
Procedure MMR(U, k)
Begin
Set current number of cluster CNC = 1
Set ParentNode = U
Label 1:
If CNC < k and CNC 5 1 then
ParentNode = ProcParentNode (CNC)
End if
// Clustering the ParentNode
For each ai 2 A (i = 1 to n, where n is the number of attributes in A)
Determine ½xi Indðai Þ
For each aj 2 A(j = 1 to n, where n is the number of attributes in A, j 5 i)
Calculate Roughaj ðai Þ
Next
Min-Roughness (ai) = Min (Roughaj ðai ÞÞ
Next
Set Min–Min-Roughness = Min (Min-Roughness (ai)), i = 1,. . .,n
Determine splitting attribute ai corresponding to the Min–Min-Roughness
Do binary split on the splitting attribute ai
CNC = the number of leaf nodes
Go to Label 1
End
ProcParentNode (CNC)
Begin
Set i = 1
Do until i < CNC
Size (i) = Count (Set of Elements in Cluster i)
i=i+1
Loop
Determine Max (Size (i))
Return (Set of Elements in cluster i) corresponding to Max (Size (i))
End

Table 2
Example data set (I)
Rows

a1

a2

a3

a4

a5

a6

1
2
3
4
5
6
7
8
9
10

Big
Medium
Small
Medium
Small
Big
Small
Small
Big
Medium

Blue
Red
Yellow
Blue
Yellow
Green
Yellow
Yellow
Green
Green

Hard
Moderate
Soft
Moderate
Soft
Hard
Hard
Soft
Hard
Moderate

Indeﬁnite
Smooth
Fuzzy
Fuzzy
Indeﬁnite
Smooth
Indeﬁnite
Indeﬁnite
Smooth
Smooth

Plastic
Wood
Plush
Plastic
Plastic
Wood
Metal
Plastic
Wood
Plastic

Negative
Neutral
Positive
Negative
Neutral
Positive
Positive
Positive
Neutral
Neutral

Table 3
Mean roughness calculation for attribute a1
With
With
With
With
With

respect
respect
respect
respect
respect

to
to
to
to
to

{a2}
{a3}
{a4}
{a5}
{a6}

X1 (Small)

X2 (Medium)

X3 (Big)

Mean roughness

0
0.5714
1
0.7143
1

0.8333
0
1
1
1

1
1
1
1
1

0.6111
0.5238
1
0.9048
1

886

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

Table 4
MMR calculation
Attributes

Mean roughness

Min roughness

a1
a2
a3
a4
a5
a6

Roughaj ða1 Þ,
Roughaj ða2 Þ,
Roughaj ða3 Þ,
Roughaj ða4 Þ,
Roughaj ða5 Þ,
Roughaj ða6 Þ,

Min: 0.5238 Second Min: 0.6111
0.7500
Min: 0.5238 Second Min: 0.9074
0.6667
0.8820
0.6250

j = 2 3 4 5 6 (0.6111, 0.5238, 1, 0.9048, 1)
j = 1, 3, 4, 5, 6 (0.7500, 0.8929, 1, 0.9286, 0.7500)
j = 1, 2, 4, 5, 6 (0.5238, 0.9444, 1, 0.9074, 1)
j = 1, 2, 3, 5, 6 (1, 0.6667, 1, 0.7639, 1)
j = 1, 2, 3, 4, 6 (1, 0.8820, 1, 1, 0.9500)
j = 1, 2, 3, 4, 5 (1, 0.6250, 1, 1, 0.9333)

Third, the splitting point on attributes a1 is determined. Note the binary optimal partition problem is NPhard [28]. MMR determines the splitting point using the heuristic based on the roughness calculation that simpliﬁes the computational complexity. The splitting set should include the attribute value which has minimum
roughness. Taking a look at Table 3, X (a1 = Small) has overall minimum roughness with respect to {ai}
(i = 2, 3, 4, 5, 6) comparing to X (a1 = Medium) and X (a1 = Big). Thus, splitting on X (a1 = Small) vs. X
(a1 = Medium) and X (a1 = Big) is chosen. The partition at this stage can be represented as a tree and is shown
in Fig. 1.
The numbers in the parenthesis at each of the child nodes correspond to the objects in the original data set.
Set (1, 2, 4, 6, 9, 10) corresponds to all objects having either Big or Mediuma s the value for attribute a1 and set
(3, 5, 7, 8) corresponds to all objects having Small as value for attribute a1. The algorithm is applied recursively
to obtain further partitions. At subsequent iterations, the leaf node having more objects is selected for further
splitting. The algorithm terminates when it reaches a pre-deﬁned number of clusters. This is subjective and is
pre-decided based either on user requirement or domain knowledge.
Next we present a comparison with the MMR algorithm with the approach by Mazlack et al. [27].
[Comparison example]: As an extension of the approach proposed by Mazlack et al. [27], MMR provides
better solution by considering all attributes (bi-valued, multi-valued) equally. Consider the data set (II) shown
in Table 5.
Following the procedures calculating MMR, the results are summarized in Table 6.
Clearly, the approach proposed by Mazlack will partition on attribute a1 (‘‘Volume’’) since it is binary (i.e.,
has only two distinct attribute values). However, MMR will partition on attribute a2 (‘‘Material’’). Overall,
the MMR algorithm will result in a crisper solution.

a1
Medium-big
(1,2,4,6,9,10)

Small
(3,5,7,8)

Fig. 1. Partition after ﬁrst iteration.

Table 5
Example data set (II)
Volume (a1)

Material (a2)

Location (a3)

High
High
High
High
High
Low
Low
Low

Hard
Hard
Medium
Soft
Soft
Hard
Medium
Soft

Paciﬁc
Midwest
East Coast
International
Paciﬁc
Midwest
East Coast
International

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

887

Table 6
MMR results on example data set (II)
Attributes

Mean roughness

Min roughness

a1
a2
a3

Roughaj ða1 Þ ¼ ð1; 0:95Þ, j = 2, 3
Roughaj ða2 Þ ¼ ð1; 0:56Þ, j = 1, 3
Roughaj ða3 Þ ¼ ð1; 0:92Þ, j = 1, 2

0.95
0.56
0.92

Thus, given a data set, assume n is the number of attributes, m is the number of objects, k is the chosen
number of clusters and l is the maximum number of values in the attribute domains. k  1 iterations are
required to achieve k clusters from the data set. In each iteration, the time to ﬁnd all the elementary sets of
each attribute is n * m, the time to calculate the mean roughness is approximately n2l, the time to calculate
MR and MMR is 2n. Thus, the computation complexity is polynomial, that is O(knm + kn2l). For any larger
data set with an increasing number of objects (m) and an increasing number of attributes (n), the computation
time increases by m * n. Given the minimum roughness over all other attributes exists on one particular value
(e.g., p) of the attribute (e.g., ai), the splitting point can be set as (ai = p) and (ai 5 p). Since application of
RST in clustering is relatively new, our focus has been on evaluating the performance of MMR. Looking
in the future, with the ever-increasing computing capabilities, computation complexity may not be an issue.
However, future plans will include eﬀorts to reduce the complexity of the MMR algorithm. For example,
we will explore the use of special data structure to reduce the computation eﬀort.
The following section describes the implementation and the results obtained from the application of the
MMR algorithm on three real world data sets. It also includes the results of comparison of the MMR algorithm with fuzzy set theory based algorithms and a traditional hierarchical algorithm.
4. Experimental analysis
In order to test MMR, a prototype implementation system is developed using VB.Net and tested on several
data sets obtained from the UCI Machine Learning Repository. Validating clustering results is a non-trivial
task. The purity of clusters was used as a measure to test the quality of the clusters. The purity of a cluster is
deﬁned as
PurityðiÞ ¼

the number of data occuring in both the ith cluster and its corresponding class
the number of data in the data set

The overall purity is deﬁned as
P# of clusters
PurityðiÞ
Over all Purity ¼ i¼1
# of clusters

ð8Þ

ð9Þ

According to this measure, a higher value of overall purity indicates a better clustering result, with perfect
clustering yielding a value of 1. Note that similar measures have been used in Kim et al. [16] and Guha
et al. [6].
4.1. Comparison of MMR with algorithms based on fuzzy set theory
Currently, there are only a few algorithms which aim to handle uncertainty in the clustering process. These
algorithms are fuzzy set based algorithms and include K-modes, fuzzy K-modes and fuzzy centroids. K-modes
uses a dissimilarity measure between two objects which is calculated as the number of attributes whose values
do not match. The K-modes algorithm then replaces the means of the clusters with modes and uses a frequency
based method to update the modes in the clustering process to minimize the clustering cost function. Fuzzy Kmodes generates a fuzzy partition matrix from categorical data. By assigning a conﬁdence to objects in diﬀerent clusters, the core and boundary objects of the clusters are determined for clustering purposes. The fuzzy
centroids algorithm uses the concept of fuzzy set theory to derive fuzzy centroids to create clusters of objects
which have categorical attributes. In this section, MMR is compared with these three algorithms based on
Soybean and Zoo data sets in three experiments.

888

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

Table 7
Applying MMR on the Soybean data set
Cluster number

Disease 1

Disease 2

Disease 3

Disease 4

Purity

1
2
3
4

0
10
0
0

10
0
0
0

0
0
8
2

0
0
17
0

1
1
0.68
1

Overall purity

0.83

Experiment 1. The Soybean data set contains 47 objects on diseases in soybeans. Each object can be classiﬁed
as one of the four diseases namely, Diaporthe Stem Canker, Charcoal Rot, Rhizoctonia Root Rot, and
Phytophthora Rot and is described by 35 categorical attributes. The data set is comprised 17 objects for
Phytophthora Rot disease and 10 objects for each of the remaining diseases. Since there are four possible class
values (four diseases), the algorithms based on fuzzy set theory generate four clusters. For comparison
purposes the number of clusters is set to 4 for MMR. The results are summarized in Table 7. Out of 47 objects,
39 belong to the majority class label of the cluster in which they are classiﬁed. Thus, the overall purity of the
clusters is 83%.
Experiment 2. The Zoo data set is comprised of 101 objects, where each data point represents information of
an animal in terms of 18 categorical attributes. Each animal data point is classiﬁed into seven classes. Therefore, stopping criterion for MMR is set at seven clusters. Table 8 summarizes the results of running the MMR
algorithm on the Zoo data set. Out of 101 objects, 92 belong to the majority class label of the cluster in which
they are classiﬁed. Thus, the overall purity of the clusters is 91%.
Kim et al. [16] have applied the fuzzy centroid method to the Soybean and Zoo data sets and compared the
results with K-modes and fuzzy K-modes. In this research, MMR is applied to the same data sets and therefore
can be used to compare with all three algorithms. Similar to Kim et al. [16], purity is used as the evaluation
criteria for comparison study. Table 9 summarizes the results of comparison study.
As shown in Table 9, MMR out performs K-modes and fuzzy K-modes on both the data sets as well as
performing better than the fuzzy centroid method on the Zoo data set. The performance of MMR is comparable to the performance of algorithms based on the fuzzy set theory. As discussed above, fuzzy set based algorithms all face a challenging issue, namely stability. These algorithms require great eﬀort to adjust the
parameter, which is used to control the fuzziness of membership of each data point. Therefore, the algorithms

Table 8
Applying MMR on the Zoo data set
Cluster number

C1

C2

C3

C4

C5

C6

C7

Purity

1
2
3
4
5
6
7

0
39
0
0
0
2
0

0
0
0
0
0
0
20

3
0
1
1
0
0
0

0
0
0
13
0
0
0

3
0
1
0
0
0
0

0
0
0
0
2
6
0

0
0
0
0
10
0
0

0.50
1
0.50
0.93
0.83
0.75
1

Overall purity

0.91

Table 9
Purity comparison of MMR with algorithms based on Fuzzy Set Theory
Data set

K-modes

Fuzzy K-modes

Fuzzy centroids

MMR

Soybean
Zoo

0.69
0.60

0.77
0.64

0.97
0.75

0.83
0.91

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

889

need to be run at diﬀerent values of the parameter. At each value of this parameter, the algorithms need to be
run multiple times to achieve a stable solution. In these regards, MMR has no such issues and oﬀers the following advantages:
1. MMR needs just one input parameter namely the number of clusters.
2. MMR does not require multiple iterations with diﬀerent starting values.

4.2. MMR tested on a large data set
Both Soybean and Zoo data sets are relatively small (47, 101 objects, respectively). To test the applicability
of MMR on a larger data set, additional tests on the Mushroom data set (8124 objects) are performed comparing MMR to ﬁrst the Squeezer, K-modes and LCBCDC methods and second to a traditional hierarchical
algorithm in two experiments. Squeezer is a one-pass algorithm which places the ﬁrst-tuple in a cluster and
then the subsequent-tuples are either put into an existing cluster or rejected to form a new cluster based on
a given similarity function. K-modes, as previously discussed, uses a dissimilarity measure between two objects
which is calculated as the number of attributes whose values do not match. LCBCDC uses Link Clustering
(LC) and Categorical Data Clustering (CDC). Finally, the traditional hierarchical algorithm converts the categorical into boolean attributes with 0/1 values. Euclidean distance is used as the distance measure between
the centroids of clusters. Pairs of clusters whose centroids or means are the closest are then successively
merged until the desired number of clusters remain.
The Mushroom data set contains 8124 objects where each object contains information of a single mushroom. There are 22 categorical attributes; each attribute corresponds to a physical characteristic of a mushroom. An object also contains a poisonous or edible class label for a mushroom. The data set has 4208
edible mushrooms and 3916 poisonous mushrooms.
Experiment 3. According to He et al. [10], Squeezer and K-modes algorithms produce better clustering output
than other algorithms in categorical data sets with respect to clustering purity. LCBCDC was compared with
Squeezer and K-modes in He et al. [10] based on the Mushroom data set with two clusters created by each
algorithm. For comparison purposes, MMR uses two clusters as the stopping criterion. Results are shown in
Table 10. Clearly, MMR outperforms the Squeezer and K-modes algorithms, which do not handle
uncertainty. It is interesting to note that LCBCDC provides better purity (86%). However, the results shown
indicate that only partial data set (5478 records out of 8124) is used for LCBCDC. He et al. [10] explain that
LCBCDC reasonably discards the malignant records as outliers and presents better cluster results.
Experiment 4. Guha et al. [6] compare ROCK with a traditional hierarchical algorithm based on the Mushroom data set. MMR is applied on the same data set and the results are used to compare MMR to the traditional hierarchical algorithm. As the traditional hierarchical algorithm generates 20 clusters, for comparison
purpose, MMR also uses 20 clusters as the stopping criterion. Table 11 summarizes the results of running the
MMR algorithm on the Mushroom data set. Out of 8124 mushrooms, 6785 belong to the majority class label
of the cluster in which they are classiﬁed. Thus, the overall purity of the clusters is 84%.

Table 10
Comparing with Squeezer, K-modes and LCBCDC on Mushroom data set
Cluster number

Squeezer

K-modes

1

No. of poisonous
No. of edible

3873
3723

No. of poisonous
No. of edible

1856
1470

No. of poisonous
No. of edible

1768
48

No. of poisonous
No. of edible

1994
260

2

No. of poisonous
No. of edible

43
485

No. of poisonous
No. of edible

2060
2738

No. of poisonous
No. of edible

712
2950

No. of poisonous
No. of edible

1922
3948

Purity

0.56

0.56

LCBCDC

0.86

MMR

0.73

890

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

Table 11
Applying MMR on the mushroom data set with 20 clusters
Cluster number

Number of poisonous mushrooms

Number of edible mushrooms

Purity

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

0
36
0
0
0
0
151
0
115
120
782
321
6
1
28
0
323
1440
196
397

164
425
640
539
19
1
329
14
144
111
0
1177
0
0
0
40
120
88
12
385

1
0.92
1
1
1
1
0.68
1
0.56
0.52
1
0.79
1
1
1
1
0.73
0.94
0.94
0.51

Overall purity

0.84

Results were then compared with the application of the Mushroom data set on the traditional hierarchical
algorithm. One noticeable discrepancy in the results obtained from the traditional hierarchical algorithm is in
the total number of mushrooms. The total number of mushrooms turns out to be 7795 instead of 8124 in the
original data set. With the assumption that the remaining 329 mushrooms are classiﬁed correctly, 4692 out of
8124 mushrooms belong to the majority class label of the cluster in which they are classiﬁed. Thus, the overall
purity of the clusters is 58%, whereas the purity from MMR is 84%. Clearly, MMR outperforms the traditional hierarchical algorithm. However, results from MMR indicate that there are signiﬁcant diﬀerences in
the size of the clusters. This may be attributed to the fact that the leaf node with the largest number of objects
to split with at the beginning of each iteration was selected, which leaves leaf nodes with next largest number
of objects untouched. Diﬀerent splitting strategies will be explored in future work. Another reason for the differences in the size of the clusters might be due to the fact that MMR does not remove any outliers. MMR uses
the entire data set and the outliers may be aﬀecting the size of the clusters. Therefore, outliers from the data set
will be removed in future work.
Results from Guha et al. [6] indicate that ROCK performs very well on the Mushroom data set after the
parameters are ﬁne-tuned. With 21 clusters being created, the purity of ROCK is 97%. However, as discussed
by Andritsos et al. [1], ROCK has some limitations. ROCK is very sensitive to the threshold value. In many
cases, this subjectively determined threshold value could signiﬁcantly aﬀect the ﬁnal clustering results. Secondly, ROCK tends to produce one giant cluster that includes objects from most classes. Thirdly, ROCK cannot guarantee the number of generated clusters is the same as what is speciﬁed when ROCK is initially
launched. For example, the application of the ROCK algorithm on the mushroom data set resulted in 21 clusters even though the input number of clusters was 20. Therefore, a detailed analysis on comparing MMR with
ROCK on the Mushroom data set is not provided.
5. Conclusions
Most algorithms designed to cluster categorical data sets are not designed to handle uncertainty in the data
set. The majority of the clustering techniques implicitly assume that an object can be classiﬁed into, at most,
one cluster and that all objects classiﬁed into a cluster belong to it with the same degree of conﬁdence. However, in many applications, there can be objects that might have the potential of being classiﬁed into more than

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

891

one cluster. Thus, there is a need for a clustering algorithm that can handle this uncertainty in the clustering
process. The MMR algorithm proposed in this paper has potential for clustering categorical attributes in data
mining. It is capable of handling the uncertainty in the clustering process. Unlike other algorithms, MMR
requires only one input, the number of clusters, and has been tested to be stable.
Additionally, we propose a number of future research activities related to MMR. First, we propose to
explore a new stopping criterion. Instead of picking the subset of the data set with the maximum number
of objects, we can ﬁrst determine the distance between the objects falling under each leaf node. The leaf node
with the maximum distance between the objects can be picked for splitting at the subsequent iteration. Secondly, we will extend MMR to handle both numerical and categorical data. We will explore a discretization
algorithm (such as 4cDiscretizzed which is an unsupervised discretization algorithm to divide the attribute
range into a constant number of intervals containing an equal number of the attribute values). Thirdly, to
achieve lower computation complexity, we will study the roughness measure based on relationship between
ai and the set deﬁned as A-{ai} instead of calculating the maximum with respect to all {aj} where aj 5 ai.
Fourthly, we propose to study the approach proposed by Voges and Pope [38] and explore the potential application of evolutionary algorithms with rough sets to help determine the number of clusters in advance. Finally,
more experiments need to be done on even larger data sets with more objects and more attributes.
References
[1] P. Andritsos, P. Tsaparas, R.J. Miller, K.C. Sevcik, Clustering categorical data based on information loss minimization, in: Second
Hellenic Data Management Symposium, 2003, pp. 334–344.
[2] A. Dempster, N. Laird, D. Rubin, Maximum likelihood from incomplete data via the EM algorithm, Journal of the Royal Statistical
Society 39 (1) (1977) 1–38.
[3] A.I. Dimitras, R. Slowinski, R. Susmaga, C. Zopounidis, Business failure prediction using rough sets, European Journal of
Operational Research 114 (22) (1999) 263–280.
[4] V., Ganti, J. Gehrke, R. Ramakrishnan, CACTUS – clustering categorical data using summaries, in: Fifth ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, 1999, pp. 73–83.
[5] D. Gibson, J. Kleinberg, P. Raghavan, Clustering categorical data: an approach based on dynamical systems, The Very Large Data
Bases Journal 8 (3–4) (2000) 222–236.
[6] S. Guha, R. Rastogi, K. Shim, ROCK: a robust clustering algorithm for categorical attributes, Information Systems 25 (5) (2000)
345–366.
[7] M. Halkidi, Y. Batistakis, M. Vazirgiannis, On clustering validation techniques, Journal of Intelligent Information Systems 17 (2–3)
(2001) 107–145.
[8] S. Haimov, M. Michalev, A. Savchenko, O. Yordanov, Classiﬁcation of radar signatures by autoregressive model ﬁtting and cluster
analysis, IEEE Transactions on Geo Science and Remote Sensing 8 (1) (1989) 606–610.
[9] E. Han, G. Karypis, V. Kumar, B. Mobasher, Clustering based on association rule hypergraphs, in: Workshop on Research Issues on
Data Mining and Knowledge Discovery, 1997, pp. 9–13.
[10] Z. He, X. Xu, S. Deng, A link clustering based approach for clustering categorical data, Proceedings of the WAIM Conference, 2004.
<http://xxx.sf.nchc.org.tw/ftp/cs/papers/0412/0412019.pdf>.
[11] Z. He, X. Xu, S. Deng, Squeezer: an eﬃcient algorithm for clustering categorical data, Journal of Computer Science & Technology 17
(5) (2002) 611–624.
[12] Z. Huang, Extensions to the k-means algorithm for clustering large data sets with categorical values, Data Mining and Knowledge
Discovery 2 (3) (1998) 283–304.
[13] D. Jiang, C. Tang, A. Zhang, Cluster analysis for gene expression data: a survey, IEEE Transactions on Knowledge and Data
Engineering 16 (11) (2004) 1370–1386.
[14] R. Johnson, W. Wichern, Applied Multivariate Statistical Analysis, Prentice Hall, New York, 2002.
[15] A. Joshi, R. Krishnapuram, Robust fuzzy clustering methods to support web mining, in: Proceedings of the Workshop on Data
Mining and Knowledge Discovery, vol. 15, 1998, pp. 1–8.
[16] D. Kim, K. Lee, D. Lee, Fuzzy clustering of categorical data using fuzzy centroids, Pattern Recognition Letters 25 (11) (2004) 1263–1271.
[17] R. Krishnapuram, H. Frigui, O. Nasraoui, Fuzzy and possibilistic shell clustering algorithms and their application to boundary
detection and surface approximation, IEEE Transactions on Fuzzy Systems 3 (1) (1995) 29–60.
[18] R. Krishnapuram, J. Keller, A possibilistic approach to clustering, IEEE Transactions on Fuzzy Systems 1 (2) (1993) 98–110.
[19] W. Kowalczyk, F. Slisser, Analyzing customer retention with rough data models, Proceedings of the First European Symposium on
Principles of Data Mining and Knowledge Discovery, Springer-Verlag, Trondheim, Norway, 1997, pp. 4–13.
[20] A. Kusiak, Rough set theory: a data mining tool for semiconductor manufacturing, IEEE Transactions on Electronics Packaging
Manufacturing 24 (1) (2001) 44–50.
[21] S. Lee, G. Vachtsevanos, An application of rough set theory to defect detection of automotive glass, Mathematics and Computers in
Simulation 60 (3–5) (2002) 225–231.

892

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

[22] P. Lingras, Unsupervised rough set classiﬁcation using GAs, Journal of Intelligent Information Systems 16 (3) (2001) 215–228.
[23] P. Lingras, M. Hogo, M. Snorek, Interval set clustering of web users using modiﬁed kohonen self-organizing maps based on the
properties of rough sets, Web Intelligence and Agent Systems 2 (3) (2004) 217–225.
[24] P. Lingras, C. West, Interval set clustering of web users with rough K-means, Journal of Intelligent Information Systems 23 (1) (2004)
5–16.
[25] P. Lingras, P.R. Yan, M. Hogo, Rough set based clustering: evolutionary, neural, and statistical approaches, Proceedings of the First
Indian International Conference on Artiﬁcial Intelligence (2003) 1074–1087.
[26] R. Mathieu, J. Gibson, A Methodology for large scale R&D planning based on cluster analysis, IEEE Transactions on Engineering
Management 40 (3) (2004) 283–292.
[27] L. Mazlack, A. He, Y. Zhu, S. Coppock, A rough set approach in choosing partitioning attributes, in: Proceedings of the ISCA 13th
International Conference (CAINE-2000), 2000, pp. 1–6.
[28] S.H. Nguyen, H.S. Nguyen, Pattern extraction from data, Fundamentra Informaticae 34 (1–2) (1998) 1–16.
[29] Z. Pawlak, Rough sets, International Journal of Computer and Information Sciences 11 (5) (1982) 341–356.
[30] Z. Pawlak, Rough Sets – Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, Boston, 1991.
[31] Z. Pawlak, Rough set approach to knowledge-based decision support, European Journal of Operational Research 99 (1) (1997)
48–57.
[32] Z. Pawlak, Rough classiﬁcation, International Journal of Man–Machine Studies 20 (5) (1984) 469–483.
[33] H. Ralambondrainy, A conceptual version of the K-means algorithm, Pattern Recognition Letters 16 (11) (1995) 1147–1157.
[34] E. Ruspini, A new approach to clustering, Information Control 15 (1) (1969) 22–32.
[35] T.-L. Tseng, M.C. Jothishankar, T. Wu, Quality control problem in printed circuit manufacturing – an extended rough set approach,
Journal of Manufacturing Systems 23 (1) (2004) 56–72.
[36] S. Tsumoto, Extraction of experts’ decision process from clinical databases using rough set model, in: Proceedings of the First
European Symposium, 1997, pp. 58–67.
[37] K. Voges, N. Pope, M. Brown, Cluster analysis of marketing data examining on-line shopping orientation: a comparison of K-means
and rough clustering approaches, in: H. A Abbass, R. A Sarker, C.S. Newton (Eds.), Heuristics and Optimization for Knowledge
Discovery, Idea Group Publishing, Hershey, PA, 2002, pp. 207–224.
[38] K. Voges, N. Pope, Generating compact rough cluster descriptions using an evolutionary algorithm, in: K. Deb (Ed.), GECCO2004:
Genetic and Evolutionary Algorithm Conference – LNCS, Springer-Verlag, Berlin, 2004, pp. 1332–1333.
[39] K. Wong, D. Feng, S. Meikle, M. Fulham, Segmentation of dynamic pet images using cluster analysis, IEEE Transactions on Nuclear
Science 49 (1) (2002) 200–207.
[40] S. Wu, A. Liew, H. Yan, M. Yang, Cluster analysis of gene expression data based on self-splitting and merging competitive learning,
IEEE Transactions on Information Technology in BioMedicine 8 (1) (2004) 5–15.
[41] Y. Zhang, A. Fu, C. Cai, P. Heng, Clustering categorical data, in: Proceedings of the 16th International Conference on Data
Engineering, 2000, pp. 305–324.
[42] W. Ziarko, Rough sets for intelligent image processing, in: Proceedings of the International Workshop on Rough Sets and Knowledge
Discovery, 1993, pp. 399–410.
Darshit Parmar is a senior consultant in the Supply Chain Practice of IBM Global Services Group. He holds a
MS degree in Industrial Engineering from Arizona State University and is currently working on his Ph.D. His
research interests include Sense and Respond Supply Chain, Data Mining, Predictive Modeling and
Optimization.

Teresa Wu (teresa.wu@asu.edu) is an associate professor in Industrial Engineering Department of Arizona
State University. She has published papers in International Journal of Concurrent Engineering: Research and
Application, International Journal of Product Research, ASME Transactions: Journal of Computing and
Information Science in Engineering, the Journal of Operations Management. She received her Ph.D. from the
University of Iowa. Her main areas of interests are in supply chain management, distributed decision support
and information systems.

D. Parmar et al. / Data & Knowledge Engineering 63 (2007) 879–893

893

Jennifer Blackhurst is an Assistant Professor of Logistics and Supply Chain Management at Iowa State
University. She received her Ph.D. in Industrial Engineering from the University of Iowa. Her current research
interests include: supply chain risk and disruptions; supply chain coordination; and supplier assessment.
Professor Blackhurst has articles published (or accepted) in such journals as Production and Operations
Management Journal, Decision Sciences Journal, Journal of Operations Management, International Journal of
Production Research, Omega, and Supply Chain Management Review. She serves on the Editorial Review
Board for Decision Sciences and is a member of DSI and POMS.

Computerized Medical Imaging and Graphics 38 (2014) 348–357

Contents lists available at ScienceDirect

Computerized Medical Imaging and Graphics
journal homepage: www.elsevier.com/locate/compmedimag

Prediction of near-term risk of developing breast cancer using
computerized features from bilateral mammograms
Wenqing Sun a , Bin Zheng b , Fleming Lure a , Teresa Wu c,d , Jianying Zhang e ,
Benjamin Y. Wang f , Edward C. Saltzstein g , Wei Qian a,d,∗
a

Department of Electrical and Computer Engineering, University of Texas, El Paso, TX, United States
School of Electrical and Computer Engineering, University of Oklahoma, Norman, OK, United States
c
School of Computing, Informatics, Decision Systems Engineering, Arizona State University, AZ, United States
d
Sino-Dutch Biomedical and Information Engineering School, Northeastern University, Shenyang, China
e
Department of Biology, University of Texas, El Paso, TX, United States
f
Radiology Department, Sierra Providence Health Network, El Paso, TX, United States
g
University Breast Care Center at the Texas Tech University Health Sciences, El Paso, TX, United States
b

a r t i c l e

i n f o

Article history:
Received 10 May 2013
Received in revised form
27 December 2013
Accepted 3 March 2014
Keywords:
Breast cancer
Mammogram
Computerized breast cancer risk analysis
Bilateral mammographic asymmetry
feature
Near-term breast cancer risk assessment

a b s t r a c t
Asymmetry of bilateral mammographic tissue density and patterns is a potentially strong indicator of
having or developing breast abnormalities or early cancers. The purpose of this study is to design and
test the global asymmetry features from bilateral mammograms to predict the near-term risk of women
developing detectable high risk breast lesions or cancer in the next sequential screening mammography examination. The image dataset includes mammograms acquired from 90 women who underwent
routine screening examinations, all interpreted as negative and not recalled by the radiologists during the original screening procedures. A computerized breast cancer risk analysis scheme using four
image processing modules, including image preprocessing, suspicious region segmentation, image feature extraction, and classiﬁcation was designed to detect and compute image feature asymmetry between
the left and right breasts imaged on the mammograms. The highest computed area under curve (AUC) is
0.754 ± 0.024 when applying the new computerized aided diagnosis (CAD) scheme to our testing dataset.
The positive predictive value and the negative predictive value were 0.58 and 0.80, respectively.
© 2014 Elsevier Ltd. All rights reserved.

1. Introduction
Breast cancer is the most common cancer and second leading cause of cancer deaths of women [1]. Scientiﬁc evidence has
shown that early cancer detection is important to enhance the
survival rates of the patients through more effective patient management and treatment [2,3]. Since the majority of breast cancers
are detected in women with no known risk factors deﬁned in
the existing epidemiology models [4,5], a uniform mammography
screening program in the general population is currently applied
and considered important [3]. However, due to the large variability in the depiction of breast abnormalities, the overlapping dense

∗ Corresponding author at: Department of Electrical and Computer Engineering,
Medical Imaging Informatics Laboratory, College of Engineering, University of Texas,
El Paso, 500 West University Avenue, El Paso, TX 79968, United States.
Tel.: +1 915 747 8090; fax: +1 915 747 7871.
E-mail address: wqian@utep.edu (W. Qian).
URL: http://ee.utep.edu/facultyqian.htm (W. Qian).
http://dx.doi.org/10.1016/j.compmedimag.2014.03.001
0895-6111/© 2014 Elsevier Ltd. All rights reserved.

ﬁbroglandular tissue on the projection images and the low cancer
prevalence in the screening environment, both detection sensitivity and speciﬁcity of screening mammography are relatively low
[6–8]. To help radiologists improve detection and diagnosis performances in reading and interpreting screening mammograms,
a great amount of research has been conducted to develop CAD
systems or schemes including our work on developing a variety
of two-dimensional computerized image analysis algorithms optimized to enhance the performance of the traditional CAD systems
during the last two decades (e.g., [9–13]). Currently, a number of
commercialized CAD schemes, including the one originally developed in our group and then being licensed to Carestream Health,
Inc. [14,15], are widely used in the clinical practice to assist radiologists in reading and interpreting mammograms to date.
However, a number of recently reported studies made the
debate related to the efﬁcacy (risk-beneﬁt and cost-beneﬁt) of
population-based screening mammography more controversial
[16,17]. Although reducing mammography screening interval (e.g.,
from annual screening to screening every two or more years) has
the risk of missing early cancers, the currently frequent X-ray

W. Sun et al. / Computerized Medical Imaging and Graphics 38 (2014) 348–357

mammography screening and the associated high false-positive
recall rates contribute to higher cost as well as the unnecessary
harms to many cancer-free women who routinely participate in the
recommended mammography screening. As a result, since more
scientists or researchers have realized the signiﬁcance of establishing a more effective risk evaluation system to improve the
accuracy of early breast cancer risk prediction, shifting the current
population-based breast cancer screening paradigm to a new and
optimal personalized screening paradigm in which women should
be screened differently (including both the screening interval and
screening imaging methods) has been attracting wide research
interest [18,19,10,20,21]. To realize this ultimate goal, it is required
to develop and establish breast cancer risk prediction or stratiﬁcation models that have signiﬁcantly higher (clinically acceptable)
discriminatory power or positive predictive value (PPV) to stratify women into high and low risk groups of having or developing
breast cancer in the near-term (e.g., ≤5 years). Hence, a small group
of high risk women should be more aggressively screened and monitored for the early detection of breast cancer, while the majority of
low risk women can be screened in a longer interval to reduce the
potential harm (i.e., false-positive [22] and added cancer or other
health risk [23]) until their risk statuses are changed. Unfortunately,
all existing epidemiology based breast cancer risk models [5,24]
do not have clinically acceptable accuracy for this purpose [25].
As a result, much innovative research work is needed to develop
the new near-term risk models that are potentially clinically useful or acceptable for breast cancer screening purpose. Among the
different efforts, previous studies have shown bilateral mammographic image feature asymmetry, the difference of imaged breast
size [26] and average density between the left and right breast [27].
This might be a strong risk indicator of developing breast cancer in
the near-term (e.g., the next sequential screening examinations)
because such image phenotype variation may associate with genotype abnormality to break human natural bilateral symmetry in
the paired morphological traits, including breasts, which may lead
to cancer development. Instead of using registration or alignment
techniques [27], designed global features from the whole breast
area as the measurement of bilateral mammogram asymmetry.
Thus the potential misalignment and poor registration between
bilateral mammograms results from the differences in breast compression or positioning and changes in breast itself [28] are avoided.
However, to the best of our knowledge, all the global asymmetry
features are constraint in spatial feature domain, no global morphological features or textural features of asymmetry measurement are
discussed in existing literatures.
In this study, the global morphological and textural asymmetry
features are designed and we also investigate and test the possibility of converting and/or applying the traditional CAD schemes
to breast cancer risk analysis schemes using the current negative
mammograms (e.g., the negative mammograms acquired in the
“prior” examinations) to predict the likelihood of women having
high-risk breast lesions or cancers being detected in the near-term
based on the computerized image feature analysis from the bilateral (left and right breast) mammograms. Our hypothesis is that
the bilateral image feature difference computed from the left and
right breasts should also provide useful information indicating the
asymmetry of breast tissue structures that might be directly related
to the development of high risk breast lesions.

2. Materials and methods
2.1. Database
From an established in-house full-ﬁeld digital mammography
(FFDM) image database, we randomly selected 90 pairs of bilateral

349

Table 1
Distribution of our testing dataset.
Amount

CC
MLO
Total

Case
Results in ﬁrst
mammography exam

Results in second
mammography exam

Normal

Stay normal

Turn abnormal

60
30
90

39
17
56

21
13
34

mammograms, including 60 pairs of cranio-caudual (CC) view and
30 pairs of Mediolateral-oblique (MLO) view cases from 90 women
who underwent routine screening mammography examinations.
The women’s ages range from 32 to 64, with a mean age of 43.7
years and median age of 43. Among the data, 4 cases, 28 cases, 49
cases and 9 cases were respectively rated by radiologists as almost
entire fatty (BIRADS I), scattered ﬁbro-glandular (BIRADS II), heterogeneously dense (BIRADS III) and extremely dense (BIRADS IV).
All of these mammograms (treated as “prior” mammograms in this
study) were interpreted as screening normal (without recall) by
the radiologists; however, in the next sequential (annual) FFDM
screening examinations, 34 of these women were recalled by radiologists due to the highly suspicious ﬁndings depicted on the FFDM
images. Through additional imaging examinations and/or biopsies,
20 women were diagnosed and conﬁrmed having cancer. The rest of
the 56 women remaining screened negative (not recalled) during
the sequential FFDM examinations (as shown in Table 1). In this
study, 90 pairs of left and right FFDM images acquired from the
“prior” negative examinations were used and analyzed. The goal
of this study is to develop a new computerized scheme to classify risk assessment between 34 high risk women who developed
abnormalities that were detectable by radiologists during the next
sequential screening examinations and 56 low-risk women who
remained cancer-free and were not recalled by radiologists in the
next sequential screening.
2.2. Breast cancer risk analysis scheme
The proposed breast cancer risk analysis scheme was developed to detect the cases with high possibilities of having highly
suspicious breast abnormalities detected in the next sequential
screening examinations. Speciﬁcally, the scheme was divided into
four primary image processing and data analysis modules, including (1) image preprocessing, (2) region segmentation, (3) feature
extraction and selection and (4) machine learning classiﬁcation.
Fig. 1 shows the ﬂowchart of the proposed scheme.
2.2.1. Preprocessing module
In order to enhance image features and reduce image noise, we
implemented the following three image preprocessing functions
that could have impact on the following procedures as well as the
ﬁnal analysis results. These are (1) concurrent image enhancement;
(2) concurrent multiorientation transform; and (3) concurrent multiresolution transform.
First, the image enhancement eliminates noise and artifacts by
using adaptive tree-structured nonlinear ﬁltering (TSF). In order to
maximize the effect of TSF, the standardization method was used
to convert the mammogram to match our algorithm [29]. Then we
applied a central weighted median ﬁlter (CWMF) method and eight
different variable ﬁlter windows to match different edge signals
related to the breast tissues [30]. Compared to the preprocessing
module in the single-view system, using either the right side mammogram or the left side mammogram, we need to process each pair
of bilateral images at the same time and make sure each pair of corresponding parameters is the same. The CMWF and variable ﬁlter

350

W. Sun et al. / Computerized Medical Imaging and Graphics 38 (2014) 348–357

Fig. 1. Flowchart of breast cancer risk analysis scheme using global asymmetry features from bilateral mammograms.

windows are able to decrease or suppress noise and increase details
of breast tissues. Then, the TSF architecture integrated the designed
symmetric ﬁlters and variable shape windows into a three-stage
system [31]. This is a symmetric multistage ﬁlter which combined
the advantages of central weighted median ﬁlters, linear and curved
windows, and multistage operations that sequentially compare ﬁltered and raw image data with the objective of obtaining more
robust characteristics for noise suppression and detail preservation. These steps resulted in spatial feature domain images of both
the left and right breasts for later use in bilateral spatial feature
extraction.
Second, directional wavelet transform (DWT) was used for concurrent multiorientation signal decomposition. The input bilateral
images were decomposed by the directional wavelet transform
and each image yielded two output images. Every case (bilateral
mammograms) resulted in another four images: two are directional
texture images (textural domain) which were used for directional
feature extraction, and the other two were smoothed versions of
the original images (morphological images), with directional information removed [32]. The importance of this step has already been
shown in our previous breast cancer detection research, and even
more necessary for this new application because (1) the directional
feature is a good indicator of early cancer risk analysis in which the
subtle abnormalities in early stage might be difﬁcult to detect in
early stages and (2) the processed resulting image makes it possible
to quantitatively analyze spiculations for potential abnormalities.
Third, the tree structured wavelet transform (TSWT) function
was designed as an efﬁcient concurrent multi-resolution enhancement method. A two-channel tree-structured wavelet transform
was used here as a highly efﬁcient multi-resolution representation
method, as opposed to single-scale region-based image enhancement approach for suspicious areas. It combines the advantages of
lower resolution characteristics, which are useful for localization of
suspicious areas, with the advantages of higher resolution which is
essential for details [32].
After applying this image preprocessing module, the proposed
scheme generates 3 images for each mammogram including spatial
image, texture image and morphology image [33] (Fig. 2).

2.2.2. Segmentation module
In this module, morphology images and texture images were
processed with the purpose being to detect and segment the suspicious areas depicted on each image. We designed a spatial fuzzy
C-means (FCM) clustering algorithm that works by grouping similar
pixels into clusters in the feature domain. Having applied the ﬁlters,
ROIs (region of interest) containing suspicious tissue structures
or high-risk breast lesions were extracted from the preprocessed
images, and the number of clusters were chosen by the ﬁlter results.
Compared to conventional fuzzy clustering method, we integrated the spatial information in our approach. As the pixels on
the images are highly correlated, the spatial information is of great
importance to be considered in the process of clustering. We sum
the intensity membership value of each pixel in a 5 by 5 sized window as a spatial function, with the spatial function is deﬁned as
hij =



uij /n2 ,

n×n

where uij is the membership value of each pixel. The value will
be relatively larger if the majority of its neighborhood belongs to
the same cluster [34]. By incorporate the spatial function into the
membership function we get:
p q

uij

=

uij hij

c

p q
u h
k=1 kj kj

,

where p and q are the weights of the membership function and
spatial function.
Our clustering loop contains two steps namely the cluster
assignment step and move centroid step. In order to avoid local
optimum, the fuzzy C-means algorithm runs iteratively to calculate the cost function each time so that the best clustering can
gradually emerge and be selected. Then, an AND rule in fuzzy
logic is used to combine intensity membership and spatial membership together. The reduced resolution image was used in the
FCM method, since lower resolution images usually contain more
homogenous regions. After the clusters in lower resolution images

W. Sun et al. / Computerized Medical Imaging and Graphics 38 (2014) 348–357

351

Fig. 2. An example of original image and other three images obtained from preprocessing module. (a) Original mammogram, (b) spatial image, (c) texture image and (d)
morphology image.

are labeled, the clusters with the same labels were then merge
into high resolution images. As a result, the smoothed regions and
detailed edges can be segmented and acquired at the same time. The
ﬁnal clustering is achieved iteratively by calculating cost function
that is dependent on the distance of the pixels to the cluster centers in the feature domain [34]. The ﬂowchart is shown in Fig. 3, and
Fig. 4 shows the segmentation results marked on original images
based on our spatial fuzzy C-means method.
2.2.3. Feature computation and analysis
The third module of the proposed breast cancer risk analysis
scheme computes and analyzes the related image features. We
divided the features into two groups, one with unilateral features (based on single breast mammogram), and the other with
concurrent features (based on both breast mammograms). For
convenience, all the features and their groups are presented in
Appendix A. Unilateral features depict the information of every
single breast mammogram, and concurrent features describe the
relationship of the bilateral mammograms.
2.2.3.1. Unilateral features. For unilateral feature analysis, the proposed scheme computed the following groups of features including
(1) morphological features: circularity, normalized deviation of
radial length, the area of the extracted region in pixels, boundary
irregularity factor; (2) textural features: the number of spiculations,
the average spiculation length. The description of these features can
be found in our previous papers [11–13].

In addition, we also investigated and used three texture features
[35,36]. These features were computed from the images generated
by our modiﬁed multi-orientation transform using the following
approaches.
(1) Gabor Energy Feature. This feature combines the outputs of symmetric and anti-symmetric
kernel ﬁlter together and deﬁned as:

e, (x, y) =



2
2
r,,0
(x, y) + r,,−(1/2)
(x, y) where r(x, y) =

I(, )g(x − , y − )dd. r,,0 (x, y) and r,,−(1/2) (x, y)
are the responses of the linear symmetric and anti-symmetric
Gabor ﬁlters, respectively. And these are standard formulas
2

2 2

2







of Gabor ﬁlters: g,,ϕ (x, y) = e−((x + y )/2	 ) cos 2 x + ϕ ,
x = x cos  + y sin , y = −x sin  + y cos , 	 = 0.56 and
 = 0.5.
(2) Complex Moments Feature. This feature provides the information about the presence or absence of dominant
texture orientations by using real and imaginary parts
moments of the local power spectrum.
of the complex

m
n
Cmn (x, y) =
(u + iv) (u − iv) p̃u,v (x, y)dudv, m, n ∈ N where
u = 1 cos 
, v = 1 sin , p̃u,v (x, y) = p, (x, y). It is proven that
a complex moment of even order m + n is able to discriminate
textures with (m + n)/2 dominant orientations [35].
(3) Grating Cell Operator Feature. As discussed in [35,36], grating
cell operator contains two steps. The ﬁrst step is to respond
at any position to a set of three parallel bars with appropriate
periodicity, orientation and position. The second step integrates

352

W. Sun et al. / Computerized Medical Imaging and Graphics 38 (2014) 348–357

Fig. 3. Flowchart of a spatial fuzzy C-means method.

the responses of a given preferred orientation and periodicity
within a certain area to compute the response of a grating cell.
(4) Alone with the morphological and textural features, a set of 9
spatial image features were also computed as unilateral features: (1) energy, (2) entropy, (3) inertial, (4) mean value,
(5) standard deviation, (6) skewness, (7) kurtosis, (8) smoothness and (9) mean gradient.

2.2.3.2. Concurrent features. The concurrent features represent the
degree of bilateral mammogram asymmetry. In this step no
alignment or registration between two bilateral breast images is
required, as all the features we designed are global features. Hence,
for concurrent feature analysis, the features were acquired by calculating the absolute different ratios of certain measurements using
features from each corresponding bilateral mammogram. Then

Fig. 4. The segmentation result using spatial fuzzy C-means method. For better illustration, the segmentation example is implemented on the mammogram with malignant
mass tissue (a) Original image, (b) enhanced image and (c) segmentation result.

W. Sun et al. / Computerized Medical Imaging and Graphics 38 (2014) 348–357

using the measurements listed below, we computed the following
bilateral asymmetric features from each paired mammograms. In
brief, based on the gray level histogram of the entirely segmented
breast area six features (1–6) are computed: (1) the ratio between
the number of pixels with the maximum gray value in the histogram and the total number of pixels of the whole breast; (2) the
ratio between the number of pixels with a gray value larger than
the average value of the histogram and the total number of pixels of
the breast; (3) the average difference of two adjacent values in the
histogram; (4) the average value of the histogram; (5) the standard
deviation of the histogram; and (6) the uniformity of intensity in
the histogram. The rest of the seven features were computed to
mimic the density BIRADS rated by radiologists based on threshold
computation. Features (7–9) are the ratios between the average
value of pixels under thresholds of 25%, 50%, and 75% of the maximum pixel value and the average pixel value of the whole breast
area. Features (10–12) are the ratios between the number of pixels
under each threshold and the total number of pixels of the entire
segmented breast area. Feature 13 is the ratio between subtraction
of the average value of pixels under 75% threshold minus average value of whole breast area and subtraction of average value of
whole breast area minus average value of pixels under 25% threshold. Finally, feature 14 is the ratio of difference between number
of pixels under 75% threshold and total number of pixels of whole
breast area and subtraction of whole pixel number minus number
of pixels under 25% threshold.
Besides computing image features from spatial domain, we also
designed and computed bilateral asymmetry features in the textural domain and morphological domain. To our knowledge, no
publication has discussed global bilateral textural and morphological features. However, from our research we found that some
benign cases in “prior” examination also have certain suspicious
dense areas, like lobular hyperplasia or benign masses, and the
asymmetry of these areas might also provide useful information
for risk cancer development. If no suspicious areas can be found
in the mammogram, the dense areas will be extracted. All these
areas mentioned above were called suspicious areas in this study.
Unlike the suspicious region deﬁned by radiologists (on average
much less than one per typical mammogram), the suspicious areas
deﬁned here indicate the regions identiﬁed in initial processing
steps, which is as many as 839 from all the 90 pairs of cases.
Instead of struggling to avoid misalignment of bilateral corresponding areas, we treat the entire segmented suspicious areas depicted
on one image as a whole, and the average suspicious region measurement of each bilateral mammogram was used in our study.
Listed below are the global features we designed in the paper:
1. Ratio of total number of suspicious areas in each mammogram.
2. Ratio of total areas of extracted ROIs in pixels in each mammogram.
3. Ratio of average boundary irregularity factor of ROIs in each
mammogram.
4. Ratio of average circularity of ROIs in each mammogram.
5. Ratio of average rectangularity of ROIs in each mammogram.
6. Ratio of number of spiculations in each mammogram.
7. Ratio of the average spiculation length in each mammogram.
8. Ratio of Gabor energy feature in each mammogram.
9. Ratio of complex moments feature in each mammogram.
10. Ratio of cell operator feature in each mammogram.
11. Ratio of entropy of suspicious areas in each mammogram.
12. Ratio of the standard deviations of spiculations in each mammogram.
2.2.3.3. Feature selection and classiﬁcation. After computing these
features in different image feature domains, we analyzed the correlations among these features, deleting redundant features and

353

building an optimal feature set. Based on Darwin’s theory of evolution, all living things are based on the rule of “survival of ﬁttest”,
by translating this into algorithms we search for the solutions in
a more natural way, using the Genetic Algorithms (GA). GA search
from a broad spectrum of possible solutions to ﬁnd the optimal feature set [37]. It is a search technique that evaluates each individual’s
ﬁtness through a ﬁtness function. In this study, the ﬁtness function
for every chromosome is fitness(m) = Az − N(m)/2000, where Az is
the performance from previous selected feature pool and N(m) is
the number of selected features in chromosome m. We ﬁrst calculated the ﬁtness value with initial population, and then selected the
individuals with relatively high values for mutation to produce the
next generation. The crossover probability and mutation probability is 0.9 and 0.001, respectively. The process ended when the ﬁnal
condition satisﬁed. In order to solve the problem that the elimination of one feature will affect the elimination of the next feature,
a global search technique was applied in our study [38]. The best
set of features to be used in classiﬁcation was selected by testing
every possible combination of candidate features. To achieve this
goal we started with a random population that converged when all
members in the population had approximately the same ﬁtness.
Having selected the features, the Support Vector Machine (SVM)
method was used to classify and/or predict the likelihood of the
test cases developing high risk breast lesions detectable in the next
sequential FFDM screening examinations in our database. Compared with many other machine learning classiﬁers, SVM uses
unique characteristics. Alone with faster training time, it uses large
margin classiﬁers allowing the SVM to maximize the distance of the
training examples to the decision boundaries. Using a well optimized SVM training software package [39], the global minimum
or optimal result could be determined relatively easy and reliably
while substantially reducing the risk or the possibility of being
trapped inside the local minimum. Limited by our dataset size, a
10-folder cross-validation method was used to train and test our
SVM. We used nine folds as training data and the remaining one
fold as the validation data. This process was iteratively repeated
10 times until each fold was used once as the validation data. This
10-fold cross-validation method made our data set more efﬁcient
as every mammogram can be used for both training and validation.
After obtaining the SVM classiﬁcation results for all testing cases, we applied receiver operating characteristic (ROC)
data analysis methods to assess CAD performance in predicting the probabilities of each case developing high risk breast
lesions in the next sequential examinations. The area under
the ROC curve computed using the ROCKIT program (ROCKIT,
http://www-radiology.uchicago.edu/krl/) was used as the performance evaluation index. In addition, by maximizing the margin
length from SVM classiﬁcation, the dataset was divided into two
subgroups of positive and negative cases. The positive and negative predictive value were also assessed and reported. All testing
results were tabulated for comparison.

3. Results
To evaluate the efﬁciency of our proposed features, we applied
each of the 12 global bilateral symmetric features to classify the
cases in our dataset into two groups of positive and negative for
development of breast abnormalities or early cancer. The average performance level measured by AUC is 0.5933 ranging from
0.5316 ± 0.0303 to 0.6904 ± 0.0231 (Fig. 5). All twelve of our proposed global asymmetry features yielded an AUC more than the
chance (AUC = 0.5), including 5 of them achieving AUC more than
0.6. This means all these features have certain ability to assess the
bilateral breast asymmetry and hence identify a fraction of positive
cases.

354

W. Sun et al. / Computerized Medical Imaging and Graphics 38 (2014) 348–357

Fig. 5. The distribution of the performance levels (the areas under receiver operating
characteristic (ROC) curves and the standard deviations) when applying each of the
proposed 12 features to classify 90 cases into the two groups of positive and negative
for developing breast abnormalities or cancer.

All the features mentioned in Section 2.2.3 (including all the
existing features and newly designed 12 global features) were used
for GA feature selection, the following features were selected to
build the classiﬁer used in our CAD scheme: (1) the standard deviation of single pixel values of breast area; (2) the skewness of pixel
values of single breast area; (3) the ratio between the number of
pixels with the maximum gray value in the histogram and the total
number of pixels of the whole breast; (4) the standard deviation
of the histogram; (5) the ratio of average boundary irregularity
factor; (6) the ratio of average circularity; (7) the ratio of number of spiculations. The formulations of the selected features are
presented in Table 2. Due to the relatively low correlations among
these seven features (Table 3), by combining these features the
SVM yielded a signiﬁcantly improved classiﬁcation performance

Table 2
The formulations of selected features.
Feature number

Formulation

1

	
=



i,j

2
3

i,j

(x−)2

N
(x−)3

N	 3
Nmax

Ntotal


m

1/(m − 1)

4

fright

5

i=1

(Hi − H )

Table 4
Summary of using our CAD scheme to stratify our testing cases into the positive
(high risk) and negative (low risk) groups based on analysis of all features except our
newly designed features (traditional features) and all features (traditional features
and new features).

clleft
nright

7

nleft

Features selected

Table 3
The correlation coefﬁcients of the selected seven features.
Feature number
1
2
3
4
5
6

2
0.246

3
0.084
0.061

4
0.281
0.253
0.174

with AUC = 0.754 ± 0.024 (P < .001). The statistical analyses were
performed using SAS software (SAS Institute, Cary, NC).
To compare the performance of CC view mammograms and MLO
view mammograms, we applied our algorithm to CC view data and
MLO view data respectively and with AUC values (SVM selected
as the classiﬁer) being 0.762 ± 0.027 (CC view) and 0.7380 ± 0.026
(MLO view). The ROC curves are shown in Fig. 6. The truth ﬁles
were acquired from the diagnostic results of the next sequential
FFDM examinations and/or the biopsy (for the recalled and positive cases). Table 4 shows that among 34 positive high risk cases
(from the truth ﬁle), 25 (73.5%) were classiﬁed or predicted as
having high risk of developing detectable cancer breast lesions
in the next sequential screening examinations among 56 benign
cases, 38 (67.9%) were classiﬁed as low risk cases of developing
detectable cancers in the next sequential screening examinations,
which means that more than two thirds of the cases, 70.0% (63 out
of 90 cases), were predicted correctly. In this dataset, the positive
predictive value (PPV) and the negative predictive value (NPV) were
0.58 and 0.80, respectively.
By removing all global asymmetry morphological and textural
features (i.e. the 12 features we proposed) from the feature pool, we
reselected the features and rebuilt the SVM in the same way using
the spatial features only. The performance measured by the computed AUC went down to 0.701 with standard deviation of 0.029
and 95% conﬁdence interval of [0.672, 0.730]. The PPV and NPV
when using spatial image feature asymmetry resulted in 0.50 and

2

fleft
clright

6

Fig. 6. Comparison of three ROC curves generated by only MLO view cases, only CC
view cases and both view cases.

5
0.124
0.106
0.096
0.067

6
0.049
0.065
0.086
0.093
0.155

Results by
our system

Total

Low risk

High risk

Traditional features and new

Positive
Negative

18
38

25
9

43
47

Traditional features only

Positive
Negative

22
34

22
12

44
46

56

34

90

7
0.056
0.072
0.088
0.081
0.176
0.269

Truth ﬁle

Total

W. Sun et al. / Computerized Medical Imaging and Graphics 38 (2014) 348–357

0.74, respectively. The comparison in Table 4 shows that using our
CAD scheme by adding morphological and textural feature asymmetry computed from two bilateral mammograms could improve
the performance of the scheme by increasing both PPV and NPV
rate.
4. Discussion
In this study, we investigated and presented a new application
value of the CAD schemes of mammograms, which aims to identify a fraction of women with high risk of having or developing
detectable breast cancer. Computerized asymmetrical image feature analysis of two bilateral (left and right) mammograms is made
available in the near-term after the negative screening examination
of interest. This study has a number of unique characteristics. First,
subtracting the registered bilateral mammograms to identify suspicious abnormalities based on image feature difference. This has
been investigated and tested in early CAD development without
much success because these CAD schemes typically yield substantially high false-positive detection rates and often unreliable results
[40–44]. Hence, the biggest obstacle to compute and analyze the
image feature differences on the two bilateral mammograms using
CAD scheme is that the accurate image alignments or registration is
required, which has been proven to be very difﬁcult and not robust
leading to the high false positive detection rates. Due to the complexity of breast cancers, misalignment is almost unavoidable. In
this study, we adopted a different approach that did not require
image registration while extracting asymmetry features from bilateral mammograms. Our scheme directly compared the differences
of image features that were separately computed from two bilateral
images.
Second, although using computerized global spatial or statistical
image feature difference from two bilateral images to predict nearterm breast cancer risk has been previously tested and reported
[27,45], the disadvantage of these studies is that only the spatial
features were used to analyze the cases while ignoring morphological features and texture features. In this study, we tested a new
method that utilized features extracted from three domains to try
to enhance performance. This work also aimed to add a new function to an existing CAD scheme to allow for not only detect the
suspicious lesions in the positive cases but also predict the nearterm risk of the negative cases. Our research showed that adding
the morphological and texture features currently used to classify
the suspicious lesions or regions to the process of risk prediction
could increase prediction performance (as comparison shown in
Table 4). An increase of 0.053 in Az value, 0.08 in PPV rate, and 0.06
in NPV rate were observed after adding our proposed features to the
scheme. Hence, in this approach the well-developed image feature
analysis methods used in current CAD schemes can be applied to
the dual tasks of detecting suspicious breast lesions and predicting

Groups

355

future cancer risk. In this study, as we deliberately selected some
features in morphological and texture domains and mixed them
with other features computed from the spatial feature domain, our
scheme was able to correctly predict 73.5% of cases that would have
detectable cancer in the next sequential screening examinations. To
the best of our knowledge, no similar studies have been reported
to date.
Third, breast cancers occur in women with variety of image
characteristics. Although current CAD performance depends on the
case difﬁculty (e.g., breast density) [44], our approach is largely
independent from these case difﬁcult factors or varying image characteristics based on the assumption that two breasts of negative
cases are relatively symmetrical. Similar to the most existing CAD
schemes, our scheme is also likely to detect more suspicious regions
on both left and right dense breasts and few suspicious regions in
two fatty tissue dominated breasts. It is the tissue asymmetry that
determines the risk of a woman having or developing breast cancer. Hence, this approach should be more robust than the current
CAD schemes in detecting suspicious breast abnormalities based
on single images.
Despite these encouraging results, this is a preliminary study
with a number of limitations. In particular, our testing dataset
was very small, which is not sufﬁcient to represent the general
screening population seen in clinical practice. Thus, the robustness
and generalizability of the reported results need to be further tested
in future studies.
In summary, developing new near-term breast cancer risk
assessment models that enables the stratiﬁcation of women into
two groups of high and low risk. The high discriminatory power
(the clinically acceptable positive and negative predictive values)
is extremely important to help develop and establish an optimal
personalized breast cancer screening paradigm. In this study we
tested a new approach that analyzes the bilateral mammographic
image feature differences in three feature domains without image
registration or alignment. This allows us to predict the likelihood
of a woman developing breast cancer in near-term after a negative
screening examination of interest. The results showed that it is feasible to apply our modiﬁed CAD scheme to predict the near-term
breast cancer risk in a relatively large fraction of high risk cases.
If successful, the prediction results from this or other similar new
CAD-based breast cancer risk models may be used to raise a warning
ﬂag. Proper use of these methods assist radiologists in reducing the
possibility of missing or overlooking these cases in the next sequential examinations or advise the women being more aggressively
screened (e.g., using a short screening interval and other more sensitive imaging modalities) to increase the likelihood of the cancer
being detected in an early stage.
Appendix A. The feature groups and descriptions are
presented below

Feature descriptions
Morphological features

Unilateral features
Textural features
Spatial features

Circularity, normalized deviation of radial length, the area of the extracted region in pixels, boundary
irregularity factor
The number of spiculations, the average spiculation length, Gabor energy feature, complex moments feature,
grating cell operator feature
Energy, entropy, inertial, mean value, standard deviation, skewness, kurtosis, smoothness, mean gradient

356

W. Sun et al. / Computerized Medical Imaging and Graphics 38 (2014) 348–357

Appendix A (Continued)
Groups

Feature descriptions

Spatial features

Concurrent features

Textural and
morphological features

Ratio between the number of pixels with the maximum gray value in the histogram and the total number of
pixels of the whole breast
The ratio between the number of pixels with a gray value larger than the average value of the histogram and
the total number of pixels of the breast
Average difference of two adjacent values in histogram
Average value of histogram
Standard deviation of the histogram
Uniformity of intensity in the histogram
Ratios between the average value of pixels under threshold of 25% of the maximum pixel
Value and the average pixel value of the whole breast area
Ratios between the average value of pixels under threshold of 50% of the maximum pixel value and the
average pixel value of the whole breast area
Ratios between the average value of pixels under threshold of 75% of the maximum pixel value and the
average pixel value of the whole breast area
Ratios between the number of pixels under 25% threshold and the total number of pixels of the entire
segmented breast area
Ratios between the number of pixels under 50% threshold and the total number of pixels of the entire
segmented breast area
Ratios between the number of pixels under 75% threshold and the total number of pixels of the entire
segmented breast area
Ratio between subtraction of average value of pixels under 75% threshold minus average value of whole breast
area and subtraction of average value of whole breast area minus average value of pixels under 25% threshold
Ratio of difference between number of pixels under 75% threshold and total number of pixels of whole breast
area and subtraction of whole pixel number minus number of pixels under 25% threshold
Ratio of total number of suspicious areas in each mammogram
Ratio of total areas of extracted ROIs in pixels in each mammogram
Ratio of average boundary irregularity factor of ROIs in each mammogram
Ratio of average circularity of ROIs in each mammogram
Ratio of average rectangularity of ROIs in each mammogram
Ratio of number of spiculations in each mammogram
Ratio of the average spiculation length in each mammogram
Ratio of Gabor energy feature in each mammogram
Ratio of complex moments feature in each mammogram
Ratio of cell operator feature in each mammogram
Ratio of entropy of suspicious areas in each mammogram
Ratio of the standard deviations of spiculations in each mammogram

References
[1] Siegel R, Naishadham D, Jemal A. Cancer statistics, 2012. CA Cancer J Clin
2012;62(1):10–29.
[2] Smith RA, Cokkinides V, Brooks D, Saslow D, Shah M, Brawley OW. Cancer
screening in the United States, 2011. CA Cancer J Clin 2011;61(1):8–30.
[3] Madigan MP, Ziegler RG, Benichou J, Byrne C, Hoover RN. Proportion of breast
cancer cases in the United States explained by well-established risk factors. J
Natl Cancer Inst 1995;87(22):1681–5.
[4] Amir E, Freedman OC, Seruga B, Evans DG. Assessing women at high risk
of breast cancer: a review of risk assessment models. J Natl Cancer Inst
2010;102:680–91.
[5] Vachon CM, Sellers TA, Carlson EE, Cunningham JM, Hilker CA, Smalley
RL, Pankratz VS. Strong evidence of a genetic determinant for mammographic density, a major risk factor for breast cancer. Cancer Res 2007;67(17):
8412–8.
[6] Tang JS, Rangayyan RM, Xu J, Naqa EI, Yang YY. Computer-aided detection and
diagnosis of breast cancer with mammography: recent advances. IEEE Trans
Inf Technol Biomed 2009;13(2):236–51.
[7] Mandelson MT, Oestreicher N, Porter PL, White D, Finder CA, Taplin SH, White
E. Breast density as a predictor of mammographic detection: comparison of
interval-and screen-detected cancers. J Natl Cancer Inst 2000;92(13):1081–7.
[8] Berg WA, Gutierrez L, NessAiver MS, Carter WB, Bhargavan M, Lewis RS,
Ioffe OB. Diagnostic accuracy of mammography, clinical examination, US,
and MR imaging in preoperative assessment of breast cancer 1. Radiology
2004;233(3):830–49.
[9] Zheng BY, Qian W, Clarke LP. Digital mammograghy: MF-based NN for
automatic detection of microcalciﬁcations. IEEE Trans Med Imaging 1996:
589–98.
[10] Glide-Hurst CK, Duric N, Littrup P. A new method for quantitative analysis of
mammographic density. Med Phys 2007;34:4491–8.
[11] Qian W, Li LH, Clarke LP. Image feature extraction for mass detection using digital mammography: inﬂuence of wavelet analysis. Med Phys 1999;26:402–8.
[12] Qian W, Sun X, Song D, Clark RA. A novel hybrid ﬁlter architecture for image
enhancement and detection in digital mammography. Comput Med Imaging
Graph 2002;23:3–24, 2.
[13] Qian W, Song D, Lei M, Sankar R, Eikman E. Computer-aided mass detection
based on ipsilateral multiview mammograms. Acad Radiol 2007;14(5):530–8.
[14] Clarke LP, Qian W, Kallergi M. (1998). U.S. Patent No. 5,825,936. Washington,
DC: U.S. Patent and Trademark Ofﬁce.
[15] Clarke LP, Qian W, Li L. (1998). U.S. Patent No. 5,799,100. Washington, DC: U.S.
Patent and Trademark Ofﬁce.

[16] Berlin L, Hall FM. More mammography muddle: emotions, politics, science,
costs and polarization. Radiology 2010;255:311–6.
[17] Jorgensen KJ. Is the tide turning against breast screening. Breast Cancer Res
2012;14:107–9.
[18] Schousboe JT, Kerlikowske K, Loh A, Cummings SR. Personalizing mammography by breast density and other risk factors for breast cancer: analysis of health
beneﬁts and cost-effectiveness. Ann Intern Med 2011;155(1):10–20.
[19] Highnam R, Pan X, Warren R, Jeffreys M, Smith GD, Brady M. Breast composition
measurements using retrospective standard mammogram form (SMF). Phys
Med Biol 2006;51(11):2695.
[20] Chang YH, Wang XH, Hardesty LA, Chang TS, Poller WR, Good WF, Gur D. Computerized assessment of tissue composition on digitized mammograms. Acad
Radiol 2002;9(8):899–905.
[21] Timp S, Varela C, Karssemeijer N. Temporal change analysis for characterization
of mass lesions in mammography. IEEE Trans Med Imaging 2007;26(7):945–53,
http://dx.doi.org/10.1109/TMI.2007.
[22] Hubbard RA, Kerlikowske K, Flowers CI, Yankaskas BC, Zhu W, Miglioretti DL.
Cumulative Probability of False-Positive Recall or Biopsy Recommendation
After 10 Years of Screening MammographyA Cohort Study. Ann Intern Med
2011;155(8):481–92.
[23] Yaffe MJ, Mainprize JG. Risk of radiation-induced breast cancer from mammographic screening. Radiology 2011;258:98–105.
[24] Irwig L, Houssami N, van Vliet C. New technologies in screening for breast
cancer: a systematic review of their accuracy. Br J Cancer 2004;90:2118–22,
http://dx.doi.org/10.1038/sj.bjc.6601836.
[25] Gail MH, Mai PL. Comparing breast cancer risk assessment models. J Natl Cancer
Inst 2010;102:665–8.
[26] Scutt D, Lancaster GA, Manning JT. Breast asymmetry and preto
breast
cancer.
Breast
Cancer
Res
2006;8:R14,
disposition
http://dx.doi.org/10.1186/bcr1388.
[27] Zheng B, Sumkin JH, Zuley ML, Wang X, Klym AH, Gur D. Bilateral mammographic density asymmetry and breast cancer risk: a preliminary assessment.
Eur J Radiol 2012;81(11):3222–8.
[28] van Engeland S, Snoeren P, Hendriks JHCL, Karssemeijer N. A comparison of methods for mammogram registration. IEEE Trans Med Imaging
2003;22(11):1436–44.
[29] Qian W, Sankar R, Song X, Sun X, Clark R. Standardization for image characteristics in telemammography using genetic and nonlinear algorithms. Comput
Biol Med 2005;35(3):183–96.
[30] Qian W, Clarke LP, Kallergi M, Clark RA. Tree-structured nonlinear ﬁlters in digital mammography. IEEE Trans Med Imaging 1994;13(1):
25–36.

W. Sun et al. / Computerized Medical Imaging and Graphics 38 (2014) 348–357
[31] Qian W, Clarke LP, Li HD, Clark R, Silbiger ML. Digital mammography: Mchannel quadrature mirror ﬁlters (QMFs) for microcalciﬁcation extraction.
Comput Med Imaging Graph 1994;18(5):301–14.
[32] Qian W, Sun X, Song D, Clark RA. Digital mammography: wavelet transform and
Kalman-ﬁltering neural network in mass segmentation and detection. Acad
Radiol 2001;8:1074–82.
[33] Qian W, Li L, Clarke L, Clark RA, Thomas J. Digital mammography: comparison
of adaptive and nonadaptive CAD methods for mass detection. Acad Radiol
1999;6(8):471–80.
[34] Chuang KS, Tzeng HL, Chen S, Wu J, Chen TJ. Fuzzy c-means clustering with
spatial information for image segmentation. Comput Med Imaging Graph
2006;30(1):9–15.
[35] Grigorescu SE, Petkov N, Kruizinga P. Comparison of texture features based on
Gabor ﬁlters. IEEE Trans Image Process 2002;11:1160–7.
[36] Kruizinga P, Petkov N. Non-linear operator for oriented texture. IEEE Trans
Image Process 1999;8:1395–407.
[37] Ross TJ. Fuzzy logic with engineering applications. 3rd ed; 2010.
p. 198.
[38] Akay MF. Support vector machines combined with feature selection for breast
cancer diagnosis. Expert Syst Applicat 2009;36(2):3240–7.

357

[39] Chang C, Lin C. LIBSVM: a library for support vector machines. ACM Trans Intell
Syst Technol 2011;2(27):1–27. Software available at http://www.csie.ntu.
edu.tw/∼cjlin/libsvm
[40] Nishikawa RM. Current status and future directions of computer-aided diagnosis in mammography. Comput Med Imaging Graph 2007;31:224–35.
[41] Hand W, Semmlow JL, Ackerman LV, Alcorn FS. Computer screening of xeromammograms: a technique for deﬁning suspicious areas of the breast. Comput
Biomed Res 1979;12:445–60.
[42] Yin FF, Giger ML, Doi K, Metz CE, Vyborny CJ, Schmidt RA. Computerized
detection of masses in digital mammograms: Analysis of bilateral subtraction
images. Med Phys 1991;18(5):955–63.
[43] Zheng B, Chang YH, Gur D. Computerized detection of masses from digitized
mammograms: comparison of single-image segmentation and bilateral-image
subtraction. Acad Radiol 1995;2:1056–61.
[44] Gur D, Stalder J, Hardesty LA, Zheng B, Sumkin JH, Chough D, Rockette HE.
CAD performance on sequentially ascertained mammographic examinations
of masses: an assessment. Radiology 2004;233:418–23.
[45] Wang X, Lederman D, Tan J, Wang XH, Zheng B. Computerized prediction of risk
for developing breast cancer based on bilateral mammographic breast tissue
asymmetry. Med Eng Phys 2011;33(8):934–42.

NeuroImage 46 (2009) 717–725

Contents lists available at ScienceDirect

NeuroImage
j o u r n a l h o m e p a g e : w w w. e l s e v i e r. c o m / l o c a t e / y n i m g

Automated segmentation of mouse brain images using extended MRF
Min Hyeok Bae a, Rong Pan a,⁎, Teresa Wu a, Alexandra Badea b
a
b

Department of Industrial, Systems and Operations Engineering, Arizona State University, Tempe, AZ 85287–5906, USA
Center for In Vivo Microscopy, Box 3302, Duke University Medical Center, Durham, NC 27710, USA

a r t i c l e

i n f o

Article history:
Received 14 October 2008
Revised 26 December 2008
Accepted 7 February 2009
Available online 21 February 2009
Keywords:
Automated segmentation
Data mining
Magnetic resonance microscopy
Markov random ﬁeld
Mouse brain
Support vector machine

a b s t r a c t
We introduce an automated segmentation method, extended Markov random ﬁeld (eMRF), to classify 21
neuroanatomical structures of mouse brain based on three dimensional (3D) magnetic resonance images
(MRI). The image data are multispectral: T2-weighted, proton density-weighted, diffusion x, y and z
weighted. Earlier research (Ali, A.A., Dale, A.M., Badea, A., Johnson, G.A., 2005. Automated segmentation of
neuroanatomical structures in multispectral MR microscopy of the mouse brain. NeuroImage 27 (2), 425–
435) successfully explored the use of MRF for mouse brain segmentation. In this research, we study the use of
information generated from support vector machine (SVM) to represent the probabilistic information. Since
SVM in general has a stronger discriminative power than the Gaussian likelihood method and is able to
handle nonlinear classiﬁcation problems, integrating SVM into MRF improved the classiﬁcation accuracy. The
eMRF employs the posterior probability distribution obtained from SVM to generate a classiﬁcation based on
the MR intensity. Secondly, the eMRF introduces a new potential function based on location information.
Third, to maximize the classiﬁcation performance, the eMRF uses the contribution weights optimally
determined for each of the three potential functions: observation, location and contextual functions, which
are traditionally equally weighted. We use the voxel overlap percentage and volume difference percentage to
evaluate the accuracy of eMRF segmentation and compare the algorithm with three other segmentation
methods — mixed ratio sampling SVM (MRS-SVM), atlas-based segmentation and MRF. Validation using
classiﬁcation accuracy indices between automatically segmented and manually traced data shows that eMRF
outperforms other methods.
© 2009 Elsevier Inc. All rights reserved.

Introduction
MRI is often the imaging modality of choice for noninvasive
characterization of brain anatomy because of its excellent soft tissue
contrast. Its detailed resolution allows the investigation of normal
anatomical variability bounds, as well as the quantization of
volumetric changes accompanying neurological conditions. A prerequisite for such studies is an accurate segmentation of the brain;
therefore, many studies have focused on tissue classiﬁcation into
white matter, gray matter and cerebrospinal ﬂuid (CSF), as well as
anatomical structure segmentation. Several successful methods for
tissue segmentation include statistical classiﬁcation and geometrydriven segmentation (Ballester et al., 2000), statistical pattern
recognition methods based on a ﬁnite mixture model (Andersen et
al., 2002), expectation maximization algorithm (Kovacevic et al.,
2002), artiﬁcial neural network (Reddick et al., 1997), hidden Markov
random ﬁeld (Zhang et al., 2001), and region-based level-set and
fuzzy classiﬁcation (Suri, 2001). Compared to anatomical structure

⁎ Corresponding author. Fax: +1 480 965 8692.
E-mail address: rong.pan@asu.edu (R. Pan).
1053-8119/$ – see front matter © 2009 Elsevier Inc. All rights reserved.
doi:10.1016/j.neuroimage.2009.02.012

segmentation, tissue segmentation is a relatively easier task (Heckemann et al., 2006). This is because the MR signals are in general
distinctive enough to have the brain tissues classiﬁed into white
matter, gray matter and CSF, while the anatomical structures can be
composed of more than one tissue type, and have ambiguous
contours. Nevertheless, segmenting neuroanatomical structures has
recently attracted considerable attention since it can provide stronger
foundation to help in the early diagnosis of a variety of neurodegenerative disorders (Fischl et al., 2002).
Automated methods for segmenting the brain in anatomically
distinct regions can rely on either a single imaging protocol or multispectral data. For example, Duta and Sonka (1998) have segmented
T1-weighted MR images of the human brain into 10 neuroanatomical
structures using active shape models. Fischl et al. (2002) accomplished an automated labeling of each voxel in the MR human brain
images into 37 neuroanatomical structures using MRF. Heckemann
et al. (2006) segmented T1-weighted human MR images into 67
neuroanatomical structures using nonrigid registration with free-form
deformations, and combined label propagation and decision fusion for
classiﬁcation. Multispectral imaging was used as the basis of segmentation, for example, by Zavaljevski et al. (2000) and Amato et al.
(2003). Zavaljevski et al. (2000) used Gaussian MRF and maximum

718

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

likelihood method on multi-parameter MR images (including T1, T2,
proton density, Gd+ T1 and perfusion imaging) to segment the
human brain into 15 neuroanatomical structures. Amato et al. (2003)
used independent component analysis (ICA) and nonparametric
discriminant analysis to segment the human brain into nine classes.
Developments in small animal imaging capabilities have led to
increased attention being given to the problem of mouse brain
segmentation. Mice provide excellent models for the anatomy,
physiology, and neurological conditions in humans, because with
whom they share more than 90% of gene structure. Unfortunately, the
methods for human brain anatomical structure segmentation cannot
directly be applied with the same success to the mouse brain. This is
due to the facts that (1) a normal adult mouse brain is approximately
3000 times smaller than an adult human brains, thus the spatial
resolution needs to be scaled accordingly from the 1-mm3 voxels
commonly used in the study for human brains to voxel volumes
b0.001 mm3; (2) the inherent MR contrast of the mouse brain is
relatively low compared to the human brain.
Research on mouse brain image segmentation is still limited thus
far and the major directions of research include the atlas-based
approach and the probabilistic information based approach. The atlasbased approach is to create a neuroanatomical atlas, also called reference atlas, using training brains and nonlinearly register the atlas to
test brains with the scope of labeling each voxel of the test brains
(Mazoyer et al., 2002; Kovacevic et al., 2005; Ma et al., 2005; Bock
et al., 2006). For example, Ma et al. (2005) segmented T2⁎-weighted
magnetic resonance microscopy (MRM) images of 10 adult male
formalin-ﬁxed, excised C57BL/6J mouse brains into 20 anatomical
structures. They chose one mouse brain image out of the 10 mouse
brains as a reference atlas. The rest nine testing images were aligned to
the reference atlas using a six-parameter rigid-body transformation
and then the reference atlas was mapped to the nine testing brain
images using a nonlinear registration algorithm. The registration step
yields a vector ﬁeld, called a deformation ﬁeld, which contains
information on the magnitude and direction required to deform a
point in the reference atlas to the appropriate point in the testing
brain images. Using the deformation ﬁeld, the labeling of the reference
atlas is transformed to the testing image to predict the labeling of the
testing images. Kovacevic et al. (2005) built a variational atlas using
the MR images of nine 129S1/SvImJ mouse brains. The MR images of
the genetically identical mouse brains were aligned and nonlinearly
registered to create the variational atlas comprised of an unbiased
average brain. The probabilistic information based approach uses
probabilistic information extracted from training datasets of MR
images for the segmentation. Ali et al. (2005) incorporated multispectral MR intensity information, location information and contextual information of neuroanatomical structures for segmentation
of MRM images of the C57BL/6J mouse brain into 21 neuroanatomical
structures using the MRF method. They modeled the location
information as a prior probability of occurrence of a neuroanatomical
structure at a location in the 3D MRM images and the contextual
information as a prior probability of pairing of two neuroanatomical
structures.
MRF has been widely used in many image segmentation problems
and image reconstruction problems such as denoising and deconvolution (Geman and Geman, 1984). It provides a mathematical formulation for modeling local spatial relationships between classes. In the
brain image segmentation problem, the probability of a label for a
voxel is expressed as a combination of two potential functions: one is
based on the MR intensity information and the other is based on
contextual information, such as the labels of voxels in a predeﬁned
neighborhood around the voxel under study. Bae et al. (2008)
developed an enhanced SVM model, called Mix-Ratio samplingbased SVM (MRS-SVM), using the multispectral MR intensity
information and voxel location information as input features. The
MRS-SVM provided a comparable classiﬁcation performance with the

probabilistic information based approach developed in Ali et al.
(2005). Furthermore, Bae's study also suggested that compared to
MRF, the MRS-SVM outperforms for larger structures, but underperforms for smaller structures.
Based on these studies which suggest that integrating a powerful
SVM classiﬁer into the probabilistic information based approach may
improve the overall accuracy of mouse brain segmentation, we
introduce a novel automated method for brain segmentation, called
extended MRF (eMRF). In this method (eMRF), we use the voxel
location information by adding a location potential function. Other
than using Gaussian probability distribution to construct a potential
function based on MR intensity, the SVM classiﬁer is used to model the
potential function of MR intensity. We assess the accuracy of the
automated brain segmentation method in a population of ﬁve adult
C57BL6 mice, imaged using multiple (ﬁve) MR protocols. All 21
neuroanatomical structures are segmented, and the accuracy is
evaluated using two metrics including the volume overlap percentage
(VOP) and the volume difference percentage (VDP). The use of these
two metrics allows us to compare the accuracy of our method with the
atlas-based segmentation, MRF and MRS-SVM methods.
Materials and methods
Subjects and magnetic resonance microscopy data
The MRM images used in this work were provided by the Center
for In Vivo Microscopy in Duke University Medical Center and they
were previously used in Ali et al. (2005) as well. Five formalin-ﬁxed
C57BL/6J male mice of approximately 9 weeks in age were used. The
MRM image acquisition consisted of isotropic 3D T2-weighted,
proton density-weighted, diffusion x, y and z weighted scans. Image
acquisition parameters for all acquisition protocols include the ﬁeld
of view of 12 × 12 × 24 mm and matrix size of 128 × 128 × 256. All
protocols used the same ﬂip angle of 135 degrees and 2 excitations
(NEX). Speciﬁc to the PD image, TE/TR was 5/400 ms and bandwidth
was 62.5 KHz; for the T2 weighted image, TE/TR was 30/400 ms and
bandwidth was 62.5 kHz bandwidth; and for the three diffusion
scans, TE/TR was 15.52/400 ms, and bandwidth was 16.25 MHz. The
Stejskal Tanner sequence was used for the acquisition of the
diffusion-weighted scans. Bipolar diffusion gradients of 70 G/cm
with pulse duration of 5 ms and inter-pulse interval of 8.56 ms were
applied along the three axes and the effective b value of 2600 s/mm2
was used. A 9-parameter afﬁne registration which accounts for
scaling, rotation and translation was applied to each mouse brain, to
bring it into a common space. Manual labeling of 21 neuroanatomical
structures was done by two experts using T2-weighted datasets of
the ﬁve mouse brains. These manual labelings were regarded as true
labeling for each voxel. Table 1 presents the list of the 21
neuroanatomical structures and abbreviations to be segmented in
this work.

Table 1
List of 21 segmented structures and their abbreviations.
Structures

Abbrev.

Structures

Abbrev.

Structures

Abbrev.

Cerebral cortex

CORT

INFC

Pontine nuclei

PON

Cerebral peduncle

CPED

MED

Substantia nigra

SNR

Hippocampus

HC

Inferior
colliculus
Medulla
oblongata
Thalamus

THAL

INTP

Caudate putamen
Globus pallidus

CPU
GP

MIND
AC

Internal capsule
Periacqueductal gray

ICAP
PAG

Midbrain
Anterior
commissure
Cerebellum
Ventricular
system

Interpeduncular
nucleus
Olfactory bulb
Optic tract

CBLM
VEN

Trigeminal tract
Corpus callosum

TRI
CC

OLFB
OPT

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

Markov random ﬁeld theory
MRF theory is a class of probability theory for modeling the
spatial or contextual dependencies of physical phenomena. It has
been used for brain image segmentation by modeling probabilistic
distribution of the labeling of a voxel jointly with the consideration
of the labels of a neighborhood of the voxel (Fischl et al., 2002; Ali
et al., 2005). For simplicity, let us consider a 2D image represented
by an m × n matrix, let X be a vector of signal strength and Y be the
associated labeling vector, that is, X = (x11, x12,…, x1n,…, xm1,… xmn),
and Y = (y11, y12,…, y1n,…, ym1,… ymn). We will rewrite yab as yi where
i = 1,2,…,S, S = mn, Y is said to be a MRF on S with respect to a neighborhood N if and only if the following two conditions are satisﬁed:
P ðY ÞN 0


ð1Þ

P yi jys − fig





= P yi jyNi

ð2Þ

where S-{i} denotes the set difference, and Ni denotes the set of sites
neighboring site i. Eq. (1) is called the positivity property and Eq. (2) is
the Markovianity property, which states only neighboring labels (or
clique) have direct interaction with each other. If these conditions are
satisﬁed, the joint probability P(Y) of any random ﬁeld is uniquely
determined by its local conditional probabilities.
The equivalence between the MRF and Gibbs distribution (Hammersley–Clifford theorem) provides a mathematically efﬁcient way of
specifying the joint probability P(Y) of an MRF. The theorem states the
joint probability P(Y) can be speciﬁed by the clique potential function
Vc(Y) which can be deﬁned by any appropriate potential function
based on the speciﬁc system's behavior. For more information about
potential function, please refer to Li (2001). The probability P(Y) can
be equivalently speciﬁed by a Gibbs distribution as follows:
1
exp ½−U ðY Þ
ð3Þ
Z
P
where Z =
exp ½−U ðY Þ is a normalizing constant, ΩY is the set of

P ðY Þ =

YaXY

the all possible Y on S, and U(Y) is an energy function which is deﬁned
as a sum of clique potential Vc(Y) over all cliques c ∈ C:
U ðY Þ =

X

Vc ðY Þ

ð4Þ

caC

where a clique, c, is a set of points that are all neighbors of each other and
C is a set of cliques, or the neighborhood of the clique under study. The
value of Vc(Y) depends on a certain conﬁguration of labels on the clique c.
For the image segmentation problem, we try to maximize a label's
posterior probability for given speciﬁc features, that is P(Y|X). With
the assumption of feature independency the posterior probability can
be formulated using Bayesian theorem as:
P ðY jX Þ~P ðX jY ÞP ðY Þ = P ðY Þ

S
Y

P ðxi jyi Þ:

719

techniques, such as SVM, are integrated with MRF, the overall
segmentation performance will be improved due to their powerful
class discrimination abilities even for the cases with complex
relationship between features and labels.
Support vector machines theory
SVM (Vapnik, 1995) was initially designed for binary classiﬁcation
by constructing an optimal hyperplane which gives the maximum
separation margin between two classes. Considering a training set of
m samples (xi, yi), i = 1, 2, …, m where xi ∈ Rn and yi ∈ {+1, − 1}.
Samples with yi = + 1 belong to positive class while those with yi =
−1 belong to negative class. SVM training involves ﬁnding the
optimal hyperplane by solving the following optimization problem:
m
X
1 T
w w+C
ni
2
i=1
s:t: yi ðw · Φðxi Þ + bÞz 1 − ni ; ni z 0;

Min QP ðw; b; nÞ =

ð7Þ
i = 1; N m:

where w is the n dimensional vector, b is a bias term, ξ = {ξ1,…ξm} and
QP is the objective function of the prime problem. The non-negative
slack variable (ξi) allows Eq. (7) to always yield feasible solutions even in
a non-separable case. The penalty parameter (C) controls the trade-off
between maximizing the class separation margin and minimizing the
classiﬁcation error. A larger C usually leads to higher training accuracy,
but may over-ﬁt the training data and cause the classiﬁer un-robust. To
enhance the linear separability, the mapping function (Φ(xi)) projects
the samples into a higher-dimensional dot-product space called the
feature space. Fig. 1 shows the optimal hyperplane in solid line, which
can be obtained by solving Eq. (7). The squares represent the samples
from the positive class and the circles represent the samples from the
negative class. The samples which satisfy the equality are called support
vectors. In Fig. 1, the samples are represented as the ﬁlled squares and
the support vectors are the ﬁlled circles.
Eq. (7) presents a constrained optimization problem. By introducing the non-negative Lagrangian multiplier αi and βi, it can be
converted to an unconstrained problem as shown below:
Min QP ðw; b; n; α; βÞ
=

m
m
m
 


X
X
X
1 T
T
ni −
α i yi w · Φðxi Þ + b − 1 + ni −
βi ni
w w + C
2
i=1
i=1
i=1

ð8Þ

where α = α1,…,αm and β = β1,…,βm. Furthermore, by differentiating
with respect to w, b and ξi and introducing the Karush–Kuhn–Tucker

ð5Þ

i=1

Eq. (5) can be rewritten as:
"
#
X
X
1
logP ðxi jyi Þ +
Vc ðY Þ :
P ðY jX Þ = exp
Z
caC
iaS

ð6Þ

Typically in MRF, a multivariate Gaussian distribution is used for
P(X|Y) and the maximum likelihood estimation is performed to ﬁnd
the labeling based on MR signals. This model assumes that the relationship between the features and labels follows the Gaussian distribution. In some of image segmentation problems, this assumption
is too restrictive to model the complex dependencies between the
features and the labels (Lee et al., 2005). If some data mining

Fig. 1. SVM binary classiﬁcation problem (adapted from Vapnik, 1995). The solid line
between the two dashed lines is the optimal hyperplane. The squares represent the
samples from the positive class and the circles represent the samples from the negative
class. The samples represented as the ﬁlled squares and the ﬁlled circles are the
support vectors.

720

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

(KKT) condition, Eq. (8) is converted to the following Lagrangian dual
problem:
m
 
1X
T
α i α j yi yj Φðxi Þ Φ xj
2
i=1
i;j = 1
m
X
α i yi = 0:
0 V α i V C i = 1; N ; m and

Max QD ðα Þ =
s:t:

m
X

P ðY jX; LÞ =

αi −

ð9Þ

i=1

The optimal solution αi ⁎ for the dual problem determines the
parameters w⁎ and b⁎ of the following optimal hyperplane, also
known as SVM decision function:


f ðxÞ = sign w4 · Φðxi Þ + b4
= sign
= sign

m
X
i=1
m
X

4

4

+b
!

ð10Þ

4

yi α i K ðx; xi Þ + b

i=1

where K(xi,xj)is a kernel function deﬁned as K(xi,xj) = Φ(xi)T Φ (xj).
The kernel function performs the nonlinear mapping implicitly. We
chose a Radial Basis Function (RBF) kernel, deﬁned as:


K xi ; xj





2
= exp −γjjxi −xj jj ;

γN0

ð11Þ

where γ in Eq. (11) is a parameter related to the span of an RBF kernel.
The smaller the value is, the wider the kernel spans.
To extend the application of SVM for multiclass classiﬁcation, a number of methods have been developed, which mainly fall in three categories: One-Against-All (OAA), One-Against-One (OAO) and All-At-Once
(AAO). In OAA method, one SVM is trained with the positive class
representing one class and the negative class representing the others.
Therefore, it builds n different SVM models where n is the number of the
classes. The idea of AAO is similar to that of OAA, but it determines n
decision functions at once in one model, where the kth decision function
separates the kth class from the other classes. In OAO method, a SVM is
trained to classify the kth class and the lth class. Therefore, it constructs
nðn − 1Þ = 2 SVM models. Hsu and Lin (2002) reported that the training
time of OAO method is less than that of OAA or AAO method. OAO is more
efﬁcient on large datasets than OAA and AAO method. Thus, OAO method
is used in this study. In OAO method, the following problem is to be solved:
m


X
1  kl T  kl 
kl
w
Min QP wkl ; bkl ; nkl =
w +C
ni
2
i=1
 T
s:t:
wkl · Φðxi Þ + bkl z 1 − nkl
if yi = k :
i ;
 T
kl
kl
kl
· Φðxi Þ + b V − 1 + ni ; if yi = l
w

nki z 0;

i = 1; N ; m

and

k; l = 1; N ; n

P ðX; L jY ÞP ðY Þ
:
P ðX; LÞ

ð13Þ

Assuming X and L independent from each other yields following
expression:
P ðX jY ÞP ðLjY ÞP ðY Þ
P ðX ÞP ðLÞ
P ðY jX ÞP ðX ÞP ðY jLÞP ðLÞP ðY Þ
~
P ðX ÞP ðLÞ
= P ðY jX ÞP ðY jLÞP ðY Þ:

P ðY jX; LÞ =

ð14Þ

If we make logarithmic transformation to Eq. (14), we obtain

!

4
T
yi α i ΦðxÞ Φðxi Þ

given a multivariate intensity vector X and a location vector L is
formulated as follows:

ð12Þ

eMRF
As discussed earlier in Eq. (6), traditional MRF mainly focuses on
MR intensity as well as the contextual relationship. Research on brain
segmentation has concluded that beside of MR intensity and
contextual relationship with neighboring structures, voxel location
within the brain also plays an important role (Fischl et al., 2002; Ali et
al., 2005, Bae et al., 2008). To incorporate the three different types of
information into an image segmentation model, Bayesian theorem is
used. Consider a 3D MR image represented by an m × n × o matrix,
with its associated multivariate intensity vector X = (x111, x112,…, x11o,
…, x1n1,… xm11,…, xmno), a location vector L = (l111, l112,…, l11o,…, l1n1,…
lm11,…, lmno), and class label conﬁguration Y = (y111, y112,…, y11o,…,
yl1n1,… ym11,…, ymno). We rewrite yabcas yi where i = 1,2,…,S, and
S = mno. The posterior probability of having a label conﬁguration Y

log P ðY jX; LÞ~ log P ðY jX Þ + log P ðY jLÞ + log P ðY Þ:

ð15Þ

Those terms on the right hand side of Eq. (15) can be regarded as the
contribution to labeling from MR signal intensities, voxel coordinates,
and prior belief of label, which incorporates the contextual information into the labeling decision. The function can be modiﬁed as
following:
P ðY jX; LÞ~
"
 exp w1

X

logAi ðyi ; xi Þ + w2

iaS

s:t:

X

log Bi ðyi ; li Þ + w3

iaS

w1 + w2 + w3 = 1

X

#


Vi yi ; yNi

iaS

ð16Þ

where w1, w2 and w3 are model parameters which indicate the degree
of contribution of each term to the posterior probability P(Y|X,L), and
Ni denotes the neighboring sites of site i.
The ﬁrst term, Ai(yi, xi), in Eq. (16) is the observation potential
function that models the MR intensity information. The eMRF model
employs SVM in order to take advantage of the excellent discriminative ability of SVM. Since SVM decision function gives as output the
distance from an instance to the optimal separation hyperplane, Platt
(2000) proposed a method for mapping the SVM outputs into
posterior probability by applying a sigmoid function whose parameters are estimated from the training process. The observation
potential function is deﬁned as follows, for voxel i:
Ai ðyi ; xi Þ = P ðyi = kjxi Þ =

1
1 + expðα fk ðxi Þ + βÞ

ð17Þ

where fk(xi) is the SVM decision function for class k, α and β are the
parameters determined from the training data. P(yi = k|xi) is the
likelihood that the label yi is labeled as class k given the MR intensity
xi. When applying SVM to MR image segmentation, the overlapping
of MR signals is a critical issue. Adding the three coordinates (x, y and
z coordinates of each data point in a 3D MR image) as features into
SVM classiﬁer helps in classiﬁcation by relieving the class overlapping problem of MR intensities between different structures (Bae
et al., 2008). In one of our preliminary experiments, we found that
using two MR protocols (T2-weighted and proton density-weighted)
along with coordinate features yield better segmentation performance than using all the ﬁve MR protocols1. Our preliminary result
also indicates that adding more MR contrasts does not aid the classiﬁer because it increases the overlapping among MR signals and
makes the dataset noisier. Thus, the intensity feature vector X forms a
ﬁve dimensional vector, consisting of the two MR intensities (T2-

1
Using the two indices VOP (the larger the better), VDP (the smaller the better)
introduced in the next section, the experiment on two MR protocols with coordinates
yields 72.42% VOP and 19.21% VDP in average. The experiment on all ﬁve MR protocols
with coordinates generates 59.38% VOP and 24.16% VDP.

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

weighted and proton density (PD) weighted acquisition) and the
three coordinate features.
The second term Bi(yi, li) in Eq. (16), is the location potential
function. Fischl et al. (2002) point out that the number of possible
neuroanatomical structures at a given location in a brain atlas becomes small as registration becomes accurate. Therefore, the location
of a voxel in a 3D image after registration is critical for the classiﬁcation of voxels the neuroanatomical classes. The location potential
function is computed as follows:
Bi ðyi ; li Þ = P ðyi = kjli = r Þ =

of voxels labeled as k at location r
ð18Þ
of voxels at location r

where P(yi = k|li = r) is the probability that a voxel's label yi is
predicted as class k given that the location li of the voxel is r. The
denominator in Eq. (18) is equal to the number of mice used in the
training set. This location information is similar to the a priori
probability used in human brain segmentation study (Ashburner and
Friston, 1997) as implemented in SPM.
The third term Vi(yi,yNi) in Eq. (16) is the contextual potential
function which models the contextual information using MRF. Based
on the MRF theory, the prior probability of having a label at a given site
i is determined by the label conﬁguration of the neighborhood of the
site i. The contextual potential function for site i will have a higher
value as the number of neighbors that have the same label increases. It
is deﬁned as

X 
δ yi ; yj





1 if yi = yj
jaNi
where δ yi ; yj =
Vi yi ; yNi =
ð19Þ
0 if yi ≠ yj
nðNi Þ
where n(Ni) is the number of voxels in a neighborhood of site i. We
use a ﬁrst order neighborhood system as a clique, which consists of
the adjacent six voxels in the four cardinal directions in a plane and
the front and back directions through the plane.
In fact, other than observation and contextual potential function,
eMRF introduces location potential function to the problem. Noticing
that each potential function can independently classify a voxel, eMRF
model integrates them together and assigns weight optimally to each of
them based on their classiﬁcation performance from the training set.
The weight parameter indicates the contributions from the three
different potential functions to predict the labeling. The optimal weight
parameter is determined by grid search on several possible sets of w1, w2
and w3 using cross validation. The Y value that maximizes the posterior
probability P(Y|X,L) corresponds to the most likely label given the
information of X and L. This is known as the maximum a posterior
(MAP) solution which is well known in the machine vision literatures
(Li, 2001). Finding the optimal solution of the joint probability of a MRF
Y 4 = maxY P ðY jX; LÞ is very difﬁcult because of the complexity caused
by the interaction among multiple labels. A local search method called
iterated conditional modes (ICM) proposed by Besag (1986) is used in
this study to locate the optimal solutions. The ICM algorithm is to
maximize local conditional probabilities sequentially by using the
greedy search in the iterative local maximization. It is expressed as
4

yi = arg max P ðyi jxi ; li Þ:
yi aY

ð20Þ

Given the data xi and li, the algorithm sequentially updates y(t)
i into
+ 1)
y(t
by switching different labels to maximizing P(yi|xi, li) for every
i
site i in turn. The algorithm terminates when no more labels are
changed. In the ICM algorithm, how to set the initial estimator y
(0)
is very important. We use the MAP solution based on only the
location information as the initial estimator of the ICM algorithm, i.e.,
ð0Þ

yi

= arg max P ðyi jli Þ:
yi aY

ð21Þ

721

Performance measurements
The VOP and VDP are used to measure the performance of the
proposed automated segmentation procedure (Fischl et al., 2002; Ali
et al., 2005, Hsu and Lin, 2002). The VOP and VDP are calculated by
comparing the automated segmentation with the true voxel labeling
(from the manual segmentation). Denote LA and LM as labeling of the
structure i by automated segmentation and manual segmentation
respectively, and V(L) as the volume of the labeling. The volume
overlap percentage for class i is deﬁned as
VOPi ðLA ; LM Þ =

V ðLA \ LM Þ
× 100k:
ðV ðLA Þ + V ðLM ÞÞ = 2

This performance index is the larger the better. When all the
labels from the automated and manual segmentation coincide,
VOPi(LA, LM) is 100%. VOP is very sensitive to the spatial difference
of the two labelings, because a slight difference in spatial location of
the two labelings can cause signiﬁcant decreases in the numerator of
VOPi(LA, LM).
The VDP for class i is used for quantifying the volume difference
of the structures delineated by the two segmentations, and it is
deﬁned as
VDPi ðLA ; LM Þ =

jV ðLA Þ − V ðLM Þj
× 100k:
ðV ðLA Þ + V ðLM ÞÞ = 2

This performance index is smaller the better. When all the
labelings from the two segmentations are identical, VDPi(LA, LM) is 0.
Results
Segmentation and model validation
The eMRF algorithm was implemented and tested on a group of
90 μm isotropic resolution MR images of ﬁve C57/BL6 mice. To assure
the validity of all results from this experiment, a ﬁve-fold cross
validation was used in every steps of the experiment. Each mouse was
used as a test brain, while the remaining four mice were used as the
training set. Hence we tested the algorithm on ﬁve distinct training and
testing sets. To calculate the observation potential function in Eq. (17),
we built ﬁve SVM models for the ﬁve different training sets using the
MRS-SVM procedure (Bae et al., 2008). Building a SVM model for mouse
brain segmentation is challenging because the number of the classes is
large (N20) and the classes are highly imbalanced. The MRS-SVM
procedure enabled us to build SVM model for the brain segmentation
efﬁciently and effectively. Testing each SVM model yielded the posterior
probability P(Y|X) in Eq. (17) for each tested mouse. The location
potential function was calculated for the ﬁve mouse based on Eq. (18).
After the observation and location potential functions were calculated for
each mouse, the ICM algorithm was used to provide the contextual
potential function in Eq. (19) and the posterior probability P(Y|X,L) in
Eq. (16). To ﬁnd the best weight for each potential function we
conducted a grid search over the range of each wi = {0.1, 0.2,…, 0.9}
(i = 1, 2, 3), where w1 +w2 +w3 = 1. The best model parameters
determined were w1 = 0.1, w2 = 0.6 and w3 = 0.3 for observation, location and contextual functions, respectively.
We also implemented the atlas-based segmentation for the same
mouse brain images to compare its performance with the eMRF
method. One mouse brain image was chosen out of the ﬁve mouse
brains as a reference brain. First, the rest four testing images were
realigned to the reference brain using a 12-parameter afﬁne registration which adjusts the global position and size differences between
the individual brain images. Next, the reference brain was mapped to
the testing brain images using a nonlinear registration which deforms
each neuroanatomical structure into a common space. The registrations were done using Image Registration Toolkit (ITK) software and

722

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

the registration parameters were optimized by the software. Each
image of the ﬁve mouse brain was used as the reference brain, while
the remaining four images were used as the testing brains. Therefore,
each brain image was segmented four times with different reference
brain each time.
Table 2 presents the segmentation performance based on two
metrics: VOP and VDP between the automated and manual labels.
These results are based on ﬁve mouse brains, and with the use of ﬁvefold cross validation. The results for the eMRF method are compared
with three other segmentation methods — the aforementioned atlasbased segmentation method, the MRS-SVM method (Bae et al., 2008)
and the MRF method (Ali et al., 2005). Overall eMRF outperforms all
the other three segmentation methods. Compared to MRS-SVM the
average VOP of eMRF is improved by 8.68%, 10.05% compared to the
atlas-based segmentation, and 2.79% compared to MRF. Corresponding to an increase in the average VOP, the average VOD of eMRF is
decreased. It is improved by 42.04% compared to MRS-SVM, 23.84%
compared to the atlas-based segmentation, and 12.71% compared to
MRF. Fig. 2 illustrates the VOP (top) and VDP (bottom) of the four
different segmentation methods.
The eMRF method outperforms the MRS-SVM method in most
structures. The improvement in segmentation performance as
assessed by VOP is greatest for white matter structures like the
anterior commissure (99.29%), and cerebral peduncle (15.55%) but
also for ventricles (35.91%). Exceptions where eMRF underperforms to
MRS-SVM are three large structures: CORT, CBLM and OLFB. The MRSSVM method performs better for large structures because the
classiﬁer tends to generate the separation hyperplane between a
large class and a small class such that it is biased towards the large
class. Therefore, a voxel that is located at the boundary between the
large class and the small class is more likely predicted as the large
class and this reduces the misclassiﬁcation of a large class voxel to a

small class voxel. However, the eMRF method balances the contributions from the MR intensity information, the location information and
the contextual information by assigning potential functions with
different weights. In our study, the weight assigned to observation
potential function (w1) is smaller than the other two. The averages of
the VOP and the VDP of the eMRF method are improved by 8.68%, and
42.04% respectively, compared to the MRS-SVM method, which indicates that the overall segmentation performance has been improved by
the use of eMRF.
The eMRF method provides better performances in most structures
than the atlas-based segmentation method. Based on the VOP, the
eMRF method outperforms the atlas-based segmentation method in
all the structures except the two structures, AC and INTP. The
performance differences in these two structures are only 0.45% in
AC and 5.49% in INTP. The eMRF method has a better VDP performance
than the atlas-based segmentation method in 17 structures. There are
four small structures (AC, VEN, INTP and CC) that the atlas-based
segmentation method has better VDP performance than eMRF. Since
each labeling of the voxels in the reference brain is mapped to the
testing brain in the atlas-based segmentation process, the number of
labeling for a structure in the reference brain is well maintained in the
number of labeling for the structure in the testing brain. Therefore, the
VDP values of the atlas-based segmentation method in all the
structures are quite consistent (11.14–15.23). However, this method
has the worst VOP performance among the four different segmentation methods, indicating that the accuracy of atlas-based segmentation is poor. The eMRF method can improve the averages of the VOP
and the VDP by 10.05% and 23.84%, respectively, which shows that
overall the eMRF method outperforms the atlas-based segmentation
method.
Based on the voxel overlap metric, the eMRF method shows
better performance in 13 out of 21 structures compared to the MRF

Table 2
Accuracy of the four segmentation methods (eMRF, MRF-SVM, atlas-based segmentation and MRF) for each of the 21 segmented brain structures is evaluated using the VOP (top row)
for each segmentation and VDP (second row) metrics.
CORT
eMRF
MRS-SVM
Improvement
Atlas-based segmentation
Improvement
MRF
Improvement

eMRF
MRS-SVM
Improvement
Atlas-based segmentation
Improvement
MRF
Improvement

VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)

VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)
VOP (%)
VDP (%)

91.10
3.34
93.46
1.63
− 2.53
104.24
84.19
12.24
8.20
72.74
90.77
4.35
0.36
23.24

CPED
73.15
8.32
63.31
11.50
15.55
− 27.60
64.31
11.76
13.75
29.20
67.69
12.17
8.06
31.62

HC
86.20
5.56
83.92
5.87
2.71
− 5.33
84.07
11.91
2.54
53.34
87.69
6.96
− 1.70
20.08

AC

CBLM

VEN

50.76
25.55
25.47
68.49
99.29
62.70
50.99
11.51
− 0.45
− 121.98
55.38
19.13
− 8.35
− 33.54

92.68
3.73
96.29
1.48
− 3.75
− 151.51
85.91
12.49
7.89
70.13
93.08
3.48
− 0.42
− 7.22

71.83
16.83
52.85
24.92
35.91
32.48
67.30
12.40
6.73
− 35.70
70.77
19.13
1.50
12.04

CPU

GP

ICAP

PAG

INFC

MED

THAL

MIDB

87.62
3.98
85.72
3.89
2.22
2.16
82.46
11.54
6.26
65.53
88.46
6.96
− 0.95
42.84

79.64
7.54
74.47
11.34
6.95
− 33.51
68.19
13.51
16.79
44.16
78.46
7.83
1.50
3.61

73.14
11.05
63.91
12.66
14.44
− 12.78
60.83
11.40
20.25
3.13
73.85
13.04
− 0.96
15.31

90.26
3.55
80.90
7.97
11.57
− 55.44
84.47
11.14
6.85
68.11
85.38
2.61
5.71
− 36.19

85.32
5.26
83.21
7.81
2.54
− 32.64
75.67
14.87
12.76
64.60
83.08
4.35
2.70
− 21.05

91.93
7.12
90.27
10.03
1.84
− 28.98
84.24
12.50
9.12
43.05
86.15
10.43
6.70
31.76

93.53
2.48
92.07
2.78
1.59
− 10.61
91.16
11.57
2.60
78.54
93.08
3.48
0.49
28.58

93.27
2.97
90.48
3.84
3.09
− 22.57
88.56
12.47
5.32
76.18
90.77
4.35
2.76
31.68

PON

SNR

INTP

OLFB

OPT

TRI

CC

Avg.

71.61
26.09
58.18
62.22
23.08
58.06
75.77
15.23
− 5.49
− 71.32
76.92
13.91
− 6.91
− 87.55

82.50
10.79
91.23
7.85
− 9.57
− 37.49
82.19
12.40
0.38
12.98
84.62
16.52
− 2.50
34.68

80.03
12.49
74.38
14.30
7.60
12.65
70.68
14.55
13.24
14.17
73.08
17.39
9.52
28.18

78.98
10.16
60.79
25.65
29.91
60.39
73.18
12.93
7.92
21.41
68.46
15.65
15.36
35.09

68.67
5.67
59.25
19.93
15.90
71.58
44.74
12.05
53.49
52.99
53.08
14.78
29.39
61.68

73.83
7.33
69.72
17.27
5.89
57.57
59.72
12.36
23.62
40.71
63.85
10.43
15.63
29.77

65.75
20.57
57.65
24.28
14.04
15.26
49.55
12.28
32.69
− 67.49
71.54
22.61
− 8.10
9.00

80.09
9.54
73.69
16.46
8.68
42.04
72.77
12.53
10.05
23.84
77.91
10.93
2.79
12.71

The percentage improvement in segmentation accuracy is computed for eMRF versus MRS-SVM, eMRF versus atlas-based segmentation, and eMRF versus MRF. The improvements in
VOP and VOD are calculated separately. A ‘+’ sign always means that eMRF method outperforms the other methods for the speciﬁc neuroanatomical structure and ‘−’ means that
eMRF underperforms.

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

723

Fig. 2. Comparison of the relative performances of the eMRF, MRS-SVM, atlas-based segmentation and MRF methods based on the voxel overlap percent — VOP (top) and the volume
difference percent — VDP (bottom) indices.

Fig. 3. Coronal slices through the labeled brain at the level of anterior hippocampus and third ventricle (upper row), and pons and substantia nigra (lower row) show in a qualitative
manner the relative superiority of eMRF compared to MRS-SVM. Note that eMRF segmentation better preserved the shapes of striatum and corpus callosum (as seen in the manual
labels), compared to MRS-SVM; and also that eMRF was able to segment a small CSF ﬁlled region in the center of PAG, while MRS-SVM missed it.

724

M.H. Bae et al. / NeuroImage 46 (2009) 717–725

method. The eMRF method provides more than 5% performance
improvement in 7 structures (CPED, PAG, MED, PON, SNR, OPT and
TRI), the largest improvements are seen for optic tract (29.39%) and
trigeminal tract (15.63%). Based on the volume difference metric, the
eMRF method shows better performance in 16 structures out of 21
structures. VDP improvement ranged from 61.68% for optic tract and
to 3.61% for globus pallidus. In four small structures (PAG, INFC, AC
and INTP), which take less than 1% volume compared to the whole
brain volume, the MRF method shows better performance. This is
due to the fact that in the MRF method labeling a voxel depends on
the labelings of the neighborhood of the voxel, thus the identiﬁcation of small structures is enhanced. Nevertheless, the averages of
the VOP and the VDP of the eMRF method are improved by 2.79%,
and 12.71% respectively, compared to the MRF method. The eMRF
method can make a balance between the overﬁtting of the SVM
method to the larger classes and the overﬁtting of the MRF method
to the smaller classes so that the overall segmentation performance
is improved.
A visual comparison of the three segmentation methods, manual
labeling (the gold standard), eMRF and MRS-SVM, is shown for two
speciﬁc coronal levels (one at the level close to the hippocampal
commissure, and one at the level of the pons) in Fig. 3. On the left
column of Fig. 3 are displayed the manual segmentations (used as the
gold standard), on the middle column are the automated segmented
images by the eMRF method and the right column are those
produced by the MRS-SVM method. The eMRF method seems to
deviate less than the MRS-SVM method from the manual labeling,
and to respect better the topology of the structures. The testing time
of the MRS-SVM algorithm for a mouse brain dataset (472,100 voxels)
was 289.4 min with a 3.4-GHz PC. The testing experiment was run
using MATLAB, and executed the eMRF segmentation of one mouse
brain in 75 min.
Discussion and conclusion
In this paper, we developed an eMRF method, which ﬁrst adds a
potential function based on location information, then integrates the
SVM and MRF for a more accurate and robust segmentation of mouse
brain MR images by taking the advantages of the two methods. MRF
has been used in the human brain image segmentation task because it
utilizes the contextual information of a voxel, which describes the
correlation of the voxel and its neighbors for segmentation (Fischl
et al., 2002). A similar algorithm has been successfully implemented
for mouse brains (Ali et al., 2005). Other than the contextual
information and MR intensity information, Bae et al. (2008) recently
studied the use of SVM based on MR intensity information and
location information for mouse brain segmentation, and a novel
automated segmentation method, termed MRS-SVM, was proposed.
Experimental results indicate that the MRF method outperforms the
MRS-SVM method for smaller structures while the MRS-SVM method
outperforms the MRF method for larger structures. The complementary nature of the two methods directs the development of eMRF.
Speciﬁcally, other than using Gaussian probability distribution to
model the MR intensity signal, the eMRF method employs the
posterior probability distribution obtained from the SVM to generate
classiﬁcation based on the MR intensity information. Secondly, eMRF
introduces a new potential function based on the location information. Instead of considering the contributions from the three potential
functions – observation, location and contextual functions – equally,
eMRF further applies ICM to optimally determine the contribution
weights for each function.
To validate the proposed method, we conducted a comparison of
the four different algorithms: MRF (Ali et al., 2005), MRS-SVM (Bae
et al., 2008), atlas-based segmentation and eMRF for the automate
segmentation of mouse brain into 21 structures, using the same
dataset. Our test results show that the overall performance of the

eMRF method is better than all the other three segmentation
methods in terms of both the average VOP and average VDP, even
though the MRS-SVM method is slightly better on a small number of
large structures, and the atlas-based segmentation and MRF
methods are slightly better on a few small structures.
In the future, we will extend and adapt the eMRF method to human
brain segmentation and compare the results with the existing study in
literature (Powell et al., 2008). Further developments of this work
would also include obtaining higher resolution images and more
labels of neuroanatomical structures, which could provide more
information to be incorporated in atlases of normal mouse brains.
Moreover, we foresee that the automated segmentation method
described in the paper will accelerate the study of brain images of
large quantity, thus help developing small animal disease models for
many neurodegenerating disorders, such as Alzheimer disease and
Parkinson disease.
Acknowledgments
The authors would like to thank the Duke Center for In Vivo
Microscopy, an NCRR/NCI National Biomedical Technology Research
Resource (P41 RR005959/U24 CA092656), for providing the images.
They would also like to thank the Mouse Bioinformatics Research
Network (MBIRN) (U24 RR021760) for providing support for this
imaging study.
References
Ali, A.A., Dale, A.M., Badea, A., Johnson, G.A., 2005. Automated segmentation of
neuroanatomical structures in multispectral MR microscopy of the mouse brain.
NeuroImage 27 (2), 425–435.
Amato, U., Larobina, M., Antoniadis, A., Alfano, B., 2003. Segmentation of magnetic
resonance images through discriminant analysis. J. Neurosci. Methods 131, 65–74.
Andersen, A.H., Zhang, Z., Avison, M.J., Gash, D.M., 2002. Automated segmentation of
multispectral brain MR images. J. Neurosci. Methods 122, 13–23.
Ashburner, J., Friston, K., 1997. Multimodal image coregistration and partitioning — a
uniﬁed framework. NeuroImage 6, 209–217.
Bae, M.H., Wu, T., Pan, R., 2008. Mix-ratio sampling: classifying multiclass imbalanced
mouse brain images using support vector machine. Technical Report available at
http://swag.eas.asu.edu/vcie/.
Ballester, M.A., Zisserman, A., Brady, M., 2000. Segmentation and measurement of brain
structures in MRI including conﬁdence bounds. Medical Image Analysis 4, 189–200.
Besag, J., 1986. On the statistical analysis of dirty pictures. J. R. Stat. Soc., Ser. B 48 (3),
259–302.
Bock, N.A., Kovacevic, N., Lipina, T.V., Roder, J.C., Ackerman, S.L., Henkelman, R.M., 2006.
In vivo magnetic resonance imaging and semiautomated image analysis extend the
brain phenotype for cdf/cdf mice. J. Neuroscience 26 (17), 4455–4459.
Duta, N., Sonka, M., 1998. Segmentation and interpretation of MR brain images: an
improved active shape model. IEEE Trans. Med. Imag. 16, 1049–1062.
Fischl, B., Salat, D.H., Busa, E., Albert, M., Dieterich, M., Haselgrove, C., Van der Kouwe, A.,
Killiany, R., Kennedy, D., Klaveness, S., et al., 2002. Whole brain segmentation:
automated labeling of neuroanatomical structures in the human brain. Neuron 33,
341–355.
Geman, S., Geman, D., 1984. Stochastic relaxation, Gibbs distributions and the Bayesian
restoration of images. IEEE Trans. Pattern Anal. Mach. Intell. 6, 721–741.
Heckemann, R.A., Hajnal, J.V., Aljabar, P., Rueckert, D., Hammers, A., 2006. Automatic
anatomical brain MRI segmentation combining label propagation and decision
fusion. NeuroImage 33, 115–126.
Hsu, C.W., Lin, C.J., 2002. A comparison of methods for multi-class support vector
machines. IEEE Trans. Neural Netw. 13 (2), 415–425.
Kovacevic, N., Lobaugh, N.J., Bronskill, M.J., Levine, B., Feinstein, A., Black, S.E., 2002. A
robust method for extraction and automatic segmentation of brain images.
NeuroImage 17, 1087–1100.
Kovacevic, N., Henderson, J.T., Chan, E., Lifshitz, N., Bishop, J., Evans, A.C., Henkelman,
R.M., Chen, X.J., 2005. A three-dimensional MRI atlas of the mouse brain with
estimates of the average and variability. Cereb. Cortex 15 (5), 639–645.
Lee, C.H., Schmidt, M., Murtha, A., Bistritz, A., Sander, J., Greiner, R., 2005. Segmenting
brain tumors with conditional random ﬁelds and support vector machines. Lect.
Notes Comput. Sci. 3765, 469–478.
Li, S.Z., 2001. Markov Random Field Modeling in Image Analysis. Springer-Verlag, Tokyo.
Ma, Y., Hof, P.R., Grant, S.C., Blackband, S.J., Bennett, R., Slatest, L., Mcguigan, M.D.,
Benveniste, H., 2005. A three-dimensional digital atlas database of the adult
C57BL/6J mouse brain by magnetic resonance microscopy. Neuroscience 135 (4),
1203–1215.
Mazoyer, N., Landeau, B., Papathanassiou, D., Crivello, F., Etard, O., Delcroix, N.,
Mazoyer, B., Joliot, M., 2002. Automated anatomical labeling of activations in SPM
using a macroscopic anatomical parcellation of the MNI MRI single-subject brain.
NeuroImage 15, 273–289.

M.H. Bae et al. / NeuroImage 46 (2009) 717–725
Platt, J., 2000. Probabilistic Outputs for Support Vector Machines and Comparison to
Regularized Likelihood Methods. Advances in Large Margin Classiﬁers. MIT Press,
Cambridge, MA.
Powell, S., Magnotta, V.A., Johnson, H., Jammalamadaka, V.K., Pierson, R., Andreasen,
N.C., 2008. Registration and machine learning-based automated segmentation of
subcortical and cerebellar brain structures. NeuroImage 39, 238–247.
Reddick, W.E., Glass, J.O., Cook, E.N., Elkin, T.D., Deaton, R.J., 1997. Automated
segmentation and classiﬁcation of multispectral magnetic resonance images of
brain using artiﬁcial neural networks. IEEE Trans. Med. Imag. 16, 911–918.

725

Suri, J., 2001. Two-dimensional fast Magnetic Resonance brain segmentation. IEEE Eng.
Med. Biol. 84–95 July/August.
Vapnik, V.N., 1995. The Nature of Statistical Learning Theory. Springer Verlag.
Zavaljevski, A., Dhawan, A.P., Gaskil, M., Ball, W., Johnson, J.D., 2000. Multi-level
adaptive segmentation of multi-parameter MR brain images. Comput. Med.
Imaging and Graph. 24, 87–98.
Zhang, Y., Smith, S., Brady, M., 2001. Segmentation of brain MR images through a hidden
Markov random ﬁeld model and the expectation–maximization algorithm. IEEE
Trans. Med. Imag. 20, 45–57.

Proceedings of the 2009 Winter Simulation Conference
M. D. Rossetti, R. R. Hill, B. Johansson, A. Dunkin and R. G. Ingalls, eds.

ANALYSIS OF AMBULANCE DIVERSION POLICIES FOR A LARGE-SIZE HOSPITAL
Adrian Ramirez
John W. Fowler
Teresa Wu
Dept. of Industrial Systems and Operations Engineering
Arizona State University
Tempe, AZ 85287, USA
ABSTRACT
The overcrowding of Emergency Departments (EDs) is a well-known problem that has been analyzed on multiple occasions.
Queuing theory and simulation have been applied extensively to specific ED situations, such as staff planning, waiting time
reduction and capacity investment. However, there are remaining problems in the EDs that need more study. One of them is
the ambulance diversion, which may cause a delay in the treatment of urgent patients therefore jeopardizing their welfare.
Since ED are complex system and setting the diversion state in EDs is a subjective decision, a detailed modeling and analysis
of cause and effects of such a decision is beneficial. In this research, we build a case-study and analyze the impact of diversion policies in various performance measures of the ED through a designed experiment using a discrete-event simulation
model.
1

INTRODUCTION

During the last decade, national expenditures in healthcare in the United States have increased from 13.5% of the GDP to
16.2% in 2007, and is projected to reach 18% by 2012 according to the Centers for Medicare and Medicaid Services of the
United States Department of Health and Human Services (HHS 2009). This is one reason why the Institute of Medicine and
the National Academy of Engineering suggest building partnerships in the healthcare system in order to obtain more effective
results (NAE/IOM 2005).
In addition to increasing healthcare expenditures, the healthcare system faces problems in different areas, including the
Emergency Department (US General Accounting Office –GAO– 2003). In the report prepared for the U.S. Senate by the
GAO, they highlighted serious issues in EDs, such as long waiting times, patients that Leave Without Treatment (LWOT),
boarding and ambulance diversion.
This report defines boarding as the state of patients waiting in the ED to be transferred to an inpatient unit, which does
not have any available bed at that time. Patients LWOT are those who decide to leave the ED after triage but before seeing a
doctor. Ambulance Diversion (AD) is the request made by ED managers facing the overcrowding problem to bypass their facilities for those patients that would have been taken to that ED. This last problem is perhaps the one that has received least
attention from the engineering community.
The performance of nation’s EDs regarding diversion is quite concerning. The GAO found that 2 of every 3 hospitals reported ambulances being diverted to other hospitals at some point in fiscal year 2001. Furthermore, about 1 of every 10 EDs
reported being on diversion status for more than 20 percent of the year. An investigation by Associated Press (2006) shows
that emergency-care systems are at its “breaking point” and half a million times a year –about once every minute– an ambulance is diverted.
Despite the number of papers in the medical and engineering literature analyzing the ED problems, there is still a lack of
literature dealing with the diversion situation from a quantitative standpoint. It can be found in literature studies about waiting time reduction, LWOT or staffing planning using techniques like queuing theory, systems dynamics and discrete-event
simulation (Broyles and Cochran 2007; Ferrin, Miller, and McBroom 2007; Roche and Cochran 2007; Brailsford 2008; Kolb
et al. 2008; Medeiros, Swenson, and DeFlitch 2008), but few papers focuses on the design of diversion policies. However,

978-1-4244-5771-7/09/$26.00 ©2009 IEEE

1875

Ramirez, Fowler and Wu
simulation could be used with other quantitative methods to analyze different problems, including ambulance diversion in regional Healthcare Delivery Networks (Ramirez, Fowler, and Wu 2009).
Qualitative medical literature exists about AD and its relation with other factors and problems. For instance, it is necessary to remember that AD was not considered as a problem initially, but rather as a solution to reduce the congestion in overcrowded EDs (Asplin 2003). However, AD causes delays in initiating the treatment on emergent patients by transporting
them to a further facility, which may compromise the integrity and welfare of the most ill patients (Upfold 2002). Even
though diverted ambulances may not represent significant lost revenue (McConnell et al. 2006), it is still a public health
problem due to the possible negative effect on the health of those patients being diverted.
This paper provides an analysis of different factors found in the EDs, including the diversion state trigger and their impact in various performance measures, such as waiting time, time on diversion and LWOT percentage, considering a model
of a large-size hospital. The rest of the paper is structured as follows, Section 2 explains the methodology used in this research, Section 3 describes the simulation model of the hospital, Section 4 shows the experimentation plan and results for
Phase 1, Section 5 explains Phase 2 experimentation and results and finally Section 5 provides some conclusions and ideas
for future research.
2
2.1

METHODOLOGY
Objective

The objective of this research is to analyze the effects of the setting of the ED, the number of beds in the ED and the diversion state trigger on various performance measures of the emergency system, such as the waiting time, the LWOT percentage
and especially on the percentage of time on diversion, considering a large-size hospital.
The ED settings being considered are a “Regular ED” where all the patients, regardless of their condition, will be treated
in a bed; and a “Fast-Track ED”, where patients with a high level of trauma will be seen in a bed while the less ill can receive
attention in chairs or less-equipped beds. Even though the beds indicate the capacity of the system, they are an expensive resource in the ED and it is not a trivial decision to invest in them, usually other options are preferred.
The trigger of the diversion status in an ED is a subjective decision and so far there is little evidence of the impact of that
decision on other performance measures, therefore they are taken into account to analyze the results of varying this threshold.
2.2

Simulation Model

A simulation model of a fictitious large-size hospital was built for this research, which not only considers the ED, but also
other inpatient units that affect the operations of the emergency unit. Literature shows that two of the main reasons of setting
the diversion status on is the number of patients boarding and the capacity of wards highly demanded by ED patients (Huang
2004); therefore, it is important to consider the transference of patients in order to mimic more accurately the real system.
Few papers describe and provide information about the complexity of a hospital and the ED operations. Two of those
papers that can be found in the literature correspond to the same main author. Cochran and Bharti (2006) analyze the operations of a large-size hospital with an ED and 12 inpatient units. This paper describes the transferences of patients between
units and provides enough information to replicate the model. On the other hand, the analysis of an ED can be found in Cochran and Roche (2009), which provides detailed information of an ED belonging to a large-size hospital of the same region of
the other paper. These two papers are the basis for building two versions of a simulation model that captures the complexity
of the ED operations and its interaction with other units in a large-size hospital. The two versions are associated with the ED
setting: “Regular ED” and “Fast-Track ED”.
2.3

Experimentation

The simulation models built representing the whole hospital with detailed representation of the ED operations are subject to
experimentation considering the factors described in the objective. For this purpose, a two-level factorial design is implemented to analyze the response regarding waiting time, number of patients in queue, LWOT and diversion statistics.
Then, a second experiments is run using the best ED setting design and number of beds obtained in Part 1. Those factors
are fixed and a larger number of diversion thresholds will be tested to analyze the changes in the results.

1876

Ramirez, Fowler and Wu
3
3.1

MODEL BUILDING
Model Overview

As described previously, the model built for this research is derived from two papers which describes the operations of a
large-size hospital and an emergency department. The logic for the general flow of patients through the hospital was based on
Cochran and Bharti (2006). This paper provides information about twelve inpatient units, such as capacity, length of stay,
transfer probabilities and non-emergency admissions. On the other hand, the data for the ED was based on Cochran and
Roche (2009). The input data based on this paper comprises the arrival pattern of emergency patients, acuity mix and length
of stay in the ED. In addition, the logic to divert ambulances and patients leaving without treatment are incorporated to the
model. Figure 1 shows the general scheme of the model containing an ED and 12 inpatient units: Pediatrics (PEDS), Pediatric
Intensive Care Unit (PICU), Medical Intensive Care Unit (MICU), Surgical Transition Intensive Care Unit (STICU), Telemetry 1 (TELE 1), Telemetry 2 (TELE 2), Telemetry 3 (TELE 3), Oncology (ONCO), Gynecology (GYN), Orthopedics
(ORTHO), Renal and Diabetic Unit (RENAL) and General Surgery (SURG). There are three general types of arrivals: Emergency Arrivals, which can be by Ambulance or Walk-Ins, Direct Admissions and Surgical Admissions. Transference from
the ED to any inpatient unit, as well as between inpatient units is allowed. In addition to discharges and transfers, there are
another two modes of quitting the ED, one is by leaving without treatment (LWOT) and the other is by being diverted, which
only applies to ambulance arrivals.
TELE 1
RENAL

Ambulance
Diverted
Ambulance
Arrivals
(A)

ED on
Diversion?

yes

ONCO
TELE 2

no
Walk-Ins
Arrivals
(W)

PEDS

Direct
Admissions

ED

Transference?

LWOT

yes

no
Discharge

Surgical
Admissions

SURG
ORTHO
TELE 3
MICU
STICU
GYN
PICU

Figure 1: General flow of a large-size hospital
The patients arriving to the ED by any mode are classified into one of five trauma levels: 1, 2, 3, 4 and 5. Level 1 is the
most severe state and requires immediate attention, while Level 5 are the less ill patients which can wait longer. Actually,
most of patients classified as Level 5 should use another healthcare resource instead of an ED in order to avoid congestion.
The model described before has two versions, the “Regular ED” and the “Fast-Track ED”.
In the “Regular ED” (shown in Figure 2), all patients receive treatment in a bed highly equipped. However, there is a
priority based on the level of trauma to receive attention. Levels 1 and 2 receive high priority, Level 3 has medium priority
and Levels 4 and 5 have low priority. After receiving treatment, patients can be either discharged or transferred to an inpatient unit. If it is determined that a patient needs to be transferred but the recipient unit does not have an available bed, the patient will wait until a bed is free. Meanwhile, the patient occupies the same bed in the ED; this is the boarding state. Note that
patients can renege from the ED queue, meaning that they are leaving without treatment, however, the restriction is that only
patients of Level 3 that arrived walking-in, and all patients of Levels 4 and 5 are allowed to leave the ED without receiving
treatment.

1877

Ramirez, Fowler and Wu

Destroy Entity
yes
Is the ED
on Diversion?

Ambulance
Arrivals
(A)
Walk-Ins
Arrivals
(W)

no

(A)
Assign Level of
Trauma (1-5) and
Sublevel for
Levels 3&4

(W)

* Receive
ED Treatment
based on priority

Wait for
Treatment
In queue

Transfer
to other unit?

LWOT

no

Discharge

yes

Destroy Entity

Is there an available
bed at the recipient
unit?

yes

Transfer
Patient

no
Wait for an
available bed
(boarding)

Figure 2: Regular ED flow
The other version of the model shown in Figure 3 considers a “Fast-Track ED”. In this setting, the patients split according to their level of trauma. Patients requiring urgent care (Levels 1, 2 and 3) will go to the area where a bed is assigned to
receive treatment. At this point, patients of Levels 1 and 2 receive high priority and Level 3 has medium priority. On the other hand, patients with less need will go to a fast-track area where they can wait for a treatment that can be received using different resources, such as chairs or less-equipped beds. After treatment, patients can be discharged or transferred. The rule for
LWOT is the same than in the “Regular ED”.
Destroy Entity
yes
Is the ED
on Diversion?
Ambulance
Arrivals
(A)
Walk-Ins
Arrivals
(W)

no
Destroy Entity

(A)

Assign Level of
Trauma (1-5) and
Sublevel for
Levels 3&4

(W)

LWOT
Level
1,2 or 3?
no

yes

Wait for
treatment in
queue

Wait for
treatment in
Fast-Track
queue
LWOT

* Receive
ED Treatment
based on priority

Receive
ED Treatment

Destroy Entity

Transfer
to other unit?

no

Discharge

yes

Is there an available
bed at the recipient
unit?

yes

Transfer
Patient

no

Figure 3: Fast-Track ED flow

Wait for an
available bed
(boarding)

In some simulation models, LWOT is modeled as a decision made at the entry level of the entity regarding joining or not
the system. However, this logic does not allow those patients to affect the congestion of the system. In the model shown in
this paper, a LWOT routine starts every simulated hour. This routine will look up the number of patients in the queue and
will remove a proportion of the entities according to this state. The more patients the queue has, the more patients will be removed.

1878

Ramirez, Fowler and Wu
The diversion logic works as follows. Every time an ambulance arrives, the diversion status of the ED is observed. If the
current status is on diversion, then the entity will be sent to another hospital, which in the simulation means that it will be destroyed. Otherwise the patient will enter the system and the queue length in the ED is observed to determine if the diversion
state should be set to “ON”. This decision is based on a threshold on the number of patients waiting for treatment. If the diversion state is set ON, it will remain ON for 30 minutes and then a reevaluation is made in the next ambulance arrival. Figure 4 shows the diversion logic.

Ambulance
Arrivals
(A)

Is the ED
on Diversion?

no Admit patient
in the ED

Is there diversion
threshold reached?

yes

yes

Set diversion
state ON

Keep diversion
state ON for
30 minutes

no

Ambulance
Diverted
(Entity destroyed)

Set or keep diversion
state OFF

Figure 4: Diversion logic
3.2

Input Data

Arrival rates for Direct and Surgical Admissions and their inpatient destinations are assumed to follow a Poisson process.
However, the rate of direct admissions depends of the day of the week and time of the day. Table 1 shows the mean arrival
rate per hour of these types of patients.
Day
MTWTF
MTWTF
MTWTF
Sat & Sun

Table 1: Pattern of Direct Arrivals
Time
Mean Arrival Rate
7am - 11am
3.05
11am - 3pm
3.65
3pm - 7pm
2.15
8am - 12pm
1.5

On the other hand, the ED arrival rates have a pattern depending on the hour of the day. The peak time occurs between
9am to 10pm. This pattern will be used in the form of a multiplicative index to obtain the mean arrival rate per hour. The average number of arrivals to the ED per year is 55863 patients, implying an average approximately of 6.4 arrivals per hour before applying the multiplicative index.
Note that it is important to distinguish the arrival mode of the patients, which we are assuming to be of two types: ambulance or walk-in. Some papers (GAO Report 2003; McConnell et al. 2006; Burt, McCaig, and Valverde 2006) show that
about 15% of all arrivals to the ED are made by ambulance. This percentage is used to create the ambulance arrivals stream
and the rest will be part of the walk-ins. Table 2 shows the pattern of arrival rates to the ED used in the model, which are assumed to follow a Poisson process.
Time of the
Day
0am - 3am
3am - 7am
7am - 9am
9am - 10pm
10pm - 12pm

Table 2. Pattern of Arrivals to the ED
Multiplicative Mean Arrival Mean Ambulance
Index
Rate
Arrival Rate
0.6
3.8
0.6
0.4
2.6
0.4
0.84
5.4
0.8
1.32
8.4
1.3
1
6.4
0.9

Mean Walk-In
Arrival Rate
3.2
2.2
4.6
7.1
5.5

The main resources modeled in the hospital are the beds for every unit. The LOS for the non-ED areas depends on the
arrival type of the patient: ED Patient or Non-ED Patient. Table 3 shows the number of beds and mean LOS in every inpatient unit, which is assumed to be exponential as justified in Cochran and Bharti (2006).

1879

Ramirez, Fowler and Wu
Table 3: Number of Beds and LOS in inpatient units
ED-Patients LOS
Non-ED Patients
Unit
Beds
(hours)
LOS (hours)
PEDS
53
59.55
56.11
PICU
12
71.24
57.56
MICU
28
64.2
53.7
STICU
24
131
111
TELE 1
36
72.4
77.9
TELE 2
36
88.2
93
TELE 3
21
62.4
23.1
ONCO
36
88.7
96
GYN
30
62.3
45.6
ORTHO
32
86.7
63.2
RENAL
36
85.5
78.9
SURG
32
91.5
70.1
The length of stay in the ED depends on the level of trauma of the patients. As mentioned before, the levels goes from 1
to 5, however, levels 3 and 4 have sublevels, five and two, respectively. The existence of sublevels is related to the different
paths or requirements that some patients have (lab tests, procedures, behavior health problem, observation, etc.). The estimation of the percentage of patients belonging to different levels of trauma is shown in Table 4 (Burt, McCaig, and Valverde
2006).
Table 4. Percentage of Patients Corresponding to Each Level Based on Arrival Mode
Arrival Mode
Level 1 Level 2 Level 3 Level 4 Level 5
Ambulance
54
24
13
7
2
Walk-In
10
15
27
24
24
The LOS in the ED is assumed to be triangular, except for patients of level 3.2, which have exponential LOS with mean
of 192 minutes. Table 5 shows the parameters involving the LOS for the other kind of patients.
Level
1 and 2
3.1
3.3
3.4
3.5
4.1
4.2
5

Min
60
120
60
70
700
70
70
15

Table 5. LOS in the ED
Mode
Max
Mean (min)
180
600
280.00
180
2400
900.00
90
900
350.00
90
100
86.67
950
1600
1083.33
80
100
83.33
90
100
86.67
22
30
22.33

Std. Dev (min)
115.76
530.47
194.55
6.24
189.66
6.24
6.24
3.06

The last component of the input data is the transfer probability matrix, which is taken from the model in Cochran and
Bharti (2006). This matrix shows the probability that a patient is transferred from the ED to any other unit, as well between
inpatient units. The information of this matrix implies that 17.89% of the ED patients are transferred to an inpatient unit.
4
4.1

PHASE 1 EXPERIMENTATION
Experimental Design

The first part of the experimentation comprises the analysis of the impact of different factors in the performance of the ED.
For this purpose, a screening design of the type 23 is considered with 5 replications for each combination. Table 6 shows the
factors and the values considered for each level.

1880

Ramirez, Fowler and Wu
Table 6. Factors and Levels Considered for Experimentation in Phase 1
Factor
Low Level
High Level
A: ED Setting
Regular
Fast-Track
B: Number of Beds in the ED
30
40
C: Diversion Threshold (number of patients waiting in the ED)
30
40
Note that in the Fast-Track setting it is required to have separate areas for treating highly acute patients and the less ill
ones. In this case, for the low level of factor B, the number of beds in the acute area is 26 and 4 in the fast-track. For the high
level, the number of beds in acute area is 32 and 8 in the fast-track.
The response variables to monitor are directly related to the ED performance, and they are the total number of patients
treated, the waiting time for all the patients and also by priority (Levels 1&2, Level 3 and Levels 4&5), utilization of the ED
beds, number of LWOT patients, LWOT percentage, number of hours on diversion, percentage of time on diversion and the
number of ambulances diverted. In order to reduce the variance of the performance estimator, the Common Random Numbers technique (Law 2007) is used on each different combination of factor levels.
After a pilot experimentation, it was determined that the run length for each replication is 5040 hours (about 7 months),
with the first 504 hours (10% of total run length) used as a warm-up period. Figure 5 shows the number of patients being
treated in the whole hospital for two opposite combinations of factors: all low level and all high levels. This demonstrates
that the warm-up length seems to be appropriate for this model.

400
350
300
250
200
150
100
50
0

Number of Occupied Beds (1,1,1)

Rep 2

400
350
300
250
200
150
100
50
0

Rep 1
Rep 2
Rep 3
Rep 4

90
0
12
00
15
00
18
00
21
00
24
00
27
00
30
00
33
00
36
00
39
00
42
00
45
00
48
00

Rep 5

0
30
0
60
0

Rep 3
Rep 4
Rep 5

Beds

Rep 1

12
00
15
00
18
00
21
00
24
00
27
00
30
00
33
00
36
00
39
00
42
00
45
00
48
00

0
30
0
60
0
90
0

Beds

Number of Occupied Beds (-1,-1,-1)

Time

Time

Figure 5. Determination of the Warm-up Length Period of the Simulation
4.2

Results

The results of the experiments described before are analyzed through ANOVA and according to them, the number of beds in
the ED affects the total number of patients seen, the waiting time of patients of Levels 4 & 5 and the LWOT performance.
The higher the number of beds, the better is the performance on these responses (as expected). On the other hand, the ED setting affects the utilization of the beds. Furthermore, the “Regular ED” has average higher utilization than a “Fast-Track” under similar circumstances.
In regards on the impact on AD, the significant factors were found to be the ED setting and the Diversion Trigger. According to the results, the best combination of factors that reduces the number of hours on diversion and consequently the
percentage of time on diversion is to have a “Fast-Track ED” and the diversion threshold is at the high level. The interactions
of these factors have an impact on the diversion performance as well. Table 7 summarizes the results of Phase 1 experimentation.
Note that the results of some of the performance measures are similar to those published in scholarly papers or media articles for hospitals in similar conditions (Cochran and Bharti 2006; Broyles and Cochran 2009; GAO Report 2003; Press Ganey 2007). Additionally, the number of patients seen added to the number of patients LWOT is close to the average number
of arrivals for six months, using the data shown in Section 3.2.
The adequacy of all the models for all the responses was checked and each of them (constant variance and independent
residuals normally distributed) passed all these assumptions.

1881

Ramirez, Fowler and Wu
Response
Number of Patients Seen
Global Waiting Time
(hours)
Waiting Time Levels 1-2
(hours)
Waiting Time Level 3
(hours)
Waiting Time Levels 4-5
(hours)
Utilization
Total Number of Patients
LWOT
LWOT Percentage
Total Number of Hours
on Diversion
Percentage of Time on
Diversion
Number of Ambulances
Diverted

Table 7. Summary of Results of Phase 1 of Experimentation
Significant
Regression
Mean
Standard
95% LL
Factors
Equation
Deviation
B
23102.53+502B 23102.53
1091.553 20892.79

95% UL

R2

25312.26

0.182

None

2.48

0.43

1.61

3.35

None

0.94

0.56

-0.19

2.07

None

2.35

0.68

0.97

3.72

B
A

4.45-0.4B
90.15-1.5A

4.45
90.15

0.52
2.34

3.40
85.41

5.50
94.89

0.385
0.304

B
B

5818.45-495B
20.13-1.71B
206.47-75.5A
101.1C + 35.5AC
4.2-1.39A -2.1C +
0.6AC

5818.45
20.13

1008.95
3.53

3775.93
12.98

7860.96
27.27

0.202
0.198

206.47

49.02

107.05

305.88

0.888

4.2

0.98

2.21

6.18

0.887

319.45-153.9

319.45

86.82

143.69

495.20

0.768

A, C, AC
A, C, AC
C

Note that the highest R2 found are related to the diversion performance. According to these results, the ambulance diversion is reduced by having a “Fast-Track ED” and setting the diversion threshold to the high level. Taking these actions, the
number of hours on diversion in the six months simulated is reduced in about 141 hours (more than 5 days, or near 3% of all
the time). This provides a good insight about the actions that could be taken in order to improve ED performance. Nevertheless, a deeper analysis is required, especially in the diversion trigger since the value that this threshold can take is not limited
to the ones chosen on this phase.
In order to analyze the effect of choosing the diversion trigger, another experiment phase is conducted, where only the
value of the threshold is modified, while the ED setting and the number of beds is fixed to “Fast-Track ED” and 40 respectively, due to the fact that they provide better results for the hospital being simulated.
5
5.1

PHASE 2 EXPERIMENTATION
Experimentation Design

The second phase of experimentation has the objective of analyzing the impact of several diversion thresholds on the performance of an ED with a fast-track area and subject to 40 beds (32 for the acute area and 8 for the fast-track). The designed experimentation with a single factor comprises 5 levels: 30, 40, 50, 60 and 70. Remember that the diversion threshold unit is
patients waiting for treatment in the ED.
Six replications are run for each level. Antithetic Random Numbers (Law 2007) are used for every consecutive pair of
replications and Common Random Numbers are used between each factor level. The length of the replications and the warmup period are the same as Phase 1 and the duration of each diversion period remains in 30 minutes.
5.2

Results

The results obtained from the experiment done in Phase 2 implies that the diversion trigger threshold has a significant impact
on the global waiting time, the waiting time for patients Levels 1 & 2, the number of patients that LWOT, the LWOT percentage, the number of hours on diversion, the percentage of time on diversion and the number of diverted ambulances. Table 8
shows the summary of the results found in Phase 2 and it includes the 95% Confidence Interval on the mean of the significant
responses, as well as the means that are significantly different using a p-value < 0.05 as a threshold. The lower limits in the
confidence intervals that produced a negative number are fixed to zero.

1882

Ramirez, Fowler and Wu
Table 8. Summary of Results Found in Phase 2 (all times in hours)
Diversion Threshold
Significant Differences
Response
Global
Waiting
Time
Waiting
Time
Levels 1&2
Total
Number
of LWOTs
LWOT
Perrcentage
Total
Hours On
Diversion
Diversion
Percentage
Total
Diverted
Ambulances

95% CI
LL
Mean
UL
LL
Mean
UL
LL
Mean
UL
LL
Mean
UL
LL
Mean
UL
LL
Mean
UL
LL
Mean
UL

1-30
1.42
2.01
2.59
0.00
0.53
1.34
3672.46
4625.50
5578.54
12.71
16.00
19.30
113.76
144.92
176.07
2.26
2.88
3.49
254.19
326.00
397.81

2-40
2.33
2.92
3.51
0.86
1.66
2.46
5577.13
6530.17
7483.20
19.22
22.52
25.82
61.51
92.67
123.82
1.22
1.84
2.46
143.53
215.33
287.14

3-50
1.79
2.37
2.96
0.18
0.98
1.78
5041.46
5994.50
6947.54
17.19
20.49
23.79
0.00
21.17
52.32
0.00
0.42
1.04
0.00
46.50
118.31

4-60
2.06
2.65
3.23
0.79
1.59
2.39
4732.46
5685.50
6638.54
16.22
19.52
22.82
0.00
26.17
57.32
0.00
0.52
1.14
0.00
59.67
131.47

5-70
2.85
3.44
4.03
1.96
2.76
3.56
5560.96
6514.00
7467.04
19.13
22.43
25.73
11.51
42.67
73.82
0.23
0.85
1.46
24.53
96.33
168.14

R2

(pair wise comparison)
1&2, 1&5, 3&5

0.368
1&5, 3&5, 4&5
0.427
1&2, 1&3, 1&5
0.313
1&2, 1&5
0.307

0.659

1&2, 1&3, 1&4, 1&5,
2&3, 2&4, 2&5

0.659

1&2,1&3, 1&4, 1&5,
2&3, 2&4, 2&5

0.652

1&2, 1&3, 1&4, 1&5,
2&3, 2&4, 2&5

It can be seen from the table above that the diversion threshold affects not only the diversion performance of the ED, but
also the waiting time, especially the global and the waiting time for the most acute patients, as well as the LWOT statistics. In
general, it seems that the threshold equals 30 is significantly different from the others. Regarding diversion performance,
thresholds 30 and 40 are significant to the rest of the levels.
Setting the diversion trigger to 30, a better performance in waiting time and LWOT is reached. However, the diversion
performance is the worst among the levels analyzed. This is due to the fact that having a low value for triggering the diversion status helps to avoid the congestion in the ED. Therefore, patients do not have to wait too long to get a bed and less patients will leave the ED before receiving treatment.
On the other hand, the diversion performance improves as the diversion threshold is increased, obviously because it takes
more time to set the diversion status on. However, the mean value of the number of hours on diversion and percentage of
time on diversion increases again after 50. This is likely due to the time required by the system to relief the congestion.
In order to make a decision about diversion threshold, it is necessary to evaluate the problem in a multi-objective fashion. Since correlation exists between global waiting time and waiting time for Levels 1&2, number of patients LWOT and
LWOT percentage, and number of hours on diversion and percentage of time on diversion, the following analysis will be
based only on the percentage of time on diversion, the waiting time for the most acute patients and the LWOT percentage.
Figure 6 shows the objective space using a bi-criteria graph.
Looking at the results, there seems to exists a tradeoff between diversion performance and other measures, such as waiting time and LWOT. Furthermore, not all the diversion thresholds provide good responses. For instance, it can be seen from
the figure shown above, that if the percentage of time on diversion and the waiting time of the acutest levels of trauma are
considered, the supported Pareto Optimal solutions for this specific case are: 50 and 30. If LWOT percentage is considered
instead of waiting time, then the supported Pareto Optimal are 50, 60 and 30.

1883

Ramirez, Fowler and Wu
3.0

23

40

70

70

2.5
21
LWOT Percentage

Waiting Time of Levels 1&2

22

2.0
40

60

1.5

50

20

60

19
18

50

1.0

17
30

0.5
0.5

1.0

1.5
2.0
Percentage of Time on Diversion

2.5

30

16

3.0

0.5

1.0

1.5
2.0
Percentage of Time on Diversion

2.5

3.0

Figure 6. Objective Space Considering Percentage of Time on Diversion versus Waiting Time of Levels 1&2 (left) and
LWOT Percentage (right)
Choosing a threshold that minimizes the waiting time and the LWOT percentage will cause that the time on diversion increases (threshold = 30). However, minimizing the percentage of time on diversion does not provide the maximum waiting
time or LWOT percentage (thresholds = 50 or 60). Actually, the performance of those values does not seem to be bad at all.
In order to consider the variation in the results, another similar graph is shown in Figure 7, considering now the mean value
for each replication at each level.
Analyzing this graph, there is a clear difference in the variation degree for each level. The results that are closer to the
origin (thresholds 50 and 60) have means closer to each other than the other thresholds (30, 40 and 70). This indicates that for
this model, under the conditions considered, thresholds of 50 and 60 provide good performance on diversion, LWOT and
waiting time, which also provides a small variation, allowing a better prediction capability.

Waiting Time of Levels 1&2

5
4
3
2

Div ersion
Threshold
30
40
50
60
70

30

LWOT Percentage

Div ersion
Threshold
30
40
50
60
70

6

25

20

15
1
10

0
0

1

2
3
Percentage of Time on Diversion

4

0

5

1
2
3
Percentage of Time on Diversion

4

5

Figure 7. Objective Space Considering Percentage of Time on Diversion versus Waiting Time of Levels 1&2 (left) and
LWOT Percentage (right) taking into account the mean of each replication
So far, the boarding problem has not been analyzed on this problem, however, literature suggests that there is a correlation between the number of patients boarding in the ED and the diversion status (Burt and McCaig 2004). In order to analyze
this situation, a figure of the number of patients boarding when it was decided to set the diversion status on is built, and it is
shown in Figure 8.
The last figure indicates that there is a high number of patients boarding in the ED waiting for a bed in an inpatient unit
when the diversion status is set on. This might be one root cause of the congestion of the ED that finally contributes not only
to the diversion, but also to leaving without treatment and having long waiting times. However, the boarding problem is not
caused by all the inpatients units. According to the graph shown in the right hand side of Figure 8, most of the patients boarding in the ED are waiting to be transferred to Oncology, Renal, Surgery or Telemetry 1. Therefore, a global solution should
include some actions on the capacity of those units.

1884

Ramirez, Fowler and Wu

Number of boarding patients during diversion
episodes when diversion threshold = 50
Boarding Patients
Rep 1
Rep 2
Rep 3
Rep 4
Rep 5

1

5

7
6

30
40
50
60
70

5
4
3
2
1
0

G
YN
M
IC
U
O
N
C
O O
R
TH
O
PE
D
S
PI
C
U
R
EN
AL
SI
C
U
SU
R
G
TE
LE
1
TE
LE
TE 2
LE
3

Rep 6

Avg. Patients Boarding

40
35
30
25
20
15
10
5
0

Average Patients Boarding

9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69
Diversion Episodes

Target Unit

Figure 8. Number of Patients Boarding During Each Diversion Episode When Threshold = 50 (left) and Average Number of
Patients Boarding According to their Target Unit for all Thresholds (right)
6

CONCLUSIONS AND FUTURE RESEARCH

This paper analyzes the performance measures in the ED of a large-size hospital, using data provided by published papers.
Furthermore, the impact of the ED setting, the number of beds and the diversion threshold were studied using a simulation
model. Results show that a hospital with the characteristics modeled performs better with a fast-track area dedicated to less
acute ED patients.
However, the most interesting findings rely on the effect of the diversion threshold on the performance of the ED. The
results suggest that there is a trade-off between diversion performance and other measurements, such as waiting time and
LWOT. This implies that in order to reduce congestion of the system, patients will have to avoid joining the system either by
diversion or leaving before treatment. Nevertheless, it seems that a diversion threshold that can provide a good performance
over other policies and that also produces consistent behavior exists.
Due to the lack of studies on diversion policies, this part of the research will serve as the basis to design a methodology
to obtain an appropriate threshold that can improve the performance of the emergency system under a diversity of situations.
Note that one of the advantages of this experimentation is the robustness of the model built, since it includes a wide variety of inputs, path flows, interaction among departments and seasonal patterns. This level of detailed is not common in
similar models found in the literature. Therefore, the possibility to characterize different types of hospitals into a model with
this level of detail is a challenge that will be part of the future research.
REFERENCES
Asplin, B.R. 2003. Does ambulance diversion matter?. Annals of Emergency Medicine/Editorial. 2003. 41:477-480.
Associated Press. 2006. Press Report: ER care in US at 'breaking point'. Available via
<http://www.msnbc.msn.com/id/13320317/from/ET/> [accessed May 8, 2009].
Brailsford, S.C. 2008. System dynamics: What’s in it for healthcare simulation modelers. In Proceedings of the 2008 Winter
Simulation Conference, ed. S.J. Mason, R.R. Hill, L. Monch, O. Rose. T. Jefferson, and J.W. Fowler. 1478-1483. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, Inc.
Broyles, J.R., and J.K. Cochran. 2007. Estimating business loss to a hospital emergency department from patient reneging by
queuing-based regression. In Proceedings of the 2007 Industrial Engineering Research Conference, ed. G. Bayraksan,
W. Kin, Y. Son, and R. Wysk.
Burt, C.W., and L.F.McCaig. Staffing, capacity and ambulance diversion in emergency departments: United States, 20032004. Advance Data from Vital and Health Statistics of the CDC.
Burt, C.W., L.F. McCaig and R.H. Valverde. 2006. Analysis of ambulance transports and diversions among US emergency
departments. Annals of Emergency Medicine.47:317-326.
Cochran, J.K., and A. Bharti. 2006. A multi-stage stochastic methodology for whole hospital bed planning under peak loading. International Journal of Industrial and Systems Engineering. 1:8-36.
Cochran, J.K., and K.T. Roche. 2009. A multi-class queueing network analysis methodology for improving hospital emergency department performance. Computers and Operations Research. 36:1497-1512.

1885

Ramirez, Fowler and Wu
Ferrin, D.M., M.J. Miller, and D.L. McBroom. 2007. Maximizing hospital financial impact and emergency department
throughput with simulation. In Proceedings of the 2007 Winter Simulation Conference, ed. S.G. Henderson, B. Biller,
M.-H. Hsieh, J. Shortle, J.D. Tew, and R.R. Barton 1566-1573. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, Inc.
Huang, D.T. 2004. Clinical review: Impact of emergency department care on intensive care unit costs. Review in Critical
Care. 8:498-502.
Kolb, E.M.W., J. Peck, S. Schoening, and T. Lee. 2008. Reducing emergency department overcrowding-five patient buffer
concepts in comparison. In Proceedings of the 2008 Winter Simulation Conference, ed. S.J. Mason, R.R. Hill, L. Monch,
O. Rose. T. Jefferson, and J.W. Fowler. 1478-1483. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, Inc.
Law, A.M. 2007. Simulation modeling & analysis. 4th ed. McGraw-Hill, Inc.
McConnell, K.J., C.F. Richards, M. Daya, C.C. Weathers, and R.A. Lowe. 2006. Ambulance diversion and lost hospital revenues. Annals of Emergency Medicine. 48:702-710.
Medeiros, D.J., E. Swenson, and C. DeFlitch. 2008. Improving patient flow in a hospital emergency department. In Proceedings of the 2008 Winter Simulation Conference, ed. S.J. Mason, R.R. Hill, L. Monch, O. Rose. T. Jefferson, and J.W.
Fowler. 1478-1483. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, Inc.
National Academy of Engineering and Institute of Medicine. 2005. Building a better delivery system. A new engineering/health care partnership. The National Academies Press.
Press Ganey Associates. 2007. Emergency Department Pulse Report.
Roche, K.T., and J.K. Cochran. 2007. Improving patient safety by maximizing fast-track benefits in the emergency department-a queuing network approach. In Proceedings of the 2007 Industrial Engineering Research Conference, ed. G. Bayraksan, W. Kin, Y. Son, and R. Wysk.
Ramirez, A., J.W. Fowler , and T. Wu. 2009. Modeling of regional healthcare delivery networks using distributed simulation.
In Proceedings of the 2009 Industrial Engineering Research Conference, ed. J. C. Smith, and J. Geunes.
United States General Accounting Office. 2003. Hospital Emergency Departments. Crowded conditions vary among hospitals and communities.Report to US Senate.
Upfold, J. 2002. Emergency department overcrowding: ambulance diversion and the legal duty to care. Commentary in the
Canadian Medical Association Journal. 166:445-446.
US
Department
of Health
and
Human
Services,
National
Health
Expenditure.
Available
via
<http://www.cms.hhs.gov/NationalHealthExpendData/> [accessed May 4, 2009].
AUTHOR BIOGRAPHIES
ADRIAN RAMIREZ is a PhD student in the Department of Industrial, Systems and Operations Engineering at Arizona
State University. His research interests include modeling, simulation and analysis of healthcare delivery systems. He received
a MS in Manufacturing Systems at ITESM and a BS in Industrial Engineering at Universidad de Sonora, both in Mexico. His
email address is <adrian.ramirez@asu.edu>.
JOHN W. FOWLER is a Professor in the Operations Research and Production Systems group of the Department of Industrial, Systems and Operations Engineering at Arizona State University. His research interests include modeling, analysis, and
control of manufacturing and service systems. He is a Fellow of the Institute of Industrial Engineers and is the SCS representative on the Board of Directors of the Winter Simulation Conference. He is an Area Editor of the Transactions of the Society for Computer Simulation International, an Associate Editor of IEEE Transactions on Semiconductor Manufacturing,
and will be the Editor of a new journal entitled IIE Transactions on Healthcare Systems Engineering. His email address is
<john.fowler@asu.edu>.
TERESA WU is an Associate Professor of the Department of Industrial, Systems and Operations Engineering at Arizona
State University. She received her Ph.D. in Industrial Engineering from the University of Iowa in 2001. Her current research
interests include: distributed decision support, distributed information system, supply chain modeling and disruption management. Professor Wu has over 30 articles published (or accepted) in such journals as International Journal of Production
Research, Omega, Data and Knowledge Engineering and ASME: Journal of Computing and Information Science in Engineering, IEEE Transactions on Engineering Management. She serves on the Editorial Review Board for International Journal
of Production Research, IEEE Transactions on Engineering Management, Computer and Standard Interface, International
Journal of Electronic Business Management. Her email address is <teresa.wu@asu.edu>.

1886

Information Fusion 14 (2013) 487–497

Contents lists available at SciVerse ScienceDirect

Information Fusion
journal homepage: www.elsevier.com/locate/inffus

Fuzzy clustering based ET image fusion
Shihong Yue a,⇑, Teresa Wu b, Jian Pan a, Huaxiang Wang a
a
b

School of Electrical Engineering and Automation, Tianjin University, Tianjin 300072, China
School of Computing, Informatics, Decision Systems Engineering, Arizona State University, USA

a r t i c l e

i n f o

Article history:
Received 24 July 2012
Received in revised form 29 August 2012
Accepted 25 September 2012
Available online 16 October 2012
Keywords:
Image fusion
Electrical tomography
Modality
Driven pattern

a b s t r a c t
Electrical tomography (ET) is a technique to visually reconstruct inhomogeneous medium distributions
by injecting currents or voltages at the boundary of the medium and measuring the resulted changes
in the investigated ﬁelds. The ET techniques have been widely used in industrial practices owing to
the low cost, rapid response time, non-existent radiation exposure, and non-intrusive characteristics
comparing to other tomographic modalities. However, the spatial resolution of ET images using single
modality or single-driven patterns (adjacent pattern vs. opposite pattern for imaging reconstruction) is
low, which may limit its applications. In this research, the application of fuzzy clustering based fusion
techniques for ET imaging is studied. Both multi-modality imaging and multi-driven patterns are of interest. Speciﬁcally, two modality images are fused: Electrical Capacitance Tomography (ECT), which performs well for imaging material of large permittivity difference, and Electrical Resistance Tomography
(ERT), which is suited for imaging materials having large conductivity differences. The research also
explores the fusion of adjacent and opposite patterns for either ECT or ERT modalities. Experiments show
that the proposed method can construct high quality ET images by discovering the strong complementary
natures of the modalities and/or driven patterns.
Ó 2012 Elsevier B.V. All rights reserved.

1. Introduction
Electrical tomography (ET) techniques [1,2] have rapid development over the past decades as an advanced measurement and visualization tool. The techniques can recover permittivity or
conductivity distributions by measuring boundary data over the
investigated ﬁeld and map the distributions to the gray space to recover the material distributions in the form of reconstructed
images. Several known advantages of ET over other tomographic
techniques include low-cost, rapid response, portability, noninvasiveness and robustness [3,4].
Originally, ET systems were of single modality, such as Electrical
Capacitance Tomography (ECT) [1,5], Electrical Resistance Tomography (ERT) [6,7] or Electromagnetic Tomography (EMT) [8]. Since
single modality ET systems are known to render low spatial resolution images, multi-modality measurements for ET imaging
reconstruction have become attractive lately. However, multimodality ET systems are not without challenges. One obstacle is
that the electrode arrays from different modalities in these systems
result in noise and errors caused by crosstalk [5,8]. A simple combination of such multi-modality images without optimally taking
advantage of the complementary nature of each may just lead to
even lower resolution. Therefore, data fusion is proposed as an
⇑ Corresponding author.
E-mail address: shyue1999@tju.edu.cn (S. Yue).
1566-2535/$ - see front matter Ó 2012 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.inffus.2012.09.004

approach that optimally considers the contributions from different
modalities. Data fusion has been successfully applied in many
applications such as target tracking, navigation, robotics [9], and
more recently, in disease diagnosis [10]. Cui et al. [11], who have
proposed data fusion of multiple sensor modalities for ET imaging,
found that the measurements were considerably improved by
combining data from individual sensors, thereby providing complementary information.
For any single modality ET, the data acquisition and the reconstructed image quality are impacted by the applied current or voltage driven patterns (e.g., adjacent driven pattern, opposite driven
pattern). A good current or voltage driven pattern should have
the following advantages: First, the boundary measured values
must be of as large dynamic range as possible in order to distinguish different objects for high-resolution imaging. Secondly, for
imaging construction, independent measurements should be obtained in the pattern to minimize the redundancy and possible
noises. Thirdly, even minor conductivity or permittivity changes
in objects should result in large boundary measure value changes
in order to achieve improved sensitivities. Researchers have proposed several current or voltage driven patterns, of which most
commonly used are adjacent and opposite driven patterns [12].
Altincay and Demirekler [13] presents an adaptive optimal current
pattern that requires exciting many electrodes and then measuring
their boundary voltages. Zhang and Wang [14] studies eight driven
patterns that provide better image resolution, concluding that

488

S. Yue et al. / Information Fusion 14 (2013) 487–497

adjacent and opposite driven patterns are the easiest to implement
with good performance.
Though notable efforts have been spent on improving ET imaging resolutions either via multi-modality [11] or various driven
patterns [12–14], research thus far has been mainly focused on
hardware development that supports multi-modality acquisition
[15,16]. There exists limited research on imaging construction
algorithms, among which only binary images are of the focus due
to the simplicity. One obvious weakness of the binary image application is that the rich details available from ET are thus lost during
the ﬁnal imaging process.
In this research, we propose an integrated research framework:
An ECT image and an ERT image rendered from adjacent and opposite patterns are registered in the same space. A fuzzy clustering
algorithm is then applied to construct an ET image of high quality.
Speciﬁcally, each investigated object is considered as an independent classiﬁer, the task of fusing a group of ET images then
becomes a typical classiﬁer fusion problem. Based on the representation of the fuzzy clustering, we investigate the uncertainty and
fuzziness hidden in all individual information sources, including
the importance or the interaction of their contributions. The
experimental results show that our proposed method effectively
improves the image resolution in the computed capacitance
techniques.
The rest of the paper is organized as follows. In Section 2, we
provide an overview of the related work, including a mathematical
description and some basics of the various modalities and driven
patterns followed by the proposed algorithm described in Sections
3. The comparison experiments and results analysis are detailed in
Section 4. The conclusion is drawn in Section 5.

reconstruction is to ﬁnd the unknown g from the known r by using
Eq. (3), that is

r ¼ S1 U

ð3Þ

However, a direct analytical solution for Eq. (3) does not exist since
the inverse problem is both non-linear and ill-posed; moreover,
minimal noise in the measured data could cause large errors in
the estimated conductivity. Consequently, it is necessary to use
numerical techniques to approximate S1 as accurately as possible
after applying some residual criteria. One example criteria is minimal least error deﬁned as

jjU  Srjj22 ! min

ð4Þ

Eq. (4) is used to identify the optimal values of r. Many variants of
Eq. (4) have been proposed to solve the ill-posed problems, among
which the two common ones are the linear back projection (LBP)
[17] and Landweber iteration [18] methods.
In the LBP algorithm, the conductivity or the permittivity distributions are assumed to comprise a number of discrete regions
within the measurement space and the conductivity within each
region is constant. According to Eq. (3),

S1 ¼ ST k=ST U k ;

s:t: U k ¼ ½1; 1; . . . ; 1

Eq. (5) shows that the gray values of any pixel is calculated using a
weighted form, and k is a unit vector in the algorithm.
The Landweber iteration method was originally designed for
solving the classical ill-posed problem using a strategy similar to
the gradient descending algorithm in the optimization process by
the following equation:

b kþ1 ¼ G
b k  aST ðS G
b k  kÞ
G

2. Problem background and related work

ð5Þ

ð6Þ

This section illustrates a mathematical description of ET image
reconstruction, modality, and driven pattern, as well as the two
most used ET image reconstruction algorithms – the linear back
projection (LBP) [17] and Landweber iteration [18] methods.

where the constant a is known as the gain factor and is used to control the convergence rate. As the iterative process described by Eq.
(6) proceeds, the residual values of Eq. (14) will be minimized. Since
the residual may tend to be a certain value larger than zero, the original algorithm is often modiﬁed as

2.1. The ET imaging principle

b kþ1 ¼ P½ G
b k  aST ðS G
b k  kÞ
G

The task of ET imaging reconstruction is to determine permittivity distributions using ECT or conductivity distributions using
ERT, and hence material distributions over a cross-section based
on the electrical measurements. According to the electrostatic
analogy principle [19], ERT and ECT have the same mathematical
representation, but with different physical meanings. Thus we explain ERT only as the following:
In a basic two-dimensional case, the relationship between the
spatial distribution of the conductivity and its electric potential
distribution can be derived from Maxwell’s equation [19]. Based
on the ﬁnite element method (FEM) [20], the linearized and discrete form of Maxwell’s equation can be expressed as

DM1 U ¼ JMN  DN1 r

ð1Þ

where J is a Jacobian matrix, i.e., the sensitivity distribution matrix,
giving a sensitivity map for each electrode pair, M and N, which are
the number of measurements and elements (pixels), respectively.
The normalized form of Eq. (1) is as Eq. (2),

U ¼ Sr

ð2Þ

where r is the normalized voltage vector or capacitance vector, S is
the Jacobian matrix of normalized capacitance with respect to normalized permittivity, i.e., the transducer sensitivity matrix, and r is
the normalized permittivity vector corresponding to the gray level
of pixels for visualization. In the discrete form, the aim of imaging

ð7Þ

The value of P has been adopted by the inclusion of a non-linear
function f to constrain the estimated investigated ﬁeld so that
b kþ1 2 ½0; 1; i.e., when a normalized gray level is less than ‘‘0’’, it
G
is constrained to be ‘‘0’’, when it is larger than ‘‘1’’, it is constrained
to be ‘‘1’’.
In this paper, we use the LBP and Landweber algorithms for ET
imaging construction for each modality and/or driven pattern.
2.2. Comparisons of ECT and ERT systems
ECT electrodes in an ECT data acquisition system are normally
mounted outside a pipe or vessel to measure a series of capacitance
data when applying a group of voltage excitations – Us (shown
Fig. 1a). The ECT system is known to be non-intrusive and noninvasive [6,11].
Similar to an ECT system, ERT electrodes in an ERT data
acquisition system are mounted inside a pipe or vessel to measure a series of voltage data when applying a group of current
excitations – Is (see Fig. 1b). The ERT sensor is non-intrusive, but
invasive. As seen from Table 1, both ECT and ERT are governed
by the same equation forms, the forward and inverse problems
for both ECT and ERT modalities have similar features.
To ﬁnd a solution to the inverse problem, sensitivity matrices
need to be calculated for both modalities. The similarities in mathematical models lead to the conveniences in calculating sensitivity
matrices SC for ECT and SR for ERT, as shown in the last row in

S. Yue et al. / Information Fusion 14 (2013) 487–497

489

Fig. 1. Excitation and measurements of two most used ET modalities: (a) ECT and (b) ERT.

M ¼ nðn  3Þ=2

Table 1
Mathematical comparison of ECT and ERT.
ECT

ERT

Note

E ¼ ru

E ¼ ru

E – Electrical ﬁeld intensity
u – Electric potential

r  ðe  ruÞ ¼ 0

r  ðr  ruÞ ¼ 0

e – Permittivity
r – Conductivity

H
C¼

S

H

eEdS

C¼

U

SCi;j ðx; yÞ ¼

R

r /i
S Ui



r/j
Uj

ds

S

G – Conductance
C – Capacitance

rEdS
U

SRi;j ðx; yÞ ¼

R

r /i
S Ii



r/j
Ij

ds

U – Potential difference

Table 1, where i and j are the indices of the pixel, respectively, in
rows and columns in a reconstructed image.
The ECT is good for the imaging materials of the large permittivity differences, and has been used to measure two-phase ﬂows of
materials with different permittivity values, such as gas–oil and
gas–solids. The ERT performs well for the imaging materials having
large conductivity differences, and has been used to measure twophase ﬂows of materials with different permittivity values, such as
for medical imaging purposes.
2.3. Typical current driven patterns
For both the ECT and the ERT, there are various driven patterns
used in image construction. Two commonly used driven patterns
are adjacent and opposite patterns [14,15] (Fig. 2).
To illustrate the adjacent driven pattern for the ERT modality,
we apply (without the loss of generalization) 16 electrodes that
are evenly distributed around a 16-cm-radius pipe. Fig. 2a and c
shows the distribution of electrodes in our experiments and the
adjacent exciting strategy used for data collection. First, the exciting current I is added to electrode pair (1, 2), and the 15 voltage
values of (2, 3), (3, 4), . . . , (16, 1) are measured. Next, the exciting
current is added to (2, 3), and the voltages of (3, 4), (4, 5), . . . , (1, 2)
are measured from successive pairs of neighboring electrodes and
so on. Finally, the exciting current is added to (16, 1), and the voltages of (1, 2), (2, 3), . . . , (15, 16) are measured from successive
pairs of neighboring electrodes. After excitation, the electrode pair
is switched 16 times, and 15 groups of measurements are obtained,
among which two measurements including the excitation electrode pairs, need to be discarded due to large errors. Thus, 13
groups of measurements in each excitation are used for image construction. Considering the reciprocity of electrode distributions, a
total of 104 independent values are used to extract the internal
conductivity information [11,15], which is

ð8Þ

where n is the number of measuring electrodes [11] used. Similar to
the adjacent driven pattern, in the opposite driven pattern that uses
the 16 electrode, the exciting current I is added to electrode pairs (1,
9), (2, 10), (3, 11), (4, 12), (5, 13), (6, 14), (7, 15), and (8, 16) in the
stated order. A total of 96 independent valves are obtained for
imaging construction [15], that is,

M ¼ nð3n=2  1Þ=4

ð9Þ

Fig. 2b and d shows that the corresponding voltage curves in the
above patterns are very different; one constructs a ‘‘U’’ shape (adjacent driven pattern) and the other is a double ‘‘U’’ shape (opposite
driven pattern) when the imaging ﬁeld has no object. In the case
when any uneven distributed materials is added into the investigated ﬁeld, the adjacent pattern applies an irregular increment of
the ‘‘U’’ type for the permittivity distribution while the opposite pattern presents one or both irregular increments of the double ‘‘U’’ for
the permittivity distribution. For the same number of electrodes the
adjacent pattern provides denser isopotential line distributions in
the imaging boundary ﬁeld than in the opposite pattern. Observing
the complementary characteristics, an intelligent algorithm that
optimally utilizes the information from ECT or ERT that is constructed via different driven patterns in order to identify objects
from different ﬁelds is of urgent need for high-quality ET imaging.
2.4. Information fusion methods for tomographic reconstruction
Tomographic reconstruction has a wide range of applications.
With the development of multi-sensor systems in recent decades,
information fusion methods for tomographic reconstruction are
being increasingly studied for better image understanding [6,21].
These different sensors that recover different characteristics of
the investigated objects, can be incorporated into fusion methods,
thus improving the spatial resolution and reducing noises. These
methods can be roughly divided into three categories. The ﬁrst category is to incorporate prior information of the investigated objects
into the image reconstruction process [22,23]. In many cases the
use of prior information helps improve the reconstructed image
spatial resolution. The second group uses the physical model of
equations, such as the wavelet method along with the ﬁltering process [24,25]. This class of methods may be very efﬁcient when the
used model presents the real and dynamic process. The third class
uses an advanced information fusion method, such as the Dempster–Shafer evidence theory method [26,27]. These novel image
reconstruction methods use the concept of sensor fusion and have
been validated by computer simulation studies. Results from these

490

S. Yue et al. / Information Fusion 14 (2013) 487–497

(a) Adjacent pattern

(b) Voltage measure curve

(c) Opposite pattern

(d) Voltage measure curve

Fig. 2. Typical driven patterns and corresponding voltage measure curve for 16 equally spaced electrodes.

studies indicate that signiﬁcant improvements in image quality
can be achieved, such as the correlated CT and magnetic resonance
images [28].
We note methods reviewed above have greatly improve the ET
imaging. However, most methods assume that either sufﬁcient
measurements can be obtained, or require extensive computational effort for imaging construction which limit the realistic
application for ET imaging construction. In addition, most existing
methods focus on developing new hardware systems or new imaging techniques [6,29]. To our best knowledge, research on developing construction algorithm is limited. In the case of multi-sensor
information, only binary imaging is of interest mainly due to its
simplicity. For example, a representative study is Qiu et al.’s
ERT–ECT [21] double-modality image fusion method, where the
ECT and ERT data have been reconstructed separately. The processed data is then set at a threshold to produce two binary
images. The threshold level is set by the user and is applicationdependent. The data is, therefore, segmented and fused by simply
using the ERT binary-image reconstruction to determine the different material content. Such information is used to further segment
the ECT image into several regions. Since there is prior knowledge
of the number of components contained in the image, the main
task is to calculate the hold-up percentage of different materials.
The major drawback limiting the application is the method may
ignore most useful information hidden in different information resources. In this paper, we propose a new image fusion method aiming to address these issues.
3. Fusion of ET images under integrated modalities and driven
patterns
This section explains the elements of our proposed method:
fuzziness of the gray degree in ET imaging, matching of a group
of fused ET images, and fuzzy clustering based fusion rules.

3.1. Fuzziness of the gray degree
The Bezdek’s fuzzy c-means (FCM) [30] algorithm is one of the
most efﬁcient techniques to deal with uncertain and vague information, and applies the following objective function:

min JðU; VÞ ¼

c X
n
X
2
ðuij Þm dij ;

s:t:

i¼1 j¼1

c
X

uij ¼ 1; 0 <

i¼1

n
X
uij 6 n

ð10Þ

j¼1

where c is the user-speciﬁed number of clusters, n is the number of
clustered patterns, m a fuzziness exponent, efﬁciently taken in the
range: 1.5–2.5, dij ¼ jjxj  v i jj; uij the membership degree of j-th pattern to i-th cluster, vi is the prototype of i-th cluster and xj is the j-th
pattern, j = 1, 2, . . . , n. The optimal membership and the prototype
functions of Eq. (10) is

uij ¼

c
X
2=ðm1Þ
2=ðm1Þ
dij
=drj

!1
and

vi ¼

r¼1

n
n
X
X
uij xj = uij
j¼1

ð11Þ

j¼1

All fuzzy membership degrees consist of a n  c fuzzy partition matrix U = [uij].
Many extended versions of the FCM algorithm have been developed for image processing [31,32]. For example, a fast FCM (f-FCM)
algorithm is used in [32]. The algorithm is based on a 1-dimensional attribute such as the gray level. Each pixel has a feature that
lies in the discrete gray level set X = {0, 1, . . . , L1}. Let H(l) be the
number of gray levels when partitioning the investigated ﬁeld into
L gray levels, where L is the number of gray levels. In this case, each
element of the data set represents a gray level value; uil = ui(l) representing the membership degree of the l-th gray level for the i-th
cluster (i = 1, 2, . . . , c). The objective function in the f-FCM algorithm is:

J m ðU; V; LÞ ¼

L1 X
c
X
ðuil Þm  HðlÞ  ðl  v i Þ2
l¼0 i¼1

ð12Þ

S. Yue et al. / Information Fusion 14 (2013) 487–497

The computation of membership degrees of H(l) pixels is reduced to
that of a pixel with l as the gray level value. Let

E¼

c X
L1
X
juil  uil j

ð13Þ

i¼1 l¼0

The algorithm selects the number of clusters c, and updates the partition matrix U and prototype vector V according to

vi ¼

L1
L1
X
X
ðuil Þm  HðlÞ  l= ðuil Þm and uij
l¼0

"

l¼0

1=ðm2Þ #1
c 
X
l  vi
¼
;
l  vj
i¼1

i ¼ 1; 2; . . . ; c; l ¼ 1; 2; . . . ; L

ð14Þ

when E is smaller than a user-speciﬁed threshold, the algorithm
stops. Each pixel in the ET image is assigned to the fuzzy memberships used for data fusion. When applying the fuzzy clustering algorithm, the number of clusters c, needs to be predeﬁned. There are
two applicable ways to determine the number of clusters. In most
cases, there is a prior knowledge of the actual number of clusters
since the multiphase ﬂows in a measured ﬁeld consists of known
components such as the gas, water, and oil in a crude oil transmission pipe. We will choose the number of clusters larger than three
since the ET imaging inevitably contains the trail traces that have
to be represented by some additional clusters of more than three.
In the cases no prior number of clusters available, the issue of determining the number of clusters fall into the category of clustering
validity indices in a given dataset [33], i.e., the number of clusters
can be determined by a proper function called the clustering validity index [33,34]. There are some efﬁcient validity indices in fuzzy
clustering such as the partition entropy (PE) [35], the Xie-He index
[36]. In this paper, we focus on the fusion method assuming the real
number of clusters is known as a prior.
3.2. Fuzzy clustering based multi-modality ET image fusion
Before fusing a group of images, pixel matching has to be applied. Let us assume that there are Q reconstructed ET images to
fuse I1 ; I2 ; . . . ; IQ ; corresponding to Q pixel sets X1 ; X2 ; . . . ; XQ ,
respectively. We also assume the images are uniformly distributed
as circle-shaped image ﬁelds (see Fig. 3). The fast fuzzy clustering
leads to c clustering prototypes at the q-th reconstructed image
v q1 ; v q2 ; . . . ; v qc ; q ¼ 1; 2; . . . ; Q .
Order
v q1 ; v q2 ; . . . ; v qc
to
v q1 ; v q2 ; . . . ; v qc in decreasing order by their conductivities or permittivity distributions hidden in these prototypes ; q ¼ 1; 2; . . . ; Q :
The prototypes of any s-th and t-th images are matched in the following equation:

v is ! v it ; i ¼ 1; 2; . . . ; c; s; t ¼ 1; 2; . . . ; Q

ð15Þ

491

All prototypes in any two images are matched one-to-one from the
largest to smallest by their permittivity or conductivity values (see
Fig. 3c). Since any membership degree of any pixel in an ET image is
determined by their clustering prototypes, all pixels can be
matched and compared after the clustering prototypes are matched.
Given each modality or driven pattern corresponding to a classiﬁer on the set of pixels, different classiﬁers may lead to different
partitions of X. Let F 1 ; F 2 ; . . . ; F Q be Q classiﬁers that respond to Q
fused images I1 ; I2 ; . . . ; IQ ; uqij be the membership of the j-th pixel
xj to the i-th cluster after applying fast fuzzy clustering in q-th classiﬁer, i ¼ 1; 2; . . . ; c; j ¼ 1; 2; . . . ; n, and q ¼ 1; 2; . . . ; Q . As a result,
~q ¼ ðuq ; uq ; . . . ; uq ÞT for
any pixel has a membership vector U
s1 J
s2 J
j
sc j
j ¼ 1; 2; . . . ; N; q ¼ 1; 2; . . . ; Q , and i ¼ 1; 2; . . . ; C. uqs1 J ; uqs2 J ; . . . ; uqsc j is
a rearranging of uq1j ; uq2j ; . . . ; uqcj such that s1 ; s2 ; . . . ; sc is one-to-one
responding to v q1 ; v q2 ; . . . ; v qc . The goal of the proposed fusion
method is to determine the probabilities of the ﬁnal gray degree
of any pixel based on different memberships of the pixel. This
probability function is computed after following an intuitive
example.
For a fuzzy set, the most uncertain case of the belongingness of
a member to the fuzzy set is encountered in which the membership of the pixel is 0.5. This case is further extended to 2-dimensional cases as follows. Fig. 4 shows a three cluster-contained
dataset with 100 data points. After applying fuzzy clustering by
the maximal membership assignment principle, all data points
are partitioned into three clusters with signs ‘‘s’’, ‘‘’’, and ‘‘+’’.
As shown in Fig. 4, point A is close to the ‘‘s’’ cluster prototype
(center); its membership to the ‘‘s’’ cluster is much stronger than
that of the other two clusters. As a result, point A is partitioned to
the red cluster with certainty. Considering point B, it has equal distance to all three cluster prototypes (centers), and its memberships
to the ‘‘s’’, ‘‘’’, and ‘‘+’’ clusters are non-differentiated. It is possible that a vector (1/3, 1/3, 1/3) is used to represent the belongingness and certainty of data point B, and the point A will have (1, 0,
0), indicating the belongingness to ‘‘s’’, ‘‘’’, and ‘‘+’’ clusters.
Let P1=J ¼ fðp1 ; p2 ; . . . ; pc Þjpi ¼ 1=c; j ¼ 1; 2; . . . ; cg be an evaluation vector; the larger the differences between the values, the more
distinct the membership of the pixel. The degree of certainty of any
pixel in the fuzzy clustering is then deﬁned as

rqj ¼ 1  dðuqj ; P1=J Þ; j ¼ 1; 2; . . . ; n; q ¼ 1; 2; . . . ; Q

ð16Þ

where d is an arbitrary distance measure between two vectors. Note
that r is not a probability measure and its lower limit is varied
according to the dimension of vectors (clusters). Fig. 4 shows the
computed possibility values of points A and B to the P1/3 according
to Eq. (16). Given r and any pixel xj ; j ¼ 1; 2; . . . ; N, a weighted average method can be applied to integrate all available classiﬁers and
determine the ﬁnal fused image. That is

Fig. 3. Image matching for a group of ET images: (a) circle-shaped image ﬁelds of any ET image; (b) Q fused images with the same partitioning number; and (c) matching of Q
ET images by their clustering prototypes.

492

S. Yue et al. / Information Fusion 14 (2013) 487–497

Fig. 4. Membership differences of the points A and B to three clusters.

F¼

Q
X

xq F q ðxi Þ

ð17Þ

the binary-segmentation fusion approach proposed by Qiu et al.
[21] as BS-F. The relative image errors are calculated according to

q¼1

^ =jjwjj
r ¼ jjw  wjj
2
2

xq ¼ 1  1=½1 þ expðarq Þ

ð18Þ

where xq is weight of q-th classiﬁer F q ðÞ (i.e., gray level) for
q = 1, 2, . . . , Q, but this varies as the fused pixel varies, and a is a
predetermined parameter.
It is apparent the calculated weights adaptively change as the
pixel membership probability (r) varies. Additionally, we assume
that weights can be assigned with prior knowledge, such as the
range of conductivity/permittivity values, and spatial distribution
of each pixel. The advantage is that all pixels will have varied
weights in responding to the conductivity/permittivity even from
the same classiﬁer, resulting in an improved image.
The proposed image fusion method in this paper is outlined for
two information resources as shown in Fig. 5.

4. Experiments
To study the feasibility and effectiveness of our proposed method, images from a group of experiments on the two-phase ﬂow
pattern identiﬁcation are collected. Hereafter, we term the proposed fuzzy clustering based fusion approach as FC-F; the fuzzy
exponent is taken in the range of 1.5–2.5. For comparison, we refer

Fig. 5. Framework of the proposed method for ET image fusion.

ð19Þ

^ is the real one. In the
where w is the calculated conductivity and w
following experiments, Eq. (19) is applied to evaluate the spatial
resolution of reconstructed ET images.
All imaging processes were calculated using a desktop computer with MATLAB R2009a, Intel(R) Core(TM)2 Duo CPU E7500
@2.93 GHz, 2 GB of RAM and Windows 7 2009.
4.1. Fusion of dual-modality ERT and ECT images
This set of experiments are conducted to test the effectiveness
of the FC-F algorithm for fusing dual-modality ERT and ECT images
as explained below.
4.1.1. Data acquisition
An integrated ECT and ERT dual–dual sensor is designed by the
research group ([11,12,14], Fig. 6). The capacitive and resistive
electrodes are alternately placed around the periphery of an insulating pipe, forming the main body of the dual modal sensor. In this
system, both ECT and ERT electrodes are mounted inside the pipe
wall. Since the pipe wall has a negative effect on capacitance measurements, the capacitive electrodes are placed internally. The ECT
electrodes are also mounted internally. The internally mounted
capacitive electrodes are covered with insulating layers to avoid
undesirable impacts on the ERT measurement. The sensor is used
as the data acquisition system for both ERT and ECT imaging process in the experiment. The acquisition rate is about 200 frames
per second.
The experimental model is built by a perspex vessel of 90 cm
and three movable rods inserted inside the ﬂuctuant water; two
rods are plastic and have high permittivity but low conductivity
while another semiconductor rod has low permittivity but high
conductivity. The conductivity differences can be detected by the
ECT system and the permittivity differences can be detected by
the ERT system.
4.1.2. Experiment processes
In the entire experimental process, the three rods are randomly
rotated in the water to simulate a dynamic gas–liquid ﬂow ﬁeld.
We apply 900 units/pixels for image reconstruction by using the
Landweber algorithm. Fig. 6b and c shows the individual ECT and
ERT images in the investigated ﬁeld. It is clear that the ERT sensor
can ﬁnd only the rod with high conductivity and the ECT is able to
locate the two rods with high permittivity. To reconstruct all three
rods, we apply the FC-F method fusing the ECT and ERT images. The
number of clusters in the FC-F algorithm is taken as 3 in responding

S. Yue et al. / Information Fusion 14 (2013) 487–497

493

Fig. 6. Test on the ECT and ERT fusion: (a) illustrations of the dual modal data acquisition equipments; (b) reconstructed ERT image; (c) reconstructed ECT image; (d) fused
image of ERT and ECT; and (e–h) fused images by the BS-F algorithm according to four typical thresholds around the real ratio of gas and water.

to the plastic rod, semiconductor rod and water of background in
the investigated ﬁeld. The two reconstructed images are ﬁrst
segmented by the f-FCM algorithm, an evaluation vector (1/3, 1/
3, 1/3) is applied to all pixels to identify the appropriate gray degrees based on their weighting values. Eq. (17) is then applied to
identify the ﬁnal gray degree of all pixels. Multiple initializations
are tested with the best results collected shown in this paper.
A high-speed CCD camera is installed at the dual modal sensor
plane. The dual modal sensor and the reconstructed ERT and ECT
images have been shown in a video from the CCD camera. The video shows the real-time characteristics of the ECT and ERT system
in their imaging processes. Please watch the video in the website://
www2.tju.edu.cn/colleges/automate/.
4.1.3. Experimental results and evaluations
Fig. 6d shows that the original rods are represented as three
clear clusters when applying the FC-F algorithm. The FC-F algorithm can ﬁnd these pixels of highly spatial resolution, that is,
the two plastic rods in the ECT image and the semiconductor rod
in the ECT image, but their weighting values. According to Eq.
(19), we have noted that 53.94% and 64.94% of pixels have been
identiﬁed in the single ECT and ERT images, but the fused image
by the FC-F algorithm has the average of 77.23%. We conclude
the FC-F algorithm performs under a very wide range of m of the
fuzziness exponents, and various numbers of clusters. Table 2 summarizes the performance of ECT imaging, ERT imaging and fused
imaging via FC-F algorithm with m = 1.5–2.5.
In terms of Eq. (19), it is observed that an average of 38.17% and
30.5% of pixels being mis-segmented in the ERT, ECT images, but
only 18.56% of pixels mis-segmented in the fused image. It is also
observed that for a series of available values of the number of clusters, the FC-F algorithm can always reconstruct clearer images than
the ERT or ECT alone. We conclude that for a pair of ECT and ERT
images the FC-F algorithm can perform well when the ERT and
the ECT images both contain pixels of high space resolutions with
complementary characteristics.
Alternatively, the BS-F algorithm can fuse the two isolated
images toward higher spatial resolution, but Fig. 6e–h shows these

fused images contain many pixels whose gray degrees are discontinuous due to the binarization effect of the BS-F algorithm, where
only 65.8% of pixels is provided by Eq. (19). Note that the image
reconstruction takes longer in the FC-F algorithm compared to
either ERT or ECT. The iteration process in the FC-F algorithm has
time complexity O(ctn), where c is the number of clusters, t is
the iteration times, and n is the number of pixels. In this experiment, when c is taken as 3, t is averagely taken as 57, and n is
812, the average fusing time of the FC-F algorithm is 0.221 s while
the reconstructing time is 0.075 s in the ERT imaging process and
0.085 s in the ECT imaging process. Thus the FC-F algorithm cannot
follow the speed of the reconstructed image.
A closer look at the fusion process in the FC-F algorithm shows
that the FC-F algorithm does in fact take the pixels of the higher
membership degree for the ﬁnally fused image. If any pixel is very
close to the clustering prototype, the gray of the pixel will be fused
to the ﬁnal image with high weight, no matter how low the spatial
resolution of the pixel is. This demands that at least the gray value
of each clustering prototype is nearly correct and believable, but
this is not guaranteed when a fused image is of a low space solution. Consequently, the fusion between a pair of images may not
be guaranteed as better than ERT or ECT alone. Fig. 7 illustrates
the two reconstructed images, by the LBP algorithm under an
ERT system and an ECT system and the fused image by the FC-F
algorithm. It is observed that the fused image is neither clearer
than the image from the ERT system and nor the one from the
ECT system. The reconstructed images using the LBP algorithm
have nearly a 53% and 38% mis-segmented number of pixels, while
the fused image has 44% mis-segmentation. The FC-F algorithm
may underperform for mixture cases, particularly when the fused
image contains a low-quality reconstructed image.

4.2. Fusion of ET images from adjacent and opposite driven patterns
This set of experiments are conducted to test the feasibility of
the FC-F algorithm in adjacent and opposite driven patterns in a
gas–liquid two-phase ﬂow as explained below.

494

S. Yue et al. / Information Fusion 14 (2013) 487–497

Table 2
The experimental results of the FC-F algorithm with various parameter settings.
Reconstructed ERT image

Reconstructed ECT image

Fused image of ERT and ECT

c = 3 m = 1.5

c = 4 m = 1.5

c = 5 m = 2.0

c = 6 m = 2.0

c = 7 m = 2.5

c = 8 m = 2.5

4.2.1. Data acquisition
The data resources are from the 16 electrodes in the TERT-II
test system [12]. A single ERT electrode is positioned in a circle
which is perpendicular to the horizontal pipe with inner diameter
being 350.0 mm and outer diameter being 365.0 mm. The data for
the adjacent and the opposite driven patterns are alternately
taken for ERT imaging and ECT imaging. The pattern switches
may lead to variants of the positions and shapes of the gas–liquid
two-phase ﬂow. However, in this research, the data acquisition
rate is about 200 frames per second which is much faster than
the switching rate, the positions and shapes of the gas–liquid
two-phase ﬂow in the investigated ﬁeld may approximately be

regarded as invariant between the adjacent and opposite data
acquisitions.
The velocities of air in the gas–liquid two-phase ﬂow are from
about 1.7 m s1, gradually increased to 2.5 m s1 in the horizontal
pipe, while the water ﬂow rates for all cases are kept the same. As
the gas ﬂow rate increases, the stratiﬁed smooth ﬂow becomes less
steady, and ﬁnally turns to a slug ﬂow. Consequently, three typical
water–gas ﬂow regimes: stratiﬁed smooth ﬂow, plug ﬂow and slug
ﬂow can be observed in the transparent horizontal pipe. In any
ﬂow pattern, a time-stacked image is extracted from a series of
two-dimensional reconstructed images by applying the LBP algorithm to the measurement from the ERT system, as shown in Fig. 8.

495

S. Yue et al. / Information Fusion 14 (2013) 487–497

Fig. 7. Fused images under different drive patterns and different algorithms: (a) original image; (b) image from an ERT system by the LBP algorithm; (c) image from an ECT
system by the LBP algorithm; and (d) fused image by use FC-F algorithm.

t=1

t=2

t=3

t=4

…….

t=m

t =1, 2, 3, 4

…… m

Fig. 8. Stacked images of gas–water interface by using a group of continual two-dimensional reconstructed images. Any black bar refers to the largest height of water in these
ﬁgures.

4.2.2. Experiment processes
The stacked image is used to reconstruct a two-dimensional
gas–liquid interface along the ﬂow direction at every 0.05 s. The
data acquisition speed of the system is 200 frames each second
and is much larger than the variance speed of the interface shapes.
Consequently, the interface is approximately invariant within
every 0.05 s. Fig. 8 illustrates the stacked images of the gas–water
interface by using a group of continuous two-dimensional reconstructed images, where any black bar refers to the largest height
Table 3
Comparison between high-speed camera and the fused ET image by FC-F algorithm.

of water in every two-dimensional image. When one observes
the gas–water interface at the side of the transparent horizontal
pipe, the greatest height determines the gas–liquid interface
shapes. Please note these heights in a series of reconstructed
images may appear in the internal and external ﬁelds in any
two-dimensional reconstructed image. The number of clusters in
the FC-F algorithm is taken as 2 which corresponds to air and water
in the investigated ﬁeld, and m is 5. The two reconstructed images
under adjacent and opposite driven pattern are ﬁrst segmented by

496

S. Yue et al. / Information Fusion 14 (2013) 487–497

Table 4
Relative error of image under two driven patterns.
Images and patterns

Internal ﬁeld

External ﬁeld

Full ﬁeld

I1 (Adjacent pattern)
I1 (Opposite pattern)
Fused image (FC-F/BS-F)
I2 (Adjacent pattern)
I2 (Opposite pattern)
Fused image (FC-F/BS-F)
I3 (Adjacent pattern)
I3 (Opposite pattern)
Fused image (FC-F/BS-F)

0.417
0.232
0.215/0.273
0.487
0.318
0.312/0.292
0.470
0.418
0.362/0.397

0.315
0.409
0.314/0.352
0.372
0.521
0.324/0.327
0.414
0.491
0.324/0.327

0.379
0.344
0.273/0.361
0.426
0.463
0.315/0.328
0.438
0.473
0.332/0.352

Note: I1, I2, and I3 refers to the stratiﬁed smooth ﬂow, plug ﬂow and slug ﬂow,
respectively.

the f-FCM algorithm and an evaluation vector (1/2, 1/2) is then applied to all pixels to identify which gray degree is more desirable
based on their weighting values. Again, Eq. (17) is applied to determine the ﬁnal gray degree of any pixel. Table 3 summarizes the
reconstructed gas–liquid interfaces between in the stacked images
from the adjacent, opposite and fused drive patterns.
4.2.3. Experimental results and conclusions
The adjacent and opposite patterns perform well at identifying
the investigated objects in the internal and external ﬁelds. As a result, the reconstructed interfaces by the FC-F algorithm in the
stacked images from the adjacent and opposite driven patterns
are partially clear in the internal and external ﬁelds respectively.
For comparison purpose, a high-speed CCD camera is installed at
the side of the horizontal pipe to take the real images of the gas–
liquid interfaces. The real interfaces in different ﬂow patterns are
then taken. As shown in the group of high-speed CCD images, the
interfaces vary from steady to volatile jump, and the stacked
images are consistently observed. Individual images obtained from
adjacent and opposite driven patterns can reconstruct the water–
gas interfaces with higher spatial resolutions in internal and external ﬁelds respectively. The fused images by the FC-F algorithm are
closer to the images by the high-speed CCD camera. These experiments demonstrate that the FC-F algorithm identiﬁes the complementary nature of ET images hidden in the adjacent and opposite
driven patterns based on the same ERT sensor and obtains a favorable real-time fusion effect.
Motivated by this ﬁnding, we partition the image ﬁeld into
internal and external ﬁelds, which is the boundary at half of radius
of the imaging ﬁeld circle. In term of Eq. (19), the relative image errors of the two driven patterns and the fused images can be computed in these two ﬁelds (Table 4). Table 4 shows that the internal
ﬁeld in the two images can provide higher spatial resolution using
the opposite pattern while the external or boundary ﬁeld can provide higher spatial resolution using the adjacent pattern. The FC-F
algorithm cannot only obtain higher spatial resolution in the entire
imaging ﬁeld, but also in either internal or external ﬁeld. The BS-F
algorithm provides the lower spatial resolution than the FC-F
algorithm.
5. Conclusion
ET is considered as a low cost technique with high imaging
speed. However, the achievable spatial resolution is inherently limited as the ﬁeld distribution of ETs depends on the actual material
distribution and the sensitivity is nonlinearly distributed over the
imaging ﬁeld. In this paper, we propose a fuzzy clustering based approach for ET image fusion which integrates multiple modalities
and multiple driven patterns. Strategies of choosing parameters
(e.g., number of clusters, fuzziness exponent) in the proposed

approach are discussed. Compared with single-modality or singledriven pattern images, our proposed approach is capable of rendering
robust, high-quality images. The potential for this approach has
been demonstrated via a number of simulations and experiments.
The experimental results demonstrate that the fused images using
the proposed method can obtain higher spatial resolution than any
imaging alone among a broad range of parameter settings consistently. We do note the proposed approach may require more iterations and more time per iteration comparing to the LBP and
Landerweber algorithms. However, to reach the satisfactory accuracy, our proposed approach is more computational efﬁcient.
While successful, the performance of our proposed approach on
images with very few pixels of high spatial resolution is less than
satisfactory. The necessary condition to obtain the ﬁnal image of
higher spatial resolution from a pair of isolated images is that there
should be complementary features between the pairs of images. In
addition, the proposed approach has not been directly used under
real-time conditions and has only been validated in a laboratory
setting [37,38]. We are currently integrating the data processing
software and hardware to validate and improve the proposed
method in the real-world conditions.

Acknowledgements
This work is supported by the National Science Foundation of
China under Grant Nos. 61774014, 60572065, 60772080 and the
National Science Foundation of Tianjin under Grant No.
08JCYBJC13800.

References
[1] W. Yin, A.J. Peyton, A planar EMT system for the detection of faults on thin
metallic plates, Meas. Sci. Technol. 17 (2006) 2130–2135.
[2] R.D. Cook, ACT3, A High-Speed, High-Precision Electrical Impedance
Tomography, PhD Thesis, Rensselaer Polytechnic Institute, Troy, NY, 1992.
[3] W. Warsito, Q. Marashdeh, L. Fan, Electrical capacitance volume tomography,
IEEE Sensors J. 7 (4) (2007) 525–535.
[4] Z. Cao, L. Xu, H. Wang, Image reconstruction technique of electrical capacitance
tomography for low-contrast dielectrics using Calderon’s method, Meas. Sci.
Technol. 20 (2009) 104027–104039.
[5] T. Dyakowski, L.F.C. Jeanmeure, Artur J. Jaworski, Applications of electrical
tomography for gas–solids and liquid–solids ﬂows – a review, Powder Technol.
112 (2000) 174–192.
[6] B.S. Hoyle, X. Jia, Design and application of a multi-modal process tomography
system, Meas. Sci. Technol. 12 (2001) 1157–1165.
[7] W. Yang, Key issues in designing capacitance tomography sensors, in: IEEE
Sensors 2006 Conference, 2006, pp. 22–25.
[8] W. Yang, T.A. York, New AC-based capacitance tomography system, IEE Proc.
Sci. Meas. Technol. 46 (1) (1999) 47–53.
[9] D.L. Hall, J. Llinas, An introduction to multisensor data fusion, Proc IEEE 85 (1)
(1997) 6–23.
[10] G. Steiner, Application and data fusion of different sensor modalities in
topographic imaging, Elektrotech. Inform. Stechnik 124 (7) (2007) 232–239.
[11] Z. Cui, H.X. Wang, L. Tang, L.F. Zhang, X.Y. Chen, Y. Yan, A speciﬁc data
acquisition scheme for electrical tomography, IEEE Int. Instr. Meas. Technol.
Conf. (2008) 726–729.
[12] Z. Cui, Digital ECT/ERT Dual Modal System Design, MS Thesis, Tianjin
University, Tianjin, 2007.
[13] H. Altincay, M. Demirekler, Speaker identiﬁcation by combining multiple
classiﬁers using Dempster-image segmentation, Opt. Eng. 41 (4) (2003) 760–
770.
[14] L. Zhang, H. Wang, Single source current drive patterns for electrical
impedance tomography, in: 12th IEEE Conference Proc. Instr. Meas. Tech.,
2010.
[15] F. Wang, Q. Marashdeh, L. Fan, W. Warsito, Electrical capacitance volume
tomography: design and applications, Sensors 10 (2010) 1890–1917.
[16] Y. Kim, S. Lee, U. Ljaz, et al., Sensitivity map generation in electrical
capacitance tomography using mixed normalization models, Meas. Sci.
Technol. 18 (2007) 2092–2102.
[17] F. Santoa, M. Vogelius, A back projection algorithm for electrical impedance
imaging, SIAM J. Appl. Math. 50 (1990) 216–243.
[18] W. Yang, Design of electrical capacitance tomography sensors, Meas. Sci.
Technol. 21 (4) (2010) 2001–2011.
[19] William H. Hayt Jr., John A. Buck, Engineering Electromagnetic, seventh ed.,
McGraw-Hill, 2006.

S. Yue et al. / Information Fusion 14 (2013) 487–497
[20] C.R. Vogel, Computational Methods for Inverse Problems, SIAM, Philadelphia,
PA, 2002.
[21] C. Qiu, B.S. Hoyle, F.J.W. Podd, Engineering and application of a dual-modality
process tomography system, Flow Meas. Instrum. 18 (2007) 247–254.
[22] B. Brandstatter, G. Steiner, B. Kortschak, D. Watzenig, H. Wegleiter, Fusion of
electrical capacitance with ultrasound tomography – analysis of methods, in:
Proc. 4th World Congress on Industrial Process Tomography, vol. 2, 2005b, pp.
564–569.
[23] G. Piella, A general framework for multiresolution image fusion: from pixel to
regions, Inform. Fusion 4 (4) (2003) 259–280.
[24] C. Cohade, R.L. Wahl, Applications of positron emission tomography/computed
tomography image fusion in clinical positron emission tomography – clinical
use, interpretation methods, diagnostic improvements, Semin. Nucl. Med. 33
(3) (2003) 228–237.
[25] E. Hedlund, J. Karlsson, S. Starck, Automatic and manual image fusion of Inpentetreotide SPECT and diagnostic CT in neuroendocrine tumor imaging – an
evaluation, J. Med. Phys. 35 (4) (2010) 223–228.
[26] Y. Zhu, O. Dupuis, I. Lyon, Automatic determination of mass functions in
Dempster–Shafer theory using fuzzy c-means and spatial neighborhood
information for image segmentation, Opt. Eng. 41 (4) (2002) 760–770.
[27] G.L. Sannazzari, R. Ragona, M.G.R. Redda, F.R. Giglioli, G. Isolato, A Guarneri,
CT–MRI image fusion for delineation of volumes in three-dimensional
conformal radiation therapy in the treatment of localized prostate cancer,
Brit. J. Radiol. 75 (2002) 603–607.
[28] D. Sahani, A. Mehta, M. Blake, et al., Preoperative hepatic vascular evaluation
with CT and MR angiography: implications for surgery, Radiographics 24
(2004) 1367–1380.

497

[29] M. Uchida, M. Ishibashi, S. Arikawa, et al., High-resolution computed
tomographic angiography/computed tomographic cholangiography image
fusion of the hepatobiliary system, J. Comput. Assist. Tomogr. 30 (2006)
913–916.
[30] J.C. Bezdek, Pattern Recognition with Fuzzy Objective Function Algorithms,
Plenum Press, New York, 1981.
[31] M. Ahmed, S. Yamany, C. Mohamed, A modiﬁed fuzzy c-means algorithm for
bias ﬁeld estimation and segmentation of MRI data, IEEE Trans. Med. Imaging
21 (3) (2002) 193–199.
[32] A. Boudraa, P. Clarysse, Fast fuzzy gray level image segmentation method,
Med. Biol. Eng. Comput. 35 (Suppl) (1997) 804–809.
[33] D.J. Kim, K.H. Lee, D. Lee, On cluster validity index for estimation of the optimal
number of fuzzy clusters, Pattern Recogn. 37 (10) (2004) 2009–2025.
[34] M. Kim, R.S. Ramakrishna, New indices for cluster validity assessment, Pattern
Recogn. Lett. 26 (2005) 2353–2363.
[35] T. Lange, V. Roth, L. Braum, J.M. Buhmann, Stability-based validation of
clustering solutions, Neural Comput. 16 (6) (2004) 1299–1323.
[36] C. Lee, O.R. Zaine, H. Park, J. Huang, R. Greiner, Clustering high dimensional
data: a graph-based relaxed optimization approach, Inform. Sci. 178 (2008)
4501–4511.
[37] S. Yue, T. Wu, X. Zhao, Fused multi-characteristic validity index: an application
to reconstructed image evaluation in electrical tomography, Int. J. Computat.
Intell. Syst. 4 (2011) 1788–1889.
[38] S. Yue, J. Wang, T. Gao, A new unsupervised approach to clustering, Sci. China
(Ser. F) (2010) 128–139.

Proceedings of the 2010 Winter Simulation Conference
B. Johansson, S. Jain, J. Montoya-Torres, J. Hugan, and E. Yücesan, eds.

BI-CRITERIA ANALYSIS OF AMBULANCE DIVERSION POLICIES

Adrian Ramirez Nafarrate
John W. Fowler
Teresa Wu
Arizona State University
699 S. Mill Avenue, Suite 553
Tempe, AZ 85281, USA

ABSTRACT
Overcrowding episodes in the Emergency Departments (EDs) of the United States and their consequences
have received considerable attention by the media and the medical community. One of these consequences is ambulance diversion (AD), which is adopted as a solution to relieve congestion. Current practices on AD decisions are largely dependent on human expertise, thus they tend to be subjective. This paper develops a simulation model for ED to study the impact of AD policies based on one of the following
main ED state variables: the number of patients waiting, the number of patients boarding and the number
of available beds in the inpatient unit. The objective is to analyze the impact of AD on the ED performance considering two criteria: patient average waiting time and percentage of time spent on diversion.
Results show that there exist significant differences based on the variables chosen to design the policy and
the criterion to reevaluate the AD status. This insight can be used to assist ED managers in making AD
decisions to achieve better quality of healthcare service.
1

INTRODUCTION

Saturation and overcrowding episodes in ED’s across the United States have been discussed by the media,
healthcare organizations, government agencies, the medical community as well as other professional and
academic organizations. Problems such as long waiting times, patients leaving without treatment (LWOT)
and long boarding times have been analyzed using methods such as simulation and queuing formulations.
This paper explores the use of simulation to study the decision of diverting ambulances to other facilities.
The General Accounting Office (US GAO 2003) submitted a report to the US Senate in 2003 highlighting some issues found in ED’s. This report mentioned that about 10% of the emergency care providers that participated in the study spent more than 20% of the time on diversion during fiscal year 2001.
Furthermore, more than two thirds of the participants went on diversion at least once per year. Another
study conducted in 2003 and published by the National Health Statistics Report, showed that the mean
annual hours on diversion of ED’s that reported any diversion was 403.9 hours (CDC 2006).
Since then, AD has been widely discussed in media and scholar publications. The AD decision which
leads to potential delays for treatment due to longer transportation times, has been seriously criticized.
Many local governments thus have prohibited going on AD status (CDC 2006). However, the “no ambulance diversion” policies have affected the ED performance negatively. For instance, hospitals located in
Massachusetts have seen higher waiting times and a large number of patients boarding in inappropriate
areas (Massachusetts Nurse Newsletter 2009).
In general, specialists agree that AD is a matter of public health and its implementation should be
linked with clinical outcomes, patient and provider satisfaction, quality-of-life measures, economic measures and quality management initiatives (Asplin 2003). Therefore, this paper proposes the use of model-

978-1-4244-9865-9/10/$26.00 ©2010 IEEE

2315

Ramirez, Fowler and Wu
ing methods to quantitatively analyze the effectiveness of the AD decision with multiple criteria being
considered.
In this paper, a discrete-event simulation model of an ED is built to analyze policies that trigger the
diversion status based on a single threshold. The thresholds are related to the main state variables of the
ED, which also are the main causes that practitioners take into account for diversion decisions. These variables are: the number of patients waiting in the ED, the number of patients boarding and the number of
available beds in the inpatient units. The decision outcomes are assessed based on two criteria: patient average waiting time and percentage of time spent on diversion. Design of Experiments is first employed to
select the appropriate levels for the policies and then bi-criteria Pareto analysis and simultaneous confidence ellipses are applied to study the trade off existing in the implementation of AD.
The rest of the paper reviews the literature regarding AD in Section 2, the proposed study is described
in detail in Section 3, the analysis of the results is presented in Section 4 and finally the conclusions and
future research are presented in Section 5.
2

LITERATURE REVIEW

Quantitative assessment for AD decision has been studied less than other issues related to ED (e.g., waiting time and staffing levels). This might be due to the complexity, the dependency on the local conditions
and the preference for searching alternative solutions to overcrowding in AD decisions.
Simulation, queuing analysis and game theory are some commonly used modeling tools to analyze
the impact of AD. Conclusions about this topic are varied and comprise multiple aspects. For instance, it
has been found that the patient length of stay has a positive correlation with the probability of going on
diversion (Kolker 2008). The configuration of the ED is also a significant factor on the diversion performance, for instance, design of experiments applied in a simulation model shows that a fast-track area to
treat the least urgent patients could reduce the time spent on diversion status (Ramirez et al., 2009).
Queuing analysis has been applied to obtain equations that measure the performance of an ED, which
goes on diversion based on a policy of minimum-maximum on the number of patients waiting (Ramirez
et al., 2010a). Game theory has been proposed to study the reciprocal effect found in a system of ED’s
that go on diversion. According to this analysis, there is a need of an external agent that regulates AD in a
system of several hospitals (Hagtvedt, et al. 2009).
Despite the scarcity of mathematical assessment of the situation, AD has been widely discussed by
the medical community. Important remarks found on these publications are the common factors that influence diverting ambulances. It has been found that diversion decision are made due to three main reasons: there is a high number of patients waiting in the ED, there is a high number of patients boarding or
there is a lack of available beds in the inpatient units (Asplin 2003, CDC 2006). Nevertheless, to the best
of our knowledge, quantitatively assessing the impact of these variables on an AD policy has not been
done.
This paper proposes methods to analyze the performance of AD decisions on these variables in terms
of mean performance and variability graphically. Two performance criteria are considered: (1) accessibility to emergency care, represented by percentage of time on diversion and (2) timely service,
represented by the average patient waiting time.
3

PROPOSED STUDY

First, a simulation model representing a hypothetical ED that is representative of many emergency
care facilities across the United States is developed. This model is adapted to conduct experiments for the
chosen AD policy, which have a single threshold that triggers the diversion state. Then, continuous or periodic evaluation of the state of the system is used to re-evaluate or remove the diversion status. Design of
experiments is used to define the threshold levels for all the AD policies. During the execution of the simulation model, output data regarding time on diversion and average patient waiting time is collected. Finally, Pareto analysis is applied to study and observe the main differences among the mean performance

2316

Ramirez, Fowler and Wu
of the policies and simultaneous confidence ellipses are constructed to study the variability across policies. Figure 1 depicts the framework presented on this paper.
Construct a Simula tion
Model for the ED to
Analyze
Design AD Policies and
Define Levels for its
Para meters

Select a Policy to Eva lua te
and Adapt Simulation
Model to Policy
Run a pre-defined
Number of Replications
and Collect the Results
for the Criteria to
Analyze
Analysis of Mean Performa nce Analysis of Va riability
Compare the Performance
of the Policies by
Constructing
Bi-Criteria Gra phs

Compare the Consistency
of the Policies by
Constructing Plots
of Simultaneous
Confidence Interva ls

Figure 1: Proposed framework for the bi-criteria analysis of AD policies
3.1

Simulation Model

Figure 2 illustrates the simulation model which includes one ED and one Inpatient Unit (IP).
The ED receives patients arriving by ambulance or walking in. Upon arrival, the patients are classified to one out of five acuity levels. Patients of Level 1 are considered to have the most serious problems,
while patients of Level 5 are the non-urgent patients. If the diversion status is on, any entity representing
ambulance patients arriving in that period will be destroyed. If the diversion status is off and all the beds
in the ED are occupied, the patients need to wait in a line for treatment. Treatment is only provided when
patients are able to get a bed. The beds in the ED and in the IP unit are the only resources modeled.
Beds are assigned to patients based on a priority given by the acuity level, with the most acute patients receiving the highest priority. The mean treatment time depends on the acuity level as well. If a patient remains in the ED for a long time without starting treatment in a bed, then that entity will be removed of the simulation, representing an LWOT patient.
After completing treatment, the patients can be either discharged or admitted to the hospital. Therefore, the IP unit receives patients from direct admission and admissions from the ED. The treatment time
in the IP unit depends on the origin of the patient. After completing treatment in the IP unit, the patients
are discharged. If a patient from the ED needs to be admitted into the hospital, but there is not any available bed in the IP unit, then the patient remains in the ED occupying the bed and blocking the flow of other
patients, this is referred to as boarding.

2317

Ramirez, Fowler and Wu

Ambulance
Arrival

Direct
Admissions
Send Patient
to Another
ED

Is Diversion Yes
Status ON?

IP Unit

ED

No

Treatment
Area
padm

Waiting line by acuity level

Patient
Boarding

Walk-In
Arrival

plwot
LWOT

1-padm

Discharge

Figure 2: Logic of the simulation model
3.2

Input Data

The input data used in this model intends to be representative from emergency care providers and to capture the main dynamics seen in ED’s across the United States. For instance, the arrival pattern exhibited
in the ED has been highlighted by different publications (Cochran and Roche 2009; CDC 2008; Springer’s International Series 2006; Miller, Ferrin and Shahi 2009). It is assumed in this paper that arrivals
follow a Poisson process with a mean arrival rate depending on the arrival mode and time of the day. The
rates shown in Figure 3 are obtained from a publication of an analysis of a provider in Arizona (Cochran
and Roche 2009). Note that 15% of all the arrivals to the ED correspond to ambulance arrivals; this percentage is consistent with the national average of 15.4% (CDC 2008).
Arrival Pattern to the ED
9

Mean Arrival Rate

8
7
6
5

Ambulance Arrivals

4

Walk-In Arrivals

3
2
1
0
0

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

0

Time of the Day

Figure 3: Mean arrival rate to the ED
The same publication used for the arrival rate was used to obtain the percentage of patients that belong to each acuity level. These percentages are shown in Table 1.

2318

Ramirez, Fowler and Wu
Table 1: Percentages of acuity mix depending on arrival mode.
Arrival Mode
Acuity Level
Ambulance
Walk-In
Overall
1
15
2
3.95
2
42
16
19.9
3
30
40
38.5
4
10
30
27
5
3
12
10.65
15
85
100
Overall
The treatment time in the ED is assumed to be exponentially distributed with a mean shown in Table
2. The same publication with data from an Arizona provider (Cochran and Roche 2009) was used to derive these times.
Table 2: Mean treatment time depending on acuity level.
Acuity Level
Mean Treatment Time (min)
261
1
261
2
162
3
90
4
30
5
The percentage of ED patients that require admission to the hospital is assumed to be 15%, which is
in the range of admission percentages seen in the literature (CDC 2006; CDC 2008). The inter-arrival
time of direct admission to the IP unit and the treatment time in this unit are based on a provider in Arizona (Cochran and Bharti 2006). Direct admissions are assumed to be exponentially distributed with a mean
of 10 hours. Treatment times are also assumed to be exponentially distributed with means shown in Table
3.
Table 3: Mean treatment time in the IP unit.
Patient Source
Mean Treatment Time in IP (hrs)
Direct Admission
70
Admission from ED
80
LWOTs are events that occur in real ED’s that are complex to model. However, it is important to add
it as a feature given its impact on the performance and traffic intensity of an ED. This paper uses an approach employed in other assessments of real systems (Miller, Ferrin and Shahi 2009). If a patient has not
been placed in a bed upon 24 hours of arrival, then the patient is removed from the system, representing a
LWOT patient.
3.3

Single-Threshold AD Policies

Each AD policy studied in this paper considers one out of the three main factors for going on diversion in
practice: the number of available inpatient beds, the number of patients waiting and the number of patients boarding. The first and second are the main causes to divert ambulances according to a survey published in the Advance Data from Vital and Health Statistics (CDC 2006). Meanwhile, the last one has
been highlighted as another important contributor of diversion (Asplin 2003; Pham et al. 2006). Therefore, the state variables of interest to formulate the policies are the following:

2319

Ramirez, Fowler and Wu
x = Total number of patients waiting for a bed in the ED, x = 0, 1, 2, 3, ….
y = Total number of patients boarding in the ED, y = 0, 1, 2, 3, …, BED
z = Number of beds available in the IP unit, z = 0, 1, 2, 3, …, BIP
where:
BED = Number of beds in the ED. In this paper BED = 20.
BIP = Number of beds in the Inpatient Unit. In this paper BIP = 78.
Single-threshold AD policies have the form: (Don, Doff), where the Don parameter represents the
threshold to set the diversion status on and the Doff parameter is the criterion to reevaluate or remove the
AD status. Hence, based on this form and the state variables of interests, the six policies studied in this
paper are:
P1: (Ux, ∆t)
Where Ux is the threshold on the number of patients waiting in the ED to go on diversion. Hence, diversion status is set on if at some point x > Ux. Once the ED has gone on diversion, every ∆t time units the
state of the system is evaluated until the decision to go off diversion is made. Diversion status is removed
at a re-evaluation point if x < Ux.
P2: (Ux, Lx)
Where Ux is the upper threshold on the number of patients waiting in the ED to go on diversion and Lx is
the lower threshold on the number of patients waiting in the ED to remove the diversion status. Hence,
diversion is set on if at some point x > Ux and it is removed when x < Lx.
P3: (Uy, ∆t)
Where Uy is the threshold on the number of patients boarding in the ED to go on diversion. Hence, diversion status is set on if at some point y > Uy. Once the ED has gone on diversion, every ∆t time units the
state of the system is evaluated until the decision to go off diversion is made. Diversion status is removed
at a re-evaluation point if y < Uy.
P4: (Uy, Ly)
Where Uy is the upper threshold on the number of patients boarding in the ED to go on diversion and Ly is
the lower threshold on the number of patients boarding in the ED to remove the diversion status. Hence,
diversion is set on if at some point y > Uy and it is removed when y < Ly.
P5: (Lz, ∆t)
Where Lz is the threshold on the current number of beds available in the Inpatient Unit to go on diversion.
Hence, diversion status is set on if at some point z < Lz. Once the ED has gone on diversion, every ∆t time
units the state of the system is evaluated until the decision to go off diversion is made. Diversion status is
removed at a re-evaluation point if z > Uz.
P6: (Lz, Uz)
Where Lz is the lower threshold on the number of beds available in the Inpatient Unit to go on diversion
and Uz is the upper threshold on the number of beds available in the Inpatient Unit to remove the diversion status. Hence, diversion is set on if at some point z < Uz and it is removed when z > Lz.
Note that there are two alternatives to re-evaluate the state of the system once the decision of going on diversion has been made. Periodic evaluation implies that the state of the system will be reviewed only at
discrete times, such as P1, P3 and P5. On the other hand, continuous review monitors the state of the system at every event in the ED, such as P2, P4 and P6.

2320

Ramirez, Fowler and Wu
3.4

Experimental Design

Based on the policies described in Section 3.3, the levels for each policy are defined with the purpose of
covering a wide range of values, which include conservative and not so conservative policies. The levels
are shown in Table 4. The experimentation is based on a general factorial design for each policy, where
all the feasible combinations of instances are chosen as treatment for experimentation (Montgomery
2005). Note that in the case of P2 and P4, it is required that Don > Doff; meanwhile for P6, Don < Doff is
required.

Policy
P1
P2
P3
P4
P5
P6

Table 4: Levels of the policy parameters chosen for experimentation.
Don
Doff
10, 20, 30, 40, 50, 60, 70 patients
15, 30, 45, 60 minutes
10, 20, 30, 40, 50, 60, 70 patients
0, 10, 20, 30, 40, 50, 60 patients
1, 2, 3, 4, 5, 6, 7, 8, 9, 10 patients
15, 30, 45, 60 minutes
1, 2, 3, 4, 5, 6, 7, 8, 9, 10 patients
0, 1, 2, 3, 4, 5, 6, 7, 8, 9 patients
0, 1, 2, 3, 4, 5, 6, 7, 8, 9 beds
15, 30, 45, 60 minutes
0, 1, 2, 3, 4, 5, 6, 7, 8, 9 beds
1, 2, 3, 4, 5, 6, 7, 8, 9, 10 beds

The simulation model is adapted for every policy and every value combination to collect data regarding patient waiting time and time spent on diversion. A pilot run showed that three weeks of warm up period was acceptable and then thousands of observations are collected per replicate. Forty replications are
run using antithetic random variates (Law 2007), obtaining twenty observations per treatment.
4
4.1

ANALYSIS OF RESULTS
Mean Performance Across the Policies

The mean performance for all the treatments across the six policies are presented in Figure 4. It can be
seen that the band comprising the solutions including all the policies resembles a piecewise linear graph,
whose first segment includes solutions belonging to P1, P2, P3 and P4, while the second segment includes
solutions of P5 and P6.
Mean Performance for Both Criteria Across All Policies
2.00

Policy
P1
P2
P3
P4
P5
P6

Average Waiting Time

1.75

Policies: P1, P2, P3 and P4
Slope: 2 minutes/percentage point

1.50

1 hour

1.25

Policies: P5 and P6
Slope: 0.5 minutes/percentage
point

1.00

25%
0.75

15 minutes
30%

0.50
0

10

20
30
40
Percentage of Time on Diversion

50

60

Figure 4: Bi-criteria graph with solutions across all the policies
Policies based on the number of patients waiting (P1 and P2) and the number of patients boarding (P3
and P4) produce results that yield the lowest proportion of time spent on diversion. In addition, they have
the highest gaining in the trade-off between accessibility and service; thus for every percentage point increase in diversion status, the average waiting time is reduced about two minutes.

2321

Ramirez, Fowler and Wu
On the other hand, the policies based on the availability of IP beds (P5 and P6) seem to be the most
conservative policies, yielding a small average waiting time, but spending a high percentage of time on
diversion status. Considering that the most common reason to go on diversion is the lack of IP beds available (CDC 2006), these results could explain the concerns of the large number of hours diverting ambulances in the real systems.
In addition, the policies based on the same factor are compared using bi-criteria graphs. Figure 5
shows the graph for P1 and P2, which are based on the number of patients waiting and have periodic and
continuous review, respectively. It can be seen that results with a common Don parameter form a cluster
in the case of periodic review, which means that the time to re-evaluate the state of the system is not a
significant parameter. On the other hand, the clusters using continuous review overlap each other.

Policy with periodic review shows well defined compact groups with similar results based on a
common Don value.

Figure 5: Bi-criteria graphs of the mean performance for P1 and P2
The conclusions of the comparison between P3 and P4, as well as between P5 and P6 are very similar. The clustering of solutions based on a common Don value is also evident in the case of periodic review for those policies. However, the dispersion of the solutions increases for continuous review. For a
more complete analysis of these graphs please refer to Ramirez et al. (2010b).
In summary, the analysis of the mean performance in the bi-criteria evaluation of the six policies lead
to the following observations:
•
•
•
•
•

4.2

There is a trade-off between time spent on diversion and average waiting time when AD is implemented.
Policies based on number of patients waiting and number of patients boarding offer a good balance between accessibility to emergency care and average waiting time.
In addition, these policies lead to larger reductions of average waiting time per every percentage
point spent on diversion.
Even though the policies based on availability of IP beds produce a very small average waiting
time, they yield a high percentage of time spent on diversion, which is very likely to be undesirable by the decision makers.
The periodic review of the system produce clusters of solutions with a common value for the Don
parameter, implying that Doff parameter is not significant.
Variability Across the Policies

The mean performance is important for decision makers to decide what is the best policy that balances
average waiting time and time spent on diversion according to the needs of the organization. However,
the consistency of the policies is not included in previous graphs. Therefore, this section evaluates the variability across the six policies.

2322

Ramirez, Fowler and Wu
Confidence intervals constructed for each criterion yield a good relative precision across the six policies. For instance, relative precision of 95% confidence intervals on the average waiting time is between
5.8% and 7.1% across the six policies. For the percentage of time on diversion, the precision is between
5.09% and 12.88%. These percentages imply a high chance of covering the true value; however, they do
not provide much information about the differences across the policies.
Since the purpose of this paper is to analyze both criteria simultaneously, 95% confidence ellipses
that jointly include the average waiting time and percentage of time on diversion are constructed and presented in Figure 6. Note that each confidence ellipse is built for every treatment considering all its replications. Remarkable differences can be seen from these graphs comparing all the policies.
In the first place, it is evident the change of the ellipse depending on the factor considered in the policy. Thus, policies based on the number of patients waiting have a well defined ellipse with a smaller area
than those based on the number of patients boarding. Furthermore, P1 exhibits ellipses that overlap each
other for common values of the Don parameter, which implies that Doff is not significant for this policy.
Policies based on the number of patients boarding also show a high correlation between the two variables;
however, there are more ellipses overlapping, making it difficult to decide what policies could produce
better results for the decision maker.
On the other hand, ellipses for policies based on availability of IP beds have their major axes almost
parallel to the horizontal axes. This implies that correlation between the two criteria is weak; hence the
prediction of performance becomes difficult because results are randomly distributed.
Thus, the main observations in terms of the variability in the performance that each policy produces
are:
•
•
•

5

Policies based on the number of patients waiting and number of patients boarding have a higher
correlation between the two criteria evaluated on this paper. This can increase the accuracy of the
prediction of performance under a specific AD policy.
On the contrary, policies based on the number of available beds in the IP unit have a low correlation between the two criteria.
Policies based on number of patients waiting have smaller areas in the ellipses; thus, they are
more precise. In addition, P1 confirms that Doff parameter is not significant for this policy.
CONCLUSIONS

Overcrowding of ED’s across the United States has been a concern for many people. Diverse actions exist
to relieve congestion from ED’s and one of them is AD. AD has been widely discussed by the medical
community for the potential effects of delaying treatment. However, there is a lack of applying modeling
tools to analyze the impact of AD. This paper proposes a methodology that graphically evaluates the
trade-off between time spent on diversion and average waiting time for patients in the ED.
Even though the policies studied using a single threshold may be simple, the findings can be used to
explain the behavior seen in real systems, given that most AD decisions are made based primarily on a
single factor in reality. Thus, the insights gained from this study can be used for AD decisions for improved quality of care. Future extensions of this research include the use of Genetic Algorithms to find
the form of the policies that yield the best results considering multiple criteria. In addition, Approximate
Dynamic Programming will be used to assess the diversion decision in order to minimize the non-value
added time in a multi-hospital system.

2323

Ramirez, Fowler and Wu
95% Simultaneous CI for P1

95% Simultaneous CI for P2

Confidence ellipses of instances with common Don value tend to overlap

95% Simultaneous CI for P3

95% Simultaneous CI for P4

Confidence ellipses of instances with different
Don value overlap more
frequently and have larger
area than the previous policies

95% Simultaneous CI for P5

95% Simultaneous CI for P6

Confidence ellipses of instances with different
Don value overlap more
frequently and slope of large
axes implies small correlation
between variables (random results)

Figure 6: 95% Simultaneous confidence intervals

2324

Ramirez, Fowler and Wu
REFERENCES
Asplin, B.R. 2003. Editorial: Does Ambulance Diversion Matter?, Annals of Emergency Medicine 41:
477-480.
Carlyle, W.M., J.W. Fowler, E.S. Gel, and B. Kim. 2003. Quantitative Comparison of Approximate Solution Sets for Bi-criteria Optimization Problems, Decision Sciences 34:63-82.
Centers for Disease Control and Prevention. 2006. Staffing, Capacity, and Ambulance Diversion in
Emergency Departments: United States, 2003-04, Advance Data from Vital and Health Statistics 376.
Centers for Disease Control and Prevention. 2008. National Hospital Ambulatory Medical Care Survey:
2006 Emergency Departments Summary, National Health Statistics Reports 7.
Cochran, J.K., and A. Bharti. 2006. A Multi-Stage Stochastic Methodology for Whole Hospital Bed Planning under Peak Loading, International Journal of Industrial and Systems Engineering 1:8-36.
Cochran, J.K., and K.T. Roche. 2009. A Multi-Class Queuing Network Analysis Methodology for Improving Hospital Emergency Department Performance, Computers & Operations Research 36:14971512.
Hagtvedt, R., P. Griffin, P. Keskinocak, M. Ferguson and F.T. Jones. 2009. Cooperative Strategies to Reduce Ambulance Diversion, In Proceedings of the 2009 Winter Simulation Conference, ed. M.D.
Rossetti, R.R. Hill, B. Johansson, A. Dunkin, and R.G. Ingalls, 1861-1874. Piscataway, New Jersey:
Institute of Electrical and Electronics Engineers, Inc.
Kolker, A. 2008. Process Modeling of Emergency Department Patient Flow: Effect of Patient Length of
Stay on ED Diversion, Journal of Medical Systems 32:389-401.
Law, A.M. 2007. Simulation Modeling & Analysis. 4th ed. McGraw-Hill, Inc.
Massachusetts Nurse Newsletter. 2009. State’s’ no diversion policy’ is putting strain on Massachusetts
hospitals. April 2009:8-9
Miller, M., D. Ferrin, and N. Shahi. 2009. Estimating Patient Surge Impact in Several Regional Emergency Departments, In Proceedings of the 2009 Winter Simulation Conference, ed. M.D. Rossetti, R.R.
Hill, B. Johansson, A. Dunkin, and R.G. Ingalls, 1906-1915. Piscataway, New Jersey: Institute of
Electrical and Electronics Engineers, Inc.
Montgomery, D.C.2005. Design and Analysis of Experiments. 6th ed. John Wiley & Sons, Inc.
Pham, J.C., R. Patel, M.G. Millin, T.D. Kirsch and A. Chanmugam. 2006. The Effects of Ambulance Diversion: A Comprehensive Review, Journal of the Academic Emergency Medicine 13:1220-1227.
Ramirez, A., J.W. Fowler, and T. Wu. 2009. Analysis of Ambulance Diversion Policies for a Large-Size
Hospital, In Proceedings of the 2009 Winter Simulation Conference, ed. M.D. Rossetti, R.R. Hill, B.
Johansson, A. Dunkin, and R.G. Ingalls, 1875-1886. Piscataway, New Jersey: Institute of Electrical
and Electronics Engineers, Inc.
Ramirez, A., J.W. Fowler, E.S. Gel, and T. Wu. 2010a. Analysis of Min-Max Ambulance Diversion Policies Using Queuing Theory, In Proceedings of the 2010 Industrial Engineering Research Conference,
ed. A. Johnson and J. Miller.
Ramirez, A., J.W. Fowler, and T. Wu. 2010b. Bi-Criteria Analysis of Single-Threshold Ambulance Diversion Policies. Manuscript.
Springer’s International Series. 2006. Patient Flow: Reducing Delay in Healthcare Delivery. Edited by
Randolp W. Hall.
Tan, P.-N., M. Steinbach and V. Kumar. 2006. Introduction to Data Mining. 1st ed. Pearson Education,
Inc.
United States General Accounting Office. 2003. Hospital Emergency Departments: Crowded Conditions
Vary among Hospitals and Communities. Report to the Ranking Minority, Committee on Finance,
U.S. Senate.

2325

Ramirez, Fowler and Wu
AUTHOR BIOGRAPHIES
ADRIAN RAMIREZ is a PhD student in Industrial Engineering in the School of Computing, Informatics and Decision Systems Engineering at Arizona State University. His research interests include modeling, simulation and analysis of healthcare delivery systems. He received a MS in Manufacturing Systems
at ITESM and a BS in Industrial Engineering at Universidad de Sonora, both in Mexico. His email address is <adrian.ramirez@asu.edu>.
JOHN W. FOWLER is a Professor in the Operations Research and Production Systems group of the
School of Computing, Informatics and Decision Systems Engineering at Arizona State University. His research interests include modeling, analysis, and control of manufacturing and service systems. He is a
Fellow of the Institute of Industrial Engineers and is the SCS representative on the Board of Directors of
the Winter Simulation Conference. He is an Area Editor of the Transactions of the Society for Computer
Simulation International, an Associate Editor of IEEE Transactions on Semiconductor Manufacturing,
and will be the Editor of a new journal entitled IIE Transactions on Healthcare Systems Engineering. His
email address is <john.fowler@asu.edu>.
TERESA WU is an Associate Professor of the School of Computing, Informatics and Decision Systems
Engineering at Arizona State University. She received her Ph.D. in Industrial Engineering from the University of Iowa in 2001. Her current research interests include: distributed decision support, distributed
information system, healthcare informatics and data fusion.. Professor Wu has over 40 articles published
(or accepted) in such journals as International Journal of Production Research, Information Science, IEEE
Transactions on Engineering Management, ASME: Journal of Computing and Information Science in Engineering. She serves on the Editorial Review Board for International Journal of Production Research,
IEEE Transactions on Engineering Management, Computer and Standard Interface, International Journal
of Electronic Business Management. Her email address is <teresa.wu@asu.edu>.

2326

www.sciedu.ca/air

Artificial Intelligence Research

2015, Vol. 4, No. 2

ORIGINAL RESEARCH

Augmenting cost-SVM with gaussian mixture
models for imbalanced classification
Miao He, Teresa Wu∗, Alvin Silva, Dianna-Yue Zhao, Wei Qian
School of Computing, Informatics, Decision Systems Engineering, Arizona State University, United States

Received: May 10, 2015
DOI: 10.5430/air.v4n2p93

Accepted: June 22, 2015
Online Published: July 3, 2015
URL: http://dx.doi.org/10.5430/air.v4n2p93

Abstract
The Support Vector Machine (SVM), a known discriminative classifier is ineffective in dealing with imbalanced classification
problems where the training examples of target class are outnumbered by non-target class examples. Though cost-SVM (cSVM)
has been proposed to tackle the imbalanced datasets by assigning different cost functions to different classes, the performance
is less than satisfactory due to its limited ability to enforce cost-sensitivity. In this research, a generative classifier, Gaussian
Mixture Model (GMM) is studied which can learn the distribution of the imbalanced data to improve the discriminative power
between imbalanced classes. By fusing this knowledge into cSVM, a model fusion approach, termed CSG (cSVM+GMM), is
proposed to tackle the imbalanced classification problem. Experimental results on eleven benchmark datasets and one medical
imaging dataset show the effectiveness of CSG in dealing with imbalanced classification problems.

Key Words: Imbalanced classification, Support vector machines, Gaussian mixture model, Supervised learning

1

Introduction

Classification is a supervised learning problem which identifies the labels of new observations given a training dataset.
Based on the number of classes studied, there exists multiclass classification and binary classification. Multiclass
classification is usually treated under the one-versus-one or
one-versus-all framework[1] both of which use binary classifier as the base classifier. One of the most commonly used
binary classifiers is support vector machine (SVM) developed by Vapnik et al. in 1995.[2] Extensive research has
explored the performance of SVM and concludes that SVM
outperforms many other conventional methods in classification. For example, Bazzani1 et al.[3] apply a SVM classifier
to separate false signals from micro calcifications in digital mammograms. The result shows that the SVM achieves
better/comparable performance than multi-layer perceptron

(MLP)[4] and linear discriminant analysis (LDA).[5] Shon et
al.[6] propose a SVM based classification method to tackle
the internet anomaly detection and conclude that SVM outperforms the real-world employed Network Intrusion Detection Systems (NIDS),[7] just to name a few.
While promising, SVM is known to be ineffective in dealing with imbalanced datasets[8–10] where the minority class
(named positive class in this paper) is greatly outnumbered
by the majority class (negative class). Indeed, in many applications, minority class possesses higher misclassification
cost than majority class. For example, in the field of medical
diagnosis (diseased patients), fraud detection (true frauds),
identifying the minority examples is more of interest. Unfortunately, the performance of the standard SVM on minority class labeling is less than satisfactory. This is because the
SVM algorithm assumes balanced class distribution and as-

∗ Correspondence: Teresa Wu; Email: Teresa.Wu@asu.edu; Address: School of Computing, Informatics, Decision Systems Engineering, Arizona
State University, United States.

Published by Sciedu Press

93

www.sciedu.ca/air

Artificial Intelligence Research

signs same penalty considerations to both majority and minority classes in the training process. As a result, the class
boundary of SVM skews towards the minority class leading
to high false-negative rate.[11]
Due to the significance and the prevalence of imbalanced
datasets, many researchers explore ways to extend SVM
for imbalanced classification. In general, the extensions
can be divided into two categories: data preprocessing approach and algorithmic approach. The data preprocessing
approaches use different sampling techniques to alter the
input data distribution to reduce the degree of class imbalance. The representative methods are: undersampling (US),
oversampling (OS) and synthetic data generation method
such as SMOTE.[12] The preprocessing approaches are usually combined with different classifiers to achieve classification. For instance, Akbani et al.[13] compare the performance of SMOTE-SVM and SMOTE-cSVM (cost SVM[8] )
on imbalanced datasets. Instead of modifying the distribution of the input data, the algorithmic approaches modify SVM algorithm directly to make it less sensitive to
class imbalance. Some examples of algorithmic methods
are: boundary movement (BM-SVM)[14] which shifts the
decision boundary by adjusting the threshold parameter of
the standard SVM; kernel modification method[11, 14] which
modifies the associated kernel matrix K; and cost sensitive SVM (cSVM)[8] which applies cost-sensitive learning
in SVM training by assigning different costs to different
classes. It is noted from the literatures[15–17] that cSVM
method is promising in dealing with imbalanced classification problems. This is because in Bayes decision theory, the
costs associated with false positives and false negatives are
generally unequal. Taking cancer diagnosis as an example,
if a cancer patient is diagnosed as non-cancer, the associated
cost would be missing the best timing for treatment which
can be life threatening. On the other hand, the associate cost
is much less if a non-cancer patient is diagnosed as having
cancer, in which case only follow-up tests are needed for
confirmation. The unequalness of this false positive/false
negative costs can be further aggravated by the class imbalance due to the limited number of target-class examples to
learn. Therefore, classifier designed using cost sensitive algorithms (e.g. cSVM) may be a good choice in dealing with
an imbalanced dataset.[16] However, many empirical studies[11, 16, 18] show that cSVM does not work well as expected.
As explained by Wu et al.,[11] this is due to the fact that
cSVM has limited ability to enforce cost-sensitivity. Specifically, cSVM assigns higher cost to the positive class in order to increase the influences of the positive support vectors.
The impact of a support vector is directly reflected by the
value of its coefficient. However, the cost function serves as
the upper bound, rather than lower bound, of support vector
coefficients according to the Karush Kuhn Tucker (KKT)
conditions. Thus, increasing of the cost does not necessarily affect the coefficients. In addition, the overall influences
from positive and negative support vectors are forced to be
94

2015, Vol. 4, No. 2

equal according to the KKT condition (see validation in Section 4.2). As a result, the increase of positive support vector coefficients will inevitably increase some negative support vector coefficients which may lead to the unsatisfactory
classification performance.
To address these issues, many researchers propose ways
to improve cSVM’s. Masnadi-Shirazi et al.[16] replace
the hinge lose function of cSVM with cost-sensitive hinge
lose function to enforce cost-sensitivity. Akbani et al.[13]
combine cSVM with SMOTE method to make the boundary well-defined. Brefeld et al.[19] use example dependent cost instead of class dependent cost to further enforce
cost-sensitivity of cSVM. Note these extensions focus on
the discriminative models only which are designed to classify positive and negative class examples directly based on
the provided input data.[20] While being directed to classify the data, the potential contributions from the underlying knowledge of the input data (e.g., distributions, clusters)
may be ignored. Alternatively, generative models[20] study
the probability distribution of the training data, and apply
Bayes rules to obtain the posterior probability for classification. In addition, generative models can incorporate the
domain knowledge of the training data, i.e. the prior knowledge about the interaction among the variables, the data
clustering and the parameter’s range of values into the classification process. The complementary nature of discriminative and generative models motivates us to take a model fusion approach, termed CSG, by integrating cSVM with one
type of generative models, Gaussian mixture model (GMM)
to tackle the imbalanced classification problem. GMM is
chosen here because it is computationally inexpensive and
has fewer subjective parameters to adjust.[21] In addition,
probability outputs from cSVM and GMM enable us to develop a unified formulation for integration. To test the performance of CSG, we conduct the experiments on eleven
KEEL benchmark datasets and one medical imaging dataset
collected from Mayo Clinic, Arizona. Experimental results
show that CSG is effective in dealing with imbalanced classification problems.
The rest of the paper is organized as follows: in Section 2 we
discuss the related work. In Section 3 we describe the CSG
algorithm in detail followed by the comparison experiments
in Section 4. We conclude with the findings and future work
in Section 5.

2
2.1

Related work
Data preprocessing approaches

The data preprocessing approaches use different sampling
techniques to alter the size and distribution of the training
data in order to reduce class imbalance. Some common
data preprocessing methods used in imbalanced classification are: undersampling,[22] oversampling[22] and the synthetic minority oversampling technique (SMOTE).[12]
ISSN 1927-6974

E-ISSN 1927-6982

www.sciedu.ca/air

Artificial Intelligence Research

Undersampling and oversampling are designed to rebalance
the training data in different ways: undersampling decreases
the size of majority class, while oversampling increases
the size of minority class. The problematic consequences
thus are different.[23–25] Undersampling reduces the imbalanced ratio by randomly removing the majority examples
and thus may lead to the loss of information about the majority class. Oversampling increases the size of the minority class by randomly duplicating the minority examples
which may cause over fitting.[10] The synthetic data generation method SMOTE[12] increases the size of the minority
class by generating artificial data which are convex combinations of the existing ones with its nearest neighbors, thus
improves learning.

expected performance. The reason, as discussed by Wu et
al.,[11] is that cSVM has issues for enforcing cost-sensitivity.
Though research proposes cost-sensitive hinge loss function into cSVM,[16] integrating SMOTE with cSVM[13] and
employing example dependent cost in cSVM training process,[19] the focus has only been on discriminative models. In this research, we integrate cSVM with a generative
model, GMM, which incorporates the data distribution information into the training process to tackle the imbalanced
classification problem. The detail of our proposed CSG is
explained in the following section.

3
3.1

2.2

Algorithmic approaches

The algorithmic approaches augment the SVM formulation
to make it more tolerate to the class imbalance. Based on
the parameters to be adjusted, the algorithmic approaches
are in general classified into three subcategories: boundary
movement (BM-SVM)[14, 26] kernel modification[11, 14] and
cost-SVM (cSVM).[8]
Let the decision function of SVM be:

sgn(f (x) =

n
X

Proposed algorithm: CSG
SVM basics

SVM finds the decision boundary by constructing the separation hyperplane with maximum margin between different
classes. The data points closest to the hyperplane are called
support vectors in the soft-margin formulation.[2]
n

X
1
ξi
min w · w + C
2
i=1
(2)

s.t. yi (w · Φ(xi ) + b) ≥ 1 − ξi
ξi ≥ 0, i = 1, · · · , n

yi αi K(x, xi ) + b)

(1)

i=1

As seen in (1), there are three parameters which impact the
formation of the classification boundary: b, K and α. BMSVM method shifts the class boundary by adjusting b, the
threshold of the standard SVM. In the cases the data is nonseparable, where the expected modifications should be on
both the separating hyperplane w and threshold b, BM-SVM
may not be performed.[16] The kernel modification method,
Kernel-boundary alignment (KBA) on the other hand, tackles the imbalanced learning problem by modifying the associated kernel matrix K. This method adjusts the class
boundary by using the adaptive conformal transformation
(ACT) method based on the consideration of the featurespace distance and class-imbalanced ratio, and reduces the
imbalanced support-vector ratio by reducing the number of
support vectors from the majority class. However, removing existing negative support vectors may lead to the loss
of information of the majority class and thus may introduce
new bias. The cSVM, proposed by Veropoulos,[8] assigns
different cost functions which are used as upper bounds
to constrain α (formulations are presented in Section 3.2).
Since it assigns higher cost to the minority class than majority class, the skewed class boundary can be pushed away
from the minority class thus the accuracy of minority classification is improved. Based on the Bayes decision theory, cSVM is supposed to be a promising method in dealing
with imbalanced classification problems. Yet, a number of
empirical studies[11, 16, 18] show cSVM does not always have
Published by Sciedu Press

2015, Vol. 4, No. 2

Finding the support vectors is the key issue for the SVM
classifier. This is because the decision function (in (1)) of a
new testing data x is calculated based on the similarity measurement (kernel function K) between x and all the existing support vectors. The coefficients for non-support vector
data points are zero (αi =0) in (1). This indicates that the
non-support vector data points have no impact on classification of the new testing data x once the support vectors has
been determined.
The performance of the SVM classifier mainly relies on the
choice of kernel function and the tuning of various parameters in the kernel function. The kernel function K (xi , xj )
is a similarity measure between the pair of data points xi
and xj . The kernel method works by mapping the two data
points from original input space (xi and xj ) onto the highdimensional feature space (ϕ(xi ) and ϕ(xj )). The kernel
function is calculated by taking the inner product of the
transformed data vector:
2

K(xi , xj ) = hΦ(xi ), Φ(xj )i = e(−γkxi −xj k ) , γ > 0 (3)
In this paper, we choose the most commonly used radial basis function (RBF) kernel (in (3)) for its good performance
on various domain applications.[27]
The SVM algorithm predicts the label of a test example x
by computing the sign function in (1). Instead of predicting
the label, much research requires the posterior class prob95

www.sciedu.ca/air

Artificial Intelligence Research

ability P (y|x). Platt[28] proposes a method to approximate
the posterior probability by using
1
(4)
1 + e(Af (x)+B)
where A and B are estimated by minimizing the negative log
likelihood of training dataset (xi , yi ):

2015, Vol. 4, No. 2

evitably increase some coefficients of negative support vectors which may weaken the discriminative power in identifying the minority examples.

PA,B (x) = P (Y = 1|X = x) =

(A∗ , B ∗ ) = arg max
A,B

ny

X
i=1

1 + yi
1 − yi
log(PA,B (x0i )) +
log(1 − PA,B (x0i ))
2
2

3.3

GMM is a generative model applied in many applications
such as object classification and speech recognition.[29–32]
Based on the training data, GMM models the probability
density function of the feature vector x by using a mixture

of weighted Gaussians.

(5)
In our proposed method, we also use the probability outputs
of cSVM to fuse with the GMM probabilities in order to
benefit from both methods.
3.2

PGM M (x|yi ) =

M
X

i|yi =+1

2
N (x, µim , σim
)=

i|yi =−1

(6)

s.t. yi (w · Φ(xi ) + b) ≥ 1 − ξi ;
ξi ≥ 0, i = 1, · · · , n
The Lagrangian for the cSVM formulation is:


n+
n−
X
X
w2
ξi + C −
ξi 
+ C C +
Lp =
2
i|yi =+1

−

n
X

i|yi =−1

α1 [yi (w · xi + b) − 1 + ξi ] −

n
X

(7)

µi ξi

i=1

i=1

With the constraints on αi as follows:


0 ≤ αi ≤ C +
0 ≤ αi ≤ C −

n
X
if yi = +1
and
αi yi = 0 (8)
if yi = −1
i=1

cSVM assigns different cost functions C + and C − to the
positive and negative classes respectively. The unequal setting of cost functions will allow the class boundary to be
skewed towards the class with higher costs. In cSVM, one
can assign higher costs to the minority class examples to
push the class boundary toward the majority class. Yet,
cSVM suffers from two drawbacks: first, cSVM changes the
upper bound (C + , C − ) of the support vector coefficients αi ,
instead of working on αi directly. Thus, increasing of C +
does not always
Pn guarantee a change of αi . Second, the KKT
condition i=1 αi yi = 0 (in (8)) imposes equal influences
from positive/negative support vectors. As a result, the increase of some positive support vector coefficients will in-

(9)

Where:

cSVM



n+
n−
X
X
1
ξi + C −
ξi  ;
min w · w + C C +
2

2
cim N (x, µim , σim
)

m=1

In cSVM, the formulation is given as:

96

GMM basics



1
d

2 )2
(2πσim

e

− 21

kx−µim k2



σ2
im

(10)

2
cim , µim , and σim
are the weight, mean and covariance of
th
the m mixture for class i. M is the number of mixtures
which should be defined by the user. The GMM method
is an unsupervised method that only reflects the intra-class
information. Given a training dataset with binary class labels {(x1 , y1 ), · · · , (xn , yn )}, y ∈ {−1, 1}, the data are
separated into two groups according to their class label.
2
Then the coefficients cim , µim , and σim
for each mixture
are computed using an Expectation Maximization (EM) algorithm.[33] The EM algorithm is an iterative method for
finding the maximum likelihood function of the parameters. Starting from some initial estimate of parameters,
the iteration alternates between E step and M step where
in the E step, the algorithm evaluates the expectation of
the log-likelihood using the current parameters; in the M
step, it computes the new parameters to maximize the loglikelihood function found in the E step. The stopping criterion for the iterations could be either convergence to a local
maxima, or the difference between two consecutive iterations is smaller than a small value.

Once the coefficients are obtained, Bayesian rules can be
used to calculate the posterior class probability:
P (yi |x) = P (yi )

X

2
P (m|yi )N (x|µim , σim
)

(11)

m

3.4

Proposed algorithm: CSG

In this research, we propose a model fusion based approach
to integrate cSVM discriminative algorithm with GMM
generative algorithm which is explained as follows.
ISSN 1927-6974

E-ISSN 1927-6982

www.sciedu.ca/air

Artificial Intelligence Research

Table 1: Notations used in CSG algorithm
Symbol

Meaning

Xtrain

training dataset

Xtest
y

testing dataset
True label

ypred
NumF

Predicted label
Number of folds in cross validation

n+ , n -

Number of Gaussian centers for
positive/negative class

c, µ, σ2
q

GMM parameters
Cost for positive class in cSVM

PcSVM (+1|x),
PcSVM (-1|x)

Probability outputs of cSVM

PGMM (x|+1),
PGMM (x|-1)

Probability distribution of GMM

PGMM (+1|x),
PGMM (-1|x)

Posterior probabilities of GMM

Pfinal (+1|x)

Modified posterior probability for positive
class

β1, β2

Combining coefficients

A

Search range of ߚଵ

B

Search range of ߚଶ

C-matrix
Sen

Confusion matrix
Sensitivity

Spe

Specificity

2015, Vol. 4, No. 2

positive, while the negative test examples remain negative
in prediction.

Note that the RBF parameters: kernel parameters γ, c, combining coefficients β1 and β2 , cost ratio q, are obtained by
the grid search method. The search ranges of parameters are
defined according to the empirical experience. The detailed
Figure 1: The CSG Algorithm
parameter settings are discussed in Section 4.
In the CSG algorithm, we combine posterior probabilities of
cSVM and GMM for the final classification. The Gaussian
mixtures from both positive and negative classes are used
to modify the class boundary by adjusting the positive class
posterior probability (in (12)). The prediction is made by
comparing the posterior probability for each class.

As seen in Figure 2, circles are positive class examples
and dots are negative class examples. In Figure 2(a)
and 2(b), CSG finds the mixture of Gaussians for positive and negative class respectively. Figure 2(c) shows
that CSG pushes the class boundary of cSVM towards
the negative class. This is achieved by modifying the
cSVM probability output with the GMM probabilities usPf inal (y = +1|xi ) = PcSV M (y = +1|xi )+
(12) ing (12). For illustration, let C be a positive class exβ1 · PGM M (y = +1|xi ) − β2 · PGM M (y = −1|xi )
ample, assume cSVM predicts C as negative class with
The assumption of integrating the cSVM and GMM poste- PcSV M (+1|C) = 0.45 and PcSV M (−1|C) = 0.55. By
rior probabilities as in (12) is: a positive testing example using GMM method, we find PGM M (+1|C) = 0.3 and
xi should generally be closer to the positive Gaussian mix- PGM M (−1|C) = 0.7. If we choose β1 = β2 =1, according
ture centers than negative Gaussian mixture centers. There- to (12), we have Pf inal (+1|C) = 0.45 +1*0.3-1*0.7 = 0.05.
fore, PGM M (+1|xi ) should be greater than PGM M (−1|xi ). Then, C will be predicted as positive since Pf inal (+1|C) >
On the other hand, a negative testing example should have PcSV M (−1|C). This example shows CSG can push the
PGM M (+1|xi ) less than its PGM M (−1|xi ) in general. By class boundary of cSVM towards the negative class to imcarefully tuning the coefficients β1 and β2 , the positive test prove the discriminative power in identifying the positive
examples may have a better chance of being predicted as examples.

Published by Sciedu Press

97

www.sciedu.ca/air

Artificial Intelligence Research

2015, Vol. 4, No. 2

Figure 2: Illustration example of CSG algorithm

4

Experiments and results

In this section, we first test the performance of CSG using eleven KEEL benchmark datasets.[34] Next, we use a
medical imaging dataset to test the applicability of CSG
on a real world application. To evaluate the performance
of the classifiers, we use the Gmean[35] metric which has
been widely used for evaluating classifiers
on imbalanced
√
datasets.[13, 36, 37] Gmean is defined as acc+ ? acc− , where
acc+ (also called sensitivity) and acc− (also called specificity) are positive and negative class prediction accuracy,
respectively. Other than Gmean, sensitivity is of great interest in many imbalanced learning domains,[13, 38, 39] because improving the prediction accuracy on the minority
class is the focus of many domain applications. In this
section, we focus the discussion on Gmean and sensitivity to show the outperformance of CSG. Specificity measure is also provided. In addition to sensitivity, specificity
and Gmean, Area Under the Curve (AUC) metric which
has been widely used in imbalanced classification problems[10, 22] is provided.
4.1

KEEL benchmark datasets

The eleven benchmark datasets we used in the experiments
are collected from KEEL-dataset repository.[34] The details
of the datasets are listed in Table 2. The imbalance ratio (IR)
varies from 2 to 130 among these datasets. The original
multiclass datasets are preprocessed as binary class problems, and the number in name of dataset indicates a positive
class. For example, in vehicle2, class 2 is used as positive
class and all the other classes in the original data have been
joined to represent the negative class.
In the experiments, we first compare CSG with the standard
SVM and cSVM algorithms to show fusing GMM knowledge into cSVM can improve the classification on imbalanced datasets. Then we compare the performance of CSG
with SMOTE based algorithms such as SMOTE-SVM and
SMOTE-cSVM which has been compared in many litera98

tures.[13, 18, 39] Lastly, we further explore the effect of sampling on CSG by combining SMOTE with the CSG algorithm.
We use LIBSVM[40] MATLAB codes to build the SVM and
cSVM models. SMOTE method is applied to preprocess the
datasets using KEEL data mining software.[34] The datasets
are oversampled until both the classes are equal in number.
We apply 10-fold stratified cross validation on each dataset
so that the GMM method would have equal number of positive examples to train in each fold. In each fold, we use the
SMOTE data to train the model and original data to test the
model performance. The results of the 10-folds are aggregated to form the final result. Due to the random nature of
the GMM algorithm, each experiment of CSG algorithm has
been run 20 times and the mean and standard deviation has
been listed. The parameters: RBF kernel parameters γ, c,
combining coefficients β1 , β2 , cost ratio q are obtained by
the grid search method. The searching ranges of the parameters are defined according to the empirical experience. γ is
searched from 0 to 512, c from 0 to 2048, β1 , β2 from 0 to
1010 . q is related to the class IR. The search range for q is
from 1 to IR1.4 .
Table 2: The KEEL dataset used in the experiments
Dataset

#Examples

#Attributes

#Positive

#Negative

Imbalance
Ratio

pima

768

8

268

500

1.9

haberman
contracepti
ve2
hepatitis

306

3

81

225

2.8

1473

9

333

1140

3.4

80

18

13

67

5.2

yeast3

1484

8

163

1321

8.1

glass2
cleveland_
0_vs_4
pageblocks
2
flareF
winequalit
y_red_4
abalone19

214

9

17

197

11.6

173

13

13

160

12.3

548

10

33

515

15.6

1066

11

43

1023

23.8

1599

11

53

1546

29.2

4174

9

32

4142

129.4

ISSN 1927-6974

E-ISSN 1927-6982

www.sciedu.ca/air

Artificial Intelligence Research

2015, Vol. 4, No. 2

Table 3: Results of sensitivity, specificity and Gmean
Algorithmic approach

Preprocessing approach

SVM

cSVM

CSG

SMOTE-SVM

SMOTE-cSVM

SMOTE-CSG

Sen

0.519

0.705

0.721 ± 0.016

0.728

0.746

0.791 ± 0.001

Spe

0.876

0.708

0.709 ± 0.015

0.742

0.738

0.706 ± 0.002

Dataset

pima

haberman

contracepti
ve2

hepatitis

yeast3

glass2

cleveland_0
_vs_4

pageblocks
2

flareF

winequality
_red_4

abalone19

Gmean

0.674

0.707

0.715 ± 0.003

0.735

0.742

0.747 ± 0.001

Sen

0.198

0.333

0.614 ± 0.023

0.593

0.654

0.704 ± 0.000

Spe

0.951

0.907

0.673 ± 0.013

0.742

0.680

0.661 ± 0.005

Gmean

0.433

0.550

0.643 ± 0.014

0.663

0.667

0.682 ± 0.002

Sen

0.159

0.270

0.541 ± 0.000

0.423

0.471

0.598 ± 0.000

Spe

0.969

0.932

0.725 ± 0.000

0.807

0.768

0.699 ± 0.001

Gmean

0.393

0.502

0.626 ± 0.000

0.585

0.602

0.647 ± 0.000

Sen

0.231

0.385

0.746 ± 0.043

0.769

0.846

0.923 ± 0.000

Spe

0.985

0.955

0.841 ± 0.039

0.866

0.866

0.821 ± 0.000

Gmean

0.477

0.606

0.791 ± 0.013

0.816

0.856

0.870 ± 0.000

Sen

0.791

0.840

0.853 ± 0.000

0.963

0.963

0.963 ± 0.000

Spe

0.976

0.953

0.943 ± 0.000

0.907

0.907

0.907 ± 0.000

Gmean

0.879

0.895

0.897 ± 0.000

0.935

0.935

0.935 ± 0.000

Sen

0.000

0.118

0.694 ± 0.055

0.706

0.882

0.897 ± 0.025

Spe

0.990

0.995

0.724 ± 0.047

0.858

0.711

0.721 ± 0.013

Gmean

0.000

0.342

0.707 ± 0.017

0.778

0.792

0.804 ± 0.005

Sen

0.077

0.077

0.665 ± 0.056

0.615

0.538

0.615 ± 0.000

Spe

1.000

1.000

0.565 ± 0.039

0.688

0.800

0.800 ± 0.000

Gmean

0.277

0.277

0.611 ± 0.019

0.650

0.656

0.702 ± 0.000

Sen

0.485

0.515

0.577 ± 0.000

0.606

0.636

0.879 ± 0.000

Spe

0.996

0.996

0.994 ± 0.000

0.963

0.922

0.784 ± 0.000

Gmean

0.695

0.716

0.757 ± 0.000

0.764

0.766

0.830 ± 0.000

Sen

0.023

0.116

0.653 ± 0.036

0.907

0.907

0.907 ± 0.000

Spe

0.999

0.994

0.790 ± 0.032

0.833

0.833

0.833 ± 0.000

Gmean

0.152

0.340

0.718 ± 0.019

0.869

0.869

0.869 ± 0.000

Sen

0.000

0.000

0.585 ± 0.000

0.585

0.585

0.623 ± 0.000

Spe

1.000

1.000

0.498 ± 0.002

0.735

0.735

0.704 ± 0.000

Gmean

0.000

0.000

0.540 ± 0.001

0.656

0.656

0.662 ± 0.000

Sen

0.000

0.031

0.613± 0.033

0.813

0.813

0.813 ± 0.000

Spe

1.000

0.990

0.687 ± 0.024

0.733

0.772

0.772 ± 0.000

Gmean

0.000

0.176

0.648 ± 0.015

0.772

0.792

0.792 ± 0.000

Table 3 presents the sensitivity, specificity and Gmean measures of each method. For algorithmic approaches, SVM
shows good specificity but poor sensitivity in general for all
eleven experiments since it trends to predict all examples as
majority (negative) class. Both cSVM and CSG show improvements on the sensitivity with sacrifice on specificity to
some extent. CSG achieves highest sensitivity for all eleven
datasets, and for five datasets (glass2, cleveland_0_vs_4,
flareF, winequality_red_4, abalone19) on which SVM and
Published by Sciedu Press

cSVM fails completely, CSG works reasonably well. This
is because CSG exploits the underlying knowledge of the
imbalanced data distribution in the model building and thus
further improves the discriminative power of positive examples. For SMOTE-based methods, SMOTE-CSG shows
best sensitivity on seven out of eleven datasets, and equal
sensitivity on the remaining four datasets (yeast3, cleveland_0_vs_4, flareF and abalone19). In conclusion, CSG
method is effective in dealing with imbalanced classifica99

www.sciedu.ca/air

Artificial Intelligence Research

tion problems.
In all eleven datasets, CSG achieves best Gmean among all
three algorithmic approaches, while SMOTE-CSG achieves
best Gmean among all three preprocessing approaches.
Compared with SVM, cSVM shows better Gmean measures in nine out of eleven datasets, while CSG further improves cSVM in all eleven datasets by fusing the underlying knowledge of the data distributions to the model training process. As a result, CSG is able to further enhance
the Gmean measure on datasets, such as abalone19 and
winequality_red_4, where cSVM shows little or even no
improvement over SVM. Compared with SVM and cSVM,
SMOTE based methods, SMOTE-SVM and SMOTE-cSVM
show improved Gmean on all eleven datasets. This indicates
that SMOTE is effective in enhancing the classifiers (SVM
and cSVM) on imbalanced datasets. Similarly, the SMOTE-

2015, Vol. 4, No. 2

CSG method also achieves better Gmean than the CSG
method. Among all three SMOTE based methods, SMOTECSG outperforms others in nine out of eleven datasets, and
in the other two datasets it has equal Gmean with the second
best method SMOTE-cSVM. These results show that CSG
is effective in dealing with imbalanced datasets.
SMOTE-CSG shows significant improved performance than
the CSG on all eleven datasets. SMOTE oversamples the
data by adding synthetic data instances which are generated
using convex combinations of the existing data. In SMOTECSG method, SMOTE provides more training data to CSG
algorithm which can aid the training process of cSVM and
GMM, and thus lead to better class separation. In all, the
experimental results indicate that the preprocessing method
SMOTE is necessary in dealing with imbalanced datasets.

Table 4: Results of AUC
Algorithmic approach

Preprocessing approach

SVM

cSVM

CSG

SMOTE-SVM

pima

0.719

0.728

0.736 ± 0.007

0.740

0.748

0.750 ± 0.001

haberman

0.627

0.658

0.670 ± 0.011

0.657

0.658

0.680 ± 0.002

contraceptive2

0.604

0.631

0.661 ± 0.000

0.610

0.611

0.636 ± 0.000

hepatitis

0.705

0.773

0.864 ± 0.016

0.760

0.816

0.844 ± 0.000

yeast3

0.894

0.913

0.916 ± 0.000

0.938

0.938

0.938 ± 0.000

Dataset

SMOTE-cSVM

SMOTE-CSG

glass2

0.563

0.603

0.745 ± 0.023

0.834

0.807

0.821 ± 0.013

cleveland_0_vs_4

0.608

0.608

0.666 ± 0.019

0.650

0.642

0.693 ± 0.000

pageblocks2

0.788

0.815

0.842 ± 0.000

0.797

0.778

0.829 ± 0.000

flareF

0.574

0.599

0.752 ± 0.014

0.878

0.878

0.878 ± 0.000

winequality_red_4

0.564

0.564

0.572 ± 0.001

0.657

0.657

0.658 ± 0.000

abalone19

0.569

0.599

0.682 ± 0.016

0.786

0.797

0.797 ± 0.000

The AUC metric in Table 4 shows similar results as Gmean
in Table 3. For algorithmic approaches, CSG shows better
AUC than both SVM and cSVM for all eleven datasets. For
preprocessing approaches, SMOTE-CSG shows better AUC
in eight out of eleven datasets, and equal AUC for the rest
three datasets.
Table 5: Pair t-test on Sensitivity, Specificity, G-Mean and
AUC (p<.05 indicating significant differences)
Algorithmic approach

Preprocessing approach

Metrics

CSG vs.
SVM

CSG vs.
cSVM

SMOTE-CSG vs.
SMOTE-SVM

SMOTE-CSG vs.
SMOTE-cSVM

Sensitivity

0.00008

0.00063

0.00996

0.01863

Specificity

0.00042

0.00226

0.11836

0.04760

Gmean

0.00124

0.00269

0.00344

0.01879

AUC

0.00214

0.00669

0.03539

0.01401

In addition, a pair t-test is conducted to draw statisti-

100

cal conclusions in comparing the performance of our proposed CSG with the other two algorithmic approaches:
SVM and CSG, and the performance of our proposed
SMOTE-CSG with the other two preprocessing approaches:
SMOTE-SVM and SMOTE-cSVM. As shown in Table
5, CSG statistically outperforms SVM and cSVM on
all four metrics, that is, sensitivity, specificity, Gmean
and AUC (p<.05). SMOTE-CSG statistically outperforms
SMOTE-SVM and SMOTE-cSVM on three metrics: sensitivity, Gmean and AUC. As for specificity, SMOTE-CSG
outperforms SMOTE-cSVM yet underperforms SMOTESVM. This may be explained that SMOTE-SVM (and even
SMOTE-cSVM) is designed to perform well on specificity.
As indicated earlier, this research focuses on sensitivity
which is more important for imbalanced data while Gmean
and AUC are overall measures considering the tradeoffs between the sensitivity and specificty. Therefore we conclude
CSG and SMOTE-CSG are satisfactory in handling imbalanced data.

ISSN 1927-6974

E-ISSN 1927-6982

www.sciedu.ca/air

Artificial Intelligence Research

2015, Vol. 4, No. 2

Figure 3: Gmeans for Low IR datasets and High IR datasets
To evaluate the effect of IR on each method, we divide
the datasets into low-IR group (IR<10) and high-IR group
(IR≥10). Figure 3 shows the Gmean measures of each
datasets in each group. Figure 3(a) and 3(b) are the comparison of SVM, cSVM and CSG, and Figure 3(c) and 3(d)
are the comparison of SMOTE-SVM, SMOTE-cSVM and
SMOTE-CSG. Figure 3(a) and 3(b) show that CSG has a
significant improvements of Gmean compared to SVM and
cSVM on high-IR datasets relative to low-IR datasets which
indicates CSG is very effective in dealing with highly imbalanced datasets on which SVM and cSVM performs poorly.
This is because in highly imbalanced datasets, the majority
class dominates the training of the SVM and thus the class
boundary is highly skewed. cSVM shows improved performance by assigning higher cost to the minority class, but its
performance is still less than satisfactory due to the limited
ability to enforce cost-sensitivity as we discussed in Section
3.2. CSG tackles the high imbalance issue by fusing the underlying knowledge of the data distribution (GMM) into the
training process of cSVM, and thus the skewed class boundary can be adjusted towards the majority class. In all, the
performance of CSG is much better on the high-IR group
than on low-IR group.
For SMOTE-based methods (see Figure 3(c) and (d)),
Published by Sciedu Press

SMOTE-CSG marginally improved Gmean over both
SMOTE-SVM and SMOTE-cSVM methods. This is because the SMOTE method oversamples the minority class
until the whole dataset is balanced and SVM generally performs well on balanced datasets since the class boundary of
SVM is not skewed. As a result, methods such as cSVM
and CSG which aim to adjust the skewed class boundary
would have marginal performance improvements over SVM
on balanced datasets.
To further test the performance of CSG, a real world renal
stone medical image dataset is collected from Mayo Clinic,
Arizona. The comparison experiment is discussed in the
next section.
4.2

Renal stone dataset

Renal stones, also called kidney calculi, are the solid crystal
aggregations formed in the kidneys from dietary minerals in
the urine. Renal stone disease can cause nausea and vomiting with sharp pain in the back or lower abdomen and sometimes blood in urine (e.g., hematuria).[41] It affects approximately one in eleven people in the United States.[42] Each
year, more than one million visits to health care providers
are related to the renal stone disease.[41] Based on the chemical composition, clinically relevant renal stones can be cat101

www.sciedu.ca/air

Artificial Intelligence Research

egorized into four types: unic acid, calcium oxalate, struvite
and cystine. The determination of the chemical composition
of renal stone is a key factor in preoperative patient evaluation, treatment planning and recurrence prevention.[43] The
commonly used stone analysis techniques include in vitro
x-ray diffraction, infrared spectroscopy and polarization microscopy.[44] These tests, unfortunately, are performed only
after the stones are extracted from the patients. In renal
stone preoperative evaluation, minimally invasive intervention is preferred for the benefits of the patients. Utilizing
noninvasive tests such as radiology imaging studies to identify the renal stone composition is of great interest.[45, 46]

Figure 4: The DECT image of renal stones (phantom
study)
Dual Energy CT (DECT) is a recently developed technique
used for the purpose of diagnostic imaging. Instead of acquiring a single data set as per conventional CT, it acquires
two simultaneous or near simultaneous data sets, one low
and one high energy, during a single acquisition. This setting enables DECT to differentiate materials with similar
electron densities but varying photon absorption abilities,[47]
improving noninvasive renal stone characterization.[48]
In this study, we collect 65 stones from the stone analysis laboratory at Mayo Clinic Arizona. All stones are extracted from previous patients through surgical and endoscopic intervention. The chemical composition has been determined with stereo microscopy and infrared spectrophotometry. According to the chemical composition, the 65
stones are divided into four groups: uric acid (n = 34), calcium oxalate (n = 18), cystine (n = 9) and struvite (n = 4).
The diameter of the stones varies from 2.6 mm to 6.2 mm
(mean size 3.5 mm). Among all four types of renal stones,
cystine stone is of great interest for the following reasons:

2015, Vol. 4, No. 2

first, cystine stone is usually too dense to be broken up by
applying extracorporeal shock wave lithotripsy as can be
done for some other types of stones. Instead, techniques
designed for removing dense stones, such as percutaneous
nephrolithotripsy (PNL), may be applied. Second, cysteine
stone is the result of cystinuria, which is a genetic autosomal recessive metabolic disorder.[49] Patients with cysteine stones may also need to take additional genetic screening tests other than medical treatment.[50] In this experiment, cystine stone has been selected as the target class, and
the other stone types are combined as the non-target class.
Thus, the imbalance ratio is 6.2 (n=56 for non-cystine stones
and n=9 for cystine stones). The details of the DECT renal
stone dataset are shown in Table 6.
In this comparison experiment, we are interested in showing
the outperformance of CSG over cSVM. In addition, some
commonly used machine learning algorithms in medical
data classification problems such as SVM,[51] artificial neural network (ANN),[52] C4.5[53] and NaiveBayes (NB)[54] are
also implemented for comparison. The SVM, cSVM and
CSG methods are performed using the same settings as in
section 4.1. The ANN, C4.5 and NB methods are performed
using data mining software Weka 3.6.9.[55] 5-fold stratified
cross validation is applied. In addition to sensitivity, specificity and Gmean, we also use two other important evaluation metrics for the medical diagnosis field: Positive Predictive Value (PPV) and Negative Predictive Value (NPV). PPV
indicates the probability that patients with positive screening tests truly have the disease, while NPV shows the probability that patients with negative screening tests truly don’t
have the disease. The results are shown in Figure 5 and Figure 6.
Figure 5 shows that the standard SVM method performs
poorly on this imbalanced dataset. The zero sensitivity
shows that SVM has no recognition ability of the cystine
stones. cSVM improves the sensitivity very little (11.1%),
and still far less than satisfactory. CSG method has much
better sensitivity than SVM or cSVM (77.8% vs. 0% and
11.1%). ANN has equal sensitivity with C4.5 (44.4%)
but higher specificity (96.4% vs. 92.9%). Compared with
ANN, NB has better sensitivity (66.6%), but lower specificity (83.9%). The CSG method achieves highest sensitivity (77.8%) and Gmean (86.6%) among all six methods while maintaining high specificity (96.4%). The CSG
method also achieves the second highest values in PPV
(77.8%) and the highest value in NPV (96.4%) according
to Figure 6. In conclusion, CSG outperforms the other five
methods in classification of cystine stones.

Table 6: The RenalStone_cys dataset
Dataset

#Examples

#Features

#Positive

#Negative

IR

RenalStone_cys

65

18

9

56

6.2

102

Feature Description
11 energy level measures
1 effective atomic number
6 material density measures
ISSN 1927-6974

E-ISSN 1927-6982

www.sciedu.ca/air

Artificial Intelligence Research

2015, Vol. 4, No. 2

Figure 5: Sensitivity, Specificity and Gmean on RenalStone_cys dataset

Figure 6: PPV and NPV on RenalStone_cys dataset

5

Conclusion and discussion

In this research, we propose a model fusion based approach
integrating cSVM with GMM for the imbalanced classification problem. The CSG method augments cSVM by incorporating the GMM modeling of imbalanced data distribution into the training process and thus leads to better identification of the minority class examples. Experimental results on KEEL benchmark datasets and the medical imaging
dataset show the CSG method to be effective in dealing with
imbalanced classification problems.

References
[1] Duan KB, Keerthi SS. Which is the best Multiclass SVM method?
An Empirical Study. Springer Berlin Heidelberg. 2005: 278-285.
[2] Cortes C, Vapnik V. Support-vector Networks. Machine Learning.
Published by Sciedu Press

We also find that CSG performs even better when the dataset
is preprocessed by SMOTE method. This is because the
synthetic data instances generated by SMOTE creates larger
and less specific decision regions for the cSVM and GMM
models to learn from, thus the decision boundary can be further adjusted towards the majority class and thus lead to better class separation. In all, the SMOTE method can further
improve the CSG method in dealing with imbalance classification problems.

1995; 20(3): 273-297. http://dx.doi.org/10.1023/A:102262
7411411
[3] Bazzani A, Bevilacqua A, Bollini D, et al. An SVM Classifier to
Separate False Signals from Microcalcifications in Digital Mammograms. Physics in Medicine and Biology. 2001; 46(5): 1651.
PMid:11419625

103

www.sciedu.ca/air

Artificial Intelligence Research

[4] Collobert R, Bengio S. Links Between Perceptrons, MLPs and
SVMs. In Proceedings of the twenty-first international conference
on Machine learning, ACM, 2004.
[5] McLachlan G. Discriminant Analysis and Statistical Pattern Recognition. 2004; 544.
[6] Shon T, Kim Y, Lee C, et al. A Machine Learning Framework for
Network Anomaly Detection using SVM and GA. In Information
Assurance Workshop, 2005. IAW’05. Proceedings from the Sixth
Annual IEEE SMC, 2005.
[7] Scarfone K, Mell P. Guide to Intrusion Detection and Prevention
Systems (IDPS). NIST Special Publication. 2007; 800: 94.
[8] Veropoulos K, Campbell C, Cristianini N. Controlling the Sensitivity of Support Vector Machines. Proceedings of the International
Joint Conference on Artificial Intelligence, 1999.
[9] Wu G, Chang EY. Adaptive Feature-space Conformal Transformation for Imbalanced-data Learning. MACHINE LEARNINGINTERNATIONAL WORKSHOP THEN CONFERENCE, 2002.
[10] He H, Garcia EA. Learning from Imbalanced Data. Knowledge and
Data Engineering. IEEE Transactions. 2009; 21(9): 1263-84.
[11] Wu G, Chang EY. Aligning Boundary in Kernel Space for Learning
Imbalanced Dataset. Data Mining, ICDM’04. Fourth IEEE International Conference on. IEEE, 2004.
[12] Chawla NV, Bowyer KW, Hall LO, et al. SMOTE: Synthetic Minority Over-sampling Technique. Journal of Artificial Intelligence
Research. 2002; 16: 321-357.
[13] Akbani R, Kwek S, Japkowicz N. Applying Support Vector Machines to Imbalanced Datasets. Machine Learning: ECML 2004,
Berlin Heidelberg, 2004.
[14] Wu G, Chang EY. Class-boundary Alignment for Imbalanced
Dataset Learning, in ICML 2003 Workshop on Learning from Imbalanced Data Sets II, Washington, DC, 2003.
[15] Chawla NV, Japkowicz N, Kotcz A. Editorial: Special Issue on
Learning from Imbalanced Data Sets. ACM SIGKDD Explorations
Newsletter. 2004; 6(1): 1-6.
[16] Masnadi-Shirazi H, Vasconcelos N, Iranmehr A. Cost-Sensitive
Support Vector Machines. arXiv preprint arXiv:1212.0975, 2012.
[17] Maloof MA. Learning when Data Sets are Imbalanced and when
Costs are Unequal and Unknown. In ICML-2003 Workshop on
Learning from Imbalanced Data Sets II, 2003.
[18] Cao P, Zhao D, Zaiane O. An Optimized Cost-Sensitive SVM for
Imbalanced Data Learning. In Advances in Knowledge Discovery
and Data Mining, 2013. p.280-292.
[19] Brefeld U, Geibel P, Wysotzki F. Support Vector Machines with Example Dependent Costs. In Machine Learning: ECML 2003, 2003.
[20] Jordan A. On Discriminative vs. Generative Classifiers: A Comparison of Logistic Regression and Naive Bayes. Advances in Neural
Information Processing Systems. 2002; 14: 841.
[21] Bishop CM, Nasrabadi NM. Pattern Recognition and Machine
Learning, New York: springer, 2006.
[22] Chawla NV. Data Mining for Imbalanced Datasets: An Overview. In
Data Mining and Knowledge Discovery Handbook, Springer, 2005.
p.853-867.
[23] Batista GE, Prati RC, Monard MC. A Study of the Behavior of
Several Methods for Balancing Machine Learning Training Data.
ACM SIGKDD Explorations Newsletter. 2004; 6(1): 20-29. http:
//dx.doi.org/10.1145/1007730.1007735
[24] Holte RC, Acker L, Porter BW. Concept Learning and the Problem
of Small Disjuncts. Proceedings of the Eleventh International Joint
Conference on Artificial Intelligence, 1989.
[25] Estabrooks A, Jo T, Japkowicz N. A Multiple Resampling Method
for Learning from Imbalanced Data Sets. Computational Intelligence. 2004; 20(1): 18-36. http://dx.doi.org/10.1111/j.0
824-7935.2004.t01-1-00228.x
[26] Karakoulas G, Shawe-Taylor J. Optimizing Classifiers for Imbalanced Training Sets. In Proceedings of the 1998 Conference on Advances in Neural Information Processing Systems II. 1999: 253-259.
[27] Bishop CM. Neural Networks for Pattern Recognition, Oxford university press, 1995.

104

2015, Vol. 4, No. 2

[28] Platt J. Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods. Advances in Large
Margin Classifiers, the MIT Press. 2000: 61-74.
[29] Kim D, Lee SC. Pairwise Threshold for Gaussian Mixture Classification and its Application on Human Tracking Enhancemen. Advanced Video and Signal-Based Surveillance (AVSS) 2012 IEEE
Ninth International Conference on, 2012.
[30] Wang K, Ren Z. Enhanced Gaussian Mixture Models for Object
Recognition using Salient Image Features. Mechatronics and Automation, 2007. ICMA 2007. International Conference on, 2007.
[31] Reynolds DA, Rose RC. Robust Text-independent Speaker Identification using Gaussian Mixture Speaker Models. Speech and Audio
Processing, IEEE Transactions. 1995; 3(1): 72-83.
[32] Fauve BG, Evans NM, Pearson N, et al. Influence of Task Duration in Text-independent Speaker Verification. In Proc. Interspeech,
2007.
[33] Dempster AP, Laird NM, Rubin DB. Maximum Likelihood from
Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society. Series B (Methodological). 1977: 1-38.
[34] Alcalá J, Fernández A, Luengo J, et al. KEEL Data-Mining Software Tool: Data Set Repository, Integration of Algorithms and Experimental Analysis Framework. Journal of Multiple-Valued Logic
and Soft Computing. 2011; 17(2-3): 255-287.
[35] Kubat M, Holte R, Matwin S. Learning when Negative Examples
Abound. Machine Learning: ECML-97, 1997.
[36] Wang HY. Combination Approach of SMOTE and Biased-SVM
for Imbalanced Datasets. In Neural Networks, 2008. IJCNN
2008.(IEEE World Congress on Computational Intelligence). IEEE
International Joint Conference on IEEE, 2008.
[37] Imam T, Ting KM, Kamruzzaman J. z-SVM: An SVM for Improved
Classification of Imbalanced Data. In AI 2006: Advances in Artificial Intelligence, Berlin Heidelberg, 2006.
[38] Maciejewski T, Stefanowski J. Local Neighbourhood Extension of
SMOTE for Mining Imbalanced Data. In Computational Intelligence and Data Mining (CIDM), 2011 IEEE Symposium on, 2011.
[39] Hui H, Wang WY, Mao BH. Borderline-SMOTE: A new Oversampling Method in Imbalanced Data Sets Learning. In Advances
in Intelligent Computing, Berlin Heidelberg, 2005.
[40] Chang CC, Lin CJ. LIBSVM: A Library for Support Vector Machines. ACM Transac-tions on Intelligent Systems and Technology.
2011; 2(27): 1-27.
[41] NKUDIC. Kidney Stones in Adults, 2013. Available from:
http://kidney.niddk.nih.gov/kudiseases/pubs/stone
sadults/?control=Pubs. [Accessed 31 10 2013].
[42] Scales CD, Smith AC, Hanley JM, et al. Prevalence of kidney stones
in the United States. European urology. 2012; 62(1): 160-165.
PMid:22498635.
[43] Eliahou R, Hidas G, Duvdevani M, et al. Determination of renal stone composition with dual-energy computed tomography: an
emerging application. Seminars in Ultrasound, CT, and MRI. 2010;
31(4): 315-320.
[44] Hidas G, Eliahou R, Duvdevani M, et al. Determination of renal stone composition with dual-energy CT: in vivo analysis and
comparison with x-ray diffraction. Radiology. 2010; 257(2): 394401. PMid:20807846. http://dx.doi.org/10.1148/radiol.
10100249
[45] Abdel-Halim RE, Abdel-Halim MR. A review of urinary stone
analysis techniques. Saudi medical journal. 2006; 27(10): 1462.
PMid:17013464.
[46] GOEL R, WASSERSTEIN AG. Kidney Stones: Diagnostic and
Treatment Strategies. Consultant. 2012; 52: 121-130.
[47] Riedel M. An Introduction to Dual Energy Computed Tomography [Internet]. Available from:
http:
//ric.uthscsa.edu/personalpages/lancaster/DI2_
Projects_2010/dual-energy_CT.pdf. [Accessed 1 11 2013].
[48] Graser A, Johnson TR, Bader M, et al. Dual energy CT characterization of urinary calculi: initial in vitro and clinical experience.
Investigative radiology. 2008; 43(2): 112-119. PMid:18197063.
[49] Wu J. Chapter 58 – Urolithiasis. Integrative Medicine, 3rd ed, WB
Saunders Company, 2012.
ISSN 1927-6974

E-ISSN 1927-6982

www.sciedu.ca/air

Artificial Intelligence Research

[50] Breuning MH, Hamdy NA. From gene to disease; SLC3A1,
SLC7A9 and cystinuria. Nederlands tijdschrift voor geneeskunde.
2003; 147(6): 245. PMid:12621979.
[51] Dal Moro F, Abate A, Lanckriet GRG, et al. A Novel Approach
for Accurate Prediction of Spontaneous Passage of Ureteral Stones:
Support Vector Machines. Kidney International. 2006; 69(1): 157160. PMid:16374437. http://dx.doi.org/10.1038/sj.ki.5
000010
[52] Chiang D, Chiang H, Chen W, et al. Prediction of Stone Disease
by Discriminant Analysis and Artificial Neural Networks in Genetic

Published by Sciedu Press

2015, Vol. 4, No. 2

Polymorphisms: a New Method. BJU International. 2003; 7: 661666.
[53] Kaladhar D, Krishna AR, Varahalarao V. Statistical and Data Mining Aspects on Kidney Stones: A Systematic Review and Metaanalysis. 2012; 1: 543. http://dx.doi.org/10.4172/scienti
ficreports
[54] Lavanya D, Rani K. Performance Evaluation of Decision Tree Classifiers on Medical Datasets. International Journal of Computer Applications. 2011; 26(4): 1-4.
[55] Hall M, Frank E, Holmes G, et al. The WEKA Data Mining Software: An Update. SIGKDD Explorations. 2009; 11(1).

105

Mining Brain Region Connectivity for Alzheimer’s Disease
Study via Sparse Inverse Covariance Estimation

1

Liang Sun1,2 , Rinkal Patel2 , Jun Liu1 , Kewei Chen4 ,
Teresa Wu3 , Jing Li3 , Eric Reiman4 , and Jieping Ye1,2
Center for Evolutionary Functional Genomics, The Biodesign Institute, Arizona State University, Tempe, AZ 85287
2
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287
3
Department of Industrial Engineering, Arizona State University, Tempe, AZ 85287
4
Banner Alzheimer’s Institute and Banner PET Center, Banner Good Samaritan Medical Center,
Phoenix, AZ 85006

ABSTRACT

1. INTRODUCTION

Eﬀective diagnosis of Alzheimer’s disease (AD), the most
common type of dementia in elderly patients, is of primary
importance in biomedical research. Recent studies have
demonstrated that AD is closely related to the structure
change of the brain network, i.e., the connectivity among
diﬀerent brain regions. The connectivity patterns will provide useful imaging-based biomarkers to distinguish Normal
Controls (NC), patients with Mild Cognitive Impairment
(MCI), and patients with AD. In this paper, we investigate the sparse inverse covariance estimation technique for
identifying the connectivity among diﬀerent brain regions.
In particular, a novel algorithm based on the block coordinate descent approach is proposed for the direct estimation
of the inverse covariance matrix. One appealing feature of
the proposed algorithm is that it allows the user feedback
(e.g., prior domain knowledge) to be incorporated into the
estimation process, while the connectivity patterns can be
discovered automatically. We apply the proposed algorithm
to a collection of FDG-PET images from 232 NC, MCI, and
AD subjects. Our experimental results demonstrate that
the proposed algorithm is promising in revealing the brain
region connectivity diﬀerences among these groups.

Alzheimer’s Disease (AD) is a progressively neurodegenerative disease. It is the most common type of dementia in
elderly patients. Currently, approximately 5 million people
(about 10% of the population over 60) in the U.S. are afﬂicted by AD. The estimated direct cost to care the patients
is over $100 billion per year. As the population ages over
the next several decades, the AD cases and the associated
costs are expected to go up dramatically. AD researchers
have thus intensiﬁed their eﬀorts to investigate ways to delay, cure, or prevent the onset and progression of AD.
Objective and quantitative criteria, so called, biomarkers,
are essential to evaluate the eﬀectiveness of a potential treatment or prevention strategy. Although clinical assessment
and neuropsychological tests provide valuable information
for AD diagnosis [19, 11], recent studies have demonstrated
that imaging parameters from brain scans are more sensitive
and consistent measures of disease progression than cognitive assessment [20]. Some studies have shown that imaging
measures correlate with cognitive test performance in Mild
Cognitive Impairment (MCI) and AD – an initial step in
the validation of markers that accurately predict the course
of the disease. Thus, the neuroimaging research oﬀers great
potential to identify the sensitive and speciﬁc biomarkers
that can distinguish between diﬀerent types of subjects and
open up opportunities to implement treatments in the early
stages of disease when intervention may be most beneﬁcial.
There are two commonly used neuroimaging techniques
for AD study: [18F]-2-ﬂuoro-2-deoxy-D-glucose positron emission tomography (FDG-PET) and volumetric Magnetic
Resonance Imaging (MRI). FDG-PET is a functional imaging technique that measures the cerebral metabolic rate for
glucose. MRI is a high-resolution structural imaging technique that allows for the visualization of brain anatomy with
high degree of contrast between the brain tissues. There
imaging techniques have been shown to be eﬀective for AD
study [1, 30].
Recent studies have demonstrated that AD is closely related to the alternations of the brain network, i.e., the connectivity among diﬀerent brain regions [7, 23, 24]. It has
been shown that the brain regions are moderately or less
inter-connected for AD patients, and cognitive decline in
AD patients is associated with disrupted functional connectivity in the brain [24]. The connectivity patterns may be
useful as imaging-based biomarkers to distinguish Normal
Controls, MCI and AD patients. It is thus important to

Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications Data Mining; J.3 [Life and Medical Sciences]: Health,
Medical information systems

General Terms
Algorithm

Keywords
Brain network, Alzheimer’s disease, neuroimaging, FDGPET, sparse inverse covariance estimation

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD’09, June 28–July 1, 2009, Paris, France.
Copyright 2009 ACM 978-1-60558-495-9/09/06 ...$5.00.

1335

develop computational tools for identifying the connectivity
among diﬀerent brain regions.
In this paper, we study the Sparse Inverse Covariance
Estimation (SICE) for the identiﬁcation of brain region connectivity. SICE originates from the more broad problem of
covariance matrix estimation from data [3]. It has been observed that the covariance matrix can be estimated robustly
when enough entries of the inverse covariance matrix are set
to zero [8]. On the other hand, it has been demonstrated
in the literature that many inverse covariance matrices are
sparse, such as genetic interaction networks [13, 25]. Furthermore, the sparse inverse covariance can be interpreted
from the perspective of the undirected graphical model [16]
which models and explains the relationship among a set of
variables. In practice, it is usually reasonable to assume
that the patterns of some variables can be predicted by a
small subset of all variables, which leads to the sparsity
of the inverse covariance matrix in the multivariate Gaussian distribution. These observations in theory and practice
motivate the wide use of the sparse inverse covariance estimation. It has been shown to be useful in various applications, including evaluating patterns of association among
variables [9], exploration of genetic networks [17], senator
voting records analysis [2], hyperspectral image classiﬁcation [4], and speech recognition [5].
In this paper, we propose a novel algorithm for SICE. Unlike most of the existing algorithms [2, 6, 12, 15, 17, 18], the
proposed SICE algorithm estimates the inverse covariance
matrix directly. One appealing feature of the proposed algorithm is that it allows the user feedback (e.g., prior domain
knowledge) to be incorporated into the estimation process
by imposing additional constraints in the optimization formulation, while discovering the connectivity patterns automatically. We apply the proposed algorithm to a collection
of FDG-PET images from 232 NC, AD, and MCI subjects
enrolled in the Alzheimer’s Disease Neuroimaging Initiative
(ADNI)1 . The experimental results reveal several interesting
connectivity patterns among diﬀerent brain regions. Our results also demonstrate the beneﬁt of using the user feedback
for the understanding of the brain network.
The rest of the paper is organized as follows. Section 2
provides the background on AD and two neuroimaging techniques including MRI and FDG-PET. In section 3, we review the sparse inverse covariance estimation. The proposed
algorithm is presented in section 4. Section 5 presents the
experimental results. Section 6 concludes the paper and discusses some future work.

2.

Subjects
AD-Male
AD-Female
MCI-Male
MCI-Female
NC-Male
NC-Female

Table 1: Test Data Set.
No. Mean-Age Std deviation-Age
27
76.74
6.75
22
75.59
6.11
76
75.84
6.12
40
75.85
6.08
43
76.74
6.26
24
75.33
5.82

Although clinical assessment and neuropsychological tests
provide valuable information for AD diagnosis, recent studies have demonstrated that imaging parameters from brain
scans are more sensitive and consistent measures of disease
progression than cognitive assessment [20].
Medical Imaging techniques like MRI and FDG-PET have
been used widely in understanding the human brain. Volumetric MRI is a high-resolution structural imaging technique
that allows for the visualization of brain anatomy with high
degree of contrast between the brain tissues. T1-weighting of
the MRI procedure is now a routine for visualizing and investigating brain gray, white and cerebrospinal ﬂuid (CSF) tissue. With this procedure, the contrast between these diﬀerent brain tissue types are high and segmentation of the MRI
data to diﬀerent tissue types becomes feasible. The structural information is used to study the volumetric changes in
the brain related AD, which causes signiﬁcant shrinkages in
the gray matter, regionally and globally.
PET is a functional imaging technique that captures the
metabolic activity of various brain regions. The PET scanner detects pairs of gamma rays emitted by a positronemitting radionuclide (tracer), which is introduced into the
body on a biologically active molecule. If the biologically
active molecule chosen for PET is FDG, an radioactive analogue of glucose, the concentrations of tracer imaged then
give tissue metabolic activity, in terms of regional glucose
uptake.

2.2 Data Acquisition and Preprocessing
FDG-PET 3D images used in this study were from 49
AD’s, 67 NC’s, 116 MCI’s downloaded from ADNI. All the
FDG-PET images available from ADNI undergo various processing like co-registration to a common coordinate system,
averaging, standardization of image and voxel size and uniformity in resolution. Demographic information of the subjects is shown in Table 1. Using SPM52 , the PET images are
normalized to the standard Montreal Neurological Institute
(MNI) template space, and the normalization quality was
visually inspected.

BACKGROUND

2.1 Alzheimer’s Disease and Neuroimaging

2.3 Regional Averages using AAL

Alzheimer’s disease is an irreversible, progressive brain
disease which slowly destroys memory and thinking skills,
eventually even the ability to carry out simple daily tasks [14].
Most AD diagnosis is based on clinical and psychometric
assessment. [19] summarizes the clinical criteria for clinically probable AD, which include insidious onset and progressive impairment of memory and other cognitive functions. Mini Mental State Examination (MMSE) is one of
a number of cognitive tests carried out to clinical patients
for disease diagnosis (MCI, AD, and NC classiﬁcation) [11].

Our current research is based on the regions of interest
(ROI) from the PET data. The whole brain volume is divided into 116 anatomical volumes of interest (AVOI), deﬁned by Automated Anatomical Labeling (AAL) [28]. Then
the average of the voxel intensities over each AVOI for each
subject is extracted and used for our analysis. Neuroimaging AD researchers have identiﬁed a number of AD aﬀected
brain regions, such as hippocampus, that are a sub-set of the
116 AAL deﬁned ROIs. The domain expertise is useful for

1

2

http://www.loni.ucla.edu/ADNI/

1336

http://www.ﬁl.ion.ucl.ac.uk/spm/

Table 2: 116 brain regions deﬁned by AAL. The regions highlighted are those used in our experiments.
Sr.

Name

Sr.

Name

Sr.

Name

Sr.

Name

1

Precentral L

2

Precentral R

30

Insula R

31

Cingulum Ant L

59

Parietal Sup L

88

Tem Pole Mid R

60

Parietal Sup R

89

Temporal Inf L

3
4

Frontal Sup L

32

Cingulum Ant R

Frontal Sup R

33

Cingulum Mid L

61

Parietal Inf L

90

Temporal Inf R

62

Parietal Inf R

91

Cerebelm Crus1 L

5

Frontal Sup Orb L

34

Cingulum Mid R

63

SupraMarginal L

92

Cerebelm Crus1 R

6

Frontal Sup Orb R

35

7

Frontal Mid L

36

Cingulum Post L

64

SupraMarginal R

93

Cerebelm Crus2 L

Cingulum Post R

65

Angular L

94

Cerebelm Crus2 R

8

Frontal Mid R

37

Hippocampus L

9

Frontal Mid Orb L

38

Hippocampus R

66

Angular R

95

Cerebelum 3 L

67

Precuneus L

96

Cerebelum 3 R

10

Frontal Mid Orb R

39

ParaHippo L

11

Frontal Inf Oper L

40

ParaHippo R

68

Precuneus R

97

Cerebelum 4 5 L

69

Paracentral Lob L

98

Cerebelum 4 5 R

12

Frontal Inf Oper R

41

13

Frontal Inf Tri L

42

Amygdala L

70

Paracentral Lob R

99

Cerebelum 6 L

Amygdala R

71

Caudate L

100

Cerebelum 6 R

14

Frontal Inf Tri R

43

Calcarine L

15

Frontal Inf Orb L

44

Calcarine R

72

Caudate R

101

Cerebelum 7b L

73

Putamen L

102

Cerebelum 7b R

16

Frontal Inf Orb R

45

17

Rolandic Oper L

46

Cuneus L

74

Putamen R

103

Cerebelum 8 L

Cuneus R

75

Pallidum L

104

Cerebelum 8 R

18

Rolandic Oper R

47

Lingual L

76

Pallidum R

105

Cerebelum 9 L

19

Supp Motor L

48

Lingual R

77

Thalamus L

106

Cerebelum 9 R

20

Supp Motor R

49

Occipital Sup L

78

Thalamus R

107

Cerebelum 10 L

21

Olfactory L

50

Occipital Sup R

79

Heschl L

108

Cerebelum 10 R

22

Olfactory R

51

Occipital Mid L

80

Heschl R

109

Vermis 1 2

23

Frontl Sup Medl L

52

Occipital Mid R

81

Temporal Sup L

110

Vermis 3

24

Frontl Sup Med R

53

Occipital Inf L

82

Temporal Sup R

111

Vermis 4 5

25

Frontal Mid Orb L

54

Occipital Inf R

83

Templ Pole Sup L

112

Vermis 6

26

Frontal Mid Orb R

55

Fusiform L

84

Templ Pole Sup R

113

Vermis 7

27

Rectus L

56

Fusiform R

85

Temporal Mid L

114

Vermis 8

28

Rectus R

57

Postcentral L

86

Temporal Mid R

115

Vermis 9

29

Insula L

58

Postcentral R

87

Templ Pole Mid L

116

Vermis 10

understanding the results of future data analysis. Table 2
gives the names of the 116 AVOIs.

3.

It is straightforward to derive that the maximum log likelihood of inverse covariance matrix under a multivariate
Gaussian model. Formally, the maximum likelihood estimate of Θ = Σ−1 can be obtained by maximizing

SPARSE INVERSE COVARIANCE ESTIMATION

max f = log detΘ − tr (SΘ) ,
Θ0

In this section, we brieﬂy introduce the sparse inverse covariance estimation as well as some related work with the
emphasis on numerical algorithms.
Suppose we have n samples independently drawn from a
multivariate Gaussian distribution, and these samples are
denoted as x1 , · · · , xn ∼ N (μ, Σ), where xi is a p-dimension
vector, μ ∈ IRp is the mean, and Σ ∈ IRp×p is the covariance
to be estimated. Let Θ = Σ−1 be the inverse covariance (or
precision) matrix. The empirical covariance is denoted as S:
S=

(1)

where tr (SΘ) is the trace of SΘ. Assume that S is nonsingular. Computing the derivative of f w.r.t. Θ and setting
it to zero, we get
Θ−1 − S = 0.
Thus, the maximum likelihood estimate of the inverse covariance Θ is Θ = S −1 . If the dimensionality is larger than
the sample size, i.e., p > n, some types of regularization are
necessary in order to estimate Θ since S is singular. The
connection between the estimation of Θ and S in Eq. (1)
suggests the possibility that we can obtain shrunken estimates through maximization of the penalized log likelihood

n
1X
(xi − μ)(xi − μ)T .
n i=1

1337

function. Formally, we estimate Θ = Σ−1 by maximizing
the following objective function:
log detΘ − tr (SΘ) − λJ(Θ),

is considered, and two eﬃcient algorithms are proposed: one
is based on Nesterov’s ﬁrst-order algorithm [22] which yields
a rigorous complexity estimate; the second one uses the
block coordinate decent approach to update rows/columns
of the covariance matrix sequentially. The block coordinate
decent algorithm is further improved in [12].

(2)

where J(Θ) is a penalty function. In particular, in this paper
we consider the following formulation of J(Θ):
J(Θ) = vec(Θ)1 ,

(3)

4. THE PROPOSED ALGORITHM

where vec(Θ) is the vector form of matrix Θ. In other words,
vec(Θ)1 denotes the sum of the absolute values of all elements of the positive deﬁnite matrix Θ.
It is well-known that model sparsity can often be achieved
by applying the 1 -norm regularization [10, 26]. This has
been introduced into the least squares formulation and the
resulting model is called lasso [26]. Similarly, the formulation in Eq. (2) with J(Θ) deﬁned in Eq. (3) produces a
sparse estimate for Θ. Speciﬁcally, if the ijth component of
the inverse covariance matrix Θ is zero, then variables i and
j are conditionally independent, given the other variables in
the multivariate Gaussian distribution. Thus, it makes sense
to impose the 1 penalty to the estimation of Θ to increase
its sparsity as in lasso [26].

In this section, we present the proposed algorithm for solving the following optimization problem:
max
Θ0

3.1 Related Work
A number of papers in the literature are devoted to the
estimation of covariance matrices. It is observed that the
covariance matrix can be estimated robustly when enough
entries of the inverse covariance matrix are set to zero [8].
Meanwhile, it has been demonstrated in the literature that
many biomedical and genetic networks are not fully connected and many genetic interaction networks contain many
genes with few interactions and a few genes with many interactions [25, 13]. Therefore, many biomedical and genetic
networks are intrinsically sparse and the corresponding inverse covariance matrix is sparse. These observations in theory and practice motivate the development of eﬃcient and
eﬀective sparse inverse covariance estimates.
In Gaussian graphical model [16], it assumes that the multivariate vector follows a multivariate normal distribution
with a particular structure of the inverse covariance structure, a.k.a. precision or concentration matrix. It usually assumes that the patterns of some variables can be predicted
by a small subset of all variables. This assumption leads to
sparsity in the precision matrix of the multivariate distribution, and results in the so-called neighborhood selection or
covariance selection problem [8]. In other words, zeros in
the inverse covariance matrix correspond to conditional independence properties among the variables. In this setting,
a sparse inverse covariance matrix, if it ﬁts the data well, is
very useful to practitioners, as it simpliﬁes the understanding, and provides the insights into the data.
To determine the zero patterns in the inverse covariance
matrix, the traditional method is based on the greedy forwardbackward search algorithm [16]. Recently, some algorithms
are proposed to solve sparse inverse covariance matrix. A
gradient descent algorithm is proposed in [17] in which the
sparse inverse covariance is computed by deﬁning a loss function that is the negative of the log likelihood function. A penalized maximum likelihood estimation is considered in [15].
In [6], a set of large-scale methods are proposed to solve
problems where a sparse structure of inverse covariance is
known a prior. The interior point method for the “maxdet”
problem is proposed [18]. In [2], the 1 -norm regularization

1338

log detΘ − tr(SΘ) − λvec(Θ)1 ,

(4)

where S is the empirical covariance matrix, and λ is the
regularization parameter.
One drawback of many existing algorithms [12, 2] is that
they directly estimate the covariance matrix. Since the goal
is to estimate the sparsity structure of the inverse covariance
matrix, a better approach is to compute the inverse covariance matrix directly. This also facilitates the incorporation
of the user feedback (e.g., prior domain knowledge) into the
formulation. The proposed approach estimates the inverse
covariance matrix Θ directly, and it also follows the framework of the block coordinate descent [12, 2]. Speciﬁcally, we
partition S and Θ in the form of block matrix:
»
–
»
–
S11 s12
Θ11 θ12
S= T
,Θ =
,
T
s12 s22
θ12
θ22
where S11 , Θ11 ∈ IR(p−1)×(p−1) , s12 , θ12 ∈ IRp−1 , and s22 , θ22 ∈
IR. Then we can reformulate log det Θ as follows:
“
”
T
log det Θ = log det Θ11 (θ22 − θ12
Θ−1
11 θ12 )
”
“
T
= log det Θ11 + log θ22 − θ12
Θ−1
11 θ12 .
Thus, the problem in Eq. (4) can be formulated as:
“
”
T
Θ−1
max f = log det Θ11 + log θ22 − θ12
11 θ12
Θ0

−tr(SΘ) − λvec(Θ))1 .

(5)

In the block coordinate descent approach, we update each
row/column while ﬁxing other elements of matrix Θ in each
iteration. To compute the optimal Θ, we use (S + λI)−1 as
the initial guess of Θ, then update each row/column of Θ
repeatedly until convergence.
In the following, we assume that Θ11 is ﬁxed and we need
to update θ12 and θ22 . We can apply similar techniques to
update other rows/columns. The subdiﬀerential of f w.r.t.
θ12 can be computed as follows:
∂f = −

2
θ22 −

T
θ12
Θ−1
11 θ12

Θ−1
11 θ12 − 2s12 − 2λSGN(θ12 ), (6)

where SGN(t) is a set-valued mapping for t ∈ IR, and it is
deﬁned as:
8
>
< {1} t > 0,
SGN(t) := [−1, 1] t = 0,
(7)
>
:
{−1} t < 0.
In fact, SGN(t) is the subdiﬀerential of |t|. For the vector
v ∈ IRp , SGN(v) is deﬁned component-wise, i.e., the ith
component of SGN(v) is deﬁned as SGN(vi ) where vi is the
ith component of v. Clearly, SGN(v) is the subdiﬀerential
of v1 .

We also compute the subgradient of f with respect to θ22 .
Note that θ22 > 0. We have
∂f

=

⇔

1
− s22 − λ = 0
T
θ22 − θ12
Θ−1
11 θ12
1
T
> 0.
Θ−1
θ22 − θ12
11 θ12 =
s22 + λ

Algorithm 1 Sparse Inverse Covariance Estimation
Input: empirical covariance S, parameter λ
Initialize Θ(0) := (S + λI)−1
repeat
for j = 1 to p do
Let Θ(j−1) denote the current iterate. Solve the minmax problem:
„
«
1
max min h(α, κ) = − κT Θ11 κ + κT α −sT12 α+λα1 .
κ
α
2

(8)

The inequality in Eq. (8) guarantees the positive deﬁniteness
of Θ. Let
θ12
α=−
= −(s22 + λ)θ12 .
T
θ22 − θ12
Θ−1
11 θ12

Compute ŷ:
ŷ = −

It is clear that SGN(α) = −SGN(θ12 ). Then we can represent the subdiﬀerential of Eq. (6) as:

Compute θ̂jj :

∂f = 2Θ−1
11 α − 2s12 + 2λSGN(α).

θ̂jj =

It is clear that ∂f is (a constant multiple of) the subdiﬀerential of the following optimization problem:
1
T
min g = αT Θ−1
11 α − s12 α + λα1 .
α
2

(9)

proposition is true in the basis step. Next suppose that
Θ(i)  0, we consider Θ(i+1) after one iteration. It follows
from Eq. (8) that at each step we have
T
Θ−1
θ22 − θ12
11 θ12 =

(11)

=
=
=
=

1
>0
s22 + λ

Thus, we have

“
”
(i)
(i+1)
(i+1) T (i) −1 (i+1)
det Θ(i+1) = det Θ11 θ22 − θ12
Θ11 θ12
> 0.

Thus, κ can be eliminated and the resulting problem is
equivalent to Eq. (9). In practice, we use the prox method [21]
to solve the min-max problem.
When α is solved, we can recover θ12 = − s221+λ α. Next
we show how to derive θ22 . From Eq. (8), we have
θ22

”
“
1
T
κ+1
−θ12
s22 + λ

Update rule: Θ(j) is Θ(j−1) with column/row replaced by [ŷ; θ̂jj ].
end for
until converge

Thus, the problem in Eq. (4) is equivalent to the one in
Eq. (9) if Θ11 is ﬁxed. Note that the problem in Eq. (9) is
equivalent to the following min-max problem:
„
«
1
max min h(α, κ) = − κT Θ11 κ + κT α − sT12 α + λα1 .
κ
α
2
(10)
The equivalence relationship can be veriﬁed easily by computing the derivative of h(α, κ) w.r.t. κ:
−Θ11 κ + α = 0 ⇔ κ = Θ−1
11 α.

α
.
s22 + λ

(i)

Note that Θ11 is positive deﬁnite since Θ(i)  0, thus Θ(i+1)
is also positive deﬁnite after this iteration.
We check the convergence of Algorithm 1 by comparing
the Θ matrices between two consecutive iterations. More
speciﬁcally, let Θnew and Θold be the solutions at the current
and previous iterations, respectively. Then, the algorithm
stops if ||Θnew − Θold ||F   holds for a certain threshold .
We set  = 10−4 in our experiments.
Compared with the algorithms discussed in the last section, the proposed Algorithm 1 directly estimates the inverse
covariance matrix Θ. Thus, it facilitates the incorporation of
the user feedback (e.g., prior domain knowledge) by imposing constraints to guide the optimization problem. For example, we can set θij = 0 if the ith region and the jth region
are known to be disconnected. Note that α = −(s22 + λ)θ12
in each iteration of the block coordinate descent approach,
thus θij = 0 implies that the corresponding entry in α is 0.
Hence, we can enforce the corresponding entries in α to be 0
in each iteration. For a given Θ11 , the optimization problem
in Eq. (9) can be reformulated as follows:

1
T
θ12
Θ−1
11 θ12 +
s22 + λ
«
„
1
α
T
+
−
Θ−1
θ12
11
s22 + λ
s22 + λ
1
1
T
θ12
κ+
−
s22 + λ
s22 + λ
”
“
1
T
κ+1 .
−θ12
s22 + λ

The outline of the proposed algorithm is given in Algorithm 1. The global convergence of Algorithm 1 is guaranteed due to the separability of the non-smooth 1 penalty
term [27]. We have following property:
Theorem 1. The produced Θ(j) at the j-th iteration is
strictly positive definite, i.e., for 1 ≤ j ≤ p, Θ(j)  0. Furthermore, the updating of θ12 and θ22 at each iteration permits a unique solution.
Proof. Note that if Θ11 is positive deﬁnite, the problem
in Eq. (9) is strictly convex, then the unique solution of α
is guaranteed. Hence, we can obtain unique solutions of θ12
and θ22 . Therefore, it suﬃces to show that Θ(i) is positive
deﬁnite at each step in Algorithm 1.
We use mathematical induction to prove the positive definiteness of Θ(i) , 1 ≤ i ≤ p. Note that Θ(0)  0, thus the

minα
s.t.

T
g = αT Θ−1
11 α − s12 α + λα1
αi = 0 if i ∈ V,

(12)
(13)

where V is the set of indices (based on the user feedback)
corresponding to zero entries in α. Note that this problem is
also strictly convex and can be solved eﬃciently. Similarly,
we can recover θ12 and θ22 from α.

1339

5.

EXPERIMENTS

Temporal Mid R, the link between Temporal Pole Mid L
and Temporal Pole Sup R. We then run the algorithm under the above constraints. The results are summarized in
Figure 5. It can be observed that the connectivity in the subgraph including the occipital and parietal regions remains
almost the same. The experimental results show that the
perturbation tends to aﬀect the local topology of the generated network only. We performed several other similar
studies and observed a similar trend.
We compute the empirical correlation between twelve occipital and parietal regions (form a subgraph) of AD patients. The results are summarized in Table 3. The entries
corresponding to the connections in the network generated
by the proposed algorithm are highlighted in bold type and
red color. We can observe from the table that a large correlation usually implies connectivity in the graph. However,
it is not always the case, e.g., the (2, 6)th entry. This may
be due to the fact that the correlation captures the pairwise information only, while the sparse inverse covariance
estimation captures the interaction among all regions.

We have performed experiments on the PET-AAL data
which is acquired and preprocessed using the methods discussed in Section 2. We applied the proposed algorithm to
three diﬀerent groups, including Normal Control (NC), Mild
Cognition Impairment (MCI) and Alzheimer’s Disease (AD)
patients. We set λ = 1.1 in all experiments. The regions
considered in this study are the ones marked in bold type in
Table 2. These regions are identiﬁed as being signiﬁcantly
impacted by AD. The names of regions with “L” stand for
regions in the left hemisphere of the brain, and those with
“R” stand for regions in the right hemisphere.
The resulting connectivity among diﬀerent brain regions
for AD, MCI, and NC are shown in Figures 1, 2, and 3,
respectively. The resulting sparse inverse covariance matrix
is represented using a graph, where nodes correspond to the
regions of the brain, and the edges connecting the nodes deﬁne the conditional partial correlation between the nodes.
Speciﬁcally, if Θ(i, j) = 0, then regions i and j are conditionally independent, and there is no edge between them.
From these ﬁgures, we observe several interesting patterns
in the brain network for each of the 3 diagnostic groups.
Comparing Figures 1, 2, and 3, we can observe that the
Occipital regions and the Temporal regions in MCI’s are
not as strongly connected as in NC’s. As for the AD patients, we can observe from Figure 1 that these two types
of regions are completely set apart. It can also be observed that hippocampus and parahippocampal gyrus interact with other regions in NC’s, while they become isolated in
AD’s and MCI’s. The connectivity among the these four regions including Hippocampus L, Hippocampus R, ParaHippocampal L, and ParaHippocampal R also becomes weaker
in AD’s and MCI’s. The hippocampus belongs to the limbic system and plays major roles in short-term memory and
spatial navigation. In AD, the hippocampus is one of the
ﬁrst regions of the brain to suﬀer damage, which results in
memory problems among the ﬁrst symptoms. Our experimental results clearly demonstrate that AD patients show
a decreased level of functional connectivity within this network, which is consistent of the ﬁndings in AD study [29].
We can observe from Figure 2 that the edge between Occipital Mid R and Temporal Mid R is the only connection
between the Occipital and Temporal regions, while these two
regions are completely separated in AD as shown in Figure 1.
To test the signiﬁcance of the connectivity between these
two types of regions, we run the algorithm under the constraint that Occipital Mid R and Temporal Mid R are isolated. The results are summarized in Figure 4. It is shown
that the Occipital and Temporal regions are still connected
with a link between Occipital Inf R and Temporal Mid R.
Next, we investigate the robustness of the generated network in AD study. Recall that one key feature of the proposed SICE algorithm is that it allows the user feedback to
be incorporated into the estimation process by imposing additional constraints in the optimization formulation. In this
experiment we study how the connectivity changes as more
constraints are enforced in the network. Since the patterns
in AD patients are of great interest, we focus on the AD subjects. Speciﬁcally, we remove ﬁve links in the network in Figure 1: the link between Temporal Pole Mid R and Temporal Pole Inf R 8302, the link between Cingulum Post L and
Cingulum Post R, the link between Hippocampus L and Par
-raHippocampal L, the link between Temporal Sup R and

6. CONCLUSIONS AND FUTURE WORK
In this paper, we propose a novel sparse inverse covariance
estimation algorithm to discover the connectivity among different brain regions for AD study. One appealing feature of
the proposed algorithm is that it can incorporate the user
feedback into the estimation process, while the connectivity
patterns can be discovered automatically. Our experimental
results on a collection of FDG-PET images demonstrate the
eﬀectiveness of the proposed algorithm for analyzing brain
region connectivity for Alzheimer’s disease study.
We plan to develop an interactive software tool to help
AD domain experts to investigate the connectivity among
diﬀerent brain regions. The typical diagnosis is an interactive process, in which the prior knowledge can be obtained
and updated. We aim to integrate such prior knowledge
seamlessly and reveal the connectivity more precisely.
Functional magnetic resonance imaging (fMRI) is a procedure that is widely used for human brain function analysis in normal controls as well as in diseased individuals.
Most recently, it has been used to study the intrinsic activity
and connectivity of the brain under resting condition (i.e.,
there is no cognitive task), a.k.a the default mode network
(DMN). Recent researches have found signiﬁcantly diﬀerent
DMN diﬀerences between AD patients and NC, opening the
possibility of using the resting DMN as an AD diagnosis
biomarker. We plan to investigate the functional MRI data
for Alzheimer’s disease study using the proposed algorithm.

Acknowledgments
This research is sponsored in part by the Arizona Alzheimer’s
Consortium and by NSF IIS-0612069 and IIS-0812551.
Data collection and sharing for this research was funded
by the Alzheimer’s Disease Neuroimaging Initiative (ADNI;
PI: Michael Weiner; NIH grant U01 AG024904). ADNI is
funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering (NIBIB),
and through generous contributions from the following: Pﬁzer
Inc., Wyeth Research, Bristol-Myers Squibb, Eli Lilly and
Company, GlaxoSmithKline, Merck & Co. Inc., AstraZeneca
AB, Novartis Pharmaceuticals Corporation, Alzheimer’s Association, Eisai Global Clinical Development, Elan Corpora-

1340

Cingulum_Post_L

Hippocampus_L

Occipital_Sup_L

Cingulum_Post_R

ParaHippocampal_L

Fusiform_R

Hippocampus_R

ParaHippocampal_R

Parietal_Inf_R

Temporal_Sup_L

Occipital_Sup_R
Occipital_Mid_L
Occipital_Inf_L

Occipital_Inf_R

Temporal_Pole_Sup_R

Temporal_Sup_R

Temporal_Pole_Mid_L

Temporal_Mid_R

Temporal_Pole_Mid_R

Occipital_Mid_R

Fusiform_L

Temporal_Pole_Sup_L

Temporal_Mid_L
Temporal_Inf_L 8301

Temporal_Inf_R 8302

Parietal_Sup_L
Parietal_Inf_L

Parietal_Sup_R

Precuneus_L
Precuneus_R

Figure 1: The connectivity among diﬀerent brain regions for AD patients.
Temporal_Sup_L
Precuneus_L

Precuneus_R

Parietal_Sup_L

Parietal_Sup_R

Fusiform_L

Fusiform_R

Parietal_Inf_L

Parietal_Inf_R

Occipital_Mid_L
Occipital_Sup_L

Hippocampus_L

Cingulum_Post_L

Occipital_Sup_R

Occipital_Inf_L

Occipital_Inf_R

Occipital_Mid_R

Hippocampus_R

Temporal_Sup_R

Temporal_Mid_R

ParaHippocampal_L

Temporal_Mid_L

Temporal_Inf_L 8301

ParaHippocampal_R

Temporal_Pole_Mid_L

Temporal_Pole_Mid_R

Cingulum_Post_R

Temporal_Pole_Sup_L

Temporal_Inf_R 8302

Temporal_Pole_Sup_R

Figure 2: The connectivity among diﬀerent brain regions for Mild Cognition Impairment patients.
Temporal_Pole_Mid_L
Temporal_Pole_Sup_L
Temporal_Sup_L

Temporal_Mid_L

Precuneus_L

Precuneus_R

Parietal_Inf_L

Parietal_Inf_R

Parietal_Sup_L

Parietal_Sup_R

Occipital_Sup_L

Occipital_Sup_R

Temporal_Inf_L 8301

Temporal_Pole_Sup_R
Temporal_Pole_Mid_R

Hippocampus_R
ParaHippocampal_R

Hippocampus_L
ParaHippocampal_L

Fusiform_L

Fusiform_R
Temporal_Inf_R 8302

Temporal_Sup_R
Temporal_Mid_R
Occipital_Mid_R
Occipital_Mid_L

Cingulum_Post_L

Occipital_Inf_L

Occipital_Inf_R

Cingulum_Post_R

Figure 3: The connectivity among diﬀerent brain regions for Normal Controls.

1341

Table 3: The empirical correlation matrix of the 12 brain regions for AD patients. Each row/column in the
table corresponds to a region. The (i, j)th entry is the correlation between the ith region and the jth region.
The entries highlighted in bold type and red color correspond to a connection in the network generated
by the proposed algorithm. These 12 regions include: Occipital Sup L, Occipital Sup R, Occipital Mid L,
Occipital Mid R, Occipital Inf L, Occipital Inf R, Fusiform L, Parietal Sup L, Parietal Sup R, Parietal Inf L
Precuneus L, and Precuneus R.
1.00

0.83

0.91

0.75

0.79

0.66

0.50

0.78

0.48

0.63

0.67

0.54

0.83

1.00

0.71

0.90

0.57

0.76

0.20

0.78

0.71

0.54

0.60

0.71

0.91

0.71

1.00

0.72

0.92

0.62

0.67

0.73

0.40

0.68

0.64

0.44

0.75

0.90

0.72

1.00

0.64

0.86

0.29

0.68

0.61

0.48

0.44

0.56

0.79

0.57

0.92

0.64

1.00

0.65

0.77

0.57

0.25

0.50

0.44

0.26

0.66

0.76

0.62

0.86

0.65

1.00

0.32

0.55

0.48

0.33

0.31

0.41

0.50

0.20

0.67

0.29

0.77

0.32

1.00

0.28

-0.02

0.20

0.26

0.01

0.78

0.78

0.73

0.68

0.57

0.55

0.28

1.00

0.81

0.81

0.82

0.71

0.48

0.71

0.40

0.61

0.25

0.48

-0.02

0.81

1.00

0.54

0.55

0.66

0.63

0.54

0.68

0.48

0.50

0.33

0.20

0.81

0.54

1.00

0.83

0.66

0.67

0.60

0.64

0.44

0.44

0.31

0.26

0.82

0.55

0.83

1.00

0.80

0.54

0.71

0.44

0.56

0.26

0.41

0.01

0.71

0.66

0.66

0.80

1.00

Temporal_Sup_L
Precuneus_R
Precuneus_L
Parietal_Sup_L

Parietal_Inf_L

Parietal_Inf_R

Parietal_Sup_R

Temporal_Sup_R
Occipital_Sup_R
Occipital_Sup_L

Occipital_Mid_R

Occipital_Inf_R

Temporal_Mid_R

Temporal_Mid_L

Temporal_Inf_L 8301

Fusiform_L

Fusiform_R

Temporal_Pole_Mid_L

Temporal_Pole_Mid_R

Occipital_Inf_L

Occipital_Mid_L
Hippocampus_R

Hippocampus_L

ParaHippocampal_L

Cingulum_Post_L

Cingulum_Post_R

Temporal_Inf_R 8302

ParaHippocampal_R

Temporal_Pole_Sup_L

Temporal_Pole_Sup_R

Figure 4: The connectivity among diﬀerent brain regions for Mild Cognition Impairment patients after
removing one link between Occipital Mid R and Temporal Mid R.
Cingulum_Post_L

Cingulum_Post_R

Hippocampus_L

ParaHippocampal_L

Occipital_Sup_L

Hippocampus_R

ParaHippocampal_R

Occipital_Sup_R

Fusiform_R

Parietal_Inf_R

Temporal_Mid_R

Temporal_Pole_Sup_L

Temporal_Pole_Sup_R

Temporal_Pole_Mid_L

Temporal_Mid_L
Temporal_Inf_L 8301

Temporal_Pole_Mid_R

Occipital_Mid_R

Parietal_Inf_L

Temporal_Sup_R

Temporal_Inf_R 8302

Occipital_Mid_L

Parietal_Sup_L

Temporal_Sup_L

Occipital_Inf_L

Occipital_Inf_R

Fusiform_L
Parietal_Sup_R

Precuneus_L
Precuneus_R

Figure 5: The connectivity among diﬀerent brain regions for AD patients after removing ﬁve links (see the
texts for details).

1342

[15] J.Z. Huang, N. Liu, M. Pourahmadi, and L. Liu.
Covariance matrix selection and estimation via
penalised normal likelihood. Biometrika, 93(1):85–98,
2006.
[16] S.L. Lauritzen. Graphical models. Oxford University
Press, Clarendon, 1996.
[17] H. Li and J. Gui. Gradient directed regularization for
sparse Gaussian concentration graphs, with
applications to inference of genetic networks.
Biostatistics, 7(2):302–317, 2005.
[18] Y. Lin. Model selection and estimation in the Gaussian
graphical model. Biometrika, 94(1):19–35, 2007.
[19] G. McKhann, D. Drachman, and M. Folstein. Mental
and clinical diagnosis of alzheimer’s disease: report of
the nincdsadrda work group under the auspices of the
department of health and human services task force on
alzheimers disease. Neurology, 34(7):939–944, 1984.
[20] S. Molchan. The Alzheimer’s disease neuroimaging
initiative. Business Briefing: US Neurology Review,
pages 30–32, 2005.
[21] A. Nemirovski. Prox-method with rate of convergence
o(1/t) for variational inequalities with lipschitz
continuous monotone operators and smooth
convex-concave saddle point problems. SIAM Journal
on Optimization, 15(1):229–251, 2005.
[22] Y. Nesterov. Smooth minimization of non-smooth
functions. Mathematical Programming,
103(1):127–152, 2005.
[23] C.J. Stam, B.F. Jones, G. Nolte, M. Breakspear, and
P. Scheltens. Small-world networks and functional
connectivity in Alzheimer’s disease. Cereb Cortex,
17:92–99, 2007.
[24] K. Supekar, V. Menon, D. Rubin, M. Musen, and
M.D. Greicius. Network analysis of intrinsic functional
brain connectivity in alzheimer’s disease. PLoS
Computational Biology, 4(6):e1000100, 2008.
[25] J. Tegner, M.K. Yeung, J. Hasty, and J.J. Collins.
Reverse engineering gene networks: integrating genetic
perturbations with dynamical modeling. Proceedings
of the National Academy of Sciences,
100(10):5944–5949, 2003.
[26] R. Tibshirani. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society
Series B, 58(1):267–288, 1996.
[27] P. Tseng. Convergence of block coordinate descent
method for nondiﬀerentiable maximation. J. Opt.
Theory and Applications, 109(3):474–494, 2001.
[28] N. Tzourio-Mazoyer, B. Landeau, D. Papathanassiou,
F. Crivello, O. Etard, N. Delcroix, B. Mazoyer, and
M. Joliot. Automated anatomical labeling of
activations in spm using a macroscopic anatomical
parcellation of the mni mri single-subject brain.
NeuroImage, 15(1):273–289, 2002.
[29] S. Wakana, H. Jiang, L.M. Nagae-Poetscher, P.C. van
Zijl, and S. Mori. Fiber tract-based atlas of human
white matter anatomy. Radiology, 230(1):77–87, 2004.
[30] J. Ye, K. Chen, T. Wu, J. Li, Z. Zhao, R. Patel,
M. Bae, R. Janardan, H. Liu, G. Alexander, and
E. Reiman. Heterogeneous data fusion for Alzheimer’s
disease study. In KDD, pages 1025–1033, 2008.

tion plc, Forest Laboratories, and the Institute for the Study
of Aging, with participation from the U.S. Food and Drug
Administration. Industry partnerships are coordinated through the Foundation for the National Institutes of Health.
The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer’s Disease Cooperative Study at the
University of California, San Diego. ADNI data are disseminated by the Laboratory of NeuroImaging at the University
of California, Los Angeles.

7.

REFERENCES

[1] G. Alexander and E. Reiman. Neuroimaging. In M.F.
Weiner and A.M. Lipton, editors, The dementias:
diagnosis, treatment and research, 2003.
[2] O. Banerjee, L. El Ghaoui, and A. d’Aspremont.
Model selection through sparse maximum likelihood
estimation for multivariate Gaussian or binary data.
Journal of Machine Learning Ressearch, 9:485–516,
2008.
[3] O. Banerjee, L. El Ghaoui, A. d’Aspremont, and
G. Natsoulis. Convex optimization techniques for
ﬁtting sparse Gaussian graphical models. In ICML,
pages 89–96, 2006.
[4] A. Berge, A.C. Jensen, and A.H.S. Solberg. Sparse
inverse covariance estimates for hyperspectral image
classiﬁcation. Geoscience and Remote Sensing, IEEE
Transactions on, 45(5):1399–1407, 2007.
[5] J.A. Bilmes. Factored sparse inverse covariance
matrices. In ICASSP, pages 1009–1012, 2000.
[6] J. Dahl, L. Vandenberghe, and V. Roychowdhury.
Covariance selection for nonchordal graphs via chordal
embedding. Optimization Methods Software,
23(4):501–520, 2008.
[7] X. Delbeuck, M. Van der Linden, and F. Collette.
Alzheimer’s disease as a disconnection syndrome?
Neuropsychology Review, 13(2):79–92, 2003.
[8] A.P. Dempster. Covariance selection. Biometrics,
28(1):157–175, 1972.
[9] A. Dobra, C. Hans, B. Jones, J. R. Nevins, G. Yao,
and M. West. Sparse graphical models for exploring
gene expression data. Journal of Multivariate
Analysis, 90(1):196–212, 2004.
[10] D.L. Donoho. For most large underdetermined
systems of linear equations, the minimal 11-norm
near-solution approximates the sparsest near-solution.
Communications on Pure and Applied Mathematics,
59(7):907–934, 2006.
[11] M.F. Folstein, S. Folstein, and P.R. McHugh.
Minimental state: a practical method for grading the
cognitive state of patients for the clinician. Journal of
Psychiatric Research, 12(3):189–198, 1975.
[12] J. Friedman, T. Hastie, and R. Tibshirani. Sparse
inverse covariance estimation with the graphical lasso.
Biostatistics, 8(1):1–10, 2007.
[13] T.S. Gardner, D. di Bernardo, D. Lorenz, and J.J.
Collins. Inferring genetic networks and identifying
compound mode of action via expression proﬁling.
Science, 301(5629):102–105, 2003.
[14] L. Heston and J. White. The vanishing mind: A
practical guide to Alzheimer’s disease and other
dementias. W. H. Freeman and Co., New York, 1983.

1343

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 17, NO. 5, OCTOBER 2013

705

An Adaptive Particle Swarm Optimization With
Multiple Adaptive Methods
Mengqi Hu, Teresa Wu, and Jeffery D. Weir

Abstract—Particle swarm optimization (PSO) has attracted
much attention and has been applied to many scientific and
engineering applications in the last decade. Most recently, an
intelligent augmented particle swarm optimization with multiple
adaptive methods (PSO-MAM) was proposed and was demonstrated to be effective for diverse functions. However, inherited
from PSO, the performance of PSO-MAM heavily depends
on the settings of three parameters: the two learning factors
and the inertia weight. In this paper, we propose a parameter
control mechanism to adaptively change the parameters and thus
improve the robustness of PSO-MAM. A new method, adaptive
PSO-MAM (APSO-MAM) is developed that is expected to be
more robust than PSO-MAM. We comprehensively evaluate the
performance of APSO-MAM by comparing it with PSO-MAM
and several state-of-the-art PSO algorithms and evolutionary
algorithms. The proposed parameter control method is also
compared with several existing parameter control methods. The
experimental results demonstrate that APSO-MAM outperforms
the compared PSO algorithms and evolutionary algorithms, and
is more robust than PSO-MAM.
Index Terms—Adaptive, cauchy mutation, nonuniform mutation, parameter control, particle swarm optimization (PSO),
subgradient.

I. Introduction

I

NSPIRED BY the social cooperative and competitive behavior of bird flocking and fish schooling, Kennedy and
Eberhart [1], [2] proposed a new optimization technique called
particle swarm optimization (PSO). Unlike certain evolutionary algorithms (EAs), PSO adopts leaders to guide the search
of each particle in the swarm [3]. In PSO, the particles cooperate with each other and explore the search space directed
by a combination of the swarm’s previous best (gBest) and
their own previous best (pBest), with an additional stochastic
element [4].
During the last two decades, PSO has attracted great attention and has been successfully applied to various industry
applications [5]. PSO may outperform other evolutionary
Manuscript received September 24, 2011; revised July 10, 2012; accepted
December 4, 2012. Date of publication December 11, 2012; date of current
version September 27, 2013.
M. Hu is with the Department of Industrial and Systems Engineering, Mississippi State University, Starkville, MS 39762 USA (e-mail:
mhu@ise.msstate.edu).
T. Wu is with the School of Computing, Informatics, Decision Systems
Engineering, Arizona State University, Tempe, AZ 85287 USA (e-mail:
teresa.wu@asu.edu).
J. D. Weir is with the Department of Operational Sciences, Graduate School
of Engineering and Management, Air Force Institute of Technology, WrightPatterson AFB, OH 45433 USA (e-mail: jeffery.weir@afit.edu).
Digital Object Identifier 10.1109/TEVC.2012.2232931

algorithms (e.g., genetic algorithms, memetic algorithms) in
terms of solution quality and computational efficiency on
some optimization problems [6], [7]. However, three common criticisms exist. First, most existing PSOs are designed
for a specific search space thus an algorithm performing
well on a diverse set of problems is lacking. For example,
the comprehensive learning strategy proposed in CLPSO [8]
achieves high-quality performance on complex multimodal
functions due to its effectiveness in avoiding local optima
while its convergence rate on unimodal functions is unsatisfactory. Secondly, PSO suffers premature convergence [4].
Thirdly, the performance of PSO is heavily dependent on
the settings of the three parameters: 1) cognitive learning
factor c1 ; 2) social learning factor c2 ; and 3) inertia weight
w.
In order to address the first two issues, an intelligent
augmented PSO with multiple adaptive methods (PSO-MAM)
is proposed in [9] to obtain a PSO method that performs
well across multiple functions. Although PSO-MAM has
been demonstrated to be effective for functions with different
properties, we observe that its performance is sensitive to
the parameter settings. This is also a notable issue for most
existing PSO algorithms [10]–[17]. Realizing this, a number
of researchers have successfully studied different parameter
control mechanisms to improve the performance [18]–[26].
We propose an adaptive parameter control mechanism to
enhance PSO-MAM which formulates the parameter control
problem as a convex optimization problem, and adopts the
subgradient method [27] to adaptively change the parameters (see Section III-B). It is expected that adaptive PSOMAM (APSO-MAM) is more robust than PSO-MAM as it
is less sensitive to the initial parameter settings. Secondly, the
overall performance (solution quality and convergence speed)
could be improved. We have demonstrated the advantages of
APSO-MAM through comprehensive comparisons with several state-of-the-art PSO and evolutionary algorithms. Meanwhile, shortcomings of the proposed methods are identified
for future research.
This paper is organized as follows. Several state-ofthe-art PSO algorithms are reviewed in Section II, followed by the detailed explanation on the proposed APSOMAM in Section III. A comparison study between stateof-the-art PSO algorithms and evolutionary algorithms is
conducted in Section IV. Conclusions are drawn in Section V.

c 2012 IEEE
1089-778X 

706

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 17, NO. 5, OCTOBER 2013

II. Literature Review
A. Parameter Selection in Particle Swarm Optimization
In PSO with inertia weight, the velocity and position for
particle p at iteration i are updated as [28]


 i
 i
i
i
i
i
i
vi+1
p = wvp + c1 r1,p × pp − xp + c2 r2,p × pg − xp
xpi+1 = xpi + vpi+1

(1)
(2)

where
D-dimensional vector vpi = velocity of the pth particle (vpi ∈
[−Vmax , +Vmax ]), Vmax is used to constrain the velocity for
each particle and is usually set between 0.1 and 1.0 times the
search range of the solution space [4];
D-dimensional vector xpi = position of the pth particle;
pip best position found so far by the pth particle;
pig best position found so far by the swarm;
i
i
and r2,p
r1,p
two independent random numbers uniformly
distributed on [0, 1];
c1 cognitive learning factor that represents the attraction that
a particle has toward its own success pip ;
c2 social learning factor that represents the attraction that a
particle has toward the swarm’s best position pig ;
w inertia weight.
Over the last decade, two different theoretical models (deterministic and stochastic) have been proposed to study the
impacts of PSO parameters on the performance of PSO. In
the deterministic model, the stochastic components r1 and r2
in (1) are ignored. For the deterministic model, van den Bergh
[13] studies the PSO with inertia weight and concludes that
the particle will converge to the stable point


xp = c1 pp + c2 pg (c1 + c2 )
(3)
when


and w > (c1 + c2 ) 2 − 1.

0≤w<1

(4)

And the particles will become divergent if the conditions
in (4) are not satisfied. By using stochastic process theory
to analyze the particle trajectory of the stochastic model,
Jiang et al. [15] demonstrate that the expectation of particle
position is guaranteed to converge to the stable point in (3)
when
0≤w<1

and

0 < c1 + c2 < 4 (w + 1) .

(5)

Though the theoretical results for the two models may hold
only for some specific PSO algorithms (e.g., original PSO
[2], PSO with inertia weight [28]), these results could provide
some general guidelines on parameter settings for PSO and its
variants.
B. Some PSO Variants
In general, the performance of PSO is affected by swarm
size [29] and swarm neighborhood topology [3], [30]–[34].
For example, Liang and Suganthan [32] dynamically divide
the swarm into several small swarms which can interact
with each other. A dynamic hierarchy is used to define the
neighborhood topology in [33]. Recently, notable efforts are

devoted to studying the impact of exemplar learning strategy
(selection of gBest and pBest) on the performance of PSO
[8], [35]–[40]. In CLPSO [8], the particle learns from other
particle’s personal best and no swarm best is used. Two
learning strategies—standard PSO learning and generalized
opposition-based learning are alternatively applied based on
a dynamically updated probability in [38]. A set of multiple
swarm best particles is selected to guide the movement of
particles in ELPSO [39].
Extensive research has demonstrated that the performance
of PSO (e.g, convergence rate, solution quality) could be
much improved by using the memetic framework [41]. Plevris
and Papadrakakis [42] improve the performance of PSO for
global structural optimization by integrating it with gradientbased quasi-Newton sequential quadratic programming. Fan
and Zahara [43] explore the integration of PSO with the
Nelder–Mead simplex search method for unconstrained optimization. By incorporating local search techniques in PSO,
Petalas et al. [44] propose a memetic PSO algorithm which
is demonstrated to outperform the global and local variants
of PSO algorithms. Li et al. [45] explore combining PSO
with a modified Broyden–Fletcher–Goldfarb–Shanno method
to achieve high solution quality for multimodal functions.
Hu et al. [9] propose a novel PSO algorithm, termed as PSOMAM by incorporating multiple adaptive search methods to
improve PSO’s performance on a diverse set of optimization
problems. This paper intends to extend PSO-MAM to further
improve the robustness. PSO-MAM is briefly reviewed in the
following section.
C. PSO-MAM
1) Intelligent Multiple Search Methods: In order to improve PSO’s performance on a diverse set of optimization
problems, an intelligent multiple search methods module is
adopted to improve particle xgi in the current swarm where g
is
  

.
(6)
g = p : pip = argminp=1,...,P f pip
The particle xgi will be replaced if it is improved by the
multiple search methods module. Please note we study a
minimization problem in this paper.
Two search techniques (nonuniform mutation-based method
and adaptive subgradient method) are studied in the intelligent multiple search methods. The nonuniform mutation-based
method [46] may be preferred by multimodal functions due
to its good balance between exploration and exploitation. To
complement the search capability for unimodal function, the
subgradient method [27] is chosen because of its performance
for finding a local optimum very fast and good exploitation
capability [42]. In order to balance the subgradient method’s
exploration and exploitation capability, the step size αi is
defined according to the velocity of the gth particle vgi
   
αi = vgi 2 γgi 
(7)
2

where x2 is the Euclidean norm of vector x, γgi is the
subgradient of function f evaluated at xgi . At each iteration
of the PSO-MAM, the effective search method is selected by

HU et al.: ADAPTIVE PARTICLE SWARM OPTIMIZATION WITH MULTIPLE ADAPTIVE METHODS

using roulette wheel selection based on the search method’s
performance [9].
2) Extended Cauchy Mutation: The extended Cauchy
mutation operator is launched to introduce randomness to
increase the diversity of the swarm. In the extended Cauchy
mutation operator, a randomly selected dimension d of a
randomly selected particle m will be mutated as
i

i
xm,d
= xm,d
+ Cauchy (δi )

of iterations. Jiao et al. [19], Chatterjee and Siarry [20]
improve the performance of PSO by changing the inertia
weight according to nonlinear functions which are expressed
as
w = w0 u−i ,

(8)

where δi is the scale parameter of Cauchy distribution which
is adaptively changed according to particles’ velocity information. The proposed extended Cauchy mutation is demonstrated
to outperform the Cauchy mutation with constant scale parameter in [9].
3) Issues Exist in PSO-MAM: Although PSO-MAM was
demonstrated to perform well across multiple functions with
different properties [9], its performance is highly dependent
on the settings of its parameters: the cognitive learning factor
c1 , the social learning factor c2 , and the inertia weight w. In
general, the solution quality and convergence rate could be
greatly impacted by the parameter settings of the evolutionary
algorithms, and the tuning and controlling of the parameter
settings is one of the most critical and promising research areas
in evolutionary computation [47]. Therefore, we propose to
develop an adaptive parameter control mechanism to enhance
the robustness and the performance of PSO, specifically PSOMAM, which is discussed in the next section.
III. Proposed Adaptive Parameter Control
Mechanism
In this section, various methods on parameter control are
reviewed followed by the detailed explanation on the proposed
control mechanism. By applying the adaptive control to the
published PSO-MAM method, APSO-MAM is developed.
The comparison experiments to demonstrate the effectiveness
of the adaptive control mechanism are first conducted. The
improvements of APSO-MAM over PSO-MAM are then assessed.
A. Literature Review on Parameter Control Mechanisms for
PSO
Extensive research [18]–[26] has attempted to develop parameter control rules to reduce the sensitivity of PSO to its
parameter settings. The existing parameter control methods
could be classified into two categories: 1) simple rule based
parameter control where linear, nonlinear functions, or fuzzy
rules are explored, and 2) adaptive parameter control that
considers the current evolutionary/search information in the
parameter control. Each mechanism is reviewed in the following sections.
1) Simple Rule Based Parameter Control: Shi and Eberhart [18] show that the performance of PSO can be greatly
improved by linearly decreasing the inertia weight as

w = wmax − (wmax − wmin ) i I
(9)
where wmax and wmin are usually fixed as 0.9 and 0.4; i is
the current iteration number; and I is the maximum number

707

w0 ∈ [0, 1] ,

u ∈ [1.001, 1.005]


w = wmin + (wmax − wmin ) (I − i)n I n

(10)

(11)

where n is the nonlinear modulation index [20]. A random
function is implemented in [22] to change the inertia weight
as

w = 0.5 + random (0, 1) 2.

(12)

A fuzzy-rule-based control mechanism is proposed in [21] to
modify the inertia weight.
Although a simple rule based parameter control mechanism
could improve performance of PSO for several problem instances, the performance on a broader spectrum of problems
is unsatisfactory [18]–[23]. Another criticism as in [19], [20],
is that additional parameters may be needed for this type of
mechanism.
2) Adaptive Parameter Control: Zhan et al. [24] employ
the evolutionary state estimation method to identify the evolutionary states of the swarm as exploration, exploitation, convergence and jumping out, and propose an adaptive parameter
control mechanism to change c1 and c2 . The inertia weight w
is modified as
w=1




1 + 1.5e−2.6f ∈ [0.4, 0.9] ∀f ∈ [0, 1]

(13)

where f is the evolutionary factor proposed in [24]. The
proposed adaptive particle swarm optimization is demonstrated
to outperform other existing PSO algorithms for most problem
instances studied in [24]. Juang et al. [25] propose an adaptive
fuzzy PSO termed as AFPSO where the inertia weight is
altered according to (9) and the two learning factors c1 and c2
are changed according to three fuzzy rules proposed in [25].
Please note the methods reviewed above [24], [25] provide
a fuzzy guideline (e.g., slightly increase, slightly decrease)
for the changes. To explicitly quantify the parameter settings,
Yamaguchi and Yasuda [26] assign c1 and c2 for each particle
and propose to update c1 and c2 using the following equations:


i+1
i
i
c1,p
= c1,p
+ αip cbest1i − c1,p

(14)



i+1
i
i
= c2,p
c2,p
+ αip cbest2i − c2,p

(15)

where cbest1i and cbest2i are parameters for the global best
particles at iteration i; αip is selected from two values 0 and 2/I.
Experiments though show that [26] is effective on only a few
instances. In this paper, we propose to adaptively and explicitly
change the parameters to improve PSO-MAM’s performance
for diverse functions.

708

Fig. 1.

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 17, NO. 5, OCTOBER 2013

Flowchart of APSO-MAM (S1: nonuniform mutation-based method; S2: subgradient method).

B. Proposed Adaptive Parameter Control for PSO-MAM
As shown in Fig. 1, the proposed APSO-MAM has four
modules.
1) PSO module (PSO with inertia weight [28]): The swarm
is randomly initialized with the PSO operator being
employed to update the swarm.
2) Intelligent multiple search methods module (adopted
from PSO-MAM): Two search methods (nonuniform mutation-based method and adaptive subgradient
method) are implemented to improve the best particle in

the current iteration which updates the gBest. At each
iteration, an appropriate search method will be triggered
using the roulette wheel selection.
3) Mutation module (adopted from PSO-MAM): After further improvement on the best particle, the mutation
operator is used to update one randomly selected particle
and increase the diversity of the swarm.
4) Parameter control module: The three parameters for one
randomly selected particle will be changed by the adaptive parameter control mechanism (see Section III-B) to
improve the robustness of the APSO-MAM algorithm.

HU et al.: ADAPTIVE PARTICLE SWARM OPTIMIZATION WITH MULTIPLE ADAPTIVE METHODS

The algorithm will stop if the stopping criterion (such
as the maximum number of PSO iterations, predefined
solution accuracy) is satisfied.
The PSO-MAM [9] attempts to balance the performance
on both the unimodal and multimodal functions by using the intelligent multiple search methods module and
the extended Cauchy mutation. By intelligently choosing
nonuniform mutation-based method or adaptive subgradient
method, PSO-MAM is able to select the effective search
technique during each iteration to improve the performance of
PSO-MAM on both unimodal and multimodal functions. Although extended Cauchy mutation performs well for multimodal functions to avoid local trapping, it may slow the
convergence speed of PSO-MAM for unimodal functions.
Therefore, the proposed parameter control method attempts
to reduce this effect, and improves the convergence speed for
unimodal functions and multimodal functions if a good leader
is found at each iteration.
The proposed parameter control method is expected to:
1) improve the convergence speed of PSO-MAM for unimodal functions and some of the multimodal functions, and
2) improve the robustness of PSO-MAM. Motivated by these
two purposes, the proposed parameter control method is to
choose the particles close to the gBest through updating
their parameter settings if a better gBest is identified at
each iteration. This parameter control process is triggered to
increase the attraction from gBest only when a gBest with
improved performance is located by multiple search methods
and/or extended Cauchy mutation. Given the extended Cauchy
mutation is capable of preventing local trapping to increase the
possibility for generating a good leader (gBest) for multimodal
functions, and the proposed adaptive parameter control could
increase the convergence speed on unimodal functions, the
integration of the two approaches will improve the overall
performance of APSO-MAM on both the multimodal and
unimodal functions.
In this paper, the parameter control is formulated as minimizing the distance between the particles and gBest. Instead
of using the same parameters for all particles, each particle
is allowed to adjust its parameters. Therefore, different parameters may be adopted by different particles. In order to
reduce the computational time spent on parameter control, at
iteration i, we focus on one randomly selected particle among
those particles whose fitness values (FVs) corresponding to
the pBest are smaller than the average FV corresponding to
the pBest for the swarm, and minimize the distance between
this randomly selected particle xli+1 and the gBest pig . Taking
i
i
wil , c1,l
and c2,l
as decision variables, a convex optimization
problem is formulated as
 2


i i
i i
 i+1
  i
c1,l
r1,l × pil − xli 
i
i 2  xl + wl vl + 


min fdist = xl − pg 2 = 
i i

+c2,l
r2,l × pig − xli − pig
2
s.t. 0.5 ≤ c1 ≤ 2.5, 0.5 ≤ c2 ≤ 2.5, 0.4 ≤ w ≤ 0.9.
(16)
It is intuitive that the three constraints in (16) satisfy the
particle stability conditions expressed in (5). The subgradient
method [27] is employed to solve the convex optimization
problem formulated in (16) for one iteration. Taking wil as

709

an example, we can update it as described in the following
equation:
i
= wil − αil gw
wi+1
l
l

(17)

i
where αil and gw
are the step size and subgradient of the
l
objective function expressed in (16) at PSO iteration i for
particle l. Since the objective function in (16) is derivable,
i
i
gw
is computed as the derivative of fdist
evaluated at wil . The
l
∗
optimal step size when the optimal value fdist
of the convex
objective function is known as Polyak’s step size [27] which
is computed as
	


 i

 i  2  i 2  i 2
∗
gwl + gc1,l + gc2,l
. (18)
− fdist
αil = fdist
∗
is 0. In the proposed parameter control
The optimal value fdist
mechanism, the parameters w, c1 and c2 are adjusted following
(17), (19), and (20)
i+1
i
c1,l
= c1,l
− αil gci 1,l

(19)

i+1
i
= c2,l
− αil gci 2,l .
c2,l

(20)

The subgradients of the objective function expressed in (16)
i
i
corresponding to wil , c1,l
and c2,l
are computed as
i
gw
=
l

D

 i+1

2 xl,d
− pig,d × vil,d

(21)

d=1

gci 1,l =

D

 i+1



i
i
2 xl,d
− pig,d × r1,l
× pil,d − xl,d

(22)

d=1

gci 2,l =

D

 i+1



i
i
2 xl,d
− pig,d × r2,l
× pig,d − xl,d

(23)

d=1
i
where xl,d
is the dth component of xli .
It is observed from (17), (19), and (20) that the proposed
adaptive parameter control mechanism explicitly directs the
parameter changes instead of providing a fuzzy guideline [24],
[25]. In addition, when the current gBest is updated, that is,
a gBest with improved performance is identified, the particles
should be encouraged to get close to the gBest. As a result,
all three parameters are adjusted in response to gBest where
to our best knowledge, existing methods reviewed in Section
III-A focus on adjusting each parameter individually. Taking
a 1-D search space as an example, the changes of w, c1 and
c2 are
 


wi+1
= wil + αil × 2 pig − xli+1 × vil
(24)
l



 

i+1
i
i
= c1,l
+ αil × 2 pig − xli+1 × r1,l
× pil − xli
c1,l

(25)



 

i+1
i
i
= c2,l
+ αil × 2 pig − xli+1 × r2,l
× pig − xli .
c2,l

(26)

As demonstrated in Fig. 2, w will increase
if the velocity

impact vil has the same direction as pig − xli+1 . Otherwise,
w will decrease. It indicates that the velocity impact will
be strengthened by increasing w if it has a positive effect

710

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 17, NO. 5, OCTOBER 2013

TABLE I
t-Test Comparison Between APSO-MAM and Other Four Parameter Control Methods
APSO-MAM v.s.
Linear control
Nonlinear control
Adaptive fuzzy control
Explicit control

+ (Better)
6
5
6
3

FV
= (Same)
24
25
23
26

– (Worse)
1
1
2
2

+ (Better)
25
26
25
25

SP
= (Same)
6
5
6
6

– (Worse)
0
0
0
0

TABLE II
Wilcoxon Test Between APSO-MAM and Other Four Parameter Control Methods
APSO-MAM v.s.
R+
430
422.5
404.5
341

Linear control
Nonlinear control
Adaptive fuzzy control
Explicit control

Fig. 2.

FV
R−
66
73.5
91.5
155

Changing for w on one-dimension search space.
TABLE III

Rankings Obtained Through Friedman’s Test for Different
Parameter Control Methods
Algorithms
Linear control
Nonlinear control
Adaptive fuzzy control
Explicit control
APSO-MAM

FV Ranking
3.355
3.565
3.339
2.613
2.129

SP Ranking
3.710
4.548
3.226
2.194
1.323

on improving the attraction of gBest which could achieve
fast convergence speed. Similarly,
c1 and c2 will
if

 i increase

i
the pBest impact pil − xli , and gBest
impact
p
have
−
x
g
l

the same directions as pig − xli+1 . Note when extending to
multidimensional search space, the changes of w, c1 and c2
depend on the velocity impact, pBest impact, and gBest impact
on each dimension. The dth dimension has a positive effect
on increasing parameters w, c1 and c2 if the velocity impact,
pBest impact, and gBest impact
 on this dimension have the
i+1
same directions as pig,d − xl,d
.
C. Experiments to Assess the Effectiveness of the Proposed
Parameter Control Mechanism
1) Performance Metrics: To evaluate the overall performance in regard to both the solution quality and computing
cost, we adopt the metrics FV, success performance (SP), and
success rate (SR) from [48]. In the experiments conducted
in Section III-C, the population size is set to be 30 and
the maximum number of function evaluations is set to be
300 000. Thirty independent runs are conducted to collect
data for statistical analysis. A run during which the algorithm
achieves the fixed accuracy level within the maximum number

p-value
0.001
0.002
0.004
0.036

R+
488.5
488.5
488.5
488.5

SP
R−
7.5
7.5
7.5
7.5

p-value
8.298E-06
8.298E-06
8.298E-06
8.298E-06

of function evaluations is considered to be successful. In this
example, the accuracy level is set to be 10−5 . The FV is the
objective value when the algorithm stops. The SR is defined
as

SR = No. of successful runs total # of runs.
(27)
The SP is the number of function evaluations for the algorithm
to reach the fixed accuracy level. The mean of SP is defined
in [48] as

 
mean (SP) = (1 − SR) SR FEmax
+mean (# of function evaluations for successful runs)
(28)
where FEmax is the maximum number of function evaluations. For fair comparison, we propose to evaluate the
computational complexity of subgradient calculation by using
equivalent number of function evaluations for subgradient
calculation (SGFE). SGFE is determined as the computational time for the D-dimensional subgradient calculations
divided by the computational time of the objective function
evaluation



SGFE = ceil tocγg − ticγg
tocf − ticf
(29)
where ceil(·) rounds the element to the nearest integer toward infinity; toc and tic are the output of “toc” and “tic”
functions in MATLAB® ; tocγg − ticγg is the elapsed time for
calculating the subgradient of the objective function f on xg ;
tocf − ticf is the elapsed time for calculating the objective
function f .
2) Experiment I: To compare the performance of the
proposed adaptive control mechanism over existing methods,
four existing parameter control methods: 1) linear control [18];
2) nonlinear control [20]; 3) adaptive fuzzy control [25]; and
4) explicit control [26] are applied to PSO-MAM, the results
are then compared with APSO-MAM on 31 test functions
studied in PSO-MAM [9]. Pairwise comparison results using
t-test and Wilcoxon test [49] are summarized in Tables I
and II, respectively. Please note the level of significance

HU et al.: ADAPTIVE PARTICLE SWARM OPTIMIZATION WITH MULTIPLE ADAPTIVE METHODS

711

TABLE IV a)
p-Values Obtained for Bonferroni--Dunn’s, Holm’s, and Hochberg’s Procedures on FV
APSO-MAM v.s.
Nonlinear control
Linear control
Adaptive fuzzy control
Explicit control

z
3.574
3.052
3.012
1.205

Unadjusted p
0.000
0.002
0.003
0.228

Bonferroni–Dunn p
0.001
0.009
0.010
0.913

Holm p
0.001
0.007
0.007
0.228

Hochberg p
0.001
0.005
0.005
0.228

TABLE IVb)
p-Values Obtained for Bonferroni--Dunn’s, Holm’s, and Hochberg’s Procedures on SP
APSO-MAM v.s.
Nonlinear control
Linear control
Adaptive fuzzy control
Explicit control

z
8.032
5.944
4.739
2.169

Unadjusted p
8.882E-16
2.785E-09
2.148E-06
0.030

considered in this paper for all the statistics tests is 0.05.
Values “+,” “=” and “-” in Table I denote that APSO-MAM
performs significantly better than, almost the same as, and
significantly worse than the compared method, respectively. R+
and R− in Table II denote the sum of ranks that APSO-MAM
outperforms and underperforms the compared method. It is
observed from Table I that APSO-MAM performs significantly
better than the other four parameter control methods on at
least 25 out of 31 functions in terms of convergence rate
(SP). Meanwhile, APSO-MAM significantly outperforms the
other four parameter control methods both in terms of solution
quality (FV) and convergence rate (SP) using the Wilcoxon test
in Table II.
Other than the pairwise comparisons, we employ the multiple comparisons studied in [49] to comprehensively evaluate
the effectiveness of our proposed adaptive parameter control
method. The rankings in terms of FV and SP obtained by
Friedman’s test are summarized in Table III. Our proposed
adaptive parameter control method is the best among these
five parameter control methods.
The p-value obtained for Bonferroni–Dunn’s, Holm’s,
and Hochberg’s procedure on FV and SP are recorded in
Tables IV(a) and (b). APSO-MAM significantly outperforms
the first three parameter control methods in terms of FV and
SP. Comparing with the parameter control methods proposed
in [26], APSO-MAM outperforms it in terms of SP by using
the Holm’s and Hochberg’s procedures.
We conclude the proposed adaptive parameter control
method outperforms existing methods and could significantly
improve PSO-MAM’s performance in terms of convergence
rate.
3) Experiment II: To assess the robustness improvements
of APSO-MAM over PSO-MAM, 31 test functions studied
in PSO-MAM [9] are employed as benchmark functions. Ten
different initial parameter settings are tested which are listed
in Table V.
The Friedman’s and Iman-Davenport’s tests [49] are employed to test whether there are global differences between
different parameter settings. Table VI demonstrates that there
are no significant differences for APSO-MAM under different
initial parameter settings in terms of FV.

Bonferroni–Dunn p
3.553E-15
1.114E-08
8.591E-06
0.120

Holm p
3.553E-15
8.354E-09
4.296E-06
0.030

Hochberg p
3.553E-15
8.354E-09
4.296E-06
0.030

TABLE V
Ten Different Initial Parameter Settings (IPS) for Robustness
Testing
No.
IPS1
IPS2
IPS3
IPS4
IPS5
IPS6
IPS7
IPS8
IPS9
IPS10

Parameter Settings
w=0.7298, c1 =c2 = 1.4961
w=0.7298, c1 =2.5, c2 = 0.5
w=0.7298, c1 =0.5, c2 = 2.5
w=0.9, c1 =c2 = 1.4961
w=0.4, c1 =c2 = 1.4961
w = 0.9, c1 = 0.5, c2 = 2.5
w = 0.4, c1 =0.5, c2 = 2.5
w = 0.9, c1 = 2.5, c2 = 0.5
w = 0.4, c1 = 2.5, c2 = 0.5
Random initial

A two-tailed t-test and Wilcoxon signed-rank test at a
0.05 level of significance are employed to test the difference
of APSO-MAM under these 10 different initial parameter
settings, and the number of functions that APSO-MAM performs the same in terms of FV and SP under different initial
parameter settings is recorded. To quantitatively evaluate the
robustness of APSO-MAM, we propose the identical rate (see
Table VII) for each parameter setting which is computed as
the cumulative number of identical functions for this parameter setting compared with the other nine parameter settings
divided by the ideal case which has 279 (31×9) cumulative
identical functions. It is observed that the performances in
terms of FV for APSO-MAM under different initial parameter
settings are almost the same. APSO-MAM also improves the
robustness of PSO-MAM in terms of SP. For SP, it is observed
from the experiments that APSO-MAM performs significantly
different on most of the test functions with SP less than 5000
function evaluations.
In summary, APSO-MAM could improve the robustness of
PSO-MAM and make it insensitive to the initial parameter
settings of PSO-MAM.

IV. Experimental Analysis for APSO-MAM
In Section III, we demonstrate that the proposed adaptive
parameter control method is more effective than the existing

712

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 17, NO. 5, OCTOBER 2013

TABLE VI
Results of the Friedman and Iman-Davenport Tests Under Different Parameter Settings (α = 0.05)

APSOMAM
PSOMAM

FV
SP
FV
SP

Friedman value
7.401
67.788
117.449
178.283

χ2 Value
16.919
16.919
16.919
16.919

p-value
0.595
4.127E-11
0.000E+00
0.000E+00

Iman-Davenport value
0.817
9.628
21.810
53.104

FF value
1.915
1.915
1.915
1.915

p-value
0.601
9.571E-13
0.000E+00
0.000E+00

TABLE VII
Identical Rate for APSO-MAM and PSO-MAM Under Different Parameter Settings

FV
APSOMAM

SP
FV

PSOMAM

SP

t-test
Wilcoxon
t-test
Wilcoxon
t-test
Wilcoxon
t-test
Wilcoxon

IPS1
0.996
0.978
0.832
0.778
0.674
0.534
0.534
0.373

IPS2
0.996
0.982
0.842
0.785
0.659
0.505
0.559
0.380

IPS3
0.996
0.989
0.839
0.760
0.699
0.541
0.570
0.369

parameter control methods, and APSO-MAM could improve
the robustness of PSO-MAM. In this section, we first compare
the performance of APSO-MAM with 10 state-of-the-art PSO
algorithms and PSO-MAM on 31 test functions collected from
[9] to demonstrate that APSO-MAM performs well on a diverse set of problems with different properties (e.g., unimodal,
multimodal, shifted, rotated, noisy, mis-scaled). Secondly, we
demonstrate the effectiveness of APSO-MAM by comparing
the performance of APSO-MAM with nine state-of-the-art
evolutionary algorithms on six benchmark functions from CEC
2008 [50] with dimension 100, 500, and 1000.
A. Experimental Comparison with Other PSO Algorithms
The parameter settings of the 10 state-of-the-art PSO algorithms and PSO-MAM are provided in [9]. The 31 test
functions studied in [9] are divided into six groups:
1) six unimodal nonrotated functions (scaled and nonnoisy);
2) six unimodal rotated functions (scaled and non-noisy);
3) eleven multimodal nonrotated functions (scaled and nonnoisy);
4) four multimodal rotated functions (scaled and nonnoisy);
5) two noisy functions;
6) two mis-scaled functions.
In the experiments, the population size is set to be 30 and the
maximum number of function evaluations is set to be 300 000.
For all test functions, the algorithms carry out 30 independent
runs. To evaluate the overall performance in regard to both
the solution quality and computing cost, we adopt the same
metrics discussed in Section III-C: FV, SP, and SR from [48].
The statistical comparison of the APSO-MAM with the
other 11 PSO algorithms uses a two-tailed t-test with 58
degrees of freedom at a 0.05 level of significance. Values
“+,” “=,” and “-” in column “h” in Tables VIII(a)–(f) denote
that APSO-MAM performs significantly better than, almost the
same as, and significantly worse than the compared algorithm,

IPS4
0.996
0.986
0.842
0.792
0.502
0.179
0.462
0.276

IPS5
0.996
0.978
0.767
0.706
0.692
0.552
0.556
0.330

IPS6
0.968
0.957
0.814
0.756
0.401
0.233
0.659
0.308

IPS7
0.996
0.989
0.749
0.681
0.663
0.556
0.530
0.323

IPS8
0.996
0.986
0.724
0.631
0.423
0.229
0.667
0.315

IPS9
0.996
0.986
0.774
0.724
0.538
0.416
0.591
0.369

IPS10
0.996
0.982
0.860
0.814
0.785
0.577
0.692
0.462

respectively. The first value in column h is the t-test result on
the FV and the second value is on the SP.
1) Unimodal Nonrotated Functions: In the first set of
experiments, six unimodal and nonrotated functions are studied. The optimization results are summarized in Table VIII(a).
Please note column SP is blank when there are no successful
runs among the 30 runs (SR = 0).
APSO-MAM outperforms the other 10 algorithms on all of
the six functions in terms of solution quality and convergence
rate (SP). Comparing APSO-MAM with PSO-MAM, its performances in terms of solution quality and convergence rate
are almost the same as PSO-MAM, and APSO-MAM could
improve the convergence rate for shifted Schwefel P2.22 (f2 )
function.
2) Unimodal Rotated Functions: In the second set of
experiments, six unimodal rotated functions are tested. The
optimization results are summarized in Table VIII(b).
APSO-MAM outperforms the other 11 algorithms on all six
functions in terms of solution quality and/or convergence rate.
APSO-MAM and PSO-MAM are the most reliable algorithms
(large SR) among these 12 algorithms.
3) Multimodal Nonrotated Functions: In the third set of
experiments, 11 multimodal nonrotated functions are explored.
The optimization results are summarized in Table VIII(c).
APSO-MAM outperforms in terms of solution quality and
convergence rate for 9 out of 11 multimodal nonrotated functions except functions Weierstrass (f19 ) and Schwefel P2.13
(f21 ). Comparing APSO-MAM with PSO-MAM, APSOMAM could improve the convergence rate for 10 out of 11
functions, and APSO-MAM could significantly improve PSOMAM’s performance on function Weierstrass (f19 ).
4) Multimodal Rotated Functions: In the fourth set of
experiments, four multimodal rotated functions are studied.
The optimization results are summarized in Table VIII(d).
Comparing APSO-MAM with PSO-MAM, APSO-MAM outperforms PSO-MAM on these four multimodal rotated functions in terms of solution quality and/or convergence speed.
However, the improvements on rotated 2D minima (f24 ) and

HU et al.: ADAPTIVE PARTICLE SWARM OPTIMIZATION WITH MULTIPLE ADAPTIVE METHODS

713

TABLE VIII a)
Optimization Results for Unimodal Nonrotated Functions
Algorithms
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM

FV

SP
SR (%)
Shifted Sphere (f1 )
2.21E-28
176 162
100
1.77E-27
14 517
100
2.02E-27
2 19 943
100
6.84E-30
24 389
100
15708
100
0.00E+00
5.21E-27
79 319
100
1.26E-30
99 527
100
2.42E-12
95 478
100
92 887
100
0.00E+00
7.15E-30
24 082
100
0.00E+00
1810
100
0.00E+00
1505
100
Shifted Schwefel P2.21 (f4 )
2.86E-01
0
3.08E-10
167 606
100
1.18E+00
0
2.16E-09
186 927
100
1.54E-05
571 530
50
4.97E-05
0
4.11E-04
0
7.98E-05
0
5.35E-01
0
9.53E-12
154 354
100
2.37E-16
5913
100
3967
100
0.00E+00

h
+/+
=/+
=/+
+/+
=/+
+/+
=/+
+/+
=/+
=/+
=/+

+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
=/+

FV

SP
SR (%)
Shifted Schwefel P2.22 (f2 )
9.55E-16
176 346
100
1.41E-13
28 835
100
5.63E-16
221 545
100
0.00E+00
31 038
100
0.00E+00
20 956
100
2.46E-14
107719
100
0.00E+00
101 755
100
2.71E-07
201064
100
0.00E+00
107 976
100
0.00E+00
30 186
100
0.00E+00
26 996
100
0.00E+00
18138
100
Shifted Rosenbrock (f5 )
4.97E+01
0
9.74E+00
0
6.42E+01
0
1.38E+01
0
1.13E+01
0
2.85E+01
0
1.46E+00
8 975 940
3.33
2.95E+01
0
3.71E+00
0
2.80E+01
0
6.62E-28
54 025
100
6.41E-28
40 541
100

h
+/+
=/+
+/+
=/+
=/+
+/+
=/+
+/+
=/+
=/+
=/+

+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
=/=

FV
SP
SR (%)
Shifted Schwefel P1.2 (f3 )
2.47E-02
0
1.73E-22
103 109
100
2.80E+03
0
1.16E-09
198 400
100
5.47E-11
183693
100
1.91E+00
0
2.55E-18
186 429
100
4.96E+03
0
2.17E+02
0
1.10E+00
0
4.49E-27
48 343
100
9.09E-27
35 094
100
Shifted Step (f6 )
1.00E-01
186 372
90
1.56E+01
0
0.00E+00 191 646
100
0.00E+00
12 408
100
0.00E+00
8943
100
0.00E+00
33 422
100
3.00E-01
169 571
76.7
0.00E+00
8709
100
0.00E+00
55 895
100
0.00E+00
11 696
100
0.00E+00
834
100
0.00E+00
767
100

h
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
-/+

=/+
+/+
=/+
=/+
=/+
=/+
+/+
=/+
=/+
=/+
=/=

TABLE VIII b)
Optimization Results for Unimodal Rotated Functions
Algorithms
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM

FV
SP
SR (%)
Shifted Rotated Sphere (f7 )
1.96E-28
176 633
100
6.40E-29
14 618
100
2.46E-27
220 891
100
5.05E-30
24 307
100
8.41E-31
15 833
100
4.98E-27
79 141
100
0.00E+00
99 737
100
3.05E-12
93 277
100
0.00E+00
93 487
100
7.15E-30
24 111
100
0.00E+00
1870
100
0.00E+00
1628
100
Shifted Rotated Tablet (f10 )
4.47E+02
0
2.87E+01
0
1.89E+03
0
2.87E+02
0
1.06E+03
0
1.21E+03
0
2.20E+02
0
1.52E+04
0
4.68E+02
0
3.93E+01
0
0.00E+00
2759
100
0.00E+00
2385
100

h
+/+
+/+
+/+
+/+
=/+
+/+
=/+
+/+
=/+
=/+
=/+

+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
=/+

FV
SP
SR (%)
h
Shifted Rotated Schwefel P2.21 (f8 )
4.72E-03
0
+/+
5.87E-13
128 094
100
=/+
1.45E-01
0
+/+
4.28E-13
135 275
100
+/+
4.10E-08
196 394
100
=/+
1.16E-06
262 428
100
+/+
8.49E-07
237174
96.7
=/+
5.52E+00
0
=/+
1.05E-01
0
+/+
1.03E-12
143 619
100
+/+
0.00E+00
5783
100
=/+
0.00E+00
4012
100
Shifted Rotated Ellipse (f11 )
2.49E-04
2 990 170
10
=/+
3.39E-25
62 506
100
+/+
4.87E+04
0
+/+
2.43E-17
129 467
100
+/+
1.01E-21
108 496
100
+/+
8.83E-03
0
+/+
1.22E-25
163 049
100
+/+
4.53E+03
0
+/+
1.08E+02
0
+/+
4.15E-10
2 12 431
100
+/+
3.44E-26
23 724
100
=/+
3.77E-26
16967
100

FV
SP
SR (%)
h
Shifted Rotated Rosenbrock (f9 )
6.26E+02
0
+/+
4.94E+02
0
=/+
4.23E+02
0
+/+
8.94E+01
0
+/+
3.95E+01
0
+/+
5.67E+01
0
+/+
2.40E+01
0
+/+
2.54E+02
0
=/+
2.99E+01
0
+/+
4.35E+01
0
+/+
3.99E-01
87119
90
=/=
3.99E-01
72499
90
Shifted Rotated Diff Power (f12 )
7.78E-06
2 29 806
96.7
=/+
2.18E-13
26 680
100
+/+
1.95E+06
8 64 481
33.3
+/+
4.92E-13
39 895
100
+/+
1.19E-13
30 406
100
+/+
8.44E-11
102 229
100
+/+
3.91E-14
1 033 30
100
+/+
1.13E+07
1 043 833
23.3
=/+
7.62E-09
173 461
100
+/+
4.03E-12
45 216
100
+/+
9.07E-16
4981
100
+/+
4.63E-16
3889
100

714

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 17, NO. 5, OCTOBER 2013

TABLE VIII c)
Optimization Results For Multimodal Nonrotated Functions
Algorithms
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM

FV

SP
SR (%)
h
Schwefel (f13 )
1.09E+03
0
+/+
2.92E+03
0
+/+
5.16E+03
0
+/+
2.18E+03
0
+/+
3.65E+03
0
+/+
2.37E+01
220 819
80
+/+
3.10E+03
0
+/+
2.37E+02
8 783 760
3.33
+/+
95 772
100
=/+
1.70E-08
2.43E+03
0
+/+
1.70E-08
64 795
100
=/=
1.70E-08
55 927
100
Shifted Noncontinuous Rastrigin (f16 )
7.57E+00
2 968 630
10
+/+
4.08E+01
0
+/+
1.66E+01
0
+/+
6.07E+00
715 855
36.7
+/+
7.09E+01
0
+/+
4.16E+01
0
+/+
7.63E+00
0
+/+
2.33E-01
158 412
80
+/+
167 351
100
=/+
0.00E+00
3.80E+00
3 77 190
63. 3
+/+
0.00E+00
58 421
100
=/=
55 604
100
0.00E+00
Weierstrass (f19 )
1.01E-01
3 08 081
73.3
=/+
7.61E+00
0
+/+
2 38 863
100
−/+
0.00E+00
7.27E-01
8 74 227
26.7
+/+
4.22E-01
6 35 110
33.3
+/+
0.00E+00
167 853
100
−/+
1.07E-01
9 50 523
26.7
=/+
9.88E-05
0
+/+
0.00E+00
142 057
100
−/+
0.00E+00
48 357
100
−/−
6.29E-05
5 58 522
43.3
+/+
9.00E-15
91 644
100
Shifted Penalized 1 (f22 )
2.07E-02
2 49 245
80
+/+
3.74E-01
361 848
46.7
+/+
6.91E-03
2 46 538
93.3
=/+
1.21E-01
2 24 849
60
+/+
2.76E-02
86 255
83.3
=/+
1.08E-29
61 000
100
+/+
3.46E-03
99 463
96.7
=/+
1.98E-14
38 407
100
=/+
1.57E-32
83 945
100
=/+
1.57E-32
21 791
100
=/+
15 011
100
=/=
1.57E-32
1.57E-32
13 507
100

FV

SP
SR (%)
2D minima (f14 )
4.71E+00
0
1.12E+01
0
4.57E-10
2 00 131
100
7.01E+00
0
8.07E+00
0
4.08E-01
2 69 664
60
1.06E+01
0
4.57E-10
37 136
100
4.57E-10
75 820
100
3.61E+00
4 228 670
6.67
4.57E-10
17 364
100
4.57E-10
15813
100
Shifted Ackley (f17 )
3.10E-14
192348
100
1.53E+00
1 222 345
20
2.34E-14
2 39 791
100
6.39E-15
34 510
100
3.55E-15
22997
100
2.52E-14
117 343
100
1.85E-14
109 428
100
3.03E-07
199 111
100
8.05E-15
118 336
100
3.55E-15
34 502
100
0.00E+00
20 436
100
0.00E+00
20 012
100
Shifted Salomon (f20 )
3.81E-01
0
5.90E-01
0
3.13E-01
0
2.33E-01
0
5.37E-01
0
2.00E-01
0
3.50E-01
0
1.37E+00
0
2.32E-01
0
2.00E-01
0
0.00E+00
11 498
100
0.00E+00
10 624
100
Shifted Penalized 2 (f23 )
2.20E-03
2 57 914
80
2.76E-01
3 17 006
50
1.83E-03
301 082
83.3
2.17E-03
84 526
83.3
7.32E-04
39 932
93.3
3.42E-28
69 452
100
1.10E-03
124 916
90
1.61E-13
60 603
100
1.35E-32
89 144
100
5.56E-32
24 968
100
1.35E-32
13 510
100
1.35E-32
13 035
100

h
+/+
+/+
=/+
+/+
+/+
+/+
+/+
=/+
=/+
+/+
=/=

+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
=/=

+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
=/=

+/+
=/+
+/+
+/+
=/=
+/+
=/+
+/+
=/+
=/+
=/=

FV

SP
SR (%)
Shifted Rastrigin (f15 )
2.01E+01
0
7.26E+01
0
2.93E+01
0
4.23E+01
0
6.87E+01
0
2.80E+01
0
2.84E+01
0
9.95E-02
117 335
90
0.00E+00
159 419
100
7.16E+00
0
0.00E+00
64 024
100
0.00E+00
60842
100
Shifted Griewank (f18 )
2.12E-02
1 004 104
26.7
2.44E-02
7 15 564
30
5.91E-03
4 42 035
56.7
8.19E-03
257348
56.7
1.89E-03
85077
83.3
0.00E+00
98 894
100
1.26E-02
6 25 812
36.7
2.07E-02
621 219
36.7
0.00E+00
121255
100
0.00E+00
29 281
100
0.00E+00
3905
100
0.00E+00
3000
100
Schwefel P2.13 (f21 )
8.65E+04
0
1.61E+05
0
7.02E+04
0
6.26E+04
0
1.09E+05
0
1.22E+04
0
1.38E+04
0
2.74E+04
0
1.01E+04
0
1.01E+04
0
1.01E+04
8 789 090
3.33
9.92E+03
0

h
+/+
+/+
+/+
+/+
+/+
+/+
+/+
=/+
=/+
+/+
=/=

+/+
+/+
+/+
+/+
+/+
=/+
+/+
+/+
=/+
=/+
=/+

+/=
+/=
+/=
+/=
+/=
=/=
=/=
+/=
=/=
=/=
=/−

HU et al.: ADAPTIVE PARTICLE SWARM OPTIMIZATION WITH MULTIPLE ADAPTIVE METHODS

715

TABLE VIII d)
Optimization Results for Multimodal Rotated Functions
Algorithms
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM

FV

SP
SR (%)
Rotated 2D minima (f24 )
6.71E+00
0
1.02E+01
0
3.75E+00
0
9.19E+00
0
9.75E+00
0
2.08E+00
0
1.10E+01
0
6.81E+00
0
1.89E+00
0
6.20E+00
0
7.74E+00
0
7.20E+00
0
Rotated Weierstrass (f26 )
7.33E+00
0
1.25E+01
0
2.76E+00
0
7.40E+00
0
1.48E+01
0
6.31E-02
421625
60
2.44E+00
0
1.24E+01
0
1.37E+00
0
1.17E-01
0
9.53E+00
0
9.52E+00
0

h
=/=
+/=
−/=
+/=
+/=
−/=
+/=
=/=
−/=
=/=
=/=

−/=
+/=
−/=
−/=
+/=
−/−
−/=
+/=
−/=
−/=
=/=

FV
SP
SR (%)
Shifted Rotated Griewank (f25 )
1.32E-02
10 12 572
26.7
1.86E-02
4 08 508
43.3
7.71E-03
4 83 820
53.3
6.65E-03
3 27 566
50
3.38E-03
154 359
70
0.00E+00
99049
100
1.34E-02
7 08 387
33.3
3.42E-02
8 10 937
30
4.20E-10
143 041
100
6.57E-04
58 795
93.3
4067
100
0.00E+00
0.00E+00
3447
100
Shifted Rotated Salomon (f27 )
3.93E-01
0
5.97E-01
0
3.01E-01
0
2.53E-01
0
5.47E-01
0
1.97E-01
0
3.43E-01
0
1.30E+00
0
2.24E-01
0
1.97E-01
0
0.00E+00
11 234
100
10 338
0.00E+00
100

h
+/+
+/+
+/+
+/+
+/+
=/+
+/+
+/+
=/+
=/+
=/=

+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
=/=

TABLE VIII e)
Optimization Results for Noisy Functions
Algorithms
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM

FV
SP
SR (%)
h
Shifted Noise Schwefel P1.2 (f28 )
1.20E+03
0
+/=
7.43E+02
0
+/=
1.38E+03
0
+/=
1.37E+02
0
=/=
2.69E+03
0
+/=
2.09E+02
0
=/=
2.52E+02
0
=/=
1.76E+04
0
+/=
2.09E+03
0
+/=
2.34E+02
0
=/=
8.95E+01 1 253 912
23.3
=/−
2.08E+02

rotated Weierstrass (f26 ) are very small, and the performance
of APSO-MAM on these two complex functions is still inferior
to other algorithms designed for the specific search space
(multimodal rotated space).
5) Noisy Functions: In the fifth set of experiments, two
noisy functions are tested. The optimization results are summarized in Table VIII(e).
Unlike other groups of functions, APSO-MAM is inferior
to PSO-MAM on these two noisy functions. The parameter
control mechanism accelerates the convergence speed of the
particles to the global best solution which may be a bad
particle among the swarm without considering noise.

FV
SP
SR (%)
Shifted Rotated Noise Quadric
9.24E-03
0
3.90E-03
0
2.07E-02
0
2.37E-03
0
1.89E-02
0
2.68E-03
0
7.07E-03
0
1.45E-02
0
4.15E-03
0
3.84E-03
0
3.53E-04
0
4.98E-04
0

h
(f29 )
+/=
+/=
+/=
+/=
+/=
+/=
+/=
+/=
+/=
+/=
=/=

6) Mis-Scaled Functions: Two mis-scaled functions are
studied in this set of experiments. The optimization results
are summarized in Table VIII(f).
APSO-MAM outperforms the other 11 algorithms in terms
of convergence rate. Comparing APSO-MAM with PSOMAM, it could significantly improve PSO-MAM’s performance in terms of convergence rate on function Shifted
Rastrigin100 (f31 ).
7) Statistical Comparison Between APSO-MAM and 11
PSO Algorithms: We observe the proposed APSO-MAM in
general outperforms the other PSO algorithms on most of
the test functions. APSO-MAM could improve PSO-MAM’s

716

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 17, NO. 5, OCTOBER 2013

TABLE VIII f)
Optimization Results for Mis-Scaled Functions
Algorithms

FV

SP
SR (%)
Shifted Rastrigin10 (f30 )
2.89E+01
0
1.06E+02
0
3.92E+01
0
5.47E+01
0
8.91E+01
0
5.32E+01
0
3.61E+01
0
1.33E-01
123 673
96.7
0.00E+00
176 403
100
9.65E+00
8 914 706
3.33
0.00E+00
65 546
100
0.00E+00
63 381
100

PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM

h
+/+
+/+
+/+
+/+
+/+
+/+
+/+
=/+
=/+
+/+
=/=

FV

SP
SR (%)
Shifted Rastrigin100 (f31 )
2.58E+01
0
1.46E+02
0
3.93E+01
0
6.40E+01
0
1.14E+02
0
5.27E+01
0
5.18E+01
0
2.32E-01
2 43 602
76. 7
0.00E+00
176 792
100
1.37E+01
0
1.84E-15
105 771
100
5.68E-15
80 461
100

h
+/+
+/+
+/+
+/+
+/+
+/+
+/+
+/+
−/+
+/+
=/+

TABLE IX
t-test Comparison Between APSO-MAM and Other 11 Algorithms
Algorithms
Metrics
FV

SP

+ (Better)
= (Same)
− (Worse)
+ (Better)
= (Same)
− (Worse)

PSO
-w
25
5
1
26
5
0

PSO
-cf
26
5
0
26
5
0

PSO-w
-local
24
4
3
26
5
0

PSO-cf
-local
27
3
1
26
5
0

UPS
O
24
7
0
25
6
0

wFI
PS
23
5
3
26
4
1

FDR
PSO
21
9
1
26
5
0

CPS
O-H
22
9
0
26
5
0

CLP
SO
13
14
4
26
5
0

TABLE X
Wilcoxon Test Between APSO-MAM and Other 11 Algorithms
APSO-MAM
v.s.
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM

R+
457
496
453.5
447.5
493
442
473.5
476.5
380
441
276

FV
R−
39
0
42.5
48.5
3
54
22.5
19.5
116
55
220

p-value
4.209E-05
1.174E-06
7.939E-05
1.611E-04
3.790E-06
2.462E-04
1.769E-05
1.318E-05
7.908E-03
2.857E-04
4.802E-01

R+
488.5
488.5
488.5
488.5
488.5
472
488.5
488.5
488.5
473.5
432

SP
R−
7.5
7.5
7.5
7.5
7.5
24
7.5
7.5
7.5
22.5
64

TABLE XI
Rankings Obtained Through Friedman’s Test
Algorithms
PSO-w
PSO-cf
PSO-w-local
PSO-cf-local
UPSO
wFIPS
FDR-PSO
CPSO-H
CLPSO
DMS-PSO
PSO-MAM
APSO-MAM

FV Ranking
8.484
9.226
8.565
6.871
7.935
6.435
6.887
8.774
4.710
4.919
2.629
2.565

SP Ranking
8.823
7.468
8.839
6.968
6.355
6.984
7.952
7.306
6.871
5.887
2.581
1.968

p-value
8.298E-06
8.298E-06
8.298E-06
8.298E-06
8.298E-06
2.911E-05
8.298E-06
8.298E-06
8.298E-06
2.630E-05
7.512E-04

DMSPSO
18
11
2
25
5
1

PSOMAM
2
28
1
12
17
2

HU et al.: ADAPTIVE PARTICLE SWARM OPTIMIZATION WITH MULTIPLE ADAPTIVE METHODS

717

TABLE XII a)
p-Values Obtained for Bonferroni--Dunn’s, Holm’s, and Hochberg’s Procedures on FV
APSO-MAM v.s.
PSO-cf
CPSO-H
PSO-w-local
PSO-w
UPSO
FDR-PSO
PSO-cf-local
wFIPS
DMS-PSO
CLPSO
PSO-MAM

z
7.274
6.781
6.552
6.464
5.865
4.720
4.702
4.227
2.571
2.342
0.070

Unadjusted p
3.499E-13
1.197E-11
5.694E-11
1.023E-10
4.499E-09
2.359E-06
2.572E-06
2.370E-05
0.010
0.019
0.944

Bonferroni–Dunn p
3.849E-12
1.317E-10
6.263E-10
1.125E-09
4.949E-08
2.595E-05
2.829E-05
2.607E-04
0.111
0.211
1.000

Holm p
3.849E-12
1.197E-10
5.124E-10
8.184E-10
3.149E-08
1.415E-05
1.415E-05
9.481E-05
0.030
0.038
0.944

Hochberg p
3.849E-12
1.197E-10
5.124E-10
8.184E-10
3.149E-08
1.286E-05
1.286E-05
9.481E-05
0.030
0.038
0.944

TABLE XII b)
p-values Obtained for Bonferroni--Dunn’s, Holm’s, and Hochberg’s Procedures on SP
APSO-MAM v.s.
PSO-w-local
PSO-w
FDR-PSO
PSO-cf
CPSO-H
wFIPS
PSO-cf-local
CLPSO
UPSO
DMS-PSO
PSO-MAM

z
7.503
7.485
6.534
6.006
5.829
5.477
5.460
5.354
4.790
4.280
0.669

Unadjusted p
6.262E-14
7.150E-14
6.405E-11
1.906E-09
5.560E-09
4.320E-08
4.771E-08
8.604E-08
1.665E-06
1.872E-05
0.503

Bonferroni–Dunn p
6.888E-13
7.865E-13
7.046E-10
2.097E-08
6.116E-08
4.752E-07
5.248E-07
9.465E-07
1.831E-05
2.059E-04
1.000

Holm p
6.888E-13
7.150E-13
5.765E-10
1.525E-08
3.892E-08
2.592E-07
2.592E-07
3.442E-07
4.994E-06
3.744E-05
0.503

Hochberg p
6.888E-13
7.150E-13
5.765E-10
1.525E-08
3.892E-08
2.385E-07
2.385E-07
3.442E-07
4.994E-06
3.744E-05
0.503

TABLE XIII
Algorithms
DMS-L-PSO
DEwSAcc
MLCC
LSEDA-gl
MTS
EPUS-PSO
jDEdynNP-F
CCPSO2
sep-CMA-ES
APSO-MAM
DMS-L-PSO
DEwSAcc
MLCC
LSEDA-gl
MTS
EPUS-PSO
jDEdynNP-F
CCPSO2
sep-CMA-ES
APSO-MAM
DMS-L-PSO
DEwSAcc
MLCC
LSEDA-gl
MTS
EPUS-PSO
jDEdynNP-F
CCPSO2
sep-CMA-ES
APSO-MAM

Fitness Value for CEC 2008 Benchmark Functions
FV
f1 (100D)
f2 (100D)
f3 (100D)
f4 (100D)
f5 (100D)
0.0000E+00
3.6452E+00
2.8301E+02
1.8294E+02
0.0000E+00
5.6843E-14
8.2500E+00
1.4463E+02
4.3778E+00
3.0695E-14
6.8212E-14
2.5262E+01
1.4984E+02
4.3883E-13
3.4106E-14
5.6843E-14
2.2055E-13
2.8121E+02
1.0353E+02
2.8422E-14
0.0000E+00
1.4406E-11
5.1707E-08
0.0000E+00
0.0000E+00
7.4700E-01
1.8600E+01
4.9900E+03
4.7100E+02
3.7200E-01
5.6843E-14
4.2875E-01
1.1158E+02
5.4570E-14
2.8422E-14
7.7300E-14
6.0800E+00
4.2300E+02
3.9800E-02
3.4500E-03
9.0200E-15
2.3100E+01
4.3100E+00
2.7800E+02
2.9600E-04
0.0000E+00
6.8212E-15
2.2919E+01
0.0000E+00
1.8190E-14
f1 (500D)
f2 (500D)
f3 (500D)
f4 (500D)
f5 (500D)
0.0000E+00
6.8934E+01
4.6712E+07
1.6088E+03
0.0000E+00
2.0958E-09
7.5737E+01
1.8130E+03
3.6403E+02
6.9013E-04
4.2974E-13
6.6663E+01
9.2466E+02
1.7933E-11
2.1259E-13
2.2737E-13
2.7152E-10
8.6745E+02
8.5569E+02
1.1369E-13
0.0000E+00
7.3194E-06
5.0366E-03
0.0000E+00
0.0000E+00
8.4500E+01
4.3500E+01
5.7700E+04
3.4900E+03
1.6400E+00
9.3223E-14
8.4648E+00
6.6115E+02
1.4688E-12
4.2064E-14
3.0000E-13
5.7900E+01
7.2400E+02
3.9800E-02
1.1800E-03
2.2500E-14
2.1200E+02
2.9300E+02
2.1800E+03
7.8800E-04
0.0000E+00
6.8212E-15
7.9581E-14
0.0000E+00
4.2064E-14
f1 (1000D)
f2 (1000D)
f3 (1000D)
f4 (1000D)
f5 (1000D)
0.0000E+00
9.1519E+01
8.9849E+09
3.8399E+03
0.0000E+00
8.7874E-03
9.6058E+01
9.1498E+03
1.8239E+03
3.5826E-03
8.4583E-13
1.0871E+02
1.7986E+03
1.3744E-10
4.1837E-13
3.2211E-13
1.0394E-05
1.7288E+03
5.4493E+02
1.7053E-13
0.0000E+00
4.7239E-02
3.4060E-04
0.0000E+00
0.0000E+00
5.5300E+02
4.6600E+01
8.3700E+05
7.5800E+03
5.8900E+00
1.1369E-13
1.9529E+01
1.3136E+03
2.1668E-04
3.9790E-14
5.1800E-13
7.8200E+01
1.3300E+03
1.9900E-01
1.1800E-03
7.8100E-15
3.6500E+02
9.1000E+02
5.3100E+03
3.9400E-04
0.0000E+00
1.3642E-14
7.2760E-14
3.9798E-02
7.7307E-14

f6 (100D)
0.0000E+00
1.1255E-13
1.1141E-13
9.7771E-14
0.0000E+00
2.0600E+00
5.6843E-14
1.4400E-13
2.1200E+01
0.0000E+00
f6 (500D)
2.0022E+00
4.8041E-01
5.3433E-13
3.1264E-13
6.1812E-12
6.6400E+00
1.4893E-13
5.3400E-13
2.1500E+01
0.0000E+00
f6 (1000D)
7.7566E+00
2.2956E+00
1.0607E-12
4.2633E-13
1.2378E-11
1.8900E+01
1.4687E-11
1.0200E-12
2.1500E+01
0.0000E+00

718

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 17, NO. 5, OCTOBER 2013

TABLE XIV
t-Test Comparison Between APSO-MAM and Other Nine Algorithms
Algorithms
t-test
+ (Better)
= (Same)
− (Worse)

DMS-LPSO
10
5
3

DEwSA
cc
17
1
0

MLCC

LSEDA
-gl
18
0
0

16
2
0

MTS
6
9
3

EPUSPSO
18
0
0

jDEdyn
NP-F
15
3
0

CCPSO
2
13
5
0

sep-CM
A-ES
14
4
0

TABLE XV
Wilcoxon Test Between APSO-MAM and Other Nine Algorithms
APSO-MAM
v.s.
R+
R−
p-value

DMS-L-PS
O
148
23
0.004

DEwSA
cc
171
0
0.000

MLCC

LSEDAgl
171
0
0.000

159
12
0.001

MTS
102.5
68.5
0.388

EPUS-P
SO
171
0
0.000

jDEdynN
P-F
155.5
15.5
0.003

CCPS
O2
171
0
0.000

sep-CM
A-ES
164
7
0.001

TABLE XVI
Rankings Obtained Through Friedman’s Test
Algorithms
Ranking

DMS-LPSO
5.583

DEw
SAcc
7.278

MLCC
6.333

LSED
A-gl
4.861

MTS
2.306

EPUSPSO
9.056

jDEdy
nNP-F
4.000

CCPS
O2
6.444

sep-CM
A-ES
7.167

APSOMAM
1.972

TABLE XVII
p-Values Obtained for Bonferroni--Dunn’s, Holm’s, and Hochberg’s Procedures on F V
APSO-MAM v.s.
EPUS-PSO
DEwSAcc
sep-CMA-ES
CCPSO2
MLCC
DMS-L-PSO
LSEDA-gl
jDEdynNP-F
MTS

z
7.019
5.257
5.147
4.431
4.321
3.578
2.863
2.009
0.330

Unadjusted p
2.240E-12
1.463E-07
2.647E-07
9.363E-06
1.551E-05
3.461E-04
0.004
0.045
0.741

performance in terms of solution quality and/or convergence
rate for most of the test functions, especially when the gBest
is a good solution, not a local optimum. The performance of
the proposed APSO-MAM may not be very good when the
gBest is a local optimum or the gBest is not a good solution.
For example, in our experiments, we have demonstrated that
the performance of our proposed method on noisy functions is
less than satisfactory. Pairwise comparisons results using t-test
and Wilcoxon test [49] are summarized in Tables IX and X,
respectively. From Table IX, it is observed that APSO-MAM
is better for improvement on convergence rate than solution
quality. Comparing APSO-MAM with CLPSO and DMSPSO, APSO-MAM’s performance on rotated multimodal still
needs to be improved. Although APSO-MAM improves PSOMAM’s performance in terms of convergence rate on 12 out
of 31 functions, its performance on noisy functions is worse
than PSO-MAM. Meanwhile, APSO-MAM significantly outperforms 10 out of 11 algorithms in terms of solution quality
and significantly outperforms the 11 algorithms in terms of
convergence rate using the Wilcoxon test in Table X.
Other than the pairwise comparisons, we employ the multiple comparisons studied in [49] to comprehensively evaluate
the effectiveness of APSO-MAM. The rankings in terms of

Bonferroni–Dunn p
2.016E-11
1.317E-06
2.382E-06
8.427E-05
1.396E-04
3.115E-03
0.038
0.401
1.000

Holm p
2.016E-11
1.171E-06
1.853E-06
5.618E-05
7.756E-05
1.384E-03
0.013
0.089
0.741

Hochberg p
2.016E-11
1.171E-06
1.853E-06
5.618E-05
7.756E-05
1.384E-03
0.013
0.089
0.741

FV and SP obtained by Friedman’s test are summarized in
Table XI. It is observed that APSO-MAM is the most effective
algorithm among these 12 algorithms.
The p-value obtained for Bonferroni–Dunn’s, Holm’s, and
Hochberg’s procedure on FV and SP are recorded in Tables XII(a) and (b). APSO-MAM significantly outperforms
the other 10 algorithms in terms of convergence rate using
the three procedures, and significantly outperforms most of the
other 10 algorithms in terms of solution quality using the three
procedures except DMS-PSO and CLPSO using Bonferroni–
Dunn procedure.
In summary, APSO-MAM performs significantly better than
the other 10 PSO algorithms in terms of solution quality and
convergence rate. Comparing APSO-MAM with PSO-MAM,
APSO-MAM could improve the performance of PSO-MAM
in terms of solution quality and/or convergence rate on most
of the test functions except noisy functions.
B. Experimental Comparison With Other Evolutionary Algorithms
In these experiments, the population size is set to be 30
and the maximum number of function evaluations is set to be
5000D where D is the dimension of the benchmark functions.

HU et al.: ADAPTIVE PARTICLE SWARM OPTIMIZATION WITH MULTIPLE ADAPTIVE METHODS

For all test functions, the algorithms carry out 25 independent runs. Since the SP and SR are not available for these
nine state-of-the-art evolutionary algorithms, we compare the
performance of each algorithm by using the FV. The FVs for
the nine evolutionary algorithms and APSO-MAM on the six
benchmark functions from CEC 2008 [50] are recorded in
Table XIII.
Pairwise comparisons results using t-test and Wilcoxon test
[49] are summarized in Tables XIV and XV, respectively. From
Table XIV, it is observed that APSO-MAM is better than
the nine algorithms. Meanwhile, APSO-MAM significantly
outperforms the eight out of nine algorithms, and outperforms
MTS in terms of solution quality using the Wilcoxon test in
Table XV.
Other than the pairwise comparisons, we employ the multiple comparisons studied in [49] to comprehensively evaluate
the effectiveness of APSO-MAM. The rankings in terms of FV
obtained by Friedman’s test are summarized in Table XVI. It
is observed that APSO-MAM is the most effective algorithm
among these 10 algorithms.
The p-value obtained for Bonferroni–Dunn’s, Holm’s, and
Hochberg’s procedure on FV are recorded in Table XVII.
APSO-MAM significantly outperforms seven out of nine
algorithms by using the three procedures. For the other
two algorithms, APSO-MAM outperforms jDEdynNP-F and
MTS.
V. Conclusion
Most recently, an intelligent augmented particle swarm
optimization with multiple adaptive methods (PSO-MAM)
was proposed, which was demonstrated to be effective for
diverse functions with different properties. However, like
most of the PSO algorithms, the performance of PSOMAM still depends on the settings of the three parameters.
In this paper, we proposed an adaptive parameter control
mechanism to change the three parameters in PSO, and
attempted to pick one randomly selected particle close to
the gBest if the gBest is updated at each iteration. The
experiments conducted in this paper demonstrated that the
enhanced APSO-MAM outperformed 10 state-of-the-art PSO
algorithms, PSO-MAM, and nine state-of-the-art evolutionary algorithms. In addition, the proposed APSO-MAM was
more robust to the settings of the three parameters than
PSO-MAM.
Due to the stochastic elements in noisy functions, APSOMAM may pick the particle close to a pseudo gBest and
underperform PSO-MAM on the noisy function. Therefore,
the parameter control mechanism should be more intelligent
to avoid this issue. The parameter control mechanism could
be implemented in other PSO algorithms (e.g, CLPSO) to
improve their performance and robustness. Although APSOMAM outperforms PSO-MAM on multimodal rotated functions, its performance could still be improved when comparing
it with CLPSO and DMS-PSO.
Some complex engineering problems such as building energy system operation decisions [51] should be used to test
the effectiveness of APSO-MAM. Indeed, we are motivated

719

to conduct this PSO study by a real world project from the
energy industry. Currently, we are collaborating with an energy
service provider on operation decisions for energy usage of
a building cluster. In our earlier study, we used a Memetic
algorithm which took around 2.5 h (Intel Core i5 1.3 GHz
CPU and 8 GB memory) to obtain the operation decisions.
Since the operation decisions for a building cluster need to
be made within a 15–30-min time frame ideally, we started to
explore the use of PSO and improve its performance in terms
of solution quality and convergence rate in this paper. It is
expected that the proposed APSO-MAM could be successfully
applied to obtain operation decisions for the building cluster.

Acknowledgment
The authors would like to thank P. N. Suganthan for
providing the source codes of his group.

References
[1] R. Eberhart and J. Kennedy, “A new optimizer using particle swarm
theory,” in Proc. 6th Int. Symp. Micro Machine Human Sci., 1995, pp.
39–43.
[2] J. Kennedy and R. Eberhart, “Particle swarm optimization,” in Proc.
IEEE Int. Conf. Neural Netw., vol. 4. Nov.–Dec. 1995, pp. 1942–1948.
[3] M. Reyes-Sierra and C. A. Coello Coello, “Multiobjective particle
swarm optimizers: A survey of the state-of-the-art,” Int. J. Comput.
Intell. Res., vol. 2, no. 3, pp. 287–308, 2006.
[4] A. Banks, J. Vincent, and C. Anyakoha, “A review of particle swarm
optimization: Part I. Background and development,” Natural Comput.,
vol. 6, pp. 467–484, Dec. 2007.
[5] A. P. Engelbrecht, Fundamentals of Computational Swarm Intelligence.
New York: Wiley, 2006.
[6] E. Elbeltagi, T. Hegazy, and D. Grierson, “Comparison among five
evolutionary-based optimization algorithms,” Adv. Eng. Inform., vol. 19,
pp. 43–53, Jan. 2005.
[7] R. Hassan, B. Cohanim, O. de-Weck, and G. Venter, “A comparison
of particle swarm optimization and the genetic algorithm,” in Proc. 1st
AIAA Multidisciplinary Des. Optimization Specialist Conf., 2005, pp.
1–13.
[8] J. J. Liang, A. K. Qin, P. N. Suganthan, and S. Baskar, “Comprehensive
learning particle swarm optimizer for global optimization of multimodal
functions,” IEEE Trans. Evol. Comput., vol. 10, no. 3, pp. 281–295, Jun.
2006.
[9] M. Hu, T. Wu, and J. D. Weir, “An intelligent augmentation of particle
swarm optimization with multiple adaptive methods,” Inform. Sci., vol.
213, pp. 68–83, Dec. 2012.
[10] E. Ozcan and C. K. Mohan, “Analysis of a simple particle swarm
optimization system,” in Proc. Intelligent Engineering Systems through
Artificial Neural Networks, 1998, pp. 253–258.
[11] E. Ozcan and C. K. Mohan, “Particle swarm optimization: Surfing the
waves,” in Proc. Congr. Evol. Comput., 1999, pp. 1939–1944.
[12] M. Clerc and J. Kennedy, “The particle swarm-explosion, stability, and
convergence in a multidimensional complex space,” IEEE Trans. Evol.
Comput., vol. 6, no. 1, pp. 58–73, Feb. 2002.
[13] F. van den Bergh, “An analysis of particle swarm optimizers,” Ph.D.
dissertation, Dept. Comput. Sci., Univ. Pretoria, Pretoria, South Africa,
2002.
[14] V. Kadirkamanathan, K. Selvarajah, and P. J. Fleming, “Stability analysis
of the particle dynamics in particle swarm optimizer,” IEEE Trans. Evol.
Comput., vol. 10, no. 3, pp. 245–255, Jun. 2006.
[15] M. Jiang, Y. P. Luo, and S. Y. Yang, “Stochastic convergence analysis
and parameter selection of the standard particle swarm optimization
algorithm,” Inform. Process. Lett., vol. 102, pp. 8–16, Apr. 2007.
[16] J. L. Fernandez Martınez and E. Garcıa Gonzalo, “The generalized PSO:
A new door to PSO evolution,” J. Artif. Evol. App., vol. 2008, article
861275, 2008.
[17] W. Zhang, H. Li, Q. Zhao, and H. Wang, “Guidelines for parameter
selection in particle swarm optimization according to control theory,” in
Proc. 5th Int. Conf. Natural Comput., 2009, pp. 520–524.

720

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 17, NO. 5, OCTOBER 2013

[18] Y. Shi and R. C. Eberhart, “Empirical study of particle swarm optimization,” in Proc. Congr. Evol. Comput., 1999, pp. 1950–1955.
[19] B. Jiao, Z. Lian, and X. Gu, “A dynamic inertia weight particle swarm
optimization algorithm,” Chaos, Solitons Fractals, vol. 37, pp. 698–705,
Aug. 2008.
[20] A. Chatterjee and P. Siarry, “Nonlinear inertia weight variation for
dynamic adaptation in particle swarm optimization,” Comput. Oper. Res.,
vol. 33, pp. 859–871, Mar. 2006.
[21] Y. Shi and R. C. Eberhart, “Fuzzy adaptive particle swarm optimization,”
in Proc. Congr. Evol. Comput., 2001, pp. 101–106.
[22] R. C. Eberhart and Y. Shi, “Tracking and optimizing dynamic systems
with particle swarms,” in Proc. Congr. Evol. Comput., 2001, pp. 94–100.
[23] A. Ratnaweera, S. K. Halgamuge, and H. C. Watson, “Self-organizing
hierarchical particle swarm optimizer with time-varying acceleration
coefficients,” IEEE Trans. Evol. Comput., vol. 8, no. 3, pp. 240–255,
Jun. 2004.
[24] Z.-H. Zhan, J. Zhang, Y. Li, and H. S.-H. Chung, “Adaptive particle
swarm optimization,” IEEE Trans. Syst., Man, Cybern. B, Cybern., vol.
39, no. 6, pp. 1362–1381, Apr. 2009.
[25] Y.-T. Juang, S.-L. Tung, and H.-C. Chiu, “Adaptive fuzzy particle swarm
optimization for global optimization of multimodal functions,” Inform.
Sci., vol. 181, pp. 4539–4549, Oct. 2011.
[26] T. Yamaguchi and K. Yasuda, “Adaptive particle swarm optimization:
Self-coordinating mechanism with updating information,” in Proc. IEEE
Int. Conf. Syst. Man Cybern., Oct. 2006, pp. 2303–2308.
[27] S. Boyd, EE364b Course Notes: Sub-Gradient Methods. Stanford, CA:
Stanford Univ., 2010.
[28] Y. Shi and R. Eberhart, “A modified particle swarm optimizer,” in Proc.
IEEE Int. Conf. Evol. Comput., IEEE World Congr. Comput. Intell., May
1998, pp. 69–73.
[29] R. Poli, J. Kennedy, and T. Blackwell, “Particle swarm optimization: An
overview,” Swarm Intell., vol. 1, pp. 33–57, Jun. 2007.
[30] J. Kennedy and R. Mendes, “Population structure and particle swarm
performance,” in Proc. Congr. Evol. Comput., 2002, pp. 1671–1676.
[31] J. Kennedy, “Small worlds and mega-minds: Effects of neighborhood
topology on particle swarm performance,” in Proc. Congr. Evol. Comput., 1999, pp. 1931–1938.
[32] J. J. Liang and P. N. Suganthan, “Dynamic multi-swarm particle swarm
optimizer,” in Proc. IEEE Swarm Intell. Symp., Jun. 2005, pp. 124–129.
[33] S. Janson and M. Middendorf, “A hierarchical particle swarm optimizer
and its adaptive variant,” IEEE Trans. Syst., Man, Cybern. B, Cybern.,
vol. 35, no. 6, pp. 1272–1282, Dec. 2005.
[34] K. E. Parsopoulos and M. N. Vrahatis, “UPSO: A unified particle swarm
optimization scheme,” in Proc. LNCS, 2004, pp. 868–873.
[35] R. Mendes, J. Kennedy, and J. Neves, “The fully informed particle
swarm: Simpler, maybe better,” IEEE Trans. Evol. Comput., vol. 8, no.
3, pp. 204–210, Jun. 2004.
[36] T. Peram, K. Veeramachaneni, and C. K. Mohan, “Fitness-distance-ratio
based particle swarm optimization,” in Proc. IEEE Swarm Intell. Symp.,
Apr. 2003, pp. 174–181.
[37] F. van den Bergh and A. P. Engelbrecht, “A cooperative approach to
particle swarm optimization,” IEEE Trans. Evol. Comput., vol. 8, no. 3,
pp. 225–239, Jun. 2004.
[38] H. Wang, Z. Wu, S. Rahnamayan, Y. Liu, and M. Ventresca, “Enhancing
particle swarm optimization using generalized opposition-based learning,” Inform. Sci., vol. 181, pp. 4699–4714, Oct. 2011.
[39] H. Huang, H. Qin, Z. Hao, and A. Lim, “Example-based learning particle
swarm optimization for continuous optimization,” Inform. Sci., vol. 182,
pp. 125–138, Jan. 2012.
[40] Y. Wang, B. Li, T. Weise, J. Wang, B. Yuan, and Q. Tian, “Self-adaptive
learning based particle swarm optimization,” Inform. Sci., vol. 181, pp.
4515–4538, Oct. 2011.
[41] F. Neri, C. Cotta, and P. Moscato, Handbook of Memetic Algorithms.
Berlin, Germany: Springer, 2011.
[42] V. Plevris and M. Papadrakakis, “A hybrid particle swarm-gradient
algorithm for global structural optimization,” Computer-Aided Civil
Infrastructure Eng., vol. 26, pp. 48–68, Jan. 2011.
[43] S.-K. S. Fan and E. Zahara, “A hybrid simplex search and particle swarm
optimization for unconstrained optimization,” Eur. J. Oper. Res., vol.
181, pp. 527–548, Sep. 2007.

[44] Y. G. Petalas, K. E. Parsopoulos, and M. N. Vrahatis, “Memetic
particle swarm optimization,” Ann. Oper. Res., vol. 156, pp. 99–127,
Dec. 2007.
[45] S. Li, M. Tan, I. W. Tsang, and J. T.-Y. Kwok, “A hybrid PSO-BFGS
strategy for global optimization of multimodal functions,” IEEE Trans.
Syst., Man, Cybern. B, Cybern., vol. 41, no. 4, pp. 1003–1014, Aug.
2011.
[46] Z. Michalewicz, Genetic Algorithms + Data Structures = Evolution
Programs, 3rd ed. London, U.K.: Springer-Verlag, 1996.
[47] A. E. Eiben, R. Hinterding, and Z. Michalewicz, “Parameter control in
evolutionary algorithms,” IEEE Trans. Evol. Comput., vol. 3, no. 2, pp.
124–141, Jul. 1999.
[48] A. Auger and N. Hansen, “Performance evaluation of an advanced local
search evolutionary algorithm,” in Proc. Congr. Evol. Comput., 2005,
pp. 1777–1784.
[49] S. Garcı́a, D. Molina, M. Lozano, and F. Herrera, “A study on
the use of non-parametric tests for analyzing the evolutionary algorithms’ behaviour: A case study on the CEC’2005 Special Session
on Real Parameter Optimization,” J. Heuristics, vol. 15, pp. 617–644,
Dec. 2009.
[50] K. Tang, X. Yao, P. N. Suganthan, C. MacNish, Y. P. Chen, C. M.
Chen, and Z. Yang, “Benchmark functions for the CEC special session
and competition on large scale global optimization,” 2008.
[51] M. Hu, J. D. Weir, and T. Wu, “Decentralized operation strategies for
an integrated building energy system using a memetic algorithm,” Eur.
J. Oper. Res., vol. 217, pp. 185–197, Feb. 2012.

Mengqi Hu received the Ph.D. degree in industrial
engineering from Arizona State University, Tempe,
in 2012.
He is currently an Assistant Professor with the
Department of Industrial and Systems Engineering,
Mississippi State University, Starkville. His publications have appeared in journals such as Information
Sciences, the European Journal of Operational Research, and Engineering Optimization. His current
research interests include complex system modeling,
simulation and optimization, swarm intelligence and
evolutionary computation, and intelligent decision support, with applications
in energy systems, healthcare systems, and manufacturing systems.

Teresa Wu received the Ph.D. degree in industrial
engineering from the University of Iowa, Iowa City,
in 2001.
She is currently an Associate Professor with the Industrial Engineering Program, School of Computing,
Informatics, Decision Systems Engineering, Arizona
State University, Tempe. Her current research interests include distributed decision support, distributed
information systems, and health informatics.
Dr. Wu serves on the Editorial Review Board
for the IEEE Transactions on Engineering
Management, the International Journal of Production Research, the Journal
of Medical and Health Informatics, Computer and Standard Interface, and the
International Journal of Electronic Business Management.

Jeffery D. Weir received the Ph.D. degree in industrial and systems engineering from the Georgia
Institute of Technology, Atlanta.
He is currently an Associate Professor with the
Department of Operational Sciences, Air Force Institute of Technology, Wright Patterson AFB, OH. He
teaches courses in decision analysis, risk analysis,
and multiobjective optimization. His current research
interests include decision analysis and transportation
modeling.

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 4, APRIL 2015

1051

Small Blob Identification in Medical Images Using
Regional Features From Optimum Scale
Min Zhang, Teresa Wu∗ , and Kevin M. Bennett

Abstract—Recent advances in medical imaging technology have
greatly enhanced imaging-based diagnosis which requires computational effective and accurate algorithms to process the images
(e.g., measure the objects) for quantitative assessment. In this research, we are interested in one type of imaging objects: small
blobs. Examples of small blob objects are cells in histopathology
images, glomeruli in MR images, etc. This problem is particularly
challenging because the small blobs often have inhomogeneous
intensity distribution and an indistinct boundary against the background. Yet, in general, these blobs have similar sizes. Motivated
by this finding, we propose a novel detector termed Hessian-based
Laplacian of Gaussian (HLoG) using scale space theory as the foundation. Like most imaging detectors, an image is first smoothed via
LoG. Hessian analysis is then launched to identify the single optimal scale on which a presegmentation is conducted. The advantage
of the Hessian process is that it is capable of delineating the blobs.
As a result, regional features can be retrieved. These features enable an unsupervised clustering algorithm for postpruning which
should be more robust and sensitive than the traditional thresholdbased postpruning commonly used in most imaging detectors. To
test the performance of the proposed HLoG, two sets of 2-D grey
medical images are studied. HLoG is compared against three stateof-the-art detectors: generalized LoG, Radial-Symmetry and LoG
using precision, recall, and F-score metrics. We observe that HLoG
statistically outperforms the compared detectors.
Index Terms—Blob detection, Hessian analysis, scale space,
Laplacian of Gaussian (LoG), unsupervised learning.

I. INTRODUCTION
HE rapid development of medical imaging technology has
dramatically increased the spatial and temporal resolution, and therefore size, of clinical imaging data. Typically, image segmentation methods are used to delineate specific objects
and boundaries. The derived features, such as the number, size,
and shape of the objects, are then used for clinical analysis.
Some examples of objects in images for clinical studies include
cell nuclei in histopathology images or fluoroscopic images to
quantify cytology or histology [1]–[3], cerebral blood vessels in
magnetic resonance (MR) images to diagnose vascular disease
[4], brain tumors in MR images to assess treatment [5], breast
lesions in ultrasound images to stage breast cancer lesions [6],

T

Manuscript received May 6, 2014; accepted September 17, 2014. Date of
publication September 25, 2014; date of current version March 17, 2015. This
work was supported by NIH DK091722, and a grant from the NIH Diabetic
Complications Consortium. Asterisk indicates corresponding author.
M. Zhang is with the Department of Industrial Engineering, Arizona State
University, Tempe, AZ 85281 USA (e-mail: mzhang33@asu.edu).
∗ T. Wu is with the Department of Industrial Engineering, Arizona State
University, Tempe, AZ 85281 USA (e-mail: teresa.wu@asu.edu).
K. M. Bennett is with the Department of Biology, University of Hawaii,
Manoa, HI 96822 USA (e-mail: kevinben@hawaii.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2014.2360154

and glomeruli in kidney MR images to characterize renal disease [7], [8]. Of particular interest in this research is a common
type of object that is small in structure and convex elliptic in
shape. In the field of computer vision, the problem of detecting
such objects is known as blob detection. Using mathematical
methods, blobs with different properties such as brightness or
shape can be identified against the image background.
Extensive research has proposed various blob detectors,
among which one popular approach is to use local extrema in a
transformed space in conjunction with a vector of derived features (e.g., local intensity histogram or orientation histogram)
to identify the blobs. In general, this type of blob detector can
be categorized as an interest point detector or interest region detector. As interest point detectors, the Radial-Symmetry [9] and
the Radial Gradient Transform detectors [10] use radial symmetric space as the transformation base. One advantage is that
the Radial-Symmetry and Radial Gradient Transform detectors
are fast to compute and are rotationally invariant, which would
be beneficial to detect radial symmetric blobs. However, for
radial asymmetric blobs, these two detectors may lose their sensitivities [11]. Other types of interest point detectors like SIFT
[12], SURF [13], and BRISK [14] are rooted in a scale-invariant
space transformation. Scale-invariant features are extracted, associated with affine invariant features such as the orientation
histogram to characterize the local properties in the affine space.
These detectors are claimed to be affine invariant such that local
structures with similar affine properties within or across images
can be identified.
Note interest point detectors are developed for each
pixel/voxel, ignoring the contributions of neighboring pixels/voxels to the object. Consequently, these detectors tend to
be less tolerant of local noise. Motivated by the affine-invariant
interest point detector, interest region detectors are introduced to
derive aggregated measures of a number of pixels/voxels within
regions of local extrema. Some examples of interest region detectors are the Harris-affine detector, Hessian-affine detector,
and Hessian–Laplace detector [15]–[17]. While these detectors
are shown to be more robust to noise, they are computationally expensive to adapt the shape estimation. In addition, the
massive number of local extrema necessitates careful pruning,
which adds to the computational burden. It is also challenging
to identify the appropriate pruning parameter, which tends to be
subjective for both interest point and interest region detectors.
Although both interest point and region detectors well describe affine invariant properties of local image structures, it is
observed in the study by Kong et al. [11] that the performance of
these detectors on some blob identification problems in pathological and fluorescent images are unsatisfactory. Instead, another type of detector, Laplacian of Gaussian (LoG) detector

0018-9294 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

1052

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 4, APRIL 2015

[18], [19], under the scale space theory, has attracted great attention for the blob detection problem. Scale space theory is
a formal theory that considers one image as a stack of images
controlled by one parameter (scale parameter, t), in the so-called
scale space representation. The Gaussian scale space representation of an image is defined by the convolution of the image
function with the Gaussian kernel, preserving important spatial
properties of the imaged structures [20], [21]. Specifically, as
the scale parameter increases, the number of local minima in a
dark blob does not increase, and the number of local maxima
in a bright blob does not decrease. This means that neighboring
blob objects will diffuse and eventually combine to be identifiable blobs at a specific scale. If a multiscaled representation with
regards to scale parameter t is constructed, there thus exists one
“optimal” scale for blobs with similar sizes. Individual blobs can
then be detected with corresponding scale parameters. Previous
research has shown that the detector generated by applying the
LoG kernels can successfully detect blobs [22], [23]. However,
the symmetric nature of the LoG detector limits its performance
in rotational asymmetric blob detection and computational cost
for scale adaption under multiscale representation is expensive.
Therefore, a number of LoG extensions have been proposed.
For example, the difference of Gaussian (DoG) [12], [15], [17],
[24] is developed to approximate the LoG and improve computational performance. The generalized Laplacian of Gaussian
(gLoG) [11] is proposed to extend the detection to rotational
asymmetric structures by using different Gaussian kernels. The
gLoG is thus able to detect general elliptical structures such as
rotationally symmetric and asymmetric blobs.
All detectors reviewed previously use the same workflow, pinpointing the centroid of the blob and carefully pruning to remove
the overlapping local extrema. A regular ellipse with a proper
radius that is associated with its scale is then superimposed on
the center to approximate the blob shapes. Consequently, the derived features (e.g., the volumes of the blobs) are only estimates
rather than accurate measurements defined by true boundaries.
Furthermore, while scale space theory provides the foundation
for complex detectors, it is based on a multiscale representation,
which may waste computing effort in optimally selecting scales
when the blobs are approximately uniform in size. Uniformly
sized blobs are common in a number of clinical applications,
such as in detecting cells and nuclei in pathologic or fluorescent microscopic images [11], [25]–[27], and segmenting kidney glomerulus in 3-D MR images [7], [28], [29]. These images
generally contain very large number of small blobs, each with
a convex elliptical shape. Though “uniform size” may relax the
blob recognition problem, there exist some unique challenges:
1) the blob’s small size corresponds to a high spatial frequency
close to that of image noise. As a result, small blob detection
is sensitive to local noise; 2) the heterogeneous distribution of
intensities and heterogeneous boundaries make it difficult to
threshold the small blobs from the image background. We contend features derived from small imaging objects/regions may
help address the challenges. This will require the detailed delineations of the blobs instead of estimating the blob boundaries.
In this study, the convexity and elliptic shape of the blobs of
interest motivates us to explore the Hessian analysis. Here, we

propose a novel approach, named Hessian-based Laplacian of
Gaussian detector (HLoG) for small blob detection. Specifically,
similar to the aforementioned detectors, a multiscale representation is first derived using LoG for each image. Since the blobs
are approximately homogeneous in size, an “optimal” scale can
be identified from the Hessian analysis; thus; a single-scale representation can be obtained from the images. Hessian analysis
is then applied to presegment the blob candidates. It is known
that the theoretical foundation of Hessian guarantees that the
presegmentation will recognize all the true blobs, as well as
some nonblob objects. This leads to the need to fine-tune the
presegmentation results. We note that the detectors reviewed
earlier such as LoG, gLoG, and Radial-Symmetry all employ
threshold-based fine pruning procedures that may be less tolerant to image noise. Fortunately, the Hessian presegmentation
has greatly reduced data size (by filtering out most nonblob
regions) and delineated the boundary of blob candidates. As
a result, we can afford to extract multiple features from the
small blob candidates. In addition to the eight features commonly used in the literature, we introduce three new features
to measure the “blobness.” After comprehensive assessment,
three out of eleven significant features are selected and used in
the variational Bayesian Gaussian mixture model (VBGMM)
[30] to finalize blob detection. VBGMM is chosen here since
it is an unsupervised clustering algorithm which is also tuning
free. During the LoG transformation, a dark blob is converted
to a bright blob and vice versa. To avoid confusion, we define the blob after the LoG operation as transformed-blob hereafter. This paper focuses on the dark blob (transformed-bright
blob) identification. The same process applies for the bright blob
(transformed-dark blob) identification.
The main contributions of this proposed HLoG detector for
small blobs with similar sizes are threefolds. First, the proposed
novel Hessian presegmentation is capable of generating blob
candidate regions that theoretically enclose all the true blobs
and accurately delineate the shape of imaging objects so multiple regional features can be extracted for post pruning. Second,
the use of Hessian presegmentation enables the identification
of a single optimum scale as the smoothing parameter for normalized LoG filter to greatly reduce the computational burden.
Third, together with average intensity information (known from
literature), two out of three new regional features, after evaluation, are introduced to characterize the local blobs to prune out
the nonblob objects for improved final segmentation.
The following sections are organized as follows: Section II describes our method in details followed by the comparison experiments in Section III. Conclusions are presented in Section IV.
All code and results in this paper can be found at our website
(http://swag.engineering.asu.edu/HLoG.htm).

II. HLOG DETECTOR
A. Normalized LoG Transformation
Given 2-D image f : R2 → R, the scale space representation
L(x, y; t) at point (x, y) with scale parameter t is the convolution
of image f (x, y) with Gaussian kernel G (x, y; t):

ZHANG et al.: SMALL BLOB IDENTIFICATION IN MEDICAL IMAGES USING REGIONAL FEATURES FROM OPTIMUM SCALE

L(x, y; t) = f (x, y)∗ G(x, y; t)
where
1
2π t 2

∗
−x

exp

1053

(1)

is the convolution operator and G(x, y; t) =
2+y2
2t2

. The Laplacian of L(x, y; t) is

∇2 L(x, y; t) = ∇2 f (x, y)∗ G(x, y; t).

(2)

Since differentiation commutes with convolution, we have
∇2 L(x, y; t) = f (x, y)∗ ∇2 G(x, y; t).

(3)

A seminal paper by Lindeberg [23] explains that the LoG
response always decreases when t increases resulting the maximum LoG is at the stage without the convolution. A normalizing
factor γ is introduced as the power of scale to obtain the maximum LoG invariance over scale [23]:
LoG(x, y; t) = f (x, y)∗ tγ ∇2 G(x, y; t).

(4)

By using γ, the maximum of LoG responses are the same
regardless of the scales. Ideally, let the intensity distribution
of blob be a perfect Gaussian (without noise and distortion);
it is proved that when γ = 2, the scale invariance is achieved
[23]. As a result, the size of blobs can be determined at the
scale when the normalized LoG reaches the maximum. Yet, in
practice, the normalizing factor γ needs to be tuned to make the
LoG maxima invariant to the blob sizes (note we have adopted
different γ values in our experiments, see Section III). Let γ be
set, (4) transforms the raw image into LoG space (LoG(x, y; t)),
a t-controlled, smoothed space in which transformed-blobs are
highlighted with enhanced boundaries.
In scale space theory, different structures can be highlighted
with regards to the scale parameter t in the multiscale space
representation. Every transformed-blob can be graphically represented by a circle centered at the local spatial maximum over
LoG space with the radius r proportional to the scale at which
the maximum over scales is obtained. Since the blobs studied in
this research have approximately uniform sizes, one optimal t
may be identified for all the blobs in the image. When t is small,
unnecessary details can inadvertently be highlighted, leading
to oversegmentation. When t is large, several small structures
could be identified as one object, leading to undersegmentation.
Therefore, determining the optimum t to extract most of small
blobs of interest is critical. Fortunately, Hessian analysis can assist in determining the optimum t. In addition, Hessian analysis
can be used to highlight blob candidates and preserve the true
geometric shapes of the candidate regions as presegmentation.
B. Hessian Presegmentation
1) Hessian Analysis: It is known that the eigenvalues of the
Hessian matrix of a blob-like structure can be used to describe
the structure’s geometry. Let the image be smoothed via LoG
first, for any pixel (x, y) in the LoG image LoG(x, y; t) at scale
t, the Hessian Matrix for this pixel is
⎞
⎛ 2
∂ LoG(x, y; t) ∂ 2 LoG(x, y; t)
⎟
⎜
∂x2
∂x∂y
⎟
H(x, y; t) = ⎜
⎝ ∂ 2 LoG(x, y; t) ∂ 2 LoG(x, y; t) ⎠ . (5)
∂x∂y

∂y 2

Fig. 1. Transformed-bright blob has a negative-definite Hessian in normalized
LoG space. (a) Dark blob in the raw image. (b) Transformed-bright blob after
normalized LoG transformation from (a). (c) Region of pixels having negativedefinite Hessian in (b) is contoured in green over (b). (d) Region of pixels having
negative-definite Hessian in (b) is contoured in green over original image (a).
As seen in (d), negative-definite Hessian outlines the blob over (a). The irregular
shape of the blob is clearly delineated.

Given geometric classification as a pixel [31] and specific
orientation patterns [4], if pixel (x, y) is concave elliptic, both
of the eigenvalues λ1 , λ2 of H(x, y; t) are negative, meaning
λ1 < 0, λ2 < 0. This motivates us to identify the transformedbright blobs by the following proposition.
Proposition 1: In a transformed 2-D LoG image, every pixel
of a transformed-bright blob has a negative definite Hessian.
Proof : Since every pixel in the transformed-bright blob is
concave elliptic, the eigenvalues of its Hessian are negative,
requiring that the Hessian matrix is negative definite.

Proposition 1 provides one necessary but not sufficient property that a pixel in a transformed-bright blob must satisfy. In
other words, if a pixel resides in a transformed-bright blob, the
Hessian matrix of the pixel is negative definite. But the pixel
having negative definite Hessian may not be from a transformedbright blob. This proposition provides us the theoretical foundation to identify the blob candidates defined by Definition 1 to
ensure all true blobs are recognized.
Definition 1: A blob candidate T in LoG space is
a four-connected component of set U = {(x, y)|(x, y) ∈
LoG(x, y; t), I(x, y; t) = 1}, where I(x, y; t) is the binary indicator such that if the pixel (x, y) has a negative definite Hessian,
then I(x, y; t) = 1; otherwise, I(x, y; t) = 0.
Note the definiteness of the Hessian can be assessed by the
leading principal minors instead of calculating its eigenvalues
of the matrix. Taking a 2 × 2 matrix A as an example, if A11
is negative and determinant of A is positive, then A is negative definite. Following the proposition and the definition, the
Hessian matrix can populate the pool of blob candidates that
theoretically is the superset of all the blobs with their geometric
shapes. Fig. 1 presents an illustrative example showing an identified blob candidate using the Hessian matrix and the true blob
for a given scale. In the next section, we will explain how to use
the Hessian analysis to obtain the optimal scale.
2) Hessian-Based Optimal Scale Identification: In addition
to identifying blob candidates, Hessian analysis can be used to
determine the optimal scale parameter t. For blobs in different scales, Lindeberg [23] uses the maximum normalized LoG
(trace of Hessian) to select the optimum scales across the scalespace for each individual blob. Specifically, each blob achieves
the most saliency at the scale at which its average of LoG reaches
the maxima. Since only blobs of similar sizes are discussed in
this paper, a single scale can be selected to approximate the size

1054

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 4, APRIL 2015

TABLE I
COMPARISON RESULTS OF HLOG, GLOG, RADIAL-SYMMETRY AND LOG ON 15 PATHOLOGIC IMAGES WITHOUT THE POSTPRUNING PROCESS
Hessian

gLoG (no thresholding)

Radial-Symmetry (no thresholding)

LoG (no thresholding)

IMG

d

Precision

Recall

F-score

Precision

Recall

F-score

Precision

Recall

F-score

Precision

Recall

F-score

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Avg
Std

13.85
13.54
14.30
11.75
13.88
12.03
14.01
14.00
14.06
11.99
14.00
14.58
13.87
12.96
13.14
13.46
0.89

0.915
0.804
0.696
0.846
0.884
0.826
0.781
0.846
0.836
0.840
0.808
0.790
0.774
0.829
0.809
0.819
0.051

0.946
0.957
0.985
0.966
0.942
0.968
0.970
0.971
0.972
0.958
0.971
0.960
0.969
0.972
0.968
0.965
0.011

0.931
0.873
0.815
0.902
0.912
0.891
0.865
0.904
0.899
0.895
0.882
0.867
0.860
0.895
0.881
0.885
0.027

0.929
0.844
0.677
0.933
0.907
0.920
0.822
0.881
0.875
0.922
0.850
0.805
0.812
0.875
0.862
0.861
0.066

0.924
0.927
0.974
0.931
0.925
0.898
0.959
0.951
0.943
0.879
0.957
0.960
0.950
0.930
0.952
0.937
0.025

0.927
0.883
0.799
0.932
0.916
0.909
0.885
0.915
0.908
0.900
0.900
0.876
0.876
0.902
0.905
0.895
0.032

0.339
0.285
0.246
0.314
0.339
0.398
0.304
0.405
0.354
0.297
0.296
0.300
0.281
0.361
0.335
0.324
0.044

0.997
0.995
0.991
0.998
0.995
0.994
0.993
0.991
0.987
0.995
1.000
1.000
0.998
0.999
1.000
0.996
0.004

0.506
0.444
0.394
0.478
0.506
0.568
0.466
0.575
0.522
0.457
0.456
0.461
0.438
0.530
0.502
0.487
0.050

0.038
0.032
0.018
0.040
0.040
0.034
0.030
0.034
0.029
0.041
0.031
0.025
0.028
0.030
0.027
0.032
0.006

0.997
0.998
1.000
0.981
0.969
0.998
1.000
0.994
0.997
0.973
0.991
0.998
0.993
0.995
0.993
0.992
0.010

0.073
0.061
0.036
0.077
0.076
0.066
0.059
0.065
0.057
0.079
0.061
0.048
0.055
0.058
0.053
0.061
0.012

of all blobs. The maximum value of averaged normalized LoG
is used here to autodetermine the single optimum scale for small
blobs. Let the image be transformed to a normalized multiscale
LoG space representation. To determine the optimum scale of
the blob candidates, let the average LoG value per candidate
pixel measure Cr be

(x,y ) LoG(x, y; t)I(x, y; t)

(6)
Cr (t) =
(x,y ) I(x, y; t)
where I(x, y; t) is the binary indicator defined in Definition 1.
As Cr (t) increases, the blob candidates are more salient against
their background. Therefore, the optimum scale tb est can be
calculated as
tb est = argmaxt Cr (t).

(7)

Using tb est , the raw image is transformed into a single-scale
LoG space from which the blob candidates are populated.
3) Validation of Hessian Presegmentation: To evaluate the
performance of Hessian presegmentation, the precision, recall,
and F-score metrics are used. Precision measures the fraction
of retrieved candidates that are relevant to the ground truth.
Recall measures the fraction of ground-truth data retrieved. Fscore measures overall performance. Since ground-truth data
are provided in the form of dots (the coordinates of the blob
centers), the same as in the literature [11], [25], a candidate is
considered to be true positive if its intensity centroid is within an
evaluation threshold parameter d of the corresponding ground
truth dot. Specifically, if the Euclidian distance between dot i
and blob candidate j is less equal than d, the blob is considered
true positive. To avoid duplicate counting, the number of true
positives TP is calculated by (8). Precision, recall, and F-score
are calculated by
m

n

i=1

j =1

TP = min{#{j : min Dij ≤ d}, #{i : min Dij ≤ d}}
(8)

TABLE II
ANOVA ON DETECTORS WITHOUT POSTPRUNING USING TUKEY’S HSD
PAIRWISE TEST ON 15 PATHOLOGIC IMAGES WITH 0.05 SIGNIFICANCE LEVEL
Precision

Recall

F-score

Contrast (Hessian v.s)

p (significant)

p

p

gLoG
Radial-Symmetry

0.081(No)
< 0.0001 (Yes)

< 0.0001 (Yes)
0.893 (No)

0.813 (No)
< 0.0001 (Yes)

precision =
recall =

TP
n

TP
m

F- score = 2 ×

(9)
(10)

precision × recall
(precision + recall)

(11)

where m is the number of ground truth and n is the number of
blob candidates and d is a thresholding parameter for evaluation
purpose and can be set to a positive value (0, +∞). If d is small,
fewer blob candidates will be counted since the distance between blob candidate centroid and ground-truth should be small
in order to be counted. If d is set to large, more blob candidates
will be counted due to the relaxation of the distance. In this
paper, since the local intensity extreme could be located anywhere within the small blob region with an irregular shape, we
set evaluation threshold
	 parameter d to the average diameter of
blobs: d = 2 ×
(x,y ) I(x, y; t)/π. The comprehensive experimental results with different values of d will be discussed in
Section III-C.
For validation purpose, three commonly used detectors are
chosen for comparison. One is an interest point detector, RadialSymmetry detector, which is fast and has good performance
on rational symmetric image structure detection [9]. The other
two are LoG detectors which are specifically designed for blob

ZHANG et al.: SMALL BLOB IDENTIFICATION IN MEDICAL IMAGES USING REGIONAL FEATURES FROM OPTIMUM SCALE

1055

Fig. 2. Hessian presegmentation results on selected pathologic cell image. (a) Raw image. (b) Autoscale selection: section t = tb e st = 7 is selected. (c) LoG
transformed image. (d) Hessian presegmentation result: purple mask shows the shapes retrieved by the Hessian presegmentation method.

detection: the original LoG method [23] and its variant—gLoG
method [11]. Note that all three detectors employ thresholdbased post pruning to finalize the segmentation. To validate the
Hessian analysis as a presegmentation procedure, we compare
the Hessian presegmentation with the three detectors without
the postpruning. The comprehensive comparison of HLoG with
the three detectors with postpruning is provided in Section III.
In this experiment, a dataset of 15 600 × 800 pathological cell images [11] is studied. The source code for the LoG
and gLoG algorithms is implemented from [11] and source
code of Radial-Symmetry is implemented from [9] (all those
source codes are available online). Since the Radial-Symmetry
MATLAB package only provides the transformation from raw
image to Radial-Symmetry space, we use the radial-symmetric
centers (local extrema) with maximum intensity values as the
centroids of the small blobs.
The parameter settings for the proposed method are the following: the normalizing factor γ is set to 2 based on our rough
tuning experiments. The optimum scale tb est of small blobs is
automatically selected for each images with (6) and (7) from the
range of 2–10 and step size 0.5. For each image, all the algo-



rithms adopt the same value d (d = 2 ×


I(x,
y;
t)
π
(x,y )



as discussed earlier in this section).
The performance of Hessian presegmentation on the three
metrics, i.e., precision, recall, and F-score, is summarized in
Table I with regards to tb est and d. It is interesting to note that
though both gLoG and Radial-Symmetry detectors claim that
no postpruning is necessary, the results indicate the room for
much improvement. Indeed, in the literature, both detectors [9],
[11] have employed the postpruning for experiments. As for
LoG, it is designed to have postpruning as a must. Therefore, it
is not surprising that the performance of LoG in this validation
experiment is far inferior.
In looking at the average performance (see Table I), gLoG
performs the best (0.861) in precision and F-score (0.895), and
Radial-Symmetry performs the best in recall (0.996). We further conduct analysis of variance (ANOVA) for statistical conclusion. As seen in Table II, there is no significant difference
in precision and F-score in comparing our Hessian presegmentation with gLoG. The same conclusion is drawn in comparing
our Hessian presegmentation with Radial-Symmetry in recall.

1056

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 4, APRIL 2015

However, Hessian presegmentation significantly outperforms
gLoG in recall, and Radial-Symmetry in both precision and
F-score.
Theoretically, all the true blobs with elliptic convex shape
assumption can be retrieved by Hessian presegmentation which
is proved by Proposition 1. However, in practice, as expected,
the Hessian presegmentation in this experiment retrieves most
true blobs (96.5% close to 100%). The discrepancy is due to the
modeling error that some true blobs may not comply with the
elliptic convex assumption.
As discussed earlier, the results from Table I indicate that
both gLoG and Radial-Symmetry need postpruning for improved performance. All three comparison detectors employ
the thresholding-based postpruning process. We argue that the
advantage of our proposed Hessian presegmentation enables the
extraction of blob specific features to be used in the postpruning. Such advantage can be shown in the full comparison (see
Section III).
For illustration purposes, Fig. 2 shows the process of our
proposed Hessian presegmentation on a selected pathologic image. For this specific image, tb est = 7 [see Fig. 2(b)]. Fig. 2(c)
shows the LoG-transformed image, in which small blob structures are enhanced. Based on the transformed image, the Hessian presegmentation method generates blob candidates [purple
in Fig. 2(d)], and the LoG intensity extrema are marked as green
circles to represent the centroids of these candidates compared to
the ground truth dots marked as yellow cross [see Fig. 2(d)]. For
a better view, we circle three representative blobs in Fig. 2(a). As
seen, these blobs have inhomogeneous intensity distributions:
blob 3 is much darker than blobs 1 and 2. Blobs 1 and 2 also have
ambiguous boundaries against the background. Fig. 2(d) shows
that Hessian presegmentation is able to recognize these blobs
and we conclude the Hessian analysis is robust for identifying
blobs with inhomogeneous intensity distributions and blurred
boundaries. In the next section, the regional features extracted
from the blobs are discussed.
C. Features Extraction and Evaluation
1) Regional-Based Features Extraction: There are two common geometric measures in blob detection: R—the likelihood
of “blobness” measure, and S—the second-order structureness
used by [4]
|λ1 |
|λ2 |
	
S=
(λ21 + λ22 )

R=

(12)

background. Given R and S, the single measure to describe
blobness is given by [6] as


 





S2
R2
1 − exp − 2
B(R, S) = 1 − exp − 2
2β
2c
(14)
where β and c are constant parameters that control the sensitivity
of R and S, respectively, which are usually set to 0.5. Although
these measures quantify the geometric information of blobs,
the calculations are relatively computationally expensive. Computing the likelihood of blobness R is particularly expensive
because it requires the calculation and sorting of all eigenvalues
in every pixel. To efficiently calculate the likelihood of blobness, we propose a modified likelihood blobness measure R	 for
each pixel as
R	 (x, y; t) =

=

2
|λ1 |
|λ2 |

+

|λ2 |
|λ1 |

=

2 |λ1 | |λ2 |
(λ1 + λ2 )2 − 2 |λ1 | |λ2 |

2 × |det(H(LoG(x, y; t)))|
(15)
tr(H(LoG(x, y; t)))2 − 2 × |det(H(LoG(x, y; t)))|

where H(·) is the Hessian. The advantage of this modification
is that instead of calculating the eigenvalues λ1 , λ2 , only trace
and determinant are calculated which is more computationally
efficient. Moreover, R	 functions the same as R for the measurement of blobness as replacement. Next, we will prove that
R	 is a monotonic increasing function of R, thus preserving the
ability to measure the blobness.
Proposition 2: R	 is a monotonic increasing function of R.
2
1|
	
When
Proof: When |λ1 | ≤ |λ2 |, R = |λ
|λ2 | , R = R + 1 ;
|λ1 | ≥ |λ2 |, R =
1
R2

−1

|λ2 |
	
|λ1 | , R

=

1
R

2
,
+R

so R	 =

R

1
R

2
.
+R

Then,

∂ R	
∂R

=

	
2 > 0 when R ∈ (0, 1). Therefore, R is a monotonic
( R1 +R )
increasing function of R.

This proposition proves that R	 is a valid and efficient replacement of R that preserves the measurement of blobness.
Equation (15) shows that 0 < R	 ≤ 1. For any pixel, if it is
within a blob-like structure, R is close to 1, the modified R	 is
also close to 1; otherwise, R is close to 0, the modified R	 is
also close to 0. We conclude that R	 is equivalent to R in measuring blobness, and R	 is much efficient to compute compared
to R since it only requires the computation of trace and determinant. Keeping second-order structureness the same, (13) can
be rewritten using the trace and determinant of the Hessian:

(13)

S(x, y; t)

= tr(H(LoG(x, y; t)))2 − 2 × det(H(LoG(x, y; t). (16)

where λ1 andλ2 are eigenvalues of the Hessian and |λ1 | ≤ |λ2 |.
R is the ratio of the two principal curvatures and falls in (0, 1].
(The negative definite Hessian guarantees λ1 , λ2 = 0.) If R →
1, and the curvatures along two principal directions are similar,
the pixel most likely resides in a blob-like structure; if R →
0, i.e., the curvatures along two principal directions are fairly
different and the pixel is most likely on a line [4]. Measure S
indicates the contrast of the pixel against the background. With
larger S, the pixel within the object is more salient against the

The modified blobness measure based on R	 and S is
B = B(R	 , S). Since we are interested in the features within
the regions, i.e., blob candidates instead of individual pixels,
aggregated measures of the pixels within each region are required. One approach is to average the measures, giving us
RT ,m ean , ST ,m ean , and BT ,m ean . Alternatively, the maximum
values of the measures within a region can be calculated as
RT ,m ax , ST ,m ax , and BT ,m ax . In this research, given the true
shape information available from the Hessian presegmentation
	

ZHANG et al.: SMALL BLOB IDENTIFICATION IN MEDICAL IMAGES USING REGIONAL FEATURES FROM OPTIMUM SCALE

and inspired by the design of Harris regional detectors [15],
[16], [32], we introduce three new measures: RT —regional
likelihood of blobness, ST —regional structureness, and BT for
each blob candidate T (a function of RT and ST , as (14)) based
on the matrix constructed by the sum of second-order derivatives
over the candidate T :

HT =
H(x, y; t)
(x,y )∈T

⎡ 
∂ 2 (LoG(x, y; t))
⎢
∂x2
⎢ (x,y )∈T
⎢
=⎢ 
∂ 2 LoG(x, y; t)
⎣
∂x∂y
(x,y )∈T

 ∂ 2 LoG(x, y; t) ⎤
⎥
∂x∂y
⎥
(x,y )∈T
⎥
 ∂ 2 (LoG(x, y; t)) ⎥ .
⎦
∂y 2

Based on this matrix, we get the regional likelihood of blobness RT :
RT =
where
ST =



TABLE III
SUMMARY OF FEATURES OVER BLOB CANDIDATE t
Group

2 × |det(HT )|
ST2

(tr(HT ))2 − 2 × det(HT ).

Common
Features
Using regional
max information

Using regional
mean
information

Proposed
Features

The summation of Hessian matrix in (17) describes the
second-order derivative distribution within the region of blob
candidate. The derivatives are equally weighted averaged (sum
over the region T ) at the centroid of T over the region. The eigenvalues of this matrix represent the two principal curvatures of
the centroid over blob candidate. Thus, this can be utilized to
measure the likelihood of blobness over the region. Together,
we have three groups of features for blobness measures: mean
based, max based, and blob-candidate based. In addition, the
common features average intensity of candidate T (MT ) and
the area of candidate T (AT ) are added into the features list.
Table III summarizes the features and their calculations.
2) Regional-Based Features Selection: With the features extracted earlier, a clustering algorithm based on the VBGMM [30]
is used to evaluate the contributions from the features in identifying the blobs. Unlike the supervised learning algorithm, where
every feature contributes with adjusted weights after training,
the VBGMM is an unsupervised learning algorithm with equal
weights for all features without training. Since this paper studies blobs with similar size, to verify the trivialness of blob size
feature, we add additional tests with and without area features.
Also, because blobness is the nonlinear combination of likelihood of blobness and structureness, these features need to have
separate experiments to test their performances. Therefore, to
evaluate the regional features based on different types of measures, the features listed in Table III are categorized into the 18
features sets (2 × 3 × 3) listed in Table IV. The experiments are
performed on the same dataset as for Hessian presegmentation.
The parameter settings and evaluation metrics are the same as
in Section II-B for model setup and performance evaluations.
The results are shown in Table IV.
Table IV shows that the feature set (MT , RT , ST ) provides
the best performance on the testing data over other feature

Formulation

MT


( x , y ) ∈T
AT

( x , y ) ∈T I (x, y ; t b e s t )
m ax{R 	 (x, y ; t b e s t ), (x, y ) ∈ T }

ST , m a x

m ax{S (x, y ; t b e s t ), (x, y ) ∈ T }

BT ,m ax
RT ,m ean

m ax{B (R 	 , S ), (x, y ) ∈ T }
m ean{R 	 (x, y ; t b e s t ), (x, y ) ∈ T }

ST , m e a n

m ean{S (x, y ; t b e s t ), (x, y ) ∈ T }

BT ,m ean
RT

m ean{B (R 	 , S ), (x, y ) ∈ T }

BT

2 ×|d e t ( H T ) |
S2
T



Description

f (x ,y )

AT
RT ,m ax

ST

(18)

(19)

Features



(x,y )∈T

(17)

1057

(tr (H T )) 2 − 2 × |det(H T )|)
B (R T , S T )

Average
Intensity
Area
Max likelihood
of blobness
Max
Structureness
Max blobness
Mean likelihood
of blobness
Mean
Structureness
Mean blobness
Regional
likelihood of
blobness
Regional
structureness
Regional
blobness

TABLE IV
EVALUATION OF DIFFERENT FEATURE SETS ON 15 PATHOLOGIC IMAGES (MEAN
AND STANDARD DEVIATION OF THE MEASURES ON PRECISION, RECALL,
F-SCORE)
Feature set

Precision
M ean ± S td

Recall
M ean ± S td

F-score
M ean ± S td

MT , AT , RT ,m ax ,
ST , m a x , BT , m a x
MT , AT , RT ,m ax ,
ST , m a x
MT , AT , BT ,m ax
MT , AT , RT ,m ean ,
ST , m e a n , BT , m e a n
MT , AT , RT ,m ean ,
ST , m e a n
MT , AT , BT ,m ean
MT , AT , RT ,
ST , BT
M T , AT , RT , ST
MT , AT , BT
MT , RT ,m ax ,
ST , m a x , BT , m a x
M T , RT , m a x , ST , m a x
MT , BT ,m ax
MT , RT ,m ean ,
ST , m e a n , BT , m e a n
MT , RT ,m ean ,
ST , m e a n
MT , BT ,m ean
M T , RT , ST , BT
MT , BT

0.856 ± 0.046

0.953 ± 0.014

0.901 ± 0.021

0.865 ± 0.046

0.950 ± 0.014

0.904 ± 0.021

0.832 ± 0.054
0.821 ± 0.051

0.964 ± 0.011
0.965 ± 0.011

0.892 ± 0.029
0.886 ± 0.027

0.828 ± 0.053

0.964 ± 0.011

0.890 ± 0.028

0.832 ± 0.053
0.969 ± 0.017

0.964 ± 0.011
0.767 ± 0.051

0.892 ± 0.029
0.855 ± 0.030

0.945 ± 0.027
0.963 ± 0.022
0.854 ± 0.047

0.880 ± 0.034
0.822 ± 0.029
0.953 ± 0.014

0.911 ± 0.016
0.887 ± 0.014
0.900 ± 0.022

0.862 ± 0.047
0.828 ± 0.057
0.822 ± 0.053

0.950 ± 0.014
0.964 ± 0.011
0.965 ± 0.011

0.903 ± 0.022
0.890 ± 0.030
0.887 ± 0.028

0.828 ± 0.057

0.964 ± 0.011

0.890 ± 0.030

0.828 ± 0.057
0.972 ± 0.015
0.959 ± 0.028

0.964 ± 0.011
0.775 ± 0.049
0.829 ± 0.043

0.890 ± 0.030
0.861 ± 0.029
0.888 ± 0.018

sets. This feature set balances the precision and recall measures leading to the highest F-score. Moreover, the feature set
(MT , RT , ST ) is relatively more stable than other feature sets
since it has minimal standard deviation across all the combinations. In addition, we conduct the ANOVA to test the statistical
performance of the selected feature set against other features.
As shown in Table V, the test groups the feature sets into four

1058

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 4, APRIL 2015

TABLE V
STATISTICAL GROUPS BASED ON ANOVA TUKEY’S HSD PAIRWISE TEST
Category
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT

, RT , ST
, AT , RT , ST
, AT , RT , m a x , ST , m a x
, RT , m a x , ST , m a x
, AT , RT , m a x , ST , m a x , BT , m a x
, RT , m a x , ST , m a x , BT , m a x
, AT , BT ,m ax
, AT , ST , m e a n
, RT , m e a n , ST , m e a n
, BT ,m ax
, BT ,m ean
, AT , RT , m e a n , ST , m e a n
, BT
, AT , BT
, RT , m e a n , ST , m e a n , BT , m e a n
, AT , RT , m e a n , ST , m e a n , BT , m e a n
, RT , ST , BT
, AT , RT , ST , BT

F-score means
0.924
0.911
0.904
0.903
0.901
0.900
0.892
0.892
0.890
0.890
0.890
0.890
0.888
0.887
0.887
0.886
0.861
0.855

Statistical groups∗
A
A
A
A
A
A

B
B
B
B
B
B
B
B
B
B
B
B
B
B
B

Fig. 4.

HLOG for blob detection.

TABLE VI
DETAIL STEPS OF HLOG
C
C
C
C
C
C
C
C
C
C
C

1. Initialize the normalize factor γ , range and step-size of parameter t to transform the
raw image into normalized LoG space
2. Binarize each section of normalized LoG space with the negative definite Hessian (for
dark small blob in raw image).
 
D
D
D
D
D

∗
Note: there is no statistically significant difference (with 0.05 significance level) between
feature sets within a statistical group.

3. Calculate average LoG intensity C r (t) =

x

L o G (x , y ;t )I (x , y ;t )

y 
x

y

I (x , y ;t )

and find

optimum scale section by t b e s t = argm ax t C r (t).
4. Choose the optimum scale section t = t b e s t and extract the regional featuresM T in
raw image space and R T , S T in LoG space
5. Input those three features to variational Bayesian mixture models with two clusters
setting
6. Choose the cluster with highest value of R T as final segmentation

In the next section, we discuss the integration of Hessian
analysis, regional feature extraction, and clustering-based final
pruning for the blob identification problem.
D. HLoG for Blob Identification

Fig. 3. Blob identification result from part of Fig. 2(a). (a) Presegmentation
result. (b) Final identification result. The purple regions are blob candidates and
their centroids are marked as green circle, while the centroids of ground-truth
data are marked as yellow cross.

groups (A, B, C, D). The selected feature set (MT , RT , ST ) statistically outperforms the 12 feature sets from groups B, C, and
D. Though there is no significant difference between the feature
set (MT , RT , ST ) with the other five feature sets from group A,
the other five feature sets have also been assigned to group B
(inferior to group A). Therefore, we consider (MT , RT , ST ) as
a reasonable feature set describing the characteristics of blobs.
Fig. 3 shows a comparison between Hessian presegmentation
(from Fig. 2) and final identification using the three regional features. False-positive blobs are removed, as shown in the circles
in Fig. 3. We conclude that using the three features in VBGMM
can refine the presegmentation by removing false-positive blobs
in the image. Note that the pruning may also remove a few true
blobs. This is seen by comparing Table I (precision: 0.819, recall: 0.965, F-score: 0.885), and Table III (precision: 0.924,
recall: 0.925, F-score: 0.924). The recall measure decreases by
0.04, and the precision increases from 0.819 to 0.924. The overall performance of F-score increases from 0.885 to 0.924. We
conclude that regional features-based postpruning is promising
to improve the blob detection.

We propose a three-phased HLoG workflow for blob detection, integrating raw image transformation, Hessian presegmentation, feature extraction, and evaluation (see Fig. 4). First, the
raw image is transformed into Normalized LoG space. Next,
the Hessian presegmentation method is conducted for initial
segmentation to generate the blob candidates. Finally, the average intensity feature MT , regional likelihood of blobness RT ,
and regional structureness ST are used in the VBGMM clustering algorithm for identification. The VBGMM is more robust
than maximum likelihood Gaussian mixture models because
it treats parameters, i.e., mean vector and variance-covariance
matrix in Gaussian mixtures models as distributions instead of
deterministic values and uses hyper parameters to control them.
This helps avoid the singularity issues faced by the maximum
likelihood Gaussian mixture models. In addition, unlike other
pruning algorithms like thresholding, the VBGMM requires no
parameter tuning. The detailed steps are listed in Table VI.
III. COMPARISON EXPERIMENTS
In this section, three sets of experiments are conducted to
validate the performance of our proposed HLoG detector. In the
first set of experiments, the complete version of the three blob
detectors above is compared on 15 pathological images and new
supplemental data consisting of 200 fluorescence microscopy
cell images [33] are tested in the second set of experiments.
The 200-cell image dataset is of interest because the blobs are
small, and each image can be used to test the performance of the
algorithm in tolerating the noise from the background. The first
two sets of experiments are to evaluate the performance given

ZHANG et al.: SMALL BLOB IDENTIFICATION IN MEDICAL IMAGES USING REGIONAL FEATURES FROM OPTIMUM SCALE

1059

TABLE VII
COMPARISON RESULTS OF COMPLETE VERSION OF HLOG, GLOG, RADIAL-SYMMETRY AND LOG ON 15 PATHOLOGIC IMAGES
HLoG

gLoG

Radial-Symmetry

LoG

IMG

d

Precision

Recall

F-score

Precision

Recall

F-score

Precision

Recall

F-score

Precision

Recall

F-score

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Avg
Std

13.85
13.54
14.30
11.75
13.88
12.03
14.01
14.00
14.06
11.99
14.00
14.58
13.87
12.96
13.14
13.46
0.89

0.980
0.969
0.858
0.900
0.962
0.902
0.948
0.957
0.936
0.898
0.905
0.897
0.878
0.937
0.941
0.924
0.036

0.886
0.884
0.930
0.949
0.897
0.943
0.955
0.910
0.917
0.946
0.954
0.943
0.937
0.935
0.894
0.925
0.025

0.931
0.925
0.892
0.924
0.928
0.922
0.952
0.933
0.926
0.922
0.929
0.919
0.906
0.936
0.917
0.924
0.013

0.950
0.906
0.769
0.943
0.951
0.941
0.900
0.939
0.915
0.939
0.877
0.856
0.885
0.904
0.888
0.904
0.048

0.903
0.915
0.971
0.916
0.920
0.899
0.950
0.947
0.937
0.865
0.946
0.947
0.942
0.926
0.932
0.928
0.026

0.926
0.911
0.858
0.929
0.935
0.919
0.924
0.943
0.926
0.901
0.910
0.899
0.913
0.915
0.910
0.915
0.020

0.932
0.858
0.704
0.939
0.911
0.944
0.816
0.897
0.869
0.943
0.865
0.810
0.818
0.905
0.876
0.873
0.065

0.903
0.927
0.958
0.863
0.921
0.871
0.952
0.951
0.924
0.857
0.964
0.970
0.965
0.942
0.941
0.927
0.038

0.918
0.891
0.812
0.899
0.916
0.906
0.878
0.923
0.896
0.898
0.912
0.882
0.886
0.923
0.907
0.897
0.027

0.832
0.825
0.691
0.825
0.825
0.810
0.844
0.811
0.829
0.822
0.704
0.802
0.767
0.810
0.803
0.800
0.045

0.897
0.889
0.932
0.769
0.734
0.885
0.889
0.855
0.880
0.803
0.869
0.854
0.726
0.858
0.890
0.849
0.062

0.863
0.855
0.793
0.796
0.777
0.846
0.866
0.833
0.854
0.812
0.778
0.827
0.746
0.833
0.844
0.821
0.036

F-score metric is highlighted in gray since it provides a comprehensive measurement to evaluate the performance.

TABLE VIII
ANOVA USING TUKEY’S HSD PAIRWISE TEST ON 15 PATHOLOGIC IMAGES
WITH 0.05 SIGNIFICANCE LEVEL

the estimated diameter d of blob candidates. As explained in
Section II, the choice of d may impact the evaluation outcomes.
Here, we further conduct a separate experiment to validate the
performance of HLoG with different values of d compared to
the other detectors on both image datasets.

that the underpruned and overpruned issue may be overcome by
tuning the parameter (γ) for each image, yet this will require
manual processing. In looking into overall performance, our
approach outperforms the three algorithms on average F-score
with lower variation. Radial-Symmetry and LoG do not perform as well as gLoG and HLoG, evidenced by lower detection
of rotationally asymmetric blobs. HLoG balances the recall and
precision metrics leading to overall better F-score. Although the
average recall (0.925) of HLoG is marginally lower than that
of the gLoG (0.928) and Radial-Symmetry (0.927), the precision performance is significantly better. The pruning algorithm
sacrifices marginal recall to improve F-score and precision.
We further conduct ANOVA Tukey’s HSD test to draw statistical conclusions. Table VIII indicates, on F-score metric,
HLoG statistically outperforms the Radial-Symmetry and LoG
methods (p < 0.05) while comparable to the gLoG.

A. Experiments on Pathologic Images

B. Experiments on Fluorescence Images

Since the results of detection by the complete version of the
gLoG method, Radial-Symmetry method and LoG methods on
15 pathological images are available online [11], the results are
directly used in this paper to avoid any of the parameters tunings.
The parameter settings for HLoG are the same as presented in
Section II: the normalizing factor γ is set to 2, and the range of
parameter t is set from 2 to 10 with step-size 0.5, resulting in 17
sections of normalized LoG space in total. For each image, all
the algorithms adopt the same value d, which is the estimated
average diameter of all small blobs, to calculate the precision,
recall, and F-score. The results are shown in Table VII.
As shown in Table VII, HLoG outperforms the three algorithms on the F-score for 11 out of 15 images. HLoG underperforms the gLoG method for four images where the blob
candidates are either underpruned (image 4 and image 13) or
overpruned (image 5 and image 8) resulting in a lower F-score.
This is due to the fact that the parameter (γ) is tuned to the
group of images instead of each individual image. We argue

A new supplemental dataset is added in this experiment consisting of 200 256 × 256 fluorescence-light microscopy cell images. Unlike the data used earlier, these images contain bright
blobs rather than dark blobs. Therefore, the data are converted
into image with dark small structures by 1 − f (x, y). (We assume that f (x, y) varies from 0 to 1; otherwise, we need to
standardize f (x, y) into [0, 1] range.)
For the LoG algorithm, the range t is set as log(t) ∈ [0.5, 3]
with step-size 0.2, as suggested in [11], and the extrema intensity value is set to 0.005 based on our experiments after tuning.
For the gLoG algorithm, α = 1, σstep = −1, θstep = π/9 and
the postpruning threshold is set to 100. (This algorithm uses the
intensity range [0, 255] rather than [0,1].) The other parameter
settings are the same as presented in [11]. For Radial-Symmetry
detector, based on our tuning after experiments, the local intensity threshold is set to 0.0003, and the postpruning threshold is
set to 1 for refinement. In HLoG, the normalizing factor γ is set
to 1 to avoid over smoothing since many blobs are clustered in

Precision

Recall

F-score

Contrast (HLoG v.s)

p (significant)

p

p

gLoG
Radial Symmetry
LoG

0.679 (No)
0.029 (Yes)
< 0.0001(Yes)

0.998 (No)
0.999 (No)
< 0.0001 (Yes)

0.741 (No)
0.023 (Yes)
< 0.0001 (Yes)

1060

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 4, APRIL 2015

Fig. 5. Comparison results of full version of HLoG, gLoG, Radial-Symmetry,
and LoG on 200 Fluoro images. The error bar indicates the standard deviation
of the corresponding measure across 200 images.

Fig. 6. F-score of HLoG, gLoG, Radial-Symmetry, and LoG on 15 pathological images at different parameter d.

TABLE IX
ANOVA USING TUKEY’S HSD PAIRWISE TEST ON 200 FLUORESCENT IMAGES
WITH 0.05 SIGNIFICANCE LEVEL
Precision

Recall

F-score

Contrast (HLoG v.s)

p (significant)

p

p

gLoG
Radial Symmetry
LoG

< 0.0001(Yes)
< 0.0001(Yes)
< 0.0001(Yes)

0.269 (No)
0.654 (No)
< 0.0001(Yes)

< 0.0001(Yes)
< 0.0001(Yes)
< 0.0001(Yes)

this set of images by observation. The scale-space representation
is the same as that of the first experiment. Similarly, the parameter d is the same for all the algorithms.
Fig. 5 compares HLoG to the gLoG, LoG and RadialSymmetry algorithms. The result shows that though HLoG is
comparable to gLoG and Radial Symmetry algorithms on recall, it outperforms the three algorithms in both precision and
F-score. The variation of our results is also lower than others.
(The standard deviation of F-score in HLoG is 0.0377, compared to 0.1436 with the gLoG method, 0.0795 with the RadialSymmetry method, and 0.0385 with the LoG method.) We conclude that HLoG provides more accurate and stable detection
of blobs in this dataset. Again, statistical analysis is performed
with the results summarized in Table IX. It is observed that while
comparable to the three algorithms on the recall metric, our approach statistically outperforms the comparison algorithms on
precision and F-score.
Fig. 8 shows detection of cells in a single fluorescence image.
In Fig. 8(b), false-positive blobs using the gLoG algorithm [example as yellow circle 1 in Fig. 8(b)] are caused by the added
noise, since gLoG uses aggregated LoG map which may be
sensitive to local noise. Some false-positive blobs using RadialSymmetry detector are due to the symmetric structures near true
blobs as shown circle 2 in Fig. 8(c), because Radial-Symmetry
detector only detects symmetric structures and lacks the ability
to distinct the differences between symmetric structures. There
are false blobs using LoG around the edge of the image in
Fig. 8(d) (circle 3 as an example). This is because many local
extrema occurring around the boundary of the image in multi-

Fig. 7. F-score of HLoG, gLoG, Radial-Symmetry and LoG on 200 pathological images at different parameter d.

scale space which are difficult to remove only by thresholding.
However, those types of false-positive blobs are not shown in
HLoG as seen in circles 1, 2, and 3 in Fig. 8(a). This is because
rather than utilizing symmetric properties and using thresholding, HLoG uses the three regional geometric features for pruning and is therefore more robust in the presence of removing
false-positive blobs against noisy background.
C. Evaluation of HLoG at Different d
The performance metrics (precision, recall, and F-score) calculated by (8)–(11) could be highly affected by the value of
parameter d. In the previous experiments, d is set to be the estimated diameter of blobs generated by Hessian presegmentation.
To explore the effects of the change of d on the performance of
HLoG detector, additional experiments are conducted.
Figs. 6 and 7 show the comparison results of HLoG, gLoG,
Radial-Symmetry, and LoG at different d on 15 pathological
images and 200 fluorescence images, respectively. Since F-score
is the geometric average of precision and recall, only F-score
is plotted in the Figure. As discussed in Section II-B3 when
d is increasing, the average F-score will increase. More and more
blob candidates will be treated as true positives because more
and more blob candidates have their distance to the ground truth

ZHANG et al.: SMALL BLOB IDENTIFICATION IN MEDICAL IMAGES USING REGIONAL FEATURES FROM OPTIMUM SCALE

1061

Fig. 8. Detection results on selected fluorescent image. (a) Detection result by HLoG. (b) Detection result by the gLoG algorithm. (c) Detection result by the
Radial-Symmetry algorithm. (d) Detection result by LoG.

within the range of d. Since the blob size and image size (256 ×
256) of fluorescence data are smaller than the pathological image
(600 × 800), the range of d is set to be small ([0, 8]compared
to [0, 16]) to avoid it being greater than the distance of two
neighboring blobs.
From both Figures, it is evident that HLoG outperforms other
detectors on F-score across the change of d. On the first set of
images, HLoG is comparable to gLoG and outperforms the other
two detectors on F-score across the change of d. On the second
set of images, HLoG outperforms the other three detectors on
F-score across the change of d. We conclude HLoG in general
outperforms the three detectors regardless of the d value.

D. Discussion on Computational Cost
The proposed method was programmed in MATLAB 2012b
on a Windows (Microsoft, Inc.) platform, and the experiments
are done on a Windows PC with Intel Xeon 2.0 GHz CPU and
32 GB of memory. For the 600 × 800 pathologic images, the average time cost is about 10.0 s/image compared to 30 s/image for
the gLoG algorithm [11]. The time cost spent on 200 256 × 256
fluorescence-light microscopy images of cells is 1.2 s/image
on average, compared to 10.0 s/image for the gLoG algorithm. This shows that our algorithm is efficient for 2-D images and may be extended to 3-D grey images. Also, the LoG

1062

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 4, APRIL 2015

transformation could be replaced by DoG approximation to improve the computational time [17].
[9]

IV. CONCLUSION
In this paper, we propose a novel imaging detector, termed
HLoG to identify small blobs in medical images. After the raw
image is transformed into normalized LoG space, an optimum
scale is automatically determined based on the Hessian analysis. The blob candidates are also populated with their geometric
shapes as the result of Hessian presegmentation. This process
allows us to extract multiple regional features to characterize
the accurate regional properties of small blobs. Three regional
features, the average intensity feature, the regional likelihood of
blobness, and the regional structureness, are extracted and used
in a tuning-free, VBGMM to prune the presegmentation results.
One set of pathologic images (15) and one set of fluorescencelight microscopy images (200) are used to compare HLoG with
gLoG, Radial-Symmetry and LoG using recall, precision, and
F-score metrics. In the experiments when d is estimated based
on the size of the blobs, we observe HLoG outperforms RadialSymmetry, LoG on both datasets, outperforms gLoG on the
second dataset but with comparable performance for the first
dataset for the precision metric. On the recall metric, HLoG only
outperforms LoG on both datasets, but comparable to gLoG and
Radial-Symmetry. In exploring the impact of d on the performance evaluation (F-score), we observe that HLoG outperforms
Radial-Symmetry, LoG on both datasets. In addition, HLoG is
computationally efficient and provides a tuning-free pruning
process with only one parameter, the normalizing factor γ that
needs to be specified.

[10]

[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]

ACKNOWLEDGMENT

[23]

The authors would like to thank Dr. H. Kong for providing
gLoG source code and results.

[24]
[25]

REFERENCES
[1] Y. Al-Kofahi, W. Lassoued, W. Lee, and B. Roysam, “Improved automatic
detection and segmentation of cell nuclei in histopathology images,” IEEE
Trans Biomed. Eng., vol. 57, no. 4, pp. 841–852, Apr. 2010.
[2] N. Harder, B. Neumann, M. Held, U. Liebel, H. Erfle, J. Ellenberg, R. Eils,
and K. Rohr, “Automated recognition of mitotic patterns in fluorescence
microscopy images of human cells,” in Proc. IEEE Int. Symp. Biomed.
Imag., 2006, pp. 1016–1019.
[3] C. Xiaowei, Z. Xiaobo, and S. T. C. Wong, “Automated segmentation,
classification, and tracking of cancer cell nuclei in time-lapse microscopy,”
IEEE Trans. Biomed. Eng., vol. 53, no. 4, pp. 762–766, Apr. 2006.
[4] A. Frangi, W. Niessen, K. Vincken, and M. Viergever, “Multiscale vessel
enhancement filtering,” in Proc. Med. Image Comput. Comput.-Assisted
Intervention Conf., vol. 1496, 1998, pp. 130–137.
[5] M. Prastawa, E. Bullitt, S. Ho, and G. Gerig, “A brain tumor segmentation
framework based on outlier detection,” Med. Image Anal., vol. 8, no. 3,
pp. 275–283, 2004.
[6] M. Woo Kyung, S. Yi-Wei, B. Min Sun, H. Chiun-Sheng, C. Jeon-Hor,
and C. Ruey-Feng, “Computer-aided tumor detection based on multi-scale
blob detection algorithm in automated breast ultrasound images,” IEEE
Trans. Med. Imag., vol. 32, no. 7, pp. 1191–1200, Jul. 2013.
[7] S. C. Beeman, M. Zhang, L. Gubhaju, T. Wu, J. F. Bertram, D. H. Frakes,
B. R. Cherry, and K. M. Bennett, “Measuring glomerular number and
size in perfused kidneys using MRI,” Amer. J. Physiol. Renal Physiol.,
vol. 300, no. 6, pp. F1454–F1457, 2011.
[8] S. C. Beeman, L. Cullen-McEwen, M. Zhang, T. Wu, E. J. Baldelomar, J.
Dowling, J. R. Charlton, M. S. Forbes, A. Ng, Q.-Z. Wu, J. A. Armitage,
V. G. Puelles, G. F. Egan, J. F. Bertram, and K. M. Bennett, “Measuring

[26]

[27]
[28]
[29]
[30]
[31]
[32]
[33]

human glomerular morphology and pathology with MRI,” presented at the
International Society for Magnetic Resonance in Medicine, Milan, Italy,
2014.
G. Loy and A. Zelinsky, “Fast radial symmetry for detecting points
of interest,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 25, no. 8,
pp. 959–973, Aug. 2003.
G. Takacs, V. Chandrasekhar, S. S. Tsai, D. Chen, R. Grzeszczuk, and
B. Girod, “Fast computation of rotation-invariant image features by an
approximate radial gradient transform,” IEEE Trans. Image Process.,
vol. 22, no. 8, pp. 2970–2982, Aug. 2013.
H. Kong, H. C. Akakin, and S. E. Sarma, “A generalized Laplacian of
Gaussian filter for blob detection and its applications,” IEEE Trans. Cybern., vol. 43, no. 6, pp. 1719–1733, Dec. 2013.
D. Lowe, “Distinctive image features from scale-invariant keypoints,” Int.
J. Comput. Vis., vol. 60, no. 2, pp. 91–110, 2004.
H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool, “Speeded-up robust
features (SURF),” Comput. Vis. Image Understanding, vol. 110, no. 3,
pp. 346–359, 2008.
S. Leutenegger, M. Chli, and R. Y. Siegwart, “BRISK: Binary robust
invariant scalable keypoints,” in Proc. IEEE Int. Comput. Vis., 2011,
pp. 2548–2555.
K. Mikolajczyk and C. Schmid, “Scale & affine invariant interest point
detectors,” Int. J. Comput. Vis., vol. 60, no. 1, pp. 63–86, 2004.
K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F.
Schaffalitzky, T. Kadir, and L. V. Gool, “A comparison of affine region
detectors,” Int. J. Comput. Vis., vol. 65, no. 1/2, pp. 43–72, 2005.
K. Mikolajczyk and C. Schmid, “A performance evaluation of local
descriptors,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 27, no. 10,
pp. 1615–1630, Oct. 2005.
A. P. Witkin, “Scale-space filtering: A new approach to multi-scale description,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.,
vol. 9, 1984, pp. 150–153.
J. Koenderink, “The structure of images,” Biol. Cybern., vol. 50, no. 5,
pp. 363–370, 1984.
T. Lindeberg, “Scale-space theory: A basic tool for analyzing structures
at different scales,” J. Appl. Statist., vol. 21, no. 1/2, pp. 225–270, 1994.
J. Babaud, A. P. Witkin, M. Baudin, and R. O. Duda, “Uniqueness of
the Gaussian kernel for scale-space filtering,” IEEE Trans. Pattern Anal.
Mach. Intell., vol. PAMI-8, no. 1, pp. 26–33, Jan. 1986.
T. Lindeberg, “Detecting salient blob-like image structures and their scales
with a scale-space primal sketch: A method for focus-of-attention,” Int. J.
Comput. Vis., vol. 11, no. 3, pp. 283–318, 1993.
T. Lindeberg, “Feature detection with automatic scale selection,” Int. J.
Comput. Vis., vol. 30, no. 2, pp. 79–116, 1998.
T. Tuytelaars and K. Mikolajczyk, “Local invariant feature detectors: A
survey,” Found. Trends Comput. Graph. Vis., vol. 3, no. 3, pp. 177–280,
2008.
E. Bernardis and S. X. Yu, “Pop out many small structures from a very
large microscopic image,” Med. Image Anal., vol. 15, no. 5, pp. 690–707,
2011.
A. A. Dima, J. T. Elliott, J. J. Filliben, M. Halter, A. Peskin, J. Bernal,
M. Kociolek, M. C. Brady, H. C. Tang, and A. L. Plant, “Comparison
of segmentation algorithms for fluorescence microscopy images of cells,”
Cytometry Part A, vol. 79A, no. 7, pp. 545–559, 2011.
S. Di Cataldo, E. Ficarra, A. Acquaviva, and E. Macii, “Automated segmentation of tissue images for computerized IHC analysis,” Comput.
Methods Programs Biomed., vol. 100, no. 1, pp. 1–15, 2010.
S. C. Beeman, J. F. Georges, and K. M. Bennett, “Toxicity, biodistribution,
and ex vivo MRI detection of intravenously injected cationized ferritin,”
Magn. Reson. Med., vol. 69, no. 3, pp. 853–861, 2013.
K. M. Bennett, J. F. Bertram, S. C. Beeman, and N. Gretz, “The emerging role of MRI in quantitative renal glomerular morphology,” Amer. J.
Physiol. Renal. Physiol., vol. 304, no. 10, pp. F1252–F1257, 2013.
C. M. Bishop, Pattern Recognition and Machine Learning. New York,
NY, USA: Springer, 2006.
A. H. Salden, B. M. T. H. Romeny, M. A. Viergever, L. M. J. Florack, and
J. J. Koenderink, “Differential geometric description of 3D scalar images,”
Internal Report 3DCV, vol. 91-05, 1991.
C. Harris and M. Stephens, “A combined corner and edge detector,” in
Proc. 4th Alvey Vis. Conf., 1988, pp. 147–151.
V. Lempitsky and A. Zisserman, “Learning to count objects in images,”
in Proc. Adv. Neural Inf. Process. Syst., 2010, pp. 1324–1332.

Authors’ photographs and biographies not available at the time of publication.

Proceedings of the 2011 Winter Simulation Conference
S. Jain, R.R. Creasey, J. Himmelspach, K.P. White, and M. Fu, eds.

DESIGN OF CENTRALIZED AMBULANCE DIVERSION POLICIES USING
SIMULATION-OPTIMIZATION
Adrian Ramirez-Nafarrate
John W. Fowler
Teresa Wu
Arizona State University
699 S. Mill Avenue
Tempe, AZ 85281, USA
ABSTRACT
Ambulance Diversion (AD) has been an issue of concern for the medical community because of the potential harmful effects of long transportation; however, AD can be used to reduce the waiting time in
Emergency Departments (EDs) by redirecting patients to less crowded facilities. This paper proposes a
Simulation-Optimization approach to find the appropriate parameters of diversion policies for all the facilities in a geographical area to minimize the expected time that patients spend in non-value added activities, such as transporting, waiting and boarding. In addition, two destination policies are tested in combination with the AD policies. The use of diversion and destination policies can be seen as ambulance flow
control within an emergency care system. The results of this research show significant improvement in
the flow of emergency patients in the system as a result of the optimization of destination-diversion policies compared to not using AD at all.
1

INTRODUCTION

Ambulance Diversion (AD) are the periods when overcrowded Emergency Departments (EDs) request
ambulance services to bypass their facilities (GAO 2003). The Center for Disease Control and Prevention
reported that the mean annual hours on AD in metropolitan areas was 404 hours from 2003 to 2004 (CDC
2006).
The medical community strongly recommends to avoid or minimize the periods on AD because of the
potential harmful effect of longer transportation on the health status of the patient (ACEP 1999a; ACEP
2008). However, there is evidence suggesting that not diverting ambulances may increase the waiting
time and the number of patients boarding in EDs (Massachusetts Nurse Newsletter 2009).
On the other hand, the effect of ambulance patients on the operations of EDs determines the effectiveness of the system in many performance measures. Thus, ambulance patients tend to receive higher
priority to start treatment than walk-ins because of their severity conditions. In addition, ambulance patients also have longer treatment times and larger admission probability than walk-ins. Therefore, the allocation of the patient affects the flow of other patients.
This paper proposes the centralized design of AD policies combined with effective destination policies as a mechanism of ambulance flow control. The destination policy determines the hospital destination of a patient when there are more than one open hospital in the region. The objective is to minimize
the time that patients who require emergency assistance spend in suboptimal treatment (a.k.a non-value
added time). The results show the potential of designing effective destination and diversion policies to
smooth the flow through different stages of care.

978-1-4577-2109-0/11/$26.00 ©2011 IEEE

1251

Ramirez-Nafarrate, Fowler, and Wu
2

LITERATURE REVIEW

Empirical studies regarding ambulance diversion exists in the literature. These type of papers identify the
main causes to divert ambulances and show efforts to design system-wide policies that enable the reduction of the diversion episodes in a region.
A survey conducted by the CDC (2006) revealed that the main causes for initiating AD episodes are
the lack of beds in inpatient units, the high number of patients waiting in the ED and the complexity of
ED cases. These factors are also identified in other publications. Furthermore, an additional cause that is
consistently named in other publications is the high number of patients boarding, which are the patients in
the ED waiting for an open bed in an inpatient unit (ACEP 2008; Pham et al. 2006).
Empirical studies to design AD guidelines are available in the literature. The objective of these guidelines is to minimize the amount of time spent on AD in a region. For example, Vilke et al. (2004a) designed a plan to observe the effect of AD in two hospitals. The authors found a reciprocating effect. Thus,
when one hospital goes on diversion, it is very likely that the neighboring hospital starts diverting ambulances within a short period. An enhanced project that comprised multiple hospitals had the objective of
redesigning the guidelines to start AD. These new guidelines are more restrictive and the results show a
significant reduction of AD hours in the region (Vilke et al. 2004b).
Two similar studies were conducted in other regions. The first of them introduces a new AD protocol
for a county with 600,000 inhabitants and 10 hospitals (Asamoah et al. 2008). The new protocol constrained the time on AD to only one hour out of every eight. The mean number of hours on diversion in
the system was reduced by about 82%. The second study is a project carried out during three years involving 17 hospitals (Patel at al. 2006). Similarly, this study is based on the redesign of AD guidelines
that restrict the causes to initiate AD and limits the duration of the episodes. The results show a reduction
of the hours spent on AD in the system by about 75%.
Even though these results show a significant reduction of AD in a system, which increases the accessibility to emergency care, these publications do not provide information about the impact of reducing AD
in other measures, such as waiting time. It is known that AD can relieve congestion from an ED and reduce the average waiting time within a facility (Ramirez, Fowler, and Wu 2010). Therefore, restricting
the use of AD might have undesirable effect if appropriate actions are not implemented to reduce congestion. Furthermore, “No AD” laws approved in some regions might cause a rise in the waiting time and patients boarding (Massachusetts Nurse Newsletter 2009).
On the other hand, few analytical studies on the use of AD in multiple hospitals are available in the
literature. For example, Hagtvedt et al. (2009) models the AD decision of two hospitals using an analogy
of the Prisoner’s Dilemma and introduces a payoff function that includes a penalty for diverting ambulances. The authors found that a centralized system to control AD episodes is needed given that voluntary
cooperation might not be a robust approach.
Deo and Gurvich (2011) analyze the effect of AD in two hospitals using a model based on queuing
networks. The authors use the average waiting time in each ED as the performance measure of their interest. They observed that a centralized AD can be Pareto improving compared with not allowing AD in the
system. In addition, they introduce the rule that initiates AD when all the beds in the ED are occupied.
These results suggest that AD can bring benefits to the system if the guidelines are properly designed
based on a centralized system. However, their models do not include important aspects observed in emergency systems, such as non-stationary arrivals, transference from EDs to inpatient units and distance to
other hospitals.
This paper proposes a model based on simulation to analyze the effect of AD and destination policies
in the flow of patients in an emergency care system. Furthermore, a simulation-optimization approach using genetic algorithms is introduced as a method to design effective AD policies from a centralized perspective. The objective is to find Pareto improving policies that reduce the average-patient non-value
added time in each hospital of a system.

1252

Ramirez-Nafarrate, Fowler, and Wu
3

EMERGENCY CARE DELIVERY SYSTEM MODEL

This research is based on a discrete-event simulation model of an emergency care delivery system
(ECDS) that includes multiple hospitals and ambulances delivering patients to their EDs. There are three
main modules that allows the execution of the simulation and the evaluation of the decision policies:
emergency patient generation, ambulance destination decision and hospital simulation.
The emergency patient generation module creates patients that require transportation to an ED. This
module assigns a random location that represents the departure point from the ambulance
Then, the ambulance destination decision evaluates the options related to the open hospitals that can
receive the new patient. The decision of where to take the new patient is based on a destination policy.
After determining the destination hospital, the module schedules the arrival of the patient taking into account distance and a random variable associated with velocity.
The hospital simulation module executes the events related to the operations of each hospital, including ambulance and walk-in arrivals, end of treatments, direct admissions, etc. In addition, it keeps track of
the status of the hospital to initiate AD if the conditions of the policy under evaluation are satisfied. Figure 1 presents an overview of the model.

Figure 1: Overview of the simulation model of an ECDS
Each hospital in the model has a similar structure. They include an ED and one inpatient unit (IP).
Upon arrival to the ED, a severity level is assigned to the patient. This level determines the priority for
being placed in a bed. The probability of receiving a specific severity level depends on the arrival mode.
Patients arriving by ambulance are more likely to have higher severity level than walk-ins. The duration
of the treatment in the ED depends also on the severity level. Higher severity implies longer treatment.
After ending treatment in the ED, the patient can be discharged or be admitted to the IP unit. The admission probability is also correlated with the severity level. The IP unit receives direct admissions as well. If
a patient in the ED requires admission, but the IP does not have available beds, then the patient waits in
the bed of the ED (boarding) until an IP bed opens up. In addition, patients in the ED can leave without
treatment (LWOT) if they have waited for a long period. Figure 2 depicts the patient flow in each hospital
included in the model. Appendix A describes the input data for the generic hospital used in the model.
Two assumptions are made regarding the accessibility to emergency care in the system. First, if the
arrival of an ambulance with a patient is scheduled to the destination hospital and that hospital goes on
diversion before the arrival event takes place, then the patient is still accepted in the hospital. This avoids
redirecting patients while they are on the road. The second assumption avoids having all the hospitals in
the system on diversion at the same time. Thus, if the last open hospital reaches the condition to go on diversion, then all the hospitals go off diversion. This type of practice is commonly used in real settings
(AEMS 2000).

1253

Ramirez-Nafarrate, Fowler, and Wu

Figure 2: Patient flow within the hospitals included in the model
4

CENTRALIZED DESIGN OF AD POLICIES

This paper introduces a simulation-optimization approach to allow a centralized design of AD policies
that enables the improvement of the hospitals’ performance in the system. Particularly, this research is
based on the use of genetic algorithms (GA) to define the parameters of the AD policies for each hospital
in a system and it uses the simulation model described in Section 3 to evaluate the performance of the set
of policies.
Thus, the GA chromosome represents the union of AD policies for all the hospitals in the system.
Therefore, the recombination and mutation of chromosomes enables the exploration of sets of policies
that can improve the performance in the ECDS. Then, simulation evaluates the fitness of each chromosome by obtaining the average-patient non-value added time per hospital. Finally, the evolution of the GA
allows finding the set of policies that have the best performance after a finite number of generations. Figure 3 depicts the process of the centralized design of AD policies proposed.

Figure 3: Centralized design of AD policies using simulation-optimization
4.1

Performance Evaluation

The hypothesis of this research is based on the assumption that a smart allocation of ambulance patients,
through ambulance diversion and destination policies, can reduce the time that patients spend in suboptimal treatment at different stages of care. Therefore, the performance of the ECDS is evaluated by the vector that contains the average-patient non-value added time for each hospital in the system: (
,
,…,	
), where n is the number of hospitals in the system.
The average-patient non-value added time of a hospital is a measure that includes transportation,
waiting in the ED and boarding; and it is computed in the following form:
1254

Ramirez-Nafarrate, Fowler, and Wu
∑

,

,

(1)

where,
= Average patient non-value added time in hospital Hi
= Fraction of ambulance arrivals to hospital Hi
= Average transportation time of ambulance patients received at hospital Hi. This includes patients
whose final destination is Hi, and patients diverted from Hj to Hi, for all i ≠ j
, = Weight given to the average waiting time of patients with severity level k in hospital Hi
, = Average waiting time of patients with severity level k in hospital Hi
= Fraction of ED patients admitted to hospital Hi
= Average boarding time in hospital Hi
The first term on the right hand side of the equation takes into account the average transportation time
of ambulance patients only. The second term is a weighted average, based the on severity level, of all the
patients in the ED, except the patients that left without treatment (LWOT). The third term includes all the
patients that were admitted from the ED. Note that the components related to waiting and boarding considers walk-in patients.
Although the average proportion of ambulance arrivals to EDs is 15% (CDC 2010), the characteristics
of their patients can cause important disruptions to the flow of patients in a hospital. Therefore, an effective design and combination of AD and destination policies could smooth the patient flow, which will be
observed as an improvement on the performance vector.
4.2

Ambulance Diversion and Destination Policies

This paper compares three types of AD policies, each of them combined with two types of destination
policies.
Ambulance Diversion Policies:
1. No AD. Ambulance diversion is not implemented if this type of policy is used.
2. Simple AD. The simple AD policy initiates a period of AD when all the beds in the ED are occupied.
3. Optimized Single-Factor AD Policy (SF AD). This type of policy observes if a particular state
variable reaches a threshold in order to initiate an AD episode. The optimal thresholds are obtained via GA.
Ambulance Destination Policies:
1. Nearest Hospital (NH). With this policy, the patient is transported to the nearest hospital from the
emergency location.
2. Least Crowded Hospital (LCH). The patient is transported to the hospital with the fewest number
of patients waiting in the ED.
The first type of AD policy reflects the situation in some regions and the recommendations of the
medical community. The simple AD policy is suggested by Deo and Gurvich (2011) as Pareto improving
policies in their queuing analysis. The optimized single-factor AD policy is the core of this research. It is
based on a proposed structure for the policy that takes into account one of the main crowding variables
and it includes parameters to reevaluate and remove the diversion status. The next section provides a
deeper explanation of this type of policy.
The guidelines of EMS suggest to take a patient to the nearest appropriate hospital (ACEP 1999b).
Therefore, this research evaluates two types of destination policies related to this recommendation. The

1255

Ramirez-Nafarrate, Fowler, and Wu
nearest hospital policy could improve the first term of Equation 1, while the second policy could have an
effect of the second and third term.
4.3

Encoding of Single-Factor AD Policies

The AD policies proposed in this paper are based on the observation of one of the main variables that
triggers the diversion status in practice. These state variables are:
NQi: Number of patients waiting in the ED of hospital Hi.
NBi: Number of patients boarding in the ED of hospital Hi.
NIPBi: Number of beds available in the IP unit of hospital Hi.
Ramirez, Fowler, and Wu (2010) presented a bi-criteria analysis of single-factor policies based on
these variables for a single hospital. Those policies comprises two parameters to set or remove/reevaluate
the diversion status. Hence, the reevaluation could be continuous or at discrete points.
This paper improves the definition of an AD policy by considering three parameters: (Don, Doff, t).
The Don parameter represents a threshold on a state variable to set the diversion status on. The Doff parameter is another threshold on the same state variable to remove the diversion status. t is the reevaluation frequency after going on diversion. Diversion status can be removed only at a reevaluation point.
Since three state variables are considered in this research, then there are three types of single-factor
AD policies, one for each state variable. Therefore, the length of the AD chromosome that contains the
AD policies of all the hospitals in the ECDS is 10n, where n is the number of hospitals. The subchromosome that represents the AD policy of a particular hospital is depicted in Figure 4.
Gene
Variable

1
Pi

2
UNQi

3
LNQi

4
tNQi

5
UNBi

6
LNBi

7
tNBi

8
LNIPBi

9
UNIPBi

10
tNIPBi

Figure 4: Chromosome partition that represents an SF AD policy in one hospital
The first gene describes the type of factor to consider in the policy of hospital Hi. Thus, Pi = 1 implies
that AD policy of hospital Hi is based only on number of patients waiting in the ED (NQi); Pi = 2 indicates that AD is based on the number of patients boarding (NBi); and Pi = 3 means that AD is based on the
number of beds available in the IP unit (NIPBi). Therefore, the execution of an SF AD policy requires
values for three parameters. If the policy is type 1, then the parameters are in the genes 2, 3 and 4. If it is
type 2, then the genes of interest are 5, 6 and 7. If the type is 3, then the related genes are 8, 9 and 10.
The first of the three parameters that define an SF AD policy is a threshold that triggers the diversion
status (Don parameter in genes 2, 5 or 8). Thus, if policy is type 1, then the hospital Hi sets the diversion
status when NQi > UNQi. If it is type 2, then Hi goes on diversion when NBi > UNBi. If it is type 3, then diversion is set when NIPBi < LNIPBi. After going on diversion, the state of the system is reviewed every t
time units, represented by genes 4, 7 and 10 for policies type 1, 2 and 3, respectively.
The Doff parameter (represented in genes 3, 6 and 9) is a threshold used to decide the removal of the
diversion status at a review point. If policy is 1, then the diversion status is removed if NQi < LNQi. If policy is type 2, then diversion is removed if NBi < LNBi. If the policy is type 3, then diversion is removed if
NIPBi > UNIPBi. Note that for all the policies the threshold U is greater than the threshold L. Policy type 3
has the U and L interchanged because of the meaning of the state variable (number of available beds in
the IP).
An example of an SF AD policy is: “set the diversion status if there are at least 15 patients waiting in
the ED, reevaluate every hour after going on diversion and remove the diversion status if there are 5 or
less patients waiting”. This policy is encoded as:

1256

Ramirez-Nafarrate, Fowler, and Wu
Gene
Variable

1
1

2
15

3
5

4
60

5
Null

6
Null

7
Null

8
Null

9
Null

10
Null

In this example, genes 5 to 10 can take any value and the simulation code does not take them into account because the first gene specifies the type of policy.
4.4

Multi-Objective Genetic Algorithm

This research implements the NSGA-II algorithm (Deb et al. 2002). This algorithm selects the survivor
chromosomes based on a front number and a crowding distance. The front number of a specific policy P
is related to the number of policies which dominate P (domination count). The nondominated policies of a
set of policies have front number equal to one. Then, policies in front one are removed from the total set
and the process repeats. The new set of nondominated policies is assigned to front two. The process repeats until a front number is assigned to all the policies. The crowding distance is related to the diversity
of the policies. The crowding distance of a specific policy P is an estimation of the perimeter of the cuboid formed by the nearest policy neighbors of P. The policies with larger crowding distance are more
likely to be included in the parent selection since diversity encourages exploring areas with low density of
policies.
5

CASE STUDY: ECDS WITH THREE HOSPITALS

The AD and destination policies presented in Section 4.2 are used in a case study that comprises three
hospitals. Two configurations of random locations are presented; one of them assumes that the ECDS is
in a 10x10 squared-miles area (Random 1), while another assumes that the area is 20x20 squared-miles
(Random 2).
The hospital built for this research is a generic hospital that incorporates data from published papers
and other sources. Based on the generic hospital, two configurations of relative size are used in the experimentation: one assumes the same relative size (1:1:1) and another assumes different sizes for all the hospitals, one of them has 10% more arrivals than the generic and another 20% more arrivals than the generic
(1:1.1:1.2). The combinations of scenarios and strategies are summarized in Table 1.
Table 1: Scenarios and strategies used in the experimentation process.
Scenarios
Location (H1, H2, H3)
Random 1: (1.7, 9.2), (4.8, 3.8) & (8.5, 7.3)
Random 2: (19.2, 6.4), (6, 10.5) & (12.3, 18.9)

Relative Size
(H1: H2: H3)
1:1:1
1:1.1:1.2

Strategies
Diversion
Destination
Policies
Policies
No AD
NH
Simple AD
LCH
Optimized SF-AD

The results for the case study are shown in Table 2. It includes the average-patient non-value added
time per hospital for each strategy, the sum of the non-value added time in the system, the standard deviation and the percentage of time spent on diversion in each hospital. More than one solution can be seen
for the SF AD strategies because the multi-objective GA can produce multiple Pareto solutions.
For each scenario, the strategies that allow AD outperform the No AD strategy. Thus, AD can reduce
the total average-patient non-value added time. Furthermore, the SF AD policy proposed in this paper
produce Pareto improving solutions in most of the scenarios. The Simple AD policy have better performance than No AD, but the SF AD is better than the Simple policy.

1257

Ramirez-Nafarrate, Fowler, and Wu
Table 2: Results from the experimentation process
Average-Patient NVT
(mins)
Scenario

Random 1
1:1:1

Random 1
1:1.1:1.2

Random 2
1:1:1

Random 2
1:1.1:1.2

Strategy
No AD - NH
Simple AD - NH
SF AD - NH
SF AD - NH*
No AD - LCH
Simple AD - LCH
SF AD - LCH
SF AD - LCH*
No AD - NH
Simple AD - NH
SF AD - NH
No AD - LCH
Simple AD - LCH
SF AD - LCH***
No AD - NH
Simple AD - NH
SF AD - NH
SF AD - NH*
No AD - LCH
Simple AD - LCH
SF AD - LCH**
No AD - NH
Simple AD - NH
SF AD - NH**
SF AD - NH
No AD - LCH
Simple AD-LCH*
SF AD - LCH
SF AD - LCH*
SF AD - LCH***

H1
21.28
25.55
67.26
20.47
54.86
37.12
36.48
53.56
25.24
46.87
71.34
132.93
125.77
117.39
34.12
38.97
43.66
26.37
58.40
42.06
34.34
50.57
83.50
83.18
155.39
168.27
159.98
76.71
151.06
153.12

H2
155.63
86.00
52.54
101.09
41.47
43.32
42.78
36.56
273.94
197.28
155.17
123.55
126.18
106.46
144.28
81.91
75.64
104.73
43.88
46.15
36.89
317.08
238.65
226.75
189.56
156.33
153.01
61.11
131.86
144.20

H3
45.29
44.03
27.58
36.96
36.74
40.73
37.67
29.76
160.93
144.58
148.48
119.53
123.65
111.13
26.26
32.92
33.92
25.56
37.62
41.27
40.07
131.81
137.97
123.35
103.21
150.16
149.22
290.24
149.27
141.33

Percentage of time on
diversion
Total
NVT
222.21
155.59
147.38
158.52
133.08
121.17
116.93
119.88
460.11
388.73
374.99
376.01
375.61
334.98
204.66
153.79
153.21
156.66
139.90
129.48
111.30
499.47
460.12
433.28
448.17
474.76
462.20
428.05
432.19
438.66

Std. Dev
NVT
71.65
30.98
20.06
42.59
9.40
3.12
3.35
12.26
124.52
76.32
46.59
6.88
1.36
5.48
65.99
26.71
21.82
45.48
10.66
2.62
2.87
136.60
78.71
74.07
43.49
9.21
5.46
128.02
10.61
6.15

H1

H2

H3

6.33
8.77
10.45

22.43
33.38
23.43

11.36
21.55
16.64

6.24
7.57
9.50

10.01
0.82
32.10

7.67
5.41
26.37

2.24
12.92

22.63
31.63

19.05
25.43

3.35
2.89

5.61
18.71

5.95
13.25

10.45
10.55
13.89

22.31
24.57
22.68

8.24
8.90
11.21

7.56
9.71

10.42
15.58

6.84
3.40

3.46
3.92
5.59

18.11
18.14
23.71

12.57
12.31
17.65

3.41
44.70
10.67
3.60

4.52
55.61
20.80
9.91

4.65
3.30
15.48
5.70

*Dominates No AD strategy; **Dominates Simple AD strategy; ***Dominates No AD and Simple AD
strategies
Regarding the destination policies, LCH hospitals outperform NH. In addition, LCH can balance the
performance across hospitals, reducing the standard deviation of the average patient non-value added
time.
The reduction in the average-patient non-value added time using AD policies depends on the destination policy implemented. Thus, if NH is used, then the Simple AD policy reduces the total average-patient
non value-added time by 19.56% and the SF AD reduces it by 22.64%. If LCH is used, then the reduction
produced by Simple AD and SF AD are 4.79% and 13.33% respectively. In any case, the gaining of using
intelligent diversion is significant compared to No AD.
An important aspect to highlight related to the destination policy is that the case study assumes an urban area for the hospitals. If the analysis is conducted in a rural area, it is very likely that contradictory results would be observed. However, AD is not recommended in rural areas because the significant increase
in transportation would cancel the potential benefits of AD and it would jeopardize the health condition of
the patients.

1258

Ramirez-Nafarrate, Fowler, and Wu
6

CONCLUSIONS

This paper presented a centralized design of AD policies using GA and simulation to evaluate the performance. The AD policies are combined with destination policies in an ambulance flow control framework
that allows the allocation of ambulance patient in an ECDS. The findings suggest that the centralized design of diversion policies and effective destination rules can reduce the time that patients spend in suboptimal care, including the walk-in patients.
The results of the experimentation show that the proposed SF AD policies outperform not diverting at
all and they produce better results than a simple AD policy. These observations imply that an intelligent
strategy for implementing AD in a region can smooth the patient flow in the entire ECDS. The improvement depends on the destination policy that accompanies the AD policy. The LCH destination policy
outperforms NH. However, this might hold only in urban settings, like the one assumed in this research.
The centralized design of AD policies assumes a high level of collaboration among hospitals to improve the patient safety. This might be an issue in the real setting, but the efforts shown in the literature
suggest that healthcare organizations are willing to work together to bring benefits to their systems.
Even though the model used for this research considers a generic hospital built from data obtained in
the literature, the effectiveness of the proposed methodology does not depend on the model but in the encoding of the AD policy and the evaluation of the system.
Future extensions of this research include the optimization of multiple-factor AD policies, the evaluation of a destination policy that includes transportation and crowding factors simultaneously and the optimization of the destination policy using simulation-optimization.
A INPUT DATA FOR GENERIC HOSPITAL
The generic hospital described in Section 3 was built using C++ with information published in different
sources. The main sources of are: Cochran and Bharti (2006), Cochran and Roche (2009) and CDC
(2010). Figure 5 depicts the pattern of the arrivals considered for the model. The pattern observed in the
arrival rate is also identified in other papers and official reports across the United States (Burt, McCaig,
and Valverde 2006; CDC 2008). Note that ambulance arrivals comprise 15% of all the arrivals to an ED
according to Figure 7. This percentage is consistent with the averages published by the CDC (2010).
Green (2006) proposes a set of arguments to assume Poisson process for the arrivals to healthcare systems. Hence, this paper assumes Poisson process for all its arrivals. In order to schedule ambulance arrivals to an appropriate hospital, the transportation time is estimated by xM(l, Hi), where M(l, Hi) is the
Manhattan distance between the emergency location and the selected hospital, and  is the transportation
time per mile. This paper assumes a distribution for , such that  ~ Normal(1.25, 0.5). This implies that
the average transportation time is 1.25 minutes per mile, which is similar to the data presented by Google
Maps as transportation time per mile in Maricopa County, AZ. The severity level assigned to each patient depends on the arrival mode. Table 3 presents the percentages of each severity level.
The mean treatment time per severity level is shown in Table 4. This paper assumes that the treatment
time follows an Erlang distribution with shape parameter of 3.
After ending treatment in the ED, the patients can be admitted to the IP unit with a probability that
depends on the severity level. These probabilities are presented in Table 5. The overall admission percentage is 15% which is in the range the average seen in metropolitan areas in the United States (CDC
2010).
Direct admissions to IP occur according to a Poisson process with a mean of one admission per hour,
which is similar to the total external arrival rates of the hospital analyzed in Cochran and Bharti (2006).
The treatment time in the IP is also assumed to be an Erlang distribution with shape parameter equal to 3
and a mean of four days, which is similar to the data found by Cochran and Bharti (2006) and close to
mean length of stay in IP units according to the CDC (2010).

1259

Ramirez-Nafarrate, Fowler, and Wu

Mean arrival rate

Mean Arrival Rate to ED
10
8
6
4
2
0

Ambulance arrivals
Walk‐ins
0

2

4

6

8

10 12 14 16 18 20 22 24
Hour of the day

Figure 5: Arrival rate to a single ED
Table 3: Percentages for severity mix
Severity Level
1
2
3
4
5
Overall

Arrival Mode
Ambulance Walk-Ins
15
2
42
16
30
40
10
30
3
12
15
85

Overall
3.95
19.90
38.50
27.00
10.65

Table 4: Mean Treatment Times in the ED
Severity Level
1
2
3
4
5

Mean
Treatment
Time (min)
273
273
140
106
30

Table 5. Admission probabilities to IP
Severity Level
1
2
3
4
5
Overall

Admission Percentage
70
34
10
5
3
15

In order to model the LWOT patients, this paper incorporates an approach presented by Miller, Ferrin, and Shahi (2009). The LWOT routine consists of removing patients from the queue if they have not
been placed in a bed within 24 hours. This paper assumes that LWOT patients go home or visit a nonemergency physician; therefore, they are not scheduled to arrive to another hospital in the model.

1260

Ramirez-Nafarrate, Fowler, and Wu
The hospitals in the model have 20 beds in the ED and 200 IP beds. The number of beds considered
for the ED is similar to the median in the United States (CDC 2006) and the size of the IP unit is suitable
for a medium-size hospital.
The simulation length for the research is fixed to six months after a warm-up period of one month and
ten replications per strategy are considered. These parameters were defined after a set of pilot runs to obtain precise estimation of the performance measure of interest. In addition, Common Random Numbers
(Banks et al. 2010) are used to expose the different strategies to similar conditions and reduce the noise
among them.
REFERENCES
ACEP (American College of Emergency Physicians). 1999a. “Guidelines for Ambulance Diversion.” Accessed May 7, 2011. http://www.acep.org/content.aspx?id=30038.
ACEP (American College of Emergency Physicians). 1999b. “Emergency Ambulance Destination.” Accessed May 7, 2011. http://www.acep.org/content.aspx?id=29196.
ACEP (American College of Emergency Physicians). 2008. Emergency Department Crowding: HighImpact Solutions. Accessed May 7, 2011. www.acep.org/WorkArea/DownloadAsset.aspx?id=50026.
AEMS (Arizona Emergency Medical Systems, Inc.). 2000. “Transfer of Care. Guideline Statements on
Prehospital Diversion.” Chapter 8 in Red Book. Accessed May 7, 2011.
http://www.aems.org/aems/redbook/chap8.pdf.
Asamoah, O. K., S. J. Weiss, A. A. Ernst, M. Richards, and D. P. Sklar. 2008. “A Novel Diversion Protocol Dramatically Reduces Diversion Hours.” The American Journal of Emergency Medicine
26:670-675.
Banks, J., J. S. Carson, B. L. Nelson, and D. M. Nicol. 2010. Discrete-Event System Simulation. 5th ed.
Upper Saddle River, New Jersey: Prentice-Hall, Inc.
Burt, C. W., L. F. McCaig, and R. H. Valverde. 2006. “Analysis of Ambulance Transports And Diversions Among US Emergency Departments.” Annals of Emergency Medicine 47(4):317-326.
CDC (Centers for Disease Control and Prevention). 2006. “Staffing, Capacity, and Ambulance Diversion
in Emergency Departments: United States, 2003-04.” In Advance Data from Vital and Health Statistics 376.
CDC (Centers for Disease Control and Prevention). 2008. “National Hospital Ambulatory Medical Care
Survey: 2006 Emergency Departments Summary.” National Health Statistics Reports 7.
CDC (Centers for Disease Control and Prevention). 2010. “National Hospital Ambulatory Medical Care
Survey: 2007 Emergency Department Summary.” In National Health Statistics Report 26.
Cochran, J. K., and A. Bharti. 2006. “A Multi-Stage Stochastic Methodology for Whole Hospital Bed
Planning under Peak Loading.” International Journal of Industrial and Systems Engineering 1:8-36.
Cochran, J. K., and K. T. Roche. 2009. “A Multi-Class Queuing Network Analysis Methodology for Improving Hospital Emergency Department Performance.” Computers & Operations Research
36:1497-1512.
Deb, K., A. Pratap, S. Agrawal, and T. Meyarivan. 2002. “A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II.” IEEE Transactions on Evolutionary Computation 6(2):182-197.
Deo, S., and I. Gurvich. 2011. “Centralized vs. Decentralized Ambulance Diversion: A Network Perspective.” Management Science 57(7):1300-1319.
GAO. 2003. Hospital Emergency Departments: Crowded Conditions Vary among Hospitals and Communities. United States General Accounting Office. Report to the Ranking Minority, Committee on Finance, U.S. Senate.
Green, L. V. 2006. “Queuing Analysis in Healthcare.” In Patient Flow: Reducing Delay In Healthcare
Delivery, edited by R.W. Hall, 281-307. Springer’s International Series.
Hagtvedt, R., P. Griffin, P. Keskinocak, M. Ferguson, and F. T. Jones. 2009. “Cooperative Strategies to
Reduce Ambulance Diversion.” In Proceedings of the 2009 Winter Simulation Conference, edited by

1261

Ramirez-Nafarrate, Fowler, and Wu
M.D. Rossetti, R.R. Hill, B. Johansson, A. Dunkin, and R.G. Ingalls, 1861-1874. Piscataway, New
Jersey: Institute of Electrical and Electronics Engineers, Inc.
Massachusetts Nurse Newsletter. 2009. “State’s ‘No Diversion Policy’ Is Putting Strain on Massachusetts
Hospitals.” April 2009:8-9.
Miller, M., D. Ferrin, and N. Shahi. 2009. “Estimating Patient Surge Impact in Several Regional Emergency Departments.” In Proceedings of the 2009 Winter Simulation Conference, edited by M. D.
Rossetti, R. R. Hill, B. Johansson, A. Dunkin, and R. G. Ingalls, 1906-1915. Piscataway, New Jersey:
Institute of Electrical and Electronics Engineers, Inc.
Patel, P. B., R. W. Derlet, D. R. Vinson, M. Williams, and J. Wills. 2006. “Ambulance Diversion Reduction: The Sacramento Solution.” American Journal of Emergency Medicine 24:206-213.
Pham, J. C., R. Patel, M. G. Millin, T. D. Kirsch, and A. Chanmugam. 2006. “The Effects of Ambulance
Diversion: A Comprehensive Review.” Journal of the Academic Emergency Medicine 13(11):12201227.
Ramirez, A., J. W. Fowler, and T. Wu. 2010. “Bi-Criteria Analysis of Ambulance Diversion Policies.” In
Proceedings of the 2010 Winter Simulation Conference, edited by B. Johansson, S. Jain, J. MontoyaTorres, J. Hugan, and E. Yucesan, 2315-2326. Piscataway, New Jersey: Institute of Electrical and
Electronics Engineers, Inc.
Vilke, G. M., L. Brown, P. Skogland, C. Simmons, and D. A. Guss. 2004a. “Approach to Decreasing
Emergency Department Ambulance Diversion Hours.” Journal of Emergency Medicine 26(2):189192.
Vilke, G. M., E. M. Castillo, M. A. Metz, L. U. Ray, P. A. Murrin, R. Lev, and T. C. Chan. 2004b.
“Community Trial to Decrease Ambulance Diversion Hours: The San Diego County Patient Destination Trial.” Annals of Emergency Medicine 44(4):295-303.
AUTHOR BIOGRAPHIES
ADRIAN RAMIREZ-NAFARRATE is a PhD candidate in Industrial Engineering in the School of
Computing, Informatics and Decision Systems Engineering at Arizona State University. His research interests include modeling, simulation and analysis of healthcare delivery systems. He received a MS in
Manufacturing Systems at ITESM and a BS in Industrial Engineering at Universidad de Sonora, both in
Mexico. His email address is adrian.ramirez@asu.edu.
JOHN W. FOWLER is a Professor in the Operations Research and Production Systems group of the
School of Computing, Informatics and Decision Systems Engineering at Arizona State University. His research interests include modeling, analysis, and control of manufacturing and service systems. He is a
Fellow of the Institute of Industrial Engineers and is the SCS representative on the Board of Directors of
the Winter Simulation Conference. He is an Area Editor of the Transactions of the Society for Computer
Simulation International, an Associate Editor of IEEE Transactions on Semiconductor Manufacturing,
and Editor of IIE Transactions on Healthcare Systems Engineering. His email address is
john.fowler@asu.edu.
TERESA WU is an Associate Professor of the School of Computing, Informatics and Decision Systems
Engineering at Arizona State University. She received her Ph.D. in Industrial Engineering from the University of Iowa in 2001. Her current research interests include: distributed decision support, distributed
information system, healthcare informatics and data fusion.. Professor Wu has over 40 articles published
(or accepted) in such journals as International Journal of Production Research, Information Science,
IEEE Transactions on Engineering Management, ASME: Journal of Computing and Information Science
in Engineering. She serves on the Editorial Review Board for International Journal of Production Research, IEEE Transactions on Engineering Management, Computer and Standard Interface, International
Journal of Electronic Business Management. Her email address is teresa.wu@asu.edu.

1262

J Digit Imaging (2015) 28:362–367
DOI 10.1007/s10278-014-9743-7

Automated Detection of Z-Axis Coverage with Abdomen-Pelvis
Computed Tomography Examinations
Min Zhang & Clinton Wellnitz & Can Cui &
William Pavlicek & Teresa Wu

Published online: 21 November 2014
# Society for Imaging Informatics in Medicine 2014

Abstract Excessive cephalocaudal anatomic (Z-axis) coverage can lead to unnecessary radiation exposure to a patient. In
this study, an automated computing model was developed for
identifying instances of potentially excessive Z-axis coverage
with abdomen-pelvis examinations. Eight patient and imaging
attributes including patient gender, age, height, weight, volume CT dose index (CTDIvol), dose length product (DLP),
maximum abdomen width, and maximum abdomen thickness
were used to build a feedforward neural network model to
predict a target Z-axis coverage whether it is an excessive or
non-excessive Z-axis coverage scans. 264 CT abdomen-pelvis
exams were used to develop the model which is validated
using 10-fold cross validation. The result showed that 244 out
of 264 exams (92.4 %) correctly predicted Z-axis excessive
coverage. The promising results indicate that this tool has the
potential to be used for CT exams of the chest and colon,
urography, and other site-specified CT studies having defined
limited length.
Keywords CT dose . Z-axis coverage . Monitoring . Feature
analysis . Neural network . Quality control

Background
Computer-aided tomography (CT) is undergoing an intensive
refinement of x-ray radiation dose administration controls
with the goal of markedly lowering the patient dose [1]. A
M. Zhang : C. Cui : T. Wu
School of Computing, Informatics Decisions and System
Engineering, Arizona State University, Tempe, AZ, USA
M. Zhang : C. Wellnitz : W. Pavlicek (*)
Department of Radiology, Mayo Clinic, 13400 E. Shea Boulevard,
Scottsdale, AZ 85259, USA
e-mail: pavlicek.william@mayo.edu

number of advanced technologies and practice changes are
now used to lower radiation dose with CT, such as automatic
tube current modulation (ATCM) [2], a variety of de-noising
techniques enabling reduced patient exposure [3], reducing
kVp for smaller patient size [4], and reducing the number of
contrast phase acquisitions [5]. One additional source of potential unnecessary exposure to a patient can occur due to
excessive Z-axis coverage. Z-axis coverage (extent of CT xray exposure to the patient in the cephalocaudal direction)
primarily depends upon the patient size and the specific exam
being performed (i.e., the CT protocol). For the abdomenpelvis exam, the most frequently encountered body CT use, it
also depends on (1) the movement of the diaphragm during
breathing, (2) the possibility of actual patient movement on
the table, (3) the need to always fully include all the required
anatomy, and (4) the CT scanner software controls for refinement of the extent of Z-axis coverage (indicated by the defined
scan range) in comparison to the actual radiation-exposed
anatomy (overscan compensation). Some clinical sites standardize the anatomical features to be selected by the technologists when positioning the defined scan range acquisition
box. Technologists recognize any “missed anatomy” results
in the need to acquire a second acquisition—thereby causing
added (overlapped) exposure to the patient and a second
acquisition to be merged with the CT exam for interpretation.
A second scan to acquire a few added centimeters of anatomy
not only results in overlapped radiation being given to “imaged” tissue but also includes added overlapped radiation due
to “overscan”—the term used to make note that the radiation
field is larger than the Z-axis detector coverage in order to
fully expose the detectors. As a result, to be fully confident
that complete coverage occurs the first time, it is possible that
an overly generous setting of defined scan range is chosen—
one which may well result in greater Z-axis coverage than is
needed. Some recent studies note that as much as 98 % of
body CT scans exceeds the predefined anatomic boundaries of

J Digit Imaging (2015) 28:362–367

363

The location of the superior most
aspect of left or right hemidiaphragm,
whichever is most superior (red
circled in Fig. 1).
A location less than or equal to 2 cm
superior to the location of the superior
most aspect of the left or right
hemidiaphragm. The technologists

ideally start scanning no more than
1 cm superior to the ideal start
position, but it is acceptable to start
up to 2 cm.
Threshold of excessive A location greater than 2 cm superior
start position
to the location of the superior most
aspect of the left or right
hemidiaphragm, whichever is more
superior.
Ideal stop position
The location of the inferior most
aspect of left or right ischial
tuberosity, whichever is more inferior
(red circled in Fig. 2).
Acceptable stop
A location less than or equal to 2 cm
position
inferior to the location of the inferior
most aspect of the left or right ischial
tuberosity, whichever is more
inferior.
Threshold excessive A location greater than 2 cm inferior
stop position
to the location of the inferior most
aspect of the left or right ischial
tuberosity, whichever is more
inferior.
Ideal Z-axis coverage A scan length equal to the
absolute value of the difference
between the ideal start and stop
positions as shown in Fig. 3a. (A
Z-axis coverage may be ideal per
this definition, but the start and
stop positions may not.)
Acceptable Z-axis
A scan length less than or equal to the
coverage
absolute value of the difference
between the acceptable start and stop

Fig. 1 Ideal start position of CT abdominal scan. Slice location of first
image with subdiaphragmatic tissue (left or right) is defined as ideal start
position of CT scan

Fig. 2 Ideal stop position of CT abdominal scan. Slice location of last
image with ischial tuberosity bone (left or right) is defined as ideal stop
position of CT abdominal scan

their respective scan protocols [2, 6]. Therefore, a method that is able to automatically detect excessive Z-axis
coverage may provide a helpful quality assurance (QA)
tool for potentially reducing this source of unnecessary
patient exposure.

Methods
Excessive Z-Axis Coverage
There is no agreement or standardized definition of excessive
Z-axis coverage of CT exam in the literature for the routine
abdomen-pelvis examination. A reason for this may be that
this type of exam is a breath hold acquisition, so inpatients or
respiratory-compromised patients may be found to have generous Z-axis coverage that is appropriate due to their condition. Based on a sample of the normal variation observed with
the outpatient population by the radiologists in our
hospital, 2 cm was chosen to determine the acceptable
start and stop positions. To aid in the specification of
appropriate versus excessive Z-axis coverage, Z-axis
coverage for the abdomen-pelvis CT examination are
defined as follows:
“Ideal” start position

Acceptable start
position

364

J Digit Imaging (2015) 28:362–367

Fig. 3 Different situations of Zaxis coverages. Transparent
boxes are the defined scan ranges
for Z-axis coverage. a Ideal Z-axis
coverage. b Acceptable Z-axis
coverage. c Excessive Z-axis
coverage

Excessive Z-axis
coverage

positions but greater than the ideal Zaxis coverage as shown in Fig. 3b. (A
Z-axis coverage may be acceptable
per this definition, but the start and
stop positions may not be.)
A scan length greater than the
absolute value of the difference
between the acceptable start and stop
positions as shown in Fig. 3c.

In this study, the actual Z-axis coverages are calculated
manually as the distance of first axial image slice to the last
axial image slice which is based on actual patient CT and
anatomical data from picture archiving and communication
system (PACS).
For newer CT devices, the actual Z-axis coverage information is provided in enhanced Digital Imaging and Communications in Medicine Structured Report (DICOM SR) object.
The start and stop positions are available in the DICOM
header (TAG: 0008,0104) of Radiation Dose Structure Report
(RDSR) as shown in Table 1. Therefore, the actual Z-axis
coverage can be obtained by calculating the distance between
the top Z location and the bottom Z location of the
scanning length. For those scanners not having RDSR,
Z-axis coverage can be either measured manually by
PACS or measured automatically by using image processing techniques [13, 14]. At this time, the anatomical
landmarks of the patient that correspond to an irradiation event are not available in DICOM and thus no
reference to patient coverage can be directly obtained.
Table 1
RDSR

Z-axis coverage information from new DICOM standard of

(0008,0104)—Code meaning [LO][34][1]: top Z location of scanning
length
(0008,0104)—Code meaning [LO][36][1]: bottom Z location of scanning
length

We specify that should our measured Z-axis coverage is
greater than and/or equal to ideal Z-axis coverage and less than
and/or equal to acceptable Z-axis coverage, it is not
excessive Z-axis coverage. Greater values of coverage
are defined as excessive Z-axis coverage and should be
detectable when using a QA tool. The real patient
examples are shown in Fig. 4.

Data Collection and Features Selection
This institutional review board and HIPAA compliant study
included 264 patients having routine abdominal studies between year 2012 and year 2014. The age of the patients in this
study ranges from 19 to 93 and the gender distribution includes 140 females and 124 males.
In abdominal exams, six patient physical features,
including age, gender, height, weight, lateralanteroposterior abdominal dimension (thickness), and
posteroanterior-mediolateral abdominal dimension
(width) of the patient, and two radiation dose features,
volume CT dose index (CTDIvol) and dose length
product (DLP), were used. As a result, eight features
can be collected for the predictive model development.
DICOM Index Tracker (DIT) [7] was used to obtain
patient gender, age, height, weight, CTDIvol, and DLP
data. GE Centricity (our PACS) provided values of
maximum abdominal width and maximum abdominal
thickness information as individually measured using a
measurement tool. Examples of these measurements are
given in Figs. 5 and 6. Note that our use of patient
“width” as a feature for the statistical approach may be
a source of error as this measurement is computed for a
plane at isocenter, while a patient’s projected image size
depends upon the source to object distance and may
differ from isocenter. Here we used the measured width
as an estimation to approximate the real abdominal
width of the patient in developing the model. It is our

J Digit Imaging (2015) 28:362–367

365

Fig. 4 Real patient Z-axis
coverages. a Not excessive Z-axis
coverage. b Excessive Z-axis
coverage

intention to study the impact of the estimation on model
performance as a future work.
To validate the model, the ideal Z-axis coverage and
acceptable Z-axis coverage were manually measured.
Thus those exams found having excessive Z-axis coverage were formally identified for model building and
training purposes.

and oversampled while the data from majority class is randomly eliminated and downsampled. This results in the number of instances from minority class being reasonably similar
to the number of instances from majority class. After resampling process, the class distribution is balanced and
uniformed.
Feedforward Neural Networks

Imbalanced Data Resampling
In this study, among 264 cases, it is observed that 237 out of
264 patients do not have excessive Z-axis coverage resulting
in the dataset being imbalanced. When imbalanced data sets
are presented, most data learning algorithms may fail to provide favorable accuracies to determine the classes (e.g., excessive Z-axis coverage vs. non-excessive Z-axis coverage)
and the data [8]. To handle the imbalanced classification, a
statistically appropriate way is to use resampling methodology
in which data from the minority class is randomly repeated

Artificial neural network (ANN) model simulates biological
neural networks of human brain for decision-making (supervised learning algorithm) as a type of supervised learning. The
type of ANN used in this study was feedforward neural
network with back-propagation learning algorithm [9].
Feedforward neural network is composed of three different
layers: input layer, hidden layers, and output layer, and there is
no feedback between layers (thus “feedforward”). The input
layer is formed of attributes of input data; the hidden layer
extracts important features contained in the input data, while

Fig. 5 Maximum abdominal width measurement. Width is measured as
the widest skin to skin horizontal dimension below the diaphragm but
above the iliac crests

Fig. 6 Maximum abdominal thickness measurement. Thickness is
measured as the thickest skin to skin horizontal dimension below the
diaphragm but above the iliac crests

366

J Digit Imaging (2015) 28:362–367

The maximum number of epochs is set to 600 when the model
is not converging.
As shown in Table 2, true positive rate (TP rate), false
positive rate (FP rate), precision, and recall were used as
metrics to evaluate the performance of ANN model on the
data. For excessive class, the TP rate is 0.942, which means
that 94.2 % of true positive excessive Z-axis scanners can be
retrieved and identified by our model. That is very important
to quality assurance of radiation dose. In our test data set, it
shows that 244 instances out of 264 are classified correctly
whereas only 20 of them are misclassified. The results show
that this model is very promising and 92.42 % of the data was
classified correctly.

Fig. 7 Diagram of feedforward neural network model with five hidden
units and two outputs

the output classes form the output layer. The number of hidden
layers and number of hidden units were determined following
the rules in [10]. The neural network model in final use is
given in Fig. 7.

Experimental Results
First, the eight features of input data were standardized (data
was subtracted by their mean and divided by their standard
deviation) for the purpose of improvement of model performance. Next, the resampling method was performed as we
stated above. After standardization and resampling, 10-fold
cross validation was used in which data was equally divided
into 10 folds. In each experiment, onefold data was used for
testing while the other ninefold data were used as training
data. A total of 10 experiments were conducted. Results from
each experiment were accumulated and summarized as
final result.
The ANN model available from WEKA [10] is used. For
the parameter settings of neural networks, the logistic sigmoid
function was chosen as transfer function; the learning rate is
set to be 0.2, while the momentum is set to be 0.1 after tuning.

Table 2

Discussion
Ideally, a tool that could automate a correct Z-axis
coverage for each CT scanner with a minimum of operator interaction would be very helpful. This tool
would require some landmarking of the patient on the
patient support system. If not available a priori an
acquisition, a tool that is accurate and available for
use as a background review and process QA of a
clinically used CT scanner can provide an opportunity
for radiation dose reduction [11, 12]. Its help will be to
elevate awareness and improve technologist selection of
defined scan range and Z-axis coverage for the
abdomen-pelvis CT exam. This approach can be applied
to protocol-specific measurement of Z-axis coverage
including CT brain exams and CT chest exams which
have clearly defined boundaries. However, potential limitations may exist with the ANN predicting Z-axis coverage in pediatric patients, since size is quite variable
among pediatric patients. It remains to be seen if the
tissue contrast is sufficient to provide actionable results.
In this study, the maximum width and maximum
thickness of abdomen were determined manually using
the PACS measurement tool. However, this process
could be automated by using an image processing technique that includes the consideration of the effect of
magnification of the CT radiograph [13, 14]. While
eight attributes were used and were shown to be

Detection result on 264 CT abdominal exams

Class

TP rate

FP rate

Precision

Recall

F measure

ROC area

Excessive
Not excessive

0.942
0.909

0.091
0.058

0.898
0.949

0.942
0.909

0.919
0.929

0.905
0.905

Correctly classified instances 244 of 264, percentage 92.4242 %. Incorrectly classified instances 20 of 264, percentage 7.5758 %

J Digit Imaging (2015) 28:362–367

reasonably predictive of instances of truly excessive (as
defined) Z-axis coverage, other feature sets could be use
or more could be added to further improve the predictive accuracy.

Conclusion
This study indicates that a model using eight attributes can
provide high performance for routine monitoring of Z-axis
anatomical coverage with abdomen-pelvis CT exams. An
abdomen-pelvis exam identified by this tool as potentially
having “excessive” Z-axis coverage is correctly identified
about 92.4 % of the time. This quality assurance tool can be
used with CT exams of the chest and colon, urography, and
other site-specified CT studies having defined limited length.

References
1. Yu L, Liu X, Leng S, Kofler JM, Ramirez-Giraldo JC, Qu M,
Christner J, Fletcher JG, McCollough CH: Radiation dose reduction
in computed tomography: techniques and future perspective.
Imaging Med 1:65–84, 2009
2. Kalra MK, Maher MM, Toth TL, Kamath RS, Halpern EF, Saini S:
Radiation from “extra” images acquired with abdominal and/or pelvic CT: effect of automatic tube current modulation1”. Radiology
232(2):409–414, 2004
3. Manduca A, Yu L, Trzasko JD, Khaylova N, Kofler JM, McCollough
CM, Fletcher JG: Projection space denoising with bilateral filtering
and CT noise modeling for dose reduction in CT. Med Phys 36(11):
4911–4919, 2009

367
4. Szucs-Farkas Z, Verdun FR, von Allmen G, Mini RL, Vock P:
Effect of X-ray tube parameters, iodine concentration, and
patient size on image quality in pulmonary computed tomography angiography: a chest-phantom-study. Investig Radiol
43(6):374–381, 2008
5. Graser A, Johnson TR, Chandarana H, Macari M: Dual energy CT:
preliminary observations and potential clinical applications in the
abdomen. Eur Radiol 19(1):13–23, 2009
6. Liao EA, Quint LE, Goodsitt MM, Francis IR, Khalatbari S, Myles
JD: Extra Z-axis coverage at CT imaging resulting in excess radiation
dose: frequency, degree and contributory factors. J Comput Assist
Tomogr 35(1):50, 2011
7. Wang S, Pavlicek W, Roberts CC, Langer SG, Zhang M, Hu M,
Morin RL, Schueler BA, Wellnitz CV, Wu T: An automated DICOM
database capable of arbitrary data mining (including radiation dose
indicators) for quality monitoring. J Digit Imaging 24(2):223–
233, 2011
8. Haibo H, Garcia EA: Learning from imbalanced data. IEEE Trans
Knowl Data Eng 21(9):1263–1284, 2009
9. Sandberg IW: Nonlinear dynamical systems: feedforward neural
network perspectives. John Wiley & Sons, 2001
10. Hall M, Frank E, Holmes G, Pfahringer B, Reutemann P, Witten IH:
The WEKA data mining software: an update. ACM SIGKDD Explor
Newsl 11(1):10–18, 2009
11. McCollough CH: Automated data mining of exposure information
for dose management and patient safety initiatives in medical imaging. Radiology 264(2):322–324, 2012
12. Hara AK, Wellnitz CV, Paden RG, Pavlicek W, Sahani DV: Reducing
body CT radiation dose: beyond just changing the numbers. Am J
Roentgenol 201(1):33–40, 2013
13. Ikuta I, Warden GI, Andriole KP, Khorasani R, Sodickson A:
Estimating patient dose from X-ray tube output metrics: automated measurement of patient size from CT images enables
large-scale size-specific dose estimates. Radiology 270(2):472–
480, 2014
14. Cheng PM, Vachon LA, Duddalwar VA: Automated pediatric abdominal effective diameter measurements versus age-predicted body
size for normalization of CT dose. J Digit Imaging 26(6):1151–1155,
2013

3D Small Structure Detection in Medical Image Using Texture
Analysis
Fei Gao, Min Zhang, Member IEEE, Teresa Wu and Kevin M. Bennett


Abstract—Small structure segmentation from medical images
is a challenging problem yet has important applications.
Examples are labeling cell, lesion and glomeruli for disease
diagnosis, just to name a few. Though extensive research has
proposed various detectors for this type of problem, most are 2D
detectors. Recently, we have developed a Hessian based 3D
detector to segment small structures from medical images (e.g.,
MRI). In our detector, two 3D geometrical features: regional
blobness and flatness, in conjunction with the intensity features
are fully utilized to serve the segmentation purpose. The
objective of this research is to further improve the 3D detector
with additions of texture features. Medical images contain rich
information which can be presented as texture, the local
characteristics pattern of image intensity. We hypothesize the
Hessian based detector extended with the 3D texture features is
expected to have improved performance in segmenting small
structures. To thoroughly evaluate the contributions from the
textual features, 25 synthetic images and 6 real world rat MR
images are studied. It is observed the combination of intensity,
blobness, and two texture features: intensity standard deviation
and entropy improves performance in synthetic dataset by
about 19% in F-score, and performs as well as other detectors
on rat MR images.

I. INTRODUCTION
3D small structure (also known as blob) detection is a
challenging task in medical imaging research because: 1)
medical images tend to be noisy; 2) small blobs tend to be
miss-classified against background and other structures in the
images and 3) small blobs in images may have large intensity
variations [1]. Though difficult, it attracts great attention due
to its important applications, for example, cell detection in
histopathology/fluoroscopic images [2] or MR images [3],
exudative lesions detection in retinal images [4], and
glomeruli detection in MR kidney images [5][6][7]. A variety
of blob detection methods have been developed, including the
classic approach Laplacian of Gaussian (LoG) [8] and
generalized Laplacian of the Gaussian(gLoG) [9]. We want to
note that most of these detectors are restricted to 2D medical
images or a slice of a 3D image because of the computational
limitation. To address this issue, Zhang et al. [1][6] develop an
efficient method termed Hessian Based Difference of
Gaussian (HDoG). In HDoG, a 3D raw image is first
smoothed with a DoG (Difference of Gaussian)
*Research supported by NIH DK091722 and a grant from the NIH
Diabetic Complications Consortium.
Fei Gao and Teresa Wu are with School of Computing, Informatics, and
Decision Systems Engineering, Arizona State University, Tempe, AZ 85287
USA (e-mail: fgao16@asu.edu)(Corresponding author: Teresa Wu, phone:
480-965-4157; e-mail: teresa.wu@asu.edu).
Min Zhang is with Department of Radiology, Mayo Clinic, Scottsdale,
AZ 85259 USA (e-mail: zhang.min@mayo.edu).
Kevin. M. Bennett is with Department of Biology, University of Hawaii at
Manoa, Manoa, HI 96822 USA (e-mail: kevinben@hawaii.edu).

978-1-4577-0220-4/16/$31.00 ©2016 IEEE

transformation, a Hessian pre-segmentation is then applied
and blob candidates are obtained. This pre-segmentation can
greatly reduce the size of the problem enabling the 3D
geometric feature extraction. In HDoG, two 3D geometric
features: blobness and flatness, and one average intensity
feature are extracted for the candidates which are then
clustered with an un-supervised method for the final
segmentation. Here blobness and flatness are to describe
blob’s 3D spatial shape information, and average intensity is
the summary of voxel intensities within blob. Zhang et al.
conducted extensive comparison experiments and concluded
HDoG is capable to label 3D small structures from the
imaging with expected performance [1][6]. While successful,
the experiments also showed the F-score (a common metric
for accuracy assessment) of HDoG ranges from 0.75 to 0.85
which motivates us to explore ways to further improve the
detectability of HDoG.
It is noted an emerging field in medical imaging research
lately is texture analysis which is to derive multiple texture
features in hoping to better describe and understand image
properties. However, similar to that of blob detector, efforts so
far have been mainly focusing on 2D textures due to the
computing concern. In this research, we note that HDoG is
capable to delineate blob candidates in 3D. Along with the 3D
geometric features extraction, texture features shall be derived.
We thus introduce five texture features (skewness, intensity
standard deviation, entropy, kurtosis, uniformity) associated
with the distribution of image intensity from the 3D blob
candidates. To comprehensively evaluate the contributions of
the features, 256 feature sets (28) are constructed. Synthetic
images are first studied to identify the outperforming feature
sets. In the second experiment, we focus on the feature sets
that selected in first experiment and validate them using 6 MR
rat kidney images. We observe average intensity, blobness,
intensity standard deviation and entropy together perform well
on the 3D MR images.
In conclusion, the main contributions of this research are
two folds. First, five textual features in 3D are first introduced
for small structures segmentation. Second, a thorough
investigation on the feature sets including texture features and
geometric features is conducted with the outperforming
feature set(s) being identified. The remainders are organized
as follows: Section II describes our data sets used in
experiments, followed by features description in Section III.
Detailed experiment process and results are shown in Section
IV. Conclusions are presented in Section V.
II. IMAGE DATA ACQUISITION
Two image datasets: 25 3D synthetic images and 6 3D MR
rat kidney images are used in this paper to test the performance
of each feature set. The main differences between synthetic

6433

images and rat kidney images are as follows; first, in synthetic
data set, we have all the true blobs labeled while not in MR
images. This is also the reason that we quantify the accuracy
of our method on synthetic images. Second, there are different
types of false blobs in MR images that caused by noise, blood
flowing or other small cells, while in synthetic we just model
all these false blobs as Gaussian noise.

E. Skewness
Skewness is a measure of the asymmetry of the probability
distribution of the real value intensity about its mean.

A. 3D Synthetic Image Data
In this experiment, 25 3D images are randomly generated.
During the simulation, a 3D Gaussian function is implemented
to generate the intensity distributions of blobs with size 𝜎. The
radius of the blobs is estimated by 2𝜎 + 0.5 voxels by
observation. In this experiment, N=30000 blobs with size
σ = 1 are randomly placed in a 3D matrix with size 256 ×
256 × 256 to generate those 25 images. The blob size σ and
blob number N are set based on the fact that real rat kidney
glomerulus count is around 30000 and radius is around 2.5
voxels [5]. The noise is added randomly replacing the voxel
intensity with normal distributed value to more realistically
mimic MR images. One sample slice of these images is shown
in Figure 4 (A).

Before introducing the blobness and flatness, we’d like to
introduce regional Hessian matrix of DoG-transformed
(smoothed) blob candidates T, which is defined as:

B. 3D Rat Image Data
The experimental procedures involving animal models
described in this paper were approved by the Institutional
Animal Care and Ethics Committee. Six kidney glomeruli are
magnetically labeled using the cationic ferritin (CF)
nanoparticle, derived from horse spleen ferritin (Sigma
Aldrich, St. Louis, MO). Male Sprague Dawley Rats (215–
245 g) are anesthetized using inhaled isoflurane and given
three intravenous bolus injections of CF, (5.75 mg/100 g total
in phosphate buffered saline), spaced 1.5 h apart. The rats are
then euthanized under anesthesia by transcardial perfusion of
saline followed by 10% formalin. Kidneys are removed and
stored and imaged in glutaraldehyde. MRI is performed using
a Varian (Agilent, Palo Alto, CA) 800 MHz NMR with an 89
mm bore using a three-axis imaging gradient set. Then a 3D
gradient-echo image is acquired and reconstructed with image
size 256 × 256 × 256 [1]. One sample slice of these images
is shown in Figure 4 (C).

G. Regional Blobness
Regional Blobness RT is a modified regional feature that
based on blob candidate’s regional Hessian matrix, it
describes the likelihood of being a blob with improved
efficiency of computation.

F. Uniformity
Uniformity is a measure for the uniformity of intensity
distribution.

H T ( D oG nor ( x , y , z ; t )) 



( x , y , z ) T

  2 D oG nor ( x , y , z ; t )

2
x

2
  D oG ( x , y , z ; t )
nor

xy

  2 D oG ( x , y , z ; t )
nor

xz


2

 D oG nor ( x , y , z ; t )
xy
2

 D oG nor ( x , y , z ; t )
y

2

2

 D oG nor ( x , y , z ; t )
yz

 D oG nor ( x , y , z ; t ) 

xz

2
 D oG nor ( x , y , z ; t ) 

yz

2
 D oG nor ( x , y , z ; t ) 

2
z

2

(1)

Next, we let 𝜆1 , 𝜆2 , 𝜆3 be the eigenvalues of the regional
Hessian matrix, denoting the semi-axis lengths of the
ellipsoid.

2

RT 

3  d et( H T )

(2)

3

pm ( H T )

Where pm(HT)=𝜆1 𝜆2 + 𝜆2 𝜆3 + 𝜆1 𝜆3 , using [14], pm(HT) can
be computed as:
 H T1,1
det  2 ,1
 HT

1,2
 H T2 ,2
HT 
 det  3,2
2 ,2 
HT 
 HT

2 ,3
 H T1,1
HT 
 det  3,1
3,3 
HT 
 HT

1,3
HT 
3,3 
HT 

(3)

H. Regional Flatness
Regional flatness is defined as:

III. FEATURES DESCRIPTION

1   2  3

(4)

tr ( H T  2  pm ( H T ))

(5)

ST 

In the experiment, eight features, including both texture
features and regional features are extracted. Due to page limit,
the first five standard texture features are briefly described.

2

2

2

Or
ST 

A. Average Intensity
Average intensity is a measure of overall brightness.

IV. EXPERIMENTS AND RESULTS

B. Standard Deviation
Standard deviation is a measure of intensity variation.
C. Entropy
Entropy is a measure of molecular disorder within a
macroscopic system [10].
D. Kurtosis
Kurtosis is a measure of whether the data are heavy-tailed
or light-tailed relative to a normal distribution [11].

Based on the HDoG framework in [1], we first use the
Hessian pre-segmentation method to detect the blob candidate
regions, which includes both true blobs and false detections.
For each blob candidate region, we extract features
aforementioned in Section III. Here we mainly focus on the
test on the performance of different feature sets, which are
used as inputs of the un-supervised variational Bayesian
Gaussian Mixture Model (VBGMM) [12] for the final
detection.
In our 25 synthetic images, ground truth data are provided
in the form of the coordinates of the blob centers. A blob
candidate i is true positive (TP) if and only if its center is in a
detection pair (𝑖, 𝑗) for which the corresponding (nearest)

6434

ground truth center 𝑗 has not been paired, and their Euclidean
distance 𝐷𝑖𝑗 is within the scope of certain diameter d (here we
set 𝑑 = 4𝜎 + 1 ). Based on TP, recall ( 𝑟𝑒𝑐𝑎𝑙𝑙 =
𝑇𝑃
) and precision ( 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =
𝑁𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑇𝑟𝑢𝑒 𝐵𝑙𝑜𝑏𝑠
𝑇𝑃

), F-score is obtained which is the
harmonic mean of recall and precision ( 𝐹 − 𝑠𝑐𝑜𝑟𝑒 =
2×𝑟𝑒𝑐𝑎𝑙𝑙×𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛
).

glomerular objects (blobs) (count 𝐶 ) on these six images,
compared to the golden standard counts ( 𝐶𝑠 ) by acid
maceration [13]. To evaluate the performance of our feature
sets, a measure on closeness of the two results named
difference ratio is used and is defined as:

𝑁𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝐷𝑒𝑡𝑒𝑐𝑡𝑖𝑜𝑛𝑠

𝑟𝑒𝑐𝑎𝑙+𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛

We have 8 features in total, so there are 28=256
combinations of features needed to be fed into VBGMM
individually to identify blob candidates. For each
combination, the average F-score describing its performance
on those 25 3D images is calculated. Distribution of average
F-scores is shown in Figure 1. From this figure, we can see
there is a group of feature sets that have higher F-scores than
that of HDoG detector (which is 0.79 and marked with red line
in Figure 1). Furthermore, 63 feature sets have F-score greater
than 0.90, which outperform others significantly.

Figure 1. Histogram for average F-scores of all feature sets on 25
images(Average F-score (0.7946) of HDoG is marked with red line )

Next, we focus on the 63 feature sets having F-score
greater than 0.90. We conduct the ANOVA to test the
statistical average performance of selected feature set against
other features, where p=0.05 is used as significant level. As
seen from Figure 2, 52 feature sets forming a group (marked in
green box in Figure 2) that statistically outperform the other
11 feature sets in F-score and there are no statistically
significant differences between any of the two feature sets
within the 52 feature sets.

Figure 2. ANOVA group comparison result. The point in blue is the feature
set S1 which has highest F-score (by average). All the sample points in green
box are feature sets having statistically comparable performacne as that of S1.

To further evaluate the 52 feature sets, a real world dataset
is studied. The data set consists of six 3D MRI rat kidney
images. Each of the 52 feature sets are tested to count the

𝑟=

|𝐶−𝐶𝑠|
𝐶𝑠

× 100%

(7)

The smaller 𝑟 is, the closer our count is to the golden standard
count. The results are shown in Table I (due to limit of page
length, only feature sets with difference ratio less than 25% are
listed).
TABLE I.
EXPERIMENT RESULTS FOR SELECTED GROUP OF
FEATURES (*COMPARING TO ACID MACERATION (KNOWN AS GOLDEN
STANDARD IN MEDICINE), AVERAGE ACID MACERATION COUNT OVER THE 6
RAT MRI IS 30085). THE FIRST ROW IS THE BLUE POINT IN FIGURE 2.
.

Features

Mean
difference
ratio with
on 6
images

A

B

C

D

E

F

G

H

Mean blob
counts on 6
images

√

√

√

×

×

×

√

×

26794

12%

√

√

√

×

√

×

×

×

26887

13%

√

√

√

×

√

×

√

×

26437

14%

√

√

√

×

×

×

×

×

25168

14%

√

×

√

×

×

×

√

×

26041

15%

√

×

√

×

×

×

×

×

27803

16%

√

√

×

×

×

×

×

×

26569

18%

√

√

×

×

√

×

×

×

25312

19%

×

√

×

×

√

√

√

×

36971

21%

√

×

×

×

×

√

×

×

31138

21%

√

√

×

×

×

√

×

×

24118

21%

×

√

√

×

×

×

×

×

24705

23%

√

×

×

×

√

√

×

×

33839

24%

×

√

√

×

√

×

×

×

25629

24%

The numbers their corresponding features are as follows: A: average intensity, B: standard deviation,
C: entropy, D: kurtosis, E: skewness, F: uniformity, G: blobness, H: flatness. √ indicates the feature is
included while × means it is not included in the experiment

For demonstration purpose, the first feature set which
obtains the closest count to the ground truth is chosen. This
feature set includes average intensity, blobness, intensity
standard deviation and entropy. Figure 3 is the comparison
result of glomeruli counts on six 3D rat kidney MR images
(CF1-CF6) among HDoG using texture features, HDoG and
the acid maceration (golden standard) methods. The figure
shows that our counts are very close to and are consistent with
the golden standard counts. It is also noticed that our method is
comparable to HDoG, and it is difficult to show the
improvement on accuracy based on this dataset. This is
because there is no labeled ground truth data available.
However, the previous experiment on noised synthetic images
shows the significant improvement on accuracy when using
texture features (F-score: 0.95 using our feature set compared
to 0.79 using HDoG), therefore we can conclude that our
method may outperform HDoG on blob detection (or at least
as good as HDoG on rat kidney 3D MRI glomeruli counting).

6435

It is our intention to explore other real world datasets from
mice, human kidney MRI as our immediate next step to
validate the added values from texture features for small blob
detection.

the first 3D detector designed for small blob segmentation
from medical images. In this paper, we extend HDoG with 5
novel 3D texture features. We conduct comprehensive
evaluations for all feature combinations on both synthetic and
real world 3D images, to test their accuracy and applicability.
Based on the evaluation results, one feature set combining
texture features and regional geometric features is selected.
This feature set includes average intensity, blobness, intensity
standard deviation and entropy. It achieves average F-score of
0.95 on noised synthetic images, and a difference ratio of 12%
on real 3D rat kidney images comparing to the golden
standard. We conclude that this feature set may be robust to
noises and be capable to accurately identify small blobs from
3D images. In the future, we plan to conduct more validation
experiments using other real world datasets, e.g., mouse
kidney MR and human kidney MR.
REFERENCES
[1] Min Zhang, Teresa Wu, Scott Beeman, Luise Cullen-McEwen, John

Figure 3. Histogram for six rat kidney counts using HDoG with new feature
set, acid maceration and HDoG respectively

[2]
[3]
[4]

[5]
[6]
[7]
[8]
[9]
[10]
Figure 4. Blob detection result for synthetic image and 3D MR rat kidney
image (A is slice 100 for synthetic image #1, B is the same slice with detected
blobs highlighted in green, C is slice 100 for kidney image of rat CF1, D is
the same slice with detected blobs highlighted in green )

Figure 4 shows the detected blobs using this new feature set
on slices from synthetic images and real rat kidney images. It
gives us intuitive insight of its performance. From 4(A) and
4(B), we observe that this feature set has detected synthetic
blobs clearly while excluding noise points. Through 4(C) and
4(D), we can observe that this feature set can also successfully
detected rat kidney glomeruli which are located in the
outer-ring area of kidney.

[11]
[12]
[13]
[14]

V. CONCLUSION
Segmenting small structures (blob) in 3D has important
clinical utilities. To the best of our knowledge, HDoG may be

6436

Bertram, Jennifer Charlton, Edwin Baldelomar, Kevin Bennett:
“Efficient Small Blob Detection based on Local Convexity, Intensity
and Shape Information.” IEEE Trans. Medical Imaging 12/2015;
PP:1-1. DOI:10.1109/TMI.2015.2509463
M. Zhang, T. Wu, and K. Bennett, "Small Blob Identification in
Medical Images Using Regional Features from Optimum Scale," IEEE
Trans. Biomedical Engineering, vol 22, pp 1051-1062, Apr. 2015.
P. H. Mills et al., “Automated detection and characterization of
PIOlabeled cells and capsules using magnetic field perturbations,”
Magn Reson. Med., vol. 67, pp. 278–289, Jan. 2012.
C. I. Sanchez et al., “Contextual computer-aided detection: Improving
bright lesion detection in retinal images and coronary calcification
identification in CT scans,” Med. Image Anal., vol. 16, pp. 50–62, Jan.
2012.
S. C. Beeman et al., “Measuring glomerular number and size in
perfused kidneys using MRI,” Am. J. Physiol. Renal Physiol., vol. 300,
pp. F1454–F1457, Jun. 1, 2011. DOI: 10.1152/ajprenal.00044.2011
M. Zhang, T. Wu, and K. M. Bennett, “A novel Hessian based
algorithm for kidney glomeruli detection in 3D MRI,” in Proc. SPIE
9413 Med. Imag., Orlando, FL, 2015, pp. 94132N–94132N-9.
S. C. Beeman et al. MRI-based glomerular morphology and pathology
in whole human kidneys. American Journal of Physiology-Renal
Physiology, 306(11), F1381-F1390. (2014)
R. Gonzales, R. Woods, and S. Eddins, Digital Image Processing With
Matlab. Englewood Cliffs, NJ: Prentice-Hall, 2004.
H. Kong, H.C. Akakin, S.E. Sarma, "A Generalized Laplacian of
Gaussian filter for Blob Detection and Its Applications", IEEE Trans.
Cybernetics, Vol:43, pp 1719-1733
R. M. Haralick, K. Shanmugam and I. H. Dinstein, “Textural features
for image classification”, IEEE Trans. Systems, Man and Cybernetics,
vol.6, pp 610-621, 1973
S. Drabycz, R. G. Stockwell and J. R. Mitchell. Image texture
characterization using the discrete orthonormal S-transform. Journal of
digital imaging, 22(6), 696-708. (2009)
C. M. Bishop, Pattern Recognition and Machine Learning. New York:
Springer, 2006, vol 1.
J.-P. Bonvalet, M. Champion, F. Wanstok, and G. Berjal,
“Compensatory renal hypertrophy in young rats: Increase in the
number of nephrons,” Kidney Int., vol. 1, pp. 391–396, 1972.
C. D. Meyer, Matrix analysis and applied linear algebra: Siam, 2000.

Proceedings of the 2014 Winter Simulation Conference
A. Tolk, S. Y. Diallo, I. O. Ryzhov, L. Yilmaz, S. Buckley, and J. A. Miller, eds.

ACCURACY VS. ROBUSTNESS: BI-CRITERIA OPTIMIZED ENSEMBLE OF METAMODELS

Can Cui
Teresa Wu
School of Computing, Informatics, Decision
Systems Engineering
Arizona State University
699 S. Mill Ave.
Tempe, AZ 85281, USA

Mengqi Hu
Department of Industrial and Systems Engineering
Mississippi State University
260M McCain Hall
Starkville, MS 39762, USA

Jeffery D. Weir

Xianghua Chu

Department of Operational Sciences
Air Force Institute of Technology
2950 Hobson Way
Wright-Patterson Afb, Ohio 45433, USA

Shenzhen Graduate School,
Harbin Institute of Technology
Xili University Town
Guangdong 518055, CHINA

ABSTRACT
Simulation has been widely used in modeling engineering systems. A metamodel is a surrogate model
used to approximate a computationally expensive simulation model. Extensive research has investigated
the performance of different metamodeling techniques in terms of accuracy and/or robustness and
concluded no model outperforms others across diverse problem structures. Motivated by this finding, this
research proposes a bi-criteria (accuracy and robustness) optimized ensemble framework to optimally
identify the contributions from each metamodel (Kriging, Support Vector Regression and Radial Basis
Function), where uncertainties are modeled for evaluating robustness. Twenty-eight functions from the
literature are tested. It is observed for most problems, a Pareto Frontier is obtained, while for some
problems only a single point is obtained. Seven geometrical and statistical metrics are introduced to
explore the relationships between the function properties and the ensemble models. It is concluded that
the bi-criteria optimized ensembles render not only accurate but also robust metamodels.
1

INTRODUCTION

The growing complexity of real-world systems drives research in developing simulation models to imitate
the underlying functionality of the actual system (Banks et al. 2001). However, development of a
simulation model requires expertise and is sometimes time consuming. A Metamodel, or a surrogate
model, also known as a “model of the model” (Kleijnen 1995), is often built when the simulation is not
computationally easily implemented. Metamodels can be used to approximate and thus replace the
complex simulation model for engineering decisions. A comprehensive review of metamodeling
applications in computer-based engineering design and design optimization can be found in previous
references (Simpson et al. 1997; Wang and Shan 2007).
Researchers have conducted comprehensive comparisons of different metamodeling techniques. With
respect to accuracy (see Section 3.1 for definition), they have concluded that none of the metamodels can
perform uniformly well on diverse problems. Yet, to the best of our knowledge, the comparison studies

978-1-4799-7486-3/14/$31.00 ©2014 IEEE

616

Cui, Hu, Weir, Chu, and Wu
were conducted without considering various uncertainties (see Section 4.2 for definition and
classification) on design variables and parameters. It is expected that the accuracy of metamodels may
deteriorate with uncertainties existing in the system. In addition, adding uncertainties into the testing
problems will introduce another evaluation metric: robustness, the quality of being robust (see Section 3.1
for definition). Thus, in this research, twenty eight test functions from the literature are used for
comparison study. For each function, uncertainties (from a Gaussian distribution) are purposely added, in
order to exploit the robustness performance of metamodels. A robust model endeavors to uniformly
behave well on each task it performs. In the case of metamodeling, the model is supposed to give
uniformly accurate predictions across all sample points, rather than asymmetrically highly accurate
predictions in some areas (or points) while poorly accurate predictions in some other areas (or points),
which results in a high variability of the local prediction accuracy. Therefore, to develop a metamodel,
global accuracy is no longer the only objective. In this context, we must incorporate another evaluation
metric which entitles the metamodel insensitivity to uncertainties. Thus, we are looking for a model that is
both accurate and robust, in the sense that it maintains a harmonically balanced control on boosting
accurate global predictions while preventing local exaggerators. This has practical meaning. Imagine we
are developing a model for forecasting the hourly energy consumption for a building. Without
consideration of a robustness control, we might build a prediction model with good global accuracy, but a
number of local outliers which will deteriorate the decision making and make the system unreliable.
Driven by this motivation, a bi-criteria (accuracy and robustness) ensemble optimization framework of
three well-known metamodel techniques, namely Kriging (Matheron 1960), Support Vector Regression
(SVR) (Drucker et al. 1996; Clarke, Griebsch, and Simpson 2005) and Radial Basis Function (RBF) (Dyn,
Levin, and Rippa 1986) is developed. We observe that Pareto frontiers are obtained for most problems,
while for a small subset of problems, the Pareto front converges to a single point, which is due to the
dominant advantage of one metamodel over others. Moreover, to gain insight on single-point Pareto
frontier, we introduce seven geometric and statistical properties describing the function. The relationships
between the function properties and metamodel performance are then summarized.
This paper is organized as follows. In Section 2, background knowledge on metamodeling techniques,
and ensemble are introduced. The Bi-objective optimization framework for ensemble weight selection is
proposed in Section 3, in which accuracy and robustness are elaborated upon. In Section 4, comparison
experiments are detailed with insights on the results provided. Conclusions and future research are
discussed in Section 5.
2

REVIEW ON METAMODELING TECHNIQUES

Three metamodeling techniques are of interests in this research: Kriging, SVR and RBF, due to their
extensive use in surrogate modeling. Each is reviewed in this section. In addition, research in developing
an ensemble model (single criteria) is reviewed.
2.1

Kriging

Kriging (also known as Gaussian process regression) is an interpolation method that assumes that the
simulation output may be modeled by a Gaussian process; it gives the best linear unbiased prediction of
simulation output not yet observed. It generates the prediction in the form of a combination of a global
model with local random noise:
‫ݕ‬ሺ‫ݔ‬ሻ ൌ ݂ሺ‫ݔ‬ሻɀ ൅ ܼሺ‫ݔ‬ሻ,
(1)
where ‫ ݔ‬is the input vector, ɀ is the weight vector, and ܼሺ‫ݔ‬ሻ, is a stochastic process with zero mean and
stationary covariance of
‫ܸܱܥ‬ൣܼሺ‫ݔ‬௜ ሻǡ ܼ൫‫ݔ‬௝ ൯൧ ൌ ߪ ଶ ܴሺ‫ݔ‬௜ ǡ ‫ݔ‬௝ ሻ,
(2)
ଶ
where ߪ is the process variance, ܴሺ‫ݔ‬௜ ǡ ‫ݔ‬௝ ሻ is an n by n correlation matrix where n is the sample size of
the training data. ܴ is usually depicted by a Gaussian correlation function, ݁‫݌ݔ‬ሺെߠሺ‫ݔ‬௜ െ ‫ݔ‬௝ ሻଶ ሻ with

617

Cui, Hu, Weir, Chu, and Wu
parameter ߠ. Kriging is one of the most intensively studied metamodels among the others because it is
flexible with a number of correlation functions and regression functions (with polynomial degree of 0, 1
or 2) to choose from. It is generally acknowledged that the Kriging model outperforms other metamodels
on nonlinear problems but is not easy to develop due to the time consumed by Maximum Likelihood
Estimation of the correlation parameters in ܴ which is a multi-dimensional optimization problem
(Simpson et al. 2001). In this study, we use deterministic Kriging.
2.2

Support Vector Regression (SVR)

Support vector regression is analogous to support vector classification, which tries to maximize the
distance between two classes of data by selecting two hyperplanes in a way that they separate the training
data with no points between them, the mathematical form of SVR is:
݂ሺ‫ݔ‬ሻ ൌ ‫ ߱ۃ‬ή ‫ ۄݔ‬൅ ܾ,
(3)
௕
where ߱ is the norm vector to the hyperplane and ԡఠԡ determines the offset of the hyperplane from the
origin. The goal is to find a hyperplane that separates the data points optimally without error and separates
the closest points with the hyperplane as far as possible. Thus, it can be constructed as an optimization
problem:
ଵ
Minimize: ଶ ȁ߱ȁଶ
‫ ݕ‬െ ‫ ߱ۃ‬ή ‫ݔ‬௜ ‫ ۄ‬െ ܾ ൑ ߝ
.
(4)
subject to ൜ ௜
‫ ߱ۃ‬ή ‫ݔ‬௜ ‫ ۄ‬൅ ܾ െ ‫ݕ‬௜ ൑ ߝ
According to the duality principle, the nonlinear regression problem is given by:
‫כ‬
݂ሺ‫ݔ‬ሻ ൌ σ௠
(5)
௜ୀଵሺߙ௜ െ ߙ௜ ሻ ݇‫ݔۃ‬௜ ή ‫ݔ‬௝ ‫ ۄ‬൅ ܾ,
‫כ‬
‫ۄ‬
where ߙ௜ and ߙ௜ are two introduced dual variables, and ݇‫ݔۃ‬௜ ή ‫ݔ‬௝ is a kernel function, e.g. Gaussian
kernel. It is interesting to note that research has demonstrated SVR performs well on a number of
problems, and even outperforms Kriging for some cases (Wang and Shan 2007). However, most studies
have been empirical.
2.3

Radial Basis Function (RBF)

RBF is used to develop interpolation on scattered multivariate data. A RBF is a linear combination of a
real-valued radially symmetric function, ‫׎‬ሺ‫ݔ‬ሻ, based on distance from the origin,
݂ሺ‫ݔ‬ሻ ൌ σ௡௜ୀଵ ߠ௜ ‫׎‬ሺԡ‫ ݔ‬െ ‫ݔ‬௜ ԡሻ,
(6)
where ߠ௜ is the unknown interpolation coefficient determined by the least-squares method, n is the
number of sampling points and ԡ‫ ݔ‬െ ‫ݔ‬௜ ԡ is the Euclidean norm of the radial distance from design point ‫ݔ‬
to the sampling point ‫ݔ‬௜ . Fang et al. (2005) found RBF performs well on highly nonlinear problems.
2.4

Single-Criteria Ensembles

Ensemble is a technique of combining multiple models in order to create a stronger overall representation
of the system studied. Acar and Rais-Rohani (2009) proposed an ensemble of surrogate models and
demonstrate that the resulting hybrid model improves the prediction accuracy.
In general, a weighted average surrogate model is (Bishop 1995):
‫ݕ‬ො௔௩௚ ሺ‫ݔ‬ሻ ൌ σெ
ො௝ ሺ‫ݔ‬ሻ,
(7)
௝ୀଵ ‫ݓ‬௝ ሺ‫ݔ‬ሻ‫ݕ‬
where ‫ݕ‬ො௔௩௚ ሺ‫ݔ‬ሻ is the ensemble response prediction by the weighted sum of each surrogate model
response prediction ‫ݕ‬ො௝ ሺ‫ݔ‬ሻ, ‫ݓ‬௝ ሺ‫ݔ‬ሻ is the weight corresponding to the ݆th surrogate, and ‫ ܯ‬is the number of
surrogate models. In addition, to achieve an unbiased estimator of the ensemble, the weights ߱௝ ሺ‫ݔ‬ሻ must
sum to one.
It is expected that an outperforming surrogate is deemed to be assigned larger weight while an
underperforming surrogate deserves smaller weight in an ensemble. The evaluation metrics on “goodness-

618

Cui, Hu, Weir, Chu, and Wu
of-fit” are considered to be a confident measurement of the model accuracy. Acar and Rais-Rohani (2009)
proposed a weight selection approach by solving an optimization problem to identify the weight for each
surrogate that would minimize a selected error metric (e.g., root mean square error). That is,
Minimize: ߝ௘ ൌ ‫ݎݎܧ‬ሼ‫ݕ‬ො௘ ቀ‫ݓ‬௝ ሺ‫ݔ‬ሻǡ ‫ݕ‬ො௝ ሺ‫ݔ‬ሻǡ ‫ݕ‬௝ ሺ‫ݔ‬ሻቁ ǡ ݆ ൌ ͳǡ ǥ ‫ܯ‬ሽ
subject to σெ
௝ୀଵ ‫ݓ‬௝ ሺ‫ݔ‬ሻ ൌ ͳ,
(8)
‫ݓ‬௝ ሺ‫ݔ‬ሻ ൒ Ͳǡ ݆ ൌ ͳǡ ǥ ǡ ‫ܯ‬,
where Err{} is the error metric of the built ensemble predicted response. We want to note that this
framework used a single criteria (in this case, accuracy) to develop the ensemble. In the next section, we
propose a bi-objective optimization problem that not only considers accuracy but also robustness in a
weight selection mechanism.
3

FRAMEWORK OF BI-CRITERIA OPTIMIZATION ON ENSEMBLE WEIGHT
FACTORS SELECTION

3.1

Metamodel Performance Criteria

In a real-world engineering system, the impacts of uncertainties and noises may not be negligible. This
requires a model (surrogate, ensemble) to perform not only accurately but also robustly. Accuracy is
defined as how well the metamodel predicts the unknown data, which is usually evaluated by an error
measurement. Robustness measures how consistently a model performs over different problems (Li et al.
2010), or how consistently a model performs over different design regions on one problem. The accuracy
metrics reflect the degree of closeness of the metamodel measurement outputs ‫ݕ‬ො to true output y which is
obtained from the deterministic input. One global measurement for accuracy is Normalized Root Mean
Square Error (NRMSE)
σ೙
ො ೔ ሻమ
೔సభሺ௬೔ ି௬

ܴܰ‫ ܧܵܯ‬ൌ ට

௡

Ȁሺ‫ݕ‬௠௔௫ െ ‫ݕ‬௠௜௡ ሻ.

(9)

Robustness considers the metamodel’s ability to consistently achieve similar accuracy over the whole
design space, thus it could be evaluated by the standard deviation of local prediction error (SDPE):
ܵ‫ ܧܲܦ‬ൌ ‫݀ݐݏ‬Ǥ ȁ‫ݕ‬௜ െ ‫ݕ‬ො௜ ȁ‫݊ א ݅׊‬,
(10)
where ȁ‫ݕ‬௜ െ ‫ݕ‬ො௜ ȁ is the absolute difference between the true output and prediction output at data point i,
and n is the number of data points. Formulation of Normalized SDPE is given in the next section.
3.2

Bi-Criteria Optimization Framework

Given NRMSE and NSDPE, a bi-objective optimization problem is introduced:
Minimize: (ܴܰ‫ܧܵܯ‬௘௡ , ܰܵ‫ܧܲܦ‬௘௡ ),
where
ଵ

ܴܰ‫ܧܵܯ‬௘௡ ൌ ට σ௡௜ୀଵሾ‫ݕ‬௜ െ  σெ
ො௜௝ ሻሿଶ Ȁ(‫ݕ‬௠௔௫ െ ‫ݕ‬௠௜௡ ),
௝ୀଵሺ‫ݓ‬௝ ‫ݕ כ‬
௡
ଵ

ଵ

ଶ

ො௜௝ ሻห െ  ௡ σ௡௜ୀଵ ȁ‫ݕ‬௜ െ  σெ
ො௜௝ ሻȁቅ Ȁሺ‫ݕ‬௠௔௫ െ ‫ݕ‬௠௜௡ ሻ,
ܰܵ‫ܧܲܦ‬௘௡ ൌ ට௡ିଵ σ௡௜ୀଵ ቄห‫ݕ‬௜ െ  σெ
௝ୀଵሺ‫ݓ‬௝ ‫ݕ כ‬
௝ୀଵሺ‫ݓ‬௝ ‫ݕ כ‬
subject to σெ
௝ୀଵ ‫ݓ‬௝ ൌ ͳ,
‫ݓ‬௝ ൒ Ͳǡ ݆ ൌ ͳǡ ǥ ǡ ‫ܯ‬,
(11)
where ‫ ܯ‬is the number of metamodel techniques in the ensemble (in this study, ‫=ܯ‬3, ݆ ൑ ‫ܯ‬, ݆=1 is
Kriging; ݆=2 is SVR; ݆=3 is RBF), ‫ݕ‬௜ is the true fitness value at point ݅; ‫ݕ‬௠௔௫ and ‫ݕ‬௠௜௡ are the maximum
and minimum value of ‫ݕ‬௜ over ݊ sample points; ‫ݕ‬ො௜௝ is the prediction at point ݅ by metamodel ݆, ‫ݓ‬௝ is the
ensemble weight of metamodel ݆ . Note both objectives are normalized for comparison convenience
among functions of various fitness magnitudes.

619

Cui, Hu, Weir, Chu, and Wu
We use a weighted sum to transform the multi-objective problem into a single objective function as in
(Hwang and Masud 2012):
Minimize: ሺߚଵ ‫ܧܵܯܴܰ כ‬௘௡ ൅ ߚଶ ‫ܧܲܦܵܰ כ‬௘௡ ሻ,
subject to ߚଵ ൅ ߚଶ ൌ ͳ,
ߚଵ ǡ ߚଶ ൒ Ͳ,
(12)
where ߚଵ and ߚଶ are importance weights for the two objectives. In this research, a sequential quadratic
programming algorithm (SQP) (Nocedal and Wright 2006), an iterative method for nonlinear
optimization, is used to solve this bi-criteria optimization problem.

4

METAMODELING EXPERIMENTS

4.1

Test Functions

The test functions used in this research are composed of 28 black-box benchmark functions that are
originally proposed for competition on real parameter single objective optimization (Liang and Suganthan
2013). There are three main categories of black-box functions: 5 unimodal functions, 15 multimodal
functions and 8 composition functions, each of which has the range [-100,100]D, where D denotes the
number of dimensions of the function. These three function categories have distinguishing characteristics,
for example, a unimodal function has only one global optima (valley/peak), while a multimodal can have
many local optima (valleys/peaks), and a composition function is composed from unimodal and
multimodal functions.
4.2
4.2.1

Design of Experiment
Training, Validation and Test Data Generation

Latin hypercube sampling (LHS) is a statistical sampling method used in construction of computer
experiments for good uniformity and coverage from a multidimensional distribution (Eglajs and Audze
1977). It is widely used because the sample size is not strictly determined by the number of dimensions of
the underlying simulation model (Zhang et al. 2013). For fitting a quadratic polynomial model, the
minimum number of sample points is ሺ݀ ൅ ͳሻ ‫ כ‬ሺ݀ ൅ ʹሻȀʹ, where d denotes the number of variables (Jin,
Du, and Chen 2003; Shan and Wang 2005). We generate training data of two distinct dimensions of
design variables, 5 and 10, with the sampling size varying from 21 to 66 respectively, using LHS. With
comparative settings of different dimensions of the design variables, we are able to see if the
dimensionality impacts the performance of the metamodels. In order to avoid over fitting, we implement a
k-fold cross-validation process to the training data (Kohavi 1995). Here k=2, because we have a relatively
small number of sampling data points. Test data is randomly generated with 1000 data points over the
design space, which is treated as a testing data set for the metamodel performance evaluation.
Given the test problems, we purposely add uncertainties to the inputs and the outputs of the functions.
Specifically, parametric uncertainty (variability on each input variable) and residual uncertainty
(variability on the outputs) are added in the black-box functions. For the parametric uncertainty, a random
number within 10% of each design variable range of [-100,100], [-10, 10] is generated and added. For the
residual uncertainty, we add a random number from a Normal distribution ~N (0,ߪ2 ), where ߪ2=10%
times the logarithm of the difference between the maximum and minimum of the training output for each
black-box problem. It is easily understood that with the existence of uncertainties, the same input may not
generate the same output, thus 25 simulation replicates are conducted to mitigate the randomness. An
average value of the 25 output replicates (‫ݕ‬ത) is taken as the output for training data while the input for
training data takes its nominal value (‫)ݔ‬, which is the exact value without noise contamination. And the
same operation is applied to test data.

620

Cui, Hu, Weir, Chu, and Wu
4.2.2

Parameter Tuning

The grid search method (Chang and Lin 2011) is applied in this study to select the optimal parameters that
give the minimum validation error, then the test data is applied to the optimally trained model to obtain its
generalization error. For example, based on experiments, the degree of polynomial regression function for
Kriging is selected as 0 for a Sphere function, while for Composition Function 1, it is 2. Table 1
summarizes the parameter search space for each model.
Table 1: Parameter Search Space for Each Metamodel.
Technique Parameters
Regression function: polynomial (degree=0, 1, 2);
Kriging Correlation function: cubic (ͳ െ ͵ߝ ଶ ൅ ʹߝ ଶ ǡ ߝ ൌ ݉݅݊൛ͳǡ ߠห‫ݔ‬௜ െ ‫ݔ‬௝ หൟሻ, exponential
(݁‫݌ݔ‬ሺെߠห‫ݔ‬௜ െ ‫ݔ‬௝ หሻ) and Gaussian.
SVR type: nu-SVR (nu=0,0.25,0.5,0.75,1);
SVR
Kernel function: RBF;
RBF center: set as equal to input;
RBF
Basis function: Gaussian or polyharmonicspline.

4.3

Result Analysis

In this section, the performance of the three metamodels simulated on 28 black-box problems under
parametric and residual uncertainties is analyzed with statistical and graphical methods. An ensemble
solely based on error minimization is first built and compared to the three metamodels in terms of
accuracy. Then a bi-criteria ensemble is built for analysis on the relationship between accuracy and
robustness, and between function properties and metamodel performance.
4.3.1

Metamodels’ Performance Comparison using Accuracy

In response to different distributions among the metamodels, a nonparametric statistics test is preferable
as it has fewer restrictive assumptions about data levels and underlying probability distributions. The
Friedman test (Friedman 1937), a non-parametric statistical test, is applied to detect differences in error
measurements across the three metamodels and the ensemble.
It is observed that there is no significant difference on the model performance by statistically
comparing the mean of each model’s NRMSE between the two different settings of dimensions, 5 and 10,
thus we only report the results of testing problems from the 10 dimension problems. The average NRMSE
and standard deviation of NRMSE across 28 functions, which provides a rough indication of accuracy and
robustness, for Kriging, SVR, RBF and their ensemble are 0.106, 0.131, 2.726, and 0.086, and 0.465,
0.055, 10.034 and 0.050, respectively. It is noticeable that RBF gives two outliers, function 8 and function
20. From a geometric point of view, these two functions share one common characteristic: an almost flat
response surface for most of the space with an abrupt altitude change within a small area. Moreover, the
change in altitude is not uniformly spreading, but is an asymmetrical pattern centered from a point. By
examining the fundamental modeling mechanism of RBF, which makes inferences based on radial
symmetrical distance, the outliers can be explained. Specifically, RBF might be inaccurate for a response
surface with an asymmetrical shape. Thus, to effectively compare the three metamodels performance, we
remove the outliers, which results in an average NRMSE of 0.110 that is comparable to Kriging and a
standard deviation of NRMSE of 0.118 that is still inferior to the others. As a result, we need to keep in
mind that RBF might not be a good choice for a robust model, especially when the response surface of the
system is unknown. The Friedman’s test results indicate that the four groups of metamodels have
621

Cui, Hu, Weir, Chu, and Wu
significant differences due to significant P-values given in Table 2. The Nemenyi’s test (Nemenyi 1963)
further divides the four metamodels types into three groups labeled A, B, and C, in which Kriging and
RBF have similar rankings, whereas ensemble is ranked first and SVR is ranked last, given by Table 3.
These statistical tests to some extent demonstrate that Kriging and RBF generally perform better than
SVR when all of the 28 functions are aggregately considered, while even so, the ensemble of the three
metamodels performs even better than any stand-alone metamodel, which is indicated by the sum of ranks
and mean of ranks values in the two-tailed test. Therefore, building an ensemble to take advantage of each
metamodel’s strength and mitigate their weakness is an effective way to avoid biased and sub-optimal
solutions. In a word, the comprehensive performance among all models could be summarized as:
ensemble outperforms all, followed by Kriging, and RBF with two outliers, and the last is SVR.
However, the ensemble we obtained to this extent is built solely based on NRMSE minimization,
which is not sufficient when uncertainties exist, so in the next section, a bi-criteria ensemble is built to
further incorporate robustness.
Table 2: Friedman’s Test on NRMSE.
Q (Observed value) Q (Critical value) DF p-value (Two-tailed) alpha
52.325
7.815
3
< 0.0001
0.05
Table 3: Multiple Pairwise Comparisons Using Nemenyi’s Procedure/ Two-tailed Test on NRMSE.
Sample
Ensemble
RBF
Kriging
SVR
4.3.2

Sum of ranks
34.000
71.500
71.500
103.000

Mean of ranks
1.214
2.554
2.554
3.679

Groups
A
B
B
C

Bi-Criteria Ensemble

In this section, the case of dimension=10 is selected as an illustrative example. 28 bi-criteria ensembles
are built from the optimization framework defined in Section 3. One hundred and one single objective
optimization problems are solved by changing ߚଵ (see (12)) from 0 to 1 with 0.01 step size. The Pareto
optimality plots of most of black-box problems show that the two objectives are conflictive.
For clarification, we take one un-normalized unimodal black-box problem, Rotated High Conditioned
Elliptic Function’s Pareto Frontier as an example, which is shown in Figure 1. The two end points in the
Pareto frontier indicate when RMSE fully controls the optimization problem, it reaches its minimum,
while SDPE reaches its maximum, and vice versa. The middle point reflects when RMSE and SDPE are
of equal importance. To illustrate, see Figure 2, the sum of the two optimal solutions for each normalized
objectives is plotted over all iterations, as the point is located in the lower middle of the curve, we can
intuitively reconsider the two objectives as two costs, then to reach the minimum cost, we can plot the
sum of the two objectives w.r.t. the solutions, then in this case, we find that the 51st solution gives the
minimum cost.
As we stated, most of the problems have a typical Pareto Frontier as Figure 1 shows, however, some
of them do not, instead, their Pareto Frontier is a single point. The reason for this phenomenon is because
for some of the black-box functions, one metamodel performs significantly better than others. In each
iteration of the optimization problem, it is assigned the full weight of 1 while the others are evicted from
the ensemble. It is necessary to study these functions’ properties so we can gain some useful insights on
the relationship between the response surface properties and metamodel performance. In the next Section,
we employ some geometrical and statistical metrics to evaluate the properties of the black-box functions
with a single-point Pareto frontier.

622

Cui, Hu, Weir, Chu, and Wu
4.3.3

Properties of Black-box Functions with Single-point on Pareto Frontier

In this section, 7 properties are introduced. The definitions are listed in Table 4. The response surface of
each function is meshed by star-studded sampling points. Each dimension of sampling points are
uniformly distributed within the range of [-100,100], with the interval of 1. “Gradient of Response
Surface Point” denotes the first derivative of a function evaluated at a sampling point on the surface. This
measures the change rate of bumpiness. A single-point Pareto frontier is detected in 7 out of the 28
functions and its corresponding predictor metamodel is given in Table 5.
To intuitively understand the data given in Table 5, plots of the seven metrics for the 28 functions are
provided in Figures 3-9.
1.57

x 10

9

3.664

x 10

9

3.662
3.66

1.56

SDPE

3.658
3.656
1.55

3.654
3.652

1.54
2.09

2.1

2.11
RMSE

Figure 1: Pareto Frontier of Rotated High
Conditioned Elliptic Function.

3.65
0

2.12
x 10

9

0.2
0.4
0.6
0.8
Weight Factor of NSDPE: beta2

1

Figure 2: Sum of RMSE & SDPE on Rotated High
Conditioned Elliptic Function.

Table 4: Definition and Calculation Methods of 7 Geometric and Statistical Properties.

Gradient of Response
Surface Point

Properties Description
Mean Mean of absolute values of gradients on all response surface points generally
evaluates how steep and rugged the surface is by looking into its rate of change
on each point
Median Median of absolute values of gradients on all response surface points finds the
median value of all rates of change on each point
Std. Standard deviation of absolute values of gradients on all response surface
points evaluates the rate of the rate of change on each point
Min Minimum of absolute values of gradients on all response surface points gives a
lower bound of rate of change on each point
Max Maximum of absolute values of gradients on all response surface points gives
an upper bound of rate of change on each point
Std. of Function Standard deviation of function values on all design points can evaluate how
values bumpy the surface is by looking into its each value’s deviation from the mean
Biggest Averaged local biggest difference of function values can evaluates the average
difference bumpiness by looking into the difference between “valley” and “peak” on
each local area

It is noticeable that the single metamodel solution for the Pareto point is either Kriging (functions 3, 7, 8,
20) or RBF (functions 4, 12, 19). This indicates that Kriging and RBF perform better than SVR in terms of

623

Cui, Hu, Weir, Chu, and Wu
both accuracy and robustness on these functions. In addition, except for max of gradient of response
surface point for Kriging, and std. of function values for RBF, all the rest of the plots show that the
functions with a single Pareto point have relatively smaller values of the seven evaluation metrics. In fact,
functions with higher values of evaluation metrics tend to be more complicated and changeful, and vice
versa. Therefore, we can intuitively interpret the values of the evaluation metrics as a measure of a
function’s complexity, i.e. the lower value indicates a simpler form of the response surface. Therefore, we
can conclude that for a black-box problem with a relatively simple form of a response surface, there is a
higher chance that one metamodel will overwhelmingly perform better than others and take full charge of
the ensemble development. In another words, because the function is simple to predict, any metamodel can
easily produce a good enough approximation, so the performance difference among all the metamodels is
marginal. When dealing with a complicated response surface, it is difficult with only one metamodel to
achieve a good approximation, and the strength of all metamodels must be aggregated in order to derive a
good approximation of the surface, so an ensemble built from all metamodels is naturally required.
Table 5: Evaluation Metrics Statistics Summary for Black-box Functions with Single Pareto Point.
Properties
Function
Number
3
4
7
8
12
19
20

Gradient of Response Surface Point
Mean

Median

Std.

Min

Max

0.0001
0.0030
0.0004
0.0158
0.0018
0.0021
0.0134

0.0000
0.0027
0.0000
0.0142
0.0008
0.0003
0.0000

0.0030
0.0024
0.0056
0.0111
0.0027
0.0043
0.0395

0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000

0.1841
0.0109
0.5441
0.2804
0.0158
0.0272
0.4541

Std. of
Function
values
0.0148
0.1842
0.0163
0.0399
0.1319
0.1389
0.0554

Biggest
Single
difference Pareto
0.0032
0.0596
0.0049
0.1256
0.0308
0.0415
0.0804

Kriging
RBF
Kriging
Kriging
RBF
RBF
Kriging

0.2
0.6

0.15

0.4

0.1

0.2

0.05
0

0

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

Figure 4: Max of Gradient of Response Surface.

Figure 3: Mean of Gradient of Response Surface.
0.002

1
0.8

0.001

0.6
0.4

0.001

0.2
0.000

1

3

5

7

9

0

11 13 15 17 19 21 23 25 27

1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728

Figure 6: Biggest Difference.

Figure 5: Min of Gradient of Response Surface.

624

Cui, Hu, Weir, Chu, and Wu
0.1

0.3

0.08
0.2

0.06
0.04

0.1
0

0.02
0

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

Figure 7: Std. of Function Values.

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

Figure 8: Std. of Gradient of Response Surface.

0.2
0.15
0.1
0.05
0

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

Figure 9: Median of Gradient of Response Surface.
5

CONCLUSION

A new approach on selecting weight factors for developing an ensemble of metamodels is proposed in
this research. Based on earlier efforts, this approach treats the development of ensembles by extending a
single objective optimization to a bi-criteria optimization problem, which is achieved by simultaneously
optimizing on both accuracy and robustness. The addition of uncertainties is of practical rationale as in
real world design engineering, noises and errors are inevitable. For a complete study, 28 black-box
problems of distinct characteristics are set as test problems to evaluate the performance of each
metamodel in terms of accuracy and robustness and to test our proposed approach.
When both accuracy and robustness are considered in selecting weight factors of the ensemble,
different Pareto Frontiers are achieved. 21 out of the 28 functions have typical Pareto Frontiers where the
two objectives present a conflictive relationship. However, 7 out of the 28 functions display a single
Pareto point, which is because one metamodel overwhelmingly performs better than the others. Seven
evaluation metrics are introduced to empirically analyze the single Pareto point phenomenon. The results
show that functions with a single Pareto point are more likely to have a simpler form of response surface
than others.
In summary, some of the key findings in this study are:
x It’s not conclusive that any metamodel predicts better than others across diverse problems;
x RBF performs poorly on problems with an asymmetrically distributed response surface, which
indicates it is less robust than the other two models;
x Kriging and RBF statistically perform better than SVR in terms of accuracy. An ensemble of
three metamodels statistically performs better than any stand-alone metamodel. Therefore, it is
recommended that if computationally permissible, an ensemble is built instead of a stand-alone
metamodel, given that sufficient prior knowledge regarding each member of ensemble is fully
understood by users;
x A novel ensemble weight selection mechanism is successfully implemented, which renders not
only accurate but also robust surrogate models, and provides us with new insights on the
relationship between accuracy and robustness, given uncertain conditions;
625

Cui, Hu, Weir, Chu, and Wu
x

In most cases, accuracy and robustness have a conflictive relationship, of which the Pareto
Frontier looks like the left half of a convex parabola. We suggest equal weight on both objectives
for a balanced solution.
In future research, we plan to implement bi-criteria ensemble metamodel to replace an expensive
simulation model of a real world problem, e.g., building energy consumption, in order to achieve an
accurate and robust metamodel under uncertain conditions with unequal variance. Some algorithms that
are more adaptive to a stochastic environment will be considered instead, e.g., stochastic Kriging. What’s
more, relationship between the problem properties and metamodeling performance will also be further
explored. According to the meta-learning concept (Vilalta and Drissi 2002), which studies how to
properly select a model for a specific problem based on experience, it would be more efficient and
effective if connections between the training data characteristics and metamodel performance can be
established.
REFERENCES
Acar, E., M. Rais-Rohani. 2009. “Ensemble of Meta-models with Optimized Weight Factors.” Structural
and Multidisciplinary Optimization 37: 279–294.
Banks, J., J. S. Carson, B. L. Nelson, and D. M. Nicol. 2000. Discrete-Event System Simulation 3rd ed.
Upper Saddle River, New Jersey: Prentice-Hall, Inc.
Bishop, C. M. 1995. Neural Networks for Pattern Recognition. New York: Oxford University Press.
Chang, C., C. Lin. 2011. “LIBSVM: A Library for Support Vector Machines.” ACM Transactions on
Intelligent Systems and Technology (TIST) 2: Article No. 27.
Clarke, S. M., J. H. Griebsch, T. W. Simpson. 2005. “Analysis of Support Vector Regression for
Approximation of Complex Engineering Analyses.” Journal of Mechanical Design 127(11):1077–
1087.
Drucker, H., C. J. C. Burges, L. Kaufman, A. Smola, and V. Vapnik. 1996. "Support Vector Regression
Machines," Advances in Neural Information Processing Systems 9, NIPS, 155–161, MIT Press.
Dyn, N., D. Levin, S. Rippa. 1986. “Numerical Procedures for Surface Fitting of Scattered Data by Radial
Basis Functions.” SIAM Journal on Scientific and Statistical Computing 7(2): 639–659.
Eglajs, V., P. Audze. 1977. “New Approach to the Design of Multifactor Experiments,” Problems of
Dynamics and Strengths 35: 104–107.
Fang, H., M. Rais-Rohani, Z. Liu, M. F. Horstemeyer. 2005. “A Comparative Study of Metamodeling
Methods for Multiobjective Crashworthiness Optimization.” Computers & Structures 83: 2121–2136.
Friedman, M. 1937. “The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of
Variance,” Journal of the American Statistical Association 32 (200): 675–701.
Hwang, C. L., A. S. M. Masud. 1979. “Multiple Objective Decision Making, Methods and Applications:
A State-of-the-art Survey.” Economics and Mathematical Systems 164.
Jin, R., X. Du, and W. Chen. 2003. “The Use of Metamodeling Techniques for Optimization under
Uncertainty.” Structural and Multidisciplinary Optimization 25: 99-116.
Kleijnen, J. 1995. “Theory and Methodology Verification and Validation of Simulation Models.”
European Journal of Operational Research 82:145-162.
Kohavi, R. 1995. “A Study of Cross-validation and Bootstrap for Accuracy Estimation and Model
Selection.” Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence 2
(12): 1137–1143.
Li, Y. F., S. H. Ng, M. Xie, and T. N. Goh. 2010. “A Systematic Comparison of Metamodeling
Techniques for Simulation Optimization in Decision Support Systems.” Applied Soft Computing 10:
1257–1273.
Liang, J. J., B. Y. Qu, and P. N. Suganthan. 2013. “Problem Definitions and Evaluation Criteria for the
CEC 2013 Special Session on Real-Parameter Optimization.” Technical Report Computational

626

Cui, Hu, Weir, Chu, and Wu
Intelligence Laboratory, Zhengzhou University, Zhengzhou China and Technical Report, Nanyang
Technological University, Singapore.
Matheron, G. 1960. “Krigeage d’un Panneau Rectangulaire Par sa Périphérie,” Note géostatistique no.28,
CG, Ecole des Mines de Paris.
Nemenyi, P. B. 1963. “Distribution-free Multiple Comparisons,” PhD thesis, Princeton University.
Nocedal, J., and S. J. Wright. 2006. Numerical Optimization. Springer.
Shan, S., and G. G. Wang. 2005. “An Efficient Pareto Set Identification Approach for Multiobjective
Optimization on Black-Box Functions.” Transactions of the ASME 127: 866-874.
Simpson, T. W., J. Peplinski, P. N. Koch, and J. K. Allen. 1997. “On the Use of Statistics in Design and
the Implications for De terministic Computer Experiments.” ASME Design Engineering Technical
Conferences DTM-3881.
Simpson, T. W., J. Peplinski, P. N., Koch, and J. K. Allen. 2001. “Metamodels for Computer-based
Engineering Design: Survey and Recommendations.” Engineering with Computers 17(2): 129-150.
Vilalta, R., and Y. Drissi. 2002. “A Perspective View and Survey of Meta-Learning.” Artificial
Intelligence Review 18: 77–95.
Wang, G. G., and S. Shan. 2007. “Review of Metamodeling Techniques in Support of Engineering
Design Optimization.” Journal of Mechanical Design 370.
Zhang, S., P. Zhu, W. Chen, and P. Arendt. 2013. “Concurrent Treatment of Parametric Uncertainty and
Metamodeling Uncertainty in Robust Design,” Structural and Multidisciplinary Optimization 47: 6376.
AUTHOR BIOGRAPHIES
Can Cui is a research assistant of Industrial Engineering program in Arizona State University. Her
research mainly deals with dynamic system modeling, simulation and optimization. She is a Ph.D. student.
Her email address is ccan1@asu.edu.
Mengqi Hu received the Ph.D. degree in industrial engineering from Arizona State University, in 2012.
He is currently an Assistant Professor with the Department of Industrial and Systems Engineering,
Mississippi State University. His current research interests include complex system modeling, simulation
and optimization, swarm intelligence and evolutionary computation, with applications in energy systems,
healthcare systems. His email address is mhu@ise.msstate.edu.
Jeffery D. Weir received the Ph.D. degree in industrial and systems engineering from the Georgia
Institute of Technology. He is currently an Associate Professor with the Department of Operational
Sciences, Air Force Institute of Technology. His current research interests include decision analysis and
transportation modeling. His email address is Jeffery.Weir@afit.edu.
Xianghua Chu is currently working toward his Ph.D. in Management Science, Harbin Institute of
Technology, China. His current research interests include swarm intelligence, evolutionary algorithms
and intelligent decision support. His email address is xianghua.chu@gmail.com.
Teresa Wu received the Ph.D. degree in industrial engineering from the University of Iowa, in 2001. She
is currently a Professor with the School of Computing, Informatics, Decision Systems Engineering,
Arizona State University. Her current research interests include distributed decision support, distributed
information systems, and health informatics. Her email address is Teresa.Wu@asu.edu.

627

Proceedings of the 3rd Annual
IEEE Conference on Automation Science and Engineering
Scottsdale, AZ, USA, Sept 22-25, 2007

TuRP-A05.3

An Importance Sampling Based Approach for Reliability Analysis
Fan Li and Teresa Wu

Abstract - In this paper, an importance sampling based
approach for reliability analysis is proposed. The fundamental
of this approach is to bias the realization of random variables
around the most probable point (MPP) such that the number of
simulations can be reduced significantly. Compared to the basic
Monte Carlo simulation (MCS), the proposed approach
requires less computational effort since it only evaluates the
system performance functions at the reduced probability space.
Two comparison experiments are conducted at the end of the
paper. One is used to demonstrate the proposed method
improves the efficiency comparing with basic MCS without
losing accuracy. The second one is used to illustrate the
proposed method generates more accurate results than that of
FORM (first order reliability method).

effect of uncertainty have been developed in order to make
reliable decisions by using of probabilistic information.
Let us now focus on the probabilistic constraint functions
in a RBDO problem gi ( d,X ) . We simplify the formation

to g ( X ) , which is considered as system response. g (⋅)

represents the system model under study and is called a
system performance function. X is a vector of the random
input variables, i.e. X = ( X 1 ," , X n ) . For simplicity, d is

I. INTRODUCTION

R

eliability based design optimization (RBDO) and
robust design are two important research topics when
considering engineering design under uncertainty. In RBDO
and robust design, probabilistic models are used instead of
deterministic models since design variables are considered
uncertain. Reliability analysis is an indispensable component
which assesses the feasibility of the design [1].
A typical probabilistic optimization model is defined as a
mean performance measure which is optimized subject to
probabilistic constraints:
[Problem 1]
Find d, µ x
Minimize h ( d,µ x )

Subject to P { gi ( d,x ) ≤ θ } ≥ ri , i = 1, 2,3,..., m.

where d is a vector of the deterministic variables; X is the
vector of the random variables with the mean µx and x is its

realization; h(⋅) is the mean performance measure to be
optimized. Note that a character in bold denotes a vector and
an upper-case character indicates a random variable.
In problem 1, design feasibility, or called reliability, is
defined as the probability of constraint satisfaction. That
is, Pi , the probability for g i ( d,x ) ≤ θ should be no less than
the required reliability level ri (e.g., 99.75%).
Manuscript received May 18, 2007. This work was supported in part by
the NSF under Grant DMI-0239276.
Fan Li is a Ph.D. student in Industrial Engineering Department at Arizona
State University. Teresa Wu is an associate professor in Industrial
Engineering Department at Arizona State University, AZ, 85287, USA
(phone: 480-965-4157; fax: 480-965-8692; email: teresa.wu@asu.edu).

1-4244-1154-8/07/$25.00 ©2007 IEEE.

The computation effort required in analyzing probabilistic
models increases dramatically in comparison to the
deterministic models. Most of the computational time of an
RBDO problem is spent on the evaluation of the probabilistic
constraints, particularly, the so called limit state
function gi ( d,X ) . Recently, many methods for analyzing the

neglected here.
Let g ( x ) ≥ θ denote the system fails, then in the general
reliability problems, the calculation of the reliability
(probability of success), p f , of the system requires the
evaluation of the following multidimensional integral [2]:
p f = P ( g (x) ≤ θ ) = ∫
f X ( X ) dX
[Equation 1]
g ( x ) ≤θ

where f X ( X ) is the joint probability density function of the
random variables.
In general, f X ( X ) is a multidimensional density function
with complex mathematical form, and the failure domain
g ( x ) ≤ θ may be irregular in shape. The problem could be
even more complicated if the failure domain is formed by
several failure regions. In these cases, the integral boundaries
can be piecewise and highly nonlinear. Hence, an accurate
analytical evaluation of the success probability defined by the
constraints in [Equation 1] is almost impossible. This leads to
the need of approximate approaches. Generally, researches
on the approximate reliability analysis can be classified into
four categories: sensitivity-based approximation approaches,
MPP (most probable point) based approaches, Monte Carlo
simulation (MCS) and response surface models based
approaches.
Unfortunately,
MCS
requires
great
computational effort to achieve accurate results;
sensitivity-based approaches and MPP based approaches are
much less computational expensive, while the accuracy is
sacrificed in return; as to the response surface models, the
results are heavily relied on the approximated model
developed. It may potentially have both issues of efficiency
and accuracy.

956

TuRP-A05.3

This paper proposes an efficient method for reliability
analysis based on the concept of importance sampling to
overcome the limitations of the exiting reliability analysis
methods. Importance sampling [3] is a competent approach
which can reduce the number of simulations required while
achieving comparable accuracy as the basic MCS. In this
research, the generation of random variables is biased around
MPP so that the simulating space can be reduced significantly.
The MPP (or called design point) is a particular point in the
design space that has been used to approximately evaluate the
probability of system failure. The design space is thereafter
divided into the safe region and the important region.
Evaluating the system performance function only at the
important region will greatly enhance the computation
efficiency.
This paper is organized as following: the related literature
is briefly surveyed in section 2, followed by the detailed
explanation on the proposed approach in section 3. Section 4
then uses illustrative examples to show how the proposed
approach can be used for reliability analysis and a series of
comparison experiments are conducted. Finally, the
conclusion is drawn in section 5.

II. LITERATURE REVIEW
A. Sensitivity-based approximation approaches
One commonly used method for reliability analysis is the
sensitivity-based approximation approaches which include
the worst case analysis and the moment matching method [4]
[5]. The worst case analysis assumes all the random variables
are contributing to the worst situation (s) of a system at the
same time. The worst value of the system output (extreme
condition) can be found by the first order Taylor expansion or
optimization. In most cases, the worst case analysis is too
conservative e as it is unlikely that all the random variables
could occur simultaneously so that the system reaches the
worse case. The moment matching method uses the first order
moment (mean value) and the second order moment (standard
deviation) to analyze a system output. Generally the moment
matching method is not sufficiently accurate, especially when
the random variables have large variations or are not normally
distributed.
B. MPP based approximate analytic methods
MPP based approaches require the variables normally
distributed. In case the variables X are not normally
distributed, transformation, e.g. Rosenblatt [6], is commonly
applied. System performance function g ( X ) is thus
transformed to g ( U ) . Then, in a probabilistic model, the
limit-state function can be defined as:
g ' (u ) = g (u ) − θ = 0

where U = (U1 , U 2 ,..., U n ) , U i are mutually independent and
normally distributed, θ is the performance specification.
MPP based approaches are characterized by the use of

analytical techniques to find a particular point in the design
space which can be used to estimate the probability of system
success, defined by the limit state function. This point is often
referred to as the MPP, which has the highest probability of
producing the value of θ for g ( u ) . In the standardized
normal space, the MPP can be found by finding the point on
the constraint boundary (limit state) which has the minimum
distance to the origin:
β = min u
Subject to g ' ( u ) = 0
The minimum distance β is called reliability index.
Recently, MPP based approaches have emerged as
efficient uncertainty analysis methods. The typical reliability
analysis methods based on the MPP include the FORM (first
order reliability method) and SORM (second order reliability
method) [7] [8] [9]. FORM and SORM estimate the
probability by simplifying the limit state function g ' ( U )
using first or second order Taylor series expansion at the
MPP.
If the FORM is used, the probability of constraint
satisfaction is given by the following equation:
P ( g ' (u ) ≤ 0) = φ ( β )
[Equation 2]
where φ (⋅) is the standard normal cumulative distribution
function.
Figure 1 is the demonstration of MPP in two-dimension
design space. Figure 2 is the illustration of the FORM in
two-dimension design space.
u2
MPP

β

.

g ' ( u1 , u2 ) = 0
Failure Region g ' ( u1 , u2 ) > 0

u1

Figure 1 the limit state function and MPP

While promising, there are several disadvantages that limit
the application of MPP based methods.
 In most cases, the limit state functions are highly
nonlinear especial when the transformation into
independent normal space is applied. Because of the high
nonlinearity, both FORM and SORM may not estimate
the limit state function accurately especially for high
dimension.
 Both FORM and SORM are based on the concept of
MPP. However, Annis [10] uses the real laboratory
fatigue crack growth data to demonstrate that the “most
probable point” does not exist in the real world. Thus,
analyzing the reliability solely based on MPP is
questionable.

957

TuRP-A05.3

smoothed function is used to evaluate the sensitivities in
probability analysis.
In appreciation of the above literature survey, we recognize
the need for a generalized approach for reliability analysis
with both high accuracy and computational efficiency. It
should not lead to either over-conservative or infeasible
design solutions as most simplified approaches do. Such an
approach should also be affordable in complex engineering
design. This paper proposes an efficient method for reliability
analysis that is practically affordable in engineering design
applications.

f ( u1 , u2 )

β

FROM

u1

•
u2

Failure Region g ' ( u1 , u2 ) > 0

MPP

III. IMPORTANCE SAMPLING BASED RELIABILITY
ANALYSIS

g ( u1 , u2 ) = 0
'

Figure 2 the first order reliability method

C. Monte Carlo Simulation
Among the approximate methods, a class called the Monte
Carlo sampling method is widely used. The basic Monte
Carlo simulation method (MCS) is to use numerical sampling
technique that simulates a process involving two steps:
realize the random variables, and then determine if a
particular event (e.g. failure) occurs for each simulation. The
ratio of the number of failures to the total number of
simulations is an estimate of the failure probability.
MCS can generate the cumulative distribution function
(CDF) and the probability density function (PDF) of a system
output based on data sampling. Neglecting the algorithmic
error caused by simulations, if sufficient numbers of
simulations are used, MCS often results in solutions with a
high accuracy. In addition, it is flexible for any types of input
distributions and any forms of model functions. Compared
with other methods, MCS has a remarkable feature that its
accuracy does not depend on the dimension of the random
model input variables.
However, the shortcoming of MCS is that great
computational effort is required for any general cases. Since
low failure probabilities are typically expected or required for
many real-world engineering systems, the total number of
simulations necessary to obtain a sufficient number of
“failure samples” can be extremely large, and for those
complex problems, the MCS is too computational expensive.
D. Response Surface Models
To alleviate the computation burden of the direct
simulation on the original models, an alternative approach
that is considered uses the response surface models to replace
the original system performance functions [11] [12] [13].
Data sampling is then performed based on the response
surface model instead of the original complex model. The
drawback is the high cost for generating an accurate response
surface model for the purpose of reliability analysis. Besides,
some of the response surface methods tend to “smooth” the
performance. This results in accuracy issue when the

MCS is accurate and reliable approach for reliability
analysis, especially for problems having multiple failure
regions. Yet the basic MCS is computationally intensive. To
reduce the number of simulations required, some modified
MCS methods have been proposed to improve the
computational efficiency. One commonly accepted method
among them is the importance sampling method.
Importance sampling is a variance reduction technique. It
biases the generation of the random variables using an
importance sampling density function such that the number of
occurrences of system failure will increase. The results are
then scaled to account for the bias in the sampling distribution
[2]. Importance-sampling technique has been used in recent
years in conjunction with MCS for reliability evaluation. Ang,
et al, estimate the optimal importance-sampling density based
on kernel method [2]. However, kernel method based
importance sampling is hard to obtain the optimal
importance-sampling density and the knowledge of the form
of the optimal importance-sampling density is only a guide
toward effective sampling. Secondly, it is not easy to
determine the number of simulations after the
importance-sampling density is obtained. Finally, the
importance sampling may result in a biased reliable estimate
since it only focuses a small region over the whole failure
domain.
In order to solve this dilemma, this proposed approach uses
a novel importance sampling method to improve MPP based
reliability analysis. First, the MPP is located. Secondly, based
on the MPP, the whole design space is then divided into two
regions: the safe region and the important region. The
simulation is then performed in the important region and the
biased reliability is estimated in this reduced space. Finally,
this biased reliability is scaled to account for the bias over the
whole design space. The process is illustrated in figure 3.

958

TuRP-A05.3

(safe region) is P ( u ≤ β) = (2φ(β) − 1)n , in which n is the

f (u )

number of random variables (also called dimension of the
problems). For example, for one dimension problems, the
probability of the removed region is P ( u ≤ β ) = 2φ(β) − 1 ; for

Safe region

two dimension problems, the probability of the removed part

Important region

is (2φ (β ) − 1)2 . The remaining space, shown in figure 3(b), is
the importance sampling region.
The remaining region is called important or reduced region.
Because the total probability is 1 and the removed region has

u

β

(a)

β

the probability of (2φ(β) − 1)n , the remaining probability is

f (u )

β

1 − (2φ(β) − 1)n .
The process that the random variables are biased to the
reduced region (around MPP) is illustrated as figure 3. Since
figure 3(b) is the importance sampling region, we need to
determine the probability distribution of this region.

u

β

C. Determine the importance sampling density function in
the important region
After the random variables are biased around the MPP, in
order to performance the simulation, we need to determine
the importance sampling density function, i.e. to determine
the distribution of the random variables in the remaining
reunited space shown in figure 3(b). However, it is not easy to
determine the distribution of U in this region. In order to
solve this problem, we reunite the remaining region as figure
3(c). Since the original space has normal distribution, it is
interesting to note that each random variable within the
remaining region still has the standard normal probability
density function.

(b)

f (u )

u
(c)
Figure 3 the reduced region for
systems with two random variables

D. Simulation
A. Search the MPP
The first step of the proposed approach is to search the
MPP. In order to find the MPP, random variables X are
transformed into a standardized normal space, called U space.
Then the equivalent mean and standard deviation are
estimated using the Rackwitz-Fiessler method, or Chen-Lind
method [14], etc. After the transformation, different
techniques can be used to search the MPP, such as the
methods in [9] [15] [16], and the optimization methods. In
this research, it is formulated as an optimization problem.

1) Sample from multivariate distribution
During the sampling process, each variable has the
distribution with normal probability density function. Since
the random variables are biased around the MPP, the failure
rate increases greatly and the simulation numbers would
decrease compared to the original simulation numbers.
Suppose the original simulation number is N 0 , then to
achieve the same accuracy, the simulation numbers of the

B. Bias the random variables around the MPP
In this step, the random variables are biased to a reduced
region and the simulation number can be reduced greatly.
Please recall that after transformation in the above step, each
variable has standard normal distribution, so the whole
probability space has multivariate normal distribution.
Suppose the distance from the origin to MPP is β , we then
draw a circle with the origin as the center and β as the radius.
Since we are certain that the failure events will not happen
within this circle, we can remove the region within the circle
from the whole probability space. We call this removed
region as safe region. The probability of the removed region

simulation numbers N 0 is 10 6 , then if β = 2 , N = 8.9 × 10 4 ;

(

)

proposed approach is only N = 1 − (2φ (β ) − 1)n × N 0 . If
β increase, N decreases. For example, assume the original
if β = 2.5 , N = 2.5 × 10 4 ; if β = 3 , N = 0.5 × 10 4 .
For low probability systems, such as the aircraft systems,
the reliability needs to be very high and the failure rate is very
low, that is, β is very large and the reduced region has small
probability. In this case, the simulation numbers decrease
drastically, so the proposed approach is expecially efficient
for high reliability systems.
2) Evaluate the biased probability
Since the random variables is biased to the reunited
important region (around the MPP) and the random variables
are distributed with the normal importance sampling density

959

TuRP-A05.3
Table1. Results from the proposed approach and the basic MCS

function, the vector of the coordinates of a sample, u , should
be transformed to u ' , where u ' is the vector of the coordinates
of the samples in the original space. The transformation for
systems with two random variables is shown in figure 4.
u2

(

β •u1' ,u 2'

)

u1

Figure 4 Transformation from the reduced
region to the original space

In figure 4,
u1 ' = ( r + β ) cos α

u2 ' = ( r + β ) sin α
u
where tan α = 2 ; r = u1 + u 2
u1

106

5392

Table2. Results from the proposed approach and the FORM
FORM

where N is the number of samples which satisfy g ( u ' ) ≤ θ ,
N 0 is the total number of simulations.

Proposed approach
Value
Error

Value

Error

P ( g ( x ) < 45 )

0.9987

+0.21%

0.9977

+0.01%

P ( g ( x ) < 37.5)

0.9969

+0.25%

0.9946

+0.02%

P ( g ( x ) < 30 )

0.9928

+0.57%

0.9871

0

Note: “+” means the value is greater than that from MCS.

V. CONCLUSION

E. Evaluate the overall reliability
The overall probability is then calculated by compensating
for the bias of the importance sampling distribution as
follows:

(

0.9977

NFE

NFE: Number of function evaluation

The biased reliability of the system is:
N
P { g ( u ') ≤ θ } =
N0

)

P { g ( u ) ≤ θ } = ( 2φ ( β ) − 1) + 1 − ( 2φ ( β ) − 1) × P { g ( u ') ≤ θ }
n

Proposed approach

0.9976

B. Experiment II
This experiment shows that the proposed approach has a
very accurate estimation of reliability, while FORM is
over-conservative or infeasible. The basic Monte Carlo
simulation is used here as the standard for judgment.
Again, suppose the system performance function used here
is the same as that in experiment I. The results are listed and
compared in Table 2. Note the values from MCS is not listed,
but the difference between FROM/proposed approach and
MCS is shown respectively.

r •(u , u )
1 2

α

Basic MCS

P ( g ( x ) < 45 )

n

IV. COMPARISON EXPERIMENTS
A system with two random design variables is considered in
this research. Assume the system performance function is
g (x ) = 5 * x12 − x22
where both x1 and x2 are random variables with standard
normal distributions.
A. Experiment I
This experiment shows that the proposed approach has
almost the same accuracy as basic Monte Carlo simulation
with much less computational effort.
In this experiment, we are test the probability for g (x ) ≤ 45 .
The results are listed in Table 1, where the solutions of basic
Monte Carlo simulation and the proposed approach are
compared. The number of function evaluation is used for the
comparison. Note extra computation time is required for the
proposed approach for the transformation, and the time to
obtain the MPP is neglectable.

The evaluation of reliability or feasibility is often a
computationally intensive process in reliability design. In this
paper, an importance sampling based reliability analysis
method is proposed to accurately and efficiently estimate the
reliability given the distributions of input variables. In the
proposed approach, the MPP is located and used to reduce the
simulation space. The random variables are biased around the
MPP, so the number of simulations can be reduced
significantly.
Generally, the proposed method is more accurate than MPP
based approaches and more efficient than basic MCS with
comparable accuracy. This work can tackle limit state
functions with any formulations, including problems with
multiple failure regions, and may serve as a better alternative
to basic MCS for low probability failure problems. The
significance of this work lies in the development of efficient
feasibility evaluation methods that can support
reliability-based design optimization (RBDO) and quality
engineering practice, such as the well known Six Sigma
design. For example, after we find the optimal solution at the
end of the RBDO process by using FORM based approaches,
we may want to conduct the accuracy checking. By using the
proposed approach, we can assess whether the solution
obtained through RBDO satisfies the reliability requirement
or not. For Six Sigma design, we can also verify whether a

960

TuRP-A05.3

design model meets Six Sigma standard or not by using the
proposed approach.
Since the simulation number of the proposed approach

(

)

is N = 1 − (2φ (β ) − 1)n × N 0 , when the number of random
variables, n , increases, the number of simulation of the
proposed approach will also increase. For high dimension
problems, the overall efficiency of the proposed approach
decreases with the number of random variables. In addition,
the transformation of the coordinates of samples between the
reduced region and the original space becomes more complex
when n is large. Simplifying the transformation will be the
future research focus.

[14]Haldar, A. and Mahadevan, S., 2000, Probability, Reliability and
Statistical Methods in Engineering Design, John Wiley & Son, New
York.
[15]Y. T. Wu, H. R. Millwater and T. A. Cruse, "Advanced probabilistic
structural analysis method for implicit performance functions," AIAA J.,
vol. 28, pp. 1663-1669, 1990.
[16]X. Du and W. Chen, "A most probable point based method for efficient
uncertainty analysis," J. Design Manuf. Autom., vol. 4, pp. 47-66, 2001.

VI. ACKNOWLEDGMENT
This research has been partially supported by funds from
the National Science Foundation CAREER awards under
grant number DMI-0239276. The U.S. government is
authorized to reproduce and distribute reprints for
governmental purposes notwithstanding any copyright
annotation thereon. The views and conclusions contained
herein are those of the authors and should not be interpreted
as necessarily representing the official policies or
endorsements, either expressed or implied, of NSF or the U.S.
Government.

REFERENCES
[1] B. Huang and X. Du, "Uncertainty analysis by dimension reduction
integration and saddlepoint approximations," J Mech Des, Trans ASME,
vol. 128, pp. 26-33, 2006.
[2] G. L. Ang, A. H. -. Ang and W. H. Tang, "Optimal importance-sampling
density estimator," J. Eng. Mech., vol. 118, pp. 1146-1163, 1992.
[3] A. Harbitz, "An efficient sampling method for probability of failure
calculation," Struct. Safety, vol. 3, pp. 109-115, 1986.
[4] R. J. Eggert, "Quantifying design feasibility using probabilistic
feasibility analysis," in 17th Design Automation Conference Presented at
the 1991 ASME Design Technical Conferences, 1991, pp. 235-240.
[5] A. Parkinson, C. Sorensen and N. Pourhassan, "General approach for
robust optimal design," J Mech Des, Trans ASME, vol. 115, pp. 74-80,
1993.
[6] M. Rosenblatt, "Remarks on a Multivariate Transformation," Annual of
Mathematical Statistic, vol. 23, pp. 470-472, 1952.
[7] M. Hohenbichler, S. Gollwitzer, W. Kruse and R. Rackwitz, "NEW
LIGHT ON FIRST- AND SECOND-ORDER RELIABILITY
METHODS," Struct. Saf., vol. 4, pp. 267-284, 1987.
[8] H. U. Koyluoglu and S. R. K. Nielsen, "New approximations for SORM
integrals," Struct. Saf., vol. 13, pp. 235-246, 1994.
[9] A. M. Hasofer and N. C. Lind, "EXACT AND INVARIANT
SECOND-MOMENT CODE FORMAT," vol. 100, pp. 111-121, 1974.
[10]C. Annis. Why the "most probable point" isn't. Available:
http://www.statisticalengineering.com/Most_Probable_Point.htm
[11]W. Chen, J. K. Allen, K. Tsui and F. Mistree, "Procedure for robust
design: Minimizing variations caused by noise factors and control
factors," J Mech Des, Trans ASME, vol. 118, pp. 478-485, 1996.
[12]R. H. Sues, D. R. Oakley and G. S. Rhodes, "Multidisciplinary stochastic
optimization," in Part 2 (of 2), 1995, pp. 934-937.
[13]P. N. Koch, T. W. Simpson, J. K. Allen and F. Mistree, "Statistical
approximations for multidisciplinary design optimization: The problem
of size," J. Aircr., vol. 36, pp. 275-286, 1999.

961

An Automated DICOM Database Capable of Arbitrary Data Mining
(Including Radiation Dose Indicators) for Quality Monitoring
Shanshan Wang,1 William Pavlicek,2 Catherine C. Roberts,2 Steve G. Langer,3 Muhong Zhang,1
Mengqi Hu,1 Richard L. Morin,4 Beth A. Schueler,3 Clinton V. Wellnitz,2 and Teresa Wu1
The U.S. National Press has brought to full public
discussion concerns regarding the use of medical
radiation, specifically x-ray computed tomography (CT),
in diagnosis. A need exists for developing methods
whereby assurance is given that all diagnostic medical
radiation use is properly prescribed, and all patients’
radiation exposure is monitored. The “DICOM Index
Tracker©” (DIT) transparently captures desired digital
imaging and communications in medicine (DICOM) tags
from CT, nuclear imaging equipment, and other DICOM
devices across an enterprise. Its initial use is recording,
monitoring, and providing automatic alerts to medical
professionals of excursions beyond internally determined trigger action levels of radiation. A flexible
knowledge base, aware of equipment in use, enables
automatic alerts to system administrators of newly
identified equipment models or software versions so
that DIT can be adapted to the new equipment or
software. A dosimetry module accepts mammography
breast organ dose, skin air kerma values from XA
modalities, exposure indices from computed radiography, etc. upon receipt. The American Association of
Physicists in Medicine recommended a methodology for
effective dose calculations which are performed with CT
units having DICOM structured dose reports. Web interface reporting is provided for accessing the database in
real-time. DIT is DICOM-compliant and, thus, is standardized for international comparisons. Automatic alerts
currently in use include: email, cell phone text message,
and internal pager text messaging. This system extends
the utility of DICOM for standardizing the capturing and
computing of radiation dose as well as other quality
measures.
KEY WORDS: Data extraction, medical informatics
applications, radiation dose, database management
systems, knowledge base

BACKGROUND

Q

uality assurance (QA) monitoring of ionizing radiation use with certain diagnostic and

Journal of Digital Imaging, Vol 24, No 2 (April), 2011: pp 223Y233

procedural imaging is required by state regulatory bodies and by The Joint Commission in
the US.1 Of the eight U.S. National Quality
Measures Clearinghouse2,3 (federal) metrics specific to Radiology, two deal with radiation
exposure: reduction of computed tomography
(CT) dose and documentation of x-ray fluoroscopy duration. Since these requirements are
enterprise-wide, radiologists, cardiologists, vascular surgeons, urologists, GI physicians, and other
permitted users of x-ray and nuclear equipment are
required to be monitored to satisfy the national
goals. Recently, public discussion in the U.S
national news has expressed concerns regarding
the use of CT, including its expanding use, and
instances of erythema that have occurred following
certain CT examinations.4–6 These concerns drive
medical facilities to provide enhanced monitoring
of radiation use.7,8 While picture archiving and
communication systems (PACS) can provide some

1

From the School of Computing, Informatics and Decision
Systems Engineering, Arizona State University, 699 S. Mills
Ave.( Tempe, AZ 85287, USA.
2
From the Department of Radiology, Mayo Clinic Arizona,
13400 East Shea Blvd( Scottsdale, AZ 85258, USA.
3
From the Department of Radiology, Mayo Clinic Rochester,
200 First St. SW Rochester, MN 55905, USA.
4
From the Department of Radiology, Mayo Clinic Florida,
4500 San Pablo Road( Jacksonville, FL 32224, USA.
Correspondence to: Teresa Wu, School of Computing,
Informatics and Decision Systems Engineering, Arizona State
University, 699 S. Mills Ave.( Tempe, AZ 85287, USA; e-mail:
teresa.wu@asu.edu
Copyright * 2010 by Society for Imaging Informatics in
Medicine
Online publication 8 September 2010
doi: 10.1007/s10278-010-9329-y
223

224

WANG ET AL.

insights from archived examinations, they suffer
from several drawbacks:
1. PACS are typically department-level devices,
and do not offer insight into a patient’s
exposure across multiple departments or sites.9
2. Commercial PACS, for performance reasons,
generally do not permit performing a query on a
live production system.
3. Radiation dose relevant digital imaging and
communications in medicine (DICOM) tag values
can be missing, inconsistent, or inappropriate.
At this time, records of fluoroscopy radiation
use are commonly hand-recorded into paper logbooks typically at the operator’s control console,
or are manually retrieved from PACS. However,
fetching the whole exam to determine the relevant
information (e.g., dose and exam duration time) is
obviously uneconomic, as is the common approach
of manually reviewing a large number of exams to
generate user- or protocol-specific radiation quality
reports. A standardized toolkit is needed to
automatically capture and integrate radiation dose
records in a timely fashion.
One precursor of such standardization is the
integration of Structured Reporting (DICOM WG—
08,10) standards to exchange structured data during
image acquistion or post-examination so that radiation dose indicators or dose-related information is
available in PACS.11 Two tools currently available
are CT dose SR and X-ray angiography (XA) SR.10
CT dose SR develops a record of patient episode of
radiation that includes the radiation-dose-related
parameters. Summary reports can be sent to PACS
and included in a patient’s health record. However,
no known tools currently exist for centralizing the
documented administration of ionizing radiation
episodes of care into a form suitable for routine use
by quality assurance monitoring staff. In this
research, DICOM Index Tracker© (DIT) is designed
to serve as a patient-centric, integrated DICOM
information repository (including radiation dose
index history) combined with continual monitoring
and reporting tools.

nal patient-specific exam indices of interest for all
diagnostic and procedural uses of imaging modalities. This information system consists of six
modules: a DICOM receiver accepting and parsing
imaging files sent from equipment or PACS, a
knowledge base (i.e., modalities, manufacturers,
model, and software versions of known equipment) with mappings of the standard and proprietary DICOM tags of interest, a patient episode
tracking database, dosimetry analyzer tools, a
configurable web reporter, and an automated
alert/messaging mechanism (e.g., unknown scanners, high radiation exposure, etc.). The system
architecture of DIT, including the composite
modules and the connection with other software,
are shown in Figure 1. The implementation and
evaluation of each component is summarized as
follows.
DICOM Receiver and Parser In DIT, the module
called ‘DICOM Receiver and Parser’ receives
images from DICOM-compliant devices/systems,
extracts the tag information, and stores them into a
DIT database. The Receiver takes advantage of the
DICOM transport protocol12—the de facto standard
in medical imaging transfer fully supported by the
majority of device manufacturers and PACS
vendors. DIT is able to receive and process medical
images from various locations, modalities, and
equipment. Following the receipt of images, the
DICOM header information is parsed and
transformed to XML format and then desposited to
the tables in DIT database. DIT does not store image
data (it can accept the DICOM Secondary Capture)
to enhance performance and reduce storage needs.
Knowledge Base DICOM tags commonly differ by
modality, vendor, and software version even for the

METHODS

DIT is designed and implemented to be fully
integrated into a DICOM-compliant infrastructure
with the ability to capture and maintain longitudi-

Fig 1. The architecture of DIT.

AN AUTOMATED DICOM DATABASE CAPABLE OF ARBITRARY DATA MINING

same vendor and modality. The continuously
evolving DICOM standard often results in a given
vendored product version being behind the newer
DICOM committee standards. Nonetheless, even the
base standard provides extensive information about
the patient, the exact medical examination protocols,
and the imaging modality acquisition settings. Every
DICOM-compliant image file contains modality-/
exam-specific attributes including: patient age,
gender, examination ordered, protocol, and series
descriptors, location of equipment, equipment
software version, etc. Radiation dose-specific
information varies by modality, but includes: kVp,
mA and/or mAs, field of view, exposure index, air
kerma at the patient entrance reference point (PERP),
breast organ dose, body part, and with some nuclear
equipment, the administered radioactivity (Bq). In
the case of mammography, mean glandular organ
dose is provided. Other important information might
be in an image file (DICOM Secondary Capture). An
example of this is the Exam Protocol screenshot, a
method commonly used with CT to record the CT
dose index (CTDIvol) and dose length product
(DLP) for individual series and total examination.
Because of the variation in supplied DICOM tags
from different scanners, the Knowledge Base
architecture design (Fig. 2) is included. Its schema
identifies known scanners (defined as those having
their hardware and software specifications, as well as
the standard and proprietary dose-related tags present
with this equipment) as being fully known by the
database. The tag mappings drive the proper
algorithm selection for calculating a dose
approximation. Specifically, each known scanner
(termed Known_scanners) belongs to one group of
specific tags (termed Grp_specific_tags). In
Known_scanners, the Grp_ID defines the group of
proprietary data elements that will be harvested
(chosen once, during a new equipment installation

Fig 2. The database schema of Knowledge Base.

225

or with a new software version) from the DICOM
header (in addition to the standard elements e.g.,
image UID, accession). The mapping from Grp_ID
to dose-related information is defined in
Grp_specific_tags. The advantage of such schema
design is that it provides the system extensibility. For
example, when a new scanner is added, or for an
existing scanner, an update of software version, one
record is added to known_scanners, and its list of
dose-related tags are determined and appended into
Grp_specific_tags. Importantly, this extension does
not interfere with the processing of existing images,
and it does not require any modification of
programming with new images. Overall, the cost of
adding an unknown scanner software version to DIT
is minimal.
Since the Knowledge Base serves as the
foundation of the DIT system, its comprehensive
and accurate content is essential for a successful
implementation. The list of vendor modality specific
tags was initially created as a team effort: radiology
specialists from each modality, DICOM engineering
staff, quality assurance staff, and system developer.
The task ensures that the collected tags will satisfy all
the facets of technical and expected QA needs e.g.,
dose calculation and quality monitoring within
system technical constraints.
DIT Database The database is designed to store all
harvested tags while avoiding redundancy and
maintaining a balanced storage to optimize data
access efficiency. The resultant schema contains five
parts: patient information, exam information (e.g.,
protocol), series information, image information, and
vendor modality-specific-related information (e.g.,
dose). When images are sent from varied imaging
devices into the DIT, the DICOM header is
extracted; the data elements are parsed out and
populated into the database. The specified elements
are placed in database tables appropriate to the level
of the information.
In Figure 3, the table Patient has one record
for each patient with information such as
patient_local_ID, gender, and DOB. The table
Exam contains the basic information pertinent to
each exam common with each modality. This
includes exam information (exam description,
referring doctor, and radiologist), exam time,
location, etc. The table Series stores the
information related to series, such as series_UID
and Series Description; and the table Images keeps

226

WANG ET AL.

Fig 3. The database schema of DIT Database.

the information at the image level, such as
Image_UID and image acquisition time. The table
Dose_Related_Info contains the data elements
appropriate for the modality, device, and software.
The schema is in a serial arrangement as in
Figure 3, where the relationships between two
neighboring tables are all one to many. That is, one
Patient may have one or more exams, one Exam
record contains one or more series, one Series
usually have one or more Image records, and one
Image record may be related to one or more
Dose_Related_Info records. In this manner, the
hierarchical structure of patient, exam, series,
images, and dose information is reflected in the
database design.
The DIT database connects with the Knowledge
Base to populate Dose_Related_Info. When each
exam reaches DIT, tags for modality, manufacturer,
model, and software version will be used to match
the image to a known scanner in Knowledge Base. If
the match is successful, DIT will fetch the list of
specific desired tags from the Knowledge Base table
Grp_specific_tags and populate these tags (taken
from image header) into the DIT Database table
Dose_Related_Info. If the match fails (the scanner/
software is unknown), the database will automatically
alert the database manager or medical physicist of the
need to exactly specify DICOM tag dose-related
fields for future archiving.
Such database design minimizes data redundancy
and accelerates a database search. The lower-level
tables (left side of table in Fig. 3) have a far greater
number of records, and thus, results in signficant
computational cost to conduct the query as compared
to the higher-level (right side of table in Fig. 3)
tables. For example, to list the exams of specific
patients, a query on the tables Patient and Exam is
sufficient without involving table Series, Images, and
Dose_Related_Info. Additionally, the design of table
Dose_Related_Info gives improved flexibilty for

querying any desired information since the number
of dose-related data elements, their meaning, and
units are not restricted by the table structure.
Dosimetry Analyzer For each ionizing radiation
modality, i.e., CT, CR, XA, PET, DX, etc., the
computation of a patient-specific radiation dose
estimate for a single (and each) episode of care is
required in order to develop an overall cumulative
estimate of dose associated with an individual.
Extracted (tag) information available in the DIT
includes wide ranging dose information;
examination, and protocol, gender, age, projection
angles, fluoroscopy exposure rate and time, air
kerman rate, air kerman area, PERP, DLP, Bq
breast organ dose, CTDIvol, dose area product
(DAP), etc. This modality-specific information can
be used by physicists to assess or estimate a dose
particular to a specific exam, or to compare types of
procedures, different vendors, or software versions,
physican or technologist operator, and even different
medical facilities. In some cases, the equipment and
software version do not support any tag to precisely
compute the dose. For example, with CT, a lack of
consensus currently exists on the best method of
computing an estimate of dose. Nonetheless, at least
a recording of the number of specific examinations,
by modality, can be identified for each patient.
Web Reporter With increasing numbers and multilocation episodes of diagnostic and interventional
radiation examinations and procedures, IT systems
must be capable of providing online reporting; this is
arguably the only appoach to handling the
complexity of access to dose information. It is also
needed to provide distributed real-time review by
current practice.13 For quality assurance purposes, a
web reporting module enables the management, and
performs ad hoc execution, of configurable reports.
Each report template is created in Oracle® Reporter

AN AUTOMATED DICOM DATABASE CAPABLE OF ARBITRARY DATA MINING

(Oracle®, Redwood Shores, CA) to run on an Oracle
Report® server. The report profile can be selected in
ASP.NET web pages; users set their configuration
and generate reports online. As the focus of this
project is quality assurance, in which the content is
varied but not fully defined, the tool is intentionally
general in design, providing flexibility to other
database-based web reporting applications.
This tool can be modified to provide reports on
many of the DICOM tag values that may be of
future interest. Alternatively, the results of a search
also create a listing of the data requested which is
rendered in a data table. A mammography search
might include radiation dose, breast thickness, and
the filter type (either chosen by the technologist or
automatically selected by the equipment). A Save
button creates an Excel file that is available for
creation of any desired graph format using i.e.,
Chart Wizard (the user-friendly Excel function to
generate charts).14 A box to insert legal disclaimers
regarding the appropriate use of privileged QA
information is included.
Alert Mechanism This highly important instant
message alert system enables the time by
notification of user-defined critical events. By setting
different levels of urgency, the designated QA staff
are alerted to preconditioned events in near real-time.
DIT alerts are generated automatically during userconfigurable time period monitoring (i.e., every hour,
or at midnight). When the service level reaches some
warning threshold or Triggers, an alert is sent.
At this time, alerts are sent via internal email,
designated cell phone text messaging, and internal
pager text messages. All messaging are organized
by a distribution list, and multiple staff receive
specific messages. Alerts are also needed to
maintain the robustness and accuracy of the
system; any newly arriving images received by
an unknown scanner generate an email message to
support staff for a review of the data record and for
an update on the Knowledge Base with the
appropriate DICOM tags.

RESULTS

Knowledge Base The system consisting of all the
above six modules was developed at Mayo Clinic
Arizona (MCA) starting in January, 2009. Testing
and development of DIT has continued since August

227

2009. DIT (v1.0) was introduced into clinical use in
September, 2009. The initial test database contained
approximately 650 exams (~70,000 image headers)
purposely chosen from different vendors and
modalities to expand the Knowledge Base to
approximately 25 different devices at MCA.
Currently, additional devices are being added from
Mayo Clinic Florida and Mayo Clinic Rochester.
These include CT, MRI, US, interventional
radiology, computed radiography, DR, cardiac
catheterization, nuclear PET, and mammography.
Ongoing testing and validation enables the system to
improve and be accepted for use in clinical service at
MCA. The MCA volume expected for 2010 are
250 K examinations having ~1 M image headers;
however, historical archive of examinations is being
considered.
Web Reports QA staff are provided with ad hoc
access to protocol and device specific as well as
population-wide and patient-specific radiation
exposures indices. Figure 4 shows the current web
user interface for creating ad hoc reports. The user
selects from the drop down menu for one of the eight
available reports—DAP Report, CT mSv report,
Exam Duration Time Report, Table Utilization
Report, Inter-Exam Time Report, Appointment
Interval Time Report, and/or Inter-Series Time
Report. Each report is designed to address a quality
assurance concern and is provided with configuration
options. For example, the DAP Report contains the
population-wide statistics of DAP for IR exams
during a certain period.15 If DAP report is selected,
the user is allowed to further input

 Dates: start date and end date of the report
time scope
 Action Level I: the Level I DAP threshold.
The exams with DAP beyond this level will
trigger the control Action I. For example, an
up-to-date report of exams can be reviewed in
the quarterly QA committee meeting.
 Action Level II: the Level II DAP threshold. The
exams with DAP beyond this level will trigger
the control Action II or, if wished, an Alert
 Period (day, week, month, quarter): the
frequency by which the statistics should be
summarized.
A request with these chosen configuration
parameters is sent to the web server when the

228

WANG ET AL.

Fig 4. Web based user interface for report selection. In this case, dose area product (DAP) is selected.

user clicks the GenerateReport button. The web
server will execute the request on an Oracle
Report® server which is responsible for querying the database, composing the report, and
returning the result to the user. The report will
display in an ASP.NET web page, and the user
can easily save it as a pdf file on the local drive
(Fig. 5).

Reports provide full details and, to faciliate the
interpretation of data, summarize statistics, charts,
and diagrams. For example, the DAP Report lists
the related exams and shows the histogram of
DAP distribution by period. Figure 5 is the last
page of a DAP Report, summarizing the weekly
DAP statistics from 01 January 2009 to 31
December 2009. The week is plotted only if data

AN AUTOMATED DICOM DATABASE CAPABLE OF ARBITRARY DATA MINING

229

Fig 5. Example report of DAP trends for the year 2009 (based on test data).

exists in that particular time period; therefore, the
x-axis is not necessarily continuous. The y-axis
shows the percentages of exams falling into three
categories: values less than Action Level I are
considered safe (in pattern), caution is noted for
values between Action Level I and II (gray), and
values that exceed Action Level II require attention
(black). Such diagrams may prove useful in
indicating the trends that may require special
attention. Figure 5 shows an increasing number of
exams with high DAP from the 27th week to the
30th week.
Figure 6a is a CT Effective Dose Report
comparing dose usage with exams from different
protocols (head vs. neck) using test data obtained
with a DICOM Dose SR-compliant CT scanner.
Effective Dose metric quantifies the radiation
effective dose of these CT exams.16 In this
report, Effective Dose metric is computed using
DLP, patient’s age, and target region following
the method described by AAPM Report 96.17
Plotting the number of exams in the discretized
range of Effective Dose, the report renders a range
of use, as illustrated. From Figure 6a, it is seen
that test exams on the neck result had a higher
Effective Dose than the head test data. Any
desired comparisons of Effective Dose by equip-

ment and/or by exam types and physical location
is possible.
Other report examples are shown in Figure 6b to d,

 Exam Duration Report: summarizes the statistics of exam duration. Users can generate
the reports based on modality, or device, or
protocol.
 Inter-Exam Time Report: summarizes the
statistics of inter-exam time. Inter-exam time
is an estimate of empty table time between
successive patient exams.
 Inter-Series Time Report: summarizes the
statistics of inter-series time. Inter-Series time
is the time between the successive series in
one exam.
The results of these reports are described in
details in the paper of M. Hu et al.18
Alert Messaging Real-time notification to QA staff
is automatically performed as the application
executes the alerting process on a predefined
frequency (e.g., every 2 h, each day) configurable
by the user. Periodic queries by the database check
each predetermined criteria and send alerts when
any condition is satisfied. The existing triggers are
managed via a web user interface (Fig. 7). Users

230

WANG ET AL.

Fig 6. a Dose Report for head and neck exams with a DICOM SR-compliant CT scanner. b Exam Duration (time) report. c Inter-exam
report: the empty table time between patient exams. d Inter-series report: the waiting time between series of the same exam.

can select the alert from a dropdown list. The
explanation and conditional parameters are shown.
Currently, six alert types are in clinical use:

 Multiple CT perfusion: monitors for multiple
CT perfusion examinations on the same
patient within a specified period of time
 Single-exam DAP: monitors for a single-exam
instance in which a DAP value exceeds a
configurable threshold
 Mulitple-exam DAP: monitors for multiple
exams on the same patient where the
accumlated DAP value exceeds a configurable
threshold
 Single CT perfusion: monitors for a CT perfusion
single instance which exceeds predetermined
values of mA, or seconds, or kVp

 Device Idle Time: monitors for long idle
equipment time during scheduled working
hours
 Unknown Scanner: notifies staff when there
are DICOM images sent to DIT from an
unknown scanner
As an example, Figure 7 illustrates a single CT
perfusion alert addressing an individual patient’s
potential for high exposure during CT perfusion
exam. The exam time(s), kVp, and mA in each CT
perfusion exam is monitored by being compared
with the predetermined thresholds. If any of the
values exceed the trigger value, an alert (mobile
text or email) is sent to all the recipients in the
distribution list to call for immediate investigation
and response.

AN AUTOMATED DICOM DATABASE CAPABLE OF ARBITRARY DATA MINING

231

Fig 7. The web page to configure the alerts in the event of a patient experiencing excessive procedure time/kVp/exposure in CT
perfusion.

Each trigger has parameters for configuration. In
the case of Multiple-exam DAP alerts, the following
can be set:

 The vTimeLimit (in seconds) is the upper
limit of the procedure duration time

 The vKVPLimit (in kV) is the upper limit of
kVp level adopted in the procedure
 The vExposureLimit (in mAs) is the upper
limit of exposure in the procedure
 The vKeyword is the indicator of CT perfusion procedures that is contained in the Series

232

WANG ET AL.

Description tag (0008,103E) in DICOM
header
 The recipients of the email alerts
 The title of the email
 The contents of the email. In addition to plain
text, the user can customize the email with
dynamic information items, e.g., {patient_ID},
{Accession}, and {date of exam} so that the
email recipient will be supplied with enough
information to do further investigation. For
instance, given the {Accession}=123456, the
template ‘Exam {Accession} has …’ results in
an email content ‘Exam 123456 has ….’
Alert messages contain addressable destinations (email box, cell phone text address, and
internal pager text mailbox). In this manner,
individualized alerting is permitted, including
full national coverage. For internal paging, alerts
are sent to Arizona, Florida, and Minnesota from
a central database.
As one of the testings, email, and internal pager
alerts for 27 most recently performed CT perfusion
examinations notified the QA staff of the exact xray acquisition employed. This automatic real-time
review of all Perfusion Exams provided a validation of the dose employed as recently recommended by the FDA.

DISCUSSION

In 2006, the European SENTINEL project19
carried out an evaluation of the practice of computerized patient records. The importance of recording
and analyzing DICOM header information for the
audit of radiation dose is echoed in the paper of Vano
et al..20 As they pointed out, automatic dose analysis
would improve the clinical practice. As also noted,
important differences exist among x-ray systems
regarding the content and format of dosmetric
information. The enormous variability of equipment-specific data, coupled with the evolving and
varied software versions in actual use, creates
challenges in assembling a universal (multiple
location, modality, and device) view of patientspecific or population-wide longitudinal exam
parameters (such as radiation exposure or dose).
The wide availability of DICOM-compliant
enterprise archives9,21 in the US has enabled the

development of a centralized system to manage
certain device-specific QA information obtained
with medical images. By parsing the DICOM
image headers and storing the messages to a
database, Prieto et al. automated the detection of
exam retakes.22 However, the usage of such a
system is limited if vendor equipment-specific data
(including custom “shadow” tags) are not considered. The difficulties of handling cross-institutional,
multi-modality data are reflected in the work of
Laprise et al.,23 where a process involving different
roles was defined (Clinical Research Associate,
documentation coordinators) to review, classify,
and store the exams. However, this process requires
considerable human intervention. With DIT, the
DICOM images are sent directly from the modality
to both the DIT and the PACS, automating the data
importing process. The Knowledge Base enables the
automatic storage/retrieval of the various information for different modalities and across software
versions.
It is believed that DIT is the first implementation that provides a vertically integrated DICOMcompliant solution through information collection
to intelligent data analysis/reporting/alerting in a
clinical quality assurance context. The system is
empowered by a number of IT techniques, including Oracle® database, Oracle Report®, ASP.NET,
JavaScript, web, and email server, and it applies
the domain knowledge of radiology specialists in
dose prescription, estimates, and compliance metrics. DIT provides multiple options to generate
patient-centric or population-wide dose reports and
alerts. The data pool forms the foundation of
evidence-based quality assurance practices and is
ready for use in other applications.
The future development of DIT will bring
tangible and intangible benefits to the practice of
patient care and quality assurance. For example,
we are currently conducting two pilot studies: the
use of DIT for peak skin dose calculation and
localization, and imaging devices efficiency.
These will be reported separately.14,18,24 Dose
reports for an individual patient may be customized to include the radiation history from all
modalities and locations employing DICOM. This
goal requires initializing a web call for the Master
Patient Index to gather the patient global ID across
multiple sites, to be accomplished upon the receipt
of access approval from the respective institutional departments. Since DIT provides different

AN AUTOMATED DICOM DATABASE CAPABLE OF ARBITRARY DATA MINING

data than other patient health information system,
integrating DIT with existing Radiology Information
System and Healthcare Information System systems
will extend the benefits of all systems and will be
explored in the future. Other future developments
will include the use of business intelligence techniques (forecasting, risk–benefit analysis, optimization, and data mining) which have been successfully
applied in other industries (manufacturing and
finance service) for web reporting.

CONCLUSION

Requirements and recommendations from the
Joint Commission, FDA, and national, regional,
and individual healthcare providers in the US
regarding radiation exposure can be addressed
using DIT. The contributions of this work are
twofold: providing an enterprise system solution
for automated dose calculation in a multi-location,
multi-modality, and multi-device environment, and
forming a basis for evidence-based quality assurance and intelligent analysis regarding radiation
dose estimates. Continuing advances with dosimetry analysis are predicated on greater availability
of the DICOM Dose SR.
REFERENCES
1. The Joint Commission: The Joint Commission Sentinel
Event Policy and Procedures, Oakbrook Terrace, IL, 2007
2. Agency for Healthcare Research and Quality, Available at
http://www.ahrq.gov/. Accessed 03 January, 2010
3. National Quality Measures Clearinghouse. Available at
http://www.guideline.gov/browse/xrefnqmc.aspx. Accessed 03
January, 2010
4. FDA News Release. ‘FDA makes interim recommendations to address concerns of excess radiation exposure during
CT Perfusion’, Dec 7, 2009. Available at http://www.fda.gov/
NewsEvents/Newsroom/PressAnnouncements/2009/ucm193190.
htm. Accessed 18 January, 2010
5. Opreanu RC, Kepros JP: Radiation doses associated with
cardiac computed tomography angiography. JAMA 301
(22):2324–2325, 2009
6. Brenner DJ, Hall EJ: Computed tomography: an increasing
source of radiation exposure. N Engl J Med 357:2277–2284, 2007
7. Amis Jr, ES, Butler PF, Applegate KE, Birnbaum SB,
Brateman LF, Hevezi JM, Mettler FA, Morin RL, Pentecost MJ,
Smith GG, Strauss KJ, Zeman RK: American College of
Radiology white paper on radiation dose in medicine. J. Am
Coll Radiol 4:272–284, 2007
8. Center for Devices and Radiological Health, U.S. Food
and Drug Administration, White Paper: Initiative to Reduce
Unnecessary Radiation Exposure from Medical Imaging, 2010.

233

http://www.fda.gov/downloads/Radiation-EmittingProducts/
RadiationSafety/RadiationDoseReduction/UCM200087.pdf
9. Langer S: Issues Surrounding PACS Archiving to External.
Third-Party DICOM Archives. J Digit Imaging 22(1):48–52, 2009
10. DICOM Structured Report WG-08 and WG-15 Supplement 94, WG-21 CT Dose SR
11. Reiner BI: The challenges, opportunities, and imperative
of structured reporting in medical imaging. J Digit Imaging 22
(6):1–7, 2009
12. DICOM Standards, available at ftp://medical.nema.org/
medical/dicom/2009/, Accessed 23 July, 2010
13. Prevedello LM, Sodickson AD, Andriole KP, Khorasani
R: IT tools will be critical in helping reduce radiation exposure
from medical imaging. J Am Coll Radiol. 6:125–126, 2009
14. Peter MB, Schueler B, Pavlicek W, Langer S, Wang SS,
Hu MQ, Wu T: Quality assurance and radiation dose monitoring for digital mammography using the dose index tracker,
2010. Annual Meeting of the Society for Imaging Informatics in
Medicine (SIIM), Minneapolis, Minnesota, June 3–6, 2010
15. Yakoumakis E, Tsalafoutas IA, Nikolaou D, Nazos I,
Koulentianos E, Proukakis C: Differences in effective dose estimation
from dose-area product and entrance surface dose measurements in
intravenous urography. Br J Radiol 74:727–34, 2001
16. Einstein AJ, Henzlova MJ, Rajagopalan S: Estimating
risk of cancer associated with radiation exposure from 64-slice
computed tomography cornoary angiography. JAMA 298:317–
323, 2007
17. The Diagnostic Imaging Council CT Committee: The
measurement, reporting, and management of radiation dose in
CT, Report No. 96, AAPM, 2008
18. Hu M, Pavlicek W, Liu P, Zhang M, Langer S, Wang S,
Place V, Miranda R, Wu T: “Efficiency Metrics for Imaging
Device Productivity”, revised and resubmitted to RadioGraphics
at 07/16/2010
19. WORKPACKAGE 4 -Efficacy and safety in high individual
dose examination, the European SENTINEL(Safety and Efficacy
for New Techniques and Imaging using New Equipment to Support
European Legislation) project, http://www.dimond3.org/
20. Vano E, Padovani R, Neofotistou V, Tsapaki V, Kottou
S, Ten JI, Fernandez JM, Faulkner K: Improving patient dose
management using DICOM header information. The European
SENTINEL experience. Proceedings of the International Special Topic Conference on Information Technology in Biomedicine. Available at http://medlab.cs.uoi.gr/itab2006/proceedings/
Medical%20Imaging/149.pdf, Accessed 22 January 2010
21. Langer S, Charboneau N, French T: DCMTB: A virtual
appliance DICOM toolbox. J Digit Imaging. http://www.
springerlink.com/content/34l2635303wg57l5, 2009
22. Prieto C, Vano E, Ten JI, Fernandez JM, Iniguez AI,
Arevalo N, Litcheva A, Crespo E, Floriano A, Martinez D:
Image retake analysis in digital radiography using DICOM
header information. J Digit Imaging 22(4):393–399, 2009
23. Laprise NK, Hanusik R, FitzGerald TJ, Rosen N, White
KS: Developing a multi-institutional PACS archive and designing processes to manage the shift from a film to a digital-based
archive. J Digit Imaging 22(1):15–24, 2009
24. Khodadadegan Y, Zhang M, Pavlicek W, Paden RG,
Chong B, Schueler BA, Fetterly KA, Langer SG, Wu T:
Automatic monitoring of localized skin dose with fluoroscopic
and interventional procedures. revised and resubmitted to J
Digit Imaging at 7/9/2010

Identifying Alzheimer’s Disease-Related Brain Regions
from Multi-Modality Neuroimaging Data using Sparse
Composite Linear Discrimination Analysis
Shuai Huang 1, Jing Li1, Jieping Ye 2,3, Kewei Chen 4, Teresa Wu 1, Adam Fleisher 4, Eric
Reiman 4
1
Industrial Engineering, 2Computer Science and Engineering, and 3Center for Evolutionary
Medicine and Informatics, The Biodesign Institute, Arizona State University, Tempe, USA
{shuang31, jing.li.8, jieping.ye, teresa.wu}@asu.edu
4
Banner Alzheimer’s Institute and Banner PET Center, Banner Good Samaritan Medical
Center, Phoenix, USA
{kewei.chen, adam.fleisher, eric.reiman}@bannerhealth.com

Abstract
Diagnosis of Alzheimer’s disease (AD) at the early stage of the disease development is of great
clinical importance. Current clinical assessment that relies primarily on cognitive measures proves
low sensitivity and specificity. The fast growing neuroimaging techniques hold great promise.
Research so far has focused on single neuroimaging modality. However, as different modalities
provide complementary measures for the same disease pathology, fusion of multi-modality data
may increase the statistical power in identification of disease-related brain regions. This is
especially true for early AD, at which stage the disease-related regions are most likely to be weakeffect regions that are difficult to be detected from a single modality alone. We propose a sparse
composite linear discriminant analysis model (SCLDA) for identification of disease-related brain
regions of early AD from multi-modality data. SCLDA uses a novel formulation that decomposes
each LDA parameter into a product of a common parameter shared by all the modalities and a
parameter specific to each modality, which enables joint analysis of all the modalities and
borrowing strength from one another. We prove that this formulation is equivalent to a penalized
likelihood with non-convex regularization, which can be solved by the DC (difference of convex
functions) programming. We show that in using the DC programming, the property of the nonconvex regularization in terms of preserving weak-effect features can be nicely revealed. We
perform extensive simulations to show that SCLDA outperforms existing competing algorithms on
feature selection, especially on the ability for identifying weak-effect features. We apply SCLDA
to the Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET) images of
49 AD patients and 67 normal controls (NC). Our study identifies disease-related brain regions
consistent with findings in the AD literature.

1

In tro du cti on

Alzheimer’s disease (AD) is a fatal, neurodegenerative disorder that currently affects over five
million people in the U.S. It leads to substantial, progressive neuron damage that is irreversible,
which eventually causes death. Early diagnosis of AD is of great clinical importance, because
disease-modifying therapies given to patients at the early stage of their disease development will
have a much better effect in slowing down the disease progression and helping preserve some
cognitive functions of the brain. However, current clinical assessment that majorly relies on
cognitive measures proves low sensitivity and specificity in early diagnosis of AD. This is because
these cognitive measures are vulnerable to the confounding effect from some non-AD related
factors such as patients’ mood, and presence of other illnesses or major life events [1]. The
confounding effect is especially severe in the diagnosis of early AD, at which time cognitive

1

impairment is not yet apparent. On the other hand, fast growing neuroimaging techniques, such as
Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET), provide great
opportunities for improving early diagnosis of AD, due to their ability for overcoming the
limitations of conventional cognitive measures. There are two major categories of neuroimaging
techniques, i.e., functional and structure neuroimaging. MRI is a typical structural neuroimaging
technique, which allows for visualization of brain anatomy. PET is a typical functional
neuroimaging technique, which measures the cerebral metabolic rate for glucose. Both techniques
have been extensively applied to AD studies. For example, studies based on MRI have
consistently revealed brain atrophy that involves the hippocampus and entorhinal cortex [2-6];
studies based on PET have revealed functional abnormality that involves the posterior temporal
and parietal association cortices [8-10], posterior cingulate, precuneus, and medial temporal
cortices [11-14].
There is overlap between the disease-related brain regions detected by MRI and those by PET,
such as regions in the hippocampus area and the mesia temporal lobe [15-17]. This is not
surprising since MRI and PET are two complementary measures for the same disease pathology,
i.e., it starts mainly in the hippocampus and entorhinal cortex, and subsequently spreads
throughout temporal and orbiogrontal cortext, poseterior cingulated, and association cortex [7].
However, most existing studies only exploited structural and functional alterations in separation,
which ignore the potential interaction between them. The fusion of MRI and PET imaging
modalities will increase the statistical power in identification of disease-related brain regions,
especially for early AD, at which stage the disease-related regions are most likely to be weakeffect regions that are difficult to be detected from MRI or PET alone. Once a good set of diseaserelated brain regions is identified, they can be further used to build an effective classifier (i.e., a
biomarker from the clinical perspective) to enable AD diagnose with high sensitivity and
specificity.
The idea of multi-modality data fusion in the research of neurodegenerative disorders has been
exploited before. For example, a number of models have been proposed to combine
electroencephalography (EEG) and functional MRI (fMRI), including parallel EEG-fMRI
independent component analysis [18]-[19], EEG-informed fMRI analysis [18] [20], and
variational Bayesian methods [18] [21]. The purpose of these studies is different from ours, i.e.,
they aim to combine EEG, which has high temporal resolution but low spatial resolution, and
fMRI, which has low temporal resolution but high spatial resolution, so as to obtain an accurate
picture for the whole brain with both high spatial and high temporal resolutions [18]-[21]. Also,
there have been some studies that include both MRI and PET data for classification [15], [22][25]. However, these studies do not make use of the fact that MRI and PET measure the same
underlying disease pathology from two complementary perspectives (i.e., structural and functional
perspectives), so that the analysis of one imaging modality can borrow strength from the other.
In this paper, we focus on the problem of identifying disease-related brain regions from multimodality data. This is actually a variable selection problem. Because MRI and PET data are highdimensional, regularization techniques are needed for effective variable selection, such as the L1regularization technique [25]-[30] and the L2/L1-regularization technique [31]. In particular,
L2/L1-regularization has been used for variable selection jointly on multiple related datasets, also
known as multitask feature selection [31], which has a similar nature to our problem. Note that
both L1- and L2/L1-regularizations are convex regularizations, which have gained them popularity
in the literature. On the other hand, there is increasing evidence that these convex regularizations
tend to produce too severely shrunken parameter estimates. Therefore, these convex
regularizations could lead to miss-identification of the weak-effect disease-related brain regions,
which unfortunately make up a large portion of the disease-related brain regions especially in early
AD. Also, convex regularizations tend to select many irrelevant variables to compensate for the
overly severe shrinkage in the parameters of the relevant variables. Considering these limitations
of convex regularizations, we study non-convex regularizations [33]-[35] [39], which have the
advantage of producing mildly or slightly shrunken parameter estimates so as to be able to
preserve weak-effect disease-related brain regions and the advantage of avoiding selecting many
disease-irrelevant regions.
Specifically in this paper, we propose a sparse composite linear discriminant analysis model,
called SCLDA, for identification of disease-related brain regions from multi-modality data. The
contributions of our paper include:

2

•

•

•

2

Formulation: We propose a novel formulation that decomposes each LDA parameter into a
product of a common parameter shared by all the data sources and a parameter specific to
each data source, which enables joint analysis of all the data sources and borrowing strength
from one another. We further prove that this formulation is equivalent to a penalized
likelihood with non-convex regularization.
Algorithm: We show that the proposed non-convex optimization can be solved by the DC
(difference of convex functions) programming [39]. More importantly, we show that in using
the DC programming, the property of the non-convex regularization in terms of preserving
weak-effect features can be nicely revealed.
Application: We apply the proposed SCLDA to the PET and MRI data of early AD patients
and normal controls (NC). Our study identifies disease-related brain regions that are
consistent with the findings in the AD literature. AD vs. NC classification based on these
identified regions achieves high accuracy, which makes the proposed method a useful tool for
clinical diagnosis of early AD. In contrast, the convex-regularization based multitask feature
selection method [31] identifies more irrelevant brain regions and yields a lower classification
accuracy.

Rev iew o f L D A a nd it s v ari an ts
!

Denote 𝒁 = 𝑍!, 𝑍! , … , 𝑍! as the variables and assume there are 𝐽 classes. Denote 𝑁! as the
!
sample size of class 𝑗 and 𝑁 = !!! 𝑁! is the total sample size. Let 𝐳 = 𝒛! , 𝒛! , … , 𝒛! ! be the
𝑁×𝑝 sample matrix, where 𝒛! is the 𝑖 !! sample and 𝑔 𝑖 is its associated class index. Let
!
! !
!
𝛍! =
!!! 𝒛! be the overall sample mean,
!!!,! ! !! 𝒛! be the sample mean of class 𝑗, 𝛍 =
!!
! !
𝒛 −𝛍
! !!! !
!
!
𝐖! =
!! !!!,! ! !!
! !
𝑁 𝐖 be the
! !!! ! !

𝐓=

!

𝒛! − 𝛍

!

be the total normalized sum of squares and products (SSQP),

𝒛! − 𝛍! 𝒛! − 𝛍!

!

be the normalized class SSQP of class 𝑗, and 𝐖 =

overall normalized class SSQP.

The objective of LDA is to seek for a 𝑝×𝑞 linear transformation matrix, 𝛉! , with which 𝛉!! 𝑍
retains the maximum amount of class discrimination information in 𝑍. To achieve this objective,
one approach is to seek for the 𝛉! that maximizes the between-class variance of 𝛉!! 𝑍, which can
be measured by tr(𝛉!! 𝐓𝛉! ), while minimizing the within-class variance of 𝛉!! 𝑍, which can be
measured by tr(𝛉!! 𝐖𝛉! ). Here tr() is the matrix trace operator. This is equivalent to solving the
following optimization problem:
 𝛉! = argmax𝛉!

𝐭𝐫(𝛉!
! 𝐓𝛉! )
𝐭𝐫(𝛉!
! 𝐖𝛉! )

.

(1)

Note that 𝛉! corresponds to the right eigenvector of 𝐖 !! 𝐓 and 𝑞 = 𝐽 − 1.
Another approach used for finding the 𝛉! is to use the maximum likelihood estimation for
Gaussian populations that have different means and a common covariance matrix. Specifically, as
in [36], this approach is developed by assuming the class distributions are Gaussian with a
common covariance matrix, and their mean differences lie in a 𝑞-dimensional subspace of the 𝑝dimensional original variable space. Hastie [37] further generalized this approach by assuming
that class distributions are a mixture of Gaussians, which has more flexibility than LDA. However,
both approaches assume a common covariance matrix for all the classes, which is too strict in
many practical applications, especially in high-dimensional problems where the covariance
matrices of different classes tend to be different. Consequently, the linear transformation explored
by LDA may not be effective.
In [38], a heterogeneous LDA (HLDA) is developed to relax this assumption. The HLDA seeks
for a 𝑝×𝑝 linear transformation matrix, 𝛉, in which only the first 𝑞 columns (𝛉! ) contain
discrimination information and the remaining 𝑝 − 𝑞 columns (𝛉!!! ) contain no discrimination
information. For Gaussian models, assuming lack of discrimination information is equivalent to
assuming that the means and the covariance matrices of the class distributions are the same for all

3

classes, in the 𝑝 − 𝑞 dimensional subspace. Following this, the log-likelihood function of 𝛉 can be
written as below [38]:
!

𝑙 𝛉|𝐙 = − log 𝛉!!!! 𝐓𝛉!!! −
!

!!
!
!!! ! log

𝛉!! 𝐖! 𝛉! + 𝑁 log 𝛉 ,

(2)

Here 𝐀 denotes the determinant of matrix 𝐀. There is no closed-form solution for 𝛉. As a result,
numeric methods are needed to derive the maximum likelihood estimate for 𝛉. It is worth
mentioning that the LDA in the form of (1) is a special case of the HLDA [38].

3

T he p ro po sed SC L DA

Suppose that there are multiple data sources, 𝐙 ! , 𝐙 ! , … , 𝐙 ! , with each data source capturing
one aspect of the same set of physical variables, e.g., the MRI and PET capture the structural and
functional aspects of the same brain regions. For each data source, 𝐙 ! , there is a linear
transformation matrix 𝛉 ! , which retains the maximum amount of class discrimination
information in 𝐙 ! . A naive way for estimating 𝚯 = 𝛉 ! , 𝛉 ! , … , 𝛉 ! is to separately estimate
each 𝛉 ! based on 𝐙 ! . Apparently, this approach does not take advantage of the fact that all the
data sources measure the same physical process. Also, when the sample size of each data source is
small, this approach may lead to unreliable estimates for the 𝛉 ! ’s.
To tackle these problems, we propose a composite parameterization following the line as [40].
!
Specifically, let 𝜃!,! be the element at the k-th row and l-th column of 𝛉 ! . We treat
!

!

!

!

!

!

𝜃!,! , 𝜃!,! , … , 𝜃!,! as an interrelated group and parameterize each 𝜃!,! as 𝜃!,! = 𝛿! 𝛾!,! , for
1 ≤ 𝑘 ≤ 𝑝, 1 ≤ 𝑙 ≤ 𝑝 and 1 ≤ 𝑚 ≤ 𝑀. In order to assure identifiability, we restrict each 𝛿! ≥ 0.
Here, 𝛿! represents the common information shared by all the data sources about variable 𝑘, while
!
𝛾!,! represents the specific information only captured by the 𝑚 !! data source. For example, for
disease-related brain region identification, if 𝛿! = 0, it means that all the data sources indicate
variable 𝑘 is not a disease-related brain region; otherwise, variable 𝑘 is a disease-related brain
!
region. 𝛾!,! ≠ 0 means that the 𝑚 !! data source supports this assertion.
The log-likelihood function of 𝚯 is:
𝑙!

𝚯| 𝐙 ! , 𝐙 ! , … , 𝐙 !

=

!
!!!

−

!

!

!

𝑁

! !

log 𝛉!!! 𝐓
!

log 𝛉

!

!

!

!

𝛉!!! −

!!
!
!!! !

!

log 𝛉! 𝐖!

!

!

𝛉!

+

 ,

which follows the same line as (2). However, our formulation includes the following constraints
on 𝚯:
!

!

𝜃!,! = 𝛿! 𝛾!,! , 𝛿! ≥ 0, 1 ≤ 𝑘, 𝑙 ≤ 𝑝, 1 ≤ 𝑚 ≤ 𝑀.
!

Let 𝚪 = 𝛾!,! , 1 ≤ 𝑘 ≤ 𝑝, 1 ≤ 𝑙 ≤ 𝑝, 1 ≤ 𝑚 ≤ 𝑀

(3)

and 𝚿 = 𝛿! , 1 ≤ 𝑘 ≤ 𝑝 . An intuitive

choice for estimation of 𝚪 and 𝚿 is to maximize the 𝑙! 𝚯| 𝐙 ! , 𝐙 ! , … , 𝐙 !   subject to the
constraints in (3). However, it can be anticipated that no element in the estimated 𝚪 and 𝚿 will be
exactly zero, resulting in a model which is not interpretable, i.e., poor identification of diseaserelated regions. Thus, we encourage the estimation of 𝚿 and the first 𝑞 columns of 𝚪 (i.e., the
columns containing discrimination information) to be sparse, by imposing the L1-penalty on 𝚪 and
𝚿. By doing so, we obtain the following optimization problem for the proposed SCLDA:
𝚯 = argmin𝚯 𝑙!

𝚯| 𝐙 ! , 𝐙 ! , … , 𝐙 !

= argmin𝚯 −𝑙!

𝚯| 𝐙 ! , 𝐙 ! , … , 𝐙 !

!
𝜆! !,!,! 𝛾!,!   , subject to
!
!
𝜃!,! = 𝛿! 𝛾!,! , 𝛿! ≥ 0, 1 ≤ 𝑘, 𝑙 ≤

+   𝜆!

! 𝛿!

+

𝑝, 1 ≤ 𝑚 ≤ 𝑀.                                (4) 

Here, 𝜆! and 𝜆! control the degrees of sparsity of 𝚿 and 𝚪, respectively. Tuning of two
regularization parameters is difficult. Fortunately, we prove the following Theorem which
indicates that formulation (4) is equivalent to a simpler optimization problem involving only one
regularization parameter.

4

Theorem 1: The optimization problem (4) is equivalent to the following optimization problem:
𝚯 = argmin𝚯 𝑙! 𝚯| 𝐙
= argmin𝚯 −𝑙! 𝚯| 𝐙
!

!

,𝐙

!

!

,𝐙

,…,𝐙

!

!

,…,𝐙
+  𝜆

!

!

!
!!!

!
!!!

!

𝜃!,!

 ,

(5)

!

with 𝜆 = 2 𝜆! 𝜆! , i.e., 𝜃!,! = 𝜃!,! .
The proof can be found in the supplementary document. It can also be found in the supplementary
material how this formulation will serve the purpose of the composite parameterization, i.e.,
common information and specific information can be estimated separately and simultaneously.
The optimization problem (5) is a non-convex optimization problem that is difficult to solve. We
address this problem by using an iterative two-stage procedure known as Difference of Convex
functions (DC) programming [39]. A full description of the algorithm can be found in the
supplemental material.

4

S im ula tion s tu d ies

In this section, we conduct experiments to compare the performance of the proposed SCLDA with
sparse LDA (SLDA) [42] and multitask feature selection [31]. Specifically, as we focus on LDA,
we use the multitask feature selection method developed in [31] on LDA, denoted as MSLDA.
Both SLDA and MSLDA adopt convex regularizations. Specifically, SLDA selects features from
one single data source with L1-regularization; MSLDA selects features from multiple data sources
with L2/L1 regularization.
We evaluate the performances of these three methods across various parameters settings, including
the number of variables, 𝑝, the number of features, 𝑙, the number of data sources, M, sample size,
𝑛, and the degree of overlapping of the features across different data sources, s% (the larger the
𝑠%, the more shared features among the datasets). Definition of 𝑠% can be found in the simulation
procedure that is included in the supplemental material. For each specification of the parameters
settings, 𝑀 datasets can be generated following the simulation procedure. We apply the proposed
SCLDA to the 𝑀 datasets, and identify one feature vector 𝛉(!) for each dataset, with 𝜆 and 𝑞
chosen by the method described in section 3.3. The result can be described by the number of true
positives (TPs) as well as the number of false positives (FPs). Here, true positives are the non-zero
elements in the learned feature vector 𝛉(!) which are also non-zero in the 𝛃! ; false positives are the
non-zero elements in 𝛉(!) , which are actually zero in 𝛃! . As there are 𝑚 pairs of the TPs and FPs
for the 𝑀 datasets, the average TP over the M datasets and the average FP over the M datasets are
used as the performance measures. This procedure (i.e., from data simulation, to SCLDA, to TPs
and FPs generation) can be repeated for 𝐵 times, and 𝐵 pairs of average TP and average FP are
collected for SCLDA. In a similar way, we can obtain 𝐵 pairs of average TP and average FP for
both SLDA and MSLDA.
Figures 1 (a) and (b) show comparison between SCLDA, SLDA and MSLDA by scattering the
average TP against the average FP for each method. Each point corresponds to one of the N
repetitions. The comparison is across various parameters settings, including the number of
variables (𝑝 = 100,200,500), the number of data sources (𝑚 = 2,5,10), and the degree of
overlapping of the features across different data sources (𝑠% = 90%, 70%). Additionally, 𝑛 𝑝 is
kept constant, 𝑛 𝑝 = 1. A general observation is that SCLDA is better than SLDA and MSLDA
across all the parameter settings. Some specific trends can be summarized as follows: (i) Both
SCLDA and MSLDA outperform SLDA in terms of TPs; SCLDA further outperforms MSLDA in
terms of FPs. (ii) In Figure 2 (a), rows correspond to different numbers of data sources, i.e.,
𝑚 = 2,5,10 respectively. It is clear that the advantage of SCLDA over both SLDA and MSLDA is
more significant when there are more data sources. Also, MSLDA performs consistently better
than SLDA. Similar phenomena are shown in Figure 2 (b). This demonstrates that in analyzing
each data source, both SCLDA and MSLDA are able to make use of the information contained in
other data sources. SCLDA can use this information more efficiently, as SCLDA can produce less
shrunken parameter estimates than MSLDA and thus it is able to preserve weak-effect features.
(iii) Comparing Figures 2 (a) and (b), it can be seen that the advantage of SCLDA or MSLDA
over SLDA is more significant as the data sources have more degree of overlapping in their

5

features. Finally, although not presented here, our simulation shows that the three methods
perform similarly when 𝑠% = 40 or less.

(a)

(b)

Figure 1: Average numbers of TPs vs FPs for SCLDA (green symbols “+”), SLDA (blue symbols
“*”) and MSLDA (red symbols “o”) (a) 𝑠% = 90%, 𝑛 𝑝 = 1; (b) 𝑠% = 70%, 𝑛 𝑝 = 1

5

C ase st ud y

5.1

Data preprocessing

Our study includes 49 AD patient and 67 age-matched normal controls (NC), with each subject of
AD or NC being scanned both by PET and MRI. The PET and MRI images can be downloaded
from the database by the Alzheimer’s Disease Neuroimaging Initiative. In what follows, we
outline the data preprocessing steps.
Each image is spatially normalized to the Montreal Neurological Institute (MNI) template, using
the affine transformation and subsequent non-linear wraping algorithm [43] implemented in the
SPM MATLAB toolbox. This is to ensure that each voxel is located in the same anatomical region
for all subjects, so that spatial locations can be reported and interpreted in a consistent manner.
Once all the images in the MNI template, we further apply the Automated Anatomical Labeling
(AAL) technique [43] to segment the whole brain of each subject into 116 brain regions. The 90
regions that belong to the cerebral cortex are selected for the later analysis, as the other regions are
not included in the cerebral cortex are rarely considered related with AD in the literature. The
measurement of each region in the PET data is regional cerebral blood flow (rCBF); the
measurement of each region in the MRI data is the structural volume of the region.
5.2

Disease-related brain regions

SCLDA is applied to the preprocessed PET and MRI data of AD and NC with the penalty
parameter selected by the AIC method mentioned in section 3. 26 disease-related brain regions are
identified from PET and 21 from MRI (see Table 1 for their names). The maps of the diseaserelated brain regions identified from PET and MRI are highlighted in Figure 2 (a) and (b),
respectively, with different colors given to neighboring regions in order to distinguish them. Each
figure is a set of horizontal cut away slices of the brain as seen from the top, which aims to
provide a full view of locations of the regions.
One major observation is that the identified disease-related brain regions from MRI are in the
hippocampus, parahippocampus, temporal lobe, frontal lobe, and precuneus, which is consistent
with the existing literature that reports structural atrophy in these brain areas. [3-6,12-14]. The
identified disease-related brain regions from PET are in the temporal, frontal and parietal lobes,
which is consistent with many functional neuroimaging studies that report reduced rCBF or

6

reduced cortical glucose metabolism in these areas [8-10, 12-14]. Many of these identified
disease-related regions can be explained in terms of the AD pathology. For example, hippocampus
is a region affected by AD the earliest and severely [6] Also, as regions in the temporal lobe are
essential for memory, damage on these regions by AD can explain the memory loss which is a
major clinic symptom of AD. The consistency of our findings with the AD literature supports
effectiveness of the proposed SCLDA.
Another finding is that there is a large overlap between the identified disease-related regions from
PET and those from MRI, which implies strong interaction between functional and structural
alterations in these regions. Although well-accepted biological mechanisms underlying this
interaction are still not very clear, there are several explanations existing in the literature. The first
explanation is that both functional and structural alterations could be the consequence of dendritic
arborizations, which results from intracellular accumulation of PHFtau and further leads to neuron
death and grey matter loss [14]. The second explanation is that the AD pathology may include a
vascular component, which may result in reduced rCBF due to limited blood supply and may
ultimately result in structural alteration such as brain atrophy [45].

(a)

(b)

Figure 2: locations of disease-related brain regions identified from (a) MRI; (b) PET
5.3

Classification accuracy

As one of our primary goals is to distinguish AD from NC, the identified disease-related brain
regions through SCLDA are further utilized for establishing a classification model. Specifically,
for each subject, the rCBF values of the 26 disease-related brain regions identified from PET and
the structural volumes of the 21 disease-related brain regions identified from MRI are used, as a
joint spatial pattern of both brain physiology and structure. As a result, each subject is associated
with a vector with 47 features/variables. Linear SVM (Support Vector Machine) is employed as
the classifier. The classification accuracy based on 10-fold cross-validation is 94.3%. For
comparison purposes, MSLDA is also applied, which identifies 45 and 38 disease-related brain
regions for PET and MRI, respectively. Linear SVM applied to the 45+38 features gives a
classification accuracy of only 85.8%. Note that MSLDA identifies a much larger number of
disease-related brain regions than SCLDA, but some of the identified regions by MSLDA may
indeed be disease-irrelevant, so including them deteriorates the classification.
5.4
Relationship between structural atrophy and abnormal rCBF, and
severity of cognitive impairment in AD
In addition to classification, it is also of interest to further verify relevance of the identified
disease-related regions with AD in an alternative way. One approach is to investigate the degree to
which those disease-related regions are relevant to cognitive impairment that can be measured by
the Alzheimer’s disease assessment scale – cognitive subscale (ADAS-cog). ADAS measures
severity of the most important symptoms of AD, while its subscale, ADAS-cog, is the most

7

popular cognitive testing instrument used in clinic trails. The ADAS-cog consists of 11 items
measuring disturbances of memory, language, praxis, attention and other cognitive abilities that
are often affected by AD. As the total score of these 11 items provides an overall assessment of
cognitive impairment, we regress this ADAS-cog total score (the response) against the rCBF or
structure volume measurement (the predictor) of each identified brain region, using a simple
regression. The regression results are listed in Table 1.
It is not surprising to find that some regions in the hippocampus area and temporal lobes are
among the best predictors, as these regions are extensively reported in the literature as the most
severely affected by AD [3-6]. Also, it is found that most of these brain regions are weak-effect
predictors, as most of them can only explain a small portion of the variability in the ADAS-cog
total score, i.e., many R-square values in Table 1 are less than 10%. However, although the effects
are weak, most of them are significant, i.e., most of the p-values in Table 1 are smaller than 0.05.
Furthermore, it is worth noting that 70.22% variability in ADAS-cog can be explained by taking
all the 26 brain regions identified from PET as predictors in a multiple regression model; 49.72%
variability can be explained by taking all the 21 brain regions from MRI as predictors in a multiple
regression model. All this findings imply that the disease-related brain regions are indeed weakeffect features if considered individually, but jointly they can play a strong role for characterizing
AD. This verifies the suitability of the proposed SCLDA for AD studies, as SCLDA can preserve
weak-effect features.
Table 1: Explanatory power of regional rCBF and structural volume for variability in ADAS-cog
(“~” means this region is not identified from PET (or MRI) as a disease-related region by SCLDA)
PET
Brain regions
Precentral_L
Precentral_R
Frontal_Sup_L
Frontal_Sup_R
Frontal_Mid_R
Frontal_M_O_L
Frontal_M_O_R
Insula_L
Insula_R
Cingulum_A_R
Cingulum_Mid_L
Cingulum_Post_L
Hippocampus_L
Hippocampus_R
ParaHippocamp_L

6

R

2

0.003
0.044
0.051
0.044
0.056
0.036
0.019
0.016
~
0.004
0.001
0.184
0.158
~
0.206

pvalue
0.503
0.022
0.013
0.023
0.010
0.040
0.138
0.171
~
0.497
0.733
<10-4
<10-4
~
<10-4

MRI
R

2

0.027
~
0.047
~
0.072
0.086
0.126
0.163
0.125
0.082
0.040
~
~
0.242
~

PET
Brain regions

pvalue
0.077
~
0.018
~
0.003
0.001
0.000
<10-4
0.000
0.001
0.030
~
~
<10-4
~

Amygdala_L
Calcarine_L
Lingual_L
Postcentral_L
Parietal_Sup_R
Angular_R
Precuneus_R
Paracentr_Lobu_L
Pallidum_L
Pallidum_R
Heschl_L
Heschl_R
Temporal_P_S_R
Temporal_Inf_R
All regions

R

2

0.090
0.038
0.066
0.038
0.001
0.173
0.063
0.035
0.082
~
0.001
0.000
0.008
0.187
0.702

pvalue
0.001
0.034
0.005
0.035
0.677
<10-4
0.006
0.043
0.001
~
0.640
0.744
0.336
<10-4
<10-4

MRI
R

2

0.313
0.028
0.044
0.026
~
0.063
0.025
0.000
~
0.020
~
0.111
0.071
0.147
0.497

pvalue
<10-4
0.070
0.023
0.081
~
0.006
0.084
0.769
~
0.122
~
0.000
0.003
<10-4
<10-4

C on clu sio n

In the paper, we proposed a SCLDA model for identification of disease-related brain regions of
AD from multi-modality data, which is capable to preserve weak-effect disease-related brain
regions due to its less shrinkage imposed on its parameters. We applied SCLDA to the PET and
MRI data of early AD patients and normal controls. As MRI and PET measure two
complementary aspects (structural and functional aspects, respectively) of the same AD pathology,
fusion of these two image modalities can make effective use of their interaction and thus improve
the statistical power in identification of disease-related brain regions. Our findings were consistent
with the literature and also showed some new aspects that may suggest further investigation in
neuroimaging research in the future.

8

References
[1] deToledo-Morrell, L., Stoub, T.R., Bulgakova, M. 2004. MRI-derived entorhinal volume is a good predictor of
conversion from MCI to AD. Neurobiol. Aging 25, 1197–1203.

[2] Morra, J.H., Tu, Z. Validation of automated hippocampal segmentation method. NeuroImage 43, 59–68, 2008.
[3] Morra, J.H., Tu, Z. 2009a. Automated 3D mapping of hippocampal atrophy. Hum. Brain Map. 30, 2766–2788.
[4] Morra, J.H., Tu, Z. 2009b. Automated mapping of hippocampal atrophy in 1-year repeat MRI data. NeuroImage 45,
213-221.

[5] Schroeter, M.L., Stein, T. 2009. Neural correlates of AD and MCI. NeuroImage 47, 1196–1206.
[6] Braak, H., Braak, E. 1991. Neuropathological stageing of Alzheimer-related changes. Acta Neuro. 82, 239–259.
[7] Bradley, K.M., O'Sullivan. 2002. Cerebral perfusion SPET correlated with Braak pathological stage in AD. Brain
125, 1772–1781.

[8] Keilp, J.G., Alexander, G.E. 1996. Inferior parietal perfusion, lateralization, and neuropsychological dysfunction in
AD. Brain Cogn. 32, 365–383.

[9] Schroeter, M.L., Stein, T. 2009. Neural correlates of AD and MCI. NeuroImage 47, 1196–1206.
[10] Asllani, I., Habeck, C. 2008. Multivariate and univariate analysis of continuous arterial spin labeling perfusion MRI
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]
[26]
[27]
[28]
[29]
[30]
[31]
[32]
[33]
[34]
[35]
[36]
[37]
[38]
[39]
[40]
[41]
[42]
[43]
[44]
[45]

in AD. J. Cereb. Blood Flow Metab. 28, 725–736.
Du,A.T., Jahng, G.H. 2006. Hypoperfusion in frontotemporal dementia and AD. Neurology 67, 1215–1220.
Ishii, K., Kitagaki, H. 1996. Decreased medial temporal oxygen metabolism in AD. J. Nucl. Med. 37, 1159–1165.
Johnson, N.A., Jahng, G.H. 2005. Pattern of cerebral hypoperfusion in AD. Radiology 234, 851–859.
Wolf, H., Jelic, V. 2003. A critical discussion of the role of neuroimaging in MCI. Acta Neuroal: 107 (4), 52-76.
Tosun, D., Mojabi, P. 2010. Joint analysis of structural and perfusion MRI for cognitive assessment and classification
of AD and normal aging. NeuroImage 52, 186-197.
Alsop, D., Casement, M. 2008. Hippocampal hyperperfusion in Alzheimer's disease. NeuroImage 42, 1267–1274.
Mosconi, L., Tsui, W.-H. 2005. Reduced hippocampal metabolism in MCI and AD. Neurology 64, 1860–1867.
Mulert, C., Lemieux, L. 2010. EEG-fMRI: physiological basis, technique and applications. Springer.
Xu, L., Qiu, C., Xu, P. and Yao, D. 2010. A parallel framework for simultaneous EEG/fMRI analysis: methodology
and simulation. NeuroImage, 52(3), 1123-1134.
Philiastides, M. and Sajda, P. 2007. EEG-informed fMRI reveals spatiotemporal characteristics of perceptual decision
making. Journal of Neuroscience, 27(48), 13082-13091.
Daunizeau, J., Grova, C. 2007. Symmetrical event-related EEG/fMRI information fusion. NeuroImage 36, 69-87.
Jagust, W. 2006. PET and MRI in the diagnosis and prediction of dementia. Alzheimer’s Dement 2, 36-42.
Kawachi, T., Ishii, K. and Sakamoto, S. 2006. Comparison of the diagnostic performance of FDG-PET and VBM.
Eur.J.Nucl.Med.Mol.Imaging 33, 801-809.
Matsunari, I., Samuraki, M. 2007. Comparison of 18F-FDG PET and optimized voxel-based morphometry for
detection of AD. J.Nucl.Med 48, 1961-1970.
Schmidt, M., Fung, G. and Rosales, R. 2007. Fast optimization methods for L1-regularization: a comparative study
and 2 new approaches. ECML 2007.
Liu, J., Ji, S. and Ye, J. 2009. SLEP: sparse learning with efficient projections, Arizona state university.
Tibshirani, R. 1996. Regression Shrinkage and Selection via the Lasso, JRSS, Series B, 58(1):267–288.
Friedman, J., Hastie, T. and Tibshirani, R. 2007. Sparse inverse covariance estimation with the graphical lasso.
Biostatistics, 8(1):1–10.
Zou, H., Hastie, T. and Tibshirani, R. 2006. Sparse PCA, J. of Comp. and Graphical Statistics, 15(2), 262-286.
Qiao, Z., Zhou, L and Huang, J. 2006. Sparse LDA with applications to high dimensional low sample size data.
IAENG applied mathematics, 39(1).
Argyriou, A., Evgeniou, T. and Pontil, M. 2008. Convex multi-task feature learning. Machine Learning 73(3): 243–
272.
Huang, S., Li, J., et al. 2010. Learning Brain Connectivity of AD by Sparse Inverse Covariance Estimation,
NeuroImage, 50, 935-949.
Candes, E., Wakin, M. and Boyd, S. 2008. Enhancing sparsity by reweighted L1 minimization. Journal of Fourier
analysis and applications, 14(5), 877-905.
Mazumder, R.; Friedman, J. 2009. SparseNet: Coordinate Descent with Non-Convex Penalties. Manuscript.
Zhang, T. 2008. Multi-stage Convex Relaxation for Learning with Sparse Regularization. NIPS 2008.
Campbell, N. 1984. Canonical variate analysis ageneral formulation. Australian Jour of Stat 26, 86–96.
Hastie, T. and Tibshirani, R. 1994. Discriminant analysis by gaussian mixtures. Technical report. AT&T Bell Lab.
Kumar, N. and Andreou, G. 1998. Heteroscedastic discriminant analysis and reduced rank HMMs for improved
speech recognition. Speech Communication, 26 (4), 283-297.
Gasso, G., Rakotomamonjy, A. and Canu, S. 2009. Recovering sparse signals with non-convex penalties and DC
programming. IEEE Trans. Signal Processing 57( 12), 4686-4698.
Guo, J., Levina, E., Michailidis, G. and Zhu, J. 2011. Joint estimation of multiple graphical models. Biometrika 98(1)
1-15.
Bertsekas, D. 1982. Projected newton methods for optimization problems with simple constraints. SIAM J. Control
Optim 20, 221-246.
Clemmensen, L., Hastie, T., Witten, D. and Ersboll:, B. 2011. Sparse Discriminant Analysis. Technometrics (in press)
Friston, K.J., Ashburner, J. 1995. Spatial registration and normalization of images. HBM 2, 89–165.
Tzourio-Mazoyer, N., et al., 2002. Automated anatomical labelling of activations in SPM. NeuroImage 15, 273–289.
Bidzan, L. 2005. Vascular factors in dementia. Psychiatr. Pol. 39, 977-986.

9

Expert Systems With Applications 46 (2016) 33–44

Contents lists available at ScienceDirect

Expert Systems With Applications
journal homepage: www.elsevier.com/locate/eswa

A recommendation system for meta-modeling: A meta-learning based
approach
Can Cui a, Mengqi Hu b, Jeffery D. Weir c, Teresa Wu a,∗
a

School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, 699 S. Mill Ave., Tempe, AZ 85281, USA
Department of Mechanical and Industrial Engineering, University of Illinois at Chicago, 842 W. Taylor St., Chicago, IL, 60607
c
Department of Operational Sciences, Air Force Institute of Technology, 2950 Hobson Way, Wright-Patterson Afb, Ohio 45433, USA
b

a r t i c l e

i n f o

Keywords:
Meta-learning
Meta-model
Simulation
Recommendation system
Algorithm selection
Feature reduction

a b s t r a c t
Various meta-modeling techniques have been developed to replace computationally expensive simulation
models. The performance of these meta-modeling techniques on different models is varied which makes existing model selection/recommendation approaches (e.g., trial-and-error, ensemble) problematic. To address
these research gaps, we propose a general meta-modeling recommendation system using meta-learning
which can automate the meta-modeling recommendation process by intelligently adapting the learning bias
to problem characterizations. The proposed intelligent recommendation system includes four modules: (1)
problem module, (2) meta-feature module which includes a comprehensive set of meta-features to characterize the geometrical properties of problems, (3) meta-learner module which compares the performance of
instance-based and model-based learning approaches for optimal framework design, and (4) performance
evaluation module which introduces two criteria, Spearman’s ranking correlation coeﬃcient and hit ratio, to
evaluate the system on the accuracy of model ranking prediction and the precision of the best model recommendation, respectively. To further improve the performance of meta-learning for meta-modeling recommendation, different types of feature reduction techniques, including singular value decomposition, stepwise
regression and ReliefF, are studied. Experiments show that our proposed framework is able to achieve 94%
correlation on model rankings, and a 91% hit ratio on best model recommendation. Moreover, the computational cost of meta-modeling recommendation is signiﬁcantly reduced from an order of minutes to seconds
compared to traditional trial-and-error and ensemble process. The proposed framework can signiﬁcantly advance the research in meta-modeling recommendation, and can be applied for data-driven system modeling.
© 2015 Elsevier Ltd. All rights reserved.

1. Introduction
The growing complexity of real-world systems drives research
to develop simulation models to imitate the underlying functionality of the actual system (Banks, Carson, Nelson, & Nicol, 2004). In
general, the models can be categorized into three groups: physicsbased modeling, data-driven modeling and a hybrid of the two.
Physics-based models simulate the behavior of a real system based
on the fundamental physics of each component and the interactions
of the components, thus it can provide a high-ﬁdelity description
of the systems. However, the development of such models requires
domain expertise for setting up and implementation. In addition,
it suffers from high computational cost. A hybrid model is built
upon the physics-based model using statistical tools to estimate the
model parameters (Kristensen, Madsen, & Jørgensen, 2004). It again,
∗

Corresponding author. Tel.: +1 480 965-4157.
E-mail addresses: ccan1@asu.edu (C. Cui), mhu@uic.edu (M. Hu),
Jeffery.Weir@aﬁt.edu (J.D. Weir), Teresa.Wu@asu.edu (T. Wu).
http://dx.doi.org/10.1016/j.eswa.2015.10.021
0957-4174/© 2015 Elsevier Ltd. All rights reserved.

requires partial knowledge of the underlying system as a prior, which
may not be easily obtained. Recently, the data-driven modeling approach has emerged as an alternative to model the system purely
from the data available. A data-driven model, also known as a metamodel or surrogate model, is a “model of the model” (Kleijnen, 1995).
It is constructed using data which can provide fast approximations of
the objects and has been used for design optimization, design space
exploration, sensitivity analysis, what-if analysis and real-time engineering decisions.
Extensive research has explored a number of meta-models, e.g.,
Kriging (Matheron, 1960), support vector regression (SVR) (Clarke,
Griebsch, & Simpson, 2005; Drucker, Chris, Kaufman, Smola, &
Vapnik, 1997), radial basis function (RBF) (Dyn, Levin, & Rippa, 1986),
multivariate adaptive regression splines (MARS) (Friedman, 1991),
artiﬁcial neural network (ANN) (McCulloch & Pitts, 1943) and polynomial regression (PR) (Gergonne, 1974), just to name a few. A comprehensive review of the meta-modeling applications in computerbased engineering design and optimization can be found in (Simpson,
Peplinski, Koch, & Allen, 1997; Wang & Shan, 2007). As expected,

34

C. Cui et al. / Expert Systems With Applications 46 (2016) 33–44

the general conclusion from these studies is that the performances
of the meta-models vary depending on the problems investigated.
This is also conﬁrmed by (Clarke et al., 2005) and (Cui, Wu, Hu,
Weir, & Chu, 2014). Therefore, researchers have taken a trial-anderror approach, that is, investigating a number of different metamodels among which the best performer (evaluated against metrics,
e.g., accuracy) is selected. It is not until recently that research started
to explore the use of an ensemble, an optimal combination of several
models. The distinct challenge these approaches (trail-and-error and
ensemble) face is the expensive computational costs. Taking a largescale meta-model based design optimization problem as an example,
where thousands or even millions of ﬁtness evaluations are triggered
in support of the optimization process, building several meta-models
or an ensemble might be computationally unaffordable.
In this research, we propose a meta-model recommendation system using a meta-learning technique to identify the appropriate
meta-models for engineering simulation problems which are known
to be computationally expensive. Please note meta-learning is not
new, it has been studied in machine learning ﬁelds, e.g., gene expression classiﬁcation (Souza, Carvalho, & Soares, 2008), failure prediction (Lan, Gu, Zheng, Thakur, & Coghlan, 2010), gold market
forecasting (Zhou, Lai, & Yen, 2012) and recommendation of classiﬁcation algorithms on educational datasets (Romero, Olmo, & Ventura, 2013). The idea of meta-learning is that the information gained
from learned instances shall be valuable to study future instances.
To the best of our knowledge, most existing meta-learning systems
handle the learning process on instances with a large volume of data
records provided. As a result, the overall underlying structure of the
instances can be well captured by the features extracted from the
dataset. In this research, we are motivated to develop meta-model
recommendation expert system for simulation purpose. Therefore,
several unique challenges arise:
 How to intelligently select sample data for meta-modeling?
 In identifying the exemplar meta-model for a speciﬁc new
problem, researchers have proposed instance-based (e.g., knearest-neighbors) vs. model-based (e.g., artiﬁcial neural network) meta-learning algorithms. To develop a meta-learning
based meta-modeling recommendation for simulation, which
approach is appropriate?
 Given the dataset, existing research tends to collect as many
meta-features as possible which may lead to a large yet redundant set of meta-features. Which feature reduction technique is appropriate to reduce the dimensionality of the
meta-features?
To answer these questions, our proposed recommendation system is designed with four modules: the problem space with an intelligent sampling module, a meta-feature space module, an algorithm space module, and a performance space module. The problem
space module is the repository of the problems being studied; intelligent sampling is launched to identify the representative dataset.
The problem space is to be updated accordingly as new problems
emerge. From the derived dataset, the meta-level features describing
the characteristics of the problems/datasets are to be captured. Dimension reduction techniques, which include singular value decomposition (SVD) (Fallucchi, Zanzotto, & Rome, 2009; Simek et al., 2004),
stepwise regression (Draper & Smith, 1981; Efroymson, 1960; Hocking, 1976) and the ReliefF method (Kira & Rendell, 1992; Kononenko,
Šimec, & Robnik-Šikonja, 1997) may be applied to process the high dimensional meta-features. The algorithm space module consists of the
meta-models to be chosen from and the performance space provides
the metric(s) on which the meta-model is evaluated (multiple metrics may apply depending on the problem scope). To test the applicability of the proposed recommendation system: (1) 44 benchmark
functions with distinct characteristics and properties, are collected
from IEEE CEC 2013&2014 (Liang & Qu, 2013a; Liang, Qu, & Suganthan,

2013b); (2) Latin hypercube sampling is applied for the generation
of a representative dataset for each problem; (3) 15 meta-features
(statistical and geometrical) are derived from the generated dataset,
and three feature reduction methods (SVD, stepwise regression, ReliefF) are then applied to reduce the dimensionality of the features, respectively; (4) Six meta-models are of interest including Kriging, SVR,
RBF, MARS, ANN and PR; (5) Two types of meta-learning algorithms
(instance-based and model-based) are applied and compared, for exploration on appropriate designs; (6) Normalized root mean square
error (NRMSE) is used as the accuracy measurement of each metamodel studied in the algorithm space module; (7) The performance
of the proposed meta-learning framework is ﬁrst assessed using the
Spearman’s ranking correlation coeﬃcient (Brazdil, Soares, Costa, &
Da, 2003; De Souto et al., 2008), a nonparametric measure of statistical dependence between derived rankings and ideal rankings. A second assessment metric, hit ratio, is introduced which is deﬁned as the
percentage of matches between the recommended best performer to
the true best performer. Experiments show that our proposed framework is able to achieve 94% correlation on rankings, and a 91% hit
ratio on best performer recommendation (40 out of 44 problems).
In summary, the contributions of the proposed recommendation
system are four-fold: (1) To the best our knowledge, this may be
the ﬁrst attempt to apply meta-learning on meta-modeling for automating the surrogate modeling process on computationally expensive simulation tasks. (2) The proposed generalized meta-model recommendation framework can signiﬁcantly reduce the computational
cost in the traditional trial-and-error or ensemble modeling process.
(3) A comprehensive set of meta-features is proposed to characterize
the properties of various black box problems. Different types of feature reduction techniques, including singular value decomposition,
stepwise regression and ReliefF are studied to improve the recommendation system performance. (4) The proposed recommendation
system is validated on a large number of benchmark cases, which is
shown to be able to signiﬁcantly improve the meta-modeling process, both on the eﬃciency of model construction and the quality of
the meta-model selection. The resulting intelligent expert system can
beneﬁt extensive research applications where automatic model selection is desired.
This paper is organized as follows: Section 2 reviews background
of meta-modeling and meta-learning; In Section 3, the proposed
methodology is elaborated; Section 4 describes the design of experiments and discusses results obtained; Finally, Section 5 draws the
conclusions and points out future research directions.
2. Background
This section gives a general review on meta-modeling and metalearning. “Meta”, meaning an abstraction from a concept is used to
complete or add to that concept. Meta-modeling refers to the modeling of a model, while meta-learning refers to the learning of the
learning process. As a matter of fact, both deal with meta-level learning, while in different domains.
2.1. Meta-modeling
The meta-modeling process involves model ﬁtting or function approximation to the sampled data of design variables and responses
from the detailed model (Ryberg, Bäckryd, & Nilsson, 2012). To
demonstrate the idea of our proposed framework, one parametric
technique (PR), and ﬁve non-parametric techniques (Kriging, SVR,
RBF, MARS and ANN) are chosen due to their extensive use in metamodeling. Each is reviewed in the following section. For parametric techniques, a chosen functional relationship between the design variables and the response is presumed. While non-parametric
techniques, also known as distribution free methods, rely less on a

C. Cui et al. / Expert Systems With Applications 46 (2016) 33–44

priori knowledge about the form of the true function but mainly on
the sample data for function construction.
2.1.1. Kriging
Kriging (also known as Gaussian process regression) is an interpolation method that assumes the simulation output may be modeled
by a Gaussian process. It gives the best linear unbiased prediction of
simulation output not yet observed. It generates the prediction in the
form of a combination of a global model with local random noise:

y(x) = f (x)β + Z (x),

(1)

where x is the input vector, β is the weight vector, and Z(x) is a
stochastic process with zero mean and stationary covariance of



 

COV Z (xi ), Z x j





= σ R xi , x j ,
2

(2)

σ2

where
is the process variance, R(xi , xj ) is an n by n correlation
matrix where n is the sample size of the training data. R is usually de2
picted by a Gaussian correlation function, exp(−θ (xi − x j ) ) with parameter θ . Kriging is one of the most intensively studied meta-models
because it is ﬂexible with a number of correlation functions and regression functions (with polynomial degree of 0, 1 or 2) to choose
from. It is generally acknowledged that the Kriging model outperforms others on nonlinear problems. However, it is also noted that
it is time consuming to implement Maximum Likelihood Estimation
of the correlation parameters in R, which is a multi-dimensional optimization problem (Jin, Chen, & Simpson, 2001).
2.1.2. Support vector regression
Support Vector Regression (SVR) is analogous to support vector
classiﬁcation, which attempts to maximize the distance between two
classes of data by selecting two hyperplanes to optimally separate the
training data. The mathematical form of SVR is:

f (x) = ω · x + b,

(3)

where ω is the norm vector to the hyperplane and b/ω determines
the offset of the hyperplane from the origin. The goal is to ﬁnd a hyperplane that separates the data points optimally without error and
separates the closest points with the hyperplane as far as possible.
Thus, it can be constructed as an optimization problem:

min1/2|ω|2



s.t.

yi − ω · xi  − b ≤ ε

ω · xi  + b − yi ≤ ε

(4)

f ( x) =

m


(αi∗ − αi )kxi · x j  + b,

(5)

i=1

where αi∗ and α i are two introduced dual variables, and kxi · xj  is
a kernel function, e.g. Gaussian kernel. It is noted that there exists
research demonstrating the outperformances of SVR (Wang & Shan,
2007), yet, most so far have been empirical studies.
2.1.3. Radial Basis Function
The Radial Basis Function (RBF) is used to develop interpolation
on scattered multivariate data. An RBF is a linear combination of a
real-valued radially symmetric function,∅(x), based on distance from
the origin,

f ( x) =

n


θi . ∅(x − xi ),

2.1.4. Multivariate Adaptive Regression Splines
Multivariate Adaptive Regression Splines (MARS) is a form of regression analysis introduced by Friedman (1991). A set of basis functions, deﬁned as constant, hinge function, or the product of two or
more hinge functions, are combined in the weighted sum form, as the
approximation of the response function. A MARS model is built with
generalized cross validation regularization in a forward/backward iterative process. The general model of MARS can be written as:

f (x) = γ0 +

m


γi hi (x),

(7)

i=1

where γ i is the constant coeﬃcient of the combination whose value
is jointly adjusted to give the best ﬁt to the data, and the basis function hi , can be represented as:

hi (x) =

Km





sk,m · xv(k,m) − tk,m

q

k=1

+

,

(8)

where Km is the number of splits given to the mth basis function, sk, m
= ± 1 indicates the right/left sense of the associated step function,
v(k, m) is the label of the variable, and tk, m represents values (knot
locations) of the corresponding variables. The superscript q and subscript + indicate the truncated power functions with polynomials of
lower order than q. According to (Jin et al., 2001), MARS procedure
appears to be accurate due to its distribution free assumption compared to other algorithms.
2.1.5. Artiﬁcial Neural Network
Artiﬁcial Neural Network (ANN) (Rosenblatt, 1958) is a computational model inspired by an animal’s central nervous system. It is apt
at solving problems with complicated structures. Due to its promising results in numerous ﬁelds, ANN has been extensively applied in
stochastic simulation meta-modeling (Fonseca, Navaresse, & Moynihan, 2003; Nasereddin & Mollaghasemi, 1999). An ANN model typically consists of three separate layers: the input layer, the hidden
layer(s), and the output layer. The neurons across different layers are
interconnected to transmit and deduce information. A typical ANN
with three layers and one single output neuron has the following
mathematical form:

f ( x) =

J

j=1

According to the duality principle, the nonlinear regression problem is given by:

35

	

ω jδ

I





wi j δ(xi ) + α j

+β +ε

(9)

i=1

where x is a k-dimensional vector, the input unit represents the raw
information that is fed into the network, δ ( · ) is the user deﬁned
transfer function, wij is the weight factor on the connection between
the ith input neuron and the jth hidden neuron, α j is the bias in the
jth hidden neuron, ωj is the weight on connection between the jth
hidden neuron and the output neuron, β is the bias of the output
neuron, ε is a random error with a mean of 0, and I and J are the
number of input neurons and hidden neurons. In supervised learning, the output unit is trained to simulate the underlying structure of
the input signals and response. The trained structure is depicted by
several parameters, the weights on each connection, the biases, the
number of hidden layers, the transfer functions, and the number of
hidden nodes in each hidden layer. It is worth mentioning that the
performance of ANN is highly dependent on parameter tuning, and
extensive research have been done on this regard (Bashiri & Farshbaf
Geranmayeh, 2011; Packianather, Drake, & Rowlands, 2000).

(6)

i=1

where θ i is the unknown interpolation coeﬃcient determined by
the least-squares method, n is the number of sampling points and
x − xi  is the Euclidean norm of the radial distance from design
point x to the sampling point xi . Fang, Rais-Rohani, Liu, and Horstemeyer (2005) found RBF performs well on highly nonlinear problems.

2.1.6. Polynomial regression
Polynomial Regression (PR) is a variation of linear regression
in which an nth order polynomial is modeled to formulate the
relationship between the independent variable x and the dependent variable y. PR models have been applied to various engineering domains such as mechanical, medical and industrial

36

C. Cui et al. / Expert Systems With Applications 46 (2016) 33–44

(Barker et al., 2001; Greenland, 1995; Shaw et al., 2006). A secondorder polynomial model can be expressed as:

f (x) = β0 +

k

i=1

βi xi +

k

i=1

βii xi 2 +


i

βi j xi x j + 


(10)

j

where β is the constant coeﬃcient, k is the number of variables, and

 is an unobserved random error with zero mean. PR models are usually ﬁt using the least squares method. One advantage of PR models
is the straightforward hierarchical structure, where the signiﬁcances
of different design variables are directly reﬂected by the magnitude
of the coeﬃcients in the model. This is especially useful when the design dimension is large, where only signiﬁcant factors are kept in the
model and thus reduce the possibility of over-ﬁtting. However, when
ﬁtting on highly nonlinear behaviors, PR may suffer from numerical
instabilities (Barton, 1992).
Wolpert (1996) showed that bias-free learning is futile. In fact,
researchers have claimed that a learning process without any prior
knowledge about the system’s nature may lead to random solutions.
As a result, existing research concluded the performance of metamodels is problem dependent, which conﬁrms the classical No Free
Lunch Theorem (NFL) (D.H. Wolpert & Macready, 1997), that is, no
algorithm can outperform any other algorithm when performance is
amortized over all functions. Therefore, traditional approaches take
a trial-and-error manner where a number of different meta-models
are separately built and the best one is ﬁnally chosen. A comparison
study on polynomial, Kriging, RBF, and MARS meta-models was conducted by Clarke et al. (2005), which concluded that SVR generally
outperforms others on accuracy and robustness. In a separate study
(Cui et al., 2014), in which Kriging, SVR and RBF were compared in
terms of accuracy and robustness, it was found that Kriging overall performs the best. The discrepancy on the conclusions between
the two studies shows that the meta-modeling performance not only
depends on the test problems, but also is compounded by the design of experiments and the model parameter settings. A Gaussian
process meta-model was used as the surrogate model for the timeconsuming ﬁnite-element model on a simple ﬂat steel plate and a
full-scale arch bridge in (Wan & Ren, 2015). The authors favored a
Gaussian process meta-model because of its probabilistic, nonparametric features and high capability of modeling a complex physical
system. However, Gaussian process is not the only one that bears
these merits, e.g., ANN is also nonparametric and is of powerful capability on complex system modeling. The selection of a single metamodel is very risky in the sense that researchers may end up with a
sub-optimal model solution given no justiﬁcation on other models’
inappropriateness. Therefore, traditional research has also explored
the application of ensemble (Acar, 2015), the combination of several
models, which takes advantage of each meta-model’s strength and
mitigate the weakness, thus result in stronger than any standalone
meta-model. A multi-objective design optimization using dynamic
ensemble metamodeling method was conducted to seek the optimal
designs of a proposed functionally graded foam-ﬁlled tapered tube in
(Yin et al., 2014). The authors claimed that the ensemble metamodeling method performs better than a single static meta-model. However, as the ensemble is built by four different meta-models, including Kriging, SVR, RBF, and PR, the computational cost is much higher
than building a single model, which was not addressed in this work.
In effect, for large-scale problems, e.g., meta-model based design optimization, in which thousands of ﬁtness evaluations are called in
support of the optimization process, building several meta-models
or ensemble for each evaluation might be impractical. To summarize,
two approaches are mainly involved with traditional meta-modeling
research: (1) subjectively select a single meta-model for the given
surrogate modeling tasks, regardless of applicability and adaptability;
(2) Ensemble on several meta-models, but at the expense of higher
computational cost. Therefore, there is a need of a meta-learning

approach to effectively associating the algorithm performance with
the problem.
2.2. Meta-learning
Meta-learning is a machine learning approach to explore the
learning process and understand the mechanism of the process,
which could be re-used for future learning. Compared to baselearning, which learns a speciﬁc task (e.g., credit rating, fraud detection, etc.) on the corresponding data, meta-learning is a learning
process that continuously gains knowledge as tasks being accomplished by the base-learners accumulate. The main goal is to build
a ﬂexible automatic learning machine that can solve different kinds
of learning problems by using meta-data such as, the learning algorithm properties, the characteristics of the learning problems, or
patterns previously derived from the relationship between learning
problems and the effectiveness of different learning algorithms, and
hence to improve the performance of the learning algorithms. For
a comprehensive review of meta-learning research and its applications, we refer the reader to (Giraud-Carrier, 2008; P Brazdil, Carrier,
Soares, & Vilalta, 2008; Vilalta & Drissi, 2002). Here we provide a general overview of a meta-learning framework followed by a review
of its application to regression algorithm selection/recommendation
which is of interest in this research.
2.2.1. Meta-learning – Rice’s Model
The early contribution related to computer programming on
meta-learning dates back to 1986, when STABB (Shift to A Better
Bias) is proposed by Utgoff (1986), as the ﬁrst system capable of dynamically adjusting a learner’s bias. Following Utgoff’s work, Rendell,
Seshu, and Tcheng (1987) propose a variable bias management system (VBMS), which selects an algorithm (out of three), based on two
meta-features: the number of training instances and the number of
features. The StatLog project (Brazdil, Gama, & Henery, 1994) further
extends VBMS by introducing a larger number of dataset characteristics, together with a broad class of candidate classiﬁcation models
and algorithms for selection.
The ﬁrst formal abstract model for algorithm recommendation
corresponds to Rice’s model (Rice, 1975). As shown in Fig. 1, Rice’s
model has four component spaces: (1) problem space P represents
the datasets of learning instances; (2) feature space F includes the
features or characteristics extracted from the datasets in P, as an abstract representation of the instances; (3) algorithm space A contains
all the candidate algorithms considered in the context; (4) performance space Y is the performance measurement of an algorithm instance in A on a problem instance in P. This framework is well accepted for component-based learning since it is easily extensible with
respect to any component, and is capable of strengthening learning
capability over time (Matijaš, Suykens, & Krajcar, 2013). Speciﬁcally,
given a problem x ∈ P, the features f(x) ∈ F are mapped to the algorithm a ∈ A by selection algorithm S(f(x)), so as to maximize the
performance y(a(x)) ∈ Y. A general procedure for meta-learning induction begins with a process of gaining experience: base-line learning. The instances x ∈ P are learned by all the candidate algorithms a
∈ A, evaluated by the performance measures in y ∈ Y. The features
f(x) ∈ F are called meta-features, which comprehensively depict the
characteristics of the instances x ∈ P. It later involves in the metalevel computation for algorithm recommendation S(f(x)). Similarly,
the learned instance datasets are called meta-examples. As suﬃcient
meta-examples are accumulated in P, the induction process proceeds
to the stage of learning from experience: meta-level learning. A learning process is imposed to meta-features f(x) of the meta-examples x
∈ P, the new instance xnew ∈ P, and the performance of the metaexamples y(a(x)). Finally, in the stage of applying learning knowledge:
the meta-level algorithm recommendation, the new instance is provided with a recommendation on algorithm selection, guided by the

C. Cui et al. / Expert Systems With Applications 46 (2016) 33–44

37

Fig. 1. A schematic diagram of Rice’s model with algorithm selection based on features of the problem.

learned knowledge by mapping the meta-features of the new to the
old ones. In this way, when a new instance is encountered, the user
does not need to try each one of the candidate algorithms, instead,
the recommended algorithm may provide satisfactory solutions. It is
noteworthy that the meta-learning system is dynamically updated,
once an instance is meta-learned, it could be immediately absorbed
as new gained experience that backs up future learning. As this is the
case, in the long run, one can expect expertise of the meta-learner,
which adaptively changes its bias according to the characteristics of
each task, as the system grows more experienced with accumulated
knowledge.
Based on the Rice’s model, the machine learning community has
studied the application of meta-learning for classiﬁcation problems
where the classiﬁcation algorithm which best labels each data instance to the classes is recommended. As we stated in Section 1,
the meta-model for simulation is used to predict continuous outputs,
thus regression algorithms shall be studied. A brief review on recommendation for regression problems is given in the next section.
2.2.2. Meta-learning for regression problems
The METAL project funded in 1998 by ESPRIT (a meta-learning
assistant for providing user support in machine learning and data
mining) is among the ﬁrst few attempts to explore the application
of meta-learning for regression problems. The project delivered the
Data Mining Advisor (DMA), a web-based meta-learning system for
the automatic selection of learning algorithms. In addition, Köpf,
Taylor, and Keller (2000) tested the suitability of meta-learning applied to regression problems using primarily the StatLog features.
The number of test regression problems is over 5,000, with various
sample sizes in the range of (110, 2,000), and 3 candidate regression
models were considered. Kuba, Brazdil, Soares, and Woznica (2002)
investigated new features for regression problems, e.g., presence of
outliers in the target, coeﬃcient of variation, etc., providing a supplement to StatLog measures as tested by Köpf et al. (2000). SmithMiles (2008)„ pointed out the potential of extending the algorithm
selection problem to cross-disciplinary developments, and a uniﬁed
framework was proposed to generalize the meta-learning concepts
for tasks such as regression, sorting, forecasting, constraint satisfaction, and optimization. Smith, Mitchell, Giraud-Carrier, and Martinez
(2014) applied a collaborative ﬁltering technique, meta-CF (MCF),
for the meta-learning and hyperparameter selection. MCF does not
rely on meta-features but only considers the similarity of the performance of the learning algorithms associated with their hyperparameter settings. MCF was validated on 125 data sets and 9 diverse learning algorithms, and shown to be a viable technique for recommending learning algorithms and hyperparameters. M. Smith and White
(2014) proposed the machine learning results repository (MLRR), an
easily accessible and extensible database for metalearning. MLRR was
designed as a data repository to facilitate meta-learning and provide
benchmark meta-data sets of previous experiment results, which is a
downloadable resource for other researchers.
As we discussed in Section 2.1, traditional meta-modeling approaches fail to provide an effective and eﬃcient way for model

selection, resulting in sub-optimal modeling solution and waste of
computations. While more investigations have focused on metalearning on cross-disciplinary studies, the applicability of metalearning on meta-model selection has yet to be fully deﬁned and
studied. In this study, we propose a generalized framework of metalearning for recommending meta-models speciﬁcally designed for
data-driven simulation modeling to investigate the suitability of the
approach and improvement it could achieve.

3. Proposed framework
3.1. Recommendation system for meta-modeling – a generalized
framework
The proposed framework is built upon Rice’s work (Fig. 1) with
two main advancements: First, feature reduction component is added
to the framework. Second, we expand the meta-learning algorithm
into a ranking based method including model-based learners and
instance-based learners, to strengthen the recommending capability
of the system. The pseudo-code of the proposed framework is presented in Fig. 2.

3.2. Meta-features
Before meta-learning is applied, one task to fulﬁll is to identify
available “features of instances that can be calculated and that correlate
with hardness/complexity” (Smith-Miles, 2008). The idea behind this
is to use learning algorithms to extract a uniﬁed body of knowledge
from the dataset, which adequately represents the entire dataset for
meta-level induction learning. Because the meta-learning algorithm
(meta-learner) is sensitive to the underlying structure of the data, the
determination and selection of appropriate features is a crucial step.
In this research, the statistical and geometrical meta-features are
derived. A total of 15 meta-features are proposed, of which the definitions and calculations are given below. Some of the features are
extensively used in meta-learning on classiﬁcation (Romero et al.,
2013; Sun & Pfahringer, 2013). For example, the basic statistical characterizations of the dataset, such as mean, median, standard deviation, skewness and kurtosis. Moreover, geometrical measurements
for data characterization, such as the gradient-based features on response values (1–4), outlier ratio (12), ratio of local extrema (13 &
14) and biggest difference (15) are derived. For a thorough review
on meta-features speciﬁcally for regression problem characterization,
we refer the reader to (Köpf et al., 2000; Pavel Brazdil et al., 1994).
Given N sample data points, for the ith sample point, let Gi be the
gradient and fi be the response of the point, point j is the nearest
neighbor of point i in Euclidian space. Gi is calculated as:

Gi = f i − f j ,

i 	= j.

(11)

(1) Mean of Gradient of Response Surface Point: Mean of absolute
values of gradient, Ḡ, which evaluates how steep and rugged
the surface is, by looking into its rate of change on the sample

38

C. Cui et al. / Expert Systems With Applications 46 (2016) 33–44

Fig. 2. A pseudo-code of meta-learning based recommendation system for meta-modeling.

data,

Ḡ = 1/N

N


|Gi |.

(12)

i=1

(2) Median of Gradient of Response Surface Point: Median of absolute values of gradient.
(3) SD of Gradient of Response Surface Point: Standard deviation
of gradient, SD (G), which evaluates the variation of the rate of
change on the sample data,



SD(G) =

1/(N − 1)

N


2

(Gi − Ḡ) .

(13)

i=1

(4) Max of Gradient of Response Surface Point: Maximum of absolute values of gradients on all response surface points, Gmax ,
which gives an upper bound of rate of change on the sample
data, a measure of the degree of sudden change on the surface.

Gmax = max|Gi |,

i = 1, . . . , N.

(14)

(5) Mean of Function values: Mean of response values, f¯, which
evaluates the general magnitude of the surface

f¯ = 1/N

N


fi .

(15)

(12) Outlier Ratio: Ratio of outliers of response values, which measures percentage of extreme values among all. An iterative implementation of the Grubbs Test (Grubbs, 1950), which is a statistical test used to detect outliers is applied in this study.
(13) & (14) Ratio of local extrema: Ratio of local minima and maxima within a given neighborhood, which measures the percentage of local ﬂuctuations. Note these measurements can
differentiate problems with a bumpy response surface and
with a ﬂat response surface. The neighborhood is deﬁned
within 5 nearest neighbors in this study.
(15) Biggest difference: Averaged local biggest difference of function values, D p , which evaluates the average bumpiness by
looking into the difference between “valley” and “peak” on
each local area

D̄ p = 1/s

s


D p , p = 1, . . . , s,

(19)

p=1

where s is the number of local areas, and Dp is the difference between “valley” and “peak” on area p. This measurement
gives an estimate on the magnitude of the bumpiness for each
response surface. 100 local areas are deﬁned in this study, by
dividing the whole design surface into smaller sub areas.
3.3. Meta-learners

i=1

(6) SD of Function values: Standard deviation of response values,
SD (f), which evaluates how bumpy the surface is by looking
into each value’s deviation from the mean.



SD ( f ) =

1/(N − 1)

N 




2
fi − f¯ .

(16)

i=1

(7) Skewness of Function values: Skewness of response values,
γ 1 (f), which evaluates the lack of symmetry on the surface




3

γ1 ( f ) = E [( fi − f¯)/Std.( fi )] , i = 1, . . . , N.

(17)

(8) Kurtosis of Function values: Kurtosis of response values, γ 2 (f),
which evaluates the ﬂatness relative to a normal distribution

γ2 ( f ) = E[( fi − f¯)4 /(E[ fi − f¯)2 ])2 , i = 1, . . . , N.

(18)

(9) Q1 of Function values: 25% quartile of response values, which
is the lower quartile of function values.
(10) Q2 of Function values: 50% quartile of response values, which
is the median of function values.
(11) Q3 of Function values: 75% quartile of response values, which
is the upper quartile of function values.

Meta-learning algorithms are generally categorized into two
groups: instance-based learning and model-based learning (Matijaš,
2013). The former learning method assumes the meta-modeling techniques exhibit similar performance on similar problems, where the
similarity is measured by some distance metric, e.g., Euclidean distance. While for the latter, one assumes that an underlying model
governs the way that algorithms perform on different problems.
3.3.1. Instance-based meta-learner
The k-Nearest Neighbor ranking approach is commonly selected
as an instance-based learner, due to its eﬃcient and effective performance in numerous applications. One naive approach is to solely
learn from the nearest neighbor of the target problem, by calculating
the Euclidean distance between the target problem i and the metaexamples:

dist (i, j) =



(xi − x j )2 ), j = 1, . . . , m

(20)

where xi is the meta-feature vector of i, and m is the number of metaexamples. The nearest neighbor is found by comparing all the distance measures and target the minimum. We call it the 1-NN method.
The k-NN method involves the nearest neighbors search step and a
ranking generation step. We ﬁrst select the k nearest neighbors by

C. Cui et al. / Expert Systems With Applications 46 (2016) 33–44

39

calculating the similarity between the test problem and the metaexamples, based on the meta-features. Next, the performance is calculated to make the recommendation. The cosine similarity is calculated as follows:

sim(i, j) = cos(xi , x j ) =



xi x j
2

x i  ×



x i 

2

, j = 1, . . . , m

(21)

The ranking of the algorithm a on the target problem i is predicted
as



ri,a =

j∈N(i)



sim(i, j)r j,a

j∈N(i)

(22)

sim(i, j)

where N(i) represents the set of k-NN of problem i.
3.3.2. Model-based meta-learner
The rank position values of each algorithm are the target (response) in the meta-learning models. A regression-based learner
assumes an underlying model between the meta-features and the
algorithm rankings, which could be trained by the meta-example
datasets. In addition, due to the correlations among the various metafeatures, a nonlinear model might be more appropriate. In this study,
we choose ANN as it is superior on non-linear function modeling
(Fonseca et al., 2003) and more robust to noisy and redundant features (Goodarzi, Deshpande, Murugesan, Katti, & Prabhakar, 2009).

Fig. 3. Uni-modal function: sphere function.

3.4. Performance space
The accuracy metrics reﬂect the degree of closeness of the metamodel measurement outputs ŷ to true output y. One global measurement for meta-modeling accuracy used in the performance space Y
(see Fig. 1) is Normalized Root Mean Square Error (NRMSE)



NRMSE =

N
i=1

(yi − ŷi )

2

N

/(ymax − ymin )

(23)

Since the focus of this research is to make a recommendation on
the meta-modeling algorithms from the algorithm space, we choose
to make the recommendation based on the ranks derived from the
NRMSE measures. Ranking is a relative measure which is scale-free
and case-wise independent. Since different problems are of different
levels of diﬃculty to be modeled, this may result in varied magnitudes of NRMSE measurements. The use of a relative measure, in this
study, rank, shall better facilitate the recommendation process. Given
the predicted rankings, two evaluation metrics are introduced:
 The Spearman’s rank correlation coeﬃcient (Neave & Worthington, 1989) which is employed to measure the agreement
between recommended rankings and ideal rankings. For two
samples of size N, the coeﬃcient of the recommended ranks xi
and the ideal ranks yi is computed as

ρ =1−6·

N

2
i=1 di
N2 − 1

N(

)

,

(24)

where di = xi − yi , is the difference of ranks of two samples.
The value of 1 represents perfect agreement while −1, perfect
disagreement. A correlation of 0 means that the rankings are
not related, which would be the expected score of the random
ranking method.
 Hit ratio: the percentage of exact matches between ideal best
performer and recommended best performer among all problems. This is to evaluate the “precision” of the meta-learning
algorithms. As a matter of fact, in the case of the meta-model
recommendation, users are more concerned if the recommended best performer (top 1) matches the ideal one, so only
one meta-model is built and computational eﬃciency is ensured. Therefore, besides the Spearman’s rank correlation coefﬁcient, the hit ratio is also proposed to comprehensively compare the performance of different meta-learners.

Fig. 4. Multi-modal function: rotated Weierstrass function.

4. Experiments and results analysis
To test the performance of our proposed framework, 44 benchmark functions are collected from IEEE CEC 2013&2014. There are
8 uni-modal functions which have only one global optimum (valley/peak), 28 multi-modal functions which have many local optima
(valleys/peaks), and 8 composition functions which are composed of
uni-modal and multi-modal functions. For illustration purpose, three
3-dimensional plots for 2-dimensional example test functions are
given in Figs. 3–5 (x, y-axis are the independent (input) variables, and
z-axis is the dependent (output) variable).
Note in this study, 10 dimensional functions are studied. These
functions are treated as simulation models without prior knowledge
for analysis. To simulate stochastic behavior of real world systems,
we purposely add uncertainties to the inputs and the outputs of the
functions. Speciﬁcally, parametric uncertainty (variability on each input variable) and residual uncertainty (variability on the outputs)
are considered. Random numbers generated within 10% of each input variable range in [–100, 100] depicts parametric uncertainty. For
the residual uncertainty, a random number is added to the training
data output, which is generated from a Normal distribution ∼ N(0,
σ 2 ), where σ 2 equals to 10% times the logarithm of the difference
between the maximum and minimum of the training output for each
black-box function. Since it is expected that with the existence of uncertainties, the same input does not generate the same output, 25

40

C. Cui et al. / Expert Systems With Applications 46 (2016) 33–44

Fig. 5. Composition function: composed of three multimodal functions.

simulation replicates are conducted. An average value of the 25 output replicates (ȳ) is taken as the output for training data while the
input for training data takes its nominal value (x), which is the value
without noise contamination. And the same operation is applied to
the test data.
Three successive experiments are conducted. In the ﬁrst experiment on meta-modeling, different sizes of training data generated
from the benchmark functions are tested on the six meta-models.
Meta-models’ performances are sensitive to the number of training
data, which will impact the accuracy of model recommendation on
meta-learning. Thus we need to decide the appropriate sample size
for promising and stabilized meta-modeling accuracy performance.
Once the sample size is settled, we implement experiments involving two types of meta-learner models, artiﬁcial neural network and
k-NN in the second experiments to explore the performance of the
meta-learners. In the third experiment, feature (meta-feature) reduction techniques are studied.
4.1. Experiment I – identiﬁcation of meta-modeling training size
The objective of this experiment is to identify the appropriate size
of the training data to be collected from the simulation. In this ex-

periment, Latin hypercube sampling (LHS) is chosen as the sampling
technique on each function of which the design space is set within
the range of [–100,100]. LHS is a statistical sampling method used
in construction of computer experiments for its good uniformity and
coverage from a multidimensional distribution. It is widely used because the sample size is not strictly determined by the number of dimensions of the simulation design space (Zhang, Zhu, Chen, & Arendt,
2012). Moreover, given the sample size is small, it is shown that LHS
makes simulations converge faster than traditional random sampling
strategies, e.g., Monte Carlo sampling (Matala, 2008).
The six meta-modeling techniques are separately trained on 10dimensional training datasets of ﬁve different training sizes, 80, 100,
150, 300 and 400. In order to avoid over ﬁtting, we implement 5fold cross-validation on the training process (Kohavi, 1995). 1,000
data points is randomly generated over the design space, which is
treated as the testing data set. The grid search method (Chang &
Lin, 2011) is implemented on the six meta-models to select the optimal parameters that give the minimum validation error. The test
data is applied to the optimally trained model to obtain its generalization error. A multiple comparison test is conducted on the mean
estimation of NRMSE of the six meta-models, across the ﬁve experiments. As is shown in Fig. 6, we observe that the slope of performance
improvement is steep from training size 80 to 200, while it changes
slowly after 200. Thus the “elbow” point of training size is identiﬁed
at 300. In the following experiment, all the meta-models are trained
with a sample size equal to 300.

4.2. Experiment II – meta-learning for meta-modeling
The objective of this experiment is to compare the instance-based
meta-learner vs. the model-based meta-learner. In this set of experiments, we adopt a leave-one-out strategy, that is, of the 44 problems,
43 are used as a training set, and the remaining one is used to test the
resulting meta-learner, which is repeated 44 times (Prudencio & Ludermir, 2004). The average recommendation performance measured
by Spearman’s rank correlation coeﬃcients and hit ratio are reported.
For each meta-learner, the Spearman’s correlation coeﬃcient is ﬁrst
calculated on each test problem by comparing the learned ranking
and ideal ranking of the six meta-models. When all the coeﬃcients
are gathered, they are averaged over 44 problems. The hit ratio is calculated as the ratio of the total number of matches on the recommended best performers among the 44 problems.

Fig. 6. Multiple comparison test on mean NRMSE of six meta-models of different sample sizes.

C. Cui et al. / Expert Systems With Applications 46 (2016) 33–44

41

Table 3
(Approximate) Computational cost comparison between the traditional trial-and-error
approach and meta-learning approach on each test problem.

Table 1
Performance statistics of meta-learners.
Meta-learner

Spearman’s correlation coeﬃcient

Hit ratio

ANN
1-NN
3-NN

0.8831
0.5486
0.5603

86.36% (38/44)
81.82% (36/44)
84.09% (37/44)

In this experiment, k-NN is chosen as the instance-based metalearner, Eqs. (20)–(21) are applied to identify the exemplar problem for the new studied problem which is then used to identify
the appropriate algorithm. ANN is chosen as the model-based metalearner which takes the following parameter settings: the hidden
layer size is tuned within the range of [10, 20], and the transfer functions are selected between radial basis and log sigmoid. We apply
10-fold cross validation with 70% split to training and 30% to validation for prevention of over-ﬁtting. Six ANN models are built on
six sets of the rankings of each meta-model across all 44 training
problems. Based on the preliminary experiment, we found the k-NN
method with k set to 3 is suitable. Table 1 summarizes the overall results of the meta-learners’ recommendation performance on
the 44 test problems. In Table 2, the top recommended meta-model
given by the meta-learners for each test problem is summarized
(the highlighted model is marked as inconsistent with the true best
model).
As can be seen in Table 1, ANN (model-based meta-learner) outperforms k-NN (1-NN and 3-NN, the instance-based meta-learner)
on both Spearman’s correlation coeﬃcient and Hit ratio. Though all
three meta-learners are able to recommend the appropriate algorithm for each problem (38, 36, 37 out of 44 test functions), ANN
is better at identifying the rankings overall (measured by the Spearman’s correlation coeﬃcient). We believe that this may be due to the
fact that the instance-based meta-learner solely relies on the features
that characterize the problems. If the features do not adequately represent the picture of the data, it is diﬃcult to ﬁnd the true similarity between the problems, thus making the learners ineffective on
recommending good models. While the model-based meta-learner
is a supervised learning approach as it derives the model to relate
the meta-features to the meta-model performance. As a result, it
may be more tolerant to the noises from the meta-features. In addition, we observe that the performance of 1-NN is lower than 3NN which indicates that as the number of neighbors increase, the
accuracy of k-NN learning improves. Therefore, we conclude that
model-based meta-learner generally outperforms instance-based
meta-learner.
Table 3 summarizes the (approximate) computational cost of
the two approaches on each test problem on an Intel i5 CPU 16G
computer. Here ANN, the meta-learner example, takes slightly longer
time to develop the model compared to the instance-based metalearner. As seen, the computational eﬃciency of meta-modeling
could be signiﬁcantly improved from an order of an hour to a minute,
by summing up the computational time of 44 functions.

Traditional
trial-and-error
approach

Meta-learning approach

Learning tasks

Meta-modeling with
Kriging, SVR, RBF,
MARS,ANN and PR

Learning cost

5 ∼ 10 min

Feature extraction,
Meta-learning (ANN) and one
meta-modeling with
recommended algorithm
0.05 s + 3 ∼ 5 s + 1 ∼ 1.5 min

4.3. Experiment III –feature reduction techniques comparison
The objective of this experiment is to explore the potential improvements that could be made by employing a feature reduction
technique on the meta-learning process. The features deﬁned in
Section 3.2 are tentatively selected in the hope that they could effectively represent the dataset. However, it is not guaranteed that
all of them are useful. As it is well accepted that redundant and irrelevant features deteriorate the model performance, we propose to
use advanced feature reduction techniques to address the noise the
curse of dimensionality issues. Three commonly used feature reduction techniques are studied including singular value decomposition
(SVD), stepwise regression and ReliefF.
 SVD is of interest in this research due to its known performance in
tolerating data noise (Chakroborty & Saha, 2010; Phillips, Watson,
Wynne, & Blinn, 2009; Simek, 2003; Simek et al., 2004). It is a
factorization of a real matrix X ∈ Rm × n , m ≥ n,

X = USV t ,

(25)

where U ∈ Rm × m and V ∈ Rn × n are orthogonal matrices and S
∈ Rm × n is a diagonal matrix. A rank-k (k  min(m, n)) matrix
C is deﬁned as the best low-rank approximation of matrix X if
it minimizes the Frobenius norm of the matrix (X − C), which is
known as the Eckart–Young theorem (Eckart & Young, 1936). This
approximation matrix can be computed by SVD factorization and
keeping the ﬁrst k columns of U, truncating S to the ﬁrst k diagonal components, and keeping the ﬁrst k rows of Vt . This results
in noise reduction by assuming the matrix X is low rank, which is
not generated at random but has an underlying structure. In this
research, the optimal rank of the reduced matrix is solved by the
random projection method. The optimal rank is identiﬁed as 3 resulting in a feature space dimension reduction from 44 by 15 (44
test functions, 15 meta-features) to 44 by 3 (44 test functions, 3
derived SVD “meta-features”).
 Stepwise regression carries out an automatic procedure on the
choice of predictive variables when building regression models.
It’s a wrapper method which uses a predictive model to score
feature subsets. The stepwise regression is set up with bidirectional elimination, with p-value threshold equal to 0.1. As a result, 7 meta-features are selected: (1) max of gradient of response

Table 2
Top recommended meta-model given by different meta-learners (K-Kriging, S-SVR, R-RBF, M-MARS, A-ANN, P-PR).
Problem #
True Best
ANN
1-NN
3-NN
Problem #
True Best
ANN
1-NN
3-NN

1
P
P
P
P
23
K
K
K
K

2
P
P
R
P
24
K
K
K
K

3
R
R
K
K
25
R
K
K
K

4
K
K
K
K
26
K
M
M
K

5
K
K
K
K
27
R
K
K
K

6
K
K
K
K
28
R
M
K
K

7
R
R
R
K
29
P
P
P
P

8
S
S
S
S
30
P
P
P
P

9
K
K
K
K
31
P
P
P
P

10
P
P
P
P
32
A
A
A
A

11
K
K
K
K
33
S
S
S
S

12
K
A
K
K
34
K
K
K
K

13
A
A
A
A
35
P
P
P
P

14
K
K
K
K
36
P
P
P
P

15
K
K
K
K
37
P
P
P
P

16
S
S
S
S
38
K
K
K
K

17
P
P
P
P
39
K
K
K
K

18
P
P
P
P
40
K
K
K
K

19
M
K
K
K
41
P
P
P
P

20
S
S
R
R
42
P
P
P
P

21
S
S
S
S
43
K
K
K
K

22
K
K
K
K
44
S
S
S
S

42

C. Cui et al. / Expert Systems With Applications 46 (2016) 33–44

Table 4
Summary statistics of three feature selection techniques: SVD, stepwise regression and
ReliefF.
Feature selection methods

Spearman’s correlation
coeﬃcient

Hit ratio

Singular value decomposition
Stepwise regression
ReliefF
Without feature selection

0.9351
0.9060
0.8956
0.8831

90.90% (40/44)
88.64% (39/44)
88.64% (39/44)
86.36% (38/44)

surface point, (2) standard deviation of gradient of response surface point, (3) mean of function values, (4) skewness of function
values, (5) kurtosis of function values, (6) Q2 of function values,
and (7) outlier ratio.
 The ReliefF algorithm examines the difference between features
of nearby instances and iteratively updates the weight of each
feature, where features are selected with higher averaged weight.
Due to the sensitivity of ReliefF to the settings of number of nearest neighbors, we tentatively set the k-value as 5, 10, 15, and 20,
and the ranks of the features are averaged across different k values. The averaged ranks decide which features will be selected
in the ﬁnal model. As a result, 10 meta-features are selected: (1)
mean of gradient of response surface point, (2) max of gradient of
response surface point, (3) median of gradient of response surface
point, (4) standard deviation of gradient of response surface point,
(5) standard deviation of function values, (6) kurtosis of function
values, (7) Q1 of function values (8) Q2 of function values, (9) Q3
of function values, and (10) outlier ratio.
Since we conclude the model-based meta-learner (ANN) outperforms the instance-based meta-learner, in this experiment, we
choose ANN as the test case to evaluate the eﬃcacy of the feature
reduction techniques. The summary statistics of the three methods is
given in Table 4. It is observed that both Spearman’s Correlation Coeﬃcient and hit ratio are improved by using feature reduction, where
SVD achieves the best performance. Moreover, the number of successful best performer recommendations increases to 40, resulting
in a hit ratio of 90.90%, using SVD. The performance of the reduced
ANN model using stepwise regression and ReliefF do not observe signiﬁcant difference, and compared to SVD, they are both slightly inferior. We contend that SVD may perform well when noise exists as
stated by (Baker, 2005). The second conclusion we draw from this
experiment is, given the 15 meta-features derived, there is redundancy among the features, therefore employing feature reduction
techniques has proved to be valuable in improving the recommendation system performance.
5. Conclusion
In this paper, we develop a meta-learning framework of a metamodel recommendation system for computation-intensive simulation problems. It addresses the problem of meta-model selection,
where appropriate meta-models are recommended for surrogate
modeling in substitute for physical models. The learned relationships
could be used to make predictions on model rankings for unseen
problems. Speciﬁcally, we propose a number of novel meta-features
such as the gradient-based features for characterizing the geometrical properties of the response surface. Next, we explore the use of
different meta-leaners (instance-based vs. model-based). The Modelbased learner outperforms the instance-based learner which may be
due to the fact that the model-based learner is a supervised method
which takes both of the meta-features and the model performance
into consideration in the learning process. We further explore the
contribution of feature reduction techniques and conclude SVD may
signiﬁcantly reduce the dimensionality of the feature space while

retaining the core information, which not only expedites the metalearning process, but also improves the overall performance.
To demonstrate the applicability and eﬃcacy of the proposed recommendation system, 44 benchmark problems have been tested, including uni-modal, multi-modal and composition problems covering
a wide range of feature domains. To evaluate the predictive capability
of the proposed framework, we have also implemented various popular meta-modeling methods in the literature, including Kriging, SVR,
RBF, MARS, ANN and PR. Computational experiments clearly show
that the proposed system signiﬁcantly improves the computational
eﬃciency on meta-modeling and is consistently capable of recommending appropriate models across the 44 benchmark test cases. The
results indicate our proposed framework is able to serve as an alternative approach for traditional meta-modeling tasks, especially when
the number of candidate meta-models is large and little prior knowledge of the problems is available.
Regarding to practical advantages and research contribution in
expert and intelligent systems, the proposed recommendation system in this work can be used to facilitate the development of various
expert systems, such as decision making and support systems. The
proposed meta-learning based recommendation system augments
the traditional trial-and-error meta-modeling method to a structured
and automated form suitable for computer manipulation, opening up
many possibilities for using it. The generic system is able to automate
and optimize the modeling process without human involvement and
excessive computations. It emulates the human’s decision-making
ability, which is to reason about knowledge based on past experience to solve complex problems. Speciﬁcally, it consists of two components: the knowledge base, which represents facts and rules, and
the inference engine, which applies the rules to the known facts to
deduce new facts. This work provides practical guidelines in the design, development, implementation, and testing of a meta-model recommendation expert system for simulation engineering and machine
learning. Due to these theoretical contributions and advantages, the
recommendation system can be applied to the simulation industry
to reduce the cost and improve modeling and operation eﬃciency.
Moreover, it is advised to facilitate simulation optimization applications where surrogate modeling is of signiﬁcant implementation
in support of effective model construction and computational cost
saving.
While promising, we want to note there is room for improvement. For example, extended efforts on feature characterization on
the meta-models for knowledge extraction can be explored. In addition, the ranks used for recommendation are derived from a single
NRMSE measure. This may be extended to include multi-criteria metrics, e.g., robustness and computational cost. We believe there is room
for improvement on extendibility of candidate models and test case
sets, as this study uses a subset of the possible meta-models and test
problems available in the literature. Inclusion of other meta-models
and test cases may extend the expert system knowledge base. We
plan to extend our proposed framework reported in this paper with
these future research directions.
For future research suggestions in expert and intelligent systems,
the proposed model recommendation system can beneﬁt by automatically identifying the appropriate models for a given task. Therefore, the meta-learning could not only be used in a meta-modeling
application, but can also be used in optimization with meta-heuristic
algorithms, where hundreds of algorithms are available but little insight has been gained regarding which algorithms perform well on
which problems. Similarly, the idea could further inspire or enhance
a number of research applications, such as classiﬁcation, forecasting
and general regression tasks, where model selection and model recommendation is of urgent need. For example, in the research ﬁelds
of complex systems such as aircraft design, the task is a sophisticated system engineering one where multiple disciplines are often
involved, such as, aerodynamics, multi-objective optimization, and

C. Cui et al. / Expert Systems With Applications 46 (2016) 33–44

computationally-intensive processes. Due to the computational efﬁciency and automatic learning capability of meta-learning, it can
be applied in both the optimization process for algorithm selection and the computationally-intensive process for meta-model recommendation. This is especially true when the number of design
parts are large, and the parts can be described by shared common
features.
Acknowledgement
This research was partially supported by funds from the National
Science Foundation award (CNS-1239257), from the United States
Transportation Command (USTRANSCOM) in concert with the Air
Force Institute of Technology (AFIT) under an ongoing Memorandum
of Agreement, and National Science Foundation of China (71501132).
The U.S. Government is authorized to reproduce and distribute for
governmental purposes notwithstanding any copyright annotation
of the work by the author(s). The views and conclusions contained
herein are those of the authors and should not be interpreted as necessarily representing the oﬃcial policies or endorsements, either expressed or implied, of USTRANSCOM, AFIT, the Department of Defense, or the U.S. Government.
Supplementary materials
Supplementary material associated with this article can be found,
in the online version, at doi:10.1016/j.eswa.2015.10.021.
References
Acar, E. (2015). Effect of error metrics on optimum weight factor selection for ensemble
of metamodels. Expert Systems with Applications, 42(5), 2703–2709.
Baker, K. (2005). Singular value decomposition tutorial. The Ohio State University, 2005,
24.
Banks, J., Carson, J., Nelson, B. L., & Nicol, D. (2004). Discrete-Event System Simulation.
PrenticeHall International series in industrial and systems engineering.
Barker, P. A., Street-Perrott, F. A., Leng, M. J., Greenwood, P. B., Swain, D. L., Perrott, R. A.,
et al. (2001). A 14,000-year oxygen isotope record from diatom silica in two alpine
lakes on Mt. Kenya. Science, 292, 2307–2310.
Barton, R. R. (1992). Metamodels for simulation input-output relations. In Proceedings
of Winter Simulation Conference: Vol. 9 (pp. 289–299).
Bashiri, M., & Farshbaf Geranmayeh, A. (2011). Tuning the parameters of an artiﬁcial
neural network using central composite design and genetic algorithm. Scientia
Iranica, 18, 1600–1608.
Brazdil, P., Carrier, C., Soares, C., & Vilalta, R. (2008). Metalearning: Applications to data
mining. Springer Science & Business Media.
Brazdil, P., Gama, J., & Henery, B. (1994). Characterizing the applicability of classiﬁcation algorithms using meta-level learning. In F. Bergadano, & L. De Raedt (Eds.), Machine Learning: ECML-94 SE - 6: Vol. 784 (pp. 83–102). Berlin Heidelberg: Springer.
Brazdil, P., Soares, Costa, C., & Da, J. (2003). Ranking learning algorithms: Using IBL and
meta-learning on accuracy and time results. Machine Learning, 251–277.
Chakroborty, S., & Saha, G. (2010). Feature selection using singular value decomposition
and QR factorization with column pivoting for text-independent speaker identiﬁcation. Speech Communication, 52(9), 693–709.
Chang, C.-C., & Lin, C.-J. (2011). LIBSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology, 2, 1–27 27.
Clarke, S. M., Griebsch, J. H., & Simpson, T. W. (2005). Analysis of support vector regression for approximation of complex engineering analyses. Journal of Mechanical
Design, Transactions of the ASME, 127, 1077–1087.
Cui, C., Wu, T., Hu, M., Weir, J. D., & Chu, X. (2014). Accuracy vs. robustness: Bi-criteria
optimizaed ensemble of metamodels. In Proceedings of the 2014 Winter Simulation
Conference (pp. 616–627).
De Souto, M. C. P., Prudencio, R. B. C., Soares, R. G. F., de Araujo, D. S. a., Costa, I. G.,
Ludermir, T. B., et al. (2008). Ranking and selecting clustering algorithms using a meta-learning approach. In Proceedings of the 2008 IEEE International Joint
Conference on Neural Networks (IEEE World Congress on Computational Intelligence)
(pp. 3729–3735).
Draper, N., & Smith, H. (1981). Applied regression analysis 2nd ed. Technometrics, 47 pp.
380–380.
Drucker, H., Chris, Kaufman, B. L., Smola, A., & Vapnik, V. (1997). Support vector regression machines. In Advances in Neural Information Processing Systems 9: Vol. 9
(pp. 155–161).
Dyn, N., Levin, D., & Rippa, S. (1986). Numerical procedures for surface ﬁtting of scattered data by radial functions. SIAM Journal on Scientiﬁc and Statistical Computing,
7, 639–659.
Eckart, C., & Young, G. (1936). The approximation of one matrix by another of lower
rank. Psychometrika, 1, 211–218.

43

Efroymson, M. A. (1960). Multiple regression analysis. Mathematical Methods for Digital
Computers, 1, 191–203.
Fallucchi, F., Zanzotto, F.M., & Rome, P. (2009). Singular Value Decomposition for Feature Selection in Taxonomy Learning Unsupervised Feature Selec-, 82–87.
Fang, H., Rais-Rohani, M., Liu, Z., & Horstemeyer, M. F. (2005). A comparative study of
metamodeling methods for multiobjective crashworthiness optimization. Computers & Structures, 83(25–26), 2121–2136.
Fonseca, D. J., Navaresse, D. O., & Moynihan, G. P. (2003). Simulation metamodeling
through artiﬁcial neural networks. Engineering Applications of Artiﬁcial Intelligence,
16, 177–183.
Friedman, J. H. (1991). Multivariate adaptive regression splines. The Annals of Statistics,
19, 1–67.
Gergonne, J. D. (1974). The application of the method of least squares to the interpolation of sequences. Historia Mathematica, 1(4), 439–447.
Giraud-Carrier, C. (2008). Metalearning-a tutorial. In Proceedings of the Tutorial at the
2008 International Conference on Machine Learning and Applications, ICMLA (December).
Goodarzi, M., Deshpande, S., Murugesan, V., Katti, S., & Prabhakar, Y. (2009). Is feature
selection essential for ANN Modeling? QSAR & Combinatorial Science, 28(11-12),
1487–1499.
Greenland, S. (1995). Dose-response and trend analysis in epidemiology: Alternatives
to categorical analysis. Epidemiology (Cambridge, Mass.), 6, 356–365.
Grubbs, F. E. (1950). Sample criteria for testing outlying observations. The Annals of
Mathematical Statistics, 21, 27–58.
Hocking, R. R. (1976). The analysis and selection of variables in linear regression. Biometrics, 32, 1–49.
Jin, R., Chen, W., & Simpson, T. W. (2001). Comparative studies of metamodelling techniques under multiple modelling criteria. Structural and Multidisciplinary Optimization, 23(1), 1–13.
Kira, K., & Rendell, L. (1992). The feature selection problem: Traditional methods and a
new algorithm. In Proceedings of AAAI (pp. 129–134).
Kleijnen, J. P. C. (1995). Veriﬁcation and validation of simulation models. European Journal of Operational Research, 82, 145–162.
Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and
model selection. Proceedings of IJCAI.
Kononenko, I., Šimec, E., & Robnik-Šikonja, M. (1997). Overcoming the myopia of inductive learning algorithms with RELIEFF. Applied Intelligence, 7,
39–55.
Köpf, C., Taylor, C., & Keller, J. (2000). Meta-analysis: From data characterisation for
meta-learning to meta-regression. In Proceedings of the PKDD-00 Workshop on Data
Mining, Decision Support, Meta-Learning and ILP (Ml).
Kristensen, N. R., Madsen, H., & Jørgensen, S. B. (2004). A method for systematic improvement of stochastic grey-box models. Computers & Chemical Engineering, 28(8),
1431–1449.
Kuba, P., Brazdil, P., Soares, C., & Woznica, A. (2002). Exploiting sampling and
meta-learning for parameter setting for support vector machines. In Proceedings
of the Workshop de Minería de Datos Y Aprendizaje of {(IBERAMIA} 2002) (pp. 217–
225).
Lan, Z., Gu, J., Zheng, Z., Thakur, R., & Coghlan, S. (2010). A study of dynamic metalearning for failure prediction in large-scale systems. Journal of Parallel and Distributed Computing, 70(6), 630–643.
Liang, J., & Qu, B. (2013a). Problem deﬁnitions and evaluation criteria for the CEC 2013 special session on real-parameter optimization, Technical Report, (12). Singapore: Computational Intelligence Laboratory, Zhengzhou University, Zhengzhou, China and
Nanyang Technological University.
Liang, J., Qu, B., & Suganthan, P. (2013b). Problem deﬁnitions and evaluation criteria for
the CEC 2014 special session and competition on single objective real-parameter numerical optimization. Computational Intelligence Laboratory (December 2013).
Matala, A. (2008). Sample size requirement for Monte Carlo simulations using Latin hypercube sampling. Helsinki University of Technology, Department of Engineering
Physics and Mathematics, Systems Analysis Laboratory.
Matheron, G. (1960). Krigeage d’un Panneau Rectangulaire par sa Périphérie. Note Géostatistique, (28).
Matijaš, M. (2013). Electric Load Forecasting using Multivariate Meta-learning. Fakultet
elektrotehnike i računarstva, Sveučilište u Zagrebu.
Matijaš, M., Suykens, J. A. K., & Krajcar, S. (2013). Load forecasting using a multivariate
meta-learning system. Expert Systems with Applications, 40(11), 1–11.
McCulloch, W. S., & Pitts, W. H. (1943). A logical calculus of ideas imminent in nervous
activity. Bulletin of Mathematics Biophysics, 5, 115–133.
Nasereddin, M., & Mollaghasemi, M. (1999). The development of a methodology for the
use of neural networks and simulation modeling in system design. In Proceedings
of 1999 Winter Simulation Conference (WSC’99) – Vol. 1 (pp. 537–542).
Neave, H. R., & Worthington, P. L. (1989). Distribution-free tests. London: Routledge
Routledge.
Packianather, M., Drake, P., & Rowlands, H. (2000). Optimizing the parameters of multilayered feedforward neural networks through Taguchi design of experiments.
Quality and Reliability Engineering International, 16(6), 461–473 (February).
Phillips, R. D., Watson, L. T., Wynne, R. H., & Blinn, C. E. (2009). Feature reduction
using a singular value decomposition for the iterative guided spectral class rejection hybrid classiﬁer. ISPRS Journal of Photogrammetry and Remote Sensing, 64(1),
107–116.
Prudencio, R., & Ludermir, T. (2004). Using machine learning techniques to combine
forecasting methods. In Proceedings of the AI 2004: Advances in Artiﬁcial Intelligence:
3339 (pp. 1122–1127).
Rendell, L., Seshu, R., & Tcheng, D. (1987). Layered concept-learning and dynamicallyvariable bias management. In Proceedings of IJCAI-87 (pp. 308–314).

44

C. Cui et al. / Expert Systems With Applications 46 (2016) 33–44

Rice, J. (1975). The algorithm selection problem, 75–152.
Romero, C., Olmo, J., & Ventura, S. (2013). A meta-learning approach for recommending
a subset of white-box classiﬁcation algorithms for Moodle datasets. Available at:
Educationaldatamining.org.
Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage
and organization in the brain. Psychological Review, 65(6), 386–408.
Ryberg A.B., Domeij Backryd R., Nilsson L. (2012). Metamodel-based multidisciplinary
design optimization for automotive applications. Linkoping, Sweden: Linkoping
University. Technical Report.
Shaw, P., Greenstein, D., Lerch, J., Clasen, L., Lenroot, R., Gogtay, N., et al. (2006). Intellectual ability and cortical development in children and adolescents. Nature, 440,
676–679.
Simek, K., Fujarewicz, K., Świerniak, A., Kimmel, M., Jarzab,
˛ B., Wiench, M., et al. (2004).
Using SVD and SVM methods for selection, classiﬁcation, clustering and modeling
of DNA microarray data. Engineering Applications of Artiﬁcial Intelligence, 17(4), 417–
427.
Simek, K. R. (2003). Properties of a singular value decomposition based dynamical
model of gene expression data. International Journal of Applied Mathematics and
Computer Science, 13(3), 337–345.
Simpson, T., Peplinski, J., Koch, P. N., & Allen, J. K. (1997). On the use of statistics in design and the implications for deterministic computer experiments. Design Theory
and Methodology, 14–17.
Smith, M. R., Mitchell, L., Giraud-Carrier, C., & Martinez, T. (2014). Recommending
learning algorithms and their associated hyperparameters. In Proceedings of MetaLearning and Algorithm Selection (p. 2).
Smith, M., & White, A. (2014). An Easy to Use Repository for comparing and improving
machine learning algorithm usage. In Proceedings of Meta-Learning and Algorithm
Selection (p. 7).

Smith-Miles, K. a. (2008). Cross-disciplinary perspectives on meta-learning for algorithm selection. ACM Computing Surveys, 41(1), 1–25.
Souza, B. F. De, Carvalho, A. De, & Soares, C. (2008). Metalearning for gene expression
data classiﬁcation. In Proceedings of 2008 Eighth International Conference on Hybrid
Intelligent Systems (pp. 441–446).
Sun, Q., & Pfahringer, B. (2013). Pairwise meta-rules for better meta-learning-based
algorithm ranking. Machine Learning, 93(1), 141–161.
Utgoff, P. (1986). Shift of bias for inductive concept learning. In Machine Learning: An
Artiﬁcial Intelligence Approach (pp. 107–148).
Vilalta, R., & Drissi, Y. (2002). A perspective view and survey of meta-learning. Artiﬁcial
Intelligence Review, 1997, 77–95.
Wan, H., & Ren, W. (2015). Parameter selection in ﬁnite-element-model updating by
global sensitivity analysis using Gaussian process metamodel, 1–11.
Wang, G. G., & Shan, S. (2007). Review of metamodeling techniques in support of engineering design optimization. Journal of Mechanical Design, 129(4), 370.
Wolpert, D. H. (1996). The lack of a priori distinctions between learning algorithms.
Neural Computation, 8(7), 1341–1390.
Wolpert, D. H., & Macready, W. G. (1997). No free lunch theorems for optimization. IEEE
Transactions on Evolutionary Computation, 1(1), 67–82.
Yin, H., Wen, G., Fang, H., Qing, Q., Kong, X., Xiao, J, et al. (2014). Multiobjective crashworthiness optimization design of functionally graded foam-ﬁlled tapered tube
based on dynamic ensemble metamodel. Materials & Design, 55(2014), 747–757.
Zhang, S., Zhu, P., Chen, W., & Arendt, P. (2012). Concurrent treatment of parametric
uncertainty and metamodeling uncertainty in robust design. Structural and Multidisciplinary Optimization, 47(1), 63–76.
Zhou, S., Lai, K. K., & Yen, J. (2012). A dynamic meta-learning rate-based model for gold
market forecasting. Expert Systems with Applications, 39(6), 6168–6173.

TRENDS & CONTROVERSIES
Editors: Hui yang, University of South Florida, huiyang@usf.edu
erhun Kundakcioglu, Özyeğin University, erhun.kundakcioglu@ozyegin.edu.tr

Healthcare Intelligence:
Turning Data into
Knowledge
Hui Yang, University of South Florida
Erhun Kundakcioglu, Özyeğin University

O

ne of the major concerns for the US government is its healthcare system. It’s estimated

that US healthcare spending is close to 17 percent
of its gross domestic product (that is, $2.5 trillion),
and will continue the historical upward trend,
reaching 19.5 percent of the GDP by 2017.1
Notably, the US healthcare system in the 21st century is investing in a variety of sensors, communication technologies, and dedicated data centers
to increase information visibility and operational
efficiency. The rapid advancements of health information technologies have led to Big Data environments in hospitals. Nevertheless, meaningful
information extracted from rich healthcare datasets is still limited. In the general practice of medicine, healthcare providers lack decision-support
tools that will enable and assist
•	 the handling of rich and large-scale healthcare
recordings,
•	 the optimization of operational dynamics in a
healthcare system,
•	 the extraction of pertinent knowledge about
health conditions of patients, and
•	 the provision of more personalized and effective
healthcare services.
With massive data readily available, there are unprecedented opportunities to manage, analyze, visualize, and extract useful information from large,
diverse, distributed and heterogeneous datasets so
as to make better medical decisions and improve
the performance of healthcare system. Big Data
is motivating a profound transformation in every
corner of healthcare systems. New advances in informatics and analytics research are transforming
how biomedical research is being conducted, and
how healthcare is delivered and managed in the
54

world. Better medical decision making, improved
patient monitoring systems, and effective public
health surveillance are increasingly viewed by the
medical community, the government, and the general public as key drivers to promote innovation
and reduce costs in the arena of healthcare.

Research Initiatives
In recent years, several government and academia
initiatives to advance data science and health informatics have begun. On December 8 2010, the President’s Council of Advisors on Science and Technology (PCAST) released the report Realizing the Full
Potential of Health Information Technology to
Improve Healthcare for Americans: The Path Forward (see www.whitehouse.gov/sites/default/fi les/
microsites/ostp/pcast-health-it-report.pdf). This
report identified a data-centric approach to realizing the potential of health IT. Also, advanced
health informatics is featured as one of the 14
grand challenges of engineering by the National
Academy of Engineering.2 The primary research
goal is to greatly enhance the quality and efficiency of medical care and to respond to widespread public health emergencies through the acquisition, management, and use of information in
health data.
The National Institutes of Health (NIH) supports fundamental research in all medical fields.
Notably, a number of NIH programs are relevant
to the research in data mining and health informatics. For example, through the program announcement (PA) of Innovations in Biomedical Computational Science and Technology (R01), NIH
institutes and centers offer support for fundamental research in biomedical informatics and computational biology; development of new computational tools and technologies; and applications of
computational technologies to a particular domain

1541-1672/14/$31.00 © 2014 IEEE

Ieee InTeLLIGenT SySTeMS

Published by the IEEE Computer Society

IS-29-03-Trends.indd 54

7/21/14 11:07 AM

area(s) in biomedical research.3 In addition, the Agency for Healthcare
Research & Quality (AHRQ) announced a PA of Understanding Clinical Information Needs and Health
Care Decision-Making Processes in
the Context of Health ­Information
Technology (R01). Research projects submitted to this program are
expected to address current knowledge gaps regarding our understanding of healthcare providers’
information needs and healthcare
decision-­
making processes, both individually and collectively, and as a
healthcare team.4
The US National Science Foundation (NSF) has also released solicitations jointly with NIH to seek research in data mining and health
informatics, such as Core Techniques
and Technologies for Advancing Big
Data Science & Engineering, Smart
and Connected Health (SCH),5 and
Computational and Data-Enabled
Science and Engineering (CDS&E).6
Moreover, the NSF Division of
Mathematical Sciences and NIH Institute of General Medical Sciences
(DMS/NIGMS) are working together
in their Joint DMS/NIGMS Initiative
to Support Research at the Interface
of the Biological and Mathematical
Sciences. Due to the extraordinary
growth of data-rich biology, this effort is aimed at supporting research
in mathematics and statistics on
questions in the biological and biomedical sciences.7
In light of such overwhelming interests, there are exceptional opportunities for researchers and practitioners
to invest in conducting innovative
and transformative research in data
mining and health informatics. This
IEEE Intelligent Systems “Trends
and Controversies” (T&C) department hopes to raise awareness and
highlight selected recent research to
move toward such goals.
MAy/June 2014	

IS-29-03-Trends.indd 55

In This Issue
The T&C department includes this
introduction and four other articles
on recent advances and trends in research and development in data mining and health informatics. While
data sciences have accelerated significantly and integrated with biomedical sciences in the past decade, many
fundamental questions regarding Big
Data analytics, knowledge discovery, large-scale performance, personalized computing, and parallel processing remain to be explored and
answered. Nevertheless, there has
been a proliferation of data analytical methods, techniques, and tools,
many of which have already been
used quite successfully in real-world
healthcare settings.
However, data-driven approaches
emphasize the extraction of meaningful patterns in the data, and they may
overlook the physics governing complex biological systems that generate
the data. As such, data-driven black
box approaches are often found to
be difficult to interpret and communicate with medical scientists. On the
other hand, physics-based approaches
are developed purely based on the
underlying physics of biological processes. It’s not uncommon that physics-based models don’t always match
satisfactorily with real-world data.
To advance the science of health informatics, it’s imperative to establish
close collaborations between data scientists and medical scientists. Such a
constructive interaction will promote
the tight integration of data-driven
approaches with biological physics,
thereby leading to the identification
of new solutions to persistent challenges in healthcare. The goal of this
T&C department is to provide stateof-the-art methods and practices in
data mining and health informatics. While several topics are covered
here, we must concede that some
www.computer.org/intelligent	

i­mportant challenges remain to be
treated elsewhere.
In the first article, “Empowering
Excellence of Care by Radiology Informatics,” Jing Li and her colleagues
present three major areas of informatics research from their group that
aim to provide quality-assured, accessible, and patient-centered radiology best practices:
•	 a novel enterprise-wide information system technology, Dose Index
Tracking (DIT), that’s developed
to automatically track and monitor cumulative radiation doses for
each patient and support real-time
query, reporting, and alarm notification if dose limit guidelines are
exceeded;
•	 a mobile application on IOS ­devices,
ResolutionMD Mobile, that’s developed for radiology image viewing
to reduce the time to treatment; and
•	 Big Data analytics and machine
learning approaches that are proposed for inter- and intra-tumor
imagegenomics, thereby enabling
individualized, outcome-optimized
cancer treatment.
In the second article, “Opportunities for Operations Research in Medical Decision Making,” Sait Tunc and
his colleagues discuss the effective
use of operations research (OR) in
medical decision making as a means
to improve the health of patients and
provide cost-effective healthcare.
They highlighted recent successful
applications of OR approaches, particularly Markov decision processes,
in the field of medical decision making. In addition, they envision future
research directions of OR in medicine, including the personalization
of screening and treatment, quantitative modeling of patient behavior,
and optimal c­ommunication within
multi­disciplinary care models.
55

7/21/14 11:07 AM

In the third article, “Diagnostic
Network Modeling of Neural Connectivity Using Functional Magnetic
Resonance Imaging,” W. Art Chaovalitwongse and his colleagues discuss their experience in applying network modeling approaches to explore
the brain’s functional organization
and to examine whether alterations to
functional connectivity in neurological diseases can be used to improve
diagnosis and answer other clinical
questions.
Finally, in “Spatial Clustering in
Public Health: Advances and Challenges,” Lianjie Shu and his colleagues
review basic principles and properties
in the methodologies of spatial clustering of diseases, including the spatial scan method, graph-based method,
and false discovery rate (FDR)-based
method. They further pointed out that
there’s no versatile method that can
handle all types of clustering problems.
Each different clustering approach has
its own advantages and disadvantages.
The major challenges for spatial clustering of diseases are robust algorithms
in the presence of model uncertainty,
multivariate spatial clustering, spatiotemporal clustering, and sensitivity and
accuracy.

These

articles represent a broad
spectrum of research and education for realizing the healthcare intelligence. They present unique perspectives, analytical methodologies,
healthcare applications, and technology transfer opportunities. They also
demonstrate the power of turning the
Big Data generated in healthcare industry into useful knowledge for intelligent healthcare decisions. In the
future, it’s anticipated that Big Data
analytics and informatics research
will bring a profound transformation
in the arena of healthcare.

56		

IS-29-03-Trends.indd 56

Acknowledgments

This research is supported in part by US National Science Foundation grants CMMI1266331 and IOS-1146882.

References
	 1.	S. Keehan et al., “Health Spending Projections through 2017: The Baby-Boom
Generation Is Coming to Medicare,”
Health Affairs, 2008; doi:10.1377/
hlthaff.27.2.w145.
	 2.	National Academy of Engineering,
Grand Challenges for Engineering,
2012; www.engineeringchallenges.
org/cms/8996.aspx.
	 3.	National Institute of Health (NIH),
Innovations in Biomedical Computational Science and Technology (R01),
program announcement, 2013; http://
grants.nih.gov/grants/guide/pa-files/
PAR-09-218.html.
	 4.	Agency for Healthcare Research and
Quality (AHRQ), Understanding Clinical Information Needs and Health Care
Decision-Making Processes in the Context of Health Information Technology
(R01), program announcement, 2013;
http://grants.nih.gov/grants/guide/pafiles/PA-11-198.html.
	 5.	National Science Foundation (NSF),
Smart and Connected Health (SCH),
program announcement NSF 13-543,
2013; www.nsf.gov/pubs/2013/
nsf13543/nsf13543.htm.
	 6.	NSF, Computational and DataEnabled Science and Engineering
(CDS&E), program announcement,
2014; www.nsf.gov/funding/pgm_
summ.jsp?pims_id=504813.
	 7.	NSF, Joint DMS/NIGMS Initiative to
Support Research at the Interface of the
Biological and Mathematical Sciences,
program announcement, 2013; www.
nsf.gov/pubs/2013/nsf13570/nsf13570.
htm.

Erhun Kundakcioglu is an assistant professor in the Department of Industrial Engineering at Özyeğin University. Contact him
at erhun.kundakcioglu@ozyegin.edu.tr.

Empowering Excellence
of Care by Radiology
Informatics
Jing Li and Teresa Wu, Arizona State
University
J. Ross Mitchell, Amy K. Hara,
William Pavlicek, Leland S. Hu,
Alvin C. Silva, and Christine M. Zwart
Mayo Clinic Arizona

Radiology uses various imaging techniques to diagnose, monitor, and treat
diseases. With the increasing sophistication of imaging techniques, the utilization of radiology has increased dramatically over the past 20 years. Radiology
has now become a diagnostic standard
for cancer, stroke, heart diseases, hepatobiliary diseases, neurological diseases,
and plays a role in almost all major
medical conditions. Despite this, there
have been controversies over the current radiology practices with respect to
the high costs, radiation dose, and proliferation of number of images. Among
many contributing factors, lack of systematic development and the application of informatics tools pose a significant obstacle to the optimal utilization
of radiology. Here, we present three
major areas of informatics research
carried out by our group, along with
our developments that intend to provide quality-assured, accessible, and
patient-centered radiology best prac­
tices. We’ll briefly introduce the work
here and detail it further in a bit.

Overview
Hui Yang is the director of the Com-

plex Systems Monitoring, Modeling, and
A nalysis Lab at the University of South
­
Florida. Contact him at huiyang@usf.edu.
www.computer.org/intelligent	

First, associated with the widespread
use of radiology is a serious ­concern
about the overutilization of imaging
services. Scans save lives, but cost a
IEEE INTELLIGENT SYSTEMS

7/21/14 11:07 AM

lot and increase radiation e­xposure.
A recent study found that a dramatic
increase in diagnostic tests, including
X-rays, computed tomography (CT)
scans, and mammograms, means
that Americans are exposed to seven
times more radiation than they were
30 years ago. This increased radiation exposure from medical imaging raises concerns about long-term
potential side effects. Currently, cumulative radiation doses from different imaging exams over a lifetime isn’t closely monitored. This is
because imaging dose information
is saved in disparate, unconnected
medical records and consolidating
all the information together manually is time-consuming and errorprone. To contend with this, we’ve
developed a novel enterprise-wide information system technology, Dose
Index Tracking (DIT),1 that can automatically track and monitor cumulative radiation doses for each patient
and support real-time query, reporting, and alarm notification if dose
limit guidelines are exceeded. This is
a fundamental capability toward providing quality and safety-assured radiology care to patients
Second, access to medical images
is of critical importance not only for
radiologists, but also for all clinical
providers, to aid in decision making or for communication with patients or other providers. A major
problem today is limited or slow access to medical images, particularly
after hours or outside the hospital.
This limited accessibility can delay decision making, unnecessarily
prolong a patient’s episode of care,
and may lead to higher costs. In addition, the lack of mobile image access requires many on-call clinicians to be closely tethered to home
computers after work hours, leading to decreased ­
clinician satisfaction and work/life balance issues.
MAy/June 2014	

IS-29-03-Trends.indd 57

To ­a ddress this, we’ve developed a
­mobile ­a pplication for radiology
image viewing. Our application
runs on iOS and Android devices,
such as smartphones and tablets.
This technology was transferred to
Calgary Scientific Inc. (CSI), where
it was enhanced to become ResolutionMD Mobile. 2,3 ResolutionMD
Mobile is FDA cleared, Health
Canada licensed, CE Marked, and
Chinese FDA registered for diagnostic use. This technology is available now in 22 countries around the
world. One of its primary applications is to accelerate diagnosis during acute care scenarios. Several
studies have shown that for acute
stroke it provides diagnostic performance on par with traditional radiology workstations, but reduces the
time to treatment. The saved time
should translate into reduced costs
and better patient outcomes.
Third, inter- and intra-tumor genomic diversity is one of the major
contributing factors to poor clinical
outcomes of cancer treatment, such
as low survival, recurrence, and
treatment resistance. Genomic profiling of tumors requires surgical biopsy that’s invasive, costly, and time
consuming. Recent studies have provided mounting evidence that radiologic imaging can inform tumor
molecular-level detail, thereby introducing the field of imagenomics.
We propose Big Data analytics and
machine learning approaches to correlate multimodality imaging with
high-throughput genomic signatures
and identify imaging features that
can serve as genomic surrogates.
With this capability, we can use imaging to non-invasively and conveniently obtain an in vivo portrait for
tumor genomics. This, together with
other patient-specific clinical attributes, ­enables individualized, outcome-optimized cancer treatment.
www.computer.org/intelligent	

DIT: Supporting Quality
Assurance in Radiology
Practices
Quality assurance (QA) monitoring
of ionizing radiation use with certain
diagnostic and procedural imaging
is required by state regulatory bodies and by the Joint Commission in
the US. Recently, public discussion in
US national news has expressed concerns regarding the use of CT and fluoroscopy. Their expanding use, when
coupled with instances of erythema
and overexposure that have occurred
following certain CT examinations,
has resulted in increased regulatory
oversight. These concerns drive medical facilities to provide enhanced capabilities for monitoring any and all
radiation use.
Radiology Picture Archiving and
Communication Systems (PACS) can
provide some insights from archived
examinations, but they fail to offer insight into a patient’s exposure
across multiple modalities, departments, or sites. In addition, radiation
dose-relevant information presented
in Digital Imaging and Communications in Medicine (DICOM) can be
missing, inconsistent, or inappropriate. DIT is designed and implemented
to be fully integrated into a DICOMcompliant infrastructure with the
ability to capture and maintain longitudinal patient-specific exam indices
of interest for all diagnostic and procedural uses of imaging modalities.
It performs computation of a patient
dose, not just an occurrence of exposure. This information system consists of a DICOM receiver accepting
and parsing imaging files sent from
equipment or PACS, a knowledge
base (for example, modalities, manufacturers, and model and software
versions of known equipment) with
mappings of the standard and proprietary DICOM tags of interest, a patient centric imaging exam database,
57

7/21/14 11:07 AM

Dose index tracking

PACS (radiology
examination images)

Database layer:
Patient centric
imaging exam
database
DICOM
knowledge
database

Imaging API:
DICOM Receiver &
parser

Application layer:
Web reporting
Alerting mechanism
(pager, text, email)

Figure 1. A schematic view of Dose Index Tracking (DIT). This enterprise-wide
information system technology can automatically track and monitor cumulative
radiation doses for each patient and support real-time query, reporting, and alarm
notification if dose limit guidelines are exceeded. DICOM stands for Digital Imaging
and Communications in Medicine.

D

DICOM
D.4
D.3
D.1

PACS

ResolutionMD
enterpriseTM

D.2

Figure 2. ResolutionMD architecture to support interactive image viewing on mobile
devices. This figure is adapted from related work. 2

a configurable Web reporter, and an
automated alert/messaging mechanism (for high radiation exposure, for
example). Figure 1 shows the system
architecture of DIT, including the
composite modules and the connection with other software.

ResolutionMD: A Mobile App
for Imaging Accessibility
Mobile devices, like smartphones,
have become increasingly prevalent
in modern society. Many doctors now
carry one in their pocket. However,
mobile devices may lack the computational capabilities to perform advanced visualizations that can aid
the diagnostic process. Consequently,
our application c­ ommunicates with a
58		

IS-29-03-Trends.indd 58

server running ResolutionMD Enterprise Calgary Scientific Inc. (CSI), a
Web-based platform for medical image viewing and analysis. The ResolutionMD server performs all rendering
operations, and streams the resulting
images to our application over a secure HTTP connection. Our application, in turn, provides user interaction
events back to the server to control
the visualization (see Figure 2). With
this arrangement, workstation functionality such as interactive multiplanar reformatting and advanced 3D
visualization can be done and viewed
rapidly on a remote mobile device.
Of critical importance, private
health information is not stored on
the mobile device. When the user
www.computer.org/intelligent	

e­ xits the mobile application, or if the
application is left idle for more than
30 seconds, the connection to the
server is closed, the rendering operation ends, a blank image is displayed
on the mobile device, and the volatile
RAM on the device is cleared.

Imagenomics: Empowering
Personalized Radiology
Treatment
An individualized approach to medicine could potentially improve outcomes for a number of aggressive
cancers such as glioblastoma and
pancreatic cancer, by tailoring treatments to key genetic mutations that
are expressed by each patient’s tumor. Yet, intra-tumoral genetic heterogeneity can degrade the diagnostic accuracy of individualized care,
because genetic targets from one biopsy location can misrepresent other
tumoral regions that haven’t been biopsied. This can lead to inaccurate
diagnosis and suboptimal treatment.
Imagenomics research (see Figure 3)
aims to use imaging to characterize
inter- and intra-tumoral genomic diversity and inform more effective diagnosis and individualized treatment
for each patient.
There are several challenging issues
in imagenomics research from a data
analytics perspective. First, there are
multiple variation sources that must
be accounted for in identification of
imaging-genomics association, including patient-to-patient variation, within-­
tumor regional variation, and temporal variation across multiple treatment
episodes. Second, sample sizes for characterizing each variation source are
small. For example, patient records are
relatively ­limited. Biopsy sites on each
tumor are just a few, because biopsy is
invasive and poses risk or complication
to patients. Third, both imaging and
genomic data are high-dimensional,
adding to the difficulty of ­
statistical
IEEE INTELLIGENT SYSTEMS

7/21/14 11:07 AM

Matched pairs of MRI
& genomic data

modeling with multisource variation
and limited sample sizes. Fourth, modeling uncertainty for imagenomic association estimation will translate into
uncertainty in using the association to
predict un-biopsied tumoral regions,
and further translate into the uncertainty of treatment outcomes. This
chain of uncertainty propagation must
be clearly quantified and taken into
account when prescribing treatment
to patients. Finally, each patient is a
unique individual. Personalized treatment optimized for each individual
patient requires the data analytics approach also to consider non-imaging
data such as demographics, health history, and comorbid conditions.

To address these challenges, statisti-

cal methods, such as mixed-effect models and partial least squares, optimized
for the Big Data setting by modern
machine learning developments, such
as sparse learning4 and transfer learning,5 provide promising solutions. Radiologic domain knowledge integrated
with data-driven analytics methods
helps reduce the modeling complexity and also ensure the findings to have
clear clinical impact and utility.

Reference
	 1.	S. Wang et al., “An Automated DICOM
Database Capable of Arbitrary Data
Mining (Including Radiation Dose
Indicators) for Quality Monitoring,” J.
Digital Imaging, vol. 24, no. 2, 2011,
pp. 223–233.
	 2.	J.R. Mitchell et al., “A Smartphone
Client-Server Teleradiology System for
Primary Diagnosis of Acute Stroke,” J.
Medical Internet Research, vol. 13,
no. 2, 2011, pp. 1–12.
	 3.	B.M. Demaerschalk et al., “Smartphone Teleradiology Application is Successfully Incorporated into a Telestroke
Network Environment,” Stroke,
MAy/June 2014	

IS-29-03-Trends.indd 59

MRI-guided
stereotactic biopsy
Multi-parametric MRI

Machine learning
Computational
analysis

Prediction of regional
genomic status
Figure 3. An illustration of imagenomic research. The research aims to use imaging
to characterize inter- and intra-tumoral genomic diversity and inform more effective
diagnosis and individualized treatment for each patient.

vol. 43, no. 11, 2012; doi:10.1161/
STROKEAHA.112.669325.
	 4.	 S. Huang et al., “Learning Brain Connectivity of Alzheimer’s Disease by Sparse
Inverse Covariance Estimation,” NeuroImage, vol. 50, 2010, pp. 935–949.
	 5.	S. Huang et al., “A Transfer Learning
Approach for Network Modeling,” IIE
Trans., vol. 44, no. 11, 2012, pp. 1–17.

William Pavlicek is an associate professor in the Department of Radiology at the
Mayo Clinic Arizona. Contact him at pavlicek.william@mayo.edu.
Leland S. Hu is an assistant professor in the
Department of Radiology at the Mayo Clinic
Arizona. Contact him at hu.leland@mayo.edu.
Alvin C. Silva is an associate professor in the

Jing Li is an associate professor in the

School of Computing, Informatics, and Decision Systems Engineering at Arizona State
University. Contact her at jinglz@asu.edu.
Teresa Wu is a professor in the School of

Computing, Informatics, and Decision Systems Engineering at Arizona State University and an associate professor in the Department of Radiology at Mayo Clinic
Arizona. Contact her at teresa.wu@asu.edu.
J. Ross Mitchell is a professor in the Department of Radiology at the Mayo Clinic ­Arizona.
Contact him at mitchell.ross@mayo.edu.
Amy K. Hara is a professor in the Department of Radiology at the Mayo Clinic Arizona. Contact her at hara.amy@mayo.edu.
www.computer.org/intelligent	

Department of Radiology at the Mayo Clinic
Arizona. Contact him at silva.alvin@mayo.edu.
Christine M. Zwart is a scientist/program-

mer in the Department of Radiology at the
Mayo Clinic Arizona. Contact her at zwart.
christine@mayo.edu.

Opportunities for
Operations Research in
Medical Decision Making
Sait Tunc, Oguzhan Alagoz, and
Elizabeth Burnside, University of
Wisconsin–Madison

Medical decision making (MDM),
the discipline applying systematic
59

7/21/14 11:07 AM

approaches to solve the decision-­
making problems in healthcare, aims
to develop standards for ideal decision
making, to understand the motivation
behind the routine decisions of physicians and patients, and to provide effective tools for physicians, patients,
and healthcare policymakers for better
decision making. To this end, MDM
relies heavily on quantitative models.
Applications of MDM include decision problems in breast cancer diagnosis and treatment, disease modeling,
drug selection in HIV treatment, optimal timing of organ transplantation,
and optimizing radiotherapy treatment planning, among many others.

Why Is MDM Becoming
More Popular?
Recently, MDM and the use of quantitative models in MDM have attracted significant interest due to several factors. First, a dramatic rise in
healthcare expenditures demonstrated
the importance of cost-effective decision making in healthcare. As of
2012, health expenditures in the US
exceeded $2.5 trillion. Healthcare expenditures are expected to grow faster
than other segments of the GDP due
to developing technology, aging populations, and increasing access to care.
Second, developing a high-performance medical data collection infrastructure results in access to better
data; this in turn helps with effective quantitative modeling. We expect
that this trend will continue, especially with the exciting developments
in genomics.
Third, a high level of preventable
medical errors, which was the focus
of several national reports, showed
the importance of effective medical
decision making. For instance, according to the Institute of Medicine’s
1999 report,1 medical errors were a
leading cause of death in the United
States with almost 100,000 deaths
60		

IS-29-03-Trends.indd 60

each year. Medical errors also cost
the US approximately $37.6 billion
each year; about $17 billion of those
costs are ­
associated with preventable errors. Previous experience indicates that expensive, high-tech medical solutions may bring new kinds
of errors and efficiency problems if
evidence-based engineering methods
are not employed in their design and
implementation.
Finally, there is notable variability in
medical practice, which compromises
care, causes patient dissatisfaction, and
exacerbates existing inefficiencies. If
the variation in medical practice is in
response to clinically relevant patient
characteristics, this is acceptable; however, there’s strong evidence that these
variations are primarily due to variations in delivery of care without clinical rationale or benefit.2 All of these
factors suggest that MDM will become
even more important in the future.

How Could Operations
Research Be Useful?
Currently, healthcare providers often must rely on ad hoc and heuristic decision-making strategies, which
may fall short when making complex
screening/diagnostic/treatment
decisions that involve consideration of
many uncertain factors (for example,
the uncertainty of future outcomes
or long-term treatment effects). To
this end, operations research (OR),
the discipline utilizing advanced analytical methods to help make better
decisions, has found numerous applications in MDM. OR enables the
realistic modeling of complex MDM
problems that must balance the benefits as well as the unintended consequences of medical treatment.
In particular, there has been recent
interest in applying OR tools that are
used for sequential decision ­
making
under uncertainty, such as Markov decision processes (MDP), since medical
www.computer.org/intelligent	

decisions are often made sequentially
in highly stochastic e­nvironments.
The sequential nature of healthcare
problems arises because patients have
multiple opportunities to make decisions throughout their lifetimes, and
each decision depends on the situation
and the decisions made previously.
Uncertainty arises from each individual patient’s situation: for example,
their response to treatments (chemotherapy or antibiotics), access to limited resources (cadaveric organs for
transplantation), and behavior (compliance to medical recommendations).

Successful OR
Applications to MDM
Successful recent applications of OR
and particular MDPs to MDM suggest
that OR may provide powerful tools
for MDM and will become even more
popular in the future. Among these
successful applications, we briefly
summarize three studies from our research group that utilized MDPs.
Jagpreet Chhatwal and his colleagues studied when a patient undergoing screening mammography should
be sent for biopsy based on her mammographic features and demographic
risk factors using an MDP model.3
The authors found that optimal biopsy thresholds (that is, the probability of cancer value beyond which the
patient should be recommended a biopsy) should take the patient’s age
into account. This article proved analytically and demonstrated numerically that the probability threshold for
biopsy should be higher in an older
woman than a younger woman. This
work is a good example for how OR
can be used to develop clinical strategies and inform medical practitioners.
Turgay Ayer and his colleagues developed a personalized ­mammography
screening schedule utilizing the prior
screening history and personal risk
characteristics of women using a
IEEE INTELLIGENT SYSTEMS

7/21/14 11:07 AM

­artially observable MDP model.4
p
While the cancer community agrees
that a screening strategy tailored to
individual women’s risk is desirable,
no validated framework exists to implement such a personalized strategy.
To this end, the authors proposed the
first analytical model to individualize mammography screening. They
showed that their proposed personalized screening schedules outperform population-based screening recommendations that are currently in
use by improving the total expected
quality-adjusted life years (QALYs),
while decreasing the number of mammograms and false-positives. This
work serves as an example for how
OR methods can optimize the efficiency of complex medical decisions
in highly uncertain environments.
Mehmet Ayvaci and his colleagues
investigated the problem of optimizing diagnostic decisions after mammography under resource constraints
using a constrained MDP model.5
They found that using optimal thresholds for decision making instead of
the traditional methods in clinical
practice might lead to savings of approximately 22 percent in overall
cost, while maintaining the same level
of total expected QALYs. The authors
conducted an incremental cost-effectiveness analysis, which enables determining optimal diagnostic actions
under budget constraints and/or optimizing the resource distribution given
the patient subgroups. Their work illustrates an effective method to maximize health-related objectives while
simultaneously observing economic
constraints. This work is important,
especially for resource-constrained
settings such as developing countries.

A

lthough research on applying
OR tools in MDM is at a relatively
MAy/June 2014	

IS-29-03-Trends.indd 61

early stage, there are several promising studies in the literature and
many potential decision-making problems. The successful application of
OR in MDM often involves several
themes. Selection of a problem for
which a solution would have a substantial impact is a better approach
than selecting a problem that’s complex but without real-world importance. Selecting a good problem can
be achieved by working with medical professionals and healthcare researchers who are actual decision
makers with domain knowledge. For
example, health professionals have
been closely involved in all three of
the aforementioned studies. Furthermore, working with real clinical data
provides compelling evidence that a
solution derived through OR techniques will have value outside of the
lab. In short, the development of collaborations between operations researchers and physicians or healthcare researchers is an invaluable
investment that will reap rewards in
both research and application.
There are several emerging research problems related to MDM
that may be of interest to operations
researchers interested in healthcare
applications. Brian T. Denton and
his colleagues 6 list the following future research directions (among others): personalization of screening and
treatment, quantitative modeling of
patient behavior, and optimal communication within multidisciplinary
care models. Personalized medicine,
which aims to determine the patientspecific assignment of healthcare solutions for individuals using each
person’s unique clinical, genetic,
genomic, and environmental infor­
mation is a research direction well
suited to OR, since the use of personalized medicine will substantially
complicate the treatment process.
­
Another potential problem noted by
www.computer.org/intelligent	

Denton and his colleagues is developing quantitative models to accurately characterize patient behavior,
which has a substantial influence on
treatment success. An accurate characterization of patient behavior using OR methods will result in better understanding of its effects in
MDM. Finally, healthcare in general is becoming more “patient-centered,” requiring physicians to work
together in multidisciplinary teams.
As a result, the intercommunication
between different disciplines is becoming an important issue in MDM.
While optimizing communication
between diverse practitioners is a
complex and challenging problem,
OR researchers are well-suited to address it because OR has the tools to
solve large-scale computational models that are often required in such
problems.

Acknowledgments

This study is funded in part by grant
CMMI-0844423 from the US National
Science Foundation as well as grants
R01CA165229 & R01LM010921 from the
US National Institutes of Health.

References
	 1.	Institute of Medicine, To Err is Human:
Building a Safer Health System,
L. Kohn, J. Corrigan and M.E. Donaldson, eds., Nat’l Academy Press, 2000;
www.nap.edu/catalog.php?record_
id=9728.
	 2.	A.J. Schaefer et al., “Modeling Medical
Treatment Using Markov Decision,”
Handbook of Operations Research/
Management, 2004. pp. 597–616.
	 3.	J. Chhatwal, O. Alagoz and E. S. Burnside, “Optimal Breast Biopsy DecisionMaking Based on Mammographic
Features and Demographic Factors,”
Operations Research, vol. 58, no. 6,
2010, pp. 1577–1591.
	 4.	T. Ayer, O. Alagoz and N. K. Stout,
“A POMDP Approach to Personalize
61

7/21/14 11:07 AM

Mammography Screening Decisions,”
Operations Research, vol. 60, no. 5,
2012, pp. 1019–1034.
	 5.	M.U.S. Ayvaci, O. Alagoz, and E.S.
Burnside, “The Effect of Budgetary Restrictions on Breast Cancer Diagnostic
Decisions,” Manufacturing & Service
Operations Management, vol. 14, no.
4, 2012, p. 600–617.
	 6.	B.T. Denton et al., “Medical Decision
Making: Open Research Challenges,”
IIE Trans. Healthcare Systems Eng.,
vol. 1, no. 3, 2011, pp. 161–167.
Sait Tunc is a PhD student in the Depart-

ment of Industrial and Systems Engineering
at the University of Wisconsin–Madison.
Contact him at stunc@wisc.edu.
Oguzhan Alagoz is an Associate Professor

of Industrial and Systems Engineering and
Population Health Sciences at the University of Wisconsin–Madison. Contact him at
alagoz@engr.wisc.edu.
Elizabeth Burnside, MD, MPH is an As-

sociate Professor of Radiology at the University of Wisconsin–Madison School of
Medicine and Public Health. Contact her at
Eburnside@uwhealth.org.

Diagnostic Network
Modeling of Neural
Connectivity Using
Functional Magnetic
Resonance Imaging
W. Art Chaovalitwongse, Georgiy
Presnyakov, Yulian Cao, Sirirat
Sujitnapitsatham, Daehan Won, Tara
Madhyastha, Kurt E. Weaver, Paul R.
Borghesani, and Thomas J. Grabowski,
University of Washington, Seattle

A variety of brain-imaging data are
routinely collected from human subjects in the hope that such data ­contain
meaningful information and will lead
to a greater understanding of the brain
62		

IS-29-03-Trends.indd 62

and ultimately more accurate diagnosis of neurological and psychiatric diseases. Functional magnetic resonance
imaging (fMRI) is a non-invasive neuroimaging tool that has been used in
both clinical and scientific settings to
investigate brain function. Over the
past decade, fMRI has emerged as an
approach to study the large-scale connectivity underlying brain diseases and
to investigate how the diseases alter
and/or disrupt brain function.

Network Modeling of
Functional Connectivity
MRI (fcMRI)
Functional connectivity MRI (fcMRI)
is an emerging approach in fMRI
data analysis used to capture statistical dependencies of temporally correlated neurophysiological events.
These dependencies are then mapped
as patterns of connectivity across and
between functional networks.1 Specifically, the underlying concept of
fcMRI is to quantify intrinsic connectivity in the brain through correlations of activity, and in turn to link
the connectivity patterns to information about organizational properties
of neural systems. Recently, fcMRI
has been widely applied to fMRI data
collected during task-evoked “activation” and during the unengaged, taskfree resting state in order to assess
clinical disorders and cognitive behaviors. Resting-state fMRI (rs-fMRI) in
particular has emerged as a source of
information about systems-level brain
function not obtainable through other
modalities. Currently, rs-fMRI is
more readily available in clinical settings because it doesn’t require patients to perform explicit tasks that
may be difficult to implement.
Functional Localization:
Defining Functional Nodes
To define functional regions (nodes),
there are four general methods that
www.computer.org/intelligent	

have become standard in rs-fMRI
data analysis. The first method is
the seed-based approach, in which
the time-varying fMRI signal within
a specific, spatially defined voxel (or
region) is correlated with the time
series from voxels across the entire brain. The second method employs independent component analysis (ICA), a statistical approach that
can decompose rs-fMRI data into
non-overlapping spatial components.
This approach is highly data driven
and doesn’t require a priori guidance.
The third method is the anatomical
parcellation approach, in which the
brain is segmented into established,
known regions of interest (ROIs),
typically based on standard anatomical atlases. The fourth and most recent method is the functional parcellation approach, which defines 264
ROIs as small spheres centered upon
ROI coordinates.2
Functional Connectivity:
­Inter-Nodal versus IntraNodal Connectivity
Neuronal activity is tightly coupled to
cerebral blood flow. The blood-oxygen
level dependent (BOLD) signal can be
used to monitor changes in blood flow
in fMRI. Correlated BOLD fluctuations of neighboring voxels likely represent coordinated neural activity—for
example, that the regions are working
together either directly or indirectly.
Most fcMRI studies focus on internodal connectivity, investigating the
interactions of multiple nodes within
large-scale neural systems. Inter-nodal
connectivity is defined by a correlated
BOLD signal (that is, synchronized
fluctuations) between spatially distinct
nodes. An individual node is usually
comprised of multiple voxels and the
time averaged fMRI signal across all
the voxels with the node is used to determine inter-nodal c­ onnectivity. Thus,
current fcMRI research is focused on
IEEE INTELLIGENT SYSTEMS

7/21/14 11:07 AM

large-scale inter-nodal connectivity
and has often minimized or ignored
the importance of intra-nodal local
connectivity. To understand how the
assignment and connectivity of local
regions (nodes) impact the assessment
of inter-nodal connectivity, researchers have recently started to investigate
small-scale connectivity specific to the
local environment of functional networks. Regional Homogeneity (ReHo)
is an approach that focuses on analysis
of local connectivity mapping correlations restricted to a finite set of voxels
within a region of interest. Intra-nodal
connectivity (IRC) was recently developed to model connectivity within
each ROI as a full graph and measure
an average of all pair-wise correlation
between BOLD time courses of all
possible node pairs.3,4

Asymmetrically Disrupted
Local Connectivity in
Epilepsy
About 25 percent of patients with
temporal-lobe epilepsy have seizures
that don’t respond to commonly
used drug treatments. For these patients with refractory epilepsy, temporal lobe resection has emerged as the
standard care of treatment. This surgical procedure removes the epileptogenic zone, which is a region that
covers the seizure focus (that is, the
area that’s thought to originate seizures) and surrounding tissue. Surgical candidates usually undergo extensive pre-operative assessments that
involve multimodality tests and invasive procedures to localize the seizure
focus and identify the resection target. However, current standard procedures significantly increase risk, morbidity, and healthcare cost, and may
also yield conflicting results. fcMRI is
a non-invasive neuroimaging tool that
could provide a distinct advantage in
mapping the epileptic network and
identify the epileptogenic zone.
MAy/June 2014	

IS-29-03-Trends.indd 63

Previous investigations of fcMRI
in epilepsy predominantly focused on
seed-based analysis of brain connectivity with some degree of success.
Most studies carried out retrospective
analyses to identify specific markers
(such as alterations in functional connectivity) around epileptogenic areas.
Our group has undertaken prospective analysis of seizure focus localization.4 Based on the IRC measure that
we’ve developed, the concept of asymmetry of local functional connectivity
was investigated to capture anomalous patterns that are relevant to dysfunctional alterations in epilepsy and
in turn identify the epileptogenic zone.
The standard 96 HO ROIs (48 in each
hemisphere) were used to define our
functional nodes. The IRC within
each of the 96 nodes was then quantified and subsequently contrasted with
its contralateral node. A ratio of IRC
in the left hemisphere ROI to the connectivity in the right hemisphere ROI
was proposed. If the ratio is close to
1, the brain’s functional connectivity is more symmetric, and vice versa.
Asymmetry in local connectivity between hemispheres was observed, especially the IRC within hippocampi.
In addition, we also observed functional asymmetries in other regions,
which may result from more impaired
function ipsilateral to the seizure focus. The results of our study suggest
that impaired neuronal connectivity of
the seizure focus may lead to an asymmetry of local connectivity between
the two hemispheres, and provide a
biomarker of the epileptogenic zone.
The approach may further ­
provide
evidence of comorbid dysfunction in
other functional networks.

Lost Connectivity in
Alzheimer’s Disease
Several studies in the literature suggest that rs-fMRI may be used to
characterize neurodegenerative brain
www.computer.org/intelligent	

diseases such as Alzheimer’s disease
(AD). As discussed, large-scale networks identified in rs-fMRI data are
composed of multiple nodes. Cognitive decline in AD may result from
the reduced ability of networks to reconfigure themselves; that is, the reduced efficiency of individual nodes
to switch between different functional networks. In a complete picture of the system-level network dynamics, each functional node should
belong to multiple large-scale networks, reflecting its multiple roles
in information processing. As metabolic activity and network connectivity are disrupted by cognitive decline
and dementia, we expect the dynamics of these systems to be similarly altered. Functional neuroimaging may
provide an earlier physiologic biomarker of these alterations than other
modalities.
Our group investigated dynamics of
brain connectivity structures in AD using a recently developed mathematical framework for analysis of community structure in graphs (called link
communities).5 The main principle of
a link community is the clustering of
the links between nodes, rather than
the nodes themselves. Consequently,
nodes may belong to multiple communities, accommodating and quantifying the overlapping roles of key
network nodes. As described by YongYeol Ahn and his colleagues, links
that share a common node (keystone
node) are assigned a similarity measure (the Jaccard index) based on their
topological connections—the number of neighbors that they have in
common divided by the total number
of neighbors, excluding connections
to the keystone node. The weighted
graph extension to this measure uses
the Tanimoto index. This edge similarity matrix is clustered using singlelinkage hierarchical ­clustering to find
community structures, merging all
63

7/21/14 11:07 AM

p-value

0.047

0.005

(a)

(b)

(c)

Figure 4. (a) Link community structure of the AD population. (b) Link community structure of the control population. (c) Regions
with significantly (p < 0.05) different average connectivity between the cognitive decline group and the control group.

e­ lements until all links are members of
a single cluster. An objective function
called the partition density,5 based on
the link density within communities,
is maximized to determine the optimal number of clusters. The higher the
partition density, the more “cliquish”
the link communities, and the lower
the partition density, the more “treelike.” This algorithm has been implemented as an R package linkcomm.6
We used this link community method
to extract communities in rs-fMRI data
from both AD patients and normal
controls. A total of 264 nodes (ROIs)
were defined by using small spheres
centered upon the coordinates of putative areas.2 The link weights between
these ROIs were quantified using a
Pearson correlation between averaged
BOLD time courses of voxels within
individual ROIs, and the links with
weights smaller than a predetermined
threshold were subsequently removed.
Our preliminary results indicate that
the number of communities in the AD
group is less than that in the control
group. In addition, it was found that
the AD group lost long-range connectivity between the posterior cingulate
cortex (PCC) area and the frontal area
(see Figures 4a and 4b).

Reduced Connectivity in
Cognitive Decline
Increasing evidence suggests that reduced functional connectivity of
64		

IS-29-03-Trends.indd 64

large-scale networks as defined by rsfMRI is associated with loss of executive function and reductions in overall cognitive processing speed. Several
studies including ours (as previously
discussed) have found decreased efficiency and strength of frontal lobe
networks with other regions that are
related to the disease onset and severity. fcMRI may be used to quantify the integrity of a given brain, for
example, by examining how fcMRI
measures deviate from norms in elderly subjects without a decline. In
the literature, there are divergent findings that large-scale connectivity may
increase or decrease with aging, depending on the functional networks
assessed. Our group has therefore undertaken an investigation of the association between local connectivity and executive function decline in
healthy aging.3 We used the standard
96 Harvard-Oxford Atlas ROIs to define nodes, and the IRC for each of the
96 nodes was quantified by calculating an average absolute correlation
coefficient of all voxel-pairs within
the node. Differences in the IRC values of each ROI between the two test
populations (normal controls versus
subjects with cognitive decline) were
assessed by simple non-parametric
­t-tests. Results of the tests’ p-values,
shown in Figure 4c, indicate that a decrease in the local connectivity within
the temporal lobe regions is quite
www.computer.org/intelligent	

s­ignificant in subjects with cognitive
decline. This finding is also supported
by the fluorine-18 fluorodeoxyglucose
positron emission tomography (FDGPET) literature, which observes hypometabolism in temporal/parietal lobes
in the early stages of degenerative
cognitive impairment. Based on the
results of our study, we propose that
altered strength of local connectivity
may provide an early descriptive biomarker to distinguish normal aging
from pathological cognitive decline.

N

etwork modeling has become an
intriguing approach to map patterns
of brain connectivity, especially in
rs-fMRI data. Two main, yet challenging, steps of network modeling
are to identify functional nodes and
to quantify functional connectivity. The resulting brain connectivity model has to be interpretable in
neurophysiological terms, and the
­
network components must be reliable, reproducible, and robust across
individuals in the testing population.
This article presents our experience
in applying network modeling approaches to explore the brain’s functional organization and to examine
whether alterations functional connectivity in neurological diseases can
be used to improve diagnosis and answer other clinical questions.
IEEE INTELLIGENT SYSTEMS

7/21/14 11:07 AM

Acknowledgments

This work was supported in part by US National Science Foundation grants 1219638
and 1219639, and by the Alzheimer’s Disease Neuroimaging Initiative (ADNI) with
National Institutes of Health Grant U01
AG024904. We thank Jeff Ojemann, Edward Novotny, Andrew Poliakov, and
Sherry Willis for their research support.
Data used in preparation of Figures 4a and
4b were obtained from the ADNI database.

of ­Washington, S­ eattle. Contact him at
­a rtchao@uw.edu.
Georgiy Presnyakov is a graduate student

in the Department of Industrial and Systems
Engineering and a research assistant in IBIC
at the University of Washington, Seattle.
Contact him at presnyak@uw.edu.
Yulian Cao is a PhD student in the Depart-

References
	 1.	K.J. Friston, “Functional and Effective Connectivity in Neuroimaging: A
Synthesis,” Human Brain Mapping, vol.
2, nos. 1–2, 1994, pp. 56–78.
	 2.	J.D. Power et al., “Functional Network
Organization of the Human Brain,”
Neuron, vol. 72, no. 4, 2011, pp.
665–678.
	 3.	P. Borghesani et al., “Network
Modeling Approaches to Investigate
Intranodal Connectivity within the
Default Mode Network during Aging,”
Proc. Third Biennial Conf. Resting
State Brain Connectivity, 2012; www.
conventus.de/fileadmin/media/2012/
resting/abstracts/86_Abstract.pdf.
	 4.	K.E. Weaver et al., “Local Functional
Connectivity as a Pre-Surgical Tool
for Seizure Focus Identification in
Non-Lesion, Focal Epilepsy,” Frontal
Neurology, vol. 4, no. 19, 2013, p. 43;
doi:10.3389/fneur.2013.00043.
	 5.	 Y.-Y. Ahn, J.P. Bagrow, and S. Lehmann,
“Link Communities Reveal Multiscale
Complexity in Networks,” Nature, vol.
466, Aug. 2010, pp. 761–764.
	 6.	A.T. Kalinka and P. Tomancak, “Linkcomm: An R Package for the Generation, Visualization, and Analysis of
Link Communities in Networks of Arbitrary Size and Type,” Bioinformatics,
vol. 27, no. 14, 2011, pp. 2011–2012.

ment of Logistic Engineering at Wuhan University of Technology, China and a visiting
PhD student in the Department of Industrial
and Systems Engineering and IBIC at the
University of Washington, Seattle. Contact
her at yulian@uw.edu.
Sirirat Sujitnapitsatham is a graduate

student in the Department of Industrial and
Systems Engineering and a research assistant in IBIC at the University of Washington, Seattle. Contact her at sirirat@uw.edu.
Daehan Won is a PhD student in the De-

partment of Industrial and Systems Engineering and a research assistant in IBIC at
the University of Washington, Seattle. Contact him at wondae@uw.edu.
Tara Madhyastha is an acting assistant
professor in the Department of Radiology
and a core member of IBIC at the University of Washington, Seattle. Contact her at
­madhyt@uw.edu.
Kurt E. Weaver is an assistant professor in the Department of Radiology and
a core member of IBIC at the University
of ­
Washington, Seattle. Contact him at
weaver@uw.edu.
Paul R. Borghesani is an assistant pro-

W. Art Chaovalitwongse is a profes-

fessor in the Department of Psychiatry and
Behavioral Sciences and a core member of
IBIC at the University of Washington, Seattle. Contact him at paulrb@uw.edu.

sor of industrial and systems engineering and r­adiology (jointly), and the associate director of the Integrated Brain
Imaging Center (IBIC) at the University

Thomas J. Grabowski is a professor of radiology and neurology (jointly)
and the director of IBIC at the University

MAy/June 2014	

IS-29-03-Trends.indd 65

www.computer.org/intelligent	

of ­
Washington, Seattle. Contact him at
tgrabow@uw.edu.

Spatial Clustering in
Public Health: Advances
and Challenges
Lianjie Shu, University of Macao
Man Ho Ling, Hong Kong Institute of
Education
Shui-Yee Wong and Kwok-Leung Tsui,
City University of Hong Kong

Public health surveillance aimed at
time detection and prevention of various types of adverse health events
has received much attention recently.
Identifying and localizing an outbreak of adverse events in its early
stages can be crucial for the sake of
public health, as it’s beneficial to both
individuals and society in terms of
the reduction of risk, as well as medical expenditures. Therefore, a reliable
public health surveillance system is
greatly needed.1
A multitude of efforts have been
devoted to the temporal surveillance
of adverse health events, aimed at
quickly detecting when the incidence
rate of an adverse health event has
increased. However, health data are
often collected across different geographic locations. It’s also important
to take spatial information into account in public health surveillance,
because environmental factors could
be important etiological agents in
diseases. Being able to study and pinpoint regions with higher risk levels
is also helpful, and this has motivated
the recent study of spatial clustering
of disease data.

Recent Advances
in Spatial Clustering
In a public health setting, it’s often of
interest to identify the disease clusters
in a geographical region with two
65

7/21/14 11:07 AM

main tasks: to test whether there are
any anomalous patterns in the data
in the early stages, and to pinpoint
which subsets of data records have
been affected by those abnormal patterns. The tests for spatial clustering
of disease can be generally grouped
into the following categories: the spatial scan, graph-based, and false discovery rate-based methods. In the following, we review the basic principles
and properties of these tests.
Spatial Scan Method
The standard spatial scan statistic,
based on likelihood, is conceptually
intuitive. It has become one of the
most widely used tools for testing
spatial clusters of disease, which is
now available in a freely downloadable software package, SaTScan. The
goal of this method is to search over
a given set of spatial regions and find
if there’s any spatial region for which
the counts are significantly higher
than expected. Randomization testing is used to determine the significance of the most likely cluster. The
original spatial scan method detects a
single cluster only. To adjust the standard scan statistics for testing multiple clusters over the study region, a
sequential method 2 was recently proposed to evaluate the significance of
secondary clusters.
In spatial clustering by using the
spatial scan statistics, we must prespecify the scanning windows across
the whole study region. The number
of possible subsets increases exponentially, and the exhaustive search
over all subsets is therefore computationally infeasible. Motivated by this,
Daniel B. Neill3 proposed a fast subset scan approach for computationally
efficient detection of spatial clusters.
This method can greatly reduce the
computation load of the standard spatial scan statistic, and also can automatically and ­simultaneously ­identify
66		

IS-29-03-Trends.indd 66

the locations of multiple clusters with
arbitrary shapes, as compared to the
standard spatial scan statistic and its
variants. In this sense, this method is
a general approach to the detection of
spatial clusters, which represents an
important extension to the standard
spatial scan method.
In practice, the incidence rate is often unknown. In the standard spatial
scan statistics, the overall incidence
rate and the incidence rates inside
and outside the potential cluster are
estimated by using their maximum
likelihood estimates. Therefore, the
misspecification of model parameters
may deteriorate the detection power
of the spatial scan statistics. To remedy this problem, the Bayesian spatial
scan statistic can be used. The Bayesian method uses a marginal likelihood approach, making the model
more flexible and reducing the potential effect of model misspecification.
Graph-Based Clustering Methods
The standard and Bayesian spatial
scan methods are computationally intensive due to an exhaustive search of
the potential clusters on various locations within a study region. A great
deal of attention has been devoted to
the design of more effective clustering
algorithms in the recent decades. Due
to its effective data representation,
the graph-based clustering method
has become another dominant approach to spatial data analysis.
The simplest graph-based clustering method is perhaps the K-means.
However, it requires users to provide a number of clusters that’s usually unknown in advance. Among
various graph-based clustering methods, minimum spanning tree (MST)
has been extensively discussed in a
wide variety of settings, not limited
to public health. Cluster design using
MST is to first construct MST over
the given dataset and then r­ emove the
www.computer.org/intelligent	

i­ nconsistent edges to create connected
components. Many different algorithms have been proposed to remove
the inconsistent edges. A sample of
these algorithms includes removing
the longest MST-edges, maximizing/
minimizing the degree of the vertices,
maximizing the overall standard deviation, maximizing the coefficient of
variation, using the divide and conquer algorithm, and so on. In general, these algorithms fall into three
categories: density-based and local
search algorithms, hierarchical algorithms, and optimization-based algorithms. To improve the identification accuracy, a constrained MST
algorithm has been recently proposed
for detecting clusters of any size and
shape.4
False Discovery Rate-Based
Clustering Method
Aimed at automatically and simultaneously identifying the locations
of multiple clusters with arbitrary
shapes, another feasible solution
consists of using a false discovery
rate (FDR) to adjust the scan statistics. FDR is a criterion for controlling a Type I error while simultaneously testing multiple hypotheses.
Many authors have contributed to
its advancement and theoretical development, and it’s expected to have
promising potentials for testing spatial clusters. One recent application
of FDR is to test for the presence of
spatial signals in functional magnetic
resonance imaging data.5 By taking
advantage of the increased signalto-noise ratio from an aggregation
of locations within a cluster, we can
expect a higher detection power by
controlling the FDR on clusters over
a single location analysis.
For spatial scanning statistics, due
to local dependence caused by overlapping scanning windows, clusters
of neighboring hypotheses are likely
IEEE INTELLIGENT SYSTEMS

7/21/14 11:07 AM

to be rejected together. In such situations, a modified definition of FDR is
required, in which all the neighboring
rejections were grouped together and
counted as a single discovery. Then
FDR can be generally defined as the
proportion of clusters that are falsely
declared among all declared clusters.
The rejected regions are treated as
clusters. It’s not conceptually difficult to extend the idea of FDR to test
for the presence of disease clusters in
public health.

Challenges in Spatial
Clustering for Public
Health Problems
The ultimate aim of clustering approaches is to automatically and
simultaneously identify the locations of multiple clusters with arbitrary shapes. It’s important to point
out that there’s no versatile method
that can handle all types of clustering problems. For different types of
clustering approaches, each has its
own advantages and disadvantages.
Some main challenges for spatial
clustering of diseases are identified
as follows.

For example, to detect an ­
outbreak
of influenza, healthcare facilities
collect health-related data of multiple sources continuously. However,
current clustering approaches mainly
focus on detecting clusters of a single data stream. They generally can’t
integrate information from multiple
types of data because of the likelihood
function’s complexity. Integrating the
information from multiple steams of
data in spatial clustering isn’t an easy
task. Although the Bayesian s­patial
scan statistic—enabling sophisticated
models to ­
describe the relationship
among the observed events from multiple sources, is a feasible solution—
it’s often complex and requires the
selection of prior information of unknown ­
parameters. More efficient
search algorithms are expected in the
future.

Robust Clustering Algorithms in
the Presence of Model Uncertainty
Most popular methods for testing spatial clusters of diseases assume that the population size at risk
is known. In practice, the population size at risk often needs to be estimated. The estimation error would
cause model misspecification, which
in turn would affect the detection
performance of clustering algorithms.
How to develop robust algorithms in
the presence of model uncertainty is
challenging.

Spatiotemporal Clustering
Spatial and temporal surveillance focuses on the analysis of data based on
either the space or time dimension. In
practice, we’ve data collected at different locations at regular time intervals. Spatiotemporal clustering deals
with data based on their spatial and
temporal similarity. However, the
current research on spatiotemporal
clustering is in a nascent stage. Several strong assumptions are often
made: the population size at risk in
each location remains the same over
time; the shift in the incidence rate is
a step shift that stays at the new level
since its occurrence; and the locations
are independent. In practice, these assumptions are difficult to meet, and
developing more flexible spatiotemporal clustering algorithms is greatly
needed.

Multivariate Spatial Clustering
It’s also common that public health
surveillance applications are characterized by multiple streams of data.

Sensitivity and Accuracy
In spatially clustering diseases, most
of the current research focuses on
solutions to test for the presence of

MAy/June 2014	

IS-29-03-Trends.indd 67

www.computer.org/intelligent	

spatial signals, but underestimates
the accuracy of pinpointing the
­location of clusters. It’s very likely
that the estimated clusters may not
be exactly the same as the true clusters. As you might expect, the spatial scan statistics using a circular
window can’t identify an elongated
cluster very well. Therefore, the spatial scan test could have very poor
identification accuracy even if it has
high detection capabilities. This
clearly illustrates that identification
accuracy is another important metric in the performance assessment of
clustering algorithms. There are several possible ways to measure clustering algorithms’ identification accuracy. Up to this point, there’s been
no unique standard for researchers to use as a metric for measuring
identification accuracy. In practice,
either one could be used in order
to provide a complete performance
analysis. It’s possible that using different measures of performance
could lead to different conclusions.
Much more work on the analysis of
identification accuracy is needed in
the future.

Acknowledgments

Lianjie Shu’s research was supported by the
University Research Fund, granted by the University Research Committee (Ref. MYRG090FBA13-SLJ), and the Research Fund, granted
by Macau Science and Technology Development Fund (Ref. FDCT/002/2013/A). KwokLeung Tsui’s work was supported by the
­Research Fund for the Control of Infectious
Diseases, granted by the Food and Health
Bureau, HKSAR (Ref. 11101262) and the
Collaborative Research Fund, granted by
the Research Grants Council (Ref. CityU8/
CRF/12G).

References
	 1.	K.L. Tsui et al., “Recent Research
and Developments in Temporal and
Spatiotemporal Surveillance for Public
Health”, IEEE Trans. Reliability, vol.
60, no. 1, 2011, pp. 49–58.
67

7/21/14 11:07 AM

	 2.	Z. Zhang et al., “Spatial Scan Statistics Adjusted for Multiple Clusters,” J.
Probability and Statistics, 2010; http://
dx.doi.org/10.1155/2010/642379.
	 3.	D.B. Neill, “Fast Subset Scan for Spatial
Pattern Detection,” J. Royal Statistical
Soc.: Series B (Statistical Methodology),
vol. 74, no. 2, 2012, pp. 337–360.
	 4.	M.A. Costa et al., “Constrained Spanning Tree Algorithms for IrregularlyShaped Spatial Clustering,” Computational Statistics & Data Analysis, vol.
56, no. 6, 2012, pp. 1771–1783.
	 5.	Y. Benjamini and R. Heller, “False
Discovery Rates for Spatial Signals,” J.

American Statistical Assoc., vol. 102,
no. 480, 2007, pp. 1272–1281.
Lianjie Shu is an associate professor of decision science in the Faculty of Business Administration at the University of Macau.
Contact him at ljshu@mac.mo.

at City University of Hong Kong. Contact
her at zoiewong@cityu.edu.hk.
Kwok-Leung Tsui is the chair professor

and head of the Department of Systems
­Engineering and Engineering Management
at City University of Hong Kong. Contact
him at kltsui@cityu.edu.hk.

Man Ho Ling is a lecturer in the Department

of Mathematics and Information ­Technology
at the Hong Kong Institute of Education.
Contact him at amhling@ied.edu.hk.
Shui-Yee Wong is a scientific officer in the

Center for Systems Informatics Engineering

Selected CS articles and columns
are also available for free at
http://ComputingNow.computer.org.

ADVERTISER INFORMATION
Advertising Personnel

Southwest, California:
Mike Hughes
Email: mikehughes@computer.org
Phone: +1 805 529 6790

Marian Anderson: Sr. Advertising Coordinator
Email: manderson@computer.org
Phone: +1 714 816 2139 | Fax: +1 714 821 4010
Sandy Brown: Sr. Business Development Mgr.
Email sbrown@computer.org
Phone: +1 714 816 2144 | Fax: +1 714 821 4010
Advertising Sales Representatives (display)

Advertising Sales Representatives (Classified Line)

Central, Northwest, Far East:
Eric Kincaid
Email: e.kincaid@computer.org
Phone: +1 214 673 3742
Fax: +1 888 886 8599

Heather Buonadies
Email: h.buonadies@computer.org
Phone: +1 973 304 4123
Fax: +1 973 585 7071

Northeast, Midwest, Europe, Middle East:
Ann & David Schissler
Email: a.schissler@computer.org, d.schissler@computer.org
Phone: +1 508 394 4026
Fax: +1 508 394 1707

68		

IS-29-03-Trends.indd 68

Southeast:
Heather Buonadies
Email: h.buonadies@computer.org
Phone: +1 973 304 4123
Fax: +1 973 585 7071

Advertising Sales Representatives (Jobs Board)

Heather Buonadies
Email: h.buonadies@computer.org
Phone: +1 973 304 4123
Fax: +1 973 585 7071

www.computer.org/intelligent	

IEEE INTELLIGENT SYSTEMS

7/21/14 11:07 AM

European Journal of Operational Research 238 (2014) 270–280

Contents lists available at ScienceDirect

European Journal of Operational Research
journal homepage: www.elsevier.com/locate/ejor

Decision Support

An intelligent decomposition of pairwise comparison matrices
for large-scale decisions
Eugene Rex Jalao b,⇑, Teresa Wu a, Dan Shunk a
a
b

School of Computing, Informatics and Decision Systems Engineering, Arizona State University, 699 S. Mill Ave., Tempe, AZ 85281, United States
Department of Industrial Engineering and Operations Research, University of the Philippines, Diliman 1101, Philippines

a r t i c l e

i n f o

Article history:
Received 22 February 2013
Accepted 21 March 2014
Available online 2 April 2014
Keywords:
AHP
ANP
Pairwise comparison matrices
Inconsistency
Binary integer programming

a b s t r a c t
A Pairwise Comparison Matrix (PCM) has been used to compute for relative priorities of elements and are
integral components in widely applied decision making tools: the Analytic Hierarchy Process (AHP) and
its generalized form, the Analytic Network Process (ANP). However, PCMs suffer from several issues limiting their applications to large-scale decision problems. These limitations can be attributed to the curse
of dimensionality, that is, a large number of pairwise comparisons need to be elicited from a decision
maker. This issue results to inconsistent preferences due to the limited cognitive powers of decision makers. To address these limitations, this research proposes a PCM decomposition methodology that reduces
the elicited pairwise comparisons. A binary integer program is proposed to intelligently decompose a
PCM into several smaller subsets using interdependence scores among elements. Since the subsets are
disjoint, the most independent pivot element is identiﬁed to connect all subsets to derive the global
weights of the elements from the original PCM. As a result, the number of pairwise comparison is reduced
and consistency is of the comparisons is improved. The proposed decomposition methodology is applied
to both AHP and ANP to demonstrate its advantages.
Published by Elsevier B.V.

1. Introduction
An m  m pairwise comparison matrix (PCM) denoted by A is a
reciprocal matrix which is composed of pairwise comparisons
aij 2 ½1=9; 9 that represent the scaled relative importance scores
of element i as compared to element j. These elements could be criteria or alternatives within a PCM. Typically, a PCM is generated
from pairwise comparisons elicited from a decision maker (DM)
to estimate element priorities for any decision problem. One of
the most widely used multiple criteria decision making (MCDM)
methodology that use PCMs is the Analytic Hierarchy Process
(AHP) developed by Saaty (1977). Currently, there are several successful applications of the AHP in a wide-range of MCDM problems
(Ishizaka & Labib, 2011). Conversely, the AHP fails to account for
the interdependencies of the criteria and alternatives, and hence
it assumes that all criteria and alternatives are independent. If left
unchecked, any DM using the AHP would then provide inaccurate
decisions. To address this issue, the Analytic Network Process
⇑ Corresponding author. Contact Address: Melchor Hall Room 402, Department of
Industrial Engineering and Operations Research, College of Engineering, University
of the Philippines Diliman, Quezon City 1101, Philippines. Tel.: +632 981 8500 loc
3128.
E-mail address: eljalao@up.edu.ph (E.R. Jalao).
http://dx.doi.org/10.1016/j.ejor.2014.03.032
0377-2217/Published by Elsevier B.V.

(ANP) has been developed by Saaty and Takizawa (1986) as a generalization of the AHP. The ANP requires additional pairwise comparisons to estimate the inner and outer dependencies of the
criteria and alternatives. Although it addresses the limitations of
the AHP, the ANP still use PCMs which are faced with the following
issues: (1) numerous pairwise comparisons elicited from the DM
are required for the ANP to work and (2) inconsistent pairwise
comparisons are obtained when numerous pairwise comparisons
are elicited from the DM are large.
The ﬁrst limitation is attributed to the fact that both methodologies suffer from the curse of dimensionality. Consider a PCM of m
criteria. A total of mðm  1Þ=2 pairwise comparisons are needed to
obtain the priorities. Additionally, for the ANP, m2 comparisons are
needed to estimate the inner dependencies of the criteria. This
would be impractical when m is large. Saaty (1977) argues that
the redundancy of the questioning process provides weights which
are much less sensitive to biases and judgement errors. In a case
study by Lin, Wang, and Yu (2008), it took two and a half hours
on average to complete a three-level AHP decision problem per
DM and a total of 380 man-hours to complete all pairwise comparisons. This could be greater for the case of the ANP. On the other
hand, there are generally three reasons why a DM is reluctant to
complete the required comparisons speciﬁcally: (1) there is insufﬁcient time to complete all comparisons; (2) the DM is unwilling to

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

make direct comparisons of two alternatives and (3) the DM is
unsure of some of the comparisons (Harker, 1987a). In a MonteCarlo simulation study (Carmone, Kara, & Zanakis, 1997), comparisons are deleted from large matrices and results show that at most
50% of the comparisons can be deleted without signiﬁcantly affecting the weights of the criteria. Furthermore, to obtain a reasonable
and consistent PCM, Saaty (1977) recommends that the number of
criteria or alternatives within a PCM should only be at most seven.
Hence, any PCM with eight or more elements is considered large.
Unfortunately, lots of decision problems far exceed this maximum
threshold. There exist several articles that attempt to address the
issue of numerous pairwise comparisons, one of which are called
decomposition methodologies (Ishizaka, 2008; Ishizaka, 2012;
Shen, Hoerl, & McConnell, 1992; Triantaphyllou, 1995). When a
PCM A of n alternatives is decomposed into k subsets, pairwise
comparisons are elicited only in those subsets. Since the dimensions of the decomposed matrices are smaller than A, a reduction
in the number of pairwise comparisons is realized. Unfortunately,
these methods are not without any disadvantages. Firstly, these
methodologies focus on decomposing PCMs for alternatives. Lots
of pairwise comparisons can be saved when these methodologies
are extended to decompose the criteria PCMs since both the AHP
and ANP can have multiple criteria PCMs within the hierarchy or
network respectively. Secondly, when a PCM is decomposed into
subsets, the obtained relative weights of the elements are valid
only within those subsets and the problem arises when aggregating the results back to its global counterpart. As a result, a pivot
element is selected arbitrarily and assigned to all subsets and is
used as a basis for comparing the elements across all disjoint subsets. The global weights can then be estimated (Shen et al., 1992;
Ishizaka, 2008). Please note that pivot element selection is a challenging issue as the decisions should consider reducing the number and inconsistency of the pairwise comparisons, as well as
reducing the amount of dependence present among elements
within each subset. Thirdly, these methodologies lack guidelines
to assign criteria or alternatives to respective subsets since they
are done arbitrarily. This does not guarantee that the number of
pairwise comparisons elicited is reduced optimally.
The second limitation related to the curse of dimensionality is
attributed to the consistency of the pairwise comparisons elicited
from the DM when the number of alternatives or criteria is large.
As the number of pairwise comparisons increases, the consistency
of these comparisons is expected to be less reliable and results to
inconsistent decisions (Weiss & Rao, 1987). A performance metric
called consistency index (CI) is generally used to estimate the
inconsistency of a PCM A (Saaty, 1977). The CI is computed by
obtaining the eigenvalue of the pairwise comparison matrix using
Eq. (1):

CIðAÞ ¼

kmax  m
m1

ð1Þ

where m is the dimension of the PCM A and kmax is the maximal
eigenvalue of matrix A. The consistency ratio (CR) is the ratio of
CI and RI and is computed using Eq. (2):

CRðAÞ ¼

CIðAÞ
RIðmÞ

ð2Þ

where RIðmÞ is the random index obtained from the average CI of
500 randomly ﬁlled matrices and is dependent on the value of the
m. According to Saaty (1977), if a PCM A has CR < 10%, then A is considered to have an acceptable level of consistency. Nevertheless,
DMs that use the PCMs are faced with the issues on bounded rationality (Simon, 1972). With this, due to their limited cognitive processing powers, the DMs are not expected to provide consistent
pairwise comparisons all throughout the pairwise comparison elicitation process especially when the number of pairwise comparisons

271

is large. Therefore, a methodology that reduces the pairwise comparisons elicited from the DM would lead to improved consistency levels since only a handful of pairwise comparisons are elicited and
would not be cognitively taxing to the DM.
This research proposes the PCM Decomposition Methodology
(PDM) to address the limitations of PCMs when used in either
the AHP or the ANP methodology. The contributions of the PDM
are twofold: (1) The PDM seeks to reduce the number of pairwise
comparisons elicited from the DM thereby increasing its consistency. A binary integer programming (BIP) model is proposed to
accomplish this by intelligently decomposing PCMs into smaller
and manageable subsets. Only pairwise comparisons within those
subsets are elicited from the DM. The BIP uses the inner dependence comparisons of the elements to assign these elements into
mutually exclusive subsets. Hence, interdependent elements are
separated as much as possible thereby reducing the amount of
interdependencies among subsets. (2) Since the subsets are disjoint, a pivot element is optimally selected and is used to connect
all pairwise comparison matrices within each PCM. The pivot is
selected that minimizes the interdependencies of the elements.
Using the pivot element and the local weights, the global weights
of the elements of the PCM are then calculated.
The rest of this paper is organized as follows. Section 2 reviews
existing literature that tries to solve the aforementioned problems.
Section 3 illustrates the steps of the proposed PDM, while Section 4
describes the application of the PDM in reducing the number of
pairwise comparisons in an AHP problem. On the other hand, Section 5 presents the application of the PDM in reducing the number
of pairwise comparisons in an ANP problem. Finally, Section 6 concludes the paper and proposes further research areas.
2. Review of related literature
There exist methodologies that address the limitations for the
PCMs, in terms of the numerous required pairwise comparisons
and its inconsistency, can be mainly classiﬁed as: (1) optimization
methods and (2) heuristic methods. Each category is reviewed as
follows.
2.1. Optimization methods
Optimization models start with a handful of pairwise comparisons only. The remaining pairwise comparisons are estimated
using optimization algorithms by taking advantage of the matrix
properties of A. Starting with a of minimum m  1 comparisons,
a gradient descent method is proposed to select the next pairwise
comparison that would have the biggest information gain (Harker,
1987b). Additionally, stopping rules are provided for terminating
the pairwise comparison elicitation process. The methodology by
Bozoki, Fulop, and Ronyai (2009) uses nonlinear optimization with
exponential scaling to estimate the missing pairwise comparisons
from available ones. However, all possible combinations of connecting paths must be considered. The number of combinations
exponentially grows as the number of missing comparisons
increases and thus would be inefﬁcient to solve. A linear programming formulation by Triantaphyllou (1995) is used to estimate the
missing pairwise comparisons of A by considering two arbitrary
subsets s1 and s2 of the criteria PCM where s1 [ s2 – ;. By solving
the linear programming problem, the global weights of the m criteria of the PCM can be estimated. Nevertheless, the algorithm only
focuses on dividing the PCM A into two subsets. If m is large, then
the two subsets are still large. Moreover, the error rates of estimating the missing comparisons are dependent on the number of
common elements of subsets s1 and s2 . The smaller the s1 [ s2 ,
the estimation of the missing comparisons is expected to be less

272

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

accurate and deviation error increases signiﬁcantly. The issue of
pairwise elicitation is addressed by Despotis and Derpanis (2008)
which is composed of a min–max goal programming formulation
to derive the priorities from an interval. Zhang and Chen (2009)
develop a nonlinear programming model to improve the consistency of a PCM that uses a genetic simulated annealing algorithm.
In general, the challenge of optimization based approach lies to the
scalability of the problem. That is, as most decision problems tend
to have large number of alternatives and criteria, these analytical
approaches may suffer.
2.2. Heuristic methods
There are several types of heuristic methodologies but all of
propose to reduce the number of pairwise comparisons elicited
from the DM. Saaty (1990) proposes the idea to group alternatives
into subsets according to a common decision criterion. Ishizaka
(2012) propose to assign the alternatives into k subsets based on
a subjective absolute scale in which alternatives that have close
‘‘magnitudes’’ are grouped together. By using several pivot alternatives that is common to at most two subsets, the global priorities of
the alternatives are then obtained. Note, the deﬁnition of close
‘‘magnitudes’’ is not well deﬁned and is highly subjective. Furthermore, no guidelines are provided to determine which alternatives
are assigned to which subset. Shen et al. (1992) propose an arbitrary decomposition of the alternative PCM into k subsets such that
these k subsets have one common pivot alternative. Pairwise comparisons are ﬁrst performed on each subset and local priorities are
calculated. The global priority is then derived by using common
pivot alternative and local priorities of each subset. Ishizaka
(2012) applies the same decomposition algorithm on supplier
selection. There exist models that use the concept of a balanced
incomplete block designs in which subsets of the PCM A are
assigned to different DMs treated as replicates in contrast to having all DMs focus on the large PCM (Takahashi, 1990; Weiss &
Rao, 1987). The computation of the global alternative weights is
done by using the geometric mean. To the best of our knowledge,
no methodology has tried to optimally assign PCM elements to
subsets that minimize the number of pairwise comparisons, the
amount of dependence among criteria and the consistency of the
pairwise comparisons. Any methodology that reduces the required
number of pairwise comparisons for a PCM is fruitful for wider
adoption of the AHP and ANP methodologies (Ishizaka & Labib,
2011). Triantaphyllou (2000) develops a method to reduce the
number of pairwise comparisons via the duality approach when
the number of alternatives is greater than the number of criteria
plus one. Islam and Abdullah (2006) consider reducing the number
decision criteria by the nominal group technique. The decision criteria that have insigniﬁcant weights are eliminated from future
pairwise comparison elicitation process.
There are several articles (Benítez, Delgado-Galván, Izquierdo, &
Pérez-García, 2011b; Cao, Leung, & Law, 2008; Ergu, Kou, Peng, &
Shi, 2011; Saaty, 2003) that focus on reducing the inconsistency
of a given PCM without reducing the number of pairwise comparisons. Benítez et al. (2011b) propose a linearization heuristic that
provides the closest consistent PCM which is later extended to balance the consistency and the preferences of the DM (Benítez,
Delgado-Galván, Gutiérrez, & Izquierdo, 2011a). Still, upon applying these methodologies, the original pairwise comparisons significantly deviate from the resulting consistent pairwise comparisons
and thus would not reﬂect the actual preferences of the DM. These
methods focus on improving a given PCM which could be large. If
the number of decision criteria or alternatives is large, these
heuristics would still take a lot of time to complete and are more
subject to human error when providing the initial pairwise
comparisons.

We present Table 1 as an overview of the reviewed existing
related literature and their corresponding methodologies on
addressing PCM limitations.
In summary based on the analysis from Table 1, the following
four gaps in literature can be gleaned from the review as follows:
(1) existing decomposition methodologies focus on decomposing
the alternative PCMs with alternative elements. Additional pairwise comparisons can be saved when decomposing algorithms
are extended to criteria PCMs. (2) These methodologies lack guidelines to assign elements to subsets of the PCMs that minimize the
number of pairwise comparisons elicited, as well as the independence of elements within each subset. (3) These methodologies
select the pivot element arbitrarily and no rules are provided in
literature. (4) To the best of our knowledge, there is a lack of methodologies that try to reduce the number of pairwise comparisons of
a PCM for the ANP but not for the AHP. Any methodology that can
simplify the ANP would be beneﬁcial for any decision with interdependent criteria and alternatives.
3. Proposed PCM decomposition methodology
This section outlines the proposed PDM decision making framework. Fig. 1 presents a high level overview of the proposed PDM.
We illustrate the decomposition of a PCM A with m elements. In
step 1, the m elements are collected and a value of the number of
subsets k 2 ½2; m  1 is elicited from the DM. The pairwise comparisons that measure the inner dependencies of the elements are
qualitatively elicited or quantitatively gathered. Quantitative pairwise comparisons are direct observations from the attributes of
alternatives, while qualitative comparisons are elicited from the
DM to quantify the degree of preference between any two
elements. This is completed in step 2 (see Section 3.1). Let these
0
comparisons be R ¼ frii0 ji; i ¼ 1; 2; . . . ; mg. A symmetric interdependence matrix for the elements is derived from the R scores.
Using the obtained interdependence matrices, the m elements
are decomposed into k mutually exclusive subsets sl 2 S using
the proposed BIP decomposition methodology (see Section 3.1).
Additionally, in step 4, the pivot element is selected by choosing
the most independent one and assigned to all subsets (see Section 3.2). In step 5, local pairwise comparisons aij are then elicited
for all subsets of S and local weights are calculated (see Section 3.3).
In step 6, global priorities are estimated from the local pairwise
comparisons for all elements (see Section 3.4).
3.1. Decompose PCM into subsets
We start with the elicited m2 inner dependence pairwise comparisons denoted by matrix R as follows:

2

r 11
6
6r
6 12
R¼6
6 ..
4 .

r 21

...

r m1
..
.
..
.

r 22
..
.

...
..
.

r 1m

...

. . . rmm

3
7
7
7
7
7
5

ð3Þ

According to Saaty and Takizawa (1986) if all elements are
independent, then R ¼ Im where Im is an identity matrix of size
m. Otherwise, a score r ij is used to denote the dependence of element i to element j. Since it is unintuitive to partition the elements
using a directed graph, we transform the directed graph into a
symmetric undirected graph as follows:
T

e ¼RþR
R
2

ð4Þ

e with scores ~r ij 2 R
e where
We then obtain a symmetric matrix R
~rij ¼ ~rji . Given this, independent elements are intelligently assigned
into the k subsets using the proposed BIP formulation as follows:

273

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280
Table 1
Summary analysis of existing research.
Papers

Methodology

Limitations

Bozoki et al. (2009), Harker (1987b), Despotis and
Derpanis (2008)
Ishizaka (2012), Saaty (1990), Shen et al. (1992)

Estimate Missing Comparisons Using LP

Cao et al. (2008), Saaty (2003), Benítez et al. (2011b),
Ergu et al. (2011)

Changes Pairwise Comparisons to be
More Consistent

Arbitrary Subsets, Not Scalable, Number of pairwise comparisons
not minimized
Arbitrary Assignment, Alternatives only, Number of pairwise
comparisons not minimized
Large Number of Pairwise Comparisons Elicited, Values differ from
preferences of DM

Group Alternatives into Subsets

Fig. 1. The proposed PCM decomposition methodology.

Let the decision variables be:

have at least 1 element, then the kth subset has m  k elements
assigned.

0

~rii0 :¼ dependence of element i to i

1 if element i is assigned to subset with index l
xil ¼
0 otherwise
(
0
1 if both elements i and i is assigned to l
yii0 l :¼ xil xi0 l
0 otherwise

3.2. Select pivot element

Objective Function:

max

k X
m X
X
~r ii0 yii0 l

ð5Þ

l¼1 i¼1 i0 <i

Constraints:

xil  yii0 l P 0;

i 2 ½1; m; l 2 ½1; k

xi0 l  yii0 l P 0;

i 2 ½1; m; l 2 ½1; k

0

xi0 l þ xil  yii0 l 6 1;

0

i; i 2 ½1; m; l 2 ½1; k

k
X
xil ¼ 1 i 2 ½1; m

ð6Þ
ð7Þ
ð8Þ
ð9Þ

l¼1
m
X

xil P 1 l 2 ½1; k

When the PCM elements are decomposed into k subsets, the
elements are disjoint since there are no pairwise comparisons
across subsets. In order to determine the relative priorities of the
elements across subsets, a pivot element is selected and assigned
to all subsets and is used as a basis for the global weights. To select
the best pivot element, we select the element that has the least
sum of interdependencies ~r ii0 across among elements within the
PCM. Hence we shall choose the element that minimizes the following function:

ð10Þ

Piv ot Element i ¼ argmini

X
~r ii0

!
ð12Þ

i0

Eq. (12) selects the least interdependent element as compared to all
other elements within the PCM, and the selected element is then
assigned to all k subsets.

i¼1

yii0 l 2 BmC2k ;

xi0 l 2 Bmk

0

i; i 2 ½1; m; l 2 ½1; k

ð11Þ

The output of the BIP is a mutually exclusive assignment of the
m elements to subsets S ¼ fsl jl ¼ 1; 2; . . . ; kg. Eq. (5) describes the
objective function of minimizing the inner dependencies of the elements to be assigned in each subset sl . The BIP formulation would
work hard to assign two elements to two different subsets if
~r ii0 > 0. Constraint sets (6)–(8) are constraints that linearize the
quadratic constraint yii0 l :¼ xil xi0 l . Constraint set (9) forces each element to be a member of a subset while constraint set (10) forces all
subsets to have at least one element. Eq. (11) deﬁnes xil as binary
integer variables.
Given the BIP formulation, the following properties can be realized: (1) the minimum number of elements assigned to a given
subset is one and (2) the maximum number of elements assigned
to a given subset is m  k. The minimum number of elements
assigned follows from the BIP formulation; speciﬁcally constraint
set (10) forces the number of elements assigned to subsets to be
at least 1. In terms of the maximum, since the k  1 subsets would

3.3. Elicit local pairwise comparisons and calculate local weights
After decomposition, local pairwise comparisons are elicited
from the DM for all subsets after the elements are assigned to subsets. The local pairwise comparisons for elements subset sl are
illustrated in matrix form Al as shown in Eq. (13) as follows:

2

1

6 a2;1
6
Al ¼ 6
6 ..
4 .
aml ;1

a1;2
1
..
.
aml ;2

   a1;ml

3

   a2;ml 7
7
7
..
. 7;
.
.
. 5

1

8s l 2 S

ð13Þ

A new performance measure is proposed to keep track of the
consistency of the pairwise comparisons. The original deﬁnition
of the CR of matrix A is no longer applicable since the m elements
are assigned into k subsets. With this, a new deﬁnition of consistency is proposed as follows:

274

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

Deﬁnition 1. The Average Consistency Ratio (ACR) performance
measure of an AHP problem decomposed into k subsets is deﬁned
as:

ACR ¼

k
X
1
ml CRðAl Þ
m þ k  1 l¼1

ð14Þ

where CRðAl Þ is the consistency ratio of the pairwise comparison
matrix Al of subset l.
In simple terms, the ACR is the weighted average of the CR of
each of the local pairwise comparison matrices. The ACR is used
to estimate the overall CR of the pairwise comparisons across all
subsets.
Given the assignment of m elements into k subsets and the
addition of the pivot element in all subsets, we deﬁne a quantity
to determine the reduction in the number of pairwise comparisons
elicited from the DM as follows:
Deﬁnition 2. Given the set of the assigned elements into the k
subsets with one pivot element, the total number of required
pairwise comparisons needed to obtain the local priorities of the
elements denoted by dðAk Þ is computed as:

dðAk Þ ¼


k 
X
ml
l¼1

2

¼

k
X
ml ðml  1Þ
l¼1

2

ð15Þ

Remark: The maximum number of elements assigned to k subsets
after including the pivot elements is:
k
X
ml 6 m þ k  1

DðAk Þ ¼

m



2

 dðAk Þ ¼

The total number of pairwise comparisons after decomposition
is dependent on the distribution of the assignment of elements to
the k subsets and as such is shown using Eq. (15). If k ¼ 1, no
decomposition is performed, then all the required mðm  1Þ=2
pairwise comparisons are elicited. On the other hand when
k ¼ m  1, then the pivot element is compared to all other elements with a total of only m  1 pairwise comparisons. The following propositions can be drawn with regards to the number of
pairwise comparisons.
Proposition 1. Given values of k, function dðAk Þ is maximized when
ml ¼ m þ ðk  1Þ for l 2 S and ml0 ¼ 2; 8l 2 S n flg

$
%



kþ2
ðk  1Þðm  1Þ2
k
6 DðA Þ 6
ðk  1Þ m 
2
2k

R Z

Proof. See Appendix. h

~
a
j¼1 1j

6 m
6
..
wðAl Þ ¼ 6
6
4 Pm.l

j¼1

m

Deﬁnition 3. The difference between the original number of
pairwise comparisons of matrix A and number of pairwise comparisons after decomposition into k subsets denoted by DðAk Þ is
given by:

~m j
a
l

3

wð1; lÞ

3

7 6 wð2; lÞ
7 6
7¼6
..
7 6
5 4
.

7
7
7;
7
5

2

l ¼ 1; 2;    ; k

ð19Þ

wðml ; lÞ

3.4. Calculate global weights of the PCM
This subsection describes the methodology to compute global
weights of the elements of the decomposed PCM from the local
subsets using a pivot element. Given values of k there will be
m þ k  1 instances of wði; lÞ. The local element weight in subset l
is divided by the weight of pivot element in that subset and is
~ l Þ be the vector
repeated for all subsets. To illustrate this, let wðA
~ lÞ 2 wðA
~ l Þ is computed using
of normalized weights where each wði;
Eq. (20).

1
½wði; lÞ;
wði ¼ cp ; lÞ

8i 2 sl ; 8sl 2 S

ð20Þ

Given this, the normalized pivot element weight in each subset has
a value equal to one. Since all normalized pivot element weights
have a value equal to one, all the other elements in the other subsets can be compared to the pivot elements. For the computation
of the global weights, let w0 ðAÞ be the vector of global weights
where w0 ðiÞ 2 w0 ðAÞ is computed using Eq. (21).

w0 ðiÞ ¼ Pk P
l¼1

Deﬁnition 3 illustrates the difference between the original total
number of pairwise comparisons mðm  1Þ=2 and the amount of
pairwise comparisons needed, DðAk Þ when the elements are
assigned to subsets.

ð18Þ

Proof. See Appendix. h
Theorem 1 provides the pessimistic and the optimistic estimates of the reduction of the required mðm  1Þ=2 pairwise comparisons of A. This performance metric would be a good yardstick
to determine the amount time saved by the DM when making a
complex decision using the proposed PDM.
After eliciting local pairwise comparisons for all subsets Al we
now deﬁne the local element weights computed for each subset.
Let wðAl Þ be the vector of local weights from Al where
wði; lÞ 2 wðAl Þ is the local weight of element i. The original eigenvector methodology is used to calculate the local element weights
as follows:

~ lÞ ¼
wði;

 ml ¼ mþðk1Þ
for l 2 S if mþðk1Þ
2 Z or
k
j k
k
l
m
0
mþðk1Þ
for some l 2 S and ml0 ¼ mþðk1Þ
for some l if
 ml ¼
k
k

ð17Þ

Theorem 1. Given m elements grouped into k subsets, the total
number of required local pairwise comparison saved is bounded by:

Proof. See Appendix. h
Proposition 2. Given values of k, function dðAk Þ is minimized when:


k 
ml
mðm  1Þ X

2
2
l¼1

Given Propositions 1 and 2, the amount of time saved by the DM
in terms of the reduction of the number of pairwise comparisons
can be generalized in terms of Theorem 1.

2 Pm l
ð16Þ

l¼1

mþðk1Þ
k



1
~ lÞ;
wði;
~
wði;
lÞ  k þ 1
i2sl

8i 2 ½1; m

ð21Þ

4. Decomposing PCMs for the AHP methodology
This section illustrates the application of the PDM on an AHP
decision. The AHP assumes that the criteria and alternatives are
independent and hence, we seek an alternative way to measure
the interdependence of the elements. Speciﬁcally, the correlation
of the alternative scores rij is used to estimate the inner dependencies of the criteria. The correlation of the alternative scores could

275

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

act as an alternative to estimate inner dependencies of the criteria.
The alternative scores rij represent the raw rating score of alternative j on criterion i. We formally deﬁne the correlation denoted by
Rðci ; ci0 Þ as follows:
Deﬁnition 4. Let Rðci ; ci0 Þ be the correlation between criterion ci
and criterion ci0 which is calculated using (22) as follows





Pn



0
0


j¼1 ðr ij  r ij Þðr i j  r i j Þ
0
Rðci ; ci Þ ¼ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

P
P

n
2
n
2

0
0

j¼1 ðr ij  r ij Þ
j¼1 ðr i j  r i j Þ 

ð22Þ

These correlation coefﬁcients will be used in the proposed BIP
to group uncorrelated criteria. A value Rðci ; ci0 Þ ¼ 1 means that
the criterion ci is positively or negatively correlated to criterion
ci0 . Hence, the PDM can be applied to decompose the lowest level
criteria PCM, directly above the alternatives within an AHP decision problem. Otherwise, we decompose all other AHP PCMs arbitrarily as is done in existing literature. Fig. 2 illustrates the
proposed PDM as applied to a 3-level AHP problem.
In step 1, the DM decides on the m criteria and n alternatives
and the three-level decision hierarchy (see Section 4.1). Step 2 is
the alternative scores pairwise comparison elicitation stage and
the corresponding correlation matrix is computed (see Section 4.2).
The PDM is applied to decompose the criteria PCM and the global
weights of the m criteria are calculated in step 3 (see Section 4.3).
Lastly, step 4 computes the weighted scores of the n alternatives for
decision making (see Section 4.4). A peer reviewed AHP dataset
from existing literature is used to illustrate the application of the
PDM for AHP. Önüt, Efendigil, and Kara (2010) use a fuzzy AHP
model for shopping center site selection and is illustrated in this
subsection.

Fig. 3. AHP Hierarchy structure for the dataset of Önüt et al. (2010).

Table 2
Raw data of the scores of each alternative on each criterion
Alternative

Criterion

A
B
C
D
E
F

1

2

3

4

5

6

7

8

5
7
5
5
7
5

7
7
5
5
9
5

5
5
7
8
5
7

9
7
5
3
5
3

5
7
5
5
7
5

5
7
5
5
7
5

3
5
9
9
3
9

3
5
7
5
3
5

Table 3
Correlation matrix computed from Table 2.

4.1. Initialize AHP hierarchy

Criterion

1

2

3

4

5

6

7

8

The m criteria and n alternatives are identiﬁed and arranged
into a three-level decision hierarchy. In terms of the dataset from
Önüt et al. (2010), the goal, eight criteria and six alternatives are
setup as a hierarchy as is done in the traditional AHP methodology.
Fig. 3 illustrates the proposed three-level AHP hierarchy structure.

1
2
3
4
5
6
7
8

1.000
0.791
0.680
0.221
1.000
1.000
0.600
0.343

0.791
1.000
0.860
0.489
0.791
0.791
0.922
0.759

0.680
0.860
1.000
0.794
0.680
0.680
0.933
0.633

0.221
0.489
0.794
1.000
0.221
0.221
0.758
0.417

1.000
0.791
0.680
0.221
1.000
1.000
0.600
0.343

1.000
0.791
0.680
0.221
1.000
1.000
0.600
0.343

0.600
0.922
0.933
0.758
0.600
0.600
1.000
0.824

0.343
0.759
0.633
0.417
0.343
0.343
0.824
1.000

4.2. Compute correlations of criteria
Table 2 presents the most likely scores of the six site alternatives over the eight selection criteria. Additionally, the corresponding 8  8 correlation matrix is computed and is presented in
Table 3.

criteria. Speciﬁcally, the criteria PCM is decomposed into two subsets (k ¼ 2), then a pivot criterion is selected. Furthermore, the
local pairwise comparisons are collected for each subset and the
corresponding global criteria PCM weights are calculated.

4.3. Apply PCM to the criteria PCM
This subsection illustrates the decomposition of the criteria
PCM and the calculation of the global weights of the eight decision

4.3.1. Decompose criteria PCM
The proposed BIP methodology from Section 3.1 is applied to
the criteria PCM using the correlation scores from Table 3. After

Fig. 2. Application of the PDM for a 3-level AHP problem.

276

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

executing the proposed BIP methodology, criteria 1, 4, 6 and 7 are
assigned to subset s1 while criteria 2, 3, 5 and 8 are assigned to
subset s2 .

Table 5
PCM for subset 1.

1
4
6
7

4.3.2. Select pivot element
The optimal pivot criterion is selected by applying Eq. (12) and
Table 4 presents the results of the sum of the individual correlations of criterion i to all other criteria. It is evident from Table 4
that criterion 4 is the least independent criterion. And thus criterion 4 is assigned to all subsets. Hence, subsets s1 and s2 now have
1, 4, 6 and 7 and 2, 3, 4, 5 and 8 criteria respectively.

k
X
1
4
5
ml CRðAl Þ ¼ 0:0547 þ 0:119 ¼ 0:0904
m þ k  1 l¼1
9
9

4

6

7

Local priority (%)

1.00
3.00
7.00
0.67

0.33
1.00
1.00
0.33

0.14
1.00
1.00
0.20

1.50
3.00
5.00
1.00

10.31
33.65
46.99
9.05

Table 6
PCM for subset 2.

4.3.3. Elicit local pairwise comparisons
To illustrate the eliciting of local pairwise comparisons, the original most likely fuzzy values are used from the original 8  8
fuzzy AHP criteria PCM. Tables 5 and 6 summarize the local pairwise comparison matrices for the two subsets respectively.
A total of 4C2 þ 5C2 ¼ 16 pairwise comparisons are elicited in
this setup, which is a reduction of DðAk Þ ¼ 12, as compared to
the original 28 required pairwise comparisons when the original
AHP methodology is used. The average consistency of these two
priority matrices is computed using Eq. (14) as follows:

ACR ¼

1

2
3
4
5
8

2

3

4

5

8

Local priority (%)

1.00
3.00
5.00
3.00
1.00

0.33
1.00
0.33
3.00
0.33

0.20
3.00
1.00
1.00
0.33

0.33
0.33
1.00
1.00
0.20

1.00
3.00
3.00
5.00
1.00

7.43
26.39
24.24
34.94
7.00

Table 7
Results of the PDM as compared to the original AHP criteria PCM.

ð23Þ

The original 8  8 AHP matrix has a CR of 12.96% which is highly
inconsistent while the decomposed matrix has an ACR of only
9.04%. Hence, in this setup, several inconsistent pairwise comparisons are excluded from the decision making. Furthermore the subset PCMs have a smaller dimensions (dimðs1 Þ ¼ 4; dimðs2 Þ ¼ 5) thus
making the elicitation of pairwise comparisons less taxing for the
DM.

Criterion

Original criteria PCM weights

PDM weights

5
6
4
3
1
2
8
7

26.09%
21.95%
15.69%
15.36%
7.52%
5.24%
4.46%
3.69%

23.64%
22.90%
17.86%
16.40%
5.027%
5.025%
4.736%
4.409%

DðA2 Þ
ACR

None

12

12.96%

9.63%

from the alternatives. Hence, the application of the PDM for AHP
problems is limited. Therefore, we illustrate the full potential of
the PDM in terms of an ANP network in Section 5.

4.3.4. Calculate global weights of the criteria PCM
After calculating the priorities of the local criteria, the corresponding global priorities of the criteria need to be calculated.
Using the local weights from subset 1 and subset 2 and Eq. (20),
all local weights are divided by the local weight of pivot criterion
4. And as such, we obtain normalized weights for subset
~ 1 Þ ¼ ½0:31; 1:00; 1:40; 0:27T and subset 2: wðA
~ 2Þ ¼
1: wðA
½0:31; 1:00; 1:00; 1:44; 0:29T . Using the normalized weights and
using Eq. (21) we sum the normalized weights and we obtain the
following global weights as summarized in Table 7.
Based on the results, the weights obtained from the proposed
PDM is relatively similar to the value of the original weights with
a lower average consistency ratio. We observe from Table 7 that we
have saved 12 pairwise comparisons for the criteria PCM while
having similar weights. This savings is attributed to the decomposition of the PCM into two subsets in which the pairwise comparisons of the criteria across subsets are not elicited. Furthermore, a
reduction of the CR is observed from 12.96% to 9.63%. In this setup,
lots of inconsistent pairwise comparisons are omitted. Furthermore, the PCMs for the subsets are smaller well below Saaty’s
threshold of seven elements.
However, the PDM can only be applied to the special case of a
three-level AHP problem. Furthermore, by deﬁnition of the correlation, we only measure the linear interdependencies of the criteria

4.4. Compute weights of alternatives
After computing for the global weights of the eight criteria, we
now compute the weights of the six alternatives. Table 8 presents
the results of the weighted scores by multiplying the scores from
Table 2 with the obtained global weights from Table 7. Hence,
alternative B is selected since it has the highest weighted score followed by alternative E, then A, C, D and E.

5. Decomposing PCMs for the ANP methodology
This section illustrates the application of the PDM on an ANP
problem. Since all PCMs that form clusters within the ANP network
require inner dependencies to measure the interdependence of elements, these inner dependencies are then used to decompose all
PCMs within the network. Hence, all PCMs in the network can be
decomposed using the PDM which leads to larger pairwise comparison savings. We illustrate the decomposition of the eight decision criteria into three subsets using Fig. 4 as follows.

Table 4
Sum of correlation.
Criterion

Correlation

1

2

3

4

5

6

7

8

5.6346

6.402

6.2592

4.1196

5.6346

5.6346

6.2365

4.6615

277

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280
Table 8
Weighted scores of the six alternatives.
Alternative

Weighted score

A
B
C
D
E
F

5.6319
6.4889
5.5989
5.3110
6.0493
5.1470

In step 1, the ANP network structure is developed with the n
alternatives and m criteria (see Section 5.1). In Step 2, the inner
dependencies of all the elements of each PCM are elicited (see
Section 5.2). Furthermore, we apply the PDM to all PCMs of the
network in step 3 (see Section 5.3). Finally, step 4, we calculate
the limiting weights of the alternatives for decision making (see
Section 5.4). A dataset from existing literature is used to demonstrate the application of the PDM for the ANP. Cheng and Li
(2004) propose an ANP methodology for contractor selection. This
dataset is selected since the eight decision criteria are considered
simultaneously in a single cluster. Furthermore, the decision
focuses on three alternatives.

Fig. 5. The ANP network structure for the dataset of Cheng and Li (2004).

Table 9
Inner dependency scores for the eight criteria.

5.1. Initialize ANP Network
The goal, eight criteria and four alternatives are setup as a
network as is done in the traditional ANP methodology. Fig. 5
illustrates the proposed three-cluster ANP network structure.
5.2. Elicit inner dependence
Inner dependencies are elicited from the DM to estimate the
interdependencies of the criteria. Table 9 summarizes the elicited
inner dependency scores of all eight decision criteria which are
elicited from the DM from Cheng and Li (2004). We then compute
the corresponding symmetric matrix using Eq. (4). Table 10
presents the symmetric matrix.
5.3. Apply PDM to all PCMs
This subsection illustrates the application of the PDM to decompose all PCMs. In our example, we illustrate the decomposition of
the criteria cluster into three subsets. Furthermore, the optimal
pivot criterion is selected and is used to link all three subsets.
The local pairwise comparisons are then elicited and local weights
are calculated. The corresponding global criteria PCM weights are
computed from the local weights.
5.3.1. Decompose criteria PCM
The proposed BIP methodology is applied from Section 3.1 on
the symmetric inner dependency scores from Table 10. Hence,

Criterion

1

2

3

4

5

6

7

8

1
2
3
4
5
6
7
8

0.00
0.14
0.20
0.11
0.20
0.11
0.11
0.11

0.56
0.00
0.06
0.06
0.06
0.06
0.06
0.12

0.33
0.37
0.00
0.06
0.06
0.06
0.06
0.06

0.31
0.32
0.20
0.00
0.04
0.04
0.06
0.03

0.21
0.24
0.21
0.18
0.00
0.09
0.04
0.04

0.20
0.26
0.18
0.18
0.07
0.00
0.03
0.07

0.17
0.17
0.17
0.09
0.17
0.17
0.00
0.04

0.18
0.14
0.17
0.16
0.23
0.03
0.10
0.00

Table 10
Symmetric inner dependency scores for the eight criteria.
Criterion

1

2

3

4

5

6

7

8

1
2
3
4
5
6
7
8

0.00
0.35
0.27
0.21
0.21
0.16
0.14
0.15

0.35
0.00
0.22
0.19
0.15
0.16
0.12
0.13

0.27
0.22
0.00
0.13
0.14
0.12
0.12
0.12

0.21
0.19
0.13
0.00
0.11
0.11
0.08
0.10

0.21
0.15
0.14
0.11
0.00
0.08
0.11
0.14

0.16
0.16
0.12
0.11
0.08
0.00
0.10
0.05

0.14
0.12
0.12
0.08
0.11
0.10
0.00
0.07

0.15
0.13
0.12
0.10
0.14
0.05
0.07
0.00

criteria 2, 5, and 7 are assigned to subset s1 while criteria 1, 6
and 8 are assigned to subset s2 and lastly criteria 3 and 4 are
assigned to subset s3 .

Fig. 4. Application of the PDM for an ANP problem.

278

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

5.3.2. Select pivot element
The optimal pivot criterion is selected by applying Eq. (12) and
Table 11 presents the results of the sum of the individual inner
dependencies of criterion i to all other criteria.
It is evident from Table 11 that criterion 7 is the least independent criterion. And thus criterion 7 is assigned to all subsets.
Hence, subset s1 has criteria 2, 5 and 7, subset s2 has criteria 1, 6,
7 and 8 and subset s3 has 3, 4 and 7.
5.3.3. Elicit local pairwise comparisons
Local pairwise comparisons are elicited for all subsets of the criteria cluster. Tables 12–14 show the local priorities of the decomposed AHP obtained from the dataset of Cheng and Li (2004).
A total of 3C2 þ 4C2 þ 3C2 ¼ 10 pairwise comparisons are elicited in this setup. A reduction of DðAk Þ ¼ 18 pairwise comparisons
is realized from the original 28 required pairwise comparisons. The
average consistency of these three PCMs is computed using Eq.
(14) as follows:

ACR ¼
¼

1
mþk1

k
X
ml CRðAl Þ

Table 12
PCM for subset 1.

2
5
7

2

5

7

Local priority (%)

1
1/6
1/7

6
1
1

7
1
1

76.38
12.11
11.51

Table 13
PCM for subset 2.

1
6
7
8

1

6

7

8

Local priority (%)

1
1/9
1/9
1/9

9
1
2
2

9
1/2
1
2

9
1/2
1/2
1

73.28
6.01
8.79
11.93

Table 14
PCM for Subset 3.
3

4

7

Local priority (%)

1
1/3
1/3

3
1
1/2

3
2
1

58.89
25.19
15.93

l¼1

3
4
3
0:0042 þ
0:0847 þ
0:0607 ¼ 0:05335
10
10
10

ð24Þ

The original 8  8 ANP cluster has a CR of 9.44% which is around the
threshold of 10% while the decomposed matrix has an ACR of only
5.335%. Again we observe a reduction of the inconsistency of the
PCMs. Several inconsistent pairwise comparisons are not elicited
in this setup and the dimensions of the three subsets are considerably less than the original criteria PCM.
5.3.4. Calculate global weights of the criteria PCM
Again using the local weights from subset 1, 2 and 3 and
Eq. (20), all local weights are divided by the local weight of pivot
criterion 7. And as such we obtain normalized weights for subset
~ 1 Þ¼ ½6:64;1:00;1:00T , subset 2: wðA
~ 2 Þ¼ ½8:64;0:68;1:00;1:36T
1: wðA
~
and subset 3: wðA3 Þ ¼ ½3:70;1:58;1:00T Using the normalized
weights and using Eq. (21) we sum the normalized weights and
we obtain the following global weights as summarized in Table 15.
The weights obtained from the proposed PDM is relatively similar to the value of the original weights with a lower average consistency ratio. Additionally, we observe from Table 15 that a
reduction of 18 pairwise comparisons is realized for the criteria
PCM while having similar weights. This savings is again attributed
to the decomposition of the criteria PCM into three subsets in
which the pairwise comparisons of the criteria across subsets are
not elicited. Furthermore, a reduction of the CR is observed from
9.44% to 5.34%. In this setup, lots of inconsistent pairwise comparisons are omitted. Furthermore, the three subset PCMs are smaller
which are within Saaty’s threshold of seven elements.
We further test the PDM by changing the number of subsets.
The same dataset from Cheng and Li (2004) is used in terms of
the decomposition of the 8 criteria into k 2 ½2; 7 subsets. The traditional ANP is used and the results are presented in Table 16.
It is observed from Table 16 that as the value of k increases, we
generally observe an increase in the number of pairwise comparisons saved. This is attributed to smaller subset PCMs when we
increase the number of subsets. Furthermore, we observe a gradual

3
4
7

Table 15
Results of the PDM as compared to the original ANP criteria PCM.
Criterion

Original criteria PCM weights

PDM weights

1
2
3
4
8
5
7
6

33.9%
27.3%
14.8%
9.1%
3.4%
3.2%
3.8%
4.5%

33.3%
27.3%
15.2%
6.5%
5.6%
4.3%
4.1%
2.8%

DðA2 Þ
ACR

None

18

9.44%

5.34%

Table 16
Performance of the PDM.
Metric

Methodology
No Decomposition

k

DðA Þ
ACR (%)

Proposed PDM
k¼2

k¼3

k¼4

k¼5

k¼6

k¼7

0

12

16

18

19

20

21

9.44

8.99

5.34

5.24

5.17

4.82

0.00

Table 17
Weighted scores of the three alternatives.
Alternative

Weighted score

A
B
C

0.47
0.27
0.26

Table 11
Sum of inner dependencies.
Criterion

Sum of Inner dependence

1

2

3

4

5

6

7

8

1.47

1.31

1.095

0.92

0.92

0.775

0.72

0.74

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

decrease on the average CR of the PCMs. However, lots of pairwise
comparisons are not elicited for values of k P 4 hence, the redundancy advantage of PCMs is reduced and thus the preferences of
the DM would be subject to more biases and judgement errors. A
trade-off mechanism must be developed to address this which is
the subject of our future research.

After computing the weights of the criteria cluster for the eight
decision criteria, we now calculate the weights of the three alternatives using the traditional ANP methodology. Table 17 presents
the results of the three alternatives. Hence, alternative A is selected
since it has the highest limiting score followed by alternative B
then alternative C.
6. Conclusions and future work
A Pairwise Comparison Matrix (PCM) is an integral component
of decision making methodologies: Analytic Hierarchy Process
(AHP) and Analytic Network Process (ANP). These PCMs are used
to determine relative weights of criteria and alternatives. However,
a PCM suffers from the curse of dimensionality and hence the issue
of inconsistent pairwise comparisons when elicited from a decision
maker (DM). The proposed PCM Decomposition Methodology
(PDM) addresses these disadvantages. The PDM decomposes a
PCM into smaller manageable subsets using binary integer programming. As a result, the number and the inconsistency of pairwise comparisons elicited are reduced. Since the subsets are
disjoint, the most independent pivot element is selected to connect
all disjoint subsets. Hence the inner dependencies of the elements
are minimized within each subset. Using local priorities and the
pivot element, global priorities are then estimated for the elements
of the PCM.
The PDM is applied to a three-level AHP problem to decompose
the criteria PCM. Correlation of the criteria from alternative scores
is used as an alternative to estimate the interdependencies of the
criteria. The proposed methodology does indeed reduce the number of pairwise comparisons and the consistency ratio. Nevertheless, more pairwise comparisons is saved when the PDM is
applied to the ANP methodology. The PDM can be applied to all
cluster PCMs within the network since inner dependencies of the
elements are elicited for each PCM.
The authors plan to extend the framework by determining the
optimal number of subsets k for each ANP cluster by balancing
the (1) time savings by reducing pairwise comparisons, (2) the
amount of inner dependency among criteria and alternatives (3)
the level of consistency, and (4) the accuracy of the global weights.
Furthermore, multiple pivot elements are to be studied to further
improve the estimation of the global weights.
Appendix A. Proof of propositions
A.1. Proof of Proposition 1
Maximizing the value of dðAk Þ is determined by
P
Pk
max dðAk Þ ¼ kl¼1 ml ðm2l 1Þ subject to
l¼1 ml 6 m þ ðk  1Þ; l 2 ½1; k;
ml P 2; l 2 ½1; k and ml 2 Zk ; l 2 ½1; k. It is obvious that the integer
program tends to assign all elements to a single subset while minimizing the assignment to other subsets. Therefore, ml P 2 would
be a binding lower bound. And thus, the solution is
0

ml ¼ m þ ðk  1Þ for l 2 S and ml0 ¼ 2; 8l 2 S n flg maximizes dðAk Þ
where:

ðm  k þ 1Þðm  kÞ
þk1
2

A.2. Proof of Proposition 2
The proof for Proposition 2 is similar to the proof of Proposition
1 but the IP problem is set to minimize and the other extreme
point is obtained. However, by solving the IP problem, if
mþðk1Þ
k

2 Z, then the solution of the IP problem would be equal to

the LP relaxation problem where ml ¼ mþðk1Þ
in which all elements
k
are equally distributed among the k subsets. However, when

5.4. Calculate weights of alternatives

dðAk Þ ¼

279

ðA:1Þ

mþðk1Þ
R Z, then the solution of the LP relation of the IP problem
k
is different and thus the solution would require round off values
j
k
l
m
0
for some l 2 S and ml0 ¼ mþðk1Þ
for all l 2 S n flg.
of ml ¼ mþðk1Þ
k
k

A.3. Proof of Theorem 1
Part 1 (Lower Limit): Given that there are mðm  1Þ=2 required
pairwise comparisons for an original PCM, the minimum number
of pairwise comparisons reduced is bounded by:

DðAk Þ P

mðm  1Þ
 max dðAk Þ
2

ðA:2Þ

Based on Proposition 1, by substituting the values ml ¼ m þ ðk  1Þ
0
for l 2 S and ml0 ¼ 2; 8l 2 S n flg that maximizes dðAk Þ, we have:

DðAk Þ P

	


mðm  1Þ
ðm  k þ 1Þðm  kÞ

þk1
2
2

ðA:3Þ

Simplifying Eq. (A.3) we have:




kþ2
DðAk Þ P ðk  1Þ m 
2

ðA:4Þ

Part 2 (Upper Limit): Given that there are mðm  1Þ=2 required
pairwise comparisons for the original PCM, the maximum number
of pairwise comparisons reduced is bounded by:

DðAk Þ P

mðm  1Þ
 min dðAk Þ
2

ðA:5Þ

Based on Proposition 1, by substituting the values ml ¼ mþðk1Þ
if
k
2 Z, we have:

mþðk1Þ
k

DðAk Þ 6

mðm  1Þ k

2
2

	



m þ ðk  1Þ m þ ðk  1Þ
1
k
k

ðA:6Þ

Simplifying Eq. (A.6) we have:

$
DðAk Þ 6

ðk  1Þðm  1Þ2
2k

%
ðA:7Þ

References
Benítez, J., Delgado-Galván, X., Gutiérrez, J., & Izquierdo, J. (2011a). Balancing
consistency and expert judgment in AHP. Mathematical and Computer Modelling,
54, 1785–1790.
Benítez, J., Delgado-Galván, X., Izquierdo, J., & Pérez-García, R. (2011b). Achieving
matrix consistency in AHP through linearization. Applied Mathematical
Modelling.
Bozoki, S., Fulop, J., & Ronyai, L. (2009). Incomplete pairwise comparison matrices in
multi-attribute decision making. In IEEE international conference on industrial
engineering and engineering management, 2009. IEEM 2009 (pp. 2256–2260).
IEEE.
Cao, D., Leung, L., & Law, J. (2008). Modifying inconsistent comparison matrix in
analytic hierarchy process: A heuristic approach. Decision Support Systems, 44,
944–953.
Carmone, F. J., Kara, A., & Zanakis, S. H. (1997). A monte carlo investigation of
incomplete pairwise comparison matrices in AHP. European Journal of
Operational Research, 102, 538–553.
Cheng, E. W. L., & Li, H. (2004). Contractor selection using the analytic network
process. Construction Management and Economics, 22, 1021–1032.
Despotis, D. K., & Derpanis, D. (2008). A min–max goal programming approach to
priority derivation in ahp with interval judgements. International Journal of
Information Technology and Decision Making, 7, 175–182.

280

E.R. Jalao et al. / European Journal of Operational Research 238 (2014) 270–280

Ergu, D., Kou, G., Peng, Y., & Shi, Y. (2011). A simple method to improve the
consistency ratio of the pair-wise comparison matrix in ANP. European Journal
of Operational Research, 213, 246–259.
Harker, P. T. (1987a). Alternative modes of questioning in the analytic hierarchy
process. Mathematical Modelling, 9, 353–360.
Harker, P. T. (1987b). Shortening the comparison process in the AHP. Mathematical
Modelling, 8, 139–141.
Ishizaka, A. (2008). A multicriteria approach with AHP and clusters for supplier
selection. In 15th international annual EurOMA conference. Groningen.
Ishizaka, A. (2012). Clusters and pivots for evaluating a large number of alternatives
in AHP. Brazilian Operations Research Society, 31.
Ishizaka, A., & Labib, A. (2011). Review of the main developments of the analytic
hierarchy process. Expert Systems with Applications, 38, 14336–14345.
Islam, R., & Abdullah, N. A. (2006). Management decision-making by the analytic
hierarchy process: A proposed modiﬁcation for large-scale problems. Journal for
International Business and Entrepreneurship Development, 3, 18–40.
Lin, C. C., Wang, W. C., & Yu, W. D. (2008). Improving AHP for construction with an
adaptive AHP approach (A3). Automation in Construction, 17, 180–187.
Önüt, S., Efendigil, T., & Kara, S. S. (2010). A combined fuzzy MCDM approach for
selecting shopping center site: An example from istanbul, turkey. Expert Systems
with Applications, 37, 1973–1980.
Saaty, T. L. (1977). A scaling method for priorities in hierarchical structures. Journal
of Mathematical Psychology, 15, 234–281.

Saaty, T. L. (1990). How to make a decision: The analytic hierarchy process.
European Journal of Operational Research, 48, 9–26.
Saaty, T. L. (2003). Decision-making with the AHP: Why is the principal eigenvector
necessary. European Journal of Operational Research, 145, 85–91.
Saaty, T. L., & Takizawa, M. (1986). Dependence and independence: From linear
hierarchies to nonlinear networks. European Journal of Operational Research, 26,
229–237.
Shen, Y., Hoerl, A., & McConnell, W. (1992). An incomplete design in the analytic
hierarchy process. Mathematical and Computer Modelling, 16, 121–129.
Simon, H. A. (1972). Theories of bounded rationality. Decision and Organization, 1,
161–176.
Takahashi, I. (1990). Analysis of AHP by BIBD. Journal of the Operations Research
Society of Japan, 33, 12–21.
Triantaphyllou, E. (1995). Linear programming based decomposition approach in
evaluating priorities from pairwise comparisons and error analysis. Journal of
Optimization Theory and Applications, 84, 207–234.
Triantaphyllou, E. (2000). Multi-criteria decision making methods: A comparative
study (vol. 44). Springer.
Weiss, E. N., & Rao, V. R. (1987). AHP design issues for large-scale systems. Decision
Sciences, 18, 43–57.
Zhang, G., & Chen, Y. (2009). A new method for improving the consistency of
the comparison matrix in AHP. Mathematics in Practice and Theory, 23,
140–146.

J Digit Imaging (2014) 27:824–832
DOI 10.1007/s10278-014-9707-y

Pilot Study: Evaluation of Dual-Energy Computed Tomography
Measurement Strategies for Positron Emission Tomography
Correlation in Pancreatic Adenocarcinoma
Jorge Oldan & Miao He & Teresa Wu & Alvin C. Silva &
Jing Li & J. Ross Mitchell & William M. Pavlicek &
Michael C. Roarke & Amy K. Hara

Published online: 4 July 2014
# Society for Imaging Informatics in Medicine 2014

Abstract We sought to determine whether dual-energy computed tomography (DECT) measurements correlate with positron emission tomography (PET) standardized uptake values
(SUVs) in pancreatic adenocarcinoma, and to determine the
optimal DECT imaging variables and modeling strategy to
produce the highest correlation with maximum SUV
(SUVmax). We reviewed 25 patients with unresectable pancreatic adenocarcinoma seen at Mayo Clinic, Scottsdale, Arizona, who had PET–computed tomography (PET/CT) and enhanced DECT performed the same week between March 25,
2010 and December 9, 2011. For each examination, DECT
measurements were taken using one of three methods: (1)
average values of three tumor regions of interest (ROIs)
(method 1); (2) one ROI in the area of highest subjective
DECT enhancement (method 2); and (3) one ROI in the area
corresponding to PET SUVmax (method 3). There were 133
DECT variables using method 1, and 89 using the other
methods. Univariate and multivariate analysis regression
models were used to identify important correlations between
DECT variables and PET SUVmax. Both R2 and adjusted R2
were calculated for the multivariate model to compensate for
the increased number of predictors. The average SUVmax was
5 (range, 1.8–12.0). Multivariate analysis of DECT imaging
variables outperformed univariate analysis (r=0.91; R2 =0.82;
adjusted R2 =0.75 vs r<0.58; adjusted R2 <0.34). Method 3
had the highest correlation with PET SUVmax (R2 =0.82),
followed by method 1 (R2 =0.79) and method 2 (R2 =0.57).
J. Oldan : A. C. Silva : J. R. Mitchell : W. M. Pavlicek :
M. C. Roarke : A. K. Hara (*)
Department of Radiology, Mayo Clinic, 13400 E Shea Blvd,
Scottsdale, AZ 85259, USA
e-mail: hara.amy@mayo.edu
M. He : T. Wu : J. Li
School of Computing, Informatics, and Decision Systems
Engineering, Arizona State University, Tempe, AZ, USA

DECT thus has clinical potential as a surrogate for, or as a
complement to, PET in patients with pancreatic
adenocarcinoma.
Keywords Cancer . Dual-energy CT . Informatics .
Pancreas . Pancreatic adenocarcinoma . PET . PET/CT
Abbreviations
CfsSubsetEval
CT
DECT
PA
PET
ROI
SUV
SUVmax
VIF

Correlation-based feature subset selection
evaluation
Computed tomography
Dual-energy computed tomography
Pancreatic adenocarcinoma
Positron emission tomography
Region of interest
Standardized uptake value
Maximum standardized uptake value
Variance inflation factor

Introduction
Single-energy computed tomography (CT) is commonly used
for the diagnosis, staging, and follow-up of patients with
pancreatic cancer [1]. Single-energy CT utilizes Hounsfield
unit attenuation of lesions to infer tumor viability. These
attenuation measurements, however, can be inaccurate due
to volume averaging, beam hardening, or the presence of
high-density blood products, protein, or calcification [2]. Although positron emission tomography (PET) is generally not
as effective as CT for locoregional and nodal staging of
pancreatic cancer [3], it can be used for preoperative diagnosis
of pancreatic adenocarcinoma (PA) when CT and/or biopsy
are nondiagnostic or when the patient has concurrent chronic

J Digit Imaging (2014) 27:824–832

825

pancreatitis and/or cystic tumors. It can also be used for
detection of distant metastases, for differentiation of
radiation-induced fibrosis from tumor recurrence, and, according to some preliminary findings, for monitoring of therapy [4,
5]. PET, however, is expensive, time-consuming, and less
widely available than CT.
Dual-energy CT (DECT) is an imaging technique approved
by the US Food and Drug Administration that uses two
different energies (usually 80 and 140 kVp) instead of a single
energy to produce CT images. Scanning at different radiographic energies facilitates differentiation of materials such as
calcium, uric acid, iodine, and water, which can be helpful in
various clinical applications (e.g., evaluating renal stone composition [6, 7] or differentiating cysts from solid tumors [8]).
Specifically, in cases of PA, DECT has shown excellent
differentiation of tumors from normal pancreas [9], and it
may even allow for the elimination of noncontrast acquisition
[10]; the DECT technology, by separating out the iodineloaded images, allows one to generate a “virtual noncontrast”
image. Intravenous contrast is still necessary. Recently, high
correlation was demonstrated between positron emission tomography (PET) maximum standardized uptake value
(SUVmax) and DECT iodine values in nonsmall-cell lung
cancer [11]. If high correlations between DECT and PET
signals are observed in other cancers, then DECT may reduce
the need for more costly and time-consuming PET imaging.
The purpose of this study was to determine which DECT
imaging variables and which modeling strategy would produce the highest correlation with PET SUVmax in patients with
unresectable PA.

A total of 25 patients with a diagnosis of PA were identified
who met study criteria: 17 men and 8 women, with a mean age
of 65 years (range, 50–81 years). The average time between
PET and DECT was 1 day (range, 0–7 days). The mean
maximum axial diameter of the lesions was 4.3 cm (range,
1.5–8.4 cm). Pancreatic cancers were located in the uncinate
process (n=4), the head (n=7), the neck/body (n=10), and the
tail (n=8) (some cancers were present in more than one
region). All patients had unresectable or metastatic disease at
the time of DECT. Of the 25 patients, 13 had hepatic metastases, 12 had nodal metastases, and 21 had vascular involvement of the mesenteric vessels or portal vein. All had pathologically proven PA.
CT Protocol
All DECT examinations were performed as part of a standard
biphasic pancreatic CT protocol on a 64-slice single-source
DECT scanner with fast kilovolt switching (CT750 HD; GE
Healthcare, Milwaukee, Wisconsin). This protocol consisted
of a single-energy pancreatic phase (approximately 40 s after
injection of a contrast agent) and a dual-energy portal venous
phase (70 s after injection of contrast), using a body weightbased volume (1 mL/kg) of low molecular weight, nonionic,
iodinated contrast medium at 4 mL/s. Specific imaging parameters are shown in Table 1. The DECT portal venous data
produced monochromatic images from 40 to 140 keV, and
iodine and water (virtual precontrast) basis pairs, as well as
other basis pairs (eg, calcium–iodine) more commonly used
for other applications such as distinguishing kidney stones.
PET Protocol

Materials and Methods
This retrospective study was approved by the Mayo Clinic
Institutional Review Board, which waived the need for signed,
informed consent because this retrospective review was
deemed minimal risk.
Patient Selection
Inclusion criteria included a diagnosis of pathologically proven unresectable PA and PET/CT and contrast-enhanced DECT
conducted within the same week between March 25, 2010 and
December 9, 2011. All patients were adults.

All PET scans were performed on a combined 16-slice PET/
CT scanner (GE Discovery 600 PET/CT; GE Healthcare)
within 7 days of the DECT scan. The PET scan used 3.27mm slices and a three-dimensional acquisition with the VUE
Point HD reconstruction filter. Image matrix was 192×
192 pixels. A total of seven to nine 15-cm bed positions were
acquired at 2 min per bed position if body mass index was <35
or at 3 min per bed position if body mass index was ≥35. A
simultaneous noncontrast CT was performed at 120 kVp,
100 mA to 120 mA, nonhelical, with 3.75-mm slice thickness.
Coregistration between PET and noncontrast CT was performed using MIMvista software (MIMvista 5.2.3; MIM

Table 1 Pancreatic and venous phase protocols
Series

Speed, mm/rot

Pitch

Collimation, mm

Slice thickness, mm

Reconstruction interval, mm

kVp

Min/max, mA

Pancreatic
DECT venous

39.38
39.38

0.98
0.98

0.625
0.625

2.50
3.75

2
3

120
80,140

150–450
630

DECT dual-energy computed tomography, Max maximum, Min minimum, rot rotation

826

J Digit Imaging (2014) 27:824–832

Fig. 1 Pancreatic
adenocarcinoma regions of
interest on CT. Method 1 used
three regions of interest (a–c,
circles) in separate areas of tumor
heterogeneity, each in a different
slice

Software Inc., Cleveland, Ohio). The PET SUVmax within the
tumor was used for analysis.
DECT Region of Interest Selection

minima, maxima, and SDs of ROIs generated using different
basis pairs (of which there were 5 pairs of materials, producing
10 sets of images, and 1 for effective Z, or atomic number) and
keV settings (of which there were 11, from 40 to 140 keV in
10-keV increments). Methods 2 and 3 produced four variables
to analyze (mean, minimum, maximum, and SD), and method
1 produced six variables (as both maxima and minima could
be either the maximum or the minimum of the three ROIs, or
their average). Each method included a variable for the patient’s mass, which impacts standardized uptake value (SUV):
 .

 . 
local concentration g mCi
.
SUV g mL ¼
injected doseðmCiÞ weightðgÞ

All DECT monochromatic images were sent to an offline
computer workstation (GE Advantage Workstation, version
4.5; GE Healthcare) for image analysis using a commercially
available viewer software (Gemstone Spectral Imaging Viewer, version 2.0; GE Healthcare).
In this study, three different DECT measurement methods
(method 1, method 2, and method 3) were evaluated. All
regions of interest (ROIs) were placed by a single boardcertified radiologist (J.O.) for this early pilot study. For method 1, a total of three ROIs were manually drawn in three
different locations of tumor heterogeneity in the solid enhancing tumor on the DECT images (Fig. 1). The three different
slices were selected manually to be within the substance of the
tumor in areas that appeared visually similar to the rest of the
tumor and were located on different image slices. For method
2, a single ROI was drawn in the area of the tumor with the
highest visible DECT enhancement (Fig. 2a). For method 3, a
single ROI was drawn at DECT (Fig. 2b) in the area of the
tumor corresponding to the highest PET SUVmax (Fig. 2c).
Each ROI covered as much tumor as possible without including adjacent vessels or structures. The observer had access to
the PET/CT scanner at the time of ROI selection, but it was not
used for methods 1 and 2.

The analysis software saved all data to Excel (Microsoft
Inc; Redmond, Washington) files at every kiloelectronvolt in
10-keV increments from 40 to 140 keV. There were multiple
CT data sets, but the radiologist only drew his ROIs on the
standard 70 keV data set. Since the images are reconstructed
from the same pair of high-energy and low-energy images,
which are perfectly registered because they are acquired at the
same time, the ROI is simultaneously applied to all data sets.
Measurements were reported in Hounsfield units for monochromatic kiloelectronvolt images and in microgram per milliliter (iodine) and milligrams per milliliter (water) for basis
pairs.

DECT Measurements

Statistical Analysis

Methods 1, 2, and 3 provided 133, 89, and 89 variables,
respectively, for analysis (Table 2). These included means,

Univariate analysis was conducted first to determine the correlation between each DECT variable and PET SUVmax.

Fig. 2 Regions of interest on methods 2 and 3 (CT and PET/CT). a
Region of interest corresponding to maximum computed tomography
(CT) attenuation. b Region of interest corresponding to maximum

positron emission tomography (PET). c PET of the same tumor. All
images are from the same patient

J Digit Imaging (2014) 27:824–832

827

Table 2 Differences among region-of-interest placement methods and the total number of DECT variables measured
Method 1a

Method 2

Method 3

No. of DECT ROIs used

Three ROIs in areas of the greatest tumor heterogeneity One ROI corresponding to the One ROI corresponding to area
(variables averaged across all three)
highest CT attenuation
of the highest SUVmax
No. of image contrasts
22 Images: 1 effective Z image + 11 keV images (40–140 keV in 10-keV increments) + 10 material images: two images per
basis pair × five basis pairs: (1) I-water, (2) Ca-I, (3) Ca-water, (4) HAP-I, and (5) HAP-Ca
No. of variables measured Six variables: mean, SD, mean–max, mean–min, max– Four variables: mean, SD, max, min
per image contrast
max, min–min
Total No. of DECT
133 Variables: (22 image contrasts × 6 variables per 89 Variables: (22 image contrasts × 4 variables per contrast) +
variables
contrast) + mass
mass
Ca calcium, CT computed tomography, DECT dual-energy computed tomography, HAP hydroxyapatite, I iodine, max maximum, min minimum, ROI
region of interest, SUV standardized uptake value
In method 1, three ROIs were measured, resulting in three maxima and three minima. “Max–max” refers to the highest of the three maxima, “mean–
max” to their arithmetic mean. Similarly, “min–min” is the lowest of the three minima, whereas “mean–min” is their arithmetic mean

a

Multivariate analysis was then performed to jointly consider the predictive power of multiple features. Because of the
large number of DECT variables, we used Weka 3.2 (Waikato
Environment for Knowledge Analysis, University of Waikato,
Hamilton, New Zealand) [12], an open-source machinelearning software, for variable selection. This process may
improve the interpretation and predictive accuracy of statistical model development [13]. A linear forward feature selection method (using the linear forward feature selection option
from the CfsSubsetEval [correlation-based feature subset selection evaluation] method within Weka) was applied to select
subsets from the original DECT variables by removing uncorrelated, redundant, and noisy data. A list of features was
then selected in order of importance for correlation with
SUVmax.
For each ROI method, the statistical modeling software
Minitab 16 (Minitab Inc; State College, Pennsylvania) was
used to model the correlations between the SUVmax and the
selected variable subsets. Addressing the issue of potential
multicollinearity among the selected features was done by
Fig. 3 Multivariate and
univariate correlations.
Multivariate analysis of methods
1 to 3 outperformed the highest
correlations with univariate
analysis. MONO_50_max,
MONO_60_max, and
MONO_70_max refer to maxima
at 50 keV, 60 keV, and 70 keV, the
best correlated single variables

developing a forward statistical model by adding one
feature at a time. The variance inflation factor (VIF) was
then calculated to assess possible correlations between
features. VIF is a common index for measuring the severity of multicollinearity in statistical models, and VIF >10
indicates that a model has multicollinearity issues [2].
Using this guideline, we developed the regression models
for first order, second order, and second order with interaction effects. For model performance, we calculated both
R2 and adjusted R2.
R2, also known as the coefficient of determination, is a
measure of the proportion of the total variation in a variable
that is explained by a given model. It is defined as
R2 ≡1−ðSSres =SStot Þ
where
SStot ¼

X
i

2

ðyi −ȳ Þ and SSres ¼

X
i

2

ðyi −f i Þ

828

J Digit Imaging (2014) 27:824–832

where “yi” is each individual observation, “fi” is the predicted
observation due to the model, and “y” is the overall mean of all
observations.
Adjusted R2 is meant to counterbalance the tendency of R2
to rise as additional predictors are added. Adjusted R2 is
defined as


Adjusted R2 ¼ R2 − 1−R2 p=ðn−p−1Þ

where “n” is the sample size and “p” is the number of
predictors.
R2 is known to increase with the addition of predictors to
the model, whereas adjusted R2 compensates for the number
of predictors and thus adjusts for this inflation. For comparison purposes, we transformed R2, obtained from the multivariate regression models, to r to compare against the performance of univariate analysis.
The “r” is the coefficient of correlation between two variables, measuring the strength of a linear association. It is
calculated as
r¼

X 
i

X i −X



Y i −Y

 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃqX

. qX

2 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

2
X
−X
Y
−Y
i
i
i
i

for a single variable, R2 =r2.

second order, and the second order with interactions models
included 3, 6, and 7 variables, respectively. As shown in
Table 4, all predictors in the three models proved to be
significant (P<0.05). In addition, when a predictor had VIF
<10, it had relatively little correlation with other predictors
(although not necessarily no correlation).
Since method 3 requires the use of PET/CT to fully
investigate the clinical use of DECT as a surrogate for
PET/CT, we report the best model (second order with
interactions) from method 1, which provides comparable
results to those from method 3 (Table 5). We observed
high multicollinearity among the features, both in method 1 and in method 2. Although reports in the medical
literature indicate that multicollinearity does not reduce
the predictive power of the model as a whole [14], it
does increase the standard errors of estimates of the
coefficients, thus complicating interpretation of the model. To remove multicollinearity from the model, we first
identified the highly correlated features as a group, then
introduced a novel composite feature (as described in
Table 5) as a linear combination of the correlated features. As a result, the regression model showed R2 =0.79
and adjusted R2 =0.75, with all predictors (including the
composite feature) being significant (P<0.05) with VIF
<10.
Optimal Models
Optimal models themselves are as follows. For method 3, the
optimal first-order model was the following:

Results
DECT and PET/CT Correlation Results
Univariate modeling resulted in only three DECT variables
(the maximum values of 50, 60, and 70 keV monochromatic)
with a correlation of r>0.50 with SUVmax. The greatest of
these was r=0.58. Multivariate analysis of DECT imaging
variables outperformed univariate analysis (Fig. 3), achieving
r=0.91 (R2 =0.82) compared to r=0.58 (R2 =0.34) with univariate analysis. Table 3 lists the R2 between DECT variables
and SUVmax for the three different ROI methods and multivariate modeling strategies. For each ROI placement method,
the most complex modeling strategy (second order with interactions) produced the highest correlations. Overall, the highest
correlation obtained between DECT variables and SUVmax
was found in method 3 (R2 =0.82), followed by method 1
(R2 =0.79) and method 2 (R2 =0.57). The area of highest
PET SUV and of highest visible DECT enhancement correlated in 20 of the 25 cases.
The DECT variables that correlated best with SUVmax from
method 3 (P<0.05) are shown in Table 4. In addition, the
coefficient, P value, and VIF are provided. The first order, the

SUVmax ¼ 0:029  90 keV mean−0:017  iodine calcium min
þ 0:019  weight

The optimal second-order model was the following:
SUVmax ¼ 0:024  90 keV mean−0:025  iodine calciummin
þ 0:024  iodinecalciummax−0:116  iodine water SD
þ 0:00185  calcium iodine max2 þ 0:0016243
 90keVmean2

The optimal second-order model with interactions was the
following:
SUVmax ¼ 0:026  90 keV mean−0:0142  iodine calciummin
þ 0:0107  iodine calciummax þ 0:01  weight
þ 0:00162  calcium iodinemax  weight þ 0:00147
 90 keV mean  iodine calciummin−0:000605
 iodine calcium max2

J Digit Imaging (2014) 27:824–832

829

Table 3 Summary of the coefficient of determination (R2) for the three ROI placement methods and different models
ROI placement

No. of variables

First-order model

Second-order model

Second-order model with interactions

Method 1
Method 2
Method 3

133
89
89

0.65
0.36
0.55

0.76
0.36
0.68

0.79
0.57
0.82a

ROI region of interest
a

The best result (highest correlation) in each row and column was using method 3

For method 1, the optimal model was the following:
SUVmax ¼ 9:871  effective ZSD−0:001608  100 keV meanmax
 140 keVmeanmax−0:12867  140 keV meanmax
 effectiveZ meanmax þ 0:751  composite

with the composite defined as in Table 5.

Discussion
The first goal of this study was to identify the correlations
between DECT variables obtained via three different measurement methods and PET SUVmax. Method 3 provided the

highest correlation (R2 =0.82) between DECT variables and
PET signal, which is not surprising since DECT data were
obtained at the site of maximal fluorine 18 ( 1 8 F)
fluorodeoxyglucose uptake. However, it required manually
coregistered DECT and PET scans. In clinical practice, a
PET scan may not always be available or desired. In that case,
method 1, which required three ROIs in the area of greatest
tumor heterogeneity in the DECT scan, performed nearly as
well (R2 =0.79). Conversely, method 2, a single ROI in the
area of highest DECT enhancement, was not as effective (R2 =
0.57), although the selected variables overlapped considerably, with measurements corresponding to SUVmax.
As a result, in order to make maximum use of DECT results
to properly model the effects of PET/CT without performing a
PET/CT scan, it may be necessary to take multiple

Table 4 Multivariate models for methoda,b

First-order model
R2 =0.55
R2 (adjusted)=0.39
Second-order model
R2 =0.68
R2 (adjusted)=0.57

Second-order model with interactions
R2 =0.82
R2 (adjusted)=0.75

Features

Coefficient

P value

VIF

90 keV mean
Iodine–calcium min
BMI
90 keV mean
Iodine–calcium min
Iodine–calcium max
Iodine–water SD
Calcium–iodine max ^2
90 keV mean ^2
90 keV mean
Iodine–calcium min
Iodine–calcium max
Weight

0.029
−0.017
0.019
0.024
−0.025
0.024
−0.116
0.00185
0.0016243
0.0263
−0.0142
0.0107
0.0100

0.006
0.04
0.005
0.008
0.01
0.01
0.047
0.01
0.01
0.001
0.02
0.04
0.047

1.207
1.080
1.144
1.269
1.944
2.912
4.812
1.582
1.079
1.38
1.30
1.63
1.70

Calcium–iodine max × weight
90 keV mean × iodine–calcium min
Iodine–calcium max ^2

0.00162
0.00147
−0.000605

0.001
0.02
0.02

1.38
1.20
1.51

The significant value p<0.05
BMI body mass index, max maximum, min minimum, VIF variance inflation factor
a

Pairs of substances represent the use of proprietary software designed to separate a pair of substances. For example, the iodine–calcium min is the lowest
value pixel (minimum) on an image designed to separate iodine from calcium. On such an image, iodine will have higher values and calcium will have
lower values. Conversely, on the calcium–iodine images, calcium will have higher values and iodine will have lower values. “Effective Z” refers to an
image where the value is an estimate of the effective atomic number (Z); on these images, iodine will be higher than calcium, which will, in turn, be
higher than the hydrogen, oxygen, nitrogen, and carbon that make up most of human tissue

b

Variables in italics are used in all three regression models

830

J Digit Imaging (2014) 27:824–832

measurements of the tumor, as we did in method 1 when
drawing three ROIs, to reflect the heterogeneity of the tumor.
This may reflect linkage between tumor heterogeneity and
metabolic activity, as hypothesized below. Although we observed high correlations among the measures using this approach (and method 2), the use of composite features formed
by aggregating the highly correlated features (Table 5) enabled us to identify the regression model with interpretability.
With univariate analysis, the best single DECT predictors
that correlated with PET SUVmax (the ROI maxima at 50, 60,
and 70 keV) were from lower energy levels (in this study, 50,
60, and 70 keV). However, multivariate analysis of DECT
imaging predictors significantly outperformed univariate analysis (r=0.91 [R2 =0.82] vs r<0.60 [R2 <0.36]) in our particular model. In other words, analysis using combinations of
multiple imaging variables correlated with PET better than
using only single variables such as iodine values or a 70-keV
monochromatic Hounsfield unit. However, our results were
different from those of a prior study of DECT and PET in
small cell lung cancer [10] that found a univariate correlation
(r=0.88) between SUVmax and iodine-related enhancement
(equivalent to an iodine–water basis pair). One explanation
may be the lower SUVs in pancreatic cancers (mean, 5 [our
study]), whereas the aforementioned lung cancer study reported a high mean SUV of 14. Although precise cutoffs vary, an
abnormal SUV for pancreatic cancer is considered to be above
2 to 3 [15–18].
The authors of the lung cancer study did not perform
multivariate correlation for comparison. Correlation of a single DECT value would be simple to apply in clinical practice,
but in pancreatic cancer, it may not be as accurate as more
complex multivariate models. The practical clinical application of a multivariate model (e.g., from method 1) will likely
require the complexity to be hidden by implementation in an
easy-to-use computer program. Such a program could potentially produce a “virtual PET SUV value” in response to
placement of a DECT ROI and thus greatly enhance the use
of DECT for diagnosis of PA. It is our intention to create a

“virtual PET SUVmax” as the subject of future research and to
test its applicability in the clinical setting.
The second goal of this study was to identify the DECT
imaging variables and modeling strategy that produced the
highest correlations with SUVmax. With method 3, weight was
positively correlated with SUVmax. The model also showed
that some of the most significant variables correlated to images with lower iodine, such as 90 keV (positive correlation)
and iodine–calcium minimum (negative correlation). At
90 keV, the image is more heavily weighted toward water
because iodine is the brightest at a lower keV (e.g., 40 keV)
and lowest at a higher kiloelectronvolt (e.g., 140 keV) (Fig. 4).
The exact correlation between the keV and the iodine and
water basis pair images is shown in Fig. 5 for a typical ROI.
Conversely, the iodine–calcium image would be weighted
more heavily toward iodine. The fact that the method 3 model
showed a significant negative correlation to an iodine–calcium minimum implies that a lower iodine concentration visualized on DECT correlates with a higher PET SUV. The
correlation with images associated with low iodine suggests
that precontrast images may be more relevant in pancreatic
cancer than previously thought. Although prior CT investigations of the pancreas without intravenous contrast demonstrated no clinically significant difference in noncontrast density
[19–21], some authors have noted that PA is denser than
healthy tissue [19]. As tumors are effectively treated, they
typically become less dense, breaking up the fibrotic
stroma, and they become less metabolic. This correlation of low iodine and high PET SUV could indicate
that lower Hounsfield unit values at DECT imply denser
tissue and correlate with more aggressive or metabolically active PA. If so, that could aid the prognosis or
the development of effective treatment strategies not
currently available for similar-appearing hypodense PA.
In the future, it may be possible to evaluate tumor
viability with DECT using this strategy (similar to
PET/CT), but larger studies are necessary to confirm
this hypothesis.

Table 5 Second-order with interactions model from method 1

Second-order interaction
R2 =0.79
R2 (adjusted)=0.75

Features

Coefficient

P value

VIF

Effective_Z SD
100 keV meanmaxa × 140 keV meanmax

9.871
−0.001608

0.001
0.03

1.35
8.43

140 keV meanmax × effective_Z meanmax
Compositeb

−0.12867
0.7510

0.003
<0.001

9.59
6.27

VIF, variance inflation factor
The term “meanmax” refers to the mean of the maxima of the three regions of interest (ROIs) drawn. The term “maxmax” refers to the highest of the
maxima of the three ROIs drawn (i.e., the highest pixel value among the three ROIs)

a

Composite = −0.1432981 × 90keV_maxmax + 0.2718076 × 100keV_meanmax + 17.3583 × effective_Z_meanmax −0.458754 ×
calcium_NaUrate_meanmax −1.167255 × effective_Z_standard deviation × 90 keV_maxmax + 2.309526 × effective_Z_standard deviation ×
100keV_meanmax

b

J Digit Imaging (2014) 27:824–832

Fig. 4 Pancreatic adenocarcinoma on DECT at different simulated keV
levels. Same pancreatic tumor as in Fig. 2 at (a) 40 keV and (b) 140 keV.
Note that, at the lower energy, the iodine attenuation is enhanced
(arrows). At the higher energy, the water attenuation is dominant,
appearing like a precontrast image

Limitations of this study include its retrospective design,
use of a single observer, and small sample size (n=25).
Replicating this study with a larger sample size and multiple
observers would be a logical next step. In addition, the ROIs
were placed subjectively, which might have impacted the
reproducibility of the results. At the time of this study, no
automated or semiautomated software programs were available to measure pancreatic tumors during DECT to decrease
measurement variability. Additionally, SD and SUVmax are
relatively crude measures of heterogeneity; more sophisticated
measures of tumor texture, such as those based on the
Stockwell transform [22, 23] could be used as well. A more
objective measurement approach such as texture analysis may
ultimately achieve more reliable and reproducible results than
our subjective placement of ROIs; however, these techniques
are not currently commercially available. Some authors have
already shown the predictive value of baseline texture parameters and resulting changes in chemotherapy for renal cell
carcinoma [24]. Similarly, texture analysis has been used to
classify lung tumors [25] and head and neck tumors [26], so

Fig. 5 Correlation of image
value with iodine member of
iodine–water basis pair for a
typical region of interest

831

these techniques might also be useful in pancreatic cancer.
Another direction for future study would be to investigate
changes in the correlation of DECT with PET/CT over time,
looking specifically at patients with high SUVs who are
treated with chemotherapy.
Another potential limitation is that reconstruction algorithms might produce different results from different vendor
scanners. Our results only represent those from a single vendor scanner (General Electric). Currently, this type of DECT
analysis is not automated or routinely available clinically. If
these results are confirmed in larger groups of patients, it may
be possible to develop a “virtual PET” DECT image or results.
DECT has many potential advantages over PET/CT in
terms of imaging time, cost, and spatial resolution that make
it an attractive option. DECT image acquisition takes less than
1 min compared with 120 min for the PET scan and a
noncontrast CT acquisition. DECT can cost $300 to $500,
whereas PET can cost $1,000 or more. The disadvantages of
DECT are similar to those of PET/CT: it is a less accessible
modality than single-energy CT and it requires similar radiation doses. Whereas there is an increasing number of DECT
scanners available in the USA, compelling clinical uses have
been lacking, unlike those for PET/CT, which has been shown
to provide valuable additional clinical information.
In summary, we note that our results demonstrated that
univariate analysis does not perform as well as multivariate
analysis in terms of correlating quantitative DECT values to
PET SUVmax in patients with PA. Additionally, multivariate
analysis reveals that noncontrast images may contain important predictive information, an interesting finding that should
be followed up in future studies with larger sample sizes.
Enhanced DECT shows a moderate correlation with SUVmax
when measured on the area of the tumor correlating to the
highest SUVmax, or, if one wishes to avoid performing PET/
CT altogether, when taking three measurements in the greatest

832

areas of tumor heterogeneity (which is almost as effective in
mimicking PET). In the future, if these results are confirmed
in larger studies and the interface for acquiring this information is simplified, DECT could serve as a useful complement
or alternative to PET/CT imaging for PA.

Conclusion
We have demonstrated that DECT can, under certain conditions, serve as a surrogate for a PET scan in terms of estimating SUVmax. DECT thus has clinical potential as a surrogate
for, or as a complement to, PET in patients with PA. In this
case, DECT might be able to be used in place of PET in
locations without access to a PET scanner, or to improve
diagnosis and/or prognostication in centers with access to a
PET scanner. Additionally, the future use of more complicated
texture parameters may further improve the diagnostic and
prognostic ability of DECT, and may help to investigate
response to therapy for cancers where PET is presently used.

Conflict of Interest None.

References
1. National Comprehensive Cancer Network (NCCN). NCCN clinical
practice guidelines in oncology: pancreatic adenocarcinoma
[Internet]. Fort Washington (PA): National Comprehensive Cancer
Network; 2011 Dec 7 [updated 2013 Apr 9; cited 2012 Aug 2012].
Available from: http://www.nccn.org/professionals/physician_gls/
PDF/pancreatic.pdf
2. Montgomery DC, Peck EA, Vining GG. Introduction to linear regression analysis. 4th ed. Hoboken (NJ). Wiley-Interscience; c2006:
p. 110
3. Pery C, Meurette G, Ansquer C, Frampas E, Regenet N: Role and
limitations of 18F-FDG positron emission tomography (PET) in the
management of patients with pancreatic lesions. Gastroenterol Clin
Biol 34(8–9):465–474, 2010
4. Delbeke D, Martin WH: PET and PET/CT for pancreatic malignancies. Surg Oncol Clin N Am 19(2):235–254, 2010
5. Cameron K, Golan S, Simpson W, Peti S, Roayaie S, Labow D,
Kostakoglu L: Recurrent pancreatic carcinoma and cholangiocarcinoma: 18F-fluorodeoxyglucose positron emission tomography/
computed tomography (PET/CT). Abdom Imaging 36(4):463–471,
2011
6. Silva AC, Morse BG, Hara AK, Paden RG, Hongo N, Pavlicek W:
Dual-energy (spectral) CT: applications in abdominal imaging.
Radiographics 31(4):1031–1046, 2011
7. Graser A, Johnson TR, Bader M, Staehler M, Haseke N, Nikolaou K,
Reiser MF, Stief CG, Becker CR: Dual energy CT characterization of
urinary calculi: initial in vitro and clinical experience. Investig Radiol
43(2):112–119, 2008
8. Graser A, Johnson TR, Hecht EM, Becker CR, Leidecker C, Staehler
M, Stief CG, Hildebrandt H, Godoy MC, Finn ME, Stepansky F,
Reiser MF, Macari M: Dual-energy CT in patients suspected of
having renal masses: can virtual nonenhanced images replace true
nonenhanced images? Radiology 252(2):433–440, 2009

J Digit Imaging (2014) 27:824–832
9. Macari M, Spieler B, Kim D, Graser A, Megibow AJ, Babb J,
Chandarana H: Dual-source dual-energy MDCT of pancreatic adenocarcinoma: initial observations with data generated at 80 kVp and
at simulated weighted-average 120 kVp. AJR Am J Roentgenol
194(1):W27–W32, 2010
10. Mileto A, Mazziotti S, Gaeta M, Bottari A, Zimbaro F, Giardina C,
Ascenti G: Pancreatic dual-source dual-energy CT: is it time to
discard unenhanced imaging? Clin Radiol 67(4):334–339, 2012
11. Schmid-Bindert G, Henzler T, Chu TQ, Meyer M, Nance Jr, JW,
Schoepf UJ, Dinter DJ, Apfaltrer P, Krissak R, Manegold C,
Schoenberg SO, Fink C: Functional imaging of lung cancer using
dual energy CT: how does iodine related attenuation correlate with
standardized uptake value of 18FDG-PET-CT? Eur Radiol 22(1):93–
103, 2012
12. Hall M, Frank E, Holmes G, Pfahringer B, Reutemann P, Witten IH:
The WEKA data mining software: an update. SIGKDD Explor 11(1):
10–18, 2009
13. Hastie T, Tibshirani R, Friedman J, editors. The elements of statistical
learning: data mining, inference, and prediction. New York: Springer
Science and Illustration Media, Inc.; c2001. Chapter 3, Linnear
methods for regression; p. 41–78
14. Guyon I, Elisseeff A: An introduction to variable and feature selection. JMLR 3:1157–1182, 2003
15. Seo S, Doi R, Machimoto T, Kami K, Masui T, Hatano E, Ogawa K,
Higashi T, Uemoto S: Contribution of 18F-fluorodeoxyglucose positron emission tomography to the diagnosis of early pancreatic carcinoma. J Hepatobiliary Pancreat Surg 15(6):634–639, 2008
16. Koyama K, Okamura T, Kawabe J, Nakata B, Chung KH, Ochi H,
Yamada R: Diagnostic usefulness of FDG PET for pancreatic mass
lesions. Ann Nucl Med 15(3):217–224, 2001
17. Delbeke D, Rose DM, Chapman WC, Pinson CW, Wright JK,
Beauchamp RD, Shyr Y, Leach SD: Optimal interpretation of FDG
PET in the diagnosis, staging and management of pancreatic carcinoma. J Nucl Med 40(11):1784–1791, 1999
18. Lee SM, Kim TS, Lee JW, Kim SK, Park SJ, Han SS: Improved
prognostic value of standardized uptake value corrected for blood
glucose level in pancreatic cancer using F-18 FDG PET. Clin Nucl
Med 36(5):331–336, 2011
19. Haaga JR, Alfidi RJ, Zelch MG, Meany TF, Boller M, Gonzalez L,
Jelden GL: Computed tomography of the pancreas. Radiology 120:
589–595, 1976
20. Sheedy 2nd, PF, Stephens DH, Hattery RR, MacCarty RL:
Computed tomography in the evaluation of patients with
suspected carcinoma of the pancreas. Radiology 124(3):731–
737, 1977
21. Ros PR, Mortele KJ: Imaging features of pancreatic neoplasms. JBRBTR 84(6):239–249, 2001
22. Drabycz S, Stockwell RG, Mitchell JR: Image texture characterization using the discrete orthonormal S-transform. J Digit Imaging
22(6):696–708, 2009
23. Zhang Y, Zhu H, Mitchell JR, Costello F, Metz LM: T2 MRI texture
analysis is a sensitive measure of tissue injury and recovery resulting
from acute inflammatory lesions in multiple sclerosis. Neuroimage
47(1):107–111, 2009
24. Goh V, Ganeshan B, Nathan P, Juttla JK, Vinayan A, Miles KA:
Assessment of response to tyrosine kinase inhibitors in metastatic
renal cell cancer: CT texture as a predictive biomarker. Radiology
261(1):165–171, 2011
25. Wang H, Guo XH, Jia ZW, Li HK, Liang ZG, Li KC, He Q:
Multilevel binomial logistic prediction model for malignant pulmonary nodules based on texture features of CT image. Eur J Radiol
74(1):124–129, 2010
26. Yu H, Caldwell C, Mah K, Poon I, Balogh J, MacKenzie R,
Khaouam N, Tirona R: Automated radiation targeting in head-andneck cancer using region-based texture analysis of PET and CT
images. Int J Radiat Oncol Biol Phys 75(2):618–625, 2009

Concurrent Engineering:
A Framework and Implementation

Integrated Enterprise
1
Tong Wu,

Jennifer

Department of Industrial Engineering, Arizona
1

Department of Industrial Engineering,
2

Blackhurst2

State

and Peter

2,*
O’Grady

University, P.O. Box 875906, Tempe, Arizona 85287-5906,

Seamans Center,

University of lowa,

lowa

City,

USA

lowa 52242, USA

Received 5 June 2001
Abstract: Concurrent

Engineering (CE) is concerned with improving the product development process by the consideration, during the early
stages of product design, of the disparate factors associated with the life cycle of the product. An extensive body of research has emerged in
CE focused mainly on the internal operations of an enterprise. However, early participation of suppliers and customers in CE, in what may be
termed integrated enterprise CE, is generally considered to be of significant benefit. While integrated enterprise CE may be of considerable
advantage, there still remains the need for a methodology for modeling the integrated enterprise CE process, connecting suppliers,
manufacturers and customers as a seamless network. Such a methodology for modeling the integrated enterprise CE process will give insights
how to improve the system performance. What is giving added urgency to the need for such a methodology are the recent developments in
communications, primarily based on Internet technologies, that readily allow for the linking of information systems between organizations. This
paper aims to address this need for a methodology by describing an approach to modeling integrated enterprise CE, called Trans-Nets, that
allows for an abstraction of information, removing many of the complexities, while retaining the ability to analyze the more important
characteristics of the system. The computer implementation of the Trans-Nets approach, in a system called Trans, is then described. Finally,
an industry example is presented to illustrate how Trans-Nets can be applied to integrated enterprise CE.
on

1. Introduction
Concurrent Engineering (CE), also known as
Simultaneous Engineering or Life Cycle Engineering,
involves the design of products while taking into
account a wide variety of considerations, including
manufacturing, assembly, testing, maintenance, disposal, service, cost, quality, outsourcing, etc. It is a
cross-functional process that can help to increase
product quality/robustness, decrease the life cycle cost
and shorten the response time. A number of approaches
have been proposed to help designers with CE (O’Grady
and Young,
Park et al.,

1991; Young et al., 1992; Prasad, 1999;
1999) with a concentration on in-house

operations. Prasad (1996) grouped the agents influencing CE into seven categories: talents, tasks, teams,
techniques, technology, time and tools. There are several
methodologies reported on the implementation of CE,
as reviewed by O’Grady and Young (1991), and these
are generally categorized according to the tools used for
the implementation. Prasad (1997) classified CE techniques into six levels based on the degree of creativity

E-mail:

peter-ogrady@uiowa.edu;

http://www.iil.ecn.uiowa.edu/internetlab

and cooperation. The level with the lowest degree of
creativity and cooperation being network-based techniques, followed by documentation-based techniques,
variable-driven techniques, predictive techniques,
knowledge-based techniques, with agent-based techniques as having the highest degree of creativity and
cooperation.
Product development using CE can be carried out as
shown in the example in Figure 1. When an order for a
new product arrives from a customer, the manufacturer’s CE team works together on the product
development. This may trigger subsequent orders to
suppliers, who then invoke their own CE teams. In a
similar manner, this process then cascades down the
supply chain. The flow of orders passing from custom-

through manufacturers, to suppliers comprises
information jlo,,’, while the material floii, goes the
opposite direction. In practice, the process is much more
complex than that shown in Figure 1, with a considerable mesh of customers, manufacturers and suppliers.
One frequent outcome is an excessive time taken to
respond to customer demand. Adding to the problem is
that it is frequently perceived that customers have
become increasingly demanding, expecting higher levels
of product and service performance as, such effects as
the globalization of trade lead to greater competition
ers,

the

211
10-1 106/MNM9-8E48-W777-FCRR

212

Figure 1. Typical CE Operations.

and product innovation (Anderson and Lee, 1999). This
is reflected in, for example, the greater demand for the
customization of products and services to the needs of
the customers.
One consequence is the increased need for companies to
collaborate in product development. O’Brien (2000)
states that streamlining data and product flows throughout the supply chain has been one of the primary focus
areas for leading companies during the past decade.
Harley-Davidson Inc., for instance, reduced order-todelivery times by 30% with a corresponding 15%
reduction in overall supply-chain costs. However,
much of this work has been based on improving the
operation of existing supply chains to reduce response
times and costs for existing products rather than
including supply chain considerations in the design stage.
While much work has been done in a number of areas
of CE, the area of designing products taking the
suppliers and the supply chain into account (termed
integrated enterprise CE) remains relatively untouched.
This may be partly explained by the complexity inherent
in this area. This complexity arises from (Wu and

O’Grady, 2000):
~

The

.

large mesh

of interlinked suppliers, manufacdistributors.
The fact that each participant (supplier, manufacturer and distributor) may be a member of a large
number of other supply chains.
The dynamic nature of the supply chain.
turers and

~

~

key issue, therefore in integrated enterprise CE is the
development of a methodology that can model the
suppliers, manufacturers and customers as a seamless
network to interchange material and information. What
is giving added urgency to the need for such a
methodology are the recent developments in communications, primarily based on Internet technologies, that
provides the information pipeline structure which makes
suppliers, manufacturers and customers information

and allows an integrated enterprise to
have a unified mechanism for storage, control and
retrieval of the information and data relevant to the

sharing possible

product.
Thus, this paper addresses

two primary research
issues: Ii~ltat is a suitable approach to model Integrated
Enterprise Concurrent Engineering and how call this
methodology be ttllpletttettleft?
In this paper, we describe Trans-Nets, a variant of
Petri Net, to model integrated enterprise CE. This
allows for an abstraction of information, removing
many of the complexities, while retaining the ability to
analyze the more important characteristics of the
system. Based on Trans-Nets, a test bed system, called
&dquo;Trans&dquo;, is implemented. Trans can automatically
generate a design model from data contained in a
database and helps CE teams make changes in a

straightforward

manner.

The format of this paper is as follows. First, the basis
of Trans-Nets, including the definitions and operations,
is described. Then, the architecture of Trans is

illustrated with each component being described.
Finally, a case study from Rockwell Collins Inc., is
presented to show how the Trans system can be applied
in a practical environment.

2. Trans-Nets Basics

2.1 Nomenclature

A

1.) Information

set

A = Transition node set A
aj = Elements of the
ai = (Di, Fi)

C’= Attribute

=

(a,, a~, ...}

transition node set, a; E A,

set of place node iiii, C’ = {c~, c2, ...}
= Elements of place node (iiii) attribute set
D’= Attribute set of transition node n;, D’ --{d~, d2, ...}
= Elements of transition node
(ai) attribute set

cl.

d~

213

F~=Algorithm

set

of transition

node

ai,

F’ _

{.f i ~ .f2 ~ ...I

,

place

Elements of transition node (ai) algorithm set
Directed bipartite graph, G N U L
I’ = Input place node set of transition node a;
L Set of arcs, L
{II, l2, ...), L e A~ x A
/,== Elements of the arc set
Af =Place node set Af = {» y, ntZ, ...}I
/?!,= Elements of the place node set, m; E Af
Af’=Subset of place node set, such that M’ e M,

fG

indicated

by

small tokens in the circle

t

=

=

=

=

=

All

Ii U Oi
N=Set of nodes, N

DEFINITION 6 INPUT PLACE SUBSET
If an arc is from place node iiii to transition node
aj(11l¡ - aj), rn; is called an input place node. For a
transition node ay, all the input place nodes consist of
a place node subset called the input place subset IJ.
DEFINITION 7 OUTPUT PLACE SUBSET
is from transition node aj to place node
ay), nT; is called output place node. For a
transition node ay, all the output place nodes consist
of a place node subset called the output place subset O J.
Therefore, ATj Ij U oj, where Al j is a subset of the
place node set related to ay.
If

=

=

{nl, n2, ...), N = ~s U A

/?,= Elements of the node set, N
0’= Output place node set of transition node a;
T=Token set of Trans-Nets, T = (11, t2, ...}
//=The number of tokens of place node m;, li E

arc

nr;(nr;

f-

----

T

2.2 Definition of Trans-Nets

DEFINITION 9 TRANS-NETS
A Trans-Nets is a directed bipartite graph G. It
consists of a set of nodes N
{/!i,/!2,~..,~} and a set of
directed arcs L
{It, l2, ... , l&dquo;,}, G = N U L; the node
set N is decomposed into two disjoint subsets N~ and N
such that every arc in the graph joins a node in N, with
a node in N2 and that no arc joins nodes within the same
subset. N, is the set of place nodes and N2 is the set of
transition nodes.
=

=

DEFINITION 2 PLACE NODE
Let M
{11l1, 11l2,..., md be a finite set of objects 11l¡
and where, in turn, each m; is an object, consisting of
M is called
a Ipl tuple attribute set C‘ _ {c~, cl,...,
a place node set, 1Jl¡ E RT is called a place node and is
denoted by a circle in the graphical representation.
=

eP}.

DEFINITION 8 TRANS-NETS STRUCTURE
A quadruple TN= (AT, A, L, n is called a Trans-Nets
structure if, and only if, AI, A, L, and T follow the above
definitions.
An example is shown in Figure 2. It consists of 4 place
nodes, AI ={/?!], 7~2~3~&dquo;4L 3 transition nodes, A =
{~,~2~3L and a set of arcs, L = {(rn, ~ a~), (/)~ +* a2),

(l112 H ad, (~~3
The

marking

a2), ~&dquo;2

+*

is

DEFlNI TION 3 TRANSITION NODE
Let A = {al, ~2,..., al) be a finite set of objects, aj and
where, in turn, each aj consists of a Iql tuple attribute set
Dj.= (dl, d2, ... ,
and a II’I tuple algorithm set F! _
( f ~, ./2~ - - -ff }. a~ (Dj, F~). A is called transition node
set, each aj E A is called a transition node and denoted
by a solid bar in the graphical representation.

<I§)

=

2.3 Trans-Nets

Let L---.{(rrt,a)~ntEMaC_AT, crEA}c(MxA)be
a finite set of arcs connecting place nodes and transition
nodes. Each (m, a) E L is denoted by a uni-directed or
a bi-directed arc in the graphical representation.
DEFINlTION 5 MARKING ATTRIBUTE
Let T be a K-vector, T = (ti, t2, ... , t~). T is called
marking and is a function from At to nonnegative

integers,

such that, for all k, t~ : m~ ~ {0,1, 2, ...}. In
graphical representation, a marking t~ =t is

a3), (’&dquo;3 ~ a3), (11l4

f-

~3)}.

Operations

DEFINITION 9 ENABLED TRANSITION
Consider a Trans-Nets 7W=(A~,L,r), where
function T [A (/; 0)] refers to the markings of the
place node for the specific transition node. Thus, a
transition node, aj, is enabled if and only if,
e

Ij, ~ti[nl(I~, ~~)] ~

1

DEFINITION 10 TRANSITION FIRING
Any transition node in a Trans-Nets can fire if it is
enabled. Firing a transition node, ay, results in firing the
algorithm set F’, updating the attribute sets, and new
marking distribution T’. The firing algorithm is
illustrated

as

in Table 1.

Character_Update
DEFINITION 4 ARC

---~

~={1,0,0,0}.

VI11¡

the

representing

node I1lk.

transition
transition

algorithm

{rrrq, I’)
set.

is fulfilled based on the
For example, if there is a

node ay with algorithm set Fi~~ , f2 ,.. fl),
~=E(.~-2,...)(~,2,...~), attribute D~~,...~),
node »i; with attribute
result is:

place

/

~4 i

C’ ~

= Fi~

’cj’

C~,c!,,...c-),
~~

then, the

’~’B

C~ Cr 2 ... C~ C’~ Di d’
z

a

cp c~ ~- -~-/

214

Figure 2.
Table 1. Transition

An

Example Trans-Nets Structure.

Firing Procedure

to model relatively complex systems by abstracting
essential information. A prototype implementation
system of the Trans-Nets methodology (called Trans)
has been developed. This section gives an overview of
this implementation by a description of the Trans

system.
3 shows the architecture of Trans, which
of
three main modules: Information Enabler,
consists
Enabler
and Trans Engine. The Information
Design
Enabler acts as a conduit for information; the Design
Enabler allows CE team to design products interactively ; while the Trans Engine implements the TransNets algorithms. Each of these modules is described
below.

Figure

3.1 Information Enabler
The Information Enabler module acts as an information conduit, using Internet technologies to allow
information to pass between suppliers, customers and
the CE team, while also linking to corporate databases.
There are two main components in the Information
Enabler module: the Internet Gateway and the

Database.
k =

I, 2, ... , l that is,

C’<Ci =.f~ ~C1 ~~~ ~~ ... , Ck(c1), Dl {di ))

-=Cl~~i)+...~..C~~~~~+D’{di)
Ci(C2) =l’(C~(c?~~...,C~~~), D~(rt2))
= c1 c1) ...~.....~.. C~ (~ + Dj(d1)
.

3. 1.1 IIVTERNET GA TEWA Y
The Internet Gateway provides an interface to allow
suppliers and customers to interact with the Trans
system. Its main functions are:
9

o

2

Ci(cp) = fp (C1 ~cp?, ... , ck(c1), D’ ~dp~)
=

c1 (C§)

+ ... +

C~~ p~ + j]~~dp}
9

3. Trans -

Prototype Implementation

The above has described the basis for Trans-Nets, an
the potential

abstracting network methodology that has

so that only authorized users can access the system.
Product Catalog (Figure 4, right): if the user is
identified as an authorized supplier, its supply record
is presented just above the product catalog, with the
status of each product shown (e.g., in process,
completed etc.). The user can specify a product and
view detailed design information. For example, if the
product 634-1161-001 is selected, the screen shown in
Figure 5 will be shown.
Design Web: Figure 5 shows the design web of
product 634-1161-001. The part number is the
identification of the part; the description indicates
the category (e.g. product, sub component, raw
material, tool/fixture, NC program, silk screen,

Security Login (Figure 4, left):

215

Figure

3. The Architechture of Trans.

Figure 4.

*

Trans Internet

Gateway (1).

etc.); the sotirce indicates the suppliers for outsourced

3.1.2 DATABASE

components, otherwise it is defined

In a realistic distributed environment, data will be
stored in distributed databases. An integrated enterprise
CE system would therefore need to interface to multiple
databases. The database aspect in Trans is implemented
as ODBC connections that have the capability of
interfacing to multiple databases. Data can be taken
from these multiple distributed databases and copied to
the Trans central database. In this central database, data
are sorted into categories such as security, order,
iii-hoitse and outsource (as shown in Figure 6). Security
includes information to validate the request, such as
the identification, password and limits of authority.

as

in-house; the

lead-time is the overall response time; the on hand
indicates the inventory level; on order- indicates what
has been ordered.
Information Update: The Extensible Markup
Language (XML) is used for data interchange
between Trans and suppliers/customers. XML is
a flexible way to create common information
formats and share both the format and the data. It
can be used by suppliers and customers sharing
information in a consistent way on a variety of

computer systems.

216

Design Web (Product: 634-1161-001)
Figure 5. Trans Internet Gateway (II).

Figure 6.

Order involves the information about

Structure of Trans Central Database.

customers

and

product orders. IIl-hollse consists of information on
existing designs and components available in-stock,
while the layout of existing designs, attributes and
inventory information of each component are provided
within the existing design portion, and the list of
available components, attribute and inventory of each
component are provided within in-house components.
OWSOllrce consists of information about suppliers and
outsourced components.

3.2 DESIGN ENABLER
The Design Enabler allows the CE team to analyze
and customize designs. Two operation modes are
available in the Design Enabler: Automatic and
Manual.

3.2.1 AUTOlVIATIC MODE
The Automatic Mode allows the Trans-Net for
an existing design to be generated automatically from
the data contained in the Trans central database.

217

Figure

7. Trans

Design Enabler in Automatic Mode.

user invokes Automatic Mode and selects a specific
product design in the central database. Trans then
automatically generates the tree structure from the
system layout table (Figure 7, left) and generates the
design as a Trans-Net graphical representation (Figure 7,
right). Each component in the representation has its
associated properties (from the central database)

4. Case

The

attached.
Automatic Mode therefore allows the design team to
build relatively easily the Trans-Net representation of an
existing design. This design can be modified by the
designers invoking manual mode, described in the
following section.

Study

This section describes a case study that applies the
Trans-Net approach, and the Trans system, to integrated enterprise CE. It should be noted that some
details have been omitted or simplified to reduce the
length of the case study.

4.1

Background

below).
The Design Enabler in manual mode therefore allows
designers to build up new design, or alter existing
designs, in a straightforward manner.

This case study concerns the Rockwell Collins
Fabrication Department in Cedar Rapids, Iowa.
Rockwell Collins develops and produces advanced
communication and aviation electronics. This includes
equipment for flight management communication,
navigation, cabin management, entertainment and
flight control systems. In addition, Rockwell Collins
provides military airborne and ground based communication and navigation systems.
Approximately 12,000-14,000 work orders are completed each year with each order having a median order
size of 22 parts. Fabrication at Rockwell Collins can
therefore be considered to be a job-shop environment,
where there is a preponderance of high mix - low
volume work. However, the materials and components
are obtained from suppliers and, hence, a consideration
of suppliers in the design process is of importance.
Lead time is the major issue for Fabrication at
Rockwell Collins and will therefore be the main
criterion used for evaluating designs.

3.3 Trans

4.2

3.2.2 MANUAL MODE
Manual mode is applicable for both starting a new
design and making changes to the design model
obtained using automatic mode. Users invoke manual
mode and can then &dquo;drag and drop&dquo; nodes to produce
a graphical representation of the design. When the
user drops a new node on the design, a window of
available database records pops up (Figure 8, above).
The user can make a selection, update the database
record, and record and refresh attributes. Users can
also specify the node, then click on the right mouse
button to customize the attributes of the node (Figure 8,

Engine

Example - Product 634-1309-001
Design

Initial

Engine implements the Trans-Nets algorithms,
the
attributes of each node and the database
updates
records accordingly. Furthermore, the Trans Engine can
be used to identify the system problem areas.
Trans

Product 634-1309-001 is an antenna that is manufactured from raw materials procured from suppliers
and this product will be the focus of this case study.

218

Figure

8. Trans

Design Enabler in Manual Mode.

The initial

design data on suppliers and in-house manufacturing processes are shown in Table 2. This gives the
component identification (ID), Description, Source and
Lead-Time. A Desci-iptioii indicated as a &dquo;Transition&dquo;
refers to a manufacturing procedure, usually carried out
within Fabrication

as an in-house process, and Lead.
Time refers to the process time. The lead-time of

suppliers includes procurement time, in-house purchasing process time (3 days) and in-house inspection and
stock location assignment time (2 days). An additional
time delay in Fabrication is the Process Engineering
Delay. This is discussed in more detail later in this paper.
4.3 Trans-Net Analysis of Initial
Using Trans

Design

F’ _

{ f~ },

is associated with

calculating

the lead-

times, where

.1i (C’ (~i )~ ... C~ (c; ), D’(d; )) C*(en + Di(dD,
C*(c*) =Max ( C 1( c i) +...+C ~( c i))
=

(1)

4.3.1 IIVVOKE AUTOMATIC MODE
The process of evaluating the existing design and
determining improvements can be started by invoking
automatic mode in the Trans system (described earlier),
whereby the Trans-Nets representation can be built
automatically from the data. If this is done using the
data in Table 2, the result is as shown in Figure 9, which
shows the multiple tiers: 2nd tier, lst tier and top tier
shown in Figure 9).
The result is a schematic of the overall design of Product 634-1309-001 that can now be adjusted manually.

(as
This

study is concerned with determining
improvements to the initial design of Product 6341309-001. Since lead time is the main concern, Ci = (cb )}
where c’ represents the lead-time at each place node (in
calendar days). In terms of transition nodes, D&dquo; = {di}
where di represents the lead-time invoked at that
transition node (in calendar days), the algorithm,
case

4.3.2 INVOKE MANUAL MODE
In this particular case, Fabrication does not order
1st tier raw material until 1st tier sub-components
(Y634-1309-004H and Y634-1309-OIOH) are completed.

219
Table 2. fi34-1309-001 Data

Figure

9. Trans-Nets Structure of 634-1309-001.

In other words, raw material will not be ordered until all
lower level sub-components (if any) are complete. This
additional delay, called the Process Engineering Delay,
is not indicated in the data in Table 2 and Manual Mode
can be invoked to add this additional delay.
Figure 10 shows the revised Trans-Nets structure of
product 634-1309-001 that has an additional transition
node called &dquo;Process
the necessary linking

Engineering Delay&dquo; together with
arcs. For this Process Engineering

Delay

transition node:

D* # (dQ) # 0
F* = ( fj*) is associated
the

with

calculating

lead-times, where

{C’~~i~ =.fi ~C~ {~i?~ ... C~~ci~~ Ct~~i~~ D’M))~~
MaxC1 (c( ) + ... + C~(c§)) + Cr~~i~
=

(2)

220

.

Figure

-. - ---- - ---&dquo;..--

10. Revised Trans-Nets Structure of 634-1309-001.

4.3.3 RUNNING TRANS ENGINE
The Trans Engine portion of Trans can now be used
to determine the attribute set for each place node. This is
done automatically in Trans and gives the total lead
time for the top tier of c;
73days, being the total leadtime for Product 634-1309-001.
According to the Trans-Nets algorithm, the contribution of each place node or transition node to the overall
lead-time being considered can be determined as a
proportion of the total for a particular node, calculated
layer by layer. For example, in the top tier level, the
overall lead-time of product 634-1309-001 is made of
contributions from Y634-1309-005H and Transitionl.
The lead-time proportion of Y634-1309-005H is:
=

=

~

Lead-Time{Y634-1309-001)/
Lead-Time{634-1309-001)

in this case, 80%.
The Trans Engine can identify the key contributors
and generate a report (as shown in Figure 11) showing
the contribution to the lead time at each tier layer.
4.3.4 POSSIBLE IMPROVEMENT l:
EFFECTIVE INFORMATION SHARING
Thus far, the Trans system has determined the
structure of the design (using both Automatic and

Manual

modes) and has calculated, in a relatively
straightforward manner, the overall lead-time of
the final product 634-1309-001 (73 calendar days)
and identified the main contributors to this. From
Figure 11, it can be seen that the delivery delay of
R804-1273-543 makes a significant contribution to the
long lead-time. The data in Table 2 indicates the
material delivering time of R804-1273-543 is only 17
days. However, due to the Process Engineering Delay,
the procurement of R804-1273-543 from Benjamin
Metals will not start until all the sub-components of
2nd level are available (Figure 10), in this case, 35 days.
The application of Trans system in Fabrication has
the potential to improve the solution by using the
Internet Gateway to make information visible to
suppliers, who then could be proactively involved in
the process and deliver the parts/components as the 2nd
components become available. By doing this, the
Process Engineering Delay lead-time will be eliminated,
which will then change the original design shown in
Figure 9. The Trans system can now be rerun to
determine the attributes of the revised design. The result
is a lead-time of 55 days, a reduction of 18 days - a 25%
reduction in lead-time. A new report can be generated to
indicate the lead time contributions of the revised

design.

221

Figure

11. Lead Time Contribution

that further improvements could be
by introducing Trans to the design engineers
(outside the Fabrication department). With information
sharing using the Internet Gateway in the Trans system,
Let
made

us assume

Fabrication and raw material vendors will have
information about orders and will be able to order
raw materials and supplies as needed. In the ideal case,
the lead-time delays associated with raw materials are
eliminated, the revised design will then be 37 days,
giving another 18 days reduction, representing a 49%
reduction in lead time.
The use of Trans has thus far, therefore, allowed the
main contributors to lead-time to be identified and for
the effects of changes in the material ordering process to
be identified. This can result in the lead-time being
reduced from the original of 73 days to 37 days, an
overall reduction of 49%.
4.3.5 POSSIBLE IMPROVEMENT ll:
I N-HO USE DESI GN REVI SION
Trans also allows for a concurrent design process with
the capability to reevaluate alternate suppliers or
alternate materials/components. After the analysis and
improvements described above, the main contributors
to the lead-time are now the 1 st tier components
Y634-1309-004H and Y634-1309-OIOH. If the design
can be improved by combining these components
together by manufacturing both from a single plate
R804-1163-436 (AL 6061-T6) supplied by Copper+
Brass Sales. If the manufacturing process time for this
combined design is 7 days, the lead time for the revised
design will then be further reduced from 37 days to 27
days, representing an overall reduction of 63% compared with the original design.
Additionally, the information visibility provided by
Trans can help Fabrication plan the shop load and
better manage work loading at capacity and scheduling
so that manufacturing time could be reduced. The
application of Trans would have the potential to allow
Fabrication to manage the flow of work thereby
reducing queues and decreasing throughput time.

Report.

This industry example has shown how Trans can be
used to identify the main areas where improvements can
be carried out and to provide an information sharing
framework for designers and others to exchange information. In this example, Rockwell Collins identified leadtime as the main attribute to be reduced. Other companies
may want to include other aspects, such as cost, and these
other factors can be readily included in the Trans system.

5. Conclusion

This paper has described the Trans-Nets networkbased approach to modeling integrated enterprise CE
and the

application to an industry example using a
computerized implementation of the Trans-Nets
approach, in a system called Trans. The Trans-Nets
approach models the enterprise as an abstracted network, removing many of the complexities, while
retaining the ability to analyze the more important
characteristics of the system The basis of Trans-Nets,
including the definitions and operations, has been
detailed. Then, the implementation of the Trans-Nets
approach, in a system called Trans, has been described.
Finally, an industry example has been presented to
illustrate how Trans-Nets can be applied to integrated
enterprise CE. In this example, the company was mainly
concerned with reducing lead-times and this criterion
was used in the subsequent analysis.
The industry example illustrates the operation of Trans
showing the relative ease by which the Trans-Nets
representation of the design can be built from information in the Trans central database using both the
Automatic and Manual modes of Trans. The Trans
engine was then invoked to identify the main contributors
to the lead-time and various improvements to the system,
the supply chain and the design were identified. The
overall lead-time in the industry example was reduced
from 73 to 27 days, with further reductions identified.
The Trans-Nets approach, as implemented in the
Trans system, allows for information sharing of

222

the supply chain of a
company, and the automatic identification of problem
areas. The potential overall benefits to organizations can
be substantial.

engineering programs throughout

References
D. and Lee, H. (1999). Synchronized Supply
Chains: The New Frontier, Achieving Supply Chain
Excellence Through Technology (ASCET) Forum with

Anderson,

Andersen

Consulting,

available at

http://www.ascet.com

O’Brien, K. (April, 2000). Value-Chain Report, Industry
Week, available at http://www.bettermanagement.com/

supplychainauthority/library/articles.asp
O’Grady, P. and Young, R.E. (1991). Issues in concurrent
engineering systems.
Journal of Design and Manufacturing
,
1(1): 27-34.

Park, H. and Cukodky, M.R. (1999). Framework for modeling

dependencies in collaborative engineering process. Research
in Engineering-Theory Applications and Concurrent
Engineering, 11(2): 84-102.
Prasad, B. (1999). Enabling principles of concurrency and
simultaneity in concurrent engineering, Artificial Intelligence for Engineering Design Analysis and Manufacturing,
13(3): 185-204.
Prasad, B. (1996). Concurrent Engineering Fundamentals
,
Volume I. New Jersey: Prentice Hall.
Prasad, B. (1997). Concurrent Engineering Fundamentals,
Volume II. New Jersey: Prentice Hall.
Wu, T. and O’Grady, P. (2000). A Network Based Approach
to Integrated Supply Chain Design, Technical Report,
available at http://www.iil.ecn.uiowa.edu
Young, R.E., Greef, A. and O’Grady, P. (1992). An Artificial
Intelligence-based Constraint Network System for Concurrent Engineering. International Journal of Production
Research, 30
(7): 1715-1735.

1328

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 35,

NO. 6,

JUNE 2013

A Sparse Structure Learning Algorithm for
Gaussian Bayesian Network Identification
from High-Dimensional Data
Shuai Huang, Jing Li, Jieping Ye, Adam Fleisher, Kewei Chen, Teresa Wu, Eric Reiman, and
the Alzheimer’s Disease Neuroimaging Initiative
Abstract—Structure learning of Bayesian Networks (BNs) is an important topic in machine learning. Driven by modern applications in
genetics and brain sciences, accurate and efficient learning of large-scale BN structures from high-dimensional data becomes a
challenging problem. To tackle this challenge, we propose a Sparse Bayesian Network (SBN) structure learning algorithm that employs
a novel formulation involving one L1-norm penalty term to impose sparsity and another penalty term to ensure that the learned BN is a
Directed Acyclic Graph (DAG)—a required property of BNs. Through both theoretical analysis and extensive experiments on
11 moderate and large benchmark networks with various sample sizes, we show that SBN leads to improved learning accuracy,
scalability, and efficiency as compared with 10 existing popular BN learning algorithms. We apply SBN to a real-world application of
brain connectivity modeling for Alzheimer’s disease (AD) and reveal findings that could lead to advancements in AD research.
Index Terms—Bayesian network, machine learning, data mining

Ç
1

INTRODUCTION

A

Bayesian network (BN) is a graphical model for
representing the probabilistic relationships among
variables. BNs have been widely used in the fields of
genetics [1], [2], ecology [3], [4], social sciences [5], medical
sciences [6], brain sciences [7], [8], and manufacturing [9]. A
BN consists of two components: the structure, which is a
Directed Acyclic Graph (DAG), for representing the
dependency and independency among variables, and a set
of parameters for representing the quantitative information
of the dependency. Accordingly, learning a BN from data
includes structure learning and parameter learning. This
paper focuses on structure learning.
One type of structure learning method is constraint
based. Constraint-based methods [10], [11], [12], [13], [14]
use conditional independence tests to identify the dependent and independent relationships among variables. A
major weakness of these methods is that too many tests may
have to be performed, with each test being built upon the
results of another, leading to escalated errors in the BN
structure identification.
Another type of structure learning method is score based,
in which a “score” is defined for each possible BN structure
and then a search algorithm is used to find the structure
. S. Huang, J. Li, J. Ye, and T. Wu are with the School of Computing,
Informatics, and Decision Systems Engineering, Arizona State University,
PO Box 878809, Tempe, AZ 85287-8809. E-mail: jinglz@asu.edu.
. A. Fleisher, K. Chen, and E. Reiman are with the Banner Alzheimer’s
Institute, 1111 E. McDowell Road, Phoenix AZ 85006.
Manuscript received 23 Dec. 2011; revised 10 Apr. 2012; accepted 12 May
2012; published online 31 May 2012.
Recommended for acceptance by A.J. Storkey.
For information on obtaining reprints of this article, please send e-mail to:
tpami@computer.org, and reference IEEECS Log Number
TPAMI-2011-12-0921.
Digital Object Identifier no. 10.1109/TPAMI.2012.129.
0162-8828/13/$31.00 ß 2013 IEEE

with the highest score. Various score functions have been
proposed, including those based on the Bayesian method
[15], [16], [17], [18], [19], minimum description length [20],
[21], [22], [23], and entropy [10], [24]. Furthermore, once a
score function is specified, a search method is needed to find
the structure with the highest score. Because the number of
possible structures grows exponentially with respect to the
number of variables, an exhaustive search over all possible
structures may be computationally too expensive or unfeasible. Therefore, various inexact search methods have
been proposed, such as heuristic search techniques [15],
[24], [25], [26], genetic algorithms [28], [29], and simulated
annealing [30]. Sampling methods such as Markov Chain
Monte Carlo (MCMC) [18], [24] have also been utilized to
travel through the DAG space. These methods usually find a
BN structure that is a local optimum, and have been less
effective in high-dimensional DAG spaces. In addition,
some work has been done to combine score-based methods
with constraint-based methods [31]. Then there is the
recently developed novel additive noise model [32], which
differs from both constraint-based and score-based methods
and has the advantage of learning nonlinear interactions for
non-Gaussian BNs.1
Driven by modern applications in brain sciences and
genetics, there has been a great need of algorithms capable
of learning large BN structures with high accuracy and
efficiency from limited samples. For example, BNs provide
an effective tool for identifying how different brain regions
interact with each other in task performance, skill learning,
1. Data used in preparation of this paper were obtained from the
Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). As such, the investigators within the ADNI contributed to the
design and implementation of ADNI and/or provided data but did not
participate in the analysis or writing of this report. A complete listing of
ADNI investigators can be found at: http://adni.loni.ucla.edu/wpcontent/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.
Published by the IEEE Computer Society

HUANG ET AL.: A SPARSE STRUCTURE LEARNING ALGORITHM FOR GAUSSIAN BAYESIAN NETWORK IDENTIFICATION FROM HIGH-...

and disease processes from neuroimaging data [7], [8]. A
typical neuroimaging dataset includes hundreds of variables (brain regions), while the sample size (number of
experimental subjects) is usually in tens. Also, BNs are very
useful for modeling the interacting patterns between genes
from microarray gene expression data, which measures
thousands of genes with sample size being no more than a
few hundred [1], [2].
For the purpose of learning a large BN with small sample
sizes, a useful strategy is to impose a “sparsity” constraint
of some kind. Many real-world networks are indeed sparse,
such as the gene association networks [1], [33] and brain
connectivity networks [34]. When learning the structure of
these networks, a sparsity constraint helps prevent overfitting and improves computational efficiency. For example,
the Sparse Candidate (SC) algorithm [35], one of the first
large-scale BN structure learning algorithms, achieves
sparsity by assuming that the maximum number of parents
for each node is limited to a small constant. One major
problem with SC is that the user has to guess the maximum
number of parents. Also, it is usually unrealistic to assume
that all the nodes have the same maximum number of
parents. The L1MB-DAG algorithm [36] does not require a
prior specification on the maximum number of parents.
Instead, it uses LASSO to select a small set of potential
parents for each variable. LASSO is known for sparse
variable selection [37].
In addition to the sparsity consideration, recently
developed BN structure learning methods usually consist
of two stages: Stage 1 is to identify the potential parents of
each variable; Stage 2 applies some search methods to
identify the parents out of the potential parent set. The
advantage of the two-stage approach is improved efficiency, as Stage 2 is a local search over a possibly small set
of potential parents for each variable identified by Stage 1,
rather than a global search over all the variables. The twostage approach has been popularly adopted by many
existing algorithms, including the SC and the L1MB-DAG
algorithms, mentioned previously, as well as the HillClimbing (MMHC) [38], the Grow-Shrink [39], the TC, and
the TC-bw [40] algorithms. The difference between these
algorithms primarily lies in how they identify the potential
parent set in Stage 1. For example, L1MB-DAG uses LASSO,
MMHC uses the G2 statistic, and TC and TC-bw use a t-test.
An apparent weakness of the two-stage approach is that if a
true parent is missed in Stage 1, it will never be recovered in
Stage 2. Another weakness of the existing algorithms is that
the computational efficiency is still too low for learning
large BNs. For example, it may take hours or days to learn a
BN with 500 nodes.
In this paper, we propose a new sparse Gaussian BN
structure learning algorithm called Sparse Bayesian Network (SBN). It is a one-stage approach that identifies the
parents of all variables directly, thus having a low risk of
missing parents (i.e., a high accuracy in BN structure
identification) compared with many existing algorithms
that employ the two-stage approach. Specifically, in development of the SBN, we propose a novel formulation with
one L1-norm penalty term to impose sparsity and another
penalty term to ensure that the learned BN is a Directed
Acyclic Graph—a required property of BN. The theoretical
property about how to select the regularization parameter
associated with the second penalty term is discussed. Under

1329

Fig. 1. A Bayesian network structure (DAG).

this formulation, we propose to use the Block Coordinate
Descent (BCD) and shooting algorithms to estimate the BN
structure. Further, our theoretical analysis indicates that the
computational complexity of SBN is linear in the sample
size and quadratic in the number of variables. This
characteristic makes SBN more scalable and efficient than
most existing algorithms, and thus well suited for largescale BN structure learning from high-dimensional datasets.
In addition, we perform theoretical analysis to show why
the two-stage approach popularly adopted in the existing
literature has a high risk of misidentifying the true parents
and how the proposed SBN overcomes this deficiency. Also,
extensive experiments on synthetic data are performed to
compare SBN and the existing algorithms in terms of the
learning accuracy, scalability, and efficiency. Finally, we
apply SBN to a real-world application of brain connectivity
modeling for Alzheimer’s disease (AD). In particular, SBN
is applied to the neuroimaging PDG-PET data of 42 AD
patients and 67 matching normal control (NC) subjects in
order to identify the brain connectivity model for each of
the two study groups. A connectivity model represented by
a BN reveals the directional effects of one brain region over
another—called the effective connectivity. Effective connectivity has been much less studied in the AD literature, as
most existing work focuses on functional connectivity, i.e.,
the correlations among brain regions. In this sense, the
application of SBN to AD has the advantage over undirected graphical models of providing new insights into the
mechanisms/pathways that distinct brain regions communicate with each other. In this application, the effective
connectivity model of AD identified by SBN is compared in
many different ways with that of NC, including the
connectivity at the global scale, intra/interlobe and interhemisphere connectivity distribution, and the connectivity
associated with specific brain regions. The findings are
consistent with known pathology and the clinical progression in AD.
The rest of the paper is organized as follows: Section 2
introduces the key definitions and concepts of BN. Section 3
presents the development of SBN. Section 4 performs a
theoretical analysis on the competitive advantage of SBN over
the existing algorithms that employ the two-stage approach.
Section 5 presents the results of the experiments on synthetic
data. Section 6 presents the application of SBN to brain
connectivity modeling of AD. Section 7 is the conclusion.

2

BAYESIAN NETWORK: KEY DEFINITIONS AND
CONCEPTS

In this section, we give a brief introduction to the key
definitions and concepts of BNs:
A BN is composed by a structure and a set of parameters.
The structure (Fig. 1) is a DAG that consists of p nodes

1330

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

½X1 ; . . . ; Xp  and directed arcs between some nodes; no cycle
is allowed in a DAG. Each node represents a random
variable. In this paper, we will use nodes and variables
interchangeably. The directed arcs encode the dependent
and independent relationships among the variables. If there
is a directed arc from Xi to Xj , Xi is called a parent of Xj and
Xj is called a child of Xi . Two nodes are called spouses of each
other if they share a common child. If there is a directed path
from Xi to Xj , i.e., Xi !    ! Xj , Xi is called an ancestor of
Xj . A directed arc is also a directed path and a parent is also
an ancestor according to this definition. The Markov Blanket
(MB) of Xj is a set of variables and, given this set of variables,
Xj will be independent of all other variables. The MB consists
of the parents, children, and spouses of Xj .
In this paper, we will adopt the following notations with
respect to a BN structure: We denote the structure by a p  p
matrix G, with entry Gij ¼ 1 representing a directed arc
from Xi to Xj and Gij ¼ 0, otherwise. The set of parents of a
node Xi is denoted by PAðXi Þ. In addition, we define a
p  p matrix, P, which records all the directed paths in the
structure, i.e., if there is a directed path from Xi to Xj , entry
Pij ¼ 1; otherwise, Pij ¼ 0.
In addition to the structure, another important component of a BN is the parameters. The parameters are the
conditional probability distribution of each node given its
parents. Specifically, when the nodes follow a multivariate
normal distribution, a regression-type parameterization can
be adopted, i.e., Xi ¼  Ti PAðXi Þ þ "i with "i  Nð0; 2i Þ and
 i being a vector of regression coefficients. Without loss of
generality, we assume in this paper that the nodes are
standardized, i.e., each with a zero mean and unit variance.
Then, the parameters of a BN are B ¼ ½
 1 ; . . . ;  p .

3

THE PROPOSED SPARSE BN STRUCTURE
LEARNING ALGORITHM—SBN

One of the challenging issues in BN structure learning is to
ensure that the learned structure must be a DAG, i.e., no
cycle is present. To achieve this, we first identify a sufficient
and necessary condition for a DAG
Lemma 1. A sufficient and necessary condition for a DAG is
ji  Pij ¼ 0 for every pair of nodes Xi and Xj .
Proof. To prove the necessary condition, suppose that a BN
structure, G, is a DAG. Let’s assume that  ji  Pij 6¼ 0
for a pair of nodes Xi and Xj . Then, there exists a
directed path from Xj to Xi and a directed path from Xi
to Xj , i.e., there is a cycle in G which is a contradiction to
our presumption that G is a DAG. To prove the sufficient
condition, suppose that  ji  Pij ¼ 0 for every pair of
nodes Xi and Xj . If G is not a DAG, i.e., there is a cycle, it
means that there exist two variables, Xi and Xj , with a
directed arc from Xj to Xi (
 ji 6¼ 0) and a directed path
from Xi to Xj (Pij ¼ 1). This is a contradiction to our
presumption that  ji  Pij ¼ 0 for every pair of nodes Xi
and Xj .
u
t
Based on Lemma 1, we further present our formulation
for sparse BN structure learning. It is an optimization
problem with the objective function and constraints given by

^ ¼ min
B
B

VOL. 35,

NO. 6,

JUNE 2013

(

T )
p
X
x i   Ti x =i x i   Ti x =i =2
i¼1

 i k1
þ1 k

;

ð1Þ

s:t: ji  Pij ¼ 0; i; j ¼ 1; . . . ; p; i 6¼ j:
According to the definition of P, P is a function of B. So
the constraints in (1) are functions of B. The notations in (1)
are explained as follows: x i ¼ ½xi1 ; . . . ; xin  denote the sample
vector for Xi , where n is the sample size. x =i denotes the
sample matrix for all the variables except Xi . The first term in
P
x i   Ti x =i Þðx
x i   Ti x =i ÞT =2g, is
the objective function, pi¼1 fðx
a profile likelihood to measure the model fit. In the second
term, k
 i k1 is the sum of the absolute values of the elements
in  i and thus is the so-called L1-norm penalty [37]. The
regularization parameter, 1 , controls the number of nonzero
elements in the solution to  i , ^ i ; the larger the 1 , the fewer
nonzero elements in ^ i . Because fewer nonzero elements in
^ i correspond to fewer arcs in the learned BN structure, a
larger 1 results in a sparser structure. In addition, the
constraints are to assure that the learned BN is a DAG (see
Lemma 1 and Theorem 1 below).
Solving the constrained optimization in (1) is difficult.
Therefore, the penalty method [42] is employed to transform it into an unconstrained optimization problem,
through adding an extra L1-norm penalty into the objective
function, i.e.,
^ ap ¼ min
B
B

p
X

fi ð
iÞ
i¼1 8



T 9
T
T
p >
=
< x i   i x =i x i   i x =i =2 >
X
X
¼ min
 i k1 þ 2
jji  Pij j >;
B
> þ1 k
;
i¼1 :


ð2Þ

j X=i

 X=i denotes that the variable indexed by j, i.e.,Xj ,
where j
P
is a variable different from Xi . Here, 2 j
 X=i jji  Pij j is
to push ji  Pij to become zero. Under some mild
conditions [42], there exists a 2 such that for all 2 	 2 ,
^ ap is also a minimizer for (1). Later, in Theorem 1, we will
B
show how to derive a practical estimation for 2 .
Given 1 and 2 , the BCD algorithm [43] can be
employed to solve (2). The BCD algorithm updates each
 i iteratively, assuming that all other parameters are fixed.
 i Þ in
In our situation, this is equivalent to optimizing fi ð
(3) iteratively and the algorithm will terminate when some
convergence conditions are satisfied. We remark that
 i Þ, after some transformation, is similar to LASSO
fi ð
[37], i.e.,



T
 i Þ ¼ x i   Ti x =i x i   Ti x =i =2
fi ð
X
ð1 þ 2 jPij jÞjji j:
þ

ð3Þ

 X=i
j

As a result, the shooting algorithm [44] for LASSO may
 i Þ in each iteration. Note that at
be used to optimize fi ð
 i Þ, we also need to
each iteration for optimizing fi ð
 X=i . This can be done by a Breadthcalculate Pij for j
first search on G with Xi being the root node [45]. A more
detailed description of the BCD algorithm and the shooting
algorithm used to solve (3) is given in Figs. 2 and 3,
respectively.

HUANG ET AL.: A SPARSE STRUCTURE LEARNING ALGORITHM FOR GAUSSIAN BAYESIAN NETWORK IDENTIFICATION FROM HIGH-...

1331

Fig. 2. The BCD algorithm used for solving (2).

Fig. 3. The shooting algorithm used for solving (3).

Choosing two free parameters, 1 and 2 , may be a
difficult task in practice. Fortunately, Theorem 1 shows that,
with a given 1 , any 2 > ðn  1Þ2 p=1  1 will guarantee
the output of the BCD algorithm to be a DAG.
^ ap to
Theorem 1. Any 2 > ðn  1Þ2 p=1  1 will guarantee B

Theorem 1 implies that if we specify any 2 > ðn  1Þ2 p =
1  1 , we will get a minimizer of (1) through solving (2).
However, in practice, directly solving (2) by specifying a
large 2 may converge slowly. This is because the unconstrained problem in (2) may be ill-conditioned with a too
large value for 2 [42]. To avoid this situation, the “warm
start” method [42] can be used, which works in the following
way: First, it specifies a series of values for 2 , i.e.,
0
M
02 < 12 < 22 <    < M
2 , with a small 2 and 2 >
2
ðn  1Þ p=1  1 ; next, it optimizes (2) with 2 ¼ 02 to get
^ 0 , using an arbitrary initial value; then, it
a minimizer B
ap
^ 0 . as an initial value; this
optimizes (2) with 2 ¼ 12 , using B
ap
process iterates until it optimizes (2) with 2 ¼ M
2 . With the
last minimizer as the initial value for the next optimization
problem, this method can be quite efficient.
Finally, we want to mention that the L2-norm penalty,
P
2 j  X=i ðji  Pij Þ2 , might also be used in (2). The
advantage is that it is a differentiable function of ji . Also,
as shown in [42], ji  Pij ! 0 when 2 ! 1. However, the
weakness of the L2-norm penalty, compared with the L1norm penalty, is that there is no guarantee that a finite 2
exists to assure ji  Pij ¼ 0 for all pairs of Xi and Xj .
Time complexity analysis. Each iteration of the BCD
algorithm consists of two operations: a shooting algorithm
and a Breadth-first search on G. These two operations cost
OðpnÞ [46] and Oðp þ jGjÞ, respectively. Here, jGj is the
number of nonzero elements in G. If G is sparse, i.e., jGj ¼
Cp with a small constant C, then Oðp þ jGjÞ ¼ OðpÞ. Thus,
the computational cost at each iteration is only OðpnÞ.
Furthermore, each sweep through all columns of B costs
Oðp2 nÞ. Our simulation study shows that it usually takes no
more than 5 sweeps to converge.

be a DAG.
Proof. To prove this, we first need to prove that, with a
^ ap is bounded,
certain value of 1 and any value of 2 , B
T
T
T
^
^
^
x i   i x =i Þðx
x i   i x =i Þ þ 1 k^ i k1 þ 2
i.e., 1 k i k1 
 ðx
P
T
^
^
 X=i jji  Pij j 
 x i x i ¼ n  1, for each  i . The second
j
inequality holds because x i x Ti is the value of the left-hand
side of the inequality when  i ¼ 0, which is obviously
larger than that when  i ¼ ^ i . The last equality holds
because we have standardized all the variables. Thus we
^ j 
 ðn  1Þ=1 . Now, we use
know that maxk
 X=i j
ki
proof-by-contradiction to show that, with any 2 >
ðn  1Þ2 p = 1  1 , we will get a DAG. Suppose that
such a 2 doesn’t guarantee a DAG. Then, there must be
at least a pair of variables Xi and Xj with ji  Pij 6¼ 0,
which is ji 6¼ 0 and Pij ¼ 1, based on the first order
x i  ^ Ti=j x =ði;jÞ Þx
x Tj j 
optimality condition, ji 6¼ 0, i.f.f. jðx
T
^
ð1 þ 2 jPij jÞ > 0. Here,  i=j denotes the elements in ^ i
without ^ji and x =ði;jÞ denotes the sample matrix for all
the variables except Xi and Xj . However,
X 

 


ðx
^ki x k x T 
x Tj  
 x i x Tj  þ
xi  ^ Ti=j x =ði;jÞ Þx
j
 X=ði;jÞ
k

< ðn  1Þp max ^ki < ðn  1Þ2 p=1 ;
 X=ði;jÞ
k

xTj j  ð1 þ 2 jPij jÞ < 0.
resulting in jðx
x i  ^ Ti=j x =ði;jÞ Þx

u
t

1332

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 35,

NO. 6,

JUNE 2013

Fig. 4. A general tree.

Fig. 5. A general inverse tree.

4

variables; Theorem 3 focuses on a general inverse tree, which
becomes a general tree if reversing all the arcs. Proof of
Theorem 2 can be found in Appendix A, which can be found
on the Computer Society Digital Library at http://doi.
ieeecomputersociety.org/10.1109/TPAMI.2012.130. Proof
of Theorem 3 can also be found in the supplemental material
available online.

SOME THEORETICAL ANALYSIS ON THE
COMPETITIVE ADVANTAGE OF THE PROPOSED
SBN ALGORITHM

Simulation studies in Section 5 will show that SBN is more
accurate than various existing algorithms that employ a
two-stage approach. This section aims to provide some
theoretical insights about why the existing algorithms are
less accurate. Please note that although a comprehensive
analysis of this kind on all types of BNs and all two-stage
algorithms is the most desirable, it is also very challenging,
if not impossible, and beyond the scope of this paper.
Therefore, in this section, we focus on some specific types of
BNs and one popular two-stage algorithm, so as to provide
some supporting evidence for the proposed SBN in addition
to the results of the simulation studies in Section 5.
Recall that Stage 1 of the two-stage approach is to
identify the potential parents of each Xi . The existing
algorithms achieve this goal by identifying the MB of Xi . A
typical method is variable selection based on regressions,
i.e., to build a regression of Xi on all other variables and
consider the variables selected to be the MB. One
difference between various algorithms is the type of
regression used and the method used for variable selection.
For example, the TC algorithm [40] uses ordinary regression and a t-test for variable selection; the L1MB-DAG
algorithm [36] uses LASSO.
However, in the regression of Xi , not only will the
coefficients for the variables not in the MB be small
(theoretically zero due to the definition of MB), the
coefficients for the parents may also be very small due to
the correlation between the parents and the children. As a
result, some parents may not be selected in the variable
selection, i.e., they will be missed in Stage 1 of the two-stage
approach, leading to greater BN learning errors. In contrast,
SBN may not suffer from this problem because it is a onestage approach that identifies the parents directly.
To further illustrate this point, we analyze one two-stage
algorithm, the TC algorithm. TC does variable selection using
a t-test. To determine whether a variable should be selected, a
^
^ where ^ is the least-square
t-test uses the statistic =seð
Þ,
estimate for the regression coefficient of this variable and
^ is the standard error. The larger the value of =seð
^
^
seðÞ
Þ,
the higher the chance that the variable will be selected.
Theorems 2 and 3 below show that even though the value of
^
^ corresponding to a parent of Xi is large in the true
=seð
Þ
BN, its value may decrease drastically in the regression of Xi
on all other variables. Theorem 2 focuses on a specific type of
BN, a general tree, in which all variables have one common
ancestor and there is at most one directed path between two

Theorem 2. Consider a general tree with m variables, whose
structure and parameters are given by X1 ¼ e1 , X2 ¼ 12 X1 þ
e2 , Xi ¼ 2i X2 þ ei , i ¼ 3; 4; . . . ; m (Fig. 4). All the variables
have unit variance. Let ^12 denote the least-square estimate for
MB
12 in regression X2 ¼ 12 X1 þ e2 . Let ^12
denote the leastMB
MB
square estimate for 12 in regression X2 ¼ 12
X1 þ
MB
MB
MB
23 X3 þ    þ 2m Xm þ e2 (i.e., a regression that regresses
X2 on all other variables in the general tree).Then, the following
relations hold:

MB
j ¼ j^12 j
j^12


Qm


^2

 ^
i¼3 ð1  2i Þ
 Qm 

<j12 j;
P
Q
m ^2
2 þ
^2 Þ m
^2 Þ
 i¼3 1^2i
½

ð1

ð1


12
2j
i¼3 2i
j¼3;j6¼i




MB
 ^12 
^12


 ¼   
MB
se ^12 
se ^12
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

u
Qm 
u
^2
u
i¼3 1  2i
 tQm 
 Pm  2 Qm

	
^2
^
^2
i¼4 1  2i þ
i¼4 2i
j¼3;j6¼i 1  2j


 ^

 12 
<
;
seð^12 Þ

where ^ij denotes the least-square estimate for a regression
coefficient ij and seð^ij Þ denotes the standard error for ^ij .
Theorem 3. Consider a general inverse tree with m þ l þ 2
variables, whose structure and parameters are given by
X k ¼ ek ;
Xlþ1 ¼

l
X

k ¼ 1; 2; . . . ; l;

k;lþ1 Xk
k¼1
m
X

þ

l þ 3; . . . ; l þ m;

þ elþ1 ; Xlþ2 ¼ lþ1;lþ2 Xlþ1

lþi;lþ2 Xlþi þ elþ2

i¼3

HUANG ET AL.: A SPARSE STRUCTURE LEARNING ALGORITHM FOR GAUSSIAN BAYESIAN NETWORK IDENTIFICATION FROM HIGH-...

1333

Fig. 6. (a) General tree used in the simulation study in Section 5.1; (b) general inverse tree used in the simulation study in Section 5.2 (regression
coefficients of arcs generated from Uniformð0:5; 1Þ).

(Fig. 5). All the variables have unit variance. Let ^k;lþ1 denote
the least-square estimate for k;lþ1 in regression Xlþ1 ¼
Pl
^MB
k¼1 k;lþ1 Xk þ elþ1 , k ¼ 1; 2; . . . ; l. Let k;lþ1 denote the
P
MB
least-square estimate for k;lþ1 in regression Xlþ1 ¼ lk¼1
P
MB
MB
MB
MB
k;lþ1
Xk þ lþ1;lþ2
Xlþ2 þ m
i¼3 lþi;lþ2 Xlþi þ elþ1 (i.e., a regression that regresses Xlþ1 on all other variables in the
general inverse tree). Then, the following relations hold:


P


^2
^2
 MB 
1 m

i¼1 lþ2þi;lþ2  lþ1;lþ2
^
 ¼ j^k;lþ1 j  

P
P
k;lþ1
l
1  m ^2
 ^2
^2 
i¼1

lþ2þi;lþ2

lþ1;lþ2

i¼1

i;lþ1

< j^k;lþ1 j;
 



 ^MB   ^
 k;lþ1   k;lþ1 

 

 ¼  


MB 
se ^k;lþ1
se ^k;lþ1 


P


^2
^2
1 m


i¼1 lþ2þi;lþ2  lþ1;lþ2


Pm ^2
P
l
^2
^2 
1 




i¼1 lþ2þi;lþ2
i¼1 i;lþ1
lþ1;lþ2
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Pl ^2
P
P
u
2
^2
^2
uð1  li¼1 ^i;lþ1
Þð1  m
i¼1 lþ2þi;lþ2  lþ1;lþ2
i¼1 i;lþ1 Þ
t

Pl
Pm ^2
2
2
^
^
ð1  i¼1 lþ2þi;lþ2  lþ1;lþ2 i¼1;i6¼k i;lþ1 Þ
vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

 ﬃ
u
P
Pm ^2
2
^
u 1  li¼1 ^2
i¼1 lþ2þi;lþ2  lþ1;lþ2
i;lþ1 1 
u


t
Pl
P
2
^2
^2
1  i¼1;i6¼k ^i;lþ1
1 m
i¼1 lþ2þi;lþ2  lþ1;lþ2




 ^k;lþ1 

:
< 
se ^k;lþ1 
Here, we use two examples to illustrate the theorems.
Consider a general tree with m ¼ 8 (see Fig. 4 to recall the
definition for m) and least-square estimates for the parameters being ^12 ¼ 0:3 and ^2i ¼ 0:8, i ¼ 3; . . . ; 8. Then,
MB
MB
using the formula for ^12
in Theorem 2, we can get j^12
j¼
MB
MB
j^12 j  0:093 < j^12 j. Using the formula for ^12 =seð^12 Þ, we
can get j^MB =seð^MB Þj ¼ j^12 =seð^12 Þj  0:29 < j^12 =seð^12 Þj.
12

12

Consider a general inverse tree with l ¼ 5 and m ¼ 0 (see
Fig. 5 to recall definitions for l and m) and least-square

estimates for the parameters being ½^16 ; . . . ; ^56  ¼ ½0:24;
0:325; 0:256; 0:304; 0:216 and ^67 ¼ 0:38. Then, using the
MB
MB
formula for ^k;lþ1
(i.e., ^k;6
, k ¼ 1; . . . ; 5) in Theorem 3, we
can get
MB
j^16
j ¼ j^16 j  0:15 < j^16 j;
j^MB j ¼ j^36 j  0:48 < j^36 j;
36

MB
j^26
j ¼ j^26 j  0:163 < j^26 j;
j^MB j ¼ j^46 j  0:148 < j^46 j;
46

MB
and j^56
j ¼ j^56 j  0:148 < j^56 j. Using the formula for
MB
MB
MB
MB
Þ, we can verify j^k;lþ1
=seð^k;lþ1
Þj < j^k;lþ1 =
^k;lþ1 =seð^k;lþ1
^
seðk;lþ1 Þj.
Note that the theoretical study in this section focuses on
Stage 1 of the two-stage approach. It would also be
interesting to analyze Stage 2, e.g., to find out the relative
significance of the coefficients for variables in the MB and
identify under what conditions the true parents may be
missed. We plan to conduct such analysis in the future.

5

SIMULATION STUDY ON SYNTHETIC DATA

We perform five simulations. The first two show that, on a
general tree and a general inverse tree, the existing algorithms
based on the two-stage approach may miss some true parents
with a high probability, while SBN performs well. The third
simulation is to compare the structure learning accuracy of
SBN with other competing algorithms using some benchmark networks. The fourth and fifth simulations are to
investigate the scalability and efficiency of SBN and compare
it with other competing algorithms. The code is available at
http://www.public.asu.edu/~shuang31/codes/SBN.rar.

5.1 Learning Accuracy for General Tree
We select 10 existing algorithms in our study: HITON-PC
[47], IAMB and three of its variants [48], GS [39], SC [35], TC
and its advanced version TC-bw [40], and L1MB-DAG [36].
We focus on the general tree shown in Fig. 6a in which the
regression coefficient of each arc is randomly generated
from Uniformð0:5; 1Þ. We simulate data from this general
tree with a sample size of 200.

1334

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 35,

NO. 6,

JUNE 2013

Fig. 7. (a) Frequency of X1 being identified as a parent of Xi , i ¼ 2; . . . ; 7; (b) ratio of number of correctly identified arcs in learned BN to number of
arcs in true BN; (c) ratio of total learning error in learned BN (false positives plus false negatives) to number of arcs in true BN.

We apply the selected existing algorithms on the
simulated data; the parameters of each algorithm are
selected in the way that has been suggested in the respective
paper. Specifically, HITON-PC is applied with a significance level of 5 percent used in the G2 test of statistical
independence and degrees of freedom set according to
reference 14 cited in the paper of HITON-PC [47]. IAMB and
its variants are applied with the significant level set to be
5 percent. GS is applied using the default value of 0.05 in its
algorithm. SC is applied using the Bayesian scoring
heuristic and the maximum number of parents chosen for
the SC algorithm to be 5 and 10 (the one with better
performance is kept and its corresponding result is
presented). TC and TC-bw are applied by setting parameter
 ¼ 2=ðpðp  1ÞÞ as suggested and adopted in the paper [40].
There is no free parameter in L1MB-DAG.
In applying the proposed SBN, 1 is selected by BIC (i.e.,
a step search is employed to find the 1 that produces the
minimum BIC value). Following Theorem 1, 2 is set to be
10½ðn  1Þ2 p=1  1 , which empirically guarantees a DAG
to be learned. Furthermore, note that the optimization
problem in (2) is nonconvex, so a good initial value for B
would be helpful. We tried various options and found that a
good initial value can be the output from Stage 1 of the twostage approaches (i.e., the potential parent set). Specifically,
in our experiments we set the initial value to be the output
from Stage 1 of L1MB, which is a parameter-free algorithm
that can be easily assembled with SBN.
The results averaged over 100 repetitions are shown in
Figs. 7a, 7b, and 7c. The X-axis records the 10 selected

algorithms and the proposed SBN (the last one). The Y -axis
of each figure in Figs. 7a, 7b, and 7c is a different
performance measure, i.e., the frequency for X1 being
identified as a parent of Xi , i ¼ 2; . . . ; 7, in (a), the ratio of
the number of correctly identified arcs in the learned BN to
the number of arcs in the true BN in (b), and the ratio of the
total learning error in the learned BN (false positives plus
false negatives) to the number or arcs in the true BN in (c).
Note that Fig. 7a focuses on the arcs between X1 and Xi ,
i ¼ 2; . . . ; 7, in order to demonstrate Theorem 2 (i.e., because
the MB of Xi includes not only parent X1 but also six
children, the coefficient of the arc between parent X1 and Xi
may be underestimated so that X1 may not be included in
the MB identified in Stage 1 of the competing algorithms).
The observation from Fig. 7a is consistent with this
theoretical explanation, which shows that the competing
algorithms do not perform as well as SBN. Figs. 7b and 7c
are performance measures defined on all arcs. They also
show SBN’s better performance.

5.2 Learning Accuracy for General Inverse Tree
We focus on the general inverse tree in Fig. 6b, in which the
regression coefficient of each arc is randomly generated
from Uniformð0:5; 1Þ. We simulate data from this general
tree with a sample size of 200.
We apply the 10 selected existing algorithms and SBN on
the simulated data in the same way as that in Section 5.1.
The results of 100 repetitions are shown in Figs. 8a, 8b, 8c,
which can be read in a similar way to Fig. 7. Note that Fig. 8a
focuses on the arcs between Xi , i ¼ 1; . . . ; 30, and their

Fig. 8. (a) Frequency of Xi being identified as parents of their respective child in true BN, i ¼ 1; . . . ; 30; (b) ratio of number of correctly identified arcs
in learned BN to number of arcs in true BN; (c) ratio of total learning error in learned BN (false positives plus false negatives) to number of arcs in true
BN

HUANG ET AL.: A SPARSE STRUCTURE LEARNING ALGORITHM FOR GAUSSIAN BAYESIAN NETWORK IDENTIFICATION FROM HIGH-...

TABLE 1
Benchmark Networks

1335

respective children in order to demonstrate Theorem 3.
Figs. 8a, 8b, 8c show that SBN performs better.

5.3 Learning Accuracy for Benchmark Networks
To evaluate the performance of SBN on general (i.e., nontree-like) BNs, we select seven moderately large networks
from the Bayesian Network Repository (BNR) [49]. None of
these networks are tree-like except for the “Chain”
network. These networks are selected based on the
consideration that they provide a range of small-tomoderately-large networks with the number of nodes
ranging from 7 to 61, they are sparse, and they were also
used in [36], which is a competing algorithm of ours. We
also use the tiling technique [50] to produce two large BNs,
Alarm2, and Hailfinder2. Two other networks with specific
structures, Factor and Chain [51], are also considered. The
numbers of nodes and arcs in each of the 11 networks are
shown in Table 1.
To specify the parameters of a network, i.e., to specify
the regression coefficients of each variable on its parents,
we randomly sample from Uniformð0:5; 1Þ. Then, we
simulate data for each network with a sample size 1,000,

Fig. 9. (a) Ratio of total learning error in the learned BN (false positives plus false negatives) to the number of arcs in the true BN for the 10
competing algorithms and SBN on 11 benchmark networks; (b) ratio of correctly identified arcs in the learned BN (i.e., true positives) to the number
of arcs in the true BN; (c) ratio of falsely identified arcs in the learned BN (i.e., false positives) to the number of arcs in the true BN; (d) ratio of the total
learning error in the learned PDAG to the number of arcs in the true PDAG. The learned BN and PDAG in (a)-(d) are based on a simulation dataset of
sample size 1,000. Dots are means and error bars are standard deviations.

1336

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 35,

NO. 6,

JUNE 2013

Fig. 10. (a) Ratio of total learning error in the learned BN (false positives plus false negatives) to the number of arcs in the true BN for the 10
competing algorithms and SBN on 11 benchmark networks; (b) ratio of correctly identified arcs in the learned BN (i.e., true positives) to the number
of arcs in the true BN; (c) ratio of falsely identified arcs in the learned BN (i.e., false positives) to the number of arcs in the true BN; (d) ratio of the total
learning error in the learned PDAG to the number of arcs in the true PDAG. The learned BN and PDAG in (a)-(d) are based on a simulation dataset of
sample size 100. Dots are means and error bars are standard deviations.

and apply the 10 competing algorithms and SBN to learn
the BN structure. The results over 100 repetitions are
shown in Fig. 9a, in which the X-axis records the
11 networks and the Y -axis records the ratio of the total
learning error in the learned BN (false positives plus false
negatives) to the number of arcs in the true BN. This figure
deserves more explanation: We found it hard to show all
10 competing algorithms, i.e., they become indistinguishable. Thus, for each benchmark network (i.e., a tick on the
X-axis), we only show the three competing algorithms
with the best performance. For example, for network
“Carpo” (fourth tick on the X-axis) in Fig. 9a, the top three
competing algorithms shown are GS, TC, and SC. Figs. 9b,
9c, 9d are comparison plots in terms of other criteria.
Specifically, Fig. 9b plots the ratio of the correctly
identified arcs in the learned BN (i.e., true positives) to
the number of arcs in the true BN. Fig. 9c plots the ratio of
the falsely identified arcs in the learned BN (i.e., false
positives) to the number of arcs in the true BN. Fig. 9d is
similar to Fig. 9a but for Partially Directed Acyclic Graph
(PDAG). Given a BN (a learned one or true one), the

corresponding PDAG can be obtained by the method
proposed in [13]. A PDAG is a collection of statistically
equivalent BN structures, i.e., these structures all represent
the same set of dependent and independent relationships
so they are statistically indistinguishable. The PDAG of a
BN can be constructed by replacing a directed arc between
Xi and Xj in the BN with an undirected one, if some
statistically equivalent BN structures have Xi ! Xj and
others have Xi
Xj . A PDAG is very useful when making
a causal interpretation, i.e., we may interpret the directed
arcs in the PDAG as representing the direction of direct
causal influence. Figs. 9a, 9b, 9c, 9d show that SBN
performs much better than all the competing algorithms
in BN- and PDAG-identification.
Furthermore, we would like to compare SBN with the
competing algorithms under small sample sizes. We
decrease the sample size to 100 and repeat the above
procedure. The results are shown in Figs. 10a, 10b, 10c, 10d.
It can be seen that SBN still performs much better than all
the competing algorithms in BN- and PDAG-identification
even for small sample sizes.

HUANG ET AL.: A SPARSE STRUCTURE LEARNING ALGORITHM FOR GAUSSIAN BAYESIAN NETWORK IDENTIFICATION FROM HIGH-...

1337

Fig. 11. Scalability of SBN with respect to (a) the number of variables, p,
(b) the sample size, n.

5.4 Scalability
We study two aspects of scalability for SBN: the scalability
with respect to the number of variables in a BN, p, and the
scalability with respect to the sample size, n. We use the CPU
time for each sweep through all the columns of B as the
parameter for measurement. Specifically, we fix n ¼ 1,000,
and vary p by using the 11 benchmark networks. Also, we fix
p ¼ 37 (the Alarm network). The results over 100 repetitions
are shown in Figs. 11a and 11b, respectively. It can be seen that
the times are linear in n and quadratic in p, which confirms
our theoretical time complexity analysis in Section 3.
5.5 Efficiency
We further compare the CPU time of SBN with other
competing algorithms in structure learning of the 11 benchmark networks. In particular, the CPU time of SBN is the
time it takes the algorithm in Fig. 2 to converge for given
regularization parameters and initial value. The CPU times
of other competing algorithms are recorded in a similar
way. The results of 100 repetitions are shown in Table 2 (the
two large networks, Alarm 2 and Haifinder 2) and Fig. 12
(the other networks). It can be seen that SBN is the fastest
algorithm in structure learning of all the benchmark
networks. This is expected since the fastest algorithms
among the 10 competing algorithms, i.e., GS and TC, have a
time complexity Oðp3 nÞ, while SBN only costs Oðp2 nÞ (i.e.,
each sweep of SBN costs Oðp2 nÞ and our simulation study
TABLE 2
Comparison of SBN with Competing Algorithms
on the CPU Time in Structure Learning of Two Large Networks
(Standard Derivation Is Shown in the Bracket)

Fig. 12. Comparison of SBN with competing algorithms on CPU time in
structure learning. Y -axis is the CPU time for each sweep through all the
columns of B on a computer with Intel Core 2, 2.2 GHz, 4 GB memory.
The X-axis is the first nine networks in Table 1.

shows that SBN usually takes no more than five sweeps to
converge).
Note that the CPU times being compared here do not
include the time of initialization and selection of parameters that need to be preset for each algorithm. Inclusion
of this time is obviously more desirable for a comprehensive assessment of each algorithm’s efficiency. This, on the
other hand, is quite difficult because different algorithms
have different initial values and parameters to be preset
and there are many different ways to set them. Also, how
to set them depends on the requirement for learning
accuracy. We leave such a comprehensive assessment and
comparison for future study and acknowledge the limitation of the current study.

6

BRAIN CONNECTIVITY MODELING OF AD BY SBN

FDG-PET images of 49 AD and 67 matching normal control
subjects are downloaded from the Alzheimer’s Disease
Neuroimaging Initiative website (www.loni.ucla.edu/
ADNI). Demographic information and MMSE scores of
the subjects are given in Table 3. The ADNI was launched in
2003 by the National Institute on Aging (NIA), the National
Institute of Biomedical Imaging andBioengineering (NIBIB),
the Food and Drug Administration (FDA), private pharmaceutical companies, and nonprofit organizations as a
$60 million, 5-year public-private partnership. The primary
goal of ADNI has been to test whether serial MRI, PET,
other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of MCI and early AD.
We apply Automated Anatomical Labeling [52] to
segment each image into 116 anatomical volumes of interest
(AVOIs) and then select 42 AVOIs that are considered to be
TABLE 3
Demographic Information and MMSE

1338

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

TABLE 4
Names of the AVOI for Brain Connectivity Modeling
(L = Left Hemisphere, R = Right Hemisphere)

potentially relevant to AD based on the literature. Each
AVOI becomes a region/variable/node in SBN. Please see
Table 4 for the name of each AVOI brain region. These
regions distributed in the four lobes of the brain, i.e., the
frontal, parietal, occipital, and temporal lobes. The measurement data of each region, according to the mechanism
of FDG-PET, is the regional average FDG binding counts,
representing the degree of glucose metabolism.
We apply SBN to learn a BN for AD and another one for
NC to represent their respective brain connectivity models.
Note that because BNs are directed graphical models, a
connectivity model learned by SBN reveals the directional
effects of one brain region over another—called the
effective connectivity of the brain [59]. Effective connectivity has been much less studied in the AD literature, while
most existing work focuses on the functional connectivity,
i.e., the correlations among brain regions. Studies on
effective connectivity can greatly complement the existing
functional connectivity studies by providing insight into
how the correlations are mediated, which may further lead
to an understanding of the mechanism underlying the
communication among distinct brain regions. In this sense,
SBN has the advantage over undirected graphical models
of discovering new knowledge about AD.
In the learning of an AD (or NC) effective connectivity
model, the value for 1 needs to be selected. In this paper,
we adopt two criteria in selecting 1 : One is to minimize the

NO. 6,

JUNE 2013

TABLE 5
Intra and Interlobe Effective Connectivity Amounts

prediction error of the model and the other is to minimize
the BIC. Both criteria have been popularly adopted in
sparse learning [20], [21], [22], [37]. The two criteria lead to
similar findings from the effective connectivity models, so
only the results based on the minimum prediction error
are shown in this section and the results based on BIC are
included in the supplemental material, which is available
online. For a given 1 value, the prediction error of the
corresponding BN is computed as follows: First, a regression is fit for each node using the parents as predictors, and
the regression coefficients are estimated by MLE. Then, the
mean square error between the true and predicted values of
each node is computed based on leave-one-out cross
validation. Finally, the mean square errors of all the nodes
are summed to represent the prediction error of the BN. The
1 value that leads to the minimum prediction error is
selected; with this 1 , SBN is applied to learn a BN brain
connectivity model. Fig. 13 shows the connectivity models
for AD and NC. Each model is represented by a “matrix.”
Each row/column is one AVOI, Xj . A black cell at the
ith row and jth column of the matrix represents that Xi is a
parent of Xj . On each matrix, four red cubes are used to
highlight the four lobes, i.e., the frontal, parietal, occipital,
and temporal lobes, from top-left to bottom-right. The black
cells inside each red cube reflect intralobe effective
connectivity, whereas the black cells outside the cubes
reflect interlobe effective connectivity.
The following interesting observations can be drawn
from the connectivity models.

6.1 Global-Scale Effective Connectivity:
The total number of arcs in a BN connectivity model— equal
to the number of black cells in a matrix plot in Fig. 13—
represents the amount of effective connectivity (i.e., the
amount of directional information flow) in the whole brain.
This number is 285 and 329 for AD and NC, respectively. In
other words, AD has 13.4 percent less amount of effective
connectivity than NC. Loss of connectivity in AD has been
widely reported in the literature [60], [68], [69], [70].
6.2

Fig. 13. Brain effective connectivity models by SBN. (a) AD; (b) NC.

VOL. 35,

Intra/Interlobe Effective Connectivity
Distribution.
Aside from having different amounts of effective connectivity at the global scale, AD may also have a different
distribution pattern of connectivity across the brain from
NC. Therefore, we count the number of arcs in each of the
four lobes and between each pair of lobes in the AD and NC
effective connectivity models. The results are summarized in
Table 5. It can be seen that the temporal lobe of AD has
22.9 percent less amount of effective connectivity than NC.
The decrease in connectivity in the temporal lobe of AD has
been extensively reported in the literature [53], [54], [55]. The
interpretation may be that AD is featured by dramatic

HUANG ET AL.: A SPARSE STRUCTURE LEARNING ALGORITHM FOR GAUSSIAN BAYESIAN NETWORK IDENTIFICATION FROM HIGH-...

cognitive decline and the temporal lobe is responsible for
delivering memory and other cognitive functions. As a
result, the temporal lobe is affected early and severely by
AD, and the connectivity network in this lobe is severely
disrupted. On the other hand, the frontal lobe of AD has
27.6 percent more amount of connectivity than NC. This
observation has been interpreted as compensatory reallocation or recruitment of cognitive resources [56], [53], [57].
Because the regions in the frontal lobe are typically affected
later in the course of AD (our data uses mild to moderate
AD), the increased connectivity in the frontal lobe may help
preserve some cognitive functions in AD patients. In
addition, AD shows a decrease in the amount of connectivity
in the parietal lobe, which has also been reported to be
affected by AD. There is no significant difference between
AD and NC in the occipital lobe. This observation is
reasonable because the occipital lobe is primarily involved
in the brain’s visual function, which is not affected by AD.
In addition to generating the connectivity models of AD
and NC based on the minimum prediction error and
minimum BIC criteria, we also generate the connectivity
models by making the total numbers of arcs the same for
AD and NC. We choose to do this to factor out the
connectivity difference between AD and NC that is due to
the difference at the global scale so that the remaining
difference will reflect their difference in connectivity
distribution. Specifically, the connectivity models with the
total number of arcs equal to 120, 80, and 60 are generated
(see the supplemental material, which is available online),
which show similar intra and interlobe effective connectivity distribution patterns to those discussed previously.

6.3 Direction of Local Effective Connectivity.
As mentioned previously, one advantage of BNs over
undirected graphical models in brain connectivity modeling
is that the directed arcs in a BN reflect the directional effect
of one region over another, i.e., the effective connectivity.
Specifically, if there is a directed arc from brain regions Xi
to Xj , it indicates that Xi takes a dominant role in the
communication with Xj . The connectivity modes in Fig. 13
reveal a number of interesting findings in this regard.
1.

2.

There are substantially fewer black cells in the area
defined by rows 27-42 and columns 1-26 in AD than
NC. Recall that rows 27-42 correspond to regions in
the temporal lobe. Thus, this pattern indicates a
substantial reduction in arcs pointing from temporal
regions to the other regions in the AD brain, i.e.,
temporal regions lose their dominating roles in
communicating information with the other regions
as a result of AD. The loss is the most severe in the
communication from the temporal to frontal regions.
Rows 31 and 35, corresponding to brain regions
“Temporal_Mid_L” and “Temporal_Inf_L”, respectively, are among the rows with the largest number
black cells in NC, i.e., these two regions take a
significantly dominant role in communicating with
other regions in normal brains. However, the
dominancy of the two regions is substantially
reduced by 34.8 and 36.8 percent, respectively, in
AD. A possible interpretation is that these are

1339

neocortical regions associated with amyloid deposition and early FDG hypometabolism in AD [60], [61],
[62], [63], [64], [65].
3. Columns 39 and 40 correspond to regions “Hippocampus_L” and “Hippocampus_R,” respectively.
There are a total of 33 black cells in these two
columns in NC, i.e., 33 other regions dominantly
communicate information with the hippocampus.
However, this number reduces to 22 (33.3 percent
reduction) in AD. The reduction is more severe in
Hippocampus_L—actually a 50 percent reduction.
The hippocampus is well known to play a prominent
role in making new memories and recalling. It has
been widely reported that the hippocampus is
affected early in the course of AD, leading to
memory loss—the most common symptom of AD.
4. There are a total of 93 arcs pointing from the left to
the right hemispheres of the brain in NC; this
number reduces to 71 (23.7 percent reduction) in
AD. The number of arcs from the right to the left
hemispheres in AD is close to that in NC. This
provides evidence that AD may be associated with
interhemispheric disconnection and the disconnection is mostly unilateral, which has also been
reported by some other papers [66], [67].
Finally, we would like to point out that although using
BNs to infer effective connectivity is common in the AD
literature, it would be more appropriate to study effective
connectivity based on PDAGs due to the statistical
equivalence of BNs. Therefore, we derive the PDAGs for
the DAGs in Fig. 13 (see Fig. S-3 in the supplemental
material, which is available online), which turn out to be
very similar to the DAGs. We also verify that all the above
findings hold based on the PDAGs.

7

CONCLUSION

In this paper, we proposed a BN structure learning
algorithm, SBN, for learning large-scale BN structures from
high-dimensional data. SBN adopted a novel formulation
that involves one L1-norm penalty term to impose sparsity
on the learning and another penalty to ensure the learned
BN to be a DAG. We studied the theoretical property of the
formulation and identified a finite value for the regularization parameter of the second penalty; this value ensures that
the learned BN is a DAG. Under this formulation, we
further proposed use of the BCD and shooting algorithms to
estimate the BN structure.
Our theoretical analysis on the time complexity of SBN
showed that it is linear in the sample size and quadratic in
the number of variables. This makes SBN more scalable and
efficient than most existing algorithms, and thus makes it
well suited for large-scale BN structure learning from highdimensional datasets. In addition, we performed theoretical
analysis on the competitive advantage of SBN over the
existing algorithms in terms of learning accuracy. Our
analysis showed that the existing algorithms employ a twostage approach in BN structure identification, and thus
having a high risk of misidentifying parents of each
variable, whereas SBN does not suffer from this problem.

1340

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

Our experiments on 11 moderate to large benchmark
networks showed that SBN outperforms 10 competing
algorithms in all metrics defined for measuring the learning
accuracy and under various sample sizes. Also, SBN
outperforms the 10 competing algorithms in scalability
and efficiency.
We applied SBN to identify the effective brain connectivity model of AD from neuroimaging PDG-PET data.
Compared with a brain connectivity model of NC, we
found that AD had significantly reduced amounts of
effective connectivity in key pathological regions. This is
consistent with known pathology and the clinical progression in AD. Clinically, our findings may be useful for
monitoring disease progress, evaluating treatment effects
(both symptomatic and disease modifying), and enabling
early detection of network disconnection in prodromal AD.
In future work, we will investigate how to measure
statistical significance of the DAG identified by our
algorithm. Potential methods include bootstrap [71], permutation tests [72], and stability selection [73]. This study is
also important from the medical point of view as it will help
verify the significance of the identified brain connectivity
loss based on the DAG. Also, although this paper focuses on
structure learning of Gaussian BNs, the same formulation
may be adopted for discrete BNs, which will be interesting
to explore. In addition, we will investigate the behavior of
SBN on Markov equivalent class. Our empirical observation
has shown that the objective function of SBN is not Markov
equivalent, i.e., SBN attributes different scores to BNs that
are Markov equivalent. More in-depth theoretical analysis
will be performed in future research.

This material is based in part on work supported by the US
National Science Foundation under Grant No. CMMI0825827, CMMI-1069246, MCB-1026710, and the National
Institutes of Health under Grant No. 1ROIGM096194-01.
Data collection and sharing for this project were funded by
the Alzheimer’s Disease Neuroimaging Initiative (ADNI;
Principal Investigator: Michael Weiner; NIH grant U01
AG024904). ADNI is funded byt he National Institute on
Aging, the National Institute of Biomedical Imaging and
Bioengineering (NIBIB), and through generous contributions from the following: Pfizer Inc., Wyeth Research,
Bristol-Myers Squibb, Eli Lilly and Company, GlaxoSmithKline, Merck & Co. Inc., AstraZeneca AB, Novartis
Pharmaceuticals Corporation, Alzheimer’sAssociation, Eisai Global Clinical Development, Elan Corporation plc,
Forest Laboratories, and the Institute for the Study of
Aging, with participation from the US Food and Drug
Administration. Industry partnerships are coordinated
through the Foundation for the National Institutes of
Health. The grantee organization is the Northern California
Institute for Research and Education, and the study is
coordinated by the Alzheimer’s Disease Cooperative Study
at the University of California, San Diego. ADNI data are
disseminated by the Laboratory of Neuro Imaging at the
University of California, Los Angeles.

NO. 6,

JUNE 2013

REFERENCES
[1]
[2]

[3]

[4]

[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]

ACKNOWLEDGMENTS

VOL. 35,

[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]

N. Friedman, M. Linial, I. Nachman, and D. Péer, “Using Bayesian
Networks to Analyze Expression Data,” J. Computational Biology,
vol. 7, pp. 601-620, 2000.
A.S. Rodin and E. Boerwinkle, “Mining Genetic Epidemiology
Data with Bayesian Networks I: Bayesian Networks and Example
Application (Plasma apoE Levels),” Bioinformatics, vol. 21, no. 15,
pp. 3273-3278, 2005.
B.G. Marcot, R.S. Holthausen, M.G. Raphael, M. Rowland, and M.
Wisdom, “Using Bayesian Belief Networks to Evaluate Fish and
Wildlife Population Viability under Land Management Alternatives from an Environmental Impact Statement,” Forest Ecology and
Management, vol. 153, nos. 1-3, pp. 29-42, 2001.
M.E. Borsuk, C.A. Stow, and K.H. Reckhow, “A Bayesian
Network of Eutrophication Models for Synthesis, Prediction,
and Uncertainty Analysis,” Ecological Modelling, vol. 173,
pp. 219-239, 2004.
H. Dai, K.B. Korb, C.S. Wallace, and X. Wu, “A Study of Casual
Discovery with Weak Links and Small Samples,” Proc. 15th Int’l
Joint Conf. Artificial Intelligence, pp. 1304-1309, 1997.
S. Mani and G.F. Cooper, “A Study in Casual Discovery from
Population-Based Infant Birth and Death Records,” Proc. AMIA
Ann. Fall Symp., pp. 315-319, 1999.
J.C. Rajapakse and J. Zhou, “Learning Effective Brain Connectivity
with Dynamic Bayesian Networks,” NeuroImage, vol. 37, pp. 749760, 2007.
J.N. Li, Z.J. Wang, S.J. Palmer, and M.J. McKeown, “Dynamic
Bayesian Network Modeling of fMRI: A Comparison of GroupAnalysis Methods,” NeuroImage, vol. 37, pp. 749-760, 2008.
J. Li and J. Shi, “Knowledge Discovery from Observational Data
for Process Control through Causal Bayesian Networks,” IIE
Trans., vol. 39, no. 6, pp. 681-690, 2007.
L. De Campos, “Independency Relationships and Learning
Algorithms for Singly Connected Networks,” J. Experimental and
Theoretical Artificial Intelligence, vol. 10, pp. 511-549, 1998.
L. De Campos and J. Huete, “A New Approach for Learning Belief
Networks Using Independence Criteria,” Int’l J. Approximate
Reasoning, vol. 24, pp. 11-37, 2000.
J. Pearl and T. Verma, “Equivalence and Synthesis of Causal
Models,” Proc. Sixth Conf. Uncertainty in Artificial Intelligence, 1990.
P. Spirtes, C. Glymour, and R. Scheines, Causation, Prediction and
Search. Springer, 1993.
C. Meek, “Causal Inference and Causal Explanation with Background Knowledge,” Proc. 11th Conf. Uncertainty in Artificial
Intelligence, 1995.
G. Cooper and E. Herskovits, “A Bayesian Method for the
Induction of Probabilistic Networks from Data,” Machine Learning,
vol. 9, pp. 309-347, 1992.
D. Heckerman, D. Geiger, and D. Chickering, “Learning Bayesian
Networks: The Combination of Knowledge and Statistical Data,”
Machine Learning, vol. 20, pp. 197-243, 1995.
W. Buntine, “A Guide to the Literature on Learning Probabilistic
Networks from Data,” IEEE Trans. Knowledge and Data Eng., vol. 8,
no. 2, pp. 195-210, Apr. 1996.
N. Friedman and D. Koller, “Being Bayesian about Network
Structure: A Bayesian Approach to Structure Discovery in
Bayesian Networks,” Machine Learning, vol. 50, pp. 95-125, 2003.
D. Heckerman, “A Tutorial on Learning Bayesian Networks,”
Technical Report MSR-TR-95-06, Microsoft Research, 1996.
W. Lam and F. Bacchus, “Learning Bayesian Belief Networks, an
Approach Based on the MDL Principle,” Computational Intelligence,
vol. 10, pp. 269-293, 1994.
J.A. Suzuki, “Construction of Bayesian Networks from Databases
Based on an MDL Principle,” Proc. Ninth Conf. Uncertainty in
Artificial Intelligence, pp. 266-273, 1993.
R. Bouckaert, “Belief Networks Construction Using the Minimum
Description Length Principle,” Symbolic and Quantitative Approaches to Reasoning and Uncertainty, pp. 41-48, Springer, 1993.
N. Friedman and M. Goldszmidt, “Learning Bayesian Networks
with Local Structure,” Proc. 12th Conf. Uncertainty in Artificial
Intelligence, 1996.
C. Chow and C. Liu, “Approximating Discrete Probability
Distributions with Dependence Trees,” IEEE Trans. Information
Theory, vol. 14, no. 3, pp. 462-467, May 1968.
D. Chickering, “Optimal Structure Identification with Greedy
Search,” J. Machine Learning Research, vol. 3, pp. 507-554, 2002.

HUANG ET AL.: A SPARSE STRUCTURE LEARNING ALGORITHM FOR GAUSSIAN BAYESIAN NETWORK IDENTIFICATION FROM HIGH-...

[26] S. Acid and J. De Campos, “Searching for Bayesian Network
Structures in the Space of Restricted Acyclic Partially Directed
Graphs,” J. Artificial Intelligence Research, vol. 18, pp. 445-490, 2003.
[27] R. Castelo and T. Kocka, “On Inclusion-Driven Learning of
Bayesian Networks,” J. Machine Learning Research, vol. 4, pp. 527574, 2003.
[28] R. Larranaga, C. Kuijpers, R. Murga, and Y. Yurramendi,
“Learning Bayesian Network Structures by Searching for the Best
Ordering with Genetic Algorithms,” IEEE Trans. Systems, Man, and
Cybernetics, vol. 26, no. 4, pp. 487-493, July 1996.
[29] P. Larranaga, M. Poza, Y. Yurramendi, R. Murga, and C. Kuijpers,
“Structure Learning of Bayesian Networks by Genetic Algorithms:
A Performance Analysis of Control Parameters,” IEEE Trans.
Pattern Analysis and Machine Intelligence, vol. 18, no. 9, pp. 912-926,
Sept. 1996.
[30] D. Chickering, D. Geiger, and D. Heckerman, “Learning Bayesian
Networks: Search Methods and Experimental Results,” Proc.
Preliminary Papers Fifth Int’l Workshop Artificial Intelligence and
Statistics, 1995.
[31] X.W. Chen, G. Anantha, and X.T. Lin, “Improving Bayesian
Network Structure Learning with Mutual Information-Based
Node Ordering in the K2 Algorithm,” IEEE Trans. Knowledge and
Data Eng., vol. 20, no. 5, pp. 628-640, May 2008.
[32] P.O. Hoyer, D. Janzing, J.M. Mooij, J. Peters, and B. Scholkopf,
“Nonlinear Causal Discovery with Additive Noise Models,” Proc.
Conf. Neural Information Processing Systems, 2009.
[33] J. Peng, P. Wang, N. Zhou, and J. Zhu, “Partial Correlation
Estimation by Joint Sparse Regression Models,” J. Am. Statistical
Assoc., vol. 104, pp. 735-746, 2009.
[34] O. Sporns, D.R. Chialvo, M. Kaiser, and C.C. Hilgetag, “Organization, Development and Function of Complex Brain Networks,”
Trends in Cognitive Sciences, vol. 8, pp. 418-425, 2004.
[35] N. Friedman, I. Nachman, and D. Péer, “Learning Bayesian
Network Structure from Massive Datasets: The ‘Sparse Candidate’
Algorithm,” Proc. 15th Conf. Uncertainty in Artificial Intelligence,
1999.
[36] M. Schmidt, A. Niculescu-Mizil, and K. Murphy, “Learning
Graphical Model Structures using L1-Regularization Paths,” Proc.
22nd Nat’l Conf. Artificial Intelligence, 2007.
[37] R. Tibshirani, “Regression Shrinkage and Selection via the Lasso,”
J. Royal Statistical Soc. Series B, vol. 58, no. 1, pp. 267-288, 1996.
[38] I. Tsamardinos, L.E. Brown, and C.F. Aliferis, “The Max-Min HillClimbing Bayesian Network Structure Learning Algorithm,”
Machine Learning, vol. 65, no. 1, pp. 31-78, 2006.
[39] D. Margaritis and S. Thrun, “Bayesian Network Induction via
Local Neighborhoods,” Proc. Conf. Advances in Neural Information
Processing Systems, 1999.
[40] J.P. Pellet and A. Elisseeff, “Using Markov Blankets for Causal
Structure Learning,” J. Machine Learning Research, vol. 9, pp. 12951342, 2008.
[41] E. Estrada and H. Naomichi, “Communicability in Complex
Networks,” Physics Rev. E, vol. 77, p. 036111, 2008.
[42] R. Luus and R. Wyrwicz, “Use of Penalty Functions in Direct
Search Optimization.” Hungarian J. Industrial Chemistry , vol. 24,
pp. 273-278, 1996.
[43] D.P. Bertsekas, Nonlinear Programming, second ed. Athena
Scientific, 1999.
[44] W. Fu, “Penalized Regressions: The Bridge vs the Lasso,”
J. Computational and Graphical Statistics, vol. 7, no. 3, pp. 397-416,
1998.
[45] T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein, Introduction
to Algorithms, third ed. MIT Press, 2001.
[46] J. Friedman, T. Hastie, H. Hofling, and R. Tibshirani, “Pathwise
Coordinate Optimization,” The Annals of Applied Statistics, vol. 1,
no.2, pp.302-332, 2007.
[47] C.F. Aliferis, I. Tsamardinos, and A. Statnikov, “HITON, a Novel
Markov Blanket Algorithm for Optimal Variable Selection,” Proc.
AMIA Ann. Symp., 2003.
[48] I. Tsamardinos and C. Aliferis, “Towards Principled Feature
Selection: Relevancy, Filters and Wrappers,” Proc. Ninth Int’l
Workshop Artificial Intelligence and Statistics, 2003.
[49] Bayesian Network Repository: http://www.cs.huji.ac.il/labs/
compbio/Repository, 2011.
[50] I. Tsamardinos, A. Statnikov, L.E. Brown, and C.F. Aliferis,
“Generating Realistic Large Bayesian Networks by Tiling,” Proc.
19th Int’l FLAIRS Conf., 2006.

1341

[51] D. Mackey, Information Theory, Inference, and Learning Algorithms.
Cambridge Univ. Press, 2003.
[52] N. Tzourio-Mazoyer, “Automated Anatomical Labelling of Activations in SPM Using a Macroscopic Anatomical Parcellation of
the MNI MRI Single Subject Brain,” NeuroImage, vol. 15, pp. 273289, 2002.
[53] K. Supekar, V. Menon, D. Rubin, M. Musen, and M.D. Greicius,
“Network Analysis of Intrinsic Functional Brain Connectivity in
Alzheimer’s Disease,” PLoS Computational Biology, vol. 4, no. 6, pp.
1-11, 2008.
[54] N.P. Azari, S.I. Rapoport, C.L. Grady, M.B. Schapiro, J.A. Salerno,
and A. Gonzales-Aviles, “Patterns of Interregional Correlations of
Cerebral Glucose Metabolic Rates in Patients with Dementia of the
Alzheimer Type,” Neurodegeneration, vol. 1, pp. 101-111, 1992.
[55] K. Wang, M. Liang, L. Wang, L. Tian, X. Zhang, and T. Jiang,
“Altered Functional Connectivity in Early Alzheimer’s Disease: A
Resting-State fMRI Study,” Human Brain Mapping, vol. 28, pp. 967978, 2007.
[56] R.L. Gould, B. Arroyo, R.G. Brown, A.M. Owen, and R.J. Howard,
“Brain Mechanisms of Successful Compensation during Learning
in Alzheimer Disease,” Neurology, vol. 67, pp. 1011-1017, 2006.
[57] Y. Stern, “Cognitive Reserve and Alzheimer Disease,” Alzheimer
Disease Associated Disorder, vol. 20, pp. 69-74, 2006.
[58] K.B. Korb and A.E. Nicholson, Bayesian Artificial Intelligence.
Chapman & Hall/CRC, 2003.
[59] K.J. Friston, “Functional and Effective Connectivity in Neuroimaging: A Synthesis,” Human Brain Mapping, vol. 2, pp. 56-78, 1994.
[60] M.D. Greicius, G. Srivastava, A.L. Reiss, and V. Menon, “DefaultMode Network Activity Distinguishes AD from Healthy Aging:
Evidence from Functional MRI,” Proc. Nat’l Academy Sciences USA,
vol. 101, pp. 4637-4642, 2004.
[61] G.E. Alexander, K. Chen, P. Pietrini, S.I. Rapoport, and E.M.
Reiman, “Longitudinal PET Evaluation of Cerebral Metabolic
Decline in Dementia: A Potential Outcome Measure in Alzheimer’s Disease Treatment Studies,” Am. J. Psychiatry, vol. 159,
pp. 738-745, 2002.
[62] H. Braak and E. Braak, “Evolution of the Neuropathology of
Alzheimer’s Disease,” Acta Neurologica Scandinavica Supplementum,
vol. 165, pp. 3-12, 1996.
[63] H. Braak, E. Braak, and J. Bohl, “Staging of Alzheimer-Related
Cortical Destruction,” European Neurology, vol. 33, pp. 403-408,
1993.
[64] M.D. Ikonomovic, W.E. Klunk, E.E. Abrahamson, C.A. Mathis, J.C.
Price, N.D. Tsopelas, B.J. Lopresti, S. Ziolko, W. Bi, W.R. Paljug,
M.L. Debnath, C.E. Hope, B.A. Isanski, R.L. Hamilton, and S.T.
DeKosky, “Post-Mortem Correlates of In Vivo PiB-PET Amyloid
Imaging in a Typical Case of Alzheimer’s Disease,” Brain, vol. 131,
pp. 1630-1645, 2008.
[65] W.E. Klunk, H. Engler, A. Nordberg, Y. Wang, G. Blomqvist,
D.P. Holt, M. Bergstrom, I. Savitcheva, G.F. Huang, S. Estrada,
B. Ausen, M.L. Debnath, J. Barletta, J.C. Price, J. Sandell, B.J.
Lopresti, A. Wall, P. Koivisto, G. Antoni, C.A. Mathis, and B.
Langstrom, “Imaging Brain Amyloid in Alzheimer’s Disease
with Pittsburgh Compound-B,” Annals of Neurology, vol. 55,
pp. 306-319, 2004.
[66] P.A. Reuter-Lorenz and J.A. Mikels, “A Split-Brain Model of
Alzheimer’s Disease? Behavioral Evidence for Comparable Intra
and Interhemispheric Decline,” Neuropscyhologia, vol. 43, pp. 13071317, 2005.
[67] A.M. Lipton, R. Benavides, L.S. Hynan, F.J. Bonte, T.S. Harris, C.L.
White III, and E.H. Bigio, “Lateralization on Neuroimaging Does
Not Differentiate Frontotemporal Lobar Degeneration from
Alzheimer’s Disease,” Dementia and Geriatric Cognitive Disorders,
vol. 17, no. 4, pp. 324-327, 2004.
[68] T. Hedden, K.R. Van Dijk et al., “Disruption of Functional
Connectivity in Clinically Normal Older Adults Harboring
Amyloid Burden,” J. Neuroscience, vol. 29, pp. 12686-12694, 2009.
[69] J.R. Andrews-Hanna et al., “Disruption of Large-Scale Brain
Systems in Advanced Aging,” Neuron, vol. 56, pp. 924-935, 2007.
[70] X. Wu, R. Li, A.S. Fleisher, E.M. Reiman, K. Chen, and L. Yao,
“Altered Default Mode Network Connectivity in AD—A Resting
Functional MRI and Bayesian Network Study,” Human Brain
Mapping, vol. 32, pp. 1868-1881, 2011.
[71] B. Efron and R.J. Ribshirani, An Introduction to the Bootstrap. CRC
Press, 1994.
[72] P. Good, Permutation, Parametric and Bootstrap Tests of Hypotheses,
third ed. Springer, 2005.

1342

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

[73] N. Meinshausen and P. Buehlmann, “Stability Selection,” J. Royal
Statistical Soc., Series B, vol. 72, pp. 417-473, 2010.
Shuai Huang received the bachelor’s degree in
statistics from the University of Science and
Technology of China in 2007. Currently, he is
working toward the PhD degree in the School of
Computing, Informatics, and Decision Systems
Engineering at Arizona State University (ASU).
His research interests include data mining and
machine learning with applications in health and
manufacturing. One of his papers was selected
as a Feature Article by IIE Magazine.

Jing Li received the PhD degree in industrial
and operations engineering from the University
of Michigan in 2007. Currently, she is an
assistant professor in industrial engineering in
the School of Computing, Informatics, and
Decision Systems Engineering, Arizona State
University. Her research interests include data
mining, machine learning, and bioinformatics.
She was a recipient of the Best Paper Award
from the Institute of Industrial Engineers (IIE)
Annual Conference (twice). Two of her papers were selected as Feature
Articles by IIE Magazine.
Jieping Ye received the PhD degree in computer science from the University of Minnesota,
Twin Cities, in 2005. He is an associate
professor ini the Department of Computer
Science and Engineering at Arizona State
University (ASU). His research interests include
machine learning, data mining, and biomedical
informatics. He won the outstanding student
paper award at ICML in 2004, the SCI Young
Investigator of the Year Award at ASU in 2007,
the SCI Researcher of the Year Award at ASU in 2009, the US National
Secience Foundation Career Award in 2010, and the KDD Best
Research Paper Award honorable mention in 2010.

VOL. 35,

NO. 6,

JUNE 2013

Adam Fleisher received the MD degree from
the University of California, San Diego. He is an
associate director of brain imaging for the
Banner Alzheimer’s Institute (BAI), medical
director for the Alzheimer’s Disease Cooperative
Study, and an associate professor, Department
of Neurosciences, University of California, San
Diego. He is recognized for his contributions to
the literature in brain imaging of individuals at
increased risk for Alzheimer’s Disease (AD),
utilizing various volumetric and functional MRI and PET techniques. He
has expertise in both neuroimaging and multicenter clinical trials in AD,
with a clinical specialty in geriatric neurology, evaluating patients at the
BAI Memory Disorders Clinic.
Kewei Chen received the PhD degree in
biomathematics from the University of California,
Los Angeles, in 1991. Currently, he is a senior
scientist and director of the computational image
analysis program, Banner Alzheimer Institute
(BAI). He also holds adjunct professorships at
Arizona State University (ASU), Beijing Normal
University, and Shanghai Jiaotong University.
His research interests include neuroimage data
analysis, processing, multivariate analysis, and
radioactive PET tracer kinetic modeling. He has published in the fields of
Alzheimer’s neuroimaging studies, on normal human brain functions,
and methods in processing neuroimaging data.
Teresa Wu is an associate professor in the
industrial engineering program at the School of
Computing, Informatics, Decision Systems Engineering, Arizona State University (ASU). Her
research interests include distributed decision
support and healthcare informatics. Her papers
in healthcare informatics have appeared in
NeuroImage, the Journal of Digital Image, and
RadioGraphics.

Eric Reiman received the MD degree from Duke
University. He is an executive director of the
Banner Alzheimer’s Institute (BAI), chief scientific officer at the Banner Research Institute,
clinical director of the Neurogenomics Division at
the Translational Genomics Research Institute
(TGen), professor and associate head of Psychiatry at the University of Arizona, and director
of the NIA and state-supported Arizona Alzheimer’s Consortium. His research interests include
brain imaging, genomics, the presymptomatic detection, tracking and
scientific study of Alzheimer’s Disease (AD), the accelerated evaluation
of presymptomatic AD treatments using brain imaging and other
biomarker methods, and the development of methods with improved
power to address these and other objectives.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

European Journal of Operational Research 217 (2012) 185–197

Contents lists available at SciVerse ScienceDirect

European Journal of Operational Research
journal homepage: www.elsevier.com/locate/ejor

Decision Support

Decentralized operation strategies for an integrated building energy system using a
memetic algorithm
Mengqi Hu a, Jeffery D. Weir b, Teresa Wu a,⇑
a
b

School of Computing, Informatics, Decision Systems Engineering, Arizona State University, Tempe, AZ 85287-5906, USA
Department of Operational Sciences, Graduate School of Engineering & Management, Air Force Institute of Technology, Wright-Patterson AFB, OH 45433-7765, USA

a r t i c l e

i n f o

Article history:
Received 28 October 2010
Accepted 5 September 2011
Available online 19 September 2011
Keywords:
OR in energy
Net-zero
Smart grid
Pareto optimality
Memetic algorithm

a b s t r a c t
The emerging technology in net-zero building and smart grids drives research moving from centralized
operation decisions on a single building to decentralized decisions on a group of buildings, termed a
building cluster which shares energy resources locally and globally. However, current research has
focused on developing an accurate simulation of single building energy usage which limits its application
to building clusters as scenarios such as energy sharing and competition cannot be modeled and studied.
We hypothesize that the study of energy usage for a group of buildings instead of one single building will
result in a cost effective building system which in turn will be resilient to power disruption. To this end,
this paper develops a decision model based on a building cluster simulator with each building modeled
by energy consumption, storage and generation sub modules. Assuming each building is interested in
minimizing its energy cost, a bi-level operation decision framework based on a memetic algorithm is proposed to study the tradeoff in energy usage among the group of buildings. Two additional metrics, measuring the comfort level and the degree of dependencies on the power grid are introduced for the
analysis. The experimental result demonstrates that the proposed framework is capable of deriving the
Pareto solutions for the building cluster in a decentralized manner. The Pareto solutions not only enable
multiple dimensional tradeoff analysis, but also provide valuable insight for determining pricing mechanisms and power grid capacity.
Ó 2011 Elsevier B.V. All rights reserved.

1. Introduction
According to the Electric Power Research Institute (EPRI), the
electricity consumption of the US grew 1.7% annually from 1996
to 2006 with the expectation of total growth through 2030 being
26% (Parks, 2009). Buildings (approximately half commercial and
half residential) consume over 70% of the electricity among all
the consumption units (Friedman, 2009). The fact is between 4
and 20% of energy used for heating, ventilating and air conditioning (HVAC), lighting and refrigeration in buildings is wasted due
to problems with system operation. Therefore, extensive research
in the past two decades has explored optimal operation strategies
including on-site generation and storage for net-zero buildings to
reduce energy cost and improve energy efﬁciency for building
systems.
Research on HVAC has employed simulation and mathematical
modeling for optimal strategies. For example, Fong et al. (2006)
develop a simulation-evolutionary programming coupled approach
to optimize the HVAC control which demonstrates a 7% cost savings

⇑ Corresponding author. Tel.: +1 480 965 4157; fax: +1 480 965 8692.
E-mail address: Teresa.Wu@asu.edu (T. Wu).
0377-2217/$ - see front matter Ó 2011 Elsevier B.V. All rights reserved.
doi:10.1016/j.ejor.2011.09.008

compared with the existing control methods. Lu et al. (2005)
formulate a mixed integer nonlinear programming problem and apply a modiﬁed genetic algorithm to derive the HVAC system optimal
control strategy which signiﬁcantly improves the HVAC performance. Wright et al. (2002) and Nassif et al. (2005) study the multi-objective genetic algorithm to optimize building thermal control
and HVAC control aiming to minimize energy cost and maximize
zone thermal comfort.
Other than HVAC, recent literature indicates the use of energy
storage such as thermal mass control strategies can alleviate the
energy load and thus potentially reduce the energy cost (Braun,
2003). As an example, Keeney and Braun (1996) successfully
demonstrate pre-cooling of a building can reduce the peaking
cooling load, electricity demand and energy cost. Hämäläinen and
Mäntysaari (2002) employ dynamic goal programming to study
the tradeoff between energy cost, energy consumption and living
comfort for the residential house heating system. Braun (2007)
and Sun et al. (2006) further develop a heuristic near-optimal
control strategy for thermal storage systems with dynamic realtime electric rates. The results indicate that the annual cost under
this control method is close to optimum (less than 2% higher than
the minimal costs). Drees and Braun (1996) present a rule-based
control strategy for a thermal storage system which outperforms

186

M. Hu et al. / European Journal of Operational Research 217 (2012) 185–197

the conventional control strategy such as chiller-priority and
storage-priority strategies. The monthly electrical cost is near-optimal (less than 3% higher than optimum obtained from dynamic programming). Henze et al. (2005) further develop model-based
predictive optimal control of active and passive building thermal
storage and successfully achieve 18% and 7% of cost savings compared to the reference case and base case which are two testing
cases proposed in (Henze et al., 2005). Henze and Schoenmann
(2003) and Liu and Henze (2007) demonstrate that the model-free
reinforcement learning control for the thermal storage system can
achieve more cost savings than the conventional storage control
strategies but less than the predictive optimal control strategies.
In addition, Liu and Henze (2006a,b) propose a hybrid reinforcement learning control approach combining model-based with model-free control to locate the optimal control for passive and active
thermal storage which can achieve 8.3% cost savings compared to
the base case proposed in (Liu and Henze, 2006b). Lee et al.
(2009) employ the particle swarm algorithm to optimize operations
of the ice-storage air-conditioning system which can minimize the
life cycle cost and reduce the CO2 emission.
Another noteworthy emerging effort is energy generation and the
use of energy generation on site. For example, Manolakos et al.
(2001) develop a simulation–optimization program to design and
control a hybrid energy system which consists of a battery, wind
generator and photovoltaic module. Rong et al. (2008b) develop an
efﬁcient and near-optimal planning strategy for the tri-generation
system which includes an electric power, heat, cooling and storage
system using a Lagrangian relaxation based algorithm. Garcı́aGonzález et al. (2007) optimize the short-term scheduling which is
formulated as a mixed integer linear programming problem for
hydroelectric generation units. Arun et al. (2009) adopt chance constraint programming to optimize the design of a battery-integrated
diesel generator system and identify the optimum conﬁguration.
El-shatter et al. (2006) design a fuzzy logic control based management system to improve the energy efﬁciency for a hybrid wind/
photovoltaic/fuel cell generation system. Henze and Dodier (2003)
propose a model-free reinforcement learning algorithm which outperforms the conventional control strategy to adaptively control a
grid-independent photovoltaic system which has a collector, storage
and load. Rong and Lahdelma (2007) propose an envelope-based
branch and bound algorithm to derive the long-term planning strategy for single-period combined heat and power system. Rong et al.
(2008a) further study the multi-period combined heat and power
system planning using a modiﬁed dynamic programming approach.
While promising, we notice that most literature focuses on
operation strategies for one subsystem only, that is, HVAC, energy
storage or energy generation. Considering a building is an integrated system as a whole, studying the interactions among the
subsystems is necessary. Secondly, even though there exists research exploring a building as a system consisting of subsystems,
a majority of the research formulates the decision problem for a
single building only. Realizing the emerging technologies in multi-energy source building (Corrado et al., 2007), net-zero building
(Torcellini et al., 2006) and smart grid (Parks, 2009) it is becoming
urgent critical to develop a decentralized decision framework
modeling the coordination among a cluster of buildings to obtain
Pareto decisions which enable tradeoff analysis. There are notable
efforts taken in this direction. For example, Kiesling (2009) investigates the decentralized coordination mechanism to increase energy efﬁciency through markets, technology and institutions.
However, to our knowledge, decisions for buildings consisting of
multiple interacting subsystems, which coordinate with other
buildings and the power grid has been less explored. This is probably due to the complexity of the problem which involves both
subsystems, and clusters of buildings. An even further level of complexity is the varied time scales, ranging from models running

based on minutes (e.g., energy consumption subsystem), to hourly
and possibly even daily. Given the complexities discussed above,
this paper demonstrates a methodology for modeling the coordination among a cluster of buildings. Speciﬁcally, with this methodology, decision makers can determine when to charge or discharge
the storage system or leave it dormant. They can determine an
optimal strategy for a generation system (e.g., power the building
vs. charge its storage system vs. sell back to the power grid). Given
competing owners in multiple buildings, HVAC set-point temperature strategies can be coordinated with each other on the shared
energy splitting to reach a win–win goal. Finally, decision makers
can use the methodology to determine how local energy pricing
levels and power grid capacity can inﬂuence the operation of a
building cluster.
This paper extends the agent-based simulator developed by Hu
et al. (2010), and proposes a bi-level decision framework for building cluster operation decisions. A memetic algorithm (MA) based
model is employed to identify the Pareto optimal control strategies
for the building cluster. Thermal comfort represented as the predicted percentage of dissatisﬁed (PPD) (Nassif et al., 2005) and
power grid dependency rate (PGDR) are utilized to evaluate the
Pareto operation strategies. In addition, different pricing mechanisms and power capacities are studied to demonstrate the impacts on energy costs for the group of buildings.
This paper is organized as follows: the building energy model is
introduced in Section 2; the decision model is formulated in Section
3 followed by the detailed explanation on the proposed bi-level
decentralized framework in Section 4; the MA based framework is
discussed in Section 5; the experimental results in Section 6 demonstrate how the proposed framework can be used for decentralized
decision making. Finally, conclusions are drawn in Section 7.

2. Integrated building energy system simulator
Kosny et al. (2001) and Zhou et al. (2005) employ thermal mass
concept which determines the building’s capability to utilize its
structural mass for thermal storage to differentiate heavy and light
weight/mass buildings. The thermal mass can be modiﬁed by changing either the thickness or density of the wall material without altering the architectural and construction of the building model (Zhou
et al., 2005). In this study, we choose to change the density of the wall
material (Fig. 1) to distinguish heavy and light mass buildings.
A simpliﬁed building cluster consisting of two different mass
level – heavy mass (HM) and light mass (LM) buildings is then
modeled. The two buildings, each having its own battery and photovoltaic (PV) panel, share one ice storage system and one base
chiller. The ice storage system charged by a dedicated chiller is
conﬁgured in parallel with the base chiller. During on-peak hours,
the buildings’ cooling loads are met primarily by the ice storage
system with the remaining cooling request satisﬁed by the base
chiller. The overall schematic of the building energy system conﬁguration is illustrated in Fig. 1 with the arrows denoting the energy
ﬂow among each component in the system.
In this paper, the PV panel for each building can be in only one
of the following four states: charging battery, powering building,
selling power to grid or being dormant. We assume that the extra
electricity of the PV panel will be wasted when the PV panel is at
the state of charging battery or powering building. If the electricity
generated by the PV panel is not sufﬁcient to charge the battery,
energy from the power grid will be supplied. We have developed
an integrated simulator using MATLABÒ which includes ﬁve subsystems modules: building consumption model, chiller model, ice
storage model, battery model, and photovoltaic model. Three
parameters are considered: dry/wet bulb temperature, solar radiation on inclined surfaces, and real-time pricing rate. The modules

187

M. Hu et al. / European Journal of Operational Research 217 (2012) 185–197

Fig. 1. Overall schematic of the integrated building energy system.

Table 1
Building system development.
Modules/parameters

Source

Building consumption model
(module)
Chiller model (module)
Ice storage model (module)

The building cooling load is from a building thermal model. Non-cooling load data from (Valenzuela et al., 2000) has been
appropriately scaled down for this research
The chiller model is adopted from (Sun et al., 2006)
The ice storage model is from (West and Braun, 1999). The parameters of the ice tank have been appropriately scaled down to
make it applicable for this research
The battery model is from (Lu, 2004)
The PV-panel model is from (Lu and Yang, 2004). The angular losses factor in (Martin and Ruiz, 2001) is employed to compute the
absorbed solar energy
The hourly dry/wet bulb temperature Tdb (°F), Twb (°F) in Phoenix is obtained from (NCDC, 2010)

Battery model (module)
Photovoltaic model (module)
Dry/wet bulb temperature
(parameter)
Solar radiation on inclined surfaces
(parameter)
Real-time pricing rate (parameter)

The total hourly solar irradiance on the inclined surface with slope angle b (degree) and surface azimuth angle ! (degree) is
estimated using the model from (Lorenzo, 2003; Lu, 2004;NREL, 2010)
Three pricing plans used by Salt River Project (SRP) Company ( SRP, 2010) are considered in this research

are collected from the literature and validated from experiments
and the parameters are collected from the literature and industry
practices for the use of this study (Table 1). The simulator is a
black-box which is used to evaluate the operation decisions. The
detailed decision model is explained in the next section.
3. Formulation of building energy system decision model
In the decision model, three building operation modes (Liu and
Henze, 2007) are considered for each day: (1) from midnight to the
onset of the on-peak period (0 am–1 pm); (2) the on-peak period
(1 pm–8 pm); and (3) from the end of on-peak period to midnight
(8 pm–0 am). The building shares the same characteristics (e.g.,
set-point temperature, pricing rate structure, etc.) during the successive hours in each building operation mode.
3.1. Decision variables
Each building will control its set-point temperature. The shared
ice storage will decide when to be charged or discharged to cool

the buildings, and how to distribute its discharged cooling energy
to each building. The decisions will be made for the battery on
when to be charged or discharged to provide electricity for its
served building. The decisions for the photovoltaic collector are
charging battery, powering building, selling power to grid. Let M
be the number of buildings, K be the number of modes, Table 2 lists
the decision variables for building m (m = 1, . . . , M) at building
operation mode k (k = 1, . . . , K):
In this research, we employ a group of binary intermediate varim
ables BI for the last three state variables ðSis;k ; Sm
bat;k ; SPV;k Þ in Table 2
to simplify the problem formulations which is deﬁned as:

(
BI ¼

decimal2binaryð2S1 Þ S > 0
decimal2binaryð0Þ

S60

ð1Þ

where decimal2binary () is commonly available function used to
transform the decimal number to a binary number, S denotes the
m
m
three state variables ðSis;k ; Sm
bat;k ; SPV;k Þ. Taking SPV;k ¼ 3 (PV sells
power to grid) as an example, we have S = 3, and 2S1 = 4, by
employing function decimal2binary (4), we obtain 100. Thus,

188

M. Hu et al. / European Journal of Operational Research 217 (2012) 185–197

Table 2
List of decision variables.
Decision variables

Description

Type

Tm
sp;k
m
k

Temperature set-point

Continuous

Sis,k
Sm
bat;k

Percentage of the energy from ice storage to building
State of ice storage
State of battery

Continuous
Integer (0: dormant; 1: charging; 2: discharging)
Integer (0: dormant; 1: charging; 2: discharging)

Sm
PV;k

State of PV panel

Integer (0: dormant; 1: charging battery; 2: powering building; 3: selling power to grid)

g

following Eq. (1), BI = 000, 001, 010, 100 represents S = 0, 1, 2, 3
respectively.
3.2. Objective functions
Let us assume each building has the objective to minimize its
energy cost for one day, which is written as:

fm ¼

Hk 
K X

X
m
m m
Rm
p;j P p;j  Rs;j P s;j

ð2Þ

k¼1 j¼1

where Hk is the number of hours in the building operation mode
m
k ðk ¼ 1; . . . ; KÞ; Rm
p;j and Rs;j ($/kWh) are the energy purchase and
m
selling price at time j for building m respectively; P m
p;j and P s;j
(kW) are the purchase energy from power grid and selling energy
back to the power grid at time j for building m respectively.
Please note from Fig. 1, the power grid supplies energy to the
shared cooling (base chiller and dedicated chiller) to satisfy the
cooling load of each building, and to each building to satisfy
the non-cooling load. Let us assume for building m at time j,
m
the purchase energy Pm
p;j is composed of: (1) P bch;j (kW) which represents the base chiller electrical consumption allocated to building (see Eq. (8)); (2) Pm
dch;j (kW) which is electrical consumption
for the dedicated chiller allocated to on building (see Eq. (6));
(3) Pm
nc;j (kW) which is the building non-cooling electrical consumption. We have
m
m
m
Pm
p;j ¼ P nc;j þ P dch;j þ P bch;j

ð3Þ

The building non-cooling electrical consumption includes
building itself and battery’s electrical consumption supplied by
the power grid, and is determined as



m
m
m
m
m
Pm
nc;j ¼ max P load;j  P bat;j gconv BIbat;k ð2Þ  P PV;j ginv BI PV;k ð2Þ; 0


m
m
m
þ max Pm
bat;j BIbat;k ð1Þ  P PV;j BIPV;k ð1Þ; 0 =gconv

Zhou et al., 2005). Therefore, we employ the storage-priority strategy in this research.
The energy request from dedicated chiller (primary cooling provider) is determined by the states of the ice tank. That is, the dedicated chiller will request energy from the power grid if and only if
the ice tank is in charge stage, and the ice tank will provide cooling
energy for buildings if and only if the ice tank is in discharge stage.
Here, to realistically model the system, we introduce decision variable gm
k to control the percentage of cooling energy allocated to
each building (ﬂow control valve 3 and ﬂow control valve 4). Considering we are interested in the daily energy consumption thus
 m the daily average percentage of cooling encost, we introduce g
ergy allocated to building. That is,

g m ¼

Hk
K X
X


gmk uj



,

Hk
M X
K X
X


gmk uj



ð5Þ

m¼1 k¼1 j¼1

k¼1 j¼1

The energy request for building m at time j is:

Pm
dch;j

 m BIis;k ð1Þ
¼ Pdedicated;j g

ð6Þ

where Pdedicated,j (kW) is the electrical consumption for the dedicated chiller at time j; uj (Btu/h) is the discharging rate of the ice
storage at time j.
As secondary cooling provider, the base chiller only provides
the amount of cooling to the building when the ice tank’s supply
is not sufﬁcient. Thus, the cooling energy supplied by the base chiller for each building Q m
b;j (Btu/h) is determined by Eq. (7), and the
electrical consumption proportional to the cooling request from
the base chiller for each building Pm
bch;j is computed in Eq. (8).



m
m
Qm
b;j ¼ max Q c;j  uj BI is;k ð2Þgk ; 0
,
M
X
m
¼
P
Q
Qm
Pm
base;j b;j
b;j
bch;j

ð7Þ
ð8Þ

m¼1

ð4Þ

where P m
load;j (kW) is the non-cooling electricity load for building m
at time j; P m
bat;j (kW) is the charging/discharging power of the battery for building m at time j; gconv is the battery AC/DC converter
efﬁciency, which is 0.9 according to its speciﬁcation in this research; P m
PV;j (kW) is the energy generated by the PV panel for building m at time j; ginv is the PV panel inverter efﬁciency, which is 0.92
m
in this research (Lu, 2004); BIm
PV;k ð2Þ is the second digit of BI PV;k
(from right to left).
Considering dedicated chiller based ice storage and base chiller,
there are two common control strategies: chiller-priority control
where the base chiller is the primary cooling provider with the
dedicated chiller based ice storage as the secondary, and storagepriority control where the dedicated chiller based ice storage is
the primary with the base chiller being the secondary. Extensive
research has demonstrated that storage-priority control can successfully shift the energy cost from on-peak period to off-peak period when the price rate of the electricity changes thus save more
energy costs (Braun, 2007; Henze, 2003a,b, 2004; Henze et al.,
2005, 2003; Henze and Schoenmann, 2003; Liu and Henze, 2007;

where Q m
c;j (Btu/h) is the cooling load for building m at time j; uj (Btu/
h) is the charging/discharging rate of the ice storage at time j; Pbase,j
(kW) is the electrical consumption for the base chiller at time j.
The electricity selling back to the power grid from building m at
time j is computed as
m
m
Pm
s;j ¼ P PV;j ginv BI PV;k ð3Þ

ð9Þ

Other than energy cost, we propose two additional metrics to
evaluate the decentralized decisions, which are thermal comfort
represented as the predicted percentage of dissatisﬁed (PPD)
(Nassif et al., 2005) and power grid dependency rate (PGDR).
 The PPD is a measure of the comfort level of a resident. Considering an apartment complex, PPD is a metric reﬂecting if the
occupants are satisﬁed/comfortable with the room temperature. The PPD of building m at time j is calculated as

 

4
m
PPDm
j ¼100  95  exp  0:03353  PMV j

2 	
þ0:3179  PMV m
j

ð10Þ

M. Hu et al. / European Journal of Operational Research 217 (2012) 185–197

where predicted mean vote (PMV) is a function of building indoor
air temperature T , mean radiant temperature, relative humidity,
air velocity, clothing
thermal resistance and metabolic rate, and
i
its calculation is given in (Fanger, 1970).

 The PGDR is a measure of the degree of dependencies of a building to the power grid. For the building (e.g., net-zero building)
with on-site generation and storage capability, PGDR metric
may reﬂect the resilience of the building to a power disruption.
For building m at time j, PGDR is deﬁned as



m
m
PGDRm
j ¼ P p;j  P s;j =P grid

ð11Þ




m
m
BIm
bat;k ð1Þ 6 ceil max 0; SOC bat;max  SOC bat;k



m
m
BIm
bat;k ð2Þ 6 ceil max 0; SOC bat;k  SOC bat;min

ð18Þ
ð19Þ

where ceil() rounds the element to the nearest integer towards
m
inﬁnity; SOC m
bat;max and SOC bat;min are maximum and minimum state
of charge for building m’s battery; SOC m
bat;k is the battery’s initial
state of charge for building m at building operation mode k. The
state of charge is a percentage value in this research.
(6) PV-panel: the PV-panel can charge the battery only when
the battery is in the charging state (Eq. (20)).
m
BIm
PV;k ð1Þ 6 BI bat;k ð1Þ

where Pgrid (kW) is the power grid capacity.

189

ð20Þ

3.3. Constraints
where m = 1, . . . , M and k = 1, . . . , K.
(1) Power grid: the total electricity purchased from the power
grid for M buildings cannot exceed the capacity of the power
grid at each time j. That is,
M
X

Pm
p;j 6 P grid

ð12Þ

m¼1

(2) Building: the building should keep its indoor temperature at
a comfort level at each time j.

T mL
i

6

Tm
i;j

6

T mU
i

ð13Þ

where T m
i;j is the average indoor temperature for building m at time
j; T mL
and T mU
are 74 °F and 81 °F in this research.
i
i
(3) Base chiller: the base chiller load cannot exceed its capacity
at each time j.
M
X

Qm
b;j 6 Q max;j

ð14Þ

m¼1

where Qmax,j is the chiller capacity at time j.
(4) Ice storage: at each building operation mode k, the ice storage system cannot be charged if the state of charge is at the
maximum level (Eq. (15)) and cannot be discharged if the
state of charge is at the minimum level (Eq. (16)). The
summed percentage of cooling from ice storage to each
building cannot exceed one (Eq. (17)) when the ice storage
is in the discharging state.

Thus, for M buildings, M decision models are introduced with
each model having the objective function shown in Eq. (2) and
constraints shown in Eqs. (12)–(20). A decentralized decision
framework based on a memetic algorithm is then introduced to
ensure M decision models can converge to Pareto solutions.
4. Decentralized decision making framework
In the proposed bi-level decentralized framework, other than
the building agents each representing one building with the decision model explained in the Section 3, we introduce a facilitator
agent aiming to coordinate the buildings to reach converged solutions. This is achieved by deriving a weighted-sum of the buildings’
objectives as the function for the facilitator agent. The facilitator
agent then classiﬁes the decision variables from the derived objective function into local variables (X) which are controlled by each
building and coupled variables (Y) which are jointly controlled
by more than one building. Similarly, the constraints are classiﬁed
into local constraints which apply for each building and system
constraints which apply for the group of buildings. Artiﬁcial coupled variables Z are introduced to decompose the system constraints into separable pieces so that each building can solve
fully independent sub-problems. Let us assume the jth coupled
system constraint bj(X1, . . . , XM, Y) 6 hj can be written as:

b1;j ðX1 ; YÞ þ b2;j ðX2 ; YÞ þ    þ bM;j ðXM ; YÞ 6 hj

where b1,j(X1, Y), b2,j(X2, Y), . . . , bM,j(XM, Y) are the constraints local to
each building respectively. M  1 artiﬁcial variables (z1,j, . . . , zM1,j)
can be introduced as:

BIis;k ð1Þ 6 ceilðmaxð0; SOC is;max  SOC is;k ÞÞ

ð15Þ

BIis;k ð2Þ 6 ceilðmaxð0; SOC is;k  SOC is;min ÞÞ

ð16Þ

b1;j ðX1 ; YÞ 6 z1;j


ð17Þ

bM;j ðXM ; YÞ 6 hj 

M
X

gmk 6 BIis;k ð2Þ

m¼1

where ceil() rounds the element to the nearest integer towards
inﬁnity; SOCis,max and SOCis,min are maximum and minimum state
of charge for the ice storage; SOCis,k is the initial state of charge
for ice storage at building operation mode k. The state of charge is
a percentage value in this research.
(5) Battery: at each building operation mode k, the battery cannot be charged if the state of charge is at the maximum level
(Eq. (18)) and cannot be discharged if the state of charge is at
the minimum level (Eq. (19)).

ð21Þ

M1
P

ð22Þ
zm;j

m¼1

Thus, the coordination function of the facilitator agent has decision variables of X, Y and Z. It will employ genetic algorithm (GA)
operators including crossover and mutation to explore the global
decision space (X, Y, Z) followed by local search (LS) to exploit the
coupled decision space (Y, Z). The updated decisions are passed
to each building agent who attempts to ‘‘optimize’’ its own objective over the local variables (X) only and feeds the decisions on local variables back to the facilitator agent. At the end of each MA
iteration, the Pareto ﬁlter (Loukil et al., 2007) is applied on the population to ﬁlter out the dominated solution. The proposed decision
framework is illustrated in Fig. 2.

190

M. Hu et al. / European Journal of Operational Research 217 (2012) 185–197

5. Implementation of decentralized memetic algorithm

5.2. Solution representation and population initialization

5.1. Decentralized decision model

Real code GA is used to encode the continuous variables and binary code for the binary variables. Researchers have demonstrated
that utilizing the building thermal mass (pre-cooling building),
and shifting the peaking load by using a storage system can significantly reduce the energy cost (Braun, 2007; Drees and Braun, 1996;
Sun et al., 2006). In this research, the set-point temperature T m
sp;k for
building m at building operation mode k is initialized as follows:

Based on the discussion in Section 4, we construct the decentralized decision model for the building cluster as shown in
m
Fig. 3. The decision variables T m
sp;k , Sis,k, and gk in Table 2 are the
coupled variables Y, while the remaining variables in Table 2 are
the local variables Xm. The constraints in Eqs. (12)–(17) are system
constraints which are handled by the facilitator agent. Artiﬁcial
coupled variables Z are employed to decompose the coupled system constraint in Eq. (12) as two constraints Eqs. (12.1) and
(12.2) shown in Fig. 3. Please note it is not necessary to decompose
constraints in Eqs. (13)–(17) since they do not contain local variables. The constraints in Eqs. (18)–(20) are local constraints handled by each building agent.

Tm
sp;k

8


mU
mL
>
< T mL
k is prepeak period
sp;k þ T sp;k  T sp;k  r=2


¼
mL
mU
mL
>
: T sp;k þ T sp;k  T sp;k  r
otherwise

mU
where the uniform random number r 2 ½0; 1; T mL
sp;k and T sp;k are 74 °F
and 81 °F in this study.

Facilitator
Agent
Start

Partial Population Initialization

Crossover & Mutation

Building Agents

X=X *
Y=Y *
Z=Z*
Fitness Function Evaluation

Simulator

Objective Value
Constraints Feasibility

Building Agents
Select Parents
Sub-problem Optimizer
Y=Y *
Z=Z*

Objective
Value;
Constraints
Feasibility

X=X *
Y=Y *
Z=Z*

Building Agents Local
Decisions

Simulator
X=X*

X=X *
Y=Y *
Z=Z*

Building Agents
Simulator

Local Search
Objective Value
Constraints Feasibility

Pareto Filter

N

ð23Þ

Stop MA
Y
End

Fig. 2. Proposed decentralized framework based on MA.

191

M. Hu et al. / European Journal of Operational Research 217 (2012) 185–197

Fig. 3. Decentralized decision model for building cluster.

The state of the ice storage system Sis,k at peak building operation mode k is initialized as follows:

Sis;k ¼ 2 if r 6 0:8;

otherwise Sis;k ¼ 0

ð24Þ

At off-peak building operation mode k, the state of the ice storage system is

Sis;k ¼ 2 if r 6 0:2; Sis;k ¼ 1 if 0:2 < r 6 0:7;
otherwise Sis;k ¼ 0

ð25Þ

Partial population initialization strategy (Fig. 2) is used to balance the exploration and exploitation search capability. At each
iteration g, we replace the n worst solutions in the population with
new solutions where n is computed as

n ¼ roundðexpððg  1Þ2:6 =G2 Þ  NP Þ

ð26Þ

where round() rounds the element to the nearest integer; G is the
maximum iterations of MA; NP is the population size.
The initial population is generated from the feasible solutions
after building agents check the feasibility of the tentative solutions.
5.3. Fitness function and parents selection
We assign different weights for each solution in the population
to realize various search directions (Arroyo and Armentano, 2005).
The weight combination is randomly generated as

wm ¼ r m =ðr1 þ    þ rM Þ m ¼ 1; . . . ; M

ð27Þ

where the uniform random number rm 2 [0, 1].
NP weight combinations will be generated using Eq. (27). For
each weight combination, we select the solution from the population with best value for the ﬁtness function deﬁned as
M
X

wm fm ðXm ; YÞ

ð28Þ

m¼1

5.4. Crossover and mutation
We apply the 3-points crossover operator which chooses 3 cut
points randomly for the binary variables. Real-parameter crossover
operators (Lozano et al., 2004), which take advantage of numerical
values, are employed
for continuous variables.
Given
 two chromo

somes C 1 ¼ X11 ; . . . ; X1M ; Y1 ; Z11 ; . . . ; Z1M and C 2 ¼ X21 ; . . . ; X2M ; Y2 ;
Z21 ; . . . ; Z2M Þ, the offspring are generated through the following crossover operator as, C 01 ¼ hC 1 þ ð1  hÞC 2 and C 02 ¼ hC 2 þ ð1  hÞC 1 ,
where h 2 [0, 1].
Note the feasibility of the new generated offspring needs to be
checked by each building agent. The mutation operation is triggered if a solution is not feasible where a new feasible solution is
generated to replace the infeasible one.

5.5. Local Search (LS)
The facilitator agent applies LS over coupled variables (Y, Z) to
improve the solutions. The LS adopted in this research is the simulated annealing algorithm with adaptive neighborhood (Zhao,
2011). As a part of memetic algorithms, the simulated annealing
algorithm works as a local optimizer to ﬁnd the local optimal solutions of the weighted system problem.
First, each building agent evaluates the objective functions and
constraints of its sub-problems wrt the coupled variables after the
local variables are obtained by solving the sub-problem. For example, given the coupled variables Y = Y⁄ and Z = Z⁄, the building
agents solve the sub-problems independently and obtain the optimal values of the local variables.
Let G() denote the value of the weighted objective function, S0
denote the best solution found so far, Si denote the current solution
at iteration i, Sc denote the candidate solution, bi is the cooling constant at iteration i, I is the maximum number of iterations of the
simulated annealing; then the simulated annealing algorithm in
the proposed framework is as follows:
Step 1. Facilitator agent sets i = 1, bi = 0.95, and Si = S0
Step 2. A candidate solution Sc is generated according to the following steps.
Step 2.1. The state of the ice storage system is the most critical factor impacting the weighted system objective value. So here we employ a uniform random
number r 2 [0, 1] to control the convergence speed
of the state of the ice storage system. The state of
the ice storage system will be the same as the state
5
in the best solution S0 when r 6 0:1ð1i=IÞ . Otherwise the state of ice storage system will be generated by Eqs. (24) and (25).
Step 2.2. The set-point temperature for building m at building operation mode k


8
m
mL
>
Tm
ðk is pre-peak period and r 6 0:8Þ
>
sp;k  D i; T sp;k  T sp;k
>
>
>
>
>
>
<
or ðk is on peak period and r > 0:8Þ
T m0


sp;k ¼
>
m
mU
m
>
>
>
> T sp;k þ D i; T sp;k  T sp;k ðk is pre-peak period and r > 0:8Þ
>
>
>
:
or ðk is on peak period and r 6 0:8Þ
ð29Þ

Step 2.3. The percentage of energy from the ice storage system to each building is generated as:

gmk 0 ¼

8
M


P
>
>
m
< gm
wm Rm
r 6 wm Rm
p;k =
p;k
k þ D i; 1  gk
>
>
: gm  Di; gm 
k

k

m¼1

otherwise

ð30Þ

192

M. Hu et al. / European Journal of Operational Research 217 (2012) 185–197

where wm is the weight for building m in the weighted system
objective; and Rm
p;k is the average power grid purchase price for
the building m at building operation mode k.
The following function is adopted from (Zhao, 2011):
2

Dði; yÞ ¼ y  ð1  qð1i=IÞ Þ

ð31Þ

where q is a uniform random number from (0, 1).
Step 2.4. The artiﬁcial coupled variables Z are updated only
when at least one of the constraints (12.1) and
(12.2) is violated.
Step 3. Facilitator agent checks the feasibility for the system constraints (12)–(17). Each building agent checks the feasibility of constraints (18)–(20), (12.1) or (12.2), and returns its
objective value and constraint feasibility information to
the facilitator agent.
Step 4. If G(Sc) < G(Si), then a move is made, setting Si+1 = Sc. If
G(Sc) < G(S0), then set S0 = Sc. If G(Sc) P G(Si), then a move
is made to Sc with probability

PðSi ; Sc Þ ¼ expððGðSi Þ  GðSc ÞÞ=bi Þ

ð32Þ

If Sc is rejected then Si+1 = Si.
Step 5. i is set to i + 1, bi is set to 0.95bi1. Go to Step 2 until a stopping criterion is met.

6. Experimental analysis
The proposed MA based framework is applied to study a simple
building cluster (two buildings) located in the Phoenix, Arizona
area. Since Phoenix is known for hot summers when energy usage
is critically important, we take July 21, 2009 as an example day for
the experiments with data from SRP (http://www.srpnet.com), a
local electricity provider.

6.1. Analysis on Pareto Frontier for decentralized decision
The capacity of the power grid is assumed to be 15 kW. The heavy mass building applies the time-of-use (TOU) plan and the light
mass building adopts the SRP EZ-3 option plan. In the EZ-3 plan,
3 pm–6 pm are the peak-hours where the price is much higher than
the off-peak hours. In the TOU plan, 1 pm–8 pm are the peak-hours
where the price is also higher (less than that of EZ-3) than the offpeak hours. During the off-peak hours, the price of the EZ-3 plan is
relatively lower than that of the TOU plan. The following parameters of MA are applied: (1) the MA population size NP is set to 40;
(2) the maximal number of iteration G for MA is 30; (3) the maximal
number of iterations I for the simulated annealing is 20.
The Pareto frontier in the single building energy cost performance space obtained by the proposed framework is shown in
Fig. 4. Five Pareto solutions (A, B, C, D, and E) are highlighted for
energy costs for the heavy mass building vs. the light mass building, where A is (4.97, 9.96), B is (6.38, 8.40), C is (5.19, 9.05), D is
(6.16, 8.44), E is (5.55, 8.88). A centralized decision model where
one optimization problem aiming to minimize the summed energy
cost of the two buildings is formulated. Solution from the centralized model is highlighted as F in Fig. 4 where F is (5.27, 8.99). Solution G (5.13, 10.07) is obtained when the heavy mass building fully
controls the ice storage which means the heavy mass building consumes all the energy from ice storage and the two buildings cooperate with each other to minimize its own energy cost. Solution H
(6.93, 8.89) is the case when the light mass building fully controls
the ice storage. Obviously, solutions G and H are dominated by the
Pareto frontier and it will be more cost effective if the two buildings share ice storage. We also obtain two non-cooperative game
theoretical approaches (leader/follower) solutions I (5.05, 10.59)
and J (6.77, 8.42). In the leader/follower case, the leader will make
the decisions ﬁrst with the assumption that the follower’s behavior
is rational (Lewis and Mistree, 1998), and then the follower will
solve its problem subject to the leader’s decision. Solution I is the
case when the heavy mass building is the leader and the light mass
building is the follower and solution J is when the light mass
building is the leader and the heavy mass building is the follower.

Fig. 4. Pareto frontier on energy cost for each building.

M. Hu et al. / European Journal of Operational Research 217 (2012) 185–197

Comparing solutions I and J with the Pareto solutions A–E, we observe that these two solutions are dominated by the Pareto solutions, and two buildings will have more cost savings when they
cooperate with each other to make decisions.
In addition, we have the overall costs of the two buildings for all
the Pareto solutions and solutions F–J (from Fig. 4) illustrated in
Fig. 5. Pareto solution C achieves minimum total energy cost. This
demonstrates that a Pareto solution can achieve the same cost
effectiveness for the whole system as centralized decision given
an appropriate weight assigned to each building. Solution H (light
mass building fully controls the ice storage system) is the least cost
effective since the light mass building cannot utilize the ice storage
efﬁciently.

193

From Fig. 4, one may argue if the manager is keen on minimizing the energy cost of the heavy mass building (e.g., an apartment
complex with more residents), Solution A is more preferable than C
than E than D than B, and vice versa if the light mass building energy costs are to be minimized. Not surprisingly, Fig. 5 indicates
that a centralized decision F is more cost effective than some
Pareto solutions for the whole system. The Pareto solutions outperform the solutions G–J on the total energy cost. If one building is
heavily preferred over another, the solution is least cost effective
for the whole system (e.g., solutions A, B). Although most Pareto
solutions are inferior to the centralized optimal solution on the total cost metric, the Pareto analysis enables tradeoff analysis on
other dimensions, for example, in this study, thermal comfort

Fig. 5. Overall energy cost of two buildings.

Fig. 6. PPD for two buildings.

194

M. Hu et al. / European Journal of Operational Research 217 (2012) 185–197

they dominate other Pareto solutions on the single building energy
cost space. Obviously, solution C (16.53, 23.29) indicates the heavy
mass building is least dependent on the power grid; and the light
mass building is most independent from the power grid for solution G (17.65, 22.87).
Table 3 summarizes the values for the ten solutions A–J highlighted in Fig. 4 on the four metrics (single building cost; PPD;
PGDR and total cost). The decisions based on these individual metrics are suggested in the last row. We conclude the proposed decision framework is capable of recommending decisions based on
different performance metrics.

which can be represented as the predicted percentage of dissatisﬁed (PPD) (Nassif et al., 2005), and power grid dependency rate
(PGDR) (see Section 3.2 for deﬁnitions).
The Pareto frontier on the average PPD and PGDR metrics based
on all the Pareto solutions on the single building energy cost space
in Fig. 4 are demonstrated in Figs. 6 and 7 respectively. The Pareto
frontier on the PPD performance space is denoted as the line in
Fig. 6. Please note that only solutions B and D are Pareto solutions
on the PPD space, and they dominate other Pareto solutions on the
single building energy cost space. As shown in Fig. 6, solution G
(7.42, 8.18) will be chosen by the decision maker if the heavy mass
building has the higher priority to keep its comfort level. On the
other hand, solution I (8.97, 6.47) can keep the best thermal comfort level for the light mass building. It is observed that the centralized decision F is inferior to most of the decentralized Pareto
solutions in terms of PPD, and it achieves higher cost savings by
sacriﬁcing the comfort level for residents.
Since the ultimate goal of the net-zero building cluster is to be
resilient to disturbance and robust to the power grid, we assume
less dependence on the power grid will be more preferable for
the net-zero building managers. The Pareto frontier on the PGDR
performance space is denoted as the line in Fig. 7. Please note that
only solutions C and E are Pareto solutions on the PGDR space, and

6.2. Decentralized decision under different pricing mechanism
Next, we are interested in studying the impact of different pricing mechanisms on the performance of the decentralized decisions,
and exploring how the decentralized decisions may assist building
managers in choosing a pricing mechanism. Nowadays, it is common that electricity providers are implementing different pricing
mechanisms for on-peak and off-peak hours. Taking SRP as an
example, three pricing mechanisms have been implemented: basic
plan, SRP EZ-3 plan and time-of-use (TOU) plan. In the basic plan,
the price is constant. Details of EZ-3 and TOU plans are discussed in

Fig. 7. PGDR for two buildings.

Table 3
Solutions value and decision makers’ decisions (The best value on each metric is highlighted in bold).
Solutions

A
B
C
D
E
F
G
H
I
J
Recommended decisions

Single building cost ($/day)

PPD (%)

HM

LM

HM

LM

HM

PGDR (%)
LM

Total cost ($/day)

4.97
6.38
5.19
6.16
5.55
5.27
5.13
6.93
5.05
6.77
A

9.96
8.40
9.05
8.44
8.88
8.99
10.07
8.89
10.59
8.42
B

11.59
8.84
10.39
7.83
9.05
9.97
7.42
9.43
8.97
8.18
G

9.58
8.50
11.46
12.39
10.66
12.29
8.18
7.68
6.47
8.95
I

16.71
17.55
16.53
18.29
17.39
16.72
17.65
17.05
17.35
18.37
C

24.29
23.34
23.29
23.14
23.13
23.12
22.87
25.26
24.88
23.60
G

14.93
14.78
14.24
14.60
14.43
14.26
15.20
15.82
15.64
15.19
C

195

M. Hu et al. / European Journal of Operational Research 217 (2012) 185–197

effective, and it is dominated by the basic plan, and the basic plan
is dominated by the TOU plan. This may be due to the fact that
three operation modes are adopted (0 am–1 pm, 1 pm–8 pm and
8 pm–0 am) in this research. Given the peak building operation
mode is from 1 pm to 8 pm which is 2 hours earlier than the peak
price hours in the EZ-3 plan (3 pm–6 pm), the inefﬁcient usage of
the storage system under the EZ-3 plan makes the performance
unfavorable. Since the pricing plans are set by the energy provider (e.g., SRP), one solution is to investigate more building operation modes (e.g., 24 modes on the hourly basis). The challenges
though lie in the increased complexity of the decision model. This
motivates us to improve the computational performance of the
decentralized decision framework to ensure the model is able to

Section 6.1. This pricing mechanism has been widely applied
across the Phoenix metropolitan area and the detailed prices can
be found at SRP website. Please note the building’s decision should
be highly dependent on its pricing mechanism. That is, during the
off-peak period in the basic plan, a building should not utilize the
storage system and the PV panel should power the building. However, during the off-peak period in the EZ-3 plan and the TOU plan
cost savings should be realized using a storage system and the energy from the PV panel being sold back to the grid.
We explore different Pareto frontiers when the two buildings
use the same pricing mechanisms in Fig. 8. ‘‘Basic vs. Basic’’ in
Fig. 8 means that both the HM building and LM building use
the basic plan. It is observed that the EZ-3 plan is the least

Fig. 8. Two buildings use the same pricing mechanism.

10

9.5

10
Basic vs EZ-3
Basic vs TOU
TOU vs TOU

9.5

EZ-3 vs Basic
EZ-3 vs TOU
TOU vs TOU

9

9

9

8.5
8.5

8.5
8

8

8
7.5

7.5
7
4

TOU vs Basic
TOU vs EZ-3
TOU vs TOU

9.5

5

6

7

8

9

7
4
10

7.5
5

6

7

8

7
4.5
9.5

9.5

5

5.5

6.5

Basic vs TOU
EZ-3 vs TOU
TOU vs TOU

9

8.5

6

9
8.5
8

8.5
8
8

7.5

7
4

EZ-3 vs Basic
TOU vs Basic
TOU vs TOU
5

6

7.5
7

8

7
4.5

Basic vs EZ-3
TOU vs EZ-3
TOU vs TOU
5

5.5

7.5

6

6.5

7
4

Fig. 9. Two buildings use different pricing mechanisms.

5

6

7

8

196

M. Hu et al. / European Journal of Operational Research 217 (2012) 185–197

10.5
Unlimited

Light Mass Building Energy Cost ($/day)

Higher
Medium
Lower

10

9.5

9

8.5

4.8

5

5.2

5.4

5.6

5.8

6

6.2

6.4

6.6

6.8

Heavy Mass Building Energy Cost ($/day)
Fig. 10. Pareto frontiers under four capacities.

generate decisions within the shortened time frame in the future
study.
The case where the two buildings use different pricing mechanisms is demonstrated in Fig. 9. ‘‘Basic vs. EZ-3’’ in Fig. 9 means
HM building uses the basic plan and LM building uses the EZ-3 plan,
the horizontal and vertical axis represents daily energy cost of the
HM building and LM building respectively. ‘‘TOU vs. TOU’’ is also
presented in Fig. 9 and it dominates all the other pricing mechanism
combinations. As in Figs. 8 and 9 also indicates that the EZ-3 plan is
not preferred by the two buildings, and the TOU plan is the most
preferred by the two buildings. Under the TOU plan, the two buildings can utilize the building thermal mass (pre-cooling building),
and storage system to reduce the energy cost more effectively. In
addition, we have observed from the experiments that the relative
cooling consumption of the HM building to the LM building during
the on-peak period (1 pm–8 pm) is much smaller than the pre-peak
period (0 am–1 pm) which demonstrates that the HM building can
shift more cooling consumption from the on-peak period to the prepeak period by using the precooling strategy (Zhou et al., 2005). The
results from this experiment may help the decision maker choose
an appropriate pricing mechanism for different buildings (heavy
mass vs. light mass). In addition, the results indicate the EZ-3 plan
is not cost effective from a building perspective which may be a limitation of fewer building operation modes (K = 3). Therefore, it may
be necessary to develop pricing mechanisms with more building
operation modes which is one of the future directions we will
explore.
6.3. Decentralized decision under different power grid capacity
In the ﬁrst set of experiments, the energy from the power grid is
assumed to be at an unlimited level (15 kW). However, in reality,
such an assumption does not always hold. Therefore, we further
investigate the effect of three different power grid capacities on
the performance of the decentralized decisions. We assume the
power capacity for the high level is 12 kW, 10 kW for the medium
level and 8 kW for the lower level. The three Pareto frontiers for
these three capacities are plotted in Fig. 10.
It is observed that the Pareto frontier under the higher level
capacity is almost the same as the Pareto frontier under the

unlimited level, and dominates the Pareto frontiers under the medium and lower level capacity. Pre-cooling a building and shifting
the peaking load to the storage system can signiﬁcantly reduce
the energy cost. However, the power grid capacity restricts the
capability to pre-cool building and charge the storage system at
the pre-peak period under the medium and lower capacity levels.
Only two of the three storage systems (one ice storage and two
batteries) can be charged simultaneously under the medium level
capacity. Moreover, only one of the three storage systems can be
charged under the lower level capacity. It is not surprising that Pareto solutions from the tighter power grid capacity are dominated
by those with more capacity. Therefore, our immediate next step
is to explore the Pareto solutions with four performance metrics
(single building cost, PPD, PGDR, overall cost) for a tighter power
grid capacity where tradeoff analysis is critical.
7. Conclusion
Energy usage has attracted national and even international
attention lately. Yet academic research and industry practices have
focused on either improving the efﬁciency of one sub system of a
building or one single building. In this research, we take a systematic approach with the hypothesis being a group of buildings
jointly sharing energy resources locally will be both cost effective
and resilient to a power disturbance. To test the hypothesis, a hybrid model for building systems which consists of energy consumption, storage and generation subsystems is developed.
Based on this model, decentralized decisions on energy usage for
a cluster of buildings are explored. To accomplish this, we propose
a bi-level decision framework based on MA and different Pareto
frontiers are generated. The derived Pareto frontier can assist
building managers to: (1) make decisions on different metrics
(e.g. single building cost; PPD; PGDR; total cost; etc.); (2) choose
pricing mechanisms effectively; and (3) increase net-zero buildings’ disaster resilience capability. With the development of new
energy technology, we envision that a building cluster concept
and decentralized energy operation decisions will attract more
interest. This research makes the ﬁrst attempt to address some
important issues related to the research. The research outcomes
could gain the insights for developing a building management

M. Hu et al. / European Journal of Operational Research 217 (2012) 185–197

system aiming to reduce energy costs and improve building energy
efﬁciency.
While promising, the proposed framework based on MA is computationally expensive due to the number of sub systems being
modeled and the large number of decision variables. This may
prohibit its application to real time operation decisions which
are usually made on an hourly (or even less) basis. In the future,
the particle swarm optimization (PSO) algorithm which is capable
of deriving good results with very low computational cost
(Reyes-Sierra and Coello, 2006) will be employed to improve the
performance of the decentralized decision framework. With the
improved performance, we can reﬁne the building operation
modes to make it capable of hourly real-time decisions, and
improve the performance for any pricing mechanism. In addition,
uncertainties can be introduced into the energy systems and an
adaptive decentralized optimization algorithm under uncertainty
should be investigated to guarantee buildings can be adaptive to
this dynamic environment. Another extension we will explore is
the chiller efﬁciency. That is, the equipment (valve, control) of
the chillers may have different efﬁciency performance. It is our
intention to extend our simulation model to incorporate this
parameter in the future research.
References
Arroyo, J.E.C., Armentano, V.A., 2005. Genetic local search for multi-objective
ﬂowshop scheduling problems. European Journal of Operational Research 167,
717–738.
Arun, P., Banerjee, R., Bandyopadhyay, S., 2009. Optimum design of batteryintegrated diesel generator systems incorporating demand uncertainty.
Industrial & Engineering Chemistry Research 48, 4908–4916.
Braun, J.E., 2003. Load control using building thermal mass. Journal of Solar Energy
Engineering 125, 292–301.
Braun, J.E., 2007. A near-optimal control strategy for cool storage systems with
dynamic electric rates. HVAC&R Research 13, 557–580.
Corrado, V., Fabrizio, E., Filippi, M., 2007. Modelling and optimization of multienergy source building systems in the design concept phase. In: Proceedings of
Clima 2007 WellBeing Indoors International Conference, Helsinki, vol. 4, pp.
260–268.
Drees, K.H., Braun, J.E., 1996. Development and evaluation of a rule-based control
strategy for ice storage systems. HVAC&R Research 2, 312–336.
El-shatter, T.F., Eskander, M.N., El-Hagry, M.T., 2006. Energy ﬂow and management
of a hybrid wind/PV/fuel cell generation system. Energy Conversion and
Management 47, 1264–1280.
Fanger, P.O., 1970. Thermal Comfort: Analysis and Applications in Environmental
Engineering. McGraw-Hill, New York.
Fong, K., Hanby, V., Chow, T., 2006. HVAC system optimization for energy
management by evolutionary programming. Energy and Buildings 38, 220–231.
Friedman, H., 2009. Wiring the smart grid for energy savings: Integrating buildings
to maximize investment. In: Portland, Oregon: Portland Energy Conservation
Inc. (PECI.).
Garcı́a-González, J., Parrilla, E., Mateo, A., 2007. Risk-averse proﬁt-based optimal
scheduling of a hydro-chain in the day-ahead electricity market. European
Journal of Operational Research 181, 1354–1369.
Hämäläinen, R.P., Mäntysaari, J., 2002. Dynamic multi-objective heating
optimization. European Journal of Operational Research 142, 1–15.
Henze, G.P., 2003a. An overview of optimal control for central cooling plants with
ice thermal energy storage. Journal of Solar Energy Engineering 125, 302.
Henze, G.P., 2003b. Impact of real-time pricing rate uncertainty on the annual
performance of cool storage systems. Energy and Buildings 35, 313–325.
Henze, G.P., 2004. Evaluation of optimal control for active and passive building
thermal storage. International Journal of Thermal Sciences 43, 173–183.
Henze, G.P., Dodier, R.H., 2003. Adaptive optimal control of a grid-independent
photovoltaic system. Journal of Solar Energy Engineering 125, 34–42.
Henze, G.P., Schoenmann, J., 2003. Evaluation of reinforcement learning control for
thermal energy storage systems. HVAC&R Research 9, 259–275.
Henze, G.P., Krarti, M., Brandemuehl, M.J., 2003. Guidelines for improved
performance of ice storage systems. Energy and Buildings 35, 111–127.
Henze, G.P., Kalz, D.E., Liu, S., Felsmann, C., 2005. Experimental analysis of modelbased predictive optimal control for active and passive building thermal storage
inventory. HVAC&R Research 11, 189–213.
Hu, M., Wen, J., Li, F., Haghnevis, M., Khodadadegan, Y., Sanchez, L.M., Wang, S.,
Zhuang, X., Wu, T., 2010. An agent based simulation for building energy system
modeling. In: Proceeding of 2010 ASME Dynamic Systems and Control
Conference, Cambridge, Massachusetts.
Keeney, K., Braun, J.E., 1996. A simpliﬁed method for determining optimal cooling
control strategies for thermal storage in building mass. HVAC&R Research 2,
59–78.

197

Kiesling, L.L., 2009. Markets, technology and institutions: increasing energy
efﬁciency through decentralized coordination. In: EcoAlign Project Energy
Code, series #2.
Kosny, J., Petrie, T., Gawin, D., Childs, P., Desjarlais, A., Christian, J., 2001. ‘‘Thermal
mass – Energy savings potential in residential buildings.’’ Retrieved April 14,
2011, from <http://www.ornl.gov/sci/roofs+walls/research/detailed_papers/
thermal/index.html>.
Lee, W.-S., Chen, Y.T., Wu, T.-H., 2009. Optimization for ice-storage air-conditioning
system using particle swarm algorithm. Applied Energy 86, 1589–1595.
Lewis, K., Mistree, F., 1998. Collaborative, sequential, and isolated decisions in
design. Journal of Mechanical Design 120, 643–652.
Liu, S., Henze, G., 2006a. Experimental analysis of simulated reinforcement learning
control for active and passive building thermal storage inventory. Part 1:
Theoretical foundation. Energy and Buildings 38, 142–147.
Liu, S., Henze, G., 2006b. Experimental analysis of simulated reinforcement learning
control for active and passive building thermal storage inventory. Part 2:
Results and analysis. Energy and Buildings 38, 148–161.
Liu, S., Henze, G.P., 2007. Evaluation of reinforcement learning for optimal control of
building active and passive thermal storage inventory. Journal of Solar Energy
Engineering 129, 215–225.
Lorenzo, E., 2003. Energy collected and delivered by PV modules. In: Handbook of
Photovoltaic Science and Engineering. John Wiley & Sons Ltd, New York, NY, pp.
905–970.
Loukil, T., Teghem, J., Fortemps, P., 2007. A multi-objective production scheduling
case study solved by simulated annealing. European Journal of Operational
Research 179, 709–722.
Lozano, M., Herrera, F., Krasnogor, N., Molina, D., 2004. Real-coded memetic
algorithms with crossover hill-climbing. Evolutionary Computation 12, 273–
302.
Lu, L., 2004. Investigation on characteristics and application of hybrid solar-wind
power generation systems. Unpublished Ph.D., Hong Kong Polytechnic
University (Hong Kong), Hong Kong.
Lu, L., Yang, H.X., 2004. A study on simulations of the power output and practical
models for building integrated photovoltaic systems. Journal of Solar Energy
Engineering 126, 929–935.
Lu, L., Cai, W., Xie, L., Li, S., Soh, Y.C., 2005. HVAC system optimization – In-building
section. Energy and Buildings 37, 11–22.
Manolakos, D., Papadakis, G., Papantonis, D., Kyritsis, S., 2001. A simulationoptimisation programme for designing hybrid energy systems for supplying
electricity and fresh water through desalination to remote areas: case study:
The Merssini village, Donoussa island, Aegean Sea, Greece. Energy 26, 679–704.
Martin, N., Ruiz, J.M., 2001. Calculation of the PV modules angular losses under ﬁeld
conditions by means of an analytical model. Solar Energy Materials and Solar
Cells 70, 25–38.
Nassif, N., Kajl, S., Sabourin, R., 2005. Optimization of HVAC control system strategy
using two-objective genetic algorithm. HVAC&R Research 11, 459–486.
NCDC. ‘‘Quality controlled local climatological data. Retrieved Sept. 15, 2010, from
<http://cdo.ncdc.noaa.gov/qclcd/QCLCD>.
NREL. ‘‘National solar radiation data base.’’ Retrieved Sept. 15, 2010, from <http://
www.nrel.gov/midc/pfci/>.
Parks, N., 2009. Energy efﬁciency and the smart grid. Environmental Science &
Technology 43, 2999–3000.
Reyes-Sierra, M., Coello, C.A.C., 2006. Multi-objective particle swarm optimizers: a
survey of the state-of-the-art. International Journal of Computational
Intelligence Research 2, 287–308.
Rong, A., Lahdelma, R., 2007. An efﬁcient envelope-based branch and bound
algorithm for non-convex combined heat and power production planning.
European Journal of Operational Research 183, 412–431.
Rong, A., Hakonen, H., Lahdelma, R., 2008a. A variant of the dynamic programming
algorithm for unit commitment of combined heat and power systems. European
Journal of Operational Research 190, 741–755.
Rong, A., Lahdelma, R., Luh, P.B., 2008b. Lagrangian relaxation based algorithm for
trigeneration planning with storages. European Journal of Operational Research
188, 240–257.
SRP. ‘‘SRP price plan.’’ Retrieved Sept. 15, 2010, from <http://www.srpnet.com/
menu/PayBillPrice.aspx?res>.
Sun, C., Temple, K., Rossi, T., Braun, J.E., 2006. Interaction between dynamic electric
rates and thermal energy storage control: ﬁnal report for RP-1252. In. Atlanta,
GA: American Society of Heating, Refrigerating and Air-Conditioning Engineers,
Inc.
Torcellini, P., Pless, S., Deru, M., Crawley, D., 2006. Zero energy buildings: A critical
look at the deﬁnition. In: Paciﬁc Grove, California: NREL Report No. CP-55039833.
Valenzuela, J., Mazumdar, M., Kapoor, A., 2000. Inﬂuence of temperature and load
forecast uncertainty on estimates of power generation production costs. IEEE
Transactions on Power System 15, 668–674.
West, J.D., Braun, J.E., 1999. Modeling partial charging and discharging of areaconstrained ice storage tanks. HVAC&R Research 5, 209–228.
Wright, J.A., Loosemore, H.A., Farmani, R., 2002. Optimization of building thermal
design and control by multi-criterion genetic algorithm. Energy and Buildings
34, 959–972.
Zhao, X., 2011. Simulated annealing algorithm with adaptive neighborhood. Applied
Soft Computing 11, 1827–1836.
Zhou, G., Krarti, M., Henze, G.P., 2005. Parametric analysis of active and passive
building thermal storage utilization. Journal of Solar Energy Engineering 127,
37–46.

Proceeding of the 2006 IEEE
International Conference on Automation Science and Engineering
Shanghai, China, October 7-10, 2006

An Adaptive Distributed Simulation Framework for a
Server Fulfillment Supply Chain
Yan Chen, John Fowler, Teresa Wu

Thomas Callarman

Dept. of Industrial Engineering
Arizona State University
Tempe, AZ, United States
{jennychen, john. fowler,
teresa.wu ,thomas.callarman}@asu.edu

China Europe International
Business School
Shanghai, China

Quinn Business School
University Collage Dublin
Dublin, Ireland
{eamonn.ambrose,
vincent.hargaden}@ucd.ie

tecallarman@ceibs.edu

configuration, the servers wait on the docks to be
transported to customers.

. Abstract
– Supply chains that produce and
distribute computer servers are globally dispersed and
have a high degree of uncertainty. To excel at servicing
customers, a supplier must be highly skilled in
matching the assets in the system with customer
demand. Discrete event simulation has been proven
valuable for system state estimation of supply chains.
However, irregularities and disruptions occurring at
any site along the system and the resulting bullwhip
effects can lead to significant departures of simulationbased estimation from the performance of the real
system. These departures reduce the ability of the
model to assist in making correct decisions. In this
paper, we propose an adaptive distributed simulation
framework for a server fulfillment supply chain, and a
Kalman filter to improve our estimates of job
completion times.

IC
s

Wafers

Wafer Fabrication

IC Customers

Assembly
&Test

Server
Fulfillment Servers
Center

Server
Customers
Peripheral
Warehouse

Fig 1. Scope of the Server Fulfillment Supply Chain

Index Terms – Simulation, Server fulfillment supply
chain

For end-customers, good service means on-time receipt
of parts ordered and the required quantities [1]. In this
server fulfillment supply chain, each manufacturing facility
faces a big challenge to meet the quota at the end of the
each quarter. The cost of unfulfilled orders can run into
millions of dollars. Two things in this server fulfillment
supply chain make the task above challenging. The first is
the scope of the problem, which includes complex
manufacturing flow and long cycle time in wafer
fabrication, globally dispersed locations of the wafer
fabrication, assembly and test facilities, and the server
fulfillment center. The second is irregularities and
disruptions occurring at any point in the system without
warning due to the dynamic nature of a supply chain.
Discrete event simulation has been proven valuable as a
practical tool for representing complex interdependencies,
evaluating alternative designs and policies, and analysing
performance tradeoffs for supply chain systems [2, 3, 4, 5,
6, 7]. Jain et al. [8] describe a simulation study on the
supply chain for a large logistics operation. The results
indicate that improvement in forecast accuracy can provide
larger savings than process automation changes.
Applicability of distributed simulation for decision-making
in semiconductor manufacturing has been demonstrated by
Lendermann et al. [9]. Its popularity is also reflected in
industry applications. IBM developed a supply chain

I. INTRODUCTION
Computer server fulfillment supply chain studied in this
work is illustrated in Figure 1. It consists of 6 main
elements: a wafer fabrication facility; an assembly and test
facility; a server fulfillment center; a peripheral warehouse;
end-customers for Integrated Circuits (ICs); and endcustomers for servers. Two types of products are produced
in this system: integrated circuits (ICs) and configured
servers.
The core material transformation flow in this supply
chain is wafers to ICs to servers. The wafers go through an
elaborate process in a wafer fabrication facility in which
thousands of circuits are fabricated on the wafers. Once the
wafers are completed, they are then cut, packaged and
tested to create integrated circuits in an assembly and test
facility. After that, ICs are shipped to either IC customers
or the server fulfillment center depending on demand. In
the server fulfillment center, the ICs go through a series of
panel assemblies and system tests with other peripherals
from a warehouse to configure ordered servers. After

1-4244-0311-1/06/$20.00 ©2006 IEEE

Eamonn Ambrose and Vincent
Hargaden

649

simulator, which has a mix of simulation and optimization
functions, to model and analyse its own supply chain
issues [10]. IBM also used its own simulation-based supply

The High Level Architecture (HLA), which is a framework
developed by the Defense Modeling and Simulation Office
(DMSO), provides the necessary infrastructure for large-

Fig 2. Interaction between Federates in Distributed Simulation System
chain analyzer to visualize and quantify the effects of
making changes on a hypothetical supply chain, and the
impact of the changes on system performance [11].
Further, the need for executing supply chain simulations
based on a full-detailed model has also been pointed out:
Jain et al. [12] compared two models with different levels
of detail for semiconductor manufacturing supply chains.
The result shows that simulations incorporating detailed
models are required when attempting to determine the
correct inventory levels for maintaining desired customer
responsiveness. In these cases, abstracted models can give
inaccurate results that may subsequently lead to erroneous
decisions. Venkateswaran et al. [5] drew similar
conclusions in their paper.
In this paper, we describe how a distributed, detailed
simulation model is built as a prototype for the server
fulfillment supply chain above to be studied. Meanwhile,
we notice that irregularities and disruptions occurring at
any site along the system and their resulting bullwhip
effects can make significant departures of simulation-based
estimates about the system state from the real situation,
which subsequently impairs its functionality in making
correct decisions. To address this issue, we propose a
Kalman filter based approach to calibrate the estimates for
entry and exit times at the wafer fabrication, the assembly
and test facility, the server fulfillment center and the
peripheral warehouse. Some preliminary work has been
done for the server fulfillment center only [13].

scale distributed simulation. In HLA, a federate can be
viewed as a component simulation model that is taking part
in a large simulation [14]. A federation consists of a set of
federates. For instance, in the case of supply chain
simulation, federates can be embodied factories or
suppliers and the federation is then the entire supply chain
itself.
In our distributed simulation framework, the federation
includes 6 federates.
1. IC demand generator
2. Server demand generator
3. Wafer fabrication facility
4. Assembly & Test facility
5. Server fulfillment center
6. Peripheral warehouse
Each federate is a sub-model which executes on
separate process in workstations and can be geographically
distributed. It improves the simulation execution speed,
supports reusability of existing simulation models and
interoperability between different simulation packages.
The simulation model is composed of the basic elements
of a supply chain [12]. These elements include
manufacturing, transportation, business processes and
customer orders as depicted in Figure 2.
Three successive stages of material transformation,
wafers to ICs to servers, are modeled. The transportation
among wafer fabrication facility, assembly and test facility,
server fulfillment center and peripheral warehouse are
modelled. So is the shipment to end-customers.
Forecasting, production and inventory planning that are
related to business processes are incorporated in the model.
Customer orders are generated with the actual rate allowed
to be different from the forecasted rate so as to simulate
real life situations. The major components in this
distributed simulation framework and their interaction are
summarized in Figure 2. The federates interact with each
other through information and material flow. The
information flow is represented using dashed lines while
the material flow using solid lines. Each of the federates is
described below.

II. DISTRIBUTED SIMULATION MODEL OF SERVER
FULFILLMENT SUPPLY CHAIN

The distributed simulation test bed used in our study is
an HLA-based discrete event simulation system that
originated from a semiconductor supply chain simulator
developed in C++ under a joint project between Singapore
Institute of Manufacturing Technology and the School of
Computer Engineering at Nanyang Technological
University, Singapore [12]. The test bed is implemented
using the Run Time Infrastructure (RTI), which is an
implementation of the HLA Interface Specification [14].

650

A. IC Demand Generator (D/G)

The manufacturing process in the server fulfillment
center is shown in Figure 3. The chips provided by the
A&T facility along with other peripherals are put on the
boards in panel assembly. Then the assembly is tested at
this stage. Next, the tested assembly is put together with
additional peripherals to form a basic untested server
system. This basic untested server system then goes for
system test. After the test, the server system is
disassembled (Dekit) and the resulting tested components
are put into storage to fulfill a future customer order. Once
a customer order is issued by the server demand generator,
servers are configured depending on the actual
requirements. The configured customer servers are then
tested and sent for packing and shipping.
The production of servers before the point of fulfillment
assembly is based on a make-to-stock strategy driven by
forecast, whereas the production starting after the
fulfillment assembly is based on a make-to-order strategy.
The server fulfillment center releases multiple chip
modules and other peripherals into production based on the
product’s work-in-progress level in the factory and in
transit, the inventory level in storage, the desired safety
stock level and the forecasted demand for the server. At
this moment, only one type of server is considered in this
study.
Once the orders from the server D/G are issued, based
on the availability of factory capacity and inventory, the
server fulfillment center assigns orders to fulfillment
assembly. Meanwhile, it re-evaluates its stock level and
generates IC demand for Assembly and Test and peripheral
demand for warehouse, if necessary.

The IC D/G generates orders for integrated circuits daily
based on a predefined demand profile. These orders are
then fed to the assembly and test facility. The volume for
the orders of ICs can be varied each week by altering the
demand profile. Each order is randomly assigned a
customer weight and due dates are randomly assigned to
each customer order, based on a uniform distribution.
B. Server Demand Generator (D/G)
The server D/G generator works similarly to the IC
D/G. Server orders are produced daily based on another
predefined demand profile. The volume can be varied each
week by altering the demand profile. Each order is
randomly assigned a customer weight and due dates are
randomly assigned to each customer order, based on a
uniform distribution.
C. Wafer Fabrication
The production of wafers in the wafer fabrication
facility is based on a make-to-stock strategy driven by
forecast, whereas the production of ICs in the assembly
and test facility is based on a make-to-order strategy. The
W/F facility releases wafer lots into production based on
the product’s work-in-progress level in the factory and in
transit, the inventory level of the wafer product in the
warehouse of the A&T facility, the desired safety stock
level and the forecasted demand of the product. For further
details, refer to the paper by Chong et al. [15]. On a daily
basis, wafer fabrication ships completed wafers to the
A&T warehouse with a shipment delay of one day.
The wafer fabrication plant data is based on factory
data from Sematech dataset 1, which is available through
the Internet [16]. It produces two wafer products, which go
through 210 and 245 process steps respectively. There are
32 operator groups in the dataset. The primary dispatching
rules for machines are FIFO and Setup Avoidance (only
for the medium and high current implantation machines).
D.

Start

Panel Assembly

Assembly Test

Fab Assembly

Customer
Requir.

Assembly and Test (A&T)
Assembly Test

The orders from IC D/G and the demand from the server
fulfillment center are fed to the assembly and test facility.
Based on the availability of factory capacity and wafer
inventory, the A&T facility assigns lots to orders and
releases the lots into the facility.
The data for the assembly and test facility is based on
previous projects the authors have had with the
semiconductor industry. The data, particularly volume
release and factory capacities, has been adapted to ensure
that the production quantities and the utilization of
facilities are consistent with what is typically found in the
industry. Approximately 25 process steps exist in the A&T
facility and the major dispatching rule for machines is
FIFO.

Customer System
Test

Dekit

Packing

Storage

Shipping

Fulfillment
Assembly

End

Fig 3. Manufacturing Process in Server Fulfillment Center
F. Peripheral Warehouse
The peripheral warehouse supplies peripherals based on
the demand issued by the server fulfillment center. A lead
time consistent with industrial experience is randomly
generated using a normal distribution.

E. Server Fulfillment Center

651

Transition Equation: xk = G*xk-1 + Hk-1, x  R n
Measurement Equation: zk = H* xk + Kk, z  R m
where Gnxn is the system state matrix that relates the state
at the previous time step k-1 to the present step k, and Hmxn
relates the system states to the measurements. The random
variables Hk and Kk represent the process and measurement
noise respectively. They are assumed to be white noise
with normal distributions: p (Hk) a N (0, Qk) and p (Kk) a N
(0, Rk). The equations for the Kalman filter fall into two
groups: time based equations (Equation 1 and 2), applied
to obtain the current system state, and measurement based
equations (Equation 3-5), used to adjust the system state
from the measurements.
(1)
xˆ k Gxk 1

While the distributed simulation testbed works well to
represent the operations of the real world supply chain, the
uncertainties inherent to supply chains, (such as
unpredictable environmental changes, unpredictable
failures, and emergent behaviour) impose significant
challenges on supply chain management and on supply
chain simulation, which have been designed, planned, and
controlled based on a static, deterministic paradigm of data
acquisition and decision-making, and an (essentially) openloop mode of control. Often, decisions on SC models
cannot be changed until the gap between the planned
system state and actual system state has become
significant, making correction expensive. Therefore, there
is a need for an integrated framework combines actual
system state from the real operation and predicted system
state from simulation results to narrow the gap and provide
better overall system estimation.
Note hardware and software technologies are
developing which could enable a new paradigm of realtime decision making in supply chains. In particular,
technologies like RFID, GPS, grid computing, and
universal access to the World Wide Web promise instant
availability and communication of state change data.
In this study, we propose a framework integrating
Kalman filter and discrete event simulation. The basics of
the Kalman filter are introduced in next section.

Pk

GP k 1 G T  Qk

Kk

Pk H T ( HPk H T  Rk ) 1

k



xk

xˆ  K k z k  Hx

Pk

I  K k H P

(2)

k




k

(3)
(4)
(5)

where Kk is the Kalman gain, Pk is the error covariance,

x̂k is the estimation of xk before the measurement,
and x̂k is the estimation of xk given measurement zk. The
Kalman filter assembles the two groups of equations to
give the best estimate of the system state. The system of
measurement and transition equations can be combined
into an iterative process to determine the state of the
system x.

III. KALMAN FILTER
Kalman filters have traditionally been used for
stochastic estimation and control. Recently, the Kalman
filter has been applied in a variety of applications as well
including Inertial Navigation and Guidance [17], Global
Positioning Systems [18], Target Tracking [19], Finance
[20, 21], etc. In the supply chain domain, Aviv [22]
proposes an adaptive inventory replenishment policy that
utilizes the Kalman filtering technique. Wu and O’Grady
[23] develop an integrated approach that uses Kalman
filtering and a Petri Net model to obtain a better state
estimation of a supply chain system. Vensim
[http://www.vensim.com/], an optimizer tool provided by
Ventana Systems, uses Kalman filtering to track the actual
inventory. Ventana Systems claim that Kalman filtering
tracks the inventory much better than either simple
simulation alone or the measured inventory alone. In this
work, a Kalman filter approach is proposed to take into
account the measurement error to obtain a better state
estimate, namely, the estimated start and end processing
times of jobs at each major supply chain component.
A Kalman filter [24, 25] is a set of mathematical
equations that supports estimations of past, present, and
even future states. The power comes from the fact that it
can do these estimations even when the precise nature of
the modeled system is unknown [26].
Mathematically, a Kalman filter is a set of recursive
equations used to estimate the state x  Rn of a discretetime controlled process such as a manufacturing process
that is governed by a transition equation and a
measurement equation.

IV. INTEGRATED FRAMEWORK
In this section, we describe how to integrate the
distributed simulation system with the Kalman filter to get
an estimate of the entry and exit time for each job at the
wafer fabrication facility, the assembly and test facility, the
server fulfillment center and the peripheral warehouse in
the server fulfillment supply chain.
The proposed framework is shown in Figure 4, where
the estimated state for each job is obtained by running the
distributed simulation model. When a job is in a process,
an emulation module provides real progress update. The
Kalman filter module will calibrate the results and provide
a more realistic estimate.
First, an estimate of when an entity will pass through
every major process is obtained. This estimate is obtained
using simulation. The entire process is simulated for 30
replications. For each replication, the arrival time of the
entity to the system, is kept the same. An estimate of the
queue times for all entities and an estimate of the
processing times for all entities at every process are
calculated by taking the average from the 30 replications.
The standard deviation in these estimates is also calculated.
To emulate the real world, an additional simulation is
run, using the same distribution and using the same
parameters for the processing times as used in the
simulation with 30 replications. We call this emulation. In
other words, measurements obtained from emulation are
assumed to represent measurements obtained from the real

652

variation in
the estimate
of the start
time.
Equation (8)
calculates
the Kalman
correction
required to
account for
the
measuremen
t error in the
state.
Equation (9)
applies the
Kalman
correction
and obtains
the corrected
estimates of
the
state.
Equation (10)
estimates the
variance in the corrected estimate of the state and equation
(11) uses the corrected estimate for intermediate states to
estimate when an entity will complete the last process
(process at server fulfillment center). Equation (12)
estimates the corrected variance in the estimate of the state
when an entity will complete the last process (process at
server fulfillment center). Equation (13) estimates the next
entity state from

world. At
this point,
informatio
n from the
two
different
sources is
available
and both
these
sources
have
variability

Fig 4. Proposed Framework
– simulation has variance in estimation and emulation has
variance in measurement (one source of measurement
variance might be human recording errors). The Kalman
filter can then be applied to generate estimates by
optimally combining simulated predictions with
measurement using equations (6) – (14). The variables in
these equations are defined in Table 1.
(6)
Est L1  E AT  TB E AT o L1 

Vˆ 2 Est L1  Vˆ 2 E AT  TB E AT o L1  Vˆ 2 T BE AT o L1 
(7)
K Li Vˆ Est Li  / (Vˆ Est Li   Vˆ Msd Li )

(8)

Cor Li  Est Li   K Li  * Msd Li   Est Li 

(9)

V̂ 2 Cor Li  Vˆ 2 Est Li   K Li  * Vˆ 2 Est Li 

(10)

Est L12  Cor Li   TBLi o L12 

(11)

V̂ 2 Est L12  Vˆ 2 Cor Li   Vˆ 2 TBLi o L12 

(12)

Est Li 1  Cor Li   TBLi o Li 1 

(13)

V̂ 2 Est Li 1  Vˆ 2 Cor Li   Vˆ 2 TBLi o Li 1 

(14)

2

2

2

the corrected current state. Finally, equation (14) estimates
the variance in the estimate of the next entity state.

Equation (6) estimates the start time of the process at the
wafer fabrication facility and equation (7) estimates the

TABLE I

653

VARIABLE DEFINITION

Li

In addition, a Kalman filter approach is proposed to
calibrate the system state estimate to get more realistic job
completion time forecasts.

Time when entity starts or completes a
process. For example, L1 refers to the start
of the first process, L2 refers to the end of
the first process, L3 refers to the start of the
second process, and so on.

EAT

Entity arrival time

TB(EAT oL1)

Estimate of time between entity arrival
and start time of the first process

TB(Li oLi+1)

Estimate of time between the instant when
entity is at the start of process i and the
end of the process. This accounts for the
queue time between the end and beginning
of a process and the processing time at a
process.

TB(Li oLn)

Estimate of time between the start of one
process to the end of the overall process

Est ( Li )

Estimated time for entity to be at the start
or the end of the process

Msd Li 

Measured time for entity to be at the start
or the end of the process

Cor Li 

Corrected time for entity to be at the start
or the end of the process

K Li 

Kalman correction

Vˆ 2 Est Li 

Estimate of variance in estimated time of
Li

Vˆ 2 Msd Li 

Estimate of variance in measurment of Li
(assumed to be 36)

Vˆ 2 Cor Li 

Estimate of variance in corrected time of
Li

Vˆ 2TBEAT oL1

Estimate of variance in estimated time
between entity arrival time and the start
time of the first process

Vˆ2 TB Li oLi1 

Estimate of variance in estimated time
between the instance when entity is at the
start and the end of the process

Vˆ 2 TB  Li o Ln  

Estimate of variance in estimated time
between the start of one process and the
end of the overall process

REFERENCES
[1] K. Fordyce, “New supply chain management applications provide
better customer service: serious gets exciting,” IBM Microelectronics,
Second Quarter, 2001.
[2] M. Hennessee, “Challenges facing global supply chains in the 21st
century,” Proceedings of the 1998 Winter Simulation Conference, ed.
D.J. Medeiros, E.F. Watson, J.S. Carson and M.S. Manivannan, 3-4.
1998.
[3] L. Chwif, M.R.P. Barretto, E. Saliby. “Supply chain analysis:
Spreadsheet or simulation?” Proceedings of the 2002 Winter
Simulation Conference, ed. E. Yücesan, C.-H. Chen, J.L. Snowdon,
and J.M. Charnes, 59-66. 2002.
[4] S. Jain, N.F. Choong, and W. Lee. “Modeling computer assembly
operations for supply chain integration,” Proceedings of the 2002
Winter Simulation Conference, ed. E. Yücesan, C.-H. Chen, J.L.
Snowdon, and J.M. Charnes, 1165-1173. 2002.
[5] J. Venkateswaran, Y.-J. Son, and B. Kulvatunyou. “Investigation of
influence of modeling fidelities on supply chain dynamics.”
Proceedings of the 2002 Winter Simulation Conference, ed. E.
Yücesan, C.-H. Chen, J.L. Snowdon, and J.M. Charnes, 1183-1191.
2002.
[6] S.T. Enns, and P. Suwanruji. “A simulation test bed for production
and supply chain modeling,” Proceedings of the 2003 Winter
Simulation Conference, ed. S. Chick, P.J. Sánchez, D. Ferrin, and D.J.
Morrice, 1174-1182. IEEE. 2003.
[7] B.P. Gan, L. Liu, S. Jain, S.J. Turner, W. Cai, and W.J. Hsu.
“Distributed supply chain simulation across enterprise boundaries.”
Proceedings of the 2000 Winter Simulation Conference, ed. J.A.
Joines, R.R. Barton, K. Kang and P.A. Fishwick, 1245-1251. 2000.
[8] S. Jain, E.C. Ervin, A.P. Lathrop, R.W. Workman, L.M. Collins.
“Analyzing the supply chain for a large logistics operation using
simulation.” Proceedings of the 2001 Winter Simulation Conference,
ed. B.A. Peters, J.S. Smith, D.J. Medeiros, and M.W. Rohrer, 11231128. 2001.
[9] P. Lendermann, N. Julka, B.P. Gan, D. Chen, L.F. McGinnis, and J.P.
McGinnis. “Distributed supply chain simulation as a decision-support
tool for the semiconductor industry,” Simulation, 79(3): 126-138.
2003.
[10]S. Bagchi, S.J. Buckley, M. Ettl, and G.Y. Lin. “Experience using the
IBM supply chain simulator,” Proceedings of the 1998 Winter
Simulation Conference, ed. D.J. Medeiros, E.F. Watson, J.S. Carson
and M.S. Manivannan, 1387-1394. 1998.
[11]G. Archibald, N. Karabakal, and P. Karlsson. “Supply chain vs.
supply chain: Using simulation to compete beyond the four walls,”
Proceedings of the 1999 Winter Simulation Conference, ed.
Farrington, H.B. Nembhard, D.T. Sturrock, and G.W. Evans, 888896. 1999.
[12]S. Jain, C.C. Lim, B.P. Gan, and Y.H. Low. “Criticality of detailed
modeling in semiconductor supply chain simulation,” Proceedings of
the 1999 Winter Simulation Conference, ed. P.A. Farrington, H.B.
Nembhard, D.T. Sturrock, and G.W. Evans, 888-896. 1999.
[13]D. Parmar, T. Wu, J. Fowler, T. Callarman and V.Hargaden. “An
intergrated framework for responsive supply chain management,” The
16th International Conference on Flexible Automation and Intelligent
Manufacturing, Limerick, Ireland, June, 2006.
[14]S.J. Turner, W. Cai, and B.P. Gan. “Adapting a supply chain
simulation for HLA,” Proceedings of IEEE 4th International
Workshop on Distributed Simulation and Real Time Applications, 7178, San Francisco, USA. 2000.
[15]C.S. Chong, P. Lendermann, B.P. Gan, B.M. Duarte, J.W. Fowler,
and T.E. Callarman. “Analysis of a customer demand driven
semiconductor supply chain in a distributed simulation test bed,”
Proceedings of the 2004 Winter Simulation Conference, ed. R.G.
Ingalls, M.D. Rossetti, J.S. Smith, and B.A. Peters.1902-1909, 2004.
[16]MASMLab Test-Bed datasets [Online], maintained by Modeling and
Analysis of Semiconductor Manufacturing Laboratory, Industrial

Some preliminary work has been done with a focus on
the manufacturing portion of the server fulfillment center
in our previous research [13]. The initial experimentation
results show that using a Kalman filter can help in getting a
more realistic estimate of when an entity is likely to come
out of the server fulfillment center. The improved
estimates are then used to sense whether an entity is on
course to meet customer delivery expectations. In the
future, we will explore it for the entire server fulfillment
supply chain.
V. SUMMARY
This paper describes a computer server fulfillment
supply chain and its modular structure in a prototype
distributed simulation model, and it explores the dynamics
and interaction among the components across the system.

654

Engineering department, Arizona State University, USA.
http://www.eas.asu.edu/~masmlab/ftp.htm, 2004.
[17]P. Zarrchan., "Tactical and strategic missile guidance," AIAA, Inc.,
Washington, DC, 1994.
[18]R.P. Denaro and P.V.W. Loomis., "GPS Navigation Processing and
Kalman filtering," AGARD, NO. 161, pp. 11.1-11.9, 1989.
[19]C.K. Chui and G. Chen, Kalman filtering with real-time applications,
Springer-Verlag, New York, 1987.
[20]C. Wells, The Kalman filter in finance, Kluwer Academic Publishers,
Dordrecht, 1995.
[21]P.J. Bolland and J.T. Connor, "A Constrained neural network Kalman
filter for price estimation in high frequency financial data,"
International Journal of Neural Systems, Vol. 8., No. 4, August,
1997.
[22]Y. Aviv: “A Time series framework for supply chain inventory
management,” Operations Research, Vol. 51, No. 2, pp. 210-227,
2003.
[23]T. Wu and P. O’Grady: “A Methodology for improving the design of
a supply chain,” International Journal of Computer Integrated
Manufacturing, Vol. 17, No. 4, pp. 281-293, 2004.
[24]R.E. Kalman, and R.S. Bucy., "New results in linear filtering and
prediction theory," Transactions of the ASME series D: Journal of
Basic Engineering, 83 (3): 95 - 108, 1961.
[25]R.E. Kalman, "A New approach to linear filtering and prediction
problems," Transactions of the ASME series D: Journal of Basic
Engineering, 82 (1): 35 - 45, 1960.
[26]G. Welch and G. Bishop: “An introduction to the Kalman filter,”
Technical Report No. TR 95-041, Department of Computer Science,
University of North Carolina, 1995.
[27]P.S. Maybeck. Stochastic Models, Estimation, and Control,
Academic Press, Inc., New York, Vol 1, 1979.

655

Applied Soft Computing 25 (2014) 347–359

Contents lists available at ScienceDirect

Applied Soft Computing
journal homepage: www.elsevier.com/locate/asoc

An augmented multi-objective particle swarm optimizer for building
cluster operation decisions
Mengqi Hu a,∗ , Jeffery D. Weir b , Teresa Wu c
a

Department of Industrial and Systems Engineering, Mississippi State University, Mississippi State, MS 39762, USA
Department of Operational Sciences, Graduate School of Engineering & Management, Air Force Institute of Technology, Wright-Patterson AFB,
OH 45433-7765, USA
c
School of Computing, Informatics, Decision Systems Engineering, Arizona State University, Tempe, AZ 85287-5906, USA
b

a r t i c l e

i n f o

Article history:
Received 29 September 2013
Received in revised form 21 March 2014
Accepted 16 August 2014
Available online 8 September 2014
Keywords:
Particle swarm optimization
Multi-objective optimization
Pareto optimality
Smart grid
Smart building

a b s t r a c t
It is envisioned that other than the grid-building communication, the smart buildings could potentially
treat connected neighborhood buildings as a local buffer thus forming a local area energy network through
the smart grid. As the hardware technology is in place, what is needed is an intelligent algorithm that
coordinates a cluster of buildings to obtain Pareto decisions on short time scale operations. Research
has proposed a memetic algorithm (MA) based framework for building cluster operation decisions and
it demonstrated the framework is capable of deriving the Pareto solutions on an 8-h operation horizon
and reducing overall energy costs. While successful, the memetic algorithm is computational expensive
which limits its application to building operation decisions on an hourly time scale. To address this
challenge, we propose a particle swarm framework, termed augmented multi-objective particle swarm
optimization (AMOPSO). The performance of the proposed AMOPSO in terms of solution quality and
convergence speed is improved via the fusion of multiple search methods. Extensive experiments are
conducted to compare the proposed AMOPSO with nine multi-objective PSO algorithms (MOPSOs) and
multi-objective evolutionary algorithms (MOEAs) collected from the literature. Results demonstrate that
AMOPSO outperforms the nine state-of-the-art MOPSOs and MOEAs in terms of epsilon, spread, and
hypervolume indicator. A building cluster case is then studied to show that the AMOPSO based decision
framework is able to make hourly based operation decisions which could signiﬁcantly improve energy
efﬁciency and achieve more energy cost savings for the smart buildings.
© 2014 Elsevier B.V. All rights reserved.

1. Introduction
In the United States, buildings use approximately 70% of the
total electricity usage and emit approximately 40% of the greenhouse gases annually [1]. Today, industry is attempting to design
an intelligent building termed as a “smart building” [2] which is able
to meet the environmental sustainability goals, keep occupants
safe and comfortable, and reduce the energy consumption and cost
[2]. Although energy efﬁcient building materials and appliances
in the smart buildings are capable of energy demand reduction,
it is still not sufﬁcient to satisfy requirements of smart buildings
due to ineffective operation strategies for those efﬁcient appliances
[2]. Therefore, intelligent and effective operation strategies which

∗ Corresponding author. Tel.: +1 662 325 7623; fax: +1 662 325 7618.
E-mail address: mhu@ise.msstate.edu (M. Hu).
http://dx.doi.org/10.1016/j.asoc.2014.08.069
1568-4946/© 2014 Elsevier B.V. All rights reserved.

could achieve the greatest energy efﬁciency are of urgent need for
smart buildings.
The initial study of building operation and control research
focuses on utilizing building thermal mass to achieve cost savings. Pre-cooling a building through optimally controlling building
temperature set-points can signiﬁcantly reduce energy costs [3–6].
Other than using the building thermal mass, extensive research
investigates utilizing thermal storage systems [7–11] and energy
generation systems [12–20] to reduce energy consumption and
energy costs. We want to note that the main stream of research so
far has been on single building. Only recent advancements in technology enable smart buildings in the neighborhood to share energy
as a local energy network [21]. To the best of our knowledge, the
ﬁrst attempt to make operation decisions for multiple buildings
(building cluster) is a memetic algorithm (MA) based framework
[22]. In the building cluster decision model, each building aims
to minimize its energy cost by sharing energy with other buildings, and the MA is employed to solve a multi-objective nonlinear

348

M. Hu et al. / Applied Soft Computing 25 (2014) 347–359

programing problem to derive Pareto operation decisions for the
building cluster to manage the usage of shared energy. It is demonstrated that the building cluster is more energy efﬁcient than a
single building [22]. Due to the poor computational performance of
the MA based decision framework, it is not able to study the hourly
(or even less time scale) operation decision which is expected to
achieve more cost savings [22].
It was demonstrated that particle swarm optimization is capable
of deriving good results with low computational cost in [23]. Therefore we propose an augmented multi-objective particle swarm
optimization (AMOPSO) algorithm to improve the computational
performance of the decision framework. The proposed AMOPSO is
augmented via the fusion of multiple search methods (e.g., subgradient method [24]) to improve its performance in terms of
solution quality and convergence speed, and a crowding distance
method is employed to maintain the non-dominated solutions
found during the search process. To test the efﬁcacy of the proposed
AMOPSO, we ﬁrst compare AMOPSO with several state-of-the-art
multi-objective particle swarm optimization algorithms (MOPSOs) and multi-objective evolutionary algorithms (MOEAs) using
the Zitzler–Deb–Thiele (ZDT) and Deb–Thiele–Laumanns–Zitzler
(DTLZ) benchmark suits [25]. The AMOPSO based bi-level decentralized decision framework is then applied to a building cluster
case to demonstrate its applicability to reach hourly operation decisions for a group of buildings.
This paper is organized as follows: Section 2 brieﬂy reviews the
existing research on building operation decision support; the proposed AMOPSO and its performance assessment are presented in
Section 3; followed by the application of the AMOPSO for building
cluster decentralized operation decision support in Section 4, and
conclusions are drawn in Section 5.
2. Building operation decision support overview
Among all the consumption units, buildings are responsible for
over 70% of electricity consumption with approximately half from
commercial sources and the remainder from residential [26]. However, the fact is between 4 and 20% of energy used for heating,
ventilating and air conditioning (HVAC), lighting and refrigeration
in buildings is wasted due to problems with system operation. Thus,
extensive research has been conducted to develop the operation or
control strategy to improve the energy efﬁciency and reduce energy
costs for buildings.
It is demonstrated that the building thermal mass could be
efﬁciently used to reduce energy consumption and energy cost,
therefore lots of research focuses on pre-cooling the building by
developing an optimal/near-optimal operation strategy to control
the set-point temperature for the HVAC system [3–6]. Pre-cooling
the building could signiﬁcantly reduce energy costs [3–5]. For
example, the optimal strategy for building thermal mass determined by a dynamic programing and on-line simulation based
technique is able to signiﬁcantly reduce energy consumption and
operating cost [5]. A comprehensive review on building thermal
mass operation strategy research is provided in [4].
Similar to the passive thermal storage system (building thermal
mass), the active thermal storage system could shift the energy
demand from peak hour to off-peak hour to balance the energy
demand and reduce energy costs [7–11]. Some meta-heuristic algorithms (e.g., particle swarm optimization [7]) are studied to obtain
an optimal operation strategy for a thermal storage system. The
rule based near-optimal control strategy for a storage system is
determined from monthly simulation of a cooling system in [8].
To efﬁciently utilize the storage system, the model-free reinforcement learning control strategy is studied in [11] and the hybrid
reinforcement learning control approach combining model-based
with model-free method is presented in [9,10].

As the development of on-site generator technology advances,
another mainstream for reducing energy cost is to utilize the
energy generation system which could increase the buildings’
resilience to power disturbances. Extensive research has been
conducted to develop operation strategies to optimally utilizing a generation system [12–20]. For example, the long-term
planning strategy for a single-period combined heat and power
system is derived by a branch and bound algorithm in [12],
and a modiﬁed dynamic programing approach is applied on
a multi-period combined heat and power system planning
in [13]. The short-term production plans for a hydropower
system are developed using multi-stage mixed-integer linear
stochastic programing in [14]. Particle swarm optimization (PSO)
algorithm is also employed to study the generation system
scheduling problems in [15,18]. The multi-objective optimization model is employed to study the power system scheduling in
[17,20].
Based on our knowledge, most of the existing literature focuses
on operation decisions for a single building, and the ﬁrst attempt
to make operation decisions for multiple buildings (building cluster) which could share energy locally or globally is a MA based
framework [22]. A decision model based on a building cluster
simulator with each building modeled by energy consumption,
storage and generation sub modules is developed in [22]. Assuming each building is interested in minimizing its energy cost,
a bi-level operation decision framework based on MA is proposed to study the tradeoff in energy usage among the multiple
buildings [22] and is demonstrated to be more energy efﬁcient
than a single building. In this research, we focus on the operation decisions for a building cluster in a short time scale (e.g.,
hourly) by improving the computation performance of the decision framework with an augmented multi-objective particle swarm
optimization.
3. Proposed augmented multi-objective particle swarm
optimization
3.1. Multi-objective particle swarm optimization overview
Particle swarm optimization which mimics a ﬂock of birds that
communicate together as they ﬂy was proposed in 1995 [27]. During the last two decades, PSO has attracted great attention and has
been successfully applied to various industry applications [28]. In
PSO with inertia weight, the velocity and position for particle p at
iteration i are updated as [29],







i
i
vi+1
= wvip + c1 r1,p
× pip − xip + c2 r2,p
× pig − xip
p

xi+1
p

=

xip

+ vi+1
p



(1)
(2)

where D-dimensional vector vip is the velocity of the pth particle
(vip ∈ [−vmax , +vmax ]), vmax is used to constrain the velocity for each
particle and is usually set between 0.1 and 1.0 times the search
range of the solution space [30]; D-dimensional vector xip is the
position of the pth particle; pip is the best position (pBest) found
so far by the pth particle; pig is the best position (gBest) found so
i
i
far by the swarm; r1,p
and r2,p
represent two independent random
numbers uniformly distributed on [0,1]; c1 is the cognitive learning
factor which represents the attraction that a particle has toward its
own success pip ; c2 is the social learning factor which represents the

attraction that a particle has toward its neighbors’ best position pig ;
w is the inertia weight.
During the last decade, extensive research has been conducted
to study PSO for multi-objective optimization (MOO) problems [23]
due to its simplicity of implementation and good performance. In
general, the algorithms can be classiﬁed in two categories. The

M. Hu et al. / Applied Soft Computing 25 (2014) 347–359

ﬁrst category employs effective approaches (e.g., archive technique,
Pareto ranking approach, etc.) which are utilized in existing MOEAs
to study MOO problems [31–35]. For example, non-dominated sorting PSO [31] adopts a fast non-dominated sorting and sharing
approach to maintain non-dominated solutions which is used in the
non-dominated sorting genetic algorithm (NSGA-II) [36]. MOPSO
proposed in [32] employs a hyper-cubes based adaptive grid technique to produce a well-distributed Pareto frontier. The second
category is to augment PSO methods with focuses on exemplar
learning, parameter control and extend it to study MOO problems
[37–40]. For example, CLMOPSO [37] is an extension of comprehensive learning PSO for multi-objective problems. Cultural-based
MOPSO [40] adaptively changes PSO parameter settings using a
cultural framework.
While great efforts have been spent on exploring the application of PSO and its extensions for multi-objective problems,
there is still much room for improving the performance of MOPSO
to simultaneously handle a diverse search space. It is common
that multi-objective problems consist of different objectives with
different properties (e.g., uni-modal, multi-modal). Thus, in this
research we enhance the MOPSO by intelligently fusing multiple search methods to improve its search capability on a diverse
set of search spaces (e.g., uni-modal, multi-modal). The intelligent multiple search methods fusion technique which fuses a
non-uniform mutation-based method [41] and a sub-gradient
method [24] has been demonstrated to successfully improve the
performance of PSO on a diverse set of single objective optimization problems in terms of solution quality and convergence
speed [42]. Due to its good balance between exploration and
exploitation, the non-uniform mutation-based method [41] may
be preferred by multi-modal functions. The sub-gradient method
[24] is chosen to complement the search capability for unimodal functions because of its performance in ﬁnding a local
optimum very fast and good exploitation capability [43]. At
each iteration, the effective search method is selected by using
roulette wheel selection based on the search method’s performance
[41].
3.2. Augmented multi-objective particle swarm optimization

349

is employed in this research. Several variants of weighted methods [46] such as weighted exponential sum, weighted min-max,
and weighted product method will be investigated in the future
study.

f =

M


wm fm

(3)

m=1

where
wm =

rm

M

(4)

r
m=1 m

rm are uniform random numbers from (0, 1) and m = 1,. . .,M. The

newly generated solution xig will replace the particle xig if it is better


than xig in terms of function f, otherwise xig will replace a randomly
selected particle to introduce randomness to prevent premature
convergence.
The non-uniform mutation-based method [41] is able to have
the merits of large jumping (exploration) at the early stage and
ﬁne-tuning (exploitation) at the later stage [47] which could beneﬁt PSO for multi-modal functions. Like gradient based methods,
the sub-gradient method [24] exhibits good exploitation capability
and could ﬁnd a local optimum very fast [43] which could enhance
PSO’s performance on uni-modal functions. By intelligently fusing these two search techniques in the AMOPSO, it is expected to
balance AMOPSO’s search capability on both the uni-modal and
multi-modal multi-objective optimization problems.
In the non-uniform mutation based method, the dth dimension
of the solution xig is randomly picked to be mutated to generate a
new solution as

	


⎧
i
i
⎨ xg,d
+  i, Ud − xg,d
if r ≥ 0.5
i
	


xg,d
=
⎩ xi −  i, xi − Ld
if r < 0.5
g,d
g,d

(5)

where i is the current iteration index of PSO; Ud and Ld are the upper
i ; r is a uniform random number from (0,
and lower bounds of xg,d
1). The function (i,y) is deﬁned as

The proposed AMOPSO (shown in Fig. 1) has three modules:
(1) PSO module: the swarm is randomly initialized with the
PSO operator being employed to update the swarm. (2) Intelligent multiple search methods module (see Section 3.2.1): two
search methods (non-uniform mutation-based method [41] and
sub-gradient method [24]) are implemented. At each iteration,
an appropriate search method will be triggered based on its
effectiveness index. (3) Archive module (see Section 3.2.2): the
external archive which is used to maintain the non-dominated
solutions set is updated and followed by updating pBest and
gBest for each particle. The algorithm will stop if the stopping criterion (e.g., the maximum number of PSO iterations) is
satisﬁed.
3.2.1. Intelligent multiple search methods
The intelligent multiple search methods module is employed
to enhance the performance of PSO. After each PSO iteration, a
weighted sum function f is randomly generated to select a particle xig in the swarm which has a minimal value on f to be
improved by the multiple search methods module. The main purpose of weighted sum method is to assist the selection of particle
xig which could further improve the quality of the leader used for
the next PSO iteration. It was demonstrated in [44] that integration of weighted sum with population-based algorithm (e.g., GA)
could overcome the disadvantages of weighted sum as discussed
in [45]. Due to its simplicity of implementation, the weighted sum

	

 (i, y) = y × 1 − (1−i/I )

b



(6)

where  is a uniform random number from (0, 1); I is the maximum
number of iterations for PSO; b is a system parameter determining
the degree of dependency on iteration number (non-uniformity).
Sub-gradient method [24] is a commonly used algorithm to
solve the convex optimization, and is also applicable for nonconvex problems. A sub-gradient at solution x ∈  D for a convex
function f is a vector  (x) such that
f (y) ≥ f (x) + (x)T (y − x) , ∀y ∈ D

(7)

Sub-gradient method is equivalent to the gradient-based
method when the objective function is differentiable. In the case
that the objective function is non-differentiable which prohibits the
application of gradient-based method, the sub-gradient method
may still work very well. The sub-gradient method was demonstrated in [42] to be able to enhance PSO’s search performance
on uni-modal function. Therefore, the sub-gradient method is
employed to complement AMOPSO’s performance on uni-modal

functions. In the sub-gradient method, a new solution xig is generated as


xig = xig − ˛i gi

(8)

350

M. Hu et al. / Applied Soft Computing 25 (2014) 347–359

Start
Randomly initialize position xp, velocity vp,
Set c1=c2=1.4961 and w=0.7298;
Set i=1, pp=xp;
Calculate pg
Randomly select one search technique

M2: MS
Module

Adopt S1 or not ?
Y

i=i+1

S1
M1: PSO
Module

N

S2

k=1

p=1
Generate xg’

Generate xg’
Update the velocity and position of
particle p
xg’ is feasible

N

xg’ is feasible

Y
xp is feasible

Y

N
Evaluate xg’

Evaluate xg’

Y
N

Evaluate xp

k=k+1

N
N

p=p+1

xg’ is better than
xg or k>3

p>P

Y

Y
xg’ is better than xg
N

M3:
Archive
Module

Y

Replace a randomly
selected particle with xg’

Update external archive

Replace xg with
xg’

Update pp, pg
Self-adaptive select search technique
for next iteration i+1

N

i>I

Y
End
Fig. 1. Flowchart of AMOPSO (“S1”: non-uniform mutation-based method; “S2”: sub-gradient method).

where gi is the sub-gradient of the objective function f in Eq. (3);
˛i is the step size. The step size in this research is selected to satisfy
the conditions as [24]
˛i ≥ 0,

∞


˛i = ∞, limi→∞ ˛i = 0

(9)

to determine the probability that the sth search method will be
adopted for the following iteration i + 1. Then roulette wheel selection method is employed to choose the effective search method
based on its execution probability. The execution probability probis
is calculated as

i=1

The sub-gradient of the objective function is evaluated as the
gradient of the function if the gradient is available, otherwise the
sub-gradient will be approximated by the Simultaneous Perturbation Stochastic Approximation method [48].
In order to intelligently select the effective search technique,
we assign an execution probability probis to each search method



probis =

⎧
i
⎪
⎨  iis
⎪
⎩ ii=1

ns

1/S

ifns ≥ N, ∀s = 1, . . ., S

(10)

otherwise



probis =

probis

S

s=1

probis

(11)

M. Hu et al. / Applied Soft Computing 25 (2014) 347–359

where ns is the number of iterations that the sth method is adopted;
I is the maximum number of PSO iterations; S is the number
of search methods implemented in AMOPSO which is 2 in this
research; N is the minimal required execution number for each
search method which is set as 50 [42] in this research; is is the
effectiveness index for the sth search method at iteration i which
is deﬁned as

⎧

i
i
⎪
⎨ fg− fg
sth search method is used at iteraion i
fgi 
is =
⎪
⎩
0

(12)

otherwise



where fgi and fgi are the weighted objective value of xig evaluated
in Eq. (3) before and after the multi-method search, respectively
(please note we study a minimization problem in this research).
It is observed from Eq. (10) that each search method has an equal
probability to be selected at the early stage, and the effective search
method tends to be preferred iteration by iteration.

3.2.2. Archive and leader update
In this research, we employ an external archive to maintain
at most Nmax (capacity of the external archive) non-dominated
solutions found during the search process, and the solutions in
the external archive will be reported as the approximated Pareto
solutions when the algorithm stops. At each iteration, the external
archive and particles’ leaders (pBest and gBest) will be updated after
the multiple search methods module. Particle xip is discarded if it is
dominated by any solution in the external archive. Otherwise it will
be added into the external archive and all the solutions in the external archive which are dominated by xip will be removed from the
external archive. The external archive will increase quickly and it
will be computationally prohibitive to update the external archive,
therefore some techniques attempting to keep less crowded solutions by deleting crowded solutions are employed to prune the
archive when its size exceeds the capacity. These techniques are not
only able to bound the archive but also make the non-dominated
solutions evenly spread on the Pareto frontier. The most commonly
used techniques include: (1) adaptive grid [32]: it may be computationally expensive especially when the objective space should
be frequently divided. (2) Clustering technique [49]: the size and
quality of the archive depends on the number of clusters. (3) εDominance [50–53]: the size of the archive is impacted by the user
deﬁned parameter ε. (4) Niche count [31,54]: the spread of the
Pareto frontier depends on the user deﬁned parameter  share . (5)
Crowding distance [37,55]. Due to its good computational performance and independency on user deﬁned parameters, we employ
the crowding distance method as studied in [37,55] to keep the
capacity of the archive as Nmax . For each particle p, a weighted sum
objective function f is randomly generated to assist the selection
of pBest (pip ) and gBest (pig,p )



f =

M





rm

M

r
m=1 m

m=1

fm

(13)

The pBest (pip ) is updated as


pi+1
p

=

 

 

 

 

pip

if f  pip < f  xip

xip

if f  pip ≥ f  xip

(14)

The gBest (pig,p ) is updated as the non-dominated solution in the
external archive which has a minimal f value.

351

Table 1
Features of ZDT and DTLZ benchmark problems.
Name

Dimension D

Search range

Frontier
property

Objective
property

ZDT1
ZDT2
ZDT3
ZDT4

30
30
30
10

Convex
Concave
Disconnected
Convex

f1 : U; f2 : U
f1 : U; f2 : U
f1 : U; f2 : M
f1 : U; f2 : M

ZDT6
DTLZ1
DTLZ2
DTLZ3
DTLZ4
DTLZ5
DTLZ6
DTLZ7

10
30
30
30
30
30
30
30

[0,1]D
[0,1]D
[0,1]D
[0,1] for x1 ,
[−5,5] for
x2,. . .,D
[0,1]D
[0,1]D
[0,1]D
[0,1]D
[0,1]D
[0,1]D
[0,1]D
[0,1]D

Concave
Linear
Concave
Concave
Concave
Concave
Concave
Disconnected

f1 : M; f2 : M
f1 : M; f2 : M
f1 : U; f2 : U
f1 : M; f2 : M
f1 : U; f2 : U
f1 : U; f2 : U
f1 : U; f2 : U
f1 : U; f2 : M

3.3. Performance assessment
To evaluate the overall performance of the proposed AMOPSO
algorithm, we adopt three quality indicators from [25]: additive
1 ), spread (), and hypervolume (HV).
unary epsilon indicator (Iε+
The ﬁrst indicator measures the convergence of the approximated
Pareto frontier, the second indicator measures the diversity of the
approximated Pareto frontier, and the third indicator measures
both convergence and diversity. For the ﬁrst two indicators smaller
values are better, for the third indicator larger value is better. A
detailed explanation of the three indicators are provided in [25].
In the experiments, the swarm size of AMOPSO is set to be 30,
the capacity of the external archive is set as 100, and the maximum number of function evaluations is set to be 10,000 for two
objectives problem and 25,000 for three objectives problem. One
hundred independent runs are conducted to collect data for statistical analysis. The performance of AMOPSO is compared with nine
state-of-the-art MOPSOs and MOEAs by using ﬁve ZDT, seven two
objectives DTLZ and seven three objectives DTLZ benchmark problems [25]. The features of the ZDT and DTLZ are presented in Table 1.
Please note “U” denotes a uni-modal function, and “M” denotes a
multi-modal function.
The nine algorithms compared are: (1) AbYSS (archive-based
hybrid scatter search) [56]; (2) CellDE (cellular differential evolution) [57]; (3) dMOPSO (decomposition based MOPSO) [58]; (4)
MOCell (cellular genetic algorithm for multiobjective optimization) [59]; (5) NSGA-II (non-dominated sorting genetic algorithm II)
[36]; (6) NSGA-IIa (adaptive variation operator selection NSGA-II)
[60]; (7) NSGA-IIr (random variation operator selection NSGAII) [60]; (8) OMOPSO (optimized MOPSO) [61]; and (9) SPEA2
(improved strength Pareto evolutionary algorithm) [62]. These nine
algorithms are implemented in jMetal framework [63]. The opti1 , , and HV are summarized in
mization results in terms of Iε+
Tables 2–4, respectively.
It is observed that AMOPSO outperforms other nine algorithms
on 9 out of 19 benchmark problems in terms of epsilon indicator.
AMOPSO is comparable with the best algorithms for the remaining
10 benchmark problems.
In terms of spread, AMOPSO outperforms other nine algorithms
on 5 out of 19 problems. AMOPSO still could be considered as the
best algorithm which performs best on largest number of problems.
The blank values in Table 4 indicate that the obtained Pareto
frontier is far away from the true Pareto frontier, and the HV result
obtained for this type of Pareto frontier is unreliable [25], therefore
we do not compute the HV value for this type of Pareto frontier.
Please note that a larger value of HV is better. It is observed that
AMOPSO outperforms the nine algorithms on 10 out of 19 problems. AMOPSO is comparable with the best algorithms for the
remaining 9 benchmark problems.

352

M. Hu et al. / Applied Soft Computing 25 (2014) 347–359

Table 2
Median of the epsilon indicator.

AbYSS
CellDE
dMOPSO
MOCell
NSGA-II
NSGA-IIa
NSGA-IIr
OMOPSO
SPEA2
AMOPSO
AbYSS
CellDE
dMOPSO
MOCell
NSGA-II
NSGA-IIa
NSGA-IIr
OMOPSO
SPEA2
AMOPSO
AbYSS
CellDE
dMOPSO
MOCell
NSGA-II
NSGA-IIa
NSGA-IIr
OMOPSO
SPEA2
AMOPSO

ZDT1

ZDT2

ZDT3

ZDT4

ZDT6

DTLZ1-2D

DTLZ2-2D

4.206E − 02
7.077E − 02
8.129E − 03
2.092E − 02
2.622E − 02
2.251E − 02
3.048E − 02
1.060E − 02
4.402E − 02
1.045E − 02
DTLZ3-2D
1.212E + 02
3.694E + 02
6.932E − 01
9.500E + 01
1.306E + 02
3.526E + 02
2.909E + 02
3.194E + 02
1.717E + 02
2.076E − 02
DTLZ3-3D
9.085E + 01
2.858E + 02
5.941E − 01
1.172E + 02
1.152E + 02
2.632E + 02
2.172E + 02
2.956E + 02
1.309E + 02
1.835E − 01

6.155E − 02
4.805E − 02
6.549E − 03
7.801E − 02
6.441E − 02
7.513E − 02
7.947E − 02
8.844E − 03
1.872E − 01
1.011E − 02
DTLZ4-2D
1.251E − 02
7.370E − 02
4.035E − 02
1.000E + 00
1.578E − 02
1.890E − 02
1.774E − 02
2.361E − 02
1.575E − 02
2.563E − 02
DTLZ4-3D
6.304E − 01
1.380E − 01
2.547E − 01
1.430E − 01
1.176E − 01
1.174E − 01
1.285E − 01
1.574E − 01
9.067E − 02
1.446E − 01

1.813E − 01
1.775E − 01
1.578E − 02
5.754E − 02
4.230E − 02
5.547E − 02
6.687E − 02
5.376E − 02
7.534E − 02
6.644E − 03
DTLZ5-2D
1.159E − 02
4.499E − 02
1.046E − 02
9.495E − 03
1.384E − 02
1.557E − 02
1.595E − 02
1.581E − 02
1.377E − 02
9.937E − 03
DTLZ5-3D
7.938E − 03
6.352E − 02
2.133E − 02
8.711E − 03
1.309E − 02
1.327E − 02
1.793E − 02
1.780E − 02
1.291E − 02
7.780E − 03

6.604E − 01
9.251E + 00
1.256E − 02
3.161E − 01
8.058E − 01
1.688E + 00
1.031E + 00
8.224E + 00
1.452E + 00
1.038E − 02
DTLZ6-2D
8.340E + 00
2.359E + 00
7.047E − 03
6.167E + 00
8.104E + 00
6.214E + 00
3.848E + 00
8.327E − 01
9.176E + 00
7.985E − 01
DTLZ6-3D
8.553E + 00
4.333E + 00
1.940E − 02
1.008E + 01
7.636E + 00
6.957E + 00
3.117E + 00
3.230E + 00
7.444E + 00
7.385E − 03

4.762E − 02
9.184E − 03
5.179E − 03
6.165E − 02
4.244E − 01
3.544E − 01
1.241E − 02
6.551E − 03
6.818E − 01
8.113E − 03
DTLZ7-2D
5.360E − 02
1.468E − 01
1.078E − 02
3.929E − 02
6.073E − 02
1.095E − 01
9.148E − 02
1.177E − 02
1.445E − 01
8.674E − 03
DTLZ7-3D
1.292E + 00
7.783E − 01
1.582E − 01
2.866E − 01
1.791E − 01
1.466E − 01
1.759E − 01
2.660E − 01
1.328E − 01
1.754E − 01

4.206E + 01
1.413E + 02
2.306E − 01
3.399E + 01
4.730E + 01
1.284E + 02
1.132E + 02
1.449E + 02
6.186E + 01
8.676E − 03
DTLZ1-3D
3.280E + 01
1.106E + 02
1.665E − 01
5.710E + 01
4.432E + 01
9.552E + 01
8.608E + 01
8.875E + 01
3.913E + 01
5.016E − 02

1.264E − 02
4.572E − 02
1.128E − 02
1.005E − 02
1.535E − 02
1.698E − 02
1.721E − 02
1.656E − 02
1.383E − 02
1.067E − 02
DTLZ2-3D
1.337E − 01
1.465E − 01
1.345E − 01
1.369E − 01
1.292E − 01
1.260E − 01
1.375E − 01
1.540E − 01
8.617E − 02
1.423E − 01

Pairwise comparison results using Wilcoxon test [64] in terms
1 ), spread (), and hypervolume (HV) are
of epsilon indicator (Iε+
1 ), AMOPSO
summarized in Table 5. In terms of epsilon indicator (Iε+
signiﬁcantly outperforms 8 out of 9 algorithms and outperforms

dMOPSO. In terms of spread (), AMOPSO signiﬁcantly outperforms 7 out of 9 algorithms, and outperforms MOCell and OMOPSO.
In terms of hypervolume (HV), AMOPSO signiﬁcantly outperforms
all of the nine algorithms.

Table 3
Median of the spread indicator.

AbYSS
CellDE
dMOPSO
MOCell
NSGA-II
NSGA-IIa
NSGA-IIr
OMOPSO
SPEA2
AMOPSO
AbYSS
CellDE
dMOPSO
MOCell
NSGA-II
NSGA-IIa
NSGA-IIr
OMOPSO
SPEA2
AMOPSO
AbYSS
CellDE
dMOPSO
MOCell
NSGA-II
NSGA-IIa
NSGA-IIr
OMOPSO
SPEA2
AMOPSO

ZDT1

ZDT2

ZDT3

ZDT4

ZDT6

DTLZ1-2D

DTLZ2-2D

4.991E − 01
5.470E − 01
2.836E − 01
2.488E − 01
3.657E − 01
3.653E − 01
3.948E − 01
1.563E − 01
4.371E − 01
3.040E − 01
DTLZ3-2D
9.857E − 01
8.770E − 01
1.479E + 00
9.950E − 01
9.739E − 01
1.099E + 00
9.853E − 01
7.655E − 01
9.851E − 01
5.898E − 01
DTLZ3-3D
8.621E − 01
8.507E − 01
1.524E + 00
9.322E − 01
9.896E − 01
1.006E + 00
8.689E − 01
7.804E − 01
1.016E + 00
9.765E − 01

6.612E − 01
5.128E − 01
1.419E − 01
3.052E − 01
4.974E − 01
4.382E − 01
6.297E − 01
1.427E − 01
7.237E − 01
2.933E − 01
DTLZ4-2D
2.808E − 01
7.043E − 01
1.253E + 00
1.000E + 00
4.074E − 01
4.269E − 01
4.637E − 01
4.077E − 01
3.883E − 01
7.528E − 01
DTLZ4-3D
5.675E − 01
5.596E − 01
1.540E + 00
7.071E − 01
6.822E − 01
6.574E − 01
6.285E − 01
7.880E − 01
5.384E − 01
9.334E − 01

8.729E − 01
8.159E − 01
1.016E + 00
7.420E − 01
7.528E − 01
7.644E − 01
7.879E − 01
8.405E − 01
7.623E − 01
7.248E − 01
DTLZ5-2D
2.401E − 01
6.260E − 01
5.103E − 01
2.275E − 01
3.859E − 01
3.984E − 01
4.358E − 01
3.115E − 01
3.380E − 01
3.134E − 01
DTLZ5-3D
1.886E − 01
5.745E − 01
8.069E − 01
2.939E − 01
4.929E − 01
4.896E − 01
5.765E − 01
5.929E − 01
4.395E − 01
2.997E − 01

9.666E − 01
9.659E − 01
3.355E − 01
8.272E − 01
8.989E − 01
8.901E − 01
9.165E − 01
9.318E − 01
9.330E − 01
2.588E − 01
DTLZ6-2D
8.921E − 01
7.840E − 01
1.871E − 01
8.637E − 01
8.629E − 01
9.675E − 01
8.893E − 01
8.154E − 01
8.807E − 01
4.718E − 01
DTLZ6-3D
8.008E − 01
7.366E − 01
7.385E − 01
6.975E − 01
7.146E − 01
8.656E − 01
8.245E − 01
9.198E − 01
5.533E − 01
3.324E − 01

4.818E − 01
1.232E + 00
1.527E − 01
3.762E − 01
7.244E − 01
7.153E − 01
7.219E − 01
1.168E + 00
8.078E − 01
2.870E − 01
DTLZ7-2D
7.812E − 01
7.453E − 01
7.519E − 01
5.859E − 01
6.584E − 01
6.741E − 01
7.753E − 01
5.399E − 01
7.911E − 01
5.799E − 01
DTLZ7-3D
6.895E − 01
7.102E − 01
1.149E + 00
7.208E − 01
7.619E − 01
7.280E − 01
7.268E − 01
7.056E − 01
6.055E − 01
7.506E − 01

8.768E − 01
8.709E − 01
1.480E + 00
8.554E − 01
8.825E − 01
1.099E + 00
9.217E − 01
8.095E − 01
9.009E − 01
3.824E − 01
DTLZ1-3D
8.538E − 01
6.825E − 01
1.484E + 00
7.854E − 01
8.341E − 01
9.168E − 01
8.041E − 01
7.663E − 01
7.510E − 01
7.177E − 01

2.397E − 01
6.172E − 01
5.262E − 01
2.246E − 01
3.855E − 01
3.957E − 01
4.416E − 01
3.067E − 01
3.355E − 01
3.047E − 01
DTLZ2-3D
7.832E − 01
5.577E − 01
8.945E − 01
6.947E − 01
7.090E − 01
6.504E − 01
6.315E − 01
6.128E − 01
5.288E − 01
6.704E − 01

M. Hu et al. / Applied Soft Computing 25 (2014) 347–359

353

Table 4
Median of the hypervolume indicator.

AbYSS
CellDE
dMOPSO
MOCell
NSGA-II
NSGA-IIa
NSGA-IIr
OMOPSO
SPEA2
AMOPSO
AbYSS
CellDE
dMOPSO
MOCell
NSGA-II
NSGA-IIa
NSGA-IIr
OMOPSO
SPEA2
AMOPSO
AbYSS
CellDE
dMOPSO
MOCell
NSGA-II
NSGA-IIa
NSGA-IIr
OMOPSO
SPEA2
AMOPSO

ZDT1

ZDT2

ZDT3

ZDT4

ZDT6

6.244E − 01
5.683E − 01
6.614E − 01
6.429E − 01
6.373E − 01
6.433E − 01
6.293E − 01
6.566E − 01
6.177E − 01
6.613E − 01
DTLZ3-2D

2.894E − 01
2.859E − 01
3.283E − 01
3.160E − 01
2.873E − 01
2.821E − 01
2.777E − 01
3.256E − 01
2.330E − 01
3.281E − 01
DTLZ4-2D
2.055E − 01
1.414E − 01
1.936E − 01

4.848E − 01
4.122E − 01
5.133E − 01
4.934E − 01
4.959E − 01
4.890E − 01
4.845E − 01
5.005E − 01
4.812E − 01
5.157E − 01
DTLZ5-2D
2.076E − 01
1.643E − 01
2.099E − 01
2.059E − 01
2.046E − 01
2.031E − 01
2.010E − 01
1.968E − 01
2.013E − 01
2.090E − 01
DTLZ5-3D
9.315E − 02
5.491E − 02
9.077E − 02
9.087E − 02
9.111E − 02
9.089E − 02
8.789E − 02
8.596E − 02
8.849E − 02
9.358E − 02

2.109E − 01

3.529E − 01
4.007E − 01
4.013E − 01
3.466E − 01
1.100E − 01
1.586E − 01
3.990E − 01
4.011E − 01
2.296E − 02
4.008E − 01
DTLZ7-2D
3.015E − 01
2.538E − 01
3.330E − 01
3.120E − 01
3.004E − 01
2.720E − 01
2.841E − 01
3.298E − 01
2.622E − 01
3.342E − 01
DTLZ7-3D
2.485E − 01
1.539E − 01
2.416E − 01
2.328E − 01
2.625E − 01
2.774E − 01
2.633E − 01
2.500E − 01
2.645E − 01
2.820E − 01

1.243E − 01

2.020E − 01
1.981E − 01
1.974E − 01
1.870E − 01
1.984E − 01
2.051E − 01
DTLZ4-3D
2.069E − 01
2.826E − 01
2.694E − 01
3.449E − 01
3.608E − 01
3.579E − 01
3.193E − 01
2.995E − 01
3.800E − 01
3.613E − 01

2.066E − 01
DTLZ3-3D

1.885E − 01

3.553E − 01

In summary, AMOPSO outperforms the nine algorithms in terms
of the three performance metrics. OMOPSO was demonstrated
to signiﬁcantly outperform ﬁve state-of-the-art MOPSOs in [25]
in terms of the three performance metrics. Therefore, we could
conclude that AMOPSO signiﬁcantly outperforms the ﬁve state-ofthe-art MOPSOs in [25]. In terms of epsilon indicator, dMOPSO is
a very competitive algorithm due to the fact that the decomposition strategy proposed in [65] and the memory re-initialization
process are employed. The intelligent feedback and replacement
strategy implemented in MOCell [59] could provide effective communication between the archive and population set. It may be
the reason that MOCell is comparable with AMOPSO in terms of
spread. The sub-part (sub-swarm) concept proposed in OMOPSO
[61] could promote diversity of the swarm and improve its capability to generate an evenly distributed Pareto frontier. In the future
study, we will investigate the integration of these techniques (e.g.,
decomposition, feedback strategy, sub-swarm concept) to further
improve the performance of AMOPSO. In addition, the experimental results also demonstrate that AMOPSO could perform well
when the computational resources are limited (smaller number of

6.590E − 01
5.468E − 01
1.065E − 01

6.604E − 01
DTLZ6-2D

2.118E − 01

5.365E − 02
DTLZ6-3D

9.219E − 02

9.458E − 02

DTLZ1-2D

3.413E − 01

4.915E − 01
DTLZ1-3D

5.732E − 01

7.638E − 01

DTLZ2-2D
2.060E − 01
1.615E − 01
2.083E − 01
2.045E − 01
2.026E − 01
2.014E − 01
1.995E − 01
1.948E − 01
2.000E − 01
2.077E − 01
DTLZ2-3D
3.754E − 01
2.857E − 01
3.764E − 01
3.465E − 01
3.645E − 01
3.589E − 01
3.245E − 01
3.126E − 01
3.911E − 01
3.766E − 01

function evaluation). In our earlier study of the decentralized decision support for building cluster, the computational cost to call the
simulator to evaluate energy consumption is very high. Therefore,
a computationally efﬁcient multi-objective optimization algorithm
like AMOPSO which could obtain good results with less computational cost is preferred to study the decentralized decision support
for the building cluster.
4. AMOPSO for building cluster operation decision support
In Section 3, we demonstrate that AMOPSO outperforms the
nine state-of-the-art MOPSOs and MOEAs. In this section, we will
extend the bi-level decentralized decision framework by using
the proposed AMOPSO to further study decentralized decisions
for building clusters. The building cluster simulation model is
introduced in Section 4.1, followed by the building cluster decision
model in Section 4.2, the bi-level decentralized decision framework
using AMOPSO is presented in Section 4.3, and the experimental
results for the building cluster operation decisions are provided in
Section 4.4.

Table 5
Wilcoxon test between AMOPSO and other 9 algorithms.
AMOPSO v.s.

Epsilon indicator

AbYSS
CellDE
dMOPSO
MOCell
NSGA-II
NSGA-IIa
NSGA-IIr
OMOPSO
SPEA2

R+
181
187
132
178
171
165
173
181
165

R−
9
3
58
12
19
25
17
9
25

Spread
p-Value
5.385E − 04
2.137E − 04
1.365E − 01
8.375E − 04
2.225E − 03
4.848E − 03
1.696E − 03
5.385E − 04
4.848E − 03

R+
145
158
173
134
165
162
158
122
148

Hypervolume
R−
45
32
17
56
25
28
32
68
42

p-Value
4.421E − 02
1.124E − 02
1.696E − 03
1.165E − 01
4.848E − 03
7.013E − 03
1.124E − 02
2.772E − 01
3.294E − 02

R+
189
190
154.5
190
190
190
190
189
178

R−
1
0
35.5
0
0
0
0
1
12

p-Value
1.551E − 04
1.318E − 04
1.664E − 02
1.318E − 04
1.318E − 04
1.318E − 04
1.318E − 04
1.551E − 04
8.365E − 04

354

M. Hu et al. / Applied Soft Computing 25 (2014) 347–359
M


4.1. Integrated building energy system simulator
A simpliﬁed building cluster [22] consisting of two different
mass levels—heavy mass (HM) and light mass (LM) buildings is
studied in this research. As illustrated in Fig. 2 [22], an ice storage system and a base chiller are shared by these two buildings,
and each building has a battery and photovoltaic (PV) system. During on-peak hours, the ice storage system is the primary source
to meet the buildings cooling loads with the remaining cooling
request satisﬁed by the base chiller. Each building could not only
request electricity from the power grid, but also could sell its surplus electricity generated by the PV panel back to the power grid.
The operation decisions are evaluated by using the simulator. The
decision model is explained in the following section.

m
Pp,j
≤ Pgrid , j = 1, . . ., Hk ;

k = 1, . . ., K

(16)

m=1
L

U

m
Tim ≤ T̄i,j
≤ Tim , j = 1, . . ., Hk ; k = 1, . . ., K;
M

m=1

m
Qb,j
≤ Qmax,j , j = 1, . . ., Hk ; k = 1, . . ., K













BIis,k (1) ≤ ceil max 0, SOCis,max − SOCis,k
BIis,k (2) ≤ ceil max 0, SOCis,k − SOCis,min
M


m = 1, . . ., M

(17)

(18)
, k = 1, . . ., K

(19)

, k = 1, . . ., K

(20)

m
≤ BIis,k (2) , k = 1, . . ., K
k

(21)

m=1



4.2. Building cluster decision model
In the building cluster decision process, each building could control the set-point temperature of its HVAC system which impacts
the “non-cooling load” in Fig. 2. To efﬁciently utilize the energy
stored in ice storage, the buildings should negotiate with each other
to determine the charging and discharging state of ice storage,
and determine how to distribute the energy discharged from ice
storage to each building. Since the battery and PV are individually
owned by each building, therefore each building could independently determine the charging and discharging states of the battery,
and determine how to use energy from PV (e.g., charging battery,
powering building, or selling back to grid).
To mathematically model the decision process of the building
cluster, let M be the number of buildings, K be the number of building operation modes [11], Hk be the number of hours in the kth
building operation mode, then the decisions for the shared energy
provider and building m (m = 1,. . .,M) at building operation mode k
(k = 1,. . .,K) in this decision model are expressed by: (1) a set of conm for building HVAC set-point temperature; (2)
tinuous variables Tsp,k
a set of integer variables Sis,k for ice storage state (0: dormant; 1:
for
charging; 2: discharging); (3) a set of continuous variables m
k
percentage of energy from ice storage to building m; (4) a set of
m
integer variables Sbat,k
for battery state (0: dormant; 1: charging;
m
2: discharging); and (5) a set of integer variables SPV,k
for PV panel
state (0: dormant; 1: charging battery; 2: powering building; 3:
selling power to grid).
The objective for each building is to minimize its daily energy
cost (see Eq. (15)) subject to several constraints: (1) Power grid
capacity constraint (see Eq. (16)): the total energy requested from
the power grid cannot exceed the grid capacity. (2) Building comfort level constraint (see Eq. (17)): the indoor temperature for the
building should be kept at a comfort level. (3) Base chiller capacity
constraint (see Eq. (18)): the total cooling energy requested from
the base chiller cannot exceed the chiller capacity. (4) Ice storage
state constraint (see Eqs. (19)–(21)): the total percentage of cooling
energy provided by the ice storage system cannot exceed 1. The ice
storage cannot be discharged if it is at the minimum state of charge,
and cannot be charged if it is at a maximum state of charge. (5) Battery state constraint (see Eqs. (22) and (23)): the battery cannot be
discharged if it is at the minimum state of charge, and cannot be
charged if it is at a maximum state of charge. (6) PV panel state
constraint (see Eq. (24)): the PV panel cannot be used to charge the
battery if the battery is not at the charging state.

min fm =

K Hk


k=1 j=1









m
m
m
BIbat,k
− SOCbat,k
(1) ≤ ceil max 0, SOCbat,max



m m
m m
Rp,j
Pp,j − Rs,j
Ps,j , m = 1, . . ., M

(15)

, k = 1, . . ., K;

m = 1, . . ., M



(22)

m
m
m
BIbat,k
− SOCbat,min
(2) ≤ ceil max 0, SOCbat,k

, k = 1, . . ., K;

m = 1, . . ., M

(23)

m
m
BIPV,k
(1) ≤ BIbat,k
(1) , k = 1, . . ., K;

m = 1, . . ., M

(24)

m and Rm ($/kWh) are the energy purchase and selling
where Rp,j
s,j
m and P m (kW) are
price at time j for building m, respectively; Pp,j
s,j
the purchase energy from power grid and selling energy back to
the power grid at time j for building m, respectively; Pgrid (kW)
m is the average indoor temperature
is the power grid capacity; T̄i,j
L

U

for building m at time j; Tim and Tim are 74 ◦ F and 81 ◦ F in this
m (Btu/h) is the cooling energy supplied by the base
research; Qb,j
chiller for building m at time j; Qmax,j is the chiller capacity at time
j; BI is a group of binary intermediate variables to denote the three
m , S m ); ceil(.) rounds the element to the
state variables (Sis,k , Sbat,k
PV,k
nearest integer toward inﬁnity; SOCis,max and SOCis,min are maximum and minimum state of charge for the ice storage; SOCis,k is
the initial state of charge for ice storage at building operation mode
m
m
and SOCbat,min
are maximum and minimum state of
k; SOCbat,max
m
charge for building m’s battery; SOCbat,k
is the battery’s initial state
of charge for building m at building operation mode k. Please refer
to [22] for a detailed explanation of the decision model.
We take the advantage of the bi-level decentralized decision
framework proposed in [22] which has one facilitator agent at the
system level and a number of local optimization engines at the
building level. At the system level, the optimizer classiﬁes decision variables into local variables X (e.g., state of battery, state
of PV system) for each building and coupled variables Y and Z
shared by more than one building (e.g., shared ice storage). The constraints are separated into local constraints which belong to each
building, and system constraints which apply to the group of buildings. By decomposing the decision variables space and constraints,
each building can solve fully independent sub-problems simultaneously which could reduce the overall computational cost. In the
decentralized decision framework, the facilitator agent will assist
buildings to communicate with others to determine the utilization
of shared energy resources. Since each building has its objective to
minimize its daily energy cost, therefore a multi-objective problem
is formulated at the system level which is handled by the facilitator
agent. The decentralized decision model for the building cluster is
m , S
m
constructed in Fig. 3. The decision variables Tsp,k
is,k , and k are
m
the coupled variables Y, while the remaining variables Sbat,k
and
m
SPV,k
are the local variables X. The constraints in Eqs. (16)–(21) are

M. Hu et al. / Applied Soft Computing 25 (2014) 347–359

355

Power Grid

10

Shared Cooling
Non-Cooling Load

PV Panel

9
Base Chiller

8
3

6

Cooling Load

7

Battery

Heavy Mass Building (

)

15
Non-Cooling Load

Dedicated
Chiller

PV Panel

14
4
1
Cooling Load
2

.
.
. 11

Light Mass Building (

13
12

Battery
)

Ice Tank
Ice Storage System

5

By-pass
Flow Control Valve
On/Off Control Valve

Fig. 2. Overall schematic of the integrated building energy system (adopted from [22]).

system constraints which are handled by the facilitator agent. Artiﬁcial coupled variables Z are employed to decompose the coupled
system constraint in Eq. (16) as two constraints Eqs. (16.1) and
(16.2) shown in Fig. 3. The constraints in Eqs. (22)–(24) are local
constraints handled by each building agent.

4.3. AMOPSO based bi-level decentralized decision framework
In the proposed decentralized decision framework, the facilitator agent employs AMOPSO to explore the coupled decision space
Y and Z. Given the updated values of coupled variables provided by
the system optimizer, each building separately optimizes its own
objective over local variables X and updates the local decisions back
to the system optimizer. The “optimum” of the coupled variables
may be shifted in response to the changes from each local optimizer.

The proposed AMOPSO based bi-level decision framework is
illustrated in Fig. 4. Compared to the MA based decision framework, the computational efﬁciency of the AMOPSO based decision
framework could allow us to study the performance of the smart
building cluster at a different decision time scale (e.g., hourly). By
shortening the decision time scale, we may more accurately track
the dynamics of the building system, and derive a more effective
operation strategy to reduce energy consumption and achieve more
cost savings.
4.4. Experimental analysis
The proposed AMOPSO based framework is applied to study a
simple building cluster (two buildings) located in the Phoenix, Arizona area. Since Phoenix is known for hot summers when energy
usage is critically important, we take July 21, 2009 as an example

Fig. 3. Decentralized decision model for building cluster.

356

M. Hu et al. / Applied Soft Computing 25 (2014) 347–359

Facilitator
Agent

Start

Population Initialization

Velocity and Position Update
X=X*
Y=Y*
Z=Z*
Fitness Function Evaluation

Simulation

Objective Value
Constraints Feasibility
X=X*
Y=Y*
Z=Z*

Intelligent Multiple Search

Building Agents

Building Agents
Simulation

Objective Value
Constraints Feasibility
Y=Y*
Z=Z*

Building Agents

BAs Decision
X=X*

Update External Archive

Solve Subproblem
Objective
Value;
Constraints
Feasibility

X=X*
Y=Y*
Z=Z*

Simulation
Particle’s Leaders Update

N

Stop PSO
Y
End

Fig. 4. Proposed bi-level decentralized framework based on AMOPSO.

day for the experiments with data from the Salt River Project (SRP)
(http://www.srpnet.com), a local electricity provider [22].
4.4.1. Comparison between MA and AMOPSO based decision
framework
In this experiment, we attempt to compare the computational
performance of the AMOPSO based framework and MA based
framework. Three building operation modes are considered: (1)
from midnight to the onset of the on-peak period (0 am-1 pm);
(2) the on-peak period (1 pm–8 pm); and (3) from the end of the
on-peak period to midnight (8 pm–0 am). The capacity of the power
grid is assumed to be 15 kW. The heavy mass building applies the
time-of-use (TOU) plan and the light mass building adopts the SRP
EZ-3 option plan. In the EZ-3 plan, 3 pm–6 pm are the peak-hours
where the price is much higher than the off-peak hours. In the TOU
plan, 1 pm–8 pm are the peak-hours where the price is also higher
(less than that of EZ-3) than the off-peak hours. During the off-peak
hours, the price of the EZ-3 plan is relatively lower than that of the
TOU plan. The population/swarm size for both the MA and AMOPSO
is 30, and the archive capacity for both the MA and AMOPSO is 50.

In the MA based decision framework, the maximal number of MA
iterations is 30, and the maximum number of local search iterations is 20. In the AMOPSO based decision framework, the maximal
number of AMOPSO iterations is 60, and the maximum number of
multiple search methods module iterations is 5.
The Pareto frontier in the single building energy cost performance space obtained by the proposed AMOPSO framework is
shown in Fig. 5. The MA based Pareto frontier is also presented
in Fig. 5 for comparison. In the decision model, most of the computational time is spent on calling the simulator to compute building
cooling load, therefore, we use the number of simulator calls to
measure the computational performance of the MA and AMOPSO
based framework. We adopt three quality indicators from [25]:
1 ), spread (), and hypervolume
additive unary epsilon indicator (Iε+
(HV) to evaluate the solution performance of the MA and AMOPSO
based framework in Table 6. All the non-dominated solutions in
the MA based Pareto set and AMOPSO based Pareto set are used to
approximate the true Pareto frontier (PF*).
It is observed from Table 6 that the AMOPSO based decision
framework signiﬁcantly reduces the computational cost (lower

M. Hu et al. / Applied Soft Computing 25 (2014) 347–359

357

Table 6
Comparison between MA based framework and AMOPSO based framework.
Algorithms

No. of simulator calls

1
Epsilon indicator (Iε+
)

Spread ()

Hypervolume (HV)

MA
AMOPSO

11846
2030

1.22E − 01
2.85E − 02

9.27E − 01
5.52E − 01

7.43E − 01
7.65E − 01

Table 7
Comparison between three modes decision and hourly decision.
No. of operation modes

HM building cost ($/day)

LM building cost ($/day)

Total cost ($/day)

HM building PGDR (%)

LM building PGDR (%)

K=3
K = 24

5.01
4.89

8.45
7.53

14.29
13.03

16.49
16.81

23.15
23.03

4.4.2. Decentralized decision under different time scales
In Section 4.4.1, we have demonstrated that the AMOPSO based
decision framework is able to signiﬁcantly improve the computational performance without losing solution accuracy. This more
efﬁcient decision framework enables us to explore decisions on
different time scales, which is expected to achieve cost savings
for the buildings. In this experiment, we study the decentralized
decision under three operation modes and twenty-four operation modes (hourly operation decision). The Pareto frontiers in
the single building energy cost performance space obtained by
the proposed AMOPSO framework for the three modes and hourly
operation decisions are demonstrated in Fig. 6. It is observed that
the Pareto frontier for three modes decision is dominated by the
Pareto frontier for hourly decisions. This is due to the fact that
reﬁning the time scale for the on-peak period to an hourly basis
allows buildings to use the storage system more effectively, which
is able to signiﬁcantly reduce energy cost, and thus achieve more
cost savings.
The minimal energy cost for each building and total energy cost
for the two buildings under three operation modes and 24 operation modes are recorded in Table 7. The power grid dependency rate
(PGDR) proposed in [22] which measures the degree of dependencies of a building to the power grid is adopted to evaluate the
three modes and hourly decisions. The lower value of PGDR is preferred which reﬂects a higher resilience capability of the building
to a power disruption. According to the ﬁve metrics presented in
Table 7, the hourly decision outperforms the three modes decision
in terms of four out of ﬁve metrics which indicates that the hourly
operation decision could achieve more cost savings.

MA based Decision Framework
AMOPSO based Decision Framework

Light Mass Building Energy Cost ($/day)

10
9.8
9.6
9.4
9.2
9
8.8
8.6

10

Light Mass Building Energy Cost ($/day)

number of simulator calls) and is more accurate than the MA based
decision framework (smaller values of epsilon indicator and spread,
larger value of hypervolume). This can also be seen directly in Fig. 5.

Three Modes Decision
Hourly Decision

9.5

9

8.5

8

7.5

5

5.2
5.4
5.6
5.8
6
Heavy Mass Building Energy Cost ($/day)

6.2

6.4

Fig. 6. Decentralized decisions under different time scales.

5. Conclusions
The bi-level decision framework based on the memetic algorithm is demonstrated to be capable of deriving the Pareto solutions
for the building cluster in a decentralized manner [22]. While
promising, the framework is computationally expensive which
prohibits its application to hourly (or even less) based operation decisions. In the research, we propose a bi-level decision
framework based on augmented multi-objective particle swarm
optimization which is capable of deriving good results with very
low computational cost to improve the computational performance
of the decision framework. The proposed AMOPSO outperforms
nine state-of-the-art MOPSOs and MOEAs. The hourly operation
decisions obtained by the AMOPSO based decision framework
enables the buildings to utilize the storage systems in a more efﬁcient way, reduce energy waste and improve energy efﬁciency, and
thus reduce the energy cost.
Although the AMOPSO based decision framework is capable of
hourly decisions, the decision framework is still under deterministic assumptions which means that the noise that exists in weather
(e.g., temperature, solar radiation), sensors and the model itself are
not considered. In the future, we will conduct some statistical analysis to investigate properties of these noises and then integrate
model calibration techniques (e.g., Kalman ﬁlter, particle ﬁlter [66])
to the decision framework to handle these noises. The integrated
decision framework is expected to derive adaptive decisions for the
building cluster which will enable buildings to quickly respond to
the dynamic environment.

8.4
8.2

5

5.2

5.4
5.6
5.8
6
Heavy Mass Building Energy Cost ($/day)

6.2

6.4

Fig. 5. Pareto frontier obtained by the MA and AMOPSO based decision framework.

Acknowledgement
This research was partially supported by funds from the National
Science Foundation award under grant number CNS-1239257 and

358

M. Hu et al. / Applied Soft Computing 25 (2014) 347–359

from the United States Transportation Command (USTRANSCOM)
in concert with the Air Force Institute of Technology (AFIT) under
proposal ID 2009-121. The U.S. Government is authorized to reproduce and distribute for governmental purposes notwithstanding
any copyright annotation of the work by the author(s). The views
and conclusions contained herein are those of the authors and
should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of
USTRANSCOM, AFIT, the Department of Defense, or the U.S. Government.

References
[1] J. Kleissl, Y. Agarwal, Cyber-physical energy systems: focus on smart buildings,
in: Proc. 47th Des. Autom. Conf., 2010, pp. 749–754.
[2] T. Hoffmann, Smart Buildings, Johnson Controls Inc., Milwaukee, WI, 2009.
[3] J.E. Braun, K.W. Montgomery, N. Chaturvedi, Evaluating the performance of
building thermal mass control strategies, HVAC&R Res. 7 (2001) 403–428.
[4] J.E. Braun, Load control using building thermal mass, J. Sol. Energy Eng. 125
(2003) 292–301.
[5] T.Y. Chen, Real-time predictive supervisory operation of building thermal systems with thermal mass, Energy Build. 33 (2001) 141–150.
[6] N. Etik, N. Allahverdi, I.U. Sert, I. Saritas, Fuzzy expert system design for
operating room air-condition control systems, Expert Syst. Appl. 36 (2009)
9753–9758.
[7] W.-S. Lee, Y. Ting Chen, T.-H. Wu, Optimization for ice-storage air-conditioning
system using particle swarm algorithm, Appl. Energy 86 (2009) 1589–1595.
[8] K.H. Drees, J.E. Braun, Development and evaluation of a rule-based control
strategy for ice storage systems, HVAC&R Res. 2 (1996) 312–336.
[9] S. Liu, G.P. Henze, Experimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory Part 1:
Theoretical foundation, Energy Build. 38 (2006) 142–147.
[10] S. Liu, G.P. Henze, Experimental analysis of simulated reinforcement learning
control for active and passive building thermal storage inventory Part 2: Results
and analysis, Energy Build. 38 (2006) 148–161.
[11] S. Liu, G.P. Henze, Evaluation of reinforcement learning for optimal control of
building active and passive thermal storage inventory, J. Sol. Energy Eng. 129
(2007) 215–225.
[12] A. Rong, R. Lahdelma, An efﬁcient envelope-based branch and bound algorithm
for non-convex combined heat and power production planning, Eur. J. Oper.
Res. 183 (2007) 412–431.
[13] A. Rong, H. Hakonen, R. Lahdelma, A variant of the dynamic programming algorithm for unit commitment of combined heat and power systems, Eur. J. Oper.
Res. 190 (2008) 741–755.
[14] S.-E. Fleten, T.K. Kristoffersen, Short-term hydropower production planning by
stochastic programming, Comput. Oper. Res. 35 (2008) 2656–2671.
[15] Y. Wang, J. Zhou, C. Zhou, Y. Wang, H. Qin, Y. Lu, An improved self-adaptive
PSO technique for short-term hydrothermal scheduling, Expert Syst. Appl. 39
(2012) 2288–2295.
[16] G. Mokryani, P. Siano, Optimal wind turbines placement within a distribution
market environment, Appl. Soft Comput. 13 (2013) 4038–4046.
[17] A. Ahmadi, J. Aghaei, H.A. Shayanfar, A. Rabiee, Mixed integer programming
of multiobjective hydro-thermal self scheduling, Appl. Soft Comput. 12 (2012)
2137–2146.
[18] K.K. Mandal, M. Basu, N. Chakraborty, Particle swarm optimization technique based short-term hydrothermal scheduling, Appl. Soft Comput. 8 (2008)
1392–1399.
[19] M. Sakawa, K. Kato, S. Ushiro, M. Inaoka, Operation planning of district heating
and cooling plants using genetic algorithms for mixed integer programming,
Appl. Soft Comput. 1 (2001) 139–150.
[20] K. Deb, F. Ruiz, M. Luque, R. Tewari, J.M. Cabello, J.M. Cejudo, On the sizing
of a solar thermal electricity plant for multiple objectives using evolutionary
optimization, Appl. Soft Comput. 12 (2012) 3300–3311.
[21] SIEMENS, Smart Grids Make it Possible for Building Operators to Actively Participate in the Electricity Market, Siemens Industry Sector, Barcelona, Spain,
2009.
[22] M. Hu, J.D. Weir, T. Wu, Decentralized operation strategies for an integrated
building energy system using a memetic algorithm, Eur. J. Oper. Res. 217 (2012)
185–197.
[23] M. Reyes-Sierra, C.A. Coello Coello, Multi-objective particle swarm optimizers:
a survey of the state-of-the-art, Int. J. Comput. Intell. Res. 2 (2006) 287–308.
[24] S. Boyd, EE364b Course Notes: Sub-Gradient Methods, Stanford University, Stanford University, 2014, available at http://stanford.edu/class/ee364b/
lectures.html on March 10.
[25] J.J. Durillo, J. García-Nieto, A.J. Nebro, C.A. Coello Coello, F. Luna, E. Alba,
Multi-objective particle swarm optimizers: an experimental comparison, in:
M. Ehrgott, C. Fonseca, X. Gandibleux, J.-K. Hao, M. Sevaux (Eds.), Evol. MultiCriterion Optim., Springer, Berlin, Heidelberg, 2009, pp. 495–509.
[26] H. Friedman, Wiring the Smart Grid for Energy Savings: Integrating Buildings
to Maximize Investment, Portland Energy Conservation Inc. (PECI.), Portland,
OR, 2009.

[27] J. Kennedy, R. Eberhart, Particle swarm optimization, Proc. IEEE Int. Conf. Neural
Networks 1995 (1995) 1942–1948.
[28] A.P. Engelbrecht, Fundamentals of Computational Swarm Intelligence, John
Wiley & Sons, Hoboken, NJ, 2006.
[29] Y. Shi, R. Eberhart, A modiﬁed particle swarm optimizer, in: IEEE Int. Conf.
Evol. Comput. Proceedings, 1998. IEEE World Congr. Comput. Intell., 1998, pp.
69–73.
[30] A. Banks, J. Vincent, C. Anyakoha, A review of particle swarm optimization. Part
I: Background and development, Nat. Comput. 6 (2007) 467–484.
[31] X. Li, in: E. Cantú-Paz, J. Foster, K. Deb, L. Davis, R. Roy, U.-M. O’Reilly, et al.
(Eds.), A Non-dominated Sorting Particle Swarm Optimizer for Multiobjective
Optimization, Springer, Berlin, Heidelberg, 2003, p. 198.
[32] C.A. Coello Coello, G. Toscano Pulido, M.S. Lechuga, Handling multiple objectives with particle swarm optimization, IEEE Trans. Evol. Comput. 8 (2004)
256–279.
[33] J.E. Alvarez-Benitez, R.M. Everson, J.E. Fieldsend, A MOPSO algorithm based
exclusively on Pareto dominance concepts, in: C. Coello Coello, A. Hernández
Aguirre, E. Zitzler (Eds.), Evol. Multi-Criterion Optim., Springer, Berlin, Heidelberg, 2005, pp. 459–473.
[34] Y. Wang, Y. Yang, Particle swarm optimization with preference order ranking
for multi-objective optimization, Inf. Sci. (NY) 179 (2009) 1944–1959.
[35] C.K. Goh, K.C. Tan, D.S. Liu, S.C. Chiam, A competitive and cooperative
co-evolutionary approach to multi-objective particle swarm optimization algorithm design, Eur. J. Oper. Res. 202 (2010) 42–54.
[36] K. Deb, A. Pratap, S. Agarwal, T. Meyarivan, A fast and elitist multiobjective
genetic algorithm: NSGA-II, IEEE Trans. Evol. Comput. 6 (2002) 182–197.
[37] V.L. Huang, P.N. Suganthan, J.J. Liang, Comprehensive learning particle swarm
optimizer for solving multiobjective optimization problems, Int. J. Intell. Syst.
21 (2006) 209–226.
[38] P.K. Tripathi, S. Bandyopadhyay, S.K. Pal, Multi-objective particle swarm optimization with time variant inertia and acceleration coefﬁcients, Inf. Sci. (NY)
177 (2007) 5033–5049.
[39] G.G. Yen, W.F. Leong, Dynamic multiple swarms in multiobjective particle
swarm optimization, IEEE Trans. Syst. Man, Cybern., A: Syst. Humans 39 (2009)
890–911.
[40] M. Daneshyari, G.G. Yen, Cultural-based multiobjective particle swarm optimization, IEEE Trans. Syst. Man, Cybern., B: Cybern. 41 (2011) 553–567.
[41] Z. Michalewicz, Genetic Algorithms + Data Structures = Evolution Programs,
third ed., Springer-Verlag, London, UK, 1996.
[42] M. Hu, T. Wu, J.D. Weir, An intelligent augmentation of particle swarm
optimization with multiple adaptive methods, Inf. Sci. (NY) 213 (2012)
68–83.
[43] V. Plevris, M. Papadrakakis, A hybrid particle swarm–gradient algorithm
for global structural optimization, Comput. Civ. Infrastruct. Eng. 26 (2011)
48–68.
[44] F. Li, T. Wu, M. Hu, Design of a decentralized framework for collaborative product design using memetic algorithms, Optim. Eng (2013), http://dx.doi.org/
10.1007/s11081-012-9210-6, published online January 22.
[45] I. Das, J.E. Dennis, A closer look at drawbacks of minimizing weighted sums
of objectives for Pareto set generation in multicriteria optimization problems,
Struct. Multi. Optim. 14 (1997) 63–69.
[46] R.T. Marler, J.S. Arora, Survey of multi-objective optimization methods for engineering, Struct. Multi. Optim. 26 (2004) 369–395.
[47] X. Zhao, Simulated annealing algorithm with adaptive neighborhood, Appl. Soft
Comput. 11 (2011) 1827–1836.
[48] J.C. Spall, Multivariate stochastic approximation using a simultaneous perturbation gradient approximation, IEEE Trans. Autom. Control 37 (1992)
332–341.
[49] S. Mostaghim, J. Teich, Strategies for ﬁnding good local guides in multi-objective
particle swarm optimization (MOPSO), in: Proc. IEEE Swarm Intell. Symp. 2003,
2003, pp. 26–33.
[50] M. Laumanns, L. Thiele, K. Deb, E. Zitzler, Combining convergence and diversity in evolutionary multiobjective optimization, Evol. Comput. 10 (2002)
263–282.
[51] O. Schütze, M. Laumanns, E. Tantar, C.A.C. Coello, E.-G. Talbi, Computing gap free
pareto front approximations with stochastic search algorithms, Evol. Comput.
18 (2010) 65–96.
[52] O. Schütze, M. Laumanns, C. Coello Coello, M. Dellnitz, E.-G. Talbi, Convergence
of stochastic search algorithms to ﬁnite size pareto set approximations, J. Global
Optim. 41 (2008) 559–577.
[53] S. Mostaghim, J. Teich, The role of e-dominance in multi objective particle swarm optimization methods, in: Congr. Evol. Comput. 2003, Canberra,
Australia, 2003, pp. 1764–1771.
[54] M. Salazar-Lechuga, J.E. Rowe, Particle swarm optimization and ﬁtness sharing
to solve multi-objective optimization problems, in: Congr. Evol. Comput. 2005.,
2005, pp. 1204–1211.
[55] C.R. Raquel, P.C. Naval Jr., An effective use of crowding distance in multiobjective particle swarm optimization, in: Proc. 2005 Conf. Genet. Evol. Comput.,
2005, pp. 257–264.
[56] A.J. Nebro, F. Luna, E. Alba, B. Dorronsoro, J.J. Durillo, A. Beham, AbYSS. Adapting scatter search to multiobjective optimization, IEEE Trans. Evol. Comput 12
(2008) 439–457.
[57] J. Durillo, A. Nebro, F. Luna, E. Alba, Solving three-objective optimization problems using a new hybrid cellular genetic algorithm, in: G. Rudolph, T. Jansen,
S. Lucas, C. Poloni, N. Beume (Eds.), Parallel Probl. Solving from Nat.—PPSN X
SE—66, Springer, Berlin, Heidelberg, 2008, pp. 661–670.

M. Hu et al. / Applied Soft Computing 25 (2014) 347–359
[58] S.Z. Martínez, C.A. Coello Coello, A multi-objective particle swarm optimizer
based on decomposition, in: Proc. 13th Annu. Conf. Genet. Evol. Comput., New
York, NY, 2011, pp. 69–76.
[59] A.J. Nebro, J.J. Durillo, F. Luna, B. Dorronsoro, E. Alba, MOCell A cellular genetic
algorithm for multiobjective optimization, Int. J. Intell. Syst. 24 (2009) 726–746.
[60] A. Nebro, J. Durillo, M. Machín, C. Coello Coello, B. Dorronsoro, A study
of the combination of variation operators in the NSGA-II algorithm, in: C.
Bielza, A. Salmerón, A. Alonso-Betanzos, J.I. Hidalgo, L. Martínez, A. Troncoso,
et al. (Eds.), Adv. Artif. Intell. SE—28, Springer, Berlin, Heidelberg, 2013, pp.
269–278.
[61] M. Reyes-Sierra, C.A. Coello Coello, Improving PSO-based multi-objective optimization using crowding, mutation and ∈-dominance, in: C. Coello Coello, A.
Hernández Aguirre, E. Zitzler (Eds.), Evol. Multi-Criterion Optim., Springer,
Berlin, Heidelberg, 2005, pp. 505–519.

359

[62] E. Zitzler, M. Laumanns, L. Thiele, SPEA2. Improving the strength pareto evolutionary algorithm for multiobjective optimization, in: Evol. Methods Des.
Optim. Control with Appl. to Ind. Probl, 2001, pp. 95–100.
[63] J.J. Durillo, A.J. Nebro, jMetal: a Java framework for multi-objective optimization, Adv. Eng. Softw. 42 (2011) 760–771.
[64] S. García, D. Molina, M. Lozano, F. Herrera, A study on the use of non-parametric
tests for analyzing the evolutionary algorithms’ behaviour: a case study on the
CEC’2005 special session on real parameter optimization, J. Heuristics 15 (2009)
617–644.
[65] Q. Zhang, H. Li, MOEA/D. A multiobjective evolutionary algorithm based on
decomposition, IEEE Trans. Evol. Comput. 11 (2007) 712–731.
[66] J. Carpenter, P. Clifford, P. Fearnhead, Improved particle ﬁlter for nonlinear
problems, IEE Proc. Radar Sonar Navig. 146 (1999) 2–7.

Automatic Monitoring of Localized Skin Dose with Fluoroscopic
and Interventional Procedures
Yasaman Khodadadegan,1 Muhong Zhang,1 William Pavlicek,2 Robert G. Paden,2 Brian Chong,2 Beth A.
Schueler,3 Kenneth A. Fetterly,4 Steve G. Langer,3 and Teresa Wu1

This software tool locates and computes the intensity of
radiation skin dose resulting from fluoroscopically
guided interventional procedures. It is comprised of
multiple modules. Using standardized body specific
geometric values, a software module defines a set of
male and female patients arbitarily positioned on a
fluoroscopy table. Simulated X-ray angiographic (XA)
equipment includes XRII and digital detectors with or
without bi-plane configurations and left and right facing
tables. Skin dose estimates are localized by computing
the exposure to each 0.01×0.01 m2 on the surface of a
patient irradiated by the X-ray beam. Digital Imaging and
Communications in Medicine (DICOM) Structured Report
Dose data sent to a modular dosimetry database
automatically extracts the 11 XA tags necessary for
peak skin dose computation. Skin dose calculation
software uses these tags (gantry angles, air kerma at
the patient entrance reference point, etc.) and applies
appropriate corrections of exposure and beam location
based on each irradiation event (fluoroscopy and acquistions). A physicist screen records the initial validation of
the accuracy, patient and equipment geometry, DICOM
compliance, exposure output calibration, backscatter
factor, and table and pad attenuation once per system.
A technologist screen specifies patient positioning,
patient height and weight, and physician user. Peak skin
dose is computed and localized; additionally, fluoroscopy duration and kerma area product values are
electronically recorded and sent to the XA database.
This approach fully addresses current limitations in
meeting accreditation criteria, eliminates the need for
paper logs at a XA console, and provides a method
where automated ALARA montoring is possible including email and pager alerts.

KEY WORDS: Peak skin dose, sentinal event, DICOM
structured report dose, patient entrance reference point,
fluoroscopy, interventional radiology, Joint Commission
(JC), radiation dose, Digital Imaging
and Communications in Medicine (DICOM)
626

BACKGROUND

N

umerous episodes of severe skin damage and
harm to patients have occurred due to
elevated X-ray exposures during fluoroscopically
guided interventional procedures.1,2 Patient skin
doses from the use of X-rays in interventional Xray angiography (XA) procedures continue to
exceed the threshold doses for deterministic effects
such as erythema and epilation.3,4 Since 1994, the
FDA and more recently the Joint Commission5
have specified that skin exposures from fluoroscopy be routinely monitored as a quality assurance (QA) metric. High skin doses that cause
erythema must be identified and the patient must
be informed. Appropriate medical care must be
provided should this occur. An X-ray exposure to
a localized region of the skin (i.e., peak skin dose)
that exceeds 15 Gy is now included as a sentinel
1

From the School of Computing, Informatics and Decision
Systems Engineering, Arizona State University, Tempe, AZ,
85281, USA.
2
From the Department of Radiology, Mayo Clinic Arizona,
13400 East Shea Blvd, Scottsdale, AZ, 85258, USA.
3
From the Department of Radiology, Mayo Clinic Rochester,
200 First St. SW, Rochester, MN, 55905, USA.
4
From the Department of Cardiology, Mayo Clinic Rochester,
200 First St. SW, Rochester, MN, 55905, USA.
Correspondence to: Muhong Zhang, School of Computing,
Informatics and Decision Systems Engineering, Arizona State
University, Tempe, AZ, 85281, USA; e-mail: muhong.zhang@
asu.edu
Copyright * 2010 by Society for Imaging Informatics in
Medicine
Online publication 13 August 2010
doi: 10.1007/s10278-010-9320-7
Journal of Digital Imaging, Vol 24, No 4 (August), 2011: pp 626Y639

AUTOMATIC MONITORING OF LOCALIZED SKIN DOSE

event, and it is mandated to be reported to the Joint
Commission.6
Nonetheless, peak skin dose itself is rarely, if
ever, monitored because of the current inability to
compute and localize the skin exposure to the
patient. Today, monitoring is commonly accomplished via a paper or electronic recording at the
end of examination of the cumulative air kerma
(Gy) to the patient entrance reference point
(PERP),7 kerma area product (KAP or Gym2),
and/or elapsed fluoroscopic time (minutes). The
result of choosing a unit such as KAP to predict an
episode of erythema or epilation leads to assumptions which can easily be overly conservative and
are known to be inaccurate estimates of peak skin
dose.8,9 To address these limitations and to
enhance the routine QA monitoring of XA peak
skin dose exposures, a set of modular simulation
tools and a database were developed. This simulation tool works by capturing required information from Digital Imaging and Communications in
Medicine (DICOM) tags including data from
DICOM Structured Report Dose. With other
system-specific parameters, this tool automatically
maps peak skin dose on the phantom patient.

METHODS

The complete system is comprised of simulated
male and female patients, simulated and configurable C-arms, a robust database that automatically
accepts DICOM tags including DICOM Structured
Report Dose tags (11 are required) ,and the four
additional system parameters that are necessary for
peak dose estimation (these are entered via a
technologist and physicist screen). One additional
tag is read from DICOM header file to determine
whether collimator shape is round or rectangle.
Two-way exchange of data from all XA systems
with the database are needed to complete the
computation, recording, and archiving of patient
dose and location.
The approach taken to estimate skin dose is to
first locate and properly position the virtual patient
on the table. With the center top of the patients
head as the origin, the air kerma to each 0.01×
0.01 m2 region at the surface of the patient likely
to be struck by the X-ray beam is computed.
Knowledge of the shape of the surface and the

627

distance from each region to the origin permits
trigometric determination of the 0.01×0.01 m2
regions exact location in space. Knowledge of
table positions and gantry angulations (validated
data from DICOM and system parameters) specifies the field size on the patient surface. Knowledge of each 0.01×0.01 m2 of the patient surface
relative to PERP permits appropriate corrections
for inverse square law. Other required corrections
also apply backscatter for determination of skin
dose (since both the kVp and the field size on the
patient surface are determined). Corrections also
can be applied for table and pad attenuation and
calibration factors for the exposure (corrections to
the vendors specified value at PERP).
Computed peak skin dose (Gy) and location
recorded in the database after each exam can
automatically initiate email or pager alerts based
upon predetermined threshold values for peak skin
dose (Action Levels).
A schematic of data flow is summarized as
shown in Figure 1.
Male and Female Virtual Patients
SolidWorks (2009 x64 Edition SP5.0 Concord,
MA, USA) simulation software was used to
construct standardized male and female phantoms
using biometric values using NASA, Man-Systems
Integration Standards.10 While multiple phantom
options exist,11,12 mathematical models enable the
straightforward location of any point on the
surface of the model because the geometry, shape,
and dimensions are known. This was considered
advantageous in this application. The height of the
standard adult male phantom is approximately
1.86 m and the weight is ~90 kg, while the height
for adult female phantom is ~1.67 m and weight is
72 kg. While the arms (Figure 2) are included for
completeness, they are not used in the calculation
of skin dose. In clinical practice, the arms are
carefully avoided during any XA procedure and
would be extended away from the region of X-ray
interest. The female phantom breast is included in
this model, and skin dose to the breast is
computed. Because these three-dimensional mathematical male and female models are constructed
from defined elliptical cylinders and cones, coordinates of the surface of the body are derived using
geometric computations. Upon completion of each
procedure, the technologist enters gender, height,

628

KHODADADEGAN ET AL.

Fig 1. Skin dose tool. (1) A physicist validates and enters system parameters once per system. (2) The technologist enters modality,
station ID, and date to retrieve DICOM data from DIT after each patient exam is finished. (3) Technologist checks patient position on the
table and C-arm setup at the end of the exam. (4) Clicking on Skin Dose Calculator performs the calculation and archives a patient skin
dose record to DIT. PDS paper printable dose summary—available with some XA units. These data can be electronically captured if
desired.

and weight estimates, along with patient orientation on the table (supine/prone, feet or head first,
and left or right lateral), and a corresponding
phantom is automatically loaded. Thus, calculation
of all skin dose values are performed after the
exam is finished.
The surface of the phantom is reduced to
regions of 0.01×0.01 m2 (Figure 3). Knowing the
geometry of the phantom (surface) relative to the
placement of the patient on the table permits the
calculated (x, y, f (x, y)) location for each point of
the skin (center of the 0.01×0.01 m2 square),
where f(x, y) maps x and y coordinate of each point
to corresponding z coordinate based on mathematical geometry used in phantom construction.

positioning for technologist and physicist validation. Figure 4 is a typical single C-arm digital
detector configuration with the patient positioned
head first and supine. The direction of +x axis is
the longitude (moving cranial–caudal), while the
direction of +y axis is to the patient left. The +z
axis directs to the front of the patient.13 The

Virtual Equipment Configuration
and Geometry
SolidWorks simulation software was used to
depict conventionally available angiographic Carm equipment including XRII (X-ray image
intensification) and solid state (flat panel digital)
X-ray receptors. The patient can be positioned on
the table with the patient head to the right
(Figure 4) as with head first, or feet to the right
(feet first). The patient can also be positioned as
prone, supine, or left or right side. Software
provides visual confirmation of each episode

Fig 2. Standard male and standard female phantoms. Sizes
are based on NASA, Man-Systems Integration Standards.10

AUTOMATIC MONITORING OF LOCALIZED SKIN DOSE

629

Fig 5. Simulated fluoroscopic X-ray equipment configurations
and system coordination.

Fig 3. Standard male phantom. The surface area of the model
(except for arms) is localized to each 0.01×0.01 m2. These
regions are referenced to the origin (top center of the head).

software simulation image display actually rotates
the displayed gantry following keyboard data entry
(to visualize in comparison with the actual gantry).
This was found particularly useful as an aid in
validating DICOM tag geometry values during
initial physicist acceptance when the patient can be
switched from prone to supine as an example.
Patient—Table Geometry
Primary and Secondary Gantry Angles
The primary gantry angle (Figure 5) is when a
patient is left anterior oblique–right anterior

oblique (LAO-RAO) (y–z plane). At 0°, the Xray tube is beneath the patient and is pointed
vertically upwards. A positive primary angle corresponds to an X-ray beam that enters the patient from
the patient’s right posterior and exits to the patients
left anterior. The secondary angle is the cranial–
caudal (x–z plane). At 0° for secondary angulations,
the X-ray tube is beneath the patient; a positive
secondary angle corresponds with the tube arcing to
the patient toes and detector moving toward the
patient head. Movement of the C-arm or table
horizontally or vertically is discussed below. Note:
Importantly, the technologist enters the patient
positioning (i.e., supine or prone) at the XA operator
console, which alters (as validated) the DICOM tag
values recorded in the database at the end of the
exam for primary and secondary angles. These
values are considered patient centric when matching
with position automatically occurs.

Definition of System Origin and PERP

Fig 4. Displayed gantry angle of −30° for an image intensifier
C-arm. Male patient is supine, centered in y, and head first.

Regardless of recorded values of table movement (longitudinal, lateral, and up–down), we
assume that the origin be always set at the center
of the top of the head of the phantom (Figure 5),
and the coordinate system for all 0.01×0.01 m2
regions is referenced to this point location. The
PERP is a point in space located 0.15 m toward
the X-ray source from the isocenter of rotation of
the C-arm.7 The program calculates the skin
exposure that occurs to each 0.01×0.01 m2 from
the reported exposure to the PERP using knowledge of distances.

630

KHODADADEGAN ET AL.

Table 1. Input data for type I and type II equipment
Input data

Description

Type I

Type II

α1
α2

Primary end angle
Secondary end angle

In DICOM Acquisition. But no fluoroscopy
In DICOM acquisition. But no fluoroscopy

KAP acquisition

Kerma area product per event

KAP fluoroscopy
Acquisition dose

Kerma area product per event
Air kerma at PERP (Gy)

In DSR acquisition and fluoroscopy
In DSR acquisition and fluoroscopy
In DSR acquisition KAP/acquisition
dose=area at PERP
In DSR fluoro KAP/fluoro
dose=area at PERP
In DSR, per event

Fluoroscopy dose Air kerma at PERP (Gy)
Radius
Dist Source to the C-arm Isocenter
Vert. dist isocenter to the table
dv
plane
TLatInc
Table lateral increment
TLongInc
Table longitudinal increment
Patient orientation Orientation of the patient
Horiz dist from patient origin to the
dh
head of table
Horiz dist from current C-arm to
HCH
home position
Horiz dist table home to C-arm
HTC
home position
TL
Table length
Collimator shape Round shape or rectangular

PDS KAP/acquisition dose=area at PERP

In DSR, per event
In DSR

Summary is available in PDS but not used
In PDS, per event
In PDS (not by event) proportionally added
to acquisition dose
Physicist screen data

In
In
In
In

DICOM header file
Default value of 0
Default value of 0
Normally provided as tag

DSR
DSR
DSR
DSR

Technologist screen data

Technologist screen data

Physicist screen data

Physicist screen data

Physicist screen data
Physicist screen data
DICOM header file

Physicist screen data
Physicist screen data
DICOM header file

PDS printable dose summary

Fig 6. Physicist validation interface once per system. Copyrighted and used with permission from Mayo Foundation for Medical
Education and Research. All rights reserved.

AUTOMATIC MONITORING OF LOCALIZED SKIN DOSE

631

Table 3. Vertical distance from origin (top center of patient
head) to the table top for standard phantom

Parameter

Notation

Vertical distance
from origin
(patient head)
to the table top Horigin

Fig 7. Depiction of certain needed distances (listed in Table 2)
based on our proposed coordinate system.

DICOM Tags: Type I and II Equipment
XA equipment for localized skin dose computations are classified into two groups: type I and
type II.
Type I equipment are provided with DICOM
Structured Report Dose (DSR). Each footswitch
activation provides data for both acquisitions
(digital spots, cine, and DSA) and fluoroscopy.
DSR provides 11 of the 16 necessary data to
calculate exposure to each 0.01×0.01 m2 areas.
Four additional input parameters are needed (see
Table 1, type I column). An XA technologist must
enter one distance measurement once per procedure, and three entries are needed to be entered by
the physicist once per system during validation to
calculate localized skin dose.
Type II XA equipment are not provided with DSR
but do provide a number of useful DICOM tags,

Position
of the body

Standard
Standard
female
male body body

Prone

0.10 m

0.09425 m

Left lateral or
right lateral

0.20 m

0.1885 m

which permit estimation of peak skin dose. Type II
equipment operator consoles may be provided with a
printable dose summary (PDS) designed for paper
printing. It is possible to print these data to a virtual
printer and electronically capture dose descriptive
information to the DICOM Index Tracker (DIT)
database. DICOM tag and PDS information are
footswitch specific for acquisition, and therefore,
peak skin dose can be directly calculated for type II
equipment, except for the fluoroscopy component.
Fluoroscopy data in type II equipment are only
provided in summary form. In our implementation,
the total (reported) fluoroscopy exposure is distributed to peak skin dose locations in an amount
proportional to the dose received from acquisitions.
The localized skin dose and locations with acquisition
is first calculated, and the total skin dose is then scaled
prorated to these locations. With type II equipment,
default values are used for lateral and longitudinal
table position. As with type I equipment, an entry of a
distance measure by the technologist and three onetime specifications by the physicist are required.
Physicist System Validation

Table 2. List of required distances for calculating coordinates of
isocenter, source, and PERP
Notation

Definition

Comprehensive reviews of XA equipment geometry, radiation dose measurement, and DICOM tag

Value

Vertical distance
Male, 0.10 m
from origin
(patient head)
to the table top
Horigin
depicted in Figure 7 Female, 0.09425 m
Horizontal distance
from isocenter toward
patient head to the
edge of table—head
IStoTable first position
HCH—HTC−TLongInc+TL/2
Distance from isocenter
to top head of
Distance the patient
|IStoTable−dh|

Table 4. Adjustment required for primary and secondary angles
for different patient positions
Patient position

Primary angle

Secondary angle

Supine—head first
Supine—feet first
Prone—head first
Prone—feet first
Left lateral—head first
Left lateral—feet first
Right lateral—head first
Right lateral—feet first

α1
−α1
π+α1
π−α1
π/2+α1
π/2+α1
−π/2−α1
−π/2−α1

α2
-α2
α2
−α2
α2
−α2
α2
−α2

632

KHODADADEGAN ET AL.

inspection are necessary to implement this approach
for localizing peak skin dose. As an aid to these
tasks, a physicist screen (Figure 6) provides some
assistance in validating the system. Screen data fields
are available for attenuation corrections for table and
pad, separate calibration corrections for fluoroscopy
and acquisitions KAP at PERP, and desired back
scatter factor use. Choice of preferred default values
may be selected. DICOM tag values must be
reviewed and validated, including gantry angulations, table positions, C-arm movement if possible,
and correctness of a patient centric use of distances
and equipment type, including bi-plane and detector
type. The equipment chosen is displayed, and by
clicking on Display, the positional information on
the C-arm image is updated so that the physical
gantry, virtual C-arm, and tag data are co-registered.
Also specified (keystroke entered) are the list of
physician authorized users and ALARA values.

once during validation. The notations in Table 2
are used in the calculation of specified locations.
The following notation is used to represent
indices for calculation.

Computation of Peak Skin Dose

A general formulation corresponding to the
coordinates of X-ray source (xspot, yspot, zspot) and
PERP (xRF, yRF, zRF) is:

IS
RF
Spot
c 1, c 2, c 3, c 4

Index identifying isocenter
Index identifying PERP
Index identifying X-ray source
Corners of area at patient
entrance reference point for
rectangular cone

The coordinates of isocenter (xIs, yIs, zIs) can be
computed as follows:

A “point” within a 0.01×0.01 m2 area is assigned
the computed value. Each representing point on the
discretized surface has the coordinate (m±0.5, n±
0.5, f(m±0.5, n±0.5)) assuming m and n as integers.
To expedite the computation, the irradiated regions
(projected X-ray field on the surface of the patient)
are initially determined using the positioning of the
C-arm. Other patient geometries are discussed
below.
HCH is the horizontal distance from the C-arm
current (radiation event) position to C-arm home
position (home positions for both table and C-arm
are defined by the system (0, 0)). HTC is the
horizontal distance from table home position to Carm home position and is specified by the physicist

xIs ¼ distance

ð1Þ

yIs ¼ TLatInc

ð2Þ

zIs ¼ dv  horigin

ð3Þ

xspot ¼ distance þ Radius  Sinða2 Þ

ð4Þ

yspot ¼ TLatInc  Radius  Sinða1 Þ  Cosða2 Þ

zspot ¼ Radius  Cosða1 Þ  Cosða2 Þ þ dv  horigin ð6Þ

xRF ¼ distance þ 0:15  Sinða2 Þ

ð7Þ

yRF ¼ TLatInc  0:15  Sinða1 Þ  Cosða2 Þ

ð8Þ

zRF ¼ 0:15  Cosða1 Þ  Cosða2 Þ þ dv  horigin

Field size
ICRU TISSUE

Tube voltage (kV)

Filter

HVL (mm AI)

80
80
80
90
90
90

2.5
3.0
3.0
2.5
3.0
3.0

2.78
3.04
4.55
3.17
3.45
5.12

+0.1 mm Cu

ð9Þ

Note: In Eqs. 7, 8, and 9, the length 0.15 m is the
distance from PERP to the X-ray source.

Table 5. Partial table of back scatter factors reproduced from ICRU14 for body tissue

+0.1 mm Cu

ð5Þ

0.1×
0.1 m2

1.33
1.34
1.40
1.34
1.36
1.41

0.2×
0.2 m2

1.39
1.40
1.50
1.41
1.43
1.51

0.25×
0.25 m2

1.39
1.41
1.51
1.42
1.44
1.53

AUTOMATIC MONITORING OF LOCALIZED SKIN DOSE

633

Calculation of Air Kerma to Each Region of Skin
(0.01 × 0.01 m2)
The method of how to isolate the irradiated
points of the patient surface struck by the X-ray
beam is discussed in the Appendix. For each
point of (x, y, z) of the radiated area, we
calculate the distance from the X-ray source to
each 0.01×0.01 m2 point of the applicable area
using Eq. 26 of the Appendix as follows,
Dist ¼k v4 k2

ð10Þ

Since PERP is defined as 0.15 m from isocenter
toward the X-ray source.


Kskin ¼ KPERP  ðRadius  0:15Þ2 =Dist2

ð11Þ

Here, Kskin and KPERP are the air kerma at the
patient skin and PERP, respectively.

Each of the irradiated points at the surface of the
virtual patient for each run is calculated, and
inverse square law corrections are made for
individually differing distances to the surface of
the patient.
Tables 3 and 4 list the needed corrections when
the patient is optionally positioned on the table.
An adjustment must be applied when patient
positioning is feet first; then, distance in Eqs. 1, 4,
and 7 will be represented as Eq. 12.
Distance ¼ TL  IStoTable  dh

ð12Þ

These adjustments would be applied to the
above equations as applicable.
Back Scatter Factor
Table 5 is a partial listing of back scatter factors
that are used, a required and significant correction

Fig 8. Technologist screen validates the patient phantom size and positioning. It automatically connects to the XA database for
calculating and recording patient dose once per procedure. Copyrighted and used with permission from Mayo Foundation for Medical
Education and Research. All rights reserved.

634

KHODADADEGAN ET AL.

to the calculated air kerma.14 A back scatter factor
for a tube voltage of 80 kV, filter 3.0, HVL 3.04,
and field size 0.2×0.2 m2, is 1.40, which is the
default value. Calculated air kerma (Eq. 11) values
are corrected using back scatter factor, thereby
converting air kerma to skin dose.
Skin dose ¼ Kskin  back scatter factor

ð13Þ

Upon summing the skin dose received by each
0.01×0.01 m2 regions of skin (when overlap
occurs with each fluoroscopic and acquisition
event of one exam), the skin dose at each 0.01×
0.01 m2 region is computed.

RESULTS

Technologist Screen
Figure 8 presents the screen used by the
technologist for a procedure. Selecting the gender,
height, and weight of the patient establishes the
match for the phantom. Physician operators are
chosen from a predetermined listing of users.
Patient positioning is noted, including the distance
from the top of the patient’s head to the end of the
table. The equipment selection confirms the imaging lab being used and is used to retrieve the
patient-specific acquisition and fluoroscopy information for calculation at the end of the procedure.

Fig 9. Patient summary dose report. Copyrighted and used with permission from Mayo Foundation for Medical Education and
Research. All rights reserved.

AUTOMATIC MONITORING OF LOCALIZED SKIN DOSE

635

Fig 10. Inverted pyramid with base located at PERP. Each side of the pyramid intercepts the skin of the patient to parse those radiated
sections of the skin.

Clicking on “Calculate Skin Dose” results in a
patient-specific dose screen (Figure 9).
Patient Dose Summary Screen
This tool localizes the peak skin dose only upon
completion of a procedure. A completed XA exam
is sent to PACs, and a copy of all data are received
by the XA database module in DIT.15 This database captures DICOM tags and maintains longitudinal record of patient specific dose indices for
all diagnostic and procedural uses of ionizing
radiation. It is comprised of a knowledge base of
known devices (i.e., modalities), the standard and
proprietary DICOM tags, a patient episode dose
tracking database, dosimetry analyzer tools, a

Fig 11. Cone with circular base.

configurable web reporter, and an alert and automessaging reporting mechanism. The XA Skin
Dose tool connects to this database and fetches the
required data. An Excel macro automatically
retreives to the imaging suite all available XA tags.
With the four required system parameters, the
program performs the calculations and localizes the
peak skin dose. A Patient Summary screen displays
the results and other data that may be collected by the
XA techologist for QA monitoring. Action Levels of

Fig 12. Checking a point whether it is inside the cone or not.

636

KHODADADEGAN ET AL.

localized dose are color-coded on the patient
phantom. Individual 0.01×0.01 m2 computed values
describing localized dose are sent to DIT, which
permits the alerting function of DIT to email or page
the physician user and the QA staff when ALARA
criteria are met (Figure 9).
Automatic email, cell phone, and internal pager
alerts from the DIT are in use.

DISCUSSION

A clear need exists to more fully communicate to
the patient, medical staff, and the hospital QA review
committee the actual or likelihood of a deterministic
radiation effect following an XA procedure. Minutes
of fluoroscopy are highly inaccurate, while KAP and
cumulative exposure are quite variable in estimating
peak skin dose.16 The impediments to recording the
localized skin dose value are founded in the need for
having a reasonable representation of the patient’s
specific size and shape and by having representative
patient positioning in actual XA equipment use. We
attempt to address this problem by using virtual
patients coupled with visualized representation of the
equipment. With DICOM Structured Report Dose
tag data and four added data entries, a reasonable
estimation of peak skin dose appears attainable.
Future effort is needed to fully document the
comparison of computed and measured values of
localized dose. Although we have performed several
blinded comparisons of measured and computed
dose values having reasonable agreement, we intend
to fully report on the accuracy of these computations
in a future publication.
An important component of this implementation
is the connection to a database that is robust with
XA equipment and DICOM tag knowledge and
having the ability to exchange information with the
XA tool. A central database is necessary for
properly identifying the patient, the equipment,
and the maintenance of longitudinal records of
skin dose. Repeated episodes of XA procedures
are common and result in a time sensitive dose
additive concern; a database enables peak skin
dose summations for patients having multiple
procedures.
Since each 0.01 × 0.01 m2 region may be
categorized within predetermined ALARA Action
level settings specified in the XA database, the
monitoring needs of XA include not only patient

specific episode dose summary reports but also the
ability to automatically notify physician users of
skin dose levels potentially needing medical
attention—an FDA recommendation.
Several limitations of this approach are recognized:
1. The calculation of peak skin dose is based
upon DICOM data that wrongly assumes
that all skin exposure takes place at the
event location reported by the XA unit—
namely the very last position during an Xray event. In truth, the C-arm gantry or table
can move while an X-ray event is happening. The magnitude of the error in peak skin
dose estimation due to this assumption is
difficult to quantitate. It appears reasonable
to use the last positioning information as a
conservative estimate at this time.
2. Errors in technologist positioning measurements of the origin can happen. We expect
this error to affect the location of the peak
skin dose more than its estimate. The use of
a default setting may obviate this positioning
error somewhat.
3. The shape of irradiated skin is taken to be either
square or circle. The rectangular shape of
collimator is estimated to a square which may
cause error in peak skin dose location. Further
research is required to determine peak skin dose
based on rectangle shape of collimator, using a
DICOM tag of shutter position. XR II may be
collimated fully open, giving rise to a circular
field while flat panel detectors are provided with
a circular beam shape. Thus, the shape of the
recorded peak skin dose may be in error.
4. Table attenuation values are angle dependent
and may be a cause of error. Applying a tube
angle-dependent correction factor would be
helpful. Not using table or pad attenuation in
the calcuations will result in conservative
values of peak skin dose. Attenuation values
of 25% have been observed.
5. C-arm operators at times need “heads up display” so that catheter visualization on the
monitor tracks with hand movements; a patient
that is actually prone may well be entered by the
technologist as supine (to keep image display
conventional). This would cause erroroneous
angulation data to be recorded. With position
information from the DSR and additonal techonologist input, a correction is possible.

AUTOMATIC MONITORING OF LOCALIZED SKIN DOSE

6. For type II equipment, the estimate of localized fluoroscopy dose is proportionally
assigned to the locations of acquisition dose.
7. Limitations exist in the variability of patient
geometry differing from these models. These
three-dimensional mathematical male and
female models are constructed from defined
elliptical cylinders and cones, permitting semicustomization of an actual patient (height and
weight). This is a planned future work.
8. Mass energy absorption coefficient ratios for
tissue to air (f factor) is ignored in this study
since it is 0.999 for skin17 and 1.06 for
muscle18 at typically used kVp values.
9. A default value for C-arm increments (Physicist Screen) is currently used. C-arm movements may occur with variable table side
positions of the operators. During physicist
validation, it is important to discuss the
procedures and their normal workflow. Alternatively, if a C-arm postional value is provided
per footswitch event, it can be included.
10. XA C-arms are provided with metal filters
(wedges), which are called upon to block or
attenuate a portion of the X-ray field by the
operator. Since wedges intercept the beam
before the ionization chamber, the recorded
KAP exposure value is correctly measured as
lower. Our computation of the point exposure
is obtained by dividing KAP by the field size,
which would give a lower than actual value.
The use of wedge filters and their effect on
accuracy are to be evaluated.

CONCLUSION

A Skin Dose Simulation Tool for XA interventional procedures has been created using software
to virtually describe a patient positioned and
located on a C-arm. Using DICOM Structured
report Dose tag information as well as conventional DICOM tag data, printable dose summaries,
and system parameters entered by the XA technologist and validated by the physicist, the
exposure values at the PERP are computed to the
surface of a patients skin. With backscatter and
other corrections, a localized skin dose estimate is
calculated and presented to a patient summary
dose screen. These data are archived in a database
which provides automatic alerts to physician users

637

or QA staff. An increase in the number of patients
having second, third, and more interventional
procedures is expected.19 Further, a skin dose
simulation tool would reasonably include an
accounting of skin dose received via other modalities. For example, computed tomography brain
perfusion commonly results in 1–2 Gy to a
localized portion of skin, which would be additive
to patients having brain XA procedures. Experimental validations for skin dose values and
regions is our future work.
ACKNOWLEDGEMENT
The authors of this paper appreciate the helpful and insightful
comments of the reviewers.

APPENDIX

A Point Calculation of Air Kerma to a 0.01 ×
0.01 m2 Radiated Region
The origin is the X-ray source, and the base of the solid
angle (cone for XRII pyramid for digital detectors) occurs
at the patient entrance reference point (PERP). For a squareshaped field, the irradiated area will (Figure 10) depict an
arbitrary surface area of the virtual patient surrounded by a
pyramid-shaped field of radiation. The base of the pyramid
is at the PERP distance (modified to be the projected s).
Step 1 determines the total surface area lying within the
projected field. The PERP plane (base of the pyramid)
passes through PERP. The goal is to locate the four
triangular faces to delineate an irradiated area on the patient.
First, we find two orthogonal vectors at PERP area as
follows,

v1 ¼ ðCosða2 Þ; Sinða1 Þ  Sinða2 Þ; Cosða1 Þ  Sinða2 ÞÞ
ð14Þ

v2 ¼ ð0; Cosða1 Þ; Sinða1 ÞÞ

ð15Þ

where v1 and v2 are parallel to the edges of the square
field.
We denote the length of the side of the radiated area
at PERP with l. The calculation for the coordinates of
the four corners of the area is:

l¼

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
KAPIRP =Kair

ð16Þ

638

KHODADADEGAN ET AL.

ðxc1 ; yc1 ; zc1 Þ ¼ ðxRF ; yRF ; zRF Þ  l=2  v1 þ l=2  v2
ð17Þ

ðxc2 ; yc2 ; zc2 Þ ¼ ðxRF ; yRF ; zRF Þ þ l=2  v1  l=2  v2

Assume that (x, y, z) represents the coordinate of a
point on the patient skin to be checked if it has been
exposed or not. Figure 12 depicts the vector v4 and
schematic of the feasibility method. The distance from
this point to the source is computed as follows,

ð18Þ



v4 ¼ x  xSpot ; y  ySpot ; z  zSpot

ð26Þ

d1 ¼k v4 k2

ð27Þ

ðxc3 ; yc3 ; zc3 Þ ¼ ðxRF ; yRF ; zRF Þ  l=2  v1  l=2  v2
ð19Þ

ðxc4 ; yc4 ; zc4 Þ ¼ ðxRF ; yRF ; zRF Þ þ l=2  v1 þ l=2  v2

d2 ¼ ðv3 =k v3 k2 Þ:v4

ð20Þ

With three points on each triangle face, we can
construct the four faces of the pyramid such as
Ax þ By þ Cz þ D ¼ 0, where A, B, C, D are coefficients. As an example, for the plane consisting of source,
ðxc1 ; yc1 ; zc1 Þ and ðxc4 ; yc4 ; zc4 Þ coefficients of plane are
as follows,



A ¼ ySpot  ðzc1  zc4 Þ þ yc1  zc4  zSpot
ð21Þ


þyc4  zSpot  zc1



B ¼ zSpot  ðxc1  xc4 Þ þ zc1  xc4  xSpot
ð22Þ


þzc4  xSpot  xc1
C ¼ xSpot  ðyc1  yc4 Þ þ xc1




 yc4  ySpot þ xc4  ySpot  yc1 ð23Þ

ð28Þ

Here,
k k2 meansﬃ the l2 norm, that is, k ða; b; c Þ
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

k2 ¼

a2 þ b2 þ c 2:

The radius of the area at Patient Entrance Reference
Point level;

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
KAPIRP =ðKair p Þ

ð29Þ

 ¼ ArgtanðR=ðk v3 k2 ÞÞ

ð30Þ

R¼

For the two vectors, we have v3  v4 ¼k v3 k2 
k v4 k2  Cos , where θ is the angle between vector

v3 and v4. Thus, any point on the patient skin, i.e., (x, y,
z) satisfying

d22  d12  Cos2 

ð31Þ

is inside the cone and would have been radiated.

D ¼ xSpot  ðyc1  zc4  yc4  zc1 Þ  xc1


 yc4  zSpot  ySpot  zc4  xc4


 ySpot  zc1  yc1  zSpot

REFERENCES

ð24Þ

Coefficients of other planes can be computed
accordingly.
A conically shaped field having a circular base (at
PERP) has a similar approach (Figure 11).
The vector v3 from X-ray tube to PERP as in
Figure 12 is



v3 ¼ xRF  XSpot ; yRF  ySpot ; zRF  zSpot
ð25Þ

1. Food and Drug Administration: Public Health Advisory:
Avoidance of Serious X-Ray-Induced Skin Injuries to Patients
during Fluoroscopically-Guided Procedures. September 30,
1994.
2. Wagner LK, McNeese MD, Marx MV, Siegel EL: Severe
skin reactions from interventional fluoroscopy: case report and
review of the literature. Radiology 213:773–776, 1999
3. Ukisu R, Kushihashi T, Soh I: Skin injuries caused by
fluoroscopically guided interventional procedures: case-based
review and self-assessment module. American Journal of
Radiology 193:59–69, 2009
4. Miller DL, Balter S, Noonan PT, Georgia JD: Minimizing
radiation-induced skin injury in interventional radiology procedures. Radiology 225(2):329–36, 2002

AUTOMATIC MONITORING OF LOCALIZED SKIN DOSE

5. Balter S, Miller D: The new Joint Commission sentinel
event pertaining to prolonged fluoroscopy. Journal of the
American College of Radiology 4(7):497–500, 2007
6. Joint Commission. Radiation Overdose as a Reviewable
Sentinel Event. March 7, 2006. http://www.jointcommission.
org/NR/rdonlyres/10A599B4-832D-40C1-8A5B-5929E9
E0B09D/0/Radiation_Overdose.pdf
7. International Electrotechnical Commission. Medical electrical equipment—Part 2–54: Particular requirements for the
basic safety and essential performance of X-ray equipment for
radiography and radioscopy Geneva: IEC 60601-2-54; 2009.
8. Morrell RE, Rogers AT: A mathematical model for
patient skin dose assessment in cardiac catheterization procedures. British Journal of Radiology 79:756–761, 2006
9. Miller DL, Balter S, Cole PE, Lu HT, Berenstein A, Albert
R, Schueler BA, Georgia JD, Noonan PT, Russell EJ, Malisch TW,
Vogelzand RL, Geisinger M, Cardella JF, George JS, Miller 3rd,
GL, Anderson J: Radiation doses in interventional radiology
procedures: the RAD-IR study: part II: skin dose. Journal of
Vascular and Interventional Radiology 14(8):977–990, 2003
10. Man-Systems Integration Standards. Revision B, NASASTD-3000. Vol 1, July 1995.
11. Kerr GD, Hwang JML, JONES RM: A mathematical
model of a phantom developed for use in calculations of radiation
dose to the body and major internal organs of a Japanese adult.
Journal of Radiation Research 17:211–229, 1976
12. Eckerman CM: Specific Absorbed Dose Fractions of
Energy at Various Ages from Internal Photon Sources.
Appendix A: Description of the Mathematical Phantoms.

639

ORNL/TM-8381/VI. Oak Ridge National Laboratory, Oak
Ridge, 1987
13. Digital Imaging and Communications in Medicine
(DICOM) Supplement 4 X-Ray Angiographic Image Objects
and Media Storage. Oct 9, 1996.
14. ICRU: International Commission on Radiation Units and
Measurements. Patient Dosimetry for X–rays used in Medical
Imaging. Journal of ICRU, 5, No 2, Report 74, Oxford
University Press, 2005.
15. Wang S, Pavlicek W, Langer SG, Wu T, Zhang M,
Morin RL, Hu M, Schueler BA. The ‘Dose Index Tracker’:
An Automated Database of Patient Radiation Dose Records
for Quality Monitoring. Journal of Digital Imaging, 2010 (in
press).
16. Balter S: Capturing patient doses from fluoroscopically
based diagnostic and interventional systems. Health Physics 95–
5:535–540, 2008
17. Ma C-M, Seuntjens JP: Mass-energy absorption coefficient and backscatter factor ratios for kilovoltage X-ray beams.
Physics Medical Biology 44:131–143, 1999
18. Mcparland BJ: Entrance skin dose estimates derived
from dose–area product measurements in interventional radiological procedures. The British journal of Radiology 71:1288–
1295, 1998
19. Paden RG, Pavlicek W, Peter MB: Review of Cumulative Skin Dose in Estimation for Multi-episode Interventional
Examinations. Proceedings of the 39th Midyear Topical Meeting, Medical and Laboratory Health Physics, Scottsdale,
Arizona, 2006.

NeuroImage 50 (2010) 935–949

Contents lists available at ScienceDirect

NeuroImage
j o u r n a l h o m e p a g e : w w w. e l s e v i e r. c o m / l o c a t e / y n i m g

Learning brain connectivity of Alzheimer's disease by sparse inverse
covariance estimation
Shuai Huang a, Jing Li a,⁎, Liang Sun b, Jieping Ye b, Adam Fleisher c, Teresa Wu a, Kewei Chen c, Eric Reiman b
and the Alzheimer's Disease NeuroImaging Initiative 1
a
b
c

Department of Industrial Engineering, Arizona State University, Tempe, AZ 85287-8809, USA
Department of Computer Science, Arizona State University, Tempe, AZ, USA
Banner Alzheimer's Institute, Phoenix, AZ, USA

a r t i c l e

i n f o

Article history:
Received 12 August 2009
Revised 29 December 2009
Accepted 30 December 2009
Available online 14 January 2010
Keywords:
Brain connectivity
Sparse inverse covariance
Alzheimer's
PET
Biomarker

a b s t r a c t
Rapid advances in neuroimaging techniques provide great potentials for study of Alzheimer's disease (AD).
Existing ﬁndings have shown that AD is closely related to alteration in the functional brain network, i.e., the
functional connectivity between different brain regions. In this paper, we propose a method based on sparse
inverse covariance estimation (SICE) to identify functional brain connectivity networks from PET data. Our
method is able to identify both the connectivity network structure and strength for a large number of brain
regions with small sample sizes. We apply the proposed method to the PET data of AD, mild cognitive
impairment (MCI), and normal control (NC) subjects. Compared with NC, AD shows decrease in the amount
of inter-region functional connectivity within the temporal lobe especially between the area around
hippocampus and other regions and increase in the amount of connectivity within the frontal lobe as well as
between the parietal and occipital lobes. Also, AD shows weaker between-lobe connectivity than within-lobe
connectivity and weaker between-hemisphere connectivity, compared with NC. In addition to being a
method for knowledge discovery about AD, the proposed SICE method can also be used for classifying new
subjects, which makes it a suitable approach for novel connectivity-based AD biomarker identiﬁcation. Our
experiments show that the best sensitivity and speciﬁcity our method can achieve in AD vs. NC classiﬁcation
are 88% and 88%, respectively.
© 2010 Elsevier Inc. All rights reserved.

Introduction
Alzheimer's disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions.
It has been speculated by a number studies and accepted more widely
recently that higher cognition results from different brain regions
interacting with each other, rather than individual regions working
independently (Horwitz, 2003; Delbeuck et al., 2003). This leads to the
belief that AD, with major symptoms being dramatic global cognitive
decline, may have abnormal functional brain connectivity patterns.
Functional connectivity refers to the coherence of the activities among
distinct brain regions (Horwitz, 2003). Some past research in AD has
shown that AD brains may have different connectivity patterns from
normal brains. For example, functional connectivity is reduced between
⁎ Corresponding author. Fax: +1 480 9652751.
E-mail address: Jinglz@asu.edu (J. Li).
1
Data used in the preparation of this article were obtained from the Alzheimer's
Disease Neuroimaging Initiative (ADNI) database (www.loni.ucla.edu/ADNI). As such,
the investigators within the ADNI contributed to the design and implementation of
ADNI and/or provided data but did not participate in the analyses or writing of this
report. ADNI investigators include (complete listing available at www.loni.ucla.edu\
ADNI\Collaboration\ADNI_Authorship_list.pdf).
1053-8119/$ – see front matter © 2010 Elsevier Inc. All rights reserved.
doi:10.1016/j.neuroimage.2009.12.120

the hippocampus and other regions of AD brains (Supekar et al., 2008;
Wang et al., 2007; Azari et al., 1992; Horwitz et al., 1987; Grady et al.,
2001). A key pathological correlate, which may be implicated in
hippocampal network disfunction relates to neuroﬁbrillary tangles
(NFTs), a hallmark of AD, which causes hippocampal neurodegeneration
early in the course of AD pathology. This results from selective affects on
speciﬁc cortical layers within the hippocampal formation (Hirano and
Zimmerman, 1962), which raises the possibility that the functional
interaction between the hippocampus and other related brain regions
may be disrupted. In addition to changes in hippocampal networks, some
studies of early AD and mild cognitive impairment (MCI) have found
increased connectivity between the frontal lobe and other brain regions
(Gould et al., 2006; Stern, 2006; Becker et al., 1996; Woodard et al.,
1998; Saykin et al., 2004; Grady et al., 2003). This has been interpreted
by some investigators as a compensatory reallocation or recruitment of
cognitive resources (Gould et al., 2006). Since regions in the frontal lobe
are typically affected later in the course of the disease, it is argued that an
increase in frontal connectivity could help preserve some memory and
attention ability in early AD patients (Stern, 2006).
Recent years have witnessed the rapid advancement of neuroimaging technologies, which provides an unprecedented opportunity for
brain connectivity research. Based on the brain data acquired by

936

S. Huang et al. / NeuroImage 50 (2010) 935–949

functional neuroimaging techniques such as positron emission
tomography (PET) and functional magnetic resonance imaging
(fMRI), quite a number of analytic methodologies have been proposed
to investigate functional brain connectivity.
Multivariate statistical methods have been used, such as principle
component analysis (PCA) (Friston, 1994), PCA-based scaled subproﬁle model (Alexander and Moeller, 1994), independent component analysis (Calhoun et al., 2001; Calhoun et al., 2003), and partial
least squares (McIntosh et al., 1996; Worsley et al., 1997). These
methods tend to group brain regions into a few latent components.
The brain regions within each component are believed to have strong
connectivity, while the connectivity between components is weak.
One limitation of these methods is that the latent components are
mostly obtained based on statistical modeling consideration, so they
may not necessarily correspond to biological entities, causing
difﬁculty in interpretation.
A large body of functional connectivity modeling has been based
on correlation analysis (Azari et al., 1992; Horwitz et al., 1987;
Supekar et al., 2008; Stam et al., 2007). Correlation analysis captures
pairwise information, which may not be able to effectively
characterize the interactions of many brain regions working
together. To overcome this limitation, partial correlation analysis
has been adopted (Salvador et al., 2005a,b; Marrelec et al., 2006,
2007; Hampson et al., 2002). A partial correlation measures the
association between two brain regions after factoring out the
contribution to the pairwise correlation that might be due to global
or third-party effects. Because partial correlations correspond to the
off-diagonal entries of the inverse covariance (IC) matrix of the
data, estimation of partial correlations is usually achieved by
maximum likelihood estimation (MLE) of the IC matrix. A limitation
of MLE is that reliable estimation requires the sample size of the
data to be substantially larger than the number of brain regions
modeled. This condition may be satisﬁed in studies based on fMRI
data, in which the sample size corresponds to the length of the fMRI
time series. However, in PET studies, because the samples size is the
number of subjects which is usually very limited due to cost or
availability constraints, existing partial correlation or IC based
research has only been able to focus on a few (around ten) preselected brain regions.
We propose a new method for functional connectivity modeling,
called sparse inverse covariance estimation (SICE), also known as
Gaussian graphical models or graphical Lasso. This method imposes a
“sparsity” constraint on the MLE of an IC matrix, which leads to
reliable estimation of the IC with small sample sizes. Here, “small”
means that the sample size can be close to or even less than the
number of brain regions modeled. Using SICE to model brain
connectivity is appropriate because many past studies based on
anatomical brain databases have shown that the true brain network is
sparse (Hilgetag et al., 2002; Kotter and Stephan, 2003; Sporns et al.,
2004). Using sparse models of other kinds for brain connectivity
modeling, such as multivariate or vector autoregressions, have been
explored in the past, with a focus on time series data such as fMRI and
EEG (Chiang et al., 2009; Thompson et al., 2009; Valdes-Sosa et al.,
2005). In contrast, the proposal SICE can be used to model crosssectional data such as PET.
Speciﬁcally in this paper, we apply one SICE method, developed by
us in a previous paper (Huang et al. 2009), to identify brain
connectivity models for AD, MCI, and normal control (NC) subjects
based on FDG-PET data. SICE has been recognized as an effective tool
for identifying the structure of an IC matrix, i.e., the zero and non-zero
entries, but is not recommended to be used for estimating the
magnitude of the non-zero entries. Therefore, we use SICE to identify
brain connectivity model structures, i.e., existence and non-existence
of functional connections between brain regions. Furthermore, we
prove a monotone property of SICE, which enables us to develop a
quasi-measure for the strength of functional connections. To our best

knowledge, our work is among the ﬁrst ones that utilize SICE and its
associated property for functional brain connectivity structure and
strength identiﬁcation in AD studies. In addition, we show how to use
the results of SICE to classify new subjects, which makes SICE a
potential method for identifying connectivity-based biomarkers for
AD. Another unique perspective of our work is that it utilizes PET data.
While a majority of existing AD brain connectivity research has been
based on fMRI data (Rajapakse and Zhou, 2007; Li et al., 2008; Zheng
and Rajapkse, 2004; Zhuang et al., 2005; Chen and Herskovits, 2007),
research based on PET data is still limited. Our work intends to bridge
this gap.
Method
SICE for brain connectivity model structure identiﬁcation
Suppose that there are p brain regions to be modeled, i.e., {X1, … ,Xp}.
The measurement data for each brain region is the regional cerebral
metabolic rate for glucose by FDG-PET. The data can be reasonably
assumed to follow a multivariate normality distribution, as this assumption has been adopted in a number of past publications. Statistical
normality checks based on the data in our experiments also support this
assumption (see Supplementary material). Given the measurement
data of the brain regions from subjects, i.e., AD patients, SICE ﬁnds an
estimate for the inverse covariance of the brain regions by solving the
following optimization:
Θ̂ = argmax

Θ>0 logðdetðΘÞÞ −

trðSΘÞ − λ‖Θ‖1 ;

ð1Þ

where Θ and Θ̂ denote the IC and its estimate, S is the sample
covariance matrix, det (·) and tr (·) denote the determinant and trace
of a matrix, ||·||1 denotes the sum of absolute values of all the entries
in a matrix, and λ is a pre-selected so-called regularization parameter,
λ N 0. To help understand (1), (1) can be equivalently written as
Θ̂ = argmax

Θ>0 logðdetðΘÞÞ −

ðSΘÞ; subject to ‖Θ‖1 ≤∈;

ð2Þ

where ∈ is reversely related to λ, ∈ N 0. It is easy to see from (2) that
SICE aims to ﬁnd a Θ̂ that maximizes the likelihood function, under a
constraint that the sum of the magnitudes of all entries in Θ̂ is
bounded by ∈ (or λ, equivalently). Furthermore, it can be seen from
(2) that when ∈ is large enough (i.e., small λ), the constraint has
little effect and SICE is just the usual MLE. However, when ∈ is small
(i.e., big λ), SICE is able to produce an estimate for Θ that is a
shrunken version of the estimate by MLE. This is a advantage,
because it has been found that the estimate for Θ by MLE is likely to
contain very few zero entries even when the Θ is actually sparse,
while the shrunken estimate for Θ provided by SICE is able to recover
those zero entries in Θ. This advantage of SICE is especially signiﬁcant
under small sample sizes, which has been demonstrated by many
papers (Yuan and Lin, 2007; Friedman et al., 2007; Schafer and
Strimmer, 2005; Li and Gui, 2006).
Various methods have been developed to solve for the optimization in Eq. (1) and achieve the SICE in recent years (Yuan and Lin,
2007; Friedman et al., 2007; Levina et al., 2008; Li and Gui, 2006),
including a method by us (Sun et al., 2009). A common characteristic
of the SICE methods is that while they are good at discovering which
entries in the IC matrix are zero and which are non-zero, they may
not be good at estimating the magnitude of the non-zero entries due
to the “shrinking” effect. Therefore, these methods may be more
appropriate to be used for identifying the IC matrix structure, but not
the parameters. As a result, once an estimate, Θ̂, is obtained from
SICE, we should use only the structural information (i.e., zero and
non-zero entries) in to Θ̂ build a brain connectivity model.
Speciﬁcally, if we use a graph with nodes and undirected arcs to

S. Huang et al. / NeuroImage 50 (2010) 935–949

Fig. 1. (a) Estimated IC matrix by SICE θ iĵ ≠ 0. (b) Brain connectivity model built from
the Θ̂ in (a).

represent the brain connectivity model, we put an arc between
nodes (i.e., brain regions) Xi and Xj if and only if θ̂ij ≠ 0, where θ̂ij is
the entry at the ith row jth column of Θ̂. An example of this is shown
in Fig. 1.
A monotone property of SICE for brain connectivity
strength identiﬁcation
The brain connectivity model obtained by SICE, such as the one in
Fig. 1(b), may be interpreted in the following way. An arc between
regions Xi and Xj may indicate that these two regions are directly
connected in some functional process. This is because an arc
represents a non-zero partial correlation which reﬂects the remaining
association between two brain regions after the effect of other regions
on their overall association has been factored out and the remaining
association is likely to reﬂect the direct functional connection
between the two regions. Furthermore, if two brain regions are not
connected by an arc, but by a path consisting of more than one arc,
they are not directed connected but may be reasonably considered as
connected indirectly. For example, X1 and X3 in Fig. 1(b) may be
considered as directly connected, while X1 and X5 may be considered
as indirectly connected because there is not a single arc between them
but two paths, X1–X3–X5 and X1–X4–X5.
The above discussion leads to the following deﬁnitions:
Deﬁnitions: Two brain regions are directly connected if there is an
arc between them in the brain connectivity model. They are indirectly
connected if there exists a path consisting of more than one arc
between them. They are connected if they are either directly or
indirectly connected.
The knowledge that two brain regions are connected is important
for understanding the brain's functional process. It is also important to
ﬁnd out the strength of the connection. However, a direct measure on
the strength of connection is not possible, because the brain
connectivity model obtained by SICE contains only structural
information. A quasi-measure, on the other hand, may be possible
due to the following considerations: The λ in the SICE formulation in
Eq. (1) (or equivalently, the ∈ in Eq. (2)) has a similar effect to
controlling the number of connections in the connectivity model
estimated. A larger λ (smaller ∈) allows for a smaller number of
connections. Therefore, as λ goes from λ1 to λ2, λ2 N λ1, some
connections existing in the model corresponding to λ1 must drop.
The connections that drop should be those such that the remaining

937

connections maximize the likelihood (i.e., the objective functions in
Eqs. (1) and (2)). To achieve this, obviously, the weakest connections
should drop. In other words, as λ goes from λ1 to λ2, the connections
that drop should be weaker than those that stay in. As a result, we can
use λ2 as a quasi-measure for the strength of the connections that
drop; in particular, the connections that drop at a bigger λ are
stronger than those that drop at a smaller λ. Formally, the quasimeasure is deﬁned as follows:
A quasi-measure for the strength of connection: A quasimeasure for the strength of connection between Xi and Xj is the
critical λ value at which Xi and Xj change from being connected to
being not connected.
For example, Fig. 2 shows the brain connectivity models estimated
by SICE at four λ's (λ1 b λ2 b λ3 b λ4). According to the above deﬁnition,
the quasi-measure for the strength of connection between X6 and any
of the other regions is λ2; that between X5 and X4 is λ3; that between
any pair of regions in the cluster of X3, X2, and X1 is λ4.
Note that in order for λ to be an appropriate quasi-measure for the
strength of connections, we must be able to prove that if the
connection between two brain regions drops at a certain λ, these two
regions will never be connected again at larger λ's. Otherwise, the
strength of their connection cannot be uniquely determined. This is
the so-called monotone property we discover, which is stated as
follows (please see proof in the Appendix A):
Monotone property of SICE: If two brain regions are not
connected in the connectivity model at a certain λ, they will never
become connected as λ goes larger.
Discussion: Lastly in this section, we would like to provide some
discussions on the use and potential beneﬁts of the proposed quasimeasure:
• With the aid of the quasi-measure, we can order the inter-region
connections in terms of connection strength. For example, in Fig. 2,
the connection between X6 and any of the other regions should be
the weakest, that between X5 and X4 is the second weakest, and
that between any pair of regions in the cluster of X3, X2, and X1 is
the strongest.
• Because λ is a quasi-measure, but not a direct one, for the strength
of connections, there must be discrepancy between the measured
strength by λ and the true strength even without sampling errors.
Therefore, we would recommend using λ to order the connections,
but not treating its value as a close estimate for the strength of a
connection. Here is a simple example to illustrate this point: Even
when two regions are not connected in the true brain network, the
quasi-measure for their connection, i.e., the λ at which they
change from being connected to being not connected in the
estimated connectivity model by SICE, is still non-zero, because the
λ in SICE is always positive.
• λ is a quasi-measure for the strength of connections, but not for
strength of the arcs in the brain connectivity model. Recall that
arcs correspond to direct connections or partial correlations. In
fact, we have found that the monotone property, although holds
for connections, does not hold for direct connections, because an
arc that drops at a certain λ may come back again as λ goes larger.

Fig. 2. Brain connectivity models at four λ's (λ1 b λ2 b λ3 b λ4) showing the monotone property.

938

S. Huang et al. / NeuroImage 50 (2010) 935–949

Therefore, λ should not be used as a measure for the strength of
the arcs. An example of this is shown in Fig. 2, in which the arc
between X1 and X3 drops at λ2 but it comes back at λ3.
• The potential beneﬁts of the proposed quasi-measure are two-fold:
First, it enables us to compare AD, MCI, and NC in terms of the order of
inter-regions connections (results are shown in the next section). To
our best knowledge, such a comparison has not been explored before,
which provides new knowledge to AD studies. Second, in the literature
of SICE, there has been a lack of a clear interpretation on λ, which is
now being provided by our research. So, this research contributes to
both the domain (AD) and the methodology (SICE).
Use of SICE for classifying new subjects
In this section, we will show how to use SICE to classify new
subjects, which makes it possible to deﬁne connectivity-based AD
biomarkers based on the connectivity models built from PET data. The
basic idea is to apply SICE to the training data of AD and NC and
estimate an IC matrix for each group. The estimated IC matrices will
then be used to predict a new subject's likelihood of being AD and NC,
based on this subject's PET measurement. The detailed procedure is
illustrated as follows. Note that although the procedure is illustrated
for AD vs. NC, it can be easily adapted for MCI vs. NC, or three-group
(AD, MCI, and NC) classiﬁcation.
Assume that there is a new subject with PET measurement x = [x1,
…, xp], where xi is the PET measurement on brain region i, i = 1, …, p,
and p is the total number of brain regions. The objective is to
determine whether to classify this subject as AD or NC. This may be
achieved by comparing the likelihoods of x given that the subject is AD
and NC, respectively. Because x follows a multivariate Gaussian
distribution, its likelihood functions with respect to AD and NC can be
written as,


jΘAD j 1 = 2
1 T
f ðx jΘAD Þ =
x
exp
−
Θ
x
;
AD
2
ð2π Þp = 2


jΘNC j 1 = 2
1 T
exp − x ΘNC x ;
f ðx jΘNC Þ =
p=2
2
ð2πÞ
respectively, where ΘAD and ΘNC are the inverse covariance matrices
of AD and NC, respectively; the mean vector is considered to be zero
without loss of generality. The estimates for ΘAD and ΘNC, Θ̂AD and Θ̂NC,
can be obtained by SCIE based on training data. So, the classiﬁcation
rule can be: classify the new subject as AD if f(x|Θ̂AD) N f(x|Θ̂NC) and NC
otherwise. Note that we have mentioned previously that SCIE is good
at identifying the IC matrix structure but not parameters. Therefore, to
get better estimates for Θ̂AD and Θ̂NC, we can use the zero entries in the
estimated IC matrices by SCIE as constraints and estimate the
magnitudes of the non-zero entries by MLE, i.e., to solve a constrained
optimization problem (Dempster, 1972).
Experiments and results
This section summarizes our experiments and ﬁndings for SICEbased brain connectivity modeling of AD, MCI, and NC using FDG-PET
data. Purposes of our study focus on how aspects of the connectivity
patterns exhibited in the models relate to existing ﬁndings in the
literature, and on how other aspects may suggest further investigations in brain connectivity research.
Data acquisition and preprocessing
The data used in our study include FDG-PET images from 49 AD,
348 116 MCI, and 67 NC subjects downloaded from the Alzheimer's
349 disease neuroimaging initiative (ADNI) database. Demographic
information and MMSE 352 scores of the subjects are summarized in
Table 1. The ADNI was launched in 2003 by the National Institute on

Table 1
Demographic information and MMSE scores.

Age (mean ± SD)
Gender (male/female)
Years of education
(mean ± SD)
Baseline MMSE

NC

MCI

AD

P-value

76.0 ± 4.69
43/24
15.9 ± 3.24

74.9 ± 7.36
76/40
16.0 ± 2.86

75.3 ± 6.85
27/22
14.7 ± 3.02

0.53
0.77
0.01

29.0 ± 1.18

27.2 ± 1.67

23.6 ± 1.93

b 0.001

Aging (NIA), the National Institute of Biomedical Imaging and
Bioengineering (NIBIB), the Food and Drug Administration (FDA),
private pharmaceutical companies and non-proﬁt organizations, as a
$60 million, 5-year public-private partnership. The primary goal of
ADNI has been to test whether serial MRI, PET, other biological
markers, and clinical and neuropsychological assessment can be
combined to measure the progression of MCI and early AD. The initial
goal of ADNI was to recruit 800 adults, ages 55 to 90, to participate in
the research – approximately 200 cognitively normal older individuals to be followed for 3 years, 400 people with MCI to be followed
for 3 years, and 200 people with early AD to be followed for 2 years.
Preprocessing the images involves the following steps. Each
subject's FDG-PET image is spatially normalized to the MNI PET
template, using the afﬁne transformation and subsequent non-linear
warping algorithm (Friston et al., 1995) implemented in SPM. The
afﬁne deformation adjusts each whole brain image for its position,
orientation, size, and global shape based upon minimizing the mean
residual variance (Zhilkin and Alexander, 2004). The non-linear warp,
a linear combination of low spatial frequency basis discrete cosine
functions, determines the optimal coefﬁcients for each of the basis
functions by minimizing the sum of squared differences between the
raw MRI brain and the template image (Ashburner and Friston, 1999).
Simultaneously, the smoothness of the transformation is maximized
using maximum a posterior (MAP). Once in the MNI template,
Automated Anatomical Labeling (AAL) (Tzourio-Mazoyer et al., 2002)
is applied to extract data from each of the 116 anatomical volumes of
interest (AVOI), and derived average of each AVOI for every subject
based on the PET images.
Brain connectivity modeling by SICE and visualization techniques
42 AVOI are empirically chosen, which are brain regions known to
be most affected by AD (Azari et al., 1992; Horwitz et al., 1987). These
regions distribute in the frontal, parietal, occipital, and temporal lobes.
Pease see Table 2 for names of the AVOI and the lobe each of them
belong to.
To build a brain connectivity model for AD, we ﬁrst compute a
sample covariance matrix, S, of the 42 AVOI, based on the
measurement data of the 42 AVOI from 49 AD patients. Then, we
apply SICE to solve the optimization problem in Eq. (1) based on the S
and a pre-selected λ. The solution Θ̂ is further converted to a graph
consisting of nodes (AVOI) and arcs (non-zero entries in Θ̂).
Furthermore, considering that a graph of this kind may be too
space-consuming for the paper, we adopt a matrix representation for
the graph. Please see the ﬁrst matrix in Fig. 3(a), for an example,
which represents the brain connectivity model structure estimated by
SICE at a certain λ. In the matrix, each row (column) corresponds to
one of the 42 AVOI. A black cell corresponds to an arc. Because the
matrix is symmetric, the total number of black cells is equal to twice
the total number of arcs in the corresponding connectivity graph.
Moreover, on each matrix, four red cubes are used to highlight the
brain regions in each of the four lobes; that is, from top-left to bottomright, the red cubes highlight the frontal, parietal, occipital, and
temporal lobes, respectively.
Furthermore, to facilitate the comparison between AD, MCI, and
NC, connectivity models should also be developed for MCI and NC,

S. Huang et al. / NeuroImage 50 (2010) 935–949

939

Table 2
Names of the AVOI for connectivity modeling (L = Left hemisphere, R = Right hemisphere).
Frontal lobe
1
2
3
4
5
6
7
8
9
10
11
12

Parietal lobe
Frontal_Sup_L
Frontal_Sup_R
Frontal_Mid_L
Frontal_Mid_R
Frontal_Sup_Medial_L
Frontal_Sup_Medial_R
Frontal_Mid_Orb_L
Frontal_Mid_Orb_R
Rectus_L
Rectus_R
Cingulum_Ant_L
Cingulum_Ant_R

13
14
15
16
17
18
19
20

Occipital lobe
Parietal_Sup_L
Parietal_Sup_R
Parietal_Inf_L
Parietal_Inf_R
Precuneus_L
Precuneus_R
Cingulum_Post_L
Cingulum_Post_R

respectively. The problem is how to select the λ value for each of three
groups, so that the comparison between them will make sense. In this
paper, we focus on comparing AD, MCI, and NC in terms of the
distribution/organization of the connectivity, which has been less
studied in the literature, but not in terms of the global scale of the
connectivity, which has been studied substantially. To achieve this, we
must factor out the connectivity difference between the three groups
that is due to their difference at the global scale, so that the remaining
difference will reﬂect their difference in the connectivity distribution/
organization. A common strategy is to control the total number of arcs
for each group to be the same, which has been adopted by a number of
other studies (Supekar et al., 2008; Stam et al., 2007). We also adopt
this strategy; specially, we adjust the λ in the estimation of the
connectivity model of each group, such that the three models,
corresponding to AD, MCI, and NC, respectively, will have the same
total number of arcs. Also, by selecting different values for the total
number of arcs, we can obtain models representing the brain
connectivity at different strength levels. Speciﬁcally, given a small
value for the total number of arcs, only strong arcs will show up in the
resulting connectivity model, so the model is a model of strong brain
connectivity; when increasing the total number of arcs, mild (or even
weak) arcs will also show up in the resulting connectivity model, so
the model is a model of mild-to-strong (or even weak-to-strong)
brain connectivity. For example, Fig. 3 shows the connectivity models
for AD, MCI, and NC with the total number of arcs equal to 60, 120,
and 180.
Finally, we introduce some other ways to visualize the connectivity models to facilitate the comparison between AD, MCI, and NC,
such as graphs of nodes and arcs (e.g., Fig. 4(i)) and brain images (e.g.,
Fig. 4(ii)):
Fig. 4(i) displays a portion of the connectivity model for AD. Each
node is an AVOI in Table 2 in the temporal lobe. This graph focuses on
the connectivity regarding the sub-network consisting of Hippocampus_L & R ((X39 & X40)) and ParaHippocampal_L & R ( X41 & X42), so it
only displays the arcs between each region in the sub-network and
other regions in the temporal lobe, as well as the arcs between the
regions in the sub-network. Other arcs are omitted. Furthermore,
green arcs are arcs (black cells) appearing in the matrix plot of AD in
Fig. 3(a), so they represent strong connectivity. Blue arcs are arcs not
appearing in the matrix plot of AD in Fig. 3(a), but appearing in that in
Fig. 3(b), so they represent less strong connectivity. Red arcs are arcs
not appearing in the matrix plots of AD in Fig. 3(a) or (b), but
appearing in that in Fig. 3(c), so they represent even less strong
connectivity.
Fig. 4 (ii) shows four axial slices of an AD brain. This graph focuses
on displaying the connectivity between region X41, ParaHippocampal_L, and other regions in the temporal lobe. ParaHippocampal_L is

21
22
23
24
25
26

Temporal lobe
Occipital_Sup_L
Occipital_Sup_R
Occipital_Mid_L
Occipital_Mid_R
Occipital_Inf_L
Occipital_Inf_R

27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

Temporal_Sup_L
Temporal_Sup_R
Temporal_Pole_Sup_L
Temporal_Pole_Sup_R
Temporal_Mid_L
Temporal_Mid_R
Temporal_Pole_Mid_L
Temporal_Pole_Mid_R
Temporal_Inf_L
Temporal_Inf_R
Fusiform_L
Fusiform_R
Hippocampus_L
Hippocampus_R
ParaHippocampal_L
ParaHippocampal_R

highlighted in yellow. Regions highlighted in green, blue, and red are
those in the temporal lobe that have strong, less strong, even less
strong connectivity with ParaHippocampal_L, respectively. In a
similar way, Fig. 5(i) and (ii) are developed for NC. Note that similar
graphs to Figs. 4 and 5 can be developed for other portions of the brain
connectivity model, or even the whole brain, which are not shown
here due to space limits.
Comparison between AD, MCI, and NC in connectivity
organization/distribution
The connectivity models estimated by SICE and various types of
visualization techniques (matrix, graph, and brain slice) enable us
to see the difference between AD, MCI, and NC in terms of
connectivity organization/distribution. For example, Fig. 3(a) shows
fewer black cells in the temporal lobe of AD than NC, but more
black cells in the frontal lobe of AD than NC, where black cells
correspond to direct connections between brain regions. While
visual comparison is an important initial step to pick out the
differences, it should be followed by rigorous statistical hypothesis
testing to check if the observed differences are statistically
signiﬁcant. Therefore, we perform hypothesis testing to check if
the number of black cells within each lobe, used to represent the
amount of direct connections within that lobe, is signiﬁcantly
different between each pair of the study groups (i.e., AD, MCI, and
NC). The hypothesis testing is also performed for the number of
black cells between lobes. Here, we show the steps for testing if the
number of black cells within the temporal lobe of AD, nAD_T, is
signiﬁcantly different from that of NC, nNC_T.
(i) Draw samples of AD patients and samples of NC subjects, with
replacement, from the original AD and NC datasets, respectively.
(ii) Apply the SICE method to learn one connectivity model for AD
and one for NC, based on the samples drawn. During the learning
of each connectivity model, adjust the λ such that the two models
have the same total number of arcs.
(iii) Count the number of arcs (or equivalently, the number of black
cells in the matrix representation) within the temporal lobe of the
AD connectivity model. This number is a bootstrap sample for nAD_T;
in a similar way, a bootstrap sample for nNC_T can be obtained.
(iv) Repeat (i)–(iii) N times and obtain N bootstrap samples for
nAD_T and nNC_T, respectively.
(v) Test the hypothesis that nAD_T and nNC_T are equal based on
their respective bootstrap samples, and compute the P-value of the
hypothesis test. Interpretation of the P-value is following: P-value
b0.05 means strong evidence for nAD_T ≠ nAD_T; 0.05 b P b0.1
means some evidence (not strong though) for nAD_T ≠ nAD_T; P

940

S. Huang et al. / NeuroImage 50 (2010) 935–949

Fig. 3. (a) Brain connectivity models with total number of arcs equal to 60. (b) Brain connectivity models with total number of arcs equal to 120. (c) Brain connectivity models with
total number of arcs equal to 180.

N0.1 means little evidence for nAD_T ≠ nAD_T, i.e., there is no
signiﬁcant difference between nAD_T and nNC_T.
Following similar steps to the above, we can also compare the
amount of direct connections within other lobes as well as between
lobes, for each pair of the study groups. The results are summarized in

Table 3, which gives the P-value of the hypothesis testing. Speciﬁcally,
a P-value is shown if it is smaller than 0.1 and replaced by a “–”
otherwise. A P-value is highlighted if it is smaller than 0.05. The Pvalues presented here are those after correcting the effect of multiple
testing using the standard False Discovery Rate (FDR) approach by
Benjamini and Hochberg (1995). Note that because our multiple tests

S. Huang et al. / NeuroImage 50 (2010) 935–949

941

Fig. 4. (i) Hippocampus and parahippcampus sub-network connectivity for AD, i.e., connectivity within the network and connectivity between the network and other regions in
temporal lobe; green, blue, red arcs represent connectivity from strong to weak. (ii) Four axial slices of AD brain, showing connectivity between ParaHippocampal_L (yellow) and
other regions in the temporal lobe; green, blue, and red highlight regions connected with ParaHippocampal_L from strong to weak.

are not independent (the same datasets are used multiple times), the
standard FDR approach might generate conservative results. This is a
limitation of our current work and we will investigate how to
overcome this limitation in future research. In addition to using Pvalue for comparison, we also develop box plots, as shown in Fig. 6
(only the box plots for AD vs. NC are shown due to the page limit).
Inspection of the results from visualizations, hypothesis testing,
and box plots reveals the following interesting observations:
Within-lobe connectivity
The temporal lobe of AD has a signiﬁcantly lesser amount of direct
connections than NC. This is true across the connectivity models at
different strength levels (i.e., total arc number equal to 60, 120, and
180). In other words, even the direct connections between some
strongly-connected brain regions in the temporal lobe may be
disrupted by AD. In particular, it is clearly from Fig. 3(b) that the
regions “Hippocampus” and “ParaHippocampal” (numbered by 39–
42, located at the right-bottom corner of Fig. 3(b)) are much more
separated from other regions in AD than in NC. The decrease in the
amount of connections in the temporal lobe of AD, especially between
the Hippocampus and other regions, has been extensively reported in
the literature (Supekar et al., 2008; Wang et al., 2007; Azari et al.,
1992; Horwitz et al., 1987; Grady et al., 2001). However, the temporal
lobe of MCI does not show a signiﬁcant decrease in the amount of
direct connections, compared with NC. This may be because MCI does
not disrupt the temporal lobe as severely as AD.
The frontal lobe of AD has a signiﬁcantly more amount of direct
connections than NC, which is true across the connectivity models at

different strength levels. This is consistent with previous literature
and has been interpreted as compensatory reallocation or recruitment
of cognitive resources (Gould et al., 2006; Stern, 2006; Becker et al.,
1996; Woodard et al., 1998; Saykin et al., 2004; Grady et al., 2003).
Because the regions in the frontal lobe are typically affected later in
the course of AD (our data are early AD), increase in the amount of
connections in the frontal lobe may help preserve some cognitive
functions in AD patients. Furthermore, the frontal lobe of MCI does not
show a signiﬁcant increase in the amount of direct connections,
compared with NC. This indicates that the compensatory effect in MCI
brain may not be as strong as that in AD brains.
There is no signiﬁcant difference between AD, MCI, and NC in
terms of the amount of direct connections within the parietal lobe and
within the occipital lobe.
Between-lobe connectivity
In general, human brains tend to have a less amount of betweenlobe connections than within-lobe connections. A majority of the
strong connections occurs within lobes, but rarely between lobes.
These can be clearly seen from Fig. 3 (especially Fig. 3(a)) in which
there are a lot more black cells inside the red cubes than outside the
red cubes, regardless of AD, MCI, and NC. Recall that the red cubes are
used to highlight the four lobes.
AD has a signiﬁcantly more amount of parietal-occipital direct
connections than NC, which is true across the connectivity models at
different strength levels. Increase in the amount of connections
between the parietal and occipital lobes of AD has been previously
reported in (Supekar, 2008). It may also be interpreted as a

Fig. 5. (i) Hippocampus and parahippcampus sub-network connectivity for NC, i.e., connectivity within the network and connectivity between the network and other regions in
temporal lobe; green, blue, red arcs represent connectivity from strong to weak. (ii) Four axial slices of NC brain, showing connectivity between ParaHippocampal_L (yellow) and
other regions in the temporal lobe; green, blue, and red highlight regions connected with ParaHippocampal_L from strong to weak.

942

S. Huang et al. / NeuroImage 50 (2010) 935–949

Table 3
P-value from the hypothesis test of connectivity difference between AD, MCI, and NC.
(a) Total number of arcs = 60

(b) Total number of arcs = 120

(c) Total number of arcs = 180

AD vs. NC

Frontal

Parietal

Occipital

Temporal

AD vs. NC

Frontal

Parietal

Occipital

Temporal

AD vs. NC

Frontal

Parietal

Occipital

Temporal

Frontal
Parietal
Occipital
Temporal
AD vs. MCI
Frontal
Parietal
Occipital
Temporal
MCI vs. NC
Frontal
Parietal
Occipital
Temporal

0.013

–
–

–
0.018
–

–
–

–
0.007
–

–
0.022
–

Frontal
–

Parietal
–
–

Occipital
–
–
–

Frontal
–

Parietal
–
–

Occipital
0.036
–
–

Frontal
–

Parietal
–
–

Occipital
0.020
–
–

Frontal
–

Parietal
–
–

Occipital
0.002
0.078
–

Frontal
Parietal
Occipital
Temporal
AD vs. MCI
Frontal
Parietal
Occipital
Temporal
MCI vs. NC
Frontal
Parietal
Occipital
Temporal

–
–

Occipital
–
0.013
–

–
–
0.040
0.001
Temporal
–
–
0.046
–
Temporal
–
–
–
–

0.091

Parietal
–
–

Frontal
Parietal
Occipital
Temporal
AD vs. MCI
Frontal
Parietal
Occipital
Temporal
MCI vs. NC
Frontal
Parietal
Occipital
Temporal

0.060

Frontal
–

–
–
–
0.063
Temporal
–
–
–
0.035
Temporal
–
–
–
–

Frontal
–

Parietal
–
–

Occipital
0.007
–
–

–
0.027
–
0.041
Temporal
–
–
–
–
Temporal
–
0.033
–
–

compensatory effect. Furthermore, MCI also shows increase in the
amount of direct connections between the parietal and occipital
lobes, compared with NC, but the increase is not as signiﬁcant as
AD.
While the amount of direct connections between the frontal and
occipital lobes shows little difference between AD and NC, this
amount for MCI shows a signiﬁcant decrease. Also, AD has a less
amount of temporal-occipital connections, a less amount of frontalparietal connections, but a more amount of parietal-temporal
connections than NC.
Between-hemisphere connectivity
We are also interested in knowing if there is a difference between
AD, MCI, and NC, in terms of the amount of direct connections
between hemispheres. To achieve this, we can count how many left–
right pairs of the same regions have an arc (or black cell) between
them in the connectivity models of AD, MCI, and NC, respectively. In
addition to directly comparing the counts, we can also perform
hypothesis testing (similar to the ones used for within-lobe and
between-lobe comparisons). Results show that when the total
number of arcs in the connectivity models is equal to 180 or 120,
none of the tests is signiﬁcant. However, when the total number of
arcs is equal to 60, the P-value of the tests for “AD vs. NC”, “AD vs.
MCI”, and “MCI vs. NC” are 0.038, 0.061, and 0.376, respectively. We
further perform tests for the total number of arcs equal to 50 and ﬁnd
the P-value to be 0.026, 0.079, and 0.198 respectively. These results
indicate that AD disrupts the strong connection between the same
regions in the left and right hemispheres, whereas this disruption is
not signiﬁcant in MCI.
Comparison between AD, MCI, and NC in connection strength
We can use the quasi-measure developed in Section 2.2 to obtain
an order for the inter-region connections in terms of the connection
strength, for each of the three study groups. To present this order in a
way that facilitates the comparison between the three groups, we
propose a tree-like plot. As an illustrative example, Fig. 7 is a tree-like
plot developed from the brain connectivity models in Fig. 2. One way
to read off information from Fig. 7 is to look at it from right (small λ)
to left (large λ). At a very small λ, i.e., λ = λ1, all regions are
connected. As λ goes larger, i.e., λ1 b λ ≤ λ2, X6, is the ﬁrst region
disconnected with other regions, so the connection between X6 and
other regions is the weakest. As λ continues to go larger, i.e., λ2 λ ≤ λ3, X4, X5, and the cluster of X1, X2, and X3 are disconnected, so the
connection between them is the second weakest. Finally, with λ3 b λ,
X1, X2, and X3 are disconnected, so the connectivity between them is
the strongest.

Following a similar manner, we develop a tree-like plot for AD, as
shown in Fig. 8. Specially, the range of λ is determined such that the
lower bound (λ = λL) corresponds to a “fully-connected” graph, i.e.,
every node has at least one arc attached, and the upper bound
(λ = λU) corresponds to a “null” graph which has no arcs. Starting
from the lower bound λ = λL, as λ goes larger, i.e., λL b λ ≤ λ2, region
“Tempora_Sup_L” is the ﬁrst one disconnected with the rest of the
brain, so “Tempora_Sup_L” may be the weakest connected region. As
λ continues to go larger, i.e., λ2 b λ ≤ λ3, the rest of the brain further
splits into three disconnected clusters, including the cluster of
“Cingulum_Post_R” and “Cingulum_Post_L”, the cluster of “Fusiform_R” up to “Temporal_Sup_R”, and the cluster of the other regions.
As λ continuously increases, each current cluster further splits into
smaller clusters. Eventually, when λ reaches λU, all regions become
disconnected. The sequence of the splitting gives an order for the
inter-region connections in terms of the connection strength.
Speciﬁcally, the earlier (i.e., smaller λ) a region or a cluster of regions
becomes disconnected with the rest of the brain, the weaker it is
connected with the rest of the brain. For example, in Fig. 8, it can be
known that “Tempora_Sup_L” may be weakest connected with the
rest of the brain in the brain network of AD; the second weakest ones
are the cluster of “Cingulum_Post_R” and “Cingulum_Post_L”, and the
cluster of “Fusiform_R” up to “Temporal_Sup_R”. It is very interesting
to see that the weakest and second weakest connected brain regions
in the brain network of AD include “Cingulum_Post_R” and “Cingulum_Post_L” as well as regions all in the temporal lobe, all of which
have been found to be affected by AD early and severely (Supekar et
al., 2008; Wang et al., 2007; Azari et al., 1992; Horwitz et al., 1987;
Grady et al., 2001).
Next, to facilitate the comparison between AD and NC, a tree-like
plot is also constructed for NC, as shown in Fig. 9. By comparing the
plots for AD and NC, we can observe the following two distinct
phenomena: First, in AD, between-lobe connections tend to be
weaker than within-lobe connections. This can be seen from Fig.
8 which shows a clear pattern that the lobes become disconnected
with each other before the regions within each lobe become
disconnected with each other, as λ goes from small to large. This
pattern does not show in Fig. 9 for NC. Second, the same brain
regions in the left and right hemispheres are connected much
weaker in AD than in NC. This can be seen from Fig. 9 for NC, in
which the same brain regions in the left and right hemispheres are
still connected even at a very large . However, this pattern does not
show in Fig. 8 for AD.
Furthermore, a tree-like plot is also constructed for MCI (Fig. 10)
and compared with the plots for AD and NC. In terms of the two
phenomena discussed previously, MCI shows similar patterns to AD,
but these patterns are not as distinct from NC as AD. Speciﬁcally, in
terms of the ﬁrst phenomenon, MCI also shows weaker between-

S. Huang et al. / NeuroImage 50 (2010) 935–949

943

Fig. 7. A tree-like plot showing the order of connections between the brain regions in
Fig. 2.

lobe connections than within-lobe connections, which is similar to
AD. However, this phenomenon is not as distinctive as AD. For
example, a few regions in the temporal lobe of MCI, including
“Temporal_Mid_R” and “Temporal_Sup_R”, appear to be more
strongly connected with the occipital lobe than with other regions
in the temporal lobe. In terms of the second phenomenon, MCI also
shows weaker between-hemisphere connections in the same brain
region than NC. However, this phenomenon is not as distinctive as
AD. For example, several left–right pairs of the same brain regions
are still connected even at a very large λ, such as “Rectus_R”
and “Rectus_L”, “Frontal_Mid_Orb_R” and “Frontal_Mid_Orb _L”,
“Parietal_Sup_R” and “Parietal_Sup_L”, as well as “Precuneus_R” and
“Precuneus_L”. All above ﬁndings are consistent with the knowledge
that MCI may be considered as a transition stage between normal
aging and AD. Note that the tree-like plots in Figs. 8, 9 and 10 reveal
the “observed” differences between AD, MCI, and NC. This serves as
a starting point for comparing AD, MCI, and NC in terms of the
connection strength. A challenging task following this may be to
formulate an appropriate hypothesis testing to test the statistical
signiﬁcance of the observed difference, which will be investigated in
future research.
Use of SICE for classiﬁcation of AD and NC

Fig. 6. Box plots for comparing AD (yellow) vs. NC (green) in terms of the amount of intraand inter-lobe direct connections (i.e., black cells or arcs).

The purpose of this experiment is to assess the classiﬁcation
accuracy of the proposed method in Section 2.3. The experiment is
performed on the PET dataset of 49 AD and 67 NC subjects. Leave-oneout cross-validation is applied. Speciﬁcally, we use each of the 116 (49
AD plus 67 NC) subjects as the “new” subject and the remaining
subjects as the training data. Then, we apply the proposed method in
Section 2.3 and obtain a “predicted” class (AD or NC) for the new
subject. In this manner, we can obtain predicted classes for all 116
subjects. The predicted classes are compared with the true classes and
classiﬁcation accuracy is computed.
Results from the experiment are shown in Figs. 11 and 12. In
particular, Fig. 11 shows the classiﬁcation accuracy (vertical axis) of
the 49 AD vs. λ values (horizontal axis), based on all 42 regions (blue
curve), frontal regions only (red curve), and temporal regions only
(green curve). Note that the classiﬁcation accuracy varies with
different λ's, because different λ's lead to different estimates for the
inverse covariance matrices, ΘAD and ΘNC, which further affect
performance of the classiﬁcation based on them. In practice, we can
choose a value for λ that achieves the desired classiﬁcation accuracy
for AD and NC, i.e., the desired sensitivity and speciﬁcity; and keep
this λ for classifying future subjects.
Some observations can be made based on the results in Figs. 11 and
12. The best sensitivity and speciﬁcity the proposed method can
achieve are 88% and 88%, respectively. However, they are not achieved
at the same λ, because gain in sensitivity is associated with loss in
speciﬁcity. Furthermore, performance of the classiﬁcation based on all
42 regions is much better than that based on frontal or temporal
regions alone. This may be because both frontal and temporal

944

S. Huang et al. / NeuroImage 50 (2010) 935–949

Fig. 8. A tree-like plot for AD, showing the order of connections in terms of connection strength.

connectivity, as well as other connectivity patterns identiﬁed in the
previous section of the paper (e.g., left-right hemisphere connectivity), have some discriminating power. Thus, using all 42 regions in the
classiﬁcation takes advantage of the combinatory effect of the
discriminating powers of these local connectivity patterns.

Discussion
In this paper, we proposed SICE for identifying functional brain
connectivity models from PET data. SICE was able to identify the
connectivity model structures, and with the aid of a quasi-measure we
developed, it can also identify the order of inter-region connections in
terms of the connection strength. We applied the proposed method to
the ADNI FDG-PET data of AD, MCI, and NC subjects. We compared
these three groups in terms of the amounts of connections within
lobes, between lobes, and between hemispheres, and in terms of the
strength of connections. Note that “strength” of connections is not the
same concept as “amount” of connections. For example, a lobe may
have a large amount of connections between the regions in that lobe,

but these regions may just be weakly connected to each other. Our
ﬁndings showed that:
Decrease in the amount of connections: Comparing AD with NC, we
found decrease in the amount of connections in the temporal lobe.
Within the temporal lobe, we found that hippocampus, in conjunction
with parahippocampus, is quite separated from other regions. These
ﬁndings support that the temporal lobe, especially the area around
hippocampus, is the ﬁrst and most severely affected by AD and are
consistent with previous ﬁndings in the literature (Supekar et al.,
2008; Wang et al., 2007; Azari et al., 1992; Horwitz et al., 1987; Grady
et al., 2001). Also, we found decrease in the amount of connections
between the temporal and occipital lobes, and between the frontal
and parietal lobes.
Increase in the amount of connections: We found increase in the
amount of connections in the frontal lobe of AD brains. This may be
interpreted as a compensatory effect or cognitive resource allocation
(Gould et al., 2006; Stern, 2006; Becker et al., 1996; Woodard et al.,
1998; Saykin et al., 2004; Grady et al., 2003; Supekar, 2008). In the
literature, the compensatory effect has not only been found in early
AD patients but also during memory tasks in healthy old adults

S. Huang et al. / NeuroImage 50 (2010) 935–949

945

Fig. 9. A tree-like plot for NC, showing the order of connections in terms of connection strength.

compared with younger adults (Cabeza et al., 1997; Madden et al.,
1999), suggesting that this might be a general response of human
brains to functional loss resulting from various causes. Engagement of
the frontal network is commonly found in studies of sustained
attention (Fuster, 2000). Thus, signiﬁcant increase in the amount of
connections in the frontal lobe may reﬂect a greater engagement of the
attentional resource to compensate for the decrease in the amount of
connections in other part of the brain. Furthermore, most previous
ﬁndings in the AD literature demonstrated the compensation effect
during task performance. Our ﬁndings suggested that even during
resting state, the increased recruitment of frontal resource still exists
and may reﬂect a more general adaptation to the deﬁcits of AD.
In addition, we also found increase in the amount of connections
between the parietal and occipital lobes of AD brains. It is interesting
to note that although there is signiﬁcant increase in the amount of
parietal-occipital connections, the amount of within-parietal and
within-occipital connections of AD is not signiﬁcant different from NC.
This may indicate that the compensatory effect takes place between
the parietal and occipital lobes but not within these lobes.

Strength of connections: The brain regions that are weakest
connected to the rest of the brain in the brain network of AD include
“Cingulum_Post_R” and “Cingulum_Post_L” as well as regions all in
the temporal lobe, all of which have been found to be affected by AD
early and severely (Supekar et al., 2008; Wang et al., 2007; Azari et al.,
1992; Horwitz et al., 1987; Grady et al., 2001). Furthermore, betweenlobe connections tend to be much weaker than within-lobe
connectivity for AD, while this phenomenon is not signiﬁcant for
NC. Also, the same brain regions in the left and right hemispheres are
connected much weaker in AD than in NC.
Findings about MCI: A unique perspective this paper provided is
that we also studied MCI. While abundant literature exists in
studying the brain connectivity difference between AD and NC,
studies on MCI are limited. Our ﬁndings included that: MCI does not
show as much decrease in the amount of connections in the temporal
lobe as AD, nor does MCI show as much increase in the amount of
connections in the frontal lobe and between the parietal and occipital
lobes as AD. In addition, MCI does not seem to disrupt the strong
connections between the same regions of the left and right hemispheres,

946

S. Huang et al. / NeuroImage 50 (2010) 935–949

Fig. 10. A tree-like plot for MCI, showing the order of connections in terms of connection strength.

Fig. 11. Classiﬁcation accuracy of AD.

S. Huang et al. / NeuroImage 50 (2010) 935–949

947

Fig. 12. Classiﬁcation accuracy of NC.

as AD does. All these ﬁndings have supported the clinical observation that
that MCI is a transition stage between normal aging and AD. One
interesting ﬁnding about MCI is that it shows signiﬁcant decrease in the
amount of connections between the frontal and occipital lobes, compared
with NC, while such decrease does not show for AD. This may suggest that
MCI does have some uniqueness, or heterogeneity, of its own, rather than
always being a clear pre-stage of AD.
On the other hand, our study on MCI does have limitations. First,
the 116 MCI subjects used for brain connectivity modeling is a
heterogeneous group. While they have all been diagnosed with MCI at
the baseline-time checkup, some of them may convert to AD, some to
NC, and others stay as MCI at later (e.g., 6th month or 12th month)
checkups. However, during the time when this paper was developed,
the diagnostic results of the 116 subjects at later checkups had not
been available. Therefore, although the sample size of the MCI group is
much larger than that of the AD and NC groups, the brain connectivity
models of MCI may still be unreliable due to the data heterogeneity. In
future work, we will split the current MCI group into MCI converters
and non-converters according to the diagnostic results at later
checkups, and build brain connectivity models for each subgroup.
This will lead to more reliable models.
Clinical relevance of this research: First, this work may be used in
clinical trials. Speciﬁcally, the modeling and analysis procedure on AD,
MCI, and NC groups proposed in this paper can be readily applied to
the groups given and not given a certain drug. Then, difference in the
connectivity patterns of the two groups can be used to assess the drug
efﬁcacy. A signiﬁcant advantage of applying this work to clinical trials
is that SICE is able to produce reliable brain connectivity models with
small sample sizes. This can help signiﬁcantly lower the sample size
requirement in clinical trials, increase the statistical power of the
between-group comparison, and expedite drug efﬁcacy assessment.
Second, this work may be used for functional brain connectivity
modeling based on fMRI data. The difference between PET and fMRI
modeling is that the “samples” in PET modeling are subjects, while in
fMRI modeling they are consecutive time points in the fMRI time
series. As a result, the connectivity models built from PET data are
models for each group (e.g., AD, MCI, or NC), while those from fMRI
data are models for each subject. Once subject-level brain connectivity
models are available, we can further identify connectivity-based
biomarkers for AD and MCI. Note that most existing imaging
biomarkers are based on individual brain regions, which may be
greatly complemented by connectivity-based markers which charac-

terize how the interactions between brain regions are affected by AD
or MCI pathology. Third, it is also possible to deﬁne connectivitybased biomarkers based on the connectivity models built from PET
data. To achieve this, we proposed a method in Section 2.3 and
showed the experimental results using our data in Section 3.5.
Supplementary material and future work: Due the space limit, we
put some additional results in the Supplementary Material: (1)
Normality check of the data, in order to justify the multivariate
normality assumption in SICE. The results showed that the data can be
reasonably assumed to follow a multivariate normal distribution. (2)
A whole-brain 116-region connectivity modeling. The results conﬁrmed the major ﬁndings from the 42-region modeling.
Several future research directions are pointed out here. First, SICE
provides a model for the linear interactions between brain regions,
because it is based on the covariance matrix of the data. An
interesting future direction is to explore the nonlinear interactions,
which may be achieved by ﬁrst discretizing the measurement of each
brain region and then building a graphical model (e.g., a Bayesian
network) of the brain regions based on the discretized measurements. Second, the current preprocessing procedure involves the use
of the default SPM5 registration. We will explore the use of improved
image registration algorithms in SPM5/8 (DARTEL).
Acknowledgments
Data collection and sharing for this project were funded by the
Alzheimer's Disease Neuroimaging Initiative (ADNI; Principal Investigator: Michael Weiner; NIH grant U01 AG024904). ADNI is funded by
the National Institute on Aging, the National Institute of Biomedical
Imaging and Bioengineering (NIBIB), and through generous contributions from the following: Pﬁzer Inc., Wyeth Research, Bristol-Myers
Squibb, Eli Lilly and Company, GlaxoSmithKline, Merck & Co. Inc.,
AstraZeneca AB, Novartis Pharmaceuticals Corporation, Alzheimer's
Association, Eisai Global Clinical Development, Elan Corporation plc,
Forest Laboratories, and the Institute for the Study of Aging, with
participation from the U.S. Food and Drug Administration. Industry
partnerships are coordinated through the Foundation for the National
Institutes of Health. The grantee organization is the Northern California
Institute for Research and Education, and the study is coordinated by the
Alzheimer's Disease Cooperative Study at the University of California,
San Diego. ADNI data are disseminated by the Laboratory of Neuro
Imaging at the University of California, Los Angeles.

948

S. Huang et al. / NeuroImage 50 (2010) 935–949

Appendix A. Proof of the monotone property of the SICE method
A sufﬁcient and necessary condition of the monotone property is
as follow:
n
o
n
o
Theorem 1: Let C1λ1 ; :::; CLλ11 and C1λ2 ; :::; CLλ22 denote the clusters
of nodes in the SICE-based brain connectivity models, with λ equal to
λ1 and λ2 (λ1 b λ2), respectively. Here, a “cluster” of nodes means
these nodes are all connected to one another either directly or
indirectly. Then, for any Ciλ2 , i ∈ {1, 2 , …, L2}, there must exist a Cjλ1 j ∈
{1, 2, …, L1}, such that Ciλ2 pCjλ1 .
This section proves the monotone property by proving that
Theorem 1 is true.
(1) can be equivalently written as


−1
−1
Σ̂ = argmin log detðΣÞ + tr SΣ
+ λjjΣ jj1 :

ðSÞkl −ðΣÞkl = − λ;
ðSÞkl −ðΣÞkl = λ;
j ðSÞkl −ðΣÞkl jVλ;

ðA  2Þ

kl

where (·)kl denotes the element at the kth row, lth column of a matrix.
When λ = λ1, denote the solution to Eq. (A-1) by Σ̂ λ1 . Furthermore,
we can rearrange the rows and columns of Σ̂ λ1 , such that Σ̂ λ1 becomes
a block diagonal matrix and each sub-matrix along the main diagonal
of the rearranged Σ̂ λ1 correspond to a cluster of nodes in the SICEbased graphical model. Denote the sub-matrices by Σ̂ λλ11 ; j = 1; :::; L1 .
Cj
Recall that Cλj 1 is the jth cluster of nodes in the graphical
model. As a
λ1
̂
result, Σ can be written as:
2

λ
Σ̂ 1

λ
Σ̂ λ11
6 C1
6
6 0
6
=6
6
6 v
6
4
0

0

:::

λ
Σ̂ λ11

:::

v

O
:::

C2

0

7
7
0 7
7
7:
7
v 7
7
λ 5
Σ̂ λ11

λ
Σ̂ 2

λ
Σ̂ λ21
6 C1
6
6 0
6
=6
6
6 v
6
4
0

0

:::

λ
Σ̂ λ21

:::

v

O
:::

C2

0

x̂

x̂

λ2

λ
C11

6
6
6
6 0
6
=6
6
6 v
6
4
0

λ2

0

:::

λ2

:::

x̂

λ

C21

v

O

0

:::

3

0

7
7
7
0 7
7
7:
7
v 7
7
λ 5
x̂ λ21

ðA  7Þ

CL

1

It is obvious that x̂λ2 has the same structure as Σ̂λ1 .
Step Two:
This step aims to prove that the x̂λ2 in Eq. (A-7) satisﬁes Eq. (A-2)
with λ = λ2. To prove this, we need to prove that (i) the elements
in x̂λλ21 , j = 1,…, L1, satisfy Eq. (A-2) and that (ii) the elements not in
C
x̂λλ21 , jall of which are equal to zero, also satisfy Eq. (A-2).


Cj
λ2
(i) Suppose that x̂λ2 kl is
 anelement in x̂Cλj 1 , j ∈ {1, …, L1}; more
speciﬁcally, suppose that x̂λ2 kl is the element
at the hth row,
sth column of x̂λλ21 , i.e.,
Cj




λ2
λ2
= x̂
:
ðA  8Þ
x̂ λ1
Cj

kl

hs

Because x λ2 is the solution to the optimization in Eq. (A-6), it must
satisfy Eq. (A-9):



hs


SCλ1
j

hs



λ
− x λ21
Cj

hs

Cj

hs

= λ2 ;






λ2
 S λ1
−
x
λ
 Cj
C 1

hs

j



Vλ ;
 2

It is easy to know that
row, lth column of S, i.e.,


= ðSÞkl ;
SCλ1
j

hs


and


xλλ21

−1

−1

Cj

Cj

hs


 − 1
λ
for x λ21
b0;
Cj


 − 1
λ
for x λ21
Cj



SCλ1
j

ðA  9Þ

hs

= 0;
hs

is in fact the element at the kth

hs

ðA  10Þ

!
is the element at the kth row, lth column of

Cj

hs

x̂λ2
, i.e.,

 − 1
λ
x 2λ1
hs

1


 − 1
λ
for x λ21
N 0;

= − λ2 ;



λ
− x λ21

hs

To prove this sufﬁcient condition, our strategy will include two steps:
step one aims to ﬁnd a matrix having the same structure as Σ̂ λ1 ; step two
aims to prove that this matrix is a solution to Eq. (A-2) with λ =λ2.
Step One:
The rows and columns of the sample covariance matrix, S, can be
rearranged in the same way as Σ̂ λ1 , i.e.,
2
3
SCλ1 : : : : : : : : :
1
7
6
6 :::
7
SCλ1 : : : : : : 7
6
6
7
2
S=6
ðA  5Þ
7:
6 v
7
v
O
v
6
7
4 ::: ::: :::
5
SCλ1
L1

2

j

ðA  4Þ

1

Furthermore, the solutions to (A-6), i.e., x̂λλ21 , j = 1,…, L1, can be
Cj
put together and form a big matrix xλ2 , i.e.,

3

7
7
0 7
7
7:
7
v 7
7
λ 5
Σ̂ λ21

Cj

ðA  6Þ


ðA  3Þ

Cj

j

1

0

CL

Cj

SCλ1

A sufﬁcient condition for Theorem 1 being true is that the solution
to Eq. (A-1) when λ = λ2, denoted by Σ̂ λ2 , must share the same
structure as Eq. (A-3), i.e., Σ̂ λ2 can be written as:
2

Cj

3

0

CL

j






− 1 
 
 λ − 1 
λ
λ
 ;
+ λ2  x λ21
x̂ λ1 = armin log det x λ21 + tr SCλ1 x λ21

λ2

ðA  1Þ

It is known from (Banerjee et al., 2008) that the solution, Σ̂, is
unique with a ﬁxed positive λ, and Σ̂ must satisfy the equations in Eq.
(A-2):


for Σ − 1 N 0;
kl


for Σ − 1 b0;

 kl
−1
for Σ
= 0;

Next, one optimization problem can be formulated corresponding
to one sub-matrix SCλ1 , j = 1,…, L1, i.e.,

=



x̂

λ2

 − 1

:

ðA  11Þ

kl

Inserting Eqs. (A-8), (A-10), and (A-11) into Eq. (A-9) results in Eq.


(A-2) with λ = λ2.


(ii) Suppose that x̂λ2 kl is an element not in x̂λ2λ2 ; j = 1; :::;
 Cj  − 1 


= 0,
L1 ; i:e:; x̂λ2 kl = 0. Furthermore, it can be known that x̂λ2

 − 1  kl
because x̂λ2 is a block diagonal matrix. Since
x̂λ2
= 0, to
kl


prove that x̂λ2 kl satisﬁes Eq. (A-2) with λ = λ2 is to prove that




j ðSÞkl − x̂λ2 kl jVλ2 . It can be derive that j ðSÞkl − x̂λ2 kl j = j ðSÞkl j =
 λ 
j ðSÞ − x̂ 1 kl jVλ1 , where the second equality holds because
 kl
Σ̂λ1
= 0, and the “≤” holds due to the last equation in Eq. (A-1)
kl

with λ = λ1. Also, it has been known that λ1 = λ2. Therefore,


j ðSÞkl − x̂λ2 kl jVλ1 Vλ2 .

S. Huang et al. / NeuroImage 50 (2010) 935–949

Appendix B. Supplementary data
Supplementary data associated with this article can be found, in
the online version, at doi:10.1016/j.neuroimage.2009.12.120.
References
Alexander, G., Moeller, J., 1994. application of the scaled subproﬁle model: a statistical
approach to the analysis of functional patterns in neuropsychiatric disorders: a
principal component approach to modeling regional patterns of brain function in
disease. Hum. Brain Mapp. 79–94.
Ashburner, J., Friston, K.J, 1999. Nonlinear spatial normalization using basis functions.
Hum. Brain Mapp. 7, 254–266.
Azari, N.P., Rapoport, S.I., Grady, C.L., Schapiro, M.B., Salerno, J.A., Gonzales-Aviles, A,
1992. Patterns of interregional correlations of cerebral glucose metabolic rates in
patients with dementia of the Alzheimer type. Neurodegeneration 1, 101–111.
Banerjee, O., Ghaoui, L.E., d'Aspremont, A, 2008. Model selection through sparse
maximum likelihood estimation for multivariate gaussian or binary data. J. Mach.
Learn. Res. 9, 485–516.
Becker, J.T., Mintun, M.A., Aleva, K., Wiseman, M.B., Nichols, T., DeKosky, S.T, 1996.
Compensatory reallocation of brain resources supporting verbal episodic memory
in Alzheimer's disease. Neurology 46, 692–700.
Benjamini, Y., Hochberg, Y., 1995. Controlling the false discovery rate: a practical and
powerful approach to multiple testing. J. R. Stat. Soc. Ser. B 57, 289–300.
Cabeza, R., Grady, C.L., Nyberg, L., McIntosh, A.R., Tulving, E., Kapur, S., Jennings, J.M.,
Houle, S., Craik, F.I.M., 1997. Age-related differences in neural activity during
memory encoding and retrieval: a positron emission tomography study. J. Neurosci.
17, 391–400.
Calhoun, V.D., Adali, T., Pearlson, G.D., Pekar, J.J, 2001. Spatial and temporal
independent component analysis of functional MRI data containing a pair of
task-related waveforms. Hum. Brain Mapp. 13, 43–53.
Calhoun, V.D., Adali, T., Pekar, J.J., Pearlson, G.D., 2003. Latency (in)sensitive ICA. Group
independent component analysis of fMRI data in the temporal frequency domain.
NeuroImage 20, 1661–1669.
Chen, R., Herskovits, E.H, 2007. Graphical-model-based multivariate analysis of
functional magnetic-resonance data. NeuroImage 35 (2), 635–647.
Chiang, J.; Wang, Z.J.; and McKeown, M.J. 2009 Sparse Multivariate Autoregressive
(mAR)-based Partial Directed Coherence (PDC) for Electroencephalogram (EEG)
Analysis, Proceedings of the 2009 IEEE International Conference on Acoustics,
Speech and Signal Processing: 457-460, 2009.
Delbeuck, X., Van der Linden, M., Collette, F, 2003. Alzheimer's disease as a
disconnection syndrome? Neuropsychol. Rev. 13 (2), 79–92.
Dempster, A.P., 1972. Covariance selection. Biometrics 28 (1), 157–175.
Friedman, J., Tastie, Tibsirani, R., 2007. Sparse inverse covariance estimation with the
graphical lasso. Biostatistics 8 (1), 1–10.
Friston, K.J., 1994. Functional and effective connectivity: a synthesis. Hum. Brain Mapp.
2, 56–78.
Friston, K.J., Ashburner, J., Frith, C.D., Pline, J.-B., Heather, J.D., Frachowiak, R.S.J, 1995.
Spatial registration and normalization of images. Hum. Brain Mapp. 2, 89–165.
Fuster, J.M, 2000. Executive frontal functions. Exp. Brain Res. 133, 66–70.
Gould, R.L., Arroyo, B., Brown, R.G., Owen, A.M., Bullmore, E.T., Howard, R.J., 2006. Brain
mechanisms of successful compensation during learning in Alzheimer disease.
Neurology 67, 1011–1017.
Grady, C.L., Furey, M.L., Pietrini, P., Horwitz, B., Rapoport, S.I, 2001. Altered brain
functional connectivity and impaired short-term memory in Alzheimer's disease.
Brain 124, 739–756.
Grady, C.L., McIntosh, A.R., Beig, S., Keightley, M.L., Burian, H., Black, S.E, 2003. Evidence
from functional neuroimaging of a compensatory prefrontal network in Alzheimer's
disease. J. Neurosci. 23, 986–993.
Hampson, M., Peterson, B.J., Skudlarski, P., Gatenby, J.C., Gore, J.C, 2002. Detection of
functional connectivity using temporal correlations in MR images. Hum. Brain
Mapp. 15, 247—262.
Hilgetag, C., Kotter, R., Stephan, K.E, 2002. Computational methods for the analysis of
brain connectivity. In: Ascoli, G.A. (Ed.), Computational Neuroanatomy. Humana
Press, Totowa, NJ.
Hirano, A., Zimmerman, H.M., 1962. Alzheimer's neuroﬁbrillary changes. A topographic
study. Arch. Neurol. 7, 227—242.
Horwitz, B, 2003. The elusive concept of brain connectivity. NeuroImage 19, 466–470.
Horwitz, B., Grady, C.L., Sclageter, N.L., Duara, R., Rapoport, S.I, 1987. intercorrelations of
regional glucose metabolic rates in Alzheimer's disease. Brain Res. 407, 294–306.
Huang, S., Li, J., Sun, Li., Liu, J., Wu, T., Chen, K., Fleisher, A., Reiman, E., and Ye, J. 2009
Learning Brain Connectivity of Azheimer's Disease from Neuroimaging Data.
Proceedings of Neural Information Processing Systems Conference (NIPS) 2009
(acceptance rate 8%), Vancouver, B.C., Canada, September 7-9, 2009.

949

Kotter, R., Stephan, M.E., 2003. Network participation indices: characterizing
component roles for information processing in neural networks. Neural Netw.
16, 1261–1275.
Levina, E., Rothman, A.J., Zhu, J, 2008. Sparse Estimation of Large CovarianceMatrices via
a Nested Lasso Penalty. Ann. Appl. Stat. 2, 245–263.
Li, H., Gui, J, 2006. Gradient directed regularization for sparse gaussian concentration
graphs, with applications to inference of genetic networks. Biostatistics 7,
302–317.
Li, J.N., Wang, Z.J., Palmer, S.J., McKeown, M.J., 2008. Dynamic Bayesian network
modeling of fMRI: a comparison of group-analysis methods. NeuroImage 41,
398–407.
Madden, D.J., Turkington, T.G., Provenzale, J.M., Denny, L.L., Hawk, T.C., Gottlob, L.R.,
Coleman, R.E, 1999. Adult age differences in the functional neuroanatomy of verbal
recognition memory. Hum. Brain Mapp. 7, 115–135.
Marrelec, G., Krainik, A., Duffau, H., Pe´le´grini-Issac, M., Lehe´ricy, S., Doyon, J., et al.,
2006. Partial correlation for functional brain interactivity investigation in
functional MRI. NeuroImage 32, 228–237.
Marrelec, et al., 2007. Using partial correlation to enhance structural equation modeling
of functional MRI data. Magn. Reson. Imaging 25, 1181–1189.
McIntosh, A.R., Bookstein, F.L., Haxby, J.V., Grady, C.L, 1996. Spatial pattern analysis of
functional brain images using partial least squares. NeuroImage 3, 143–157.
Rajapakse, J.C., Zhou, J, 2007. Learning effective brain connectivity with dynamic
Bayesian networks. NeuroImage 37, 749–760.
Salvador, R., Suckling, J., Coleman, M., Pickard, J.D., Menon, D., Bullmore, E, 2005a.
Neurophysiological architecture of functional magnetic resonanceimages of human
brain. Cereb. Cortex 34, 387–413.
Salvador, R., Suckling, J., Schwarzbauer, C., Bullmore, E., 2005b. Undirected graphs of
frequency dependent functional connectivity in whole brain networks. Philos.
Trans. R. Soc. Lond. B Biol. Sci. 360, 937–946.
Saykin, A.J., Wishart, H.A., Rabin, L.A., et al., 2004. Cholinergic enhancement of frontal
lobe activity in mild cognitive impairment. Brain 127, 1574–1583.
Schafer, J., Strimmer, K.A, 2005. Shrinkage approach to large-scale covariance matrix
estimation and implications for functional genomics. Stat. Appl. Genet. Mol. Biol. 4
(1) Article 32.
Sporns, O., Chialvo, D.R., Kaiser, M., Hilgetag, C.C., 2004. Organization, development and
function of complex brain networks. Trends Cogn. Sci. 8, 418–425.
Stam, C.J., Jones, B.F., Nolte, G., Breakspear, M., Scheltens, P, 2007. Small-world networks
and functional connectivity in Alzheimer's disease. Cereb. Cortex 17, 92–99.
Stern, Y, 2006. Cognitive reserve and Alzheimer disease. Alzheimer Disease Associated
Disorder 20, 69–74.
Sun, L.; Patel, R.; Liu, J.; Chen, K.; Wu, T.; Li, J.; Reiman, E.; Ye, J. 2009 Mining Brain
Region Connectivity for Alzheimer's Disease Study via Sparse Inverse Covariance
Estimation. Proceedings of Knowledge Discovery and Data Mining Conference KDD
2009.
Supekar, K., Menon, V., Rubin, D., Musen, M., Greicius, M.D, 2008. Network analysis of
intrinsic functional brain connectivity in Alzheimer's disease. PLoS Comput. Biol. 4
(6), 1–11.
Thompson, W.K., Barber, A., Siegle, G.J.A, 2009. Bayesian sparse vector autoregressive
model for resting state connectivity analyses. NeuroImage 47 (Supplement 1),
S39–S41.
Tzourio-Mazoyer, N., et al., 2002. Automated anatomical labelling of activations in SPM
using a macroscopic anatomical parcellation of the MNI MRI single subject brain.
NeuroImage 15, 273–289.
Valdés-Sosa, P.A., Sanchez-Bornot, J.M., Lage-Castellanos, A., Vega-Hernandez, M.,
Bosch-Bayard, J., Melie-García, L., Canales-Rodriguez, E., 2005. Estimating brain
functional connectivity with sparse multivariate autoregression. Phil. Trans. R. Soc.
B. 969–981.
Wang, K., Liang, M., Wang, L., Tian, L., Zhang, X., Li, K., Jiang, T., 2007. Altered functional
connectivity in early Alzheimer's disease: a resting-state fMRI study. Hum. Brain
Mapp. 28, 967–978.
Woodard, J.L., Grafton, S.T., Votaw, J.R., Green, R.C., Dobraski, M.E., Hoffman, J.M, 1998.
Compensatory recruitment of neural resources during overt rehearsal of word lists
in Alzheimer's disease. Neuropsychology 12, 491–504.
Worsley, K.J., Poline, J.B., Friston, K.J., Evans, A.C, 1997. Characterizing the response
of PET and fMRI data using multivariate linear models. NeuroImage 6,
305–319.
Yuan, M., Lin, Y, 2007. Model selection and estimation in the Gaussian graphical model.
Biometrika 94 (1), 19–35.
Zheng, X., Rajapakse, J.C, 2004. Graphical models for brain connectivity from functional
imaging data. Proceedings of 2004 IEEE International Joint Conference on Neural
Networks. .
Zhilkin, P., Alexander, M.E., 2004. Afﬁne registration: a comparison of several programs.
Magn. Reson. Imaging 22, 55–66.
Zhuang, J.C., Laconte, S., Peltier, S., Zhang, K., Hu, X.P., 2005. Connectivity exploration
with structural equation modeling: an fMRI study of bimanual motor coordination.
NeuroImage 25, 462–470.

Learning Brain Connectivity of Alzheimer's
Disease from Neuroimaging Data

Shuai Huang 1, Jing Li 1, Liang Sun 2,3, Jun Liu 2,3, Teresa Wu1, Kewei Chen 4,
Adam Fleisher 4, Eric Reiman 4, Jieping Ye 2,3
1
Industrial Engineering, 2Computer Science and Engineering, and 3 Center for Evolutionary
Functional Genomics, The Biodesign Institute, Arizona State University, Tempe, USA
{shuang31, jing.li.8, sun.liang, j.liu, teresa.wu, jieping.ye}@asu.edu
4
Banner Alzheimer’s Institute and Banner PET Center, Banner Good Samaritan Medical
Center, Phoenix, USA
{kewei.chen , adam.fleisher, eric.reiman}@bannerhealth.com

Abstract
Recent advances in neuroimaging techniques provide great potentials for
effective diagnosis of Alzheimer’s disease (AD), the most common form of
dementia. Previous studies have shown that AD is closely related to the
alternation in the functional brain network, i.e., the functional connectivity
among different brain regions. In this paper, we consider the problem of
learning functional brain connectivity from neuroimaging, which holds
great promise for identifying image-based markers used to distinguish
Normal Controls (NC), patients with Mild Cognitive Impairment (MCI),
and patients with AD.
More specifically, we study sparse inverse
covariance estimation (SICE), also known as exploratory Gaussian
graphical models, for brain connectivity modeling. In particular, we apply
SICE to learn and analyze functional brain connectivity patterns from
different subject groups, based on a key property of SICE, called the
“monotone property” we established in this paper. Our experimental results
on neuroimaging PET data of 42 AD, 116 MCI, and 67 NC subjects reveal
several interesting connectivity patterns consistent with literature findings,
and also some new patterns that can help the knowledge discovery of AD.

1

In trod u cti on

Alzheimer’s disease (AD) is a fatal, neurodegenerative disorder characterized by progressive
impairment of memory and other cognitive functions. It is the most common form of
dementia and currently affects over five million Americans; this number will grow to as
many as 14 million by year 2050. The current knowledge about the cause of AD is very
limited; clinical diagnosis is imprecise with definite diagnosis only possible by autopsy;
also, there is currently no cure for AD, while most drugs only alleviate the symptoms.
To tackle these challenging issues, the rapidly advancing neuroimaging techniques provide
great potentials. These techniques, such as MRI, PET, and fMRI, produce data (images) of
brain structure and function, making it possible to identify the difference between AD and
normal brains. Recent studies have demonstrated that neuroimaging data provide more
sensitive and consistent measures of AD onset and progression than conventional clinical

assessment and neuropsychological tests [1].
Recent studies have found that AD is closely related to the alternation in the functional brain
network, i.e., the functional connectivity among different brain regions [ 2]-[3]. Specifically,
it has been shown that functional connectivity substantially decreases between the
hippocampus and other regions of AD brains [3]-[4]. Also, some studies have found
increased connectivity between the regions in the frontal lobe [ 6]-[7].
Learning functional brain connectivity from neuroimaging data holds great promise for
identifying image-based markers used to distinguish among AD, MCI (Mild Cognitive
Impairment), and normal aging. Note that MCI is a transition stage from normal aging to
AD. Understanding and precise diagnosis of MCI have significant clinical value since it can
serve as an early warning sign of AD. Despite all these, existing research in functional brain
connectivity modeling suffers from limitations. A large body of functional connectivity
modeling has been based on correlation analysis [2]-[3], [5]. However, correlation only
captures pairwise information and fails to provide a complete account for the interaction of
many (more than two) brain regions. Other multivariate statistical methods have also been
used, such as Principle Component Analysis (PCA) [8], PCA-based Scaled Subprofile Model
[9], Independent Component Analysis [10]-[11], and Partial Least Squares [12]-[13], which
group brain regions into latent components. The brain regions within each component are
believed to have strong connectivity, while the connectivity between components is weak.
One major drawback of these methods is that the latent components may not correspond to
any biological entities, causing difficulty in interpretation. In addition, graphical models
have been used to study brain connectivity, such as structural equation models [14]-[15],
dynamic causal models [16], and Granger causality. However, most of these approaches are
confirmative, rather than exploratory, in the sense that they require a prior model of brain
connectivity to begin with. This makes them inadequate for studying AD brain connectivity,
because there is little prior knowledge about which regions should be involved and how they
are connected. This makes exploratory models highly desirable.
In this paper, we study sparse inverse covariance estimation (SICE), also known as
exploratory Gaussian graphical models, for brain connectivity modeling. Inverse covariance
matrix has a clear interpretation that the off-diagonal elements correspond to partial
correlations, i.e., the correlation between each pair of brain regions given all other regions.
This provides a much better model for brain connectivity than simple correlation analysis
which models each pair of regions without considering other regions. Also, imposing
sparsity on the inverse covariance estimation ensures a reliable brain connectivity to be
modeled with limited sample size, which is usually the case in AD studies since clinical
samples are difficult to obtain. From a domain perspective, imposing sparsity is also valid
because neurological findings have demonstrated that a brain region usually only directly
interacts with a few other brain regions in neurological processes [ 2]-[3]. Various algorithms
for achieving SICE have been developed in recent year [ 17]-[22]. In addition, SICE has been
used in various applications [17], [21], [23]-[26].
In this paper, we apply SICE to learn functional brain connectivity from neuroimaging and
analyze the difference among AD, MCI, and NC based on a key property of SICE, called the
“monotone property” we established in this paper. Unlike the previous study which is based
on a specific level of sparsity [26], the monotone property allows us to study the
connectivity pattern using different levels of sparsity and obtain an order for the strength of
connection between pairs of brain regions. In addition, we apply bootstrap hypothesis testing
to assess the significance of the connection. Our experimental results on PET data of 42 AD,
116 MCI, and 67 NC subjects enrolled in the Alzheimer’s Disease Neuroimaging Initiative
project reveal several interesting connectivity patterns consistent with literature findings,
and also some new patterns that can help the knowledge discovery of AD.

2

S ICE : B ack grou n d an d th e Mon oton e P rop erty

An inverse covariance matrix can be represented graphically. If used to represent brain
connectivity, the nodes are activated brain regions; existence of an arc between two nodes
means that the two brain regions are closely related in the brain's functiona l process.

Let
be all the brain regions under study. We assume that
follows a
multivariate Gaussian distribution with mean and covariance matrix . Let
be the
inverse covariance matrix. Suppose we have samples (e.g., subjects with AD) for these
brain regions. Note that we will only illustrate here the SICE for AD, whereas the SICE for
MCI and NC can be achieved in a similar way.
We can formulate the SICE into an optimization problem, i.e.,
(1)
where
is the sample covariance matrix;
,
, and
denote the
determinant, trace, and sum of the absolute values of all elements of a matrix, respectively.
The part “
” in (1) is the log-likelihood, whereas the part “
”
represents the “sparsity” of the inverse covariance matrix . (1) aims to achieve a tradeoff
between the likelihood fit of the inverse covariance estimate and the sparsity. The tradeoff is
controlled by , called the regularization parameter; larger will result in more sparse estimate
for . The formulation in (1) follows the same line of the -norm regularization, which has been
introduced into the least squares formulation to achieve model sparsity and the resulting model is
called Lasso [27]. We employ the algorithm in [19] in this paper. Next, we show that with
going from small to large, the resulting brain connectivity models have a monotone property.
Before introducing the monotone property, the following definitions are needed.
Definition: In the graphical representation of the inverse covariance, if node
to
by an arc, then
is called a “neighbor” of . If
is connected to
chain of arcs, then
is called a “connectivity component” of .

is connected
though some

Intuitively, being neighbors means that two nodes (i.e., brain regions) are directly connected,
whereas being connectivity components means that two brain regions are indirectly
connected, i.e., the connection is mediated through other regions. In other words, not being
connectivity components (i.e., two nodes completely separated in the graph) means that the
two corresponding brain regions are completely independent of each other. Connectivity
components have the following monotone property:
Monotone property of SICE: Let
components of
with
and

and
be the sets of all the connectivity
, respectively. If
, then
.

Intuitively, if two regions are connected (either directly or indirectly) at one level of
sparseness (
), they will be connected at all lower levels of sparseness (
). Proof
of the monotone property can be found in the supplementary file [29]. This monotone
property can be used to identify how strongly connected each node (brain region)
to its
connectivity components. For example, assuming that
and
,
this means that
is more strongly connected to
than . Thus, by changing from small
to large, we can obtain an order for the strength of connection between pairs of brain
regions. As will be shown in Section 3, this order is different among AD, MCI, and NC.

3
3.1

Ap p l i cati on i n B rai n Con n ecti vi ty M od el i n g of AD
D a t a a c q u i s i t i o n a n d p re p ro c e s s i n g

We apply SICE on FDG-PET images for 49 AD, 116 MCI, and 67 NC subjects downloaded from
the ADNI website. We apply Automated Anatomical Labeling (AAL) [28] to extract data from
each of the 116 anatomical volumes of interest (AVOI), and derived average of each AVOI for
every subject. The AVOIs represent different regions of the whole brain.
3.2

B r a i n c o n n e c t i v i t y mo d e l i n g b y S I C E

42 AVOIs are selected for brain connectivity modeling, as they are considered to be potentially
related to AD. These regions distribute in the frontal, parietal, occipital, and temporal lobes. Table
1 list of the names of the AVOIs with their corresponding lobes. The number before each AVOI is
used to index the node in the connectivity models.

We apply the SICE algorithm to learn one connectivity model for AD, one for MCI, and one for
NC, for a given . With different ’s, the resulting connectivity models hold a monotone property,
which can help obtain an order for the strength of connection between brain regions. To show the
order clearly, we develop a tree-like plot in Fig. 1, which is for the AD group. To generate this
plot, we start at a very small value (i.e., the right-most of the horizontal axis), which results in a
fully-connected connectivity model. A fully-connected connectivity model is one that contains no
region disconnected with the rest of the brain. Then, we decrease by small steps and record the
order of the regions disconnected with the rest of the brain regions.
Table 1: Names of the AVOIs for connectivity modeling (“L” means that the brain region
is located at the left hemisphere; “R” means right hemisphere.)
Frontal lobe

Parietal lobe

Occipital lobe

Temporal lobe

1 Frontal_Sup_L

13 Parietal_Sup_L

21 Occipital_Sup_L 27 T emporal_Sup_L

2 Frontal_Sup_R

14 Parietal_Sup_R

22 Occipital_Sup_R 28 T emporal_Sup_R

3 Frontal_Mid_L

15 Parietal_Inf_L

23 Occipital_Mid_L 29 T emporal_Pole_Sup_L

4 Frontal_Mid_R

16 Parietal_Inf_R

24 Occipital_Mid_R 30 T emporal_Pole_Sup_R

5 Frontal_Sup_Medial_L

17 Precuneus_L

25 Occipital_Inf_L

31 T emporal_Mid_L

6 Frontal_Sup_Medial_R

18 Precuneus_R

26 Occipital_Inf_R

32 T emporal_Mid_R

7 Frontal_Mid_Orb_L

19 Cingulum_Post_L

33 T emporal_Pole_Mid_L

8 Frontal_Mid_Orb_R

20 Cingulum_Post_R

34 T emporal_Pole_Mid_R

9 Rectus_L

35 T emporal_Inf_L 8301

10 Rectus_R

36 T emporal_Inf_R 8302

11 Cingulum_Ant_L

37 Fusiform_L

12 Cingulum_Ant_R

38 Fusiform_R
39 Hippocampus_L
40 Hippocampus_R
41 ParaHippocampal_L
42 ParaHippocampal_R

For example, in Fig. 1, as decreases below
(but still above ), region “Tempora_Sup_L” is
the first one becoming disconnected from the rest of the brain. As decreases below
(but still
above ), the rest of the brain further divides into three disconnected clusters, including the
cluster of “Cingulum_Post_R” and “Cingulum_Post_L”, the cluster of “Fusiform_R” up to
“Hippocampus_L”, and the cluster of the other regions. As continuously decreases, each current
cluster will split into smaller clusters; eventually, when reaches a very large value, there will be
no arc in the IC model, i.e., each region is now a cluster of itself and the split will stop. The
sequence of the splitting gives an order for the strength of connection between brain regions.
Specifically, the earlier (i.e., smaller ) a region or a cluster of regions becomes disconnected from
the rest of the brain, the weaker it is connected with the rest of the brain. For example, in Fig. 1, it
can be known that “Tempora_Sup_L” may be the weakest region in the brain network of AD; the
second weakest ones are the cluster of “Cingulum_Post_R” and “Cingulum_Post_L”, and the
cluster of “Fusiform_R” up to “Hippocampus_L”. It is very interesting to see that the weakest and
second weakest brain regions in the brain network include “Cingulum_Post_R” and
“Cingulum_Post_L” as well as regions all in the temporal lobe, all of which have been found to be
affected by AD early and severely [3]-[5].
Next, to facilitate the comparison between AD and NC, a tree-like plot is also constructed for NC,
as shown in Fig. 2. By comparing the plots for AD and NC, we can observe the following two
distinct phenomena: First, in AD, between-lobe connectivity tends to be weaker than within-lobe
connectivity. This can be seen from Fig. 1 which shows a clear pattern that the lobes become
disconnected with each other before the regions within each lobe become disconnected with each
other, as goes from small to large. This pattern does not show in Fig. 2 for NC. Second, the
same brain regions in the left and right hemisphere are connected much weaker in AD than in NC.
This can be seen from Fig. 2 for NC, in which the same brain regions in the left and right
hemisphere are still connected even at a very large for NC. However, this pattern does not show
in Fig. 1 for AD.
Furthermore, a tree-like plot is also constructed for MCI (Fig. 3), and compared with the plots for
AD and NC. In terms of the two phenomena discussed previously, MCI shows similar patterns to
AD, but these patterns are not as distinct from NC as AD. Specifically, in terms of the first

phenomenon, MCI also shows weaker between-lobe connectivity than within-lobe connectivity,
which is similar to AD. However, the degree of weakerness is not as distinctive as AD. For
example, a few regions in the temporal lobe of MCI, including “Temporal_Mid_R” and
“Temporal_Sup_R”, appear to be more strongly connected with the occipital lobe than with other
regions in the temporal lobe. In terms of the second phenomenon, MCI also shows weaker
between-hemisphere connectivity in the same brain region than NC. However, the degree of
weakerness is not as distinctive as AD. For example, several left-right pairs of the same brain
regions are still connected even at a very large , such as “Rectus_R” and “Rectus_L”,
“Frontal_Mid_Orb_R” and “Frontal_Mid_Orb _L”, “Parietal_Sup_R” and “Parietal_Sup_L”, as
well as “Precuneus_R” and “Precuneus_L”. All above findings are consistent with the knowledge
that MCI is a transition stage between normal aging and AD.

Large λ

λ3

λ2

λ1

Small λ

Fig 1: Order for the strength of connection between brain regions of AD

Large λ

Small λ

Fig 2: Order for the strength of connection between brain regions of NC

Fig 3: Order for the strength of connection between brain regions of MCI
Furthermore, we would like to compare how within-lobe and between-lobe connectivity is
different across AD, MCI, and NC. To achieve this, we first learn one connectivity model for AD,
one for MCI, and one for NC. We adjust the in the learning of each model such that the three
models, corresponding to AD, MCI, and NC, respectively, will have the same total number of
arcs. This is to “normalize” the models, so that the comparison will be more focused on how the
arcs distribute differently across different models. By selecting different values for the total
number of arcs, we can obtain models representing the brain connectivity at different levels of
strength. Specifically, given a small value for the total number of arcs, only strong arcs will show
up in the resulting connectivity model, so the model is a model of strong brain connectivity; when
increasing the total number of arcs, mild arcs will also show up in the resulting connectivity
model, so the model is a model of mild and strong brain connectivity.
For example, Fig. 4 shows the connectivity models for AD, MCI, and NC with the total number of
arcs equal to 50 (Fig. 4(a)), 120 (Fig. 4(b)), and 180 (Fig. 4(c)). In this paper, we use a “matrix”
representation for the SICE of a connectivity model. In the matrix, each row represents one node
and each column also represents one node. Please see Table 1 for the correspondence between the
numbering of the nodes and the brain region each number represents. The matrix contains black
and white cells: a black cell at the -th row, -th column of the matrix represents existence of an
arc between nodes
and
in the SICE-based connectivity model, whereas a white cell
represents absence of an arc. According to this definition, the total number of black cells in the
matrix is equal to twice the total number of arcs in the SICE-based connectivity model. Moreover,
on each matrix, four red cubes are used to highlight the brain regions in each of the four lobes; that
is, from top-left to bottom-right, the red cubes highlight the frontal, parietal, occipital, and
temporal lobes, respectively. The black cells inside each red cube reflect within-lobe connectivity,
whereas the black cells outside the cubes reflect between-lobe connectivity.
While the connectivity models in Fig. 4 clearly show some connectivity difference between AD,
MCI, and NC, it is highly desirable to test if the observed difference is statistically significant.
Therefore, we further perform a hypothesis testing and the results are summarized in Table 2.
Specifically, a P-value is recorded in the sub-table if it is smaller than 0.1, such a P-value is further
highlighted if it is even smaller than 0.05; a “---” indicates that the corresponding test is not
significant (P-value>0.1). We can observe from Fig. 4 and Table 2:

Within-lobe connectivity: The temporal lobe of AD has significantly less connectivity than NC.
This is true across different strength levels (e.g., strong, mild, and weak) of the connectivity; in
other words, even the connectivity between some strongly-connected brain regions in the temporal
lobe may be disrupted by AD. In particular, it is clearly from Fig. 4(b) that the regions
“Hippocampus” and “ParaHippocampal” (numbered by 39-42, located at the right-bottom corner
of Fig. 4(b)) are much more separated from other regions in AD than in NC. The decrease in
connectivity in the temporal lobe of AD, especially between the Hippocampus and other regions,
has been extensively reported in the literature [3]-[5]. Furthermore, the temporal lobe of MCI does
not show a significant decrease in connectivity, compared with NC. This may be because MCI
does not disrupt the temporal lobe as badly as AD.

AD

MCI

NC

Fig 4(a): SICE-based brain connectivity models (total number of arcs equal to 50)

AD

MCI

NC

Fig 4(b): SICE-based brain connectivity models (total number of arcs equal to 120)

AD

MCI

NC

Fig 4(c): SICE-based brain connectivity models (total number of arcs equal to 180)
The frontal lobe of AD has significantly more connectivity than NC, which is true across different
strength levels of the connectivity. This has been interpreted as compensatory reallocation or
recruitment of cognitive resources [6]-[7]. Because the regions in the frontal lobe are typically
affected later in the course of AD (our data are early AD), the increased connectivity in the frontal
lobe may help preserve some cognitive functions in AD patients. Furthermore, the frontal lobe of
MCI does not show a significant increase in connectivity, compared with NC. This indicates that
the compensatory effect in MCI brain may not be as strong as that in AD brains.

Table 2: P-values from the statistical significance test of connectivity difference among
AD, MCI, and NC
(a) Total number of arcs = 50

(b) Total number of arcs = 120 (c) Total number of arcs = 180

There is no significant difference among AD, MCI, and NC in terms of the connectivity within the
parietal lobe and within the occipital lobe. Another interesting finding is that all the P-values in the
third sub-table of Table 2(a) are insignificant. This implies that distribution of the strong
connectivity within and between lobes for MCI is very similar to NC; in other words, MCI has not
been able to disrupt the strong connectivity among brain regions (it disrupts some mild and weak
connectivity though).
Between-lobe connectivity: In general, human brains tend to have less between-lobe connectivity
than within-lobe connectivity. A majority of the strong connectivity occurs within lobes, but rarely
between lobes. These can be clearly seen from Fig. 4 (especially Fig. 4(a)) in which there are
much more black cells along the diagonal direction than the off-diagonal direction, regardless of
AD, MCI, and NC.
The connectivity between the parietal and occipital lobes of AD is significantly more than NC
which is true especially for mild and weak connectivity. The increased connectivity between the
parietal and occipital lobes of AD has been previously reported in [3]. It is also interpreted as a
compensatory effect in [6]-[7]. Furthermore, MCI also shows increased connectivity between the
parietal and occipital lobes, compared with NC, but the increase is not as significant as AD.
While the connectivity between the frontal and occipital lobes shows little difference between AD
and NC, such connectivity for MCI shows a significant decrease especially for mild and weak
connectivity. Also, AD may have less temporal-occipital connectivity, less frontal-parietal
connectivity, but more parietal-temporal connectivity than NC.
Between-hemisphere connectivity: Recall that we have observed from the tree-like plots in Figs. 3
and 4 that the same brain regions in the left and right hemisphere are connected much weaker in
AD than in NC. It is desirable to test if this observed difference is statistically significant. To
achieve this, we test the statistical significance of the difference among AD, MCI, and NC, in term
of the number of connected same-region left-right pairs. Results show that when the total number
of arcs in the connectivity models is equal to 120 or 90, none of the tests is significant. However,
when the total number of arcs is equal to 50, the P-values of the tests for “AD vs. NC”, “AD vs.
MCI”, and “MCI vs. NC” are 0.009, 0.004, and 0.315, respectively. We further perform tests for
the total number of arcs equal to 30 and find the P-values to be 0. 0055, 0.053, and 0.158,
respectively. These results indicate that AD disrupts the strong connectivity between the same
regions of the left and right hemispheres, whereas this disruption is not significant in MCI.

4

Con cl u si on

In the paper, we applied SICE to model functional brain connectivity of AD, MCI, and NC based
on PET neuroimaging data, and analyze the patterns based on the monotone property of SICE. Our
findings were consistent with the previous literature and also showed some new aspects that may
suggest further investigation in brain connectivity research in the future.

R e f e re n c e s
[1] S. Molchan. (2005) The Alzheimer's disease neuroimaging initiative. Business Briefing: US
Neurology Review, pp.30-32, 2005.
[2] C.J. Stam, B.F. Jones, G. Nolte, M. Breakspear, and P. Scheltens. (2007) Small-world networks and
functional connectivity in Alzheimer’s disease. Cerebral Corter 17:92-99.
[3] K. Supekar, V. Menon, D. Rubin, M. Musen, M.D. Greicius. (2008)
Network Analysis of Intrinsic
Functional Brain Connectivity in Alzheimer's Disease. PLoS Comput Biol 4(6) 1-11.
[4] K. Wang, M. Liang, L. Wang, L. Tian, X. Zhang, K. Li and T. Jiang. (2007) Altered Functional
Connectivity in Early Alzheimer’s Disease: A Resting-State fMRI Study, Human Brain Mapping 28, 967978.
[5] N.P. Azari, S.I. Rapoport, C.L. Grady, M.B. Schapiro, J.A. Salerno, A. Gonzales-Aviles. (1992) Patterns
of interregional correlations of cerebral glucose metabolic rates in patients with dementia of the Alzheimer
type. Neurodegeneration 1: 101–111.
[6] R.L. Gould, B.Arroyo, R,G. Brown, A.M. Owen, E.T. Bullmore and R.J. Howard. (2006) Brain
Mechanisms of Successful Compensation during Learning in Alzheimer Disease, Neurology 67, 1011-1017.
[7] Y. Stern. (2006) Cognitive Reserve and Alzheimer Disease, Alzheimer Disease Associated Disorder 20,
69-74.
[8] K.J. Friston. (1994) Functional and effective connectivity: A synthesis. Human Brain Mapping 2, 56-78.
[9] G. Alexander, J. Moeller. (1994) Application of the Scaled Subprofile model: a statistical approach to the
analysis of functional patterns in neuropsychiatric disorders: A principal component approach to modeling
regional patterns of brain function in disease. Human Brain Mapping, 79-94.
[10] V.D. Calhoun, T. Adali, G.D. Pearlson, J.J. Pekar. (2001) Spatial and temporal independent component
analysis of functional MRI data containing a pair of task-related waveforms. Hum.Brain Mapp. 13, 43-53.
[11] V.D. Calhoun, T. Adali, J.J. Pekar, G.D. Pearlson. (2003) Latency (in)sensitive ICA. Group independent
component analysis of fMRI data in the temporal frequency domain. Neuroimage. 20, 1661-1669.
[12] A.R. McIntosh, F.L. Bookstein, J.V. Haxby, C.L. Grady. (1996) Spatial pattern analysis of functional
brain images using partial least squares. Neuroimage. 3, 143-157.
[13] K.J. Worsley, J.B. Poline, K.J. Friston, A.C. Evans. (1997) Characterizing the response of PET and
fMRI data using multivariate linear models. Neuroimage. 6, 305-319.
[14] E. Bullmore, B. Horwitz, G. Honey, M. Brammer, S. Williams, T. Sharma. (2000) How good is good
enough in path analysis of fMRI data? NeuroImage 11, 289–301.
[15] A.R. McIntosh, C.L. Grady, L.G. Ungerieider, J.V. Haxby, S.I. Rapoport, B. Horwitz. (1994) Network
analysis of cortical visual pathways mapped with PET. J. Neurosci. 14 (2), 655–666.
[16] K.J. Friston, L. Harrison, W. Penny. (2003) Dynamic causal modelling. Neuroimage 19, 1273-1302.
[17] O. Banerjee, L. El Ghaoui, and A. d’Aspremont. (2008) Model selection through sparse maximum
likelihood estimation for multivariate gaussian or binary data. Journal of Machine Learning Research 9:485516.
[18] J. Dahl, L. Vandenberghe, and V. Roycowdhury. (2008) Covariance selection for nonchordal graphs via
chordal embedding. Optimization Methods Software 23(4):501-520.
[19] J. Friedman, T.astie, and R. Tibsirani. (2007) Spares inverse covariance estimation with the graphical
lasso, Biostatistics 8(1):1-10.
[20] J.Z. Huang, N. Liu, M. Pourahmadi, and L. Liu. (2006) Covariance matrix selection and estimation via
penalized normal likelihood. Biometrika, 93(1):85-98.
[21] H. Li and J. Gui. (2005) Gradient directed regularization for sparse Gaussian concentration graphs, with
applications to inference of genetic networks. Biostatistics 7(2):302-317.
[22] Y. Lin. (2007) Model selection and estimation in the gaussian graphical model. Biometrika 94(1)19-35,
2007.
[23] A. Dobra, C. Hans, B. Jones, J.R. Nevins, G. Yao, and M. West. (2004) Sparse graphical models for
exploring gene expression data. Journal of Multivariate Analysis 90(1):196-212.
[24] A. Berge, A.C. Jensen, and A.H.S. Solberg. (2007) Sparse inverse covariance estimates for hyperspectral
image classification, Geoscience and Remote Sensing, IEEE Transactions on, 45(5):1399-1407.
[25] J.A. Bilmes. (2000) Factored sparse inverse covariance matrices. In ICASSP:1009-1012.
[26] L. Sun and et al. (2009) Mining Brain Region Connectivity for Alzheimer's Disease Study via Sparse
Inverse Covariance Estimation. In KDD: 1335-1344.
[27] R. Tibshirani. (1996) Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society Series B 58(1):267-288.
[28] N. Tzourio-Mazoyer and et al. (2002) Automated anatomical labeling of activations in SPM using a
macroscopic anatomical parcellation of the MNI MRI single subject brain. Neuroimage 15:273-289.
[29] Supplemental information for “Learning Brain Connectivity of Alzheimer's Disease from Neuroimaging
Data”. http://www.public.asu.edu/~jye02/Publications/AD-supplemental-NIPS09.pdf

Brain Effective Connectivity Modeling for Alzheimer’s
Disease by Sparse Gaussian Bayesian Network
Shuai Huang1, Jing Li1, Jieping Ye1, Adam Fleisher2, Kewei Chen2, Teresa Wu1, Eric Reiman2
1

School of Computing, Informatics, and Decisions Systems Engineering, Arizona State University, Tempe, AZ,
85287
2

Banner Alzheimer’s Institute, Banner Good Samaritan Medical Center, Phoenix, AZ, 85006

ABSTRACT

1. INTRODUCTION

Recent studies have shown that Alzheimer's disease (AD) is
related to alteration in brain connectivity networks. One type
of connectivity, called effective connectivity, defined as the
directional relationship between brain regions, is essential to
brain function. However, there have been few studies on
modeling the effective connectivity of AD and characterizing
its difference from normal controls (NC). In this paper, we
investigate the sparse Bayesian Network (BN) for effective
connectivity modeling. Specifically, we propose a novel
formulation for the structure learning of BNs, which involves
one L1-norm penalty term to impose sparsity and another
penalty to ensure the learned BN to be a directed acyclic
graph – a required property of BNs. We show, through both
theoretical analysis and extensive experiments on eleven moderate
and large benchmark networks with various sample sizes, that the
proposed method has much improved learning accuracy and
scalability compared with ten competing algorithms. We apply the
proposed method to FDG-PET images of 42 AD and 67 NC
subjects, and identify the effective connectivity models for AD
and NC, respectively. Our study reveals that the effective
connectivity of AD is different from that of NC in many ways,
including the global-scale effective connectivity, intra-lobe, interlobe, and inter-hemispheric effective connectivity distributions, as
well as the effective connectivity associated with specific brain
regions. These findings are consistent with known pathology and
clinical progression of AD, and will contribute to AD knowledge
discovery.

Alzheimer’s disease (AD) is the most common cause of
dementia and the fifth leading cause of death in people over
65 in the US. The current annual cost of AD care in the U.S.
is more than $100 billion, which will continue to grow fast.
The existing knowledge about the cause of AD is very
limited. Clinical diagnosis is imprecise with a definite
diagnosis only possible by autopsy. Also, there is currently no
cure for AD, while most drugs only modestly alleviate
symptoms. To tackle these challenging issues in AD studies,
fast advancing neuroimaging techniques hold great promise.
Recent studies have shown that neuroimaging can provide
sensitive and reliable measures of AD onset and progression,
which can complement the conventional clinical-based
assessments and cognitive measures.
In neuroimaging-based AD research, one important area is
brain connectivity modeling, i.e., identification of how
different brain regions interact to produce a cognitive
function in AD, compared with normal aging. Research in this
area can substantially promote AD knowledge discovery and
identification of novel connectivity-based AD biomarkers to
be used in clinical practice. There are two types of
connectivity being studied: functional connectivity refers to
the covarying pattern of different brain regions; effective
connectivity refers to the directional relationship between
regions [1].
A vast majority of the existing research focuses on functional
connectivity modeling. Various methods have been adopted
such as correlation analysis [2], Principal Component
Analysis (PCA) [3], PCA-based Scaled Subprofile Model [4],
Independent Component Analysis [5], and Partial Least
Squares [6]. Recently, sparse models have also been
introduced, such as sparse multivariate or vector
autoregressions [7] and sparse inverse covariance estimation
[8]. Sparse models have shown great effectiveness because
neuroimaging datasets are featured by "small n large p", i.e.,
the number of AD patients (n) can be close to or less than the
number of brain regions modeled (p). Also, many past studies
based on anatomical brain databases have shown that the true
brain network is indeed sparse [9].

Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications – Data
Mining; J.3 [Life and Medical Sciences]: Health, Medical
information systems

General Terms
Algorithms

Keywords
Brain network, Alzheimer’s disease, neuroimaging, FDG-PET,
Bayesian network, sparse learning

Compared with functional connectivity modeling, effective
connectivity modeling has the advantage of helping identify
the pathway/mechanism whereby distinct brain regions
communicate with each other. However, the existing research
in effective connectivity modeling is much less extensive.
Models that have been adopted include structural equation
models [10] and dynamic causal models [11]. The limitations
of these models include (i) they are confirmative, rather than

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise,
or republish, to post on servers or to redistribute to lists, requires prior
specific permission and/or a fee.
KDD’11, August 21–24, 2011, San Diego, California, USA.
Copyright 2011 ACM 978-1-4503-0813-7/11/08...$10.00.

931

explanatory, i.e., they require a prior model of connectivity to
start with; (ii) they require a substantially larger sample size
than the number of regions modeled; a typical number of
regions included is less than 10 given the sample size limit.
These limitations make them inappropriate for AD
connectivity modeling, because there is little prior knowledge
of which regions should be included and how they are
connected.

Our study reveals that the effective connectivity of AD is
different from NC in many ways, including the global-scale
effective connectivity, intra-lobe, inter-lobe, and interhemisphere effective connectivity distributions, as well as the
effective connectivity associated with specific brain regions.
The findings are consistent with known pathology and
clinical progression of AD.

2. BAYESIAN NETWORK: KEY
DEFINITIONS AND CONCEPTS

We propose sparse Bayesian Networks (BN) for effective
connectivity modeling. A BN is an explanatory model and the
sparse estimation makes it possible to include a large number
of brain regions. In a BN representation of effective
connectivity, the nodes are brain regions. A directed arc from
to
( is called a parent of ) indicates a direct
node
influence from
to
. Sparsity consideration has been
common in the BN learning literature. For example, some
early work used score functions, such as BIC and MDL, to
measure the goodness-of-fit of a BN, in which a penalty term
on the model complexity is usually included in the score [12].
Recently, driven by modern applications such as genetics,
learning of large-scale BNs has been very popular, in which
sparsity consideration is indispensible. The Sparse Candidate
(SC) algorithm [13], one of the first BN structure learning
algorithms to be applied to a large number of variables,
assumes that the maximum number of parents for each node is
limited to a small constant. The L1MB-DAG algorithm
developed in [14] uses LASSO [15] to identify a small set of
potential parents for each variable. Some other algorithms,
such as the Max-Min Hill-Climbing (MMHC) [16], GrowShrink [17], TC and TC_bw [18], all follow the same line.
Most of these existing algorithms employ a two-stage
approach: Stage 1 is to identify the potential parents of each
variable; Stage 2 usually applies some heuristic search
algorithms (e.g., hill-climbing) or orientation methods (e.g.,
Meek's rules [19]) to identify the parents out of the potential
parent set. An apparent weakness of the two-stage approach is
that if a true parent is missed in Stage 1, it will never be
recovered in Stage 2. Another weakness of the existing
algorithms is computational efficiency, i.e., it may take hours
or days to learn a large-scale BN such as one with 500 nodes.

This section introduces the key definitions and concepts of BNs
that are relevant to this paper:
A BN is composed of a structure and a set of parameters. The
structure (Fig. 1) is a DAG that consists of nodes
,…,
and directed arcs between some nodes; no cycle is allowed in a
DAG. Each node represents a random variable. If there is a
directed arc from
to ,
is
 X
X2
and
is
called a parent of
1
called a child of . Two nodes are
X3
X4
called spouses if they share a
common child. If there is a
directed path from
to , i.e.,
X5
,
is called an
Figure 1. A BN structure
ancestor of . A directed arc is
also a directed path and a parent is
also an ancestor according to this definition. The Markov Blanket
(MB) of
is a set of variables given which
will be
independent of all other variables. The MB includes the parents,
children, and spouses of .
In this paper, we use the following notations with respect to a BN
structure: Denote the structure by a
matrix , with entry
1 representing a directed arc from
to . The set of
is denoted by
,…,
. In
parents of a node
addition, we define a
matrix, , which records all the
directed paths in the structure, i.e., if there is a directed path from
to , entry
1; otherwise,
0.
In addition to the structure, other important components of a BN
are the parameters. The parameters are the conditional probability
distribution of each node given its parents. Specifically, when the
nodes follow a multivariate Gaussian distribution, a regressiontype parameterization can be adopted, i.e.,

In this paper, we propose a new sparse BN learning algorithm,
which is called SBN. It is a one-stage approach that identifies
the parents of all variables directly. The main contributions of
this paper include:







and
, ,
. Then, the parameters
with ~ 0,
,…,
. Without loss of generality, we
of a BN are
assume that the nodes are standardized, i.e., each with a zero
,…,
to
mean and unit variance. This means, if using
denote the sample vector for , and to denote the sample size, we
0 and
1.
have ∑

We propose a novel sparse BN learning algorithm, i.e., SBN,
using one L1-norm penalty term to impose sparsity and
another penalty term to ensure the learned BN is a directed
acyclic graph (DAG).
We present theoretical guidance on how to select the
regularization parameter associated with the second
penalty.
We perform theoretical analysis to reason why the two-stage
approach popularly adopted in the existing literature has a
high risk of failing to identify the true parents. Also, we
conduct extensive experiments on synthetic data to compare
SBN and the existing algorithms in terms of the learning
accuracy and scalability.
We apply SBN to FDG-PET data of 42 AD patients and 67
normal controls (NC) subjects enrolled in the ADNI
(Alzheimer's Disease Neuroimaging Initiative) project, and
identify the effective connectivity models for AD and NC.

3. THE PROPOSED SPARSE BN
STRUCTURE LEARNING ALGORITHM
One of the challenging issues in BN structure learning is to ensure that
the learned structure must be a DAG, i.e., no cycle is present. To
achieve this, we first identify a sufficient and necessary condition for
a DAG, which is given as Lemma 1 below.
Lemma 1. A sufficient and necessary condition for a DAG is
0 for every pair of nodes and .
Proof. To prove the necessary condition, suppose that a BN structure,

932

⁄
contradiction to show that, with any
1
, we
will get a DAG. Suppose that such a doesn’t guarantee a DAG.
Then, there must be at least a pair of variables
and
with
0, i.e.,
0 and
1. Based on the first order
optimality condition,
0 i.f.f.
/ / ,
0. Here, / denotes the elements in without ,
and / , denotes the sample matrix for all the variables except
and . However, we have

, is a DAG. Let’s assume that
0 for a pair of nodes
and . Then, there exists a directed path from to and a directed
path from to , i.e., there is a cycle in , which is a contradiction
to our presumption that
is a DAG. To prove the sufficient
condition, suppose that
0 for every pair of nodes and
. If is not a DAG, i.e., there is a cycle, it means that there exist
two variables, and , with a directed arc from to (
0)
and a directed path from to (
1). This contradicts with our
assumption that
0 for every pair of nodes and . □

/

∑

/

0, ,

. .

1, … , ,

1

.

∑

∑

/

⁄

1

will guarantee

∑

/
/

to

/

1 , for each

/

/

.

(3)

Input: sample matrix, ;
;
, ; initial
Initialize: Let
0;
Repeat
For
1,2, … ,
A Breadth-first search on with being
the root node to calculate
for
1, … , .
Use the shooting algorithm to optimize
;
and get
End for
Until converge
Figure 2. The BCD algorithm used for solving (2)

Proof. To prove this we first need to show that with a certain value of
and any value of
,
is bounded. This can be seen
from

0.□

/ ,

As a result, the shooting algorithm [23] for LASSO may be used to
in each iteration. Note that at each iteration for
optimize
, we also need to calculate
for
optimizing
/ . This can
be done by a Breadth-first search on with being the root node. A
more detailed description of the BCD algorithm used to solve (2) is
given in Figure 2.

(2)

is a
where
/ denotes that the variable indexed by , i.e.,
∑ /
variable different from . Here,
is to push
to become zero. Under some mild conditions, there exists a
such that for all
,
is also a minimizer for (1) [21].
Theorem 1 gives a practical estimation for .
Theorem 1. Any
be a DAG.

/

/

min ∑
,

/

X/ ,

⁄ ,

Given and , the BCD algorithm [22] can be employed to solve
(2). The BCD algorithm updates each iteratively, assuming that all
other parameters are fixed. In our situation, this is equivalent to
optimizing
iteratively and the algorithm will terminate when
,
some convergence conditions are satisfied. We remark that
after some transformation, is similar to LASSO [15], i.e.,

Solving the constrained optimization in (1) is difficult. Therefore, the
penalty method [21] is employed to transform it into an unconstrained
optimization problem, through adding an extra L1-norm penalty into
the original objective function, i.e.,

/

1

⁄
Theorem 1 implies that if we specify any
1
,
we will get a minimizer of (1) through solving (2). However, in
practice, directly solving (2) by specifying a large may converge
slowly. This is because that the unconstrained problem in (2) may be
ill-conditioned with a too large value for [40]. To avoid this, the
“warm start” method [21] can be used, which works in the following
way: first, it specifies a series of values for , i.e.,
⁄
, where is small and
1
; next, it
to get a minimizer
, using an arbitrary
optimizes (2) with
, using
as an
initial value; then, it optimizes (2) with
initial value; this process iterates, until it optimizes (2) with
.
With the last minimizer as the initial value for the next optimization
problem, this method can be quite efficient.

(1)

, is a profile
objective function, ∑
/
/
likelihood to measure the model fit. The second term,
, is the
sum of the absolute values of the elements in and thus is the so,
called L1-norm penalty [15]. The regularization parameter,
controls the number of nonzero elements in the solution to , ;
larger , less nonzero elements. Because less nonzero elements in
correspond to fewer arcs in the learned BN structure, a larger
results in a sparser structure. In addition, the constraints are to assure
that the learned BN is a DAG (see Lemma 1 and Theorem 1 below).
We remark that these constraints are functions of only , since
expm
[20]. Here, expm
is the matrix exponential of .

/

max

which results in

,…,
denotes the
The notations are explained as follows:
sample vector for , where n is the sample size. / denotes the
sample matrix for all the variables except
.The first term in the

∑

∑

/ ,

/

Based on Lemma 1, we further present our formulation of the sparse
BN structure learning. It is an optimization problem with the objective
function and constraints given by:

. The last

inequality holds because
is the value of the function on the
left hand side with
0 , which is obviously larger than the
. The last equality holds since we have
function value with
standardized all the variables. Thus, we know that
max
1 ⁄ . Now, we use proof-by-

Finally, we want to mention that the L2-norm penalty,
∑ /
, might also be used in (2). The advantage is
that it is a differentiable function of
. Also, as shown in [21],

933

0 when
∞ . However, the limitation of the L2norm penalty, compared with the L1-norm penalty, is that there is no
guarantee that a finite exists to assure
0 for all pairs of
and .

is regressed on all other variables in Stage 1 of TC,
variance. If
, the coefficient
i.e.,
for 's parent, , is:
∏
∏

Time complexity analysis of the proposed algorithm: Each
iteration of the BCD algorithm consists of two operations: a
shooting algorithm and a Breadth-first search on . These two
| | , respectively. Here
operations cost
[24] and
| | is the number of nonzero elements in . If is sparse, i.e.,
| |
| |
with a small constant ,
. Thus, the
computational cost at each iteration is only
. Furthermore,
each sweep through all columns of
costs
. Our
simulation study shows that it usually takes no more than 5
sweeps to converge.

, and

∏

∑

,

∏
∏

.

∏

∑

,

Theorem 3. Consider a general inverse tree with
2
,
variables, whose structure and parameters are given by
∑
1,2, … , ,
3, … ,
,
,
,
∑
.
All
the
variables
,
,
have unit variance. If
is regressed on all other variables, i.e.,
∑
∑
,
,
,
,
the coefficients for
's parent,
(
1,2, … , ), are:

4. THEORETICAL ANALYSIS OF THE
COMPETITIVE ADVANTAGE OF THE
PROPOSED SBN ALGORITHM

∑
,

,

Simulation studies in Sec. 5 will show that SBN is more accurate
than various existing algorithms that employ a two-stage
approach. This section aims to provide some theoretical insights
about why it is so. Recall that Stage 1 of the two-stage approach is
to identify potential parents of each variable . The existing
algorithms achieve this by identifying the MB of . A typical
way is variable selection based on regression, i.e., to build a
regression of on all other variables and consider the variables
selected to be the MB. The key differences between various
algorithms are the type of regression used and the method in
variable selection. For example, the TC algorithm [18] uses
ordinary regression and a t-test for variable selection; the L1MBDAG algorithm [14] uses LASSO.

,

∑

,

∑

∑
∑

,

∑
,

,
,

For example, consider a general tree with
0.028
8. Based on Theorem 2,
⁄
0.092√
1
⁄
0.3144√
1.

In the regression of , the coefficients for the variables that are
not in the MB will be small (theoretically zero due to the
definition of MB). However, the coefficients for the parents may
also be very small due to the correlation between the parents and
children. As a result, some parents may not be selected in the
variable selection, i.e., they will be missed in Stage 1 of the twostage approach, leading to greater BN learning errors. In contrast,
the SBN may not suffer from this, because it is a one-stage
approach that identifies the parents directly.

,

,

,

,

,

∑

,

∑

,

∑

∑

,

,
,

,
,

,

,

∑

,

∑

, and

,

,
,

∑

,

∑

∑

,

,
,

,

,

,

.

,

0.3,
and

Consider a general inverse tree with seven variables:
are parents of which is a parent of ;

,

0.8,

1,

,5,

, ,
0.3.
0.24, 0.325, 0.256, 0.304, 0.216 ;
Based on Theorem 3,
, ,
0.036, 0.053, 0.038, 0.045, 0.032
⁄
⁄
, ,
, ,
is equal to
and
0.1142,0.1672,0.1216, 0.1444,0.1026
1

To further illustrate this point, we analyze one two-stage
algorithm, the TC algorithm. TC does variable selection using a ttest. To determine whether a variable should be selected, a t-test
uses the statistic ⁄
, where
is the estimate for the
regression coefficient of this variable and
is the standard
error. The larger the ⁄
, the higher chance the variable will
be selected. Theorems 2 and 3 below show that even though the
value of ⁄
corresponding to a parent of is large in the
true BN, its value may decrease drastically in the regression of
on all other variables. Theorem 2 focuses on a specific type of
BNs, a general tree, in which all variables have one common
ancestor and there is at most one directed path between two
variables. Theorem 3 focuses on a general inverse tree, which
becomes a general tree if all the arcs are reversed. The proof of
Theorem 2 can be found in the Appendix. The proof of Theorem 3
is not shown here due to space limitations.

which is less than
one is equal to

,

,

0.306,0.4491,0.3266,0.3878,0.2785

since the later
1 .

Note that these theorems derive the relationship between
) and ( ⁄
) at the population-level (i.e.,
(
sampling error is not considered), so they use the notation “ ” not
“ ”.

5. SIMULATION STUDIES ON
SYNTHETIC DATA
We show four sets of simulations. The first one is to show that, on
a general tree, the existing algorithms based on the two-stage
approach may miss some true parents with high probabilities,
while the proposed SBN performs well. The second simulation is
to compare the structure learning accuracy of SBN with other
competing algorithms, on benchmark networks. The third and
fourth simulations are to investigate the scalability of SBN and
compare it with other competing algorithms.

Theorem 2. Consider a general tree with
2 variables, whose
,
,
structure and parameters are given by
,
3,4, … , . All the variables have unit

934

(a)

(b)

(c)

Figure 3. (a): The general tree in Sec. 5.1 (regression coefficients are 0.3 for arcs between
and ,
, , ,
and 0.8 for others); (b) Simulation results for the general tree; (c) Simulation results for 11 benchmark networks.
ratio of the total learning error (false positives plus false
negatives) to the number of arcs in the true BN. Each curve
corresponds to one of the 11 algorithms under comparison. We
can observe that the lowest curve (i.e., best performance) is SBN.

5.1 Learning accuracy for general tree
Ten existing algorithms are selected: HITON-PC [25], IAMB and
three of its variants [26], GS [17], SC [13], TC and its advanced
version TC-bw [18], and L1MB [14]. We simulate data from the
general tree in Figure 3(a) with a sample size of 200.
We apply the selected algorithms on the simulated data; the
parameters of each algorithm are selected in the way that the
authors have suggested in their papers, respectively. In applying
the proposed SBN, is selected by BIC; is set to be 10
⁄
1
which empirically guarantees a DAG to be
learned. The initial value of SBN is the output of L1MB which
uses LASSO in Stage 1 to identify the MB for each variable. We
treat the identified MB by L1MB as parents and use the resulting
“BN” (not necessarily a DAG) as the initial value for SBN. The
results over 100 repetitions are shown in Figure 3(b). The X-axis
records the 10 selected algorithms and the proposed SBN (the last
one). The Y-axis records the frequency of a true arc being
identified. Each color bar corresponds to a true arc indicated in the
legend. Six true arcs are shown, i.e., the arcs between
and ,
is the parent of , the Y-axis actually
2, ,7. Because
shows how well the parent of can be identified by each of the
algorithms. Figure 3 (b) shows that, SBN performs much better
than all others. This can be explained by Theorem 1. Specifically,
the MB of
includes
and six children. Although the
coefficient linking
to is as high as 0.3, this coefficient in the
regression that regresses
on its MB reduces to 0.028, due to the
inclusion of the children in the regression. As a result,
will
have a high probability of being excluded from the MB identified
in Stage 1 of the existing algorithms.

(a)

(b)

Figure 4. Scalability of SBN with respect to (a) the
number of variables in a BN, , and (b) the sample size,
, on a computer with Intel Core 2, 2.2 G Hz, 4G

5.2 Learning accuracy for Benchmark
networks

Figure 5. Comparison of SBN with competing
algorithms on the CPU time in structure learning of
the other nine benchmark networks

We select seven moderately large networks from BNR [27]. We
also use the tiling technique to produce two large BNs, alarm2 and
hailfinder2. Two other networks with specific structures, factor
and chain, are also considered. The 11 networks are: (number of
nodes/edges): 1. factors (27/68), 2. alarm (37/46), 3. barley
(48/84), 4. carpo (61/74), 5. chain(7/6), 6. hailfinder (56/66), 7.
insurance (27/52), 8. mildew (35/46), 9. water (32/66), 10. alarm2
(296/410), 11. hailfinder2 (280/390). To specify the parameters of
a network, i.e., to specify the regression coefficients of each
variable on its parents, we randomly sample from 1
0,1/16 . Then, we simulate data from the networks with
sample size 100, and apply the 10 algorithms to learn the BN
structures. The results over 100 repetitions are shown in Figure
3(c). The X-axis records the 11 networks. The Y-axis records the

5.3 Scalability
We study two aspects of scalability for SBN: the scalability with
respect to the number of variables in a BN, , and the scalability
with respect to the sample size, . We use the CPU time for each
sweep through all the columns of
as the parameter for
measurement. Specifically, we fix =1000, and vary by using
the 11 benchmark networks. Also, we fix =37 (the Alarm
network). The results over 100 repetitions are shown in Figure 4
(a) and (b), respectively. It can be seen that the computational cost
is linear in and quadratic in , which confirms our theoretical
time complexity analysis in Section 3.

935

Table 2. Names of the AVOIs for effective connectivity
modeling (L = left hemisphere, R = right hemisphere)
Frontal lobe

Parietal lobe

Occipital lobe

Temporal lobe

1 Front al_Sup_L

13 P ariet al_Sup_L

21 Occipit al_Sup_L 27 T emporal_Sup_L

2 Front al_Sup_R

14 P ariet al_Sup_R

22 Occipit al_Sup_R 28 T emporal_Sup_R

3 Front al_Mid_L

15 P ariet al_Inf_L

23 Occipit al_Mid_L 29 T emporal_Pole_Sup_L

4 Front al_Mid_R

16 P ariet al_Inf_R

24 Occipit al_Mid_R 30 T emporal_Pole_Sup_R

5 Front al_Sup_Medial_L

17 P recuneus_L

25 Occipit al_Inf_L

31 T emporal_Mid_L

6 Front al_Sup_Medial_R

18 P recuneus_R

26 Occipit al_Inf_R

32 T emporal_Mid_R

7 Front al_Mid_Orb_L

19 Cingulum_P ost _L

33 T emporal_Pole_Mid_L

8 Front al_Mid_Orb_R

20 Cingulum_P ost _R

34 T emporal_Pole_Mid_R

9 Rect us_L

35 T emporal_Inf_L 8301

10 Rect us_R

36 T emporal_Inf_R 8302

11 Cingulum_Ant _L

37 Fusiform_L

12 Cingulum_Ant _R

(a)

(b)

Figure 6. Brain effective connectivity models by SBN. (a)
AD. (b) NC.

38 Fusiform_R
39 Hippocampus_L

Intra-/inter-lobe effective connectivity distribution:

40 Hippocampus_R
41 P araHippocampal_L

Aside from having different amounts of effective connectivity at
the global scale, AD may also have a different distribution pattern
of connectivity across the brain from NC. Therefore, we count the
number of arcs in each of the four lobes and between each pair of
lobes in the AD and NC effective connectivity models. The results
are summarized in Table 3. It can be seen that the temporal lobe
of AD has 22.9% less effective connectivity than NC. The
decrease in connectivity in the temporal lobe of AD has been
extensively reported in the literature [29, 2, 30]. The interpretation
may be that AD is featured by a dramatic cognitive decline and
the temporal lobe is responsible for delivering memory and other
cognitive functions. As a result, the temporal lobe is affected early
and severely by AD and the connectivity network in this lobe is
severely disrupted. On the other hand, the frontal lobe of AD has
27.6% more connectivity than NC. This has been interpreted as
compensatory reallocation or recruitment of cognitive resources
[29-32]. Because the regions in the frontal lobe are typically
affected later in the course of AD (our data are mild to moderate
AD), the increased connectivity in the frontal lobe may help
preserve some cognitive functions in AD patients. In addition, AD
shows a decrease in the amount of effective connectivity in the
parietal lobe which has also been reported to be affected by AD.
There is no significant difference between AD and NC in the
occipital lobe. This is reasonable because the occipital lobe is
primarily involved in the brain’s visual function which is not
affected by AD.

42 P araHippocampal_R

6. BRAIN EFFECTIVE CONNECTIVITY
MODELING OF AD BY SBN
FDG-PET baseline images of 49 AD and 67 normal control (NC)
subjects from the ADNI project (www.loni.ucla.edu/ADNI) were
used in this study. After spatially normalizing the images to the
Montreal Neurological Institute (MNI) template coordinate space,
average PET counts were extracted from the 116 brain anatomical
regions of interest (AVOIs) defined by the Automated Anatomical
Labeling [28] technique. We then selected 42 AVOIs that are
considered to be potentially relevant to AD, as reported in the
literature. Each of the 42 AVOIs became a node in the SBN.
Please see Table 2 for the name of each AVOI brain region. These
regions are distributed in the four major neocortical lobes of the
brain, i.e., the frontal, parietal, occipital, and temporal lobes.
We apply SBN to learn a BN for AD and another one for NC, to
represent their respective effective connectivity models. In the
learning of an AD (or NC) effective connectivity model, the value
for needs to be selected. In this paper, we adopt two criteria in
selecting : one is to minimize the prediction error of the model
and the other is to minimize the BIC. Both criteria have been
popularly adopted in sparse learning [12,14,15]. The two criteria
lead to similar findings discovered from the effective connectivity
models, so only the results based on the minimum prediction error
are shown in this section. Specifically, Figure 6 shows the
effective connectivity models for AD and NC. Each model is
represented by a "matrix". Each row/column is one AVOI, . A
black cell at the i-th row and j-th column of the matrix represents
that is a parent of . On each matrix, four red squares are used
to highlight the four lobes, i.e., the frontal, parietal, occipital, and
temporal lobes, from top-left to bottom-right. The black cells
inside each red square reflect intra-lobe effective connectivity,
whereas the black cells outside the squares reflect inter-lobe
effective connectivity. The following interesting observations can
be drawn from the effective connectivity models:

Table 3. Intra – and inter – lobe effective connectivity
amounts
(a) AD

(b) NC

Furthermore, the most significant reduction in inter-lobe
connectivity in AD is found between the frontal and temporal
lobes, i.e., AD has 29.5% less effective connectivity than NC.
This may be attributed to the temporal lobe disruption of the
default mode network in AD [34].

Global-scale effective connectivity:
The total number of arcs in a BN connectivity model, equal to the
number of black cells in a matrix plot in Figure 6, represents the
amount of effective connectivity (i.e., the amount of directional
information flow) in the whole brain. This number is 285 and 329
for AD and NC, respectively. In other words, AD has 13.4% less
effective connectivity than NC. Loss of connectivity in AD has
been widely reported in the literature [34, 38-40].

Direction of local effective connectivity:
One advantage of BNs over undirected graphical models in brain
connectivity modeling is that the directed arcs in a BN reflect
directional effect of one region over another, i.e., the effective
connectivity. Specifically, if there is a directed arc from brain
regions to , it indicates that takes a dominant role in the

936

enabling early detection of network disconnection in prodromal
AD. Future studies may be conducted to verify the statistical
significance of the identified effective connectivity difference
between AD and NC, and estimate the strength of the directed
arcs identified by SBN to help improve the sensitivity and
specificity of the effective connectivity models. Finally, although
this paper focuses on the structure learning of Gaussian BNs, the
same formulation can be adopted to discrete BNs, which will be
explored in our future research.

communication with . The connectivity modes in Figure 6
reveal a number of interesting findings in this regard:
(i) There are substantially fewer black cells in the area defined by
rows 27-42 and columns 1-26 in AD than NC. Recall that rows
27-42 correspond to regions in the temporal lobe. Thus, this
pattern indicates a substantial reduction in arcs pointing from
temporal regions to the other regions in the AD brain, i.e.,
temporal regions lose their dominating roles in communicating
information with the other regions as a result of AD. The loss is
most severe in the communication from temporal to frontal
regions.

Appendix A
There are some notation changes as follows for the convenience
of the derivation: we use
,
,
, …,
,
,
,…,
.

(ii) Rows 31 and 35, corresponding to brain regions
“Temporal_Mid_L” and “Temporal_Inf_L”, respectively, are
among the rows with the largest number of black cells in NC, i.e.,
these two regions take a significantly dominant role in
communicating with other regions in normal brains. However, the
dominancy of the two regions is substantially reduced by 34.8%
and 36.8%, respectively, in AD. A possible interpretation is that
these are neocortical regions associated with amyloid deposition
and early FDG hypometabolism in AD [34-37, 41-42].

Based on Wright’s second decomposition rule [33], the
covariance matrix of all the variables, , can be represented as a
function of the parameters of the BN, , , , i.e.,
T
1

,

,

,

,

1
1

(iii) Columns 39 and 40 correspond to regions “Hippocampus_L”
and “Hippocampus_R”, respectively. There are a total of 33 black
cells in these two columns in NC, i.e., 33 other regions
dominantly communicate information with the hippocampus.
However, this number reduces to 22 (33.3% reduction) in AD.
The reduction is more severe in Hippocampus_L (50%). The
hippocampus is well known to play a prominent role in making
new memories and in recall. It has been widely reported that the
hippocampus is affected early in the course of AD, leading to
memory loss – the most common symptom of AD.

(A-1)

1
1
Now, consider the regression of

on all other variables, i.e.,
. According to the
Least Square criterion, the regression coefficients,
,

,

,

,

,

(A-2)

where
is a sub-matrix of T by deleting the 1st column and 1st
row from . Denote C by A
. Then,

(iv) There are a total of 93 arcs pointing from the left hemisphere
to the right hemisphere of the brain in NC; this number reduces to
71 (23.7% reduction) in AD. The number of arcs from the right
hemisphere to the left hemisphere in AD is close to that in NC.
This provides evidence that AD may be associated with interhemispheric disconnection and the disconnection is mostly
unilateral, which has also been reported by some other papers [4344].

.

(A-3)

by the parameters of the BN.
Our final objective is to express
This can be achieved if we can express
,
,
, ,
by
the parameters of the BN, which is the goal of the following
derivation.
It is known that

7. CONCLUSION
In this paper, we proposed a BN structure learning algorithm,
called SBN, for identifying effective connectivity of AD from
FDG-PET data. SBN adopted a novel formulation that involves
one L1-norm penalty term to impose sparsity on the learning and
another penalty to ensure the learned BN to be a DAG. We
performed theoretical analysis on the competitive advantage of
SBN over the existing algorithms in terms of learning accuracy.
Our analysis showed that the existing algorithms employ a twostage approach in BN structure identification, which makes them
have a high risk of failing to identify the parents of each variable.
Also, we performed theoretical analysis on the time complexity of
SBN, which showed that it is linear in the sample size and
quadratic in the number of variables. Furthermore, we conducted
experiments to compare SBN with 10 competing algorithms and
found that SBN has significantly better performance in learning
accuracy.

,

1

,

(A-4)

st
th
where
, is a matrix by deleting the 1 row and the j column
from
. So, the problem becomes calculation of det
and
det
.
,

(i) Calculation of det
det

∏

: We first show the result:
∑

1

∏

1

,

(A-5)

Next, we will use the induction method in 1)-2) below to prove
(A-5):
1, it is easy to see that (A-5) holds.

1) When

2) Assume that (A-5) holds for
∏

det

We applied SBN to identify the effective connectivity models of
AD and NC from PDG-PET data, and found that the effective
connectivity of AD is different from NC in many ways.
Clinically, our findings may provide an additional tool for
monitoring disease progress, evaluating treatment effects, and

1, i.e.,
∏

∑

1

,

Then we need prove that (A-5) holds for
definition of a determinant,
det

937

∑

1

det

,

1

.

. Based on the
,

(A-6)

is the entry at the 1st row and jth column of

where

Now we need to derive det
due to page limits):

Also, se
1
/
9) and (A-10), we can get:

.

(only results are shown below

,

n

b0MB
b0


se  b0MB  se  b0 

1,

When
det

∏

,

∑

1

∏

1

,

, (A-7)

det

∏

1

,

n

det(Cn )   (1  b )   (b
2
i

i2

2
i

i2

n

n

n

j 1

j 1

k 0
k j

.

1,

Then, insert (A-7), (A-8), and
into (A-6):
n

1

,

n

2
i

i 1

n

n

n

 (1  b )   (b  (1  b ))
2
j

j 2

2
j

k j
k 1

2
k

It is obvious that the part in the “{ }” is less than one. Therefore,
/
/
.


(A-8)
, …,

n

n

Acknowledgments

 (1  b ))   b b  (1  b )
2
j

j i
j 1

 (1  b )
j 2

1,

When

1 . Putting this together with (A-

2 2
0 j

j 1

k 1
k j

2
k

This research is sponsored in part by NSF IIS-0953662 and NSF
MCB-1026710.

  (1  b 2j )   b 2j  (1  bk2 )

8. REFERENCES
[1] Horwitz, B. 2003. The Elusive Concept of Brain
Connectivity, Neuroimage 19, 466-470.

This completes the proof for (A-5).
(ii) Calculation of det
, : det
, has been obtained by (A7) and (A-8). Inserting (A-5), (A-7), and (A-8) into (A-4), we get:
n

n

n

[2] Azari, N., Rapoport, S. 1992. Patterns of Interregional
Correlations of Cerebral Glucose Metabolic Rates in Patients
with Dementia of the Alzheimer Type. Neurodegeneration 1:
101–111.

 (1  b )   (b  (1  b ))
a11 

2
j

j 2
n

2
j

j 2
n

2
k

k j
k 1
n

[3] Friston, K.J. 1994. Functional and Effective Connectivity in
Neuroimaging: A Synthesis. Human Brain Mapping 2, 5678.

 (1  b )   (b  (1  b ))
2
i

i 1

i 1

2
i

k i
k 0

2
k

Furthermore,
1

Inserting this into (A-3), we get
above, we can get:
Plugging in the
n

1

 (1  b )
2
j

j 1

b0MB  b0

n

n

n

[4] Alexander, G., Moeller, J. 1994. Application of the Scaled
Subprofile Model to Functional Imaging in Neuropsychiatric
Disorders: A Principal Component Approach to Modeling
Brain Function in Disease. Human Brain Mapping 2, 79-94.

1 .
/ .

[5] Calhoun, V., Adali, T., Pearison, G. and Pekar, J. 2001.
Spatial and Temporal Independent Component Analysis of
Functional MRI Data Containing a Pair of Task-Related
Waveforms. HumanBrain Mapping 13, 43-53.

(A-9)

[6] McIntosh, A., Bookstein, F., Haxby, J. and Grady, C. 1996.
Spatial Pattern Analysis of Functional Brain Images Using
Partial Least Squares. Neuroimage 3, 143-157.

 (1  b )   (b  (1  b ))
2
i

i 1

2
i

i 1

k i
k 0

2
k

[7] Chiang, J., Wang, Z., and McKeown, M.J. 2009. Sparse
Multivariate Autoregressive (mAR)-based Partial Directed
Coherence (PDC) for EEG Analysis. Proceedings of the
2009 IEEE International Conference on Acoustics, Speech
and Signal Processing: 457-460.

Obviously, the fraction at the right-hand side is between 0 and 1.
| | |.
Therefore, |
/

Next we derive the formula for
/
1
det
/ det
det
/∏
given in (A-5), we can get:

.
1

n

se  b
2

n

MB
0

  n 1 1

. It is known that
Since
and det
is

[8] Huang, S., Li, J., Sun, L., Ye, J., Fleisher, A., Wu, T., Chen,
K. and Reiman, E. 2010. Learning Brain Connectivity of
Alzheimer’s Disease by Sparse Inverse Covariance
Estimation, NeuroImage, 50, 935-949.

 (1  b )
2
i

[9] Hilgetag, C., Kotter, R., Stephan, K.E. and Sporns, O. 2002.
Computational Methods for the Analysis of Brain
Connectivity. In: Ascoli, G.A. (Ed.), Computational
Neuroanatomy. Humana Press, Totowa, NJ.

i 0

n

n

n

i 1

i 1

k i
k 0

 (1  bi2 )   (bi2  (1  bk2 ))

n

n

(A-10)

 (1  b )   (b  (1  b ))


2
j

j 2
n

j 2
n

2
j

k j
k 1
n

[10] Bullmore, E., Horwitz, B., Honey, G., Brammer, M.,
Williams, S. and Sharma, T. 2000. How Good is Good
Enough in Path Analysis of fMRI? Neuroimage 11, 289–301.

2
k

 (1  b )   (b  (1  b ))
i 1

2
i

i 1

2
i

k i
k 0

2
k

[11] Friston, K.J., Harrison, L. and Penny, W. 2003. Dynamic
Causal Modeling. Neuroimage 19, 1273-1302.

938

AD: A Resting-State fMRI Study. Human Brain Mapping
28, 967-978.

[12] Lam, W. and Bacchus, F. 1994. Learning Bayesian Belief
Networks: an Approach based on the MDL Principle,
Computational Intelligence 10, p.269–293.

[31] Gould, R., Arroyo, B. Brown, R., Owen, A., Bullmore, E and
Howard, R. 2006. Brain Mechanisms of Successful
Compensation during Learning in AD, Neurology 67 (6),
1011-1017.

[13] Friedman, N., Nachman, I. and Pe’er, D. 1999. Learning
Bayesian Network Structure from Massive Datasets: The
“Sparse Candidate” Algorithm, UAI 1999.

[32] Stern, Y. 2006. Cognitive Reserve and Alzheimer Disease,
Alzheimer Disease Associated Disorder 20, 69-74.

[14] Schmidt, M., Niculescu-Mizil, A. and Murphy, K. 2007. A.
Learning Graphical Model Structures using L1Regularization Paths, AAAI 2007.

[33] Korb, K.B. and Nicholson, A.E. 2003. Bayesian Artificial
Intelligence. Chapman & Hall/CRC, London, UK.

[15] Tibshirani, R. 1996. Regression Shrinkage and Selection via
the Lasso, Journal of Royal Statistical Society, Series B,
58(1):267–288.

[34] Greicius, M., Srivastava, G., Reiss, A. and Menon, V. 2004.
Default-mode Network Activity Distinguishes AD from
Healthy Aging: Evidence from Functional MRI, PNAS. 101,
4637–4642.

[16] Tsamardinos, I., Brown, L. and Aliferis, C. 2006. The MaxMin Hill-Climbing Bayesian Network Structure Learning
Algorithm, Machine Learning, 65(1), 31-78.

[18] Pellet, J. and Elisseeff, A. 2008. Using Markov Blankets for
Causal Structure Learning, JMLR 9, 1295-1342.

[35] Alexander, G.E., Chen, K., Pietrini, P., Rapoport S. and
Reiman, E. 2002. Longitudinal PET Evaluation of Cerebral
Metabolic Decline in Dementia: A Potential Outcome
Measure in AD Treatment Studies. Am.J.Psychiatry 159,
738-745.

[19] Meek, C. 1995. Causal inference and causal explanation with
background knowledge. UAI 1995.

[36] Braak, H., Braak, E., 1996. Evolution of the Neuropathology
of Alzheimer's Disease. Acta Neurol Scand Suppl 165, 3-12.

[20] Estrada, E. and Naomichi, H. 2008. Communicability in
Complex Networks. Phys. Rev. E 77 036111.

[37] Braak, H., Braak, E., Bohl, J., 1993. Staging of AlzheimerRelated Cortical Destruction. Eur Neurol 33, 403-408.

[21] Gill, P.E. Class Notes for Math 271ABC: Numerical
Optimization. Department of Mathematics, UCSD.

[38] Hedden, T., Van Dijk, K. Becker, J., Mehta, A., Sperling, R.,
Johnson, K. and Buckner, R. 2009. Disruption of Functional
Connectivity in Clinically Normal Older Adults Harboring
Amyloid Burden. J. Neurosci. 29, 12686–12694.

[17] Margaritis, D. and Thrun, S. 1999. Bayesian Network
induction via local neighborhoods. NIPS 1999.

[22] BERTSEKAS, D. 1999. Nonlinear Programming, 2nd
Edition, Athena Scientific, Belmont.

[39] Andrews-Hanna, J., Snyder, A., Vincent, J., Lustig, C., Head,
D., Raichle, M and Buckner, R. 2007. Disruption of LargeScale Brain Systems in advanced Aging, Neuron 56, 924–
935.

[23] Fu, W. 1998. Penalized Regressions: the Bridge versus the
Lasso, Computational and Graphical Statistics, 7 (3), 397416.
[24] Friedman, J.; Hastie, T.; Hofling, H. and Tibshirani, R. 2007.
Pathwise Coordinate Optimization, The Annals of Applied
Statistics 2, 302-332.

[40] Wu, X., Li, R. Fleisher, A.S., Reiman, E., Guan, X., Zhang,
Y., Chen, K. and Yao, L. 2011. Altered Default Mode
Network Connectivity in Alzheimer’s Disease - A Resting
Functional MRI and BN Study, Human Brain Mapping, in
press.

[25] Aliferis, C., Tsamardinos, I. and Statnikov, A. 2003. HITON,
a Novel Markov Blanket Algorithm for Optimal Variable
Selection. AMIA 2003, 21-25.

[41] Ikonomovic, M., Klunk, W. Abrahamson, E. Mathis, C.
Price, J., Tsopelas, N., Lopresti, B. et al. 2008. Post-mortem
Correlates of in vivo PiB-PET Amyloid Imaging in a Typical
Case of Alzheimer's Disease. Brain 131, 1630-1645.

[26] Tsamardinos, I. and Aliferis, C. 2003. Towards Principled
Feature Selection: Relevancy, Filters and Wrappers. AISTAT
2003.

[42] Klunk, W., Engler, H., Nordberg, A. et al. 2004. Imaging
brain amyloid in Alzheimer's disease with Pittsburgh
Compound-B. Ann Neurol. 55, 306-319.

[27] http://www.cs.huji.ac.il/labs/compbio/Repository.
[28] Tzourio-Mazoyer, N., Landeau, B., Papathanassiou, D.,
Crivello, F., Etard, O., Delcroix, N., Mazoyer, B. and Joliot,
M. 2002. Automated Anatomical Labeling of Activations in
SPM using a Macroscopic Anatomical Parcellation of the
MNI MRI Single-Subject Brain. Neuroimage,15:273-289.

[43] Reuter-Lorenz, P. and Mikels, J. 2005. A Split-Brain Model
of Alzheimer’s Disease? Behavioral Evidence for
Comparable intra and interhemispheric Decline.
Neuropscyhologia 43, 1307-1317.

[29] Supekar, K., Menon, V., Rubin, D., Musen, M and Grecius,
M. 2008. Network Analysis of Intrinsic Functional Brain
Connectivity in AD. PLoS Comput Biol 4(6) 1-11.

[44] Lipton, A., Benavides, R., Hynan, L.S., Bonte, F.J., Harris,
T.S., White, C.L. 3rd, Bigio, E.G. 2004. Lateralization on
Neuroimaging does not Differentiate Frontotemporal Lobar
Degeneration from Alzheimer’s Disease. Dement Geriatr
Cogn Disord 17(4), 324-327.

[30] Wang, K., Liang, M., Wang, L., Tian, L., Zhang, X., Li, K.
and Jiang, T. 2007. Altered Functional Connectivity in Early

939

496

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 55, NO. 3, AUGUST 2008

A Price-Based Negotiation Mechanism
for Distributed Collaborative Design
Sandipan Ganguly, Teresa Wu, and Jennifer Blackhurst

Abstract—With an increased focus on collaborative design for
success in global markets, there is a need for a negotiation mechanism for designs distributed over autonomous stakeholders. This
paper utilizes the unit price of the design variables as a medium for
negotiation between autonomous and self-interested design participants represented by design agents (DAs). An artificial agent—the
facilitator agent (FA), maximizes the overall utility of the design
as a function of the prices of the design variables, and guides the
design toward global feasibility. By relating the FA’s price space to
the DAs’ design space through price-variable curves, a negotiation
platform is developed and tested.
Index Terms—Bilevel optimization, collaborative product design, design negotiation, distributed optimization, multiattribute
utility, multidisciplinary design optimization.

I. INTRODUCTION
HERE HAS been an increasing demand from complex
product markets for suppliers to deliver a high variety of
customized products quickly in a global environment [13]. This
has led firms to focus on design process flexibility as well as
the rapid delivery of innovative products [15]. As a result, many
successful firms focus on collaborative design to improve product quality, reduce costs, and decrease time to market. Models in distributed collaborative design optimization have used
multiobjective optimization, game theory, constraint programming, and bilevel optimization techniques. A major limitation
to multiobjective optimization methods is the requirement of
full knowledge of the entire design space. In a real distributed
design environment, the likelihood for any autonomous design
entity to have complete knowledge of the entire design space
is very low. Game-theory-based techniques can have computational difficulties in calculating the “best reply correspondence”
and the rational reaction sets when designs are complex. Also,
it may not be practically possible to explicitly characterize the
relationships between design entities as noncooperative, cooperative, or leader-follower. Furthermore, the negotiation mechanism in game-theory-based design has a dependency on the design methodology used by participating players. While, bilevel
techniques have been used extensively for cooperative situa-

T

Manuscript received August 1, 2006; revised July 1, 2007, December 1, 2007,
and January 1, 2008. Review of this manuscript was arranged by Department
Editor A. Marucheck.
S. Ganguly is with Expedia, Inc., Bellevue, WA 98005 USA (e-mail:
sganguly@expedia.com).
T. Wu is with the Industrial Engineering Department, Arizona State University, Mesa, AZ 85212 USA (e-mail: teresa.wu@asu.edu).
J. Blackhurst is with Iowa State University, Ames, IA 50011 USA (e-mail:
jvblackh@iastate.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TEM.2008.922629

tions in a real world collaborative environment, the importance
for a noncooperative situation with parties having conflicting
objectives is increasing. In fact, Alexandrov and Lewis [1] explore the analytical and computational properties of these techniques and conclude that discipline-level autonomy often causes
computational and analytical difficulties, which result in severe
convergence problems.
This research addresses limitations of previous work through
the development of a price-based negotiation (PBN): a generalized bilevel, bispace optimization mechanism for distributed
collaborative design. The proposed mechanism eliminates the
computational and analytical difficulties of previous bilevel
methods by introducing the price space (see Section III-D1).
The “price” in PBN refers to the unit price of the design variables assigned by the designer. The unit price is defined as the
proportional amount of the product’s price that can be attributed
to individual design variables. The idea of unit price for design
variables was first introduced by Parunak et al. [22]. Each design variable has a qualitative curve that the designer specifies,
showing how the price varies as a function of the design variable. (Section III-C elaborates more on the prices of the design
variables.) The PBN mechanism introduces two types of agents:
the facilitator agent (FA) that monitors and controls the iterative
negotiation process and the design agents (DAs) that represent
the participants in the collaboration. At the system level, the FA
uses multiattribute utility theory on the price space of the design variables to maximize the utility of the overall design and
to adjust the prices of the variables according to the constraint
violations. Changes in the prices are mapped into changes to
the feasible regions of the DAs. At the lower discipline level,
the DAs optimize their own objectives within their own feasible
regions and generate a candidate design point. Thus, bilevel and
bispace optimization is achieved.
The research contributions of the PBN mechanism are fourfold. First, PBN is a generalized bilevel, bilevel optimization
mechanism. Since the relationship between the DAs is not
considered, PBN supports collaborative design in the context of
both cooperative and noncooperative situations. Second, PBN is
independent of the design methodologies used by the DAs. That
is, the mechanism does not have to be altered if the DA changes
its solution strategy. Third, PBN reduces complexity in the
design stage by not requiring any agent to have full knowledge
of the entire design space. Since the bilevel optimization is
in two different spaces, namely, price and design, the DAs
do not have to reveal their entire design space. Fourth, PBN
offers a highly flexible negotiation platform, which can be
used to test various rules and collaboration attitudes. By
tuning simple parameters, participants can test various risk

0018-9391/$25.00 © 2008 IEEE

GANGULY et al.: PRICE-BASED NEGOTIATION MECHANISM FOR DISTRIBUTED COLLABORATIVE DESIGN

and collaboration natures. Additionally, by separating the
system-level optimization from the lower-level discipline-level
optimization using the two agent types, we ensure that the
Lagrange multipliers do exist at any solution point of the price
space optimization. Using the price space (derived from the
design space) offers an effective way to evaluate alternative
designs in the early stages of design development.
The remainder of the paper is organized as follows. Section II
provides a literature review of collaborative design models. Section III presents the detail of the PBN model. In Section IV, comparison experiments with other algorithms to test to validity and
usefulness of PBN are shown, as well as an example industry
application to illustrate the efficacy of PBN in a real-world setting. Finally, conclusions, limitations, and future research are
presented in Section V.
II. LITERATURE REVIEW
Research in decision support for collaborative design primarily deals with design methodologies, design negotiation and constraint management techniques, design rationale, design space
exploration, and product conceptualization. In general, there are
four main categories: decision-based design, distributed problem solving, set-based reasoning, and engineering as collaborative negotiation. Each of these categories is presented in turn.
The embodiment of decision-based design [6], [10], [18],
[20], [30], [38], [39] is the decision support problem that provides a means to model the decisions encountered in design and
aims at finding “satisficing” solutions [28], [29]. Decision-based
design recognizes the roles that decisions play in design and can
select the preferred method in a rigorous manner [38], [39]. Research in decision-based design include the use of adaptive linear programming if continuous variables are used and foragingdirected adaptive linear programming in cases of mixed (discrete
and continuous) variables [16]. Other applications in a decisionbased design context include use of discrete choice analysis for
demand modeling [38], [39] and the development of a decisionbased approach for reliability design [20]. In addition, there
has been research ranging from single-objective decision-based
design [38] to multiobjective models [35].
The distributed problem solving paradigm considers the entire collaborative design process as a special consolidated problem and solves it using various optimization or satisfying techniques. Sobieszczanski-Sobieski [32] introduces optimization
by linear decomposition and hierarchical linear decomposition,
where the disciplines are given the autonomous task of minimizing discipline-level design infeasibility, while maintaining system-level consistency. The system-level problem is to
drive the design infeasibility to zero. Braun and Kroo [3] introduce collaborative optimization where the disciplines are given
the autonomous task of minimizing system-level inconsistency,
while maintaining discipline-level design feasibility. In both approaches, optimization of the system-level objective, subject to
interdisciplinary consistency, is performed in the system-level
problem. Cooperative environments can be modeled as regular
multiobjective optimization problems that seek a Pareto efficient
solution as in Petrie [26]. Other methods in cooperative product

497

design include distributed constraint satisfaction [33], [34] and
distributed multidisciplinary optimization [27].
The MarCon algorithm [23]–[25] applies set-based reasoning, which considers ranges of design parameters instead of
point values. Design parameters’ domains shrink monotonically over time from their initial full extents, collapsing on the
solution to the problem. The MarCon algorithm bridges two
important areas of research in multiagent systems for product
design: distributed constraint optimization and market-based
programming.
Engineering as collaborative negotiation aims to capture all
aspects of collaboration including both technical and nontechnical factors [17]. Its foundations are rooted in a wide range
of subjects, including philosophy, psychology, sociology, management science, and decision sciences. It recognizes that engineering tasks can be formulated as a min–max problem that
satisfies minimum functional requirements (guided by the natural sciences) and maximizes a human purpose profit (guided by
sciences of the artificial).
Though extensive research has been done in collaborative
design models, limitations exist. For example, Lu and Cai [17]
concentrate on cooperative relationships only. Decision-based
design depends heavily on the design methodology used by
design participants as well as explicit characterization regarding
the extent of cooperation between the design participants [5],
[10]–[12], [30]. Additionally, some models require full
knowledge of the design space and are computationally
complex. Therefore, there is a need to develop a collaborative
design negotiation platform, which is both generalizable and
flexible in noncooperative, and cooperative situations does not
require full knowledge of the design space and requires less
computing effort. A PBN mechanism is proposed to achieve
these goals. PBN adds value in terms of proposing a new
bilevel priced-based optimization methodology, which provides
optimal solutions in cooperative and noncooperative situations
without requiring information sharing between design entities.
PBN is independent of the design methodologies used by
the DAs and reduces complexity and information sharing
requirements in the design stage by not requiring any DA to
have full knowledge of the entire design space.
III. PBN MODEL
In this section, the PBN model is presented. A protocol defining the step-by-step use of PBN may be found in Appendix A
for those interested readers.
A. Definitions and Assumptions
The PBN mechanism is developed to provide solution to
distributed collaborative engineering design in the context of
both cooperative and noncooperative environments. To achieve
this, some definitions and assumptions have been made.
Definition 1: The DA refers to a stakeholder in the collaboration comprised of a team of designers and their associated software tools. Usually DAs are domain experts that are responsible
for a portion of the overall design. Each DA is self-interested
(e.g., seeks to optimize its own objectives) and rational. Note

498

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 55, NO. 3, AUGUST 2008

that if a DA wishes to share information with other design
agents, it is free to do so, in which case, the two agents would
combine into a single agent, thereby supporting cooperative and
noncooperative environments.
Definition 2: The FA is an artificial agent referring to the main
stakeholder of the design collaboration. The FA is responsible
for maximizing the return on investment from the collaboration
and ensuring the product meets feasibility conditions and customer requirements. The FA does not contribute to the design,
but rather evaluates the design. The FA has knowledge of system (global) constraints. The negotiation is iterative and each
stage involves a compromise by some DAs to satisfy the system
(global) constraints.
Assumption 1: Each design variable has a qualitative curve
that the designer specifies, showing how the price varies as
a function of the variable. In this research, the price-variable
curves are assumed to be either monotonic or convex.
Assumption 2: The prices of design variables are utility independent. That is, even though the values of the design variables
are interdependent, their prices are independent.
B. Price of Design Variables
As discussed before, each design variable is associated with
a price, which refers to the proportional amount of the product’s price that can be attributed to individual design variables.
See Parunak et al. [22] for a detailed discussion on the pricevariable space for ordinal, nominal, and shape variables. Where
actual cost data are available for some attributes such as weight
or torque, the price-variable curve can be directly generated.
Where data are not present, the designers allocate budgets to
components (smaller units that comprise the whole design) and
use design decomposition techniques to assign prices to the
variable values. For instance, the responsible agents for product
process integrated design (RAPPID) project [21], [22] broke
down the design of container cargo ship into structural and
functional areas to generate the price-variable curves for the required design variables. Singer and Parson [31] use fuzzy logic
to generate crisp value functions that indicate the preference of
a design variable at every point in its range. In this research, the
price-variable curve concept is employed, and it is assumed that
each design variable has a corresponding price-variable curve
that is known only to the DA responsible for that variable. Note
that it is possible that there are different price-variable curves for
the variable belonging to different agents to represent the same
design attribute (for example, the same physical dimension).
C. Multiattribute Utility Theory
The application of utility analysis to conceptual design has
been long recognized by researchers. Thurston [36] provides details of the application of multiattribute utility theory, and illustrates its superiority over weighted average methods. Thurston
et al. [37] use a lottery method described by Keeney and
Raiffa [14] for the optimization of design utility. The multiplicative form of utility used in Thurston et al. [37] to evaluate
the merit of the overall collaborative design is used in this research. However, instead of using design variable values as the
attributes, the prices of the variables are used. Since optimiza-

tion of the overall design utility involves all components, this
task is entrusted to the FA overseeing the entire design process.
Each design variable represents an attribute of the design,
where the assigned unit price of the variable (the proportional
amount of the product’s price that can be attributed to individual
design variables) is designated by the designer. We assume a
measure of the overall utility of the design is the effectiveness
of the design in the marketplace and that the price of the product
is a determinant in market performance. Therefore, we propose
that the utility of a design can be expressed as a function of price
as shown in Lemma 1.
Lemma 1: The single-attribute utility achieved by a DA
through any design variable can be represented as a monotonically nondecreasing function of the unit price of the design
variable.
Assuming that a higher price of a variable is preferable for
a DA, any descending sequence of price p0i > p1i > · · · > pni
forms a weak order p0i  p1i  · · ·  pni , where  means “is
preferred to.” From the fundamental theorem of utility [8], it
follows that ∃ is a function U (p∗i ) such that pji  pli ⇔ U (pji ) 
U (pli ). Given pji the unit price and U (pji ) the utility function for
design variable i at stage j, which represents the desirability or
the level of satisfaction of the variable. U (Pj ), the overall utility
function, represents the total desirability or level of satisfaction
of the whole design. Using Keeney and Raiffa’s [14] multiplicative form of total utility and assuming utility independence of
the prices of design variables, the overall utility of the design at
stage j is defined as:


 m

1
j
j
j
(Kki U (pi ) + 1) − 1
(1)
U (P ) =
K
i=1
where m is the number of design variables and kij is the single
attribute scaling constant that sets the value of U (pji ) in the
range [0,1], kij is 1 when U (pji ) ranges between [0,1], K is
normalizing constant derived from:
K=

m


(1 + Kkij ) − 1.

(2)

i=1

In this research, we model the utility function as “desirability
functions” originally used by Myers and Montgomery [19] for
optimizing multiple responses using response surfaces. That is,
the utility function of design variable i with price pji can be
given by:

0,
when pji <px i m i n



	

r


pji − px i m i n
j
,
when px i m i n ≤pji ≤px i m a x
U (pi )=
p
−
p

x
x
im ax
i m in




1,
when pji >px i m a x
(3)
where xi m a x and xi m i n are the maximum and minimum values
of design variable i, respectively, px i m a x and px i m i n are the unit
prices of design variables i at xi m a x and xi m i n , respectively, r is
a nonnegative risk parameter used to tune the risk nature of the
design variable. The DA is risk averse in increasing the price of
the variable when 0 < r < 1, risk prone when r > 1, and risk

GANGULY et al.: PRICE-BASED NEGOTIATION MECHANISM FOR DISTRIBUTED COLLABORATIVE DESIGN

Fig. 1.

Utility curves as a function of price for variable i at stage j.

neutral when r =1. Fig. 1 shows the shape of the utility function.
The value of r is assigned by the DA and is based on its expert
opinion. If expert opinion is not available a number of possible
scenarios can be explored by changing the value of r.
D. Negotiation Mechanism
As shown in Table I, the negotiation mechanism starts with
each DA providing the FA with information regarding pricel
l
l
l
variable curves in the form of (Xim
in , px i m i n ), (Xim ax , px i m a x ),
as well as the risk nature of each variable with respect to collaboration. If the price-variable relation is convex, then the DA
provides the coordinates of the minima so that the price-variable
curve can be divided into two monotonic curves. The DAs run
their respective optimization modules [e.g., Linear Programming (LP) models and Neuro-Linguistic Programming (NLP)
models], and provide the initial values of the design variables
[step 0(c)] that the FA uses to determine the system-wide constraint values. Note that PBN will stop if there is no constraint
violation given the proposed solution from each designer (step
1), otherwise, step 2 will be enabled to explore how the price
should be adjusted. In step 2, in order to maximize total utility
that is the multiplicative form of single utilities and to force the
design to feasibility, the FA imposes changes in the price values
of variables involved in system constraint violations (see Section III-D2). Note that the PBN may stop if the price cannot be
changed anymore and there exists system constraint violation.
In this case, we conclude that PBN cannot generate a feasible solution. In step 3, the changes in the prices translate into
changes in the feasible ranges of the variables (see Section IIID3). The DAs rerun their respective optimization modules and
provide a new set of design variables based on the new feasible
region (step 4). This marks the beginning of a new stage in the
negotiation process.
1) Bilevel, bispace optimization: The PBN is a bilevel optimization mechanism that is conducted in two different spaces:
price space and design space. The first level of optimization in
the design space level is at the discipline level used by the DAs.
The second level of optimization in the price space is at the system level used by the FA. Alexandrov and Lewis [1] show how
computational difficulties are often associated with bilevel optimization techniques (such as in collaborative optimization and
optimization by linear decomposition) on the design space. The
difficulties are introduced due to the fact that the Lagrange multipliers do not exist unless the solution is also an unconstrained

499

stationary point, which happens very rarely. Therefore, bilevel
algorithms that use Karush Kuhn Tucker (KKT) conditions for
convergence face considerable computational difficulties. In this
research, by separating problem spaces of the system-level optimization (used by the FA) from the discipline-level optimization
(used by the DAs), the conditions that cause the Lagrange multipliers to vanish are eliminated. The Jacobian of the price space
problem does not vanish at a feasible design space. This ensures that the Lagrange multipliers do exist at any solution point
of the price space optimization. Additionally, due to the fact
that most large collaborative design problems have complex design spaces that may not be completely known, using the price
space (which is derived from design space) offers a simpler
way to evaluate alternative designs in the early stages of design
development. That is, alternative designs with different design
parameter settings can be evaluated in the design optimization
problem and evaluated in the system optimization problem after
being transformed to the price space.
2) Calculation of price change: The FA determines the price
changes by solving a price space optimization problem. To incorporate compromise in the objective of the FA, we include
a penalty function that moves the prices in the direction of increasing feasibility. The penalty function approach has been
used extensively in engineering design and structural optimization (for example, see [9] and [4]).
At stage j:
Max : U (P j ) − [µj ][P j ]T ,
subject to : Pm in ≤ P j ≤ Pm ax

(4)

where U (P j ) is total utility. [µj ]∗ [P j ]T is the compromise term
based on the constraint violation and [µj ] is the compromise
vector that can be represented by
[µj ] = [µj −1 ] + [∆µj ]
where [∆µj ] = αj [Gj ]T CH

(5)

αj is a nonnegative scaling constant to control the compromise
for each variable in each agent.
Assume that there are m design variables, n systemlevel constraints. Given each constraint is of the form
gk (xj1 , . . . , xji , . . .) ≤ 0, [Gj ] is a vector that weights
the compromise term by the magnitude of the systemlevel constraints violations, and is given by [Gj ]T =
[Max(0, g1 (xj1 , . . . , xjm )), Max(0, g2 (xj1 , . . . , xjm )), . . . , Max
(0, gn (xj1 , . . . , xjm ))]. C is an n × m index matrix, which ensures that the right variable is appropriately changed (increased
or decreased) in order to reduce constraint violation (6). Matrix
H is an m × m rate matrix with the diagonal terms being used
to determine the direction and rate of change of price with
respect to xi variable. hii >= 0 indicates that the higher the
xi , the higher pxi . hii < 0 indicates that the higher the xi , the

500

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 55, NO. 3, AUGUST 2008

TABLE I
PBN MECHANISM

lower the pxi



1,







cik = −1,








 0,

if
if
if

∂gk (Xj )
∂xji
∂gk (Xj )
∂xji
∂gk (Xj )
∂xji


 (px i m a x − px i m i n ) ,
hik = (xim ax − xim in )

0,

>0
<0 ,

i=1, . . . , m, k=1, . . . , n

=0
(6)
if i = k, i = 1 . . . m, k = 1 . . . n
if i = k, i = 1 . . . m, k = 1 . . . n.

(7)
3) Updating the design space: Once the FA determines how
much to increase or decrease the price, the changes are mapped
to the design space by updating the feasible region. Assume that
one DA has nondecreasing price-variable function for design
variable i. Fig. 2(a) and (b) illustrates how the increasing of
the price and the decreasing of the price are used to change the
feasible region of the variable.
As shown in Fig. 2, the increase of the price updates the
lower bound of the design variable, while the decrease of the
price updates the upper bound of the design variable. Thus, the
design space of each DA is updated accordingly. Note that the
change of price induces the desired change in feasible region as
long as the price-variable curves are monotonic or convex.
4) Convergence of the negotiation mechanism: This model
ensures that the prices always change in the direction of reducing
constraint violation. Given [∆µj ] = αj [Gj ]T CH, let for any
variable xi in constraint gk :
Case 1: If cik = 1 and hii < 0, then ∆µji < 0, µji is decreasing,
which leads to the increasing of pji . The higher the price pji
is, the lower the xi , reducing constraint violation.
Case 2: If cik = 1 and hii ≥ 0, then ∆µji ≥ 0, µji is increasing,
which leads to the decreasing of pji . The lower the price pji
is, the lower the variable xi , reducing constraint violation.

Case 3: If cik = −1 and hii < 0, then ∆µji ≥ 0, µji is increasing, which leads to the decreasing of pji . The lower the price
pji is, the higher the variable xi , reducing constraint violation.
Case 4: If cik = −1 and hii ≥ 0, then ∆µji < 0, µji is decreasing, which leads to the increasing of pji . The higher the price
pji is, the higher the xi , reducing constraint violation.
Note that the upper and lower bounds of price correspond
to upper and lower bounds of the design variables. If a feasible
design exists, then there would not be an out-of-bound situation.
If there is no feasible design, i.e., the systems constraints are
violated, then the prices cannot change any further and the
collaboration fails. In other words, if it reaches the bound, that
is the last acceptable solution. This is embedded into the model
through the stopping criteria (step 2 in Table I). In addition,
step 2 in the PBN can be shown to converge in the price space.
The modified objective function can be given by:
F (P) = U (P) − µj PT

(8)

from KKT conditions
∇F (P j ) =

m





λjk ∇gk (P j )

(9)

k =1

where λjk are the Lagrange multipliers.
Given

  
∇F (P j ) = ∇U (P j ) − µj




∂U (P j )
∂g1 (P j )
 j 
 ∂P j 
 ∂P j 
µ1




 . . .1  =  . . .  + λj  . . .1 
1



 ∂U (P j ) 
 ∂g1 (P j ) 
µjm
∂Pmj

∂Pmj
 ∂g (P j ) 
m
 ∂Pmj



...
+ · · · + λjm 
.
 ∂g (P j ) 
m
∂Pmj

(10)

GANGULY et al.: PRICE-BASED NEGOTIATION MECHANISM FOR DISTRIBUTED COLLABORATIVE DESIGN

Fig. 2.

501

Change of price inducing the change of variable feasible range.

Assume that the optimal solutions at stage j and j–1 are Pj
and Pj −1 , then
 


∂U (P j −1 )
∂U (P j )
 j

 ∂P j   ∂P j −1 
µ1 − µj1−1
 


1
1
=
 ...  − 
 + φ (11)
...
...
 


j
j −1
 ∂U (P j )   ∂U (P j −1 ) 
µm − µm
∂Pmj
∂Pmj −1
where φ represents the remaining terms.
Given a small distance , let ||Pj − Pj −1 || < ε, let
 j

µ1 − µj1−1


...
µjm − µjm−1

be an arbitrarily large vector, which means the difference in
gradient of the curve U(P) can be made large within a small
distance ε.
This implies that U (P) is discontinuous between Pj and
j −1
P , which is a contradiction because from the definition of
U (P) in (1),
Hence, after a finite number of

 it is continuous.
iterations, Pj − Pj −1  > ε (step 2 of the algorithm) convergence in the price space is achieved. From the theorem for
convergence of algorithms with composite maps [2] that states:
“If there are two algorithmic maps B and C, B converges to
a solution and C is a process where the decent (ascent) function does not increase (decrease), the composite map BC converges to a solution,” we conclude that the PBN negotiation
mechanism converges. In PBN, the algorithmic map B refers
to the maximization problem solved by the FA, which converges (step 2.3 ensures the convergence). Let the descent function be the summation of all violated constraint values, that
is, Σmax(0, gk (xj1 , . . . , xjm )), k = 1, . . . , n. As prices always
change in the direction of reducing constraint violation, the
function is a decent function. With map C, as the disciplinelevel optimization is run (step 4), the price changes reflect on
the design space in a manner that only reduces the constraint
violation. Hence, the decent function keeps reducing or does not
increase. Thus, using the convergence of composite maps, the
proposed PBN converges.
IV. COMPARISON EXPERIMENTS AND
EXAMPLE INDUSTRY APPLICATION
In this section, comparison experiments with other algorithms
are conducted to test the validity and usefulness of PBN. In ad-

TABLE II
COMPARISON OF RESULTS

dition, an example industry application is presented to illustrate
the efficacy of PBN in a real-world setting.
A. Comparison Experiments
To compare the results of PBN with other algorithms used
for collaborative design, a simple example is adopted from Fernandez et al. [7]. The example assumes that the objectives of
two independent agents can be represented by quadratic functions. Results of PBN are compared with seven other different
algorithms including game theoretic protocols in Table II. In
the example, there are two designers: A and B. A’s goal is to
minimize FA and B’s goal is to maximize FB . The desired
performance of Designer A is an FA of 130 or lower, while
Designer B is an FB of 290 or higher.
FA =Min(2.5x21 + x1 x22 − 30x1 + 200)
FB =Max(−5x22 − x1 x2 + 50x2 + 200),

x1 , x2 ∈ [0, 10].

Assume that the price-variable curves for design variables x1 ,
x2 for designers A and B are given as in Fig. 3. Regarding x1 ,
designer A prefers the value to be 6, thus the price increases
when x1 changes from 0 to 6 and decreases when x1 changes
from 6 to 10. As for x2 , designer A prefers 0, the price decrease
when x2 changes from 0 to 10. Designer B prefers 0 for x1 ,
therefore the price monotonically decreases as x1 changes from
0 to 10. Designer B prefers 5 for x2 , therefore the price for
x2 increases as x2 changes from 0 to 5 and decreases as x2
changes from 5 to 10. Designer A will first inform the FA of
two monotonic ranges of x1 : [0, 6] and [6, 10], and designer B
will first inform the FA of two monotonic ranges of x2 : [0, 5]
and [5, 10]. Risk parameters are assumed to be 1 (that is, risk
neural for all the design variables). PBN results are shown in
Table II and illustrated Fig. 4.

502

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 55, NO. 3, AUGUST 2008

Fig. 3.

Price-variable curves (x-axis: variable; y-axis: price).

Fig. 4.

(a) Objectives of A and B. (b) Comparison.

The percent compromise for any agent is defined as the percentage of its ideal objective value (the value it would have
if it had complete control, i.e., objective value with no compromise) it has to sacrifice in order to come to the present
solution. For example, the percent compromise for agent A
when agent A is in complete control is 0, and is 100 when
agent B is in complete control. This definition normalizes
the compromises of each agent by their range of objective
values.
Fig. 4(a) shows how the objectives of the two agents differ.
Fig. 4(b) overlays the contour plots and shows the eight solutions
given in Table II (where each algorithm solution is numbered
on the figure). From Fig. 4(b), it is apparent that PBN is close
to the Nash solution. This example illustrates that PBN produces comparable solutions without having the computational
and analytical problems of existing techniques.
B. Example Industry Application
Next, in order to illustrate the application and use of the PBN
mechanism in an industry setting, we model the design of a
radiator support bracket made by a tier-one automotive supplier. The supplier is small Iowa-based automotive stamping
manufacturer. Recently, the supplier has experienced increased
pressure from the automotive manufacturer to reduce the weight
of the part due to requirements for increased fuel mileage without sacrificing the strength of the part within specified cost

constraints. Therefore, alternative materials with varying wall
thicknesses and inner wall radii are being investigated for the
radiator support design: Aluminum 6063 alloy tubing with 0.25
in walls and two thicknesses of steel 1040 alloy tubing with
0.25 or 0.1875 in walls. The inner wall radius of the tubing
may vary from 0.50 to 1.0 in. Constraints for the design include
the part must have a tensile strength of at least 35 000 lbs/in2 ,
the material for the part must cost no more than $30.00 (the
lower the better due to increased pressure for cost reduction
by the automotive manufacturer), and the overall weight of the
part must not exceed 5 lb (again, the lower the better for improved fuel mileage. Regardless of the material used, 5-ft length
of material (metal tubing) is required. The three alternatives
being investigated are listed in Table III with their associated
characteristics.
In analyzing these three design alternatives using PBN, two
DAs, cost agent and weight agent, are used. Specifications for
each agent are listed in Table IV. Assume that the cost agent
aims to minimize the cost, while the weight agent aims to reduce
the weight. Two mixed integer nonlinear programming models
are developed to decide on the values of the design variables.
We also assume that all price-variable functions are linear, as
shown in Figs. 5 and 6.
Since this design problem involves distinct choices in material, discrete variables are used to indicate the selection of
material. The price-variable curve for binary variables would
essentially have two points: the price of choosing it and the

GANGULY et al.: PRICE-BASED NEGOTIATION MECHANISM FOR DISTRIBUTED COLLABORATIVE DESIGN

503

TABLE III
MATERIAL ALTERNATIVES

TABLE IV
COST AND WEIGHT AGENT SPECIFICATIONS

Fig. 5.

Price-variable curves for binary values (Choice on Al, steel 1, steel 2).

price of not choosing it. We will assume that the price of not
choosing a material is zero. Since both agents have minimization problems, we calculate the price (or value) of selecting a
material as the reciprocal of the objective function when the
material is chosen at the lowest possible value of radius (0.5 in,
in this example). Thus, for the weight agent, the material that
has the lowest weight has the highest utility. Similarly, for the

cost agent, the material with the lowest cost has the highest
utility. Fig. 5 shows the price variable curves for the binary
variables.
The inner wall radius measurement is a continuous variable
between 0.50 and 1.0 in. Fig. 6 illustrates the price-variable
curves for the continuous variables. It should be noted that both
continuous and discrete variables might be easily used in PBN

504

Fig. 6.

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 55, NO. 3, AUGUST 2008

Price-variable curves for continuous values (radius).

depending on the requirements of the design. Without the loss
of generality, risk parameters (r) are assumed to be 1 (that is,
risk neural for all the design variables).
To begin the process, an FA is created to maximize overall
utility and monitor the system constraints given by:
xcA l − xw
A l ≤ 0.01
c
xw
A l − xA l ≤ 0.01

xcS t1 − xw
S t1 ≤ 0.01
c
xw
S t1 − xS t1 ≤ 0.01

xcS t2 − xw
S t2 ≤ 0.01

Given the ten constraints and the information shown in
Table II, the following matrices H8×8 are created, as shown
at the bottom of the next page.
Thus,


0.088842
 −0.57 


 −0.1017 


 −0.4966 
(13)
[∆µj ] = αj [Gj ]T CH = 0.001 

 −0.1781 


0




0
0

10 π Rc [0.28xcA l + 0.178xcS t1 + 0.382xcS t2 ] − 30 ≤ 0.01

where αj is 0.001.
The optimization problem on price space of the FA is then
formulated as:

w
w
10 π Rw [0.28xw
A l + 0.178xS t1 + 0.382xS t2 ] − 30 ≤ 0.01

Max : U (Pj ) − [µj ][Pj ]T

10 π Rc [0.128xcA l + 0.3575xcS t1 + 0.175xcS t2 ] − 5 ≤ 0.01

subject to:

c
xw
S t2 − xS t2 ≤ 0.01

10 π R

w

[0.128xw
Al

+

0.3575xw
S t1

+

0.175xw
S t2 ]

− 5 ≤ 0.01.

The first six constraints ensure that the material choice is the
same for both agents. The last two pairs of constraints are the
cost and weight constraints. The single attribute utility functions are of the form given in (3), and since the utility values
range between 0 and 1, the single attribute scaling constants
kij are equal to 1. Solving (2) for K, K = −2 or 0. Zero does
not give finite solutions, therefore, K = −2 is selected. The
multiattribute utility function is given by:

 n

1 
j
j
1 − 2U (pi ) − 1 .
U (P ) =
(12)
−2 i=1
Thus, the multiattribute utility function of the FA is represented as
U (Pj ) = 0.5 − 0.5(1 − 8.77963px cA l )
(1 − 5.61325px cS t 1 )(1 − 11.9976px cS t 2 )
(3 − 2pR c )(3 − 2pR w )(1 − 4.02739px wA l )
(1 − 11.2296px wS t 1 )(1 − 5.50691px wS t 2 ).

1 ≤ pR c ≤ 2
0 ≤ px cA l ≤ 0.227
0 ≤ px cS t 1 ≤ 0.3563
0 ≤ px cS t 2 ≤ 0.1667
1 ≤ pR w ≤ 2
0 ≤ px wA l ≤ 0.4997
0 ≤ px wS t 1 ≤ 0.3663
0 ≤ px wS t 2 ≤ 0.1781.

(14)

The results for PBN give the optimum collaborative solution
as:
w
{xcA l = 1, xcS t1 = 0, xcS t2 = 0, xw
A l = 1, xS t1 = 0,
c
w
xw
S t2 = 0, R = 0.5, R = 0.5}.

Results from the previous solution set indicate that aluminum
was collaboratively chosen as the material with a radius of 0.5 in.
(This is consistent with the new design of the product actually
used by the company.)

GANGULY et al.: PRICE-BASED NEGOTIATION MECHANISM FOR DISTRIBUTED COLLABORATIVE DESIGN

In this example, we have shown how PBN, our proposed bispace, bilevel optimization design negotiation methodology, can
guide an initially infeasible design to a feasible one by maximizing the overall utility of the design. By transforming the
problem from design space to the price (utility) space, the algorithm can select the best design without knowing the specifics
of the individual design spaces for the agents. Use of PBN also
allows for flexibility with regard to collaboration. For example,
the user may change the risk parameter (r) for each variable to
reflect differing levels of risk aversion of the DA toward each
design variable. Additionally, different step sizes (αj ) may be
used to regulate the rate of collaboration. PBN does not require
sharing of information between entities, and therefore, can be
used in cooperative and noncooperative situations. Moreover,
PBN is independent of the design methodologies used. For instance, it is possible for one designer to use a method such as
finite element method (FEM) to analyze the design, while another designer conducts sheet metal design use software (such
as Pro/E). Rather than being required to understand the details
of each methodology, by passing the numerical values of the
design parameters mapped into the price space, this negotiation mechanism is able to reach a solution independent of the
methodology used.

V. CONCLUSION AND FUTURE WORK
In globally competitive markets, firms wishing to gain a competitive edge are focusing on developing quick and efficient
design processes to facilitate the rapid delivery of innovative
products. Thus, there is a need for an effective collaborative
mechanism in today’s globally dispersed and multidisciplinary
design environments. Recent advances in collaborative product
design adopt innovative techniques from game theory, utility
theory, constraint optimization, and fuzzy logic. The complexity of the design spaces and the size of the design impose several
computational limitations on these techniques.


H8×8

C10×8

505

The intent of this research is to develop a bilevel, agent-based
negotiation mechanism, termed PBN, as an effective generalized
platform for distributed, collaborative design. The mechanism
uses optimizations at two levels: the discipline level and the system level. The discipline-level optimizations are undertaken by
individual DAs that have full knowledge about their own design
space. The system-level optimization is undertaken by the FA
that maximizes the overall design utility as a function of the
variable unit prices. The system-wide structural and customer
specifications are enforced by the FA that uses price changes as
a mechanism for DAs to change the variable values. Negotiation continues in stages, where each stage represents an iteration
of price changes followed by corresponding changes in design
variable.
Results from the application of the PBN mechanism show
that unit prices of variables and multiattribute utility theory can
be combined to develop an effective medium for negotiation
between DAs in both cooperative and noncooperative environments. The separation of the price space and the design space
makes the mechanism applicable to any design context and independent of methodologies used by the designers. Additionally,
it does not require any agent to have complete knowledge of
the entire design space. This is, an advantage in the conceptual design stage when designers have limited knowledge of the
overall design project and are focused on their limited portion
of the design. The mechanism also has the ability to incorporate
changes in the design space as the design is evolving. Constraints
and variables may be dynamically added without changing the
mechanism.
PBN is a promising method for modeling collaborative design
problems. However, PBN is not without limitations. First, the
assignment of a crisp preference range of the prices over the design variables may not be possible at the conceptual design stage
for new products. Even though a formal methodology exists in
assigning prices to variables [22], the designers might not have
enough knowledge or experience to develop the price-variable

0.2278
0
0
0
0.3563
0
0
 0

0
0.1667
0
 0

0
0
0.4966
 0
=
0
0
0
 0

0
0
0
 0

0
0
0
0
0
0
0
0

1
0
0 −1 0
0 0
0
1
0
0 0
 −1 0

1
0
0 −1 0 0
 0

0
1
0 0
 0 −1 0

0
1
0
0 −1 0
 0
=
0 −1 0
0
1 0
 0

1
1
0
0
0 1
 1

0
0
1
1
1 0
 0

1
1
1
0
0
0 1
0
0
0
1
1
1 0

0
0
0
0
0.1781
0
0
0

0
0

0

0

0
,
0

0

1

0
1

0
0
0
0
0
0.3632
0
0

G10×1


0
0
0
0 

0
0 

0
0 

0
0 

0
0 

−4 0
0 −4


0
 1 


 0 


 1 


 0 
=

 0 


 0.61 


 0 


0
0

506

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 55, NO. 3, AUGUST 2008

curves. Under such circumstances, we recommend exploring
several price-variable curves and validating the curves through
evaluating the final design. Second, it may not be possible for
a DA to determine the risk nature (note that in this research,
we use risk neutral r = 1 for all the design variables). More
experiments should be done to study how different settings of
the risk nature impact the design outcomes. Third, the performance of PBN deteriorates when the number of coupled design
variables increases. In highly coupled designs, where there is a
great interdependency between the components, the number of
feasibility constraints increases dramatically, which cause larger
number of violations. Thus, it may take longer for PBN to converge. Fourth, we have made a number of assumptions that need
to be further explored. Due to the complexities of the design
process and political issues associated with multidisciplinary
endeavors, the assumption of truthful revelation of the initial
unit prices needs further investigation. We have also assumed
that the price is defined by domain expertise without bias. We
would like to further explore the bias issue in future work. Fifth,
the concept of unit price has a limited scope. Future work will
expand the concept of the unit price to a “virtual price” to consider other important factors in the success of a design such as
cost, quality, and time. The virtual price will be a combination of
such factors. Finally, additional testing needs to be done in larger
and more complex real-world applications through the creation
of an implementation and validation mechanism of PBN.

APPENDIX
PBN PROTOCOL
Stage 0) Formation of the Agents (Section III-A):
Define the group of DAs collaborating to design a product.
Each DA is in charge of designing a specific component of the
final product or is in charge of addressing a goal (e.g. minimizing
weight or cost) for the product. An artificial FA is created to
monitor the collaboration and make the final product feasible per
customer requirements. The goal is to produce a feasible design
with design variables as close to the DA’s preferred values as
possible.
Stage 1) Initialization Stage (Step 0 in Table I):
Each DA selects its best (locally optimal) design and passes
the following information to the FA:
1) the values of the design variables at its best design point;
2) the DA’s preference nature and preference range for each
design variable. This is passed on to the FA in the form
of a range for the design variable value and the corresponding range of prices. A higher price refers to a more
preferred value of the design variable which translates to a
higher utility realizes by the DA at that value of the design
variable;
3) the risk nature or attitude toward compromise for each of
the variables.
Stage 2) Penalization Stage (Steps 1 and 2 in Table I):
The FA takes the information from each of the DAs and
evaluates if the overall design is feasible and per customer
specifications.

1) If the overall design is feasible (and meets customer specifications), then the collaboration is complete, the mechanism stops and the design is final.
Or
1) If the overall design is infeasible or does not meet customer satisfaction, the FA investigates further into which
variables are causing the violation and in what way.
a) The FA then calculates and recommends a new preference range for all variables that are involved in
system-wide constraint violations. The FA’s philosophy in recommending new preference ranges is
that feasibility (or specifications) should be achieved
while making the DAs compromise the least. The
change in preference ranges for any design variable
is proportional to its contribution in constraint violations. At the same time, the FA also strives to keep
the variables as close to the DA’s most preferred
values as possible.
Stage 3) Redesign Stage (Steps 3 and 4 in Table I):
1) The DA takes new recommended preference ranges from
the FA and changes its design to adhere to the recommendations. It then passes on its new design based on the
recommended preference ranges back to the FA.
2) The FA then undertakes step 2 again.
PBN iterates between steps 1, 2, and 3, such that at every iteration, the overall design moves toward feasibility (or customer
specs) until it stops at step 2 at some iteration with a feasible
design meeting the customer requirements. PBN guarantees a
final feasible design while trying to reduce the deviation of design variables from their local optima. The amount of deviation
(or compromise) a DA undertakes can be controlled using step
sizes and risk attitude adjustments.
REFERENCES
[1] N. M. Alexandrov and R. M. Lewis, “Analytical and computational
properties of distributed approaches to MDO,” presented at the 8th
AIAA/USAF/NASA/ISSMO Symp. Multidisciplinary Anal. Optim.,
Long Beach, CA, 6–8 Sep. 2000.
[2] M. S. Bazarra, H. D. Sherali, and C. M. Shetty, Nonlinear Programming,
Theory and Algorithms. New York: Wiley, 1993.
[3] R. D. Braun and I. M. Kroo, “Development and application of the collaborative optimization architecture in a multidisciplinary design environment,” in Multidisciplinary Design Optimization: State of the Art, N.
M. Alexandrov and M. Y. Hussaini, Eds. Philadelphia, PA: SIAM, 1997,
pp. 98–116.
[4] J. H. Cassis and L. A. Schmit, “On implementation of external penalty
function,” Int. J. Numerical Methods Eng., vol. 10, no. 1, pp. 3–23, 1976.
[5] R. A. Callaghan and K. Lewis, “A 2-phase aspiration-level and utility
theory approach to large scale design,” presented at the Design Eng. Tech.
Conf. Comput. Inform. Eng. Conf., Baltimore, MD, DETC2000/DTM14569, Sep. 10–13, 2000.
[6] H. Choi, J. H. Panchal, K. J. Allen, W. D. Rosen, and F. Mistree, “Towards a
standardized engineering framework for distributed collaborative product
realization,” presented at the Design Eng. Tech. Conf. Comput. Inform.
Eng. Conf., Chicago, IL, 2003.
[7] M. G. Fernandez, J. H. Panchal, J. K. Allen, and F. Mistree, “An interactions protocol for collaborative decision making—Concise interactions and effective management of shared design spaces,” presented at the
ASME 2005: Design Eng. Tech. Conf. Comput. Inform. Eng. Conf., Long
Beach, CA. Paper No. DETC2005-85381, 2005.
[8] C. P. Fishburn, Utility Theory for Decision Making. New York: Wiley,
1970.
[9] R. T. Haftka and J. H. Starnes, “Applications of a quadratic extended
interior penalty function for structural optimization,” AIAA J., vol. 14,
no. 6, pp. 718–724, 1976.

GANGULY et al.: PRICE-BASED NEGOTIATION MECHANISM FOR DISTRIBUTED COLLABORATIVE DESIGN

[10] G. Hernandez and C. C. Seepersad, “Design for maintenance: A game
theoretic approach,” Eng. Optim., vol. 34, no. 6, pp. 561–577,
2002.
[11] G. Hernandez, C. C. Seepersad, K. J. Allen, and F. Mistree, “A framework
for interactive decision-making in collaborative, distributed engineering
design,” Int. J. Adv. Manuf. Syst., vol. 5, no. 1, pp. 47–65, 2002.
[12] G. Hernandez, K. J. Allen, and F. Mistree, “A framework for customizable
products as a problem of access in a geometric space,” Eng. Optim.,
vol. 35, no. 3, pp. 229–254, 2002.
[13] C.-C. Huang, “A multi-agent approach to collaborative design of modular
products,” Concurrent Eng.: Res. Appl., vol. 12, no. 2, pp. 39–47, 2004.
[14] R. Keeney and H. Raiffa, Decisions With Multiple Objectives: Preferences
and Value Tradeoffs. New York: Wiley, 1976.
[15] W. D. Li, S. K. Ong, J. Y. H. Fuh, Y. S. Wong, Y. Q. Lu, and A. Nee,
“Feature-based design in a distributed and collaborative environment,”
Comput.-Aided Design, vol. 36, pp. 775–797, 2004.
[16] K. Lewis and F. Mistree, “FALP: Foraging directed adaptive linear programming. A hybrid algorithm for discrete/continuous problems,” Eng.
Optim., vol. 32, no. 2, pp. 191–218, 1999.
[17] C.-Y. L. Stephen and J. Cai, “A collaborative design process model in the
socio-technical engineering design framework,” Artif. Intell. Eng. Design,
Anal. Manuf., vol. 15, pp. 3–20, 2001.
[18] C. M. Martson, “Game based design: A game theory based approach to
engineering design,” Ph.D. dissertation, Georgia Inst. Technol., Atlanta,
GA, 2000.
[19] H. R. Myers and D. C. Montgomery, Response Surface Methodology:
Product and Process Optimization Using Designed Experiments, 2nd ed.
New York: Wiley, 2002.
[20] E. Nikolaidis, “Decision-based approach for reliability design,” ASME J.
Mech. Design, vol. 129, no. 5, pp. 466–475, 2007.
[21] M. G. Parson, D. J. Singer, and J. A. Sauter, “A hybrid agent approach
for set-based conceptual ship design,” presented at the 10th Int. Conf.
Comput. Appl. Shipbuilding, Cambridge, MA, Jun. 1999.
[22] V. Parunak, A. C. Ward, M. Fleischer, and J. A. Sauter, “The RAPPID
project: Symbiosis between industrial requirements and MAS research,”
Auton. Agents Multi-Agent Syst., vol. 2, no. 2, pp. 111–140, 1999.
[23] V. Parunak, A. Ward, M. Fleisher, J. Sauter, and T.-C. Chang, “Distributed
component centered design as agent-based distributed constraint optimization,” in Proc. AAAI’97 Workshop Constraints Agents, 1997, pp. 93–99.
[24] V. Parunak, A. Ward, M. Fleisher, and J. Sauter, “A marketplace of design
agents for distributed concurrent set-based design,” presented at the 4th
ISPE Int. Conf. Concurrent Eng.: Res. Appl., Troy, MI, 1997.
[25] V. Parunak, A. Ward, and J. Sauter, “MarCon algorithm: A systematic market approach to distributed constraint problems,” presented at the ICMAS,
Paris, France, 1998.
[26] C. Petrie, “Agent-based engineering, the web, and intelligence,” IEEE
Expert Intell. Agents, vol. 11, no. 6, pp. 24–29, 1996.
[27] W. Shen and H. H. Ghenniwa, “A distributed multidisciplinary design
optimization framework based on web and agents,” presented at the Design
Eng. Tech. Conf. Comput. Inform. Eng. Conf., Montreal, QC, Canada,
2002.
[28] H. Simon, “A behavioral model of rational choice,” Quart. J. Economics,
vol. 6, pp. 99–118, 1955.
[29] H. Simon, “Rational choice and the structure of the environment,” Psychol. Rev., vol. 63, no. 2, pp. 129–138, 1956.
[30] T. W. Simpson, C. C. Seepersad, and F. Mistree, “Balancing commonality
and performance within the concurrent design of multiple products in a
product family,” Concurrent Eng. Res. Appl., vol. 9, no. 3, pp. 177–190,
2001.
[31] D. Singer and M. Parson, “Evaluation of the effectiveness of a fuzzy
logic software agent to aid design team negotiation and communication,”
presented at the 2nd Int. Conf. Comput. Appl. Inf. Technol. Marine Ind.,
Hamburg, Germany, Apr. 2003.
[32] J. Sobieszczanski-Sobieski, “Two alternative ways for solving the coordination problem in multilevel optimization,” Struct. Optim., vol. 6,
pp. 205–215, 1993.
[33] P. K. Sycara and J. Liu, “Exploiting problem structure for distributed
constraint optimization,” presented at the 1st Int. Conf. Multiagent Syst.,
San Francisco, CA, 1995.
[34] P. K. Sycara, S. Roth, N. Sadeh, and M. Fox, “Distributed constrained
heuristic search,” IEEE Trans. Syst., Man, Cybern., vol. 21, no. 6,
pp. 1446–1461, Nov./Dec. 1991.
[35] R. Tappeta and J. Renaud, “Interactive multiobjective optimization design
strategy for decision based design,” ASME J. Mech. Design, vol. 123,
no. 2, pp. 205–215, 2001.

507

[36] D. L. Thurston, “A formal method for subjective design evaluation with
multiple attributes,” Res. Eng. Design, vol. 3, pp. 105–122, 1991.
[37] D. L. Thurston, J. V. Carnahan, and T. Liu, “Optimization of design utility,”
J. Mech. Design, vol. 116, pp. 801–808, Sep. 1994.
[38] H. Wassenaar and W. Chen, “An approach to decision-based design with
discrete choice analysis for demand modeling,” ASME J. Mech. Design,
vol. 125, no. 3, pp. 480–497, 2003.
[39] H. Wassenaar, W. Chen, J. Cheng, and A. Sudjianto, “Enhancing discrete
choice demand modeling for decision-based design,” ASME J. Mech.
Design, vol. 127, no. 4, pp. 514–523, 2005.

Sandipan Ganguly received the first and second Masters’ degrees in industrial engineering and
aerospace engineering from Virginia Tech, Blacksburg, in 2002 and 2003, respectively, and the Ph.D.
degree in industrial engineering majoring in operations research from Arizona State University,
Phoenix, in 2007.
He is currently a Senior Statistical Modeler/Developer at Expedia, Inc., Bellevue, WA. His
current research interests include distributed optimization techniques, collaborative product design,
uncertainty modeling, product selection and pricing, dynamic pricing, and distributed data mining. He is the author or coauthor of papers published in the
Journal of Design and Process Science and several conferences.
Dr. Ganguly is an active member of the Institute for Operations Research
and the Management Sciences (INFORMS).

Teresa Wu received the Ph.D. degree in industrial
engineering from the University of Iowa, Iowa City,
in 2001.
She is currently with the Industrial Engineering
Department, Arizona State University, Phoenix, as
an Associate Professor. Her current research interests include distributed decision support, distributed
information system, supply chain modeling, and disruption management. She is the author or coauthor
of papers published (or accepted) in such journals
as the International Journal of Production Research,
Omega, Data and Knowledge Engineering, and Journal of Computing and Information Science in Engineering.
Prof. Wu serves on the Editorial Review Board for Computer and Standard
Interface, International Journal of Electronic Business Management. She is a
member of the Institute of Industrial Engineers (IIE).

Jennifer Blackhurst received the Ph.D. degree in
industrial engineering from the University of Iowa,
Iowa City, in 2002.
She is currently with Iowa State University, Ames,
as an Assistant Professor of Logistics and Supply
Chain Management. Her current research interests include supply chain risk and disruptions, supply chain
coordination, and supplier assessment. She is the author or coauthor of papers published (or accepted) in
such journals as the Production and Operations Management Journal, Decision Sciences Journal, Journal of Operations Management, International Journal of Production Research,
Omega, and Supply Chain Management Review.
Prof. Blackhurst is on the Editorial Review Board for Decision Sciences
and is a member of the Decision Sciences Institute (DSI) and Production and
Operations Management Society (POMS).

A I REDU X

Machine Learning Approaches
for the Neuroimaging Study
of Alzheimer’s Disease
Jieping Ye, Teresa Wu, and Jing Li, Arizona State University
Kewei Chen, Banner Alzheimer’s Institute and Shanghai Jiao Tong University

Machine learning tools aid many Alzheimer’s disease-related
investigations by enabling multisource data fusion and
biomarker identification as well as analysis of functional brain
connectivity.

A

lzheimer’s disease (AD) is
the most common type
of dementia, accounting for 60-80 percent of
age-related dementia cases. AD progressively destroys neurons and their
connections in the brain, leading to
loss of cognitive function and, ultimately, death.
The disease currently affects about
5.3 million people in the US, and the
number of victims will significantly
increase in the near future without
the development of therapeutics.
AD was the seventh-leading cause
of death across all ages in the US in
2006; it was the fifth-leading cause of
death for those 65 and older (www.
alz.org). The direct cost to care for AD
patients by family members or healthcare professionals is more than $100
billion per year; this figure is expected
to rise dramatically as the population
ages during the next several decades.
To avert a healthcare crisis, AD
researchers have recently intensified
their efforts to delay, cure, or prevent
the onset and progression of the disease. These efforts have generated a
large amount of data, including brain
neuroimages, that provides unprec-

0018-9162/11/$26.00 © 2011 IEEE	

edented opportunities to investigate
AD-related questions with higher
confidence and precision. Especially
promising is the use of machine
learning approaches to analyze neuroimages to improve AD detection
and diagnosis. Emerging techniques
include the fusion of AD data from
multiple sources, AD biomarker
identification from multiple sources,
and the analysis of functional brain
connectivity.

NEUROIMAGING
Recent studies have demonstrated
that imaging parameters based on
brain scans are more sensitive and
consistent measures of AD disease
diagnosis and progression than cognitive assessment. Thus, neuroimaging
techniques offer great potential to
identify the specific biomarkers that
can identify individuals early in the
course of a dementing illness even
before onset of the disease.
One com mon neu roima ging
technique is structural magnetic
resonance imaging (MRI), which
visualizes brain anatomy with a high
degree of contrast between brain
tissue types. Researchers can use

Published by the IEEE Computer Society

structural MRI to measure specific
structures such as the hippocampus,
entorhinal cortex, and amygdyla to
detect abnormal volumetric changes
related to AD. Another popular neuroimaging technique is positron
emission tomography (PET). Using
different radioactive tracers, PET
provides information on various physiological, biochemical, and metabolic
processes.
Recognizing these technologies’
importance, the National Institutes of
Health in 2004 funded the Alzheimer’s Disease Neuroimaging Initiative
(ADNI; www.adni-info.org), which
has become a landmark study in the
development of neuroimaging and
other biosignatures for the disease.
All ADNI subjects undergo 1.5T or 3T
structural MRI scans (T, for Tesla, is
a unit of magnetic flux density); half
undergo fluorodeoxyglucose (FDG)
PET scans.
FDG-PET scans have been shown
to be highly sensitive at detecting
AD-related glucose uptake abnormalities even before onset of the disease.
For example, Eric Reiman and his
colleagues examined the cerebral
metabolic rate for glucose (CMRgl)

APRIL 2011

99

A I REDU X

Figure 1. Brain regions with abnormally low CMRgl in young adult carriers of the
ApoE4 allele and their relation to brain regions with abnormally low CMRgl in patients
with probable AD. Source: E. Reiman et al., “Functional Brain Abnormalities in Young
Adults at Genetic Risk for Late-Onset Alzheimer’s Dementia,” Proc. National Academy of
Sciences, 6 Jan. 2004, pp. 284-289).

among normally healthy young subjects who had 0, 1, or 2 copies of the
apolipoprotein E4 (ApoE4) allele, a
known genetic risk factor associated
with AD (E. Reiman et al., “Functional Brain Abnormalities in Young
Adults at Genetic Risk for Late-Onset
Alzheimer’s Dementia,” Proc. National
Academy of Sciences, 6 Jan. 2004,
pp. 284-289). As Figure 1 shows, the
healthy ApoE4 carriers had lower
CMRgl (bright blue) than the noncarriers in brain regions whereas CMRgl
was abnormally low in AD patients
(purple areas).
In addition to neuroimaging data,
ADNI compiles demographic information such as age, gender, and years of
education; genetic markers (such as
possession of ApoE4 allele); clinical
ratings; various cognitive tests; and
protein abnormalities in cerebrospinal fluid (CSF), also associated with
AD.

100

COMPUTER	

MULTISOURCE DATA FUSION
Current research has focused on
using either regions of interest (ROIs)
or a voxel-based approach to extract
features from one neuroimaging
modality—for example, structural
MRI or PET alone. Integrating complementary ROI and voxel-based
information from different neuroimaging sources and incorporating
additional information such as demographic and genetic data will likely
improve the sensitivity and specificity
of AD detection.
One way to combine numerous AD
data sources is to treat the variables in
all datasets indiscriminately, without
considering their different levels of
relevance to AD. Toward this end, AD
researchers have explored multiple
kernel learning (MKL), a technique
that synthesizes information from
multiple heterogeneous data sources
into a rigorous optimization prob-

lem (J. Ye et al., “Heterogeneous Data
Fusion for Alzheimer’s Disease Study,”
Proc. 14th ACM SIGKDD Int’l Conf.
Knowledge Discovery and Data Mining,
ACM Press, 2008, pp. 1025-1033). MKL
works by first constructing a kernel
from each of the data sources and
then combining these kernels into
a single one based on a certain criterion for improved classification
performance.
To illustrate, assume we have a
collection of p data sources with n
samples. We first construct p kernel
Gram matrices {G i}i =1, …, p of size
n by n, one for each of the p data
sources. A kernel Gram matrix
computes the dot product of the
samples in some feature space, thus
capturing the similarities between
samples. For example, the ( j,k)-th
entry of the matrix Gi captures the
similarity between the j-th sample
and the k-th sample based on the i-th
data source alone. MKL integrates
these p data sources by computing
a composite kernel Gram matrix
G = Σiθi Gi, where the optimal combination coefficients are obtained by
optimizing a certain criterion. Figure
2 illustrates the use of MKL to fuse
data from five AD-related sources:
structural MRI, PET, genetic, CSF,
and demographic.
To separate AD patients from
nor ma l cont rol ( NC) subject s,
researchers in the MKL study cited
above applied the technique to fuse
structural MRI data based on tensor
factorization, structural MRI data
based on anatomical automatic
labeling, genetic information based
on ApoE4, gender, and age. They
measured performance in terms of
sensitivity (accuracy in correctly identifying AD patients) and specificity
(accuracy in correctly identifying NC
subjects). MKL achieved 95.0 percent
sensitivity and 89.5 percent specificity, significantly outperforming even
the best prediction (80.0 percent sensitivity and 79.5 percent specificity)
using each of the five data sources
individually.

MULTISOURCE BIOMARKER
SELECTION
Another urgent task in current
AD research is biomarker identification, which can be considered a
general feature selection problem.
Feature selection algorithms attempt
to remove as many irrelevant and
redundant features as possible and
to find a feature subset such that,
with dimensionally reduced data, a
learning algorithm can achieve better
performance.
Feature selection has a wide variety of applications including text
mining, image processing, and biomedical informatics. Traditional
feature selection algorithms work on
a single data source only. The challenge for AD researchers is developing
effective algorithms for data from
multiple sources to enable the identification of composite biomarkers.
MKL fuses the contributions of biomarkers from multiple data sources.
Specifically, the combined kernel
matrix extracts data patterns in the
form of pairwise similarities, which
can then serve as the input for a
generic feature selection algorithm.
Preliminary studies indicate that MKL
not only adequately distinguishes AD
and NC subjects but also identifies
brain regions that play more significant roles than others in AD.

FUNCTIONAL BRAIN
CONNECTIVITY
Recent studies have shown that
higher cognitive processing results
from different brain regions interacting with one another rather than
working independently. Dramatic
global cognitive decline is a major
symptom of AD, and patients’ brains
thus exhibit abnormal patterns of
functional connectivity—for example, reduced hippocampal network
activity.
Some studies of early AD and mild
cognitive impairment have found
increased connectivity between the
frontal lobe and other brain regions.
Researchers have interpreted this as a

G1

G2

G3

G4

G5

MRI

PET

Genetic

CSF

Demographic

θ3

θ4

θ5

θ1

θ2

G

Figure 2. Illustration of using multiple kernel learning to fuse data from five sources:
structural magnetic resonance imaging (MRI), positron emission tomography (PET),
genetic, cerebrospinal fluid (CSF), and demographic. The composite kernel matrix G is a
linear combination of the five kernel matrices constructed from these five data sources.

compensatory reallocation or recruitment of cognitive resources. Because
regions in the frontal lobe are typically affected later in the course of
the disease, an increase in frontal
connectivity arguably helps early AD
patients maintain some memory and
attention abilities.
A method based on sparse inverse
covariance estimation (SICE) identifies
functional brain connectivity networks from FDG-PET data (S. Huang
et al., “Learning Brain Connectivity of
Alzheimer’s Disease by Sparse Inverse
Covariance Estimation,” NeuroImage,
15 Apr. 2010, pp. 935-949). SICE
makes it possible to identify both
the connectivity network structure
and connectivity strength for a large
number of brain regions with small
sample sizes.
Researchers using SICE have
observed distinct connectivity patterns for AD patients and NC subjects
in terms of the number of connections within lobes, between lobes,
and between hemispheres, and in
terms of the strength of such connections—findings consistent with the
AD literature. SICE techniques can
also identify connectivity-based FDGPET biomarkers.

multisource data fusion and biomarker identification as well as
analysis of functional brain connectivity. Despite these advances,
many challenges remain, including
more effectively predicting disease
progression and using multisource
data for efficient clinical treatment
evaluation.

M

Editor: Naren Ramakrishnan, Dept. of
Computer Science, Virginia Tech, Blacksburg,
VA; naren@cs.vt.edu

achine learning tools aid
many AD-related investigations by enabling

Jieping Ye is an associate professor
of computer science, as well as a core
faculty member of the Center for Evolutionary Medicine and Informatics, at
Arizona State University (ASU). Contact him at jieping.ye@asu.edu.
Teresa Wu is an associate professor
of industrial engineering at ASU. Contact her at teresa.wu@asu.edu.
Jing Li is an assistant professor of
industrial engineering at ASU. Contact her at jing.li.8@asu.edu.
Kewei Chen is a senior scientist at
Banner Alzheimer’s Institute as well
as director of the Computational
Image Analysis lab at Banner Good
Samaritan Medical Center’s PET
Center; he is also an adjunct professor
at Med-X Research Institute, Shanghai
Jiao Tong University, China. Contact
him at kewei.chen@bannerhealth.
com.

APRIL 2011

101

CONCURRENT ENGINEERING: Research and Applications
SMWA: A CAD-based Decision Support System for the Efficient
Design of Welding
Yongjin Kwon,1 Teresa Wu2,* and Juan Ochoa Saldivar3
1

Applied Engineering Technology, Goodwin College of Professional Studies, Drexel University, Philadelphia, PA 19104, USA
2
Department of Industrial Engineering, Arizona State University, PO Box 875906, Tempe, AZ 85287-5906, USA
3
Information Technology Analysis Division, Deere & Company, Dubuque, IA 52004, USA

Abstract: Welding is one of the most widely used permanent joining technologies in assembly. As with other manufacturing processes,
manufacturing feasibility and efficiency of weld components should be considered early in the part design stages to avoid costly redesign and
delay. While there have been considerable research interests in weldability assessment and predictive modeling of welding distortion, under
current practices, only after the part has been designed, welding engineers start working on the welding process plan. One significant reason
for the current practice is the complexity involved in the decision and selection of feasible welding parameters. Therefore, there is a great need
for a methodology that can incorporate benefits from Concurrent Engineering (CE) concepts into weld design. The objective of this study is to
develop such a methodology to avoid welding infeasibility due to lack of manufacturability evaluations at the design stages and to generate a
more complete and improved welding design in a shorter time. A CAD-based decision support system, named Sheet Metal Welding Advisor
(SMWA), is developed for the purpose. The advantages include: (1) integration of CE concepts in welding design, thus improving the overall
efficiency of design and manufacturing practices, (2) automatic generation of optimum welding parameters and associated manufacturing
data for economic considerations, and (3) a great potential for the real-world applications.
Key Words: Concurrent Engineering, decision support system, CAD, weld design efficiency, Pro/engineer.

1. Introduction
Concurrent Engineering (CE), otherwise known as
Simultaneous Engineering or Life Cycle Engineering
is concerned with improving the product development
process by taking into account a wide variety of
considerations such as manufacturability, assemblability, testing, etc., during early stages of the product
design [29]. It has proven to be an effective strategy for
industry to maintain competitiveness in responding
to the dynamically changing, globalized, customerdriven market. Some Japanese companies take half
the time that U.S. companies do to deliver major
products (e.g., aircraft and automobiles) [1]. This
success comes from the fact that CE contributes
significantly to the reduction of product development
cycle [18]. Various approaches have been developed to
help designers with CE, including O’Grady and Young
[13], Young et al. [30], Prasad [17], and Park and
Cukodky [14]. Prasad [15,16], after the thorough

*Author to whom correspondence should be addressed.
E-mail: teresa.wu@asu.edu

investigations, classifies the agents that influence CE
into seven categories (i.e., talents, tasks, teams, techniques, technology, time, and tools) and categorizes CE
techniques into six levels with increasing degree of
creativity and cooperation. These include networkbased techniques, documentation-based techniques,
variable-driven techniques, predictive techniques,
knowledge-based techniques, and agent-based techniques. O’Grady and Young [13] reviewed methodologies
on the implementation of CE and categorized them
according to the tools used for the implementation. One
effective approach to implement CE is Design for
Manufacturing (DFM). In a broad sense, DFM includes
any necessary steps, e.g., selection of materials and
machine tools, manufacturing methods, process planning, assembly methods, testing and quality control, etc.
for the product development [12]. Since up to 70% of a
product’s manufacturing cost can be determined at the
design stages [6], it is important for designers to evaluate
alternatives for the reduction of manufacturing or
assembly difficulties and costs. For example, designers
should not over-specify the design parameters to
avoid unnecessary increases in cost. The principle of
DFM is to provide design with less cost, increased
quality, increased reliability, and shorter time to market.
Therefore, industry is pressing hard to make use of

Volume 12 Number 4 December 2004
1063-293X/04/04 0295–10 $10.00/0
DOI: 10.1177/1063293X04042470
ß 2004 Sage Publications

295

296

Y. KWON

modern strategies and technologies and to apply DFM
concepts into their product design cycles.
In this study, our main interest is the design process
of welding, one of the most common and effective way
to join metals permanently. Heavy farming and
construction equipment, in particular, relies heavily
on high speed gas metallic arc welding (HSGMAW) for
their assembly. It is reported that over 50% of the
gross national product of the U.S. is related to welding
in one way or another [10]. As with other manufacturing processes, weldability and manufacturability of
weld components should be considered early in the
design stages. Due to the fact that product designers
do not necessarily have process knowledge or experience in welding, the manufacturing related issues
are only evaluated after the design is complete. This
can entail design changes, causing longer product
development cycle and extra cost. Common decisions
that welding engineers have to make are numerous (but
not limited to): design dimensions for weld joint;
consistency of joint design with approved standards;
decisions on intermittent versus continuous weld; values
for the process parameters; accessibility and fixture
interference to weld joints; levels of distortion within
the accepted range; efficiency of welding sequence;
wire diameter; energy requirements for current and
voltage; weld rod stick-out distance; weld gun angles;
penetration depth and number of passes; and welding
speed. Welding parameters have major interactions
with part geometry, plate thickness, weld bead geometry, degree of heat dissipation, and types of fixture
elements, hence the complexity involved in setting and
adjusting the welding parameters requires years of
experience. Even so, finding optimum parameters with
minimum distortion, maximum production rate, and
lower production cost is quite difficult. For design
engineers with no practical welding experience, a decision support system that can provide a set of feasible
manufacturing data would be a very helpful tool, if the
welding process should be considered from the early
design stages to fully implement the benefits of CE and
DFM. This is the central research issues addressed in
this paper.
The overall structure of this paper is formatted as
follows: Section 2 provides review on the theoretical
backgrounds of computer-based welding analysis tools
and the difficulties involved in the current practice
of design and welding process planning. In Section 3, the
motivations and the steps in the development of Sheet
Metal Welding Advisor (SMWA), a CAD-based decision support system, is elaborated. In Section 4, an
illustrative example, a construction equipment subassembly, is used to demonstrate how the proposed
methodology can help designers with critical manufacturing related information. Finally, Section 5 provides a
conclusion.

ET AL.

2. Background Review
The capacity to predict downstream design attributes
has potential benefits that include reduction of design
iterations, avoiding wasted effort that occurs by exploring paths leading to infeasible designs, and reducing the
need for time-consuming analysis and simulation as well
as physical prototyping [26]. These advantages can be
translated into reduced design costs and time to market.
Design and manufacturing are two activities intrinsically
related, yet difficult to connect. The ‘‘brick wall’’ that
clearly separates these two stages of the product life
cycle represents a challenging task to overcome. Product
designers often talk a different language than process
engineers. Thus, leverage of information from one stage
of the product development cycle to another does not
become a systematic, efficient process. The traditional
production flow of a manufacturing factory is characterized by three main separated stages: design, process
planning and manufacturing [31]. Process planning
acts as a bridge between design and manufacturing
by translating design specifications into manufacturing process details. Under this approach, manufacturability issues are not addressed in the early stages of the
design development. The lack of relevant information
during the design process can be reflected in poor
manufacturing specifications and in impractical manufacturing solutions.
As with other manufacturing processes, it is critical to
assess the manufacturability and efficiency of weld
design at the early design stages. Under current
practices, however, only after the parts have been
designed, the welding process planning is followed.
Among other duties, welding engineers have to find the
process parameters to ensure the feasibility of each weld
in an assembly. Moreover, evaluating and locating the
optimum process conditions based on the part design
necessitate a computer-based solution for fast mathematical calculations. Without a computer-based system,
welding parameter selection is primarily based on the
experience or values obtained from the reference
materials. Many research efforts have been directed to
the development of computer systems for evaluating
manufacturability and cost assessment in welding [7].
Kwon and Fischer [5] proposed a numerical analysis
approach for welding process optimization with emphasis in minimum welding distortion. The significance of
this approach was to consider the adverse effects of heat
distortion and constraining force generated by fixture
elements, and calculate residual stresses in the welded
components for the structural analysis. Michaleris et al.
[9] proposed a mathematical approach for the design
of weakly coupled thermo-elasto-plastic systems in
welding. Abundant materials can be found in the
area of numerical analysis and modeling of welding
distortion and processes: Tekriwal and Mazumder [23],

A CAD-based Decision Support System

Tekriwal and Mazumder [24], Shim et al. [20], Goldak
et al. [2], Tsai et al. [25], and Zhao et al. [32], to just name
a few. The intrinsic problems related to the previously
developed methods are their limited scopes, applicationdependent modeling, lack of considerations in heat
dissipation through welding fixture and heat sink materials, and solutions based on simple part geometry. The
inherent complexity in HSGMAW prevents the accurate
and precise mathematical mapping between input and
output variables (e.g., welding distortion). Even if this
is accomplished, slight changes in welding settings may
render the model useless. Therefore, a decision support
system is needed that can consider critical issues in
welding design in an efficient manner without encumbering the users (designers) with a minimum burden of
computational complexity to the design systems.
3. Development of a CAD-based Decision
Support System
To address these problems, a CAD-based decision
support system, Sheet Metal Welding Advisor (SMWA),
has been developed using customized Cþþ codes.
SMWA evaluates the geometric features created under
Pro/EngineerTM and Pro/WELDINGTM, and addresses
the critical problems in welding, such that the designers
can evaluate and assess the design before it is released
to the production engineers (see Figure 1 for overall
working of SMWA). In general, information such as
welding path sequence and welding conditions are
not clearly spelled out during the design processes.
This means that skilled technicians must intervene to

297

compensate, therefore, shop floor technology related
to welding robots and welding control relies on
human skills and experience. To promote integration
of the entire process from upstream to downstream,
it is first required to convert the welding expertise of
the skilled welder into software, and develop algorithms
to optimize welding conditions at the design level.
Based on the geometry information, the SMWA
determines the feasible set of plate thickness and
diameter of welding rod, then calculates the optimum
weld parameters along with the manufacturing data.
Rather than focusing on a narrow scope for the accurate
prediction of welding parameters, SMWA automatically
generates a feasible set of welding parameters based
on the weld geometry. In case of zero solutions,
SMWA indicates the manufacturing infeasibility so
that the necessary modifications to the design can be
initiated. Any design changes are automatically updated
into the solutions, and SMWA displays critical production-related information in the form of Manufacturing
Data. Therefore, the purpose is not to develop a precise
mathematical or empirical model, yet devising a decision
support system for easy and fast visualization of welding
design feasibility and generation of production data
for economic considerations. SMWA is based on the
interaction of process parameters, energy input, and
distortion as it relates to a part geometry (see Figure 2),
and intends to find process parameters for maximum
heat input (i.e., maximum welding speeds) at a minimum
level of distortion. The theoretical base for the application comes from the heat flow in the work piece, the
calculation of energy input for rapidly moving power

Figure 1. Overall sequence of how SMWA works.

298

Y. KWON

ET AL.

Table 1. Proposed weld bead dimensions.
Plate
Thickness, h (in.)
T  1/8
1/8 < T  5/32
5/32 < T

Legs of the
Weld, L (in.)

Length of the Fillet
Weld Segment (in.)

L ¼T
L ¼ 4/5T
L ¼ 2/3T

12  T
12  T
12  T

Y
is
ax

W
L1

Figure 2. Interaction of current, energy input, and welding
distortion as it relates to part geometry.

sources in the infinite plate, the electrode wire heating
and the relation among wire feed rate, current density,
and the electrode extension [3,4,19].
Heat effects of welding such as distortions are
computed using the results from the inherent angular
distortion, and the longitudinal shrinkage of welds,
and the longitudinal and transversal shrinkage for
continuous and intermittent welds [19,27]. Part geometry and the welding standards, ANSI/AWS D1.1-98
[28], regulate the interaction among the heat sources.
Many approaches to predict weld bead geometry
(the shape or cross section of the weld deposit) resulted
in unrealistic predictive solutions because of the
difficulties in dealing with a large number of complex
interactions among HSGMAW variables. Quantitative
approach [21] and coupling statistically designed
experiments with compatible analytical techniques [8]
showed relatively accurate results. To overcome the
difficulties, the weld bead cross section was calculated
based on the rules and structural welding codes established in ANSI/AWS D1.1-98. Based on the welding
design restrictions in ANSI/AWS D1.1-98, the design
constraints for weld bead dimensions are developed, as
represented in Table 1.
Weld bead geometry plays a very important role for
SMWA. The geometry of the weld bead cross section is
shown in Figure 3. ANSI/AWS D1.1-98 characterizes
fillet weld profiles as slightly convex flat or slightly
concave shapes for the faces of fillet welds.
Table 2 provides convexity values that shall not be
exceeded in actual welding for a structural safety reason.
Assuming the weld bead has a parabolic shape, the
convexity of a weld can be related to the width of the
weld bead. A parabola is the set of all points equidistant
from a fixed point, a focus, a fixed line, and the directrix
in the plane [22].

X

is
ax

C0

L2

Figure 3. Cross section of weld bead.

Table 2. Maximum convexity values for a width of weld
face (ANSI/AWS D1.1-98).
Width of Weld Face or
Individual Surface Bead, W (mm)

Maximum Convexity,
Co (mm)

W<8
8 < W < 25
W > 25

1.6
3
5

The simplest analytical form for the parabola is
obtained when the symmetry axis coincides with one
coordinate axis and the vertex (the intersection of the
axis with the curve) is at the origin. The simple parabola
can be represented by the equation:
y2 ¼ 4ax
where a is the distance from the vertex to the focus of
the parabola. The ‘‘latus rectum’’ is the chord perpendicular to the axis that goes through the focus and its
length is 4  a. Based on the values from Table 2, the
following relationship is developed:
Co ¼

a
3:5

where Co is the weld bead convexity. Then, replacing
this value on the parabola equation:
 2
W
¼ 4  ð3:5CoÞ  Co
2

299

A CAD-based Decision Support System

where W is the hypotenuse of the triangle made of L1
and L2. The weld bead convexity becomes:
W
Co ¼ pﬃﬃﬃﬃﬃﬃﬃ
4 3:5
The half of the crown cross-sectional area (Accs ) of the
weld bead
be obtained by integrating the curve,
pﬃﬃﬃﬃﬃcan
ﬃ
f ðxÞ ¼ 2 ax, from 0 to Co, that is:
Acss
¼
2

Z

Co
0

pﬃﬃﬃﬃﬃﬃ
4 pﬃﬃﬃ 3=2 Co 4 pﬃﬃﬃﬃﬃﬃﬃ 2
ax 0 ¼
3:5Co
2 axdx ¼
3
3

Replacing the value of Co, the crown sectional area
(Accs ) becomes:
pﬃﬃﬃﬃﬃﬃﬃ
3:5 2
W
Accs ¼ 2 
42
Therefore, the cross-sectional area (Ad) of the weld
bead can be expressed in the form of:
pﬃﬃﬃﬃﬃﬃﬃ
3:5 2
L1  L2
W
þ
Ad ¼
21
2
where L1 and L2 (mm) are the legs of weld bead. Based
on the observation and common practice, it was noted
that any suitable wire diameter for a specific plate
thickness will generate a ratio number () within a
constant interval. The objective of the  is to avoid
unnecessary computations for wire diameters that have
no practical use for a particular plate thickness, hence
the  is an expression that correlates wire diameters with
plate thickness, in the form of:
rﬃﬃﬃﬃﬃﬃﬃ
dn
Ae i
¼ þ
Ad
h

and sheet metal component thickness. Values that fit the
 requirements are shown in gray cells.
High levels of energy allow faster travel speed and
greater melting rates. However, setting high levels
for amperage and voltage might produce, among
other negative consequences, unacceptable quantities
of distortion. Melting rate of the electrode is calculated
on the assumption that at steady state the melting
rate is equal to the wire feed speed. It is also assumed
that deposited wire is equal to the volume of the
weld bead. Welding speed is calculated based on the
rapidity that the melting wire can fill the calculated weld
bead volume. The time for the temperature field is
calculated assuming a weld pool width equal to the weld
bead width. Arc efficiency for HSGMAW is applied to
compensate for heat dissipation into an atmosphere
due to convection and radiation of the arc energy.
In order to find welding parameters that maximize
the heat input (hmax) into the welding spot, thus achieving maximum levels of welding speed at the lowest
magnitude of distortion, the following functional
relationship is defined:
f : x1 , x2 , . . . , xn ! hmax

where dn is the diameter of the normal distribution heat
source, h is the average plate thickness, Aei are the crosssectional areas of electrodes (i ¼ 1, 2, . . . , n). The upper
and lower limit values are set as, 0.40<<0.70. Based
on the proposed range, Table 3 relates wire diameters

where f denotes nonlinear relationship between a number
of welding variables (x1 , x2 ,; . . . , xn ) and a maximum
heat input into the welding spots, provided that
f ) 8Sj , where Sj is a set of constraints. Before starting
a calculation of welding parameters, the conditions
should be checked to determine: (1)  and plate thickness of weld components and (2) range of voltage,
current, heat input, wire feed speed, welding speed, and
subsequent distortions that are derived from the attributes in (1). The derived parameters in (2) should satisfy
the range of constraints (Sj ) of which equations are not
elaborated in this paper due to lengthy derivation
procedures. Otherwise, the solution becomes invalid.
The coefficients are derived from the commonly used
values for a particular plate thickness in ANSI/AWS
D1.1-98, and the empirically defined optimize function
is proposed in the form of:

Table 3. k values for different weld plate combinations.

1
1
hmax ¼ 1  I  N11  2    NAD
 3  L1  NLD1

Plate
Thickness,
h (in.)
1/32
1/16
3/32
1/8
5/32
3/16
7/32
1/4

k1
k2
(0.8 mm- (0.9 mmdiameter diameter
Wire)
Wire)
2.09
1.05
0.70
0.52
0.47
0.44
0.38
0.33

2.35
1.18
0.78
0.59
0.53
0.49
0.42
0.37

k3
(1.0 mmdiameter
Wire)

k4
(1.2 mmdiameter
Wire)

2.62
1.31
0.87
0.65
0.59
0.55
0.47
0.41

1.51
1.57
1.05
0.78
0.71
0.66
0.56
0.49

1
1
 4  L2  NLD2
 5  Tsc  NTD1
1
 6  TsI  NTD2

where 1 is the coefficient for current density, 2 is
the coefficient for angular distortion, 3 is the coefficient
for longitudinal distortion for the case based on heat,
speed, and thickness, 4 is the coefficient for longitudinal distortion for the case based on amperage and
thickness and speed, 5 is the coefficient for transverse
distortion in continuous weld, 6 is the coefficient for
transverse distortion in intermittent weld, I is the

300

Y. KWON

amperage,  is the inherent longitudinal distortion, L1
is the longitudinal shrinkage with energy considerations,
L2 is the longitudinal shrinkage design considerations,
Tsc is the value of transversal distortion continuous
weld, TsI is the value of transversal distortion intermittent weld,
NI, NAD, NLD1, NLD2, NTD1, and NTD2 are the
welding constants commonly used for a particular plate
thickness.
Figure 4 shows the flowchart for the working of
SMWA, where after the weld geometry is defined using
Pro/Engineer, subsequent calculations are performed
automatically. The optimize function is tested with the
signal-to-ratio (SN) derived from the quadratic loss
function by Taguchi [11]. The larger the better SN ratio
is selected because the system is considered optimized
when the response is as large as possible [11], which is in
the form of:
"

n
1X
1
SN ratio ¼ 10  log
n i¼1 2i

#

ET AL.

where i is the heat input, and n is the number of
iteration. The SN ratio is evaluated until the maximum
heat input (hmax) is found, while the parameters are
continually calculated and adjusted.

4. An Illustrative Example
A total of five cases are investigated to verify
the developed methodology, however, only two
cases are presented for space purposes. Case One is
a very common weld in the production of construction
equipment, consisting of a bracket and a curved arm
(see Figure 5). In Figure 5, only the first weld segment is
visible. Arm thickness is 1/400 and the bracket thickness
is 1/800 . Values for the actual cross section of the weld
bead are provided in the welding data file, automatically
generated by SMWA based on Pro/EngineerTM design.
SMWA can handle sheetmetal components with thickness up to 1/400 . Sheetmetal components in this range of
thickness can be welded in one pass, therefore, all the
welds are done in a single pass.

Figure 4. SMWA flowchart structure.

301

A CAD-based Decision Support System

Figure 5. Perspective view of case one.

Table 4. SMWA welding data file for case one.
Characteristics of Weld 01
Weld Bead Cross Section
Component Main Dimensions
No. of
Segment Gap
Segments Length (in.) (in.) Leg 1 (in.) Leg 2 (in.) Throat (mm) Convexity (mm) Thick 1 (in.) Thick 2 (in.) Width 1 (in.) Width 2 (in.)
3

2.25

1.625

0.125

0.167

2.54

0.707

0.125

0.25

3

2

Table 5. SMWA parameter file for case one.
Weld 01 Optimum Parameters
Amperage
(Amp)
384

Voltage
(V)

Stick-out
(mm)

Wire Feed
Speed (mm/s)

Wire Diameter
(mm)

Welding Speed
(mm/s)

Heat Input
(J/min)

Gas Flow
(L/min)

34

10

17.1

1.2

19

563.47

18

Table 6. SMWA manufacturing data file for case one.
Manufacturing Data

Time on Gas
(min/part)
0.88

Time on Arc
(min/part)

Production
Time
(min/part)

Wire
Consumption
(g/part)

Gas
Consumption
(L/part)

Energy
Consumption
(KW/part)

Energy
Consumption
(KW/part)

Productivity
(part/h)

0.75

0.89

62

15.84

585

1,88,004

51

After displaying geometric characteristics and constrains for the weld (see Table 4), SMWA displays
welding process parameters for each weld segment.
Table 5 provides the parameter file for the weld segment
depicted in Figure 5. After SMWA has examined
all welds in the assembly, the Manufacturing Data

file provides information for process planning and
production requirements (see Table 6). The Manufacturing Data include total time gas, total time arc,
total production time, volume of wire, volume of gas,
energy consumption, and productivity for economic
considerations.

302

Y. KWON

ET AL.

As can be seen from the examples, SMAW implements CE concept by considering weldability issues in
the early stage of the design. Therefore, it has great
potential to reduce the cost and lead time of the product.
Such a decision support system supports the Intelligent
Information System proposed by Prasad [15] by
addressing some of the main requirements identified
by Prasad.

Case Two shows the I beam, which is one of the
most common welded structures. The I beam is composed of two different components: the keel on the inner
part and the top and bottom flanges (same component
in the assembly). Continuous and intermittent welds
were applied on both sides of the keel. The I beam was
run five times under SMWA. Each time different values
for plate thickness were assigned. For each run, two
Pro/E blue print are included. One that shows the part
in perspective (see Figure 6) and the second to show
details of the weld bead in the front view. It is
worth mentioning that the welds are displayed as a
simple representation, that is, they do not show the
actual geometry calculated in SMWA. Values for the
actual cross section of the weld bead are provided in the
welding data file.
For each weld segment, a welding data file is
generated, showing the weld bead cross-section characteristics, the process parameters, and the production
requirements for a part (see Table 7).
After displaying geometric characteristics and constrains for the weld, the welding data file shows the
welding process parameters. GMAW process parameters in the welding data file are shown in Table 8.
After SMWA has examined all welds in the assembly,
under the subtitle ‘‘Manufacturing Data’’ the welding
data file provides information for process planning and
production requirements (see Table 9).

Figure 6. Pro/ENGINEER perspective view for case two.

Table 7. SMWA welding data file for case two.
Characteristics of Weld 01
Weld Bead Cross Section
No. of
Segments
3

Component Main Dimensions

Segment
Length
(in.)

Gap
(in.)

Legth 1
(in.)

Leg 2
(in.)

Throat
(mm)

Convexity
(mm)

Thick 1
(in.)

Thick 2
(in.)

Width 1

Width 2

0.75

3.875

0.031

0.094

0.753

0.335

0.031

0.094

3

2

Table 8. SMWA parameter file for case two.
Weld 01 Optimum Parameters
Amperage
(Amp)
124

Voltage
(V)

Stick-out
(mm)

Wire Feed
Speed (mm/s)

Wire Diameter
(mm)

Welding Speed
(mm/s)

Heat Input
(J/min)

Gas Flow
(L/min)

15

11

110

0.8

34

49.24

16

Table 9. SMWA manufacturing data file for case two.
Manufacturing Data

Time on Gas
(min/part)
044

Time on Arc
(min/part)

Production
Time
(min/part)

Wire
Consumption
(g/part)

Gas
Consumption
(L/part)

Energy
Consumption
(kW/part)

Energy
Consumption
(kW/hr)

Productivity
(Parts/h)

0.31

0.47

7

7.01

34

26,784

97

A CAD-based Decision Support System

5. Conclusions
In an effort to build a methodology to integrate
HSGMAW requirements early in the design process,
a CAD-based decision support system, SMWA, has
been developed. SMWA enables the designer with
very little knowledge of HSGMAW to evaluate the
feasibility of a design. SMWA extracts dimensional
information from Pro/EngineerTM, which serves as
a platform for the design, then automatically generates
a feasible set of welding parameters and associated
manufacturing data. In case of manufacturing conflicts
between the design and welding, SMWA indicates the
infeasibility. Since process parameters are geometry
dependent, in the likely event of a change in design,
any changes in part geometry will be automatically
reflected in the welding parameters. SMWA developed
in this study shows great potential for reduction of
product cost and lead time through the implementation
of CE and DFM concepts in the early design stages in a
complex welding environment. As a part of the future
works, the automatic generation of off-line programming will be investigated. The 3D nature of the welding
path and welding gun interference considerations make
it difficult to generate robust algorithms for automatic
off-line programming. The great variety of robot controllers is another issue for effective CAD-based robot
off-line programming. Part of the works done in this
paper gives insights on how to access relevant geometric
features for automatic CAD-based generation of robotic
welding paths. Additionally, the downstream activities
and associated constraints (such as material change or
major changeover in the production line) can be
considered in the design stages to achieve a more
comprehensive analysis of welding design.
References
1. Evanczuk, S. (1990). Concurrent Engineering the New Look
of Design. High Performance Systems, April, 16–17.
2. Goldak, J., Gu, M. and Hughes, E. (1993). Steady State
Thermal Analysis of Welds with Filler Metal Addition.
Canadian Metallurgical Quarterly, 32(1): 49–55.
3. Halmoy, E. (1979). Wire Melting Rate, Droplet Temperature, and Effective Anode Melting Potential. Int. Conf. Arc
Physics and Weld Pool Behavior, London: The Welding
Institute.
4. Halmoy, E. (1986). Electrode Wire Heating in Terms of
Welding Parameters, (Appendix A, The Physics of Welding),
2nd edn, New York: Pergamon, Oxford.
5. Kwon, Y. and Fischer, G. (2001). Process Optimization
for Minimization of Welding Distortion and Modeling/
Simulation of Welding Distortion using a Numerical
Analysis. A Proposal Submitted to Deere & Company,
Dubuque, IA.
6. Maddux, K.C. and Jain, S.C. (1986). CAE for the
Manufacturing Engineer: The Role of Process Simulation

303

in Concurrent Engineering. In: Tseng, A.A., Durham, D.R.
and Komanduri, R. (eds), Manufacturing Simulation and
Processes, NY: ASME.
7. Maropoulos, P.G., Yao, Z., Bradley, H.D. and
Paramor, K.Y.G. (2000). An Integrated Design and
Planning Environment for Welding, Part 1: Product
Modeling. Journal of Materials Processing Technology,
107: 3–8.
8. McGlone, J.C. (1982). Weld Bead Geometry Prediction:
A Review. Metal Construction.
9. Michaleris, P., Tortorelli, D. and Vidal, C. (1995). Analysis
and Optimization of Weakly Coupled Thermo-ElastoPlastic Systems with Applications to Weldment Design.
International Journal of Numerical Methods in Engineering,
38: 1259–1285.
10. Modern Welding Technology, 1998, available at http://
www.welding.com/edu_weld1.shtml.
11. Montgomery, D.C. (1997). Design and Analysis of
Experiments, 4th edn, New York, NY: John Wiley &
Sons, Inc.
12. O’Donnell, W.J. and Gomba, K.M. (1992). Design for
Manufacturability/Continuous Flow Manufacturing: An
Operating Procedure. Design for Manufacturing, ASME,
51: 1–5.
13. O’Grady, P. and Young, R.E. (1991). Issues in Concurrent
Engineering
Systems.
Journal
of
Design
and
Manufacturing, 1(1): 27–34.
14. Park, H. and Cukodky, MR. (1999) Framework for
Modeling Dependencies in Collaborative Engineering
Process, Research in Engineering – Theory Applications
and Concurrent Engineering, 11(2): 84–102.
15. Prasad, B. (1996). Concurrent Engineering Fundamentals, I,
New Jersey: Prentice Hall.
16. Prasad, B., (1997). Concurrent Engineering Fundamentals.
II, New Jersey: Prentice Hall.
17. Prasad, B. (1999). Enabling Principles of Concurrency and
Simultaneity in Concurrent Engineering. Artificial
Intelligence for Engineering Design Analysis and
Manufacturing, 13(3): 185–204.
18. Priest, J.W. and Sánchez, J.M. (2001). Product
Development and Design for Manufacturing, Marcel
Dekker, Inc., New York, NY.
19. Radaj, D. (1992). Heat Effects of Welding: Temperature
Field, Residual Stress, Distortion, New York, NY: Springer.
20. Shim, Y., Feng, Z., Lee, S., Kim, D.S., Jaeger, J.,
Paparitan, J.C. and Tsai, C.L. (1992). Determination of
Residual Stress in Thick Section Weldments: Welding
Journal, 71: 305–312.
21. Shinoda, T. and Doherty, J.J. (1978) The Relationship
between Arc Welding Parameters and Weld Bead
Geometry a Literature Survey, Welding Institute
Members Report.
22. Swokowski, E.W. (1983). Calculus with Analytic Geometry,
Marquette University, Prindle, Boston, MA: Weber &
Schmidt.
23. Tekriwal, P. and Mazumder, J. (1988). Finite Element
Analysis of Three-Dimensional Transient Heat Transfer in
GMA Welding. Welding Journal, Research Supplement, 67:
150–156.
24. Tekriwal, P. and Mazumder, J. (1991). Transient and
Residual Thermal Strain–Stress Analysis of GMAW,
Journal of Engineering Materials and Technology, 113:
336–343.

304

Y. KWON

25. Tsai, C., Park, S. and Cheng, W. (1999). Welding
Distortion of a Thin-Plate Panel Structure, Welding
Journal, Research Supplement, 78: 156–165.
26. Ullman, D.G. (1997). The Mechanical Design Process, 2nd
edn, Boston: Mcgraw-hill.
27. Watanabe, Masaki and Satoh Kunihiko (1961). Effects of
Welding Conditions on the Shrinkage Distortion in
Welded Structures, Welding Journal, 65(2): 377–384.
28. (1991). Welding Handbook, 8th edn, Vol. 2, American
Welding Society, Englewood, CO.
29. Wu, T., Blackhurst, J. and O’Grady, P. (2001). Integrated
Enterprise Concurrent Engineering: A Framework and
Implementation. International Journal of Concurrent
Engineering: Research and Applications, 9(3).
30. Young, R.E., Greef, A. and O’Grady, P. (1992).
An Artificial Intelligence-based Constraint Network
System for Concurrent Engineering. International Journal
of Production Research, 30(7): 1715–1735.
31. Zhang, H.Z. and Altin, L. (1994). Computerized
Manufacturing Process Planning Systems. Chapman &
Hall, New York, NY.
32. Zhao, H.Y., Shi, Q.Y., Lu, A.L. and Wu, A.P., (2000).
Development and Application of Adaptive Mesh
Technique in Three Dimensional Numerical Simulation of Welding Process. ACTA Metallurgica Sinica,
13: 33–39.

Yongjin Kwon
Yongjin Kwon is an Assistant Professor in the
Applied Engineering Technology Program, Goodwin
College of Professional Studies, at Drexel University.
He obtained his PhD from The University of Iowa
in Industrial Engineering. His research interests include
AI in manufacturing, knowledge-based CAD, manufacturing process modeling, and micro manufacturing and

ET AL.

assembly. His research work has been published in the
Journal of Manufacturing Systems, International
Journal of Machine Tools and Manufacture, Journal
of Aviation Psychology, Human Factors Journal, and
Journal of Machining Science and Technology.
Tong (Teresa) Wu
Tong (Teresa) Wu (teresa.wu@asu.edu) is an
Assistant Professor in Industrial Engineering Department of Arizona State University. Teresa has published
papers in International Journal of Concurrent Engineering: Research and Application, International Journal of
Agile Manufacturing, Production Planning and Control, International Journal of Production Research,
International Journal of Computer Integrated Manufacturing. She received her PhD from the University of
Iowa. Her main areas of interests are in supply chain
management, multi-agent system, data mining, and
collaborative product development.
Juan Ochoa Saldivar
Juan Ochoa Saldivar is an Information Technology
Analyst at Deere & Company in Dubuque, IA. He
obtained his Masters in Industrial Engineering from The
University of Iowa. He worked as a research scientist at
National Institute of Standards and Technology (NIST)
where he was engaged in the robotic, high-speed welding
research. His research interest is in the area of
integration of information management network with
manufacturing equipment for remote process monitoring and control and real-time anomaly detection.

A Novel Hessian based Algorithm for Rat Kidney Glomerulus
Detection in 3D MRI
Min Zhang1,2, Teresa Wu1,2,3, Kevin M. Bennett4
School of Computing, Informatics and Decision systems Engineering, Arizona State University,
Tempe, U.S.A.
2
Department of Radiology, Mayo Clinic Arizona, U.S.A.
3
Sino-Dutch Biomedical and Information Engineering, Northeastern University, Shenyang, China
4
Department of Biology and Department of Physics, University of Hawaii at Manoa, U.S.A.
1

ABSTRACT
The glomeruli of the kidney perform the key role of blood filtration and the number of glomeruli in a kidney is correlated
with susceptibility to chronic kidney disease and chronic cardiovascular disease. This motivates the development of new
technology using magnetic resonance imaging (MRI) to measure the number of glomeruli and nephrons in vivo.
However, there is currently a lack of computationally efficient techniques to perform fast, reliable and accurate counts of
glomeruli in MR images due to the issues inherent in MRI, such as acquisition noise, partial volume effects (the mixture
of several tissue signals in a voxel) and bias field (spatial intensity inhomogeneity). Such challenges are particularly
severe because the glomeruli are very small, (in our case, a MRI image is ~16 million voxels, each glomerulus is in the
size of 8~20 voxels), and the number of glomeruli is very large. To address this, we have developed an efficient Hessian
based Difference of Gaussians (HDoG) detector to identify the glomeruli on 3D rat MR images. The image is first
smoothed via DoG followed by the Hessian process to pre-segment and delineate the boundary of the glomerulus
candidates. This then provides a basis to extract regional features used in an unsupervised clustering algorithm,
completing segmentation by removing the false identifications occurred in the pre-segmentation. The experimental
results show that Hessian based DoG has the potential to automatically detect glomeruli,from MRI in 3D, enabling new
measurements of renal microstructure and pathology in preclinical and clinical studies.

Keywords: Glomeruli detection, Hessian Analysis, Scale Space, Unsupervised Learning

Medical Imaging 2015: Image Processing, edited by Sébastien Ourselin, Martin A. Styner, Proc. of SPIE Vol. 9413,
94132N · © 2015 SPIE · CCC code: 1605-7422/15/$18 · doi: 10.1117/12.2081484

Proc. of SPIE Vol. 9413 94132N-1
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

1. INTRODUCTION
Magnetic resonance imaging (MRI) is an important tool for investigating tissue microstructure. The emerging field of
molecular MRI develops tools to localize molecules and cells within the body. Recently, nanoparticle contrast agents
have been developed that deliver a large payload to a specific site within the tissue, changing the image where they
accumulate and allowing detection of the labeled molecules or cells. One type of biologically- based class of MRI
detectable nanoparticles is based on the natural or modified iron storage protein, ferritin. Ferritin is superparamagnetic
and readily functionalized for targeted molecular imaging. Cationic ferritin (CF) nanoparticles, shown to bind to
anionic proteoglycans in the glomerular basement membrane after intravenous injection, can be detected with T2*weighted MRI in 3D. This technique can be used to detect, measure, and count every perfused glomerulus in the whole
kidney ex vivo and in vivo, enabling estimates of whole kidney nephron endowment and structural damage[1, 2]. While
exciting, the widespread use of this technique for preclinical and clinical studies is compromised due to the lack of image
processing tools to reliably and accurately segment glomeruli in the MR images.
The shape and the intensity distribution of CF-labeled glomeruli in MRI motivate us to treat this as a blob detection
problem, which has been extensively investigated. The detectors can be generally categorized as supervised vs.
unsupervised detectors. The unsupervised approach is of the interest in this paper since supervised methods require
intensively manually labeling, which is prohibitive in 3D kidney glomerular images. Various unsupervised blob
detectors have already been developed, but most of them are restricted to 2D medical images due to computational
limitations. Among 2D blob detectors, the most investigated may be the Laplacian of Gaussian (LoG) [3]. LoG is based
on the scale-space theory, where a 2D image or slice of a 3D image is considered as a series of images, with each being
controlled by a specific scale parameter t. For images having objects with varied sizes, multiple scales exist. Using a
Gaussian kernel, the image can be transformed by convolution with respect to the scales to preserve spatial properties[4].
Since as t increases, the number of local minima in a dark blob does not increase and the number of local maxima in a
bright blob does not decrease, a diffusion process may apply to retrieve the blobs from the image. Detectors generated
via LoG kernels have been successfully implemented for 2D blob detection [3], but, the symmetric nature of the LoG
detector limits its application to detect rotationally asymmetric blobs. To address this, a generalized form of LoG (gLoG)
is proposed [5] that employs different Gaussian kernels. However, both detectors identify the centroids of the blob first,
and a regular ellipse with an estimated radius is then superimposed on each centroid to derive necessary measurements.
A foreseeable issue with this approach is robustness. Specifically, the derived measurements are from an estimated
region (regular ellipse) instead of the true region (precise shape). Such estimation would be problematic for MR images,
which have considerable local noise. To overcome this issue, Zhang et. al [6] first developed a 2D detector - Hessianbased Laplacian of Gaussian (HLoG) detector to accurately detect and delineate blob shapes. HLoG extracts regional
blob features for an unsupervised learning algorithm to detect blobs, therefore it is robust to local noise and can provide
accurate detection. However the computational burden of LoG transformation limits its application in 3D blob detection.
To address these outstanding challenges, we propose an efficient four-phased image processing framework for highthroughput assessment of CF-labeled glomeruli in 3D MRI images, termed the Hessian-based Difference of Gaussians
(HDoG). In Phase I, the raw images are smoothed/transformed into the DoG space, which is an approximation of LoG
but can be efficiently derived to attenuate noise and highlight the blob (glomerular) region. Next, in Phase II, we use the
convexity property of the Hessian Matrix to quickly identify candidate glomerular regions from the whole image. Phase
III focuses on feature extraction. Other than the features derived from the intensity (average intensity and gradient), we
introduce two new features: regional blobness and regional flatness to characterize the local geometric information of the
glomerulus. In Phase IV, a Variational Bayesian Gaussian Mixture Model [7] is implemented on the three features to
finalize glomerulus identification. We test the applicability of HDoG using three rat MR images. The “gold standard”
stereology counts and acid maceration counts, are then used to compare our counts and the result shows that HDoG can
accurately and efficiently count the glomeruli in 3D MR images.

Proc. of SPIE Vol. 9413 94132N-2
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

2. METHODS
M
As shown in Figure 1, the proposed
p
HDoG
G analysis connsists of (1) DooG transformatiion, (2) Hessiaan pre-segmenttation, (3)
feature extracction, and (4) post-pruning.
p
E
Each
phase is discussed
d
below
w.
ed Glomeruli

s

Hessian Pre Segmentation

approx.)

Th

Best Sca e Determined by

$ DoG(i.Y%t)/(i.Y;
arel

Features Extraction:

Avenge intensity
Resolve" Blntrnesa

Amiens] Flatness

Erti.ru)

VBGMM cluster ng

Figurre. 1. Flowchart of the Hessian-bbased differencee of Gaussians (H
HDoG) analysis..

2.1 Phase I:: DoG approxiimation of LoG transformaation
In this study,, the glomeruli in 3D MRI aree roughly spheerical in shape and are darkerr than the surrooundings. We choose
c
to
use DoG to smooth
s
the imaages, filter the noises and havve the sphericaal shape small objects
o
retrieveed. This is becaause it is
a computatioonally efficientt approximation of the LoG filter known too be able to sm
mooth the image noise by ennhancing
the objects att the selected scale.
If we let a 3D
D image be	 :
→ , the scale-space repreesentation ( , , ; ) at poinnt ( , , ) withh scale parameeter	 	is
the convolutiion of image ( , , ) with the
t Gaussian keernel ( , , ; ):
( , , ; )= ( , , ; )∗ ( , , )
Where ∗ is thhe convolution operator and ( , , ; ) =

(

) 	

). Thhe Laplacian off ( , , ; ) iss:

exp	(−
−

( , , ; )=
Since

( , , ; )=

(1)

,

+

+

.

(2)

( , , ; )/ ,	w
we have the DooG approximattion of LoG is

( , , ; ) = lim

( , , ; +

)− ( , , ; )

( , , ; +

)− ( , , ; )

→

( , , )∗

( , , ;

)

( , , ; )

.

(3)

To locate thee optimal scale for the blobs, -normalizatioon is added to the
t DoG detecttor as the norm
malized DoG deetector
( , , ; ) resulting the
t approximattion of normaliized LoG [3] ass:
( , , ; )=

( , , )∗

( , , ;

)

( , , ; )

.

This normalized DoG transsformation undderlies the Hesssian analysis we
w use for pre-ssegmentation, described
d
next..

Proc. of SPIE Vol. 9413 94132N-3
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

(4)

2.2 Phase II: Hessian pre-segmentation
After the DoG transformation (Phase I), each darker glomerulus is transformed to a bright blob, which is in a concave
elliptical shape, (brightness is faded isotropically). Every voxel within the blob is concave elliptic.
Hessian for Blob Identification:
Since the eigenvalues of the Hessian matrix of a blob-like structure have been used to describe the structure’s geometry,
we use this information to identify the transformed glomeruli using the following proposition:
Proposition 1. In a transformed, 3D, normalized DoG image, every voxel of a transformed gloermulus has a negative
definite Hessian matrix.
Proof.
Let the Hessian matrix for voxel ( , , ) after the transformation be:

( , , ; ) =

( , , ; )

( , , ; )

( , , ; )

( , , ; )

( , , ; )

( , , ; )

( , , ; )

( , , ; )

( , , ; )

.

(5)

Given the geometric classification of a voxel [8] and specific orientation patterns [9], if voxel ( , , ) is concave
elliptical, all the eigenvalues , , 	of ( , ; ) are negative, meaning
< 0,
< 0, and
< 0. Since every
voxel in the transformed glomerulus is concave elliptical, its eigenvalues are all negative, so the Hessian matrix of the
voxel is negative definite.∎
Proposition 1 provides one necessary but not sufficient property that a voxel in a transformed glomerulus must satisfy: If
a voxel resides in a transformed glomerulus, the Hessian matrix of the voxel is negative definite. But, every voxel that
has a negative definite Hessian matrix may not be from a transformed glomerulus. This proposition ensures that all true
glomeruli are identified in the group of glomerular candidates defined by Definition 1.
Definition 1. A glomerulus candidate
in normalized DoG space is a 6-connected component of set
( , , ; ), ( , , ; ) = 1} , where ( , , ; ) is the binary indicator such that if the
= {( , , )|( , , ) ∈
voxel ( , , ) has a negative definite Hessian matrix, then ( , , ; ) = 1; otherwise, ( , , ; ) = 0.
The definiteness of the Hessian matrix can be assessed by the three leading principal minors rather than by its
( , , ; ) , we
eigenvalues. Specifically, if ,
be the 1st, 2nd and 3rd leading principal minors of matrix
conclude the Hessian matrix is negative definite if and only if
> 0,
> 0		 and
< 0. Following Proposition and
Definition 1, we are able to identify all the blob objects having negative definite Hessian matrices. These blob objects
consists of the superset which theoretically, contains all the true glomeruli and some false identifications.
Hessian for Scale Selection:
In addition to identifying blob candidates, Hessian analysis can be used to determine the optimal scale parameter t. For
blobs in different scales, Lindeberg [3] uses the maximum normalized LoG (trace of Hessian) to select the optimum
scales across scale-space for each individual blob. Specifically, each blob achieves the most saliency at the scale at
which its average of LoG reaches the maximum. Since only blobs of similar sizes are discussed in this paper, a single
scale can be selected to approximate the size of all blobs. The maximum value of averaged normalized DoG
(approximation of LoG) is used here to automatically determine the single optimum scale for small blobs. Let the image

Proc. of SPIE Vol. 9413 94132N-4
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

be transformed to a normalized multi-scale DoG space representation. To determine the optimum scale of the blob
candidates, let the average DoG value per candidate pixel measure C be:
( )=

∑( , )

( , , ; ) ( , , ; )

(6)

∑( , ) ( , , ; )

Where ( , , ; ) is the binary indicator defined in Definition 1. As 	C (t) increases, the blob candidates are more
salient against their background. Therefore, the optimum scale
can be calculated as:
=
Using t

( )

(7)

,	 the raw image is transformed into a single-scale DoG space from which the blob candidates are populated.

2.3 Phase III: Feature extraction
Blob detectors often employ geometric features, among which two classic ones are , the likelihood of blobness, and
, flatness [9]. These features are based on solving the eigenvalues (assuming	| | ≤ 	 | | ≤ | |) of the Hessian matrix
at each voxel. That is,
=
=

|

|
|		

|

+

																																

(8)

+

(9)

For an idealized blob (3D ball), | | = | | = | |,
= 1.		On the other extreme, i.e.
≪ 1, the structure could either
be a line structure or a planar structure [9].
measures the magnitude of saliency of the objects comparing to the
background.
and	 , eigenvalues of the Hessian matrix must be solved at each voxel, which is computational
To calculate
prohibitive for 3D MR. In this research, we propose two new features based on the blobness and flatness concepts to
describe blobs. These features are based on the regional Hessian matrix evaluated at each glomerulus candidate instead
of each voxel.
Give region T, voxel ( , , ) ∈ , the regional Hessian matrix(after the DoG transformation) is:

(

( , ; )) = ∑(

, , )∈

( , , ; )

( , , ; )

( , , ; )

( , , ; )

( , , ; )

( , , ; )

( , , ; )

( , , ; )

( , , ; )

. (10)

Eq. (10) is the summation of all the Hessian matrices of the voxels within the region (similar to the Hessian-affine
detector in [10]). This matrix describes the second-order derivative distribution within the region of the glomerulus
candidate, and thus can be used to describe the geometric properties of the glomerulus candidate. To avoid confusion, we
use , , .
To efficiently measure the likelihood of blobness, we used a modified version of Eq. (8):
=|

×|
| |

|
| |

|

Proc. of SPIE Vol. 9413 94132N-5
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

(11)

Where , , ′ are eigenvalues of the regional Hessian matrix . Since the Hessian matrix is negative definite at
every voxel within a blob candidate, the sum of the Hessian matrix over a blob candidate is negative definite, meaning
, , < 0. Thus,

Where
of :

(

)=

′ ′+

′ ′+

(

′ ′. By [11],

′ ′+

′ ′ = det

(

×|

=

(

)|

,

)

(12)

) can be obtained by calculating three 2×2 principal minors
,

′ ′+

|

×|

=

,

,

,

,

+ det

,

,

+ det

,

,

,

,

,

	 . (13)

To calculate the flatness over blob candidate , we have
=

( ′+

′+

′) − 2( ′ ′ +

′ ′+

′ ′) =

(

) −2×

(

) . (14)

These two modified features greatly reduce the computational cost of Eqs. (8) and (9), because (1)
and
are based
on the regional Hessian matrix evaluated at each blob region instead of at every voxel, and (2)
and
only require us
to calculate the trace and determinant values rather than the roots (eigenvalues) of the Hessian matrix in Eqs. (8) and (9).
Overall, we derived three regional features of blobs: , , and
(the average intensity of region ).
and
are
novel because they can be calculated quickly, which has remained a challenge for detecting glomeruli in 3D MR images.
We then input these three features into a clustering algorithm, the variational Bayesian Gaussian mixture Model
(VBGMM), to remove the false identifications from the glomeruli candidate pool. The details of the HDoG algorithm is
listed in Table 1.
TABLE 1. DETAIL STEPS OF HDOG
1. Initialize the normalize factor , range and step-size of parameter to transform the raw image
into normalized DoG space
2. Binarize each section of normalized DoG space with the negative definite Hessian (for dark small
blob in raw image).
∑ ∑

( , ; ) ( , ; )

and find optimum scale section
3. Calculate average DoG intensity ( ) =
∑ ∑ ( , ; )
by	
=
( ).
and extract the regional features
in raw image
4. Choose the optimum scale section =
space and 	 , in DoG space
5. Input those three features to variational Bayesian Mixture Models with 2 clusters setting
as final segmentation
6. Choose the cluster with higher value of

3. EXPERIMENTAL RESULT
Six 3D MR images of CF-labeled rat kidneys were studied. After obtaining glomerular counts using HDoG, we compare
our counts with counts obtained using a manual acid maceration method [12], which uses acid to extract glomeruli from
kidney tissue, as well as to the disector-fractionator stereological method [13], whereby we estimate the number of
glomeruli by analyzing pairs of histological sections. These methods are established non-imaging techniques for
estimating glomerular number, so we consider them to be “ground truth” data.

Proc. of SPIE Vol. 9413 94132N-6
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

All rat studies were approved by the institutional animal care and use committee, consistent with the NIH Guide to the
Care and Use of Laboratory Animals. A 19T NMR with a DOTY 3-axis imaging probe was used to scan rat kidneys. The
total scan tim
me was 6 h perr kidney with a 3D GRE seqquence with TE
E/TR = 7/40 ms
m and a resoluution of 62×62×
×78 μm3
and a re-sliceed matrix sizee of 256×256×2256. Those sixx rat kidney MR
M images are counted usingg the HDoG algorithm
coded in Maatlab, and the result of the glomerular coounts obtained with each meethod (HDoG,, acid macerattion, and
disector-fracttionator stereollogy) for the siix kidneys is shhown in Figuree 2, and the com
mputational tim
me is listed in Table
T
2.
TABLE 2 COMPUTATION TIME FOR HDOG (INTEL XEON 2.00 GHZ CPU AND
D 32 GB OF MEMO
ORY)
Rat

CF1

C
CF2

CF33

CF4

CF5

CF6

Avg std

Time
(seconds)

268

2
294

2422

243

237

242

255 20

From Table 2 and Figure 2,
2 we observe that
t
HDoG connsistently identtifies glomerulli in all six kiddneys very fast,, and the
three methodds generally ag
gree well. Thee Student’s t-teest shows therre is no signifficant differencce between ouur counts
against the golden
g
standard
d counts. Our histology expperts confirmedd that the HDooG accurately detects the gllomeruli.
Figure. 3 shoows the segmeentation resultss on representaative axial view
w slices of the six rats, wherre slice 100 (off 256) is
shown for ratts CF1, CF2, and
a CF3, and slice 150 (of 2556) is shown foor rats CF4, CF
F5, and CF6. From
F
the figuree, we see
that most gloomeruli are identified and contoured in grreen. After com
mparing countt numbers andd visually checcking the
accuracy, wee conclude that HDoG can auutomatically ideentify glomeruuli in rat MR im
mages.

Figure 2. Gloomerulus countss for six rat kiddneys using the HDoG, acid maceration,
m
and stereology 1 methhods ( =2). Too test the
statistical signnificance, two-taailed student t-ttest is performeed to test the detector
d
with thhe two golden standard
s
counts, with pvalue=0.243 (H
HDoG v.s. Acid
d Maceration) annd p-value= 0.8115(HDoG v.s. Stereology),
S
resppectively, which indicates that thhere is no
significant diffference between
n the counts.

1

Only 3 CF rats
r were coun
nted by stereoloogy.

Proc. of SPIE Vol. 9413 94132N-7
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

4. CO
ONCLUSION
N
We propose a computation
nally efficient detector term
med HDoG forr labeling small objects, succh as CF-labelled renal
glomeruli, frrom 3D MRI im
mages. First, we
w smoothed thhe images via DoG. Then we
w conducted thhe Hessian anaalysis on
the DoG trannsformation, wh
hich (1) ensurees that all true glomeruli
g
are contained
c
withhin the glomeruulus candidate pool,
p
(2)
outlines the boundaries of each candidatte (3) determinnes the optimuum scale of thhe glomeruli. Next,
N
we derivved three
regional featuures and fed th
hem into a clustering algorithhm, the variatiional Bayesiann Gaussian mixxture model, too remove
any false gloomeruli from th
he candidate poool. The glomerular counts obtained
o
with HDoG
H
are com
mpared with thhe counts
obtained usinng design-baseed stereology and
a acid macerration. To conclude, HDoG automatically and accuratelyy labeled
glomeruli in a reasonable time
t
frame (<55 mins per rat MRI). Thus, HDoG
H
may bee a powerful cllinical tool thaat can be
used to identify glomeruli non-invasively
in vivo, enabliing studies of chronic
n
c
kidneyy and cardiovasscular disease.

Figure 3. Glom
merular segmenttation results froom 3D MR imagges of rat kidneyss (selected slicess presented). (a––c) Slice 100 for rats CF1,
CF2, and CF3. (d–f) Identificaation results for (a–c),
(
respectiveely. Identified glomeruli are conttoured in green. (g–i) Slice 150 for
f rats
CF4, CF5, andd CF6. (j–l) Iden
ntification resultss for (g–i), respeectively, where iddentified glomerruli are contoureed in green.

Proc. of SPIE Vol. 9413 94132N-8
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

ACKNOWLEDGMENTS
This work is supported by NIH DK091722, and a grant from the NIH Diabetic Complications Consortium.

References
[1] S. C. Beeman, M. Zhang, L. Gubhaju et al., "Measuring glomerular number and size in perfused kidneys using MRI,"
American Journal of Physiology - Renal Physiology, vol. 300, no. 6, pp. F1454-F1457, (2011).
[2] S. C. Beeman, L. A. Cullen-McEwen, V. G. Puelles et al., "MRI-based glomerular morphology and pathology in
whole human kidneys," American Journal of Physiology-Renal Physiology, vol. 306, no. 11, pp. F1381-F1390, (2014).
[3] T. Lindeberg, "Feature detection with automatic scale selection," International journal of computer vision, vol. 30, no.
2, pp. 79-116, (1998).
[4] T. Lindeberg, [Scale-space theory in computer vision], Springer, (1993).
[5] H. Kong, H. C. Akakin, and S. E. Sarma, "A Generalized Laplacian of Gaussian Filter for Blob Detection and Its
Applications," IEEE Transactions on Cybernetics, vol. PP, no. 99, pp. 1-15, (2013).
[6] M. Zhang, T. Wu, and K. Bennett, "Small Blob Identification in Medical Images Using Regional Features from
Optimum Scale," IEEE Transactions on Biomedical Engineering, (2014).
[7] C. M. Bishop, [Pattern recognition and machine learning], springer, New York, (2006).
[8] A. H. Salden, B. M. t. H. Romeny, M. A. Viergever et al., "Differential Geometric Description of 3D Scalar Images,"
Internal Report 3DCV, vol. 91-05, (1991).
[9] A. Frangi, W. Niessen, K. Vincken et al., "Multiscale vessel enhancement filtering," Medical Image Computing and
Computer-Assisted Interventation — MICCAI’98, Lecture Notes in Computer Science W. Wells, A. Colchester and S.
Delp, eds., pp. 130-137: Springer Berlin Heidelberg, (1998).
[10] K. Mikolajczyk, T. Tuytelaars, C. Schmid et al., "A Comparison of Affine Region Detectors," International Journal
of Computer Vision, vol. 65, no. 1-2, pp. 43-72, (2005).
[11] C. D. Meyer, [Matrix analysis and applied linear algebra], Siam, (2000).
[12] J.-P. Bonvalet, M. Champion, F. Wanstok et al., "Compensatory renal hypertrophy in young rats: Increase in the
number of nephrons," Kidney Int, vol. 1, no. 6, pp. 391-396, (1972).
[13] J. F. Bertram, M. C. Soosaipillai, S. D. Ricardo et al., "Total numbers of glomeruli and individual glomerular cell
types in the normal rat kidney," Cell and tissue research, vol. 270, no. 1, pp. 37-45, (1992).

Proc. of SPIE Vol. 9413 94132N-9
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

NeuroImage 59 (2012) 2298–2306

Contents lists available at SciVerse ScienceDirect

NeuroImage
journal homepage: www.elsevier.com/locate/ynimg

A prior feature SVM-MRF based method for mouse brain segmentation
Teresa Wu a,⁎, Min Hyeok Bae a, Min Zhang a, Rong Pan a, Alexandra Badea b
a
b

School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe, Arizona 85287-5906, USA
Center for In Vivo Microscopy, Box 3302, Duke University Medical Center, Durham, NC 27710, USA

a r t i c l e

i n f o

Article history:
Received 1 June 2011
Revised 26 August 2011
Accepted 22 September 2011
Available online 1 October 2011
Keywords:
Automated segmentation
Magnetic resonance microscopy
Markov Random Field
Mouse brain
Support Vector Machine

a b s t r a c t
We introduce an automated method, called prior feature Support Vector Machine-Markov Random Field
(pSVMRF), to segment three-dimensional mouse brain Magnetic Resonance Microscopy (MRM) images.
Our earlier work, extended MRF (eMRF) integrated Support Vector Machine (SVM) and Markov Random
Field (MRF) approaches, leading to improved segmentation accuracy; however, the computation of eMRF is
very expensive, which may limit its performance on segmentation and robustness. In this study pSVMRF
reduces training and testing time for SVM, while boosting segmentation performance. Unlike the eMRF
approach, where MR intensity information and location priors are linearly combined, pSVMRF combines
this information in a nonlinear fashion, and enhances the discriminative ability of the algorithm. We validate
the proposed method using MR imaging of unstained and actively stained mouse brain specimens, and
compare segmentation accuracy with two existing methods: eMRF and MRF. C57BL/6 mice are used for training and testing, using cross validation. For formalin ﬁxed C57BL/6 specimens, pSVMRF outperforms both eMRF
and MRF. The segmentation accuracy for C57BL/6 brains, stained or not, was similar for larger structures like
hippocampus and caudate putamen, (~ 87%), but increased substantially for smaller regions like susbtantia
nigra (from 78.36% to 91.55%), and anterior commissure (from ~ 50% to ~ 80%). To test segmentation robustness against increased anatomical variability we add two strains, BXD29 and a transgenic mouse model of
Alzheimer's disease. Segmentation accuracy for new strains is 80% for hippocampus, and caudate putamen,
indicating that pSVMRF is a promising approach for phenotyping mouse models of human brain disorders.
© 2011 Elsevier Inc. All rights reserved.

Introduction
Precise delineation of human neuroanatomical structures helps in
the early diagnosis of a variety of neurodegenerative and psychiatric
disorders (Fischl et al., 2002). The importance of human brain
segmentation has given great momentum to the development of segmentation methods, and considerable progress has been made. In the
meantime, the study of mouse models has also drawn substantial
attention of the biomedical community due to the close evolutionary
relationship between humans and mice, which enables scientists to
use mouse mutants as models of human neurological disease, and
to understand structural and functional changes of human brains
(Bock et al., 2006; Kovacevic et al., 2005). For example, transgenic
mouse models which mimic neurodegenerative diseases were
investigated to study the functions of particular genes or other
defects, and to test novel therapeutic interventions (McDaniel et al.,
2001). However, developing automated segmentation methods for
mouse brain MR images is a difﬁcult task. First, the MR signal is proportional to the voxel volume (Edelstein et al., 1986), around 1 mm 3
for the human brain, but more than 100,000 times smaller in higher

⁎ Corresponding author. Fax: +1 480 965 2751.
E-mail address: teresa.wu@asu.edu (T. Wu).
1053-8119/$ – see front matter © 2011 Elsevier Inc. All rights reserved.
doi:10.1016/j.neuroimage.2011.09.053

resolution (21.5 μm) mouse brain images necessary to resolve detailed anatomical features. Improvements in imaging technology,
complemented with the use of T1 shortening contrast agents (Badea
et al., 2007; Dorr et al., 2008; Johnson et al. 2002) have allowed the
segmentation of more than 30 mouse brain structures based on MR
images (Kovacevic et al., 2005; Ma et al., 2005; Badea et al., 2007;
Dorr et al., 2008). These large image arrays (e.g. 1024 × 512 × 512 voxels, Badea et al., 2007) pose increasing computational demands.
Second, most studies using mouse models require large numbers of
animals to achieve statistical power for detecting subtle variations in
neuroanatomy. This requirement translates into a pressing need for
the development of high-throughput segmentation methods for 3D
brain images. The segmentation results should be robust, consistent
and with acceptable computational time. To handle these challenges,
we need to develop an automated mouse brain image segmentation
method that is accurate, reliable and fast.
Previous research on developing automated segmentation methods
for human and mouse brain images includes atlas based segmentation,
probabilistic information based segmentation, and machine learning
based segmentation. The atlas based segmentation method can involve
nonlinear registration of a manually labeled atlas image to a new image
set. The label of each voxel in the atlas image is elastically matched to
the image being segmented. The segmentation performance can be improved by using an average atlas obtained from multiple subjects

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

instead of a single subject (Rohlﬁng et al., 2004). Most existing methods
for mouse brain segmentation have used the atlas based segmentation.
Ma et al. (2005) used six-parameter rigid-body transformation, and
nonlinear registration to segment T2*-weighted MRM images of
C57BL/6 mouse brains into 20 structures using an atlas image of a single
mouse brain. Kovacevic et al. (2005) used an average atlas for atlas
based segmentation of the MR images of 129S1/SvImJ mouse brain.
The probabilistic atlas based segmentation incorporates different
kinds of probabilistic information based on multi-spectral MR signals
(Fischl et al., 2002). The probabilistic information on MR intensity is
modeled as a Gaussian distribution. The prior probability of a label at
one voxel location in the 3D image provides the location prior, and
the pairwise probability of a labeling, given the labels of neighboring
voxels is deﬁned by the MRF theory. Ali et al. (2005) adapted Fischl's
method to segment T2, Proton Density (PD) and diffusion-weighted
MRM images of the C57BL/6 mouse brain into 21 neuroanatomical
structures.
Machine learning based segmentation was used in human brain
segmentation, and uses various classiﬁers to assign each voxel to a
number of classes. For example, Artiﬁcial Neural Network (ANN)
was used to segment MR images into three tissues types: white
matter, gray matter and cerebrospinal ﬂuid based on T1, T2 and PDweighted MR signal intensity (Reddick et al., 1997). Powell et al.
(2008) used probability map values, spherical coordinates, T1 and
T2-weighted MR signal intensity as input features for ANN and SVM
to segment MR images of human brains into eight structures. They
showed that machine learning based segmentation outperforms the
atlas or probability based segmentation methods. In our previous
work (Bae et al., 2010), we segmented MRM images of the C57BL/6
mouse brain into 21 neuroanatomical structures using an enhanced
SVM model, called Mix-Ratio sampling-based SVM (MRS-SVM),
which relieved the data imbalance problem in multiclass classiﬁcation. Only the location and MR intensity are used as features for the
SVM model. The results showed much improved performance compared to the atlas-based method and comparable classiﬁcation
performance to the probabilistic information based method for larger
structures (Bae et al., 2010).
Each segmentation method has its drawbacks. In the case of the
atlas based segmentation, registration errors can severely hurt the
overall segmentation performance (Sharief et al., 2008) since a poor
registration can cause structure mismatches and boundary blurring.
The probability information based segmentation uses MR intensity
information and contextual information based on neighbors' labels,
as well as location information, which depend on the registration
quality. The additional information — MR intensity information and
contextual information, could make up for the loss of segmentation
performance resulting from imperfect registration. Therefore, the
probability information based segmentation is less affected by the
registration quality than the atlas based segmentation. This is why
MRF, a class of probability theory modeling contextual dependencies
has been widely applied for image segmentation (Li, 2009). However,
the probability information based segmentation methods use a weak
classiﬁer, multivariate Gaussian distribution, to model the MR intensity information (Ali et al., 2005; Fischl et al., 2002). The contribution
of MR intensity information to the segmentation is undermined due
to the poor discriminative power of the classiﬁer. We proposed a
hybrid of probability information–machine learning based segmentation, termed eMRF (Bae et al., 2009) where SVM is employed to replace the weak classiﬁer in the probability information based
method. In the eMRF method, the overall segmentation performance
was improved by employing SVM to model the MR intensity information instead of Gaussian distribution. Using manual labeling as gold
standard, the eMRF method overall provides 10.05% higher percentage voxel overlap (VOP) and 23.84% less label volume difference
(VDP) compared with the atlas based segmentation, and 2.79% higher
percentage voxel overlap and 12.71% less label volume difference

2299

compared with the probability information based segmentation.
Note for labeling overlap, higher is better, for label volume difference,
less is better. While the machine learning based segmentation improves the segmentation accuracy, it requires enormous computation
time. The long training and testing time and the difﬁculty in model
parameter selection limit the practical application of the method to
large data-sets and large samples. Powell et al. (2008) reported that
it took a day to train a neural net for the classiﬁcation of one structure
from others even though they used a random sampled data
(500,000 voxels per structure) instead of using the whole data set.
It is known that the training time for SVM is approximated as O(N 4)
where N is the total number of training data points. For mouse
MRM images (128 × 128 × 256), N is over 16 million. Hence, in the
eMRF study, it took ~ 7 days for training and the 4.82 h for testing
using a 3.4-GHz PC. The classiﬁcation performance of classiﬁers largely depends on the selection of model parameters (e.g. kernel
functions and related parameters for SVM). To ﬁnd the best model
parameters for a data set, additional large number of runs with different parameter settings should be conducted. However, the long
training and testing time for the brain image segmentation make it
prohibitive to run large number of experiments, which implies that
the best performance of the machine learning based segmentation
would be difﬁcult to obtain due to computing concerns. The robustness of the algorithm for mutant mice which has large anatomical
variability is also difﬁcult to be assessed.
In this study, we develop a new algorithm that samples fewer
voxels, enabling the identiﬁcation of optimal parameters for the machine learning classiﬁer. This new algorithm is called prior feature
SVM-MRF (pSVMRF) which is robust and computationally efﬁcient.
pSVMRF integrates the good classiﬁcation ability of SVM into the MRF
image segmentation framework. Both voxel location prior and MR
intensity are used as input features for the training and testing of
SVM. Adding the location prior as the input features is inspired by
the previous work of Powell et al. (2008). The probabilistic outputs of
the prior feature SVM (pSVM) are treated as inputs to the MRF segmentation formula. The contribution of the SVM and contextual information is controlled with two model parameters. This is different from
the eMRF method, where the MR intensity information and location
prior are combined linearly by weights that are tuned by grid-search.
Since in the new approach the training sample size is small in each
experiment, we can easily run a large number of experiments to ﬁnd
the best SVM parameters to give the best and robust classiﬁcation
performance.
We assess segmentation performance and compare the new segmentation method with two other methods: MRF (Ali et al. 2005),
and eMRF (Bae et al., 2009) for the segmentation of MRM brain images
of adult C57BL/6 mice. To test the robustness of the algorithm when
faced with increased anatomical variability, we add two different
strains: BXD29, a recombinant inbred strain derived from an intercross
between C57BL/6 and DBA/2J, and a double transgenic mouse model of
Alzheimer's Disease (AD), overexpressing mutant amyloid precursor
protein (Jankowsky et al., 2005).
Methods
MRF based image segmentation
The contextual dependency is a general and meaningful way to
model the spatial property (Zhang et al., 2001). MRF theory is a
class of probability theory for modeling the contextual dependencies
of physical phenomena such as image pixels and correlated features. It
has become increasingly popular in many image segmentation
problems and image reconstruction problems. In the ﬁeld of medical
image segmentation, MRF has been used for brain tissue segmentation
(Awate et al., 2006; Held et al., 1997; Zhang et al., 2001), neuroanatomical structure segmentation (Ali et al., 2005; Fischl et al., 2002),

2300

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

detection of microcalciﬁcation in digital mammograms (Yu et al., 2006)
and detection of multiple sclerosis lesions in MR images (Khayatia et
al., 2008), etc.
Let S = {1, 2, …, n} be the set of sites in a image, X be a vector of site's
signal, and Y be the associated labeling vector, that is, X = {xi, i ∈ S} and
Y = {yi, i ∈ S}. Let N be a neighborhood system deﬁned as N = {Ni, i ∈ S}
where Ni denotes the set of sites neighboring site i. Y is said to be a
MRF on S with respect to a neighborhood system N if and only if
P ðY ÞN0 and P ðyi jyS−fig Þ ¼ P ðyi jyNi Þ

ð1Þ

for brain tumor recognition (Luts et al., 2007), brain states classiﬁcation
of functional MRI (Mourao-Miranda et al., 2005), breast cancer detection in dynamic contrast-enhanced MRI (Levman et al., 2008), and
knee bone segmentation in MR images (Bourgeat et al., 2007).
The basic idea of SVM is to construct an optimal hyperplane which
gives maximum separation margin between two classes. Assuming a binary classiﬁcation problem with a n dimensional training set xi ∈ Rn
with its label set yi ∈ {+1,−1}, where i = 1, 2,…, m. The hyperplane f
(x), that separates the given data, is deﬁned as:
T

where S-{i} denotes the set difference. The condition of (1) means that
only neighboring labels have direct interaction with each other, and the
joint probability P(Y) can be uniquely determined by its local conditional probabilities.
According to Hammersley–Clifford theorem (Li, 2009), the probability P(Y) of an MRF can be equivalently speciﬁed by a Gibbs distribution as follows:
P ðY Þ ¼



1
exp − ∑ Vc ðY Þ
Z
c∈C

ð2Þ

where Z is a normalizing constant and Vc(Y) is a clique potential function over all cliques c ∈ C. A clique c is a subset of sites in S that are all
neighbors of each other, and C is a set of cliques or the neighborhood
of the clique under study. The value of Vc(Y) depends on a certain conﬁguration of labels on the clique c. For the image segmentation problem,
the posterior probability of the label of a site, given speciﬁc signal, can
be formulated using Bayesian theorem and the Hammersley–Clifford
theorem as:
P ðY jXÞ ¼



1
exp ∑ logP ðxi jyi Þ þ ∑ Vc ðY Þ :
Z
c∈C
i∈S

ð3Þ

∑ logP ðxi jyi Þ in the right hand side in (3) is the sum of the log
i∈S

likelihood function of the given labeling for the site's signal. Usually
a multivariate Gaussian distribution is used for modeling P(X|Y) (Ali
et al., 2005; Fischl et al., 2002; Held et al., 1997; Zhang et al., 2001),
which is based on the assumption of Gaussian relationship between
features and labels. This assumption is too restrictive to model the
complex dependencies between features and labels in some cases.
By employing a machine learning classiﬁer, such as SVM, the performance of the MRF based image segmentation was improved since
SVM is generally better at modeling the complex dependencies due
to the virtue of the non-linear transformation (Bae et al., 2009; Lee
et al., 2005). The well-accepted generalization ability of SVM is
explained in next section.
Segmentation enhancement by SVM
SVM has received a lot of attention from the machine learning and
pattern recognition community due to the following reasons (Abe,
2005). First, SVM works well for classifying objects which are not
linearly separable. The objects are mapped from their input space into
a high-dimensional feature space by kernel transformations; thus
SVM can separate objects which are not linearly separable. Secondly,
SVM has good generalization ability. SVM attempts to maximize the
separation margin between the classes, so the generalization performance does not drop signiﬁcantly even when the training data are
scarce. In addition, SVM can achieve a global optimal solution because
it is solved with quadratic programming. Because of the generalization
ability of SVM, it has accomplished great success in a variety of
applications including fault detection, fraud detection, handwritten
character recognition, object detection and recognition, and text classiﬁcation. In the ﬁeld of medical image classiﬁcation, SVM has been used

f ðxÞ ¼ w ΦðxÞ þ b

ð4Þ

where w is the n dimensional normal vector perpendicular to the hyperplane, b is a bias term and Φ(xi) is a non-linear transformation
which maps the samples into a higher-dimensional dot-product space
called the feature space. The optimal hyperplane is obtained by solving
the following optimization problem:
m
1 T
Min w w þ C ∑ξi
2
i¼1


T
s:t: yi w ⋅Φðxi Þ þ b ≥1−ξi ; i ¼ 1; …; m:

ξi ≥0;

ð5Þ

i ¼ 1; …; m

where ξ = {ξ1, …, ξm} is a slack variable and C is the penalty parameter
which controls the balance between the model complexity and classiﬁcation error. The proper value of penalty parameter (C) is determined
by the training set to avoid overﬁtting. The non-negative slack variable
(ξi) allows (5) to always yield feasible solutions by relieving the
constraint of maximum margin.
The constrained optimization problem in (5) can be converted to an
unconstrained optimization problem by introducing the non-negative
Lagrangian multipliers αi, and the unconstrained optimization problem
is converted to a Lagrangian dual problem by introducing the Karush–
Kuhn–Tucker (KKT) condition. The optimal solution αi * of the dual
problem (Abe, 2005) yields the following optimal hyperplane:
m

!

f ðxÞ ¼ sign ∑ yi α⁎i K ðx;xi Þ þ b⁎

ð6Þ

i¼1

where xi are support vectors and K ðx; xi Þ is a kernel function deﬁned as
K ðx; xi Þ ¼ ΦðxÞT Φðxi Þ. The kernel function performs the nonlinear
mapping implicitly so that we can avoid the complexity of mapping
and the curse of dimensionality resulted from the nonlinear mapping.
Commonly used kernel functions are linear, polynomial and RBF
among which nonlinear kernel function, RBF has been recommended
in many studies. For example, a comparative study on SVM using fMRI
to decipher brain patterns concludes RBF outperforms linear SVM
signiﬁcantly (Song et al., 2011). Our study on Alzheimer's disease
(AD) diagnosis using MRI imaging indicates that RBF kernel outperforms both linear and polynomial kernels for differentiating AD patients
with normal individuals (Zhang et al., 2008). Therefore, in this study we
used the Radial Basis Function (RBF) kernel, deﬁned as follows:


2
K ðx; xi Þ ¼ exp −γ‖x−xi ‖ ; γN0

ð7Þ

where γ in (7) is a parameter related to the span of an RBF kernel.
Brain image segmentation by eMRF
In our previous research (Bae et al., 2009) we proposed eMRF method, in which we integrate three different types of information – MR
intensity, voxel location and contextual relationship with neighboring
voxels' labels – to improve the overall segmentation performance, and

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

the MR intensity information is modeled by an enhanced SVM, which
takes different sampling ratio for different brain structures. The three
pieces of information are linearly combined with the model parameters
which control their relative contributions. The model parameters are
determined through a training process to maximize the segmentation
performance. The experimental results from using the eMRF method
showed that the integration of the probability information based
segmentation and the machine learning based segmentation can improve the overall segmentation performance, compared with the atlas
based segmentation method and the MRF method (Ali et al., 2005).
This is because it takes advantage of the classiﬁcation ability of machine
learning classiﬁers, in addition to the virtue of the location information
and the contextual information of the probability information based
segmentation, which are critical information for classifying each voxel
in a 3D image into the multiple classes.
Even though employing machine learning classiﬁers for brain image
segmentation improves the overall segmentation performance, computation time remains a big challenge. As stated earlier, the eMRF method
requires long training and testing times due to the difﬁculty in selection
of SVM parameters. These drawbacks are mainly associated with the
large data size. The number of data points in a 3D MRM images with
the matrix of 128 × 128 × 256 is more than 4 million. Multiplied by the
number of training sets this number is ~16 million. The number of
data for training and testing directly affects the training and testing
time of SVM. Therefore it is desirable to use the minimum number of
data necessary to produce comparable classiﬁcation performance. The
key to solve the problem of the large data sets is to reduce the number
of the training data while maintaining the classiﬁcation performance of
classiﬁers. The pSVMRF method proposed in the next section is built
based on the eMRF method, but tries to reduce the training and testing
time while maintaining the segmentation performance.
The proposed segmentation method: pSVMRF
Let S = {1, 2, …, n} be the set of voxels in a 3D MR image, X be a
vector of voxels signal intensity, and Y be the associated labeling vector, that is, X = {xi, i ∈ S} and Y = {yi, i ∈ S}. A location prior vector of a
voxel, li, has m elements, where m is the number of structures to be
k
segmented and ∑m
k¼1 li ¼ 1. Let K be the set of the classes to which
voxels will be assigned, i.e. K = {1, 2, …, m}, and L = {li} is the collection of location prior vectors. The k th element of the location prior
vector of the voxel i is deﬁned as:
q

∑ of voxels labeled as k at location r ðiÞ
k

li ¼

j¼1

ð8Þ

q

where q is the number of the images in the training set and r(i) is
location function which informs us the location of the voxel i in the
3D image. Using Hammersley–Clifford theorem and the assumption
of P(Y) N 0 and P(X,L) N 0, the posterior probability of having a label
conﬁguration Y given a MR intensity vector X and a location prior vector L is formulated as follows:



P ðY jX; L Þ∝ exp w1 ∑ logAi ðyi ; xi ; li Þ þ w2 ∑ Vi yi ; yNi
i∈S

ð9Þ

i∈S

s:t: w1 þ w2 ¼ 1
where w1 and w2 are model parameters which control the contribution
of the two terms in (9) to the posterior probability P(Y|X,L). Based on
the MRF theory, the prior probability of having a label at a given site i
is determined by the label conﬁguration of the neighborhood of the
site i. The Hammersley–Clifford theorem enables us to calculate the
joint probability P(Y) as a sum of the clique potential functions. We
use a ﬁrst order neighborhood system of a 3D image as a clique,
which consists of the adjacent six voxels in the four cardinal directions

2301

in a plane and the front and back directions through the plane. The clique potential function Vi(yi,yNi), called contextual potential function, in
(9) will have a higher value when the number of neighbors that have
the same label increases. This function is, thus, deﬁned as



 ∑j∈N δ yi ; yj

  1 if y ¼ y
i
i
j
where δ yi ; yj ¼
Vi yi ; yNi ¼
0 if yi ≠yj
nðNi Þ

ð10Þ

where n(Ni) is the number of voxels in a neighborhood of site i.
The location information of a voxel in a 3D image after registration is
important for classiﬁcation of the voxel into the neuroanatomical
structures. Fischl et al. (2002) pointed out that if the image registration
does well, only small numbers of neuroanatomical structures are available at a given location in a 3D brain atlas and the location information
have anatomical meaning so that it can help in classiﬁcation. Powell et
al. (2008) included probability map values as input features for the
machine learning based segmentation. The probability map was created
by calculating the probability of a neuroanatomical structure being
located at a voxel location in the atlas space across all subjects in a training set. For example, given four subjects, if one out of four subjects labeled voxel i as structure k, and three out of four subjects labeled
voxel i as structure l, in the probability map for structure k, the value
for voxel i is 25% while the probability map for structure l will have
75% for voxel i. Separate probability maps for each structure were calculated and included as one of elements in the input vectors for the binary
classiﬁcation of ANN and SVM in Powell et al.'s experiment. Taking a
similar approach, pSVMRF employs a prior feature SVM (pSVM), which
includes the features from MR intensity and location prior vectors, for
simultaneously modeling the MR intensity information and location information. Similar to eMRF, OAO (One-Against-One) method is applied
to train SVM for classifying the kth class against the l th class since it is
more efﬁcient on large datasets than OAA (One-Against-All) and AAO
(All-At-Once) (Hsu and Lin, 2002). Thus, overall n*(n-1)/2 models will
be trained. In each SVM training, the location prior being derived from
the speciﬁc probability map with respect to the speciﬁc structure together with MR intensity for each voxel are being the input features
for the model.
Since pSVM performs a multiclass classiﬁcation of SVM, the number of elements in the location prior vector is identical to the number
of structures to be segmented. Each element of the location prior
vector represents the number of times a particular structure occurs
at a given location in all the brain images of the training set. The
location prior vector deﬁned in (8) can model all the probabilities of
the m structures being located at a speciﬁc location and be used as a
feature for the multiclass classiﬁcation. As explained earlier, SVM
can enhance linear separation by mapping the original input space
into a high-dimensional feature space using the nonlinear transformation. By adding the MR intensity information and the location
prior vector as input features, we anticipate that pSVM can boost the
separability with the power of nonlinear transformation of SVM.
The ﬁrst term, Ai(yi, xi, li), in (9) is called as the observation potential
function that models the MR intensity information and the location
information. To be incorporated into pSVMRF, the decisions made by
pSVM need to be probabilistic output. Platt (2000) proposed a method
for mapping the SVM outputs into posterior probability by applying a
sigmoid function. The observation potential function for voxel i is
deﬁned as follows, for class k:
Ai ðyi ; xi ; li Þ ¼ Pi ðyi ¼ 1jxi ; li Þ ¼

1
1 þ expðα fk ðxi ; li Þ þ βÞ

ð11Þ

where fk(xi,li) is the SVM decision function for class k, α and β are the
parameters estimated from the training data. That is, SVM model is
trained using the MR intensity (xi) and location (li) to determine
the belonging of voxel i to class k (yi = − 1, 1). Let us deﬁne a new

2302

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

training set (ti, xi, li), where ti here is the target probabilities deﬁned
as:
ti ¼

yi þ 1
2

ð12Þ

The parameters α and β can be found by solving the following
minimization problem (Platt 2000):
Min− ∑ ti logðpi Þ þ ð1−ti Þ logð1−pi Þ

ð13Þ

i

where pi is deﬁned in (11). We used MATLAB (Natick, MA) to solve
the optimization problem (13) and obtain the values of α and β
values for each class. The next step is to ﬁnd the label conﬁguration
Y⁎ that maximizes the posterior probability P(Y|X,L) in (11), i.e.,
Y  ¼ arg maxY P ðY jX; LÞ. This is known as the maximum a posterior
(MAP) solution. Because of the highly complicated interactions
among multiple labels, it is very difﬁcult to ﬁnd the optimal solution of the joint probability P(Y|X,L). We adopt a local search method called iterated conditional modes (ICM), which maximize local
conditional probabilities iteratively by using the greedy search in
the local optimization. It is expressed as


yi ¼ arg max P ðyi jxi ; li Þ
yi ∈Y

ð14Þ

The ICM algorithm sequentially updates yi(t) into yi(t + 1) by switching the different labels to ﬁnd the maximum value of P(yi|xi, li). We use
the MAP solution based on the location prior vector as the initial estimator y (0) of the ICM algorithm. In this study, the algorithm continues
until no improvement is made and the iteration which gives the best
solution and terminates the algorithm is the optimal terminating
point. We estimate the optimal terminating point from the training
process and apply the terminating points for predicting labels of
new testing data.
Results and discussion
Performance measurements
To estimate the performance of segmentation methods, we use
the two performance metrics: volume overlap percentage (VOP)
and volume difference percentage (VDP) (Ali et al., 2005; Fischl et
al., 2002). They are calculated by comparing the automated labeling
with the manual labeling (gold standard) of each voxel. Denote LA
and LM as labeling of the structure k by automated and manual segmentation respectively, and V(L) as a function which calculates the
volume of the labeling. VOP and VDP for a structure k are deﬁned as:
VOPk ðLA ; LM Þ ¼
¼

V ðLA ∩LM Þ
×100 and VDPk ðLA ; LM Þ
ðV ðLA Þ þ V ðLM ÞÞ=2

ð15Þ

jV ðLA Þ−V ðLM Þj
×100
ðV ðLA Þ þ V ðLM ÞÞ=2

VOP is the larger the better, VDP is the smaller the better. VOP is
more sensitive to the spatial difference of the two labels than the
volumetric difference, but VDP is more sensitive to the volumetric
difference. To estimate the overall segmentation performance of a
particular method, we use average VOP (AVOP) and average VDP
(AVDP), which are calculated by dividing the sum of VOP or VDP for
all structures by the number of structures.
Implementation of the segmentation method
We assessed the performance of pSVMRF using MRM images of
mouse brains acquired by the Center for In Vivo Microscopy, at Duke

University Medical Center, and previously used in Ali et al. (2005);
Sharief et al. (2008); Badea et al. (2010). T2-weighted MRM mouse
brain images from ﬁve formalin-ﬁxed C57BL/6 male mice (approximately 9 weeks in age) were used. Image acquisition parameters
were: TE/TR = 30/400 ms, bandwidth 62.5 kHz, ﬁeld of view=
12× 12× 24 mm and matrix size= 128 × 128 × 256, 86 μm isotropic
resolution. A 9-parameter afﬁne registration was applied to each
image. 21 manual labels were used as gold standard to evaluate
segmentation accuracy. Table 1 presents the 21 neuroanatomical
structures and abbreviations used in this study.
Two different strains were introduced to test the segmentation, a
BXD29 and an AD mouse models (Jankowsky et al., 2005). These
mice and an additional set of ﬁve C57BL/6 were actively stained
(Johnson et al., 2002), and imaged as described in Sharief et al.
(2008). Imaging consisted of two protocols: T1 weighted (3D spin
warp, TE/TR 5.2/50 ms, ﬁeld of view 11 × 11 × 22 mm, matrix size
512 × 512 × 1024), and MEFIC enhanced T2 weighted acquisitions
(3D CMPG, TR 400 ms, echo spacing 7 ms, 7 echoes, ﬁeld of view
11 × 11 × 22 mm, matrix size 256 × 256 × 512) (Sharief and Johnson,
2006) were used to provide intensity priors. T1 images were resampled to match the resolution (43 microns) of the T2 weighted
image set. Using both image channels, 33 manual labels were produced for the C57BL/6 brains, and a set of 7 labels was traced to test
the BXD and AD segmentation.
The implementation of the pSVMRF method consists of two steps.
The ﬁrst step is to build the pSVM models and test the models. The
pSVM models were trained using the randomly sampled training set,
consisting of 300 randomly selected data points from each of the
structures (all neuroanatomical structures to be deﬁned, and one
added miscellaneous structure). Each of the training and testing
data has the input feature vector, which consists of one feature for
T2-w MR signal intensity and additional N features (N = 22 for the
formalin ﬁxed, N = 34 for actively stained specimens) for the location
prior. As mentioned earlier, the selection of the penalty parameter
(C) and RBF kernel parameter (γ) greatly affects on the classiﬁcation
accuracy and the generalization ability of SVM. We conducted a grid
search to ﬁnd the best penalty parameter and the best RBF kernel
parameter using the ﬁve-fold cross validation, which can help in
avoiding the overﬁtting problem and estimating the generalization
ability. Each of the trained pSVM models was tested on each of the
testing data to calculate the observation potential function in (11).
The training and testing time of the pSVM models for the randomly
sampled mouse brain dataset (formalin ﬁxed) were 1.12 min, and
14.24 min respectively, using a 3.4-GHz PC and LibSVM for MATLAB
(Chang and Lin, 2001). Since SVM can transform the linear combination of the MR intensity and location prior vector to a nonlinear combination that can help in classiﬁcation, pSVM could train a better
model using a small number of data (6600 voxels for each formalin
mouse, 10,200 voxels for each actively stained mouse). In the eMRF
method (Bae et al., 2009), a large size of training set (472,100 voxels
per a mouse), which is over-sampled from some classes, was needed
for the SVM training. That results in a very long training and testing
time: 7.56 days for training and 4.82 h for testing.

Table 1
List of the 21 neuroanatomical structures and abbreviations.
Cerebral cortex (CORT)
Cerebral peduncle (CPED)
Hippocampus (HC)
Caudate putamen (CPU)
Globus pallidus (GP)
Internal capsule
Periacqueductal gray
(PAG)

Inferior colliculus
(INFC)
Medulla oblongata (MED)
Thalamus (THAL)
Midbrain (MIDB)
Anterior commissure (AC)
Cerebellum (CBLM)
Ventricular system (VEN)

Pontine nuclei (PON)
Substantia nigra (SNR)
Interpeduncular nuclues
(INTP)
Olfactory bulb (OLFB)
Optic tract (OPT)
Trigeminal tract (TRI)
Corpus callosum (CC)

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

The second step was to implement the ICM algorithm to calculate
the contextual potential function in (10) and the posterior probability
P(Y|X,L) in (9). We did a grid search over the range W ¼ f0:01≤
wi ≤0:99; ∑i wi ¼ 1 and i ¼ 1; 2g to ﬁnd the best model parameters,
which were chosen as w1 = 0.89 and w2 = 0.11 for observation and
contextual functions, respectively. During the grid search for model
parameters, a large number of the ICM implementations with the
different parameter values were performed. Each of the ICM implementations run until there was no change in labels assignment, but
the best solution was achieved at the ﬁrst iteration from every ICM
implementation. In this grid search, we tried to ﬁnd the model that
has the maximum AVOP and the minimum AVDP. Since one model
has the maximum AVOP and the other model has the minimum
AVDP, we could not ﬁnd a best model which satisﬁed both criteria.
Therefore, we calculated the margins of AVOP and AVDP compared
with those of the MRF method (Ali et al., 2005). Total margin,
which is sum of the two margins, was used as the criterion for
selecting the best model. Fig. 1 provides the plot of the total margin
vs. iterations of the ICM algorithm, with the best pSVMRF model of
w1 = 0.89 and w2 = 0.11. The maximum of the total margin was
achieved at the ﬁrst iteration within 11.86 min using a 3.4-GHz PC.
Therefore, we chose the ﬁrst iteration as the optimal terminating
point for this pSVMRF model. This optimal terminating point will
then be used for testing new mouse brain images. Since the ICM
algorithm is a local optimization algorithm and does not guarantee
a global optimal, the optimal terminating point should be determined
based on the model and data set. Estimating and using the optimal
termination point enables the ICM algorithm to converge much faster
at better solution.
Validation of the segmentation method
To validate the proposed method, pSVMRF, we ﬁrst test on MR images of ﬁve C57BL/6 mice using a ﬁve-fold cross validation. Each of
the ﬁve mice was used as the testing set while the remaining four
mice were used as the training set. The results are compared with
two existing methods: the MRF method (Ali et al., 2005) and the
eMRF method (Bae et al., 2009). In Table 2, the segmentation
performances of the three automated mouse brain image segmentation
method are estimated based on VOP and VDP. The performance
estimates in Table 2 are based on the average values from testing all
the mice using the ﬁve-fold cross validation. The upper rows include
VOP and the lower rows include VDP. A ‘+’ sign always means that
pSVMRF method outperforms the other methods for the speciﬁc

2303

structure and ‘−’ means that pSVMRF underperforms. pSVMRF
outperformed eMRF in 16 structures, there was no change in one
structure and a slight underperformance in 4 structures. Major improvements in segmentation performance were noted for the olfactory
bulbs (from 83% to 91%), pons (from 80% to 86%), and trigeminal tract
(from 74% to 82%). In comparison to MRF, pSVMRF outperformed in 14
structures, most notable in the cases of optic tract (53% to 73%), trigeminal tract (from 64% to 82%), and pons (73% to 86%). Table 3 presents
the comparisons of the overall segmentation performance and the
computation time of the three automated segmentation methods.
Overall pSVMRF outperforms the two existing methods. AVOP and
AVDP of pSVMRF are improved by 2.55% and 9.57% compared with
eMRF, and by 5.41% and 21.07% compared with MRF. The total testing
time of pSVMRF in MATLAB, which includes the testing time of pSVM
testing and the ICM algorithm, was 26.10 min, which is improved by
92.85% compared with the testing time of eMRF (364.4 min). The testing time of MRF (15 min) is less than pSVMRF. However, pSVMRF can
produce 26.48% (total margin from MRF) more accurate segmentation
than MRF by spending 16 min more. The proposed method, pSVMRF,
gives better segmentation than eMRF and MRF, within a short testing
time.
Even though pSVMRF outperforms eMRF in 16 structures out of 21,
eMRF is still better than pSVMRF in some small structures such as GP,
PAG, OPT and TRI. This results from the fact that eMRF use Mix-ratio
sampling based SVM (MRS-SVM; Bae et al., 2010) and an over-sampled
training set for some smaller structures to improve the classiﬁcation
performance for these structures. In contrast pSVMRF uses the same
number of training data from each of the structures regardless their
size. MRF is also better than pSVMRF in some small structures such as
GP, PAG, AC and INTP even though pSVMRF is better for most structures.
That is because MRF relies more on the contextual information, which
enhances the identiﬁcation of the smaller structures, for segmentation
than pSVMRF. The proposed method, pSVMRF, still needs to be improved
for the smaller structures.
The combination of higher resolution imaging and higher contrast
given by active staining boosted segmentation accuracy, relative to
that obtained for the formalin ﬁxed brains, as illustrated in Fig. 2 for
adult C57BL/6 mice. The percent voxel overlap (VOP) increased substantially for smaller structures like the anterior commissure (from
50.76% to 83.5%), corpus callosum (from 65.59% to 85.44%), substantia nigra (78.36 to 91.55%) and ventricles (72.56 to 81.72%). For hippocampus and caudate putamen the VOP values were more similar.
VOP changed from 87.07 to 87.67% for hippocampus, and increased
from 97.67% to 90.87% for caudate putamen.

Fig. 1. Convergence of the ICM algorithm with w1 = 0.89 and w2 = 0.11.

2304

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

Table 2
Comparison of segmentation performance of the pSVMRF, eMRF and MRF methods based on VOP and VDP for 21 structures. A. pSVMRF vs. eMRF based on VOP; B. pSVMRF vs. eMRF
based on VDP; C. pSVMRF vs. MRF based on VOP; D. pSVMRF vs. MRF based on VDP.
A
VOP

CORT.

CPED

HC

CPU

GP

ICAP

PAG

INFC

MED

THAL

MIDB

pSVMRF
eMRF
+/−

94.08
91.10
3.27

74.22
73.15
1.47

87.07
86.20
1.01

87.67
87.62
0.05

79.49
79.64
− 0.19

73.33
73.14
0.26

90.16
90.26
− 0.11

88.40
85.32
3.61

93.27
91.93
1.45

94.18
93.53
0.69

93.81
93.27
0.58

VOP

AC

CBLM

VEN

PON

SNR

INTP.

OLFB.

OPT

TRI

CC

pSVMRF
eMRF
+/−

50.76
50.76
0.00

96.45
92.68
4.06

72.56
71.83
1.01

86.04
80.03
7.50

78.36
78.98
− 0.77

72.41
71.61
1.12

90.99
82.50
10.29

73.62
68.67
7.20

82.19
73.83
11.33

65.59
65.75
− 0.23

B
VDP

CORT.

CPED

HC

CPU

GP

ICAP

PAG

INFC

MED

THAL

MIDB

pSVMRF
eMRF
+/−

3.69
3.34
− 10.52

2.31
8.32
72.27

5.13
5.56
7.72

3.57
3.98
10.19

9.34
7.54
− 23.85

11.03
11.05
0.14

4.01
3.55
− 12.93

4.93
5.26
6.33

7.62
7.12
− 7.05

2.34
2.48
5.82

3.08
2.97
− 3.72

VDP

AC

CBLM

VEN

PON

SNR

INTP.

OLFB.

OPT

TRI

CC

pSVMRF
eMRF
+/−

25.16
25.55
1.53

3.34
3.73
10.50

14.26
16.83
15.29

7.28
12.49
41.68

9.05
10.16
10.90

19.71
26.09
24.47

7.30
10.79
32.32

12.52
5.67
− 120.91

9.87
7.33
− 34.65

15.67
20.57
23.86

C
VOP

CORT.

CPED

HC

CPU

GP

ICAP

PAG

INFC

MED

THAL

MIDB

pSVMRF
eMRF
+/−

94.08
90.77
3.65

74.22
67.69
9.65

87.07
87.69
− 0.71

87.67
88.46
− 0.90

79.49
78.46
1.31

73.33
73.85
− 0.70

90.16
85.38
5.59

88.40
83.08
6.41

93.27
86.15
8.26

94.18
93.08
1.19

93.81
90.77
3.35

VOP

AC

CBLM

VEN

PON

SNR

INTP.

OLFB.

OPT

TRI

CC

pSVMRF
eMRF
+/−

50.76
55.38
− 8.35

96.45
93.08
3.63

72.56
70.77
2.53

86.04
73.08
17.73

78.36
68.46
14.47

72.41
76.92
− 5.87

90.99
84.62
7.53

73.62
53.08
38.70

82.19
63.85
28.73

65.59
71.54
− 8.31

D
VDP

CORT.

CPED

HC

CPU

GP

ICAP

PAG

INFC

MED

THAL

MIDB

pSVMRF
eMRF
+/−

3.69
4.35
15.16

2.31
12.17
81.04

5.13
6.96
26.25

3.57
6.96
48.66

9.34
7.83
− 19.38

11.03
13.04
15.43

4.01
2.61
− 53.80

4.93
4.35
− 13.39

7.62
10.43
26.95

2.34
3.48
32.73

3.08
4.35
29.14

VDP

AC

CBLM

VEN

PON

SNR

INTP.

OLFB.

OPT

TRI

CC

pSVMRF
eMRF
+/−

25.16
19.13
− 31.50

3.34
3.48
4.03

14.26
19.13
25.48

7.28
17.39
58.11

9.05
15.65
42.17

19.71
13.91
− 41.66

7.30
16.52
55.79

12.52
14.78
15.34

9.87
10.43
5.44

15.67
22.61
30.71

To test the robustness of the pSVMRF on mutant mice which has
large anatomical variability, we examined the performance of the segmentation in two new strains, BXD29 the APP/TTA mouse model of
AD, and contrasted it with the baseline accuracy for stained C57BL/6
mice, imaged using the same protocol, and using a full sampling strategy for training/classiﬁcation. We evaluated the segmentation qualitatively (Fig. 3) and quantitatively (Fig. 4).
Segmenting the overall brain is a very accurate process (N90%
VOP), even in strains other than the C57BL6 used for generating priors.
However, the increased anatomical variability introduced by new
strains resulted in overall decreased performance for a subset of structures including: hippocampus, caudate putamen, anterior commisure,
corpus callosum, substantia nigra and ventricles. When using 7 labels

Table 3
Comparisons of overall segmentation performances and computation time for the
pSVMRF, eMRF and MRF methods.

pSVMRF
eMRF
MRF

AVOP

AVDP

Testing time (min)

82.13
80.09
77.91

8.63
9.54
10.93

26.1
364.4
15.0

only during training, the larger structures such as hippocampus and
caudate putamen could be segmented with accuracy of ~ 80% and
greater. The hippocampus VOP was 94.11 ± 0.73% for C57BL6, vs.:
85.81 ± 1.19% for the other two strains, while for caudate putamen
VOP was 92.21 ± 0.71% for C57BL/6, and 83.48 ± 5.93% for the new
strains. Smaller white matter tracts and nuclei, and especially the
widely variable ventricles remain challenging for the automated segmentation task. VOP for the corpus callosum was 86.11 ± 10.06% for
C57BL/6, but 59.54 ± 6.06% for the additional strains, while for the
substantia nigra VOP was 64.31 ± 2.12% for C57BL/6, and 61.81 ±
10.0% in the other strains.
Multiple avenues exist to increase accuracy of the segmentation.
Improved registration, together with a denser sampling strategy,
has the potential to increase segmentation accuracy, while increasing
computational demands. For example the use of a full sampling strategy on C57BL/6 mice yielded VOP values of 92.38 ± 0.20% for Hc, versus 87.67 ± 0.86 for under-sampled data. Similarly for CPu the VOP
was 94.58 ± 0.9%, versus 90.87 ± 0.16%. However the VOP for other
structures, including ventricles did not increase using this strategy,
e.g. VOP for ventricles was 81.72 ± 0.19% for under-sampled strategy
but only 75.92 ± 4.33 for the full sampling strategy. We noted that a
denser parcellation of the brain yields in general better segmentation

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

Fig. 2. Increased segmentation accuracy was obtained for the higher resolution, actively stained sets, relative to the formalin ﬁxed sets, particularly in smaller structures like
the anterior commissure (ac: from 50.76% to 83.5%), corpus callosum (cc: 65.59% to
85.44%), substantia nigra (SN: 78.36 to 91.55%) and ventricles (VS: 72.56 to 81.72%).
For hippocampus and caudate putamen the values are more similar (~ 87% for Hc,
and increased from 87.67 to 90.87% for CPu).

results, compared to a reduced set of labels, embedded in the larger
brain area, perhaps by more accurately constraining individual regions deﬁnition.
Conclusion
Given recent imaging technology development, we can acquire
higher resolution mouse brain images which have eight times larger
data than the current data. Hence, there has been a pressing need

2305

Fig. 4. Segmenting strains other than the one used for generating the priors (C57BL/6)
is a more challenging task, as illustrated by the examples of a BXD29 and an APP/TTA
mouse model of AD. Using a full sampling strategy, but only a subset of 7 labels, yields
VOP for hippocampus, ranging from 94.11 ± 0.73% in the C57BL/6 (for the 5 specimens)
to 86.65% for the BXD29 and 84.97% for APP/TTA mouse. For the caudate putamen VOP
ranges from 92.21 ± 0.71% for C57BL6, to 87.68% for BXD29 and 79.28% for APP/TTA.
However smaller white matter tracts and nuclei, and especially the ventricles remain
challenging for automated segmentation (e.g. VOP for corpus callosum 86.11 ± 2.12%
in C57BL/6, 55.25% in BXD29, and 63.83% in APP/TTA).

for computationally efﬁcient segmentation method. We have presented an automated method for mouse brain images, pSVMRF,
which is computationally efﬁcient. It integrates pSVM and MRF for a
more accurate and faster segmentation by modeling the three kinds
of information which are critical for the brain image segmentation.
Even though eMRF produced a more accurate delineation of the
mouse brain MRM images than the atlas based segmentation and
the probability information based segmentation by integration of
SVM and MRF, eMRF suffers from the long training and testing time

Fig. 3. Visual assessment of comparable coronal levels through the brains C57BL/6, BXD29 and APP/TTA mouse model of AD, overlaid with automatically generated labels. The labeled regions are: anterior commisure (ac), corpus callosum (cc), caudate putamen (CPu), hippocampus (Hc), susbtantia nigra (SN) and the ventricular system (VS).

2306

T. Wu et al. / NeuroImage 59 (2012) 2298–2306

due to the use of SVM which requires of the long training and testing
time. To reduce the training and testing time of SVM, we use pSVM
which relies on location priors as well as MR intensity information
as input features. By the virtue of nonlinear transformation of these
two critical pieces of information, pSVM can train better models with
a small size of training sets and reduce the testing time by 92.85% compared with the SVM testing of eMRF. By using the optimal termination
point for the ICM implementation, the ICM algorithm converges much
faster with the better solution. The AVOP and AVDP of pSVMRF are improved by 2.55% and 9.57% compared with eMRF, and by 5.41% and
21.07% compared with MRF. In the future, we will make efforts to improve the performance of smaller structures in which pSVMRF still
produces poorer performance than the other two methods.
The C57BL/6 is a widely used mouse strain, at the basis of a large
number of derived strains, and therefore was chosen to create priors,
and training the classiﬁer. There is a wide interest in segmenting
other mouse strains, many of them having a C57BL/6 background, to
identify anatomical phenotypes. While more studies on larger groups
of animals from different strains are required to validate and optimize
a more general segmentation/anatomical phenotyping task in the
future, we have shown the initial applicability of the method to
other strains as well, including a recombinant inbred mouse strain
derived from parental C57BL/6 and DBA2 (BXD29), and a model of
Alzheimer's disease (APP/TTA). The improvements in accuracy while
reducing the computational time will allow us to address the issue
of brain segmentation in larger population studies, and higher resolution images, therefore facilitating image based phenotyping of mouse
models of neurological and psychiatric conditions.

Acknowledgments
The authors would like to thank Dr. Yutong Liu and Mariano G.
Uberti in the Department of Radiology of University of Nebraska,
and Sally Zimney at CIVM, Duke University Medical Center. Images
were provided by the Duke Center for In Vivo Microscopy (CIVM),
supported by NIH grants (NCRR P41 RR005959/ NCI U24
CA092656). CIVM has also received support from the Biomedical Informatics Research Network (mBIRN) (U24 RR021760).

References
Abe, S., 2005. Support Vector Machines for Pattern Classiﬁcation (Advances in Pattern
Recognition). Springer-Verlag New York, Inc., Secaucus, NJ.
Ali, A.A., Dale, A.M., Badea, A., Johnson, G.A., 2005. Automated segmentation of neuroanatomical structures in multispectral MR microscopy of the mouse brain. Neuroimage 27 (2), 425–435.
Awate, S.P., Tasdizen, T., Foster, N., Whitaker, R.T., 2006. Adaptive Markov modeling for
mutual-information-based, unsupervised MRI brain-tissue classiﬁcation. Med.
Image Anal. 10, 726–739.
Badea, A., Ali-Sharief, A.A., Johnson, G.A., 2007. Morphometric analysis of the C57BL/6J
mouse brain. Neuroimage 37 (3), 683–693 Sep 1.
Badea, A., Johnson, G.A., 2010. Remote sites of structural atrophy predict later amyloid
formation in a mouse model of Alzheimer's disease. Neuroimage 50 (2), 416–427.
Bae, M.H., Pan, R., Wu, T., Badea, A., 2009. Automated segmentation of mouse brain images using extended MRF. Neuroimage 46, 717–725.
Bae, M.H., Wu, T., Pan, R., 2010. Mix-ratio sampling: classifying multiclass imbalanced mouse
brain images using support vector machine. Expert Syst. Appl. 37 (7), 4955–4965.
Bock, N.A., Kovacevic, N., Lipina, T.V., Roder, J.C., Ackerman, S.L., Henkelman, R.M., 2006.
In vivo magnetic resonance imaging and semiautomated image analysis extend the
brain phenotype for cdf/cdf mice. J. Neurosci. 26 (17), 4455–4459.
Bourgeat, P., Fripp, J., Stanwell, P., Ramadan, S., Ourselin, S., 2007. MR image segmentation
of the knee bone using phase information. Med.Image Anal. 11, 325–335.
Chang, C., Lin, C., 2001. LIBSVM: A Library for Support Vector MachinesSoftware
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm 2001.

Dorr, A.E., Lerch, J.P., Spring, S., Kabani, N., Henkelman, R.M., 2008. High resolution
three-dimensional brain atlas using an average magnetic resonance image of 40
adult C57Bl/6J mice. Neuroimage 42 (1), 60–69 Aug 1.
Edelstein, W.A., Glover, G.H., Hardy, C.J., Redington, R.W., 1986. The intrinsic signal-tonoise ratio in NMR imaging. Magn. Reson. Med. 3, 604–618.
Fischl, B., Salat, D.H., Busa, E., Albert, M., Dieterich, M., Haselgrove, C., Van der Kouwe,
A., Killiany, R., Kennedy, D., Klaveness, S., et al., 2002. Whole brain segmentation:
automated labeling of neuroanatomical structures in the human brain. Neuron
33, 341–355.
Held, K., Kops, E.R., Krause, B.J., Wells III, W.M., Kikinis, R., Muller-Gartner, H.W., 1997.
Markov random ﬁeld segmentation of brain MR images. IEEE Trans. Med. Imaging
16 (6), 878–886.
Hsu, C.W., Lin, C.J., 2002. A comparison of methods for multi-class support vector machines. IEEE Trans. Neural Netw. 13 (2), 415–425.
Jankowsky, J.L., Slunt, H.H., Gonzales, V., Savonenko, A.V., Wen, J.C., Jenkins, N.A., Copeland, N.G., Younkin, L.H., Lester, H.A., Younkin, S.G., Borchelt, D.R., 2005. Persistent
amyloidosis following suppression of Aβ production in a transgenic model of Alzheimer disease. PLoS Med. 2 (12), 1318–1333 Dec.
Johnson, G.A., Cofer, G.P., Gewalt, S.L., Hedlund, L.W., 2002. Morphologic phenotyping
with magnetic resonance microscopy: the visible mouse. Radiology 222 (3),
789–793.
Khayatia, R., Vafadusta, M., Towhidkhaha, F., Nabavib, M., 2008. Fully automatic segmentation of multiple sclerosis lesions in brain MR FLAIR images using adaptive
mixtures method and Markov random ﬁeld model. Comput. Biol. Med. 38,
379–390.
Kovacevic, N., Henderson, J.T., Chan, E., Lifshitz, N., Bishop, J., Evans, A.C., Henkelman, R.
M., Chen, X.J., 2005. A three-dimensional MRI atlas of the mouse brain with
estimates of the average and variability. Cereb. Cortex 15 (5), 639–645.
Lee, C.H., Schmidt, M., Murtha, A., Bistritz, A., Sander, J., Greiner, R., 2005. Segmenting
brain tumors with conditional random ﬁelds and support vector machines. Lect.
Notes Comput. Sci. 3765, 469–478.
Levman, J., Leung, T., Causer, P., Plewes, D., Martel, A.L., 2008. Classiﬁcation of dynamic
contrast-enhanced magnetic resonance breast lesions by support vector machines.
IEEE Trans. Med. Imaging 27 (5), 688–696.
Li, S.Z., 2009. Markov Random Field Modeling in Image Analysis, 3rd Edition. Springer.
Luts, J., Heerschap, A., Suykens, J.A.K., Huffel, S.V., 2007. A combined MRI and MRSI
based multiclass system for brain tumour recognition using LS-SVMs with class
probabilities and feature selection. Artif. Intell. Med. 40, 87–102.
Ma, Y., Hof, P.R., Grant, S.C., Blackband, S.J., Bennett, R., Slatest, L., Mcguigan, M.D., Benveniste, H., 2005. A three-dimensional digital atlas database of the adult C57BL/6J
mouse brain by magnetic resonance microscopy. Neuroscience 135 (4),
1203–1215.
McDaniel, B., Sheng, H., et al., 2001. Tracking brain volume changes in C57BL/6J and
ApoE-deﬁcient mice in a model of neurodegeneration: a 5-week longitudinal
micro-MRI study. Neuroimage 14 (6), 1244–1255.
Mourao-Miranda, J., Bokde, A.L.W., Born, C., Hampel, H., Stetter, M., 2005. Classifying
brain states and determining the discriminating activation patterns: Support
Vector Machine on functional MRI data. Neuroimage 28, 980–995.
Platt, J., 2000. Probabilistic outputs for support vector machines and comparison to
regularized likelihood methods. Advances in Large Margin Classiﬁers. MIT Press,
Cambridge, MA.
Powell, S., Magnotta, V.A., Johnson, H., Jammalamadaka, V.K., Pierson, R., Andreasen, N.
C., 2008. Registration and machine learning-based automated segmentation of
subcortical and cerebellar brain structures. Neuroimage 39, 238–247.
Reddick, W.E., Glass, J.O., Cook, E.N., Elkin, T.D., Deaton, R.J., 1997. Automated
segmentation and classiﬁcation of multispectral magnetic resonance images
of brain using artiﬁcial neural networks. IEEE Trans. Med. Imaging 16,
911–918.
Rohlﬁng, T., Brandt, R., Menzel, R., Maurer Jr., C.R., 2004. Evaluation of atlas selection
strategies for atlas-based image segmentation with application to confocal microscopy images of bee brains. Neuroimage 21 (4), 1428–1442.
Sharief, A.A., Johnson, G.A., 2006. Enhanced T2 contrast for MR histology of the mouse
brain. Magn. Reson. Med. 56 (4), 717–725 Oct.
Sharief, A.A., Badea, A., Dale, A.M., Johnson, G.A., 2008. Automated segmentation of the
actively stained mouse brain using multi-spectral MR microscopy. Neuroimage 39,
136–145.
Song, S., Zhan, A., Long, A., Zhang, J., Yao, L., 2011. Comparative study of SVM methods
combined with voxel selection for object category classiﬁcation on fMRI data. PLoS
One 6 (2), e17191. doi:10.1371/journal.pone.0017191.
Yu, S.N., Li, K.Y., Huang, Y.K., 2006. Detection of microcalciﬁcations in digital mammograms using wavelet ﬁlter and Markov random ﬁeld model. Comput. Med. Imaging
Graph. 30 (3), 163–173.
Zhang, Y., Smith, S., Brady, M., 2001. Segmentation of brain MR images through a
hidden Markov random ﬁeld model and the expectation–maximization algorithm.
IEEE Trans. Med. Imaging 20, 45–57.
Zhang, H., Wu, T., Bae, M., Chen, K., Reiman, E., Alexander, G.E., 2008. Diagnosing
Alzheimer disease using artiﬁcial neural network and support vector machines
classiﬁers. ICAD 2008: Alzheimer's Association International Conference on
Alzheimer's Disease.

Computers in Industry 57 (2006) 350–365
www.elsevier.com/locate/compind

A model for inbound supply risk analysis
Teresa Wu a, Jennifer Blackhurst b,*, Vellayappan Chidambaram a
b

a
Department of Industrial Engineering, Arizona State University, P.O. Box 875906, Tempe, AZ 85287-5906, USA
Department of Logistics, Operations and MIS, Iowa State University, 3131 Gerdin Business Building, Ames, IA 50011-1350, USA

Received 15 October 2004; accepted 24 November 2005
Available online 25 January 2006

Abstract
Managing risk has become a critical component of supply chain management. The implications of supply chain failures can be costly and lead
to significant customer delivery delays. Though, different types of supply chain vulnerability management methodologies have been proposed for
managing supply risk, most offer only point-based solutions that deal with a limited set of risks. This research aims to reinforce inbound supply
chain risk management by proposing an integrated methodology to classify, manage and assess inbound supply risks. The contributions of this
paper are four-fold: (1) inbound supply risk factors are identified through both an extensive academic literature review on supply risk literature
review as well as a series of industry interviews; (2) from these factors, a hierarchical risk factor classification structure is created; (3) an analytical
hierarchy processing (AHP) method with enhanced consistency to rank risk factor for suppliers is created; and (4) a prototype computer
implementation system is developed and tested on an industry example.
# 2006 Elsevier B.V. All rights reserved.
Keywords: Analytical hierarchy process (AHP); Inbound supply risk analysis; Prototype implementation

1. Introduction
Inbound supply risk is defined as the potential occurrence of
an incident associated with inbound supply from individual
supplier failures or the supply market, resulting in the inability
of the purchasing firm to meet customer demand [1] and as
involving the potential occurrence of events associated with
inbound supply that can have significant detrimental effects on
the purchasing firm [2]. These risks or supply chain failures can
be costly and lead to significant delays in customer deliveries.
Therefore, managing supply risk is a critical component of
managing the supply chain. Consequently, it is important to an
organization’s success to understand the sources of supply risk
and how to best manage them [3].
A typical supply chain system can be large in scale, having
many tiers of suppliers, where each supplier tier of the supply
chain provides goods or services to the next level supplier tier in
the supply chain. Moreover, each tier may have multiple
components or members, creating a mesh network within the

* Corresponding author. Tel.: +1 515 294 2839; fax: +1 515 294 2534.
E-mail addresses: jvblackh@iastate.edu, jenblackhurst@earthlink.net
(J. Blackhurst).
0166-3615/$ – see front matter # 2006 Elsevier B.V. All rights reserved.
doi:10.1016/j.compind.2005.11.001

supply chain where the linear flow of goods is rare [4].
Managing inbound supply risk can be a challenging task due in
part to the complex and dynamic nature of supply chain
systems. Risk is inherent in such systems and can manifest itself
in uncertainty existing in demand requirements, capacity,
delivery time, manufacturing time, and costs [5]. The
detrimental effects of uncertainties may be compounded by
inefficient operations in manufacturing systems, production
policies, and inflexible process changes, which are for most part
controllable by supply chain managers. There are tools for
managing controllable uncertainties such as theory of
constraints (TOC) for improving production efficiencies [6],
accurate response [7] for improving forecasting accuracy and
multi-channel manufacturing [8] for managing demand
fluctuations, to name a few.
Uncertainty also exists due to other factors such as changes
in markets, technology, competitors, political issues, and
governmental regulations [9]. Recent examples include the
2002 US west coast port strike, terrorist attacks such as
September 11, 2001, the 1999 Taiwan earthquake, and most
recently, Hurricane Katrina in 2005. These type of uncertainties, though difficult to control, can be managed through
efficient contingency planning. One such example is Sears, a
US based retail company, who managed the 9/11 crisis by

T. Wu et al. / Computers in Industry 57 (2006) 350–365

creating a contingency cell or team at its head quarters
immediately after the incident to control nation-wide truck
traffic [10]. Relatively, there are many failure stories in
managing uncontrollable uncertainties as cited by Martha and
Subbakrishna [11] which are primarily due to lack of
contingency plans or poor execution. One example of poor
contingency planning is as follows: US based auto manufacturer Ford Motor Company shutting down five of its US plants
during the 9/11 incident because components could not be
delivered from Canada [12].
Inbound supply chain risk management has drawn increasing attention from both practitioners and academic researchers.
Different types of supply chain vulnerabilities and management
methodologies have been proposed. Yet, most of them offer
only point-based solution that deals with one particular or a
limited set of risks or may focus on outbound supply risk rather
than inbound [3,13–19]. There is a need, therefore, for a general
inbound supply risk management methodology to classify,
manage and assess risks. This research proposes a supply chain
risk management framework concentrating on inbound supply
risk analysis. The contributions of this research are four-fold:
first, risk factors are enumerated through a review of the
literature and industry interviews. Second, a general risk
classification structure is proposed. Third, pair-wise comparison method using analytical hierarchy processing (AHP) with
enhanced consistency is developed to assist supply chain
managers in assessing risks. Fourth, a prototype system is
implemented and tested on an industry example.
This paper is structured as follows. Section 2 reviews exiting
supply chain risk management methods. The proposed
methodology is illustrated in Section 3. Section 4 details the
implementation prototype system followed by an industry
application example in Section 5.
2. Inbound supply risk
The area of risk analysis is a well-researched topic. However,
much of the research on risk and the physical flow of goods in a
supply chain has focused more in the outbound flow rather than
inbound supply risk analysis [20]. Zsidisin and Ellram [3] state
that a supplier’s failure to deliver inbound goods or services can
have a detrimental effect throughout the purchasing firm and
subsequently through the supply chain, ultimately reaching the
consumer. Therefore, we focus specifically on research in the
area of inbound supply risk analysis. In this section, an inbound
risk analysis procedure is defined, followed by a review of the
literature in terms of this procedure.
2.1. Inbound risk analysis procedure
Supply risk analysis is a systematic procedure [21] and often
times it is the most detailed component of a risk management
system [22]. Based on review of the literature, the risk analysis
procedure is classified into four components: risk classification,
risk identification, risk calculation and implementation/validation [23]. The methodology developed in this paper is therefore
described in these terms.

351

2.1.1. Risk classification
The primary purpose of classifying risk is to get a collective
viewpoint on a group of factors, which will help the managers
to identify the group that contributes the maximum risk. This
can enable supply managers to give the required level of
importance (in the risk management process) for every group/
type of factors. A classification system can be designed to cater
the needs of a particular supply chain or can be chain
independent.
2.1.2. Risk identification
This step entails the enumeration of risk factors, is
performed to be later categorized into appropriate branches
in the classification system. It can be product focused, which
means all the factors generated will focus on a particular
product type, or supplier focused, where all the factors will be
oriented to evaluate a particular supplier, which might be used
as an abstract level risk for all kinds of product types that is
provided by that supplier.
2.1.3. Risk calculation
In general, risk factors identified in the previous step need to
be evaluated to calculate the factor’s impact on overall risk.
This step can be viewed as a decision support tool for predicting
risk factors.
2.1.4. Implementation/validation
Usability is a factor for implementing a risk analysis system.
It refers to the functionality of the application, as to whether it
can be used only for sourcing decisions or only for
strengthening existing supply base or for both. Validating the
system needs external support from supply chain managers to
provide data and critique the analysis results.
In order to understand the current literature base and more
clearly define gaps in the literature, a comparison of current
inbound risk methodologies is performed based on the four
main components in a risk analysis procedure in the following
section.
2.2. Existing research in inbound risk analysis
Kraljic [13] discusses the need to recognize the extent of
supply weakness and treats the weaknesses with a 4-phase
strategy to manage supply. The strategy presents key risk
factors such as availability, number of suppliers, competitive
demand, and make-buy opportunity. Additional market
oriented aspects of supplier risks are analyzed such as capacity
utilization, break-even stability, and expected demand growth.
Steele and Court [14] address vulnerability management, which
deals with an overall supply risk analysis procedure including
risk identification, and calculation. A list of customary supply
risk evaluation factors has been enumerated and a standard risk
evaluation methodology is recommended by multiplying
probability, consequence and duration of risks. Finnman [16]
proposes a supply risk management framework and a
methodology to evaluate risks to help supplier selection
process. It uses preliminary hazard analysis (PHA) to filter out

352

T. Wu et al. / Computers in Industry 57 (2006) 350–365

Table 1
Summary of existing research (inbound supply risk)
Citations

Kraljic [13]

Steele and Court [14]

Zsidisin and Ellram [15]

Finnman [16]

Zsidisin [2]

Risk classification
Risk identification
Risk calculation
Implementation/validation

Chain dependent
Product-focused
No calculation
No

Chain dependent
Product-focused
Weighted sum
No

Chain dependent
Product-focused
Highest factorial risk
Yes

Chain dependent
Supplier-focused
AHP
Yes

Chain dependent
Product-focused
No calculation
Yes

low risk suppliers. The rest of the high-risk suppliers undergo a
second phase risk evaluation using analytical hierarchy
processing (AHP) technique. While it has been proven that
AHP is effective to evaluate the risks, Finnman’s work deals
with one level pair-wise comparison risk and the issue of
consistency not considered. Zsidisin and Ellram [15] propose a
10-step approach for risk assessment by giving equal
importance for 8 identified risk factors and using a 5-point
nominal risk scale. The maximum of the factorial risk is
assigned as the overall risk of a project. It implements a
nominal risk scale using a scorecard based on best practice case
study. In 2003, Zsidisin [2] extends his work to propose a new
supply risk classification system and identifies key risk factors
based on case studies and managerial interviews.
Next, the literature is compared based on the four
components discussed in Section 2.1. Table 1 summarizes
the results.
In summary, all of the models are developed not for general
supply chains but are supply chain type dependent. Secondly,
most of existing research relies on a product-focused approach.
While this approach has value, a supplier-focused approach
may be more appropriate from a supply risk identification
perspective. Certainly, it has been recognized that with
approaches such as lean manufacturing, total quality management (TQM) and other initiatives applied to the growing
number of global supply chains, these systems are more than
ever, susceptible to disruptive events [24]. Managing risk from
a supplier perspective in such large global networks can help
companies to identify and manage sources of risk to their
inbound supply. (See Zsidisin et al. [25] for a high level analysis
of inbound supply risk assessment methods found through case
studies of seven firms.) Thirdly, choosing the risk calculation
technique is crucial. A variety of methods including weighted
sum, highest factorial risk and AHP (a basic version that does
not consider consistency) are used. AHP is a promising
technique, which reduces the complex and error-prone risk
assessment process to a series of straightforward one-to-one
comparisons, then synthesizes results. Yet, the consistency
issue in AHP needs to be addressed. Lastly, the system should
be implemented to aid both sourcing decisions and strengthening the existing supply base. Based on these findings, there is a
need for an integrated methodology to address inbound supply
risk that is discussed in the following section.
3. Proposed inbound supply risk methodology
In this research, an inbound supply risk analysis
methodology is proposed, which includes four components:

(1) a supply chain independent risk classification; (2)
supplier-based risk identification; (3) an enhanced AHP
based risk calculation; and (4) a prototype computer implementation system. The first three components are detailed in
this section and the implementation system is explained in
Section 4.
3.1. Risk classification
In this paper, a wide range of inbound supply risk
characteristics have been compiled from reviewing the
literature and conducting industry interviews with supply
chain and purchasing executives to create a new hierarchical
supply risk classification system. Table 2 lists examples of the
risk factors identified from the literature review.
Based upon a literature review and industry interviews a
hierarchical inbound risk classification system is created based
upon internal and external types of risk. Executives from four
different industries were interviewed as a validation method
and were asked to critique the classification. Industries
interviewed included PC manufacturing (assembly and
distribution), PC manufacturing (OEM component manufacturer), information technology and food manufacturing.
Examples of job titles interviewed included Vice President
of Operations, Vice President of Procurement, and Procurement
Manager and Project Manager. Using the literature base and the
interviews, the authors have enumerated and classified inbound
supply risk factors. These factors have been grouped by the
following categories: internal: controllable, partial controllable, and uncontrollable and external: controllable, partial
controllable, uncontrollable. The hierarchical structure of the
risk classification system is based upon the following
categories:
 ‘‘Internal Controllable’’ refers to the internal risk factors that
originate from sources that are most likely controllable by the
company. Examples are the quality and cost of the product.
 ‘‘Internal Partially Controllable’’ refers to the internal risk
factors that originate from sources that are partially
controllable by the company. For example, fire accident in
the company.
 ‘‘Internal Uncontrollable’’ refers to the internal risk factors
that originate from sources that are uncontrollable by the
company.
 ‘‘External Controllable’’ refers to the external risk factors
that originate from sources that are most likely controllable
by the supplier company. For example, selection of the next
tier suppliers.

T. Wu et al. / Computers in Industry 57 (2006) 350–365

353

Table 2
Risk factors identified by literature review
Source

Factors

Cooke [26]

Risks of supplying market from or over-dependence on a foreign plant,
especially during times of political tension, and risk stemming from
increased regulation
Quality, delivery, price, production facilities and capacity, technical capability,
financial position, management and organization, performance history,
warranties and claim policies
Procedural compliance, communication system, operating controls, and
labor relation record
Personnel lacking knowledge and ability to manage/absorb new processes
Lockout or shutdown of transportation hubs such as docks, impacts of
conflicts between employer management and labor groups, and
reaction of employers in the form of work slowdowns to changes such
as deployment of information technology
Natural accidents, normal accidents (system overloads), and abnormal
accidents (deliberate evil intentions)
Impacts of labor law and trade policy, and different corporate cultures
and complex management structures between vendor and supplier
Terrorism, cyber-vandalism, other criminal activity, natural disasters
Natural disaster and technology failure
Natural events like earthquake, floods, or summer heat wave
Price, inventories and schedule, technology, and quality
Changes in markets, products, technology, competitors and
governmental regulations.
Availability, configuration, and currencies
Supplier integration and communication
Capacity constraints, cycle time, disasters, financial health of suppliers,
legal liabilities, currency fluctuations, management vision, market price
increases, incompatible information systems, and product design changes
Unanticipated changes in the volume requirements and mix of items needed,
production or technological changes, price increases, product unavailability,
and product quality problems

Dickson [27]

Gooley [28]
Machalaba and Kim [29]

Mitroff and Alpalsan [30]
Seaman and Stodghill [31]
Simpson [20]
Stafford et al. [32]
Terhune [33]
Treleven and Schweikhart [34]
van der Vorst and Beulens [9]
Virolainen and Tuominen [35]
Wagner [36]
Zsidisin [2]

Zsidisin and Ellram [3]

 ‘‘External Partially Controllable’’ refers to the external risk
factors that originate from sources that are partially controllable
by the supplier company. For example, customer demand can be
partially impacted by company’s promotion plan.
 ‘‘External Uncontrollable’’ refers to the external risk factors
that originate from sources that are uncontrollable by the
supplier company. Examples are nature disasters such as
earthquake, tsunami.
The system is based upon a difference of internal versus
external type of risks. This helps the manager get a clear idea of
where the risks are located. Controllable versus partial
controllable versus uncontrollable again helps the manager
clearly identify the type of risk factors in order to better
determine how to handle avoid or mitigate each type. The
industry executives interviewed are confirmed and fine-tuned
this classification through series of interviews.
The proposed classification structure addresses all the key
aspects of a classification system. It gives a collective
viewpoint of factor groups. From a tactical perspective, it
will enable supply managers to identify the group of factors that
contributes the maximum level risk. This will enable supply
managers to classify the importance (in the risk management
process) for every group of factors. In a strategic perspective it
will enable the ability to outline long-term plans.

3.2. Risk identification
Risk identification is an important component in the risk
analysis procedure. The procedure followed in this research
included: (1) developing a set of risk factors based upon a
review of the relevant supply risk literature; (2) creating a
prototype classification system for supplier based risk; (3)
validating the classification system with industry managers: an
electronics distributor, a food services manufacturer, an
electronics manufacturer, and a PC manufacturer; (4) finalizing
the risk classification system (using both the literature review
and the industry interviews) comprised of 54 individual factors;
and (5) consolidating these factors into 19 supply risk groups.
Fig. 1 shows the consolidated factors. Definitions of each factor
group are presented in Appendix A.
3.3. Risk calculation
The key aspects of this phase are: (1) calculate the relative
weights of the risk factors at two levels of the classification
system; (2) evaluate the probability of the occurrence of each
risk factor; (3) calculate the risk (the weight of the risk factor *
the probability of the risk factor). In this research, the
occurrence probability is collected from industry manager and
is given. Therefore, techniques to compute the relative weights

354

T. Wu et al. / Computers in Industry 57 (2006) 350–365

Fig. 1. Identified risk factors fitted into the new classification system.

of risk factor and carry out the risk calculation are the focus of
this research.
3.3.1. AHP with enhanced consistency
AHP is a multi attribute decision-making (MADM)
technique [37]. It enables a decision maker to structure a
MADM problem visually in an attribute hierarchy. Dey [38]
states that as risks are subjective by nature and AHP is an
effective tool for predicting risk. AHP provides a flexible and
easy to understand way of analyzing complicated problems. It
allows for both subjective and objective factors to be
considered. Additionally many risk factors are conflicting
where achieving success in one factor lead to sacrificing
another. Therefore, AHP gives managers a rational basis for
decision-making.
AHP depends on a hierarchy where the top level drives the
focus of the problem and the bottom level consists of the
decision options. Dey [38] states that ‘‘once a hierarchy is
constructed, the decision-maker begins a prioritization procedure to determine the relative importance of the elements in
each level of the hierarchy.’’ The elements in each level are
compared as pairs with respect to their importance in making

the decision under consideration. A verbal scale is used in AHP
that enables the decision-maker to incorporate subjectivity,
experience, and knowledge in an intuitive and natural way.
After comparison matrices are created, relative weights are
derived for the various elements. The relative weights of the
elements of each level with respect to an element in the adjacent
upper level are computed as the components of the normalized
eigenvector associated with the largest eigenvalue of their
comparison matrix. Composite weights are then determined by
aggregating the weights through the hierarchy. This is done by
following a path from the top of the hierarchy to each
alternative at the lowest level, and multiplying the weights
along each segment of the path. The outcome of this
aggregation is a normalized vector of the overall weights of
the options. A comprehensive understanding of AHP can be
found in Saaty [37].
The methodology presented in this paper is an integrated
approach to risk management utilizing an enhanced AHP. To
effectively utilize AHP, a two level risk classification
structure is developed. After the classification, enhanced
AHP is applied to quantify the risk to give the manager a
measure of the robustness or the vulnerability of the supply

T. Wu et al. / Computers in Industry 57 (2006) 350–365

355

Fig. 2. AHP consistency algorithm [43].

chain. As stated earlier, AHP is applied hierarchically. Here,
AHP is used for first level, to rank how important one
category is over another category. Next is a comparison of
important one factor is over another factor in the same
category. Therefore, two weights exist. Additionally, a
subjective measure of the probability of occurrence is
considered for each risk so that the measure includes not
only its importance but also probability of occurrence. AHP
has been applied to a variety of research issues including:
supplier selection [39], design of automated cellular
manufacturing systems [40], sourcing decisions [41], and
software selection [42], just to name a few. While traditional
AHP proven effective in decision-making, inconsistency can
be an issue. AHP consistency is also known as the consistency
ratio (CR). This consistency ratio simply reflects the
consistency of the pair-wise judgments. For example,
judgments should be transitive in the sense that if A is
considered more important than B, and B more important than
C, then A should be more important than C. If, however, the
user rates A is as important as C, the comparisons
are inconsistent and the user should revisit the assessment.
Saaty [37] explains that CR is calculated by using the
equation in (1), where x stands for the maximum eigenvalue
of the pair-wise matrix, and n is the size of the pair-wise
matrix, RI is the random index value recommended by
Saaty [36].
CR ¼

ðx  nÞ=ðn  1Þ
RI

(1)

AHP has some tolerance for inconsistency, but comparisons
with a consistency index that exceeds 0.1 should be
reconsidered [43]. A number of methods were examined that
deal with inconsistency including Peters and Zelewski [44],
Ishizaka and Lusti [45], and Wedley et al. [46]. The method by
Peters and Zelewski [44] was chosen based upon its ability to
convert an inconsistent matrix into a consistent matrix as it

tends to lead to smaller Euclidean distance (the measure of
inconsistency by measuring eigenvector values in a matrix) in
comparison experiments on a small dataset. The detailed
consistency algorithm is explained in Fig. 2.
3.3.2. Enhanced AHP for risk calculation
In general, AHP reduces complex decisions to a series of oneto-one comparisons, then synthesizes results based on hierarchical structure. In this research, AHP is applied to calculate the
weights, a factor indicating how important the particular risk is.
First, comparison among six risk types ‘‘Internal Controllable’’,
‘‘Internal Partially Controllable’’, ‘‘Internal Uncontrollable’’,
‘‘External Controllable’’, ‘‘External Partially Controllable’’ and
‘‘External Uncontrollable’’ is conducted with Wi (i = 1, . . ., # of
risk types) calculated. Second, the comparison within each ‘‘risk
type’’ is conducted with RWij calculated (i = 1, . . ., # of risk
types, j = 1, . . ., # of factors in the ith risk type). With the
probability of each Level II factors provided by manager, termed
Pij (i = 1, . . ., # of risk types, j = 1, . . ., # of factors in the ith risk
type), the risk can be quantified by:
IUF ¼

n X
m
X

ðWi  RWij  Pij Þ

i¼1 j¼1
n X
m
X
Pij ¼ 1

(2)

i¼1 j¼1

i ¼ 1; 2; . . . n
j ¼ 1; 2; . . . m
where IUF stands for integrated uncertainty factor, which is
overall risk value for a supplier. Note that given six types of
risks discussed above, n is set to 6.
The detailed procedure is as follows: first, the original pairwise comparison matrix MI is obtained from the supply chain
manager. MI is 6  6 matrix indicating the comparison between
6 Level I factors (categories). The consistency algorithm
(Fig. 2) is then called to get Wi (i = 1, . . ., 6). Second, the supply

356

T. Wu et al. / Computers in Industry 57 (2006) 350–365

Fig. 3. Implementation system architecture.

chain manager sets the original pair-wise comparison matrices
MII(1), MII(2), MII(3), MII(4), MII(5), MII(6), where MII(1) is
12  12 matrix, MII(2) is 4  4 matrix, MII(5) is 3  3 matrix
and MII(6) is 3  3 matrix indicating the comparison between
the Level II factors under each category. Note there is no need to
retrieve MII(3) and MII(4) due to the fact that MII(3) has no
factor specified and MII(4) has only one factor. Again, the
consistency algorithm (Fig. 2) is called to generate RWij (i =
1, . . ., 6, j = 1, . . ., # of factors in the ith risk type). Third, the
supply chain manager sets the probability of occurrence of each
level II factors (i = 1, . . ., 6, j = 1, . . ., # of factors in the ith risk
type). Eq. (2) is then used to calculate IUF for each risk factor.
Introducing Wi helps to evaluate a group of risk factors as a
single entity, and to evaluate peer risk factors of similar origin.
Introducing RWij helps to evaluate consequence or impact-onbusiness due to a risky event caused by that factor, and gives
required level of importance depending on the impact of that
risk factor. Pij defines the probability of occurrence of each risk
factor, which helps to evaluate likelihood of a risky event
occurring due to that risk factor. Note Wi and RWij are
calculated from AHP and Pij is provided by the supply chain
manager. Though, subjectivity might be introduced as the
probability of occurrence is input by manager, the method is
based upon getting the inputs for AHP matrices. This can be
certainly improved by plugging in a module to compute the
probability based on collected historical data, which will be
conducted in future research.
4. Prototype system implementation
It is interesting to note that some commercial software
packages, such as Expert Choice, Super Decision Software,
have been developed in an attempt to address supply risk
management. Yet, the unique two level structure and risk
calculation proposed by this research cannot be conducted
solely by such commercial software. Therefore, a prototype
system is developed and implemented in a case application to
validate the proposed framework. The application is implemented in .NET programming environment using VB.NET and
SQL Server 2000. The data can be pooled from a SQL database
server. VB.NET is used to build up forms as the front end user
interfaces. The system can be easily extended with Web forms

as user interfaces to get inputs, which most existing software is
not capable of. The overall architecture of the application with
three components identified is presented in Fig. 3. The
respective process flow charts of each component are illustrated
in Fig. 4.
4.1. Risk classification and identification
The main functionality of this component is to establish the
basic risk classification structure and fit the identified risk
factors in appropriate categories in the classification system. As
discussed in Section 3, the proposed model recommends a twolevel classification system with six-types in the second level. A
feature of this component is to let the managers customize risk
classification system. Similarly, the proposed model in Section
3, recommends a set of 19 risk factors that falls in either one of
the six risk types. Note that the managers can create their own
customized set of risk factors by adding new factors, dropping
existing factors or modifying existing factors.
The process flow of risk classification component is
represented as the first branch in Fig. 5. Steps include
customization of the risk factors, identification of the factors
and a finalized conformation.
4.2. Data input and comparison
The main functionality of this component is to obtain user
input data for pair-wise comparison matrices and the
probability of occurrence for each risk factor. As discussed
in Section 3, there would be a total of n + 1 pair-wise
comparison matrices, where n is the total number of risk types.
The first matrix will always compare risk types and the
following matrices will compare risk factors within each risk
type. As the main purpose of pair-wise comparison is to obtain
relative weights for risk types and risk factors within each risk
type, this component is designed to do the calculation for
relative weights and consistency ratio using the data inputted
from pair-wise matrices.
The process flow of data input component is represented as
the second branch in Fig. 5. There are two sub-branches in this
component. The first sub-branch is for pair-wise data input
while the second sub-branch is for probability data input. In the

T. Wu et al. / Computers in Industry 57 (2006) 350–365

357

Fig. 4. Process flow.

first sub-branch, the first form would be to obtain pair-wise
inputs for risk types and depending on how many risk types are
chosen in the risk classification component, there would be that
many number of pair-wise forms for risk factors. Immediately
after obtaining user input for each pair-wise matrix, the
consistency ratio will be calculated and if it is above threshold
value established by Saaty [36], then the consistency
improvement algorithm will be executed and the matrix scores
will be modified so that the matrix is turned into a consistent
matrix. Finally, after the matrix is reconfigured, the relative
weights will be calculated and displayed. In the second subbranch, the first form is to add new supplier information or
choose a particular supplier to proceed further to the next form
to enter probability of occurrence for each risk factor.

4.3. Risk analysis and summary measure
This component calculates the overall and factor-wise risk
for each supplier. The main functionality of this component is
to calculate risk based on the formula given in Section 3. The
risk calculation utilizes relative weights calculated from the
previous component. The risk value is calculated for every
risk factor and is also summed up to give the overall risk
value.
The third branch of the process flow chart in Fig. 5 represents
this component. The first form is for choosing a particular
supplier from the list of existing suppliers. The second form is
to calculate factor-wise and overall risk for that supplier and
display it in a use-friendly manner.

Fig. 5. Customize risk classification category/sub-categories.

358

T. Wu et al. / Computers in Industry 57 (2006) 350–365

Fig. 6. Risk calculation data input.

5. Industry application of the prototype system

5.4. Step 4: risk analysis

In this section, the inbound supply risk analysis model is
applied to a major US PC manufacturer. The company termed
PC manufacturer (PCM) designs, develops, manufactures and
supports a wide range of computer systems to custom
requirements. PCM relies heavily of its supply base and
therefore, managing inbound supply risk is of interest to PCM.
The model was applied to two key suppliers to test the
application system and the results were discussed with supply
chain managers for validation of the results.

The PCM case was implemented and the final results were
obtained from the ‘risk analysis’ component of the application.
Two suppliers (denoted as Supplier-1 and Supplier-2) were
analyzed to estimate the level-of-risk they pose to the inbound
supply of PCM. It was found that Supplier-2 was riskier than
Supplier-1. In a risk scale of 0–1, the risk values were
determined to be 0.31 for Supplier-2 and 0.17 for Supplier-1. In
other words, Supplier-2 is almost 80% more risky than
Supplier-1. Refer Fig. 8 for details. As a word of caution, this
risk value is just an indicator of which zone is riskier than the
other, but it does not necessarily give an absolute value.
Fig. 9(a) provides detail of Supplier-1’s risk among the 4 risk
types. It is interesting to note that 89% of the risk is caused due
to internal factors and 11% is due to external factors. This might
sound a caution to assign more resources to managing internal
risks. Among external factors, risks due to uncontrollable
factors are higher than partially controllable factors. This infers
that though uncontrollable factors generally have less probability-of-occurrences it still has a large consequence-ofoccurrence. Similarly, Fig. 9(b) provides detail of Supplier-2’s

5.1. Step 1: risk classification
The proposed model offers 6 risk types and 19 risk factors in
each risk type. The implementation system allows the manager
to customize the model by only picking the factors that are
applicable to their supply base (shown in Fig. 5). In this case,
four types of risk are considered: ‘‘Internal Controllable’’,
‘‘Internal Partially Controllable’’, ‘‘External Partial Controllable’’ and ‘‘External Uncontrollable’’. Thus, the pair-wise
comparison matrix MI is 4  4 matrix.
5.2. Step 2: input data
As shown in Fig. 6, the data for pair-wise matrices and
probability of occurrence are entered in this component. The
first part deals with calculating relative weights for risk types
and risk factors. There is one matrix for comparing 4 risk types
and four matrices for comparing risk factors within each risk
type. The second part handles the probability of occurrence of
each risk factor. See Tables A.6–A.11 in the appendix for
detailed data input from the PCM supply chain manager.
5.3. Step 3: risk calculation
After all the information is entered, the AHP consistency
algorithm (Fig. 2) is first applied to the matrices gathered from
Step 2 to get Wi (i = 1, 2, 3, 4) and RWij (i = 1, . . ., 4, j = 1, . . ., #
of factors in the ith risk type). Given probability information,
Eq. (2) is then used to calculate the integrated risk value
(Fig. 7).

Fig. 7. Risk calculation for one supplier.

T. Wu et al. / Computers in Industry 57 (2006) 350–365

Fig. 8. Comparative analysis of overall risk values.

risk among the 4 risk types. Supplier-2 gets most of its risk from
internal factors. Note that within internal factors; partially
controllable risk is higher than it was in Supplier-1’s case. This
means that factors like supplier’s market strength, and
continuity of supply needs to be monitored more carefully
than it was in Supplier-1.
A detailed treatment of risk factors will help to gain insights
on prioritizing future risk management activities. A Pareto
analysis of relative weights among risk factors revealed that the
top 5 factors garnered 80% of PCM’s attention among a total of

359

18 risk factors. Cost, quality, on-time delivery, continuity of
supply, and engineering/production were the 5 risk key factors
for PCM.
The graph in Fig. 10 shows a comparative analysis between
Supplier-1 and Supplier-2 on each of the top-10 risk factors.
The risk factors are arranged as per PCM’s preference level
from left to right. It is clearly seen that Supplier-2 considerably
exceeds Supplier-1 in almost all risk factors. In examining the
top-5 factors it is clear that Supplier-2 is riskier than Supplier-1
in quality, on-time delivery, and continuity of supply. Therefore, emphasis must be laid on Supplier-2’s management to
reduce risk in top-5 factors especially the 3 factors mentioned
above.
Supplier-1 should also continuously work to bring down the
probability of occurrence on its key risk factors, which will help
to reduce its overall risk. Table 3 gives the exact risk scores for
the top-10 risk factors for Supplier-1 and Supplier-2.
In another analysis conducted separately for Supplier-1 and
Supplier-2, it was found that Supplier-1 and Supplier-2 had
different sets of top-5 risk factors. Though, Supplier-1 and
Supplier-2’s top-5 factors are more similar to PCM’s top-5
factors they were ordered in a different manner. The top 5 risky
factors for Supplier-1 were cost, on-time delivery, quality,

Fig. 9. Percentage of total risk by risk categories for Supplier-1 and Supplier-2.

Fig. 10. Comparative analysis of suppliers.

360

T. Wu et al. / Computers in Industry 57 (2006) 350–365

Table 3
Comparative analysis of suppliers on PCM’s top10 risk factors
Risk factors

PCMs
preference (%)

Risk of
Supplier-1

Risk of
Supplier-2

Cost
Quality
On-time delivery
Continuity of supply
Engineering/production
II Tier supplier
Demand
Internal legal issues
Natural/man-made disasters
Politics/economics
Others

23.153
21.727
18.838
11.767
3.978
2.979
2.832
2.589
2.414
2.414
7.305

0.046
0.021
0.037
0.011
0.015
0.008
0.002
0.002
0.009
0.002
0.018

0.046
0.065
0.075
0.058
0.019
0.011
0.011
0.002
0.007
0.002
0.017

engineering/production and continuity of supply. These 5
factors contribute to 80% of total risk value for Supplier1. The
top 5 risky factors for Supplier-2 were on-time delivery, quality,
continuity-of-supply, cost, and engineering/production. These
5 factors contribute to 87% of total risk value for Supplier-2.
6. Conclusion and future research
Managing risk in inbound supply chain operations has
become increasingly important in today’s competitive and
globally dispersed environment. Supply chain managers spend
a significant amount of time and resources to manage inbound
supply risk. Some methods include implementing safety
mechanisms such as expediting orders, frequent checking of
order status, and buffer inventories. Recent events such as 9/11
and labor strikes have brought the issue of risk to the forefront
and while risk management is listed on agenda of many

companies, the implementation is disjointed and inconsistent.
In fact, in many cases it is up to the manager’s experience to
assess and evaluate the risk. In addition, the major research gaps
identified are analyzing the risks from root cause, generating
comprehensive list of risk factors and fitting into a suitable risk
classification system to strengthen the risk management
methodologies. There is, therefore, a need for an integrated,
systematic approach to assess and manage inbound supply risk.
This paper has presented an inbound supply risk analysis
methodology to classify, manage and assess the risks. This
methodology classifies the risk factors in a hierarchical
structure that, identifies supplier-oriented risk factors collectively through both literature review and managerial interviews,
and applies analytical hierarchy processing technique to
calculate weight of risk factors, which is an indicator how
important the risk factor is. A scalable, customizable
implementation presented for ease of use.
The model developed was built to address the key aspects of
a risk analysis procedure and proved to be promising. However,
there are some recommendations for further developments.
First, the scope of the model is limited to a single-tier
environment. It could be extended to multi-tier supply chain.
Secondly, the risk calculation equation in the current model did
not consider the length of time a particular risk exists but just
considers the probability and consequence of occurrence. It can
be improved by including a time parameter while calculating
the risk. Thirdly, the probability of the factors is collected from
industry; this can be improved by computing the probability
based on historical data.
Appendix A
See Tables A.1–A.11.

T. Wu et al. / Computers in Industry 57 (2006) 350–365
Table A.1
Internal controllable risk factors

361

362
Table A.2
Internal partially controllable risk factors

Table A.3
External controllable risk factors

Table A.4
External partially controllable risk factors

T. Wu et al. / Computers in Industry 57 (2006) 350–365

T. Wu et al. / Computers in Industry 57 (2006) 350–365
Table A.5
External uncontrollable risk factors

Table A.6
Pair-wise matrix for risk types

Table A.7
Pair-wise matrix for risk factors in ‘‘Internal Controllable’’

Table A.8
Pair-wise matrix for risk factors in ‘‘Internal Partially Controllable’’

363

364

T. Wu et al. / Computers in Industry 57 (2006) 350–365

Table A.9
Pair-wise matrix for risk factors in ‘‘External Partially Controllable’’

Table A.10
Pair-wise matrix for risk factors in ‘‘external uncontrollable’’

Table A.11
Probability for risk factors
No.

Risk factors

Probability for Supplier 1

Probability for Supplier 2

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.

Quality
Cost
On-time delivery
Engineering/production
Technical/knowledge resources
Financial and insurance issues
Management related issues
Accidents
Market strength
Internal legal issues
Continuity of supply
II Tier supplier
External legal issues
Demand
Security
Natural/man-made disasters
Political/economical stability
Market characteristics

1
2
2
4
3
7
6
1
2
1
1
3
2
1
2
4
1
1

3
2
4
5
4
2
4
2
3
1
5
4
1
4
2
3
1
3

References
[1] G.A. Zsidisin, Defining supply risk: a grounded theory approach, in:
Proceedings from the Decision Sciences Institute Annual Meeting, San
Diego, CA, 2002.
[2] G.A. Zsidisin, Managerial perceptions of supply risk, Journal of Supply
Chain Management 39 (1) (2003) 14–25.
[3] G.A. Zsidisin, L.M. Ellram, An agency theory investigation of supply risk
management, Journal of Supply Chain Management 39 (3) (2003) 15–29.
[4] C. Riddalls, S. Bennett, N. Tipi, Modeling the dynamics of supply chains,
International Journal of Systems Science 31 (8) (2000) 969–976.
[5] D. Taylor, D. Brunt, Manufacturing Operations and Supply Chain Management: The Lean Approach, Thomson International Business Press,
London, England, 2001.
[6] E.M. Goldratt, J. Cox, The Goal: A Process of Ongoing Improvement,
North River Press, Great Barriton, Mass, 1992.
[7] L.M. Fisher, J.H. Hammond, W.R. Obermeyer, A. Raman, Making Supply
Meet Demand in an Uncertain World, Harvard Business Review, May
1994.
[8] D.J. Kulonda, Managing erratic demand: the multi-channel manufacturing
approach, Journal of Textile and Apparel Technology and Management 2
(3) (2002).

–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–

(0–10% Probability)
(10–20% Probability)
(10–20% Probability)
(30–40% Probability)
(20–30% Probability)
(60–70% Probability)
(50–60% Probability)
(0–10% Probability)
(10–20% Probability)
(0–10% Probability)
(0–10% Probability)
(20–30% Probability)
(10–20% Probability)
(0–10% Probability)
(10–20% Probability)
(30–40% Probability)
(0–10% Probability)
(0–10% Probability)

–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–

(20–30% Probability)
(10–20% Probability)
(30–40% Probability)
(40–50% Probability)
(30–40% Probability)
(10–20% Probability)
(30–40% Probability)
(10–20% Probability)
(20–30% Probability)
(0–10% Probability)
(40–50% Probability)
(30–40% Probability)
(0–10% Probability)
(30–40% Probability)
(10–20% Probability)
(20–30% Probability)
(0–10% Probability)
(20–30% Probability)

[9] J. Van der Vorst, A. Beulens, Identifying sources of uncertainty to generate
supply chain redesign strategies, International Journal of Physical
Distribution and Logistics Management 33 (6) (2002) 409–430.
[10] E. Hellweg. Supply-chain hero. Business 2.0 [Online], available: https://
www.business2.com/subscribers/articles/mag/0,1640,35883,00.html,
January 2002.
[11] J. Martha, S. Subbakrishna, Targeting a just-in-case supply chain for the
inevitable next disaster, Supply Chain Management Review 6 (5) (2002)
18–24.
[12] J. Martha, Just-in-case operations, Warehousing Forum 17 (2) (2002).
[13] P. Kraljic, Purchasing must become supply management, Harvard
Business Review (September 2001) 407–415.
[14] P.T. Steele, B.H. Court, Profitable Purchasing Strategies: A Manager’s
Guide for Improving Organizational Competitiveness through the Skills of
Purchasing, McGraw-Hill Press, London, 1996.
[15] G.A. Zsidisin, L.M. Ellram, Supply risk assessment analysis, Practix 2 (4)
(1999) 9–12.
[16] F. Finnman, Supplier Selection when Considering Risks for Disturbances
in the Inbound Flow to Scania—A Model for Supply Chain Risk Management, M.S. thesis, Dept. of Industrial Management and Logistics, Division
of Engineering Logistics, Lund Institute of Technology, Lund, Sweden,
2002.

T. Wu et al. / Computers in Industry 57 (2006) 350–365
[17] J. Hallikas, V.-M. Virolainen, M. Tuominen, Risk analysis and assessment
in network environments: a dyadic case study, International Journal of
Production Economics 78 (2002) 45–55.
[18] J.H.M. Tah, V. Carr, Towards a framework for project risk knowledge
management in the construction supply chain, Advances in Engineering
Software 32 (2002) 835–846.
[19] P.N. Sharratt, P.M. Choong, A life-cycle framework to analyze business
risk in process industry project, Journal of Cleaner Production 10 (2002)
479–493.
[20] G.A. Zsidisin, A. Panelli, R. Upton, Purchasing organization involvement
in risk assessments, contingency plans, and risk management: an exploratory study, Supply Chain Management: An International Journal 5 (4)
(2000) 187–197.
[21] C. Chapman, S. Ward, Project Risk Management, Wiley Europe Press,
West Sussex, UK, 1996.
[22] E.H. Conrow, Effective Risk Management: Some Keys to Success, AIAA
Press, Reston, VA, 2000.
[23] V. Chidambaram, Supply Chain Risk Management: A Study on Inbound
Supply Risk Analysis, MS Thesis, Arizona State University, Department
of Industrial Engineering, 2003.
[24] G. Zsidisin, S. Melnyk, G. Ragatz, An institutional theory perspective of
business continuity planning for purchasing and supply management,
International Journal of Production Research 43 (16) (2005) 3401–3420.
[25] G. Zsidisin, L. Ellram, J. Carter, J. Cavinato, An analysis of supply risk
assessment techniques, International Journal of Physical Distribution and
Logistics Management 34 (5) (2004) 97–413.
[26] J.A. Cooke, Brave new world, Logistics Management & Distribution
Report 41 (1) (2002) 31–34.
[27] G.W. Dickson, An analysis of vendor selection: systems and decisions,
Journal of Purchasing 2 (1) (1996) 5–17.
[28] T.B. Gooley, Take charge of change! Logistics Management & Distribution Report 38 (8) (1999).
[29] D. Machalaba, Q.S. Kim, West coast docks are shut down after series of
work disruptions, The Wall Street Journal (Eastern Edition) A.2. (September 30) (2002).
[30] I. Mitroff, M. Alpasan, Preparing for evil, Harvard Business Review (April
2003) 109–115.
[31] B. Seaman, B. Stodghill, Here comes the road test: the Daimler-Chrysler
deal, Time 151 (19) (1998) 66–69.
[32] G. Stafford, L. Yu, A.K. Armoo, Crisis management and recovery: how
Washington, DC, hotels responded to terrorism, Cornell Hotel and Restaurant Administration Quarterly 43 (5) (2002) 27–40.
[33] C. Terhune, Coke sees 12% increase in earnings. Problems in Japan, India
are offset by lower taxes and brisk sales in Europe, The Wall Street Journal
(Eastern edition) B5 (17) (2003).
[34] M. Treleven, S.B. Schweikhart, A risk/benefit analysis of sourcing
strategies: single vs. multiple sourcing, Journal of Operations Management 7 (4) (1988) 93–114.
[35] V. Virolainen, M. Tuominen, Hankintatoimeen liittyvat riskit teollisuusyrityksessaK, in: H. Kuusela, R. Ollikainen (Eds.), Riskit ja Riskienhallinta, University Press, Nancy, 1998, pp. 164–178.
[36] S. Wagner, Intensity and managerial scope of supplier integration, Journal
of Supply Chain Management 39 (4) (2003).
[37] T.L. Saaty, The Analytic Hierarchy Process, McGraw-Hill Press, New
York, 1980.
[38] P. Dey, Decision support system for inspection and maintenance: a case
study of oil pipelines, IEEE Transactions on Engineering Management 15
(1) (2004) 47–56.
[39] C. Kahraman, U. Cebeci, Z. Ulukan, Multi-criteria supplier selection
using fuzzy AHP, Logistics Information Management 16 (6) (2003).
[40] F. Chan, K. Abhary, Design and evaluation of automated cellular manufacturing systems with simulation modelling and AHP approach: a case
study, Integrated Manufacturing Systems 7 (6) (1996).

365

[41] R.P. Mohanty, S.G. Deshmukh, Use of analytic hierarchic process for
evaluating sources of supply, International Journal of Physical Distribution and Logistics Management 23 (3) (1993).
[42] H. Min, Selection of software: the analytic hierarchy process, International Journal of Physical Distribution and Logistics Management 22 (1)
(1992).
[43] G.R. Finnie, G.E. Wittig, An intelligent web tool for collection of
comparative survey data with an application to IS curriculum design,
in: Proceedings of the 10th Australasian Conference on Information
Systems, Wellington, New Zealand, December 1–3, 1999.
[44] M.L. Peters, S. Zelewski, A heuristic algorithm to improve the consistency
of judgments in the analytical hierarchy process. Universität DuisburgEssen,Essen,Germany [Online], available: http://www.pim.uni-essen.de/
publikationen/peters/bericht18.pdf, 2003.
[45] Ishizaka, M. Lusti, An expert module to improve the consistency of
AHP matrices, in: Proceedings of the 9th International Conference
on Operational Research KOI 2002, Trogir, Croatia, (2003), pp. 215–
223.
[46] W.C. Wedley, B. Schoner, T.S. Tang, Starting rules for incomplete
comparisons in the analytic hierarchy process, in: V. Luis, M.Z. Fatemeh
(Eds.), Analytic Hierarchy Process, Mathematical and Computer Modeling, 17, 4–5, Pergamon Press, Oxford, 1993, pp. 93–100.
Tong (Teresa) Wu (teresa.wu@asu.edu) is an assistant professor in Industrial Engineering Department
of Arizona State University. Teresa has published
papers in International Journal of Concurrent Engineering: Research and Application, International
Journal of Agile Manufacturing, International Journal of Product Research. She received her PhD from
the University of Iowa. Her main areas of interests
are in supply chain management, multi-agent system,
data mining, computer integrated manufacturing.
Jennifer Blackhurst, PhD is Assistant Professor of
Logistics and Supply Chain Management in the
College of Business at Iowa State University. She
received her doctorate in Industrial Engineering
from the University of Iowa in 2002. Her research
interests include: Distributed Systems/Supply Chain
Modeling and Design; Supply Chain Disruption
Modeling and Management; and Collaborative Product Development. Her publications have appeared
or been accepted in such journal as Journal of
Operations Management, International Journal of Production Research,
Supply Chain Management Review and ASME Transactions: Journal of
Computing and Information Science in Engineering. She is a member of
POMS, INFORMS and DSI.
Vellay Chidambaram specializes in the fields of
manufacturing and warehouse operations, and
supply chain management. He currently works for
a high-tech computer manufacturing and services
company located at Austin, TX. This paper is a
master’s thesis work pursued by Vellay at Arizona
State University in 2003. He graduated with a BE in
Mechanical Engineering from Bharathiar University,
India and a MS in Industrial Engineering from
Arizona State University. Contact address: Vellay
Chidambaram, 12001 Dessau Road #1624, Austin, TX 78754, USA. E-mail:
vellaychiddu@yahoo.com. Tel.: +1 512 294 8818.

